Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks
Peter Wu, Paul Pu Liang, Jiatong Shi, Ruslan Salakhutdinov, Shinji Watanabe, Louis-Philippe Morency
Carnegie Mellon University, PA, USA E-mail: peterw1@cs.cmu.edu

arXiv:2101.08919v2 [eess.AS] 22 Oct 2021

Abstract—As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on serverside methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally deﬁne client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limitedbandwidth devices. Solving these challenges requires new models that achieve high-ﬁdelity reconstruction, privacy preservation of sensitive personal attributes, and efﬁciency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in clientside privacy to ensure a safer deployment of cloud-based speech processing services.
I. INTRODUCTION
Users increasingly rely on cloud-based machine learning models to process their personal data [1–7]. For example, in cloud-based automatic speech recognition (ASR) systems, audio data recorded on client-side mobile devices are typically uploaded to centralized servers for server-side processing [5– 8], which enables general server-side ASR models to improve over time and enjoy economies of scale. However, there is growing concern that sending raw speech data to the cloud leaves users vulnerable to giving away sensitive personal biometric information such as gender, age, race, and other social constructs [9].
In order to have full control over their privacy, users should be able to encrypt their data themselves before uploading to downstream applications. We refer to this type of privacy as client-side privacy, which requires removing sensitive information on the client device while keeping the resulting data compatible with cloud-based services. For example, for a cloud-based ASR service, a client-side privacy algorithm needs to remove biometric information from raw speech data while keeping the resulting audio signal useful for training the ASR model on the cloud.
As a step towards achieving client-side privacy for speech data, we contribute the following in this work:

1) We formally deﬁne client-side privacy and describe its three unique technical challenges: (1) direct manipulation and regeneration of raw data on client devices, (2) adaptability with a wide range of server-side processing methods, and (3) low time and space complexity for compatibility with limited-bandwidth client devices.
2) We study three different client-side privacy approaches for speech: signal processing, disentangled representation learning, and adversarial training.
3) We conduct experiments on protecting gender and accent information for downstream ASR systems and provide an empirical comparison of current approaches.
We ﬁnd that each of our three approaches performs well on a subset of metrics, and quantify remaining areas for improvement using multiple privacy metrics. Based on these insights, we propose several extensions for future work and call for more research in client-side privacy to ensure safe cloud-based speech processing.
We proceed by discussing related privacy algorithms in Section II. In Section III, we formalize client-side privacy and describe its unique technical challenges. Then, we describe our client-side privacy approaches for downstream ASR in Section IV and detail the experiments in Section V. Finally, we summarize our results and propose future directions in Section VII. All our code and other supplementary material can be found at https://github.com/peter-yh-wu/speech-privacy.
II. RELATED WORK
A. Client- and Server-side Privacy
One way to view privacy algorithms is by whether they preserve privacy on the client-side, the server-side, or both. Client-side algorithms execute operations on the user’s local device, and server-side algorithms run on a remote server [10– 12]. For example, one way for cloud services to strengthen data privacy is by encrypting on the client-side and decrypting on the server-side [13]. For a privacy algorithm to be client-side only, all operations must run on-device without any additional work needed on the server.
Based on these deﬁnitions, we can categorize existing ways to preserve the privacy of data processed by machine learning models. Currently, utilizing cryptography algorithms like secure multi-party computation (SMC) and fully homomorphic encryption (FHE) requires both client and server-side

components [14, 15]. Other popular approaches like federated learning and Private Aggregation of Teacher Ensembles (PATE) require both client and server-side modiﬁcations as well [11, 12, 16]. Server-side only approaches also exist, including differentially private SGD (DP-SGD) and global differential privacy [17, 18]. We identify two types of approaches that are client-side only: (1) local differential privacy and (2) methods we refer to as client-side transforms. We deﬁne client-side transforms as algorithms that anonymize sensitive information on-device while preserving content needed for downstream tasks. In other words, client-side privacy can be obtained using performant client-side transforms. We note that by being entirely on-device, client-side transforms can be used in conjunction with the aforementioned privacy algorithms by simply applying the client-side transforms ﬁrst. All three approaches that we study in this paper are client-side transforms, as detailed in Section III-A. We observe that client-side transforms are not constrained by trade-offs inherent in local differential privacy. Figure 1 summarizes the aforementioned categorization of privacy algorithms.
We note that many client-side transforms exist for downstream tasks simpler than ASR. For example, for the downstream task of storage, client-side encryption is sufﬁcient [19]. In contrast, complex downstream tasks like ASR require that the output of the client-side transforms preserve complex content like transcribable audio. Thus, a challenge arises from ensuring that this resulting content also lacks sensitive information like biometrics. Other vulnerabilities like lexicalbased ones and sensitive contextual information can also be protected using client-side transforms [20–22]. Since methods like changing language usage can mitigate these vulnerabilities much more easily than biometrics in speech, we focus on client-side privacy for downstream speech tasks here [9, 23].
Fig. 1: Privacy approaches. Unlike other algorithms, our clientside transforms are housed entirely on the client side and circumvent trade offs inherent in differential privacy.
B. Privacy-preserving Speech Processing ASR models are getting larger and more powerful, making
the case for putting them on the server side stronger [24]. Since the best ASR models are currently on the server side, measures must be taken to ensure user data sent to the server remains private.
Early research on privacy for speech data has focused on voice encryption, which aims to make the original audio

hard to recover from the encrypted data [25–27]. We note that these methods cannot be only on the client side since they require the receiver to decrypt the signal. Another more recent direction focuses on hiding speaker identity [28–34]. A range of methods relying on server-side operations have been proposed, including rearranging audio segments on the server [31] or leveraging server communication protocols [32]. Similar to research in voice encryption, these studies, to our knowledge, do not address the privacy of sensitive information beyond speaker identity like race, gender, or accent.
Speaker anonymization generally refers to approaches that hide speaker identity on the client side [29, 34, 35]. Namely, these works leverage voice conversion techniques to transform raw speech into that of another speaker [28, 29, 33– 35]. Current approaches are predominantly neural, utilizing adversarial, disentanglement, or other encoder-decoder-related architectures [29, 30, 33–39]. Since voice conversion has already shown success in anonymizing speaker identity, our client side transforms in this work extend these ideas. Among related work, several address biometric information like gender [30, 38–42], but to our knowledge only two evaluate on complex downstream tasks like ASR [30, 42]. Since both of these works report high ASR error rates, namely word error rates (WER) above 60% on LibriSpeech [43], they are unable to maintain downstream performance while preserving privacy. In our paper, we study three distinct approaches that achieve lower WER while preserving privacy.
We focus on complex downstream tasks like ASR in this work since differential privacy or on-device approaches may be preferable for simpler tasks like classiﬁcation [44–47]. In Section V, we also show that differential privacy is not suitable for ensuring privacy in downstream ASR tasks. Additionally, it is much easier to anonymize speaker identity than biometrics like gender, since the former generally requires a much smaller user data distribution shift than the latter [48]. Thus, in this paper, we study how well our client-side transforms can anonymize gender and accent, being the ﬁrst to our knowledge to explore the latter. We note that speaker anonymization is a subtask of our client-side privacy task deﬁned in Section I and detailed below.
III. CLIENT-SIDE PRIVACY
As deﬁned in Section I, client-side privacy refers to privacy obtained only via on-device operations, which remove sensitive information while preserving content needed for downstream tasks. We proceed to formalize the problem statement in Section III-A and discuss the technical challenges in Section III-B.
A. Problem Statement
We start with a set of users U each of which has access to a client-side device mu, u ∈ U. On each client-side device, data xu is collected which potentially contains information about their private attributes yu such as gender, age, race, or accent. While it is ideal to leverage shared data collected at a large scale across users, it is also imperative to prevent leakage of

private attributes yu outside of the client’s device. Therefore, the goal in client-side privacy is to learn an encrypted signal xu from xu using a privacy-preserving function fθ : xu → xu with parameters θ. fθ should perform transformations efﬁciently and learn an encrypted signal xu that balances both ﬁdelity and privacy:
1. Efﬁciency: |θ| should be small and applying the encoding function fθ : xu → xu should be fast for cheap inference and storage on resource-constrained mobile devices.
2. Fidelity: Given a downstream model trained for a certain task (e.g., ASR) deﬁned on the server, the performance of model on the encrypted signal xu should be as close as possible to that of the original signal xu.
3. Privacy: One should not be able to decode the private attributes yu from an encrypted signal xu regardless of the function used to predict private attributes.
To avoid confounding factors, both evaluation models (ASR and private attribute classiﬁer) are trained on data completely separate from those used to train encryption approaches. In this paper, we measure ﬁdelity using ASR performance, namely character error rate (CER) and word error rate (WER), and privacy using gender and accent classiﬁcation accuracy. In other words, low ASR error rate and low classiﬁcation accuracy would indicate high ﬁdelity and high privacy, respectively. Section IV contains the efﬁciency of each of our three approaches, and further details are described in Section V.
B. Technical Challenges
Client-side privacy for downstream speech tasks essentially requires one to re-generate raw audio with data-level private attributes masked out. This poses three compelling challenges. First, it requires directly manipulating the user’s audio [34, 35] and re-generating high-dimensional raw speech. Second, the encrypted audio must still be compatible with downstream server tasks without any modiﬁcation on the server. For downstream tasks like ASR, this means that the encrypted data should still be comprehensible for downstream tasks. This is challenging as it requires preserving information at the high-dimensional data level rather than the feature level. Third, methods for client-side privacy must be efﬁcient and have low time and space complexity to be compatible with limited-bandwidth client devices.
As a result, client-side privacy presents novel challenges over commonly studied server-side methods, particularly on ﬁdelity and efﬁciency perspectives. Furthermore, it is much more challenging to preserve privacy for cluster-level attributes such as race, gender, and accent as compared to individuallevel attributes such as speaker identity. This is because transforming data across clusters requires a larger distribution shift than transformations to a new speaker, who could be in the same cluster [48].
IV. CLIENT-SIDE TRANSFORMS FOR DOWNSTREAM ASR
Given our deﬁnition in Section II, client-side transforms are one approach to obtaining client-side privacy as deﬁned in Section I. Namely, client-side transforms anonymize sensitive

information on-device while preserving content needed for downstream tasks. In this paper, we study three client-side transforms adapted from existing voice conversion literature and analyze their pros and cons.

A. Pitch Standardization
For our ﬁrst client-side transform approach, we perform pitch standardization using signal processing [49, 50]. Specifically, we shift the average pitch of each utterance to a predeﬁned value while preserving formants. For each utterance, we calculate its fundamental frequency (F0) and then perform a pitch shift from that value to a reference F0. We calculate the sequence of F0’s for each utterance using REAPER,1 and deﬁne the utterance F0 as the average of the non-negative F0’s. We then use the Rubber Band Library to perform pitch shifting with formant preservation using a phase vocoder.2 For utterance u with F0 value of fu, we shift its pitch by 12 log2(fr/fu) semitones, where fr is the reference F0. This approach easily has the highest efﬁciency out of our three since it does not depend on a neural model.

B. Disentangled Representation Learning
Our second proposed approach uses variational autoencoders (VAE) to disentangle private attributes from non-private speech features [30, 42, 51]. VAEs allow us to learn a set of latent representations that best reconstruct a given input audio signal, while enforcing disentanglement into a set of speaker-dependent and speaker-independent factors [52]. Here, we deﬁne a private attribute encoder e(x; θp) that encodes the input audio signal x into speaker-dependent private factors zp. We also deﬁne a content encoder e(x; θc) that encodes x into speaker-independent content factors zc. Since the private factors should capture the private attributes y using a classiﬁer, our goal is to exclude the private factor when decoding the encrypted signal. We optimize the following loss function:

Ldis = d(e(x; θc); θd) − x 1 − λp log P (y|e(x; θc)) (1)

+ λdisKL ([e(x; θc), e(x; θp)] N (0, Id)) ,

(2)

where d(z; θd) is a decoder from latent space z back into the audio space. The ﬁrst term measures the reconstruction of the input signal, the second term measures how well zp captures the private attributes y, and the third term measures disentanglement of zc and zp by ensuring minimal correlated entries. λp and λdis are tunable hyperparameters controlling the tradeoff between privacy disentanglement and performance.
We train a convolutional VAE using the hyperparameters described in Chou et al. [51]. The model has log-magnitude spectrograms as its input and output acoustic features, and we use the Grifﬁn-Lim algorithm to convert the model output into waveforms [53]. Instance normalization is added to the content encoder e(x; θc) in order to remove speaker information. Also, an adaptive instance normalization layer is added to

1https://github.com/google/REAPER 2https://github.com/breakfastquay/rubberband

the decoder in order to add the desired speaker information [54, 55]. This allows for the reconstruction of content while transforming speaker information. As far as we are aware, this architecture is considered fairly recent among the voice conversion literature [35, 36, 51]. Given the success of related architectures in anonymizing speaker identity [42, 56], we study this model’s efﬁcacy in anonymizing biometrics like gender and accent.
C. Adversarial Training
Reconstruction of high-dimensional signals is difﬁcult and has been shown to cause poor generation quality [57–59]. Our ﬁnal approach attempts to ﬁx this by using adversarial training to ensure high-ﬁdelity generation of encrypted audio [56, 60, 61]. In the ﬁrst stage, we disentangle the input audio x into speaker-dependent private factors zp and speaker-independent content factors zc learned using an auto-encoder, similarly to Section IV-B. The second stage trains a generative adversarial network (GAN) [62] to generate realistic audio. The generator is conditioned on the content factor zc and a new speaker label. The discriminator predicts whether an audio sample was from the true dataset or from the generator, and also classiﬁes the speaker. The loss function for the generator is
log c2(x) + log(1 − c2(g(x, y)) − log Pc2 (y|g(x)), (3)
where g is the generator, and the loss function for the discriminator is
− log c2(x) − log(1 − c2(g(x, y)) − log Pc2 (y|x). (4)
This model is suitable for our work because it explicitly separates speaker identity from content twice. We additionally modify this architecture by substituting the speaker label with either the gender label or the accent label. We refer to these three approaches as the speaker, gender, and accent adversarial approaches. While this architecture predates our disentangled one, we observe that our modiﬁed methods outperform the disentangled approach on multiple metrics, as detailed in Section V. We train a convolutional autoencoder using the hyperparameters described in Chou et al. [60]. The model has log-magnitude spectrograms as its input and outputs acoustic features, and we use the Grifﬁn-Lim algorithm to convert the model output into waveforms [53]. Thus, we note that our reported results in Section V can be further improved using more complex vocoders [63].
V. EXPERIMENTS
Our experiments test whether our proposed approaches are able to balance the trade-offs in ﬁdelity, privacy, and efﬁciency required for client-side privacy. We test these approaches on masking gender and accents in speech recognition.3
3Our code and models are publicly available at https://github.com/peter-yhwu/speech-privacy.

A. Setup
Datasets: We train all of our encryption models on the VCTK corpus, and the ASR model on LibriSpeech [43, 64]. For both our VCTK and LibriSpeech experiments, we test on speakers unseen during training. We train our privacy attribute classifer on the respective dataset used during testing. In our LibriSpeech experiments, the ASR model and the gender classiﬁer are both evaluated on the test-clean subset. In our VCTK experiments, we evaluate on a hold-out set of 20 speakers comprised of 10 males and 10 females.
Classiﬁer: We use the VGGVox model, a modiﬁed version of the VGG-16 CNN, as our privacy attribute classiﬁer [65, 66], slightly modifying the network by adding a ReLU activation followed by a fully-connected layer with size2 output. We approximate the data available to an adversary by training in two stages: 1. we pre-train the classiﬁer on 100 hours of labeled, unmodiﬁed speech from the train data, and 2. we ﬁne-tune the classiﬁer on the encrypted speech of a handful of speakers from the same subset. For each privacy attribute, we measure the masking ability of each encryption approach by calculating the classiﬁer’s accuracy on an encrypted version of the test subset after being ﬁnetuned on data encrypted using the respective approach.
ASR model: Unless mentioned otherwise, we use a pretrained ESPNet Transformer ASR model to evaluate downstream ASR performance [67]. This model was trained on 960 hours of LibriSpeech data.
B. Privacy-Fidelity Tradeoff
Table I compares gender classiﬁcation accuracy with CER and WER for different levels of Gaussian noise added to the VCTK test data at the waveform level. We increment the standard deviation of the noise by 0.01 for each subsequent experiment. As expected, we observe a negative correlation between classiﬁcation accuracy and ASR performance. In other words, these results reveal a tradeoff between privacy and ﬁdelity.

TABLE I: Tradeoff between privacy and ﬁdelity on the VCTK test set for different levels of added Gaussian noise. We observe a negative correlation between classiﬁcation accuracy and ASR performance, as expected.

Noise
0.00 0.01 0.02 0.03 0.04

Classiﬁcation
0.99 0.91 0.79 0.68 0.61

CER
4.5 11.2 19.3 28.5 33.9

WER
9.5 26.7 31.9 41.0 48.1

C. Gender Classiﬁcation
Table II describes the gender classiﬁcation accuracy on VCTK and LibriSpeech using gender-masked audio. All proposed approaches can successfully mask gender when no encrypted training examples are available. However, given encrypted training data from a male and female speaker, the

signal processing samples become much easier to classify than those generated from the other approaches. This makes sense as pitch shifting would retain some underlying speaker qualities that could be readily identiﬁed by a neural classiﬁer. Also, the adversarial approach using the gender-based loss outperforms the speaker-based loss approach here, which reﬂects how the former explicitly learns to hide gender information.

TABLE II: Gender classiﬁcation accuracy on two datasets using gender-masked audio. The integer n in each column denotes the number of speakers whose encrypted audio was used to ﬁnetune the classiﬁer, where n/2 are male and n/2 are female. All our voice conversion approaches perform better than the baseline without any masking for n = 0. For higher n values, the disentanglement approach performs the best, followed by the adversarial approach with the modiﬁed gender loss.

VCTK
No Masking Signal Processing Disentanglement Adversarial (Speaker) Adversarial (Gender)
LibriSpeech
No Masking Signal Processing Disentanglement Adversarial (Speaker) Adversarial (Gender)

0
0.991 0.590 0.590 0.591 0.590
0
0.972 0.422 0.590 0.580 0.570

2
0.991 0.987 0.590 0.824 0.707
2
0.972 0.869 0.627 0.712 0.621

4
0.991 0.990 0.598 0.840 0.740
4
0.972 0.887 0.702 0.714 0.628

20
0.991 0.997 0.757 0.935 0.877
20
0.972 0.933 0.781 0.838 0.833

transforms described in Section IV are more suitable for clientside privacy than other approaches like differential privacy.

TABLE III: ASR performance on two datasets using gendermasked audio. Among our voice conversion approaches, the signal processing method performs the best. For the LibriSpeech dataset, our adversarial method with the modiﬁed gender loss performs similarly to the disentangled method in the ﬁnetuned scenario. Moreover, our disentenglement and adversarial approaches do not improve when ﬁnetuned for the VCTK experiments. This suggests that the ASR model may not be robust enough or our neural converted samples may be acting like adversarial samples during the training procedure [68].

Method
No Masking Signal Processing Disentanglement Adversarial (Speaker) Adversarial (Gender)

VCTK

CER 0-Shot Finetune

WER 0-Shot Finetune

4.5

3.4

9.5

4.8

15.0

7.7

24.7

9.8

21.1

21.1

35.0

35.0

31.1

31.1

48.5

48.5

25.0

25.0

40.1

40.1

Method
No Masking Signal Processing Disentanglement Adversarial (Speaker) Adversarial (Gender)

LibriSpeech

CER 0-Shot Finetune

WER 0-Shot Finetune

2.4

2.4

4.6

4.6

5.0

5.0

8.8

8.8

15.5

15.5

25.0

25.0

29.7

17.5

47.5

28.0

22.0

15.8

36.1

25.3

D. ASR after Gender Encryption
Table III describes the ASR results when transcribing gender-encrypted data, measured using mean character and word error rates. For each approach, we provide results on both VCTK and LibriSpeech [43, 64]. All ASR models are pretrained on LibriSpeech as aforementioned. The VCTK model is further ﬁnetuned on the data from speakers outside our test set. All ﬁnetuned ASR models are tuned on the respective converted data of 20 train speakers. The signal processing approach performs the best, potentially since Grifﬁn-Lim and output distribution priors inherent in neural network architectures introduce artifacts. The adversarial approach using the gender-based loss again outperforms the speaker-based loss approach. This reﬂects how the former can model less style information than the latter and thus can model more content. For the LibriSpeech dataset, our adversarial method with the modiﬁed gender loss performs similarly to the disentangled method in the ﬁnetuned scenario. Moreover, our disentanglement and adversarial approaches do not improve when ﬁnetuned for the VCTK experiments. This suggests that the ASR model may not be robust enough or our neural converted samples may be acting like adversarial samples during the training procedure [68]. We also note that, compared to Table I, our voice conversion approaches generally achieve lower WER for ﬁxed privacy performances. This suggests that our client-side

E. Gender Listening Tests
In addition to our automatic metrics, we perform mean opinion score (MOS) preference tests using eight human listeners. Namely, we compare the signal processing, the disentanglement, and the adversarial gender-based loss approaches for the VCTK gender encryption task. For the MOS test, we ask listeners to rate audio samples on a naturalness scale of 1 to 5. We use 40 utterances for each test, where 2 are randomly chosen from each test speaker. In other words, each listener listens to 120 unique audio clips. Table IV summarizes these results. The signal processing approach performs the best for female speakers and the worst for male speakers. This is likely due to the reference F0 being from a female speaker and the relative absence of artifacts. Also, while the disentanglement approach outperforms the adversarial one in both the classiﬁcation and ASR metrics, listeners consistently rated the latter higher. This suggests that the disentanglement approach may be standardizing the audio to waveforms that are unnatural to people but suitable for downstream ASR systems. We perceive such utterances as robotic but understandable.
F. Accent Classiﬁcation
Table V describes the accent classiﬁcation accuracy on VCTK using accent-masked audio. Given that the largest class

TABLE IV: Listening test results on VCTK gender encryption approaches. Our signal processing and adversarial approaches perform the best. While our signal processing approach performs the best for female speakers, our adversarial approach does the best for males, suggesting that the latter is more robust to different speaker attributes.

MOS
Signal Processing Disentanglement Adversarial (Gender)

M
1.8 ± 0.2 2.4 ± 0.5 2.7 ± 0.6

F
4.3 ± 0.2 2.4 ± 0.5 3.7 ± 0.3

Both
3.3 ± 0.2 2.4 ± 0.5 3.3 ± 0.4

in the test set contains 31% of the samples, we observe that both the disentanglement and adversarial approaches are able to successfully fool the accent classiﬁer. Moreover, our adversarial approach using the modiﬁed accent loss performs the best, indicating the usefulness of our modiﬁed loss function.

TABLE V: Accent classiﬁcation accuracy using accent-masked audio. Given that the largest class in the test set has 31% of the samples, our results indicate that both our disentanglement and adversarial approaches successfully fooled the accent classiﬁer. Moreover, our adversarial approach using the modiﬁed accent loss performs the best.

Classiﬁcation

0

20

Largest Class

0.31 0.31

No Masking

0.36 0.36

Disentanglement

0.29 0.29

Adversarial (Speaker) 0.25 0.25

Adversarial (Gender) 0.25 0.25

Adversarial (Accent) 0.23 0.23

G. ASR after Accent Encryption
Table VI describes the ASR results when transcribing gender-encrypted data, measured using mean character and word error rates. Our experimental setup here follows that of the gender ASR experiment. We observe trends similar to Section V-D. Additionally, ASR results here are consistently better than those in the gender encryption task. This reﬂects how transforming gender requires a larger data distribution shift than transforming accent.

TABLE VI: ASR performance using accent-masked audio. As with our other ASR experiments, we observe that our disentanglement approach outperforms our adversarial one. We also note that these ASR results are consistently better than those for the gender experiment, reﬂecting the larger data distribution shift required for transforming gender.

Speech Recognition
Disentanglement Adversarial (Speaker) Adversarial (Gender) Adversarial (Accent)

CER
17.5 26.5 19.5 23.1

WER
29.9 42.4 32.4 37.4

VI. KEY TAKEAWAYS
In this section, we outline several key takeaways from our experimental results which we hope will help practitioners working on client-side privacy for complex downstream tasks.
1. Pitch standardization approaches unfortunately are not very effective in keeping gender private, as the gender classiﬁcation accuracy on data encrypted this way is much higher than those of the neural methods. In other words, this approach yielded artifacts that were readily recognizable by the adversary gender classiﬁer, which implies poorer performance in maintaining privacy. When observing the attention map of the classiﬁer, we noticed that the classiﬁer learned to identify speciﬁc patterns that resulted from the pitch shift. Thus, the gender classiﬁcation accuracy of the neural methods was much lower than those of the signal processing methods.
2. VAEs and GANs: VAEs, through use of an encoder, are suitable to learn latent disentangled representations [69, 70] which are useful in our task of disentangling content from private attributes. GANs are also suitable for learning latent representations. While they have been used less in the disentanglement literature, adding attribute-speciﬁc loss functions can disentangle sensitive information from content well. While we found GANs to be harder to train than VAEs, GANs that converge appear to perform better.
3. Memory: The large differences in memory consumption are a consequence of the large memory costs of using neural models compared to signal processing approaches. Overall, our conclusions point out a ripe opportunity for future work to reconcile the privacy beneﬁts of neural methods with the performance and memory advantages of signal processing approaches.
VII. CONCLUSION AND FUTURE DIRECTIONS
In this work, we setup the problem of ensuring the privacy of speech data sent to downstream services that does not rely on any server-side privacy guarantees. We formalized several desirable properties regarding performance, privacy, and computation and performed a large-scale empirical study of existing approaches. We ﬁnd that while GAN-based approaches currently have the best tradeoff between gender masking, downstream performance, and memory usage, all existing approaches still fall short of ideal performance. Our initial empirical analysis opens the door towards more reliable evaluations of the tradeoffs underlying privacy-preserving approaches on the client side, a property crucial for safe real-world deployment of speech systems at scale across mobile devices. In addition to developing privacy-preserving algorithms that satisfy the various desiderata as outlined in this paper, future work should also analyze other downstream speech tasks, including speech translation and other speech recognition settings.
REFERENCES
[1] Y. Chen et al., “Fedhealth: A federated transfer learning framework for wearable healthcare,” IEEE Intelligent Systems, 2020.

[2] R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated learning: A client level perspective,” arXiv preprint arXiv:1712.07557, 2017.
[3] P. P. Liang et al., “Think locally, act globally: Federated learning with local and global representations,” arXiv preprint arXiv:2001.01523, 2020.
[4] J. Xu and F. Wang, “Federated learning for healthcare informatics,” Journal of Healthcare Informatics Research, pp. 1 – 19, 2021.
[5] I. McGraw et al., “Personalized speech recognition on mobile devices,” ICASSP, pp. 5955–5959, 2016.
[6] X. Gong et al., “Model extraction attacks and defenses on cloud-based machine learning models,” IEEE Communications Magazine, vol. 58, no. 12, pp. 83–89, 2020.
[7] N. Anggraini, A. Kurniawan, L. K. Wardhani, and N. Hakiem, “Speech recognition application for the speech impaired using the android-based google cloud speech api,” Telkomnika, vol. 16, no. 6, pp. 2733–2739, 2018.
[8] D. Leroy et al., “Federated learning for keyword spotting,” ICASSP, pp. 6341–6345, 2019.
[9] R. Singh, Proﬁling Humans from their Voice. Springer Singapore, 2019.
[10] J. Jing, A. S. Helal, and A. Elmagarmid, “Client-server computing in mobile environments,” CSUR, vol. 31, no. 2, pp. 117–157, 1999.
[11] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning: Challenges, methods, and future directions,” IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50–60, 2020.
[12] S. Caldas et al., “Leaf: A benchmark for federated settings,” Workshop on Federated Learning for Data Privacy and Conﬁdentiality, 2018.
[13] J.-J. Hwang, H.-K. Chuang, Y.-C. Hsu, and C.-H. Wu, “A business model for cloud computing based on a separate encryption and decryption service,” in 2011 International Conference on Information Science and Applications, 2011, pp. 1–7.
[14] C. Zhao et al., “Secure multi-party computation: Theory, practice and applications,” IS, vol. 476, pp. 357–372, 2019.
[15] X. Sun et al., “Private machine learning classiﬁcation based on fully homomorphic encryption,” IEEE Transactions on Emerging Topics in Computing, vol. 8, no. 2, pp. 352–364, 2018.
[16] N. Papernot et al., “Scalable private learning with PATE,” ICLR, 2018.
[17] M. Abadi et al., “Deep learning with differential privacy,” in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016, pp. 308– 318.
[18] E. De Cristofaro, “A critical overview of privacy in machine learning,” IEEE Security Privacy, vol. 19, no. 4, pp. 19–27, 2021.
[19] D. C. Wilson and G. Ateniese, ““to share or not to share” in client-side encrypted clouds,” in ICIS. Springer, 2014,

pp. 401–412. [20] S. Silessi, C. Varol, and M. Karabatak, “Identifying
gender from sms text messages,” in ICMLA, 2016, pp. 488–491. [21] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in 2017 IEEE Symposium on Security and Privacy (SP), 2017, pp. 3–18. [22] A. Caliskan Islam, J. Walsh, and R. Greenstadt, “Privacy detective: Detecting private information and collective privacy behavior in a large social network,” in Proceedings of the 13th Workshop on Privacy in the Electronic Society, 2014, pp. 35–46. [23] T. Sun et al., “Mitigating gender bias in natural language processing: Literature review,” in ACL, 2019. [24] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” NeurIPS, 2020. [25] S. C. Kak and N. Jayant, “On speech encryption using waveform scrambling,” Bell System Technical Journal, vol. 56, no. 5, pp. 781–808, 1977. [26] S. Sridharan, E. Dawson, and B. Goldburg, “Fast fourier transform based speech encryption system,” IEE Proceedings I-Communications, Speech and Vision, vol. 138, no. 3, pp. 215–223, 1991. [27] P. Smaragdis and M. Shashanka, “A framework for secure speech recognition,” TASLP, vol. 15, no. 4, pp. 1404– 1413, 2007. [28] Q. Jin, A. R. Toth, T. Schultz, and A. W. Black, “Speaker de-identiﬁcation via voice transformation,” in ASRU. IEEE, 2009, pp. 529–533. [29] N. Tomashenko et al., “Introducing the voiceprivacy initiative,” Interspeech, 2020. [30] R. Alouﬁ, H. Haddadi, and D. Boyle, “Privacy-preserving voice analysis via disentangled representations,” ACM SIGSAC, 2020. [31] S. Ahmed, A. R. Chowdhury, K. Fawaz, and P. Ramanathan, “Preech: A system for privacy-preserving speech transcription,” in USENIX. USENIX Association, Aug. 2020, pp. 2703–2720. [32] Z. Ma, Y. Liu, X. Liu, J. Ma, and F. Li, “Privacypreserving outsourced speech recognition for smart iot devices,” IEEE Internet of Things Journal, vol. 6, no. 5, pp. 8406–8420, 2019. [33] A. Nautsch et al., “Preserving privacy in speaker and speech characterisation,” CSL, vol. 58, pp. 441–480, 2019. [34] B. M. L. Srivastava et al., “Design choices for x-vector based speaker anonymization,” Interspeech, 2020. [35] W.-C. Huang, T. Hayashi, S. Watanabe, and T. Toda, “The sequence-to-sequence baseline for the voice conversion challenge 2020: Cascading asr and tts,” arXiv preprint arXiv:2010.02434, 2020. [36] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of voice conversion and its challenges: From statistical modeling to deep learning,” TASLP, vol. 29,

pp. 132–157, 2021. [37] L. Zheng et al., “When automatic voice disguise meets
automatic speaker veriﬁcation,” IEEE Transactions on Information Forensics and Security, vol. 16, pp. 824– 837, 2020. [38] P.-G. Noe´ et al., “Adversarial disentanglement of speaker representation for attribute-driven privacy preservation,” arXiv preprint arXiv:2012.04454, 2020. [39] D. Ericsson et al., “Adversarial representation learning for private speech generation,” arXiv preprint arXiv:2006.09114, 2020. [40] J. Chen et al., “Using keyword spotting and replacement for speech anonymization,” in ICME. IEEE, 2007, pp. 548–551. [41] K. Kondo and H. Sakurai, “Gender-dependent babble maskers created from multi-speaker speech for speech privacy protection,” in 2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing, 2014, pp. 251–254. [42] D. Stoidis and A. Cavallaro, “Protecting gender and identity with disentangled speech representations,” arXiv preprint arXiv:2104.11051, 2021. [43] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in ICASSP. IEEE, 2015, pp. 5206–5210. [44] Z. Ji, Z. C. Lipton, and C. Elkan, “Differential privacy and machine learning: a survey and review,” arXiv preprint arXiv:1412.7584, 2014. [45] P. Wu, S. K. Rallabandi, A. W. Black, and E. Nyberg, “Ordinal triplet loss: Investigating sleepiness detection from speech.” in Interspeech, 2019, pp. 2403–2407. [46] P. Wu, Y. Zhong, and A. W. Black, “Automatically identifying language family from acoustic examples in low resource scenarios,” arXiv preprint arXiv:2012.00876, 2020. [47] P. P. Liang, P. Wu, L. Ziyin, L.-P. Morency, and R. Salakhutdinov, “Cross-modal generalization: Learning in low resource modalities via meta-alignment,” arXiv preprint arXiv:2012.02813, 2020. [48] Y. Wang, J. Du, L.-R. Dai, and C.-H. Lee, “A gender mixture detection approach to unsupervised singlechannel speech separation based on deep neural networks,” TASLP, vol. 25, no. 7, pp. 1535–1546, 2017. [49] A. Mousa, “Voice conversion using pitch shifting algorithm by time stretching with psola and re-sampling,” Journal of electrical engineering, vol. 61, no. 1, p. 57, 2010. [50] R. H. Laskar, K. Banerjee, F. A. Talukdar, and K. S. Rao, “A pitch synchronous approach to design voice conversion system using source-ﬁlter correlation,” International Journal of Speech Technology, vol. 15, no. 3, pp. 419– 431, 2012. [51] J.-c. Chou and H.-y. Lee, “One-shot voice conversion by separating speaker and content representations with instance normalization,” in Interspeech, 2019. [52] C. P. Burgess et al., “Understanding disentangling in β-

vae,” arXiv preprint arXiv:1804.03599, 2018. [53] D. Grifﬁn and Jae Lim, “Signal estimation from modi-
ﬁed short-time fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236–243, 1984. [54] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Instance normalization: The missing ingredient for fast stylization,” arXiv preprint arXiv:1607.08022, 2016. [55] X. Huang and S. Belongie, “Arbitrary style transfer in real-time with adaptive instance normalization,” in ICCV, 2017, pp. 1510–1519. [56] I.-C. Yoo et al., “Speaker anonymization for personal information protection using voice conversion techniques,” IEEE Access, vol. 8, pp. 198 637–198 645, 2020. [57] P. Dhariwal et al., “Jukebox: A generative model for music,” arXiv preprint arXiv:2005.00341, 2020. [58] S. Rallabandi, P. Wu, and A. W. Black, “Submission from cmu for blizzard challenge 2019,” Proceedings of Blizzard Challenge, vol. 2019, 2019. [59] M. Ranzato et al., “Video (language) modeling: a baseline for generative models of natural videos,” arXiv preprint arXiv:1412.6604, 2014. [60] J.-c. Chou, C.-c. Yeh, H.-y. Lee, and L.-S. Lee, “Multitarget voice conversion without parallel data by adversarially learning disentangled audio representations,” in Interspeech, 2018. [61] K. Zhou, B. Sisman, R. Liu, and H. Li, “Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset,” in ICASSP, 2021, pp. 920– 924. [62] I. Goodfellow et al., “Generative adversarial nets,” NIPS, vol. 27, 2014. [63] A. v. d. Oord et al., “Wavenet: A generative model for raw audio,” in SSW, 2016. [64] C. Veaux, J. Yamagishi, K. MacDonald et al., “CSTR VCTK corpus: English multi-speaker corpus for cstr voice cloning toolkit,” University of Edinburgh, 2017. [65] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: A large-scale speaker identiﬁcation dataset,” in Interspeech, 2017. [66] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in ICLR, 2015. [67] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Interspeech, 2018, pp. 2207–2211. [68] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in ICLR, 2015. [69] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, “betavae: Learning basic visual concepts with a constrained variational framework,” ICLR, 2016. [70] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Scho¨lkopf, and O. Bachem, “Challenging common assumptions in the unsupervised learning of disentangled representations,” in ICML. PMLR, 2019, pp. 4114– 4124.

