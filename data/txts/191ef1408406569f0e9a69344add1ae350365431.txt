Characterizing Fairness Over the Set of Good Models Under Selective Labels

arXiv:2101.00352v3 [cs.LG] 1 May 2021

Amanda Coston∗

Ashesh Rambachan†

Alexandra Chouldechova‡

May 4, 2021
Abstract
Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the “Rashomon Effect.” These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or “the set of good models.” Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) replace an existing model with one that has better fairness properties; or 2) audit for predictive bias. We illustrate these uses cases on a real-world credit-scoring task and a recidivism prediction task.
1 Introduction
Algorithmic risk assessments are used to inform decisions in high-stakes settings such as health care, child welfare, criminal justice, consumer lending and hiring [1, 2, 3, 4, 5]. Unfettered use of such algorithms in these settings risks disproportionate harm to marginalized or protected groups [6, 7, 8]. As a result, there is widespread interest in measuring and limiting predictive disparities across groups.
The vast literature on algorithmic fairness offers numerous methods for learning anew the best performing model among those that satisfy a chosen notion of predictive fairness (e.g. [9], [10], [11]). However, for real-world settings where a risk assessment is already in use, practitioners and auditors may instead want to assess disparities with respect to the current model, which we term the benchmark model. For example, the benchmark model for a bank may be an existing credit score used to approve loans. The relevant question for practitioners is: Can we improve upon the benchmark model in terms of predictive fairness with minimal change in overall accuracy?
We explore this question through the lens of the “Rashomon Effect,” a common empirical phenomenon whereby multiple models perform similarly overall but differ markedly in their predictions for individual cases [12]. These models may perform differently over various groups, and therefore have different predictive fairness properties [13]. We propose an algorithm, Fairness in the Rashomon Set (FaiRS), to probe predictive fairness properties over the set of
∗Carnegie Mellon University, Heinz College and Machine Learning Department: acoston@andrew.cmu.edu †Harvard University, Department of Economics: asheshr@g.harvard.edu ‡Carnegie Mellon University, Heinz College: achoulde@andrew.cmu.edu

models that perform similarly to a chosen benchmark model. We refer to this set as the set of good models [14]. FaiRS is designed to efﬁciently answer the following questions: What are the range of predictive disparities that could be generated over the set of good models? What is the disparity minimizing model within the set of good models?
A key empirical challenge in domains such as credit lending is that outcomes are not observed for all cases [15, 3]. This selective labels problem is particularly vexing in the context of assessing predictive fairness. Our framework addresses the challenges of selectively labelled data in contexts where the selection decision and outcome are unconfounded given the observed data features.
Our methods are useful for legal audits of disparate impact. In various domains, decisions that generate disparate impact must be justiﬁed by “business necessity" [16, 17, 6]. For instance, ﬁnancial regulators investigate whether credit lenders could have offered more loans to minority applicants without affecting default rates [18]. Our methods provide one possible formalization of the business necessity criteria. An auditor can use FaiRS to assess whether there exists an alternative model that reduces predictive disparities without compromising performance relative to the benchmark model. If possible, then it is difﬁcult to justify the benchmark model on the grounds of business necessity.
Our methods can also be a useful tool for decision makers who want to improve upon an existing model. A decision maker may use FaiRS to search for a prediction function that reduces predictive disparities without compromising performance relative to the benchmark model. We emphasize that the effective usage of our methods requires careful thought about the broader social context surrounding the setting of interest [19, 20].
Contributions: We (1) develop an algorithmic framework, Fairness in the Rashomon Set (FaiRS), to investigate predictive disparities over the set of good models; (2) provide theoretical guarantees on the generalization error and predictive disparities of FaiRS [§ 4]; (3) propose a variant of FaiRS that addresses the selective labels problem and achieves the same guarantees under oracle access to the outcome regression function [§ 5]; (4) use FaiRS on a selectively labelled credit-scoring dataset to build a model with lower predictive disparities than the benchmark model [§ 6]; and (5) use FaiRS to audit the COMPAS risk assessment, ﬁnding that it generates larger predictive disparities between black and white defendants than any model in the set of good models [§ 7].
2 Background and Related Work
2.1 Rashomon Effect
In a seminal paper on statistical modeling, [12] observed that often a multiplicity of good models achieve similar accuracy by relying on different features, which he termed the “Rashomon effect.” These models may differ along key dimensions, and recent work considers the implications of the Rashomon effect for model simplicity, interpretability, and explainability [21, 22, 23, 14, 24]. Focusing on algorithmic fairness, we develop techniques to investigate the range of predictive disparities that may be generated over the set of good models.
2.2 Fair Classiﬁcation and Fair Regression
An inﬂuential literature on fair classiﬁcation and fair regression constructs prediction functions that minimize loss subject to a predictive fairness constraint chosen by the decision maker [25, 9, 26, 27, 28, 10, 11, 29]. In contrast, we construct prediction functions that minimize a chosen measure of predictive disparities subject to a constraint on overall performance. This is useful when decision makers ﬁnd it difﬁcult to specify acceptable levels of predictive disparities, but instead know what performance loss is tolerable. It may be unclear, for instance, how a lending institution should specify acceptable differences in credit risk scores across groups, but the lending institution can easily specify an acceptable average default rate among approved loans. Similar in spirit to our work, [29] provide a method for selecting a classiﬁer that minimizes a particular notion of predictive fairness, “decision boundary covariance,” subject to a
2

performance constraint. Our method applies more generally to a large class of predictive disparities and covers both classiﬁcation and regression tasks.
While originally developed to solve fair classiﬁcation and fair regression problems, we show that the “reductions approach” used in [10, 11] can be suitably adapted to solve general optimization problems over the set of good models. This provides a general computational approach that may be useful for investigating the implications of the Rashomon Effect for other model properties.
In constructing the set of good models with comparable performance to a benchmark model, our work bears resemblance to techniques that “post-process” existing models. Post-processing techniques typically modify the predictions from an existing model to achieve a target notion of fairness [26, 30, 31]. By contrast, our methods only use the existing model to calibrate the performance constraint, but need not share any other properties with the benchmark model. While post-processing techniques require access to individual predictions from the benchmark model, our approach only requires that we know its average loss.
2.3 Selective Labels and Missing Data
In settings such as criminal justice and credit lending, the training data only contain labeled outcomes for a selectively observed sample from the full population of interest. This is a missing data problem [32]. Because the outcome label is missing based on a selection mechanism, this type of missing data is known as the selective labels problem [15, 3]. One solution treats the selectively labelled population as if it were the population of interest, and proceeds with training and evaluation on the selectively labelled population only. This is also called the “known good-bad” (KGB) approach [33, 34]. However, evaluating a model on a population different than the one on which it will be used can be highly misleading, particularly with regards to predictive fairness measures [35, 36]. Unfortunately, most fair classiﬁcation and fair regression methods do not offer modiﬁcations to address the selective labels problem, whereas our framework does.
Popular in credit lending applications, “reject inference” procedures incorporate information from the selectively unobserved cases (i.e., rejected applicants) in model construction and evaluation by imputing missing outcomes using augmentation, reweighing or extrapolation-based approaches [37, 38]. These approaches are similar to domain adaptation techniques, and indeed the selective labels problem can be cast as domain adaptation since the labelled training data is not sampled from the target distribution. Most relevant to our setting are covariate shift methods for domain adaptation. Reweighing procedures have been proposed for jointly addressing covariate shift and fairness [39, 40]. While FaiRS similarly uses iterative reweighing to solve our joint optimization problem, we explicitly use extrapolation to address covariate shift. Empirically we ﬁnd extrapolation can achieve lower disparities than reweighing.
3 Setting and Problem Formulation
The population of interest is described by the random vector (Xi, Ai, Di, Yi∗) ∼ P , where Xi ∈ X is a feature vector, Ai ∈ {0, 1} is a protected or sensitive attribute, Di ∈ D is the decision and Yi∗ ∈ Y ⊆ [0, 1] is a discrete or continuous outcome. The training data consist of n i.i.d. draws from the joint distribution P and may suffer from a selective labels problem: There exists D∗ ⊆ D such that the outcome is observed if and only if the decision satisﬁes Di ∈ D∗. Hence, the training data are {(Xi, Ai, Di, Yi)}ni=1, where Yi = Yi∗1{Di ∈ D∗}) is the observed outcome and 1{·} denotes the indicator function.
Given a speciﬁed set of prediction functions F with elements f : X → [0, 1], we search for the prediction function f ∈ F that minimizes or maximizes a measure of predictive disparities with respect to the sensitive attribute subject to a constraint on predictive performance. We measure performance using average loss, where l : Y × [0, 1] → [0, 1] is the loss function and loss(f ) := E [l(Yi∗, f (Xi))]. The loss function is assumed to be 1-Lipshitz under the l1-norm following [11]. The constraint on performance takes the form loss(f ) ≤ for some speciﬁed loss tolerance ≥ 0. The set of prediction functions satisfying this constraint is the set of good models.
3

The loss tolerance may be chosen based on an existing benchmark model f˜ such as an existing risk score, e.g., by setting = (1 + δ) loss(f˜) for some δ ∈ [0, 1]. The set of good models now describes the set of models whose performance
lies within a δ-neighborhood of the benchmark model. When deﬁned in this manner, the set of good models is also
called the “Rashomon set” [21, 22, 14, 24].

3.1 Measures of Predictive Disparities

We consider measures of predictive disparity of the form

disparity(f ) := β0E [f (Xi)|Ei,0] + β1E [f (Xi)|Ei,1] ,

(1)

where Ei,a is a group-speciﬁc conditioning event that depends on (Ai, Yi∗) and βa ∈ R for a ∈ {0, 1} are chosen parameters. Note that we measure predictive disparities over the full population (i.e., not conditional on Di).

For different choices of the conditioning events Ei,0, Ei,1 and parameters β0, β1, our predictive disparity measure summarizes violations of common deﬁnitions of predictive fairness.

Deﬁnition 1. Statistical parity (SP) requires the prediction f (Xi) to be independent of the attribute Ai [25, 9, 41]. By setting Ei,a = {Ai = a} for a ∈ {0, 1} and β0 = −1, β1 = 1, disparity(f ) measures the difference in average predictions across values of the sensitive attribute.

Deﬁnition 2. Suppose Y = {0, 1}. Balance for the positive class (BFPC) and balance for the negative class (BFNC)
requires the prediction f (Xi) to be independent of the attribute Ai conditional on Yi∗ = 1 and Yi∗ = 0 respectively (e.g., Chapter 2 of [42]). Deﬁning Ei,a = {Yi∗ = 1, Ai = a} for a ∈ {0, 1} and β0 = −1, β1 = 1, disparity(f ) describes the difference in average predictions across values of the sensitive attribute given Yi∗ = 1. If instead Ei,a = {Yi∗ = 0, Ai = a} for a ∈ {0, 1}, then disparity(f ) equals the difference in average predictions across values of the sensitive attribute given Yi∗ = 0.

Our focus on differences in average predictions across groups is a common relaxation of parity-based predictive fairness deﬁnitions [43, 44].
Our predictive disparity measure can also be used for fairness promoting interventions, which aim to increase opportunities for a particular group. For instance, the decision maker may wish to search for the prediction function among the set of good models that minimizes the average predicted risk score f (Xi) for a historically disadvantaged group.
Deﬁnition 3. Deﬁning Ei,1 = {Ai = 1} and β0 = 0, β1 = 1, disparity(f ) measures the average risk score for the group with Ai = 1. This is an afﬁrmative action-based fairness promoting intervention. Further assuming Y = {0, 1} and deﬁning Ei,1 = {Yi∗ = 1, Ai = 1}, disparity(f ) measures the average risk score for the group with both Yi∗ = 1, Ai = 1. This is a qualiﬁed afﬁrmative action-based fairness promoting intervention.

Our approach can accommodate other notions of predictive disparities. For instance, in the Supplement, we show how to achieve bounded group loss, which requires that the average loss conditional on each value of the sensitive attribute reach some threshold [11].

3.2 Characterizing Predictive Disparities over the Set of Good Models

We develop the algorithmic framework, Fairness in the Rashomon Set (FaiRS), to solve two related problems over the set of good models. First, we characterize the range of predictive disparities by minimizing or maximizing the predictive disparity measure over the set of good models. We focus on the minimization problem

min disparity(f ) s.t. loss(f ) ≤ .

(2)

f ∈F

4

Second, we search for the prediction function that minimizes the absolute predictive disparity over the set of good

models

min |disparity(f )| s.t. loss(f ) ≤ .

(3)

f ∈F

For auditors, (2) traces out the range of predictive disparities that could be generated in a given setting, thereby identifying where the benchmark model lies on this frontier. This is crucially related to the legal notion of “business necessity” in assessing disparate impact – the regulator may audit whether there exist alternative prediction functions that achieve similar performance yet generate different predictive disparities [16, 17, 6]. For decision makers, (3) searches for prediction functions that reduce absolute predictive disparities without compromising predictive performance.

4 A Reductions Approach to Optimizing over the Set of Good Models
We characterize the range of predictive disparities (2) and ﬁnd the absolute predictive disparity minimizing model (3) over the set of good models using techniques inspired by the reductions approach in [10, 11]. Although originally developed to solve fair classiﬁcation and fair regression problems in the case without selective labels, we show that the reductions approach can be appropriately modiﬁed to solve general optimization problems over the set of good models in the presence of selective labels. For exposition, we ﬁrst focus on the case without selective labels, where D∗ = D and the outcome Yi∗ is observed for all observations. We solve (2) in the main text and (3) in § A.3 of the Supplement. We cover selective labels in § 5.

4.1 Computing the Range of Predictive Disparities

We consider randomized prediction functions that select f ∈ F according to some distribution Q ∈ ∆(F) where ∆

denotes the probability simplex. Let loss(Q) := f∈F Q(f ) loss(f ) and disparity(Q) := f∈F Q(f ) disparity(f ).

We solve

min disparity(Q) s.t. loss(Q) ≤ .

(4)

Q∈∆(F )

While it may be possible to solve this problem directly for certain parametric function classes, we develop an approach that can be applied to any generic function class.4 A key object for doing so will be classiﬁers obtained by thresholding prediction functions. For cutoff z ∈ [0, 1], deﬁne hf (x, z) = 1{f (x) ≥ z} and let H := {hf : f ∈ F} be the set of all classiﬁers obtained by thresholding prediction functions f ∈ F. We ﬁrst reduce the optimization problem (4) to a constrained classiﬁcation problem through a discretization argument, and then solve the resulting constrained classiﬁcation problem through a further reduction to ﬁnding the saddle point of a min-max problem.

Following the notation in [11], we deﬁne a discretization grid for [0, 1] of size N with α := 1/N and Zα := {jα : j =

1, . . . , N }. Let Y˜α be an α2 -cover of Y. The piecewise approximation to the loss function is lα(y, u) := l(y, [u]α + α2 ),

where

y

is

the

smallest

y˜

∈

Y˜α

such

that

|y

−

y˜|

≤

α 2

and

[u]α

rounds

u

down

to

the

nearest

integer

multiple

of

α.

For

a ﬁne enough discretization grid, lossα(f ) := E [lα(Yi∗, f (Xi))] approximates loss(f ).

Deﬁne c(y, z) := N × l(y, z + α2 ) − l(y, z − α2 ) and Zα to be the random variable that uniformly samples zα ∈ Zα

and is independent of the data (Xi, Ai, Yi∗). For hf ∈ H, deﬁne the cost-sensitive average loss function as cost(hf ) :=

E

[c(Y

∗ i

,

Zα

)hf

(Xi

,

Zα

)].

Lemma 1 in [11] shows cost(hf ) + c0

=

lossα(f ) for any f

∈

F,

where c0

≥

0 is a

constant that does not depend on f . Since lossα(f ) approximates loss(f ), cost(hf ) also approximates loss(f ). For

Q ∈ ∆(F ), deﬁne Qh ∈ ∆(H) to be the induced distribution over threshold classiﬁers hf . By the same argument,

cost(Qh) + c0 = lossα(Q), where cost(Qh) := hf ∈H Qh(h) cost(hf ) and lossα(Q) is deﬁned analogously.

We next relate the predictive disparity measure deﬁned on prediction functions to a predictive disparity measure deﬁned on threshold classiﬁers. Deﬁne disparity(hf ) := β0E [hf (Xi, Zα) | Ei,0] + β1E [hf (Xi, Zα) | Ei,1] .

4Our error analysis only covers function classes whose Rademacher complexity can be bounded as in Assumption 1.

5

Lemma 1. Given any distribution over (Xi, Ai, Yi∗) and f ∈ F , |disparity(hf ) − disparity(f )| ≤ (|β0| + |β1|) α. Lemma 1 combined with Jensen’s Inequality imply | disparity(Qh) − disparity(Q)| ≤ (|β0| + |β1|) α. Based on these results, we approximate (4) with its analogue over threshold classiﬁers

min disparity(Qh) s.t. cost(Qh) ≤ − c0.

(5)

Qh ∈∆(H)

We solve the sample analogue in which we minimize disparity(Qh) subject to cost(Qh) ≤ ˆ, where ˆ := − cˆ0 plus additional slack, and cˆ0, disparity(Qh), cost(Qh) are the associated sample analogues. We form the Lagrangian L(Qh, λ) := disparity(Qh)+λ(cost(Qh)−ˆ) with primal variable Qh ∈ ∆(H) and dual variable λ ∈ R+. Solving the sample analogue is equivalent to ﬁnding the saddle point of the min-max problem minQh∈∆(H) max0≤λ≤Bλ L(Qh, λ), where Bλ ≥ 0 bounds the Lagrange multiplier. We search for the saddle point by adapting the exponentiated gradient algorithm used in [10, 11]. The algorithm delivers a ν-approximate saddle point of the Lagrangian, denoted (Qˆh, λˆ).
Since it is standard, we provide the details of and the pseudocode for the exponentiated gradient algorithm in § A.1 of
the Supplement.

4.2 Error Analysis

The suboptimality of the returned solution Qˆh can be controlled under conditions on the complexity of the model class F and how various parameters are set.

Assumption 1. Let Rn(H) be the Radermacher complexity of H. There exists constants C, C , C > 0 and φ ≤ 1/2 such that Rn(H) ≤ Cn−φ and ˆ = − cˆ0 + C n−φ − C n−1/2.

Theorem 1. Suppose Assumption 1 holds for C ≥ 2C + 2 + 2 ln(8N/δ) and C ≥ the number of samples satisfying the events Ei,0, Ei,1 respectively.

− log2(δ/8) . Let n0, n1 denote

Then, the exponentiated gradient algorithm with ν ∝ n−φ, Bλ ∝ nφ and N ∝ nφ terminates in O(n4φ) iterations and
returns Qˆh, which when viewed as a distribution over F, satisﬁes with probability at least 1 − δ one of the following: 1) Qˆh = null, loss(Qˆh) ≤ + O˜(n−φ) and disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−0 φ) + O˜(n−1 φ) for any Q˜ that is feasible in (4); or 2) Qˆh = null and (4) is infeasible.5

Theorem 1 shows that the returned solution Qˆh is approximately feasible and achieves the lowest possible predictive disparity up to some error. Infeasibility is a concern if no prediction function f ∈ F satisﬁes the average loss constraint. Assumption 1 is satisﬁed for instance under LASSO and ridge regression. If Assumption 1 does not hold, FaiRS delivers good solutions to the sample analogue of Eq. 5 (see Supplement § C.1.2).

A practical challenge is that the solution returned by the exponentiated gradient algorithm Qˆh is a stochastic prediction function with possibly large support. Therefore it may be difﬁcult to describe, time-intensive to evaluate, and memoryintensive to store. Results from [45] show that the support of the returned stochastic prediction function may be shrunk while maintaining the same guarantees on its performance by solving a simple linear program. The linear programming reduction reduces the stochastic prediction function to have at most two support points and we use this linear programming reduction in our empirical work (see § A.2 of the Supplement for details).

5 Optimizing Over the Set of Good Models Under Selective Labels
We now modify the reductions approach to the empirically relevant case in which the training data suffer from the selective labels problem, whereby the outcome Yi∗ is observed only if Di ∈ D∗ with D∗ ⊂ D. The main challenge concerns evaluating model properties over the target population when we only observe labels for a selective (i.e., biased) sample. We propose a solution that uses outcome modeling, also known as extrapolation, to estimate these properties.
5The notation O˜(·) suppresses polynomial dependence on ln(n) and ln(1/δ)

6

To motivate this approach, we observe that average loss and measures of predictive disparity (1) that condition on Yi∗ are not identiﬁed under selective labels without further assumptions. We introduce the following assumption on the nature of the selective labels problem for the binary decision setting with D = {0, 1} and D∗ = {1}. Assumption 2. The joint distribution (Xi, Ai, Di, Yi∗) ∼ P satisﬁes 1) selection on observables: Di ⊥⊥ Yi∗ | Xi, and 2) positivity: P (Di = 1 | Xi = x) > 1 with probability one.
This assumption is common in causal inference and selection bias settings (e.g., Chapter 12 of [46] and [47])6 and in covariate shift learning [48]. Under Assumption 2, the regression function µ(x) := E[Yi∗ | Xi = x] is identiﬁed as E[Yi | Xi, Di = 1], and may be estimated by regressing the observed outcome Yi on the features Xi among observations with Di = 1, yielding the outcome model µˆ(x).
We can use the outcome model to estimate loss on the full population. One approach, Reject inference by extrapolation (RIE), uses µˆ(x) as pseudo-outcomes for the unknown observations [49]. We consider a second approach, Interpolation & extrapolation (IE), which uses µˆ(x) as pseudo-outcomes for all applicants, replacing the {0, 1} labels for known cases with smoothed estimates of their underlying risks. Letting n0, n1 be the number of observations in the training data with Di = 0, Di = 1 respectively, Algorithms 1-2 summarize the RIE and IE methods. If the outcome model could perfectly recover µ(x), then the IE approach recovers an oracle setting for which the FaiRS error analysis continues to hold (Theorem 2 below).
Algorithm 1: Reject inference by extrapolation (RIE) for the selective labels setting Input: {(Xi, Yi, Di = 1, Ai)}ni=11, {(Xi, Di = 0, Ai)}ni=01 Estimate µˆ(x) by regressing Yi ∼ Xi | Di = 1. Yˆ (Xi) ← (1 − Di)µˆ(Xi) + DiYi Output: {(Xi, Yˆi(Xi), Di, Ai)}ni=11, {(Xi, Yˆi(Xi), Di, Ai)}ni=01
Algorithm 2: Interpolation and extrapolation (IE) method for the selective labels setting Input: {(Xi, Yi, Di = 1, Ai)}ni=11, {(Xi, Di = 0, Ai)}ni=01 Estimate µˆ(x) by regressing Yi ∼ Xi | Di = 1. Yˆ (Xi) ← µˆ(Xi) Output: {(Xi, Yˆi(Xi), Di, Ai)}ni=11, {(Xi, Yˆi(Xi), Di, Ai)}ni=01
Estimating predictive disparity measures on the full population requires a more general deﬁnition of predictive disparity than previously given in Eq. 1. Deﬁne the modiﬁed predictive disparity measure over threshold classiﬁers as
disparity(hf ) =β0 E [g(Xi, Yi)hf (Xi, Zα) | Ei,0] + E[g(Xi, Yi) | Ei,0] (6)
β1 E [g(Xi, Yi)hf (Xi, Zα)|Ei,1] , E[g(Xi, Yi) | Ei,1]
where the nuisance function g(Xi, Yi) is constructed to identify the measure of interest.7 To illustrate, the qualiﬁed afﬁrmative action fairness-promoting intervention (Def. 3) is identiﬁed as E[f (Xi)|Yi∗ = 1, Ai = 1] = E[fE(X[µi()Xµi()X|Ai)i|=A1i]=1] under Assumption 2 (See proof of Lemma 8 in the Supplement). This may be estimated by plugging in the outcome model estimate µˆ(x). Therefore, Eq. 6 speciﬁes the qualiﬁed afﬁrmative action fairness-promoting intervention by setting β0 = 0, β1 = 1, Ei,1 = 1 {Ai = 1}, and g(Xi, Yi) = µˆ(Xi). This more general deﬁnition (Eq. 6) is only required for predictive disparity measures that condition on events E depending on both Y ∗ and
6Casting this into potential outcomes notation where Yid is the counterfactual outcome if decision d were assigned, we deﬁne Yi0 = 0 and Yi1 = Yi∗ (e.g., a rejected loan application cannot default). The observed outcome Yi then equals Yi1Di.
7Note that we state this general form of g to allow g to use Yi for e.g. doubly-robust style estimates.
7

A; It is straightforward to compute disparities based on events E that only depend on A over the full population.

To compute disparities based on events E that also depend on Y ∗, we ﬁnd the saddle point of the following La-

grangian: L(hf , λ) = Eˆ EZα cλ(µˆi, Ai, Zα)hf (Xi, Zα) − λˆ, where we now use case weights cλ(µˆi, Ai, Zα) :=

β0 pˆ

g(Xi,

Yi)1

{Ei,0}

+

β1 pˆ

g(Xi,

Yi)1

{Ei,1}

+

λc(µˆi,

Zα)

and

pˆ

=

Eˆ [g(Xi ,

Yi)].

Finally,

as

before,

we

ﬁnd

the

saddle

point using the exponentiated gradient algorithm.

5.1 Error Analysis under Selective Labels
Deﬁne lossµ(f ) := E[l(µ(Xi), f (Xi))] for f ∈ F with lossµ(Q) deﬁned analogously for Q ∈ ∆(F ). The error analysis of the exponentiated gradient algorithm continues to hold in the presence of selective labels under oracle access to the true outcome regression function µ.
Theorem 2 (Selective Labels). Suppose Assumption 2 holds and the exponentiated gradient algorithm is given as input the modiﬁed training data {(Xi, Ai, µ(Xi)}ni=1. Under the same conditions as Theorem 1, the exponentiated gradient algorithm terminates in O(n4φ) iterations and returns Qˆh, which when viewed as a distribution over F, satisﬁes with probability at least 1 − δ either one of the following: 1) Qˆh = null, lossµ(Qˆh) ≤ + O˜(n−φ) and disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−0 φ) + O˜(n−1 φ) for any Q˜ that is feasible in (4); or 2) Qˆh = null and (4) is infeasible.
In practice, estimation error in µˆ will affect the bounds in Theorem 2. The empirical analysis in the next section ﬁnds that our method nonetheless performs well when using µˆ.

6 Application: Consumer Lending
Suppose a ﬁnancial institution wishes to replace an existing credit scoring model with one that has better fairness properties and comparable performance, if such a model exists. The following empirical analysis demonstrates how to use FaiRS for this task. We use FaiRS to ﬁnd the absolute predictive disparity-minimizing model over the set of good models on a real world consumer lending dataset with selectively labeled outcomes.
We use data from Commonwealth Bank of Australia, a large ﬁnancial institution in Australia (henceforth, "CommBank"), on a sample of 7,414 personal loan applications submitted from July 2017 to July 2019 by customers that did not have a prior ﬁnancial relationship with CommBank. A personal loan is a credit product that is paid back with monthly installments and used for a variety of purposes such as purchasing a used car or reﬁnancing existing debt. In our sample, the median personal loan size is AU$10,000 and the median interest rate is 13.9% per annum. For each loan application, we observe application-level information such as the applicant’s credit score and reported income, whether the application was approved by CommBank, the offered terms of the loan, and whether the applicant defaulted on the loan. There is a selective labels problem as we only observe whether an applicant defaulted on the loan within 5 months (Yi) if the application was funded.8 In our sample, 44.9% of applications were funded and 2.0% of funded loans defaulted within 5 months.
Motivated by a decision maker that wishes to reduce credit access disparities across geographic regions, we focus on the task of predicting the likelihood of default Yi∗ = 1 based on information in the loan application Xi while limiting predictive disparities across SA4 geographic regions within Australia. SA4 regions are statistical geographic areas deﬁned by the Australian Bureau of Statistics (ABS) and are analogous to counties in the United States. An SA4 region is classiﬁed as socioeconomically disadvantaged (Ai = 1) if it falls in the top quartile of SA4 regions based on the ABS’ Index of Relative Socioeconomic Disadvantage (IRSD), which is an index that aggregates census data related
8An application is funded when it is both approved by CommBank and the offered terms were accepted by the applicant
8

to socioeconomic disadvantage.9 Applicants from disadvantaged SA4 regions are under-represented among funded applications, comprising 21.7% of all loan applications, but only 19.7% of all funded loan applications.
Our experiment investigates the performance of FaiRS under our two proposed extrapolation-based solutions to selective labels, RIE and IE (See Algorithms 1), as well as the Known-Good Bad (KGB) approach that uses only the selectively labelled population. Because we do not observe default outcomes for all applications, we conduct a semi-synthetic simulation experiment by generating synthetic funding decisions and default outcomes. On a 20% sample of applicants, we learn π(x) := Pˆ(Di = 1|Xi = x) and µ(x) := Pˆ(Yi = 1|Xi = x, Di = 1) using random forests. We generate synthetic funding decisions Di according to Di | Xi ∼ Bernoulli(π(Xi)) and synthetic default outcomes Yi∗ according to Yi∗ | Xi ∼ Bernoulli(µ(Xi)). We train all models as if we only knew the synthetic outcome for the synthetically funded applications. We estimate µˆ(x) := Pˆ(Y˜i = 1|Xi = x, D˜i = 1) using random forests and use µˆ(Xi) to generate the pseudo-outcomes Yˆ (Xi) for RIE and IE as described in Algorithms 1 and 2. As benchmark models, we use the loss-minimizing linear models learned using KGB, RIE, and IE approaches, whose respective training losses are used to select the corresponding loss tolerances .
We compare against the fair reductions approach to classiﬁcation (fairlearn) and the Target-Fair Covariate Shift (TFCS) method. TFCS iteratively reweighs the training data via gradient descent on a loss term comprised of the covariate shift-reweighed classiﬁcation loss and a fairness loss [39]. Fairlearn searches for the loss-minimizing model subject to a fairness parity constraint [10]. The fairlearn model is effectively a KGB model since the fairlearn package does not offer modiﬁcations for selective labels.10 We use logistic regression as the base model for both fairlearn and TFCS. Results are reported on all applicants in a held out test set, and performance metrics are constructed with respect to the synthetic outcome Y˜i∗.
Figure 1 shows the AUC (y-axis) against disparity (x-axis) for the KGB, RIE, IE benchmarks and their FaiRS variants as well as the TFCS models and fairlearn models. Colors denote the adjustment strategy for selective labels, and the shape speciﬁes the optimization method. The ﬁrst row evaluates the models on all applicants in the test set (i.e., the target population). On the target population, FaiRS with reject extrapolation (RIE and IE) reduces disparities while achieving performance comparable to the benchmarks and to the reweighing approach (TFCS). It also achieves lower disparities than TFCS, likely because TFCS optimizes a non-convex objective function and may therefore converge to a local minimum. Reject extrapolation achieves better AUC than all KGB models, and only one KGB model (fairlearn) achieves a lower disparity. The second row evaluates the models on only the funded applicants. Evaluation on the funded cases underestimates disparities across the methods and overestimates AUC for the TFCS and KGB models. This underscores the importance of accounting for the selective labels problem in both model construction and evaluation.
7 Application: Recidivism Risk Prediction
We next explore the range of disparities over the set of good models in a recidivism risk prediction task applied to ProPublica’s COMPAS recidivism data [51]. Our goal is to illustrate how an auditor may use FaiRS to characterize the range of predictive disparities over the set of good models, and examine whether the COMPAS risk assessment generates larger disparities than other competing good models. Such an analysis is a crucial step to assessing legal claims of disparate impact.
COMPAS is a proprietary risk assessment developed by Northpointe (now Equivant) using up to 137 features [52]. As this data is not publicly available, our audit makes use of ProPublica’s COMPAS dataset which contains demographic information and prior criminal history for criminal defendants in Broward County, Florida. Lacking access to the
9Complete details on the IRSD may be found in [50] and additional details on the deﬁnition of socioeconomic disadvantage are given in § D.1 of the Supplement.
10To accommodate reject inference, a method must support real-valued outcomes. The fairlearn package does not, but the related fair regressions method does [11]. This is sufﬁcient for statistical parity (Def. 1), but other parities such as BFPC and BFNC (Def. 2) require further modiﬁcations as discussed in § 5
9

0.70

All applicants

0.69 0.68 qq q q

0.67

q

q

AUC

0.66 0.70

Funded applicants

0.69 qqq q qq
0.68

0.67

0.66

0.00

0.02

0.04

Disparity: E[f(X)|A = 1] − E[f(X)|A = 0]

q Known Good−Bad (KGB) q Reject extrapolation (RIE) q Inter & extrapolation (IE) q Reweighed q benchmark Fairlearn TFCS q FaiRS (ours)

Figure 1: Area under the ROC curve (AUC) with respect to the synthetic outcome against disparity in the average risk prediction for the disadvantaged (Ai = 1) vs advantaged (Ai = 0) groups. FaiRS reduces disparities for the RIE and IE approaches while maintaining AUCs comparable to the benchmark models (ﬁrst row). Evaluation on only funded applicants (second row) overestimates the performance of TFCS and KGB models and underestimates disparities for all models. Error bars show the 95% conﬁdence intervals. See § 6 for details.

data used to train COMPAS, our set of good models may not include COMPAS itself. Nonetheless, prior work has shown that simple models using age and criminal history perform on par with COMPAS [53]. These features will therefore sufﬁce to perform our audit. A notable limitation of the ProPublica COMPAS dataset is that it does not contain information for defendants who remained incarcerated. Lacking both features and outcomes for this group, we proceed without addressing this source of selection bias. We also make no distinction between criminal defendants who had varying lengths of incarceration before release, effectively assuming a null treatment effect of incarceration on recidivism. This assumption is based on ﬁndings that a counterfactual audit of COMPAS yields equivalent conclusions [54].
We analyze the range of predictive disparities with respect to race for three common notions of fairness (Deﬁnitions 1-2) among logistic regression models on a quadratic polynomial of the defendant’s age and number of prior offenses whose training loss is near-comparable to COMPAS (loss tolerance = 1% of COMPAS training loss).11 We split the data 50%-50% into a train and test set. Table 1 summarizes the range of predictive disparities on the test set. The disparity minimizing and disparity maximizing models over the set of good of models achieve a test loss that is comparable to COMPAS (see § E.3 of the Supplement).
11We use a quadratic form following the analysis in [52].
10

Table 1: COMPAS fails an audit of the “business necessity" defense for disparate impact by race. The set of good models (performing within 1% of COMPAS’s training loss) includes models that achieve signiﬁcantly lower disparities than COMPAS. The ﬁrst panel (SP) displays the disparity in average predictions for black versus white defendants (Def. 1). The second panel (BFPC) analyzes the disparity in average predictions for black versus white defendants in the positive class, and the third panel examines the disparity in average predictions for black versus white defendants in the negative class (Def. 2). Standard errors are reported in parentheses. See § 7 for details.

SP BFPC BFNC

MIN. DISP.
−0.060 (0.004)
0.049 (0.005)
0.044 (0.005)

MAX. DISP.
0.120 (0.007)
0.125 (0.012)
0.117 (0.009)

COMPAS
0.194 (0.013)
0.156 (0.016)
0.174 (0.016)

For each predictive disparity measure, the set of good models includes models that achieve signiﬁcantly lower disparities than COMPAS. In this sense, COMPAS generates “unjustiﬁed” disparate impact across groups as there exists competing models in the set of good models that would reduce disparities without compromising performance. Notably, COMPAS’ disparities are also larger than the maximum disparity over the set of good models. For example, the difference in COMPAS’ average predictions for black relative to white defendants is strictly larger than that of any model in the set of good models (Table 1, SP). Interestingly, the minimal balance for the positive class and balance for the negative class disparities between black and white defendants over the set of good models are strictly positive (Table 1, BFPC and BFNC). For example any model whose performance lies in a neighborhood of COMPAS’ loss has a higher false positive rate for black defendants than white defendants. This suggests while we can reduce predictive disparities between black and white defendants relative to COMPAS on all measures, we may be unable to eliminate balance for the positive class and balance for the negative class disparities without harming predictive performance.
In Supplement §E.4, we conducted regression experiments on the Communities & Crime dataset, comparing FaiRS against a loss minimizing least squares regression and ﬁnding that FaiRS improves on statistical parity without compromising performance relative to the benchmark model.
8 Conclusion
We develop a framework, Fairness in the Rashomon Set (FaiRS), to characterize the range of predictive disparities and ﬁnd the absolute disparity minimizing model over the set of good models. FaiRS is generic, applying to both a large class of prediction functions and a large class of predictive disparities. FaiRS is suitable for a variety of applications including settings with selectively labelled outcomes where the selection decision and outcome are unconfounded given the observed features. FaiRS can facilitate audits for disparate impact and efﬁciently search for a more equitable model with performance comparable to a benchmark model. In many settings the set of good models is a rich class, in which models differ substantially in terms of their fairness properties. By leveraging this phenomenon, our approach can reduce predictive disparities without compromising overall performance.

11

References
[1] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 1721–1730, 2015.
[2] Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. volume 81 of Proceedings of Machine Learning Research, pages 134–148, 2018.
[3] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human decisions and machine predictions. The Quarterly Journal of Economics, 133(1):237–293, 2018.
[4] Andreas Fuster, Paul Goldsmith-Pinkham, Tarun Ramadorai, and Ansgar Walther. Predictably unequal? the effects of machine learning on credit markets. Technical report, 2020.
[5] Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, page 469–481, 2020.
[6] Solon Barocas and Andrew Selbst. Big data’s disparate impact. California Law Review, 104:671–732, 2016.
[7] Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women, Oct 2018.
[8] Neil Vigdor. Apple card investigated after gender discrimination complaints, Nov 2019.
[9] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. pages 325–333, 2013.
[10] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classiﬁcation. In Proceedings of the 35th International Conference on Machine Learning, pages 60–69, 2018.
[11] Alekh Agarwal, Miroslav Dudík, and Zhiwei Steven Wu. Fair regression: Quantitative deﬁnitions and reductionbased algorithms. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019.
[12] Leo Breiman. Statistical modeling: The two cultures. Statistical Science, 16(3):199–215, 2001.
[13] Alexandra Chouldechova and Max G’Sell. Fairer and more accurate, but for whom? Technical report, In Proceedings of the 2017 FAT/ML Workshop, 2017.
[14] Jiayun Dong and Cynthia Rudin. Variable importance clouds: A way to explore variable importance for the set of good models. Technical report, arXiv preprint arXiv:1901.03209, 2020.
[15] Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 275–284, 2017.
[16] Civil rights act, 1964. 42 U.S.C. § 2000e.
[17] Equal credit opportunity act, 1974. 15 U.S.C. § 1691.
[18] Talia Gillis. False dreams of algorithmic fairness: The case of credit pricing. 2019.
[19] Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, page 59–68, 2019.
[20] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI ’19, page 1–16, 2019.
[21] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1:206–215, 2019.
[22] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Technical report, arXiv preprint arXiv:1801.01489, 2019.
[23] Charles T. Marx, Flavio du Pin Calmon, and Berk Ustun. Predictive multiplicity in classiﬁcation. Technical report, arXiv preprint arXiv:1909.06677, 2019.
12

[24] Lesia Semenova, Cynthia Rudin, and Ronald Parr. A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. Technical report, arXiv preprint arXiv:1908.01755, 2020.
[25] Cynthia Dwork, Toniann Pitassi Moritz Hardt, Omer Reingold, and Richard Zemel. Fairness through awareness. In ITCS ’12: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214–226, 2012.
[26] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In NIPS’16: Proceedings of the 30th International Conference on Neural Information Processing Systems, page 3323–3331, 2016.
[27] Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classiﬁcation. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, pages 107–118. 2018.
[28] Michele Donini, Luca Oneto, Shai Ben-David, John R Shawe-Taylor, and Massimiliano A. Pontil. Empirical risk minimization under fairness constraints. In NIPS’18: Proceedings of the 32nd International Conference on Neural Information Processing Systems, page 2796–2806, 2018.
[29] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness constraints: A ﬂexible approach for fair classiﬁcation. Journal of Machine Learning Research, 20(75):1–42, 2019.
[30] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and calibration. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pages 5680–5689. 2017.
[31] Michael P. Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classiﬁcation. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, page 247–254, 2019.
[32] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John Wiley & Sons, 2019.
[33] Guoping Zeng and Qi Zhao. A rule of thumb for reject inference in credit scoring. Math. Finance Lett., 2014:Article–ID, 2014.
[34] Ha-Thu Nguyen et al. Reject inference in application scorecards: evidence from france. Technical report, University of Paris Nanterre, EconomiX, 2016.
[35] Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. arXiv preprint arXiv:1806.02887, 2018.
[36] Amanda Coston, Alan Mishler, Edward H. Kennedy, and Alexandra Chouldechova. Counterfactual risk assessments, evaluation and fairness. In FAT* ’20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 582–593, 2020.
[37] Zhiyong Li, Xinyi Hu, Ke Li, Fanyin Zhou, and Feng Shen. Inferring the outcomes of rejected loans: An application of semisupervised clustering. Journal of the Royal Statistical Society: Series A, 183(2):631–654, 2020.
[38] Rogelio A Mancisidor, Michael Kampffmeyer, Kjersti Aas, and Robert Jenssen. Deep generative models for reject inference in credit scoring. Knowledge-Based Systems, page 105758, 2020.
[39] Amanda Coston, Karthikeyan Natesan Ramamurthy, Dennis Wei, Kush R Varshney, Skyler Speakman, Zairah Mustahsan, and Supriyo Chakraborty. Fair transfer learning with missing protected attributes. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 91–98, 2019.
[40] Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. Fairness violations and mitigation under covariate shift. Technical report, arXiv preprint arXiv:1911.00677, 2021.
[41] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In KDD ’15: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259–268, 2015.
[42] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019. http://www.fairmlbook.org.
[43] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. pages 797–806, 2017.
[44] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. Prediction-based decisions and fairness: A catalogue of choices, assumptions, and deﬁnitions. Technical report, arXiv Working Paper, arXiv:1811.07867, 2019.
[45] Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for efﬁcient non-convex constrained optimization. In Proceedings of the 30th International Conference on Algorithmic Learning Theory, pages 300–332, 2019.
13

[46] Guido W Imbens and Donald B Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction. Cambridge University Press, Cambridge, United Kingdom, 2015.
[47] James J. Heckman. Varieties of selection bias. The American Economic Review, 80(2):313–318, 1990. [48] Jose G Moreno-Torres, Troy Raeder, Rocío Alaiz-Rodríguez, Nitesh V Chawla, and Francisco Herrera. A unifying
view on dataset shift in classiﬁcation. Pattern Recognition, 45(1):521–530, 2012. [49] Jonathan Crook and John Banasik. Does reject inference really improve the performance of application scoring
models? Journal of Banking & Finance, 28(4):857–874, 2004. [50] Australian Bureau of Statistics. Socio-economic indexes for areas (seifa) technical paper. Technical report, 2016. [51] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. there’s software used across the
country to predict future criminals. and it’s biased against blacks. ProPublica, 2016. [52] Cynthia Rudin, Caroline Wang, and Beau Coker. The age of secrecy and unfairness in recidivism prediction.
Harvard Data Science Review, 2(1), 2020. [53] Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin. Learning certiﬁably
optimal rule lists for categorical data. Journal of Machine Learning Research, 18:1–78, 2018. [54] Alan Mishler. Modeling risk and achieving algorithmic fairness using potential outcomes. In Proceedings of the
2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 555–556, 2019. [55] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
14

A Additional Theoretical Results

A.1 Implementation of the Exponentiated Gradient Algorithm in § 4.1

We now provide the details of the exponentiated gradient algorithm discussed in § 4.1 for ﬁnding the predictive disparity minimizing model within the set of good models. Algorithm 3 implements the exponentiated gradient algorithm, except for the best-response functions of the λ-player and the Qh-player. The best-response function of the λ-player is

Bestλ(Qh) := 0 if cost(Qh) − ˆ ≤ 0,

(7)

Bλ otherwise.

The best-response function of the Qh-player may be constructed through a further reduction to cost-sensitive classiﬁcation. The Lagrangian may be re-written as

L(hf , λ) = Eˆ [EZα [cλ(Y ∗i , Ai, Zα)hf (Xi, Zα)]] − λˆ,

(8)

where

cλ(Y ∗, Ai, Zα) := β0 1 {Ei,0} + β1 1 {Ei,1} + λc(Y ∗, Zα)

(9)

i

pˆ0

pˆ1

i

and pˆa := Eˆ [Ei,a] for a ∈ {0, 1}. This is solved by calling cost-sensitive classiﬁcation oracle on an augmented dataset

of

size

n×N

with

observations

{(Xi,zα , Ci,zα }i∈[n],zα∈Zα

with

Xi,zα

=

(Xi, zα)

and

Ci,zα

=

cλ(Y

∗ i

,

Ai

,

zα

)

.

In

our

empirical implementation, we use the heuristic least-squares reduction described in [11], which eases the computational

burden of the algorithm. The heuristic reduction generally performed well in our empirical work, but performance

losses depended on the dataset and the choice of predictive disparity.

Algorithm 3: Algorithm for ﬁnding the predictive disparity minimizing model

Input: Training data {(Xi, Yi, Ai)}ni=1, parameters β0, β1, events Ei,0, Ei,1, empirical loss tolerance ˆ, bound

Bλ, accuracy ν and learning rate η.

Result: ν-approximate saddle point (Qˆh, λˆ)

Set θ1 = 0 ∈ R ;

for t = 1, 2, . . . do

Set λt = Bλ 1+exepx(pθ(tθ)t) ;

ht ← Besth(λt);

Qˆ h,t

←

1 t

t s=1

hs,

L¯ ← L(Qˆh,t, Bestλ(Qˆh,t);

λˆt

←

1 t

t s=1

λs,

L ← L(Besth(λˆt), λˆt);

νt ← max L(Qˆh,t, λˆt) − L, L¯ − L(Qˆh,t, λˆt) ;

if νt ≤ ν then if cost(Qˆh,t) ≤ ˆ + |β0|+B|βλ1|+2ν return (Qˆh,t, λˆt); else return null end
end

then

Set θt+1 = θt + η cost(ht) − ˆ ;

end

A.2 Shrinking the Support of the Stochastic Risk Score
As discussed in § 4.1, a key challenge to the practical use of Algorithm 3 is it returns a stochastic prediction function Qˆh with possibly large support. The number of prediction functions in the support of Qˆh is equal to the total number of iterations taken by the respective algorithm. As a result, Qˆh may be complex to describe, time-intensive to evaluate, and memory-intensive to store.
15

The support of the returned stochastic prediction may be shrunk while maintaining the same guarantees on its performance by solving a simple linear program. To do so, we take the set of prediction functions in the support of Qˆh and solve the following linear program

T

T

min ptdisparity(ht) s.t. ptcost(ht) ≤ ˆ + 2ν,

(10)

p∈∆T

t=1

t=1

where T is the number of iterations of Algorithm 3, ∆T is the T -dimensional unit simplex and ht is the t-th prediction function in the support of Qˆh (i.e., the prediction function constructed at the t-th iteration of Algorithm 3). We then use the randomized prediction function that assigns probability pt to each prediction function in the support of Qˆh. In practice, we calibrate the constraint in (10) by choosing the smallest ν ≥ 0 such that the linear program has a feasible
solution, following the practical recommendations in [45].

Lemma 7 of [45] shows that the solution to (10) has at most 2 support points and the same performance guarantees as the original solution Qˆh.

A.3 Computing the Absolute Predictive Disparity Minimizing Model

In this section, we extend the reductions approach to compute the prediction function that minimizes the absolute predictive disparity over the set of good models (3). We solve

min |disparity(Q)| s.t. loss(Q) ≤ .

(11)

Q∈∆(F )

Through the same discretization argument, this problem may be reduced to a constrained classiﬁcation problem over the set of threshold classiﬁers

min |disparity(Qh)| s.t. cost(Qh) ≤ − c0.

(12)

Qh ∈∆(H)

To further deal with the absolute value operator in the objective function, we introduce a slack variable ξ and deﬁne the equivalent problem over both Qh ∈ ∆(H), ξ ∈ R

min ξ
ξ,Qh ∈∆(H)

(13)

s.t. disparity(Qh) − ξ ≤ 0,

− disparity(Qh) − ξ ≤ 0,

cost(Qh) ≤ − c0.

We construct solutions to the empirical analogue of (13).

Solving the empirical analogue of (13) is equivalent to ﬁnding the saddle point minQh∈∆(H),ξ∈[0,Bξ] max λ ≤Bλ L(ξ, Qh, λ) with Lagrangian L(ξ, Qh, λ) = ξ + λ+ dispˆarity(Qh) − ξ +
λ− −dispˆarity(Qh) − ξ + λcost cost(Qh) − ˆ , λ = (λ+, λ−, λcost) and Bξ is a bound on the slack variable.
Since the absolute predictive disparity is bounded by one, we deﬁne Bξ = 1 in practice. We search for the saddle point by treating it as the equilibrium of a two-player zero-sum game in which one player chooses (ξ, Qh) and the other chooses λ.

Algorithm 4 computes a ν-approximate saddle point of L(ξ, Qh, λ). The best-response of the λ-player sets the Lagrange multiplier associated with the maximally violated constraint equal to Bλ. Otherwise, she sets all Lagrange multipliers to zero if all constraints are satisﬁed. In order to analyze the best-response of the (ξ, Qh)-player, rewrite the Lagrangian as

L(ξ, Qh, λ) = (1 − λ+ − λ−)ξ

(14)

+ (λ+ − λ−)disparity(Qh) + λcost(cost(Qh) − ˆ).

For a ﬁxed value of λ, minimizing L(ξ, Qh, λ) over (ξ, Qh) jointly is equivalent to separately minimizing the ﬁrst term involving ξ and the remaining terms involving Qh. To minimize (1 − λ+ − λ−)ξ, the best-response is to set ξ = Bξ if 1 − λ+ − λ− < 0, and set ξ = 0 otherwise. Minimizing

(λ+ − λ−)disparity(Qh) + λcost(cost(Qh) − ˆ)

(15)

16

over Qh can be achieved through a reduction to cost-sensitive classiﬁcation since minimizing the previous display is

equivalent to minimizing

Eˆ [EZα [cλ(Y ∗i , Ai, Zα)hf (Xi, Zα)]] ,

(16)

where now cλ(Y ∗i , Ai, Zα) := (λ+ − λ−)

β0 pˆ

1

{Ei,0

}

+

β1 pˆ

1

{Ei,1

}

+

λcost

c(Y

∗ i

,

Zα

).

0

1

We use an analogous linear program reduction (§ A.2) to shrink the support of the solution returned by Algorithm 4.

Algorithm 4: Algorithm for ﬁnding the absolute predictive disparity minimizing model among the set of good

models

Input: Training data {(Xi, Yi, Ai)}ni=1, Parameters β0, β1, Events Ei,0, Ei,1, and empirical loss tolerance ˆ

Bounds Bλ, Bξ, accuracy ν and learning rate η

Result: ν-approximate saddle point (ξˆ, Qˆ, λˆ) Set θ1 = 0 ∈ R3 ;

for t = 1, 2, . . . do

Set λt,k = Bλ 1+ exkpe(θxtp,k(θ)t,k ) for all k = {cost, +, −};

ht ← Besth(λt), ξt ← Bestξ(λt) ;

Qˆ h,t

←

1 t

t s=1

hs,

ξˆt

←

1 t

L¯ ← L(ξˆt, Qˆt, Bestλ(ξˆt.Qˆt);

t s=1

ξt

;

λˆt

←

1 t

t s=1

λs,

L ← L(Bestξ(λt), Besth(λˆt), λˆt);

νt ← max L(ξˆt, Qˆt, λˆt) − L, L¯ − L(ξˆt, Qˆt, λˆt) ;

if νt ≤ ν then if cost(Qˆ) ≤ ˆ + BξB+λ2ν then return (ξˆt, Qˆt, λˆt);

else

return null;

end

end





disparity(ht) − ξt

Set θt+1 = θt + η −disparity(ht) − ξt;

cost(ht) − ˆ

end

A.3.1 Error Analysis
We analyze the suboptimality of the solution returned by Algorithm 4.
Theorem 3. Suppose Assumption 1 holds for C ≥ 2C + 2 + 2 ln(8N/δ) and C ≥ − log2(δ/8) .
Then, Algorithm 4 with ν ∝ n−φ, Bλ ∝ nφ, N ∝ nφ terminates in at most O(n4φ) iterations. It returns Qˆh, which when viewed as a distribution over F, satisﬁes with probability at least 1 − δ either one of the following: 1) Qˆh = null, loss(Qˆh) ≤ + O˜(n−φ) and disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−0 φ) + O˜(n−1 φ) for any Q˜ that is feasible in (11); or 2) Qˆh = null and (11) is infeasible.
We next provide an oracle result for the absolute disparity minimizing algorithm under selective labels. Theorem 4 (Selective Labels for Algorithm 4). Suppose Assumption 2 holds and Algorithm 4 is given as input the modiﬁed training data {(Xi, Ai, µ(Xi)}ni=1. Under the same conditions as Theorem 3, Algorithm 4 terminates in at most O(n4φ) iterations. It returns Qˆh, which when viewed as a distribution over F, satisﬁes with probability at least 1 − δ either one of the following: 1) Qˆh = null, lossµ(Qˆh) ≤ + O˜(n−φ) and disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−0 φ) + O˜(n−1 φ) for any Q˜ that is feasible in (11); or 2) Qˆh = null and (11) is infeasible.
17

We omit the proof of Theorem 4 since the analogous steps are given in proofs of Theorems 2-3 below.

A.4 Bounded Group Loss Disparity

Bounded group loss is a common notion of predictive fairness that examines the variation in average loss across values of the protected or sensitive attribute. It is commonly used to ensure that the prediction function achieves some minimal threshold of predictive performance across all values of the attribute [11]. We deﬁne a bounded group loss disparity to be the difference in average loss across values of the attribute, disparity(f ) = E [l(Yi∗, f (Xi)) | Ai = 1] − E [l(Yi∗, f (Xi)) | Ai = 0]. This choice of predictive disparity measure is convenient as it allows us to drastically simplify our algorithm by skipping the discretization step entirely and reducing the problem to an instance of weighted loss minimization. [11] apply the same idea in their analysis of fair regression under bounded group loss.
Take, for example, the problem of ﬁnding the range of bounded group loss disparities that are possible over the set of good models. Letting loss(f | Ai = a) := E [l(Yi∗, f (Xi) | Ai = a] and loss(Q | Ai = a) := f∈F Q(f ) loss(f | Ai = a), we solve
min loss(Q | Ai = 1) − loss(f | Ai = 0)
Q∈∆(F )
s.t. loss(Q) ≤ .

The sample version of this problem is to minimize loss(Q | Ai = 1) − loss(f | Ai = 0) subject to loss(Q) ≤ . We
solve the sample problem by ﬁnding a saddle point of the associated Lagrangian L(Q, λ) = loss(Q | Ai = 1) − loss(f |
Ai = 0) + λ(loss(Q) − ). We compute a ν-approximate saddle point by treating it as a zero-sum game between a Q-player and a λ-player. The best response of the λ-player is the same as before: if the constraint loˆss(Q) − is violated, she sets λ = Bλ, and otherwise she sets λ = 0. The best-response of the Q-player may reduced to an instance of weighted loss minimization since

loss(f |Ei,0) − loss(f |Ei,1) + λ(loss(f ) − )

= Eˆ

1

1

1{E0} − 1{E1} + λ l(Yi, f (Xi))

pˆ0

pˆ1

Therefore, deﬁning the weights Wi = pˆ10 1{Ei,0} − pˆ11 1{Ei,1} + λ, we see that minimizing L(h, λ) is equivalent to solving an instance of weighted loss minimization. Algorithm 5 formally states the procedure for ﬁnding the range
of bounded group loss disparities. We may analogously extend Algorithm 4 to ﬁnd the absolute bounded group loss
minimizing model among the set of good models.

B Proofs of Main Results

Proof of Lemma 1

Fix f ∈ F . For x ∈ X and zα ∈ Zα

hf (x, zα) = 1{f (x) ≥ zα} = 1{f (x) ≥ zα},

Therefore, and for any a ∈ {0, 1},

EZα [hf (x, Zα)] = EZα 1{f (x) ≥ Zα} = f (x),

|E [hf (X, Zα)|Ei,a] − E [f (X)|Ei,a] | = |E [EZα [hf (X, Zα)] − f (X)|Ei,a] | = |E f (X) − f (X)|Ei,a | ≤ α

where the ﬁrst equality uses iterated expectations plus the fact that Zα is independent of (X, A, Y ∗) and the ﬁnal equality follows by the deﬁnition of f (X). The claim is immediate after noticing disparity(hf ) − disparity(f ) equals
β0 (E [hf (X, Zα) − f (X)|Ei,0]) + β1 (E [hf (X, Zα) − f (X)|Ei,1]) and applying the triangle inequality.

18

Algorithm 5: Algorithm for ﬁnding the bounded group loss disparity minimizing model over the set of good

models

Input: Training data {(Xi, Yi, Ai)}ni=1, Parameters β0, β1, Events Ei,0, Ei,1, and loss tolerance ˆ

Bound Bλ, accuracy ν and learning rate η

Result: ν-approximate saddle point (Qˆh, λˆ)

Set θ1 = 0 ∈ R ;

for t = 1, 2, . . . do

Set λt = Bλ 1+exepx(pθ(tθ)t) ;

ft ← Bestf (λt);

Qˆt

←

1 t

λˆt

←

1 t

t s=1

fs,

t s=1

λs,

L¯ ← L(Qˆt, Bestλ(Qˆt); L ← L(Bestf (λˆt), λˆt);

νt ← max L(Qˆt, λˆt) − L, L¯ − L(Qˆt, λˆt) ;

if νt ≤ ν then if loss(Qˆt) ≤ ˆ + |β0|+B|βλ1|+2ν return (Qˆt, λˆt); else return null end
end

then

Set θt+1 = θt + η loss(ft) − ˆ ;

end

Proof of Theorem 1
The claim about the iteration complexity of Algorithm 3 follows immediately from Lemma 2, substituting in the stated choices of ν and B. The proof strategy for the remaining claims follows the proof of Theorems 2-3 in [11]. We consider two cases.

Case 1: There is a feasible solution Q∗ to the population problem (4) Using Lemmas 4-5, the ν-approximate saddle point Qˆh satisﬁes

disparity(Qˆh) ≤ disparity(Qh) + 2ν

(17)

cost(Qˆh) ≤ ˆ + |β0| + |β1| + 2ν

(18)

B

for any distribution Qh that is feasible in the empirical problem. This implies that Algorithm 3 returns Qˆ = null. We now show that the returned Qˆh provides an approximate solution to the discretized population problem.

First,

deﬁne

costz (h)

:=

Eˆ

[c(Y

∗ i

,

z

)h(Xi

,

z

)]

and

costz (h)

:=

E

[c(Y

∗ i

,

z

)h(Xi

,

z

)].

Since

c(Y

∗ i

,

z

)

∈

[−1, 1],

we

invoke Lemma 7 with Si = c(Y ∗i , zi), Ui = (Xi, z), G = H and ψ(s, t) = st to obtain that with probability at least

1

−

δ 4

for

all

z

∈

Zα

and

h

∈

H

costz(h) − costz(h) ≤

2 2Rn(H) + √ +
n

2 ln(8N/δ) = O˜(n−φ), n

where the last equality follows by the bound on Rn(H) in Assumption 1 and setting N ∝ nφ. Averaging over z ∈ Zα and taking a convex combination of according to Qh ∈ ∆(H) then delivers via Jensen’s Inequality that with probability at least 1 − δ/4 for all Q ∈ ∆(H)

cost(Qh) − cost(Qh) ≤ O˜(n−φ).

(19)

19

Next, deﬁne disparityz(h) := β0Eˆ [h(Xi, z)|Ei,0] + β1Eˆ [h(Xi, z)|Ei,1] and disparityz(h) := β0E [h(Xi, z)|Ei,0] + β1E [h(Xi, z)|Ei,1], where the difference can be expressed as
disparityz(h) − disparityz(h) = β0 Eˆ [h(Xi, z)|Ei,0] − E [h(Xi, z)|Ei,0] + β1 Eˆ [h(Xi, z)|Ei,1] − E [h(Xi, z)|Ei,1] .
Therefore, by the triangle inequality,

disparityz(h) − disparityz(h) ≤ |β0| Eˆ [h(Xi, z)|Ei,0] − E [h(Xi, z)|Ei,0] + |β1| Eˆ [h(Xi, z)|Ei,1] − E [h(Xi, z)|Ei,1] .

For each term on the right-hand side of the previous display, we invoke Lemma 7 applied to the data distribution

conditional

on

E0

and

E1.

We

set

S

=

1,

U

=

(Xi, z),

G

=

H

and

ψ(s, t)

=

st.

With

probability

at

least

1

−

δ 4

for

all

z ∈ Zα,

Eˆ [h(Xi, z)|Ei,0] − E [h(Xi, z)|Ei,0] ≤

2

Rn0 (H)

+

√ n0

+

2 ln(8N/δ) ,
n0

Eˆ [h(Xi, z)|Ei,1] − E [h(Xi, z)|Ei,1] ≤

2

Rn1 (H)

+

√ n1

+

2 ln(8N/δ) .
n1

Then, averaging over z ∈ Zα and taking a convex combination according to Qh ∈ ∆(H) delivers via Jensen’s Inequality that with probability at least 1 − δ/4 for all Q ∈ ∆(H)

Eˆ [Qh|Ei,0] − E [Qh|Ei,0]

≤ Rn

2 (H) + √

+

2 ln(8N/δ)

(20)

0

n0

n0

Eˆ [Qh|Ei,1] − E [Qh|Ei,1]

≤ Rn

2 (H) + √

+

2 ln(8N/δ)

(21)

1

n1

n1

By the union bound, both inequalities hold with probability at least 1 − δ/2. Finally, Hoeffding’s Inequality implies that with probability at least 1 − δ/4,

− log(δ/8) |cˆ0 − c0| ≤ 2n . (22)

From Lemma 6, we have that Algorithm 3 terminates and delivers a distribution Qˆh that compares favorably against any feasible Q in the discretized sample problem. That is, for any such Qh,

disparity(Qˆh) ≤ disparity(Qh) + O(n−φ)

(23)

cost(Qˆh) ≤ ˆ + O(n−φ)

(24)

where we used the fact that ν ∝ n−φ and B ∝ nφ by assumption. First, (19), (22), (24) imply

cost(Qˆh) ≤ ˆ + O˜(n−φ) ≤ − c0 + O˜(n−φ),

(25)

20

where we used that ˆ =

− Eˆ[l(Y ∗i , α2 )] + C n−φ − C n−1/2. by assumption. Second, the bounds in (20), (21) imply

disparity(Qˆh) ≤ disparity(Qh) + O˜(n−0 β) + O˜(n−1 φ).

(26)

We assumed that Qh was a feasible point in the discretized sample problem. Assuming that (19) holds implies that any feasible solution of the population problem is also feasible in the empirical problem due to how we have set C and C . Therefore, we have just shown in (25), (26) that Qˆh is approximately feasible and approximately optimal in the discretized population problem (5). Our last step is to relate Qˆh to the original problem over f ∈ F (2).
From Lemma 1 in [11] and (25), we observe that

(1)
lossα(Qˆh) ≤
(2)
loss(Qˆh) ≤

+ O˜(n−φ), + O˜(n−φ),

where (1) used Lemma 1 in [11] and we now view Qˆh as a distribution of risk scores f ∈ F, (2) used that loss(Q) ≤ lossα(Q) + α. Next, from Lemma 1 and (26), we observe that

disparity(Qˆh) ≤ disparity(Q˜) + (|β0| + |β1|) α + O˜(n−0 φ) + O˜(n−1 φ).
where Qˆh is viewed as a distribution over risk scores f ∈ F and Q˜ is now any distribution over risk scores f ∈ F that is feasible in the fairness frontier problem. This proves the result for Case I.

Case II: There is no feasible solution to the population problem (4) This follows the proof of Case II in Theorem
3 of [11]. If the algorithm returns a ν-approximate saddle point Qˆh, then the theorem holds vacuously since there is no feasible Q˜. Similarly, if the algorithm returns null, then the theorem also holds.

Proof of Theorem 2

Under oracle access to µ(x), the iteration complexity and bound on cost hold immediately from Theorem 1. The bound on disparity holds immediately for choices Ei,0, Ei,1 that depend on only A. For choices of Ei,0, Ei,1 that depends on Yi, such as the qualiﬁed afﬁrmative action fairness-enhancing intervention, we rely on Lemma 8. We ﬁrst observe that under oracle access to µ(x), we can identify any disparity as

β1E[f (X)g(µ(X)) | A = 1] − β0E[f (X)g(µ(X)) | A = 0] , (27)

E[g(µ(X)) | A = 1]

E[g(µ(X)) | A = 0]

where g(x) = x for the balance for the positive class and qualiﬁed afﬁrmative action criteria; g(x) = (1 − x) for balance for the negative class; and g(x) = 1 for the statistical parity and the afﬁrmative action criteria (see proof of Lemma 8 below proof for an example). We deﬁne the shorthand

ω1 := E[f (X)g(µ(X)) | A = 1] ω¯1 := E[g(µ(X)) | A = 1]
ω0 := E[f (X)g(µ(X)) | A = 0] ω¯0 := E[g(µ(X)) | A = 0]
and we use ωˆ1, ωˆ¯1, ωˆ0, and ωˆ¯0 to denote their empirical estimates. Lemma 8 gives the following bound on the empirical estimate of the disparity:

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

P

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≥

n ≤ 4 exp −
2

2

ω¯∧

−

4Rn(G)

−

2 √

8β

n

+ 2 exp

+ 2 exp −nω¯∧2 4

where ω∨ = max(ω1, ω0), ω¯∧ = min(ω¯1, ω¯0) and β = max(|β1| , |β0|).

−n 2ω¯∧4 64β2ω∨2

21

We now proceed to relax and simplify the bound. For ≤ 4 βω¯ω∧∨ , we have

−n 2ω¯∧4

−nω¯∧2

2 exp 64β2ω2 ≥ 2 exp 4

∨

Case 1: We ﬁrst consider the likely case that ω¯∧ ≥ ω∨. Then we have

−n 2ω¯∧4

−n 2ω¯∧2

2 exp 64β2ω2 ≤ 2 exp 64β2

∨

1a) If

ω¯∧

≥

4Rn(G) +

2 √

(28)

8β

n

then

2

exp −n 2ω¯∧2 ≤ exp − n

64β2

2

ω¯∧

−

4Rn(G)

−

2 √

8β

n

Then we have

P βω1ˆ¯ωˆ1 1 − βω0ˆ¯ωˆ0 0 − βω1¯ω1 1 − βω0¯ω0 0 ≥

≤ 8 exp − n2

2

ω¯∧ 8β

− 4Rn(G) −

√2 n

(29) (30)

Inverting this bound yields the following: with probability at least 1 − δ,

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≤

8β

2

4Rn(G) + √ +

ω¯∧

n

2

8

log

nδ

1b)

ω¯∧

<

4Rn(G) +

2 √

(31)

8β

n

implies that

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≤

8β

2

4Rn(G) + √ .

ω¯∧

n

Case 2: We now consider the unlikely but plausible case that ω¯∧ < ω∨. Then we have

n exp −
2

2

ω¯∧

−

4Rn(G)

−

2 √

≤

8β

n

n exp −
2

2

ω∨

−

4Rn(G)

−

2 √

8β

n

and

−n 2ω¯∧4

−n 2ω∨2

exp 64β2ω2 ≤ exp 64β2

∨

22

We proceed with the same steps as in Case 1 to conclude that with probability at least 1 − δ,

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≤

8β

2

4Rn(G) + √ +

ω¯∧

n

2

8

log

nδ

Applying our assumption that

Rn(H) ≤ Cn−φ and ˆ = − cˆ0 + C n−φ − C n−1/2.

for φ ≤ 1/2 and C ≥ 2C + 2 + 2 ln(8N/δ) and C ≥ − log2(δ/8) , then

disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−φ),

(32)

which implies

disparity(Qˆh) ≤ disparity(Q˜) + O˜(n−0 φ) + O˜(n−1 φ).

(33)

Proof of Theorem 3
The claim about the iteration complexity of Algorithm 4 follows from Lemma 9 after substituting in the stated choices of ν, Bλ. We consider two cases.

Case 1: There is a feasible solution Q˜ to the population problem (11) Using Lemmas 11-13, the ν-approximate saddle point (ξˆ, Qˆh) satisﬁes

disparity(Qˆh)

−

ξˆ ≤

Bξ

+

2ν ,

(34)

Bλ

− disparity(Qˆh) − ξˆ ≤ Bξ + 2ν

(35)

Bλ

cost(Qˆh) − ˆcost ≤ Bξ + 2ν

(36)

Bλ

for any (ξ, Q) that is feasible in the empirical problem. This implies that Algorithm 4 returns Qˆ = null. We will now show that the (ξˆ, Qˆ) provides an approximate solution to the discretized population problem.

First, through the same argument as in the proof of Theorem 1, we obtain that with probability at least 1 − δ/4 for all

Qh ∈ ∆(H)

cost(Qh) − cost(Qh) ≤ O˜(n−φ).

(37)

Second, with probability at least 1 − δ/2 for all Q ∈ ∆(H),

Eˆ[Qh|Ei,0] − E[Qh|Ei,0] ≤ O˜(n−0 φ)

(38)

Eˆ[Qh|Ei,1] − E[Qh|Ei,1] ≤ O˜(n−1 φ).

(39)

Finally, Hoeffding’s Inequality implies that with probability at least 1 − δ/4,

− log(δ/8) |cˆ0 − c0| ≤ 2n . (40)

23

From Lemma 14, we have that Algorithm 4 terminates and delivers (ξˆ, Qˆh) that compares favorable with any feasible (ξ, Qh) in the discretized sample problem. That is, for any such (ξ, Qh),

ξˆ ≤ ξ + O(n−φ),

(41)

disparity(Qˆh) ≤ ξˆ + O(n−φ),

(42)

− disparity(Qˆh) ≤ ξˆ + O(n−φ)

(43)

cost(Qˆh) ≤ ˆcost + O(n−φ)

(44)

Notice that (37), (40) and (44) imply that

cost(Qˆh) ≤ − c0 + O˜(n−φ),

(45)

where we used that ˆ = − cˆ0 + C n−φ − C n−φ. For any feasible (ξ, Qh), then (|disparity(Qh)| , Qh) is also feasible. Then, combining (41)-(43) yields

disparity(Qˆh) ≤ |disparity(Qh)| + O˜(n−φ)

(46)

Second, notice that this implies that

disparity(Qˆh) ≤ |disparity(Qh)| + O˜(n−0 φ) + O˜(n−1 φ)

(47)

We assumed that (ξ, Qh) were feasible in the discretized sample problem. Assuming that (37) holds implies that any feasible solution of the population problem is also feasible in the empirical problem due to how we set C , C . Therefore, we have just shown that (ξˆ, Qˆh) are approximately optimal in the discretized population problem.
Then, following the proof of Theorem 1, we observe that loss(Qˆh) ≤ + O˜(n−φ), where we now interpret Qˆh as a distribution over risk scores f ∈ F. This proves the result for Case I.

Case II: There is no feasible solution to the population problem (11) This follows the proof of Case II of Theorem
3 in [11]. If the algorithm returns a ν-approximate saddle point Qˆh, then the theorem holds vacuously since there is no feasible Q˜. Similarly, if the algorithm returns null, then the theorem also holds.

C Auxiliary Lemmas for Main Results
In this section, we state and prove a series of auxiliary lemmas that are used in the proofs of our main results in the text.
C.1 Auxiliary Lemmas for the Proof of Theorem 1 C.1.1 Iteration Complexity of Algorithm 3 Lemma 2. Letting ρ := maxh∈H |cost(h) − ˆ|, Algorithm 3 satisﬁes the inequality
νt ≤ B log(2) + ηρ2B. ηt
For η = 2ρν2B , Algorithm 3 will return a ν-approximate saddle point of L in at most 4ρ2Bν22log(2) . Since in our setting, ρ ≤ 1, the iteration complexity of Algorithm 3 is 4B2 log(2)/ν2.

Proof. Follows immediately from the proof of iteration complexity in Theorem 3 of [11]. Since the cost is bounded on [−1, 1] and cost(h) − ˆ ≤ cost(h) ≤ 1 for any h ∈ H, we see that ρ ≤ 1.
24

C.1.2 Solution Quality for Algorithm 3
Let Λ := {λ ∈ R+ : λ ≤ B} denote the domain of λ. Throughout this section, we assume we are given a pair (Qˆh, λˆ) that is a ν-approximate saddle point of the Lagrangian
L(Qˆh, λˆ) ≤ L(Qh, λˆ) + ν for all Qh ∈ ∆(H), L(Qˆh, λˆ) ≥ L(Qˆh, λ) − ν for all 0 ≤ λ ≤ B. We extend Lemma 1, Lemma 2 and Lemma 3 of [10] to our setting. Lemma 3. The pair (Qˆh, λˆ) satisﬁes
λˆ cost(Qˆh) − ˆ ≥ B cost(Qˆh) − ˆ − ν,
+
where (x)+ = max{x, 0}.

Proof. We consider a dual variable λ that is deﬁned as λ = 0 if cost(Qˆh) ≤ ˆ B otherwise.
From the ν-approximate optimality conditions, disparity(Qˆ) + λˆ cost(Qˆ) − ˆ = L(Qˆ, λˆ) ≥ L(Qˆ, λ) − ν = disparity(Qˆ) + λ cost(Q) − ˆ ,
and the claim follows by our choice of λ. Lemma 4. The distribution Qˆh satisﬁes
disparity(Qˆh) ≤ disparity(Qh) + 2ν for any Qh satisfying the empirical constraint (i.e., any Qh such that cost(Qh) ≤ ˆ).

Proof. Assume Qh satisﬁes cost(Qh) ≤ ˆ. Since λˆ ≥ 0, we have that L(Qh, λˆ) = disparity(Qh) + λˆ cost(Qh) − ˆ ≤ disparity(Qh).

Moreover, the ν-approximate optimality conditions imply that L(Qˆh, λˆ) ≤ L(Qh, λˆ) + ν. Together, these inequalities imply that
L(Qˆh, λˆ) ≤ disparity(Qh) + ν. Next, we use Lemma 3 to construct a lower bound for L(Qˆh, λˆ). We have that
L(Qˆh, λˆ) = disparity(Qˆh) + λˆ cost(Qh) − ˆ

≥ disparity(Qˆh) + B cost(Qˆ) − ˆ − ν
+
≥ disparity(Qˆh) − ν.
By combining the inequalities L(Qˆh, λˆ) ≥ disparity(Qˆh) − ν and L(Qˆh, λˆ) ≤ disparity(Qh) + ν, we arrive at the claim.

Lemma 5. Assume the empirical constraint cost(Qh) ≤ ˆ is feasible. Then, the distribution Qˆh approximately satisﬁes

the empirical cost constraint with

cost(Qˆh)

−

ˆ≤

|β0|

+

|β1|

+

2ν .

B

25

Proof. Let Qh satisfy cost(Qh) ≤ ˆ. Recall from the proof of Lemma 4, we showed that

disparity(Qˆh) + B cost(Qˆh) − ˆ − ν ≤ L(Qˆh, λˆ) ≤
+

Therefore, we observe that

disparity(Qh) + ν.

B cost(Qh) − ˆ ≤ disparity(Qh) − disparity(Qˆh) + 2ν.

Since we can bound disparity(Qh) − disparity(Qˆh) by |β0| + |β1|, the result follows.
Lemma 6. Suppose that Qh is any feasible solution to discretized sample problem. Then, the solution Qˆh returned by Algorithm 3 satisﬁes
disparity(Qˆh) ≤ disparity(Qh) + 2ν cost(Qˆh) ≤ ˆ + |β0| + |β1| + 2ν .
B

Proof. This is an immediate consequence of Lemma 2, Lemma 4 and Lemma 5. If the algorithm returns null, then these inequalities are vacuously satisﬁed.

C.1.3 Concentration Inequality

We restate Lemma 2 in [11], which provides a uniform concentration inequality on the convergence of a sample moment over a function class.

Let G be a class of functions g : U → R over some space U. The Rademacher complexity of the function class G is

deﬁned as

1n Rn(G) := u1,.s..u,upn∈U Eσ sgu∈pG n σig(ui) ,
i=1

where the expectation is deﬁned over the i.i.d. random variables σ1, . . . , σn with P (σi = 1) = P (σi = −1) = 1/2.

Lemma 7 (Lemma 2 in [11]). Let D be a distribution over a pair of random variables (S, U ) taking values in S × U. Let G be a class of functions g : U → [0, 1], and let ψ : S × [0, 1] → [−1, 1] be a contraction in its second argument (i.e., for all s ∈ S and t, t ∈ [0, 1], |ψ(s, t) − ψ(s, t )| ≤ |t − t |). Then, with probability 1 − δ, for all g ∈ G,

Eˆ [ψ(S, g(U ))] − E [ψ(S, g(U ))] ≤

2 4Rn(G) + √ +
n

2 ln(2/δ) ,
n

where the expectation is with respect to D and the empirical expectation is based on n i.i.d. draws from D. If ψ is linear in its second argument, then a tighter bound holds with 4Rn(G) replaced by 2Rn(G).

C.2 Auxiliary Lemmas for the Proof of Theorem 2

C.2.1 Concentration result for disparity under selective labels

Lemma 8.

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

P

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≥

n ≤ 4 exp −
2

2

ω¯∧

−

4Rn(G)

−

2 √

8β

n

+ 2 exp −nω¯∧2 4

+ 2 exp

−n 2ω¯∧4 64β2ω∨2

26

where ω∨ = max(ω1, ω0), ω¯∧ = min(ω¯1, ω¯0) and β = max(|β1| , |β0|)

Proof. For exposition, we ﬁrst show the steps for qualiﬁed afﬁrmative action and then extend the result to the general disparity. We can rewrite the qualiﬁed afﬁrmative action criterion as

E[f (X)|Y = 1, A = 1] = E[f (X)µ(X)|A = 1] (48) E[µ(X)|A = 1]

where µ(x) := E[Y | X = x].

E[f (X)|Y = 1, A = 1]

= E[f (PX()Y1{=Y1|=A1=}|1A) =1] = E[f (EX[P)E([Y1{=Y1=|X1,}A|X=,1A)|=A1=]|1A]=1] = E[f (X)PE([Yµ=(X1|)X|A,A==1]1)|A=1]
= E[fE([Xµ()Xµ()X|A)|=A1=] 1]

(49) (50) (51) (52)

Assuming access to the oracle µ function, we can estimate this on the full training data as

Eˆ[f (X)µ(X, A = 1)|A = 1]

Eˆ[µ(X, A = 1)|A = 1]

(53)

Next we will make use of Lemma 2 of [11], which we restate here again for convenience. Under certain conditions on φ and g, with probability at least 1 − δ
Eˆ [φ(S, g(U ))] − E [φ(S, g(U ))] ≤

2 4Rn(G) + √ +
n

2 ln(2/δ) .
n

We invert the bound by setting = 4Rn(G) + √2n + 2 ln(n2/δ) and solving for δ to get

2

n

2

δ = 2 exp −

− 4Rn(G) − √

(54)

2

n

Now we can restate Lemma 2 of [11] as

P Eˆ [φ(S, g(U ))] − E [φ(S, g(U ))] >

(55)

n ≤ 2 exp −
2

2
2 − 4Rn(G) − √
n

Next we revisit the quantity that we want to bound:

ω ωˆ

ω¯ − ωˆ¯

(56)

27

where ω = E[f (X)µ(X, A = 1)|A = 1] and ω¯ = E[µ(X, A = 1)|A = 1] and correspondingly for ωˆ and ωˆ¯. We will rewrite Expression 56 as a ratio of differences. We have

ωˆ ω ωˆ¯ − ω¯

= ωˆω¯ωˆ¯−ω¯ωˆ¯ω = ω¯(ωˆω¯−(ωˆ¯ω−)−ω¯ω)+(ωˆ¯ω¯−2 ω¯)

(57)
(58) (59)

By triangle inequality and union bound, we have

ω¯(ωˆ − ω) − ω(ωˆ¯ − ω¯)

t

P | ω¯(ωˆ¯ − ω¯) + ω¯2 | ≥ ω¯2/2

< P |ω¯(ωˆ − ω)| + |ω(ωˆ¯ − ω¯)| ≥ t + P |(ωˆ¯ − ω¯) + ω¯2| ≤ ω¯2 2

t < P |ω¯(ωˆ − ω)| ≥

+ P |ω(ωˆ¯ − ω¯)| ≥ t

+ P |ω¯(ωˆ¯ − ω¯) + ω¯2| ≤ ω¯2

2

2

2

Since 0 ≤ µ(X, A = 1) ≤ 1, we can use a Hoeffding bound for the quantity |(ωˆ¯ − ω¯)|. Note that 0 ≤ ω ≤ ω¯ ≤ 1. Then applying Hoeffding’s inequality gives us

ω(ωˆ¯ − ω¯) ≥ t ≤ 2 exp −nt2 (60)

P

2

4ω2

Next we bound the third term:

P |ω¯(ωˆ¯ − ω¯) + ω¯2| ≤ ω¯2 ≤ P |ω¯(ωˆ¯ − ω¯)| ≥ ω¯2

(61)

2

2

=P

|(ωˆ¯ − ω¯)| ≥

ω¯ 2

(62)

≤ 2 exp −n4ω¯2 (63)

where we again used Hoeffding’s inequality for the last line.

We bound the ﬁrst term using the restated Lemma in 55:

2

t

nt

2

|ω¯(ωˆ − ω)| ≥ ≤ 2 exp −

− 4Rn(G) − √

(64)

P

2

2 2ω¯

n

Now

we

let

˜=

t ω¯ 2 /2

to

get

ωˆ ω

P ωˆ¯ − ω¯ ≥ ˜

(65)

2

n ˜ω¯

2

−n˜2 ω¯ 4

−nω¯ 2

≤ 2 exp − 2 4 − 4Rn(G) − √n + exp 16ω2 + exp 4

Now we turn to the general case. Recalling that we deﬁne β = max(|β1, β0|), we have

β1ωˆ1 β0ωˆ0 β1ω1 β0ω0

P

ωˆ¯1 − ωˆ¯0 −

−

ω¯1

ω¯0

≥≤

ωˆ1 ω1

ωˆ0 ω0

P |β1| ωˆ¯1 − ω¯1 + |β0| ωˆ¯0 − ω¯0 ≥ ≤

ωˆ1 ω1

ωˆ0 ω0

P

ωˆ¯1 − ω¯1

≥ 2β

+P

ωˆ¯0 − ω¯0

≥ 2β

≤

28

n 2 exp −
2

2

ω¯1

− 4R

2 (G) − √

+ exp −n 2ω¯14 + exp −nω¯12 +

8β

n

n

64βω12

4

n 2 exp −
2

2

ω¯0

−

4Rn(G)

−

2 √

+

8β

n

4 exp

−n 2ω¯04

−nω¯02

exp 64βω2 + exp 4 ≤

0

2

n −

ω¯∧

− 4R

2 (G) − √

+ 2 exp −n 2ω¯∧4

2 8β

n

n

64β2ω∨2

+2 exp −nω¯∧2 4
where the ﬁrst inequality holds by triangle inequality, the second inequality holds by the union bound, the third inequality applies (65) for ˜ = 2β , and the ﬁnal inequality simpliﬁes the bound using the notation ω∨ = max(ω1, ω0) and ω¯∧ = min(ω¯1, ω¯0).

C.3 Auxiliary Lemmas for the Proof of Theorem 3
C.3.1 Iteration Complexity for Algorithm 4
Lemma 9. Deﬁning ρ := maxh∈H,ξ∈[0,Bξ] max{disparity(h) − ξ, −disparity(h) − ξ, cost(h) − ˆ}, Algorithm 4 satisﬁes the inequality
νt ≤ Bλ log(3) + ηρ2B. ηt
For η = 2ρ2νBλ , Algorithm 4 will return a ν-approximate saddle point of L in at most 4ρ2Bλν22log(3) iterations. Setting Bξ = 1, we observe ρ ≤ 1, and so the iteration complexity of Algorithm 4 is 4Bλ2νlo2g(3) .

Proof. Follows immediately from the proof of Theorem 3 in [11] and the same argument given in the proof of Lemma 2.

C.3.2 Solution Quality for Algorithm 4
Let Λ = {λ ∈ R3+ : λ ≤ Bλ}. Assume we are given ξˆ, Qˆh, λˆ , which is a ν-approximate saddle point satisfying L(ξˆ, Qˆh, λˆ) ≤ L(ξ, Qh, λˆ) + ν for all Qh ∈ ∆(H), ξ ∈ [0, Bξ] and L(ξˆ, Qˆh, λˆ) ≥ L(ξˆ, Qˆh, λ) − ν for all λ ≤ Bλ. We extend Lemmas 3-5 to the problem of ﬁnding the absolute disparity minimizing model. Lemma 10. ξˆ, Qˆh, λˆ satisﬁes
λˆ+(disparity(Qˆh) − ξˆ) + λˆ−(−disparity(Qˆh) − ξˆ) + λˆcost cost(Qˆh) − ˆ
≥ Bλ max{disparity(Qˆh) − ξˆ, −disparity(Qˆh) − ξˆ, cost(Qˆh) − ˆ} − ν.

Proof. The argument is the same as the proof of Lemma 3.

Lemma 11. The value ξˆ satisﬁes

ξˆ ≤ ξ + 2ν

for any ξ such that there exists Qh satisfying disparity(Qh) − ξ ≤ 0, −disparity(Qh) − ξ ≤ 0 and cost(Qh) ≤ ˆ.

Proof. Assume the pair (ξ, Qh) satisﬁes disparity(Qh) − ξ ≤ 0, −disparity(Qh) − ξ ≤ 0 and cost(Qh) ≤ ˆ. Since λˆ ≥ 0, we have that L(ξ, Q, λˆ) ≤ ξ. Moreover, the ν-approximate optimality conditions imply that L(ξˆ, Qˆ, λˆ) ≤

29

L(ξ, Q, λˆ) + ν. Together, these inequalities imply that L(ξˆ, Qˆ, λˆ) ≤ ξ + ν.
Next, we can use Lemma 10 to construct a lower bound for L(ξˆ, Qˆ, λˆ). To do so, observe that L(ξˆ, Qˆ, λˆ) ≥ ξˆ + Bλ max{disparity(Qˆ) − ξˆ, −disparity(Qˆ) − ξˆ, cost(Qˆ) − ˆ} − ν ≥ ξˆ − ν.
By combining the inequalities, L(ξˆ, Qˆ, λˆ) ≥ ξˆ − ν and L(ξˆ, Qˆ, λˆ) ≤ ξ + ν, we arrive at the claim.

Lemma 12. Assume the empirical cost constraint costQh ≤ ˆand the slack variable constraints disparity(Qh)−ξ ≤ 0 and −disparity(Qh) − ξ ≤ 0 are feasible. Then, the pair (ξˆ, Qˆh) satisﬁes

disparity(Qˆh)

−

ξˆ ≤

Bξ

+

2ν ,

Bλ

−

disparity(Qˆh)

−

ξˆ ≤

Bξ

+

2ν .

Bλ

Proof. Let ξ be a feasible value of the slack variable such that there exists Qh satisfying cost(Qh) ≤ ˆ and the slack variable constraints disparity(Qh) − ξ ≤ 0, −disparity(Qh) − ξ ≤ 0. Recall from the Proof of Lemma 11, we showed that
ξˆ + Bλ max{disparity(Qˆh) − ξˆ, −disparity(Qˆh) − ξˆ, cost(Qˆh) − ˆ} − ν ≤ L(ξˆ, Qˆh, λˆ) ≤ ξ + ν.
Therefore, it is immediate that
Bλ max{disparity(Qˆh) − ξˆ, −disparity(Qˆh) − ξˆ, cost(Qˆh) − ˆ} ≤ ξ − ξˆ + 2ν,

and so

Bλ disparity(Qˆh) − ξˆ ≤ ξ − ξˆ + 2ν, Bλ −disparity(Qˆh) − ξˆ ≤ ξ − ξˆ + 2ν.

Since ξ ∈ [0, Bξ], we can bound ξ − ξˆ by Bξ. The result follows.

Lemma 13. Assume the empirical cost constraint cost(Qh) ≤ ˆand the slack variable constraints disparity(Qh)−ξ ≤ 0, −disparity(Qh) − ξ ≤ 0 are feasible. Then the distribution Qˆh satisﬁes

cost(Qˆh)

−

ˆ≤

Bξ

+

2ν .

Bλ

Proof. The proof is analogous to the proof of Lemma 12.

Lemma 14. Suppose that (ξ, Qh) is a feasible solution to the empirical version of (13). Then, the solution (ξˆ, Qˆh) returned by Algorithm 4 satisﬁes

ξˆ ≤ ξ + 2ν,

disparity(Qˆh)

−

ξˆ ≤

Bξ

+

2ν ,

Bλ

− disparity(Qˆh) − ξˆ ≤ Bξ + 2ν Bλ

cost(Qˆh)

−

ˆ≤

Bξ

+

2ν .

Bλ

30

Proof. The proof follows from Lemmas 12-13. If the algorithm returns null, then these inequalities are vacuously satisﬁed.
D Additional Details on the Consumer Lending Data
D.1 Construction of IRSD for SA4 Regions As discussed in § 6, we focus our analysis on predictive disparities across SA4 geographic regions within Australia. We use the Australian Bureau of Statistics’ Index of Relative Socioeconomic Disadvantage (IRSD) to deﬁne socioeconomically disadvantaged SA4 regions. The IRSD is calculated for SA2 regions, which are more granular statistical areas used by the ABS, by aggregating sixteen variables that were collected in the 2016 Australian census. These variables include, for example, the fraction of households making less than AU$26,000, the fraction of households with no internet access, and the fraction of residents who do not speak English well. Higher scores on the IRSD are associated with less socioeconomically disadvantaged regions, and conversely, lower scores on the IRSD are associated with more socioeconomically disadvantaged regions. The full list of variables that are included in the IRSD and complete details on how the IRSD is constructed is provided in [50].
Figure 2: SA4 regions in Australia. We classify SA4 regions as being "socioeconomically disadvantaged" (red) and "non-socioeconomically disadvantaged" (blue) based on the Index of Relative Socioeconomic Disadvantage (IRSD).
Because the IRSD is constructed for SA2 regions, we ﬁrst aggregate this index to SA4 regions. We construct an aggregated IRSD for each SA4 region by constructing a population-weighted average of the IRSD for all SA2 regions that fall within each SA4 region. This delivers a quantitative measure of which SA4 regions are the most and least socioeconomically disadvantaged. For example, the bottom ventile (i.e., the 20th ventile) of SA4 regions based upon the population-weighted average IRSD (i.e., the least socioeconomically disadvantaged SA4 regions) are regions associated with Sydney and Perth. The top ventile (i.e., the 1st ventile) of SA4 regions based upon the population-weighted average IRSD (i.e., the most socioeconomically disadvantaged SA4 regions) are regions associated with the Australian outback such as the Northern territory outback and the Southern Australia outback. Figure 2 provides a map of SA4 regions in Australia, in which colors SA4 regions classiﬁed as socioeconomically disadvantaged in blue.
E Additional Experimental Details and Results
In this section, we present additional details on our experimental setup as well as additional results for both experiments presented in the main paper.
E.1 Consumer Lending: Additional Experimental Details We performed experiments on a random 2% sample of over 360,000 loan applications submitted from July 2017 to July 2019 by customers who did not have a prior ﬁnancial relationship with CommBank, yielding our experimental sample of 7414 applications. We did a 2:1 train-test split, resulting in 4906 applications in our training set and 2508 applications in our test set. In order to evaluate our methods on the full population (including applications that are not funded), we generate synthetic funding decisions Di and outcomes Y˜i∗ from the observed application features. On a 20% sample of the full
31

360,000 applicants, we train a classiﬁer π(x) to predict the observed funding decision Di using the application features Xi, and we train a classiﬁer µ(x) on funded applicants to predict the observed default outcome Yi using the application features Xi. In other words, π(x) estimates P (Di = 1|Xi = x) and µ(x) estimates P (Yi = 1|Di = 1, Xi = x). For both models we use probability forests from the R package ranger with the default hyperparameters: 500 trees,
mtry = [dim(X)] = 6, min node size equal 10, and max depth equal to 0. To learn µ, we use bootstrap sampling of the (0, 1) classes with probabilities (0.01, 1), respectively, in order to down-sample the applicants who repaid the loans because we have signiﬁcant class imbalance: Only 2.0% of applicants have default outcomes = 1.
We generate synthetic funding decisions D˜i according to Di | Xi ∼ Bernoulli(π(Xi)) and synthetic default outcomes Yi∗ according to Yi∗ | Xi ∼ Bernoulli(µ(Xi)). We then proceed with our learning as if we only had access to labels Yi∗ for applicants with Di = 1. We estimate µˆ(x) := Pˆ(Yi = 1|Xi = x, Di = 1) using random forests with the same hyperparameters as above and use µˆ(x) to construct the pseudo-outcomes used by the IE and RIE approaches. The KGB, IE, and RIE approaches use linear regression. Our FaiRS algorithm ran the exponentiated gradient alg√orithm for at m√ost 500 iterations on a ﬁxed discretization grid, Zα = {1/40, 2/40, . . . , 1} with parameters B = n and ν = 1/ n and η = 2. These choices were guided by our theoretical results as well as prior work [11]. The average runtime for a single error tolerance was 26.4 minutes. The experiments were conducted on a machine with one Intel Xeon E5-2650 v2 processor with 2.60 GHz and 16 cores.
Our comparison against prior work used the fairlearn12 API with logistic regression, using parameters parameters C = 10 and maximum iterations = 10, 000. We ran fairlearn using both grid search and exponentiated gradient algorithm, but we report only the grid search algorithm since it traced out a larger fairness-performance tradeoff curve than the exponentiated gradient algorithm. We used a grid size of 41 with a grid limit of 2.
We also compared against the Target-Fair Covariate Shift method in [39]. To construct our covariate shift weights, we ﬁrst estimated the propensity scores P (D = 1 | X = x) by regressing D ∼ X , yielding propensity estimates πˆ(x). Our propensity model used ranger probability forests that with the default hyperparameters: 500 trees,
mtry = [dim(X)] = 6, min node size equal 10, and max depth equal to 0. We used max(πˆ(X), 50) as covariate shift weights. We ran the method for λ = {0, 10, 1000, 20000, 50000} using step size η = 0.01. We terminated the algorithm when the L1 distance in the weight vector ≤ 1e − 7 or after 500 iterations (whichever came ﬁrst).

E.2 Consumer Lending Risk Scores: Additional Results
Figure 3 provides an extended version of Figure 1 that reports models over a range of hyperparameters (e.g. loss tolerance for FaiRS) to show the range of possible fairness-performance combinations.

0.70

All applicants

0.69 0.68 qqqqqqq q q

0.67

q

q

qqq q

0.66

0.70

AUC

Funded applicants

0.69

qqqqqqqqq qq

0.68

qqq

q

0.67

0.66

0.00

0.02

0.04

Disparity: E[f(X)|A = 1] − E[f(X)|A = 0]

q Known Good−Bad (KGB) q Reject extrapolation (RIE) q Inter & extrapolation (IE) q Reweighed q benchmark Fairlearn TFCS q FaiRS (ours)

Figure 3: Area under the ROC curve (AUC) with respect to the synthetic outcome against disparity in the average risk prediction for the disadvantaged (Ai = 1) vs advantaged (Ai = 0) groups. FaiRS reduces disparities for the RIE and IE approaches while maintaining AUCs comparable to the benchmark models (ﬁrst row). Evaluation on only funded applicants (second row) overestimates the performance of TFCS and KGB models and underestimates disparities for all models. Error bars show the 95% conﬁdence intervals. See § 6 of the main paper for details.

12See Fairlearn Github for code.

32

We next consider the implications of FaiRS for the credit applicants from the sensitive group. One might hope that encouraging statistical parity will increase access to credit for the sensitive group, and indeed we see some evidence for this. Figure 4 shows the distribution of risk scores for the disadvantaged group for the KGB, RIE, and IE models for the benchmark models (ﬁrst row) and for FaiRS with 1% loss tolerance (second row). The 75% percentile score is given as a dashed line. FaiRS shifts the 75% percentile KGB score to the left (left column). FaiRS therefore reduces the predicted risk of the sensitive group, thereby expanding access to credit. We see a smaller shift for the RIE and IE approaches, which have lower risk distributions than the KGB model. As we have seen elsewhere, evaluation on the funded only applicants (right column) lends misleading conclusions, e.g., underestimating both the difference in distributions between the KGB and RIE/IE approaches as well as differences between the benchmark and FaiRS variants.
Finally, we present results for the benchmarks and FaiRS models with respect to the loss they were trained to minimize, mean-squared error. Figure 5 shows the mean square error (MSE) against predictive disparity for the KGB, RIE, IE benchmarks and FaiRS variants on held-out test data. The qualitative patterns are the same as Figure 1 in § 6 of the main text. Evaluation on all applicants shows that FaiRS with reject extrapolation (RIE and IE) reduces disparities without impacting MSE. The RIE and IE methods achieve lower disparity and lower MSE than the KGB model trained only on funded data, highlighting the importance of adjusting for selective labels. We again observe that evaluation on only funded applications is misleading as it suggests that the KGB models have comparable MSE and it drastically underestimates predictive disparities for all models.
Figure 1 in § 6 of the main text and Figure 5 shows that the FaiRS KGB model appears to produce larger predictive disparities than the benchmark KGB model. This is likely due to generalization error on the held-out test data. To verify this hypothesis, Figure 6 shows the MSE against predictive disparity for the KGB, RIE, IE benchmarks and FaiRS variants on the training data. Indeed among funded applicants in the train data, FaiRS-KGB models produce smaller absolute predictive disparities than the benchmark KGB model (second row).

All applicants 3 2 1 0

Funded applicants

0%

density

3

2

1%

1

0

0.00

0.25

0.50

0.75

1.00 0.00

0.25

0.50

0.75

1.00

Predicted risk scores

Loss Tolerance

Known Good−Bad (KGB)

Reject extrapolation (RIE)

Inter & extrapolation (IE)

Figure 4: Predicted risk distributions for disadvantaged group Ai = 1 for FaiRS algorithm using KGB, RIE and IE approaches. The ﬁrst row shows the benchmark model risk scores. The second row shows our FaiRS’s risk scores for a loss tolerance of 1%. The left and right columns show risk scores on all applicants and funded applicants from the disadvantaged group respectively. The dashed line indicates the 75-percentile score. The RIE and IE methods predict lower rates of default for the disadvantaged group than the KGB method. The densities for the funded applicants (right column) underestimate the differences in risk scores across the KGB, RIE, and IE methods (compare to left column). See § 6 for details.

E.3 Recidivism Risk Prediction: Additional Results
ProPublica’s COMPAS recidivism data [51] contains 7,214 examples. We randomly split this data 50%-50% into a train and test set. We evaluate models using logistic regression loss, deﬁned as l(y, f (x)) = log(1 + e−C(2y−1)(2f(x)−1))/(log(1 + eC )) for C = 5. We ran the exponentiated gradient algorithm for at most 500 iterations on a ﬁxed discretization grid, Zα =√{1/40, 2/40, . . . , 1}. Letting n = 3, 60√7, we set the parameters of the expone√ntiated gradient algorithm to be B = n/2 for minimization problems, B = n for maximization problems, ν = 1/ n and η = 2. We report the average run time results for a single run of the exponentiated gradient algorithm to
33

0.228 0.226 0.224

q q qqq q

All applicants

Error: E[(~Y − f(X))2]

0.222

qqqq qqq

0.220

qq

0.228

qqq

0.226

qqq

q

q

qq q qq
qq

0.224

Funded applicants

0.222

0.220

0.000

0.005

0.010

Disparity: E[f(X)|A = 1] − E[f(X)|A = 0]

0.015

q Known Good−Bad (KGB) q Reject extrapolation (RIE) q Inter & extrapolation (IE) q benchmark q FaiRS (ours)

Figure 5: Mean square error (MSE) with respect to the synthetic outcome Y˜i against disparity in the average risk prediction for the disadvantaged (Ai = 1) vs. advantaged (Ai = 0) groups in held-out test data. The ﬁrst row evaluates each method on all applicants and and the second row evaluates each method on funded applicants only. See § 6 and
§ E.2 for details.

All applicants

Error: E[(~Y − f(X))2]

0.225

0.223 0.221

qqqq qq

0.225 0.223 0.221 q

qq

qq

qqq

q

q

q

qq qq q

q qq

q qqqq q

Funded applicants

−0.01

0.00

0.01

Disparity: E[f(X)|A = 1] − E[f(X)|A = 0]

q Known Good−Bad (KGB) q Reject extrapolation (RIE) q Inter & extrapolation (IE) q benchmark q FaiRS (ours)

Figure 6: Mean square error (MSE) with respect to the synthetic outcome Y˜i against disparity in the average risk prediction for the disadvantaged (Ai = 1) vs. advantaged (Ai = 0) groups in the training data. The ﬁrst row evaluates each method on all applicants and and the second row evaluates each method on funded applicants only. See § 6 and
§ E.2 for details.

34

Table 2: Timing for the recidivism risk prediction experiment on the ProPublic COMPAS dataset. We report the average time for the exponentiated gradient algorithm to complete at most 500 iterations on the train set (ntrain = 3, 607) in computing the disparity minimizing model (Min. Disp.) and the disparity maximizing model (Max. Disp.). Timing is reported in minutes. See § 7 for details.

SP BFPC BFNC

TIMING (IN MINUTES) MIN. DISP. MAX. DISP.

7.29 8.45 22.24

24.10 24.18 23.64

Table 3: The disparity minimizing and disparity maximizing models over the set of good models (performing within 1% of COMPAS’s training loss) achieve comparable test loss to COMPAS. The ﬁrst panel (SP) displays the test loss for the models that minimize (Min. Disp.) and maximize (Max. Disp.) the disparity in average predictions for black versus white defendants (Def. 1). The second panel (BFPC) analyzes the test loss for the models that minimize and maximize the disparity in average predictions for black versus white defendants in the positive class, and the third panel examines the test loss for the models that minimize and maximize the disparity in average predictions for black versus white defendants in the negative class (Def. 2). Standard errors are reported in parentheses. See § 7 for details.

SP BFPC BFNC

MIN. DISP.
0.095 (0.001)
0.099 (0.003)
0.094 (0.004)

TEST LOSS MAX. DISP.
0.067 (0.002)
0.085 (0.002)
0.073 (0.001)

COMPAS
0.102 (0.003)
0.102 (0.003)
0.102 (0.003)

solve the minimization and maximization problems for each disparity measure in Table 2 below. These experiments were conducted on a 2012 MacBook Pro with a 2.3 GHz Quad-Core Intel Core i7.
E.3.1 Test Loss
Table 3 reports the test loss of COMPAS and the test losses of the disparity minimizing and disparity maximizing models over the set of good models. The disparity minimizing and disparity maximizing models achieve comparable and in some cases lower test loss than COMPAS.
E.3.2 Train Set Performance
Figure 7 plots the range of predictive disparities over the train set when the parameter is calibrated using COMPAS. We report the train set performance for various choices of the loss tolerance parameter, setting = 1%, 5%, 10% of COMPAS’ training loss. The blue error bars plot the relative disparities associated with the linear program reduction (§ A.2), the green error bars plot the relative disparities associated with the stochastic prediction function returned by Algorithm 3 and the orange dashed line plots the relative disparity associated with COMPAS. The range of disparities produced by the linear program reduction closely track the range of disparities produced by the stochastic prediction function returned by Algorithm 3 in the train set, conﬁrming the quality of the linear programming reduction.
E.3.3 Results for Predictive Disparities across Young and Older Defendants
We also examine the range of predictive disparities between defendants that are younger than 25 years old (Ai = 1) and defendants older than 25 years old (Ai = 0), focusing on the range of predictive disparities that could be generated by a risk score that is constructed using logistic regression on a quadratic polynomial of the defendant’s age and number of prior offenses. We calibrate the loss tolerance parameter such that (2) constructs the fairness frontier over all models that achieve a logistic regression loss within 1% of COMPAS’s training loss. We provide the results for the statistical parity, balance for the positive class, and balance for the negative class disparity measures (Def. 1 and Def 2). Table 4 summarizes the range of predictive disparities over the test set when the parameter is calibrated using COMPAS’ training loss. While COMPAS lies within the range of possible disparities for each measure, notice that there exists a
35

(a) Statistical Parity

(b) Balance for the Positive Class

(c) Balance for the Negative Class
Figure 7: The minimal and maximal predictive disparities between black defendants (Ai = 1) and white defendants (Ai = 0) over the set of good models in the train set. We set the loss tolerance as = 1%, 5%, 10% of COMPAS’ training loss. The blue error bars plot the relative disparities associated with the linear program reduction (§ A.2), the green error bars plot the relative disparities associated with the stochastic prediction function returned by Algorithm 3 and the orange dashed line plots the predictive disparity associated with COMPAS. See § 7 and § E.3.2 for details.

predictive model that produces strictly smaller disparities between young and older defendants than the COMPAS risk assessment at minimal cost to predictive performance. The disparity minimizing and disparity maximizing models over the set of good models achieve a test loss that is comparable to COMPAS (see Table 5).
Figure 8 plots the range of predictive disparities over the train set when the parameter is calibrated using COMPAS. We report the train set performance for various choices of the loss tolerance parameter, setting = 1%, 5%, 10% of COMPAS’ training loss. The blue error bars plot the relative disparities associated with the linear program reduction (Section A.2), the green error bars plot the relative disparities associated with the stochastic prediction function returned by Algorithm 3 and the orange dashed line plots the relative disparity associated with COMPAS. We again ﬁnd that the range of disparities produced by the linear program reduction closely track the range of disparities produced by the stochastic prediction function returned by Algorithm 3.

Table 4: The minimal and maximal disparities between young defendants (Ai = 1) and older defendants (Ai = 0) over the set of good models (performing within 1% of COMPAS’ training loss) on the test set. The ﬁrst panel (SP) displays the disparity in average predictions for young versus older defendants (Def. 1). The second panel (BFPC) displaces the disparity in average predictions for young versus old defendants in the positive class, and the third panel examines the disparity in average predictions for young versus older defendants in the negative class (Def. 2). Standard errors are reported in parentheses. See § E.3.3 of the Supplement for details.

SP BFPC BFNC

MIN. DISP.
-0.296 (0.019)
-0.207 (0.010)
-0.040 (0.038)

MAX. DISP.
0.433 (0.008)
0.260 (0.008)
0.329 (0.008)

COMPAS
0.173 (0.014)
0.101 (0.019)
0.200 (0.022)

36

Table 5: The disparity minimizing and disparity maximizing models over the set of good models (performing within 1% of COMPAS’s training loss) achieve comparable test loss to COMPAS. The ﬁrst panel (SP) displays the test loss for the models that minimize (Min. Disp.) and maximize (Max. Disp.) the disparity in average predictions for young versus older defendants (Def. 1). The second panel (BFPC) analyzes the test loss for the models that minimize and maximize the disparity in average predictions for young versus older defendants in the positive class, and the third panel examines the test loss for the models that minimize and maximize the disparity in average predictions for young versus older defendants in the negative class (Def. 2). Standard errors are reported in parentheses. See § 7 for details.

SP BFPC BFNC

MIN. DISP.
0.096 (0.004)
0.098 (0.002)
0.094 (0.016)

TEST LOSS MAX. DISP.
0.097 (0.003)
0.098 (0.003)
0.093 (0.002)

COMPAS
0.102 (0.003)
0.102 (0.003)
0.102 (0.003)

(a) Statistical Parity

(b) Balance for the Positive Class

(c) Balance for the Negative Class
Figure 8: The minimal and maximal disparities between young defendants (Ai = 1) and older defendants (Ai = 0) over the set of good models on the train set. We set the loss tolerance as = 1%, 5%, 10% of COMPAS’ training loss. The blue error bars plot the relative disparities associated with the linear program reduction (§ A.2), the green error bars plot the relative disparities associated with the stochastic prediction function returned by Algorithm 3 and the orange dashed line plots the predictive disparity associated with COMPAS. See § E.3.3 of the Supplement for details.
E.4 Regression Experiments: Communities & Crime Dataset
The Communities & Crime dataset [55] contains 1,994 examples. We randomly split this data 50%-50% into a train and test set. We train models to predict the violent crime rate within each community (the number of violent crimes per 100,000 people), which is a continuous outcome. We evaluate models using least squares loss, deﬁne the benchmark model to be the loss-minimizing linear regression and focus on the statistical parity measure of predictive disparities between communities that are majority white vs. majority non-white. We use FaiRS to search for the predictive disparity minimizing linear regression that achieves a loss that is comparable to the benchmark (loss tolerance = 1%, 5%, 10% of the loss-minimizing linear regression). In this dataset, there is no selective labels problem, and so we construct the FaiRS model following approach detailed in § 4 and Supplement § A.3.
Table 6 summarizes both the predictive disparities and least squares losses over the test set of the F aiRS models and the benchmark linear regression. The FaiRS models achieve comparable performance to the test loss of the benchmark
37

Table 6: The FaiRS models over the set of good models (performing within 1%, 5%, and 10% of the loss-minimizing linear regression’s training loss) achieve comparable performance to the test loss of the loss-minimizing linear regression and produce lower predictive disparities. The ﬁrst column reports the disparity in average predictions between majority white and majority non-white communities (Def. 1). The second column reports the test losses for each model. See § E.4 for details.

BENCHMARK
FAIRS = 1% = 5% = 10%

LOSS
0.0101 (0.0007)
0.0103 (0.0008) 0.0105 (0.0008) 0.0108 (0.0008)

DISP.
-0.3386 (0.0135)
-0.2989 (0.0130) -0.2856 (0.0129) -0.2658 (0.0127)

loss-minimizing linear regression while producing lower predictive disparities. These results highlight that our proposed methods continue to perform well in regression tasks.

38

