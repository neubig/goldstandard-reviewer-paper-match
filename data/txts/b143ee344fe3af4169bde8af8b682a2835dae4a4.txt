A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks

Unnat Jain1 , Luca Weihs2 , Eric Kolve2, Ali Farhadi3, Svetlana Lazebnik1, Aniruddha Kembhavi2,3, and Alexander Schwing1

1 University of Illinois at Urbana-Champaign

2 Allen Institute for AI

3 University of Washington

arXiv:2007.04979v1 [cs.CV] 9 Jul 2020

Abstract. Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task’s diﬃculty outpaces a single agent’s abilities. While multi-agent collaboration research has ﬂourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 percentage points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at https://unnat.github.io/cordial-sync.
Keywords: Embodied agents, multi-agent reinforcement learning, collaboration, emergent communication, AI2-THOR
1 Introduction
Collaboration is the deﬁning principle of our society. Humans have reﬁned strategies to eﬃciently collaborate, developing verbal, deictic, and kinesthetic means. In contrast, progress towards enabling artiﬁcial embodied agents to learn collaborative strategies is still in its infancy. Prior work mostly studies collaborative agents in grid-world like environments. Visual, multi-agent, collaborative tasks have not been studied until very recently [23,42]. While existing tasks are well designed to study some aspects of collaboration, they often don’t require agents to closely collaborate throughout the task. Instead such tasks either require initial coordination (distributing tasks) followed by almost independent execution,
denotes equal contribution by UJ and LW

2

U. Jain & L. Weihs et al.

Ending Position l Object
Goa

Starting Position

Goal object VieiswoOccblusdtreudcted
Agent 1 view
Goal object Goaisl vOisbibselerved

Agent 2 view
Fig. 1: Two agents communicate and synchronize their actions to move a heavy object through a complex indoor environment towards a goal. (a) Agents are initialized holding the object in a randomly chosen location. (b) Note the agent’s egocentric views. Successful navigation requires agents to communicate their intent to reposition themselves, and the object, while contending with collisions, mutual occlusion, and partial information. (c) Agents successfully moved the object above the goal

or collaboration at a task’s end (e.g., verifying completion). Few tasks require frequent coordination, and we are aware of none within a visual setting.
To study our algorithmic ability to address tasks which require close and frequent collaboration, we introduce the furniture moving (FurnMove) task (see Fig. 1), set in the AI2-THOR environment. Given only their egocentric visual observations, agents jointly hold a lifted piece of furniture in a living room scene and must collaborate to move it to a visually distinct goal location. As a piece of furniture cannot be moved without both agents agreeing on the direction, agents must explicitly coordinate at every timestep. Beyond coordinating actions, high performance in our task requires agents to visually anticipate possible collisions, handle occlusion due to obstacles and other agents, and estimate free space. Akin to the challenges faced by a group of roommates relocating a widescreen television, this task necessitates extensive and ongoing coordination amongst all agents at every time step.
In prior work, collaboration between multiple agents has been enabled primarily by (i) sharing observations or (ii) learning low-bandwidth communication. (i) is often implemented using a centralized agent, i.e., a single agent with access to all observations from all agents [9,71,93]. While eﬀective it is also unrealistic: the real world poses restrictions on communication bandwidth, latency, and modality. We are interested in the more realistic decentralized setting enabled via option (ii). This is often implemented by one or more rounds of message passing between agents before they choose their actions [27,58,42]. Training decentralized agents when faced with FurnMove’s requirement of coordination at each

A Cordial Sync

3

timestep leads to two technical challenges. Challenge 1: as each agent independently samples an action from its policy at every timestep, the joint probability tensor of all agents’ actions at any given time is rank-one. This severely limits which multi-agent policies are representable. Challenge 2: the number of possible mis-steps or failed actions increases dramatically when requiring that agents closely coordinate with each other, complicating training.
Addressing challenge 1, we introduce SYNC (Synchronize Your actioNs Coherently) policies which permit expressive (i.e., beyond rank-one) joint policies for decentralized agents while using interpretable communication. To ameliorate challenge 2 we introduce the Coordination Loss (CORDIAL) that replaces the standard entropy loss in actor-critic algorithms and guides agents away from actions that are mutually incompatible. A 2-agent system using SYNC and CORDIAL obtains a 58% success rate on test scenes in FurnMove, an impressive absolute gain of 25 percentage points over the baseline from [42] (76% relative gain). In a 3-agent setting, this diﬀerence is even more extreme.
In summary, our contributions are: (i) FurnMove, a new multi-agent embodied task that demands ongoing coordination, (ii) SYNC, a collaborative mechanism that permits expressive joint action policies for decentralized agents, (iii) CORDIAL, a training loss for multi-agent setups which, when combined with SYNC, leads to large gains, and (iv) improvements to the open-source AI2-THOR environment including a 16× faster gridworld equivalent enabling fast prototyping.

2 Related work
We start by reviewing single agent embodied AI tasks followed by non-visual Multi-Agent RL (MARL) and end with visual MARL. Single-agent embodied systems: Single-agent embodied systems have been considered extensively in the literature. For instance, literature on visual navigation, i.e., locating an object of interest given only visual input, spans geometric and learning based methods. Geometric approaches have been proposed separately for mapping and planning phases of navigation. Methods entailing structure-from-motion and SLAM [91,80,25,13,72,81] were used to build maps. Planning algorithms on existing maps [14,46,52] and combined mapping & planning [26,50,49,30,6] are other related research directions.
While these works propose geometric approaches, the task of navigation can be cast as a reinforcement learning (RL) problem, mapping pixels to policies in an end-to-end manner. RL approaches [68,1,20,33,44,92,62,86] have been proposed to address navigation in synthetic layouts like mazes, arcade games and other visual environments [100,8,47,54,43,84]. Navigation within photo-realistic environments [11,79,15,48,102,5,35,101,59] led development of embodied AI agents. The early work [107] addressed object navigation (ﬁnd an object given an image) in AI2-THOR. Soon after, [35] showed how imitation learning permits agents to learn to build a map from which they navigate. Methods also investigate the utility of topological and latent memory maps [35,78,37,99], graph-

4

U. Jain & L. Weihs et al.

based learning [99,103], meta-learning [98], unimodal baselines [90], 3D point clouds [97], and eﬀective exploration [95,78,16,74] to improve embodied navigational agents. Embodied navigation also aids AI agents to develop behavior such as instruction following [38,4,82,95,3], city navigation [18,64,63,94], question answering [21,22,34,97,24], and active visual recognition [105,104]. Recently, with visual and acoustic rendering, agents have been trained for audio-visual embodied navigation [19,31].
In contrast to the above single-agent embodied tasks and approaches, we focus on collaboration between multiple embodied agents. Porting the above single-agent architectural novelties (or a combination of them) to multi-agent systems such as ours is an interesting direction for future work.
Non-visual MARL: Multi-agent reinforcement learning (MARL) is challenging due to non-stationarity when learning. Multiple methods have been proposed to address resulting issues [88,89,87,29]. For instance, permutation invariant critics have been developed recently [57]. In addition, for MARL, cooperation and competition between agents has been studied [51,70,60,12,69,36,58,28,57]. Similarly, communication and language in the multi-agent setting has been investigated [32,45,10,61,53,27,83,67,7] in maze-based setups, tabular tasks, or Markov games. These algorithms mostly operate on low-dimensional observations such as kinematic measurements (position, velocity, etc.) and top-down occupancy grids. For a survey of centralized and decentralized MARL methods, kindly refer to [106]. Our work diﬀers from the aforementioned MARL works in that we consider complex visual environments. Our contribution of SYNC-Policies is largely orthogonal to RL loss function or method. For a fair comparison to [42], we used the same RL algorithm (A3C) but it is straightforward to integrate SYNC into other MARL methods [75,28,58] (for details, see Sec. A.3 of the supplement).
Visual MARL: Recently, Jain et al . [42] introduced a collaborative task for two embodied visual agents, which we refer as FurnLift. In this task, two agents are randomly initialized in an AI2-THOR living room scene, must visually navigate to a TV, and, in a singe coordinated PickUp action, work to lift that TV up. Note that FurnLift doesn’t demand that agents coordinate their actions at each timestep. Instead, such coordination only occurs at the last timestep of an episode. Moreover, as success of an action executed by an agent is independent (with the exception of the PickUp action), a high performance joint policy need not be complex, i.e., it may be near low-rank. More details on this analysis and the complexity of our proposed FurnMove task are provided in Sec. 3.
Similarly, a recent preprint [17] proposes a visual hide-and-seek task, where agents can move independently. Das et al . [23] enable agents to learn who to communicate with, on predominantly 2D tasks. In visual environments they study the task where multiple agents parallely navigate to the same object. Jaderberg et al . [41] recently studied the game of Quake III and Weihs et al . [96] develop agents to play an adversarial hiding game in AI2-THOR. Collaborative perception for semantic segmentation and recognition classiﬁcation have also been investigated recently [55,56].

A Cordial Sync

5

To the best of our knowledge, all previous visual or non-visual MARL in the decentralized setting operate with a single marginal probability distribution per agent, i.e., a rank-one joint distribution. Moreover, FurnMove is the ﬁrst multiagent collaborative task in a visually rich domain requiring close coordination between agents at every timestep.

3 The furniture moving task (FurnMove)

We describe our new multi-agent task FurnMove, grounded in the real-world

experience of moving furniture. We begin by introducing notation.

RL background and notation. Consider N ≥ 1 collaborative agents A1, . . . ,

AN . At every timestep t ∈ N = {0, 1, . . .} the agents, and environment, are in

some state st ∈ S and each agent Ai obtains an observation oit recording some partial information about st. For instance, oit might be the egocentric visual view of an agent Ai embedded in some simulated environment. From observation oit and history hit−1, which records prior observations and decisions made by the agent, each agent Ai forms a policy πti : A → [0, 1] where πti(a) is the probability that agent Ai chooses to take action a ∈ A from a ﬁnite set of options A at

time t. After the agents execute their respective actions (a1t , . . . , aNt ), which we

call a multi-action, they enter a new state st+1 and receive individual rewards

rt1

,

.

.

.

,

r

N t

∈

R.

For

more

on

RL

see

[85,65,66].

Task deﬁnition. FurnMove is set in the near-photorealistic and physics en-

abled simulated environment AI2-THOR [48]. In FurnMove, N agents collab-

orate to move a lifted object through an indoor environment with the goal of

placing this object above a visually distinct target as illustrated in Fig. 1. Akin

to humans moving large items, agents must navigate around other furniture and

frequently walk in-between obstacles on the ﬂoor.

In FurnMove, each agent at every timestep receives an egocentric obser-

vation (a 3 × 84 × 84 RGB image) from AI2-THOR. In addition, agents are

allowed to communicate with other agents at each timestep via a low band-

width communication channel. Based on their local observation and communi-

cation, each agent must take an action from the set A. The space of actions A = ANAV ∪ AMWO ∪ AMO ∪ ARO available to an agent is comprised of the four single-agent navigational actions ANAV = {MoveAhead, RotateLeft, RotateRight, Pass} used to move the agent independently, four actions AMWO =

{MoveWithObjectX | X ∈ {Ahead, Right, Left, Back}} used to move

the lifted object and the agents simultaneously in the same direction, four ac-

tions AMO = {MoveObjectX| X ∈ {Ahead, Right, Left, Back}} used

to move the lifted object while the agents stay in place, and a single action

used to rotate the lifted object clockwise ARO = {RotateObjectRight}. We

assume that all movement actions for agents and the lifted object result in a

displacement of 0.25 meters (similar to [42,59]) and all rotation actions result

in a rotation of 90 degrees (counter-)clockwise when viewing the agents from

above.

6

U. Jain & L. Weihs et al.

Agent 1 action (a1)
MAhead RotateLeft RotateRight Pass MWOAhead MWORight MWOBack MWOLeft RORight MOAhead MORight MOBack MOLeft
Agent 1 action (a1)
MAhead RotateLeft RotateRight Pass PickUp

Single-agent navigation

MoOvbejeWctith RO

Move Object

MAhead X X X X X X X X X X X X X RotateLeft X X X X X X X X X X X X X RotateRight X X X X X X X X X X X X X
Pass X X X X X X X X X X X X X MWOAhead X X X X X X X X X X X X X MWORight X X X X X X X X X X X X X
MWOBack X X X X X X X X X X X X X MWOLeft X X X X X X X X X X X X X RORight X X X X X X X X X X X X X MOAhead X X X X X X X X X X X X X MORight X X X X X X X X X X X X X
MOBack X X X X X X X X X X X X X MOLeft X X X X X X X X X X X X X
Agent 2 action (a2)
(a) FurnMove

Legend 0° 90° 180°

270°

A1

A2

angle angle

Relative orientation

Relative orientation defines validity

X - Always Invalid Always Valid

Single-agent navigation

MAhead

X

RotateLeft

X

RotateRight

X

Pass

X

PickUp X X X X

Agent 2 action (a2)

Legend X - Always Invalid
Always Valid

(b) FurnLift

Fig. 2: Coordination matrix for tasks. The matrix St records the validity of multi-action (a1, a2) for diﬀerent relative orientations of agents A1 & A2. (a)
Overlay of St for all four relative orientation of two agents, for FurnMove. Notice that only 16/169 = 9.5% multi-actions are coordinated at any given
relative orientation, (b) FurnLift where single agent actions are always valid and coordination is needed only for PickUp action, i.e. at least 16/25 = 64% actions are always valid.

Close and on-going collaboration is required in FurnMove due to restrictions on the set of actions which can be successfully completed jointly by all the agents. These restrictions reﬂect physical constraints: for instance, if two people attempt to move in opposite directions while carrying a heavy object they will either fail to move or drop the object. For two agents, we summarize these restrictions using the coordination matrix shown in Fig. 2a. For comparison, we include a similar matrix in Fig. 2b corresponding to the FurnLift task from [42]. We defer a more detailed discussion of these restrictions to Sec. A.1 of the supplement. Generalizing the coordination matrix shown in Fig. 2a, at every timestep t we let St be the {0, 1}-valued |A|N -dimensional tensor where (St)i1,...,iN = 1 if and only if the agents are conﬁgured such that multi-action (ai1 , . . . , aiN ) satisﬁes the restrictions detailed in Sec. A.1. If (St)i1,...,iN = 1 we say the actions (ai1 , . . . , aiN ) are coordinated.
3.1 Technical challenges
As we show in our experiments in Sec. 6, standard communication-based models similar to the ones proposed in [42] perform rather poorly when trained to complete the FurnMove task. In the following we identify two key challenges that contribute to this poor performance. Challenge 1: rank-one joint policies. In classical multi-agent settings [12,70,58], each agent Ai samples its action ait ∼ πti independently of all other agents. Due

Agent 1

CNN

TBONE backbone (prior work)

LSTM

Talk Stage

Reply Stage

To Policy Head

Communication band

𝐶 Comm.
Symbols

Agent 2 CNN LSTM

Talk Stage

Reply Stage

To Policy Head

A Cordial Sync

7

Marginal Policy (prior work)

Marginal

𝜋1

Marginal

𝜋2

SYNC-Policies (ours)

Mixture

... 𝜋11 ... 𝜋𝑗1
𝜋𝐾1

𝑓𝜃

𝛂

Sample 𝑗∼𝛂
(shared seed)

Mixture

... 𝜋12 ... 𝜋𝑗2
𝜋𝐾2

Fig. 3: Model overview for 2 communicative agents in the decentralized setting. Left: all decentralized methods in this paper have the same TBONE [42] backbone architecture. Right: marginal vs SYNC-policies. With marginal policies, the standard in prior work, each agent constructs its own policy and independently samples from this policy. With SYNC-policies, agents communicate to construct a distribution α over multiple “strategies” which they then sample from using a shared random seed

to this independent sampling, at time t, the probability of the agents taking

multi-action (a1, . . . , aN ) equals

N i=1

πti(ai).

This

means

that

the

joint

prob-

ability tensor of all actions at time t can be written as the rank-one tensor

Πt = πt1 ⊗ · · · ⊗ πtN . This rank-one constraint limits the joint policy that can

be executed by the agents, which has real impact. Sec. A.2 considers two agents

playing rock-paper-scissors with an adversary: the rank-one constraint reduces

the expected reward achieved by an optimal policy from 0 to -0.657 (minimal

reward being -1). Intuitively, a high-rank joint policy is not well approximated

by a rank-one probability tensor obtained via independent sampling.

Challenge 2: exponential failed actions. The number of possible multi-

actions |A|N increases exponentially as the number of agents N grows. While

this is not problematic if agents act relatively independently, it’s a signiﬁcant

obstacle when the agents are tightly coupled, i.e., when the success of agent Ai’s

action ai is highly dependent on the actions of the other agents. Just consider a

randomly initialized policy (the starting point of almost all RL problems): agents

stumble upon positive rewards with an extremely low probability which leads to

slow learning. We focus on small N , nonetheless, the proportion of coordinated

action tuples is small (9.5% when N = 2 and 2.1% when N = 3).

4 A cordial sync
To address the aforementioned two challenges we develop: (a) a novel action sampling procedure named Synchronize Your actioNs Coherently (SYNC) and (b) an intuitive & eﬀective multi-agent training loss named the Coordination Loss (CORDIAL). Addressing challenge 1: SYNC-policies. For readability, we consider N = 2 agents and illustrates an overview in Fig. 3. The joint probability tensor Πt is

8

U. Jain & L. Weihs et al.

hence a matrix of size |A| × |A|. Recall our goal: using little communication,

multiple agents should sample their actions from a high-rank joint policy. This

is diﬃcult as (i) little communication means that, except in degenerate cases,

no agent can form the full joint policy and (ii) even if all agents had access to

the joint policy it is not obvious how to ensure that the decentralized agents will

sample a valid coordinated action.

To achieve this note that, for any rank m ≤ |A| matrix L ∈ R|A|×|A|, there are

vectors v1, w1, . . . , vm, wm ∈ R|A| such that L =

m j=1

vj

⊗

wj .

Here,

⊗

denotes

the

outer

product.

Also,

the

non-negative

rank

of

a

matrix

L

∈

|A|×|A|
R≥0

equals

the smallest integer s such that L can be written as the sum of s non-negative

rank-one

matrices.

Furthermore,

a

non-negative

matrix

L

∈

|A|×|A|
R≥0

has

non-

negative rank bounded above by |A|. Since Πt is a |A|×|A| joint probability ma-

trix, i.e., Πt is non-negative and its entries sum to one, it has non-negative rank m ≤ |A|, i.e., there exist non-negative vectors α ∈ Rm ≥0 and p1, q1, . . . , pm, qm ∈

|A|
R≥0

whose

entries

sum

to

one

such

that

Πt

=

m j=1

αj

·

pj

⊗

qj .

We call a sum of the form

m j=1

αj

· pj

⊗ qj

a

mixture-of-marginals.

With

this

decomposition at hand, randomly sampling action pairs (a1, a2) from

m j=1

αj

·

pj ⊗ qj can be interpreted as a two step process: ﬁrst sample an index j ∼

Multinomial(α) and then sample a1 ∼ Multinomial(pj) and a2 ∼ Multinomial(qj).

This stage-wise procedure suggests a strategy for sampling actions in a multi-

agent setting, which we refer to as SYNC-policies. Generalizing to an N agent setup, suppose that agents (Ai)Ni=1 have access to a shared random stream of numbers. This can be accomplished if all agents share a random seed or if all

agents initially communicate their individual random seeds and sum them to

obtain a shared seed. Furthermore, suppose that all agents locally store a shared

function fθ : RK → ∆m−1 where θ are learnable parameters, K is the dimen-

sionality of all communication between the agents in a timestep, and ∆m−1 is

the standard (m − 1)-probability simplex. Finally, at time t suppose that each

agent Ai produces not a single policy πti but instead a collection of policies

π

i t,1

,

.

.

.

,

π

i t,m

.

Let

Ct

∈

RK

be

all

communication

sent

between

agents

at

time

t. Each agent Ai then samples its action as follows: (i) compute the shared

probabilities αt = fθ(Ct), (ii) sample an index j ∼ Multinomial(αt) using the shared random number stream, (iii) sample, independently, an action ai from

the policy πti,j. Since both fθ and the random number stream are shared, the quantities in (i) and (ii) are equal across all agents despite being computed in-

dividually. This sampling procedure is equivalent to sampling from the tensor

m j=1

αj

· πt1,j

⊗ . . . ⊗ πtN,j

which,

as

discussed

above,

may

have

rank

up

to

m.

Intuitively, SYNC enables decentralized agents to have a more expressive joint

policy by allowing them to agree upon a strategy by sampling from αt.

Addressing challenge 2: CORDIAL. We encourage agents to rapidly learn

to choose coordinated actions via a new loss. In particular, letting Πt be the

joint policy of our agents, we propose the coordination loss (CORDIAL)

CLβ(St, Πt) = −β · St, log(Πt) /

(St)ij ,

(1)

1≤i,j≤|A|

A Cordial Sync

9

where log is applied element-wise, ∗, ∗ is the usual Frobenius inner product,

and St is deﬁned in Sec. 3. Notice that CORDIAL encourages agents to have

a near uniform policy over the actions which are coordinated. We use this loss

to replace the standard entropy encouraging loss in policy gradient algorithms

(e.g., the A3C algorithm [66]). Similarly to the parameter for the entropy loss in

A3C, β is chosen to be a small positive constant so as to not overly discourage

learning.

Note that the coordination loss is less meaningful when Πt = π1 ⊗ · · · ⊗ πN , i.e., when Πt is rank-one. For instance, suppose that St has ones along

the diagonal, and zeros elsewhere, so that we wish to encourage the agents

to all take the same action. In this case it is straightforward to show that

CLβ(St, Πt) = −β

N i=1

M j=1

(1/M

)

log

πti

(aj

)

so

that

CLβ(St, Πt)

simply

en-

courages each agent to have a uniform distribution over its actions and thus

actually encourages the agents to place a large amount of probability mass on

uncoordinated actions. Indeed, Tab. 4 shows that using CORDIAL without

SYNC leads to poor results.

5 Models
We study four distinct policy types: central, marginal, marginal w/o comm, and SYNC . Central samples actions from a joint policy generated by a central agent with access to observations from all agents. While often unrealistic in practice due to communication bottlenecks, central serves as an informative baseline. Marginal follows prior work, e.g., [42]: each agent independently samples its actions from its individual policy after communication. Marginal w/o comm is identical to marginal but does not permit agents to communicate explicitly (agents may still see each other). Finally, SYNC is our newly proposed policy described in Sec. 4. For a fair comparison, all decentralized agents (i.e., SYNC , marginal, and marginal w/o comm), use the same TBONE backbone architecture from [42], see Fig. 3. We have ensured that parameters are fairly balanced so that our proposed SYNC has close to (and never more) parameters than the marginal and marginal w/o comm nets. Note, we train central and SYNC with CORDIAL, and the marginal and marginal w/o comm without it. This choice is mathematically explained in Sec. 4 and empirically validated in Sec. 6.3. Architecture details: For readability we describe the policy and value net for the 2 agent setup while noting that it can be trivially extended to any number of agents. As noted above, decentralized agents use the TBONE backbone from [42]. Our primary architectural novelty extends TBONE to SYNC-policies. An overview of the TBONE backbone and diﬀerences between sampling with marginal and SYNC policies is shown in Fig. 3.
As a brief summary of TBONE, agent i observes at time t inputs oit, i.e., a 3×84×84 RGB image returned from AI2-THOR which represents the i-th agent’s egocentric view. For each agent, the observation is encoded by a four layer CNN and combined with an agent speciﬁc learned embedding (that encodes the ID of that agent) along with the history embedding hit−1. The resulting vector is fed

10

U. Jain & L. Weihs et al.

into a long-short-term-memory (LSTM) [39] unit to produce a 512-dimensional embedding h˜it corresponding to the ith agent.
The agents then undergo two rounds of communication resulting in two ﬁnal hidden states h1t , h2t and messages cit,j ∈ R16, 1 ≤ i, j ≤ 2 with message cit,j being produced by agent i in round j and then sent to the other agent in that
round. In [42], the value of the agents’ state as well as logits corresponding to the policy of the agents are formed by applying linear functions to h1t , h2t .
We now show how SYNC can be integrated into TBONE to allow our agents
to represent high rank joint distributions over multi-actions (see Fig. 3). First
each agent computes the logits corresponding to αt. This is done using a 2-layer MLP applied to the messages sent between the agents, at the second stage. In particular, αt = W3 ReLU(W2 ReLU(W1 [c1t,2; c2t,2] + b1) + b2) + b3 where W1 ∈ R64×32, W2 ∈ R64×64, W3 ∈ Rm×64, b1 ∈ R32, b2 ∈ R64, and b3 ∈ Rm are a learnable collection of weight matrices and biases. After computing αt we compute a collection of policies πti,1, . . . , πti,m for i ∈ {1, 2}. Each of these policies is computed following the TBONE architecture but using m − 1 additional, and
learnable, linear layers per agent.

6 Experiments
6.1 Experimental setup
Simulator. We evaluate our models using the AI2-THOR environment [48] with several novel upgrades. First, we introduce new methods which permit to (a) randomly initialize the lifted object and agent locations close to each other and looking towards the lifted object, and (b) simultaneously move agents and the lifted object in a given direction with collision detection. Secondly, we build a top-down gridworld version of AI2-THOR for faster prototyping, that is 16× faster than [42]. For details about framework upgrades, see Sec. A.3 of the supplement. Tasks. We compare against baselines on FurnMove, Gridworld-FurnMove, and FurnLift [42]. FurnMove is the novel task introduced in this work (Sec. 3): agents observe egocentric visual views, with ﬁeld-of-view 90 degrees. In FurnMove-Gridworld the agents are provided a top-down egocentric 3D tensor as observations. The third dimension of the tensor contains semantic information such as, if the location is navigable by an agent or navigable by the lifted object, or whether the location is occupied by another agent, the lifted object, or the goal object. Hence, FurnMove-Gridworld agents do not need visual understanding, but face other challenges of the FurnMove task – coordinating actions and planning trajectories. We consider only the harder variant of FurnLift, where communication was shown to be most important (‘constrained’ with no implicit communication in [42]). In FurnLift, agents observe egocentric visual views. Data. As in [42], we train and evaluate on a split of the 30 living room scenes. As FurnMove is already quite challenging, we only consider a single piece of lifted

A Cordial Sync

11

furniture (a television) and a single goal object (a TV-stand). Twenty rooms are used for training, 5 for validation, and 5 for testing. The test scenes have very diﬀerent lighting conditions, furniture, and layouts. For evaluation our test set includes 1000 episodes equally distributed over the ﬁve scenes. Training. For training we augment the A3C algorithm [66] with CORDIAL. For our studies in the visual domain, we use 45 workers and 8 GPUs. Models take around two days to train. For more details about training, including hyperparameter values and the reward structure, see Sec. A.3 of the supplement.

6.2 Metrics
For completeness, we consider a variety of metrics. We adapt SPL, i.e., Success weighted by (normalized inverse) Path Length [2], so that it doesn’t require shortest paths but still provides similar semantic information4: We deﬁne a Manhattan Distance based SPL as MD-SPL = Ne−p1 Ni=e1p Si max(mpii,/mdgir/iddgrid) , where i denotes an index over episodes, Nep equals the number of test episodes, and Si is a binary indicator for success of episode i. Also pi is the number of actions taken per agent, mi is the Manhattan distance from the lifted object’s start location to the goal, and dgrid is the distance between adjacent grid points, for us 0.25m. We also report other metrics capturing complementary information. These include mean number of actions in an episode per agent (Ep len), success rate (Success), and distance to target at the end of the episode (Final dist).
We also introduce two metrics unique to coordinating actions: TVD, the mean total variation distance between Πt and its best rank-one approximation, and Invalid prob, the average probability mass allotted to uncoordinated actions, i.e., the dot product between 1 − St and Πt. By deﬁnition, TVD is zero for the marginal model, and higher values indicate divergence from independent marginal sampling. Note that, without measuring TVD we would have no way of knowing if our SYNC model was actually using the extra expressivity we’ve aﬀorded it. Lower Invalid prob values imply an improved ability to avoid uncoordination actions as detailed in Sec. 3 and Fig. 2.

6.3 Quantitative evaluation
We conduct four studies: (a) performance of diﬀerent methods and relative difﬁculty of the three tasks, (b) eﬀect of number of components on SYNC performance, (c) eﬀect of CORDIAL (ablation), and (d) eﬀect of number of agents. Comparing methods and tasks. We compare models detailed in Sec. 5 on tasks of varying diﬃculty, report metrics in Tab. 1, and show the progress of metrics over training episodes in Fig. 4. In our FurnMove experiments, SYNC performs better than the best performing method of [42] (i.e., marginal ) on all metrics. Success rate increases by 25.9% and 6.8% absolute percentage points on
4 For FurnMove, each location of the lifted furniture corresponds to 404, 480 states, making shortest path computation intractable (more details in Sec. A.4 of the supplement).

12

U. Jain & L. Weihs et al.

Table 1: Quantitative results on three tasks. ↑ (or ↓) indicates that higher (or

lower) value of the metric is desirable while denotes that the metric is simply

informational and no value is, a priori, better than another. †denotes that a

centralized agent serves only as an upper bound to decentralized methods and

cannot be fairly compared with. Note that, among decentralized agents, our

SYNC model has the best metric values across all reported metrics (bolded

values). Values are highlighted in green if their 95% conﬁdence interval has no

overlap with the conﬁdence intervals of other values

Methods

MD-SPL ↑ Success ↑ Ep len ↓ Final ↓ Invalid ↓ TVD

dist

prob.

FurnMove (ours)

Marginal w/o comm [42]

0.032

0.164

224.1

2.143

0.815

0

Marginal [42]

0.064

0.328

194.6

1.828

0.647

0

SYNC Central†

0.114 0.161

0.587 0.648

153.5 139.8

1.153 0.903

0.31 0.075

0.474 0.543

Gridworld-FurnMove (ours)

Marginal w/o comm [42]

0.111

0.484

172.6

1.525

0.73

0

Marginal [42]

0.218

0.694

120.1

0.960

0.399

0

SYNC Central†

0.228 0.323

0.762 0.818

110.4 87.7

0.711 0.611

0.275 0.039

0.429 0.347

Gridworld-FurnMove-3Agents (ours)

Marginal [42]

0

0

250.0

3.564

0.823

0

SYNC Central†

0.152 0.066

0.578 0.352

149.1 195.4

1.05 1.522

0.181 0.138

0.514 0.521

Table 2: Quantitative results on the FurnLift task. For legend, see

Methods

MD-SPL ↑ Success ↑ Ep len ↓ Final ↓ Invalid ↓ TVD

dist

prob.

Failed ↓ pickups

FurnLift [42] (‘constrained’ setting with no implicit communication)

Marginal w/o comm [42]

0.029

0.15

229.5

2.455

0.11

0

25.219

Marginal [42]

0.145

0.449

174.1

2.259

0.042

0

8.933

SYNC Central†

0.139 0.145

0.423

176.9

2.228

0

0.027

4.873

0.453

172.3

2.331

0

0.059

5.145

Tab. 1
Missed ↓ pickups
6.501 1.426 1.048 0.639

FurnMove and Gridworld-FurnMove respectively. Importantly, SYNC is signiﬁcantly better at allowing agents to coordinate their actions: for FurnMove, the joint policy of SYNC assigns, on average, 0.31 probability mass to invalid actions pairs while the marginal and marginal w/o comm models assign 0.647 and 0.815 probability mass to invalid action pairs. Additionally, SYNC goes beyond rank-one marginal methods by capturing a more expressive joint policy using the mixture of marginals. This is evidenced by the high TVD of 0.474 vs. 0 for marginal. In Gridworld-FurnMove, oracle-perception of a 2D gridworld helps raise performance of all methods, though the trends are similar. Tab. 2 shows similar trends for FurnLift but, perhaps surprisingly, the Success of SYNC is somewhat lower than the marginal model (2.6% lower, within statistical error). As is emphasized in [42] however, Success alone is a poor measure of model performance: equally important are the failed pickups and missed pickups metrics (for details, see Sec. A.4 of the supplement). For these metrics, SYNC outperforms the marginal model. That SYNC does not completely outperform marginal in FurnLift is intuitive, as FurnLift does not require continuous close coordination the beneﬁts of SYNC are less pronounced.

A Cordial Sync

13

Reached Target Successfully
Reached Target Successfully Find and Lift Success

1.0

Central

SYNC

0.8

Marginal

0.6 Marginal (w/o comm)

0.4

0.2

0.0 0

100 200 300 400 500 Training Episodes (Thousands)

(a) FurnMove

1.0 0.8 0.6 0.4 0.2

0

200

400

600

800

1000

Training Episodes (Thousands)

(b) Gridworld-FurnMove

1.0

0.8

0.6

0.4

0.2

0.0 0

20

40

60

80

Training Episodes (Thousands)

100

(c) FurnLift

Fig. 4: Success rate during training. Train (solid lines) and validation (dashed lines) performance of our agents for FurnMove, Gridworld-FurnMove, and FurnLift. 95% conﬁdence bars are included. For additional plots, see Sec. A.4 of the supplement

Table 3: Eﬀect of number of mixture components m on SYNC ’s performance

(in FurnMove). Generally, larger m means larger TVD values and better per-

formance.

Final Invalid K in SYNC MD-SPL ↑ Success ↑ Ep len ↓ dist ↓ prob. ↓ TVD

FurnMove

1 component

0.064

0.328

194.6 1.828 0.647

0

2 components 0.084

0.502

175.5 1.227 0.308 0.206

4 components 0.114

0.569

154.1 1.078 0.339 0.421

13 components 0.114

0.587

153.5 1.153

0.31

0.474

While the diﬃculty of a task is hard to quantify, we will consider the relative test-set metrics of agents on various tasks as an informative proxy. Replacing the complex egocentric vision in FurnMove with the semantic 2D gridworld in Gridworld-FurnMove, we see that all agents show large gains in Success and MD-SPL, suggesting that Gridworld-FurnMove is a dramatic simpliﬁcation of FurnMove. Comparing FurnMove to FurnLift is particularly interesting. The MD-SPL and Success metrics for the central agent do not provide a clear indication regarding task diﬃculty amongst the two. However, notice the much higher TVD for the central agent for FurnMove and the superior MD-SPL and Success of the Marginal agents for FurnLift. These numbers clearly indicate that FurnMove requires more coordination and additional expressivity of the joint distribution than FurnLift. Eﬀect of number of mixture components in SYNC. Recall (Sec. 4) that the number of mixture components m in SYNC is a hyperparameter controlling the maximal rank of the joint policy. SYNC with m = 1 is equivalent to marginal. In Tab. 3 we see T V D increase from 0.206 to 0.474 when increasing m from 2 to 13. This suggests that SYNC learns to use the additional expressivity. Moreover, we see that this increased expressivity results in better performance. A success rate jump of 17.4% from m = 1 to m = 2 demonstrates that substantial beneﬁts are obtained by even small increases in expressitivity. Moreover with more components, i.e., m = 4 & m = 13 we obtain more improvements. Notice

14

U. Jain & L. Weihs et al.

Table 4: Ablation study of coordination loss on marginal [42], SYNC , and central

methods. Marginal performs better without CORDIAL whereas SYNC and central show improvement with CORDIAL added to overall loss value. †denotes

that a centralized agent serve only as an upper bound to decentralized methods.

Final Invalid Method CORDIAL MD-SPL ↑ Success ↑ Ep len ↓ dist ↓ prob. ↓ TVD

Marginal



Marginal



SYNC



SYNC



Central†



Central†



0.064 0.015 0.091 0.114 0.14 0.161

FurnMove 0.328 0.099 0.488 0.587 0.609 0.648

194.6 236.9 170.3 153.5 146.9 139.8

1.828 2.134 1.458 1.153 1.018 0.903

0.647 0.492 0.47 0.31 0.155 0.075

0 0 0.36 0.474 0.6245 0.543

however that there are diminishing returns, the m = 4 model performs nearly as well as the m = 13 model. This suggests a trade-oﬀ between the beneﬁts of expressivity and the increasing complexities in optimization. Eﬀect of CORDIAL. In Tab. 4 we quantify the eﬀect of CORDIAL. Note the 9.9% improvement in success rate when adding CORDIAL to SYNC . This is accompanied by a drop in Invalid prob. from 0.47 to 0.31, which signiﬁes better coordination of actions. Similar improvements are seen for the central model. In ‘Challenge 2’ (Sec. 4) we mathematically laid out why marginal models gain little from CORDIAL. We substantiate this empirically with a 22.9% drop in success rate when training the marginal model with CORDIAL. Eﬀect of more agents. The ﬁnal three rows of Tab. 1 show the test-set performance of SYNC , marginal, and central models trained to accomplish a 3-agent variant of our Gridworld-FurnMove task. In this task the marginal model fails to train at all, achieving a 0% success rate. SYNC , on the other hand, successfully completes the task 57.8% of the time. Notice that SYNC ’s success rate drops by nearly 20 percentage points when moving from the 2- to the 3-agent variant of the task: clearly increasing the number of agents substantially increases the task’s diﬃcult. Surprisingly, the central model performs worse than SYNC in this setting. A discussion of this phenomenon is deferred to Sec. A.4 of the supplement.
6.4 Qualitative evaluation
We present three qualitative results on FurnMove: joint policy summaries, analysis of learnt communication, and visualizations of agent trajectories. Joint policy summaries. In Fig. 5 we show summaries of the joint policy captured by the central, SYNC , and marginal models. These matrices average over action steps in the test-set episodes for FurnMove. Other tasks show similar trends, see Sec. A.5 of the supplement. In Fig. 5a, the sub-matrices corresponding to AMWO and AMO are diagonal-dominant, indicating that agents are looking in the same direction (0◦ relative orientation in Fig. 2). Also note the high probability associated to (Pass, RotateX) and (RotateX, Pass),

A Cordial Sync

15

Avg. Prob.

Nav. actions

High

Low

RO actions

MWO actions

MO actions

a. Central (upper bound)

b. SYNC (ours)

c. Marginal (prior work)

Reply weights

Agent1 or Agent2 took a pass action

Agent1 or

Agent2

attempted a

Steps in episode →

MoveWithObject action

d. Communication analysis

of reply stage

Fig. 5: Qualitative results. (a,b,c) joint policy summary (Πt averaged over steps in test episodes in FurnMove) and (d) communication analysis.

within the ANAV block. Together, this means that the central method learns to coordinate single-agent navigational actions to rotate one of the agents (while the other holds the TV by executing Pass) until both face the same direction. They then execute the same action from AMO (AMWO) to move the lifted object. Comparing Fig. 5b vs. Fig. 5c, shows the eﬀect of CORDIAL. Recall that the marginal model doesn’t support CORDIAL and thus suﬀers by assigning probability to invalid action pairs (color outside the block-diagonal submatrices). Also note the banded nature of Fig. 5c resulting from its construction as an outer product of marginals. Communication analysis. A qualitative discussion of communication follows. Agent are colored red and green. We defer a quantitative treatment to Sec. A.5 of the supplement. As we apply SYNC on the TBONE backbone introduced by Jain et al . [42], we use similar tools to understand the communication emerging with SYNC policy heads. In line with [42], we plot the weight assigned by each agent to the ﬁrst communication symbol in the reply stage. Fig. 5d strongly suggests that the reply stage is directly used by the agents to coordinate the modality of actions they intend to take. In particular, note that a large weight being assigned to the ﬁrst reply symbol is consistently associated with the other agent taking a Pass action. Similarly, we see that small reply weights coincide with agents taking a MoveWithObject action. The talk weights’ interpretation is intertwined with the reply weights, and is deferred to Sec. A.5 of the supplement. Agent trajectories. Our supplementary video includes examples of policy rollouts. These clips include both agents’ egocentric views and a top-down trajectory visualization. This enables direct comparisons of marginal and SYNC on the same test episode. We also allow for hearing patterns in agents’ communication: we convert scalar weights (associated with reply symbols) to audio.
7 Conclusion
We introduce FurnMove, a collaborative, visual, multi-agent task requiring close coordination between agents and develop novel methods that allow for moving beyond existing marginal action sampling procedures, these methods lead to large gains across a diverse suite of metrics.

16

U. Jain & L. Weihs et al.

Acknowledgements: This material is based upon work supported in part by the National Science Foundation under Grants No. 1563727, 1718221, 1637479, 165205, 1703166, Samsung, 3M, Sloan Fellowship, NVIDIA Artiﬁcial Intelligence Lab, Allen Institute for AI, Amazon, and AWS Research Awards. UJ is thankful to Thomas & Stacey Siebel Foundation for Siebel Scholars Award. We thank Mitchell Wortsman and Kuo-Hao Zeng for their insightful suggestions on how to clarify and structure this work.

References
1. Abel, D., Agarwal, A., Diaz, F., Krishnamurthy, A., Schapire, R.E.: Exploratory gradient boosting for reinforcement learning in complex domains. arXiv preprint arXiv:1603.04119 (2016)
2. Anderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et al.: On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757 (2018)
3. Anderson, P., Shrivastava, A., Parikh, D., Batra, D., Lee, S.: Chasing ghosts: Instruction following as bayesian state tracking. In: NeurIPS (2019)
4. Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Su¨nderhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In: CVPR (2018)
5. Armeni, I., Sax, S., Zamir, A.R., Savarese, S.: Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105 (2017)
6. Aydemir, A., Pronobis, A., Gbelbecker, M., Jensfelt, P.: Active visual object search in unknown environments using uncertain semantics. In: IEEE Trans. on Robotics (2013)
7. Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mordatch, I.: Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528 (2019)
8. Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning environment: An evaluation platform for general agents. J. of Artiﬁcial Intelligence Research (2013)
9. Boutilier, C.: Sequential optimality and coordination in multiagent systems. In: IJCAI (1999)
10. Bratman, J., Shvartsman, M., Lewis, R.L., Singh, S.: A new approach to exploring language emergence as boundedly optimal control in the face of environmental and cognitive constraints. In: Proc. Int.’l Conv. on Cognitive Modeling (2010)
11. Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L., Strub, F., Rouat, J., Larochelle, H., Courville, A.: Home: A household multimodal environment. arXiv preprint arXiv:1711.11017 (2017)
12. Busoniu, L., Babuska, R., Schutter, B.D.: A comprehensive survey of multiagent reinforcement learning. In: IEEE Trans. on Systems, Man and Cybernetics (2008)
13. Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I., Leonard, J.J.: Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Trans. on Robotics (2016)
14. Canny, J.: The complexity of robot motion planning. MIT Press (1988) 15. Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song,
S., Zeng, A., Zhang, Y.: Matterport3D: Learning from RGB-D data in indoor environments. In: 3DV (2017)

A Cordial Sync

17

16. Chaplot, D.S., Gupta, S., Gupta, A., Salakhutdinov, R.: Learning to explore using active neural mapping. In: ICLR (2020)
17. Chen, B., Song, S., Lipson, H., Vondrick, C.: Visual hide and seek. arXiv preprint arXiv:1910.07882 (2019)
18. Chen, H., Suhr, A., Misra, D., Snavely, N., Artzi, Y.: Touchdown: Natural language navigation and spatial reasoning in visual street environments. In: CVPR (2019)
19. Chen∗, C., Jain∗, U., Schissler, C., Gari, S.V.A., Al-Halah, Z., Ithapu, V.K., Robinson, P., Grauman, K.: Audio-visual embodied navigation. arXiv preprint arXiv:1912.11474 (2019), ∗ equal contribution
20. Daftry, S., Bagnell, J.A., Hebert, M.: Learning transferable policies for monocular reactive mav control. In: Proc. ISER (2016)
21. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied Question Answering. In: CVPR (2018)
22. Das, A., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Neural Modular Control for Embodied Question Answering. In: ECCV (2018)
23. Das, A., Gervet, T., Romoﬀ, J., Batra, D., Parikh, D., Rabbat, M., Pineau, J.: Tarmac: Targeted multi-agent communication. In: ICML (2019)
24. Das∗, A., Carnevale∗, F., Merzic, H., Rimell, L., Schneider, R., Abramson, J., Hung, A., Ahuja, A., Clark, S., Wayne, G., et al.: Probing emergent semantics in predictive agents via question answering. In: ICML (2020), ∗ equal contribution
25. Dellaert, F., Seitz, S., Thorpe, C., Thrun, S.: Structure from Motion without Correspondence. In: CVPR (2000)
26. Elfes, A.: Using occupancy grids for mobile robot perception and navigation. Computer (1989)
27. Foerster, J.N., Assael, Y.M., de Freitas, N., Whiteson, S.: Learning to Communicate with Deep Multi-Agent Reinforcement Learning. In: NeurIPS (2016)
28. Foerster, J.N., Farquhar, G., Afouras, T., NArdelli, N., Whiteson, S.: Counterfactual Multi-Agent Policy Gradients. In: AAAI (2018)
29. Foerster, J.N., Nardelli, N., Farquhar, G., Torr, P.H.S., Kohli, P., Whiteson, S.: Stabilising experience replay for deep multi-agent reinforcement learning. In: ICML (2017)
30. Fraundorfer, F., Heng, L., Honegger, D., Lee, G.H., Meier, L., Tanskanen, P., Pollefeys, M.: Vision-based autonomous mapping and exploration using a quadrotor mav. In: IROS (2012)
31. Gao, R., Chen, C., Al-Halah, Z., Schissler, C., Grauman, K.: Visualechoes: Spatial image representation learning through echolocation. arXiv preprint arXiv:2005.01616 (2020)
32. Giles, C.L., Jim, K.C.: Learning communication for multi-agent systems. In: Proc. Innovative Concepts for Agent-Based Systems (2002)
33. Giusti, A., Guzzi, J., Cire¸san, D.C., He, F.L., Rodr´ıguez, J.P., Fontana, F., Faessler, M., Forster, C., Schmidhuber, J., Di Caro, G., et al.: A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters (2015)
34. Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: IQA: Visual Question Answering in Interactive Environments. In: CVPR (2018)
35. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social gan: Socially acceptable trajectories with generative adversarial networks. In: CVPR (2018)
36. Gupta, J.K., Egorov, M., Kochenderfer, M.: Cooperative Multi-Agent Control Using Deep Reinforcement Learning. In: AAMAS (2017)

18

U. Jain & L. Weihs et al.

37. Henriques, J.F., Vedaldi, A.: Mapnet: An allocentric spatial memory for mapping environments. In: CVPR (2018)
38. Hill, F., Hermann, K.M., Blunsom, P., Clark, S.: Understanding grounded language learning agents. arXiv preprint arXiv:1710.09867 (2017)
39. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation (1997)
40. Inc., W.R.: Mathematica, Version 12.1, https://www.wolfram.com/mathematica, champaign, IL, 2020
41. Jaderberg, M., Czarnecki, W.M., Dunning, I., Marris, L., Lever, G., Castaneda, A.G., Beattie, C., Rabinowitz, N.C., Morcos, A.S., Ruderman, A., et al.: Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science (2019)
42. Jain∗, U., Weihs∗, L., Kolve, E., Rastegari, M., Lazebnik, S., Farhadi, A., Schwing, A.G., Kembhavi, A.: Two body problem: Collaborative visual task completion. In: CVPR (2019), ∗ equal contribution
43. Johnson, M., Hofmann, K., Hutton, T., Bignell, D.: The malmo platform for artiﬁcial intelligence experimentation. In: IJCAI (2016)
44. Kahn, G., Zhang, T., Levine, S., Abbeel, P.: Plato: Policy learning using adaptive trajectory optimization. In: ICRA (2017)
45. Kasai, T., Tenmoto, H., Kamiya, A.: Learning of communication codes in multiagent reinforcement learning problem. In: Proc. IEEE Soft Computing in Industrial Applications (2008)
46. Kavraki, L.E., Svestka, P., Latombe, J.C., Overmars, M.H.: Probabilistic roadmaps for path planning in high-dimensional conﬁguration spaces. IEEE transactions on Robotics and Automation (1996)
47. Kempka, M., Wydmuch, M., Runc, G., Toczek, J., Jakowski, W.: Vizdoom: A doom-based ai research platform for visual reinforce- ment learning. In: Proc. IEEE Conf. on Computational Intelligence and Games (2016)
48. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR: an interactive 3d environment for visual AI. arXiv preprint arXiv:1712.05474 (2019)
49. Konolige, K., Bowman, J., Chen, J., Mihelich, P., Calonder, M., Lepetit, V., Fua, P.: View-based maps. Intl. J. of Robotics Research (2010)
50. Kuipers, B., Byun, Y.T.: A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Robotics and autonomous systems (1991)
51. Lauer, M., Riedmiller, M.: An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In: ICML (2000)
52. Lavalle, S.M., Kuﬀner, J.J.: Rapidly-exploring random trees: Progress and prospects. Algorithmic and Computational Robotics: New Directions (2000)
53. Lazaridou, A., Peysakhovich, A., Baroni, M.: Multi-agent cooperation and the emergence of (natural) language. In: arXiv preprint arXiv:1612.07182 (2016)
54. Lerer, A., Gross, S., Fergus, R.: Learning physical intuition of block towers by example. In: ICML (2016)
55. Liu, Y.C., Tian, J., Glaser, N., Kira, Z.: When2com: Multi-agent perception via communication graph grouping. In: CVPR (2020)
56. Liu, Y.C., Tian, J., Ma, C.Y., Glaser, N., Kuo, C.W., Kira, Z.: Who2com: Collaborative perception via learnable handshake communication. In: ICRA (2020)
57. Liu∗, I.J., Yeh∗, R., Schwing, A.G.: PIC: Permutation Invariant Critic for MultiAgent Deep Reinforcement Learning. In: CoRL (2019), ∗ equal contribution

A Cordial Sync

19

58. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I.: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. In: NeurIPS (2017)
59. Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: A Platform for Embodied AI Research. In: ICCV (2019)
60. Matignon, L., Laurent, G.J., Fort-Piat, N.L.: Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In: IROS (2007)
61. Melo, F.S., Spaan, M., Witwicki, S.J.: QueryPOMDP: POMDP-based communication in multiagent systems. In: Eurpoean Workshop on Multi-Agent Systems (2011)
62. Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., et al.: Learning to navigate in complex environments. In: ICLR (2017)
63. Mirowski, P., Banki-Horvath, A., Anderson, K., Teplyashin, D., Hermann, K.M., Malinowski, M., Grimes, M.K., Simonyan, K., Kavukcuoglu, K., Zisserman, A., et al.: The streetlearn environment and dataset. arXiv preprint arXiv:1903.01292 (2019)
64. Mirowski, P., Grimes, M., Malinowski, M., Hermann, K.M., Anderson, K., Teplyashin, D., Simonyan, K., Zisserman, A., Hadsell, R., et al.: Learning to navigate in cities without a map. In: NeurIPS (2018)
65. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D.: Human-level control through deep reinforcement learning. Nature (2015)
66. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In: ICML (2016)
67. Mordatch, I., Abbeel, P.: Emergence of Grounded Compositional Language in Multi-Agent Populations. In: AAAI (2018)
68. Oh, J., Chockalingam, V., Singh, S., Lee, H.: Control of memory, active perception, and action in minecraft. In: ICML (2016)
69. Omidshaﬁei, S., Pazis, J., Amato, C., How, J.P., Vian, J.: Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In: ICML (2017)
70. Panait, L., Luke, S.: Cooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems. In: AAMAS (2005)
71. Peng, P., Wen, Y., Yang, Y., Yuan, Q., Tang, Z., Long, H., Wang, J.: Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069 (2017)
72. R. C. Smith, R.C., Cheeseman, P.: On the representation and estimation of spatial uncertainty. Intl. J. Robotics Research (1986)
73. R Core Team: R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria (2019), https://www. R-project.org/
74. Ramakrishnan, S.K., Jayaraman, D., Grauman, K.: An exploration of embodied visual exploration. arXiv preprint arXiv:2001.02192 (2020)

20

U. Jain & L. Weihs et al.

75. Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., Whiteson, S.: Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In: ICML (2018)
76. Recht, B., Re, C., Wright, S., Niu, F.: Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In: NeurIPS (2011)
77. Ross, S., Gordon, G., Bagnell, D.: A reduction of imitation learning and structured prediction to no-regret online learning. In: AISTATS (2011)
78. Savinov, N., Dosovitskiy, A., Koltun, V.: Semi-parametric topological memory for navigation. In: ICLR (2018)
79. Savva, M., Chang, A.X., Dosovitskiy, A., Funkhouser, T., Koltun, V.: Minos: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931 (2017)
80. Schnberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: CVPR (2016) 81. Smith, R.C., Self, M., Cheeseman, P.: Estimating uncertain spatial relationships
in robotics. In: UAI (1986) 82. Suhr, A., Yan, C., Schluger, J., Yu, S., Khader, H., Mouallem, M., Zhang, I., Artzi,
Y.: Executing instructions in situated collaborative interactions. In: EMNLP (2019) 83. Sukhbaatar, S., Szlam, A., Fergus, R.: Learning multiagent communication with backpropagation. In: NeurIPS (2016) 84. Sukhbaatar, S., Szlam, A., Synnaeve, G., Chintala, S., Fergus, R.: Mazebase: A sandbox for learning from games. arXiv preprint arXiv:1511.07401 (2015) 85. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press (1998) 86. Tamar, A., Wu, Y., Thomas, G., Levine, S., Abbeel, P.: Value iteration networks. In: NeurIPS (2016) 87. Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., Vicente, R.: Multiagent cooperation and competition with deep reinforcement learning. In: PloS (2017) 88. Tan, M.: Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents. In: ICML (1993) 89. Tesauro, G.: Extending q-learning to general adaptive multi-agent systems. In: NeurIPS (2004) 90. Thomason, J., Gordon, D., Bisk, Y.: Shifting the baseline: Single modality performance on visual navigation & qa. In: NAACL (2019) 91. Tomasi, C., Kanade, T.: Shape and motion from image streams under orthography: a factorization method. IJCV (1992) 92. Toussaint, M.: Learning a world model and planning with a self-organizing, dynamic neural system. In: NeurIPS (2003) 93. Usunier, N., Synnaeve, G., Lin, Z., Chintala, S.: Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. In: ICLR (2016) 94. de Vries, H., Shuster, K., Batra, D., Parikh, D., Weston, J., Kiela, D.: Talk the walk: Navigating new york city through grounded dialogue. arXiv preprint arXiv:1807.03367 (2018) 95. Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L.: Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In: CVPR (2019) 96. Weihs, L., Kembhavi, A., Han, W., Herrasti, A., Kolve, E., Schwenk, D., Mottaghi, R., Farhadi, A.: Artiﬁcial agents learn ﬂexible visual representations by playing a hiding game. arXiv preprint arXiv:1912.08195 (2019)

A Cordial Sync

21

97. Wijmans, E., Datta, S., Maksymets, O., Das, A., Gkioxari, G., Lee, S., Essa, I., Parikh, D., Batra, D.: Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. In: CVPR (2019)
98. Wortsman, M., Ehsani, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In: CVPR (2019)
99. Wu, Y., Wu, Y., Tamar, A., Russell, S., Gkioxari, G., Tian, Y.: Bayesian relational memory for semantic visual navigation. ICCV (2019)
100. Wymann, B., Espi´e, E., Guionneau, C., Dimitrakakis, C., Coulom, R., Sumner, A.: Torcs, the open racing car simulator (2013), http://www.torcs.org
101. Xia, F., Shen, W.B., Li, C., Kasimbeg, P., Tchapmi, M., Toshev, A., Mart´ınMart´ın, R., Savarese, S.: Interactive gibson: A benchmark for interactive navigation in cluttered environments. arXiv preprint arXiv:1910.14442 (2019)
102. Xia, F., Zamir, A.R., He, Z., Sax, A., Malik, J., Savarese, S.: Gibson env: Realworld perception for embodied agents. In: CVPR (2018)
103. Yang, J., Lu, J., Lee, S., Batra, D., Parikh, D.: Visual curiosity: Learning to ask questions to learn visual recognition. In: CoRL (2018)
104. Yang, J., Ren, Z., Xu, M., Chen, X., Crandall, D., Parikh, D., Batra, D.: Embodied amodal recognition: Learning to move to perceive objects. In: ICCV (2019)
105. Yang, W., Wang, X., Farhadi, A., Gupta, A., Mottaghi, R.: Visual semantic navigation using scene priors. In: ICLR (2018)
106. Zhang, K., Yang, Z., Ba¸sar, T.: Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635 (2019)
107. Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A.: Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. In: ICRA (2017)

22

U. Jain & L. Weihs et al.

A Supplementary Material

This supplementary material provides:

A.1 The conditions for a collection of actions to be considered coordinated. A.2 An example showing that standard independent multi-agent action sampling
makes it impossible to, even in principle, obtain an optimal joint policy. A.3 Training details including hyperparameter choices, hardware conﬁgurations,
and reward structure. We also discuss our upgrades to AI2-THOR. A.4 Additional discussion, tables, and plots regarding our quantitative results. A.5 Additional discussion, tables, and plots of our qualitative results including
a description of our supplementary video as well as an in-depth quantitative evaluation of communication learned by our agents.

A.1 Action restrictions
We now comprehensively describe the restrictions deﬁning when actions taken by agents are globally consistent with one another. In the following we will, for readability, focus on the two agent setting. All conditions deﬁned here easily generalize to any number of agents. Recall the sets ANAV, AMWO, AMO, and ARO deﬁned in Sec. 3. We call these sets the modalities of action. Two actions a1, a2 ∈ A are said to be of the same modality if they both are an element of the same modality of action. Let a1 and a2 be the actions chosen by the two agents. Below we describe the conditions when a1 and a2 are considered coordinated. If the agents’ actions are uncoordinated, both actions fail and no action is taken for time t. These conditions are summarized in Fig. 2a. Same action modality. A ﬁrst necessary, but not suﬃcient, condition for successful coordination is that the agents agree on the modality of action to perform. Namely that both a1 and a2 are of the same action modality. Notice the block diagonal structure in Fig. 2a. No independent movement. Our second condition models the intuitive expectation that if one agent wishes to reposition itself by performing a singleagent navigational action, the other agent must remain stationary. Thus, if a1, a2 ∈ ANAV, then (a1, a2) are coordinated if and only if one of a1 or a2 is a Pass action. The {1, 2, 3, 4}2 entries of the matrix in Fig. 2a show coordinated pairs of single-agent navigational actions. Orientation synchronized object movement. Suppose that both agents wish to move (with) the object in a direction so that a1, a2 ∈ AMWO or a1, a2 ∈ AMO. Note that, as actions are taken from an egocentric perspective, it is possible, for example, that moving ahead from one agent’s perspective is the same as moving left from the other’s. This condition requires that the direction speciﬁed by both of the agents is consistent globally. Hence a1, a2 are coordinated if and only if the direction speciﬁed by both actions is the same in a global reference frame. For example, if both agents are facing the same direction this condition requires that a1 = a2 while if the second agent is rotated 90 degrees clockwise

A Cordial Sync

23

from the ﬁrst agent then a1 = MoveObjectAhead will be coordinated if and only if a2 = MoveObjectLeft. See the multicolored 4×4 blocks in Fig. 2a. Simultaneous object rotation. For the lifted object to be rotated, both agents must rotate it in the same direction in a global reference frame. As we only allow the agents to rotate the object in a single direction (clockwise) this means that a1 = RotateObjectRight requires a2 = a1. See the (9, 9) entry of the matrix in Fig. 2a.
While a pair of uncoordinated actions are always unsuccessful, it need not be true that a pair of coordinated actions is successful. A pair of coordinated actions will be unsuccessful in two cases: performing the action pair would result in (a) an agent, or the lifted object, colliding with one another or another object in the scene; or (b) an agent moving to a position more than 0.76m from the lifted object. Here (a) enforces the physical constraints of the environment while (b) makes the intuitive requirement that an agent has a ﬁnite reach and cannot lift an object when being far away.

A.2 Challenge 1 (rank-one joint policies) example
We now illustrate how requiring two agents to independently sample actions from marginal policies can result in failing to capture an optimal, high-rank, joint policy.
Consider two agents A1 and A2 who must work together to play rock-paperscissors (RPS) against some adversary E. In particular, our game takes place in a single timestep where each agent Ai, after perhaps communicating with the other agent, must choose some action ai ∈ A = {R, P, S}. During this time the adversary also chooses some action aE ∈ A. Now, in our game, the pair of agents A1, A2 lose if they choose diﬀerent actions (i.e., a1 = a2), tie with the adversary if all players choose the same action (i.e., a1 = a2 = aE), and ﬁnally win or lose if they jointly choose an action that beats or losses against the adversary’s choice following the normal rules of RPS (i.e., win if (a1, a2, aE) ∈ {(R, R, S), (P, P, R), (S, S, P )}, lose if (a1, a2, aE) ∈ {(S, S, R), (R, R, P ), (P, P, S)}).
Moreover, we consider the challenging setting where A1, A2 communicate in the open so that the adversary can view their joint policy Π before choosing the action it wishes to take. Notice that we’ve dropped the t subscript on Π as there is only a single timestep. Finally, we treat this game as zero-sum so that our agents obtain a reward of 1 for victory, 0 for a tie, and -1 for a loss. We refer to the optimal joint policy as Π∗. If the agents operate in a decentralized manner using their own (single) marginal policies, their eﬀective rank-one joint policy equals Π = π1 ⊗ π2. Optimal joint policy: It is well known, and easy to show, that the optimal joint policy equals Π∗ = I3/3, where I3 is the identity matrix of size 3. Hence, the agents take multi-action (R, R), (P, P ), or (S, S) with equal probability obtaining an expected reward of zero. Optimal rank-one joint policy: Π∗ (the optimal joint policy) is of rank three and thus cannot be captured by Π (an outer product of marginals). Instead, brute-force symbolic minimization, using Mathematica [40], shows that

24

U. Jain & L. Weihs et al.

Agent 2 Agent 1

CNN

MLP

LSTM

MLP

Joint

... 𝜋central

Fig. 6: Central model architecture. The central backbone observes the aggregate of all agents’ observations. Moreover, the actor in the central model explicitly captures the joint policy distribution.

an optimal strategy for A1 and A2 is to let π1 = π2 with

√

π1(R) = 2 − 2 ≈ 0.586,

(2)

π1(P ) = 0, and

(3)

π1(S) = 1 − π1(R) ≈ 0.414.

(4)

√ The expected reward from this strategy is 5 − 4 2 ≈ −.657, far less than the optimal expected reward of 0.

A.3 Training details
Centralized agent. Fig. 6 provides an overview of the architecture of the centralized agent. The ﬁnal joint policy is constructed using a single linear layer applied to a hidden state. As this architecture varies slightly when changing the number of agents and the environment (i.e., AI2-THOR or our gridworld variant of AI2-THOR) we direct anyone interested in exact replication to our codebase.

AI2-THOR upgrades. As we described in Sec. 6 we have made several upgrades to AI2-THOR in order to make it possible to run our FurnMove task. These upgrades are described below. Implementing FurnMove methods in AI2-THOR’s Unity codebase. The AI2-THOR simulator has been built using C# in Unity. While multi-agent support exists in AI2-THOR, our FurnMove task required implementing a collection of new methods to support randomly initializing our task and moving agents in tandem with the lifted object. Initialization is accomplished by a randomized search procedure that ﬁrst ﬁnds locations in which the lifted television can be placed and then determines if the agents can be situated around the lifted object so that they are suﬃciently close to the lifted object and looking at it. Implementing the joint movement actions (recall AMWO) required checking that all agents and objects can be moved along straight-line paths without encountering collisions. Top-down Gridworld Mirroring AI2-THOR. To enable fast prototyping and comparisons between diﬀering input modalities, we built an eﬃcient gridworld mirroring AI2-THOR. See Fig. 7 for a side-by-side comparison of AI2-

Agent 1 (egocentric view)

Agent 2 (egocentric view)

A Cordial Sync

25

Top-Down View (not available to agents)

Visual AI2-THOR

New Gridwold Mirroring AI2-THOR

Agent 2 sees agent 1
ahead and to the right

Goal

Agent 2 Agent 1

TV
Locations within 0.76m of TV
Agents can walk on grey areas
Outlines indicates where the TV can be moved (and in what
orientation)

Fig. 7: Directly comparing visual AI2-THOR with our gridworld. The same scene with identical agent, TV, and TV-stand, positions in AI2-THOR and our gridworld mirroring AI2-THOR. Gridworld agents receive clean, taskrelevant, information directly from the environment while, in AI2-THOR, agents must infer this information from complex images.

THOR and our gridworld. This gridworld was implemented primarily in Python with careful caching of data returned from AI2-THOR.
Reward structure. Rewards are provided to each agent individually at every step. These rewards include: (a) +1 whenever the lifted object is moved closer, in Euclidean distance, to the goal object than it had been previously in the episode, (b) a constant −0.01 step penalty to encourage short trajectories, and (c) a penalty of −0.02 whenever the agents action fails. The minimum total reward achievable for a single agent is −7.5 corresponding to making only failed actions, while the maximum total reward equals 0.99 · d where d is the total number of steps it would take to move the lifted furniture directly to the goal avoiding all obstructions. Our models are trained to maximize the expected discounted cumulative gain with discounting factor γ = 0.99.
Optimization and learning hyperparameters. For all tasks, we train our agents using reinforcement learning, particularly the popular A3C algorithm [66]. For FurnLift, we follow [42] and additionally use a warm start via imitation learning (DAgger [77]). When we deploy the coordination loss (CORDIAL), we modify the A3C algorithm by replacing the entropy loss with the coordination

26

U. Jain & L. Weihs et al.

loss CORDIAL deﬁned in Eq. (1). In our experiments we anneal the β parameter from a starting value of β = 1 to a ﬁnal value of β = 0.01 over the ﬁrst 5000 episodes of training. We use an ADAM optimizer with a learning rate of 10−4, momentum parameters of 0.9 and 0.999, with optimizer statistics shared across processes. Gradient updates are performed in an unsynchronized fashion using a HogWild! style approach [76]. Each episode has a maximum length of 250 total steps per agent. Task-wise details follow:

– FurnMove: Visual agents for FurnMove are trained for 500, 000 episodes, across 8 TITAN V or TITAN X GPUs with 45 workers and take approximately three days to train.
– Gridworld-FurnMove: Agents for Gridworld-FurnMove are trained for 1,000,000 episodes using 45 workers. Apart from parsing and caching the scene once, gridworld agents do not need to render images. Hence, we train the agents with only 1 G4 GPU, particularly the g4dn.16xlarge virtual machine on AWS. Agents (i.e., two) for Gridworld-FurnMove take approximately 1 day to train.
– Gridworld-FurnMove-3Agents: Same implementation as above, except that agents (i.e., three) for Gridworld-FurnMove-3Agents take approximately 3 days to train. This is due to an increase in the number of forward and backward passes and a CPU bottleneck. Due to the action space blowing up to |A| × |A| × |A| = 2197 (vs. 169 for two agents), positive rewards become increasingly sparse. This leads to grave ineﬃciency in training, with no learning for ∼500k episodes. To overcome this, we double the positive rewards for the RL formulation for all methods within the three agent setup.
– FurnLift: We adhere to the exact training procedure laid out by Jain et al . [42]. Visual agents for FurnLift are trained for 100,000 episodes with the ﬁrst 10,000 being warm started with a DAgger-styled imitation learning. Reinforcement learning (A3C) takes over after the warm-start period.

Integration with other MARL methods. As mentioned in Sec. 2, our con-
tributions are orthogonal to the RL method deployed. Here we give some pointers
for integration with a deep Q-Learning and a policy gradient method. QMIX. While we focus on policy-gradients and QMIX [75] uses Q-learning, we
can formulate a SYNC for Q-Learning (and QMIX). Analogous to an actor with
multiple policies, consider a value head where each agent’s Q-function Qi is replaced by a collection of Q-functions Qai for a ∈ A. Action sampling is done stagewise, i.e. agents jointly pick a strategy as arg maxa QSY NC (communications, a), and then individually choose action arg maxui Qai (τ i, ui). These Qai in turn can incorporated into the QMIX mixing network. COMA/MADDPG. Both these policy gradient algorithms utilize a central-
ized critic. Since our contributions focus on the actor head, we can directly
replace their per-agent policy with our SYNC policies and thus beneﬁt directly
from the counterfactual baseline in COMA [28] or the centralized critic in MAD-
DPG [58].

A Cordial Sync

27

A.4 Quantitative evaluation details
Conﬁdence intervals for metrics reported. In the main paper, we mentioned that we mark the best performing decentralized method in bold and highlight it in green if it has non-overlapping 95% conﬁdence intervals. In this supplement, particularly in Tab. 5, Tab. 6, Tab. 7, and Tab. 8 we include the 95% conﬁdence intervals for the metrics reported in Tab. 1, Tab. 2, Tab. 3, and Tab. 4.

Hypotheses on 3-agent central method performance. In Fig. 1 and Sec. 6.3 of the main paper, we mention that the central method performs worse than SYNC for the Gridworld-FurnMove-3Agent task. We hypothesize that this is because the central method for the -3Agent setup is signiﬁcantly slower as its actor head has dramatically more parameters requiring more time to train. In numbers – the central ’s actor head alone has D × |A|3 parameters, where D is the dimensionality of the ﬁnal representation fed into the actor (please see Fig. 6 for central ’s architecture). Note, D = 512 for our architecture means the central ’s actor head has 512 · 133 =1,124,864 parameters. Contrast this to SYNC ’s D × |A| × K parameters for a K mixture component. Even for the highest K in the mixture component study (Tab. 3), i.e., K = 13, this value is 86, 528 parameters. Such a large number of parameters makes learning with the central agent slow even after 1M episodes (this is already 10× more training episodes than used in [42]).

Why MD-SPL instead of SPL? SPL was introduced in [2] for evaluating single-agent navigational agents, and is deﬁned as follows:

Nep

1

li

SPL = Nep Si max(xi, li) , (5)

i=1

where i denotes an index over episodes, Nep equals the number of test episodes, and Si is a binary indicator for success of episode i. Also xi is the length of the agent’s path and li is the shortest-path distance from agent’s start location to the goal. Directly adopting SPL isn’t pragmatic for two reasons:

(a) Coordinating actions at every timestep is critical to this multi-agent task. Therefore, the number of actions taken by agents instead of distance (say in meters) should be incorporated in the metric.
(b) Shortest-path distance has been calculated for two agent systems for FurnLift [42] by ﬁnding the shortest path for each agent in a state graph. This can be done eﬀectively for fairly independent agents. While each position of the agent corresponds to 4 states (if 4 rotations are possible), each position of the furniture object corresponds to

# States = (#pos. for A1 near obj) × (#pos. for A2 near obj) (6) × (#rot. for obj) × (#rot. for A1) × (#rot. for A2),

28

U. Jain & L. Weihs et al.

This leads to 404,480 states for an agent-object-agent assembly. We found the shortest path algorithm to be intractable in a state graph of this magnitude. Hence we resort to the closest approximation of Manhattan distance from the object’s start position to the goal’s position. This is the shortest path, if there were no obstacles for navigation.

Minimal edits to resolve the above two problems lead us to using actions instead of distance, and leveraging Manhattan distance instead of shortest-path distance. This leads us to deﬁning, as described in Section Sec. 6.2 of the main paper, the Manhattan distance based SPL (MDSPL) as the quantity

1 Nep

mi/dgrid

MDSPL = Nep Si max(pi, mi/dgrid) . (7)

i=1

Deﬁning additional metrics used for FurnLift. Jain et al . [42] use two metrics which they refer to as failed pickups (picked up, but not ‘pickupable’) and missed pickups (‘pickupable’ but not picked up). ‘Pickupable’ means when the object and agent conﬁgurations were valid for a PickUp action.

Plots for additional metrics. See Fig. 8, 9, and 10 for plots of additional metric recorded during training for the FurnMove, Gridworld-FurnMove, and FurnLift tasks. Fig. 10 in particular shows how the failed pickups and missed pickups metrics described above are substantially improved when using our SYNC models.

Additional 3-agent experiments. In the main paper we present results when training SYNC , marginal, and central models to complete the 3-agent Gridworld-FurnMove task. We have also trained the same methods to complete the (visual) 3-agent FurnMove task. Rendering and simulating 3-agent interactions in AI2-THOR is computationally taxing. For this reason we trained our SYNC and central models for 300k episodes instead of the 500k episodes we used when training 2-agent models. As it showed no training progress, we also stopped the marginal model’s training after 100k episodes. Training until 300k episodes took approximately four days using eight 12GB GPUs (∼ 768 GPU hours per model).
After training, the SYNC , marginal, and central obtained a test-set success rate of 23.2 ± 2.6%, 0.0 ± 0.0%, and 12.4 ± 2.0% respectively. These results mirror those of the 3-agent Gridworld-FurnMove task from the main paper. Particularly, both the SYNC and central models train to reasonable success rates but the central model actually performs worse than the SYNC model. A discussion of our hypothesis for why this is the case can be found earlier in this section. In terms of our other illustrative metrics, our SYNC , marginal, and central respectively obtain MDSPL values of 0.029, 0.0, and 0.012, and Invalid prob values of 0.336, 0.854, and 0.132.

A Cordial Sync

29

Fig. 8: Metrics recorded during training for the FurnMove task for various models. These plots add to the graph shown in Fig. 4a. Here solid lines indicate performance on the training set and dashed lines the performance on the validation set. For the Invalid prob and TVD metrics, only training set values are shown. For the TVD metric the black line (corresponding to the Marginal (w/o comm) model completely covers the green line corresponding to the Marginal model.
A.5 Qualitative evaluation details and a statistical analysis of learned communication
Discussion of our qualitative video. We include a video of policy roll-outs in the supplementary material. This includes four clips, each corresponding to the rollout on a test scene of one of our models trained to complete the FurnMove task. Clip A. Marginal agents attempt to move the TV to the goal but get stuck in a narrow corridor as they struggle to successfully coordinate their actions. The episode is considered a failure as the agents do not reach the goal in the allotted 250 timesteps. A top-down summary of this trajectory is included in Fig. 11. Clip B. Unlike the marginal agents from Clip A., in this clip two SYNC agents successfully coordinate actions and move the TV to the goal location in 186 steps. A top-down summary of this trajectory is included in Fig. 12. Clip C. Here we show SYNC agents completing the Gridworld-FurnMove in a test scene (the same scene and initial starting positions as in Clip A and Clip B). The agents complete the task in 148 timesteps even after an initial search in the incorrect direction. Clip D (contains audio). This clip is an attempt to experience what agents ‘hear.’ The video for this clip is the same as Clip B showing the SYNC method. The audio is a rendering of the communication between agents in the reply stage. Particularly, we discretize the [0, 1] value associated with the ﬁrst reply

30

U. Jain & L. Weihs et al.

Fig. 9: Metrics recorded during training for the Gridworld-FurnMove task for various models. These plots add to the graph shown in Fig. 4b. Here solid lines indicate performance on the training set and dashed lines the performance on the validation set. For the Invalid prob and TVD metrics, only training set values are shown. For the TVD metric the black line (corresponding to the Marginal (w/o comm) model completely covers the green line corresponding to the Marginal model.
weight of each agent into 128 evenly spaced bins corresponding to the 128 notes on a MIDI keyboard (0 corresponding to a frequency of ∼8.18 Hz and 127 to ∼12500 Hz). Next, we post-process the audio so that the communication from the agents is played on diﬀerent channels (stereo) and has the Tech Bass tonal quality. As a result, the reader can experience what agent 1 hears (i.e., agent 2’s reply weight) via the left earphone/speaker and what agent 2 hears (i.e., agent 1’s reply weight) via the right speaker. In addition to the study in Sec. 6.4 and Sec. A.5, we notice a higher pitch/frequency for the agent which is passing. We also notice lower pitches for MoveWithObject and MoveObject actions.
Joint policy summaries. These provide a way to visualize the eﬀective joint distribution that each method captures. For each episode in the test set, we log each multi-action attempted by a method. We average over steps in the episode to obtain a matrix (which sums to one). Afterwards, we average these matrices (one for each episode) to create a joint policy summary of the method for the entire test set. This two-staged averaging prevents the snapshot from being skewed towards actions enacted in longer (failed or challenging) episodes. In the main paper, we included snapshots for FurnMove in Fig. 5. In Fig. 13 we include additional visualizations for all methods including (Marginal w/o comm model) for FurnMove and Gridworld-FurnMove.

A Cordial Sync

31

Fig. 10: Metrics recorded during training for the FurnLift task for various models. These plots add to the graph shown in Fig. 4c. Notice that we have included plots corresponding to the failed pickups (picked up, but not ‘pickupable’) and missed pickups (‘pickupable’ but not picked up) metrics described in Sec. A.4. Solid lines indicate performance on the training set and dashed lines the performance on the validation set. For the Invalid prob and TVD metrics, only training set values are shown. For the TVD metric the black line (corresponding to the Marginal (w/o comm) model completely covers the green line corresponding to the Marginal model.
Communication analysis. As shown in Fig. 5d and discussed in Sec. 6.4, there is very strong qualitative evidence suggesting that our agents use their talk and reply communication channels to explicitly relay their intentions and coordinate their actions. We now produce a statistical, quantitative, evaluation of this phenomenon by ﬁtting multiple logistic regression models where we attempt to predict, from the agents communications, certain aspects of their environment as well as their future actions. In particular, we run 1000 episodes on our test set using our mixture model in the visual testbed. This produces a dataset of 159,380 observations where each observation records, for a single step by both agents at time t:
(a) The two weights p1talk,t, p2talk,t where pitalk,t is the weight agent Ai assigns to the ﬁrst symbol in the “talk” vocabulary.

32

U. Jain & L. Weihs et al.

Table 5: 95% conﬁdence intervals included in addition to Tab. 1, evaluating

methods on FurnMove, Gridworld-FurnMove, and Gridworld-FurnMove-

3Agents. For legend details, see Tab. 1.

Methods

MD-SPL ↑ Success ↑ Ep len ↓

Final ↓ dist

Invalid ↓ prob.

TVD

FurnMove (ours)

Marginal w/o comm [42]

0.032 (±0.007)

0.164 (±0.023)

224.1 (±2.031)

2.143 (±0.104)

0.815 (±0.005)

0 (±0)

Marginal [42]

0.064 (±0.008)

0.328 (±0.029)

194.6 (±2.693)

1.828 (±0.105)

0.647 (±0.010)

0 (±0)

SYNC

0.114 (±0.009)

0.587 (±0.031)

153.5 (±2.870)

1.153 (±0.089)

0.31 (±0.004)

0.474 (±0.005)

Central†

0.161 (±0.012)

0.648 (±0.030)

139.8 (±2.958)

0.903 (±0.076)

0.075 (±0.006)

0.543 (±0.006)

Gridworld-FurnMove (ours)

Marginal w/o comm [42]

0.111 (±0.012)

0.484 (±0.031)

172.6 (±2.825)

1.525 (±0.121)

0.73 (±0.008)

0 (±0)

Marginal [42]

0.218 (±0.015)

0.694 (±0.029)

120.1 (±2.974)

0.960 (±0.100)

0.399 (±0.011)

0 (±0)

SYNC

0.228 (±0.014)

0.762 (±0.026)

110.4 (±2.832)

0.711 (±0.076)

0.275 (±0.005)

0.429 (±0.005)

Central†

0.323 (±0.016)

0.818 (±0.024)

87.7 (±2.729)

0.611 (±0.067)

0.039 (±0.004)

0.347 (±0.006)

Gridworld-FurnMove-3Agents (ours)

Marginal [42]

0 (±0)

0 (±0)

250.0 (±0)

3.564 (±0.111)

0.823 (±0)

0 (±0)

SYNC

0.152 (±0.012)

0.578 (±0.031)

149.1 (±6.020)

1.05 (±0.091)

0.181 (±0.006)

0.514 (±0.009)

Central†

0.066 (±0.008)

0.352 (±0.03)

195.4 (±5.200)

1.522 (±0.099)

0.138 (±0.005)

0.521 (±0.006)

(b) The two weights p1reply,t, p2reply,t where pireply,t is the weight agent Ai assigns to the ﬁrst symbol in the “reply” vocabulary.
(c) The two values tvit ∈ {0, 1} where tvit equals 1 if and only if agent Ai sees the TV at timestep t (before taking its action).
(d) The two values WillPassit ∈ {0, 1} where WillPassit equals 1 if and only if agent i ends up choosing to take the Pass action at time t (i.e., after
ﬁnishing communication). (e) The two values WillMWOit ∈ {0, 1} where WillMWOit equals 1 if and only if
agent i ends up choosing to take some MoveWithObject action at time t.

In the following we will drop the subscript t and consider the above quantities
as random samples drawn from the distribution of possible steps taken by our agents in randomly initialized trajectories. As A1 and A2 share almost all of their
parameters they are, essentially, interchangeable. Because of this our following analysis will be solely taking the perspective of agent A1, similar results hold for A2. We consider ﬁtting the three models:

σ−1P (tv1t = 1) = βtv + βt1alk, tv · p1talk

(8)

+ βr1eply, tv · p1talk,

+ βt1alk*reply, tv · p1talk · p1reply,

A Cordial Sync

33

Table 6: 95% conﬁdence intervals included in addition to Tab. 2, evaluating

methods on FurnLift. Marginal and SYNC perform equally well, and mostly

lie within conﬁdence intervals of each other. Invalid prob. and failed pickups

metrics for SYNC have non-overlapping conﬁdence bounds (lighted in green).

For more details on the legend, see Tab. 1.

Methods

MD-SPL ↑ Success ↑ Ep len ↓ Final ↓ Invalid ↓ TVD

dist

prob.

Failed ↓ Missed ↓ pickups pickups

FurnLift [42] (‘constrained’ setting with no implicit communication)

Marginal w/o comm [42] 0.029 (±0.007)

0.15

229.5 2.455

0.11

(±0.022) (±3.482) (±0.105) (±0.004)

0 (±0)

25.219 (±1.001)

6.501 (±0.784)

Marginal [42]

0.145 (±0.016)

0.449 174.1 2.259 0.042 (±0.031) (±5.934) (±0.094) (±0.003)

0 (±0)

8.933 (±0.867)

1.426 (±0.284)

SYNC

0.139 (±0.016)

0.423

176.9 2.228

(±0.031) (±5.939) (±0.083)

0 (±0)

0.027 4.873 (±0.002) (±0.453)

1.048 (±0.192)

Central†

0.145 (±0.016)

0.453

172.3 2.331

(±0.031) (±5.954) (±0.088)

0 (±0)

0.059 (±0.002)

5.145 (±0.5)

0.639 (±0.164)

Table 7: 95% conﬁdence intervals included in addition to Tab. 3 by varying

number of components in SYNC-policies for FurnMove.

Final Invalid K in SYNC MD-SPL ↑ Success ↑ Ep len ↓ dist ↓ prob. ↓ TVD

1 component 2 components 4 components 13 components

0.064 (±0.004)
0.084 (±0.008)
0.114 (±0.009)
0.114 (±0.009)

FurnMove

0.328

194.6

(±0.019) (±2.833)

0.502

175.5

(±0.031) (±5.321)

0.569

154.1

(±0.031) (±5.783)

0.587 153.5

(±0.031) (±5.739)

1.828 (±0.105)
1.227 (±0.091)
1.078 (±0.083)
1.153 (±0.089)

0.647 (±0.002)
0.308 (±0.004)
0.339 (±0.004)
0.31 (±0.004)

0 (±0) 0.206 (±0.004) 0.421 (±0.005) 0.474 (±0.005)

σ−1P (WillPass1 = 1) = βpass

(9)

2
+ βtialk, pass · pitalk
i=1

2
+ βrieply, pass · pireply, and
i=1

σ−1P (WillMWO1 = 1) = βMWO
2
+ βtialk, MWO · pitalk
i=1
2
+ βrieply, MWO · pireply,
i=1

(10)

where σ is the usual logistic function. Here Eq. (8) attempts to determine the relationship between what A1 communicates and whether or not A1 is currently

34

U. Jain & L. Weihs et al.

Table 8: 95% conﬁdence intervals included in addition to Tab. 4, ablating coordination loss on marginal [42], SYNC , and central methods. †denotes that a

centralized agent serve only as an upper bound to decentralized methods.

Final Invalid Method CORDIAL MD-SPL ↑ Success ↑ Ep len ↓ dist ↓ prob. ↓ TVD

Marginal



0.064 (±0.008)

FurnMove

0.328

194.6

(±0.029) (±5.385)

1.828 (±0.105)

0.647 (±0.01)

0 (±0.0)

Marginal



0.015 (±0.004)

0.099

236.9

2.134

0.492

(±0.019) (±2.833) (±0.105) (±0.002)

0 (±0.0)

SYNC



0.091

0.488

170.3

1.458

0.47

0.36

(±0.008) (±0.031) (±5.665) (±0.104) (±0.008) (±0.008)

SYNC



0.114

0.587

153.5

1.153

0.31

0.474

(±0.009) (±0.031) (±5.739) (±0.089) (±0.004) (±0.005)

Central†



0.14 (±0.011)

0.609 (±0.03)

146.9

1.018

0.155 0.6245

(±5.895) (±0.084) (±0.006) (±0.005)

Central†



0.161 (±0.012)

0.648 (±0.03)

139.8

0.903

0.075

0.543

(±5.915) (±0.076) (±0.006) (±0.006)

Episode start

Progress after 83 steps

Progress after 166 steps

Progress after 250 steps (failure)

Fig. 11: Clip A trajectory summary. The marginal agents quickly get stuck in a narrow area between a sofa and the wall and fail to make progress.

seeing the TV, Eq. (9) probes whether or not any communication symbol is associated with A1 choosing to take a Pass action, and ﬁnally Eq. (10) considers whether or not A1 will choose to take a MoveWithObject action. We ﬁt each
of the above models using the glm function in the R programming language
[73]. Moreover, we compute conﬁdence intervals for our coeﬃcient values using
a robust bootstrap procedure. Fitted parameter values can be found in Tab. 9.
From Tab. 9 we draw several conclusions. First, in our dataset, there is a somewhat complex association between agent A1 seeing the TV and the communication symbols it sends. In particular, for a ﬁxed reply weight p1reply < 0.821, a larger value of p1talk is associated with higher odds of the TV being visible to A1 but if p1reply > 0.821 then larger values of p1talk are associated with smaller odds of the TV being visible. When considering whether or not A1 will pass, the table shows that this decision is strongly associated with the value of p2reply

A Cordial Sync

35

Episode start

Progress after 62 steps

Progress after 124 steps

Progress after 186 steps (success)

Fig. 12: Clip B trajectory summary. The SYNC agents successfully navigate the TV to the goal location without getting stuck in the narrow corridor.

Table 9: Estimates, and corresponding robust bootstrap standard errors, for the

parameters of communication analysis (Sec. A.5).

βtv

βt1alk, tv βr1eply, tv βt1alk*reply, tv

-

Est. -2.62 6.93

3.35

-8.44

-

SE 0.33 0.52

0.38

0.62

-

βpass Est. -7.55

βt1alk, pass 2.69

βt2alk, pass -2.2

βr1eply, pass -1.72

βr2eply, pass 9.98

SE 0.09 0.09

0.08

0.07

0.11

βMWO βt1alk, MWO βt2alk, MWO βr1eply, MWO βr2eply, MWO

Est. 2.71 0.39

0.28

-3.34

-3.37

SE 0.05 0.06

0.06

0.06

0.06

where, given ﬁxed values for the other talk and reply weights, p2reply being larger by a unit of 0.1 is associated with 2.7× larger odds of A1 taking the pass action. This suggests the interpretation of a large value of p2reply as A2 communicating that it wishes A1 to pass so that A1 may perform a single-agent navigation
action to reposition itself. Finally, when considering the ﬁtted values correspond-
ing to Eq. (10) we see that while the talk symbols communicated by the agents are weakly related with whether or not A1 takes a MoveWithObject action,
the reply symbols are associated with coeﬃcients with an order of magnitude
larger values. In particular, assuming all other communication values are ﬁxed, a smaller value of either p1reply or p2reply is associated with substantially larger odds of A1 choosing a MoveWithObject action. This suggests interpreting an especially small value of pireply as agent Ai indicating its readiness to move the object.

36

U. Jain & L. Weihs et al.

Nav. actions

FurnMove

MWO actions

RO actions MO actions

(a) Central

(b) SYNC

(c) Marginal

FurnMove-Gridworld

(d) Marginal w/o comm

Avg. Prob.
High Low
Avg. Prob.
High Low

(a) Central

(b) SYNC

(c) Marginal

(d) Marginal w/o comm

Fig. 13: Additional results for Fig. 5. Joint policy summaries for all methods for both FurnMove and Gridworld-FurnMove.

