EARLY FUSION for Goal Directed Robotic Vision

Aaron Walsman1 Yonatan Bisk1 Saadia Gabriel1 Dipendra Misra2

Yoav Artzi2

Yejin Choi1

Dieter Fox1,3

arXiv:1811.08824v3 [cs.CV] 7 Aug 2019 F1

Abstract— Building perceptual systems for robotics which perform well under tight computational budgets requires novel architectures which rethink the traditional computer vision pipeline. Modern vision architectures require the agent to build a summary representation of the entire scene, even if most of the input is irrelevant to the agent’s current goal. In this work, we ﬂip this paradigm, by introducing EARLYFUSION vision models that condition on a goal to build custom representations for downstream tasks. We show that these goal speciﬁc representations can be learned more quickly, are substantially more parameter efﬁcient, and more robust than existing attention mechanisms in our domain. We demonstrate the effectiveness of these methods on a simulated item retrieval problem that is trained in a fully end-to-end manner via imitation learning.

I. INTRODUCTION
Robotics has beneﬁted greatly from advances in computer vision, but sometimes the objectives of these ﬁelds have been misaligned. While the goal of a computer vision researcher is often “tell me what you see,” the roboticist’s is “do what I say.” In goal directed tasks, most of the scene is a distraction. When grabbing an apple, an agent only needs to care about the table or chairs if they interfere with accomplishing the goal. Additionally, when a robot learns through grounded interactions, architectures must be sample efﬁcient in order to learn visual representations quickly for new environments. In this work we show how inverting the traditional perception pipeline: Vision → Scene Representation + Goal → Action to incorporate goal information early into the visual stream allows agents to jointly reason and perceive: Vision + Goal → Action, yielding faster and more robust learning.
We focus on retrieving objects in a 3D environment as an example domain for testing our vision architectures. This task includes vocabulary learning, navigation, and scene understanding. Task completion requires computing action trajectories and resolving 3D occlusions from a 2D image which satisfy the user’s requests. Fast and efﬁcient planners work well in the presence of ground-truth knowledge of the world [1]. However, in practice, this ground-truth knowledge is difﬁcult to obtain, and we must often settle for noisy estimates. Additionally, when many objects need to be collected or moved, the planning problem search space grows rapidly.
Unlike computationally expensive modern vision algorithms, we are interested in training perception algorithms with a more natural source of supervision, example demonstrations and imitation learning, in lieu of expensive large scale collections of labeled images. This is

1Paul G. Allen School of Computer Science and Engineering, University

of Washington.

2Cornell University.

3NVIDIA.

…

…

Softmax Attention

Neural Attention

Our Early Fusion

1

0.75

0.5

0.25

0

128

64

32

16

Hidden Dimension Size

Fig. 1: We introduce a novel neural architecture for goal directed object detection which we demonstrate in a simulated table clearing task shown in the top row. We demonstrate that unlike conventional approaches, this structure is stable under extreme parameter budgets as seen in the bottom row.

particularly important for developing agents that learn new object classes on the ﬂy (e.g. when being integrated into a new environment). Our work is most closely related to recent advances in instruction following and visual attention [2], [3], but we do not provide explicit supervision for object detections or classiﬁcations. Finally, we will make the assumption that goals are speciﬁed by a simple list of object IDs, so as to avoid the ambiguity introduced by natural language commands.
Contributions: We show that early fusion of goal information in the visual processing pipeline (EARLY FUSION) outperforms traditional approaches and learns faster. Furthermore, model accuracy does not degrade in performance even when reducing model parameters by orders of magnitude (from 6M to ∼25K).
II. TASK DEFINITION
In order to test the performance of EARLY FUSION we built a simulated robotic task in which the objective is to collect objects in a 3D scene as efﬁciently as possible. The agent is presented with a cluttered scene and a list of requested objects. Often there are multiple instances of the same object, and there can be unrequested objects blocking the agent’s ability to reach a target. This forces the agent to reason about which object is closest and remove obstructions

as necessary. The list of requested objects that remain in the scene is presented to the agent at every time step, to avoid conﬂating scene understanding performance with issues of memory. The goal (Fig. 1 and 3) is to train an agent to optimally collect a list of objects from a cluttered counter.

A. Simulation Environment: CHALET

Our environment con-

sists of a tabletop set-

ting with randomly placed

objects, within a kitchen

from the CHALET [4]

house environment. Every

episode consists of a ran-

domly sampled environ-

ment which determines the

set of objects (number,

position, orientation and

type) in addition to which subset will be requested.

Fig. 2: Our 16 object types.

When there is more than one instance of a particular object,

collecting any instance will satisfy the collection criteria, but

one may be closer and require fewer steps to reach. Fig. 2

shows the sixteen object types that we use for this task (six

from CHALET and ten from the YCB dataset). Importantly,

these are common household items, many of which cannot

be detected by off the shelf ImageNet trained models.

The objects are chosen randomly and placed at a random

location (x,y) on the table with a random upright orientation

(θ). Positions and orientations are sampled until a non-

colliding conﬁguration is found. A random subset of the

instances on the table are used for the list of requested

objects. This process allows the same object type to be

requested multiple times if multiple of those objects exist in

the scene. Additionally, random sampling means an object

may serve as a target in one episode and a distractor in

the next. The agent receives 128x128 pixel images of the world and has a 60◦ horizontal ﬁeld of view, requiring

some exploration if a requested object is not in view.

Our agent consists of a ﬁrst-person camera that can tilt up and down and pan left and right with additional collect, remove and idle actions. Each of the pan and tilt actions deterministically rotate the camera 2◦ in the speciﬁed direction. The collect action removes the nearest object that is within 3◦ of the center axis of the camera and registers the object as having been collected for the purposes of calculating

Fig. 3: Collecting the Jello (blue box) requires more steps than the peach (orange box) due to occluding objects. An object may be collected if it is within the magenta circle.
the agent’s score. This region

is visualized in Fig. 3 as a magenta circle in the center of the frame. The remove action does the same thing as collect, but does not register the item as having been collected. This is used to remove superﬂuous items occluding the requested target. Finally, the idle action performs no action and should only be used once all requested items have been collected. All actions require one time step, therefore objects which are physically closer to the center of the camera may take more time steps to reach if they are occluded. For example, in Fig. 3 the peach (orange box) requires fewer steps to collect than the Jello box (blue box) because the banana and Rubik’s cube must be removed ﬁrst. The precision required to successfully collect an object makes this a difﬁcult task to master from visual data alone.
III. MODELS
In our task, models must learn to ground visual representations of the world to the description of what to collect. How to best combine this information is a crucial modelling decision. Most multimodal approaches compute a visual feature map representing the contents of the entire image before selectively ﬁltering based on the goal. This is commonly achieved using soft attention mechanisms developed in the language [5] and vision [6], [7], [8] communities.
Attention re-weights the image representation and leads to more informative gradients, helping models learn quickly and efﬁciently. Despite its successes, attention has important limitations Most notably, because task speciﬁc knowledge is only incorporated late in the visual processing pipeline, the model must ﬁrst build dense image representations that encode anything the attention might want to extract for all possible future goals. In complex scenes and tasks, this places a heavy burden on the initial stages of the vision system. In contrast, we present a technique that injects goal information early into the visual pipeline in order to build a task speciﬁc representation of the image from the bottom up. Our approach avoids the traditional bottleneck imposed on perception systems, and allows the model to discard irrelevant information immediately. Our system may still need to reason about multiple objects in the case of clutter and occlusion (e.g. target vs distractor), but its perception can ignore all objects and details that are not relevant for the current task.
Below, we brieﬂy describe the three models (Figure 4) we compare: Traditional approaches with delayed goal information (LATE FUSION & ATTENTION MAP) versus our goal conditioned EARLY FUSION architecture.
A. LATE FUSION
LATE FUSION constructs a single holistic representation of the entire image via a stack of convolution and pooling layers before concatenating an embedding of the requested objects in order to predict an action. An object embedding is computed using a simple linear layer designed to turn a onehot encoding of the object into a dense representation. The complete request for multiple objects is computed as a sum of these individual object embeddings. This design forces

Late Fusion

Attention Map

Early Fusion

Σ

Target: 1 Peach

Target: 1 Peach

Target: 1 Peach

3x3 Convolution, 2x2 Maxpool, ReLU, Batchnorm Linear Embedding Element-wise Dot Product

Multiple Concatenation with Convolution Concatenation Element-wise Multiplication

Fully Connected with ReLU, Dropout Except Last Softmax Σ Summation

Fig. 4: We compare a simple concatenation of visual and goal representations (LATE FUSION), against two variations of the attention mechanism above, and EARLY FUSION to isolate the effects of when multimodal representations are formed.

the vision module to store semantic and spatial information about every object in the scene so the ﬁnal fully connected layers can ground target objects and reason about actions.

B. ATTENTION MAP

We test traditional attention mechanisms over image re-

gions. As with LATE FUSION, the ﬁrst step of this model

is to pass the image through a stack of convolution layers.

Rather than concatenate the request embedding directly onto

the resulting representation, these models ﬁrst compute an

attention map over the spatial dimensions of the convolution

output. This is accomplished by comparing the embedded

target vector with each region of the convolutional feature

map via a simple dot product. This provides a weight to

each region which can then be used to form the ﬁnal image

representation I =

i

αi Z

hi

.

Next,

I

is

concatenated

to

the

request to make an action decision. We test two attention

models: SOFTMAX ATTENTION MAP which is deﬁned above

and ATTENTION MAP which is unnormalized. Using a

softmax leads to a peakier distribution which focuses the

model on fewer regions of the image (see Fig. 8).

In contrast to the LATE FUSION model, the attention

mechanism provides a ﬁlter on extraneous aspects of the

image to simplify the control processing. In these models

the grounding from image features to goal objects is done

with a direct comparison operator (the dot product). These

models are widely used for Visual Question Answering

(VQA) problems on static images. We also explored more

complex models [8] for computing attention maps, but found

this traditional version worked the best in our setting and

provided a strong baseline for comparison. In our results, we

follow [9] and append spatial grids to the ﬁrst layer of this

network to encode spatial knowledge. This extra information

proved necessary for the attention models to compete with

EARLY FUSION.1

C. EARLY FUSION
Finally, our EARLY FUSION approach concatenates the request embedding to every region of a convolutional ﬁlter map. This feature is then processed normally by a set of convolution kernels that have been augmented to account

1Spatial grids did not aid nor hinder LATE FUSION.

for the extra channels. Fig. 5 shows this process. All further

processing in the network is computed normally. The model’s

subsequent convolution and fully connected layers may ﬁlter

the visual information according to the goal description that

is now combined with the visual input. This results in an

image representation which contains only the necessary in-

formation for deciding the next action, effectively gaining the

beneﬁts of a bottleneck while dispersing the logic throughout

the network. Critically, this means that the network does not

have to build a semantic representation of the entire image

(See section IV-D for details).

Two important results of this architecture are: 1. Because the goal information is incorporated early,

Convolution Map i

Unrolled Map Regions Convolution Concatenation In Early Fusion

the network can learn to ground the image features

Convolution Map i+1

to the goal objects at any

point in the model without additional machinery (like attention); and 2. The

Goal Embedding

Augmented Convolution
Weights

model can compute and retain the spatial information needed for its next action without requiring the addition of a spatial grid.

Fig. 5: EARLY FUSION of goal information with visual data. The goal is concatenated with each block of visual data.

These beneﬁts allow us to obviate the complexity of other

approaches, minimize parameters, and outperform other ap-

proaches on our task.

D. Imitation Learning
All models are trained with imitation learning using an oracle with direct access to the simulator’s state. Similar to DAgger [10] and Scheduled Sampling [11] we use an alternating two-step training process. In the ﬁrst step, we roll out trajectories using the current model while collecting supervision from the expert. In the second step we use batches of recent trajectories to train the model for a small number of epochs. We then repeat this process and collect more data with the improved policy[12]. We found that for our item retrieval problem this was faster to train than a more faithful implementation of DAgger which would train a new policy on all previous data at each step, and offered

signiﬁcant improvements over behavior cloning (training on trajectories demonstrated by the expert policy).2
Rather than teach our agents to ﬁnd the shortest path to multiple objects, which is intractable in general, we design our expert policy to behave greedily and move to collect the requested object that would take the fewest steps to reach (including the time necessary to remove occluding objects).
E. Implementation Details
Since our goal is to construct a lightweight network that is fast to train and evaluate, we use a simple image processing stack of four convolution layers. While this is small relative to models trained for state-of-the-art performance on real images, it is consistent with other approaches in simple simulated settings [13]. All convolutions have 3×3 kernels with a padding of one, followed by 2×2 max-pooling, a ReLU nonlinearity [14] and batch normalization [15]. This means each layer produces a feature map with half the spatial dimensions of the input. The convolution layers are followed by two fully connected layers, the ﬁrst of which uses a ReLU nonlinearity and Dropout [16] and the second of which uses a softmax to produce output controls. The number of convolution channels and hidden dimensions in the fully connected layers vary by experiment (see Section IV-B). 3
a) Images: Our images are RGB and 128x128 pixels, but as is common practice in visual episodic settings [19] we found our models performed best when we concatenated the most recent three frames to create a 9x128x128 input. 4
b) Requests: Models are provided the remaining items to collect as a list of one-hot vectors. Each of these items is passed through a learned embedding (linear) layer to produce an encoding. These are then summed to produce a single dense vector (T arget). Because the sequence order is not important to our task, we found no beneﬁt from RNN based encodings, though the use of an embedding layer, rather than a count vector, proved essential to model performance.
IV. EXPERIMENTS
We tested all four models on a series of increasingly cluttered and difﬁcult problems. We also tested these models with varying network capacity by reducing the number of convolution channels and features in the fully connected layers. In all of these experiments, our EARLY FUSION model performs as well or better than the others, while typically training faster and with fewer parameters.
A. Varying Problem Difﬁculty
To test models on problems of increasing difﬁculty, we built three variations of the basic task by varying clutter and the number of requested items. In the simplest task (SIMPLE), each episode starts with four instances randomly
2Collecting 50 traj. in each roll-out step, and training on the most recent 150 traj. for three epochs in each training step produced the best results.
3 All of our models are optimized with Adam [17] with a learning rate of 1e-4, and trained with dropout [16]. The training loss was computed with cross-entropy over the action space. All models and code were written in PyTorch [18] and will be made available.
4Frames are black when they are not available in the ﬁrst two frames.

Early Fusion

Attention

SIMPLE

1

(4 Instances, 1 Request)

Softmax Attention

MEDIUM

1

(8 Instances, 2 Requests)

Late Fusion

HARD

1

(12 Instances, 3 Requests)

Agreement

Agreement

Agreement

0

0

0

0

5,000

10,000 0

10,000

20,000 0

15,000

30,000

Training Trajectories

Training Trajectories

Training Trajectories

1

1

1

F1

F1

F1

0

0

0

0

5,000

10,000 0

10,000

20,000 0

15,000

30,000

Training Trajectories

Training Trajectories

Training Trajectories

Fig. 6: Model performance on SIMPLE, MEDIUM, and HARD learning paradigms. Models were run to convergence.

placed on the table and one object type is requested. Next, for MEDIUM eight instances are placed and two are requested. Finally, for HARD twelve instances are placed and three are requested. Note that as the clutter increases, the agent is presented with not only a more complicated visual environment, but must also work in a more complex action domain where it is increasingly important to use the remove action to deal with occluding objects. The agent’s goal is to collect only the requested items in the allotted time. To evaluate peak performance for these experiments we ﬁxed the number of convolutions and hidden layer dimensions in the fully connected layers to 128.
Each episode runs for forty-eight steps, during which it is possible for the agent to both successfully collect requested objects and erroneously collect items that were not requested. We therefore measure task completion using an F1 score. Precision is the percentage of collected objects that were actually requested, and recall is the percentage of requested objects that were collected. The F1 score is computed at the end of each episode. In addition, we report overall agreement between the model and the expert’s actions over the entire episode. Figure 6 plots the results of all four models on each of these problems as a function of training time.
a) SIMPLE: Except for the LATE FUSION model, which performs poorly in all scenarios, all models are able to master the easiest task. The EARLY FUSION and SOFTMAX ATTENTION MAP models learn quickly, but ATTENTION MAP eventually catches up to them. The failure of the LATE FUSION baseline on this task shows that even the simplest version of this problem is non-trivial.
b) MEDIUM: The intermediate problem formulation is clearly more difﬁcult, as no models are able to perform as well on it as the easiest problem. The EARLY FUSION model gains a small but signiﬁcant improvement in performance while SOFTMAX ATTENTION MAP and ATTENTION MAP are slightly worse, but comparable to each other.
c) HARD: This case contains more cluttered images and more complex goal descriptions. The EARLY FUSION model is clearly superior; it learns signiﬁcantly faster than the other models and results in higher peak performance.

Early Fusion

Early Fusion (1/2 channels)

Attention

16 channels/layer
1

32 channels/layer
1

Softmax Attention 64 channels/layer
1

Late Fusion 128 channels/layer
1

256 channels/layer
1

Agreement

Agreement

Agreement

Agreement

Agreement

00 25,000 50,000 00 25,000 50,000 00 25,000 50,000 00

1

1

1

1

7,500

15,000 00 1

5,000

10,000

F1

F1

F1

F1

F1

00 25,000 50,000 00 25,000 50,000 00 25,000 50,000 00

7,500

15,000 00

5,000

10,000

Training Trajectories

Training Trajectories

Training Trajectories

Training Trajectories

Training Trajectories

Fig. 7: Ablation analysis showing the effect of the number of convolution channels and fully-connected hidden units on network performance. Note that the scale of the x-axes in these plots varies due to longer training times for smaller networks to converge. Dashed EARLY FUSION lines plot the performance of the model with half the reported number of ﬁlters to include a comparison ensure where model has fewer parameters than the attention based approaches.

Fig. 8: Attention visualizations for ATTENTION MAP and SOFTMAX ATTENTION MAP models. Targets are indicated here with magenta boxes in the top row.
It is also worth comparing the ATTENTION MAP and SOFTMAX ATTENTION MAP models. While these models perform similarly on these tasks, the SOFTMAX ATTENTION MAP model learns faster than the ATTENTION MAP model on the easiest task, but slightly slower on the more difﬁcult ones. We posit that the softmax focuses the attention heavily on only a few regions, which is useful for sparse uncluttered environments, but less appropriate when the network must reason about multiple objects in different regions.
Fig. 8 provides a comparison of attention maps. Unsurprisingly, the SOFTMAX ATTENTION MAP model produces a sharper distribution around the requested objects, but both methods correctly highlight the objects of interest. In this work, we have limited our deﬁnition of clutter to 12 items per scene, in part for ease of visualization and compute time.

B. Varying Network Capacity
Having demonstrated that EARLY FUSION is at least as powerful as attention based approaches while being simpler (no grid information or attention logic), we explore how these approaches perform on varying parameter budgets. Real-time and embedded systems require efﬁciency both when training and during inference. Since EARLY FUSION removes irrelevant information early in the processing pipeline, we expect it to require less network capacity than the other methods. To test this claim, we re-run our MEDIUM difﬁculty setting (because attention models performed well) and compare performance when models have access to 256, 128, 64, 32, or only 16 channel convolutions and fully connected layers, reducing our model sizes by several orders of magnitude.
In Fig. 7, training time increases for small networks, but EARLY FUSION is able to quickly achieve around the same ﬁnal performance regardless of the extremely small network capacity. This allows for dramatically more efﬁcient inference and parameter/memory usage. In contrast, other models degrade substantially as the network size decreases. Note that after 50,000 trajectories it appears that attention based models are still slowly improving, but there is a stark contrast in learning rates. In particular, for the smallest models (16) we see that ATTENTION MAP, even after training for twice as long as EARLY FUSION, still has half the performance.
Because attention mechanisms collapse their ﬁnal representations, they have a smaller fully connected layer and therefore fewer parameters for the same number of channels. To account for this, we have also included a dashed orange line in Fig. 7 labeled early fusion (1/2 channels) which shows the performance of EARLY FUSION with half the channels as the other models and fewer parameters. Again smaller EARLY FUSION networks outperform and learn faster than the other approaches.

C. Generalization

To measure generalization we conduct experiments in which the agent is trained on a subset of the possible request combinations and then tested on unseen requests. Here the agent is trained with 128 different two-item combinations, and then tested on a held out 128 two-item combinations (Rows 1 and 2 below). In this setting, the agent generalizes to unseen item pairs, indicating that the agent is not merely memorizing these combinations, but learning to recognize the structure of requests composed of individual objects.

Agreement F1

Two-Item Train Two-Item Test Three-Item Test

0.8918 0.8695 0.8140

0.9215 0.8938 0.8243

In the second experiment, the same agent was tested on a random collection of three-item combinations to determine if the agent can generalize to higher counts than during training (Row 3). The agent is surprisingly robust to this variation.

D. Information Retention
We have argued above that knowing the request allows the network to discard information about irrelevant objects in the scene. To investigate how much information is retained in the intermediate stages of the network we use the hidden states from models trained on the SIMPLE task and assess whether they can be used to predict the correct action for a new query that is different than the one they were conditioned on. This is implemented by freezing the original model, and training a new set of ﬁnal layers with a second conditional (Figure 9). In this experiment, we use the LATE FUSION model as a proxy for the layer prior to attention in those models.
We ﬁnd that if the same request is fed to both the original network and the new branch, we achieve performance comparable to the original model (dotted lines). On the other hand if mismatched requests are fed into the two branches all models suffer a substantial degradation of performance, with most unable to collect a single object (solid lines). Both Early Fusion and the attention models have completely removed irrelevant information, while Late Fusion approaches appear to only retain much of the irrelevant information.

V. RELATED WORK
Processing strategies for goal-directed visual search have been an important area of study in psychology, neuroscience and computer vision for many years [20], [21]. Early work in this area drew on the observation that human and primate vision seems to be at least partially driven by goal-directed top-down signals [22], [23].
More recently there has been a proliferation of works examining goal directed visual learning in simulated worlds [24], [25], [26], [27], [28], [29] which each aim to bring different amounts of language, vision and interaction to the task of navigating a 3D environment. This has also been

5Note 40% agreement is the majority class baseline as movement is more common than collection.

Pretrained and Frozen

Original Output Discarded

Old Target: 1 Peach
Early Fusion
1

New Target: 1 Mustard

Newly Trained

Attention

Softmax Attention
1

Late Fusion

Agreement F1

0

0

0

500

1,000

0

500

1,000

Training Trajectories

Training Trajectories

Fig. 9: The top is the frozen EARLY FUSION network with an new trainable branch for collecting an alternate test object. On bottom is the performance of this approach when the old target and the new target are aligned (dotted lines) and performance for when they are different (solid lines).5

attempted in real 3D environments [30]. Importantly, in contrast to our work, these approaches often pretrain as much of their networks as possible. [26] do not pretrain for their RL based language learning. Their work does not address learning with occulusion or larger vocabularies. In parallel, the robotics literature has investigated grounding instructions directly to robotic control [31], [32], [33], [29], [12], [34], a domain where data is expensive to collect.
Training end-to-end visual and control networks [35], has proven difﬁcult due to long roll outs and large action spaces. Within reinforcement learning, several approaches for mapping natural language instructions to actions rely on reward shapping [2], [3] and imitation learning [12], [34]. Imitation learning has also proven effective for ﬁne grained activities like grasping [36], leading to state-of-the-art results on a broad set of tasks [37]. The difﬁculty encountered in these scenarios emphasizes the need to explore new methods for efﬁcient learning of multimodal representations. [8] explored attention model architectures, but do not include early fusion techniques. Early fusion of goal information has shown promise with small observation spaces [38], but our work begins to explore this method for high-dimensional visual domains. In this paper, we hope to provide the community with a missing analysis and insights into this approach and its power in interactive settings.
VI. CONCLUSION
Goal directed computer vision is an important area for robotics research and efﬁcient training of high performing models with minimal footprints are essential for in situ learning. We take one step in this direction by showing how EARLY FUSION is ideal for the simulated robotic object

retrieval task, and preferable to traditional attention based ap-
proaches. Future work should investigate how our approach
and analysis can be generalized to on-device learning.
REFERENCES
[1] S. Srinivasa, G. Johnson, A.and Lee, M. Koval, S. Choudhury, J. King, C. Dellin, M. Harding, D. Butterworth, P. Velagapudi, and A. Thackston, “A system for multi-step mobile manipulation: Architecture, algorithms, and experiments,” in International Symposium on Experimental Robotics, 2016.
[2] D. K. Misra, J. Langford, and Y. Artzi, “Mapping instructions and visual observations to actions with reinforcement learning,” 04 2017.
[3] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkhin, and Y. Artzi, “Mapping instructions to actions in 3D environments with visual goal prediction,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.
[4] C. Yan, D. Misra, A. Bennett, A. Walsman, Y. Bisk, and Y. Artzi, “CHALET: Cornell House Agent Learning Environment,” 2018. [Online]. Available: https://arxiv.org/abs/1801.07357
[5] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in ICLR 2015, 09 2015.
[6] J. Ba, V. Mnih, and K. Kavukcuoglu, “Multiple object recognition with visual attention,” in ICLR 2015, 12 2015.
[7] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models of visual attention,” in NIPS, 06 2014.
[8] J. Singh, V. Ying, and A. Nutkiewicz, “Attention on attention: Architectures for visual question answering (vqa),” arXiv preprint arXiv:1803.07724, 2018.
[9] R. Liu, J. Lehman, P. Molino, F. P. Such, E. Frank, A. Sergeev, and J. Yosinski, “An intriguing failing of convolutional neural networks and the coordconv solution,” in Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018.
[10] S. Ross, G. J. Gordon, and J. A. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), 11 2011.
[11] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling for sequence prediction with recurrent neural networks,” in Advances in Neural Information Processing Systems, 2015, pp. 1171–1179.
[12] V. Blukis, N. Brukhim, A. Bennett, R. A. Knepper, and Y. Artzi, “Following high-level navigation instructions on a simulated quadcopter with imitation learning,” in Proceedings of the Robotics: Science and Systems Conference, 2018.
[13] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap, “A simple neural network module for relational reasoning,” in Advances in neural information processing systems, 2017, pp. 4967–4976.
[14] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.
[15] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proceedings of the 32nd International Conference on International Conference on Machine Learning, 02 2015.
[16] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overﬁtting,” The Journal of Machine Learning Research, vol. 15, pp. 1929–1958, 01 2014.
[17] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference for Learning Representations, 2015.
[18] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” in 31st Conference on Neural Information Processing Systems (NIPS 2017), 2017.

[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement learning,” in NIPS Deep Learning Workshop 2013, 2013.
[20] J. M. Wolfe, “Guided search 2.0 a revised model of visual search,” Psychonomic Bulletin & Review, vol. 1, no. 2, pp. 202–238, Jun 1994.
[21] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis, and F. Nuﬂo, “Modeling visual attention via selective tuning,” Artiﬁcial Intelligence, vol. 78, no. 1, pp. 507 – 545, 1995.
[22] S. Frintrop, E. Rome, and H. I. Christensen, “Computational visual attention systems and their cognitive foundations: A survey,” ACM Trans. Appl. Percept., vol. 7, no. 1, pp. 6:1–6:39, Jan. 2010.
[23] S. Frintrop, G. Backer, and E. Rome, “Goal-directed search with a top-down modulated computational attention system,” in Proceedings of the 27th DAGM Conference on Pattern Recognition, ser. PR’05. Berlin, Heidelberg: Springer-Verlag, 2005, pp. 117–124.
[24] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi, “IQA: Visual Question Answering in Interactive Environments,” in Computer Vision and Pattern Recognition, 2018.
[25] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Embodied Question Answering,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[26] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. M. Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, and P. Blunsom, “Grounded Language Learning in a Simulated 3D World,” 2017. [Online]. Available: http://arxiv.org/abs/1706.06551
[27] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Su¨nderhauf, I. D. Reid, S. Gould, and A. van den Hengel, “Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[28] F. Codevilla, M. Mu¨ller, A. Lo´pez, V. Koltun, and A. Dosovitskiy, “End-to-end driving via conditional imitation learning,” in ICRA, 2017.
[29] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, “Target-driven visual navigation in indoor scenes using deep reinforcement learning,” in ICRA, 2017.
[30] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, “Cognitive mapping and planning for visual navigation,” vol. 3, 2017.
[31] C. Matuszek, D. Fox, and K. Koscher, “Following directions using statistical machine translation,” in Proceedings of the international conference on Human-robot interaction, 2010.
[32] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. Teller, and N. Roy, “Understanding natural language commands for robotic navigation and mobile manipulation,” in Proceedings of the National Conference on Artiﬁcial Intelligence, 2011.
[33] D. K. Misra, J. Sung, K. Lee, and A. Saxena, “Tell me dave: Contextsensitive grounding of natural language to mobile manipulation instructions,” in Robotics: Science and Systems, ser. RSS, 2014.
[34] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi, “Mapping navigation instructions to continuous control actions with position visitation prediction,” in Proceedings of the Conference on Robot Learning, 2018.
[35] S. Levin, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of deep visuomotor policies,” in Journal of Machine Learning Research, 2017.
[36] C. Eppner, J. Sturm, M. Bennewitz, C. Stachniss, and W. Burgard, “Imitation learning with generalized task descriptions,” in 2009 IEEE International Conference on Robotics and Automation, May 2009, pp. 3968–3974.
[37] C. Eppner, S. Hfer, R. Jonschkowski, R. Martn-Martn, A. Sieverling, V. Wall, and O. Brock, “Lessons from the amazon picking challenge: Four aspects of building robotic systems,” in Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence, IJCAI-17, 2017, pp. 4831–4835.
[38] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation,” in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 31–36.

