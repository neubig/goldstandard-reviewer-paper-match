GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio
Guoguo Chen∗1,2, Shuzhou Chai∗1,3, Guanbo Wang∗1,3,6, Jiayu Du∗1, Wei-Qiang Zhang∗1,3, Chao Weng†4, Dan Su†4, Daniel Povey†5, Jan Trmal†6, Junbo Zhang†5, Mingjie Jin†4,
Sanjeev Khudanpur†6, Shinji Watanabe†6,7, Shuaijiang Zhao†8, Wei Zou†8, Xiangang Li†8, Xuchen Yao†2, Yongqing Wang†5, Yujun Wang†5, Zhao You†4, Zhiyong Yan†5,
1SpeechColab 2Seasalt AI Inc 3Dept EE, Tsinghua University 4Tencent AI Lab 5Xiaomi Corporation 6CLSP & HLTCOE, The Johns Hopkins University 7Carnegie Mellon University 8KE Holdings Inc
gigaspeech@speechcolab.org

arXiv:2106.06909v1 [cs.SD] 13 Jun 2021

Abstract
This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is ﬁrst collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to ﬁlter out segments with low-quality transcription. For system training, GigaSpeech provides ﬁve subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the ﬁltering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika. Index Terms: corpus, forced alignment, segmentation, speech recognition
1. Introduction
Thanks to the rapid development of the neural network models, automatic speech recognition (ASR) has made tremendous progress in the past decade. Various system architectures, from hybrid [1] to end-to-end [2], are proposed, and state-of-the-art results on standard benchmarks are being frequently updated.
The mainstream speech recognition corpora, on the other hand, have not changed much in decades. To take the English speech recognition task as an example, the Wall Street Journal corpus, which consists of 80 hours of narrated news articles [3], is almost 20 years old, and has a word error rate (WER) of 2.32% on its eval92 benchmark [4]. The Switchboard and Fisher corpus, which consists of 262 and 1,698 hours of telephone conversational speech, is also around 20 years old, and has a WER of 5.5% on the Switchboard portion of the Hub5’00 benchmark [5]. Even LibriSpeech [6], one of the most popular corpora for speech recognition tasks, is more than 5 years old, and has a WER of 1.9% on its test clean benchmark [7]. It consists of 1,000 hours of read English speech. Due to the fast development of speech recognition techniques, ASR performance on those data sets appears to have saturated, making it difﬁcult to track further improvements from new techniques.
There is some progress on creating better corpora/benchmarks for English speech recognition, from both academia and industry. TEDLIUM [8] is a series of corpora created by the Ubiqus company and the University of Le Mans. It consists of 452 hours of audio from TED talks in its latest release TED-LIUM 3. The corpora size, however, is less than 1,000 hours, making it not suitable for algorithms which demand a large amount of data. People’s Speech [9] released by ML
* Co-ﬁrst authors, equal contribution. † Authors listed in alphabetical order.

Commons consists of 87,000 hours of audio, covering 59 different languages. It’s source, however, is mostly audiobook, lacking crucial acoustic diversity. Another work is SPGISpeech [10], a corpus released by Kensho Technologies. It consists of 5,000 hours of transcribed audio from earnings calls transcribed by S&P Global, Inc. The corpus by its nature gives an emphasis to the business domain.
We release a complementary English speech recognition corpus named GigaSpeech, an evolving, multi-domain ASR corpus with 10,000 hours of transcribed Audio. The initial release of GigaSpeech is complementary to the existing corpora in the following ways:
• Extensible, the metadata is designed in a way that it can be easily reused for other tasks, such as speaker identiﬁcation.
• Large scale, 10,000 hours of transcribed speech.
• Multi-source, covers audiobook, podcast and YouTube.
• Multi-style, covers both read and spontaneous speech.
• Multi-topic, covers a variety of topics, such as arts, science, sports, etc.
• Original/Normalized transcription pairs, suitable for training end-to-end systems with post-processing (punctuation, case/date/time normalization, etc) included.
We make two contributions in this work. First, we release an evolving, multi-domain speech recognition corpus with 10,000 hours of labeled audio. Second we provide a scalable, reliable pipeline for generating speech recognition corpora.
The rest of the paper is organized as follows. Section 2 introduces the GigaSpeech corpus, and Section 3 presents the full pipeline to create the GigaSpeech corpus. We describe the speech recognition baseline systems for various toolkits, and provide experiment setup and results in Section 4. Finally, acknowledgements are given in Section 5.
2. GigaSpeech Corpus
This section explains the structure of the GigaSpeech corpus, including metadata, data partition, audio format, etc. Instructions and scripts for downloading GigaSpeech can be found on GigaSpeech’s GitHub repository1.
2.1. Metadata
We save all the metadata information to a single JSON ﬁle named GigaSpeech.json. Figure 1 shows a snip of this ﬁle. For better presentation of this paper, we skip a lot of non-critical entries in the snip, such as “format”, “md5”, “source”, etc.
To use the corpus, users are expected to extract the relevant information from GigaSpeech.json. For example, for the speech recognition task, one should ﬁrst follow the “audios” entry, and work out a list of audio ﬁles. One can then follow the “url” entry to download the original audio ﬁle, or “path” if preprocessed audio ﬁles have been downloaded to the disk. After that, for each audio ﬁle, one can follow the “segments” entry, and work out the trainable audio segments, as
1https://github.com/SpeechColab/GigaSpeech

Figure 1: A snip of the metadata ﬁle GigaSpeech.json

Subset
XL L M S XS

Table 1: GigaSpeech training subsets

Audiobook Podcast YouTube

2,655h 650h 260h 65h 2.6h

3,499h 875h 350h 87.5h 3.5h

3,846h 975h 390h 97.5h 3.9h

Total
10,000h 2,500h 1,000h
250h 10h

well as their corresponding transcripts. Of course, we also have various supplementary entries, such as “subsets”, “md5”, which will also be helpful for your task.
The metadata ﬁle GigaSpeech.json is version controlled, and is supposed to get updated over the time. In future releases, we plan to add speaker information to the metadata ﬁle, so that it will be suitable for speaker identiﬁcation/veriﬁcation tasks. We also plan to add more data from different sources to increase the diversity.
2.2. Training Subsets
We provide 5 training subsets in GigaSpeech, namely XS, S, M, L and XL, listed here in order of increasing audio hours. Table 1 shows a detailed breakdown of the 5 GigaSpeech training subsets.
2.3. Evaluation Sets
We provide 2 evaluation sets in GigaSpeech: a DEV set for development and tuning, which consists of 12.5 hours of audio, and a TEST set for ﬁnal evaluation, which consists of 40.3 hours of audio.
A breakdown of our evaluation sets is illustrated in Table 2. Note that our evaluation sets do not have a coverage for the audiobooks. We make sure that audio ﬁles from the LibriSpeech [6] evaluation sets (dev-clean, dev-other, test-clean and test-other) are not presented in our corpus, therefore, the LibriSpeech evaluation sets can be used as our evaluation sets as well.
2.4. Audio Format
To reduce the ﬁle size of the GigaSpeech corpus, we compress the original audio using the Opus audio codec. Original audio ﬁles are ﬁrst converted to 16k sampling rate, single channel and 16-bit signed-integer format. Opus compression is then applied to achieve an output bit rate of 32 kpbs, which results in a compression ratio of 8.
Table 3 shows the impact of Opus audio compression in terms of WER (%). Kaldi systems (see Section 4.3, but without recurrent neural network language model rescoring) are built for our M (1000h) training subset, with or without Opus compression. These two systems are then

Table 2: GigaSpeech evaluation sets

Sets Podcast YouTube Total

DEV TEST

6.3h 16.1h

6.2h 24.2h

12.5h 40.3h

Table 3: Impact of Opus audio compression (WER in %)

Train

Eval

DEV

TEST

Opus Wav Opus Wav

M Opus Wav

19.0 18.7 18.8 18.5

18.5 18.3 18.3 18.2

used to decode the DEV and TEST evaluation sets, with or without Opus compression. From Table 3, it is clear that compressing the training data with the Opus codec at 32 kpbs output bit rate has very small impact on the DEV and TEST set (0.1 - 0.2% WER degradation).
3. GigaSpeech Creation Pipeline
This section presents the detailed pipeline for creating the GigaSpeech corpus, which can be applied to other data generation tasks as well.
3.1. Stage 1: Audio Collection
We start the task by manually deﬁning the categories that we are interested in. We selected 24 categories in total, namely Arts, Business, Education, Autos and Vehicles, Comedy, Crime, Entertainment, Film and Animation, Gaming, Health and Fitness, History, Howto and Style, Kids and Family, Leisure, Music, News and Politics, Nonproﬁts and Activism, People and Blogs, Pets and Animals, Religion and Spirituality, Science and Technology, Society and Culture, Sports, Travel and Events.
For podcasts, we follow the above categories, and select episodes that come with manual transcriptions. For YouTube, we use the above categories as seed keywords, and select videos with human-generated closed captions. For audiobooks, we do not enforce those categories.
Once we have the list of audio ﬁles, we create tools and download all audio ﬁles with their corresponding transcripts.
3.2. Stage 2: Text Normalization
The audio transcripts we download from various sources are created by different transcribers with diversiﬁed transcription standards and styles, therefore it is necessary to apply text normalization to the original transcripts. We perform standard text normalization, including case normalization, special symbol removal, number to word rewriting, date/time rewriting, etc.
For audiobooks and podcasts, transcripts are usually at the episodes or chapter/book level. For speech recognition, however, smaller segments less than 20 seconds are needed for training. The next step is to segment the long audio ﬁle into smaller segments. For YouTube, closed captions are provided at the sentence level, but unfortunately we ﬁnd that the timestamps of closed captions are not reliable for segmentation. As a result, we decide to splice the closed captions all together, and perform the same segmentation as audiobooks and podcasts.
3.3. Stage 3: Forced Alignment
Our aligner is implemented with Kaldi [11], and the alignment procedure follows the work here [12], which adopts a divide and conquer strategy to tackle the alignment problem. First, both audio and transcript are uniformly chunked into smaller pieces. Second, audio segments are decoded with a biased language model (LM), and hypotheses with timestamps are generated. Third, each hypothesis segment is matched to one transcript segment via TF-IDF similarity. For each matched pair, hypothesis is further aligned with the transcript segment using the Smith-Waterman algorithm [13]. Finally, through this alignment, timestamps are attached to the transcript segments, and eventually to the whole transcripts by stitching the independently aligned segments together. Note that we modify the Smith-Waterman algorithm to handle

Figure 2: A forced alignment graph for the sentence “<s> A B C D E </s>” (4-gram)
silence and punctuation, and this is essential to enable the sentencebased segmentation in the next section.
To achieve better alignment performance, we ﬁrst align and segment the downloaded audio with a close-domain acoustic model. We then train an in-domain acoustic model with the audio segments created (around 3,000 hours). This model is used to align the whole corpus.
3.4. Stage 4: Audio Segmentation
We work out the audio segments from the alignment information above. Several rules are applied during the segmentation process:
• Split allowed at silence that is longer than 1 second.
• Split allowed at punctuation (“,”, “.”, “!” or “?”) that is longer than 0.2 seconds.
• Segments with alignment WER ≥ 75% re removed.
• Segments with length ≥ 20 seconds are removed.
• Silence at segment boundaries are truncated to 0.15 seconds.
It is worth pointing out that we keep 4 types of punctuation, namely comma, period, question mark and exclamation mark, so that split can happen at sentence boundaries (second rule above). We map them to special words “<COMMA>”, “<PERIOD>”, “<QUESTIONMARK>” and “<EXCLAMATIONMARK>” respectively. Besides, this also allows us to build end-to-end speech recognition systems that includes punctuation tagging, and end point detection.
3.5. Stage 5: Segment Validation
The segmentation stage generates a list of candidate segments, but potentially with high transcription error rate. We therefore apply segment validation to ﬁlter out bad segments.
3.5.1. Forced Alignment Graph
To better detect transcription errors made by human transcribers, we propose a variation of alignment graph in n-gram framework, as shown in Figure 2. The bold arrow path represents a typical LM-free forced alignment graph. Each state on the forced alignment path has a dotted “leaky” arc (with weight) that allows the token to leak out the forced alignment path, from higher order n-gram states down to lower order n-gram states, until reaching the null state. A garbage word loop (containing top 1,000 uni-gram words) is added around the null state to consume additional acoustic frames. Besides, there are extra states and arcs that allow the token to return to the forced alignment path. In general, this alignment graph allows the decoder to perform insertion/deletion/substitution to the reference. This essentially

brings more ﬂexibility to the forced alignment stage, making it possible to capture the discrepancy between the audio and the corresponding transcript.
3.5.2. Validation Decoding Pass
During the validation decoding pass, we detect transcription errors and ﬁlter out segments with high error rate. Figure 3 gives two examples of how errors are being detected. In the ﬁrst example, the transcriber misses a word “YOU” in the transcript, which is caught by our decoder. In the second example, the transcriber writes a typo which is also successfully detected.
For the podcast and YouTube portion of our XL training subset, we cap the maximum WER at 4%, and throw away all segments with higher WER. For the audiobook portion of the XL training subset, as well as all other smaller subsets, we cap the maximum WER at 0%, meaning we don’t allow any transcription errors.
3.5.3. Reference Rewriting
Investigation into the validated segments reveals three common types of transcriber errors:
• Fillers ignored, such as AH, UH, UM, ER, ERR, YOU KNOW, I MEAN, SORT OF, etc.
• Conjunctions ignored, such as AND, OR, BUT, etc.
• Disﬂuency removed, such as “It’s it’s it’s a great thing!”.
Discarding these segments will fundamentally limit the diversity of the corpus. To ﬁx those common errors, we add a ﬁller loop (see Figure 2) to the forced alignment graph, which contains the above common ﬁllers and conjunctions. Besides, we also employ a disﬂuency detector. The ﬁller loop and the disﬂuency detector may modify the reference (reference rewriting), and if that happens, those words will be counted as correct. We only apply reference rewriting to our XL subset (10,000h).
3.6. Stage 6: Evaluation
Since our evaluation sets are manually processed by professional human transcribers, we take that as the ground truth, and use it to compute the frame level segmentation precision and recall, see Figure 4. Here recall tells us how many frames can be retrieved by our segmentation and validation pipeline, and precision tells us how many of these retrieved frames are correctly labeled (consistent with the human labels).
Figure 5 illustrates the precision-recall curve on the podcast portion of the DEV evaluation set. For the XL training subset, we select a working point that gives us 10,000 hours of validated audio data, while keeping the maximum WER under 4%. And for all our other training subsets, we select the working point that keeps the maximum WER at 0% (the left most working point on Figure 5).
4. Experiments
This section describes the baseline systems and experimental results for four popular speech recognition toolkits, namely Athena, ESPnet [14], Kaldi [11] and Pika [15].
4.1. Athena Baseline System2
The Athena baseline implements an encoder-decoder based transformer model, which is similar to [16], with parameters e = 12, d = 6, dmodel = 1024, dﬀ = 2048 and dhead = 8.
During the training, the output sequence of the encoder is also used for connectionist temporal classiﬁcation (CTC) for joint training to enforce monotonic alignment between speech and label sequences. We use the Adam optimizer and varied learning rate with a warmup schedule (warmup steps = 8000). The model is trained with a total batch size of 128 for 5 epochs.
During the decoding, beam search with a beam size of 8 is used, which combines the scores of the decoder, the CTC with weight 0.5 and the LM with weight 0.2. The LM is a RNN based model with two long short-term memory (LSTM) layers, each with 1024 nodes.
2https://github.com/athena-team/athena/tree/ master/examples/asr/gigaspeech

Figure 3: Examples of transcription errors detected by forced alignment

Table 4: GigaSpeech baselines for the XL training subset (WER in %)

Toolkit

Model

DEV TEST

Athena ESPnet Kaldi
Pika

Transformer-AED + RNNLM Conformer/Transformer-AED
Chain + RNNLM RNN-T

13.60 10.90 14.78 12.30

12.70 10.80 14.84 12.30

Figure 4: Frame level segmentation accuracy
Figure 5: Frame level precision-recall curve for the segmentation and validation pipeline
4.2. ESPnet Baseline System3 The ESPnet baseline uses conformer (Convolution-Augmented Transformer) [7], which is a recently proposed architecture combining the local sensitivies of convolutional neural networks with the long-range interactions of transformers [17]. We use the implementation provided in the ESPnet toolkit [18]. We use a set of 5k BPE tokens generated by the SentencePiece tokenizer [19]. The model has a 12 conformer blocks with an output dimension of 512 and a kernel size of 31 in the encoder, and 6 transformer blocks in the decoder. Both encoder and decoder have 8 attention heads with 2048 feed-forward unit dimension. Conformer was trained using four 24Gb memory Titan RTX GPUs. The max trainable epoch is 20. Mini-batch size is 35 million acoustic feature bins. Adam optimizer with no weight decay was used. Noam learning rate scheduler was set to 25k warmup steps with a learning rate of 0.0015. SpecAug used 2 frequency masks and 5 time masks. The last 10 best checkpoints were averaged as the ﬁnal model. 4.3. Kaldi Baseline System4 The Kaldi baseline implements a typical chain model. First, a GMMHMM model is trained to obtain the alignments with no data cleaning. Second, volume and speed augmentation techniques are applied. Ivectors are then extracted and pasted to the basic acoustic features, as most Kaldi’s recipes do. Finally, a neural network is trained with
3https://github.com/espnet/espnet/tree/ master/egs2/gigaspeech/asr1
4https://github.com/kaldi-asr/kaldi/tree/ master/egs/gigaspeech

Table 5: Kaldi baselines for GigaSpeech training subsets (WER in %)

Subset DEV TEST

XL 14.78 14.84

L

16.60 16.28

M

17.96 17.53

S

22.59 22.14

XS

N/A N/A

both cross-entropy and LF-MMI criteria. Our neural network stacks 6 convolutional neural network (CNN) layers, 10 TDNN-F layers, 1 attention-relu-renorm-layer, 1 TDNN-F layer, 1 fast-lstmp-layer, 1 TDNN-F layer and ﬁnally 1 more fast-lstmp-layer. During the decoding stage, a 4-gram LM is ﬁrst used for decoding, followed by a Recurrent Neural Network Language Model (RNNLM) rescoring pass.
4.4. Pika Baseline System5
The Pika baseline adopts a convolution and transformer based architecture [15] for the encoder of our RNN-T system. Five-layer transformer is used for the decoder and the hidden dimension of each layer is 512. We apply on-the-ﬂy speed and volume perturbation during training where speed rates are set to 0.9/1.0/1.1/1.2 and the volume range is from -55dB to -10dB. For input features, we use 80 dimensional log Fbanks. The targets of our RNN-T system are a set of English wordpieces plus blank symbol which lead to an output dimension of 5000. The number of total parameters in the RNN-T is about 87M. MBR training and two extra two-layer transformer based forward/backward rescorers are also adopted. All training is conducted on 16 V100 GPUs. Our distributed training strategy is based on block-wise model-update ﬁltering (BMUF) with a Nesterov momentum scheme but with different learning rate scheduling [15] where both initial and ﬁnal learning rates are set before training and number of training epochs are ﬁxed (therefore there is no early stop and no development/validation set is used). One single sweep of the GigaSpeech XL training subset takes about 5hrs. For decoding, we set beam-size to 8 and the temperature of softmax to 1.25.
4.5. Experimental Results
Table 4 demonstrates baseline results for Athena, ESPnet, Kaldi and Pika. Results listed here are purely for the purpose of providing baseline systems for each toolkit. They do not reﬂect the state-of-theart performance of each toolkit, and cannot be used to compare the performance across toolkits.
Table 5 illustrates the Kaldi baseline results for 4 training subsets. Generally speaking, as the training subset gets bigger, the performance goes up. Our smallest XS training subset (10h) is designed for system building and debugging only, and is not expected to give strong performance.

5. Acknowledgements
The authors would like to thank Xingyu Na for his various suggestions on the Kaldi baseline system. The authors also would like to thank Speechocean for transcribing the GigaSpeech evaluation sets.
5https://github.com/tencent-ailab/pika/tree/ main/egs/gigaspeech

6. References
[1] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2011.
[2] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proc. International Conference on Machine Learning (ICML). PMLR, 2014, pp. 1764– 1772.
[3] D. B. Paul and J. Baker, “The design for the wall street journalbased csr corpus,” in Proc. International Conference on Spoken Language Processing (ICSLP). ISCA, 1992.
[4] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, “Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI,” in Proc. Interspeech 2016. ISCA, 2016, pp. 2751–2755.
[5] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L.-L. Lim et al., “English conversational telephone speech recognition by humans and machines,” arXiv preprint arXiv:1703.02136, 2017.
[6] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in Proc. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.
[7] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech 2020, 2020, pp. 5036–5040.
[8] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Este`ve, “TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation,” in Proc. International Conference on Speech and Computer. Springer, 2018, pp. 198–208.
[9] “People’s Speech,” https://mlcommons.org/en/peoples-speech/, accessed April 1, 2021.
[10] P. K. O’Neill, V. Lavrukhin, S. Majumdar, V. Noroozi, Y. Zhang, O. Kuchaiev, J. Balam, Y. Dovzhenko, K. Freyberg, M. D. Shulman, B. Ginsburg, S. Watanabe, and G. Kucsko, “SPGISpeech: 5,000 hours of transcribed ﬁnancial audio for fully formattedend-to-end speech recognition,” in Submitted to Interspeech, 2021.
[11] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The Kaldi speech recognition toolkit,” in Proc. 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2011.
[12] V. Manohar, D. Povey, and S. Khudanpur, “JHU Kaldi system for Arabic MGB-3 ASR challenge using diarization, audiotranscript alignment and transfer learning,” in Proc. 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 346–352.
[13] W. R. Pearson, “Searching protein sequence libraries: Comparison of the sensitivity and selectivity of the Smith-Waterman and FASTA algorithms,” Genomics, vol. 11, no. 3, pp. 635–650, 1991.
[14] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, and N. Chen, “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech 2018. ISCA, 2018.
[15] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, “Minimum bayes risk training of RNN-transducer for end-to-end speech recognition,” in Proc. Interspeech 2020. ISCA, 2020, pp. 966– 970.
[16] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A comparative study on transformer vs rnn in speech applications,” in Proc. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 449– 456.

[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017.
[18] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi et al., “Recent developments on espnet toolkit boosted by conformer,” arXiv preprint arXiv:2010.13956, 2020.
[19] T. Kudo and J. Richardson, “Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,” arXiv preprint arXiv:1808.06226, 2018.

