Variable Demand and Multi-commodity Flow in Markovian Network Equilibrium

arXiv:1901.08731v4 [math.OC] 18 Oct 2021

Yue Yu, Dan Calderone, Sarah H. Q. Li, Lillian J. Ratliﬀ, Beh¸cet A¸cıkme¸se
Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, TX, 78712 Department of Aeronautics and Astronautics, University of Washington, Seattle, WA, 98195
Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, 98195

Abstract
Markovian network equilibrium generalizes the classical Wardrop equilibrium in network games. At a Markovian network equilibrium, each player of the game solves a Markov decision process instead of a shortest path problem. We propose two novel extensions of Markovian network equilibrium by considering 1) variable demand, which oﬀers the players a quitting option, and 2) multi-commodity ﬂow, which allows players to have heterogeneous ending time. We further develop dynamic-programmingbased iterative algorithms for the proposed equilibrium problems, together with their arithmetic complexity analysis. Finally, we illustrate our network equilibrium model via a multi-commodity ride-sharing example, and compare the computational eﬃciency of our algorithms against state-of-the-art optimization software Mosek over extensive numerical experiments.
Key words: Wardrop equilibrium, Markov decision process, network optimization

1 Introduction
Network equilibrium problems arise in a variety of applications, such as resource allocation and routing in communication or transportation networks [Rockafellar, 1984,Bertsekas, 1998,Xiao et al., 2004,Bu¨rger et al., 2014]. Among the most well-studied examples is the Wardrop equilibrium model in routing games [Beckmann et al., 1956, Gartner, 1980a, Gartner, 1980b, Correa and Stier-Moses, 2010, Patriksson, 1994]. In this model, users in a transportation network are assumed to choose routes with cost that they perceive as the lowest, i.e., each user solves a shortest path problem, under the prevailing traﬃc conditions [Correa and Stier-Moses, 2010]. With this assumption, the resulting equilibra are characterized by the Wardrop equilibrium principle: the
1 Y. Yu is with the Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, Texas, 78712 (e-mail: yueyu@utexas.edu). S. H. Q. Li, and B. Ac¸ıkme¸se are with the William E. Boeing Department of Aeronautics & Astronautics, University of Washington, Seattle, Washington, 98195 (e-mail: yueyu@uw.edu, sarahli@uw.edu, behcet@uw.edu). D. Calderone and L. J. Ratliﬀ are with the Department of Electrical & Computer Engineering, University of Washington, Seattle, Washington, 98195 (e-mail:djcal@uw.edu, ratliffl@uw.edu).

cost of all the routes actually used are equal, and less than those which would be experienced by a single user on any unused route [Wardrop and Whitehead, 1952].
To ensure their practical relevance, it is often necessary to incorporating stochasticity into the network equilibrium problems. For example, the stochastic user equilibrium (SUE) model [Fisk, 1980, Sheﬃ and Powell, 1982, Liu et al., 2009] considers independent stochastic error on the route cost perceived by the users, leading to user distribution based on the logit [Dial, 1971] or probit model [Daganzo and Sheﬃ, 1977]; see [Patriksson, 1994, Sec. 2.8.1] and [Cominetti et al., 2012] for a detailed discussion. Unfortunately, the SUE model presents several drawbacks: it requires computationally expensive route enumeration, and is not suited for problems with overlapping routes due to its assumption of independent route cost.
To address these drawbacks, diﬀerent network models consider diﬀerent type of stochasticity. In particular, [Baillon and Cominetti, 2008, Ahipa¸sao˘glu et al., 2019] introduced a Markovian network equilibrium model where users are assumed to choose, instead of routes, sequences of actions with accumulated cost that they perceive as the lowest. Each action is accompanied by a deterministic outcome and a stochastic cost. For

Preprint submitted to Automatica

19 October 2021

example, each vehicle in a transportation network is assumed to choose a sequence of arcs, where each arc leads to deterministic transition to the next node in the network and a stochastic amount of travel time [Baillon and Cominetti, 2008].
On the other hand, [Calderone and Sastry, 2017b, Calderone and Sastry, 2017a] proposed a diﬀerent stochastic network equilibrium model. Unlike the one in [Baillon and Cominetti, 2008], each action is accompanied by a stochastic outcome and a deterministic cost. For example, an aircraft ﬂying in stormy weather is assumed to choose a sequence of waypoints to ﬂy towards, where each choice costs a deterministic amount of fuel usage and is accompanied by a stochastic change in the weather condition [Nilim and El Ghaoui, 2005]. As a result, instead of a shortest path problem, each user solves a Markov decision process (MDP) [Puterman, 1994, Bertsekas, 1996], where the cost of diﬀerent actions is determined by the prevailing choices of all users. This model has found a variety of applications in modern transportation including ridesharing and parking [Calderone, 2017].
Although the results in [Calderone and Sastry, 2017b, Calderone and Sastry, 2017a] serves as a ﬁrst step toward a more general class of stochastic dynamic network equilibrium model, it has the following limitations: a) it does not incorporate many important features of Wardrop equilibrium, such as variable demand and multi-commodity ﬂow and b) its solution method relies exclusively on oﬀ-the-shelf optimization software, which does not fully exploit the problem structure. We address these limitations by making the following contributions.
(1) We develop novel extensions to the Markovian network equilibrium model by considering a) variable demand, which oﬀers the users a quitting option, and b) multi-commodity ﬂow, which allows users having heterogeneous ending time.
(2) We design novel dynamic-programming-based algorithms for Markovian network equilibrium problems with detailed arithematical complexity analysis. Our algorithms outperform state-of-the-art optimization software Mosek in extensive numerical experiments.
The rest of the paper is organized as follows. We ﬁrst revisit some background on MDP in Section 2, then present our variable demand and multi-commodity ﬂow equilibrium models in Section 3. Section 4 focuses on developing eﬃcient iterative algorithms for our equilibrium problems. Section 5 ﬁrst illustrates the equilibrium models in Section 3 via a multi-commodity ride-sharing example, then compares the algorithms in Section 4 against commercial software Mosek. Finally, we conclude with discussions and comments on the future directions of research in Section 6.

Throughout the paper we will use the following notation:

R denotes the set of real numbers, R+ denotes the set of

nonnegative real numbers, and N denotes the set of pos-

itive integers; [N ] denotes the set {1, 2, . . . , N } for inte-

ger N ; aijk denotes the ijk–th component of the threedimensional tensor a ∈ Rn1×n2×n3 , and analogously, aij

for the two-dimensional case. Given b1, . . . , bN ∈ R, we

say (b , i ) = min bi, if b = min bi and i ∈ argmin bi.

i∈[N ]

i∈[N ]

i∈[N ]

2 Preliminaries and background

A T -horizon MDP is deﬁned by a set of states [S], a set of actions [A], a cost tensor c ∈ RT ×S×A, and a transition probablity tensor P ∈ [0, 1]S×A×S, where T, S, A ∈ N denote the number of time steps, states, and actions, re-
spectively. Further, ctsa ∈ R denotes the cost of choosing action a ∈ [A] in state s ∈ [S] at time t ∈ [T ], and
Psas ∈ [0, 1] denotes the probability of transition from state s ∈ [S] to s ∈ [S] when choosing action a ∈ [A]. In
order to ﬁnd the optimal sequence of action that mini-
mizes the expected accumulated cost, one can solve either one of the two following linear programs 2 :

min

ctsaytsa

y t,s,a

s.t. y1sa = p1s,
a

yt+1,sa = pt+1,s + Ps asyts a, t ∈ [T − 1],

a

s ,a

0 ≤ ytsa, ∀t ∈ [T ], s ∈ [S], a ∈ [A]. (1)

max ptsvts
v t,s

s.t. vT s ≤ cT sa, (2) vts ≤ ctsa + Psas vt+1,s , t ∈ [T − 1],
s
∀s ∈ [S], a ∈ [A].

where

p

∈

T ×S
R+

is

such

that

p1s

>

0

for

some

s

∈

[S].

If

s∈[S] p1s = 1 and pts = 0 for all 1 ≤ t ≤ T and s ∈ [S],

then p1s represents the probability of starting the MDP

in state s. Variable ytsa in optimization (1) represents the

probability of choosing action a in state s at time t, and

variable vts in optimization (2) represents the expected

accumulated cost between time t and time T starting

from state s. In general, optimization (1) and (2) can be

interpreted as the linear optimal distribution [Rockafel-

lar, 1984, Sec.7A] and diﬀerential problem [Rockafellar,

1984, Sec.7E] deﬁned on a T -layered Markovian network

(see Fig. 1), where pts represents the divergence on state

node s in the t-th layer, ytsa represents the ﬂow from

state s to action a in the t-th layer, and vts represents

the potential on state s in the t-th layer.

2 Compared with the formulation in [Puterman, 1994], the linear program here also allows pts > 0 when t > 1.

2

states

actions

states

...

...

.

...

.

..

..

layer t

Fig. 1. A Markovian network
The following lemma shows that solutions of optimizations (1) and (2) satisfy the dynamic programming principle.

Lemma 1 ( [Puterman, 1994]) Suppose y solves (1),
and v solves (2). If ytsa > 0 for any t ∈ [T ], s ∈ [S], a ∈ [A], then

(vT s, a) = min cT sa ,
a ∈[A]

(vts, a) = min ctsa +

a ∈[A]

s

Psa s vt+1,s ,

for all t ∈ [T − 1].

Perhaps the most eﬃcient solution algorithm for problem (1) and (2) is dynamic programming, given by the following Algorithm 1 and Algorithm 2.

Algorithm 1 Backward induction

Input: P , c, T .

Output: v, π.

1: Let (vT s, πT s) = min cT sa, ∀s ∈ [S].
a∈[A]

2: for t = T − 1, T − 2, . . . , 1 do

3: (vts, πts) = min ctsa + Psas vt+1,s , ∀s ∈

a∈[A]

s

[S]

4: end for

Algorithm 2 Forward induction

Input: π, p, P , T .

Output: y.

1: Initialize y = 0, let y1sπ1s ← p1s for all s ∈ [S].

2: for t = 1, 2, . . . , T − 1 do

3:

yt+1,sπt+1,s ← pt+1,s + Pjπtj sytjπtj , ∀s ∈ [S]

j

4: end for

Let (v, π) be the output of Algorithm 1 with input (P, c, T ), and y be the output of Algorithm 2 with input (π, p, P, T ), then one can easily show that such solution pair (y, v) directly satisﬁes the Karush-Kuhn-Tucker

(KKT) conditions [Rockafellar, 1970, Thm. 28.3] of (1) and (2), hence it is an optimal primal-dual solution pair. If we deﬁne the sparsity level of an MDP as follows

σ = max{N1, N2}/S ∈ [1/S, 1],

(3)

where N1 = maxs,a {s |Psas > 0} and N2 =
maxs ,a {s|Psas > 0} , then σS measures the maximum number of states connected by the transition kernel P . Further, it is straightforward to check that Algorithm 1 costs O(σT S2A) arithmetic operations, and Algorithm 2 costs O(σT S2) arithmetic operations. In addition, Algorithm 1 and Algorithm 2 can be implemented as convolutional neural networks that allows eﬃcient parallel computation [Tamar et al., 2016].

3 Markovian network equilibrium
By combining MDP together with classical routing games, Calderone and Sastry [Calderone and Sastry, 2017b] proposed MDP routing games where a ﬁxed amount of players with the same planning horizon choose sequences of actions that they perceive as achieving the lowest expected accumulated cost under the prevailing choices of other players. Such games are similar to routing games where a ﬁxed amount of players with the same destination choose routes that they perceive as the shortest under the prevailing choices of other players.
In this section, we introduce two generalizations to MDP routing games that allow the amount of players to vary and the planning horizon to diﬀer. We will also show that, under mild assumptions, the equilibra of such games can be computed eﬃciently using convex optimization.
3.1 Variable demand
One limitation of the MDP routing games in [Calderone and Sastry, 2017b] is the assumption that total amount of players is ﬁxed. However, an important feature in network games is to allow the total amount of players to vary, or equivalently, to provide the players with a quitting action [Patriksson, 1994, Sec. 2.1.2]. Aiming to address this limitation, we propose the following variable demand MDP routing games.
Game 1 At each time t ∈ [T ], pts new players start the game from state s ∈ [S]. Among these pts players, each one can choose to
(1) quit the game immediately at the cost of ψts(zts), (2) take action a ∈ [A] at the cost of φtsa(ytsa) and
reach state s ∈ [S] with probability Psas at time t + 1, then repeat such process till t = T , when the player ends the game after choosing the last action,

3

where zts and ytsa denote the total amount of players choosing to quit the game in state s at time t, and, respectively, taking action a in state s at time t.
Remark 1 Game 1 is a special case of mean ﬁeld games over graphs [Gomes et al., 2009, Gomes et al., 2010, Gu´eant, 2011,Gu´eant, 2015,Tanaka et al., 2020]. The interactions among diﬀerent players is mediated by a mean ﬁeld, described by function φtsa and function ψts for all t ∈ [T ], s ∈ [S], a ∈ [A].
Intuitively, one can interpret Game 1 as a competitive market model. The supply side corresponds to the stochastic environment, providing the option of playing or quitting the game. The demand side corresponds to the amount of players that decided to play the game, which changes with the expected accumulated cost of the playing option according to curve ψts for all t ∈ [T ] and s ∈ [S].
Remark 2 If the quitting option is not available, then Game 1 reduces to an MDP routing game with ﬁxed demand, introduced in [Calderone and Sastry, 2017b]. On the other hand, if the transition is Game 1 is deterministic, i.e., for each s ∈ [S] and a ∈ [A], there exists s ∈ [S] such that Psas = 1, then Game 1 reduces to a classical single-commodity routing game, with and a variable demand [Patriksson, 1994, Sec. 2.2.3]. Particularly, each player solves an MDP with deterministic transition, which is equivalent to a shortest path problem.
The Wardrop equilibrium principle is a key characterization of the equilibra of network games [Patriksson, 1994,Correa and Stier-Moses, 2010]. The principle states that, at equilibra, only the strategies with the lowest cost are actually used. Does this principle apply to Game 1? As we show in the following, the answer is aﬃrmative.
First, we make the following assumptions on Game 1.
Assumption 1 We assume that p ∈ RT+×S, P ∈ [0, 1]S×A×S and s Psas = 1 for all s ∈ [S], a ∈ [A]. Further, the function φtsa : [0, ρ] → R and function ψts : [0, ρ] → R are continuous and strictly increasing over their respective domains, where ρ = t,s pts.
With these assumptions, we now introduce the following pair of primal-dual optimization problems associated

with Game 1.

min
y,z t,s,a

ytsa

φtsa(α)dα +

0

t,s

zts
ψts(α)dα
0

s.t. y1sa = p1s − z1s,
a

yt+1,sa = pt+1,s − zt+1,s + Ps asyts a,

a

s ,a

t ∈ [T − 1],

0 ≤ ytsa, 0 ≤ zts ≤ pts, ∀t ∈ [T ], s ∈ [S], a ∈ [A].

(4)

utsa

max pts(vts − λts) −

u,v,w,λ t,s

t,s,a

wts

φ−ts1a(α)dα
φtsa (0)

−

ψt−s1(α)dα

t,s ψts(0)

s.t. vT s ≤ uT sa,

(5)

vts ≤ utsa + Psas vt+1,s , t ∈ [T − 1],
s
vts ≤ wts + λts, 0 ≤ λts,

∀t ∈ [T ], s ∈ [S], a ∈ [A].

In particular, the constraint 0 ≤ zts ≤ pts allows the number of players choosing to quit the game in state s at time t to vary winthin interval [0, pts]. If variable zts is zero and function φtsa is constant-valued for all t ∈ [T ], s ∈ [S], a ∈ [A], i.e., the quitting option is removed and the cost of each action does not depend on y in in Game 1, then one can verify that optimization (4) will reduce to (1) and optimization (5) will reduce to (2).

The following theorem shows that, under Assumption 1, the solution to the optimizations in (4) and (5) satisfy an equilibrium condition of Game 1. Similar to the Wardrop equilibrium principle, this equilibrium condition impliesthat no individual player can beneﬁt from unilaterally switching its actions.

Theorem 1 Suppose Assumption 1 holds, (y, z) solves (4), and (u, v, w, λ) solves (5), then for any pts > 0,

if zts = 0, then vts ≤ ψts(pts),

if 0 < zts < pts, then vts = ψts(zts),

(6)

if zts = pts, then vts ≥ ψts(0).

Further, if ytsa > 0, then

(vT s, a) = min φT sa (yT sa ),

a ∈[A]

(7)

(vts, a) = min φtsa (ytsa ) + Psa s vt+1,s ,

a ∈[A]

s

for all t ∈ [T − 1].

Proof See Appendix A.1.

4

Theorem 1 shows that an equilibrium of Game 1 that satisﬁes the Wardrop equilibrium principle not only exists, but can be computed by solving optimization (4) and (5). In particular, if action a is chosen in state s at time t by any player at equilibrium, i.e., ytsa > 0, then action a must be optimal in the sense of Algorithm 1. On the other hand, equation (6) says that if some players choose the quitting option in state s at time t at equilibrium, i.e., zts > 0, then the cost of playing is no more than quitting, i.e., vts ≥ ψts(pts). Similarly, if some players choose to play, i.e., zts < pts, then the cost of playing is no more than quitting, i.e., vts ≤ ψts(pts). Therefore, Theorem 1 indeed describes a Wardrop equilibrium where no individual player can beneﬁt from unilaterally switching to alternative actions.

3.2 Multicommodity ﬂow

Another limitation of the MDP routing games in [Calderone and Sastry, 2017b] is that all players are assumed to end their game at the same time, which is analogous to the single commodity routing game where all vehicles have the same destination. Aiming to address this limitation, we propose the following multicommodity MDP routing game, where players can have heterogeneous ending time, denote by T. We assume, without loss of generality, that T ⊂ [T ] and T ∈ T.
Game 2 At each time t ∈ [T ], pτts new players who have a common ending time τ ∈ T with τ ≥ t, start the game from state s. Each of these players can choose the action a at the cost of φtsa( τ,τ≥t ytτsa) and reach state s with probability Psas at time t + 1, then repeat this process till t = τ , when the player ends the game after choosing the last action. Here ytτsa denotes the total amount of players who plan to end the game at time τ and choose action a in state s at time t.

Remark 3 If T = {T }, then Game 2 reduces to a MDP routing game introduced in [Calderone and Sastry, 2017b]. On the other hand, if the transition in Game 2 is deterministic, i.e., for each s ∈ [S] and a ∈ [A], there exists s ∈ [S] such that Psas = 1, then Game 2 reduces to the traditional multi-commodity routing game with a ﬁxed demand [Patriksson, 1994, Sec. 2.1.1]. Particularly, the state where a player start and end the game form a origin-destination pair, which is jointly determined by the starting state and the deterministic transition.

Similar to Game 1, the equilibrium of Game 2 can also be computed by solving convex optimization problems, as we show in the following.

First, we make the following assumptions on Game 2.

Assumption 2 We assume T ∈ T ⊆ [T ], pτts ∈ R+

for all τ

∈

T, t

≥

τ

and s

∈

[S]; P

∈

S×A×S
R+

and

s Psas = 1 for all s ∈ [S], a ∈ [A]. Further, the function φtsa : [0, ρ] → R is continuous and strictly increasing, where ρ = τ t≤τ,s pτts.

With these assumptions, we now introduce the following pair of primal-dual optimization problems associated with Game 2. Notice that if T = {T }, then they reduce to optimization (1) and (2), respectively.

min
{yτ }τ∈T t,s,a

ytτsa
τ,τ≥t φtsa(α)dα
0

s.t.

y1τsa = pτ1s

a

ytτ+1,sa = pτt+1,s + Ps asytτs a, t ∈ [τ − 1],

a

s ,a

0 ≤ ytτsa, ∀τ ∈ T, t ∈ [τ ], s ∈ [S], a ∈ [A]

(8)

utsa

max

pτtsvtτs −

φ−ts1a(α)dα

u,{vτ }τ∈T t,s τ,τ ≥t

t,s,a φtsa(0)

s.t.

vττs ≤ uτsa,

(9)

vtτs ≤ utsa + Psas vtτ+1,s , t ∈ [τ − 1],

s

∀τ ∈ T, s ∈ [S], a ∈ [A]

The following theorem shows that, under Assumption 2, the solutions to optimization problems (8) and (9) satisfy the equilibrium condition of Game 2. Similar to the Wardrop equilibrium principle, this equilibrium condition implies that no individual player can beneﬁt from unilaterally switching actions.

Theorem 2 Suppose Assumption 2 holds, y solves (8),
and (u, v) solves (9). If ytτsa > 0 for any τ ∈ T, s ∈ [S], a ∈ [A], then

(vττs, a) = min φτsa

yττsa ,

a ∈[A]

τ,τ ≥t

(vtτs, a) = min φtsa

ytτsa + Psa s vtτ+1,s ,

a ∈[A]

τ,τ ≥t

s

(10) for all t ∈ [τ − 1].

Proof See Appendix A.2.

Theorem 2 shows that a Wardrop equilibrium of Game 2 not only exists, but can be found by solving optimization problems (8) and (9). In particular, the equations in (10) characterize a multi-commodity ﬂow Wardrop equilibrium in the sense that no individual player can beneﬁt from using alternative actions before his/her ending time τ for all τ ∈ T.

5

4 Eﬃcient algorithms via linearization

In this section, we develop eﬃcient iterative algorithms for the network equilibrium problems introduced in the previous section. In particular, we ﬁrst prove that the linearized versions of problem (4) and problem (8) can both be solved in closed form via Algorithm 1 and Algorithm 2. This observation motivates eﬃcient iterative algorithms that enjoy detailed arithematical complexity analysis.

We will use the following notation to simply our later discussions. Given y, u ∈ RT ×S×A and z, w ∈ RT ×S, we let φ(y), φ−1(u) ∈ RT ×S×A and ψ(z), ψ−1(w) ∈ RT ×S be such that
[φ(y)]tsa = φtsa(ytsa), [φ−1(u)]tsa = φ−ts1a(utsa), (11) [ψ(z)]ts = φts(zts), [ψ−1(w)]ts = ψt−s1(wts),

for all t ∈ [T ], s ∈ [S], a ∈ [A]. We also let u, u ∈ RT ×S×A and w, w ∈ RT ×S be such that

utsa = φtsa(0), utsa = φtsa(ρ), wts = ψts(0), wts = ψts(ρ),

(12)

for all t ∈ [T ], s ∈ [S], a ∈ [A].

4.1 Linearization and dynamic programming
If we approximate the objective function in (4) using its linearization at u ∈ RT ×S×A and w ∈ RT ×S, we obtain the following

−g(u, w) = min utsaytsa + wtszts

y,z t,s,a

t,s

(13)

s.t. constraints in problem (4).

Observe that the above optimization is a modiﬁcation to problem (1), by including an additional variable z. This suggest that (13) may also be solved using dynamic programming, which is conﬁrmed by the following lemma.

Lemma 2 Suppose Assumption 1 holds. Let (vˆ, πˆ) be the output of Algorithm 1 with input (P, u, T ), and

zˆts = pts, vˆts > wts ∀t ∈ [T ], s ∈ [S]. 0, vˆts ≤ wts

In addition, let yˆ be the output of Algorithm 2 with input (πˆ, p − zˆ, P, T ). Then

−g(u, w) = utsayˆtsa + wtszˆts.

t,s,a

t,s

Further, for any u ∈ RT ×S×A and w ∈ RT ×S,

g(u , w ) − g(u, w)

≥ (utsa − utsa)(−yˆtsa) + (wts − wts)(−zˆts).

t,s,a

t,s

Proof See Appendix A.3.

Similarly, if we approximate the objective function in (8) using a linear function, we obtain the following

−h(u) = min

utsaytτsa

yτ ,τ ∈T t,s,a τ,τ ≥t

(14)

s.t. constraints in problem (8).

where u ∈ RT ×S×A is the approximation parameter, −h(u) is the optimal value of (14). The following lemma shows that optimization (14) can be solved using Algorithm 1 and 2 as well.

Lemma 3 Suppose Assumption 2 holds. Let (vˆτ , πˆ) be the output of Algorithm 1 with input (P, u, τ ), yˆτ be the output of Algorithm 2 with input (πˆτ , pτ , P, τ ). Then

−h(u) =

utsayˆtτsa.

t,s,a τ,τ ≥t

Further, for any u ∈ RT ×S×A,

h(u ) − h(u) ≥

(utsa − utsa)(−yˆtτsa).

t,s,a τ,τ ≥t

Proof See Appendix A.4.
Remark 4 Function g : RT ×S×A × RT ×S → R in Lemma 2 is the support function of a polyhedron, which is closed and convex [Rockafellar, 1970, p.28]. Further, Lemma 2 shows that the slope of a linear underestimator, or subgradient, of function g(u, w) can be computed using Algorithm 1 and Algorithm 2. Similar observation is made in Lemma 3 for function h : RT ×S×A → R.
4.2 Iterative algorithms using linearization
We now develop iterative algorithms for optimization problems in Section 3 using the results from the previous subsection. We will use the following notion of -optimal solution.
Deﬁnition 1 Given a constrained optimization where an objective function is optimized subject to constraints, we say a solution is -optimal ∈ R+ if it satisﬁes all the constraints and the objective function value evaluated at this solution is at most away from the optimal value.

6

We will also use the following additional assumptions on Game 1 and, respectively, Game 2.
Assumption 3 Function φtsa : [0, ρ] → R and ψts : [0, ρ] → R are L-Lipschitz continuous over their respective domains for all t ∈ [T ], s ∈ [S] and a ∈ [A].
Assumption 4 Function φtsa : [0, ρ] → R is LLipschitz continuous over its domain for all t ∈ [T ], s ∈ [S] and a ∈ [A].
Remark 5 Assumption 3 and Assumption 4 are mild assumptions on the diﬀerentiability of the corresponding functions. For example, if φtsa is continuously diﬀerentiable, then the mean value theorem states that for any α1, α2 ∈ [0, ρ], there exists α3 ∈ [0, ρ] such that
|φtsa(α1) − φtsa(α2)| ≤ |φtsa(α3)| · |α1 − α2|.
where φtsa is the derivative of function φtsa. Hence Assumption 4 is satisﬁed by choosing
L ≥ max |φtsa(α)|, ∀t ∈ [T ], s ∈ [S], a ∈ [A].
α∈[0,ρ]
which takes a bounded value since φtsa is continuous. However, the continuity of φtsa is not necessary. For example, if φtsa is a piecewise linear function, i.e., a function that is aﬃne over a collection of intervals, then it is still Lipschitz continuous even if its derivative is not continuous.
Based on Lemma 2 and Lemma 3, we propose to solve optimiztaion (4) and (8) using Frank-Wolfe method [Frank and Wolfe, 1956], which repeatedly solve the linearized versions of (4) and (8). We summarize the Frank-Wolf method for optimization (4) and (8) in Algorithm 3 and, respectively, Algorithm 4. The following theorem provides the overall arithmetic complexity analysis of Algorithm 3 and Algorithm 4.
Algorithm 3 Frank-Wolfe method Input: p, P, φ, ψ, T, {αk}, initial value for y, z. 1: for k = 1, 2, . . . , K do 2: (vˆ, πˆ) ← Alg. 1(P, φ(y), T ). 3: zˆts = pts, vˆts > ψts(zts) , ∀t ∈ [T ], s ∈ [S]
0, vˆts ≤ ψts(zts) 4: yˆ ← Alg. 2 (πˆ, p − zˆ, P, T ). 5: y ← y − αk(y − yˆ) 6: z ← z − αk(z − zˆ) 7: end for
The following theorem shows the convergence property of Algorithm 3 and Algorithm 4.
Theorem 3 Let σ be given by (3). If Assumption 1 and 3 hold, then Algorithm 3 with αk = k+2 1 gives an optimal solution to (4) in O( σT S2A ) arithmetic opera-

Algorithm 4 Multicommodity Frank-Wolfe method
Input: p, P, φ, T, {αk}, initial value for yτ for all τ ∈ T. 1: for k = 1, 2, . . . , K do 2: yˆtsa = τ,τ≥t ytτsa, ∀t ∈ [T ], s ∈ [S], a ∈ [A] 3: (vˆτ , πˆτ ) ←Alg. 1(P, φ(yˆ), τ ), ∀τ ∈ T 4: yˆτ ← Alg. 2(πˆτ , pτ , P, τ ), ∀τ ∈ T 5: yτ ← yτ − αk(yτ − yˆτ ), ∀τ ∈ T 6: end for
tions. Similarly, if Assumption 2 and 4 hold, then Algorithm 4 with αk = k+2 1 gives an -optimal solution to (8) in O( σT 2S2A ) arithmetic operations.

Proof See Appendix A.5.

Theorem 1 provides arithmetical complexity of Algorithm 3 and 4, which not only depends on the problem size (i.e., T, S, A), but also the sparsity of the constraints (i.e., σ) in (4) and (8).

What about the dual problems? Observe that the optimization in (5) can be separated into two layers: an outer layer that optimizes over (u, w), and an inner layer that optimizes over (v, λ) for a given value of (u, w); namely, (5) is equivalent to the following

max
u,w

utsa

−g(u, w) −

φ−ts1a(α)dα

wts t,s,a φtsa(0)

−

ψt−s1(α)dα

t,s ψts(0)

(15)

where

−g(u, w) = max pts(vts − λts)

v,λ t,s

(16)

s.t. constraints in (5).

One can show that optimization (16) is exactly the dual problem of (13). Since the constraint sets in (13) and (16) are both nonempty, the optimal value of (13) and (16) are the same [Von Neumann and Morgenstern, 1953]. In other words, (5) can be equivalently written as follows

max
u,w

utsa

−g(u, w) −

φ−ts1a(α)dα

wts t,s,a φtsa(0)

−

ψt−s1(α)dα

t,s ψts(0)

s.t. −g(u, w) is the optimal value of (13).

(17)

7

Using similar reasoning, we can rewrite (9) as follows

max −h(u) −

u

t,s,a

utsa
φ−ts1a(α)dα
φtsa (0)

(18)

s.t. −h(u) is the optimal value of (14).

We already discussed in Remark 4 how the subgradients of function g(u, w) and h(u) can be computed eﬃciently using Algorithm 1 and Algorithm 2. In addition, all the other terms in the objective functions of problem (17) and (18) are continuously diﬀerentiable. This suggest that (17) and (18) are suited for the projected subgradient method, which optimizes a non-smooth function by repeatedly computing its subgradients and projections onto its domain. We summarize the projected subgradient method applied to (17) and (18) in Algorithm 5 and, respectively, Algorithm 6.

Algorithm 5 Subgradient method

Input: P, p, φ, ψ, T, {αk}, initial value for u, w. 1: for k = 1, 2, . . . , K do 2: (vˆ, πˆ) ← Alg. 1(P, u, T )

3:

zˆts = pts, vˆts > wts , ∀t ∈ [T ], s ∈ [S]

0, vˆts ≤ wts

4: yˆ ← Alg. 2 (πˆ, p − zˆ, P, T ). 5: u ← min{u, max{u, u + αk yˆ − φ−1(u) }}

6: w ← min{w, max{w, w + αk zˆ − ψ−1(w) }} 7: end for

Algorithm 6 Multi-commodity subgradient method
Input: p, P, φ, T, {αk}, initial value of u. 1: for k = 1, 2, . . . , K do 2: (vˆτ , πˆτ ) ← Alg. 1(P, u, τ ), ∀τ ∈ T 3: yˆτ ←Alg. 2(πˆτ , pτ , P, τ ), ∀τ ∈ T 4: yˆtsa = τ,τ≥t ytτsa, ∀t ∈ [T ], s ∈ [S], a ∈ [A] 5: u ← min{u, max{u, u + αk yˆ − φ−1(u) }}
6: end for

The following theorem shows the the convergence property of Algorithm 5 and Algorithm 6.

Theorem 4 Let σ be given by (3). If Assumption 1 and
Assumption 3 hold, then Algorithm 6 with αk = k2+L1 gives an -optimal solution to (17) using O( σT S2A ) arithmetic
operations. Similarly, if Assumption 2 and Assumption 4 hold, then Algorithm 6 with αk ≡ 2kL+T1 gives an -optimal solution to (18) in O( σT 2S2A ) arithmetic operations.

Proof See Appendix A.6

5 Numerical examples
In this section, we ﬁrst illustrate the equilibrium models in Section 3 using a ride-sharing example, then

demonstrate the eﬃciency of algorithms in Section 4 by comparing them against commercial software Mosek (https://www.mosek.com) over extensive numerical experiments.
5.1 Multicommodity ride-sharing game
We consider the game played by ride-sharing drivers in Seattle, competing for customers. We ﬁrst abstract the Seattle area as an undirected graph illustrated in Fig. 2, whose nodes denote various neighborhoods in Seattle, and edges denote available routes, labeled by its driving distance. We denote the set of neighboring nodes of node s as Ns. We model the decision-making of an ride-sharing driver on a typical weekend night (7pm-1am) as an MDP deﬁned as follows.

Fig. 2. Seattle transportation network and candidate LRT routes: 7-9-10-11(red) and 6-8-9-11(blue).

• Time steps: t = 1, 2, . . . , 36 denotes the (end of) 10minute-intervals between 7pm and 1am.
• States: [S] correspond to diﬀerent nodes in graph G. • Actions: in state s, as denotes picking up a waiting
rider with destination s for all s ∈ Ns; await denotes waiting for a future rider. • Transition kernel: we assume Psas is given by

1, Psas =

if a = as , s ∈ Ns,

1/(|Ns| + 1), if a = await, s ∈ Ns ∪ {s}.

All other entries of Psas are zero. Here we use an uniform distribution over neighboring states to describe the uncertain destinations of future riders 3 .
• Cost: due to the competition among drivers, we as-
sume the proﬁt for picking up a rider decreases with
the amount of drivers making the same oﬀer, namely

ftss = α + β 1 − ytsas γtss

distss ,

(19)

3 Such distribution can be approximated more accurately using historical data in practical applications.

8

for all t ∈ [T ], s ∈ [S], where α and β is the baseline proﬁt and, respectively, nominal proﬁt per mile. We let distss denotes distance(miles) between s and s , γtss denotes the rider demand from s to s at time t, and ﬁnally ytsas denotes the amount of drivers choosing action as in state s at time t. The cost of action a in state s is a function of ytsa deﬁned as follows



−ftss ,

φtsa(ytsa) = −

Psas ftss ,

 s ∈Ns

if a = as , s ∈ Ns. if a = await.

• Planning time windows: We assume that 10 drivers
start working from each state every 10 minutes be-
tween 7pm and 9pm. Once started, each driver is as-
sumed to only work for 4 consecutive hours to avoid driver fatigue, i.e., p(tst+24) = 10 for all s ∈ [S] and t ∈ [12].

Notice that the function φtsa deﬁned above is linear with slope α, hence Assumption 2 is satisﬁed with L = α. In general, as long as φtsa is modeled or approximated as a continuously diﬀerentiable function, Assumption 4 is always satisﬁed, as we discussed in Remark 5. We also assume that each driver can travel between neighboring nodes within one time step in this simpliﬁed transportation network. In practice, such assumption can be ensured by adding more nodes to the network using a ﬁner discretization of the interested area.

Notice that since drivers can start the game at diﬀerent times during 1 ≤ t ≤ 12 and they will only plan for the next 24 time steps. In other words, drivers with heterogeneous planning time windows will coexist in the network for t = 2, 3, . . . , 24. Therefore the equilibrium of this game is a multi-commodity Markovian network equilibrium discussed in Section 3.2. We consider the scenario where α = 10, β = 0.2, γtss is given in Table 1 and distss is given in Fig. 2. We compute the driver number in the downtown area D = {9, 10, 11} by solving the optimization in (8) using commercial software Mosek (https://www.mosek.com). The results are demonstrate in Fig. 3, where we can see that the driver number increases during 1 ≤ t ≤ 12, then decreases during 24 ≤ t ≤ 36. There are also two sudden changes in the increasing/decreasing rate around t = 7 and t = 31, which is due to the corresponding changes in values of γ in Table 1. This example extends the single commodity case considered in [Calderone and Sastry, 2017b] and [Li et al., 2019] where all players enter and exit the game simultaneously.

A relevant application of the above simulation framework is transportation network design. For example, suppose that Seattle city council is considering two candidate light rail transit (LRT) routes, 7-9-10-11 and 6-8-9-11 (see Fig. 2), as a means to alleviate the congestion caused by the ride-sharing traﬃc in downtown

area, assuming that the LRT will reduce the demand of ride-sharing services (namely, value of γtss ) by 50% along its route. The simulated equilibrium with diﬀerent LRT routes are also given in Fig. 3, which shows that route 6-8-9-11 is more eﬀective that route 7-9-10-11 in terms of reducing amount of drivers in D. These results clearly demonstrate the power of Markovian network equilibrium model in transportation system design.

drivers

600

500

400

300

w/o LRT

route 7-9-10-11

route 6-8-9-11

200

100

0

6

12

18

24

30

Fig. 3. Number of drivers in downtown area D = {9, 10, 11}.

Table 1 Values of γ where D = {9, 10, 11}.

γtss

s ∈/ D s ∈ D

s ∈D s ∈D

1 ≤ t ≤ 6 600 200

7 ≤ t ≤ 30 200 400

31 ≤ t ≤ 36 60

100

s∈D s ∈/ D
60 200 600

s ∈/ D s ∈/ D
60 60 60

5.2 Computation experiments
To demonstrate the eﬃciency of the algorithms developed in Section 4, we compare the computation time of our algorithms against commercial software Mosek, used in the previous section, over randomly generated examples. We use rand(a, b) to denote a random number sampled from uniform distribution over interval [a, b] where a, b ∈ R and a ≤ b.
• Psas = rand(0, 1) for all s ∈ [S], a ∈ [A], then normalized such that s Psas = 1
• φtsa(α) = rand(1, 2)α + rand(1, 2) for all t ∈ [T ], s ∈ [S], a ∈ [A].
• pts = rand(0, 1) for all s ∈ [S] if t = 1 and zero otherwise.
In the variable demand case, we let ψts(α) = rand(1, 2)α− t + 21 for all t ∈ [T ], s ∈ [S]. In the multi-commodity ﬂow case, we let T = {5, 10}.

9

102

101

100

10-1

10-2

Mosek

Algorithm 4

Algorithm 6

10-3

40

80

120

160

200

(a) Variable demand

102

101

100

10-1

10-2

Mosek

Algorithm 4

Algorithm 6

10-3

40

80

120

160

200

(b) Multi-commodity ﬂow
Fig. 4. Average computation time and 3-standard deviation intervals of 100 experiments with T = A = 10.

We ﬁx T = A = 10 and let S range between 20 and 200, then test the computation time of Algorithm 3, Algorithm 5, Algorithm 4 and Algorithm 6, where all algorithms terminate when their objective function value agrees with the optimal one obtained by Mosek with less than 0.5% relative error. The average computation time over 100 examples, along with corresponding 3-standard deviation interval are reported in Fig. 4. All codes are in MATLAB and run on a 1.6GHz laptop. From results in Fig. 4 we can see that, over the randomly generated 2000 examples, subgradient method and Frank-Wolfe method reduces the computation time consumed by Mosek by one and, respectively, two orders of magnitudes, at the price of a mere 0.5% of relative accuracy.

6 Conclusion
We study the variable demand and multi-commodity extensions in Markovian network equilibrium. We also propose eﬃcient algorithms that outperform state-of-theart commercial optimization software. However, the current work still has several limitations. For example, the cost of actions perceived by the players is assumed to be exact, rather than corrupted by stochastic noise, as considered in stochastic user equilibrium model. Further, the ending time of each player is ﬁxed at the beginning of the game. A more realistic assumption is to allow the players to change their ending time and recompute the equilibrium periodically. We aim to address these limitations in future work.

A Appendix

A.1 Proof of Theorem 1

The objective function of problem (4) is convex (since
φtsa and ψts are strictly increasing), its constraints are aﬃne, and the optimal value is obviously ﬁnite (since
φtsa and ψts are ﬁnitely valued). These imply that a solution pair to (4) and (5) necessarily satisfy the KKT con-
ditions [Rockafellar, 1970, Thm. 28.3.1]. Let vts be the dual variable corresponding to the equality constraint
containing pts, let µtsa, θts, λts ≥ 0 be the dual variables corresponding to constraint ytsa ≥ 0, zts ≥ 0 and, respectively, zts ≤ pts. Then the Lagrangian of (4) and (5) is given by

ytsa

L(y, z, v, µ, λ, θ) =

φtsa(α)dα − µtsaytsa

t,s,a 0

t,s,a

zts

+

ψts(α)dα + vts(pts − zts − ytsa)

t,s 0

t,s,a

+

vt+1,sPs asyts a − ((θts − λts)zts + λtspts).

t,t<T s ,a,s

t,s

The KKT conditions [Rockafellar, 1970, Thm.28.3] of this Lagrangian include the following vanishing gradient conditions (by setting ∂L/∂ytsa, ∂L/∂xts equal to zero)
vT s =φT sa(yT sa) − µT sa, vts =φtsa(ytsa) + Psas vt+1,s − µtsa, t ∈ [T − 1],
s
vts =ψts(zts) + λts − θts, t ∈ [T ], (A.1)
for all s ∈ [S], a ∈ [A], and the complementarity conditions
ytsaµtsa = 0, ztsθts = 0, λts(zts − pts) = 0 ytsa, zts, µtsa, θts, λts ≥ 0, ∀t ∈ [T ], s ∈ [S], a ∈ [A].
(A.2) Combining (A.1) and (A.2) yields (6) and (7). Note that same results can be derived from the dual problem (5).

10

A.2 Proof of Theorem 2

The objective function of problem (8) is convex (since
φtsa is increasing), its constraints are aﬃne, and the optimal value is obviously ﬁnite (since φtsa is ﬁnitely valued).These imply that a solution pair to (8) and (9) nec-
essarily satisﬁes the KKT conditions [Rockafellar, 1970, Cor. 28.3.1]. Let vtτs be the dual variable corresponding to the equality constraints containing pτt s, let µτtsa ≥ 0 be the dual variables corresponding to constraint ytτsa ≥ 0. Then the Lagrangian of (8) and (9) is given by

ytτsa

L(y, v, µ) =

τ,τ ≥t

t,s,a 0

φtsa(α)dα

+

(vtτs(pτts − ytτsa) − µτtsaytτsa)

τ t,t≤τ s,a

+

vtτ+1,sPs asytτs a.

τ t,t<τ s ,a,s

The KKT conditions [Rockafellar, 1970, Thm.28.3] of
this Lagrangian include the following vanishing gradient conditions (by setting ∂L/∂ytτsa equal to zero)

vττs =φτ sa

yτj sa − µττsa = 0,

j,j≥τ

vtτs =φtsa

ytjsa + Psas vtτ+1,s − µτtsa = 0,

j,j≥t

s

(A.3)

for all t ∈ [τ − 1], τ ∈ T, s ∈ [S], a ∈ [A], and the complementarity conditions

ytτsaµτtsa = 0, ytτsa, µτtsa ≥ 0,

(A.4)

for all t ∈ [τ ], τ ∈ T, s ∈ [S], a ∈ [A]. Combining (A.3) and (A.4) yields (10). Again, same results can be derived from the dual problem (9).

A.3 Proof of Lemma 2

Using a similar argument as in the proof of Theorem 1, we can show that the KKT conditions of (13) are given by the following

y1sa =p1s − z1s,
a

yt+1,sa =pt+1,s − zt+1,s + Ps asyts a, t ∈ [T − 1],

a

s ,a

vT s =uT sa − µT sa,

vts =utsa + Psas vt+1,s − µtsa, t ∈ [T − 1],
s

vts =wts + λts − θts,

ytsaµtsa =0, ztsθts = 0, λts(zts − pts) = 0,

ytsa, zts, µtsa, θts, λts ≥ 0, (A.5)
for all t ∈ [T ], s ∈ [S], a ∈ [A]. Let (vˆ, πˆ) be the output of Algorithm 1 with input (P, u, T ), zˆ = p (vˆ > w), yˆ be

the output of Algorithm 2 with input (πˆ, p − zˆ, P, T ), let
µˆT sa = − vˆT s + uT sa, µˆtsa = − vˆts + utsa + Psas vˆt+1,s , t ∈ [T − 1],
s
λˆts = max{vˆts − wts, 0}, θˆts = max{wts − vˆts, 0}, (A.6)
for all t ∈ [T ], s ∈ [S], a ∈ [A]. Then it is straightforward to verify that (yˆ, zˆ, vˆ, µˆ, λˆ, θˆ) satisﬁes all the KKT conditions in (A.5), hence (yˆ, zˆ) solves (13), which proves the equality. The inequality follows from the fact that, when (u, w) in (13) is perturbed to another value (u , w ), solution (yˆ, zˆ) is still feasible, but can be suboptimal.

A.4 Proof of Lemma 3

Notice that in optimization (14), both objective function and constraints are completely separable across yτ with diﬀerent value of τ . In other words, solving (14) is equivalent to solve the following optimization problem for each value of τ ∈ T separately

min.

utsaytτsa

yτ t≤τ,s,a

s.t.

y1τsa = pτ1s

a

ytτ+1,sa = pτt+1,s + Ps asytτs a, t ∈ [τ − 1],

a

s ,a

0 ≤ ytτsa, ∀t ∈ [τ ], s ∈ [S], a ∈ [A].

(A.7)

Since problem (A.7) is nothing but an instance of (1)

with T = τ , it can be solved by the output of Algorithm 2 with input (πˆτ , pτ , P, τ ), where πˆτ is the output of Al-

gorithm 1 with input (P, u, τ ). This proves the equality;

the inequality follows from the fact that, when u in (14) is perturbed to another value u , solution (yˆτ , τ ∈ T) is
still feasible, but can be suboptimal.

A.5 Proof of Theorem 3

We start with Algorithm 3. The per-iteration computation of Algorithm 3 is clearly dominated by the execution of Algorithm 1 and Algorithm 2, which together cost O(σT S2A) arithmetical operations. Let f (y, z) denote the objective function of problem (4). Then the gradients of f is given by
∂yf = φ(y), ∂zf = ψ(z),
where φ(y) and ψ(z) are deﬁned as in (11). Then under Assumption 3, we know both ∂∂fy and ∂∂fz are Lipschitz. In addition, for any (y, z) satisfying the constraints of problem (4), one must have ytsa ∈ [0, ρ] and zts ∈ [0, ρ], due to Assumption 1. In other words, the constraint set of problem (4) is a subset of [0, ρ]T ×S×A × [0, ρ]T ×S, which is bounded.

11

Therefore problem (4) is minimizing a function with Lipschitz gradients over a bounded set. Hence the Frank-Wolfe method given by Algorithm 3 converges to -optimal solution in O( 1 ) iterations [Bubeck et al., 2015, Thm.3.8]. The proof for Algorithm 4 is similar.
A.6 Proof of Theorem 4
We start with Algorithm 5. First, the per-iteration computation of Algorithm 5 is clearly dominated by the execution of Algorithm 1 and Algorithm 1, which together cost O(σT S2A) arithmetical operations. From Assumption 1 and 3 we have, for any utsa, utsa, wts, wts ∈ [0, ρ]
|utsa − utsa| =|φtsa(φ−ts1a)(utsa) − φtsa(φ−ts1a)(utsa)| ≤L|φ−ts1a(utsa) − φtsa(φ−ts1a)(utsa)|,
|wts − wts| =|ψts(ψt−s1)(wts) − ψts(ψt−s1)(wts)| ≤L|φ−ts1a(utsa) − φtsa(φ−ts1a)(utsa)|,
for all t ∈ [T ], s ∈ [S], a ∈ [A], which implies the objective function of problem (17) (in particular, the integral terms) is L1 -strongly convex [Nesterov, 2013, Thm.2.1.10]. Let −f (u, w) denote the objective function of problem (17). Then from Lemma 2 we know that the subgradients of f is given by
∂uf = −yˆ + φ−1(u), ∂wf = −zˆ + ψ−1(w),
where φ−1(u), ψ−1(w) are deﬁned as in (11), (yˆ, zˆ) is a solution to problem (13). From Assumption 1 we know that φ−1(u) ∈ [0, ρ]T ×S×A and ψ−1(w) ∈ [0, ρ]T ×S. Further, (yˆ, zˆ) must satisfy the constraints in problem (13), which implies that yˆ ∈ [0, ρ]T ×S×A and zˆ ∈ [0, ρ]T ×S. Hence the elements in ∂uf and ∂wf are bounded.
Therefore, problem (17) is minimizing a strongly convex function whose subgradients have bounded elements. hence the projected subgradient method given by Algorithm 5 converges to an -optimal solution in O( 1 ) iterations [Bubeck et al., 2015, Th,.3.9]. The proof for Algorithm 6 is similar.

References

[Ahipa¸saog˘lu et al., 2019] Ahipa¸sao˘glu, S. D., Arıkan, U., and Natarajan, K. (2019). Distributionally robust markovian traﬃc equilibrium. Transp. Sci., 53(6):1546–1562.

[Baillon and Cominetti, 2008] Baillon, J.-B. and Cominetti, R. (2008). Markovian traﬃc equilibrium. Math. Prog., 111(12):33–56.

[Beckmann et al., 1956] Beckmann, M., McGuire, C. B., and Winsten, C. B. (1956). Studies in the Economics of Transportation. Yale University Press.

[Bertsekas, 1996] Bertsekas, D. P. (1996). Programming. Athena scientiﬁc Belmont.

Neuro-Dynamic

[Bertsekas, 1998] Bertsekas, D. P. (1998). Network Optimization: Continuous and Discrete Models. Citeseer.

[Bubeck et al., 2015] Bubeck, S. et al. (2015). Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3-4):231–357.

[Bu¨rger et al., 2014] Bu¨rger, M., Zelazo, D., and Allg¨ower, F. (2014). Duality and network theory in passivity-based cooperative control. Automatica, 50(8):2051–2061.

[Calderone and Sastry, 2017a] Calderone, D. and Sastry, S. (2017a). Inﬁnite-horizon average-cost markov decision process routing games. In Proc. Int. Conf. Intell. Transp. Syst., pages 1–6. IEEE.

[Calderone and Sastry, 2017b] Calderone, D. and Sastry, S. (2017b). Markov decision process routing games. In Proc. Int. Conf. Cyber-Phys. Syst., pages 273–279. ACM.

[Calderone, 2017] Calderone, D. J. (2017).

Models of

Competition for Intelligent Transportation Infrastructure:

Parking, Ridesharing, and External Factors in Routing

Decisions. PhD Thesis.

[Cominetti et al., 2012] Cominetti, R., Facchinei, F., and Lasserre, J. B. (2012). Wardrop and stochastic user equilibrium. In Modern Optimization Modelling Techniques, pages 213–220. Springer.

[Correa and Stier-Moses, 2010] Correa, J. R. and Stier-Moses, N. E. (2010). Wardrop equilibria. Wiley Encyclopedia of Operations Research and Management Science.

[Daganzo and Sheﬃ, 1977] Daganzo, C. F. and Sheﬃ, Y. (1977). On stochastic models of traﬃc assignment. Transp. Sci., 11(3):253–274.

[Dial, 1971] Dial, R. B. (1971). A probabilistic multipath traﬃc assignment model which obviates path enumeration. Transp. Res., 5(2):83–111.

[Fisk, 1980] Fisk, C. (1980). Some developments in equilibrium traﬃc assignment. Transp. Res. B, 14(3):243–255.

[Frank and Wolfe, 1956] Frank, M. and Wolfe, P. (1956). An algorithm for quadratic programming. Naval Res. Logist. Quart., 3(1-2):95–110.

[Gartner, 1980a] Gartner, N. H. (1980a). Optimal traﬃc assignment with elastic demands: A review, Part I. Analysis framework. Transp. Sci., 14(2):174–191.

[Gartner, 1980b] Gartner, N. H. (1980b). Optimal traﬃc assignment with elastic demands: A review, Part II. Algorithmic approaches. Transp. Sci., 14(2):192–208.

[Gomes et al., 2009] Gomes, D. A., Mohr, J., and Souza, R. R. (2009). Discrete mean ﬁeld games. arXiv preprint arXiv:0903.1620.

[Gomes et al., 2010] Gomes, D. A., Mohr, J., and Souza, R. R. (2010). Discrete time, ﬁnite state space mean ﬁeld games. J. Math. Pures Appl., 93(3):308–328.

[Gu´eant, 2011] Gu´eant, O. (2011). From inﬁnity to one: The reduction of some mean ﬁeld games to a global control problem. arXiv preprint arXiv:1110.3441.

[Gu´eant, 2015] Gu´eant, O. (2015). Existence and uniqueness result for mean ﬁeld games with congestion eﬀect on graphs. Appl. Math. Optim., 72(2):291–303.

[Li et al., 2019] Li, S. H., Yu, Y., Calderone, D., Ratliﬀ, L., and A¸crkme¸se, B. (2019). Tolling for constraint satisfaction in markov decision process congestion games. In Proc. Amer. Control Conf., pages 1238–1243. IEEE.

[Liu et al., 2009] Liu, H. X., He, X., and He, B. (2009). Method of successive weighted averages (mswa) and self-regulated averaging schemes for solving stochastic user equilibrium problem. Networks and Spatial Economics, 9(4):485.

12

[Nesterov, 2013] Nesterov, Y. (2013). Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media.
[Nilim and El Ghaoui, 2005] Nilim, A. and El Ghaoui, L. (2005). Robust control of markov decision processes with uncertain transition matrices. Oper. Res., 53(5):780–798.
[Patriksson, 1994] Patriksson, M. (1994). The Traﬃc Assignment Problem: Models and Methods. Courier Dover Publications.
[Puterman, 1994] Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons.
[Rockafellar, 1970] Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press.
[Rockafellar, 1984] Rockafellar, R. T. (1984). Network Flows and Monotropic Optimization. John Wiley & Sons.
[Sheﬃ and Powell, 1982] Sheﬃ, Y. and Powell, W. B. (1982). An algorithm for the equilibrium assignment problem with random link times. Networks, 12(2):191–207.
[Tamar et al., 2016] Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In Proc. Adv. Neural Inf. Process. Syst., pages 2154–2162.
[Tanaka et al., 2020] Tanaka, T., Nekouei, E., Pedram, A. R., and Johansson, K. H. (2020). Linearly solvable mean-ﬁeld traﬃc routing games. IEEE Trans. Autom. Control.
[Von Neumann and Morgenstern, 1953] Von Neumann, J. and Morgenstern, O. (1953). Theory of games and economic behavior. Princeton University Press.
[Wardrop and Whitehead, 1952] Wardrop, J. G. and Whitehead, J. I. (1952). Correspondence. some theoretical aspects of road traﬃc research. Proc. Inst. Civil Eng., 1(5):767–768.
[Xiao et al., 2004] Xiao, L., Johansson, M., and Boyd, S. P. (2004). Simultaneous routing and resource allocation via dual decomposition. IEEE Trans. Commun., 52(7):1136–1144.
13

