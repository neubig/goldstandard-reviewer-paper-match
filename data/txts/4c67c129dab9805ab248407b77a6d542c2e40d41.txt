Adversarial Crowdsourcing Through Robust Rank-One Matrix Completion

arXiv:2010.12181v1 [cs.LG] 23 Oct 2020

Qianqian Ma Boston University maqq@bu.edu

Alex Olshevsky Boston University alexols@bu.edu

Abstract
We consider the problem of reconstructing a rank-one matrix from a revealed subset of its entries when some of the revealed entries are corrupted with perturbations that are unknown and can be arbitrarily large. It is not known which revealed entries are corrupted. We propose a new algorithm combining alternating minimization with extreme-value ﬁltering and provide sufﬁcient and necessary conditions to recover the original rank-one matrix. In particular, we show that our proposed algorithm is optimal when the set of revealed entries is given by an Erdo˝s-Rényi random graph.
These results are then applied to the problem of classiﬁcation from crowdsourced data under the assumption that while the majority of the workers are governed by the standard single-coin David-Skene model (i.e., they output the correct answer with a certain probability), some of the workers can deviate arbitrarily from this model. In particular, the “adversarial” workers could even make decisions designed to make the algorithm output an incorrect answer. Extensive experimental results show our algorithm for this problem, based on rank-one matrix completion with perturbations, outperforms all other state-of-the-art methods in such an adversarial scenario.1
1 Introduction
Matrix completion [10] [9] [13] refers to the problem of recovering a low-rank matrix from a subset of its entries. A fundamental challenge in the study of matrix completion is that, in some applications, the revealed entries will be inaccurate or corrupted. When these perturbations can be arbitrarily large, we will refer to the problem as “robust matrix completion.” In particular, the motivating application for this paper is estimation of worker reliability in crowdsourcing [30] [41] [21][19][45], where this issue appears if some workers deviate from their instructions.
The simplest and one of the most widely used crowdsourcing models is Dawid & Skene’s (D&S) single coin model [6]. In the D&S model, the workers are assumed to make mistakes independently of other workers and with the same error probability for each task. It has previously been observed that optimal estimation in the D&S model requires estimation of worker reliabilities [1], which in turn can be framed as a rank-one matrix completion problem [30].
In the case when some of the workers are poorly described by the D&S model or even consciously acting to subvert the underlying algorithm, the problem is naturally framed as a robust rank-one matrix completion problem. The motivating scenario is either a platform such as Amazon Mechanical Turk where workers without a long record of reliability typically have cheaper rates, or estimation from online ratings on a platform such as Yelp, where there is strong incentive for business owners to write fake reviews.
1The code is available on https://github.com/maqqbu/MMSR
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

We propose a new algorithm for robust rank-one matrix completion which, in at least one regime, is provably optimal. We then perform a computational study which compares our method to twelve state-of-the-art methods from the crowdsourcing literature on both synthetic and real world datasets (in the latter case, we introduce the corrupted workers ourselves), and show that our method strongly outperforms all of them in this adversarial setting.
Notation and conventions: [n] = {1, · · · , n}; |S| is the size of set P ; x is the smallest integer greater than x; x is the largest integer smaller than x; X ∗ is the nuclear norm of matrix L, i.e., the sum of the singular values of matrix X; Z+ is the set of positive integers; Z≥i is the set of integers which are greater than i; Given S1, S2, the reduction of S1 by S2 is denoted as S1\S2 = {i ∈ S1 : i ∈/ S2}; ﬁnally, A(n) ≈ B(n) means A(n)/B(n) → 1 as n → ∞.

2 Related Work

Matrix Completion: the standard approach to low-rank matrix completion [2][3] usually proceeds by nuclear norm minimization:

minL L ∗

s.t. [L]ij = [L0]ij, ∀(i, j) ∈ Ω,

(1)

where L ∗ is the nuclear norm of matrix L, Ω is the set of locations of the observed entries, L0 is the matrix to be recovered. Candès and Recht [2] proved that L0 can be recovered with high probability via solving (1) if L0 is incoherent and Ω is sampled uniformly at random. These are strong assumptions and many papers, including this one, have sought to relax them. A popular approach has
been to focus on non-uniform sampling. In particular, Negahban et al. [31] relaxed the condition
of uniform sampling to weighted entrywise sampling. Király et al. [20] considered deterministic
sampling. Liu et al. proposed a new hypothesis called "isomeric condition" in [27], which is weaker than uniform sampling, and proved that the matrix L0 can be recovered by a nonconvex approach under this condition.

Unlike these general methods, we consider rank-one matrix completion problem with arbitrary Ω as well as adversarial corruptions. We do not assume any kind of incoherence of the underlying matrix. Of course, the rank-one matrix completion problem for uncorrupted cases is trivial and can be solved by going through the revealed answers recursively. Nevertheless, of particular note to us is Gamarnik et al. [10] which studied how well an alternating minimization methods in the uncorrupted case; we will build on those results in this work. Very closely related to our work is the recent paper Fatahi et al. [9], which considered solving the robust rank-one matrix completion with perturbations by solving the optimization problem

minu∈Rm + ,v∈Rn+ PΩ(X − uv ) 1 + Rβ (u, v),
where Ω represents the set of observed entries and Rβ represents a regularization term. The approach in [9] can deal with asymmetric matrices as well as sparse unknown perturbations. However, their approach has strong assumption for the structure of the graph G(Ω) and can only deal with sparse perturbations.

Crowdsourcing: For crowdsourcing problems [46] [39] [35], the D&S model has become a standard theoretical framework and has led to a ﬂurry of research in recent years, e.g., [41][11][15] among many others. In the D&S model, the workers are assumed to make errors independently with each other and the error probabilities of the workers are task-independent. In the single-coin D&S model, each worker is further assumed to have the same accuracy on each task.

Ghosh et al. [11] considered a binary classiﬁcation problem based on single coin D&S model, and a SVD-based algorithm was proposed. The underlying assumption was that the observation matrix (representing which workers give answers for which tasks) is dense. To relax this constraint, Dalvi et al. [5] proposed another SVD-based algorithm which allows the observation matrix to be sparser. Karger et al. [18] extended the single coin D&S model to multi-class labeling issues, and they proposed an iterative algorithm to solve it. Zhang et al. [41] developed a two-stage spectral method for multi-class crowdsourcing labeling problems based on general D&S model. In [30], the skill estimate of workers in single one-coin D&S model were formulated as a rank-one matrix completion problem, which is also the approach we will adopt in this paper. We mention that in Section 6, we will compare our proposed algorithm with these existing approaches from [11][5][18][41][30].

2

The adversarial version of the D&S model, where adversaries represent workers who deviate from the model, has previously been investigated in a number of papers. Raykar et al. [34] assumed the adversaries are workers who give answers randomly, and they proposed approach to eliminate such adversaries in general D&S model. Jagabathula [17] also considered the speciﬁc types of adversaries (e.g all the adversaries provide a label +1) and tried to detect these adversaries from normal workers and eliminate their impact to the ﬁnal predictions. Kleindessner et al. [21] proposed an approach to deal with arbitrary and colluding adversaries in the David&Skene model. Overall, the algorithm in [21] can deal with cases when nearly half of the workers are adversaries. However, this theoretical guarantee requires the task assignment matrix to be full matrix or dense matrix. Our work considers the same problem as [21] but without assuming the task assignment matrix is dense.

3 Problem Setup and Formulation

Let X = M + P ∈ Rm×n, where M = ab is a positive rank-1 matrix with a = [a1 a2 · · · am] , b = [b1 b2 · · · bn] . Let XΩ be a subset of the entries of X, i.e.,

XΩ = {Xij = Mij + Pij| ∀(i, j) ∈ Ω},

(2)

where Ω ⊂ [m] × [n]. Our goal is to efﬁciently reconstruct M given XΩ. The elements of the matrix P can take on any value; if Pij = 0 we will say that the (i, j)’th entry of X is “corrupted.” An entry that is not corrupted will be referred to as “normal.” We will be considering the situation where a number of rows and columns of X are completely corrupted, and our goal is to correctly recover the remaining rows and columns.
We begin with a sequence of deﬁnitions. The structure of the set of pairs Ω can be conveniently represented by an undirected bipartite graph G(Ω) as follows:
Deﬁnition 1. G(Ω) is deﬁned to be a bipartite graph with vertex partitions Vu := {1, 2, · · · , m} and Vv := {1, · · · , n} and includes the edge (i, j), with i ∈ Vu, j ∈ Vv, if and only if (i, j) ∈ Ω.
For i ∈ Vu, we let Ωi denotes the set of neighbors of node i, and similarly for j ∈ Vv, let Ωj denote the set of neighbors of node j (the apostrophe will be useful to be able to tell at a glance whether a node belongs to Vu or Vv). Our next step is to formalize the fault model, i.e., what we will be assuming about the corruption matrix P .
Deﬁnition 2. A set S ⊂ V (G) is F -local if its intersection with each Ωi and each Ωj has at most F nodes.
Deﬁnition 3. A node i ∈ Vu is corrupted if Pij = 0 for some (i, j) ∈ Ω. Likewise, a node j ∈ Vv is corrupted if Pij = 0, for some (i, j) ∈ Ω. The graph G(Ω) is said to be F-local corrupted if the set of corrupted nodes is F-local.
Next, we will introduce some concepts from [25] dealing with the redundancy of edges between subsets of nodes; later, these will turn out to be closely related to the robustness of the graph G(Ω) to corruptions.
Deﬁnition 4. A set S ⊂ V (G) is an r-reachable set if there exists a node in S with at least r neighbors outside S. G(Ω) is r-robust if for every pair of nonempty, disjoint subsets of V (G), at least one of the subsets is r-reachable.
Crowdsourcing: we now explain the connection between robust rank-one matrix completion and crowdsourcing. We consider the single-coin D&S model where W workers are asked to provide labels for a series of M -class classiﬁcation tasks. The ground truths gt (t ∈ [T ]) for these tasks are unknown (here T is the number of tasks). The set A ⊂ [W ] × [T ] is a worker-task assignment set. The observations (Yw,t)(i,t)∈A are a collection of independent random variables. The single-coin D&S model supposes the accuracy of the worker i is pi which means that the answer it returns is:

P(Yi,t = |gt) = pi1{ =g } + 1 − pi 1{ =g }.

t M −1

t

In words, worker i returns the correct answer with probability pi and a random incorrect answer with probability 1 − pi. Under such assumptions, it was observed [30] that the probability of each worker can be estimated via solving a rank-one matrix completion problem as follows: letting

3

si

=

MM−1 pi

−

1 M −1

we

have

that

[30]

M

1

E

C−

11 = ss ,

(3)

M −1 M −1

where C is the covariance matrix between agents i and j, 11 is the all-ones matrix which has the same size as C. Since the RHS of (3) is a rank-one matrix, the skill level vector s can be estimated by computing the empirical covariance matrix and applying a rank-one matrix completion method.
In the adversarial setting, we need to further consider the case when worker i may deviate from the D&S model. In that case, all the entries in row i and column i should be viewed as corruptions as the derivation of Eq. (3) is no longer valid for that row; this is why we adopt a model in this paper where entire rows/columns are corrupted. Naturally, we do not know which rows/columns are corrupted. Extending our notation from above, we will refer to uncorrupted rows and columns as normal.
The hope is that identiﬁcation of skill levels for the uncorrupted agents is still possible if the number of corrupted agents is not too large, or the corrupted agents are not placed in central location in the graph G(Ω); making this intuition into a precise theorem is one of the goals of this paper.
With the above background in place, we can now state the main concerns of our paper formally: (i) Given XΩ, how can we reconstruct the normal rows and columns of the rank-one matrix M under F -local fault-models? (ii) How can we estimate the workers’ skill level s and consequently give accurate predictions for tasks in the single-coin D&S model with adversaries?

4 The M-MSR method

In this section, we present the details of our approach. We will start by explaining how our algorithm
was constructed. For the uncorrupted rank-one matrix completion problem, we begin by observing
that if (a, b) is an optimal solution, then we actually have the group of optimal solutions (u, v) = (ka, k−1b). On the other hand, given arbitrary positive vectors u, v, we can represent them as

u = [a1k1 a2k2 · · · amkm] ,

v=

b1 k

b2 k

· · · kbn

,

1

2

n

where ai, bj represent the values of optimal solution (a, b). We will refer to ki as the “value” of vertex i and likewise for kj. Observe that to ﬁnd the optimal solution, we need some algorithms to update u and v so that all the vertices can have the same value. To accomplish this, our starting point
is the update

ui(t + 1) =

wij Xij , j∈Ωi vj (t)

∀i ∈ [m],

Xij vj(t + 1) = i∈Ωj wij ui(t + 1) , ∀j ∈ [n], (4)

where the coefﬁcients wij, wij form a convex combination. This update rule is motivated by [10], to which it is closely related, and can be interpreted in terms of an minimization method which alternates
between ﬁnding the best u and the best v. In the unperturbed case, via elementary algebra this can be
rewritten in terms of the variables ki, kj introduced above as

ki(t + 1) =

wijkj(t), ∀i ∈ [m],
j∈Ωi

1

1

kj(t + 1) = i∈Ωj wij ki(t + 1) , ∀j ∈ [n]. (5)

We will show with update rule (5), all the vertices can converge to the same value.

The main difﬁculty is what to do to account for corruptions: since the corrupted elements can be arbitrary, even a single corrupted element can completely destabilize this iteration. A natural approach is to ﬁlter the extreme values in each update of Eq. (4). To that end, let us deﬁne
Ji(t) = Xij j ∈ Ωi . vj (t)

4

We will then set Ri(t) to be the set of nodes with the F largest and smallest values in Ji(t) (if there are fewer than F values strictly smaller/larger than ui(t), then Ri(t) contains the ones that are strictly smaller/larger than ui(t)); the quantities Jj(t), Rj(t) are deﬁned similarly for nodes j ∈ Vv. Our algorithm is presented next; we will call it the Matrix-Mean-Subsequence-Reduced (M-MSR)
algorithm.

Algorithm 1 M-MSR
Input: Positive matrix X, set Ω, F and v(0) > 0 Output: Xˆ = u(T )v(T ) 1: for t = 1, 2, . . . , T do 2: For each i = 1, · · · , m, let
ui(t + 1) = wij Xij , (6) j∈Ωi\Ri(t) vj (t)
where the coefﬁcients wij form a convex combination. 3: For each j = 1, · · · , n, let
vj (t + 1) = wij Xij , (7) i∈Ωj \Rj (t) ui(t + 1)
where the coefﬁcients wij form a convex combination. 4: end for 5: return u(t) = [u1(t), u2(t), · · · , um(t)] , v(t) = [v1(t), v2(t), · · · , vn(t)]

For convenience, we will not consider the case m = n = 1. We will be assuming that G(Ω) is connected. Finally, introducing the notation α for the smallest of wij, wij, we will be assuming that α ≤ 1/2. This can easily satisﬁed by e.g., choosing wij = 1/degree(i) and likewise for wij.

5 Convergence Analysis

We begin by considering the case where the revealed entry set Ω is randomly chosen, which corresponds to the case of G(Ω) being an Erdo˝s-Rényi bipartite graphs. For simplicity, we assume m = n.

Theorem 1(a). Suppose G(Ω) is a random bipartite graph Gn,n,p where each edge is generated with probability p. Then a sufﬁcient condition for M-MSR algorithm (with parameter F ) to successfully recover the normal rows and columns of X under the assumption the the corruptions are F -local is

log n + 2F log log n + x

p≥

,

(8)

n

where x = o(log log n) → ∞, when n → ∞.

Thus, on a random graph, the M-MSR method can successfully recover the true matrix provided the graph is not too sparse. We will later discuss the guarantees this theorem implies on the total fraction of corruptions (note that F is an upper bound on the number of corruptions in each neighborhood). For now we observe that Theorem 1(a) is tight in a sense described next.

As we will argue in the supplementary information, M-MSR has the property that it is skew nonamplifying in the following sense: at every step of the method, it maintains estimates u(t), v(t) such that ui(t)vj(t) converges to the correct answer aibj when i, j are normal and which satisfy

min ui(t) , bi ≥ min ui(0) , bi , (9)

normal i ai vi(t)

normal i ai vi(0)

max ui(t) , bi ≤ max ui(0) , bi , (10)

normal i ai vi(t)

normal i ai vi(0)

Intuitively, the quantities ui(t)/ai and bi/vi(t) measure the “skew” between the vectors u(t), v(t) and the optimal solution. Let us call any algorithm that satisﬁes Eq. (9) and Eq. (10) skew-nonamplifying.
Skew nonampliﬁcation is clearly a desirable property. In principle, any algorithm for robust rank-1 matrix factorization can converge to (αa)(α−1bT ) where abT is the true rank-1 matrix and α can

5

be any real number. It is natural to bound how large the constants α and α−1 can be. The skewnonamplifying property does that by ensuring the ﬁnal skew is not worse than the skew on the initial conditions.

Our next result shows that M-MSR is essentially optimal on random graphs among all skewnonamplifying methods.

Theorem 1(b). Suppose G(Ω) is a random bipartite graph Gn,n,p where each edge is generated with probability p. Suppose

log n + 2F log log n − x

p=

,

n

where x = o(log log n) → ∞, when n → ∞. Then, with probability approaching 1 as n → ∞, the normal rows and columns of X cannot be recovered by any skew-nonamplifying algorithm in the presence of F -local corruptions.

In other words, if Eq. (8) just barely fails due to the replacement of x by −x, then Theorem 1(b) tells us that Gn,n,p will, with high probability, be a graph for which there exists a set of F -local corruptions which prevent any skew-nonamplifying algorithm from recovering the true matrix X.

Theorem 1 in parts (a) and (b) provides a justiﬁcation for the M-MSR algorithm: it is essentially an optimal algorithm to use on bipartite random graphs. This theorem is actually derived from the following somewhat more general theorem, which gives the exact conditions for the M-MSR algorithm to work on an arbitrary graph.

Theorem 2. Suppose G(Ω) is a connected graph where the nodes are updated according to M-MSR algorithm with parameter F . Under F -local nodes-corrupted model, the rows and columns of X without corruptions can be correctly recovered by the M-MSR method if and only if G(Ω) is 2F + 1-robust.

Theorem 2 gives substance to the intuition that recovery is possible if the number of adversarial agents is not too large, and if their placement in the graph is not central. It quantiﬁes this intuition through the concept of 2F + 1 robustness. The proofs of these theorems can be found in the supplementary information. Our results are connected to earlier work in resilient consensus [25] which introduced the concept of robustness, as well as the work [40] which analyzed the threshold of robustness in general random graphs.

5.1 Applying M-MSR to crowdsourcing

We have already spelled out how skill determination in crowdsourcing with adversaries can be reduced to rank-one matrix completion with perturbations. Here we discuss additional details that are needed to apply the M-MSR method.

Prediction. When the skills are known, according to [26], the optimal prediction method under D&S model is weighted majority voting, i.e.

γs∗,A(Y ) = arg max

vi∗1{Yi,t = },

(11)

∈[M ] i:(i,t)∈A

where vi∗ = log (M1−−p1i)pi , ∀i ∈ [W ]. As discussed above, we will use perturbed rank-one matrix completion to estimate the skills.

Implications of Theorem 1. Consider the case where a total of βn adversaries exist among n workers, where β < 1. Of course, it is unknown who is an adversary. As explained earlier, a certain correlation matrix between the normal workers is rank-1 in expectation. Of course, we may not know all the entries of this correlation matrix, since only correlations among workers with tasks in common are revealed. A natural approach is to create a random bipartite graph of revealed entries by assigning tasks randomly.

This can be done in a number of ways. We may, for example, generate a random bipartite graph G from Gn,n,p ﬁrst. Then, assuming there is a sufﬁciently large incoming stream of tasks, we assign each task to a random pair of workers i and j such (i, j) is an edge in G. After each pair of agents has been assigned enough tasks, the empirical correlation matrix is approximately rank-1, and we reveal the entries of this matrix corresponding to G. Note that, even though the correlation matrix is symmetric, this method will reveal an asymmetric subset of entries. Other methods to generate the

6

graph randomly via random task assignment are also possible, for example by assigning tasks to more than two workers. The key point, however, is that the fraction of adversaries in every neighborhood will then concentrate around β: for each node i, each of its randomly chosen neighbors is adversarial with probability β.
How many adversaries can we have and still correctly recover the skills of all the normal workers? Unfortunately, using the strategy of the previous paragraph, any constant fraction β of adversaries will result in a failure. Indeed, glancing at Eq. (8), if F scales as βpn (pn is the expected degree, and an expected fraction β of these nodes will be adversarial), then Eq. (8) can never be satisﬁed.
Although this sounds discouraging, the guarantees of Theorem 1 are still useful, as we explain now. Glancing at Eq. (8), it is easy to see that the choice of β = 1/[(2+ ) log(log(n))] (and corresponding F = βpn) leads to that equation being satisﬁed with the choice of p = Ω((1 + −1) log n)/n. In other words, we can tolerate a fraction of 1/[(2 + ) log(log(n))] of adversaries.
Fortunately, 1/ log(log(n)) decays to zero quite slowly. Recall that in the crowdsourcing scenario, n will be the number of users; using an upper bound of 10 billion people for the population of planet Earth, we see that on any real-world data set, we have 1/[2 log(log(1010))] ≈ 16%, which is a healthy proportion of adversaries to tolerate.
Sign Determination. In the M-MSR algorithm, we assume the rank-one matrix to recover is positive. However the rank-one matrix which we aim to recover in crowdsourcing problem is not necessarily positive. In fact, a worker’s skill level si ∈ [− M1−1 , 1] as si = MM−1 pi − M1−1 , pi ∈ [0, 1]. To solve this issue, we can compute the entry-wise absolute value of the rank-one matrix, then apply M-MSR to get |s|, ﬁnally, we apply a post-processing step to identify the sign pattern of s. Details are available in Supplementary Sec. I.
6 Experiments
In several crowdsourcing experiments, we will compare the average prediction error ( T1 t=1,··· ,T {Yˆt = gt}) of M-MSR algorithm with the straightforward majority voting (referred to as MV) and the following methods from the literature: [18](KOS), [11](Ghost-SVD), [5]( EoR), [41] ( MV-D&S and OPT-D&S), [30](PGD), [28](BP-twocoin, EM-twocoin, and MFA-twocoin), [42](Entropy(O)), [44] (Minmax) . A detailed description of all of these methods can be found in Supplementary Sec. A. In all cases we choose the corrupted workers at random, and the reported results are from an average of 50 runs, with the shaded region denoting the standard deviation.
Synthetic Experiments. Figure 1 shows the results of a number of experiments we have generated. We want to study the impact of graph types (Figure 1(b), 1(d)), task assignments (Figure 1(c), 1(f)) and skill level of “normal” workers (Figure 1(e)) to the performance of the crowdsourcing methods. Additionally, we want to study the impact of different adversarial strategies (Figure 1(g), 1(h), 1(i), 1(k), 1(j)). To do this, we applied an adversarial model, by varying the parameters of this model, we can generate a number of different adversarial strategies (e.g., always return the wrong answer, return a random answer, return a certain fraction of the correct answer, some of the adversaries return exactly the same answers, some adversaries return the perfectly colluding answers, assign each adversary with every task, assign each adversary with every task with a ﬁxed probability). The details about the adversarial model as well as a deﬁnition of these parameters appears in Supplementary Sec. C.
Each graph in Figure 1 shows what happens when we vary one parameter. It can be seen that the M-MSR algorithm strongly outperforms the baseline methods on various datasets and under almost all of the adversarial strategies. Besides, the most damaging strategy in terms of reducing prediction error across the methods we tried seems to return a correct answer a fraction q < 1/2 of the time and an incorrect answer 1 − q of the time, and the adversaries should locate at the central places and be highly dependent with each other.
Experiments on real data. We implemented similar experiments on 17 publicly available data sets that are commonly used to evaluate the crowdsourcing algorithms. A detailed discussion of all the datasets can be found in Supplementary Sec. E, and the details of the how the experiments were conducted can be found in supplementary Sec. D. As shown in Figure 2 and Figure 6 (Supplementary Sec. D), the M-MSR algorithm consistently outperforms all the baseline methods. In particular, when the number of the corrupted workers increases, the prediction error of M-MSR algorithm maintains the smallest on almost every dataset.
7

Prediction error Prediction error

PPrreeddiiccttiiPPoPoPrrrrnneeeeddeeiiddirirccrrccttiittooiirroooonnnneeeerrrrrrroooorrrr PPrreeddiiccttiiPoornnePrdeeirrerrcdtiooirrcotinoenrreorrror

MVMMVVMMVVKOKSKOOSKKSOOSSGhGoGhsohtGSGsohtVshSoDtosVSstDVStSDVVDDEoERoERREoEoRoRRMVMM-MVDVM--V&MDDV-S&D-V&DS-&SD&S&SS PGPPDGPGPDGDGPDGDD OPOOTPOOP-TDPTPO--&DTTPD--S&TDD&-S&&SDSS&S BPBB-PBBtPw-PPB-towt--Pwtctwow-otcowicooncocoioniocninionin EMEEEM-EMtMw-EMt--wMott-wwcto-wotocwicoconocoiniocinnoinin MMFMMAFMF-FAMtAAFw-F-tA-wtoAtw-wcto-wootcwciocnooocinicnoinoinin EnEEtEnnrEonttErrpntoorntyopprt(orpyOyo(py(Op)O(yOy))(O()O)) MMMinMiimnnMMimmnaiinmxanamxmxaxaaxx MMM-M--MMMS-MS-SR-MRMRS--tS-SRwttwRw-oto-wcotcwocooioocniniconoinin MMM--MM-MS-SMS-RRMRSSRR

1

1 1111

MMVVMMVV KOKKSOOSS GGhGhGoohhssototSsSstVtVSSDDVVDD EEoEoERoRoRRMV MMMVV-VD--KD&DO&S&SSS

10.8

0.08.01800.81..881

1 11111

111111

11111

PGPPGhGDGoDDDstSVDOOOPOPTPP-TTTD---D&DDE&So&&SRSS BBMPBBP-PVPt-w-t-wDttoww&ococSococionioninin PGEEMDEMEM--Mttw-wt-owtowcocoOoocicoinPnoiTni-nD&SMMMFFAMF-AFtw-ABtw-oPtcwo-otcowioncoioncinoin EnEtnrEootrpnEpoytMypr((oOyO-p(t)O)wy)(oOc)oinMMiMninmimnMamaixMnxamxFAax-twMMo-Mc-MMo-SiMMnSRRS--Mt-Rwt-wSotwRocEco-tonicwnitonrooincpoyi(nOM)M-M-MS-MSRMRSM-RMinSmRax

0.0801.100.808..8.1818

0.08010.18.0.0188..1881

000.81..01880.81.8 1

1

PPPrrreeedddiiiccctttiiiPPPPPrrrrroooPeeeeernnnddedddiiiiieeedccrrrccicttrrrtttiiiciioootooiooorrrnnonnnneeeeerrrrerrrrrorooorrrror Prediction error

PPPPPrrrrreeeeedddddiiiiiccccctttttiiiiiPPPPPrrorrrooooPeeneeernnnnddedddiieiiieeeerdccrrrrccicrttrrrrtttiiiciioooootoorioororrrnnonnnneeeeerrrrerrrrrorooorrrror

PPPrrreeedddiiiccctttiiiPPPPPrrorrrooPeeneeernnddedddiieiiieerdccrrccicrttrrtttiiiciioootooriooorrnnonnnneeeeerrrrrerrrrrroorooorrrrror

PPPrrreeedddiiiccctttiiiPPPPPrrorrrooPeeneeernnddedddiieiiieerdccrrccicrttrrtttiiiciioootoorioorornnonnnneeeeerrrrrerrrrrroorooorrrrror

0.80.6

0.60.4

0.40.2

0.2 0 0.02 0.04 0.06 0.08 0.1

(b) Mean of observation sparsity

1 0111110.02 G0.r0a4ph0.0s6pa0r.s0i8ty0.1

Mean of observation sparsity

0.8

0.080.0.1880.18.811

0.6

0.00600..0.866.0.806.0.68.8

0.4

0.040.0.440.4.4

00.6.6000..6.26

0.020.0.220.2.2

00.4.400.4.04

000.020.0200000.002.020 Accura0c.050y.0.550o.50.f5.5adversa1rie11s111

AcAAcAccuAccrccuuacurrcaauryaccracyyocyfooyaoffdofaafvaddeadvvrdeevsverrasserraasriesarrisiareeiressiess

00 00

00 00

0.05.050.5.5

1 111

AAccAcAucurccarcuacurcyrayacocyofyfaoodaffvdaaevddrevsvreaesrrasiseraaiserriiseess

PPPrrreeedddiiiccctttiiiPPoPPooPrrrrrnnneeeeePddredieeiddiirirreccrrrcccttdtiittoooiiirrroocoootnninnnoeeeeerrnrrrrrrreoooroorrrrror

0.006..0086000..068....686

000.06.0.8.8000.6006....86.686

000.0.6080.0.80.6.0.0866.0..866.8

0000.0..86.8.00660..68.06.8

0.8

Prediction error

(c) (d) (e) (f) 0.004..0064000..046....464

000.04.0.6.6000.4004....64.464

000.0.4060.0.60.4.0.0644.0..644.6

0000.0..64.6.00440..46.04.6

0.6

0.0020..0.0422000..024....242

000.02.0.4.4000.2002....42.242

000.0.2040.0.40.2.0.0422.0..422.4

0000.0..42.4.00220..24.02.4

0.4

0 00000 500 1000 1500 2000 2500 0 00000 50 100 150 200 000000 0.2 0.4 0.6 0.8

00000 10 20 30 40

0.020.02..25055005050001001100100100#0010o05101f0515t010a5050s020k0020s200200200002005202052502050500000 00..2200..22 5055050500 1#01100w10100o00r0k1e5r11s05151050500 2022020200000 00.02.20.2.020.S2.02k0.20i.l2l.2in0.t04e0.4r0.v4.a40l .0u60.p60.p6.6e0r.08b0.8o0.8.u8nd 00..220.20.21#10w0110o0rke2r20s022i20n00gr3o30u03p33000B 4400444000 0.2

# ##o#fo#otfofaotftsafatkstassaksksskskss

##w##w#owrwowokroerkokrrerksekrersesrrss

SSkSiklSSklilkiiklnlliilntlientinretvertaevrvlarvaul aluplupuppepperperberborboubounoundundndd

##w#w#oworwkorkoerekrksreserinrsrisnsigniignrnorggougrruoproopuuBuppBpBBB

0 001 1 1111

505N05000u01m01010b000e0010r5101o505f00200t0a202s00k00s20052025050000

00 010 1 1111

55005550001G00r10a01p10100h5000s0i1z2510e1051050500025220202000

1001011011000.52.002.02.2S00.k.4014i.004l0.l400l.e.606v.106e5.6l00..80.80.2800

# o##footfafsttakasskkss

## o#w##fwotwwarokosroekrrkreskseresrrss

SSkSkilSikllikilnilniltiltn#eeitrnrewvvtraeaovlrlravukulapepulprpsueepprrepbbreoobruobnuodnudnd

01011T10a10s.02k110-01a00s1.s40ig220n020m0.62e0n330t0.380v3a04r40i0-40 40 0
Ski#l#l w#iwnotwo#errokrkwvereakrorslesrurkisnipeniprgngserrogrionurbuopgopuruBpoBnuBdp B

Prediction error

PPPrrreeedddiiiccctttiiiPPPPoooPrrrrrnnneeeeePddrdieeeiddiiirrreccrrrcccttdtiittioooiiirrroocoootnninnnoeeeeerrnrrrrrrrreoooroorrrrror Prediction error

PPPPPrrrrreeeeedddddiiiiiccccctttttiiiiiPPoPPooooPrrrrrnnnnneeeeePddreedieeeiddiirirrrreccrrrrrcccttdtiittooioooiiirrrrroocoootnninnnoeeeeerrnrrrrrrrreoooroorrrrrror

PPrreeddiiccttiiPPPPooPrrrrrnneeeeePddrdieeiddiiirreccrrcccttdtiittiooiiirroocoootnninnnoeeeeerrnrrrrrrrreoooroorrrrrror

0.8

0.8

0.8

ance0.8

0.080.10.1880.81.81

0.0801.10.808..1818

0.0810.180.018..1881

00.81.0180.81.8 1

1

0.6

0.6

0.6

0.6

00.0060..8.0.086600..068...68

000.06.0.8.800.6006...8.686

000.0.608.0.80.60.086.0..866.8

000.0..868.00600..68..066.8

0.8

0.4

0.4

0.4

0.4

0.040.0.440.4.4

0.040.0.404..44

0.040.40.04..44

00.4.0400.4..44

00..60600.06...62

00..66000...626

00.060.60..62.06.6

000..6.602.60.6

0.6

0.020.0.220.2.2

0.020.0.202..22

0.020.20.02..22

00.2.0200.2..22

00..4040.04..40

00..4400..404

00.04.40.40.04.4

00..4400.40.4

0.4

000..202000.020.0A.02.d020.v0.2200e.2..r22s0a.040r.0y.4400.4o..440b.s060e.0.6060r.v.6.66a0t.0i80o.0.8n080..8.8s8parsity 000..2200000.0.20020000

10 20 30 1#01c101o010r0ru2p02t2e0202d00w3o03r3k0303e00rs

00.002.F20.02r0.0a2000.c2.0t100i.o.10.011n..110o.0f020.c.200.2o2..2l2l0u.0d030..i3n030.3.g.330a.00d40..v4400.4e..44rsaries

00..20200.20000.2 #55a5d555ver1s110a001ry10100gro1115u551p15s155

20 22002200

0.2

AdAAvAddAedvvrdevesvrerasesrrasarysraryoayryrbooysboboesbsbresevsreravevrtarvaiovtatianiototinoisnonpsnsapspsrapasprairsatsryirsitstyiyittyy

# #c##oc#crocorcruorrorpururrtprupeutpdtepetdtewdedwdowrowwokroerkokrrerksekrersesrrss

FFraFrFcarFartcriacaotcitconitotniioononfnofcoofocffcollccoullooludlluldillnuudingddingiianngadggadvaadevddversvveraseersarrissareiaaresirresiieesss

##a#ad##davaadveddevrsvvresaeerarsrsysrayaargryrgyryorggougrruoproopusuuspppsss

00 00

00 00

00000

00 0 0

0

0 0.200.2.2 0.400.4.04.50.600..66 0.800..88 1

0 00.2 1011.10400 2002.620200 03.80330300

0 00.1.01.01.0100..202.022.2000..303.03.3000..40.40.4

0.1 5505.25 101010.03 10105.11455 152020 20

AdAAvAdedvcrvecseruasrrsayarcoryyboosbfebsrasvedearrvvtveiaaorttnsiiooasnrnpiesasspprsaairtrsysiittyy Adver#sa#c#r#oyccrcooroourbrrprusrutupeepprtdevtteadewdtdiwowwronkoroeskrkrpekseraesrrsrssity FFrFraaFrcacrtctai#oiotcnicontionoorfnrfouccfopoocfltloecluuldodlduliwlindnugiogdnrigaankddegavrvdsaeevrdesvraesrariesrsiaersieFsraction##oa#adfdacv#vdeoeavrlrlsedusarvadsreryianyrrgsygarroagroyudruopvgpuesrsprossuaprises

(g) Adversary accuracy (h) Adversary ob-sparsity (i) Number of adversaries (j) Adv. dependence level (k) Adv. dependence level

M-MSR-twocoin

M-MSR

10 20 30 40 # workers in group B

5 10 15 20 # adversary groups

Figure 1: Experiments on synthetic data (see Supplementary Section C for a full explanation).

Prediction error

MV

KOS

GhostSVD

EoR

MV-D&S

PGD

OPT-D&S

BP-twocoin

EM-twocoin

1

RTE

1

Temp

1

Web

0.8

0.8

0.8

Prediction error

Prediction error

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0 0 20 40 60 80 # corrupted workers

1

TREC

0 0 10 20 30 40 # corrupted workers

1

Fashion1

0 0 20 40 60 80 # corrupted workers

1

Fashion2

0.8

0.8

0.8

Prediction error

Prediction error

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

100

200

# corrupted workers

0 0 15 30 45 60 # corrupted workers

0 0 15 30 45 60 # corrupted workers

Prediction error

Prediction error

MFA-twocoin 1

Entropy(O)
Dog

Minmax 1

M-MSR-twocoin
Bird

M-MSR

0.8

0.8

Prediction error

0.6

0.6

0.4

0.4

0.2

0.2

0

0

20

40

# corrupted workers

1

Anger

0

0

5 10 15 20

# corrupted workers

1

Fear

0.8

0.8

Prediction error

0.6

0.6

0.4

0.4

0.2

0.2

0

0

5 10 15 20

# corrupted workers

0

0

5 10 15 20

# corrupted workers

Prediction error

Figure 2: Experimental results on real datasets (see Supplementary Section D for a full explanation).

The M-MSR algorithm performs especially well on large datasets like RTE, Temp, TREC, Fashion1 and Fashion2. It is the only method which can handle around n2 (n is the number of the total workers) corrupted workers on these datasets. Out of 17 real datasets, our algorithm is the best on 16 of them. The only exception is dataset Surprise ( Figure 6 in Supplementary Sec. D) – the reason is that the “normal” workers on this dataset do not appear to be reliable. Besides, on some of the original real datasets, i.e., no adversaries are introduced, M-MSR algorithm is not the best one among all the baseline methods. This shows that the superiority of the M-MSR algorithm is mainly on large datasets in adversarial setting.
6.1 Exact Recovery
We compare the M-MSR algorithm with PCA and RPCA algorithms in [9]. We study the recovery rate of the rank-one matrices with different level of noises. The experiment results are shown in Figure 3 (the details of this experiment can be found in Supplementary Sec. F). It can be seen that our algorithm strongly outperforms these methods for robust rank-one matrix completion. We also note that the M-MSR algorithm is very efﬁcient: when the dimension of the matrices increases from 10 to 1000, the running time of M-MSR increases from 0.01 seconds to 0.42 seconds while RPCA increases
8

Noise probability
10 20 30 40 50 60 70 80 90 100
Noise probability
10 20 30 40 50 60 70 80 90 100
Noise probability
10 20 30 40 50 60 70 80 90 100
Running time

0.50 0.40 0.30 0.20 0.10
Dimension
(a) PCA

0 0.50
0.2 0.40
0.4 0.30
0.6 0.20
0.8 0.10
1.0

Dimension
(b) RPCA

0 0.50
0.2 0.40
0.4 0.30
0.6 0.20
0.8 0.10
1.0

Dimension
(c) M-MSR

0 100

0.2 80

M-MSR RPCA

0.4 60

0.6 40

0.8 20

1.0

0

200

400

600

800 1000

Dimension

(d) Running Time

Figure 3: Experimental results of exact recovery experiment. (a) Recovery rate heatmap of subgradient method for PCA (The intensity of the color is proportional to the recovery rate). (b) Recovery rate heatmap of subgradient method for RPCA (c) The recovery rate heatmap for M-MSR (d) Running time of the subgradient method for RPCA and running time for M-MSR (for each dimension, the average running time and the standard deviation conﬁdence interval over 100 independent trials are shown). Details are provided in Supplementary Section D.

from 0.46 seconds to 80.62 seconds. This is an additional advantage of the M-MSR algorithm when dealing with large datasets.
7 Discussion and Conclusions
We studied a crowdsourcing model with (i) The presence of users who might choose adversarial responses (ii) General worker-task assignment sets resulting in arbitrary interaction graphs G(Ω) among workers. Because approaches based on sparse recovery are not able to handle arbitrary A, we proposed a new algorithm, M-MSR, for skill determination (and consequently prediction) in this context. Our algorithm is based on a connection to the robust rank-1 matrix completion.
Our main results are: (i) A necessary and sufﬁcient condition for our algorithm to work on any graph, and a proof that our algorithm is optimal on random graphs (ii) An empirical evaluation which shows that our algorithm outperforms existing methods on both synthetic and real data sets.
Future work will analyze M-MSR when the graph is partially random. While some scenarios, like Amazon’s Mechanical Turk, allow any set A to be speciﬁed, other practical scenarios do not; consider, for example, estimation of item quality from online ratings (e.g., Yelp), where the assignment of users to items is not random. Adversarial interactions are particularly important in this context, as business owners might be tempted to skew the ratings by leaving reviews from fake accounts.
Theorem 2 provides a bound to how many adversaries can be tolerated in this setting, and this bound will be quite good as long as the underlying graph is dense. For a sparse graph, however, this is not the case: in that case, Theorem 2 could fail to guarantee that even a small number of adversaries can’t skew the result. One possibility is to strategically add more random edges (by giving users suggestions of items to rate) to make the resulting graph 2F + 1-robust for a large F . Subsequent work could consider how well such schemes perform both in theory and in practice.

Broader Impact
In this work, we provide a new robust matrix completion methods which can make recommendation systems more accurate in the presence of spam. This can beneﬁt users of platforms like Amazon Mechanical Turk and Yelp.
In terms of negative impact, our work could allow the same platforms to learn more about the preferences of their users. It is possible that this data could be leaked, resulting in privacy loss.

Acknowledgments and Disclosure of Funding
This work is supported by NSF awards 1914792 and 1933027.

References
[1] D. Berend and A. Kontorovich. Consistency of weighted majority votes. In Proceedings of Advances in Neural Information Processing Systems, pages 3446–3454, 2014.

9

[2] E. J. Candès and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717–772, 2009.
[3] E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.
[4] I. Dagan, O. Glickman, and B. Magnini. The pascal recognising textual entailment challenge. In Proceedings of Machine Learning Challenges Workshop, pages 177–190. Springer, 2005.
[5] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of the 22nd International Conference on World Wide Web, pages 285–294, 2013.
[6] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1): 20–28, 1979.
[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255. Ieee, 2009.
[8] D. Dolev, C. Dwork, O. Waarts, and M. Yung. Perfectly secure message transmission. Journal of the ACM, 40(1):17–47, 1993.
[9] S. Fattahi and S. Sojoudi. Exact guarantees on the absence of spurious local minima for nonnegative robust principal component analysis. Journal of Machine Learning Research, 21:1–51, 2020.
[10] D. Gamarnik and S. Misra. A note on alternating minimization algorithm for the matrix completion problem. IEEE Signal Processing Letters, 23(10):1340–1343, 2016.
[11] A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators? crowdsourcing abuse detection in user-generated content. In Proceedings of the 12th ACM Conference on Electronic Commerce, pages 167–176, 2011.
[12] N. Hartsﬁeld and G. Ringel. Pearls in graph theory: a comprehensive introduction. Courier Corporation, 2013.
[13] J. M. Hendrickx, A. Olshevsky, and V. Saligrama. Minimax rank-1 factorization. In Proceedings of 23rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.
[14] J. Hromkovicˇ, R. Klasing, A. Pelc, P. Ruzicka, and W. Unger. Dissemination of Information in Communication Networks: Broadcasting, Gossiping, Leader Election, and Fault-tolerance. Springer Science & Business Media, 2005.
[15] S. Ibrahim, X. Fu, N. Kargas, and K. Huang. Crowdsourcing via pairwise co-occurrences: Identiﬁability and algorithms. In Proceedings of Advances in Neural Information Processing Systems, pages 7845–7855, 2019.
[16] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 64–67, 2010.
[17] S. Jagabathula, L. Subramanian, and A. Venkataraman. Identifying unreliable and adversarial workers in crowdsourced labeling tasks. The Journal of Machine Learning Research, 18(1): 3233–3299, 2017.
[18] D. R. Karger, S. Oh, and D. Shah. Efﬁcient crowdsourcing for multi-class labeling. In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems, pages 81–92, 2013.
[19] A. Khetan and S. Oh. Achieving budget-optimality with adaptive schemes in crowdsourcing. In Advances in Neural Information Processing Systems 29, pages 4844–4852. 2016.
[20] F. J. Király, L. Theran, and R. Tomioka. The algebraic combinatorial approach for low-rank matrix completion. Journal of Machine Learning Research, pages 1391–1436, 2015.
10

[21] M. Kleindessner and P. Awasthi. Crowdsourcing with arbitrary adversaries. In Proceedings of International Conference on Machine Learning, pages 2708–2717, 2018.
[22] W. Kordecki. Poisson convergence of numbers of vertices of a given degree in random graphs. Discussiones Mathematicae Graph Theory, 16(2):157–172, 1996.
[23] H. Landau and A. Odlyzko. Bounds for eigenvalues of certain stochastic matrices. Linear Algebra and its Applications, 38:5–15, 1981.
[24] M. Lease and G. Kazai. Overview of the trec 2011 crowdsourcing track. In Proceedings of the Text Retrieval Conference, 2011.
[25] H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram. Resilient asymptotic consensus in robust networks. IEEE Journal on Selected Areas in Communications, 31(4):766–781, 2013.
[26] H. Li and B. Yu. Error rate bounds and iterative weighted majority voting for crowdsourcing. arXiv preprint arXiv:1411.4086, 2014.
[27] G. Liu, Q. Liu, and X. Yuan. A new theory for matrix completion. In Proceedings of Advances in Neural Information Processing Systems, pages 785–794, 2017.
[28] Q. Liu, J. Peng, and A. T. Ihler. Variational inference for crowdsourcing. In Advances in Neural Information Processing Systems 25, pages 692–700. 2012.
[29] B. Loni, M. Menendez, M. Georgescu, L. Galli, C. Massari, I. S. Altingovde, D. Martinenghi, M. Melenhorst, R. Vliegendhart, and M. Larson. Fashion-focused creative commons social dataset. In Proceedings of the 4th ACM Multimedia Systems Conference, pages 72–77, 2013.
[30] Y. Ma, A. Olshevsky, C. Szepesvari, and V. Saligrama. Gradient descent for sparse rankone matrix completion for crowd-sourced aggregation of sparsely interacting workers. In Proceedings of International Conference on Machine Learning, pages 3335–3344, 2018.
[31] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May):1665–1697, 2012.
[32] S. Pradhan, E. Loper, D. Dligach, and M. Palmer. Semeval-2007 task-17: English lexical sample, srl and all words. In Proceedings of the fourth international workshop on semantic evaluations (SemEval-2007), pages 87–92, 2007.
[33] J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics, pages 647–656. Lancaster, UK., 2003.
[34] V. C. Raykar and S. Yu. Eliminating spammers and ranking annotators for crowdsourced labeling tasks. Journal of Machine Learning Research, 13(Feb):491–518, 2012.
[35] N. B. Shah, S. Balakrishnan, and M. J. Wainwright. A permutation-based model for crowd labeling: Optimal estimation and robustness. arXiv preprint arXiv:1606.09632, 2016.
[36] R. Snow, B. O’connor, D. Jurafsky, and A. Y. Ng. Cheap and fast–but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 254–263, 2008.
[37] C. Strapparava and R. Mihalcea. Semeval-2007 task 14: Affective text. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 70–74, 2007.
[38] P. Welinder, S. Branson, P. Perona, and S. J. Belongie. The multidimensional wisdom of crowds. In Proceedings of Advances in Neural Information Processing Systems, pages 2424–2432, 2010.
[39] H. Xiao, J. Gao, Q. Li, F. Ma, L. Su, Y. Feng, and A. Zhang. Towards conﬁdence interval estimation in truth discovery. IEEE Transactions on Knowledge and Data Engineering, 31(3): 575–588, 2018.
[40] H. Zhang, E. Fata, and S. Sundaram. A notion of robustness in complex networks. IEEE Transactions on Control of Network Systems, 2(3):310–320, 2015.
11

[41] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet em: A provably optimal algorithm for crowdsourcing. In Proceedings of Advances in Neural Information Processing Systems, pages 1260–1268, 2014.
[42] D. Zhou, S. Basu, Y. Mao, and J. C. Platt. Learning from the wisdom of crowds by minimax entropy. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2195–2203. 2012.
[43] D. Zhou, S. Basu, Y. Mao, and J. C. Platt. Learning from the wisdom of crowds by minimax entropy. In Proceedings of Advances in Neural Information Processing Systems, pages 2195– 2203, 2012.
[44] D. Zhou, Q. Liu, J. Platt, and C. Meek. Aggregating ordinal labels from crowds by minimax conditional entropy. Proceedings of Machine Learning Research, 32(2):262–270, 2014.
[45] Y. Zhou and J. He. Crowdsourcing via tensor augmentation and completion. In IJCAI, pages 2435–2441, 2016.
[46] Y. Zhou, L. Ying, and J. He. Multic2: an optimization framework for learning from task and worker dual heterogeneity. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 579–587. SIAM, 2017.
12

A Baselines

In this section, we describe all the methods used as baselines for comparisons.

A.1 Crowdsoucing

• Majority Voting (MV) is a simple method where the true label of the tasks are estimated via the majority voting among the workers.
• KOS algorithm [18] is an approach for multi-class crowdsourcing problem with D&S model. The algorithm based on the assumption that the tasks are assigned to workers according to a random regular bipartite graph. To estimate the true labels, the k-class labeing problem is converted to a k − 1 binary labeling problems. Then these binary problems are iteratively solved via obtaining low-rank approximation of appropriate matrices. Though this algorithm requires speciﬁc constraints for the task assignment matrix, it can achieve good redundancy-accuracy trade-off.
• Ghost-SVD [11] algorithm considers the binary labeling problem based on single-coin D&S model. The true labels and the error probabilities of the workers are estimated via conducting the Singular Value Decomposition (SVD) to the observation matrix. This algorithm assumes that the probability error of one speciﬁc worker is smaller than 0.5. This algorithm has been proved to learn the true labels of the tasks with bounded error. However, this bound only works for the cases that the observation matrices are dense matrices.
• Eigenvectors of Ratio (EoR) [5] algorithm also considers the binary labeling problem which satisﬁes single-coin D&S model. Unlike Ghost-SVD and KOS approaches, this algorithm allows the task-assignment matrices to be arbitrary. The algorithm also applies a SVD-based method to obtain the estimation for the true labels and the workers’ reliability. This algorithm has been proved to have an improved error-bound guarantee for single-coin crowdsourcing model than the previous methods.
• EM algorithm (MV-D&S and OPT-D&S) [41] is a two stage algorithm for multi-class crowdsourcing problem based on D&S model. In the ﬁrst stage, the probability parameters of the D&S model are estimated by some approaches. In the second stage, the estimation of the parameters are reﬁned by the standard EM algorithm (the results of stage 1 are used as an initialization). For MV-D&S, the majority voting method is used to get the initial parameter estimation in the ﬁrst stage. For OPT-D&S, a spectral method is employed to obtain the initial parameter estimation in the ﬁrst stage.
• Projected Gradient Descent (PGD) [30]. In [30], the skill estimation of the single-coin D&S model is formulated as a rank-one correlation-matrix completion problem. PGD approach updates the skill level of the workers via solving the following optimization problem with the conventional projected gradient descent algorithm with ﬁxed stepsize.

1 arg min

Nij (C˜ij − xixj )2,

x∈[−1,+1]W 2

(i,j)∈Ω

where W , Nij and C˜ij are deﬁned as in section 3, x represents the skill level vector.
• Variational Approaches (BP-twocoin, MFA-twocoin, EM-twocoin) [28] address the crowdsourcing problems by using the tools and concepts from variational inference methods for graphical models. BP algorithm is a belief-propagation-based method, via choosing speciﬁc prior distributions of the workers’ abilities, this algorithm can be reduced to KOS or majority voting. The BP algorithm can also be extended to more complicated models. MFA algorithm is a mean ﬁeld algorithm which closely related to EM algorithm. In this work, we just consider the two coin version of these algorithms.
• Regularized Minimax Conditional Entropy (Entropy(O)) [44] algorithm considers the crowdourcing classiﬁcation problems with noises. In this algorithm, the confusion matrices of the workers are estimated as a minimax conditional entropy problem subject worker and items constraints that the workers can distinguish between classes which are far away from each other better than the ones which are adjacent.
• Minimax Entropy Learning from Crowds (Minmax) [42] algorithm also consider the crowdourcing classiﬁcation problems with noises. The difference is that the Minmax

13

algorithm estimates the confusion matrices via maximizing the entropy of the probability distribution over workers. Besides, they give prediction for the tasks by minimizing the KL divergence between the probability distribution and the unknown truth.

A.2 Exact Recovery
• Robust Principle Component Analysis (RPCA) [9] considers non-negative rank-one matrix completion problem. The rank-one matrix can be reconstructed via solving the following optimization problem.

min PΩ(X − uv ) 1 + Rβ(u, v),

(12)

u∈Rm + ,v∈Rn+

where X is the matrix to be recovered, Ω represents the set of the locations of the observed entries, Rβ(u, v) = α|u u − vv | is a regularization term. It has been proved that [9] does not have local minimum when some speciﬁc conditions satisﬁed. RPCA algorithm can also recover the matrix X when there exists some sparse noises. In [9], the optimization problem (12) is solved via the subgradient descent method, whereas other optimization algorithms can also be used to solve it. In our experiment, we used subgradient descent method with diminishing step size to get the optimal solution of (12).
• Principle Component Analysis (PCA) [9] considers the same problem as RPCA. PCA algorithm recover the rank-one matrix X via solving the following smooth optimization problem,

min

PΩ(X − uv

)

2 F

.

u∈Rm + ,v∈Rn+

(13)

In our experiment, we employed gradient descent with ﬁxed stepsize to optimize (13).

B Additional baselines and the two-coin model

The M-MSR method can be extended to solve rank-2 matrix completion problems with corruptions. Suppose the rank-2 matrix we aim to recover is X ∈ Rn×m, Ω is the set of the revealed locations. Let X = uvT , where u ∈ Rn×2, v ∈ Rm×2, we want to ﬁnd u and v. To deal with the corrupted
entries, we deﬁne

Ji(t) =

Xij (i, j) ∈ Ω . vj (t)

We will then set Ri(t) to be the set of nodes with the F largest and smallest values in Ji(t) (if there are fewer than F values strictly smaller/larger than ui(t), then Ri(t) contains the ones that are strictly smaller/larger than ui(t)); the quantities Jj(t), Rj(t) are deﬁned similarly for nodes j ∈ Vv. The extended algorithm is presented next.
Now cosider the two-coin model of the crowdsourcing problem, where the ability of worker i is speciﬁed by two parameters si, ti:

sj = Prob[Yij = +1|gj = +1], tj = Prob[Yij = −1|gj = −1].

(16)

Suppose assignment of tasks to workers is random and the proportion of tasks which have an answer of +1 is p. Suppose p is known. Let Kab be the set of tasks assigned to both workers a and b. Then

1 Kab Yaj Ybj ≈ E[Yaj Ybj ]
j∈Kab

= p (sasb + (1 − sa)(1 − sb)) + (1 − p)(tatb + (1 − ta)(1 − tb))

11

11

= p 2 + 2 (2sa − 1)(2sb − 1) + (1 − p) 2 + 2 (2ta − 1)(2tb − 1)

1p

1−p

= 2 + 2 (2sa − 1)(2sb − 1) + 2 (2ta − 1)(2tb − 1).

14

Algorithm 2 M-MSR-twocoin
Input: Positive matrix X, set Ω, F and v(0) > 0 Output: Xˆ = u(T )v(T )
1: for t = 1, 2, . . . , T do 2: For each i = 1, · · · , m, let

ui(t + 1) = arg min

(u vj (t) − Xij )2;

(14)

u

j ∈Ωi \Ri (t)

where ui ∈ R2 denotes the ith row of u. 3: For each j = 1, · · · , n, let

vj(t + 1) = arg min

(ui(t) v − Xij )2,

(15)

v

i∈Ωj \Rj (t)

and vj ∈ R2 denotes the jth row of v. 4: end for 5: return u(t) = [u1(t), u2(t), · · · , um(t)] , v(t) = [v1(t), v2(t), · · · , vn(t)]

In particular, we have





1

1

 Kab

YajYbj − 2  ≈ rank-2 matrix

j∈Kab

ab

In this case, we can apply the M-MSR-twocoin algorithm to estimate the skill level of the workers in two-coin model, and further give predictions for the tasks.

C Synthetic Experiments

The purpose of this section is to discuss the details of the synthetic experiments as shown in Figure 1. We will analyze the impact of graphs, task assignments, skill level and different adversarial strategies to the performance of the M-MSR algorithm as well as other baseline methods. Speciﬁcally, we will experiment with increasing level of graph sparsity, number of tasks, graph size, average skill and decreasing level of tasks assignment variance.
For the adversarial model, we randomly choose a certain number of workers, and let these workers be adversaries. Then these adversaries will be evenly divided into some groups, and the members of the same group will produce exactly the same response for each task (for the tasks they are assigned with). In this case, the adversaries in the same group are no longer independent of each other. The answer set of each group will be generated randomly according to a given accuracy (we randomly choose a fraction of tasks and give correct answer for them, for others we give wrong answers). Each adversary will be assigned with every task according to a ﬁxed probability (obs-sparsity), and then they will produce answers for the assigned tasks from the answer set. Hence, by varying the total number of the adversaries, the number of groups, the level of accuracy and obs-sparsity, we can generate a number of different adversarial strategies.
For each experiment, we vary one parameter while keep the others ﬁxed. The detailed information of the original dataset we apply is given in Table 1. Among them, the skill distribution represents the grid from which we choose the skill level of each normal worker uniformly at random. The group-B-#tasks is the number of tasks in group B, the details will be introduced in the "impact of task assignment variance". The obs-sparsity represents the probability that each task can be assigned to a speciﬁc worker.

Table 1: Synthetic dataset: characteristics values of the original dataset

#tasks 1600

#workers 80

#class 2

skill [−0.1, 0.7]

group-B- ave.obs#tasks sparsity

20

0.04

#corruptions adversary ACC

20

0.3

adversary obs-sparsity
0.4

#ad-groups 5

15

In the synthetic experiments, we make one minor modiﬁcation to the M-MSR algorithm, i.e. after the algorithm converging, we will project the obtained si which away from cube [− M1−1 + √1Ni , 1− √1Ni ] onto it, where Ni is the number of the tasks assigned to worker i. The reason for this modiﬁcation is
to stay away from the boundary of the hypecube where the weight log-odds function is changing very
rapidly. For the convenience of the analysis, we deﬁne

Cˆij = M Cij − 1

M −1

M −1

M1

1

= M − 1 Nij Yi,t, Yj,t − M − 1 , (17)

t|(i,t),(j,t)∈A

then the skill vector s in the RHS of (3) can be estimated by reconstructing the matrix Cˆ.
Next, we will provide the details of each experiment and discuss the experiment results of each graph in Figure 1 respectively.
Impact of graph sparsity: Figure 1(b) shows the experimental results of varying the sparsity of the interaction graph. The interaction graph sparsity implies the sparsity of G(Ω) corresponding to the rank-one matrix Cˆ in (17), which has a close connection to the connectedness and robustness of the graph. To see the impact of the interaction graph sparsity to the performance of different methods, we increase the observation sparsity level (the probability such that one element of Yw,t is nonzero) which in turn increases the sparsity of the workers’ interaction graph. The relationship of the graph sparsity and observation sparsity is shown in Figure 4(a), where we generate Yw,t randomly according to the observation sparsity and then observe the graph sparsity corresponds to Cˆ. It can be seen when the observation sparsity increases from 0 to 0.08, the graph sparsity increases from 0 to around 1, and after that, the graph sparsity maintains approximately 1. Therefore, we can vary the mean of the observation sparsity from 0 to 0.1 to see the impact of the graph sparsity to the experimental results.
It can be observed when the mean of the observation sparsity varies from 0 to 0.1, the prediction errors of the baseline methods except the M-MSR algorithm keeps greater than 0.6. The is because the adversaries are dominating the prediction results of these baseline methods. Why these adversaries can dominate the prediction results? There are three reasons. First, 1/4 of the workers are adversaries with accuracy 0.3 in this experiment, and each of them was assigned with around 40% tasks (the default value of the adversary obs-sparsity is 0.4). In other words, among all of the collected answers, a large fraction of them come from the adversaries and are incorrect. Next, the obs-sparsity of the adversaries is 0.4 implies that they have common tasks with almost all of the normal workers, hence the adversaries are located in the central places of the graph, and their behaviors can have a great impact to the prediction results. Moreover, these adversaries are not following the single-coin D&S model, they are highly dependent with other, therefore the baseline methods can not correctly estimated the behaviors of the adversaries.
However, it can also be observed that the prediction error of the M-MSR algorithm maintains smaller than 0.2 in most time. This is because that the true skill level of the normal workers can be correctly estimated by the M-MSR algorithm. More importantly, though the adversaries are not following the single-coin D&S model, they will be assigned with the negative skill levels when applying the
M-MSR algorithm. Consider an adversary i, it is very likely that the value of Cij will be quite small if j is a normal worker. Meanwhile, as we have discussed, adversary i is located in the central place, which means it can be connected to almost every normal workers. Thus, in the ith row and column of
C, the majority of elements will be very small, and according to (17), the corresponding elements in Cˆ will be negative. Though the elements in Cˆ related to the correlation with other adversaries can be positive or even be 1, the M-MSR algorithm can still assign adversary i with a negative skill level si. This negative si can produce a negative weight for the wrong answers from i in the prediction. Therefore, the M-MSR algorithm can give accurate predictions in such adversarial scenario.
Besides, the prediction error of the M-MSR algorithm tends to decrease when the graph sparsity increases. This is because the robustness of G(Ω) increases when the graph are becoming more dense. In this case, according to Theorem 2, it is more likely that the true skill level of the normal workers can be correctly estimated, and the adversaries will be assigned with smaller negative weights, hence the prediction accuracy can be improved.

16

Impact of the number of tasks: Figure 1(c) shows the experimental results of varying the the number of the tasks. In this experiment, we vary the number of tasks from 100 to 2500. It can be observed that the prediction accuracy of M-MSR algorithm increases when the number of the tasks increases. The reason for this phenomenon is that the noise level of the empirical estimation of Cˆ is decreasing. When the number of the tasks is small, even for the normal workers, the corresponding elements in Cˆ can be regarded as corrupted. The M-MSR algorithm can not work very well when there exists no reliable workers. Besides, the prediction error of the baseline methods keeps greater than 0.6 in most of the time, the reason is similar as in the experiment of the graph sparsity.
Impact of the graph size: Figure 1(d) shows the experimental results of varying the level of the graph size. Since the graph size is associated with the number of the workers, we can vary the number of the workers to see the impact of the graph size to the performance. From Figure 1(d) , we can see when the number of workers is small, the standard deviation of the prediction error for M-MSR algorithm is large. This is because we have assumed the workers’ skill level distribution is [−0.1, 0.7], and we randomly corrupt 14 of the normal workers each time. When the number of the workers is small, the skill levels of these normal workers may be quite different. If different normal workers are corrupted, the skill level of the remaining normal workers can be different, and the corresponding prediction error can also be different. When the number of the workers is large, it is more likely that the remaining normal workers can cover all possible skill level, hence the prediction error tends to be steady. For the other baseline methods, due to the reason we analyzed, the performance is dominated by adversaries, hence the difference of the number of the works does not have much impact to the prediction error.
Impact of different skill-distribution: Figure 1(e) shows the experimental results when varying the skill level of the normal workers. In the synthetic experiments, we assign the workers skill level uniformly at random on a grid. To see the impact of the skill level to the prediction accuracy, we ﬁx the lower bound of the grid as -0.1 and vary the upper bound of the grid from 0.1 to 1. It can be observed in Figure 1(e) that the prediction error of the M-MSR algorithm decreases when the upper bound of the skill level increases. When the average skill level of the normal workers is low, the corresponding elements in C can be relatively large as both the adversaries and normal workers are providing the wrong answers. As a consequence, the estimated skill level for the adversaries may be larger, or even positive. Hence the prediction error can be large. Meanwhile, the performance of the other algorithms keeps greater than 0.6. The reason is that these algorithms are still dominated by the adversaries, hence the performance does not change when the skill level of the normal workers changes.
Impact of the tasks assignment variance: Figure 4(b) shows the experimental results when varying the level of the tasks assignment variance. For a real-world crowdsourcing dataset, it is highly possible that the number of the tasks assigned to different workers can be different. Hence, we want to see the impact of the tasks assignment variance to the prediction accuracy. However, it is not easy to directly vary the variance while keep the total number of the answers ﬁxed. To solve this issue, we vary another parameter which is closely related to the task assignment variance. We divide the workers into two groups, i.e., group A and group B. The workers in group A and group B provide approximately the same number of answers in total (we ﬁx the observation sparsity sum in the two groups), and each worker in the same group will be assigned with approximately the same number of tasks (the observation sparsity for each worker is the average of the ﬁxed observation sparsity sum). As a consequence, when we increase the number of workers in group B, the tasks assignment variance will decrease. This can be observed in Figure 4(b) which shows the results where we vary the number of the workers in group B and observe the task assignment standard deviation.
From Figure 1(f) , we can see the task assignment variance does not have much impact to the prediction accuracy for all the algorithms. When varying the level of the task assignment variance, the prediction error of the M-MSR algorithm maintains around 0.2 while the prediction error of other baseline methods maintains greater than 0.6.
Impact of adversary accuracy: Figure 1(g) shows the experimental results of varying the level of the adversary accuracy. It can be observed that the prediction error of the baseline methods except the M-MSR decreases as the accuracy of the adversaries increases. The prediction error of these algorithms are approximately equal to the error of the adversaries. This phenomenon largely results from the fact that the adversaries are dominating the prediction results.
17

Graph sparsity Task assignment standard deviation

1
0.8
0.6
0.4
0.2
0 0.02 0.04 0.06 0.08 0.1 The observation sparsity
(a) The relationship of graph sparsity
and observation sparsity. The details can be found in "Impact of Graph Sparsity".

100
80
60
40
20
0 10 20 30 40
The num of workers in group B
(b) The relationship of task assignment
variance and number of workers in group B. The details can be found in "Impact of tasks assignment variance".

Figure 4: The parameter design analysis

However, the situation is quite different for the M-MSR algorithm. When the accuracy of the adversaries increases from 0 to 0.5, the prediction error of the M-MSR algorithm increases from 0 to around 0.35, and then the predction error of the M-MSR algorithm decreases again to 0 when the accuracy increases from 0.5 to 1. A major factor which contributes to such phenomenon is that the adversaries were assigned with a negative skill level si when the adversary accuracy is smaller than 0.5, and they were assigned with positive skill level si when the accuracy is greater than 0.5. When the adversary accuracy is smaller than 0.5, the adversaries can produce more wrong answers than correct answers. As a consequence, in the correlation matrix C, the elements which corresponds to the correlation between the adversaries and the normal workers have relatively small values. As we analyzed in the experiment of the graph sparsity, the adversaries can be assigned with negative skill level by M-MSR algorithm. In this case, the smaller the adversary accuracy is, the smaller the skill level will be. When the accuracy of the adversaries is greater than 0.5, both the adversaries and the normal workers will produce more correct answers. Thus, it is very likely that the elements in C corresponds to the correlation between the adversaries and the normal workers have the values greater than 0.5, which can further lead to the estimated skill level of the adversaries be positive. In such scenario, the more accurate the adversaries are, the more accurate the prediction results given by M-MSR will be.
Impact of adversary observation sparsity: Figure 1(h) shows the experimental results when varying the observation sparsity of the adversaries. It can be observed that the prediction error of the baseline methods except the M-MSR increases when the adversary obs-sparsity increases. This largely results from the fact the number of answers from the adversaries (the majority of them are incorrect) are increasing when the obs-sparsity increases. Another factor contributes to this phenomenon is that the adversaries can be connected to more normal workers as the obs-sparsity increases, which can make the adversaries have greater impact to the prediction results. Particularly, when the obs-sparsity of the adversaries is approximately equal to the normal workers, i.e., it is 0.05, the prediction error of almost all of the crowdsourcing methods are less than 0.5. In other words, when the adversaries are assigned with very small number of tasks, and they are not located in the central positions, then these adversaries can not greatly damage the performance of these methods. However, once the obs-sparsity of the adversaries increases to 0.1, the adversaries again can dominate the prediction results of these methods.
Next consider the M-MSR algorithm. The prediction error of the M-MSR algorithm maintains smaller than 0.2 as we analyzed in the graph sparsity experiments. Meanwhile, it can be seen that the prediction error of the M-MSR algorithm tends to decrease when the adversary obs-sparsity increases. One possible cause is that the adversaries can be connected to more normal neighbors which allow the M-MSR algorithm to give smaller negative skill estimation for the adversaries. Another reason maybe the weight of the wrong answer tends to be smaller when more adversaries are involved in the sum of (11) as the adversaries are assigned with negative weights.
Impact of the number of the adversaries Figure 1(i) shows the experimental results when varying the number of the corruptions. In this experiment, we vary the number of the workers from 0 to 40 (the total number of the workers is 80). It can be observed the prediction errors of all the algorithms
18

increase when the number of the corruptions increases. The M-MSR algorithm is the only method which can handle the corruptions up to 28. The reasons are similar to our previous analyses.
Impact of adversary dependence level To study the impact of the adversary dependence level, we implemented two different experiments. Figure 1(j) shows the experimental results of the ﬁrst experiment, where we vary the number of adversary groups. In our adversarial model, the members in the same group produce exactly the same response to every task. Thus, the dependence level between the adversaries decreases as the number of the adversary groups increases. When the number of the groups is 20, each of the adversary is independent of each other (the default number of the corrupted workers is 20). It can be observed that the prediction errors of the MultiSPA, MultiSPA-EM, and MultiSPA-KL decreases as the number of the adversary groups increases. When the number of the groups is 20, the prediction error of the MultiSPA-EM decrease to be smaller than 0.2. Such phenomenon is understandable, if every adversary is independent of each other, then the adversaries satisﬁes the requirement of the single-coin model, and they will just be normal workers with low skill level. It is possible that some methods can handle such kind of adversaries. Meanwhile, it can be observed that the prediction error of the M-MSR algorithm maintains to be smaller than 0.2 in almost all of the time.
Figure 1(k) shows the experimental results of the second experiment, where the 20 adversaries are divided into two groups. The members in the ﬁrst group work as before, they produce the same response for every task, the accuracy of the answer set is also 0.7, and the obs-sparsity of the adversaries is 0.4. However, the answer set of the second adversary group is set to be perfectly colluding with the answer set of the ﬁrst group, i.e., the accuracy of the second group is 0.7. Then similarly, the adversaries in the second group are also assigned with tasks with probability 0.4, and each of them will give answers for these tasks from the answer set. In this experiment, we vary the number of the adversaries in second group from 0 to 10, which is equivalent to vary the fraction of the second group over the total number of the adversaries from 0 to 0.5. It can be observed when this fraction increases, the prediction error of the baseline methods except the M-MSR algorithm decrease. This phenomenon largely results from the fact that the adversaries in the second group can produce more accurate answers than the ones in the ﬁrst group. Besides, it can be seen that the prediction error of the M-MSR algorithm keeps to be 0.3 when the fraction of the second group changes. This is because the M-MSR algorithm give the adversaries in the ﬁrst group the negative skill level estimations and give the ones in the second group the positive skill level estimations. In fact, all of these skill estimations will lead to the prediction results be dominated by the answers of the second group, where the error is 0.3.
From the two experiments, we can see returning the same answers can reduce the prediction error across the crowdsourcing methods more than returning colluding answers. Besides, the higher the dependence level of the adversaries is, the more damaging the adversarial strategy will be.

ABCDE F

A

1 0.08 0.08 0.08 0.08

B1

0.08 0.08 0.08 0.08

C 0.08 0.08

0.64

D 0.08 0.08 0.64

0.64 0.64

E 0.08 0.08

0.64

0.64

F 0.08 0.08

0.64 0.64

Figure 5: An illustration example of the covariance matrix C when there exists adversaries as in our adversarial model. The elements colored with red are corrupted, and the green ones are normal elements, the white ones are missing elements. A and B represent adversaries which produce the same response for every task, and the accuracy of their answer set is 0.1. C, D, E, and F represent normal workers with skill level 0.8. The number of tasks are large enough. For such a covariance
matrix C, the M-MSR algorithm can correctly estimate the true skill level of the normal workers and give negative skill estimation for the adversaries, and further provide accurate prediction results. However, for algorithms like PGD, the prediction is not accurate due to the impact of the corruptions.

19

D Real dataset Experiments
In this section, we provide the implementation details and results analyses about the real dataset crowdsourcing experiments as shown in Figure 2 and Figure 6.
In the real dataset experiments, all of the datasets come with ground truth label for each task (we remove the tasks with missing ground truth labels). In order to improve the efﬁciency of the experiments and reduce the sparseness, we remove the workers who provided less than 10 answers for each dataset. Since the labels of the datasets Emotion (Joy, Surprise, Anger, Disgust, Sadness, Fear) and Valence are numerical values, we transform these numerical labels to binary labels according to different partitions to the label range. The characteristic values and detailed introduction of these datasets are provided in Supplementary Sec E. Moreover, since Ghost-SVD algorithm and EoR algorithm work only for binary tasks, they will not be evaluated on the multiple-class datasets Adult2, Dog, Web and WSD. Besides, we will also apply the same projection strategy as in synthetic experiments for the M-MSR algorithm, i.e. after the algorithm converging, we will project the obtained si which away from cube [− M1−1 + √1Ni , 1 − √1Ni ] onto it.
In the real data experiments, the adversaries are also randomly corrupted, and they follow the same adversarial model as in synthetic experiments. Here, we set the number of the adversary groups is 1, the accuracy of the adversaries is 0.1, and the observation sparsity is 0.5. According to the results of the synthetic experiments, the most damaging strategy is to return a correct answer a fraction q < 1/2 of the time and an incorrect answer 1 − q of the time, and the adversaries should locate at the central places and be highly dependent with each other. Follow this idea, we set the parameters of the adversarial model as listed above. Moreover, for each dataset, we vary the number of the adversaries from 0 to around half of the number of the workers and then observe the corresponding prediction error of the crowdsourcing methods. The results of the real dataset experiments are given in Figure 2 and Figure 6.
It can be seen that the prediction error of almost all the algorithms on each dataset converge to 0.9 when the number of the corruptions increases. The reason is that the prediction error of the adversaries is 0.9 and they will gradually dominate the prediction results when their number grows. Besides, on large datasets Fashion1, Fashion2, TREC, Temp and RTE, the prediction error of the baseline methods except the M-MSR increases to 0.9 rapidly when the number of the adversaries increases. This largely results from the fact that the number of the tasks of these datasets is large. When the number of the adversaries increases, the fraction of the answers from the adversaries will increase rapidly. In other words, the fraction of the wrong answers among all of the answers increases rapidly, which can lead to such phenomenon. However, on these datasets, the prediction error of the M-MSR algorithm maintains to be 0.1 in most of the time. This is because the noise level of the covariance matrix C is low on these datasets. In this case, as we analyzed in “Impact of graph sparsity”, it is more likely that the true skill level of the normal workers can be correctly estimated and the adversaries can be assigned with negative skill estimations. According to prediction rule (11), such skill estimation can lead to the prediction results be opposite to the answer set of the adversaries. Therefore, the prediction error of the M-MSR algorithm can be 0.1 in most of the time. When the number of the adversaries increases to around a half of the total numbers, the M-MSR algorithm can not give negative skill estimation for the adversaries and hence its prediction error will also converge to 0.9.
Moreover, out of 17 real datasets, our algorithm is the best on 16 of them. The only exception is dataset Surprise (Figure 6) – the reason is that the “normal” workers on this dataset do not appear to be reliable. We can see from Table 2 that the average error probability of the normal workers is greater than 0.5 for this dataset, which means the average skill level of the normal workers is negative. From Figure 1(e) and the analysis in synthetic experiments, we can see such phenomenon is normal. Though our algorithm can eliminate the impact of the adversaries, we can not give accurate predictions if the remaining normal workers are not reliable, either. Besides, the Surprise dataset is a relatively small dataset, which implies that the noise level of the C can be large. This can further reduce the prediction accuracy of the M-MSR algorithm.
20

Prediction error

1 0.8 0.6 0.4 0.2
0 0
1 0.8 0.6 0.4 0.2
0 0

MV

KOS

GhostSVD

Synthetic two-coin

5

10

15

# corrupted workers

Sadness

5

10

15

# corrupted workers

EoR 20 20

Prediction error

Prediction error

MV-D&S 1

PGD

OPT-D&S

Adult2

BP-twocoin

EM-twocoin

1

MFA-twocoin WSD

Entropy(O)

Minmax 1

M-MSR-twocoin Joy

M-MSR

0.8

0.8

0.8

Prediction error

Prediction error

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

50

100

# corrupted workers

1

Surprise

0

0

5

10

15

# corrupted workers

1

Valence

0

0

5

10

15

20

# corrupted workers

1

Disgust

0.8

0.8

0.8

Prediction error

Prediction error

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

5

10

15

20

# corrupted workers

0

0

5

10

15

20

# corrupted workers

0

0

5

10

15

20

# corrupted workers

Prediction error

Figure 6: Experimental results of real data as well as a two-coin synthetic dataset. The synthetic
dataset is created following two-coin model rule, where a worker j is parametrized sj and tj, which are deﬁned as in (16). In this experiment, we let prior probability p = 0.5, and choose sj and tj uniformly in [0.5, 1].

E Datasets
In this section, we introduce the real datasets we applied in the crowdsourcing experiments. We employ 17 public real datasets to evaluate the effectiveness of the M-MSR algorithm and the baseline methods in crowdsourcing experiments. The followings are the brief introduction of these datasets.
• Fashion (Fashion1, Fashion2) 2 [29] is a fashion-focused Creative Commons images dataset associated with two different labels. Fashion1 dataset corresponds to the ﬁrst label, which indicates if an image is fashion-related or not. Fashion2 dataset corresponds to the second label, which indicates whether the fashion category of the image can correctly characterize the content in the image. The ground truth and the labels of the dataset was collected on Amazon Mechanical Turk (MTurk) platform. Fashion1 contains 13727 labels for 4711 images which are provided by 202 workers. Fashion2 contains 13474 labels for 4710 images which are provided by 208 workers.
• TREC3 [24] is a binary-class dataset where the task is to judge the relevance of the documents. The dataset is provided in TREC 2011 crowddourcing track. There are 88385 labels collected from 762 workers for 19033 documents in total.
• Waterbird Dataset (Bird)3 [38] is a binary-class dataset where the task is to identify whether an image contains a duck or not. There are 108 images in toal, and 4212 labels are collected from 39 workers. The labels are collected on MTurk platform.
• Dog3 [7] is a multiclass-dataset where the task is to recognize a breed (out of Norfolk Terrier, Norwich Terrier, Irish Wolfhound, and Scottish Deerhound) for a given dog. There are 7354 labels collected from 52 workers for 807 documents in total.
• Temporal Ordering (Temp)4 [36] is a binary-class dataset about the temporal ordering of event pairs. The workers are presented with event pairs and are asked to decide if the event described by the ﬁrst verb occurs before the second one. The verb event pairs are extracted from [33] by Snow et. al.[36]. There are 4620 labels provided by 76 workers for 462 event pairs in total. The labels are collected on MTurk platform.
• Recognizing Textual Entailment (RTE)4 [36] is a dataset where the workers are presented with two sentences in each example and are asked to decide whether the scond sentence can be inferred from the ﬁrst one or not. These sentence pairs come from PASCAL Recognizing
2Available at http://skulddata.cs.umass.edu/traces/mmsys/2013/fashion/ 3Available at https://github.com/zhangyuc/SpectralMethodsMeetEM/tree/master/ src

21

Textual Entailment task [4]. There are 800 sentence pairs in total which are labeled by 80 workers on MTurk platform, and 8000 labels are collected.
• Web Search Relevance Judging (Web)3 [43] is a multi-class datset where the task is to judge the relevance of query-URL pairs with a 5-level rating scale (from 1 to 5). There are 2665 query-URL pairs labeled by 177 workers, and the total number of the collected labels is 15567. The labels are collected on MTurk platform.
• Word Sense Disambiguation (WSD)4 [36] is a dataset to identify the most appropriate sense (out of three given senses) of the word "president" in a given paragraph. These paragraph examples are sampled from SemEval Word Sense Disambiguation Lexical Sample task [32] by Snow et al. [36]. There 1770 labels collected for 177 examples from 10 workers on MTurk platform.
• Emotions (Fear, Surprise, Sadness, Disgust, Joy, Anger, Valence)4 [36] is a group of datasets about ratings of different emotions for a given headline. There are six emotions datasets (fear, surprise, sadness, disgust, joy, anger) where the workers are asked to give numerical judgements in the interval [0, 100] rating the headline for each emotion. Besides, there is a valence dataset where the workers give numerical rating in the interval [−100, 100] which represents the overall positive or negative velence of the emotional content of the headline. The headlines are sampled from the SemEval-2007 Task 14 [37] by Snow et al. [36]. The labels are collected on the MTurk platform. There are 1000 labels for 100 headlines which are provided by 10 workers for each dataset. Since the labels of these datasets are numerical values, we convert them to binary-class datasets according to different partitions of the interval range. For emotions datsets, we let the rating value 0 represnt negative class and the rating interval (0, 100] represent the negative class (0 means the corresponding emotion is not observed). For valence dataset, we segment the interval [−100, 100] to [−100, 0) (negative) and [0, 100] (positive) respectively.
• Adult2 5 [16] is a multi-class dataset about the adult level of websites (G, PG, R and X). The labels are provided by workers on AMT platform. This dataset contains 3317 labels for 333 websites which are offered by 269 workers.

In our real dataset crowdsoucing experiments, we remove the workers who provide less than 10 labels for each dataset to reduce the sparsity of G(Ω) as well as improve the efﬁciency. Table 2 shows the characteristic values of the real datasets after this change.

Table 2: Real data: characteristic values after removing workers who provide less than 10 labels

Dataset
Adult2 Anger Bird Disgust Dog Fashion1 Fashion2 Fear
Joy RTE Sadness Surprise TEMP TREC Valence Web WSD

#workers
269 38 39 38 109 196 198 38 38 164 38 38 76 677 38 176 34

#tasks
333 100 108 100 807 3742 3601 100 100 800 100 100 462 2275 100 2653 177

#class
4 2 2 2 4 2 2 2 2 2 2 2 2 2 2 5 3

graph density
0.14 0.30 1.00 0.30 0.58 0.07 0.07 0.30 0.30 0.09 0.30 0.30 0.25 0.04 0.30 0.15 0.44

#crowdsourced ave.(min/max) labels(overall) #labels/worker

3317 1000 4212 1000 8070 10983 10420 1000 1000 8000 1000 1000 4620 12863 1000 15539 1770

12.3 (1/184) 26.3 (20/100) 108 (108/108) 26.3 (20/100) 74.0 (1/345) 56.0 (1/962) 52.6 (1/925) 26.3 (20/100) 26.3(20/100) 48.8 (20/800) 26.3 (20/100) 26.3 (20/100) 60.8 (10/462)
19 (1/ 967) 26.3 (20/100) 88.3 (1/1225) 52.1 (17/177)

ave.(min/max) #workers/tasks
10.0 (1/21) 10 (10/10) 39 (39/39) 10 (10/10) 10 (10/10) 2.9 (1/3) 2.9 (1/3) 10 (10/10) 10 (10/10) 10 (10/10) 10 (10/10) 10 (10/10) 10 (10/10) 5.7 (1/10) 10 (10/10) 5.9 (2/12) 10 (10/10)

average(min/max) prob. error
0.35 (0.00/1.00) 0.35 (0.10/0.60) 0.36 (0.11/0.68) 0.26 (0.05/0.50) 0.30 (0.00/1.00) 0.18 (0.00/1.00) 0.11 (0.00/1.00) 0.35 (0.10/0.80) 0.43 (0.10/0.65) 0.16 (0.00/0.60) 0.36 (0.15/0.65) 0.51 (0.00/0.85) 0.16 (0.00/0.60) 0.32 (0.00/1.00) 0.34 (0.10/0.65) 0.63 (0.00/1.00) 0.02 (0.00/0.17)

4Available at https://sites.google.com/site/nlpannotations/ 5Available at https://github.com/ipeirotis/Get-Another-Label/tree/master/ data
22

F Further Experiments: Exact Recovery
The purpose of this section is to discuss the details of the exact recovery experiments as shown in Figure 3. For this experiment, we compare the proposed M-MSR algorithm to AN-RPCA, PCA algorithms in [9]. We consider thousands of randomly generated positive rank-1 matrix with different sizes and and different noise levels. The size of the matrices ranges from 10 × 10 to 100 × 100. The elements of u∗ and v∗ are uniformly chosen from the interval [0, 2]. Each element of the noise matrix S is generated to be 200 with probability p and 0 with probability 1 − p. We assume that a matrix can be exactly recovered if uv − u∗v∗ F / u∗v∗ F ≤ 10−4. For each dimension and noise probability, we generate 100 random matrices under such conditions and demonstrate its exact recovery rate. To improve the efﬁciency of the M-MSR algorithm, we did not adopt the random initialization in the exact recovery experiment. Instead, we choose arbitrary row of X, and complete the unobserved entries of this row with random positive constants, then let this row be v(0). In this case, part of the nodes in G(Ω) have the same value corresponding to kj = a1i in the beginning (as vj(0) = aibj = kjb(j0) ), hence the consensus process can be facilitated.
Figure 3(a), 3(b), 2 shows the heatmap of the exact recovery rate of PCA, AN-RPCA, and M-MSR algorithm, respectively. It can be seen that the M-MSR algorithm can exactly recover the matrices when around 30% of the entries are severely corrupted. However, AN-RPCA algorithm can only recover matrices with around 20% corrupted entries and PCA can not recover the matrices with such severe corruptions for almost all dimensions and noise probabilities. Besides, we also compare the convergence time of the RPCA and M-MSR for exact recovery experiments. It can be observed that the running time of the M-MSR algorithm increases from 0.01s to 0.42s when the dimension of the matrices increases from 10 × 10 to 1000 × 1000, and the running time of the RPCA algorithm increases from 0.46s to 80.62s. The M-MSR algorithm is much more efﬁcient than the RPCA algorithm, especially when applied on large datasets. This is an additional advantage of the M-MSR algorithm when dealing with rank-one matrix completion problems with corruptions on large datasets.

G Convergence Analysis for Arbitrary Graph

In this section, we provide the proof of Theorem 2.

G.1 Proof of Theorem 2

Proof. (sufﬁciency) Suppose

ui(t) = aiki(t), i ∈ [m], vj(t) = bj , j ∈ [n], (18)
kj (t)

where ki(t) (i ∈ [m]) represents the value of the node in partition Vu at iteration t, kj(t) (j ∈ [n]) represents the value of the node in partition Vv at iteration t, and the uncorrupted rank-one matrix is ab . Let M (t) and m(t) be the maximum and minimum value of normal nodes at iteration t
respectively, i.e.,

m(t) ≤ ki(t) ≤ M (t), i ∈ [m] ∩ N , m(t) ≤ kj(t) ≤ M (t), j ∈ [n] ∩ N ,

where N is the set of normal nodes. Our ﬁrst step is to show that M (t) and m(t) are monotone bounded functions.

Let us consider a normal node i ∈ [m]. The value it receives from a neighbor j at iteration t + 1 is vXj(itj) . If j is also a normal node,

Xij

aibj

vj(t) = bj/kj(t) = aikj(t) ∈ [aim(t), aiM (t)]. (19)

On the other hand, if j is corrupted, it is possible that vXj(itj) is not in the interval [aim(t), aiM (t)]. However, G(Ω) is a F -local nodes-corrupted graph; and the largest and smallest F values of vXj(itj)

23

are removed when updating ui(t + 1). In other words, after ﬁltering, the values the node i receives from its neighbors are in the interval [aim(t), aiM (t)]. Because ui(t + 1) is a convex combination of such ﬁltered values, we have
u(it+1) ∈ [aim(t), aiM (t)].
which implies
m(t) ≤ ki(t + 1) ≤ M (t).

We next make a similar argument for a normal node j ∈ [n]. The value j receives from a neighbor i at iteration t + 1 is uiX(ti+j1) . If i is also normal node, then

Xij = aibj = bj ∈ bj , bj . (20) ui(t + 1) aiki(t + 1) ki(t + 1) M (t) m(t)

On the other hand, if i is corrupted, it is possible that uiX(ti+j1) is not in the interval [ Mb(jt) , mb(jt) ]. However, when we update vj(t + 1), the largest and smallest F values of uiX(ti+j1) are also removed. As a result, vj(t+1) ∈ [ Mb(jt) , mb(jt) ], which implies

m(t) ≤ kj(t + 1) ≤ M (t).

We have thus derived that M (t + 1) ≤ M (t), m(t + 1) ≥ m(t), i.e., M (t) and m(t) are both monotone bounded functions. Recall the property of skew-nonamplifying in (9) and (10), this also implies that M-MSR algorithm is skew-nonamplifying.

Next, that M (t) and m(t) are monotone bounded functions means each of them has some limits. Suppose the limit of m(t) is km, the limit of M (t) is kM . If we have kM = km = k, where k is a positive constant, then all the normal nodes will asymptotically converge to k at sometime T and we can get

 a1k1(T )  u(T )v(T ) =  a2k·2·(·T )  · k1b(1T ) k2b(2T ) · · · knb(nT ) = u∗v∗ . (21)
amkm(T )

We will next prove (21) is actually always true.

Indeed, suppose kM = km; then there exists some 0 > 0, such that kM − 0 > km + 0. Let SM (t, ) denote the set of normal nodes which have values greater than kM − at time-setp t, and let Sm(t, ) denote the set of normal nodes which have values smaller than km + at time-setp t. If we can ﬁnd ∗ > 0, ¯∗ > 0, t∗ < ∞ so that SM (t∗, ∗) or Sm(t∗, ¯∗) is empty, then all the normal nodes have values strictly smaller than kM − ∗ or strictly greater than km + ¯∗. This would contradict the assertion that kM is the limit of M (t), or contradicts the assumption that km is the limit of m(t), respectively. Thus our goal is to prove that such ∗, ¯∗, t∗ do exist.

Let

(α/2)m+n−F

0 < < 1−α (1 − (α/2)m+n−F ) + (1 − α + √2)/α 0.

(22)

2−α

Since we assumed G(Ω) is 2F + 1-robust, at the very least we have that any node in V (G) has at least 2F + 1 neighbors, so that

2F + 1 ≤ min{m, n}.

Therefore, we have m + n − F ≥ 2, and since α ≤ 1/2, we have

0 < < 0.

Since we assumed that there exists 0 > 0 such that kM − 0 > km + 0. If we choose smaller value of 0, the inequality kM − 0 > km + 0 still holds. In this case, we can always choose 0 be small enough such that

α(1 − α)( + 0)2 + 0

km ≥

.

(23)

α 0 − (1 − α)

24

Choose t0 so that M (t0) < kM + , m(t0) > km − (the existence of t0 is guaranteed by the convergence of M (t) and m(t)). Then consider the two disjoint subsets SM (t0, 0) and Sm(t0, 0). If SM (t0, 0) or Sm(t0, 0) is empty, we can directly let ∗ = 0, ¯∗ = 0, t∗ = t0 and we are done. Therefore we just need consider the case that both SM (t0, 0) and Sm(t0, 0) are nonempty. As G(Ω) is 2F + 1-robust, there exists a node s in SM (t0, 0) or Sm(t0, 0) such that it has at least 2F + 1 neighbors outside.

Because both SM (t0, 0) and Sm(t0, 0) consist of nodes from Vu and Vv, which have different update rules, we need discuss the following four cases respectively.

Case A: If s ∈ SM (t0, 0) is a node in Vu, it has at least 2F + 1 neighbors outside SM (t0, 0), out of which at least F + 1 must be normal. Therefore, after removing F largest and F smallest neighbors, s still can receive values from at least one normal node outside of SM (t0, 0).

By the same argument made in Eq. (19), the values s receives from any normal neighbor lies in the interval [asm(t0), asM (t0)]. Since G(Ω) is a F -local corrupted graph and the largest and smallest F values s receives are removed, all the values s receives from its neighbors lie in the interval [asm(t0), asM (t0)], which implies ∀j ∈ Ωs\Rs(t0),

Xsj ≤ asM (t0). vj (t0)

Then according to the update rule of Eq. (6) we have

us(t0 + 1) =

wsj Xsj

j ∈Ωs \Rs (t0 )

vj (t0)

≤ (1 − α)asM (t0) + αas(kM − 0)

≤ (1 − α)as(kM + ) + αas(kM − 0)

≤ as[kM − (α 0 − (1 − α) )],

which implies

ks(t0 + 1) ≤ kM − (α 0 − (1 − α) )

= kM − a,

(24)

where a = α 0 − (1 − α) .
Since 0 < α ≤ 21 , and m + n − F ≥ 2, then

(α/2)m+n−F

α

< 1−α (1 − (α/2)m+n−F ) + (1 − α + √2)/α 0 < 1 − α 0,

2−α

which implies

0 < a < 0.

(25)

Case B: If s ∈ SM (t0, 0) is a node in Vv, then by the same argument, s will receive a value from at least one normal node with value bounded above kM − 0.
Similarly to Eq. (20), the values s receive from its normal neighbors lie in the interval [ Mb(jt0) , mb(jt0) ]. Reprising the argument in Case 1, G(Ω) is a F -local corrupted graph and the largest and smallest F values s receives are removed in each iteration. Thus according to the update rule (7) we have

vs(t0 + 1) =

Xis i∈Ωs\Rs(t0) wis ui(t0 + 1)

≥ (1 − α) bs + α bs M (t0) kM − 0

≥ (1 − α) bs + α bs .

kM +

kM − 0

Thus

25

bs

1

ks(t0

+

1)

=

vs(t0

+

1)

≤ (1

−

α)/(kM

+

) + α/(kM −

0)

= kM 2 + kM − kM 0 − 0 kM − (1 − α) 0 + α

= kM (kM − (1 − α) 0 + α ) − αkM 0 + (1 − α)kM − 0 kM − (1 − α) 0 + α

=kM −

0 + αkM 0 − (1 − α)kM . kM − (1 − α) 0 + α

Let Then,

b = 0 + αkM 0 − (1 − α)kM . kM − (1 − α) 0 + α

α(1 − α)( + 0)2

− a=

> 0,

b

kM − (1 − α) 0 + α

which based on the fact that 0 < α < 21 , and α(1 − α)( + 0)2 +
kM > km ≥ α 0 − (1 − α)

0 > (1 − α) 0 − α ,

where the last inequality is true as

α(1 − α)( + 0)2 + 0

20

− [(1 − α) 0 − α ] =

> 0,

α 0 − (1 − α)

α 0 − (1 − α)

where the denominator is positive as we have shown in (25). As a result, we have b > a and

ks(t0 + 1) ≤ kM − b < kM − a.

(26)

Case C: If s ∈ Sm(t0, 0) is a node in Vu, via a similar process, we can get

ks(t0 + 1) ≥ km + α 0 − (1 − α) = km + c.

(27)

Case D: If s ∈ Sm(t0, 0) is a node in Vv, we can obtain

k (t0+1) ≥ km + αkm 0 + αkm − km − 0 .

s

km + (1 − α) 0 − α

Let d = αkmkm0++α(1k−mα)−0k−mα− 0 , then

−α(1 − α)( + 0)2

− c=

< 0,

d

km + (1 − α) 0 − α

which implies d < c. However, according to (23), we can derive

α(1 − α)( + 0)2 km + (1 − α) 0 − α

α(1 − α)( + 0)2 ≤ α(1−α)( + )2+
α 0−(1−0α) 0 + (1 − α) 0 − α

= α(1 − α)( + 0)2(α 0 − (1 − α) ) 2α(1 − α)( + 0)2

1

1

= 2 (α 0 − (1 − α) ) = 2 c,

which means

d

≥

1 2

c, and

1

ks(t0 + 1) ≥ km + d ≥ km + 2 c.

(28)

26

Summary: Let

1 = a = α 0 − (1 − α) ,
11 ¯1 = 2 c = 2 (α 0 − (1 − α) ),
we see that in each case, at least one normal node s in SM (t0, 0) decreases to kM − 1 (or below), or one normal node in Sm(t0, 0) increases to km + ¯1 (or above), or both. Therefore, if we deﬁne the sets SM (t0 + 1, 1) and Sm(t0 + 1, ¯1), then we either have

|SM (t0 + 1, 1)| < |SM (t0, 0)|,

or

|Sm(t0 + 1, ¯1)| < |Sm(t0, 0)|,

or both. Since ¯1 < 1 < 0, we have

kM − 1 > kM − 0 > km + 0 > km + ¯1,

which means SM (t0 + 1, 1) ⊆ SM (t0 + 1, 0) and Sm(t0 + 1, ¯1) ⊆ Sm(t0 + 1, 0). As set SM (t0 + 1, 0) and set Sm(t0 + 1, 0) are disjoint, set SM (t0 + 1, 1) and set Sm(t0 + 1, 1) are disjoint too.

For j ≥ 2, let

j = α j−1 − (1 − α) , 1
¯j = 2 (α¯j−1 − (1 − α) ),
then j < j−1, ¯j < ¯j−1. If both sets SM (t0 + j, j) and Sm(t0 + j, ¯j) are nonempty, we can repeat the analysis above for time-step t0 + j. If we can still show

Case A: ks(t0 + j) ≤ kM − j,

(29)

Case B: ks(t0 + j) ≤ kM − j,

(30)

Case C: ks(t0 + j) ≥ km + ¯j,

(31)

Case D: ks(t0 + j) ≥ km + ¯j,

(32)

we can derive that either

|SM (t0 + j, j)| < |SM (t0 + j − 1, j−1)|,

or

|Sm(t0 + j, ¯j)| < |Sm(t0 + j − 1, ¯j−1)|,

or both. Since

|SM (t0, 0)| + |Sm(t0, 0)| ≤ |N | = m + n − F,

then there exists T ≤ m + n − F such that at the end of the iteration t0 + T , the set SM (t0 + T, T ) or set Sm(t0 + T, ¯T ) will be empty or both. Moreover, if we can further show

T > 0, ¯T > 0,

(33)

then we can conclude that T , ¯T , and t0 + T are exactly the ∗, ¯∗, and t∗ we are looking for. Next we will show that the inequalities (29)–(33) are actually always true.

For j = 2, . . . , m + n − F ,

j = α j−1 − (1 − α)

= α(α j−2 − (1 − α) ) − (1 − α)

= αj 0 − (1 − αj) ,

1 ¯j = 2 (α¯j−1 − (1 − α) )

= ( α )j 0 − 1 − α (1 − ( α )j) ,

2

2−α

2

27

then

j<

j−1, ¯j < ¯j−1, and ¯j <

j

(¯1

=

1 2

1). In other words, we have

√

j > ¯j ≥ ¯m+n−F = ( α )m+n−F

0

−

1

−

α (1

−

( α )m+n−F )

1−α+ >

2 > 0,

(34)

2

2−α

2

α

where the second inequality is obtained from assumption (22). As T ≤ m + n − F , we can derive

that inequalities (33) are always true.

Then we will prove the inequalities (29)–(32) when j ≥ 2. For case A and case C, we can show that

the inequalities (29), (31) are true by simply replace 0 with j−1 in the analysis above. For Case B,

we need further show

α(1 − α)( + j−1)2 + j−1

km ≥

,

(35)

α j−1 − (1 − α)

to prove inequality (30), and for Case D, we need further show

α(1 − α)( + ¯j−1)2 + ¯j−1

km ≥

,

(36)

α¯j−1 − (1 − α)

to prove the inequality (32). To do this, consider function

α(1 − α)( + x)2 + x

f (x) =

,

αx − (1 − α)

its derivative is

(1 − α)(α2x2 − 2α(1 − α) x + 2(α2 − 2α − 1))

f (x) =

(αx − (1 − α) )2 .

√
When x > 1−αα+ 2 , we have f (x) > 0, which means f (x) is monotonically increasing in this interval. From equation (34), we have
√ 1−α+ 2
α < ¯j−1 < j−1 < 0,

where j = 2, . . . , m + n − F. Therefore, for j = 2, . . . , m + n − F, we have

α(1 − α)( + 0)2 + 0 km ≥ (α 0 − (1 − α) )2 = f ( 0) > f ( j−1) > f (¯j−1),

where the ﬁrst inequality is the assumption (23). In this case, we complete the proof of inequality (35) and inequality (36).

Then in Case B, we can replace 0 with j−1, and we have

α(1 − α)( + j−1)2 + kM > km ≥
α j−1 − (1 − α)

j−1 > (1 − α) j−1 − α .

Next by conducting the similar analysis as above, we can prove the inequality (30). In Case D, we can replace 0 with ¯j−1, and we can derive

α(1 − α)( + ¯j−1)2 1 ≤ (α¯j−1 − (1 − α) ),
km + (1 − α)¯j−1 − α 2

based on inequality (36). We can also follow the similar analysis above to prove (32). Thus, we complete the proof of sufﬁciency.

(Necessity) To prove the necessity, we want to show if G(Ω) is not 2F + 1-robust, then there exists
cases which can not achieve consensus by applying M-MSR algorithm. Since G(Ω) is not 2F + 1-
robust, there exists a pair of nonempty and disjoint sets S1, S2 ∈ V (G) such that each node in S1 or S2 has at most 2F neighbors outside. Suppose S1 consists of normal nodes which have values a (meaning ki = a or ki = a) while S2 consists of normal nodes which have values b (meaning ki = b or ki = b) with a > b > 0, let all the other normal nodes have the values inside the interval (b, a) (a < ki < b or a < ki < b). Since G(Ω) is F -local corrupted graph, we can let each normal node in S1 have F corrupted neighbors which always send the normal node with value corresponding to ki = a or ki = a. This is possible, since in our cases, the corrupted elements Xij can be any value. Also, let normal node in S2 have F corrupted neighbors which always send the normal node with value corresponding to ki = b or ki = b. Thus, for the normal nodes in S1 and S2, the values which are different from their own values will always be ﬁltered and they can only use the values equal to
their own values to update. In this case, the consensus can never be achieved.

28

H Convergence Analysis for Random Graphs

In this section, we present the proof of Theorem 1. To do that, we provide the sharp threshold of being r-connected, r-robust for Gn,n,p as well as some other related lemmas. Besides, we also present two lemmas about how to represent a F -local model as a F -total model.

We start our analysis from introducing some deﬁnitions for Gn,n,p which will be used in our proof.

Deﬁnition 5. A(n) ≈ B(n) means A(n)/B(n) → 1 as n → ∞.

Deﬁnition 6. A graph property P is a class of graphs on vertex sets L and W , which is closed under isomorphism. In particular, |P| ≤ 2n2 .

Deﬁnition 7. A graph property P is monotone increasing if G ∈ P implies G + e ∈ P, i.e., adding an edge e to a graph G does not destroy the property.

Deﬁnition 8. Consider a function p∗(n) = g(n) , where g(n) → ∞ as n → ∞. Let x be any function

such

that

x

=

o(g(n))

and

x

→

∞

as

n

n
→ ∞.

Then

p∗(n)

is

a

sharp

threshold

for

a

monotone

increasing graph property P in the random graph Gn,n,p if

lim P(Gn,n,p ∈ P) = 1 p = (g(n) + x)/n . (37)

n→∞

0 p = (g(n) − x)/n

We start our analysis from a general lemma about the necessary condition such that any approach achieves consensus when there exists malicious nodes in a network.
Deﬁnition 9. For an undirected arbitrary graph G, let cp(G) denote the least number k such by removing k appropriately chosen vertices from G and the eges incident on then results in a graph that is not connected.
Deﬁnition 10. [25] Consider an undirected arbitrary graph G, suppose each normal node begins with some private value xi(0) ∈ R (The initial values can be arbitrary). The nodes interact synchronously by conveying their values to their neighbors in the graph. Each normal node updates its own value over time according to a prescribed rule, which is modeled as
xi(t + 1) = fi(xij(·)), j ∈ Ωi, i ∈ N ,
where xij(·) is the value sent from node j to node i before time-step t + 1. The update rule f (·) can be arbitrary deterministic function, and may be different for different nodes. Then the normal nodes of G are said to achieve resilient asymptotic consensus in the presence of malicious nodes if
• ∃k ∈ R such that limt→∞ xi(t) = k for all i ∈ N , • the normal values remains in the interval [m(0), M (0)] for all t,
where m(0), M (0) are the initial values.
Lemma 1. (Theorem 5.2 of [8], Proposition 6.2.2 of [14]) Suppose there exists F malicious nodes in an undirected arbitrary graph G, the positions of the malicious nodes are unknown, then the necessary condition that G can achieve consensus on a ﬁxed value regardless of the mechanism used is
cp(G) ≥ 2F + 1.

H.1 r-connected for random bipartite graph
In this subsection, we provide the sharp threshold function of being r-connected for random bipartite graphs Gn,n,p. The general outline of this proof is to show the sharp threshold function for graph property that minimum degree δ(Gn,n,p) = r ﬁrstly, then show that the graph property for Gn,n,p being r-connected is equal to the graph property that δ(Gn,n,p) = r.
Deﬁnition 11. For Gn,n,p and constant r ∈ Z≥1, let the properties of being r-connected, having minimum degree δ(Gn,n,p) = r be denoted by Kr, Dr, respectively.
Suppose Xr is the random variable counting the number of the vertices with degree r in Gn,n,p, λr(n) is the expectation of Xr, i.e., λr(n) = E(Xr). Let Po(λ) be the Poisson distribution with parameter λ, i.e., P(X = k) = λkke!−λ . Let N(0, 1) be standard normal distribution.
29

𝑏

𝑏

𝑎

𝑐

𝑎

𝑒

𝑐

𝑑
(a) A 2-connected graph

𝑑
(b) A 3-connected graph

Figure 7: Illustration example for Lemma 1. Red nodes represent malicious nodes, green nodes represent normal nodes. In a 2-connected graph, malicious node b can prevent node a from getting correct information from node c. In a 3-connected graph, there exists three disjoint paths from node c to node a, hence it is possible to apply some strategy to eliminate the inﬂuence of malicious node b.

Firstly, we present the sharp threshold function for property Dr, to do that, we introduce the following lemma from [22]. Lemma 2. [22] If np → ∞ but np/nα = o(1) for every α > 0, then the distribution of Xr → Po(λ) if λr(n) → λ < ∞ and if λr(n) → ∞, then the distribution of (Xr − λr(n))/ λr(n) → N(0, 1).
In the following lemma, we provide the threshold function for property Dr.
Lemma 3. Consider a random bipartite graph Gn,n,p. For any constant r ∈ Z≥1,

p∗(n) = log n + (r − 1) log log n n
is a sharp threshold function for the property Dr. Proof. Let

log n + (r − 1) log log n + x

p=

,

(38)

n

where x = o(log log n) → ∞ when n → ∞. We will show that with this probability, we can obtain

(i) λt(n) = E(Xt) = o(1) when t ≤ r − 2, n → ∞. (ii) λr−1(n) = E(Xr−1) = (2re−−1x)! when n → ∞.

(iii) λr(n) = E(Xr) → ∞ when n → ∞.
To do this, we will consider the vertices with degree t (t ≤ r − 1) in L and W respectively, where the notation L denotes the set of left-nodes of the bipartition of this graph Gn,n,p, and W denote the set of right nodes. Let

Iv = 1 v is a vertex with degree t in Gn,n,p , 0 otherwise

then we can obtain

E(number of nodes with degree t in L )

= E( Iv)
v∈L

= E(Iv)

v∈L

= n n pt(1 − p)n−t,

(39)

t

30

As

n n(n − 1) · · · (n − t) nt

=

≈,

(40)

t

t!

t!

pt = log n + (r − 1) log log n + x t ≈ log n t , (41)

n

n

(1 − p)n−t =e(n−t) log(1−p) = e−(n−t)

∞ pk k=1 k

=e−(n−t)p · e−(n−t)

∞ pk k=2 k

= e−(n−t)p · e−o(1) ≈

e−x ,

n(log n)r−1

(42)

where x = o(log log n) → ∞ when n → ∞, and in (42), we applied

(n − t) ∞ pk < (n − t) ∞ pk = (n − t)p2 = o(1).

k

1−p

k=2

k=2

Plug (40), (41), (42) into (39), we can get

nt E(number of nodes with degree t in L ) ≈ n t!

log n t e−x

e−x (log n)t

n n(log n)r−1 = t! (log n)r−1 .

Via similar process, we can obtain

e−x (log n)t E(number of nodes with degree t in W ) ≈ t! (log n)r−1 .

Then we have

E(Xt) = E(number of nodes with degree t in L ) + E(number of nodes with degree t in W )

2e−x (log n)t

≈ t! (log n)r−1 .

(43)

Thus (i), (ii) and (iii) follows immediately.

For t ≤ r − 2, observing that Xt is a nonnegative random variable, and E(Xt) → o(1), we can derive that

P(Xt = 0) → o(1).

(44)

For Xr, as

np = log n + (r − 1) log log n + x → ∞,

np log n + (r − 1) log log n + x

nα = nα

= o(1),

when n → ∞. According to Lemma 2, we have

∀α > 0,

Xr − λr(n) → N(0, 1), λr (n)

then we obtain

P(Xr = 0) =1 − P(Xr = 0) ≈ 1,

(45)

In summary, when p has the value in (38), it is not likely that the nodes with degree up to r − 2 exist
in Gn,n,p. Meanwhile, the probability that the nodes with degree r exist in Gn,n,p tends to be 1. Thus, the probability that minimum degree δ(Gn,n,p) = r is decided by the existence of the nodes with degree r − 1. Consider (ii), since x = o(log log n) → ∞ as n → ∞, we have

2e−x λr−1(n) = (r − 1)! = o(1),

then we can also obtain P(Xr−1 = 0) ≈ 0. Thus we have

P(δ(Gn,n,p) = r) ≈ P(Xr−1 = 0) ≈ 1.

31

If

log n + (r − 1) log log n − x

p=

,

(46)

n

we can just replace x with −x in (43) and produce

2e−x (log n)t E(Xt) ≈ t! (log n)r−1 .

In this case, (i) and (iii) remain the same while (ii) is updated as: (ii) λr−1(n) = E(Xr−1) = (r2−ex1)! → ∞. Therefore, (44) and (45) still hold while

Xr−1 − λr−1(n) → N(0, 1), λr−1(n)

then we can derive that

P(δ(Gn,n,p) = r) ≈ P(Xr−1 = 0) ≈ 0.

Now, we are ready to present the sharp threshold function of being r-connected for Gn,n,p. Lemma 4. Consider a random bipartite graph Gn,n,p. For any constant r ∈ Z≥1,

p∗(n) = log n + (r − 1) log log n

(47)

n

is a sharp threshold function for the property Kr.

Proof. Gn,n,p is r-connected means by removing r suitably chosen vertices (but not by removing less than r vertices) Gn,n,p can be disconnected. Let this event be denoted by A(S, T ), where the removed vertices form set S, and T is the smallest connected component of Gn,n,p\S. In this case, T has no neighbor after removing S. Besides, every node in S is incident with at least one edge leading
to T , otherwise Gn,n,p can be disconnected be removing less than r vertices. We want to show if

log n

p = (1 + o(1)) ,

(48)

n

then

1 P ∃S, T, 2 ≤ |T | ≤ 2 (2n − r) : A(S, T ) = o(1). (49)

Here T is upper bounded by 12 (2n − r) as T is assumed to be the smallest remaining component after removing S. We choose p with the value in (48) as it includes all the values of p (t) and p (t),
t ∈ [r], where

log n + (t − 1) log log n + x p (t) =
n

log n + (t − 1) log log n − x

p (t) =

.

n

where x = o(log log n) → ∞ when n → ∞. According to the deﬁnition of the sharp threshold, p (t) and the p (t) are the probabilities we need discuss to show that log n+(t−n1) log log n is the sharp threshold function of being t-connected. If (49) is true, the only case we need consider for event A(S, T ) is |T | = 1. In other words, if A(S, T ) happens, the remaining subgraph Gn,n,p\S after removing S from Gn,n,p consists of some isolated vertices and a huge component, where the isolated vertices have degree r. Therefore,

P(G ∈ Kr) ≈ P(G ∈ Dr).

This is because δ(Gn,n,p) = r means the connectivity of Gn,n,p is less than or equal to r. However, if the connectivity is less than r, according to (49), there exists some vertices have degree less than r, which contradicts δ(Gn,n,p) = r. On the other hand, (49) also implies that if Gn,n,p is r-connected,

32

then δ(Gn,n,p) = r. In this case, we can show that the sharp threshold of Kr is equal to the sharp threshold of Dr, i.e., (47).

Next, we will prove (49) holds with p = (1 + o(1)) long n . Fix the set S and T , where S consists of exactly s1 nodes from partition L and exactly s2 nodes from partition W , T consists of exactly t1 nodes T from L, and exactly t2 nodes from W . Under such assumptions, let Ps1,s2,t1,t2 denote the probability that event A(S, T ) happens. Let A1 denote the event that T is connected, A2 denote the event that T is not connected to any vertex in Gn,n,p\(S ∪ T ), and A3 denote the event that each vertex in S is incident with at least one edge leading to T . Event A(S, T ) happens when event A1, A2, and A3 happen at the same time, thus we have
Ps1,s2,t1,t2 = P(A1 and A2 and A3)

= P(A1)P(A2)P(A3),

(50)

where in the second equality, we applied the fact that A1, A2, and A3 are independent of each other (the appearance of the edges in Gn,n,p are identical independent random variables, A1, A2, and A3

refers to different edges).

Next we consider the probability of event A1, A2, and A3 respectively. T is a connected subgraph implies that it contains a spanning tree with t1 + t2 − 1 edges. According to [12], the number of different spanning trees in Kt1,t2 is tt12−1tt21−1. By applying the union bound, we have
P(A1) ≤ tt12−1tt21−1pt1+t2−1.

The probability that T is disconnected with Gn,n,p\(S ∪ T ) is

P(A2) = (1 − p)t1(n−s2−t2)+t2(n−s1−t1).

As Gn,n,p is a bipartite graph, the vertices in L can only be connected with vertices in W . Let S1 be

the subset of S which contains all the nodes from L, and S2 be the subset of S which contains all

the nodes from W . Similarly, suppose T1 be the subset of T which consists of all the nodes from L,

and T2 consists of all the nodes from W . Therefore, each vertex in S is incident with at least one

edge leading to T implies that there exists at least s1 edges between S1 and T2, and at least s2 edges

between S2 and T1, hence we have

P(A3) ≤

s1t2 ps1 s2t1 ps2

s1

s2

Then we can bound Ps1,s2,t1,t2 as following

Ps1,s2,t1,t2 = P(A1)P(A2)P(A3)

≤ tt2−1tt1−1pt1+t2−1(1 − p)t1(n−s2−t2)+t2(n−s1−t1) s1t2 ps1 s2t1 ps2

12

s1

s2

≤ tt12−1tt21−1pt1+t2−1e−p[t1(n−s2−t2)+t2(n−s1−t1)] (t2ep)s1 (t1ep)s2 , where in the second inequality, we used the facts

n

≤

ne k ,

(1 − p) ≤ e−p,

∀0 ≤ p ≤ 1.

k

k

Now, by applying the union bound, we can bound the probability P(∃S, T ) in (49) as

2n−r

2

n n n − s1 n − s2

P(∃S, T ) ≤

s1 s2

t1

t2

Ps1 ,s2 ,t1 ,t2

s1+s2=r t1+t2=2

2n−r 2
≤
s1+s2=r t1+t2=2

ne s1 s1

ne s2 s2

(n − s1)e t1 t1

e−p[t1(n−s2−t2)+t2(n−s1−t1)] (t2ep)s1 (t1ep)s2

(n − s2)e t2

t2
t1t2 −1 tt21 −1 pt1 +t2 −1

2n−r

2

≤

net2epept2 s1 net1epept1 s2 (enp)t1+t2 e−p[t1(n−t2)+t2(n−t1)]p−1

s1+s2=r t1+t2=2

2n−r

2

≤

p−1As1 Bs2 C,

(51)

s1+s2=r t1+t2=2

33

where in the second inequality, we also applied nk t1 t2−t1 ≤ 1, t2

≤ nke k, in the third inequality, we applied (t1t2)−1 ≤ 1

and in the last step,

A

=

e2npt2ept2

=

e2(1

+

t2 +o(t2 )
o(1))t2n n

log n,

B

=

e2npt1ept1

=

e2(1

+

t1 +o(t1 )
o(1))t1n n

log n,

C = (enp)t1+t2 e−p[t1(n−t2)+t2(n−t1)].

Let t1 + t2 = t, we have

C = (enp)t1+t2 e−p[t1(n−t2)+t2(n−t1)]

= (enp)te−npte2pt1t2

≤

(enp

)

t

e

−

npt+

t2 p 2

=

enpe

−

np+

pt 2

t

= Dt,

where in the inequality, we used

t2 2pt1t2 = 2pt1(t − t1) ≤ 2 p,

as f (x) = 2px(t − x) attains its maximum at x = 2t , and in the last step

D

=

enpe

−

np

+

pt 2

t +o( t )

= e(1 + o(1))n−1−o(1)+ 2

2 n

log n.

Thus, we can further bound the probability P(∃S, T ) as

P(∃S, T ) ≤ p−1

2n−r 2
As1 Bs2 Dt.

s1+s2=r t=2

(52)

Since if 1 ≤ t1 ≤ log n, 1 ≤ t2 ≤ log n, then A = O((log n)2), B = O((log n)2),

D = n−1+o(1),

if t1 > log n, t2 > log n, then A = O(n3),

B = O(n3),

D

≤

n

−

1 3

,

as t < n, and if 1 ≤ t1 ≤ log n, t2 > log n, then A = O(n3), B = O((log n)2),

D

≤

n

−

1 3

,

if 1 ≤ t2 ≤ log n, t1 > log n, then A = O((log n)2),

B = O(n3),

D

≤

n

−

1 3

.

No matter which case, we have

p−1As1 Bs2 Dt = o(1),

Thus the sum in (52) is o(1).

We have found the threshold function of r-connected for Gn,n,p, however, the sufﬁcient and necessary conditions for M-MSR algorithm to succeed are deﬁned with the property robustness. Therefore, we also need conﬁrm the threshold function of r-robustness. The following lemma is an important step to derive this threshold function.
34

H.2 r-robustness for random bipartite graph

Deﬁnition 12. For Gn,n,p and constant r ∈ Z≥1, let Er be the property that every subset of V (G) with size up to n is r-reachable.

Here, Gn,n,p is a bipartite graph where the total number of the nodes is 2n.

Lemma 5. Consider random bipartite graph Gn,n,p. Then

lim P(Gn,n,p ∈ Er) = 1,

(53)

n→∞

if

log n + (r − 1) log log n + x

p(n) =

,

(54)

n

where x = o(log log n) satisfying x → ∞ when n → ∞ .

Proof. Let Ae denote the event that there exists a subset of V (G) with size less than n is not r-reachable, then we have

P(Gn,n,p ∈ Er) = 1 − P(Ae).

To prove (53) holds, we can show that P(Ae) = o(1) with probability p in (54).

Recall that L and W are the vertex partitions of the bipartite graph Gn,n,p. Consider a subgraph of Gn,n,p where there exists k1 vertices from L and k2 vertices from W . Denote this subgraph as S, and let the probability that S is not r-reachable be Pk1,k2 . From the proof of Lemma 4, we know when p has the value as in (54), the probability that a vertex has degree less than r is o(1). In other words,
the probability that a subset consists of one node is not r-reachable is o(1). Then by applying the
union bound, we have

n

P(Ae) ≤

Pk1,k2 .

k1 +k2 =2

Consider a vertex j ∈ S, j is not r-reachable means it has less than r neighbors from outside. If j is a vertex in L, the probability that it is not r-reachable is

r−1 n − k2 pi(1 − p)n−k2−i. i
i=0
If j is a vertex in W , the probability that it is not r-reachable is

r−1 n − k1 pi(1 − p)n−k1−i. i
i=0
As S is not r-reachable implies that every vertex in S is not r-reachable, also there exists k1 vertices of S in L and k2 vertices of S in W . By applying the union bound, we have

Pk1 ,k2

nn ≤
k1 k2

r−1

k1 r−1

n − k2 pi(1 − p)n−k2−i

n − k1 pi(1 − p)n−k1−i

i=0 i

i=0 i

r−1

k1

r−1

k2

ne ≤

nipi(1 − p)n−k2−i

ne nipi 1 − p)n−k1−i

k1 i=0

k2 i=0

r−1 k1

r−1 k2

≤ ne (1 − p)n−k2 r np

ne (1 − p)n−k1 r np

k1

1−p

k2

1−p

≤ er n e−p(n−k2)(np)r−1 k1 er n e−p(n−k1)(np)r−1 k2 ,

(1 − p)r−1 k1

(1 − p)r−1 k2

k2
(55)

35

where in the second inequality we applied the inequalities

n

en k

≤

,

k

k

n − k ≤ (n − k)i ≤ ni, i

and in the third inequality, we used

r−1 np i

np r−1

≤r

,

i=0 1 − p

1−p

which based on the fact that 1n−pp > 1, in the last inequality of (55), we applied 1 − p ≤ e−p for 0 ≤ p ≤ 1.

Let c1

be a constant satisfying

er (1−p)r−1

≤

c1.

For sufﬁciently large n, we have 0

<

c1

<

2er.

Let

k1 + k2 = k, then (55) can be rewritten as

k nk 2k k p −pnk

(r−1)k

Pk1,k2 ≤ c1 k k e 1 2 e

(np)

k11 k22

k nk 2k k p

e−kx

k(r−1)

≈ c1 kk1 kk2 e 1 2 nk(log n)k(r−1) (log n)

12

k e2k1k2p −kx = c1 k k e ,
k11 k22

(56)

where in the approximate equality, we applied

e−pnk =e−k log n−k(r−1) log log n−kx =

e−kx ,

nk(log n)k(r−1)

np = log n + (r − 1) log log n + x ≈ log n.

Now

consider

the

term

e2k1 k2 p

and

k

k1 1

k2k2

in

(56),

which

can

be

written

as

e2k1k2p = e2k1(k−k1)p, k1k1 k2k2 = ek1 log k1+k2 log k2 = ek1 log k1+(k−k1) log(k−k1).

Let

f (x) = 2x(k − x)p, g(x) = x log x + (k − x) log(k − x).

For x ∈ [0, k], f (x) attains its minimum at x = k2 , g(x) attains its maximum at x = k2 , i.e.,

k

k2p

k

k

f (x) ≤ f

= , g(x) ≥ g

= k log

, ∀x ∈ [0, k].

2

2

2

2

As a consequence, we have

e2k1 k2 p

k2 p
≤e 2 ,

k1k1 k2k2 ≥ek log( k2 ) =

kk ,
2

and (56) can be bounded by

k

e

1 2

k

2

p

−kx

Pk1,k2 ≤ c1 2−kkk e =

2c1

e

1 2

k

p

−

log

k

e

−x

k
.

Now we can bound the probability P(Ae) as

n

n

k

1 kp−log ke−x .

P(Ae) ≤

Pk1,k2 =

2c1e 2

k1 +k2 =2

k=2

(57)

36

Consider

the

term

e

1 2

k

p

−log

k

in

(57),

let

1 f (k) = kp − log k.
2

Then

1 1 1 log n

1

f (k) = p − =

(1 + o(1)) − .

2 k 2n

k

As k ranges in the interval [2, n], f (k) = 0 has only one solution in [2, n], and f (2) < 0 while f (n) > 0, which implies

f (k) ≤ max{f (2), f (n)},

where

Therefore,
n
P(Ae) =
k=2

f (2) = 2p − log 2 < 0,

n log n

f (n) =

(1 + o(1)) − log(n) < 0.

2n

1 kp−log k −x k n

−x k

4c1e−2x

2c1e 2

e

< (2c1e ) ≤ 1 − 2c1e−x = o(1),

k=2

where in the second inequality, we applied

2c1e−2x ≤ 4er < 1. e2x

Lemma 6. For any r ∈ Z≥1, if a graph G is r-robust, then G is at least r-connected.
Proof. We will prove this lemma by contradiction. Suppose there exists a graph G which is r-robust and its connectivity is less than or equal to r − 1. According to the deﬁnition of r-connected, there exists a subset of V (G) with size r − 1 such that G will be disconnected with the removal of this subset. In other words, if this speciﬁc subset is removed, there will be at least two components remains. Choose one of the remained components arbitrarily, let it be S1, let the union of all the other remained components be S2. Then S1 and S2 are nonempty and disjoint, however, none of the node in S1 or S2 has more than r − 1 neighbors outside, which means both S1 and S2 are not r-reachable. This contradicts our assumption that G is r-robust (for every pair of disjoint and nonempty subsets of V (G), at least one of them is r-reachable), thus we can prove that if G is r-robust, then G is at least r-connected.
Deﬁnition 13. For Gn,n,p and constant Z≥1, let Rr be the property of r-robust.
Now we are ready to present the sharp threshold for the property of Rr. The following theorem is connected to the work [40] which analyzed the threshold of 2F + 1 robustness in general random graphs.

Theorem 3. Consider random bipartite graph Gn,n,p. For any constant r ∈ Z≥1,

p∗(n) = log n + (r − 1) log log n n
is the sharp threshold function for property Rr. Proof. Let

log n + (r − 1) log log n + x

p=

,

(58)

n

where x = o(log log n) → ∞ when n → ∞. Recall the deﬁnition of r-robust, i.e., a graph G is r-robust if for every pair of nonempty, disjoint subsets of V (G), at least one of the two sets is r-reachable. To show Gn,n,p is r-robust, consider any two disjoint and nonempty subsets of V (G),

37

deﬁne the two sets as S1 and S2. Then at least one of the two sets has size up to n, without loss of generality, let this set be S1. According to Lemma 5, with probability (58), we have
lim P(S1 is r-reachable) = 1,
n→∞
which implies
lim P(Gn,n,p ∈ Rr) = 1.
n→∞

Next consider Gn,n,p with

log n + (r − 1) log log n − x

p(n) =

,

(59)

n

where x = o(log log n) → ∞ when n → ∞. We want to show

lim P(Gn,n,p ∈ Rr) = 0.

(60)

n→∞

From Lemma 6, we know that if Gn,n,p is r-robust, then Gn,n,p is at least r-connected, which means if the connectivity of Gn,n,p is less than r, then Gn,n,p is not r-robust. Besides, according to Lemma 4, with probability (59) we have

lim P(Gn,n,p ∈ Kt) = 0, ∀t ≥ r.
n→∞

Thus, we can obtain (60).

H.3 Proof of Theorem 1

Now, we are ready to present the proof of Theorem 1.

Proof.

We ﬁrst argue that any skew-nonamplifying matrix completion method in the F -local model can be used to achieve resilient consensus in the F -local model over the bipartite graph corresponding to the revealed entries (recall that the resilient consensus problem was deﬁned earlier in Deﬁnition 11).

Indeed, to achieve consensus starting from the initial conditions xi(0), we simply reveal entries corresponding to the all-ones matrix, and initialize ui(0) = xi(0) on the left-side of the bipartition and vi(0) = 1/xi(0) on the right-hand side of the bipartition. We then apply the skew-nonamplifying matrix completion method.

We can then deﬁne the values ki(t), ki(t) of a node just as in Eq. (18). Then according to the skew-nonamplifying property, it follows that for normal nodes

ki(t), kj(t) ∈ [min xi(0), max xi(0)]

i

i

Since

ui(t)vj(t) = ki(t) · 1 = ki(t) → 1, kj(t) kj(t)

for all the normal nodes, we obtain that the quantities ki(t), kj(t) among the normal nodes achieve consensus, and, as already remarked above, the quantities always stay in [mini xi(0), maxi xi(0)]. It follows that we have an algorithm for resilient consensus.

Next, according to Lemma 1, a necessary condition for resilient consensus in the F -local corrupted model is cp(G) ≥ 2F + 1. It follows that a necessary condition for skew-nonamplifying matrix completion in the F local model is cp(G) ≥ 2F + 1.

But from the result of Lemma 4, we know that a sharp threshold for being r-connected is

log n + (r − 1) log log n

p=

,

(61)

n

where x = o(log log n) → ∞, when n → ∞. Therefore, if

log n + 2F log log n − x

p=

,

(62)

n

38

then the graph G will be such that no skew-nonamplifying algorithm can guarantee convergence under the F -local model. This proves Theorem 1(b).
Next consider the sufﬁcient condition to correctly recover X by applying the M-MSR algorithm. According to Theorem 2, the sufﬁcient condition that the normal rows and columns of X can be correctly recovered by M-MSR is G(Ω) is 2F + 1-robust. On the other hand, Theorem 3 shows if p has the value as in (61), G(Ω) is r-robust. Thus, the sufﬁcient condition that the normal rows and columns of X can be correctly recovered is (62), proving Theorem 1(a).
From Theorem 1 and Corollary 3, we can see when G(Ω) is a random bipartite graph Gn,n,p, the proposed M-MSR algorithm is the optimal algorithm in rank-one matrix completion problem with corruptions.

H.4 Further Results

In practical applications, It is not easy to conﬁrm if G(Ω) is F -local nodes-corrupted model. However, in random graph, we can represent a F -local model with a F -total model, which is more convenient to be veriﬁed. The following lemma provides a bridge from F -total model to F -local model. Particularly, we provide the fraction of the corrupted nodes in L and W , respectively, so that G(Ω) can be f -fraction local model.

Lemma

7.

Consider

a

random

bipartite

graph

Gn,n,p,

let

p

≥

12(1+η) log n n

for

some

η

>

0,

then

corrupt αn left nodes and βn right nodes uniformly at random, where 0 ≤ α, β < 1. In this case, for

each normal node, fraction of edges from every normal node leading to corrupted nodes is less than

f with high probability when n → ∞ if

α ≤ f − 1,

(63)

β ≤ f − 2,

(64)

where 0 < 1, 2 ≤ f are any constants.

Proof. First, we will show that the vertex degree of Gn,n,p is bounded with high probability when n → ∞. Since the degree of each node in Gn,n,p is the sum of n independent Bernoulli random variables with parameter p. Then for a node i ∈ V (G), by applying Chernoff bound, we can obtain

P(di − µ ≤ −δµ) ≤ e−µδ2/3,

where di represents the degree of node i, δ is a constant satisfying 0 ≤ δ ≤ 1, µ is the expected degree of node i, i.e., µ = np. By applying the union bound, we have

P(δ(Gn,n,p) ≤ (1 − δ)np) ≤ 2ne−npδ2/3 = 2elog n−npδ2/3,

(65)

where δ(G(n, n, p)) is the minimum degree of graph G(n, n, p). Let δ = 21 , and apply the inequality p ≥ 12(1+nη) log n , we can rewrite (65) as

P

1 δ(Gn,n,p) ≤ np

≤ 2elog n−npδ2/3 ≤ 2 = o(1),

2

nη

which implies that

1 P δ(G(n, n, p)) > 2 np = P (δ(G(n, n, p)) > 6(1 + η) log n) ≈ 1.

As δ(G(n, n, p)) is the minimum degree of the graph G(n, n, p), for every node i in G(n, n, p), we also have

P (di > 6(1 + η) log n) ≈ 1.

(66)

Next, consider the number of corrupted neighbors for each normal node. Suppose i ∈ L, then the probability of one outgoing edge of i leading to corrupted nodes is β. Hence, the expected number of corrupted neighbors for node i is βdi. Let Yj be a random variable which represents the expected number of corrupted neighbors of node i given that the ﬁrst j edges leaving i have been revealed. Then the sequence of random variables Y0, · · · , Ydi is a martingale such that

|Yj+1 − Yj| ≤ 1, ∀j ∈ [di].

39

Note that among the sequence of random variables, Y0 is the expected number of the infected neighbors of node i, i.e., Y0 = βdi, Ydi is the expected number of corrupted neighbors of node i when all of the outgoing edges have been revealed, which is exactly the number of corrupted
neighbors of node i. Thus, according to Azuma’s Inequality, we have

λ2 d2

−
2

i di 12

− 1 λ2d

P (Ydi − Y0 ≥ λdi) ≤ e j=1 = e 2 i .

(67)

Let λ = f − β, according to (63), we have λ ≥ 1. By combining (66) and (67), we can derive that the following inequality holds with high probability

− 1 λ2d

− 1 26(1+η) log n

e3(1+η)

2 1

P(Ydi − Y0 ≥ λdi) = P(Ydi ≥ f di) ≤ e 2 i < e 2 1

=

= o(1),

n

which implies with high probability

P(Ydi < f di) ≈ 1.

Thus we complete the proof for the nodes in L, the proof for nodes in W is similar.

I Sign Determination

In this section, we present the details about the sign pattern determination for rank-one matrices.
In the crowdsourcing problem, we allow the existence of the adversaries, which can lead to some of the entries of Cˆ are corrupted. Besides, Cˆ is an empirical estimate for ss . In other words, it is possible that we can not ﬁnd a sign pattern for s which perfectly matches with the sign pattern of Cˆ. Therefore, our goal is to ﬁnd a sign pattern for s to minimize the number of mismatching elements of sign(ss ) and sign(Cˆ).
To start with, consider a two-coloring problem: given a graph, color every node with one of two colors(e.g., red or blue) minimizing the number of "violations", where we say a violation occurs for each edge connecting nodes of the same color. Next, we will transfer our sign pattern determination problem to a two-coloring problem. Suppose the nodes value of node i is |si|, sign + represent color blue, sign − represent color red. If an edge has two same color incident nodes, we call this edge is a "same color" edge, otherwise is an "opposite" color edge. In our problem, we are given the pattern of the edges, and we aim to color the nodes. To be consistent with the two coloring problem, we introduce some new nodes and edges as following: if an edge is an "opposite color" edge, we will put a new node in the middle of this edge, then the original "opposite color" edge becomes two "same color" edges. The obtained new graph is denoted as G˜. Thus, if we can ﬁnd a way to color G˜ so that it satisﬁes the two-coloring rule, we also can get the sign pattern we are looking for. In Figure 8, an example is provided to illustrate this process.

-1 2 -4 -2 3

1

-1

1

1

2

2

-4

2

2

-2

3

3

1

3

1

1

-1

2

-2

2

1

3

1

Figure 8: Illustration for the sign pattern determination with a two-coloring method

The next step is to solve the two-coloring problem. To do that, we deﬁne a stochastic matrix A for graph G˜ as following: let Aij = d1i , Aji = d1j whenever nodes i and j are connected, and Aij = Aji = 0 otherwise. Then compute the eigenvector v of A corresponding to the smallest eigenvalue. Finally assign + to node i if vi > 0 and − to node i if vi < 0.
There are two reasons why this approach works for two-coloring problem. First, suppose there exists a perfect assignment solution for this issue. The the graph G˜ is a bipartite graph. In this case, according to [23], the smallest eigenvalue of A will be −1 and the corresponding eigenvector will be

40

composed of +1s and −1s which corresponds to different components of the bipartite graph. Second, in the event that there is no perfect assignment, according to [23],

λr(n) = min 2xixj,

(68)

E(G˜)

n
s.t. dix2i = 1,

i=1

where λr(n) is the smallest eigenvalue of A, xi, xj are the elements of the eigenvector corresponding
to λr(n) which are connected by edge (i, j). It can be seen the eigenvector x is the solution of the optimization problem (68) which minimizes the number of the same color edges of G˜.

41

