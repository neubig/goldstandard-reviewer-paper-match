arXiv:2010.12561v1 [cs.LG] 23 Oct 2020

Train simultaneously, generalize better: Stability of gradient-based minimax learners
Farzan Farnia, Asuman Ozdaglar {farnia,asuman}@mit.edu
Massachusetts Institute of Technology
Abstract The success of minimax learning problems of generative adversarial networks (GANs) has been observed to depend on the minimax optimization algorithm used for their training. This dependence is commonly attributed to the convergence speed and robustness properties of the underlying optimization algorithm. In this paper, we show that the optimization algorithm also plays a key role in the generalization performance of the trained minimax model. To this end, we analyze the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms through the lens of algorithmic stability under both convex concave and non-convex non-concave minimax settings. While the GDA algorithm is not guaranteed to have a vanishing excess risk in convex concave problems, we show the PPM algorithm enjoys a bounded excess risk in the same setup. For non-convex non-concave problems, we compare the generalization performance of stochastic GDA and GDmax algorithms where the latter fully solves the maximization subproblem at every iteration. Our generalization analysis suggests the superiority of GDA provided that the minimization and maximization subproblems are solved simultaneously with similar learning rates. We discuss several numerical results indicating the role of optimization algorithms in the generalization of the learned minimax models.
1 Introduction
Minimax learning frameworks including generative adversarial networks (GANs) (Goodfellow et al., 2014) and adversarial training (Madry et al., 2017) have recently achieved great success over a wide array of learning tasks. In these frameworks, the learning problem is modeled as a zero-sum game between a ”min" and ”max" player that is solved by a minimax optimization algorithm. The minimax optimization problem of these learning frameworks is typically formulated using deep neural networks, which greatly complicates the theoretical and numerical analysis of the optimization problem. Current studies in the machine learning literature focus on fundamental understanding of general minimax problems with emphasis on convergence speed and optimality.
The primary focus of optimization-related studies of minimax learning problems has been on the convergence and robustness properties of minimax optimization algorithms. Several recently proposed algorithms have been shown to achieve faster convergence rates and more robust behavior around local solutions. However, training speed and robustness are not the only factors required for the success of a minimax optimization algorithm in a learning task. In this work, our goal is to show that the generalization performance of the learned minimax model is another key property that is inﬂuenced by the underlying optimization algorithm. To this end, we present theoretical and numerical results demonstrating that:
Diﬀerent minimax optimization algorithms can learn models with diﬀerent generalization properties.
1

In order to analyze the generalization behavior of minimax optimization algorithms, we use the algorithmic stability framework as deﬁned by Bousquet and Elisseeﬀ (2002) for general learning problems and applied by Hardt et al. (2016) for analyzing stochastic gradient descent. Our extension of (Bousquet and Elisseeﬀ, 2002)’s stability approach to minimax settings allows us to analyze and compare the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms. Furthermore, we compare the generalization performance between the following two types of minimax optimization algorithms: 1) simultaneous update algorithms such as GDA where the minimization and maximization subproblems are simultaneously solved, and 2) non-simultaneous update algorithms such as GDmax where the maximization variable is fully optimized at every iteration.
In our generalization analysis, we consider both the traditional convex concave and general non-convex nonconcave classes of minimax optimization problems. For convex concave minimax problems, our bounds indicate a similar generalization performance for simultaneous and non-simultaneous update methods. Speciﬁcally, we show for strongly-convex strongly-concave minimax problems all the mentioned algorithms have a bounded generalization risk on the order of O(1/n) with n denoting the number of training samples. However, in general convex concave problems we show that the GDA algorithm with a constant learning rate is not guaranteed to have a bounded generalization risk. On the other hand, we prove that proximal point methods still achieve a controlled generalization risk, resulting in a vanishing O( 1/n) excess risk with respect to the best minimax learner with the optimal performance on the underlying distribution.
For more general minimax problems, our results indicate that models trained by simultaneous and nonsimultaneous update algorithms can achieve diﬀerent generalization performances. Speciﬁcally, we consider the class of non-convex strongly-concave problems where we establish stability-based generalization bounds for both stochastic GDA and GDmax algorithms. Our generalization bounds indicate that the stochastic GDA learner is expected to generalize better provided that the min and max players are trained simultaneously with similar learning rates. In addition, we show a generalization bound for the stochastic GDA algorithm in general non-convex non-concave problems, which further supports the simultaneous optimization of the two min and max players in general minimax settings. Our results indicate that simultaneous training of the two players not only can provide faster training, but also can learn a model with better generalization performance. Our generalization analysis, therefore, revisits the notion of implicit competitive regularization introduced by Schäfer et al. (2019) for simultaneous gradient methods in training GANs.
Finally, we discuss the results of our numerical experiments and compare the generalization performance of GDA and PPM algorithms in convex concave settings and single-step and multi-step gradient-based methods in non-convex non-concave GAN problems. Our numerical results also suggest that in general non-convex non-concave problems the models learned by simultaneous optimization algorithms can generalize better than the models learned by non-simultaneous optimization methods. We can summarize the main contributions of this paper as follows:
• Extending the algorithmic stability framework for analyzing generalization in minimax settings,
• Analyzing the generalization properties of minimax models learned by GDA and PPM algorithms in convex concave problems,
• Studying the generalization of stochastic GDA and GDmax learners in non-convex non-concave problems,
• Providing numerical results on the role of optimization algorithms in the generalization performance of learned minimax models.
2 Related Work
Generalization in GANs: Several related papers have studied the generalization properties of GANs. Arora et al. (2017) study the generalization behavior of GANs’ learned models and prove a uniform convergence generalization bound in terms of the number of the discriminator’s parameters. Wu et al. (2019) connect the algorithmic stability notion to diﬀerential privacy in GANs and numerically analyze the generalization behavior
2

of GANs. References (Zhang et al., 2017; Bai et al., 2018) show uniform convergence bounds for GANs by analyzing the Rademacher complexity of the players. Feizi et al. (2020) provide a uniform convergence bound for the W2GAN problem. Unlike the mentioned related papers, our work provides algorithm-dependent generalization bounds by analyzing the stability of gradient-based optimization algorithms. Also, the related works (Arora and Zhang, 2017; Thanh-Tung et al., 2019) conduct empirical studies of generalization in GANs using birthday paradox-based and gradient penalty-based approaches, respectively.
Generalization in adversarial training: Understanding generalization in the context of adversarial training has recently received great attention. Schmidt et al. (2018) show that in a simpliﬁed Gaussian setting generalization in adversarial training requires more training samples than standard non-adversarial learning. Farnia et al. (2018); Yin et al. (2019); Khim and Loh (2018); Wei and Ma (2019); Attias et al. (2019) prove uniform convergence generalization bounds for adversarial training schemes through Pac-Bayes (McAllester, 1999; Neyshabur et al., 2017b), Rademacher analysis, margin-based, and VC analysis approaches. Zhai et al. (2019) study the value of unlabeled samples in obtaining a better generalization performance in adversarial training. We note that unlike our work the generalization analyses in the mentioned papers prove uniform convergence results. In another related work, Rice et al. (2020) empirically study the generalization performance of adversarially-trained models and suggest that the generalization behavior can signiﬁcantly change during training.
Stability-based generalization analysis: Algorithmic stability and its connections to the generalization properties of learning algorithms have been studied in several related works. Shalev-Shwartz et al. (2010) discuss learning problems where learnability is feasible considering algorithmic stability, while it is infeasible with uniform convergence. Hardt et al. (2016) bound the generalization risk of the stochastic gradient descent learner by analyzing its algorithmic stability. Feldman and Vondrak (2018, 2019); Bousquet et al. (2020) provide sharper stability-based generalization bounds for standard learning problems. While the above works focus on standard learning problems with a single learner, we use algorithmic stability to analyze generalization in minimax settings with two players.
Connections between generalization and optimization in deep learning: The connections between generalization and optimization in deep learning have been studied in several related works. Analyzing the double descent phenomenon (Belkin et al., 2019; Nakkiran et al., 2019; Mei and Montanari, 2019), the eﬀect of overparameterization on generalization (Li and Liang, 2018; Allen-Zhu et al., 2019; Arora et al., 2019; Cao and Gu, 2019; Wei et al., 2019; Bietti and Mairal, 2019; Allen-Zhu and Li, 2019; Ongie et al., 2019; Ji and Telgarsky, 2019; Bai and Lee, 2019), and the sharpness of local minima (Keskar et al., 2016; Dinh et al., 2017; Neyshabur et al., 2017a) have been performed in the literature to understand the implicit regularization of gradient methods in deep learning (Neyshabur et al., 2014; Zhang et al., 2016; Ma et al., 2018; Lyu and Li, 2019; Chatterjee, 2020). Schäfer et al. (2019) extend the notion of implicit regularization to simultaneous gradient methods in GAN settings and discuss an optimization-based perspective to this regularization mechanism. However, we focus on the generalization aspect of the implicit regularization mechanism. Also, Nagarajan and Kolter (2019) suggest that uniform convergence bounds may be unable to explain generalization in supervised deep learning.
Analyzing convergence and stability of minimax optimization algorithms: A large body of related papers (Heusel et al., 2017; Sanjabi et al., 2018; Lin et al., 2019; Schäfer and Anandkumar, 2019; Fiez et al., 2019; Nouiehed et al., 2019; Hsieh et al., 2019; Du and Hu, 2019; Wang et al., 2019; Mazumdar et al., 2019; Thekumparampil et al., 2019; Farnia and Ozdaglar, 2020; Mazumdar et al., 2020; Zhang et al., 2020) study convergence properties of ﬁrst-order and second-order minimax optimization algorithms. Also, the related works (Daskalakis et al., 2017; Daskalakis and Panageas, 2018; Gidel et al., 2018; Liang and Stokes, 2019; Mokhtari et al., 2020) analyze the convergence behavior of optimistic methods and extra gradient (EG) methods as approximations of the proximal point method. We also note that we use the algorithmic stability notion as deﬁned by Bousquet and Elisseeﬀ (2002), which is diﬀerent from the local and global stability properties of GDA methods around optimal solutions studied in the related papers (Mescheder et al., 2017; Nagarajan and Kolter, 2017; Mescheder et al., 2018; Feizi et al., 2020).
3

3 Preliminaries
In this paper, we focus on two standard families of minimax optimization algorithms: Gradient Descent Ascent (GDA) and Proximal Point Method (PPM). To review the update rules of these algorithms, consider the following minimax optimization problem for minimax objective f (w, θ) and feasible sets W, Θ:

min max f (w, θ).

(1)

w∈W θ∈Θ

Then, for stepsize values αw, αθ, the followings are the GDA’s and GDmax’s update rules:

GGDA( w ) := w − αw∇wf (w, θ) , GGDmax( w ) := w − αw∇wf (w, θ)

(2)

θ

θ + αθ∇θf (w, θ)

θ

argmaxθ∈Θf (w, θ)

In the above, argmaxθ∈Θf (w, θ) is the optimal maximizer for w. Also, given stepsize parameter η the update rule of PPM is as follows:

GPPM( w ) := argmin argmax

1 f (w, θ) +

w−w 2− 1

θ−θ 2 ,

(3)

θ

w∈W θ∈Θ

2η

2 2η

2

In the Appendix, we also consider and analyze the PPmax algorithm that is a proximal point method fully solving the maximization subproblem at every iteration. Throughout the paper, we commonly use the following assumptions on the Lipschitzness and smoothness of the minimax objective.

Assumption 1. f (w, θ) is jointly L-Lipschitz in (w, θ) and Lw-Lipschitz in w over W × Θ, i.e., for every w, w ∈ W, θ, θ ∈ Θ we have

f (w, θ) − f (w , θ ) ≤ L

w−w

2 2

+

θ−θ

22,

f (w, θ) − f (w , θ) ≤ Lw w − w 2.

(4)

Assumption 2. f (w, θ) is continuously diﬀerentiable and -smooth on W × Θ, i.e., ∇wf (w, θ), ∇θf (w, θ) is -Lipschitz on W × Θ and for every w, w ∈ W, θ, θ ∈ Θ we have

∇wf (w, θ) − ∇wf (w , θ ) 22 + ∇θf (w, θ) − ∇θf (w , θ ) 22 ≤ 2 w − w 22 + θ − θ 22 . (5)

We focus on several classes of minimax optimization problems based on the convexity properties of the

objective function. Note that a diﬀerentiable function g(u) is called convex in u if it satisﬁes the following

inequality for every u1, u2:

g(u2) ≥ g(u1) + ∇g(u1) (u2 − u1).

(6)

Furthermore, g is called µ-strongly-convex if for every u1, u2 it satisﬁes

g(u2) ≥ g(u1) + ∇g(u1) (u2 − u1) + µ2 u2 − u1 22. (7)

Also, g is called concave and µ-strongly-concave if −g is convex and µ-strongly-convex, respectively.

Deﬁnition 1. Consider convex feasible sets W, Θ in minimax problem (1). Then,

• The problem is called convex concave if f (·, θ) and f (w, ·) are respectively convex and concave functions for every w, θ.

• The problem is called µ-strongly-convex strongly-concave if f (·, θ) and f (w, ·) are respectively µ-stronglyconvex and µ-strongly-concave functions for every w, θ.

• The problem is called non-convex µ-strongly-concave if f (w, ·) is µ-strongly-concave for every w.

4

4 Stability-based Generalization Analysis in Minimax Settings

Consider the following optimization problem for a minimax learning task:

min max R(w, θ) := EZ∼PZ f (w, θ; Z)

(8)

w∈W θ∈Θ

The above minimax objective represents a cost function f (w, θ; Z) for minimization and maximization

variables w, θ and data variable Z that is averaged under the underlying distribution PZ. We call the

objective function R(w, θ) the true minimax risk. We also deﬁne R(w) as the worst-case minimax risk over

the maximization variable θ:

R(w) := max R(w, θ)

(9)

θ∈Θ

In the context of GANs, the worst-case risk R(w) represents a divergence measure between the learned and true distributions, and in the context of adversarial training it represents the learner’s risk under adversarial perturbations. Since the learner does not have access to the underlying distribution PZ, we estimate the minimax objective using the empirical samples in dataset S = (z1, . . . , zn) which are drawn according to PZ. We deﬁne the empirical minimax risk as:

1n RS(w, θ) := n f (w, θ; zi). (10)
i=1

Then, the worst-case empirical risk over the maximization variable θ is deﬁned as

RS(w) := max RS(w, θ).

(11)

θ∈Θ

We deﬁne the minimax generalization risk gen(w) of minimization variable w as the diﬀerence between the

worst-case true and empirical risks:

gen(w) := R(w) − RS(w).

(12)

The above generalization score measures the diﬀerence of empirical and true worst-case minimax risks. For a randomized algorithm A which outputs random outcome A(S) = (Aw(S), Aθ(S)) for dataset S we deﬁne A’s expected generalization risk as

gen(A) := ES,A R(Aw(S)) − RS(Aw(S)) .

(13)

Deﬁnition 2. A randomized minimax optimization algorithm A is called -uniformly stable in minimization if for every two datasets S, S ∈ Zn which diﬀer in only one sample, for every z ∈ Z, θ ∈ Θ we have

EA f (Aw(S), θ; z) − f (Aw(S ), θ; z) ≤ .

(14)

Considering the above deﬁnition, we show the following theorem that connects the deﬁnition of uniform stability to the generalization risk of the learned minimax model.

Theorem 1. Assume minimax learner A is -uniformly stable in minimization. Then, A’s expected general-

ization risk is bounded as

gen(A) ≤ .

(15)

Proof. We defer the proof to the Appendix.

In the following sections, we apply the above result to analyze generalization for convex concave and non-convex non-concave minimax learning problems.

5

5 Generalization Analysis for Convex Concave Minimax Problems

Analyzing convergence rates for convex concave minimax problems is well-explored in the optimization literature. Here, we use the algorithmic stability framework to bound the expected generalization risk in convex concave minimax learning problems. We start by analyzing the generalization risk in strongly-convex strongly-concave problems. The following theorem applies the stability framework to bound the expected generalization risk under this scenario.

Theorem 2. Let minimax learning objective f (·, ·; z) be µ-strongly-convex strongly-concave and satisfy

Assumption 2 for every z. Assume that Assumption 1 holds for convex-concave f (w, θ; z) := f (w, θ; z) +

µ2 (

θ

2 2

−

w 22) and every z.

Then, full-batch and stochastic GDA and GDmax algorithms with stepsize

αw

= αθ

≤

µ
2

will

satisfy

the

following

bounds

over

T

iterations:

gen(GDA) ≤ (µ −2LαLww2 )n , gen(GDmax) ≤ 2µLn2w . (16)
2
Proof. We defer the proof to the Appendix. In the Appendix, we also prove similar bounds for full-batch and stochastic proximal point methods.

Note that regarding Assumption 1 in the above theorem, we suppose the assumption holds for the deregularized f , because a strongly-convex strongly-concave objective cannot be Lipschitz over an unbounded feasible set. We still note that the theorem’s bounds will hold for the original f if in Assumption 1 we deﬁne f ’s Lipschitz constants over bounded feasible sets W, Θ.
Given suﬃciently small stepsizes for GDA, Theorem 2 suggests a similar generalization performance between GDA and GDmax which are diﬀerent by a factor of L/Lw. For general convex concave problems, it is well-known in the minimax optimization literature that the GDA algorithm can diverge from an optimal saddle point solution. As we show in the following remark, the generalization bound suggested by the stability framework will also grow exponentially with the iteration count in this scenario.

Remark 1. Consider a convex concave minimax objective f (·, ·; z) satisfying Assumptions 1 and 2. Given constant stepsizes αw = αθ = α, the GDA’s generalization risk over T iterations will be bounded as:

(GDA) ≤ O αLLw(1 + α2 2)T/2 . (17)

gen

n

In particular, the bound’s exponential dependence on T is tight for the GDA’s generalization risk in the special case of f (w, θ; z) = w (z − θ).

Proof. We defer the proof to the Appendix.

On the other hand, proximal point methods have been shown to resolve the convergence issues of GDA methods in convex concave problems (Mokhtari et al., 2019, 2020). Here, we also show that these algorithms enjoy a generalization risk growing at most linearly with T .

Theorem 3. Consider a convex-concave minimax learning objective f (·, ·; z) satisfying Assumptions 1 and 2

for every z. Then, full-batch and stochastic PPM with parameter η will satisfy the following bound over T

iterations:

gen(PPM) ≤ 2ηLLwT .

(18)

n

Proof. We defer the proof to the Appendix. In the Appendix, we also show a similar bound for the PPmax algorithm.

The above generalization bound allows us to analyze the true worst-case minimax risk of PPM learners in convex concave problems. To this end, we decompose the true worst-case risk into the sum of the stability and empirical worst-case risks and optimize the sum of these two error components’ upper-bounds. Note that

6

Theorem 3 bounds the generalization risk of PPM in terms of stepsize parameter η and number of iterations T . Therefore, we only need to bound the iteration complexity of PPM’s convergence to an -approximate saddle point. To do this, we show the following theorem that extends Mokhtari et al. (2019)’s result for PPM to stochastic PPM.

Theorem 4. Given a diﬀerentiable minimax objective f (w, θ; z) the average iterate updates w¯ (T ) := T1 Tt=1 w(t), θ¯(T ) := T1 Tt=1 θ(t) of stochastic PPM (SPPM) with setpsize parameter η will satisfy the following for a saddle point [wS∗ , θS∗ ] of the empirical risk under dataset S:

EA RS (w¯ (T )) − RS (wS∗ ) ≤

[w(0), θ(0)] − [wS∗ , θS∗ ] 22 . 2ηT

(19)

Proof. We defer the proof to the Appendix. In the Appendix, we also prove a similar result for stochastic PPmax.
The above convergence result suggests that the expected empirical worst-case risk of applying T iterations of stochastic PPM will be at most O(1/ηT ). In addition, Theorem 3 shows that using that number of iterations the generalization risk will be bounded by O(ηT /n). Minimizing the sum of these two error components, the following corollary bounds the excess risk suﬀered by the PPM algorithm.

Corollary 1. Consider a convex concave minimax objective and a proximal point method optimizer with constant parameter η. Given that w(0) − w∗ 2 + θ(0) − θ∗ 2 ≤ D2 holds with probability 1 for optimal
saddle solution (w∗, θ∗) of the minimax risk, it will take TPPM = 2ηn2DLL2 w iterations for the average iterate w¯ (T ) = T1 Tt=1 w(t) of full-batch and stochastic PPM to have the following bounded excess risk:

ES,A R(w¯ (TPPM)) − R(w∗) ≤

2D2LLw . n

(20)

Proof. We defer the proof to the Appendix. In the Appendix, we prove a similar bound for full-batch and stochastic PPmax as well.

6 Generalization Analysis for Non-convex Non-concave Minimax Problems

In the previous section, we showed that in convex-concave minimax problems simultaneous and nonsimultaneous optimization algorithms have similar generalization error bounds which are diﬀerent by a constant factor L/Lw. However, here we demonstrate that this result does not generalize to general nonconvex non-concave problems. We ﬁrst study the case of non-convex strongly-concave minimax learning problems, where we can analytically characterize the generalization bounds for both stochastic GDA and GDmax algorithms. The following theorem states the results of applying the algorithmic stability framework to bound the generalization risk in such minimax problems.

Theorem 5. Let learning objective f (w, θ; z) be non-convex µ-strongly-concave and satisfy Assumptions 1 and 2. Also, we assume that fmax(w; z) := maxθ∈Θ f (w, θ; z) is bounded as 0 ≤ fmax(w; z) ≤ 1 for every w, z. Then, deﬁning κ := /µ we have

1. The stochastic GDA (SGDA) algorithm with stepsizes αw,t = c/t, αθ,t = cr2/t for constants c > 0, 1 ≤ r ≤ κ satisﬁes the following bound over T iterations:

1+ 1

SGDA) ≤

(r+1)c

gen( n

1

(r+1)c

12(r + 1)cLLw (r+1)c +1 T (r+1)c +1 .

(21)

7

2. The stochastic GDmax (SGDmax) algorithm with stepsize αw,t = c/t for constant c > 0 satisﬁes the following bound over T iterations:

1+ 2

SGDmax) ≤

(κ+2) c

gen( n

2cL2w

2

(κ+2) c

(κ+2) c+2 T (κ+2) c+2 .

(22)

Proof. We defer the proof to the Appendix.
The above result shows that the generalization risks of stochastic GDA and GDmax change with the number of iterations and training set size as:

(r+1)c
gen(SGDA) ≈ O T (r+1)c+1 /n ,

( κ2 +1)c

gen(SGDmax) ≈ O

T

(

κ 2

+1)c+1

/n

.

(23)

Therefore, considering a maximization to minimization stepsize ratio of r2 < κ2/4 will result in a better generalization bound for stochastic GDA compared to stochastic GDmax over a ﬁxed and suﬃciently large number of iterations.
Next, we consider general non-convex non-concave minimax problems and apply the algorithmic stability framework to bound the generalization risk of the stochastic GDA algorithm. Note that the maximized value of a non-strongly-concave function is in general non-smooth. Consequently, the stability framework does not result in a bounded generalization risk for the GDmax algorithm in general non-convex non-concave problems.

Theorem 6. Let 0 ≤ f (·, ·; z) ≤ 1 be a bounded non-convex non-concave objective satisfying Assumptions 1 and 2. Then, the SGDA algorithm with stepsizes max{αw,t, αθ,t} ≤ c/t for constant c > 0 satisﬁes the following bound over T iterations:

1+ 1

(SGDA) ≤

c 2cLLw

gen

n

T . 1
c+1

c c+1

(24)

Proof. We defer the proof to the Appendix.

Theorem 6 also shows that the SGDA algorithm with vanishing stepsize values will have a bounded

generalization risk of O(T

c
c+1 /n)

over

T

iterations.

On the other hand, the stochastic GDmax algorithm

does not enjoy a bounded algorithmic stablility degree in non-convex non-concave problems, since the optimal

maximization value behaves non-smoothly in general.

7 Numerical Experiments
Here, we numerically examine the theoretical results of the previous sections. We ﬁrst focus on a Gaussian setting for analyzing strongly-convex strongly-concave and convex concave minimax problems. Then, we empirically study generative adversarial networks (GANs) as non-convex non-concave minimax learning tasks.
7.1 Convex Concave Minimax Problems
To analyze our generalization results for convex concave minimax settings, we considered an isotropic Gaussian data vector Z ∼ N (0, Id×d) with zero mean and identity covariance. In our experiments, we chose Z’s dimension to be d = 50. We drew n = 1000 independent samples from the underlying Gaussian distribution to form a training dataset S = (z1, . . . , zn). For the µ-strongly-convex strongly-concave scenario, we considered the following minimax objective:
f1(w, θ; z) = w (z − θ) + µ2 w 22 − θ 22 . (25)

8

(a) Generalization risk vs. iteration count in the strongly-convex strongly-concave setting optimized by (top) stochastic GDA and (bottom) stochastic PPM.

(b) Generalization risk vs. iteration count in the convex concave bilinear setting optimized by (top) stochastic GDA and (bottom) stochastic PPM.

Figure 1: Numerical results for convex concave minimax problems

In our experiments, we used µ = 0.1 and constrained the optimization variables to satisfy the norm bounds

w 2, θ 2 ≤ 100 which we enforced by projection after every optimization step. Note that for the above

minimax objective we have

gen(w) = w (E[Z] − ES[Z]),

(26)

where

E[Z]

=

0

is

the

underlying

mean

and

ES [Z]

:=

1 n

n i=1

zi

is

the

empirical

mean.

To optimize the empirical minimax risk, we applied stochastic GDA with stepsize parameters αw = αθ =

0.02 and stochastic PPM with parameter η = 0.02 each for T = 20, 000 iterations. Figure 1a shows the

generalization risk values over the optimization achieved by the stochastic GDA (top) and PPM (bottom)

algorithms. As shown in this ﬁgure, the absolute value of generalization risk remained bounded during the

optimization for both the learning algorithms. In our experiments, we also observed a similar generalization

behavior with full-batch GDA and PPM algorithms. We defer the results of those experiments to the

supplementary document. Hence, our experimental results support Theorem 2’s generalization bounds.

Regarding convex concave minimax problems, as suggested by Remark 1 we considered the following

bilinear minimax objective in our experiments:

f2(w, θ; z) = w (z − θ).

(27)

We constrained the norm of optimization variables as w 2, θ 2 ≤ 100 which we enforced through projection after every optimization iteration. Similar to the strongly-convex strongly-concave objective (25), for the

9

(a) Minimax risk vs. iteration count in the non-convex non-concave SN-GAN problem on CIFAR-10 data optimized by (top) 1,1 Adam descent ascent and (bottom) 1,100 Adam descent ascent

(b) Minimax risk vs. iteration count in the non-convex non-concave SN-GAN problem on CelebA data optimized by (top) 1,1 Adam descent ascent and (bottom) 1,100 Adam descent ascent.

Figure 2: Numerical results for non-convex non-concave minimax problems

above minimax objective we have the generalization risk in (26) with E[Z] and ES[Z] being the true and empirical mean vectors.
We optimized the minimax objective (27) via stochastic and full-batch GDA and PPM algorithms. Figure 1b demonstrates the generalization risk evaluated at diﬀerent iterations of applying stochastic GDA and PPM algorithms. As suggested by Remark 1, the generalization risk of stochastic GDA grew exponentially over the ﬁrst 15,000 iterations before the variables reached the boundary of their feasible sets and then the generalization risk oscillated with a nearly constant amplitude of 6.2. On the other hand, we observed that the generalization risk of the stochastic PPM algorithm stayed bounded and below 0.5 for all the 20,000 iterations (Figure 1b-bottom). Therefore, our numerical experiments also indicate that while in general convex concave problems the stochastic GDA learner can potentially suﬀer from a poor generalization performance, the PPM algorithm has a bounded generalization risk as shown by Theorem 3.
7.2 Non-convex Non-concave Problems
To numerically analyze generalization in general non-convex non-concave minimax problems, we experimented the performance of simultaneous and non-simultaneous optimization algorithms in training GANs. In our GAN experiments, we considered the standard architecture of DC-GANs (Radford et al., 2015) with 4-layer

10

convolutional neural net generator and discriminator functions. For the minimax objective, we used the formulation of vanilla GAN (Goodfellow et al., 2014) that is

f (w, θ; z) = log(Dw(z)) + Eν log(1 − Dw(Gθ(ν))) .

(28)

For computing the above objective, we used Monte-Carlo simulation using 100 fresh latent samples νi ∼ N (0, Ir=128) to approximate the expected value over generator’s latent variable ν at every optimization step. We followed all the experimental details from Gulrajani et al. (2017)’s standard implementation of DC-GAN. Furthermore, we applied spectral normalization (Miyato et al., 2018) to regularize the discriminator function and assist reaching a near optimal solution for discriminator via boundedly many iterations needed for non-simultaneous optimization methods. We trained the spectrally-normalized GAN (SN-GAN) problem over CIFAR-10 (Krizhevsky et al., 2009) and CelebA (Liu et al., 2018) datasets. We divided the CIFAR-10 and CelebA datasets to 50,000, 160,000 training and 10,000, 40,000 test samples, respectively.
To optimize the minimax risk function, we used the standard Adam algorithm (Kingma and Ba, 2014) with batch-size 100. For simultaneous optimization algorithms we applied 1,1 Adam descent ascent with the parameters lr = 10−4, β1 = 0.5, β2 = 0.9 for both minimization and maximization updates. To apply a non-simultaneous algorithm, we used 100 Adam maximization steps per minimization step and increased the maximization learning rate to 5×10−4. We ran each GAN experiment for T =100,000 iterations.
Figure 2 shows the estimates of the empirical and true minimax risks in the CIFAR-10 and CelebA experiments, respectively. We used 2000 randomly-selected samples from the training and test sets for every estimation task. As seen in Figure 2’s plots, for the experiments applying simultaneous 1,1 Adam optimization the empirical minimax risk generalizes properly from training to test samples (Figure 2-top). In contrast, in both the experiments with non-simultaneous methods after 30,000 iterations the empirical minimax risk suﬀers from a considerable generalization gap from the true minimax risk (Figure 2-bottom). The gap between the training and test minimax risks grew between iterations 30,000-60,000. The test minimax risk ﬂuctuated over the subsequent iterations, which could be due to the insuﬃciency of 100 Adam ascent steps to follow the optimal discriminator solution at those iterations.
The numerical results of our GAN experiments suggest that non-simultaneous algorithms which attempt to fully solve the maximization subproblem at every iteration can lead to large generalization errors. On the other hand, standard simultaneous algorithms used for training GANs enjoy a bounded generalization error which can help the training process ﬁnd a model with nice generalization properties. We defer further experimental results to the supplementary document.

References
Allen-Zhu, Z. and Li, Y. (2019). What can resnet learn eﬃciently, going beyond kernels? In Advances in Neural Information Processing Systems, pages 9017–9028. 3
Allen-Zhu, Z., Li, Y., and Liang, Y. (2019). Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing systems, pages 6158–6169. 3
Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. (2019). Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584. 3
Arora, S., Ge, R., Liang, Y., Ma, T., and Zhang, Y. (2017). Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573. 2
Arora, S. and Zhang, Y. (2017). Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224. 3
Attias, I., Kontorovich, A., and Mansour, Y. (2019). Improved generalization bounds for robust learning. In Algorithmic Learning Theory, pages 162–183. 3

11

Bai, Y. and Lee, J. D. (2019). Beyond linearization: On quadratic and higher-order approximation of wide neural networks. arXiv preprint arXiv:1910.01619. 3
Bai, Y., Ma, T., and Risteski, A. (2018). Approximability of discriminators implies diversity in gans. arXiv preprint arXiv:1806.10586. 3
Belkin, M., Hsu, D., Ma, S., and Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences, 116(32):15849–15854. 3
Bernhard, P. and Rapaport, A. (1995). On a theorem of danskin with an application to a theorem of von neumann-sion. Nonlinear Analysis: Theory, Methods & Applications, 24(8):1163–1181. 32
Bietti, A. and Mairal, J. (2019). On the inductive bias of neural tangent kernels. In Advances in Neural Information Processing Systems, pages 12893–12904. 3
Bousquet, O. and Elisseeﬀ, A. (2002). Stability and generalization. Journal of machine learning research, 2(Mar):499–526. 2, 3, 21
Bousquet, O., Klochkov, Y., and Zhivotovskiy, N. (2020). Sharper bounds for uniformly stable algorithms. In Conference on Learning Theory, pages 610–626. 3
Cao, Y. and Gu, Q. (2019). Generalization bounds of stochastic gradient descent for wide and deep neural networks. In Advances in Neural Information Processing Systems, pages 10836–10846. 3
Chatterjee, S. (2020). Coherent gradients: An approach to understanding generalization in gradient descentbased optimization. arXiv preprint arXiv:2002.10657. 3
Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H. (2017). Training gans with optimism. arXiv preprint arXiv:1711.00141. 3
Daskalakis, C. and Panageas, I. (2018). The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9236–9246. 3
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933. 3
Du, S. S. and Hu, W. (2019). Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 196–205. PMLR. 3
Farnia, F. and Ozdaglar, A. (2020). Gans may have no nash equilibria. arXiv preprint arXiv:2002.09124. 3
Farnia, F., Zhang, J. M., and Tse, D. (2018). Generalizable adversarial training via spectral normalization. arXiv preprint arXiv:1811.07457. 3
Feizi, S., Farnia, F., Ginart, T., and Tse, D. (2020). Understanding gans in the lqg setting: Formulation, generalization and stability. IEEE Journal on Selected Areas in Information Theory. 3
Feldman, V. and Vondrak, J. (2018). Generalization bounds for uniformly stable algorithms. In Advances in Neural Information Processing Systems, pages 9747–9757. 3
Feldman, V. and Vondrak, J. (2019). High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. arXiv preprint arXiv:1902.10710. 3
Fiez, T., Chasnov, B., and Ratliﬀ, L. J. (2019). Convergence of learning dynamics in stackelberg games. arXiv preprint arXiv:1906.01217. 3
Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. (2018). A variational inequality perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551. 3
12

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680. 1, 11
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved training of wasserstein gans. In Advances in neural information processing systems, pages 5767–5777. 11
Hardt, M., Recht, B., and Singer, Y. (2016). Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225–1234. 2, 3, 24, 25, 27, 31, 35, 36
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, pages 6626–6637. 3
Hsieh, Y.-P., Liu, C., and Cevher, V. (2019). Finding mixed nash equilibria of generative adversarial networks. In International Conference on Machine Learning, pages 2810–2819. 3
Ji, Z. and Telgarsky, M. (2019). Polylogarithmic width suﬃces for gradient descent to achieve arbitrarily small test error with shallow relu networks. arXiv preprint arXiv:1909.12292. 3
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2016). On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836. 3
Khim, J. and Loh, P.-L. (2018). Adversarial risk bounds via function transformation. arXiv preprint arXiv:1810.09519. 3
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 11
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. 11
Li, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pages 8157–8166. 3
Liang, T. and Stokes, J. (2019). Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 907–915. 3
Lin, T., Jin, C., and Jordan, M. I. (2019). On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331. 3
Liu, Z., Luo, P., Wang, X., and Tang, X. (2018). Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15:2018. 11
Lyu, K. and Li, J. (2019). Gradient descent maximizes the margin of homogeneous neural networks. arXiv preprint arXiv:1906.05890. 3
Ma, C., Wang, K., Chi, Y., and Chen, Y. (2018). Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion. In International Conference on Machine Learning, pages 3345–3354. 3
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083. 1
Mazumdar, E., Ratliﬀ, L. J., and Sastry, S. S. (2020). On gradient-based learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103–131. 3
13

Mazumdar, E. V., Jordan, M. I., and Sastry, S. S. (2019). On ﬁnding local nash equilibria (and only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838. 3
McAllester, D. A. (1999). Some pac-bayesian theorems. Machine Learning, 37(3):355–363. 3
Mei, S. and Montanari, A. (2019). The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv preprint arXiv:1908.05355. 3
Mescheder, L., Geiger, A., and Nowozin, S. (2018). Which training methods for gans do actually converge? arXiv preprint arXiv:1801.04406. 3
Mescheder, L., Nowozin, S., and Geiger, A. (2017). The numerics of gans. In Advances in Neural Information Processing Systems, pages 1825–1835. 3
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. (2018). Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957. 11
Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2019). Convergence rate of o(1/k) for optimistic gradient and extra-gradient methods in smooth convex-concave saddle point problems. arXiv preprint arXiv:1906.01115. 6, 7, 29
Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2020). A uniﬁed analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1497–1507. PMLR. 3, 6
Nagarajan, V. and Kolter, J. Z. (2017). Gradient descent gan optimization is locally stable. In Advances in neural information processing systems, pages 5585–5595. 3
Nagarajan, V. and Kolter, J. Z. (2019). Uniform convergence may be unable to explain generalization in deep learning. In Advances in Neural Information Processing Systems, pages 11615–11626. 3
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. (2019). Deep double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292. 3
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017a). Exploring generalization in deep learning. In Advances in neural information processing systems, pages 5947–5956. 3
Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2017b). A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564. 3
Neyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614. 3
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Razaviyayn, M. (2019). Solving a class of non-convex min-max games using iterative ﬁrst order methods. In Advances in Neural Information Processing Systems, pages 14934–14942. 3
Ongie, G., Willett, R., Soudry, D., and Srebro, N. (2019). A function space view of bounded norm inﬁnite width relu nets: The multivariate case. arXiv preprint arXiv:1910.01635. 3
Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434. 10
Rice, L., Wong, E., and Kolter, J. Z. (2020). Overﬁtting in adversarially robust deep learning. arXiv preprint arXiv:2002.11569. 3
Rockafellar, R. T. (1976). Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877–898. 19
14

Sanjabi, M., Ba, J., Razaviyayn, M., and Lee, J. D. (2018). On the convergence and robustness of training gans with regularized optimal transport. In Advances in Neural Information Processing Systems, pages 7091–7101. 3
Schäfer, F. and Anandkumar, A. (2019). Competitive gradient descent. In Advances in Neural Information Processing Systems, pages 7625–7635. 3
Schäfer, F., Zheng, H., and Anandkumar, A. (2019). Implicit competitive regularization in gans. arXiv preprint arXiv:1910.05852. 2, 3
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A. (2018). Adversarially robust generalization requires more data. In Advances in Neural Information Processing Systems, pages 5014–5026. 3
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. (2010). Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635–2670. 3
Thanh-Tung, H., Tran, T., and Venkatesh, S. (2019). Improving generalization and stability of generative adversarial networks. arXiv preprint arXiv:1902.03984. 3
Thekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S. (2019). Eﬃcient algorithms for smooth minimax optimization. In Advances in Neural Information Processing Systems, pages 12680–12691. 3
Wang, Y., Zhang, G., and Ba, J. (2019). On solving minimax optimization locally: A follow-the-ridge approach. arXiv preprint arXiv:1910.07512. 3
Wei, C., Lee, J. D., Liu, Q., and Ma, T. (2019). Regularization matters: Generalization and optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing Systems, pages 9712–9724. 3
Wei, C. and Ma, T. (2019). Improved sample complexities for deep networks and robust classiﬁcation via an all-layer margin. arXiv preprint arXiv:1910.04284. 3
Wu, B., Zhao, S., Chen, C., Xu, H., Wang, L., Zhang, X., Sun, G., and Zhou, J. (2019). Generalization in generative adversarial networks: A novel perspective from privacy protection. In Advances in Neural Information Processing Systems, pages 307–317. 2
Yin, D., Kannan, R., and Bartlett, P. (2019). Rademacher complexity for adversarially robust generalization. In International Conference on Machine Learning, pages 7085–7094. PMLR. 3
Zhai, R., Cai, T., He, D., Dan, C., He, K., Hopcroft, J., and Wang, L. (2019). Adversarially robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555. 3
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530. 3
Zhang, G., Wu, K., Poupart, P., and Yu, Y. (2020). Newton-type methods for minimax optimization. arXiv preprint arXiv:2006.14592. 3
Zhang, P., Liu, Q., Zhou, D., Xu, T., and He, X. (2017). On the discrimination-generalization tradeoﬀ in gans. arXiv preprint arXiv:1711.02771. 3
15

Figure 3: Generalization risk vs. iteration count of full-batch GDA optimization in the (Left) strongly-convex strongly-concave setting and (Right) bilinear convex concave setting.
Appendix A Additional Numerical Results
A.1 Convex Concave Minimax Settings
Here, we provide the results of the numerical experiments discussed in the main text for full-batch GDA and PPM algorithms as well as stochastic and full-batch GDmax algorithms. Note that in these experiments we use the same minimax objective and hyperparameters mentioned in the main text. Figure 3 shows the generalization risk in our experiments for the GDA algorithm. As seen in Figure 3 (right), the results for full-batch and stochastic GDA algorithms in the bilinear convex concave case look similar, with the only exception that the generalization risk in the full-batch case reached a slightly higher amplitude of 7.8. On the other hand, in the strongly-convex strongly-concave case, full-batch GDA demonstrated a vanishing generalization risk, whereas stochastic GDA could not reach below an amplitude of 0.2.
Figure 4 shows the results of our experiments for full-batch PPM. Observe that the generalization risk in both cases decreases to reach smaller values than those for stochastic PPM. Finally, Figures 5 and 6 include the results for ful-batch and stochastic GDmax algorithms. With the exception of the full-batch GDmax case for the bilinear objective (Figure 5-right), in all the other cases the generalization risk did not grow during the optimization, which is comparable to our results in the GDA experiments.
A.2 Non-convex Non-concave Minimax Settings
Here, we provide the image samples generated by the trained GANs discussed in the main text. Figure 7 shows the CIFAR-10 samples generated by the simultaneous 1,1 Adam training (Figure 7-left) and non-simultaneous 1,100-Adam optimization (Figure 7-right). While we observed that the simultaneous training experiment generated qualitatively sharper samples, the non-simultaneous optimization did not lead to any signiﬁcant training failures. However, as we discussed in the main text the generalization risk in the non-simultaneous training was signiﬁcantly larger than that of simultaneous training. Figure 8 shows the generated images in the CelebA experiments, which are qualitatively comparable between the two training algorithms. However, as discussed in the text the trained discriminator had a harder task in classifying the training samples from the generated samples than in classifying the test samples from the generated samples, suggesting a potential overﬁtting of the training samples in the non-simultaneous training experiment.
16

Figure 4: Generalization risk vs. iteration count of full-batch PPM optimization in the (Left) strongly-convex strongly-concave setting and (Right) bilinear convex concave setting.

Figure 5: Generalization risk vs. iteration count of full-batch GDmax optimization in the (Left) stronglyconvex strongly-concave setting and (Right) bilinear convex concave setting.

Appendix B Proofs
B.1 The Expansivity Lemma for Minimax Problems
We will apply the following lemma to analyze the stability of gradient-based methods. We call an update rule G γ-expansive if for every w, w ∈ W, θ, θ ∈ Θ we have

G(w, θ) − G(w , θ ) 2 ≤ γ

w−w

2 2

+

θ−θ

22.

(29)

Lemma 1. Consider the GDA and PPM updates for the following minimax problem

min max f (w, θ),

(30)

w∈W θ∈Θ

where we assume objective f (w, θ) satisﬁes Assumptions 1 and 2. Then,
1. For a non-convex non-concave minimax problem, GGDA is (1 + max{αw, αθ})-expansive. Assuming η < 1 , GPPM will be 1/(1 − η)-expansive.

17

Figure 6: Generalization risk vs. iteration count of stochastic GDmax optimization in the (Left) stronglyconvex strongly-concave setting and (Right) bilinear convex concave setting.

Figure 7: SN-GAN generated pictures in the CIFAR-10 experiments for (Left) simultaneous 1,1-Adam training (Right) non-simultaneous 1,100-Adam training.

2. For a convex concave minimax problem with αw = αθ, GGDA is 1-expansive.

1 + 2αw2 -expansive and GPPM will be

3.

For

a

µ-strongly-convex

strongly-concave

minimax

problem,

given

that

αw

= αθ

≤

2µ
2

,

GGDA

is

(1 − αwµ +

αw2 2/2)-expansive and GPPM will be 1/(1 + µη)-expansive.

Proof. In Case 1 with non-convex non-concave minimax objective, f ’s smoothness property implies that for every (w, θ) and (w , θ ):

w

w

GGDA( θ ) − GGDA( θ ) =

w − w − αw(∇wf (w, θ) − ∇wf (w , θ )) θ − θ + αθ(∇θf (w, θ) − ∇θf (w , θ ))

w−w ≤ θ−θ

+ αw(∇wf (w, θ) − ∇wf (w , θ )) αθ(∇θf (w, θ) − ∇θf (w , θ ))

≤ (1 + max{αw, αθ}) w − w ,

(31)

θ

θ

which completes the proof for the GDA update. For the proximal operator, note that given η ≤ 1 the proximal optimization reduces to optimizing a strongly-convex strongly-concave minimax problem with a unique saddle solution and therefore at (wPPM, θPPM) = GPPM(w, θ) we have

wPPM − w = η∇wf (wPPM, θPPM), θ − θPPM = η∇θf (wPPM, θPPM).

(32)

18

Figure 8: SN-GAN generated pictures in the CelebA-10 experiments for (Left) simultaneous 1,1-Adam training (Right) non-simultaneous 1,100-Adam training.

As a result, we have

w

w

GPPM( θ ) − GPPM( θ )

= w − w + η(∇wf (GPPM(w, θ)) − ∇wf (GPPM(w , θ ))) θ − θ − η(∇θf (GPPM(w, θ)) − ∇θf (GPPM(w , θ ))

w−w ≤ θ−θ

+ η(∇wf (GPPM(w, θ)) − ∇wf (GPPM(w , θ ))) η(∇θf (GPPM(w, θ)) − ∇θf (GPPM(w , θ ))

≤ w−w

η + GPPM(w, θ) − GPPM(w , θ ) .

(33)

θ

θ

The ﬁnal result of the above inequalities implies that

(1 − η ) GPPM(w, θ) − GPPM(w , θ ) ≤ w − w ,

(34)

θ

θ

which completes the proof for the case of non-convex non-concave case. For convex-concave objectives, the proof is mainly based on the monotonicity of convex concave objective’s
gradients (Rockafellar, 1976), implying that for every w, w , θ, θ :

w − w T ∇wf (w, θ) − ∇wf (w , θ ) ≥ 0.

(35)

θ

θ

−∇θf (w, θ) −∇θf (w , θ )

As shown by Rockafellar (1976), the above property implies that the proximal operator for a convex-concave minimax objective will also be monotone and 1-expansive for any positive choice of η. For the GDA update, note that due to the monotonicity property

w

w2

GGDA( θ ) − GGDA( θ ) 2

w−w 2

w − w T ∇wf (w, θ) − ∇wf (w , θ )

= θ − θ 2 − 2αw θ − θ

−∇θf (w, θ) + ∇θf (w , θ )

+ α2 ∇wf (w, θ) − ∇wf (w , θ ) 2 w ∇θf (w, θ) − ∇θf (w , θ ) 2

≤ (1 + αw2 2) wθ −− θw 22, (36)

19

which results in the following inequality and completes the proof for the convex-concave case:

GGDA( w ) − GGDA( w ) ≤ 1 + α2 2 w − w .

(37)

θ

θ2

w

θ−θ 2

Finally, for the strongly-convex strongly-concave case, note that f˜(w, θ) = f (w, θ) + µ2 ( θ 2 − w 2) will be convex-concave and hence the proximal update (wPPM, θPPM) = GPPM(w, θ) will satisfy

1 w = wPPM +

η ∇wf˜(wPPM, θPPM),

1 + µη

1 + µη

1 θ = θPPM −

η ∇θf˜(wPPM, θPPM),

(38)

1 + µη

1 + µη

where the right-hand side follows from the proximal update for f˜ with stepsize η/(1 + µη) and hence 1expansive. Therefore, the proximal update for f will be 1/(1 + µη)-expansive. Furthemore, for GDA udpates note that

w

w2

GGDA( θ ) − GGDA( θ ) 2

= (1 − µαw)2 w − w θ−θ

2

w − w T ∇wf˜(w, θ) − ∇wf˜(w , θ )

2 − 2(1 − µαw)αw θ − θ

−∇θf˜(w, θ) + ∇θf˜(w , θ )

2 ∇wf˜(w, θ) − ∇wf˜(w , θ ) 2 + αw ∇θf˜(w, θ) − ∇θf˜(w , θ ) 2

≤ ((1 − µαw)2 + αw2 ( 2 − µ2)) wθ −− θw 22

≤ (1 − 2µαw + αw2 2) wθ −− θw 22. (39)
√ Note that the above result ﬁnishes the proof because 1 − t ≤ 1 − t/2 holds for every t ≤ 1, which is based on the lemma’s assumption αw ≤ 2µ/ 2. Also, the last inequality in the above holds since f˜ will be
2 − µ2-smooth. This is because f is assumed to be -smooth, implying that for every w, w , θ, θ we have

2 w−w θ−θ

2 ≥ ∇wf (w, θ) − ∇wf (w , θ ) 2

2

∇θf (w, θ) − ∇θf (w , θ ) 2

= µ2 w − w θ−θ

2

w − w T ∇wf˜(w, θ) − ∇wf˜(w , θ )

2 + 2µ θ − θ ∇θf˜(w, θ) − ∇θf˜(w , θ )

∇wf˜(w, θ) − ∇wf˜(w , θ ) 2 + ∇θf˜(w, θ) − ∇θf˜(w , θ ) 2

≥ µ2 w − w 2 + ∇wf˜(w, θ) − ∇wf˜(w , θ ) 2,

(40)

θ−θ 2

∇θf˜(w, θ) − ∇θf˜(w , θ ) 2

where the inequality uses the monotonicity of the gradient operator. The ﬁnal inequality shows that f˜ will be 2 − µ2-smooth and hence ﬁnishes the proof.

B.2 Proof of Theorem 1
Theorem. Assume minimax learner A is -uniformly stable in minimization. Then, A’s expected generalization risk is bounded as gen(A) ≤ .

20

Proof. Here, we provide a proof based on standard techniques in stability-based generalization theory
(Bousquet and Elisseeﬀ, 2002). To show this theorem, consider two independent datasets S = (z1, . . . , zn) and S = (z1, . . . , zn). Using S(i) = (z1, . . . , zi−1, zi, zi+1, . . . , zn) to denote the dataset with the ith sample replaced with zi, we will have

1n

ESEA[RS(Aw(S))] = ESEA n

max f (Aw(S), θ; zi)
θ∈Θ

i=1

1n

= ES ES EA

max f (Aw(S(i)), θ; zi)

n

θ∈Θ

i=1

1n

= ESES EA n

max f (Aw(S), θ; zi) + ζ
θ∈Θ

i=1

= ESEA[R(Aw(S))] + ζ.

(41)

In the above, ζ is deﬁned as

1n

ζ := ESES EA

max f (Aw(S(i)), θ; zi) − max f (Aw(S), θ ; zi) .

(42)

n

θ∈Θ

θ ∈Θ

i=1

Note that due to the uniform stability assumption for every data point z and datasets S, S with only one diﬀerent sample we have

max f (Aw(S), θ; z) − max f (Aw(S ), θ ; z) ≤ max f (Aw(S), θ; z) − f (Aw(S ), θ; z) ≤ . (43)

θ∈Θ

θ ∈Θ

θ∈Θ

Therefore, replacing the order of S, S in the above inequality we obtain

max f (Aw(S), θ; z) − max f (Aw(S ), θ ; z) ≤ .

(44)

θ∈Θ

θ ∈Θ

As a result, we conclude that |ζ| ≤ which shows that

ESEA[RS(Aw(S))] − ESEA[R(Aw(S))] ≤ .

(45)

The proof is hence complete.

B.3 Proof of Theorem 2

Note that in the following discussion we deﬁne PPmax as a proximal point method which fully optimizes the maximization variable at every iteration with the following update rule:

GPPmax( w ) := argmin argmax f (w, θ) + 1 w − w 2 .

θ

w∈W θ∈Θ

2ηw

2

Theorem. Let minimax learning objective f (·, ·; z) be µ-strongly convex strongly-concave and satisfy Assump-

tion

2

for

every

z.

Assume

that

Assumption

1

holds

for

convex-concave

f (w, θ; z) := f (w, θ; z) +

µ2 (

θ

2 2

−

w 22) and every z. Then,

1.

Full-batch

and

Stochastic

GDA

and

GDmax

with

constant

stepsize

αw

= αθ

≤

2µ
2

for

T

iterations

will

satisfy

gen(GDA), gen(SGDA) ≤ (µ −2LαLww2 )n , gen(GDmax), gen(SGDmax) ≤ 2µLn2w . (46)
2

2. Full-batch and stochastic PPM and PPmax with constant parameter η for T iterations will satisfy

gen(PPM), gen(SPPM) ≤ 2LµLnw , gen(PPmax), gen(SPPmax) ≤ 2µLn2w . (47)

21

Proof. We start by proving the following lemmas.

Lemma 2 (Growth Lemma). Consider two sequences of updates G1, . . . , GT and G1, . . . .Gt with the same starting point w0 = w0, θ0 = θ0. We deﬁne δt := wt − wt 2 + θt − θt 2. Then, if GT , GT is ξ-expansive
we have δt+1 ≤ ξδt for identical Gt = Gt, and in general we have

δt+1 ≤ min{ξ, 1}δt + sup{ [w, θ] − Gt([w, θ]) } + sup{ [w, θ] − Gt([w, θ]) }.

(48)

w,θ

w,θ

Furthermore, for any constant r we have

δt+1 ≤ ξδt + sup{ r[w, θ] − Gt([w, θ]) } + sup{ r[w, θ] − Gt([w, θ]) }.

(49)

w,θ

w,θ

Finally, if Gt = G + G˜t and Gt = G + G˜t for ξ0-expansive G and ξ1-expansive G˜t and G˜t, then for any constant r we have

δt+1 ≤ (ξ0 + ξ1)δt + sup{ r[w, θ] − G˜t([w, θ]) } + sup{ r[w, θ] − G˜t([w, θ]) }.

(50)

w,θ

w,θ

Proof. The ﬁrst part of the theorem is a direct consequence of the deﬁnition of ξ-expansive operators. For the second part, note that

δt+1 = Gt([wt, θt]) − Gt([wt, θt])

= Gt([wt, θt]) − [wt, θt] + [wt, θt] − [wt, θt] + [wt, θt] − Gt([wt, θt])

≤ Gt([wt, θt]) − [wt, θt] + [wt, θt] − [wt, θt] + [wt, θt] − Gt([wt, θt])

≤ δt + sup{ [w, θ] − Gt([w, θ]) } + sup{ [w, θ] − Gt([w, θ]) }.

(51)

w,θ

w,θ

In addition, we can bound δt+1 as

δt+1 = Gt([wt, θt]) − Gt([wt, θt])

= Gt([wt, θt]) − Gt([wt, θt]) + Gt([wt, θt]) − Gt([wt, θt])

≤ Gt([wt, θt]) − Gt([wt, θt]) + Gt([wt, θt]) − Gt([wt, θt])

≤ ξδt + Gt([wt, θt]) − r[wt, θt] + r[wt, θt] − Gt([wt, θt])

≤ ξδt + sup{ r[w, θ] − Gt([w, θ]) } + sup{ r[w, θ] − Gt([w, θ]) }.

(52)

w,θ

w,θ

The above result for general constant r and also combined with the previous result with r = 1 ﬁnishes the proof of the ﬁrst two parts. For the ﬁnal segment of the lemma, note that

δt+1 = Gt([wt, θt]) − Gt([wt, θt])

= G([wt, θt]) + G˜t([wt, θt]) − G([wt, θt]) − G˜t([wt, θt])

≤ G([wt, θt]) − G([wt, θt]) + G˜t([wt, θt]) − G˜t([wt, θt])

≤ ξ0δt + G˜t([wt, θt]) − G˜t([wt, θt])

≤ ξ0δt + ξ1δt + sup{ r[w, θ] − G˜t([w, θ]) } + sup{ r[w, θ] − G˜t([w, θ]) }.

(53)

w,θ

w,θ

In the above equations, the last line follows from the second part of the lemma which ﬁnishes the proof.

In order to show the Theorem for SGDA updates, note that given two datsets S, S of size n with only

one diﬀerent sample at every iteration of stochastic GDA the update rule will be the same with probability

1 − 1/n and with probability 1/n we have two diﬀerent (1 − αwµ + αw2 2/2)-expansive operators both of

which satisfy

sup{ (1 − αwµ)[w, θ] − GSGDA([w, θ]) } ≤ Lαw.

(54)

w,θ

22

The above inequality holds, because f˜ is assumed to be continuously diﬀerentiable and L-Lipschitz. As a
result, Lemmas 2,1 together with the law of total probability imply that the expected norm of δtSGDA = wt − wt 2 + θt − θt 2 for the SGDA updates applied to the two datasets will satisfy

SGDA

1

αw2 2

SGDA 1

αw2 2

SGDA

E[δt+1 ] ≤ (1 − n )(1 − αwµ + 2 )E[δt ] + n (1 − αwµ + 2 )E[δt ] + 2αwL

= (1 − αwµ + αw22 2 )E[δtSGDA] + 2αnwL . (55)

Note

that

in

the

above

upper-bound

1−

1 n

is

the

probability

that

the

stochastic

GDA

algorithm

chooses

a

shared

sample

between

the

two

datasets

and

1 n

is

the

probability

of

picking

the

index

of

the

diﬀerent

sample.

Similarly, the update rule for the full-batch GDA algorithm can be written as the sum of the updates for

the shared samples, i.e.,

n−1 i=1

1 n

GGDA

([w

,

θ

];

z

i

),

and

the

diﬀerent

sample

zn’s

update

1 n

GGDA

([w

,

θ

];

z

n

)

.

As a result, the last part of Lemma 2 together with Lemma 1 implies that

GDA

11

αw2 2 GDA 1

δt+1 ≤ (1 − n + n )(1 − αwµ + 2 )δt + n (2αwL)

= (1 − αwµ + αw22 2 )E[δtGDA] + 2αnwL , (56)

which is the same bound we derived for stochastic GDA. Therefore, given that δ0 = 0, for SGDA updates we have

SGDA 2αwL t

αw2 2 i

E[δt

]≤ n

(1 − αwµ + 2 )

i=0

≤ 2αwL ∞ (1 − αwµ + αw2 2 )i

n

2

i=0

2αw L

=

α2 2

n(αwµ −

w
2

)

2L

=

n(µ −

αw

2. )

2

(57)

Note that

wt − wt

≤

δt

and

for

every

θ, z

f (w, θ; z)

−

µ 2

w

2 2

is

Lw -Lipschitz

in

w.

As a result, the SGDA

algorithm applied for T iterations will be (2LLw/n(µ − αw 2/2))-uniformly stable in minimization, and the

result follows from Theorem 1. The result for the GDA algorithm will follow from the same steps, since it

shares the same growth rule with the SGDA algorithm.

Similarly, the SPPM updates will be 1/(1 + µη)-expansive due to Lemma 1. Furthermore, they will satisfy

sup{ 1 [w, θ] − GSPPM([w, θ]) } ≤ Lη . (58)

w,θ 1 + ηµ

1 + ηµ

The above equation holds, because for a SPPM update [wSPPM, θSPPM] = GSPPM([w, θ]) at sample z we have

w

(1 + ηµ)wSPPM + η∇wf˜(wSPPM, θSPPM; z)

θ = (1 + ηµ)θSPPM − η∇θf˜(wSPPM, θSPPM; z)

1w

wSPPM

η

∇wf˜(wSPPM, θSPPM; z)

⇒ 1 + ηµ

θ

−

θSPPM

= 1 + ηµ

−∇θf˜(wSPPM, θSPPM; z).

(59)

Therefore, applying the law of total probability we will have E[δtS+P1PM] ≤ (1 − n1 ) 1 +1µη E[δtSPPM] + n1 1 +1µη E[δtSPPM] + 2 1 +Lηηµ

23

= 1 +1µη E[δtSPPM] + (1 +2Lηηµ)n . (60)

Also, for the PPM algorithm given that zn denotes the diﬀerent sample between datasets S, S note that

w

=

(1

+

ηµ)wPPM

+

η n

n−1 i=1

∇wf˜(wPPM, θSPPM; zi)

+

η n

∇

w

f˜(w

PPM

,

θ

SPPM

;

z

n

)

θ

(1

+

ηµ)θPPM

−

η n

n−1 i=1

∇θf˜(wPPM, θPPM; zi)

+

η n

∇

θ

f˜(

wPPM

,

θ

PPM

;

z

n

)

⇒ 1 w − wSPPM = η

1 + ηµ θ

θSPPM 1 + ηµ

1 n

n−1 i=1

∇w

f˜(wPPM,

θPPM;

zi)

− n1

n−1 i=1

∇θ

f˜(wPPM,

θPPM;

zi)

+

1 n

∇

w

f˜(

wPPM

,

θ

PPM

;

z

n

)

.

−

1 n

∇

θ

f˜(w

PPM

,

θ

PPM

;

z

n

)

(61)

In the above, the last line shows the sum of the updates for shared samples between the two datasets, i.e., z1, . . . , zn−1, and the diﬀerent sample zn. Therefore, Lemma 2 together with Lemma 1 implies that

δtP+P1M ≤ (1 − n1 + n1 ) 1 +1µη δtPPM + n(12+Lηηµ) = 1 +1µη δtPPM + (1 +2Lηηµ)n , (62)

which proves the same growth rule shown for SPPM also applies to the PPM algorithm. Since δ0 = 0, the above discussion implies the following for SPPM updates:

SPPM

2Lη

t

1i

E[δt

]≤ (1 + ηµ)n

1 + µη

i=0

2Lη ∞ 1 i ≤
(1 + ηµ)n i=0 1 + µη

2Lη =
(1 + ηµ)n(1 − 1/(1 + µη))

= 2L . (63) nµ

Since wt − wt ≤ δt and for every θ, z f˜(w, θ; z) is Lw-Lipschitz in w, the SPPM algorithm applied for T

iterations will be (2LLw/nµ)-uniformly stable in minimization. Therefore, the theorem’s result is a corollary

of Theorem 1. We can prove the result for the PPM algorithm by repeating the same steps we did for SPPM,

as the two algorithms were shown to share the same growth rule.

For

GDmax

and

PPmax

algorithms,

note

that

f˜max(w; z)

:=

maxθ

f˜(w, θ; z)

−

µ 2

θ

2

will be convex

and Lw-Lipschitz in w.

Therefore, summing this function with

µ 2

w

2

will be µ-strongly convex.

Since

GDmax and SGDmax apply gradient descent to the maximized function, the theorem’s result for GDmax

and SGDmax follows from Theorem 3.9 in (Hardt et al., 2016). For SPPmax, we note that similar to Lemma

1 it can be seen that the proximal point updates will be 1/(1 + µη)-expansive. Moreover for the update

wSPPmax = GSPPmax(w), we will have

w = (1 + ηµ)wSPPmax + η∇wf˜max(wSPPmax; z)

⇒

1 w − wSPPmax =

η ∇wf˜max(wSPPmax; z).

1 + ηµ

1 + ηµ

(64)

As a result of Lemma 2.5 in (Hardt et al., 2016), deﬁning δtSPPmax = wt − wt for datatsets S, S have:
E[δtS+P1Pmax] ≤ (1 − n1 ) 1 +1µη E[δtSPPmax] + n1 1 +1µη E[δtSPPmax] + 2 1L+wηηµ

we will

24

= 1 +1µη E[δtSPPmax] + (12+Lηwµη)n . (65)

Furthermore for PPmax, we will have the following for wPPM = GPPM(w) when applied to the two datasets diﬀerent in only the zn sample:

w = (1 + ηµ)w

η n−1

+

∇

f˜

(w

η ;z )+ ∇

f˜

(w

;z )

PPmax n

w max PPmax i n w max PPmax n

i=1

1

⇒

w−w

η =

1 n−1 ∇

f˜

(w

;z )

1 + ηµ

PPM 1 + ηµ n

w max PPmax i

i=1

η +

1 ∇wf˜max(wPPmax; zn) .

1 + ηµ n

(66)

Applying Lemma 2.5 from (Hardt et al., 2016) and deﬁning δtPPmax = wt − wt have:

δtP+P1max ≤ (1 − n1 + n1 ) 1 +1µη δtPPmax + n1 = 1 +1µη δtPPmax + (12+Lηwµη)n .

2 Lwη 1 + ηµ

for datatsets S, S we will (67)

Note that δ0PPmax = δ0SPPmax = 0 which implies that:

SPPmax

2Lwη t

1i

E[δt

]≤ (1 + ηµ)n

1 + µη

i=0

2Lwη ∞ 1 i ≤
(1 + ηµ)n i=0 1 + µη

= 2Lwη (1 + ηµ)n(1 − 1/(1 + µη))

= 2Lw .

(68)

nµ

Therefore, the SPPMax algorithm applied for T iterations will be (2L2w/nµ)-uniformly stable according to (Hardt et al., 2016)’s Deﬁnition 2.1. The result is hence a consequence of Theorem 2.2 in (Hardt et al., 2016).
We can prove the result for the PPmax algorithm by repeating the same steps.

B.4 Proof of Remark 1

Remark. Consider a convex concave minimax objective f (·, ·; z) satisfying Assumptions 1 and 2. Given constant stepsizes αw = αθ = α, the GDA’s generalization risk over T iterations will be bounded as:

(GDA) ≤ O αLLw(1 + α2 2)T/2 . (69)

gen

n

In particular, the bound’s exponential dependence on T is tight for the GDA’s generalization risk in the special

case of f (w, θ; z) = w (z − θ).

√ Proof. As shown in Lemma 1, the GDA’s update will be 1 + α2 2-expansive in this case. As a result, in

learning over two datasets S, S which are diﬀerent in only one sample, Lemma 2 shows the following growth

rule for δt =

wt − wt

2 2

+

θt − θt 22:

δt+1 ≤ ( n − 1 + 1 ) 1 + α2 2δt + 2αL

nn

n

25

= 1 + α2 2δt + 2αL . (70) n
Considering that δ0 = 0, we get the following exponentially growing bound in T for δT :

δT ≤ T 1 + α2 2 t/2 2αL n
t=1

2αL 1 + α2 2 (T +1)/2 − 1

=

√

n

1 + α2 2 − 1

αL 1 + α2 2 T /2

=O

,

n

(71)

which considering that f (w, θ; z) is Lw-Lipschitz in w together with Theorem 1 shows that

(GDA) ≤ O αLLw(1 + α2 2)T/2 .

gen

n

Also,

note

that

for

the

special

convex-concave

case

f (w, θ; z)

=

wT (z − θ)

given

that

z¯

=

1 n

GDA’s update rule will satisfy the following

(72)

n i=1

zi

the

wt+1 = I αI

θt+1 − z¯

−αI I

wt , θt − z¯

⇒ wt+1 = I αI wt − αz¯ .

(73)

θt+1

−αI I θt

0

As a result, for the updates on the two datasets S, S with size n diﬀering in only the sample zn we have:

wt+1 − wt+1 = I αI

θt+1 − θt+1

−αI I

wt − wt

+

α n

(zn

−

zn)

.

θt − θt

0

(74)

Hence, knowing that w0 = w0, θ0 = θ0 we have

wT − wT = I

αI T

α n

(zn

−

zn)

.

(75)

θT − θT

−αI I

0

Since the matrix 1 α has the conjugate complex eigenvalues 1 ± αi, we will have −α 1

√

T

wT − wT

=

1 + α2 T

α n

(zn

−

zn)

α 1 + α2 =

z −z

(76)

θT − θT 2

0

2

n

n

n2

As a consequence of the conjugate eigenvalues and the resulting iterative rotations in the complex space, the above equality shows that as long as α = 0 for any constant 0 < C < 1 there will exist arbitrarily large T values such that

Cα √1 + α2 T

wT − wT 2 ≥

n

zn − zn 2.

(77)

Equivalently, we have

α(1 + α2 T/2

wT − wT 2 = ΩT

n

zn − zn 2 ,

(78)

which proves the exponential dependence of the expected generalization risk on T and completes the proof.

26

B.5 Proof of Theorem 3
Here we prove the following generalized version of Theorem 3 in the text for general time-varying stepsize values.

Theorem. Consider a convex-concave minimax learning objective f (·, ·; z) satisfying Assumptions 1 and 2 for every z. Then, stochastic PPM with stepsizes ηt at iteration t over T iterations will satisfy

gen(PPM), gen(SPPM) ≤ 2LLw T ηt, gen(PPmax), gen(SPPmax) ≤ 2L2w T ηt. (79)

n

n

t=1

t=1

Proof. Consider two datasets S, S with size n which have only one diﬀerent sample. As a result of Lemma
1, the proximal point updates will be 1-expansive. Therefore, according to Lemma 2 and the Law of total probability, deﬁning δtSPPM = wt − wt 2 + θt − θt 2 we will have

E[δtS+P1PM] ≤ (1 − n1 )E[δtSPPM] + n1 = E[δtSPPM] + 2ηntL

E[δtSPPM] + 2ηtL

(80)

Given that δ0SPPM = 0, we reach the following inequality for every T

SPPM 2L T

E[δT

]≤ n

ηt.

t=1

(81)

Note that for every θ, z, f (w, θ; z) is Lw-Lipschitz, which with the above inequality implies that SPPM will be uniformly-stable in minimization with the following degree

2LLw T n ηt.
t=1

(82)

The theorem’s result for SPPM then becomes a consequence of Theorem 1. Furthermore, regarding the PPM algorithm applying Lemma 2 and Lemma 1 implies that

δtP+P1M ≤ (1 − n1 + n1 )δtPPM + 2ηntL = δtPPM + 2ηntL . (83)
The above equation holds because the update rule of PPM can be written in the following way where zn denotes the only diﬀerent sample between the two datasets,

w − wPPM = η

1 n

n−1 i=1

∇w

f

(wPPM,

θPPM;

zi)

+

1 n

∇

w

f

(w

PPM

,

θ

PPM

;

z

n

)

.

θ

θPPM

− n1

n−1 i=1

∇θ

f

(wPPM,

θPPM;

zi)

−

1 n

∇

θ

f

(

wPPM

,

θ

PPM

;

z

n

)

(84)

Since δ0PPM = 0, at iteration T we have

PPM 2L T

δT

≤ n

ηt.

t=1

(85)

As a result, we can repeat the last step of our proof for the case of SPPM to complete the proof for the PPM case. For the PPmax and SPPmax algorithms, note that fmax(w; z) := maxθ f (w, θ; z) will be convex and Lw-Lipschitz in w. The result is therefore a corollary of Theorem 3.8 and Lemma 4.6 in (Hardt et al., 2016).

27

B.6 Proof of Theorem 4

Theorem. Given a diﬀerentiable minimax objective f (w, θ; z) the average iterate updates w¯ (T ) := T1 Tt=1 w(t), θ¯(T ) := T1 Tt=1 θ(t) of SPPM and SPPmax with setpsize parameter η will satisfy the following optimality gaps for a saddle solution [wS∗ , θS∗ ] of the empirical risk for dataset S:

SPPM : E RS (w¯ (T )) − RS (wS∗ ) ≤ w(0) − wS∗ 22η+T θ(0) − θS∗ 2 , SPPmax : E RS(w¯ (T )) − RS(wS∗ ) ≤ w(0)2η−TwS∗ 2 . (86)

Proof. Note that for any proximal operator Fk such that vk+1 = vk − ηFk(vk+1) we will have the following for every v:

1 vk − v 2 − 1 vk+1 − v 2 − 1 vk+1 − vk 2

2η

2η

2η

1 =−
η

vk+1 2 − vkT vk+1 − vT vk+1 + vT vk

=

−

1 (vk+1

−

vk)T (vk+1

−

v)

η

= Fk(vk+1)T (vk+1 − v).

(87)

As a result, we have

1T

1

Fk(vk)T (vk − v) =

v0 − v 2 − 1

1T vT − v 2 −

vk − vk−1 2

T

2ηT

2ηT

T

k=0

k=0

1 ≤

v0 − v 2.

(88)

2ηT

Given that every Fk is a stochastic proximal rule for a uniformly random training sample, the law of iterated expectation conditioned to random update vt at iteration t implies that

1T

1T

E

Fk(vk)T (vk − v) =

E Fk(vk)T (vk − v)

T

T

k=0

k=0

1T

=

E E Fk(vk)T (vk − v) vk

T

k=0

1T

T

= T E E Fk(vk) vk (vk − v)

k=0

= T1 T E E F¯(vk) vk T (vk − v)
k=0

= T1 T E F¯(vk)T (vk − v)
k=0

= E T1 T F¯(vk)T (vk − v) (89)
k=0

where F¯ denotes the gradient update for the averaged loss over the training samples. Therefore, we have

E T1 T F¯(vk)T (vk − v) ≤ 2η1T v0 − v 2. (90)
k=0

28

Considering the optimal saddle solution v = [wS∗ , θS∗ ] for the SPPM algorithm, combining the above result with Lemma 2 in (Mokhtari et al., 2019) proves the theorem’s result on the convergence of SPPM’s average
iterates. For the convergence result on SPPmax updates, note that given a convex function f and its gradient F and minimizer v∗ we have

f 1 T v(t) − f (v∗) ≤ 1 T f (v(t)) − f (v∗) ≤ 1 T F (v(t))T (v(t) − v∗).

T

T

T

t=1

t=1

t=1

(91)

The above equation together with the property shown for the stochastic updates of SPPmax completes the theorem’s proof.

B.7 Proof of Corollary 1

Corollary* 1. Consider a convex concave minimax objective which we optimize via PPM and PPmax with

setpsize parameter η. Then, given that w(0) − wS∗ 2 + θ(0) − θS∗ 2 ≤ D2 for PPM and w(0) − wS∗ ≤ D

for PPmax holds with probability 1, it will take TSPPM =

nD2 2η 2 LL

and TSPPmax =

2nη2DL22 iterations for the

w

w

average iterates to achieve the following bounded excess risks where w∗ denotes the optimal learner minimizing

the true risk R(w):

PPM, SPPM : E R(w¯ (TSPPM)) − R(w∗) ≤

2D2LLw , n

PPmax, SPPmax : E R(w¯ (TSPPmax)) − R(w∗) ≤ 2D2L2w .

(92)

n

Proof. First, we show that using a constant stepsize parameter η the average iterates reach 1/2 of the generalization bound for the ﬁnal iterates in Theorem 3. For the average iterates (w¯ t, θ¯t) and (w¯ t, θ¯t) we
have the following application of Jensen’s inequality on the convex norm function for the diﬀerence of average iterates δ¯t = w¯ (t) − w¯ (t) 2 + θ¯(t) − θ¯ (t) 2

δ¯t : = w¯ (t) − w¯ (t) 2 + θ¯(t) − θ¯ (t) 2

1 t−1 ≤
t k=0

wk − wk 2 + θk − θk 2

1 t−1 = t δt.
k=0

(93)

Similarly,

one

can

show

that

δ¯w,t

≤

1 t

t k=1

δw,t.

Therefore,

knowing

that

E[δt] ≤

2LLw tη n

implies

that

E[δ¯ ] ≤ 1

t

1 t−1 2LLwkη LLwtη

E[δ ] ≤

≤

.

(94)

tt

tt

n

n

k=1

k=0

Hence, at the T th average iterate of PPM and SPPM we will have

EA[R(w¯ (T ))] − RS[w¯ (T )] ≤ LLwT η

(95)

n

which together with (Mokhtari et al., 2019)’s Theorem 1 for the PPM and and our generalization of that theorem to stochastic PPM in Theorem 4 shows that
EA,S[R(w¯ (T ))] − ES[RS[w¯ (T )]] ≤ LLnwηT + 2DηT2 . (96)

29

Note that ES[RS(wS)] ≤ ES[RS(w∗)] = R(w∗), indicating that EA,S[R(w¯ (T ))] − R(w∗) ≤ LLnwηT + 2DηT2 . (97)

The above upper-bound will be minimized when ηT = 2nLDL2w and the optimized excess risk upper-bound

for PPM and SPPM will be

EA,S[R(w¯ (T ))] − R(w∗) ≤ 2LLwD2 .

(98)

n

Similarly, it can be seen that for PPmax and SPPmax the optimal bound will be achieved at ηT =

nD2
2

2Lw

which suggests the following excess risk bound:

EA,S[R(w¯ (T ))] − R(w∗) ≤ 2L2wD2 .

(99)

n

The proof is therefore complete.

B.8 Proof of Theorem 5

Theorem. Let learning objective f (w, θ; z) be non-convex µ-strongly-concave and satisfy Assumptions 1 and 2. Also, we assume that fmax(w; z) := maxθ∈Θ f (w, θ; z) is bounded as 0 ≤ fmax(w; z) ≤ 1 for every w, z. Then, deﬁning κ := /µ we have 1. The SGDA algorithm with vanishing stepsizes αw,t = c/t, αθ,t = cr2/t for constants c > 0, 1 ≤ r ≤ κ satisﬁes the following bound over T iterations:

1 + (r+11)c

1

(r+1)c

gen(SGDA) ≤

n (12(r + 1)cLLw) (r+1)c +1 T (r+1)c +1 .

(100)

2. The SGDmax algorithm with vanishing stepsize αw,t = c/t for constant c > 0 satisﬁes the following bound

over T iterations:

1+ 2

SGDmax) ≤

(κ+2) c

gen( n − 1

2cL2w

2

(κ+2) c

(κ+2) c+2 T (κ+2) c+2 .

(101)

Proof. We start by proving the following lemmas.

Lemma 3. Let f (w, θ; z) be Lw-Lipschitz in w and assume that fmax(w; z) := maxθ f (bw, θ; z is bounded
0 ≤ fmax(w; z) ≤ 1. Then, in applying SGDA for learning over two datasets S, S which diﬀer in only
one sample the updated variables wt, wt will satisfy the following inequality for every t0 ∈ {1, . . . , n} where δt := wt − wt 2 + θ − θt 2:

∀z : E |fmax(wt; z) − fmax(wt; z)| ≤ tn0 + LwE[δt|δt0 = 0].

(102)

Proof. Deﬁne the event Et0 = I(δt0 = 0) as the indicator of the outcome δt0 = 0. Then, due to the law of total probability

E |fmax(wt; z) − fmax(wt; z)| = Pr(Et0 )E |fmax(wt; z) − fmax(wt; z)| Et0 + Pr(Etc0 )E |fmax(wt; z) − fmax(wt; z)| Etc0
(a)
≤ E |fmax(wt; z) − fmax(wt; z)| Et0 + Pr(Etc0 )
(b)
≤ LwE wt − wt Et0 + Pr(Etc0 )

30

(c)

t0

≤ LwE δt

δt0 = 0

+. n

(103)

In the above equations, (a) follows from the boundedness assumption on fmax. (b) is the consequence of Lw-Lipschitzness of f which also transfers to fmax. Finally, (c) holds because wt − wt ≤ δt according to the deﬁnition. Then, using the union bound on the outcome I = It where I is the index of diﬀerent samples in S, S and It is the index of sample used by SGDA at iteration t we obtain that

c

t0

t0

Pr(Et0 ) = Pr(δt0 > 0) ≤ Pr(I = Ii) = n .

i=1

(104)

The lemma’s proof is therefore complete.

In order to prove the theorem for SGDA updates, we provide an extension of Lemma 1 for non-convex concave minimax objectives.
Lemma 4. Consider a non-convex µ-strongly concace objective f (w, θ) satisfying Assumption 2. Then, for every two pairs (w, θ), (w , θ ) the GDA updates [wGDA, θGDA] = GGDA([w, θ]), [wGDA, θGDA] = GGDA([w , θ ]) with stepsizes αw, αθ ≤ 1 will satisfy the following expansivity equation:

Proof. Note that

wGDA − wGDA θGDA − θGDA

≤ 1 + αw αθ

αw 1 − α2θµ

w−w θ−θ .

(105)

wGDA − wGDA Furthermore, we have

= w − αw∇wf (w, θ) − w + αw∇wf (w , θ ) ≤ w − αw∇wf (w, θ) − w + αw∇wf (w , θ)
+ αw∇wf (w , θ) − αw∇wf (w , θ ) ≤ (1 + αw ) w − w + αw θ − θ .

(106)

θGDA − θGDA = θ + αθ∇θf (w, θ) − θ − αθ∇θf (w , θ ) ≤ θ + αθ∇θf (w, θ) − θ − αθ∇θf (w, θ )
+ αθ∇θf (w, θ ) − αθ∇θf (w , θ ) ≤ 1 − αθµ θ − θ + αθ w − w ,
2
where the last inequality follows from Lemma 3.7 in (Hardt et al., 2016) knowing that µ ≤ lemma’s proof is complete.

(107) . Therefore, the

Lemma 5. Consider two sequence of updates G1, . . . , GT and G1, . . . , GT for minimax objective f (w, θ). Deﬁne δw,t = wt − wt and δθ,t = θt − θt . Assume that Gt is η-expansive for matrix η2×2, i.e. it satisﬁes the following inequality for every [wGt , θGt ] := Gt(w, θ), [wGt , θGt ] := Gt(w , θ )

wGt − wGt ≤ η w − w .

θGt − θGt

θ−θ

(108)

Also, suppose that for every [wGt , θGt ] := Gt(w, θ), [wGt , θGt ] := Gt(w, θ) we have

sup wGt − w ≤ σw, sup θGt − θ ≤ σθ,

w,θ

w,θ

sup wGt − w ≤ σw, sup θGt − θ ≤ σθ.

w,θ

w,θ

Then, we have

δw,t+1 ≤ η δw,t + 2 σw .

δθ,t+1

δθ,t

σθ

(109) (110)

31

Proof. Note that

δw,t+1 = δθ,t+1

Gt,w(wt, θt) − Gt,w(wt, θt) Gt,θ(wt, θt) − Gt,θ(wt, θt)

= Gt,w(wt, θt) − Gt,w(wt, θt) + Gt,w(wt, θt) − Gt,w(wt, θt) Gt,θ(wt, θt) − Gt,θ(wt, θt) + Gt,θ(wt, θt) − Gt,θ(wt, θt)

= Gt,w(wt, θt) − Gt,w(wt, θt) + Gt,w(wt, θt) − Gt,w(wt, θt)

Gt,θ(wt, θt) − Gt,θ(wt, θt)

Gt,θ(wt, θt) − Gt,θ(wt, θt)

= Gt,w(wt, θt) − Gt,w(wt, θt) + Gt,w(wt, θt) − wt

Gt,θ(wt, θt) − Gt,θ(wt, θt)

Gt,θ(wt, θt) − θt

+ wt − Gt,w(wt, θt) θt − Gt,θ(wt, θt)

≤ η δw,t + 2 σw ,

δθ,t

σθ

(111)

which makes the proof complete.

Lemma 6. Consider a non-convex µ-strongly convex minimax objective f (w, θ) satisfying Assumption 2 over a convex feasible set Θ. Then, the maximized objective fmax(w) := maxθ∈Θ f (w, θ) will be ( + 2/2µ)-smooth, i.e., for every w1, w2 ∈ W it satisﬁes

∇fmax(w2) − ∇fmax(w1) 2 ≤

2
+ 2µ

w2 − w1 2.

(112)

Proof. Consider two arbitrary points w1, w2 ∈ W and deﬁne θ∗(w1), θ∗(w2) as the optimal maximizers over
Θ for f (w1, ·), f (w2, ·), respectively. Since, f (w, ·) is -smooth and µ-strongly-convex, there exists a unique solution θ∗(w) for every w. Then, the µ-strongly concavity implies that

µ θ∗(w1) − θ∗(w2) 22 ≤ θ∗(w2) − θ∗(w1) T ∇θf (w1, θ∗(w1)) − ∇θf (w1, θ∗(w2)) . Due to the optimality of θ∗(w1), θ∗(w2) over the convex feasible set Θ we further have

(113)

θ∗(w2) − θ∗(w1) T ∇θf (w1, θ∗(w1)) − ∇θf (w2, θ∗(w2)) = θ∗(w2) − θ∗(w1) T ∇θf (w1, θ∗(w1)) + θ∗(w1) − θ∗(w2) T ∇θf (w2, θ∗(w2)) ≤ 0.

(114)

Combining the above two equations, we obtain

µ θ∗(w1) − θ∗(w2) 22 ≤ θ∗(w2) − θ∗(w1) T ∇θf (w2, θ∗(w2)) − ∇θf (w1, θ∗(w2)) ≤ θ∗(w1) − θ∗(w2) 2 w2 − w1 2.

(115)

The above equation results in

θ∗(w1) − θ∗(w2) 2 ≤ µ w2 − w1 2.

(116)

As a result, applying the Danskin’s theorem for smooth objectives with a unique solution (Bernhard and Rapaport, 1995) implies that

∇fmax(w2) − ∇fmax(w1) 2 = ∇wf (w2, θ∗(w2)) − ∇wf (w1, θ∗(w1)) 2

32

≤ w2 − w1 22 + θ∗(w1) − θ∗(w2) 22

≤

1 + ( /µ)2

w2 − w1

2 2

= 1 + ( /µ)2 w2 − w1 2
2
≤ + 2µ2 w2 − w1 2, √ where the last line holds since 1 + t ≤ 1 + t/2 for every t ≥ −1. The proof is hence complete.

(117)

To prove the theorem’s result on SGDA note that Lemma 4 suggests that the SGDA update at iteration t for non-convex non-concave problems will be expansive with the following matrix:

Bt := 1 + αw,t αθ,t

αw,t 1 − αθ2,tµ

= I + αw,t

1

1

c1 1

ααwθ,,tt − µααwθ,,tt = I + t r2 −r2/κ .

(118)

For analyzing the powers of the above matrix, we diagonalize it using its eigenvalues λ1, λ2 and corresponding eigenvectors ν1, ν2. Note that the product of the eigenvalues of r12 −r12/κ , i.e. the matrix’s determinant, is negative and hence the matrix has two diﬀerent real eigenvalues with opposite signs. This implies that the matrix is diagonlizable and so is a linear combination of the matrix with the identity matrix. As a result, given the invertible matrix ν = [ν1, ν2] we have

Bt = 1 + αw,t αθ,t

αw,t 1 − αθ2,tµ

= ν−1

1 + c tλ1 0

0 1 + c λ2 ν.
t

(119)

Also, notice that we have the following closed-form solution for λ1, λ2:

κ − r2 + λ1 =

4κ2r2 + (κ + r2)2 ,
2κ

Therefore, since we assume 1 ≤ r ≤ κ,

κ − r2 − λ2 =

4κ2r2 + (κ + r2)2 .
2κ

(120)

1 − r2 + (2r + ( r2 + 1))

max{λ1, λ2} ≤

κ

κ

= r + 1.

2

Now, applying the law of total probability as well as Lemma 5 shows that

(121)

E[δw,t+1]

1 ≤ (1 − )Bt

E[δw,t]

1 + Bt

E[δw,t]

+2

αw,tLw

E[δθ,t+1]

n

E[δθ,t] n

E[δθ,t]

αθ,tLθ

= B E[δw,t] +

2cLw nt

.

t E[δθ,t]

2cr2 Lθ

nt

(122)

Therefore, over T iterations we will have

E[δw,T ] E[δθ,T ]

T

T

≤

Bk

t=t0+1 k=t+1

T

T

=

ν −1

t=t0 +1

k=t+1

2cLw nt
2cr2 Lθ nt
1 + c kλ1 0

0 1 + c kλ2

2cLw
ν 2crn2tLθ .
nt

(123)

Hence, denoting the minimum and maximum singular values of ν with σmin(ν), σmax(ν) and noting that ν−1’s operator norm is equal to 1/σmin(ν) we will have

E[δw,T ] E[δθ,T ]

σmax(ν) T ≤ 2 σmin(ν) t=t0+1

T k=t+1

1 + c kλ1 0

0 1 + c kλ2

2cLw

nt 2cr2 Lθ

nt

2

33

σmax(ν) T

T exp( c λ1 )

0

2cLw

≤ σ (ν)

k

0

exp( c λ2 )

nt 2cr2Lθ 2

min

t=t0+1 k=t+1

k

2

nt

σmax(ν) T

exp( T

c λ1 )

0

2cLw

= σ (ν)

k=t+1 k
0

exp( T

c λ2 )

nt 2cr2Lθ 2

min

t=t0 +1

k=t+1 k

2

nt

σmax(ν) T

T c (r + 1)

2cLw

≤

exp(

σ (ν)

k

)

nt 2cr2 Lθ

2

min

t=t0 +1

k=t+1

nt

2crLσmax(ν) T exp( ≤
nσmin(ν) t=t0+1

Tk=t+1 c (rk+1) ) t

= 2crLσmax(ν)T c (r+1) T t−c (r+1)−1

nσmin (ν )

t=t0 +1

≤ 2rLσmax(ν) T c (2r+1) (r + 1) nσmin(ν) t0

12L T c (r+1)

≤

.

n t0

(124)

√

√

We note that assuming r ≥ 1 we have ν’s condition number σmax(ν)/σmin(ν) ≤ ( 2 + 1)/( 2 − 1) ≤ 6.

This is because given an eigenvalue λ of r12 −r12/κ and its corresponding eigenvector [ν1, ν2] we have

ν2 = (λ − 1)ν1 and hence the eigenvector aligns with [1, λ − 1]. Therefore, we can bound the condition

number of the following symmetric matrix, because we can consider any vector column along the eigenvector’s

direction:

1 λ1 − 1

λ1 − 1 (λ1 − 1)(λ2 − 1)


=  −1− r2 +√4r12+(1+r2/κ)2 κ 2

−1− r2 +√4r2+(1+r2/κ)2  κ

2

.

−r

Since the above matrix is symmetric, its eigenvalues have the same absolute value as its singular values, and therefore the condition number will be bounded as

σmax(ν) ≤ σmin (ν )
≤

(r − 1)2 + 4(r + (λ1 − 1)2) + (r − 1) (r − 1)2 + 4(r + (λ1 − 1)2) − (r − 1) (r − 1)2 + 4(r + (r − r+2 1 )2) + (r − 1) (r − 1)2 + 4(r + (r − r+2 1 )2) − (r − 1)

(r − 1)2 + 4(r − r+2 1 )2 + (r − 1) ≤
(r − 1)2 + 4(r − r+2 1 )2 − (r − 1)
2(r − 1)2 + (r − 1) =
2(r − 1)2 − (r − 1) √ = √2 + 1 .
2−1

(125)

As a result, we showed that conditioned to δt0 = 0 we will have

12L T c (r+1)

E δw,T δt0 = 0 ≤ n t0

.

(126)

34

Combining the above equation with Lemma 3, we obtain that

∀z, t0 : E |fmax(wT ; z) − fmax(w ; z)| ≤ t0 + 12LLw T c (r+1).

T

n

n t0

The above bound will be approaximately minimized at

(127)

1

(r+1)c

t0 = (12(r + 1)cLLw) (r+1)c +1 T (r+1)c +1

which leads to the following bound

∀z :

E |fmax(wT ; z) − fmax(wT ; z)|

1 + (r+11)c

1

(r+1)c

≤ n (12(r + 1)cLLw) (r+1)c +1 T (r+1)c +1 .

(128)

The theorem’s bound on SGDA updates is then a consequence of Theorem 2.2 in (Hardt et al., 2016).
For the theorem’s bound on SGDmax updates, note that fmax(w; z) will be Lw-Lipschitz. Also, Lemma 6 implies that fmax(w; z) will be ( κ2 + 1)-smooth in w. Therefore, the result directly follows from Theorem 3.12 in (Hardt et al., 2016).

B.9 Proof of Theorem 6

Theorem. Let minimax cost 0 ≤ f (·, ·; z) ≤ 1 be a bounded non-convex non-concave objective which satisﬁes Assumptions 1 and 2. Then, the SGDA algorithm with vanishing stepsizes max{αw,t, αθ,t} ≤ c/t for constant c > 0 satisﬁes the following bound over T iterations:

1+ 1

(SGDA) ≤

c 2cLLw

gen

n

T . 1
c+1

c c+1

(129)

Proof. To show this result, we apply Lemma 3. Deﬁning δt = wt − wt 2 + θt − θt 2 for the norm diﬀerence of parameters learned by SGDA over two datasets S, S with one diﬀerent sample, according to the
law of total probability we have:

1

c

1

c

2cL

E[δt+1] ≤ (1 − n )(1 + t )E[δt] + n (1 + t )E[δt] + t

c

2cL

= (1 + t )E[δt] + nt .

As a result, conditioned on δt0 = 0 we will have

(130)

T

T

c 2cL

E[δT δt0 = 0] ≤

1+ k nt

t=t0+1 k=t+1

T

T

c 2cL

≤

exp( )

t=t0+1 k=t+1

k nt

T

T c 2cL

=

exp

t=t0 +1

k=t+1 k nt

T

2cL

≤

exp(c log(T /t))

t=t0+1 nt

2cLT c =
n

T
t−c −1
t=t0 +1

35

2L T c

≤

.

n t0

Therefore, Lemma 3 shows that for every t0 and z:

t0 2LLw T c

E |fmax(wt; z) − fmax(wt; z)|

≤+ n

n

. t0

The above upper-bound will be approximately minimized at

1

c

t0 = (2cLLw) c+1 T c+1 .

Plugging in the above t0 to the upper-bound we obtain the following bound for every z:

1+ 1

1

c

E |fmax(wt; z) − fmax(wt; z)| ≤

c (2cLLw) c+1 T c+1 . n

The above result combined with Theorem 2.2 from (Hardt et al., 2016) proves the theorem.

(131) (132) (133) (134)

36

