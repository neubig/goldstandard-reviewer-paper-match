Safe Reinforcement Learning of Control-Afﬁne Systems with Vertex Networks

arXiv:2003.09488v1 [cs.LG] 20 Mar 2020

Liyuan Zheng, Yuanyuan Shi, Lillian J. Ratliff, and Baosen Zhang Department of Electrical & Computer Engineering, University of Washington
{liyuanz8,yyshi,ratliffl,zhangbao}@uw.edu

Abstract
This paper focuses on ﬁnding reinforcement learning policies for control systems with hard state and action constraints. Despite its success in many domains, reinforcement learning is challenging to apply to problems with hard constraints, especially if both the state variables and actions are constrained. Previous works seeking to ensure constraint satisfaction, or safety, have focused on adding a projection step to a learned policy. Yet, this approach requires solving an optimization problem at every policy execution step, which can lead to signiﬁcant computational costs.
To tackle this problem, this paper proposes a new approach, termed Vertex Networks (VNs), with guarantees on safety during exploration and on learned control policies by incorporating the safety constraints into the policy network architecture. Leveraging the geometric property that all points within a convex set can be represented as the convex combination of its vertices, the proposed algorithm ﬁrst learns the convex combination weights and then uses these weights along with the pre-calculated vertices to output an action. The output action is guaranteed to be safe by construction. Numerical examples illustrate that the proposed VN algorithm outperforms vanilla reinforcement learning in a variety of benchmark control tasks.
1 Introduction
Over the last couple of years, reinforcement learning (RL) algorithms have yielded impressive results on a variety of applications. These successes in-

clude playing video games with super-human performance [Mnih et al., 2015], robot locomotion and manipulation [Lillicrap et al., 2015, Levine et al., 2016], autonomous vehicles [Sallab et al., 2017], and many benchmark continuous control tasks [Duan et al., 2016].
In RL, an agent learns to make sequential decisions by interacting with the environment, gradually improving its performance at the task as learning progresses. Policy optimization algorithms [Lillicrap et al., 2015, Schulman et al., 2015] for RL assume that agents are free to explore any behavior during learning, so long as it leads to performance improvement. However, in many real-world applications, there is often additional safety constraints, or speciﬁcations that lead to constraints, on the learning problem. For instance, a robot arm should prevent some behaviors that could cause it to damage itself or the objects around it, and autonomous vehicles must avoid crashing into others while navigating [Garcıa and Ferna´ndez, 2015].
In real-world applications such as the above, constraints are an integral part of the problem description, and maintaining constraint satisfaction during learning is critical (i.e., these are hard constraints). Therefore, in this work, our goal is to maintain constraint satisfaction at each step throughout the whole learning process. This problem is sometimes called the safe exploration problem [Amodei et al., 2016, Wachi et al., 2018]. In particular, we deﬁne safety as remaining within some pre-speciﬁed polytope constraints on both states and actions. Correspondingly, the action we take at each step should result in a state that in the safety set.
In the safe exploration literature, the projection technique is often leveraged to maintain safety during exploration [Cheng et al., 2019, Dalal et al., 2018]. Speciﬁcally, at each step, an action is suggested by an unconstrained policy optimization algorithm, and then is projected into the safety region. However, this projection step either involves solving a computationally expensive optimiza-

tion problem online [Cheng et al., 2019], or has strict assumptions such as allowing for only one of the halfspaces to be violated [Dalal et al., 2018]. More over, if real-time optimization is allowed by the application, then it is often more advantageous to solve a model predictive control problem than to ask for a policy learned by RL.
To alleviate the limitation, we proposed Vertex Networks (VNs), where we encode the safety constraints into the policy via neural network architecture design. In VNs, we compute the vertices of the safety region at each time step and design the action to be the convex combination of those vertices, allowing policy optimization algorithms to explore only in the safe region.
The contributions of this work can be brieﬂy summarized as follows: (1) To the best of our knowledge, this is the ﬁrst attempt to encode safety constraints into policies by explicit network design. (2) In simulation, the proposed approach achieves good performance while maintaining constraint satisfaction.
2 Related work
In safe RL, safety in expectation is a widely used criterion [Altman, 1999, Yu et al., 2019]. In recent literature, policy optimization algorithms have been proposed as a means to learn a policy for a continuous Markov decision process (MDP). Two state-of-the-art exemplars in terms of performance are the Lagrangian-based actor-critic algorithm [Bhatnagar and Lakshmanan, 2012, Chow et al., 2017] and Constrained Policy Optimization (CPO) [Achiam et al., 2017]. However, for these methods, constraint satisfaction can only be guaranteed in expectation. In a safety critical environment, this is not sufﬁcient since even if safety is guaranteed in expectation, there is still a non-zero probability that unsafe trajectories will be generated by the controller.
Safe exploration, on the other hand, requires constraint satisfaction at each steps. Recent approaches [Sui et al., 2015, Wachi et al., 2018] model safety as unknown functions, proposed algorithm that trades off between exploring the safety function and reward function. However, their approaches require solving MDPs or constrained optimizations to obtain policy in each exploration iteration, which are less efﬁcient than the policy optimization algorithm leveraged in our approach.
Literature that uses policy optimization algorithm in safe exploration is closely related to our work. Among those, projection technique often used for maintaining safety. In [Dalal et al., 2018], the safety set is deﬁned in terms of half-space constraints on the action. A policy optimization algorithm—in particular, deep deterministic policy

gradient (DDPG) [Lillicrap et al., 2015]—is leveraged to generate an action which is then projected into the safe set. Imposing that only one half-space constraint can be violated, the projection optimization problem can be solved in closed form. In [Cheng et al., 2019], this projection step is solved as a quadratic program, based on conﬁdence intervals for the approximation of the system dynamics, modeled as Gaussian processes.
It is possible to integrate a projection step into the policy network. Indeed, using the methodology provided in [Amos and Kolter, 2017], one can leverage a policy optimization algorithm to train the policy network in an endto-end fashion. However, the feed forward computation of the policy network in this case is computationally expensive as it involves solving an optimization problem before executing every single action. One might prefer to solve a model predictive control problem instead if solving the optimization problem online is involved. Instead of integrating the projection step, we propose VNs which leverage the convex combination of vertices to enforce safety.

3 Model Setup

Consider a discrete time afﬁne control system in which the system evolves according to

xt+1 = f (xt) + H(xt)ut,

(1)

where x ∈ Rn, u ∈ Rm, and f and H are known functions of appropriate dimensions. Our goal is to minimize a cost over time horizon T , subject to safety constraints on x and actuator constraints on u:

T

min C(xt, ut)

(2a)

u

t=1

s.t. xt+1 = f (xt) + H(xt)ut

(2b)

xt ∈ X

(2c)

ut ∈ U ,

(2d)

where X and U are convex polytopes. A convex polytope can be deﬁned as an intersection of linear inequalities (half-space representation) or equivalently as a convex combination of a ﬁnite number of points (convexhull representation) [Gru¨nbaum, 2013]. This type of constraints are widely used in theory and practice—for example, see [Blanchini and Miani, 2008] and the references within.

The goal of safe RL is to ﬁnd an optimal feedback controller ut = πθ(xt), that minimize the overall system cost (2a) while satisﬁes the safety constraints (2c) and the actuator constraints (2d). Solving (2) is a difﬁcult task, even for linear systems with only the actuator constraints,

except for a class of systems where analytic solutions can be found [Gokcek et al., 2001]. Therefore, RL (and its different variants) have been proposed to search for a feedback controller.
Numerous learning approaches have been adopted to solve the problem when the constraints (2c) and (2d) are not present. However, there are considerably less successful applications of RL to problems with hard constraints. One such approach is the two-stage method used in [Dalal et al., 2018]. The ﬁrst step is to simply train a policy that solves the problem in (2) without the constraints on state nor the action. To enforce the constraints, a projection step is solved, where the action determined by the unconstrained policy is projected into the constraint sets.
This two-step process is referred to as safe exploration in [Dalal et al., 2018], since it leverages the fact that RL algorithms explore the action space while the projection satisﬁes the hard constraints. However, this approach has two drawbacks that we address in the current paper. Firstly, the projection step itself requires an optimization problem to be solved. This step could be computationally expensive. More fundamentally, it brings the question of why not directly solve (2) as a model predictive control problem, since online optimization needs to be used either way. Secondly, decoupling the policy and projection steps may lead to solutions that signiﬁcantly deviate from the original unconstrained policy. To overcome these challenges, we propose a novel vertex policy network that encodes the geometry of the constraints into the network architecture, and train it in an end-to-end way. We will discuss the proposed vertex policy network framework in detail in the next section.
4 Vertex Policy Network
The key idea of our proposed VN is using a basic fact of the geometry of a convex polytope. Given a bounded convex polytope P, it is always possible to ﬁnd a ﬁnite number of vertices such that the convex hull is P. In addition, there is no smaller set of points whose convex hull forms P [Gru¨nbaum, 2013]. Then, the next proposition follows directly. Proposition 4.1. Let P be a convex polytope with vertices P1, . . . , PN . For every point p ∈ P, there exists λ1, . . . , λN , such that
p = λ1P1 + · · · + λN PN ,
where λi ≥ 0, ∀i and λ1 + · · · + λN = 1.
The preceding proposition implies that we can search for the set of weights λi’s instead of directly ﬁnding a point inside polytope.

Proposition 4.1 can be applied to ﬁnd a feedback control policy. Since both the constraint sets X and U are convex polytopes, the control action at each timestep must also live in a convex polytope. If its vertices are known, the output of a policy can be the weights λi’s. The beneﬁt of having the weights as the output is threefold. Firstly, it is much easier to normalize a set of real numbers to be all positive and to sum to unity (the probability simplex) than to project into an arbitrary polytope. For this paper, we use a softmax layer. Secondly, this approach allows us to fully explore the interior of the feasible space, where projections could be biased towards the boundary of the set. Thirdly, we are able to use standard policy gradient training techniques.

In particular, we use DDPG as the policy evaluation and update algorithm, where the policy is a neural network parameterized by θ and updated by

θ ← θ + α∇θJ(θ).

(3)

where J(θ) is the expected return using the current policy and is deﬁned by

T

J(θ) = E −C(xt, ut) .

(4)

t=1

We approximate J(θ) by − N1

N i=1

T t=1

C

(xi,t,

ui,t),

where N are the number of sampled trajectories gener-

ated by running the current policy πθ(ut|xt) and T is the

trajectory length. The overall algorithm procedure of the

proposed VN framework is provided in Fig 1.

Below, we discuss the two major components of VN in detail: 1) the safety region and vertex calculation, and 2) the neural network architecture design for the safe layer.

4.1 Evolution of the Action Constraint Set

We require that at each time step the states of the system stay in the set X , and the control actions at constrained to be in the set U. As stated earlier, we assume X and U to be convex polytopes. The main algorithmic challenge comes from the need to repeatedly intersect translated versions of these polytopes. To be concrete, suppose we are given xt. Then for the next step, we require that xt ∈ X . This translates into an afﬁne constraint on ut, since the control action mush satisfy

H(xt)ut ∈ X − f (xt).

Since xt is known, H(xt) is a constant in the above equation, and the constraint on ut is again polytopic. We denote this polytope as St. The set to which ut must belong is the intersection of St and the actuator constraints:

Ut = St ∩ U .

(5)

Figure 2: Evolution of action safety set for a twodimensional linear system toy example. The left plot visualizes the safety set at time t = 1, and the right plot shows the safety set at time t = 2.

Now suppose a feasible action u = [0.1 0.1] is chosen and the system evolves. Then, S2 = {−0.6 ≤ u1 ≤ 0.4, −0.6 ≤ u2 ≤ 0.4}. Performing the intersection of S2 and U , we get that U2 is a rectangle deﬁned by the vertices {(0, 0), (0, 0.4), (0.4, 0), (0.4, 0.4)} as depicted in Fig. 2 (right).

Figure 1: Flowchart of the proposed VN framework.

After identifying the vertices of Ut, the algorithm in Fig. 1 can be used to ﬁnd the optimal feedback policies.

In general, it is fairly straightforward to ﬁnd either the convex hull or the half-space representations of St, since it just requires a linear transformation of X . However, the intersection step in (5) and the process of ﬁnding the representation of its convex hull are nontrivial [Blanchini and Miani, 2008]. Below, we work through a simple example to illustrate the steps and then discuss how to overcome the computational challenges.
Example 1 (Intersection Step). Consider the following two–dimensional linear system:

10

10

xt+1 = 0 1 xt + 0 1 ut.

Suppose the action safety set U is a convex polytope
deﬁned by: 0 ≤ u1 ≤ 1, 0 ≤ u2 ≤ 1 and u1 + u2 ≤ 1.5. The state safety set X is a square deﬁned by 0 ≤ x1 ≤ 1, 0 ≤ x2 ≤ 1 and the initial state is x0 = 0.5 0.5 T . By simple calculation, S1 = {−0.5 ≤ u1 ≤ 0.5, −0.5 ≤ u2 ≤ 0.5} and U1 is the box bounded by {(0, 0), (0, 0.5), (0.5, 0), (0.5, 0.5)}.
Fig. 2 (left) visualizes the intersection operations.

4.2 Intersection of Polytopes
It should be noted that ﬁnding the vertices of an intersection of polytopes is not easy [Tiwary, 2008]. If the polytopes are in half-space representation, then their intersection can be found by simply stacking the inequalities. However, ﬁnding the vertices of the resulting polytope can be computationally expensive. Similarly, directly intersecting two polytopes based on their convex hull representation is also intractable in general.
Luckily, in many applications, we are not intersecting two generic polytopes at each step. Rather, there are only two ”basic” sets, X and U, and we are intersecting a linear transformation of these. It turns out that for many systems (see Section 5), we can ﬁnd the resulting vertices by hand-designed rules. In addition, there are heuristics that work well for low-dimensional systems [Broman and Shensa, 1990]. Applying the proposed VN technique to high-dimensional systems is the main future direction for this work.
In the case that St and U do not overlap, one can choose to stop the training process. However, in our rules of ﬁnding vertices, we pick the point in U that closest to St to be the vertex. By design, the output of the VN is the action within set U, meanwhile transiting to the state closest to the safe state set X .
4.3 Safe layer
Once we obtain Ut, the next step is to encode the geometry information into the policy network such that the gen-

erated action stays in Ut. According to Proposition 4.1, it sufﬁces for the policy network to generate the weights (or coefﬁcients) of that convex combination.

Suppose that Ut can have at most N vertices, labeled

P

(t 1

)

,

.

.

.

,

P

(t N

)

.

In the policy network architecture de-

sign, we add an intermediate safe layer that ﬁrst gen-

erates N nodes λ1, . . . , λN . The value of these nodes,

however, are not positive nor do they sum to 1. There-

fore, a softmax unit is included as the activation func-

tion in order to guarantee the non-negativity and the summation constraints. In particular, we deﬁne λ¯i = eλi /( Nj=1 eλj ), the weights of a convex combination.
The ﬁnal output layer (action ut) is deﬁned as the multiplication of these normalized weights λ¯i and the corre-

sponding vertex values,

N

ut = λ¯iPi(t).

(6)

i=1

An illustration diagram is provided in Fig. 3.

Figure 3: Illustration of the proposed safe layer archi-

tecture. The output of the policy network is modiﬁed to

predict the weights λi, i ∈ [1, N ]. These weights are normalized to λ¯i, i ∈ [1, N ] that satisﬁes λ¯i ≥ 0 and

N i=1

λ¯i

=

1,

via

the

softmax

activation

function.

The

action output is calculated as

N i=1

λ¯iPi(t),

where

Pi(t)

are the safety polytope vertices.

5 Simulation
In this section, we present and analyze the performance of the proposed VN. We ﬁrst describe the baseline algorithms and then demonstrate the performance comparisons in three benchmark control tasks: (i) inverted pendulum, (ii) mass-spring and (iii) hovercraft tracking.
5.1 Baseline Algorithm and Architecture Design
As mentioned earlier, the baseline algorithm for the policy update is DDPG [Lillicrap et al., 2015], which is a

state-of-the-art continuous control RL algorithm. To add safety constraints to the vanilla DDPG algorithm, a natural approach is to artiﬁcially shape the reward such that the agent will learn to avoid undesired areas. This can be done by setting a negative reward to the unsafe states and actions. In standard policy network (PN) baselines to which we compare, we include such a soft penalty in the rewards. We train such PN along with VN for comparison. The main difference between PN and VN is that PN only has the feed-forward network (white block in Fig. 3) and does not contain the ﬁnal safe layer (blue block). The output of PN is truncated to ensure the actuator safety constraints.
We use the following hyperparameters for all experiments. For PN, we use a three-layer feed-forward neural network, with 256 nodes in each hidden layer. For VN, it has two feed-forward layers (with 256 nodes in each hidden layer) and a ﬁnal safe layer as described in Section 4.3.

5.2 Pendulum

For the inverted pendulum simulation, we use the OpenAI gym environment (pendulum-v0), with the following pendulum speciﬁcations: mass m = 1, length l = 1. The system state is two-dimension that include angle θ and angular velocity ω of the pendulum, and the control variable is the applied torque u. We set the safe region to be θ ∈ [−1, 1] (radius) and torque limits u ∈ U = [−15, 15]. The reward function is deﬁned as r = −(θ2 + 0.1ω2 + 0.001u2), with the goal of learning an optimal feedback controller.

With a discretization step size of ∆ = 0.05, the following are the discretized system dynamics:

θt+1 = θt + ωt∆ + 3g sin(θt)∆2 + 3 u∆2 (7)

2l

ml2

3g

3

ωt+1 = ωt + 2l sin(θt)∆ + ml2 u∆

To keep the next state in the safe region θt+1 ∈ [−1, 1], we can compute the corresponding upper and lower bound of u to represent set St by (7). Therefore, the vertices of VN can be found by intersecting St and U . Under the case where St and U have no overlap, we pick −15 as the vertices if the upper bound of St is less than −15. Otherwise, we pick 15 as the vertices.
For comparison, the output of PN is constrained in [−15, 15] using tanh activation function in the ﬁnal layer. The initial state of each episode is randomly sampled in the safe state region [−1, 1]. In Fig. 4, we show a comparison of the accumulated reward and the max angle of each episode in training of PN and VN. We ob-

Figure 4: Comparison of accumulated reward and constraint violation (max angle) for the pendulum problem using PN and VN.

Figure 5: Comparison of accumulated reward and constraint violation (max speed) for Mass-Spring problem using PN and VN.

serve that VN maintains safety throughout the training process, and as a result, it achieves higher reward in the early stage. It is also interesting to observe that the PN also becomes “safe” after training, since the reward function itself drives θ to be small. This suggests if we can train the PN ofﬂine, it might be able to obey the safety constraint for some control systems. However, the next example shows that even a well-trained policy may violate constraints if these hard constraints are not explicitly taken into account.

5.3 Mass-Spring

Now we consider the task of damping an oscillating system Mass-Spring to the equilibrium point [Blanchini and Miani, 2008]. The system includes a mass m = 1 and a spring k = 1 and the state is twodimensional with position x and speed v of the mass. The control variable is the force exerted on the mass u. We set the safe region to be v ∈ [−1, 1], u ∈ [−1, 1]. We deﬁne the reward function to be r = −(x2 + v2). The initial state are randomly sampled from x ∈ [−2, 2], v ∈ [−1, 1].
The system dynamics are deﬁned as follows,

xt+1 = 1 ∆

vt+1

− mk ∆ 1

xt + 0 u

vt

m1 ∆

Fig. 5 compares the accumulated reward and the max speed of each episode in the training of PN and VN. VN maintains safety during training and receives higher reward in the early training stage. Note that even trained PN could still violate the constraints.

5.4 Hovercraft
Consider the task of controlling a hovercraft illustrated in Fig. 6 (left), which has system dynamics deﬁned as follows:

Figure 6: (Left) Hovercraft example. u1 and u2 denote starboard and port fan forces. θ, x, y are the tilt angle and the coordinate position. (Right) Illustration of computing vertices of intersection of polytopes St and U .
xt+1 = xt + vx,t∆ + 1 sin θt(u1 + u2)∆2 2m
1 vx,t+1 = vx,t + m sin θt(u1 + u2)∆
yt+1 = yt + vy,t∆ + 1 (cos θt(u1 + u2) − g)∆2 2m
1 vy,t+1 = vy,t + m (cos θt(u1 + u2) − g)∆
θt+1 = θt + vθ,t∆ + 1 (u1 − u2)∆2 2l
1 vθ,t+1 = vθ,t + l (u1 − u2)∆
Let m = l = 1, g = 10. Considering the force exerted on two fans are coupled, the actuator constraint set is deﬁned as {u1, u2|u1 ≥ 0, u2 ≥ 0, u1 +u2 ≤ 20}. To keep the tile angle of the hovercraft in a safety region, we set the safe state region to be θ ∈ [−θ, θ]. Deﬁne the reward function r = −(x − x0)2 − (y − y0)2 − θ2 − 0.1(vx2 + vy2 +vθ2)−0.001(u21 +u22) to learn a controller that tracks the target position (x0, y0). Fig. 6 (right) shows how to use at most ﬁve vertices to represent the intersection of safe state region and the actuator constraint region.

ture called VN that guarantees the output satisﬁes the safety constraints by design. Empirically, we show that VN yields signiﬁcantly better safety performance than a vanilla policy network architecture with a constraint violation penalty in several benchmark control systems. An important future direction is to extend the proposed method to high-dimensional control systems.

Figure 7: Comparison of accumulated reward and constraint violation (max tile angle) from Hovercraft control using PN and VN with different constraint upper bound.
Figure 8: Trajectories generated by trained PN and VN (with different tilt angle upper limit) policies. (Left) tilt angle and (Right) square of distance to the target position.
In our experiment, the initial state is set at position (0, 0) and the target position is (5, 5). To better investigate the effect of the constraint, we train VNs for tilt angle upper bounds of θ = 0.01 and θ = 0.25 radians. Fig. 7 compares the accumulated reward and max tilt angle of each episode in the training of PN and VN. Fig. 8 visualizes the trajectories of trained PN and VN policies. In the trajectories, we observe that the angle of hovercraft ﬁrst turn positive to have some momentum pointed to the right, then turn to slow down the speed. When θ = 0.01, the hovercraft has a strict constraint on its tilt angle and is unable to reach the target position. In both choices of the tilt angle upper limit θ, the constraint is never violated in the whole trajectory executing learned VN. However, running learned PN will still reach large tilt angle, even if the soft penalty is added in the reward.
6 Conclusions
Motivated by the problem of training an RL algorithm with hard state and action constraints, leveraging the geometric property that a convex polytope can be equivalently represented as the convex hull of a ﬁnite set of vertices, we design a novel policy network architec-

References
[Achiam et al., 2017] Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 22–31. JMLR. org.
[Altman, 1999] Altman, E. (1999). Constrained Markov decision processes, volume 7. CRC Press.
[Amodei et al., 2016] Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mane´, D. (2016). Concrete problems in ai safety. arXiv preprint arXiv:1606.06565.
[Amos and Kolter, 2017] Amos, B. and Kolter, J. Z. (2017). Optnet: Differentiable optimization as a layer in neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 136–145. JMLR. org.
[Bhatnagar and Lakshmanan, 2012] Bhatnagar, S. and Lakshmanan, K. (2012). An online actor–critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688–708.
[Blanchini and Miani, 2008] Blanchini, F. and Miani, S. (2008). Set-theoretic methods in control. Springer.
[Broman and Shensa, 1990] Broman, V. and Shensa, M. (1990). A compact algorithm for the intersection and approximation of n-dimensional polytopes. Mathematics and computers in simulation, 32(5-6):469– 480.
[Cheng et al., 2019] Cheng, R., Orosz, G., Murray, R. M., and Burdick, J. W. (2019). End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3387–3395.
[Chow et al., 2017] Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. (2017). Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070– 6120.

[Dalal et al., 2018] Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y. (2018). Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757.
[Duan et al., 2016] Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329–1338.
[Garcıa and Ferna´ndez, 2015] Garcıa, J. and Ferna´ndez, F. (2015). A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437–1480.
[Gokcek et al., 2001] Gokcek, C., Kabamba, P. T., and Meerkov, S. M. (2001). An lqr/lqg theory for systems with saturating actuators. IEEE Transactions on Automatic Control, 46(10):1529–1542.
[Gru¨nbaum, 2013] Gru¨nbaum, B. (2013). Convex polytopes, volume 221. Springer Science & Business Media.
[Levine et al., 2016] Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373.
[Lillicrap et al., 2015] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
[Mnih et al., 2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529–533.
[Sallab et al., 2017] Sallab, A. E., Abdou, M., Perot, E., and Yogamani, S. (2017). Deep reinforcement learning framework for autonomous driving. Electronic Imaging, 2017(19):70–76.
[Schulman et al., 2015] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning, pages 1889–1897.
[Sui et al., 2015] Sui, Y., Gotovos, A., Burdick, J. W., and Krause, A. (2015). Safe exploration for optimization with gaussian processes. Proceedings of Machine Learning Research, 37:997–1005.
[Tiwary, 2008] Tiwary, H. R. (2008). On the hardness of computing intersection, union and minkowski sum

of polytopes. Discrete & Computational Geometry, 40(3):469–479.
[Wachi et al., 2018] Wachi, A., Sui, Y., Yue, Y., and Ono, M. (2018). Safe exploration and optimization of constrained mdps using gaussian processes. In ThirtySecond AAAI Conference on Artiﬁcial Intelligence.
[Yu et al., 2019] Yu, M., Yang, Z., Kolar, M., and Wang, Z. (2019). Convergent policy optimization for safe reinforcement learning. In Advances in Neural Information Processing Systems, pages 3121–3133.

