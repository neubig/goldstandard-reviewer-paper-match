arXiv:2201.05666v1 [cs.LG] 14 Jan 2022

Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions
Ignavier Ng1, Yujia Zheng1, Jiji Zhang2, Kun Zhang1 1 Carnegie Mellon University
2 Hong Kong Baptist University {ignavierng, yujiazh}@cmu.edu, zhangjiji@hkbu.edu.hk, kunz1@cmu.edu
Abstract
Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-deﬁned score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the superstructure. Numerical experiments validate the efﬁcacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy.
1 Introduction
Although it is often more reliable to discover causal relationships by making use of interventions or randomized experiments, they are practically challenging, expensive, or even prohibited owing to ethical considerations. Thus, causal discovery from observational data has received considerable attention in recent decades, and has been widely applied in different ﬁelds such as genetics [29].
One major class of causal discovery methods is the constraint-based methods, such as PC [40] and FCI [43, 4], that leverage conditional independence tests to estimate the skeleton and then perform edge orientation. These methods are guaranteed to asymptotically return the true Markov equivalence class (MEC) under the Markov and faithfulness assumptions. Several modiﬁcations [30, 41] to these constraint-based methods have been developed to allow certain types of unfaithfulness, which, however, generally give rise to weaker claims and are not guaranteed to estimate the true MEC.
Another popular approach is the GES [3] algorithm that searches in the space of MECs greedily by maximizing a well-deﬁned score, such as the Bayesian information criterion (BIC) [36] score. It starts with an empty structure and consists of two phases: (1) adding edges until a local maximum is found, and (2) removing edges until a local maximum is reached. In spite of the greedy strategy, GES converges in the large sample limit to the true MEC under the Markov and faithfulness assumptions, similar to the aforementioned constraint-based methods.
Recently, NOTEARS [52] casts the Bayesian network structure learning task into a continuous constrained optimization problem with the least squares objective, using an algebraic characterization of directed acyclic graph (DAG). Subsequent work GOLEM [23] adopts a continuous unconstrained optimization formulation with a likelihood-based objective. For NOTEARS, it remains unclear
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

of the required assumptions for asymptotic correctness, whereas GOLEM adopts the generalized faithfulness assumption [9] to learn linear Gaussian DAGs, which could be converted to their MECs for causal interpretation [42]. These methods enable the application of numerical solvers and GPU acceleration, which thus are scalable to large graphs. However, they are only guaranteed to ﬁnd a local optimum of the optimization problem, and therefore the quality of the solution in practice may not be guaranteed, even in the asymptotic case.
Another line of research focuses on weakening the faithfulness assumption required for asymptotic correctness of the search results, since, given ﬁnite samples, approximate violations of faithfulness occur surprisingly often, especially when there is a large number of variables [46]. For instance, exact search methods ﬁnd the optimal Bayesian network based on a predeﬁned score function, such as dynamic programming (DP) [18, 24, 39, 38], A* [49, 48], and integer programming [1, 5]. The DAGs estimated by these methods can be converted to their MECs for causal interpretation [42]. Note that the approaches based on sparsest permutation (SP) [32] and Boolean satisﬁability solver (SAT) [15, 16] can be viewed as instances of exact methods. Lu et al. [22] further demonstrated that these exact methods may produce correct results in cases where methods relying on faithfulness fail.
Due to the large search space of possible DAGs [2, 13], exact search methods are feasible only for small structures. Therefore, super-structure has been adopted to constrain the search space [27, 45], which is deﬁned to be an undirected graph that restricts the search to candidate DAGs whose skeleton is its subgraph. However, most of these methods rely on discovering the skeleton of the true DAG for use as a super-structure, utilizing estimation methods like MMPC [45], which require the faithfulness assumption to be asymptotically correct. Under approximate violations of faithfulness, these skeleton estimation methods may miss some edges owing to unfaithful conditional independencies in the data distribution; thus, further exact search procedures are guaranteed to miss those edges.
Contributions. In this work, we introduce several strategies to improve the scalability of exact search in the linear Gaussian setting, giving rise to a more reliable causal discovery procedure. Our main contributions can be summarized as follows:
• We develop a super-structure estimation method based on the support of inverse covariance matrix of the data distribution, and show that it is asymptotically correct under assumptions strictly weaker than faithfulness (or, more speciﬁcally, than triangle-faithfulness). We combine this with exact search method like DP or A* to reduce search space.
• To further scale up exact search, we develop a local search strategy, called Local A*, on the local clusters formed by each variable and its neighbors within two hops in the super-structure.
• We demonstrate the efﬁcacy of our super-structure estimation method and local search strategy by conducting extensive experiments, and show that it scales up to hundreds of nodes with a high accuracy.
Paper organization. We review the common assumptions for causal discovery and the linear structural equation model (SEM) in Section 2. In Section 3, we establish weaker variants of faithfulness and show how they could be used to learn a sound super-structure. We further formulate an improved exact search strategy in Section 4. The empirical studies in Section 5 validate our theoretical results and the efﬁcacy of the proposed procedure. We then conclude our work in Section 6.
2 Background
We ﬁrst review the concepts of causal Bayesian networks and some commonly used assumptions that are related to our further analysis. We then give a brief overview of the linear SEM.
2.1 Causal Bayesian Network and Common Assumptions
Let G = (V, E) be a DAG with vertex set V = {X1, . . . , Xd} in which each node Xi corresponds to a random variable. Denote X = (X1, . . . , Xd) as the random vector concatenating all variables and its associated probability distribution P. Let Xpa(i) be the set of parental nodes of Xi in G such that there is a directed edge from Xj ∈ Xpa(i) to Xi, or Xj → Xi ∈ E. We assume causal sufﬁciency, i.e., no hidden variables, throughout the paper.
In a Bayesian network, the distribution P is assumed to be Markov w.r.t. to DAG G, as deﬁned below.
2

Assumption 1 (Markov). Given a DAG G and distribution P over the variable set V, every variable X in V is probabilistically independent of its non-descendants given its parents in G.
A causal Bayesian network can be viewed as a Bayesian network where the directed edges are provided a causal meaning, which thereby allows it to answer interventional queries [20]. In general, there are many DAGs that induce the same conditional independence (CI) relations with the distribution P, and are said to be Markov equivalent. The Markov equivalence class (MEC) consists of all DAGs that entail the same conditional independence (CI) relations as G does, and is uniquely determined by its skeleton and v-structures [25]. V-structure is deﬁned to be a collider X → Y ← Z where X and Z are not adjacent in G, therefore referred to as an unshielded collider. If X and Z are adjacent, then it is called a shielded collider.
The following faithfulness assumption is commonly used to relate the CI relations in the distribution to the DAG, and can be thought of as the converse to the Markov assumption.
Assumption 2 (Faithfulness [44]). Given a DAG G and distribution P over the variable set V, P implies no CI relations not already entailed by the Markov assumption.
Under the Markov and faithfulness assumptions, constraint-based methods such as PC have been shown to asymptotically output the correct MEC. However, in the ﬁnite sample regime, the faithfulness assumption is sensitive to statistical testing errors when inferring the CI relations, and its approximate violations occur surprisingly often, especially when there is a large number of variables [46]. Thus, different relaxations of faithfulness have been proposed, such as orientation-faithfulness, adjacencyfaithfulness [30], and triangle-faithfulness [51], which we review in Appendix A.
Clearly, the triangle-faithfulness assumption is a consequence of adjacency-faithfulness. Based on these weaker assumptions, different modiﬁcations to constraint-based methods have been proposed, such as Conservative PC [30] and Very Conservative SGS [41]. As their names suggest, these methods make weaker claims about the estimated causal structure and therefore are not guaranteed to estimate the true MEC. On the other hand, Raskutti and Uhler [32] proposed the following assumption and show that it is strictly weaker than faithfulness.
Assumption 3 (Sparsest Markov representation (SMR) [32]). Given a DAG G and distribution P over the variable set V, the MEC of G is the unique sparsest MEC that satisﬁes the Markov assumption with P.
Forster et al. [7] referred to the above assumption as the (unique) frugality assumption, and argued that it has multiple desirable properties compared to faithfulness. Under the SMR assumption, the SP method [32] has been shown to produce asymptotically correct results. Raskutti and Uhler [32] also conjectured that SP reaches the information-theoretic limit in the sense that the SMR assumption may be the weakest assumption guaranteeing the asymptotic correctness of any method for learning the true MEC. It is worth noting that SP can be viewed as an instance of exact score-based method. The study by Lu et al. [22] demonstrated that causal discovery methods that rely on the faithfulness assumption (e.g., PC, GES) may output sub-optimal solutions in various cases, whereas exact methods (e.g., SP, A*, SAT) are able to produce the correct results. Therefore, in this work, we aim at improving the scalability of exact score-based methods for reliable causal discovery, by relying on the SMR assumption and relaxing the faithfulness assumption as much as is viable.
2.2 Linear Structural Equation Model
Given a linear SEM, each random variable follows the relationship Xi = BTi X + Ni, where Bi is a coefﬁcient vector and Ni is an exogenous noise variable associated with variable Xi. In this work we focus on the linear Gaussian model where the variables Ni’s follow the Gaussian distribution. The linear SEM can be written in matrix form as X = BTX + N, where B = [B1, B2, · · · , Bd] corresponds to a weighted adjacency matrix, and N = (N1, . . . , Nd) is a noise vector characterized by the covariance matrix Ω = cov[N] = diag(σ12, . . . , σd2). We assume that σi2 > 0 for i = 1, . . . , d so that the distribution P has positive measure everywhere. As a standard assumption, we also assume structural minimality [29] which implies that the nonzero coefﬁcients in B deﬁne the structure of G, i.e., Xj → Xi ∈ E if and only if the coefﬁcient in Bi corresponding to variable Xj is nonzero. Since one can always center the data, we assume E[X] = E[N] = 0 without loss of generality. The inverse covariance matrix Θ = Σ−1 of X is given by Θ = (I − B)Ω−1(I − B)T. Note that Θij = 0 if and only if Xi ⊥⊥ Xj|V \ {Xi, Xj} in the linear Gaussian case.
3

3 Weaker Assumptions for Super-Structure Estimation

We establish several weaker variants of faithfulness assumption, and show how they are both necessary and sufﬁcient for learning a super-structure of the true DAG via inverse covariance estimation.

3.1 Weaker Assumptions than Faithfulness

We describe several relaxed assumptions of faithfulness required for our super-structure estimation procedure. We ﬁrst start with the speciﬁc types of faithfulness related to different kinds of colliders.
Assumption 4 (Shielded-collider-faithfulness (SCF)). Given a DAG G and distribution P over the variable set V, let X → Y ← Z be any shielded collider in G. Then X and Z are dependent conditional on any subset of V \ {X, Z} that contains Y .
Assumption 5 (Unshielded-collider-faithfulness (UCF)). Given a DAG G and distribution P over the variable set V, let X → Y ← Z be any unshielded collider in G. Then X and Z are dependent conditional on any subset of V \ {X, Z} that contains Y .

The above assumptions are restrictions of the triangle-faithfulness [51] and orientation-faithfulness [30] assumptions, respectively, to collider structures. Note that they differ only in the type of collider being considered. However, these assumptions require dependence conditioning on any subset of V \ {X, Z}, which may be restrictive in practice. In this work, we further relax them to require only dependence conditioning on V \ {X, Z}, and will show in Section 3.2 how they are both necessary and sufﬁcient conditions for estimating a sound super-structure of the true DAG. These relaxed assumptions are stated below.

Assumption 6 (Single shielded-collider-faithfulness (SSCF)). Given a DAG G and distribution P over the variable set V, let X → Y ← Z be any shielded collider in G. Then X ⊥⊥ Z|V \ {X, Z}.

Assumption 7 (Single unshielded-collider-faithfulness (SUCF)). Given a DAG G and distribution

P over the variable set V, let X → Y ← Z be any unshielded collider in G. Then X ⊥⊥ Z|V\{X, Z}.

Figure 1 illustrate the examples in which the SCF and UCF assumptions are violated, respectively, but the SSCF and SUCF assumptions hold. In these

X 0.5 Y
0.5 0.5 1.0

examples, we have X ⊥⊥ Z|Y and X ⊥⊥ Z|{Y, W }, where the former unfaithful

CI relation X ⊥⊥ Z|Y is constructed via path cancellations.

W 1.0 Z

Given a distribution P, let Gfai(P), Gadj(P), Gori(P), Gtri(P), Gscf (P), Gucf (P), Gsscf (P), and Gsucf (P) be the set of DAGs satisfying the faithfulness, adjacency-faithfulness, orientation-faithfulness, triangle-faithfulness, SCF, UCF, SSCF, and SUCF assumptions, respectively. Also, denote by ⊂ the proper subset symbol. Combining with the results by Ramsey et al. [30], Zhang and Spirtes [51], we have the following nesting properties, showing that SSCF and SUCF are intuitively much weaker than the faithfulness assumption.

(a) Violation of SCF.
X 0.5 Y

0.5

0.5

W 1.0 Z

Remark 1. Gfai(P) ⊂ Gadj (P) ⊂ Gtri(P) ⊂ Gscf (P) ⊂ Gsscf (P).

(b) Violation of UCF.

Remark 2. Gfai(P) ⊂ Gori(P) ⊂ Gucf (P) ⊂ Gsucf (P).

Figure 1: Examples.

3.2 Inverse Covariance Estimation for Learning Super-Structure
Based on the assumptions described in Section 3.1, we develop a super-structure estimation method via inverse covariance estimation. We ﬁrst study the speciﬁc assumptions required for the support of inverse covariance, denoted as supp(Θ), to be the same as the moralized graph of the underlying DAG. This is similar to the analysis by Loh and Bühlmann [21], but we focus on formulating precisely the required assumptions from a causal discovery perspective, to shed light on how weak they are as compared to faithfulness. This enables us to further weaken the required assumptions in order to recover a super-structure of the true DAG based on the support of inverse covariance.
As described, we ﬁrst have the following theorem that relates the moral graph and the support of inverse covariance to the SUCF and SSCF assumptions, with a proof given in Appendix C.1.
Theorem 1. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ, under Markov assumption, the SSCF and SUCF assumptions are satisﬁed if and only if the structure deﬁned by supp(Θ) is the same as the moralized graph of the true DAG G.

4

Although the above theorem guarantees recovering the moral- 0.20

Min. value of | ij|

ized graph via supp(Θ), in practice the SUCF assumption may 0.15 be approximately violated. To illustrate, we simulate the linear Gaussian model with edge weights sampled uniformly from 0.10

[−0.8, −0.2] ∪ [0.2, 0.8] and exogenous noise variances sampled 0.05

uniformly from [1, 2]. We then compute the minimum values

101

102

103

mini,j{|Θij| : Xi and Xj correspond to a pair of neighbors in

Number of nodes

G} and mini,j{|Θij| : Xi and Xj correspond to a pair of non-

Neighbors

Non-adjacent spouses

adjacent spouses in G} over 100 simulations, by considering Figure 2: Expected degree of 2. different expected degrees and number of variables d ∈

{10, 20, 50, 100, 200, 500, 1000}. Note that Xi and Xj being a pair of non-adjacent spouses in

DAG G implies that they share a common child and have an unshielded collider; therefore, SUCF

requires that Xi ⊥⊥ Xj|V \ {Xi, Xj}, i.e., Θij = 0 in the linear Gaussian case. The visualizations

for expected degree of 2 is shown in Figure 2, while those for degrees of 5 and 8 can be be found in

Appendix B. Although the values in both cases decrease with larger graphs,1 the minimum values

corresponding to the non-adjacent spouses are signiﬁcantly smaller than those of neighbors, where

the difference grows in the number of nodes. For instance, on 1000-node graphs with degree of

2, the average minimum value of the former is 0.025, while that of the latter is close to 0.1. With

ﬁnite samples, it is very challenging to discover those undirected edges in the former case, since the

weights are very close to zero, leading to approximate violations of SUCF especially when there

is a large number of variables. Indeed, the simulations above are based on a certain range of edge

weights and noise variances, whose true values are not known in practice. Therefore, it is possible

that the data distribution considered falls into similar setting, leading to violation of SUCF. It is thus

desirable to further weaken the required assumptions to develop a more reliable procedure.

If we drop the SUCF assumption, the structure deﬁned by supp(Θ) may miss some edges as compared to the moralized graph of the true DAG. Fortunately, these edges correspond to the nonadjacent spouses in the DAG. That is, supp(Θ) is still guaranteed to contain all undirected edges corresponding to neighbors in the DAG, as long as SSCF (in addition to the Markov assumption) holds, indicating that it is a sound super-structure of the true DAG. Based on the simulations considered in Figure 2, approximate violations of SSCF are much less likely to happen than those of SUCF.
Theorem 2. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ, under Markov assumption, the SSCF assumption is satisﬁed if and only if the structure deﬁned by supp(Θ) is a super-structure of the true DAG G.

The proof is provided in Appendix C.1. With an asymptotically correct approach to estimate supp(Θ), the theorem implies that one can estimate a sound super-structure of the true DAG under the SSCF assumption that is strictly weaker than triangle-faithfulness (cf. Remark 1), an assumption which is intuitively much weaker than faithfulness. This may be considered as a signiﬁcant improvement over the existing methods that estimate the skeleton of the true DAG for use as a super-structure, utilizing methods like MMPC [45], which require faithfulness to be asymptotically correct.
The question remains as how to estimate supp(Θ). In the high-dimensional setting, the 1-penalized maximum likelihood estimator can be used to recover the support, with consistency result provided by Ravikumar et al. [33, 34]. In this work, we adopt the widely used graphical Lasso (GLasso) [8] method based on the block coordinate descent algorithm for estimating supp(Θ). Other efﬁcient estimation method, e.g., QUIC [14], can also be adopted, which is treated as a future work.
We provide empirical studies in Appendix E.1, in which faithfulness is exactly violated but not SSCF, to demonstrate that GLasso ﬁnds the true super-structure in these cases, whereas MMPC fails. This is consistent with the experiments in Section 5.1 which show that GLasso is more reliable in practice, since approximate violations of SSCF are less likely to happen as compared to those of faithfulness.

4 Improved Exact Search
We introduce several strategies to improve the scalability of exact score-based search. In Section 4.1, we ﬁrst establish the connection between the SMR assumption and exact search with BIC. We then leverage the super-structure estimation method described in Section 3.2 to restrict the search space of exact methods in Section 4.2, and develop a local search strategy in Section 4.3.
1Note that this is consistent with the analysis of the strong faithfulness assumption by Uhler et al. [46].

5

4.1 Exact Search and the SMR Assumption
As described in Section 2.1, classical methods such as PC and GES may produce sub-optimal solutions when faithfulness fails. In the following, we show the connection between the SMR assumption, which is strictly weaker than faithfulness, and the asymptotic correctness of exact search with BIC. To the best of our knowledge, this is the ﬁrst time that this result has been formally established.
Theorem 3. Exact score-based search with BIC asymptotically outputs a DAG that belongs to the MEC of the true DAG G if and only if the DAG G and distribution P satisfy the SMR assumption.
The proof can be found in Appendix C.2, which is straightforward from the consistency of BIC [12, 3]. Under the SMR assumption, the DAG estimated by exact search with BIC, is not necessarily identical to, but belongs to the same MEC as, the true DAG G. Therefore, one has to convert the estimated DAG to its MEC for causal interpretation [42].
4.2 A* with Super-Structure
Although exact search methods rely on the SMR assumption that is strictly weaker than faithfulness, it is challenging to scale them up to large graphs as the task is NP-hard [2]. To remedy this issue, similar to [27], we propose to explicitly constrain the search space of exact score-based search algorithms using super-structure. This is achieved by limiting the size of parent graphs in the search procedure.
To give a brief overview, for exact score-based methods such as DP [39, 38] and A* [49, 48], parent graph plays an important role in constructing the search space, which is a data structure that stores the costs for the arcs of the order graph. These methods typically construct a parent graph for each variable, which is used to build the order graph. The search is performed on the order graph [39, 48], by solving a shortest path problem from the root node, which corresponds to the empty set, to the leaf node, which corresponds to the complete set of variables. Each variable has its own parent graph, and all optimal parent sets are selected from the candidate parent sets, by selecting the ones with the best score in the corresponding parent graph. Here we omit the details of the parent graph and order graph, and refer the interested reader to the references above for their complete deﬁnitions and procedure.
Using the super-structure estimation method in Section 3.2, for each variable, we are able to obtain the set of nodes that must not be its candidate parents, and directly remove the entries involving these nodes in the corresponding parent graph. These constrained parent graphs help reduce search space in the order graph, from which the shortest path problem is formulated, and thereby improve the efﬁciency of the exact search procedure. As a byproduct, this strategy also reduces the memory cost, since one does not have to enumerate all candidate parent sets when constructing the parent graphs.
Note that DP and A* differ mainly in the search phase of the order graph. That is, DP has to consider all combinations of nodes (i.e., 2d combinations), and therefore is exponentially expensive in computation and quickly becomes infeasible when the graph size increases, while A* uses a heuristic function to only expand the most promising node [48], and has been shown empirically to achieve better efﬁciency. Therefore, in this work we adopt the A* algorithm with the incorporated super-structure to further constrain the search space, which we call A*-SS, although similar strategy also works for DP. Under the Markov and SSCF assumptions, our estimated super-structure via inverse covariance estimation is guaranteed to asymptotically contain the skeleton of the ground-truth DAG (cf. Theorem 2); thus, the proposed constrained search will not suffer from the trade-off between scalability and reliability given a sufﬁcient number of samples.
Besides using super-structure, we apply several existing strategies to improve the efﬁciency and memory cost of A*. In particular, we use the sparse representations [48] of the parent graphs to further remove unnecessary entries, which have been shown to improve the efﬁciency and substantially reduce the memory cost. We also adopt the optimal path extension [17] and dynamic k-cycle conﬂict heuristics [48] to reduce search space during the A* search. We refer the interested reader to the aforementioned references for detailed explanations of these strategies.
4.3 Local A*
Even with super-structure and the other techniques described in Section 4.2, scaling up exact scorebased search remains a challenge because of the large search space. Given the popularity of distributed computation, the idea of divide-and-conquer has been explored for scaling up Bayesian network
6

structure learning [10, 50, 19, 47]. However, most of them involve conditional independence tests, which therefore rely on the faithfulness assumption in some way.
To avoid relying on the faithfulness assumption, we aim to develop a local search strategy based on exact search that relies on the SMR assumption. Lu et al. [22] proposed an approximation algorithm, called Triplet A*, to do so. In particular, they ﬁrst apply estimation method like MMPC [45] to obtain a skeleton. For each variable X and each pair of its neighbors Y and Z, the algorithm constructs a cluster that consists of variables X, Y, Z and their neighbors indicated by the skeleton, and runs exact score-based search such as A* on each of these clusters independently. Finally, the ﬁnal structure is obtained by combining the search results from all clusters. A possible drawback is that the skeleton estimation method like MMPC usually requires the faithfulness assumption, although Triplet A* has been shown to be able to recover some of the missing edges caused by the unfaithful CI relations.
Without faithfulness, fortunately, one is still able to obtain a super-structure of the underlying DAG using our procedure under a strictly weaker assumption. In particular, the proposed super-structure estimation method involving inverse covariance estimation described in Section 3.2 requires only the SSCF assumption that is intuitively much weaker than faithfulness. Therefore, our goal is to develop a local search strategy based on this super-structure estimation method.
Following the similar strategy in [22], our main idea is that, for any variable X, its parents, children, spouses, and grandparents contain sufﬁcient information for exact score-based search (with SMR assumption) to correctly discover the undirected edges and v-structures involving X (if there is any). This is because this variable set, denoted as VX , includes all variables that are direct common causes of X and any of its direct neighbors. The question is then how to correctly identify a set that contains those variables in VX . A naive approach is to apply skeleton estimation methods like MMPC [45] to estimate a skeleton of the true DAG G. Clearly, for each variable X, its neighbors within two hops in the skeleton are guaranteed to contain its parents, children, spouses, and grandparents. However, MMPC requires the faithfulness assumption to be asymptotically correct, which may be restrictive in practice. Fortunately, based on Theorem 2, under the Markov and SSCF assumptions, the structure deﬁned by supp(Θ), denoted as G(supp(Θ)), is asymptotically a super-structure of the true DAG. It straightforwardly follows that the neighbors of variable X within two hops in G(supp(Θ)) must contain its parents, children, spouses, and grandparents. These variables, together with X itself, then contain sufﬁcient information for exact score-based search to correctly discover the undirected edges and v-structures involving X. The empirical studies in Section 5.2 suggest that the proposed local search procedure is asymptotically correct as it returns the same solutions as the A*-SS method.
As described in Section 3.2, in practice, one can use the GLasso method to produce an estimate of the inverse covariance matrix ˆΘ, and obtain its deﬁned structure, denoted as G(supp(ˆΘ)). Then, we construct a local cluster for each variable, which consists of the variable itself and its neighbors within two hops in the structure G(supp(ˆΘ)), and apply an exact score-based method such as A* on these local clusters independently. We then obtain the ﬁnal structure by combining these local results.
Our approach can be easily parallelized by running the local searches for all variables in parallel, in which the complexity depends on the maximum size of local clusters. In practice, the computational resources may be limited and one is not able to do so. Furthermore, there is a huge overlap between different local clusters, leading to the redundancy of computation during the search procedure. To alleviate this, we consider an iterative approach starting from the smallest local cluster to the largest one that keeps some edges ﬁxed to take full advantage of the information obtained from the previous searches. More speciﬁcally, in each iteration, we save the discovered undirected edges and v-structures involving the target variable from the local search. Then in the next iteration for, e.g., variable X, we look up the undirected edges and v-structures involving X that have been discovered and saved from previous iterations, and keep these edges ﬁxed in the local search for variable X. By trusting the information conveyed by these edges and keeping them ﬁxed, the search space for the following clusters can be drastically constrained. This improved procedure, called Local A*, is illustrated in Algorithm 1. Note that one could run some of the local searches in parallel to accelerate the iterative procedure, depending on the available computational resources.
It may be challenging to derive the exact computational complexity of the proposed method, as is the case for the A* algorithm, partly owing to the heuristic involved [11, Section 4]. This is because the complexity is affected by different factors such as the size of sparse parent graphs and the heuristic function (i.e., dynamic k-cycle conﬂict heuristics) [48]. Thus, we are only able to provide the running time as a proxy of the computational complexity in Section 5.4.
7

Algorithm 1 Local A*

Require: Structure deﬁned by the support of estimated inverse covariance matrix G(supp(ˆΘ)). 1: Initialize M ˆ as a d × d matrix in which all entries are zero. 2: Let N (i) denote the set of neighbors of variable Xi in the structure G(supp(ˆΘ)). 3: Obtain a variable ordering π by sorting the variables Xi, i = 1, . . . , d based on the cardinality

of their corresponding local cluster Ci = {Xi} ∪ N (i) ∪ j∈N(i) N (j) . Speciﬁcally, π(j)

refers to the index such that Cπ(j) is the j-th smallest local cluster.

4: for i = π(1), π(2), . . . , π(d) do

5: Obtain the local cluster Ci. 6: From M ˆ, obtain the previously discovered undirected edges and v-structures involving Xi.

7: Orient the undirected edges such that they do not create any cycle or additional v-structure.

8: With these edges ﬁxed, run A* (or A*-SS) on cluster Ci.

9: Convert the DAG estimated by A* to its MEC, and save the newly discovered undirected

10:

edges and v-structures involving variable Xi to M ˆ.

11: end for 12: Output the matrix M ˆ that represents the ﬁnal MEC.

5 Experiments
We ﬁrst conduct experiments to compare the efﬁcacy of GLasso and MMPC for estimating superstructures of the true DAGs. We then validate the proposed search strategies, and compare them to different baselines. Lastly, we compare different approaches to scale up A* search, including the proposed ones. The baselines include Triplet A* [22], PC [40, 30], FGES [31], MMHC [45], and SP [32]. The implementation details of our method and the baselines are given in Appendix D.
In our experiments, the ground truth DAGs are simulated using the Erdös–Rényi model [6] with different degrees and number of variables. We construct the weighted adjacency matrix of each DAG using edge weights sampled uniformly from [−0.8, −0.2] ∪ [0.2, 0.8]. Based on the weighted matrix constructed, we simulate n ∈ {300, 10000} samples using the linear Gaussian model with exogenous noise variances sampled uniformly from [1, 2]. We report the structural Hamming distance (SHD) over the complete partial DAGs (CPDAGs). We also compute the F1 score of the undirected and directed edges in the estimated CPDAGs. We do not provide the complete results for structural intervention distance (SID) [28], and include only a summary in Appendix E.2. Unless otherwise stated, we report the results and standard errors computed over 10 random simulations.
5.1 Different Super-Structure Estimation Methods
To demonstrate the efﬁcacy of GLasso for estimating super-structures with weaker assumptions, we compare the quality of super-structures estimated by GLasso to those by MMPC. We evaluate their ability for discovering the direct neighbors in the ground truth, by computing the true positive rates (TPRs) and false discovery rates (FDRs) of estimating the true neighbors. Although the output of GLasso may contain non-adjacent spouses, here we only evaluate its discovered direct neighbors as only those are important for further exact search. We consider 10-node graphs with varying sample sizes and expected degrees, and report the results of MMPC with different signiﬁcance levels α.
Due to space limit, the results for sample size n = 300 are shown in Figure 3, while those for n = 10000 are reported in Figure 7 in Appendix E.3. In the ﬁrst panel of Figure 3, one observes that GLasso achieves TPRs close to one across all cases, indicating that it rarely misses any direct neighbor, as compared to MMPC with lower TPRs. Notice also that the difference grows as the degree increases. Although GLasso has slightly higher FDRs than MMPC, we believe that the higher TPRs justify the cost of having a larger search space for exact search. It is interesting to observe that even with a high signiﬁcance level α such as 0.5 (i.e., the hypothesis tests used by MMPC tend to produce more edges), MMPC has low TPRs on denser graphs. For instance, its TPR is close to 0.7 on graphs with degree of 5. Similar observations can be made for the case of n = 10000. These may be ascribed to the approximate violations of faithfulness in the simulated data, and therefore MMPC may miss some neighbors because of unfaithful conditional independencies. On the other hand, GLasso requires only the SSCF assumption, which is intuitively much weaker than faithfulness.

8

TPR of Neighbors

1.0 0.9 0.8 0.7 0.6
1

2

3

4

5

Expected degree

GLasso

0.8

FDR of Neighbors

0.7

0.6

0.5

0.4

0.3

1

2

3

4

5

Expected degree

MMPC ( = 0.01)

MMPC ( = 0.1)

SHD of CPDAG

25 20 15 10 5

1

2

3

4

5

Expected degree

MMPC ( = 0.5)

Figure 3: Results of different super-structure estimation methods on 10-node graphs with different degrees. The sample size is n = 300. Lower is better, except for TPR.

SHD of CPDAG

F1 score of arrows

F1 score of skeleton

2.0

1.000

1.0

0.995

1.5 0.990

0.9

1.0

0.985

0.8

0.5

0.980

0.0 5

10

15

Number of nodes

0.975

20

5

10

15

20

Number of nodes

0.7 5

10

15

20

Number of nodes

A*

A*-SS

Local A*

Figure 4: Validation of the proposed methods on graphs with expected degree of 2. The sample size is n = 10000. For A*-SS and Local A*, it is assumed here that the ground truth supp(Θ) is known. Higher is better, except for SHD.

We also consider the use of the structures estimated by GLasso and MMPC as super-structures for A*-SS, to study how they affect the exact search procedure. The third panels of Figures 3 and 7 show that the super-structures estimated by GLasso lead to better search results than MMPC, especially on graphs with high expected degrees. This is because the performance of recovering the true CPDAGs is upper-bounded by the proportion of direct neighbors discovered by the super-structure.
5.2 Validation of the Proposed Search Strategies
We now conduct experiments to study the asymptotic correctness of the proposed strategies, by comparing the A*-SS and Local A* methods to A*. For A*-SS and Local A*, we assume that the support of the true inverse covariance matrix is known to ensure that the exact search does not make errors just because of an inaccurate estimate of the inverse covariance. In Sections 5.3 and 5.4, we consider the more realistic setting in which the ground truth supp(Θ) is not known. Here we simulate n = 10000 samples for the graphs with expected degree of 2 and {5, 10, 15, 20} nodes.
The results are reported in Figure 4, which show that the performance of Local A* and A*-SS is consistent across all metrics, including the F1 score of undirected and directed edges. This indicates that Local A* is able to output the right MECs that are the same as A*-SS in most cases, which suggests that Local A* is asymptotically correct. It is not surprising that these two methods perform better than A* for graphs with 15 and 20 nodes, because we assume here that the true supp(Θ) is known, which greatly reduces the search space and therefore is less susceptible to statistical errors owing to ﬁnite samples. This implies that a sound super-structure not only improves the efﬁciency, but also the performance. On the other hand, although A* is guaranteed to ﬁnd the global optimum, it may still miss or incorrectly estimate some edges because of statistical errors.
5.3 Comparison with Other Baselines
To compare the proposed methods, i.e., A*-SS and Local A*, with the baselines, we consider graphs with varying sample sizes n ∈ {300, 10000} and expected degrees from {1, 2, 3, 4, 5}. The baselines include Triplet A*, MMHC, PC, FGES, and SP. We start with 7-node graphs since the computation of SP may be too slow on graphs larger than that.
Due to space limit, we report the results for 300 and 10000 samples in Figures 8a and 8b, respectively, in Appendix E.4. With n = 300 samples, A*-SS and Local A* outperform other baselines across all metrics in most settings. Consistent with the observation in Section 5.2, the results of them are

9

SHD of CPDAG F1 score of skeleton F1 score of arrows
Time (minutes)

40
30
20
10
0 20 40 60 80 100
Number of nodes

1.00

0.95

0.90

0.85

20 40 60 80
Number of nodes

A*

A*-SS

1.0 0.9 0.8 0.7 100
Local A*

20 40 60 80 100
Number of nodes

Triplet A*

MMHC

103 101 10 1
20 40 60 80 100
Number of nodes

Figure 5: Results and time for graphs with different sizes. The sample size is n = 10000. Lower is better, except for F1 score.

exactly the same in all settings, which indicates that, similar to A*-SS, Local A* appears to output the globally optimal solutions. With a larger sample size n = 10000, the advantage of A*-SS and Local A* slightly diminishes as their performance is similar to the other baselines. A possible reason is that approximate violations of faithfulness are less likely to occur with more samples; thus, methods like PC and FGES are able to ﬁnd the right solutions. This also demonstrates that exact methods like A*-SS and Local A* that rely on the SMR assumption are relatively reliable in practice since they are less susceptible to approximate violations of faithfulness. Notice that SP performs the best for 10000 samples; this is not surprising as it is guaranteed to ﬁnd the globally optimal solutions. For A*-SS and Local A* that rely on super-structure, their performance may be upper-bounded by the proportion of direct neighbors discovered by the super-structure estimation method (see Section 5.1). However, SP does not scale well to large graphs and can handle at most 8 nodes due to the huge search space.
5.4 Scaling up A* to Large Graphs
We have proposed several strategies to scale up exact methods like A*, such as local search and constraining the parent graphs based on super-structure. To study the contribution of each strategy, we compare different variants of A*, including A*, A*-SS, and Local A*. Here we adopt Triplet A* and MMHC as our baselines since they have been shown to have similar performance to the other baselines in Section 5.3. Furthermore, they are the most relevant to our proposed Local A* method as they are also two-stage hybrid methods, i.e., they ﬁrst estimate a super-structure (or skeleton), and then perform score-based search on it. We conduct experiments with different graph sizes up to 300 nodes, and terminate the experiments that run for more than four days.
For better readability, the results and time for graphs with 100 nodes or less are depicted in Figure 5, and those for larger graphs are provided in Appendix E.5. One observes that Local A* and MMHC signiﬁcantly outperform the others in terms of scalability. Both of them can scale up to 300 nodes. Note that A*-SS scales up to 30 nodes, while A* can only be scaled up to 20 nodes in our experiments. This veriﬁes that incorporating super-structure with A* indeed helps reduces search space. Moreover, the search time for Local A* increases gently in the number of nodes, which demonstrates the potential of its scalability for large graphs. Note that in some of the experiments, Local A* did not ﬁnish within four days and were terminated. As described in Section 4.3, we observe that its running time depends on the maximum size of local clusters. If some of the local clusters contain many variables, the running time may be much longer. On the other hand, MMHC has a shorter running time than Local A*, because it adopts a hill-climbing strategy and is known to run relatively fast, similar to PC and FGES that generally ﬁnish within a few minutes in our experiments. Nevertheless, on large graphs, Local A* has much better structure recovery results than MMHC.

6 Conclusion
We studied the problem of reliable causal discovery with assumptions weaker than faithfulness. Speciﬁcally, in our proposed procedure, we adopted exact search that requires the SMR assumption, and relaxed the faithfulness assumption as much as is viable. We developed (1) a sound super-structure estimation method based on the SSCF assumption that is intuitively much weaker than faithfulness, (2) an improved exact score-based method with the constraint of the estimated super-structure, and (3) a local search strategy that improves the scalability of exact search. The efﬁcacy of the proposed method has been validated in our experiments conducted across various settings. It is worth noting that our procedure, at least in its current form, works only in the linear Gaussian setting. Therefore, a future direction is to extend it to the non-Gaussian and discrete cases.

10

Acknowledgments
The authors would like to thank the anonymous reviewers for helpful comments and suggestions. This work was supported in part by the National Institutes of Health (NIH) under Contract R01HL159805, by the NSF-Convergence Accelerator Track-D award #2134901, by the United States Air Force under Contract No. FA8650-17-C7715, and by a grant from Apple. The NIH or NSF is not responsible for the views reported in this article. JZ’s research was supported in part by the RGC of Hong Kong under GRF 13602720.
References
[1] M. Bartlett and J. Cussens. Integer linear programming for the Bayesian network structure learning problem. Artiﬁcial Intelligence, 244:258–271, 2017.
[2] D. M. Chickering. Learning Bayesian networks is NP-complete. In Learning from Data: Artiﬁcial Intelligence and Statistics V. Springer, 1996.
[3] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research, 3(Nov):507–554, 2002.
[4] D. Colombo, M. Maathuis, M. Kalisch, and T. Richardson. Learning high-dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics, 40:294–321, 2011.
[5] J. Cussens. Bayesian network learning with cutting planes. In Conference on Uncertainty in Artiﬁcial Intelligence, 2011.
[6] P. Erdös and A. Rényi. On random graphs I. Publicationes Mathematicae, 6:290–297, 1959.
[7] M. Forster, G. Raskutti, R. Stern, and N. Weinberger. The frugal inference of causal relations. The British Journal for the Philosophy of Science, 69, 04 2017.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso. Biostatistics, 9:432–41, 2008.
[9] A. Ghassami, A. Yang, N. Kiyavash, and K. Zhang. Characterizing distribution equivalence and structure learning for cyclic and acyclic directed graphs. In International Conference on Machine Learning, 2020.
[10] J. Gu and Q. Zhou. Learning big gaussian Bayesian networks: Partition, estimation and fusion. Journal of Machine Learning Research, 21(Aug):1–31, 2020.
[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100–107, 1968.
[12] D. Haughton. On the choice of a model to ﬁt data from an exponential family. The Annals of Statistics, 16, 03 1988.
[13] Y. He, J. Jia, and B. Yu. Counting and exploring sizes of Markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 16:2589–2609, 2015.
[14] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. QUIC: Quadratic approximation for sparse inverse covariance estimation. Journal of Machine Learning Research, 15(83): 2911–2947, 2014.
[15] A. Hyttinen, P. Hoyer, F. Eberhardt, and M. Jarvisalo. Discovering cyclic causal models with latent variables: A general SAT-based procedure. In Conference on Uncertainty in Artiﬁcial Intelligence, 2013.
[16] A. Hyttinen, F. Eberhardt, and M. Järvisalo. Constraint-based causal discovery: Conﬂict resolution with answer set programming. In Conference on Uncertainty in Artiﬁcial Intelligence, 2014.
11

[17] S. Karan and J. Zola. Exact structure learning of Bayesian networks by optimal path extension. In IEEE International Conference on Big Data, pages 48–55. IEEE Computer Society, 2016.
[18] M. Koivisto and K. Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research, 5(Dec):549–573, 2004.
[19] K. Kojima, E. Perrier, S. Imoto, and S. Miyano. Optimal search on clustered structural constraint for learning Bayesian network structure. Journal of Machine Learning Research, 11(Jan):285– 310, 2010.
[20] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, Cambridge, MA, 2009.
[21] P.-L. Loh and P. Bühlmann. High-dimensional learning of linear causal networks via inverse covariance estimation. Journal of Machine Learning Research, 15(88):3065–3105, 2014.
[22] N. Y. Lu, K. Zhang, and C. Yuan. Improving causal discovery by optimal Bayesian network learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021.
[23] I. Ng, A. Ghassami, and K. Zhang. On the role of sparsity and DAG constraints for learning linear DAGs. In Advances in Neural Information Processing Systems, 2020.
[24] S. Ott, S. Imoto, and S. Miyano. Finding optimal models for small gene networks. Paciﬁc Symposium on Biocomputing. Paciﬁc Symposium on Biocomputing, 9:557–67, 2004.
[25] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2009.
[26] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[27] E. Perrier, S. Imoto, and S. Miyano. Finding optimal Bayesian network given a super-structure. Journal of Machine Learning Research, 9(74):2251–2286, 2008.
[28] J. Peters and P. Bühlmann. Structural intervention distance (SID) for evaluating causal graphs. Neural Computation, 27, 2015.
[29] J. Peters, D. Janzing, and B. Schölkopf. Elements of Causal Inference - Foundations and Learning Algorithms. MIT Press, 2017.
[30] J. Ramsey, J. Zhang, and P. Spirtes. Adjacency-faithfulness and conservative causal inference. In Conference on Uncertainty in Artiﬁcial Intelligence, 2006.
[31] J. Ramsey, M. Glymour, R. Sanchez-Romero, and C. Glymour. A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images. International Journal of Data Science and Analytics, 3(2):121–129, 2017.
[32] G. Raskutti and C. Uhler. Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1):e183, 2018.
[33] P. Ravikumar, G. Raskutti, M. J. Wainwright, and B. Yu. Model selection in Gaussian graphical models: High-dimensional consistency of 1-regularized MLE. In Advances in Neural Information Processing Systems, 2008.
[34] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing 1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 2011.
[35] R. Scheines, P. Spirtes, C. Glymour, C. Meek, and T. Richardson. The TETRAD project: Constraint based aids to causal model speciﬁcation. Multivariate Behavioral Research, 33: 65–117, 1998.
12

[36] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978.
[37] M. Scutari. Learning Bayesian networks with the bnlearn R package. Journal of Statistical Software, 35(3):1—22, 2010.
[38] T. Silander and P. Myllymäki. A simple approach for ﬁnding the globally optimal Bayesian network structure. In Conference on Uncertainty in Artiﬁcial Intelligence, 2006.
[39] A. P. Singh and A. W. Moore. Finding optimal Bayesian networks by dynamic programming. Technical report, Carnegie Mellon University, 2005.
[40] P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social Science Computer Review, 9:62–72, 1991.
[41] P. Spirtes and J. Zhang. A uniformly consistent estimator of causal effects under the k-trianglefaithfulness assumption. Statistical Science, 29(4):662–678, 2014.
[42] P. Spirtes and K. Zhang. Search for causal models. In M. Maathuis, M. Drton, S. Lauritzen, and M. Wainwright, editors, Handbook of Graphical Models, chapter 18. CRC Press, Inc., 2018.
[43] P. Spirtes, C. Meek, and T. Richardson. Causal inference in the presence of latent variables and selection bias. In Conference on Uncertainty in Artiﬁcial Intelligence, 1995.
[44] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2001.
[45] I. Tsamardinos, L. Brown, and C. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65:31–78, 10 2006.
[46] C. Uhler, G. Raskutti, P. Bühlmann, and B. Yu. Geometry of the faithfulness assumption in causal inference. The Annals of Statistics, 41(2):436–463, 2013.
[47] X. Xie and Z. Geng. A recursive method for structural learning of directed acyclic graphs. The Journal of Machine Learning Research, 9(March):459–483, 2008.
[48] C. Yuan and B. Malone. Learning optimal Bayesian networks: A shortest path perspective. Journal of Artiﬁcial Intelligence Research, 48(1):23–65, 2013.
[49] C. Yuan, B. Malone, and X. Wu. Learning optimal Bayesian networks using A* search. In International Joint Conference on Artiﬁcial Intelligence, 2011.
[50] H. Zhang, S. Zhou, C. Yan, J. Guan, X. Wang, J. Zhang, and J. Huan. Learning causal structures based on divide and conquer. IEEE Transactions on Cybernetics, 2020.
[51] J. Zhang and P. Spirtes. Detection of unfaithfulness and robust causal inference. Minds and Machines, 18, 06 2008.
[52] X. Zheng, B. Aragam, P. Ravikumar, and E. P. Xing. DAGs with NO TEARS: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, 2018.
13

Appendices

A Further Common Assumptions for Causal Discovery
Following from Section 2.1, we review several relaxations of faithfulness, which give rise to different constraint-based causal discovery methods that allow certain type of unfaithfulness.
Assumption 8 (Adjacency-faithfulness [30]). Given a DAG G and distribution P over the variable set V, if two variables X and Y are adjacent in G, then they are dependent conditional on any subset of V \ {X, Z}.
Assumption 9 (Orientation-faithfulness [30]). Given a DAG G and distribution P over the variable set V, let <X, Y, Z> be any unshielded triple in G.
(1) If X → Y ← Z, then X and Z are dependent conditional on any subset of V \ {X, Z} that contains Y ;
(2) otherwise, X and Z are dependent conditional on any subset of V \ {X, Z} that does not contain Y.
Under the Markov and adjacency-faithfulness assumptions, any violation of orientation-faithfulness is detectable in the sense that the true distribution P is not faithful to any DAG [30, 51]. This presents a concrete method to detect unfaithfulness, leading to a variation of the PC method known as Conversative PC [30] that avoids making deﬁnite claim of the causal structure when violation of orientation-faithfulness is detected. Along this line, a further restriction of the adjacency-faithfulness assumption has been formulated and also adopted by the Very Conservative SGS algorithm [41] to relax the type of faithfulness assumption required.
Assumption 10 (Triangle-faithfulness [51]). Given a DAG G and distribution P over the variable set V, let <X, Y, Z> be any three variables that form a triangle in G (i.e., they are adjacent to one another).
(1) If Y is a non-collider on the path <X, Y, Z>, then X and Z are dependent conditional on any subset of V \ {X, Z} that does not contain Y .
(2) If Y is a collider on the path <X, Y, Z>, then X and Z are dependent conditional on any subset of V \ {X, Z} that contains Y .

B Analysis of the SUCF Assumption

We provide further simulation results for the analysis of the SUCF assumption in Section 3.2.

Min. value of | ij| Min. value of | ij|

0.10 0.08 0.06 0.04 0.02 0.00
101

102

103

Number of nodes

Neighbors

Non-adjacent spouses

0.20

0.15

0.10

0.05

0.00 101

102

103

Number of nodes

Neighbors

Non-adjacent spouses

(a) Expected degree of 5.

(b) Expected degree of 8.

Figure 6: Visualizations of the minimum values mini,j{|Θij| : Xi and Xj correspond to a pair of neighbors in G} and mini,j {|Θij| : Xi and Xj correspond to a pair of non-adjacent spouses in G} computed across 100 simulations. Different number of nodes and expected degrees are considered.
X-axes are visualized in log scale.

14

C Proofs

C.1 Proof of Theorems 1 and 2

We ﬁrst describe Lemmas 1 and 2 required to prove the main theorems, and these lemmas are similar to Lemma 1 and Theorem 2 by Loh and Bühlmann [21], respectively, but we do assume that B is strictly upper/lower triangular. Here, Lemma 1 is straightforward from expanding the equation Θ = (I − B)Ω−1(I − B)T with Ω = diag(σ12, . . . , σd2).
Lemma 1. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ. The entries of Θ are given by

Θjk = −σj−2Bkj − σk−2Bjk + σ−2Bj Bk ,
=j,k

∀j = k,

Θjj = σj−2 + σ−2B2j ,

∀j.

=j

Lemma 2. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ, the structure deﬁned by supp(Θ) is a subgraph of the moralized graph of the true DAG G.
Proof. Let Xj and Xk, j = k be two variables that are not adjacent in the moralized graph of G. Then it sufﬁces to show that Θjk = Θkj = 0. Clearly, Xj and Xk must not be adjacent in the DAG G, indicating that Bkj = Bjk = 0. They also cannot share a common child; otherwise they must be adjacent in the moralized graph. Therefore, we have Bj = 0 and Bk = 0 for = j, k. Applying Lemma 1 gives Θjk = Θkj = 0.
With the above lemmas, we now provide the proofs of the main results.
Theorem 1. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ, under Markov assumption, the SSCF and SUCF assumptions are satisﬁed if and only if the structure deﬁned by supp(Θ) is the same as the moralized graph of the true DAG G.
Proof. We proceed by contraposition in both parts of the proof.
If part: Suppose either the SSCF or SUCF assumption is violated, i.e., there exists a collider Xj → Xi ← Xk in the DAG G such that Xj ⊥⊥ Xk|V \ {Xj, Xk}. This indicates that Θjk = Θkj = 0. Since Xj and Xk are either neighbors or spouses, there exists an edge between them in the moralized graph of G, but is not contained in the structure deﬁned by supp(Θ), showing that they are not the same.
Only if part: Suppose that the structure deﬁned by supp(Θ) is not the same as the moralized graph of G. Then, by Lemma 2, there exists a pair of variables Xj and Xk, j = k that are adjacent in the moralized graph but Θjk = Θkj = 0. In the linear Gaussian case, we have Xj ⊥⊥ Xk|V \ {Xj, Xk}. It remains to consider the following cases:
• If variables Xj and Xk correspond to a pair of non-adjacent spouses in G, then they have an unshielded collider, indicating that the SUCF assumption is violated.
• Otherwise, variables Xj and Xk correspond to a pair of neighbors in G. Assume without loss of generality that Xj is a parent of Xk, i.e., Xj → Xk ∈ E. This implies that Bjk = 0 and Bkj = 0, which, by Lemma 1, yields
Θjk = −σk−2Bjk + σ−2Bj Bk = 0.
=j,k
Since σk−2Bjk = 0, there exists a variable Xi with i = j, k such that σi−2BjiBki = 0. We then have Bji = 0 and Bki = 0, indicating that Xi is a common child of the variables Xj and Xk. In this case, Xj → Xi ← Xk forms a shielded collider in G, which, with the CI relation Xj ⊥⊥ Xk|V \ {Xj, Xk}, implies that the SSCF assumption is violated.
In both cases, either the SUCF or SSCF assumption is violated.

15

Theorem 2. Given a DAG G and distribution P that follow a linear Gaussian model with inverse covariance matrix Θ, under Markov assumption, the SSCF assumption is satisﬁed if and only if the structure deﬁned by supp(Θ) is a super-structure of the true DAG G.
Proof. We proceed by contraposition in both parts of the proof. Note that the proof is similar to that of Theorem 1.
If part: Suppose the SSCF assumption is violated, i.e., there exists a shielded collider Xj → Xi ← Xk in the DAG G such that Xj ⊥⊥ Xk|V \ {Xj, Xk}. This indicates that Θjk = Θkj = 0. Since Xj and Xk are neighbors, there exists an edge between them in G, but is not contained in the structure deﬁned by supp(Θ), showing that it is not a super-structure of the true DAG G.
Only if part: Suppose that the structure deﬁned by supp(Θ) is not a super-structure of the DAG G. Then, there exists a pair of neighbors Xj and Xk, j = k in G such that Θjk = Θkj = 0. In the linear Gaussian case, we have Xj ⊥⊥ Xk|V \ {Xj, Xk}. Assume without loss of generality that Xj is a parent of Xk, i.e., Xj → Xk ∈ E. This implies that Bjk = 0 and Bkj = 0, which, by Lemma 1, yields
Θjk = −σk−2Bjk + σ−2Bj Bk = 0.
=j,k
Since σk−2Bjk = 0, there exists a variable Xi, i = j, k such that σi−2BjiBki = 0. We then have Bji = 0 and Bki = 0, indicating that Xi is a common child of the variables Xj and Xk. In this case, Xj → Xi ← Xk forms a shielded collider in G, which, with the CI relation Xj ⊥⊥ Xk|V \ {Xj, Xk}, implies that the SSCF assumption is violated.
C.2 Proof of Theorem 3
Theorem 3. Exact score-based search with BIC asymptotically outputs a DAG that belongs to the MEC of the true DAG G if and only if the DAG G and distribution P satisfy the SMR assumption.
Proof. We provide a proof by contrapositive in both directions based on the consistency of the BIC score [12, 3].
If part: Suppose that exact score-based search asymptotically outputs a DAG H (having the highest BIC score) that does not belong to the MEC of the true DAG G. Since the BIC score is known to be consistent, (H, P) must satisfy the Markov assumption, because otherwise its BIC score is lower than that of the true DAG G and exact search would not have output H. By assumption, the BIC score of H is higher than that of G, which, by the consistency of BIC, implies that |H| ≤ |G|, and therefore, (G, P) does not satisfy the SMR assumption.
Only if part: Suppose that (G, P) does not satisfy the SMR assumption. Then there exists a DAG H not in the MEC of G such that |H| ≤ |G|, and (H, P) satisﬁes the Markov assumption. Without loss of generality, we choose H with the least number of edges. We ﬁrst consider the case in which |H| < |G|. Since both H and G satisfy the Markov assumption, by the consistency of BIC, the BIC score of H is higher than that of G, which implies that exact score-based search will not output any DAG from the MEC of G. For the case with |H| = |G|, since they are both Markov with distribution P, they have the same BIC score. Therefore, exact search will output a DAG that belongs to the MEC of either H or G, and is not guaranteed to output a DAG from the MEC of the true DAG G.
D Implementation Details
This section provides the implementation details of the proposed Local A* method and the baselines.
D.1 Local A*
Local A* ﬁrst uses inverse covariance estimation to discover a super-structure of the underlying DAG, and then applies the A*-SS method on the local clusters formed by each variable and its neighbors within two hops in the super-structure, using some further strategy to reduce search
16

space (see Section 4.3). In our experiments, we use GLasso to estimate the support of the inverse covariance matrix, implemented through the scikit-learn package [26]. We set the coefﬁcient of 1 penalty term to 0.05 for 20-node graphs or smaller, and otherwise to 0.2. For graphs with more than 40 nodes, a thresholding step is applied on the estimated covariance matrix to remove the entries whose absolute values are less than 0.03. We use relatively small values for conservative variable selection. If needed, one may further use suitable model selection methods (e.g., crossvalidation) to select these hyperparameters. Since we focus on the linear Gaussian setting, we use the BIC score for the exact search procedure. To accelerate Algoritnm 1, we run the local search procedure on 12 local clusters in parallel (i.e., on 12 CPUs). The code is available at https://github.com/ignavierng/local-astar.
D.2 Baselines
The implementation details of the baselines are described below:
• PC and FGES are implemented using the py-causal package [35] distributed under the LGPL 2.1 license. For the former, we use the Conservative PC algorithm [30] with Fisher Z test, while the BIC score [36] is adopted for the latter.
• The implementation of MMHC is available through the bnlearn package [37] in R that is published under the GPL-3 license.
• SP is implemented using the causaldag package in Python under the 3-Clause BSD License.
• We use our own Python implementation of Triplet A* as we were not able to run the ofﬁcial C++ implementation on graphs with more than 5 nodes. Similar to the proposed Local A* approach, parallel computing is used to accelerate the iterative search procedure of Triplet A*.
We use the default hyperparameters unless otherwise stated. For a fair comparison, we run all experiments of the baselines on 12 CPUs.
E Supplementary Experiment Results
E.1 Exact Violations of Faithfulness
To demonstrate the efﬁcacy of GLasso for estimating super-structures with weaker assumptions, we experiment with several examples by using GLasso and MMPC to discover the direct neighbors of the true DAG when faithfulness is guaranteed to be violated. Consider an example where X → Y → Z → W and X → W such that X ⊥⊥ W due to path cancellation. In this case, the faithfulness assumption is violated but SSCF holds. In particular, we consider the linear SEM X = NX , Y = X + NY , Z = Y + NZ , W = −X + Z + NW where NX , NY , NZ , NW ∼ N (0, 1). We conduct 100 random simulations for 20, 100, and 106 samples. For 20 samples, GLasso is able to discover all the direct neighbors (in the true DAG) in 98 of the simulations, whereas for 100 and 106 samples, it discovers the neighbors in all simulations. For MMPC, it fails to discover the edge between the direct neighbors X and W in 69, 56, and 98 of the simulations for 20, 100, and 106 samples, respectively, because of the unfaithful independency X ⊥⊥ W . This demonstrates that GLasso is able to recover the direct neighbors in cases where MMPC fails, as the former requires only the SSCF assumption that is intuitively much weaker than faithfulness required by the latter.
E.2 Structural Intervention Distance
We use SHD and F1 score to compare the different CPDAGs. If we are given DAGs, then we can compute their corresponding SID in a straightforward way. Comparing CPDAGs with SID is more complicated, although it is doable according to Peters and Bühlmann [28, Section 2.4.2], since it requires computing the lower and upper bounds of the SID. For this reason, we did not include the results of SID in this paper. Nevertheless, the observations of SID are nearly identical to those based on SHD. For instance, for the dataset considered in Figure 8a (i.e., 7-node graphs with 300 samples) with expected degree of 2, the average lower and upper bounds of the SID for A*-SS, Local A*, Triplet A*, PC, FGES, MMHC, and SP are (7.5, 10.5), (7.5, 10.5), (10.2, 13.1), (15.0, 20.1), (14.2, 17.8), (16.2, 18.4), and (14.3, 18.4), respectively.
17

E.3 Different Super-Structure Estimation Methods
This section provides further empirical results to compare the efﬁcacy of GLasso and MMPC for estimating super-structures in practice (see Section 5.1).

TPR of Neighbors FDR of Neighbors SHD of CPDAG

1.0 0.9 0.8 0.7
1

2

3

4

5

Expected degree

GLasso

0.8

0.6

0.4

0.2

1

2

3

4

5

Expected degree

MMPC ( = 0.01)

MMPC ( = 0.1)

25

20

15

10

5

0

1

2

3

4

5

Expected degree

MMPC ( = 0.5)

Figure 7: Results of different super-structure estimation methods on 10-node graphs with different degrees. The sample size is n = 10000. Lower is better, except for TPR.

E.4 Comparison with Other Baselines
This section provides further empirical results to compare the proposed methods to the baselines (see Section 5.3).

SHD of CPDAG

15

10

5

0

1

2

3

4

5

Expected degree

F1 score of skeleton

1.0 0.9 0.8 0.7

1

2

3

4

5

Expected degree

F1 score of arrows

0.8

0.6

0.4

0.2

1

2

3

4

5

Expected degree

A*-SS

Local A*

Triplet A*

PC

FGES

MMHC

SP

SHD of CPDAG

(a) n = 300.

15

1.00

1.0

F1 score of arrows

F1 score of skeleton

10 0.95 0.8

0.90

0.6

5 0.85 0.4

0

1

2

3

4

5

Expected degree

0.80

1

2

3

4

5

Expected degree

1

2

3

4

5

Expected degree

A*-SS

Local A*

Triplet A*

PC

FGES

MMHC

SP

(b) n = 10000.
Figure 8: Results of different structure learning methods on 7-node graphs with different degrees and sample sizes. Higher is better, except for SHD. For better visualization, we do not include the standard errors here as each panel has a number of lines.

18

E.5 Scaling up A* to Large Graphs
This section provides empirical results for Local A* and MMHC on graphs with {50, 100, 150, 200, 250, 300} nodes (see Section 5.4).

SHD of CPDAG F1 score of skeleton F1 score of arrows
Time (minutes)

150
100
50
0 50 100 150 200 250 300
Number of nodes

0.975 0.950 0.925 0.900
50 100 150 200 250 300
Number of nodes Local A*

0.9
0.8
0.7 50 100 150 200 250 300
Number of nodes MMHC

102 101 100 10 1
50 100 150 200 250 300
Number of nodes

Figure 9: Results and time for Local A* and MMHC on graphs with different sizes. The sample size is n = 10000. Lower is better, except for F1 score.

19

