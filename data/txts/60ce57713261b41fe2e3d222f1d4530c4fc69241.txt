arXiv:1501.06626v1 [cs.GT] 27 Jan 2015

Manipulating the Probabilistic Serial Rule
Haris Aziz, Serge Gaspers, Simon Mackenzie, Nicholas Mattei
NICTA and UNSW, Kensington 2033, Australia
Nina Narodytska
Carnegie Mellon University, Pittsburgh, PA 15213-3891, USA
Toby Walsh
NICTA and UNSW, Kensington 2033, Australia
Abstract The probabilistic serial (PS) rule is one of the most prominent randomized rules for the assignment problem. It is well-known for its superior fairness and welfare properties. However, PS is not immune to manipulative behaviour by the agents. We initiate the study of the computational complexity of an agent manipulating the PS rule. We show that computing an expected utility better response is NPhard. On the other hand, we present a polynomial-time algorithm to compute a lexicographic best response. For the case of two agents, we show that even an expected utility best response can be computed in polynomial time. Our result for the case of two agents relies on an interesting connection with sequential allocation of discrete objects. Keywords: Assignment problem, probabilistic serial mechanism, fair allocation JEL: C62, C63, and C78
1. Introduction
The assignment problem is one of the most fundamental and important problems in economics and computer science [see e.g., 3, 4, 6, 12, 13]. In the setting, agents express preferences over objects and, based on these preferences, the objects are allocated to the agents. The model is applicable to many resource allocation or fair division settings where the objects may be public houses, school seats, course enrollments, kidneys for transplant, car park spaces, chores, joint
Email addresses: haris.aziz@nicta.com.au (Haris Aziz), serge.gaspers@nicta.com.au (Serge Gaspers), Simon.Mackenzie@nicta.com.au (Simon Mackenzie), Nicholas.Mattei@nicta.com.au (Nicholas Mattei), ninan@cs.cmu.edu (Nina Narodytska), toby.walsh@nicta.com.au (Toby Walsh)
1

assets, or time slots in schedules. A randomized or fractional assignment rule takes the preferences of the agents into account in order to allocate each agent a fraction of the object. If the objects are indivisible but allocated in a randomized way, the fraction can also be interpreted as the probability of receiving the object. Randomization is widespread in resource allocation since it is one of the most natural ways to ensure procedural fairness [8]. Randomized assignments have been used to assign public land, radio spectra to broadcasting companies, and US permanent visas to applicants [Footnote 1, 8].
Among the various randomized/fractional assignment rules, the probabilistic serial (PS) rule is one of the most prominent rules [1, 5, 6, 8, 14, 16, 19, 17]. PS works as follows. Each agent expresses a linear order over the set of houses (we use the term house throughout the paper though we stress any object could be allocated with these mechanisms). Each house is considered to have a divisible probability weight of one, and agents simultaneously and with the same speed eat the probability weight of their most preferred house. Once a house has been eaten by a subset of agents, these agents proceed to eat their next most preferred house that has not been completely eaten. The procedure terminates after all the houses have been eaten. The random allocation of an agent by PS is the amount of each object he has eaten. Although PS was originally deﬁned for the setting where the number of houses is equal to the number of agents, it can be used without any modiﬁcation for fewer or more houses than agents [see e.g., 6, 16].
The probabilistic serial (PS) rule fares better than any other random assignment rule in terms of fairness and welfare [5, 6, 8, 16, 19]. In particular, it satisﬁes strong envy-freeness and eﬃciency with respect to both stochastic dominance (SD) and downward lexicographic (DL) relations [6, 18, 16]. SD is one of the most fundamental relations between fractional allocations because one allocation is SD-preferred over another if for every utility function consistent with the ordinal preferences, the former yields at least as much expected utility as the latter. DL is a reﬁnement of SD and based on lexicographic comparisons between fractional allocations. Generalizations of the PS rule have been recommended in many settings [see e.g., 8]. The PS rule also satisﬁes some desirable incentive properties. If the number of objects is at most the number of agents, then PS is weak SD-strategyproof [6]. Another well-established rule random serial dictator (RSD) is not envy-free, not as eﬃcient as PS [6] and the fractional allocations under RSD are #P-complete to compute [2]. However, unlike RSD, PS is not strategyproof.
In this paper, we examine the following natural question for the ﬁrst time: what is the computational complexity of an agent computing a diﬀerent preference to report so as to get a better PS outcome? This problem of computing the optimal manipulation has already been studied in great depth for voting rules [see e.g., 11]. Ekici and Kesten [10] showed that when agents are not truthful, the outcome of PS may not satisfy desirable properties related to efﬁciency and envy-freeness. Hence, it is important to check that even if agents can in principle manipulate, how hard it is to compute a beneﬁcial misreport of their preferences. The complexity of manipulation of the PS rule is also related
2

to the study of Nash dynamics and better responses. Eﬃcient algorithms to compute best responses can be used to understand Nash dynamics under the mechanism.
In order to compare random allocations, an agent needs to consider relations between them. We consider three well-known relations between random allocations [see e.g., 6, 18, 17, 9]: (i) expected utility (EU), (ii) stochastic dominance (SD), and (iii) downward lexicographic (DL). For EU, an agent seeks a diﬀerent allocation that yields more expected utility. For DL, an agent seeks an allocation that gives a higher probability to the most preferred alternative that has diﬀerent probabilities in the two allocations. Throughout the paper, we assume that agents express strict preferences, i.e., they are not indiﬀerent between any two houses.
Contributions. We initiate the study of computing best responses for the PS mechanism — one of the most established randomized rules for the assignment problem. The study is additionally motivated by complementing experimental work where we observe that as the number of houses relative to the number of agents grows, the percentage of manipulable proﬁles (for which at least one agent has incentive to manipulate) increases, maximizing at around 99%. We present a polynomial-time algorithm to compute the DL best response for multiple agents and houses. For the case of two agents, we present a polynomial-time algorithm to compute an EU best response for any utilities consistent with the ordinal preferences. The two-agent case is also of special importance since various disputes arise between two parties. The result for the EU best response relies on an interesting connection between the PS rule and the sequential allocation rule for indivisible objects. In a sequential allocation, a picking sequence is speciﬁed for the agents and agent get his most preferred available object when his turns comes. For general n, we show that computing an EU best response is NP-hard. The result contrasts sharply with the recent result of Bouveret and Lang [7] that a best response can be computed in polynomial time for sequential allocation.
2. Preliminaries
An assignment problem (N, H, ≻) consists of a set of agents N = {1, . . . , n}, a set of houses H = {h1, . . . , hm} and a preference proﬁle ≻= (≻1, . . . , ≻n) in which ≻i denotes a complete, transitive and strict ordering on H representing the preferences of agent i over the houses in H. A fractional assignment is an (n × m) matrix [p(i)(hj)]1≤i≤n,1≤j≤m such that for all i ∈ N , and hj ∈ H, 0 ≤ p(i)(hj) ≤ 1; and for all j ∈ {1, . . . , n}, i∈N p(i)(hj) = 1. The value p(i)(hj) is the fraction of house hj that agent i gets. Each row p(i) = (p(i)(h1), . . . , p(i)(hm)) represents the allocation of agent i. A fractional assignment can also be interpreted as a random assignment where p(i)(hj) is the probability of agent i getting house hj.
A standard method to compare random allocations is to use the SD (stochastic dominance) relation. Given two random assignments p and q, p(i) ≻Si D q(i) i.e., a player i SD prefers allocation p(i) to q(i) if hj∈{hk:hk≻ih} p(i)(hj) ≥
3

hj∈{hk:hk≻ih} q(i)(hj ) for all h ∈ H and hj∈{hk:hk≻ih} p(i)(hj ) > hj∈{hk:hk≻ih} q(i)(hj ) for some h ∈ H. Given two random assignments p and q, p(i) ≻Di L q(i) i.e., a player i DL (downward lexicographic) prefers allocation p(i) to q(i) if p(i) = q(i) and
for the most preferred house h such that p(i)(h) = q(i)(h), we have that
p(i)(h) > q(i)(h).
When agents are considered to have cardinal utilities for the objects, we
denote by ui(h) the utility that agent i gets from house h. We will assume that
the total utility of an agent equals the sum of the utilities that he gets from each of the houses. Given two random assignments p and q, p(i) ≻Ei U q(i) i.e., a player i EU (expected utility) prefers allocation p(i) to q(i) if h∈H ui(h) · p(i)(h) > h∈H ui(h) · q(i)(h).
Since for all i ∈ N , agent i compares assignment p with assignment q only
with respect to his allocations p(i) and q(i), we will sometimes abuse the notation and use p ≻Si D q for p(i) ≻Si D q(i). A random assignment rule takes as input an assignment problem (N, H, ≻) and returns a random assignment which
speciﬁes what fraction or probability of each house is allocated to each agent.

3. The Probabilistic Serial Rule and its Manipulation

The Probabilistic Serial (PS) rule is a random assignment algorithm in which we consider each house as inﬁnitely divisible [6, 16]. At each point in time, each agent is eating (consuming the probability mass of) his most preferred house that has not been completely eaten and each agent eats at the same unit speed. Hence all the houses are eaten at time m/n and each agent receives a total of m/n units of houses. The probability of house hj being allocated to i is the fraction of house hj that i has eaten. The following example adapted from [Section 7, 6] shows how PS works.
Example 1 (PS rule). Consider an assignment problem with the following preference proﬁle.

≻1: h1, h2, h3

≻2: h2, h1, h3

≻3: h2, h3, h1

Agents 2 and 3 start eating h2 simultaneously whereas agent 1 eats h1. When 2 and 3 ﬁnish h2, agent 1 has only eaten half of h1. The timing of the eating can be seen below.

Agent 1

h1

Agent 2

h2

Agent 3

h2

h1

h3

h1

h3

h3

h3

0 12 43 1 Time

The ﬁnal allocation computed by PS is
3/4 0 1/4 P S(≻1, ≻2, ≻3) = 1/4 1/2 1/4 .
0 1/2 1/2

4

Consider the assignment problem in Example 1. If agent 1 misreports his preferences as follows: ≻′1: h2, h1, h3, then
1/2 1/3 1/6 P S(≻′1, ≻2, ≻3) = 1/2 1/3 1/6 .
0 1/3 2/3

Then, if u1(h1) = 7, u1(h2) = 6, and u1(h3) = 0, then agent 1 gets more expected utility when he reports ≻′1. In the example, although truth-telling is

a DL best response, it is not necessarily an EU best response for agent 1.

Examples 1 and 2 of Kojima [16] show that manipulating the PS mechanism

can lead to an SD improvement when each agent can be allocated more than one

house. In light of the fact that the PS rule can be manipulated, we examine the

complexity of a single agent computing a manipulation, in other words, the best

response for the PS rule.1 For a preference proﬁle ≻, we denote by (≻−i, ≻′i) the preference proﬁle obtained from ≻ by replacing agent i’s preference by ≻′i. For

E ∈ {SD, EU, DL}, we deﬁne the problem E -BR: Given (N, H, ≻), compute

a preference ≻′1 for agent 1 such that there exists no preference ≻′1′ such that

P

S

(N

,

H

,

(≻

′′ 1

,

≻

−

1

))

≻E1

P

S

(N

,

H

,

(≻

′ 1

,

≻

−

1

)).

For a constant m, the problem E -BR can be solved by brute force by trying

out each of the m! preferences. Hence we will not assume that m is a constant.

We establish some more notation and terminology for the rest of the paper.

We will often refer to the PS outcomes for partial lists of houses and preferences.

We will denote by P S(≻Li , ≻−i)(i), the allocation that agent i receives when his

preference is according to ordered list L. Note that preferences and ordered lists

are interchangeable, except that a list need not contain all houses in H. When

an agent runs out of houses in his preference list, he stops eating. The length

of a list L is denoted |L|, and we refer to the kth house in L as L(k). In the PS

rule, the eating start time of a house is the time point at which the house starts

to be eaten by some agent. In Example 1, the eating start times of h1, h2 and

h3 are 0, 0 and 0.5, respectively.

4. Lexicographic best response
In this section, we present a polynomial-time algorithm for DL-BR. Lexicographic preferences are well-established in the assignment literature [see e.g., 17, 18, 9]. Let (N, H, ≻) be an assignment problem where N = {1, . . . , n} and H = {h1, . . . , hm}. We will show how to compute a DL best response for agent 1 ∈ N . It has been shown that when m ≤ n, then truth-telling is the DL best response but if m > n, then this need not be the case [17, 18, 16].
Recall that a preference ≻′1 is a DL best response for agent 1 if the fractional allocation agent 1 receives by reporting ≻′1 is DL preferred to any fractional

1Note that if an agent is risk-averse and does not have information about the other agent’s preferences, then his maximin strategy is to be truthful. The reason is that if all agents have the same preferences, then the optimal strategy is to be truthful.

5

allocation agent 1 receives by reporting another preference. That is, there is no

preference ≻′1′ such that his share of a house h when reporting ≻′1′ is strictly larger than when reporting ≻′1 while the share of all houses he prefers to h (according to his true preference ≻1) is the same whether reporting ≻′1 or ≻′1′.

Our algorithm will iteratively construct a partial preference list for the

i most preferred houses of agent 1. Without loss of generality, denote ≻1:

h1, h2, . . . , hm.

For any i, 1 ≤ i ≤ m, denote Hi = {h1, . . . , hi}. A preference of agent 1

restricted to Hi is a preference over a subset of Hi. For the preference of agent

1 restricted to Hi, the PS rule computes an allocation where the preference of

agent 1 is replaced with this preference and the preferences of all other agents

remain unchanged. The notions of DL best response and DL preferred fractional

assignments with respect to a subset of houses Hi are deﬁned accordingly for

restricted preferences of agent 1.

For

a

house

h

∈

H,

let

P S1(L, h)

=

(P

S

(≻

L 1

,

≻

−

1

)(1

))(h

)

denote

the

frac-

tion of house h that the PS rule assigns to agent 1 when he reports the (partial)

preference L. We start with a simple lemma showing that a DL best response

for agent 1 for the whole set H can be no better and no worse on Hi than a DL

best response for Hi.

Lemma 1. Let i ∈ {1, . . . , m}. A DL best response for agent 1 on H gives the same fractional assignment to the houses in Hi as a DL best response for agent 1 on Hi.

Our algorithm will compute a list Li such that Li ⊆ Hi.2 The list Li will be

a DL best response for agent 1 with respect to Hi. Suppose the algorithm has

computed Li−1. Then, when considering Hi = Hi−1 ∪ {hi}, it needs to make

sure that the new fractional allocation restricted to the houses in Hi−1 remains

the same (due to Lemma 1). For the preference to be optimal with respect to

Hi, the algorithm needs to maximize the fractional allocation of hi to agent 1

under the previous constraint.

Our algorithm will compute a canonical DL best response that has several

additional properties. A preference Li for Hi is no-0 if Li contains no house h

with P S1(Li, h) = 0. Any DL best response for agent 1 for Hi can be converted

into a no-0 DL best response by removing the houses for which agent 1 obtains a

fraction of 0. For a no-0 preference Li for Hi, the stingy ordering for a position j

is determined by running the PS rule with the preference Li(1)⊕· · ·⊕Li(j−1) for

agent 1, where ⊕ denotes concatenation. It orders the houses from

|Li| k=j

Li(k)

by increasing eating start times, and when two houses h, h′ have the same eating

start time, we order h before h′ iﬀ h ≻1 h′. Intuitively, houses occurring early

in this ordering are the most threatened by the other agents at the time point

when agent 1 comes to position j. The following deﬁnition takes into account

that the eating start times of later houses may change depending on agent 1’s

ordering of earlier houses.

2When we treat a list as a set we refer to the set of all elements occurring in the list.
6

Algorithm 1 DL best response for n agents

Input: (N, H, ≻) Output: DL Best response of agent 1

L1 ← h1

// Best response for agent 1 w.r.t. H1 = {h1}

for i = 2 to n do

// Compute a best response w.r.t. H2, . . . , Hn

p←0

if ∃q ∈ {1, . . . , i − 1} such that 0 < P S1(Li−1, Li−1(q)) < 1 then

p ← max{q ∈ {1, . . . , i − 1} : 0 < P S1(Li−1, Li−1(q)) < 1}

end if

for q ← p + 1 to |Li| + 1 do

// New house hi inserted

Lqi ← Li−1(1) ⊕ · · · ⊕ Li−1(q − 1) ⊕ hi

while |Lqi | ≤ |Li−1| do

// Complete the list according to the

est ← EST(N, H, (Lqi , ≻2, . . . , ≻n))

S ← {h ∈ Li−1 \ Lqi : est(h) is minimum}

hs ← ﬁrst house among S in ≻1

Lqi ← Lqi ⊕ hs

end while

if P S1(Lqi , hi) = 0 then

Lqi ← Li−1

end if

after position p stingy ordering

end for q←p worse[p − 1] ← true

// Determine which Lqi is stingy

ﬁnished ← false

while ﬁnished = false do

if

∃h ∈

Hi−1

such

that

P

S

1(L

q i

,

h)

=

P S1(Li−1, h)

then

worse[q] ← true

q ← q+1

else

worse[q] ← false if P S1(Lqi , h1) > 0 and P S1(Lqi , h1) < 1 then
if worse[q − 1] = false then

q ← q−1

end if

ﬁnished ← true else if P S1(Lqi , h1) = 1 then
est ← EST(N, H, (Lqi (1) ⊕ · · · ⊕ Lqi (q − 1), ≻2, . . . , ≻n)) if ∃h ∈ {Lqi (q + 1), . . . , Lqi (|Lqi |)} such that est(h) ≤ est(hi) then
q ← q+1

else

ﬁnished = true

end if

end if

end if

end while Li ← Lqi end for

return Ln

A preference Li for Hi is stingy if it is a no-0 DL best response for agent 1 on Hi, and for every j ∈ {1, . . . , i}, Li(j) is the ﬁrst house in the stingy ordering for this position such that there exists a DL best response starting with Li(1) ⊕ · · · ⊕ Li(j). We note that, due to Lemma 1, there is a unique stingy preference for each Hi.
Example 2. Consider the following assignment problem.

≻1: h1, h2, h3, h4, h5, h6

≻2: h3, h6, h4, h5, h1, h2

The preferences h3, h1, h4, h2 and h3, h2, h4, h1 are both no-0 DL best responses for agent 1 with respect to H4, allocating p(1)(h1) = 1, p(1)(h2) = 1, p(1)(h3) =

7

1/2, p(1)(h4) = 1/2 to agent 1. When running the PS rule with h3 as the preference list, h4’s eating start time comes ﬁrst among {h1, h2, h4}. However, there is no DL best response for H4 starting with h3, h4. The next house in the stingy ordering is h1. The preference h3, h1, h4, h2 is the stingy preference for H4.
The next lemma shows that when agent 1 receives a house partially (a fraction diﬀerent from 0 and 1) in a DL best response, a stingy preference would not order a less preferred house before that house.
Lemma 2. Let Li be a stingy preference for Hi. Suppose there is a hj ∈ Hi such that 0 < P S1(Li, hj) < 1. Then, P ⊆ Hj, where Li = P ⊕ hj ⊕ S.
The next lemma shows how the houses allocated completely to agent 1 are ordered in a stingy preference.
Lemma 3. Let Li be a stingy preference for Hi. If hj, hk ∈ Hi are two houses such that P S1(Li, hj) = P S1(Li, hk) = 1, with Li = P ⊕ hj ⊕ M ⊕ hk ⊕ S, then either the eating start time of hj is smaller than hk’s eating start time when agent 1 reports P , or it is the same and hj ≻1 hk.
Proof. Suppose not. But then, Li is not stingy since swapping hj and hk in Li gives the same fractional allocation to agent 1.
We now show that when iterating from a set of houses Hi−1 to Hi, the previous solution can be reused up to the last house that agent 1 receives partially.
Lemma 4. Let Li−1 and Li be stingy preferences for Hi−1 and Hi, respectively. Suppose there is a h ∈ Hi−1 such that 0 < P S1(Li−1, h) < 1. Then the preﬁxes of Li−1 and Li coincide up to h.
We are now ready to describe how to obtain Li from Li−1. See Algorithm 1 for the pseudocode. The subroutine EST(N, H, ≻) executes the PS rule for (N, H, ≻) and for each item, records the ﬁrst time point where some agent starts eating it. It returns the eating start times est(h) for each house h ∈ H.
Let p be the last position in Li−1 such that the house Li−1(p) is partially allocated to agent 1. In case agent 1 receives no house partially, set p := 0 and interpret Li−1(p) as an imaginary house before the ﬁrst house of Li−1. By Lemma 4, we have that Li−1(s) = Li(s) for all s ≤ p. By Lemma 1, we have that the fractional assignment resulting from Li must wholly allocate all houses Li−1(p + 1), . . . , Li−1(|Li−1|) to agent 1, and allocate a share of 0 to all houses in Hi−1 \ Li−1.
It remains to ﬁnd the right ordering for {Li−1(s) : p + 1 ≤ s ≤ |Li−1|} ∪ {hi}. By Lemmas 2 and 3, the preﬁxes of Li−1 and Li coincide up to h. We will describe in the next paragraph how to determine the position q where hi should be inserted. Having determined this position one may then need to re-order the subsequent houses. This is because inserting hi in the list may change the eating start times of the subsequent houses. This leads us to the following insertion procedure. The list Lqi obtained from Li−1 by inserting hi
8

at position q, with p < q ≤ |Li| + 1, is determined as follows. Start with Lqi := Li−1(1) ⊕ · · · ⊕ Li−1(q − 1) ⊕ hi. While |Lqi | ≤ |Li−1|, we append to the end of Lqi the ﬁrst house among Li−1 \ Lqi in the stingy ordering for this

position. After the while-loop terminates, run the PS rule for the resulting list Lqi . In case we obtain that P S1(Lqi , hi) = 0, we remove hi again from this list (and actually obtain Lqi = Li−1).

The position q where hi is inserted is determined as follows. Start with

q := p.

We

have

an

array

worse

keeping

track

of

whether

the

lists

L

p i

,

.

.

.

,

L

i i

produce a worse outcome for agent 1 than the list Li−1. Set worse[p − 1] := true.

As long as the list Li has not been determined, proceed as follows. Obtain Lqi from Li−1 by inserting hi at position q, as described earlier. Consider the allocation of agent 1 when he reports Lqi . If this allocation is not the same for

the houses in Hi−1 as when reporting Li−1, then set worse[q] := true, otherwise

set worse[q] := false. If worse[q], then increment q. This is because, by Lemma 1,

this preference would not be a DL best response with respect to Hi. Otherwise,

if

0

<

P

S

1

(L

q i

,

h

i

)

<

1,

then

we

can

determine

hi’s

position.

If worse[q − 1],

then set Li := Lqi , otherwise set Li := Liq−1. This position for hi is optimal

since moving hi later in the list would decrease its share to agent 1. Otherwise,

we

have

that

worse[q]

=

false

and

P

S

1

(L

q i

,

h

i

)

∈

{0, 1}.

This

will

be

the

share

agent 1 receives of hi.

If

P

S

1

(L

q i

,

h

i

)

=

0,

then

set

Li

:=

Li−1.

Otherwise

(P

S

1

(L

q i

,

h

i

)

=

1),

it

still

remains

to

check

whether

the

current

position

for

hi gives a stingy preference. For this, run the PS rule with the preference Lqi (1) ⊕ · · · ⊕ Lqi (q − 1) for agent 1. If hi’s eating start time is smaller than the eating start time of each house Lqi (r) with r > q, then set Li := Lqi , otherwise

increment q.

Thus, given Li−1, the preference Li can be computed by executing the PS

rule O(m) times. The DL best response computed by the algorithm is Lm.

Since the PS rule can be implemented to run in linear time O(nm), the running

time of this DL best response algorithm is O(nm3).

Theorem 1. DL-BR can be solved in O(nm3) time.

Example 3. Consider the following instance.

≻1: h1, h2, h3, h4, h5, h6, h7, h8, h9, h10 ≻2: h8, h3, h5, h2, h10, h1, h6, h7, h4, h9 ≻3: h9, h4, h7, h1, h2, h6, h5, h3, h8, h10

After having computed L2 = h1, h2, the algorithm is now to consider H3. Since

P S1(L2, h1) = P S1(L2, h2) = 1, the algorithm ﬁrst considers L13 = h3, h2, h1.

Note that h1 and h2 have been swapped with respect to L2 since agent 2 starts

eating h2 before agent 3 starts eating h1 when agent 1 reports the preference

list consisting of only h3.

It

turns

out

that

P

S

1

(L

1 3

,

h

1

)

=

P

S

1

(L

1 3

,

h

2

)

=

P

S

1

(L

1 3

,

h

3

)

=

1.

Thus, worse[1] = false.

Since h3

does not come ﬁrst in the

stingy ordering, the algorithm needs to verify whether moving h3 later will still

give a DL best response with respect to H3. It then considers L23 = h1, h3, h2.

However, this allocates only half of h3 to agent 1, implying worse[2] = true. Since

9

worse[1] = false, the algorithm sets L3 = L13. The DL best response computed by the algorithm is L10 = h3, h2, h1, h6.
We note that a DL best response is also an SD best response. One may wonder whether an algorithm to compute the DL best response also provides us with an algorithm to compute an EU best response. However, a DL best response may not be an EU best response for three or more agents. Consider the preference proﬁle in Example 1. Since the number of houses is equal to the number of agents, reporting the truthful preference is a DL best response [18]. However, we have shown a diﬀerent preference for agent 1 where he may obtain higher utility.

5. Expected utility best response In this section, we consider the problem of expected utility best response.

5.1. Case of two agents

We ﬁrst show that for the case of two agents, an EU best response can

be computed in linear time. The result hinges on a close connection that we

identify between PS and discrete allocation of objects to agents via sequential

allocation. In the sequential allocation setting (N, O, ≻′, π), there is an agent set

N , an object set O = {o1, . . . om′ }, a preference proﬁle ≻′ that speciﬁes for each agent i ∈ N his preferences ≻′i over O, and a policy π : {1, . . . , m′} → N . The sequential allocation rule works as follows. Starting from j = 1 to m′, agent π(j)

gets his most preferred object that is not yet allocated. If no unallocated object

is on the preference list of the agent, then the agent does not get any object when

his turn comes. The assignment as a result of sequential allocation is denoted

by SA(N, O, ≻′, π). We will restrict ourselves to the case where N = {1, 2} and

will only consider the alternating policy π∗ = 1212 . . . in which agent 1 starts

ﬁrst and then the agents keep alternating. The sequential allocation setting was

introduced by Kohler and Chandrasekaran [15] where they showed that the best

response can be computed in linear time when |N | = 2 and the policy is the

alternating sequence. Recently, Bouveret and Lang [7] generalized their result

to the case of any number of agents, any policy, and where the manipulator may

be indiﬀerent between objects.

We highlight a close connection between sequential allocation and PS and

thereby between allocation mechanisms for indivisible and divisible houses. For

the random assignment setting ({1, 2}, H, ≻), the half-house reduction gives us

the sequential allocation setting ({1, 2}, O, ≻′, π∗). In the reduction, each house

hj ∈ H is cloned so that we have two half-houses h1j and h2j for each house hj:

O

=

{

h

1 j

,

h

2 j

:j

=

1, . . . , m}.

Both

agents

have

preferences

over

half-houses

that

are consistent with their preferences over houses and for each house, each agent

prefers the ﬁrst half-house slightly more than the second half-house: if hj ≻i hk,

then h1j ≻′i h2j ≻′i h1k ≻′i h2k. We show that for n = 2, the assignment under

PS is ‘essentially’ the same as the assignment obtained by applying sequential

allocation to the setting resulting from the half-house reduction:

10

Remark 1. The assignment P S({1, 2}, H, ≻) and the assignment

SA({1, 2}, O, ≻′, π∗) are related as follows: P S({1, 2}, H, ≻)(i)(hj) =

1 2

·

(SA({1,

2},

O,

≻′,

π∗)(i)(h1j )

+

SA({1,

2},

O,

≻′,

π∗)(i)(h2j )).

We note that in the half-house reduction, each preference list ≻′i satisﬁes the consecutivity property: half-houses corresponding to the same house are placed
consecutively in the preference list. We will use the consecutivity property in
our argument.

Theorem 2. For the case of two agents, an EU best response can be computed in linear time.

Proof. We consider the EU best response problem for PS where the manipula-

tor, agent 1, has preferences ≻1: h1, . . . , hm. The main idea is to reduce the EU

best response problem ({1, 2}, H, ≻) for PS to the EU best response problem

({1, 2}, O, ′, π∗) for sequential allocation. The reduction is a slight modiﬁca-

tion of the half-house reduction with the diﬀerence that agent 1 is indiﬀerent

between two half-houses corresponding to the same house. The object set is

O

=

{

h

1 j

,

h

2 j

:j

=

1, . . . , m}.

In

′, both agents have preferences over half-

houses that are consistent with their preferences over houses. We will assume

without loss of generality that agent 2 prefers the ﬁrst half-house slightly more

than the second half-house. Agent 1 is indiﬀerent between any two half-houses

corresponding to the same house: h1j ∼′1 h2j for all j ∈ {1, . . . , m} but will be re-

quired to report strict preferences. When we consider sequential allocation, we

will view it in rounds so that in each round, ﬁrst agent 1 picks a most preferred

available house and then agent 2 picks a most preferred available house.

In the algorithm by Bouveret and Lang [7], when agents have strict prefer-

ences, it is checked whether the manipulator (agent 1) can get diﬀerent target

sets of objects. In the algorithm, only a linear number of target sets need to be

considered. Given target set Tk which is restricted to objects from o1, . . . , ok, we

can compute target set Tk+1 as follows: check whether target set Tk ∪ {ok+1}

can be achieved or not. Tk+1 = Tk ∪ {ok+1} if Tk ∪ {ok+1} can be achieved

and Tk+1 = Tk otherwise. Tm is then the most preferred allocation that agent

1 achieves and the allocation is unique. When the manipulator is indiﬀerent

among objects, Bouveret and Lang [7] showed that their algorithm can be eas-

ily modiﬁed as follows: agent 1 considers a linear order instead of his actual

weak order where the linear order is achieved by breaking ties between the in-

diﬀerent objects in the same order as the preference of agent 2. Based on this

insight, observe that both agents will pick h1j before h2j for any j ∈ {1, . . . , m}

if they report truthfully.

We ﬁrst show that there exists a best response of agent 1 in the sequential al-

location setting (N, O, ′, π∗) that satisﬁes the consecutivity property. If agent

1 either gets both half-houses corresponding to a house or none of them, then his

optimal preference report for sequential allocation trivially satisﬁes the consec-

utivity property. If this is not the case, then let us consider the most preferred

house hj for which agent 1 gets one of the corresponding half-houses but not the

other. If agent 1 only gets h1j but not h2j , this means that in his best response for

11

houses restricted to {h11, . . . , h2j }, h2j was already taken by agent 2 in a round in

which agent 1 picked some other object. Then agent 1 can eventually insert h2j

immediately after h1j in his best response preference knowing well that he will

not get h2j . Thus, the best response for sequential allocation can be modiﬁed so

that it satisﬁes the consecutivity property and yields the same optimal alloca-

tion. Now consider the case where agent 2 gets h1j but agent 1 gets h2j . Then

this means that agent 1 cannot get h1j in his best response when his preference

is

restricted

only

to

houses

from

the

set

{

h

1 1

,

h

2 1

,

.

.

.

,

h

1 j−

1

,

h

2 j−

1

,

h

1 j

}

.

Therefore,

agent 1 can still insert h1j eventually just before h2j in his best response so that

the consecutivity property is satisﬁed and the allocation does not change even

though agent 1 does not get h1j in his best response.

We now show that the best response of agent 1 in the sequential allocation

setting (N, O, ′, π∗) can be used to compute the best response of agent 1 in

(N, H, ≻) under PS. Let U be the expected utility for agent 1 under his best response ≻∗1 in the PS setting. The best response ≻∗1 corresponds to ≻∗1′ over

the set of half-houses. By Remark 1, agent 1 achieves essentially the same

allocation and hence the same utility U in the sequential allocation setting if he submits preference ≻∗1′. Conversely, if agent 1 achieves utility U in the

sequential allocation setting via a preference report, then he achieves at least as much utility by reporting his optimal preference ≻∗1′ constructed via the algorithm of Bouveret and Lang [7]. Hence, the preference ≻∗1′ can be modiﬁed

as shown above so that it satisﬁes the consecutivity property. In this case, there exists a preference ≻∗1 over H which is consistent with the preferences ≻∗1′ over O. If agent 1 reports ≻∗1, then he gets essentially the same allocation as SA({1, 2}, O, (≻∗1′, ≻′2)(1) and thus gets utility U .

The best response algorithm of Bouveret and Lang [7] returns the same optimal preference report for all cardinal utilities consistent with the ordinal preference of the manipulator. Next, we point out that for the case of two agents and the PS rule, a DL best response and an EU best response are equivalent.

Proposition 1. For the case of two agents and the PS rule, a DL best response is an EU best response and an EU best response is a DL best response.

Proof. For two agents, PS assigns probabilities from the set {0, 1/2, 1}. Hence DL preferences can be represented by EU preferences where the utilities are exponential: the utility of a more preferred house is twice the utility of the next most preferred house. Hence a response is a DL best response if it is an EU best response for exponential utilities. On the other hand we have shown that for two agents and the PS rule, an EU best response is the same for any utilities compatible with the preferences. Hence for two agents, an EU best response for any utilities is the same as the EU best response for exponential utilities which in turn is the same as a DL best response.

5.2. General case We show that an EU best response is NP-hard to compute. The result
contrasts with Theorem 1 which states that a DL best response can be computed

12

in polynomial time.

Theorem 3. EU-BR is NP-hard.

Proof. To show hardness we show that the following problem is NP-complete:

given an assignment setting as well as a utility function u : H → N specifying

the utility of each house for the manipulator (agent 1) and a target utility T ,

can the manipulator specify preferences such that the utility for his allocation

under the PS rule is at least T ? We reduce from a restricted NP-hard version

of 3SAT where each literal appears exactly twice in the formula. Given such a

3SAT instance F = (X, C) where X = {x1, . . . , xn} is the set of variables and C

the set of clauses, we build an instance of EU-BR where the manipulator can

obtain utility ≥ T if and only if the formula is satisﬁable. At a high level, we

will create an instance of the assignment problem which can be conceptualized

as 18 (mostly) disjoint parts that we index by D ∈ {1, . . . , 18}. We will describe

the main (ﬁrst) part in detail and explain how it is duplicated to create the

other 17 parts. Each of the 18 parts is divided up into n choice rounds which

we index from 1 to n. For each part there is an additional clause round. The

18 parts are linked by a special set of houses which allow us to synchronize the

timing of the manipulator with respect to all the other agents. The set of agents

is N = {1} ∪

1D8=2{aDdummy} ∪

18 D=1

ADliterals

where

the

manipulator

is

Agent

1, 17 ‘dummy’ manipulators for the 17 copies of the main part, and two agents

for each positive and negative literal in the formula for each of the 18 parts,

ADliterals = {a1x,iD, a2x,iD, a1¬,xDi , a2¬,xDi : xi ∈ X }.

The set of houses is H = Hslow∪

18 D=1

HrDounds ∪

18 D=1

HcDlause ∪

18 D=

2

{

h

D CP

}

∪

{hprize} where Hslow = {hrs : r ∈ {1, . . . , n − 1}} is the set of slowdown houses

that will be used to control the timing of the manipulator’s decisions. Note

that there is only one slowdown house per round and these houses are shared

between all 18 parts. HrDounds = {hrx,iD, hr¬,xDi : r ∈ {1, . . . , n}, i ∈ {1, . . . , n}} is a set of houses consisting of one house for each positive and negative literal in the

formula for each of the n rounds; Hclause = {h1c,D, h2c,D, h3c,D : c ∈ {1, . . . , C}}

is a triplet of houses for each clause in the formula; hprize is the prize house for

the manipulator; and 1D8=2{hDCP } is the set of consolation prize houses for the dummy manipulators.

We will describe how to construct the preferences for the main part which

contains the manipulator, agent 1, and then explain the small diﬀerences neces-

sary to create the 17 other duplicate instances. Example 4 gives an illustration

of the main part of a small instance and may be helpful for reference during the

discussion.

Main part. We will describe the rounds by declaring which houses are eaten in them and show how the preference lists of the agents are constructed. Each agent’s preference list can be described has having a head and a tail. To ease the description, we will omit the round index D = 1 in the variable names. Intuitively, the head consists of the houses that the agent will consume during the running of the PS algorithm while the tail consists of houses that will not

13

be eaten. When we describe how we add houses to an agent’s preference list,

we will say append the house(s) to the head to mean add this set of houses to

the end of the head of the preference list, behind those that have been placed

before. We say append the house(s) to the tail of the preferences to mean place

them last amongst all houses which have been placed in the preferences so far.

In each choice round r, houses hrxi and hr¬xi for each i ∈ {1, . . . , n} will be eaten. Append those houses to the head of the preferences of the agents

corresponding to the same literal and append them to the tail of the preferences

of agents associated to a diﬀerent literal. Append houses hrxr and hr¬xr to the head of the manipulator’s preferences (the order in which we add them in is not

important). Houses hrxi and hr¬xi where i = r are appended to the tail of the manipulator’s preferences. In each choice round except the last one, slowdown

house hrs will be eaten. We append it to the tail of the preferences of the literal

agents, and to the head of the preferences of the manipulator agent (after the

literal houses we added for this round).

Finally we describe the clause round. For each clause, we have the 3 houses

h

1 c

,

h

2 c

,

h

3 c

.

We

append

these

3

houses

to

the

head

of

the

preferences

of

exactly

1

agent corresponding to the negation of each of the clause c’s literals. If an agent

has already had houses added to his preferences in the clause round, we add

them to the other agent corresponding to the same literal (since a literal appears

only twice in the formula, this ensures each agent has only one triplet of houses

appended to the head of their preferences). The prize house hprize is appended

to the head of both the manipulator’s and the literal agents’ preferences (after

the clause houses we just added to the literal agents).

Duplicate parts. For each of the duplicate parts, D ∈ {2, . . . , 18}, we will de-
scribe the necessary modiﬁcations. For clarity we call the copy of the prize house in the duplicated parts of the instance consolation prize houses denoted hDCP for each D ∈ {2, . . . , 18}. Recall that the set of slowdown houses Hslow is shared between all the parts; thus all the parallel constructions ‘merge’ at the set of slowdown houses. We are left with the fact that houses from a given duplicate
part D of the instance have not been added to the preferences of agents from all other parts of the instance. We can append all these houses to the tail of
the preferences of the agents outside this part in any order.

The manipulator’s utilities. We will give the manipulator’s utility in terms of

a number α to be ﬁxed later. The prize house has utility 1. The literal houses

that are appended to the head of the manipulator’s preferences during round i

(hixi

and

hi¬xi )

have

utility

(2α)2(n−i)

and

(2α)2(n−i) + ǫ

where

ǫ

is

O

(

1 2n

).

The

slowdown houses have utility (2α)2(n−i−1)+1. All other houses have negligible

utility. By negligible we mean that adding up all their combined utilities will

yield

less

than

1 α

utility.

This

can

be

done

since

we

have

a

polynomial

number

of houses and we can make the utilities exponentially small.

Based on these utilities we can now derive a target value for T and anal-

yse the behaviour that the manipulator must have to reach that target. The

manipulator may only start eating a new house once the house he is currently

14

eating is no longer available. This means that if he starts eating a house, he is ‘stuck’ eating said house for a certain amount of time. We now constrain the manipulator’s possibilities by showing that by diverging from the literal and slowdown houses he should be eating according to his preferences, he will commit to a house for which he has exponentially less utility for an amount of time which is at least some constant. By setting α to be large enough, we can ensure that this loss in utility is irrecuperable. We say the manipulator behaves as prescribed if he declares preferences which correspond to his true preferences up to permutations of the literals associated with one same variable.
Let t1 > 0 be the smallest amount of time the manipulator will eat a new house if he has behaved as prescribed in all his previous choices. The next lemma shows that t1 is independent of the instance size.
Lemma 5. t1 ∈ O(1).

Proof. As the algorithm progresses, we may group the agents in a constant

number of groups based on the extent they have eaten their current house when

the manipulator ﬁnishes consuming one of his houses and the number of agents

eating that house. Each group is associated with a value, which corresponds to

the amount of time the manipulator would have to spend if he decided to eat

a house currently being eaten by members of that group. By showing that the

number of these groups is constant, and therefore so is the number of values, we

show that t1 is a constant. The groups can be characterized by the type of house

that the members are eating. At any point in the algorithm we say that a literal

has been chosen by the manipulator if the round r is greater than the index i

of that literal, r > i. We say that a literal is untouched by the manipulator for

i > r. The groups are deﬁned as follows: 1) Agents eating houses being eaten by

an agent corresponding to a literal which has been chosen by the manipulator

2) Agents eating houses being eaten by an agent corresponding to a literal

which is the negation of one chosen by the manipulator 3) Agents eating houses

corresponding to literals untouched by the manipulator 4) Agents eating houses

being eaten by dummy manipulators. At the start of any round i, eating a house from group j would take g1j time.

The manipulator then ﬁnishes eating the ﬁrst literal, and eating a house from group j would take g2j time. After eating the second literal, eating a house from group j would take g3j time. Finally the manipulator eats the slowdown houses and we have corresponding value g4j. We will now show that the values for glj

are the same for all rounds. To show this we simply need to make sure that

all the agents stay ‘synchronised’. It takes the manipulator 0.5 units of time

to

ﬁnish

the

current

round

( 31

on

the

ﬁrst

literal,

1 9

on

the

second,

and

1 18

on

the slowdown house). Let us now show that it also takes 0.5 units of time for

every other group to get to the same point in the next round. The exception are

the agents eating a house that is also being eaten by the manipulator or some

dummy in that round, which fall out of sync with their previous group (group

3 or 4) and transit either to group 1 or 2. For groups 1-3, all these agents pair

up and have 1 house per round. It therefore takes them each 0.5 time to eat

15

it. For Group 4, the dummy manipulators eat a ﬁrst literal ( 31 ) then a second

( 91 ) and ﬁnally all 18 manipulators join together and eat the slowdown houses

in

the

round,

which

takes

them

time

118 .

This

adds

up

to

9 18

=

0.5.

Corollary 1. There is value for α ∈ O(1) such that the manipulator behaves as prescribed.

Lemma 6. In the clause round all agents corresponding to literals chosen by the

manipulator start the round at the same time as the manipulator, whilst agents

corresponding to negation of the choice of the manipulator are in advance and

start

the

round

1 9

units

of

time

before

the

manipulator.

Proof. In Lemma 5 we argued that the agents took the same amount of time to

ﬁnish a round. The exception to this is the last round where the manipulator

does not eat any slowdown houses and therefore ﬁnishes the round at the same

time

as

group

1.

Group

2

ﬁnishes

the

round

1 9

before

group

1

since

the

manip-

ulator

spent

1 3

time

eating

a

house

with

them

whereas

he

spent

1 9

time

eating

a

house

with

agents

from

group

2.

This

results

in

a

4 9

−

3 9

=

1 9

delay

between

the

two.

The manipulator’s choice corresponds to an assignment of the variables in

the SAT formula. If the manipulator chose to eat house hrxr before hr¬xr then

this corresponds to setting xr to true (and vice versa). Thus, in each round

the manipulator choses an assignment for a variable in the formula. The target

utility

T

is

the

sum

of

4 9

of

the

utility

of

hrxr

and 118

of

the

utility

of

the

slowdown

house hrs (except in the last round) for each round r and an extra 2257 .

Lemma 7. In the clause round, the manipulator must eat the prize house before any other agent to reach the target utility T .

Lemma 8. F is satisﬁable iﬀ the manipulator can reach the target utility T .

Proof. (⇒) We have set T so that if the manipulator declared a prescribed

preference proﬁle, he will require an extra

25 27

−

ǫ

·

n

utility

to

reach

T.

If all

clauses are satisﬁed, at most 2 of the agents eating the houses corresponding to

a

clause

will

be

in

advance

and

the

manipulator

will

have

25 27

units

of

time

to

eat

the

prize

house

alone.

The

manipulator

will

always

have

8 9

units

of

time

to

eat

the prize house alone while the other literal agents are eating the corresponding

clause houses. In the worst case, 2 agents are in advance for any clause by 91 , units of time, which means that they, along with the third agent in the clause,

will

ﬁnish

their

triplet

of

clause

houses

after

8 9

+

1 27

units

of

time,

at

which

time

all three agents will begin eating the prize house. This leaves the manipulator

to

eat

alone

for

1 27

extra

time

thus

ensuring

him

extra

utility

≥

2257 .

(⇐) If the truth assignment causes a clause to be unsatisﬁed, the agents corre-

sponding to the negation of the literal in the clause (and therefore eating the

clause houses corresponding to the clause) will all be in advance and will ﬁnish

eating

the

clause

houses

before

the

manipulator

has

eaten

25 27

of

the

prize

house.

If

all

3

agents

are

in

advance,

they

will

ﬁnish

eating

the

clause

houses

24 27

units

16

of time after the manipulator has started eating the prize house. Therefore for

3 27

of

the

prize

house

there

are

at

least

3

extra

agents

eating

the

prize

house.

Since

this

makes

at

least

4

agents

eating

3 27

of

the

prize

house,

the

manipulator

will

get

at

most

1 36

instead

of

the

required

1 27

of

the

prize

house

after

he

has

eaten a share of 2247 . Since the prize house is the only remaining house with

non-negligible utility, and we have made α large enough, he cannot compensate

this loss of utility by getting more of some other house.

The reduction can be used to show that even checking whether there exists any report that yields more utility than the truthful report is NP-hard.

Example 4. We illustrate the reduction in the proof of Theorem 3. For the following SAT formula, the table below illustrates the preference proﬁle for the agents in the main part. Houses not shown in the preferences are never eaten by the agents and come later in the preference lists.

(x1 ∨ x2 ∨ x3) (¬x1 ∨ ¬x2 ∨ ¬x3) (x1 ∨ ¬x2 ∨ x3) (¬x1 ∨ x2 ∨ ¬x3)

c1

c2

c3

c4

choice round 1

choice round 2

choice round 3 clause round

1 h1x1 , h1¬x1

h1s h2x2 , h2¬x2

h2s h3x3 , h3¬x3

hprize

a1x1 h1x1 h2x1 h3x1 h1c2 , h2c2 , h3c2 hprize a2x1 h1x1 h2x1 h3x1 h1c4 , h2c4 , h3c4 hprize

a1¬x1 a2¬x1

h1¬x1 h1¬x1

h2¬x1 h2¬x1

h3¬x1 h3¬x1

h1c1 , h2c1 , h3c1 h1c3 , h2c3 , h3c3

hprize hprize

a1x2 h1x2 h2x2 h3x2 h1c2 , h2c2 , h3c2 hprize a2x2 h1x2 h2x2 h3x2 h1c3 , h2c3 , h3c3 hprize

a1¬x2 a2¬x2

h1¬x2 h1¬x2

h2¬x2 h2¬x2

h3¬x2 h3¬x2

h1c1 , h2c1 , h3c1 h1c4 , h2c4 , h3c4

hprize hprize

a1x3 h1x3 h2x3 h3x3 h1c2 , h2c2 , h3c2 hprize a2x3 h1x3 h2x3 h3x3 h1c4 , h2c4 , h3c4 hprize

a1¬x3 a2¬x3

h1¬x3 h1¬x3

h2¬x3 h2¬x3

h3¬x3 h3¬x3

h1c1 , h2c1 , h3c1 h1c3 , h2c3 , h3c3

hprize hprize

6. Conclusions
We conducted a detailed computational analysis of strategic aspects of the PS rule. Since PS performs better than RSD in terms of eﬃciency and envyfreeness, the only drawback it has in comparison with RSD is its manipulability. We have shown that although PS is manipulable, ﬁnding an optimal manipulation is a complex task for an agent even if he has complete knowledge about the preferences of other agents. There is scope to conduct detailed experiments on the pecentage of instances that are manipulable and the extent and eﬀects of manipulation. Initial experiments show that manipulation is often possible and more often decreases social welfare than increases it, though the overall eﬀect is small. As the number of houses relative to the number of agents grows, the opportunities to manipulate increase, maximizing around 99%. It will be interesting to extend our results to the extension of PS for indiﬀerences [14]. Finally, studying coalitional manipulations and a deeper analysis of Nash dynamics are other interesting directions.
17

Acknowledgments NICTA is funded by the Australian Government through the Department of
Communications and the Australian Research Council through the ICT Centre of Excellence Program. Serge Gaspers is the recipient of an Australian Research Council Discovery Early Career Researcher Award (project number DE120101761).
References
[1] H. Aziz and P. Stursberg. A generalization of probabilistic serial to randomized social choice. In C. E. Brodley and P. Stone, editors, Proceedings of the 28th AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 559–565. AAAI Press, 2014.
[2] H. Aziz, F. Brandt, and M. Brill. The computational complexity of random serial dictatorship. Economics Letters, 121(3):341–345, 2013.
[3] H. Aziz, F. Brandt, and P. Stursberg. On popular random assignments. In B. Vo¨cking, editor, Proceedings of the 6th International Symposium on Algorithmic Game Theory (SAGT), volume 8146 of Lecture Notes in Computer Science (LNCS), pages 183–194. Springer-Verlag, 2013.
[4] H. Aziz, S. Gaspers, S. Mackenzie, and T. Walsh. Fair assignment of indivisible objects under ordinal preferences. In Proceedings of the 13th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 1305–1312, 2014.
[5] A. Bogomolnaia and E. J. Heo. Probabilistic assignment of objects: Characterizing the serial rule. Journal of Economic Theory, 147:2072–2082, 2012.
[6] A. Bogomolnaia and H. Moulin. A new solution to the random assignment problem. Journal of Economic Theory, 100(2):295–328, 2001.
[7] S. Bouveret and J. Lang. Manipulating picking sequences. In Proceedings of the 5th International Workshop on Computational Social Choice (COMSOC), 2014.
[8] E. Budish, Y.-K. Che, F. Kojima, and P. Milgrom. Designing random allocation mechanisms: Theory and applications. American Economic Review, 103(2):585–623, 2013.
[9] W. J. Cho. Probabilistic assignment: A two-fold axiomatic approach. Unpublished manuscript, 2012.
[10] O. Ekici and O. Kesten. An equilibrium analysis of the probabilistic serial mechanism. Technical report, O¨ zyeg˘in University, Istanbul, May 2012.
18

[11] P. Faliszewski, E. Hemaspaandra, and L. Hemaspaandra. Using complexity to protect elections. Communications of the ACM, 53(11):74–82, 2010.
[12] P. G¨ardenfors. Assignment problem based on ordinal preferences. Management Science, 20:331–340, 1973.
[13] A. Hylland and R. Zeckhauser. The eﬃcient allocation of individuals to positions. The Journal of Political Economy, 87(2):293–314, 1979.
[14] A.-K. Katta and J. Sethuraman. A solution to the random assignment problem on the full preference domain. Journal of Economic Theory, 131 (1):231–250, 2006.
[15] D. A. Kohler and R. Chandrasekaran. A class of sequential games. Operations Research, 19(2):270–277, 1971.
[16] F. Kojima. Random assignment of multiple indivisible objects. Mathematical Social Sciences, 57(1):134—142, 2009.
[17] D. Saban and J. Sethuraman. A note on object allocation under lexicographic preferences. Journal of Mathematical Economics, 2013.
[18] L. J. Schulman and V. V. Vazirani. Allocation of divisible goods under lexicographic preferences. Technical Report arXiv:1206.4366, arXiv.org, 2012.
[19] O. Yilmaz. The probabilistic serial mechanism with private endowments. Games and Economic Behavior, 69(2):475–491, 2010.
19

