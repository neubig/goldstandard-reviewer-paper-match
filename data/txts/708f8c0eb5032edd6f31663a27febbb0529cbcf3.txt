Visually Grounded Neural Syntax Acquisition

Haoyue Shi‚Ä†,‚àó

Jiayuan Mao‚Ä°,‚àó

Kevin Gimpel‚Ä†

Karen Livescu‚Ä†

‚Ä†: Toyota Technological Institute at Chicago, IL, USA

‚Ä°: ITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University, China

{freda, kgimpel, klivescu}@ttic.edu, mjy14@mails.tsinghua.edu.cn

arXiv:1906.02890v2 [cs.CL] 24 Sep 2019

Abstract
We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We deÔ¨Åne the concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We Ô¨Ånd that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also Ô¨Ånd that the concreteness acquired by VG-NSL correlates well with a similar measure deÔ¨Åned by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.1
1 Introduction
We study the problem of visually grounded syntax acquisition. Consider the images in Figure 1, paired with the descriptive texts (captions) in English. Given no prior knowledge of English, and sufÔ¨Åcient such pairs, one can infer the correspondence between certain words and visual attributes, (e.g., recognizing that ‚Äúa cat‚Äù refers to the objects in the blue boxes). One can further extract constituents, by assuming that concrete spans of words should be processed as a whole, and thus form the
‚àóHS and JM contributed equally to the work. 1 Project page: https://ttic.uchicago.edu/ Àúfreda/project/vgnsl

A cat is on the ground.
A cat stands under an umbrella.
A dog sits under an umbrella.
Figure 1: We propose to use image-caption pairs to extract constituents from text, based on the assumption that similar spans should be matched to similar visual objects and these concrete spans form constituents.
constituents. Similarly, the same process can be applied to verb or prepositional phrases.
This intuition motivates the use of image-text pairs to facilitate automated language learning, including both syntax and semantics. In this paper we focus on learning syntactic structures, and propose the Visually Grounded Neural Syntax Learner (VG-NSL, shown in Figure 2). VG-NSL acquires syntax, in the form of constituency parsing, by looking at images and reading captions.
At a high level, VG-NSL builds latent constituency trees of word sequences and recursively composes representations for constituents. Next, it matches the visual and textual representations. The training procedure is built on the hypothesis that a better syntactic structure contributes to a better representation of constituents, which then leads to better alignment between vision and language. We use no human-labeled constituency trees or other syntactic labeling (such as part-of-speech tags). Instead, we deÔ¨Åne a concreteness score of constituents based on their matching with images, and use it to guide the parsing of sentences. At test time, no images paired with the text are needed.
We compare VG-NSL with prior approaches to unsupervised language learning, most of which

do not use visual grounding. Our Ô¨Årst Ô¨Ånding is that VG-NSL improves over the best previous approaches to unsupervised constituency parsing in terms of F1 scores against gold parse trees. We also Ô¨Ånd that many existing approaches are quite unstable with respect to the choice of random initialization, whereas VG-NSL exhibits consistent parsing results across multiple training runs. Third, we analyze the performance of different models on different types of constituents, and Ô¨Ånd that our model shows substantial improvement on noun phrases and prepositional phrases which are common in captions. Fourth, VG-NSL is much more data-efÔ¨Åcient than prior work based purely on text, achieving comparable performance to other approaches using only 20% of the training captions. In addition, the concreteness score, which emerges during the matching between constituents and images, correlates well with a similar measure deÔ¨Åned by linguists. Finally, VG-NSL can be easily extended to multiple languages, which we evaluate on the Multi30K data set (Elliott et al., 2016, 2017) consisting of German and French image captions.
2 Related Work
Linguistic structure induction from text. Recent work has proposed several approaches for inducing latent syntactic structures, including constituency trees (Choi et al., 2018; Yogatama et al., 2017; Maillard and Clark, 2018; Havrylov et al., 2019; Kim et al., 2019; Drozdov et al., 2019) and dependency trees (Shi et al., 2019), from the distant supervision of downstream tasks. However, most of the methods are not able to produce linguistically sound structures, or even consistent ones with Ô¨Åxed data and hyperparameters but different random initializations (Williams et al., 2018).
A related line of research is to induce latent syntactic structure via language modeling. This approach has achieved remarkable performance on unsupervised constituency parsing (Shen et al., 2018a, 2019), especially in identifying the boundaries of higher-level (i.e., larger) constituents. To our knowledge, the Parsing-Reading-Predict Network (PRPN; Shen et al., 2018a) and the Ordered Neuron LSTM (ON-LSTM; Shen et al., 2019) currently produce the best fully unsupervised constituency parsing results. One issue with PRPN, however, is that it tends to produce meaningless parses for lower-level (smaller) constituents (Phu Mon Htut et al., 2018).

Over the last two decades, there has been extensive study targeting unsupervised constituency parsing (Klein and Manning, 2002, 2004, 2005; Bod, 2006a,b; Ponvert et al., 2011) and dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2010; Han et al., 2017). However, all of these approaches are based on linguistic annotations. SpeciÔ¨Åcally, they operate on the part-of-speech tags of words instead of word tokens. One exception is Spitkovsky et al. (2011), which produces dependency parse trees based on automatically induced pseudo tags.
In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplementary material.
There has been some prior work on improving unsupervised parsing by leveraging extra signals, such as parallel text (Snyder et al., 2009), annotated data in another language with parallel text (Ganchev et al., 2009), annotated data in other languages without parallel text (Cohen et al., 2011), or non-parallel text from multiple languages (Cohen and Smith, 2009). We leave the integration of other grounding signals as future work.
Grounded language acquisition. Grounded language acquisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures with no humandeÔ¨Åned labels or rules.
Meanwhile, learning visual-semantic representations in a joint embedding space (Ngiam et al., 2011) is a widely studied approach, and has achieved remarkable results on image-caption retrieval (Kiros et al., 2014; Faghri et al., 2018; Shi et al., 2018a), image caption generation (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Ma et al., 2015), and visual question answering (Malinowski et al., 2015). In this work, we borrow this idea to match visual and textual representations.
Concreteness estimation. Turney et al. (2011) deÔ¨Åne concrete words as those referring to things, events, and properties that we can perceive directly with our senses. Subsequent work has studied

A cat is on the ground Caption

Structure and Representation Inference
(Score-Sample-Combine)

Constituency Parse Tree Embeddings of Constituents

Image

Image Encoder

ùíó(ùëñ)
Image Embedding

(ùëñ)

ùíÑ3(ùëñ)

ùíÑ1

(ùëñ)

ùíÑ2

ùíó(ùëñ)

Visual-Semantic Embeddings

Figure 2: VG-NSL consists of two modules: a textual module for inferring structures and representations for captions, and a visual-semantic module for matching constituents with images. VG-NSL induces constituency parse trees of captions by looking at images and reading paired captions.

word-level concreteness estimation based on text (Turney et al., 2011; Hill et al., 2013), human judgments (Silberer and Lapata, 2012; Hill and Korhonen, 2014a; Brysbaert et al., 2014), and multimodal data (Hill and Korhonen, 2014b; Hill et al., 2014; Kiela et al., 2014; Young et al., 2014; Hessel et al., 2018; Silberer et al., 2017; Bhaskar et al., 2017). As with Hessel et al. (2018) and Kiela et al. (2014), our model uses multi-modal data to estimate concreteness. Compared with them, we deÔ¨Åne concreteness for spans instead of words, and use it to induce linguistic structures.
3 Visually Grounded Neural Syntax Learner
Given a set of paired images and captions, our goal is to learn representations and structures for words and constituents. Toward this goal, we propose the Visually Grounded Neural Syntax Learner (VGNSL), an approach for the grounded acquisition of syntax of natural language. VG-NSL is inspired by the idea of semantic bootstrapping (Pinker, 1984), which suggests that children acquire syntax by Ô¨Årst understanding the meaning of words and phrases, and linking them with the syntax of words.
At a high level (Figure 2), VG-NSL consists of 2 modules. First, given an input caption (i.e., a sentence or a smaller constituent), as a sequence of tokens, VG-NSL builds a latent constituency parse tree, and recursively composes representations for every constituent. Next, it matches textual representations with visual inputs, such as the paired image with the constituents. Both modules are jointly optimized from natural supervision: the model acquires constituency structures, composes textual representations, and links them with visual scenes, by looking at images and reading paired captions.

3.1 Textual Representations and Structures
VG-NSL starts by composing a binary constituency structure of text, using an easy-Ô¨Årst bottom-up parser (Goldberg and Elhadad, 2010). The composition of the tree from a caption of length n consists of n ‚àí 1 steps. Let X(t) = (x(1t), x(2t), ¬∑ ¬∑ ¬∑ , x(kt)) denote the textual representations of a sequence of constituents after step t, where k = n ‚àí t. For simplicity, we use X(0) to denote the word embeddings for all tokens (the initial representations).
At step t, a score function score(¬∑; Œò), parameterized by Œò, is evaluated on all pairs of consecutive constituents, resulting in a vector score(X(t‚àí1); Œò) of length n ‚àí t:
score(X(t‚àí1); Œò)j
score x(jt‚àí1), x(jt+‚àí11) ; Œò .

We implement score(¬∑; Œò) as a two-layer feedforward network.
A pair of constituents x(jt‚àó‚àí1), x(jt‚àó‚àí+11) is sampled from all pairs of consecutive constituents, with respect to the distribution produced by a softmax:2

Pr [j‚àó] =

exp score X(t‚àí1); Œò j‚àó .
j exp score X(t‚àí1); Œò j

The selected pair is combined to form a single new constituent. Thus, after step t, the number of constituents is decreased by 1. The textual representation for the new constituent is deÔ¨Åned as the L2normed sum of the two component constituents:

combine x(jt‚àó‚àí1), xj(t‚àó‚àí+11)

x(jt‚àó‚àí1) + x(jt‚àó‚àí+11) . x(jt‚àó‚àí1) + x(jt‚àó‚àí+11) 2

2 At test time, we take the argmax.

((a cat) (is (on (the ground))))

Step #5:

1.0
(a cat) (is (on (the ground)))

Step #4:

0.35

0.65

(a cat) is (on (the ground))

Step #3:

0.25 0.15 0.6
(a cat) is on (the ground)

Step #2:

0.25 0.15 0.15 0.45
(a cat) is on the ground

Step #1: 0.4 0.1 0.1 0.1 0.3
a cat is on the ground
Figure 3: An illustration of how VG-NSL composes a constituency parse tree. At each step, the score function score is evaluated on all pairs of consecutive constituents (dashed lines). Next, a pair of constituents is sampled from all pairs w.r.t. a distribution computed by the softmax of all predicted scores. The selected pair of constituents is combined into a larger one, while the other constituents remain unchanged (solid lines).

We Ô¨Ånd that using a more complex encoder for constituents, such as GRUs, will cause the representations to be highly biased towards a few salient words in the sentence (e.g., the encoder encodes only the word ‚Äúcat‚Äù while ignoring the rest part of the caption; Shi et al., 2018a; Wu et al., 2019). This signiÔ¨Åcantly degrades the performance of linguistic structure induction.
We repeat this score-sample-combine process for n ‚àí 1 steps, until all words in the input text have been combined into a single constituent (Figure 3). This ends the inference of the constituency parse tree. Since at each time step we combine two consecutive constituents, the derived tree t contains 2n ‚àí 1 constituents (including all words).

3.2 Visual-Semantic Embeddings
We follow an approach similar to that of Kiros et al. (2014) to deÔ¨Åne the visual-semantic embedding (VSE) space for paired images and text constituents. Let v(i) denote the vector representation of an image i, and c(ji) denote the vector representation of the j-th constituent of its corresponding text caption. During the matching with images, we ignore the tree structure and index them as a list of constituents. A function m(¬∑, ¬∑; Œ¶) is deÔ¨Åned as the

matching score between images and texts:

m(v(i), c(ji); Œ¶) cos(Œ¶v, c),

where the parameter vector Œ¶ aligns the visual and textual representations into a joint space.

3.3 Training
We optimize the visual-semantic representations (Œ¶) and constituency structures (Œò) in an alternating approach. At each iteration, given constituency parsing results of caption, Œ¶ is optimized for matching the visual and the textual representations. Next, given the visual grounding of constituents, Œò is optimized for producing constituents that can be better matched with images. SpeciÔ¨Åcally, we optimize textual representations and the visual-semantic embedding space using a hinge-based triplet ranking loss:

L(Œ¶; V, C) =

i,k=i,j,

m(c(k), v(i)) ‚àí m(c(i), v(i)) + Œ¥

j

+

+

m(c(i), v(k)) ‚àí m(c(i), v(i)) + Œ¥ ,

j

j

+

i,k=i,j

where i and k index over all image-caption pairs in the data set, while j and enumerate all constituents of a speciÔ¨Åc caption (c(i) and c(k), respectively), V = {v(i)} is the set of image representations, C = {c(ji)} is the set of textual representations of all constituents, and Œ¥ is a constant margin, [¬∑]+ denotes max(0, ¬∑). The loss L extends the loss for image-caption retrieval of Kiros et al. (2014), by introducing the alignments between images and sub-sentence constituents.
We optimize textual structures via distant supervision: they are optimized for a better alignment between the derived constituents and the images. Intuitively, the following objective encourages adjectives to be associated (combined) with the corresponding nouns, and verbs/prepositions to be associated (combined) with the corresponding subjects and objects. SpeciÔ¨Åcally, we use REINFORCE (Williams, 1992) as the gradient estimator for Œò. Consider the parsing process of a speciÔ¨Åc caption c(i), and denote the corresponding image embedding v(i). For a constituent z of c(i), we deÔ¨Åne its

(visual) concreteness concrete(z, v(i)) as:

concrete(z, v(i)) = m(z, v(i)) ‚àí m(c(pk), v(i)) ‚àí Œ¥
+ k=i,p
+ m(z, v(i)) ‚àí m(z, v(k)) ‚àí Œ¥ , (1)
+ k=i

where Œ¥ is a Ô¨Åxed margin. At step t, we deÔ¨Åne
the reward function for a combination of a pair of constituents (x(jt‚àí1), x(jt+‚àí11)) as:

r(xj(t‚àí1), x(jt+‚àí11)) = concrete(z, v(i)), (2)

where z

combine(x

(t‚àí1) j

,

x

(t‚àí1) j+1

).

In plain

words, at each step, we encourage the model to

compose a constituent that maximizes the align-

ment between the new constituent and the corre-

sponding image. During training, we sample con-

stituency parse trees of captions, and reinforce each

composition step using Equation 2. During test, no

paired images of text are needed.

3.4 The Head-Initial Inductive Bias
English and many other Indo-European languages are usually head-initial (Baker, 2001). For example, in verb phrases or prepositional phrases, the verb (or the preposition) precedes the complements (e.g., the object of the verb). Consider the simple caption a white cat on the lawn. While the association of the adjective (white) could be induced from the visual grounding of phrases, whether the preposition (on) should be associated with a white cat or the lawn is more challenging to induce. Thus, we impose an inductive bias to guide the learner to correctly associate prepositions with their complements, determiners with corresponding noun phrases, and complementizers with the corresponding relative clauses. SpeciÔ¨Åcally, we discourage abstract constituents (i.e., constituents that cannot be grounded in the image) from being combined with a preceding constituent, by modifying the original reward deÔ¨Ånition (Equation 2) as:

r (x(jt‚àí1),x(jt+‚àí11))

= r(x(jt‚àí1), x(jt+‚àí11))

(3) ,

Œª ¬∑ abstract(x(jt+‚àí11), v(i)) + 1

where Œª is a scalar hyperparameter, v(i) is the image embedding corresponding to the caption being parsed, and abstract denotes the abstractness

of the span, deÔ¨Åned analogously to concreteness (Equation 1):
abstract(z, v(i)) =
m(c(pk), v(i)) ‚àí m(z, v(i)) + Œ¥
+ k=i,p
+ m(z, v(k)) ‚àí m(z, v(i)) + Œ¥ ,
+ k=i
The intuition here is that the initial heads for prepositional phrases (e.g., on) and relative clauses (e.g., which, where) are usually abstract words. During training, we encourage the model to associate these abstract words with the succeeding constituents instead of the preceding ones. It is worth noting that such an inductive bias is languagespeciÔ¨Åc, and cannot be applied to head-Ô¨Ånal languages such as Japanese (Baker, 2001). We leave the design of head-directionality inductive biases for other languages as future work.
4 Experiments
We evaluate VG-NSL for unsupervised parsing in a few ways: F1 score with gold trees, selfconsistency across different choices of random initialization, performance on different types of constituents, and data efÔ¨Åciency. In addition, we Ô¨Ånd that the concreteness score acquired by VG-NSL is consistent with a similar measure deÔ¨Åned by linguists. We focus on English for the main experiments, but also extend to German and French.
4.1 Data Sets and Metrics
We use the standard split of the MSCOCO data set (Lin et al., 2014), following Karpathy and FeiFei (2015). It contains 82,783 images for training, 1,000 for development, and another 1,000 for testing. Each image is associated with 5 captions.
For the evaluation of constituency parsing, the Penn Treebank (PTB; Marcus et al., 1993) is a widely used, manually annotated data set. However, PTB consists of sentences from abstract domains, e.g., the Wall Street Journal (WSJ), which are not visually grounded and whose linguistic structures can hardly be induced by VG-NSL. Here we evaluate models on the MSCOCO test set, which is well-matched to the training domain; we leave the extension of our work to more abstract domains to future work. We apply Benepar (Kitaev and Klein, 2018),3 an off-the-shelf constituency parser
3 https://pypi.org/project/benepar

with state-of-the-art performance (95.52 F1 score) on the WSJ test set,4 to parse the captions in the MSCOCO test set as gold constituency parse trees. We evaluate all of the investigated models using the F1 score compared to these gold parse trees.5
4.2 Baselines
We compare VG-NSL with various baselines for unsupervised tree structure modeling of texts. We can categorize the baselines by their training objective or supervision.
Trivial tree structures. Similarly to recent work on latent tree structures (Williams et al., 2018; Phu Mon Htut et al., 2018; Shi et al., 2018b), we include three types of trivial baselines without linguistic information: random binary trees, left-branching binary trees, and right-branching binary trees.
Syntax acquisition by language modeling and statistics. Shen et al. (2018a) proposes the Parsing-Reading-Predict Network (PRPN), which predicts syntactic distances (Shen et al., 2018b) between adjacent words, and composes a binary tree based on the syntactic distances to improve language modeling. The learned distances can be mapped into a binary constituency parse tree, by recursively splitting the sentence between the two consecutive words with the largest syntactic distance.
Ordered neurons (ON-LSTM; Shen et al., 2019) is a recurrent unit based on the LSTM cell (Hochreiter and Schmidhuber, 1997) that explicitly regularizes different neurons in a cell to represent shortterm or long-term information. After being trained on the language modeling task, Shen et al. (2019) suggest that the gate values in ON-LSTM cells can be viewed as syntactic distances (Shen et al., 2018b) between adjacent words to induce latent tree structures. ON-LSTM has the state-of-the-art unsupervised constituency parsing performance on the WSJ test set. We train both PRPN and ONLSTM on all captions in the MSCOCO training set and use the models as baselines.
Inspired by the syntactic distance‚Äìbased approaches (Shen et al., 2018a, 2019), we also introduce another baseline, PMI, which uses negative
4 We also manually label the constituency parse trees for 50 captions randomly sampled from the MSCOCO test split, where Benepar has an F1 score of 95.65 with the manual labels. Details can be found in the supplementary material.
5 Following convention (Sekine and Collins, 1997), we report the F1 score across all constituents in the data set, instead of the average of sentence-level F1 scores.

pointwise mutual information (Church and Hanks, 1990) between adjacent words as the syntactic distance. We compose constituency parse trees based on the distances in the same way as PRPN and ON-LSTM.
Syntax acquisition from downstream tasks. Choi et al. (2018) propose to compose binary constituency parse trees directly from downstream tasks using the Gumbel softmax trick (Jang et al., 2017). We integrate a Gumbel tree-based caption encoder into the visual semantic embedding approach (Kiros et al., 2014). The model is trained on the downstream task of image-caption retrieval.
Syntax acquisition from concreteness estimation. Since we apply concreteness information to train VG-NSL, it is worth comparing against unsupervised constituency parsing based on previous approaches for predicting word concreteness. This set of baselines includes semi-supervised estimation (Turney et al., 2011), crowdsourced labeling (Brysbaert et al., 2014), and multimodal estimation (Hessel et al., 2018). Note that none of these approaches has been applied to unsupervised constituency parsing. Implementation details can be found in the supplementary material.
Based on the concreteness score of words, we introduce another baseline similar to VG-NSL. SpeciÔ¨Åcally, we recursively combine two consecutive constituents with the largest average concreteness, and use the average concreteness as the score for the composed constituent. The algorithm generates binary constituency parse trees of captions. For a fair comparison, we implement a variant of this algorithm that also uses a head-initial inductive bias and include the details in the appendix.
4.3 Implementation Details
Across all experiments and all models (including baselines such as PRPN, ON-LSTM, and Gumbel), the embedding dimension for words and constituents is 512. For VG-NSL, we use a pre-trained ResNet-101 (He et al., 2016), trained on ImageNet (Russakovsky et al., 2015), to extract vector embeddings for images. Thus, Œ¶ is a mapping from a 2048-D image embedding space to a 512-D visualsemantic embedding space. As for the score function in constituency parsing, we use a hidden dimension of 128 and ReLU activation. All VG-NSL models are trained for 30 epochs. We use an Adam optimizer (Kingma and Ba, 2015) with initial learning rate 5 √ó 10‚àí4 to train VG-NSL. The learning

Model

NP

VP

PP

ADJP

Random Left Right PMI PRPN (Shen et al., 2018a) ON-LSTM (Shen et al., 2019) Gumbel (Choi et al., 2018)‚Ä†

47.3 ¬±0.3 51.4 32.2 54.2 72.8 ¬±9.7 74.4 ¬±7.1 50.4 ¬±0.3

10.5 ¬±0.4 1.8
23.4 16.0 33.0 ¬±9.1 11.8 ¬±5.6 8.7 ¬±0.3

17.3 ¬±0.7 0.2
18.7 14.3 61.6 ¬±9.9 41.3 ¬±16.4 15.5 ¬±0.0

33.5 ¬±0.8 16.0 14.4 39.2 35.4 ¬±4.3 44.0 ¬±14.0 34.8 ¬±1.6

VG-NSL (ours)‚Ä† VG-NSL+HI (ours)‚Ä† VG-NSL+HI+FastText (ours)*‚Ä†

79.6 ¬±0.4 74.6 ¬±0.5 78.8 ¬±0.5

26.2 ¬±0.4 32.5 ¬±1.5 24.4 ¬±0.9

42.0 ¬±0.6 66.5 ¬±1.2 65.6 ¬±1.1

22.0 ¬±0.4 21.7 ¬±1.1 22.0 ¬±0.7

Concreteness estimation‚Äìbased models

Turney et al. (2011)*

65.5

30.8

35.3

30.4

Turney et al. (2011)+HI*

74.5

26.2

47.6

25.6

Brysbaert et al. (2014)*

54.1

27.8

27.0

33.1

Brysbaert et al. (2014)+HI* 73.4

23.9

50.0

26.1

Hessel et al. (2018)‚Ä†

50.9

21.7

32.8

27.5

Hessel et al. (2018)+HI‚Ä†

72.5

34.4

65.8

26.2

Avg. F1
27.1 ¬±0.2 23.3 22.9 30.5 52.5 ¬±2.6 45.5 ¬±3.3 27.9 ¬±0.2
50.4 ¬±0.3 53.3 ¬±0.2 54.4 ¬±0.4

Self F1
32.4 N/A N/A N/A 60.3 69.3 40.1
87.1 90.2 89.8

42.5

N/A

48.9

N/A

34.1

N/A

47.9

N/A

33.2

N/A

52.9

N/A

Table 1: Recall of speciÔ¨Åc typed phrases, and overall F1 score, evaluated on the MSCOCO test split, averaged over 5 runs with different random initializations. We also include self-agreement F1 score (Williams et al., 2018) across the 5 runs. ¬± denotes standard deviation. * denotes models requiring extra labels and/or corpus, and ‚Ä† denotes models requiring a pre-trained visual feature extractor. We highlight the best number in each column among all models that do not require extra data other than paired image-caption data, as well as the overall best number. The Left, Right, PMI, and concreteness estimation‚Äìbased models have no standard deviation or self F1 (shown as N/A) as they are deterministic given the training and/or testing data.

rate is re-initialized to 5 √ó 10‚àí5 after 15 epochs. We tune other hyperparameters of VG-NSL on the development set using the self-agreement F1 score (Williams et al., 2018) over 5 runs with different choices of random initialization.
4.4 Results: Unsupervised Constituency Parsing
We evaluate the induced constituency parse trees via the overall F1 score, as well as the recall of four types of constituents: noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and adjective phrases (ADJP) (Table 1). We also evaluate the robustness of models trained with Ô¨Åxed data and hyperparameters, but different random initialization, in two ways: via the standard deviation of performance across multiple runs, and via the selfagreement F1 score (Williams et al., 2018), which is the average F1 taken over pairs of different runs.
Among all of the models which do not require extra labels, VG-NSL with the head-initial inductive bias (VG-NSL+HI) achieves the best F1 score. PRPN (Shen et al., 2018a) and a concreteness estimation-based baseline (Hessel et al., 2018) both

produce competitive results. It is worth noting that the PRPN baseline reaches this performance without any information from images. However, the performance of PRPN is less stable than that of VG-NSL across random initializations. In contrast to its state-of-the-art performance on the WSJ full set (Shen et al., 2019), we observe that ON-LSTM does not perform well on the MSCOCO caption data set. However, it remains the best model for adjective phrases, which is consistent with the result reported by Shen et al. (2019).
In addition to the best overall F1 scores, VGNSL+HI achieves competitive scores across most phrase types (NP, VP and PP). Our models (VGNSL and VG-NSL+HI) perform the best on NP and PP, which are the most common visually grounded phrases in the MSCOCO data set. In addition, our models produce much higher self F1 than the baselines (Shen et al., 2018a, 2019; Choi et al., 2018), showing that they reliably produce reasonable constituency parse trees with different initialization.
We also test the effectiveness of using pretrained word embeddings. SpeciÔ¨Åcally, for VGNSL+HI+FastText, we use a pre-trained FastText

F1 with gold trees

50

45

40

PRPN

35

ON-LSTM

VG-NSL

30

VG-NSL+HI

0

20

40

60

80

100

Percentage (%)

(a) The percent data-F1 curves.

90

80

self F1

70

60

50

PRPN

VG-NSL

ON-LSTM

VG-NSL+HI

40 0

20

40

60

80

100

Percentage (%)

(b) The percent data-self F1 curves.

Figure 4: F1 score and self F1 score with respect to the amount of training data. All numbers are averaged over 5 runs with different random initialization.

embedding (300-D, Joulin et al., 2016), concatenated with a 212-D trainable embedding, as the word embedding. Using pre-trained word embeddings further improves performance to an average F1 of 54.4% while keeping a comparable self F1.

4.5 Results: Data EfÔ¨Åciency
We compare the data efÔ¨Åciency for PRPN (the strongest baseline method), ON-LSTM, VG-NSL, and VG-NSL+HI. We train the models using 1%, 2%, 5%, 10%, 20%, 50% and 100% of the MSCOCO training set, and report the overall F1 and self F1 scores on the test set (Figure 4).
Compared to PRPN trained on the full training set, VG-NSL and VG-NSL+HI reach comparable performance using only 20% of the data (i.e., 8K images with 40K captions). VG-NSL tends to quickly become more stable (in terms of the self F1 score) as the amount of data increases, while PRPN and ON-LSTM remain less stable.

4.6 Analysis: Consistency with Linguistic Concreteness
During training, VG-NSL acquires concreteness estimates for constituents via Equation 1. Here, we evaluate the consistency between word-level concreteness estimates induced by VG-NSL and those produced by other methods (Turney et al., 2011; Brysbaert et al., 2014; Hessel et al., 2018). SpeciÔ¨Åcally, we measure the correlation between the con-

Model/method

VG-NSL (+HI)

Turney et al. (2011)

0.74 0.72

Brysbaert et al. (2014) 0.71 0.71

Hessel et al. (2018)

0.84 0.85

Table 2: Agreement between our concreteness estimates and existing models or labels, evaluated via the Pearson correlation coefÔ¨Åcient computed over the most frequent 100 words in the MSCOCO test set, averaged over 5 runs with different random initialization.

Model
VG-NSL VG-NSL
VG-NSL+HI VG-NSL+HI

Criterion
Self F1 R@1
Self F1 R@1

Avg. F1
50.4 ¬±0.3 47.7 ¬±0.6
53.3 ¬±0.2 53.1 ¬±0.2

Self F1
87.1 83.4
90.2 88.7

Table 3: Average F1 scores and Self F1 scores of VGNSL and VG-NSL+HI with different model selection methods. R@1 denotes using recall at 1 (Kiros et al., 2014) as the model selection criterion. All hyperparameters are tuned with respect to self-agreement F1 score. The numbers are comparable to those in Table 1.

creteness estimated by VG-NSL on MSCOCO test set and existing linguistic concreteness deÔ¨Ånitions (Table 2). For any word, of which the representation is z, we estimate its concreteness by taking the average of concrete(z, v(i)), across all associated images v(i). The high correlation between VG-NSL and the concreteness scores produced by Turney et al. (2011) and Brysbaert et al. (2014) supports the argument that the linguistic concept of concreteness can be acquired in an unsupervised way. Our model also achieves a high correlation with Hessel et al. (2018), which also estimates word concreteness based on visual-domain information.

4.7 Analysis: Self-Agreement F1 Score as the Criterion for Model Selection
We introduce a novel hyperparameter tuning and model selection method based on the selfagreement F1 score.
Let M(Hi,j) denote the j-th checkpoint of the ith model trained with hyperparameters H, where M(Hi1,¬∑) and M(Hi2,¬∑) differ in their random initialization. The hyperparameters H are tuned to maximize:
max F1 M(Hi,ji), M(Hk,jk) ,
1‚â§i<k‚â§N |ji‚àíjk|<Œ¥

where F1(¬∑, ¬∑) denotes the F1 score between the trees generated by two models, N the number of

Model

EN

DE

FR

PRPN ON-LSTM VG-NSL VG-NSL+HI

30.8 ¬±17.9 38.7 ¬±12.7 33.5 ¬±0.2 38.7 ¬±0.2

31.5 ¬±8.9 34.9 ¬±12.3 36.3 ¬±0.2 38.3 ¬±0.2

27.5 ¬±7.0 27.7 ¬±5.6 34.3 ¬±0.6 38.1 ¬±0.6

Table 4: F1 scores on the Multi30K test split (Young et al., 2014; Elliott et al., 2016, 2017), averaged over 5 runs with different random initialization. ¬± denotes the standard deviation.

different runs, and Œ¥ the margin to ensure only nearby checkpoints are compared.6
After Ô¨Ånding the best hyperparameters H0, we train the model for another N times with different
random initialization, and select the best models by

arg max

F1 M(Hi,ji), M(Hk,jk) .

{j }N=1 1‚â§i<k‚â§N

0

0

We compare the performance of VG-NSL selected by the self F1 score and that selected by recall at 1 in image-to-text retrieval (R@1 in Table 3; Kiros et al., 2014). As a model selection criterion, self F1 consistently outperforms R@1 (avg. F1: 50.4 vs. 47.7 and 53.3 vs. 53.1 for VG-NSL and VG-NSL+HI, respectively). Meanwhile, it is worth noting that even if we select VG-NSL by R@1, it shows better stability compared with PRPN and ON-LSTM (Table 1), in terms of the score variance across different random initialization and self F1. SpeciÔ¨Åcally, the variance of avg. F1 is always less than 0.6 while the self F1 is greater than 80.
Note that the PRPN and ON-LSTM models are not tuned using self F1, since these models are usually trained for hundreds or thousands of epochs and thus it is computationally expensive to evaluate self F1. We leave the efÔ¨Åcient tuning of these baselines by self F1 as a future work.

4.8 Extension to Multiple Languages
We extend our experiments to the Multi30K data set, which is built on the Flickr30K data set (Young et al., 2014) and consists of English, German (Elliott et al., 2016), and French (Elliott et al., 2017) captions. For Multi30K, there are 29,000 images in the training set, 1,014 in the development set and 1,000 in the test set. Each image is associated with one caption in each language.
We compare our models to PRPN and ONLSTM in terms of overall F1 score (Table 4). VGNSL with the head-initial inductive bias consis-
6 In all of our experiments, N = 5, Œ¥ = 2.

tently performs the best across the three languages, all of which are highly head-initial (Baker, 2001). Note that the F1 scores here are not comparable to those in Table 1, since Multi30K (English) has 13x fewer captions than MSCOCO.
5 Discussion
We have proposed a simple but effective model, the Visually Grounded Neural Syntax Learner, for visually grounded language structure acquisition. VG-NSL jointly learns parse trees and visually grounded textual representations. In our experiments, we Ô¨Ånd that this approach to grounded language learning produces parsing models that are both accurate and stable, and that the learning is much more data-efÔ¨Åcient than a state-of-the-art text-only approach. Along the way, the model acquires estimates of word concreteness.
The results suggest multiple future research directions. First, VG-NSL matches text embeddings directly with embeddings of entire images. Its performance may be boosted by considering structured representations of both images (e.g., Lu et al., 2016; Wu et al., 2019) and texts (Steedman, 2000). Second, thus far we have used a shared representation for both syntax and semantics, but it may be useful to disentangle their representations (Steedman, 2000). Third, our best model is based on the head-initial inductive bias. Automatically acquiring such inductive biases from data remains challenging (Kemp et al., 2006; Gauthier et al., 2018). Finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (Christie et al., 2016b), coreference resolution (Kottur et al., 2018), and learning pragmatics beyond semantics (Andreas and Klein, 2016).
There are also limitations to the idea of grounded language acquisition. In particular, the current approach has thus far been applied to understanding grounded texts in a single domain (static visual scenes for VG-NSL). Its applicability could be extended by learning shared representations across multiple modalities (Castrejon et al., 2016) or integrating with pure text-domain models (such as PRPN, Shen et al., 2018a).
Acknowledgement
We thank Allyson Ettinger for helpful suggestions on this work, and the anonymous reviewers for their valuable feedback.

References
Jacob Andreas and Dan Klein. 2016. Reasoning about pragmatics with neural listeners and speakers. In Proc. of EMNLP.
Mark C. Baker. 2001. The Atoms of Language: The Mind‚Äôs Hidden Rules of Grammar. Basic books.
Sai Abishek Bhaskar, Maximilian Ko¬®per, Sabine Schulte Im Walde, and Diego Frassinelli. 2017. Exploring multi-modal text+image models to distinguish between abstract and concrete nouns. In Proc. of the IWCS workshop on Foundations of Situated and Multimodal Communication.
Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for treebank II style Penn treebank project. University of Pennsylvania, 97.
Rens Bod. 2006a. An all-subtrees approach to unsupervised parsing. In Proc. of COLING-ACL.
Rens Bod. 2006b. Unsupervised parsing with U-DOP. In Proc. of CoNLL.
Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman. 2014. Concreteness ratings for 40 thousand generally known English word lemmas. Behav. Res. Methods, 46(3):904‚Äì911.
Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. 2016. Learning aligned cross-modal representations from weakly aligned data. In Proc. of CVPR.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciÔ¨Åc tree structures. In Proc. of AAAI.
Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, and Dhruv Batra. 2016a. Resolving language and vision ambiguities together: Joint segmentation & prepositional attachment resolution in captioned scenes. In Proc. of EMNLP.
Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, and Dhruv Batra. 2016b. Resolving language and vision ambiguities together: Joint segmentation & prepositional attachment resolution in captioned scenes. In Proc. of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22‚Äì29.
Shay Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of NAACLHLT.

Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011. Unsupervised structure prediction with nonparallel multilingual guidance. In Proc. of EMNLP.
Max Coltheart. 1981. The MRC psycholinguistic database. Q. J. Exp. Psychol., 33(4):497‚Äì505.
Andrew Drozdov, Pat Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. 2019. Unsupervised latent tree induction with deep inside-outside recursive autoencoders. In Proc. of NAACL-HLT.
Desmond Elliott, Stella Frank, Lo¬®ƒ±c Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proc. of WMT.
Desmond Elliott, Stella Frank, Khalil Sima‚Äôan, and Lucia Specia. 2016. Multi30K: Multilingual EnglishGerman image descriptions. In Proc. of the 5th Workshop on Vision and Language.
Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. VSE++: Improving visualsemantic embeddings with hard negatives. In Proc. of BMVC.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proc. of ACLIJCNLP.
Jon Gauthier, Roger Levy, and Joshua B. Tenenbaum. 2018. Word learning and the acquisition of syntactic‚Äìsemantic overhypotheses. In Proc. of CogSci.
Yoav Goldberg and Michael Elhadad. 2010. An efÔ¨Åcient algorithm for easy-Ô¨Årst non-directional dependency parsing. In Proc. of NAACL-HLT.
Wenjuan Han, Yong Jiang, and Kewei Tu. 2017. Dependency grammar induction with neural lexicalization and big training data. In Proc. of EMNLP.
Serhii Havrylov, Germa¬¥n Kruszewski, and Armand Joulin. 2019. Cooperative learning of disjoint syntax and semantics. In Proc. of NAACL-HLT.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proc. of CVPR.
Jack Hessel, David Mimno, and Lillian Lee. 2018. Quantifying the visual concreteness of words and topics in multimodal datasets. In Proc. of NAACLHLT.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013. Concreteness and corpora: A theoretical and practical study. In Proc. of CMCL.
Felix Hill and Anna Korhonen. 2014a. Concreteness and subjectivity as dimensions of lexical meaning. In Proc. of ACL.

Felix Hill and Anna Korhonen. 2014b. Learning abstract concept embeddings from multi-modal data: Since you probably cant see what i mean. In Proc. of EMNLP.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Multi-modal models for concrete and abstract concept meaning. TACL, 2(1):285‚Äì296.
Sepp Hochreiter and Ju¬®rgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735‚Äì 1780.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with Gumbel-softmax. In Proc. of ICLR.
Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity‚Äîa measure of the difÔ¨Åculty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63‚Äì S63.
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, He¬¥rve Je¬¥gou, and Tomas Mikolov. 2016. FastText.zip: Compressing text classiÔ¨Åcation models. arXiv preprint arXiv:1612.03651.
Andrej Karpathy and Li Fei-Fei. 2015. Deep visualsemantic alignments for generating image descriptions. In Proc. of CVPR.
Charles K. Kemp, Amy Perfors, and Joshua B. Tenenbaum. 2006. Learning overhypotheses. In Proc. of CogSci.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proc. of ACL.
Yoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Ga¬¥bor Melis. 2019. Unsupervised recurrent neural network grammars. In Proc. of NAACL-HLT.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proc. of ICLR.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. arXiv:1411.2539.
Nikita Kitaev and Dan Klein. 2018. Constituency parsing with a self-attentive encoder. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.

Dan Klein and Christopher D. Manning. 2005. Natural language grammar induction with a generative constituent-context model. Pattern Recognition, 38(9):1407‚Äì1419.
Satwik Kottur, Jose¬¥ MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2018. Visual coreference resolution in visual dialog using neural module networks. In Proc. of ECCV.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla¬¥r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In Proc. of ECCV.
Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. 2016. Visual relationship detection with language priors. In Proc. of ECCV.
Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li. 2015. Multimodal convolutional neural networks for matching image and sentence. In Proc. of CVPR.
Jean Maillard and Stephen Clark. 2018. Latent tree learning with differentiable parsers: Shift-reduce rarsing and chart parsing. In Proc. of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP.
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015. Ask your neurons: A neural-based approach to answering questions about images. In Proc. of ICCV.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. 2019. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In Proc. of ICLR.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Comput. Linguist., 19(2).
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng. 2011. Multimodal deep learning. In Proc. of ICML.
Phu Mon Htut, Kyunghyun Cho, and Samuel R. Bowman. 2018. Grammar induction with neural language models: An unsual replication. In Proc. of EMNLP.
Steven Pinker. 1984. Language Learnability and Language Development. Cambridge University Press.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw rext with cascaded Ô¨Ånite state models. In Proc. of ACL.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet large scale visual recognition challenge. IJCV, 115(3):211‚Äì252.

Satoshi Sekine and Michael Collins. 1997. Evalb bracket scoring program.
Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. 2018a. Neural language modeling by jointly learning syntax and lexicon. In Proc. of ICLR.
Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, and Yoshua Bengio. 2018b. Straight to the tree: Constituency parsing with neural syntactic distance. In Proc. of ACL.
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2019. Ordered neurons: Integrating tree structures into recurrent neural networks. In Proc. of ICLR.
Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, and Jian Sun. 2018a. Learning visually-grounded semantics from contrastive adversarial samples. In Proc. of COLING.
Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. 2018b. On tree-based neural sentence modeling. In Proc. of EMNLP.
Jiaxin Shi, Lei Hou, Juanzi Li, Zhiyuan Liu, and Hanwang Zhang. 2019. Learning to embed sentences using attentive recursive trees. In Proc. of AAAI.
N. Siddharth, Andrei Barbu, and Jeffrey Mark Siskind. 2014. Seeing what you‚Äôre told: Sentence-guided activity recognition in video. In Proc. of CVPR.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2017. Visually grounded meaning representations. TPAMI, 39(11):2284‚Äì2297.
Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proc. of EMNLP-CoNLL.
Noah A. Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. of COLING-ACL.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proc. of ACL-IJCNLP.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and Daniel Jurafsky. 2011. Unsupervised dependency parsing without gold part-of-speech tags. In Proc. of EMNLP.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010. From baby steps to leapfrog: How ‚Äúless is more‚Äù in unsupervised dependency parsing. In Proc. of NAACL-HLT.
Mark Steedman. 2000. The Syntactic Process. MIT press Cambridge, MA.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identiÔ¨Åcation through concrete and abstract context. In Proc. of EMNLP.

Adina Williams, Andrew Drozdov, and Samuel R. Bowman. 2018. Do latent tree learning models identify meaningful structure in sentences? TACL, 6:253‚Äì267.
Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Mach. Learn., 8(3-4):229‚Äì256.
Hao Wu, Jiayuan Mao, Yufeng Zhang, Weiwei Sun, Yuning Jiang, Lei Li, and Wei-Ying Ma. 2019. UniÔ¨Åed Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations. In Proc. of CVPR.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2017. Learning to compose words into sentences with reinforcement learning. In Proc. of ICLR.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67‚Äì78.
Haonan Yu, N. Siddharth, Andrei Barbu, and Jeffrey Mark Siskind. 2015. A compositional framework for grounding language inference, generation, and acquisition in video. JAIR, 52:601‚Äì713.
Supplementary Material
The supplementary material is organized as follows. First, in Section A, we summarize and compare existing models for constituency parsing without explicit syntactic supervision. Next, in Section B, we present more implementation details of VG-NSL. Third, in Section C, we present the implementation details for all of our baseline models. Fourth, in Section D, we present the evaluation details of Benepar (Kitaev and Klein, 2018) on the MSCOCO data set. Fifth, in Section E, we qualitatively and quantitatively compare the concreteness scores estimated or labeled by different methods. Finally, in Section F, we show sample trees generated by VG-NSL on the MSCOCO test set.
A Overview of Models for Constituency Parsing without Explicit Syntactic Supervision
Shown in Table 5, we compare existing models for constituency parsing without explicit syntactic supervision, with respect to their learning objective, dependence on extra labels or extra corpus, and other features. The table also includes the analysis of previous works on parsing sentences based on gold part-of-speech tags.

Model

Objective

Extra Label

CCM (Klein and Manning, 2002)*

MAP

POS

DMV-CCM (Klein and Manning, 2005)* MAP

POS

U-DOP (Bod, 2006b)*

Probability Estimation POS

UML-DOP (Bod, 2006a)*

MAP

POS

PMI Random Left Right PRPN (Shen et al., 2018a) ON-LSTM (Shen et al., 2019) Gumbel softmax(Choi et al., 2018)

N/A



N/A



N/A



N/A



LM



LM



Cross-modal Retrieval 

VG-NSL (ours) VG-NSL+HI (ours)

Cross-modal Retrieval  Cross-modal Retrieval 

Concreteness estimation based models

Turney et al. (2011)*

N/A

Turney et al. (2011)+HI*

N/A

Brysbaert et al. (2014)*

N/A

Brysbaert et al. (2014)+HI*

N/A

Hessel et al. (2018)

N/A

Hessel et al. (2018)+HI

N/A

Concreteness (Partial) Concreteness (Partial) Concreteness (Full) Concreteness (Full)  

Multimodal
   
      
 




 

Stochastic
            




 

Extra Corpus
   
      
 




 

Table 5: Comparison of models for constituency parsing without explicit syntactic supervision. * denotes models requiring extra labels, such as POS tags or manually labeled concreteness scores. All multimodal methods listed in the table require a pretrained visual feature extractor (i.e., ResNet-101; He et al., 2016). A model is labeled as stochastic if for Ô¨Åxed training data and hyperparameters the model may produce different results (e.g., due to different choices of random initialization). To the best of our knowledge, results on concreteness estimation (Turney et al., 2011; Brysbaert et al., 2014; Hessel et al., 2018) have not been applied to unsupervised parsing so far.

3¬ó

3¬ó

3

¬ó

ground

4¬ò
44 ¬ò¬ò the

¬Å
¬Å on

&¬ê &¬ê
is

dd A cat

(a) Left-branching tree.

¬®r ¬®¬® rr A
4¬ò 44 ¬ò¬ò cat
4¬ò 4¬ò is
4¬ò 4¬ò on
&¬ê &¬ê the ground
(b) Right-branching tree.

Figure 5: Examples of some trivial tree structures.

B Implementation Details for VG-NSL
We adopt the code released by Faghri et al. (2018)7 as the visual-semantic embedding module for VGNSL. Following them, we Ô¨Åx the margin Œ¥ to 0.2. We also use the vocabulary provided by Faghri et al.
7https://github.com/fartashf/vsepp

(2018),8 which contains 10,000 frequent words in the MSCOCO data set. Out-of-vocabulary words are treated as unseen words. For either VG-NSL or baselines, we use the same vocabulary if applicable.
8http://www.cs.toronto.edu/Àúfaghri/ vsepp/vocab.tar

Hyperparameter tuning. As stated in main text, we use the self-agreement F1 score (Williams et al., 2018) as an unsupervised signal for tuning all hyperparamters. Besides the learning rate and other conventional hyperparameters, we also tune Œª, the hyperparameter for the head-initial bias model. Œª indicates the weight of penalization for ‚Äúright abstract constituents‚Äù. We choose Œª from {1, 2, 5, 10, 20, 50, 100} and found that Œª = 20 gives the best self-agreement F1 score.
C Implementation Details for Baselines
Trivial tree structures. We show examples for left-branching binary trees and right-branching binary trees in Figure 5. As for binary random trees, we iteratively combine two randomly selected adjacent constituents. This procedure is similar to that shown in Algorithm 2.
Parsing-Reading-Predict Network (PRPN). We use the code released by Shen et al. (2018a) to train PRPN.9 We tune the hyperparameters with respect to language modeling perplexity (Jelinek et al., 1977). For a fair comparison, we Ô¨Åx the hidden dimension of all hidden layers of PRPN as 512. We use an Adam optimizer (Kingma and Ba, 2015) to optimize the parameters. The tuned parameters are number of layers (1, 2, 3) and learning rate (1 √ó 10‚àí3, 5 √ó 10‚àí4, 2 √ó 10‚àí4). The models are trained for 100 epochs on the MSCOCO dataset and 1,000 epochs on the Multi30K dataset, and are early stopped using the criterion of language model perplexity.
Ordered Neurons (ON-LSTM). We use the code release by Shen et al. (2019) to train ONLSTM.10 We tune the hyperparameters with respect to language modeling perplexity (Jelinek et al., 1977), and use perplexity as an early stopping criterion. For a fair comparison, the hidden dimension of all hidden layers is set to 512, and the chunk size is changed to 16 to Ô¨Åt the hidden layer size. Following the original paper (Shen et al., 2019), we set the number of layers to be 3, and report the constituency parse tree with respect to the gate values output by the second layer of ON-LSTM. In order to obtain a better perplexity, we explore both Adam (Kingma and Ba, 2015) and SGD as the optimizer. We tune the learning rate (1 √ó 10‚àí3,
9https://github.com/yikangshen/PRPN 10https://github.com/yikangshen/ Ordered-Neurons

Algorithm 1: Constituency parsing based on given syntactic distance.
Input: text length m, list of syntactic distances d = (d1, d2, . . . , dm‚àí1)
Output: Boundaries of constituents B = {(Li, Ri)}i=1,...,m‚àí1
B = parse(d, 1, m)
Function parse(d, left, right) if left = right then
return EmptySet end p = arg maxj‚àà[left,right-1] dj boundaries = union(
{(left, right)}, parse (d, left, p), parse (d, p + 1, right) ) return boundaries
5 √ó 10‚àí4, 2 √ó 10‚àí4 for Adam, and 0.1, 1, 10, 30 for SGD). The models are trained for 100 epochs on the MSCOCO dataset and 1,000 epochs on the Multi30K dataset, and are early stopped using the criterion of language model perplexity.
PMI based constituency parsing. We estimate the pointwise mutual information (PMI; Church and Hanks, 1990) between two words using all captions in MSCOCO training set. We apply negative PMI as syntactic distance (Shen et al., 2018b) to generate a binary constituency parse tree recursively. The method of constituency parsing with a given list of syntactic distances is shown in Algorithm 1.
Gumbel-softmax based latent tree. We integrate Gumbel-softmax latent tree based text encoder (Choi et al., 2018)11 to the visual semantic embedding framework (Faghri et al., 2018), and use the tree structure produced by it as a baseline.
Concreteness estimation. For the semisupervised concreteness estimation, we reproduce the experiments by Turney et al. (2011), applying the manually labeled concreteness scores for 4,295 words from the MRC Psycholinguistic Database Machine Usable Dictionary (Coltheart, 1981) as supervision,12 and use English Wikipedia pages
11https://github.com/jihunchoi/ unsupervised-treelstm
12http://ota.oucs.ox.ac.uk/headers/1054. xml

Turney et al. (2011) Brysbaert et al. (2014) Hessel et al. (2018) VG-NSL+HI

Turney et al. (2011)
1.00 0.84 0.58 0.72

Brysbaert et al. (2014)
0.84 1.00 0.55 0.71

Hessel et al. (2018)
0.58 0.55 1.00 0.85

VG-NSL+HI
0.72 0.71 0.85 1.00

Table 6: Pearson correlation coefÔ¨Åcients between existing concreteness estimation methods, including baselines and VG-NSL+HI. In order to make a fair comparison, the correlation coefÔ¨Åcients are evaluated on the 100 most frequent words on MSCOCO test set.

1

0.5

0

-0.5

-1

-1.5

cat

on

ground

while

young

wood

who

wet

Turney et al., 2011 Brysbaert et al., 2014 Hessel et al., 2018 VG-NSL+HI (ours)

Figure 6: Normalized concreteness scores of example words.

to estimate PMI between words.13 The PMI is then used to compute similarity between seen and unseen words, which is further used as weights to estimate concreteness for unseen words. For the concreteness scores from crowdsourcing, we use the released data set of Brysbaert et al. (2014).14 Similarly to VG-NSL, the multimodal concreteness score (Hessel et al., 2018) is also estimated on the MSCOCO training set, using an open-sourced implementation.15

Constituency parsing with concreteness scores. Denote Œ±(w) as the concreteness score estimated by a model for the word w. Given a sequence of concreteness scores of caption tokens denoted by (Œ±(w1), Œ±(w2), . . . , Œ±(wm)), we aim to produce a binary constituency parse tree. We Ô¨Årst normalize the concreteness scores to the range of [‚àí1, 1], via:16

2

Œ±(wi)

‚àí

maxj

Œ±(wj )‚àíminj 2

Œ±(wj )

Œ± (wi) = maxj Œ±(wj) ‚àí minj Œ±(wj) .

We treat unseen words (i.e., out-of-vocabulary words) in the same way in VG-NSL, by assigning

13https://dumps.wikimedia.org/other/
static_html_dumps/April_2007/en/ 14http://crr.ugent.be/archives/1330 15https://github.com/victorssilva/
concreteness 16 For the concreteness scores estimated by Hessel et al.
(2018), we let Œ±(w) = log Œ±(w) before normalizing, as the original scores are in the range of (0, +‚àû).

the concreteness of ‚àí1 to unseen words, with the assumption that unseen words are the most abstract ones.
We compose constituency parse trees using the normalized concreteness scores by iteratively combining consecutive constituents. At each step, we select two adjacent constituents (initially, words) with the highest average concreteness score and combine them into a larger constituent, of which the concreteness is the average of its children. We repeat the above procedure until there is only one constituent left.
As for the head-initial inductive bias, we weight the concreteness of the right constituent with a hyperparemeter œÑ > 1 when ranking all pairs of consecutive constituents during selection. Meanwhile, the concreteness of the composed constituent remains the average of the two component constituents. In order to keep consistent with VG-NSL, we set œÑ = 20 in all of our experiments.
The procedure is summarized in Algorithm 2.
D Details of Manual Ground Truth Evaluation
It is important to conÔ¨Årm that the constituency parse trees of the MSCOCO captions produced by Benepar (Kitaev and Klein, 2018) are of high enough qualities, so that they can serve as reliable ground truth for further evaluation of other models. To verify this, we randomly sample 50 captions

$$¬à¬à

$

¬à¬à

$$$

¬à¬à

¬Ä

¬Ä



¬Ä¬Ä

Three white sinks

¬Ä

¬Ä



¬Ä



¬Ä

in

$$¬à¬à

$$$

¬à¬à¬à

¬Å

4¬ò

¬Å

44 ¬ò¬ò

a bathroom under mirrors

(a) Constituency parse tree labeled by Benepar (Kitaev and Klein, 2018).

@@@@hhhh

@@@@@@

 

hhhhhh

¬Ä

¬Ä



¬Ä¬Ä

Three white sinks

¬Å ¬Å in
¬Å ¬Å a bathroom

4¬ò
4¬ò under mirrors

(b) Manually labeled constituency parse tree.

Figure 7: A failure example by Benepar, where it fails to parse the noun phrase ‚Äúthree white sinks in a bathroom under mirrors‚Äù ‚Äì according to human commonsense, it is much more common for sinks, rather than a bathroom, to be under mirrors. However, most of the constituents (e.g., ‚Äúthree white sinks‚Äù and ‚Äúunder mirrors‚Äù) are still successfully extracted by Benepar.

Algorithm 2: Constituency parsing based on concreteness estimation. Input: list of normalized concreteness scores
a = (a1, a2, . . . , am), hyperparameter œÑ Output: Boundaries of constituents
B = {(Li, Ri)}i=1,...,m‚àí1 for j = 1 to m do
leftj = j rightj = j end while len(a) > 1 do p = arg maxj aj + œÑ aj+1 add (leftp, rightp+1) to B a = a<p + ( ap+2ap+1 ) + a>p+1 left = left<p + (leftp) + left>p+1 right = right<p + (rightp+1) + right>p+1 end
from the MSCOCO test split, and manually label the constituency parse trees without reference to either Benepar or the paired images, following the principles by Bies et al. (1995) as much as possible.17 Note that we only label the tree structures
17 The manually labeled constituency parse trees are publicly available at https://ttic.uchicago.edu/ Àúfreda/vgnsl/manually_labeled_trees.txt

without constituency labels (e.g., NP and PP). Most failure cases by Benepar are related to human commonsense in resolving parsing ambiguities, e.g., prepositional phrase attachments (Figure 7).
We compare the manually labeled trees and those produced by Benepar (Kitaev and Klein, 2018), and Ô¨Ånd that the F1 score between them are 95.65.
E Concreteness by Different Models
E.1 Correlation between Different Concreteness Estimations
We report the correlation of different methods for concreteness estimation, shown in (Table 6). The concreteness given by Turney et al. (2011) and Brysbaert et al. (2014) highly correlate with each other. The concreteness scores estimated on multimodal dataset (Hessel et al., 2018) also moderately correlates with the aforementioned two methods (Turney et al., 2011; Brysbaert et al., 2014). Compared to the concreteness estimated by Hessel et al. (2018), the one estimated by our model has a stronger correlation with the scores estimated from linguistic data (Turney et al., 2011; Brysbaert et al., 2014).

E.2 Concreteness Scores of Sample Words by Different Methods
We present the concreteness scores estimated or labeled by different methods in Figure 6, which qualitatively shows that different methods correlate with others well.
F Sample Trees Generated by VG-NSL
Figure 8 shows the sample trees generated by VG-NSL with the head-initial inductive bias (VGNSL+HI). All captions are chosen from the MSCOCO test set.

$$¬à¬à

$$

¬à¬à

$

¬à

5¬ô 5¬ô
a kitchen

$$¬à¬à

$$$

¬à¬à¬à

with

$$¬à¬à

$$$

¬à¬à¬à

4¬ò 4¬ò two windows

¬®r

¬®r

¬®

r

and

¬®r ¬®¬® rr two

¬Å ¬Å
metal sinks

(a) a kitchen with two windows and two metal sinks

@@@@hhhh

@@@

hhh

¬®r ¬®¬® rr a

¬®r

¬®r

¬®

r

blue

¬Å ¬Å small plane

3¬ó

3¬ó

3

¬ó

standing

4¬ò
4¬ò at

¬Å ¬Å
the airstrip

(b) a blue small plane standing at the airstrip

¬Ä

¬Ä



¬Ä



¬Ä

¬®r ¬®¬® rr
sitting
&¬ê &¬ê young boy

¬®r ¬®¬® rr on
4¬ò 4¬ò top
¬Å ¬Å of
¬Å ¬Å a briefcase

(c) young boy sitting on top of a briefcase

$$¬à¬à

$

¬à¬à

$$$

¬à¬à

¬Ä

¬Ä



¬Ä¬Ä

eating

3¬ó

3¬ó

3

¬ó

4¬ò
44 ¬ò¬ò a

d

&¬ê

d

&¬ê

a plate of broccoli

&¬ê &¬ê
small dog

(d) a small dog eating a plate of broccoli

@@@@hhhh

@@@@

hhhh

it

@@@@hhhh

@@

hhhh

@@@@

hh

¬Ä

¬Ä



¬Ä



¬Ä

&¬ê &¬ê a building

¬®r ¬®¬® rr with
¬®r ¬®¬® rr a
¬®r ¬®¬® rr bunch

¬®r ¬®¬® rr
standing around

5¬ô 5¬ô
of people

(e) a building with a bunch of people standing around it

$$¬à¬à

$$$

¬à¬à¬à

3¬ó

3¬ó

3

¬ó

walking

Dl Dl a horse

3¬ó

3¬ó

3

¬ó

by

3¬ó

3¬ó

3

¬ó

4¬ò dd 4 ¬ò a tree in

&¬ê &¬ê the woods

(f) a horse walking by a tree in the woods

222¬ñ¬ñ¬ñ

2222

¬ñ¬ñ¬ñ¬ñ

$$¬à¬à

$$

¬à¬à

$

¬à

3¬ó 33 ¬ó¬ó the
4¬ò 4¬ò golden wafÔ¨Çe

4¬ò 4¬ò has
5¬ô 5¬ô a banana

(g) the golden wafÔ¨Çe has a banana in it .

((¬í¬í in it

@@@@hhhh

@@@

hhh

3¬ó

3¬ó

33

¬ó¬ó

¬®r

¬®r

¬®

r

that

4¬ò

dd

4¬ò

a bowl full

¬®r ¬®¬® rr still

&¬ê
&¬ê of oranges

¬Å ¬Å
have stems

(h) a bowl full of oranges that still have stems

222¬ñ¬ñ¬ñ

222

¬ñ¬ñ¬ñ

4¬ò 44 ¬ò¬ò there
&¬ê &¬ê is
5¬ô 5¬ô a person

3¬ó

3¬ó

3

¬ó

that

¬Ä

¬Ä



¬Ä¬Ä

is

$$¬à¬à

$$$

¬à¬à¬à

sitting

¬Ä

¬Ä



¬Ä



¬Ä

¬Å
¬Å in

4¬ò 4¬ò on

Dl Dl the boat

5¬ô 5¬ô
the water

(i) there is a person that is sitting in the boat on the water

$$¬à¬à

$$

¬à¬à

$

¬à

¬Ä

¬Ä



¬Ä¬Ä

¬Å ¬Å

5¬ô 5¬ô

a sandwich and soup

4¬ò 4¬ò sit
&¬ê &¬ê on

d d a table

(j) a sandwich and soup sit on a table

$$¬à¬à

$

¬à¬à

$$$

¬à¬à

4¬ò 4¬ò a
4¬ò 4¬ò big umbrella

¬®r ¬®¬® rr sitting
4¬ò 4¬ò on

&¬ê &¬ê
the beach

(k) a big umbrella sitting on the beach

Figure 8: Examples of parsing trees generated by VG-NSL.

