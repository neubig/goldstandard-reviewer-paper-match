Morphological Inﬂection Generation Using Character Sequence to Sequence Learning
Manaal Faruqui1 Yulia Tsvetkov1 Graham Neubig2 Chris Dyer1 1Language Technologies Institute, Carnegie Mellon University, USA
2Graduate School of Information Science, Nara Institute of Science and Technology, Japan {mfaruqui,ytsvetko,cdyer}@cs.cmu.edu neubig@is.naist.jp

arXiv:1512.06110v3 [cs.CL] 22 Mar 2016

Abstract
Morphological inﬂection generation is the task of generating the inﬂected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inﬂection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inﬂection generation.
1 Introduction
Inﬂection is the word-formation mechanism to express different grammatical categories such as tense, mood, voice, aspect, person, gender, number and case. Inﬂectional morphology is often realized by the concatenation of bound morphemes (preﬁxes and sufﬁxes) to a root form or stem, but nonconcatenative processes such as ablaut and inﬁxation are found in many languages as well. Table 1 shows the possible inﬂected forms of the German stem Kalb (calf) when it is used in different cases and numbers. The inﬂected forms are the result of both ablaut (e.g., a→a¨) and sufﬁxation (e.g., +ern).
Inﬂection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an

singular plural

nominative accusative dative genitive

Kalb Kalb Kalb Kalbes

Ka¨lber Ka¨lber Ka¨lbern Ka¨lber

Table 1: An example of an inﬂection table from the German

noun dataset for the word Kalb (calf).

independent entity. Translating into lemmas in the target language, and then applying inﬂection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inﬂection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identiﬁcation of multi-word expressions (Oﬂazer et al., 2004), among other applications.
The traditional approach to modeling inﬂection relies on hand-crafted ﬁnite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oﬂazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inﬂections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes

kalb
case=nominative number=plural

inﬂection generation

kälber

Figure 1: A general inﬂection generation model.

(e.g. afﬁxation) or require careful feature engineering.
In this paper, we present a model of inﬂection generation based on a neural network sequence to sequence transducer. The root form is represented as sequence of characters, and this is the input to an encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014). The model transforms its input to a sequence of output characters representing the inﬂected form (§4). Our model makes no assumptions about morphological processes, and our features are simply the individual characters. The model is trained on pairs of root form and inﬂected forms obtained from inﬂection tables extracted from Wiktionary.1 We improve the supervised model with unlabeled data, by integrating a character language model trained on the vocabulary of the language.
Our experiments show that the model achieves better or comparable results to state-of-the-art methods on the benchmark inﬂection generation tasks (§5). For example, our model is able to learn longrange relations between character sequences in the string aiding the inﬂection generation process required by Finnish vowel harmony (§6), which helps it obtain the current best results in that language. We have publicly released our code for inﬂection generation.2
2 Inﬂection Generation: Background
Durrett and DeNero (2013) formulate the task of supervised inﬂection generation for a given root form, based on a large number of training inﬂection tables extracted from Wiktionary. Every inﬂection table contains the inﬂected form of a given root word corresponding to different linguistic transformations (cf. Table 1). Figure 1 shows the inﬂection generation framework. Since the release of the Wiktionary
1www.wiktionary.org 2https://github.com/mfaruqui/morph-_ trans

dataset, several different models have reported performance on this dataset. As we are also using this dataset, we will now review these models.
We denote the models of Durrett and DeNero (2013), Ahlberg et al. (2014), Ahlberg et al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inﬂection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms.
The ﬁrst step is learning character alignments across inﬂected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inﬂection generation for different inﬂection types as shown in Figure 2 (b)–(d).
By applying the extracted rules to new root forms, inﬂected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features extracted from words to match an input word to a rule table. NCK15 use a semi-Markov model inspired by DDN13, but additionally use target n-grams and joint n-grams as features sequences while selecting the rules.
Motivation for our model. Morphology often makes references to segmental features, like place or manner of articulation, or voicing status (Chomsky and Halle, 1968). While these can be encoded as features in existing work, our approach treats segments as vectors of features “natively”. Our approach represents every character as a bundle of continuous features, instead of using discrete surface character sequence features. Also, our model uses features as part of the transduction rules themselves, whereas in existing work features are only used to rescore rule applications.
In existing work, the learner implicitly speci-

(a) <w> k a l b </w> <w> k ä l b e r </w> <w> k a l b e s </w>

(b) <w> x1 a x2 </w>

(c) a

<w> x1 ä x2 er </w>

ä

<w> x1 a x2 es </w>

a

(d) a ä

a </w>

</w>

k

a er </w> es </w> k

</w>
er </w> es </w>
lb lb

Figure 2: Rule extraction: (a) Character aligned-table; (b) Table-level rule of AFH14, AFH15 (c) Vertical rules of DDN13 and (d) Atomic rules of NCK15.

ﬁes the class of rules that can be learned, such as “delete” or “concatenate”. To deal with phenomenona like segment lengthening in English: run → running; or reduplication in Hebrew: Kelev → Klavlav, Chatul → Chataltul; (or consonant gradation in Finnish), where the afﬁxes are induced from characters of the root form, one must engineer a new rule class, which leads to poorer estimates due to data sparsity. By modeling inﬂection generation as a task of generating a character sequence, one character at a time, we do away with such problems.

3 Neural Encoder-Decoder Models

Here, we describe brieﬂy the underlying framework of our inﬂection generation model, called the recurrent neural network (RNN) encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) which is used to transform an input sequence x to output sequence y. We represent an item by x, a sequence of items by x, vectors by x, matrices by X, and sequences of vectors by x.

3.1 Formulation
In the encoder-decoder framework, an encoder reads a variable length input sequence, a sequence of vectors x = x1, · · · , xT (corresponding to a sequence of input symbols x = x1, · · · , xT ) and generates a ﬁxed-dimensional vector representation of the sequence. xt ∈ Rl is an input vector of length l. The most common approach is to use an RNN such that:

ht = f (ht−1, xt)

(1)

where ht ∈ Rn is a hidden state at time t, and f is generally a non-linear transformation, producing

e := hT +1 as the input representation. The decoder is trained to predict the next output yt given the encoded input vector e and all the previously predicted outputs y1, · · · yt−1 . In other words, the decoder deﬁnes a probability over the output sequence y = y1, · · · , yT by decomposing the joint probability into ordered conditionals:

p(y|x) =

T
t=1 p(yt|e, y1, · · · , yt−1 ) (2)

With a decoder RNN, we can ﬁrst obtain the hidden layer at time t as: st = g(st−1, {e, yt−1}) and feed this into a softmax layer to obtain the conditional probability as:

p(yt = i|e, y<t) = softmax(Wsst + bs)i (3)

where, y<t = y1, · · · , yt−1 . In recent work, both f and g are generally LSTMs, a kind of RNN which we describe next.

3.2 Long Short-Term Memory (LSTM)
In principle, RNNs allow retaining information from time steps in the distant past, but the nonlinear “squashing” functions applied in the calculation of each ht result in a decay of the error signal used in training with backpropagation. LSTMs are a variant of RNNs designed to cope with this “vanishing gradient” problem using an extra memory “cell” (Hochreiter and Schmidhuber, 1997; Graves, 2013). Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory or forget. We refer interested readers to the original papers for details.

4 Inﬂection Generation Model
We frame the problem of inﬂection generation as a sequence to sequence learning problem of character sequences. The standard encoder-decoder models were designed for machine translation where the objective is to translate a sentence (sequence of words) from one language to a semantically equivalent sentence (sequence of words) in another language. We can easily port the encoder-decoder translation model for inﬂection generation. Our model predicts the sequence of characters in the inﬂected string given the characters in the root word (input).

e k ä l b e r </w>

<w> k a l b </w>

<w> k ka ee

ä l be r l bεε ε e e eee

Figure 3: characters

· The modiﬁed encoder-decoder architecture for inﬂection
are shown in red. indicates the append operation.

generation.

Input

characters

are

shown

in

black

and

predicted

However, our problem differs from the above setting in two ways: (1) the input and output character sequences are mostly similar except for the inﬂections; (2) the input and output character sequences have different semantics. Regarding the ﬁrst difference, taking the word play as an example, the inﬂected forms corresponding to past tense and continuous forms are played and playing. To better use this correspondence between the input and output sequence, we also feed the input sequence directly into the decoder:

st = g(st−1, {e, yt−1, xt})

(4)

where, g is the decoder LSTM, and xt and yt are the input and output character vectors respectively. Because the lengths of the input and output sequences are not equal, we feed an character in the decoder, indicating null input, once the input sequence runs out of characters. These character vectors are parameters that are learned by our model, exactly as other character vectors.
Regarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an afﬁne transformation on the encoded vector e:

e ← Wtranse + btrans

(5)

where, Wtrans, btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively

(Ling et al., 2015; Ballesteros et al., 2015; Bahdanau et al., 2015). Our resultant inﬂection generation model is shown in Figure 3.
4.1 Supervised Learning
The parameters of our model are the set of character vectors, the transformation parameters (Wtrans, btrans), and the parameters of the encoder and decoder LSTMs (§3.2). We use negative loglikelihood of the output character sequence as the loss function:
T
− log p(y|x) = − t=1 log p(yt|e, y<t) (6)
We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inﬂection generation and we evaluate it in two different settings as established by previous work:
Factored Model. In the ﬁrst setting, we learn a separate model for each type of inﬂection independent of the other possible inﬂections. For example, in case of German nouns, we learn 8, and for German verbs, we learn 27 individual encoder-decoder inﬂection models (cf. Table 3). There is no parameter sharing across these models. We call these factored models of inﬂection generation.
Joint Model. In the second setting, while learning a model for an inﬂection type, we also use the information of how the lemma inﬂects across all other inﬂection types i.e., the inﬂection table of a root form is used to learn different inﬂection models. We model this, by having the same encoder

pLM(y) len(y) - len(x) same-sufﬁx(y, x)? same-preﬁx(y, x)?

p(y|x) levenshtein(y, x) subsequence(y, x)? subsequence(x, y)?

Table 2: Features used to rerank the inﬂected outputs. x, y

denote the root and inﬂected character sequences resp.

in the encoder-decoder model across all inﬂection models.3 The encoder in our model is learning a representation of the input character sequence. Because all inﬂection models take the same input but produce different outputs, we hypothesize that having the same encoder can lead to better estimates.
4.2 Semi-supervised Learning
The model we described so far relies entirely on the availability of pairs of root form and inﬂected word form for learning to generate inﬂections. Although such supervised models can be used to obtain inﬂection generation models (Durrett and DeNero, 2013; Ahlberg et al., 2015), it has been shown that unlabeled data can generally improve the performance of such systems (Ahlberg et al., 2014; Nicolai et al., 2015). The vocabulary of the words of a language encode information about what correct sequences of characters in a language look like. Thus, we learn a language model over the character sequences in a vocabulary extracted from a large unlabeled corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings.
Output Reranking. In the ﬁrst setting, we ﬁrst train the inﬂection generation model using the supervised setting as described in §4.1. While making predictions for inﬂections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set.
3We also tried having the same encoder and decoder across inﬂection types, with just the transformation matrix being different (equ. 5), and observed consistently worse results.

Dataset

root forms Inﬂ.

German Nouns (DE-N)

2764 8

German Verbs (DE-V)

2027 27

Spanish Verbs (ES-V)

4055 57

Finnish NN & Adj. (FI-NA)

6400 28

Finnish Verbs (FI-V)

7249 53

Dutch Verbs (NL-V)

11200 9

French Verbs (FR-V)

6957 48

Table 3: The number of root forms and types of inﬂections

across datasets.

Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inﬂection generation model. Thus, the loss function becomes:

1 −log p(y|x) =
Z

T
t=1 − log p(yt|e, y<t) − λlog pLM(yt|y<t) (7)

where pLM (yt|y<t) is the probability of observing the word yt given the history estimated according to a language model, λ ≥ 0 is the interpolation parameter which is learned during training and Z is the normalization factor. This formulation lets us use any off-the-shelf pre-trained character language model easily (details in §5).

4.3 Ensembling

Our loss functions (equ. 6 & 7) formulated using a

neural network architecture are non-convex in nature

and are thus difﬁcult to optimize. It has been shown

that taking an ensemble of models which were ini-

tialized differently and trained independently leads

to improved performance (Hansen and Salamon,

1990; Collobert et al., 2011). Thus, for each model

type used in this work, we report results obtained

using an ensemble of models. So, while decoding

we compute the probability of emitting a charac-

ter as the product-of-experts of the individual mod-

els

in

the

ensemble:

pens(yt|·)

=

1 Z

k i=1

pi(yt|·)

1 k

where, pi(yt|·) is the probability according to i-th

model and Z is the normalization factor.

5 Experiments
We now conduct experiments using the described models. Note that not all previously published mod-

els present results on all settings, and thus we compare our results to them wherever appropriate.
Hyperparameters. Across all models described in this paper, we use the following hyperparameters. In both the encoder and decoder models we use single layer LSTMs with the hidden vector of length 100. The length of character vectors is the size of character vocabulary according to each dataset. The parameters are regularized with 2, with the regularization constant 10−5.4 The number of models for ensembling are k = 5. Models are trained for at most 30 epochs and the model with best result on development set is selected.
5.1 Data
Durrett and DeNero (2013) published the Wiktionary inﬂection dataset with training, development and test splits. The development and test sets contain 200 inﬂection tables each and the training sets consist of the remaining data. This dataset contains inﬂections for German, Finnish and Spanish. This dataset was further augmented by (Nicolai et al., 2015), by adding Dutch verbs extracted from CELEX lexical database (Baayen et al., 1995), French verbs from Verbsite, an online French conjugation dictionary and Czech nouns and verbs from the Prague Dependnecy Treebank (Hajicˇ et al., 2001). As the dataset for Czech contains many incomplete tables, we do not use it for our experiments. These datasets come with pre-speciﬁed training/dev/test splits, which we use(cf. Table 3). For each of these sets, the training data is restricted to 80% of the total inﬂection tables, with 10% for development and 10% for testing.
For semi-supervised experiments, we train a 5gram character language model with Witten-Bell smoothing (Bell et al., 1990) using the SRILM toolkit (Stolcke, 2002). We train the character language models on the list of unique word types extracted from the Wikipedia dump for each language after ﬁltering out words with characters unseen in the inﬂection generation training dataset. We obtained ≈2 million unique words for each language.

DDN13 NCK15 Ours

DE-V DE-N ES-V FI-V FI-NA NL-V FR-V

94.76 88.31 99.61 97.23 92.14 90.50 98.80

97.50 88.60 99.80 98.10 93.00 96.10 99.20

96.72 88.12 99.81 97.81 95.44 96.71 98.82

Avg.

94.47 96.04 96.20

Table 4: Individual form prediction accuracy for factored su-

pervised models.

DDN13 AFH14 AFH15 Ours

DE-V DE-N ES-V FI-V FI-NA

96.19 88.94 99.67 96.43 93.41

97.01 87.81 99.52 96.36 91.91

98.11 89.88 99.92 97.14 93.68

97.25 88.37 99.86 97.97 94.71

Avg.

94.93 94.53 95.74 95.63

NL-V 93.88

–

– 96.16

FR-V 98.60

–

– 98.74

Avg.

95.30

–

– 96.15

Table 5: Individual form prediction accuracy for joint super-

vised models.

5.2 Results
Supervised Models. The individual inﬂected form accuracy for the factored model (§4.1) is shown in Table 4. Across datasets, we obtain either comparable or better results than NCK15 while obtaining on average an accuracy of 96.20% which is higher than both DDN13 and NCK15. Our factored model performs better than DDN13 and NCK15 on datasets with large training set (ES-V, FI-V, FINA, NL-V, FR-V) as opposed to datasets with small training set (DE-N, DE-V). In the joint model setting (cf. Table 5), on average, we perform better than DDN13 and AFH14 but are behind AFH15 by 0.11%. Our model improves in performance over our factored model for DE-N, DE-V, and ES-V, which are the three smallest training datasets. Thus, parameter sharing across different inﬂection types helps the low-resourced scenarios.5
4Using dropout did not improve our results. 5Although NCK15 provide results in the joint model setting, they also use raw data in the joint model which makes it incomparable to our model and other previous models.

AFH14 NCK15 Interpol Rerank

DE-V DE-N ES-V FI-V FI-NA

97.87 91.81 99.58 96.63 93.82

97.90 89.90 99.90 98.10 93.60

96.79 88.31 99.78 96.66 94.60

97.11 89.31 99.94 97.62 95.66

Avg.

95.93 95.88 95.42 95.93

NL-V FR-V

– 96.60 96.66 96.64 – 99.20 98.81 98.94

Avg.

– 96.45 96.08 96.45

Table 6: Individual form prediction accuracy for factored

semi-supervised models.

Model

Accuracy

Encoder-Decoder

79.08

Encoder-Decoder Attention

95.64

Ours W/O Encoder

84.04

Ours

96.20

Table 7: Avg. accuracy across datasets of the encoder-decoder,

attentional encoder-decoder & our model without encoder.

Semi-supervised Models. We now evaluate the utility of character language models in inﬂection generation, in two different settings as described earlier (§4.2). We use the factored model as our base model in the following experiments as it performed better than the joint model (cf. Table 4 & 5). Our reranking model which uses the character language model along with other features (cf. Table 2) to select the best answer from a beam of predictions, improves over almost all the datasets with respect to the supervised model and is equal on average to AFH14 and NCK15 semi-supervised models with 96.45% accuracy. We obtain the best reported results on ES-V and FI-NA datasets (99.94% and 95.66% respectively). However, our second semi-supervised model, the interpolation model, on average obtains 96.08% and is surprisingly worse than our supervised model (96.20%).
Comparison to Other Architectures. Finally it is of interest how our proposed model compares to more traditional neural models. We compare our model against a standard encoder-decoder model, and an encoder-decoder model with attention, both trained on root form to inﬂected form character sequences. In a standard encoder-decoder model (Sutskever et al., 2014), the encoded input sequence

100

98

Accuracy (%)

96

94

92 90 5

10

15

Word length

Ours DDN13 NCK15
20

Figure 4: Plot of inﬂection prediction accuracy against the length of gold inﬂected forms. The points are shown with minor offset along the x-axis to enhance clarity.

vector is fed into the hidden layer of the decoder as input, and is not available at every time step in contrast to our model, where we additionally feed in xt at every time step as in equ. 4. An attentional model computes a weighted average of the hidden layer of the input sequence, which is then used along with the decoder hidden layer to make a prediction (Bahdanau et al., 2015). These models also do not take the root form character sequence as inputs to the decoder. We also evaluate the utility of having an encoder which computes a representation of the input character sequence in a vector e by removing the encoder from our model in Figure 3. The results in Table 7 show that we outperform the encoder-decoder model, and the model without an encoder substantially. Our model is slightly better than the attentional encoder-decoder model, and is simpler as it does not have the additional attention layer.
6 Analysis
Length of Inﬂected Forms. In Figure 4 we show how the prediction accuracy of an inﬂected form varies with respect to the length of the correct inﬂected form.To get stable estimates, we bin the inﬂected forms according to their length: < 5, [5, 10), [10, 15), and ≥ 15. The accuracy for each bin is macro-averaged across 6 datasets6 for our factored model and the best models of DDN13 and NCK15.
6We remove DE-N as its the smallest and shows high variance in results.

Our model consistently shows improvement in performance as word length increases and is signiﬁcantly better than DDN13 on words of length more than 20 and is approximately equal to NCK15. On words of length < 5, we perform worse than DDN13 but better than NCK15. On average, our model has the least error margin across bins of different word length as compared to both DDN13 and NCK15. Using LSTMs in our model helps us make better predictions for long sequences, since they have the ability to capture long-range dependencies.
Finnish Vowel Harmony. Our model obtains the current best result on the Finnish noun and adjective dataset, this dataset has the longest inﬂected words, some of which are > 30 characters long. Finnish exhibits vowel harmony, i.e, the occurrence of a vowel is controlled by other vowels in the word. Finnish vowels are divided into three groups: front (a¨, o¨, y), back (a, o, u), and neutral (e, i). If back vowels are present in a stem, then the harmony is back (i.e, front vowels will be absent), else the harmony is front (i.e, back vowels will be absent). In compound words the sufﬁx harmony is determined by the ﬁnal stem in the compound. For example, our model correctly inﬂects the word fasisti (fascist) to obtain fasisteissa and the compound ta¨rkkelyspitoinen (starch containing) to ta¨rkkelyspitoisissa. The ability of our model to learn such relations between these vowels helps capture vowel harmony. For FI-NA, our model obtains 99.87% for correctly predicting vowel harmony, and NCK15 obtains 98.50%.We plot the character vectors of these Finnish vowels (cf. Figure 5) using t-SNE projection (van der Maaten and Hinton, 2008) and observe that the vowels are correctly grouped with visible transition from the back to the front vowels.
7 Related Work
Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forwardbackward LSTM from a word, and use them in a traditional weighted FST to generate inﬂected forms. Neural encoder-decoder models of string transduction have also been used for sub-word level transformations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015).
Generation of inﬂectional morphology has been

Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences.
particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Schu¨tze, 2015).
Inﬂection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that beneﬁts from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns.
8 Conclusion
We have presented a model that generates inﬂected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-of-the-art results and performs at par or

better than existing inﬂection generation models on seven different datasets. Our model is able to learn long-range dependencies within character sequences for inﬂection generation which makes it specially suitable for morphologically rich languages.
Acknowledgements
We thank Mans Hulden for help in explaining Finnish vowel harmony, and Garrett Nicolai for making the output of his system available for comparison. This work was sponsored in part by the National Science Foundation through award IIS1526745.
References
[Ahlberg et al.2014] Malin Ahlberg, Markus Forsberg, and Mans Hulden. 2014. Semi-supervised learning of morphological paradigms and lexicons. In Proc. of EACL.
[Ahlberg et al.2015] Malin Ahlberg, Markus Forsberg, and Mans Hulden. 2015. Paradigm classiﬁcation in supervised learning of morphology. Proc. of NAACL.
[Baayen et al.1995] Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1995. The CELEX Lexical Database. Release 2 (CD-ROM). LDC, University of Pennsylvania.
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR.
[Ballesteros et al.2015] Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2015. Improved transition-based parsing by modeling characters instead of words with lstms. In Proc. of EMNLP.
[Bell et al.1990] Timothy C Bell, John G Cleary, and Ian H Witten. 1990. Text compression. Prentice-Hall, Inc.
[Bergroth et al.2000] Lasse Bergroth, Harri Hakonen, and Timo Raita. 2000. A survey of longest common subsequence algorithms. In Proc. of SPIRE.
[Chahuneau et al.2013a] Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013a. Translating into morphologically rich languages with synthetic phrases. In Proc. of EMNLP.
[Chahuneau et al.2013b] Victor Chahuneau, Noah A Smith, and Chris Dyer. 2013b. Knowledge-rich morphological priors for bayesian language models. In Proc. of NAACL.
[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,

Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proc. of EMNLP. [Chomsky and Halle1968] N. Chomsky and M. Halle. 1968. The Sound Pattern of English. Harper & Row, New York, NY. [Clark2003] Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proc. of EACL. [Clifton and Sarkar2011] Ann Clifton and Anoop Sarkar. 2011. Combining morpheme-based machine translation with post-processing morpheme prediction. In Proc. of ACL. [Collobert et al.2011] Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537. [Cotterell and Schu¨tze2015] Ryan Cotterell and Hinrich Schu¨tze. 2015. Morphological word-embeddings. In Proc. of NAACL. [Cotterell et al.2015] Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association for Computational Linguistics, 3:433– 447. [Cotterell et al.2016] Ryan Cotterell, Tim Vieria, and Hinrich Schu¨tze. 2016. A joint model of orthography and morphological segmentation. In Proc. of NAACL. [Creutz and Lagus2005] Mathias Creutz and Krista Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Helsinki University of Technology. [Dreyer and Eisner2011] Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a dirichlet process mixture model. In Proc. of EMNLP. [Durrett and DeNero2013] Greg Durrett and John DeNero. 2013. Supervised learning of complete morphological paradigms. In Proc. of NAACL. [Eisner2002] Jason Eisner. 2002. Parameter estimation for probabilistic ﬁnite-state transducers. In Proc. of ACL. [Fraser et al.2012] Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling inﬂection and word-formation in SMT. In Proc. of EACL. [Goldwater and McClosky2005] Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proc. of EMNLP, pages 676–683. [Graves et al.2005] Alex Graves, Santiago Ferna´ndez, and Ju¨rgen Schmidhuber. 2005. Bidirectional lstm

networks for improved phoneme classiﬁcation and recognition. In Proc. of ICANN.
[Graves2013] Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850.
[Hajicˇ et al.2001] Jan Hajicˇ, Barbora Vidova´-Hladka´, and Petr Pajas. 2001. The Prague Dependency Treebank: Annotation structure and support. In Proc. of the IRCS Workshop on Linguistic Databases.
[Hansen and Salamon1990] Lars Kai Hansen and Peter Salamon. 1990. Neural network ensembles. In Proc. of PAMI.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780.
[Hopkins and May2011] Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proc. of EMNLP.
[Hulden2014] Mans Hulden. 2014. Generalizing inﬂection tables into paradigms with ﬁnite state operations. In Proc. of the Joint Meeting of SIGMORPHON and SIGFSM.
[Kaplan and Kay1994] Ronald M Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational linguistics, 20(3):331–378.
[Koskenniemi1983] Kimmo Koskenniemi. 1983. Twolevel morphology: A general computational model for word-form recognition and production. University of Helsinki.
[Ling et al.2015] Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ra´mon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proc. of EMNLP.
[Minkov et al.2007] Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proc. of ACL.
[Narasimhan et al.2015] Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. 2015. An unsupervised method for uncovering morphological chains. TACL.
[Nicolai et al.2015] Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak. 2015. Inﬂection generation as discriminative string transduction. In Proc. of NAACL.
[Oﬂazer et al.2004] Kemal Oﬂazer, O¨ zlem c¸etinog˘lu, and Bilge Say. 2004. Integrating morphology with multiword expression processing in turkish. In Proc. of the Workshop on Multiword Expressions.
[Oﬂazer1996] Kemal Oﬂazer. 1996. Error-tolerant ﬁnitestate recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.

[Oncina and Sebban2006] Jose Oncina and Marc Sebban. 2006. Learning stochastic edit distance: Application in handwritten character recognition. Pattern recognition, 39(9):1575–1587.
[Poon et al.2009] Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proc. of NAACL.
[Rao et al.2015] Kanishka Rao, Fuchun Peng, Hasim Sak, and Franc¸oise Beaufays. 2015. Grapheme-tophoneme conversion using long short-term memory recurrent neural networks. In Proc. of ICASSP.
[Rastogi et al.2016] Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner. 2016. Weighting ﬁnite-state transductions with neural context. In Proc. of NAACL.
[Santos and Zadrozny2014] Cicero D. Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proc. of ICML.
[Sarawagi and Cohen2004] Sunita Sarawagi and William W Cohen. 2004. Semi-markov conditional random ﬁelds for information extraction. In Proc. of NIPS.
[Snyder and Barzilay2008] Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In In The Annual Conference of the.
[Stolcke2002] Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proc. of Interspeech.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Proc. of NIPS.
[Toutanova et al.2008] Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proc. of ACL, pages 514–522.
[van der Maaten and Hinton2008] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research, 9:2579–2605.
[Wicentowski2004] Richard Wicentowski. 2004. Multilingual noise-robust supervised morphological analysis using the wordframe model. In Proc. of SIGPHON.
[Yao and Zweig2015] Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-to-sequence neural net models for grapheme-to-phoneme conversion. In Proc. of ICASSP.
[Yarowsky and Wicentowski2000] David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proc. of ACL.
[Zeiler2012] Matthew D Zeiler. 2012. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.

