arXiv:1712.02768v1 [cs.CL] 6 Dec 2017

Convolutional Neural Networks for Medical Diagnosis from Admission Notes
Convolutional Neural Networks for Medical Diagnosis from Admission Notes
Christy Yuan Li1, Dimitris Konomis1, Graham Neubig1, Pengtao Xie1, Carol Cheng1,2, and Eric Xing1,3
1Petuum Inc., Pittsburgh, 15222, USA 2Department of Psychiatry, University of Pittsburgh Medical Center, Pittsburgh, 15213, USA 3eric.xing@petuum.com
Abstract
Objective Develop an automatic diagnostic system which only uses textual admission information from Electronic Health Records (EHRs) and assist clinicians with a timely and statistically proved decision tool. The hope is that the tool can be used to reduce mis-diagnosis.
Materials and Methods We use the real-world clinical notes from MIMIC-III, a freely available dataset consistsing of clinical data of more than forty thousand patients who stayed in intensive care units of the Beth Israel Deaconess Medical Center between 2001 and 2012 (Johnson et al., 2016). We proposed a Convolutional Neural Network model to learn semantic features from unstructured textual input and automatically predict primary discharge diagnosis.
Results The proposed model achieved an overall 96.11% accuracy and 80.48% weighted F1 score values on 10 most frequent disease classes, signiﬁcantly outperforming four strong baseline models by at least 12.7% in weighted F1 score.
Discussion Experimental results imply that the CNN model is suitable for supporting diagnosis decision making in the presence of complex, noisy and unstructured clinical data while at the same time using fewer layers and parameters that other traditional Deep Network models.
Conclusion Our model demonstrated capability of representing complex medical meaningful features from unstructured clinical notes and prediction power for commonly misdiagnosed frequent diseases. It can use easily adopted in clinical setting to provide timely and statistically proved decision support.
Keywords Convolutional neural network, text classiﬁcation, discharge diagnosis prediction, admission information from EHRs.
1. Background and Signiﬁcance
Mis-diagnosis is one of the most severe problems in healthcare (dia, April 17, 2014), inducing signiﬁcant harm to patients’ well-being. As reported by (Hardeep Singh, April 17, 2014), approximately 12 million adults are misdiagnosed in outpatient medical care, which amounts to 1 out of 20 adult patients. More importantly, around 40,000 - 80,000 patients die annually in the U.S.A due to diagnostic errors, as evidence from autopsies (dia, April 17, 2014) indicates.
While a plethora of factors can lead to mis-diagnosis, untimely diagnosis and lack of guidelines stand out as the most signiﬁcant factors (Anthony T. DiPietro, May 1, 2012). Many diseases such as asthma, chronic obstructive pulmonary disease and bronchiectasis, share similar symptoms such as dyspnea, coughing, wheezing and expectoration, with both
1

factors rendering their diﬀerential diagnosis a diﬃcult task. However, diﬀerentiating between these diseases in the early stage, such as at patient’s admission time, is extremely important since diﬀerent treatments could lead to very diﬀerent clinical outcomes, and the adoption of an improper treatment plan could prove to be disastrous for the patient’s health. Additionally, insuﬃciency of clinical guidelines for handling rare symptoms quite often can result in failure to correctly explain a patient’s symptoms. As a result, no suitable treatments are immediately administered to the patient.
The signiﬁcant improvement of Electronic Health Record (EHR) systems over the past decades has facilitated standardization of data collection by health-care professionals and allowed clinicians to access patients’ important and clinical information at their earliest convenience, preventing mis-diagnosis or delayed diagnosis. However, initial diagnostic decision-making based on EHRs still faces some challenges. The absence of a protocol for ﬁlling an EHR as well as the fact that an EHR usually contains information collected from multiple clinicians, with diﬀerent personalized writing styles who potentially work at different health institutions, results in EHRs not sharing the same length or writing style, missing structure, alternating between oﬃcial names, unoﬃcial names, medical names and abbreviations of diseases and containing various misspellings and grammar errors. According to a recent study (Sinsky et al., 2016), clinicians were found spending up to half of their total working time on EHR and desk work and less than a third of their time interacting directly with patients.
Convolutional Neural Network (CNN) models’ ability to identify patterns and learn appropriate latent representations from textual data has rendered them as one of the most powerful models for classiﬁcation. In this work, we investigated the potential of a CNN model to support the clinician’s diagnostic decision making, formulating discharge diagnosis prediction as a multiclass classiﬁcation problem. Our model takes as input only a subset of the information contained in a patients EHR upon admission and produces a discharge diagnosis prediction in its output. We trained a separate neural network in order to learn the embeddings (low dimensional real valued vectors that act as features) of the words in the vocabulary of our dataset and used the weights of this model to initialize the weights of the embedding layer of the CNN model. This is a critical optimization that further boosted the performance of the CNN model.
MIMIC-III is a freely available dataset containing clinical data of more than forty thousand patients who stayed in intensive care units of the Beth Israel Deaconess Medical Center between 2001 and 2012 (Johnson et al., 2016). After appropriate preprocessing, which included the application of the coreference resolution method on the disease names (seeks to ﬁnd the mentions in text that refer to the same real-world entity), extracting the clinical notes that correspond to the most frequent diseases (and ignoring the rest), discharge diagnosis prediction was formulated as a multiclass classiﬁcation problem, with the following 10 classes: coronary artery disease, hemorrhage, pneumonia, myocardial infarction, gastrointestinal bleeding, fracture, aortic stenosis, cardiac failure, prematurity and stroke. We evaluated the performance of our CNN model as well as other baseline classiﬁcation models such as Support Vector Machines (SVM), Random Forest (RF), Multi Layer Perceptron (MLP) and Logistic Regression (LR) on the MIMIC-III dataset using precision, accuracy, recall and F1 score metrics for each of the 10 diﬀerent diseases. Experiments indicate that our CNN model, achieving overall 96.11% accuracy and 80.84% weighted F1 score, outper-
2

Convolutional Neural Networks for Medical Diagnosis from Admission Notes
Figure 1: Demonstration of CNN model which consists of four components: embedding layer, convolutional layer, max-pooling layer, and fully-connected layer.
forms all of the baseline models with respect to 9 out of the 10 disease classes. Furthermore the F1 score value of 80.84%, at least 12% higher than that of the best of the baseline models, reﬂects that the CNN model performed equally well on all disease classes independently of how frequently they appear and are diagnosed in the MIMIC corpus. We hence believe it be a great tool for discharge diagnosis support.
2. Methods
2.1 CNN-based multi-class text classiﬁcation The discharge diagnosis prediction problem is cast as a multi-class text classiﬁcation
problem, with an admission note (after appropriate preprocessing) being fed into the input of a convolutional neural network, and classiﬁed by the latter into one of K primary discharge diseases. We now shed light on the diﬀerent parts of the models’ architecture, as depicted in ﬁgure 1.
The CNN among other things, learns an underlying representation of an admission note in a high-dimensional space. The architecture of the CNN model is depicted in Figure 1. The embedding layer maps each word in the admission note to its embedding, (a lowdimensional real-valued vector) and acts essentially as a lookup table. The embedding layers can be represented by a V × E matrix, where V is the number of diﬀerent words of the vocabulary considered and E the dimension of an embedding. The embedding layer outputs a L × E matrix for an admission note consisting of L words.
3

The convolutional layer consists of F independent ﬁltering operations, (F is usually 128 or 256) with each ﬁlter trying to extract diﬀerent type of information from the L × E embedding representation of the admission note. The i-th ﬁltering operation can be thought of as sliding a Hi × E matrix (ﬁlter) through the output of the embedding layer and computing its dot product with the corresponding Hi × E area of the admission note’s embedding representation. Performing this operation trivially, the i-th ﬁltering operation will output an (L − Hi + 1) dimensional vector; padding the output of the embedding layer by Hi − 1 rows of zeros, we ensure the i−th ﬁltering operation’s output is an L dimensional vector. The output of the convolutional layer results from stacking together these vectors to obtain a L × F matrix; this is the “feature representation” that the CNN is learning.
The output of the convolutional layer, an L × F matrix, is then fed into a max-pooling layer. The i-th max-pooling operation selects the maximum value of the i-th convolution (i-th column of the matrix). The output of the max-pooling layer is thus an F -dimensional vector, having a max-value per ﬁlter. A CNN usually contains ﬁlters of diﬀerent sizes that capture patterns across contexts of diﬀerent sizes (number of words). Our model uses an embedding of E = 128 and total of F1 = 64 ﬁlters of size H1 × E = 3 × 128, F2 = 64 ﬁlters of size H2 × E = 4 × 128 and F3 = 64 ﬁlters of size H3 × E = 5 × 128.
The F -dimensional output of the max-pooling layer enters a fully-connected layer. This is essentially a layer of F input neurons connected to all K output neurons. Complex coadaptations of the fully-connected layer’s weights on the training data can cause the CNN model to overﬁt. To prevent overﬁtting, individual neurons in the fully-connected layer are either kept with probability p or “dropped out” with probability (1 − p), a technique known as dropout.
A ﬁnal softmax layer converts the K-dimensional output of the fully-connected layer, x into a K-dimensional probability distribution vector π, applying essentially the normalized exponential function to x:

exj

πj =

K

, k = 1, 2, . . . K exk

(1)

k=1

Finally, the true class of a clinical note is represented as a “one-hot” K-dimensional vector y, with yi = 1 if i corresponds to the disease that was actually diagnosed and yi = 0 otherwise. The CNN is then trained, end-to-end, using the stochastic gradient descent algorithm in order to minimize the cross-entropy loss of y and π:

K

CE(y, π) = − yk log πk

(2)

k=1

2.2 Word embedding pre-training
We employed Skip-gram model (Mikolov et al., 2013) for training medical word embeddings for initializing the CNN-based text classiﬁcation model. This is beneﬁcial because the semantic information of clinical notes can be incorporated through the pre-trained embeddings of words from clinical notes. We trained the embedding model on the whole MIMIC III dataset where all data ﬁelds including admission information and non-admission information sush as brief hospital course, discharge instructions, discharge plan, discharge

4

Convolutional Neural Networks for Medical Diagnosis from Admission Notes

Figure 2: An example of disease coreference resolution of ST Segment Elevation Myocardial Infarction

medication, lab test during hospitalization are used. The Skip-gram model is designed to learn the probability distribution of words that appear closely in the corpus. The objective function is shown as follows:

1 T T logP (wt+j wt) (3)
t=1 −c≤j≤c,j=0
where the T is the number of words in the input sequence, c is the size of the training context. The conditional probability of a target word given current word is deﬁned by softmax function:

P (w w ) = exp(vTwOvwI )

(4)

t+j t

W w=1

exp(vTwO

vwI

)

where the W is the vocabulary size, and vwI and vwO are the input and output vector representations of word w.

In practice, we used fasttext (Joulin et al., 2016) to train the embedding model. Wrods

whose frequency is less than or equal to 1 were removed. During initializing of the classiﬁ-

cation model, we used embedding vectors of in-vocabulary words, and computed the vector

representations of out-vocabulary words using the function provided by the Fasttext.

2.3 Disease coreference resolution
Real-world clinical notes are characterized by lack of structure and the usage of different words referring to the same term. This is a result of doctors’ personalized writing styles as well as their tendency to alternate between oﬃcial names, abbreviations, unoﬃcial names and medical custom names when they refer to a particular term. Moreover, various misspellings and inconsistency in using lowercase/uppercase letters further increase the variety of diﬀerent words used to refer to the same term. Figure 2 illustrates an example where the terms “segment elevation myocardial infarction”, “stents elevation myocardial infarction”, “st-elevation myocardial infarction”, “st elevation myocardial infarction”, “st

5

elevated myocardial infarction”, “st segment elevation myocardial infarction”, “st elevation mi”, “st-elevation mi” and “stemi” all refer to the disease with oﬃcial name “ST Segment Elevation Myocardial Infarction”.
Coreference resolution, the task of ﬁnding all expressions that refer to the same entity in a text, is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction. Although there exists a plethora of machine learning algorithms for coreference resolution, both supervised and un-supervised, we used a remarkably simple, fast and still successful approach on the MIMIC dataset: we collected all discharge diagnosis disease names appearing after phrases such as “diagnosis”, “primary diagnosis” and: (i) manually grouped words that refer to the same disease, (ii) replaced all the words appearing in the groups by the name of the disease.
The MIMIC III dataset includes the International Statistical Classiﬁcation of Diseases and Related Health Problems (ICD-9) codes associated with the disease(s) of each clinical note. It is important to note that even though ICD-9 is very ﬁne-grained and contains the oﬃcial names of diseases and as many sub-diseases as possible, the clinical records reﬂect the doctors’ tendency to not closely follow ICD-9 terminology. In a preliminary experiment, we measured the classiﬁcation performance of our CNN model under two diﬀerent settings with respect to the classiﬁcation labels used: in the ﬁrst setting, we used the disease names that result from coreference resolution whereas in the second setting ICD-9 oﬃcial disease names. Given that the classiﬁcation accuracy was about 5% higher in the ﬁrst setting, we chose the disease names that resulted from our simple coreference resolution method as the classes for the multi-class classiﬁcation problem.
3. Experiments & Results
3.1 Data
3.1.1 Dataset
We evaluated the performance of the proposed CNN model using MIMIC-III, a freely available dataset that consists of clinical data for more than 40,000 patients who stayed in the intensive care units of the Beth Israel Deaconess Medical Center between 2001 and 2012 (Johnson et al., 2016). The dataset consists of more than 50,000 anonymized clinical records that include, among others, demographics, chief complaints, past medical history, vital signs, procedures, lab tests, medications and discharge diagnosis information. Given that our model tries to provide an accurate prediction of the discharge diagnosis by focusing solely on information available upon admission, we ﬁltered information such as chief complaints, past medical history, past surgical history, social history, family history, allergies, laboratory examination results upon admission and medications taken upon admission from the clinical records.
3.1.2 Data preprocessing
The ﬁltered information, initially in text format and containing abbreviations, misspellings and grammar errors, needed to undergo through several pre-processing tasks before it could be feeded into the input of the CNN model. These preprocessing tasks included
6

Convolutional Neural Networks for Medical Diagnosis from Admission Notes

disease Coronary artery disease Hemorrhage Pneumonia Myocardial infarction Gastrointestinal bleeding Fracture Aortic stenosis Cardiac failure Prematurity Stroke

number of samples 3193 1955 1634 1229 1158 1047 934 927 559 504

Table 1: Sample distribution of the 10 disease categories.

the concatenation of separate lines, the removal of duplicate spaces, the splitting of words by punctuation, the transformation of words to lowercase as well as the replacement of speciﬁc numbers, person names, hospital names, dates and times by “***” for the sake of anonymization. Finally, since the length (in number of words) was not unique among the diﬀerent clinical notes that resulted from the previous preprocessing tasks and the CNN model only operates on documents of a ﬁxed common length, an additional preprocessing task was carried out. This last preprocessing task involved the computation of a maximum document length as the maximum length (in number of words) not exceeded by 90% of the clinical notes that resulted from the previous preprocessing tasks, and a truncation at exactly this length for those preprocessed clinical notes whose length was higher than this threshold value.
3.1.3 Classiﬁcation categories
We applied coreference resolution on the MIMIC III dataset, resulting with 479 unique disease categories. We further analyzed the dataset and discovered the top 10 most frequent diseases, which we used as the labels for our classiﬁcation problem. In the case of a clinical note that mentions multiple diseases, only the ﬁrst one is counted, based on the assumption that the ﬁrst disease is the most important. Furthermore, we discarded clinical notes which did not mention at all any of these 10 diseases, ending up with a total of 13152 EHRs. The sample distribution of the 10 diseases is illustrated in ﬁgure 1 and table 1.
3.2 Evaluation Approach
For each individual disease category, we evaluated the model performance on the testing set keeping track of the following metrics: accuracy, true negative rate, false positive rate, false negative rate, precision, recall and F1 score. We additionally measured, for each of these metrics, a un-weighted and a weighted average value across the diﬀerent disease categories (where the weight wi for disease category i is simply the ratio of the clinical notes where disease i was diagnosed over the total number of clinical notes in the testing set. The weighted average values capture well the distribution of diagnosed diseases in the testing set.
7

Figure 3: Bar chart of sample distribution of the 10 disease categories. We can see that the sample distribution is unbalanced. More than 23% of samples drop in the ﬁrst category coronary artery disease while less than 4% of samples are in the last two disease categories which are prematurity and stroke.
3.3 Baseline Models
For multi-class disease classiﬁcation, we compare our CNN text classiﬁcation model against several baseline models: support vector machine (SVM), random forest (RF), multilayer perception (MLP) and logistic regression (LR). Tf-idf (term frequency-inverse document frequency) (Salton et al., 1975) is a common numerical statistic reﬂecting the importance of a word to a document in a corpus. For each clinical note, we ﬁrst create a V -dimensional vector (V being the size of our vocabulary) containing L tf-idf values for the words that appear in it and (V − L) zero values for the words that are absent. Therefore, for a set with T clinical notes we obtain a T × V feature matrix. Given the large value of V , we further apply the Principal Components Analysis (PCA) dimensionality reduction technique to obtain a T × V feature matrix with V V . The rows of this matrix are the actual clinical notes’ feature vectors used by each of the baseline classiﬁcation models during training and testing.
3.4 Implementation Details
We randomly split the MIMIC III dataset into training, validation and testing sets by a ratio of 7:1.5: 1.5. The best trained model is selected based on the performance on the validation set and tested on the testing set. For each model, we further trained 5 diﬀerent times with each using a diﬀerent random seed for splitting the dataset, and ended up with 5 evaluation results. The 5 evaluations results were averaged to represent the ﬁnal evaluation results for the corresponding model.
8

Convolutional Neural Networks for Medical Diagnosis from Admission Notes

Metric ACC TNR FPR FNR Precision Recall F1 WACC WTNR WFPR WFNR WPrecision WRecall WF1

SVM 93.71±0.37 96.59±0.20 3.41±0.20 21.61±0.25 60.81±1.58 72.39±2.53 60.86±1.50 92.59±0.14 96.59±0.21 3.41±0.21 23.38±0.93 68.56±1.86 72.21±1.39 66.08±1.23

MLP 91.54±1.69 93.44±2.76 4.56±0.77 29.48±5.78 49.02±9.87 50.52±12.22 47.17±10.94 88.49±3.56 90.26±5.46 4.90±0.64 30.87±3.34 57.68±8.46 53.18±11.90 53.15±11.01

LR 90.74±0.14 95.21±0.08 4.79±0.08 15.88±0.54 40.41±0.47 68.12±2.40 39.48±0.54 86.10±0.23 95.80±0.11 4.20±0.11 25.66±0.55 53.69±0.70 64.61±1.14 45.51±0.79

RF 93.69±0.13 96.41±0.07 3.59±0.07 32.91±0.49 64.45±0.58 67.09±0.49 65.04±0.61 92.22±0.17 95.73±0.08 4.27±0.08 31.96±0.69 68.43±0.64 68.04±0.69 67.77±0.72

CNN 96.11±0.08 97.78±0.05 2.22±0.05 20.92±0.62 78.72±0.67 79.08±0.62 78.45±0.50 95.29±0.08 97.13±0.10 2.87±0.10 19.06±0.36 80.55±0.42 80.94±0.36 80.48±0.41

Table 2: Evaluation metrics of convolutional-based text classiﬁcation (CNN), support vector machine (SVM) and random forest (RF), multilayer perceptron (MLP) and logistic regression (LR) models on diagnosis classiﬁcation. The evaluation metric ACC, TNR, FPR, FNR, Precision, Recall, F1 mean accuracy, true negative rate, false positive rate, false negative rate, precision, recall and F1 score respectively. They are computed by averaging the corresponding metric of individual classes. The same metric name with ”W” attached at the beginning denote the weighted average of that metric. The weights are sample weights in testing dataset. The standard errors computed by 5 set of results of each model using diﬀerent random splitting are appended after the metric values with a ± sign. We see that CNN model consistently outperforms baseline models on all measurements.

For our CNN model, we trained the Fasttext (Joulin et al., 2016) word embedding model (using embedding dimension E = 128) on the full MIMIC dataset and obtained an initialization of the embedding layer weights. We used a learning rate α = 0.0001, F1 = 64 ﬁlters of size H1 × E = 3 × 100, F2 = 64 ﬁlters of size H2 × E = 4 × 128 and F3 ﬁlters of size H3 × E = 5 × 100, dropout probability p = 0.5, the adam optimizer for gradient descent and trained on a single GPU. We used two layers with 100 and 10 neurons in each layer for MLP model, along with Relu activation functions and early stopping. We used a kernelized one-vs-all SVM with an RBF kernel and tolerance = 0.001.
3.5 Results on multi-class diagnosis classiﬁcation
Figure 4 shows the evaluation metrics including accuracy, average or weighted average true positive, false positive, false negative, precision, recall and F1. The exact values of these metrics are shown in Table 2.
9

Figure 4: Bar chart demonstrating the performance diﬀerence among SVM, random forest, MLP, logistic regression and CNN models on discharge diagnosis classiﬁcation. From the table, we can see that CNN model consistently outperform baseline models on all measurement metrics.
Figure 5: Bar chart demonstrating the F1 score diﬀerence on among SVM, random forest, MLP, logistic regression and CNN models on individual discharge diagnosis classiﬁcation. From the table, we can see that CNN model consistently outperform baseline models on 9 out of 10 diagnosis categories by mostly 10% to 15% margins.
10

Convolutional Neural Networks for Medical Diagnosis from Admission Notes

Model SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN SVM MLP LR RF CNN

Label 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7 7 7 7 8 8 8 8 8 9 9 9 9 9

ACC 87.41±0.46 75.56±12.84 64.08±0.66 87.40±0.32 92.63±0.27 93.87±0.24 91.69±1.43 91.49±0.24 92.10±0.28 95.37±0.28 93.10±0.82 90.87±1.36 91.96±0.20 91.80±0.29 94.96±0.20 92.61±0.56 89.76±0.71 90.81±0.22 93.14±0.24 95.16±0.27 95.28±1.88 93.12±0.93 93.60±0.20 96.31±0.16 98.34±0.09 96.14±0.22 93.88±0.97 94.31±0.26 94.54±0.11 96.60±0.16 93.02±0.29 91.72±0.40 92.60±0.18 91.65±0.09 94.50±0.38 95.29±0.11 94.49±0.63 92.96±0.13 94.42±0.19 96.95±0.16 90.96±3.98 95.99±0.12 96.19±0.16 96.54±0.15 97.76±0.23 99.44±0.04 98.29±0.68 99.37±0.05 98.96±0.10 98.83±0.06

TNR 97.00±0.20 74.86±18.72 99.73±0.11 93.47±0.19 94.64±0.44 97.72±0.25 94.90±2.17 97.66±0.15 95.79±0.16 97.40±0.14 96.60±0.72 94.31±2.00 94.57±0.30 95.96±0.24 97.43±0.30 93.12±0.84 93.50±1.12 90.81±0.22 96.03±0.27 97.18±0.22 98.42±0.61 96.50±1.31 93.55±0.20 97.96±0.14 99.09±0.13 97.35±0.31 95.47±0.96 94.60±0.31 96.29±0.14 98.26±0.30 93.31±0.45 94.18±0.42 92.60±0.18 95.40±0.12 97.32±0.16 95.77±0.07 95.59±0.93 92.98±0.13 95.90±0.19 98.15±0.09 97.04±0.33 96.27±0.19 96.19±0.16 97.41±0.13 98.34±0.12 99.55±0.03 98.78±0.68 99.37±0.04 99.90±0.05 99.94±0.03

FPR 3.00±0.20 5.14±1.35 0.27±0.11 6.53±0.19 5.36±0.44 2.28±0.25 5.10±2.17 2.34±0.15 4.21±0.16 2.60±0.14 3.40±0.72 5.69±2.00 5.43±0.30 4.04±0.24 2.57±0.30 6.88±0.84 6.50±1.12 9.19±0.22 3.97±0.27 2.82±0.22 1.58±0.61 3.50±1.31 6.45±0.20 2.04±0.14 0.91±0.13 2.65±0.31 4.53±0.96 5.40±0.31 3.71±0.14 1.74±0.30 6.69±0.45 5.82±0.42 7.40±0.18 4.60±0.12 2.68±0.16 4.23±0.07 4.41±0.93 7.02±0.13 4.10±0.19 1.85±0.09 2.96±0.33 3.73±0.19 3.81±0.16 2.59±0.13 1.66±0.12 0.45±0.03 1.22±0.68 0.63±0.04 0.10±0.05 0.06±0.03

FNR 32.66±1.02 36.31±9.94 60.22±0.59 29.16±0.68 13.93±1.26 25.26±1.56 21.39±5.93 34.88±1.25 28.87±1.43 16.10±1.71 24.85±5.84 26.03±7.36 28.05±1.56 33.08±1.38 20.83±1.64 16.33±4.43 46.26±12.21 4.00±4.00 36.51±1.39 23.35±3.13 20.89±10.60 31.47±8.98 4.41±0.72 20.92±1.19 9.02±1.56 20.33±3.08 28.15±9.10 15.11±2.45 31.71±2.48 21.74±1.02 12.15±7.53 47.28±11.88 0.00±0.00 56.62±0.99 40.41±2.32 16.10±2.12 23.75±6.90 11.34±3.33 32.83±1.93 20.91±1.47 44.30±18.58 25.59±16.73 0.00±0.00 41.53±2.39 22.20±5.28 3.18±0.59 8.52±6.47 0.75±0.36 17.81±1.46 20.72±1.34

Precision 91.44±0.61 83.79±4.29 99.55±0.17 79.85±0.66 82.86±1.41 86.54±1.71 66.27±16.70 86.66±0.97 74.81±1.35 85.00±0.76 77.20±5.20 60.56±15.27 63.38±1.22 73.41±1.95 82.13±2.39 28.36±8.11 34.43±11.69 0.92±0.25 61.03±2.22 73.68±2.40 83.34±6.79 61.83±15.86 28.50±1.02 78.82±1.58 91.06±1.20 67.89±4.18 43.34±13.30 32.81±1.38 55.16±1.02 79.88±3.83 10.38±6.45 24.43±7.14 0.00±0.00 42.28±0.72 63.55±2.63 44.92±0.91 41.76±13.25 5.38±0.54 47.27±2.70 74.07±1.54 27.64±9.79 2.95±1.65 0.41±0.25 33.95±1.20 56.33±3.00 90.40±0.72 70.87±17.79 86.48±0.95 97.87±1.09 98.63±0.60

Recall 67.34±1.02 63.69±9.94 39.78±0.59 70.84±0.68 86.07±1.26 74.74±1.56 58.61±14.87 65.12±1.25 71.13±1.43 83.90±1.71 75.15±5.84 53.97±13.92 71.95±1.56 66.92±1.38 79.17±1.64 83.67±4.43 33.74±9.29 96.00±4.00 63.49±1.39 76.65±3.13 79.11±10.60 48.53±12.88 95.59±0.72 79.08±1.19 90.98±1.56 79.67±3.08 51.85±14.19 84.89±2.45 68.29±2.48 78.26±1.02 27.85±17.09 32.72±8.27 0.00±0.00 43.38±0.99 59.59±2.32 83.90±2.12 56.25±14.50 88.66±3.33 67.17±1.93 79.09±1.47 55.70±18.58 34.41±19.19 40.00±24.49 58.47±2.39 77.80±5.28 96.82±0.59 71.48±18.88 99.25±0.36 82.19±1.46 79.28±1.34

F1 77.54±0.71 68.96±7.52 56.84±0.60 75.07±0.55 84.37±0.60 80.09±0.58 61.97±15.54 74.34±1.04 72.92±1.34 84.39±0.84 74.63±1.74 56.90±14.42 67.31±0.65 69.99±1.50 80.43±0.38 38.90±8.09 33.17±10.27 1.81±0.49 62.15±1.47 74.79±1.11 77.68±5.86 54.32±14.18 43.87±1.24 78.92±1.16 90.95±0.47 72.88±2.46 46.93±13.49 47.24±1.45 60.98±1.52 78.85±1.86 15.02±9.25 27.27±7.18 0.00±0.00 42.80±0.73 61.29±1.77 58.49±1.14 46.82±13.46 10.12±0.95 55.41±2.48 76.50±1.51 19.85±5.18 4.67±2.47 0.81±0.50 42.85±1.15 65.04±3.42 93.49±0.42 70.62±17.96 92.42±0.61 89.33±1.13 87.88±0.88

Table 3: Evaluation metrics on individual disease classiﬁcation. The ten class labels correspond to coronary artery disease, hemorrhage, pneumonia, myocardial infarction, gastrointestinal bleeding, fracture, aortic stenos, cardiac failure, prematurity and stroke. The evaluation metrics ACC, TNR, FPR, FNR refer to accuracy, true positive rate, false positive rate, false negative rate. The mapping between label indices and diseases are: 0-coronary artery disease, 1-hemorrhage, 2-pneumonia, 3-myocardial infarction, 4-gastrointestinal bleeding, 5-fracture, 6-aortic stenosis, 7-cardiac failure, 8-prematurity, 9-stroke. The standard errors computed by 5 set of results of each model using diﬀerent random splitting are appended after the metric values with a ± sign.

11

3.6 Results on individual diagnosis classiﬁcation
The measurement results on individual diagnosis categories are also evaluated and shown in Table 3. A bar chart comparing F1 score among the three models is also provided in Figure 5.
3.7 T-test on performance across models
In addition, a t-test has also been done between the weighted average F1 score of the CNN and each of the baseline models, assuming the variances are not equal. The p-values of weighted average F1 metric value of CNN model with that of SVM, random forest, MLP and logistic regression are 1.20e-4, 3.05e-06, 6.81e-2, 1.81e-08 respectively. These values are much less than 0.5, indicating signiﬁcant diﬀerence between the respective performance metrics.
3.8 CNN ﬁlter visualization
To visualize the patterns learned by each ﬁlter category in the convolutional layer of the proposed CNN model, we ranked all phrases in the testing dataset which are scanned by the ﬁlter window of convolutional layer by their activation scores in descending order. The top 10 phrases of randomly selected 2 ﬁlters per ﬁlter size from 3 to 5 is shown in Table 4.
4. Discussion
4.1 Overall results
The CNN model we presented achieves, to the best of our knowledge, state-of-the-art prediction performance in discovering complex patterns in unstructured clinical notes for diagnosis decision making. Of the models evaluated, the best performance was achieved with the proposed CNN model that uses pretrained word embeddings and disease coreference resolution. The proposed model achieved 96.11% accuracy and 80.48% weighted average F1 score which is around 13% higher than the best result from baseline models. The proposed model also signiﬁcantly outperformed traditional machine learning models that rely on bag-of-word features and the PCA feature dimensionality reduction technique. The result suggests the CNN model is suitable for supporting diagnosis decision making in the presence of complex, noisy and unstructured clinical data while at the same time using less layers and parameters that other traditional Deep Network models.
In addition, CNN model achieved close to 80% precision and recall, while all baseline models have less than 73% values and are struggling with doing equally well on both measurements. Having relatively equally good performance in precision and recall is important in medical supporting systems since not only the majority diseases should be detected early and accurately, but also the rare diseases need to be identiﬁed timely and with conﬁdence. Sometimes, it is even more important to detect rare diseases since it is less likely to detect them and a wrong treatment plan could put the patients’ health or even life at risk.
Besides, it is interesting that the Logistic Regression model achieved the minimum false negative rate, something that came at the expense of having the lowest precision value among all models. Intuitively, the model tends to predict a sample with majority category
12

Convolutional Neural Networks for Medical Diagnosis from Admission Notes

ﬁlter 1 of trigram aortic stenosis dr aortic stenosis noted aortic stenosis referred aortic stenosis bicuspid aortic stenosis hepatitis aortic stenosis valve aortic stenosis followed aortic stenosis most aortic stenosis treated aortic stenosis coronary ﬁlter 1 of 4-gram cesarean section delivery membranes spontaneous vaginal delivery in prior to delivery besides by vaginal delivery one term vaginal delivery surgically spontaneous vaginal delivery required section at delivery infant induced vaginal delivery apgar spontaneous vaginal delivery mother delivery vaginal delivery apgars ﬁlter 1 of 5-gram was stable without chest pain catheterization after developing chest pain to have substernal chest pain ﬂow she had chest pain patient began having chest pain her bms no chest pain who complained of chest pain and developed sharp chest pain onset heavy substernal chest pain st elevation improved chest pain

ﬁlter 2 of trigram hyperlipidemia degenerative joint hyperlipidemia obesity tobacco hyperlipidemia s p hyperlipidemia not known hyperlipidemia complete heart hyperlipidemia percutaneous coronary hyperlipidemia niddm tobacco hyperlipidemia obesity diabetes hyperlipidemia aspirin allergy hyperlipidemia hypertension tobacco ﬁlter 2 of 4-gram impending respiratory failure injuries developed respiratory failure requiring hypoxic respiratory failure successfully hypoxic respiratory failure discharged use respiratory failure non copd respiratory failure requiring mother respiratory failure hepatitis hypoxic respiratory failure a pancytopenia respiratory failure s hypoxic respiratory failure transferred ﬁlter 2 of 5-gram multiple loose watery bm stool small amount of loose stool noticed red blood around stool her last semi formed stool guaiac positive with red stool have black guaiac positive stool had a well formed stool and passed dark black stool for presumed infectious colitis stool dark brown well formed stool

Table 4: Top 10 3-grams, 4-grams, and 5-grams ranked by activation scores by convolutional ﬁlters in the proposed CNN model in descending order. 2 ﬁlters per ﬁlter size from 3 to 5 are selected.

13

if it is not very conﬁdent about whether the sample belongs to a rare class or not, increasing the likelihood of predicting correctly samples labeled with a rare disease class.
4.2 Individual results
According to table 3, the CNN model achieved the highest F1 score on all 10 disease classes. Furthermore, although some models have relatively high F1 score in categories with large sample size (e.g., coronary artery disease, hemorrhage, pneumonia), they performed badly on categories with less samples (e.g., myocardial infarction, aortic stenosis, prematurity). For example, the SVM achieved an F1 score value of 80.09% for the hemorrhage class, only around 4% lower than the F1 score value of the CNN model, it was only able to achieve F1 score values of 15.02% on aortic stenosis and 19.85% on prematurity. On the contrary, the CNN model performed equally well on all disease classes independently of frequently they appear and are diagnosed in the MIMIC corpus.
Table 3 also indicate that aortic stenosis and prematurity are the hardest disease classes to classify. The highest F1 score values achieved from any of the baseline models for aortic stenosis and prematurity are 42.80% and 42.85% respectively. Among all the baseline models, the logistic regression and the MLP model achieved less than 5% F1 score for the prematurity class. On the other side, the CNN model still achieved F1 score values of 61.29% and 65.04% on the same diseases. This fact further conﬁrms that the proposed CNN model works remarkably well at classifying rare (not appearing frequently in the corpus) diseases.
The Logistic Regression model had the worst performance among all models with respect to most evaluation metrics. Being essentially a linear model, Logistic Regression usually works well with data that can be separated by a hyperplane. However, the latent features of the clinical notes are complex and are very likely not linearly separable. This explains the poor performance of the Logistic Regression model on the discharge diagnosis prediction problem.
4.3 Understanding the model features
Last but not least, Table 4 shows the top 10 phrases ranked by activation scores by convolutional ﬁlters in CNN model. We can see from the table that each ﬁlter tends to detect a speciﬁc pattern. For example, the ﬁrst H1 × E = 3 × 128 ﬁlter detects phrases containing the words “aortic stenosis”, while the second H1 × E = 3 × 128 ﬁlter detects phrases highly related to ”hyperlipidemia”. While 3-grams ranked with the highest activation scores by convolutional ﬁlters tend to preserve certain words or phrases, 4-grams and 5-grams assigned with high activation scores, despite very centered around a speciﬁc symptom or medical event with diﬀerent descriptions, are more ﬂexible. For example, the ﬁrst H3 × E = 5 × 100 ﬁlter of 5-gram describes chest pain with various conditions such as substernal chest pain, sharp chest pain, onset heavy substernal chest pain and st elevation improved chest pain. We conclude that the convolutional ﬁlters or our CNN model are indeed extracting critical patterns for diagnosis, such as symptoms, lab tests, diseases, procedures and abnormalities. It is quite impressive how the CNN model learns to mimic the human clinician’s procedure diagnosis decision making.
14

Convolutional Neural Networks for Medical Diagnosis from Admission Notes
5. Future work
A ﬁrst direction of future work is to train a CNN model that can distinguish among more than 10 diseases. The fact that in this work our CNN model uses only 10 classes (corresponding to the 10 most frequently mentioned diseases in the MIMIC corpus) does by no means imply that it is not generalizable. We still have to take care of solving the problem of some diseases appearing and being diagnosed signiﬁcantly more than others when training our model with more than 10 classes. Towards this end, we can use a slightly diﬀerent loss function, where the parts of the loss pertaining to diﬀerent diseases are weighted inversely proportional to the frequency of the respective diseases (penalizing mis-predictions of rare diseases more than mis-predictions of common diseases) and the total loss is a sum of these weighted losses.
A second direction of future work is to extend our CNN model so that it performs multi-label classiﬁcation, predicting more than one diseases per clinical note or multi-task classiﬁcation, predicting, in addition to the disease itself, signiﬁcant factors such as mortality possibility or severity level of the disease.
A third direction of future work would be to extend our CNN model so that it supports hierarchical disease prediction. A model that in addition to predicting pneumonia can specify the speciﬁc type of pneumonia such as aspiration pneumonia, bacteria pneumonia, hospital-acquired pneumonia, or community-acquired pneumonia for example, could provide a much more useful tool for diagnosis decision making. The performance and success of such a model that supports hierarchical disease prediction highly depends on training on a large corpus that contains adequate samples per disease type and subtype.
A fourth direction of future work is to train our model on a much larger clinical notes corpus, hoping that the beneﬁts of using pretrained word embeddings for the initialization of the CNN model will become more evident. The word embeddings used by our model were trained on the MIMIC dataset that contains only 50000 documents, whereas state-of-the-art word-embedding models are usually trained on millions and billions of documents.
6. Conclusion
We have presented a novel data-driven technique for diagnosis prediction, that exploits convolutional neural networks’ ability to learn latent features. Unlike many existing works, such as (Bond et al., 2012), (Grady and Berkowitz, 2011), (Ebell, 2010), and (Achour et al., 2001) we did not use any human designed rules (based on prior medical knowledge) for clinical notes’ feature learning. Our approach is ﬂexible in the sense that it can be easily adapted to other datasets or employed in various clinical settings where data availability, characteristics, format and statistical distribution of text vary. Moreover, the fact that it is only based on a subset of the information that is available upon admission time, allows our method to integrate well with the clinical setting workﬂow and provide timely feedback to the clinician. The eﬃciency is further improved by the fact that our models can be trained end-to-end, without speciﬁc need for ﬁne-tuning hyperparameters of individual components.
References
About diagnostic error. Society to improve diagnosis in medicine, April 17, 2014.
15

Soumeya L Achour, Michel Dojat, Claire Rieux, Philippe Bierling, and Eric Lepage. A umls-based knowledge acquisition tool for rule-based clinical decision support system development. Journal of the American Medical Informatics Association, 8(4):351–360, 2001.
Esq. Anthony T. DiPietro. Types of medical diagnostic errors. Medical Malpractice: Misdiagnosis and Delayed Diagnosis, page 2, May 1, 2012.
William F Bond, Linda M Schwartz, Kevin R Weaver, Donald Levick, Michael Giuliano, and Mark L Graber. Diﬀerential diagnosis generators: an evaluation of currently available computer programs. Journal of general internal medicine, 27(2):213–219, 2012.
Mark Ebell. Ahrq white paper: use of clinical decision rules for point-of-care decision support. Medical Decision Making, 30(6):712–721, 2010.
Deborah Grady and Seth A Berkowitz. Why is a good clinical prediction rule so hard to ﬁnd? Archives of internal medicine, 171(19):1701–1702, 2011.
Eric J Thomas Hardeep Singh, Ashley N D Meyer. The frequency of diagnostic errors in outpatient care: estimations from three large observational studies involving us adult populations. BMJ Quality & Safety, April 17, 2014.
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientiﬁc data, 3, 2016.
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H´erve J´egou, and Tomas Mikolov. Fasttext.zip: Compressing text classiﬁcation models. arXiv preprint arXiv:1612.03651, 2016.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.
Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620, 1975.
Christine Sinsky, Lacey Colligan, Ling Li, Mirela Prgomet, Sam Reynolds, Lindsey Goeders, Johanna Westbrook, Michael Tutty, and George Blike. Allocation of physician time in ambulatory practice: A time and motion study in 4 specialties. Annals of Internal Medicine, 165(11):753–760, 2016.
16

