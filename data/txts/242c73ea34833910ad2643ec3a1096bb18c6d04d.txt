IMPROVING RNN TRANSDUCER WITH TARGET SPEAKER EXTRACTION AND NEURAL UNCERTAINTY ESTIMATION
Jiatong Shi ∗, Chunlei Zhang†, Chao Weng†, Shinji Watanabe , Meng Yu†, Dong Yu†
Johns Hopkins University, USA † Tencent AI Lab, Bellevue WA, USA
{jiatong shi, shinjw}@jhu.edu, {cleizhang, cweng, raymondmyu, dyu}@tencent.com

arXiv:2011.13393v2 [cs.SD] 26 Feb 2021

ABSTRACT
Target-speaker speech recognition aims to recognize target-speaker speech from noisy environments with background noise and interfering speakers. This work presents a joint framework that combines time-domain target-speaker speech extraction and Recurrent Neural Network Transducer (RNN-T). To stabilize the joint-training, we propose a multi-stage training strategy that pre-trains and ﬁne-tunes each module in the system before joint-training. Meanwhile, speaker identity and speech enhancement uncertainty measures are proposed to compensate for residual noise and artifacts from the target speech extraction module. Compared to a recognizer ﬁne-tuned with a target speech extraction model, our experiments show that adding the neural uncertainty module signiﬁcantly reduces 17% relative Character Error Rate (CER) on multi-speaker signals with background noise. The multi-condition experiments indicate that our method can achieve 9% relative performance gain in the noisy condition while maintaining the performance in the clean condition.
Index Terms— Target-Speaker Speech Recognition, TargetSpeaker Speech Extraction, Uncertainty Estimation
1. INTRODUCTION
Target-speaker speech extraction is a process to separate speciﬁc speaker’s speech from a speech mixture by eliminating other interfering speakers and noises. Conventional methods for the problem include anchored speech detection [1, 2] and speaker-speciﬁc training [3, 4]. However, both methods have their limitations when dealing with the problem. The ﬁrst can only handle non-overlap speech, while the second cannot be applied to unseen speakers without re-training of the entire network. State-of-the-art methods adopt a couple of seconds of enrollment target-speaker speech to extract a target-speaker embedding and employ it as auxiliary information to enhancement and separation networks [5–10].
One of the applications of target-speaker speech extraction is to use it as a prepossessing module of Automatic Speech Recognition (ASR) systems. Previous literature investigated the target-speaker speech extraction with different back-end ASR architectures. In [8], Wang et al. adopted a word-based Connectionist temporal classiﬁcation recognizer. HMM-DNN (Hidden Markov Model-Deep Neural Networks) was employed in [5–7]. Wang et al. tested the VoiceﬁlterLite with an RNN-T [10]. Aforementioned works mainly investigated the Time-Frequency (TF) domain target speech extraction. Zˇ mol´ıkova´ et al. have studied the joint-training of target speaker extraction and HMM-based ASR [6, 7]. Others, however, employed a pre-trained recognizer without joint-training.
∗Jiatong Shi performed the work during his internship at Tencent AI Lab.

On the other hand, in the other robust ASR ﬁeld including speech enhancement/separation, joint training becomes a popular methodology to make a system robust against the mismatch between the preprocessing and ASR modules [11–17]. Similar to target speaker ASR, most previous studies investigated the jointtraining on the TF domain. However, in recent work, Neumann et al. showed that the joint-training with time-domain speech separation and encoder-decoder-based ASR could signiﬁcantly elevate the performances on multi-talker speech recognition [18].
Given this trend, this paper proposes to subsume time-domain target-speaker speech extraction into an RNN-T end-to-end ASR system. Practically, RNN-T end-to-end speech recognizer is a good candidate for on-device scenarios because of its small system footprints. Also, the integration of the target speaker extraction module in this on-device system can avoid the requirement for users to upload their enrollment voice, which greatly mitigates the concern of users’ privacy. However, one of the important problems for this joint training is how to compensate for residual noises and artifacts introduced from the extraction process. To deal with this issue, we leverage a neural network based speech enhancement and speaker identity uncertainty estimation [19,20] for the joint system. This uncertainty information is then used as a feature for ASR training. Our empirical results show that the joint-framework achieves signiﬁcant improvement over the pipeline-based baseline. Speech enhancement and speaker identity uncertainties are also shown to contribute to the system, respectively. In the meanwhile, multi-condition experiments suggest that our system can preserve comparable to or better results than the pipeline-based baseline for both clean and noise scenarios.

2. TARGET-SPEAKER SPEECH RECOGNITION

2.1. Time-domain Target-Speaker Speech Extraction
As shown in the Fig. 1a, we follow the architecture in [9] for timedomain target-speaker speech extraction. The system ﬁrst extracts target speaker embedding etarget (i.e., a ﬁxed-dimension vector) from enrollment speech senroll.

etarget = EnrollEmbedder(senroll)

(1)

The enroll embedder is a Time-Delay Neural Network (TDNN). The training of the embedder adopts multi-task objective regarding etarget, and they are combined with scaling factors α and β as follows:

Lspk = Ltriplet(etarget) + α · Llmc(etarget) + β · Lr(etarget) (2)

where Ltriplet is the triplet loss [21,22], Llmc is the large margin cosine loss, and Lr is the L2 regularization term . For the next step, the

Target Speaker Enrollment

Enroll Embedder
Noisy Speech Mixture

Enhanced Target Speech

Audio Encoder

Feature Fusion

Target Mask

Audio Decoder

(a)

MFCC Instance Norm
Conv1D

Enhanced Target Speech Feature
Extraction
RNN Encoder

RNN Decoder

Joint Network
Transcription (b)

Target Speaker Enrollment
Enroll Embedder

Noisy Speech Mixture
Mixture Embedder

Target Speaker Enrollment
Enroll Embedder

Noisy Speech Mixture

Target Speech Extractor

Joint-train with uncertainty module

Enhanced Target Speech

Target Speech Extractor

Speaker Uncertainty

Speech Uncertainty

RNN-T Transcription

RNN-T Transcription

(c)

(d)

Fig. 1. Architectures in this Paper: (a) is the target-speaker speech recognition model discussed in Section 2.1; (b) is RNN-T recognizer introduced in Section 2.2; (c) is a joint-network of (a) and (b). Its training strategy is shown in Section 2.3; (d) is the joint-model with speaker identity and speech enhancement uncertainties, which is introduced in Section 3.

extractor produces the target speech ˆstarget from speech mixture smix conditioned on etarget obtained from Eq. (1), as follows:

ˆstarget = Extractor(smix|etarget)

(3)

The architecture of the extractor, shown in Fig. 1a, adopts from the Conv-TasNet [23] consisting of an audio encoder, a feature fusion module, a prediction module for target mask, and an audio decoder to reconstruct the signal. The encoded information from the audio mixture and speaker embedding etarget are fused in a feature fusion layer. The objective of the extractor is the Scale-Invariant Signal-toNoise Ratio (SI-SNR) that computes between enhanced speech ˆstarget and ground truth target speech starget.

2.2. Recurrent Neural Network Transducer

End-to-end ASR models have received considerable interests in re-

cent years [24–26]. These models have a relatively small system

footprint than HMM-based systems, which allows it to be easier

adapted to on-device scenarios. However, one drawback of these

end-to-end architectures is that they mostly attend to whole utter-

ances for decoding. Thus, they cannot work for real-time streaming

recognition. The end-to-end RNN-T [27], on the other hand, has

shown its effectiveness for real-time decoding [28]. To keep the

streaming feature possible in future deployment, we construct our

system on RNN-T.

Unlike other joint-framework for robust ASR, our system ac-

cepts the time-domain audio signal instead of time-frequency (TF)

domain features. As shown in Fig. 1b, we add a wrapper network.

It ﬁrst converts the input into the Mel-Frequency Cepstrum Coefﬁ-

cients (MFCC) and then aggregates the features with a ﬁxed context

using convolution operation. For our target speaker recognition, we

ﬁrst feed target speech ˆstarget into the warpper layer “FE(·)” to ex-

tract features and then input the features to RNN-T to predict the

transcription Wˆ

Wˆ = RNN-T(FE(ˆstarget))

(4)

Note that the warpper layer “FE(·)” is differentiable and we can perform the back-propagation algorithm for all modules jointly.

2.3. Multi-stage Training Strategy

an enrollment embedder, target speech extraction, and speech recognizer modules. Given the complexity of the framework, our pilot study indicated that it is difﬁcult to directly train a joint-network that combines all the modules. Therefore, a multi-stage training strategy is proposed to stabilize this joint-training.
In the ﬁrst stage, the enrollment embedder is trained on a speaker veriﬁcation task as Eq. (1). Then, it is jointly trained with the target speech extractor in Eq. (3). The extractor’s loss follows the objective function deﬁned in [9], which is an interpolation of the speakerrelated loss Lspk and SI-SNR loss. In parallel, an RNN-T recognizer shown in Fig. 1b and Eq. (4) is pre-trained on a corpus with only clean speech. As shown in Fig. 1c, the two pre-trained systems are combined together. In the ﬁrst, we freeze the parameter sets in target speech extraction networks and ﬁne-tune the pre-trained RNN-T with only RNN-T objective. After the above system converges, we joint-train the whole framework with all three modules. The ﬁnal stage is trained with the multi-task objective deﬁned as:
Ljoint = LRNN-T(Wˆ , W ) + γ(LSI-SNR(ˆstarget, starget) + φLspk), (5)
where φ and γ are weight hyper-parameters. Lspk is inherited from the speaker veriﬁcation task to stabilize the speaker embedding extraction. It is a simpliﬁed version from Eq. (2) that only contains Ltriplet or Llmc1.
3. NEURAL UNCERTAINTY ESTIMATION
Speech enhancement and separation modules are leveraged to mitigate the speech distortion due to the noise and interfering speakers. However, the enhanced signals are usually not ideal compensation, since they often contain residual noises and artifacts introduced from the algorithms. In conventional statistical models like Gaussian Mixture Models (GMM), the statistical formulation facilitates the development of probabilistic uncertainty-of-observation (UoO) methods [19, 20]. It considers the enhanced speech predictions as random variables rather than deterministic estimations. The following works extended the uncertainty processing to DNNs [29–31]. Rather than using the uncertainty propagation, this work focuses on a more straightforward way that encodes uncertainty information as additional features to the following RNN-T speech recognizer.

As shown in Fig. 1c, the framework of our target-speaker speech recognition systems cascadingly consists of three main components:

1Our empirical tests showed that the two losses have a similar effect to the model.

3.1. Speaker Identity Uncertainty
Even though the target speech extraction module suppresses speech from interfering speaker, the enhanced speech often contains residual noises, especially from overlapped speech [7, 8]. This indicates that the recognizer should process overlap and non-overlapped speech differently. To explicitly inform the distinct information between overlap and non-overlapped speech, we propose to use an entropy computed from a speaker identiﬁcation distribution entropy as additional features.
We ﬁrst introduce a mixture embedder for noisy mixtures. The network is a convolutional neural networks (CNN) with a linear projection layer. The input of the mixture embedder is Short Time Fourier Transform (STFT) frames of smix. The output of the linear projection layer is frame-wise mixture embeddings emix(t) as follows

emix(t) = MixtureEmbedder(STFT(smix)[t])

(6)

where t is the frame index. A further linear layer and a softmax layer then convert emix(t) into each speaker’s probability. The speaker identity uncertainty at frame t is deﬁned using the entropy as fol-

lows:

Uspk(t) = − p(k, t) · log p(k, t)

(7)

k∈S

where k stands for a speaker ID, S is a set of speaker IDs in the training corpus. To explicitly train the mixture embedder, we employ a cross-entropy loss and interpolate it when training the target speech extraction model shown in Eq. (3). Inspired by [32], we also use the mixture embedding emix(t) as a speaker embedding of the mixture. After mean-pooling, the embedding is fed into the feature fusion module discussed in Section 2.1 with target speaker embedding.

3.2. Speech Enhancement Uncertainty

Three uncertainty estimators are frequently used in uncertainty estimation for enhanced speech, including oracle uncertainty (OU) estimator [33], Kolossa’s uncertainty (KU) estimator [29, 31], and Neural network-based uncertainty (NNU) estimator [30,33]. The OU estimator is the best possible uncertainty estimator. The OU for frame t is deﬁned as follows:

OU(t) = (y(t) − yˆ(t))2

(8)

where y and yˆ(t) stands for clean and enhanced speech features (e.g., MFCC of starget and ˆstarget), respectively. As the OU estimator needs information of clean speech, it is not available for practical usage. NNU estimators are the state-of-the-art methods that train neural networks to predict the OU. As shown in [30], they achieve comparable performance to the OU estimator.
NNU estimators in previous works were often using DNN architectures and were trained separately using noisy & enhanced speech features (e.g., MFCC) as inputs and OU as targets [30, 33]. In this work, we extend the NNU estimator to a convolutional neural uncertainty (CNU) estimator that jointly trains the NNU estimator with the target-speaker speech recognition system discussed in Section 2.3. The CNU contains an InstanceNorm layer, a convolutional layer, a linear layer with the Leaky-ReLU activation function, and a prediction layer. The CNU is applied to the system as follows:

OˆUnorm = CNU(yˆ, (ymix − yˆ)2)

(9)

where ymix is the mixture speech features, the inputs of the CNU are enhanced speech features yˆ and a distance (ymix − yˆ)2, the prediction target is the OUnorm .2 We adopt L1 loss as the training objective
of CNU. We use the hidden states of the linear layer as speech en-
hancement uncertainty features Uspeech and concatenate it with other
recognizer features. Our empirical studies showed that using the hidden states instead of the OˆUnorm, is more stable in training.

3.3. Proposed System with Uncertainty
Our proposed system with uncertainty training is shown in Fig. 1c, including three steps. In the ﬁrst step, the system extracts a mixture embedding and an enrolled embedding. The mixture embedding is extracted from noisy speech mixture using a mixture embedder discussed in Section 3.1. The enrolled embedding is from enrollment speech shown in Section 2.1. Then, the two embeddings’ concatenation is fed into the target speech extractor with a noisy speech mixture for the second step. The output of this step is the enhanced time-domain speech signal. For the last step, we extract speaker identity uncertainty using the mixture embedder’s probabilistic distribution in 3.1 and speech enhancement uncertainty features using CNU proposed in 3.2. We then concatenate the uncertainty measures with the frequency features (i.e., MFCC) of the enhanced speech and feed it into RNN-T.
The model is also trained in a multi-stage manner. First, the enroll embedder and RNN-T for clean speech recognition are trained separately. Next, they are subsumed into the target-speaker speech extraction for joint-training.3 At last, we freeze the target speech extraction network and ﬁne-tune the pre-trained RNN-T system but with two additional uncertainty features. Noted that the CNU for Uspeech is also trained in the last stage. The objective at this stage is with multi-task learning as follows:

Ljoint2 = LRNN-T + LCNU

(10)

Our experiments found that the model with uncertainty did not show signiﬁcant improvements after joint-training as a whole model. Therefore, we do not additionally join-train the whole model.

4. EXPERIMENTS
4.1. Dataset
For the training of enroll embedder, a subset of King-ASR-216 and King-ASR-210 is chosen for pre-training. The pre-processing is the same as in [9]. The other experiments are trained on AISHELL2 [34] and its noisy simulation corpus. For each utterance, we mix it with utterances from other speakers at a randomly signalto-interference ratio (SIR) in {0dB, 6dB, 12dB}. The number of interfering speakers in each mixture is randomly selected from {0, 1, 2}. Environmental noise is added to the mixture as well with signal-to-noise ratio (SIR) randomly ranged from {6dB, 12dB, 18dB, 24dB, 30dB}. We adopt the same noise set as [9], which including several ambient noises from “daily life” environments. Different room impulse responses are considered in the simulation. All the above samplings are with equal chance. The ﬁnal simulation corpus contains 963,469 utterances for training, 20,347 utterances for development, and 25,382 utterances for testing. 4
2We use InstanceNorm for OU normalization. 3The mixture embedder also trains at this step. 4Noted that the speakers in each subset are not overlapped.

4.2. Network and Training Settings
For our proposed model, the enroll embedder and the target-speaker speech extraction module adopted the same conﬁguration as [9]. The RNN-T system has four encoder layers and two decoder layers. Each layer is a bi-directional LSTM layer and contained 512 hidden states with a 0.2 dropout rate. The MFCC layer in Fig. 1b uses 25ms window length, 10ms window shift, 512-point FFT, and 40 Mel bins. The Conv1D shown in Fig. 1b has a kernel size of 7 and stride of 3 for feature down-sampling. The CNU discussed in Section 3.2 adopts a Conv1D layer with 128 output channels and 11 kernel size but without stride. The following FC layer (i.e., the speech enhancement uncertainty features) has 32 dimensions. For all the stages, we adopt Adam optimizer. For most stages, the initial learning rate is 0.001, and it is annealed by half if no improvement is observed for the next epoch. However, for joint training, each component is attached with optimizers of different learning rates: the initial learning rates ware set to 1e-5 for RNN-T, 1e-7 for target speech extractor, 2e-7 for the enroll embedder. α and β in Eq. (2) are set to 0.2 and 0.001. γ and φ in Eq. (5) are set to 0.01 and 1.0.
We conduct the experiments in two folds. For the ﬁrst (Noisy Training), we only train and test our model in noisy settings. We compare seven models: Model I that is trained on original AISHELL-2 using the model in Section 2.2, Model II that directly adopts the pre-trained Clean RNN-T and target speech extraction module, Model III that freezes the target speech extraction module and ﬁne-tunes the Clean RNN-T, Model IV that jointly trains the whole network from model II, Model V, VI, VII ﬁne-tuned RNN-T and froze the target speech extraction module with speaker identity (Uspk), speech enhancement (Uspeech), and both (Uboth) uncertainty measures, respectively. For the second fold (Multi-condition Training), we present the stability of the system with multi-condition (i.e., clean and noise) training. Four models including III, V, VI, VII are trained while Model I is presented as references. We employ an on-the-ﬂy loader that randomly fed clean and noisy speech for model training. As clean utterances introduce artifacts for uncertainty training5, we adopt 20% training clean speech ratio for models using uncertainty, but 50% for other models.
4.3. Results
The CER results are shown in Table 1. Among the models that do not consider uncertainty features, model IV with joint-training reaches the best performance. As discussed in Section 2.3, the model is trained by the multi-stage order of “I-II-III-IV”. The results in Table 1 indicate the system can receive progressive improvements over each stage. Our best results come from the model with both speech enhancement and speaker identity uncertainty. Even though the uncertainty only trained with a frozen target speech extraction module, both model VI and model VII signiﬁcantly outperform the model IV with joint-training. Meanwhile, the speaker identity uncertainty does not offer enough improvement to the system when it is introduced alone. It signiﬁcantly reduces the CER when it is combined with the speech enhancement uncertainty.
Table 2 shows the multi-condition experiment results. Generally speaking, the noisy test results are aligned with the noisy training experiments in Table 1. The clean test results suggest that training using a multi-condition setting can also elevate the model performances for clean speech recognition. Note that the speech enhancement uncertainty may down-grade the clean speech recognition performance (see model VI in Table 2) because of the artifacts discussed
5the OU for clean utterances are zero matrices.

Table 1. Noisy Training Result (CER): Model I is a RNN-T

trained on clean speech, others were trained and tested on noisy data

only; Model II is a pipeline system that used pre-trained modules;

means that we ﬁne-tuned RNN-T while froze the pre-trained tar-

get speech extraction module; † stands for the model using joint-

training. Model V, VI, VII employed speaker identity uncertainty

(Uspk), speech enhancement uncertainty, (Uspeech), and both uncer-

tainties (Uboth), respectively.

Class

Model

Overall 1Spk 2Spk 3Spk

I

81.9

20.4 84.0 139.0

Baseline

II

45.9

13.1 37.2 85.7

III

26.2

12.5 26.2 39.3

Joint-Train IV†

24.4

11.1 24.5 37.1

V (Uspk)

25.8

12.2 25.1 39.5

Uncertainty VI (Uspeech) 22.5

10.0 22.0 34.9

VII (Uboth) 21.8

9.6 21.3 33.8

Table 2. Multi-condition Results (CER): Model I is a RNN-T

trained on clean speech, others were trained on a combination of

noisy and clean data; means that we ﬁne-tuned RNN-T while froze

the pre-trained target speech extraction module; Model V, VI, VII

employed speaker identity uncertainty (Uspk), speech enhancement

uncertainty, (Uspeech), and both uncertainties (Uboth), respectively.

Model

All (Clean/Noisy) 1Spk 2Spk 3Spk

I

10.3/81.9

20.4 84.0 139.0

III

9.9/23.7

9.8 23.8 37.0

V (Uspk)

11.3/23.5

10.0 23.1 36.9

VI (Uspeech)

11.7/22.5

9.1 21.5 36.4

VII (Uboth)

10.0/21.5

9.0 21.2 33.8

in Section 4.2. However, the speaker identity uncertainty has shown its effectiveness in eliminating the instability, as shown in model VII in Table 2.
The uncertainty features are especially useful when handling multi-speaker speech. Our further investigation found that the features can signiﬁcantly reduce the recognition insertion error for the recognizer. As shown in Table 3, Model VII gained 17.5% relative insertion error reduction comparing to Model III for the three speaker case in noisy training (i.e., Table 1). For multi-condition training (i.e., Table 2), Model VII received similar 15.5% relative insertion error reduction comparing to Model III. The insertion errors are likely to happen when there are residual noises from other speakers in the enhanced speech. Thus, the reduction of the insertion error rate aligns with our motivation of adding uncertainty features.

Table 3. Error Distribution Between Models with and without Un-

certainty. The results are from three speaker cases in noisy training.

Model Insertion Deletion Substitution

III

2123

3321

16075

VII

1751

3260

13673

5. CONCLUSION

This work presents a joint-framework combining target speech extraction and RNN-T for target-speaker speech recognition. We explored the framework with a multi-stage training strategy to stabilize the training. Next, we designed two novel uncertainty measures and subsumed them into the system. Speciﬁcally, we propose the speaker identity uncertainty based on entropy and the speech enhancement uncertainty based on a neural uncertainty estimator. Our experiments on simulated AISHELL-2 corpus show that both the joint-train framework and the uncertainties signiﬁcantly improve the system performance.

6. REFERENCES
[1] B.n King, I. Chen, Y.n Vaizman, Y. Liu, R. Maas, et al., “Robust speech recognition via anchor word representations.,” in Interspeech, 2017, pp. 2471–2475.
[2] S. H. Mallidi, R. Maas, K. Goehner, A. Rastrow, S. Matsoukas, et al., “Device-directed utterance detection,” Interspeech, pp. 1225–1228, 2018.
[3] X. Zhang and D. Wang, “A deep ensemble learning method for monaural speech separation,” IEEE/ACM trans. on ASLP, vol. 24, no. 5, pp. 967–977, 2016.
[4] J. Du, Y.i Tu, L. Dai, and C. Lee, “A regression approach to single-channel speech separation via high-resolution deep neural networks,” IEEE/ACM trans. on ASLP, vol. 24, no. 8, pp. 1424–1437, 2016.
[5] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa, et al., “Speaker-aware neural network based beamformer for speaker extraction in speech mixtures.,” in Interspeech, 2017, pp. 2655–2659.
[6] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, and T. Nakatani, “Single channel target speaker extraction and recognition with speaker beam,” in ICASSP, 2018, pp. 5554– 5558.
[7] K. Zˇ mol´ıkova´, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, et al., “Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures,” IEEE JSTSP, vol. 13, no. 4, pp. 800–814, 2019.
[8] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, et al., “Voiceﬁlter: Targeted voice separation by speaker-conditioned spectrogram masking,” in Interspeech, 2019, pp. 2728–2732.
[9] X. Ji, M. Yu, C. Zhang, D. Su, T. Yu, et al., “Speaker-aware target speaker enhancement by jointly learning with speaker embedding extraction,” in ICASSP, 2020, pp. 7294–7298.
[10] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A.n Chiao, R. Liu, et al., “Voiceﬁlter-lite: Streaming targeted voice separation for on-device speech recognition,” 2020.
[11] A. Narayanan and D. Wang, “Joint noise adaptive training for robust automatic speech recognition,” in ICASSP, 2014, pp. 2504–2508.
[12] T. Gao, J. Du, L. Dai, and C. Lee, “Joint training of front-end and back-end deep neural networks for robust speech recognition,” in ICASSP, 2015, pp. 4375–4379.
[13] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, and R. Haeb-Umbach, “Beamnet: End-to-end training of a beamformer-supported multi-channel asr system,” in ICASSP, 2017, pp. 5325–5329.
[14] T. Ochiai, S. Watanabe, T.i Hori, and J. R Hershey, “Multichannel end-to-end speech recognition,” in ICML, 2017, pp. 2632–2641.
[15] S. Settle, J. Le Roux, T. Hori, S. Watanabe, and J. R Hershey, “End-to-end multi-speaker speech recognition,” in ICASSP, 2018, pp. 4819–4823.
[16] A. S. Subramanian, C. Weng, M. Yu, S. Zhang, Y. Xu, et al., “Far-ﬁeld location guided target speech extraction using endto-end speech recognition objectives,” in ICASSP, 2020, pp. 7299–7303.

[17] Y. Xu, C. Weng, L. Hui, J. Liu, M. Yu, et al., “Joint training of complex ratio mask based beamformer and acoustic model for noise robust asr,” in ICASSP, 2019, pp. 6745–6749.
[18] T. Von Neumann, C. Boeddeker, L. Drude, K. Kinoshita, M. Delcroix, et al., “Multi-talker asr for an unknown number of sources: Joint training of source counting, separation and asr,” arXiv preprint arXiv:2006.02786, 2020.
[19] L. Deng, J. Droppo, and A. Acero, “Dynamic compensation of hmm variances using the feature enhancement uncertainty computed from a parametric model of speech distortion,” IEEE Trans. SAP, vol. 13, no. 3, pp. 412–421, 2005.
[20] D. T Tran, E. Vincent, and D. Jouvet, “Fusion of multiple uncertainty estimators and propagators for noise robust asr,” in ICASSP, 2014, pp. 5512–5516.
[21] C. Zhang and K. Koishida, “End-to-end text-independent speaker veriﬁcation with triplet loss on short utterances,” in Interspeech, 2017, pp. 1487–1491.
[22] C. Zhang, K. Koishida, and J. H.L. Hansen, “Text-independent speaker veriﬁcation based on triplet convolutional neural network embeddings,” IEEE/ACM Trans. on ASLP, vol. 26, no. 9, pp. 1633–1644, 2018.
[23] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM trans. on ASLP, vol. 27, pp. 1256–1266, 2019.
[24] C. Chiu, T. N Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, et al., “State-of-the-art speech recognition with sequence-tosequence models,” in ICASSP, 2018, pp. 4774–4778.
[25] N. Pham, T. Nguyen, J. Niehues, M. Mu¨ller, and A. Waibel, “Very deep self-attention networks for end-to-end speech recognition,” in Interspeech, 2019, pp. 66–70.
[26] S. Karita, Na. Chen, T. Hayashi, T. Hori, H. Inaguma, et al., “A comparative study on transformer vs rnn in speech applications,” in ASRU, 2019, pp. 449–456.
[27] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012.
[28] Ka. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,” in ASRU, 2017, pp. 193–199.
[29] A. H. Abdelaziz, S. Watanabe, J. R Hershey, E. Vincent, and D. Kolossa, “Uncertainty propagation through deep neural networks,” in Interspeech, 2015.
[30] K. Nathwani, J. A Morales-Cordovilla, S. Sivasankaran, I. Illina, and E. Vincent, “An extended experimental investigation of dnn uncertainty propagation for noise robust asr,” in HSCMA, 2017, pp. 26–30.
[31] K. Nathwani, E. Vincent, and I. Illina, “Consistent dnn uncertainty training and decoding for robust asr,” in ASRU, 2017, pp. 185–192.
[32] Y. Koizumi, K. Yaiabe, M. Delcroix, Y. Maxuxama, and D. Takeuchi, “Speech enhancement using self-adaptation and multi-head self-attention,” in ICASSP, 2020, pp. 181–185.
[33] D. T Tran, E. Vincent, and D. Jouvet, “Nonparametric uncertainty estimation and propagation for noise robust asr,” IEEE/ACM Trans. on ASLP, vol. 23, no. 11, pp. 1835–1846, 2015.
[34] J. Du, X. Na, X. Liu, and H. Bu, “Aishell-2: transforming mandarin asr research into industrial scale,” arXiv preprint arXiv:1808.10583, 2018.

