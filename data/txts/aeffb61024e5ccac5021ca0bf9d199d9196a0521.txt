Adaptive Constraint Satisfaction for Markov Decision Process Congestion Games: Application to Transportation Networks

Sarah H.Q. Li a, Yue Yu a, Nico Miguel c, Daniel Calderone b Lillian J. Ratliff b Behc¸et Ac¸ıkmes¸e a
aDepartment of Aeronautics and Astronautics, University of Washington, Seattle, USA. (e-mail:{sarahli, yueyu, behcet}@uw.edu). bDepartment of Electrical Engineering, University of Washington, Seattle, Washington, USA. (e-mail: {djcal, ratlifﬂ}@uw.edu) cDepartment of Mechanical Engineering, University of Washington, Seattle, Washington, USA. (e-mail: nmiguel@uw.edu)

arXiv:1907.08912v3 [cs.GT] 13 Dec 2021

Abstract
Under the Markov decision process (MDP) congestion game framework, we study the problem of enforcing population distribution constraints on a population of players with stochastic dynamics and coupled congestion costs. Existing research demonstrates that the constraints on the players’ population distribution can be satisﬁed by enforcing tolls. However, to compute the minimum toll value for constraint satisfaction requires accurate modeling of the player’s congestion costs. Motivated by settings where an accurate congestion cost model is unavailable (e.g. transportation networks), we consider a MDP congestion game with unknown congestion costs. We assume that a constraint-enforcing authority can repeatedly enforce tolls on a population of players who converges to an -optimal population distribution for any given toll. We then construct a myopic update algorithm to compute the minimum toll value while ensuring that the constraints are satisﬁed on average. We analyze how the players’ sub-optimal responses to tolls impact the rates of convergence towards the minimum toll value and constraint satisfaction. Finally, we apply our results to transportation by building a high-ﬁdelity game model using data from the New York City’s (NYC) Taxi and Limousine Commission (TLC), and illustrate how to efﬁciently reduce congestion while minimizing the impact on driver earnings.
Key words: Markov decision process, incentive design, congestion games, online optimization, transportation systems, stochastic games

1 Introduction
Congestion games play a fundamental role in engineering [26]. In large-scale networks such as urban trafﬁc and electricity markets, congestion games capture how the competition among self-motivated decision-makers, known as the players, impacts network-level trends [23,10]. In particular, when all the players are equipped with identical congestion costs and transition probabilities in a ﬁnite state-action space, the game can be modeled as a Markov decision process (MDP) congestion game [6].
We study the feasibility of using tolls to enforce system-level constraints on a game in which both the players and the system operator do not know the true congestion costs. We are motivated by a plethora of system-level constraints encountered in large-scale networks, including satisfying safety constraints for decentralized autonomous swarms, meeting carbon-emission targets in urban transportation systems and minimizing voltage violations in competitive electricity markets [7,15,17]. Since tolling is a common and easily imple-

mentable mechanism in networked systems [31], we assume the system operator can freely impose tolls on the players.
We are interested in the minimum toll value that ensures constraint satisfaction, which can be solved as a function of the MDP congestion costs [14]. However, extracting the MDP congestion cost is difﬁcult when the player objectives are complex and unknown to the constraint-enforcing entity. This is also true in simulation engines and higher complexity models, where the effects of a given toll can be computed, but not the minimum toll value. As such, we assume that there exists an oracle who can compute an -optimal solution for a game with a known toll. The inexact oracle is motivated by model-free, learning-based algorithms that can approximate the Nash equilibria for routing games with unknown link costs [33,12].
Contributions. We derive a gradient-based tolling algorithm that enforces linear population distribution constraints on a class of MDP congestion games with unknown but strictly increasing congestion costs. The algorithm requires access

Preprint submitted to Automatica

14 December 2021

to an inexact oracle that takes an input toll and returns a population distribution that is -optimal for the toll-augmented game. We show a direct relationship between the -optimal population distribution and an inexact gradient of the tollaugmented game with respect to the toll. We bound the following quantities as functions of the oracle’s sub-optimality : convergence of 1) the average toll value towards the minimum toll value, 2) the average population distribution towards the optimal distribution under the minimum toll, 3) the average constraint violation towards zero. Finally, we construct a high-ﬁdelity game model using real-world data from NYC TLC [21] to demonstrate our algorithm’s effectiveness at reducing congestion in transportation networks.
The rest of this paper is organized as follows. In Section 2, we review related work. In Section 3, we review MDP congestion games. In Section 4, we introduce the toll-augmented game and the inexact oracle. In Section 5, we introduce the tolling algorithm and prove its convergence properties. In Section 6, the MDP congestion game model is used to reduce ride-share congestion levels in Manhattan, NYC.
2 Literature Overview
MDP congestion games [6] are related to non-atomic routing games [2,23], stochastic games [29], and mean ﬁeld games [13], but differ in modeling assumptions. MDP congestion games extend non-atomic routing games by generalizing the player dynamics from deterministic to stochastic. Stochastic games assume that player costs differ and are functions of the joint policy. MDP congestion games assume that players costs are identical and are functions of the population distribution [32]. Finally, MDP congestion games are analogous to a discrete mean ﬁeld game where the continuous stochastic processes are discretized in time, state, and action spaces. We show that these differences in assumptions enable MDP congestion games to more easily model large-scale networks such as transportation systems.
Tolling schemes for non-atomic routing games have been studied under capacitated trafﬁc assignment literature [23, Sec. 2.8.2]. Adaptive incentive design for games has also been considered in deterministic and stochastic settings in [25] for players without MDP dynamics. Presently, we adopt a form of adaptive incentive design to guarantee constraint satisfaction. Tolling to satisfy external objectives is more generally interpreted as a Stackelberg game between a leader and its followers [30]. Techniques for updating the Stackelberg leader’s actions to optimize the social cost of its followers are derived in [27]. Tolling non-atomic games where players have identical MDP dynamics and unknown congestion costs has not yet been analyzed.
Our minimum toll computation algorithm is an inexact gradient descent [8], and has been applied to settings such as distributed optimization and model predictive control [11,19]. In the game theory, the inexact gradient descent method has been applied to computing the Nash equilibria of a two

player min-max game [22]. However, it is not been previously connected to approximate Wardrop equilibria.

3 MDP Congestion Game

Notation. The notation [K] = {0, . . . , K − 1} denotes an
index set of length K, R(R+) denotes a set of real (nonnegative) numbers, 1N denotes a vector of ones of size N , and [x]+ = max{x, 0} denotes a vector-valued function in which max is element-wise applied to vectors x and 0.

Consider a continuous population of players, each with identical MDP dynamics and congestion costs over a state-action set [S] × [A] for (T + 1) time steps. Under the non-atomic game assumption, the relationship between individual player distributions and the population distribution is described in [12, Sec.2]. Presently, we are only concerned with the resulting population distribution. We denote the set of feasible population distributions as Y(P, p), given by

Y (P,

p)

=

{y

∈

(T +1)SA
R+

ayt+1,sa = s ,aPtss ayts a,

ay0sa = ps},

(1)

where ytsa is the portion of the playing population who takes

action a from state s at time t. We emphasize that y is a

vector

in

(T +1)SA
R+

whose

coordinates

are

ordered

as

y = y000 . . . y010 . . . y100 . . . yT (S−1)(A−1)

(2)

The stochastic transition dynamics are given by P ∈ RT ×S×S×A, where Ptss a denotes the transition probability from state s to s under action a at time t. The transition dy-
namics satisfy s ∈[S] Pts sa = 1 ∀ (t, s, a) ∈ T × S × A,
and Pts sa ≥ 0, ∀ (t, s , s, a) ∈ T × S × S × A. The initial population distribution is given by p ∈ RS+, where ps denotes the portion of the players’ population in state s at
t = 0.

At time t, each player incurs a cost as a function of y, tsa : R(T +1)SA → R. We collect tsa into a cost vector (y) ∈ R(T +1)SA under the same ordering as y in (2).

Similar to MDP literature, a player’s expected cost-to-go at (t, s, a) is its Q-value function, recursively deﬁned as

Qtsa(y) =

tsa(y)

tsa(y) + Pts sa min Qt+1,s a (y)

s

a ∈[A]

t=T t ∈ [T ]

(3)

In an MDP congestion game, the Q-value functions depend

on the players’ collective action choices through y, the pop-

ulation distribution. Each player’s objective is to minimize

its own Q-value function by choosing individual actions. An

MDP Wardrop equilibrium y is reached if no player can

unilaterally decrease its Q-value function further by chang-

ing its actions.

2

Deﬁnition 1 (MDP Wardrop Equilibrium [6]) A population distribution y is a MDP Wardrop equilibrium if for every (t, s, a) ∈ [T + 1] × [S] × [A],

ytsa > 0 ⇒ Qtsa(y ) ≤ Qtsa (y ), ∀ a ∈ [A] (4) The set of y ∈ Y(P, p) that satisﬁes (4) is denoted by W( ).

At an MDP Wardrop equilibrium, every positive portion of the population distribution exclusively selects actions with the lowest Q-values.

Remark 1 The player costs vary with the population distribution in MDP congestion games and with the joint policy in stochastic games [29]. Furthermore, policy and population distribution are the primal and dual variables of MDP’s linear program [24, Eqn 6.9.2], respectively. Therefore, the two games can be interpreted as game extensions of the MDP using either its primal or dual form.

If is a continuous vector-valued function and there exists an explicit potential function F satisfying ∇F (y) = (y), then the MDP congestion game is a potential game [18].

Proposition 1 [6, Thm.1.3] For a MDP congestion game with cost vector , if a potential function F satisﬁes

∇F (y) = (y), F : R(T +1)×[S]×[A] → R, (5)

then the MDP Wardrop equilibrium is given by the optimal

solution of

min F (y)

y

(6)

s.t. y ∈ Y(P, p).

Games of form (6) can be solved by convex optimization techniques [14]. Using the corresponding potential function, we can characterize the degree of sub-optimality for any feasible population distribution within Y(P, p0).
Deﬁnition 2 ( -MDP Wardrop equilibrium) For a game with the cost vector , potential function F (5), and MDP Wardrop equilibrium y , if for some > 0, ˆy( ) satisﬁes
F (ˆy( )) ≤ F (y ) + , ˆy( ) ∈ Y(P, p0),
then ˆy( ) is a -MDP Wardrop equilibrium. The set of MDP Wardrop equilibria is given by W( , ).
Among cost vectors that have explicit potential functions, we focus on those that are strongly convex [3, Eqn B.6].
Assumption 1 The cost function has an explicit potential F (5) that is α-strongly convex for all y ∈ Y(P, p).
∇y (y) ∈ R(T +1)SA×(T +1)SA, ∇y (y) αI, α > 0.

Assumption 1 implies congestion in all state-action costs. To model games in which some state-action costs are constant, we can approximate the constant cost by a slowly growing congestion cost.

Remark 2 If tsa : R+ → R ∀ (t, s, a) ∈ [T + 1] × [S] × [A] are scalar functions, then Assumption 1 implies that
each tsa strictly increases and satisﬁes α|ytsa − ytsa| ≤ | tsa(ytsa) − tsa(ytsa)|. Its potential is also given by

F0(y) =

t,s,a

ytsa 0

tsa(u)du.

(7)

For all -MDP Wardrop equilibria ˆy( ), Assumption 1 implies ˆy( ) − y 22 ≤ 2α .

4 Tolling for constraint satisfaction

In this section, we formulate system-level constraints using afﬁne population distributions inequalities and relate the inexact oracle of the tolled MDP congestion game to an -MDP Wardrop equilibrium. Afﬁne constraints cover many design requirements for large-scale networks. For example, the Department of Transportation may meet carbon-emission targets by tolling fossil fuel vehicles on major freeways [15]. This can be expressed as an afﬁne inequality on the fossil fuel vehicle population. In competitive electricity markets, power auction are followed by a procedure to predict and eliminate power violations by initializing ofﬂine generators [17]. To minimize the incurred cost of these initialization, the system operator may use tolls during the auction to limit voltage demands. This can again be expressed as an afﬁne constraint on local voltages.

Deﬁnition 3 (Afﬁne Constraints) The set of constraints on the population distribution is given by

C=

y

∈

(T +1)SA
R+

|

Ay

−

b

≤

0

(8)

where A ∈ RC×(T +1)SA, b ∈ RC , and 0 ≤ C < ∞ denotes the total number of constraints imposed.

Let Ai ∈ R(T +1)SA be the ith row of the matrix A. For each constraint i, instead of searching over all possible tolls, we only consider tolls of the form τiAi ∈ R(T +1)SA for τi ∈ R+. This formulation ensures that τi only affects the (t, s, a) component of when Ai,tsa is non-zero, where the toll magnitude is controlled by τi. We denote the tollaugmented game cost as

τ (y) := (y) + A τ, τ ∈ RC+.

(9)

When satisﬁes Assumption 1, we denote τ ’s potential as L(·, τ ), such that ∇yL(y, τ ) = τ and L augments F (5) as

L(y, τ ) = F (y) + τ (Ay − b).

(10)

3

Find the smallest that
enforces for
.

Gradient Descent

Inexact gradient oracle

Approximate MDP Wardrop
equilibrium

Fig. 1. Using approximate MDP Wardrop equilibrium, we perform inexact gradient descent on τ to ﬁnd the minimum toll value.
For a toll value τ , the toll-augmented game and the tolled MDP Wardrop equilibrium, yτ ∈ W( τ ), are given by

d(τ ) = min L(y, τ ), yτ ∈ argmin L(y, τ ). (11)

y∈Y (P,p)

y∈Y (P,p0 )

Using (9), any feasible afﬁne constraint can be satisﬁed for arbitrarily large magnitudes of τ [14]. We speciﬁcally want to compute the minimum toll value that enforces C (8) on the MDP Wardrop equilibrium of the toll-augmented game.

Deﬁnition 4 (Minimum toll value) For a constraint set C (8), the minimum toll value, τ ∈ RC+, is the minimal non-negative toll which ensures that the toll-augmented
cost vector τ (9) generates a constraint-satisfying MDP Wardrop equilibrium—i.e.,

τ

= min

τ

∈

C
R+

|

W(

τ)

⊆

C

.

(12)

We proved a sufﬁcient condition for the existence of the minimum toll value in [14].
Proposition 2 [14]: When C is convex, C ∩Y(P, p0) is nonempty, and the cost vector satisﬁes Assumption 1, a unique minimum toll value τ exists that uniquely maximizes d(τ ).

τ = argmax min L(y, τ ) = argmax d(τ ). (13)

τ ∈RC+ y∈Y(P,p0)

τ ∈RC+

When is known, (13) directly computes τ . When ’s is unknown, we cannot explicitly solve for either τ or d(τ ).
Problem 1 To ﬁnd τ when satisﬁes Assumption 1 and is unknown, we apply gradient descent to d(τ ) by querying the -MDP Wardrop equilibria of d(τ ) and forming an inexact oracle. Our set-up is summarized in Figure 1.
To demonstrate how the -MDP Wardrop equilibrium of a tolled game induces an inexact oracle for ∇d(τ ), we ﬁrst derive the analytical expression of ∇d(τ ).
Proposition 3 When the congestion costs satisfy Assumption 1 and C satisﬁes Deﬁnition 3, d from (11) has the following properties.
• d is concave.

• d is α¯-smooth with α¯ = Aα 22 . I.e., for any σ, τ ∈ RC ,

α¯

2

d(τ ) + ∇d(τ )

(σ − τ ) − 2

σ − τ 2 ≤ d(σ). (14)

• Let yτ be deﬁned as (11), then ∇d(τ ) is given by

∇d(τ ) = Ayτ − b.

(15)

See Appendix A.1 for proof. When the costs are unknown, we can still obtain -approximate MDP Wardrop equilibrium via learning algorithms [33,12,32]. When applied to games with tolls, these -approximate MDP Wardrop equilibria form -inexact oracles of ∇d and d.

Deﬁnition 5 ( -inexact oracle) The -inexact oracles of ∇d(τ ) and d(τ ) are given by

∇ˆ d(τ ) = Ayˆτ ( ) − b, dˆ(τ ) = L(yˆτ ( ), τ ), (16)

where yˆτ ( ) ∈ W( τ , ) is an -MDP Wardrop equilibrium

satisfying

L(yˆτ ( ), τ ) ≤ L(yτ , τ ) + .

(17)

When = 0, the oracle is exact. When > 0, the oracle’s ‘inexactness’ can be characterized by how much it changes the concavity and smoothness of d.
Lemma 1 (Concavity) Under Assumption 1, all -MDP Wardrop equilibria yˆτ ( ) given by (17) will generate inexact oracles (16) that satisfy
d(σ) ≤ dˆ(τ ) + ∇ˆ d(τ ) (σ − τ ), ∀ σ, τ ∈ RC+.

See Appendix A.2 for proof. Next, we show that dˆ and ∇ˆ d preserve d’s smoothness up to 2 .

Lemma 2 ( -approximate smoothness) Let α¯ =

A

2
2.

α

Under Assumption 1, all -MDP Wardrop equilibria yˆτ ( )

given by (17) will generate inexact oracles (16) that satisfy

dˆ(τ ) + ∇ˆ d(τ ) (σ − τ ) − α¯ σ − τ 22 ≤ d(σ) + 2 , (18) ∀ τ, σ ∈ RC .

See Appendix A.3 for proof.
5 Minimum toll algorithm
Convergence of ﬁrst-order gradient methods relies on the objective’s convexity and smoothness. If an inexact gradient preserves the concavity and smoothness, it can also be shown to converge [8]. In this section, we apply the same concept

4

Algorithm 1 Iterative toll synthesis
Input: , P , ps, τ0. Output: τ N , yN . 1: for k = 0, 1, . . . do 2: yk ∈ W( + A τ k, k) 3: τ k+1 = [τ k + γk(Ayk − b)]+ 4: end for

in the context of tolling for MDP congestion games and elaborate on how constraint violation is affected by the MDP Wardrop equilibrium.

In Algorithm 1, we denote the kth toll charged, the kth MDP Wardrop equilibrium, and the in the kth -inexact oracle as τ k, yk, and k, respectively. When k = 0 ∀k ∈
N, Algorithm 1 corresponds a projected gradient ascent on
d(τ ), for which the convergence rate is sublinear [4]. We analyze Algorithm 1’s convergence properties when k > 0
by utilizing the following quantities,

¯τ k = 1 k τ s, ¯yk = 1 k−1 ys, Ek = k−1 s, (19)

k

k

s=1

s=0

s=0

where ¯τ k/ ¯yk/Ek is the average toll/average -MDP Wardrop equilibrium/accumulated up to iteration k, respectively.

Theorem 1 If the cost vector satisﬁes Assumption 1, and γ ≤ α 2 for each k ∈ N, then ¯τ k from (19) satisﬁes
2A2

d(τ ) − d(¯τ k) ≤ 1 1 τ 0 − τ 2 + 2Ek , (20)

k 2γ

2

where τ is the minimum toll value (12).

PROOF. Let rs = τ s − τ α 2 , we have
2A2

22. From Lemma 3, when γ ≤

rs+1 ≤

(21)

rs + 2γ d(τ s+1) − L(ys, τ s) + 2 s + ∇ˆ d(τ s) (τ s − τ )

From Lemma 1, we have
∇ˆ d(τ s) (τ s − τ ) ≤ L(ys, τ s) − d(τ ) (22)
Summing up (21) and 2γ×(22), we obtain
rs+1 − rs ≤ 2γ(d(τ s+1) − d(τ ) + 2 s) (23)
Summing over (23) for s = 0 . . . , k −1, we obtain 0 ≤ rk ≤ r0 − 2γ ks=1 d(τ ) − d(τ s) + 4γ ks=−01 s. Finally, the concavity of d from Proposition 3 implies that −kd(¯τ k) = −kd( ks=1 τ s) ≤ − ks=1 d(τ s). This completes the proof.

Remark 3 When k = for all k ∈ N, (20) becomes

d(τ ) − d(¯τ k) ≤ k1 21γ τ 0 − τ 22 + 2 .

We note that this convergence rate is sublinear in k, which is comparable to gradient descent with exact oracles. Additionally, the convergence rate depends linearly on the degree of sub-optimality of the oracles as 2 .

Our proof is inspired by [8,19]. Theorem 1 shows that Ek’s
effect on the convergence rate is independent of the chosen step-size. Furthermore, the convergence of ¯τ k towards τ depends on the magnitude of the minimum toll value required to enforce C and the accumulated error made in ap-
proximating the MDP Wardrop equilibrium. The constraint
violation of the average MDP Wardrop equilibrium approximation ¯yk is similarly bounded.

Corollary 1 If the cost vector satisﬁes Assumption 1 and
γ ≤ α 2 , then the constraint violation of the average
2A2
population distribution ¯yk from (19) satisﬁes

[A¯yk − b]+

1 ≤

2 γk

τ 2 + τ0 − τ 2 + 2 γEk . (24)

PROOF. We ﬁrst derive an upper bound for τ k 2 and then bound the left hand side of (24) by τ k 2. Recall (23), we use d(τ )−d(τ k) ≥ 0 to derive rs+1 ≤ rs+4γ s. Summing over s = 0, . . . , k − 1, we have

τ k − τ 2 ≤ τ 0 − τ 2 + 4γEk.

(25)

2

2

Taking th√e square r√oot of√both sides of (25) and noting the identity a + b ≤ a + b, we obtain

τ k − τ 2 ≤ τ 0 − τ 2 + 4γEk. (26)

We add τ 2 to both sides of (26) and use the triangle inequality τ k 2 ≤ τ k − τ 2 + τ 2 to obtain

τ k 2 ≤ τ 2 + τ 0 − τ 2 + 4γEk.

(27)

Next, we bound [A¯yk − b]+ 2 using τ k 2. From line 3 of Algorithm 1, τ s+1 ≥ τ s + γ(Ays − b). We sum over s = 0, . . . , k − 1 to obtain τ k ≥ τ 0 + γk(A¯yk − b). Noting τ 0 ∈ RC+ can be dropped, γk[A¯yk − b]+ ≤ τ k combined
with (27) completes the proof.

Remark 4 If the oracle incurs a constant error k = at each iteration k, Corollary 1 shows that the average constraint violation will still asymptotically reduce to zero.

5

Remark 5 As Corollary 1 is concerned with the average constraint violation, Algorithm 1 is not appropriate for enforcing safety-critical system constraints.

Corollary 1 shows that larger step sizes in Algorithm 1 can reduce the amount of average constraint violation. Larger step-sizes can also directly reduce the effect of approximation error on constraint violation by a factor of 2 Ekγ−1.

We can further show that the average population distribution ¯yk (19) converges to the same optimal population distribu-
tion under the minimum toll value.

Theorem 2 If the cost vector satisﬁes Assumption 1 and
γ ≤ α 2 , then the average player population distribution
2A2
given by ¯yk (19) satisﬁes

¯yk − y 2 ≤ α D(τ 0, τ , Ek),

(28)

2 2γk

where τ is the minimum toll value, y is the optimal population distribution for d(τ ), and D(τ 0, τ , Ek) is given by

D(τ 0, τ , Ek) = max 21 τ 0 22 + 2Ek, τ 22 + τ 2 τ 0 − τ 2 + 2 γEk . (29)

PROOF. We ﬁrst upper bound and lower bound the term F (¯yk) − F (y ). First consider the upper bound. From Lemma 3, let τ = 0,
τ s+1 22 ≤ τ s 22 + 2γ d(τ s+1) + 2 s − L(ys, τ s) (30)
+ ∇ˆ d(τ s) τ s .

Recall from (15) and (10), ∇ˆ d(τ s) = Ays − b and L(ys, τ s) = F (ys)+(τ s) (Ays−b). Therefore L(ys, τ s)−
∇ˆ d(τ s) τ s = F (ys). Then (30) becomes

τ s+1 22 + 2γ(F (ys) − d(τ s+1)) ≤ τ s 22 + 4γ s (31)

Summing over s = 0, . . . k − 1, ks=−01 F (ys) − d(τ s+1) ≤ 21γ τ 0 22 + 2Ek. Taking the average ¯yk and noting that d(τ k) ≤ d(τ ) = F (y ) for all τ k ∈ RC+,

F (¯yk) − F (y ) ≤ 1 τ 0 2 + 2Ek . (32)

2γk

2k

Next, consider the lower bound of F (¯yk) − F (y ). By def-
inition, y solves miny∈Y(P,p0) F (y) + (Ay − b) τ where (Ay − b) τ = 0. This implies that F (y ) ≤ L(¯yk, τ ). We expand L(¯yk, τ ) with (10) to obtain

F (y ) − F (¯yk) ≤ (A¯yk − b) τ ≤ [A¯yk − b]+τ .

We can then bound the difference F (y ) − F (¯yk) by τ 2 [A¯yk − b]+ 2. From Corollary 1,

F (y )−F (¯yk) ≤ τ 2 γk

τ 2+ τ0 − τ 2+2 γEk .

(33)

Together, (32) and (33) imply

|F (y ) − F (¯yk)| ≤

1 D(τ

, τ0, Ek)

(34)

γk

Strong convexity of F follows from Assumption 1, such that ¯yk − y 22 ≤ α2 |F (¯yk) − F (y )|. This combined with (34)
completes the proof.

Remark 6 Theorems 1 and 2 show that Algorithm 1 will converge to τ and y only if Ek’s growth rate is sublinear.
If the oracle incurs a constant error every iteration, then limk→∞ τ¯k − τ 2 = 2 and limk→∞ y¯k − y 2 = αγ .

As in the case of average constraint violation, we observe that the step size inﬂuences the effects of -Wardrop equilibrium on the convergence of ¯yk towards y . If we assume that at each step k, an approximate MDP Wardrop equilibrium yk with the same constant is returned, then the error ¯yk − y is minimized for the largest step size, γ = α 2 .
A2
Fast ﬁrst-order gradient method. An alternative to Algorithm 1 is the fast gradient method [8]. When assuming k = for all k ∈ N, the fast gradient method is equivalent to appending to Algorithm 1 the following after Step 3,

τ k+1 = A 22 [k+1 α(k + 3)
i=1

i(i + 1)(Ayˆk −b)]+ + k + 1 τ k+1. k+3

In large networked systems where the inexact oracle may have low accuracy, the fast gradient method theoretically and empirically diverges from the optimal objective, d(τ ) [8]. It has also been observed that the fast gradient method’s constraint violation is comparable to Algorithm 1 [19]. Thus, we focus on the classical ﬁrst-order gradient method instead.

6 Congestion Reduction in Ride-share Networks
In this section, we model the competition among NYC’s ride-share drivers as a MDP congestion game and apply Algorithm 1 to demonstrate how ride-share companies can implicitly enforce constraints by utilizing tolls. Since origindestination-speciﬁc trip data for ride-share companies are not publicly available, we use the rider demand distribution provided by the NYC TLC as a proxy for Uber’s rider demand distribution. In [28], the overall rider demand for TLC is estimated to be about 40% of the rider demand for Uber. We normalize the TLC rider demand distribution accordingly and assume that the TLC data is a proportionally accurate estimate of the rider demand for Uber.

6

6.1 Ride-sharing MDP

We consider a cohort of competitive ride-share drivers in Manhattan, NYC who repeatedly operate between 9 am and noon. Using over six hundred thousand data points from the yellow taxi data during January, 2019 [21], we model individual driver dynamics as a ﬁnite time horizon MDP.

Modelling assumptions. 1) In modelling the ride-share competition as a ﬁnite horizon MDP congestion game, we assume that all trips take identical time regardless of destination or congestion level. 2) The results we obtain are for an uniform initial driver distribution, i.e. ps = 10000/63 for each s ∈ [S]. We found that varying the initial distribution did not signiﬁcantly impact the time-averaged MDP Wardrop equilibrium or the approximate norm of the resulting tolls, as long the initial distribution does not violate constraints. 3) From [16], the Uber driver population in NYC is approximately 50000. We assume that 20% of the total population works in Manhattan between 9 am and noon.

States. As shown in Figure 2, Manhattan is discretized into 63 states that correspond to TLC’s taxi zones (excluding the island zones). For state s, its neighbor states are those geographically adjacent (sharing one or more edges) to s in Figure 2. The set of neighbors is denoted by N (s).

Actions. Actions are state-speciﬁc. At s ∈ [S], two types of actions are available to drivers. Action as for any s ∈ N (s) transitions the driver from s to s . Action as is taken when the driver stays in the current state s and attempts to pick up a rider. For all states, the total number of available actions is given by
A = 1 + max |N (s)|.
s
where we use |N (s)| to denote the number of neighbors that state s has. Each action ai ∈ [A] corresponds to different actions for different states. In states with less neighbors than maxs |N (s)|, the extra actions are repeats of previous actions. For example, for state 1 with neighbors {3, 4}, the available actions are {a1, a3, a4, a3, a4}. For state 2 with neighbors {3, 4, 5, 6}, the available actions are {a2, a3, a4, a5, a6}.

Time. The average trip time from the TLC data is 12.02 minutes. Therefore, we take 12 minutes time intervals between 9 am and noon for a total of T = 15 time steps.

Transition Dynamics. For action as from state s at time t, the transition probability to any state s¯ is given by



 1 − δ, if s¯ = s , 



P (s, as , s¯, t) = |N (sδ)|−1 , if s¯ = s , s¯ ∈ N (s),



 0,

otherwise,

where δ ∈ [0, 1) models the driver’s probability of real-time deviation from a chosen strategy. We set δ = 0.1.

For action as from state s at time t, the transition probabilities to any state s¯ is derived using the rider demand distribution at s. Let N (s, s¯, t) be the number of riders whose trip starts at s and ends at s¯ during time step t. The transition probability to state s¯ is given by

P (s¯, s, as, t) =

N (s,s¯,t) ,
N (s,s ,t)
s

∀ s¯ ∈ [S], t ∈ [T ].

Note that as may transition drivers to states outside of N (s).

Driver costs. As in [14], the driver cost is the sum of expected earnings, fuel cost, and an artiﬁcial congestion cost.

tsa(ytsa) = Es cttrsavs − mts s + cwt ait · ytsa = s Pts sa [cttrsavs − mts s] + cwt ait · ytsa

where mts s is the monetary reward that the driver receives for transitioning from state s to s (only available when a = as), cttrsavs is the fuel cost of travelling from state s to s , cwt ait is the coefﬁcient of congestion, scaled linearly by the portion of drivers who are waiting for a rider. For a trip
time of ∆t, the cost of UberX in NYC [1] is

mts s = max $7, $2.55+$0.35·∆t(min)+$1.75·∆d(mi) ,
(35) where a base price of 7 is applied to each trip, augmented by charges proportional to the trip time and the trip distance. To compute mts s, we assign ∆t = 12 minutes to be the average trip time, and ∆d = dss to be the geographical distance between s and s in miles. The other parameters in tsa are computed as

cttrsavs = µ dss

Vel −1 + Fuel
Price

Fuel −1 dss
Eff

mi hr/mi

$/gal gal/mi mi


E [mts s] · cwtsaait = s
 ωtsas ,

−1

Customer Demand Rate

,

rides/∆t

if a = as if a = as

(36a) (36b)

where µ is a time-money tradeoff parameter, ωtsas = 0.1 models the minor congestion effects for Uber drivers who
decide to traverse from s to s .

The distance between states dss is the Haversine distance between s and s . If s = s, we average existing TLC data for trips that start and end in the same state to estimate dss .

The customer demand rate is derived from TLC data per time interval per day. We assume that the TLC data is proportionally a reasonable representation of ride demands in Manhattan. From [5], the Yellow Taxi’s ride demand is about 40% of the Uber’s ride demand at the beginning of 2019. Therefore we scale the TLC ride demand data by 2.5.

Other parameter values are listed in Table 1.

7

µ

Velocity Fuel Price

$15 /mi 8 mph $2.5/gal
Table 1 Parameters for the driver cost function.

Fuel Eff 20 mi/gal

440

Financial District North World Trade Center

430 Midtown Center

420

410

400

500

450

400

350

300

250

Financial District North

200

World Trade Center Midtown Center

150

0 2 4 6Time8 10 12 14

Manhattan

400

350

300

250

200

150

100

50

Fig. 2. Predicted ride-share trafﬁc in Manhattan. 6.2 Online learning via conditional gradient descent

In the previous section, we built a MDP congestion game model for the purpose of simulating Manhattan’s ride-share game. We assume that the players cannot access the congestion costs model and transition dynamics. Instead, the drivers repeatedly receive costs for a chosen joint policy, and collectively learn the MDP Wardrop equilibrium.
We implement the learning method from [14, Alg.3]. Inspired by conditional gradient descent (Frank-Wolfe), this method implicitly enforces y ∈ Y(P, p0) by solving the linearized objectives via dynamic programming. Although conditional gradient descent can be slow to converge to higher accuracy, it converges faster to lower accuracy solutions. Using the stopping criterion for conditional gradient descent, k-MDP Wardrop equilibrium is achieved when the driver costs and population distribution satisfy

k = (yk) + A τ k (yk − yk+1).

(37)

We set k = 2e3, which is approximately equivalent to a normalized error of 1% for the game potential F0 (7). The corresponding -MDP Wardrop equilibrium as well as the time-averaged/varying driver densities of the most congested states are shown in the map/line/bars plot in Figure 2.
From Figure 2, we see that the three most congested zones are the red zones on the bottom-left and center of Manhattan. Interestingly, our simulation shows that they attract 13%

0.12

Manhattan

400

0.10

0.08

350

0.06

300

0.04

0.02

250

0.00

400

200

350
150
300

250

100

Financial District North

200

World Trade Center

Midtown Center

50

150

0 2 4 6Time8 10 12 14

Fig. 3. Manhattan game with constraints (38) enforced.The plot labeled Manhattan shows the time-averaged driver distribution after tolling. The top line plot shows the time-varying toll as a function of time for each constraint-violating state, toll unit is $. The bottom line plot shows the corresponding driver distribution after 2000 iterations.
of the working driver population This further motivates the need to reduce congestion within ride-share drivers.

6.3 Reducing driver presence in congested taxi zones

If ride-share companies iteratively impose tolls according to Algorithm 1, they can reduce the driver presence in the congested taxi zones without constraining driver behaviour or signiﬁcantly impacting driver earnings.
Suppose the ride-share company wishes to limit the driver count to no more than 400 per zone per time. This constraint can be formulated as
a ytsa ≤ 400, ∀ (t, s) ∈ [T + 1] × [S]. (38)
For each (t, s), the corresponding constraint is given by Ai ∈ R(T +1)SA, where the (t, s, a)th entry is 1 and all other entries are 0. Although only three states violate the capacity constraint in Figure 2, we toll all states at all times to ensure that no states will violate constraints in the resulting equilibria. Thus, we enforce a total of 63 × 16 = 1008 constraints of form (38). The constraint matrix is A = [A1, . . . , A(T +1)S ] ∈ R(T +1)S×(T +1)SA.

6.4 Discussion

We run Algorithm 1 for 500 iterations. The results are shown in Figure 3. When k is about 1% of the unconstrained optimal potential value, the resulting constraint violation is quite low, with less than 5 driver violations per time step. As shown in the upper left plot of Figure 3, the tolls required to enforce these constraints do not extend beyond $0.15 per time step for all three states.

8

102
101 100
10 1 10 2
0

total toll value total constraint violation 250 500 750 1000 1250 1500 1750 2000 Iterations

0.0010 0.0005 0.0000 0.0005

Social Cost for driver

100

101

102

103

Iterations

Fig. 4. Top: the total toll enforced and the total constraint violation per iteration of toll synthesis. Bottom: the average driver cost per iteration of toll synthesis, scaled by the nominal average driver cost without toll.
From Figure. 4, we conclude that for a total of less than $1 toll over all states per time step, we can decrease the total constraint violation from over 200 drivers to less than 5 drivers over the entire time horizon. Furthermore, our tolls amounts to $3.25 for spending 4 hours in the most congested state; this is comparable in value to the current toll proposed for lower Manhattan ($2.25 per entry) [9].
A major concern for the ride-share company is the effect of tolls on the average driver earning. If the average driver earning decreases signiﬁcantly under tolls, then drivers may choose to quit, leading to less workforce for the ride-share company. In general, tolling does not necessarily cause average player earning to decrease [14]. This is because the average player earning can be viewed as the socially optimal population distribution, which minimizes a different optimization objective than the game’s potential (6). Here, we can demonstrate empirically that the tolls do not significantly detract from driver earnings. In Figure 4, we plotted the average driver cost over the tolling iteration process, normalized by the unconstrained average cost. We observe that the average cost of playing the game changes by less than 0.1% with the changing tolls. Therefore tolling will not promote quitting among the proﬁt-driven drivers under this constraint satisfaction scenario.
Next, we investigate the effects of k on both the tolling value and average constraint violation. In Figure 5, we compare the total average tolling value 1 ¯τ k after k = 500 iterations for inexact oracles values = [1e3, 2e3, 2e4, 1e5] and the average constraint violation 1 [A¯yk −b]+ after k = 500 iterations. We see that increased accuracy in the oracle decreases both the tolling value and the constraint violation level during the toll iteration process, thus providing more

6 × 10 1

5 × 100

5 × 10 1

4 × 100

4 × 10 1

103

104

105

103

104

105

Fig. 5. Left: total average toll. Right: total constraint violation of average driver distribution. Both ﬁgures are obtained for k = 500 iterations as a function of inexact oracles.
incentive to ﬁnd the minimum toll value to higher accuracy.

7 Conclusion
We presented an iterative tolling method that allows systemlevel operators to enforce constraints on a MDP congestion game with unknown congestion costs. We showed that an -MDP Wardrop equilibrium corresponds to an inexact gradient oracle of the tolled game, and derived conditions for convergence of the inexact gradient descent problem. We applied our results the ride-share system in Manhattan, NYC. Future extensions to this work include extending the toll synthesis method to work for general convex constraints.

References
[1] Alvia. Uber new york. http://www.alvia.com/uber-city/uber-newyork/, 2021. Accessed: 2021-02-14.
[2] Martin Beckmann. A continuous model of transportation. Econometrica, pages 643–660, 1952.
[3] Dimitri P Bertsekas. Nonlinear Programming. Athena Scientiﬁc Belmont, 1999.
[4] Se´bastien Bubeck et al. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3-4):231–357, 2015.
[5] Nicu Calcea. Nycdot’s experience with big data and use in transportation projects. https://citymonitor.ai/transport/uber-lyftrides-during-coronavirus-pandemic-taxi-data-5232, 2017. Accessed: 2021-02-14.
[6] Dan Calderone and S Shankar Sastry. Markov decision process routing games. In Proc. Int. Conf. Cyber-Physical Syst., pages 273– 279. ACM, 2017.
[7] Nazlı Demir, Utku Eren, and Behc¸et Ac¸ıkmes¸e. Decentralized probabilistic density control of autonomous swarms with safety constraints. Autonomous Robots, 39(4):537–554, 2015.
[8] Olivier Devolder, Franc¸ois Glineur, and Yurii Nesterov. Firstorder methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2014.
[9] Lauren Aratani Erin Durkin. New york becomes ﬁrst city in us to approve congestion pricing. https://www.theguardian.com/usnews/2019/apr/01/new-york-congestion-pricing-manhattan. Accessed: 2021-02-14.
[10] S Rasoul Etesami, Walid Saad, Narayan B Mandayam, and H Vincent Poor. Smart routing of electric vehicles for load balancing in smart grids. Automatica, 120:109148, 2020.
[11] Mahyar Fazlyab, Santiago Paternain, Alejandro Ribeiro, and Victor M Preciado. Distributed smooth and strongly convex optimization with inexact dual methods. In 2018 Annual American Control Conference (ACC), pages 3768–3773. IEEE, 2018.

9

[12] W Krichene, B Drighe`s, and A Bayen. Online Learning of Nash Equilibria in Congestion Games. SIAM J. Control Optim., 53(2):1056–1081, 2015.

[13] Jean-Michel Lasry and Pierre-Louis Lions. Mean ﬁeld games. Japan J. Math., 2(1):229–260, 2007.

[14] Sarah HQ Li, Yue Yu, Daniel Calderone, Lillian Ratliff, and Behc¸et Ac¸ıkmes¸e. Tolling for constraint satisfaction in markov decision process congestion games. arXiv preprint arXiv:1903.00747, 2019.

[15] X Lin. Environmental constraints in urban trafﬁc management: Trafﬁc impacts and an optimal control framework. 2018.

[16] Aarian Marshall. New york city ﬂexes again, extending cap on uber and lyft. https://www.wired.com/story/new-york-city-ﬂexesextending-cap-uber-lyft/. Accessed: 2021-02-14.

[17] Enrique Lobato Migue´lez, Luis Rouco Rodr´ıguez, TGS Roman, FM Echavarren Cerezo, Ma Isabel Navarrete Ferna´ndez, Rosa Casanova Lafarga, and Gerardo Lo´pez Camino. A practical approach to solve power system constraints with application to the spanish electricity market. IEEE Transactions on Power Systems, 19(4):2029–2037, 2004.

[18] Dov Monderer and Lloyd S Shapley. Potential games. Games Economic Behavior, 14(1):124–143, 1996.

[19] Ion Necoara and Valentin Nedelcu. Rate analysis of inexact dual ﬁrstorder methods application to dual decomposition. IEEE Transactions on Automatic Control, 59(5):1232–1243, 2013.

[20] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127–152, 2005.

[21] City

of

New

York.

Tlc

trip record data. https://www1.nyc.gov/site/tlc/about/tlc-trip-record-

data.page. Accessed: 2021-02-14.

[22] Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efﬁcient search of ﬁrst-order nash equilibria in nonconvex-concave smooth min-max problems. SIAM Journal on Optimization, 31(4):2508–2538, 2021.

[23] Michael Patriksson. The Trafﬁc Assignment Problem: Models and Methods. Courier Dover Publications, 2015.

[24] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.

[25] Lillian J Ratliff and Tanner Fiez. Adaptive incentive design. arXiv preprint arXiv:1806.05749 [cs.GT], 2018.

[26] Robert W Rosenthal. A class of games possessing pure-strategy nash equilibria. International Journal of Game Theory, 2(1):65–67, 1973.

[27] Aaron Roth, Jonathan Ullman, and Zhiwei Steven Wu. Watch and learn: Optimizing from revealed preferences feedback. In Proc. Ann. ACM Symp. Theory Comput., pages 949–962. ACM, 2016.

[28] Todd Schneider. Taxi and ridehailing usage in new york city. https://toddwschneider.com/dashboards/nyc-taxi-ridehailinguber-lyft-data/, 2021. Accessed: 2021-02-14.

[29] Lloyd S Shapley. Stochastic games. Proc. Nat. Acad. Sci., 39(10):1095–1100, 1953.

[30] Chaitanya Swamy. The effectiveness of stackelberg strategies and tolls for network congestion games. In Proc. ACM-SIAM Symp. Discrete Algorithms, pages 1133–1142. Society for industrial and applied mathematics, 2007.

[31] Uber. How surge pricing works. https://www.uber.com/us/en/drive/driverapp/how-surge-works/, 2021. Accessed: 2021-04-27.

[32] Yue Yu, Dan Calderone, Sarah HQ Li, Lillian J Ratliff, and Behc¸et Ac¸ıkmes¸e. A primal-dual approach to markovian network optimization. arXiv preprint arXiv:1901.08731[math.OC], 2019.

[33] Bo Zhou, Qiankun Song, Zhenjiang Zhao, and Tangzhi Liu. A reinforcement learning scheme for the equilibrium of the invehicle route choice problem based on congestion game. Applied Mathematics and Computation, 371:124895, 2020.

A Appendix A.1 Proof of Proposition 3

PROOF. d(τ ) is the dual function of the optimization problem
min F (x) s.t. Ay ≤ b.
y∈Y (P,p0 )
As the dual function of a convex optimization with linear constraints, it is concave [3, Prop 5.1.2]. The smoothness constant of d(τ ) follows from [20, Thm 1], where α is the strong convexity factor of F0. Finally, the computation of ∇d(τ ) follows directly from [3, Prop.B.25].
A.2 Proof of Lemma 1

PROOF. We denote yˆτ ( ) by yˆτ for simplicity. Since yˆτ ∈ W( τ , ) ⊂ Y(P, p0), using (11) we can show that

d(σ) ≤ L(yˆτ , σ).

(A.1)

Combining (A.1) with the fact that L(yˆτ , σ) = L(yˆτ , τ ) + ∇ˆ d(τ ) (σ − τ ), we obtain Lemma 1.

A.3 Proof of Lemma 2

PROOF. We denote yˆτ ( ) by yˆτ for simplicity and recall yτ from (11). From Proposition 3, we know that

∇d(τ ) = ∇ˆ d(τ ) + A(yτ − yˆτ ).

(A.2)

Substituting (A.2) into (14), we obtain the following

0 ≤d(σ) − d(τ ) − ∇ˆ d(τ )

α¯ (σ − τ ) +

σ−τ

2

2

2 (A.3)

− A(yτ − yˆτ ) (σ − τ ).

Furthermore, we can show

A(yτ − yˆτ ) (σ − τ ) ≤ yˆτ − yτ 2 · A 2 · σ − τ 2

α

2 A 22

2

≤ 2

yˆτ − yτ 2 +

2α

σ−τ 2,

(A.4)

where the ﬁrst step is due to the Cauchy–Schwarz inequality,

and the second step is due to the inequality of arithmetic

and geometric inequalities.

Next, we note that F (5) and subsequently L(y, τ ) (10) is strongly convex under Assumption 1. We combine this with the fact that L(yτ , τ ) = d(τ ) from (11) to obtain

α

2

2 yˆτ − yτ 2 ≤ L(yˆτ , τ ) − d(τ ).

(A.5)

10

From (17), yˆτ satisﬁes

L(yˆτ , τ ) − d(τ ) ≤ .

(A.6)

Summing up (17), (A.3), (A.4), (A.5), and 2×(A.6), we obtain (18), which completes the proof.

A.4 Lemma 3

Lemma 3 Under Assumption 1, if γ ≤ 21α¯ , τ s from Algorithm 1 satisfy

τ s+1 − τ

22 ≤ τ s − τ 22 + 2γ d(τ s+1) − L(ys, τ s) + 2 s

+ ∇ˆ d(τ s) (τ s − τ ) ,

∀ τ ∈ RC+, k ≥ 0. (A.7)

PROOF. Given τ ∈ RC+, let rs = τ s − τ 22. We compute rs+1 − rs using the law of cosine as

rs+1 − rs =2(τ s+1 − τ s) (τ s+1 − τ ) − τ s+1 − τ s 2 .
2
(A.8) From line 3 of Algorithm 1, τ s+1 = [τ s + γ(Ays − b)]+. Using [4, Lem 3.1], the projection onto RC+ implies that

0 ≤ (τ s + γ∇ˆ d(τ s) − τ s+1) (τ s+1 − τ )

(A.9)

From (A.9), we can upper bound (τ s+1 − τ s) (τ s+1 − τ ) and combine with (A.8) to obtain

rs+1 − rs ≤ 2γ∇ˆ d(τ s) (τ s+1 − τ ) − From Lemma 2, we recall

τ s+1 − τ s 2
2
(A.10)

L(ys, τ s) − d(τ s+1) − 2 s ≤∇ˆ d(τ s) (τ s − τ s+1) + α¯ τ s+1 − τ s 2 .
2

(A.11)

We can then combine (A.10) and 2γ×(A.11) to derive

rs+1 − rs + 2γ(L(ys, τ s) − d(τ s+1) − 2 s)
≤2γ∇ˆ d(τ s) (τ s − τ ) + (2γα¯ − 1) τ s+1 − τ s 2 ,
2
(A.12) and use the fact that 2γα¯ ≤ 1 to eliminate the τ s+1 − τ s 2
2
term and complete the proof.

11

