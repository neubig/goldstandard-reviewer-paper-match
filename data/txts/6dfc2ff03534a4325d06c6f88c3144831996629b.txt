VCR
Visual Commonsense Reasoning
From Recognition to Cognition: Visual Commonsense Reasoning

Rowan Zellers♠ Yonatan Bisk♠ Ali Farhadi♠♥ Yejin Choi♠♥ ♠Paul G. Allen School of Computer Science & Engineering, University of Washington
♥Allen Institute for Artiﬁcial Intelligence
visualcommonsense.com

Why is [person4 ] pointing at [person1 ]?

a) He is telling [person3 ] that [person1 b) He just told a joke. c) He is feeling accusatory towards [person1 d) He is giving [person1 ] directions.

] ordered the pancakes. ].

beI ccahuossee…a)

a) [person1 ] has the pancakes in front of him. b) [person4 ] is taking everyone’s order and asked for clariﬁcation. c) [person3 ] is looking at the pancakes and both she and
[person2 ] are smiling slightly. d) [person3 ] is delivering food to the table, and she might not
know whose order is whose.

How did [person2 ] get the money that’s in front of her?

a) [person2 ] is selling things on the street. b) [person2 ] earned this money playing music. c) She may work jobs for the maﬁa. d) She won money playing poker.

beI ccahuossee…b)

a) She is playing guitar for money. b) [person2 ] is a professional musician in an orchestra.

c) [person2 ] and [person1 ]are both holding instruments,

and were probably busking for that money. d) [person1 ] is putting money in [person2

]’s tip jar, while

she plays music.

Figure 1: VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right. Our questions challenge computer vision systems to go beyond recognition-level understanding, towards a higher-order cognitive and commonsense understanding of the world depicted by the image.

Abstract
Visual understanding goes well beyond object recognition. With one glance at an image, we can eﬀortlessly imagine the world beyond the pixels: for instance, we can infer people’s actions, goals, and mental states. While this task is easy for humans, it is tremendously diﬃcult for today’s vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer.
Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and highquality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show

that while humans ﬁnd VCR easy (over 90% accuracy), state-of-the-art vision models struggle (∼45%).
To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (∼65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.
1. Introduction
With one glance at an image, we can immediately infer what is happening in the scene beyond what is visually obvious. For example, in the top image of Figure 1, not only do we see several objects (people, plates, and cups), we can also reason about the entire situation: three people are dining together, they have already ordered their food before

1

the photo has been taken, [person3 ] is serving and
not eating with them, and what [person1 ] ordered are the pancakes and bacon (as opposed to the cheesecake), be-
cause [person4 ] is pointing to [person1 ] while
looking at the server, [person3 ] . Visual understanding requires seamless integration be-
tween recognition and cognition: beyond recognition-level perception (e.g., detecting objects and their attributes), one must perform cognition-level reasoning (e.g., inferring the likely intents, goals, and social dynamics of people) [13]. State-of-the-art vision systems can reliably perform recognition-level image understanding, but struggle with complex inferences, like those in Figure 1. We argue that as the ﬁeld has made signiﬁcant progress on recognition-level building blocks, such as object detection, pose estimation, and segmentation, now is the right time to tackle cognitionlevel reasoning at scale.
As a critical step toward complete visual understanding, we present the task of Visual Commonsense Reasoning. Given an image, a machine must answer a question that requires a thorough understanding of the visual world evoked by the image. Moreover, the machine must provide a rationale justifying why that answer is true, referring to the details of the scene, as well as background knowledge about how the world works. These questions, answers, and rationales are expressed using a mixture of rich natural language as well as explicit references to image regions. To support clean-cut evaluation, all our tasks are framed as multiple choice QA.
Our new dataset for this task, VCR, is the ﬁrst of its kind and is large-scale — 290k pairs of questions, answers, and rationales, over 110k unique movie scenes. A crucial challenge in constructing a dataset of this complexity at this scale is how to avoid annotation artifacts. A recurring challenge in most recent QA datasets has been that humanwritten answers contain unexpected but distinct biases that models can easily exploit. Often these biases are so prominent so that models can select the right answers without even looking at the questions [28, 61, 72].
Thus, we present Adversarial Matching, a novel QA assignment algorithm that allows for robust multiple-choice dataset creation at scale. The key idea is to recycle each correct answer for a question exactly three times — as a negative answer for three other questions. Each answer thus has the same probability (25%) of being correct: this resolves the issue of answer-only biases, and disincentivizes machines from always selecting the most generic answer. We formulate the answer recycling problem as a constrained optimization based on the relevance and entailment scores between each candidate negative answer and the gold answer, as measured by state-of-the-art natural language inference models [10, 57, 15]. A neat feature of our recycling algorithm is a knob that can control the tradeoﬀ between

human and machine diﬃculty: we want the problems to be hard for machines while easy for humans.
Narrowing the gap between recognition- and cognitionlevel image understanding requires grounding the meaning of the natural language passage in the visual data, understanding the answer in the context of the question, and reasoning over the shared and grounded understanding of the question, the answer, the rationale and the image. In this paper we introduce a new model, Recognition to Cognition Networks (R2C). Our model performs three inference steps. First, it grounds the meaning of a natural language passage with respect to the image regions (objects) that are directly referred to. It then contextualizes the meaning of an answer with respect to the question that was asked, as well as the global objects not mentioned. Finally, it reasons over this shared representation to arrive at an answer.
Experiments on VCR show that R2C greatly outperforms state-of-the-art visual question-answering systems: obtaining 65% accuracy at question answering, 67% at answer justiﬁcation, and 44% at staged answering and justiﬁcation. Still, the task and dataset is far from solved: humans score roughly 90% on each. We provide detailed insights and an ablation study to point to avenues for future research.
In sum, our major contributions are fourfold: (1) we formalize a new task, Visual Commonsense Reasoning, and (2) present a large-scale multiple-choice QA dataset, VCR, (3) that is automatically assigned using Adversarial Matching, a new algorithm for robust multiple-choice dataset creation. (4) We also propose a new model, R2C, that aims to mimic the layered inferences from recognition to cognition; this also establishes baseline performance on our new challenge. The dataset is available to download, along with code for our model, at visualcommonsense.com.
2. Task Overview
We present VCR, a new task that challenges vision systems to holistically and cognitively understand the content of an image. For instance, in Figure 1, we need to understand the activities ( [person3 ] is delivering
food), the roles of people ( [person1 ] is a customer who previously ordered food), the mental states of people ( [person1 ] wants to eat), and the likely events before
and after the scene ( [person3 ] will serve the pancakes next). Our task covers these categories and more: a distribution of the inferences required is in Figure 2.
Visual understanding requires not only answering questions correctly, but doing so for the right reasons. We thus require a model to give a rationale that explains why its answer is true. Our questions, answers, and rationales are written in a mixture of rich natural language as well as detection tags, like ‘ [person2 ] ’: this helps to provide an unambiguous link between the textual description of an

2

Hypothetical 5%
Role Scene 7% 5%

Mental 8%

Explanation 38%

Temporal 13%

Activity 24%

Why is [person11] wearing sunglasses inside? What are [person1] and [person2] doing?
Explanation WhatAwctiilvl i[typerson6] do after unpaTcekminpgotrhael groceries? WhatMise[nptaelrson3] thinking while [perRSscooelnen5e] shakes his hand? WhatHiysp[optheertsicoanl 1]’s relation to [person4]?
Where is [person1] now?
What would happen if [person3] fell asleep?

Figure 2: Overview of the types of inference required by questions in VCR. Of note, 38% of the questions are explanatory ‘why’ or ‘how’ questions, 24% involve cognitionlevel activities, and 13% require temporal reasoning (i.e., what might come next). These categories are not mutually exclusive; an answer might require several hops of diﬀerent types of inferences (see appendix Sec A).

object (‘the man on the left in the white shirt’) and the corresponding image region.
To make evaluation straightforward, we frame our ultimate task – of staged answering and justiﬁcation – in a multiple-choice setting. Given a question along with four answer choices, a model must ﬁrst select the right answer. If its answer was correct, then it is provided four rationale choices (that could purportedly justify its correct answer), and it must select the correct rationale. We call this Q→AR as for the model prediction to be correct requires both the chosen answer and then the chosen rationale to be correct.
Our task can be decomposed into two multiple-choice sub-tasks, that correspond to answering (Q→A) and justiﬁcation (QA→R) respectively:
Deﬁnition VCR subtask. A single example of a VCR subtask consists of an image I, and:
• A sequence o of object detections. Each object detection oi consists of a bounding box b, a segmentation mask m1, and a class label i ∈ L.
• A query q, posed using a mix of natural language and pointing. Each word qi in the query is either a word in a vocabulary V, or is a tag referring to an object in o.
• A set of N responses, where each response r(i) is written in the same manner as the query: with natural language and pointing. Exactly one response is correct.
The model chooses a single (best) response.
In question-answering (Q→A), the query is the question and the responses are answer choices. In answer justiﬁcation (QA→R), the query is the concatenated question and correct answer, while the responses are rationale choices.
1The task is agnostic to the representation of the mask, but it could be thought of as a list of polygons p, with each polygon consisting of a sequence of 2d vertices inside the box p j = {xt, yt}t.

In this paper, we evaluate models in terms of accuracy and use N=4 responses. Baseline accuracy on each subtask is then 25% (1/N). In the holistic setting (Q→AR), baseline accuracy is 6.25% (1/N2) as there are two subtasks.
3. Data Collection
In this section, we describe how we collect the questions, correct answers and correct rationales for VCR. Our key insight – towards collecting commonsense visual reasoning problems at scale – is to carefully select interesting situations. We thus extract still images from movie clips. The images from these clips describe complex situations that humans can decipher without additional context: for instance, in Figure 1, we know that [person3 ] will serve
[person1 ] pancakes, whereas a machine might not understand this unless it sees the entire clip.
Interesting and Diverse Situations To ensure diversity, we make no limiting assumptions about the predeﬁned set of actions. Rather than searching for predeﬁned labels, which can introduce search engine bias [76, 16, 20], we collect images from movie scenes. The underlying scenes come from the Large Scale Movie Description Challenge [67] and YouTube movie clips.2 To avoid simple images, we train and apply an ‘interestingness ﬁlter’ (e.g. a closeup of a syringe in Figure 3).3
We center our task around challenging questions requiring cognition-level reasoning. To make these cognitionlevel questions simple to ask, and to avoid the clunkiness of referring expressions, VCR’s language integrates object tags ( [person2 ] ) and explicitly excludes referring expressions (‘the woman on the right.’) These object tags are detected from Mask-RCNN [29, 24], and the images are ﬁltered so as to have at least three high-conﬁdence tags.
Crowdsourcing Quality Annotations Workers on Amazon Mechanical Turk were given an image with detections, along with additional context in the form of video captions.4 They then ask one to three questions about the image; for each question, they provide a reasonable answer and a rationale. To ensure top-tier work, we used a system of quality checks and paid our workers well.5
The result is an underlying dataset with high agreement and diversity of reasoning. Our dataset contains a myriad of interesting commonsense phenomena (Figure 2) and a great diversity in terms of unique examples (Supp Section A); almost every answer and rationale is unique.
2Namely, Fandango MovieClips: youtube.com/user/movieclips. 3We annotated images for ‘interestingness’ and trained a classiﬁer using CNN features and detection statistics, details in the appendix, Sec B. 4This additional clip-level context helps workers ask and answer about what will happen next. 5More details in the appendix, Sec B.

3

LSMDC

Shot+Object Detection

t-1 Someone lifts up the adrenaline needle. t He looks down at her. t+1 She sits up with the needle in her chest.

Interestingness Filter

Crowd workers ask and answer questions

Question: What is [person1] doing?

[person1] is injecting a needle into Answer: someone on the floor.

(likely)

Rationale: [person1] has a needle in his hand and is aggressively lowering it, in a stabbing motion.

Figure 3: An overview of the construction of VCR. Using a state-of-the-art object detector [29, 24], we identify the objects in each image. The most interesting images are passed to crowd workers, along with scene-level context in the form of scene descriptions (MovieClips) and video captions (LSMDC, [67]). The crowd workers use a combination of natural language and detection tags to ask and answer challenging visual questions, also providing a rationale justifying their answer.

4. Adversarial Matching
We cast VCR as a four-way multiple choice task, to avoid the evaluation diﬃculties of language generation or captioning tasks where current metrics often prefer incorrect machine-written text over correct human-written text [49]. However, it is not obvious how to obtain highquality incorrect choices, or counterfactuals, at scale. While past work has asked humans to write several counterfactual choices for each correct answer [75, 46], this process is expensive. Moreover, it has the potential of introducing annotation artifacts: subtle patterns that are by themselves highly predictive of the ‘correct’ or ‘incorrect’ label [72, 28, 61].
In this work, we propose Adversarial Matching: a new method that allows for any ‘language generation’ dataset to be turned into a multiple choice test, while requiring minimal human involvement. An overview is shown in Figure 4. Our key insight is that the problem of obtaining good counterfactuals can be broken up into two subtasks: the counterfactuals must be as relevant as possible to the context (so that they appeal to machines), while they cannot be overly similar to the correct response (so that they don’t become correct answers incidentally). We balance between these two objectives to create a dataset that is challenging for machines, yet easy for humans.
Formally, our procedure requires two models: one to compute the relevance between a query and a response, Prel, and another to compute the similarity between two response choices, Psim. Here, we employ state-of-the-art models for Natural Language Inference: BERT [15] and ESIM+ELMo [10, 57], respectively.6 Then, given dataset examples (qi, ri)1≤i≤N, we obtain a counterfactual for each qi by performing maximum-weight bipartite matching [55, 40] on a weight matrix W ∈ RN×N, given by
Wi, j = log(Prel(qi, r j)) + λ log(1 − Psim(ri, r j)). (1)
Here, λ>0 controls the tradeoﬀ between similarity and rel-
6We ﬁnetune Prel (BERT), on the annotated data (taking steps to avoid data leakage), whereas Psim (ESIM+ELMo) is trained on entailment and paraphrase data - details in appendix Sec C.

Why are [person1] and [person3] holding their
foreheads together?
Why do [person1] and [person3] have their
hands clasped?
Why are [person6], [person8] and [person14] standing in close proximity?
Why are [person1] and [person2] gathered together?

q1 q2 q3 qq4 < l a t e x i t s h a 1 _ b a s e 6 4 = " l 4 z v 0 8 a C g f A P A E o 2 9 a / D + o A 5 M F M = " > A A A D B H i c d V I 7 b x N B E F 4 f j 4 T j 5 U B J c 8 J C Q h T W H Y o U 6 C J I k Q Y R J E w i 2 Z Y 1 t x 6 f V 9 6 9 P W b n k j g n 1 / k T t N B R I V r + B w X / h b V z B W e T k V b 7 7 T f P n Z m 0 0 M p x H P 9 u B T d u 3 r q 9 t X 0 n v H v v / o O H 7 Z 1 H n 5 w t S W J P W m 3 p J A W H W u X Y Y 8 U a T w p C M K n G 4 3 T 2 d q k / P k V y y u Y f e V 7 g 0 E C W q 4 m S w J 4 a t d u D 1 O q x m x t / V Z 8 X o 9 1 R u x N 3 4 5 V E m y C p Q U f U c j T a a f 0 Z j K 0 s D e Y s N T j X T + K C h x U Q K 6 l x E Q 5 K h w X I G W T Y 9 z A H g 2 5 Y r U p f R M 8 8 M 4 4 m l v z J O V q x / 3 p U Y N y y O m 9 p g K d u X b c k / 6 f r l z x 5 N a x U X p S M u b x K N C l 1 x D Z a 9 i E a K 0 L J e u 4 B S F K + 1 k h O g U C y 7 1 Y j i y k 1 K 7 J n j Z 9 U E r R s M h l B M V X y v M k S a q c u m m 2 4 J i R Z 9 l P J s y a b m u a 7 J L 0 W z B J u p k i t n T G k 7 t r E B + i n R f j O d + 5 9 g Q R s 6 U U 1 A M o M n C + q + g 7 D 0 C 9 E s j 7 + T d B 7 2 X 3 d T T 7 s d v b f 1 J u x L Z 6 I p + K 5 S M S e 2 B e H 4 k j 0 h B S n 4 o v 4 K r 4 F l 8 H 3 4 E f w 8 8 o 0 a N U + j 0 V D g l 9 / A c c Z / k w = < / l a t e x i t >

a1

They are about to kiss.

a [person1] and

2

[person3] are

praying.

a3

They are a family visiting the ﬂea

market.

a4

They are discussing a new law .

… …

Figure 4: Overview of Adversarial Matching. Incorrect choices are obtained via maximum-weight bipartite matching between queries and responses; the weights are scores from state-of-the-art natural language inference models. Assigned responses are highly relevant to the query, while they diﬀer in meaning versus the correct responses.

evance.7 To obtain multiple counterfactuals, we perform several bipartite matchings. To ensure that the negatives are diverse, during each iteration we replace the similarity term with the maximum similarity between a candidate response r j and all responses currently assigned to qi.
Ensuring dataset integrity To guarantee that there is no question/answer overlap between the training and test sets, we split our full dataset (by movie) into 11 folds. We match the answers and rationales invidually for each fold. Two folds are pulled aside for validation and testing.
5. Recognition to Cognition Networks
We introduce Recognition to Cognition Networks (R2C), a new model for visual commonsense reasoning. To perform well on this task requires a deep understanding of language, vision, and the world. For example, in Figure 5, answering ‘Why is [person4 ] pointing at [person1 ] ?’ requires multiple inference steps. First, we ground the meaning of the query and each response, which involves referring to the image for the
7We tuned this hyperparameter by asking crowd workers to answer multiple-choice questions at several thresholds, and chose the value for which human performance is above 90% - details in appendix Sec C.

4

Image
I
+ objects
o

CNN

Query q
Respro(in) se < l a t e x i t s h a 1 _ b a s e 6 4 = " R B V p o I E N a A X R u P k C / / 7 D a l f N j K A = " > A A A D C H i c d V L N j t M w E H b D 3 x L + u s C N S 0 S F t H C o k h U S c F s B B y 6 I R a L s S m 2 p J u 4 0 t W r H 0 X g C 2 4 3 y B L w E V 7 h x Q l x 5 C w 6 8 C 2 4 3 B 9 K y I 1 n + / M 2 v Z y Y t t H I c x 7 8 7 w Y W L l y 5 f 2 b k a X r t + 4 + a t 7 u 7 t 9 8 6 W J H E g r b Z 0 n I J D r X I c s G K N x w U h m F T j U b p 4 s d I f f U R y y u b v e F n g 2 E C W q 5 m S w J 6 a d O + O U q u n b m n 8 V V H 9 o d p T D + t J t x f 3 4 7 V E 2 y B p Q E 8 0 c j j Z 7 f w Z T a 0 s D e Y s N T g 3 T O K C x x U Q K 6 m x D k e l w w L k A j I c e p i D Q T e u 1 u X X 0 Q P P T K O Z J X 9 y j t b s v x 4 V G L e q 0 F s a 4 L n b 1 K 3 I / + m G J c + e j i u V F y V j L s 8 S z U o d s Y 1 W v Y i m i l C y X n o A k p S v N Z J z I J D s O 9 b K Y k r N i u y n 1 k 8 q C V q 2 m Y y g m C t 5 0 m Y J t V O n 7 T a c E 5 I s + 8 n k W Z t N T f t d k t 4 I Z g m 3 U 6 T W L h h S d 2 7 i l + i n R f j a d + 5 N g Q R s 6 V E 1 A s o M n N R V c 4 d h 6 B c i 2 R z / N h j s 9 5 / 1 k 7 e P e w f P m 8 3 Y E f f E f b E n E v F E H I h X 4 l A M h B S n 4 o v 4 K r 4 F n 4 P v w Y / g 5 5 l p 0 G l 8 7 o i W B L / + A k 5 P A A E = < / l a t e x i t >

Why is [person4 ] pointing at [person1 ]?
He is telling [person3 ] that BERT [person1 ] ordered pancakes.

LSTM1

Grounding

LSTM2

LSTM3

Why

is

[person4]

...

LSTM1 LSTM2

LSTM3

He

is

telling ...

Contextualization

Why

is

...

He

is

telling

...

He is telling ...

Reasoning

f
< l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 R J B n m g m Y p F t a 8 n b f i 2 6 6 5 U Q 0 o = " > A A A D G X i c d V L N b t N A E N 6 Y v 2 L + U j h y s Y g q p Q h F d o V E u V X A g Q u i S I R W i k M 0 3 o y d V X a 9 7 u 4 Y G i w / A a / A S 3 C F G y f E l R M H 3 o V N a i S c 0 J F W + + 0 3 v z s z S S G F p T D 8 1 f E u X L x 0 + c r W V f / a 9 R s 3 b 3 W 3 b 7 + x u j Q c h 1 x L b Y 4 T s C h F j k M S J P G 4 M A g q k X i U z J 8 u 9 U f v 0 F i h 8 9 e 0 K H C s I M t F K j i Q o y b d n X Q S 0 w w J + r E C m i V p d V I / + A t N / b b q i 9 1 6 d 9 L t h Y N w J c E m i B r Q Y 4 0 c T r Y 7 v + O p 5 q X C n L g E a 0 d R W N C 4 A k O C S 6 z 9 u L R Y A J 9 D h i M H c 1 B o x 9 X q P 3 W w 4 5 h p k G r j T k 7 B i v 3 X o w J l 7 U I l z n J Z q V 3 X L c n / 6 U Y l p f v j S u R F S Z j z s 0 R p K Q P S w b I 5 w V Q Y 5 C Q X D g A 3 w t U a 8 B k Y 4 O R a 2 M q i S k n C 6 P e t n 1 Q c J G 8 z m Y F i J v h p m z U o r f j Q b s M 5 I Y 0 m N 6 o 8 a 7 O J a r 9 L I 9 e C a Y O b K R K t 5 w S J P T f x M 3 T T M v j C d e 5 l g Q Z I m / t V D C Z T c F p X z e 3 7 v l u I a H 3 8 m 2 C 4 N 3 g 8 i F 4 9 7 B 0 8 a T Z j i 9 1 l 9 1 i f R e w R O 2 D P 2 S E b M s 4 + s s / s C / v q f f K + e d + 9 H 2 e m X q f x u c N a 4 v 3 8 A 0 s c B t E = < / l a t e x i t >

✓

(q,

r(i))

LSTM1

LSTM2

LSTM3

He [person4]

is

telling

...

is

pointing

...

...

Figure 5: High-level overview of our model, R2C. We break the challenge of Visual Commonsense Reasoning into three components: grounding the query and response, contextualizing the response within the context of the query and the entire image, and performing additional reasoning steps on top of this rich representation.

two people. Second, we contextualize the meaning of the query, response, and image together. This step includes resolving the referent ‘he,’ and why one might be pointing in a diner. Third, we reason about the interplay of relevant image regions, the query, and the response. In this example, the model must determine the social dynamics
between [person1 ] and [person4 ] . We formulate our model as three high-level stages: grounding, contextualization, and reasoning, and use standard neural building blocks to implement each component.
In more detail, recall that a model is given an image, a set of objects o, a query q, and a set of responses r(i) (of which exactly one is correct). The query q and response choices r(i) are all expressed in terms of a mixture of natural language and pointing to image regions: notation-wise, we will represent the object tagged by a word w as ow. If w isn’t a detection tag, ow refers to the entire image boundary. Our model will then consider each response r separately, using the following three components:
Grounding The grounding module will learn a joint image-language representation for each token in a sequence. Because both the query and the response contain a mixture of tags and natural language words, we apply the same grounding module for each (allowing it to share parameters). At the core of our grounding module is a bidirectional LSTM [34] which at each position is passed as input a word representation for wi, as well as visual features for owi . We use a CNN to learn object-level features: the visual representation for each region o is Roi-Aligned from its bounding region [63, 29]. To additionally encode information about the object’s class label o, we project an embedding of o (along with the object’s visual features) into a shared hidden representation. Let the output of the LSTM over all positions be r, for the response and q for the query.
Contextualization Given a grounded representation of the query and response, we use attention mechanisms to contextualize these sentences with respect to each other and the image context. For each position i in the response, we will deﬁne the attended query representation as qˆ i using the

following equation:
αi, j = softmax(riWq j) qˆ i = αi, jq j. (2)
j j
To contextualize an answer with the image, including implicitly relevant objects that have not been picked up from the grounding stage, we perform another bilinear attention between the response r and each object o’s image features. Let the result of the object attention be oˆi.
Reasoning Last, we allow the model to reason over the response, attended query and objects. We accomplish this using a bidirectional LSTM that is given as context qˆ i, ri, and oˆi for each position i. For better gradient ﬂow through the network, we concatenate the output of the reasoning LSTM along with the question and answer representations for each timestep: the resulting sequence is max-pooled and passed through a multilayer perceptron, which predicts a logit for the query-response compatibility.
Neural architecture and training details For our image features, we use ResNet50 [30]. To obtain strong representations for language, we used BERT representations [15]. BERT is applied over the entire question and answer choice, and we extract a feature vector from the second-tolast layer for each word. We train R2C by minimizing the multi-class cross entropy between the prediction for each response r(i), and the gold label. See the appendix (Sec E) for detailed training information and hyperparameters.8
6. Results
In this section, we evaluate the performance of various models on VCR. Recall that our main evaluation mode is the staged setting (Q→AR). Here, a model must choose the right answer for a question (given four answer choices), and then choose the right rationale for that question and answer (given four rationale choices). If it gets either the answer or the rationale wrong, the entire prediction will be wrong. This holistic task decomposes into two sub-tasks wherein we can train individual models: question answering (Q→A)
8Our code is also available online at visualcommonsense.com.

5

VQA Text Only

Model
Chance
BERT BERT (response only) ESIM+ELMo LSTM+ELMo
RevisitedVQA [38] BottomUpTopDown[4] MLB [42] MUTAN [6]
R2C
Human

Q→A Val Test
25.0 25.0
53.8 53.9 27.6 27.7 45.8 45.9 28.1 28.3
39.4 40.5 42.8 44.1 45.5 46.2 44.4 45.5
63.8 65.1
91.0

QA → R Val Test
25.0 25.0
64.1 64.5 26.3 26.2 55.0 55.1 28.7 28.5
34.0 33.7 25.1 25.1 36.1 36.8 32.0 32.2
67.2 67.3
93.0

Q → AR Val Test
6.2 6.2
34.8 35.0 7.6 7.3 25.3 25.6 8.3 8.4
13.5 13.8 10.7 11.0 17.0 17.2 14.6 14.6
43.1 44.0
85.0

Table 1: Experimental results on VCR. VQA models struggle on both question-answering (Q → A) as well as answer justiﬁcation (Q → AR), possibly due to the complex language and diversity of examples in the dataset. While language-only models perform well, our model R2C obtains a signiﬁcant performance boost. Still, all models underperform human accuracy at this task. For more up-to-date results, see the leaderboard at visualcommonsense.com/leaderboard.

as well as answer justiﬁcation (QA→R). Thus, in addition to reporting combined Q→AR performance, we will also report Q→A and QA→R.
Task setup A model is presented with a query q, and four response choices r(i). Like our model, we train the baselines using multi-class cross entropy between the set of responses and the label. Each model is trained separately for question answering and answer justiﬁcation.9
6.1. Baselines
We compare our R2C to several strong language and vision baselines.
Text-only baselines We evaluate the level of visual reasoning needed for the dataset by also evaluating purely text-only models. For each model, we represent q and r(i) as streams of tokens, with the detection tags replaced by the object name (e.g. chair5 → chair). To minimize the discrepancy between our task and pretrained models, we replace person detection tags with gender-neutral names. a. BERT [15]: BERT is a recently released NLP model that achieves state-of-the-art performance on many NLP tasks. b. BERT (response only) We use the same BERT model, however, during ﬁne-tuning and testing the model is only given the response choices r(i). c. ESIM+ELMo [10]: ESIM is another high performing model for sentence-pair classiﬁcation tasks, particularly when used with ELMo embeddings [57].
9We follow the standard train, val and test splits.

Model

Q→A

R2C

63.8

No query

48.3

No reasoning module 63.6

No vision representation 53.1

GloVe representations 46.4

QA → R
67.2
43.5 65.7 63.2 38.3

Q → AR
43.1
21.5 42.2 33.8 18.3

Table 2: Ablations for R2C, over the validation set. ‘No query’ tests the importance of integrating the query during contextualization; removing this reduces Q→AR performance by 20%. In ‘no reasoning’, the LSTM in the reasoning stage is removed; this hurts performance by roughly 1%. Removing the visual features during grounding, or using GloVe embeddings rather than BERT, lowers performance signiﬁcantly, by 10% and 25% respectively.

d. LSTM+ELMo: Here an LSTM with ELMo embeddings is used to score responses r(i).
VQA Baselines Additionally we compare our approach to models developed on the VQA dataset [5]. All models use the same visual backbone as R2C (ResNet 50) as well as text representations (GloVe; [56]) that match the original implementations. e. RevisitedVQA [38]: This model takes as input a query, response, and image features for the entire image, and passes the result through a multilayer perceptron, which has to classify ‘yes’ or ‘no’.10 f. Bottom-up and Top-down attention (BottomUpTopDown) [4]: This model attends over region proposals given by an object detector. To adapt to VCR, we pass this model object regions referenced by the query and response. g. Multimodal Low-rank Bilinear Attention (MLB) [42]: This model uses Hadamard products to merge the vision and language representations given by a query and each region in the image. h. Multimodal Tucker Fusion (MUTAN) [6]: This model expresses joint vision-language context in terms of a tensor decomposition, allowing for more expressivity.
We note that BottomUpTopDown, MLB, and MUTAN all treat VQA as a multilabel classiﬁcation over the top 1000 answers [4, 50]. Because VCR is highly diverse (Supp A), for these models we represent each response r(i) using a GRU [11].11 The output logit for response i is given by the dot product between the ﬁnal hidden state of the GRU encoding r(i), and the ﬁnal representation from the model.
Human performance We asked ﬁve diﬀerent workers on Amazon Mechanical Turk to answer 200 dataset questions from the test set. A diﬀerent set of ﬁve workers were asked to choose rationales for those questions and answers. Predictions were combined using a majority vote.
10For VQA, the model is trained by sampling positive or negative answers for a given question; for our dataset, we simply use the result of the perceptron (for response r(i)) as the i-th logit.
11To match the other GRUs used in [4, 42, 6] which encode q.

6

Why is [person1 ] pointing a gun at [person2 ] ?
a) [person1 ] wants to kill [person2 ] .(1%)
b) [person1 ] and [person3 ] are robbing the bank and [person2 ] is the bank manager. (71%)
c) [person2 ] has done something to upset [person1 ] . (18%)
d) Because [person2 ] is [person1 ] ’s daughter. [person1 ] wants to protect [person2 ] . (8%)

b) is right because...
a) [person1 ] is chasing [person1 ] and [person3 ] because they just robbed a bank. (33%)
b) Robbers will sometimes hold their gun in the air to get everyone’s attention. (5%)
c) The vault in the background is similar to a bank vault. [person3 ] is waiting by the vault for someone to open it. (49%)
d) A room with barred windows and a counter usually resembles a bank. (11%)

What would [person1 ] do if she caught [person2 ] and [person3 ] whispering?
a) [person1 ] would look to her left. (7%) b) She would play with [book1 ] . (7%) c) She would look concerned and ask what was funny. (39%) d) She would switch their seats. (45%)

d) is right because...
a) When students are talking in class they’re supposed to be listening - the teacher separates them. (64%)
b) Plane seats are very cramped and narrow, and it requires cooperation from your seat mates to help get through. (15%)
c) It’s not unusual for people to want to get the closest seats to a stage. (14%)
d) That’s one of the only visible seats I can see that’s still open, the plane is mostly full. (6%)

What’s going to happen next?
a) [person2 ] is going to walk up and punch [person4 ] in the face. (10%) b) Someone is going to read [person4 ] a bedtime story. (15%) c) [person2 ] is going to fall down. (5%) d) [person2 ] is going to say how cute [person4 ] ’s children are. (68%)

d) is right because...
a) They are the right age to be father and son and [person5 ] is hugging [person3 ] like they are his son. (1%)
b) It looks like [person4 ] is showing the photo to [person2 ] , and [person2 ] will want to be polite. (31%)
c) [person2 ] is smirking and looking down at [person4 ] . (6%)
d) You can see [person4 ] smiling and facing the crib and decor in the room (60%)

Why can’t [person3 ] go in the house with [person1 ] and [person2 ] ?

b) is right because... a) [person1 ] is going away by himself. (60%)

a) She does not want to be there. (12%) b) [person3 ] has [dog1 ] with her. (14%) c) She needs the light. (45%) d) She is too freaked out (26%)

b) [dog1 ] is small enough to carry. [person3 ] appears to own him. (33%)
c) If [dog1 ] was in the house, he would likely knock over [pottedplant6 ] and likely scratch [couch1 ] . (4%)

d) [person1 ] looks like he may have lead [person2 ] into the room to see [dog1 ] .(1%)

Figure 6: Qualitative examples from R2C. Correct predictions are highlighted in blue . Incorrect predictions are in red with the correct choices bolded. For more predictions, see see visualcommonsense.com/explore.

6.2. Results and Ablations
We present our results in Table 1. Of note, standard VQA models struggle on our task. The best model, in terms of Q→AR accuracy, is MLB, with 17.2% accuracy. Deep textonly models perform much better: most notably, BERT [15] obtains 35.0% accuracy. One possible justiﬁcation for this gap in performance is a bottlenecking eﬀect: whereas VQA models are often built around multilabel classiﬁcation of the top 1000 answers, VCR requires reasoning over two (often

long) text spans. Our model, R2C obtains an additional boost over BERT by 9% accuracy, reaching a ﬁnal performance of 44%. Still, this ﬁgure is nowhere near human performance: 85% on the staged task, so there is signiﬁcant headroom remaining.
Ablations We evaluated our model under several ablations to determine which components are most important. Removing the query representation (and query-response contextualization entirely) results in a drop of 21.6% ac-

7

curacy points in terms of Q → AR performance. Interestingly, this setting allows it to leverage its image representation more heavily: the text based response-only models (BERT response only, and LSTM+ELMo) perform barely better than chance. Taking the reasoning module lowers performance by 1.9%, which suggests that it is beneﬁcial, but not critical for performance. The model suﬀers most when using GloVe representations instead of BERT: a loss of 24%. This suggests that strong textual representations are crucial to VCR performance.
Qualitative results Last, we present qualitative examples in Figure 6. R2C works well for many images: for instance, in the ﬁrst row, it correctly infers that a bank robbery is happening. Moreover, it picks the right rationale: even though all of the options have something to do with ‘banks’ and ‘robbery,’ only c) makes sense. Similarly, analyzing the examples for which R2C chooses the right answer but the wrong rationale allows us to gain more insight into its understanding of the world. In the third row, the model incorrectly believes there is a crib while assigning less probability mass on the correct rationale - that [person2 ] is
being shown a photo of [person4 ] ’s children, which is
why [person2 ] might say how cute they are.
7. Related Work
Question Answering Visual Question Answering [5] was one of the ﬁrst large-scale datasets that framed visual understanding as a QA task, with questions about COCO images [49] typically answered with a short phrase. This line of work also includes ‘pointing’ questions [45, 93] and templated questions with open ended answers [86]. Recent datasets also focus on knowledge-base style content [80, 83]. On the other hand, the answers in VCR are entire sentences, and the knowledge required by our dataset is largely background knowledge about how the world works.
Recent work also includes movie or TV-clip based QA [75, 51, 46]. In these settings, a model is given a video clip, often alongside additional language context such as subtitles, a movie script, or a plot summary.12 In contrast, VCR features no extra language context besides the question. Moreover, the use of explicit detection tags means that there is no need to perform person identiﬁcation [66] or linkage with subtitles.
An orthogonal line of work has been on referring expressions: asking to what image region a natural language sentence refers to [60, 52, 65, 87, 88, 59, 36, 33]. We explicitly avoid referring expression-style questions by using indexed detection tags (like [person1 ] ).
Last, some work focuses on commonsense phenomena, such as ‘what if’ and ‘why’ questions [79, 58]. However,
12As we ﬁnd in Appendix D, including additional language context tends to boost model performance.

the space of commonsense inferences is often limited by the underlying dataset chosen (synthetic [79] or COCO [58] scenes). In our work, we ask commonsense questions in the context of rich images from movies.
Explainability AI models are often right, but for questionable or vague reasons [7]. This has motivated work in having models provide explanations for their behavior, in the form of a natural language sentence [31, 9, 41] or an attention map [32, 35, 37]. Our rationales combine the best of both of these approaches, as they involve both natural language text as well as references to image regions. Additionally, while it is hard to evaluate the quality of generated model explanations, choosing the right rationale in VCR is a multiple choice task, making evaluation straightforward.
Commonsense Reasoning Our task uniﬁes work involving reasoning about commonsense phenomena, such as physics [54, 84], social interactions [2, 77, 12, 27], procedure understanding [91, 3] and predicting what might happen next in a video [74, 17, 92, 78, 18, 64, 85].
Adversarial Datasets Past work has proposed the idea of creating adversarial datasets, whether by balancing the dataset with respect to priors [25, 28, 62] or switching them at test time [1]. Most relevant to our dataset construction methodology is the idea of Adversarial Filtering [89].13 Correct answers are human-written, while wrong answers are chosen from a pool of machine-generated text that is further validated by humans. However, the correct and wrong answers come from fundamentally diﬀerent sources, which raises the concern that models can cheat by performing authorship identiﬁcation rather than reasoning over the image. In contrast, in Adversarial Matching, the wrong choices come from the exact same distribution as the right choices, and no human validation is needed.
8. Conclusion
In this paper, we introduced Visual Commonsense Reasoning, along with a large dataset VCR for the task that was built using Adversarial Matching. We presented R2C, a model for this task, but the challenge – of cognition-level visual undertanding – is far from solved.
Acknowledgements
We thank the Mechanical Turk workers for doing such an outstanding job with dataset creation - this dataset and paper would not exist without them. Thanks also to Michael Schmitz for helping with the dataset split and Jen Dumas for legal advice. This work was supported by the National Science Foundation through a Graduate Research Fellowship (DGE1256082) and NSF grants (IIS-1524371, 1637479, 165205, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, the Sloan Research Foundation through a Sloan Fellowship, the Allen Institute for Artiﬁcial Intelligence, the NVIDIA Artiﬁcial Intelligence Lab, and gifts by Google and Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.
13This was used to create the SWAG dataset, a multiple choice NLP dataset for natural language inference.

8

Appendix

Abstract

In our work we presented the new task of Visual Commonsense Reasoning and introduced a large-scale dataset for the task, VCR, along with Adversarial Matching, the machinery that made the dataset construction possible. We also presented R2C, a new model for the task. In the supplemental material, we provide the following items that shed further insight on these contributions:
• Additional dataset analysis (Section A) • More information about dataset creation (Section B)
and Adversarial Matching (Section C) • An extended discussion on language priors (Section D) • Model hyperparameters used (Section E) • Additional VQA Baseline Results, with BERT embed-
dings (Section F) • A datasheet for VCR (Section G) • A visualization of R2C’s predictions (Section H)
For more examples, and to obtain the dataset and code, check out visualcommonsense.com.

A. Dataset Analysis
In this section, we continue our high-level analysis of VCR.
A.1. Language complexity and diversity
How challenging is the language in VCR? We show several statistics in Table 3. Of note, unlike many questionanswering datasets wherein the answer is a single word, our answers average to more than 7.5 words. The rationales are even longer, averaging at more than 16 words.
An additional informative statistic is the counts of unique answers and rationales in the dataset, which we plot in Figure 7. As shown, almost every answer and rationale is unique.
A.2. Objects covered
On average, there are roughly two objects mentioned over a question, answer, and rationale. Most of these objects are people (Figure 8), though other types of COCO objects are common too [49]. Objects such as ‘chair,’ ‘tie,’ and ‘cup’ are often detected, however, these objects vary in terms of scene importance: even though more ties exist in the data than cars, workers refer to cars more in their questions, answers, and rationales. Some objects, such as hair driers and snowboards, are rarely detected.

Examples CDF

Number of questions Number of answers per question Number of rationales per question
Number of images Number of movies covered
Average question length Average answer length Average rationale length Average # of objects mentioned

Train Val Test

212,923 26,534 25,263

4

4

4

4

4

4

80,418 9,929 9,557 1,945 244 189

6.61 7.54 16.16 1.84

6.63 7.65 16.19 1.85

6.58 7.55 16.07 1.82

Table 3: High level dataset statistics, split by fold (train, validation, and test). Note that we held out one fold in the dataset for blind evaluation at a later date; this fold is blind to us to preserve the integrity of the held-out data. Accordingly, the statistics of that fold are not represented here.
1.00

0.75

0.50

0.25 0.000.00

VQA

MovieQA

TVQA

VCR Answers

V7W

VCR Rationales

0.25

0.50

0.75

1.00

Fraction of cumulative examples

Figure 7: CDF of dataset examples ordered by frequency in question-answering datasets [5, 93, 75, 46]. To obtain this plot, we sampled 10,000 answers from each dataset (or rationales, for ‘VCR rationales’). We consider two examples to be the same if they exactly match, after tokenization, lemmatization, and removal of stopwords. Where many datasets in this space are light-tailed, our dataset shows great diversity (e.g. almost every rationale is unique.)

A.3. Movies covered
Our dataset also covers a broad range of movies - over 2000 in all, mostly via MovieClips (Figure 9). We note that since we split the dataset by movie, the validation and test sets cover a completely disjoint set of movies, which forces a model to generalize. For each movie image, workers ask 2.6 questions on average (Figure 10), though the exact number varies - by design, workers ask more questions for more interesting images.
A.4. Inference types
It is challenging to accurately categorize commonsense and cognition-level phenomena in the dataset. One approach that we presented in Figure 2 is to categorize questions by type: to estimate this over the entire training set, we used a several patterns, which we show in Table 4. Still,

9

Type Freq. Patterns
Explanation 38% why, how come, how does Activity 24% doing, looking, event, playing, preparing Temporal 13% happened, before, after, earlier, later, next Mental 8% feeling, thinking, saying, love, upset, angry Role 7% relation, occupation, strangers, married Scene 5% where, time, near
Hypothetical 5% if, would, could, chance, might, may
Table 4: Some of the rules we used to determine the type of each question. Any question containing a word from one of the above groups (such as ‘why’) was determined to be of that type (‘explanation’).
we note that automatic categorization of the inference types required for this task is hard. This is in part because a single question might require multiple types of reasoning: for example, ‘Why does person1 feel embarrassed?’ requires reasoning about person1’s mental state, as well as requiring an explanation. For this reason, we argue that this breakdown underestimates the task diﬃculty.
B. Dataset Creation Details
In this section, we elaborate more on how we collected VCR, and about our crowdsourcing process.
B.1. Shot detection pipeline
The images in VCR are extracted from video clips from LSMDC [67] and MovieClips. These clips vary in length from a few seconds (LSMDC) to several minutes (MovieClips). Thus, to obtain more still images from these clips, we performed shot detection. Our pipeline is as follows:
• We iterate through a video clip at a speed of one frame per second.
• During each iteration, we also perform shot detection: if we detect a mean diﬀerence of 30 pixels in HSV space, then we register a shot boundary.
• After a shot boundary is found, we apply Mask-RCNN [29, 24] on the middle frame for the shot, and save the resulting image and detection information.
We used a threshold of 0.7 for Mask-RCNN, and the best detection/segmentation model available for us at the time: X-101-64x4d-FPN14, which obtains 42.4 box mAP on COCO, and 37.5 mask mAP.
B.2. Interestingness Filter
Recall that we use an ‘interestingness ﬁlter’ to ensure that the images in our dataset are high quality. First, every image had to have at least two people in it, as detected by
14Available via the Detectron Model Zoo.

person chair tie cup bottle book car
diningtable wineglass pottedplant handbag
bowl tv
cellphone horse vase couch
backpack bench
umbrella bed clock
laptop truck trafficlight sportsball bicycle remote suitcase sink dog boat bird refrigerator knife spoon motorcycle teddybear apple surfboard oven cow fork cake keyboard bus airplane baseballbat orange toilet toothbrush skateboard baseballglove microwave mouse train pizza banana sheep firehydrant sandwich stopsign
cat elephant parkingmeter
donut frisbee tennisracket scissors
kite skis hotdog zebra broccoli giraffe carrot toaster bear snowboard hairdrier

n total n referenced

100 101 102 103 104 105 106 107
Object counts n

Figure 8: Distribution of the referenced COCO [49] objects

in VCR. We count an object as being ‘referenced’ if, for a

given question, answer, and rationale, that object is men-

tioned explicitly. Note that we do not double-count objects

here - if person5 is mentioned in the question and the an-

swer, we count it once. This chart suggests that our dataset

is mostly human-centric, with some categories being refer-

enced more than others (cars are mentioned more than ties,

even though cars appear less often).

Mask RCNN. However, we also found that many images with two or more people were still not very interesting. The two main failure cases here are when there are one or two

10

ForrestGGaunmdhpi FHluigghot
JuliTeoAonthd FJauilriay InsiTdietaMniacn2
Harry Potter And The PhilosRopehaer rWs Sintdoonwe Harry PotterTAhnedATdhjeusGtmobelnettBOufrFeairue The GSrpeidaet rG-matasnb2y The Girl With The DraTghoenHTuastttoleor Mr PopperCsaPseanbglaunincas
The Curious Case OHfoBwenDjoamYionuBKunttoown 21 JumYpesStMreaent
Harry Potter And The Order Of PThitoaennici1x MoPrnuilnpgFGicltoiorny SBpoadryklOe f2L0i1e2s
Pride And PrejudiceIrDoinskMOanne2 How To Lose Friends And AlienZaotme bPieeolapnled Harry Potter And The ChambeIdreOnftiStyeTcrheietsf
Confessions Of ASpSihdoepr-amhaonli1c We BTohueghBtigAFZiosho
The TDheeviBl WouenatrysHPurnatdear The Lord Of The Rings The ReTthuernCOryfinTgheGKaminge Harry Potter And The Deathly Hallows DAiuskstTrawlioa
CinJdaecrkelAlandMJainll CBahdinaStaonwtna
Harry Potter And The Deat5h0ly0HDaallyoswOsfDSisukmOmneer The Art Of GetEtinagsyBAy Defi2a0n1ce2 This IsS4a0lt
Die Nacht DeUsgJlyaeTgreurths No ResAemrvaadtioenuss RobTihneHVooowd
TThheeLDosetsWceenedkaenntds QuantuSmnoOwf SFloolwaceer ChasinRguMbyavSepraicrkkss
Just Go WBriuthnoIt CoBloumrlebsiaqnuae
TheGGetraSdhuoarttey Harry Potter And The HaSlf-ebvloeondPPoruinncdes
SluAmmdeorgicManillBioenaauirtey ParentaGlrGanuiTdoarninceo DinnerItFsoCroSmchpmlicuactekds
2T7heDQreuseseens No StriAngnsEAdtutaccahtieodn The DIadmesneOdf UMnaitrecdh AThTehBouusttaenrfdlyWEoffredcst Nanny McpLhaerrey RCerotuwrnnes
Think LTikhee AWMatacnh Young ATdhuolrt
Day The Earth SDtoiosdtriSctti9ll The PrinScoeuslsSBurrifdeer 40 YTehaerUOgldlyVTirrugtinh
Katy PerCryhPaasirnt gOAf Mmey As GoModenAsInItBGlaectks ThelmThaeAGnudilLt uTirsiep
A Good Day TZooDoikeeHeapredr Ghost Rider Spirit Of VenSguepaenrce8
MarleyLiAfendOfMPei 30 MLineustMesisOerraLbelesss SoDrceeartehrsAtAAppFruennetircael
HoLrritibtleleFBoocskseerss The Social NeCtwleorkrks BatTtleheLoRsoAomngmealetes
HaTllhoewCeeanll Get HiCmhTaorliTehSet GCrloeuedk
FJaurgnoo Back To GThroewFnuUtupres
Land POrfoTmheethLeousst ResKenrvoocikreDdoUgps Bad TPesaycchheor
Frankie AndSaJonhctnunmy The MagnificeGntirSlseTverinp
DRraofctkDyaIyv IntolerableCCarffueeinltey From PEravdaan TAolmNigahdtay One Direction: TheBIinllysiMdeadSitsoorny
PitchSPenesrfeelcets3s The LSepgyaNllyexBtloDnodoer The School LOiaf rRLoicakr BrewTshteerG'soMdfilalitohnesr The Legend OfLDerguanllkyeBnloMnadsete2r Man On TRhoeunMdoeorns
SBelureenCdhipipitys ReadByrTinogWIt eOanr FlTirhtiengBWlueitsh BDriostahsetersr
GeRt eOdvCerlifIft TAhceceSpttinegd TheCHoeuaprletbsreRaektrKeaidt CKihcakrinlieg W& iSlscorne'asmWinagr We ArCehBicloaogdo 20T0h1e MWaitncihaecss FunnHy oPoesoieprles Animal HDouasvee The GreKatinDkeybBaoteortss The Meaning OBf rLaiftez They Came TogeFtrhidear Nutty ProfAesVseorry2B: rTahdeyKSleuqmupesl In GHoaopdpyCoGmilmpaonrey View From The To5p4 RelatiBveesSt tIrnanSgheorws EighHt iMghenRoOaudt other MovieClips
0

LSMDC MovieClips

5000

10000 15000 20000 25000 30000

Number of images

Figure 9: Distribution of movies in the VCR training set by number of images. Blue bars are movies from LSMDC (46k images); red are MovieClips (33k images). The MovieClips images are spread over a wider range of movies: due to space restrictions, most are under ‘other MovieClips.’

Number of images with n questions asked

30000

20000

10000

0

1

2

3

4

5

6

Number of questions asked n

Figure 10: Number of questions asked per image on the VCR training set. The average number of questions asked per image is 2.645. Note that while workers could ask anywhere between one to three questions per image, images that were ﬂagged as especially interesting by workers got re-annotated with additional annotations.

people detected, but they aren’t doing anything interesting (Figure 11a), or when the image is especially grainy and blurry. Thus, we opted to learn an additional classiﬁer for determining which images were interesting.
Our ﬁltering process evolved as we collected data for the task. The ﬁrst author of this paper ﬁrst manually annotated 2000 images from LSMDC [67] as being ‘interesting’ or ‘not interesting’ and trained a logistic regression model to predict said label. The model is given as input the number of people detected by Mask RCNN [29, 24], along with the number of objects (that are not people) detected. We used this model to identify interesting images in LSMDC, using a threshold that corresponded to 70% precision. This resulted in 72k images selected; these images were annotated ﬁrst.
During the crowdsourcing process, we obtained data that allowed us to build an even better interestingness ﬁlter later on. Workers were asked, along with each image, whether they thought that the image was especially interesting (and thus should go to more workers), just okay, or especially boring (and hard to ask even one good question for). We used this to train a deeper model for this task. The model uses a ResNet 50 backbone over the entire image [30] as well as a multilayer perceptron over the object counts. The entire model is trained end-to-end: 2048 dimensional features from Resnet are concatenated with a 512 dimensional projetion of the object counts, and used to predict the la-

11

a) Boring image.

b) Interesting image.
Figure 11: Two example images that come from the raw video pipeline. Image a) is ﬂagged by our initial ﬁlter as ‘boring’, because there are only two people without any additional objects, whereas image b) is ﬂagged as being interesting due to the number of people and objects detected.
bels.15 We used this model to select the most interesting 40k images from Movieclips, which ﬁnished oﬀ the annotation process.
B.3. Crowdsourcing quality data
As mentioned in the paper, crowdsourcing data at the quality and scale of VCR is challenging. We used several best practices for crowdsourcing, which we elaborate on in this section.
We used Amazon Mechanical Turk for our crowdsourcing. A screenshot of our interface is given in Figure 12. Given an image, workers asked questions, answered them, and provided a rationale explaining why their answer might be correct. These are all written in a mixture of natural language text, as well as referring to detection regions. In our annotation UI, workers refer to the regions by writing the tag number.16
Workers could ask anywhere between one to three questions per HIT. We paid the workers proportionally at $0.22 per triplet. According to workers, this resulted in $8– 25/hr. This proved necessary as workers reported feeling
15In addition to predicting interestingness, the model also predicts the number of questions a worker asks, but we never ended up using these predictions.
16Note that this diﬀers a bit from the format in the paper: we originally had workers write out the full tag, like [person5], but this is often long and the workers would sometimes forget the brackets. Thus, the tag format here is just a single number, like 5.

Figure 12: Screenshot of our annotation interface. Workers are given an image, as well as context from the video (here, captions from LSMDC [67]), and are asked to write one to three questions, answers, and rationales. For each answer, they must mark it as likely, possible, or unlikely. Workers also select whether the image was especially interesting or boring, as this allows us to train a deep model for predicting image interestingness.
“drained” by the high quality required. Automated quality checks We added several auto-
mated checks to the crowdsourcing UI to ensure high quality. The workers had to write at least four words for the question, three for the answer, and ﬁve for the rationale. Additionally, the workers had to explicitly refer to at least one detection on average per question, answer, and rationale triplet. This was automatically detected to ensure that the workers were referring to the detection tags in their submissions.
We also noticed early on was that sometimes workers would write detailed stories that were only loosely connected with the semantic content of the image. To ﬁx this, workers also had to self-report whether their answer was likely (above 75% probability), possible (25-75% probability), or unlikely (below 25% probability). We found that this helped deter workers from coming up with consistently unlikely answers for each image. The likelihood ratings were never used for the task, since we found they weren’t necessary to obtain high human agreement.
Instructions Like for any crowdsourcing task, we found wording the instructions carefully to be crucial. We encouraged workers to ask about higher-level actions, versus lower-level ones (such as ‘What is person1 wearing?’), as well as to not ask questions and answers that were overly

12

generic (and thus could apply to many images). Workers were encouraged to answer reasonably in a way that was not overly unlikely or unreasonable. To this end, we provided the workers with high-quality example questions, answers, and rationales.
Qualiﬁcation exam Since we were picky about the types of questions asked, and the format of the answers and rationales, workers had to pass a qualiﬁcation task to double check that they understood the format. The qualiﬁcation test included a mix of multiple-choice graded answers as well as a short written section, which was to provide a single question, answer, and rationale for an image. The written answer was checked manually by the ﬁrst author of this paper.
Work veriﬁcation In addition to the initial qualiﬁcation exam, we also periodically monitored the annotation quality. Every 48 hours, the ﬁrst author of this paper would review work and provide aggregate feedback to ensure that workers were asking good questions, answering them well, and structuring the rationales in the right way. Because this took signiﬁcant time, we then selected several outstanding workers and paid them to do this job for us: through a separate set of HITs, these outstanding workers were paid $0.40 to provide detailed feedback on a submission that another worker made. Roughly one in ﬁfty HITs were annotated in this way to give extra feedback. Throughout this process, workers whose submission quality dropped were dequaliﬁed from the HITs.
C. Adversarial Matching Details
There are a few more details that we found useful when performing the Adversarial Matching to create VCR, which we discuss in this section.
Aligning Detections In practice, most responses in our dataset are not relevant to most queries, due to the diversity of responses in our dataset and the range of detection tags (person1, etc.).
To ﬁx this, for each query qi (with associated object list oi and response ri) we turn each candidate r j into a template, and use a rule based system to probabilistically remap its detection tags to match the objects in oi. With some probability, a tag in r j is replaced with a tag in qi and ri. Otherwise, it is replaced with a random tag from oi.
We note that our approach isn’t perfect. The remapping system often produces responses that violate predicate/argument structure, such as ‘person1 is kissing person1.’ However, our approach does not need to be perfect: because the detections for response r j are remapped uniquely for each query qi, with some probability, there should be at least some remappings of ri that make sense, and the question relevance model Prel should select them.
Semantic categories Recall that we use 11 folds for the dataset of around 290k questions, answers, and ratio-

nales. Since we must perform Adversarial Matching once for the answers, as well as for the rationales, this would naively involve 22 matchings on a fold size of roughly 26k. We found that the major computational bottleneck wasn’t the bipartite matching17, but rather the computation of allpairs similarity and relevance between ∼26k examples.
There is one additional potential problem: we want the dataset examples to require a lot of complex commonsense reasoning, rather than simple attribute identiﬁcation. However, if the response and the query disagree in terms of gender pronouns, then many of the dataset examples can be reduced to gender identiﬁcation.
We address both of these problems by dividing each fold into ‘buckets’ of 3k examples for matching. We divide the examples up in terms of the pronouns in the response: if the response contains a female or male pronoun, then we put the example into a ‘female’ or ‘male’ bucket, respectively, otherwise the response goes into the ‘neutral’ bucket. To further divide the dataset examples, we also put diﬀerent question types in diﬀerent buckets for the question answering task (e.g. who, what, etc.). For the answer justiﬁcation task, we cluster the questions and answers using their average GloVe embeddings [56].
Relevance model details Recall that our relevance model Prel is trained to predict the probability that a response r is valid for a query q. We used BERT for this task [15], as it achieves state-of-the-art results across many two-sentence inference tasks. Each input looks like the following, where the query and response are concatenated with a separator in between:
[CLS] what is casey doing ? [SEP] casey is
getting out of car . [SEP]
Note that in the above example, object tags are replaced with the class name (car3→car). Person tags are replaced with gender neutral names (person1→casey) [19].
We ﬁne-tune BERT by treating it as a two-way classiﬁcation problem. With probability 25% for a query, BERT is given that query’s actual response, otherwise it is given a random response (where the detections were remapped). Then, the model must predict whether it was given the actual response or not. We used a learning rate of 2 · 10−5, the Adam optimizer [44], a batch size of 32, and 3 epochs of ﬁne-tuning.18
Due to computational limitations, we used BERT-Base as the architecture rather than BERT-Large - the latter is signiﬁcantly slower.19 Already, Prel has an immense computational requirement as it must compute all-pairs simi-
17We use the https://github.com/gatagat/lap implementation. 18We note that during the Adversarial Matching process, for either Question Answering or Answer Justiﬁcation, the dataset is broken up into 11 folds. For each fold, BERT is ﬁne-tuned on the other folds, not on the ﬁnal dataset splits. 19Also, BERT-Large requires much more memory, enough so that it’s harder to ﬁne-tune due to the smaller feasible batch size.

13

larity for the entire dataset, over buckets of 3000 examples. Thus, we opted to use a larger bucket size rather than a more expensive model.
Similarity model details While we want the responses to be highly relevant to the query, we also want to avoid cases where two responses might be conﬂated by humans - particularly when one is the correct response. This conﬂation might occur for several reasons: possibly, two responses are paraphrases of one another, or one response entails another. We lump both under the ‘similarity’ umbrella as mentioned in the paper and introduce a model, Psim, to predict the probability of this occurring - broadly speaking, that two responses ri and r j have the same meaning.
We used ESIM+ELMo for this task [10, 57], as it still does quite well on two-sentence natural language inference tasks (although not as well as BERT), and can be made much more eﬃcient. At test time, the model makes the similarity prediction when given two token sequences.20
We trained this model on freely available NLP corpora. We used the SNLI formalism [8], in which two sentences are an ‘entailment’ if the ﬁrst entails the second, ‘contradiction’ if the ﬁrst is contradicted by the second, and ‘neutral’ otherwise. We combined data from SNLI and MultiNLI [82] as training data. Additionally, we found that even after training on these corpora, the model would struggle with paraphrases, so we also translated SNLI sentences from English to German and back using the Nematus machine translation system [81, 73]. These sentences served as extra paraphrase data and were assigned the ‘entailment’ label. We also used randomly sampled sentence pairs from SNLI as additional ‘neutral’ training data. We held out the SNLI validation set to determine when to stop training. We used standard hyperparameters for ESIM+ELMo as given by the AllenNLP library [22].
Given the trained model Pnli, we deﬁned the similarity model as the maximum entailment probability for either way of ordering the two responses:
Psim(ri, r j) = max Pnli(ent|ri, r j), Pnli(ent|r j, ri) , (3)
where ‘ent’ refers to the ‘entailment’ label. If one response entails the other, we ﬂag them as similar, even if the reverse entailment is not true, because such a response is likely to be a false positive as a distractor.
The beneﬁt of using ESIM+ELMo for this task is that it can be made more eﬃcient for the task of all-pairs sentence similarity. While much of the ESIM architecture involves computing attention between the two text sequences, everything before the ﬁrst attention can be precomputed. This provides a large speedup, particularly as computing the ELMo representations is expensive. Now, for a fold size
20Again, with object tags replaced with the class name, and person tags replaced by gender neutral names.

accuracy

accuracy

1.0 0.8 0.6 0.4 0.2 0.0
10 2
1.0 0.8 0.6 0.4 0.2 0.0
10 3

QA

Relevance Model Worker

10 1

100

QA R

Relevance Model Worker

10 2

10 1

Figure 13: Tuning the λ hyperparameter. Workers were asked to solve 100 dataset examples from the validation set, as given by Adversarial Matching for each considered value of λ. We used these results to pick reasonable values for the hyperparameter such that the task was diﬃcult for the question relevance model Prel, while simple for human workers. We chose λ = 0.1 for Q → A and λ = 0.01 for QA → R.

of N, we only have to compute 2N ELMo representations rather than N2.
Validating the λ parameter Recall that our hyperparameter λ trades oﬀ between machine and human diﬃculty for our ﬁnal dataset. We shed more insight on how we chose the exact value for λ in Figure 13. We tried several diﬀerent values of λ and chose λ = 0.1 for Q → A and λ = 0.01 for QA → R, as at these thresholds human performance was roughly 90%. For an easier dataset for both humans and machines, we would increase the hyperparameter.

14

D. Language Priors and Annotation Artifacts Discussion
There has been much research in the last few years in understanding what ‘priors’ datasets have.21 Broadly speaking, how well do models do on VCR, as well as other visual question answering tasks, without vision?
To be more general, we will consider problems where a model is given a question and answer choices, and picks exactly one answer. The answer choices are the outputs that the model is deciding between (like the responses in VCR) and the question is the shared input that is common to all answer choices (the query, image, and detected objects in VCR). With this terminology, we can categorize unwanted dataset priors in the following ways:
• Answer Priors: A model can select a correct answer without even looking at the question. Many text-only datasets contain these priors. For instance, the RocStories dataset [53] (in which a model must classify endings to a story as correct or incorrect), a model can obtain 75% accuracy by looking at stylistic features (such as word choice and punctuation) in the endings.
• Non-Visual Priors: A model can select a correct answer using only non-visual elements of the question. One example is VQA 1.0 [5]: given a question like ‘What color is the ﬁre hydrant?’ a model will classify some answers higher than others (red). This was addressed in VQA 2.0 [26], however, some answers will still be more likely than others (VQA’s answers are open-ended, and an answer to ‘What color is the ﬁre hydrant?’ must be a color).
These priors can either arise from biases in the world (ﬁre hydrants are usually red), or, they can come from annotation artifacts [28]: patterns that arise when people write class-conditioned answers. Sometimes these biases are subliminal: when asked to write a correct or incorrect story ending, the correct endings tend to be longer [72]. Other cases are more obvious: workers often use patterns such as negation to write sentences that contradict a sentence [28].22
To what extent do vision datasets suﬀer from annotation artifacts, versus world priors? We narrow our focus to multiple-choice question answering datasets, in which for humans traditionally write correct and incorrect answers to a question (thus, potentially introducing the annotation artifacts). In Table 5 we consider several of these datasets: TVQA [46], containing video clips from TV shows, along
21This line of work is complementary to other notions of dataset bias, like understanding what phenomena datasets cover or don’t [76], particularly how that relates to how marginalized groups are represented and portrayed [71, 90, 69, 68].
22For instance, the SNLI dataset contains pairs of sentences with labels such as ‘entailed’ or ‘contradiction’ [8]. For a sentence like ‘A skateboarder is doing tricks’ workers often write ‘Nobody is doing tricks’ which is a contradiction. The result is that the word ‘nobody’ is highly predictive of a word being a contradiction.

Dataset
TVQA [46] MovieQA [75] PororoQA [43]♥ TGIFQA [39]♦

#train Chance A Q+A S+Q+A

122,039 20.0 9,848 20.0

45.0 47.4 33.8 35.4

70.6♠ 36.3♣

7,530 20.0 43.1 47.4

73,179 20.0 45.8 72.5

VCR Q→A VCR QA→R

212,923 25.0 25.0

VCRsmall Q→A VCRsmall QA → R

9,848 25.0 25.0

27.6 53.8 26.3 64.1
25.5 39.9 25.3 50.9

Table 5: Text-only results on the validation sets of vision datasets, using BERT-Base. #train shows the number of training examples. A corresponds to only seeing the answer; in Q+A the model also sees the question; in S+Q+A the model also sees subtitles from the video clip. These results suggest that many multiple choice QA datasets suﬀer from annotation artifacts, while Adversarial Matching helps produce a dataset with minimial biases; moreover, providing extra text-only information (like subtitles) greatly boosts performance. More info:

♠: State of the art.
♣: Only 45% (879/1958) of the questions in the MovieQA validation set have timestamps, which are needed to extract cliplevel subtitles, so for the other 55%, we don’t use any subtitle information.
♥: No oﬃcial train/val/test split is available, so we split the data by movie, using 20% of data for validation and the rest for training.
♦: There seem to be issues with the publicly released train-test split of TGIFQA (namely, a model with high accuracy on a held-out part of the training set doesn’t generalize to the provided test set) so we re-split the multiple-choice data ourselves by GIF and hold out 20% for validation.

with subtitles; MovieQA [75], with videos from movies and questions obtained from higher-level plot summaries; PororoQA [43], with cartoon videos; and TGIFQA [39], with templated questions from the TGIF dataset [47]. We note that these all diﬀer from our proposed VCR in terms of subject matter, questions asked, number of answers (each of the above has 5 answers possible, while we have 4) and format; our focus here is to investigate how diﬃcult these datasets are for text-only models.23 Our point of comparison is VCR, since our use of Adversarial Matching means that humans never write incorrect answers.
We tackle this problem by running BERT-Base on these models [15]: given only the answer (A), the answer and the question (Q+A), or additional language context in the form of subtitles (S+Q+A), how well does BERT do? Our results in Table 5 help support our hypothesis regarding annotation
23It should be noted that all of these datasets were released before the existence of strong text-only baselines such as BERT.

15

artifacts: whereas accuracy on VCR, only given the ending, is 27% for Q → A and 26% for Q → A, versus a 25% random baseline. Other models, where humans write the incorrect answers, have answer-only accuracies from 33.8% (MovieQA) to 45.8% (TGIFQA), over a 20% baseline.
There is also some non-visual bias for all datasets considered: from 35.4% when given the question and the answers (MovieQA) to 72.5% (TGIFQA). While these results suggest that MovieQA is incredibly diﬃcult without seeing the video clip, there are two things to consider here. First, MovieQA is roughly 20x smaller than our dataset, with 9.8k examples in training. Thus, we also tried training BERT on ‘VCRsmall’: taking 9.8k examples at random from our training set. Performance is roughly 14% worse, to the point of being roughly comparable to MovieQA.24 Second, often times the examples in MovieQA have similar structure, which might help to alleviate stylistic priors, for example:
“Who has followed Boyle to Eamon’s apartment?” Answers:
1. Thommo and his IRA squad. 2. Darren and his IRE squad. 3. Gary and his allies. 4. Quinn and his IRA squad. 5. Jimmy and his friends.
On the other hand, our dataset examples tend to be highly diverse in terms of syntax as well as high-level meaning, due to the similarity penalty. We hypothesize that this is why some language priors creep into VCR, particularly in the QA → R setting: given four very distinct rationales that ostensibly justify why an answer is true, some will likely serve as better justiﬁcations than others.
Furthermore, providing additional language information (such as subtitles) to a model tends to boost performance considerably. When given access to subtitles in TVQA,25 BERT scores 70.6%, which to the best of our knowledge is a new state-of-the-art on TVQA.
In conclusion, dataset creation is highly diﬃcult, particularly as there are many ways that unwanted bias can creep in during the dataset creation process. One such bias of this form includes annotation artifacts, which our analysis suggests is prevalent amongst multiple-choice VQA tasks wherein humans write the wrong endings. Our analysis also suggests Adversarial Matching can help minimize this effect, even when there are strong natural biases in the underlying textual data.
24Assuming an equal chance of choosing each incorrect ending, the results for BERT on an imaginary 4-answer version of TVQA and MovieQA would be 54.5% and 42.2%, respectively.
25We prepend the subtitles that are aligned to the video clip to the beginning of the question, with a special token (;) in between. We trim tokens from the subtitles when the total sequence length is above 128 tokens.

E. Model details
In this section, we discuss implementation details for our model, R2C.
BERT representations As mentioned in the paper, we used BERT to represent text [15]. We wanted to provide a fair comparison between our model and BERT, so we used BERT-Base for each. We tried to make our use of BERT to be as simple as possible, matching our use of it as a baseline. Given a query q and response choice r(i), we merge both into a single sequence to give to BERT. One example might look like the following:
[CLS] why is riley riding motorcycle while
wearing a hospital gown ? [SEP] she had to leave
the hospital in a hurry . [SEP]
Note that in the above example, we replaced person tags with gender neutral names [19] (person3→ riley) and replaced object detections by their class name (motorcycle1→ motorcycle), to minimize domain shift between BERT’s pretrained data (Wikipedia and the BookCorpus [94]) and VCR.
Each token in the sequence corresponds to a diﬀerent transformer unit in BERT. We can then use the later layers in BERT to extract contextualized representations for the each token in the query (everything from why to ?) and the response (she to .).26 Note that this gives us a diﬀerent representation for each response choice i.
We extract frozen BERT representations from the second-to-last layer of the Transformer.27 Intuitively, this makes sense as the representations that that layer are used for both of BERT’s pretraining tasks: next sentence prediction (the unit corresponding to the [CLS] token at the last layer L attends to all units at layer L − 1), as well as masked language modeling (the unit for a word at layer L looks at its hidden state at the previous layer L − 1, and uses that to attend to all other units as well). The experiments in [15] suggest that this works well, though not as well as ﬁnetuning BERT end-to-end or concatenating multiple layers of activations.28 The tradeoﬀ, however, is that precomputing BERT representations lets us substantially reduce the runtime of R2C and allows us to focus on learning more powerful vision representations.
Model Hyperparameters A more detailed discussion of the hyperparameters used for R2C is as follows. We tried
26The only slight diﬀerence is that, due to the WordPiece encoding scheme, rare words (like chortled) are broken up into subword units (cho ##rt ##led). In this case, we represent that word as the average of the BERT activations of its subwords.
27Since the domain that BERT was pretrained on (Wikipedia and the BookCorpus [94]) is still quite diﬀerent from our domain, we ﬁne-tuned BERT on the text of VCR (using the masked language modeling objective, as well as next sentence prediction) for one epoch to account for the domain shift, and then extracted the representations.
28This suggests, however, that if we also ﬁne-tuned BERT along with the rest of the model parameters, the results of R2C would be higher.

16

to stick to simple settings (and when possible, used similar conﬁgurations for the baselines, particularly with respect to learning rates and hidden state sizes).
• Our projection of image features maps a 2176 dimensional hidden size (2048 from ResNet50 and 128 dimensional class embeddings) to a 512 dimensional vector.
• Our grounding LSTM is a single-layer bidirectional LSTM with a 1280-dimensional input size (768 from BERT and 512 from image features) and uses 256 dimensional hidden states.
• Our reasoning LSTM is a two-layer bidirectional LSTM with a 1536-dimensional input size (512 from image features, and 256 for each direction in the attended, grounded query and the grounded answer). It also uses 256-dimensional hidden states.
• The representation from the reasoning LSTM, grounded answer, and attended question is maxpooled and projected to a 1024-dimensional vector. That vector is used to predict the ith logit.
• For all LSTMs, we initialized the hidden-hidden weights using orthogonal initialization [70], and applied recurrent dropout to the LSTM input with pdrop = 0.3 [21].
• The Resnet50 backbone was pretrained on Imagenet [14, 30]. The parameters in the ﬁrst three blocks of ResNet were frozen. The ﬁnal block (after the RoiAlign is applied) is ﬁne-tuned by our model. We were worried, however, that the these representations would drift and so we added an auxiliary loss to the model inspired by [48]: the 2048-dimensional representation of each object (without class embeddings) had to be predictive of that object’s label (via a linear projection to the label space and a softmax).
• Often times, there are a lot of objects in the image that are not referred to by the query or response set. We ﬁltered the objects considered by the model to include only the objects mentioned in the query and responses. We also passed in the entire image as an ‘object’ that the model could attend to in the object contextualization layer.
• We optimized R2C using Adam [44], with a learning rate of 2 · 10−4 and weight decay of 10−4. Our batch size was 96. We clipped the gradients to have a total L2 norm of at most 1.0. We lowered the learning rate by a factor of 2 when we noticed a plateau (validation accuracy not increasing for two epochs in a row). Each model was trained for 20 epochs, which took roughly 20 hours over 3 NVIDIA Titan X GPUs.

Model
R2C
Revisited BottomUp MLB MUTAN

Q→A GloVe BERT
46.4 63.8
39.4 57.5 42.8 62.3 45.5 61.8 44.4 61.0

QA → R GloVe BERT
38.3 67.2
34.0 63.5 25.1 63.0 36.1 65.4 32.0 64.4

Q → AR GloVe BERT
18.3 43.1
13.5 36.8 10.7 39.6 17.0 40.6 14.1 39.3

Table 6: VQA baselines evaluated with GloVe or BERT, evaluated on the VCR evaluation set with R2C as comparison. While BERT helps the performance of these baselines, our model still performs the best in every setting.

F. VQA baselines with BERT
We present additional results where baselines for VQA [5] are augmented with BERT embeddings in Table 6. We didn’t include these results in the main paper, because to the best of our knowledge prior work hasn’t used contextualized representations for VQA. (Contextualized representations might be overkill, particularly as VQA questions are short and often simple). From the results, we ﬁnd that while BERT also helps the baselines, our model R2C beneﬁts even more, with a 2.5% overall boost in the holistic Q → AR setting.
G. VCR Datasheet
A datasheet is a list of questions that accompany datasets that are released, in part so that people think hard about the phenomena in their data [23]. In this section, we provide a datasheet for VCR.
G.1. Motivation for Dataset Creation
Why was the dataset created? The dataset was created to study the new task of Visual Commonsense Reasoning: essentially, to have models answer challenging cognition-level questions about images and also to choose a rationale justifying each answer.
Has the dataset been used already? Yes, at the time of writing, several groups have submitted models to our leaderboard at visualcommonsense.com/leaderboard.
Who funded the dataset?? VCR was funded via a variety of sources; the biggest sponsor was the IARPA DIVA program through D17PC00343.29
G.2. Dataset Composition
What are the instances? Each instance contains an image, a sequence of object regions and classes, a query, and a list of response choices. Exactly one response is correct. There are two sub-tasks to the dataset: in Question
29However, the views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.

17

Answering (Q→A) the query is a question and the response choices are answers. In Answer Justiﬁcation (QA→R) the query is a question and the correct answer; the responses are rationales that justify why someone would conclude that the answer is true. Both the query and the rationale refer to the objects using detection tags like person1.
How many instances are there? There are 212,923 training questions, 26,534 validation questions, and 25,263 questions. Each is associated with a four answer choices, and each question+correct answer is associated with four rationale choices.
What data does each instance consist of? The image from each instance comes from a movie, while the object detector was trained to detect objects in the COCO dataset [49]. Workers ask challenging high-level questions covering a wide variety of cognition-level phenomena. Then, workers provide a rationale: one to several sentences explaining how they came at their decision. The rationale points to details in the image, as well as background knowledge about how the world works. Each instance contains one correct answer and three incorrect counterfactual answers, along with one correct rationale and three incorrect rationales.
Does the data rely on external resources? No, everything is included.
Are there recommended data splits or evaluation measures? We release the training and validation sets, as well as the test set without labels. For the test set, researchers can submit their predictions to a public leaderboard. Evaluation is fairly straightforward as our task is multiple choice, but we will also release an evaluation script.
G.3. Data Collection Process
How was the data collected? We used movie images, with objects detected using Mask RCNN [24, 29]. We collected the questions, answers, and rationales on Amazon Mechanical Turk.
Who was involved in the collection process and what were their roles? We (the authors) did several rounds of pilot studies, and collected data at scale on Amazon Mechanical Turk. In the task, workers on Amazon Mechanical Turk could ask anywhere between one to three questions. For each question, they had to provide an answer, indicate its likelihood on an ordinal scale, and provide a rationale justifying why their answer is true. Workers were paid at 22 cents per question, answer, and rationale.
Over what time frame was the data collected? August to October 2018.
Does the dataset contain all possible instances? No. Visual Commonsense Inference is very broad, and we focused on a limited set of (interesting) phenomena. Beyond looking at diﬀerent types of movies, or looking at the world

beyond still photographs, there are also diﬀerent types of inferences that we didn’t cover in our work.
If the dataset is a sample, then what is the population? The population is that of movie images that were deemed interesting by our interestingness ﬁlter (having at least three object detections, of which at least two are people).
G.4. Data Preprocessing
What preprocessing was done? The line between data preprocessing and dataset collection is blurry for VCR. After obtaining crowdsourced questions, answers, and rationales, we applied Adversarial Matching, turning raw data into a multiple choice task. We also tokenized the text spans.
Was the raw data saved in addition to the cleaned data? Yes - the raw data is the correct answers (and as such is a subset of the ‘cleaned’ data).
Does this dataset collection/preprocessing procedure achieve the initial motivation? At this point, we think so. Our dataset is challenging for existing VQA systems, but easy for humans.
G.5. Dataset Distribution
How is the dataset distributed? VCR is freely available for research use at visualcommonsense.com.
G.6. Legal and Ethical Considerations
Were workers told what the dataset would be used for and did they consent? Yes - the instructions said that workers answers would be used in a dataset. We tried to be as upfront as possible to workers. Workers also consented to have their responses used in this way through the Amazon Mechanical Turk Participation Agreement.
If it relates to people, could this dataset expose people to harm or legal action? No - the questions, answers, and responses don’t contain personal info about the crowd workers.
If it relates to people, does it unfairly advantage or disadvantage a particular social group? Unfortunately, movie data is highly biased against women and minorities [71, 69]. Our data, deriving from movies as well as from worker elicitations [68], is no diﬀerent. For these reasons, we recommend that users do not deploy models trained on VCR in the real world.
H. Additional qualitative results
In this section, we present additional qualitative results from R2C. Our use of attention mechanisms allow us to better gain insight into how the model arrives at its decisions. In particular, the model uses the answer to attend

18

over the question, and it uses the answer to attend over relevant objects in the image. Looking at the attention maps help to visualize which items in the question are important (usually, the model focuses on the second half of the question, like ‘covering his face’ in Figure 14), as well as which objects are important (usually, the objects referred to by the answer are assigned the most weight).
19

Why

is

[person1]

covering

his

face

?

[person1]

[person4]

He

is

trying

1.0

to

3%

protect

himself

from

[person1]

.

0.8

He

is

protecting

60%

himself

against

the

0.6

heat

.

[person4]

has

just

0.4

struck

3%

him

with

an

object

.

0.2

[person1]

is

afraid

that

32%

he

will

0.0

be

seen

.

Figure 14: An example from the Q → A task. Each super-row is a response choice (four in total). The ﬁrst super-column is the question: Here, ‘Why is [person1] covering his face?’ and the second super-column represents the relevant objects in the image that R2C attends to. Accordingly, each block is a heatmap of the attention between each response choice and the query, as well as each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 60% conﬁdent that the right answer is b., which is correct.

20

Why

is [person1] covering his

face

?

He

is protecting himself against the

heat

.

[person1]

He

is

wearing

0%

a

protective

vest

1.0

.

The

building

is

on

fire

71%

and

he

is

0.8

vulnerable

to

it

.

He

is

staring

at

the

waiter

0.6

who

is

slightly

out

of

frame

.

the

27%

waiter

has

0.4

a

cart

with

a

pan

of

food

that

'

s

0.2

on

fire

.

He is outside where the
0% scuann 0.0
get in his eyes .

Figure 15: An example from the QA → R task. Each super-row is a response choice (four in total). The ﬁrst super-column is the query, and the second super-column holds the relevant objects (here just a single person, as no other objects were mentioned by the query or responses). Each block is a heatmap of the attention between each response choice and the query, as well as the attention between each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 71% conﬁdent that the right rationale is b., which is correct.

21

What

is

[person13]

doing

?

[person3]

[person4]

[person13]

[person13]

seems

7%

to

1.0

be

dancing

.

[person4]

is

0.8

running

towards

the

0%

other

end

of

0.6

the

ship

.

[person3]

is

0.4

telling
5% [person13]

a

story

.

0.2 [person13]

is

trying

to

86%

go

down to 0.0

sit

.

Figure 16: An example from the Q → A task. Each super-row is a response choice (four in total). The ﬁrst super-column is the question: Here, ‘What is [person13] doing?’ and the second super-column represents the relevant objects in the image that R2C attends to. Accordingly, each block is a heatmap of the attention between each response choice and the query, as well as each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 86% conﬁdent that the right answer is d., which is correct.

22

What

is [person13] doing

? [person13] is

trying

to

go

down

to

sit

.

[person12] [person13]

There

is

an

empty

3% seat

1.0

that

[person13]

can

sit

.

[person12]

is

bending

at

0.8

the

86% waist
to

sit

in

a

chair

.

0.6 [person13]

is

standing

up

and

not

2%

sitting

down

at

the

0.4

dinner

table

.

[person13]

walks

from

an

opened

0.2

door

and

is

clearly

7%

walking

towards

some

chairs

, 0.0
presumably

where

someone

sits

.

Figure 17: An example from the QA → R task. Each super-row is a response choice (four in total). The ﬁrst super-column is the query, and the second super-column holds the relevant objects. Each block is a heatmap of the attention between each response choice and the query, as well as the attention between each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 86% conﬁdent that the right rationale is b., which is incorrect - the correct choice is a.
23

Why

is

[person2]

here

on

this

deck

?

[person2]

[person7]

[person2]

was

caught

1.0

committing

29%

a

crime

on

a

ship

0.8

.

[person2] is the
26% captaoinf 0.6
the ship
.

[person2]

is

0.4

10%

looking

for

someone

.

[person2]

is

0.2

[person7]

child

who

33%

is

also

on

0.0

the

ship

.

Figure 18: An example from the Q → A task. Each super-row is a response choice (four in total). The ﬁrst super-column is the question: Here, ‘Why is [person2] here on this deck?’ and the second super-column represents the relevant objects in the image that R2C attends to. Accordingly, each block is a heatmap of the attention between each response choice and the query, as well as each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 33% conﬁdent that the right answer is d., which is incorrect - the correct answer is correct answer is c.

24

Why is [person2] here on this deck ? [person2] is looking for someone .

[person1][person2][person3][person4][person5][person6][person7][person8][person9[]person10]

He

is

peering

over

everyone

'

s

head

appeaanrds 1.0

0%

to be

searching

for

someone

,

while

wearing

a

life

jacket

.

[person2]

is

scanning

the

0.8

area

,

which

is

filled

with

people

,

so

he

'

s

looking

for

a

person

.

there

0.6

appears

to

have

been

0%

a disaster

of

some

kind

,

people

are

cold

and

wet

,

after

0.4

disasters

most

people

try

to

find

their

friends

and

family

,

or

loved

ones

.

[person2]

0.2

would

not

be

here

normally

98%

,

and

he

is

addressing

[person10]

.

[person10]

is

walking

and

looking

around

0.0

the

0%

building he

appears

to

be

searching

for

someone

.

Figure 19: An example from the QA → R task. Each super-row is a response choice (four in total). The ﬁrst super-column is the query, and the second super-column holds the relevant objects. Each block is a heatmap of the attention between each response choice and the query, as well as the attention between each response choice and the objects. The ﬁnal prediction is given by the bar graph on the left: The model is 98% conﬁdent that the right rationale is c., which is correct.
25

References
[1] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Dont just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4971–4980, 2018. 8
[2] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961–971, 2016. 8
[3] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsupervised learning from narrated instruction videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4575–4583, 2016. 8
[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 6
[5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425– 2433, 2015. 6, 8, 9, 15, 17
[6] Hedi Ben-younes, Remi Cadene, Matthieu Cord, and Nicolas Thome. MUTAN: Multimodal Tucker Fusion for Visual Question Answering. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017. 6
[7] Or Biran and Courtenay Cotton. Explanation and justiﬁcation in machine learning: A survey. In IJCAI-17 Workshop on Explainable AI (XAI), page 8, 2017. 8
[8] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632–642, 2015. 14, 15
[9] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do explanations make vqa models more predictable to a human? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1036–1042, 2018. 8
[10] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1657–1668, 2017. 2, 4, 6, 14
[11] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, 2014. 6

[12] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining aﬀordances from images. In CVPR, 2018. 8
[13] Ernest Davis and Gary Marcus. Commonsense reasoning and commonsense knowledge in artiﬁcial intelligence. Commun. ACM, 58:92–103, 2015. 2
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. Ieee, 2009. 17
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2, 4, 5, 6, 7, 13, 15, 16
[16] Jacob Devlin, Saurabh Gupta, Ross B. Girshick, Margaret Mitchell, and C. Lawrence Zitnick. Exploring nearest neighbor approaches for image captioning. CoRR, abs/1505.04467, 2015. 3
[17] Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi, and Ali Farhadi. Who let the dogs out? modeling dog behavior from visual data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 8
[18] Panna Felsen, Pulkit Agrawal, and Jitendra Malik. What will happen next? forecasting player moves in sports videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3342–3351, 2017. 8
[19] Andrew Flowers. The Most Common Unisex Names In America: Is Yours One Of Them?, June 2015. 13, 16
[20] David F. Fouhey, Weicheng Kuo, Alexei A. Efros, and Jitendra Malik. From lifestyle vlogs to everyday interactions. In CVPR, 2018. 3
[21] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019–1027, 2016. 17
[22] Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. Allennlp: A deep semantic natural language processing platform. 2017. 14
[23] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumee´ III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018. 17
[24] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dolla´r, and Kaiming He. Detectron. https://github. com/facebookresearch/detectron, 2018. 3, 4, 10, 11, 18
[25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, volume 1, page 9, 2017. 8
[26] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 15

26

[27] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 8
[28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proc. of NAACL, 2018. 2, 4, 8, 15
[29] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross B. Girshick. Mask r-cnn. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2980–2988, 2017. 3, 4, 5, 10, 11, 18
[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 11, 17
[31] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeﬀ Donahue, Bernt Schiele, and Trevor Darrell. Generating visual explanations. In European Conference on Computer Vision, pages 3–19. Springer, 2016. 8
[32] Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding visual explanations. European Conference on Computer Vision (ECCV), 2018. 8
[33] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 8
[34] Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, Nov. 1997. 5
[35] Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stack neural module networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 53–69, 2018. 8
[36] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modeling relationships in referential expressions with compositional modular networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 4418–4427. IEEE, 2017. 8
[37] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 8
[38] Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In European conference on computer vision, pages 727–739. Springer, 2016. 6
[39] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017). Honolulu, Hawaii, pages 2680–8, 2017. 15
[40] Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. Computing, 38(4):325–340, 1987. 4

[41] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving vehicles. In 15th European Conference on Computer Vision, pages 577–593. Springer, 2018. 8
[42] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard Product for Low-rank Bilinear Pooling. In The 5th International Conference on Learning Representations, 2017. 6
[43] K Kim, C Nan, MO Heo, SH Choi, and BT Zhang. Pororoqa: Cartoon video series dataset for story understanding. In Proceedings of NIPS 2016 Workshop on Large Scale Computer Vision System, 2016. 15
[44] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 13, 17
[45] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32–73, 2017. 8
[46] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. In EMNLP, 2018. 4, 8, 9, 15
[47] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4641–4650, 2016. 15
[48] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 17
[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. 4, 8, 9, 10, 18
[50] Xiao Lin and Devi Parikh. Leveraging visual question answering for image-caption ranking. In European Conference on Computer Vision, pages 261–277. Springer, 2016. 6
[51] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron C Courville, and Christopher Joseph Pal. A dataset and exploration of models for understanding video data through ﬁllin-the-blank question-answering. In Computer Vision and Pattern Recognition (CVPR), 2017. 8
[52] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11–20, 2016. 8
[53] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016. 15
[54] Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, and Ali Farhadi. what happens if... learning to predict the eﬀect of forces in images. In European Conference on Computer Vision, pages 269–285. Springer, 2016. 8

27

[55] James Munkres. Algorithms for the assignment and transportation problems. Journal of the society for industrial and applied mathematics, 5(1):32–38, 1957. 4
[56] Jeﬀrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014. 6, 13
[57] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227–2237, 2018. 2, 4, 6, 14
[58] Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. Inferring the why in images. arXiv preprint arXiv:1406.5472, 2014. 8
[59] Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana Lazebnik. Phrase localization and visual relationship detection with comprehensive image-language cues. In Proc. ICCV, 2017. 8
[60] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641–2649, 2015. 8
[61] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis Only Baselines in Natural Language Inference. arXiv:1805.01042 [cs], May 2018. arXiv: 1805.01042. 2, 4
[62] Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. In Advances in Neural Information Processing Systems, 2018. 8
[63] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015. 5
[64] Nicholas Rhinehart and Kris M Kitani. First-person activity forecasting with online inverse reinforcement learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 3696–3705, 2017. 8
[65] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In European Conference on Computer Vision, pages 817–834. Springer, 2016. 8
[66] Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, and Bernt Schiele. Generating descriptions with grounded and co-referenced people. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017, Piscataway, NJ, USA, July 2017. IEEE. 8
[67] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie Description. International Journal of Computer Vision, 123(1):94–120, May 2017. 3, 4, 10, 11, 12

[68] Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 74–79, 2017. 15, 18
[69] Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, and Yejin Choi. Connotation frames of power and agency in modern ﬁlms. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2329–2334, 2017. 15, 18
[70] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. 17
[71] Alexandra Schoﬁeld and Leo Mehr. Gender-distinguishing features in ﬁlm dialogue. In Proceedings of the Fifth Workshop on Computational Linguistics for Literature, pages 32– 39, 2016. 15, 18
[72] Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi, and Noah A. Smith. The eﬀect of diﬀerent writing tasks on linguistic style: A case study of the ROC story cloze task. In Proc. of CoNLL, 2017. 2, 4, 15
[73] Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin JunczysDowmunt, Samuel La¨ubli, Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde. Nematus: a toolkit for neural machine translation. In Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 65–68, Valencia, Spain, April 2017. Association for Computational Linguistics. 14
[74] Krishna Kumar Singh, Kayvon Fatahalian, and Alexei A Efros. Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks. In Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on, pages 1–9. IEEE, 2016. 8
[75] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through questionanswering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631–4640, 2016. 4, 8, 9, 15
[76] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521–1528. IEEE, 2011. 3, 15
[77] Paul Vicol, Makarand Tapaswi, Lluis Castrejon, and Sanja Fidler. Moviegraphs: Towards understanding human-centric situations from videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 8
[78] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 98–106, 2016. 8
[79] Misha Wagner, Hector Basevi, Rakshith Shetty, Wenbin Li, Mateusz Malinowski, Mario Fritz, and Ales Leonardis. Answering visual what-if questions: From actions to predicted scene descriptions. In Visual Learning and Embodied Agents

28

in Simulation Environments Workshop at European Conference on Computer Vision, 2018. 8
[80] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 2017. 8
[81] John Wieting, Jonathan Mallinson, and Kevin Gimpel. Learning paraphrastic sentence embeddings from backtranslated bitext. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 274–285, 2017. 14
[82] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122. Association for Computational Linguistics, 2018. 14
[83] Qi Wu, Peng Wang, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4622–4630, 2016. 8
[84] Tian Ye, Xiaolong Wang, James Davidson, and Abhinav Gupta. Interpretable intuitive physics model. In European Conference on Computer Vision, pages 89–105. Springer, 2018. 8
[85] Yuya Yoshikawa, Jiaqing Lin, and Akikazu Takeuchi. Stair actions: A video dataset of everyday home actions. arXiv preprint arXiv:1804.04326, 2018. 8
[86] Licheng Yu, Eunbyung Park, Alexander C. Berg, and Tamara L. Berg. Visual Madlibs: Fill in the blank Image Generation and Question Answering. arXiv:1506.00278 [cs], May 2015. arXiv: 1506.00278. 8
[87] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 69–85. Springer, 2016. 8
[88] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speakerlistener-reinforcer model for referring expressions. In Computer Vision and Pattern Recognition (CVPR), volume 2, 2017. 8
[89] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. 8
[90] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias ampliﬁcation using corpus-level constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2979–2989, 2017. 15
[91] Luowei Zhou, Chenliang Xu, and Jason J. Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. 8
[92] Yipin Zhou and Tamara L Berg. Temporal perception and prediction in ego-centric video. In Proceedings of the IEEE

International Conference on Computer Vision, pages 4498– 4506, 2015. 8 [93] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7W: Grounded Question Answering in Images. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 8, 9 [94] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In arXiv preprint arXiv:1506.06724, 2015. 16

29

