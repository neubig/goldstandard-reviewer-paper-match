Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration
Ritesh Noothigattu§, Djallel Bouneffouf†, Nicholas Mattei†, Rachita Chandra†, Piyush Madan†, Kush Varshney†, Murray Campbell†, Moninder Singh†, Francesca Rossi†∗

†IBM Research
IBM T.J. Watson Research Center Yorktown Heights, NY, USA
{rachitac, krvarshn, mcam, moninder}@us.ibm.com {djallel.bouneffouf, n.mattei, piyush.madan1, francesca.rossi2}@ibm.com

§ Carnegie Mellon University
Machine Learning Department Pittsburgh, PA, USA riteshn@cmu.edu

arXiv:1809.08343v1 [cs.LG] 21 Sep 2018

Abstract
Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspeciﬁed constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.
1 Introduction
Concerns about the ways in which cyber-physical and/or autonomous decision making systems behave when deployed in the real world are growing: what various stakeholder are worried about is that the systems achieves its goal in ways that are not considered acceptable according to values and norms of the impacted community, also called “speciﬁcation gaming” behaviors. Thus, there is a growing need to
∗On leave from the University of Padova.

understand how to constrain the actions of an AI system by providing boundaries within which the system must operate.
To tackle this problem, we may take inspiration from humans, who often constrain the decisions and actions they take according to a number of exogenous priorities, be they moral, ethical, religious, or business values (Sen 1974), and we may want the systems we build to be restricted in their actions by similar principles (Arnold et al. 2017). The overriding concern is that the autonomous agents we construct may not obey these values on their way to maximizing some objective function (Simonite 2018).
The idea of teaching machines right from wrong has become an important research topic in both AI (Yu et al. 2018) and farther aﬁeld (Wallach and Allen 2008). Much of the research at the intersection of artiﬁcial intelligence and ethics falls under the heading of machine ethics, i.e., adding ethics and/or constraints to a particular system’s decision making process (Anderson and Anderson 2011). One popular technique to handle these issues is called value alignment, i.e., the idea that an agent can only pursue goals that follow values that are aligned to the human values and thus beneﬁcial to humans (Russell, Dewey, and Tegmark 2015).
Another important notion for these autonomous decision making systems is the idea of transparency or interpretability, i.e., being able to see why the system made the choices it did. Theodorou, Wortham, and Bryson (2016) observe that the Engineering and Physical Science Research Council (EPSRC) Principles of Robotics dictates the implementation of transparency in robotic systems. The authors go on to deﬁne transparency in a robotic or autonomous decision making system as, “... a mechanism to expose the decision making of the robot”.
This still leaves open the question of how to provide the behavioral constraints to the agent. A popular technique is called the bottom-up approach, i.e., teaching a machine what is right and wrong by example (Allen, Smit, and Wallach 2005). In this paper, we adopt this approach as we consider the case where only examples of the correct behavior are available to the agent, and it must therefore learn from only

these examples. We propose a framework which enables an agent to learn
two policies: (1) πR which is a reward maximizing policy obtained through direct interaction with the world and (2) πC which is obtained via inverse reinforcement learning over demonstrations by humans or other agents of how to obey a set of behavioral constraints in the domain. Our agent then uses a contextual-bandit-based orchestrator to learn to blend the policies in a way that maximizes a convex combination of the rewards and constraints. Within the RL community this can be seen as a particular type of apprenticeship learning (Abbeel and Ng 2004) where the agent is learning how to be safe, rather than only maximizing reward (Leike et al. 2017).
One may argue that we should employ πC for all decisions as it will be more “safe” than employing πR. Indeed, although one could only use πC for the agent, there are a number of reasons to employ the orchestrator. First, the humans or other demonstrators, may be good at demonstrating what not to do in a domain but may not provide examples of how best to maximize reward. Second, the demonstrators may not be as creative as the agent when mixing the two policies (Ventura and Gates 2018). By allowing the orchestrator to learn when to apply which policy, the agent may be able to devise better ways to blend the policies, leading to behavior which both follows the constraints and achieves higher reward than any of the human demonstrations. Third, we may not want to obtain demonstrations of what to do in all parts of the domain e.g., there may be dangerous or hard-to-model regions, or there may be mundane parts of the domain in which human demonstrations are too costly to obtain. In this case, having the agent learn through RL what to do in the non-demonstrated parts is of value. Finally, as we have argued, interpretability is an important feature of our system. Although the policies themselves may not be directly interpretable (though there is recent work in this area (Verma et al. 2018; Liu et al. 2018)), our system does capture the notion of transparency and interpretability as we can see which policy is being applied in real time.
Contributions. We propose and test a novel approach to teach machines to act in ways that achieve and compromise multiple objectives in a given environment. One objective is the desired goal and the other one is a set of behavioral constraints, learnt from examples. Our technique uses aspects of both traditional reinforcement learning and inverse reinforcement learning to identify policies that both maximize rewards and follow particular constraints within an environment. Our agent then blends these policies in novel and interpretable ways using an orchestrator based on the contextual bandits framework. We demonstrate the effectiveness of these techniques on the Pac-Man domain where the agent is able to learn both a reward maximizing and a constrained policy, and select between these policies in a transparent way based on context, to employ a policy that achieves high reward and obeys the demonstrated constraints.

2 Related Work
Ensuring that our autonomous systems act in line with our values while achieving their objectives is a major research topic in AI. These topics have gained popularity among a broad community including philosophers (Wallach and Allen 2008) and non-proﬁts (Russell, Dewey, and Tegmark 2015). Yu et al. (2018) provide an overview of much of the recent research at major AI conferences on ethics in artiﬁcial intelligence.
Agents may need to balance objectives and feedback from multiple sources when making decisions. One prominent example is the case of autonomous cars. There is extensive research from multidisciplinary groups into the questions of when autonomous cars should make lethal decisions (Bonnefon, Shariff, and Rahwan 2016), how to aggregate societal preferences to make these decisions (Noothigattu et al. 2017), and how to measure distances between these notions (Loreggia et al. 2018a; Loreggia et al. 2018b). In a recommender systems setting, a parent or guardian may want the agent to not recommend certain types of movies to children, even if this recommendation could lead to a high reward (Balakrishnan et al. 2018a; Balakrishnan et al. 2018b). Recently, as a compliment to their concrete problems in AI saftey which includes reward hacking and unintended side effects (Amodei et al. 2016), a DeepMind study has compiled a list of speciﬁcation gaming examples, where very different agents game the given speciﬁcation by behaving in unexpected (and undesired) ways.1
Within the ﬁeld of reinforcement learning there has been speciﬁc work on ethical and interpretable RL. Wu and Lin (2017) detail a system that is able to augment an existing RL system to behave ethically. In their framework, the assumption is that, given a set of examples, most of the examples follow ethical guidelines. The system updates the overall policy to obey the ethical guidelines learned from demonstrations using IRL. However, in this system only one policy is maintained so it has no transparency. Laroche and Feraud (2017) introduce a system that is capable of selecting among a set of RL policies depending on context. They demonstrate an orchestrator that, given a set of policies for a particular domain, is able to assign a policy to control the next episode. However, this approach use the classical multiarmed bandit, so the state context is not considered on the choice of the policy.
Interpretable RL has received signiﬁcant attention in recent years. Luss and Petrik (2016) introduce action constraints over states to enhance the interpretability of policies. Verma et al. (2018) present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and veriﬁable agent policies. PIRL represents policies using a high-level, domain-speciﬁc programming language. Such programmatic policies have the beneﬁt of being more easily interpreted than neural networks, and being amenable to veriﬁcation by symbolic methods. Additionally, Liu et
138 AI “speciﬁcation gaming” examples are available at: https:
//docs.google.com/spreadsheets/d/e/2PACX- 1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-
32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml

al. (2018) introduce Linear Model U-trees to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than ﬁve baseline methods. The transparent tree structure of an LMUT facilitates understanding the learned knowledge by analyzing feature inﬂuence, extracting rules, and highlighting the super-pixels in image inputs.

3 Background

3.1 Reinforcement Learning

Reinforcement learning deﬁnes a class of algorithms solving problems modeled as a Markov decision process (MDP) (Sutton and Barto 1998).
A Markov decision problem is usually denoted by the tuple (S, A, T , R, γ), where

• S is a set of possible states

• A is a set of actions

• T is a transition function deﬁned by T (s, a, s ) = Pr(s |s, a), where s, s ∈ S and a ∈ A

• R : S × A × S → R is a reward function

• γ is a discount factor that speciﬁes how much long term reward is kept.

The goal in an MDP is to maximize the discounted long

term reward received. Usually the inﬁnite-horizon objective

is considered:

∞

max γtR(st, at, st+1).

(1)

t=0

Solutions come in the form of policies π : S → A, which specify what action the agent should take in any given state deterministically or stochastically. One way to solve this problem is through Q-learning with function approximation (Bertsekas and Tsitsiklis 1996). The Q-value of a state-action pair, Q(s, a), is the expected future discounted reward for taking action a ∈ A in state s ∈ S. A common method to handle very large state spaces is to approximate the Q function as a linear function of some features. Let ψ(s, a) denote relevant features of the state-action pair s, a . Then, we assume Q(s, a) = θ · ψ(s, a), where θ is an unknown vector to be learned by interacting with the environment. Every time the reinforcement learning agent takes action a from state s, obtains immediate reward r and reaches new state s , the parameter θ is updated using

difference = r + γ max Q(s , a ) − Q(s, a)

a

(2)

θi ← θi + α · difference · ψi(s, a),

where α is the learning rate. -greedy is a common strategy used for exploration. That is, during the training phase, a random action is played with a probability of and the action with maximum Q-value is played otherwise. The agent follows this strategy and updates the parameter θ according to Equation (2) until the Q-value converge or for a large number of time-steps.

3.2 Inverse Reinforcement Learning
IRL seeks to ﬁnd the most likely reward function RE, which an expert E is executing (Abbeel and Ng 2004; Ng and Russell 2000). The IRL methods assume the presence of an expert that solves an MDP, where the MDP is fully known and observable by the learner except for the reward function. Since the state and action of the expert is fully observable by the learner, it has access to trajectories executed by the expert. A trajectory consists of a sequence of state and action pairs, T r = (s0, a0, s1, a1, . . . , sL−1, aL−1, sL), where st is the state of the environment at time t, at is the action played by the expert at the corresponding time and L is the length of this trajectory. The learner is given access to m such trajectories {T r(1), T r(2), . . . , T r(m)} to learn the reward function. Since the space of all possible reward functions is extremely large, it is common to represent the reward function as a linear combination of > 0 features. Rw(s, a, s ) = i=1 wiφi(s, a, s ), where wi are weights to be learned, and φi(s, a, s ) → R is a feature function that maps a state-action-state tuple to a real value, denoting the value of a speciﬁc feature of this tuple (Abbeel and Ng 2004). Current state-of-the-art IRL algorithms utilize feature expectations as a way of evaluating the quality of the learned reward function (Abbeel and Ng 2004). For a policy π, the feature expectations starting from state so is deﬁned as

∞

µ(π) = E

γtφ(st, at, st+1) π ,

t=0

where the expectation is taken with respect to the state se-
quence s1, s2, . . . achieved on taking actions according to π starting from s0. One can compute an empirical estimate of the feature expectations of the expert’s policy with the help of the trajectories {T r(1), T r(2), . . . , T r(m)}, using

1 m L−1 t (i) (i) (i)

µˆE = m

γ φ st , at , st+1 .

(3)

i=1 t=0

Given a weight vector w, one can compute the optimal pol-
icy πw for the corresponding reward function Rw, and estimate its feature expectations µˆ(πw) in a way similar to (3). IRL compares this µˆ(πw) with expert’s feature expectations µˆE to learn best ﬁtting weight vectors w. Instead of a single weight vector, the IRL algorithm by Abbeel and Ng (2004) learns a set of possible weight vectors, and they ask the agent designer to pick the most appropriate weight vector among these by inspecting their corresponding policies.

3.3 Contextual Bandits
Following Langford and Zhang (2008), the contextual bandit problem is deﬁned as follows. At each time t ∈ {0, 1, . . . , (T − 1)}, the player is presented with a context vector c(t) ∈ Rd and must choose an arm k ∈ [K] = {1, 2, . . . , K}. Let r = (r1(t), . . . , rK (t)) denote a reward vector, where rk(t) is the reward at time t associated with the arm k ∈ [K]. We assume that the expected reward is a linear function of the context, i.e. E[rk(t)|c(t)] = µTk c(t), where µk is an unknown weight vector (to be learned from the data) associated with the arm k.

The purpose of a contextual bandit algorithm A is to min-

imize the cumulative regret. Let H : C → [K] where

C is the set of possible contexts and c(t) is the context

at time t, ht ∈ H a hypothesis computed by the algorithm A at time t and h∗t = argmax rht(c(t))(t) the opti-
ht ∈H
mal hypothesis at the same round. The cumulative regret is: R(T ) = Tt=1 rh∗t (c(t))(t) − rht(c(t))(t).
One widely used way to solve the contextual bandit prob-

lem is the Contextual Thompson Sampling algorithm (CTS)

(Agrawal and Goyal 2013) given as Algorithm 1. In CTS,

the reward rk(t) for choosing arm k at time t follows a parametric likelihood function P r(r(t)|µ˜). Following Agrawal

and Goyal (2013), the posterior distribution at time t + 1,

P r(µ˜|r(t)) ∝ P r(r(t)|µ˜)P r(µ˜) is given by a multivariate Gaussian distribution N (µˆk(t + 1), v2Bk(t + 1)−1), where Bk(t) = Id + tτ−=11 c(τ )c(τ ) , d is the size of

the context vectors c, v = R 2z4 d · ln( γ1 ) and we have

R > 0, z ∈ [0, 1], γ ∈ [0, 1] constants, and µˆ(t) =

Bk (t)−1 (

t−1 τ =1

c(τ

)rk

(τ

)).

Algorithm 1 Contextual Thompson Sampling Algorithm
1: Initialize: Bk = Id, µˆk = 0d, fk = 0d for k ∈ [K]. 2: Foreach t = 0, 1, 2, . . . , (T − 1) do 3: Sample µ˜k(t) from N (µˆk, v2Bk−1). 4: Play arm kt = argmax c(t) µ˜k(t)
k∈[K ]
5: Observe rkt (t) 6: Bkt = Bkt + c(t)c(t)T , fkt = fkt + c(t)rkt (t),
µˆkt = Bk−t1fkt 7: End

Every step t consists of generating a d-dimensional sample µ˜k(t) from N (µˆk(t), v2Bk(t)−1) for each arm. We then decide which arm k to pull by solving for argmaxk∈[K] c(t) µ˜k(t). This means that at each time step we are selecting the arm that we expect to maximize the observed reward given a sample of our current beliefs over the distribution of rewards, c(t) µ˜k(t). We then observe the actual reward of pulling arm k, rk(t) and update our beliefs.
3.4 Problem Setting
In our setting, the agent is in multi-objective Markov decision processes (MOMDPs), instead of the usual scalar
reward function R(s, a, s ), a reward vector R(s, a, s ) is
present. The vector R(s, a, s ) consists of l dimensions or components representing the different objectives, i.e.,
R(s, a, s ) = (R1(s, a, s ), . . . , Rl(s, a, s )). However, not all components of the reward vector are observed in our setting. There is an objective v ∈ [l] that is hidden, and the agent is only allowed to observe expert demonstrations to learn this objective. These demonstrations are given in the form of trajectories {T r(1), T r(2), . . . , T r(m)}. To summarize, for some objectives, the agent has rewards observed from interaction with the environment, and for some objectives the agent has only expert demonstrations. The aim

Figure 1: Layout of Pac-Man

is still the same as single objective reinforcement learning,

which is trying to maximize

∞ t=0

γ

t

Ri

(st

,

at

,

st+1

)

for

each i ∈ [l].

4 Approach
4.1 Domain
We demonstrate the applicability of our approach using the classic game of Pac-Man. The layout of Pac-Man we use for this is given in Figure 1, and the following are the rules used for the environment (adopted from Berkeley AI PacMan2). The goal of the agent (which controls Pac-Man’s motion) is to eat all the dots in the maze, known as Pac-Dots, as soon as possible while simultaneously avoiding collision with ghosts. On eating a Pac-Dot, the agent obtains a reward of +10. And on successfully winning the game (which happens on eating all the Pac-Dots), the agent obtains a reward of +500. In the meantime, the ghosts in the game roam the maze trying to kill Pac-Man. On collision with a ghost, PacMan loses the game and gets a reward of −500. The game also has two special dots called capsules or Power Pellets in the corners of the maze, which on consumption, give PacMan the temporary ability of “eating” ghosts. During this phase, the ghosts are in a “scared” state for 40 frames and move at half their speed. On eating a ghost, the agent gets a reward of +200, the ghost returns to the center box and returns to its normal “unscared” state. Finally, there is a constant time-penalty of −1 for every step taken.
For the sake of demonstration of our approach, we deﬁne not eating ghosts as the desirable constraint in the game of Pac-Man. However, please recall that this constraint is not given explicitly to the agent, but only through examples. To play optimally in the original game one should eat ghosts to earn bonus points, but doing so is being demonstrated as undesirable. Hence, the agent has to combine the goal of collecting the most points while not eating ghosts if possible.

4.2 Overall Approach The overall approach we follow is depicted by Figure 2. It has three main components. The ﬁrst is the inverse reinforcement learning component to learn the desirable constraints
2 http://ai.berkeley.edu/project overview.html

(depicted in green in Figure 2). We apply inverse reinforcement learning to the demonstrations depicting desirable behavior, to learn the underlying constraint rewards being optimized by the demonstrations. We then apply reinforcement learning on these learned rewards to learn a strongly constraint satisfying policy πC .
Next, we augment this with a pure reinforcement learning component (depicted in red in Figure 2). For this, we directly apply reinforcement learning to the original environment rewards (like Pac-Man’s unmodiﬁed game) to learn a domain reward maximizing policy πR. Just to recall, the reason we have this second component is that the inverse reinforcement learning component may not be able to pick up the original environment rewards very well since the demonstrations were intended mainly to depict desirable behavior. Further, since these demonstrations are given by humans, they are prone to error, amplifying this issue. Hence, the constraint obeying policy πC is likely to exhibit strong constraint satisfying behavior, but may not be optimal in terms of maximizing environment rewards. Augmenting with the reward maximizing policy πR will help the system in this regard.
So now, we have two policies, the constraint-obeying policy πC and the reward-maximizing policy πR. To combine these two, we use the third component, the orchestrator (depicted in blue in Figure 2). This is a contextual bandit algorithm that orchestrates the two policies, picking one of them to play at each point of time. The context is the state of the environment (state of the Pac-Man game); the bandit decides which arm (policy) to play at the corresponding point of time.
4.3 Alternative Approaches
Observe that in our approach, we combine or “aggregate” the two objectives (environment rewards and desired constraints) at the policy stage. Alternative approaches to doing this are combining the two objectives at the reward stage or the demonstrations stage itself:
• Aggregation at reward phase. As before, we can perform inverse reinforcement learning to learn the underlying rewards capturing the desired constraints. Now, instead of learning a policy for each of the two reward functions (environment rewards and constraint rewards) followed by aggregating them, we could just combine the reward functions themselves. And then, we could learn a policy on this “aggregated” rewards to perform well on both the objectives, environment reward and favorable constraints. (This captures the intuitive idea of “incorporating the constraints into the environment rewards” if we were explicitly given the penalty of violating constraints).
• Aggregation at data phase. Moving another step backward, we could aggregate the two objectives of play at the data phase. This could be performed as follows. We perform pure reinforcement learning as in the original approach given in Figure 2 (depicted in red). Once we have our reward maximizing policy πR, we use it to generate numerous reward-maximizing demonstrations. Then, we combine these environment reward trajectories with the original constrained demonstrations, aggregating the two

objectives in the process. And once we have the combined data, we can perform inverse reinforcement learning to learn the appropriate rewards, followed by reinforcement learning to learn the corresponding policy.
Aggregating at the policy phase is where we go all the way to the end of the pipeline learning a policy for each of the objectives, followed by aggregating them. This is the approach we follow as mentioned in Section 4.2. Note that, we have a parameter λ (as described in more detail in Section 5.3) that trades off environmental rewards and rewards capturing constraints. A similar parameter can be used by the reward aggregation and data aggregation approaches, to decide how to weigh the two objectives while performing the corresponding aggregation.
The question now is, “which of these aggregation procedures is the most useful?”. The reason we use aggregation at the policy stage is to gain interpretability. Using an orchestrator to pick a policy at each point of time helps us identify which policy is being played at each point of time and also the reason for which it is being chosen (in the case of an interpretable orchestrator, which it is in our case). More details on this are mentioned in Section 6.
5 Concretizing Our Approach
Here we describe the exact algorithms we use for each of the components of our approach.
5.1 Details of the Pure RL
For the reinforcement learning component, we use Qlearning with linear function approximation as described in Section 3.1. For Pac-Man, some of the features we use for an s, a pair (for the ψ(s, a) function) are: “whether food will be eaten”, “distance of the next closest food”, “whether a scared (unscared) ghost collision is possible” and “distance of the closest scared (unscared) ghost”.
For the layout of Pac-Man we use (shown in Figure 1), an upper bound on the maximum score achievable in the game is 2170. This is because there are 97 Pac-Dots, each ghost can be eaten at most twice (because of two capsules in the layout), Pac-Man can win the game only once and it would require more than 100 steps in the environment. On playing a total of 100 games, our reinforcement learning algorithm (the reward maximizing policy πR) achieves an average game score of 1675.86, and the maximum score achieved is 2144. We mention this here, so that the results in Section 6 can be seen in appropriate light.
5.2 Details of the IRL
For inverse reinforcement learning, we use the linear IRL algorithm as described in Section 3.2. For Pac-Man, observe that the original reward function R(s, a, s ) depends only on the following factors: “number of Pac-Dots eating in this step (s, a, s )”, “whether Pac-Man has won in this step”, “number of ghosts eaten in this step” and “whether Pac-Man has lost in this step”. For our IRL algorithm, we use exactly these as the features φ(s, a, s ). As a sanity check, when IRL is run on environment reward optimal trajectories (generated from our policy πR), we recover something very similar to

IRL for Constraints
Constrained Demonstration

Rewards Capturing Constraints RC

Constrained Policy

RL for Game Rewards
Environment Rewards R

Reward Maximizing Policy

Orchestrator πC
πR

a(t)
Environment r(t)
s(t + 1)

Figure 2: Overview of our system. At each time step the Orchestrator selects between two policies, πC and πR depending on the observations from the Environment. The two policies are learned before engaging with the environment. πC is obtained using IRL on the demonstrations to learn a reward function that captures the particular constraints demonstrated. The second, πR is obtained by the agent through RL on the environment directly.

the original reward function R. In particular, the weights of the reward features learned is given by
1 [+2.44, +138.80, +282.49, −949.17],
1000
which when scaled is almost equivalent to the true weights [+10, +500, +200, −500] in terms of their optimal policies. The number of trajectories used for this is 100.
Ideally, we would prefer to have the constrained demonstrations given to us by humans. But for our domain of PacMan, we generate them synthetically as follows. We learn a policy πC by training it on the game with the original reward function R augmented with a very high negative reward (−1000) for eating ghosts. This causes πC to play well in the game while avoiding eating ghosts as much as possible.3 Now, to emulate erroneous human behavior, we use πC with an error probability of 3%. That is, at every time step, with 3% probability we pick a completely random action, and otherwise follow πC. This gives us our constrained demonstrations, on which we perform inverse reinforcement learning to learn the rewards capturing the constraints. The weights of the reward function learned is given by
1 [+2.84, +55.07, −970.59, −234.34],
1000
and it is evident that it has learned that eating ghosts strongly violates the favorable constraints. The number of demonstrations used for this is 100. We scale these weights to have a similar L1 norm as the original reward weights [+10, +500, +200, −500], and denote the corresponding re-
ward function by RC . Finally, running reinforcement learning on these rewards
RC , gives us our constraint policy πC . On playing a total of 100 games, πC achieves an average game score of 1268.52
3We do this only for generating demonstrations. In real domains, we would not have access to the exact constraints that we want to be satisﬁed, and hence a policy like πC cannot be learned; learning from human demonstrations would then be essential.

and eats just 0.03 ghosts on an average. Note that, when eating ghosts is prohibited in the domain, an upper bound on the maximum score achievable is 1370.
5.3 Orchestration with Contextual Bandits
We use contextual bandits to pick one of the policies (πR and πC) to play at each point of time. These two policies act as the two arms of the bandit, and we use a modiﬁed CTS algorithm to train the bandit. The context of the bandit is given by features of the current state (for which we want to decide which policy to choose), i.e., c(t) = Υ(st) ∈ Rd. For the game of Pac-Man, the features of the state we use for context c(t) are: (i) A constant 1 to represent the bias term, and (ii) The distance of Pac-Man from the closest scared ghost in st. One could use a more sophistical context with many more features, but we use this restricted context to demonstrate a very interesting behavior (shown in Section 6).
The exact algorithm used to train the orchestrator is given in Algorithm 2. Apart from the fact that arms are policies (instead of atomic actions), the main difference from the CTS algorithm is the way rewards are fed into the bandit. For simplicity, we call the constraint policy πC as arm 0 and the reward policy πR as arm 1. We now go over Algorithm 2. First, all the parameters are initialized as in the CTS algorithm (Line 1). For each time-step in the training phase (Line 3), we do the following. Pick an arm kt according to the Thompson Sampling algorithm and the context Υ(st) (Lines 4 and 5). Play the action according to the chosen policy πkt (Line 6). This takes us to the next state st+1. We also observe two rewards (Line 7): (i) the original reward in environment, raRt (t) = R(st, at, st+1) and (ii) the constraint rewards according to the rewards learnt by inverse reinforcement learning, i.e., raCt (t) = RC (st, at, st+1). raCt (t) can intuitively be seen as the predicted reward (or penalty) for any constraint satisfaction (or violation) in this step.
To train the contextual bandit to choose arms that perform well on both metrics (environment rewards and constraints), we feed it a reward that is a linear combination of raRt (t) and

Algorithm 2 Orchestrator Based Algorithm
1: Initialize: Bk = Id, µˆk = 0d, fk = 0d for k ∈ {0, 1}. 2: Observe start state s0. 3: Foreach t = 0, 1, 2, ..., (T − 1) do 4: Sample µ˜k(t) from N (µˆk, v2Bk−1). 5: Pick arm kt = argmax Υ(st) µ˜k(t).
k∈{0,1}
6: Play corresponding action at = πkt (st). 7: Observe rewards raCt (t) and raRt (t), and the next state
st+1. 8: Deﬁne rkt (t) = λ raCt (t) + γV C (st+1)
+(1 − λ) raRt (t) + γV R(st+1) 9: Update Bkt = Bkt + Υ(st)Υ(st) , fkt = fkt +
Υ(st)rkt (t), µˆkt = Bk−t1fkt 10: End
raCt (t) (Line 8). Another important point to note is that raRt (t) and raCt (t) are immediate rewards achieved on taking action at from st, they do not capture long term effects of this action. In particular, it is important to also look at the “value” of the next state st+1 reached, since we are in the sequential decision making setting. Precisely for this reason, we also incorporate the value-function of the next state st+1 according to both the reward maximizing component and constraint component (which encapsulate the long-term rewards and constraint satisfaction possible from st+1). This gives exactly Line 8, where V C is the value-function according the constraint policy πC , and V R is the value-function according to the reward maximizing policy πR. In this equation, λ is a hyperparameter chosen by a user to decide how much to trade off environment rewards for constraint satisfaction. For example, when λ is set to 0, the orchestrator would always play the reward policy πR, while for λ = 1, the orchestrator would always play the constraint policy πC. For any value of λ in-between, the orchestrator is expected to pick policies at each point of time that would perform well on both metrics (weighed according to λ). Finally, for the desired reward rkt (t) and the context Υ(st), the parameters of the bandit are updated according to the CTS algorithm (Line 9).
6 Evaluation and Test
We test our approach on the Pac-Man domain given in Figure 1, and measure its performance on two metrics, (i) the total score achieved in the game (the environment rewards) and (ii) the number of ghosts eaten (the constraint violation). We also vary λ, and observe how these metrics are traded off against each other. For each value of λ, the orchestrator is trained for 100 games. The results are shown in Figure 3. Each point in the graph is averaged over 100 test games.
The graph shows a very interesting pattern. When λ is at most than 0.215, the agent eats a lot of ghosts, but when it is above 0.22, it eats almost no ghosts. In other words, there is a value λo ∈ [0.215, 0.22] which behaves as a tipping point, across which there is drastic change in behavior. Beyond the threshold, the agent learns that eating ghosts is not worth the score it is getting and so it avoids eating as much as possible.

Figure 3: Both performance metrics as λ is varied. The red curve depicts the average game score achieved, and the blue curve depicts the average number of ghosts eaten.
On the other hand, when λ is smaller than this threshold, it learns the reverse and eats as many ghosts as possible.
Policy-switching. As mentioned before, one of the most important property of our approach is interpretability, we know exactly which policy is being played at each point of time. For moderate values of λ, the orchestrator learns a very interesting policy-switching technique: whenever at least one of the ghosts in the domain is scared, it plays πC, but if no ghosts are scared, it plays πR. In other words, it starts off the game by playing πR until a capsule is eaten. As soon as the ﬁrst capsule is eaten, it switches to πC and plays it till the scared timer runs off. Then it switches back to πR until another capsule is eaten, and so on.4 It has learned a very intuitive behavior: when there is no scared ghost in the domain, there is no possibility of violating constraints, and hence the agent is as greedy as possible (i.e., play πR), but when there are scared ghosts, better to be safe (i.e., play πC).
7 Discussion
In this paper, we have considered the problem of autonomous agents learning policies that are constrained by implicitly-speciﬁed norms and values while still optimizing their policies with respect to environmental rewards. We have taken an approach that combines IRL to determine constraint-satisfying policies from demonstrations, RL to determine reward-maximizing policies, and a contextual bandit to orchestrate between these policies in a transparent way. This proposed architecture and approach for the problem is novel. It also requires a novel technical contribution in the contextual bandit algorithm because the arms are policies rather than atomic actions, thereby requiring rewards to account for sequential decision making. We have demonstrated the algorithm on the Pac-Man video game and found it to perform interesting switching behavior among policies.
We feel that the contribution herein is only the starting point for research in this direction. We have identiﬁed sev-
4A video of our agent demonstrating this behavior is uploaded in the Supplementary Material. The agent playing the game in this video was trained with λ = 0.4.

eral avenues for future research, especially with regards to IRL. We can pursue deep IRL to learn constraints without hand-crafted features, develop an IRL that is robust to noise in the demonstrations, and research IRL algorithms to learn from just one or two demonstrations (perhaps in concert with knowledge and reasoning). In real-world settings, demonstrations will likely be given by different users with different versions of abiding behavior; we would like to exploit the partition of the set of traces by user to improve the policy or policies learned via IRL. Additionally, the current orchestrator selects a single policy at each time, but more sophisticated policy aggregation techniques for combining or mixing policies is possible. Lastly, it would be interesting to investigate whether the policy aggregation rule (λ in the current proposal) can be learned from demonstrations.
Acknowledgments We would like to thank Gerald Tesauro and Aleksandra Mojsilovic for their helpful feedback and comments on this project.
References
[2004] Abbeel, P., and Ng, A. Y. 2004. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning (ICML).
[2013] Agrawal, S., and Goyal, N. 2013. Thompson sampling for contextual bandits with linear payoffs. In ICML (3), 127–135.
[2005] Allen, C.; Smit, I.; and Wallach, W. 2005. Artiﬁcial morality: Top-down, bottom-up, and hybrid approaches. Ethics and Information Technology 7(3):149–155.
[2016] Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schulman, J.; and Mane´, D. 2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.
[2011] Anderson, M., and Anderson, S. L. 2011. Machine Ethics. Cambridge University Press.
[2017] Arnold, T.; Thomas; Kasenberg, D.; and Scheutzs, M. 2017. Value alignment or misalignment - what will keep systems accountable? In AI, Ethics, and Society, Papers from the 2017 AAAI Workshop.
[2018a] Balakrishnan, A.; Bouneffouf, D.; Mattei, N.; and Rossi, F. 2018a. Using contextual bandits with behavioral constraints for constrained online movie recommendation. In Proc. IJCAI.
[2018b] Balakrishnan, A.; Bouneffouf, D.; Mattei, N.; and Rossi, F. 2018b. Incorporating behavioral constraints in online AI systems. arXiv preprint arXiv:1809.05720.
[1996] Bertsekas, D., and Tsitsiklis, J. 1996. Neuro-dynamic programming. Athena Scientiﬁc.
[2016] Bonnefon, J.-F.; Shariff, A.; and Rahwan, I. 2016. The social dilemma of autonomous vehicles. Science 352(6293):1573–1576.
[2008] Langford, J., and Zhang, T. 2008. The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits. In Proc. 21st NIPS.

[2017] Laroche, R., and Feraud, R. 2017. Reinforcement learning algorithm selection. In Proceedings of the 6th International Conference on Learning Representations (ICLR).
[2017] Leike, J.; Martic, M.; Krakovna, V.; Ortega, P.; Everitt, T.; Lefrancq, A.; Orseau, L.; and Legg, S. 2017. AI safety gridworlds. arXiv preprint arXiv:1711.09883.
[2018] Liu, G.; Schulte, O.; Zhu, W.; and Li, Q. 2018. Toward interpretable deep reinforcement learning with linear model u-trees. CoRR abs/1807.05887.
[2018a] Loreggia, A.; Mattei, N.; Rossi, F.; and Venable, K. B. 2018a. Preferences and ethical principles in decision making. In Proceedings of the 1st AAAI/ACM Conference on AI, Ethics, and Society (AIES).
[2018b] Loreggia, A.; Mattei, N.; Rossi, F.; and Venable, K. B. 2018b. Value alignment via tractable preference distance. In Yampolskiy, R. V., ed., Artiﬁcial Intelligence Safety and Security. CRC Press. chapter 16.
[2016] Luss, R., and Petrik, M. 2016. Interpretable policies for dynamic product recommendations. In Proc. Conf. Uncertainty Artif. Intell., 74.
[2000] Ng, A. Y., and Russell, S. J. 2000. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, 663–670. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
[2017] Noothigattu, R.; Gaikwad, S.; Awad, E.; Dsouza, S.; Rahwan, I.; Ravikumar, P.; and Procaccia, A. D. 2017. A voting-based system for ethical decision making. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI).
[2015] Russell, S.; Dewey, D.; and Tegmark, M. 2015. Research priorities for robust and beneﬁcial artiﬁcial intelligence. AI Magazine 36(4):105–114.
[1974] Sen, A. 1974. Choice, ordering and morality. In Ko¨rner, S., ed., Practical Reason. Oxford: Blackwell.
[2018] Simonite, T. 2018. When bots teach themselves to cheat. Wired.
[1998] Sutton, R. S., and Barto, A. G. 1998. Introduction to Reinforcement Learning. Cambridge, MA, USA: MIT Press, 1st edition.
[2016] Theodorou, A.; Wortham, R. H.; and Bryson, J. J. 2016. Why is my robot behaving like that? designing transparency for real time inspection of autonomous robots. In AISB Workshop on Principles of Robotics. University of Bath.
[2018] Ventura, D., and Gates, D. 2018. Ethics as aesthetic: A computational creativity approach to ethical behavior. In Proc. Int. Conf. Comput. Creativity, 185–191.
[2018] Verma, A.; Murali, V.; Singh, R.; Kohli, P.; and Chaudhuri, S. 2018. Programmatically interpretable reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, 5052–5061.
[2008] Wallach, W., and Allen, C. 2008. Moral machines: Teaching robots right from wrong. Oxford University Press.

[2017] Wu, Y.-H., and Lin, S.-D. 2017. A low-cost ethics shaping approach for designing reinforcement learning agents. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI).
[2018] Yu, H.; Shen, Z.; Miao, C.; Leung, C.; Lesser, V. R.; and Yang, Q. 2018. Building ethics into artiﬁcial intelligence. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence (IJCAI), 5527–5533.

