Preprint 1–16

arXiv:1904.12206v1 [cs.LG] 27 Apr 2019

Temporal-Clustering Invariance in Irregular Healthcare Time Series

Mohammad Taha Bahadori Zachary Chase Lipton

bahadorm@amazon.com Amazon zlipton@cmu.edu Carnegie Mellon University

Abstract
Electronic records contain sequences of events, some of which take place all at once in a single visit, and others that are dispersed over multiple visits, each with a diﬀerent timestamp. We postulate that ﬁne temporal detail, e.g., whether a series of blood tests are completed at once or in rapid succession should not alter predictions based on this data. Motivated by this intuition, we propose models for analyzing sequences of multivariate clinical time series data that are invariant to this temporal clustering. We propose an eﬃcient data augmentation technique that exploits the postulated temporal-clustering invariance to regularize deep neural networks optimized for several clinical prediction tasks. We introduce two techniques to temporally coarsen (downsample) irregular time series: (i) grouping the data points based on regularly-spaced timestamps; and (ii) clustering them, yielding irregularly-paced timestamps. Moreover, we propose a MultiResolution Ensemble (MRE) model, improving predictive accuracy by ensembling predictions based on inputs sequences transformed by diﬀerent coarsening operators. Our experiments show that MRE improves the mAP on the benchmark mortality prediction task from 51.53% to 53.92%.
1. Introduction
While common academic sequence learning challenges, e.g., in natural language processing and speech, typically consist of evenly-spaced inputs, in healthcare, we frequently encounter multivariate time series, where the variables are sampled irregularly and exhibit signicantly varying base frequencies. The most popular sequence learning algorithms, based on recurrent neural networks, act upon discretized time steps, with no agreed-upon out-of-the-box method for handling such irregularities. Thus, to date, deep learning researchers have relied on heuristics, such as ignoring the timestamp information (Choi et al., 2016), binning the data into uniformly-spaced time intervals (Lipton et al., 2016b; Harutyunyan et al., 2017), and manual feature extraction (Tran et al., 2014).
Several principled approaches have been proposed, falling broadly into two categories: (i) methods that adapt classical methods naturally capable of handling irregularly sampled data, such as point process models (Islam et al., 2017; Xiao et al., 2017) and Gaussian processes (Li and Marlin, 2015; Shukla and Marlin, 2019; Futoma et al., 2017; Soleimani et al., 2017)—these works extend classical approaches with the goal of making their accuracy and scalability competitive with modern deep learning approaches; (ii) methods that adapt recurrent neural networks (Che et al., 2018a; Zheng et al., 2017; Baytas et al., 2017; Alaa et al., 2017; Ma et al., 2017; Che et al., 2018b; Song et al., 2018) or convolutional neural networks (Razavian and Sontag, 2015; Nguyen et al., 2017; Kooi and Karssemeijer, 2017) to the irregular time series setting.

c M.T. Bahadori & Z.C. Lipton.

Temporal-Clustering Invariance
To date, few papers have studied what invariances might hold in irregular time series data, or mechanisms by which learning algorithms might exploit them. Suppose a patient visits a hospital for an emergency service and undergoes a set of operations. Had she visited a smaller clinic, she might have undergone only a subset of those and the rest would have been done at a later time in another clinic. The ﬁrst process will result in a single event in the patient’s record, whereas the second will result in two events. Similar patterns can also emerge owing to the timing of the admission, insurance policies, or other factors related to the patient’s convenience. Ideally, a learning algorithm should be robust to the temporal clustering of events. However, determining the best way to exploit this structure is not straightforward, and discarding information can be problematic. For example, the frequency of visits is often informative about the severity of illness and the algorithms should be able to use it in the analysis.
As the main contribution of this work, we investigate the temporal-clustering properties of clinical time series data. To address the existence and usefulness of temporal-clustering invariance, we demonstrate data augmentation schemes to exploit it. We propose an eﬃcient data augmentation operator that coarsens the time series through a stochastic clustering. The method consists of randomly merging adjacent events, merging those more closely-spaced in time with higher probability. Our experiments demonstrate that this augmentation technique, in combination with neural network classiﬁers, can yield both more accurate classiﬁcation and greater robustness to variations in the temporal-clustering of the data. The data augmentation pipeline allows us to access representations of our inputs sequences at multiple resolutions throughout training. We discover that we can boost accuracy further by ensembling predictions across input presentation at multiple resolutions. Note that this approach requires running the multi-resolution pipeline at test time as well.
We explore two diﬀerent approaches for deterministic temporal coarsening of irregular time series. First, inspired by interpolation networks (Futoma et al., 2017; Shukla and Marlin, 2019), we shorten the time series to a time series with regularly-spaced timestamps. Alternatively, we also propose the cluster&count operator to create a shorter version of the irregular time series by clustering the timestamps. The values of the new series’ data points are the averages of the data points belonging to the corresponding clusters. We also count the number of data points in each cluster and append it to the new time series as an extra feature.
We can use the two proposed coarsening operators to create the MultiResolution Ensemble (MRE) model. We select four instances of time series using four diﬀerent coarsening probabilities. We process all versions with the same neural network and use the attention mechanism to average their predictions. The proposed model not only beneﬁts from the event-clustering invariance and is more robust to overﬁtting. It shares weights across multiple resolutions of time series (Che et al., 2018b; Razavian and Sontag, 2015) without a signiﬁcant increase in the number of parameters.
Our experiments on two benchmark clinical prediction tasks (Harutyunyan et al., 2017) deﬁned on the publicly available MIMIC-III dataset (Johnson et al., 2016) highlight the usefulness of the proposed operators. We test the performance of a large convolutional neural network with and without data augmentation and show its success in improving the generalization of the model. Second, we show that data augmentation improves robustness of the models against adversarial perturbations. Finally, our experiments show that MRE
2

Temporal-Clustering Invariance
improves the mean average percision on the benchmark mortality prediction task from 51.53% to 53.92%.
2. Methodology
In this section, we describe the structure of sequential EHR data, introduce required notation, and then describe our proposed methods.
EHR Structure and our Notation The EHR data for each patient consists of a timelabeled multivariate sequence observations. Assuming that we use r diﬀerent variables, the time series data for the n-th patient X(n) (out of N total patients) can be represented by a sequence of T (n) tuples (t(in), x(in)) ∈ R × Rr, i = 1, . . . , T (n). The timestamp t(in) denotes the time of the i-th visit of the n-th patient, x(in) denotes an r-dimensional feature vector that typically takes the value of the observed variable(s) at the corresponding component(s) (the treatment of missing variables is addressed in greater detail below), and T (n) denotes the number of visits of the n-th patient. To minimize clutter, we drop the superscript (n) whenever it is unambiguous. Depending on which precise task we are addressing, the objective of our predictive models is to predict either (i) the label corresponding to each time step yi ∈ {0, 1}s given only the left-ward context, or (ii) to predict a sequence-level label at the end of the sequence y ∈ {0, 1}s. Commonly, as in the learning to diagnose task, the prediction task is multilabel, i.e. s > 1.
Learning Tasks Our proposed methods can be used in both learning to diagnose (L2D) (Lipton et al., 2016a) and encounter sequence modeling (ESM) (Choi et al., 2016). In the L2D task, the input vector xi consists of continuous clinical measurements. If there are r diﬀerent measurements, then xi ∈ Rr. The goal of L2D is, given an input sequence x1, . . . , xT , to predict the occurrence of a speciﬁc disease (s = 1) or multiple diseases (s > 1). Without loss of generality, we will describe the algorithm for L2D, as ESM can be seen as a special case of L2D where we make a prediction at each timestamp t given the patient’s history before t.
Invariant Learning Suppose T (X) denote a transformation of the patient data X. If P (y|T (X)) = P (y|X), prediction of y with X is invariance to transformation T . If the relationship holds approximately, i.e. P (y|T (X)) ≈ P (y|X), we have a more relaxed approximate invariance. Given powerful estimators such as RNNs, we can augment the training data using the transformation and achieve a T −invariant model. In the rest of this section, we introduce the temporal-clustering approximate invariance, use it in the augmentation setting, and propose an ensemble model based on it.
2.1. Fast Data Augmentation
Data augmentation is the process of creating distorted instances of data points, nominally increasing the size of the dataset. When applied to exploit known invariances, this technique is well-known to reduce overﬁtting, and has become a standard part of the applied computer vision (Krizhevsky et al., 2012), and in automatic speech recognition, where the technique is dubbed multicondition training (Barker et al., 2001). We design a data augmentation procedure for healthcare time series demonstrating that temporal clustering is indeed an invariance worth exploiting in real-world healthcare time series.
3

Temporal-Clustering Invariance

Algorithm 1 Fast Data Augmentation

1:

Input:

patient

sequence

{(

ti

,

x

i

,

ci

)}

T i=1

,

largest clustering probability phigh, Weighted or unweighted augmentation

2: Output: transformed sequence {(ti, xi, ci)}Ti=1.

3: if Weighted then

4: ∆ ← [t2 − t1, . . . , tT − tT −1].

5: p∆ ← [1/∆1, . . . , 1/∆T −1].

6: p∆ = p∆/(1 p∆).

7: else

8: p∆ ← 1T −1/(T − 1).

9: end if

10: Draw p ∼ Unif(0, phigh)

11: E ← p · T samples without replacement from Multinomial(p∆)

12: Create clusters Ci for i = 1, . . . , T by assigning every pair (i, i + 1) to the same cluster

if i ∈ E.

13: for i = 1 to T do

14: ti ← mean({t : t ∈ Ci }) 15: xi ← mean({x : x ∈ Ci }) 16: ci ← sum({c : c ∈ Ci }) 17: end for

18: Return: sorted sequence {(ti, xi, ci)}Ti=1 based on t .

To formally describe the augmentation operator, we append a count variable to each tuple in the data, i.e. (ti, xi) → (ti, xi, ci). Clearly, ci = 1 for i = 1, . . . , T for all original sequences. Let ST denote the space of all patient sequences of length T and a count-augmented patient sequence, which is a member of ST , as X. The data augmentation operation is a probabilistic mapping dap(·) : ST → ST , where T = T − p · T and · is the ceiling operator. We draw the coarsening probability p randomly according to p ∼ Unif(0, phigh), The upper bound on the transformed sequence length (1 − phigh)T corresponds to the degree of regularization conferred by this approach; lower values of phigh give greater regularization.
Given a clustering probability p, for a sequence of length T , we choose p · T intervals at random without replacement and combine the events at both ends of the selected intervals. The details of the algorithm are provided in Algorithm 1. We further explore a weighted version of the data augmentation by joining the closer events likelier. For the weighted data augmentation, we create a time interval vector of length T − 1: ∆ = [t2 − t1, . . . , tT − tT −1]. We create a probability value for T − 1 intervals by deﬁning p∆ = [1/∆1, . . . , 1/∆T −1] and normalizing it as p∆ = p∆/(1 p∆). We deﬁne a multinominal distribution using the probability vector p∆ and draw p · T samples from it without replacement. We create the ﬁnal clustering by assigning the data points on the ends of each drawn interval to the same cluster.
The data augmentation combats overﬁtting by providing distorted versions of the data to the training algorithm. In typical data augmentation schemes the model only uses the unperturbed original data at test time. In our augmentation scheme experiments conﬁrm

4

Temporal-Clustering Invariance

Original !", $", %"
!", $", %" grid&count

!&, $&, %& !', $', %' !&* , $&* , %&*

!(, $(, %( !), $), %) !'* , $'* , %'*

cluster&count !", $", %"

!&* , $&* , %&*

!'* , $'* , %'*

Figure 1: Visualization of the coarsening operators. grid&count0.6(·) clusters data points
into uniformly spaced data points over time. cluster&count0.6(·) produces another irregular
time series with the new time stamps t2 = (t2 + t3)/2 and t3 = (t4 + t5)/2. In both cases we have x2 = (x2 + x3)/2 and x3 = (x4 + x5)/2. The count variables take values c1 = 1 and c2 = c3 = 2.

Algorithm 2 Grid&count

1:

Input:

patient

sequence

{(

ti

,

x

i

,

ci

)}

T i=1

,

clustering probability p, and observation interval [tL, tR].

2: Output: transformed sequence {(ti, xi, ci)}Ti=1.

3: T ← p · T .

4: ti +1 ← tL + i · (tR − tL)/(T − 1) for i = 0, . . . , T − 1.

5: Initialize clusters Ci = ∅ for i = 1, . . . , T .

6: for i = 1 to T do

7: i ← argmini {|ti − ti | for i = 1, . . . , T }. 8: Ci ← Ci ∪ {(ti, xi, ci)}

9: end for

10: for i = 1 to T do

11: ti ← mean({t : t ∈ Ci }) 12: xi ← mean({x : x ∈ Ci }) 13: ci ← sum({c : c ∈ Ci }) 14: end for

15: Return: sorted sequence {(ti, xi, ci)}Ti=1 based on t .

beneﬁts to providing access to the coarser-grained versions of the data at test time. To begin, we deﬁne our new deterministic time series coarsening operators.
2.2. The Coarsening Operations
In designing the coarsening operator, we seek transformations from given data sequences to various diﬀerent but plausible sequence. We exploit our intuition that multiple data points might be collected during a single visit or across multiple visits. This leads us to the idea of creating diﬀerent resolutions of the sequence by clustering the timestamps into a number of groups, (resulting in a shorter sequence than the original). We note that clustering might signiﬁcantly change the point process properties of the sequence. Thus, we append a count variable to each cluster that indicates the number of data points assigned to that cluster. We consider two possibilities for coarsening of irregular time series: regular-grid-based and clustering-based.
With the grid-based coarsening, our aim is to convert an irregular time series to a shorter time series with regularly spaced timestamps. We ﬁrst select a set of timestamps, and then
5

Temporal-Clustering Invariance

Algorithm 3 Cluster&count

1:

Input:

patient

sequence

{(

ti

,

x

i

,

ci

)}

T i=1

,

clustering probability p.

2: Output: transformed sequence {(ti, xi, ci)}Ti=1. 3: T ← p · T .

4: Create clusters Ci for i = 1, . . . , T by clustering {ti}Ti=1 into T 5: for i = 1 to T do

6: ti ← mean({t : t ∈ Ci }) 7: xi ← mean({x : x ∈ Ci }) 8: ci ← sum({c : c ∈ Ci }) 9: end for

10:

Return:

sorted

sequence

{

(t

i

,

x

i

,

ci

)}

T i=1

based

on

t.

clusters using k-means.

assign each event to the closest timestamp. Next, we compute the average values among events assigned to each new timestamp to calculate the transformed time series. If no event is assigned to a particular timestamp, we leave the values as all zeros. Finally, we append the cluster size as an extra feature to the feature vector. Notice that grid-based coarsening, similar to interpolation networks (Futoma et al., 2017; Shukla and Marlin, 2019), converts irregular time series to regular ones, which can make it easier for analysis in the next steps. The details of the grid&count operation are provided in Algorithm 2.
Our proposed alternative is a clustering-based coarsening operator, which we denote cluster&count. Here, we execute a temporal clustering of the sequence of events in the original sequence, yielding taking the cluster centers as new time stamps. The count variables ci in the transformed sequence represent the number of points in the i th cluster. Assuming that all categorical features have been encoded with one-hot or similar encoding, the new feature vectors is the average of all vectors in the same cluster.
Algorithms 2 and 3 show the details of the grid&count and cluster&count operations, respectively. Figure 1 visualizes the operators in a simple case. In both coarsening operations, we ignore the missing data during aggregation (inside the last for loop in the algorithms). If a variable is missed in all events belonging to a cluster, we impute it with zero, following the observations in (Lipton et al., 2016b).

2.3. A Multi-resolution Ensemble Network
In this section, we propose to generate multiple resolutions of the input data using the coarsening operators described in the previous section for use by our predictive models. Our model exploits multi-resolution via an attention mechanism. To avoid an unnecessary explosion in the number of parameters, we share weights of our model across all resolutions.
Formally, let fθ : ST → [0, 1]s denote a classiﬁer parameterized by θ, which maps a patient sequence to probabilities of labels being 1. Given K diﬀerent clustering factors pk for k = 1, . . . , K, we deﬁne the MultiResolution Ensemble (MRE) classiﬁer as follows:

K

gθ,β(X) = αk,β(X)fθ(Cpk (X)),

(1)

k=1

6

Temporal-Clustering Invariance
where αk,β(·) for k = 1, . . . , K are the attention values. The coarsening operator Cpk (X) can be either of grid-based or clustering based operators; we denote the corresponding networks by MRE-Grid and MRE-Cluster, respectively. The attention generation parameters β are the only additional parameters of this classiﬁer. We can either use the intermediate hidden representations obtained by fθ from the one of Cpk (X) or use simple features such as length of time series. Given the pre-determined cluster lengths, we can pre-compute the Cpk (X) for all sequences and avoid the clustering cost at the run time.
In our experiments, we use MRE with K = 4 and p1 = 1, p2 = 1/2, p3 = 1/4, and p4 = 1/8. The exponentially decreasing pk is intended to provide access to a wide range of resolutions of the input data. From an ensemble learning point of view, it is beneﬁcial to choose signiﬁcantly diﬀerent pk to create more diversity.
Unlike Razavian and Sontag (2015), who use a diﬀerent CNNs for each resolution of the data, MRE shares the weights across all resolutions of the data and reduces the number of model parameters. We can view MRE as a mixture model (Ji et al., 2016) and use the latent-variable inference tools to estimate the class probabilities instead of attention variables. In this work, we prefer to view the mixture weights as attention weights for computational simplicity. MRE may potentially be more robust to jitters in the timestamps of the sequences (Oh et al., 2018).
We note a connection between the proposed MRE and existing work that studies symmetries and invariances in neural networks (Gens and Domingos, 2014; Cohen and Welling, 2016; Kondor and Trivedi, 2018). Similar to the group convolutions, we compute the output of the neural network on the input data under diﬀerent transformations. In contrast to the group convolutions, we apply the transformation grid&count or cluster&count to the data rather than the convolutional ﬁlters. Note that the neither of the coarsening operations deﬁne a group because it is not invertible due to information loss during the clustering operation. However, the counting operation attempts to preserve more information—related to the point process—during the coarsening process.
Base learners We use the RNN base learners as fθ in Eq. (1) as they are the common deep neural networks for predictive modeling in healthcare time series (Lipton et al., 2016a; Choi et al., 2016). In this architecture, after linear embedding, we opt to use two layers of Gated Recurrent Units (GRU) (Cho et al., 2014) (instead of the better-known Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997)) owing to their comparative parsimony with respect to parameters. For the prediction layer, we use a residual block with fully connected units. We provde further details of the architectures in Appendix A.
3. Experiments
We evaluate the proposed algorithms on two L2D benchmark tasks: in-hospital mortality and length of stay (LoS) prediction (Harutyunyan et al., 2017), which are deﬁned on the publicly available MIMIC-III dataset (Johnson et al., 2016). Both predictive tasks are widely used in estimating patients’ clinical risk and managing costs in hospitals. For example, the LACE clinical risk scores (Ben-Chetrit et al., 2012; van Walraven et al., 2012, 2010; Gruneir et al., 2011) require an estimate of the length of stay of a patient soon after they are admitted to the hospital. Details of feature extraction and cohort construction can be found in the following benchmark paper (Harutyunyan et al., 2017) and we provide a brief summary below.
7

Temporal-Clustering Invariance
3.1. Dataset, tasks, normalization, and training details
Dataset We follow Harutyunyan et al. (2017), extracting the patient sequences from the MIMIC-III database and partition the data into training and testing sets. The benchmark data for mortality and LoS prediction tasks have 17,902/3,236 and 35,344/6,225 training/test data points, respectively. We do not use the benchmark discretization and normalization steps and normalize the irregular time series by ourselves as follows. In the length of stay prediction task, we do not use the dataset expansion technique used by the benchmark; i.e., for both training and testing we use 24 hours of patient history.
Normalization of real values Noting the existence of outliers in the data, we compute the robust mean and variance by excluding the values below the 2nd percentile and above 98th percentile. Using the estimated robust mean and variance, we normalize the real-valued quantities to zero mean and unit variance. Additionally, for robustness, we Winsorize the real values by thresholding them within the limits of 2nd and 98th percentiles.
Ordinal encoding We use the unary coding (Fiete and Seung, 2007; Moore et al., 2011) for the ordinal quantities such as the Glasgow Coma Score instead of the one-hot encoding in (Harutyunyan et al., 2017). A k-level ordinal variable with real values a1 < a2 < . . . < ak is encoded using a binary vector b of size k − 1. We represent each value a for = 1, . . . , k by setting the ﬁrst − 1 elements of b to 1. This approach encodes a +1 − a values and is able to model large real values for ak. We also quantize the count variables c in bins of (0 − 1], (1 − 2], (2, 4], (4, ∞) and encode them using the unary coding. After appending the encoded time and count vectors, the resulting feature vectors are 51-dimensional. We use zero-ﬁlling to handling missing variables because it is simple and eﬃcient (Lipton et al., 2016b).
Training details We hold out 15% of the training data as a validation set for tuning the hidden layer sizes and hyperparameters and report the test results based on the best validation performance. For optimization, we use Adam (Kingma and Ba, 2014) with the AMSGrad modiﬁcation (Reddi et al., 2018) with batch size of 100. We halve the learning rate after plateauing for 10 epochs (determined on validation data) and stop training after the learning rate drops below 5 × 10−6. We run each algorithm 8 times with diﬀerent random initializations and report the result of the run with the highest validation accuracy. All of the proposed models are implemented in PyTorch (Paszke et al., 2017).
3.2. Results and analysis
Data augmentation First, we test the impact of data augmentation in improving the robustness of training. An ideal data augmentation procedure needs to make complex models robust to overﬁtting, without hurting the performance of others. To show these desirable attributes for our proposed data augmentation, To show the beneﬁts of our methods vis-a-vis overﬁtting, we show results on a large CNN that tends generally to overﬁt this data quickly. We use the SGD with ﬁxed learning rate and momentum of 10−2 and 0.95, respectively. Results of this experiments are shown in Figure 2. To show smoother curves, we show the average of eight runs with random initializations for each case. Figures 2a and 2b show that data augmentation allows longer training time before the validation accuracy decreases. We also experiment with a GRU model less prone to overﬁtting. Figures 2c and 2d show that
8

Temporal-Clustering Invariance

ROC-AUC

0.86

0.84

0.82

0.80

0.78

0.76

0.74

0.72

No Aug. Weighted Aug. Unif. Aug. 1000 2000 3000 4000 5000 6000 7000 8000

Iteration

(a) Mortality, CNN 0.86

0.84

0.82

0.80

0.78

0.76

0.74

0.72

No Aug. Weighted Aug. Unif. Aug. 1000 2000 3000 4000 5000 6000 7000 8000

Iteration

(c) Mortality, RNN

Correlation Coefficient

Correlation Coefficient

0.42

0.40

0.38

0.36

0.34 No Aug. Weighted Aug. Unif. Aug. 2000 4000Iteratio6n000 8000 10000

(b) Length of stay, CNN

0.42

0.40

0.38

0.36

0.34

0.32

0.30

0.28

No Aug. Weighted Aug. Unif. Aug. 2000 4000 6000 8000 10000

Iteration

(d ) Length of stay, RNN

ROC-AUC

Figure 2: Augmenting the training data via fast temporal-clustering not only prevents overﬁtting in large CNN model but also does not signiﬁcantly hurt the small RNN model. We demonstrate the behavior of the proposed data augmentation on two tasks: mortality and length of stay (LoS) prediction. The CNN model is intentionally chosen to be overparameterized. We choose an overparameterized CNN with and without cluster&count augmentation (phigh = 0.5) and SGD with ﬁxed learning rate and momentum of 10−2 and 0.95, respectively. To show smoother curves, we show the average of eight runs with random initializations for each case.

data augmentation helps the GRU model as well but to a lesser extent. More details on both network architectures are given in Appendix A.
We experiment with both weighted and unweighted coarsening augmentation. The results Figure 2 indicated that weighted and unweighted augmentations have similar eﬀects on the performance of the algorithms. Looking at the beginning of Figure 2a, it is clear that the weighted augmentation provides a less diverse set of samples that are more similar to the actual data. It is a milder data augmentation, thus the performance improves faster early during the training. Given the milder nature of the weighted augmentation, it is a better candidate for use alongside other data augmentation techniques.
In Figure 3a, we examine the impact of data augmentation as we increase the fraction of training data used during the training. Ignoring the seemingly anomalous data point in 0.18, the general trend indicates that initially when we are given only 5% of data, our
9

Temporal-Clustering Invariance

Test ROC-AUC
ROC-AUC

0.86

0.84

0.82

0.80

0.78

0.76

w/Augmentation

0.74 0.05 0.077

0.12

0.18

w/o Augmentation 0.28 0.43 0.65 0.9

Fraction of training data

(a) Learning Curve

0.8

w/ Augmentation

w/o Augmentation

0.6

0.4

0.2

0.00.00 0.05 0.10 0.15 0.20 0.25 0.30 epsilon
(b) Input Sensitivity Analysis

Figure 3: (Left) The learning curve with and without data augmentation on the mortality prediction task. The network is the same GRU network used in Figure 2. The x-axis in on a logarithmic scale. (Right) Data augmentation using cluster&count improves robustness to adversarial examples. Here we show the ROC-AUC of the CNN network on the mortality task against FGSM attacks of varying strength perturbation ε.

data augmentation improves the test AUC by about 1 percentage point. The amount of improvement increases as we use a larger fraction of data when we use 0.077 and 0.12 fraction of training data points. As we further increase the fraction of training data, the gain by data augmentation shrinks. Note that this this result also indicates that the fact that in Figures 2c and 2d the gap between non-augmented and augmented training is negligible is only because of the size of the training data and the network in the tasks.
Our proposed temporal clustering-based data augmentation is more robust to the perturbations on the input features, as shown in Figure 3b. To demonstrate the (relative) adversarial robustness, we consider its performance under attacks via the fast gradient sign method (FGSM) due to Goodfellow et al. (2014), because it is simple and parameterized only with a single parameter ε. In this purturbation, we add the adversarial noise x ← x + ε · sign ∂L(y∂,xf(x)) to the input, where f (·) and L(·, ·) denote the prediction and binary cross-entropy loss functions, respectively. Then, we evaluate the accuracy of the classiﬁer on the perturbed data f (x). We perform this test on the convolutional neural networks in the previous example on the mortality task. The results (Figure 3b) show that the data augmentation via cluster&count does indeed increase the robustness to adversarial perturbations. The reason behind the robustness is that the time series transformed by cluster&count not only have new timestamps but also they have new values for the variables. Note that our analysis is not intended to show that our data augmentation creates robustness to adversarial attacks. Instead we oﬀer these experiments simply to provide input sensitivity analysis.
Accuracy evaluation We present the main accuracy results in Tables 1 and 2. To quantify the randomness in the test set, we report the bootstrap (1000 runs) estimate of the mean and standard error of the evaluation measures. In the mortality task, we not only include the reported single-task result in (Harutyunyan et al., 2017; Song et al., 2018), but also

10

Temporal-Clustering Invariance

Table 1: In-hospital accuracy results: bold numbers show the best results. Our results are reported in the form of ‘mean (standard error)’.

Model
LSTM (Harutyunyan et al., 2017) SAnD (Song et al., 2018) GRU (baseline preprocessing) GRU (our preprocessing)
GRU-Augmented MRE-GRU-Grid MRE-GRU-Cluster

ROC-AUC
0.8623 0.857 0.8556 (0.000307) 0.8642 (0.000306)
0.8566 (0.000310) 0.8670 (0.000310)
0.8665 (0.000313)

mAP
0.5153 0.518 0.4950 (0.000860) 0.5263 (0.000876)
0.5240 (0.000876) 0.5392 (0.000879) 0.5392 (0.000829)

Table 2: Length of stay prediction results: bold numbers show the best results. Our results are reported in the form of ‘mean (standard error)’. -marked experiments use a special dataset expansion technique.

Model

Correlation

MAE

RMSE

LSTM (Harutyunyan et al., 2017) SAnD (Song et al., 2018) GRU (baseline preprocessing) GRU (our preprocessing)

– – 0.4093 (0.000478) 0.4199 (0.000525)

94.00 –
56.3557 (0.0459) 55.9366 (0.0504)

205.34 200.93 122.9606 (0.1683) 123.1219 (0.1760)

GRU-Augmented MRE-GRU-Grid MRE-GRU-Cluster

0.4289 (0.000557) 55.9823 (0.0514) 123.1832 (0.1850) 0.4328 (0.000522) 56.3363 (0.0501) 123.2220 (0.1627) 0.4402 (0.000500) 55.5906 (0.0527) 123.6223 (0.1774)

we report our baseline GRU network on the data preprocessed by the benchmark, denoted by “GRU (base prep)”. “GRU (our prep)” shows the accuracy of our GRU model on the data with our special preprocessing describe earlier in this section. We measure accuracy of classiﬁcation with two metrics: the area under receiver true-positive vs false-positive curve known as “ROC-AUC” and the area under precision recall curve known as mean average precision or “mAP”. Because the length of stay prediction is a regression task, we use three commonly used metrics: mean absolute error (MAE) and root mean squared error (RMSE) as un-normalized and linear correlation coeﬃcient as a normalized accuracy measures.
As we can see in Tables 1 and 2, both variants of the proposed MRE together with our preprocessing outperforms the single-task baselines reported in (Harutyunyan et al., 2017; Song et al., 2018). Moreover, our GRU network works better than the baseline, because of our preprocessing techniques. Note that in our ensemble, all weak learners are the same—their outputs diﬀer only because they act upon diﬀerent input representations We also report the

11

Temporal-Clustering Invariance

Test ROC-AUC

0.870

0.865

0.860

0.855

0.850

0.845

0.840

0.835

0.830

GRU

MRE-Grid

MRE-Cluster

500 1000 1500 2000 2500 3000 3500 4000

Iteration

Figure 4: The MRE-Grid model with the GRU weak-learner converges faster per iteration on the mortality prediction task. This plot shows the average best test accuracy as we train for more iterations. Note that in Tables 1 and 2 we have used the validation accuracy for selection of the best model, thus the results are diﬀerent.

multitask results in the benchmarks only for comparison purposes, however, comparing them with our single task results may not be fair.
Two observations in the LoS prediction results in Table 2 stand out: (1) Our MAE and RMSE results in the LoS task is much better than the results reported in (Harutyunyan et al., 2017; Song et al., 2018). Inspecting the benchmark preprocessing code, we realized that the authors have expanded the training dataset by creating examples of patients with time series longer than 24 hours and LoS value recalculated at the end of training period. We observed that this dataset expansion creates signiﬁcant bias in the results and did not use it; (2) Given the large discrepancy between RMSE and MAE results, the RMSE values are likely to be impacted by the tail values. The standard errors also indicate that diﬀerences in the results are unlikely to be signiﬁcant.
In terms of speed, MRE is not signiﬁcantly slower than the corresponding weak learner. We run the coarsening operators on the data and persist the clusters before the training of MRE and only compute the batches only ﬂy during the training. In terms of training convergence speed, Figure 4 shows that MRE converges faster compared to the weak learner that it uses.

4. Related Works
Time deformation One well-studied invariance in irregular time series is robustness to the deformations in the timing of the events. Dynamic Time Warping (Vintsyuk, 1968; Sakoe and Chiba, 1978; Keogh and Ratanamahatana, 2005) is a popular traditional method to incorporate invariance to the exact timing of events. In more recent work, Oh et al. (2018) propose to learn new timestamps for the events to make neural networks invariant to jitters in event times. Given the similarity between the coarsening operators and the one-dimensional pooling operation, we expect the coarsening operation provide a degree of time deformation invariance.
Errors in measurements Measurement errors can happen both due to the devices recording and storing measurements and due to human error in registering values. Adversarial training (Goodfellow et al., 2014) is one eﬀective way to make algorithms less sensitive to

12

Temporal-Clustering Invariance
small errors in the input data. If we attach the true timing of the events as a feature to the event features, adversarial training can potentially make the algorithm robust to jitters in the timing of the events too similar to (Oh et al., 2018). Given that the proposed coarsening operators aggregate the values in the clusters, we expect to achieve a degree of invariance to small changes in measurements too.
Missing data Often, not all variables are measured for all patients at all times (Lipton et al., 2016b; Che et al., 2018a). For data missing completely at random, we might seek to make our models robust via dropout training (Srivastava et al., 2014), randomly zeroing out a subset of variables in the input time series and augment the data using the newly created data points. Other approaches include inputation via graphical models, or incorporating missing value indicators (Lipton et al., 2016b; Che et al., 2018a).
Censoring Medical data is often censored, i.e., we do not observe the full history for all patients (Klein and Moeschberger, 2006). For example, in the right censoring scenario, if a patient stays alive at the time of a survival analysis task, her future will not be available to the algorithm. Thus, we will not have the full-length records of all patients in our dataset. Target replication (Lipton et al., 2016a; Dai and Le, 2015; Ng et al., 2015) is a technique that requires the algorithm to predict the ﬁnal outcome at any point in the history of a patient. Thus, it aims at making the learning algorithm robust to right censoring.
5. Conclusions and Future Work
In this work, we proposed to exploit a postulated temporal clustering invariance and examined the beneﬁts of the derived methods for making predictions given time series of clinical healthcare data. Our proposed data augmentation techniques exploit this invariance to prevent overﬁtting in neural networks. Moreover, we demonstrate that a multi-resolution ensemble technique can improve predicted accuracy by acting simultaneously upon multiple resolutions of data. Our methods in this paper represent clusters by averaging representations. In future work, we might learn the aggregation function too. Additionally, we might seek to understand the relationship between temporal clustering invariance and other properties of multivariate clinical time series data.
References
Ahmed M Alaa, Scott Hu, and Mihaela van der Schaar. Learning from clinical judgments: Semimarkov-modulated marked hawkes processes for risk prognosis. arXiv:1705.05267, 2017.
Jon Barker, Martin Cooke, and Phil Green. Robust asr based on clean speech models: An evaluation of missing data techniques for connected digit recognition in noise. In EUROSPEECH, 2001.
Inci M Baytas, Cao Xiao, Xi Zhang, Fei Wang, Anil K Jain, and Jiayu Zhou. Patient subtyping via time-aware LSTM networks. In KDD, pages 65–74. ACM, 2017.
Eli Ben-Chetrit, Chen Chen-Shuali, Eran Zimran, Gabriel Munter, and Gideon Nesher. A simpliﬁed scoring tool for prediction of readmission in elderly patients hospitalized in internal medicine departments. Isr Med Assoc J, 14(12), 2012.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientiﬁc reports, 2018a.
Zhengping Che, Sanjay Purushotham, Guangyu Li, Bo Jiang, and Yan Liu. Hierarchical deep generative models for multi-rate multivariate time series. In ICML, pages 783–792, 2018b.
13

Temporal-Clustering Invariance
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.
Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Doctor ai: Predicting clinical events via recurrent neural networks. In MLHC, pages 301–318, 2016.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016. Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NIPS, pages 3079–3087, 2015. Ila R Fiete and H Sebastian Seung. Neural network models of birdsong production, learning, and
coding. In New Encyclopedia of Neuroscience. 2007. Joseph Futoma, Sanjay Hariharan, Katherine Heller, Mark Sendak, Nathan Brajer, Meredith Clement,
Armando Bedoya, and Cara O’Brien. An improved multi-output gaussian process rnn with real-time validation for early sepsis detection. In MLHC, 2017. Robert Gens and Pedro M Domingos. Deep symmetry networks. In NIPS, pages 2537–2545, 2014. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv:1412.6572, 2014. Andrea Gruneir, Irfan A Dhalla, Carl van Walraven, Hadas D Fischer, Ximena Camacho, Paula A Rochon, and Geoﬀrey M Anderson. Unplanned readmissions after hospital discharge among patients identiﬁed as being at high risk for readmission using a validated predictive algorithm. Open Medicine, 5(2):e104, 2011. Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. arXiv:1703.07771, 2017. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 1997. Kazi T Islam, Christian R Shelton, Juan I Casse, and Randall Wetzel. Marked point process for severity of illness assessment. In MLHC, pages 255–270, 2017. Yangfeng Ji, Gholamreza Haﬀari, and Jacob Eisenstein. A latent variable recurrent neural network for discourse relation language models. In NAACL-HLT, pages 332–342, 2016. Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. MIMIC-III, a freely accessible critical care database. Scientiﬁc data, 3:160035, 2016. Eamonn Keogh and Chotirat Ann Ratanamahatana. Exact indexing of dynamic time warping. KAIS, 2005. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. John P Klein and Melvin L Moeschberger. Survival analysis: techniques for censored and truncated data. Springer Science & Business Media, 2006. Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. arXiv:1802.03690, 2018. Thijs Kooi and Nico Karssemeijer. Classifying symmetrical diﬀerences and temporal change for the detection of malignant masses in mammography using deep neural networks. Journal of Medical Imaging, 4(4):044501, 2017. Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS, pages 1097–1105, 2012. Steven Cheng-Xian Li and Benjamin M Marlin. Classiﬁcation of sparse and irregularly sampled time series with mixtures of expected gaussian kernels and random features. In UAI, 2015. Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose with lstm recurrent neural networks. In ICLR, 2016a. Zachary C Lipton, David C Kale, and Randall Wetzel. Modeling missing data in clinical time series with rnns. MLHC, 2016b.
14

Temporal-Clustering Invariance
Fenglong Ma, Radha Chitta, Jing Zhou, Quanzeng You, Tong Sun, and Jing Gao. Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. In KDD, pages 1903–1911. ACM, 2017.
Jordan M Moore, Tamás Székely, József Büki, and Timothy J DeVoogd. Motor pathway convergence predicts syllable repertoire size in oscine birds. PNAS, 108(39):16440–16445, 2011.
Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. Beyond short snippets: Deep networks for video classiﬁcation. In CVPR, pages 4694–4702, 2015.
Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. mathtt{Deepr}: A Convolutional Net for Medical Records. BHI, 21(1):22–30, 2017.
Jeeheh Oh, Jiaxuan Wang, and Jenna Wiens. Learning to exploit invariances in clinical time-series data using sequence transformer networks. In MLHC, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic diﬀerentiation in pytorch. In NIPS-W, 2017.
Narges Razavian and David Sontag. Temporal convolutional neural networks for diagnosis from lab tests. arXiv:1511.07938, 2015.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In ICLR, 2018.
Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE Trans. Acoust., Speech, Signal Process, 26(1):43–49, 1978.
Satya Narayan Shukla and Benjamin M. Marlin. Interpolation-prediction networks for irregularly sampled time series. In ICLR, 2019.
Hossein Soleimani, Adarsh Subbaswamy, and Suchi Saria. Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions. In UAI, 2017.
Huan Song, Deepta Rajan, Jayaraman J Thiagarajan, and Andreas Spanias. Attend and diagnose: Clinical time series analysis using attention models. In AAAI, 2018.
Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958, 2014.
Truyen Tran, Wei Luo, Dinh Phung, Sunil Gupta, Santu Rana, Richard Lee Kennedy, Ann Larkins, and Svetha Venkatesh. A framework for feature extraction from hospital medical data with applications in risk prediction. BMC bioinformatics, 15(1):425, 2014.
Carl van Walraven, Irfan A Dhalla, Chaim Bell, Edward Etchells, Ian G Stiell, Kelly Zarnke, Peter C Austin, and Alan J Forster. Derivation and validation of an index to predict early death or unplanned readmission after discharge from hospital to the community. Can. Med. Assoc. J., 2010.
Carl van Walraven, Jenna Wong, and Alan J Forster. Lace+ index: extension of a validated index to predict early death or urgent readmission after hospital discharge using administrative data. Open Medicine, 6(3):e80, 2012.
T.K. Vintsyuk. Speech discrimination by dynamic programming. Cybernetics, 4(1):52–57, 1968. Shuai Xiao, Junchi Yan, Xiaokang Yang, Hongyuan Zha, and Stephen M Chu. Modeling the Intensity
Function of Point Process Via Recurrent Neural Networks. In AAAI, 2017. Kaiping Zheng, Wei Wang, Jinyang Gao, Kee Yuan Ngiam, Beng Chin Ooi, and Wei Luen James
Yip. Capturing feature-level irregularity in disease progression modeling. In CIKM, 2017.
15

Temporal-Clustering Invariance
Appendix A. Details of the base learners’ architecture
GRU models Our GRU model is a two layers GRU layers followed by a residual block for prediction. The residual block is in the form of relu(bnorm_2(fc_2(relu(bnorm_2(fc_2(x)))))) + fc_3(x). We perform light hyperparameter optimization on the size of hidden vector of GRU. The CNN model The CNN model ﬁrst embeds the input into a higher dimensional vector. Next, it applies a sequence of convolutional residual blocks with progressively larger dilations to the embedded vector. Finally, we ﬂatten the last hidden tensor and apply a batch normalization and dropout layer before fully connected prediction layer.
In a residual block with dilation factor of d, we apply d dilations in the pair of convolutions and 2d dilation in the skip connection. The kernel size for all 1d-convs is set to 3.
In the experiments for Figure 2, we intentionally large CNN model. Its speciﬁc settings are as follows:
1. Embedding (dim=200) 2. ResBlock (200, 180, 160, kernel_size=3, dil=1) 3. ResBlock (160, 120, 100, kernel_size=3, dil=10) 4. ResBlock (120, 100, 80, kernel_size=3, dil=25) 5. FC(1280, 1) On the length of stay task, we use dilation sequence of (1, 3, 3), because the sequences are shorter.
16

