POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

arXiv:1907.03712v2 [cs.LG] 16 Dec 2019

Policy-Gradient Algorithms Have No Guarantees of Convergence in Linear Quadratic Games

Eric Mazumdar Department of Electrical Engineering and Computer Science University of California, Berkeley, CA
Lillian J. Ratliff Department of Electrical and Computer Engineering University of Washington, Seattle, WA
Michael I. Jordan Division of Computer Science and Department of Statistics University of California, Berkeley, CA
S. Shankar Sastry Department of Electrical Engineering and Computer Science University of California, Berkeley, CA

EMAZUMDAR@EECS.BERKELEY.EDU RATLIFFL@UW.EDU
JORDAN@CS.BERKELEY.EDU SASTRY@COE.BERKELEY.EDU

Editor:

Abstract
We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in N –player general-sum linear quadratic games, a classic game setting which is recently emerging as a benchmark in the ﬁeld of multi-agent learning. In such games the state and action spaces are continuous and global Nash equilibria can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi-agent policygradient. We ﬁrst show that these games are surprisingly not convex games. Despite this, we are still able to show that the only critical points of the gradient dynamics are global Nash equilibria. We then give sufﬁcient conditions under which policy-gradient will avoid the Nash equilibria, and generate a large number of general-sum linear quadratic games that satisfy these conditions. In such games we empirically observe the players converging to limit cycles for which the time average does not coincide with a Nash equilibrium. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no local guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.
1. Introduction
Interest in multi-agent reinforcement learning has seen a recent surge of late, and policy-gradient algorithms are championed due to their potential scalability. Indeed, recent impressive successes of multi-agent reinforcement learning have made use of policy optimization algorithms such as multiagent actor-critic (Lowe et al., 2017; Srinivasan et al., 2018; Jaderberg et al., 2019), multi-agent proximal policy optimization (Bansal et al., 2018), and even simple multi-agent policy-gradients

1

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY
(Lanctot et al., 2017) in problems where the various agents have high-dimensional continuous state and action spaces like StarCraft II (Vinyals et al., 2019).
Despite these successes, a theoretical understanding of these algorithms in multi-agent settings is still lacking. Missing perhaps, is a tractable yet sufﬁciently complex setting in which to study these algorithms. Recently, there has been much interest in analyzing the convergence and sample complexity of policy-gradient algorithms in the classic linear quadratic regulator (LQR) problem from optimal control (Kalman, 1960). The LQR problem is a particularly apt setting to study the properties of reinforcement learning algorithms due to the existence of an optimal policy which is a linear function of the state and which can be found by solving a Ricatti equation. Indeed, the relative simplicity of the problem has allowed for new insights into the behavior of reinforcement learning algorithms in continuous action and state spaces (Dean et al., 2017; Fazel et al., 2018; Malik et al., 2019).
An extension of the LQR problem to the setting with multiple agents, known as a linear quadratic (LQ) game, has also been well studied in the literature on dynamic games and optimal control (Basar and Olsder, 1998). As the name suggests, an LQ game is a setting in which multiple agents attempt to optimally control a shared linear dynamical system subject to quadratic costs. Since the players have their own costs, the notion of ‘optimality’ in such games is a Nash equilibrium properties of which have been well analyzed in the literature Engwerda (1998); Possieri and Sassano (2015); Basar (1976); Lukes and Russell (1971).
Like LQR for the classical single-agent setting, LQ games are an appealing setting in which to analyze the behavior of multi-agent reinforcement learning algorithms in continuous action and state spaces since they admit global Nash equilibria in the space of linear feedback policies. Moreover, these equilibria can be found by solving a coupled set of Ricatti equations. As such, LQ games are a natural benchmark problem on which to test policy-gradient algorithms in multi-agent settings. Furthermore, policy gradient methods open up the possibility to new scalable approaches to ﬁnding solutions to control problems even with constraints. In the single-agent setting, it was recently shown that policy-gradient has global convergence guarantees for the LQR problem (Fazel et al., 2018). These results have recently been extended to projected policy-gradient algorithms in zerosum LQ games (Zhang et al., 2019).
Contributions. We present a negative result, showing that policy-gradient in general-sum LQ games does not enjoy even local convergence guarantees, unlike in LQR and zero-sum LQ games. In particular, we show that, if each player randomly initializes their policy and then uses a policygradient algorithm, there exists an LQ game in which the players would almost surely avoid a Nash equilibrium. Further, our numerical experiments indicate that LQ games in which this occurs may be quite common. We also observe empirically that when players fail to converge to the Nash equilibrium they do converge to stable limit cycles. These cycles do not seem to have any readily apparent relationship to the Nash equilibria of the game.
We note that non-convergence to Nash equilibria is not in itself a new phenomenon (see e.g. Mazumdar et al. (2019); Daskalakis et al. (2017); Cesa-Bianchi and Lugosi (2006)) and that the existence of cycles in the dynamics of learning dynamics in games has also been repeatedly observed in various contexts Mazumdar et al. (2018); Mertikopoulos et al. (2018); Papadimitriou and Piliouras. However, we believe that such phenomena have not yet been shown to occur in the dynamics of multi-agent reinforcement learning algorithms in continuous action and state spaces. Since such algorithms have had such striking successes in recent years, we believe a theoretical
2

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

understanding of their behaviors can lay the groundwork for the development of more efﬁcient and theoretically sound multi-agent learning algorithms.
Organization. Section 2 introduces N -player general-sum LQ games and presents previous results on the existence of the Nash equilibrium in such games. In Section 3, we show that these games are not convex games and that all the stationary points of the joint policy-gradient dynamics are Nash equilibria. Following this, we give sufﬁcient conditions under which policy-gradient almost surely avoids a Nash equilibrium in Section 4. Given these theoretical results, in Section 5 we present empirical results demonstrating that a large number of 2-player LQ games satisfy these sufﬁcient conditions. Numerical experiments showing the existence of limit cycles in the gradient dynamics of general-sum LQ games are also presented. The paper is concluded with a discussion in Section 6.

2. Preliminaries

We consider N -player LQ games subject to a discrete-time dynamical system deﬁned by

z(t + 1) = Az(t) +

N i=1

Bi

ui

(t)

z(0) = z0 ∼ Do,

(1)

where z(t) ∈ Rm is the state at time t, Do is the initial state distribution, and ui(t) ∈ Rdi is the

control input of player i ∈ 1, . . . , N . For LQ games, it is known that under reasonable assumptions,

linear feedback policies for each player that constitute a Nash equilibrium exist and are unique if

a set of coupled Ricatti equations admit a unique solution (Basar and Olsder, 1998). Thus, we

consider that each player i searches for a linear feedback policy of the form ui(t) = −Kiz(t) that

minimizes their loss, where Ki ∈ Rdi×m. We use the notation d =

N i=1

di

for

the

combined

dimension of the players’ parameterized policies.

As the name of the game implies, the players’ loss functions are quadratic functions given by

fi(u1, . . . , uN ) = Ez0∼Do

∞ t=0

z

(t)T

Qi

z

(t)

+

ui

(t)T

Ri

ui

(t)

,

where Qi and Ri are the cost matrices for the state and input, respectively.

Assumption 1 For each player i ∈ {1, . . . , N }, the state and control cost matrices satisfy Qi 0 and Ri 0.

We note that the players are coupled through the dynamics since z(t) is constrained to obey the update equation given in (1). We focus on a setting in which all players randomly initialize their strategy and then perform gradient descent simultaneously on their own cost functions with respect to their individual control inputs. That is, the players use policy-gradient algorithms of the following form:

Ki,n+1 = Ki,n − γiDifi(K1,n, . . . , KN,n)

(2)

where Difi(·, ·) denotes the derivatives of fi with respect to the i–th argument, and {γi}Ni=1 are the step-sizes of the players. We note that there is a slight abuse of notation here in the expression of Difi as functions of the parameters Ki as opposed to the control inputs ui. To ensure there is no confusion between t and n, we also point out that n indexes the policy-gradient algorithm iterations while t indexes the time of the dynamical system.

3

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY

To simplify notation, deﬁne

ΣK = Ez0∼Do

∞ t=0

z

(t)z

(t)T

,

where we use the subscript notation to denote the dependence on the collection of controllers K = (K1, . . . , KN ). Deﬁne also the initial state covariance matrix

Σ0 = Ez0∼D0 [z0z0T ].

Direct computation veriﬁes that for player i, Difi is given by:

Difi(K1, . . . , KN ) = 2(RiKi − BiT PiA¯)ΣK ,

(3)

where A¯ = A −

N i=1

BiKi,

is

the

closed–loop

dynamics

given

all

players’

control

inputs

and,

for

given (K1, . . . , KN ), the matrix Pi is the unique positive deﬁnite solution to the Bellman equation:

Pi = A¯T PiA¯ + KiT RiKi + Qi, i ∈ {1, . . . , N }.

(4)

Given that the players may have different control objectives and do not engage in coordination or cooperation, the best they can hope to achieve is a Nash equilibrium.
Deﬁnition 1 A feedback Nash equilibrium is a collection of policies (K1∗, . . . , KN∗ ) such that: fi(K1∗, . . . , Ki∗, . . . , KN∗ ) ≤ fi(K1∗, . . . , Ki, . . . , KN∗ ), ∀ Ki ∈ Rdi×m.

for each i ∈ {1, . . . , N }.

Under suitable assumptions on the cost matrices, the Nash equilibrium of an LQ game is known to exist in the space of linear policies Basar and Olsder (1998); Li and Gajic (1995). However, this Nash equilibrium may not be unique. To the best of our knowledge, there are no general set of conditions under which the Nash equilibrium is unique in general-sum LQ games outside of the scalar dynamics setting Engwerda (1998). There are, however, algebraic geometry methods to compute all Nash equilibria in LQ games Possieri and Sassano (2015). We make use of a simpler algorithm to ﬁnd Nash equilibria which solves coupled Ricatti equations using the method of Lyapunov iterations. The method is outlined in Li and Gajic (1995) for continuous time LQ games, and an analogous procedure can be followed for discrete time. Convergence of this method requires the following assumption.

Assumption 2 For at least one player i ∈ {1, . . . , N }, (A, Bi) is stabilizable.

Assumption 2 is a necessary condition for the players to be able to stabilize the system. Indeed, the player’s costs are ﬁnite only if the closed loop system A¯ is asymptotically stable, meaning that |Re(λ)| < 1 for all λ ∈ spec(A¯), where Re(λ) denotes the real part of λ and spec(M ) is the
spectrum of a matrix M .

4

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

3. Analyzing the Optimization Landscape of LQ Games

Having introduced the class of games we consider we now analyze the optimization landscape in

general-sum LQ games. Letting x = (K1, . . . , KN ), the object of interest is the map ω : Rmd →

Rmd deﬁned as follows:

 D1f1(K1, . . . , KN ) 

ω(x) =  ...  .

DN fN (K1, . . . , KN )

Note that Difi = ∂fi/∂Ki has been converted to an mdi dimensional vector and each Ki has also been vectorized. This is a slight abuse of notation and throughout we treat the Ki’s as both vectors and matrices; in general, the shape should be clear from context, and otherwise we make comments where necessary to clarify.
Before analyzing the stationary points of policy-gradient in LQ games, we show that the class of LQ games we consider are not convex games. This holds despite the linearity of the dynamics and the positive deﬁniteness of the cost matrices. This fact makes the analysis of such games non-trivial since the lack of strong structural guarantees on the players’ costs allows for non-trivial limiting behaviors like cycles, non-Nash equilibria, and chaos in the joint gradient dynamics. Mazumdar et al. (2018).

Proposition 2 There exists a N -player LQ game satisfying assumptions 1 and 2 that is not a convex game.

Proof The proof of Proposition 2 follows directly from the non-convexity of the set of stabilizing policies for the single-agent LQR problem which was shown in Fazel et al. (2018). Holding every other players’ actions ﬁxed, a player i is faced with a simple LQR problem. Since this problem is non-convex, LQ games are not convex games.

In the absence of strong structural guarantees on the players’ costs, simultaneous gradient-play in general-sum games can converge to strategies that are not Nash equilibria (Mazumdar et al., 2018). The following theorem shows that, despite the fact that LQ games are not convex for each player, such non-Nash equilibria cannot exist in the gradient dynamics of general-sum LQ games. Indeed, we show that a point x is a critical point of the policy gradient dynamics in a N -player LQ game if and only if it is a Nash equilibrium. We note that critical points of gradient-play are strategies x = (K1, . . . , KN ) such that ω(x) = 0. Such points are of particular importance since a necessary condition for a point x to be a Nash equilibrium is that it is a critical point.

Theorem 3

Consider the set of stabilizing policies x∗

=

(

K

∗ 1

,

.

.

.

,

K

∗ N

)

such

that

ΣK∗

>

0.

Difi(K1∗, . . . , KN∗ ) = 0 for each i ∈ {1, . . . , N }, if and only if x∗ is a Nash equilibrium.

Proof We prove the forward direction and show that if Difi(x∗) = 0 for each i ∈ {1, . . . , N }, then

x∗ is a Nash equilibrium. We show this by contradiction. Suppose the claim does not hold so that

ΣK∗

>

0

and

D

i

fi

(K

∗ 1

,

.

.

.

,

K

∗ N

)

=

0

for

each

i

∈

{1, . . . , N },

yet

(K1∗

,

.

.

.

,

K

∗ N

)

is

not

a

Nash

equilibrium. That is, without loss of generality, there exists a K¯1 such that

f1(K¯1, K2∗, . . . , KN∗ ) < f1(K1∗, . . . , KN∗ ).

5

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY

Now,

ﬁxing

(K

∗ 2

,

.

.

.

,

K

∗ N

)

,

player

1

can

be

seen

as

facing

an

LQR

problem.

Indeed, letting

(K

∗ 2

,

.

.

.

,

K

∗ N

)

be

ﬁxed,

player

1

aims

to

ﬁnd

a

‘best

response’

in

the

space

of

linear

feedback

policies

of

the

form

u1(t)

=

K z (t)

with

K

∈

Rdi×m

that

minimizes

f1

(·,

K2∗

,

.

.

.

,

K

∗ N

)

subject

to

the dynamics deﬁned by

z(t + 1) = A −

N i=2

BiKi

z(t) + B1u1(t).

Note that this system is necessarily stabilizable since A¯ is stable. Hence, the discrete algebraic

Riccati equation for player 1’s LQR problem has a positive deﬁnite solution P such that R1 +

B

T 1

P

B1

>

0

since

R1

>

0

by

assumption.

Since

ΣK∗

>

0

and

D

1

f1

(

K1∗

,

.

.

.

,

K

∗ N

)

=

0,

applying

Corollary 4 of Fazel et al. (2018), we have that K1∗ must be optimal for player 1’s LQR problem so

that

f1(K1∗, . . . , KN∗ ) ≤ f1(K, K2∗, . . . , KN∗ ), ∀ K ∈ Rd1×m.

In particular, the above inequality holds for K¯1, which leads to a contradiction.

To prove the reverse direction, we note that a necessary condition for a point x to be a Nash equilibrium for each player, is that Difi(x∗) = 0 for each i ∈ {1, . . . , N } Ratliff et al. (2013).

Theorem 3 shows that, just as in the single-player LQR setting and zero-sum LQ games, the critical points of gradient-play in N –player general-sum LQ games are all Nash equilibria. We note that the condition ΣK > 0 can be satisﬁed by choosing an initial state distribution Do with a full-rank covariance matrix.
A simple consequence of Theorem 3 is that when the coupled Ricatti equations characterizing the Nash equilibria of the game have a unique positive deﬁnite solution and Assumptions 1 and 2 hold, the gradient dynamics admit a unique critical point.
Corollary 4 Under Assumption 1 and 2, if the coupled Ricatti equations admit a unique solution and Σ0 0, then the map ω has a unique critical point.
Given that the critical points of the gradient dynamics in LQ games are Nash equilibria, the aim is to show, via constructing counter-examples, that games in which the gradient dynamics avoid the Nash equilibria do in fact exist. A sufﬁcient condition for this would be to ﬁnd a game in which gradient-play diverges from neighborhoods of Nash equilibria.
It is demonstrated in Mazumdar et al. (2018) that there may be Nash equilibria that are not even locally attracting under the gradient dynamics in N –player general-sum games in which the players’ costs are sufﬁciently smooth (i.e., at least twice continuously differentiable). In games that admit such Nash equilibria, the agents could initialize arbitrarily close to the Nash equilibrium, simultaneously perform individual gradient descent with arbitrarily small step sizes, and still diverge.
The class of N –player LQ games we consider does not, however, satisfy the smoothness assumptions necessary to simply invoke the results in Mazumdar et al. (2018). Indeed, the cost functions are non-smooth and, in fact, are inﬁnite whenever the players have strategies that do not stabilize the dynamics. Further, the set of stabilizing policies for a dynamical system is not even convex (Fazel et al., 2018). Despite these challenges, in the sequel we show that the negative convergence results in Mazumdar et al. (2018) extend to the general-sum LQ setting. In particular, we show that even with arbitrarily small step sizes, players using policy-gradient in LQ games may still diverge from neighborhoods of a Nash equilibrium.

6

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

4. Sufﬁcient Conditions for Policy-Gradient to Avoids Nash

We now give sufﬁcient conditions under which gradient-play has no guarantees of even local, much less global, convergence to a Nash equilibrium. Towards this end, we ﬁrst show that ω is sufﬁciently
smooth on the set of stabilizing policies. Let Smd ⊂ Rmd be the subset of stabilizing md–dimensional matrices.

Proposition 5 Consider an N –player LQ game. The vector-valued map ω associated with the game is twice continuously differentiable on Smd—i.e., ω ∈ C2(Smd, Smd).

Using our notation, Lemma 6.5 in Zhang et al. (2019) shows for two-player zero-sum LQ games

that (P1, P2), and ΣK are continuously differentiable with respect to K1 and K2 when A − B1K1 −

B2K2 is stable. This, in turn, implies that ω(K1, K2) is continuously differentiable with respect

to K1 and K2 when the closed loop system A − B1K1 − B2K2 is stable. The result follows by a

straightforward application of the implicit function theorem (Abraham et al., 1988). We utilize the

same proof technique here in extending the result to N –player general-sum LQ games and, in fact,

the proof implies that ω has even stronger regularity properties. Since the proof follows the same

techniques as in Zhang et al. (2019), we defer it to Appendix A.

Given that ω is continuously differentiable over the set of stabilizing joint policies (K1, . . . , KN ),

the following result gives sufﬁcient conditions such that the set of initial conditions in a neighbor-

hood of the Nash equilibrium from which gradient-play converges to the Nash equilibrium is of

measure zero. This implies that the players will almost surely avoid the Nash equilibrium even if

they randomly initialize in a uniformly small ball around it.

Let the Jacobian of the vector ﬁeld ω be denoted by Dω. Given a critical point x∗, let λj be

the eigenvalues of Dω(x∗), for j ∈ {1, . . . , md}, where d =

n i=1

di.

Recall

that

the

state

z(t)

is

dimension m.

Theorem 6 Suppose that Σ0 > 0. Consider any N –player LQ game satisfying Assumptions 1
and 2 that admits a Nash equilibrium that is a saddle point of the policy-gradient dynamics—i.e., LQ games for which the Jacobian of ω evaluated at the Nash equilibrium x∗ = (K1∗, . . . , KN∗ ) has eigenvalues λj such that Re(λj) < 0 for j ∈ {1, . . . , } and Re(λj) > 0 for j ∈ { +1, . . . , md} for some such that 0 < < md. Then there exists a neighborhood U of x∗ such that policy-gradient
converges on a set of measure zero.

Proof The proof is made up of three parts: (i) we show the existence of an open-convex neighborhood U of x∗ on which ω is locally Lipschitz with constant L; (ii) we show that the map g(x) = x − Γω(x) is a diffeomorphism on U ; and, (iii) we invoke the stable manifold theorem to show that the set of initializations in U on which policy-gradient converges is measure zero.
(i) ω is locally Lipschitz. Proposition 5 shows that ω is continuously differentiable on the set of stabilizing policies Smd. Given Assumptions 1 and 2, the Nash equilibrium exists and x∗ ∈ Smd. Thus, there must exist an open convex neighborhood U of x∗ such that ||Dω||2 < L for some L > 0.
(ii) g is a diffeomorphism. By the preceding argument, ω is locally Lipschitz on U with Lipschitz constant L. Consider the policy-gradient algorithm with γi < 1/L for each i ∈ {1, . . . , N }. Let Γ = diag(Γ1, . . . , ΓN ) where Γi = diag((γi)mj=d1i)—that is, Γi is an mdi × mdi diagonal matrix with γi repeated on the diagonal mdi times. Now, we claim the mapping g : Rmd → Rmd :

7

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY
x → x − Γω(x) is a diffeomorphism on U . If we can show that g is invertible on U and a local diffeomorphism, then the claim follows. Let us ﬁrst prove that g is invertible.
Consider x = y and suppose g(y) = g(x) so that y − x = γ · (ω(y) − ω(x)). Since ω(y) − ω(x) 2 ≤ L y −x 2 on U , x−y 2 ≤ L Γ 2 y −x 2 < y −x 2 since Γ 2 = maxi |γi| < 1/L.
Now, observe that Dg = I − ΓDω(x). If Dg is invertible, then the implicit function theorem (Abraham et al., 1988) implies that g is a local diffeomorphism. Hence, it sufﬁces to show that ΓDω(x) does not have an eigenvalue equal to one. Indeed, letting ρ(A) be the spectral radius of a matrix A, we know in general that ρ(A) ≤ A for any square matrix A and induced operator norm
· so that ρ(ΓDω(x)) ≤ ΓDω(x) 2 ≤ Γ 2 supx∈U Dω(x) 2 < maxi |γi|L < 1. Of course, the spectral radius is the maximum absolute value of the eigenvalues, so that the above implies that all eigenvalues of ΓDω(x)) have absolute value less than one.
Since g is injective by the preceding argument, its inverse is well-deﬁned and since g is a local diffeomorphism on U , it follows that g−1 is smooth on U . Thus, g is a diffeomorphism.
(iii) Local convergence occurs on a set of measure zero. Let B be the open ball derived from Theorem 9 in Appendix B.
Starting from x0 ∈ U , if gradient-based learning converges to a strict saddle point, then there exists an n0 such that gn(x0) ∈ B for all n ≥ n0. Applying Theorem 9 (Appendix B), we get that gn(x0) ∈ Wlcosc ∩ B. Now, using the fact that g is invertible, we can iteratively construct the sequence of sets deﬁned by W1(x∗) = g−1(Wlcosc ∩ B) ∩ U and Wk+1(x∗) = g−1(Wk(x∗) ∩ B) ∩ U . Then we have that x0 ∈ Wn(x∗) for all n ≥ n0. The set U0 = ∪∞ k=1Wk(x∗) contains all the initial points in U such that gradient-based learning converges to a strict saddle.
Since x∗ is a strict saddle, I − ΓDω(x∗) has an eigenvalue greater than one. This implies that the co-dimension of the unstable manifold is strictly less than md so that dim(Wlcosc) < md. Hence, Wlcosc ∩ B has Lebesgue measure zero in Rmd. Using again that g is a diffeomorphism, g−1 ∈ C1 so that it is locally Lipschitz and locally Lipschitz maps are null-set preserving. Hence, Wk(x∗) has measure zero for all k by induction so that U0 is a measure-zero set since it is a countable union of measure-zero sets.
Theorem 6 gives sufﬁcient conditions under which, with random initializations of Ki, policygradient methods would almost surely avoid the critical point. Let each players’ initial strategy Ki,0 be sampled from a distribution pi,0 for i ∈ {1, ..., N } , and let p0 be the resulting the joint distribution of (K1,0, . . . , KN,0).
Corollary 7 Suppose Do is chosen such that Σ0 0, and consider an N –player LQ game satisfying Assumptions 1 and 2 in which there is a Nash equilibrium which is a saddle point of the policy-gradient dynamics. If each player i ∈ {1, . . . , N } performs policy-gradient with a random initial strategy Ki,0 ∼ pi,0 such that the support of p0 is U , they will almost surely avoid the Nash equilibrium.
Corollary 7 shows that even if the players randomly initialize in a neighborhood of a Nash equilibrium that is a saddle point of the joint gradient dynamics they will almost surely avoid it. The proof follows trivially from the fact that the set of initializations that converge to the Nash equilibrium is of measure zero in U .
8

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

In the next section, we generate a large number of LQ games that satisfy the conditions of Corollary 7. Taken together, these theoretical and numerical results imply that policy-gradient algorithms have no guarantees of local, and consequently global, convergence in general-sum LQ games.
Remark 8 Theorem 6 gives us sufﬁcient conditions under which policy-gradient in general-sum LQ games does not even have local convergence guarantees, much less global convergence guarantees. We remark that this is very different from the single-player LQR setting, where policy-gradient will converge from any initialization in a neighborhood of the optimal solution (Fazel et al., 2018). In zero-sum LQ games, the structure of the game also precludes any Nash equilibrium from satisfying the conditions of Theorem 6 (Mazumdar et al., 2018), meaning that local convergence is always guaranteed. In Zhang et al. (2019), the guarantee of local convergence is strengthened to that of global convergence for a class of projected policy-gradient algorithms in zero-sum LQ games.

5. Generating Counterexamples
Since it is difﬁcult to ﬁnd a simple closed form for the Jacobian of ω due to the fact that the matrices Pi implicitly depend on all the Ki, we perform random search to ﬁnd instances of LQ games in which the Nash equilibrium is a strict saddle point of the gradient dynamics. For each LQ game we generate, we use the method of Lyapunov iterations to ﬁnd a global Nash equilibrium of the LQ game and numerically approximate the Jacobian to machine precision. We then check whether the Nash equilibrium is a strict saddle. Surprisingly, such a simple search procedure ﬁnds a large number of LQ games in which policy-gradient avoids Nash equilibria.
For simplicity, we focus on two-player LQ games where z ∈ R2 and d1 = d2 = 1. Thus, each player i = 1, 2 has two parameters to learn, which we denote Ki,j, j = 1, 2.
In the remainder of this section, we detail our experimental setup and then present our ﬁndings.

5.1 Experimental setup

To search for examples of LQ games in which policy-gradient avoids Nash equilibria, we ﬁx B1,
Q1, and R1 and parametrize B2, Q2, and R2 by b, q, and r, respectively. For various values of the parameters b, q, and r, we uniformly sample 1000 different dynamics matrices A ∈ R2×2 such
that A, B1, Q1 satisﬁes Assumption 2. Then, for each of the 1000 different LQ games we ﬁnd the optimal feedback matrices (K1∗, K2∗) using the method of Lyapunov iterations (i.e., a discrete time variant of the algorithm outlined in Li and Gajic (1995)), and then numerically approximate Dω(K1∗, K2∗) using auto-differentiation1 tools and check its eigenvalues.
The exact values of the matrices are deﬁned as follows:

A ∈ R2×2 : ai,j ∼ Uniform(0, 1) i, j = 1, 2,

1

b

0.01 0

10

B1 = 1 , B2 = 1 , Q1 = 0 1 , Q2 = 0 q , R1 = 0.01, R2 = r.

5.2 Numerical results
Using the setup outlined in the previous section we randomly generated LQ games to search for counterexamples. We ﬁrst present results that show that these counterexamples may be quite com-
1. We use auto-differentiation due to the fact that ﬁnding an analytical expression for Dω is unduly arduous even in low dimensions due to the dependence of Pi and ΣK1,K2 on (K1, K2), both of which are implicitly deﬁned.

9

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY
mon. We then use policy-gradient in two of the LQ games we generated and highlight the existence of limit cycles and the fact that the players’ time-averaged strategies do not converge to the Nash equilibrium.
Figure 1: Frequency (out of 1000) of randomly sampled LQ games with global Nash equilibria that are avoided by policy-gradient. Each point represents, for the given parameter value, the frequency of such games out of 1000 randomly sampled A matrices. The solid line shows the average frequency of these games. (i) r is varied in (0, 1), b = 0, q = 0.01. (ii) q is varied in (0, 1), b = 0, r = 0.1. (iii) b is varied in (−0.5, 0.5), q = 0.01, r = 0.1.
Avoidance of Nash in a nontrivial class of LQ games. As can be seen in Figure 1, across the different parameter values we considered, we found that anywhere from 0% to 25% of randomly sampled LQ games, had Nash equilibria that are strict saddle points of the gradient dynamics. Therefore, in up to 25% of the LQ games we generated policy-gradient would almost surely avoid a Nash solution. Of particular interest, for all values of q and r that we tested, when b = 0 at least 5% of the LQ games had a global Nash equilibrium with the strict saddle property.
These empirical observations imply that policy-gradient in competitive settings, even in the relatively straightforward setting of linear dynamics, linear policies, and quadratic costs, could fail to converge to a Nash equilibrium in up to one out of four such problems. This suggests that for more complicated cost functions, policy classes, and dynamics, Nash equilibria may often be avoided by policy-gradient.
We remark that each point in Figure 1 represents the number of counterexamples found (out of 1000) for each parameter value, meaning that for r ≈ 0.35, b = 0, and q = 0.01 we were able to consistently generate around 250 different examples of games where policy-gradient almost surely avoids the only stationary point of the dynamics.
Note also that we were unable to ﬁnd any counterexamples when b was varied in (−0.5, 0.5) and q = 0.01, r = 0.1. This suggests that depending on the structure of the dynamical system it may be possible to give stronger convergence guarantees. Convergence to Cycles. Figures 2–3 show the payoffs and parameter values of the two players when they use policy-gradient in two general-sum LQ games we identiﬁed as being counterexamples for convergence to the Nash equilibrium.
10

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

Figure 2: Payoffs of the two players in two general-sum LQ game where there is a Nash equilibrium that is avoided by the gradient dynamics. We observe empirically that in both games the two players diverge from the local Nash equilibrium and converge to a limit cycle around the Nash equilibrium.

In the two games, we initialize both players in a ball of radius 0.25 around their Nash equilibrium strategies and let them perform policy-gradient with step size 0.05. We observe that in both games the players diverge from the Nash equilibrium and converge to limit cycles.
For the two games in Figures 2–4, the game parameters are such that b = 0, r = 0.01, and q = 0.147. The two A matrices are deﬁned as follows:

(i): A = 0.588 0.028 , (ii): A = 0.511 0.064 . (5)

0.570 0.056

0.533 0.993

We also chose the initial state distribution to be [1, 1]T or [1, 1.1]T with probability 0.5 each. The eigenvalues of the corresponding game Jacobian Dω evaluated at the Nash equilibrium are as follows:

(i): spec(Dω(K1∗, K2∗)) = {10.88, 2.02, −0.21, −0.06} (ii): spec(Dω(K1∗, K2∗)) = {9.76, 0.54, −0.01 + 0.08j, −0.01 − 0.08j}.

Thus, these games do satisfy the conditions of Corollary 7 for the avoidance of Nash equilibria. We conclude this section by noting that, as shown in Figure 4, the players’ average payoffs do not necessarily converge to the Nash equilibrium payoffs.

6. Discussion
We have shown that in the relatively straightforward setting of N –player LQ games, agents performing policy-gradient have no guarantees of local, and therefore global, convergence to the Nash equilibria of the game even if they randomly initialize their ﬁrst policies in a small neighborhood of the Nash equilibrium. Since we also showed that the Nash equilibria are the only critical points of the gradient dynamics, this means that, for this class of games, policy-gradient algorithms may have no guarantees of convergence to any set of stationary policies.

11

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY
Figure 3: Parameter values of the two players in two general-sum LQ game where the Nash equilibrium is avoided by the gradient dynamics. We empirically observe in both games described in (5) that players converge to the same cycle from different initializations. Time is shown by the progressive darkening of the players’ strategies.
Figure 4: Time average parameter values of the two players in the general-sum LQ game with dynamics given in (5). We empirically observe that in both games the players’ time average strategy does not converge to the Nash equilibrium strategy. Time is shown by progressive darkening of the players’ strategies.
Since linear dynamics, quadratic costs, and linear policies are a relatively simple setup compared to many recent deep multi-agent reinforcement learning problems (Bansal et al., 2018; Jaderberg et al., 2019), we believe that the issues of non-convergence are likely to be present in more complex scenarios involving more complex dynamics and parametrizations of the policies. This can be viewed as a cautionary note, but it also suggests that the algorithms that have yielded impressive results in multi-agent settings can be further improved by leveraging the underlying game-theoretic structure.
12

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES

We remark that we only analyzed the deterministic policy-gradient setting, though the ﬁndings extend to settings in which players construct unbiased estimates of their gradients (Sutton and Barto, 2017) and even actor-critic methods (Srinivasan et al., 2018). Indeed all of these algorithms will suffer the same problems since they all seek to track the same limiting continuous-time dynamical system (Mazumdar et al., 2018).
Our numerical experiments also highlight the existence of limit cycles in the policy-gradient dynamics. Unlike in classical optimization settings in which oscillations are normally caused by the choice of step sizes, the cycles we highlight are behaviors that can occur even with arbitrarily small step sizes. They are a fundamental feature of learning in multi-agent settings and have been observed in the dynamics of many learning algorithms (Mazumdar et al., 2018; Papadimitriou and Piliouras; Hommes and Ochea, 2012; Mertikopoulos et al., 2018). We remark, however, that there is no obvious link between the limit cycles that arise in the gradient dynamics of the LQ games and the Nash equilibrium of the game. Indeed, unlike with other game dynamics in more simple games, such as the well-studied replicator dynamics in bilinear games (Mertikopoulos et al., 2018) or multiplicative weights in rock-paper-scissors (Hommes and Ochea, 2012), the time average of the players’ strategies does not coincide with the Nash equilibrium. This may be due to the fact that the Nash equilibrium is a saddle point of the gradient dynamics and not simply marginally stable, though the issue warrants further investigation.
This paper highlights how algorithms developed for classical optimization or single-agent optimal control settings may not behave as expected in multi-agent and competitive environments. Algorithms and approaches that have provable convergence guarantees and performance in competitive settings, while retaining the scalability and ease of implementation of simple policy-gradient methods, are therefore a crucial and promising open area of research.

Appendix A. Proofs of Auxiliary Results
Proposition 5 Consider an N –player LQ game. The vector-valued map ω twice continuously differentiable on Smd; i.e., ω ∈ C2(Smd, Smd).

Proof Following the proof technique of Zhang et al. (2019), we show the regularity of ω us-

ing the implicit function theorem (Abraham et al., 1988). In particular, we show that ΣK =

Ez0∼Do

∞ t=0

z

(t)z

(t)T

and Pi for i ∈ {1, . . . , N } are C1 with respect to each Ki on the space

of stabilizing matrices.

For any stabilizing (K1, . . . , KN ), ΣK is the unique solution to the following discrete-time

Lyapunov equation:

A¯ΣK A¯T + Σ0 = ΣK ,

(6)

where Σ0 = Ez0∼Do[z(0)z(0)T ] > 0 and A¯ = A −

N i=1

BiKi.

Both

sides

of

this

expression

can

be vectorized. Indeed, using the same notation as in Zhang et al. (2019), let vect(·) be the map that

vectorizes its argument and let Ψ : Rm2 × Rd1×m × · · · × RdN ×m → Rm2 be deﬁned by

Ψ(vect(ΣK ), K1, . . . , KN ) = A¯ ⊗ A¯ · vect(ΣK ) + vect(Σ0).

Then, (6) can be written as

F (vect(ΣK ), K1, . . . , KN ) = Ψ(vect(ΣK ), K1, . . . , KN ) − vect(ΣK ) = 0.

13

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY

The map F implicitly deﬁnes ΣK. Moreover, letting I denote the appropriately sized identity

matrix, we have that

∂F (vect(ΣK), K1, . . . , KN ) = A¯ ⊗ A¯ − I. ∂vectT (ΣK )

For stabilizing (K1, . . . , KN ), this matrix is an isomorphism since spec(A¯) is inside the unit circle. Thus, using the implicit function theorem, we conclude that vect(ΣK) ∈ C1. As noted in Zhang
et al. (2019), the proof for each Pi, i ∈ {1, . . . , N } is completely analogous. Since ΣK and Pi are C1 and ω is linear in these terms, the result of the proposition follows.

Appendix B. Additional Mathematical Preliminaries and Results
The following theorem is the celebrated center manifold theorem from geometry. We utilize it in showing avoidance of saddle point equilibria of the dynamics.
Theorem 9 (Stable Manifold Theorem (Shub, 1978, Thm. III.7), Smale (1967)) Let x0 be a ﬁxed point for the Cr local diffeomorphism φ : U → Rn where U ⊂ Rn is an open neighborhood of x0 in Rn and r ≥ 1. Let Es ⊕ Ec ⊕ Eu be the invariant splitting of Rn into generalized eigenspaces of Dφ(x0) corresponding to eigenvalues of absolute value less than one, equal to one, and greater than one. To the Dφ(x0) invariant subspace Es ⊕ Ec there is an associated local φ–invariant Cr embedded disc Wlcosc called the local stable center manifold of dimension dim(Es ⊕ Ec) and ball B around x0 such that φ(Wlcosc) ∩ B ⊂ Wlcosc, and if φn(x) ∈ B for all n ≥ 0, then x ∈ Wlsocc.

References

R. Abraham, J. E. Marsden, and T. Ratiu. Manifolds, Tensor Analysis, and Applications. Springer, 1988.

T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch. Emergent complexity via multi-agent competition. In International Conference on Learning Representations, 2018.

T. Basar. On the uniqueness of the nash solution in linear-quadratic differential games. International Journal of Game Theory, 1976.

T. Basar and G. Olsder. Dynamic Noncooperative Game Theory. Society for Industrial and Applied Mathematics, 2 edition, 1998.

N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.

C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. arxiv:1711.00141, 2017.

Traning GANs with Optimism.

S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic regulator. ArXiv e-prints, 2017.

J. Engwerda. On scalar feedback nash equilibria in the inﬁnite horizon lq-game. IFAC Proceedings Volumes, 1998.

14

POLICY-GRADIENT ALGORITHMS IN LINEAR QUADRATIC GAMES
M. Fazel, R. Ge, S. M. Kakade, and M. Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Conference on Machine Learning, 2018.
C. H. Hommes and M. I. Ochea. Multiple equilibria and limit cycles in evolutionary games with logit dynamics. Games and Economic Behavior, 74, 2012.
M. Jaderberg, W. Czarnecki, I. Dunning, L. Marris, G. Lever, A. Garcia Castaneda, C. Beattie, N. C. Rabinowitz, A. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science, 364, 2019.
R. E. Kalman. Contributions to the theory of optimal control. Boletin de la Sociedad Matematica Mexicana, 5, 1960.
M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Perolat, D. Silver, and T. Graepel. A uniﬁed game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems 30. 2017.
T. Li and Z. Gajic. Lyapunov iterations for solving coupled algebraic Riccati equations of Nash differential games and algebraic Riccati equations of zero-sum games. In New Trends in Dynamic Games and Applications, 1995.
R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 30. 2017.
D. L. Lukes and D. L. Russell. A global theory for linear-quadratic differential games. Journal of Mathematical Analysis and Applications, 1971.
D. Malik, A. Pananjady, K. Bhatia, K. Khamaru, P. Bartlett, and M. Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. In Proceedings of Machine Learning Research, 2019.
E. Mazumdar, L. J. Ratliff, and S Sastry. On the convergence of gradient-based learning in continuous games. ArXiv e-prints, 2018.
E. Mazumdar, M. I. Jordan, and S. S. Sastry. On ﬁnding local nash equilibria (and only local nash equilibria) in zero-sum games. CoRR, 2019.
P. Mertikopoulos, C. H. Papadimitriou, and G. Piliouras. Cycles in adversarial regularized learning. In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, 2018.
C. Papadimitriou and G. Piliouras. Game dynamics as the meaning of a game. ACM SIGecom Exchanges.
C. Possieri and M. Sassano. An algebraic geometry approach for the computation of all linear feedback Nash equilibria in lq differential games. In 2015 54th IEEE Conference on Decision and Control (CDC), 2015.
15

MAZUMDAR, RATLIFF, JORDAN, AND SASTRY
L. J. Ratliff, S. A. Burden, and S. S. Sastry. Characterization and computation of local Nash equilibria in continuous games. In Proceedings of the 51st Annual Allerton Conference on Communication, Control, and Computing, pages 917–924, Oct 2013.
M. Shub. Global Stability of Dynamical Systems. Springer-Verlag, 1978. S. Smale. Differentiable dynamical systems. Bull. Amer. Math. Soc., 73, 1967. S. Srinivasan, M. Lanctot, V. Zambaldi, J. Perolat, K. Tuyls, R. Munos, and M. Bowling. Actor-
critic policy optimization in partially observable multiagent environments. In Advances in Neural Information Processing Systems 31. 2018. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 2017. O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wunsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, Oct 2019. K. Zhang, Z. Yang, and T. Basar. Policy optimization provably converges to Nash equilibria in zero-sum linear quadratic games, 2019.
16

