End-to-End Far-Field Speech Recognition with Uniﬁed Dereverberation and Beamforming
Wangyou Zhang1, Aswin Shanmugam Subramanian2, Xuankai Chang2, Shinji Watanabe2, Yanmin Qian1
1MoE Key Lab of Artiﬁcial Intelligence & SpeechLab, Department of Computer Science and Engineering, AI Institute, Shanghai Jiao Tong University, Shanghai
2Center for Language and Speech Processing, Johns Hopkins University, USA
wyz-97@sjtu.edu.cn, {aswin, xchang14, shinjiw}@jhu.edu, yanminqian@sjtu.edu.cn

arXiv:2005.10479v2 [eess.AS] 27 Oct 2020

Abstract
Despite successful applications of end-to-end approaches in multi-channel speech recognition, the performance still degrades severely when the speech is corrupted by reverberation. In this paper, we integrate the dereverberation module into the end-to-end multi-channel speech recognition system and explore two different frontend architectures. First, a multisource mask-based weighted prediction error (WPE) module is incorporated in the frontend for dereverberation. Second, another novel frontend architecture is proposed, which extends the weighted power minimization distortionless response (WPD) convolutional beamformer to perform simultaneous separation and dereverberation. We derive a new formulation from the original WPD, which can handle multi-source input, and replace eigenvalue decomposition with the matrix inverse operation to make the back-propagation algorithm more stable. The above two architectures are optimized in a fully end-toend manner, only using the speech recognition criterion. Experiments on both spatialized wsj1-2mix corpus and REVERB show that our proposed model outperformed the conventional methods in reverberant scenarios. Index Terms: Dereverberation, speech separation, overlapped speech recognition, neural beamforming, WPD
1. Introduction
Over the past few years, thanks to the advances in deep learning, signiﬁcant progress has been made in automatic speech recognition (ASR). Both deep neural network (DNN)/hidden Markov model (HMM) hybrid systems and end-to-end (E2E) systems have attained surprisingly good performance in close-talk scenarios [1–4]. However, it is still a challenging task to recognize speech signals in far-ﬁeld scenarios, where background noise and reverberation are commonly observed and even interfering speech from other speakers is involved [5, 6]. In recent years, many studies have been focusing on the far-ﬁeld speech recognition task, including the combination of the speech enhancement frontend and ASR backend [7, 8] and noise robust adaptation approaches [9, 10]. Meanwhile, it is commonly observed that speech processing with multiple microphones usually outperforms the single-microphone one, because additional spatial information can be exploited. Therefore, many existing microphone array signal processing methods can be utilized as the frontend for end-to-end far-ﬁeld speech recognition, such as the multi-channel Weiner ﬁlter [11, 12], minimum variance distortionless response (MVDR) and minimum power distortionless
†Shinji Watanabe and Yanmin Qian are the corresponding authors.

response (MPDR) beamforming [12, 13], multi-frame beamforming [14], etc. In addition, reverberation is also an important problem in real scenarios, which can lead to dramatic degradation in the ASR performance [15]. Various deep learning based methods have been proposed for dereverberation, including DNN based approaches [16–18] incorporating the weighted prediction error (WPE) algorithm [19,20] and complex ideal ratio mask based approach for denoising and dereverberation [21].
In this work, we propose a novel E2E architecture that can perform dereverberation, beamforming and recognition simultaneously. Inspired by the recently developed uniﬁed convolutional beamformer for simultaneous denoising and dereverberation, named weighted power minimization distortionless response (WPD) [22, 23], we reformulate WPD by replacing eigenvalue decomposition with an equivalent matrix inverse operation, which makes it differentiable and more stable. The new architecture consists of a frontend and an ASR backend. In the frontend, two novel architectures are explored for joint speech dereverberation, enhancement and separation. In the backend, a joint connectionist temporal classication (CTC) / attentionbased encoder-decoder model [24] is used to recognize each separated speech stream. Note that our proposed framework can be used for both single-speaker and multi-speaker scenarios. And in this paper, we mainly focus on the multi-speaker case, which is a more difﬁcult task. It is worth noting that this end-to-end architecture is optimized only based on the ﬁnal ASR criterion, which was also proven feasible in previous works [17, 25–27]. Our experiments show that our newly proposed method outperformed the conventional end-to-end ASR systems [25, 26, 28] in both single-speaker and multi-speaker reverberant conditions.

2. End-to-End Multi-Channel ASR

This section reviews the end-to-end multi-channel speech recognition system for both single-speaker (J = 1) [28] and multi-speaker (J > 1) [25, 26] conditions, as shown in Fig. 1.
Without loss of generality, we consider the input speech as a mixture of J (J ≥ 1) different speakers. For simplicity, we consider the noise as the 0-th source (j = 0) in the input signal.
The model is composed mainly of two modules, namely
the frontend for speech separation and the backend for ASR.
The frontend is a mask-based multi-source neural beamformer. First, the masking network estimates the masks Mjc for every source j ∈ {0, 1, . . . , J} on each channel c ∈ {1, . . . , C} from the input spectrum Xc = (xt,f,c)t,f ∈ CT ×F :

M = mjt,f,c

= MaskEstimator(X) ∈ CT ×F ×C×(J+1) , (1)

t,f,c,j

where mjt,f,c ∈ [0, 1], T and F represent time and frequency

J
1 , ⋯, H H Frontend

X̂ te,nfh,1 ⋯ X̂ te,nfh,J Beamforming
⋯
PSDs1 ⋯ PSDsJ PSDn
⋯

{M

1 t,

f

}Cc=

1

⋯

{M

J t,

f

}Cc=

1

⋯

{M0t, f }Cc=1

Mask Estimator

⋯
Feature Extractor
O1, ⋯, OJ
Encoder

ASR Backend
Y1 ⋯ YJ ⋯
Decoder
Permutation π̂
CTC Lctc PIT (J > 1)

Input Spectrum {Xt, f}Cc=1

Figure 1: End-to-End Multi-channel ASR Model.

dimensions respectively. Second, the multi-source neural beam-
former separates the mixture into J streams using the MVDR
formalization [12]. The estimated masks are used to compute the cross-channel power spectral density (PSD) matrices Φj [29–31] and then the time-invariant ﬁlter gfj for each speaker j:

j

1

T j

H

C×C

Φf = Tt=1 mjt,f t=1 mt,f xt,f xt,f ∈ C

, (2)

j

( i=j Φif )−1Φjf

C

gf = Trace((

Φi )−1Φj ) u ∈ C ,

(3)

i=j f

f

where xt,f

=

{xt,f,c}Cc=1, mjt,f

=

1 C

C c=1

mjt,f ,c ,

(·)H

rep-

resents the conjugate transpose, and u ∈ RC is a vector denot-

ing the reference microphone estimated by an attention mecha-
nism [32]. Finally, the separated speech Xˆ enh,j of each speaker j is derived by applying the ﬁlters gj to the input speech X,

from which the log Mel-ﬁlterbank feature with global mean and

variance normalization (GMVN-LMF(·)) is further extracted:

xˆet,nfh,j = (gfj )H xt,f ∈ C ,

(4)

Oj = GMVN-LMF(|Xˆ enh,j |) ,

(5)

where Xˆ enh,j ∈ CT ×F , and Oj is the extracted feature for ASR.

The backend is a joint CTC/attention-based encoder-

decoder model [24] for single-channel speech recognition. First, the encoder transforms the feature Oj = {oj1, . . . , ojT } of each speaker j into a high-level representation Hj = {hj1, . . . , hjL} (L ≤ T ) with subsampling. Then, the repre-

sentation is processed by the attention-based decoder to gener-

ate

the

output

token

sequences

Yj

=

{y

j 1

,

.

.

.

,

y

j N

}

.

The

ASR

process is formulated as follows:

Hj = Encoder(Oj) ,

(6)

ynj ∼ Attention-Decoder(Hj , ynj −1) ,

(7)

where ynj is the posterior probability vector for the n-th token.

Note that in the multi-speaker case, i.e. J > 1, in order to solve

the label ambiguity problem, the permutation invariant training

(PIT) technique [33–36] is further applied in the CTC module

to determine the order of the label sequences. The whole model

is optimized with only the ASR loss L combining the attention

and CTC losses with the determined label sequence order.

3. End-to-End ASR with Uniﬁed Frontend

In this section, we introduce the proposed multi-channel speech recognition architecture for coping with the reverberant speech. First, we describe the mask-based WPE model for multichannel dereverberation. Then, we show a cascade integra-

ASR backend
Mask-WPE(·) {X̂ tw,pfe}Cc=1 WPE λt, f
Time-Variant Power {mt, f}Cc=1
Mask Estimator 1

Mask-Beamformer(·)
X̂ te,nfh,1 ⋯ X̂ te,nfh,J Beamforming
⋯
PSDs1 ⋯ PSDsJ PSDn
⋯

{M

1 t,

f

}Cc=1

⋯

{M

J t,

f

}Cc=1

⋯

{M0t, f }Cc=1

Mask Estimator 2

Input Spectrum {Xt, f}Cc=1

Figure 2: Proposed end-to-end ASR arch#1: cascaded dereverberation and beamforming frontend.

tion method which incorporates the mask-based WPE model followed by the model introduced in last Section, where the WPE ﬁlter coefﬁcients are estimated for each speaker. Furthermore, another frontend architecture extending the WPD beamformer [22] is designed, which uniﬁes the dereverberation and beamforming modules with our new formulation.

3.1. Mask-based WPE model

The mask-based WPE algorithm [16] is introduced in this sub-

section. First, the input spectrum X = (xt,f )t,f is fed into

a neural network to estimate a time-frequency mask m =

(mt,f,c)t,f,c, as formulated below:

m = MaskEstimator(X)

∈

T ×F ×C
R

,

(8)

J

xt,f = xt,f + nt,f ≈

vfj sjt,f + nt,f

∈

C
C

,

(9)

j=1

where xt,f is the direct path and early reﬂection of the source signal, sjt,f is the j-th source signal, vf = [vf(0), vf(1), · · · , vf(C−1)]T ∈ CC is the steering vector, and nt,f ∈ CC is the noise and late reverberation of source signals.

With the estimated mask, the time-variant power λt,f of the

input signal can be estimated by Eq. (10), and then the signal

can be dereverberated via a standard WPE procedure:

1C λt,f = C 1
c=1 T

mt,f,c

|xt,f ,c |2

T τ =1

mτ,f,c

∈ R,

(10)

xˆwt,fpe = WPE (xt,f , λt,f ) ,

(11)

3.2. Cascaded dereverberation and beamforming
One straightforward way to enable dereverberation in the multichannel ASR system in Section 2 is the cascade integration of the mask-based WPE model and the neural beamformer like [27]. As illustrated in Fig. 2, the multi-channel input speech mixture is ﬁrst fed into the mask-based WPE model, which is composed of a mask estimator and a WPE ﬁlter. Then the dereverberated speech is processed by the beamformer introduced in Section 2 to generate the enhanced single-channel speech of J speakers for speech recognition. The frontend process can be formulated as follows:
Xˆ enh = Mask-Beamformer (Mask-WPE (X)) , (12)
where Xˆ enh = {Xˆ enh,j }Jj=1 ∈ CT ×F ×J is the set of the separated speech from all speakers, Mask-Beamformer(·) and Mask-WPE(·) denote the respective modules in Fig. 2. The

ASR backend

WPD-Beamformer(·) X̂ te,nfh,1 ⋯ X̂ te,nfh,J

WPD Beamforming

R 1f

⋯

R Jf

⋯

Power Normalized PSD

PSDs1 ⋯ PSDsJ

λt1, f

⋯

λtJ, f

⋯

Time-Variant Power

{m1t, f }Cc=1

⋯

{mJt, f }Cc=1

{M1t, f}Cc=1 ⋯ ⋯

{MJt, f }Cc=1

Mask Estimator 1

Mask Estimator 2

Input Spectrum {Xt, f}Cc=1
Figure 3: Proposed end-to-end ASR arch#2: uniﬁed dereverberation and beamforming frontend.
ASR backend here is the same as that described in Section 2.

3.3. Uniﬁed dereverberation and beamforming

The original WPD beamformer [22] aims to eliminate the late

reverberation and noise from the noisy signal, while keeping

the direct signal undistorted. It combines the ideas of WPE and

MPDR beamformer [13], and optimizes their ﬁlters at the same

time, with the constrained optimization objective below:

w¯ fH x¯t,f 2

H

(ref)

w¯ = arg min
w¯

λt,f

s.t. w0,f vf = vf (13)

t

where w¯ = [w0T,f , wDT ,f , wDT +1,f , · · · , wK T +D−1,f ]T ∈

CC(K+1) is the WPD ﬁlter coefﬁcient, x¯t,f = [xTt,f , xTt−D,f ,

xTt−D−1,f , · · · , xTt−K−D+1,f ]T ∈ CC(K+1) is the concatena-

tion of input signals of current and previous frames, D is the de-

lay parameter, K is the number of ﬁlter taps, vf(ref) is the value of the steering vector at the reference channel, and λt,f is the

power of the desired signal as in Eq. (10).

By solving the above constrained optimization problem, we

can calculate the WPD ﬁlter w¯ f and the enhanced signal Xˆ enh

by the following formulas:

R−f 1v¯f w¯ f = v¯fH R−f 1v¯f

(ref) ∗
vf

∈

C(K+1)
C

,

(14)

T x¯t−D,f x¯Ht−D,f Rf = λt,f
t=D

∈

C(K+1)×C(K+1)
C

,

(15)

Xˆ etn,fh = w¯ fH x¯t,f

∈ C,

(16)

where Rf is the power normalized covariance matrix, v¯f = [vfT , 0, · · · , 0]T ∈ CC(K+1), and (·)∗ denotes complex conjugate. While the original WPD can perform denoising and

dereverberation simultaneously with an elegant formulation, it

is only designed for speech enhancement of the single-speaker

input. In addition, the steering vector v¯ in Eq. (14) is needed

for calculating the beamformer weights, which requires the di-

rection information of the sound source or needs to be approxi-

mated by eigenvalue decomposition of a complex matrix [23].

Based on the above formulation, we ﬁrst derive another
equivalent formula that no longer requires the steering vector
v¯f , and then extend the original WPD to the multi-speaker case. Consider the padded speech signal x˜tT,f = [xtT,f , 0, · · · , 0]T ∈ CC(K+1), it is easy to derive from Eq. (9) that:

x˜t,f = v¯f st,f ,

(17)

T mt,f x˜t,f x˜tH,f

H

H

(Φx˜ x˜ )f =

T

= v¯f φf v¯f = v¯f v¯f φf , (18)

t=1 τ =1 mτ,f

vf(ref) = v¯fT u¯ ,

(19)

where (Φx˜ x˜ )f ∈ CC(K+1)×C(K+1) is the cross-channel PSD matrix of the padded speech signal x˜t,f , u¯ = [uT , 0, · · · , 0]T ∈ RC(K+1) and u ∈ RC is the reference vector

denoting the reference microphone estimated by an attention

mechanism. Substitute Eq. (17) – (19) into Eq. (14), we can

derive that:

R−f 1(Φx˜ x˜ )f

w¯ f =

u¯ .

(20)

Trace R−f 1(Φx˜ x˜ )f

This new formula is equivalent to Eq. (14), but no longer requires the steering vector for calculating the ﬁlter weights.

Furthermore, we can easily extend WPD to the multi-
speaker case. For each speaker j, the corresponding covariance matrix Rjf can be derived from Eq. (15) and (10), where the estimated mask mj for speaker j is used for both dereverbera-
tion and beamforming. Then the WPD beamformer for speaker
j can be calculated from Eq. (20). Finally, the separated speech
of each speaker can be derived from Eq. (16), using the corre-
sponding WPD beamformer. Note that the masks for calculating Rjf and Φjx˜ x˜ for each speaker j can be either shared using a single mask estimator or estimated by two separate mask es-
timators. The WPD based architecture is illustrated in Fig. 3.

4. Experiments
To make our experimental results comparable to previous results of MIMO-Speech [25, 26], we evaluated the proposed methods on the same spatialized wsj1-2mix dataset as in [25, 26], which consists of two sub-datasets: anechoic and reverberant. The reverberation time (RT60) of the reverberant data ranges from 200 ms to 600 ms. In each sub-dataset, the duration of the spatialized speech for training, development and evaluation is 98.5 hr, 1.3 hr and 0.8 hr respectively. We also adopt the multi-condition training in [25, 37], i.e. include the WSJ train si284 in training to improve the performance. We also test our methods for single-speaker speech recognition on the REVERB dataset [38], which uses 2-channel simulated reverberant data for training and 8-channel real data for evaluation.
For feature extraction, the short-time Fourier transform (STFT) is performed with a 16-kHz sampling rate and a 25-ms Hann window with a 10-ms stride, and the spectral feature’s dimension is F = 257. After the frontend processing, 80dimensional log Mel-ﬁlterbank features are extracted from the enhanced spectrum of each separated speech, where the global mean and variance normalization is applied using the statistics from the single-speaker WSJ1 training set. The number of channels for training in our experiments is C = 2. But it can be extended to an arbitrary number of channels as described in [32].
4.1. Experimental Setup
All our proposed end-to-end multi-channel speech recognition models are implemented based on the ESPnet framework [39].1 The AdaDelta optimizer with ρ = 0.95 and = 10−8 is used for training. The data in both anechoic and reverberant conditions are used for training.

1Our experimental setup is available at https://github.com/ Emrys365/espnet/blob/wsj1_mix_spatialized/egs/ wsj1_mix_spatialized/asr1/.

Table 1: Performance (WER [%]) of the proposed arch1 / arch2 models with different numbers of ﬁlter taps (K) and microphones (C) on the spatialized reverberant wsj1-2mix eval set.

C K
1 3 5 7 10

2
28.87 / 27.44 27.62 / 26.42 21.88 / 25.54 26.62 / 25.68 26.64 / 25.81

4
17.95 / 16.67 16.65 / 15.95 15.93 / 15.72 16.09 / 16.32 18.67 / 19.55

6
14.92 / 13.97 14.63 / 14.23 15.46 / 16.81 18.67 / 22.40 27.79 / 36.28

Table 2: Performance evaluation on the spatialized reverberant wsj1-2mix corpus.

Model
baseline (RNN backend) [25] + Nara-WPE [26] baseline (Transformer backend) [26] + Nara-WPE [26] proposed arch 1 proposed arch 22

dev WER (%) 34.98 24.45 32.95 19.17 19.37 18.34

eval WER (%) 29.99 17.67 28.01 15.24 14.63 13.97

In the mask-based WPE module, the mask estimation network is a 3-layer bidirectional long-short term memory with projection (BLSTMP) network with 300 cells in each direction. The number of iterations for performing mask-based WPE is set to 1. The prediction delay D and the number of taps K is set to 3 and 5 respectively. The mask estimators in both the MVDR beamformer and the WPD beamformer are 3-layer BLSTMP networks with 512 cells. Note that in all our experiments except Section 4.3, we used a shared mask estimator instead of two separate ones in Fig. 3. In the ASR module, following the conﬁgurations in [26], we use the CNN-Transformer based encoder, which consists of 2 CNN blocks and 12 Transformer layers, and the 6-layer Transformer-based decoder. The selfattention in all Transformer layers has the same conﬁguration as in [26], i.e. 4 heads and 256 dimensions. As for decoding, a word-level language model [40] trained on the ofﬁcial text data included in the WSJ corpus is used. The interpolation factor between CTC and attention losses is set to 0.2.
For experiments on REVERB, the network conﬁguration and experimental conditions are the same as [28]. The reference microphone is ﬁxed as the second channel. Both dereverberation and denoising subnetworks are trained to predict two dimensional time-frequency masks.

4.2. Evaluation of the proposed architectures for multispeaker speech recognition
Since our proposed architectures can be tested ﬂexibly with different numbers of microphones C and ﬁlter taps K, even if the model is trained with ﬁxed C and K, we ﬁrst evaluate the performance of the proposed two architectures with different numbers of ﬁlter taps and microphones for inference on the reverberant wsj1-2mix dataset. The results are presented in Table 1. We can observe that the performance can be signiﬁcantly improved when more microphones are available. The number of ﬁlter taps closer to the training setup (K = 5) usually leads to better results, but with more microphones, using fewer ﬁlter taps may also provide enough information for dereverberation and increases the stability of the operations in Eq. (20). The best performance for arch 1 and arch 2 is achieved with
2The arch 2 model in Table 2 was trained on the basis of a pretrained MIMO-Speech model, since the direct training of arch 2 currently does not work well due to numerical instability issues.

Table 3: ASR performance on REVERB evaluation real dataset comparing uniﬁed and cascade ﬁltering with K = 5 & C = 8.

Frontend WPE + MVDR proposed arch 2

Near WER (%) 10.8 8.9

Far WER (%) 13.6 11.1

C = 6, K = 3 and C = 6, K = 1 respectively. Then we compare the performances of our proposed mod-
els with the baseline models. In Table 2, the baselines are the MIMO-Speech models with RNN backend (row 1) and Transformer backend (row 3) from our previous study [25, 26]. Since these baseline models do not contain a dereverberation module, we also introduce two enhanced baselines (row 2 & 4), i.e. MIMO-Speech with iterative Nara-WPE3 preprocessing. We ran Nara-WPE with 10 ﬁlter taps for 5 iterations to preprocess both training and evaluation data for the baseline models. By comparing the four baselines and our proposed two architectures with best results taken from Table 1, we can observe that both proposed models combining neural dereverberation and beamforming in the end-to-end structure achieve comparable results to the best baseline ones. Note that our models do not need an iterative process compared to the Nara-WPE preprocessing baselines. Finally, our proposed arch 2 model based on WPD outperforms all baseline methods.

4.3. Effectiveness of uniﬁed ﬁltering for single source robust speech recognition
We also evaluated our methods in the single source condition on REVERB. We ﬁrst trained a single source multi-channel E2E ASR model, which is a variant of a cascaded architecture in Section 3.2 based on the WPE and MVDR frontend [28]. Then, we replace the cascaded frontend with the proposed uniﬁed WPD frontend, and compare both frontends. Using our WPD uniﬁed ﬁlter gives a signiﬁcant improvement in performance over the cascade conﬁguration, as shown in Table 3. This shows that using our uniﬁed frontend is also effective for single source data.
These results indicate that our proposed E2E multi-channel speech recognition model is a powerful method for applications in reverberant single-speaker and multi-speaker scenarios.
5. Conclusion
In this paper, we proposed an end-to-end multi-channel far-ﬁeld speech recognition framework with uniﬁed dereverberation and beamforming, which is capable of performing speech dereverberation, separation and recognition simultaneously. The whole model is optimized via only the ASR criterion but can still learn relatively good dereverberation and separation skills. Two novel frontend architectures are explored, and promising performance is achieved on the spatialized wsj1-2mix corpus compared to the previous MIMO-Speech model. Experimental results on REVERB dataset also demonstrate the effectiveness of our proposed WPD based architecture.
6. Acknowledgement
This work was supported by the China NSFC project No. U1736202. Experiments have been carried out on the PI supercomputers at Shanghai Jiao Tong University. We would like to thank the NTT Communication Laboratories for the use of their DNN-WPE module4 for our implementation.

3https://github.com/fgnt/nara_wpe 4https://github.com/nttcslab-sp/dnn_wpe

7. References
[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury et al., “Deep neural networks for acoustic modeling in speech recognition,” IEEE Signal processing magazine, vol. 29, no. 6, pp. 82–97, 2012.
[2] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke, “The Microsoft 2017 conversational speech recognition system,” in Proc. IEEE ICASSP, 2018, pp. 5934–5938.
[3] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., “Stateof-the-art speech recognition with sequence-to-sequence models,” in Proc. IEEE ICASSP, 2018, pp. 4774–4778.
[4] S. Karita et al., “A comparative study on transformer vs RNN in speech applications,” in Proc. IEEE ASRU, 2019, pp. 449–456.
[5] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, B. Hoffmeister, M. L. Seltzer, H. Zen, and M. Souden, “Speech processing for digital home assistants: Combining signal processing with deep-learning techniques,” IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 111–124, 2019.
[6] S. Watanabe, M. Mandel, J. Barker, and E. Vincent, “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” arXiv preprint arXiv:2004.09249, 2020.
[7] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel multi-speaker separation using deep clustering,” in Proc. ISCA Interspeech, 2016, pp. 545–549.
[8] T. Yoshioka, H. Erdogan, Z. Chen, and F. Alleva, “Multimicrophone neural speech separation for far-ﬁeld multi-talker speech recognition,” in Proc. IEEE ICASSP, 2018, pp. 5739– 5743.
[9] A. Narayanan and D. Wang, “Joint noise adaptive training for robust automatic speech recognition,” in Proc. IEEE ICASSP, 2014, pp. 2504–2508.
[10] T. Tan, Y. Qian, H. Hu, Y. Zhou, W. Ding, and K. Yu, “Adaptive very deep convolutional residual network for noise robust speech recognition,” IEEE/ACM Trans. ASLP., vol. 26, no. 8, pp. 1393– 1405, 2018.
[11] A. Spriet, M. Moonen, and J. Wouters, “Spatially pre-processed speech distortion weighted multi-channel Wiener ﬁltering for noise reduction,” Signal Processing, vol. 84, no. 12, pp. 2367– 2387, 2004.
[12] M. Souden, J. Benesty, and S. Affes, “On optimal frequencydomain multichannel linear ﬁltering for noise reduction,” IEEE Trans. Audio, Speech, Language Process., vol. 18, no. 2, pp. 260– 276, 2009.
[13] H. L. Van Trees, Optimum array processing: Part IV of detection, estimation, and modulation theory. John Wiley & Sons, 2004.
[14] Z.-Q. Wang, H. Erdogan, S. Wisdom, K. Wilson, and J. R. Hershey, “Sequential multi-frame neural beamforming for speech separation and enhancement,” arXiv preprint arXiv:1911.07953, 2019.
[15] M. Wo¨lfel and J. W. McDonough, Distant speech recognition. John Wiley & Sons, 2009.
[16] K. Kinoshita, M. Delcroix, H. Kwon, T. Mori, and T. Nakatani, “Neural network-based spectrum estimation for online WPE dereverberation.” in Proc. ISCA Interspeech, 2017, pp. 384–388.
[17] J. Heymann, L. Drude, R. Haeb-Umbach, K. Kinoshita, and T. Nakatani, “Joint optimization of neural network-based WPE dereverberation and acoustic model for robust online ASR,” in Proc. IEEE ICASSP, 2019, pp. 6655–6659.
[18] T. Taniguchi, A. S. Subramanian, X. Wang, D. Tran, Y. Fujita, and S. Watanabe, “Generalized weighted-prediction-error dereverberation with varying source priors for reverberant speech recognition,” in Proc. IEEE WASPAA, 2019, pp. 288–292.
[19] T. Yoshioka, T. Nakatani, K. Kinoshita, and M. Miyoshi, “Speech dereverberation and denoising based on time varying speech model and autoregressive reverberation model,” in Speech Processing in Modern Communication, 2010, pp. 151–182.
[20] T. Yoshioka and T. Nakatani, “Generalization of multi-channel linear prediction methods for blind mimo impulse response short-

ening,” IEEE/ACM Trans. ASLP., vol. 20, no. 10, pp. 2707–2720, 2012.
[21] D. S. Williamson and D. Wang, “Time-frequency masking in the complex domain for speech dereverberation and denoising,” IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492–1501, 2017.
[22] T. Nakatani and K. Kinoshita, “A uniﬁed convolutional beamformer for simultaneous denoising and dereverberation,” IEEE Signal Processing Letters, vol. 26, no. 6, pp. 903–907, 2019.
[23] ——, “Simultaneous denoising and dereverberation for lowlatency applications using frame-by-frame online uniﬁed convolutional beamformer,” Proc. ISCA Interspeech, pp. 111–115, 2019.
[24] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. IEEE ICASSP, 2017, pp. 4835–4839.
[25] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, “MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition,” in Proc. IEEE ASRU, 2019, pp. 237–244.
[26] ——, “End-to-end multi-speaker speech recognition with transformer,” in Proc. IEEE ICASSP, 2020, pp. 6129–6133.
[27] A. S. Subramanian, X. Wang, M. K. Baskar, S. Watanabe, T. Taniguchi, D. Tran, and Y. Fujita, “Speech enhancement using end-to-end speech recognition objectives,” in Proc. IEEE WASPAA, 2019, pp. 229–233.
[28] A. S. Subramanian, X. Wang, S. Watanabe, T. Taniguchi, D. Tran, and Y. Fujita, “An investigation of end-to-end multichannel speech recognition for reverberant and mismatch conditions,” arXiv preprint arXiv:1904.09049, 2019.
[29] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, C. Yu, W. J. Fabian, M. Espi, T. Higuchi et al., “The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices,” in Proc. IEEE ASRU, Dec. 2015, pp. 436–443.
[30] J. Heymann, L. Drude, and R. Haeb-Umbach, “Neural network based spectral mask estimation for acoustic beamforming,” in Proc. IEEE ICASSP, Mar. 2016, pp. 196–200.
[31] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, and J. Le Roux, “Improved MVDR beamforming using single-channel mask prediction networks,” Proc. ISCA Interspeech, pp. 1981– 1985, 2016.
[32] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel end-to-end speech recognition,” in Proc. ICML, 2017, pp. 2632– 2641.
[33] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. IEEE ICASSP, 2017, pp. 241–245.
[34] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Trans. ASLP., vol. 25, no. 10, pp. 1901–1913, 2017.
[35] H. Seki, T. Hori, S. Watanabe, J. Le Roux, and J. R. Hershey, “A purely end-to-end system for multi-speaker speech recognition,” in Proc. ACL, Jul. 2018, pp. 2620–2630.
[36] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, “Improving end-to-end single-channel multi-talker speech recognition,” IEEE/ACM Trans. ASLP., vol. 28, pp. 1385–1394, 2020.
[37] T. Ochiai, S. Watanabe, and S. Katagiri, “Does speech enhancement work with end-to-end ASR objectives?: Experimental analysis of multichannel end-to-end ASR,” in Proc. MLSP. IEEE, 2017, pp. 1–6.
[38] K. Kinoshita et al., “The reverb challenge: A common evaluation framework for dereverberation and recognition of reverberant speech,” in 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. IEEE, 2013, pp. 1–4.
[39] S. Watanabe et al., “ESPnet: End-to-End Speech Processing Toolkit,” in Proc. ISCA Interspeech, 2018, pp. 2207–2211.
[40] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition with word-based RNN language models,” in Proc. IEEE SLT, 2018, pp. 389–396.

