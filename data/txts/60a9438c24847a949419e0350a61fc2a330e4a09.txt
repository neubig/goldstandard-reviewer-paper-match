1

Kernel clustering: density biases and solutions

Dmitrii Marin∗

Meng Tang∗

Ismail Ben Ayed†

Yuri Boykov∗

∗Computer Science, University of Western Ontario, Canada †E´ cole de Technologie Supe´ rieure, University of Quebec, Canada dmitrii.a.marin@gmail.com mtang73@csd.uwo.ca ismail.benayed@etsmtl.ca yuri@csd.uwo.ca

!

uniform density data

arXiv:1705.05950v5 [stat.ML] 6 Dec 2017

Abstract—Kernel methods are popular in clustering due to their generality and discriminating power. However, we show that many kernel clustering criteria have density biases theoretically explaining some practically signiﬁcant artifacts empirically observed in the past. For example, we provide conditions and formally prove the density mode isolation bias in kernel K-means for a common class of kernels. We call it Breiman’s bias due to its similarity to the histogram mode isolation previously discovered by Breiman in decision tree learning with Gini impurity. We also extend our analysis to other popular kernel clustering methods, e.g. average/normalized cut or dominant sets, where density biases can take different forms. For example, splitting isolated points by cut-based criteria is essentially the sparsest subset bias, which is the opposite of the density mode bias. Our ﬁndings suggest that a principled solution for density biases in kernel clustering should directly address data inhomogeneity. We show that density equalization can be implicitly achieved using either locally adaptive weights or locally adaptive kernels. Moreover, density equalization makes many popular kernel clustering objectives equivalent. Our synthetic and real data experiments illustrate density biases and proposed solutions. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications across the disciplines.
1 INTRODUCTION
In machine learning, kernel clustering is a well established data analysis technique [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] that can identify non-linearly separable structures, see Figure 1(a-b). Section 1.1 reviews the kernel K-means and related clustering objectives, some of which have theoretically explained biases, see Section 1.2. In particular, Section 1.2.2 describes the discrete Gini clustering criterion standard in decision tree learning where Breiman [11] proved a bias to histogram mode isolation.
Empirically, it is well known that kernel K-means or average association (see Section 1.1.1) has a bias to so-called “tight” clusters for small bandwidths [3]. Figure 1(c) demonstrates this bias on a non-uniform modiﬁcation of a typical toy example for kernel K-means with common Gaussian kernel
x−y 2 k(x, y) ∝ exp − 2σ2 . (1)
This paper shows in Section 2 that under certain conditions kernel K-means approximates the continuous generalization of the Gini criterion where we formally prove a mode isolation bias similar to the discrete case analyzed by Breiman. Thus, we refer to the “tight” clusters in kernel K-means as Breiman’s bias.
We propose a density equalization principle directly addressing the cause of Breiman’s bias. First, Section 3 discusses modiﬁcation of the density with adaptive point weights. Then, Section 4 shows that a general class of locally adaptive geodesic kernels [10]

(a) K-means

(b) kernel K-means

non-uniform data

(c) kernel K-means
(Breiman’s bias, mode isolation)

(d) kernel clustering
(adaptive weights or kernels)

Fig. 1: Kernel K-means with Gaussian kernel (1) gives desirable nonlinear separation for uniform density clusters (a,b). But, for non-uniform clusters in (c) it either isolates a small dense “clump” for smaller σ due to Breiman’s bias (Section 2) or gives results like (a) for larger σ. No ﬁxed σ yields solution (d) given by locally adaptive kernels or weights eliminating the bias (Sections 4 & 3).

implicitly transforms data and modiﬁes its density. We derive “density laws” relating adaptive weights and kernels to density transformations. They allow to implement density equalization resolving Breiman’s bias, see Figure 1(d). One popular heuristic [12] approximates a special case of our Riemannian kernels.
Besides mode isolation, kernel clustering may have the opposite density bias, e.g. sparse subsets in Normalized Cut [3], see Figure 9(a). Section 5 presents “normalization” as implicit density inversion establishing a formal relation between sparse subsets and Breiman’s bias. Equalization addresses any density biases. Interestingly, density equalization makes many standard kernel clustering criteria conceptually equivalent, see Section 6.

1.1 Kernel K-means

A popular data clustering technique, kernel K-means [1] is a

generalization of the basic K-means method. Assuming Ω denotes

a

ﬁnite

set

of

points

and

fp

∈

N
R

is

a

feature

(vector)

for

point

p,

the basic K-means minimizes the sum of squared errors within

clusters, that is, distances from points fp in each cluster Sk ⊂ Ω

to the cluster means mk

k-means criterion

fp − mk 2.

(2)

k p∈Sk

2

(a) Breiman’s bias

(b) good clustering

Fig. 2: Example of Breiman’s bias on real data. Feature vectors are 3-dimensional LAB colours corresponding to image pixels. Clustering results are shown in two ways. First, red and blue show different clusters inside LAB space. Second, pixels with colours in the “background” (red) cluster are removed from the original image. (a) shows the result for kernel K-means with a ﬁxed-width Gaussian kernel isolating a small dense group of pixels from the rest. (b) shows the result for an adaptive kernel, see Section 4.

Instead of clustering data points {fp

p

∈

Ω}

⊂

N
R

in

their

original space, kernel K-means uses mapping φ

∶

N
R →H

embedding

input

data

fp

∈

N
R

as

points

φp

≡

φ(fp)

in

a

higher-

dimensional Hilbert space H. Kernel K-means minimizes the sum

of squared errors in the embedding space corresponding to the

following (mixed) objective function

F (S, m) =

φp − mk 2

(3)

k p∈Sk

where S = (S1, S2, . . . , SK ) is a partitioning (clustering) of Ω

into K clusters, m = (m1, m2, . . . mK ) is a set of parameters for the clusters, and . denotes the Hilbertian norm1. Kernel

K-means ﬁnds clusters separated by hyperplanes in H. In general,

these hyperplanes correspond to non-linear surfaces in the original

input

space

N
R

.

In

contrast

to

(3),

standard

K-means

objective

(2)

is

able

to

identify

only

linearly

separable

clusters

in

N
R

.

Optimizing F with respect to the parameters yields closed-

form solutions corresponding to the cluster means in the embed-

ding space:

∑q∈Sk φq

mˆ k = Sk

(4)

where . denotes the cardinality (number of points) in a cluster. Plugging optimal means (4) into objective (3) yields a high-order function, which depends solely on the partition variable S:

∑q∈Sk φq 2

F (S) =

φp − Sk

.

(5)

k p∈Sk

Expanding the Euclidean distances in (5), one can obtain an equivalent pairwise clustering criterion expressed solely in terms of inner products ⟨φ(fp), φ(fq)⟩ in the embedding space H:

c

∑pq∈Sk ⟨φ(fp), φ(fq)⟩

F (S) = −

(6)

k

Sk

where

c
=

means equality up to an additive constant. The inner

product is often replaced with kernel k, a symmetric function:

k(x, y) ∶= ⟨φ(x), φ(y)⟩.

(7)

Then, kernel K-means objective (5) can be presented as

kernel k-means criterion

c

∑pq∈Sk k(fp, fq)

F (S) = −

. (8)

k

Sk

1. Our later examples use ﬁnite-dimensional embeddings φ where H = RM is an Euclidean space (M ≫ N ) and . is the Euclidean norm.

Formulation (8) enables optimization in high-dimensional space H that only uses kernel computation and does not require computing the embedding φ(x). Given a kernel function, one can use the kernel K-means without knowing the corresponding embedding. However, not any symmetric function corresponds to the inner product in some space. Mercer’s theorem [2] states that any positive semideﬁnite (p.s.d.) kernel function k(x, y) can be expressed as an inner product in a higher-dimensional space. While p.s.d. is a common assumption for kernels, pairwise clustering objective (8) is often extended beyond p.s.d. afﬁnities. There are many other extension of kernel K-means criterion (8). Despite the connection to density modes made in our paper, kernel clustering has only a weak relation to mean-shift [13], e.g. see [14].

1.1.1 Related graph clustering criteria

Positive semideﬁnite kernel k(fp, fq) in (8) can be replaced by an arbitrary pairwise similarity or afﬁnity matrix A = [Apq]. This yields the average association criterion, which is known in the context of graph clustering [3], [15], [7]:

− ∑pq∈Sk Apq . (9)

k

Sk

The standard kernel K-means algorithm [7], [9] is not guaranteed to decrease (9) for improper (non p.s.d.) kernel k ∶= A. However, [15] showed that dropping p.s.d. assumption is not essential: for arbitrary association A there is a p.s.d. kernel k such that objective (8) is equivalent to (9) up to a constant.
In [3] authors experimentally observed that the average association (9) or kernel K-means (8) objectives have a bias to separate small dense group of data points from the rest, e.g. see Figure 2.
Besides average association, there are other pairwise graph clustering criteria related to kernel K-means. Normalized cut is a common objective in the context of spectral clustering [3], [16]. It optimizes the following objective

− ∑pq∈Sk Apq . (10) k ∑p∈Sk dp

where dp = ∑q∈Ω Apq. Note that for dp = 1 equation (10) reduces to (9). It is known that Normalized cut objective is equivalent to a
weighted version of kernel K-means criterion [17], [7].

1.1.2 Probabilistic interpretation via kernel densities
Besides kernel clustering, kernels are also commonly used for probability density estimation. This section relates these two independent problems. Standard multivariate kernel density estimate

or Parzen density estimate for the distribution of data points within cluster Sk can be expressed as follows [18]:

k

∑q∈Sk k(x, fq)

PΣ(x S ) ∶=

Sk ,

(11)

with kernel k having the form:

1

1

k(x, y) = Σ − 2 ψ Σ− 2 (x − y)

(12)

where ψ is a symmetric multivariate density and Σ is a symmetric positive deﬁnite bandwidth matrix controlling the density estimator’s smoothness. One standard example is the Gaussian (normal) kernel (1) corresponding to

t2 ψ(t) ∝ exp − 2 , (13)

which is commonly used both in kernel density estimation [18]

and kernel clustering [6], [3].

The choice of bandwidth Σ is crucial for accurate density

estimation, while the choice of ψ plays only a minor role [19].

There are numerous works regarding kernel selection for accurate

density estimation using either ﬁxed [20], [19], [21] or variable

bandwidth [22]. For example, Scott’s rule of thumb is

Σii =

ri √

,

Σij = 0 for i ≠ j

(14)

N+4 n

where n is the number of points, and ri2 is the variance of the i-th feature that could be interpreted as the range or scale of the data. Scott’s rule gives optimal mean integrated squared error for normal data distribution, but in practice it works well in more general settings. In all cases the optimal bandwidth for sufﬁciently large datasets is a small fraction of the data range [23], [18]. For shortness, we use adjective r-small to describe bandwidths providing accurate density estimation.
If kernel k has form (12) up to a positive multiplicative constant then kernel K-means objective (8) can be expressed in terms of kernel densities (11) for points in each cluster [6]:

F (S)

c
=

−

PΣ(fp Sk).

(15)

k p∈Sk

1.2 Other clustering criteria and their known biases
One of the goals of this paper is a theoretical explanation for the bias of kernel K-means with small bandwidths toward tight dense clusters, which we call Breiman’s bias, see Figs 1-2. This bias was observed in the past only empirically. As discussed in Section 4.1, large bandwidth reduces kernel K-means to basic K-means where bias to equal cardinality clusters is known [24]. This section reviews other standard clustering objectives, entropy and Gini criteria, that have biases already well-understood theoretically. In Section 2 we establish a connection between Gini clustering and kernel K-means in case of r-small kernels. This connection allows theoretical analysis of Breiman’s bias in kernel K-means.

1.2.1 Probabilistic K-means and entropy criterion
Besides non-parametric kernel K-means clustering there are wellknown parametric extensions of basic K-means (2) based on probability models. Probabilistic K-means [24] or model based clustering [25] use some given likelihood functions P (fp θk) instead of distances fp − θk 2 in (2) as in clustering objective

−

log P (fp θk).

(16)

k p∈Sk

3

Note that objective (16) reduces to basic K-means (2) for Gaussian probability model P (. θk) with mean θk and a ﬁxed scalar covariance matrix.
In probabilistic K-means (16) models can differ from Gaussians depending on a priori assumptions about the data in each cluster, e.g. gamma, Gibbs, or other distributions can be used. For more complex data, each cluster can be described by highlydescriptive parametric models such as Gaussian mixtures (GMM). Instead of kernel density estimates in kernel K-means (15), probabilistic K-means (16) uses parametric distribution models. Another difference is the absence of the log in (15) compared to (16).
The analysis in [24] shows that in case of highly descriptive model P , e.g. GMM or histograms, (16) can be approximated by the standard entropy criterion for clustering:

entropy

Sk ⋅ H(Sk)

(17)

criterion k

where H(Sk) is the entropy of the distribution of the data in Sk:

H(Sk) ∶= − P (x θk) log P (x θk) dx.

The discrete version of the entropy criterion is widely used for learning binary decision trees in classiﬁcation [11], [18], [26]. It is known that the entropy criterion above is biased toward equal size clusters [11], [24], [27].

1.2.2 Discrete Gini impurity and criterion
Both Gini and entropy clustering criteria are widely used in the context of decision trees [18], [26]. These criteria are used to decide the best split at a given node of a binary classiﬁcation tree [28]. The Gini criterion can be written for clustering {Sk} as

discrete Gini criterion

Sk ⋅ G(Sk)

(18)

k

where G(Sk) is the Gini impurity for the points in Sk. Assuming

discrete

feature

space

L

instead

of

N
R

,

the

Gini

impurity

is

G(Sk) ∶= 1 − P(l Sk)2

(19)

l∈L

where P(⋅ Sk) is the empirical probability (histogram) of discrete-valued features fp ∈ L in cluster Sk.
Similarly to the entropy, Gini impurity G(Sk) can be

viewed as a measure of sparsity or “peakedness” of the distribution for points in Sk. Note that (18) has a form

similar to the entropy criterion in (17), except that entropy

H is replaced by the Gini impurity. Breiman [11] analyzed

the theoretical properties of the discrete Gini criterion (18) when P(⋅ Sk) are discrete histograms. He proved

Theorem 1 (Breiman). For K = 2 the

minimum of the Gini criterion (18) for

discrete Gini impurity (19) is achieved

by assigning all data points with the

highest-probability feature value in L

to one cluster and the remaining data

points to the other cluster, as in exam-

1 2 3

...

14 ple for L = {1, . . . , 14} on the left.

2 BREIMAN’S BIAS (NUMERICAL FEATURES)

In this section we show that the kernel K-means objective reduces

to a novel continuous Gini criterion under some general condi-

tions on the kernel function, see Section 2.1. We formally prove

in Section 2.2 that the optimum of the continuous Gini criterion

isolates the data density mode. That is, we show that the discussed

earlier biases observed in the context of clustering [3] and decision

tree learning [11] are the same phenomena. Section 2.3 establishes

connection to maximum cliques [29] and dominant sets [8].

For further analysis we reformulate the problem of clustering a discrete set of points {fp p ∈ Ω} ⊂ RN , see Section 1.1, as

a continuous domain clustering problem. Let P be a probability

measure

over

domain

N
R

and

ρ

be

the

corresponding

continuous

probability density function such that the discrete points fp could

be treated as samples from this distribution. The clustering of the

continuous domain will be described by an assignment function s ∶

N
R

→

{1, 2, . . . , K}.

Density

ρ

implies

conditional

probability

densities ρsk(x) ∶= ρ(x s(x) = k). Feature points fp in cluster Sk could be interpreted as a sample from conditional density ρsk.

Then, the continuous clustering problem is to ﬁnd an assign-

ment function optimizing a clustering criteria. For example, we

can analogously to (18) deﬁne continuous Gini clustering criterion

Gcinoni tcirniuteoruiosn wk ⋅ G(s, k), (20)
k

where wk is the probability to draw a point from k-th cluster and

G(s, k) ∶= 1 − ρsk(x)2 dx.

(21)

In the next section we show that kernel K-means energy (15) can be approximated by continuous Gini-clustering criterion (20) for r-small kernels.

2.1 Kernel K-means and continuous Gini criterion
To establish the connection between kernel clustering and the Gini criterion, let us ﬁrst recall Monte-Carlo estimation [24], which yields the following expectation-based approximation for a continuous function g(x) and cluster C ⊂ Ω:

g(fp) ≈ C g(x) ρC (x) dx

(22)

p∈C

where ρC is the “true” continuous density of features in cluster C. Using (22) for C = Sk and g(x) = PΣ(x Sk), we can approximate the kernel density formulation in (15) by its expectation

F (S)

c
≈

−

Sk

PΣ(x Sk) ρsk(x) dx.

(23)

k

Note that partition S = (S1, . . . , SK ) is determined by dataset Ω and assignment function s. We also assume

PΣ(⋅ Sk) ≈ ρsk(⋅).

(24)

This is essentially an assumption on kernel bandwidth. That is, we assume that kernel bandwidth gives accurate density estimation. For shortness, we call such bandwidths r-small, see Section 1.1.2. Then (23) reduces to approximation

F (S)

c
≈

−

Sk ⋅

k

ρsk(x)2 dx

c
≡

Sk ⋅ G(s, k). (25)
k

Additional application of Monte-Carlo estimation Sk Ω ≈ wk allows replacing set cardinality Sk by probability wk of drawing a point from Sk. This results in continuous Gini clustering

4
criterion (20), which approximates (15) or (8) up to an additive and positive multiplicative constants.
Next section proves that the continuous Gini criterion (20) has a similar bias observed by Breiman in the discrete case.

2.2 Breiman’s bias in continuous Gini criterion
This section extends Theorem 1 to continuous Gini criterion (20). Since Section 2.1 has already established a close relation between continuous Gini criterion and kernel K-means for r-small bandwidth kernels, then Breiman’s bias also applies to the latter. For simplicity, we focus on K = 2 as in Breiman’s Theorem 1.

Theorem 2 (Breiman’s bias in continuous case). For K = 2

the continuous Gini clustering criterion (20) achieves its optimal

value

at

the

partitioning

of

N
R

into

regions

s1 = arg max ρ(x)

and

s2

=

N
R

∖

s1.

x

Proof. The statement follows from Lemma 2 below.

We denote mathematical expectation of function z

∶

Ω

→

1
R

Ez ∶= z(x)ρ(x) dx.

Minimization of (20) corresponds to maximization of the following objective function
L(s) ∶= w ρs1(x)2 dx + (1 − w) ρs2(x)2 dx (26)
where the probability to draw a point from cluster 1 is

w ∶= w1 =

ρ(x) dx = E[s(x) = 1]

s(x)=1

where [⋅] is the indicator function. Note that mixed joint density

ρ(x, k) = ρ(x) ⋅ [s(x) = k]

allows to write conditional density ρs1 in (26) as

ρs(x) = ρ(x, 1) = ρ(x) ⋅ [s(x) = 1] . (27)

1

P (s(x) = 1)

w

Equations (26) and (27) give

L(s) = 1 w

ρ(x)2[s(x) = 1] dx

1 +

ρ(x)2[s(x) = 2] dx.

(28)

1−w

Introducing notation

I ∶= [s(x) = 1] and ρ ∶= ρ(x)

allows to further rewrite objective function L(s) as

L(s) = EIρ + E(1 − I)ρ . (29)

EI

1 − EI

Without loss of generality assume that E1(1−−EII)ρ ≤ EEIIρ (the opposite case would yield a similar result). We now need following

Lemma 1. Let a, b, c, d be some positive numbers, then

ac b≤d

⇒ a ≤ a+c ≤ c. b b+d d

Proof. Use reduction to a common denominator.

Lemma 1 implies inequality

E(1 − I)ρ ≤ Eρ ≤ EIρ , (30)

1 − EI

EI

which is needed to prove the Lemma below.

Lemma 2. Assume that function sε is

sε(x) ∶= ⎧⎪⎪⎨1, ρ(x) ≥ supx ρ(x) − ε, (31) ⎪⎪2, otherwise. ⎩

Then

sup L(s) = lim L(sε) = Eρ + sup ρ(x).

(32)

s

ε→0

x

Proof. Due to monotonicity of expectation we have

EIρ ≤ E (I supx ρ(x)) = sup ρ(x).

(33)

EI

EI

x

Then (30) and (33) imply

L(s) = EIρ + E(1 − I)ρ ≤ sup ρ(x) + Eρ. (34)

EI 1 − EI

x

That is, the right part of (32) is an upper bound for L(s). Let Iε ≡ [sε(x) = 1]. It is easy to check that

lim E(1 − Iε)ρ = Eρ. (35) ε→0 1 − EIε

Deﬁnition (31) also implies

lim EIερ ≥ lim E(supx ρ(x) − ε)Iε = sup ρ(x). (36)

ε→0 EIε ε→0

EIε

x

This result and (33) conclude that

lim EIερ = sup ρ(x). (37)

ε→0 EIε

x

Finally, the limits in (35) and (37) imply

lim L(sε) = lim E(1 − Iε)ρ + lim EIερ

ε→0

ε→0 1 − EIε ε→0 EIε

= Eρ + sup ρ(x).

(38)

x

This equality and bound (34) prove (32).

This result states that the optimal assignment function separates the mode of the density function from the rest of the data. The proof considers case K = 2 for continuous Gini criterion approximating kernel K-means for r-small kernels. The multicluster version for K > 2 also has Breiman’s bias. Indeed, it is easy to show that any two clusters in the optimal solution shall give optimum of objective (20). Then, these two clusters are also subject to Breiman’s bias. See a multi-cluster example in Figure 3.
Practical considerations: While Theorem 2 suggests that the isolated density mode should be a single point, in practice Breiman’s bias in kernel k-means isolates a slightly wider cluster around the mode, see Figures 2, 3, 7(a-d), 8. Indeed, Breiman’s bias holds for kernel k-means when the assumptions in Section 2.1 are valid. In practice, shrinking of the clusters invalidates approximations (23) and (24) preventing the collapse of the clusters.

2.3 Connection to maximal cliques and dominant sets
Interestingly, there is also a relation between maximum cliques and density modes. Assume 0-1 kernel [ x − y ≤ σ] with bandwidth σ. Then, kernel matrix A is a connectivity matrix corresponding to a σ-disk graph. Intuitively, the maximum clique on this graph should be inside a disk with the largest number of points in it, which corresponds to the density mode.
Formally, mode isolation bias can be linked to both maximum clique and its weighted-graph generalization, dominant set [8]. It

5

(a) density

z

2

1

0

-1

Sizes of Clusters

-2

0.89

-3

3

3

2 1 0 0.11

2 1 0 -1

-1

-2

-2 y

-3

-4

x

-3 -5

(b) Gaussian kernel, 2 clusters

2

2

1

1

0

0

z

z

-1
Sizes of Clusters

-1
Sizes of Clusters

-2

-2

0.71

-3

-3

3

3

3

3

2

2

1

1

0

0.08 0.11 0 0.11

-1

-1

-2

-2 y

-3

-4

x

-3 -5

2 0.27 0.21 6 0.23 0.25

2 1 0

0

-1

-1

-2

-2 y

-3

-4

x

-3 -5

(c) Gaussian kernel, 4 clusters (d) KNN kernel, 4 clusters

Fig. 3: Breiman’s bias in clustering of images. We select 4 categories from the LabelMe dataset [30]. The last fully connected layer of the neural network in [31] gives 4096-dimensional feature vector for each image. We reduce the dimension to 5 via PCA. For visualization purposes, we obtain 3D embeddings via MDS [32]. (a) Kernel densities estimates for data points are color-coded: darker points correspond to higher density. (b,c) The result of the kernel K-means with the Gaussian kernel (1). Scott’s rule of thumb deﬁnes the bandwidth. Breiman’s bias causes poor clustering, i.e. small cluster is formed in the densest part of the data in (b), three clusters occupy few points within densest regions while the fourth cluster contains 71% of the data in (c). The normalized mutual information (NMI) in (c) is 0.38. (d) Good clustering produced by KNN kernel up (Example 3) gives NMI of 0.90, which is slightly better than the basic K-means (0.89).

is known that maximum clique [29] and dominant set [8] solve a two-region clustering problem with energy

∑pq∈S1 Apq

− S1

(39)

corresponding to average association (9) for K = 1 and S1 ⊆ Ω. Under the same assumptions as above, Gini impurity (21) can be used as an approximation reducing objective (39) to

EIρ . (40) EI

Using (33) and (37) we can conclude that the optimum of (40) isolates the mode of density function ρ. Thus, clustering minimizing (39) for r-small bandwidths also has Breiman’s bias. That is, for such bandwidths the concepts of maximum clique and dominant set for graphs correspond to the concept of mode isolation for data densities. Dominant sets for the examples in Figures 1(c), 2(a), and 7(d) would be similar to the shown mode-isolating solutions.

3 ADAPTIVE WEIGHTS SOLVING BREIMAN’S BIAS

We can use a simple modiﬁcation of average association by introducing weights wp ≥ 0 for each point “error” within the equivalent kernel K-means objective (3)

Fw(S, m) =

wp φp − mk 2.

(41)

k p∈Sk

- original data

- replicated data

- original data

- transformed data

Breiman’s bias
(mode isolation)

6
“equi-cardinality” bias
(lack of non-linear separation)

(a) adaptive weights (Sec. 3) (b) adaptive kernels (Sec. 4.3)
Fig. 4: Density equalization via (a) adaptive weights and (b) adaptive kernels. In (a) the density is modiﬁed as in (43) via “replicating” each data point inverse-proportionately to the observed density using wp ∝ 1 ρp. For simplicity (a) assumes positive integer weights wp. In (b) the density is modiﬁed according to (58) for bandwidth (61) via implicit embedding of data points in a higher dimensional space that changes their relative positions.

Such weighting is common for K-means [23]. Similarly to Section 1.1 we can expand the Euclidean distances in (41) to obtain an equivalent weighted average association criterion generalizing (9)

− ∑pq∈Sk wpwqApq . (42)

k

∑p∈Sk wp

Weights wp have an obvious interpretation based on (41); they change the data by replicating each point p by a number of points in the same location (Figure 4a) in proportion to wp. Therefore, this weighted formulation directly modiﬁes the data density as

ρ′p ∝ wpρp

(43)

where ρp and ρ′p are respectively the densities of the original and the new (replicated) points. The choice of wp = 1 ρp is a simple way for equalizing data density to solve Breiman’s bias. As shown in Figure 4(a), such a choice enables low-density points to be replicated more frequently than high-density ones. This is one of density equalization approaches giving the solution in Figure 1(d).

4 ADAPTIVE KERNELS SOLVING BREIMAN’S BIAS

Breiman’s bias in kernel K-means is speciﬁc to r-small band-

widths. Thus, it has direct implications for the bandwidth selection

problem discussed in this section. Note that kernel bandwidth

selection for clustering should not be confused with kernel

bandwidth selection for density estimation, an entirely different

problem outlined in Section 1.1.2. In fact, r-small bandwidths

give accurate density estimation, but yield poor clustering due

to Breiman’s bias. Larger bandwidths can avoid this bias in

clustering. However, Section 4.1 shows that for extremely large

bandwidths kernel K-means reduces to standard K-means, which

loses ability of non-linear cluster separation and has a different

bias to equal cardinality clusters [24], [27].

In practice, avoiding extreme bandwidths is problematic since

the notions of small and large strongly depend on data properties

that may signiﬁcantly vary across the domain, e.g. in Figure 1c,d

where no ﬁxed bandwidth gives a reasonable separation. This

motivates locally adaptive strategies. Interestingly, Section 4.2

shows that any locally adaptive bandwidth strategy implicitly

corresponds

to

some

data

embedding

Ω

→

N′
R

deforming

density

of the points. That is, locally adaptive selection of bandwidth is

equivalent to selection of density transformation. Local kernel

bandwidth and transformed density are related via the density

law established in (59). As we already know from Theorem 2,

0 r-small σ

dΩ

∞

Fig. 5: Kernel K-means biases over the range of bandwidth σ. Data diameter is denoted by dΩ = maxpq∈Ω fp − fq . Breiman’s bias is established for r-small σ (Section 1.1.2). Points stop interacting for σ smaller than r-small making kernel K-means fail. Larger σ reduce kernel K-means to the basic K-means removing an ability to separate the clusters non-linearly. In practice, there could be no intermediate good σ. In the example of Fig.1(c) any ﬁxed σ leads to either Breiman’s bias or to the lack of non-linear separability.

Breiman’s bias is caused by high non-uniformity of the data, which can be addressed by density equalizing transformations. Section 4.3 proposes adaptive kernel strategies based on our density law and motivated by a density equalization principle addressing Breiman’s bias. In fact, a popular locally adaptive kernel in [12] is a special case of our density equalization principle.

4.1 Overview of extreme bandwidth cases
Section 2.1 and Theorem 2 prove that for r-small bandwidths the kernel K-means is biased toward “tight” clusters, as illustrated in Figures 1, 2 and 7(d). As bandwidth increases, continuous kernel density (11) no longer approximates the true distribution ρsk violating (24). Thus, Gini criterion (25) is no longer valid as an approximation for kernel K-means objective (15). In practice, Breiman’s bias disappears gradually as bandwidth gets larger. This is also consistent with experimental comparison of smaller and larger bandwidths in [3].
The other extreme case of bandwidth for kernel K-means comes from its reduction to basic K-means for large kernels. For simplicity, assume Gaussian kernels (1) of large bandwidth σ approaching data diameter. Then the kernel can be approximated by its Taylor expansion exp − x2−σy2 2 ≈ 1 − x2−σy2 2 and kernel K-means objective (8) for σ ≫ x−y becomes2 (up to a constant)

∑pq∈Sk fp − fq 2 c 1

2

2σ2 Sk

= σ2

fp − mk , (44)

k

k p∈Sk

which is equivalent to basic K-means (2) for any ﬁxed σ.
Figure 5 summarizes kernel K-means biases for different bandwidths. For large bandwidths the kernel K-means loses its ability to ﬁnd non-linear cluster separation due to reduction to the basic K-means. Moreover, it inherits the bias to equal cardinality clusters, which is well-known for the basic K-means [24], [27]. On the other hand, for small bandwidths kernel K-means has Breiman’s bias proven in Section 2. To avoid the biases in Figure 5, kernel K-means should use a bandwidth neither too small nor too large. This motivates locally adaptive bandwidths.

2. Relation (44) easily follows by substituting mk ≡

1 Sk

∑p∈Sk fp.

4.2 Adaptive kernels as density transformation
This section shows that kernel clustering (8) with any locally adaptive bandwidth strategy satisfying some reasonable assumptions is equivalent to ﬁxed bandwidth kernel clustering in a new feature space (Theorem 3) with a deformed point density. The adaptive bandwidths relate to density transformations via density law (59). To derive it, we interpret adaptiveness as non-uniform variation of distances across the feature space. In particular, we use a general concept of geodesic kernel deﬁning adaptiveness via a metric tensor and illustrate it by simple practical examples.
Our analysis of Breiman’s bias in Section 2 applies to general kernels (12) suitable for density estimation. Here we focus on clustering with kernels based on radial basis functions ψ s.t.

ψ(x − y) = ψ( x − y ).

(45)

To obtain adaptive kernels, we replace Euclidean metric with

Riemannian inside (45). In particular, x − y is replaced with

geodesic

distances

dg(x, y)

between

features

x, y

∈

N
R

based

on

any given metric tensor g(f ) for f ∈ RN . This allows to deﬁne a

geodesic or Riemannian kernel at any points fp and fq as in [10]

kg(fp, fq) ∶= ψ(dg(fp, fq)) ≡ ψ(dpq)

(46)

where dpq ∶= dg(fp, fq) is introduced for shortness. In practice, the metric tensor can be deﬁned only at the data
points gp ∶= g(fp) for p ∈ Ω. Often, quickly decaying radial basis functions ψ allow Mahalanobis distance approximation inside (46)

dg

(

fp

,

x

2
)

≈

(fp − x)T gp (fp − x),

(47)

which is normally valid only in a small neighborhood of fp. If necessary, one can use more accurate approximations for dg(fp, fq) based on Dijkstra [33] or Fast Marching method [34].

EXAMPLE 1 (Adaptive non-normalized3 Gaussian kernel). Mahalanobis distances based on (adaptive) bandwidth matrices Σp deﬁned at each point p can be used to deﬁne adaptive kernel

−(fp

−

fq

T
)

Σ−p1(fp

−

fq

)

κp(fp, fq) ∶= exp 2 , (48)

which equals ﬁxed bandwidth Gaussian kernel (1) for Σp = σ2I.
Kernel (48) approximates (46) for exponential function ψ in (13) and tensor g continuously extending matrices Σ−p1 over the whole feature space so that gp = Σ−p1 for p ∈ Ω. Indeed, assuming matrices Σ−p1 and tensor g change slowly between points within
bandwidth neighbourhoods, one can use (47) for all points in

−dg(fp, fq)2

−d2pq

κp(fp, fq) ≈ exp 2 ≡ exp 2 (49)

due to exponential decay outside the bandwidth neighbourhoods.

EXAMPLE 2 (Zelnik-Manor & Perona kernel [12]). This popular kernel is deﬁned as κpq ∶= exp − 2fσpp−σfqq 2 . This kernel’s relation to (46) is less intuitive due to the lack of “local” Riemannian tensor.
However, under assumptions similar to those in (49), it can still be
seen as an approximation of geodesic kernel (46) for some tensor

3. Lack of normalization as in (48) is critical for density equalization resolving Breiman’s bias, which is our only goal for adaptive kernels. Note that without kernel normalization as in (12) Parzen density formulation of kernel k-means (15) no longer holds invalidating the relation to Gini and Breiman’s bias in Section 2. On the contrary, normalized variable kernels are appropriate for density estimation [22] validating (15). They can also make approximation (24) more accurate strengthening connections to Gini and Breiman’s bias.

(a) space of points f with Riemannian metric g

7
(b) transformed points f ′ with Euclidean metric

g2 g1
g3

1 1
1

unit balls in Riemannian metric

unit balls in Euclidean metric

Fig. 6: Adaptive kernel (46) based on Riemannian distances (a) is equivalent to ﬁxed bandwidth kernel after some quasi-isometric (50) embedding into Euclidean space (b), see Theorem 3, mapping ellipsoids (52) to balls (54) and modifying data density as in (57).

g such that gp = σp−2I for p ∈ Ω. They use heuristic σp = RpK , which is the distance to the K-th nearest neighbour of fp.
EXAMPLE 3 (KNN kernel). This adaptive kernel is deﬁned as up(fp, fq) = [fq ∈ KNN (fp)] where KNN (fp) is the set of K nearest neighbors of fp. This kernel approximates (46) for uniform function ψ(t) = [t < 1] and tensor g such that gp = I (RpK )2.

Theorem 3. Clustering (8) with (adaptive) geodesic kernel (46) is

equivalent to clustering with ﬁxed bandwidth kernel k′(fp′ , fq′ ) ∶=

ψ′(

fp′ − fq′

)

in

new

feature

space

N′
R

for

some

radial

basis

function ψ′ using the Euclidean distance and some constant N ′.

Proof. A powerful general result in [35], [36], [15] states that for any symmetric matrix (dpq) with zeros on the diagonal there is a constant h such that squared distances

d̃2pq = d2pq + h2[p ≠ q]

(50)

form Euclidean matrix (d̃pq). That is, there exists some Euclidean

embedding

Ω

→

N′
R

where

for

∀p

∈

Ω

there

corresponds

a

point

fp′

∈

N′
R

such

that

fp′ − fq′

= d̃pq, see Figure 6. Therefore,

ψ(dpq) = ψ d̃2pq − h2 [dpq ≥ h] ≡ ψ′(d̃pq) (51)

for ψ′(t) ∶= ψ( t2 − h2[t ≥ h]) and kg(fp, fq) = k′(fp′ , fq′ ).

Theorem

3

proves

that

adaptive

kernels

for

{fp}

⊂

N
R

can

be equivalently replaced by a ﬁxed bandwidth kernel for some

implicit

embedding4

{fp′ }

⊂

N′
R

in

a

new

space.

Below

we

establish a relation between three local properties at point p :

adaptive bandwidth represented by matrix gp and two densities

ρp and ρ′p in the original and the new feature spaces. For ε > 0

consider

an

ellipsoid

in

the

original

space

N
R

,

see

Figure

6(a),

Bp ∶= {x (x − fp)T gp (x − fp) ≤ ε2}.

(52)

Assuming ε is small enough so that approximation (47) holds, ellipsoid (52) covers features {fq q ∈ Ωp} for subset of points

Ωp ∶= {q ∈ Ω dpq ≤ ε}.

(53)

Similarly,

consider

a

ball

in

the

new

space

N′
R

,

see

Figure

6(b),

Bp′ ∶= {x x − fp′ 2 ≤ ε2 + h2}

(54)

covering features {fq′ q ∈ Ω′p} for points

Ω′p ∶= {q ∈ Ω d̃2pq ≤ ε2 + h2}.

(55)

4. The implicit embedding implied by Euclidean matrix (50) should not be confused with embedding in the Mercer’s theorem for kernel methods.

using ﬁxed width kernel

using adaptive kernel

8

Box and ground truth Gaussian AA

KNN AA

(a) input image (b) 2D color histogram (e) density mapping

(c) clustering result (d) color coded result (f) clustering result
Fig. 7: (a)-(d): Breiman’s bias for ﬁxed bandwidth kernel (1). (f): result for (48) with adaptive bandwidth (61) s.t. τ (ρ) = const. (e) density equalization: scatter plot of empirical densities in the original/new feature spaces obtained via (11) and (50).

It is easy to see that (50) implies Ωp = Ω′p. Let ρp and ρ′p be the densities5 of points within Bp and Bp′ correspondingly.
Assuming ⋅ denotes volumes or cardinalities of sets, we have

ρp ⋅ Bp = Ωp = Ω′p = ρ′p ⋅ Bp′ .

(56)

Omitting a constant factor depending on ε, h, N and N ′ we get

ρ′p = ρp Bp ∝ ρp det gp − 21 (57) Bp′

representing the general form of the density law. For the basic isotropic metric tensor such that gp = I σp2 it simpliﬁes to

ρ′p ∝ ρp σpN .

(58)

Thus, bandwidth σp can be selected adaptively based on any desired transformation of density ρ′p ≡ τ (ρp) using

σp ∝ N τ (ρp) ρp.

(59)

where observed density ρp in the original feature space can be evaluated at any point p using any standard estimators, e.g. (11).

4.3 Density equalizing locally adaptive kernels

Bandwidth formula (59) works for any density transform τ . To

address Breiman’s bias, one can use density equalizing trans-

forms τ (ρ) = const or τ (ρ) = 1 log(1 + αρ), which even up

α
the highly dense parts of the feature

τ (ρ) = ρ

new density

space as illustrated on the right. Some empirical results using density equalization τ (ρ) = const for

τ (ρ) = const τ (ρ) = α1 log(1 + αρ)

synthetic and real data are shown

original density ρ

in Figures 1(d) and 7(e,f).

One way to estimate the density in (59) is KNN approach [18]

K

K

ρp ≈ nVK ∝ n(RpK )N

(60)

where n ≡ Ω is the size of the dataset, RpK is the distance to the K-th nearest neighbor of fp, VK is the volume of a ball of radius RpK centered at fp. Then, density law (59) for τ (ρ) = const gives

σp ∝ RpK

(61)

consistent with heuristic bandwidth in [12], see Example 2.

5. We use the physical rather than probability density. They differ by a factor.

Fig. 8: Representative interactive segmentation results. Regularized average association (AA) with ﬁxed bandwidth kernel (1) or adaptive KNN kernels (Example 3) is optimized as in [37]. Red boxes deﬁne initial clustering, green contours deﬁne ground-truth clustering. Table 1 provides the error statistics. Breiman’s bias manifests itself by isolating the most frequent color from the rest.

The result in Figure 1(d) uses adaptive Gaussian kernel (48)

for Σp = σpI with σp derived in (61). Theorem 3 claims equiv-

alence to a ﬁxed bandwidth kernel in some transformed higherdimensional space RN′. Bandwidths (61) are chosen speciﬁcally

to equalize the data density in this space so that τ (ρ) = const.

The picture on the right illustrates

such density equalization for the 1

data in Figure 1(d). It shows a0.5

3D projection of the transformed 0

data obtained by multi-dimensional-0.5

1

0

scaling [32] for matrix (d̃pq) in (50). The observed density equal--1

-2

-1.5

-1

-0.5

0

0.5

1

1.5

ization removes Breiman’s bias from the clustering in Figure 1(d).

Real data experiments for kernels with adaptive bandwidth

(61) are reported in Figures 2, 3, 7, 8 and Table 1. Figure 7(e)

illustrates the empirical density equalization effect for this band-

width. Such data homogenization removes the conditions leading

to Breiman’s bias, see Theorem 2. Also, we observe empirically

that KNN kernel is competitive with adaptive Gaussian kernels,

but its sparsity gives efﬁciency and simplicity of implementation.

5 NORMALIZED CUT AND BREIMAN’S BIAS
Breiman’s bias for kernel K-means criterion (8), a.k.a. average association (AA) (9), was empirically identiﬁed in [3], but our Theorem 2 is its ﬁrst theoretical explanation. This bias was the main critique against AA in [3]. They also criticize graph cut [40] that “favors cutting small sets of isolated nodes”. These critiques are used to motivate normalized cut (NC) criterion (10) aiming at balanced clustering without “clumping” or “splitting”.

regularization (boundary smoothness) none†
Euclidean length∗ contrast-sensitive∗

Gaussian AA 20.4 15.1 9.7

average error, %

Gaussian KNN

NC

AA

17.6

12.2

16.0

10.2

13.8

7.1

KNN NC 12.4 11.0 7.8

TABLE 1: Interactive segmentation errors. AA stands for the average association, NC stands for the normalized cut. Errors are averaged over the GrabCut dataset[38], see samples in Figure 8. ∗We use [37], [39] for a combination of Kernel K-means objective (8) with Markov Random Field (MRF) regularization terms. The relative weight of the MRF terms is chosen to minimize the average error on the dataset. †Without the MRF term, [37] and [39] correspond to the standard kernel K-means [7], [9].

We do not obeserve any evidence of the mode isolation bias in NC. However, Section 5.1 demonstrates that NC still has a bias to isolating sparse subsets. Moreover, using the general density analysis approach introduced in Section 4.2 we also show in Section 5.2 that normalization implicitly corresponds to some density-inverting embedding of the data. Thus, mode isolation (Breiman’s bias) in this implicit embedding corresponds to the sparse subset bias of NC in the original data.

9

σ = 2.47 NC = 0.202

σ = 2.48 NC = 0.207

(a) NC for smaller bandwidth (b) NC for larger bandwidth (bias to “sparsest” subsets) (loss of non-linear separation)

Fig. 9: Normalized Cut with kernel (1) on the same data as in Figure 1(c,d). For small bandwidths NC shows bias to small isolated subsets (a). As bandwidth increases, the ﬁrst non-trivial solution overcoming this bias (b) requires bandwidth large enough so that problems with non-linear separation become visible. Indeed, for larger bandwidths the node degrees become more uniform dp ≈ const reducing NC to average association, which is known to degenerate into basic K-means (see Section 4.1). Thus, any further increase of σ leads to solutions even worse than (b). In this simple example no ﬁxed σ leads NC to a good solution as in Figure 1(d). That good solution uses adaptive kernel from Section 4.3 making speciﬁc clustering criterion (AA, NC, or AC) irrelevant, see (68).

5.1 Sparse subset bias in Normalized Cut
The normalization in NC does not fully remove the bias to small isolated subsets and it is easy to ﬁnd examples of “splitting” for weakly connected nodes, see Figure 9(a). The motivation argument for the NC objective below Fig.1 in [3] implicitly assumes similarity matrices with zero diagonal, which excludes many common similarities like Gaussian kernel (1). Moreover, their argument is built speciﬁcally for an example with a single isolated point, while an isolated pair of points will have a nearzero NC cost even for zero diagonal similarities.
Intuitively, this NC issue can be interpreted as a bias to the “sparsest” subset (Figure 9a), the opposite of AA’s bias to the “densest” subset, i.e. Breiman’s bias (Figure 1c). The next subsection discusses the relation between these opposite biases in detail. In any case, both of these density inhomogeneity problems in NC and AA are directly addressed by our density equalization principle embodied in adaptive weights wp ∝ 1 ρp in Section 3 or in the locally adaptive kernels derived in Section 4.3. Indeed, the result in Figure 1(d) can be replicated with NC using such adaptive kernel. Interestingly, [12] observed another data non-homogeneity problem in NC different from the sparse subset bias in Figure 9(a), but suggested a similar adaptive kernel as a heuristic solving it.
5.2 Normalization as density inversion
The bias to sparse clusters in NC with small bandwidths (Figure 9a) seems the opposite of mode isolation in AA (Figure 1c). Here we show that this observation is not a coincidence since NC can be reduced to AA after some density-inverting data transformation. While it is known [17], [7] that NC is equivalent to weighted kernel K-means (i.e. weighted AA) with some modiﬁed afﬁnity, this section relates such kernel modiﬁcation to an implicit density-inverting embedding where mode isolation (Breiman’s bias) corresponds to sparse clusters in the original data.

First, consider standard weighted AA objective for any given afﬁnity/kernel matrix Aˆpq = k(fp, fq) as in (42)

− ∑pq∈Sk wpwqAˆpq .

k

∑p∈Sk wp

Clearly, weights based on node degrees w = d and “normalized” afﬁnities Aˆpq = dAppdqq turn this into NC objective (10). Thus,
average association (9) becomes NC (10) after two modiﬁcations:

● replacing Apq by normalized afﬁnities Aˆpq = dAppdqq and ● introducing point weights wp = dp.

Both of these modiﬁcations of AA can be presented as implicit

data transformations modifying denisty. In particular, we show that

the ﬁrst one “inverses” density turning sparser regions into denser

ones, see Figure 10(a). The second data modiﬁcation is generally

discussed as a density transform in (43). We show that node degree

weights wp = dp do not remove the “density inversion”.

For simplicity, assume standard Gaussian kernel (1) based on

Euclidean distances dpq =

fp − fq

in

N
R

−d2pq Apq = exp 2σ2 . To convert AA into NC we ﬁrst need an afﬁnity “normalization”

Aˆ = Apq = exp −d2pq − 2σ2 log(dpdq) = exp −dˆ2pq (62)

pq dpdq

2σ2

2σ2

equivalently formulated as a modiﬁcation of distances

dˆ2pq ∶= d2pq + 2σ2 log(dpdq).

(63)

Using a general approach in the proof of Theorem 3, there exists

some

Euclidean

embedding

f¯p

∈

N¯
R

and

constant

h

≥

0

such

that

d¯2pq ∶= f¯p − f¯q 2 = dˆ2pq + h2[p ≠ q].

(64)

τ (x) = (1+loxg x)10

τ (x) = (1+loxg2x)10

x

0

104

(a) density transform (65)
(kernel normalization only)

x

25

75

(b) density transform (66)
(with additional point weighting)

Fig. 10: “Density inversion” in sparse regions. Using node degree
approximation dp ∝ ρp (67) we show representative density
transformation plots (a) ρ¯p = τ (ρp) and (b) ρ′p = τ (ρp) corresponding to AA with kernel modiﬁcation Aˆpq = dAppdqq (65) and additional point weighting wp = dp (66) exactly corresponding to
NC. This additional weighting weakens the density inversion in
(b) compared to (a), see the x-axis scale difference. However, it
is easy to check that the minima in (65) and (66) are achieved at some x∗ exponentially growing with N¯ . This makes the density inversion signiﬁcant for NC since N¯ may equal the data size.

0.2 0.1
0 0.1

0.2 0.1

0 -0.1

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

1

0.8

-1

0.6

-1.2

0.4

0.2

-1.4

0.2

0.1

0

-0.1

1

0.8

0.2

0.6

0.1 0.4 0

-0.1

0.2

(a)

original

data

{fp}

⊂

1
R

0.2

0.1

0

-0.1

1

0.8

0.2

0.6

0.1

0.4

0

-0.1

0.2

(b)

embedding

{f¯p}

⊂

N¯
R

Fig. 11: Illustration of “density inversion” for 1D data. The

original data points (a) are getting progressively denser along

the line. The points are color-coded according to the log of their

density.

Plot

(b)

shows

3D

approximation

{yp}

⊂

3
R

of

high-

dimensional

Euclidean

embedding

{f¯p}

⊂

N¯
R

minimizing

metric

errors ∑pq(dˆ2pq −

yp − yq

22
)

where

dˆpq

are

distances

(63).

Thus, modiﬁed afﬁnities Aˆpq in (62) correspond to the Gaussian

kernel

for

the

new

embedding

{f¯p}

in

N¯
R

Aˆpq ∝ exp −2dσ¯2p2q ≡ exp − f¯p2σ−2f¯q 2 .

Assuming dq ≈ dp for features fq near fp, equations (63) and (64) imply the following relation for such neighbors of fp

d¯2pq ≈ d2pq + h2 + 4σ2 log(dp).

Then, similarly to the arguments in (56), a small ball of radius ε

centered

at

fp

in

N
R

and

a

ball

of

radius

ε2 + h2 + 4σ2 log(dp)

at

f¯p

in

N¯
R

contain

the

same

number

of

points.

Thus,

similarly

to (57) we get a relation between densities at points fp and f¯p

ρp εN

ρ¯p ≈ (ε2 + h2 + 4σ2 log(dp))N¯ 2 .

(65)

This implicit density transformation is shown in Figure 10(a). Sub-
linearity in dense regions addresses mode isolation (Breiman’s
bias). However, sparser regions become relatively dense and kernel-modiﬁed AA may split them. Indeed, the result in Figure 9(a) can be obtained by AA with normalized afﬁnity dAppdqq .
The second required modiﬁcation of AA introduces point weights wp = dp. It has an obvious equivalent formulation via
data points replication discussed in Section 3, see Figure 4(a).
Following (43), we obtain its implicit density modiﬁcation effect

10
ρ′p = dpρ¯p. Combining this with density transformation (65) implied by afﬁnity normalization dAppdqq , we obtain the following density transformation effect corresponding to NC, see Figure 10(b),

dp ρp εN ρ′p ≈ (ε2 + h2 + 4σ2 log(dp))N¯ 2 . (66)

The density inversion in sparse regions relates NC’s result in

Figure

9(a)

to

Breiman’s

bias

for

embedding

{f¯p}

in

N¯
R

.

Figure 10 shows representative plots for density transforma-

tions (65), (66) using the following node degree approximation

based on Parzen approach (11) for Gaussian afﬁnity (kernel) A

dp = Apq ∝ ρp.

(67)

q

Empirical relation between dp and ρp is illistrated below: some

overestimation occurs for sparcer regions and underestimation happens for denser regions. The node N degree for Gaussian kernels has to

dp - node degree ρp dp ∼ dp(ρp)

be at least 1 (for an isolated node) 1 and at most N (for a dense graph). 0

ρp - density

6 DISCUSSION (KERNEL CLUSTERING EQUIVALENCE)

Density equalization with adaptive weights in Section 3 or adaptive kernels in Section 4 are useful for either AA or NC due to their density biases (mode isolation or sparse subset). Interestingly, kernel clustering criteria discussed in [3] such as normalized cut (NC), average cut (AC), average association (AA) or kernel Kmeans are practically equivalent for such adaptive methods. This can be seen both empirically (Table 1) and conceptually. Note, weights wp ∝ 1 ρp in Section 3 produce modiﬁed data with near constant node degrees d′p ∝ ρ′p ∝ 1, see (67) and (43). Alternatively, KNN kernel (Example 3) with density equalizing bandwidth (61) also produce nearly constant node degrees dp ≈ K where K is the neighborhood size. Therefore, both cases give

− ∑pq∈Sk Apq ∝ − ∑pq∈Sk Apq ≈c ∑p∈Sk,q∈S¯k Apq , (68)

∑p∈Sk dp

K Sk

K Sk

which correspond to NC (10), AA (9), and AC criteria. As discussed in [3], the last objective also has very close relations with standard partitioning concepts in spectral graph theory: isoperimetric or Cheeger number, Cheeger set, ratio cut.
This equivalence argument applies to the corresponding clustering objectives and is independent of speciﬁc optimization algorithms developed for them. Interestingly, the relation between (9) and basic K-means objective (3) suggests that standard Lloyd’s algorithm can be used as a basic iterative approach for approximate optimization of all clustering criteria in (68). In practice, however, kernel K-means algorithm corresponding to the exact high-dimensional embedding {φp} in (3) is more sensitive to local minima compared to iterative K-means over approximate lowerdimensional embeddings based on PCA [14, Section 3.1]6.

6. K-means is also commonly used as a discretization heuristic for spectral relaxation [3] where a similar eigen analysis is motivated by spectral graph theory [41], [42], [43] defferently from PCA dimensionalty reduction in [14].

7 CONCLUSIONS
This paper identiﬁes and proves density biases, i.e. isolation of modes or sparsest subsets, in many well-known kernel clustering criteria such as kernel K-means (average association), ratio cut, normalized cut, dominant sets. In particular, we show conditions when such biases happen. Moreover, we propose density equalization as a general principle for resolving such biases. We suggest two types of density equalization techniques using adaptive weights or adaptive kernels. We also show that density equalization uniﬁes many popular kernel clustering objectives by making them equivalent.
ACKNOWLEDGEMENTS
The authors would like to thank Professor Kaleem Siddiqi (McGill University) for suggesting a potential link between Breiman’s bias and the dominant sets. This work was generously supported by the Discovery and RTI programs of the National Science and Engineering Research Council of Canada (NSERC).
REFERENCES
[1] B. Scho¨lkopf, A. Smola, and K.-R. Mu¨ller, “Nonlinear component analysis as a kernel eigenvalue problem,” Neural computation, vol. 10, no. 5, pp. 1299–1319, 1998. 1
[2] V. Vapnik, Statistical Learning Theory. Wiley, 1998. 1, 2 [3] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, pp. 888–905, 2000. 1, 2, 3, 4, 6, 8, 9, 10 [4] K. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf, “An introduction to kernel-based learning algorithms,” IEEE Trans. Neural Networks, vol. 12, no. 2, pp. 181–201, 2001. 1 [5] R. Zhang and A. Rudnicky, “A large scale clustering scheme for kernel k-means,” in Pattern Recognition, 2002., vol. 4, 2002, pp. 289–292. 1 [6] M. Girolami, “Mercer kernel-based clustering in feature space,” IEEE Trans. Neural Networks, vol. 13, no. 3, pp. 780–784, 2002. 1, 3 [7] I. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means, spectral clustering and normalized cuts,” in KDD, 2004. 1, 2, 9 [8] M. Pavan and M. Pelillo, “Dominant sets and pairwise clustering,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 1, pp. 167–172, 2007. 1, 4, 5 [9] R. Chitta, R. Jin, T. Havens, and A. Jain, “Scalable kernel clustering: Approximate kernel k-means,” in KDD, 2011, pp. 895–903. 1, 2, 9 [10] S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi, “Kernel methods on Riemannian manifolds with Gaussian RBF kernels,” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 37, no. 12, pp. 2464–2477, 2015. 1, 7 [11] L. Breiman, “Technical note: Some properties of splitting criteria,” Machine Learning, vol. 24, no. 1, pp. 41–47, 1996. 1, 3, 4 [12] L. Zelnik-Manor and P. Perona, “Self-tuning spectral clustering,” in Advances in NIPS, 2004, pp. 1601–1608. 1, 6, 7, 8, 9 [13] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature space analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2002. 2 [14] M. Tang, D. Marin, I. B. Ayed, and Y. Boykov, “Kernel Cuts: MRF meets kernel and spectral clustering,” in arXiv:1506.07439, September 2016 (also submitted to IJCV). 2, 10 [15] V. Roth, J. Laub, M. Kawanabe, and J. Buhmann, “Optimal cluster preserving embedding of nonmetric proximity data,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 12, pp. 1540—1551, 2003. 2, 7 [16] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and computing, vol. 17, no. 4, pp. 395–416, 2007. 2 [17] F. Bach and M. Jordan, “Learning spectral clustering,” Advances in Neural Information Processing Systems, vol. 16, pp. 305–312, 2003. 2, 9 [18] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, August 2006. 3, 8 [19] D. W. Scott, Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons, 1992. 3 [20] B. W. Silverman, Density estimation for statistics and data analysis. CRC press, 1986, vol. 26. 3

11
[21] A. J. Izenman, “Review papers: Recent developments in nonparametric density estimation,” Journal of the American Statistical Association, vol. 86, no. 413, pp. 205–224, 1991. 3
[22] G. R. Terrell and D. W. Scott, “Variable kernel density estimation,” The Annals of Statistics, vol. 20, no. 3, pp. 1236–1265, 1992. [Online]. Available: http://www.jstor.org/stable/2242011 3, 7
[23] R. O. Duda and P. E. Hart, Pattern Classiﬁcation and Scene Analysis. Wiley, 1973. 3, 6
[24] M. Kearns, Y. Mansour, and A. Ng, “An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering,” in Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), August 1997. 3, 4, 6
[25] C. Fraley and A. E. Raftery, “Model-Based Clustering, Discriminant Analysis, and Density Estimation,” Journal of the American Statistical Association, vol. 97, no. 458, pp. 611–631, 2002. 3
[26] A. Criminisi and J. Shotton, Decision Forests for Computer Vision and Medical Image Analysis. Springer, 2013. 3
[27] Y. Boykov, H. Isack, C. Olsson, and I. B. Ayed, “Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions,” in International Conference on Computer Vision (ICCV), December 2015. 3, 6
[28] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen, Classiﬁcation and regression trees. CRC press, 1984. 3
[29] T. S. Motzkin and E. G. Straus, “Maxima for graphs and a new proof of a theorem of tura´n,” Canad. J. Math, vol. 17, no. 4, pp. 533–540, 1965. 4, 5
[30] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic representation of the spatial envelope,” International journal of computer vision, vol. 42, no. 3, pp. 145–175, 2001. 5
[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105. 5
[32] T. Cox and M. Cox, Multidimensional scaling. CRC Press, 2000. 5, 8 [33] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction
to algorithms. MIT press, 2006. 7 [34] J. A. Sethian, Level set methods and fast marching methods. Cambridge
university press, 1999, vol. 3. 7 [35] J. Lingoes, “Some boundary conditions for a monotone analysis of
symmetric matrices,” Psychometrika, 1971. 7 [36] J. C. Gower and P. Legendre, “Metric and euclidean properties of
dissimilarity coefﬁcients,” Journal of classiﬁcation, vol. 3, no. 1, pp. 5–48, 1986. 7 [37] M. Tang, I. B. Ayed, D. Marin, and Y. Boykov, “Secrets of grabcut and kernel k-means,” in International Conference on Computer Vision (ICCV), Santiago, Chile, December 2015. 8, 9 [38] C. Rother, V. Kolmogorov, and A. Blake, “Grabcut - interactive foreground extraction using iterated graph cuts,” in ACM trans. on Graphics (SIGGRAPH), 2004. 9 [39] M. Tang, D. Marin, I. B. Ayed, and Y. Boykov, “Normalized Cut meets MRF,” in European Conference on Computer Vision (ECCV), 2016. 9 [40] Z. Wu and R. Leahy, “An optimal graph theoretic approach to data clustering: theory and its application to image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 15, no. 11, pp. 1101–1113, Nov 1993. 8 [41] J. Cheeger, “A lower bound for the smallest eigenvalue of the laplacian,” Problems in Analysis, R.C. Gunning, ed., pp. 195–199, 1970. 10 [42] W. Donath and A. Hoffman, “Lower bounds for the partitioning of graphs,” IBM J. Research and Development, pp. 420–425, 1973. 10 [43] M. Fiedler, “A property of eigenvectors of nonnegative symmetric matrices and its applications to graph theory,” Czech. Math. J., vol. 25, no. 100, pp. 619–633, 1975. 10
Dmitrii Marin received Diploma of Specialist from the Ufa State Aviational Technical University in 2011, and M.Sc. degree in Applied Mathematics and Information Science from the National Research University Higher School of Economics, Moscow, and graduated from the Yandex School of Data Analysis, Moscow, in 2013. In 2010 obtained a certiﬁcate of achievement at ACM ICPC World Finals, Harbin. He is a PhD candidate at the Department of Computer Science, University of Western Ontario under supervision of Yuri Boykov. His research is focused on designing general unsupervised and semi-supervised methods for accurate image segmentation and object delineation.

12

optimization

problems

Meng Tang is a PhD candidate in computer science at the University of Western Ontario, Canada, supervised by Prof. Yuri Boykov. He obtained MSc in computer science in 2014 from the same institution for his thesis titled ”Color Separation for Image Segmentation”. Previously in 2012 he received B.E. in Automation from the Huazhong University of Science and Technology, China. He is interested in image segmentation and semi-supervised data clustering. He is also obsessed and has experiences on discrete for computer vision and machine learning.

Ismail Ben Ayed received the PhD degree (with the highest honor) in computer vision from the Institut National de la Recherche Scientiﬁque (INRS-EMT), Montreal, QC, in 2007. He is currently Associate Professor at the Ecole de Technologie Superieure (ETS), University of Quebec, where he holds a research chair on Artiﬁcial Intelligence in Medical Imaging. Before joining the ETS, he worked for 8 years as a research scientist at GE Healthcare, London, ON, conducting research in medical image analysis. He also holds an adjunct professor appointment at the University of Western Ontario (since 2012). Ismail’s research interests are in computer vision, optimization, machine learning and their potential applications in medical image analysis.

Yuri Boykov received ”Diploma of Higher Education” with honors at Moscow Institute of Physics and Technology (department of Radio Engineering and Cybernetics) in 1992 and completed his Ph.D. at the department of Operations Research at Cornell University in 1996. He is currently a full professor at the department of Computer Science at the University of Western Ontario. His research is concentrated in the area of computer vision and biomedical image analysis. In particular, he is interested in problems of early vision, image segmentation, restoration, registration, stereo, motion, model ﬁtting, feature-based object recognition, photo-video editing and others. He is a recipient of the Helmholtz Prize (Test of Time) awarded at International Conference on Computer Vision (ICCV), 2011 and Florence Bucke Science Award, Faculty of Science, The University of Western Ontario, 2008.

