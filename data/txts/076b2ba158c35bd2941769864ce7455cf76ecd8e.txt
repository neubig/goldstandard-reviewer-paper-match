arXiv:2011.15083v1 [cs.HC] 30 Nov 2020

A Large Scale Randomized Controlled Trial on Herding in
Peer-Review Discussions
Ivan Stelmakh♠, Charvi Rastogi♠, Nihar B. Shah♠, Aarti Singh♠ and Hal Daumé III♥♦
♠School of Computer Science, Carnegie Mellon University ♥University of Maryland, College Park ♦Microsoft Research, New York
{stiv,crastogi,nihars,aarti}@cs.cmu.edu, hal@umiacs.umd.edu
Abstract Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing papers and making the ﬁnal acceptance/rejection decisions. Given that human decision making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of between-reviewers discussions and investigate the presence of herding behaviour therein. In that, we aim to understand whether reviewers and more senior decision makers get disproportionately inﬂuenced by the ﬁrst argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. Speciﬁcally, in conjunction with the review process of ICML 2020 — a large, top tier machine learning conference — we design and execute a randomized controlled trial with the goal of testing for the conditional causal eﬀect of the discussion initiator’s opinion on the outcome of a paper.
1 Introduction
While carefully aggregated independent opinions of a large number of people can often result in superior accuracy (Galton, 1907; Surowiecki, 2005), decisions stemming from a a group discussion are known to be susceptible to various biases related to social inﬂuence (Asch, 1951; Baron et al., 1996; Lorenz et al., 2011; Janis, 1982; Cialdini and Goldstein, 2004). For example, a seminal experiment of Asch (1951) revealed that when a strikingly incorrect judgement is made by several group members, other agents may agree with it even if they clearly understand its inaccuracy. Manifestations of social inﬂuence have been documented in political elections (Bond et al., 2012), individual and institutional investments (Nofsinger and Sias, 1999), and academia (Resnik and Smith, 2020).
This work considers a speciﬁc manifestation of social inﬂuence that results in “herding behaviour” — an eﬀect when agents are doing what others are doing rather than choosing the course of actions based on the information available to them (Banerjee, 1992) — in group discussion. A long line of work on human decision making establishes the presence of various biases related to the ﬁrst piece of information received by an individual (Tversky and Kahneman, 1974; Strack and Mussweiler, 1997; Mussweiler and Strack, 2001). In particular, these works show that an initial signal received by a decision maker (even when being clearly unrelated to the underlying task) often has a disproportionately strong inﬂuence on the ﬁnal decision. Projecting this observation on the group discussion setting, we hypothesize that the ﬁrst argument made in the discussion may exert an undue inﬂuence on opinions of subsequent contributors, thereby leading to a herding behaviour.
Past literature on group decision making (McGuire et al., 1987; Dubrovsky et al., 1991; Weisband, 1992) indeed suggests that the herding behaviour (titled “ﬁrst advocacy” eﬀect in these works) may be present
1

in group discussions.1 With this motivation, in this work we concentrate on a speciﬁc incarnation of the discussion process and analyze the presence of the herding eﬀect in the the discussion stage of conference peer review. In peer review, discussion takes place after reviewers submit their initial reviews and authors submit their responses to these reviews. The purpose of the discussion stage is to allow reviewers and area chairs (equivalents of associate editors in journal peer review) to exchange their opinions about the papers and correct each other’s misconceptions. Overall, reviewers are supposed to reach a consensus on the paper or boil their disagreement down to concrete arguments that can later be evaluated by area chairs and program chairs (equivalents of the editor in chief in journal peer review).
Given that discussion plays a central role in conference peer review and that many aspects of conference peer review such as bidding (Fiez et al., 2020; Meir et al., 2020), paper-reviewer matching (Stelmakh et al., 2018; Garg et al., 2010; Kobren et al., 2019; Jecmen et al., 2020), rebuttals (Gao et al., 2019), review ratings (Tomkins et al., 2017; Kang et al., 2018; Noothigattu et al., 2020; Wang and Shah, 2019) and rankings (Stelmakh et al., 2020; Xu et al., 2019), as well as review text (Hua et al., 2019; Manzoor and Shah, 2020) are studied by past literature, it is perhaps surprising that the discussion between reviewers remains largely unstudied. The work of Gao et al. (2019) performs an analysis of observational data on post-rebuttal review update and concludes that reviewers tend to converge to the mean of scores given in initial independent reviews. This ﬁnding is supported by a careful randomized controlled study by Teplitskiy et al. (2019) which indicates a strong impact of social inﬂuence on experts’ evaluations. However, these past works do not investigate the dynamics of the between-reviewer discussion that we consider in this work.
The most relevant past work on peer review is a study by Hofer et al. (2000) which evaluates the impact of discussion in peer review of medical care quality on the accuracy of ﬁnal decisions. The experiment by Hofer et al. (2000) reveals that while discussion between a pair of reviewers leads to higher agreement within the pair, it does not improve the agreement between reviewers from diﬀerent discussion pairs, suggesting that the inﬂation of the post-discussion agreement does not indicate an improvement of the quality of resulting decisions. This ﬁnding hints that reviewers within the pair reach a consensus not because they identify the right answer, but due to some other eﬀects, and in the present study we aim at investigating the presence of the herding behaviour — a potential cause of the eﬀect observed by Hofer et al. (2000).
In addition to being a potential cause of erroneous consensus between reviewers, the herding behaviour, if present, can contribute to the overall unfairness of the review process. In the current review system, it is the job of the area chairs to ensure that discussions do take place, and diﬀerent area chairs use diﬀerent strategies to initiate the discussion. Some of them may call upon the reviewer whose opinion is the most extreme, others may request the most positive (negative) reviewer to start. Another option for an area chair it to initiate the discussion themselves or to choose an initiator based on their seniority or expertise. In the presence of herding, the uncertainty in the choice of the strategy may impact the outcome of a paper (which becomes dependent on the essentially random choice of the strategy by the area chair), thereby increasing the undesirable randomness of the process.
With the above motivation, in this work we aim at testing the presence of the herding eﬀect in conference peer-review discussions. Speciﬁcally, in conjunction with the review process of the International Conference on Machine Learning (ICML 2020) — a ﬂagship machine learning conference that receives thousands of paper submissions and manages a pool of thousands of reviewers — we study the following research question:
Research Question: Given a set of reviewers who participate in the discussion of a paper, does the ﬁnal acceptance decision for the paper causally depend on the choice of the discussion initiator?
Let us now clarify some subtle points in the formulation of the research question. First, by saying that we are interested in a causal relationship, we underscore that our goal is to exclude the impact of the various confounding factors that are present in the observational data on peer-review discussions. For example, the fact that some reviewer decides to begin the discussion may imply that this reviewer is the most energetic and hence would dominate the discussion even if it was initiated by someone else, thereby introducing spurious correlations in data. We design the experiment to remove such spurious correlations and identify the causal impact of the order in which reviewers join the discussion.
1We discuss these past works in more detail in Section 4.
2

Second, by saying that we consider the eﬀect conditioned on reviewers who participate in the discussion, we speciﬁcally address the confounding factor that not all assigned reviewers may participate in the discussion of a paper. Hence, any intervention may impact the outcome of the paper by altering the set of reviewers involved in the discussion, thereby also leading to spurious correlations. As we explain in the sequel (Section 2), we carefully design an intervention that allows us to account for this undesirable eﬀect.
Finally, despite past work ﬁnding herding in human discussions, these ﬁndings may not necessarily apply to peer review because of the nature of the task performed by reviewers and presence of other biases. Indeed, the reviewing task is analytical and requires rational thinking, thereby potentially reducing the reliance on heuristics responsible for cognitive biases (Stanovich, 1999; Kahneman and Frederick, 2002). On the other hand, the discussion takes place after the initial reviews are submitted and hence reviewers may be anchored to opinions they have already formed (Tversky and Kahneman, 1974), which could also reduce the strength of the potential eﬀect.
2 Experiment Design
In this section, we describe the design of the experiment we executed in ICML 2020 to test for the herding eﬀect in conference peer review. Before we delve into details, let us brieﬂy describe the organization of the ICML 2020 peer-review process, which follows the conventions adopted by most top machine learning and artiﬁcial intelligence conferences. The conference peer-review pipeline comprises four main components: (i) initial reviewing, (ii) author rebuttal, (iii) reviews update and discussion, (iv) ﬁnal decision making. Upon the release of initial reviews, authors of papers have several days (12 days in the case of ICML 2020) to write a response to reviewers, followed by the discussion stage. During the discussion, reviewers and area chairs have access to the author feedback and are able to communicate with each other (but not with authors) via a special interface. For the papers assigned to them, each reviewer is expected to carefully read the author rebuttal as well as the reviews written by the other reviewers, and try to reach a consensus with other reviewers on the ﬁnal recommendation for the paper or identify the key points that require an evaluation of the area and program chairs. Area chairs are responsible for overseeing and managing discussions; speciﬁcally, they have to ensure that a discussion between the reviewers (as well as the area chair) does take place for papers that require the discussion (i.e., for which the ﬁnal decision is not yet clear) and they are free to use any strategy to initiate it. After the discussion is over, the area and program chairs analyze the author responses, reviews and discussions to make the ﬁnal decisions.
2.1 Preliminaries
To establish a causal relationship between the opinion of a discussion initiator and the outcome of a paper, the experiment we design in this work follows an A/B testing pipeline in which a set of papers submitted for review is split into two groups that receive diﬀerent treatments, where the treatments are designed such that the diﬀerence in some observable outcome of papers across groups is indicative of the presence of herding.
Intuitively, we expect herding (if present) to move the outcome of a discussion towards the opinion of the discussion initiator. Recall that discussions begin after the initial reviews are submitted, thereby enabling us to infer the initial opinions of reviewers about the papers. Thus, a naïve idea to conduct such a test is to assign the reviewer with the most positive (respectively, negative) opinion about the paper to initiate the discussion for each paper from group A (respectively, B). In this way, under the presence of herding we expect papers from group A to receive more positive ﬁnal evaluations from reviewers and enjoy higher acceptance rate than papers from group B, thereby allowing us to detect the eﬀect. However, as compared to the standard A/B testing settings, the conference peer review process features important idiosyncrasies that complicate the design of the treatments:
• First, in contrast to some conventional experiments on the herding behaviour (McGuire et al., 1987; Dubrovsky et al., 1991; Weisband, 1992) where subjects are speciﬁcally recruited to participate in the discussion and hence the participation rate is high, in peer review some reviewers may choose to ignore the discussion. In fact, the analysis of the review process of another leading machine learning conference
3

NeurIPS 2016 (Shah et al., 2018) revealed that only 30% of 13,674 paper-reviewer pairs had a message posted by the reviewer in the associated discussion, showing that the set of discussion participants is generally a strict (and somewhat small) subset of reviewers assigned to the paper.
• Second, even if a reviewer is willing to participate in the discussion, they may refrain from initiating the discussion when requested to do so by the area or program chairs. Moreover, while we can encourage some reviewers to start the discussion, we cannot prevent other reviewers from doing so if they wish. Overall, we do not have complete control over who initiates the discussion.
The ﬁrst idiosyncrasy implies that any treatment we apply to a paper may aﬀect the outcome of the paper in two ways: ﬁrst, by inducing a speciﬁc (negative or positive) shift in reviewers’ evaluations caused by herding (the intended way described above) and second, by changing the population of reviewers participating in the discussion (an unintended way orthogonal to herding that can obscure the ﬁndings). To understand the second mechanism, note that under the naïve treatments introduced above, the participation rate of the the most positive (respectively, negative) reviewers may be higher for papers from group A (respectively, B) because our treatments speciﬁcally encourage these reviewers to participate in the discussion of papers from the corresponding groups. The diﬀerence in the participation rates may aﬀect the outcome of a paper even in the absence of herding, thereby leading to an increased probability of the false alarm.
The second idiosyncrasy further complicates the design by limiting our control over the choice of the discussion initiator; even if the we request some reviewer to initiate the discussion, there is no guarantee that the reviewer will comply. To illustrate the potential consequences, let us consider a concocted scenario in which the most positive and the most negative reviewers are reluctant to initiate the discussion and where all the discussions are initiated by reviewers whose scores are not at the extremes. In this example, the naïve treatments we introduced above do not induce any diﬀerence between the conditions and, hence, the collected data will not allow to identify the herding eﬀect even if it is present.
Taking these challenges into account, we now proceed to describe the design of the intervention that we implement in the ICML 2020 peer-review process.
2.2 Intervention Design
Let us refer to the set of papers that are involved in the experiment as “participating papers” (the selection criteria is explained in Section 2.3). Recall that the idea of our intervention is to split the set of participating papers into two subgroups (A and B) and apply diﬀerent treatments to each of these groups. In this paper, the treatments we apply to the groups are speciﬁc discussion-management strategies, so in what follows we use the latter term.
To clearly formulate the requirements on the strategies applied to the diﬀerent groups of papers, let us consider a paper and two scenarios of its discussion that correspond to allocation of the paper to group A or group B. To account for the idiosyncrasies formulated in the previous section, the strategies we design should ensure that:
(i) The set of reviewers who participate in the discussion of the paper is the same in both scenarios.
(ii) The most positive (respectively, negative) reviewer is more likely to initiate the discussion when the paper is allocated to group A (respectively, B) than other reviewers.
In other words, the strategies should ensure that set of reviewers who participate in the discussion is the same for both scenarios, but the order in which reviewers join the discussion is diﬀerent depending on the group the paper is allocated to. Importantly, we underscore that these requirements do not imply that the strategies do not impact the set of reviewers who participate in the discussion of the paper. Instead, we only require that the set of participating reviewers is the same for both strategies (it may be diﬀerent from the set of reviewers who discuss the paper when no strategy is applied).
The high-level idea of the discussion-management strategies we employ in the experiment is to use the naïve approach of requesting the most positive and the most negative reviewers to initiate the discussion for the corresponding groups of papers, and then complementing it with a balancing part which attempts to equalize the set of participating reviewers across the groups. Let us now introduce this idea in more detail.
4

To describe the discussion-management strategies, let us consider any participating paper such that not all reviewers gave the same score to this paper in their initial reviews. Also let R+ (respectively, R−) be a reviewer with the most positive (respectively, negative) opinion about the paper according to the initial reviews. In other words, R+ and R− are reviewers who gave the highest and the lowest overall scores to the paper in the initial review. If more than one reviewer gave the highest (respectively, lowest) score, then R+ (respectively, R−) is selected by breaking ties uniformly at random. Consider now the following two strategies, termed S+ and S−, towards managing the discussion of this paper:
S+: First, ask reviewer R+ to initiate the discussion, then ask reviewer R− to contribute to the discussion
S−: First, ask reviewer R− to initiate the discussion, then ask reviewer R+ to contribute to the discussion
Both strategies consist of two parts: in the ﬁrst part, which we call initiating, a reviewer with an extreme opinion about the paper is asked to initiate the discussion. In the second part, which we refer to as balancing, the reviewer with the score at another extreme is asked to contribute to the discussion. The strategies are similar in that they attempt to engage both the most positive and the most negative reviewers in the discussion of the paper. However, the crucial diﬀerence between S+ and S− is the order in which reviewers are supposed to contribute: S+ encourages the most positive reviewer to start the discussion while S− encourages the reviewer with the most negative opinion to initiate it. Importantly, both strategies proceed to requesting the second reviewer (after some waiting time) even if the ﬁrst reviewer fails to fulﬁl the request.
Intuitively, if we apply strategy S+ to papers from one group and strategy S− to papers from the other, then we expect to observe a diﬀerence in opinions of initiating reviewers between the groups, because in the initiating part of the strategies we target diﬀerent kind of reviewers. Of course, the initiating part may simultaneously induce a diﬀerence in the populations of reviewers participating in the discussion. However, the balancing part of the strategies aims to compensate for it. Overall, by applying diﬀerent strategies (S+ versus S−) to the two groups of papers, we expect to obtain a diﬀerence in the opinions of discussion initiators, keeping the population of reviewers participating in discussions the same in both groups.
As a result, the intervention we perform in this experiment reduces the test for herding to a test for equality of acceptance rates between the two groups of papers. Indeed, if the herding eﬀect is absent, the choice of strategy should not impact the outcome of papers disproportionately across the two groups treated with diﬀerent strategies and hence we should not expect to observe a diﬀerence in acceptance rates between conditions. On the other hand, our intervention is designed to induce the largest possible diﬀerence between opinions of discussion initiators across the two groups. Given that under the presence of herding we expect the consensus opinion of reviewers to shift towards the opinion of the initiator of discussion, we expect the papers treated with strategy S+ to enjoy a higher acceptance rate than their counterparts treated with S−.
Of course, the reduction of the complex research question to a standard test of the diﬀerence in acceptance rates depends on the assumptions that mirror the idiosyncrasies formulated in Section 2.1:
A1 Our intervention does not introduce any diﬀerence across two groups of papers other than in the opinion of the discussion initiator (e.g., does not introduce a diﬀerence in the distributions of the participating reviewers or in some other characteristics of the discussion).
A2 Our intervention is successful in changing the order in which reviewers join the discussion of papers from diﬀerent groups.
Intuitively, we designed the intervention to ensure that these assumptions are satisﬁed. However, a priori we cannot guarantee that these assumptions always hold. For example, Assumption A1 breaks if the balancing part of the strategies fails to equalize the populations of participating reviewers across conditions. In Section 3 we provide data-based evidence that support these assumptions.
2.3 Details of the Experimental Procedure
Having designed the intervention, we now discuss how the set of participating papers was constructed and outline the details of the organization of the experiment.
5

Participating Papers. To have a high detection power, we would like to run the experiment using all the papers submitted to the ICML 2020 conference. However, the issue with using all the papers is that some reviewers may be the most positive or the most negative reviewers for multiple papers, being overburdened with requests to initiate (contribute to) the discussion of these papers.
To limit the additional load on reviewers induced by our experiment, for each reviewer we limit the number of papers the reviewer is asked to initiate the discussion or contribute to the discussion to one each. This condition puts the limit on the number of papers we can use in the experiment. Consequently, to compensate for the potential decrease of power, we focus the scope of the experiment on the borderline papers with some disagreement between reviewers as we expect the eﬀect (if any) to be the most prominent in these papers. In particular, we do the following:
• First, we use data from the past editions of the ICML conference and scores given in initial reviews to identify the borderline papers: papers that are not clear accepts or clear rejects (more details on the choice of papers are given in Appendix A).
• Second, we exclude all papers that received identical overall scores in initial reviews from all reviewers as for these papers the notions of the most positive and most negative reviewers are undeﬁned.
• Finally, from the papers not eliminated in the previous two steps, we ﬁnd a subset of 1,544 participating papers M such that each reviewer is the most positive reviewer for at most one of these papers and the most negative reviewer for at most one of these papers.
For each strategy S ∈ {S+, S−}, we refer to the reviewer who is asked to initiate the discussion as an initiating reviewer and the reviewer who is requested in the balancing part of the strategy as a balancing reviewer. We now uniformly at random allocate papers from the set M into two groups subject to the aforementioned constraint that each reviewer is the initiating reviewer for at most one paper and the balancing reviewer for at most one paper:
M+: The positive group which includes 755 papers treated with strategy S+
M−: The negative group which includes 789 papers treated with strategy S−
Having described the set of participating papers, we are now ready to discuss the timeline of the experiment.
Organization of the Experiment. Figure 1 depicts the pipeline implemented in the experiment. To ensure that initiating reviewers have suﬃcient time to initiate the discussion, we ﬁrst open the discussion interface without notifying the general pool of reviewers, and send requests to initiating reviewers only. Two days after the initial requests, we notify all other reviewers that the discussion stage is open. Finally, we email balancing reviewers in two stages: ﬁrst, we send emails only for papers with initiated discussion, giving initiating reviewers of papers with no discussion initiated a little more time to start it. We then complete the intervention by targeting the remaining balancing reviewers irrespective of whether the initiating reviewers of the corresponding papers fulﬁlled our request or not.
Through the ﬁrst ten days of the experiment, we send reminders to the initiating and balancing reviewers who have not fulﬁlled our request to initiate or contribute to the discussion of the corresponding papers. In order to avoid a disproportional impact on the discussion participants across two groups of papers, we ensure that the total number of reminders is the same for the initiating and balancing reviewers. Importantly, the research hypothesis we evaluate in this paper may be sensitive to the awareness of the subjects, so in this study we employ deception and do not notify subjects (reviewers and area chairs) about the experiment.
Before we proceed to the results of the experiment, we note that three members of the study team were involved in the ICML decision-making process. NS served as an area chair and AS and HD were program chairs. To avoid the conﬂict of interests, NS, AS and HD were not aware of what papers were used in the experiment. Moreover, we excluded papers chaired by NS from the analysis.
6

Figure 1: Timeline of the experiment. Day X is the day of the oﬃcial discussion opening.

3 Results of the Experiment
In this section we present the results of the experiment. First, we report data that supports the assumptions we made in Section 2.2. In that, we begin with some general statistics on the discussion process that support Assumption A1 (Section 3.1). We then discuss the eﬃcacy of the intervention we employed (Section 3.2) and show that Assumption A2 is well-satisﬁed. Finally, we conclude with the analysis of the research question we study in this work (Section 3.3).

3.1 Preliminary Analysis (Data in Support of Assumption A1)
Table 1 provides some comparative statistics on the discussion process for the papers involved in the experiment and treated with strategies S+ or S−. Observe in Table 1 that the parameters of the discussion are similar across the two conditions (that is, similar for M+ and M−). This provides quantitative evidence that the randomization of papers to conditions occurred successfully and Assumption A1 is satisﬁed. Rows 4 and 5 compare mean overall scores (the overall score takes integer values from 1 to 6 where larger values indicate higher quality) given by reviewers in the initial reviews, that is, before reviewers got to see the other reviews and the author feedback. Mean initial scores given by reviewers who participate in the discussion (Row 5) appear to be lower than mean scores computed over all reviewers (Row 4), suggesting that those who give lower scores are more active in discussing papers. However, there is no signiﬁcant diﬀerence between the positive (M+) and negative (M−) groups of papers in these values. Hence, the data indicates that this trend is independent of the choice of the strategy.

Comparative Statistics on the Discussion Process

1. Number of papers
2. Fraction of papers with active discussion
3. Mean discussion length (# messages)
4. Mean initial score (all revs)
5. Mean initial score (revs in discussion)
6. Standard deviation of initial scores (all revs) 7. Fraction of papers with R+ active in discussion 8. Fraction of papers with R− active in discussion
9. Mean number of discussion participants (revs + area chairs)

M+
755 0.97 4.41 3.52 3.44 1.12 0.79 0.87 3.14

M−
789 0.97 4.24 3.52 3.46 1.11 0.79 0.84 3.06

Table 1: Comparison of some discussion statistics between papers treated with diﬀerent discussionmanagement strategies. Except Row 5, all values are computed using all papers including those with no discussion. Permutation test at the level 0.05 (before multiple-testing adjustment) does not reveal signiﬁcant diﬀerences between conditions.

7

Number of papers

300
250
200
150
100
50
0 0Number o5f messag1e0s in the 1d5iscussio2n0+
Figure 2: Distribution of the discussion length over all papers M used in the experiment.
Importantly, the activity of reviewers R+ and R− in the discussion (Rows 7 and 8) is similar across the two groups of papers. This evidence shows that the choice of the discussion-management strategy does not introduce a diﬀerence across conditions in the distributions of reviewers who participate in the discussion. Finally, we observe that most of the papers used in the experiment had some discussion (Rows 2 and 3). Figure 2 presents the distribution of the discussion length over all participating papers, revealing that a nontrivial amount of discussion takes place behind the scenes, thereby conﬁrming that the experiment creates appropriate conditions for the herding behaviour to manifest.
3.2 Eﬃcacy of the Instrument (Data in Support of Assumption A2)
In the previous section we demonstrated that our intervention did not introduce a diﬀerence across conditions in metrics such as intensity of discussions and the population of participating reviewers. This observation supports Assumption A1 and indicates the appropriateness of our intervention. However, in order for the experiment to be successful from the testing standpoint, the intervention needs to satisfy Assumption A2 and introduce a diﬀerence across conditions in the order in which reviewers join the discussion of the papers. Indeed, if all the emails we sent to reviewers were ignored (i.e., our attempt to impact the order failed), the subsequent analysis will not detect the phenomena even when the phenomena is present.
Table 2 reports relevant statistics and indicates a large diﬀerence between positive and negative groups of papers, suggesting that our intervention did indeed impact the order in which reviewers joined the discussion. Importantly, Row 1 demonstrates that initiators of discussions for papers from diﬀerent groups have considerably diﬀerent opinions about the papers. Overall, we conclude that our intervention is eﬃcacious and satisﬁes the prerequisite for powerful testing (Assumption A2).

Does the Intervention Affect who Initiates the Discussion?

M+ M−

∆

∆ 95% CI p value

1. Mean initial score (initiator)

4.03 2.76 1.27

[1.15, 1.39]

2. Fraction of discussions initiated by R+ 0.53 0.09 0.44

[0.39, 0.48]

3. Fraction of discussions initiated by R− 0.15 0.59 −0.44 [−0.48, −0.39]

< .001 < .001 < .001

Table 2: The impact of the intervention on who initiates the discussion. To compute values for Row 1, we use 1,140 papers for which (i) the discussion was initiated, and (ii) the discussion initiator was a reviewer (and not the area chair). For the last two rows, we use all papers including those with no discussion. Bootstrapped conﬁdence intervals are constructed for the diﬀerence of the relevant quantities between conditions. All p values are computed using the permutation test with 10,000 iterations.

8

Does the Intervention Affect the Outcome of Papers?

1. Acceptance rate 2. Change in mean score (initiator) 3. Change in mean score (all revs) 4. Change in mean score (revs in discussion) 5. Change in standard deviation of scores (all revs)

M+
0.21 −0.10 0.01 0.03 −0.23

M−
0.25 0.20 0.01 0.02 −0.21

∆
−0.04 −0.30 0.00 0.01 −0.02

∆ 95% CI
[−0.08, 0.01] [−0.37, −0.23] [−0.03, 0.04] [−0.04, 0.06] [−0.05, 0.02]

p value
.122 < .001
.949 .697 .296

Table 3: The impact of the intervention on the ﬁnal outcome of papers. For Row 2, we use 1,140 papers for which (i) the discussion was initiated, and (ii) the discussion initiator was a reviewer (and not the area chair). For Row 4, we use papers with discussion. For all other rows, we use all papers including those with no discussion. Bootstrapped conﬁdence intervals are constructed for the diﬀerence of the relevant quantities between conditions. All p values are computed using the permutation test with 10,000 iterations.

3.3 Main Analysis
Having demonstrated that the intervention we implemented in the experiment reasonably satisﬁes the required assumptions, we now continue to the analysis directly related to the research question we study in this work. Speciﬁcally, as we explained in the introduction and in Section 2, if herding behaviour exists, we expect it to manifest in the ﬁnal decisions being disproportionately inﬂuenced by the opinion of the discussion initiator. Hence, given that for the positive group of papers M+ the initial opinion of the discussion initiator was on average signiﬁcantly more positive than that of initiators of discussions for the negative group of papers M−, we expect to observe a disparity in the eventual acceptance rates between conditions.
Table 3 formalizes the intuition and performs the comparison of acceptance rates across papers treated with diﬀerent strategies (Row 1). Additionally, Table 3 displays the updates of the scores made by reviewers (Rows 2–5). First, the data does not indicate a statistically signiﬁcant diﬀerence between acceptance rates in the two groups of papers (M+ versus M−). Second, the data on the score updates suggests that in their ﬁnal evaluations, reviewers tend to converge to the mean of initial independent opinions irrespective of the discussion-management strategy employed. Indeed, Row 2 demonstrates that the initiators of discussions update the scores towards the mean of all initial scores. Next, Rows 3 and 4 show that a signiﬁcant update made by the discussion initiators is compensated by the update made by other reviewers, such that the overall amount of change in the mean scores is negligible. As expected, the outlined dynamics result in a signiﬁcant decrease in the variance of scores per paper, but the eﬀect is the same for both groups of papers (Row 5).
Overall, the comparison of acceptance rates and changes in the mean reviewers’ scores does not reveal any signiﬁcant diﬀerence between the papers treated with diﬀerent discussion-management strategies. Hence, we ﬁnd no evidence of herding in the discussion phase of ICML 2020 peer review.
4 Discussion
The experiment we conducted in the present work aims at identifying the herding behaviour in the discussions of the ICML 2020 conference. The results presented in Section 3 show that while we managed to achieve an imbalance in the opinion of the discussion initiators across conditions, the diﬀerence in the acceptance rates is not signiﬁcant and hence there is no evidence of herding. In this section, we provide an additional discussion on certain aspects of our experiment.
4.1 Caveats Regarding the Design and Analysis of the Experiment
Given that our intervention induced the strong diﬀerence in the order in which reviewers joined the discussions (see Table 2), the absence of diﬀerence in score updates (see Table 3) allows us to conclude with a high degree

9

of conﬁdence that the choice of the discussion-management strategy (S+ versus S−) does not impact the way reviewers update their scores. Of course, herding (if present) does not necessarily need to manifest in how reviewers change their scores after the discussion. Instead, it can change some other characteristics such as what reviewers write in the textual messages which are later analyzed by the area and program chairs who make the ﬁnal decisions. To account for these potential manifestations, we compared acceptance rates between the groups of papers (see Row 1 of Table 3) and observed some diﬀerence in this quantity. However, this diﬀerence does not appear signiﬁcant despite the large sample size we had in the experiment, suggesting that even if present, the eﬀect has at most small size. That being said, we urge the reader to be aware of the following caveats.
Caveat 1. The design of the intervention. Recall that our research question deﬁnes herding as a conditional dependence of the outcome of a paper on the choice of the discussion initiator. The test and the intervention we designed attempt to compare the outcomes of papers when the discussion is initiated by the most positive versus the most negative reviewers with the motivation that this diﬀerence is expected to be the largest in the presence of herding. Strictly speaking, the absence of a diﬀerence between these choices of the initiators does not imply the absence of the diﬀerence between any other choices of the initiators: for example, it is possible that the outcome of a paper would be impacted diﬀerently if we asked the reviewer with a non-extreme score to initiate the discussion.
Caveat 2. The choice of papers. As noted in Section 2.3, in this experiment we tried to identify a set of borderline papers as these papers are more susceptible to the impact of the herding eﬀect if it is present. However, our choice of the borderline papers was based on some indirect indicators and hence we could potentially fail to uncover the set of true borderline papers which would reduce the power of our test.
To evaluate our choice of borderline papers, we use a rough classiﬁcation of submissions into clear and borderline cases made by the area chairs. Note that this classiﬁcation was performed after the discussion stage which could resolve the uncertainty present before the discussion stage when we selected the participating papers. Hence, the fraction of borderline papers in the area chairs’ classiﬁcation is a conservative estimate of the pre-discussion fraction of borderline papers. Nonetheless, 30% of submissions used in the experiment were classiﬁed by the area chairs as borderline cases in contrast to 18% of those not involved in the experiment (∆ = 0.12, p = .002). Hence, our choice of the borderline papers was better than random and the set of the participating papers M contained a large fraction of papers for which the decisions were not clear before the discussion.
Caveat 3. Validity of Assumption A1. The validity of the conclusions we make is based on the assumptions formulated in Section 2.2. Note that a violation of the assumptions could increase the false alarm probability or could reduce the power of the test. The data we presented in Section 3.1 and Section 3.2 strongly supports Assumptions A1 and A2. However, as a note of caution, we remark that there is some space for potential violations of Assumption A1. Indeed, in Table 1 we establish that the marginal values of relevant indicators of discussion activity are similar across groups. However, this observation does not imply that the value of these indicators for each paper would not change if that paper was placed in the other condition (recall the thought experiment in Section 2.2). Hence, the the outcome of this study should be considered together with this opportunity for the violation of Assumption A1.
Caveat 4. Spurious correlations induced by reviewer identity. In peer review, each reviewer participates in the discussion of multiple papers. Similarly, each area chair manages several papers. Hence, strictly speaking, the outcomes of two papers that have at least one reviewer in common (are managed by the same area chair) may not be statistically independent due to correlations introduced by the reviewer (area chair) identities. This issue puts a strain on the testing procedure because in contrast to the vanilla A/B testing framework which assumes that samples are independent of each other, in our case we receive correlated samples. In the domain of empirical studies of the peer-review procedure (Shah et al., 2018; Tomkins et al., 2017; Lawrence and Cortes, 2014) such spurious correlations are usually tolerated, because otherwise the sample size would be negligible. Additionally, simulations performed by Stelmakh et al. (2019) demonstrate that unless reviewers are involved in the discussion of dozens of submissions, the impact of such spurious correlations is limited.
Nevertheless, in this work we take some additional steps to minimize the impact of these spurious cor-
10

relations. To this end, we simultaneously also perform the analysis on a subset of 937 papers M∗ ⊂ M which is constructed such that each reviewer is the the most positive or the most negative reviewer for at most one paper from set M∗ in total.2 This additional reduction of the sample size allows us to limit the impact of the reviewer identity on the outcome of submissions. Of course, by doing so we do not guarantee that there is no reviewer who participates in the discussion of more than one paper from the set M∗, but we guarantee that the discussion participants who are targeted by our strategies are unique. Appendix A gives more details on how sets M and M∗ were constructed. The results of this additional analysis are presented in Appendix B and lead to the same conclusions as in Section 3.
Caveat 5. Opinion of the discussion initiator. In this work we used the scores given by reviewers in the initial reviews to infer the pre-discussion opinion of reviewers and assumed that reviewers begin the discussion from advocating these opinions. However, the fact that a reviewer has some pre-discussion opinion does not guarantee that they advocate the same position in the discussion because the latter is also inﬂuenced by other reviewers’ reviews and the author feedback. Indeed, past research (Teplitskiy et al., 2019) suggests that reviewers do listen to each other and may update their initial independent opinions in light of opinions expressed in initial reviews of other reviewers. The data we obtained in the experiment suggests that while such updates take place, their magnitude is small enough and does not break our intervention. Indeed, Row 2 of Table 3 and Row 1 of Table 2 indicate that reviewers with extreme pre-discussion opinions remain on the diﬀerent sides of the mean pre-discussion group opinion even according to the ﬁnal scores. Thus, we conclude that our intervention succeeded in creating a diﬀerence in opinions of discussion initiators across groups.
Caveat 6. Alternative model of herding. In this paper we assume that the herding behaviour in peer review manifests in ﬁnal decisions being moved towards the position of the reviewer who initiates the discussion. However, the data presented in Table 3 shows that initiators of the discussion tend to slightly update their scores towards the mean of initial scores given by all reviewers. Hence, an alternative model of the herding behaviour is that the sentiment demonstrated by the initiating reviewer carries over to other reviewers who could change their behaviour accordingly. For example, the positive score update of initiators with a negative initial opinion may demonstrate a positive sentiment, which could aﬀect opinions of other reviewers in a positive way. Under this alternative model of herding, we would expect papers from the negative group M− to enjoy a higher acceptance rate than their counterparts from the positive group M+. While this agrees with the observed acceptance rates reported in Table 3, we reiterate that the diﬀerence between the acceptance rates is not signiﬁcant and the eﬀect size is small, so our test does not provide evidence in support of this alternative model either.
4.2 Relation to Past Work
Past literature suggests the presence of herding behaviour in group discussions. McGuire et al. (1987) observe that the ﬁrst solution proposed to a group predicts the group decision better than an aggregate of initial opinions independently expressed in a pre-discussion survey. Dubrovsky et al. (1991) document an impact of the interplay between the status of discussion participants and the opinion of the group member who proposed the ﬁrst concrete solution on the ﬁnal group decision. Closest to the present work, Weisband (1992) further investigates the herding eﬀect in a semi-randomized controlled trial and declares that the initiators of discussion manage to inﬂuence the group opinion when they step in after an initial general discussion of the problem that is, when they have some understanding of the general opinions of other discussants, but no concrete decisions have been proposed.
In contrast, in this experiment, we did not detect herding in the peer-review discussion. Let us now discuss the relationship of the current experiment to these past works. First, we note that the papers of McGuire et al. (1987) and Dubrovsky et al. (1991) study the herding eﬀect when the discussion initiators are self-selected. The diﬀerence between the self-selected and assigned initiators appears to be signiﬁcant, because the former may be associated with other personal qualities such as assertiveness and energy. Hence,
2Compare to M which is constructed such that each reviewer is the the most positive or the most negative reviewer for at most one paper each.
11

our work is not directly comparable to these studies as we attempt to randomize the identity of the discussion initiator.
The experiment of Weisband (1992) employs randomization and in addition to the self-selection scenario considers the setup in which the ﬁrst person to propose the solution to the group is chosen uniformly at random. This work ﬁnds that the randomly assigned initiator exerts much smaller inﬂuence on the group decision than the self-selected initiator. We caveat however, that in the experiment of Weisband (1992), the fact that the initiator is selected at random was known to the whole group before the beginning of the discussion. Hence, it is plausible that other group members did not perceive the initiator as a leader and could adjust their behaviour accordingly (Weisband, 1992). In contrast, in the present experiment the non-initiating reviewers were not aware of the intervention and hence from their point of view the assigned initiator of the discussion possessed all the properties of the self-selected initiator (Hollander, 1978).
Finally, there is a subtle diﬀerence between the deﬁnition of the herding eﬀect made by Weisband (1992) and the deﬁnition we use in this paper. According to Weisband (1992), the herding is present when the ﬁrst solution formulated in the group discussion predicts the group ﬁnal decision better than the mean of the pre-discussion independent opinions. Note that according to this deﬁnition, the herding may be present even if the ﬁrst solution proposed to the group is independent of who is selected to formulate this opinion, that is, even when all discussants would propose the same solution should they be selected to start the discussion. In contrast, in our settings it is natural to deﬁne herding to be present only when the opinion of the discussion initiator is diﬀerent depending on who is selected to initiate the discussion, because the goal of the present work is to inform the area chairs about the potential consequences of their discussion initiating strategy.
In addition to the aforementioned distinctions from the past work, we note that in the present experiment reviewers are engaged in a much more analytical task as compared to the previous works in which some toy problems were used to study the discussion dynamics. Hence, the absence of the herding behaviour in peer review may be due to the fact that reviewers have a rational mindset which is hypothesized to reduce a reliance on heuristics responsible for various cognitive biases (Stanovich, 1999; Kahneman and Frederick, 2002).
Beyond testing for herding, in this paper we also document eﬀects predicted by past works on discussion in peer review (Teplitskiy et al., 2019; Hofer et al., 2000): reviewers tend to update their scores towards the consensus pre-discussion opinion, and the discussion increases the agreement among reviewers. Coupled with the observation that an increased agreement does not necessarily result in an increased accuracy of the decision (Hofer et al., 2000), our observations highlight an importance of additional research on the discussion dynamics in peer review.
Acknowledgments
We thank Edward Kennedy for useful comments on the initial design of this study. We are also grateful to the support team of the Microsoft Conference Management Toolkit (CMT) for their continuous support and help with multiple customization requests. Finally, we appreciate the eﬀorts of all reviewers and area chairs involved in the ICML 2020 review process. This study was approved by Carnegie Mellon University Institutional Review Board.
This work was supported in part by NSF CAREER award 1942124 and in part by NSF CIF 1763734.
References
Asch, S. E. (1951). Eﬀects of group pressure upon the modiﬁcation and distortion of judgments. In Guetzkow, H., editor, Groups, leadership and men; research in human relations, page 177–190. Carnegie Press.
Banerjee, A. V. (1992). A simple model of herd behavior. QUART. J. ECONOM, 107(3):797–818.
Baron, R. S., Vandello, J. A., and Brunsman, B. (1996). The forgotten variable in conformity research:
12

Impact of task importance on social inﬂuence. Journal of Personality and Social Psychology, 71(5):915– 927.
Bond, R., Fariss, C., Jones, J., Kramer, A., Marlow, C., Settle, J., and Fowler, J. (2012). A 61-million-person experiment in social inﬂuence and political mobilization. Nature, 489:295–8.
Cialdini, R. and Goldstein, N. (2004). Social inﬂuence: Compliance and conformity. Annual review of psychology, 55:591–621.
Dubrovsky, V. J., Kiesler, S., and Sethna, B. N. (1991). The equalization phenomenon: Status eﬀects in computer-mediated and face-to-face decision-making groups. Human–Computer Interaction, 6(2):119–146.
Fiez, T., Shah, N., and Ratliﬀ, L. (2020). A SUPER* algorithm to optimize paper bidding in peer review. In Conference on Uncertainty in Artiﬁcial Intelligence.
Galton, F. (1907). Vox populi. Nature, 75:450–451.
Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal matter? Insights from a major NLP conference. CoRR, abs/1903.11367.
Garg, N., Kavitha, T., Kumar, A., Mehlhorn, K., and Mestre, J. (2010). Assigning papers to referees. Algorithmica, 58(1):119–136.
Hofer, T. P., Bernstein, S. J., DeMonner, S., and Hayward, R. A. (2000). Discussion between reviewers does not improve reliability of peer review of hospital quality. Medical Care, 38(2):152–161.
Hollander, E. (1978). Leadership Dynamics: A Practical Guide to Eﬀective Relationships. Free Press/Macmillan.
Hua, X., Nikolov, M., Badugu, N., and Wang, L. (2019). Argument mining for understanding peer reviews. arXiv preprint arXiv:1903.10104.
Janis, I. (1982). Groupthink: Psychological Studies of Policy Decisions and Fiascoes. Houghton Miﬄin.
Jecmen, S., Zhang, H., Liu, R., Shah, N. B., Conitzer, V., and Fang, F. (2020). Mitigating manipulation in peer review via randomized reviewer assignments. In NeurIPS.
Kahneman, D. and Frederick, S. (2002). Representativeness revisited: Attribute substitution in intuitive judgment. Heuristics and biases: The psychology of intuitive judgment, 49:49–81.
Kang, D., Ammar, W., Dalvi, B., van Zuylen, M., Kohlmeier, S., Hovy, E. H., and Schwartz, R. (2018). A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. CoRR, abs/1804.09635.
Kobren, A., Saha, B., and McCallum, A. (2019). Paper matching with local fairness constraints. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
Lawrence, N. and Cortes, C. (2014). The NIPS experiment. http://inverseprobability.com/2014/12/ 16/the-nips-experiment. [Accessed: 05/30/2020].
Lorenz, J., Rauhut, H., Schweitzer, F., and Helbing, D. (2011). How social inﬂuence can undermine the wisdom of crowd eﬀect. Proceedings of the National Academy of Sciences of the United States of America, 108(22):9020–9025.
Manzoor, E. and Shah, N. B. (2020). Uncovering latent biases in text: Method and application to peer review. In INFORMS Workshop on Data Science.
McGuire, T. W., Kiesler, S., and Siegel, J. (1987). Group and computer-mediated discussion eﬀects in risk decision making. American Psychological Association, 52:917–930.
13

Meir, R., Lang, J., Lesca, J., Kaminsky, N., and Mattei, N. (2020). A market-inspired bidding scheme for peer review paper assignment. In Games, Agents, and Incentives Workshop at AAMAS.
Mussweiler, T. and Strack, F. (2001). Considering the impossible: Explaining the eﬀects of implausible anchors. Social Cognition - SOC COGNITION, 19:145–160.
Nofsinger, J. R. and Sias, R. W. (1999). Herding and feedback trading by institutional and individual investors. The Journal of Finance, 54(6):2263–2295.
Noothigattu, R., Shah, N. B., and Procaccia, A. D. (2020). Loss functions, axioms, and peer review. In ICML workshop on Incentives in Machine Learning.
Resnik, D. B. and Smith, E. M. (2020). Bias and Groupthink in Science’s Peer-Review System, pages 99–113. Springer International Publishing, Cham.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and Von Luxburg, U. (2018). Design and analysis of the NIPS 2016 review process. The Journal of Machine Learning Research, 19(1):1913–1946.
Stanovich, K. (1999). Who Is Rational? Studies of Individual Diﬀerences in Reasoning. Mahwah, NJ: Erlbaum.
Stelmakh, I., Shah, N., and Singh, A. (2019). On testing for biases in peer review. In NeurIPS. Stelmakh, I., Shah, N. B., and Singh, A. (2018). PeerReview4All: Fair and accurate reviewer assignment in
peer review. arXiv preprint arXiv:1806.06237. Stelmakh, I., Shah, N. B., and Singh, A. (2020). Catch me if i can: Detecting strategic behaviour in peer
assessment. In ICML Workshop on Incentives in Machine Learning. Strack, F. and Mussweiler, T. (1997). Explaining the enigmatic anchoring eﬀect: Mechanisms of selective
accessibility. Journal of Personality and Social Psychology, 73:437–446. Surowiecki, J. (2005). The Wisdom of Crowds. Anchor. Teplitskiy, M., Ranub, H., Grayb, G. S., Meniettid, M., Guinan, E. C., and Lakhani, K. R. (2019). Social
inﬂuence among experts: Field experimental evidence from peer review. Tomkins, A., Zhang, M., and Heavlin, W. D. (2017). Reviewer bias in single- versus double-blind peer review.
Proceedings of the National Academy of Sciences, 114(48):12708–12713. Tversky, A. and Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science,
185(4157):1124–1131. Wang, J. and Shah, N. B. (2019). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in
ratings. In AAMAS. Weisband, S. P. (1992). Group discussion and ﬁrst advocacy eﬀects in computer-mediated and face-to-face
decision making groups. Organizational Behavior and Human Decision Processes, 53(3):352 – 380. Xu, Y., Zhao, H., Shi, X., and Shah, N. (2019). On strategyproof conference review. In IJCAI.
14

Appendix
We provide supplementary materials and additional discussion. In Appendix A we formalize the rules we used to identify the set of borderline papers when constructing the set of participating papers M and M∗ introduced in Section 2.3 and Section 4.1, respectively. Appendix B is dedicated to the additional analysis on the subset of papers M∗.
A Construction of Sets M and M∗
In this section we specify how the sets M (introduced in Section 2.3) and M∗ (introduced in Caveat 4, Section 4.1) of participating papers were constructed from the set of all m = 4625 papers not withdrawn from ICML 2020 by the beginning of the discussion period.
We begin from set M and recall that our goal is to identify a subset of borderline papers such that each reviewer is the most positive reviewer for at most one paper from this subset and the most negative reviewer for at most one paper from this subset. To this end, we perform the following two-step procedure:
Step 1. First, we identify the set of borderline papers as follows. The overall scores given in the initial reviews were in the set {1, 2, . . . , 6} so for each paper i ∈ [m], we let λi to denote the number of reviewers assigned to the paper (typically λi equals 3 or 4) and let (θ1, θ2, . . . , θλi ) ∈ {1, 2, . . . , 6}λi to denote the collection of overall scores given to paper i in initial reviews.3 With this notation, using acceptance statistics of the ICML 2019 conference, we construct a set of borderline papers T by identifying submissions that satisfy the following criteria:
C1 The mean overall score is such that in ICML 2019 the paper is in the borderline category:
1 λi λi θj ∈ [2.7, 4.5].
j=1
C2 The minimum and maximum overall scores are on the diﬀerent sides of the decision spectrum:
max(θ1, θ2, . . . , θλi ) ≥ 4 and min(θ1, θ2, . . . , θλi ) ≤ 3.
Note that for each borderline paper i ∈ T we are guaranteed that there is some disagreement between reviewers.
Step 2. Having the set of borderline papers T deﬁned, we construct M by greedily ﬁnding a subset of T that satisﬁes the requirement of each reviewer being the most positive reviewer for at most one paper from this subset and the most negative reviewer for at most one paper from this subset.
Let us now proceed to the deﬁnition of set M∗ ⊂ M which has an additional property of each reviewer being the most positive or the most negative reviewer for at most one paper in total. This set is also constructed by greedily ﬁnding a subset of M that satisﬁes this requirement. However, to facilitate tiebreaking, we additionally request that for each paper i ∈ M∗ the most positive and most negative reviewers disagree in the initial reviews by at least 2 points:
max(θ1, θ2, . . . , θλi ) − min(θ1, θ2, . . . , θλi ) ≥ 2.
Hence, set M∗ has an additional property of containing papers with high disagreement between reviewers in the initial reviews.
3Here, we adopt the standard notation [ν] = {1, 2, . . . , ν} for any positive integer ν.
15

B Additional Analysis
Recall that the set of papers M∗ (deﬁned in Appendix A) constitutes a subset of all the participating papers M and additionally satisﬁes the constraints of (i) each reviewer serving as the initiating or balancing reviewer for at most one paper from M∗ in total and (ii) each paper i ∈ M∗ being such that the most positive and the most negative reviewers disagree in the initial reviews by at least two points. In this section, we replicate the analysis presented in Section 3 conditioning on this subset of papers.

Comparative Statistics on the Discussion Process

1. Number of papers
2. Fraction of papers with active discussion
3. Mean discussion length (# messages)
4. Mean initial score (all revs)
5. Mean initial score (revs in discussion)
6. Standard deviation of initial scores (all revs) 7. Fraction of papers with R+ active in discussion 8. Fraction of papers with R− active in discussion
9. Mean number of discussion participants (revs + area chairs)

M∗+
460 0.96 4.52 3.51 3.42 1.22 0.78 0.87 3.16

M∗−
477 0.98 4.25 3.52 3.45 1.20 0.79 0.85 3.10

Table 4: Comparison of some discussion statistics between papers treated with diﬀerent discussionmanagement strategies, conditioned on the subset of papers M∗. Except Row 5, all values are computed using all papers including those with no discussion. Permutation test at the level 0.05 (before multiple-testing adjustment) does not reveal signiﬁcant diﬀerences between conditions.

Mirroring the analysis on the full set of participating papers (Table 1), Table 4 indicates that the parameters of the discussion are similar across the two conditions. Hence, we also conclude that data supports Assumption A1 and the intervention did not result in a diﬀerence across conditions in the distributions of reviewers who participate in the discussion.
Next, we investigate the eﬃcacy of our intervention and proceed to Table 5 that compares relevant statistics. Observe that the values in Table 5 are very similar to those reported in Table 2, suggesting that the intervention continues to introduce the required diﬀerence in opinions of discussion initiators between the groups of papers even when we zoom in on the target subset of papers M∗. Thus, we conclude that Assumption A2 continues to hold.
Having conﬁrmed the eﬃcacy of the intervention, we proceed to the comparison of the outcomes of submissions. The results presented in Table 6 mimic those reported in Table 3. The most notable diﬀerence

Does the Intervention Affect who Initiates the Discussion?

M∗+ M∗−

∆

∆ 95% CI p value

1. Mean initial score (initiator)

4.09 2.69 1.40

[1.24, 1.55]

2. Fraction of discussions initiated by R+ 0.53 0.11 0.42

[0.36, 0.47]

3. Fraction of discussions initiated by R− 0.16 0.60 −0.44 [−0.49, −0.38]

< .001 < .001 < .001

Table 5: The impact of the intervention on who initiates the discussion, conditioned on the subset of papers M∗. To compute values for Row 1, we use 1,140 papers for which (i) the discussion was initiated, and (ii) the discussion initiator was a reviewer (and not the area chair). For the last two rows, we use all papers including those with no discussion. Bootstrapped conﬁdence intervals are constructed for the diﬀerence of the relevant quantities between conditions. All p values are computed using the permutation test with 10,000 iterations.

16

Does the Intervention Affect the Outcome of Papers?

1. Acceptance rate 2. Change in mean score (initiator) 3. Change in mean score (all revs) 4. Change in mean score (revs in discussion) 5. Change in standard deviation of scores (all revs)

M∗+
0.21 −0.11 0.01 0.02 −0.26

M∗−
0.26 0.21 0.01 0.02 −0.25

∆
−0.05 −0.32 0.00 0.00 −0.01

∆ 95% CI
[−0.11, 0.00] [−0.42, −0.22] [−0.05, 0.05] [−0.06, 0.07] [−0.06, 0.03]

p value
.079 < .001
.925 .867 .560

Table 6: The impact of the intervention on the ﬁnal outcome of papers, conditioned on the subset of papers M∗. For Row 2, we use 1,140 papers for which (i) the discussion was initiated, and (ii) the discussion initiator was a reviewer (and not the area chair). For Row 4, we use papers with discussion. For all other rows, we use all papers including those with no discussion. Bootstrapped conﬁdence intervals are constructed for the diﬀerence of the relevant quantities between conditions. All p values are computed using the permutation test with 10,000 iterations.

is that the signiﬁcance of the diﬀerence in acceptance rates becomes closer to the threshold of 0.05, but still does not cross it. Given that the number of submissions involved in the experiment is large, we conclude that we do not observe strong evidence of the herding behaviour even after conditioning on the set of papers M∗.

17

