arXiv:1412.7584v1 [cs.LG] 24 Dec 2014

Diﬀerential Privacy and Machine Learning:
a Survey and Review
Zhanglong Ji, Zachary C. Lipton, Charles Elkan
December 25, 2014
Abstract
The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conﬂict is to extract general characteristics of whole populations without disclosing the private information of individuals.
In this paper, we consider diﬀerential privacy, one of the most popular and powerful deﬁnitions of privacy. We explore the interplay between machine learning and diﬀerential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for diﬀerentially private algorithms.
Finally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, diﬀerentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-diﬀerentially private algorithms.
The objective of machine learning is to extract useful information from data, such as how to classify data, how to predict a quantity, or how to ﬁnd clusters of similar samples. Given a family of learning models, machine learning algorithms select and output the best one based on some given data. The output model can be used in either dealing with future data or interpreting the distribution of data. Although the output model typically far more compact than the underlying dataset, it must capture some information describing the dataset. Privacy, on the other hand, concerns the protection of private data from leakage, especially the information of individuals.1
1 [9] uses privacy to mean both population privacy and individual privacy. For example, disclosing that members of some family are highly susceptible to a given genetic condition might violate population privacy, while disclosing that some speciﬁc patient suﬀers from this condition would violate their individual privacy. However, even if one conceals all his/her
1

It would be reasonable to ask, “why is it insuﬃcient to anonymize data?” One could remove names and other obviously identiﬁable information from a database. It might seem diﬃcult then, for an attacker to identify an individual. Current HIPAA guidelines promotes such an approach, listing 18 categories of personally identiﬁable information which must be redacted in the case of research data for publication. Unfortunately, this method can leak information when the attacker already has some information about the individuals in question. In a well-known case, the personal health information of Massachusetts governor William Weld was discovered in a supposedly anonymized public database [51]. By merging overlapping records between the health database and a voter registry, researchers were able to identify the personal health records of the governor, among others.
To combat such background attacks, some more robust deﬁnitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed. In these approaches, samples are grouped if their sensitive features are the same, and a group is published if the number of samples in that group is large enough. Intuitively, it should be diﬃcult for an attacker to distinguish individual samples. However, even these deﬁnitions cannot prevent background attacks, in which the attackers already know something about the information contained in the dataset. In an extreme case, the attacker might know the contents of all but one of the rows in the set.
Consider a database which holds the address and income of four people and publishes private data using 3-anonymity. According to 3-anonymity, if any three people live in the same city, the city and average income of the three people is released. Now suppose an attacker knows that two people in the database live in Los Angeles and that a third lives in New York. If no data is published, the attacker can easily infer that the fourth person does not live in Los Angeles.
Even if one released aggregated statistics, they might risk compromising private information. Recently [23], researchers demonstrated that an attacker could infer whether an individual had participated in a genome study using only publicly available aggregated genetic data. In such cases, aggregation is no longer safe.
Diﬀerential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn’t change much if one individual in the dataset changes.
As no individual sample can signiﬁcantly aﬀect the output, attackers cannot infer the private information corresponding to any individual sample conﬁdently. This paper addresses the interplay between machine learning and diﬀerential privacy.
Although it seems that machine learning and privacy protection are in opposition, it is often possible to reconcile them. Researchers have designed many mechanisms to build models that can capture the distributions corresponding to
information, it’s still possible to breach population privacy by collecting information from other members of the subpopulation. As one cannot easily protect oneself from the disclosure of this information, we will use privacy to refer only to individual privacy in this paper.
2

large datasets while guaranteeing diﬀerential privacy with respect to individual examples. In order to achieve generalizability, machine learning models should not depend heavily on any single sample. Therefore, it is possible to hide the eﬀects of individual samples, simultaneously preserving privacy and providing utility.
0.1 Prior Work
Several recent surveys address diﬀerential privacy and data science [17, 47, 22]. Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between diﬀerential privacy and machine learning.
Our survey focuses speciﬁcally on methods by which machine learning algorithms can be made diﬀerentially private. We study current diﬀerentially private machine learning algorithms and organize them according to the fundamental machine learning tasks they address, including classiﬁcation, regression, clustering, and dimensionality reduction. We also describe some diﬀerentially private data release mechanisms, both because their mechanisms involves differential privacy, and because their output can be used to learn diﬀerentially private machine learning models. We explain how all of these mechanisms work and compare their theoretical guarantees. Some general theoretical results and discussion follow.
1 Diﬀerential Privacy
Diﬀerential privacy is one of the most popular deﬁnitions of privacy today. Intuitively, it requires that the mechanism outputting information about an underlying dataset is robust to any change of one sample, thus protecting privacy.
The following subsections mathematically deﬁne diﬀerential privacy and introduce some commonly used methods in diﬀerential privacy.
1.1 Deﬁnition of Diﬀerential Privacy
Deﬁnition 1: a mechanism f˜ is a random function that takes a dataset D as input, and outputs a random variable f˜(D).
For example, suppose D is a medical dataset, then the function that outputs the number of patients in D plus noise from the standard normal distribution is a mechanism.
Deﬁnition 2: the distance of two datasets, d(D, D′), denotes the minimum number of sample changes that are required to change D into D′.
For example, if D and D′ diﬀer on at most one individual, there is d(D, D′) = 1. We also call such a pair of datasets neighbors.
The original deﬁnition of diﬀerential privacy deﬁnes as neighbors datasets which ‘diﬀer on at most one individual’. This phrasing has given rise to two diﬀerent understandings. Some interpret this as the replacement of a sample,
3

while others also consider addition and deletion. Although the second inter-

pretation is stronger than the ﬁrst one, most of the mechanisms discussed in

this paper work for both deﬁnitions if slightly modiﬁed. Diﬀerent deﬁnitions of

distance usually lead to diﬀerent values of sensitivity, while both are bounded.

In order to make a mechanism designed for one deﬁnition of distance work for

another deﬁnition of distance, we only need to make slight changes to the scale

of noise. Therefore we won’t distinguish them. Deﬁnition 3: a mechanism f˜ satisﬁes (ǫ, δ)-diﬀerential privacy [12, 13] for

two non-negative numbers ǫ and δ iﬀ for all neighbors d(D, D′) = 1, and all subset S of f˜’s range, as long as the following probabilities are well-deﬁned,

there holds

P (f˜(D) ∈ S) ≤ δ + eǫP (f˜(D′) ∈ S)

Intuitively speaking, the number δ represents the probability that a mechanism’s output varies by more than a factor of eǫ when applied to a dataset and any one of its neighbors. A lower value of δ signiﬁes greater conﬁdence and a smaller value of ǫ tightens the standard for privacy protection. The smaller ǫ and δ are, the closer P (f˜(D) ∈ S) and P (f˜(D′) ∈ S) are, and the stronger protection is.
There is also a commonly used heuristic to choose δ[20]: when there are n samples in the dataset, δ ∈ o(1/n). This is because a mechanism can satisfy (0, δ)-diﬀerential privacy but breach privacy with high probability when δ is large. For each sample in the dataset, the mechanism releases it with probability δ, and the release of diﬀerent samples are independently. It’s easy to prove the mechanism is diﬀerentially private. However by expectation, the mechanism release nδ samples from the dataset. To prevent such leakage, δ must be smaller than 1/n.
Typically, (ǫ, 0)-diﬀerential privacy is simpliﬁed to ǫ-diﬀerential privacy. With (ǫ, δ)-diﬀerential privacy, when δ > 0, there is still a small chance that some information is leaked. When δ = 0, the guarantee is not probabilistic. [11] shows that in terms of mtutal information, ǫ-diﬀerential privacy is much stronger than (ǫ, δ)-diﬀerential privacy.
In diﬀerential privacy, the number ǫ is also called the privacy budget.

1.2 Query
Usually, we want the output of a mechanism to be both diﬀerentially private and useful. By ‘useful’, we mean the output accurately answers some queries on the dataset. Deﬁnition 4 deﬁnes query below and in the following subsections, some mechanisms that guarantee diﬀerential privacy will be introduced.
Deﬁnition 4: a query f is a function that takes a dataset as input. The answer to the query f is denoted f (D).
For example, if D is a medical dataset, then ‘how many patients were successfully cured?’ is a query since it takes D as input and outputs a number. The output of a query is not necessarily a number. However, some mechanisms,
Ê notably the Laplacian mechanism, assume that answers to queries are numer-
ical, or vectors f (D) ∈ p but not categorical. A more sophisticated query

4

can be ‘a logistic regression model trained from the dataset’, which outputs a classiﬁcation model.

1.3 The Laplacian Mechanism

Ê The Laplacian mechanism[15] is a popular ǫ-diﬀerentially private mechanism for
queries f with answers f (D) ∈ p, in which sensitivity (Deﬁnition 5) plays an important role.
Deﬁnition 5: given a query f and a norm function . over the range of f , the sensitivity s(f, . ) is deﬁned as
s(f, . ) = max f (D) − f (D′)
d(D,D′)=1

Usually, the norm function . is either L1 or L2 norm.

The Laplacian mechanism[15]: given a query f and a norm function over the range of f , the random function f˜(D) = f (D) + η satisﬁes ǫ-diﬀerential

privacy. Here η is a random variable whose probability density function is

p(η) ∝ e−ǫ η /s(f, . ).

There is a variation of the Laplacian mechanism, which replaces Lapla-

cian noise with Gaussian noise. On one side, this replacement greatly reduces

the probability of very large noise; on the other side, it only preserves (ǫ, δ)-

diﬀerential privacy for some δ > 0, which is weaker than ǫ-diﬀerential privacy.

Variation of the Laplacian mechanism: given a query f and a distance function over the range of f , the random function f˜(D) = f (D) + η satis-

ﬁes (ǫ, δ)-diﬀerential privacy. Here η is a random variable from distribution

N

(0,

2 ǫ2

(s(f,

.

))2 log δ2 ) [2].

1.4 The Exponential Mechanism

The exponential mechanism[38] is an ǫ-diﬀerentially private method to select one element from a set. Suppose the set to select from is A, and there exists a score function H whose input is a dataset D and a potential answer a ∈ A, and whose output is a real number. Given a dataset D, the exponential mechanism selects the element a ∈ A that has a large score H(D, a).
Deﬁnition 6: the sensitivity of score function H is deﬁned as

s(H, . ) = max

H(D, a) − H(D′, a)

d(D,D′)=1,a∈A

The exponential mechanism: given a dataset D and a set of possible answers A, if a random mechanism selects an answer based on the following probability, then the mechanism is ǫ-diﬀerentially private:

P (a ∈ A is selected) ∝ eǫH(D,a)/2s(H, . )

Ê Ê The Laplacian mechanism is related to the exponential mechanism. If f (D)
is a vector in p, and ∀a ∈ p H(D, a) = a − f (D) , then the output has exactly the same distribution as f˜(D) in the Laplacian mechanism with half privacy budget.

5

1.5 The Smooth Sensitivity Framework and the Sample and Aggregate Framework
Smooth sensitivity [42] is a framework which allows one to publish an (ǫ, δ)diﬀerentially private numerical answer to a query. The noise it adds is determined not only by the query but also by the database itself. By avoiding using the worst-case sensitivity, this framework can enjoy much smaller noise, though the deﬁnition of privacy is weaker in this framework compared to the Laplacian mechanism. Two concepts, local and smooth sensitivities, are introduced.
Deﬁnition 7: given a query function f , a norm function . and a dataset D, the local sensitivity of f is deﬁned as:

LS(f, . , D) = max f (D) − f (D′)
D′ :d(D,D′ )=1

One intuitive mechanism would be to add noise to the answer f (D) pro-
portional to the local sensitivity given (f, D). However such a mechanism may leak information. For example, assume d(D, D′) = 1, If D has very small local sensitivity and D′ has large local sensitivity w.r.t. some query f , the answer
given by the mechanism on dataset D is very close to f (D). However, the answer given D′ might be far away from f (D). In that case, attackers can infer whether the dataset is D or D′ according to the distance between f (D) and the
output. To overcome this problem, the smooth sensitivity framework smooths
the scale of noise across neighboring datasets.
Deﬁnition 8: Given a query function f , a norm function . , a dataset D
and a number β, the β smooth sensitivity of f is deﬁned as

s(f, . , D, β) =

max

(e−βd(D,D∗)LS(f, . , D∗))

Any dataset D∗

Smooth sensitivity framework: Given a query function f , the dimension d of the sample space, and Gaussian noise Z ∼ N (0, 1), the output f˜(D) = f (D) +

s(f,

.

,D α

∗

,

β

)

Z

is

(ǫ, δ)-diﬀerentially

private,

provided

that

α

=

ǫ/

ln(1/δ) and

β = Ω(ǫ/ d ln(1/δ)).

The sample and aggregate framework [42] is a mechanism to respond to

queries whose answers can be approximated well with a small number of samples,

while ensuring (ǫ, δ)-diﬀerential privacy. The algorithm consists of a sampling

step and an aggregating step. In the sampling step, the framework partitions

the private data set D into many subsets {D1, ..., Dk}, and the answer f (Di) is

estimated on each subset. Given the assumption that f can be measured well

with small subsets, f (D1), ..., f (Dk) are fairly accurate. However, we haven’t

yet placed a privacy constraint on the estimation, thus the estimates cannot be

released.

In the aggregating step, the framework ﬁrst deﬁnes a quantity r(i) to de-

note the distance between f (Di) and f (Di)’s t-th nearest neighbor among

f (D1), ..., f (Dk) while t ≈ k/2. Then the framework deﬁnes a function g(f (D1), ..., f (Dk))

which outputs the f (Di) with the smallest r(i). Then the smooth sensitivity

6

framework is applied to the function g(f (D1), ..., f (Dk)) to ensure diﬀerential privacy.
As changing one sample aﬀects only one estimate, the function g(f (D1), ..., f (Dk)) has small local sensitivity. Therefore the noise required is small. Further-
more, as most of the estimates f (D1), ..., f (Dk) are close to the true answer, g(f (D1), ..., f (Dk)) is accurate. Together, these two properties ensure that the output is accurate.
An eﬃcient aggregation function is provided in the paper [42]. Given m answers {f1, f2, ..., fm} and a constant t0, the function ﬁrst computes a quantity ri for each fi. The quantity ri is radius of the smallest ball that is centred at fi and covers at least t0 answers in {f1, f2, ..., fm}. Then the function outputs the answer fi which has the smallest ri.

1.6 Combination of Diﬀerentially Private Mechanisms

Sometimes we need to combine several diﬀerentially private mechanisms in data

processing, thus we need to know how the combination aﬀects the privacy protection. In this subsection, f˜i represents diﬀerentially private algorithms, D is

the dataset, and {Di} is a partition of D. Notation g() represents any function.

[37] provides the following two theorems.

Sequential Theorem[15, 13, ?]: if f˜i is (ǫi, δi)-diﬀerentially private, then

f˜(D) = g(f˜1(D), f˜2(D, f˜1(D)), ..., f˜n(D, f˜1(D), f˜2(D), ..., f˜n−1(D))) is (

n i=1

ǫi,

diﬀerentially private.

Intuitively, it means that we can split ǫ among a sequence of diﬀerentially

private mechanisms and allow a mechanism in the sequence to use both the

dataset and the outputs of previous mechanisms, while the ﬁnal output is still

diﬀerentially private. Some more sophisticated forms of this theorem can be

ﬁnd in [16, 43]. Parallel Theorem: if each f˜i is ǫ-diﬀerentially private, given a partition
{Di} of the dataset D, then f˜(D = ∪Di) = g(f˜1(D1), f˜2(D2), ..., f˜n(Dn)) is

ǫ-diﬀerentially private.

If we apply ǫ-diﬀerentially private mechanisms to each partition of the dataset,

the combined output is still ǫ-diﬀerentially private. The partitioning here can

be either independent of private data or based on output of some other diﬀer-

entially private mechanism.

Both the sequential method and the parallel method have multiple outputs.

A natural question is whether it is always beneﬁcial to the utility of such privacy-

preserving mechanisms to average those outputs. The answer is no. For the

sequential method, the privacy budget has to be split among several steps; for

the parallel method, each partition has less samples than D. In both cases, the

ratio between the amount of noise applied and the accurate answer is larger than

the corresponding ratio for the original mechanism. Therefore, simply averaging

them doesn’t necessarily lead to better performance.

n i=1

δi

)-

7

2 Machine Learning
Machine learning algorithms extract information about the distribution of data. Informally, a learning algorithm takes as input a set of samples called a training set and outputs a model that captures some knowledge about the underlying distribution. Samples are also called examples. Typically an individual as discussed in the context of diﬀerential privacy will correspond to a single sample in the machine learning context. The set of all possible samples is called a sample space, and all samples in the sample space have the same set of variables. These variables can be either categorical or numerical. In the following sections, if there are variables whose values we would like to predict, that are known in training, but unknown for future examples, that variable is denoted Y . All the other variables are denoted X. When we want to predict labels for new examples, this task is called supervised learning. The task in which there are no labels and we want to identify structure in the dataset is called unsupervised learning.
The information extracted is represented by machine learning models, and diﬀerent models are used for diﬀerent tasks. Regression models predict a numerical variable Y given a set of variables X. Classiﬁcation models predict a categorical variable Y given a set of variables X. Clustering models group unlabelled samples into several groups based on similarity. Dimension reduction models ﬁnd a projection from the original sample space to a low-dimensional space, which preserves the most useful information for further machine learning. Feature selection techniques select the variables that are most informative for further research. According to whether the learning task is supervised or unsupervised, the training set is denoted either {(xi)}ni=1 or {(xi, yi)}ni=1, while n is the number of training samples.
Given a family of possible models and a dataset, a machine learning algorithm selects one model that ﬁts the data best. The process of selection is called training.
In the following section, we assume that there is only one variable to predict in regression or classiﬁcation tasks. For binary classiﬁcations, Y ∈ {−1, 1}.
In all the following sections, we use the same notation for machine learning tasks. Usual capital letters X and Y mean random variables while bold capital letters X and Y mean data matrices. The j-th component of X is denoted Xj. There are n samples in the dataset and the i-th sample is denoted xi for unsupervised learning tasks or (xi, yi) for supervised ones. The j-th component of xi is denoted xij . All constants are denoted by capital letter C.
2.1 Performance Measurement
Many papers have analyzed the performance of their mechanisms and proven that the private models they output are very close to the true models. However, the analyses in these papers diﬀer in how they deﬁne the true model, how they deﬁne the distance between two models, and given such a distance metric, how they deﬁne closeness. These diﬀerences can impede our eﬀorts to compare
8

diﬀerent mechanisms. To assess the performance of a diﬀerentially private algorithm, it is necessary
to have some notion of a ‘true model’ against which comparisons can be made. Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the ‘true model’ to be the output of a noiseless algorithm on training data. However, others [14, 33, 40] consider the ‘true model’ to mean the optimal model if the true distribution were known.
They also diﬀer on how to deﬁne the distance between two models. Some papers [6, 25, 53] use the diﬀerence of values of the target function. Thus the distance between the private model and the true model is the diﬀerence between the values taken by the target functions corresponding to each of the two models. Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters. Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.
Finally, they diﬀer on the deﬁnition of ‘closeness’. Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model. However they do not provide a guaranteed rate of convergence. Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models. For those which prove bounds on the speed of convergence, the convergence is usually measured by (α, β)-usefulness [3]. If the mechanism output f˜(D) is an (α, β)-useful answer to f on dataset D, then with probability 1 − β, the diﬀerence between f˜(D) and f (D) is less than α. Such mechanisms usually provide a relationship between data size, model settings, α and β. A few papers [33] provide worst case guarantees on the distance, which is equivalent to (α, 0)-usefulness. Yet another paper [25] uses the expectation of diﬀerence.
Below, we will describe the utility analysis of various algorithms, but we cannot always compare two mechanisms that diﬀer on some of these aspects. Furthermore, even if we can compare the utility of two mechanisms, one might outperform in some situations while the second outperforms in others. Suppose one is (ǫ, δ)-diﬀerentially private and the other is ǫ-diﬀerentially private. If we can tolerate a very small probability that information is leaked, then the ﬁrst may be better; if we are opposed to taking any risk, the second may be better. Therefore, choice of mechanism can depend on speciﬁc applications.
2.2 General Ideas of Diﬀerentially Private Machine Learning Algorithms
Many diﬀerentially private machine learning algorithms can be grouped according to the basic approaches they use to compute a privacy-preserving model. This applies both for supervised and unsupervised learning.
Some approaches ﬁrst learn a model on clean data, and then use either the exponential mechanism or the Laplacian mechanism to generate a noisy model. For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while
9

[7, 53] use the exponential mechanism. For some other approaches that have many iterations or multiple steps, the Laplacian mechanism and the exponential mechanism are applied to output parameters of each iteration/step. Such approaches include [21, 31, 24, 19, 25, 41, 55].
Some mechanisms add noise to the target function and use the minimum/maximum of the noisy function as the output model. These technique is called objective perturbation. Some examples include [56, 5, 6, 45].
Some mechanisms use the idea of the sample and aggregate framework. They are specially designed for queries that can be measured with a small number of samples. First, they split the dataset into many small subsets. Next, they combine the results from all subsets to estimate a model, adding noise in this aggregation step. Mechanisms that employ this idea include [42, 27]. The linear regression in [14] is partially based on this idea.
Some mechanisms explore other ideas. For example, [33] partitions the sample space and uses counts in each partition to estimate the density function. [26] interprets a model as a function and uses another function to approximate it by iteratively minimizing the largest distance.
Most output perturbation and objective perturbation mechanisms require a bounded sample space. This is because unbounded sample space usually leads to unbounded sensitivity. Mechanisms based on the sample and aggregate framework don’t have this limitation. However most of them use (ǫ, δ)-diﬀerential privacy. In practice, if the sample space is unbounded and we want to use ǫ-diﬀerential privacy, we can simply truncate the values in pre-processing. If the rule to truncate is independent of the private data, then the truncation is privacy-safe.
In the next sections, some diﬀerentially private machine learning mechanisms are introduced. We will brieﬂy introduce the learning models, describe additional conditions assumed by various authors, explain how they design the mechanisms, and provide some utility analysis. However, the computation of sensitivity, the proof of diﬀerential privacy and some mechanism details won’t be discussed here.
3 Diﬀerentially Private Supervised Learning
Supervised machine learning describes the setting when labels are known for training data, and the task is to train a model to predict accurate labels given a new example. In this section we will describe diﬀerentially private versions of commonly used supervised machine learning algorithms.
3.1 Naive Bayes Model
The naive Bayes model is a classiﬁer which predicts label Y according to features in X. Given features X and a model, one can compute the conditional probability P (Y |X) for all labels Y and predict the label with largest conditional probability. The naive Bayes model is based on two assumptions.
10

The ﬁrst assumption is that Xj are conditionally independent given Y , i.e., P (Xj|Y, X1, ..., Xj−1) = P (Xj|Y ). This enables us to compute the coeﬃcient of each feature independently. The second assumption is that for all numerical features in X, P (X|Y ) is a normal distribution.
Based on the ﬁrst assumption and Bayes’ theorem, the conditional probability is as follows:
p
P (Y |X1, ..., Xp) ∝ P (Y ) P (Xj|Y )
j=1
To train the model, we need to estimate all the P (Y ) and P (Xj|Y ). The probabilities P (Y ) can be estimated by the frequencies of samples with label Y in the training set. For conditional probabilities P (Xj|Y ), the training is based on whether Xj is categorical or numerical. If Xj is a categorical feature, for all values x and y, we have P (Xj = x|Y = y) = P (Xj = x, Y = y)/P (Y = y) = i I[xij = x]I[yi = y]/ i I[yi = y]. Thus we need counts i I[yi = y] and i I[xij = x]I[yi = y] to compute the conditional probabilities. If Xj is numeric, then based on the second assumption, the normal distribution p(Xj|Y ) is decided by E[Xj|Y ] and V ar[Xj|Y ]. Thus to compute the model we only need the following information: i I[yi = y], all i I[xij = x]I[yi = y] for categorical variables and all E[Xj|Y ] and V ar[Xj|Y ] for numerical variables.
An ǫ-diﬀerentially private naive Bayes model mechanism is introduced in [52]. This mechanism relies on one additional assumption: all values for all features in the dataset are bounded by some known number. If the bound covers most of the Gaussian distribution, then both the bound assumption and Gaussian assumption hold approximately. Therefore the sensitivity of the information that is needed to compute the model can be calculated. The mechanism then adds noise to this information according to the Laplacian mechanism and computes the model. Although no analysis on utility is provided, it is easy to see that the noise on the parameters is O(1/nǫ).
Sometimes the (non-private) naive Bayes model is more accurate if we model the continuous features with histograms instead of a Gaussian distribution. However, in this case, many histograms may lead to high sensitivity. Thus as long as the Gaussian assumption is not far from the truth, there is no need to use histograms. A good assumption about a distribution can result in good performance. If in extreme cases the assumption is too far from the truth, we can represent those features with histograms in preprocessing. If the rule in preprocessing is independent of the private dataset, the preprocessing is privacyfree.
Another question is whether we can use the logarithms of counts i I[xij = x]I[yi = y] here, as we sometimes do in using the standard naive Bayes model. Clearly, we can apply the Laplacian mechanism to logarithms, however this change is useless. For example, suppose the true count is c, the noisy count is c˜ and we add noise to log(c + 1). According to the Laplacian mechanism, the sensitivity of log(c) is log 2. Thus the probability P ((c) = 1˜|c = 0) stays the same, while probabilities such as P ((c) = ˜9|c = 4) increases a lot. Such
11

transformation cannot reduce noise when the count is small, however it increases noise a lot when the count is large. Therefore, it is better to add noise to the counts directly.

3.2 Linear Regression
Linear regression is a technique for predicting numerical values Y in which the value is modelled as a linear combination wT X of features X. Here, the vector w contains the weights corresponding to each feature and constitutes the set of parameters which must be optimized during training. To train the model, w is computed by minimizing square loss i(yi − wT xi)2 over the training set.
[56] assumes bounded sample space and proposes a diﬀerentially private mechanism for linear regression. As the loss function is analytic, the mechanism expands the function with Taylor expansion, approximates it with a low order approximation, and adds noise to the coeﬃcients of the terms. The mechanism then ﬁnds the w that minimizes the approximate loss function. As the sensitivities of the coeﬃcients are easy to compute, the Laplacian mechanism can ensure the diﬀerential privacy of the noisy approximation. Since no private information is used after adding noise, the output vector w here is also ǫ-diﬀerentially private. Furthermore, the model that is decided by w is also diﬀerentially private.

3.3 Linear SVM

Linear SVM is a linear classiﬁer in which a vector w captures the model param-

eters. A linear SVM model outputs a score wT X for features X in a sample,

and usually uses sign(wT X) as the label Y . The parameter w is computed

by minimizing C i max(0, 1 − yi(wT xi)) + wT w/2, while C > 0 is an input parameter which sets the strength of prediction error.

Under the assumption that the sample space is bounded, the linear SVM

model satisﬁes the following two conditions. First, it computes w by minimizing

a strongly convex and diﬀerentiable loss function L(w). Second, a change of one

sample results in bounded change in L′(w). For linear SVM and all other models

satisfying the two conditions, [5, 6] provide an output perturbation mechanism

and an objective perturbation mechanism. The output perturbation mechanism

ﬁrst trains the model and then adds noise to it. The objective perturbation

mechanism introduces noise by adding a carefully designed linear perturbation

item to the original loss function. The w computed from the perturbed loss

function is ǫ-diﬀerentially private.

[6] also provides a performance analysis of the objective perturbation mech-

anism. To achieve (α, β)-usefulness and ǫ-diﬀerential privacy, the mechanism

needs

O

(

l

og

(1/β α2

)

+

1 ǫα

+

log(α1ǫ/β) )

samples.

As

deﬁned

earlier, (α, β)

usefulness

provides a guarantee that with respect to the true loss function, the performance

of the private model will be within a distance α of that achieved by the true

model with probability greater than 1 − β.

12

3.4 Logistic Regression

Logistic regression is model for binary classiﬁcation. It makes prediction P (Y =

1|X) = 1/(1 + e−wT X ) given features X in a sample. The parameters w are

trained by minimizing negative log-likelihood the training set.

i log(1 + exp(−yiwT xi)) over

Regularized logistic regression diﬀers from standard logistic regression in

that the loss function includes a regularization term. Its w is computed by

minimizing i log(1 + exp(−yiwT xi)) + λwT w over the training set {(xi, yi)} while λ > 0 is a hyperparameter which sets the strength of regularization.

Assuming that the sample space is bounded, the mechanism in [56] (see

Section 3.2) can be applied to make both models ǫ-diﬀerentially private. Fur-

thermore, the output perturbation and objective perturbation mechanism in

[5, 6] (see Section 3.3) can ensure ǫ-diﬀerential privacy for regularized logistic

regression.

3.5 Kernel SVM
Kernel SVM is a machine learning model that uses a kernel function K(, ), which takes two samples as input and outputs a real number. Diﬀerent kernel functions lead to diﬀerent SVM models. Kernel SVM can be used both for classiﬁcation and regression. When used to classify a sample with features X, kernel SVM predicts the label Y = sign( i wiK(X, xi)); when used for regression, kernel SVM predicts the quantity Y = i wiK(X, xi). In both cases {(xi, yi)} are training samples and wi are weights in the model to compute. Note the model includes the kernel function K(, ), all training data and a vector of weights {wi}ni=1. Although there exist many algorithms to train kernel SVM, I will only address those relevant to current diﬀerentially private versions.
Unlike previous models, kernel SVMs contain all the training data. Therefore the techniques required to make diﬀerentially private kernel SVM mechanisms are diﬀerent from those we have already described. In [6, 45], an idea for private kernel SVM is proposed. It works for all translation-invariant kernels, where there exists some function g(x) such that K(x1, x2) = g(x1 − x2)∀x1, x2. For example, radial basis function kernel is translation-invariant. The basic idea is to approximate kernel functions in the original sample space with a linear kernel in another space, so as to avoid publishing training data. It ﬁrst constructs a space independent of private training data and then projects data from the original sample space to that space. According to [44], the kernel function of two samples in the original sample space can be approximated by the inner product of their projections in the new space. Thus the kernel SVM model turns out to be a linear SVM model in the new space and we can use private linear SVM mechanisms mentioned above. Furthermore, the non-private projection can be published, thus future data can be projected to the same space and then use the parameters from private linear SVM to predict. In this way, the mechanism transforms a kernel SVM model training problem into a linear SVM training problem. To achieve both (α, β)-usefulness w.r.t. to predictions on any sample

13

and ǫ-diﬀerential privacy, this mechanism needs n = O( log1.ǫ5α(13/αβ) ) samples. The previous mechanism can not be applied to kernel functions that are not
translation-invariant, such as polynomial kernel or sigmoid kernel. Therefore [26] proposes another private kernel SVM algorithm for all RKHS kernels. An RKHS kernel means that there is some function φ(x) that projects x onto another space such that K(x1, x2) equals the inner product of φ(x1) and φ(x2). This mechanism seems similar to the one previously described (where the projection can be seen as an approximate to φ(x)), however here projection doesn’t need to be explicit.
The Test Data-independent Learner (TTDP) mechanism in [26] publishes a private kernel SVM model satisfying (ǫ, δ)-diﬀerential privacy as follows. Intuitively, it trains a non-private kernel SVM model f (x) from the private data and then approximates it in a diﬀerentially private way. The private model g(x) is trained iteratively. First, the mechanism sets g(x) = 0. Then it computes a non-private model f (x). Next, it constructs a ﬁnite set Z from the unit sphere that represents the sample space. Each iteration consists of three steps. In the ﬁrst step, the mechanism selects a point z ∈ Z where g(z) and f (z) disagree the most. This selection is based on the exponential mechanism, and |g(z)−f (z)| is used as the score function. In the second step, a noisy diﬀerence |g(z)−f (z)|+η is computed while η is Laplacian noise. In the third step, the mechanism tests whether the noisy diﬀerence exceeds some threshold. If not, the mechanism proceeds to the next iteration; if it does exceed the threshold, the mechanism updates g(x) to be closer to f (x) at point z. After many iterations, g(x) may approximate f (x). To achieve both (α, β)-usefulness w.r.t. to predictions on any sample in the unit ball and (ǫ, δ)-diﬀerential privacy, this mechanism needs n = O( logα3 11δ.5lǫo0g.17.55 β1 ) samples.
3.6 Decision Tree Learning
Learning a decision tree classiﬁer involves partitioning the sample space and assigning labels to each partition. The training algorithm for a decision tree classiﬁer consists of a tree building process and a pruning process. In the tree building process, ﬁrst the entire sample space and all samples are put in the root partition. Then the algorithm iteratively selects an existing partition, selects a variable based on the samples in that partition and a score function such as information gain or Gini index, and partitions the sample space (and the samples corresponding to that partition) according to the variable selected. If the selected variable is categorical, usually each value of that variable corresponds to a partition; if the variable is numerical, then some thresholds will be selected and the partitioning is based on those thresholds. The partitioning process ends when the spaces corresponding to all partitions are small enough or the numbers of samples in each partition are too small. After building the tree, the pruning process removes unnecessary partitions from the tree and merges their spaces and samples to their parents.
[24] proposes an ǫ-diﬀerentially private mechanism. The mechanism con-
14

structs N decision trees and uses the ensemble to make classiﬁcation. When

constructing a decision tree Ti, it randomly partitions the sample space into

partitions Pi1, ..., Pimi without using private data and computes Countijy, noisy

counts of samples with each label y in partition Pij . When predicting the la-

bel of a sample X, it looks for all the partitions P1a1 , ..., PNaN from all trees

T1, ..., TN that include X, sums the counts of samples with each label from all

those partitions Sy =

n i=1

C ountiai y ,

and

computes

the

probabilities

of

label

y′ by P (Y = y′|X) = Sy′ / y Sy.

[19] proposes another ǫ-diﬀerentially private decision tree algorithm. The

mechanism is based on the assumption that all features are categorical, in order

to avoid selecting partition points for any feature. In the partitioning pro-

cess, the mechanism uses the exponential mechanism to select the variable with

largest score (for example, information gain or Gini index) diﬀerentially pri-

vately. Each time a partition reaches a pre-determined depth, or the number of

samples in that partition is about the same scale as random noise, or the sample

space corresponding to that partition is too small, the mechanism stops operat-

ing on that partition. It then assigns to that partition a noisy count of samples

with each label. After the partitioning process has completed altogether, these

noisy counts are used to decide whether to remove those nodes without having

to consider privacy.

3.7 Online Convex Programming
Many machine learning techniques, such as logistic regression and SVM, specify optimization problems which must then be solved to ﬁnd the optimal parameters. Online algorithms, such as gradient descent, which consider examples one at a time, are widely used for this purpose. To that a machine learning algorithm is diﬀerentially private, it is therefore important to demonstrate that the optimization algorithm doesn’t leak information.
Online convex programming (OCP) solves convex programming problems in an online manner. The input to an OCP algorithm is a sequence of functions (f1, ..., fT ) and the output is a sequence of points (w1, ..., wT ) from a convex set C. The algorithm is iterative and starts from a random point w0. In the t-th iteration, the algorithm receives the function ft and outputs a point wt+1 ∈ C according to (f1, ..., ft) and (w1, ..., wt). The target of the OCP algorithm is to minimize regret, which is deﬁned as

T

T

R = ft(wt) − min ft(w).
t=1 w∈C t=1

Note that the input here is a sequence of functions instead of samples. There are many methods to ﬁnd wt+1 ∈ C according to (f1, ..., ft) and
(w1, ..., wt). [25] provides (ǫ, δ)-diﬀerentially private versions for two of them: the Implicit Gradient Descent (IGD) and the Generalized Inﬁnitesimal Gradi-
ent Ascent (GIGA) given all the functions are L-Lipschitz continuous for some

15

constant L and η-strongly convex. IGD ﬁrst computes

wˆt+1 = arg min
w∈C

1 w − wt 2 + 1 ft(w)

2

ηt

and projects wˆt+1 onto C to get the output wt+1. GIGA ﬁrst computes

wˆt+1 = wt − 1 ∇ft(wt) ηt

and then does the projection. Both algorithms ensure bounded sensitivity of

wt+1 given wt. The private mechanism in [25] adds Gaussian noise to every wˆt before it is projected to wt to preserve privacy, and then use the noisy wt for the future computation. Given T functions(samples), the expected regret by

this (ǫ, δ)-diﬀerentially private mechanism is O

T ǫ

ln2

T δ

.

4 Diﬀerentially Private Unsupervised Learning
Unsupervised learning describes the setting when there are no labels associated with each training example. In the absence of labels, unsupervised machine learning algorithms ﬁnd structure in the dataset. In clustering, for example, seeks to ﬁnd distinct groups to which each datapoint belongs. It can be useful in many contexts such as medical diagnosis, to know of an individuals member ship in a group which shares certain speciﬁc characteristics. However, releasing the high-level information about a group may inadvertently leak information about the individuals in the dataset. Therefore it is important to develop diﬀerentially private unsupervised machine learning algorithms.

4.1 K-means clustering
K-means is a commonly used model in clustering. To train the model, the algorithm starts with k randomly selected points which represent the k groups, then iteratively clusters samples to the nearest point and updates the points by the mean of the samples that are clustered to the points.
[42] proposes an (ǫ, δ)-diﬀerentially private k-means clustering algorithm using the sample and aggregate framework. The mechanism is based on the assumption that the data are well-separated. ‘Well separated’ means that the clusters can be estimated easily with a small number of samples. This is a prerequisite of the sample and aggregate framework. The mechanism randomly splits the training set into many subsets, runs the non-private k-means algorithm on each subset to get many outputs, and then uses the smooth sensitivity framework to publish the output from a dense region diﬀerentially privately. This step preserves privacy while the underlying k-means algorithm is unchanged.
Any modiﬁcations on the k-means clustering algorithm (such as k-means++) can be used in the sample step, with the sole restriction that such modiﬁcations leave intact the property that the algorithm can be estimated with a small

16

number of samples. Additionally, if the sample space is bounded and the number of samples surpasses a threshold, there is a bound on the noise added. However this bound is not directly related to the number of samples in the dataset.

5 Diﬀerentially Private Dimensionality Reduc-
tion
In machine learning contexts, when data is high dimensional, it is often desirable learn a low-dimensional representation. Lower dimensional datasets yield models with less degrees of freedom and tend to be less prone to overﬁtting. From a diﬀerential privacy perspective, lower dimensional representations are desirable because they tend to have lower sensitivity.
Feature selection is one technique for dimensionality reduction, in which a subset of features is kept from an original feature space. Principal component analysis (PCA), on the other hand is a matrix factorization technique in which a linear projection of the original dataset into a low dimensional space is learned such that the new representation explains as much of the variance in the original dataset as possible.

5.1 Feature Selection

[53] proposes an ǫ-diﬀerentially private feature selection, PrivateKD, for classi-

ﬁcation. PrivateKD is based on the assumption that all features are categorical

and each feature has ﬁnite possible values. For any set of features S, it deﬁnes

a function F (S) which tells how many pairs of samples from diﬀerent classes

can features in S distinguish. The set of selected features S′ is initialized to ∅.

Then a greedy algorithm adds new features one by one to S′. When selecting

a feature to add, the algorithm uses the exponential mechanism to select the

feature that can lead to the largest increase of F (S′). The paper provides a

utility guarantee for the special case where the cardinality of sample space m

and the number of features d have the relation m = d − 1. In that case, ex-

cept probability O(1/poly(m)) (poly(m) means a polynomial expression of m),

F (S′) ≥ (1 − 1/e)F (Soptimal) − O(log m/ǫ).

[27] proposes an (ǫ, δ)-diﬀerentially private algorithm for feature selection

when the target function is stable. Unlike the previous paper, this paper doesn’t

explicitly state the algorithm for feature selection. Instead, it only requires the

selection algorithm to be stable. By ‘stable’, we mean that either the value of

function as calculated on the input dataset doesn’t change when some samples

in the set change, or that the function can output the same result on a random

subset from the input dataset with high probability. For the ﬁrst kind of func-

tions, the mechanism uses the smooth sensitivity framework in [42] to select

features.

If adding or removing any

log(1/δβ) ǫ

samples from the input dataset

doesn’t change the selection result, then the algorithm can output the correct

selection result with probability 1 − β.

17

For the second kind of functions, the mechanism uses an idea similar to the sample and aggregate framework in [42]: it creates some bootstrap sets from the private dataset, selects features non-privately on each set, and counts the frequencies of feature sets output by the algorithm. Intuitively, if the number of samples is large and the features set is not too large, there is high probability that the correct output is far more frequent than any other one. Thus, the mechanism can release the most frequently selected set of features. If a random subset of the input dataset with ǫ/(32 log(1/δ)) samples outputs the same selection result with probability at least 3/4, then the mechanism outputs the correct solution with probability at least 1 − δ.
5.2 Principal Component Analysis
Principal Component Analysis (PCA) is a popular method in dimension reduction. It ﬁnds k orthogonal directions on which the projections of data have largest variance. The original data can then be represented by its projection onto those k directions. Usually k is much smaller than the dimension of sample space. Thus the projection greatly reduces the dimensionality of the data. It is well-known that this analysis is closely related to eigen-decomposition: if we rank the eigenvectors of matrix A = V ar[X] according to the corresponding eigenvalues λ1 ≥ λ2 ≥ ... ≥ λp, then the ﬁrst k eigenvectors are the k directions.
There are two diﬀerentially private mechanisms to select eigenvectors iteratively. The iterative methods are based on the spectral decomposition, which ensures that if the components corresponding to the ﬁrst i − 1 eigenvectors of A are subtracted from A, then the i-th largest eigenvector becomes the largest eigenvector of what remains. Therefore the process of selecting the largest k eigenvectors can be replaced by repeatedly ﬁnding the ﬁrst eigenvector and removing the component corresponding to the selected eigenvector. The following two mechanisms both make use of this idea but diﬀer on how to select the ﬁrst eigenvector.
An (ǫ, δ)-diﬀerentially private mechanism is proposed in [21]. The mechanism uses the power method: Anv/ Anv converges to the ﬁrst eigenvector of A if v is not orthogonal to the ﬁrst eigenvector. It randomly starts with a unitlength vector v, then iteratively updates v with (Av + ηi)/ Av + ηi while ηi is Gaussian noise in the i-th iteration. Since it is exceedingly improbable that a random vector is orthogonal to the ﬁrst eigenvector, the vector v will get close to the ﬁrst eigenvector. However due to the noise, v cannot converge with arbitrary accuracy. Thus it outputs v after a ﬁxed number of iterations and proceed to ﬁnd the next largest eigenvector. A utility guarantee is provided on the power method, which outputs the ﬁrst eigenvector. However there is no direct guarantee on the k eigenvectors. For each eigenvector a output by running the power method on matrix A, the distance from the ﬁrst eigenvector ( Aa / a − λ1 while λ1 is the ﬁrst eigenvalue) is O(( log(1/δ) log n)/ǫ).
[31] provides an ǫ-diﬀerentially private mechanism for principal component analysis. According to the property that the ﬁrst eigenvector v of A is the unitlength vector that maximizes vT Av, the mechanism uses H(X, v) = vT Av as
18

the score function in the exponential mechanism to select the ﬁrst eigenvector from the set {v : vT v = 1} diﬀerentially privately. The selection algorithm is

specially designed to be computable in reasonable time. This paper also provides

two proofs on utility of this mechanism. For any 0 < δ < 1 and privacy budget

ǫ, if the ﬁrst eigenvalue of matrix A, λ1 > O(ln(1/δ)/(nǫδ)), then the ﬁrst eigenvector v has the property E[vT Av] ≥ (1 − δ)λ1. For any 0 < δ < 1 and privacy budget ǫ, if the ﬁrst eigenvalue λ1 > O(1/(nǫδ6)), the k+1-th eigenvalue

is denoted λk+1, and the k-rank approximation matrix output is denoted Ak,

then the largest eigenvalue of A − Ak is smaller than λk+1 + δλ1 with large

probability.

Not all diﬀerentially private approaches to PCA rely on iterative algorithms.

[7] proposes an ǫ-diﬀerentially private mechanism, PPCA, to compute k largest

eigenvectors at the same time. The mechanism uses the property that the ﬁrst k eigenvectors of A are the columns of the p×k matrix V = arg maxV :V T V =Ik tr(V T AV ). Therefore, it uses H(X, V ) = tr(V T AV ) as the score function and selects V from the set of matrices {V : V T V = Ik} according to the exponential mecha-

nism. A Gibbs sampler method is used here to select the matrix V . This paper

provides a guarantee on the ﬁrst eigenvector. For any 0 < ρ, η < 1, if the sample

size n = O

1 ǫ(1−ρ)

log

1 η

+

log

1 1−ρ2

, then the inner product of output ﬁrst

eigenvector and true eigenvector is larger than ρ with probability 1 − η.

6 Statistical Estimators
Statistical estimators calculate approximations of quantities of interest based upon the evidence in a given dataset. Simple examples include the population mean and variance. While estimators are clearly useful, they may potentially leak information about the individuals contained in the dataset, especially when the dataset is small or features are rare. Therefore, to protect privacy, it is necessary to develop diﬀerentially private estimators.
6.1 Robust Statistics Estimator
[14] proposes an (ǫ, δ)-diﬀerentially private mechanism for robust statistical estimators. Roughly speaking, a statistical estimator produces an estimate of a vector (such as the mean and variance of a Gaussian distribution) based on the input dataset. The estimator T can be seen as a function that maps a dataset D to the output vector T (D). Most statistical estimators converge when the number of samples tends to inﬁnity and the samples are i.i.d. drawn from some distribution P . When the estimator does converge, the limit lim|D|→+∞ T (D) is denoted T (P ). The deﬁnition of robust estimator is based on the stability of estimates. An estimator is robust if for any element x in the sample space, the following limit exists limt→0(T ((1 − t)P + tδx) − T (P ))/t. The distribution (1 − t)P + tδx means that with probability 1 − t the sample is from P and with probability t the sample is x.

19

The output of a robust estimator doesn’t change much if a small number of samples change. Based on the property, [14] comes up with a Propose-
Ê Ê Test-Release framework. The framework is based on the assumption that the
statistics are in p. It divides p into small cubes, then computes the statistics T (D) from a dataset D and the number of sample changes needed to make T (D) fall into another cube. If the number is large, the statistics are stable and thus the mechanism can add Laplacian noise to the statistics to make it private; if the number is small, then the mechanism outputs ⊥, which means it fails. When the number of samples tends to inﬁnity, and the samples are i.i.d. drawn, the framework output is asymptotically equivalent to a non-private robust estimator.
Based on this framework, [14] proposes three mechanisms for interquartile range estimation, trimmed mean and median, and linear regression, respectively. When applying the framework to linear regression, the framework uses a robust estimator to learn a model from the training set {(xi, yi)}

wˆ = arg min |yi − wT xi|

w i

xi

instead of minimizing the mean square error. Given n samples, the linear regression estimator can successfully output a model with probability 1 − O(n−c ln n) for some constant c. Additionally, its output converges to the true linear regression parameter when n tends to inﬁnite.
[4] explores robust estimators in another way. They prove that if eﬀect of one sample is bounded by O(1/n), and the range of T (P ) is bounded, then the smooth sensitivity framework provides bounded error.
However, if T (P ) is not bounded, and if for any value τ in an inﬁnite range, there exists some P such that T (P ) = τ , then the error of any ǫ-diﬀerentially private mechanism cannot be upper bounded.
Here, a bound on the eﬀect of one sample means that there exists a uniform upper bound M (P ) for distribution P , such that for all distributions P ′ satisfying |P − P ′| ≤ O( 1/n), all x in the sample space, T ((1 − 1/n)P + δx/n) − T (P )) ≤ M (P )/n.
To achieve both (ǫ, δ)-diﬀerential privacy and (α, β)-usefulness, the smooth sensitivity framework requires O ln(ǫ1α/β) + ln2(1/δ)ǫ2lnl2n((α1/ǫ/β)ln(1/β)) samples.

6.2 Point Estimator
[49, 50] give a diﬀerentially private mechanism for point estimation. Using the notation in 6.1, the mechanism splits D into k subsets with equal size {D1..., Dk} randomly and estimates parameters {T (D1), T (D2), ..., T (Dk)} on each subset. Then it uses a diﬀerentially private mean of all T (Di) to approximate T (D). The mean is computed in two steps. First, if the space of parameters is unbounded, it computes two quantiles and truncates all estimates according to the quantiles and the sample size. Second, the mechanism adds Laplacian noise to the mean of truncated values and publishes the noisy mean. When the space of possible

20

parameters are bounded, then the mechanism only executes the second step and becomes ǫ-diﬀerentially private. When the space of possible parameters is not bounded, then both steps have to be executed and the mechanism is (ǫ, δ)-diﬀerentially private.
The papers give suﬃcient conditions under which the mechanism is as accurate as non-private estimators asymptotically. The conditions are listed below. n is the number of samples in D and σP is a real number. For models that don’t converge to a ﬁxed point (for example, some EM algorithms) or models with varying numbers of features (such as Kernel SVM), there is no such guarantee.

T (D) −√T (P ) → N (0, 1) when n → +∞ σP / n

E[T (D)] − T (P ) = O(1/n)

|T (D) − T (P )| 3

E

√

= O(1)

σP / n

6.3 M-estimator
[33] proposes an ǫ-diﬀerentially private mechanism for M-estimator. Unlike the robust estimator above, the deﬁnition of an M-estimator depends on the function from which the estimates come. M-estimation relies on a function m(, ), which takes a sample and a parameter θ as input and outputs a real number. An M-estimator estimates the parameter θ by computing
θˆ = arg min 1 m(xi, θ) θ ni
The mechanism in [33] ﬁrst divides the sample space ([0, 1]d) into many small cubes without using private data. Then it adds Laplacian noise to the counts of samples in the cubes and computes the density function in each cube by dividing the noisy count corresponding to that cube by the volume of the cube. The noisy density function leads to a noisy target function in the minimization problem above, and the noisy target function leads to a noisy minimum θ˜. The noisy minimum can be released as an estimate. distTrihbeutoiountpouft tpraairnaimngetedratwailwl ictohnvsperegeed tOo(nth−e1/t2ru+e (p√alroagmne/tner)2/b(ads+e2d))ounndtheer some regularity conditions. Here d is the number of features and n is the number of samples.

7 Learning in Private Data Release
Many diﬀerentially private data release mechanisms have been described, such as [3, 1, 18, 10]. In this section, we focus on data release mechanisms that are either useful for machine learning or based on machine learning algorithms.

21

Many papers use partition based algorithms to release data. [55] assumes that the density function is smooth, [8, 57] assume that the data’s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining. All these assumptions motivate partitioning the sample space and publishing counts in each partition. With respect to the mechanism design, the mechanisms can be divided into two groups. [41] ﬁrst partitions the sample space using the exponential mechanism and then adds noise to the counts using the Laplacian mechanism. Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.
Some data release mechanisms don’t depend on partitioning. Some of them assume the private data is ﬁt well by some family of models. They select an optimal model from that family privately and then generate new data according to the selected model. Some others assume some property (like sparsity) of the data, and propose mechanisms that can make use of that property.
[40, 46] publish a graph generator model based on the assumption that the private data is ﬁt well by some parametrized generative model. Though the two mechanisms use diﬀerent generative models, both train the model ﬁrst, add Laplacian noise to the parameters of the model, and then use the noisy model to generate a new graph.
[54] represents the network structure by using a statistical hierarchical random graph model. Unlike the two models above, the number of parameters in this model is proportional to the number of nodes in the graph. Thus we will introduce too much noise if we use the Laplacian mechanism to publish the model. As the parameter space is very large and no score function exists, which is both simple and meaningful, it is not easy to use the exponential mechanism directly. To overcome this diﬃculty, the authors propose a mechanism based on the Markov Chain Monte Carlo procedure. It uses the MetropolisHastings algorithm to draw a model from the distribution in the exponential mechanism given the score function is the likelihood function. Though the likelihood function is complicated w.r.t. the whole space of parameters, it is simple w.r.t. one parameter if all the others are ﬁxed. Thus the Markov Chain Monte Carlo procedure is possible. The graph can be reconstructed from the output model after many iterations.
Sometimes corresponding to a large private dataset of interest, there exists a similarly structured but smaller publicly available dataset. [28] makes use of a public dataset, assigning weights to its examples to embed information contained in the private data. The mechanism assumes the existence of such a public dataset, but does not require that it is drawn from the same distribution as the private data. First they use public/private datasets as positive/negative samples in a logistic regression model. They train a noisy logistic regression model and assign weights based on the noisy model to all public samples. The weighted public dataset can replace the private dataset in future data mining. If the weighted set is used in measuring the expectation E[f (X)] for some function f (X) while X is from t√he distribution of private data, the standard deviation of the estimate is O(1/ n). If the assumption holds exactly, then the estimate
22

is asymptotically unbiased. [30] assumes that samples have binary labels and that they are ﬁt by a
linear discriminant analysis (LDA) model. The idea is similar to the exponential mechanism: The exponential mechanism computes the non-private LDA model from the private dataset D. Then, it uses the distance between the non-private model parameters and the LDA parameters trained from another dataset D′ as the score function, and draws a dataset diﬀerentially privately.
The mechanism in this paper, however, ﬁrst computes a private LDA model by the Laplacian mechanism, and then draws a dataset that minimizes the distance. Such an output dataset can preserve the classiﬁcation information from the private data.
[35] assumes that the data matrix is sparse, thus the mechanism can make use of results from compressive sensing research. Informally speaking, if we randomly project the high-dimensional data matrix to a low-dimensional space, and then attempt to recover the high-dimensional matrix using the low-dimensional embedding and the constraint that the recovered matrix is sparse, there is high probability that the recovered matrix exactly matches the original one. [35] ﬁrst randomly projects the data matrix, then adds noise to the compressed information, and reconstructs data from the noisy compressed information. As the dimension of compressed information is much smaller than that of the original data matrix, the scale of La√placian noise needed to preserve ǫ-diﬀerential privacy can be reduced from O( n/ǫ) to O(log n/ǫ) given n samples.
8 Theoretical Results
[32] studies the general properties of private classiﬁers, instead of individual learning models. They ﬁrst deﬁne a problem to be learnable if there is an algorithm that can output a highly accurate model with a large probability given enough data. The accuracy here is measured as the percentage of samples that are correctly classiﬁed. They further claim that if a problem is learnable, then it can be learned diﬀerentially privately. The mechanism they constructs takes the number of correctly classiﬁed samples as a score function and uses the exponential mechanism to draw the best model, which it calls the best hypothesis in a class.
[39] formulates diﬀerentially private learning in an information theoretic framework. The paper uses a concept in information theory, PAC-Bayesian bound. This concept is for parametrized models that have bounded loss functions and it uses the Bayesian learning framework. PAC-Bayesian bound P AC(π˜, π, ǫ, λ) is a function of the posterior distribution π˜ of model parameters, the prior distribution π of parameters, a positive number ǫ representing the privacy budget, and another number λ ∈ (0, 1), which is related to the strength of the bound. With probability 1 − λ and prior distribution π, P AC(π˜, π, ǫ, λ) upper bounds the expected loss on the true distribution if the model parameter is from the distribution π˜. This bound varies given diﬀerent ǫ. According to [39], the output of the exponential mechanism follows the posterior distribution that minimizes
23

PAC-Bayesian bound. Note that the PAC-Bayesian bound upper bounds the loss function. It is
possible that other mechanisms have loss functions that achieve a better bound. Therefore the conclusion in [39] doesn’t necessarily mean that the exponential mechanism is the best.
9 Discussion
The papers reviewed by this survey address the question of how to train a diﬀerentially private model with as little noise as possible. To summarize, there are generally four guiding principles for reducing the scale of noise. First, adding noise only one time is usually better than adding noise many times. This is because if we add noise many times, we have to split the privacy budget into many smaller portions and let each noise addition procedure use one portion. Because the budget allocated to each procedure is small and the scale of noise is inversely related to the privacy budget, the amount of noise added in each procedure is large. Furthermore, when we aggregate the outputs, the noise can grow even larger. Therefore one-time noise addition is usually better.
For example, when we train a logistic regression model, we can add noise to the training process, the target function or the ﬁnal model. Adding noise to the target function is a one-time procedure. This is also true for the ﬁnal model. However, as the training process is iterative, adding noise during training requires adding noise many times. According to our experience, noise addition in training process leads to signiﬁcantly worse performance.
Second, lower global sensitivity (compared to the result) leads to smaller noise. In one strategy to lower global sensitivity, some queries be approximated by combining the results of other queries, each of which have far lower global sensitivities than the original query. For example, [52] adds noise to the counts that generate the naive Bayes model instead of conditional probabilities of the model directly. The global sensitivity of each conditional probability is 1, which is too high to be useful. The global sensitivity of each underlying count is 1, which is much lower compared to the counts. By adding noise to those counts, we encounter lower global sensitivity.
Another approach is to modify the model. For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.
Third, use of public data, when available, can reduce the noise in some cases. For a private dataset, there is often a smaller public dataset drawn from a similar population. This public dataset can be from a previous leak or by consent of data owners. Because diﬀerentially private mechanisms distort private data, the smaller public dataset sometimes provides similar or better utility. According to [28, 29], such a public dataset can enhance the performance of diﬀerentially private mechanisms.
Fourth, for some models, iterative noise addition may be reasonable. There are some times when the sensitivity of output model parameters is very large
24

but the iterative algorithm has smaller sensitivity. This statement may seem counterintuitive, as the sum of sensitivities of all iterations should be similar to the sensitivity of the model parameters. However, in some cases, the sensitivity of each iteration is determined by the parameter before that iteration. Thus the sum of sensitivities of those iterations in fact relies on the training path. Excepting some extreme cases, the sum can be much smaller than the sensitivity of the model parameters. In this case, it seems necessary to add noise in the iterations.
For those models, one can consider trying the MCMC-based algorithm as in [48]. Likelihood function or loss function can be used as score functions and the Metropolis Hastings algorithm ensures that the output is from the same distribution as that in the exponential mechanism. This idea is still not widely used, however it seems possible that it improves learning performance.
In addition to these four ideas, some other issues warrant attention. For example, most diﬀerentially private mechanisms use clean and complete data as input, which is not always available in practice. Furthermore, traditional methods for missing data or pre-processing may not satisfy diﬀerential privacy. Thus mechanisms that can deal with incomplete data are desired. Such mechanisms can either release data, or be combined with other diﬀerentially private learning mechanisms.
When private data is discussed, medical data is typically oﬀered as an example application. However, medical datasets are often not relational. They may be temporal, and sometimes structural. Although we can transform such data, the transformation may lose some important information and increase sensitivity. Therefore, mechanisms specially designed for such data are required.
Another important question is whether privacy can be free, i.e., achieved at no cost to utility in diﬀerentially private learning. For privacy to be free, the noise required to preserve privacy might need to be smaller than noise from sample randomness. In that case, it wouldn’t change the magnitude of noise to take privacy into account. For example, [50] proves that (ǫ, δ)-diﬀerential privacy is free for learning models satisfying a certain set of conditions. The mechanism in [6] ensures free ǫ-diﬀerential privacy for regularized logistic regression√models and linear SVM models, where noise from sample randomness is O(1/ n) and the noise to preserve privacy is O(1/n). The mechanism in [28] also proves that the eﬀect of noise brought by√diﬀerential privacy is O(1/n), while the eﬀect from sample randomness is O(1/ n).
We should also consider the extent to which privacy is compatible with and related to the idea of generalization in machine learning. Intuitively, machine learning algorithms seek to generalize patterns gleaned from a training set avoiding the eﬀects of sample randomness. Ideally, these algorithms should be robust to small changes in the empirical distribution of training data. A model which ﬁts too heavily to individual examples loses generalizability and is said to overﬁt. Perhaps the goals of diﬀerential privacy and generalization are compatible.
25

Acknowledgments
The authors would like to thank Kamalika Chaudhuri for her comments. The authors are in part funded by NLM(R00LM011392).
References
[1] Boaz Barak, Kamalika Chaudhuri, Cynthia Dwork, Satyen Kale, Frank McSherry, and Kunal Talwar. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In ACM SIGACT-SIGMODSIGART Symposium on Principles of Database Systems, pages 273–282, 2007.
[2] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the SuLQ framework. In ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, pages 128–138, 2005.
[3] Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to non-interactive database privacy. In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, pages 609–618, 2008.
[4] Kamalika Chaudhuri and Daniel Hsu. Convergence rates for diﬀerentially private statistical estimation. In ICML, 2012.
[5] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in Neural Information Processing Systems, pages 289–296, 2008.
[6] Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Diﬀerentially private empirical risk minimization. In Journal of Machine Learning Research, pages 1069–1109, 2011.
[7] Kamalika Chaudhuri, Anand D. Sarwate, and Kaushik Sinha. Near-optimal diﬀerentially private principal components. In Advances in Neural Information Processing Systems, pages 998–1006, 2012.
[8] Rui Chen, Noman Mohammed, Benjamin C. M. Fung, Bipin C. Desai, and Li Xiong. Publishing set-valued data via diﬀerential privacy. In International Conference on Very Large Data Bases, pages 1087–1098, 2011.
[9] Graham Cormode. Personal privacy vs population privacy: learning to attack anonymization. In International Conference on Knowledge Discovery and Data Mining, pages 1253–1261, 2011.
[10] Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Thanh T. L. Tran. Diﬀerentially private summaries for sparse data. In International Conference on Database Theory, pages 299–311, 2012.
26

[11] Anindya De. Lower bounds in diﬀerential privacy. In Theory of Cryptography, pages 321–338, 2012.
[12] Cynthia Dwork. Diﬀerential privacy. In Encyclopedia of Cryptography and Security (2nd Ed.), pages 338–340, 2011.
[13] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In International Conference on the Theory and Applications of Cryptographic Techniques, pages 486–503, 2006.
[14] Cynthia Dwork and Jing Lei. Diﬀerential privacy and robust statistics. In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, pages 371–380, 2009.
[15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265–284, 2006.
[16] Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and diﬀerential privacy. In FOCS, pages 51–60, 2010.
[17] Cynthia Dwork and Adam Smith. Diﬀerential privacy for statistics: What we know and what we want to learn. 2008.
[18] Chengfang Fang and Ee-Chien Chang. Adaptive diﬀerentially private histogram of low-dimensional data. In Privacy Enhancing Technologies, pages 160–179, 2012.
[19] Arik Friedman and Assaf Schuster. Data mining with diﬀerential privacy. In International Conference on Knowledge Discovery and Data Mining, pages 493–502, 2010.
[20] Srivatsava Ranjit Ganta, Shiva Prasad Kasiviswanathan, and Adam Smith. Composition attacks and auxiliary information in data privacy. In KDD, pages 265–273, 2008.
[21] Moritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector computation. In Computing Research Repository, 2012.
[22] Ori Heﬀetz and Katrina Ligett. Privacy and data-based research. 2013.
[23] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John Pearson, Dietrich Stephan, Stanley Nelson, and David Craig. Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. page 4(8):e1000167., 2008.
[24] Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N. Wright. A practical diﬀerentially private random decision tree classiﬁer. In International Conference on Data Mining Workshops, pages 114–121, 2009.
27

[25] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Diﬀerentially private online learning. In Conference on Learning Theory, pages 24.1– 24.34, 2012.
[26] Prateek Jain and Abhradeep Thakurta. Diﬀerentially private learning with kernels. In International Conference on Machine Learning, pages 118–126, 2013.
[27] J.Czerniak and H.Zarzycki. Application of rough sets in the presumptive diagnosis of urinary system diseases. In Artiﬁcal Inteligence and Security in Computing Systems, ACS’2002 9th International Conference Proceedings, pages 41–51, 2002.
[28] Zhanglong Ji and Charles Elkan. Diﬀerential privacy based on importance weighting. In Machine Learning, pages 163–183, 2013.
[29] Zhanglong Ji, Xiaoqian Jiang, Shuang Wang, Li Xiong, and Lucila OhnoMachado. Diﬀerentially private distributed logistic regression using private and public data. page 7(Suppl 1): S14., 2014.
[30] Xiaoqian Jiang, Zhanglong Ji, Shuang Wang, Noman Mohammed, Samuel Cheng, and Lucila Ohno-Machado. Diﬀerential-private data publishing through component analysis. pages 19–34, 2013.
[31] Michael Kapralov and Kunal Talwar. On diﬀerentially private low rank approximation. In ACM-SIAM Symposium on Discrete Algorithms, pages 1395–1414, 2013.
[32] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? In IEEE Symposium on Foundations of Computer Science, pages 531–540, 2008.
[33] Jing Lei. Diﬀerentially private M-estimators. In Advances in Neural Information Processing Systems, pages 361–369, 2011.
[34] Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. In International Conference on Data Engineering, pages 106–115, 2007.
[35] Yang D. Li, Zhenjie Zhang, Marianne Winslett, and Yin Yang. Compressive mechanism: utilizing sparse representation in diﬀerential privacy. In Workshop on Privacy in the Electronic Society, pages 177–182, 2011.
[36] Ashwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan Venkitasubramaniam. l-diversity: Privacy beyond kanonymity. In International Conference on Data Engineering, page 24, 2006.
[37] Frank McSherry. Privacy integrated queries: an extensible platform for privacy-preserving data analysis. In SIGMOD Conference, pages 19–30, 2009.
28

[38] Frank McSherry and Kunal Talwar. Mechanism design via diﬀerential privacy. In FOCS, pages 94–103, 2007.
[39] Darakhshan J. Mir. Diﬀerentially-private learning and information theory. In International Conference on Extending Database Technology Workshops, pages 206–210, 2012.
[40] Darakhshan J. Mir and Rebecca N. Wright. A diﬀerentially private graph estimator. In International Conference on Data Mining Workshops, pages 122–129, 2009.
[41] Noman Mohammed, Rui Chen, Benjamin C. M. Fung, and Philip S. Yu. Diﬀerentially private data release for data mining. In International Conference on Knowledge Discovery and Data Mining, pages 493–501, 2011.
[42] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. In ACM SIGACT-SIGMODSIGART Symposium on Principles of Database Systems, pages 75–84, 2007.
[43] Sewoong Oh and Pramod Viswanath. The composition theorem for diﬀerential privacy. In Computing Research Repository, 2013.
[44] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, 2007.
[45] Benjamin I. P. Rubinstein, Peter L. Bartlett, Ling Huang, and Nina Taft. Learning in a large function space: Privacy-preserving mechanisms for SVM learning. In Computing Research Repository, 2009.
[46] Alessandra Sala, Xiaohan Zhao, Christo Wilson, Haitao Zheng, and Ben Y. Zhao. Sharing graphs using diﬀerentially private graph models. In Internet Measurement Conference, pages 81–98, 2011.
[47] Anand D. Sarwate and Kamalika Chaudhuri. Signal processing and machine learning with diﬀerential privacy: Algorithms and challenges for continuous data. pages 86–94, 2013.
[48] Entong Shen and Ting Yu. Mining frequent graph patterns with diﬀerential privacy. In KDD, pages 545–553, 2013.
[49] Adam Smith. Eﬃcient, diﬀerentially private point estimators. In Computing Research Repository, 2008.
[50] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In ACM Symposium on Theory of Computing, pages 813– 822, 2011.
[51] Latanya Sweeney. k-anonymity: A model for protecting privacy. In International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, pages 557–570, 2002.
29

[52] Jaideep Vaidya, Basit Shaﬁq, Anirban Basu, and Yuan Hong. Diﬀerentially private naive Bayes classiﬁcation. In Web Intelligence, pages 571–576, 2013.
[53] Staal A. Vinterbo. Diﬀerentially private projected histograms: Construction and use for prediction. In European Conference on Machine Learning (ECML) and Conference on Principles and Practice of Knowledge Discovery in Databases, pages 19–34, 2012.
[54] Qian Xiao, Rui Chen, and Kian-Lee Tan. Diﬀerentially private network data release via structural inference. In KDD, pages 911–920, 2014.
[55] Yonghui Xiao, Li Xiong, and Chun Yuan. Diﬀerentially private data release through multidimensional partitioning. In Secure Data Management, pages 150–168, 2010.
[56] Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. Functional mechanism: Regression analysis under diﬀerential privacy. In International Conference on Very Large Data Bases, pages 1364–1375, 2012.
[57] Xiaojian Zhang, Xiaofeng Meng, and Rui Chen. Diﬀerentially private setvalued data release against incremental updates. In International Conference on Database Systems for Advanced Applications, pages 392–406, 2013.
30

