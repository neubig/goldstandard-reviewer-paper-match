TRAINING NOISY SINGLE-CHANNEL SPEECH SEPARATION WITH NOISY ORACLE SOURCES: A LARGE GAP AND A SMALL STEP
Matthew Maciejewski1,2, Jing Shi1,3, Shinji Watanabe1,2, Sanjeev Khudanpur1,2
1 Center for Language and Speech Processing, The Johns Hopkins University, USA 2 Human Language Technology Center of Excellence, The Johns Hopkins University, USA
3 Institute of Automation, Chinese Academy of Sciences, China

arXiv:2010.12430v2 [eess.AS] 22 Feb 2021

ABSTRACT
As the performance of single-channel speech separation systems has improved, there has been a desire to move to more challenging conditions than the clean, near-ﬁeld speech that initial systems were developed on. When training deep learning separation models, a need for ground truth leads to training on synthetic mixtures. As such, training in noisy conditions requires either using noise synthetically added to clean speech, preventing the use of in-domain data for a noisy-condition task, or training using mixtures of noisy speech, requiring the network to additionally separate the noise. We demonstrate the relative inseparability of noise and that this noisy speech paradigm leads to signiﬁcant degradation of system performance. We also propose an SI-SDR–inspired training objective that tries to exploit the inseparability of noise to implicitly partition the signal and discount noise separation errors, enabling the training of better separation systems with noisy oracle sources.
Index Terms— speech separation, noisy speech, deep learning
1. INTRODUCTION
In recordings of speech with multiple people present, such as the case of conversational speech, it is common for the speech signals to overlap, as people talk simultaneously [1, 2]. Most speech technologies are designed to work on only a single speaker’s speech, suffering a degradation of performance in the overlapping speech condition [3]. It can even be difﬁcult for human listeners to understand this speech. Speech separation aims to solve this problem by producing multiple waveforms from a single mixture, each containing speech from only one speaker.
In the advent of the proliferation of deep learning, the performance of speech separation has been improved greatly [4–7], leading to a desire to build better systems that are robust to a wide variety of conditions [8–10] beyond the clean, near-ﬁeld data [11] that systems were initially developed on. Due to deep learning models requiring ground truth signals, systems are trained using synthetic mixtures; and, accordingly, noisy speech separation systems have typically been trained and evaluated on mixtures where the noise has been added synthetically as well, to great success [6, 8, 12, 13]. However, the use of synthetic noise prevents using any speech data with existing noise. Recordings of speech considered to be clean rarely exist outside of recording studios, with many practical applications including some level of noise [14], effectively disallowing in-domain training using this paradigm.
In cases where access to clean speech is not possible, such as the CHiME challenges [15, 16], there has been little success in using single-channel speech separation systems. In addition, in cases

with synthetic mixtures of real noisy data, performance has been shown to suffer [9]. It is unfortunately impossible to make perfect comparisons across these data paradigms—the standard evaluation metrics [17–20] require ground truth and are affected in the same manner as the systems are in training. However, the evidence available suggests there may truly be a gap in performance.
While many real conditions include reverberation—an additional challenge—in this paper, we focus our investigation on the noisy speech issue, comparing problem formulations and hypothesizing why separation system performance suffers when trained with mixtures of noisy speech. We demonstrate a gap in performance between systems trained in these two data paradigms by using simulated data—training models on the same mixtures but controlling whether the system has access to the clean or noisy oracle speech sources during training.
In addition, we propose a new objective function for more effectively training speech separation systems with synthetic mixtures of noisy speech signals. This function is designed to exploit the general orthogonality of unrelated audio signals along with the relative inseparability of noise mixtures to minimize the effect of separation errors resulting from a part of the signal deemed to be inseparable.
The core contribution of this work is to articulate an issue with training data for noisy speech separation systems and demonstrate the impact it can have on performance. Our proposed objective presents a promising avenue for exploring solutions to the problem.

2. SEPARATION FORMULATIONS AND CHALLENGES

The basic problem formulation of single-channel speech separation is to produce time-domain estimates sˆk(t) of speech signals sk(t) of length T with sample index t for each speaker k of K total speakers in a mixture x(t) with each of those speakers talking simultaneously:

K

x(t) = sk(t)

(1)

k=1

In nearly all state-of-the-art deep learning-based speech separation systems, models are trained with a loss function that encourages each sˆk to become equivalent to sk, requiring knowledge of the groundtruth speech signals. As such, these models are trained using synthetic mixtures, i.e. x(t) is not a real recording of multiple talkers speaking simultaneously, but rather a digital sum of each of K real speech recordings sk(t).

2.1. Noisy Separation Formulations
In many real-world situations with multiple people speaking simultaneously, a recording will include more than just clean speech, such

©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

as noise or reverberation [1]. In particular, we consider a formulation in which a mixture includes a noise signal n(t) as well:

K

x(t) = scklean(t) + n(t)

(2)

k=1

In this formulation, conventionally the noise signal is considered to be undesirable, so the target for this task remains simply the scklean(t) signals. Once again, the requirement of ground truth training targets
means the mixture must be a synthetic combination of clean speech
along with a separate noise signal.
Unfortunately the need for separate clean speech and noise
signals greatly reduces the amount of training data available and
precludes the ability for noisy in-domain training. Creating arti-
ﬁcial mixtures using already-noisy speech signals opens the door
to more sources of data, but the problem formulation becomes different. Rather than having a set scklean(t) of speech signals along with a noise signal n(t), we have a set of noisy speech signals snkoisy(t) = scklean(t) + nk(t) each consisting of a combination of unique speech and noise, which are summed to create the mixture

K

K

x(t) = snkoisy(t) =

scklean(t) + nk(t)

(3)

k=1

k=1

where we never have access to the “true” speech signal scklean(t). Without access to the clean ground truth scklean(t), the network
is instead trained with the noisy speech signals snkoisy(t) as target. Though arguably more desirable for the separation network to produce scklean(t) than snkoisy(t), it is not an inherently incorrect separation solution as long as the speech signals themselves have been
separated, particularly when taking into account the possibility to
then feed the noisy separated output into a de-noising speech en-
hancement network, something shown to be successful with syn-
thetic noisy mixtures [8]. However, this paradigm nevertheless likely
has issues stemming from inseparability of the noise mixtures.

2.2. Challenges of Noisy Data Formulation
Speech separation and speech enhancement systems generally rely on the spectro-temporal properties of the signals. Speech and noise can be separated based on their differing statistical properties, and two speech signals can be separated based on their spectro-temporal structure, notably their sparseness [21]. However, noise signals are not guaranteed to be spectrally sparse and can have similar statistical properties, particularly when datasets are collected in consistent environments. As a result, it may be very difﬁcult to separate two noise signals. A noise separation task is introduced in [22], but its wide variety of noise sources and avoidance of ambient/environment tracks is not representative of background noise in conversational recordings from a consistent environment.
When training a network to separate a set of noisy sources snkoisy(t), we are in essence asking the network to be able to discriminate each scklean(t) and nk(t) from {scllean(t), nl(t) | l = k}. But, discriminating the nk(t) from the nl(t)’s may be disproportionately difﬁcult compared to the other tasks, despite being unrelated to the separation of speech. As a result, it is likely that the performance of networks trained with snkoisy(t) as target struggle disproportionately compared to networks trained with clean sources scklean(t) as target. In addition, even if the network can successfully separate the noise signals, it must correctly match them with the speech signal they originated from. As such, we believe that developing a training objective which minimizes the network’s requirement to separate the

noise will allow for training of more powerful models using datasets featuring synthetic mixtures of real noisy speech.
We do note that this formulation is very similar to the formulation in the MixIT work [23], in which separation is learned using mixtures of mixtures. This work focuses on ground truth mixtures of speech rather than a combination of speech and noise. While this approach would address the noise-source assignment problem, we believe it would struggle to solve the overall noisy speech problem due to still requiring the noise signals to be separated. Our preliminary experiments using a similar approach were unsuccessful.

3. OBJECTIVE FUNCTION

To solve this problem, we seek to develop an appropriate loss function that can encourage the network to produce estimates of clean source signals sˆk(t) and a noise estimate nˆ(t)

sˆk(t) ≈ scklean(t)

(4)

K

nˆ(t) ≈ nk(t)

(5)

k=1

given a set of ground truth noisy speech mixtures snkoisy(t)

snkoisy(t) = scklean(t) + nk(t)

(6)

The estimates in equations 4 and 5 are meant to estimate the components of the formulation presented in equation 2, treating the sum of the individual noise signals nk(t) as one single noise source n(t), despite the ground truth matching the formulation of equation 3.

3.1. Design and Description

Our approach to producing an appropriate loss function for this task is to modify the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) loss function [17] to discount reconstruction errors that have been identiﬁed as belonging to the noise estimate. The goal is to partition the mixture implicitly—if a signal is known to be inseparable, it should be better to identify it as inseparable than get the separation wrong; if a signal is separable, it should be better to separate it than identify it as inseparable—relying on the assumption that speech is generally separable and noise is generally not.
One of the key assumptions we use for our loss function is that a sequence of audio samples can be modeled as a zero-mean stochastic process; so, while treated as vectors, two unrelated audio sources are approximately orthogonal. For this reason, from this point onward we will refer to the various components of a trial with vector notation: scklean, snkoisy, ˆsk, nk, nˆk, x ∈ RT . As a result of this orthogonality property, as long as no speaker is repeated in a mixture and the noise recordings are not reused, the following is true:

a, b = 0 ∀ a, b ∈ Px, a = b

(7)

where Px = {sc1lean, n1, . . . , scklean, nk} denotes the set of independent components present in the mixture x. It should be noted that this formulation cannot consider reverberation to be a separate source from speech, as the independence assumption is not valid.
In addition, the above property leads to the following projection results for any a, b, c ∈ Px:

a2

proja+b(a + c) = a + b 2 (a + b)

(8)

proja(a + b) = a

(9)

Fig. 1. Illustration of the relationship between sources, noises, and their combinations.

3.2.2. Validation Tuning
Another problem raised by this formulation is the issue of tuning performance. In this paradigm, we do not have access to data or metrics for the “true” task we are trying to complete. As such we cannot simply tune performance according to a held-out set.
In this case, we used SI-SDR on a noisy oracle validation set as a proxy function to tune λ. We swept λ up from 0 by increments of 0.1, selecting the value prior to an SI-SDR decrease of greater than 0.667, signifying the network beginning to erroneously classify speech as noise.

These results can be useful for identifying the magnitude of shared
components between two signals comprised of elements in Px.
The basic SDR function of SI-SDR, while only having access to snkoisy as ground truth, would be deﬁned as:

snoisy 2

SDR(ˆsk) := 10 log10

k noisy

(10)

sk − ˆsk 2

Our modiﬁed loss function, Estimated-Source-to-Separation-Error Ratio (ESSER) is deﬁned as follows:

ESSER(ˆsk, nˆ) :=

10 log10

ˆsk 2 (snkoisy − ˆsk) − λ ∗ proj(snkoisy−ˆsk)nˆ + projˆsk nˆ 2

(11)

ESSER differs from the SDR formulation in three main ways:
i. noise discount: λ ∗ proj(snkoisy−ˆsk)nˆ ii. orthogonality constraint: projˆsk nˆ iii. separation encouragement: ˆsk in numerator
Modiﬁcation (i.) exploits the projection in equation 9 to discount separation errors that have been identiﬁed in the noise estimate, illustrated in Figure 1, weighted by a tunable parameter λ to prevent the network from over-categorizing the signal as noise. The orthogonality constraint modiﬁcation (ii.) encourages the speech and noise estimates to not share content. Modiﬁcation (iii.) replaces the oracle snkoisy with the estimated source ˆsk, to further encourage the network to separate as much of the waveform as possible.

3.2. Issues
Two main issues arise that are essentially unavoidable in solutions to the noisy oracle source problem. Addressing them was necessary for ESSER-based systems.

3.2.1. Scaling
Most state-of-the-art separation techniques output waveforms with semi-arbitrary scaling and must be re-scaled for proper objective function evaluation [4–7, 23]. The SI-SDR objective [17] uses the result of equation 9 to scale the signal such that the reconstruction error ek is orthogonal to the source scklean. This does not work in the noisy source formulation, as the component nk that is in snkoisy but not the estimate ˆsk changes the projection to that of equation 8.
As a result, we scale the estimates by projecting the mixture onto them, which under-scales by the factor ˆsk 2/ ˆsk + ek 2. This is not optimal scaling, but converges to optimal as the reconstruction error decreases.

4. EXPERIMENTAL CONFIGURATION
4.1. Datasets
The data used for our experiments were new synthetic mixtures created using WHAM! [12]. While using synthetic data is not ideal, it is necessary for analyzing the variations in ground truth signals while controlling for other factors. We took the wsj0-2mix [11] mixtures consisting of clean speech from the WSJ0 dataset [24] and assigned each mixture two noise sources from the WHAM! noises, one for each source. The resulting samples can be conﬁgured for training or evaluation in a number of ways. First of all, the noises are scaled to be at a given signal-to-noise ratio (SNR) relative to their source, allowing simulations of the various SNRs present in real recordings. Secondly, the samples can be conﬁgured to mix the sources with their noise to produce two noisy samples, or they can be conﬁgured to produce two clean sources along with the mixture of noise, resulting in the “noisy oracle” and “clean oracle” formulations accordingly.
For our experiments, we used the 16 kHz sample rate and ‘min’ conﬁguration of the data. We evaluated datasets created with SNRs ranging from 25 dB to -5 dB in decrements of 5 dB, as well as clean speech and pure noise conﬁgurations.
4.2. Evaluation
For evaluation we use the SI-SDR metric [17]. For all speech signals across all conditions, we evaluate the raw SI-SDR value according to clean ground truth. This serves as a measure of the objective quality of the separated speech—as all dataset conﬁgurations have identical speech signals, they can be directly compared. For evaluating noise estimates, we use SI-SDR improvement.
4.3. Model Conﬁguration
For all of our experiments we used a standard SI-SDR–trained TasNet-BLSTM [4] with four Bi-directional Long Short-Term Memory (BLSTM) layers with 600 units in each direction. For the analysis and synthesis bases, we used 500 ﬁlters of length 5 ms with a shift of 2.5 ms.
Models were trained for 100 epochs using 4 second segments using the Adam [25] algorithm with an initial learning rate of 0.001. The learning rate is decreased by a factor of two if the validation loss does not improve for three consecutive epochs. In addition, gradient clipping is performed with a maximum 2 norm of 5. All networks were trained with either negative SI-SDR loss or negative ESSER loss in an utterance-level permutation-invariant manner [26].
We feel this setup is representative of state-of-the-art methods, as most modern architectures are based on a TasNet design [5,7,23], with nearly all exceptions using an SDR training objective [6].

Table 1. SI-SDR improvement [dB] of networks trained to separate only noise compared to only speech.
Noise Separation SI-SDRi: 0.4 Speech Separation SI-SDRi: 15.3

Table 2. Noisy separation comparison across training objectives and ground truth with identical mixtures. The SI-SDR system trained with noisy ground truth sources serves as a performance ﬂoor, while the clean ground truth source SI-SDR system serves as a ceiling.

Datset
SNR [dB]
∞ 15.0 10.0
5.0 0.0 −5.0

SI-SDR
Noisy Clean
15.3 15.0 11.9 13.5
9.0 12.0 5.0 10.4 −0.1 7.8 −9.0 3.5

ESSER
Noisy
15.3 11.9†
9.0 5.7 0.8 −9.3†

ESSER
λ Noise
0.0 – 0.0† 6.3 0.0 6.1 0.3 2.1 0.3 0.7 0.2† −57.4

(a) Separation SI-SDR [dB]

(b) Noise Estimate SI-SDRi [dB]

5. RESULTS AND DISCUSSION
The result of our experiment to separate pure noise is shown in Table 1. An SI-SDR improvement of 0.4 is well below the standards of performance for speech separation, where an identical system reached 15.3 dB on clean speech. It is safe to say that this architecture is effectively unable to separate noise.

Fig. 2. Evaluation of models trained with the SI-SDR objective with varying training data SNR across both ground truth paradigms.
Table 2 shows the main results of our separation networks trained on noisy mixtures across the training data paradigms where the noise is or is not included in the oracle speech. As expected, the quality of speech produced by a model trained with clean ground truth decreases as more noise is added to the mixture. However, the performance of systems trained with noisy ground truth degraded more rapidly, with a growing gulf to the clean data paradigm as noise increased.
Figure 2 shows two evaluation conditions, a nearly-clean 25 dB set and a noisy 0 dB set, evaluated with SI-SDR models trained over a variety of training sets with varying SNR in both ground truth paradigms. Interestingly, in the nearly-clean evaluation set, there was relatively minimal degradation as more noise was added to the training data, and little difference between the data paradigms. This suggests the noisy data paradigm has minimal impact on the network’s ability to separate speech without noise.
†Validation tuning of λ failed for these systems and was selected ex post facto. While these cases occurred outside the successful operating region of ESSER, it is worth noting the lack of robustness in the parameter tuning.

However, in the noisy evaluation condition, the models trained on clean speech performed very poorly, with only the clean oracle source models signiﬁcantly improving in performance as more noise was added to the training data. This suggests that adding noise to a model’s training data does little to improve performance unless containing proper annotation as to what is speech and what is noise.
Fig. 3. Sample section of magnitude spectra from the 0 dB evaluation set comparing ESSER system output to the oracle signals. The highlighted middle row is system output. The regions boxed in red are areas demonstrating noise suppression.
Table 2 also includes the results of our experiments regarding training models with the negative ESSER loss function. These systems show modest gains over the SI-SDR noisy baseline in the 5.0 dB and 0.0 dB datasets, with sample output shown in Figure 3. The system fell apart at the highest level of noise, -5.0 dB, though the SI-SDR system did as well. At the lower levels of noise, the system performed best with λ = 0, effectively turning off the noise mitigation penalty, and accordingly matching the SI-SDR baseline.
In the ESSER systems that did not break down in very high noise, we see that the noise estimate does show an SI-SDR improvement, despite having no direct supervision, though the estimate itself resembles the mixture with only slight noise accentuation. Oddly enough, the systems where the noise discount penalty was removed (λ = 0.0), the noise estimate achieved approximately 6 dB improvement. An analysis of samples suggests that the orthogonality constraint (ii.) in conjunction with the scaling method and mask-based separation method led the network’s noise estimate to produce components of the noise outside speech frequencies, not an overall estimate of the total noise.
6. CONCLUSION
We have demonstrated that there is a drop in performance in SISDR–trained separation systems trained on noisy data when the noise is present in the sources rather than being synthetically added to clean sources, impacting the effectiveness of in-domain training in noisy conditions. This degradation is likely due to requiring separation of two nearly-inseparable noise signals. As a result, we proposed an alternate loss function which exploits the inseparability of the noise to implicitly identify noise and minimize error contributed by failing to separate it. And, we have shown that this loss function can be used to train better systems on data with noisy sources which perform more effective separation on noisy mixtures.

7. REFERENCES
[1] S. Bengio and H. Bourlard, Machine learning for multimodal interaction. Springer, 2005.
[2] O¨ . C¸ etin and E. Shriberg, “Analysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: insights for automatic speech recognition,” in Proc. ISCA Interspeech, 2006.
[3] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimitriadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang, S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov, L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao, and T. Zhou, “Advances in online audio-visual meeting transcription,” in Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 276–283.
[4] Y. Luo and N. Mesgarani, “TasNet: Time-domain audio separation network for real-time, single-channel speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2018, pp. 696– 700.
[5] Y. Luo, Z. Chen, and T. Yoshioka, “Dual-path RNN: Efﬁcient long sequence modeling for time-domain single-channel speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020, pp. 46–50.
[6] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech separation by speaker clustering,” 2020.
[7] J. Shi, J. Xu, Y. Fujita, S. Watanabe, and B. Xu, “Speakerconditional chain model for speech separation and extraction,” 2020.
[8] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux, “WHAMR!: Noisy and reverberant single-channel speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020, pp. 696– 700.
[9] M. Maciejewski, G. Sell, Y. Fujita, L. P. Garcia-Perera, S. Watanabe, and S. Khudanpur, “Analysis of robustness of deep single-channel speech separation using corpora constructed from multiple domains,” in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), Oct. 2019, pp. 165–169.
[10] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, “Librimix: An open-source dataset for generalizable speech separation,” 2020.
[11] J. R. Hershey, Z. Chen, and J. Le Roux, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2016, pp. 31–35.
[12] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. Le Roux, “WHAM!: Extending speech separation to noisy environments,” in Proc. ISCA Interspeech, Sep. 2019.
[13] Y. Liu, M. Delfarah, and D. Wang, “Deep CASA for talkerindependent monaural speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6354–6358.

[14] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, B. Hoffmeister, M. L. Seltzer, H. Zen, and M. Souden, “Speech processing for digital home assistants: Combining signal processing with deep-learning techniques,” IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 111–124, 2019.
[15] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth CHiME speech separation and recognition challenge: Dataset, task and baselines,” in Proc. ISCA Interspeech, 2018.
[16] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj, D. Snyder, A. S. Subramanian, J. Trmal, B. B. Yair, C. Boeddeker, Z. Ni, Y. Fujita, S. Horiguchi, N. Kanda, T. Yoshioka, and N. Ryant, “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” 2020.
[17] J. Le Roux, S. T. Wisdom, H. Erdogan, and J. R. Hershey, “SDR – half-baked or well done?” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019, pp. 626–630.
[18] E. Vincent, R. Gribonval, and C. Fevotte, “Performance measurement in blind audio source separation,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462–1469, 2006.
[19] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “A short-time objective intelligibility measure for time-frequency weighted noisy speech,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2010, pp. 4214–4217.
[20] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 2, 2001, pp. 749–752 vol.2.
[21] E. Vincent, T. Virtanen, and S. Gannot, Audio Source Separation and Speech Enhancement, 1st ed. Wiley Publishing, 2018.
[22] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson, J. Le Roux, and J. R. Hershey, “Universal sound separation,” in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019, pp. 175–179.
[23] S. Wisdom, E. Tzinis, H. Erdogan, R. J. Weiss, K. Wilson, and J. R. Hershey, “Unsupervised sound separation using mixtures of mixtures,” 2020.
[24] J. Garofolo, D. Graff, D. Paul, and D. Pallett, “CSR-I (WSJ0) complete LDC93S6A,” 1993, Web Download. Philadelphia: Linguistic Data Consortium.
[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. of the 3rd International Conference on Learning Representations (ICLR), 2015.
[26] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multi-talker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 25, no. 10, pp. 1901–1913, 2017.

