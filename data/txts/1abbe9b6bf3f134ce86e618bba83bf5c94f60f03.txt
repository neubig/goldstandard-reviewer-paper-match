arXiv:1602.07435v1 [cs.GT] 24 Feb 2016

Parametric Prediction from Parametric Agents
Yuan Luo
Department of Information Engineering, The Chinese University of Hong Kong, yluo@ie.cuhk.edu.hk,
Nihar B. Shah
Department of Electrical Engineering and Computer Sciences, UC Berkeley, nihar@eecs.berkeley.edu
Jianwei Huang
Department of Information Engineering, The Chinese University of Hong Kong, jwhuang@ie.cuhk.edu.hk
Jean Walrand
Department of Electrical Engineering and Computer Sciences, UC Berkeley, walrand@berkeley.edu
We consider a problem of prediction based on opinions elicited from heterogeneous rational agents with private information. Making an accurate prediction with a minimal cost requires a joint design of the incentive mechanism and the prediction algorithm. Such a problem lies at the nexus of statistical learning theory and game theory, and arises in many domains such as consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous agents’ private information and incentivize agents with diﬀerent capabilities to act in the principal’s best interest, we design an optimal joint incentive mechanism and prediction algorithm called COPE (COst and Prediction Elicitation), the analysis of which oﬀers several valuable engineering insights. First, when the costs incurred by the agents are linear in the exerted eﬀort, COPE corresponds to a “crowd contending” mechanism, where the principal only employs the agent with the highest capability. Second, when the costs are quadratic, COPE corresponds to a “crowd-sourcing” mechanism that employs multiple agents with diﬀerent capabilities at the same time. Numerical simulations show that COPE improves the principal’s proﬁt and the network proﬁt signiﬁcantly (larger than 30% in our simulations), comparing to those mechanisms that assume all agents have equal capabilities.
Key words : mechanism design, aggregated prediction, crowdsourcing
1. Introduction
1.1. Background and Motivations
Prediction markets, which aggregate information elicited from people with private beliefs, have
served as a reliable tool for estimating the outcome of speciﬁc future events (see Berg and Rietz
1

Parametric Prediction from Parametric Agents
2 (2003)). For example, these markets have been used to predict the winners of election (see Wolfers and Rothschild (2011)), future demand for a product (see Hayes (1998)), and stock prices and returns (see Gottschlich and Hinz (2014)). In these markets, values of traded information depend on future outcomes, and the accuracy of prediction can be veriﬁed based on the realized outcomes.
With the emergence of several commercial crowdsourcing platforms such as Amazon Mechanical Turk and Microworkers, collecting information from people to make prediction has become much cheaper, easier and faster. However, the information collected from people (“the agents”) can be highly unreliable due to the agents’ insuﬃcient expertise and the lack of appropriate incentives. More speciﬁcally, in a crowdsourcing platform, the agents are heterogeneous as they come from diﬀerent countries and have diﬀerent skills, which leads to signiﬁcant variations of the work quality (see Karger et al. (2014)). Furthermore, agents may exert diﬀerent levels of eﬀort to ﬁnish the allocated task based on diﬀerent levels of payments, and diﬀerent agents may react diﬀerently to the same level of payment (see Liu et al. (2014)). The chosen level of eﬀort aﬀects their performance dramatically. Due to these issues, an appropriate incentive mechanism that exploits the agents’ heterogeneity whilst incentivizes appropriate eﬀort levels is crucial to a successful prediction system.
Besides eliciting high quality of information, the prediction performance also depends on the prediction behaviour of the surveyor (“the principal”). Without an appropriate prediction rule, the principal may not be able to eﬀectively utilize the collected information and may obtain an inaccurate prediction results. This motivates us to study the incentive mechanism design together with the prediction rule optimization.
The resultant problem of joint design poses a signiﬁcant challenge due to the following reasons. First, due to the incorporation of the prediction problem, the objective of incentive mechanism changes from eliciting agents’ information truthfully to minimizing the prediction error. As the prediction error is a result of the agents’ information and actions, the designed mechanisms not only needs to motivate agents to report their truthful estimation information, but also needs to make sure that agents take appropriate actions. Hence, we cannot directly implement existing

Parametric Prediction from Parametric Agents
3 strictly proper scoring rules, e.g., the quadratic scoring rule (see Brier (1950), Selten (1998)), that only promotes truthfulness among agents to address the joint problem.
Second, the designed mechanism needs to solve both moral hazard and adverse selection problems simultaneously. Moral hazard results from the inability of the principal to observe an agent’s actions (i.e., eﬀort exerted by the agent), while adverse selection corresponds to the inability of knowing an agent’s private information (i.e., the expertise of an agent). This is diﬀerent from most incentive mechanisms designed in the existing works, which separate the mechanism design from the prediction problem and address either “hidden actions” (i.e., moral hazard) (see e.g., Fang et al. (2007), Ioannidis and Loiseau (2013)) or “hidden information” (i.e., adverse selection) (see e.g., Frongillo et al. (2015), Abernethy et al. (2015)).
Nevertheless, we formulate and optimally solve a “parametric” form of this joint design problem. More speciﬁcally, the principal desires to predict a parameter of a known distribution. Each agent is modeled in a parametric fashion, with her expertise governed by a single parameter that is the agent’s private information. We assume that agents are heterogeneous as they have diﬀerent levels of expertise. While each agent aims to maximize her own expected payoﬀ (i.e., the revenue minus the eﬀort cost), the principal optimizes a joint utility that trades oﬀ the prediction error and the monetary costs. For ease of exposition, we refer to the principal as “she” and each agent as “he”.
1.2. Results and Contributions We focus on the interactions among a principal and multiple agents, and design an appropriate incentive mechanism to facilitate the parametric prediction process. Speciﬁcally, we design a mechanism, which we call “COPE” (standing for “COst and Prediction Elicitation”), that jointly optimizes the principal’s payoﬀ in terms of the payments made to the agents and the prediction error incurred. COPE provides a systematic way for the principal to incentivize all participating agents to report their estimations truthfully and exert appropriate amounts of eﬀort based on their respective capabilities.
We summarize our key results and the main contributions as follows.

Parametric Prediction from Parametric Agents
4 • Theoretic signiﬁcance: We relax several critical assumptions that are common in papers in the
related literature, i.e., the costs incurred by the agents are all known to the principal, the agents do not incur costs for eﬀorts, and the agents are all homogeneous. Hence, the proposed model pushes this line of theoretical focused research into more realistic settings.
• Optimal incentive mechanism design: We study a generic incentive mechanism design problem situated in a prediction process. To study the optimal prediction solution, we design the COPE mechanism, which ensures that all participating agents report their estimations truthfully and exert appropriate amounts of eﬀort based on their respective capabilities, meanwhile maximizes the principal’s expected payoﬀ.
• Observations and insights: Our results show that, with Gaussian estimation noise, when the agent’s marginal cost is independent of his amount of exerted eﬀort, the principal should conduct a crowd-tender mechanism, by soliciting service only from the agent with the lowest reported cost. On the other hand, when the marginal costs depends on the exerted eﬀort, the optimal mechanism is in the form of crowd-sourcing, where the principal recruits multiple agents to complete the task.
• Numerical results: Simulation in Section 5 show that COPE improves both the principal’s proﬁt and the network proﬁt, comparing to those mechanisms that assume all agents have equal capabilities and incentivize agents exert the same amount of eﬀort. Moreover, the performance gap between COPE and the centralized benchmark solution is less than 3% under the quadratic cost function and 10% under the linear cost function.
The rest of the paper is organized as follows. After reviewing the literature in Section 2, we describe the system model in Section 3, and design the incentive mechanism in Section 4. In Section 5, we provide the simulations results. We conclude in Section 6.
2. Related Work
Mechanism design for truthful elicitation of agents’ opinions is an extensively studied problem, most recently investigated are in the context of crowdsourcing (see, e.g., Cavallo and Jain (2015), Miller et al. (2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)). In contrast to our work, this

Parametric Prediction from Parametric Agents
5 line of literature does not consider the prediction aspect, and only focuses on the elicitation problem alone. Mechanism design for truthful elicitation of agents’ opinions is also studied in the context of prediction markets (see, e.g., Wolfers and Zitzewitz (2004), Conitzer (2009)). These results, however, study the scenario where the agents take the responsibility of aggregating information. Our paper concerns a diﬀerent setting and objective, in which the principal is in charge of information gathering and making the ﬁnal prediction. Hence, the mechanism we design should not only elicit the agents’ information but also incentivize agents to exert appropriate eﬀort.
The scenario becomes quite diﬀerent when prediction must be done by taking incentives into account, and calls for the design of new procedures catering to both aspects. The recent studies (see, e.g., Fang et al. (2007), Ioannidis and Loiseau (2013), Frongillo et al. (2015), Cai et al. (2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are diﬀerent, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have diﬀerent types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents’ conﬁdences, again without considering the costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the agents cannot fabricate their observation, but may lie about their costs, and design a mechanism to ensure that agents truthfully report their costs. In contrast, we assume a more general scenario where the agents can be strategic in choosing and reporting their respective observations. Cai et al. (2015) propose a monetary mechanism to collect data and to perform an estimation of a function at one random point. However, they assume that the agent always reports truthfully once he makes an observation. In contrast, our work considers strategic agents and proposes an optimal mechanism to ensure truth-telling by the agents.

Parametric Prediction from Parametric Agents
6
Figure 1 Sequence of interactions between the principal and the agents.
3. Problem Setting
We begin with a formal description of the problem formulation. Through this description, we will set up notations to capture the behavior of the agents, the objective of the principal, the prediction problem, and the mechanism-design problem.
3.1. System Model We consider a setting where the principal wishes to make a parametric prediction, that is, to form an informed estimate about a parameter x∗ ∈ X ⊆ R. Predicting the winner of an election and predicting box oﬃce results for movies are two motivating examples. We assume that x∗ has a prior distribution that is publicly known, for instance, from the results of an earlier election. We assume that the principal will come to know the precise value of x∗ sometime in the future, for instance, upon completion of the election. In order to make a prediction, the principal queries a set A = {1, . . . , N } of N agents to report their observations.
Figure 1 pictorially depicts the interaction between agents and the principal, including the agents’ reporting strategy and the principal’s prediction and payment decision. Before we explain each individual components of the ﬁgure, we ﬁrst introduce some notations used to characterize the agents’ strategies and types in Figure 1.
Eﬀort Level and Cost Type: When queried by the principal, an agent can put in some eﬀort to form an “observation” whose value is known only to that agent. We assume that the observation of any agent is noisy, and the distribution of the observation yn comes from a parameterised family of distributions φ(x∗, qn), where qn represents the eﬀort exerted by agent n to make observation. The higher value of qn, the more eﬀort agent n exerts, and thus the better quality of agent n’s

Parametric Prediction from Parametric Agents
7
observation. An example that we focus on subsequently in the paper is additive Gaussian noise (see, e.g., Fang et al. (2007), Cai et al. (2015)), i.e., yn ∼ N (x∗, q1n ), where yn follows from Gaussian distribution with mean x∗ and variance 1/qn. Conditioned on x∗, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn, ∀n ∈ A) be the observation vector and q = (qn, ∀n ∈ A) be the eﬀort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others.
It is costly for each agent to exert a high level eﬀort when making an observation, and this cost not only depends on the eﬀort chosen by the agent, but also is aﬀected by the agent’s costtype parameter θn. The cost types of diﬀerent agents are allowed to be diﬀerent, capturing the heterogeneity of the agents. A smaller value of θn implies a higher capability of agent n. Speciﬁcally, we consider a publicly known cost function C : R+ × R+ → R+, and assume that the cost incurred by an agent n ∈ A with the cost type θn when exerting an eﬀort qn is C(qn, θn). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2.
The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed on support [θ, θ¯] for some 0 ≤ θ < θ¯ < ∞. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [θ, θ¯]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.
Next, we will discuss each individual components of Figure 1 sequentially. Reporting Observations and Making Payments: The principal employs monetary incentives to ensure that agents make their observations and report them to the principal. In order to incentivize agents to participate the prediction task, the payment to an agent must, at the least, cover the cost incurred by that agent in putting eﬀort to make the observation. However, since each agent’s cost parameter is known only to that agent, the principal needs to ask each agent to

Parametric Prediction from Parametric Agents
8
report his own cost type (Figure 1a). The agents are strategic, and any agent n ∈ A may report a cost type θˆn that is diﬀerent from his true cost type θn. Let θˆ = (θˆn, n ∈ A) be the reporting cost type vector.
As we will see subsequently, incentivizing diﬀerent agents to put diﬀerent levels of eﬀort depending on their respective cost types allows for a signiﬁcantly better prediction performance. The principal must incentivize these diﬀerent eﬀort levels, and the choice of these eﬀort levels is based on the agents’ reported cost types θˆ (Figure 1b). Let function Qp : [θ, θ¯]N → R+ denote the eﬀort that the principal requires an agent to exert. The function Qp depends on the cost types reported by the agents: Qp(θˆn, θˆ−n) represents the eﬀort required from agent n ∈ A, where θˆ−n = [θˆ1, . . . , θˆn−1, θˆn+1, . . . , θˆN ]T is the reported cost parameters of all agents except agent n. Here (and elsewhere in the paper), we use the superscript “P” to represent the principal. We let Qp(θˆ) = (Qp(θˆn, θˆ−n), ∀n ∈ A) be the eﬀort vector required by the principal.
Each agent n ∈ A is strategic and may exert an eﬀort qn = Qp(θˆn, θˆ−n) to suit his own interests. When choosing the eﬀort to exert, the agent may also exploit the fact that the principal cannot directly observe the actual eﬀort exerted. Upon exerting the chosen eﬀort qn, the agent obtains an observation yn (Figure 1c). The principal seeks to know the value of the observation yn, but the agent may report a strategically chosen value yˆn = yn to the principal (Figure 1d) that suits the agent’s own interests. We adopt the shorthand yˆ = (yˆn, n ∈ A) as the reporting vector. Based on the information obtained, the principal must make a prediction for the value of x∗ (Figure 1e).
The principal makes payment to each agent once she observes the true value of x∗. Speciﬁcally, we deﬁne the payment function as R : R × R × [θ, θ¯]N × → R+; the payment to agent n is R(x∗, yˆn, θˆn, θˆ−n), which depends on the value of x∗, the agent n’s reported observation yˆn, and all agents’ reported cost parameters θˆ.
As indicated above, the model considered is a one-shot model, i.e., once the agents simultaneously report their observations, the principal determines the payments based on the agents’ reported cost parameters and observations only, with no further adjustments on reported values or the payments.

Parametric Prediction from Parametric Agents
9 Payoﬀ of Agent: Given the payment function announced by the principal, each agent n’s payoﬀ U a : R × [θ, θ¯] × R+ × R × [θ, θ¯]N → R is deﬁned as the diﬀerence between the payment received from the principal and the cost incurred in making the observation, and is given as

U a(x∗, θˆn, qn, yˆn, θn, θˆ−n) =R(x∗, yˆn, θˆn, θˆ−n) − C qn, θn ,

(1)

Here the superscript “A” indicates a term associated to the agents. Equation (1) shows that agent n ∈ A’s payoﬀ also depends on other agents’ reported cost parameters θˆ−n. When each agent n ∈ A chooses his strategy, i.e., his cost reported value θˆn, exerted eﬀort qn, and the reported observation yˆn, to maximize his expected payoﬀ. The expected payoﬀ of the agent n is calculated as

E[U a(x∗, θˆn, qn, yˆn, θn, θˆ−n)] = E[R(x∗, yˆn, θˆn, θˆ−n)] − C(qn, θn),

(2)

where the expectation is taken with respect to the distributions of x∗ and all agents’ cost parameters θ−n. Recall that each agent n only knows his own cost parameter θn, and only has distributional information about other agents’ cost parameters.
Payoﬀ of the Principal: Let function xˆ : R × R+ → R characterize the prediction made by the principal based on the agents’ reported observations yˆ and the eﬀort assumed to be exerted by the agents Qp(θˆ). Due to the inability of the principal to observe the agents’ exerted eﬀort, the principal makes prediction based on her own knowledge (i.e., the agents reported observation yˆ and the eﬀort Qp(θˆ) required from agents). Later in Section 4, we will show that the agents’ true eﬀort level are the same as that required by the principal by employing our proposed mechanism. Let p : R × R → R+ be the loss function that characterizes the penalty term for mistakes in the prediction. For instance, one could consider the squared loss p(x∗, xˆ(yˆ, qp)) = (x∗ − xˆ(yˆ, qp))2 as the penalty for the principal, where qnp = Qp(θˆn, θˆ−n) and qp = (qnp , ∀n ∈ A). We use qp as the shorthand notation for Qp(θˆn, θˆ−n), in order to show that qnp , for any n ∈ A, is a decision variable for the principal. In Section 4.2.1, we will show how the principal determines the desired eﬀort from the agents by taking the ﬁrst order derivative of her Bayes risk with respect to qnp .

Parametric Prediction from Parametric Agents
10

x∗ xˆ qn qnp
yn yˆn θn θˆn C(qn, θn)
Qp(θˆn, θˆ−n)
Bp(qp)

Table 1 Notations
A parameter needs to be estimate Prediction based by the principal Eﬀort exerted by agent n ∈ A Eﬀort desired by the principal from agent n ∈ A, calculated by the function Qp : [θ, θ¯]N → R+ True observation of agent n ∈ A Reported observation of agent n ∈ A True cost type of agent n ∈ A
Reported cost type of agent n ∈ A Cost incurred by an agent n ∈ A with the cost type θn when exerting an eﬀort qn Eﬀort desired by the principal from agent n ∈ A given all agents reported cost type Bayes risk of the principal’s prediction

We measure the utility gained by the principal through the prediction in terms of the Bayes risk

incurred under this loss function. The reason that we use Bayes risk is that it yields a real number

(not a function of xˆ or yˆ) for each prediction, and the principal’s posterior expected prediction loss

is equivalent to the Bayes risk (see Robert (2007), Berger (2013)). If all agents report their true observations (i.e., yˆ = y) and exert eﬀorts as desired by the principal (i.e., q = qp = Qp(θˆ)), then

the Bayes risk Bp : R × R → R+ is (see Robert (2007), Berger (2013))

Bp(qp) = inf E[ p(x∗, xˆ(y, qp))],

(3)

xˆ

where the expectation is taken with respect to x∗ and y. By assuming that the principal’s utility

with perfect prediction is zero, the Bayes risk chacterzies the penality for the principal’s mistakes

in the prediction, and the prinicial’s expected utility is just −Bp(qp). Correspondingly, the payoﬀ of the principal U p : R × RN × [θ, θ¯]N → R is then deﬁned as the diﬀerence between her utility

obtained from prediction and the monetary payments to all agents:

U p(x∗, qp, y, θˆ) = −Bp(qp) − R(x∗, yn, θˆn, θˆ−n).

(4)

n∈A
Here, we assumed without loss of generality that the monetary payment and the prediction loss is

normalized to be on the same scale.

For clarity, we list the key notations in Table 1.

3.2. Design Objective Before explaining the design objective, we begin by deﬁning two standard game-theoretic terms that are required for subsequent discussions.

Parametric Prediction from Parametric Agents
11

Definition 1. (BIC: Bayesian Incentive Compatibility) A mechanism satisﬁes the Bayesian incen-

tive compatibility (BIC) if for every agent n ∈ A, his expected payoﬀ satisﬁes (see Fudenberg and

Tirole (1991), Myerson (1979)) E U a(x∗, θn, Qp(θn, θ−n), yn, θn, θ−n) ≥E U a(x∗, θˆn, qn, yˆn, θn, θ−n) ,

∀(θˆn, qn, yˆn)=(θn, Qp(θn, θ−n), yn),

(5)

where the expectation is taken with respect to x∗ and all other agents cost parameters θ−n. BIC means that for any agent n, reporting the true cost parameter, exerting the eﬀort requested
by the principal, and reporting true observation will maximize his expected payoﬀ, given common knowledge about the distribution on agents cost parameters and when other agents are truthfully report their cost parameters.

Definition 2. (BIR: Bayesian Individual Rationality) A mechanism satisﬁes the Bayesian incentive rationality (BIR), if the expected payoﬀ of every agent n ∈ A is non-negative, given that he reports truthfully, exerts eﬀort as the principal desires, and assumes that all other agents report their cost parameters truthfully, that is (see Fudenberg and Tirole (1991), Myerson (1979)),

E U a(x∗, θn, Qp(θn, θ−n), yn, θn, θ−n) ≥ 0, ∀n ∈ A,

(6)

where the expectation is taken with respect to x∗ and all other agents’ cost parameters θ−n.

Assuming (without loss of generality) that the payoﬀ of an agent not participating in this process

equals zero, BIR means that an agent will participate only if his expected payoﬀ is at least as much

as that of a non-participating agent.

Based on the revelation principal (see Myerson (1979)), the problem of ﬁnding a mechanism that

maximizes the principal’s expected payoﬀ can be restricted to the set of mechanisms where agents

are willing to reveal their private information to the principal. Moreover, the principal cannot force

agents to accept the task. Hence, the problem that we want to solve is formalized as follows. The

goal is to design a mechanism, say M, that maximizes the principal’s expected while ensuring

truthful reports from the agents:

sup E U p(x∗, y, θ)

M

(7)

subject to: BIC and BIR in (5) and (6),

Parametric Prediction from Parametric Agents
12 where the expectation is taken with respect to x∗, y and θ, and the BIR condition makes sure that the agents are willing to participate in the game. In words, the goal is to design a mechanism such that: (i) the principal’s payoﬀ is maximized in expectation; (ii) the principal can elicit truthful information from all agents; and (iii) the principal can incentivise suitable eﬀort from the agents based on their respective cost parameters.
4. The COPE Mechanism
In this section, we present our mechanism “COPE” (COst and Prediction Elicitation) that provides an optimal solution to the problem (7) of parametric prediction from parametric agents. We will ﬁrst consider two speciﬁc settings in order to illustrate the key ideas behind COPE, and to obtain some concrete engineering insights. We will then proceed to present COPE in full generality.

4.1. Two Example Settings

We consider the following speciﬁc setting in this section. We consider the Gaussian case, where

we

assume

the

prior

x∗

∼

N

(µ

0

,

σ

2 0

),

and

the

observation

of

every

agent

n

follows

the

distribution

yn ∼ N (x∗, q1n ), independent of all other events. The values of µ0 and σ0 are assumed to be public knowledge. We assume θn ∼ Uniform[θ, θ¯], independent for every n ∈ A. We consider the squared

2-loss to measure the prediction error, namely, p(x∗, xˆ) = (x∗ − xˆ)2. In what follows, we consider

two cost functions: (i) the linear cost function C(q, θ) = qθ, and (ii) the quadratic cost function

C(q, θ) = 12 θq2.

4.1.1. Linear Cost Function C(q, θ) = qθ We ﬁrst consider the linear cost function C(q, θ) = qθ and discuss the corresponding COPE mechanism. Algorithm 1 presents the higher-level structure of the mechanism which corresponds to the steps in Figure 1. The optimality of the mechanism crucially relies on the careful construction of speciﬁc functions referred to in the algorithm, and these constructions are described below.
Recall that the function Qp : [θ, θ¯] × [θ, θ¯]N−1 → R+ speciﬁes the eﬀort that the principal requires an agent to exert, based on the cost parameters reported by all agents. In Theorem 1 subsequently, we show that when the cost function is linear, the principal requires only one agent to exert eﬀort.

Parametric Prediction from Parametric Agents
13

Algorithm 1 COPE Step 1: The principal announces a payment function R. Step 2: Every agent n ∈ A independently reports a cost type θˆn ∈ [θ, θ¯] to the principal. Step 3: The principal sends each agent n ∈ A a contract, which speciﬁes the eﬀort level Qp(θˆn, θˆ−n) along with values of functions π(θˆn, θˆ−n), K(θˆn, θˆ−n), and S(θˆn, θˆ−n) that comprise the function R. Step 4: Each agent n ∈ A exerts eﬀort qn and makes an observation yn. Step 5: Each agent n ∈ A reports an estimate yˆn. Step 6: The principal makes prediction xˆ. Step 7: The true value x∗ is realized. Step 8: The principal makes the payment R(x∗, yˆn, θˆn, θˆ−n) to every agent n ∈ A.

If there are multiple agents achieving the same minimum value of θˆn, the principal would randomly

choose one agent to exert eﬀort. This property is reﬂected in the following choice of function Qp:

Qp(θˆ , θˆ

)=

max{(2θˆn

−

θ)− 21

−

σ

−2 0

,

0

}

if n = arg minm∈A θˆm

(8)

n −n

0

otherwise.

The function Qp is designed to strike an optimal balance between the prediction error and the

monetary expenditure, accommodating the fact that the agents are heterogeneous. We deﬁne n0 = arg minm∈Aθˆm, that is, n0 is the agent with the lowest reported cost parameter.

We now characterize the function R that governs the payment made by the principal to the

agents. The payments to all agents other than agent n0 are zero, since these agents are not involved

in the observation and prediction procedure. The payment made to agent n0 is

R(x∗, yˆn0 , θˆn0 , θˆ−n0 ) = π(θˆn0 , θˆ−n0 ) − (x∗ − yˆn0 )2 · K(θˆn0 , θˆ−n0 ) + S(θˆn0 , θˆ−n0 ), (9)

where

π(θˆn0 , θˆ−n0 ) = π(θˆn0 ) = θˆn0 (2θˆn0 − θ)− 12 − θ¯σ0−2 + 2[(2θ¯− θ) 12 − (2θˆn − θ) 21 ], K(θˆn0 , θˆ−n0 ) = K(θˆn0 ) = θˆn0 (2θˆn0 − θ)−1, and S(θˆn0 , θˆ−n0 ) = S(θˆn0 ) = θˆn0 (2θˆn0 − θ)− 12 .

(10)

Let us explain the main ideas behind the above choices. The detailed proof can be found in Appendix. First, The term (x∗ − yˆn0)2 in (9) ensures that the agent reports his observation truthfully. This is because no other terms in (9) depend on yˆn0, and the value of calculated by K(θˆn0)

Parametric Prediction from Parametric Agents
14

is always positive. Hence, when the agent chooses the reporting observation strategy to maximize his expected payoﬀ, he focuses on minimizing the term Ex∗[(x∗ − yˆn0)2] whose value is minimum only when the agent reports his truthful observation, i.e., yˆn0 = yn0. Second, we can verify that the expected payoﬀ of the agent is maximized only when the agent truthfully reports his cost parameter, given the agent reports his true observation. Third, the choices of functions K and S ensure that the agent exerts an eﬀort as desired by the principal. As the term (x∗ − yˆn0)2 makes the agent reports his true observation, we can verify that the expected payoﬀ of the agent is maximized only when the agent chooses qn0 = Qp(θn0, θ−n0). Finally, the function π is designed to ensure that the principal’s expected payoﬀ deﬁned in (4) is maximized while ensuring BIC and BIR condition is satisﬁed. As we shown that the term (x∗ − yˆn0)2 and the choices of functions K and S guarantee the truthful behavior of the agent, i.e., yˆn0 = yˆn and qn0 = Qp(θn0, θ−n0), the expected value of R simply equals to the value of the function π. In such case, we can focus on deriving the function of π to maximize the expected payoﬀ the principal.
Then we characterize the prediction decision made by the principal. After collecting all agents reported observations, the principal makes the prediction xˆ based on the following equation:

µ0 · 1/σ02 + qnp · g(yˆn0 )

xˆ(yˆn0 , qnp0 ) =

0
1/σ2 + qp

,

0

n0

(11)

where qnp = Qp(θˆn0 , θˆ−n0 ), and function g : R → R is deﬁned as g(yˆn0 ) = yˆn0 + (yˆnp0 −µ20) .

0

(qn0 ·σ0 )

Here, the predictor xˆ employed by the principal is the standard Bayes estimator operating on

the agents’ responses and this predictor does not aﬀect the payoﬀ of the agents.

We prove that the proposed COPE mechanism is optimal.

Theorem 1. Under the linear cost function C(q, θ) = qθ, COPE satisﬁes the BIC and BIR condition deﬁned in (5) and (6) and maximizes the principal’s expected payoﬀ deﬁned in (7).

We provide the detailed proof in Appendix A. An important consequence of the theorem is that the optimal mechanism in the case of linear costs awards the task to the single agent with the lowest bid. This corresponds to what we call a “crowd-tender” system where all agents submit their

Parametric Prediction from Parametric Agents
15
cost parameters, and the lowest bidder is awarded the task. We will discuss the intuition behind such a result in more details in Section 4.1.3.
4.1.2. Quadratic Cost Function C(q, θ) = 21 θq2 We now consider a quadratic cost function C(q, θ) = 12 θq2 and present COPE for this setting. The higher level structure of COPE is again given by Algorithm 1, and the speciﬁc functions referred to in the algorithm is provided below.
Under COPE, the function Qp : [θ, θ¯] × [θ, θ¯]N−1 → R+ that governs the eﬀort that the principal requires an agent to exert is given as

Qp(θˆn, θˆ−n) = (2θˆn − θ)−1(W (θˆ))−2,

(12)

where W is the solution of the equation [W (θ)]3 − σ12 [W (θ)]2 = m∈A 2θm1−θ . An explicit (although 0
cumbersome) solution of W is provided in (103) of Appendix B. As in the case of linear costs, the function Qp is designed to optimally harness the heterogenity
of the agents in order to minimize the prediction error with a small enough payment. Note that in contrast to the linear case (8), here the principal requires every agent to exert a positive eﬀort.
We deﬁne the function R that governs the payment to any agent n as

R(x∗, yˆn, θˆn, θˆ−n) = π(θˆn, θˆ−n) − (x∗ − yˆn)2 · K(θˆn, θˆ−n) + S(θˆn, θˆ−n),

(13)

where

π(θˆ , θˆ

1 )=

θˆ

·

Qp(θˆ , θˆ

) 2+

θ¯
Qp(z, θˆ ) 2dz ,

n −n 2 n

n −n θˆ

−n

n

K(θˆn, θˆ−n) = Qp(θˆn, θˆ−n) + 1/σ02 2θˆn · Qp(θˆn, θˆ−n),

S(θˆn, θˆ−n) = Qp(θˆn, θˆ−n) + 1/σ02 θˆn · Qp(θˆn, θˆ−n).

(14)

These functions have a form similar to those in the case of linear costs (9), except that these functions depend on the reported cost parameters of all N agents, whereas the corresponding functions in the linear cost setting depended only on the reported cost parameter of one agent. The remaining higher level intuition behind this construction is similar to that behind the linear-cost case described in the previous section.

Parametric Prediction from Parametric Agents
16

The principal uses the Bayes estimate as her predictor:

xˆ(yˆ, qp) = (1 − N )µ0/σ02 + n∈A (1/σ02 + qnp ) · yˆn , 1/σ02 + n∈A qnp
where qnp = Qp(θˆn, θˆ−n).

(15)

The following theorem establishes the optimality guarantee of COPE under quadratic costs.

Theorem 2. Under the quadratic cost function C(q, θ) = 12 θq2, COPE satisﬁes the BIC and BIR condition deﬁned in (5) and (6) and maximizes the principal’s expected payoﬀ deﬁned in (7).

The detailed proof is provided in Appendix B. As COPE under the quadratic cost function requires all agents to exert certain eﬀort and to report their observation, this corresponds to a “crowd-sourcing” system.

4.1.3. Engineering Takeaways Our results show that interestingly, it is optimal for the principal to call for a crowd-tender when the cost function is linear, while it is optimal to design a crowd-sourcing mechanism when the cost function is quadratic. Informally, the cost function acts as a regularizer on the choice of eﬀort levels qp, and the dichotomy of these two cost functions is related to the sparsity inducing properties of the 1-regularizer, and the lack thereof of the (squared) 2-regularizer.

4.2. General Setting
In this section, we will present COPE under more general forms of the cost function, the noise distribution, the prior distribution, and the prediction loss function. Under these general conditions, the structure of the mechanism remains identical to Algorithm 1. We will show that COPE is optimal and feasible under certain regularity conditions.
4.2.1. Assumptions Cost Function We ﬁrst deﬁne the general cost function of the agent n ∈ A. Speciﬁcally, for agent n ∈ A, his cost function C : R+ × R+ → R+ is C(qn, θn) = 0qn c(z, θn)dz, where c(z, θn) is the the marginal cost function. We assume that the marginal cost function c : R+ × R+ → R+ satisﬁes:
∂c(q, θ) ∂c(q, θ) ∂2c(q, θ) ∂2c(q, θ) ∂q > 0, ∂θ > 0, ∂θ2 0, ∂q∂θ 0. (16)

Parametric Prediction from Parametric Agents
17

where the ﬁrst inequality shows a nondecreasing marginal cost in agent’s eﬀort level, the second

and third inequalities show that the marginal cost is monotonically increasing and convex in the

cost parameter θn, the last inequality implies that the marginal cost with respect to cost parameter

θn is increasing in eﬀort q. These assumptions are widely used to model the cost function (see, e.g.,

Che (1993), Chen et al. (2008) and references therein).

The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [θ, θ¯] → R+ and probability density function f : [θ, θ¯] →

R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, diﬀerentiable, and log concave in [θ, θ¯]. This is a regularity condition often

assumed in auction contexts (see, e.g., Myerson (1981)). This assumption is satisﬁed by a wide

range of distributions, such as the uniform, gamma, and beta distributions. See Rosling (2002) for

an extensive discussion on log concave probability distributions.

Observation and Loss Function We assume that the distribution of the observation yn of any

agent n ∈ A comes from a parameterized family of distributions φ(x∗, qn), where qn represents the

eﬀort exerted by agent n to make observation. A typical parameterized distribution, for example,

is the Gaussian distribution with mean x∗ and variance 1/qn.

Let xˆ : RN × RN+ → RN be the prediction function that characterizes the prediction made by

the principal, and p : R × R → R+ be the loss function that characterizes the penalty term for

mistakes in the principal’s prediction. A typical loss function, for example, is the squared loss p(x∗, xˆ(y, qp)) = [x∗ − xˆ(y, qp)]2, where qnp = Qp(θˆn, θˆ−n) and qp = (qnp , ∀n ∈ A). We measure the

utility gained by the principal through the prediction in terms of the Bayes risk. Here, the Bayes

risk is calculated under the loss function p. Speciﬁcally, if all agents report their true observations (i.e., yˆ = y) and exert eﬀorts as desired by the principal (i.e., q = qp = Qp(θˆ)), then the Bayes risk

Bp : R × R → R+ is

Bp(qp) = inf E[ p(x∗, xˆ(y, qp))],

(17)

xˆ

where the expectation is taken with respect to x∗ and y. We assume that such a Bayes estimator

minimizing the Bayes risk exists, and that the Bayes risk is ﬁnite.1

Parametric Prediction from Parametric Agents
18

We will also assume the existence of a function a : R × R → R+ using which the principal may

measure the accuracy of an agent’s report. Speciﬁcally, we assume that if yn is generated according

to agent n’s observation distribution, then we assume that a satisﬁes

yn

∈ arg inf E[

a

(

x∗

,

y

† n

(

yn

))],

(18)

yn†

where function yn† : R → R characterizes the reporting strategy of the agent n ∈ A given his true

observation is yn, the inﬁmum is over all measurable functions of the observation yn, and the

assumption says that the identity function is a minimizer of the expected value of a when its ﬁrst

argument is x∗. For instance, we considered a(x∗, yˆn) = (x∗ − yˆn)2 earlier in the two motivating

examples involving the Gaussian distribution.

We let Ba : R × R → R+ denote the associated Bayes risk:

Ba(qn) = inf E[ a(x∗, yn)],

(19)

yn

where the expectation is taken with respect to x∗ and yn, and the distribution of yn depends on the

agent’s exerted eﬀort qn. The Bayes risk of the principal, i.e., Bp(qp), characterizes the principle’s

expected payoﬀ loss due to the diﬀerence between the true value x∗ and her own prediction xˆ;

while the Bayes risk of the agent, i.e., Ba(qn), characterizes the agent n ∈ A’s expected payoﬀ loss due to the diﬀerence between the true value xˆ and his reporting prediction yˆn.
We assume that the Bayes risk of the principal and the agent satisfy the following monotonicity

conditions:

∂Bp(qp) ∂qnp

0, dBa(qn) dqn

∂2Bp(qp) 0, ∂qp 2
n

0, d2Ba(qn) dqn2

∂2Bp(qp) 0,
∂qnp ∂qmp

0, ∀m = n.

(20)

In Section 4.1, we can verify that under the Gaussian distribution, the Bayes risk of the principal

is Bp(qp) =

1 2

and the Bayes risk of the agent is Ba(qn) =

1 2

, both satisfy (20).

1/σ0 + n∈A qnp

1/σ0 +qn

We assume that the principal has designed a mechanism that can control the agents’ exerted

eﬀort and elicit agents to report their observation truthfully. Hence, from the principal’s point of

view, qnp , for any n ∈ A is a decision variable for the principal, and we can take the derivative of the principal’s Bayes risk with respect to qnp in (20). However, in reality, the agents would strategically

Parametric Prediction from Parametric Agents
19 choose their exerted eﬀort. Hence, we need to design a mechanism that involves a carefully designed function Qp, so that the agents would put the eﬀort as the principal expected and truthfully report their observations.
Given these preliminaries, we now present our mechanism for this setting.

4.2.2. Mechanism Our proposed mechanism COPE for the general setting also follows Algo-

rithm 1, with the speciﬁc functions detailed below. The function Qp : [θ, θ¯] × [θ, θ¯]N−1 → R+ that governs the eﬀort that the principal requires an

agent to exert is given as the solution of the equation

maxQp Eθ

− Bp Qp(θˆ)

− C Qp(θˆn, θˆ−n), θn
n∈A

∂C Qp(θˆn, θˆ−n), θn −
n∈A ∂θn

· F (θn) f (θn)

s.t. Qp(θˆn, θˆ−n) is nonincreasing in θˆn, ∀n ∈ A.

(21)

We deﬁne the function R(·) that governs the payment to any agent n as

R(x∗, yˆn, θˆn, θˆ−n) = π(θˆn, θˆ−n) − a(x∗, yˆn) · K(θˆn, θˆ−n) + S(θˆn, θˆ−n),

(22)

where

π(θˆ , θˆ ) =C Qp(θ , θ

), θ

+

θ¯ ∂C(Qp(z, θ−n), η) dz,

n −n

n −n n θn ∂η

K(θˆ , θˆ

c(Qp(θˆn, θˆ−n), θˆn) )=−

,

n −n

dBa(qn)/dqn

qn =Qp (θˆn ,θˆ−n )

S(θˆ , θˆ

c(Qp(θˆn, θˆ−n), θˆn) · Ba(qn) )=−

,

n −n

dBa(qn)/dqn

qn =Qp (θˆn ,θˆ−n )

(23) (24) (25)

The principal uses the Bayes estimate as her predictor: xˆ(yˆ, qp) = arg infxˆ E p x∗, xˆ(yˆ, qp) . The detailed form of the principal’s estimation depends on the distribution of each agent’s observation φ(x∗, qn) deﬁned in Section 3.1 and the loss function p. With the Gaussian distribution and quadratic loss function adopted in Section 4, the principal’s predictor is calculated as xˆ(yˆ, qp) = (1−|Ap|)µ01//σσ0202++ nn∈∈AApp(q1np/σ02+qnp )·yˆn , where qnp = Qp(θˆn, θˆ−n) is the decision variable of the principal, Ap is the set of agents recruited by the principal to report their estimation, and |Ap| is the number of

agents in the set Ap.

Parametric Prediction from Parametric Agents
20

4.2.3. Guarantees The following theorem establishes the optimality guarantees of COPE.

Theorem 3. COPE satisﬁes the BIC and BIR condition deﬁned in (5) and (6) and maximizes

the principal’s expected payoﬀ deﬁned in (7) if

∂c Qp(θn, θ−n), θn ≤ 0,

(26)

∂θn

where function c characterizes the agent’s magical cost and is deﬁned in (16).

Condition (26) implies that the marginal cost of the agent should decrease with the agent’s cost type, so that COPE can induce the truthful behavior of the agents. We note that condition (26) is satisﬁed under the Gaussian case when θ follows from uniform distribution, as discussed in Appendix A and B.
5. Simulations

We conduct numerical studies to evaluate the performance of COPE. In particular, we investigate the amount of gain that can be achieved by (optimally) exploiting the heterogeneity of the agents. We ﬁrst consider an integrated system, where the principal and all agents act as an integrated decision maker to maximize their aggregate proﬁt (called network proﬁt). We denote the network proﬁt achieved under the integrated system as the centralized benchmark.2 Then we describe the details of the homogeneous benchmark mechanism under both the linear and quadratic cost function. Finally we compare the performance of COPE to the homogeneous benchmark in terms of principal’s expected payoﬀ, expected prediction error and total payment made by the principal to the agents.

5.1. Centralized Mechanism

In the integrated system, the integrated player (the principal and all agents) knows the precise value of θ (i.e., all agents’ cost types). Moreover, all participated agents would exert the eﬀort that maximizes the network proﬁt. Speciﬁcally, the expected network proﬁt is deﬁned as the diﬀerence between her utility obtained from prediction and the cost of all agents:

U np(q, θ) = −Bp(q) − C qn, θn ,

(27)

n∈A

Parametric Prediction from Parametric Agents
21

where the utility gained through the prediction is measured in terms of the Bayes risk given in

(17). Let function qo : R+ × [θ, θ¯] → R+ be the solution that maximizes the network proﬁt deﬁned

in

(27).

By

focusing

on

the

case

x∗

∼

N

(

µ

0

,

σ

2 0

),

we

have

the

optimal

solution

under

linear

cost

function as:

√

qo = max{1/ θn − 1/σ02, 0}, if n = arg minm∈A θm,

(28)

n 0,

otherwise.

The optimal solution under quadratic cost function is

qo = 1 · 1 ,

(29)

n θn W o(θ) 2

where the function W o : [θ, θ¯]N → R+ is the solution of the below equation:

W o(θ) 3 − 1 · W o(θ) 2 −

1 = 0.

σ02 m∈A θm

(30)

In the integrated system, the integrated player makes the prediction as

xˆ = µ0/σ02 + n∈A yno · qno , 1/σ02 + n∈A qno

(31)

where yno is agent n ∈ A’s true observation.

5.2. Homogenous Mechanism

The homogenous mechanism assumes all agents to be identical (although in practice they are not),

and hence does not elicit the cost parameters of individual agents. In the absence of this knowledge, the principal operates under the belief that every agent’s cost parameter equals θ† ∈ [θ, θ¯]. The

principal thus chooses payment function Rhom := α(θ†) − β(θ†) · (x∗ − yˆn)2, where the function α : [θ, θ¯] → R+ and the function β : [θ, θ¯] → R+ are chosen to incentivize every agent n to exert

optimal eﬀort and report observations truthfully in a manner that maximizes the principal’s payoﬀ.

We

focus

on

the

case

where

x∗

∼

N

(

µ

0

,

σ

2 0

).

Let

function

q†

:

R+

× [θ, θ¯]

→

R+

be

the

eﬀort

that

the principal requires every agent to exert, based on the principal’s belief that every agent’s cost

parameter equals to θ†. Then the principal makes the prediction as

xˆ = µ0/σ02 + q† n∈A g(yˆn) , 1/σ02 + N · q†

(32)

Parametric Prediction from Parametric Agents
22

where yˆn is the agent n’s reported observation, and the function g : R → R is deﬁned as

yˆn − µ0 g(yˆn) = yˆn + q† · σ02 . (33) Linear Cost Function: Under the linear cost function, the choices of functions q†, α, and β

are

q†(N, θ†) = 1

1 √

1 −

,

N θ† σ02

α(θ†) = 1/σ02 + q† · θ†q† + θ†q†, β(θ†) = 1/σ02 + q† 2 · θ†

The principal chooses the function α(·) to make sure that the agent n with the cost type θ†

is willing to participate the prediction task, and chooses the function β(·) to make sure that the

agent n exerts the eﬀort qn = q†(N, θ†) as the principal desires.

Recall that the actual cost parameter of the agent n ∈ A is θn. Hence, the agent n will exert

eﬀort qn =

β(θ†)/θn − 1/σ2

and

report

yˆn =

µ0 /σ02 +yn ·qn 2

to

maximize

his

expected

payoﬀ.

Besides,

0

1/σ0 +qn

if the expected payoﬀ of the agent n is negative, he will not participate this prediction task.

Also recall that the principal knows the prior information of x∗ ∼ N (µ0, σ02). Hence, the principal can always achieve a payoﬀ of −1/σ02 by not making any payments, and simply choosing the prior

mean has her prediction. Hence, the principal does not pay anything and simply sets xˆ = µ0 if her

expected payoﬀ is smaller than −1/σ02. Quadratic Cost Function: Under the quadratic cost function, q† is the solution of the following

equation: The functions α(·) and β(·) are :

1

− θ† · q† = 0

1/σ02 + N · q† 2

α(θ†) = 1/σ2 + q† · θ†q† + 1 θ† q† 2, β(θ†) = 1/σ2 + q† 2 · θ†q†.

0

2

0

Recall that the actually cost parameter of the agent n ∈ A is θn. Hence, the agent n will exert eﬀort qn to maximize his own expected payoﬀ, where qn is the solution of
β 2 − θnqn = 0.
1/σ02 + qn

Parametric Prediction from Parametric Agents
23 Besides, if the expected payoﬀ of the agent n is negative, he will not participate this prediction task. Also, the principal does not pay anything and simply sets xˆ = µ0 if her expected payoﬀ is smaller than −1/σ02.
5.3. Numerical Results In the simulations, we draw x∗ ∼ N (0, 1), and set θ = 0 and θ¯ = 1. We vary the number of agents from N = 3 to N = 19. Each point in the plots is an average across 50000 trials. Without loss of generality, we have normalized the principal’s payoﬀ (see (4)) so that it equals zero in the ideal (unachievable) case of zero prediction error and a zero payment. Note that the principal can always achieve a payoﬀ of −1 by not making any payments, and simply choosing the prior mean has her prediction.
Figure 2 depicts the principal’s expected payoﬀ achieved under COPE and under the homogeneous mechanism for diﬀerent values of θ† when the cost function is linear and quadratic, respectively. We use the red line with circle markers to denote COPE, the blue dash line with square markers to denote homogeneous mechanism with θ† = 0.2, the dark dash line with diamond markers to denote homogeneous mechanism with θ† = 0.5, and the magenta line with right-pointing triangle markers to denote homogeneous mechanism with θ† = 0.8.
Figure 2 shows that COPE can improve the principal’s payoﬀ by exploring the heterogeneity of users. The improvement is at least 10% under the linear cost function and 5% under the quadratic cost function.
By comparing Figure 2a to Figure 2b, we can see that the value of the belief of principal (i.e., θ†) under the homogeneous mechanism will aﬀect the ﬁnal result. Under the linear cost function, a lower value of θ† results in a better performance in terms of the principal’s payoﬀ. However, under the quadratic cost function, a higher value of θ† results in a better performance. The reasons are as follows.
Under the homogeneous mechanism, the value of θ† will determine the number and types of agents joining the task. Having a higher value of θ† would incentivize more agents to participate.

Parametric Prediction from Parametric Agents
24

0

0

COPE

Homogeneous 3y = 0:2

Homogeneous 3y = 0:5

-0.4

Homogeneous 3y = 0:8

-0.4

Expected payoff of the principal Expected payoff of the principal

-0.8

-0.8

-1.2

-1.2

-1.6
-2 3
Figure 2

COPE

-1.6

Homogeneous 3y = 0:2

Homogeneous 3y = 0:5

Homogeneous 3y = 0:8

4

5

6

7

8

9 10 11 12 13

Number of agents

-2

3

4

5

6

7

8

9 10 11 12 13

Number of agents

(a) Linear cost function

(b) Quadratic cost function

The principal’s expected payoﬀ under COPE and the homogeneous mechanism.

This is because, for an agent n ∈ A whose cost parameter θn < θ†, he can put less eﬀort to achieve the same performance as the agent with cost parameter θ† can.
Under the linear cost function, similar as COPE, ﬁnding the most capable one would be optimal for the principal, as the marginal cost is nonnegative even the agent does not put any eﬀort. Hence, having a lower value of θ† would eliminate more agents, and have a higher chance to ﬁnd the agent with θn ≤ θ†.
On the contrary, under the quadratic cost function, it would be optimal to recruit as many agents as possible to improve the prediction accuracy. The beneﬁt brought by the accuracy improvement would be higher than the additional payment to agents. Hence, having a higher value of θ† would help the principal recruit more agents.
Figure 3 depicts the network proﬁt achieved under COPE and under the homogeneous mechanism for diﬀerent values of θ† when the cost function is linear and quadratic, respectively. We use the dash brown line to denote the benchmark solution where the principal and all agents acted as an integrated player.
From Figure 3, we have the following observations. • COPE can achieve a network proﬁt close to the integrated benchmark solution, e.g., the gap is less than 10% under the linear cost function and 3% under the quadratic cost function.

Parametric Prediction from Parametric Agents
25

0

0

Benchmark

COPE

-0.2

Homogeneous 3y = 0:2

-0.2

Homogeneous 3y = 0:5

Homogeneous 3y = 0:8

-0.4

-0.4

Expected Network Profit Expected Network Profit

-0.6

-0.6

-0.8

-0.8

Benchmark

-1

-1

COPE

Homogeneous 3y = 0:2

Homogeneous 3y = 0:5

-1.2

-1.2

Homogeneous 3y = 0:8

3

4

5

6

7

8

9 10 11 12 13

Number of agents

(a) Linear cost function

3

4

5

6

7

8

9 10 11 12 13

Number of agents

(b) Quadratic cost function

Figure 3 The network proﬁt under COPE and the homogeneousmechanism.

• Network proﬁt achieved under COPE increases with the number of agents. This is because the increasing number of agents allows the principal to have a higher chance to incentivize agents with high capability to improve the prediction accuracy. This is true under both costs, even though only one agent will be recruited under the linear cost.
• COPE leads to a much higher network proﬁt, compared to the homogeneous mechanism. The reason is that COPE explores the heterogeneity of agents by eliciting their cost type information.
Recall that the principal makes the prediction based on (32) in the homogeneous mechanism. As the actually eﬀort exerted by the agents are diﬀerent from that desired by the principal (i.e., qn = q†), the prediction made by the principal is inaccurate. On the contrary, COPE elicits the cost types of agents as well as incentivizes each agent to exert the appropriate level of eﬀort, which results in a better performance than the homogeneous mechanism. Due to the joint eﬀect of the number of agents and the value of θ†, the performance of homogeneous mechanism is close to that of COPE (e.g., N < 4 under the linear cost function and N < 6 under the quadratic cost function) in terms of expected network proﬁt. However, it is diﬃcult to determine the proper choice of θ† in terms of the number of agents. Finding the optimal value of θ† given the number of agents will be an interesting future work.

Parametric Prediction from Parametric Agents
26
6. Conclusions
We study the parametric prediction market under information asymmetry. To elicit the truthful information of participating agents and exploit the heterogeneity in the agents, we propose mechanism COPE, which ensures agents to exert eﬀort desired by the principal and report their true observation. Our analysis indicate that, under the Gaussian estimation noise scenario, when the costs incurred by the agents are linear in the amount of exerted eﬀort, the principal should require service from only one agent with the lowest reported cost. On the other hand, when the costs are quadratic in the exerted eﬀort, it is optimal for the principal to recruit multiple agents to complete the task. We also present the general form of COPE that is optimal for a wide variety of settings (e.g., general cost function and the noise distribution).
In this work, we have focused on the parametric setting, where the principal recruits agents to estimate a parameter (e.g., the winner of a election) that the realized value can be observed in the future. As in some cases, such as rating the quality of a book, the true outcome cannot be easily observed or veriﬁed. In the future, we will consider how to incentivize agents’ behaviour when collecting subjective data. Moreover, in order to give theoretical insights into the problem of estimation from strategic agents, we use one parameter, i.e., the cost type θ to characterize the heterogeneity of the agents in terms of their cost. Relaxing the parametric assumption (e.g., characterizing agents’ types with random functions) and designing mechanisms with theoretical guarantees is extremely challenging. In practice, heuristics (e.g., Brousseau and Glachant (2002), Chiappori and Salani´e (1997)) are employed to circumvent the parametric assumption when using these mechanisms. In the future, we would consider how to relax such parametric assumption.
Appendix. Full Proofs
A. Proof of Theorem 1: Linear Costs The proof will proceed in four steps. The ﬁrst three steps show that our mechanism incentivizes the agents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that

Parametric Prediction from Parametric Agents
27

irrespective of what an agent reports as his cost parameter, and irrespective of the eﬀort he exerts,

the agent is always incentivized to report his true observation. We follow this up and show that

irrespective of the eﬀort that an agent exerts, he is always incentivized to report his cost parameter

correctly. The third step completes the proof of truthfulness, showing that under truthful reporting

of the cost parameter and the observation, in our mechanism, an agent is always incentivized to

exert precisely the eﬀort as desired by the principal. Finally, we show that among all mechanisms

that ensure truthful reports, our mechanism maximizes the principal’s expected utility.

We assume that the random variables {θn}n∈A are independently and identically distributed on support [θ, θ¯], with a cumulative distribution function F : [θ, θ¯] → R+ and a probability density function f : [θ, θ¯] → R+. We further assume that the c.d.f. function F is continues, diﬀerentiable, and log concave in [θ, θ¯]. This assumption is satisﬁed by a wide range of distributions, such as

uniform, gamma, and beta distributions.

Step 1. Truthful reporting of observation under COPE

We will analyze the strategies of the agent who is recruited by the principal and the agents who

are not recruited by the principal, respectively.

We ﬁrst study the observation reporting strategy of the agent n0 who is recruited and rewarded by the principal, where n0 = arg minm∈Aθˆm.

We will show that the agent n0 will choose

yˆ = µ0 · 1/σ02 + yn0 · qn0

n0

1/σ02 + qn0

(34)

to maximize his expected payoﬀ given his exerting eﬀort qn0 and own observation yn0. As shown in (9), π(θˆn0), K(θˆn0) and S(θˆn0) are independent of yˆn0. Moreover, the value of
calculated by K(θˆn0) is always positive. Hence, when the agent n0 makes reporting observation strategy to maximize his expected payoﬀ, i.e.,

yˆn0 ∈ arg maxE π(θˆn0 ) − K(θˆn0 ) · (x∗ − yˆn0 )2 + S(θˆn0 ) − C qn0 , θn0 ,

where the expectation is taken with respect to x∗ and cost parameters θ−n0 =

Parametric Prediction from Parametric Agents
28 [θ1, . . . , θn0−1, θn0+1, . . . , θN ]T except agent n0, it is equivalent for the agent n0 to choose reporting strategy such that

yˆn0 ∈ arg minEx∗ [(x∗ − yˆn0 )2]. (35)

Based on theory of Bayesian estimation (see Myerson (1979), Lehmann and Casella (1998)), only when yˆn0 = µ0·11//σσ0202++yqnn00·qn0 , the value of Ex∗ [(x∗ − yˆn0 )2] is minimized and the expected value is

Ex∗ [(x∗ − yˆn )2] =

1 .

0

1/σ02 + qn0

We then study the observation reporting strategy of other agents who are not recruited and

rewarded by the principal. For agent n ∈ A, n = n0, he will put zero eﬀort as he does not receive any reward from the principal. In this case, only when reporting his observation yˆn = µ0 can minimize Ex∗[(x∗ − yˆn)2]. The expected value of Ex∗[(x∗ − yˆn)2] is

Ex∗ [(x∗ − yˆn)2] =

1 , n ∈ A, n = n0.

1/σ02

Step 2. Truthful reporting of the cost parameter under COPE

We ﬁrst show that the agent n0 will truthfully reveals his cost type. We ﬁrst rewrite functions

π, K, and S as follows.

π(θˆn0 , θ−n0 ) =θˆn0 · Qp(θˆn0 , θ−n0 ) +

θ¯
Qp(z, θ−n0 )dz,
θˆn0

K(θˆn0 , θ−n0 ) = Qp(θˆn0 , θ−n0 ) + 1/σ02 2 · θˆn0 ,

S(θˆn0 , θ−n0 ) =

Qp

(

θˆn

0

,

θ

−

n

0

)

+

1/σ

2 0

· θˆn0 .

The expected payoﬀ of the agent who has a cost type θn0 but reports θˆn0 is:

E{x∗,yn0 ,θ−n0 } U a(x∗, θˆn0 , qn0 , yn0 , θn0 , θ−n0 )

= Eθ π(θˆn , θ−n ) − K(θˆn , θ−n ) ·

1

+ S(θˆn , θ−n ) − qn θn .

−n0

0

0

0

0 1/σ02 + qn0

0

0

00

(36)

For notation convenience, we deﬁne the function U ae : R × [θ, θ¯] × R+ × [θ, θ¯]N → R+ as

U ae(θˆ , q , θ , θ ) = π(θˆ , θ ) − K(θˆ , θ ) 1 + S(θˆ , θ ) − q θ ,

n0 n0 n0 −n0

n0 −n0

n0 −n0 1/σ02 + qn0

n0 −n0

n0 n0

(37)

Parametric Prediction from Parametric Agents
29 where θ−n0 are the random variables of all agents’ cost type except that of agent n0. By comparing (36) to (37), the expected payoﬀ of the agent n is

E{x∗,yn0 ,θ−n0 } U a(x∗, θˆn0 , qn0 , yn0 , θn0 , θ−n0 ) = Eθ−n0 U ae(θˆn0 , qn0 , θn0 , θ−n0 ) .

By the mean value theorem, we have:

E U ae(θn0 , qn0 , θn0 , θ−n0 ) − E U ae(θˆn0 , qn0 , θn0 , θ−n0 ) = E

∂U ae(η, qn0 , θn0 , θ−n0 ) ∂η

· (θn0 − θˆn0 ), (38)

where the expectation is taken with respect to θ−n0, and η lies between θn0 and θˆn0.

We further have:

∂U ae(η, qn0 , θn0 , θ−n0 )

Eθ−n0

∂η

∂ = Eθ−n0 ∂η ηQp(η, θ−n0 ) +

θ¯
Qp(z, θ−n0 )dz −
η

Qp(η, θ−n0 ) + 1/σ02 2 η 1/σ02 + qn0

+ Qp(η, θ−n0 ) + 1/σ02 η − qn0 θn0

∂Qp(η, θ−n )

Qp(η, θ−n

)

+

1/σ

2 0

2

= Eθ−n0 2η

0− ∂η

0
1/σ02 + qn0

− 2 [Qp(η, θ−n0 ) + 1/σ02] · ∂Qp(η, θ−n0 ) · η + Qp(η, θ ) + 1/σ2

1/σ02 + qn0

∂η

−n0

0

=E

1 − Qp(η, θ−n0 ) + 1/σ02 · 2η · ∂Qp(η, θ−n0 ) + Qp(η, θ ) + 1/σ2 .

θ−n0

qn0 + 1/σ02

∂η

−n0

0

If we have

(39)

− ∂Qp(η, θ−n0 )/ Qp(η, θ−n0 ) + 1/σ02 ≥ 1 ,

∂θn0 /θn0

2

(40)

then we have

2η · ∂Qp(η∂,ηθ−n0 ) + Qp(η, θ−n0 ) + 1/σ02 ≤ 0.

Lemma 1. If θn ∼ Uniform[θ, θ¯] which is independent for every n ∈ A, then (40) is satisﬁed.

Proof : First consider the case N = 1. Since there is only one agent, hence the principal can only select that agent. So Qp is simply Q of that agent:

Q(θ) = 1/ θ + F (θ)/f (θ) − 1/σ02,

Parametric Prediction from Parametric Agents
30

hence

∂Q(θ)

θ

1 + ∂∂θ Ff ((θθ))

− ∂θ Q(θ) + 1/σ02 = 2[θ + F (θ)/f (θ)]

1 θ + F (θ)/f (θ) 1/

1

1

+

∂ ∂θ

=

2 1+ 1

θ

F (θ) f (θ)
F (θ) f (θ)

1 ≥,
2

θ θ + F (θ)/f (θ)

where the ﬁnal inequality holds for uniform distribution.

We now consider N > 1. Observe that the calculation above will be violated only when the cost parameter of some other agent is inﬁnitesimally close to θn0 (since in that case, ∂Qp(θ∂nθ0n,0θ−n0) is diﬀerent from that calculated above). However given our assumptions that the distribution of θ

follows some known distribution such as uniform and normal distributions, and given that the

number of agents N is ﬁnite, θn0 will be well separated from the cost types of all other agents with

probability 1.

As the agent is selﬁsh, he will exert eﬀort qn0 to maximize his expected payoﬀ. Hence, the agent’s

exerted eﬀort can be obtained by taking the ﬁrst order derivative of (36) with respect to qn0 and

setting it to zero, which is

(1/σ02 + qnp0 )2 · θˆn0 = (1/σ02 + qn0 )2 · θn0 ,

(41)

where qnp0 is the shorthand notation for Qp(θˆn0, θ−n0). Based on (41), we have (i) if θˆn0 > θn0 , qnp0 < qn0 , (ii) if θˆn0 < θn0 , qnp0 > qn0 , and (iii) if θˆn0 = θn0 ,
qnp0 = qn0 . Hence, If θˆn0 > θn0, the equation (39) is negative and (38) is positive. This inequality also holds for
θˆn0 < θn0, by a similar argument. Therefore, agent n0 will truthfully report his own cost parameter. We then show that an agent n ∈ A, n = n0 will truthfully reveal his cost type. Recall that the
principal does not recruit and reward the agent n ∈ A, n = n0. Hence, the payment to the agent n ∈ A, n = n0 is zero. The we have

Eθ−n U ae(θn, qn, θn, θ−n) − Eθ−n U ae(θˆn, qn, θn, θ−n) = 0, ∀n ∈ A, n = n0,

Parametric Prediction from Parametric Agents
31

which shows that there is no diﬀerence between truthfully reporting cost type or not in terms of expected payoﬀ for the agent n. Without loss of generality, we assume that in this case, the agent will truthfully report their cost types.
Step 3. Incentivize agent to exert precisely the eﬀort as desired by the principal under COPE
As we have proved in Step 2 that the agent n0 would truthfully report his cost type (θˆn = θn), then we will show that the agent n0 exerts an eﬀort level such that qn0 = qnp0 would maximize his expected payoﬀ, which is given as

E

U

ae

(

θ

n

0

,

q

p n

0

,

θ

n

0

,

θ

−

n

0

)

1 = π(θn0 , θ−n0 ) − K(θn0 , θ−n0 ) 1/σ02 + qn0 + S(θn0 , θ−n0 ) − qn0 θn0 ,
(42)

where the expectation is taken with respect to θ−n0. It can be veriﬁed that (42) is concave in qn0. Hence, by taking the ﬁrst order derivative of (42)
with respect to qn0, we have

∂ ∂qn0 E U ae(θn0 , qnp0 , θn0 , θ−n0 ) =

1/σ02 + qnp0 1/σ02 + qn0

2
· θn0 − θn0 .

(43)

We can verify that the value of (43) equals to zero only when qn0 = qnp0. Hence, agent n0 will exert the eﬀort as the principal desires to maximize his expected payoﬀ. Then (34) is rewritten as

µ0 · 1/σ02 + yn0 · qnp

yˆn0 = 1/σ2 + qp 0 .

0

n0

(44)

Because the principal knows the value of µ0, σ02, and qnp0, she can infer the agent n0’s truth observation yn0 from (44).
Step 4. Maximize the principal’s expected utility under COPE

Then we look at the expected payoﬀ of the principal. The following lemma describes that COPE

is the optimal mechanism that maximizes the principal’s expected utility.

Lemma 2. The optimal predictor xˆ = µ0·1/σ02+ n∈A yn·qnp deﬁned in COPE maximizes the principal’s 1/σ02+ n∈A qnp

expected utility, and the Bayes risk of the principal’s prediction is Bp qp =

1 2

.

1/σ0 + n∈A qnp

Parametric Prediction from Parametric Agents
32

Proof : Recall that qnp = Qp(θn, θ−n). Given all agents’ observation y and agents’ exert eﬀort qp, the principal’s updated belief on the realization of x∗ can be expressed as

x∗|(y, qp) ∼ N

µ0 · 1/σ02 + 1/σ02 +

n∈A yn · qnp ,

1

.

n∈A qnp

1/σ02 + n∈A qnp

To maximize the expected utility for the prediction, the principal solves

max E v − (x∗ − xˆ)2|(y, qp) xˆ

= max v − E x∗2|(y, qp) − 2xˆE x∗|(y, qp) + xˆ2 xˆ

= max v − xˆ − µ0 · 1/σ02 +

xˆ

1/σ02 +

1 ≤v−
1/σ02 + n∈A qnp

n∈A yn · qnp n∈A qnp

2

1

− 1/σ02 +

n∈A qnp

The equality holds only when

xˆ = µ0 · 1/σ02 + 1/σ02 +

n∈A yn · qnp . n∈A qnp

(45)

Hence, the optimal predictor that maximizes the principal’s expected utility is

xˆ y, qp = µ0 · 1/σ02 + 1/σ02 +

n∈A yn · qnp n∈A qnp

(46)

and the Bayes risk is

Bp qp = inf E[(x∗ − xˆ)2] =

1 ,

xˆ

1/σ02 + n∈A qnp

where the expectation is taken with respect to x∗ and y.

Recall that under the linear cost function, the principal only recruits agent n0 to exert eﬀort, in

such case, qnp = 0, ∀n ∈ A, n = n0. Also recall that the principal can infer the true observation of

the agent n0 through the function g : R → R, and such an observation is deﬁned as yn0 = g(yˆn0) = yˆn0 + (yˆn0 − µ0)/(qnp0σ02). Then putting back to (46) we can get the conclusion.
We then show that the desired eﬀort level Qp(θn, θ−n) deﬁned in (8) and the function π(θn, θ−n)

deﬁned in (9) can maximize the principal’s expected payoﬀ and satisfy BIC and BIR conditions.

Parametric Prediction from Parametric Agents
33

Notice that as the agent n exerts eﬀort such that q = qp and reports yˆ = µ0·1/σ02+yn0 ·qnp 0 , the

0

n0

n0

n0

1/σ02+qnp 0

expected payment function is reduced to

E{x∗,yn0 ,θ−n0 } R(x∗, yn0 , qn0 , θn0 , θ−n0 ) 1
= Eθ−n0 π(θn0 , θ−n0 ) − K(θn0 , θ−n0 ) · 1/σ02 + qn0 + S(θn0 , θ−n0 ) = Eθ−n0 π(θn0 , θ−n0 ) .

(47)

For other agent n ∈ A, n = n0, as the principal does not require him to do the observation, we ﬁrst assume that the expected payment to him is as follows,

E{x∗,yn0 ,θ−n0 }[R(x∗, yn, θn, θ−n)] = Eθ−n0 π(θn, θ−n) , ∀n ∈ A, n = n0.

(48)

Later we will show that π(θn, θ−n) = 0, ∀n = n0. The expected payoﬀ of agent n ∈ A is

Eθ−n U ae θˆn0 , qnp0 , θn0 , θ−n0 =Eθ−n π(θˆn, θ−n) − θnQp(θˆn, θ−n) ,
where qnp0 is the shorthand notation of Qp(θˆn, θ−n). For notation convenience, we adopt U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn in the later proof of Theorem 1, where the function U ae is rewritten as

Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn = Eθ−n π(θˆn, θ−n) − θnQp(θˆn, θ−n) .

(49)

Correspondingly, BIC and BIR conditions, i.e., (5) and (6), can be rewritten as

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn , ∀θˆn = θn, (50) Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn ∈ [θ, θ¯]. (51)

Base on Lemma 2, (47), and (48), the expected payoﬀ of the principal is

Ex∗,y,θ[U p(x∗, qp, y, θˆ)] = −Bp qp − Ex∗,y,θ

R(x∗, yn, θn, θ−n)

n∈A

1

= − 1/σ2 +

qp − Eθ

π(θn, θ−n) .

0

n∈A n

n∈A

Parametric Prediction from Parametric Agents
34

Recall that qnp = Qp(θn, θ−n), the principal’s optimal problem deﬁned in (7) can be rewritten as

sup

E[U p(x∗, qp, y, θˆ)],

{Qp (θ),π (θ)},∀θn ∈θ,∀n∈A

subject to : BIC and BIR in (50) and (51).

(52)

In the following lemmas, we characterize an equivalent formulation for the feasible region deﬁned

by BIC and BIR. Using these lemmas, we show that Qp(θn, θ−n) deﬁned in (8) and π(θn, θ−n)

deﬁned in (9) are the optimal solution that solves the principal’s problem in (52).

Lemma 3. The solution of (52) is feasible if and only if it satisﬁes the following conditions for all θn ∈ [θ, θ¯], ∀n ∈ A:

• the expected payoﬀ of agent n is

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn

= Eθ−n

θ¯
Qp(x, θ−n)dx ;
θn

(53)

• Qp(θn, θ−n) is non-increasing in θn.

Proof : The proof of Lemma 3 is as follows. We ﬁrst show that BIC and BIR imply the condition

in (53).

Notice that the ﬁrst derivative of (49) is:

∂Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn ∂θn

= Eθ−n − Qp(θˆn, θ−n) ≤ 0.

(54)

Then, for any θn1 > θn2 , we have

Eθ−n

U

ae

(π

(

θ

1 n

,

θ

−

n

),

Q

p

(

θ

1 n

,

θ

−n

)

,

θ

1 n

)

≤ Eθ−n

U

ae

(

π

(

θn1

,

θ

−

n

)

,

Q

p

(θ

1 n

,

θ

−

n

)

,

θ

2 n

)

≤ Eθ−n

U

ae

(

π

(

θn2

,

θ

−

n

)

,

Q

p

(θ

2 n

,

θ

−

n

)

,

θ

2 n

)

;

(55)

where the ﬁrst inequality is because (54) and the second is from the BIC condition deﬁned in (50).

In other words, for the agent n ∈ A whose cost parameter θ ≤ θn ≤ θ¯, we have

Eθ−n U ae(π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯) ≤ Eθ−n U ae(π(θn, θ−n), Qp(θn, θ−n), θn) ≤ Eθ−n U ae(π(θ, θ−n), Qp(θ, θ−n), θ) .

(56)

Recall that the BIR condition is

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn ∈ [θ, θ¯], (57)

Parametric Prediction from Parametric Agents
35 which implies that, for the agent n ∈ A with any value θn ∈ [θ, θ¯], his expected payoﬀ should at least be zero. Then the expected payoﬀ of the agent n with cost parameter θ¯ must be binding at zero. Otherwise, the principal can reduce the π(θ¯, θ−n) by a small value of δ > 0, which does not violate the constraint of (57) but raises the principal’s expected payoﬀ. Hence, we have
Eθ−n U ae(π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯) = 0. (58)

Let U ae(θn, θ−n) = U ae π(θn, θ−n), Qp(θn, θ−n), θn . From the BIC condition, we have

Eθ U ae(θn, θ−n) = max Eθ U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn .

−n

θˆn

−n

By using the envelope theorem, we have:

∂Eθ−n U ae(θn, θ−n) ∂θn

= ∂Eθ−n U ae(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) ∂θn

= Eθ−n
θˆn =θn

− Qp(θn, θ−n) , (59)

where θn is a parameter. By integrating both sides from the value of θn to θ¯ and using (58) and

the assumption that the random variable θn of the agent n is independent for every n ∈ A , we get

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn = Eθ−n

θ¯
Qp(x, θ−n)dx .
θn

(60)

We prove that Qp(θn, θn) is nonincreasing in θn by contradiction. Let pn be the shorthand

notation

for

π(θn, θ−n).

Suppose

for

any

θn1

>

θn2 ,

we

have

Qp(θn1 , θ−n)

>

Qp

(

θ

2 n

,

θ

−n

).

Because

we have

∂2U ae pn, qnp , θn = −1 < 0, ∂qnp ∂θn

∂2U ae

pn

,

q

p n

,

θ

n

∂qnp 2

= 0,

(61) (62)

0 = ∂U ae pn, qnp , θn1 ∂qnp
= ∂U ae pn, qnp , θn1 ∂qnp

qnp =Qp(θn1 ,θ−n) qnp =Qp(θn2 ,θ−n)

Parametric Prediction from Parametric Agents
36

< ∂U ae pn, qnp , θn2 ∂qnp

,
qnp =Qp(θn2 ,θ−n)

(63)

where the ﬁrst equality is due to BIC when the agent n’s cost parameter θn has the value of θn1 ,

the second equality is due to (62), and the inequality is due to (61).

However, based on the BIC condition, if the agent n’s cost parameter θn has the value of θn2 ,

then we should have

∂U ae

p

n

,

q

p n

,

θ

2 n

∂qnp

= 0,
qnp =Qp(θn2 ,θ−n)

which

holds

true

for

all

scalar

values

of

pn.

Hence,

for

any

θn1

>

θn2 ,

Qp(θn1 , θ−n)

≤

Qp

(θ

2 n

,

θ

−

n

).

Then we need to prove that (53) implies BIC and BIR deﬁned in (50) and (51).

BIR is veriﬁed by putting θn back to (53). Besides, by putting θn = θ¯ back to (53), we have

Eθ−n U ae π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯ = 0.

Then we prove that (53) implies BIC. Notice that we have:

Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn

1

θ¯ ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z

= Eθ−n −
θn

dz ∂z

=2 E

U ae π(θˆ , θ

), Qp(θˆ , θ

), θˆ

−

θˆn ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z dz

θ−n n −n n −n n θn ∂z

=3 Eθ−n

θ¯

θˆn ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z

Qp(η, θ−n)dη −

dz

θˆn

θn

∂z

θn

θˆn

θˆn

=4 Eθ−n − Qp(η, θ−n)dη − Qp(η, θ−n)dη + Qp(θˆn, θ−n)dz

θ¯

θn

θn

=5 Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn +

θˆ
Qp(θˆn, θ−n) − Qp(η, θ−n) dη ,

θn

where the third equality and the ﬁfth equality are obtained by (53).

If θˆn > θn, then the above equation is non-positive (because Qp(η, θ−n) is non-increasing in η),

hence

Eθ−n U ae(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) < Eθ−n U ae(π(θn, θ−n), Qp(θn, θ−n), θn) .

Parametric Prediction from Parametric Agents
37 This inequality also holds for θˆn < θn by a similar argument. Therefore, the two condition imply BIC.
Then based on Lemma 3, we have the following Lemma.

Lemma 4. The optimisation problem in (52) has the following equivalent formulation:

{Qp(mθ)}a,∀xθn∈θEθ − 1/σ02 +

1 −
Qp(θn, θ−n)

Qp(θn, θ−n) · θn −

Qp(θ , θ ) · F (θn) , n −n f (θn)

n∈A

n∈A

n∈A

s.t. Qp(θn, θ−n) is nonincreasing in θn,

(64)

where the expectation is taken with respect to θ.

Proof : The proof of Lemma 4 is as follows. The expected payoﬀ of the principal can be written

as:

Eθ − 1/σ02 +

1 − Qp(θn, θ−n) · θn − U a

n∈A Qp(θn, θ−n) n∈A

n∈A

= Eθ − 1/σ02 +

1 − Qp(θn, θ−n) · θn −

n∈A Qp(θn, θ−n) n∈A

n∈A

π(θn, θ−n), Qp(θn, θ−n), θn
θ¯
Qp(x, θ−n)dx ,
θn

(65)

where the expectation is taken with respect to θ. Notice that

θ¯

θ¯ θ¯

θ¯

Eθn

Qp(x, θ−n)dx =

Qp(x, θ−n)dx · f (z)dz = F (z)Qp(z, θ−n)dz

θn

θz

θ

θ¯ F (z)

F (θn)

=

Qp(z, θ−n)f (z)dz = Eθn

Qp(θn, θ−n) ,

θ f (z)

f (θn)

where the ﬁrst equation is obtained by using integration by parts. Then by applying the above equa-

tion to (126) and the fact that {θn}n∈A are assumed to be random, independently and identically distributed on support [θ, θ¯], we can get the conclusion.

Based on Lemma 4, the principal’s problem reduces to choosing the desired eﬀort qnp =

Qp(θn, θ−n) for each agent n ∈ A. We ﬁrst consider the problem in (64) without the constraint. If

the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution

to the constrained problem.

Lemma 5. Qp(θn, θ−n) deﬁned in (8) and π(θn, θ−n) deﬁned in (9) are the optimal solution that solves the principal’s problem in (52)

38 Proof : We ﬁrst prove that for the agent ∀n ∈ A,

Parametric Prediction from Parametric Agents

Qp(θn, θ−n) = 0m,ax{1/ γ(θn) − 1/σ02, 0},

if n0 = arg minm∈Aθm, otherwise,

is the optimal solution of (64) by contradiction. As qnp = Qp(θn, θ−n), the problem in (64) is equivalent to

1

min qp≥0 1/σ2 +

+ qp

qnp · γ(θn),

0

n∈A n n∈A

s.t. qnp is nonincreasing in θn,

(66)

where γ(θn) = θn + F (θn)/f (θn). Without loss of generality, let γ(θ1) ≤ γ(θ2) . . . ≤ γ(θN ). If the principal’s desired eﬀorts from all
agents are positive, then the solution is

q1p

=

q1p

,∗

,

q

p 2

=

q2p

,∗

,

.

.

.

,

q

p N

=

qNp,∗.

(67)

Suppose that there is another solution such that

 q1p,† = q1p + qjp, 

qip,† = qip,  qjp,† = 0.

∀i = 1, j, i ∈ A,

(68)

Then we can verify that

qnp,† = qnp and

qnp,† · γ(θn) ≤

qnp · γ(θn) .

n∈A

n∈A

n∈A

n∈A

Hence, (67) is not an optimal solution. Then we let qnp = 0, ∀n > 1, the problem in (66) becomes

1

min qp

1/σ2 + qp + q1p · γ(θ1),

1

0

1

s.t. q1p ≥ 0.

(69)

By solving the above problem we can get that q1p = max{1/ γ(θ1) − 1/σ02, 0}. As we deﬁne n0 =

arg minm∈Aθm and the assumption that F is log-concave in [θ, θ¯], we have qnp0 = max{1/ γ(θn0) −

1/σ

2 0

,

0}

.

Parametric Prediction from Parametric Agents
39

According to (53), we have

Eθ−n π(θn, θ−n) − Qp(θn, θ−n) · θn = Eθ−n

θ¯
Qp(x, θ−n)dx .
θn

Then the optimal payment function given the agent n0 and 1/ γ(θn0) − 1/σ02 ≥ 0 is

π(θn0 ) = θn0 /

θ¯
γ(θn0 ) − θn0 /σ02 +
θn0

1

1

γ(z) − σ02

dz = θn0 /

θ¯
γ(θn0 ) − θ¯/σ02 +
θn0

1 dz,
γ(z)

and the payment will be zero if 1/ γ(θn0) − 1/σ02 < 0. For other agents, the payments will be zero as they are not involved in the observation and
prediction. As in Theorem 1 , we assume that θn ∼ Uniform[θ, θ¯], which is independent for every n ∈ A.
Putting the expression of F and f back to the above equations, we can have the conclusion.

B. Proof of Theorem 2: Quadratic Costs

The proof is similar to that in Section A. The diﬀerence is as follows.

First, the function π : [θ, θ¯]N → R+, K, S : [θ, θ¯]N × R+ → R+ are deﬁned as

π(θˆ , θ

1 )= ·

θˆ

·

Qp(θˆ , θ

) 2+

θ¯

n −n 2 n

n −n θˆ

n

Qp(z, θ−n) 2

K(θˆn, θ−n) =

Qp

(

θˆn

,

θ

−n

)

+

1/σ

2 0

2θˆn · Qp(θˆn, θ−n),

dz ,

(70) (71)

S(θˆn, θ−n) =

Qp

(

θˆn

,

θ

−n

)

+

1/σ

2 0

θˆn · Qp(θˆn, θ−n).

(72)

Step 1. Truthful reporting of observation under COPE We will analyze the strategies of the agent n, ∀n ∈ A. We will show that the agent n will choose
µ0 · 1/σ02 + yn · qn yˆn = 1/σ02 + qn (73) to maximize his expected payoﬀ given his exerting eﬀort qn and own observation yn. As π(θˆn, θ−n), K(θˆn, θ−n) and S(θˆn, θ−n) are independent of yˆn and the value of calculated by K(θˆn, θ−n) is always positive. Hence, when the agent n makes reporting observation strategy to maximize his expected payoﬀ, i.e.,

yˆn ∈ arg maxE π(θˆn, θ−n) − K(θˆn, θ−n)(x∗ − yˆn)2 + S(θˆn, θ−n) − C qn, θn ,

Parametric Prediction from Parametric Agents
40 where the expectation is taken with respect to x∗ and cost parameters θ−n = [θ1, . . . , θn−1, θn+1, . . . , θN ]T except agent n, it is equivalent for the agent n to choose reporting strategy such that

yˆn ∈ arg minEx∗ [(x∗ − yˆn)2].

(74)

The

value

of

Ex∗ [(x∗

− yˆn)2]

is

minimized

when

yˆn

=

. µ0 ·1/σ02 +yn ·qn 2

The

expected

value

in

this

1/σ0 +qn

case is

Ex∗ [(x∗ − yˆn)2] =

1 .

1/σ02 + qn

Step 2. Truthful reporting of cost parameter under COPE

We will show that the agent n, ∀n ∈ A will truthfully reveals his cost type. The expected payoﬀ

of the agent who has a cost type is θn but reports θˆn is:

E{x∗,yn,θ−n} U a(x∗, θˆn, qn, yn, θn, θ−n)

= Eθ π(θˆn, θ−n) − K(θˆn, θ−n) ·

1

+ S(θˆ , θ

1 )− θ

q2

.

n −n

n

−n

1/σ02 + qn

2n

For notation convenience, we deﬁne

(75)

U a(θˆ , q , θ , θ

) = π(θˆ , θ

) − K(θˆn, θ−n) + S(θˆ , θ

1 )− θ

q2

n n n −n

n −n 1/σ02 + qn

n −n 2 n n

By the mean value theorem, we have:

E U a(θn, qn, θn, θ−n) − E U a(θˆn, qn, θn, θ−n) = Eθ

∂U a(η, qn, θn, θ−n) (θ − θˆ ),

n

n

−n

∂η

(76)

where the expectation is taken with respect to θ−n, and η lies between θn and θˆn.

We further have

∂U a(η, qn, θn, θ−n)

Eθ−n

∂η

∂1

θ¯

2

2

= Eθ−n0 ∂η 2 · η Qp(η, θ−n) + Qp(z, θ−n) dz

η

2

Qp(η, θ−n) + 1/σ02 2

12

+ Qp(η, θ−n0 ) + 1/σ0 η Qp(η, θ−n) −

1/σ02 + qn

η Qp(η, θ−n) − 2 θnqn

Parametric Prediction from Parametric Agents
41

= Eθ−n

1 − Qp(η, θ−n) + 1/σ02 qn + 1/σ02

2Qp(η, θ−n) · η · ∂Qp(∂ηη, θ−n) + Qp(η, θ−n) + 1/σ02 · Qp(η, θ−n)

+ Qp(η, θ−n) + 1/σ02 · η · ∂∂η Qp(η, θ−n) − 12 Qp(η, θ−n) 2 . (77)

We can check that if

− ∂Qp(η, θ−n)/(Qp(η, θ−n) + 1/σ02) ≥ 1 , ∀n ∈ A, (78)

∂θn/θn

2

we have

2Qp(η, θ−n) · η · ∂Qp(∂ηη, θ−n) + Qp(η, θ−n) + 1/σ02 · Qp(η, θ−n) ≤ 0.

Later we will show that Qp(η, θ−n) is non-increasing in η, ∀n ∈ A (i.e., Lemma 7), hence,

2Qp(η, θ )η ∂Qp(η, θ−n) + Qp(η, θ ) + 1/σ2 Qp(η, θ ) + η ∂ Qp(η, θ ) ≤ 0.

−n

∂η

−n

0

−n σ02 ∂η

−n

Lemma 6. If θn ∼ Uniform[θ, θ¯] which is independent for every n ∈ A, then (78) is satisﬁed.

Proof : First, for an agent n ∈ A,

Qp(θn, θ−n) =

1 F (θ ) ·

1 2,

θn

+

n f (θn)

W (θ)

(79)

where the function W : [θ, θ¯]N → R+ is the solution of the below equation:

hence

31

2

1

W (θ) − σ02 · W (θ) −

= 0. θm + F (θm)

m∈A

f (θm)

∂Qp(θn, θ−n)

θn

1 + ∂∂θ

−

≥

∂θn

Qp(θn, θ−n) + 1/σ02 1 + 1

θ

1 ≥,
2

F (θ) f (θ)
F (θ) f (θ)

1 · 1 + [Wσ(θ2)]2 θn + Ff ((θθnn))
0

(80)

where the ﬁnal inequality holds for uniform distribution.

As the agent is selﬁsh, he will exert eﬀort qn0 to maximize his expected payoﬀ. Hence, the agent’s exerted eﬀort can be obtained by taking the ﬁrst order derivative of (75) with respect to qn and

setting it to zero, which is

(1/σ02 + qnp )2 · qnp · θˆn = (1/σ02 + qn)2 · qn · θn,

(81)

Parametric Prediction from Parametric Agents
42

where qnp is a shorthand for Qp(θˆn, θ−n). Based on (81), we have (i) if θˆn > θn, qnp < qn, (ii) if θˆn < θn, qnp > qn, and (iii) if θˆn = θn, qnp = qn. Then if θˆn > θn, the equation (77) is negative and (76) is positive. This inequality also holds for
θˆn < θn, by a similar argument. Therefore, the agent n will truthfully report his own cost type. Step 3. Incentivize agents to exert precisely the eﬀorts as desired by the principal

under COPE

Then we will show that an agent n, ∀n ∈ A exerts eﬀort such that qn = qnp would maximize his

expected payoﬀ as follows.

E{x∗,yn,θ−n} U a(x∗, qn, yn, θn, θ−n)

= Eθ π(θn, θ−n) − K(θn, θ−n) · 1 + S(θn, θ−n) − 1 θnq2 ,

−n

1/σ02 + qn

2n

(82)

where the expectation is taken with respect to θ−n, x∗, and yn.

It can be veriﬁed that (82) is concave in qn. Hence, by taking the ﬁrst order derivative of (82)

with respect to qn, we have

∂ E U a(x∗, qn, yn, θn, θ−n) = ∂qn

1/σ02 + qnp 1/σ02 + qn

2
· θn · qnp − θn0 · qn.

(83)

We can verify that the value of (83) equals to zero only when qn = qnp . Hence, agent n will exert

the eﬀort as the principal desires to maximize his expected payoﬀ. Then (73) is rewritten as

µ0 · 1/σ02 + yn · qnp

yˆn = 1/σ2 + qp .

0

n

(84)

Because the principal knows the value of µ0, σ02, and qnp , she can infer the agent n’s truth

observation yn from (84).

Step 4. Maximize the principal’s expected utility under COPE Then we look at the

expected payoﬀ of the principal. We will show that the desired eﬀort level Qp(θn, θ−n) deﬁned in (12) and the function π(θn, θ−n) deﬁned in (70) can maximize the principal’s expected payoﬀ and satisfy BIC and BIR condition.
Notice that when an agent n, ∀n ∈ A exerts eﬀort such that qn = qnp and reports yˆn = µ0·11//σσ0022++yqnpn·qnp , the expected payment function is reduced to

E{x∗,yn,θ−n} R(x∗, yn, qn, θn, θ−n)

Parametric Prediction from Parametric Agents
= Eθ−n π(θn, θ−n) − K(θn, θ−n)(x∗ − yˆn)2 + S(θn, θ−n) = Eθ−n π(θn, θ−n) .

43 (85)

The expected payoﬀ of the agent n is rewritten as

Eθ−n U a π(θˆn, θ−n), Qp(θˆn, θ−n), θn

= Eθ

π(θˆ , θ

1 )− θ

·

Qp(θˆ , θ

)2,

n −n

n

n −n

−n

2

(86)

and the BIC and BIR conditions, i.e., (5) and (6), can be rewritten as

Eθ−n U a π(θn, θ−n), Qp(θn, θ−n), θn ≥ Eθ−n U a π(θˆn, θ−n), Qp(θˆn, θ−n), θn , ∀θˆn = θn, (87)

Eθ−n U a π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn.

(88)

Base on Lemma 2 and (85), the expected payoﬀ of the principal is

Ex∗,y,θ[U p(x∗, qp, y, θˆ)] = −Bp qp − Ex∗,y,θ

R(x∗, yn, θn, θ−n)

n∈A

1

= − 1/σ2 +

qp − Eθ

π(θn, θ−n) .

0

n∈A n

n∈A

Recall that qnp = Qp(θn, θ−n), the principal’s optimal problem deﬁned in (7) can be rewritten as

sup

E[U p(x∗, qp, y, θˆ)],

{Qp (θ),π (θ)},∀θn ∈θ,∀n∈A

subject to : BIC and BIR in (87) and (88).

(89)

In the following lemmas, we characterize an equivalent formulation for the feasible region deﬁned

by BIC and BIR. Using these lemmas, we show that Qp(θn, θ−n) deﬁned in (12) and π(θn, θ−n)

deﬁned in (70) are the optimal solution that solves the principal’s problem in (89).

Lemma 7. The solution of (89) is feasible if and only if it satisﬁes the following conditions for all θn ∈ [θ, θ¯]:

• The expected payoﬀ of the agent n, ∀n ∈ A is

Eθ−n U a π(θn, θ−n), Qp(θn, θ−n), θn

1 = 2 Eθ−n

θ¯
Qp(x, θ−n) 2dx ,
θn

(90)

• Qp(θn, θ−n) is non-increasing in θn.

Proof : The proof of Lemma 7 is as follows. We ﬁrst show that BIC and BIR imply the condition in (90).

Parametric Prediction from Parametric Agents
44

Notice that the ﬁrst derivative of (86) is:

∂Eθ−n U a π(θˆn, θ−n), Qp(θˆn, θ−n), θn ∂θn

= Eθ

1 −

Qp(θˆ

,θ

) 2 ≤ 0.

n −n

−n 2

(91)

Then, for any θn1 > θn2 , we have

Eθ−n

U

a

(π

(

θ

1 n

,

θ

−n

)

,

Q

p

(

θn1

,

θ

−n

)

,

θ

1 n

)

≤ Eθ−n

U

a

(

π

(

θn1

,

θ

−

n

)

,

Q

p

(θ

1 n

,

θ

−

n

)

,

θ

2 n

)

≤ Eθ−n

U

a

(

π

(

θn2

,

θ

−

n

)

,

Q

p

(θ

2 n

,

θ

−

n

)

,

θ

2 n

)

,

(92)

where the ﬁrst inequality is due to (91) and the second is due to the BIC condition deﬁned in (87).

Recall that the BIR condition is

Eθ−n U a π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn ∈ [θ, θ¯], (93)
which implies that, for an agent n ∈ A with any value θn ∈ [θ, θ¯], his expected payoﬀ should be nonnegative. Then the expected payoﬀ of the agent n with cost parameter θ¯ must be binding at zero. Otherwise, the principal can raise the π(θ¯, θ−n) by a small value of δ > 0, which does not violate the constraint of (93) but raises the principal’s expected payoﬀ. Hence, we have

Eθ−n U a(π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯) = 0. (94)

Let U a(θn, θ−n) = U a π(θn, θ−n), Qp(θn, θ−n), θn . From BIC condition, we have

Eθ U a(θn, θ−n) = max Eθ U a π(θˆn, θ−n), Qp(θˆn, θ−n), θn .

−n

θˆn

−n

By using the envelope theorem, we have:

∂Eθ−n U a(θn, θ−n) ∂θn

= ∂Eθ−n U a(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) ∂θn

= Eθ−n
θˆn =θn

− 1 Qp(θn, θ−n) 2 , 2

where θn is a parameter. By integrating both sides from the value of θn to θ¯ and using (94), we get

Eθ−n U a π(θn, θ−n), Qp(θn, θ−n), θn

1 = 2 Eθ−n

θ¯
Qp(x, θ−n) 2dx
θn

(95)

We prove that Qp(θn, θn) is nonincreasing in θn by contradiction. Let pn as the shorthand notation

for

π(θn, θ−n).

Suppose

for

any

θn1

>

θn2 ,

we

have

Qp

(

θ

1 n

,

θ

−n

)

>

Qp

(θ

2 n

,

θ

−

n

).

Because

∂2U a

p

n

,

q

p n

,

θ

n

∂qp ∂θ

= −qnp < 0, and

nn

(96)

Parametric Prediction from Parametric Agents
45

∂2U a

p

n

,

q

p n

,

θ

n

∂qp 2

= −θn ≤ 0,

n

(97)

we have

0 = ∂U a pn, qnp , θn1 ∂qnp

qnp =Qp(θn1 ,θ−n)

≤ ∂U a pn, qnp , θn1 ∂qnp

qnp =Qp(θn2 ,θ−n)

< ∂U a pn, qnp , θn2 ∂qnp

,
qnp =Qp(θn2 ,θ−n)

where the ﬁrst equality is due to BIC when the agent n’s cost parameter θn has the value of θn1 ,

the second equality is due to(96), and the inequality is due to (97).

However, based on the BIC condition, if the agent n’s cost parameter θn has the value of θn2 ,

then we should have

∂Ua

p

n

,

q

p n

,

θ

2 n

∂qnp

= 0,
qnp =Qp(θn2 ,θ−n)

which

holds

true

for

all

scalar

value

of

pn.

Hence,

for

any

θn1

>

θn2 ,

Qp

(

θ

1 n

,

θ

−n

)

≤

Qp(θn2 , θ−n).

Then we need to prove that (90) implies BIC and BIR deﬁned in (87) and (88). Notice that we

have:

Eθ−n U a π(θˆn, θ−n), Qp(θˆn, θ−n), θn

= Eθ−n = Eθ−n = Eθ−n = Eθ−n

θ¯ ∂U a π(θˆn, θ−n), Qp(θˆn, θ−n), z

−

dz

θn

∂z

1 θ¯

2

θˆn ∂U a π(θˆn, θ−n), Qp(θˆn, θ−n), z

Qp(η, θ−n) dη −

dz

2 θˆn

θn

∂z

1 −

θn
Qp(η, θ

2

1

) dη −

θˆn
Qp(η, θ

2

1

) dη +

θˆn
Qp(θˆ , θ

)

2 θ¯

−n 2 θn

−n 2 θn n −n

1 θˆ U a π(θn, θ−n), Qp(θn, θ−n), θn +
2 θn

Qp(θˆn, θ−n) 2 − Qp(η, θ−n) 2

2dz dη ,

where the second equality and the forth equality is obtained by (90).

If θˆn > θn, then the above equation is non-positive (because Qp(η, θ−n) is non-increasing in η),

hence

Eθ−n U a(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) < Eθ−n U a(π(θn, θ−n), Qp(θn, θ−n), θn) .

Parametric Prediction from Parametric Agents
46 This inequality also holds for θˆn < θn by a similar argument. Therefore, the two condition imply BIC.
BIR is veriﬁed by putting θn back to (90). Then based on Lemma 7, we have the following lemma.

Lemma 8. The optimisation problem in (89) has the following equivalent formulation:

{Qp(mθ)}a,∀xθn∈θEθ − 1/σ02 +

1

1

−

n∈A Qp(θn, θ−n) 2 n∈A

s.t. Qp(θn, θ−n) is nonincreasing in θn.

Qp(θn, θ−n)

2

1

θn − 2

n∈A

Qp(θn, θ−n)

2 F (θn) , f (θn) (98)

Proof : The proof of Lemma 8 is as follows. The expected payoﬀ of the principal can be written

as:

Eθ − 1/σ02 +

1 − 1 Qp(θn, θ−n) 2 · θn − U a π(θn, θ−n), Qp(θn, θ−n), θn

n∈A Qp(θn, θ−n) 2 n∈A

n∈A

= Eθ − 1/σ02 +

1

1

21

θ¯

2

−

Qp(θn, θ−n) −

Qp(x, θ−n) dx

n∈A Qp(θn, θ−n) 2 n∈A

2 n∈A θn

Using integration by parts and Lemma 7, we can get the conclusion.

Based on Lemma 8, the principal’s problem thus reduces to choosing the desired eﬀort

Qp(θn, θ−n) for each agent n ∈ A.

Let qnp = Qp(θn, θ−n) and

1

1

M

q1p

,

.

.

.

,

q

p N

= − 1/σ2 +

− qp 2

0

n∈A n

2

1

qnp · θn − 2

qp 2 F (θn) . n f (θn)

n∈A

n∈A

Let G = [∂2M/∂qip∂qjp] be the matrix of second order derivatives, and it is a symmetric matrix with

negative diagonal terms as

∂2M

2

=−

∂qip∂qjp

1/σ2 +

3 , j = i, qp

0

n∈A n

∂2M

2

F (θi)

∂qp2 = − 1/σ2 +

qp

3 − θi −

. f (θi)

i

0

n∈A n

As we can verify that, for k = 1, . . . , N , the kth leading principal minors of G alternate in sign,

hence G is negative deﬁnite and M is strictly concave. Thus, the principal’s desired eﬀort levels

from agents qnp = Qp(θn, θ−n), ∀n ∈ N are the solution of the below equations:

1

− Qp(θ , θ ) · θ − Qp(θ , θ ) · F (θn) = 0, n = 1, 2, . . . , N

(99)

1/σ2 +

qp 2

n −n n

n −n f (θn)

0

n∈A n

Parametric Prediction from Parametric Agents
47

Using Cramer’s rule and the assumption that the function F is log concave in θ , we can verify that

∂Qp(θn, θ−n) ∂2M/∂qnp ∂θn

∂θn

= − ∂2M/∂qp 2 ≤ 0,

n

(100)

which shows that Qp(θn, θ−n) derived from (99) is non-increasing in θn, so that it is the feasible

solution of (98).

The solution of (99) is

Qp(θn, θ−n) =

1 F (θ ) ·

1 2,

θn

+

n f (θn)

W (θ)

where the function W : [θ, θ¯]N → R+ is the solution of the below equation:

(101)

31

2

1

W (θ) − σ02 · W (θ) −

= 0. θm + F (θm)

m∈A

f (θm)

The real root of the above cubic equation is as follows.

(102)

W (θˆ) = 1 + 3

11 +

1 +

3σ02

27σ06

2

m∈A

θˆm

+

F (θˆm) ˆ

f (θm)

where function λ : [θ, θ¯]N → R+ is given as

λ(θˆ) + 3

11 +

1 −

27σ06

2

m∈A

θˆm

+

F (θˆm) ˆ

f (θm)

λ(θˆ), (103)

λ(θˆ) = 1 27σ06

1

m∈A

θˆm

+

F (θˆm) ˆ

f (θm)

1 +
4

1

2

θˆ + F (θˆm) .

m∈A m f (θˆm)

According to (90), we have

Eθ

π(θn, θ−n) − 1 · [Qp(θn, θ−n)]2 · θn

1 = Eθ

−n

2

2 −n

θ¯
Qp(x, θ−n) 2dx .
θn

From the above equation, we can derive the optimal payment function as given in (70).

C. Proof of Theorem 3: General Setting The proof will proceed in four steps. The ﬁrst three steps show that our mechanism incentivizes the agents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that irrespective of what an agent reports as his cost parameter, and irrespective of the eﬀort he exerts,

Parametric Prediction from Parametric Agents
48 the agent is always incentivized to report his true observation. We follow this up and show that irrespective of the eﬀort that an agent exerts, he is always incentivized to report his cost parameter correctly. The third step completes the proof of truthfulness, showing that under truthful reporting of the cost parameter and the observation, in our mechanism, an agent is always incentivized to exert precisely the eﬀort as desired by the principal. Finally, we show that among all mechanisms that ensure truthful reports, our mechanism maximizes the principal’s expected utility.
Step 1. Truthful reporting of observation under COPE We ﬁrst show that the agent will choose

yn = arg infyˆn(yn)E a(x∗, yˆn) .

(104)

to maximize his expected payoﬀ, given his exerting eﬀort qn and own observation yn. As shown in (22), π(θˆn, θ−n), K(θˆn, θ−n) and S(θˆn, θ−n) are independent of yˆn. Moreover, the
value of K(θˆn, θ−n) is always positive. Hence, when the agent n determines his reporting observation strategy to maximize his expected payoﬀ, i.e.,

yˆn ∈ arg maxE π(θˆn, θ−n) − K(θˆn, θ−n) · a(x∗, yˆn) + S(θˆn, θ−n) − C qn, θn ,

where the expectation is taken with respect to x∗ and cost parameters θ−n = [θ1, . . . , θn−1, θn+1, . . . , θN ]T of all the agents except agent n, it is equivalent for the agent n to choose the reporting strategy such that

yˆn ∈ arg minEx∗[ a(x∗, yˆn)].

(105)

According to (18) and (19) in Section 4.2.1, the value of Ex∗[ a(x∗, yˆn)] is minimized when yˆn = yn, and the expected value is

Ex∗ [ a(x∗, yˆn)] = Ba qn .

Step 2. Truthful reporting of cost parameter under COPE

Parametric Prediction from Parametric Agents
49

We now show that the agent will truthfully reveals its cost type. The expected payoﬀ of the

agent whose cost type is θn0 but reports θˆn0 is: E{x∗,yn,θ−n} U a(x∗, θˆn, qn, yn, θn, θ−n) = Eθ−n π(θˆn, θ−n) − K(θˆn, θ−n) · Ba qn + S(θˆn, θn) − C(qn, θn) .

(106)

For notational convenience, we deﬁne the function U ae : R × [θ, θ¯] × R+ × [θ, θ¯]N → R+ as

U ae(θˆn, qn, θn, θ−n) = π(θˆn, θ−n) − K(θˆn, θ−n) · Ba qn + S(θˆn, θn) − C(qn, θn) ,

(107)

where θ−n are the random variables of all agents cost type except that of agent n. By comparing (106) to (107), the expected payoﬀ of the agent n is

E{x∗,yn,θ−n} U a(x∗, θˆn, qn, yn, θn, θ−n) = Eθ−n U ae(θˆn, qn, θn, θ−n) .

Then, by the mean value theorem, we have:

Eθ

U ae(θn, θn, θ−n) − Eθ

U ae(θˆn, θn, θ−n) = Eθ

∂U ae(η, θn, θ−n) · (θ − θˆ )

n

n

−n

−n

−n

∂η

We further have:

∂U ae(η, θn, θ−n)

Eθ−n

∂η

Ba(qn)

Ba(qnp )

∂c(qnp , η)

c(qnp , η) d2Ba(qnp ) ∂qnp

= E dBa(qp )/dqp · 1 − Ba(q ) ·

∂η

−

·

dBa(qp )/dqp

dqp 2

· ∂η

n

n

n

n

n

n

(108)

As the agent is selﬁsh, he will exert eﬀort qn to maximize his expected payoﬀ. Hence, the agent’s

exerted eﬀort can be obtained by taking the ﬁrst order derivative of (107) with respect to qn and

setting it to zero, which is

dBa qn

· c(qp , θˆ

dBa )=

qnp

· c(q , θ )

dqn

nn

dqp

n

nn

(109)

As

we

assume

that

dB a (z ) dz

≤ 0,

then

based

on

(109),

we

have

(i)

if

θˆn

> θn,

qnp

< qn,

(ii)

if

θˆn

< θn,

qnp > qn, and (iii) if θˆn = θn, qnp = qn.

As we assume that dBa(qnp )/dqnp ≤ 0, d2Ba(qnp )/dqnp 2 0, and later we will prove in Lemma

10 that Qp(θn, θ−n) is nonincreasing in θn, if (26) holds, i.e., ∂c (Qp(η, θ−n), η)/∂η 0, then the

Parametric Prediction from Parametric Agents
50 above equation (108) is negative when θˆn > θn. Based on this, we have Eθ−n U ae(θn, θn, θ−n) > Eθ−n U ae(θˆn, θn, θ−n) . This inequality also holds for θˆn < θn, by a similar argument. Therefore, an agent will truthfully report his own cost parameter.
Step 3. Incentivize agent to exert precisely the eﬀort as desired by the principal under COPE As we have proved in Step 2 that the agent n would truthfully report his cost type (θˆn = θn). Next we will show that the agent n exerts eﬀort such that qn = qnp would maximize his expected payoﬀ as follows.

E

U

ae

(

x∗

,

q

p n

,

y

n

,

θˆn

,

θˆ−n

)

= π(θˆn, θˆ−n) − K(θˆn) · Ba

qn

+ S(θˆn) − C(qn, θn),

(110)

where the expectation is taken with respect to θ−n.

It can be veriﬁed that (110) is concave in qn as we assume that

d2Ba qn dqn2 ≥ 0.

Hence, by taking the ﬁrst order derivative of (110) with respect to qn, we have

∂ E U ae(x∗, qp , y , θˆ θˆ ) = c qnp , θˆn

dBa qn

·

− c(q , θ ).

∂qn

n n n −n

dBa qp /dqp

dqn

nn

n

n

(111)

We can verify that the value of (111) equals to zero when qn = qnp . Hence, agent n will exert the

eﬀort as the principal desires to maximize his expected payoﬀ.

Step 4. Maximize the principal’s expected utility under COPE

Then we look at the expected payoﬀ of the principal. To maximize the expected utility for the

prediction, the principal solves

max E − p x∗, xˆ(y, qp)|(yˆn, qp) . xˆ
The principal employs the Bayes estimate xˆ as follows:

(112)

xˆ yˆ, q∗ = arg infxˆE p x∗, xˆ(yˆ, qp) .

(113)

It follows that the expected utility of the principal is

Bp qp = inf E p x∗, xˆ(y, qp) . xˆ

(114)

Parametric Prediction from Parametric Agents
51 We then show that the desired eﬀort level Qp(θˆn, θˆ−n) deﬁned as the solution of (125) and the function π(θˆn, θˆ−n) deﬁned in (23) can maximize the principal’s expected payoﬀ and satisfy BIC and BIR conditions. Notice that agent n exerts eﬀort such that qn = qnp and reports yn ∈ arg minEx∗[ a(x∗, yˆn)], the expected payment function deﬁned in (22) is reduced to

E[R(x∗, yn, θn, θ−n)] = π(θn, θ−n),

where the expectation is taken with respect to x∗ and yn. Then the expected payoﬀ of agent n is rewritten as

Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn = Eθ−n π(θˆn, θ−n) − C Qp(θˆn, θ−n), θn ,

(115)

and the BIC and BIR conditions, i.e., (5) and (6) can be rewritten as

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn ∀θˆn = θn, (116)

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn.

(117)

Then the expected payoﬀ of the principal is

E[U p(x∗, qp, y, θ)] = −Bp qp − π(θn, θ−n)
n∈A

where the expectation is taken with respect to x∗ and y.

Recall that qnp = Qp(θn, θ−n), we then rewrite (7) as

sup

E[U p(x∗, qp, y, θˆ)],

{Qp (θ),π (θ)},∀θn ∈θ

subject to : BIC and BIR in (116) and (117).

(118)

For the feasible region deﬁned by BIC and BIR, we can characterise an equivalent formulation

in the following lemma:

Lemma 9. The solution of (118) is feasible if and only if it satisﬁes the following conditions for all θn ∈ [θ, θ¯]:

Parametric Prediction from Parametric Agents
52

• The expected payoﬀ of agent n is

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn

= Eθ−n

θ¯ ∂C Qp(z, θ−n), η

dz ,

θn

∂η

• Qp(θn, θ−n) is non-increasing in θn.

(119)

Proof : We ﬁrst show that BIC and BIR imply the condition 119.

The ﬁrst derivative of (115) is

∂Eθ−n U ae(π(θˆ, θ−n), Qp(θˆ, θ−n), η)

∂C(Qp(θˆ, θ−n), η)

∂η

= Eθ−n −

∂η

≤ 0.

(120)

Then, for any θn1 > θn2 , we have

Eθ−n

U

ae

(π

(

θ

1 n

,

θ

−n

)

,

Q

p

(

θn1

,

θ

−n

)

,

θ

1 n

)

≤ Eθ−n

U

ae

(

π

(

θn1

,

θ

−

n

)

,

Q

p

(θ

1 n

,

θ

−

n

),

θ

2 n

)

≤ Eθ−n

U

ae

(

π

(

θn2

,

θ

−

n

)

,

Q

p

(θ

2 n

,

θ

−

n

),

θ

2 n

)

,

(121)

where the ﬁrst inequality is due to (120) and the second is from the BIC condition deﬁned in (116).

Recall that the BIR condition is

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn ≥ 0, ∀θn ∈ [θ, θ¯],

(122)

which implies that, for the agent n ∈ A with any value θn ∈ [θ, θ¯], his expected payoﬀ should be nonnegative. Then the expected payoﬀ of the agent n with cost parameter θ¯ must be binding at zero. Otherwise, the principal can reduce the π(θ¯, θ−n) by a small value of δ > 0, which does not violate the constraint of (122) but raises the principal’s expected payoﬀ. Hence, we have

Eθ−n U ae(π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯) = 0.

(123)

Let U ae(θn, θ−n) = U ae π(θn, θ−n), Qp(θn, θ−n), θn . From BIC condition, we have

Eθ U ae(θn, θ−n) = max Eθ U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn .

−n

θˆn

−n

By using the envelope theorem, we have:

∂Eθ−n U ae(θn, θ−n) ∂η

= ∂Eθ−n U ae(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) ∂θn
∂C(Qp(θn, θ−n), θn) = Eθ−n − ∂θn ,

θˆn =θn

Parametric Prediction from Parametric Agents
53 where θn is a parameter. By integrating both sides from θn to θ¯ and using (123) and the assumption that the random variable θn of the agent n is independent for every n ∈ A, we get

Eθ−n U ae π(θn, θ−n), Qp(θn, θ−n), θn = Eθ−n

θ¯ ∂C(Qp(z, θ−n), η)

dz .

θn

∂η

(124)

We prove that Qp(θn, θn) is nonincreasing in θn by contradiction. Let pn as the shorthand notation

for

π(θn, θ−n).

Suppose

for

any

θn1

>

θn2 ,

we

have

Qp

(

θ

1 n

,

θ

−n

)

>

Qp(θn2 , θ−n).

Because

∂2U ae pn, qnp , θn = − ∂c(q, θn) < 0, and

∂qnp ∂θn

∂θn

∂2U ae

pn

,

q

p n

,

θ

n

∂c(q, θn)

∂qp 2

=−

< 0,

∂q

n

we then have

0 = ∂U ae pn, qnp , θn1 ∂qnp
< ∂U ae pn, qnp , θn1 ∂qnp
< ∂U ae pn, qnp , θn2 ∂qnp

qnp =Qp(θn1 ,θ−n)
qnp =Qp(θn2 ,θ−n)
,
qnp =Qp(θn2 ,θ−n)

where the equality is due to BIC when the agent n’s cost parameter θn has the value of θn1 , the

second equality is due to (C), and the inequality is due to (C).

However, based on the BIC condition, if the agent n’s cost parameter θn has the value of θn2 ,

then we should have

∂U ae

p

n

,

q

p n

,

θ

2 n

∂qnp

= 0,
qnp =Qp(θn2 ,θ−n)

which

holds

true

for

all

scalar

values

of

pn.

Hence,

for

any

θn1

>

θn2 ,

Qp(θn1 , θ−n)

≤

Qp

(θ

2 n

,

θ

−

n

).

Then we need to prove that ((119)) implies BIC and BIR deﬁned in ((116)) and ((117)).

BIR is veriﬁed by putting θn back to (119). Besides, by putting θn = θ¯ back to (119), we have

Eθ−n U ae π(θ¯, θ−n), Qp(θ¯, θ−n), θ¯ = 0.

Parametric Prediction from Parametric Agents
54

Then we prove that ((119)) implies BIC. Notice that we have:

Eθ−n U ae π(θˆn, θ−n), Qp(θˆn, θ−n), θn

=1 Eθ−n =2 Eθ−n =3 Eθ−n =4 Eθ−n =5 Eθ−n

θ¯ ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z

−

dz

θn

∂z

U ae π(θˆ , θ

), Qp(θˆ , θ

), θˆ

−

θˆn ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z dz

n −n n −n n θn ∂z

θ¯ ∂C(Qp(η, θ−n), z)

θˆn ∂U ae π(θˆn, θ−n), Qp(θˆn, θ−n), z

dη −

dz

θˆn

∂z

θn

∂z

θn ∂C(Qp(η, θ−n), z)

θˆn ∂C(Qp(η, θ−n), z)

θˆn ∂C(Qp(θˆn, θ−n), z)

−

dη −

dη +

dz

θ¯

∂z

θn

∂z

θn

∂z

θˆ
U ae π(θn, θ−n), Qp(θn, θ−n), θn +
θn

∂C(Qp(θˆn, θ−n), z) ∂C(Qp(η, θ−n), z)

−

∂z

∂z

dη ,

where the third equality and the ﬁfth equality is obtained by (119).

If θˆn > θn, then the above equation is non-positive. This is because Qp(η, θ−n) is non-increasing

in η and ∂C(q, θ)/∂θ > 0. Hence,

Eθ−n U ae(π(θˆn, θ−n), Qp(θˆn, θ−n), θn) < Eθ−n U ae(π(θn, θ−n), Qp(θn, θ−n), θn) .

This inequality also holds for θˆn < θn by a similar argument. Therefore, the two condition imply BIC.
Then based on Lemma 9 and let qnp = Qp(θn, θ−n), we have the following Lemma.

Lemma 10. The optimisation problem in (118) has the following equivalent formulation:

maxEθ qp

− Bp qp

− C qnp , θn
n∈A

− ∂C qnp , θn ∂θn
n∈A

s.t. qnp is nonincreasing in θn, ∀n ∈ A.

· F (θn) f (θn)

(125)

where the expectation is taken with respect to θ.

proof : The proof of Lemma 4 is as follows. The expected payoﬀ of the principal can be written

as:

Eθ − Bp qp −

C

q

p n

,

θ

n

−

Ua

π

(

θn

,

θ

−

n

)

,

q

p n

,

θ

n

= Eθ

n∈A

n∈A

− Bp qp − C qnp , θn −

n∈A

n∈A

θ¯ ∂C qnp , η θn ∂η

dx ,

(126)

Parametric Prediction from Parametric Agents
55

where the expectation is taken with respect to θ. Notice that

θ¯ ∂C qnp , η dx = θ¯ θ¯ ∂C qnp , η dx · f (z)dz = θ¯ F (z) ∂C qnp , η dz

Eθn θn ∂η

θz

∂η

θ

∂η

θ¯ F (z) ∂C qnp , η

F (θn) ∂C qnp , η

= θ f (z) ∂η f (z)dz = Eθn f (θn) ∂η ,

where the ﬁrst equation is obtained by using integration by parts. Then by applying the above equa-

tion to (126) and the fact that {θn}n∈A are assumed to be random, independently and identically distributed on support [θ, θ¯], we can get the conclusion.

Based on Lemma 10, the principal’s problem thus reduces to choosing the desired eﬀort qnp =

Qp(θn, θ−n) for each agent n ∈ A. We ﬁrst consider the problem in (125) without the constraint. If

the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution

to the constrained problem.

Let qnp = Qp(θn, θ−n) and

M

q

p 1

,

.

.

.

,

q

p N

= − Bp qp

−

C qnp , θn −

∂C qnp , θn · F (θn) .

∂θn

f (θn)

n∈A

n∈A

As we assume that ∂2Bp(qp)/∂qip∂qjp 0, ∀j = i, ∂C(qnp , θn)/∂qnp > 0 and ∂2C(qnp , θn)/∂qnp ∂θn ≥ 0,

we can check that

∂2M

∂2Bp qp

=−

∂qip∂qjp

∂qip∂qjp

∂2M

∂2Bp qp

∂qip2 = − ∂qip2

≤ 0, j = i,

− c(qp, θ ) − ∂c(qip, θi) · F (θi) ≤ 0.

ii

∂θi f (θi)

(127)

Let G = [∂2M/∂qip∂qjp], i, j = 1, . . . , N , be the matrix of second order derivatives. Matrix G is

symmetric with negative diagonal terms as shown in (127).

As we can verify that, for k = 1, . . . , N , the kth leading principal minors of G alternate in sign,

so that G is negative deﬁnite and W is strictly concave. The computational complexity of ﬁnding

the optimal solution of (125) will depend on the speciﬁc structure of the functions.

Using Cramer’s rule, the assumption that the c.d.f. function F is log concave in θ, and the

assumption that ∂C(qip, θi)/∂qip > 0 and ∂2c(qip, θi)/∂qip∂θi ≥ 0, we can verify that

∂Qp(θn, θ−n) ∂2M/∂qnp ∂θn

∂θn

= − ∂2M/∂qp 2 ≤ 0, ∀n ∈ A,

n

(128)

Parametric Prediction from Parametric Agents
56

which shows that Qp(θn, θ−n) derived by solving (125) is nonincreasing in θn, so that it is a feasible

solution of (125).

According to (119), we have

Eθ−n π(θn, θ−n) − C Qp(θn, θ−n), θn

= Eθ−n

θ¯ ∂C Qp(z, θ−n), η

dz .

θn

∂η

From the above equation, we can derive the optimal payment function as given in (23).

Endnotes

1. We want to emphasize that whether the Bayes risk exists is a open problem. Scharf (1991), Figueiredo (2004) consider some special case such as Gaussian distribution. Characterizing the general condition for the existence and uniqueness of Bayes risk will be interesting future work. 2. To the best of our knowledge, this is the ﬁrst paper that studies how to incentivize all agents to report their truthful estimations and exert appropriate amounts of eﬀort based on their respective capabilities during a prediction process. Hence, we have not found an algorithm in the existing literature as a fair benchmark to compare with the COPE performance.

References

Abernethy, Jacob, Yiling Chen, Chien-Ju Ho, Bo Waggoner (2015) Low-cost learning via active data procurement. ACM Conference on Economics and Computation (EC).

Berg, Joyce E, Thomas A Rietz (2003) Prediction markets as decision support systems. Information systems frontiers 55(1): 79–93.

Berger, James O. (2013) Statistical decision theory and Bayesian analysis. (Springer-Verlag, New York).

Brier, Glenn W. (1950) Veriﬁcation of forecasts expressed in terms of probability. Monthly weather review 78(1): 1–3.

Brousseau, Eric, Jean-Michel Glachant (2002) The economics of contracts: theories and applications. (Cambridge University Press)

Cai, Yang, Constantinos Daskalakis, Christos H Papadimitriou (2015) Optimum statistical estimation with strategic data sources. Conference on Learning Theory (COLT).

Parametric Prediction from Parametric Agents
57 Cavallo, Ruggiero, Shaili Jain (2015) Eﬃcient crowdsourcing contests. International Conference on
Autonomous Agents and Multiagent Systems (AAMAS). Che, Yeon-Koo (1993) Design competition through multidimensional auctions. The RAND Journal of Eco-
nomics 24(4): 668–680. Chen, Ying-Ju, Sridhar Seshadri, Eitan Zemel (2008) Sourcing through auctions and audits. Production and
Operations Management 17(2): 121–138. Chiappori, Pierre-Andre, Bernard Salani´e (1997) Empirical contract theory: The case of insurance data.
European Economic Review 41(3): 943–950. Conitzer, Vincent (2009) Prediction markets, mechanism design, and cooperative game theory. Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI). Fang, Fang, Maxwell Stinchcombe, Andrew Whinston (2007) Putting your money where your mouth is-a
betting platform for better prediction. Review of Network Economics 6(2): 214–238. Figueiredo, M´ario AT. (2004) Lecture notes on bayesian estimation and classiﬁcation. Instituto de
Telecomunicacoes-Instituto Superior Tecnico. Frongillo, Rafael M, Yiling Chen, Ian A Kash (2015) Elicitation for aggregation. Proceedings of the 29th
AAAI Conference on Artiﬁcial Intelligence (AAAI). Fudenberg, Drew, Jean Tirole (1991) Game theory (The MIT Press, Cambridge MA). Gottschlich, J¨org, Oliver Hinz (2014) A decision support system for stock investment recommendations using
collective wisdom. Decision support systems 59: 52–62. Hayes, Bob E. (1998) Measuring customer satisfaction: Survey design, use, and statistical analysis methods.
International Journal of Quality & Reliability Management 16(1): 98–100. Ioannidis, Stratis, Patrick Loiseau (2013) Linear regression as a non-cooperative game. Web and Internet
Economics. Springer, 277–290. Karger, David R, Sewoong Oh, Devavrat Shah (2014) Budget-optimal task allocation for reliable crowd-
sourcing systems. Operations Research 62(1): 1–24. Koutsopoulos, Iordanis (2013) Optimal incentive-driven design of participatory sensing systems. IEEE Inter-
national Conference on Computer Communications (INFOCOM).

Parametric Prediction from Parametric Agents
58 Lehmann, Erich Leo, George Casella (1998) Theory of point estimation (Springer-Verlag, New York). Liu, Tracy Xiao, Jiang Yang, Lada A Adamic, Yan Chen (2014) Crowdsourcing with all-pay auctions: a ﬁeld
experiment on taskcn. Management Science 60(8): 2020–2037. Miller, Nolan, Paul Resnick, Richard Zeckhauser (2005) Eliciting informative feedback: The peer-prediction
method. Management Science 51(9): 1359–1373. Myerson, Roger B. (1979) Incentive compatibility and the bargaining problem. Econometrica 47(1): 61–73. Myerson, Roger B. (1981) Optimal auction design. Mathematics of operations research 6(1) 58–73. Prelec, Drazen (2004) A Bayesian truth serum for subjective data. Science 306(5695): 462–466. Robert, Christian (2007) The Bayesian choice: from decision-theoretic foundations to computational imple-
mentation (Spring-Verlag, New York). Rosling, Kaj (2002) Inventory cost rate functions with nonlinear shortage costs. Operations Research 50(6):
1007–1017. Scharf, Louis L. (1991) Statistical signal processing(Addison-Wesley, Reading Massachusetts). Selten, Reinhard (1998) Axiomatic characterization of the quadratic scoring rule. Experimental Economics
1(1): 43–62. Shah, Nihar B, Dengyong Zhou, Yuval Peres (2015) Approval voting and incentives in crowdsourcing. Inter-
national Conference on Machine Learning (ICML). Shah, Nihar B, Dengyong Zhou (2015) Double or nothing: Multiplicative incentive mechanisms for crowd-
sourcing. Advances in Neural Information Processing Systems (NIPS). Sheng, Shang-Pin, Mingyan Liu (2013) Proﬁt incentive in a secondary spectrum market: A contract design
approach. IEEE International Conference on Computer Communications (INFOCOM). Wolfers, Justin, David Rothschild (2011) Forecasting elections: Voter intentions versus expectations. Law
and Economics Workshop. Wolfers, Justin, Eric Zitzewitz (2004) Prediction markets. National Bureau of Economic Research.

