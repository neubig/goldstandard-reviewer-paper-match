arXiv:2203.00003v1 [cs.CE] 25 Feb 2022

Interfacing Finite Elements with Deep Neural Operators for Fast Multiscale Modeling of Mechanics Problems
Minglang Yinab , Enrui Zhangc , Yue Yud , George Em Karniadakisbc∗
aCenter for Biomedical Engineering, Brown University, Providence, RI bSchool of Engineering, Brown University, Providence, RI
cDivision of Applied Mathematics, Brown University, Providence, RI dDepartment of Mathematics, Lehigh University, Bethlehem, PA
Abstract
Multiscale modeling is an eﬀective approach for investigating multiphysics systems with largely disparate size features, where models with diﬀerent resolutions or heterogeneous descriptions are coupled together for predicting the system’s response. The solver with lower ﬁdelity (coarse) is responsible for simulating domains with homogeneous features, whereas the expensive high-ﬁdelity (ﬁne) model describes microscopic features with reﬁned discretization, often making the overall cost prohibitively high, especially for timedependent problems. In this work, we explore the idea of multiscale modeling with machine learning and employ DeepONet, a neural operator, as an eﬃcient surrogate of the expensive solver. DeepONet is trained oﬄine using data acquired from the ﬁne solver for learning the underlying and possibly unknown ﬁnescale dynamics. It is then coupled with standard PDE solvers for predicting the multiscale systems with new boundary/initial conditions in the coupling stage. The proposed framework signiﬁcantly reduces the computational cost of multiscale simulations since the DeepONet inference cost is negligible, facilitating readily the incorporation of a plurality of interface conditions and coupling schemes. We present various benchmarks to assess accuracy and speedup, and in particular we develop a coupling algorithm for a timedependent problem, and we also demonstrate coupling of a continuum model (ﬁnite element methods, FEM) with a neural operator representation of a particle system (Smoothed Particle Hydrodynamics, SPH) for a uniaxial tension problem with hyperelastic material. What makes this approach unique is that a well-trained over-parametrized DeepONet can generalize well and make predictions at a negligible cost.
keywords: Machine Learning, Neural Operator, DeepONet, Concurrent Multiscale Coupling, Finite Element Model, Domain Decomposition

∗Corresponding author: george karniadakis@brown.edu Preprint submitted to Elsevier

March 2, 2022

1. Introduction
Predicting and monitoring complex systems, where small-scale dynamics and interactions aﬀect global behavior are ubiquitous in science and engineering [3, 6, 20, 75, 81]. In disciplines ranging from material fracture [30] to design problems [24], models at microscale have shown their capability in representing detailed material response. However, despite their improved accuracy, the usability of microscopic models is often compromised by several computational challenges. Speciﬁcally, microscopic models require a small spatiotemporal scale to fully resolve small-scale details and capture underlying stochastic dynamics, leading to a prohibitive computational expense. Therefore, simulating material dynamics at meso- or macroscale using microscopic models is still largely beyond reach. In addition, although bottom-up approaches such as ﬁnegrained atomistic models have provided important insights into processes at microscale, they generally do not scale up to ﬁnite-size samples [34, 80, 95]. These challenges raise the need for eﬃcient mathematical models and algorithms, which are capable of capturing small-scale behaviors while being computationally expedient.
In the last two decades, a variety of multiscale approaches have been proposed to address these challenges. Microscopic eﬀects often concentrate locally, whereas a continuum model can accurately describe the system in the rest of the domain. Solving such a continuum model by well-established numerical methods would reduce the computational cost. Following this domain-decomposition strategy, several works that combine continuum and microscopic models have been proposed [18, 37, 38, 47, 58, 59, 74, 76, 85] to model multiscale systems. Another approach focuses on developing a fast surrogate as the ﬁne-scale model represented by homogenization [7, 9, 22, 82, 90, 96]. For example, [53] considered an approximation model of a partial diﬀerential equation (PDE) that contains small-scale oscillations in its coeﬃcients, in essence, replacing these coeﬃcients in the model with eﬀective properties so that the resulting solutions can adequately approximate the solutions of the original problem. However, quantifying eﬀective properties poses a challenging task in light of the feasibility of acquiring parameter values and the level of accuracy of the homogenized PDE model compared to the original microscopic model.
Recently, deep learning algorithms have been proposed for simulating physical problems [12, 14, 35, 63, 94]. In particular, neural networks have been employed in conjunction with standard numerical models to address the aforementioned challenges in multiscale modeling [3, 5, 16, 62, 66, 71, 78]. In [5], Arbabi et al. proposed a data-driven method that trains deep neural networks to learn coarse-scale partial diﬀerential operators based on ﬁne-scale data. In [8], Bhatia et al. presented a novel paradigm of multiscale modeling that couples models at diﬀerent scales using a dynamic-important sampling approach. A machine learning model is employed to dynamically sample in the phase space, hence enabling an automatic feedback from micro to macro scale. In [52, 52], Masi and his collaborators developed a thermodynamics-based artiﬁcial neural network (TANN) and applied it in multiscale modeling of materials with microstructure. Their results demonstrated that TANN is capable of implicitly learning the constitutive model from data and predicting the corresponding stress ﬁelds based on state variables. The authors also demonstrated that TANN is capable
2

Figure 1: Schematic of multiscale modeling with DeepONet. (a) In a multiscale mechanics system, traction T (x, t) acts on the boundary of domain Ω, which is decomposed into sub-domains described by a variety of microscopic models. A macroscopic model describes the response in the bulk region whereas nonlinear, microscopic, or data-driven models capture the detailed response in the small regions. (b) DeepONet, composed of a branch and a trunk net, is able to learn the response of the microscopic systems (from simulated or multi-modal data) and serve as a surrogate in the multiscale system. MD: molecular dynamics, DPD: dissipative particle dynamics, SPH: smoothed particle hydrodynamics, PD: peridynamics, NL FEM: nonlinear ﬁnite element, FEM: ﬁnite element, MF: multiﬁdelity.
of solving boundary value problem in a multiscale system with complex microstructure using double-scale homogeneization scheme. Other applications of machine learning in multiscale modeling include parameter inference [60, 64, 84, 86], uncertainty quantiﬁcation [15, 72], data-driven modeling [15, 33, 65, 71, 90], etc.
In the last few years, a new family of machine learning model, deep neural operators, have been proposed to learn the solution operator of a PDE system implicitly [42, 44, 49, 89]. Unlike another type of scientiﬁc machine learning, physics-informed neural networks (PINNs) [67], these neural operators can solve a PDE system given a new instance of interface conditions or model parameters without retraining [13, 27, 43, 45, 46, 51, 87]. Hence, such computational advantage enables neural operators to serve as an eﬃcient surrogate model in multiscale coupling tasks, and especially for time-depenendent multiscale problems, which even today have remained prohibitively expensive. Among the state-of-the-art neural operators, the Deep Operator Network (DeepONet) serves as a unique model with exceptional generalization capability and ﬂexibility, which can learn the solution operator in irregular domains even in the presence of noise [50]. Another possibility is to use the Fourier neural operator (FNO), which is fast but is limited to complex geometries and structured data [42, 44]. Herein, we choose DeepONet as the surrogate model. For a more thorough comparison between DeepONet and FNO, we refer the reader to [39, 50].
The present work aims to address the aforementioned challenges by developing a new multiscale coupling framework, as demonstrated in Fig.1. Speciﬁcally, the new framework couples a surrogate of the microscopic system (DeepONet) with a standard ﬁnite element method (FEM) that represents the macroscopic
3

Figure 2: Schematic architecture of DeepONet. DeepONet learns the mapping operator G from an input function g to its corresponding output function G(g). The input of branch and trunk net are g and y ∈ Rp, which represents a discretized function from g(x1) to g(xm) and information such as coordinates and time, respectively. Note that the output dimension of the trunk net, n, is consistent with that of the branch net. The ﬁnal output G(g)(y) is computed as the dot product of b and t.
model. The coupling framework is ﬂexible in choosing the domain decomposition algorithms, the types of boundary condition, or the nature of multiphysics problems (static or time-dependent). In addition, the surrogate model can learn from a plurality of ﬁne-resolution microscopic systems or from experimental data (Fig. 1(b)). Our approach is the ﬁrst attempt to couple neural operators with standard numerical solvers using a concurrent coupling method.
The paper is organized as follows: In Sec. 2.1, we brieﬂy introduce DeepONet and the adopted coupling algorithms. In Sec. 3, we report the coupling performance for a series of benchmarks, including a 2D Poisson equation, a 1D heat problem, and a uniaxial tension problem with elastoplastic materials and hyperelastic materials. We conclude by a brief discussion on the implications of the presented model in Sec. 4. In the appendices, we give an overview of the smoothed particle hydrodynamics (SPH) method and present additional results for FEM and SPH simulations. Finally, we provide further details on the network training and data generation.
2. Methodology
In this section, we introduce the general architecture of DeepONet (Sec. 2.1) and the domain decomposition methods for the coupling framework (Sec 2.2).
4

2.1. Deep Operator Network (DeepONet)

We brieﬂy summarize the general architecture of DeepONet employed in this work. Let Ω ⊂ Rp be a

bounded open set, which is the domain of our input and output functions; DeepONet can learn a general continuous operator between two Banach spaces of functions taking values in Rdf and Rdu , respectively. We denote the input and output function spaces as A = A(Ω; Rdf ) and U = U (Ω; Rdu ). The network aims

at approximating a mapping G : A → U between an input function g ∈ A and its corresponding output

function G(g) ∈ U. Although DeepONet can learn mappings between vector-valued functions, for simplicity of exposition we focus on learning scalar-valued functions (du = 1) in the following. For any y ∈ Ω ⊂ Rp, G(g)(y) ∈ R is the evaluation of function G(g) at y. As shown in Fig. 2, the branch network takes a function g in its discrete representation [g(x1), g(x2), ..., g(xm)] at locations {xj}m j=1 as input and yields an array of operator features, {bi}ni=1, as its output. The trunk network takes y as input and yields another array of operator features, {ti}ni=1. Finally, G(g)(y) can be approximated by [17, 49]:

n

G(g)(y) ≈ biti.

(1)

i=1

In this work, we adopt fully-connected neural networks as the architecture of both sub-networks. We refer

the readers to [41, 49] for theoretical analysis and error estimations of DeepONet.

Regarding the loss function, we adopt the mean squared error (MSE) which measures the square of L2 norm between model predictions and training data. Given M sample input functions g(i), i = 1, · · · , M , and the ground-truth of their corresponding output functions, Gdata(g(i))(·) on a set of N points {yj}Nj=1, the loss is expressed as:
L = M1N M N (Gmodel(g(i))(yj ) − Gdata(g(i))(yj ))2 + Rˆ, (2)
i=1 j=1
where Gmodel is the learned operator of DeepONet for approximation and Gdata(g(i))(yj) denotes the data measurements for the i-th output function at j-th evaluation point. Rˆ is an additional loss term for the

purpose of regularization (see Sec. 3.1 and Eq. 11).

2.2. Coupling Methods
Here we introduce the procedure of coupling DeepONet with a numerical model. Consider a system deﬁned on a computational domain Ω; we decompose the domain into ΩI and ΩII according to the features in each domain. These two domains are described by a macroscopic model (model I) and a microscopic model (model II), respectively. In this paper, we choose model I as FEM. Nonetheless, the proposed procedure can be generalized to couple other mesh-based models [32, 36, 61, 77] or particle (meshfree) models [23, 28, 56, 69] as shown in Fig. 1(b). Model II represents a DeepONet, which serves as a surrogate for the microscopic, ﬁne-scale model.
Notably, the domain decomposition can be either overlapping or non-overlapping. Fig. 3(a) shows an one-dimensional illustration of the domain decomposition method. For the overlapping setting, Γ1 (= {x1}

5

for this 1D case) represents the internal boundary of model I and Γ2 (= {x2}) is the boundary of model II. The overlapping region is ΩI ∩ ΩII = [x2, x1]. The two models communicate with each other by exchanging interface conditions on Γ1 and Γ2. For the non-overlapping setting, we have Γ1 = Γ2 := Γ (x1 = x2 for this 1D case) and the two model exchanges interface information on Γ. Fig. 3(b) and Algorithm 1 summarize the iterative procedure of the coupling framework with a Robin-type boundary condition [91]. For the n-th iteration, we utilize quantities (·)n to calculate (·)n+1. To initiate the coupling procedure (n = 0), we start with an initial guess of the interface solution value u0(x1) and derivative-related information T 0(x1) on Γ1. Then, we take a linear combination of these quantities and forms a Robin-type boundary condition on Γ1, namely, h˜0(x1) = R1un(x1) + R2T 0(x1), which is applied on Model I. The method proceeds by solving for unI +1(x), x ∈ ΩI , from Model I, and interpolating the computed solution or its derivatives at Γ2. The interpolated information will then be transmitted to Model II as the boundary condition. Then, we solve for unII+1(x), x ∈ ΩII , from Model II with the transmitted interface condition on Γ2 and interpolate its solution on Γ1. If the stopping criterion
unI +1 − unI 2L2(ΩI ) + unII+1 − unII 2L2(ΩII ) < , (3)

is satisﬁed, the coupling result is considered to be converged. The solution for the two domains is:

uI (x) := unI +1(x), x ∈ ΩI ,

(4)

uII (x) := unII+1(x), x ∈ ΩII .

(5)

If the solution is not converged, we proceed to update the interface information on Γ1 with a relaxation formulation: h˜n+1(x1) = (1 − θ)hnI +1(x1) + θhnII+1(x1) where hnI,+II1 = R1unI,II (x1) + R2TI0,II (x1) are the Robin boundary condition from Model I and II. Here, the relaxation parameter θ ∈ [0, 1] can be either ﬁxed or updated according to the Aitken’s rule [54, 91]. Then, we proceed to a new iteration by transmitting the updated boundary condition h˜n+1(x1) to Model I and repeating the procedure stated above with n ← n + 1. This procedure is repeated until the stopping criterion is satisﬁed. These coupling algorithms have their origin to the classical Schwarz coupling methods [25, 48, 57] and an iterative patching algorithms [25].

3. Results
In this section, we present the simulation results of four benchmark problems: 2D Poisson equation, 1D heat equation, 2D elastoplasticity, and 2D hyperelasticity, to demonstrate the applicability of our method. The detailed settings of these four examples, including the choices of model I and model II, interface conditions, and whether domains overlap, are provided in Table 1. For interface conditions, we studied four types of conditions, namely, Dirichlet-Dirichlet (D-D), Robin-Dirichlet (R-D), Neumann-Dirichlet (N-D), and Robin-Neumann(R-N). The ﬁrst letter represents the boundary condition for Model I and the second letter for Model II. For each problem, our framework consists of two stages. In the ﬁrst stage, we train a DeepONet oﬄine to obtain a surrogate of model II. Then, we couple Model I with DeepONet in the online
6

Figure 3: A ﬂexible framework in domain decomposition. The proposed coupling framework is able to adopt either (a) overlapping or non-overlapping domain decomposition coupled with Dirichlet, Neumann, or Robin boundary conditions at the interface. As an illustration, (b) presents a Robin-type boundary condition imposed on model I with interfacial solution uˆ(x1) updated by a relaxation scheme. Dirichlet and Neumann boundary can be imposed by adjusting the value of R1 and R2. The relaxation parameter, θ, is either ﬁxed at a value or updated dynamically. TInI (x1) represents the Neumann-related information at x1 from Model II, e.g., traction or derivatives.
Algorithm 1 Coupling Method. Γ1 := ∂ΩI , Γ2 := ∂ΩII . If non-overlapping, Γ1 = Γ2, else Γ1 = Γ2. Initialization: Set model I with u(x1) = 0 and model II with u(x1) = 0 Main Loop: for n = 0 : nmax − 1 do Model I (FEM): • Receive the interface information hn(x1) from Model II (x1 ∈ Γ1). • Solve for unI +1 from Model I. • Calculate unI +1(x2) or ∂u∂nIx+1 |x2 and pass it to Model II (x2 ∈ Γ2). Model II (NN): • Receive the interface information unI +1(x2) or ∂u∂nIx+1 |x2 from Model I (x2 ∈ Γ2). • Solve for unII+1/TInI+1 from Model II. • Calculate hnII+1(x1) = R1unII+1(x1) + R2TInI+1(x1). • Calculate h˜n+1(x1) = (1 − θ)hnII+1(x1) + θhnI +1(x1) and pass it to Model I (x1 ∈ Γ1). If converged, stop; end for
7

Problem 2D Poisson
1D Heat 2D Elastoplasticity 2D Hyperelasticity

Model I FEM FEM
Linear Elastic FEM Hyperelastic FEM

Model II/Training Data DeepONet/FEM DeepONet/FEM
DeepONet/Elastoplastic FEM DeepONet/Hyperelastic SPH

Interface Condition D-D, R-D N-D N-D
N-D, R-D/R-N

Overlap Yes No No No

Table 1: Setup for the four examples. D-D: Dirichlet-Dirichlet, R-D: Robin-Dirichlet, N-D: Neumann-Dirichlet, R-N: Robin-Neumann

Figure 4: Setup of the Poisson equation. (a) A 1 × 1 unit square is decomposed into two overlapping regions, ΩNN (light blue) and ΩF EM (meshed). ΩF EM is bounded by Γ1 and Γ0, whereas ΩNN is a 0.3 × 0.3 square in the center with boundary Γ2. (b) DeepONet takes uF EM |Γ2 as input and yields the solution in ΩNN , denoted as uNN |ΩNN . (c) Formulation of an overlapping D-D method. Given the boundary condition on Γ0, the coupling framework iteratively updates the corresponding boundary condition on Γ1. The relaxation parameter θ is either ﬁxed or updated based on the Aitken’s rule [91].

stage using our proposed coupling method. Training data of elastoplastic FEM in Sec. 3.3 and hyperelasticity in Sec. 3.4 are generated by Abaqus [1] and an SPH solver [26]. All the other usages of FEM (including FEM solver for model I and generation of training data for model II) are based on the FEniCS package [4] using second-order Lagrange polynomials.

3.1. Poisson equation We ﬁrst study the feasibility of the proposed framework for solving a static problem. Let us consider a
Poisson equation described by the PDE system in Ω := ΩF EM ∪ ΩNN :

−∆u(x) = f (x), in Ω = [0, 1]2,

(6)

u(x) = u|Γ0 (x), on Γ0,

(7)

where we set f (x) = 6 for all x ∈ Ω. As shown in Fig. 4(a), we decompose one domain into two overlapping subdomains, namely ΩF EM = [0, 1]2/[0.4, 0.6]2 (with an internal boundary Γ1) and ΩNN = [0.3, 0.7]2 (with

8

a boundary Γ2). In ΩF EM , the system is governed by Eqs. (6) and (7) with an boundary condition

u(x) = u˜|Γ1 (x), on Γ1.

(8)

In this example, we ﬁrst verify the eﬃcacy of the coupling framework with a D-D interface condition. For this setup, in ΩNN , the system is governed by Eqs. (6) and (7) with input

u(x) = uF EM |Γ2 , on Γ2.

(9)

After presenting the results of the D-D case, we display results of parametric studies for better demonstrating the inﬂuence of other factors that may inﬂuence the convergence rate.
In the oﬄine stage, we train a DeepONet as a surrogate of FEM for predicting solution in ΩNN . First, we sample a set of boundary conditions u|Γ2 from a random ﬁeld with α = 5, a parameter in the correlation function in Eq. (D.1). Qualitatively, a smaller α results in a less smooth function. More details of the random ﬁeld generation are provided in Appendix D. Then, we solve the Poisson equation with the randomly sampled boundary conditions using FEM, whose computational results in ΩNN are collected as the training cases. We generate 1,000 cases as the training dataset of DeepONet following the aforementioned process. As shown in Fig. 4(b), the branch network input uF EM |Γ2 comes from an interpolation of the FEM solution on Γ2, while the trunk network takes the coordinate x (∈ ΩNN ) as its input. The network output is uNN (x). We present more details related to the network training in Appendix C.
After the completion of the oﬄine training, we proceed to the coupling stage as shown in Fig. 4(c). Given a ﬁxed boundary condition on Γ0, we initiate the coupling framework with an initial guess of u on Γ1, which is typically zero. Then, we solve the governing equation in ΩF EM with FEM and interpolate the solution on Γ2 (denoted as uF EM |Γ2 ), which will be used as the input of the branch network. With this input, the DeepONet then predicts uNN |ΩNN , the solution of the Poisson equation in ΩNN . Following that, a relaxation scheme is adopted to update the interface condition on Γ1 as: u˜|Γ1 = (1 − θ)uF EM |Γ1 + θuNN |Γ1 , which will later be used as the boundary condition of FEM in the next iteration. The coupling iteration continues until the solution u converges.
We show the performance of our coupling framework for the Poisson equation in Fig. 5. We ﬁx the relaxation parameter as θ = 0.5 and impose a boundary condition on Γ0 from the testing dataset (unseen to the DeepONet in the training stage). Figs. 5(a-b) show a comparison between the FEM ground truth (ﬁrst column; computed directly in Ω) and the predictions from the coupling framework (second column; ﬁrst row in ΩF EM from FEM, second row in ΩNN from DeepONet) together with their diﬀerence (third column). We observe an agreement between the coupling predictions and the ground truth. The maximum absolute error presented in the third column is less than 1%. We also note that the error is larger in ΩF EM , especially in the region close to its external boundary Γ0. The observed error is mostly dominated by the interpolation accuracy in the FEM, not the error of the coupling framework. We further show a quantitative comparison between models prediction and ground-truth on Γ1 and Γ2 in Fig. 5(c): the predictions from

9

Figure 5: Results of coupling FEM and DeepONet for the Poisson equation. (a-b) Model predictions from the FEM and DeepONet with the corresponding absolute errors in ΩF EM and ΩNN . (c) Model predictions at the interfaces (u|Γ1 and u|Γ2 ) and the true solution (red line). The relative errors of model predictions at the interfaces are far less than 1%. Please see Fig. 4 for ΩNN and ΩF EM .
10

Figure 6: Parametric studies of the Poisson equation. (a) Given a boundary condition on Γ0, the convergence history of the coupling model varies with the coupling method (Robin or Dirichlet) and parameter θ. (b) The model is trained with data generated from a random ﬁeld with α = 5. We test the generalization ability represented by the relative errors of the coupling model for cases corresponding to α = 1 to 10. (c) Box plots of relative errors for DeepONets trained with 100 to 900 cases. Each column shows the coupling results by testing with 50 diﬀerent cases. The shaded area indicates that the relative error on Γ1 is less than 1%. Notice that we present the error with respect to the ground truth to show the convergence and accuracy of our framework.
coupled FEM/NN (blue dashed lines and green triangles, respectively) accurately reproduce the true solution (red lines) with a relative error at around 0.07%.
In addition to the results shown in Fig. 5, we also conducted parametric studies on diverse factors that inﬂuence the performance of our framework, including: convergence of the framework with diﬀerent coupling methods (Fig. 6(a)); the coupling accuracy inﬂuenced by the extrapolation capability of the DeepONet (Fig. 6(b)) and the number of training cases (Fig. 6(c)). In Fig. 6(a), convergence of errors with diﬀerent boundary conditions is plotted against iterations. The shaded area indicates that the relative error is less than 1%. Speciﬁcally, given various values of the relaxation parameter θ (θ = 0.25, 0.5 and 0.75), the displacement errors with Dirichlet boundary conditions satisfy the stopping criterion (L2 error less than 2 × 10−3, or equivalently, relative error less than 1%) at iteration 37, 18, and 12, respectively. With the Aitken’s relaxation strategy for θ (see, e.g., [91]), the coupling error (denoted as “Dirichlet-Aitken”) converges a bit faster than that of a ﬁxed relaxation parameter θ = 0.75. We also adopt a Robin-type boundary (RobinAitken) with dynamic update in θ (purple line). In this case of Robin boundary condition, the system in domain ΩF EM is governed by Eqs. (6) and (7) with a Robin boundary:
∂u R1u + R2 ∂n = g, on Γ1, (10) where we set R1 = 1 and R2 = 1. Since the FEM needs both the information of the solution and its derivative in the normal direction from DeepONet to update the Robin boundary condition, we adjust the
11

training loss of the network following the strategy. The regularization term Rˆ in Eq. (2) is set as:

Rˆ = ∂∂nu − ∂∂nu 2 , x ∈ Γ1 (11)

NN

true

to regularize the partial derivative with respect to the normal direction of Γ1. In Eq. (2), the partial derivative term is computed by taking the automatic diﬀerentiation of the network output with respect to the trunk

net input x [27, 79, 88, 92, 93]. With the employment of the Robin boundary condition, we observe that the

method only takes two iterations to reach a relatively small error.

In Fig. 6(b), we show the generalization ability of DeepONet and its impact on the accuracy of the

coupling framework by testing with boundary conditions outside the training region. The minimal relative

errors of u|Γ2 are plotted against the correlation length α of the random ﬁeld that was used to generate training data. When the correlation length α increases, the sampled curves become smoother and vice versa.

Notice that we train the network on training samples with α = 5 and test its performance on α ranging

from 1 to 10, each with 50 testing cases. The relative errors drop from around 10% to less than 1% with α

increasing from 1 to 5. For α > 5, the errors are statistically stable.

Fig. 6(c) exhibits the accuracy of the coupling framework as a function of the generalization ability of

DeepONet, which is reﬂected by the number of training cases for DeepONet. The median of the relative

errors of u|Γ1 decreases slightly when the number of training cases increases from 100 to 300 and then stays statistically stable even with further increase. The computational results implicitly suggest that after

300 training cases, the errors are most contributed by the accuracy of boundary interpolation, not the

generalization of DeepONet. The shaded area in (b-c) denotes relative error at 1%.

3.2. Heat equation

Next, we investigate the performance of the coupling framework for a dynamic problem, namely, 1D heat

equation, with a N-D method. Consider a PDE system in Ω := ΩF EM ∪ ΩNN :

∂u

∂2u

∂t = K ∂x2 , for (x, t) ∈ Ω := [0, 1.0] × [0, 10.0], (12)

u(x, 0) = u0(x), for x ∈ [0, 1.0],

(13)

ux(x, t) = 0, for (x, t) ∈ Γ0 := {0, 1.0} × [0, 10.0],

(14)

where the thermal diﬀusivity K is set as 0.1. As shown in Fig. 7(a), we decompose the computational domain Ω into two non-overlapping subdomains: ΩF EM := (x, t) ∈ [0, 0.5] × [0, 10.0] for FEM and ΩNN := (x, t) ∈ [0.5, 1] × [0, 10.0] for DeepONet. The interface is Γ := {0.5} × [0, 10]. For the FEM subdomain ΩF EM , we set a Dirichlet boundary condition at the interface Γ:

u(x, t) = u˜(x, t) for (x, t) ∈ Γ,

(15)

where u˜ is the updated and relaxed solution at the interface. For the DeepONet subdomain ΩNN , we impose the interfacial ﬂux from FEM K ∂u∂FxEM as input.

12

Figure 7: Setup of the time-dependent problem (heat equation). (a) The spatio-temporal domain is decomposed into two sub-domains with Γ as the interface between FEM and NN domain. (b-c) Formulation of the coupling framework for the heat equation. The framework ﬁrst solves for unF+E1M,k+1 from FEM with updated information at the interface u˜ at k-th step. Then, the computed derivative at Γ is transmitted into DeepONet, which predicts the spatial derivative in ΩNN (c). The solution in ΩNN is computed based on a time-stepping scheme, followed by a relaxation update (b). The initialization at k = 0 and n = 0 is described in the main text.

The coupling method is illustrated in Fig. 7(b-c). The solution process is a nested loop with indices

n and k, where k represents the time step and n is the current iteration step. To initiate the framework,

we set both n and k as zero with an initial guess of interfacial ﬂux for the FEM. The FEM solves for

the solution (unF+E1M,k+1|ΩFEM ) in ΩF EM at time step k + 1 given the solution at the previous time step k, denoted as u∞ F E,kM |ΩFEM . Then, we transmit the ﬂux at Γ, K ∂unF+∂Ex1M,k+1 |Γ, to DeepONet as a part of

the trunk net input (Fig. 7(c)). The branch network takes the system solution in ΩNN at k-th time step as input. The output of DeepONet estimates an approximation of the diﬀusion term K ∂2uN n∂+xN12,k+1 . The

solution in ΩNN is then calculated by a semi-discretized heat equation with the backward Euler method: unN+N1,k+1(x) = unN,Nk (x) + ∆t · K ∂2uN n∂+xN12,k+1 . Next, the interfacial ﬂux u˜n+1,k+1 is updated by the relaxation

scheme:

u˜n+1,k+1 = (1 − θ)un+1,k+1 + θun+1,k+1

F EM Γ1

NN

Γ1

(16)

In this example, we ﬁx the relaxation parameter θ = 0.5. If the updated interfacial ﬂux is not yet converged,

we continue to the next iteration. Otherwise, if the ﬂux is converged, then we proceed to the next time step

in the outer loop and restart the inner loop with the reset iteration step n = 0.

Herein, we summarize the training procedure of DeepONet. We generate 1,000 initial conditions in

x ∈ [0.5, 1.0] from a Gaussian random ﬁeld with constant mean 0 and correlation length 0.3. For each case,

we randomly sample the ﬂux at u|Γ1 20 times based on a uniform distribution from -3 to 3 as the boundary

conditions. The training data of DeepONet is then generated by running FEM simulations on ΩNN with

the sampled initial/boundary conditions. Note that training a DeepONet in a spatio-temporal domain with

13

Figure 8: Results of coupling FEM and DeepONet for the heat equation. (a) Model predictions from DeepONet and FEM. DeepONet predicts the solution in x ∈ ΩNN while FEM predicts the solution for x ∈ ΩF EM . The black line at x = 0.5 denotes the interface of the two domains. The solution at x = 0.75 (indicated by the white dashed line) is presented in (b). (c) The mean square errors of FEM (red) and DeepONet (blue) vs time (t).
disparate boundary/initial condition is a data-demanding task: one needs to sample in the spatio-temporal domain. Also, performance of the framework may be deteriorated when the network extrapolates the solution outside the training domain. Hence, we alleviate the challenges by training the network to learn the implicit spatial derivative operator K ∂∂2xu2 with an explicit method to advance in time. In practice, we acquire the spatial derivative K ∂∂2xu2 in ΩNN from FEM simulations running from from t = 0 to 1.0 with dt = 0.1. The simulated spatial derivative is then collected and utilized for DeepONet training. For more details of the network training, we refer the reader to Sec. Appendix C.
The coupling results are shown in Fig. 8. Fig. 8(a) shows the prediction from both models in the spatiotemporal domain Ω. In Fig. 8(b), the solution of DeepONet at x = 0.75 is plotted against time. The prediction from DeepONet shows a good agreement with the ground truth solution even for the temporal region outside the training dataset (t > 1), indicating that the proposed framework works well for extrapolation. The mean-squared errors (MSE) of both models in the coupling framework are plotted against time in Fig. 8(c): the coupling framework shows stability and accuracy over long-time integration. Although the prediction errors grow with the increase of t, we note that the errors accumulate at a relatively low rate, demonstrating that the proposed coupling framework is capable of solving a time-dependent system.
14

Figure 9: Setup of the elastoplasticity problem. (a) A 2l × 2l solid plate is clamped on the bottom edge with a traction t0 distributed on the top edge. (b) The yellow region (ΩF EM ) is modeled with linear elasticity, whereas the blue region (ΩNN ) is modeled with elastoplasticity. Interfacial displacement (Dirichlet boundary condition) u|Γ computed from FEM is transmitted to DeepONet as input. (c) The DeepONet estimates the stress components in ΩNN , σNN |ΩNN , based on which the interfacial traction tNN |Γ can be calculated accordingly. The predicted traction is updated and provided to FEM as a Neumann-type boundary condition. In this example, we set the interior circle radius ri = 0.1, the interface circle radius ro = 0.3, and the plate size l = 1.

3.3. Elastoplasticity

In this section, we test the proposed framework for predicting the elastoplastic behavior of a solid material. As shown in Fig. 9(a), we consider a plane strain problem for a square-shaped solid of size 2l × 2l with a circular void of radius ri, where vertical tension is applied on its top edge. In this example, we take l = 1, ri = 0.1. For linear elastic materials, the kinematics, constitutive relation, and equilibrium equations are as follows:

ε

=

1 (∇u

+

∇uT),

(17)

2

σ = λtr(ε)I + 2µε,

(18)

0 = ∇ · σ + b,

(19)

where ε, u, and σ are the strain, displacement, and (Cauchy) stress; λ and µ are Lam´e moduli, which we

take as λ = 0.5769 and µ = 0.3846, and I is the identity tensor. In this example, we consider a solid subject

to no body load, and therefore set the body force term b as zero.

To model the plastic behavior of the material, we consider small-deformation, rate-independent elasto-

plasticity with isotropic hardening. The additive decomposition of the strain tensor writes ε = εe + εp,

where εe and εp are the elastic and plastic strains, respectively. The elastic strain εe is related to the stress

by

σ = λtr(εe)I + 2µεe.

(20)

The plastic strain εp is purely deviatoric (i.e., tr(εp) = 0). We deﬁne the deviatoric stress s, the increment

15

of the equivalent plastic strain dε¯p, and the equivalent tensile stress (Mises stress) σ¯ as

1

s = σ − tr(σ)I,

(21)

3

dε¯p = 2 dεp : dεp,

(22)

3

3

σ¯ = s : s,

(23)

2

respectively. The ﬂow direction N p and the increment of the plastic strain dεp are given by

Np =

3s ,

(24)

2 σ¯

dεp = 3 dε¯pN p.

(25)

2

Then, the yield function f can be deﬁned as

f = σ¯ − Y (ε¯p),

(26)

where the linear strain-hardening function Y is taken as

Y (ε¯p) = Y0 + H0ε¯p,

(27)

The initial strength Y0 = 0.1 and the hardening modulus H0 = 0.3 are taken as two constant material

parameters. The aforementioned mechanical quantities are subject to the Kuhn-Tucker complementary

conditions:

f ≤ 0, dε¯p ≥ 0, (dε¯p)f = 0.

(28)

In addition, when f = 0, the consistency condition dε¯pdf = 0 also needs to be satisﬁed. Due to the setup of our boundary value problem, the plastic deformation concentrates around the void
whereas the material is dominated by elastic behaviors in regions away from the void. Hence, we decompose the square domain into two non-overlapping subdomains (see Fig. 9(a-b)): the internal region ΩNN , which is an annulus with interal radius ri = 0.1 and external radius ro = 0.3 (on Γ), modeled by DeepONet as a surrogate for the solid’s elastoplastic response; the external region ΩF EM , modeled by FEM for linear elasticity. The two regions share a common interface on Γ. In ΩNN , we train several DeepONets as surrogates of each stress components to capture the plastic behavior. First, we sample 1,000 displacement boundary conditions on the top edge using the sampling method described in Appendix D.2, and employ the sampled data as boundary conditions. Then, we solve for the displacement and stress ﬁelds in the entire domain with a FEM solver based on the elastoplasticity model described above. Next, we collect the simulation results in ΩNN , which will be employed as the training data of DeepONets. As depicted in Fig. 9(c), DeepONet can predict the corresponding Cauchy stresses in ΩNN with input as the displacement at the interface. In 2D problems, the Cauchy stress σ(x) for each material point is a 2 × 2 symmetric matrix. Therefore, to model the stress we only need to predict its three components, namely, σ11, σ12, and σ22. For each of these

16

components, we train an independent DeepONet separately as a surrogate of these quantities. The traction at the interface t|Γ is calculated accordingly based on the predicted stress from the DeepONets. More details regarding the training of this network are presented in Appendix C.
Then, we employ the trained DeepONets in the coupling framework. As depicted in Fig. 9, DeepONets and FEM communicate at the interface Γ by transmitting the information of displacement and traction (Fig. 9). The interfacial displacement, uF EM |Γ, is computed in FEM and transmitted to the DeepONets as the input of the branch network. Then, the network solves for the corresponding stress in ΩNN and calculates the traction at the interface (tNN |Γ). In the next iteration, the computed traction tNN |Γ will be imposed as the boundary condition of the FEM model. In this example, we employ a relaxation scheme for the traction from the DeepONets. The relaxation parameter θ is ﬁxed at 0.5.
We present the coupling results with a N-D interface condition in Fig. 10. In Figs. 10(a-b), we plot the ground truth solution from FEM in the ﬁrst column, the predictions from our coupling framework in the second column, together with their diﬀerences in the third column. In plot (a), we show the results of the normal stress (σ22) in the vertical direction of ΩF EM . The results of the equivalent plastic strain, ε¯p in ΩNN are provided in plot (b). Although the coupling framework has generally well reproduced the stress component σ22 in the bulk region of ΩF EM , there exist relatively large errors at the top edge and the bottom corners. These errors either originate from numerical interpolation in the FEM solver or are caused by a reduced solution regularity. Apart from these regions, the errors are controlled at a low value with the maximum relative error lower than 10%. In ΩNN , the proﬁle of ε¯p is well captured by the coupling scheme with the maximum relative error of ε¯p less than 10%. The results demonstrate that plasticity in the region of interest is well predicted by the surrogate model. To provide a further quantitative veriﬁcation of our coupling framework, we plot the ground-truth, the predictions of displacement, and traction components on Γ in Fig. 10(c) as functions of θ, the angle in polar coordinate. The solid lines denote the ground-truth results from the nonlinear FEM. The dashed lines denote the predicted values of the framework. The model predictions match the ground truth well, with relative errors smaller than 2%. Fig. 10(d) shows the eﬃciency of the coupling framework: the L2 errors between model predictions and the ground truth reach a plateau at the fourth iteration.
3.4. Hyperelasticity In the previous examples, DeepONet was trained based on data generated from FEM and was coupled
with another FEM in the online stage. These examples illustrated the capability of our coupling framework in solving both static and dynamic problems. In this section, we demonstrate the capability of our framework in concurrently coupling a continuum model (FEM) with a surrogate from smooth particle dynamics (SPH) for describing a microscopic particle system. We consider using the coupled framework to predict the mechanics of a hyperelastic material. We ﬁrst derive the energy minimization formulation of the continuum model. We denote by ψ the strain energy density of the hyperelastic model and seek to ﬁnd a displacement ﬁeld
17

Figure 10: Results of coupling FEM and DeepONet in elastoplaticity. (a-b) Results of the normal stress in y direction (σ22) and equivalent plastic strain (ε¯p). From left to right: True value, FEM/DeepONet predicted value from coupling, and their absolute errors. (c) Displacement and traction at the interface Γ. True values and predicted values from coupling are presented. Solid lines: displacement and traction of true data in x (blue) and y (red) directions. Dashed lines: displacement and traction of the model predictions in x (yellow) and y (black) directions. (d) The history of the relative errors of the coupling model at the interface. εFEM and εNN refer to the L2 error of displacement and traction from FEM and DeepONet, respectively. Notice that we present the error with respect to the ground truth to show the convergence and accuracy of our framework.
18

Parameter µ

K

k1 k2 α

Value 0.3846 0.8333 0.1 1.5 π/2

Table 2: Parameters value of the HGO model. We set the value of k1, k2, and α the same for i = 1 and 2.

u : Ω → R2 that minimizes the total potential energy Ψ:

Ψ = ψ(u)dx − b · udx − T0 · uds.

(29)

Ω

Ω

ΓN

Here, b denotes the body force in Ω, T is the traction load applied on the Neumann boundary ΓN . Hence, the total potential energy Ψ is the integration of strain energy density, ψ, over the entire domain Ω, deduced by the energy contributions from the body force b, the traction T.
We consider the Holzapfel-Gasser-Odgen (HGO) model [31] to describe the constitutive behavior of the material in this example. Essentially, the material is hyperelastic, anisotropic, ﬁber-reinforced in diverse directions. Its strain energy density is:

ψ = µ (I1 − 3) − µ ln(J ) + k1 2 (exp (k2 Ei 2) − 1) + K ( J 2 − 1 − ln J ), (30)

2 2k2 i=1

22

where · denotes the Macaulay bracket. In this model, the ﬁber strain of the two ﬁber groups is expressed

as:

Ei = κ(I1 − 3) + (1 − 3κ)(I4i − 1), i = 1, 2,

(31)

where k1 and k2 are ﬁber modulus and the exponential coeﬃcient, respectively, I1 is the ﬁrst principal invariant, and I4i is the fourth principal invariants corresponding to the i−th ﬁber group. Mathematically, for the i−th ﬁber group with angle direction αi from the reference direction, I4i is calculated by nTi Cni, where C is the right Cauchy-Green tensor and ni = [cosαi, sinαi]T. In our simulations, we consider a material with ﬁber reinforcement in the vertical direction (see Fig. 11 right for illustration). Therefore, for both ﬁber groups we set αi = π/2. In Eq. (31), ﬁber dispersion is denoted as κ, whose value ranges from 0 to 31 . Intuitively, κ = 0 means no ﬁber dispersion whereas κ = 13 represents an isotropic ﬁber dispersion. In this example, we consider the ﬁber oriented vertically with no dispersion (κ = 0). All parameter values in this example are summarized in Table 2.
We now present the problem set up of this example. As depicted in Fig. 11(a), we consider a 2D unit square plate with a centered circular void (radius as 0.1). The plate deforms under a uniaxial tension, T0(x), applied on its top edge. The bottom edge is clamped. We model the material response of the entire domain using an SPH model, whose solution is taken as the ground-truth solution. More details of the SPH model are provided in Appendix A. To develop a coupling model, we consider a similar setting as in the previous example and decompose the entire domain Ω into two non-overlapping subdomains. Then, we train DeepONets using SPH data to obtain a surrogate for the internal domain while the external region is described by a continuum FEM model. Models in these two domains communicate by exchanging proper

19

Figure 11: Setup of the hyperelasticity problem. (a) A unit square is decomposed into ΩNN and ΩF EM . We impose a non-uniform traction boundary condition on the top edge and ﬁx the displacement at the bottom and train multiple DeepONets to represent the mechanics of an SPH model. The material is reinforced by ﬁbers in the vertical direction. Information at the interface (displacement u and ﬁrst Piola-Kirchhoﬀ stress P) is transmitted between DeepONet and FEM. (b) Traction and displacement in diﬀerent directions are exchanged at the interface Γ. FEM predicts the external domain while the DeepONet is trained based on SPH data. (c) Two types of DeepONet are proposed to predict the mechanics of the system: predicting stresses based on displacement information (green boxes) and vice versa (yellow boxes).
20

interface conditions on their common interface, Γ. In Fig. 11(b) we present a schematic of information transmission of the coupling framework with a N-D method. At the n-th interation, the FEM solver receives an updated distributed traction on Γ from DeepONets and solves the updated displacement ﬁeld, unF EM , with the given information. Then, the FEM transmits the updated displacement information unF EM |Γ to DeepONets. With the displacement on Γ as input, DeepONets estimate the ﬁrst Piola-Kirchhoﬀ (PK1) stresses in ΩNN . Based on the predicted PK1 stresses, we then calculate the surface traction on Γ and other associated quantities, such as the equivalent plastic strain ε¯p and von Mises stress σ¯, accordingly. At the n-th iteration, the system solution at the interface Γ is updated as T˜ (x) = (1 − θ)TnI (x) + θTnII (x).
Next, we brieﬂy describe the training process of DeepONet. To generate the training/testing dataset, we sample 1, 000 diﬀerent traction loading T0(x) on the top edge from a random ﬁeld (see Algorithm in Sec. Appendix D). Then, for each sampled traction loading, we perform an SPH simulation to obtain the solutions in the entire domain and collect the corresponding solutions of displacement and PK1 stress ﬁelds of in ΩNN . Among these 1000 samples, 900 cases are employed as the training data while the rest is kept as testing data. As depicted in Fig. 11(c), we consider two approaches for network training. In the ﬁrst approach, the network (N etu2s) takes the displacement at the interface (u|Γ) as input and predicts PK1 stress as the output. In the second approach, the interfacial traction T |Γ is employed as the input of the network (N ets2u), yielding the displacement ﬁeld as output. These approaches provide a ﬂexibility of imposing diﬀerent interface conditions. We adopt the ﬁrst approach in the N-D method and combine the information of the two approaches (uˆ and Tˆ ) in the R-D/R-N method.
Fig. 12 shows the coupling results of a typical testing case with a N-D method. In Fig. 12(a-b), we compare the vertical displacement u2 in ΩF EM (ﬁrst row) and the PK1 stress component P22 in ΩNN (second row) between the SPH ground truth (ﬁrst column) and the FEM/NN prediction (second column). The prediction errors are displayed in the third column. We observe that the coupling results match well with the ground truth solution with the largest prediction errors distributed near the bottom corners. The errors are partially induced by the numerical interpolations in the FEM and reduced regularity in that region. To further examine the prediction accuracy, in Fig. 12(c) we plot a quantitative comparison of displacement and traction on the interface Γ as functions of the polar coordinate angle θ. Despite some numerical discrepancies between SPH and FEM (see Appendix B.1), the FEM/DeepONet predictions shown in dashed lines successfully reproduce the SPH simulation results depicted in solid lines. Therefore, our proposed method is capable of capturing the mechanics from SPH with a substantially improved eﬃciency: the time cost of performing an SPH simulation is approximately 4 hours whereas running its surrogate just takes a fraction of a second (Table 3). Admittedly, the excessive cost of SPH is exacerbated because we use a time-dependent SPH solver to simulate a static problem. Nonetheless, we can see that replacing a particle model with its surrogate in a multiscale coupling framework poses unique advantages in both eﬃciency and programming easiness.
To further illustrate the ﬂexibility and investigate the convergence rate of our coupling framework, we employ a R-D/R-N coupling method to our framework with a variety of values of the Robin coeﬃcient R. To
21

Figure 12: Results of coupling FEM and DeepONet in the hyperelasticity problem. Results of (a) displacement in ΩF EM and (b) P22 in ΩNN . From left to right: prediction from SPH, prediction from FEM, and their absolute diﬀerences. (c) Predicted displacement and traction at the interface. Predictions from both FEM and DeepONets are compared with the true data. Solid lines: displacement and traction of true data in x (blue) and y (red) directions. Dashed lines: displacement and traction of the model predictions in x (yellow) and y (black) directions.

Model FEM-SPH FEM-DeepONet

Wall Time ∼ 4h < 1s

Table 3: Wall time comparison for SPH and DeepONet per iteration in the hyperelastic problem. The excessive cost of SPH is exacerbated because we use a time-dependent SPH solver to simulate a static problem.

22

Figure 13: L2 errors of the interfacial displacement in the hyperelastic multiscale model with a Robin boundary. The red line with Robin coeﬃcient R=0.25 converges fastest among the testing cases. The black solid line represents the relative error history of a Neumann boundary condition (R=0). The shaded area indicates that the relative error is less than 2%. Notice that we present the error with respect to the ground truth to show the convergence and accuracy of our framework.

clarify, R-D/R-N means that the FEM, imposed with a Robin-type boundary condition, separately transmits the information of displacement and traction to N etu2s and N ets2u in order to update the Robin information in the next step. The continuum model with a Robin boundary condition is modiﬁed as:
1 Ψ = Ω ψ(u)dx − Ω b · udx − ΓN T0 · uds − ΓR r − 2 Ru · uds. (32) r is the Robin boundary condition applied on the interface ΓR. We deﬁne r as:

r = Tˆ + R · uˆ, for x ∈ ΓR,

(33)

where R is the (positive) Robin coeﬃcient, Tˆ and uˆ are the known traction and displacement. The coupling procedure is changed as well. At the n-th iteration, we ﬁrst solve the FEM model with a
Robin boundary condition ˜rn(x). Then, we transmit the interfacial traction and displacement of FEM to N etu2s and N ets2u, respectively. The corresponding displacement uˆn+1 and tractions Tˆ n+1 are estimated by DeepONets and used to update the Robin boundary condition: ˜rn+1(x) = (1 − θ)rnF+E1M (x) + θrnN+N1(x). Here, θ = 0.5 is taken as a ﬁxed relaxation parameter.
We further study the convergence of interfacial displacement errors with diﬀerent values of R. In Fig. 13, we highlight the results from the N-D coupling method with the black line and the results of R-D/R-N with R = 0.25 (best result) in red. The shaded area indicates 2% equivalent relative error of the test case. Due to the nonlinearity of the problem, it generally takes more iterations for the coupling framework to reach the stopping criterion. When taking a R-N coupling method with a sub-optimal Robin coeﬃcient (such as

23

R = 5.0 as shown in the purple line of Fig. 13), the coupling framework fails to converge. This fact again demonstrates the importance of choosing an appropriate coupling method.
4. Discussion
In this paper, we propose an eﬃcient concurrent coupling framework for multiscale modeling of mechanics problems. In lieu of coupling an expensive microscopic model, we propose to employ a deep neural operator, DeepONet, as a surrogate to approximate the microscopic solution in the domain with ﬁne-scale features. The response in the coarse-scale domain is simulated by a standard numerical model, such as the ﬁnite element method. The two models are coupled concurrently by exchanging information at the interface until convergence. To verify the performance of this framework, we study four benchmarks including solving static and dynamic problems for diﬀerent materials. We have also demonstrated that the framework is readily applicable for various interface conditions. The results show that the cases with a Robin boundary condition tend to converge faster than a Neumann/Dirichlet boundary condition. Moreover, the predictions of the coupled model agree well with the true solution acquired from a numerical method, indicating the generalization ability of DeepONet and the accuracy of the proposed framework.
In addition, using a neural operator as surrogate enables the model to be trained directly from data. Such a model is particularly promising for learning dynamics of complex materials without explicit constitutive models. Moreover, coupling DeepONet with FEM substantially improves the computational eﬃciency in multiscale modeling: DeepONet predicts the expensive microscopic behavior only at a fraction of second. Hence, the overall computational cost of the proposed framework could be shorten by orders of magnitude than the existing multiscale coupling methods. In addition, we can train the surrogate model to learn based on partial information: it can predict the information only at the interface and neglect dynamics inside the microscopic region. Thus, the overall framework behaves as an artiﬁcial-intelligence type boundary condition, which would potentially improve the eﬃciency especially in the scenarios where only the macroscopic dynamics is of interest.
As a further note, we would like to point out the keys to build a successful neural operator based coupling framework with guaranteed convergence and to avoid possible pitfalls. First, the generalization ability of DeepONet determines if the result converges and its convergence rate. Utilizing an accurate neural operator model takes fewer iterations to converge. However, a poorly trained model could lead to slow convergence or even divergence. Second, normalizing data is another key to convergence. In our experiments, the training data are at disparate scales, ranging from 10−3 − 101. Properly normalization of the training data would be essential to the convergence of numerical iterations. In addition, choosing proper coupling strategies, such as the right Robin coeﬃcient, also plays a critical role in guaranteeing fast numerical convergence as it aﬀects the condition number of the stiﬀness matrix in FEM and the coupling system [19]. A large condition number may lead to a slow convergence or even divergent result in the coupling framework (see Sec. 3.4 and [21, 29]).
Certainly, more improvements and directions can be considered in the future. Learning directly from
24

data facilitates a unique feature that DeepONet can learn data that comes from diﬀerent scales, which has been demonstrated in Sec. 3.4. In the future, it would be interesting to develop a data-driven model from noisy data, such as molecular dynamics and dissipative particle dynamics, as presented in Fig. 1. In addition, overcoming the multiscale characteristic length and time scale would be another challenge in multiscale modeling with machine learning, that is, the time and length of a microscopic model are usually orders of magnitude smaller than the continuum model, causing challenges in network training and long-term predictions. The coupling results may drift away from the true solution or even diverge due to inaccurate predictions from surrogate models. Another improvement would be considering microstructures and geometric variations (see [52]). Developing an operator-learning neural network that can predict dynamics with diﬀerent geometric variations would be a great improvement for broadening the application of the proposed method. Also, simulating fracture progression is another natural and promising application of the multiscale coupling framework. As fracture progresses, the ﬁeld of interest that includes the damage region may also move along with the tip of a crack, posing a challenge in both coupling algorithm and network training. Our framework can be further extended to employ other coupling methods such as quasicontinuum [73], multigrid [11], heterogeneous multiscale method (HMM) [83], etc.

Acknowledgment
MY, EZ, and GEK acknowledge the support by grant U01 HL142518 from the National Institutes of Health. Y. Yu would like to acknowledge support by the National Science Foundation under award DMS 1753031.

Appendix A. Smoothed Particle Hydrodynamics

In this section, we brieﬂy introduce the basic formulation of the Total Lagrangian Smoothed Particle Hydrodynamics (TLSPH). We refer the reader to [26, 70] for more details. In the SPH framework, physical quantities are approximated with the neighboring information in a kernel. Consider a function f (X) at Xi can be approximated by gˆ(Xi) with the integration

gˆ(Xi) = f (X)W (X − Xi)dX,

(A.1)

where W (X) is a weighting kernel which is chosen as a third-order polynomial

 (h − Rj)3, W (Rj, h) = A 0,

Rj < h, Rj ≤ h,

(A.2)

h denotes the radius of the integration kernel; Rj is deﬁned as the distance between Xj and the reference point in the kernel Xi with A = 10/πh5 in two dimensions or A = 10/πh6 in three dimensions. SPH numerically approximates the integration as

g(Xi) = fjVjW (Rj, h),
j∈S

(A.3)

25

where Vj is the volume of particle j. The gradient of g(Xi) with respect to its reference coordinate is

∇X g(Xi) = fjVj∇X W (Rj, h)
j∈S

(A.4)

with

∇X W (Rj, h) = ( ∂W (Rj, h) ) Rj ∂Rj Rj

(A.5)

For a vector f (X), its gradient to the referential coordinate is approximated as the following using the

integration rule:

∇X g(Xi) = fj ⊗ Vj∇X W (Rj, h)
j∈S

(A.6)

We then introduce two ad-hoc corrections for keeping symmetrization and ﬁrst-order completeness [55].

∇X g(Xi) = (fj − fi)Vj∇X W (Rj, h).
j∈S

(A.7)

Another correction guarantees the ﬁrst-order completeness [68], the corrected gradient tensor is deﬁned as

∇˜ X W (Rj, h) = A−1∇X W (Rj, h)

(A.8)

where the shape tensor A is

Ai = Vj∇X W (Rj, h) ⊗ Rj
j∈S

(A.9)

We introduce the SPH integration to the governing equation of solid at the continuum level. The equi-

librium equation is:

∇ · P + ρ0b = ρ0x¨

(A.10)

where P is the ﬁrst Piola-Kirchhoﬀ stress tensor, b the body force vector, ρ0 the referential mass density,

and x¨ the acceleration. We deﬁne the deformation gradient as

Fi = ∂xi = rj ⊗ Vj∇˜ X W (Rj, h) ∂Xi j∈S

(A.11)

According to basic continuum mechanics law,

∂W Pi = FiSi = 2Fi ∂C

(A.12)

As proved in [10], the internal forces emerge from the divergence of the ﬁrst Piola-Kirchhoﬀ stress P :

mix¨i = f int + f ext,

(A.13)

where the internal forces is expressed as

fiint = ViVj(Pi∇˜ X W (Ri, h) − Pj∇˜ X W (Ri, h)).
j∈S

We adopt another correction for suppressing spurious hourglassing mode [26]

fihg =

EViVjW (Rj, h)

rj

−α 2Rj2 (δi + δj ) rj

j∈S

(A.14) (A.15)

26

Figure B.14: Comparison of SPH and FEM. (a) We impose the same Dirichlet boundary condition on the top and internal circle for a FEM and SPH model. (b-c) Interfacial displacement and traction of both models.

The deformation derivative in time is approximated as

F˙ = 1 (F t+1 − F t) ∆t

We adopt the strain energy function W(C, M) for a ﬁber-enhanced tissue proposed in [31]

W(C, M ) = c (I1 − 3) − c ln(J ) + k1 2 (exp (k2 Ei 2) − 1) + K0 ( J 2 − 1 − ln J ),

2 2k2 i=1

22

where principal invariants, I1 and I4 are deﬁned as,

(A.16) (A.17)

I1 = C : I, J = det F, I4 = C : M ⊗ M

(A.18)

The parameters are set the same as the FEM model in Sec. 3.4. We refer the reader to [2, 70] for more details.

Appendix B. Accuracy of FEM and SPH
Appendix B.1. Accuracy of SPH To compare the numerical solution of the FEM and SPH model, We present their computational results
for a same benchmark problem. The problem is set up as shown in Fig. B.14(a). We impose a distributed traction boundary condition on the top edge and impose a displacement boundary condition (Dirichlet-type) at the interface for both models shown in Fig. B.14(b): the displacement in x and y at the interface are plotted against the angle θ. The corresponding tractions at the interface are presented in Fig. B.14(c). The solid lines are the FEM tractions in x and y directions with the dashed lines denoting the SPH results. The results correspond well in general with some deviation around θ = π/2 and θ = 3π/2 due to the diﬀerence in numerical integration methods. This discrepancy partially contributes to the overall errors observed in Sec. 3.4.
27

Figure C.15: Network size for various problems. We tabulate the branch/trunk network size for the four examples in Sec. 3 with the number of training epochs.

Appendix C. Network Training Details
We explain the details of network training in this section. Network architecture for each problem is presented in Fig. C.15. We adopt a fully-connected neural network (FNN) with four layers for all the subnetworks. The input layer dimension of the branch net depends on the number of points at the interface, which ranges from 51 to 120 in our examples. Three hidden layers are concatenated with the input layer where we adopt the hyperbolic tangent function as the activate function. Each hidden layer has 100 or 150 neurons shown in Fig. C.15. To minimize the loss function for each case, we set the training epoch as 100,000 steps for the heat and Poisson example and 1,000,000 for the other two examples. Moreover, we illustrate the error history of a few cases in Fig. C.16 where each cases shows a relatively small diﬀerence between the training and testing errors. The low testing errors indicating a good generalization capability of the network. More speciﬁcally, we tabulate the input/output of each network in Fig. C.17 with MSEs of the training and testing data for each case.

Appendix D. Data Generation

Appendix D.1. Random ﬁeld generation We provide an overview of the algorithms of random ﬁelds generation using Fast Fourier Transformation
(FFT). Let W (x) be a Gaussian white noise random ﬁeld on Rd. A random ﬁeld φ(x) can be sampled by

φ(x) = F −1(γ1/2F (W ))(x)

(D.1)

where F and F −1 denote Fourier transformation and its inverse. γ represents a correlation function k −α where k is the L2 norm of the wave number k ∈ Rd. In Sec. 3.1, 3.2, and 3.4, we adopt this method for generating the training data with α = 5. We refer the reader to ﬁnd more theoretical details in [40]

Appendix D.2. Sampling in elastoplasticity

In the case of elastoplasticity, the non-uniform tension t0(x) (see Fig. 9) is generated by

3 Ai

3 Bi

t0(x) =

cos (ix) + i

sin (ix), i

i=1

i=1

28

(D.2)

Figure C.16: History of Training/testing errors of DeepONet for diﬀerent problems.
29

Figure C.17: Details of network setup (inputs/outputs and MSE errors of training/testing) for each example in Sec. 3.
where Ai, Bi ∼ N (0, 0.052) (i ∈ {1, 2, 3}) are independent normal random variables. We choose the standard deviation as 0.05 to make sure that the dataset contains similar numbers of cases with and without plastic deformation.
References [1] Abaqus. Abaqus 2020 documentation. Dassault Syst`emes, 2020. [2] Ahmadzadeh, H., M. Rausch, and J. D. Humphrey. Modeling lamellar disruption within the aortic wall
using a particle-based approach. Scientiﬁc Reports 9:1–17, 2019. [3] Alber, M., A. B. Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. E. Karniadakis,
W. W. Lytton, P. Perdikaris, L. Petzold et al. Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences. NPJ Digital Medicine 2:1–11, 2019. [4] Alnæs, M., J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes, and G. N. Wells. The fenics project version 1.5. Archive of Numerical Software 3, 2015. [5] Arbabi, H., J. E. Bunder, G. Samaey, A. J. Roberts, and I. G. Kevrekidis. Linking machine learning with multiscale numerics: data-driven discovery of homogenized equations. JOM 72:4444–4457, 2020.
30

[6] Bazilevs, Y., V. Calo, J. Cottrell, T. Hughes, A. Reali, and G. Scovazzi. Variational multiscale residualbased turbulence modeling for large eddy simulation of incompressible ﬂows. Computer Methods in Applied Mechanics and Engineering 197:173–201, 2007.
[7] Bensoussan, A., J.-L. Lions, and G. Papanicolaou. Asymptotic analysis for periodic structures, volume 374 (American Mathematical Soc., 2011).
[8] Bhatia, H., T. S. Carpenter, H. I. Ing´olfsson, G. Dharuman, P. Karande, S. Liu, T. Oppelstrup, C. Neale, F. C. Lightstone, B. Van Essen et al. Machine-learning-based dynamic-importance sampling for adaptive multiscale simulations. Nature Machine Intelligence 3:401–409, 2021.
[9] Blumers, A., M. Yin, H. Nakajima, Y. Hasegawa, Z. Li, and G. E. Karniadakis. Multiscale parareal algorithm for long-time mesoscopic simulations of microvascular blood ﬂow in zebraﬁsh. Computational Mechanics 68:1131–1152, 2021.
[10] Bonet, J. and T.-S. Lok. Variational and momentum preservation aspects of smooth particle hydrodynamic formulations. Computer Methods in Applied Mechanics and Engineering 180:97–115, 1999.
[11] Brandt, A. Multi-level adaptive solutions to boundary-value problems. Mathematics of Computation 31:333–390, 1977.
[12] Cai, S., Z. Mao, Z. Wang, M. Yin, and G. E. Karniadakis. Physics-informed neural networks (PINNs) for ﬂuid mechanics: A review. Acta Mechanica Sinica pp. 1–12, 2022.
[13] Cai, S., Z. Wang, L. Lu, T. A. Zaki, and G. E. Karniadakis. DeepM&Mnet: Inferring the electroconvection multiphysics ﬁelds based on operator approximation by neural networks. Journal of Computational Physics 436:110296, 2021.
[14] Carleo, G., I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborov´a. Machine learning and the physical sciences. Reviews of Modern Physics 91:045002, 2019.
[15] Chan, S. and A. H. Elsheikh. A machine learning approach for eﬃcient uncertainty quantiﬁcation using multiscale methods. Journal of Computational Physics 354:493–511, 2018.
[16] Chattopadhyay, A., P. Hassanzadeh, and D. Subramanian. Data-driven predictions of a multiscale lorenz 96 chaotic system using machine-learning methods: reservoir computing, artiﬁcial neural network, and long short-term memory network. Nonlinear Processes in Geophysics 27:373–389, 2020.
[17] Chen, T. and H. Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks 6:911–917, 1995.
31

[18] D’Elia, M., D. Littlewood, J. Trageser, M. Perego, and P. Bochev. An optimization-based strategy for peridynamic-fem coupling and for the prescription of nonlocal boundary conditions. arXiv preprint arXiv:2110.04420 , 2021.
[19] Dijkstra, W. and R. Mattheij. The condition number of the bem-matrix arising from laplace’s equation. Electronic Journal of Boundary Elements 4, 2006.
[20] Dobson, M., M. Luskin, and C. Ortner. Stability, instability, and error of the force-based quasicontinuum approximation. Archive for Rational Mechanics and Analysis 197:179–202, 2010.
[21] Douglas, J. and C.-S. Huang. An accelerated domain decomposition procedure based on robin transmission conditions. BIT Numerical Mathematics 37:678–686, 1997.
[22] Efendiev, Y., J. Galvis, and T. Y. Hou. Generalized multiscale ﬁnite element methods (GMsFEM). Journal of Computational Physics 251:116–135, 2013.
[23] Espanol, P. and P. Warren. Statistical mechanics of dissipative particle dynamics. EPL (Europhysics Letters) 30:191, 1995.
[24] Fish, J., G. J. Wagner, and S. Keten. Mesoscopic and multiscale modelling in materials. Nature Materials 20:774–786, 2021.
[25] Funaro, D., A. Quarteroni, and P. Zanolli. An iterative procedure with interface relaxation for domain decomposition methods. SIAM Journal on Numerical Analysis 25:1213–1236, 1988.
[26] Ganzenmu¨ller, G. C. An hourglass control algorithm for lagrangian smooth particle hydrodynamics. Computer Methods in Applied Mechanics and Engineering 286:87–106, 2015.
[27] Goswami, S., M. Yin, Y. Yu, and G. E. Karniadakis. A physics-informed variational deeponet for predicting crack path in quasi-brittle materials. Computer Methods in Applied Mechanics and Engineering 391:114587, 2022.
[28] Groot, R. D. and P. B. Warren. Dissipative particle dynamics: Bridging the gap between atomistic and mesoscopic simulation. The Journal of Chemical Physics 107:4423–4435, 1997.
[29] Gustafson, K. Domain decomposition, operator trigonometry, robin condition. Contemporary Mathematics 218:432–437, 1998.
[30] Holian, B. L. and R. Ravelo. Fracture simulations using large-scale molecular dynamics. Physical Review B 51:11275, 1995.
[31] Holzapfel, G. A., T. C. Gasser, and R. W. Ogden. A new constitutive framework for arterial wall mechanics and a comparative study of material models. Journal of Elasticity and the Physical Science of Solids 61:1–48, 2000.
32

[32] Hughes, T. J. The ﬁnite element method: linear static and dynamic ﬁnite element analysis (Courier Corporation, 2012).
[33] Ing´olfsson, H. I., C. Neale, T. S. Carpenter, R. Shrestha, C. A. L´opez, T. H. Tran, T. Oppelstrup, H. Bhatia, L. G. Stanton, X. Zhang et al. Machine learning–driven multiscale modeling reveals lipiddependent dynamics of ras signaling proteins. Proceedings of the National Academy of Sciences 119, 2022.
[34] Jing, N., Q. Xue, C. Ling, M. Shan, T. Zhang, X. Zhou, and Z. Jiao. Eﬀect of defects on young’s modulus of graphene sheets: a molecular dynamics simulation. RSC Advances 2:9124–9129, 2012.
[35] Karniadakis, G. E., I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed machine learning. Nature Reviews Physics 3:422–440, 2021.
[36] Karniadakis, G. E. and S. Sherwin. Spectral/hp element methods for computational ﬂuid dynamics (Oxford University Press, 2005).
[37] Kevrekidis, I. G., C. W. Gear, J. M. Hyman, P. G. Kevrekidis, O. Runborg, C. Theodoropoulos et al. Equation-free, coarse-grained multiscale computation: enabling microscopic simulators to perform system-level analysis. Commun. Math. Sci 1:715–762, 2003.
[38] Kevrekidis, I. G. and G. Samaey. Equation-free multiscale computation: Algorithms and applications. Annual Review of Physical Chemistry 60:321–344, 2009.
[39] Kovachki, N., Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481 , 2021.
[40] Lang, A. and J. Potthoﬀ. Fast simulation of gaussian random ﬁelds. Monte Carlo Methods and Applications 17:195–214, 2011.
[41] Lanthaler, S., S. Mishra, and G. E. Karniadakis. Error estimates for deeponets: A deep learning framework in inﬁnite dimensions. arXiv preprint arXiv:2102.09618 , 2021.
[42] Li, Z., N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial diﬀerential equations. arXiv preprint arXiv:2010.08895 , 2020.
[43] Li, Z., N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Multipole graph neural operator for parametric partial diﬀerential equations. arXiv preprint arXiv:2006.09535 , 2020.
[44] Li, Z., N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Graph kernel network for partial diﬀerential equations. arXiv preprint arXiv:2003.03485 , 2020.
33

[45] Lin, C., Z. Li, L. Lu, S. Cai, M. Maxey, and G. E. Karniadakis. Operator learning for predicting multiscale bubble growth dynamics. The Journal of Chemical Physics 154:104118, 2021.
[46] Lin, C., M. Maxey, Z. Li, and G. E. Karniadakis. A seamless multiscale operator neural network for inferring bubble dynamics. Journal of Fluid Mechanics 929, 2021.
[47] Lin, P. Theoretical and numerical analysis for the quasi-continuum approximation of a material particle model. Mathematics of Computation 72:657–675, 2003.
[48] Lions, P.-L. et al. On the schwarz alternating method. i. In: First international symposium on domain decomposition methods for partial diﬀerential equations, volume 1, p. 42 (Paris, France, 1988).
[49] Lu, L., P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence 3:218–229, 2021.
[50] Lu, L., X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. E. Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. arXiv preprint arXiv:2111.05512 , 2021.
[51] Mao, Z., L. Lu, O. Marxen, T. A. Zaki, and G. E. Karniadakis. Deepm&mnet for hypersonics: Predicting the coupled ﬂow and ﬁnite-rate chemistry behind a normal shock using neural-network approximation of operators. Journal of Computational Physics 447:110698, 2021.
[52] Masi, F. and I. Stefanou. Thermodynamics-based artiﬁcial neural networks (tann) for multiscale modeling of materials with inelastic microstructure. arXiv preprint arXiv:2108.13137 , 2021.
[53] Milton, G. W. The Theory of Composites (Cambridge University Press, 2002).
[54] Mok, D., W. Wall, and E. Ramm. Accelerated iterative substructuring schemes for instationary ﬂuidstructure interaction. Computational Fluid and Solid Mechanics 2:1325–1328, 2001.
[55] Monaghan, J. J. An introduction to SPH. Computer Physics Communications 48:89–96, 1988.
[56] Monaghan, J. J. Smoothed particle hydrodynamics. Annual Review of Astronomy and Astrophysics 30:543–574, 1992.
[57] Mota, A., I. Tezaur, and C. Alleman. The schwarz alternating method in solid mechanics. Computer Methods in Applied Mechanics and Engineering 319:19–51, 2017.
[58] Nie, X., S. Chen, M. Robbins et al. A continuum and molecular dynamics hybrid method for micro-and nano-ﬂuid ﬂow. Journal of Fluid Mechanics 500:55–64, 2004.
[59] Ortiz, M. A method of homogenization of elastic media. International Journal of Engineering Science 25:923–934, 1987.
34

[60] Park, J. and X. Zhu. Physics-informed neural networks for learning the homogenized coeﬃcients of multiscale elliptic equations. arXiv preprint arXiv:2202.09712v1 , 2022.
[61] Patera, A. T. A spectral element method for ﬂuid dynamics: laminar ﬂow in a channel expansion. Journal of Computational Physics 54:468–488, 1984.
[62] Peng, G. C., M. Alber, A. B. Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. E. Karniadakis, W. W. Lytton, P. Perdikaris et al. Multiscale modeling meets machine learning: What can we learn? Archives of Computational Methods in Engineering 28:1017–1037, 2021.
[63] Pfau, D., J. S. Spencer, A. G. Matthews, and W. M. C. Foulkes. Ab initio solution of the many-electron schr¨odinger equation with deep neural networks. Physical Review Research 2:033429, 2020.
[64] Pled, F., C. Desceliers, and T. Zhang. A robust solution of a statistical inverse problem in multiscale computational mechanics using an artiﬁcial neural network. Computer Methods in Applied Mechanics and Engineering 373:113540, 2021.
[65] Pyrialakos, S., I. Kalogeris, G. Sotiropoulos, and V. Papadopoulos. A neural network-aided bayesian identiﬁcation framework for multiscale modeling of nanocomposites. Computer Methods in Applied Mechanics and Engineering 384:113937, 2021.
[66] Rahman, A. S., T. Hosono, J. M. Quilty, J. Das, and A. Basak. Multiscale groundwater level forecasting: coupling new machine learning approaches with wavelet transforms. Advances in Water Resources 141:103595, 2020.
[67] Raissi, M., P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations. Journal of Computational Physics 378:686–707, 2019.
[68] Randles, P. and L. D. Libersky. Smoothed particle hydrodynamics: some recent improvements and applications. Computer Methods in Applied Mechanics and Engineering 139:375–408, 1996.
[69] Rapaport, D. C. and D. C. R. Rapaport. The art of molecular dynamics simulation (Cambridge university press, 2004).
[70] Rausch, M., G. E. Karniadakis, and J. D. Humphrey. Modeling soft tissue damage and failure using a combined particle/continuum approach. Biomechanics and Modeling in Mechanobiology 16:249–261, 2017.
[71] Regazzoni, F., L. Ded`e, and A. Quarteroni. Machine learning of multiscale active force generation models for the eﬃcient simulation of cardiac electromechanics. Computer Methods in Applied Mechanics and Engineering 370:113268, 2020.
35

[72] Rocha, I., P. Kerfriden, and F. van der Meer. On-the-ﬂy construction of surrogate constitutive models for concurrent multiscale mechanical analysis through probabilistic machine learning. Journal of Computational Physics: X 9:100083, 2021.
[73] Tadmor, E. B., M. Ortiz, and R. Phillips. Quasicontinuum analysis of defects in solids. Philosophical magazine A 73:1529–1563, 1996.
[74] Theodoropoulos, C., Y.-H. Qian, and I. G. Kevrekidis. “coarse” stability and bifurcation analysis using time-steppers: A reaction-diﬀusion example. Proceedings of the National Academy of Sciences 97:9840–9843, 2000.
[75] Tinsley Oden, J., S. Prudhomme, A. Romkes, and P. T. Bauman. Multiscale modeling of physical phenomena: Adaptive control of models. SIAM Journal on Scientiﬁc Computing 28:2359–2389, 2006.
[76] Tran, J. S., D. E. Schiavazzi, A. B. Ramachandra, A. M. Kahn, and A. L. Marsden. Automated tuning for parameter identiﬁcation and uncertainty quantiﬁcation in multi-scale coronary simulations. Computers & Fluids 142:128–138, 2017.
[77] Versteeg, H. K. and W. Malalasekera. An introduction to computational ﬂuid dynamics: the ﬁnite volume method (Pearson education, 2007).
[78] Wang, K. and W. Sun. A multiscale multi-permeability poroplasticity model linked by recursive homogenizations and deep learning. Computer Methods in Applied Mechanics and Engineering 334:337–380, 2018.
[79] Wang, S., H. Wang, and P. Perdikaris. Learning the solution operator of parametric partial diﬀerential equations with physics-informed deeponets. Science Advances 7:eabi8605, 2021.
[80] Wang, Y., Z. Li, J. Xu, C. Yang, and G. E. Karniadakis. Concurrent coupling of atomistic simulation and mesoscopic hydrodynamics for ﬂows over soft multi-functional surfaces. Soft Matter 15:1747–1757, 2019.
[81] Weinan, E. Principles of multiscale modeling (Cambridge University Press, 2011).
[82] Weinan, E. and B. Engquist. Multiscale modeling and computation. Notices of the AMS 50:1062–1070, 2003.
[83] Weinan, E., B. Engquist et al. The heterognous multiscale methods. Communications in Mathematical Sciences 1:87–132, 2003.
[84] Wu, L., K. Zulueta, Z. Major, A. Arriaga, and L. Noels. Bayesian inference of non-linear multiscale model parameters accelerated by a deep neural network. Computer Methods in Applied Mechanics and Engineering 360:112693, 2020.
36

[85] Xiao, S. and T. Belytschko. A bridging domain method for coupling continua with molecular dynamics. Computer Methods in Applied Mechanics and Engineering 193:1645–1669, 2004.
[86] Xu, X., M. D’Elia, C. Glusa, and J. T. Foster. Machine-learning of nonlocal kernels for anomalous subsurface transport from breakthrough curves. arXiv preprint arXiv:2201.11146 , 2022.
[87] Yin, M., E. Ban, B. V. Rego, E. Zhang, C. Cavinato, J. D. Humphrey, and G. E. Karniadakis. Simulating progressive intramural damage leading to aortic dissection using deeponet: an operator–regression neural network. Journal of Royal Society Interface 19:20140397, 2021.
[88] Yin, M., X. Zheng, J. D. Humphrey, and G. E. Karniadakis. Non-invasive inference of thrombus material properties with physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering 375:113603, 2021.
[89] You, H., Y. Yu, M. D’Elia, T. Gao, and S. Silling. Nonlocal kernel network (nkn): a stable and resolution-independent deep neural network. arXiv preprint arXiv:2201.02217 , 2022.
[90] You, H., Y. Yu, S. Silling, and M. D’Elia. A data-driven peridynamic continuum model for upscaling molecular dynamics. Computer Methods in Applied Mechanics and Engineering 389:114400, 2022.
[91] Yu, Y., F. F. Bargos, H. You, M. L. Parks, M. L. Bittencourt, and G. E. Karniadakis. A partitioned coupling framework for peridynamics and classical theory: analysis and simulations. Computer Methods in Applied Mechanics and Engineering 340:905–931, 2018.
[92] Zhang, E., M. Dao, G. E. Karniadakis, and S. Suresh. Analyses of internal structures and defects in materials using physics-informed neural networks. Science Advances 8:eabk0644, 2022.
[93] Zhang, E., M. Yin, and G. E. Karniadakis. Physics-informed neural networks for nonhomogeneous material identiﬁcation in elasticity imaging. arXiv preprint arXiv:2009.04525 , 2020.
[94] Zhang, L., J. Han, H. Wang, R. Car, and E. Weinan. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. Physical Review Letters 120:143001, 2018.
[95] Zhang, T., X. Li, and H. Gao. Fracture of graphene: a review. International Journal of Fracture 196:1–31, 2015.
[96] Zohdi, T. I. Homogenization methods and multiscale modeling. Encyclopedia of Computational Mechanics Second Edition pp. 1–24, 2017.
37

