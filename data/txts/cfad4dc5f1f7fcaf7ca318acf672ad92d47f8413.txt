Don’t let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox

Jad Salem
jsalem7@gatech.edu

Deven R. Desai
deven.desai@scheller.gatech.edu
February 1, 2022

Swati Gupta
swatig@gatech.edu

arXiv:2201.13367v1 [cs.CY] 31 Jan 2022

Abstract
Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conﬂict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Oﬃce of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action.
In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a biasaware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.
1 Introduction
How employers identify whom to interview and then hire has important eﬀects across society. Employment signiﬁcantly aﬀects access to healthcare, continuing education and, therefore, quality of life. The beneﬁts of employment are not, however, evenly distributed across race and gender categories in the United States. After George Floyd’s death, companies acted to address racial injustice by making public statements, donations to support racial equality, and Juneteenth a company holiday [53]. Several companies went further. Microsoft announced a $150 million investment to improve diversity including setting a goal of doubling the number of “Black and African American people managers, senior individual contributors and senior leaders” in the United States by 2025 [54]. Wells Fargo made a commitment to “double Black Leadership” by 2025 and “will evaluate senior leaders based on their progress in improving diversity and inclusion in their areas of responsibility, in addition to other eﬀorts” [54]. Google has set a goal of having 30% of its leadership from “under represented groups” by 2025 [34]. Boeing seeks to increase representation of “Black
1

employees by 20% while boosting other underrepresented groups over the next three years” [34]. Adidas announced plans to ﬁll at least “30% of new positions with black or Latinx people” [52]. Yet, both Microsoft and Wells Fargo received letters from the Labor Department’s Oﬃce of Federal Contract Compliance Programs (OFCCP) due to concern that the plans may discriminate based on race [53]. At the same time, the OFCCP announced a settlement with Microsoft in September 2020 for $3 million back pay and interest to address hiring disparities “against Asian applicants” for several positions from December 2015 to November 2018 [87]. The two OFCCP positions clash and appear to create a world where inaction opens the company to litigation, if not breaking the law, and corrective action creates the same risks. One might argue that the recent OFCCP inquiries were peculiar to the Trump administration’s approach to this area of law and not something the current administration would pursue. Administrations, however, change and a new one might follow the Trump approach. Regardless of who is in the White House, legal activism to challenge steps taken to address diversity or challenge discriminatory results are not likely to go away.
The reason this challenge is not likely to go away is that a company may be pursuing diversity goals and/or be addressing aﬃrmative action plans; but the two are not the same, and the diﬀerence matters [58]. As the Equal Employment Opportunity Commission explains in “Section 15 Race and Color Discrimination” of its Compliance Manual, diversity can be understood as “a business management concept under which employers voluntarily promote an inclusive workplace” [18]. Companies have pursued diversity to attract talent and gain “a competitive advantage” [18]. In contrast, aﬃrmative action refers to “those actions appropriate to overcome the eﬀects of past or present practices, policies, or other barriers to equal employment opportunity” [4]. Such steps may occur because of a court order, negotiated settlement, or government regulation [18]. Employers may also use a voluntary aﬃrmative action plan “in appropriate circumstances, such as to eliminate a manifest imbalance in a traditionally segregated job category” [18]. There is a conceptual and practical link between diversity goals and aﬃrmative action. A company may pursue diversity “for competitive reasons rather than in response to discrimination” and “such initiatives may also help to avoid discrimination” [18]. As the legal status of diversity plans is unclear, methods to support both options are needed.
As another motivation, companies may want to see whether they are missing hiring and talent opportunities. Companies can be stuck in an equilibrium because they rely on, or exploit “old certainties,” rather than explore “new possibilities” [83]. This exploration/exploitation trade-oﬀ began in organizational business literature but has become a signiﬁcant part of how the machine learning community thinks about understanding information [51]. As a matter of best organizational and ML practices, companies need ways to explore new candidate pools.
Regardless of the motivation behind a company plan, there is a steady drumbeat for algorithmic transparency, especially in employment and admissions contexts [47]. Thus an entity may have to or wish to reveal the process at some point. In either case the entity would want to show that their process is sound from both a mathematical and a legal view. These issues could push any company to avoid steps to address diversity because of litigation risks, both real and perceived. Although some scholars argue that the use of machine learning would constitute a valid business necessity claim so long as the target variable is job-related, thus rendering the question of equality of outcomes irrelevant, debates about which actions are and are not allowed to address diversity persist, especially when using an algorithmic approach [36]. Simply put, when entities wish to be proactive regarding diversity, potential discrimination, or wish to explore whether they have missed opportunities in hiring talent [80], they will need a path that passes muster against a range of challenges.
2

Sourcing • Unequal job ad targeting

Filtering • Unnecessary barriers for some groups • Reinforcing of societal inequalities via machine learning

Hiring, Interviewing • Implicit bias among employers • Unequal impact of background checks on the over-policed • Referral bias

Retention • Implicit bias among employers • Workplace environment issues

Figure 1: Some ethical concerns in various stages of the employment pipeline.
This paper thus seeks to oﬀer techniques and legal analysis to enable companies to pursue legal and ethical hiring goals and face this question: How to improve equal opportunity and employment practices without crossing into arguably illegal discriminatory practices? The ideas discussed here are general and key takeaways can be applied to several stages in the hiring pipeline. That said, this paper uses the screening stage of employment to exemplify methods and analysis and oﬀer one way to attack the general problem.
2 Algorithms and the Hiring Process
Employers want to hire a great workforce, but reaching and assessing the full viable range of potential employees poses problems. Many parts of the hiring process use algorithms as a way to manage and sort candidates. The practice can be traced back at least 40 years [93, 94]. The problem is that there are a number of junctures in the hiring pipeline at which bias can aﬀect decisions, as depicted in Figure 1. Job advertisements on various platforms can be targeted at speciﬁc audiences [30, 78]. Application rates can diﬀer across groups due to presumed employer bias [85]. Data-driven tools for evaluating r´esum´es can be biased due to inequalities in training data [63], imbalance in data [103], or diﬀerences in false positive/negative error rates in prediction algorithms leading to bias as a downstream eﬀect [49]. Referral hiring can lead to favoritism [92]. Customer evaluations of freelancers can adversely impact certain groups [67]. Final hiring decisions can be inﬂuenced by human biases of the hiring committee [35]. After going through the hiring pipeline, candidates also see a signiﬁcant diﬀerence in salaries oﬀered [86], and retention rates can diﬀer dependent on the work environment [44]. Indeed, societal biases are pervasive and can aﬀect decisions made by experts [65].
In addition, when automated systems are used at any stage, missed opportunity (false negatives) with respect to minority candidates is often shrugged oﬀ as an artifact of the prediction model, necessary for overall accuracy [74]. These models often train on historic data, which can depict imbalanced selection rates across diﬀerent groups of candidates, and these trends can be learnt by automated methods [33, 39]. History can dictate future actions. In short, existing pipeline practices

3

can reiterate and increase disparity in opportunity and outcomes. Although the hiring pipeline can be improved in many places, we ﬁnd the screening stage to be particularly ripe for improvement, and we therefore focus the article on this stage for the reasons outlined below.
First, data-driven methods, by their nature, can pose a problem. Seemingly objective methods interact with real-world data, and so automated decisions can reﬂect and therefore, reinforce societal inequalities [56, 38]. Even when there is no intent to discriminate, and the decision system uses the same data and applies the same rule to all, there may be a disproportionate eﬀect on a protected class (i.e., groups protected by law from discrimination, such as those deﬁned by sex, race, age, etc.) [81, 31, 32]. The problems in screening map to the more general ones present when using data-driven decision-making in hiring. So, screening is a good lens through which to investigate the concerns around using algorithms and data in the employment context in general.
Second, algorithms are already used for screening applications. Such automated methods oﬀer numerous advantages: speed, cost-eﬀectiveness, potential objectivity, and uniformity in process. These properties may seem desirable at ﬁrst glance from an ethical and fairness perspective; consistency in decisions is often a good thing, and a lack of human involvement would seem to minimize the role of implicit bias in hiring decisions [66]. Thus, automated methods have become commonplace in screening. Adjusting algorithmic techniques may be a more palatable idea and more feasible in an industry currently using automated processes than using algorithms in a heretofore un-automated process. New algorithmic interventions may, therefore, be more likely to be applied in practice.
Third, changes at early stages of the hiring pipeline are vital to address later bias. Changes at later stages are only meaningful if they act on a diverse pool of candidates. Without a diverse candidate pool at those stages, eﬀorts to address bias become empty theater, because there will be few to no candidates from underrepresented groups for which the changes would help. As such, we focus speciﬁcally on automated screening processes: how should applicant-screening methods be developed? These algorithmic tools should be designed with the goal of (a) selecting applicants of a desired quality, (b) satisfying some agreed upon fairness criteria, and (c) adhering to US anti-discrimination law.
3 Biases in Data
We broadly refer to systematic inconsistencies in data which adversely aﬀect certain groups as “bias.” The ﬁrst step in reducing discrimination is to understand the source of this bias. Unfair decisions can stem from many places, and identifying the origins of the bias allows for precise interventions. In the hiring process (automated or otherwise), applications will typically be assigned a score, thus allowing comparisons of applicants based on a single number or with respect to a single ranking of candidates [91, 71]. This evaluation metric can be hard-coded into an algorithm or developed dynamically, and in either case, can be unfair. A natural question is whether we can model this bias precisely and account for it within the algorithms to make them justiﬁably (provably) fairer.
Bias in evaluations can take diﬀerent forms and be observed in diﬀerent ways. For instance, a screening algorithm developed, but not employed, by Amazon penalized r´esum´es which included the word “women’s” due to data of past hiring trends in the company [45]. This algorithm penalized, for example, those who attended all-women colleges, and rewarded vocabulary typically used by men. In a similar vein, an empirical study showed that science faculty’s assessment of r´esum´es varied dependent on the gender of the student [86]. These are fairly blatant examples of discrimination,
4

as toggling a protected attribute results in diﬀerent treatment. Note that this form of unfairness— while blatant—can be hard to observe in practice, as applicants are never truly identical but for a small number of attributes.
Many cases of bias in evaluations, however, are more nuanced. Consider using SAT scores to screen candidates—a practice employers such as McKinsey, Bain, Goldman Sachs, and Amazon have been known to use even for candidates with advanced degrees [48, 29, 64]. Studies show that even if students are equally able to perform well on a test, if the test is announced to exhibit diﬀerences across groups, students in a negatively stereotyped group perform lower than the students in a nonstereotyped group [97]. Another study from 2013 shows that SAT scores are correlated with family income, potentially pointing to issues of access [50]. Inside Higher Education looked at SAT scores in 2015 and found that despite fee waivers and increased eﬀorts to provide support and tutoring to low-income families,
“In each of the three parts of the SAT, the lowest average scores were those with less than $20,000 in family income, and the highest averages were those with more than $200,000 in income, and the gaps are signiﬁcant. In reading, for example, the average for those with family income below $20,000 is 433, while the average for those with income of above $200,000 is 570.”
Thus, compared to 2013, gaps in performance with respect to racial groups not only persisted but increased. This problem with SAT scores is further evident in a recent study by Faenza et al. [59], which showed a shift by approximately 200 points in SAT scores from schools with diﬀerent economic need indices. Thus, an employer using SAT scores appears neutral but sets up a pre-selected pool.
These issues regarding bias in data raise important design questions for algorithmic intervention. When designing a decision-making algorithm, can we control for bias in historic data (thus avoiding Amazon’s situation discussed above)? In other words, what steps can be taken to control for historic, economic, and/or social factors that are known to skew seemingly objective metrics such as the SAT?
4 Approaches to addressing bias to date and their limits
A variety of algorithmic techniques have been proposed for coping with biased data and improving fairness, from pre-processing techniques which involve modifying data before feeding it to an algorithm [61]; to in-processing techniques, which modify the algorithm itself [73, 104]; to postprocessing techniques, which modify decisions made by an algorithm after the fact [68, 72]. Current computer science literature highlights that merely scrubbing protected class information from an application may not help mitigate existing biases [46], and that algorithms have to use protected information to ﬁx existing biases in data [55]. Using protected information, however, may put the hiring process at odds with anti-discrimination law. Other prevalent approaches include iteratively removing data which is correlated with protected information [104]; such approaches, however, may remove highly predictive information.
Algorithmic bias mitigation refers to the design of algorithms which perform well despite uncertainties about candidates’ qualiﬁcations. This encompasses, for example, the design of procedures to select qualiﬁed candidates given biased data, or the design of algorithms which provably satisfy some notion of fairness. As discussed earlier, bias in evaluations can render bias-agnostic methods suboptimal [80, 57, 90, 55]; at the same time, imposing constraints such as demographic parity (i.e.,
5

proportional selection from diﬀerent demographic groups) can hinder performance in some cases [43], which points to potential trade-oﬀs between bias mitigation and quality of selections. In our approach, we will take the view of algorithmic bias mitigation, given ﬁne-tuned uncertainties in the evaluation of each individual.
Algorithmic Bias Mitigation. Attempts to mitigate bias often begin with an understanding of the nature of the bias, or in other words, the inconsistencies in measurement of the ability of candidates. Mitigating the impact of such inconsistencies is an instance-speciﬁc endeavor; no cure-all exists. Nonetheless, there is theoretical work on mitigating bias under various mathematical assumptions. For instance, attempts have been made to address miscalibration of evaluations between multiple evaluators [100], and techniques have been developed for cases where some information is known about how biased each evaluator is in each evaluation [101]. In general, mathematical techniques can be developed as long as some assumptions on bias are made.1 Certain “coarse” sources of bias seem to be prevalent across demographic groups, and algorithms can be designed with these in mind. One might say these are the ﬁrst approximations to incorporate the knowledge of large trends visible broadly across demographic groups, such as are seen in SAT scores discussed earlier [59]. Addressing these coarser sources of bias from a theoretical point of view can provide insight in dealing with other forms of bias.
A recent mathematical model that captures the dependence of errors in testing over groups is the group model of bias. The model is based on the empirical work of Wenner˚as and Wold [102], and was introduced by Kleinberg and Raghavan in the context of oﬄine selection (e.g., applicantscreening) [80], further studied by Salem and Gupta [90], Faenza et al. [59], and Blum and Stangl [37] in the context of selection problems. This model assumes that bias is fairly consistent within each demographic group, and thus evaluations oﬀer more accurate rankings within each group, but not across the groups. For example, once one accounts for diﬃculties in comparing one demographic group to another, there may be no way to conﬁdently compare an 90% attained by a white male scholar Adam to a 85% attained by a Latina scholar Tia. But one can compare Adam against another white male scholar John with 83%, and note that Adam is better.
This model is at the same time appealing and dissatisfying in its simplicity. It is appealing in the sense that the model sheds light on best practices when the data is biased consistently for certain groups. That consistency indicates that information about group membership alone allows selection algorithms to reduce bias in selections. It is dissatisfying, however, in its coarseness, as it ignores intra-group diﬀerences in testing/evaluation errors and ignores any potential comparisons between groups. Adding to the example above, let us say that Tia also belongs to a low-income family, and we want to compare Tia to another Latina scholar May (not from a low-income family). This model does not account for such confounding variables of socio-economic status. Follow-up work by Celis et al. [40] proposed a multiplicative model of bias in the context of rankings, wherein candidates in the intersection of diﬀerent groups face a consistently higher bias. This approach, however, again equalizes the amount of bias within each smallest “unique” group (e.g., male, white, and age above 45 or lesbian, Asian, aged 39). It may not be okay to equalize the experience of every male, white person above the age of 45 or of every lesbian, Asian, under age 50. The underlying problem with this is the assumption of group membership, which may not even be accurate in
1This points to multiple issues in bias-mitigation. First, the assumptions on bias are diﬃcult to justify empirically, as “ground truth” is seldom available (for example, the true ability of a candidate is never truly known, especially for candidates who are not hired). Second, it is diﬃcult to assess bias-mitigation techniques for a similar reason: if one does not know the ground truth, then it is hard to quantify how good any decision is.
6

practice. Indeed, whether a Chinese Asian, an Indian Asian, and a Filipino Asian faces the same amount of bias, and so should be treated the same, seems unlikely.
Current Industrial Practices. How then do companies actually hire candidates, while reconciling with anti-discrimination laws and biases in the hiring pipeline? In a recent survey, the only speciﬁc public claim made by vendors of pre-employment assessments was adherence to the 4/5ths rule—outlined in the 1978 Uniform Guidelines on Employee Selection Procedures—which requires that group-speciﬁc selection rates of any pre-screening are all within a factor of 4/5 of each other [89]. Yet this approach is coarse as it is agnostic to quality of candidates. Applying a 4/5ths rule in selection up front (e.g., as the current practice in the industry suggests [89]) does not change the perceived potential of candidates, nor account for uncertainties and biases in the data systematically. It can therefore simply set up the underrepresented group’s candidates for failure, and lead to resentment and enlivening of negative stereotypes [60, 69].
The trade-oﬀs in algorithmic approaches track legal issues. If an employer uses an algorithmic tool to evaluate and screen candidates, the employer may face legal challenges depending on the outputs of the tool. A likely challenge is that the tool created illegal disparate impact. Disparate impact addresses when “facially neutral policies or practices have a disproportionate adverse effect or impact on a protected class” [62, 28]. The disparate impact doctrine is thus supposed to address situations where intent is not at hand or cannot be ascertained [98]. In short, outcomes based on unaware algorithms may ﬁt quite well with disparate impact challenges, because unaware algorithms are facially neutral, may lack intent to discriminate, and nonetheless yield statistically discriminatory results.2
The possibility of a disparate impact claim leads to an obvious approach. An employer may design a more aware algorithm that takes protected class status into account. However, this approach may run into a disparate treatment challenge. Disparate treatment is the legal doctrine that prohibits intentional use of race or other protected classes in making an employment decision. Thus, we return to the paradox described above, because it seems that an employer is trapped between using facially neutral systems that reﬂect systemic and historically conditioned, biased results or facing lawsuits for using aware systems to mitigate such eﬀects. This paradox is exacerbated by current legal scholarship debating what algorithmic interventions to address bias, if any, are allowed and the implications of the lawsuit Ricci v. DeStefano, in which an action by the City of New Haven that tried to account for disparate impact of an administered promotion test led to litigation that was decided against the city. In Section 5, we outline the poset approach, which we argue provides a way to solve the hiring paradox. Section 6 turns to an in-depth discussion on the takeaways from Ricci v. DeStefano and explains how the poset approach ﬁts within legal rules so that one can use a bias aware approach to hiring and yet maintain individualized assessments of candidates.
5 A new approach: coping with uncertainty using partial orders
As discussed in Section 3, coping with uncertainties in data is a fundamental problem in applicant screening systems, as well as in data-driven decision-making more generally. In this section, we will
2Despite the fact that the 4/5 rule is mentioned as evidence of disparate impact in the 1978 Uniform Guidelines on Employee Selection Procedures, there is no precise quantiﬁcation of disparate impact. The 4/5 rule is often used as a trigger for litigation, but other statistical tests have been used in courts as well [84, 96].
7

Figure 2: Coping with Uncertainty using Partial Orders: A Path to Address Disparate Impact in Hiring Practices
discuss one method, called the poset approach, for applicant-screening in the face of uncertainty which has emerged recently in the computer science literature [90]. In Section 6, we will use this approach as a vehicle for discussing the legality of algorithmic bias mitigation in hiring.
Consider the following scenario: there are three candidates A, B, and C, with ability scores of 82, 68, and 67, respectively, and you wish to grant interviews to two of them. The ability scores are known to be a strong predictor of job performance, but are only known to be accurate up to 3 points. In this case, there is a signiﬁcant chance that C is a better candidate than B, but the utilitarian approach of selecting the highest-scoring candidates would routinely select A and B. The core idea behind the poset approach is that the latter approach is unfair to C, or more generally, that ignoring uncertainty can result in unfair decisions. In other words:
Some applicants, due to individual experiences or lack of historic data, cannot be reliably ranked. The solution need not involve producing a (possibly inaccurate) ranking. Instead, allowing for partial rankings can itself open the door to fairer decisions.
The poset approach, which we explain in more detail below, makes use of a mathematical structure called a partially ordered set, or poset, which can be used to encode uncertainty in ordinal information. Consider, for example, a set S1 = {4, 2, 5} of true hirability of three candidates (which is often not observable in practice). This set is called totally ordered since any pair of the scores can be ordered (i.e., ranked) with respect to the relation ≤. In other words, we can rank the scores: 2 ≤ 4 ≤ 5, thus inducing an order amongst the candidates.
However, in practice, one cannot observe directly how good a candidate might be at their job. This is where the poset approach can help. Intuitively speaking, one can think of a partial order as a set of comparisons, which may not cover all pairs of candidates (i.e., a total order with some comparisons missing). For example, consider a candidate A who has experience in industry, a candidate B who has experience in industry and who has an MBA, and a candidate C who has an MBA. Considering these traits as binary (yes/no) attributes, one can represent their qualiﬁcations as the set S2 = {industry}, {MBA}, {industry, MBA} . From the given information, one might rank B above both A and C, since B is qualiﬁed with respect to both measures, and the other candidates are only qualiﬁed with respect to one. However, A and C might be considered incomparable, since neither candidate’s qualiﬁcations subsume the other’s. In this case, S2 is a partially ordered set,
8

Adam Max

Trisha

80

85

90

95

T

A

M

Figure 3: Score ranges and resulting Hasse diagram for the scenario in Example 1.

but not a totally ordered set. To be precise, a relation is a partial order on a set S if three conditions hold for all a, b, c ∈ S: (1) a a; (2) if a b and b c, then a c; and (3) if a b and b a, then a = b. One can check that all these properties are satisﬁes for the set S2. A poset is often visually depicted using its Hasse diagram, which is directed graph in which edges represent orderings. For example, the Hasse diagrams for S1 and S2 are as follows:

5

{industry, MBA}

S1: 4

S2:

2

{industry} {MBA}

Note that Hasse diagrams omit redundant edges: even though 2 ≤ 5, the edge 2 → 5 is not included, since it is implied by the edges 2 → 4 and 4 → 5.
The poset approach is the process of (1) forming a partial ranking (i.e., a partial order) of the candidate pool based on uncertainties, inaccuracies, or biases in data, and (2) making selections based on this poset. By making selection decisions in this way, one can concretely take uncertainty into account and, say, avoid routinely harming candidate C in the example above. This can lead to bias mitigation in cases where the evaluation metric is biased against a certain group; e.g., if a group is underrepresented in training data and experiences large errors in the resulting ML model, the poset approach can confer beneﬁt of the doubt to those underrepresented candidates.
We next illustrate how posets can model uncertainty using two examples.
Example 1. The poset approach can illustrate how one can account for uncertainties while also avoiding prohibiting discrimination based on gender.3 Using the poset approach, one may incorporate demographic context of the candidates and quantify uncertainy in their evaluations (either by directly observing the context, or by unsupervised methods such as clustering). Suppose that in a training dataset, nonbinary candidates are underrepresented, and as a consequence have high variance in errors in the prediction model. One may ﬁnd that a nonbinary candidate Max has a wide score range of 80-90% (e.g., due lack of training data on nonbinary candidates), another male candidate Adam has a score between 85-87%, and a third female candidate Trisha has a score between 92-95% (see Fig. 3). Now, using only the score ranges to compare candidates, Trisha compares favorably to Max, but it is unclear if Max is more qualiﬁed than Adam as their ranges overlap. In this case, we can think of Max and Adam as mutually incomparable. The poset approach therefore allows for individualized treatment of inconsistencies in data processed.

3As recently as June 15, 2020, the Supreme Court of the United States ruled that the Title VII of the Civil Rights Act prohibits discrimination on the basis of sexual orientation and gender identity [27, 99].

9

College GPA

3.5

2.8 2.1 3

1.4

0.7

5

1 2
4

1

2

4

Possible Selection

Criteria

3

5

2 4 6 8 10 Work Experience
Figure 4: A depiction of the work experience and college GPA of ﬁve candidates coming from three groups (diﬀerentiated by color). The thick dashed line represents a possible selection criteria which, in this case, imposes a threshold on each of the two attributes. Conﬁdence regions are drawn around each data point to indicate, say, 95% conﬁdence in the inclusion of a candidate’s true ability. Given these conﬁdence regions, one can construct a partial ranking, as depicted by the Hasse diagram on the right. Arrows between candidates indicate ranking with certainty with respect to both attributes (e.g., Candidate 1 is ranked higher than Candidate 5 since the best work experience score (≈ 4) in the conﬁdence region of Candidate 5 is worse than the worst work experience score (≈ 5.5) of Candidate 1, and similarly for College GPA. Note that there may be other reasonable ways of constructing partial rankings as well.

Example 2. Suppose that three candidates are to be selected based on two attributes: work experience and college GPA. You have set cutoﬀs for each of these attributes and only wish to select candidates exceeding each cutoﬀ. See Figure 4 for a depiction of the candidate pool, where each color represents a particular demographic group. Let the colored areas around each candidate node represent a “conﬁdence region;” i.e., with some high degree of conﬁdence, the candidate’s latent ability lies in the drawn region. Note that we can infer partial rankings from these conﬁdence regions in a similar way to Example 1: if the conﬁdence region of candidate A is strictly above and to the right of the conﬁdence region of candidate B, then A is ranked above B.
Using only raw scores, only the two blue candidates meet the cutoﬀs. However, taking conﬁdence regions into account, we see that the two green candidates might meet the cutoﬀs as well. How, then, should one choose three candidates among the green and blue ones? One way to do so is to construct a partial ranking based on the conﬁdence regions, as shown in Figure 4. In this partial ranking, there are three candidates who are maximally ranked (i.e., are not ranked below any other candidates): the two blue candidates and the right-most green candidate. This observation could be one justiﬁcation for selecting these candidates.
The process outlined in these examples (forming score ranges/regions for each candidate and inferring comparisons therefrom) can be applied quite generally, and allows for explicit treatment of bias in data. Data-driven techniques, such as estimating latent group-bias in a machine learning model, can be applied to generate these score ranges, which in turn induce a partial ranking. Such

10

methods can be used to avoid penalizing applicants who come from underrepresented groups, who are more likely to face inaccurate evaluation via machine learning models. A recent paper by Emelianov et al. shows that groups with high error variances can receive worse treatment, even if the evaluations are unbiased for all candidates [57], pointing to the need for interventions like the poset approach that take uncertainty into account.
The poset approach in practice. We end this section by providing a framework for using the poset approach in practice. While this framework does not encompass every possible use of the poset approach [90], it will describe the process from beginning to end and put the poset approach in broader context (see Figure 2).
Step 1. Clean-up and Process Past Hiring Data. To start, collect data from previous hiring cycles. This data might include scores derived from textual analysis of r´esum´es, test scores for job-related tasks (e.g., computer programming test scores), automated scores based on analysis of video interviews [89], college GPAs, courses taken, years of work experience, job performance of those who were hired, and so on.
Step 2. Quantify Uncertainty and Bias. Use data analysis to quantify potential data biases. Clusterings, for example, can help determine if evaluations unfairly favor one group over another. Looking at the data along diﬀerent demographics (e.g., based on race, gender, age) can point to potentially discriminatory decisions in the past. Use social science studies (e.g., [97]) that highlight the impact of social status on the considered metrics (e.g., standardized test scores). This will help highlight qualitative and quantitative reasons for disparities in the past hiring data.
Step 3. Construct a Partial Order. Trends identiﬁed in Step 2 can be used to construct a partial ranking of candidates. For example, score ranges can be constructed for each attribute of interest using a prediction model and estimates of its error variances. These ranges can take into account distributional diﬀerences across protected attributes, diﬀering error variances due to training data imbalance,4 observed inaccuracies in past predictions, and so on. Unsupervised methods such as clustering can be used without the speciﬁc knowledge about protected information, or this can be abstracted out by a third-party vendor to simply provide a hiring entity with the resultant estimate of uncertainties or the poset over the candidates. These approaches are discussed in more detail in Appendix A. The goal here is to account for uncertainties, inaccuracies, and biases in a direct and mathematically justiﬁed way, thereby paving the way to fairer decisions.
Step 4. Adapt Selection Algorithms. Once the partial ranking has been constructed, selections need to be made. Presumably, a hiring committee already has a screening process (automated or otherwise) which aligns with the goals of the employer. In order to implement the poset approach, this screening process must be adapted to take a partial ranking as input instead of numeric scores or a total ranking. Typically, this can be done by prioritizing maximality and randomizing wherever incomparabilities necessitate (see [90] for an example of this in an online setting).
Step 5. Auditing for Policy Compliance. The entire hiring pipeline may be subject to auditing for compliance with anti-discrimination policy. It is prudent to document and be able to justify each decision made in the hiring process, particularly those pertaining to the four steps outlined
4This refers to the observation that a group which is underrepresented in training data often experiences large errors in a resulting prediction model. In the poset approach, these larger errors could translate to larger score ranges for the underrepresented group. Note that the groups in question could come from a clustering and need not be demographic groups.
11

above. For example, one should be able to explain how the partial ranking was constructed and be able to justify those decisions by pointing to data and relevant research. A deeper discussion of the legality of the poset approach (and algorithmic bias mitigation more generally) is in Section 6.
6 Discussion and Best Practices—Law, Mathematics, and Posets in Practice
We now return to the business cases with which we started and the tensions they present regarding diversity, equity, and legal interests. On the one hand, ﬁrms are seeking to address diversity regardless of a history of discrimination. On the other hand, when evidence of past or present practices creating barriers is found, companies addressing those practices are pursuing aﬃrmative action plans. In general, a ﬁrm that does little to account for race, gender, and other protected classes may ﬁnd it has created disparate impact; and yet, when that ﬁrm seeks to take protected classes into account, such steps may violate the ban on disparate treatment.
We oﬀer that in the unlikely case where a ﬁrm has no reason to believe that norms, traditions, or societal inequalities are negatively aﬀecting the ability for members of a protected group to pass through the stages of the hiring pipeline, action may be possible under diversity interests but not required by law. At least two shifts point to increased diversity activity. First, many companies have made public commitment to large steps to address diversity in employment. Second, there is a new push for companies to disclose workforce diversity data, which has resulted in 82 of the top 100 companies doing so. The public imperative combined with the data supports companies taking the initiative to address workforce imbalances regardless of legal requirements to do so [75]. In contrast, as matter of aﬃrmative action, a ﬁrm with evidence of discrimination seeking to address imbalances in its workforce should be able to take steps to do so. Such steps could involve, for instance, scoring applicants using a machine learning model and developing conﬁdence intervals around scores using the poset approach. From a legal perspective, it is important to be able to support the legality of each action, from the decision to address diversity to the decision to use protected class information, to each design choice in the algorithm, to each adjustment to future rounds of hiring.
The beauty of the poset approach is that it is agnostic to the motivation, diversity or addressing discrimination via aﬃrmative action, behind a company’s plan. To be clear, whether a purely diversity-driven plan is legal is an unsettled question and beyond the scope of this paper [58, 36]. Nonetheless, because of the current drive to address inequity, we expect this question to arise in the near future and suggest that the poset approach would aid and support such eﬀorts. Furthermore, because many announced diversity programs are likely backed by data about imbalances and unnecessary barriers to employment, such eﬀorts will likely be seen as aﬃrmative action plans under the law. Thus in this section we address the core question of how well the poset approach stands up to legal scrutiny as an allowed method to address aﬃrmative actions plans.
Given that eﬀorts to modify evaluation mechanisms or selection algorithms can raise both disparate impact and disparate treatment issues, we now use a hypothetical employer perspective in line with Microsoft’s and other companies’ announced goals to suggest best practices. Insights are derived from a series of questions about how to identify workforce imbalances (Section 6.1) and how to address said imbalances (Section 6.2).
12

6.1 Diagnosis
Q1: An employer is concerned that its workforce under-represents women and minorities. May they do anything to change their current hiring practices?
Yes. The purpose behind Title VII is “[T]o achieve equality of employment opportunities,” and Congress “directed the thrust of the Act to the consequences of employment practices, not simply the motivation” [2]. That means “unnecessary barriers to employment” must fall, even if “neutral on their face” and “neutral in terms of intent” [1]. Federal courts have disallowed a host of hiring and promotion practices that “operate[d] as ‘built in headwinds’ for minority groups” [19]. In addition, the Supreme Court has upheld the legality of employment plans to address discrimination without reference to its past practices or evidence of a possible violation of the law [8].
To take action, an employer “need[s] to point only to a ‘conspicuous ... imbalance in traditionally segregated job categories’ ” [8]. Logically, this requirement implies that initial, proactive analysis identifying the imbalance problems can serve as justiﬁcation for adjustments to hiring practices. As such, employers can and should use data science and analytics to identify the imbalance in their hiring pipeline that it seeks to address [82, 70, 76].
As one example, the employer can use human resources data to examine its employment practices. First, it can audit its current workforce and get ﬁne-grained information about who works at the company and at what levels. Such an approach allows the company to look beyond simple questions such as “Does it have an equal number of men and women in the workforce?” Instead, the company can see the gender and minority makeup at diﬀerent levels of employment such as upper management, upper-middle management, middle management, administration, hourly workers, contractors, and so on. Visualizing the data with pie-charts or heat maps will provide clear, vivid ways to see the current state of aﬀairs. Second, after such a study, the company can see potential sources of issues. It may ﬁnd that women and minorities rarely move beyond middle management, are rarely interviewed for promotion, or that screening to date has not selected, or under-selected, women and minorities for interviews to be potential employees. At a general level, these types of analyses support the case that there is something to ﬁx. This gets us to the next step in the process.
Q2: If a company ﬁnds that women and minorities are rarely interviewed and further ﬁnds that screening to date has not selected, or under-selected, women and minorities for interviews to be potential employees, do these conditions support allowing an employer to use protected-class information to build or apply a bias-aware algorithm at the screening stage?
Identifying a problem with a screening process or a structural problem in the company’s workforce, reveals a clear “unnecessary barrier to employment” even if the algorithm is neutral on its face and in intent. For example, if men tend to be scored higher than women (e.g., as in Fig. 5), then a facially neutral selection algorithm would disproportionately select men, even if true ability is similar across genders. In general, the identiﬁed, strong evidence of bias in current algorithmic sorting in the hiring process, including the screening stage, should constitute the sort of “built in headwind[] for minority groups” that the law seeks to eliminate. With suﬃcient evidence of bias and systemic barriers to equality of employment opportunities, an employer can make a case for using bias-aware algorithms.
6.2 Corrective action
Voluntary action to comply with the goals of Title VII is not only allowed; it is favored [9]. Nonetheless, in some cases, trying to further the goals of Title VII to address discrimination raises the
13

paradox where one approach looks like disparate impact and a corrective action looks like disparate treatment. What can a company actually do?
Q3: May an employer use protected-class information to increase diversity among interviewees? This question is complex as it entwines various parts of the process that need to be slowly
unpacked. A recent case Ricci v. DeStefano [20] illustrates some problems and provides guidance on allowed and prohibited actions.
Background. In Ricci v. DeStefano, the City of New Haven had developed a test for ﬁreﬁghter promotion with the help and validation of experts. When administered, 77 people took the lieutenant exam: “43 whites, 19 blacks, and 15 Hispanics. Of those, 34 candidates passed: 25 whites, 6 blacks, and 3 Hispanics.” 41 people took the captain’s exam: “25 whites, 8 blacks, and 8 Hispanics. Of those, 22 candidates passed: 16 whites, 3 blacks, and 3 Hispanics.” Despite the experts’ opinions and validations of the test, the City rejected the results because the pass rate caused the city to believe it might be sued for disparate impact. The Supreme Court did not allow this after-the-fact change, because New Haven’s actions relied on race, (the race of those who passed the test), to reject the results, and in that sense, New Haven engaged in disparate treatment. Thus, it may appear that an entity cannot account for and alter employment practices when there is evidence of potential disparate impact in the entity’s practices, because such changes will necessarily constitute disparate treatment [33]. That is incorrect [77].
Analysis. As the Supreme Court put it, not allowing an entity to account for race to avoid disparate impact liability “if the employer knows its practice violates the disparate-impact provision,” is contrary to “Congress’s intent that “voluntary compliance” be “the preferred means of achieving the objectives of Title VII” [24]. This rule, however, does not mean an entity can simply assert there has been a history of past discrimination and so a need to throw out a practice, because that might lead to “an unyielding racial quota” [25]. As stated above, the entity has to show why the change is needed in light of the goals of Title VII. In addition, the timing of when an entity makes changes matters.
The way the test was developed and administered by New Haven doomed the City’s decision to reject the test’s outcomes. New Haven began well by hiring experts to design a likely valid test. The City spent $100,000 on experts on designing the tests for ﬁre departments [21]. The experts conducted interviews, went on ride-alongs, interviewed incumbents at the promotional level at issue, and designed “job-analysis questionnaires and administered them to most of the incumbent battalion chiefs, captains, and lieutenants in the Department” [21]. As the Supreme Court noted, “At every stage of the job analyses, IOS [the company that developed the test], by deliberate choice, oversampled minority ﬁreﬁghters to ensure that the results—which IOS would use to develop the examinations—would not unintentionally favor white candidates” [22]. Once the test was approved, New Haven set a 3-month study period and gave candidates a study guide including the “source material for the questions, including the speciﬁc chapters from which the questions were taken” [22]. Nonetheless, after the tests were given, the results indicated disparate impact [23].
The city’s ex-post actions were the problem. The Court rejected “invalidating the test results” after the fact without “a strong basis in evidence of an impermissible disparate impact” [26]. The ex-post rejection of the results created “visible victims”—that is, those who studied for the test, passed, and whose hard work was discarded [88]. After the city gave the test, it needed strong evidence that the test would be invalidated if the city were sued for disparate impact and lose,
14

because otherwise those who had passed would be harmed. The Court did not see such evidence and so did not allow the city to reject the results.
Answer to Q3. Designing a screening system is quite diﬀerent than what happened in Ricci. Ricci was about a later stage of employment (i.e., promotions), and it involved a test for which many test-takers had prepared, including spending money on test preparation aid. The advantage of building a screening system is that the actions are ex-ante, and the system is not a test for which someone can prepare [36]. Unlike in Ricci, where applicants were seen as having an expectation that a potentially valid test for which they could study be accepted, designing and using a screening algorithm occurs at an earlier stage of the hiring process where no hiring or promotion decision is made. Thus in designing a screening algorithm, one might observe selections over time and change the parameters to create a more representative sample of qualiﬁed candidates, including making adjustments during the “training” of the algorithm. These steps are analogous to the design steps—such as making overt choices and oversampling at every stage to ensure that the test did “not unintentionally favor white candidates”—taken by New Haven and of which the Supreme Court wrote with approval [22]. In other words, designing and vetting a screening system to ensure that the results are not having discriminatory outcomes should be legal.
Recall that one of the goals of Title VII is to reduce, if not eliminate, “unnecessary barriers to employment.” The Ricci Court did not “question an employer’s aﬃrmative eﬀorts to ensure that all groups have a fair opportunity” at a given stage of the hiring process. An employer is allowed to examine “how to design. . .[a] practice in order to provide a fair opportunity for all individuals, regardless of their race” before deploying it [26]. Designing a screening algorithm is by its nature an ex-ante event for which a candidate cannot prepare in the way one might for a test.
In short, if Question 2’s requirement is met, an employer should be able to develop a bias-aware algorithm to avoid disparate impact. Of course, we still need to address the validity of the new practice and what is allowed in its design, which brings us to the next question, which we partially answer through the lens of the poset approach.
Q4: What is allowed in the design of a bias-aware algorithm? Can it be designed to improve the yield of whom to interview?
This is one of the grand challenges in this area. Let us focus our attention to the proposed poset approach, and draw arguments from the Supreme Court’s decision in Johnson. The key to using a bias-aware algorithm such as the poset approach of Salem and Gupta is to establish the facts and evidence of a need to address bias (or more generally, inconsistencies in the data) as set forth above, and then to build a plan that assesses individuals rather than setting up a purely number-driven process with quotas for each category [9]. If a plan is “blind hiring,” that is, dictates hiring “solely by reference to statistics” or “by reﬂexive adherence to a numerical standard,” the plan is not likely to be allowed [11]. But, if a plan takes “numerous factors. . .into account in making hiring decisions, including speciﬁcally the qualiﬁcations of [all] applicants for particular jobs,” the plan may take a protected class into account as part of the overall evaluation [9]. In that sense, the protected class status “may be deemed a ‘plus’ in a particular applicant’s ﬁle, yet it does not insulate the individual from comparison with all other candidates for the available seats” [12, 3].
Comparison does not require pure, numeric ranking; indeed, that might tip into the sort of “blind hiring” that is disfavored. As the Sixth Circuit stated, the “practice of rank-order hiring from a single list grouping together males and females was impermissible under Title VII because the City could not establish that higher scores on the test meant better job performance.” [15]. The
15

Second Circuit has explained that evaluations should be suﬃciently correlated with job performance to induce a rank ordering, where the quantiﬁcation of “suﬃciently correlated” may depend on the extent of adverse impact of the evaluation metric [5]. The Sixth Circuit additionally asserted that a certain cognitive ability test could not be used as the sole basis for a rank-ordering despite being predictive of job performance, since the test failed to measure certain qualities of interest. Rank orderings based on evaluations should therefore not be thought of as implicit to a screening practice, but instead as a design choice which must be justiﬁed [15].
Discretion in comparison of candidates is allowed when it is part of the overall, individual assessment. For example, in Johnson v. Transportation Agency of Santa Clara County, two candidates were deemed well-qualiﬁed based on a range of metrics, such as experience, background, and test scores taken together. But each candidate had diﬀerences within a given metric. One had more clerical work and more road maintenance work; the other had more experience at a speciﬁc part of the business. As for test scores, the man scored 75 on the interview portion of the assessment and the woman scored 73. The employer had set 70 as the minimum threshold for the interview and seven applicants crossed the 70 mark. The range of acceptable scores was 70 to 80 [7]. The woman was given the promotion over the man who had the higher score. Because the scores were within the range of acceptable scores and the ﬁnal hiring manager looked at a set of metrics with gender as “but one of numerous factors he took into account in arriving at his decision,” the plan’s incorporation of bias-awareness, here gender, was allowed [12].
Other cases also acknowledge the need for an approach beyond using an absolute score or ranking. Given problems with rank-ordering, the Second Circuit of Appeals has allowed a rather coarse approach where an employer may “acknowledge his inability to justify rank-ordering and resort to random selection from within either the entire group that achieves a properly determined passing score, or some segment of the passing group shown to be appropriate” [6]. Courts have also indicated an acceptance for more nuanced methods. For example, the act of “banding,” or considering score ranges instead of singular scores, has been accepted to account for inaccuracies in evaluation. [17, 16]. Although these cases consider banding in a quite limited sense in that scores ranges are centered on original scores and are of uniform length, they support that one might relax the assumption of an absolute ranking of candidates.
In language that tracks the poset approach, the Second Circuit has also acknowledged “that small diﬀerences between the scores of candidates indicate very little about the candidates’ relative merit and ﬁtness” [6]. Thus the court embraced an approach that assessed “a statistical computation of the likely error of measurement inherent” in its exam. The employer then used that measurement to set up zones of candidates clustered by test scores within that error measurement. That practice was seen as a good solution to “insur[e] compliance” with Title VII. The Second Circuit explained, “by creating a more valid method to assess the signiﬁcance of test scores, [the approach] eliminated the central cause of the adverse impact, i.e., the rank-ordering system, while assuring appointments on the basis of merit.” As such, if one is able to use protected information (as in Johnson, or in the context of a valid aﬃrmative action plan [36]), then the banding cases provide guideposts for adopting the poset approach as described in Section 5.
Answer to Q4. 1. An algorithmic approach should be allowed. A takeaway from Johnson and the cases on banding and rank-ordering is that a precise numerical score is not necessarily indicative of an applicant’s potential, and courts welcome approaches that better compare candidates. Thus, score ranges can be used as part of an applicant-screening procedure. This supports the use of score ranges to account for uncertainties in evaluations, as outlined in Section 3.
16

Further, note that incorporating the poset model of bias is not the same thing as normalizing distributions of scores across groups. When we normalize scores across groups, we are essentially transforming all scores so that group-speciﬁc distributions look similar, and this process results in a full ranking of applicants. In contrast, the poset approach intentionally does not reduce each applicant to a number and allows for incomparabilities between applicants. This allows for a more individual treatment of candidates, where uncertainty in rankings can be acknowledged. The result is that applicants are assessed as individuals, potentially in a more mathematically sound way.
2. There are rules about when bias-aware algorithms can be used. Recall that the stage at which an entity uses bias-aware algorithms matters. In the promotion context of Johnson, the Court gave a further reason the plan was allowed. Unlike Ricci, where applicants were seen as having an expectation that a potentially valid test for which they could study be accepted, there was “no absolute entitlement” to the position at issue in Johnson. The entity had seven qualiﬁed and eligible applicants, and choosing one over the other “unsettled no legitimate, ﬁrmly rooted expectation” of any of the candidates. By extension, a bias-aware applicant-screening plan that used a protected class as part of an overall assessment then had all selected applicants compete on the same metrics should be allowed under the law.
3. There are legal rules on the goals of any hiring plan. The law respects plans that seek to remedy an imbalance and that do not set aside positions for a given group while also conducting annual reviews of goals as it fashions future rounds of hiring and promotion [14]. One may work “to attain a balanced work force, not to maintain one” [13].
The Johnson Court also noted with approval that “the Plan sought annually to develop even more reﬁned measures of the under-representation in each job category that required attention” [10]. This idea of not maintaining a balanced workforce reﬂects the idea that an entity cannot use a plan that sets up quotas to maintain balance based purely on class statuses. By extension, suppose balance is achieved in a company through bias-aware methods, and they notice this by continuous monitoring of their hiring practices (in a sense, returning to Question 1). The company may then have to stop using bias-aware methods, even if demographic imbalance persists in the general workforce for that line of work.
4. The poset approach does not impose quotas. In contrast to methods described in some recent work (e.g., [57]), using score ranges (i.e., the poset approach) instead of raw scores does not set up a quota system.5 When using the poset approach, selection rates may be inﬂuenced by protected information (e.g., when accounting for observed, group-speciﬁc biases), but such protected information is not necessarily a determining factor in selection decisions. For example, the poset approach could result in a set of candidates which is less demographically proportional than what raw scores might produce dependent on the data and ascertained uncertainty in the data (see, e.g., Figures 7-8 in Appendix B) or could result in more demographically proportional selections (see, e.g., Figures 5-6). It simply accounts for the uncertainty in the candidate evaluations.
5There is some debate on whether quotas are allowed at a screening stage. One reason to not use quota-based approaches at screening stages is to avoid setting up a pre-destined pool of candidates who will be later rejected by the system. The poset approach accounts for uncertainties in candidate evaluations, and makes selections based on the possibility of a candidate being qualiﬁed, and this process can be irrespective of demographic features. This is attractive from a legal perspective at any stage in the hiring pipeline.
17

7 Conclusion
We summarized recent work in the context of hiring, with a focus on screening algorithms. We highlighted the seeming paradox of mathematics, law and practice that a company might observe workforce imbalance due to its past practices, but the solutions to correct for this imbalance are either at a contradiction with mathematics or anti-discrimination law. The new poset-based approach (analyzed with a streaming model in [90]) provides a framework for incorporating uncertainties in rankings into a candidate-screening practice which allows, for example, hiring committees to base decision on conﬁdence intervals of ability scores. This approach can potentially be legally justiﬁed based on past disparate impact and can be adjusted over time as the data grows and hiring goals evolve; and thus can help avoid having a static plan as the law requires.
No approach, however, is a ﬁx-all solution. The poset approach cannot discount for undetectable errors undetectable, or modeling errors due to missing data. The ranges of the intervals impact the quality of selections. Further, two diﬀerent mathematical approaches could be used to deﬁne score ranges for candidates and result in diﬀerent sets of selected candidates. A legal dispute may require addressing which one of these approaches is more valid. Further, there is an “are we there yet?” issue built into the Supreme Court’s rulings. That is, it may be unclear at which point a workforce becomes “balanced” and the current plan must be replaced. Although the poset approach is adaptive, detecting where there is no longer any impact of societal biases in the data is non-trivial and we leave this as an open question.
For any intervention in an existing framework, one has to consider if the intervention is serving those for whom it is designed [42]. Partially ordered sets that are interval-based might create an impression that certain underrepresented minorities carry high uncertainty in their potentials and as a result, lead a risk-averse hiring committee to reject those candidates. On the contrary, the poset approach can highlight missed opportunities in representation in the hiring pipeline. Taking uncertainties into account can expand and improve the talent pool to include candidates who are qualiﬁed and would have been competitive had there been no bias in the data. Thus, we believe that the analysis presented here can pave the way forward for hiring qualiﬁed candidates in a fair way in the evolving legal landscape.
Acknowledgments
The authors thank Jason Bent, Justin Biddle, Kimberly Houser, Pauline Kim, Orly Lobel, and the participants of the Data, Law, and Ethics Virtual Conference hosted by the University of Indiana, Kelly School of Business, as well as the participants of the Privacy Law Scholars Conference 2021 for their helpful comments on an earlier draft of this work.
References
[1] Griggs v. Duke Power Co., 401 U.S. 424, 431. 1971.
[2] Griggs v. Duke Power Co., 401 U.S. 424, 432. 1971.
[3] Regents of University of California v. Bakke, 438 U.S. 265, 317. 1978.
[4] EEOC Guidelines on Aﬃrmative Action, 29 C.F.R. § 1608.1(c). 1979.
18

[5] Guardians Ass’n of New York City v. Civil Serv, 630 F.2d 79 (2d Cir.). 1980. [6] Kirkland v. N.Y. State Dep’t of Correctional Serv., 711 F.2d 1117, 1133 (2d Cir.). 1983. [7] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 623-624. 1987. [8] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 627. 1987. [9] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 631. 1987. [10] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 635. 1987. [11] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 636-637. 1987. [12] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 638. 1987. [13] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 639. 1987. [14] Johnson v. Transportation Agency, Santa Clara Cty., 480 U.S. 616, 640-641. 1987. [15] Brunet v. City of Columbus, Ohio, 58 F.3d 251, 255 (6th Cir.). 1995. [16] Boston Police Superior Oﬃcers Fed’n v. City of Boston, 147 F.3d 13. 1998. [17] Bradley v. City of Lynn, 403 F. Supp. 2d 161. 2005. [18] Title VII, 29 CFR Parts 1600, 1607, 1608. 2006. [19] Ricci v. DeStefano, 557 U.S. 557, 632. 2009. [20] Ricci v. DeStefano, 557 U.S. 557. 2009. [21] Ricci v. DeStefano, 557 U.S. 557, 564. 2009. [22] Ricci v. DeStefano, 557 U.S. 557, 565. 2009. [23] Ricci v. DeStefano, 557 U.S. 557, 567. 2009. [24] Ricci v. DeStefano, 557 U.S. 557, 580-581. 2009. [25] Ricci v. DeStefano, 557 U.S. 557, 583. 2009. [26] Ricci v. DeStefano, 557 U.S. 557, 585. 2009. [27] Bostock v. Clayton County, 590 U.S. . 2020. [28] § 2000e-2(k)(1)(A). 42 U.S.C. [29] Mckinsey’s online application faqs: Careers, Accessed January 17, 2022. [30] J. Angwin, N. Scheiber, and A. Tobin. Dozens of companies are using facebook to exclude
older workers from job ads. ProPublica, December, 2017. [31] S. Barocas. Data mining and the discourse on discrimination. In Data Ethics Workshop,
Conference on Knowledge Discovery and Data Mining, pages 1–4, 2014.
19

[32] S. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019. http://www.fairmlbook.org.
[33] S. Barocas and A. D. Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.
[34] D. Bass and J. Eidelson. Microsoft plan to add black executives draws u.s. labor inquiry. Seattle Times, October 6, 2020.
[35] A. B. Batastini, A. D. Bolan˜os, R. D. Morgan, and S. M. Mitchell. Bias in hiring applicants with mental illness and criminal justice involvement: A follow-up study with employers. Criminal Justice and Behavior, 44(6):777–795, 2017.
[36] J. R. Bent. Is algorithmic aﬃrmative action legal. Geo. LJ, 108:803, 2019.
[37] A. Blum and K. Stangl. Recovering from biased data: Can fairness constraints improve accuracy? In Symposium on Foundations of Responsible Computing (FORC), volume 1, 2020.
[38] M. Bogen and A. Rieke. Help wanted: An examination of hiring algorithms, equity, and bias. 2018.
[39] A. Caliskan, J. J. Bryson, and A. Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186, 2017.
[40] L. E. Celis, A. Mehrotra, and N. K. Vishnoi. Interventions for ranking in the presence of implicit bias. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 369–380, 2020.
[41] R. Christensen. Analysis of variance, design, and regression: applied statistical methods. Page 173. CRC Press, 1996.
[42] S. Corbett-Davies and S. Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018.
[43] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 797–806. ACM, 2017.
[44] V. Das Swain, K. Saha, M. D. Reddy, H. Rajvanshy, G. D. Abowd, and M. De Choudhury. Modeling organizational culture with workplace experiences shared on glassdoor. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–15, 2020.
[45] J. Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. Reuters, Oct 2018.
[46] M. De-Arteaga, A. Romanov, H. Wallach, J. Chayes, C. Borgs, A. Chouldechova, S. Geyik, K. Kenthapadi, and A. T. Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 120–128, 2019.
20

[47] D. R. Desai and J. A. Kroll. Trust but verify: A guide to algorithms and the law. Harv. JL & Tech., 31:1, 2017.
[48] S. Dewan. How businesses use your sats. New York Times, Mar 2014.
[49] L. Dixon, J. Li, J. Sorensen, N. Thain, and L. Vasserman. Measuring and mitigating unintended bias in text classiﬁcation. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73, 2018.
[50] E. Dixon-Roman, H. Everson, and J. Mcardle. Race, Poverty and SAT Scores: Modeling the Inﬂuences of Family Income on Black and White High School Students’ SAT Performance. Teachers College Record, 115, 05 2013.
[51] P. Domingos. The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books, New York, NY, 1st. edition, 2015.
[52] C. Duﬀy. Adidas says at least 30% of new us positions will be ﬁlled by black or latinx people. CNN Business, June 9, 2020.
[53] C. Duﬀy. In the face of a cultural reckoning, it turns out massive corporations can move fast and ﬁx things. CNN Business, June 21, 2020.
[54] C. Duﬀy. Plans at microsoft and wells fargo to increase black leadership are under scrutiny from the labor dept. CNN Business, October 7, 2020.
[55] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214–226, 2012.
[56] B. Edelman, M. Luca, and D. Svirsky. Racial discrimination in the sharing economy: Evidence from a ﬁeld experiment. American Economic Journal: Applied Economics, 9(2):1–22, 2017.
[57] V. Emelianov, N. Gast, K. P. Gummadi, and P. Loiseau. On fair selection in the presence of implicit variance. In Proceedings of the 2020 ACM Conference on Economics and Computation, 2020.
[58] C. L. Estlund. Putting grutter to work: Diversity, integration, and aﬃrmative action. Berkeley J. of Labor and Employment, 26:1, 2005.
[59] Y. Faenza, S. Gupta, and X. Zhang. Impact of bias on school admissions and targeted interventions, 2020.
[60] M. J. Fischer and D. S. Massey. The eﬀects of aﬃrmative action in higher education. Social Science Research, 36(2):531–549, 2007.
[61] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, and D. Roth. A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the conference on fairness, accountability, and transparency, pages 329–338, 2019.
[62] FTC Report. Big data: a tool for inclusion or exclusion? Federal Trade Commission, January 2016.
21

[63] R. Goodman. Why amazon’s automated hiring tool discriminated against women, 2018. Published Oct. 12, 2018. Last accessed Jun. 6, 2019.
[64] A. Griswold. Why major companies like amazon ask job candidates for their sat scores. Business Insider, Mar 2014.
[65] C. Hanks. Technology and values: Essential readings, page 7. John Wiley & Sons, 2009.
[66] R. N. Hanna and L. L. Linden. Discrimination in grading. American Economic Journal: Economic Policy, 4:146–68, 02 2012.
[67] A. Hann´ak, C. Wagner, D. Garcia, A. Mislove, M. Strohmaier, and C. Wilson. Bias in online freelance marketplaces: Evidence from taskrabbit and ﬁverr. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing, pages 1914–1933, 2017.
[68] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315–3323, 2016.
[69] M. E. Heilman, C. J. Block, and P. Stathatos. The aﬃrmative action stigma of incompetence: Eﬀects of performance information ambiguity. Academy of Management Journal, 40(3):603– 625, 1997.
[70] K. A. Houser. Can ai solve the diversity problem in the tech industry? mitigating noise and bias in employment decision-making. Stanford Tech. L. Rev., 22:290, 2019.
[71] Jobscan. Applicant tracking systems, Accessed Sept. 11, 2020. Available at https://www. jobscan.co/applicant-tracking-systems.
[72] F. Kamiran, T. Calders, and M. Pechenizkiy. Discrimination aware decision tree learning. In 2010 IEEE International Conference on Data Mining, pages 869–874. IEEE, 2010.
[73] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Fairness-aware classiﬁer with prejudice remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35–50. Springer, 2012.
[74] M. Kearns and A. Roth. The Ethical Algorithm: The Science of Socially Aware Algorithm Design. oxford University Press, New York, NY, 1st. edition, 2020.
[75] M. Kempner. Georgia’s big businesses reveal staﬀ — and management — diversit. The Atlanta Constitution Journal, October 8, 2021.
[76] P. Kim. Auditing algorithms for discrimination. U. Pa. L. Rev. Online, 166:189, 2017.
[77] P. Kim. Data-driven discrimination at work. Wm. & Mary L. Rev., 58:8657, 2017.
[78] P. T. Kim. Manipulating opportunity. Va. L. Rev., 106:867, 2020.
[79] J. Kleinberg and S. Mullainathan. Simplicity creates inequity: implications for fairness, stereotypes, and interpretability. In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 807–808, 2019.
22

[80] J. M. Kleinberg and M. Raghavan. Selection problems in the presence of implicit bias. In 9th Innovations in Theoretical Computer Science Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, pages 33:1–33:17, 2018.
[81] K. Lum and W. Isaac. To predict and serve? Signiﬁcance, 13(5):14–19, 2016.
[82] M. MacCarthy. Standards of fairness for disparate impact assessment of big data algorithms. Cumberland L. Rev., 48:102, 2017.
[83] J. March. Exploration and exploitation in organizational learning. Organizational Science, 2:71, 1989.
[84] W. Miao and J. L. Gastwirth. Properties of statistical tests appropriate for the analysis of data in disparate impact cases. Law, Probability and Risk, 12(1):37–61, 2013.
[85] T. S. Mohr. Why women don’t apply for jobs unless they’re 100% qualiﬁed. Harvard Business Review, 25, 2014.
[86] C. Moss-Racusin, D. , V. Brescoll, M. Graham, and J. Handelsman. Science faculty’s subtle gender biases favor male students. Proceedings of the National Academy of Sciences, 109:16474–16479, 09 2012.
[87] U. D. of Labor Oﬃce of Federal Contract Compliance Programs. U.s. department of labor and microsoft corp. enter agreement to resolve alleged hiring discrimination aﬀecting 1,229 applicants in four states. CNN Business, September 18, 2020.
[88] R. Primus. The future of disparate impact. Mich. L. Rev., 108:1341, 2010.
[89] M. Raghavan, S. Barocas, J. Kleinberg, and K. Levy. Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 469–481, 2020.
[90] J. Salem and S. Gupta. Closing the gap: Group-aware parallelization for online selection of candidates with biased evaluations. In International Conference on Web and Internet Economics (WINE). Springer, 2020. Under major revision at Management Science, 2021.
[91] J. S´anchez-Monedero, L. Dencik, and L. Edwards. What does it mean to “solve” the problem of discrimination in hiring? social, technical and legal perspectives from the uk on automated hiring systems. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ’20, page 458–468, New York, NY, USA, 2020. Association for Computing Machinery.
[92] S. D. Schlachter and J. R. Pieper. Employee referral hiring in organizations: An integrative conceptual review, model, and agenda for future research. Journal of Applied Psychology, 2019.
[93] O. Schwartz. Untold history of AI: Algorithmic bias was born in the 1980s. IEEE Spectrum, 2019.
[94] J. Shields. Over 98% of fortune 500 companies use applicant tracking systems (ats), 2018.
23

[95] G. Smedinghoﬀ. The art, philosophy and science of data. Contingencies May/June, pages 37–40, 2007.
[96] M. G. Sobol and C. J. Ellard. Measures of employment discrimination: A statistical alternative to the four-ﬁfths rule. Industrial Relations Law Journal, pages 381–399, 1988.
[97] C. M. Steele and J. Aronson. Stereotype threat and the intellectual test performance of african americans. Journal of Personality and Social Psychology, 69:797–811, 1995.
[98] C. A. Sullivan. Disparate impact: Looking past the desert palace mirage. William & Mary Law Review, 2005.
[99] N. Totenberg. Supreme court delivers major victory to lgbtq employees, 2020.
[100] J. Wang and N. Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS Conference proceedings, 2019.
[101] J. Wang, I. Stelmakh, Y. Wei, and N. B. Shah. Debiasing evaluations that are biased by evaluations. arXiv preprint arXiv:2012.00714, 2020.
[102] A. Wold and C. Wenner˚as. Nepotism and sexism in peer review. Nature, 387(6631):341–343, 1997.
[103] S. Yucer, S. Akc¸ay, N. Al-Moubayed, and T. P. Breckon. Exploring racial bias within face recognition via per-subject adversarially-enabled data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 18– 19, 2020.
[104] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013.
A Constructing the Poset
As discussed in Section 5, a poset is a partially ranked set of candidates, and the poset approach is the process of forming a partial ranking and subsequently making selections based on the ranking. The purpose of this approach is to minimize the eﬀect of bias and other inaccuracies on selection decisions. In this section, we discuss several ways in which posets can be constructed.
One natural way to construct a partial ranking of applicants is to ﬁrst form conﬁdence intervals around raw scores, and then extract ordinal information from non-intersecting intervals (see, e.g., Examples 1 and 2). There are several factors that can be taken into account when forming a partial ranking in such a way. An evaluation metric itself can produce inaccurate scores, which can inform the lengths of intervals. Group-speciﬁc biases and error rates can also inform varying interval lengths by group. Evaluation metrics can also be biased against some groups, and intervals can be designed so as to mitigate known biases. To illustrate some ways in which the poset approach can be implemented, we consider an example from [90].
24

Example case study. In [90], Salem and Gupta analyzed the Aspiring Minds dataset, in which male and female job seekers had similar distributions of computer science test scores. As the job seekers were all in computer science ﬁelds, computer science test scores were taken as proxies for hireability. They found that by performing a linear regression on features (including gender and test scores), female6 applicants received scores that were 16.95 points lower than those of male applicants with all other attributes equal. This does not mean that the regression model will underestimate the score of every female; rather, it means that the regression model has “learned” a trend in the data that will be harmful to some female applicants.7 Since the available data on applicants indicated that mean scores were similar across gender, this learned trend can be viewed as a bias which should be corrected. Importantly, even if gender were scrubbed from the data, a machine learning model might still pick up on these trends via proxy variables, so scrubbing information is in general not advisable [79, 55].
Given that there is some data available on candidates, and a prediction mechanism that scores candidates, how can a partial ranking of the candidates be constructed in a mathematically sound way?8 In what follows, we outline three possible approaches for constructing ranges of scores for each candidate. However, we note that other statistical approaches might be applicable as well.
1. Data-centric approach to mitigate disparate error rates: It is desirable that as the errors in evaluations decrease, candidates’ score ranges shrink as well. This is statistically linked to the amount of data in the following way: a machine learning model trained on noisy data tends to have smaller errors as the amount of data increases. Therefore, in this approach, we will ensure that the size of the range decreases as the amount of the training data for a candidate increases. Formally, one can set the interval length to be r(n), where n is the number of points in the training data and r some function that decays with n. The value of r(n) should be an estimate of how much error is in the model after observing n training points, which can be calculated depending on the model class and the extent of noise; for exa√mple, when the true relationship is linear, it makes sense to choose r(n) to be proportional to 1/ n [41].9 Adopting these conﬁdence intervals gives applicants the beneﬁt of the doubt, giving room to account for imperfections in the evaluation mechanism.
As noted earlier, however, error rates can diﬀer by group. The presence of an imbalanced training dataset (that is, one with an underrepresented group) can lead to higher error rates for the underrepresented group. This problem mathematically justiﬁes having intervals of diﬀerent lengths for diﬀerent groups, dependent on how much uncertainty exists within each group. For example, if there were n1 applicants from one group and n2 applicants from another group in the training data, then one might choose intervals of length r(n1) for the former and intervals of length r(n2) for the latter.
In the context of the example case study, this would entail partitioning the pool into some
6All entries in the dataset listed a gender of male or female. 7One of the reasons for this might be imbalance of data points in the male versus female group. For example, the average score in the training set among all female applicants was 455.30, and the average score among all male applicants was 478.05 (a slightly larger discrepancy than in the entire dataset). 8In the example above, the scoring mechanism was a simple linear regression. But in general, it can involve natural language processing of the text in r´esum´es, a neural network built on top of that, or any other machine learning technique. 9If the true relationship between a candidate’s features and hireability is linear, and the noise is independently drawn from identical normal distribu√tions, then the linear regression estimation of hireability is unbiased and has standard deviation proportional to 1/ n. The proportionality constant depends on the distribution of the datapoints and the variance of the noise [41].
25

number of groups and setting diﬀering interval lengths dependent on group size. Since a bias was detected against female candidates, it may make sense to consider groups by gender, in which case the interval length for a female candidate would be proportional to √ 1 , and analogously
#females for men. There are, however, other ways to partition the pool. Each person is mapped to a point in some high-dimensional space, and clusters can be formed based on which points are close to each other with respect to some metric. We can then similarly set interval lengths based on cluster sizes.
If one insists on using equal interval lengths for all applicants, then using a length of, say, min{r(n1), r(n2)}, would give each applicant the length corresponding to the error rate in the largest group. While this last method would arguably give overly narrow intervals to members of the smaller group, it still gives more beneﬁt of the doubt to applicants (regardless of group) than using intervals of length r(n).
2. Data-centric approach to mitigate group-speciﬁc biases: When there is a known bias against groups of individuals (either due to the evaluation metric or some other systemic cause), intervals can be designed in a targeted way to correct for these errors. This might involve individualized interval lengths and translations to account for observed or inferred bias. Returning to the example case study, we will outline how Salem and Gupta accounted for the observed bias discussed above. They had hypothesized a multiplicative bias in the female group (as is assumed in the group model of bias of Kleinberg and Raghavan [80]), meaning that a “true” ability s∗ of a female applicant would produce a score of s∗/β, for some bias factor β. They therefore sought to convert this additive error of 16.95 into a multiplicative error. Note that an error of 16.95 for a low-scoring individual produces a larger multiplicative error than does an additive error of 16.95 on a high-scoring individual. They therefore found upper and lower estimates on the multiplicative bias factor by observing the multiplicative bias factor of a low-scoring female applicant (mean −0.5 standard deviations) and a high-scoring female applicant (mean +0.5 standard deviations). The parameter 0.5 for standard deviation was chosen so that corresponding score ranges of female job seekers were (1) small enough that comparisons could still be made between enough individuals, and (2) large enough that bias was mitigated.10 Once a lower multiplicative bias factor βlow and an upper multiplicative bias fact βhigh were calculated, the interval for a female applicant of score s would be [sβlow, sβhigh]. Using these intervals instead of raw scores gives female candidates more “beneﬁt of the doubt,” which the authors considered appropriate given the observed bias. Ultimately, they showed that this method mitigated gender bias more eﬀectively than a simpler gender-aware algorithm.
If feedback is available on previously evaluated applicants, then simpler approaches may be appropriate. For example, if it is known that for a particular group, the evaluation metric has a standard deviation of 1 and tends to underestimate scores by two points, then a score of 83 in that group could justiﬁably be translated to an interval of [84, 86].
3. Human-centric approach: When there is direct human involvement in evaluating applicants (e.g., through interviews), biases and inaccuracies might still persist. One might think that the poset approach would not help in this situation, but it can. Similar to the data-centric approach, one can reduce this error through repeated independent evaluations (i.e., wisdom of crowds [95]). In this case, a single applicant can be interviewed by a diverse committee, each of whose members
10The process of tuning such a parameter is an important step in the design of a task-speciﬁc algorithm, and often involves trial-and-error experimentation on training data.
26

Cutoﬀ
-4 -3 -2 -1 0 1 2 3 4 Raw scores
-4 -3 -2 -1 0 1 2 3 4 Adjusted score ranges
Figure 5: (left) Example of score distributions (blue: Group 1, orange: Group 2) and (right) potential score ranges
for candidates from these distributions. Suppose a hiring committee wants to select two of the applicants represented in the right plot. If only the raw evaluations (the centers of the intervals) are used to make these decisions, then only the two high-scoring Group 1 candidates could be selected, as they are the only applicants meeting the cutoﬀ. However, if score ranges are considered, then the highest-scoring Group 2 candidate meets the cutoﬀ as well. In this example, adopting the poset method results in a more diverse slate of candidates meeting the cutoﬀ, vis-`a-vis using raw scores.
scores the applicant. These scores can then be used to form score ranges for each applicant (e.g., the minimum score to the maximum score, or the ﬁrst to the third quartile, etc.). An applicant’s score range can be decreased through further discussion by the committee until the interval is suﬃciently small to allow for reasonably many comparisons in the applicant pool.11 If there were some way to obtain repeated evaluations (e.g., through multiple human evaluators, or access to multiple scoring algorithms) of the job seekers in the example case study, then this human-centric approach could be applied there as well. For each person in the dataset, we could repeatedly obtain evaluations until we can be conﬁdent in their score (say, up to 5 points). If certain groups (e.g., gender groups, or clusters obtained by a machine learning algorithm) experience higher variance in evaluations, then the same number of evaluations might result in conﬁdence intervals of diﬀerent lengths for diﬀerent groups.
Accounting for other sources of errors. There are sources of errors which are intrinsic to machine learning and predictions, and interval lengths can also be designed to mitigate these errors. For example, if a highly informative or causal variable is absent from the data, then predictions can suﬀer (this type of error is called Bayes error ). Another type of error, called approximation error, describes error resulting from the mismatch between the true relationship between applicant features and ability, and the class of models that can be produced by the algorithm. Both of these types of errors are independent of the size of the training data, so we generally do not expect these errors to decay with time. In an attempt to account for this sort of intrinsic error, one could impose a minimum interval length for all candidates, where the minimum length depends on the accuracy of the evaluation metric.
11Such practices are already prevalent within hiring committees and program committees for conferences such as ICLR, NeurIPS and WWW.
27

Cutoﬀ
-4 -3 -2 -1 0 1 2 3 4 Raw scores
-4 -3 -2 -1 0 1 2 3 4 Adjusted score ranges
Figure 6: (left) Example of (somewhat unusual) score distributions (blue: Group 1, orange: Group 2) and (right) potential score ranges for candidates from these distributions. Suppose a hiring committee wants to select two of the applicants corresponding to the right plot. If only the raw evaluations (the centers of the intervals) are used to make these decisions, then only the two highscoring Group 2 candidates could be selected, as they are the only applicants meeting the cutoﬀ. However, if score ranges are considered, then the two highest-scoring Group 1 candidates meet the cutoﬀ as well. From this example, we see that adopting the poset approach can be beneﬁcial to the majority group as well and does not routinely advantage the lower-mean group (in this case, Group 2).
B Poset Approach Diagrams
In Section 5, we discussed how the poset approach can be used in the screening of applications. To determine the appropriateness of this approach to screening, it is of legal, ethical, and utilitarian importance to understand the eﬀect of the poset approach on applicants. While the practical eﬀect of the poset approach will depend on context and precise implementation, it is informative to observe its eﬀect on artiﬁcial data.
The four examples shown in this section (Figures 5-8) compare candidate slates produced by a cutoﬀ on raw scores versus on score ranges. To illustrate the eﬀect of using score ranges over groups, we consider two groups, where Group 2 has a lower mean evaluation and a larger interval length (perhaps due to dataset imbalance issues as discussed earlier). In Figure 5, we see how using score ranges instead of raw scores can increase the selection rate of the minority group. However, the use of score ranges does not necessarily beneﬁt the minority group in general. Figures 6-8 consider the same scenario as Figure 5 but with diﬀerent distributional assumptions. Of note is how the use of score ranges can beneﬁt the majority or minority group, and can beneﬁt the low-scoring or the high-scoring group. This point, in particular, means that the poset approach does not inherently constitute a quota system, which is an important feature with respect to anti-discrimination law (see Section 6.2).
28

Cutoﬀ
-4 -3 -2 -1 0 1 2 3 4 Raw scores
-4 -3 -2 -1 0 1 2 3 4 Adjusted score ranges
Figure 7: (left) Example of score distributions (blue: Group 1, orange: Group 2) and (right) potential score ranges for candidates from these distributions. Suppose a hiring committee wants to select two of the applicants corresponding to the right plot. If only the raw evaluations (the centers of the intervals) are used to make these decisions, then only the two highest-scoring Group 1 candidates could be selected, as they are the only applicants meeting the cutoﬀ. If the score ranges are considered, then the four highest-scoring Group 1 candidates meet the cutoﬀ. This example shows that adopting the poset approach does not necessarily increase the selection rate for the group with the lower mean score, and that the poset approach does not necessarily constitute a quota system.
Cutoﬀ
-4 -3 -2 -1 0 1 2 3 4 Raw scores
-4 -3 -2 -1 0 1 2 3 4 Adjusted score ranges
Figure 8: (left) Example of (somewhat unusual) score distributions (blue: Group 1, orange: Group 2) and (right) potential score ranges for candidates from these distributions. Suppose a hiring committee wants to select two of the applicants corresponding to the right plot. If only the raw evaluations (the centers of the intervals) are used to make these decisions, then one Group 1 and one Group 2 candidate will be selected, as they are the only applicants meeting the cutoﬀ. In this case, demographic parity is achieved, as both groups have equal size. However, if score ranges are considered, then two additional Group 1 candidates meet the cutoﬀ as well. This example shows that adopting the poset approach does not necessarily make the new candidate slate (i.e., those meeting the cutoﬀ) more representative compared to using raw scores—indeed, in this example, adopting the poset approach moves the new candidate slate farther away from demographic parity.
29

