arXiv:1806.06237v2 [stat.ML] 15 Nov 2019

PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review
Ivan Stelmakh, Nihar B. Shah and Aarti Singh
School of Computer Science Carnegie Mellon University {stiv,nihars,aarti}@cs.cmu.edu
Abstract We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-ﬂow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent diﬃculty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.
1 Introduction
Peer review is the backbone of academia. In order to provide high-quality peer reviews, it is of utmost importance to assign papers to the right reviewers (Thurner and Hanel, 2011; Black et al., 1998; Bianchi and Squazzoni, 2015). Even a small fraction of incorrect reviews can have signiﬁcant adverse eﬀects on the quality of the published scientiﬁc standard (Thurner and Hanel, 2011) and dominate the beneﬁts yielded by the peer-review process that may have high standards otherwise (Squazzoni and Gandelli, 2012). Indeed, researchers unhappy with the peer review process are somewhat more likely to link their objections to the quality or choice of reviewers (Travis and Collins, 1991).
We focus on peer-review in conferences where a number of papers are submitted at once. These papers must simultaneously be assigned to multiple reviewers who have load constraints. The importance of the reviewer-assignment stage of the peer-review process cannot be overestimated; quoting Rodriguez et al. (2007):
“one of the ﬁrst and potentially most important stage is the one that attempts to distribute submitted manuscripts to competent referees.”
Given the massive scale of many conferences such as NeurIPS and ICML, these reviewer assignments are largely performed in an automated manner. For instance, NeurIPS 2016 assigned 5 out of 6 reviewers per paper using an automated process (Shah et al., 2017). This problem of automated reviewer assignments forms the focus of this paper.
1

Various past studies show that small changes in peer review quality can have far reaching consequences (Thorngate and Chowdhury, 2014; Squazzoni and Gandelli, 2012) not just for the papers under consideration but more generally also for the career trajectories of the researchers. These long term eﬀects arise due to the widespread prevalence of the Matthew eﬀect (“rich get richer”) in academia (Merton, 1968).
It is also known (Travis and Collins, 1991; Lamont, 2009) that works that are novel or not mainstream, particularly those interdisciplinary in nature, face signiﬁcantly higher diﬃculty in gaining acceptance. A primary reason for this undesirable state of aﬀairs is the absence of suﬃciently many good “peers” to aptly review interdisciplinary research (Porter and Rossini, 1985).
These issues strongly motivate the dual goals of the reviewer assignment procedure we consider in this paper — fairness and accuracy. By fairness, we speciﬁcally consider the notion of max-min fairness which is studied in various branches of science and engineering (Rawls, 1971; Lenstra et al., 1990; Hahne, 1991; Lavi et al., 2003; Bonald et al., 2006; Asadpour and Saberi, 2010). In our context of reviewer assignments, max-min fairness posits maximizing the review-quality of the paper with the least qualiﬁed reviewers. The max-min fair assignment guarantees that no paper is discriminated against in favor of more lucky counterparts. That is, even the most ambivalent paper with a small number of reviewers being competent enough to evaluate its merits will receive as good treatment as possible. The max-min fair assignment also ensures that in any other assignment there exists at least one paper with the fate at least as bad as the fate of the most disadvantaged paper in the aforementioned fair assignment.
Alongside, we also consider the requirement of statistical accuracy. One of the main goals of the conference peer-review process is to select the set of “top” papers for acceptance. Two key challenges towards this goal are to handle the noise in the reviews and subjective opinions of the reviewers; we accommodate these aspects in terms of existing (Ge et al., 2013; McGlohon et al., 2010; Dai et al., 2012) and novel statistical models of reviewer behavior. Prior works on the reviewer assignment problem (Long et al., 2013; Garg et al., 2010; Karimzadehgan et al., 2008; Tang et al., 2010) oﬀer a variety of algorithms that optimize the assignment for certain deterministic objectives, but do not study their assignments from the lens of statistical accuracy. In contrast, our goal is to design an assignment algorithm that can simultaneously achieve both the desired objectives of fairness and statistical accuracy.
We make several contributions towards this problem. We ﬁrst present a novel algorithm, which we call PeerReview4All, for assigning reviewers to papers. Our algorithm is based on a construction of multiple candidate assignments, each of which is obtained via an incremental execution of max-ﬂow algorithm on a carefully designed ﬂow network. These assignments cater to diﬀerent structural properties of the similarities and a judicious choice between them provides the algorithm appealing properties.
Our second contribution is an analysis of the fairness objective that our PeerReview4All algorithm can achieve. We show that our algorithm is optimal, up to a constant factor, in terms of the max-min fairness objective. Furthermore, our algorithm can adapt to the underlying structure of the given similarity data between reviewers and papers and in various cases yield better guarantees including the exact optimal solution in certain scenarios. Finally, after optimizing the outcome for the most worst-oﬀ paper and ﬁxing the assignment for that paper, our algorithm aims at ﬁnding the most fair assignment for the next worst-oﬀ paper and proceeds in this manner until the assignment for each paper is ﬁxed.
As a third contribution, we show that our PeerReview4All algorithm results in strong statistical guarantees in terms of correctly identifying the top papers that should be accepted. We consider a popular statistical model (Ge et al., 2013; McGlohon et al., 2010; Dai et al., 2012) which assumes existence of some true objective score for every paper. We provide a sharp analysis of the minimax risk in terms of “incorrect” accept/reject decisions, and show that our PeerReview4All algorithm leads to a near-optimal solution.
Fourth, noting that paper evaluations are typically subjective (Kerr et al., 1977; Mahoney, 1977; Ernst and Resch, 1994; Bakanic et al., 1987; Lamont, 2009), we propose a novel statistical model capturing subjective opinions of reviewers, which may be of independent interest. We provide a sharp minimax analysis under this subjective setting and prove that our assignment algorithm PeerReview4All is also near-optimal for this subjective-score setting.
Our ﬁfth and ﬁnal contribution comprises empirical evaluations. We designed and conducted an experiment on the Amazon Mechanical Turk crowdsourcing platform to objectively compare the performance of diﬀerent
2

reviewer-assignment algorithms. The design of the experiment is done carefully to circumvent the challenge posed by the absence of a ground truth in peer review settings, so that we can evaluate accuracy objectively. In addition to the MTurk experiment, we provide an extensive evaluation of our algorithm on synthetic data, provide an evaluation on a reconstructed similarity matrix from the ICLR 2018 conference, and report the results of the experiment on real conference data conducted by Kobren et al. (2019). The results of these experiments highlight the promise of PeerReview4All in practice, in addition to the theoretical beneﬁts discussed elsewhere in the paper. The dataset pertaining to the MTurk experiment, as well as the code for our PeerReview4All algorithm, are available on the ﬁrst author’s website.
The remainder of this paper is organized as follows. We discuss related literature in Section 2. In Section 3, we present the problem setting formally with a focus on the objective of fairness. In Section 4 we present our PeerReview4All algorithm. We establish deterministic approximation guarantees on the fairness of our PeerReview4All algorithm in Section 5. We analyze the accuracy of our PeerReview4All algorithm under an objective-score model in Section 6, and introduce and analyze a subjective score model in Section 7. We empirically evaluate the algorithm in Section 8 using synthetic and real-world experiments. We then provide the proofs of all the results in Section 9. We conclude the paper with a discussion in Section 10.
2 Related literature
The reviewer assignment process consists of two steps. First, a “similarity” between every (paper, reviewer) pair that captures the competence of the reviewer for that paper is computed. These similarities are computed based on various factors such as the text of the submitted paper, previous papers authored by reviewers, reviewers’ bids and other features. Second, given the notion of good assignment, speciﬁed by the program chairs, papers are allocated to reviewers, subject to constraints on paper/reviewer loads. This work focuses on the second step (assignment), assuming the ﬁrst step of computing similarities as a black box. In this section, we give a brief overview of the past literature on both of the steps of the reviewer-assignment process.
Computing similarities. The problem of identifying similarities between papers and reviewers is wellstudied in data mining community. For example, Mimno and McCallum (2007) introduce a novel topic model to predict reviewers’ expertise. Liu et al. (2014) use the random walk with restarts model to incorporate both expertise of reviewers and their authority in the ﬁnal similarities. Co-authorship graphs (Rodriguez and Bollen, 2008) and more general bibliographic graph-based data models (Tran et al., 2017) give appealing methods which do not require a set of reviewers to be pre-determined by conference chair. Instead, these methods recommend reviewers to be recruited, which might be particularly useful for journal editors.
One of the most widely used automated assignment algorithms today is the Toronto Paper Matching System or TPMS (Charlin and Zemel, 2013) which also computes estimations of similarities between submitted papers and available reviewers using techniques in natural language processing. These scores might be enhanced with reviewers’ self-accessed expertise adaptively queried from them in an automatic manner.
Our work uses these similarities as an input for our assignment algorithm, and considers the computation of these similarity values as a given black box.
Cumulative goal functions. With the given similarities, much of past work on reviewer assignments develop algorithms to maximize the cumulative similarity, that is, the sum of the similarities across all assigned reviewers and all papers. Such an objective is pursued by the organizers of SIGKDD conference (Flach et al., 2010) and by the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013). Various other popular conference management systems such as EasyChair (easychair.org) and HotCRP (hotcrp.com) and several other papers (see Long et al. 2013; Charlin et al. 2012; Goldsmith and Sloan 2007; Tang et al. 2010 and references therein) also aim to maximize various cumulative functionals in their automated reviewer assignment procedures. In the sequel, we argue however that optimizing such cumulative objectives is not fair — in order to maximize them, these algorithms may discriminate against some subset of papers. Moreover, it is the non-mainstream submissions that are most likely to be discriminated against. With this motivation, we consider a notion of fairness instead.
3

Fairness. In order to ensure that no papers are discriminated against, we aim at ﬁnding a fair assignment — an assignment that ensures that the most disadvantaged paper gets as competent reviewers as possible. The issue of fairness is partially tackled by Hartvigsen et al. (1999), where they necessitate every paper to have at least one reviewer with expertise higher than certain threshold, and then maximize the value of that threshold. However, this improvement only partially solves the issue of discrimination of some papers: having assigned one strong reviewer to each paper, the algorithm may still discriminate against some papers while assigning remaining reviewers. Given that nowadays large conferences such as NeurIPS and ICML assign 4-6 reviewers to each paper, a careful assessment of the paper by one strong reviewer might be lost in the noise induced by the remaining weak reviews. In the present study, we measure the quality of assignment with respect to any particular paper as sum similarity over reviewers assigned to that paper. Thus, the fairness of assignment is the minimum sum similarity across all papers; we call an assignment fair if it maximizes the fairness. We note that assignment computed by our PeerReview4All algorithm is guaranteed to have at least as large max-min fairness as that proposed by Hartvigsen et al. (1999).
Benferhat and Lang (2001) discuss diﬀerent approaches to selection of the “optimal” reviewer assignment. Together with considering a cumulative objective, they also note that one may deﬁne the optimal assignment as an assignment that minimizes a disutility of the most disadvantaged reviewer (paper). This approach resembles the notion of max-min fairness we study in this paper, but Benferhat and Lang (2001) do not propose any algorithm for computing the fair assignment.
The notion of max-min fairness was formally studied in context of peer-review by Garg et al. (2010). While studying a similar objective, our work develops both conceptual and theoretical novelties which we highlight here. First, Garg et al. (2010) measure the fairness in terms of reviewers’ bids — for every reviewer they compute a value of papers assigned to that reviewer based on her/his bids and maximize the minimum value across all reviewers. While satisfying reviewers is a useful practice, we consider fairness towards the papers in their review to be of utmost importance. During a bidding process reviewers have limited time resources and/or limited access to papers’ content to evaluate their relevance, and hence reviewers’ bids alone are not a good proxy towards the measure of fairness. In contrast, in this work we consider similarities — scores that are designed to represent a competence of reviewer in assessing a paper. Besides reviewers’ bids, similarities are computed based on the full text of the submissions and papers authored by reviewer and can additionally incorporate various factors such as quality of previous reviews, experience of reviewer and other features that cannot be self-assessed by reviewers.
The assignment algorithm proposed in Garg et al. (2010) works in two steps. In the ﬁrst step, the problem is set up as an integer programming problem and a linear programming relaxation is solved. The second step involves a carefully designed rounding procedure that returns a valid assignment. The algorithm is guaranteed to recover an assignment whose fairness is within a certain additive factor from the best possible assignment. However, the fairness guarantees provided in Garg et al. (2010) turn out to be vacuous for various similarity matrices. As we discuss later in the paper, this is a drawback of the algorithm itself and not an artifact of their guarantees. In contrast, we design an algorithm with multiplicative approximation factor that is guaranteed to always provide a non-trivial approximation which is at most constant factor away from the optimal.
Next, Garg et al. (2010) consider fairness of the assignment as an eventual metric of the assignment quality. However, we note that the main goal of the conference paper reviewing process is an accurate acceptance of the best papers. Thus, in the present work we both theoretically and empirically study the impact of the fairness of the assignment on the quality of the acceptance procedure.
Finally, although Garg et al. (2010) present their algorithm for the case of discrete reviewer’s bids, we note that this assumption can be relaxed to allow real-valued similarities with a continuous range as in our setting. In this paper we refer to the corresponding extension of their algorithm as the Integer Linear Programming Relaxation (ILPR) algorithm.
Fair division. A direction of research that is relevant to our work studies the problem of fair division where max-min fairness is extensively developed. The seminal work of Lenstra et al. (1990) provides a constant factor approximation to the minimum makespan scheduling problem where the goal is to assign a number of jobs to the unrelated parallel machines such that the maximal running time is minimized. Recently Asadpour
4

and Saberi (2010); Bansal and Sviridenko (2006) proposed approximation algorithms for the problem of assigning a number of indivisible goods to several people such that the least happy person is as happy as possible. However, we note that techniques developed in these papers cannot be directly applied for reviewer assignments problem in peer review due to the various idiosyncratic constraints of this problem. In contrast to the classical formulation studied in these works, our problem setting requires each paper to be reviewed by a ﬁxed number of reviewers and additionally has constraints on reviewers’ loads. Such constraints allow us to achieve an approximation guarantee that is independent of the total number of papers and reviewers, and depends only on λ, the number of reviewers required per paper, as λ1 . In contrast, the approximation factor of Asadpour and Saberi (2010) gets worse at a rate of √m l1og3 m , where m is a number of persons (papers in our setting).
Statistical aspects. Diﬀerent statistical aspects related to conference peer-review have been studied in the literature. McGlohon et al. (2010) and Dai et al. (2012) studied aggregation of consumers ratings to generate a ranking of restaurants or merchants. They come up with objective score model of reviewer which we also use in this work. Ge et al. (2013) also use similar model of reviewer and propose a Bayesian approach to calibrating reviewer’ scores, which allows to incorporate diﬀerent biases in context of conference peer-review. Sajjadi et al. (2016) empirically compare diﬀerent methods of score aggregation for peer grading of homeworks. Peer grading is a related problem to conference peer review, with the key diﬀerence that the questions and answers (“papers”) are more closed-ended and objective. They conclude that although more sophisticated methods are praised in the literature, the simple averaging algorithm demonstrates better performance in their experiment. Another interesting observation they make is an edge of cardinal grades over ordinal in their setup. In this work we also consider the conferences with cardinal grading scheme of submissions.
To the best of our knowledge, no prior works on conference peer-review has studied the entire pipeline — from assignment to acceptance — from a statistical point of view. In this work we take the ﬁrst steps to close this gap and provide a strong minimax analysis of na¨ıve yet interesting procedure of determining top k papers. Our ﬁndings suggest that higher fairness of the assignment leads to better quality of acceptance procedure. We consider both the objective score model (Ge et al., 2013; McGlohon et al., 2010; Dai et al., 2012) and a novel subjective-score model that we propose in the present paper.
Coverage and Diversity. For completeness, we also discuss several related works that study reviewer assignment problem.
Li et al. (2015) present a greedy algorithm that tries to avoid assigning a group of stringent reviewers or a group of lenient reviewers to a submission, thus maintaining diversity of the assignment in terms of having diﬀerent combinations of reviewers assigned to diﬀerent papers.
Another way to ensure diversity of the assignment is proposed by Liu et al. (2014). Instead of designing the special assignment algorithm, they try to incentivize the diversity by special construction of similarities. Besides incorporating expertise and authority of reviewers in similarities, they add an additional term to the optimization problem which balances similarities by increasing scores for reviewers from diﬀerent research areas.
Karimzadehgan et al. (2008) consider topic coverage as an objective and propose several approaches to maintain broad coverage, requiring reviewers assigned to paper being expert in diﬀerent subtopics covered by the paper. They empirically verify that given a paper and a set of reviewers, their algorithms lead to better coverage of paper’s topics as compared to baseline technique that assigns reviewers based on some measure of similarity between text of submission and papers authored by reviewers, but does not do topic matching.
A similar goal is formally studied by Long et al. (2013). They measure the coverage of the assignment in terms of the total number of distinct topics of papers covered by the assigned reviewers. They propose a constant factor approximation algorithm that beneﬁts from a sub-modular nature of the objective. As we show in Appendix C, the techniques of Long et al. (2013) can be combined with our proposed algorithm to obtain an assignment which maintains not only fairness, but also a broad topic coverage.
Research on peer review. The explosion in the number of submissions in many conferences has spurred research in computer science on improving peer review. In addition to problems of fairness and accuracy of the reviewer-paper assignment process, there are a number of challenges in peer review which are addressed in the literature to various extents. These include problems of bias (Tomkins et al., 2017; Stelmakh et al.,
5

2019a), miscalibration (Ge et al., 2013; Roos et al., 2011; Flach et al., 2010; Wang and Shah, 2019), subjectivity (Noothigattu et al., 2018), strategic behavior (Balietti et al., 2016; Xu et al., 2019a,b), and others (Lawrence and Cortes, 2014; Gao et al., 2019). Of particular interest is the work by Fiez et al. (2019) which optimizes the process by which reviewers can bid on which papers they prefer to review. In most automated reviewer-paper assignment systems, the bids and the text-matching similarities are then combined (Shah et al., 2017) to form the similarities used to compute the assignment. The bidding and the reviewer-paper assignments are executed separately in current systems, and given the intrinsic relations between the two, it is of interest to jointly design the two systems in the future.

3 Problem setting
In this section we present the problem setting formally with a focus on the objective of fairness. (We introduce the statistical models we consider in Sections 6 and 7.)

3.1 Preliminaries and notation
Given a collection of m ≥ 2 papers, suppose that there exists a true, unknown total ranking of the papers. The goal of the program chair (PC) of the conference is to recover top k papers, for some pre-speciﬁed value k < m. In order to achieve this goal, the PC recruits n ≥ 2 reviewers and asks each of them to read and evaluate some subset of the papers. Each reviewer can review a limited number of papers. We let µ denote the maximum number of papers that any reviewer is willing to review. Each paper must be reviewed by λ distinct reviewers. In order to ensure this setting is feasible, we assume that nµ ≥ mλ. In practice, λ is typically small (2 to 6) and hence should conceptually be thought of as a constant.
The PC has access to a similarity matrix S = {sij} ∈ [0, 1]n×m, where sij denotes the similarity between any reviewer i ∈ [n] and any paper j ∈ [m].1 These similarities are representative of the envisaged quality of the respective reviews: a higher similarity between any reviewer and paper is assumed to indicate a higher competence of that reviewer in reviewing that paper (this assumption is formalized later). We do not discuss the design of such similarities, but often they are provided by existing systems (Charlin and Zemel, 2013; Mimno and McCallum, 2007; Liu et al., 2014; Rodriguez and Bollen, 2008; Tran et al., 2017).
Our focus is on the assignment of papers to reviewers. We represent any assignment by a matrix A ∈ {0, 1}n×m, whose (i, j)th entry is 1 if reviewer i is assigned paper j and 0 otherwise. We denote the set of reviewers who review paper j under an assignment A as RA(j). We call an assignment feasible if it respects the (µ, λ) conditions on the reviewer and paper loads. We denote the set of all feasible assignments as A:

A := A ∈ {0, 1}n×m | Aij = λ ∀j ∈ [m],

Aij ≤ µ ∀i ∈ [n] .

i∈[n]

j∈[m]

Our goal is to design a reviewer-assignment algorithm with a two-fold objective: (i) fairness to all papers, (ii) strong statistical guarantees in terms of recovering the top papers.
From a statistical perspective, we assume that when any reviewer i is asked to evaluate any paper j, then she/he returns score yij ∈ R. The end goal of the PC is to accept or reject each paper. In this work we consider a simpliﬁed yet indicative setup. We assume that the PC wishes to accept the k “top” papers from the set of m submitted papers. We denote the “true” set of top k papers as Tk∗. While the PC’s decisions in practice would rely on several additional factors including the text comments by reviewers and the discussions between them, in order to quantify the quality of any assignment we assume that the top k papers are chosen through some estimator θ that operates on the scores provided by the reviewers. Such an estimator can be used in practice to serve as a guide to the program committee in order to help reduce their load. These
acceptance decisions can be described by the chosen assignment and estimator A, θ . We denote the set of

1Here, we adopt the standard notation [ν] = {1, 2, . . . , ν} for any positive integer ν.

6

Reviewer 1 Reviewer 2 Reviewer 3

Paper a
1 0 1/4

Paper b
1 0 1/4

Paper c
1 1/5 1/2

Table 1: Example similarity.

accepted papers under an assignment A and estimator θ as Tk = Tk A, θ . The PC then wishes to maximize the probability of recovering the set Tk∗ of top k papers.
Although the goal of exact recovering of top k papers is appealing, given the large number of papers submitted to a conference such as ICML and NeurIPS, this goal might be too optimistic. Another alternative is to recover top k papers allowing for a certain Hamming error tolerance t ∈ {0, . . . , k − 1}. For any two subsets M1, M2 of [m], we deﬁne their Hamming distance to be the number of items that belong to exactly one of the two sets — that is

DH (M1, M2) = card ({M1 ∪ M2} \ {M1 ∩ M2}) .

(1)

The goal of PC under this scenario is to choose a pair A, θ such that for the given error tolerance parameter
t, the probability P {DH (Tk, Tk∗) > 2t} is minimized. We return to more details on the statistical aspects later in the paper.

3.2 Fairness objective

An assignment objective that is popular in past papers (Charlin and Zemel, 2013; Charlin et al., 2012; Taylor, 2008) is to maximize the cumulative similarity over all papers. Formally, these works choose an assignment A ∈ A which maximizes the quantity

m

GS (A) :=

sij .

(2)

j=1 i∈RA(j)

An assignment algorithm that optimizes this objective (2) is implemented in the widely used Toronto Paper Matching System (Charlin and Zemel, 2013). We will refer to the feasible assignment that maximizes the objective (2) as ATPMS and denote the algorithm which computes ATPMS as TPMS.
We argue that the objective (2) does not necessarily lead to a fair assignment. The optimal assignment can discriminate some papers in order to maximize the cumulative objective. To see this issue, consider the following example.
Consider a toy problem with n = m = 3 and µ = λ = 1, with a similarity matrix shown in Table 1. In this example, paper c is easy to evaluate, having non-zero similarities with all the reviewers, while papers a and b are more speciﬁc and weak reviewer 2 has no expertise in reviewing them. Reviewer 1 is an expert and is able to assess all three papers. Maximizing total sum of similarities (2), the TPMS algorithm will assign reviewers 1, 2, and 3 to papers a, b, and c respectively. Observe that under this assignment, paper b is assigned a reviewer who has insuﬃcient expertise to evaluate the paper. On the other hand, the alternative assignment which assigns reviewers 1, 2, and 3 to papers a, c, and b respectively ensures that every paper has a reviewer with similarity at least 1/5. This “fair” assignment does not discriminate against papers a and b for improving the review quality of the already beneﬁtting paper c.
With this motivation, we now formally describe the notion of fairness that we aim to optimize in this paper. Inspired by the notion of max-min fairness in a variety of other ﬁelds (Rawls, 1971; Lenstra et al., 1990; Hahne, 1991; Lavi et al., 2003; Bonald et al., 2006; Asadpour and Saberi, 2010), we aim to ﬁnd a

7

feasible assignment A ∈ A to maximize the following objective ΓS for given similarity matrix S:

ΓS (A) = min

sij .

(3)

j∈[m]

i∈RA (j )

The assignment optimal for (3) maximizes the minimum sum similarity across all the papers. In other words, for every other assignment there exists some paper which has the same or lower sum similarity. Returning to our example, the objective (3) is maximized when reviewers 1, 2, and 3 are assigned to papers a, c, and b respectively.
Our reviewer assignment algorithm presented subsequently guarantees the aforementioned fair assignment. Importantly, while aiming at optimizing (3), our algorithm does even more — having the assignment for the worst-oﬀ paper ﬁxed, it ﬁnds an assignment that satisﬁes the second worst-oﬀ paper, then the next one and so on until all papers are assigned.
It is important to note that similarities sij obtained by diﬀerent techniques (Charlin and Zemel, 2013; Mimno and McCallum, 2007; Rodriguez and Bollen, 2008; Tran et al., 2017) all have diﬀerent meanings. Therefore, the PC might be interested to consider a slightly more general formulation and aim to maximize

ΓSf (A) = min

f (sij),

(4)

j∈[m]

i∈RA (j )

for some reasonable choice of monotonically increasing function f : [0, 1] → [0, ∞].2 While the same eﬀect might be achieved by redeﬁning sij = f (sij) for all i ∈ [n], j ∈ [m], this formulation underscores the fact that assignment procedure is not tied to any particular method of obtaining similarities. Diﬀerent choices of f represent the diﬀerent views on the meaning of similarities. As a short example, let us consider f (sij) = I {sij > ζ} for some ζ > 0.3 This choice stratiﬁes reviewers for each paper into strong (similarity higher than ζ) and weak. The fair assignment would be such that the most disadvantaged paper is assigned to as many strong reviewers as possible. We discuss other variants of f later when we come to the statistical properties of our algorithm. In what follows we refer to the problem of ﬁnding reviewer assignment that maximizes the term (4) as the fair assignment problem.
Unfortunately, the assignment optimal for (4) is hard to compute for any reasonable choices of function f . Garg et al. (2010) showed that ﬁnding a fair assignment is an NP-hard problem even if f (s) ∈ {1, 2, 3} and λ = 2.
With this motivation, in the next section we design a reviewer assignment algorithm that seeks to optimize the objective (4) and provide associated approximation guarantees. We will refer to a feasible assignment that exactly maximizes ΓSf (A) as AHf ARD and denote the algorithm that computes AHf ARD as Hard. When the function f is clear from context, we drop the subscript f and denote the Hard assignment as AHARD for brevity.
Finally we note that for our running example (Table 1 above), the ILPR algorithm (Garg et al., 2010), despite trying to optimize fairness of the assignment, also returns an unfair assignment AILPR which coincides with ATPMS. The reason for this behavior lies in the inner-working of the ILPR algorithm: a linear programming relaxation splits reviewers 1 and 2 in two and makes them review both paper a and paper b. During the rounding stage, reviewer 1 is assigned to either paper a or paper b, ensuring that the remaining paper will be reviewed by reviewer 2. Given that reviewer 2 has zero similarity with both papers a and b, the fairness of the resulting assignment will be 0. Such an issue arises more generally in the ILPR algorithm and is discussed in more detail subsequently in Section 5.3 and in Appendix A.1.

4 Reviewer assignment algorithm
In this section we ﬁrst describe our PeerReview4All algorithm followed by an illustrative example.
2We allow f (sij ) = ∞. When reviewer with similarity ∞ is assigned to paper, she/he is able to perfectly access the quality of the paper.
3We use I to denote the indicator function, that is, I {x} = 1 if x is true and I {x} = 0 otherwise.

8

4.1 Algorithm
A high level idea of the algorithm is the following. For every integer κ ∈ [λ], we try to assign each paper to κ reviewers with maximum possible similarities while respecting constraints on reviewer loads. We do so via a carefully designed “subroutine” that is explained below. Continuing for that value of κ, we complement this assignment with (λ − κ) additional reviewers for each paper. Repeating the procedure for each value of κ ∈ [λ], we obtain λ candidate assignments each with λ reviewers assigned to each paper, and then choose the one with the highest fairness. The assignment at this point ensures guarantees of worst-case fairness (4). We then also optimize for the second worst-oﬀ paper, then the third worst-oﬀ paper and so on in the following manner. In the assignment at this point, we ﬁnd the most disadvantaged papers and permanently ﬁx corresponding reviewers to these papers. Next, we repeat the procedure described above to ﬁnd the most fair assignment among the remaining papers, and so on. By doing so, we ensure that our ﬁnal assignment is not susceptible to bottlenecks which may be caused by irrelevant papers with small average similarities.
The higher-level idea behind the aforementioned subroutine to obtain the candidate assignment for any value of κ ∈ [λ] is as follows. The subroutine constructs a layered ﬂow network graph with one layer for reviewers and one layer for papers, that captures the similarities and the constraints on the paper/reviewer loads. Then the subroutine incrementally adds edges between (reviewer, paper) pairs in decreasing order of similarity and stops when the paper load constraints are met (each paper can be assigned to κ reviewers using only edges added at this point). This iterative procedure ensures that the papers are assigned reviewers with approximately the highest possible similarities.
We formally present our main algorithm as Algorithm 1 and the subroutine as Subroutine 1. In what follows, we walk the reader through the steps in the subroutine and the algorithm in more detail. Subroutine. A key component of our algorithm is a construction of a ﬂow network in a sequential manner in Subroutine 1. The subroutine takes as input, among other arguments, the set M of papers that are not yet assigned and the required number of reviewers per paper κ ≤ λ. The goal of the subroutine is to assign each paper in M with κ reviewers, respecting the reviewer load constraints, in a way that minimum similarity across all paper-reviewer pairs in resulting assignment is maximized.
The output of the subroutine is an assignment (represented by variable A) which is initially set as empty (Step 1). The subroutine begins (Step 2) with a construction of a directed acyclic graph (a “ﬂow network”) comprising 4 layers in the following order: a source, all reviewers, all papers in M, and a sink. An edge may exist only between consecutive layers. The edges between the ﬁrst two layers control the reviewers’ workloads and edges between the last two layers represent the number of reviews required by the papers. Finally, costs of the all edges in this initial construction are set to 0. Note that in subsequent steps, the edges are added only between the second and third layers. Thus, the maximum ﬂow in the network is at most |M|κ.
The crux of the subroutine is to incrementally add edges one at a time between the layers, representing the reviewers and papers, in a carefully designed manner (Steps 3 and 4). The edges are added in order of decreasing similarities. These edges control a reviewer-paper relationship: they have a unit capacity to ensure that any reviewer can review any paper at most once and their costs are equal to the similarity between the corresponding (reviewer, paper) pair.
After adding each edge, the subroutine (Step 5) tests whether a max-ﬂow of size |M|κ is feasible. Note that a feasible ﬂow of size |M|κ corresponds to a feasible assignment: by construction of the ﬂow network described earlier, we know that the reviewer and paper load constraints are satisﬁed. The capacity of each edge in our ﬂow network is a non-negative integer, thereby guaranteeing that the max-ﬂow is an integer, that it can be found in polynomial time, and that the ﬂow in every edge is a non-negative integer under the max-ﬂow. Once the max-ﬂow of size |M|κ is reached, the subroutine stops adding edges. At this point, it is ensured that the value of the lowest similarity in the resulting assignment is maximized.
Finally, the subroutine assigns each paper to κ reviewers, using only the “high similarity” edges added to the network so far (Steps 6 and 7). The existence of the corresponding assignment is guaranteed by max-ﬂow in the network being equal to |M|κ. There may be more than one feasible assignments that attain the max-ﬂow. While any of these assignments would suﬃce from the standpoint of optimizing the worst-case fairness objective (4), the PC may wish to make a speciﬁc choice for additional beneﬁts and specify the heuristic to pick the max-ﬂow in Step 6 of the subroutine. For example, if the max-ﬂow with the maximum
9

Subroutine 1 PeerReview4All Subroutine Input: κ ∈ [λ]: number of reviewers required per paper
M: set of papers to be assigned S ∈ ({−∞} ∪ [0, 1])n×|M|: similarity matrix (µ(1), . . . , µ(n)) ∈ [µ]n: reviewers’ maximum loads Output: Reviewer assignment A Algorithm:
1. Initialize A to an empty assignment 2. Initialize the ﬂow network:
• Layer 1: one vertex (source) • Layer 2: one vertex for every reviewer i ∈ [n], and directed edges of capacity µ(i) and cost 0 from
the source to every reviewer • Layer 3: one vertex for every paper j ∈ M • Layer 4: one vertex (sink), and directed edges of capacity κ and cost 0 from each paper to the
sink
3. Find (reviewer, paper) pair (i, j) such that the following two conditions are satisﬁed:
• the corresponding vertices i and j are not connected in the ﬂow network
• the similarity sij is maximal among the pairs which are not connected (ties are broken arbitrarily)
and call this pair (i , j ) 4. Add a directed edge of capacity 1 and cost si j between nodes i and j 5. Compute the max-ﬂow from source to sink, if the size of the ﬂow is strictly smaller than |M|κ, then go
to Step 3 6. If there are multiple possible max-ﬂows, choose any one arbitrarily (or use any heuristic such as max-ﬂow
with max cost) 7. For every edge (i, j) between layers 2 (reviewers) and 3 (papers) which carries a unit of ﬂow in the
selected max-ﬂow, assign reviewer i to paper j in the assignment A
cost is selected, then the resulting assignment nicely combines fairness with the high average quality of the assignment. Another choice, discussed in Appendix C, helps with broad topic coverage of the assignment. Importantly, the approximation guarantees established in Theorem 1 and Corollary 1, as well as statistical guarantees from Sections 6 and 7 hold for any max-ﬂow assignment chosen in Steps 6 and 7.
For comparison, we note that the TPMS algorithm can equivalently be interpreted in this framework as follows. The TPMS algorithm would ﬁrst connect all reviewers to all papers in layers 2 and 3 of the ﬂow graph. It will then compute a max-ﬂow with max cost in this fully connected ﬂow network and make reviewer-paper assignments corresponding to the edges with unit ﬂow between layers 2 and 3. In contrast, our sequential construction of the ﬂow graph prevents papers from being assigned to weak reviewers and is crucial towards ensuring the fairness objective.
Algorithm. The algorithm calls the subroutine iteratively and uses the outputs of these iterates in a carefully designed manner. Initially, all papers belong to a set M which represents papers that are not yet assigned. The algorithm repeats Steps 2 to 7 until all papers are assigned. In every iteration, for every value of κ ∈ [λ], the algorithm ﬁrst calls the subroutine to assign κ reviewers to each paper from M (Step 2b), and then adjusts reviewers’ capacities and the similarity matrix (Step 2c) to prevent any reviewer being assigned to the same paper twice. Next, the subroutine is called again (Step 2d) to assign another (λ − κ) reviewers to each paper. As a result, after completion of Step 2, λ feasible candidate assignments A1, . . . , Aλ are constructed. Each assignment Aκ, κ ∈ [λ], is guaranteed (through the Step 2b) to maximize the minimum similarity across pairs (i, j) where j ∈ M and reviewer i is among κ strongest reviewers assigned to paper j in Aκ; and (through the Steps 2d and 2e) to have each paper assigned with exactly λ reviewers.
10

Algorithm 1 PeerReview4All Algorithm
Input: λ ∈ [n]: number of reviewers required per paper S ∈ [0, 1]n×m: similarity matrix
µ ∈ [m]: reviewers’ maximum load
f : transformation of similarities Output: Reviewer assignment APf R4A Algorithm:

1. Initialize µ = (µ, . . . , µ) ∈ [µ]n APf R4A, A0 : empty assignments M = [m]: set of papers to be assigned
2. For κ = 1 to λ

(a) Set µtmp = µ, Stmp = S (b) Assign κ reviewers to every paper using subroutine: A1κ = Subroutine(κ, M, Stmp, µtmp) (c) Decrease µtmp for every reviewer by the number of papers she/he is assigned in A1κ, set corresponding
similarities in Stmp to −∞ (d) Run subroutine with adjusted µtmp and Stmp to assign remaining λ − κ reviewers to every paper:
A2κ = Subroutine(λ − κ, M, Stmp, µtmp) (e) Create assignment Aκ such that for every pair (i, j) of reviewer i ∈ [n] and paper j ∈ M, reviewer
i is assigned to paper j if she/he is assigned to this paper in either A1κ or A2κ

3. Choose A ∈ arg max ΓSf (Aκ) with ties broken arbitrarily
κ∈[λ]∪{0}

4. For every paper j ∈ J ∗ := arg min

f (si ), assign all reviewers RA(j) to paper j in APf R4A

∈M i∈RA( )

5. For every reviewer i ∈ [n], decrease µ(i) by the number of papers in J ∗ assigned to i

6. Delete columns corresponding to the papers J ∗ from S and A, update M = M\J ∗

7. Set A0 = A 8. If M is not empty, go to Step 2

In Step 3, the algorithm chooses the assignment with the highest fairness (4) among the λ candidate assignments and the assignment A0 from the previous iteration (empty in the ﬁrst iteration). Note that since A0 is also included in the maximizer, the fairness cannot decrease in subsequent iterations.
In the chosen assignment, the algorithm identiﬁes the papers that are most disadvantaged, and ﬁxes the assignment for these papers (Step 4). The assignment for these papers will not be changed in any subsequent step. The next steps (Steps 5 and 6) update the auxiliary variables to account for this assignment that is ﬁxed — decreasing the corresponding reviewer capacities and removing these assigned papers from the set M. Step 7 then keeps a track of the present assignment A for use in subsequent iterations, ensuring that fairness cannot decrease as the algorithm proceeds.
Remarks. We make a few additional remarks regarding the PeerReview4All algorithm. 1. Computational cost: A na¨ıve implementation of the PeerReview4All algorithm has a computational
complexity O λ(m + n)m2n . We give more details on implementation and computational aspects in Appendix B.
2. Variable reviewer or paper loads: More generally, the PeerReview4All algorithm allows for specifying diﬀerent loads for diﬀerent reviewers and/or papers. For general paper loads, we consider κ ≤ maxj∈[m] λ(j) and deﬁne the capacity of edge between node corresponding to any paper j and sink as min{κ, λ(j)}.
3. Incorporating conﬂicts of interest: One can easily incorporate any conﬂict of interest between any reviewer and paper by setting the corresponding similarity to −∞.
4. Topic coverage: The techniques developed in Long et al. (2013) can be employed to modify our algorithm in a way that it ﬁrst ensures fairness and then, among all approximately fair assignments, picks one
11

that approximately maximizes the number of distinct topics of papers covered. We discuss this modiﬁcation in Appendix C.

4.2 Example
To provide additional intuition behind the design of the algorithm, we now present an example that we also use in the next section to explain our approximation guarantees.
Let for a moment assume that f (s) = s and let ζ be a constant close to 1. Consider the following two scenarios:
(S1) The optimal assignment AHARD is such that all the papers are assigned to reviewers with high similarity:

min sij > ζ ∀j ∈ [m].

(5)

i∈RAHARD (j)

(S2) The optimal assignment AHARD is such that there are some “critical” papers which have η < λ assigned reviewers with similarities higher than ζ and the remaining assigned reviewers with small similarities. All other papers are assigned to λ reviewers with similarity higher than ζ.
Intuitively, the ﬁrst scenario corresponds to an ideal situation since there exists an assignment such that each paper has λ competent reviewers (with similarity ζ ≈ 1). In contrast, in the second scenario, even in the fair assignment, some papers lack expert reviewers. Such a scenario may occur, for example, if some non-mainstream papers were submitted to a conference. This case entails identifying and treating these disadvantaged papers as well as possible. To be able to ﬁnd the fair assignment in both scenarios, the assignment algorithm should distinguish between them and adapt its behavior to the structure of similarity matrix. Let us track the inner-workings of PeerReview4All algorithm to demonstrate this behaviour.
We note that by construction, the fairness of the resulting assignment APR4A is determined in the ﬁrst iteration of Steps 2 to 7 of Algorithm 1, so we restrict our attention to M = [m]. First, consider scenario (S1). The subroutine called with parameter κ = λ will add edges to the ﬂow network until the maximal ﬂow of size mλ is reached. Since the optimal assignment AHARD is such that the lowest similarity is higher than ζ, the last edge added to the ﬂow network will have similarity at least ζ, implying that the fairness of the candidate assignment Aλ, which is a lower bound for the fairness of resulting assignment, will be at least λζ. Given that ζ is close to one, we conclude that in this case algorithm is able to recover an assignment which is at least very close to optimal.
Now, let us consider scenario (S2). In this scenario, the subroutine called with κ = λ may return a poor assignment. Indeed, since there is a lack of competent reviewers for critical papers, there is no way to assign each paper with λ reviewers having a high minimum similarity in the assignment. However, the subroutine called with parameter κ = η will ﬁnd η strong reviewers for each paper (including the critical papers), thereby leading to a fairness ΓS APR4A ≥ ηζ. The obtained lower bound guarantees that the assignment recovered by the PeerReview4All algorithm is also close to the optimal, because in the fair assignment AHARD some papers have only η strong reviewers.
This example thus illustrates how the PeerReview4All algorithm can adapt to the structure of the similarity matrix in order to guarantee fairness, as well as other guarantees that are discussed subsequently in the paper.

5 Approximation guarantees
In this section we provide guarantees on the fairness of the reviewer-assignment by our algorithm. We ﬁrst establish guarantees on the max-min fairness objective introduced earlier (Section 5.1). We subsequently show that our algorithm optimizes not only the worst-oﬀ paper but recursively optimizes all papers (Section 5.2). We then conclude this section on deterministic approximation guarantees with a comparison to past literature (Section 5.3).

12

5.1 Max-min fairness

We begin with some notation that will help state our main approximation guarantees. For each value of κ ∈ [λ], consider the reviewer-assignment problem but where each paper requires κ (instead of λ) reviews (each reviewer still can review up to µ papers). Let us denote the family of all feasible assignments for this problem as Aκ. Now deﬁne the quantities

s∗κ := max min min sij,

(6)

A∈Aκ j∈[m] i∈RA(j)

s∗0 := max max sij, and
i∈[n] j∈[m]

s∗∞ := min min sij.
i∈[n] j∈[m]

Intuitively, for every assignment from the family Aκ, the quantity s∗κ upper bounds the minimum similarity for any assigned (reviewer, paper) pair. It also means that the value s∗κ is achievable by some assignment in Aκ. The value s∗0 captures the value of the largest entry in the similarity matrix S and gives a trivial upper bound ΓSf (A) ≤ λf (s∗0) for every feasible assignment A ∈ A. Likewise, the value s∗∞ captures the smallest entry in the similarity matrix S and yields a lower bound ΓSf (A) ≥ λf (s∗∞) for every feasible assignment A ∈ A.
We are now ready to present the main result on the approximation guarantees for the PeerReview4All algorithm as compared to the optimal assignment AHARD.

Theorem 1. Consider any feasible values of (n, m, λ, µ), any monotonically increasing function f : [0, 1] →
[0, ∞], and any similarity matrix S. The assignment APf R4A given by the PeerReview4All algorithm guarantees the following lower bound on the fairness objective (4):

ΓSf APf R4A ΓSf AHf ARD

max (κf (s∗κ) + (λ − κ)f (s∗∞))

κ∈[λ]

≥ min ((κ − 1)f (s∗) + (λ − κ + 1) f (s∗ ))

0

κ

κ∈[λ]

≥ 1/λ.

(7a) (7b)

Remarks. The numerator of (7a) is a lower bound on the fairness of the assignment returned by our algorithm. It is important to note that if λ = 1, that is, if we only need to assign one reviewer for each paper, then our PeerReview4All Algorithm ﬁnds exact solution for the problem, recovering the classical results of Garﬁnkel (1971) as a special case.
In practice, the number of reviewers λ required per paper is a small constant (typically set as 3), and in that case, our algorithm guarantees a constant factor approximation. Note that the fraction in the right hand side of (7a) can become 0/0 or ∞/∞, and in both cases it should be read as 1.

The bound (7a) can be signiﬁcantly tighter than 1/λ, as we illustrate in the following example.

Example. Consider two scenarios (S1) and (S2) from Section 4.2, and consider f (s) = s. One can see that
under scenario (S1), we have s∗λ ≥ ζ. Setting κ = λ in the numerator and κ = 1 in the denominator of the bound (7a), and recalling that ζ ≈ 1, we obtain:

ΓS APR4A

ζ

ΓS (AHARD) ≥ s∗ ≈ 1,

1

where we have also used the fact that s∗1 ≤ 1. Let us now consider the second scenario (S2) in the example of Section 4.2. In this scenario, since each paper can be assigned to η strong reviewers with similarity higher than ζ, we have s∗η = ζ ≈ 1. We then also have s∗0 ≤ 1. Moreover, there are some papers which have only η strong reviewers in optimal assignment AHARD, and hence we have s∗η+1 s∗0. Setting κ = η in the numerator and κ = η + 1 in the denominator of the bound (7a), some algebraic simpliﬁcations yield the bound

ΓS APR4A

ηs∗η + (λ − η)s∗∞ s∗η (λ − η) s∗η+1

ΓS (AHARD) ≥ ηs∗ + (λ − η)s∗ ≥ s∗ − η s∗ ≈ 1.

0

η+1

0

0

13

We now brieﬂy provide more intuition on the bound (7a) by interpreting it in terms of speciﬁc steps in the

algorithm. Setting f (s) = s, let us consider the ﬁrst iteration of the algorithm. Recalling the deﬁnition (6) of

s∗κ, the PeerReview4All subroutine called with parameter κ on Step 2b ﬁnds an assignment such that all the similarities are at least s∗κ. This guarantee in turn implies that the fairness of the corresponding assignment Aκ is at least κs∗κ + (λ − κ)s∗∞, thereby giving rise to the numerator of (7a). The denominator is an upper bound of the fairness of the optimal assignment AHARD. The expression for any value of κ is

obtained by simply appealing to the deﬁnition of s∗κ which is deﬁned in terms of the optimal assignment. By deﬁnition (6) of s∗κ, for every feasible assignment A exists at least one paper such that at most κ − 1 of the assigned reviewers are of similarity larger than s∗κ. Thus, the fairness of the optimal assignment is upper-bounded by the sum similarity of the paper that has κ − 1 reviewers with similarity s∗0 (the highest possible similarity), and λ − κ + 1 reviewers with similarity s∗κ.

Finally, one may wonder whether optimizing the objective (2) as done by prior works (Charlin and

Zemel, 2013; Charlin et al., 2012) can also guarantee fairness. It turns out that this is not the case (see

the example in Table 1 for intuition), and optimizing the objective (2) is not a suitable proxy towards the

fairness objective (4). In Appendix A.2 we show that in general the fairness objective value of the TPMS

algorithm which optimizes (2) may be arbitrarily bad as compared to that attained by our PeerReview4All

algorithm.

In Appendix A.3 we show that the analysis of the approximation factor of our algorithm is tight in a

sense that there exists a similarity matrix for which the bound (7b) is met with equality. That said, the

approximation

factor

of

our

PeerReview4All

algorithm

can

be

much

better

than

1 λ

for

various

other

similarity matrices, as demonstrated in examples (S1) and (S2).

5.2 Beyond worst case
The previous section established guarantees for the PeerReview4All algorithm on the fairness of the assignment in terms of the worst-oﬀ paper. In this section we formally show that the algorithm does more:
having the assignment for the worst-oﬀ paper ﬁxed, the algorithm then satisﬁes the second worst-oﬀ paper,
and so on.
Recall that Algorithm 1 iteratively repeats Steps 2 to 7. In fact, the ﬁrst time that Step 3 is executed, the
resulting intermediate assignment A achieves the max-min guarantees of Theorem 1. However, the algorithm
does not terminate at this point. Instead, it ﬁnds the most disadvantaged papers in the selected assignment and ﬁxes them in the ﬁnal output APf R4A (Step 4), attributing these papers to reviewers according to A. Then it repeats the entire procedure (Steps 2 to 7) again to identify and ﬁx the assignment for the most disadvantaged papers among the remaining papers and so on until the all papers are assigned in APf R4A. We denote the total number of iterations of Steps 2 to 7 in Algorithm 1 as p (≤ m). For any iteration r ∈ [p],
we let Jr be the set of papers which the algorithm, in this iteration, ﬁxes in the resulting assignment. We also let Ar, r ∈ [p], denote the assignment selected in Step 3 of the rth iteration. Note that eventually all the papers are ﬁxed in the ﬁnal assignment APf R4A, and hence we must have Jr = [m].
r∈[p]
Once papers are ﬁxed in the ﬁnal output APf R4A, the assignment for these papers are not changed any more. Thus, at the end of each iteration r ∈ [p] of Steps 2 to 7, the algorithm deletes (Step 6) the columns of
similarity matrix that correspond to the papers ﬁxed in this iteration. For example, at the end of the ﬁrst
iteration, columns which correspond to J1 are deleted from S. For each iteration r ∈ [p], we let Sr denote the similarity matrix at the beginning of the iteration. Thus, we have S1 = S, because at the beginning of the ﬁrst iteration, no papers are ﬁxed in the ﬁnal assignment APf R4A.
Moving forward, we are going to show that for every iteration r ∈ [p], the sum similarity of the worst-oﬀ
papers Jr (which coincides with the fairness of Ar) is close to the best possible, given the assignment for
the all papers ﬁxed in the previous iterations. As in Theorem 1, we will compare the fairness ΓSf Ar
with the fairness of the optimal assignment that Hard algorithm would return if called at the beginning of

14

p
the rth iteration. We stress that for every r ∈ [p], the Hard algorithm assigns papers Jl and respects
l=r
r−1
the constraints on reviewers’ loads, adjusted for the assignment of papers Jl in APf R4A. We denote the
l=1
corresponding assignment as AHf ARD(J{r:p}). Note that AHf ARD(J{1:p}) = AHf ARD. The following corollary
summarizes the main result of this section:

Corollary 1. For any integer r ∈ [p], the assignment Ar, selected by the PeerReview4All algorithm in Step 3 of the rth iteration, guarantees the following lower bound on the fairness objective (4):

ΓSf Ar

max (κf (s∗κ) + (λ − κ)f (s∗∞))
κ∈[λ]

ΓSf AHf ARD(J{r:p}) ≥ κm∈i[λn] ((κ − 1)f (s∗0) + (λ − κ + 1) f (s∗κ)) ≥ 1/λ, (8)

where values s∗κ, κ ∈ {0, . . . , λ} ∪ {∞}, are deﬁned with respect to the similarity matrix Sr and constraints on
r−1
reviewers’ loads adjusted for the assignment of papers Jl in APf R4A.
l=1
The corollary guarantees that each time the algorithm ﬁxes the assignment for some papers j ∈ M in APf R4A, the sum similarity for these papers (which is smallest among papers from M) is close to the optimal fairness, where optimal fairness is conditioned on the previously assigned papers. In case r = 1, the bound (8) coincides with the bound (7) from Theorem 1. Hence, once the assignment for the most worst-oﬀ papers is ﬁxed, the PeerReview4All algorithm adjusts maximum reviewers’ loads and looks for the most fair assignnment of the remaining papers.

5.3 Comparison to past literature
In this section we discuss how the approximation results established in previous sections relate to the past literature.
First, we note that the assignment A1, computed in Step 2 in the ﬁrst iteration of Steps 2 to 7 of Algorithm 1, recovers the assignment of Hartvigsen et al. (1999), thus ensuring that our algorithm is at least as fair as theirs. Second, if the goal is to assign only one reviewer (λ = 1) to each of the papers, then our PeerReview4All algorithm ﬁnds the optimally fair assignment and recovers the classical result of Garﬁnkel (1971).
In the remainder of this section, we provide a comparison between the guarantees of the PeerReview4All algorithm established in Theorem 1 and the guarantees of the ILPR algorithm (Garg et al., 2010). Rewriting the results of Garg et al. (2010) in our notation, we have the bound:

ΓSf AIfLPR

ΓSf AHf ARD − (f (s∗0) − f (s∗∞))

f (s∗) − f (s∗ )

≥

=1− 0

∞.

(9)

ΓSf AHf ARD

ΓSf AHf ARD

ΓSf AHf ARD

Note that our bound (7) for our PeerReview4All algorithm is multiplicative and bound for the ILPR algorithm is additive which makes them incomparable in a sense that neither one dominates another. However, we stress the following diﬀerences. First, if we assume f to be upper-bounded by one, then assignment AILPR
satisﬁes the bound

ΓSf AIfLPR ≥ ΓSf AHf ARD − 1.

(10)

This bound gives a nice additive approximation factor — for a large value of the optimal fairness ΓSf AHf ARD , the constant additive factor is negligible. However, if the optimal fairness is small, which can happen if some papers do not have a suﬃcient number of high-expertise reviewers, then the lower bound on the fairness of the ILPR assignment (10) becomes negative, making the guarantees vacuous as any arbitrary

15

assignment will achieve a non-negative fairness. Note that this issue is not an artifact of the analysis but is inherent in the ILPR algorithm itself, as we demonstrate in the example presented in Table 1 and in Appendix A.1. In contrast, our algorithm in the worst case has a multiplicative approximation factor 1/λ ensuring that it always returns a non-trivial assignment.
This discrepancy becomes more pronounced if the function f is allowed to be unbounded, and the similarities are signiﬁcantly heterogeneous. Suppose there is some reviewer i ∈ [n] and paper j ∈ [m] such that f (sij) ΓSf AHARD . Then the bound (9) for the ILPR algorithm again becomes vacuous, while the bound (7) for the PeerReview4All algorithm continues to provide a non-trivial approximation guarantee.
Finally, we note that the bound (9) is also extended by Garg et al. (2010) to obtain guarantees on the fairness for the second worst-oﬀ paper and so on.

6 Objective-score model
We now turn to establishing statistical guarantees for our PeerReview4All algorithm from Section 4. We begin by considering an “objective” score model which we borrow from past works.

6.1 Model setup

The objective-score model assumes that each paper j ∈ [m] has a true, unknown quality θj∗ ∈ R and each

reviewer i ∈ [n] assigned to paper j gives her/his estimate yij of θj∗. The eventual goal is to estimate top k

papers

according

to

true

qualities

θ

∗ j

,

j

∈

[m].

Following

the

line

of

works

by

Ge

et

al.

(2013);

McGlohon

et al. (2010); Dai et al. (2012); Sajjadi et al. (2016), we assume the score yij given by any reviewer i ∈ [n] to

any paper j ∈ [m] to be independently and normally distributed around the true paper qualities:

yij ∼ N θj∗, σi2j .

(11)

Note that McGlohon et al. (2010); Dai et al. (2012) and Sajjadi et al. (2016) consider the restricted setting with σij = σi for all (i, j) ∈ [n] × [m], which implies that the variance of the reviewers’ scores depends only on the reviewer, but not on the paper reviewed. We claim that this assumption is not appropriate for our peer-review problem: conferences today (such as ICML and NeurIPS) cover a wide spectrum of research areas and it is not reasonable to expect the reviewer to be equally competent in all of the areas.
In our analysis, we assume that the noise variances are some function of the underlying computed similarities.4 We assume that for any i ∈ [n] and j ∈ [m], the noise variance

σi2j = h(sij ),

for some monotonically decreasing function h : [0, 1] → [0, ∞). We assume that this function h is known; this assumption is reasonable as the function can, in principle, be learned from the data from the past conferences.
We note that the model (11) does not consider reviewers’ biases. However, some reviewers might be more stringent while others are more lenient. This diﬀerence results in score of any reviewer i for any paper j being centered not at θj∗, but at (θj∗ + bi). A common approach to reduce biases in reviewers’ scores is a post-processing. For example, Ge et al. (2013) compared diﬀerent statistical models of reviewers in attempt to calibrate the biases; the techniques developed in that work may be extended to the reviewer model (11). Thus, we leave that bias term out for simplicity.

6.2 Estimator
Given a valid assignment A ∈ A, the goal of an estimator is to recover the top k papers. A natural way to do so is to compute the estimates of true paper scores θj∗ and return top k papers with respect to these estimated scores. The described estimation procedure is a signiﬁcantly simpliﬁed version of what is happening in the
4Recall that the similarities can capture not only aﬃnity in research areas but may also incorporate the bids or preferences of reviewers, past history of review quality, etc.

16

real-world conferences. Nevertheless, this fully-automated procedure may serve as a guideline for area chairs, providing a ﬁrst-order estimate of the total ranking of submitted papers. In what follows, we refer to any estimator as θ and to the estimated score of any paper j as θj. Speciﬁcally, we consider the following two estimators:

• Maximum likelihood estimator (MLE) θMLE





θjMLE =

1
1

yij ∼ N θ∗,

σi2j

j

1

1

. 

σ2

i∈RA(j) σi2j i∈RA(j)

i∈RA(j) ij

(12)

Under the model (11), θjMLE is known to have minimal variance across all linear unbiased estimations. The choice of θMLE follows a paradigm that more experienced reviewers should have higher weight in decision making.
• Mean score estimator (MEAN) θMEAN





θjMEAN = λ1

yij ∼ N θj∗, λ12

σi2j  .

i∈RA (j )

i∈RA (j )

(13)

The mean score estimator is convenient in practice because it is not tied to the assumed statistical model, and in the past has been found to be predictive of ﬁnal acceptance decisions in peer-review settings such as National Science Foundation grant proposals (Cole et al., 1981) and homework grading (Sajjadi et al., 2016). This observation is supported by the program chair of ICML 2012 John Langford, who notices in his blog (Langford, 2012) that in ICML 2012 the decisions on the acceptance were “surprisingly uniform as a function of average score in reviews”.

6.3 Analysis
Here we present statistical guarantees for both θMLE and θMEAN estimators and for both exact top k recovery and recovery under a Hamming error tolerance.

6.3.1 Exact top k recovery
Let us use (k) and (k + 1) to denote the indices of the papers that are respectively ranked kth and (k + 1)th according to their true qualities. Similar to the past work by Shah and Wainwright (2015) on top k item recovery, a central quantity in our analysis is a k-separation threshold ∆k deﬁned as:

∆k := θ(∗k) − θ(∗k+1) > 0.

(14)

Intuitively, if the diﬀerence between kth and (k + 1)th papers is large enough, it should be easy to recover top k papers. To formalize this intuition, for any value of a parameter δ ≥ 0, consider a family Fk of papers’ scores

Fk(δ) := (θ1, . . . , θm) ∈ Rm θ(k) − θ(k+1) ≥ δ .

(15)

For the ﬁrst half of this section, we assume that function h is bounded, that is, h : [0, 1] → [0, 1].5 This assumption implicitly assumes that every reviewer i ∈ [n] can provide a minimum level of expertise while reviewing any paper j ∈ [m] even if she/he has zero similarity sij = 0 with that paper.
5More generally, we could consider bounded function h with range [0, c] for some c > 0. Without loss of generality, we set c = 1 which can always be achieved by appropriate scaling.

17

In addition to the gap ∆k, the hardness of the problem also depends on the similarities between reviewers and papers. For instance, if all reviewers have near-zero similarity with all the papers, then recovery is impossible unless the gap is extremely large. In order to quantify the tractability of the problem in terms of the similarities we introduce the following set S of families of similarity matrices parameterized by a non-negative value q:

S(q) := S ∈ [0, 1]n×m ΓS1−h AH1−AhRD ≥ q .

(16)

In words, if similarity matrix S belongs to S(q), then the fairness of the optimally fair (with respect to f = 1 − h) assignment is at least q.
Finally, we deﬁne a quantity τq that captures the quality of approximation provided by PeerReview4All:

ΓS1−h AP1−Rh4A

τq

:=

inf
S ∈S (q)

ΓS

AHARD .

1−h 1−h

(17)

Note that Theorem 1 gives lower bounds on the value of τq. Having deﬁned all the necessary notation, we are ready to present the ﬁrst result of this section on
recovering the set of top k papers Tk∗.

Theorem 2. (a) For any ∈ (0, 1/4), q ∈ [λ (1 − h(0)) , λ] and any monotonically decreasing h : [0, 1] → [0, 1],
√
if δ > 2 λ 2 (λ − qτq) ln √m , then for A, θ ∈ AP1−Rh4A, θMEAN , APh−R14A, θMLE

sup

P Tk A, θ

(θ1∗,...,θm ∗ )∈Fk(δ)

S ∈S (q)

= Tk∗

≤.

(18)

(b) Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, 1] and any q ∈
[λ (1 − h(0)) , λ], there exists a universal constant c > 0 such that if m > 6 and δ < λc (λ − q) ln m, then

sup inf

sup

P

S∈S(q) (θ,A∈A) (θ1∗,...,θm ∗ )∈Fk(δ)

Tk A, θ

= Tk∗

1 ≥.
2

Remarks. 1. The PeerReview4All assignment algorithm thus leads to a strong minimax guarantee on the

recovery

of

the

top

k

papers:

the

upper

and

lower

bounds

diﬀer

by

at

most

a

τq

≥

1 λ

term

in

the

requirement

on δ and constant pre-factor. Also note that as discussed in Section 5.1, approximation factor τq of the

PeerReview4All algorithm can be much better than 1/λ for various similarity matrices.

2. In addition to quantifying the performance of PeerReview4All, an important contribution of

Theorem 2 is a sharp minimax analysis of the performance of every assignment algorithm. Indeed, the

approximation ratio τq (17) can be deﬁned for any assignment algorithm, by substituting corresponding assignment instead of AP1−Rh4A. For example, if one has access to the optimal assignment AHARD (e.g., by using PeerReview4All if λ = 1) then we will have corresponding approximation ratio τq = 1 thereby yielding

bounds that are sharp up to constant pre-factors.

3. While on one hand the estimator θMLE is preferred over θMEAN when model (11) is correct, on the

other hand, if h(s) ∈ [0, 1], then the estimator θMEAN is more robust to model mismatches.

4. The technical assumption q ∈ [λ (1 − h(0)) , λ] is made without loss of any generality, because values

of q outside this range are vacuous. In more detail, for any similarity matrix S ∈ [0, 1]n×m, it must be

that ΓS1−h AH1−AhRD ≥ λ (1 − h(0)). Moreover, the co-domain of function h comprises only non-negative real values, implying that ΓS1−h AH1−AhRD ≤ λ for any similarity matrix S ∈ [0, 1]n×m.
5. The upper bound of the theorem holds for a slightly more general model of reviewers — reviewers with

sub-Gaussian noise. Formally, in addition to the Gaussian noise model (11), the proof of Theorem 2(a) also

holds for the following class of distributions of the score yij:

yij = θi∗j + sG (h(sij )) ,

(19)

where sG σ2 is an arbitrary mean zero sub-Gaussian random variable with scale parameter σ2.

18

The conditions of Theorem 2 require function h to be bounded. We now relax our earlier boundedness assumption on h and consider h : [0, 1] → [0, ∞).
In what follows we restrict our attention to MLE estimator θMLE which represents the paradigm that reviewers with higher similarity should have more weight in the ﬁnal decision. In order to demonstrate that our PeerReview4All algorithm is able to adapt to diﬀerent structures of similarity matrices — from hard cases when optimal assignment provides only one strong reviewer for some of the papers, to ideal cases when there are λ strong reviewers for every paper — let us consider the following set Sκ of families of similarity matrices parametrized by a non-negative value v and integer parameter κ ∈ [λ]:

Sκ(v) := S ∈ [0, 1]n×m s∗κ ≥ v .

(20)

Here s∗κ is as deﬁned in (6). In words, the parameter v deﬁnes the notion of strong reviewer while parameter κ denotes the maximum
number of strong (with similarity higher than v) reviewers that can be assigned to each paper without violating the (µ, λ) conditions.
Then the following adaptive analogue of Theorem 2 holds:

Corollary 2. (a) For any ∈ (0, 1/4), v ∈ [0, 1], κ ∈ [λ] and any monotonically decreasing h : [0, 1] → [0, ∞),

√ if δ > 2 2

h(v)h(0) ln √m , then

κh(0)+(λ−κ)h(v)

sup

P

(θ1∗,...,θm ∗ )∈Fk(δ)

S ∈Sκ (v )

Tk(APh−R14A, θMLE) = Tk∗

≤.

(b) Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, ∞), any v ∈ [0, 1], and any κ ∈ [λ], there exists a universal constant c > 0 such that if m > 6 and δ ≤ c κh(0h)+(v()λh−(0κ))h(v) ln m, then

sup inf

sup

P Tk(A, θ) = Tk∗

S∈Sκ(v) (θ,A∈A) (θ1∗,...,θm ∗ )∈Fk(δ)

1 ≥.
2

Remarks. 1. Observe that there is no approximation factor in the upper bound. Thus, the PeerReview4All algorithm together with θMLE are simultaneously minimax optimal up to a constant pre-factor in classes of
similarity matrices Sκ(v) for all κ ∈ [λ], v ∈ [0, 1]. 2. Corollary 2(a) remains valid for generalized sub-Gaussian model of reviewer (19).
3. Corollary 2 together with Theorem 2 show that our PeerReview4All algorithm produces the assignment APh−R14A which is simultaneously minimax (near-)optimal for various classes of similarity matrices. We thus see that our PeerReview4All algorithm is able to adapt to the underlying structure of similarity matrix S in order to construct an assignment in which even the most disadvantaged paper gets reviewers
with suﬃcient expertise to estimate the true quality of the paper.

6.3.2 Approximate recovery under Hamming error
Although our ultimate goal is to recover set Tk∗ of top k papers exactly, we note that often scores of boundary papers are close to each other so it may be impossible to distinguish between the kth and (k + 1)th papers in the total ranking. Thus, a more realistic goal would be to try to accept papers such that the set of accepted papers is in some sense “close” to the set Tk∗. In this work we consider the standard notion of Hamming distance (1) as a measure of closeness. We are interested in minimizing the quantity:

P

DH

Tk

A, θ

,

T

∗ k

> 2t

for some user-deﬁned value of t ∈ [k − 1].

19

Similar to the exact recovery setup, the key role in the analysis is played by generalized separation threshold (compare with equation 14):
∆k,t := θ(∗k−t) − θ(∗k+t+1),
where (k − t) and (k + t + 1) are indices of papers that take (k − t)th and (k + t + 1)th positions respectively in the underlying total ranking. For any value of δ > 0 we consider the following generalization of the set Fk(δ) deﬁned in (15):
Fk,t(δ) := (θ1, . . . , θm) ∈ Rm θ(k−t) − θ(k+t+1) ≥ δ .

Also recall the family of matrices S(q) from (16) and the approximation factor τq from (17) for any parameter q. With this notation in place, we now present the analogue of Theorem 2 in case of approximate recovery under the Hamming error.
Theorem 3. (a) For any ∈ (0, 1/4), q ∈ [λ (1 − h(0)) , λ], t ∈ [k − 1], and any monotonically decreasing
√
h : [0, 1] → [0, 1], if δ > 2 λ 2 (λ − qτq) ln √m , then for A, θ ∈ AP1−Rh4A, θMEAN , APh−R14A, θMLE

sup

P

DH

Tk

A, θ

,

T

∗ k

> 2t

≤.

(θ1∗,...,θm ∗ )∈Fk,t(δ)

S ∈S (q)

(b) Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, 1], any q ∈ [λ (1 − h(0)) , λ],
and any 0 < t < k, there exists a universal constant c > 0 such that for given constants ν1 ∈ (0; 1) and ν2 ∈ (0, 1) if 2t ≤ 1+1ν2 min m1−ν1 , k, m − k and δ ≤ λc (λ − q) ν1ν2 ln m, then for m larger than some (ν1, ν2)-dependent constant,

sup inf

sup

P

DH

Tk

A, θ

,T ∗

> 2t

1 ≥.

S∈S(q) (θ,A∈A) (θ∗,...,θ∗ )∈F (δ)

k

2

1

m

k,t

Remarks. This theorem provides a strong minimax characterization of the PeerReview4All algorithm for approximate recovery. Note that upper and lower bounds diﬀer by the approximation factor τq, which is at most λ1 , and a pre-factor which depends only on the constants ν1 and ν2.
To conclude the section, we state the result for the family Sκ(v) of similarity matrices deﬁned in (20) for any parameter v, showing that adaptive behavior of PeerReview4All algorithm (Corollary 2) also carries over to the Hamming error metric.

Corollary 3. (a) For any ∈ (0, 1/4), v ∈ [0, 1], κ ∈ [λ], t ∈ [k − 1], and any monotonically decreasing

√ h : [0, 1] → [0, ∞), if δ > 2 2

h(v)h(0) ln √m , then

κh(0)+(λ−κ)h(v)

sup

P

DH

Tk

APh−R14A, θMLE

,

T

∗ k

> 2t

≤.

(θ1∗,...,θm ∗ )∈Fk,t(δ)

S ∈Sκ (v )

(b) Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, ∞), any v ∈ [0, 1], κ ∈ [λ] and any t ∈ [k − 1], there exists a universal constant c > 0 such that for given constants ν1 ∈ (0; 1) and ν2 ∈ (0, 1) if 2t ≤ 1+1ν2 min m1−ν1 , k, m − k and δ ≤ c κh(0h)+(v()λh−(0κ))h(v) ν1ν2 ln m, then for m larger than some (ν1, ν2)-dependent constant,

sup inf

sup

P

DH

Tk

A, θ

,T ∗

> 2t

1 ≥.

S∈Sκ(v) (θ,A∈A) (θ∗,...,θ∗ )∈F (δ)

k

2

1

m

k,t

The results established in this section thus show that our PeerReview4All algorithm produces an assignment which is minimax (near-)optimal for both exact and approximate recovery of the top k papers.

20

7 Subjective-score model
In the previous section, we analyzed the performance of our PeerReview4All assignment algorithm under a model with objective scores. Indeed, various past works on peer-review (as well as various other domains of machine learning) assume existence of some “true” objective scores or ranking of the underlying items (papers). However, in practice, reviewers’ opinions on the quality of any paper are typically highly subjective (Kerr et al., 1977; Mahoney, 1977; Ernst and Resch, 1994; Bakanic et al., 1987; Lamont, 2009). Even two highly experienced researchers with vast experience and expertise may have considerably diﬀering opinions about the contributions of a paper. Following this intuition, we wish to move away from the assumption of some true objective scores {θj∗}j∈[m] of the paper.
With this motivation, in this section we develop a novel model to capture such subjective opinions and present a statistical analysis of our assignment algorithm under this subjective-score model.

7.1 Model

The key idea behind our subjective score model is to separate out the subjective part in any reviewer’s opinion from the noise inherent in it. Our model is best described by ﬁrst considering a hypothetical situation where every reviewer spends an inﬁnite time and eﬀort on reviewing every paper, gaining a perfect expertise in the ﬁeld of that paper and a perfect understanding of the paper’s content. We let θij ∈ R denote the score that this fully competent version of reviewer i ∈ [n] would provide to paper j ∈ [m], and denote the matrix

of reviewers subjective scores as Θ = θij

. Continuing momentarily in this hypothetical world,

i∈[n],j∈[m]

when all the reviewers are fully competent in evaluating all the papers, every feasible reviewer-assignment is

of the same quality since there is no noise in the reviewers’ scores. Since all reviewers have an equal, full

competence, a natural choice of scoring any paper j ∈ [m] is to take the mean score provided by the fully

competent reviewers who review that paper:

1

θj (A) := λ

θij .

i∈RA (j )

(21)

Let us now exit our hypothetical world and return to reality. In a real conference peer-review setting the reviews will be noisy. Following the previous noise assumptions, we assume that score of any reviewer i ∈ [n] for any paper j ∈ [m] that she/he reviews is distributed as

yij ∼ N (θij , h(sij )),
for some known continuous strictly monotonically decreasing function h : [0, 1] → [0, 1]. Under this model, the higher the similarity sij, the better the score yij represents the subjective score θij which reviewer i ∈ [n] would give to paper j ∈ [m] if she/he had inﬁnite expertise.
The goal under this model is to assign reviewers to papers such that reviewers are of enough ability to convey their opinions θij from the hypothetical full-competence world to the real world with scores yij. In other words, the goal of the assignment is to ensure the recovery of the top k papers in terms of the mean full-competence subjective scores {θj }j∈[m].

7.2 Analysis
In this section we present statistical guarantees for θMEAN in context of subjective-score model.

7.2.1 Exact top k recovery
Since the true scores for any reviewer-paper pair are subjective, and since we are interested in mean fullcompetence subjective scores, a natural choice for estimating {θj } from the actual provided scores {yij} is

21

the

averaging

estimator

θMEAN

which

for

every

paper

j

∈ [m]

estimates

θj

as

θjMEAN

=

1 λ

yij. Having

i∈RA (j )

deﬁned the model and estimator, we now provide a sharp minimax analysis for the subjective-score model.

In order to state our main result, we recall the family of similarity matrices S(q) deﬁned earlier in (16) and

the approximation ratio τq deﬁned in (17), both parameterized by some non-negative value q.

Note that the notion of the k-separation threshold (14) does not carry over directly from the objective

score model to the subjective score model. The reason is that the ranking now is induced by the assignment

and changes as we change the assignment. Consequently, we introduce the following family of papers’ scores

that are governed by the assignment A and parametrized by a positive real value δ:

Fk(A, δ) = Θ ∈ Rn×m θ(k)(A) − θ(k+1)(A) ≥ δ .

(22)

Since in this section we consider only mean score estimator θMEAN, we omit index 1 − h from AP1−Rh4A, but always imply that assignment APR4A is built with respect to the function 1 − h. For every feasible assignment A, we augment the notation Tk∗ with Tk A, θ (A) to highlight that the set of the top k papers is induced by the assignment A. Let us now present the main result of this section.
Theorem 4. (a) For any ∈ (0, 1/4), q ∈ [λ (1 − h(0)) , λ] and any monotonically decreasing h : [0, 1] → [0, 1],
√
if δ > 2λ2 (λ − qτq) ln √m , then

sup P Tk(APR4A, θMEAN) = Tk APR4A, θ (APR4A) ≤ .
Θ∈Fk (APR4A ,δ)
S ∈S (q)

(b) Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, 1] and any q ∈
[λ (1 − h(0)) , λ], there exists a universal constant c > 0 such that if m > 6 and δ < λc (λ − q) ln m, then

sup inf

sup P Tk(A, θ) = Tk

S∈S(q) (θ,A∈A) Θ∈Fk(A,δ)

A, θ (A)

1 ≥.
2

We thus see that our assignment algorithm PeerReview4All not only leads to the strong guarantees under the objective-score model but simultaneously also under the setting where the opinions of reviewers may be subjective.

7.2.2 Approximate recovery under Hamming error We now present guarantees for approximate recovering under the Hamming error for the PeerReview4All algorithm. We generalize the family of score matrices (22), for which we consider any integer error tolerance parameter t ∈ {0, . . . , k − 1} and any any feasible assignment A. Then we deﬁne the following family of subjective papers’ scores, parameterized by non-negative value δ:
Fk,t(A, δ) = Θ ∈ Rn×m θ(k−t)(A) − θ(k+t+1)(A) ≥ δ .
Observe that the class Fk,t(A, δ) coincides with the class Fk(δ) from (22) when t = 0.
Theorem 5. (a) For any ∈ (0, 1/4), q ∈ [0, λ], t ∈ [k−1], and any monotonically decreasing h : [0, 1] → [0, 1],
√
if δ > 2λ2 (λ − qτq) ln √m , then

sup

P

DH

Tk

APR4A, θ

,

T

∗ k

APR4A, θ (APR4A)

Θ∈Fk,t (APR4A ,δ )

S ∈S (q)

> 2t ≤ .

22

Conversely, for any continuous strictly monotonically decreasing h : [0, 1] → [0, 1], any q ∈ [λ (1 − h(0)) , λ],
and any 0 < t < k, there exists a universal constant c > 0 such that for given constants ν1 ∈ (0, 1) and ν2 ∈ (0, 1) if 2t ≤ 1+1ν2 min m1−ν1 , k, m − k and δ ≤ λc (λ − q) ν1ν2 ln m, then for m larger than some (ν1, ν2)-dependent constant,

sup inf

sup

P

DH

Tk

A, θ

,

T

∗ k

A, θ (A)

S∈S(q) (θ,A∈A) Θ∈Fk,t(A,δ)

1 > 2t ≥ .
2

Similar to Theorem 4, Theorem 5 shows that PeerReview4All algorithm is minimax optimal up to a constant pre-factor and approximation factor given that reviewers’ subjective scores Θ belong to the class Fk,t(A, δ).

8 Experiments
In this section we conduct empirical evaluations of the PeerReview4All algorithm and compare it with the TPMS (Charlin and Zemel, 2013), ILPR (Garg et al., 2010) and Hard algorithms. Our implementation of the PeerReview4All algorithm picks max-ﬂow with maximum cost in Step 6 of Subroutine 1.
Previous work on the conference paper assignment problem (Garg et al., 2010; Long et al., 2013; Karimzadehgan et al., 2008; Tang et al., 2010) conducted evaluations of the proposed algorithms in terms of various objective functions that measure the quality of the assignment. For example, Garg et al. (2010) compared fairness from reviewers’ perspective using the number of satisﬁed bids as a criteria. While these evaluations allow to compare algorithms in terms of particular objective, we note that the main goal of the peer-review system is to accept the best papers. It is not straightforward whether an improvement of some other objective will lead to the improvement of the quality of the paper acceptance process.
In contrast to the prior works, in this section we not only consider the fairness objective (Subsections 8.2 and 8.3), but also design experiments (Subsections 8.1 and 8.4) to directly evaluate the accuracy resulting from the assignment procedures.

8.1 Synthetic simulations
We begin with synthetic simulations. We consider the instance of the reviewer assignment problem with m = n = 100 and λ = µ = 4. We select the moderate values of m and n to keep track of the optimal assignment AHARD which we ﬁnd as a solution of the corresponding integer linear programming problem. For every real-valued constant c, we denote the matrix with all entries being equal to c as c. Similarly, we denote the matrix with entries independently sampled from a Beta distribution with parameters (α, β) as B (α, β).
We consider the objective-score model of reviewers (11) with h(s) = 1 − s together with estimator θMLE. Thus, assignments APR4A, AILPR and AHARD aim to optimize ΓS(1−s)−1 (A) while assignment ATPMS aims to maximize the cumulative sum of similarities GS (A) as deﬁned in (2).
In what follows we simulate the following problem instances:

(C1) Non-mainstream papers. There are m1 = 80 conventional papers for which there exist n1 = 80 expert reviewers with high similarity, and m2 = 20 non-mainstream papers for which all the reviewers have similarity smaller than or equal to 0.5. There are also n2 = 20 weak reviewers who have moderate similarities with papers from the ﬁrst group and low similarities with papers from the second group.
The similarities are given by the block matrix:

S1 =

0.9 0.5 } 80 0.5 0.15 } 20

80

20

23

ATPMS AHARD AILPR APR4A

Case 1
4.7 8.0 8.0 8.0

Fairness ΓS(1−s)−1 (A)

Case 2 Case 3 Case 4

5.1

13.3

4.0

13.1

26.6

14.0

5.0

4.0

14.0

13.1

22.0

6.5

Case 5
10.9 10.9 10.9 10.9

Sum of Similiarities GS (A)

Case 1
300 296 296 296

Case 2
168 162 165 166

Case 3
295 232 188 239

Case 4
296 234 293 290

Case 5
311 175 296 309

Table 2: Comparison of assignment produced by PeerReview4All, Hard, ILPR and TPMS algorithms in terms of the fairness and the sum of similarities (higher values are better).

(C2) Many weak reviewers. In this scenario there are n1 = 25 strong reviewers with high similarity with every paper and n2 = 75 weak reviewers with small similarity with every paper:

S2 =

0.8 + 0.2 × B (1, 3) } 25 0.1 + 0.2 × B (1, 3) } 75

100
(C3) Few super-strong reviewers. The following example tests the algorithms in scenario when some small number of the reviewers are much stronger than the others. Similarities for this scenario are given by the block matrix:

 0.98 S3 =  0
0.9
60

0.9  } 10 0.7  } 50 0.9 } 40
40

(C4) Adverse case. Having analyzed the inner working of our PeerReview4All algorithm, we construct a similarity matrix which is hard for the algorithm to compute the fair assignment.6

(C5) Sparse similarities. Each entry of similarity matrix S5 is zero with probability 0.8 or otherwise is drawn independently and uniformly at random from [0.1, 0.9].

8.1.1 Fairness
In this section we analyze the quality of assignments produced by PeerReview4All, Hard, ILPR and TPMS algorithms and for all the ﬁve cases described above. The results are summarized in Table 2 where we compute the measures of fairness ΓS(1−s)−1 (A) and the conventional sum of similarities GS (A) for each of the assignments.
The results in Table 2 show that in all ﬁve cases PeerReview4All algorithm ﬁnds an assignment APR4A with at least as much fairness as ATPMS. At the same time, the max cost heuristic that we use in Step 6 of Subroutine 1 helps the average quality (total sum similarity) of the assignment APR4A to be either close to or larger than average quality of both AILPR and AHARD.
In Case (C1), the TPMS algorithm sacriﬁces the quality of reviewers for non-mainstream papers, assigning them to weak reviewers. In contrast, all other algorithms assign four best possible reviewers to these unconventional papers in order to maintain fairness. In Case (C2), the PeerReview4All and Hard algorithms assign one strong reviewer for each paper while TPMS, in attempt to maximize the value of its goal function, assigns strong reviewers according to their highest similarities which leads to an unfair assignment. The ILPR algorithm fails to ﬁnd a fair assignment in Cases (C2) and (C3): the poor performance of ILPR algorithm is caused by the fact that some of the reviewers in our examples have similarities close
6We do not give an explicit expression of the matrix S4 for this case, due to its complicated structure.

24

ILPR TPMS PR4A HARD
0.5 0.4 0.3 0.2 0.1 0.0 0.0 0.5 1.0 1.5 2.0
k-separation threshold k
(C3) Few super-strong reviewers.

Error fraction

Error fraction

0.5 0.4 0.3 0.2 0.1 0.0 0.0 0.5 1.0 1.5 2.0
k-separation threshold k
(C1) Non-mainstream papers.
0.5 0.4 0.3 0.2 0.1 0.0 0.0 0.5 1.0 1.5 2.0
k-separation threshold k
(C4) Adverse case.

Error fraction

Error fraction

0.6 0.4 0.2 0.0 0.0 0.5 1.0 1.5 2.0
k-separation threshold k
(C2) Many weak reviewers.
0.6 0.4 0.2 0.0 0.0 0.5 1.0 1.5 2.0
k-separation threshold k
(C5) Sparse similarities.

Error fraction

Figure 1: Fraction of papers incorrectly accepted by θMLE based on assignments produced by PeerReview4All, Hard, ILPR and TPMS for diﬀerent values of the separation threshold.

to maximal, making the value of f (s) = 1−1 s large, which, in turn, makes the approximation guarantee (9) of ILPR algorithm weak. In Case (C4), the PeerReview4All algorithm was unable to recover the fair assignment. Instead, the assignment within approximation ratio 1/3, which is a bit better than the worst case 1/λ = 1/4 approximation, was discovered. Finally, in Case (C5), the all algorithms managed to recover fair assignment. However, we note that the total sum similarity of the AHARD assignment is low as compared to other algorithms. The reason is that the corresponding solution of the integer linear programming problem in the Hard algorithm is optimized for the fairness towards the worst-oﬀ paper and does not try to continue optimization, once the assignment for that paper is ﬁxed. In contrast, both PeerReview4All and ILPR algorithms try to maximize the fate of the second worst-oﬀ paper, when the assignment for the most worst-oﬀ paper is ﬁxed.
8.1.2 Statistical accuracy
As we have pointed out, the main goal of the assignment procedure is to ensure the acceptance of the k best papers Tk∗. While in real conferences the acceptance process is complicated and involves discussions between reviewers and/or authors, here we consider a simpliﬁed scenario. Namely, we assume an objective-score model deﬁned in Section 6 and reviewer model (11) with h(s) = 1 − s.
The experiment executes 1,000 iterations of the following procedure. We randomly choose k = 20 indices of the “true best” papers Tk∗ = {j1, . . . , jk} ⊂ [m]. Each of these papers j ∈ Tk∗ is assigned score θj∗ = 1, while for each of the remaining papers j ∈ [m]\Tk∗ we set θj∗ = 1 − ∆k, where ∆k ∈ (0, 2]. Next, given the similarity matrix S, we compute assignments APR4A, AHARD, AILPR and ATPMS. For each of these assignments we compute the estimations of the set of top k papers using the θMLE estimator and calculate the fraction of wrongly accepted papers.
For every similarity matrix Sr, r ∈ [5], and for every value of ∆k ∈ {0.1k |k ∈ [20]}, we compute the mean of the obtained values over the 1,000 iterations. Figure 1 summarizes the dependence of the fraction of

25

ILPR TPMS PR4A HARD

Sum similarity Sum similarity

40 30 20 10
0 10 Ord2e0red p3a0pers 40 50
(C4) Adverse case.

22.5 20.0 17.5 15.0 12.5 10.0 0 10 20 30 40 50
Ordered papers
(C5) Sparse similarities.

Figure 2: Sum similarity for the 50 most worst-oﬀ papers in assignments produced by PeerReview4All, Hard, ILPR and TPMS.

incorrectly accepted papers on the value of separation threshold ∆k for all ﬁve cases (C1)-(C5). The obtained results suggest that the increase in fairness of the assignment leads to an increase in the
accuracy of the acceptance procedure, provided that the average sum similarity of the assignment does not decrease dramatically. The PeerReview4All algorithm signiﬁcantly outperforms TPMS both in terms of fairness and in terms of fraction of incorrectly accepted papers for the ﬁrst four cases. The low fairness of assignments computed by ILPR in Cases (C2) and (C3) lead to the large fraction of errors in the acceptance procedure. As we noted earlier, the ILPR algorithm has weak approximation guarantees when the function f is allowed to be unbounded. In section 8.4 we will consider the mean score estimator (f (s) = s) which is more suitable scenario for ILPR algorithm.
Interestingly, in Case (C4), the PeerReview4All algorithm recovers sub-optimal assignment in terms of fairness, but still performs well in terms of the accuracy of the acceptance procedure. To understand this eﬀect, for each of the assignments ATPMS, AHARD, AILPR and APR4A we compute the sum similarity for all papers in the assignments and plot these values for 50 the most worst-oﬀ papers in each of the assignment in Figure 2. Despite the inability of PeerReview4All to ﬁnd the fair assignment for the most worst-oﬀ paper, Corollary 1 guarantees that sum similarities for the remaining papers will not be too far from the optimal, and we see this aspect in Figure 2(C4). As one can see, the sum similarity for all but tiny fraction of papers in APR4A is large enough, thus ensuring the low fraction of incorrectly accepted papers.
Finally, note that in Case (C5), the Hard algorithm, while having optimal fairness, has a lower accuracy as compared to other algorithms. As Figure 2(C5) demonstrates, the Hard algorithm does not optimize for the second worst oﬀ paper and recovers sub-optimal assignment for all but the most disadvantaged paper. In contrast, as Figure 2 suggests, the ILPR and PeerReview4All algorithms do not stop their work after the most disadvantaged paper is satisﬁed, but instead continue to optimize the assignment for the remaining papers and eventually ensure not only fairness, but also high average quality of the assignment.
8.2 Experiment on the approximation of ICLR similarity matrix
In absence of publicly available similarity matrices from conferences, we are unable to compare the assignment computed by the PeerReview4All algorithm to the actual conference assignment. To circumvent this issue, we use an approximate version of the similarity matrix from the International Conference on Learning Representations (ICLR’18) that was constructed by Xu et al. (2019b) and compare the performance of the PeerReview4All and TPMS assignment algorithms on this matrix.
8.2.1 Matrix construction
The similarity matrix we use for comparison was constructed by Xu et al. (2019b) as follows. OpenReview (openreview.net) — increasingly popular conference management system — maintains a public database of all papers (with author identities being visible) submitted to the ICLR’18 conference, thereby giving access to the pool of submissions. Next, it was assumed that all authors of submissions are simultaneously reviewers and that
26

Algorithm
ATPMS AP1R4A (one iteration) APR4A (full)

Fairness ΓS (A)
0.12 0.15 (+25%) 0.15 (+25%)

Mean sum of sim. GS (A)
0.413 0.408 (−1%) 0.406 (−2%)

Table 3: Results of the experiment on the approximation of ICLR’18 similarity matrix. Values in brackets represent relative changes as compared to the TPMS assignment.

there are no additional reviewers. The publication proﬁles of reviewers were constructed by scraping the data from databases of scientiﬁc publications. Finally, the open-source code (bitbucket.org/lcharlin/tpms/) and the material of the original paper (Charlin and Zemel, 2013) were used to compute the similarity matrix according to the TPMS procedure.
The process outlined above resulted in the similarity matrix S that has n = 2435 reviewers and m = 911 papers. Additionally, it was assumed that any reviewer has a conﬂict of interests with the submitted papers that she/he has authored; these conﬂicts are represented by a binary matrix C whose (i, j)th entry equals 1 if and only if reviewer i has a conﬂict with paper j. Similarity matrix S possesses a considerable heterogeneity as demonstrated by some papers having mean similarity with non-conﬂicting reviewers almost four times larger than others.
The large size of the similarity matrix makes computation of the optimally fair assignment infeasible, and hence in this section we do not compute the Hard assignment. Additionally, our implementation of the ILPR assignment algorithms was computationally ineﬃcient and in absence of the publicly available source code we also exclude this algorithm from comparison.
8.2.2 Evaluation
Having deﬁned the similarity matrix and matrix of conﬂicts, we compute assignments of papers to reviewers with λ = 4 (each paper is assigned to 4 reviewers) and µ = 2 (each reviewer is allocated at most 2 papers) using the TPMS and PeerReview4All assignment algorithms with the identity transformation function f (s) = s. In addition to the standard load constraints, we require that no paper is assigned to a conﬂicting reviewer. Finally, as pointed out in Section 5.2, the fairness guarantees of Theorem 1 are achieved after the ﬁrst iteration of Steps 2 to 7 of Algorithm 1. Hence, we include the corresponding assignment for comparison and denote it as AP1 R4A.
Table 3 summarizes the results of the experiment, comparing the resulting assignments in terms of fairness (3) and cumulative similarity (2). We see that the fairness of the assignment computed by the PeerReview4All algorithm is signiﬁcantly higher than the fairness of the TPMS algorithm. Similar to the case of synthetic simulations, the max cost heuristic used in Step 6 of Subroutine 1 helps our algorithm to maintain a high value of cumulative similarity, which is only marginally below the optimal value.
The large size of the similarity matrix at hands makes evaluation of the optimal fairness achieved by AHARD computationally prohibitive. However, we can still ﬁnd an upper bound on ΓS AHARD by dropping reviewer load constraints and allowing all reviewers to review unlimited number of papers. The resulting bound allows us to compute a lower bound on the approximation ratio of the PeerReview4All algorithm:
ΓS APR4A ΓS (AHARD) ≥ 0.98,
which shows that in practice the approximation factor of the PeerReview4All algorithm can be much better than the worst-case approximation factor λ1 guaranteed by Theorem 1.
Continuing the analysis, for each of the assignments ATPMS, AP1 R4A and APR4A we compute the sum similarity for all papers in the assignments and plot these values for 100 the most worst-oﬀ papers in each of the assignment in Figure 3a. This ﬁgure demonstrates that while the fairness guarantees of Theorem 1

27

Sum similarity Ratio

0.250 0.225 0.200 0.175 0.150 0.125
0

TPMS PR4A P(ORn4eA iteration)
20Orde40red p6a0pers 80 100

(a) Sum similarity for 100 most disadvantaged papers in each assignment.

1.2
1.0
0.8
0.6 0 200 400 600 800
Ordered papers
(b) Ratio of ordered sum similarities in APR4A to ordered sum similarities in ATPMS.

Figure 3: Comparison on the approximation of ICLR’18 similarity matrix.

can be achieved by a single iteration of Steps 2 to 7, subsequent iterations help to improve the assignment for the second worst-oﬀ paper and so on. Finally, for each of the assignments ATPMS and APR4A we sort papers in order of increasing sum similarity of assigned reviewers and plot the ratios (PeerReview4All to TPMS) of these sums in Figure 3b. Figure 3b shows that the PeerReview4All algorithm indeed balances the assignment by improving the quality for the worst-oﬀ papers at the expense of decreasing the quality for the most beneﬁting papers.
8.3 Experiment on MIDL and CVPR similarity matrices
Subsequent to the publication of the ﬁrst version of this paper (Stelmakh et al., 2019b), a follow-up paper by Kobren et al. (2019) has been published. There authors propose two novel assignment algorithms that also aim at ensuring the fairness of the assignment. In that work, the PeerReview4All algorithm with the identity transformation function (f (s) = s) was compared with other assignment algorithms on similarity matrices from three real conferences: Medical Imaging and Deep Learning Conference (MIDL), and two editions of the Conference on Computer Vision and Pattern Recognition (CVPR’17 and CVPR’18). With the kind permission of Ari Kobren, we describe the results of their experiments in which our algorithm was evaluated.
8.3.1 Brief discussion of the algorithms by Kobren et al.
We begin with a brief theoretical comparison of the PeerReview4All algorithm with the algorithms proposed by Kobren et al. (2019). Recall that the PeerReview4All algorithm aims at optimizing fairness of the assignment (3) and does not directly optimize for the total sum similarity. However, when in its inner workings the algorithm faces a choice between diﬀerent suitable similarity matrices (Step 6 of the Subroutine 1), it can heuristically optimize for the total sum similarity by using the max cost heuristic. In contrast, Kobren et al. (2019) consider a problem of optimizing for the total sum similarity of the assignment with an additional constraint of each paper having the sum similarity larger than some threshold T , which can be speciﬁed by user or found by the binary search. They design two novel algorithms which we refer to as FairIr and FairFlow.
Given a feasible instance of the reviewer assignment problem, the FairIr algorithm is able to compute the assignment with the optimal value of the total sum similarity, violating the fairness constraints by an additive factor which is upper bounded by the maximum entry of the similarity matrix. In that, fairness guarantees of FairIr are equivalent to those of ILPR (and hence may become vacuous when similarity matrix is signiﬁcantly heterogeneous), but additionally the FairIr algorithm achieves the highest possible value of

28

Conference MIDL CVPR’17 CVPR’18

Parameters
n = 177, m = 118 λ = 3, µ = 4
n = 1373, m = 2623 λ = 3, µ = 6
n = 2840, m = 5062 λ = 3, µ = 9

Algorithm
ATPMS APR4A AFairIr AFairFlow
ATPMS AP1 R4A AFairIr AFairFlow
ATPMS AP1 R4A AFairIr AFairFlow

Time (s)
0.1 293.8
1.6 1.2
47 3251 595 225
257 8684 3786 910

Fairness ΓS (A)
0.90 0.92 0.93 0.94
0 0.77 0.27 0.77
1.37 12.68 7.19 11.12

Mean sum of sim. GS (A)
1.71 1.67 1.71 1.68
2.08 1.96 2.05 1.69
22.23 21.48 22.18 17.98

Table 4: Results of the experiment conducted by Kobren et al. on similarity matrices from real conferences.
On large instances only a single iteration of the PeerReview4All algorithm was computed and the corresponding assignment is denoted AP1 R4A.

sum similarity.7 The FairFlow algorithm is a heuristic which does not have theoretical guarantees, but in return has much lower computational complexity.
Another diﬀerence between PeerReview4All and the algorithms proposed by Kobren et al. (2019) is that both FairIr and FairFlow allow to specify a lower bound on reviewer load, thereby ensuring that each reviewer reviews at least some number of papers. In our work, we do not study such constraints and PeerReview4All does not support such constraints as is. Hence, below we report only those comparisons in which our algorithm was evaluated by Kobren et al. (2019), that is, the comparisons in which the lower bound on reviewer load was not enforced.
Overall, the FairIr and FairFlow algorithms aim at balancing the fairness and the total sum similarity of the assignment. By choosing an appropriate heuristic in Step 6 of the Subroutine 1, one can ensure that PeerReview4All also heuristically optimizes for the total sum similarity. Let us now report the experimental results of Kobren et al. (2019) that allows to compare the algorithms on both objectives of fairness and total sum similarity.
8.3.2 Summary of the experiments
The key summary statics of the Kobren et al. (2019) experiments are represented in Table 4.8 For each similarity matrix, the assignments respecting the corresponding paper and reviewer load constraints were computed by the TPMS, PeerReview4All, FairIr and FairFlow algorithms. These assignments were then compared based on (a) running time of the algorithm, (b) fairness of the assignment and (c) mean sum similarity of the assignment. First, we notice that our naive implementation of the PeerReview4All algorithm is signiﬁcantly slower than all other algorithms, and for large instances only a single iteration of the algorithm can be computed in a reasonable time (recall that even one iteration is suﬃcient to satisfy the fairness guarantees of Theorem 1). Nonetheless, even on the largest instance with more than 5,000 papers the running time of the ﬁrst iteration of our algorithm took less than three hours which is still feasible given that the full assignment procedure needs to be run only once in the conference timeline.
The remaining two dimensions of comparison represent two notions of quality of the assignment: fairness and total sum similarity. Ideally, we would like to have an algorithm which simultaneously optimizes both of
7Observe that this value is lower than those achieved by TPMS as FairIr has additional constraint on the fairness of the assignment.
8We omit some statistics which are not of direct interest (for example, max sum similarity in the assignment).

29

Figure 4: Visualization of comparison of the algorithms based on fairness and total sum similarity.
these notions. Figure 4 visualizes the comparison of the algorithms and is constructed as follows. For each of the three experiments, we compute the maximum value of fairness achieved by any of the algorithms. Using this value, for each algorithm we compute its “competetiveness” as the fairness achieved by that algorithm divided by the maximum fairness. We then repeat the same for the total sum similarity. As a result, in each experiment the performance of each algorithm can be represented as a data point in two-dimensional space where x-axis represents the competitiveness in terms of fairness and y-axis represents the competitiveness in terms of the total sum similarity.
Figure 4 demonstrates that in each of the three experiments the PeerReview4All algorithm (even with one iteration) was able to achieve maximum or close-to-maximum values of both fairness and total sum similarity. In contrast, each of the other algorithms under consideration achieved considerably lower value of either fairness or total sum similarity in two out of three experiments.
Overall, we conclude that while being considerably (but not prohibitively) slower than other algorithms, PeerReview4All managed to achieve the best balance of fairness and total sum similarity, despite optimizing the latter objective only heuristically.
8.4 Experiment on Amazon Mechanical Turk
Even if peer-review data from conferences was available to us, it would not allow for an objective evaluation of any assignment algorithm with respect to accuracy of the acceptance procedure. There are two reasons for this hinderance: (a) No ground truth ranking is available; and (b) The data contains only reviews that correspond to one particular assignment and has missing reviews for other assignments.
In this section we present an experiment which we carefully design to overcome the fundamental issues with objective empirical evaluations of reviewer assignments. Our experiment allows us to directly measure the accuracy of ﬁnal decisions to evaluate any assignment. We execute our experiment on the Amazon Mechanical Turk (mturk.com) crowdsourcing platform.
8.4.1 Design of experiment
We designed the experiment in a manner that allows us to objectively evaluate the performance of any assignment algorithm. Speciﬁcally, the experiment should provide us access to some similarities between reviewers and papers, execute any assignment algorithm, and eventually objectively evaluate the ﬁnal outcome.
The experiment considers crowdsourcing workers as reviewers and a number of general knowledge questions as papers. Speciﬁcally, 80 workers were recruited and presented with a list of 60 ﬂags of diﬀerent countries. The workers were asked to determine the country of each ﬂag, choosing one of ﬁve options for each question.
30

Select the country whose ﬂag is shown in the picture.
Kenya Morocco Tunisia Nigeria South Africa
Figure 5: Question interface
The interface of the task is represented in Figure 5. Unknown to the worker, the 60 countries comprised 10 countries each from 6 diﬀerent geographic regions. Three participants did not attempt some of the questions and their responses were discarded from the dataset. The dataset is available on the ﬁrst author’s website.
8.4.2 Evaluation
After obtaining the data from Amazon Mechanical Turk, we executed the following procedure for 1,000 iterations. In each of the 6 regions, we ﬁrst split the 10 questions into two sets: a “gold standard” set of 8 questions chosen uniformly at random and an “unresolved” set comprising the 2 remaining questions. The set of all 12 unresolved questions are analogous to papers in the peer-review setting (m = 12). We computed the similarity of any worker to any paper (question) as the fraction of questions that the worker answered correctly among the 8 gold standard questions for the region corresponding to that paper (question). Having computed the similarities, we selected n = 40 of the workers uniformly at random and created ﬁve assignments ATPMS, APR4A, AILPR, AHARD and ARAND, with identity transformation function f (s) = s, where ARAND is a random feasible assignment. In each of these assignments, every question was answered by λ = 3 workers and every worker answered at most µ = 2 questions. Finally, for each assignment, we computed the answers for the remaining m = 12 questions by taking a majority vote of the responses from workers assigned to each question. Ties are also considered as mistakes.
At the end of all iterations, we computed the fraction of questions whose ﬁnal answers are estimated incorrectly under the ﬁve assignments as well as the mean fairness ΓS (A) and conventional sum of similarities GS (A). We summarize the results in Table 5. We see that all non-trivial algorithms signiﬁcantly outperform random assignment. However, ATPMS incurs about 8% increased error as compared to APR4A.
Similar to Case (C5) of synthetic experiments, the optimally fair assignment AHARD turns out to incur larger fraction of errors as compared to approximations APR4A and AILPR. The reason is that the assignment AHARD maximizes the quality of the assignment with respect to the most “disadvantaged” question, but in contrast to APR4A and AILPR, does not care about the fate of remaining questions.
We also see that APR4A slightly outperforms AILPR in terms of the fraction of errors while having slightly smaller average fairness. One reason for this is that in parallel with ΓS APR4A being close to optimal, PeerReview4All algorithm managed to achieve the high value of conventional sum of similarities, thus maintaining a balance between the fairness ΓS (A) and the global objective GS (A).
We ﬁnd these observations to be of notable interest for the actual conference peer-review scenarios. The task of identifying ﬂags in the experiment involved a rather homogeneous set of similarities (in the sense that each worker either knew many or only few ﬂags) where optimizing (2) or (3) would yield similar results. In contrast, the signiﬁcantly higher heterogeneity in peer-review, the presence of many non-mainstream papers as well as both very strong and very weak reviewers, is expected to further amplify the observed improvements oﬀered by the PeerReview4All algorithm as compared to TPMS and ILPR.
31

Algorithm
ARAND ATPMS AHARD AILPR APR4A

Error fraction
0.394 0.113 0.110 0.108 0.105

Error increase
+275% +8% +5% +3% —

Mean fairness ΓS (A)
6.4 20.8 21.9 21.7 21.6

Mean sum of sim. GS (A)
171.1 274.6 269.8 270.4 272.9

Table 5: Results of the experiment on Amazon Mechanical Turk.

9 Proofs
We now present the proofs of our main results.

9.1 Proof of Theorem 1
We prove the result in three steps. First, we establish a lower bound on the fairness of the PeerReview4All algorithm. Then we establish an upper bound on the fairness of the optimal assignment. Finally, we combine these bounds to obtain the result (7).

Lower bound for the PeerReview4All algorithm. We show a lower bound for the intermediate assignment A at Step 3 during the ﬁrst iteration of Steps 2 to 7. We denote this particular assignment as A1. Note that in Step 4 we ﬁx the assignment for A1’s worst-oﬀ papers into the ﬁnal output, and hence we have ΓSf A1 ≥ ΓSf APf R4A . On the other hand, by keeping track
of A0 (Step 7), we ensure that in all of the subsequent iterations of Steps 2 to 7, the temporary assignment A will be at least as fair as A1, which implies ΓSf A1 = ΓSf APf R4A .
Getting back to the ﬁrst iteration of Steps 2 to 7, we note that when Step 2 is completed, we have λ assignments A1, . . . , Aλ as candidates. Notice that for every κ ∈ [λ], assignment Aκ is constructed with a two-step procedure by joining the outputs A1κ and A2κ of Subroutine 1. Recalling the deﬁnition (6) of s∗κ, we now show that for every value of κ ∈ [λ], the assignment A1κ satisﬁes:
min min sij = s∗κ.
j∈[m] i∈RA1 (j)
κ

Consider any value of κ ∈ [λ]. The deﬁnition of s∗κ ensures that there exist an assignment, say A∗, which assigns κ reviewers to each paper in a way that minimum similarity in this assignment equals s∗κ. Now note that Subroutine 1, called in Step 2b of the algorithm, adds edges to the ﬂow network in order of decreasing similarities. Thus, at the time all edges with similarity higher or equal to s∗κ are added, we have that no edges with similarity smaller that s∗κ are added, and that all edges which correspond to the assignment A∗ are also added to the network. Thus, a maximum ﬂow of size mκ is achieved and hence each assigned (reviewer, paper) pair has similarity at least s∗κ.
Recalling that s∗∞ is the lowest similarity in similarity matrix S, one can deduce that ΓSf (Aκ) ≥ κf (s∗κ) + (λ − κ) f (s∗∞) due to the monotonicity of f . Consequently, we have

ΓSf APf R4A ≥ ΓSf (Aκ) ≥ κf (s∗κ) + (λ − κ) f (s∗∞),

(23)

for all κ ∈ [λ]. Taking a maximum over all values of κ ∈ [λ] concludes the proof.

Upper bound for the optimal assignment AH f ARD. Consider any value of κ ∈ [λ]. By deﬁnition (6) of s∗κ, for any feasible assignment A ∈ A, there exists some paper jκ∗ ∈ [m] for which at most (κ − 1) reviewers have similarity strictly greater than s∗κ. Let us now

32

consider assignment AHf ARD and corresponding paper jκ∗. This paper is assigned to at most (κ − 1) reviewers with similarity greater than s∗κ and to at least (λ − κ + 1) reviewers with similarity smaller or equal to s∗κ. Recalling that s∗0 is the largest possible similarity, we conclude that due to monotonicity of f , the following upper bound holds:

ΓSf AHf ARD = min

f (sij) ≤

f (sij∗ ) ≤ (κ − 1) f (s∗0) + (λ − κ + 1) f (s∗κ).

j∈[m]

κ

i∈RAHARD (j)

i∈RAHARD (jκ∗)

f

f

(24)

Taking a minimum over all values of κ ∈ [λ], then yields an upper bound on the fairness of AHf ARD.

Putting it together. To conclude the argument, it remains to plug in the obtained bounds (23) and (24) into ratio ( ) ΓSf APf R4A :
( ) ΓSf AHf ARD

ΓSf APf R4A ΓSf AHf ARD

max κf (s∗κ) + (λ − κ) f (s∗∞) ≥ κ∈[λ] .
min (κ − 1)f (s∗0) + (λ − κ + 1) f (s∗κ)
κ∈[λ]

Setting κ = 1 in both numerator and denominator and recalling that f (s) ≥ 0 ∀s ∈ [0, 1], we obtain a worst-case approximation in terms of required paper load: ΓΓSS((AAHPARR4AD)) ≥ λ1 .

9.2 Proof of Corollary 1
Let us pause the PeerReview4All algorithm at the beginning of the rth iteration of Steps 2 to 7 and inspect its state.

• The set M consists of papers that are not yet assigned:

r−1

M = [m]\

Jl .

l=1

• The vector of reviewers’ loads µ is adjusted with respect to assigned papers. For every reviewer i ∈ [n], we have:

r−1
µi = µ − card j ∈ Jl i ∈ RAPR4A (j) . f l=1

• The similarity matrix Sr consists of columns of the initial similarity matrix S which correspond to papers in M.

The only thing that connects the algorithm with the previous iterations is the assignment A0, computed in Step 7 of the previous iteration. However, we note that the sum similarity for the worst-oﬀ papers, determined
in Step 4 of the current iteration (in other words, fairness of Ar ), is lower-bounded by the largest fairness of the candidate assignments A1, . . . , Aλ, which are computed in Step 2.
We now repeat the proof of Theorem 1 with the following changes. Instead of the similarity matrix S, we
use the updated matrix Sr; instead of considering all papers m we consider only papers from M; instead of assuming that each reviewer i ∈ [n] can review at most µ papers, we allow reviewer i ∈ [n] to review at most µi papers. Hence, we arrive to the bound (7) on the fairness of Ar, where AHARD should be read as AHARD (M) = AHARD J{r:p} and values s∗κ, κ ∈ {0, . . . , λ} ∪ {∞} are computed for similarity matrix Sr and constraints on reviewers’ loads µ. Thus, we obtain (8) and conclude the proof of the corollary.

33

9.3 Proof of Theorem 2
Before we prove the theorem, let us formulate an auxiliary lemma which will help us show the claimed upper bound. We give the proof of this lemma subsequently in Section 9.3.3.

Lemma 1. Consider any valid assignment A ∈ A and any estimator θ ∈ θMLE, θMEAN . Then for every δ > 0, the error incurred by θ is upper bounded as

sup

P Tk A, θ = Tk∗

(θ1∗,...,θm ∗ )∈Fk(δ)


 ≤ k(m − k) exp −


δ 2σ(A, θ)

2 
,


where


  max σ2(A, θ) = j∈[m]
  max j∈[m]

−1
1 σi2j i∈RA (j )

1 λ2

σi2j

i∈RA (j )

if θ = θMLE if θ = θMEAN.

9.3.1 Proof of upper bound
First, recall from (13) the distribution of θjMEAN, j ∈ [m]. Then the PeerReview4All algorithm called with f = 1 − h simultaneously tries to maximize the fairness of the assignment with respect to f and minimize the maximum variance of the estimated scores θjMEAN, j ∈ [m]. Similarly, the choice of f = h−1 ensures that together with optimizing the corresponding fairness, the algorithm also minimizes the maximum variance of θjMLE, j ∈ [m], deﬁned in (12). Thus, the choice of the estimator deﬁnes the choice of the transformation function f which minimizes the maximum variance of the estimated scores. To maintain brevity, we denote AMEAN = AP1−Rh4A, AMLE = APh−R14A, AMEAN(j) = RAMEAN (j) and AMLE(j) = RAMLE (j).
Let now S ∈ S(q). We begin with the pair of assignment and estimator AMEAN, θMEAN . Notice that
for arbitrary feasible assignment A ∈ A and estimator θMEAN,









σ2(A, θMEAN) = max 1 j∈[m]  λ2

σ2

1 = max

ij 

λ2


j∈[m]

1 − (1 − h(sij))

i∈RA (j )

i∈RA (j )





1 = λ2 λ − jm∈[imn]

(1 − h(sij)) = λ12 λ − ΓS1−h (A) .

i∈RA (j )

Now we can write

sup σ2(AMEAN, θMEAN) = 1

S ∈S (q)

λ2

λ − q inf ΓS1−h (AMEAN)

S ∈S (q)

q

1 ≤ λ2

λ − q inf ΓS1−h (AMEAN) S∈S(q) ΓS1−h AH1−AhRD

= λ − qτq . λ2

34

Using Lemma 1, we conclude the proof for the mean score estimator:



2





sup

P Tk AMEAN, θMEAN = T ∗ ≤ k(m − k) exp − 

δ

 
 (25)

∗,...,θ∗ )∈Fk(δ)

k

  2 sup σ(AMEAN, θMEAN)  

(θ1

m





S ∈S (q)

S ∈S (q)

≤ m2 exp − λ2δ2

≤ m2 exp − ln m2 ≤ . (26)

4 (λ − qτq)

Let us now consider the pair AMLE, θMLE . It suﬃces to show that

sup σ2(AMLE, θMLE) ≤ sup σ2(AMEAN, θMEAN).

S ∈S (q)

S ∈S (q)

(27)

Let us consider S ∈ S(q). Recall from the proof of Theorem 1 that the fairness of the resulting assignment is determined in the ﬁrst iteration of Steps 2 to 7. After completion of Step 2, we have λ candidate assignments A1, . . . , Aλ. Observe that Subroutine 1 in Step 6 uses the same heuristic for both AMEAN and AMLE. Hence, the λ candidate assignments yielded when PeerReview4All constructs AMEAN coincide with the candidate assignments yielded when PeerReview4All constructs AMLE. Depending on the choice of f , in Step 3 the algorithm picks one assignment that maximizes fairness (4) with respect to f . Thus,

ΓS1−h (AMEAN) = max ΓS1−h (Aκ) and ΓSh−1 (AMLE) = max ΓSh−1 (Aκ) .

κ∈[λ]

κ∈[λ]

(28)

Hence, we have



−1





σ2(AMLE, θMLE) = max 
j∈[m]

1 σi2j 

i∈AMLE (j )

1

= max  

 1

j∈[m]

h(sij )

i∈AMLE (j )

1

1

= ΓSh−1 (AMLE) ≤ ΓSh−1 (AMEAN) .

where the last ineqaulity is due to (28). Recalling the deﬁnition of the fairness (4) and using Jensen’s inequality, we continue:





λ −

(1 − h(sij)) 

σ2(AMLE, θMLE) ≤ jm∈[amx]  λ12 h(sij ) = jm∈[amx]  i∈AMEANλ(j2) 

i∈AMEAN (j )

λ − ΓS1−h (AMEAN)

2

MEAN

=

λ2

= σ (AMEAN, θ

).

Taking a supremum over all S ∈ S(q), we obtain (27) which together with Lemma 1 and the ﬁrst part of the statement concludes the proof.

9.3.2 Proof of lower bound

Proof of our lower bound is based on Fano’s inequality (Cover and Thomas, 2005) which provides a lower

bound for probability of error in L-ary hypothesis testing problems.

Without loss of generality we assume that k ≤ 12 m. Otherwise, the result will hold by symmetry of the

problems.

We ﬁrst claim that there exists a value s ∈ [0, 1] such that h(s) = 1 − q . Indeed, by assumptions of the

theorem, h is continuous strictly monotonically decreasing function and qλ≥ 1 − h(0). Thus, h(0) ≥ 1 − q .

On

the

other

hand,

if

h(1)

>

1

−

q,

then

for

every

similarity

matrix

S

λ
we have

λ

λ

ΓS1−h (A) ≤ λ (1 − h(1)) < q.

35

The last inequality contradicts with the deﬁnition (16) of S(q), verifying that q
h(0) ≥ 1 − ≥ h(1). λ

Given that h is continuous strictly monotonically decreasing function, we conclude that these exists s =

h−1 1 − λq ∈ [0, 1]. Consider the similarity matrix S =

h−1 1 − λq

n×m. Observe that S ∈ S(q), since every feasible

assignment A ∈ A has fairness

ΓS1−h (A) = min
j∈[m]

(1 − h(sij)) = min
j∈[m]

1−h

h−1

q 1−

λ

i∈RA (j )

i∈RA (j )

= q.

Thus, in any feasible assignment each paper j ∈ [m] receives λ reviewers with similarity exactly h−1 1 − λq . To apply Fano’s inequality, we need to reduce our problem to a hypothesis testing problem. To do so, let
us introduce the set P of (m − k + 1) instances of the paper accepting/rejecting problem: every problem
instance in this set has the same similarity matrix S, but diﬀers in the set of top k papers Tk∗. We now consider the problem of distinguishing between these problem instances, which is equivalent to the problem
of correctly recovering the top k papers. More concretely, we denote the (m − k + 1) problem instances as, P = {1, 2, . . . , m − k + 1}, where for any problem ∈ P the set of top k papers is denoted as Tk∗( ) and set as {1, 2, . . . , k − 1} ∪ {k − 1 + }. The true quality of any paper j ∈ [m] in any problem instance ∈ P is

θ∗( ) = δ if j ∈ Tk∗( )

j

0 otherwise,

thereby

ensuring

that

(θ1∗(

),

.

.

.

,

θ

∗ m

(

))

∈

Fk (δ ),

for

every

instance

∈ P.

Let P denote a random variable which is uniformly distributed over elements of P. Then given P = , we

denote a random matrix of reviewers’ scores as Y ( ) ∈ Rλ×m whose (r, j)th entry is a score given by reviewer

ir, r ∈ [λ], assigned to paper j and

Y ( ) ∼ N δ, 1 − λq if j ∈ Tk∗( )

(29)

rj N 0, 1 − λq otherwise.

We denote the distribution of random matrix Y ( ) as P( ). Note that Y ( ) does not depend on the selected
assignment A ∈ A. Indeed, recall from (11), that assignment A aﬀects only variances of observed scores. On the other hand, for any reviewer i ∈ [n] and for any paper j ∈ [m], the score yij has variance 1 − λq . Thus, for any feasible assignment A and any ∈ P, the distribution of random matrix Y has the form (29).
Now let us consider the problem of determining the index P = ∈ P, given the observation Y ( ) following the distribution P( ). Fano’s inequality provides a lower bound for probability of error of every estimator ϕ : Rλ×m → P in terms of Kullback-Leibler divergence between distributions P( 1) and P( 2) ( 1 = 2, 1, 2 ∈ [m − k + 1]):

max KL P( 1)||P( 2) + log 2

P {ϕ(Y ) = P } ≥ 1 − 1= 2∈P

,

(30)

log (card(P))

where card(P) denotes the cardinality of P and equals (m − k + 1) for our construction. Let us now derive an upper bound on the quantity

max KL P( 1)||P( 2) .
1= 2∈P

(31)

First, note that for each ∈ [m − κ + 1], entries of Y ( ) are independent. Second, for arbitrary 1 = 2, the distributions of Y ( 1) and Y ( 2) diﬀer only in two columns. Thus,

KL P( 1)||P( 2)

=λ

KL N

q δ, 1 −

||N

q 0, 1 −

q

q

+ KL N 0, 1 − ||N δ, 1 −

.

λ

λ

λ

λ

36

Some simple algebraic manipulations yield:

q

q

q

q

δ2

KL N δ, 1 − ||N 0, 1 −

λ

λ

= KL N 0, 1 − ||N δ, 1 −

λ

λ

= 2

1− q

.

(32)

λ

Finally, substituting (32) in (30), for m > 6 and for a suﬃciently small constant c, we have

λλ2−δq2 + log 2

c2 ln m + 1 1

P {ϕ(Y ) = P } ≥ 1 − log (m − k + 1) ≥ 1 − log

m +1

≥. 2

2

This lower bound implies

sup inf

sup

P

S∈S(q) (θ,A∈A) (θ1∗,...,θm ∗ )∈Fk(δ)

Tk A, θ

= Tk∗

1 ≥.
2

9.3.3 Proof of Lemma 1 First, let θ = θMEAN. Then given a valid assignment A, the estimates θjMEAN, j ∈ [m], are distributed as





θjMEAN ∼ N θj∗, λ12

σi2j  = N

θ

∗ j

,

σ¯j2

,

i∈RA (j )

where

we

have

deﬁned

σ¯j2

=

1 λ2

σi2j. Now let us consider two papers j1, j2 such that j1 belongs to the

i∈RA (j )

top k papers Tk∗ and j2 ∈/ Tk∗. The probability that paper j2 receives higher score than paper j1 is upper

bounded as

P θjM1EAN ≤ θjM2EAN = P θjM1EAN − θjM2EAN − E θjM1EAN − θjM2EAN ≤ −E θjM1EAN − θjM2EAN



(i)  E θjM1EAN − θjM2EAN

≤ exp −

2 σ¯j2 + σ¯j2

 

1

2

2   2

 

(ii)



δ



≤ exp −

,



 2σ(A, θMEAN) 



where inequality (i) is due to Hoeﬀding’s inequality, and inequality (ii) holds because E θjM1EAN − θjM2EAN =
θj∗1 − θj∗2 ≥ δ and σ2(A, θMEAN) = jm∈[amx] σ¯j2. The estimator makes a mistake if and only if at least one paper from Tk∗ receives lower score than at least one paper from [m]\Tk∗. A union bound across every paper from Tk∗, paired with (m − k) papers from [m]\Tk∗, yields our claimed result.
Let us now consider θ = θMLE. Then it is not hard to see that

 

−1

θjMEAN ∼ N θj∗, 

1 σi2j 

i∈RA (j )

 = N θj∗, σ¯j2 ,

where we denoted σ¯j2 =

1 σi2j i∈RA (j )

estimator yields the claimed result.

−1
. Proceeding in a manner similar to the proof for the averaging

9.4 Proof of Corollary 2
The proof of Corollary 2 follows along similar lines as the proof of Theorem 2.

37

9.4.1 Proof of upper bound

Let us consider some κ ∈ [λ] and S ∈ Sκ(v). We apply Lemma 1 to proof the upper bound and in order to do so, we need to derive an upper bound on σ(APh−R14A, θMLE).



−1 

−1

σ2(APh−R14A, θMLE) = jm∈[amx] 

1 σi2j 

i∈RAPR4A (j)

h−1

=  min

h−1(sij )

j∈[m]



i∈RAPR4A (j)

h−1

1

h(v)h(0)

≤

κ

+

λ−κ

=

. κh(0) + (λ − κ)h(v)

h(v) h(0)

Thus,

sup σ2(APh−R14A, θMLE) ≤ κh(0)h+(v()λh−(0)κ)h(v) . (33)
S ∈Sκ (v )

It remains to apply Lemma 1 to complete our proof, and we do so by applying the chain of arguments (25) and (26) to the bound (33), where the pair (AP1−Rh4A, θMEAN) in (25) and (26) is substituted with the pair (APh−R14A, θMLE).

9.4.2 Proof of lower bound
To prove the lower bound, we use the Fano’s ineqaulity in the same way as we did when proved Theorem 2(b). However, we now need to be more careful with construction of working similarity matrix S ∈ Sκ(v).
As in the proof of Theorem 2(b), we assume k ≤ m2 . If the converse holds, than the result holds by symmetry of the problem. Next, consider arbitrary feasible assignment A ∈ Aκ. Recall, that Aκ consists of assignments which assign each paper j ∈ [m] to κ instead of λ reviewers such that each reviewer reviews at most µ papers.
Now we deﬁne a similarity matrix S as follows:

sij = v if i ∈ RA(j)

(34)

0 otherwise.

Thus, for each paper j ∈ [m] there exist exactly κ reviewers with non-zero similarity v and in every feasible assignment A ∈ A each paper j ∈ [m] is assigned to at most κ reviewers with non-zero similarity. Note that
S ∈ Sκ(v). Now let us consider the set of (m − k + 1) problem instances P deﬁned in Section 9.3.2. For every feasible
assignment A ∈ A, if Y (A, ) is a matrix of observed reviewers’ scores for instance ∈ P, then (r, j)th entry of Y (A, ) follows the distribution

Y (A, ) = N δ × I {j ∈ Tk∗( )} , h(v) if Airj = 1

(35)

rj

N δ × I {j ∈ Tk∗( )} , h(0) if Airj = 0,

where ir, r ∈ [λ] is reviewer assigned to paper j in assignment A. We denote the distribution of random matrix Y (A, ) as P(A, ). Note that in contrast to the proof of
Theorem 2, here Y (A, ) does depend on the selected assignment A ∈ A. Thus, instead of (31), we need to
derive an upper bound on the quantity

sup max KL P(A, 1)||P(A, 2) .
A∈A 1= 2∈P

38

First, note that for each ∈ [m − k + 1] and for each feasible assignment A ∈ A, the entries of Y (A, ) are independent. Second, for arbitrary 1 = 2, the distributions of Y (A, 1) and Y (A, 2) diﬀer only in two columns. Thus, for any feasible assignment A ∈ A, we have

KL P(A, 1)||P(A, 2) ≤ γ 1 KL N δ, h(v) ||N 0, h(v) + (λ − γ 1 ) KL N δ, h(0) ||N 0, h(0)

+ γ 2 KL N 0, h(v) ||N δ, h(v) + (λ − γ 2 ) KL N 0, h(0) ||N δ, h(0) (36)

δ2

δ2

= (γ 1 + γ 2 ) 2h(v) + (2λ − γ 1 − γ 2 ) 2h(0) , (37)

where γ 1 is the number of reviewers with similarity v assigned to paper (k − 1 + 1) in A and γ 2 is the

number of reviewers with similarity v assigned to paper (k − 1 + 2). By construction of similarity matrix S,
for each ∈ [m − k + 1] and for each A ∈ A, we have γ ≤ κ. Note that two summands in (37) are proportional to a convex combination of 2hδ(2v) and 2hδ(20) . Moreover, by monotonicity of h, we have 2hδ(2v) ≥ 2hδ(20) , and hence

sup max KL P(A, 1)||P(A, 2) ≤ κδ2 + (λ − κ) δ2 = δ2 κh(0) + (λ − κ) h(v) .

A∈A 1= 2∈P

h(v)

h(0)

h(v)h(0)

Applying Fano’s ineqaulity (30), we conclude that for all feasible assignments A ∈ A, if m > 6 and universal constant c is suﬃciently small, then

δ2 P {ϕ(Y ) = P } ≥ 1 −

κh(0h)+(v()λh−(0κ))h(v) + log 2

c2 ln m + 1 1

log (m − k + 1)

≥1− log

m +1

≥. 2

2

This bound thus implies

sup inf

sup

P

S∈Sκ(v) (θ,A∈A) (θ1∗,...,θm ∗ )∈Fk(δ)

Tk A, θ

= Tk∗

1 ≥.
2

9.5 Proof of Theorem 3

Before we prove the theorem, we state an auxiliary proposition which will help us to prove a lower bound.

Lemma 2 (Shah and Wainwright, 2015). Let t > 0 be an integer such that 2t ≤ 1+1ν2 min m1−ν1 , k, m − k for some constants ν1, ν2 ∈ (0; 1) and m is larger than some (ν1, ν2)-dependent constant. Then there exist a

set of binary strings

b1, b2, . . . , bL

⊆ {0, 1}m/2 with cardinality L > exp

9 10

ν1

ν2

t

log

m

such that

DH b 1 , 0m/2 = 2(1 + ν2)t and DH b 1 , b 2 > 4t ∀ 1 = 2 ∈ [L]

The proof of Lemma 2 relies on a coding-theoretic result due to Levenshtein (1971) which gives a lower bound on the number of codewords of ﬁxed length m and Hamming weights c1 with Hamming distance between each pair of codewords higher than c2.

9.5.1 Proof of upper bound

Without loss of generality we assume that the true underlying ranking of the papers is 1, 2, . . . , k, . . . , m. We

prove the claim for pair AP1−Rh4A, θMEAN below, and proof for APh−R14A, θMLE follows from the proof of the corresponding part of Theorem 2(a).
From the proof of Lemma 1 and Section 9.3.1, we know that under conditions of the theorem, for every paper j1 ≤ k − t and for every paper j2 ≥ k + t + 1,



2

sup P
S ∈S (q)

θjM1EAN − θjM2EAN ≤ 0

 

δ

 

≤ exp − 



  2 sup σ(AP1−Rh4A, θMEAN)  

S ∈S (q)

(38)

39

where

sup σ2(APR4A, θMEAN) ≤ λ − τqq .

S ∈S (q)

1−h

λ2

(39)

Taking a union bound across every paper from the top (k − t) papers, paired with the bottom (m − k − t) papers, we obtain

MEAN

MEAN

2

λ2δ2

sup P ∃j1 ≤ k − t, j2 ≥ k + t + 1 such that θj1
S ∈S (q)

≤ θj2

≤ m exp −

≤.

4(λ − τqq)

In other words, for every similarity matrix S ∈ S(q), with probability at least (1 − ), the top (k − t) papers
will receive higher score than bottom (m − k − t) papers. Thus, among accepted papers Tk AP1−Rh4A, θMEAN , at most t papers will not belong to Tk∗, thereby ensuring that

DH

Tk

AP1−Rh4A, θMEAN

,

T

∗ k

≤ 2t

with probability at least 1 − .

9.5.2 Proof of lower bound

To prove the lower bound, we follow similar path as we used when we derived a lower bound in Theorem 2.

However, we now need more advanced technique to construct necessary set of instances.
As in the proof of Theorem 2(b), we assume that k ≤ m2 . If the converse holds, than the result holds by the symmetry of the problem. Next, consider similarity matrix S = h−1 1 − λq n×m ∈ S(q). To apply Fano’s inequality, it remains to construct a set P = {1, 2, . . . , L} of suitable instances of paper accepting/rejecting

problem: every problem instance in this set has the same similarity matrix S, but diﬀers in the set of top k

papers Tk∗. We note that in contrast to the proof of Theorem 2(b), it is not enough to create (m − k + 1)

instances where the sets of top k papers diﬀer only in a single paper. As we will see below, it suﬃces to

construct instances such that for every

1,

2

∈

P,

the

sets

of

top

k

papers

satisfy

DH

(Tk∗(

1

),

T

∗ k

(

2))

>

4t.

Note that requirements of Lemma 2 are satisﬁed by the conditions of Theorem 3. Let b1, b2, . . . , bL be

the corresponding binary strings. For every problem ∈ P, consider the following binary string:

m/2

b = 1, 1, . . . , 1, 0, 0, . . . , 0, b1, b2, . . . , bm/2.

(40)

k−2(1+ν2 )t

First, note that 2t ≤ 1+1ν2 k, and hence k − 2(1 + ν2)t ≥ 0, thereby ensuring that the construction (40) is not vacuous. Now let Tk∗( ) be the set of indices such that their corresponding elements in string b equal 1. By construction, the cardinality of Tk∗( ) is k so it is a valid set of top k papers. Finally, we need to set the scores of papers. Let for every paper j ∈ [m]:

θ∗( ) = δ if bj = 1

j

0 if bj = 0,

which ensures that for every

∈

P,

(θ1∗(

), θ2∗(

),

.

.

.

,

θ

∗ m

(

))

∈

Fk

⊂

Fk,t.

The strategy for the remaining part of the proof is the following. We ﬁrst show that the problem instances

deﬁned above are well-separated in a sense that for any two of them, the corresponding sets of the top k

papers diﬀer in suﬃciently many elements. We then assume that there exists an (assignment algorithm,

estimator) pair which for every similarity matrix S ∈ S(q) recovers the set of top k papers with at most t

errors with high probability. Then this pair must be able to determine with high probability the problem

40

instance , sampled uniformly at random from P, by observing corresponding reviewers’ scores. We then apply Fano’s inequality to show the impossibility of the last implication.
Following the plan described above, we note that for every two distinct instances 1, 2 ∈ P, we have

DH

(Tk∗(

1

),

T

∗ k

(

2))

>

4t.

Consequently,

for

every

set

Tk∗

of

k

papers,

DH

(T

∗ k

,

T

∗ k

(

))

≤

2t

for

at

most

one

instance

∈ P. Now assume

for the sake of contradiction that for every similarity matrix S ∈ S(q), there exists an assignment A = A (S)

and estimator θ = θ (S) such that for arbitrarily large value of m

sup

P

DH

Tk

A, θ

,T ∗

> 2t

1 <.

(θ∗,...,θ∗ )∈F (δ)

k

2

1

m

k

(41)

This assumption implies that estimator θ(S) might be used to determine the problem P = sampled uniformly

at random from P correctly with probability greater than 1/2. Indeed, notice that similarity matrix S was

constructed in a way that Tk A, θ does not depend on assignment A.

Given P = , let Y ( ) be the random matrix of reviewers’ scores. The distribution P( ) of components of Y ( ) is deﬁned in (29). To apply Fano’s inequality (30), it remains to derive an upper bound on the quantity
max KL P( 1)||P( 2) .
1= 2∈P
First, note that entries of Y ( ) are independent. Second, note that for every pair 1 = 2 ∈ P and for every j ∈ [m/2], the distribution of the jth column of Y (A, 1) is identical to the distribution of the jth column
of Y (A, 2). Among the last m/2 columns, the distributions of at most 4(1 + ν2)t columns of Y (A, 1) diﬀer from the distributions of the corresponding columns in Y (A, 2). Thus, for arbitrary 1 = 2 ∈ P

KL P( 1)||P( 2)

≤ 2(1 + ν2)tλ

KL N

q δ, 1 −

||N

q 0, 1 −

q

q

+ KL N 0, 1 − ||N δ, 1 −

.

λ

λ

λ

λ

Recalling (32), we deduce that

max KL P( 1)||P( 2) ≤ 4(1 + ν2)tλ λδ2 = 2(1 + ν2)t λ2δ2 ≤ 4c2ν1ν2t ln m.

1= 2∈P

2(λ − q)

λ−q

Finally, Fano’s inequality together with Lemma 2 ensures that for every estimator ϕ : Y → P

4c2ν1ν2t ln m + log 2

40 2 ln m

1

1

P {ϕ(Y ) = P } ≥ 1 −

9 ν ν t log m

≥1−

c 9

− log m

9 ν ν t log m ≥ 2

10 1 2

10 1 2

for m larger than some (ν1, ν2)-dependent constant and small enough universal constant c. This leads to a contradiction with (41), thus proving the theorem.

9.6 Proof of Corollary 3
The proof of the Corollary 3 is based on the ideas of the proofs of Theorem 3 and Corollary 2 and repeats them with minor changes.

9.6.1 Proof of upper bound

To show the required upper bound, we repeat the proof of Theorem 3(a) from Section 9.5.1 with the following changes. Equation (38) should be substituted with:

sup P
S ∈Sκ (v )

θjM1LE − θjM2LE ≤ 0



2

 

δ

 

≤ exp − 

.

 

2

sup

σ(APh−R14A, θMLE) 

 

S ∈Sκ (v )

41

Equation (39) should be substituted with:

sup σ2(APR4A, θMLE) ≤

h(v)h(0) .

S ∈Sκ (v )

κh(0) + (λ − κ)h(v)

In the remaining part of the proof, pair (AP1−Rh4A, θMEAN) should be substituted with the pair (APh−R14A, θMLE).

9.6.2 Proof of lower bound
To prove the lower bound, we use the set of problems P constructed in Section 9.5.2 and the similarity matrix
S as deﬁned in (34). Given P = and any feasible assignment A ∈ A, let Y (A, ) be the random matrix of reviewers’ scores.
The distribution P(A, ) of components of Y (A, ) is deﬁned in (35). Since the distribution of reviewers’ scores
now depends on the assignment, to apply Fano’s inequality (30), we need to derive an upper bound on the quantity sup max KL P(A, 1)||P(A, 2) .
A∈A 1= 2∈P
First, note that entries of Y (A, ) are mutually independent. Second, note that for every pair 1 = 2 ∈ P and for every j ∈ [m/2], the distribution of the jth column of Y (A, 1) is identical to the distribution of the jth column of Y (A, 2). Among the last m/2 columns, the distributions of at most 4(1 + ν2)t columns of Y (A, 1) diﬀer from the distributions of the corresponding columns in Y (A, 2). Next, consider arbitrary feasible assignment A ∈ A. Let γ(r), r ∈ [2(1 + ν2)t], denote the number of strong reviewers (with similarity
1
v) assigned in A to paper j1(r) ∈ Tk∗( 1), where paper j1(r) corresponds to the the second part of the string
b 1 deﬁned in (40). Recall now that there are at most 4(1 + ν2)t papers that belong to exactly one of the sets Tk∗( 1) and Tk∗( 2). Hence, the equation for upper bound of the Kullback-Leibler divergence between P(A, 1) and P(A, 2) is obtained by assuming that all the papers that belong to the Tk∗( 1) and correspond to the second half of the string b do not belong to Tk∗( 2) and vice versa. Thus, similar to (36)-(37), for arbitrary 1 = 2 ∈ P and for arbitrary feasible assignment A ∈ A, we have

KL P(A, 1)||P(A, 2)

2(1+ν2 )t
≤
r=1

γ(r)KL N δ, h(v) ||N 0, h(v) 1

+ λ − γ(r) KL N δ, h(0) ||N 0, h(0) 1

2(1+ν2 )t
+
r=1

γ(r)KL N 0, h(v) ||N δ, h(v) 2

+ λ − γ(r) KL N 0, h(0) ||N δ, h(0) 2


2(1+ν2 )t
=
r=1

γ(r) + γ(r)

1

2





δ2

2(1+ν2 )t

 2h(v) + 4(1 + ν2)tλ −

r=1

γ(r) + γ(r)

1

2



δ2



.

2h(0)

Noting that 2hδ(2v) ≥ 2hδ(20) , we obtain

sup max KL
A∈A 1= 2∈P

P(A, 1)||P(A, 2)

κδ2 (λ − κ) δ2 ≤ 2(1 + ν2)t h(v) + h(0) = 2(1 + ν2)tδ2 κh(0) + (λ − κ) h(v)
h(v)h(0) ≤ 4c2ν1ν2t ln m.

Applying Fano’s inequality (30), we obtain the desired lower bound.

9.7 Proof of Theorem 4
Note that Theorem 4 is similar in nature with Theorem 2, the only diﬀerence is that now we are trying to recover a ranking which is induced by the assignment.

42

9.7.1 Proof of upper bound

Given any feasible assignment A, the “ground truth” ranking that we try to recover is given by

1

θj (A) = λ

θij .

i∈RA (j )

(42)

Then the estimates θjMEAN, j ∈ [m], are distributed as





θjMEAN ∼ N  λ1

1 θij , λ2

σi2j  = N

θ

j

(

A

),

σ¯

2 j

,

i∈RA (j )

i∈RA (j )

(43)

where

σ¯j2

=

1 λ2

σi2j. Now observe that Lemma 1, with Tk A, θ (A) substituted for Tk∗, also holds

i∈RA (j )

for the subjective score model and the averaging estimator θMEAN. Thus, repeating the proof of the upper

bound for averaging estimator in Theorem 2(a) and substituting Tk∗ with Tk APR4A, θ (APR4A) in (25),

yields the claimed result.

9.7.2 Proof of lower bound

The lower bound directly follows from Theorem 2(b). To see this, consider the following matrix of reviewers’

subjective scores: Θ = θij

, where θij = θj∗. Under this assumption, the total ranking induced by

i∈[n],j∈[m]

assignment A does not depend on the assignment: θj (A) = θj∗. Now we can conclude that such choice of Θ brings us to the objective model setup in which true underlying ranking exists and does not depend on the

assignment. Thus, the lower bound of Theorem 2(b) transfers to the subjective score model.

9.8 Proof of Theorem 5
The proof of the Theorem 5 is based on the ideas of the proofs of Theorem 3 and Theorem 4 and repeats them with minor changes.

9.8.1 Proof of upper bound
Having equations (42) and (43), we note that the goal now mimics the goal we achieved when proved an upper bound for averaging estimator in Theorem 3.

9.8.2 Proof of lower bound
The argument from Section 9.7.2 ensures that the lower bound established in Theorem 3 directly transfers to the to the subjective score model.

10 Discussion
Researchers submit papers to conferences expecting a fair outcome from the peer-review process. This expectation is often not met, as is illustrated by the diﬃculties that non-mainstream or inter-disciplinary research faces in present peer-review systems. We design a reviewer-assignment algorithm PeerReview4All to address the crucial issues of fairness and accuracy. Our guarantees impart promise for deploying the algorithm in conference peer-reviews.
There are number of open problems suggested by our work. The ﬁrst direction is associated with approximation algorithms and corresponding guarantees established in this work. One goal is to determine

43

whether there exists a polynomial-time algorithm with worst case approximation guarantees better than 1/λ established in this paper (7b). It would also be useful to obtain a deeper understanding of the adaptive behavior of our algorithm with bounds more nuanced than (7a). Finally, we leave the task of improving the computational eﬃciency of our PeerReview4All algorithm out of the scope of this work. However, we suggest that optimal implementation of Subroutine 1 should not be based on the general max-ﬂow algorithm and instead should rely on algorithms speciﬁcally designed to work fast on layered graphs.
The second direction is related to the statistical part of our work. In this paper we provide a minimax characterization of the simpliﬁed version of the paper acceptance problem. This simpliﬁed procedure may be considered as an initial estimate that can be used as a guideline for the ﬁnal decisions. However, there remain a number of other factors, such as self-reported conﬁdence of reviewers or inter-reviewer discussions, that may additionally be included in the model.
Finally, an important related problem is to improve the assessment of similarities between reviewers and papers. It will be interesting to see whether the problems of assessing similarities and assigning reviewers can be addressed jointly in an active manner possibly incorporating feedback from the previous iterations of the conference
Acknowledgments
This work was supported in parts by NSF grants CRII: CIF: 1755656, CIF: 1563918, and CIF: 1763734.
References
Asadpour, A. and Saberi, A. (2010). An approximation algorithm for max-min fair allocation of indivisible goods. SIAM Journal on Computing, 39(7):2970–2989.
Bakanic, V., McPhail, C., and Simon, R. J. (1987). The manuscript review and decision-making process. American Sociological Review, pages 631–642.
Balietti, S., Goldstone, R. L., and Helbing, D. (2016). Peer review and competition in the art exhibition game. Proceedings of the National Academy of Sciences, 113(30):8414–8419.
Bansal, N. and Sviridenko, M. (2006). The Santa Claus problem. In Proceedings of the Thirty-eighth Annual ACM Symposium on Theory of Computing, STOC ’06, pages 31–40, New York, NY, USA. ACM.
Benferhat, S. and Lang, J. (2001). Conference paper assignment. International Journal of Intelligent Systems, 16(10):1183–1192.
Bianchi, F. and Squazzoni, F. (2015). Is three better than one? Simulating the eﬀect of reviewer selection and behavior on the quality and eﬃciency of peer review. In Proceedings of the 2015 Winter Simulation Conference, pages 4081–4089. IEEE Press.
Black, N., Van Rooyen, S., Godlee, F., Smith, R., and Evans, S. (1998). What makes a good reviewer and a good review for a general medical journal? Jama, 280(3):231–233.
Bonald, T., Massouli´e, L., Proutiere, A., and Virtamo, J. (2006). A queueing analysis of max-min fairness, proportional fairness and balanced fairness. Queueing systems, 53(1-2):65–84.
Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated paper-reviewer assignment system. In ICML Workshop on Peer Reviewing and Publishing Models.
Charlin, L., Zemel, R. S., and Boutilier, C. (2012). A framework for optimizing paper matching. CoRR, abs/1202.3706.
Cole, S., Simon, G. A., et al. (1981). Chance and consensus in peer review. Science, 214(4523):881–886.
44

Cover, T. M. and Thomas, J. A. (2005). Entropy, Relative Entropy, and Mutual Information, pages 13–55. John Wiley & Sons, Inc.
Dai, W., Jin, G. Z., Lee, J., and Luca, M. (2012). Optimal aggregation of consumer ratings: An application to yelp.com. Working Paper 18567, National Bureau of Economic Research.
Ernst, E. and Resch, K.-L. (1994). Reviewer bias: a blinded experimental study. The Journal of laboratory and clinical medicine, 124(2):178–182.
Fiez, T., Shah, N., and Ratliﬀ, L. (2019). A SUPER* algorithm to optimize paper bidding in peer review. In ICML workshop on Real-world Sequential Decision Making: Reinforcement Learning And Beyond.
Flach, P. A., Spiegler, S., Gol´enia, B., Price, S., Guiver, J., Herbrich, R., Graepel, T., and Zaki, M. J. (2010). Novel tools to streamline the conference review process: Experiences from SIGKDD’09. SIGKDD Explor. Newsl., 11(2):63–67.
Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal matter? insights from a major nlp conference. arXiv preprint arXiv:1903.11367.
Garﬁnkel, R. S. (1971). Technical note. An improved algorithm for the bottleneck assignment problem. Operations Research, 19(7):1747–1751.
Garg, N., Kavitha, T., Kumar, A., Mehlhorn, K., and Mestre, J. (2010). Assigning papers to referees. Algorithmica, 58(1):119–136.
Ge, H., Welling, M., and Ghahramani, Z. (2013). A Bayesian model for calibrating conference review scores. http://mlg.eng.cam.ac.uk/hong/unpublished/nips-review-model.pdf [Online; accessed 13Nov-2019].
Goldsmith, J. and Sloan, R. (2007). The AI conference paper assignment problem. WS-07-10:53–57.
Hahne, E. L. (1991). Round-robin scheduling for max-min fairness in data networks. IEEE Journal on Selected Areas in communications, 9(7):1024–1039.
Hartvigsen, D., Wei, J. C., and Czuchlewski, R. (1999). The conference paper-reviewer assignment problem. Decision Sciences, 30(3):865–876.
Karimzadehgan, M., Zhai, C., and Belford, G. (2008). Multi-aspect expertise matching for review assignment. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 1113–1122, New York, NY, USA. ACM.
Kerr, S., Tolliver, J., and Petree, D. (1977). Manuscript characteristics which inﬂuence acceptance for management and social science journals. Academy of Management Journal, 20(1):132–141.
King, V., Rao, S., and Tarjan, R. (1992). A faster deterministic maximum ﬂow algorithm. In Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’92, pages 157–164, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.
Kobren, A., Saha, B., and McCallum, A. (2019). Paper matching with local fairness constraints. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, pages 1247–1257, New York, NY, USA. ACM.
Lamont, M. (2009). How professors think. Harvard University Press.
Langford, J. (2012). ICML acceptance statistics. http://hunch.net/?p=2517 (visited on 05/15/2018).
Lavi, R., Mu’Alem, A., and Nisan, N. (2003). Towards a characterization of truthful combinatorial auctions. In Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 574–583. IEEE.
45

Lawrence, N. and Cortes, C. (2014). The NIPS Experiment. http://inverseprobability.com/2014/12/ 16/the-nips-experiment. [Online; accessed 3-June-2017].
Lenstra, J. K., Shmoys, D. B., and Tardos, E´ . (1990). Approximation algorithms for scheduling unrelated parallel machines. Mathematical Programming, 46(1):259–271.
Levenshtein, V. I. (1971). Upper-bound estimates for ﬁxed-weight codes. Problemy Peredachi Informatsii, 7(4):3–12.
Li, L., Wang, Y., Liu, G., Wang, M., and Wu, X. (2015). Context-aware reviewer assignment for trust enhanced peer review. PLOS ONE, 10(6):1–28.
Lin, H. and Bilmes, J. (2011). A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 510–520, Stroudsburg, PA, USA. Association for Computational Linguistics.
Liu, X., Suel, T., and Memon, N. (2014). A robust model for paper reviewer assignment. In Proceedings of the 8th ACM Conference on Recommender Systems, RecSys ’14, pages 25–32, New York, NY, USA. ACM.
Long, C., Wong, R., Peng, Y., and Ye, L. (2013). On good and fair paper-reviewer assignment. In Proceedings - IEEE International Conference on Data Mining, ICDM, pages 1145–1150.
Mahoney, M. J. (1977). Publication prejudices: An experimental study of conﬁrmatory bias in the peer review system. Cognitive therapy and research, 1(2):161–175.
McGlohon, M., Glance, N., and Reiter, Z. (2010). Star quality: Aggregating reviews to rank products and merchants. In Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM).
Merton, R. K. (1968). The Matthew eﬀect in science. Science, 159:56–63.
Mimno, D. and McCallum, A. (2007). Expertise modeling for matching papers with reviewers. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 500–509, New York, NY, USA. ACM.
Noothigattu, R., Shah, N., and Procaccia, A. (2018). Choosing how to choose papers. arXiv preprint arxiv:1808.09057.
Orlin, J. B. (2013). Max ﬂows in O(nm) time, or better. In Proceedings of the Forty-ﬁfth Annual ACM Symposium on Theory of Computing, STOC ’13, pages 765–774, New York, NY, USA. ACM.
Porter, A. L. and Rossini, F. A. (1985). Peer review of interdisciplinary research proposals. Science, technology, & human values, 10(3):33–38.
Rawls, J. (1971). A theory of justice: Revised edition. Harvard university press.
Rodriguez, M. A. and Bollen, J. (2008). An algorithm to determine peer-reviewers. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 319–328, New York, NY, USA. ACM.
Rodriguez, M. A., Bollen, J., and Van de Sompel, H. (2007). Mapping the bid behavior of conference referees. Journal of Informetrics, 1(1):68–82.
Roos, M., Rothe, J., and Scheuermann, B. (2011). How to calibrate the scores of biased reviewers by quadratic programming. In AAAI Conference on Artiﬁcial Intelligence.
Sajjadi, M. S., Alamgir, M., and von Luxburg, U. (2016). Peer grading in a course on algorithms and data structures: Machine learning algorithms do not improve over simple baselines. In Proceedings of the Third (2016) ACM Conference on Learning @ Scale, L@S ’16, pages 369–378, New York, NY, USA. ACM.
46

Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and von Luxburg, U. (2017). Design and analysis of the NIPS 2016 review process. arXiv preprint arXiv:1708.09794.
Shah, N. B. and Wainwright, M. J. (2015). Simple, robust and optimal ranking from pairwise comparisons. CoRR, abs/1512.08949.
Squazzoni, F. and Gandelli, C. (2012). Saint Matthew strikes again: An agent-based model of peer review and the scientiﬁc community structure. Journal of Informetrics, 6(2):265–275.
Stelmakh, I., Shah, N., and Singh, A. (2019a). On testing for biases in peer review. In NeurIPS. Stelmakh, I., Shah, N. B., and Singh, A. (2019b). Peerreview4all: Fair and accurate reviewer assignment in
peer review. In Garivier, A. and Kale, S., editors, Proceedings of the 30th International Conference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning Research, pages 828–856, Chicago, Illinois. PMLR. Tang, W., Tang, J., and Tan, C. (2010). Expertise matching via constraint-based optimization. In Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01, WI-IAT ’10, pages 34–41, Washington, DC, USA. IEEE Computer Society. Taylor, C. J. (2008). On the optimal assignment of conference papers to reviewers. Technical report, Department of Computer and Information Science, University of Pennsylvania. Thorngate, W. and Chowdhury, W. (2014). By the numbers: Track record, ﬂawed reviews, journal space, and the fate of talented authors. In Advances in Social Simulation, pages 177–188. Springer. Thurner, S. and Hanel, R. (2011). Peer-review in a world with rational scientists: Toward selection of the average. The European Physical Journal B, 84(4):707–711. Tomkins, A., Zhang, M., and Heavlin, W. D. (2017). Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713. Tran, H. D., Cabanac, G., and Hubert, G. (2017). Expert suggestion for conference program committees. In 2017 11th International Conference on Research Challenges in Information Science (RCIS), pages 221–232. Travis, G. D. L. and Collins, H. M. (1991). New light on old boys: Cognitive and institutional particularism in the peer review system. Science, Technology, & Human Values, 16(3):322–341. Wang, J. and Shah, N. B. (2019). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS. Xu, Y., Zhao, H., Shi, X., and Shah, N. (2019a). On strategyproof conference review. In Proceedings of the International Joint Conferences on Artiﬁcial Intelligence. Xu, Y., Zhao, H., Shi, X., Zhang, J., and Shah, N. (2019b). On strategyproof conference review. Arxiv preprint.
47

Appendix
We provide supplementary materials and additional discussion.

A Discussion of approximation results
In this section we discuss the approximation-related results. In what follows we consider function f (s) = s and for any value c ∈ R, we denote the matrix all of whose entries are c as c.

A.1 Example for ILPR algorithm.
We begin by construction a series of similarity matrices for various λ such that ΓS AILPR assignments APR4A and AHARD have non-trivial fairness.

= 0 while

Proposition 1. For every positive integer λ, there exists a similarity matrix S such that ΓS AILPR = 0 and ΓS APR4A ≥ λ1 ΓS AHARD > 0.

Proof. Given any positive integer λ ∈ N, consider an instance of reviewer assignment problem with m = n, µ = λ and similarities given by the block matrix

1

1

0  } n1

S=  0

0

(s − ε) · 1  } n2

(44)

(s − ε) · 1 (s − ε) · 1 s · 1

} n3

m1

m1

m1

Here s = n1n+1n2 , the value ε > 0 is some small constant strictly smaller than s, and nr = mr > 0 for every r ∈ {1, 2, 3}. We also require n3 > λ and

n2 = (λ − 1) n1 + 1.

(45)

We refer to the ﬁrst m1 papers and n1 reviewers as belonging to the ﬁrst group, the second m2 papers and n2 reviewers as belonging to the second group, and so on.
The ILPR algorithm involves two steps. The ﬁrst step consists of solving a linear programming relaxation and ﬁnding the most fair fractional assignment. The second step then performs a rounding procedure in order to obtain integer assignments. Let us ﬁrst see the output of the ﬁrst step of the ILPR algorithm — the fractional assignment with the highest fairness — on the similarity matrix (44). Observe that for each of the m3 papers in the third group, the sum of the similarities of any λ reviewers is at most λs, and furthermore, that this value is achieved with equality if and only if they are reviewed by λ reviewers from the third group. Next, the n1 reviewers from the ﬁrst group can together review λn1 papers. Dividing this amount equally over the m1 + m2 papers in the ﬁrst two groups (in any arbitrary manner) and complementing the assignment with reviewers from the second group, we see that each paper from the ﬁrst and the second groups receives a sum similarity λ m1n+1m2 = λs. It is not hard to see that any deviation from the assignment introduced above will lead to a strict decrease of the fairness.
The second step of the ILPR algorithm is a rounding procedure that constructs a feasible assignment from the fractional assignment (solution of linear programming relaxation) obtained in the previous step. The rounding procedure is guaranteed to assign λ reviewers to each paper, respecting the following condition: any reviewer assigned to any paper j ∈ [m] in the resulting feasible assignment must have a non-zero fraction allocated to that paper in the fractional assignment.
Now notice that aforementioned condition ensures that all papers from the third group must be assigned to reviewers from the third group. Next, recall that on one hand, reviewers from the ﬁrst group can together review at most λn1 diﬀerent papers. On the other hand, in each optimally fair fractional assignment, the ﬁrst m1 + m2 papers are assigned to reviewers from the ﬁrst two groups. Thus, in the resulting integral assignment

48

ΓS AILPR ΓS AHARD ΓS APR4A

λ=1
0 0.49 0.49

λ=2
0 0.65 0.65

λ=3
0 0.72 0.72

λ=4
0 0.76 0.76

Table 6: Fairness of various assignment algorithms for the class of similarity matrices (44).

these papers also must be assigned to reviewers from the ﬁrst two groups. These two facts together with the inequality λn1 < m1 + m2 that we obtain from (45) ensure that at least one paper in the resulting integral assignment will be reviewed by λ reviewers with zero similarity. Hence, the assignment computed by the ILPR algorithm has zero fairness ΓS AILPR = 0.
On the other hand, it is not hard to see that ΓS AHARD ≥ s − ε. Indeed, let us assign one reviewer to each paper by the following procedure: the m1 papers from the ﬁrst group and some m2 − 1 papers from the second group are all assigned one arbitrary reviewer each from the ﬁrst group of reviewers. Such an assignment is possible since λn1 = m1 + m2 − 1 due to (45). The remaining paper from the second group is assigned one arbitrary reviewer from the third group. At this point, there are m3 papers (in the third group) which are not yet assigned to any reviewer, and n3 + n2 − 1 ≥ m3 reviewers who have not been assigned any paper and have similarity higher than s − ε with these m3 papers in the third group. Assigning one reviewer each from this set to each of these m3 papers, we obtain an assignment in which each paper is allocated to one reviewer with similarity at least s − ε. Completing the remaining assignments in an arbitrary fashion, we conclude that ΓS APR4A ≥ λ1 ΓS AHARD ≥ s − ε > 0 where ﬁrst inequality is due to Theorem 1.
The results of simulations for λ ∈ {1, 2, 3, 4}, parameters n1 = 1, n2 = λ, n3 = λ + 1, ε = 0.01 and similarity matrices S deﬁned in (44) are depicted in Table 6. Interestingly, for these choices of parameters, our PeerReview4All algorithm is not only superior to ILPR , but is also able to exactly recover the fair assignment.

A.2 Sub-optimality of TPMS
In this section we show that assignment obtained from optimizing the objective (2) can be highly sub-optimal with respect to the criterion (4) even when f is the identity function.
Proposition 2. For any λ ≥ 1, there exists a similarity matrix S such that ΓS APR4A = ΓS AHARD ≥ λ4 and ΓS ATPMS = 0.

Proof. Consider an instance of the problem with m = n = 2λ, and similarities given by the block matrix

S = 1 0.4 } λ (46) 0.4 0 } λ

λ

λ

Then ATPMS assigns the ﬁrst λ reviewers to the ﬁrst λ papers (in some arbitrary manner) and the remaining

reviewers to the remaining papers, obtaining

sij = λ2 and
j∈[m] i∈RATPMS (j)
ΓS ATPMS = 0

In contrast, assignments APR4A and AHARD assign the ﬁrst 12 n reviewers to the second group of papers

49

Reviewer 1 Reviewer 2 Reviewer 3 Reviewer 4

Paper a
0.3 + 0.3 −
0 0

Paper b
1 0 0.1 0.1

Paper c
1 1 0 0

Paper d
0 1 0.3 0.3

Table 7: An example of similarities that yield 1/λ approximation factor of the PeerReview4All algorithm.

Paper a Paper b Paper c Paper d

AHARD

1st Reviewer 2nd Reviewer

1

2

1

3

2

4

3

4

A1

1st Reviewer 2nd Reviewer

1

3

1

3

2

4

2

4

A2

1st Reviewer 2nd Reviewer

1

2

3

4

1

2

3

4

Table 8: The optimal assignment as well as and PeerReview4All ’s intermediate assignments for the similarities in Table 7.

and the remaining reviewers to the remaining papers. This assignment yields

sij =

sij = 0.8λ2 and

j∈[m] i∈RAPR4A (j)

j∈[m] i∈RAHARD (j)

ΓS

APR4A

= ΓS

AHARD

λ = 0.4λ ≥ .

4

This concludes the proof.

A.3 Example of 1/λ approximation factor for APR4A
Let us consider an instance of fair assignment problem with m = n = 4, λ = µ = 2 and similarities represented in Table 7.
First, note that ΓS AHARD ≤ 0.6. This is because in every feasible assignment A ∈ A paper 1 in the best case is assigned to reviewers 1 and 2. Moreover, there exists a feasible assignment represented as AHARD in Table 8 which achieves a max-min fairness of 0.6 and hence we have ΓS AHARD = 0.6.
Let us now analyze the performance of PeerReview4All algorithm. Again, the fairness of the resulting assignment is determined in the ﬁrst iteration of Step 2 to 7 of Algorithm 1, so we restrict our attention to that part of the algorithm. It is not hard to see that after Step 2 is executed, we have two candidates assignments, A1 and A2, represented in Table 8 (up to not important randomness in braking ties). Computing the fairness of these assignments, we obtain

ΓS (A1) = 0.3 + ε and ΓS (A2) = 0.2.

which implies that

ΓS APR4A max ΓS (A1) , ΓS (A2)

ΓS (AHARD) =

ΓS (AHARD)

1ε =+ .
2 0.6

Setting small enough, we can see that the approximation factor is very close to 1/2 = 1/λ.

50

B Computational aspects
A na¨ıve implementation of the PeerReview4All algorithm has a polynomial computational complexity (under either an arbitrary choice or one computable in polynomial-time in Step 6) and requires O λm2n iterations of the max-ﬂow algorithm. There are a number of additional ways that the algorithm may be optimized for improved computational complexity while retaining all the approximation and statistical guarantees.
One may use Orlin’s method (Orlin, 2013; King et al., 1992) to compute the max-ﬂow which yields a computational complexity of the entire algorithm at most O λ(m + n)m3n2 . Instead of adding edges is Step 3 of the subroutine one by one, a binary search may be implemented, reducing the number of max-ﬂow iterations to O (λm log mn) and the total complexity to O λ(m + n)m2n .
Finally, note that the max-min approximation guarantees (Theorem 1), as well as statistical results
(Theorems 2 to 5 and corresponding corollaries) remain valid even for the assignment A computed in Step 3 of Algorithm 1 during the ﬁrst iteration of the algorithm. The algorithm may thus be stopped at any time after the ﬁrst iteration if there is a strict time-deadline to be met. However, the results of Corollary 1 on optimizing the assignment for papers beyond the most worst-oﬀ will not hold any more.9 The computational complexity of each of the iterations is at most O (λ(m + n)mn), and stopping the algorithm after a constant number of iterations makes it comparable to the complexity of TPMS algorithm which is successfully implemented in many large scale conferences.
Let us now brieﬂy compare the computational cost of PeerReview4All and ILPR algorithms. The full version of ILPR algorithm requires O(m2) solutions of linear programming problems. Given that ﬁnding a max-ﬂow in a graph constructed by our subroutine can be casted as linear programming problem (with constraints similar to those in Garg et al. 2010), we conclude that slightly optimized implementation of our algorithm results in O(λm log mn) solutions of linear programming problems, which is asymptotically better. To be fair, the ILPR algorithm also can be terminated in an earlier stage with theoretical guarantees satisﬁed, which brings both algorithms on a similar footing with respect to the computational complexity.

C Topic coverage

In this section we discuss an additional beneﬁt of “topic coverage” that can be gained from the special choice

of heuristic in Step 6 of Subroutine 1 of our PeerReview4All algorithm.

Research is now increasingly inter-disciplinary and consequently many papers submitted to modern

conferences make contributions to multiple research ﬁelds and cannot be clearly attributed to any single

research area. For instance, computer scientists often work in collaboration with physicists or medical

researchers resulting in papers spanning diﬀerent areas of research. Thus, it is important to maintain a broad

topic coverage, that is, to ensure that such multidisciplinary papers are assigned to reviewers who not only

have high similarities with the paper, but also represent the diﬀerent research areas related to the paper. For

example, if a paper proposes an algorithm to detect new particles in the CERN collider, then that paper

should ideally be evaluated by competent physicists, computer scientists, and statisticians.

There are prior works both in peer-review (Long et al., 2013) and in text mining (Lin and Bilmes, 2011)

which propose a submodular objective function to incentivize topic coverage. According to Long et al. (2013),

the appropriate measure of coverage is a number of distinct topics of the paper covered, summed across the

all papers. Let us introduce a piece of notation to formally describe the underlying optimization problem.

For every paper j ∈ [m], let T (j) = {t(1j), . . . , t(rjj)} be related research topics and for every reviewer i ∈ [n],

let

T (i)

=

{t

(i 1

)

,

.

.

.

,

t

(i) ri

}

be

the

topics

of

expertise

of

reviewer

i.

For

every

assignment

A,

we

deﬁne

ω(A)

to

be the total number of distinct topics of all papers covered by the assigned reviewers:





ω(A) =

card 

T (j)

j∈[m]

i∈RA (j )

T (i)  ,

9If the algorithm is terminated after p iterations, then bound (8) from Corollary 1 holds for r ∈ [p ].

(47)

51

where card(C) denotes the number of elements in the set C. The goal in Long et al. (2013) is to ﬁnd an assignment that maximizes ω(A) and respects the constraints on the paper/reviewer load. However, instead of the requirement that each paper is assigned to λ reviewers as in our work, Long et al. (2013) consider a relaxed version and require each paper to be reviewed by at most λ reviewers.
Using the submodular nature of the objective (47), Long et al. (2013) propose a greedy algorithm that is guaranteed to achieve a constant-factor approximation of the optimal coverage (47). This greedy algorithm, however, has the following two important drawbacks:
(i) Like the TPMS algorithm, the greedy algorithm aims at optimizing the global functional, and consequently may fare poorly in terms of fairness. Indeed, in order to optimize the global objective (47), the greedy algorithm may sacriﬁce the topic coverage for some of the papers, assigning relevant reviewers to other papers.
(ii) While guaranteed to achieve a constant factor approximation of the objective (47), the greedy algorithm may yield an assignment in which papers are reviewed by (much) less than λ reviewers. It is not even guaranteed that in the resulting assignment each paper has at least one reviewer.
Nevertheless, both the PeerReview4All algorithm and the algorithm of Long et al. (2013) can beneﬁt from each other if the latter is used as a heuristic to choose a feasible assignment in Step 6 of the subroutine of the former. In what follows we detail the procedure to combine the two algorithms. The greedy algorithm of Long et al. (2013) picks (reviewer, paper) pairs one-by-one and adds them to the assignment. At each step, it picks the pair that yields the largest incremental gain to (47) while still meeting the paper/reviewer load constraints. In Step 6 of the subroutine of PeerReview4All, we may use the greedy algorithm, restricted to the (reviewer, paper) pairs added to the network in the previous steps, to ﬁnd an assignment that approximately maximizes (47). Next, for every (reviewer, paper) pair that belongs to this assignment, we set the cost of the corresponding edge in the ﬂow network to 1 and the costs of the remaining edges to 0. Finally, we compute the maximum ﬂow with maximum cost in the resulting network and ﬁx (reviewer, paper) pairs that correspond to edges employed in that ﬂow in the ﬁnal output of the subroutine.
Let us now discuss the beneﬁts of this approach. First, in PeerReview4All we modify only the procedure of tie-breaking among max-ﬂows, and hence all the guarantees established in the paper continue to hold. Second, the introduced procedure allows to overcome the issue (ii), because the max-ﬂow guarantees that each paper is assigned with exactly requested number of reviewers. Third, by setting the cost of selected edges to 1, we encourage the topic coverage (although the pproximation guarantee of the greedy algorithm no longer holds). Finally, we do not allow the algorithm of Long et al. (2013) to sacriﬁce some papers in order to maximize the global coverage (47), because the subroutine ensures that in the resulting assignment all the papers are assigned to pre-selected reviewers with high similarity, thereby overcoming (i).
52

