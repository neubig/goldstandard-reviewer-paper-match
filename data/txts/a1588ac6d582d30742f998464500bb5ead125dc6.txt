arXiv:2202.13110v2 [cs.LG] 9 May 2022

Optimal-er Auctions through Attention
Dmitry Ivanov, HSE University and JetBrains Research, Russia Iskander Safiulin, Independent researcher, Russia Ksenia Balabaeva, ITMO University, Russia Igor Filippov, Myna Labs, Inc., USA
RegretNet is a recent breakthrough in the automated design of revenue-maximizing auctions. It combines the expressivity of deep learning with the regret-based approach to relax and quantify the Incentive Compatibility constraint (that participants benefit from bidding truthfully). We propose two independent modifications of RegretNet, namely a new neural architecture based on the attention mechanism, denoted as RegretFormer, and an alternative loss function that is interpretable and significantly less sensitive to hyperparameters. We investigate both proposed modifications in an extensive experimental study in settings with fixed and varied input sizes and additionally test out-of-setting generalization of our network. In all experiments, we find that RegretFormer consistently outperforms existing architectures in revenue. Regarding our loss modification, we confirm its effectiveness at controlling the revenue-regret trade-off by varying a single interpretable hyperparameter.
Preliminary version.

1
Fig. 1. Our architecture RegretFormer. The detailed description is provided in Section 4.1.
1 INTRODUCTION Several decades ago, Myerson [40] proposed a general solution for the problem of optimal (revenuemaximizing) design of single-item auctions. However, despite the collaborative effort of the community, the results of the same generality for the multi-item auctions have not been obtained. Even for the auctions with two participants and two items, the problem of optimal design remains unsolved. Automated auction design [7, 8] offers an alternative to the analytical solutions by viewing auction design as a constrained optimization problem. In particular, the objective is to maximize the revenue while enforcing Incentive Compatibility (IC), which is vital for eliciting truthful preferences. This approach can provide approximate solutions in the settings where the optimal mechanisms are unknown, as well as serve as a tool that guides the research towards new discoveries. While the early approaches utilized linear programming and classic machine learning, after the renaissance of deep learning coupling auction design with neural networks was only a question of time.
The pioneers of this approach are Dütting et al. [12] with their RegretNet architecture. RegretNet is trained via differential optimization on a mixture of two objectives: to maximize revenue and to minimize regret, which is a relaxation of the IC constraint proposed in [13]. RegretNet recovers near-optimal revenue in the analytically solved settings and outperforms the previous state-of-theart in more complex settings while having vanishing regret guarantees. The success of RegretNet opened up avenues for the new studies trying to extend or improve the algorithm in different dimensions. In line with this research, we propose two independent modifications of RegretNet.
First, we propose a new neural architecture for the optimal auction design based on the attention mechanism. We denote it as RegretFormer after the widely-known Transformers. RegretFormer augments RegretNet with the expressivity of the attention mechanism while also having several crucial properties. On the one hand, it is by default insensitive to the order of items and participants, which is highly desirable for learning symmetric auctions. On the other hand, it does not assume a predetermined fixed input size, which makes it applicable to multiple settings at once, even beyond those provided during training.
Second, we propose an alternative objective formulation for RegretNet. In contrast to the previous approaches based on optimizing a mixture of revenue and regret objectives, we maximize revenue given a fixed regret budget, pre-specified by the designer. This is realised through the dual gradient decent technique. As a result, we obtain an interpretable loss function that allows the designer to balance the revenue-regret trade-off by varying the acceptable regret budget. Furthermore, our approach requires less hyperparameter tuning to optimize the performance.

2
We conduct an extensive experimental study to verify the strengths of our modifications. Specifically, we compare our and existing architectures in the settings with fixed input size that are standard in the literature, propose new settings with varied input size, and test out-of-setting generalization in both cases. In all experiments, we find that RegretFormer consistently outperforms existing architectures in revenue given the same regret budget. This result is robust to the input size and the regret budget. We additionally verify that the performance gap of our model is genuine in two newly proposed experiments. In the first experiment we approximate the regret of RegretFormer given the misreports found by RegretNet. The second experiment is based on the network distillation where we train RegretNet to mimic the predictions of RegretFormer. Regarding our loss modification, we confirm that specifying and varying the regret budget results in the intended revenue-regret trades-off, although fluctuations are possible due to the regret approximation error.

2 BACKGROUND AND NOTATIONS

2.1 Problem statement

An auction is defined as a set of 𝑛 bidders 𝑁 = {1, ..., 𝑛} and 𝑚 items 𝑀 = {1, ..., 𝑚}. Each bidder 𝑖

selects a subset of items 𝑆 ⊆ 𝑀 and evaluates it with valuation function 𝑣𝑖 : 2𝑀 → R. In our study,

we consider the case of additive valuations, meaning that that for each item 𝑗 ∈ 𝑀 a bidder has a

valuation function 𝑣𝑖 ( 𝑗), and a valuation of a subset of items 𝑆 ⊆ 𝑀 equals to the sum of valuations

of the items in the subset 𝑣𝑖 (𝑆) = Σ𝑗 ∈𝑆𝑣𝑖 ( 𝑗).

The valuation function of a bidder 𝑖 is drawn independently from a distribution 𝐹𝑖 over all possible

valuation functions 𝑉𝑖 . A set of valuations of all bidders forms a valuation profile 𝑣 = (𝑣1, ..., 𝑣𝑛). An

auctioneer does not know the reported bidder’s valuation, however, he has a sample 𝑆 = 𝑣1, ..., 𝑣𝐿

of 𝐿 valuation profiles from the distribution 𝐹 = (𝐹1, ...𝐹𝑛). The bidders report their valuation and

the auction mechanism allocates the items among them charging a payment. An auction (𝑔, 𝑝) is

denoted as a pair of allocation rules: 𝑔𝑖 : 𝑉 → [0, 1]𝑀 and payment rules: 𝑝𝑖 : 𝑉 → R≥. The bidders

propose bids 𝑏 = (𝑏1, ..., 𝑏𝑛) ∈ 𝑉 and the mechanism computes allocations 𝑔(𝑏) and payments 𝑝 (𝑏).

Based on bidder’s valuation 𝑣𝑖 and given bids 𝑏 we compute the bidder’s utility 𝑢𝑖 (𝑣𝑖, 𝑏) =

𝑣𝑖 (𝑔𝑖 (𝑏)) − 𝑝𝑖 (𝑏). Define the valuation profile without the valuation 𝑣𝑖 as 𝑣−𝑖 . Similarly, define the

bids without 𝑏𝑖 as 𝑏−𝑖 , and the valuation profile of all bidders except 𝑖 as 𝑉−𝑖 = Π𝑘≠𝑖𝑉𝑘 . An auction is

dominant strategy incentive compatible (DSIC) if the utility of each bidder is maximized by reporting

their true valuations regardless of bids of other bidders: ∀𝑣, 𝑏 : 𝑢𝑖 (𝑣𝑖, (𝑣𝑖, 𝑏−𝑖 )) ≥ 𝑢𝑖 (𝑣𝑖, (𝑏𝑖, 𝑏−𝑖 )).

Consequently, in DSIC auction the best strategy for each bidder is to report truthfully. An auction

is ex-post individually rational (IR) if for all bidders 𝑖 ∈ 𝑁 , all valuations 𝑣𝑖 ∈ 𝐹 , and all bids 𝑏𝑖 ∈ 𝑉𝑖 ,

the individual utility is non-negative: 𝑢𝑖 (𝑣𝑖, 𝑏𝑖 ) ⩾ 0.

Define the revenue of the profile 𝑣 as Σ𝑖𝑝𝑖 (𝑣). The goal of the optimal auction design is to

maximize the expected revenue, possibly subject to DSIC and IR constraints. The problem is

analytically solved for the auctions with one item in the seminal work of Myerson [40], provided

the valuation distributions are known. There are no general solutions for the auctions with 𝑚 > 1.

The task of optimal auction design can be viewed as an optimization or a machine learning

problem. In this case, the expected revenue takes the place of the objective. There is a class of

auctions

(𝑔(𝑤), 𝑝 (𝑤))

∈

𝑀

parameterized with 𝑤

∈

𝑑
R

for some 𝑑

∈

N and a set of bidders

valuation profiles 𝑆 = 𝑣1, ..., 𝑣𝐿 drawn as independent and identically distributed variables from 𝐹 .

Then, the goal of the optimization procedure is to find an auction in 𝑀 that minimizes the negated

expected revenue −E[Σ𝑖 ∈𝑁 𝑝𝑖 (𝑣; 𝑤)] while satisfying DSIC and IR constraints.

3

2.2 Multi-head self-attention
Once in a while, the technologies emerge in machine learning that set the vector for the development of the field years to come. The attention mechanism is one these technologies. Architectures based on attention have been achieving state-of-the-art performance in diverse and complex problems [10, 58, 61].
Popularized by Vaswani et al. [58], the attention function maps a query vector and a set of keyvalue vector pairs to an output vector. The procedure is typically applied to a set or a sequence of queries. The output vector is a weighted sum of the values, and each weight reflects the compatibility of the query with the corresponding key. While different attention mechanisms exist, the softmax attention is the most common:

𝑄𝐾𝑇

Attention(𝑄, 𝐾, 𝑉 ) = softmax( √ )𝑉

(1)

𝑑𝑘

where 𝑄, 𝐾, and 𝑉 are respectively the matrices of queries, keys, and values. Self-attention is a special case of attention in which 𝑄, 𝐾, and 𝑉 are linear projections of the same set of inputs. Typically, layer normalization is applied to the input before projecting [2]. The authors also propose multi-head attention (MHA) mechanism. In this extension, H different attention heads are created, and for each attention head the input matrices are projected with head-specific weight matrices

𝑄𝑊 𝑄 , 𝐾𝑊 𝐾 , 𝑉𝑊 𝑉 to calculate the inputs to attention:

ℎ

ℎ

ℎ

MHA(𝑄, 𝐾, 𝑉 ) = Concat(ℎ𝑒𝑎𝑑1, ..., ℎ𝑒𝑎𝑑𝐻 )𝑊 𝑂

(2)

ℎ𝑒𝑎𝑑ℎ

=

Attention(𝑄𝑊 𝑄, 𝐾𝑊

𝐾
,𝑉𝑊

𝐾)

(3)

ℎ

ℎ

ℎ

Attention is equivariant to the order of elements in the input. In some applications where this order is important – for example, natural language processing (NLP) tasks – an additional positional encoding technique is used. It augments the initial representation of the input data with the information about the order of the elements. However, in other problems such encoding is not required. In particular, in the design of symmetric auction mechanisms, the information about the serial numbers of participants and items is unimportant.

3 REGRETNET AND ITS EXISTING MODIFICATIONS
3.1 RegretNet
RegretNet was proposed by Dütting et al. [12] as a deep learning solution for the optimal auction design. It can be used for multi-bidder and multi-item settings, the analytical solutions for which are unknown. The goal of RegretNet is to explicitly encode the allocation and payment rules of the auction mechanism. These rules are discovered as a result of an optimization procedure.
The authors define three network architectures for different valuation types, namely additive, unit-demand, and combinatorial valuations. In this paper, we only focus on the additive valuations, although our modifications can be extended to the other valuation types.
The RegretNet architecture consists of two parts: the allocation network and the payment network. Both networks take a fixed-sized bid matrix 𝐵𝑛𝑚 flattened into a vector as an input, and process the input through multiple stacked linear layers with tanh activation function in between.
The allocation network maps the matrix of bids to the distribution of allocations between participants: 𝐴𝑛𝑒𝑡 (𝐵𝑛𝑚) = 𝑍𝑛𝑚 (𝐴𝑛𝑒𝑡 : R𝑛𝑚 → [0, 1]𝑛𝑚), where 𝑧𝑖 𝑗 is the probability of allocating the 𝑗-th item to the 𝑖-th bidder. To allow for the possibility of an item remaining unallocated, the output of the final layer has the size of (𝑛 + 1) · 𝑚, imitating an additional dummy participant with zero valuations. Then, to obtain properly scaled allocation probabilities over participants for each

4

item, the softmax activation is applied to the output of the final layer corresponding to each item.

Specifically, 𝑧𝑖 𝑗 = 𝑒𝑧˜𝑖𝑗 /

𝑛+1 𝑖 =1

𝑒

𝑧˜𝑖

𝑗

,

where

𝑧˜𝑖 𝑗

denotes

the

unnormalized

allocation

probability.

The payment network maps the matrix of bids to the vector of pay values: 𝑃𝑛𝑒𝑡 (𝐵𝑛𝑚) = 𝑃ˆ𝑛

(𝑃𝑛𝑒𝑡 : R𝑛𝑚 → [0, 1]𝑛), where 𝑝ˆ𝑖 is the fraction of the 𝑖-th bidder expected utility that the bidder

transfers to the auctioneer. The payment is then computed as 𝑝𝑖 = 𝑝ˆ𝑖 (

𝑚 𝑗 =1

𝑧𝑖 𝑗𝑏𝑖 𝑗 )

for

𝑖

=

1,

.

.

. , 𝑛.

To properly scale 𝑝ˆ𝑖 ∈ [0, 1], a sigmoid activation is applied to the output of the final layer. Because

the payment cannot exceed the bidder’s expected utility, the mechanisms discovered by RegretNet

always satisfy the IR constraint.

When applied to the auctions with known analytical solutions, e.g. Manelli-Vincent and Pavlov

auctions [35, 46], RegretNet is able to recover mechanisms very close to the analytical solutions.

On the other hand, when applied to more difficult settings with unknown solutions, RegretNet

outperforms in revenue the previous state-of-the-art in automated mechanism design [53].

Given a sample of profiles 𝐿, RegretNet aims to optimize the following empirical objective:

1 ∑︁ ∑︁

min −

𝑝𝑖 (𝑣𝑙 ; 𝑤)

𝑤 |𝐿|
𝑙 ∈𝐿 𝑖 ∈𝑁

(4)

𝑠.𝑡 . 𝑟𝑔𝑡𝑖 (𝑣𝑙 ; 𝑤) = 0, ∀𝑖 ∈ 𝑁 , 𝑙 ∈ 𝐿

where 𝑟𝑔𝑡𝑖 denotes the 𝑖-th bidder’s ex-post regret. Regret is a quantifiable relaxation of the DSIC constraint that was first introduced in [13]. It is defined as the difference between the 𝑖-th bidder’s utilities given the optimal misreport and the truthful bid:

𝑟𝑔𝑡𝑖 (𝑣𝑙 ; 𝑤 ) = max

𝑢

(𝑣𝑙 ,

(𝑣 ′,
𝑖

𝑙
𝑣−

);

𝑤)

−

𝑢

(𝑣𝑙 ,

𝑙
𝑣

;

𝑤)

(5)

𝑣′ 𝑖 𝑖 𝑖

𝑖

To solve the constrained optimization problem 4 over the space of the network parameters 𝑤,

the authors of RegretNet employ the augmented Lagrangian method. This yields a loss function

that combines revenue optimization with a penalty for a violation of DSIC constraints:

L𝑜𝑢𝑡𝑒𝑟 (𝑤 ) = ∑︁ −𝑃𝑖 + 𝜆𝑖𝑅𝑖 + 𝜌2 𝑅𝑖2 (6)
𝑖 ∈𝑁
where 𝐵 denotes mini-batch, 𝑃𝑖 = |𝐵1 | 𝑙 ∈𝐵 𝑝𝑖 (𝑣𝑙 ; 𝑤 ) is the average revenue from the 𝑖-th participant, 𝑅𝑖 = |𝐵1 | 𝑙 ∈𝐵 𝑟𝑔𝑡𝑖 (𝑣𝑖′𝑙, 𝑣𝑙 ; 𝑤 ) is the average approximate regret of the 𝑖-th participant, and 𝑟𝑔𝑡𝑖 is an approximation of 𝑟𝑔𝑡𝑖 estimated via gradient descent in the inner optimization loop by maximizing the 𝑖-th participant ex-post utility as a function of their misreport:

L𝑖𝑛𝑛𝑒𝑟

(𝑣 ′𝑙 )

=

−𝑢 (𝑣𝑙,

(𝑣 ′𝑙 ,

𝑙
𝑣−

);𝑤)

(7)

𝑖

𝑖𝑖 𝑖

To sum it up, RegretNet repeatedly updates its parameters 𝑤 via gradient descent on the outer loss function (6), and for each outer iteration updates the optimal misreport estimates 𝑣 ′ several
𝑖
times via gradient descent on the inner loss function (7). Both outer and inner updates use separate Adam optimizers [29].
While the augmented loss function (6) is intended as a solution to the constrained optimization problem (4), in practice the constraint is only approximately satisfied by a trained RegretNet. The trade-off between the resulting revenue and regret values is controlled by the hyperparameters 𝜌 and 𝜆𝑖 , as well as by their schedules. Specifically, 𝜆 ∈ R𝑛 is a vector, the coordinates of which are increased after each update of the network according to the gradient of the loss function

𝜆𝑖 ← 𝜆𝑖 + 𝜌𝑅𝑖 , and 𝜌 > 0 is also periodically increased according to a linear schedule 𝜌 ← 𝜌 + 𝜌Δ. While producing impressive results, the outlined procedure has two major issues:

5

(1) Sensitivity. In the RegretNet paper, the hyperparameters 𝜆, 𝜌, 𝜌Δ were specifically selected for each experiment. Rahme et al. [50] show that an improper selection of these hyperparameters can result in a massive degradation of the performance.
(2) Uninterpretability. The hyperparameters 𝜆, 𝜌, 𝜌Δ were selected without clear motivation and resulted in an arbitrary trade-off of revenue and regret. While it is straightforward that increasing any of these hyperparameters tightens the regret budget the network can afford at different stages of training, the exact effect of such changes on the resulting revenue and regret values is unpredictable. Furthermore, the recipe of the hyperparameter selection for the new settings that have not been studied in the literature is unclear.
3.2 Permutation-equivariant architecture
EquivariantNet was proposed by Rahme et al. [50] to effectively deal with multi-bidder symmetric auctions. The architecture relies on a theorem originally proven by [9] that there exists an optimal solution for a symmetric auction that is permutation-equivariant. The allocation and payment functions g and p are permutation-equivariant if for any two permutation matrices 𝑊1 ∈ {0, 1}𝑛𝑥𝑛 and 𝑊2 ∈ {0, 1}𝑚𝑥𝑚 and any valuation matrix 𝑉 , we have 𝑔(𝑊1𝑉𝑊2) = 𝑊1𝑔(𝑉 )𝑊2 and 𝑝 (𝑊1𝑉𝑊2) = 𝑊1𝑝 (𝑉 ). By wiring this property into the architecture, the authors reduce the solution search space.
Like RegretNet [12], EquivariantNet has an allocation network which maps the matrix of bids to the distribution of item allocations between participants and a payment network that maps the matrix of bids to the vector of payment values. Both networks consist of compositions of exchangeable layers which preserve permutation-equivariance.
The exchangeable layer is inspired by deep sets [62] and is defined as follows: a layer is specified by the number of input channels 𝐾, the number of output channels 𝑂 and five learnable parameters 𝑤1, 𝑤2, 𝑤3, 𝑤4 ∈ R𝐾𝑥𝑂 and 𝑤5 ∈ R𝑂 . The input is a tensor B of size (K, n, m) and the output is tensor Y of size (O, n, m). Then, the element (i, j) of the o-th output channel 𝑌 (𝑜) is given by:
𝑖,𝑗

𝐾

(𝑘 ,𝑜 )

(𝑘 ,𝑜 )

(𝑘 ,𝑜 )

(𝑜 )
𝑌

=

𝜎

∑︁(𝑤 (𝑘,𝑜) 𝐵 (𝑘) + 𝑤2

∑︁ 𝐵 (′𝑘) + 𝑤3

∑︁ 𝐵 (𝑘′) + 𝑤4

∑︁ 𝐵 (′𝑘)′ ) + 𝑤 (𝑜)

(8)

𝑖,𝑗

1

𝑖,𝑗

𝑛

𝑖 ,𝑗 𝑚

𝑖,𝑗 𝑛𝑚

𝑖 ,𝑗

5

𝑘 =1

𝑖′

𝑗′

𝑖 ′, 𝑗 ′

The EquiavariantNet is the first architecture that can generalize to unseen settings but its use is limited. The authors observe that it produces competitive results but does not outperform RegretNet.

3.3 Formulation as a two-player game and simplification of the loss function
The contributions of Rahme et al. [50] are two-fold. First, the authors amortize the inner optimization loop used for the approximate regret computation. Specifically, instead of the gradient descent procedure (7) they parameterize the optimal misreports with weights of an additional neural network, trained adversarially to maximize the ex-post utility of a bidder given the mechanism. Their procedure is based on the formulation of the auction learning problem as a game between the two networks, which is similar to how Generative Adversarial Networks are formalized [18]. While this modification improves the training speed, it does not improve the final performance, so incorporating it in our experiments is unnecessary. A similar adversarial approach is also used in a mirror problem of learning optimal bidding strategies [43].
Second, the authors propose the following simplified loss function in place of (6):

√︄

√︄

∑︁

∑︁

∑︁

L𝑜𝑢𝑡𝑒𝑟 (𝑤 ) = −

𝑃𝑖 + 𝛼

𝑅𝑖 + 𝛽 𝑅𝑖

(9)

𝑖 ∈𝑁

𝑖 ∈𝑁

𝑖 ∈𝑁

6
Unlike 𝜆𝑖 and 𝜌 in the original loss function, the hyperparameters 𝛼 and 𝛽 are kept constant throughout the training, which the authors refer to as time-independence. Moreover, the authors set 𝛼 = 𝛽 = 1 in all their experiments and claim their loss function to be hyperparameter-free. While this modification reduces the burden of hyperparameter tuning, we argue that the claim of being hyperparameter-free is an oversimplification. The main supporting evidence for this claim is that using their loss function yields revenue and regret estimates similar to the original RegretNet. However, since the trade-off of revenue and regret was selected arbitrarily in the original RegretNet, there is also no reason to treat the selected 𝛼 and 𝛽 as optimal, and balancing the trade-off would still require to tune these hyperparameters. And like in the original RegretNet, the exact effect of changing the hyperaparameters on the resulting revenue and regret values is unpredictable.
3.4 Other related work
Early works that explored the automated solutions to the auction design formulated the problem as linear program [7, 8, 52] or searched within specific families of DSIC auctions like Virtual Valuations Combinatorial Auctions (VVCA) and Affine Maximizer Auctions (AMA) with techniques like grid-search and gradient ascent [33, 53]. The former approach suffers from scalability issues due to the curse of dimensionality [21], while the latter approach searches within a limited family of auctions that might not contain the optimal mechanism. Classic machine learning has also been applied to the auction design [13, 32, 41], but these approaches are considered less flexible and general than the deep learning alternatives [12].
In recent years, multiple extensions of RegretNet have been proposed, including the extensions to the optimal auction design with budget constraints [14], fairness constraints [31], and human preferences over desirable allocations [48], as well as the problem of faculty allocation [15] and the matching problem [51]. Shen et al. [54] and Dütting et al. [12] propose revenue-maximizing alternatives to RegretNet that are exactly DSIC but that are only applicable to the auctions with one bidder. Deep learning has been applied to other aspects of mechanism design, such as iterative combinatorial auctions [60], minimizing economic burden on bidders within the Groves family of auctions [56], optimal bidding strategies [43, 44], optimal redistribution mechanisms [36], and E-commerce advertising [34]. A plethora of research has also focused on the questions of sample complexity of designing revenue-maximizing auctions [3, 6, 11, 16, 17, 20, 22, 25, 37–39, 42, 55].
A number of works have started to bring Reinforcement Learning tools to the auction design [26, 45, 57]. For instance, Jin et al. [26] propose Distributed Coordinated Multi-Agent Bidding algorithm for bidding optimization and Tang [57] automatically designs dynamic pricing schemes of advertisement auctions.
4 OUR MODIFICATIONS OF REGRETNET
In this section we describe our two independent modifications of RegretNet. The first modification is a novel neural architecture for the optimal auction design, which we denote as RegretFormer. The second modification is an alternative objective formulation that yields an interpretable loss function which is less sensitive to hyperparameters.
4.1 RegretFormer: enhancing RegretNet with the power of attention
It is often stated that RegretNet achieves near-optimal revenue and thus is unlikely to be outperformed [49, 50]. While successfully demonstrating the application of deep learning to the optimal auction design, the neural architecture itself is not perfect. One of the issues is the sensitivity of RegretNet to the order of items and participants in the bid matrix. Such order should not affect the outcome of a symmetric auction, and even in non-symmetric auctions, symmetric solutions often achieve high or near-optimal revenue [1, 23, 27, 28, 30], so the option to wire equivariance to

7

the input order into the architecture is desirable. Another issue is that RegretNet assumes a fixed number of participants and items, which prevents the network from learning general mechanisms and limits its applicability in practice. Finally, the multi-layer perceptron that RegretNet is based on is limited in expressivity and might not have the right inductive biases for the problem of auction design. Taking these issues into consideration, we have redesigned the neural architecture and bring to your attention RegretFormer. Its architecture is illustrated in Figure 1.
There are several high-level differences between the architectures. Whereas RegretNet uses two separate networks to calculate allocations and payments, our architecture leverages a single shared network with two outputs. Unlike RegretNet where the input is flattened into a vector, the input of RegretFormer is the matrix of bids with a dummy third dimension 𝐵𝑛𝑚1. Furthermore, unlike RegretNet, 𝑛 and 𝑚 are not fixed in RegretFormer.
We now describe the architecture in detail. After each hidden layer, we apply a Relu activation. First, we apply the exchangeable layer (8) to transform each bid into an initial vector of features that already contains information about other bids:

𝑛𝑚𝑘
𝐿1

=

ExchangeableLayer(𝐵𝑛𝑚1)

(10)

Then, we sequentially apply several attention-based blocks. Each block consists of two multi-head self-attention layers with residual connections, one applied item-wise and one applied participantwise. For each layer, we accordingly reshape the input. After the attention layers, we concatenate its predictions, and apply the same fully-connected layer (FC) to the feature vectors of each bid (to reduce the dimensionality of the feature vectors to the initial size):

𝑛𝑚𝑘
𝐿𝑡 +1,𝑖𝑡 𝑒𝑚

=

MHA𝑖𝑡𝑒𝑚 (𝐿𝑡 , 𝐿𝑡 , 𝐿𝑡 )

+

𝐿𝑡

(11)

𝑛𝑚𝑘
𝐿𝑡 +1,𝑝𝑎𝑟 𝑡

=

MHA𝑝𝑎𝑟𝑡 (𝐿𝑡 , 𝐿𝑡 , 𝐿𝑡 )

+ 𝐿𝑡

(12)

𝑛𝑚𝑘
𝐿𝑡 +1

=

FC𝑡+1 (Concat(𝐿𝑡+1,𝑖𝑡𝑒𝑚, 𝐿𝑡+1,𝑝𝑎𝑟𝑡 ))

+ 𝐿𝑡

(13)

After

applying

the

attention-based

block

N

times,

we

obtain

the

attended

feature

matrix

𝑛𝑚𝑘
𝐿𝑁 +1

.

From this matrix, we create two separate matrices by averaging over one of the dimensions – the

participant

feature

matrix

𝑛𝑘
𝑃𝑁 +1

=

1 𝑚

𝑗 (𝐿𝑛𝑁𝑗+𝑘1)

and

item

feature

matrix

𝑚𝑘
𝐼𝑁 +1

=

1 𝑛

𝑖 (𝐿𝑖𝑁𝑚+𝑘1). These

matrices are essentially embeddings of participants and items respectively and are used to compute

the allocation matrix and the payment vector.

To compute the allocation matrix, we multiply the item and the participant matrices, which gives

us an 𝑛 by 𝑚 matrix of unscaled probabilities. Before scaling, we need to additionally consider the

possibility of each item remaining unallocated. To this end, we introduce a dummy participant

𝑛 + 1, the unscaled probability for which is estimated for each item as a negated sum of the unscaled

probabilities over the real participants. Finally, we apply the softmax function along the participants

to scale the probabilities. When summarized, the allocation matrix is obtained according to the

following formulas:

𝑛𝑚
𝐿𝑁 +2

=

MatMul(𝑃𝑁𝑛𝑘+1,

(𝐼𝑁𝑚+𝑘1)𝑇

)

(14)

𝐿𝑛(𝑛𝑜𝑟+𝑚1)𝑚 = Concat(𝐿𝑛𝑁𝑚+2, − ∑︁(𝐿𝑖𝑁𝑚+2)) (15)

𝑖

(𝑛+1)𝑚
𝑍

=

SoftMax(𝐿 (𝑛+1)𝑚)

(16)

𝑜𝑢𝑡

𝑛𝑜𝑟𝑚

To estimate the payment vector, we first average the participant feature matrix over the feature dimension and apply the sigmoid activation:

8

𝑃ˆ𝑛

1 ∑︁

= sigmoid(

𝑛𝑧
𝑃

)

(17)

𝑜𝑢𝑡

𝑁 +1

𝑘

𝑧

As in RegretNet, we then calculate the final payments as 𝑝𝑖 = 𝑝ˆ𝑖 (

𝑚 𝑗 =1

𝑧𝑖 𝑗𝑏𝑖 𝑗 )

for

𝑖

=

1,

.

.

. , 𝑛.

Our architecture has several advantages. On the one hand, it leverages the expressivity of the

attention layers that historically have been able to learn much more complex dependencies than

the multi-layered perceptrons. On the other hand, it maintains the equivariance and the invariance

to the order of items and participants. The former forces the resulting mechanisms to be symmetric,

which drastically reduces the solution search space. The latter enables sharing the knowledge

between the settings, which allows RegretFormer to be used in settings with varied number of

participants and items, as well as to generalize to unseen settings.

It should be noted that while we do not discuss the non-symmetric auctions, the additional

information about the order of items or participants can easily be considered by RegretFormer via

positional encoding. Also, while we only study additive valuations, our architecture can be easily

extended to uni-demand valuations by modifying the outputs the same way as proposed in [12].

4.2 Interpretable objective formulation through specifying the regret budget
As discussed in Section 3, all existing implementations of RegretNet optimize a mixture of two conflicting objectives, namely revenue maximization and regret minimization, and control their trade-off with additional hyperparameters. The original RegretNet exhibits the issues of sensitivity to and uninterpretability of these hyperparameters, and the modification of Rahme et al. [50] only mitigates the former issue. We propose an alternative perspective that mitigates both issues. Instead of balancing the objectives, we maximize the revenue given a maximal regret budget, pre-specified by the designer. This essentially corresponds to a relaxed version of the constrained objective (4):

1 ∑︁ ∑︁

min −

𝑝𝑖 (𝑣𝑙 ; 𝑤)

𝑤 |𝐿|

1 ∑𝑙 ∈︁𝐿 𝑖∑∈︁𝑁 (18)

𝑠.𝑡 .

𝑟𝑔𝑡𝑖 (𝑣𝑙 ; 𝑤 ) ≤ 𝑅𝑚𝑎𝑥

|𝐿| 𝑙 ∈𝐿 𝑖 ∈𝑁

We will first introduce our bare-bones approach and then describe several modifications that we use in practice. To optimize the constrained objective (18), we introduce a Lagrange multiplier 𝛾:

∑︁

∑︁

L𝑜𝑢𝑡𝑒𝑟 (𝑤 ) = − 𝑃𝑖 + 𝛾

𝑅𝑖 − 𝑅𝑚𝑎𝑥

(19)

𝑖 ∈𝑁

𝑖 ∈𝑁

Critically, unlike the original RegretNet we do not hand-select the Lagrange multiplier. Instead, we adaptively update 𝛾 to enforce the constraint satisfaction while exhausting all available regret budget 𝑅𝑚𝑎𝑥 . To this end, we employ dual gradient descent [4]. Specifically, we iterate between one gradient update of the network parameters 𝑤 to minimize (19) and one update of 𝛾 according to:

∑︁

𝛾 ← max 0, 𝛾 + 𝛾Δ 𝑅𝑖 − 𝑅𝑚𝑎𝑥

(20)

𝑖 ∈𝑁

where 𝛾Δ denotes the learning rate for the dual variable. This approach is inspired by the Variational Discriminator Bottleneck technique employed by Peng et al. [47] to regularize adversarial training.
In our practical implementation, we make several quality-of-life changes to the update (20):

9

• While the choice of 𝑅𝑚𝑎𝑥 is up to the designer, it is likely to depend on the setting, e.g. the

distribution of valuations, the number of participants and items, the desirable revenue. To even

the difference between the settings and avoid specifying different 𝑅𝑚𝑎𝑥 for each experiment,

we set the regret budget as the regret normalized by the revenue 𝑖 ∈𝑁 𝑅𝑖 / 𝑖 ∈𝑁 𝑝𝑖 .

• Because the regret budget is typically small, i.e. 𝑅𝑚𝑎𝑥 ≪ 1, the convergence of the linear

update rule used in (20) can be slow. We instead use the exponential update rule by replacing

the difference terms in (20) with their logarithms.

• If the regret budget 𝑅𝑚𝑎𝑥 is too tight at the beginning of training, the network may fail to

escape the local optima of low revenue. To avoid that, we initialize 𝑅𝑚𝑎𝑥 at a higher value

𝑅𝑠𝑡𝑎𝑟𝑡 and exponentially anneal it to the desirable budget 𝑅𝑒𝑛𝑑 throughout the training.

𝑚𝑎𝑥

𝑚𝑎𝑥

Gathering all modifications, we arrive for the following update rules for 𝛾 and 𝑅𝑚𝑎𝑥 :

∑︁ ∑︁

𝛾 ← max 0, 𝛾 + 𝛾Δ log( 𝑅𝑖 / 𝑝𝑖 ) − log(𝑅𝑚𝑎𝑥 )

𝑖 ∈𝑁 𝑖 ∈𝑁

(21)

𝑅𝑚𝑎𝑥 ← max

𝑒𝑛𝑑
𝑅

𝑚𝑢𝑙𝑡
,𝑅

·

𝑅𝑚𝑎𝑥

𝑚𝑎𝑥 𝑚𝑎𝑥

where 𝑅𝑚𝑢𝑙𝑡 < 1 controls the speed of annealing of 𝑅𝑚𝑎𝑥 to the desirable regret budget 𝑅𝑒𝑛𝑑 . In

𝑚𝑎𝑥

𝑚𝑎𝑥

all our experiments, we initialize 𝛾

=

1, set 𝛾Δ

=

0.1, set 𝑅𝑠𝑡𝑎𝑟𝑡
𝑚𝑎𝑥

=

0.01, and set such 𝑅𝑚𝑢𝑙𝑡
𝑚𝑎𝑥

that

𝑅𝑚𝑎𝑥 converges to 𝑅𝑒𝑛𝑑 in two-thirds of the training time. We set 𝑅𝑒𝑛𝑑 = 0.001 by default but

𝑚𝑎𝑥

𝑚𝑎𝑥

additionally investigate the effect of choosing lower regret budgets.

The proposed approach has several advantages. On the one hand, it resolves the dichotomy

of two conflicting objectives. While the regret budget needs to be explicitly pre-selected based

on the designer’s preferences, all other hyperparameters are then straightforward to tune using

standard search procedures by maximizing the revenue given the specified regret budget. This is

unlike the existing approaches (6) and (9) where the regret budget is chosen implicitly through

specifying a combination of uninterpretable hyperparameters like 𝜆, 𝜌, 𝜌Δ or 𝛼, 𝛽, while the

same hyperparameters are also crucial for the performance. In short, our approach simplifies and

disentangles balancing the revenue-regret trade-off and optimizing the performance.

On the other hand, our approach is also significantly less sensitive to the hyperparameters

compared to the original objective of RegretNet (6), since we use the same hyperparameters in all

experiments whereas RegretNet specifies different 𝜆, 𝜌, 𝜌Δ for each setting.

In all our experiments, we train all networks with the approach described in this section, which

allows for the straightforward comparison of different architectures by evaluating their revenues

given the same regret budget.

5 EXPERIMENTS
In this section, we empirically investigate both proposed modifications of RegretNet, namely our attention-based architecture RegretFormer and our budget-based approach to training. We divide the experiments into two broad subsections.
In the first subsection denoted as ‘uni-settings‘, we compare our and existing architectures in several settings with fixed number of participants and items. These settings were used in the RegretNet paper and are standard in the subsequent the literature. In each uni-setting, we vary regret budgets, test out-of-setting generalization, and further investigate the differences between architectures using network distillation.
In the second subsection denoted as ‘multi-settings‘, we apply RegretNet and RegretFormer to the settings with a varied number of participants and items. Put simply, a multi-setting is an

10

Table 1. Neural architecture hyperparameters in uni-settings and multi-settings

Hyperparameter

1x2 2x2 2x3 2x5 3x10 small big

RegretNet

fully-connected layers 3 3 3 6 6 3

6

hidden dim

200 200 200 200 200 200 200

EquivariantNet

exchangeable layers 3 3 5 6 6 -

-

hidden dim

32 32 32 32 32 -

-

RegretFormer

exchangeable layers 1 1 1 1 1 1

1

attention layers

11122 1

2

attention heads

22244 2

4

hidden dim

32 64 64 128 128 64 128

equal mixture of several uni-settings. We first verify the ability of both architectures to learn in multi-settings when training and testing distributions coincide. Then, we examine out-of-setting generalization of the architectures by making part of the uni-settings unavailable during training.
Before presenting the results, we report the technical details of the experimental procedure. In all experiments, all networks are trained for 200000 iterations of outer optimization, each iteration corresponding to one step of the optimizer on one mini-batch. The training dataset consists of 640000 profiles (same as in the RegretNet paper) divided into 1250 mini-batches of 512 profiles. The validation dataset consists of 4096 profiles divided into 128 batches of 32 profiles. The number of inner optimization steps per one outer update equals 25 during training and 1000 during validation. The learning rate equals 0.001 for the outer optimization and 0.1 for the inner optimization. Both use separate Adam optimizers [29]. The hyperparameters related to our budget-based approach are reported in the end of the previous section. The hyperparameters related to the neural architectures are reported in Table 1. Finally, all experiments are repeated three times and the average metrics are reported.
5.1 Uni-settings
In all uni-settings, the valuations of all participants are additive and are independently drawn for each item from the Uniform distribution: 𝑣𝑖 ( 𝑗) ∈ U[0, 1]. The uni-settings only differ in the number of participants and items, so we denote the uni-settings as nxm, where n and m are respectively the number of participants and items. We conduct experiments in five uni-settings: 1x2, 2x2, 2x3, 2x5, and 3x10. The 1x2 setting is the celebrated Manelli-Vincent auction, the analytical solution for which is provided in [35]. The optimal revenue for this auction equals 0.55. For the rest of the settings, the analytical solutions are unknown.
5.1.1 Comparing architectures in-setting and varying regret budgets. In these experiments we compare three neural architectures, namely RegretNet, EquivariantNet, and RegretFormer, in the five uni-settings given three different regret budgets 𝑅𝑚𝑎𝑥 ∈ {10−3, 10−4, 10−5}.
We additionally include the classic VCG [5, 19, 59] and Myerson [40] mechanisms for comparison. The VCG mechanism coincides with the second-price auctions in our settings. Its expected revenue therefore equals the expectation of the (𝑛 − 1)-order statistic of 𝑛 draws from the valuation distribution, times 𝑚. VCG is DSIC, ex-post IR, and efficient. The Myerson auctions are optimal

𝑅𝑚𝑎𝑥 10−3

(a) revenue

(b) regret

11 (c) penalty coefficient 𝛾

𝑅𝑚𝑎𝑥 10−4
𝑅𝑚𝑎𝑥 10−5
Fig. 2. Learning curves in the uni-setting 3x10. The X-axis is in the thousands of training iterations. The shaded regions correspond to the min-max spread over three random seeds.
for 𝑚 = 1 and coincide with the second-price auctions with a specific reserve price when the participants sample from the same valuation distribution. We report two extensions of Myerson to the multi-item auctions, the ‘item-wise‘ when each item is sold separately and the ‘bundled‘ when all items are sold in a single bundle. The revenues of both variants are averaged over 10 million iid profiles. Myerson is DSIC and ex-post IR, but not efficient.
The resulting revenue and regret values are reported in Table 2. Additionally, we report the learning curves of revenue, regret, and penalty coefficient 𝛾 for the 3x10 setting in Figure 2 and for all other settings in the Appendix. Below we summarize our findings:
• In all settings and given all regret budgets, our RegretFormer architecture outperforms both RegretNet and EquivariantNet by revenue while achieving similar regret values. The performance gap is present in all settings but becomes especially prominent in the bigger settings. While the permutation-equivariance property of RegretFormer likely plays a role in these results, it cannot fully explain the results since EquivarintNet also has this property. We therefore attribute the success of RegretFormer to the expressivity of attention layers. In the next subsection we additionally verify that the performance gap is genuine and further investigate the differences between RegretFormer and RegretNet.
• In the bigger settings EquivariantNet can also outperform RegretNet. This result is somewhat surprising since it was not observed by the authors of EquivariantNet, but it can simply be

12

Table 2. Architecture comparison in different uni-settings and given different regret budgets

𝑅

setting

RegretNet

𝑚𝑎𝑥

revenue regret

EquivariantNet revenue regret

10−3 1x2 2x2 2x3 2x5 3x10

0.579 0.888 1.352 2.355 5.746

0.00123 0.00066 0.00102 0.00205 0.00292

0.576 0.883 1.404 2.428 6.057

0.0009 0.00069 0.00135 0.002 0.00305

10−4 1x2 2x2 2x3 2x5 3x10

0.55 0.849 1.265 2.235 5.016

0.00017 0.00009 0.00012 0.00034 0.00062

0.555 0.832 1.269 2.285 5.624

0.00018 0.0001 0.00023 0.00035 0.00089

10−5 1x2 2x2 2x3 2x5 3x10

0.531 0.775 1.165 1.856 4.047

0.00001 0.00004 0.00006 0.00014 0.00026

0.493 0.713 1.076 1.895 5.071

0.00005 0.00002 0.00004 0.0001 0.0003

RegretFormer revenue regret

0.6 0.917 1.416 2.489 6.141

0.00078 0.0006 0.00109 0.0018 0.00207

0.567 0.872 1.335 2.37 5.871

0.00024 0.00009 0.00015 0.00034 0.00064

0.543 0.818 1.256 2.164 5.371

0.00004 0.00002 0.00004 0.00006 0.00017

𝑅 setting

VCG

𝑚𝑎𝑥

revenue regret

0

1x2 0

0

2x2 0.667 0

2x3 1

0

2x5 1.667 0

3x10 5

0

Myerson item-wise revenue regret

0.5

0

0.833 0

1.25

0

2.083 0

5.312 0

Myerson bundled revenue regret
0.544 0 0.839 0 1.278 0 2.188 0 5.003 0

explained by the fact that they do not test the architecture in the settings that are complex enough. In fact, the most complex settings they use for direct comparison have 𝑚 = 2 and 𝑛 = 2, and we also do not observe the performance difference in such settings. Another interesting result is that given low regret budgets EquivariantNet can underperform RegretNet, especially in the smaller settings. • Overall, varying the regret budget within the same setting affects the revenue-regret trade-off as expected: decreasing 𝑅𝑚𝑎𝑥 from 10−3 to 10−4 and 10−5 respectively lowers the resulting regret by approximately one and two magnitudes. Furthermore, for each specific setting and 𝑅𝑚𝑎𝑥 we expect by design the revenue to be higher than regret by a factor of 𝑅𝑚𝑎𝑥 . We observe that often this approximately holds, but sometimes the ratio can be several times higher or lower than expected. The reason behind this is in the difference of regret computation during training and validation. Following the procedure of Dütting et al. [12], we compute 25 inner optimization steps per each outer update during training and a much higher value of 1000 steps during validation. The latter values are reported in the paper and they can both underestimate and overestimate regret compared to training due to overfitting during validation or underfitting during training, respectively. For a tighter regret approximation

13
and a better match of the revenue/regret ratio to 𝑅𝑚𝑎𝑥 , one could tune the number of inner optimization steps used during training, provided that a longer training time is acceptable. • Given the smallest regret budget, RegretNet and EquivariantNet considerably underperform both item-wise and bundled Myerson auctions. While RegretFormer performs on par with Myerson in such conditions, its regret is still non-zero. This result highlights a potential problem of the regret-based approach to the auction design: when any violations of DSIC are highly undesirable, a better revenue might be achieved with classic mechanisms that are also guaranteed to be DSIC. It should be noted that the Myerson mechanism relies on the access to the true valuation distribution and might not be applicable to arbitrary nonparametric distributions. Furthermore, RegretFormer still reliably outperforms VCG in revenue, which is more frequently used in practice than Myerson.
5.1.2 Is the performance gap genuine? In our experiments in uni-settings we observed that RegretFormer outperforms both RegretNet and EquivariantNet and attributed its success to the permutation-equivariance and the expressivity of attention layers. However, there is an alternative explanation.
Recall that the regret is approximated in the inner optimization (7) by repeatedly back-propagating through the whole neural network 𝑤. Therefore, the reported regret values explicitly depend on the neural architecture, including the number, the types, the size, and the order of layers. Due to the absence of a regret approximation procedure that is identical for all networks, there is no guarantee that similar regret estimates between different neural architectures correspond to actual similar regret values, i.e. for different architectures 𝑤1, 𝑤2 : 𝑅˜(𝑤1) ≈ 𝑅˜(𝑤2) ≠ ⇒ 𝑅(𝑤1) ≈ 𝑅(𝑤2), where 𝑅(𝑤) is the expected regret and 𝑅˜(𝑤) is its approximation. In particular, we are concerned whether RegretFormer achieves higher revenues due to approximating the regret worse, rather than due to maximizing the revenue better. We have designed two procedures to test this hypothesis.
The first procedure denoted as ‘cross-misreports‘ is based on estimating the regret of one network on the optimal misreports approximated by another network: 𝑅𝑖𝑐𝑟𝑜𝑠𝑠 (𝑤1, 𝑤2) = |𝐵1 | 𝑙 ∈𝐵 𝑟𝑔𝑡𝑖 (𝑣𝑖′𝑙 (𝑤2), 𝑣𝑙 ; 𝑤1). We apply this procedure to the trained RegretNet and RegretFormer in all five uni-settings given 𝑅𝑚𝑎𝑥 = 10−3. If the networks approximate regret equally well, we expect the regret estimated on cross-misreports to not exceed the normally computed regret. However, if the regret estimates of RegretFormer are higher on the misreports estimated by RegretNet, this would point towards poor regret approximation by RegretFormer in a given setting. We report the results in Table 3.
The second procedure denoted as ‘distillation‘ is based on training the ‘student‘ network 𝑤𝑠 to approximate the predictions of a trained ‘teacher‘ network 𝑤𝑡 . We apply this procedure in all five unisettings given 𝑅𝑚𝑎𝑥 = 10−3 to distill RegretFormer onto RegretNet, as well as to distill RegretFormer onto itself as a control experiment. The motivation is that if the architecture of RegretFormer for some reason impairs its ability to approximate optimal misreports, a RegretNet trained to closely mimic the predictions of a RegretFormer may find better misreports that produce higher regret values for the RegretFormer. Specifically, since the predictions of allocation and payment modules can respectively be treated as the categorical and the Bernoulli distributions, we train the student network to minimize the KL divergence from its predictions to the predictions of the teacher network. For example, to train the allocation module, the student minimizes 𝐾𝐿(𝑔(𝑤𝑡 ), 𝑔(𝑤𝑠 )) = |𝐵1 | 𝑙 ∈𝐵 𝑖,𝑗 𝑧𝑖 𝑗 (𝑣𝑙 ; 𝑤𝑡 ) · (log(𝑧𝑖 𝑗 (𝑣𝑙 ; 𝑤𝑡 )) − log(𝑧𝑖 𝑗 (𝑣𝑙 ; 𝑤𝑠 ))), and likewise for the payment module. This approach was initially proposed by Hinton et al. [24]. Furthermore, to satisfy DSIC on the whole support, the KL-divergence is minimized at both the true valuations 𝑣𝑙 and the approximate optimal misreports for each participant 𝑣 ′𝑙 (𝑤𝑠 ), estimated for the student network as per usual via
𝑖
inner optimization (7). We report the results in Table 4. Below we summarize our findings:

14

Table 3. Cross-misreport regret estimates

setting 1x2 2x2 2x3 2x5 3x10

misreports of RegretNet RegretFormer RegretNet RegretFormer RegretNet RegretFormer RegretNet RegretFormer RegretNet RegretFormer

regret of RegretNet RegretFormer

0.00115 (±0.00041) 0.00031 (±0.00008)

0.00188 (±0.00039) 0.00098 (±0.00037)

0.00071 (±0.00010) 0.00033 (±0.00003)

0.00070 (±0.00005) 0.00074 (±0.00009)

0.00102 (±0.00008) 0.00035 (±0.00005)

0.00099 (±0.00010) 0.00125 (±0.00008)

0.00199 (±0.00010) 0.00045 (±0.00004)

0.00228 (±0.00043) 0.00199 (±0.00073)

0.00279 (±0.00008) 0.00025 (±0.00006)

0.00191 (±0.00031) 0.00286 (±0.00017)

• In the 1x2 setting, it is indeed possible that RegretFormer underestimates the regret compared to RegretNet, and therefore that the performance gap between the networks is deceptive in this setting. In the cross-misreports experiment, the misreports of RegretNet produce the regret of RegretFormer almost 2 times higher than its own misreports. Similarly, in the distillation experiment the student RegretNet finds misreports that produce the regret of the teacher RegretFormer around 1.5 times higher than its own misreports.
• Fortunately, we do not find any evidence that RegretFormer underestimates the regret in the other settings. In the cross-misreports experiments, RegretFormer finds misreports that produce either higher or similar regret compared to the misreports of RegretNet. Likewise, in the distillation experiments the student RegretNet does not find better misreports for the teacher.
• As additional evidence towards the performance gap being genuine, in the distillation experiments the student RegretNet achieves the same revenue as the teacher while consistently producing higher regret, up to a magnitude on the hardest setting. This result combined with the previous results hint at two possible explanations. First, RegretNet gets stuck in one of multiple local optimums which prevents it from reaching lower regrets. Moreover, this consistently happens both when learning from scratch and when mimicking RegretFormer. Second, the better solutions with high revenue and low regret are simply absent from the space of the mechanisms that can be represented by RegretNet.

15

Table 4. Distillation

setting 1x2 2x2 2x3 2x5 3x10

metric
revenue regret
revenue regret
revenue regret
revenue regret
revenue regret

misreports of
teacher student
teacher student
teacher student
teacher student
teacher student

RegretFormer → RegretNet

teacher

student

0.598 0.00079 0.00117

0.598 0.00072 0.00135

0.907 0.00071 0.00062

0.908 0.00095 0.00162

1.411 0.00126 0.00073

1.414 0.00187 0.00417

2.485 0.00201 0.00161

2.487 0.00221 0.00553

6.143 0.00296 0.00109

6.158 0.00536 0.02529

RegretFormer → RegretFormer

teacher

student

0.592 0.00071 0.00062

0.592 0.00064 0.00063

0.911 0.00070 0.00039

0.910 0.00091 0.00126

1.416 0.00125 0.00054

1.417 0.00146 0.00172

2.499 0.00197 0.00095

2.502 0.00209 0.00238

6.129 0.00243 0.00117

6.131 0.00265 0.00327

• Judging by the distillation of RegretFormer onto itself, our distillation approach is successful: the revenues of the teacher and the student differ insignificantly, and while the student sometimes produces a slightly higher regret, it is of the same magnitude as the teacher’s. Furthermore, both the teacher and the student approximate the regret better in their own misreports. This result validates our main distillation experiments.
5.1.3 Testing out-of-uni-setting generalization. In this experiments we investigate how well the architectures generalize to the unseen uni-settings. It’s clear that RegretNet cannot be applied to the out-of-domain settings by design, since its layers have fixed size and cannot vary the number of participants and items. For this reason, we compare our network with EquivariantNet. The architectures were trained on five uni-settings namely: 1x2, 2x2, 2x3, 2x5, 3x10 and then were tested on all settings but the setting used for training. The resulting revenue and regret values are reported in Table 5.
The results of experiments on out-of-domain uni-setting can be summarized as follows: both approaches look promising when the number of objects varies and the number of bidders remains constant, with a slight advantage of the proposed RegretFormer. However, the generalization to the settings where the number of bidders varies is poor for both networks due to complex interactions between the participants. Similar results were observed by Rahme et al. [49].
5.2 Multi-settings
In practice, it may be desirable for a single network to be applicable to multiple settings, e.g. in order not to train a separate network for each setting to save computation or due to the small amount of data. To test the behavior of our network in such cases, we define a new type of experiments, which we call multi-setting - a setting that consists of a set of uni-settings. First, we examine the default multi-setting in which the models have access to the full set of uni-settings during training. Then,

16

Table 5. Out-of-uni-setting generalization

Training setting 1x2
2x2
2x3
2x5
3x10

Validation setting
2x2 2x3 2x5 3x10
1x2 2x3 2x5 3x10
1x2 2x2 2x5 3x10
1x2 2x2 2x3 3x10
1x2 2x2 2x3 2x5

EquivariantNet revenue regret

0.690 1.084 1.917 4.308

0.04403 0.07074 0.12465 0.29291

0.695 1.350 2.307 5.156

0.13343 0.00071 0.03169 0.55869

0.686 0.875 2.318 5.271

0.14900 0.00116 0.00615 0.37967

0.743 0.900 1.401 5.517

0.19830 0.00066 0.00103 0.24757

0.552 0.693 1.099 1.936

0.53700 0.19767 0.33665 0.59959

RegretFormer revenue regret

0.701 1.085 1.910 4.174

0.03759 0.08090 0.15761 0.33276

0.772 1.412 2.386 4.805

0.21658 0.02467 0.07099 0.26238

0.782 0.915 2.436 4.847

0.21461 0.00160 0.01610 0.29287

0.803 0.918 1.426 4.898

0.21321 0.00058 0.00097 0.25199

0.775 0.989 1.541 2.652

0.16126 0.03528 0.04724 0.06842

we experiment with out-of-setting generalization by making half of the uni-settings unavailable during training.
5.2.1 Default multi-settings. We define two types of multi-settings:
(1) Small: 𝑆𝑠𝑚𝑎𝑙𝑙 ={1x2, 2x2}. (2) Big: 𝑆𝑏𝑖𝑔={2x3, 2x4, 2x5, 2x6, 2x7, 3x3, 3x4, 3x5, 3x6, 3x7}.
In a multi-setting, all uni-settings are represented in both training and validation sets in equal proportions. We compare our network with RegretNet. The original RegretNet is not able to work with inputs of different dimensions, which is required in a multi-setting, so we adapt this network for the task as follows: the size of the input matrix is fixed at the maximum possible number of objects and participants: (max(𝑛) × max(𝑚)). For each valuation profile in the batch, we pad the matrix with zeros up to the dimension of the input. We also correct the loss calculation taking these changes into account. While RegretFormer is able to work with inputs of different dimensions, there can be profiles from different uni-settings in a batch, so we add a similar operation of padding with zeros, but only up to the dimension of the largest setting in the batch. There are no other fundamental differences with uni-setting experiments.
Table 6 presents the results of experiments in the small multi-setting. In both 1x2 and 2x2 setting, our RegretFormer shows a significantly higher revenue than RegretNet, while the regret values of the networks are within standard deviations. Interestingly, both networks produce slightly higher revenue and regret values in the 2x2 setting compared to the uni-setting experiment (Table 2).

17

Table 6. Small multi-setting

Setting 1x2 2x2

RegretNet revenue regret

0.574 (±0.001) 0.902 (±0.002)

0.0011 (±0.0003) 0.0009 (±0.00008)

RegretFormer revenue regret

0.603 (±0.012) 0.928 (±0.002)

0.00147 (±0.00043) 0.00094 (±0.00007)

Table 7. Big multi-setting

Setting average 2x3 2x4 2x5 2x6 2x7 3x3 3x4 3x5 3x6 3x7

RegretNet

revenue regret

2.573

0.00321

1.386 (±0.008) 1.855 (±0.009) 2.339 (±0.006) 2.867 (±0.0148) 3.346 (±0.024) 1.652 (±0.0122) 2.218 (±0.009) 2.785 (±0.003) 3.362 (±0.011) 3.921 (±0.003)

0.00184 (±0.00023) 0.00245 (±0.00023) 0.00313 (±0.00065) 0.00447 (±0.00032) 0.00532 (±0.00027) 0.00185 (±0.00033) 0.00214 (±0.00034) 0.00263 (±0.00035) 0.0036 (±0.00021) 0.00466 (±0.00037)

RegretFormer revenue regret 2.703 0.00327

1.437 (±0.008) 1.952 (±0.01) 2.484 (±0.02) 3.016 (±0.018) 3.57 (±0.02) 1.704 (±0.019) 2.296 (±0.022) 2.912 (±0.027) 3.525 (±0.027) 4.133 (±0.034)

0.00227 (±0.00008) 0.00282 (±0.00008) 0.0034 (±0.00017) 0.0039 (±0.00021) 0.0043 (±0.00031) 0.0021 (±0.000211) 0.0027 (±0.00021) 0.0032 (±0.00024) 0.0039 (±0.00037) 0.0042 (±0.00044)

Table 7 presents the results of experiments in the big multi-setting. While both networks perform well, RegretFormer outperforms RegretNet in terms of revenue in all uni-settings. The regret values demonstrate the following dynamics: on uni-settings with small 𝑚, RegretNet demonstrates better regret, but as 𝑚 increases, the regret becomes either indistinguishable or favourable for RegretFormer. If we average over all settings, the regrets are practically identical. Interestingly, once again we notice the pattern that the regret values are higher for both networks when trained in the multi-setting regime than in the respective 2x3 and 2x5 uni-setting experiments (Table 2). We hypothesize that the regret estimation is a more difficult task in the multi-setting regime, and consequently that the approximation is not as tight after the 25 inner optimization steps used in training.

18

Table 8. Out-of-multi-setting generalization

Setting
2x4 2x6 3x3 3x5 3x7

RegretNet revenue regret

2.115 3.166 1.743 2.918 3.722

0.083 0.078 0.010 0.016 0.144

RegretFormer revenue regret

1.932 2.981 1.697 2.866 4.030

0.003 0.004 0.003 0.003 0.008

5.2.2 Out-of-multi-settings generalization. We define two uni-setting subsets:
(1) Train: 𝑆𝑡𝑟𝑎𝑖𝑛={2x3, 2x5, 2x7, 3x4, 3x6}. (2) Test: 𝑆𝑡𝑒𝑠𝑡 ={2x4, 2x6, 3x3, 3x5, 3x7}.
In this section, we compare how networks generalize to unseen uni-settings when trained in the multi-setting regime. To this end, we train both RegretNet and RegretFormer on the 𝑆𝑡𝑟𝑎𝑖𝑛 subset and then validate them on the 𝑆𝑡𝑒𝑠𝑡 subset. The training setup is otherwise the same as in 5.2.1.
The resulting revenue and regret values are reported in Table 8. In all out-of-domain settings RegretNet produced poor results. Its regret is more than an order of magnitude larger compared to RegretFormer. In contrast, our approach stably generalizes to all unseen settings while keeping the regret low. Furthermore, its revenue is high and is close to the results obtained in the default multi-settings. The difference from RegretNet is especially prominent in the 3x7 setting where our network achieves a larger revenue while prodicing 18 times as little regret.
6 CONCLUSION
In this study, we reach the new state-of-the-art performance in the application of machine learning to the problem of optimal auction design. Taking into account the specifics of the problem and the issues of the previous solutions, we develop the RegretFormer architecture. It leverages the recent advances in deep learning to unlock the full potential of the regret-based optimization while enforcing the equivariance and the invariance to the order of participants and items. We extensively test the effectiveness of RegretFormer in multiple experimental settings, including uni-settings and multi-settings, as well as out-of-setting experiments. We find that our network consistently outperforms the existing analogues in revenue given the same regret budgets.
In addition, we rethink the objective formulation of RegretNet. The resulting loss function allows us to disentangle balancing the revenue-regret trade-off and optimizing the performance. At the same time, it reduces the burden of hyperparameter tuning and simplifies the application of the networks to new auctions.
Finally, we suggest several novel experimental procedures, including multi-settings, as well as our cross-misreports and distillation validation procedures. We hope these procedures will find their use in future studies to test the effectiveness of new architectures.
REFERENCES
[1] Saeed Alaei, Jason Hartline, Rad Niazadeh, Emmanouil Pountourakis, and Yang Yuan. 2019. Optimal auctions vs. anonymous pricing. Games and Economic Behavior 118 (2019), 494–510.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).
[3] Maria-Florina F Balcan, Tuomas Sandholm, and Ellen Vitercik. 2016. Sample complexity of automated mechanism design. Advances in Neural Information Processing Systems 29 (2016).

19
[4] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. 2004. Convex optimization. Cambridge university press. [5] Edward H Clarke. 1971. Multipart pricing of public goods. Public choice (1971), 17–33. [6] Richard Cole and Tim Roughgarden. 2014. The sample complexity of revenue maximization. In Proceedings of the
forty-sixth annual ACM symposium on Theory of computing. 243–252. [7] Vincent Conitzer and Tuomas Sandholm. 2002. Complexity of mechanism design. In Proceedings of the Eighteenth
conference on Uncertainty in artificial intelligence. 103–110. [8] Vincent Conitzer and Tuomas Sandholm. 2004. Self-interested automated mechanism design and implications for
optimal combinatorial auctions. In Proceedings of the 5th ACM Conference on Electronic Commerce. 132–141. [9] Constantinos Daskalakis and Seth Matthew Weinberg. 2012. Symmetries and optimal multi-dimensional mechanism
design. In Proceedings of the 13th ACM conference on Electronic commerce. 370–387. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. (jun 2019), 4171–4186. [11] Peerapong Dhangwatnotai, Tim Roughgarden, and Qiqi Yan. 2015. Revenue maximization with a single sample. Games
and Economic Behavior 91 (2015), 318–333. [12] Paul Dütting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath. 2019. Optimal auctions
through deep learning. In International Conference on Machine Learning. PMLR, 1706–1715. [13] Paul Dütting, Felix Fischer, Pichayut Jirapinyo, John K Lai, Benjamin Lubin, and David C Parkes. 2015. Payment rules
through discriminant-based classifiers. ACM Transactions on Economics and Computation 3, 1 (2015). [14] Zhe Feng, Harikrishna Narasimhan, and David C Parkes. 2018. Deep learning for revenue-optimal auctions with
budgets. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems. 354–362. [15] Noah Golowich, Harikrishna Narasimhan, and David C Parkes. 2018. Deep Learning for Multi-Facility Location
Mechanism Design.. In IJCAI. 261–267. [16] Yannai A Gonczarowski and Noam Nisan. 2017. Efficient empirical revenue maximization in single-parameter auction
environments. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing. 856–868. [17] Yannai A Gonczarowski and S Matthew Weinberg. 2021. The sample complexity of up-to-𝜀 multi-dimensional revenue
maximization. Journal of the ACM (JACM) 68, 3 (2021), 1–28. [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). [19] Theodore Groves. 1973. Incentives in teams. Econometrica: Journal of the Econometric Society (1973), 617–631. [20] Chenghao Guo, Zhiyi Huang, and Xinzhi Zhang. 2019. Settling the sample complexity of single-parameter revenue
maximization. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. 662–673. [21] Mingyu Guo and Vincent Conitzer. 2010. Computationally feasible automated mechanism design: General approach
and case studies. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 24. 1676–1679. [22] Jason Hartline and Samuel Taggart. 2019. Sample complexity for non-truthful mechanisms. In Proceedings of the 2019
ACM Conference on Economics and Computation. 399–416. [23] Jason D Hartline and Tim Roughgarden. 2009. Simple versus optimal mechanisms. In Proceedings of the 10th ACM
conference on Electronic commerce. 225–234. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 2, 7 (2015). [25] Zhiyi Huang, Yishay Mansour, and Tim Roughgarden. 2018. Making the most of your samples. SIAM J. Comput. 47, 3
(2018), 651–674. [26] Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. 2018. Real-time bidding with multi-agent
reinforcement learning in display advertising. In Proceedings of the 27th ACM international conference on information and knowledge management. 2193–2201. [27] Yaonan Jin, Pinyan Lu, Qi Qi, Zhihao Gavin Tang, and Tao Xiao. 2019. Tight approximation ratio of anonymous pricing. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. 674–685. [28] Yaonan Jin, Pinyan Lu, Zhihao Gavin Tang, and Tao Xiao. 2020. Tight revenue gaps among simple mechanisms. SIAM J. Comput. 49, 5 (2020), 927–958. [29] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR (Poster). [30] Pravesh Kothari, Sahil Singla, Divyarthi Mohan, Ariel Schvartzman, and S Matthew Weinberg. 2019. Approximation schemes for a unit-demand buyer with independent items via symmetries. In 2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 220–232. [31] Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J Curry, Samuel Dooley, Ping-yeh Chiang, Tom Goldstein, and John P Dickerson. 2020. Proportionnet: Balancing fairness and revenue for auction design with deep learning. arXiv preprint arXiv:2010.06398 (2020). [32] Sébastien Lahaie. 2011. A kernel-based iterative combinatorial auction. In Twenty-Fifth AAAI Conference on Artificial Intelligence.

20
[33] Anton Likhodedov, Tuomas Sandholm, et al. 2005. Approximating revenue-maximizing combinatorial auctions. In AAAI, Vol. 5. 267–274.
[34] Xiangyu Liu, Chuan Yu, Zhilin Zhang, Zhenzhe Zheng, Yu Rong, Hongtao Lv, Da Huo, Yiqing Wang, Dagui Chen, Jian Xu, et al. 2021. Neural auction: End-to-end learning of auction mechanisms for e-commerce advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3354–3364.
[35] Alejandro M Manelli and Daniel R Vincent. 2006. Bundling as an optimal selling mechanism for a multiple-good monopolist. Journal of Economic Theory 127, 1 (2006), 1–35.
[36] Padala Manisha, CV Jawahar, and Sujit Gujar. 2018. Learning Optimal Redistribution Mechanisms Through Neural Networks. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. 345–353.
[37] Mehryar Mohri and Andrés Munoz Medina. 2016. Learning algorithms for second-price auctions with reserve. The Journal of Machine Learning Research 17, 1 (2016), 2632–2656.
[38] Jamie Morgenstern and Tim Roughgarden. 2016. Learning simple auctions. In Conference on Learning Theory. PMLR, 1298–1318.
[39] Jamie H Morgenstern and Tim Roughgarden. 2015. On the pseudo-dimension of nearly optimal auctions. Advances in Neural Information Processing Systems 28 (2015).
[40] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations research 6, 1 (1981), 58–73. [41] Harikrishna Narasimhan, Shivani Brinda Agarwal, and David C Parkes. 2016. Automated mechanism design without
money via machine learning. In Proceedings of the 25th International Joint Conference on Artificial Intelligence. [42] Harikrishna Narasimhan and David C Parkes. 2016. A general statistical framework for designing strategy-proof
assignment mechanisms. In UAI’16 Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. [43] Thomas Nedelec, Jules Baudet, Vianney Perchet, and Noureddine El Karoui. 2021. Adversarial learning for revenue-
maximizing auctions. In 20th International Conference on Autonomous Agents and Multiagent Systems. [44] Thomas Nedelec, Noureddine El Karoui, and Vianney Perchet. 2019. Learning to bid in revenue-maximizing auctions.
In International Conference on Machine Learning. PMLR, 4781–4789. [45] Kim Thang Nguyen. 2020. A Bandit Learning Algorithm and Applications to Auction Design. Advances in Neural
Information Processing Systems 33 (2020), 12070–12079. [46] Gregory Pavlov. 2011. Optimal mechanism for selling two goods. The BE Journal of Theoretical Economics 11, 1 (2011). [47] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. 2018. Variational Discriminator
Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow. In International Conference on Learning Representations. [48] Neehar Peri, Michael Curry, Samuel Dooley, and John Dickerson. 2021. PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning. Advances in Neural Information Processing Systems 34 (2021). [49] Jad Rahme, Samy Jelassi, Joan Bruna, and S. Matthew Weinberg. 2021. A Permutation-Equivariant Neural Network Architecture For Auction Design. Proceedings of the AAAI Conference on Artificial Intelligence 35, 6 (May 2021), 5664–5672. https://ojs.aaai.org/index.php/AAAI/article/view/16711 [50] Jad Rahme, Samy Jelassi, and S. Matthew Weinberg. 2021. Auction Learning as a Two-Player Game. In International Conference on Learning Representations. https://openreview.net/forum?id=YHdeAO61l6T [51] Sai Srivatsa Ravindranath, Zhe Feng, Shira Li, Jonathan Ma, Scott D Kominers, and David C Parkes. 2021. Deep Learning for Two-Sided Matching. arXiv preprint arXiv:2107.03427 (2021). [52] Tuomas Sandholm. 2003. Automated mechanism design: A new application area for search algorithms. In International Conference on Principles and Practice of Constraint Programming. Springer, 19–36. [53] Tuomas Sandholm and Anton Likhodedov. 2015. Automated design of revenue-maximizing combinatorial auctions. Operations Research 63, 5 (2015), 1000–1025. [54] Weiran Shen, Pingzhong Tang, and Song Zuo. 2019. Automated Mechanism Design via Neural Networks. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. 215–223. [55] Vasilis Syrgkanis. 2017. A sample complexity measure with applications to learning optimal auctions. Advances in Neural Information Processing Systems 30 (2017). [56] Andrea Tacchetti, DJ Strouse, Marta Garnelo, Thore Graepel, and Yoram Bachrach. 2019. A Neural Architecture for Designing Truthful and Efficient Auctions. CoRR abs/1907.05181 (2019). http://arxiv.org/abs/1907.05181 [57] Pingzhong Tang. 2017. Reinforcement mechanism design.. In IJCAI. 5146–5150. [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [59] William Vickrey. 1961. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance 16, 1 (1961), 8–37. [60] Jakob Weissteiner and Sven Seuken. 2020. Deep Learning—Powered Iterative Combinatorial Auctions. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 2284–2293.

21
[61] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. 2021. Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet. (2021), 558–567.
[62] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. Deep sets. Advances in neural information processing systems 30 (2017).
A APPENDIX We present the learning curves of the revenue, the regret, and the penalty coefficient 𝛾 for the 1x2, 2x2, 2x3, and 2x5 uni-settings in Figures 3, 4, 5, and 6, respectively. A similar figure is reported for the 3x10 setting in the main text (Figure 2).

𝑅𝑚𝑎𝑥 10−3

(a) revenue

(b) regret

(c) penalty coefficient 𝛾

𝑅𝑚𝑎𝑥 10−4
𝑅𝑚𝑎𝑥 10−5

Fig. 3. Learning curves in the uni-setting 1x2

𝑅𝑚𝑎𝑥 10−3

(a) revenue

(b) regret

22 (c) penalty coefficient 𝛾

𝑅𝑚𝑎𝑥 10−4
𝑅𝑚𝑎𝑥 10−5

Fig. 4. Learning curves in the uni-setting 2x2

𝑅𝑚𝑎𝑥 10−3

(a) revenue

(b) regret

23 (c) penalty coefficient 𝛾

𝑅𝑚𝑎𝑥 10−4
𝑅𝑚𝑎𝑥 10−5

Fig. 5. Learning curves in the uni-setting 2x3

𝑅𝑚𝑎𝑥 10−3

(a) revenue

(b) regret

24 (c) penalty coefficient 𝛾

𝑅𝑚𝑎𝑥 10−4
𝑅𝑚𝑎𝑥 10−5

Fig. 6. Learning curves in the uni-setting 2x5

