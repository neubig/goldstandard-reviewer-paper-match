SIMULTANEOUS SPEECH RECOGNITION AND SPEAKER DIARIZATION FOR MONAURAL DIALOGUE RECORDINGS WITH TARGET-SPEAKER ACOUSTIC MODELS
Naoyuki Kanda1, Shota Horiguchi1, Yusuke Fujita1, Yawen Xue1, Kenji Nagamatsu1, Shinji Watanabe2
1Hitachi, Ltd., Japan 2Johns Hopkins University, USA

arXiv:1909.08103v1 [cs.CL] 17 Sep 2019

ABSTRACT
This paper investigates the use of target-speaker automatic speech recognition (TS-ASR) for simultaneous speech recognition and speaker diarization of single-channel dialogue recordings. TS-ASR is a technique to automatically extract and recognize only the speech of a target speaker given a short sample utterance of that speaker. One obvious drawback of TS-ASR is that it cannot be used when the speakers in the recordings are unknown because it requires a sample of the target speakers in advance of decoding. To remove this limitation, we propose an iterative method, in which (i) the estimation of speaker embeddings and (ii) TS-ASR based on the estimated speaker embeddings are alternately executed. We evaluated the proposed method by using very challenging dialogue recordings in which the speaker overlap ratio was over 20%. We conﬁrmed that the proposed method signiﬁcantly reduced both the word error rate (WER) and diarization error rate (DER). Our proposed method combined with i-vector speaker embeddings ultimately achieved a WER that differed by only 2.1 % from that of TS-ASR given oracle speaker embeddings. Furthermore, our method can solve speaker diarization simultaneously as a by-product and achieved better DER than that of the conventional clustering-based speaker diarization method based on i-vector.
Index Terms— multi-talker speech recognition, speaker diarization, deep learning
1. INTRODUCTION
Our main goal is to develop a monaural conversation transcription system that can not only perform automatic speech recognition (ASR) of multiple talkers but also determine who spoke the utterance when, known as speaker diarization [1, 2]. For both ASR and speaker diarization, the main difﬁculty comes from speaker overlaps. For example, a speaker-overlap ratio of about 15% was reported in real meeting recordings [3]. For such overlapped speech, neither conventional ASR nor speaker diarization provides a result with sufﬁcient accuracy. It is known that mixing two speech signiﬁcantly degrades ASR accuracy [4–6]. In addition, no speaker overlaps are assumed with most conventional speaker diarization techniques, such as clustering of speech partitions (e.g. [1, 7–10]), which works only if there are no speaker overlaps. Due to these difﬁculties, it is still very challenging to perform ASR and speaker diarization for monaural recordings of conversation.
One solution to the speaker-overlap problem is applying a speech-separation method such as deep clustering [11] or deep attractor network [12]. However, a major drawback of such a method is that the training criteria for speech separation do not necessarily maximize the accuracy of the ﬁnal target tasks. For example, if the

goal is ASR, it will be better to use training criteria that directly maximize ASR accuracy.
In one line of research using ASR-based training criteria, multispeaker ASR based on permutation invariant training (PIT) has been proposed [4, 13–16]. With PIT, the label-permutation problem is solved by considering all possible permutations when calculating the loss function [17]. PIT was ﬁrst proposed for speech separation [17] and soon extended to ASR loss with promising results [4, 13–16]. However, a PIT-ASR model produces transcriptions for each utterance of speakers in an unordered manner, and it is no longer straightforward to solve speaker permutations across utterances. To make things worse, a PIT model trained with ASR-based loss normally does not produce separated speech waveforms, which makes speaker tracing more difﬁcult.
In another line of research, target-speaker (TS) ASR, which automatically extracts and transcribes only the target speaker’s utterances given a short sample of that speaker’s speech, has been proposed [5, 18]. Zˇ mol´ıkova´ et al. proposed a target-speaker neural beamformer that extracts a target speaker’s utterances given a short sample of that speaker’s speech [18]. This model was recently extended to handle ASR-based loss to maximize ASR accuracy with promising results [5]. TS-ASR can naturally solve the speaker-permutation problem across utterances. Importantly, if we can execute TS-ASR for each speaker correctly, speaker diarization is solved at the same time just by extracting the start and end time information of the TS-ASR result. However, one obvious drawback of TS-ASR is that it cannot be applied when the speakers in the recordings are unknown because it requires a sample of the target speakers in advance of decoding.
Based on this background, we propose a speech recognition and speaker diarization method that is based on TS-ASR but can be applied without knowing the speaker information in advance. To remove the limitation of TS-ASR, we propose an iterative method, in which (i) the estimation of target-speaker embeddings and (ii) TSASR based on the estimated embeddings are alternately executed. As an initial trial, we evaluated the proposed method by using real dialogue recordings in the Corpus of Spontaneous Japanese (CSJ). Although it contains the speech of only two speakers, the speakeroverlap ratio of the dialogue speech is very high; 20.1% . Thus, this is very challenging even for state-of-the-art ASR and speaker diarization. We show that the proposed method effectively reduced both word error rate (WER) and diarizaton error rate (DER).
2. SIMULTANEOUS ASR AND SPEAKER DIARIZATION
In this section, we ﬁrst explain the problem we targeted then the proposed method with reference to Figure 1.

Recording Segments of Recording
Observation
Speaker 1 Speaker 2

(1) Split
(2) Feature Extraction
(3) Speech Recognition & Speaker Diarization
Non-silence Hypothesis for Speaker 1 Non-silence Hypothesis for Speaker 2 Silence Hypothesis

Hypotheses-dependent Speaker Embedding Extraction

Single-speaker Region
Embedding Extraction

Single-speaker Region
Embedding Extraction

Target-Speaker ASR TS-ASR

Iteration

Speaker Embeddings

Recognition Hypotheses

TS-ASR

Fig. 1. (left) Overview of simultaneous speech recognition and speaker diarization, (right) proposed iterative maximization method.

2.1. Problem statement

The overview of the problem is shown in Figure 1 (left). We assume a sequence of observations X = {X1, ..., XU }, where U is the number of observations, and Xu is the u-th observation consisting of a sequence of acoustic features. Such a sequence is naturally generated when we separate a long recording into small segments based on voice activity detection which is a basic preprocess for ASR so as not to generate overly large lattices. We also assume a tuple of word hypotheses Wu = (W1,u, ..., WJ,u) for an observation Xu where J is the number of speakers, and Wj,u represents the speechrecognition hypothesis of the j-th speaker given observation Xu. We assume Wj,u contains not only word sequences but also their corresponding frame-level time alignments of phonemes and silences. Finally, we assume a tuple of speaker embeddings E = (e1, ..., eJ ), where ej ∈ Rd represents the d-dim speaker embedding of the j-th speaker.
Then, our objective is to ﬁnd the best possible W = {W1, ..., WU } given a sequence of observations X as follows.

W˜ = arg max P (W|X )

(1)

W

= arg max{ P (W, E|X )}

(2)

W

E

arg max{max P (W, E|X )}

(3)

W

E

Here, the starting point is the conventional maximum a posterioribased decoding given X but for multiple speakers. We then introduce the speaker embeddings E as a hidden variable (Eq. 2). Finally, we approximate the summation by using a max operation (Eq. 3).
Our motivation to introduce E, which is constant across all observation indices u, is to explicitly enforce the order of speakers in W to be constant over indices u. It should be emphasized that if we can solve the problem, speaker diarization is solved at the same time just by extracting the start and end time information of each hypothesis in W. Also note that there are J! possible solutions by swapping the order of speakers in E, and it is sufﬁcient to ﬁnd just one such solution.

maximizes P (W, E|X ). We then ﬁx E and ﬁnd W that maximizes P (W, E|X ). By iterating this procedure, P (W, E|X ) can be in-
creased monotonically. Note that it can be said by a simple application of the chain rule that ﬁnding E that maximizes P (W, E|X ) with a ﬁxed W is equivalent to ﬁnding E that maximizes P (E|W, X ). The same thing can be said for the estimation of W with a ﬁxed E.
For the (i)-th iteration of the maximization (i ∈ Z≥0), we ﬁrst ﬁnd the most plausible estimation of E given the (i − 1)-th speechrecognition hypothesis W˜ (i−1) as follows.

 arg

max

P

(E

|W˜

(i−1) ,

X

)

(i ≥ 1)



E˜(i) =

E

(4)

arg max P (E|X )

(i = 0)

E

Here, the estimation of E˜(i) is dependent on W˜ (i−1) for i ≥ 1. As-
sume that the overlapped speech corresponds to a “third person” who
is different from any person in the recording, Eq. 4 can be achieved
by estimating the speaker embeddings only from non-overlapped re-
gions (upper part of Figure 1 (right)). In this study, we used i-vector
[19] as the representation of speaker embeddings, and estimated ivector based only on the non-overlapped region given W˜ (i−1) for each speaker1. Note that, since we do not have an estimation of W for the ﬁrst iteration, E˜(0) is initialized only by X . In this study, we
estimated the i-vector for each speaker given the speech region that
was estimated by the clustering-based speaker diarization method.
More precicely, we estimated the i-vector for each Xu then applied J-cluster K-means clustering. The center of each cluster2 was used for the initial speaker embeddings E˜(0).
We then update W given speaker embeddings E˜(i).

W˜ (i) = arg max P (W|E˜(i), X )

(5)

W

arg max P (Wu|E˜(i), Xu)

(6)

W1,...,WU u

arg max

P (Wj,u|e˜(ji), Xu)

(7)

W1,...,WU u j

2.2. Iterative maximization
It is not easy to directly solve P (W, E|X ), so we propose to alternately maximize W and E. Namely, we ﬁrst ﬁx W and ﬁnd E that

1The idea to extract speaker embeddings from non-overlapped regions has been proposed (e.g. [20, 21])
2Using cluster centers does not strictly follow Eq. 4, but we used them for the procedural simplicity.

Here, we estimate the most plausible hypotheses W given estimated embeddings E˜(i) and observation X (Eq. 5). We then assume the
conditional independence of Wu given Xu for each segment u (Eq. 6). Finally, we further assume the conditional independence of Wj,u given e˜(ji) for each speaker j (Eq. 7). The ﬁnal equation can be solved by applying TS-ASR for each segment u for each speaker j
(lower part of Figure 1 (right)). We will review the detail of TS-ASR
in the next section.

3. TS-ASR: REVIEW

3.1. Overview of TS-ASR

TS-ASR is a technique to extract and recognize only the speech of a target speaker given a short sample utterance of that speaker [5, 18, 22]. Originally, the sample utterance was fed into a special neural network that outputs an averaged embedding to control the weighting of speaker-dependent blocks of the acoustic model (AM). However, to make the problem simpler, we assume that a ddimensional speaker embedding etgt ∈ Rd is extracted from the sample utterance. In this context, TS-ASR can be expressed as the problem to ﬁnd the best hypothesis Wtgt given observation X and speaker embedding etgt as follows.

W˜ tgt = arg max P (Wtgt|etgt, X)

(8)

Wtgt

If we have a well-trained TS-ASR, Eq. 7 can be solved by simply applying the TS-ASR for each segment u for each speaker j.

3.2. TS-AM with auxiliary output network
3.2.1. Overview
Although any speech recognition architecture can be used for TSASR, we adopted a variant of the TS-AM that was recently proposed and has promising accuracy [6]. Figure 2 describes the TS-AM that we applied for this study. This model has two input branches. One branch accepts acoustic features X as a normal AM while the other branch accepts an embedding etgt that represents the characteristics of the target speaker. In this study, we used a log Mel-ﬁlterbank (FBANK) and i-vector [19, 23] for the acoustic features and targetspeaker embedding, respectively.
A unique component of the model is in its output branch. The model has multiple output branches that produce outputs Ytgt and Yint for the loss functions for the target and interference speakers, respectively. The loss for the target speaker is deﬁned to maximize the target-speaker ASR accuracy, while the loss for interference speakers is deﬁned to maximize the interference-speaker ASR accuracy. We used lattice-free maximum mutual information (LFMMI) [24] for both criteria.
The original motivation of the output branch for interference speakers was the improvement of TS-ASR by achieving a better representation for speaker separation in the shared layers. However, it was also shown that the output branch for interference speakers can be used for the secondary ASR for interference speakers given the embedding of the target speaker [6]. In this paper, we found out that the latter property worked very well for the ASR for dialogue recordings, which will be explained in the evaluation section.
The network is trained with a mixture of multi-speaker speech given their transcriptions. We assume that, for each training sample, (a) transcriptions of at least two speakers are given, (b) the transcription for the target speaker is marked so that we can identify the target

Target Speaker’s Numerator Graph
LF-MMI loss (Main loss)

Interference Speaker’s Numerator Graph
LF-MMI loss (Auxiliary loss)

Linear Output

Linear Output

1024 LSTM

1024 LSTM

512 ReLU

-3

3

512 ReLU

-3

3

1024 LSTM

512 ReLU

-3

3

512 ReLU

-3

3

1024 LSTM

512 ReLU
Target Speaker Embedding

512 ReLU

-1

1

512 ReLU

-1

1

512 ReLU

3x3, 100 Conv

-1

1

3x3, 200 Conv

CNN block TDNN block LSTM block Linear block

456
Acoustic Feature

Fig. 2. Overview of target-speaker AM architecture with auxiliary interference speaker loss [6]. A number with an arrow indicates a time splicing index, which forms the basis of a time-delay neural network (TDNN) [25]. The input features were advanced by ﬁve frames, which has the same effect as reference label delay.

speaker’s transcription, and (c) a sample for the target speaker can be used to extract speaker embeddings. These assumptions can be easily satisﬁed by artiﬁcially generating training data by mixing the speech of multiple speakers.

3.2.2. Loss function The main loss function for the target speaker is deﬁned as

F tgt = LFMMI(Yutgt; Gutgt),

(9)

u

=

−P (S|Yutgt, Gutgt) log P (S|Yutgt, GD), (10)

uS

where u corresponds to the index of training samples in this case. The term Gutgt indicates a numerator (or reference) graph that represents a set of possible correct state sequences for the utterance of the target speaker of the u-th training sample, S denotes a hypothesis state sequence for the u-th training sample, and GD denotes a denominator graph, which represents a possible hypothesis space and normally consists of a 4-gram phone language model in LF-MMI training [24].
The auxiliary interference speaker loss is then deﬁned to maximize the interference-speaker ASR accuracy, which we expect to enhance the speaker separation ability of the neural network. This loss is deﬁned as

F int = LFMMI(Yuint; Guint),

(11)

u

where Guint denotes a numerator (or reference) graph that represents a set of possible correct state sequences for the utterance of the in-
terference speaker of the u-th training sample. Finally, the loss function F comb for training is deﬁned as the
combination of the target and interference losses,

F comb = F tgt + α · F int,

(12)

where α is the scaling factor for the auxiliary loss. In our evaluation, we set α = 1.0. Setting α = 0.0, however, corresponds to normal TS-ASR.

4. EVALUATION
4.1. Experimental settings
4.1.1. Main evaluation data: real dialogue recordings
We conducted our experiments on the CSJ [26], which is one of the most widely used evaluation sets for Japanese speech recognition. The CSJ consists of more than 600 hrs of Japanese recordings.
While most of the content is lecture recordings by a single speaker, CSJ also contains 11.5 hrs of 54 dialogue recordings3 (average 12.8 min per recording) with two speakers, which were the main target of ASR and speaker diarization in this study. During the dialogue recordings, two speakers sat in two adjacent sound proof chambers divided by a glass window. They could talk with each other over voice connection through a headset for each speaker. Therefore, speech was recorded separately for each speaker, and we generated mixed monaural recordings by mixing the corresponding speeches of two speakers. When mixing two recordings, we did not apply any normalization of speech volume. Due to this recording procedure, we were able to use non-overlapped speech to evaluate the oracle WERs.
It should be noted that, although the dialogue consisted of only two speakers, the speaker overlap ratio of the recordings was very high due to many backchannels and natural turn-taking. Among all recordings, 16.7% of the region was overlapped speech while 66.4% was spoken by a single speaker. The remaining 16.9% was silence. Therefore, 20.1% (=16.7/(16.7+66.4)) of speech regions was speaker overlap. From the viewpoint of ASR, 33.5% (= (16.7*2)/(16.7*2+66.4)) of the total duration to be recognized was overlapped. These values were even higher than those reported for meetings with more than two speakers [3, 27]. Therefore, these dialogue recordings are very challenging for both ASR and speaker diarization. We observed signiﬁcantly high WER and DER, which is discussed in the next section.

4.1.2. Sub evaluation data: simulated 2-speaker mixture
To evaluate TS-ASR, we also used the simulated 2-speaker-mixed data by mixing the three ofﬁcial single-speaker evaluation sets of CSJ, i.e., E1, E2, and E3 [28]. Each set includes different groups of 10 lectures (5.6 hrs, 30 lectures in total). The E1 set consists of 10 lectures of 10 male speakers, and E2 and E3 each consists of 10 lectures of 5 female and 5 male speakers. We generate two-speaker mixed speech by adding randomly selected speech (= interferencespeaker speech) to the original speech (= target-speaker speech) with the constraint that the target and interference speakers were different, and each interference speaker was selected only once from the dataset. When we mixed the two speeches, we conﬁgured them to
3We excluded 4 out of 58 dialogue recordings that have speaker duplication with the ofﬁcial E1, E2, and E3 evaluation sets.

have the same power level, and shorter speech was mixed with the longer speech from a random starting point selected to ensure the end point of the shorter one did not exceed that of the longer one.
4.1.3. Training data and training settings
The rest of the 571 hrs of 3,207 lecture recordings (excluding the same speaker’s lectures in the evaluation sets) were used for AM and language model (LM) training. We generated two-speaker mixed speech for training data in accordance with the following protocol.
1. Prepare a list of speech samples (= main list). 2. Shufﬂe the main list to create a second list under the con-
straint that the same speaker does not appear in the same line in the main and second lists. 3. Mix the audio in the main and second lists one-by-one with a speciﬁc signal-to-interference ratio (SIR). For training data, we randomly sampled an SIR as follows.
• In 1/3 probability, sample the SIR from a uniform distribution between -10 and 10 dB.
• In 1/3 probability, sample the SIR from a uniform distribution between 10 and 60 dB. The transcription of the interference speaker was set to null.
• In 1/3 probability, sample the SIR from a uniform distribution between -60 and -10 dB. The transcription of the target speaker was set to null.
4. The volume of each mixed speech was randomly changed to enhance robustness against volume difference.
A speech for extracting a speaker embedding was also randomly selected for each speech mixture from the main list. Note that the random perturbation of volume was applied only for the training data, not for evaluation data.
We trained a TS-AM consisting of a convolutional neural network (CNN), time-delay NN (TDNN) [29], and long short-term memory (LSTM) [30], as shown in Fig. 2. The input acoustic feature for the network was a 40-dimensional FBANK without normalization. A 100-dimensional i-vector was also extracted and used for the target-speaker embedding to indicate the target speaker. For extracting this i-vector, we randomly selected an utterance of the same speaker. We conducted 8 epochs of training on the basis of LF-MMI, where the initial learning rate was set to 0.001 and exponentially decayed to 0.0001 by the end of the training. We applied l2-regularization and CE-regularization [24] with scales of 0.00005 and 0.1, respectively. The leaky hidden Markov model coefﬁcient was set to 0.1. A backstitch technique [31] with a backstitch scale of 1.0 and backstitch interval of 4 was also used.
For comparison, we trained another TS-AM without the auxiliary loss. We also trained a “clean AM” using clean, non-speakermixed speech. For this clean model, we used a model architecture without the auxiliary output branch, and an i-vector was extracted every 100 msec for online speaker/environment adaptation.
In decoding, we used a 4-gram LM trained using the transcription of the training data. All our experiments were conducted on the basis of the Kaldi toolkit [32].
4.2. Preliminary experiment with simulated 2-speaker mixture
4.2.1. Evaluation of TS-ASR
We ﬁrst evaluated the TS-AM with two-speaker mixture of the E1, E2, and E3 evaluation sets. For each test utterance, a sample of the target speaker was randomly selected from the other utterances in

Table 1. WERs (%) for two-speaker-mixed evaluation sets of CSJ.

Model

Evaluation Data E1

E2

E3 Avg.

Clean AM

1-spk.

8.94 7.31 7.44 7.90

Clean AM

2-spk. mixed 87.60 85.44 91.05 88.03

TS-AM (α = 0.0) 2-spk. mixed 26.01 18.16 18.16 20.78

TS-AM (α = 1.0) 2-spk. mixed 25.68 17.94 17.96 20.53

Table 2. WERs (%) for two-speaker-mixed evaluation sets of CSJ.

Main output branch was used for target-speaker ASR and auxiliary

output branch was used for interference-speaker ASR.

Test set

Target spk. Interference spk.

E1 (10 male)

25.68

26.91

E2 (5 female, 5 male) 17.94

18.46

E3 (5 female, 5 male) 17.96

18.36

Avg.

20.53

21.24

the test set. We used the same random seed over all experiments, so that they could be conducted under the same conditions.
The results are listed in Table 1. Although the clean AM produced a WER of 7.90% for the original clean dataset, the WER severely degraded to 88.03% by mixing two speakers. The TSAM then signiﬁcantly recovered the WER to 20.78% (α = 0.0). Although the improvement was not so signiﬁcant compared with that reported in [6], the auxiliary loss further improved the WER to 20.53% (α = 1.0). Note that E1 contains only male speakers while E2 and E3 contain both female and male speakers. Because of this, E1 showed larger degradation of WER when 2 speakers were mixed.

4.2.2. Interference-speaker ASR by auxiliary output branch
Before moving to the evaluation of dialogue recordings, we also evaluated the use case of the auxiliary output branch for interference speakers to conduct interference-speaker ASR. In this experiment, we provided the target speaker’s embeddings for the TS-AM and evaluated the WERs of the ASR results using the auxiliary output branch. The results are shown in Table 2. We conﬁrmed that the auxiliary output branch worked very well for the secondary ASR. This clearly indicates that the shared layers of the neural network were learned to separate speakers. In addition, we found out that this secondary ASR can be effectively incorporated into the proposed method, which we will explain in the next section.

4.3. Experiment with dialogue recordings
Since we conﬁrmed that TS-ASR worked as expected, we then conducted experiments for dialogue recordings, which were the main target of this study.

4.3.1. WER evaluation with oracle non-overlapped speech
We ﬁrst evaluated the lower limit of WER for the dialogue recordings by using the non-overlapped dialogue recordings (see Section 4.1 for recording settings). We used the original non-overlapped recordings with the ground-truth segmentation and conducted ASR with the clean AM. The results are shown in the ﬁrst line of Table 3. We observed a WER of 19.93%, which was the lower limit for the recordings in this experiment. The WER was worse compared with those for lecture recordings (E1, E2, and E3). We observed more substitution errors of backchannels for dialogue recordings, which was very short and difﬁcult to recognize.

4.3.2. WER evaluation using oracle speaker embeddings
We then conducted experiments for the mixed monaural dialogue recordings. For preprocessing, we separated each dialogue recording into speech segments by using simple power-based voice activity detection. Note that each segment could contain the speech of two speakers. We counted a recognized word as correct only when it and the recognized speaker both matched the reference label. Since there was ambiguity in the order of speakers in the reference label, we calculated the best WER among possible permutations of speakers.
We ﬁrst conducted an experiment with oracle speaker embeddings to conﬁrm the oracle WER for two-speaker mixed recordings. The results are shown in the second to fourth rows of Table 3. We extracted the oracle i-vector for each speaker by using only the nonoverlapped region determined from the ground-truth segmentation.
When we used the clean AM with the oracle speaker embeddings, we observed a very poor WER of 94.22% (# 2 of Table 3). This was within our expectation because the clean AM was not trained to extract the target speaker’s speech.
When we evaluated using the TS-AM with embeddings e1 and e2, we observed the best oracle WER of 37.96% (# 3 of Table 3). This result was the lower limit of WER for two-speaker mixed recordings in this experiment. As another application of the TSAM, we also used the auxiliary output branch of the TS-AM with embedding e1 to recognize speaker 2. This result is shown in the fourth row in Table 3. It showed a slightly worse WER of 41.09% compared with the WER with the TS-AM with embeddings e1 and e2 (# 4 of Table 3). This was also within our expectation because the auxiliary branch produced a slightly worse result than the main branch according to Table 2.
4.3.3. WER evaluation using estimated speaker embeddings with iterative update (proposed method)
Finally, we evaluated the proposed method by starting from the estimated speaker embeddings. In this evaluation, we estimated the i-vector for each speech partition divided by power-based voice activity detection then applied K-means clustering. The number of clusters was set to 2 to be the same as the number of speakers. The center of each cluster was used for the initial set of speaker embeddings. Note that we denoted the cluster center of the larger cluster as e1 and that of the smaller cluster as e2.
Similar to the comparison of # 3 and # 4 of Table 3, we also compared two methods without and with auxiliary output branch. The results are shown in the ﬁfth and sixth rows. Contrary to the experiment with the oracle speaker embeddings, the method using the auxiliary output branch with the embedding e1 to recognize the speaker 2 produced much better WER of 45.54% than the method using the main output branch with the embedding e2 to recognize the speaker 2. This is because the K-means-clustering-based speaker embedding estimation was not sufﬁcient to generate two discriminative embeddings of e1 and e2. Considering that embedding e2 was selected as the center of the smaller cluster, it would not be as reliable as embedding e1. In such a case, using a single embedding e1 with the auxiliary output branch is better than using an unreliable embedding e2 with the main output branch.
We then evaluated the proposed method of applying speakerembedding estimation and TS-ASR alternately. The results are shown in the seventh to ninth rows of Table 3. We observed clear improvement in the WER both for different gender pairs and same gender pairs, and achieved a WER of 40.03%, which differd by only 2.1% from the oracle WER of 37.96% with the oracle speaker embeddings. Note that we observed better results by the proposed

Table 3. WERs (%) for dialogue speech in CSJ

# Speaker Embeddings

AM

Evaluation

Gender Pair

Total

Initialization Update

Data

Different Same

1

-

-

Clean-AM

1-spk.

18.49† 21.14† 19.93†

2

Oracle

-

Clean-AM w/ e1 & Clean-AM w/ e2 2-spk. mixed 94.46† 94.01† 94.22†

3

Oracle

-

TS-AM (tgt) w/ e1 & TS-AM (tgt) w/ e2 2-spk. mixed 26.83† 47.33† 37.96†

4

Oracle

-

TS-AM (tgt) w/ e1 & TS-AM (int) w/ e1 2-spk. mixed 25.99† 53.80† 41.09†

5 K-means (i = 0) TS-AM (tgt) w/ e1 & TS-AM (tgt) w/ e2 2-spk. mixed 40.99 64.97 54.01

6 K-means (i = 0) TS-AM (tgt) w/ e1 & TS-AM (int) w/ e1 2-spk. mixed 30.00 58.61 45.54

7 K-means

i = 1 TS-AM (tgt) w/ e1 & TS-AM (int) w/ e1 2-spk. mixed 26.45 53.93 41.37

8 K-means

i = 2 TS-AM (tgt) w/ e1 & TS-AM (int) w/ e1 2-spk. mixed 25.46 52.82 40.31

9 K-means

i = 3 TS-AM (tgt) w/ e1 & TS-AM (int) w/ e1 2-spk. mixed 25.20 52.50 40.03

† Result obtained with some oracle information such as non-overlapped evaluation data or oracle speaker embeddings

Table 4. DERs (%) for dialogue speech

Method

Gender Pair

Different Same

i-vector with K-means 25.94 37.32

# 6 of Table 3

15.99 37.00

# 9 of Table 3

10.76 35.30

i-vector with AHC [33]‡ x-vector with AHC [33]‡

14.34 13.77

38.48 30.02

Total
32.37 27.87 24.63 27.99 22.96

‡ Trained using combination of Switchboard and NIST SRE datasets

Table 5. Details of DER (%) for # 9 of Table 3

Miss False Alarm Confusion

Different Gender Pair 9.6

0.8

0.4

Same Gender Pair 22.5

2.2

10.6

Total

16.9

1.6

6.2

DER 10.76 35.30 24.63

method than that of the method # 4 of Table 3 even though the latter method used the speaker embeddings obtained from the groundtruth segmentation. We believe it was because the ground-truth segmentation contained an unignorable amount of silence frames that degraded the purity of speaker embeddings, while strict exclusion of silence frames was achieved by using the TS-ASR results.

probabilistic linear discriminant analysis (PLDA) was used to create a speaker cluster. Although it is not directly comparable due to the difference in training data, we conﬁrmed that our method produced a reasonably good DER although it was trained much smaller training data. Our method showed a better DER than that of “i-vector with AHC” and a DER close to that of “x-vector with AHC”. The proposed method even achieved a better DER than the x-vector-based method for the different gender pairs although we used an i-vector trained by much smaller data for the proposed method.
Finally, we discuss the detailed error analysis of the DER for the method with speaker embedding updated three times (Table 5). We ﬁrst found that the main source of the DER came from the miss error. We also found that the confusion error and false alarm were low even for same gender pairs. When applying the proposed method for the different gender pairs, almost no confusion and false alarm were produced. This means that TS-ASR worked very conservatively, i.e., it tended to output null when it was not able to ﬁnd a reliable word hypothesis. From the results in Tables 4 and 5, we can expect further improvement in the DER if we apply more discriminative speaker embeddings such as d-vector [10, 35] and x-vector [34, 36], which is one important direction for our future work.
5. CONCLUSION

4.3.4. Evaluation of DERs
Table 4 lists the DERs of three methods. Note that we set 0.25 sec of the no-score collar according to convention, and calculated DER including overlapped regions. It is also noted that we regarded silence frames of less than 0.5 sec as speech regions for the proposed method because we found the silence information that the ASR produced was too strict compared to the ground-truth segmentation developed by human transcribers. The ﬁrst row is a naive method based on the clustering of i-vectors, which was used for the embedding initialization. As expected, it produced a very poor DER of 32.37% due to heavy speech overlaps in the recordings. Just by applying TS-ASR, we observed an improvement in the DER to 27.87%, especially for different gender pairs. By using the proposed method, the DER further improved to 24.63%.
To compare with the state-of-the-art speaker diarization method, we also tested the agglomerative hierarchical clustering (AHC)based method [33, 34], the results of which are shown in the last two rows in Table 4. The i-vector and x-vector extractors were trained using about 3,000 hrs of training data consisting of Switchboard-2 (Phase I, II, III), Switchboard Cellular (Part 1, Part2), and NIST Speaker Recognition Evaluation (2004, 2005, 2006, 2008) datasets. Speaker embeddings were extracted every 0.75 sec, and AHC with

In this paper, we deﬁned the problem of simultaneous ASR and speaker diarization, and proposed an iterative method, in which (i) the estimation of speaker embeddings in the recordings and (ii) TSASR based on the estimated speaker embeddings are alternately executed. We evaluated the proposed method by using real dialogue recordings in the CSJ in which the speaker overlap ratio was over 20%. We conﬁrmed that the proposed method signiﬁcantly reduced both the WER and DER. Our proposed method with i-vector speaker embeddings ultimately achieved a WER that differed by only 2.1 % from the WER of TS-ASR given oracle speaker embeddings. Furthermore, our method achieved better DER than that of the conventional clustering-based speaker diarization method based on i-vector.
There are many directions to enhance this research. First, our proposed method should be examined using recordings with more than two speakers. Second, the use of more discriminative speaker embeddings, such as d-vector [10, 35] and x-vector [34, 36], will improve performance of both ASR and speaker diarization. Third, the initialization of speaker embeddings should be explored with more advanced speaker diarization techniques [10, 37, 38]. Finally, advanced ASR techniques, such as data augmentation [39–42], model ensemble [43–45], improved training criterion [46, 47], will also improve overall performance. We will explore these directions for future work.

6. REFERENCES
[1] Sue E Tranter and Douglas A Reynolds, “An overview of automatic speaker diarization systems,” IEEE Trans. on ASLP, vol. 14, no. 5, pp. 1557–1565, 2006.
[2] Jonathan G Fiscus, Jerome Ajot, and John S Garofolo, “The Rich Transcription 2007 meeting recognition evaluation,” in Multimodal Technologies for Perception of Humans, pp. 373– 389. 2007.
[3] Takuya Yoshioka, Hakan Erdogan, Zhuo Chen, Xiong Xiao, and Fil Alleva, “Recognizing overlapped speech in meetings: A multichannel separation approach using neural networks,” in Proc. INTERSPEECH, 2018, pp. 3038–3042.
[4] Dong Yu, Xuankai Chang, and Yanmin Qian, “Recognizing multi-talker speech with permutation invariant training,” in Proc. INTERSPEECH, 2017, pp. 2456–2460.
[5] Marc Delcroix, Katerina Zmolikova, Keisuke Kinoshita, Atsunori Ogawa, and Tomohiro Nakatani, “Single channel target speaker extraction and recognition with speaker beam,” in Proc. ICASSP, 2018, pp. 5554–5558.
[6] Naoyuki Kanda, Shota Horiguchi, Ryoichi Takashima, Yusuke Fujita, Kenji Nagamatsu, and Shinji Watanabe, “Auxiliary interference speaker loss for target-speaker speech recognition,” arXiv preprint arXiv:1906.10876, 2019.
[7] Sylvain Meignier and Teva Merlin, “LIUM SpkDiarization: an open source toolkit for diarization,” in CMU SPUD Workshop, 2010.
[8] Gregory Sell and Daniel Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in Proc. SLT, 2014, pp. 413–417.
[9] Mohammed Senoussaoui, Patrick Kenny, Themos Stafylakis, and Pierre Dumouchel, “A study of the cosine distancebased mean shift for telephone speech diarization,” IEEE/ACM Trans. on ASLP, vol. 22, no. 1, pp. 217–227, 2013.
[10] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansﬁeld, and Ignacio Lopz Moreno, “Speaker diarization with LSTM,” in Proc. ICASSP, 2018, pp. 5239–5243.
[11] John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31– 35.
[12] Zhuo Chen, Yi Luo, and Nima Mesgarani, “Deep attractor network for single-microphone speaker separation,” in Proc. ICASSP, 2017, pp. 246–250.
[13] Zhehuai Chen, Jasha Droppo, Jinyu Li, Wayne Xiong, Zhehuai Chen, Jasha Droppo, Jinyu Li, and Wayne Xiong, “Progressive joint modeling in unsupervised single-channel overlapped speech recognition,” IEEE/ACM Trans. on ASLP, vol. 26, no. 1, pp. 184–196, 2018.
[14] Shane Settle, Jonathan Le Roux, Takaaki Hori, Shinji Watanabe, and John R Hershey, “End-to-end multi-speaker speech recognition,” in Proc. ICASSP, 2018, pp. 4819–4823.
[15] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Jonathan Le Roux, and John R Hershey, “A purely end-to-end system for multi-speaker speech recognition,” in Proc. ACL, 2018, pp. 2620–2630.

[16] Xuankai Chang, Yanmin Qian, Kai Yu, and Shinji Watanabe, “End-to-end monaural multi-speaker asr system without pretraining,” in Proc. ICASSP, 2019.
[17] Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen, “Permutation invariant training of deep models for speakerindependent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245.
[18] Kateˇrina Zˇ mol´ıkova´, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and Tomohiro Nakatani, “Speaker-aware neural network based beamformer for speaker extraction in speech mixtures,” in Proc. INTERSPEECH, 2017.
[19] Najim Dehak, Patrick J Kenny, Re´da Dehak, Pierre Dumouchel, and Pierre Ouellet, “Front-end factor analysis for speaker veriﬁcation,” IEEE Trans. on ASLP, vol. 19, no. 4, pp. 788–798, 2011.
[20] Naoyuki Kanda, Rintaro Ikeshita, Shota Horiguchi, Yusuke Fujita, Kenji Nagamatsu, Xiaofei Wang, Vimal Manohar, Nelson Enrique Yalta Soplin, Matthew Maciejewski, Szu-Jui Chen, et al., “The Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays,” in Proc. CHiME-5, 2018, pp. 6–10.
[21] Vimal Manohar, Szu-Jui Chen, Zhiqi Wang, Yusuke Fujita, Shinji Watanabe, and Sanjeev Khudanpur, “Acoustic modeling for overlapping speech recognition: JHU CHiME-5 challenge system,” in Proc. ICASSP, 2019, pp. 6665–6669.
[22] Kateˇrina Zˇ mol´ıkova´, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and Tomohiro Nakatani, “Learning speaker representation for neural network based multichannel speaker extraction,” in Proc. ASRU, 2017, pp. 8–15.
[23] George Saon, Hagen Soltau, David Nahamoo, and Michael Picheny, “Speaker adaptation of neural network acoustic models using i-vectors,” in Proc. ASRU, 2013, pp. 55–59.
[24] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahrmani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur, “Purely sequence-trained neural networks for ASR based on lattice-free MMI,” in Proc. INTERSPEECH, 2016, pp. 2751–2755.
[25] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, “A time delay neural network architecture for efﬁcient modeling of long temporal contexts,” in Proc. INTERSPEECH, 2015, pp. 3214–3218.
[26] Kikuo Maekawa, “Corpus of spontaneous japanese: Its design and evaluation,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[27] O¨ zgu¨r C¸ etin and Elizabeth Shriberg, “Analysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: insights for automatic speech recognition,” in Proc. ICSLP, 2006.
[28] Tatsuya Kawahara, Hiroaki Nanjo, Takahiro Shinozaki, and Sadaoki Furui, “Benchmark test for speech recognition using the Corpus of Spontaneous Japanese,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[29] Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang, “Phoneme recognition using time-delay neural networks,” IEEE Trans. ASSP, vol. 37, no. 3, pp. 328–339, 1989.

[30] Sepp Hochreiter and Ju¨rgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[31] Yiming Wang, Vijayaditya Peddinti, Hainan Xu, Xiaohui Zhang, Daniel Povey, and Sanjeev Khudanpur, “Backstitch: Counteracting ﬁnite-sample bias via negative steps,” in Proc. INTERSPEECH, 2017, pp. 1631–1635.
[32] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka´sˇ Burget, Ondˇrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motl´ıcˇek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.
[33] Gregory Sell, David Snyder, Alan McCree, Daniel GarciaRomero, Jesu´s Villalba, Matthew Maciejewski, Vimal Manohar, Najim Dehak, Daniel Povey, Shinji Watanabe, and Sanjeev Khudanpur, “Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge,” in Proc. INTERSPEECH, 2018, pp. 2808– 2812.
[34] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan McCree, “Speaker diarization using deep neural network embeddings,” in Proc. ICASSP, 2017, pp. 4930– 4934.
[35] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, “Generalized end-to-end loss for speaker veriﬁcation,” in Proc. ICASSP, 2018, pp. 4879–4883.
[36] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, “X-vectors: Robust dnn embeddings for speaker recognition,” in Proc. ICASSP, 2018, pp. 5329–5333.
[37] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. INTERSPEECH, 2019.
[38] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Yawen Xue, Kenji Nagamatsu, and Shinji Watanabe, “End-to-end neural speaker diarization with self-attention,” in Proc. ASRU, 2019, to appear.
[39] Navdeep Jaitly and Geoffrey E Hinton, “Vocal tract length perturbation (VTLP) improves speech recognition,” in Proc. ICML Workshop on Deep Learning for Audio, Speech and Language, 2013, vol. 117.
[40] Naoyuki Kanda, Ryu Takeda, and Yasunari Obuchi, “Elastic spectral distortion for low resource speech recognition with deep neural networks,” in Proc. ASRU, 2013, pp. 309–314.
[41] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, “Audio augmentation for speech recognition,” in Proc. INTERSPEECH, 2015, pp. 3586–3589.
[42] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in Proc. ICASSP, 2017, pp. 5220–5224.
[43] Yuuki Tachioka, Shinji Watanabe, Jonathan Le Roux, and John R Hershey, “A generalized discriminative training framework for system combination,” in Proc. ASRU, 2013, pp. 43– 48.
[44] Li Deng and John C Platt, “Ensemble deep learning for speech recognition,” in Proc. INTERSPEECH, 2014, pp. 1915–1919.

[45] Naoyuki Kanda, Yusuke Fujita, and Kenji Nagamatsu, “Investigation of lattice-free maximum mutual information-based acoustic models with sequence-level Kullback-Leibler divergence,” in Proc. ASRU, 2017, pp. 69–76.
[46] Naoyuki Kanda, Yusuke Fujita, and Kenji Nagamatsu, “Lattice-free state-level minimum Bayes risk training of acoustic models,” in Proc. INTERSPEECH, 2018, pp. 2923–2927.
[47] Chao Weng and Dong Yu, “A comparison of lattice-free discriminative training criteria for purely sequence-trained neural network acoustic models,” in Proc. ICASSP, 2019, pp. 6430– 6434.

