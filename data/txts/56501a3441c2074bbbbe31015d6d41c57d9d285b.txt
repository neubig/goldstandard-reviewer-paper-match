Paraphrastic Representations at Scale
John Wieting1, Kevin Gimpel2, Graham Neubig1, and Taylor Berg-Kirkpatrick3 1Carnegie Mellon University, Pittsburgh, PA, 15213, USA
2Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA 3University of California San Diego, San Diego, CA, 92093, USA
{jwieting,gneubig}@cs.cmu.edu, kgimpel@ttic.edu, tberg@eng.ucsd.edu

arXiv:2104.15114v1 [cs.CL] 30 Apr 2021

Abstract

tasks (Wieting et al., 2019a). Within this context,

We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We also release trained models for English, Arabic, German, French, Spanish, Russian, Turkish, and Chinese. We train

fast and light-weight methods are particularly useful as they make it easy to compute similarity over the ever-increasing volumes of web text available. For instance, we may want to mine a hundred million parallel sentences (Schwenk et al., 2019) or use a semantic similarity reward when ﬁne-tuning

these models on large amounts of data, achiev-

language generation models on tens of millions of

ing signiﬁcantly improved performance from the original papers proposing the methods on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on unsupervised semantic textual similarity, signiﬁcantly outperform-

training examples. These tasks are much more feasible when using approaches that are fast, can be run on CPU, and use little RAM allowing for increased batch size.
This need for fast inference is one motivation for using sentence embeddings. Sentence em-

ing even BERT-based models like Sentence-

beddings allow the search for similar sentences

BERT (Reimers and Gurevych, 2019). Additionally, our models are orders of magnitude faster than prior work and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finally, we

to be linear in the number of sentences, or even sub-linear when using highly optimized tools like Faiss (Johnson et al., 2017) that allow for efﬁcient nearest neighbor search. This is contrast to models, like cross-attention models, which are quadratic during inference as they require both of the texts being compared as inputs. As we

add signiﬁcantly increased functionality to the

show in this paper, our sentence embedding mod-

code bases for training paraphrastic sentence models, easing their use for both inference and for training them for any desired language with parallel data. We also include code to automatically download and preprocess training data.1

els, based on our prior work (Wieting and Gimpel, 2017a; Wieting et al., 2019b), are orders of magnitude faster to compute than prior embedding approaches while simultaneously possessing signiﬁcantly stronger performance on monolingual

1 Introduction
Measuring sentence similarity (Agirre et al., 2012) is an important task in natural language processing, and has found many uses including paraphrase detection (Dolan et al., 2004), bitext mining (Schwenk and Douze, 2017), language modelling (Khandelwal et al., 2019), questionanswering (Lewis et al., 2021), and as reward functions or evaluation metrics for language generation

and cross-lingual semantic similarity tasks. We also show that this approach is competitive with LASER (Artetxe and Schwenk, 2018), a state-ofthe-art model for mining bitext, with inference speeds that are twice as fast on GPU and orders of magnitude faster on CPU.
We make several contributions in this paper that go beyond our prior work (Wieting and Gimpel, 2017a; Wieting et al., 2019b). First, we reformatted the code to support training models on

1Code, released models, and data are available at tens of millions of sentence pairs efﬁciently and https://github.com/jwieting/paraphrastic-representations-at-scale.with low RAM usage. Secondly, we train an

English model on 25.85 million paraphrase pairs from ParaNMT (Wieting and Gimpel, 2017a), a paraphrase corpus we previously constructed automatically from bitext. We then train models directly on X-English bitext for Arabic, German, French, Spanish, Russian, Turkish, and Chinese producing models that are able to distinguish both paraphrases in English and their respective languages as well as crosslingual X-English paraphrases. Even though all models are able to model semantic similarity in English, we ﬁnd that training on ParaNMT speciﬁcally leads to stronger models as it is easier to ﬁlter the data to remove noise and sentence pairs with little to no diversity. We make all of these models available to the community for use on downstream tasks.
We also added functionality to our implementation. Besides the support for efﬁcient, lowmemory training on tens of million of sentence pairs described above, we added code to support (1) reading in a list of sentences and producing a saved numpy array of the sentence embeddings; (2) reading in a list of sentence pairs and producing cosine similarity scores; and (3) downloading and preprocessing evaluation data, bitext, and paraphrase data. For bitext and paraphrase data, we provide support for training using either text ﬁles or HDF5 ﬁles.
Lastly, this paper contains new experiments showcasing the limits of these scaled-up models and detailed comparisons with prior work on a suite of semantic similarity tasks in a variety of languages. We release our code and models to the community in the hope that they will be found useful for research and applications, as well as using them as a base to build stronger, faster models covering more of the world’s languages.
2 Related Work
2.1 English Semantic Similarity
Our learning and evaluation setting is the same as that of our earlier work that seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks (Wieting et al., 2016b,a; Wieting and Gimpel, 2017b; Wieting et al., 2017; Wieting and Gimpel, 2018). We trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. More recently, we made use of parallel bitext for training paraphrastic representations for other languages as well that are also able to model

cross-lingual semantic similarity (Wieting et al., 2019a,c). Prior work in learning general sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015; Gan et al., 2017), and other sources of supervision and learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017).
For English semantic similarity, we compare to well known sentence embedding models such as InferSent (Conneau et al., 2017), GenSen (Subramanian et al., 2018), the Universal Sentence Encoder (USE) (Cer et al., 2018), as well as BERT (Devlin et al., 2018).2 We used the pretrained BERT model in two ways to create a sentence embedding. The ﬁrst way is to concatenate the hidden states for the CLS token in the last four layers. The second way is to concatenate the hidden states of all word tokens in the last four layers and mean pool these representations. Both methods result in a 4096 dimension embedding. We also compare to a more recently released model called Sentence-BERT (Reimers and Gurevych, 2019). This model is similar to InferSent in that it is trained on natural language inference data (SNLI; Bowman et al., 2015). However, instead of using pretrained word embeddings, they ﬁne-tune BERT in a way to induce sentence embeddings.3
2.2 Cross-Lingual Semantic Similarity and Semantic Similarity in Non-English Languages
Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using contrastive losses (Gre´goire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). Recently other approaches using large Transformer (Vaswani et al., 2017) have been proposed, trained on vast quantities of text (Conneau et al.,
2Note that in all experiments using BERT, including Sentence-BERT, the large, uncased version is used.
3Most work evaluating accuracy on STS tasks has averaged the Pearson’s r over each individual dataset for each year of the STS competition. However, Reimers and Gurevych (2019) computed Spearman’s ρ over concatenated datasets for each year of the STS competition. To be consistent with previous work, we re-ran their model and calculated results using the standard method, and thus our results are not the same as those reported by Reimers and Gurevych (2019).

2019; Liu et al., 2020; Tran et al., 2020). We M mini-batches to create one “mega-batch” and

primarily focus our comparison for these settings selects negative examples from this mega-batch.

on LASER (Artetxe and Schwenk, 2018), a model Once each pair in the mega-batch has a negative

trained for semantic similarity across more than example, the mega-batch is split back up into M

100 languages. Their model uses an LSTM mini-batches for training. Additionally, we anneal

encoder-decoder trained on hundreds of millions the mega-batch size by slowly increasing it during

of parallel sentences. They achieve state-of-the-art training. This yields improved performance by a

performance on a variety of multilingual sentence signiﬁcant margin.

embeddings tasks including bitext mining.

Encoder. Our sentence encoder g simply aver-

3 Methods

ages the embeddings of subword units generated

We ﬁrst describe our objective function and then describe our encoder.

by sentencepiece (Kudo and Richardson, 2018); we refer to it as SP. This means that the sentence piece embeddings themselves are the

Training. The training data consists of a se- only learned parameters of this model.

quence of parallel sentence pairs (si, ti) in source and target languages respectively. Note that for

4 Code and Usage

training our English model, the source and target
languages are both English as we are able to make
use of an existing paraphrase corpus. For each sen-
tence pair, we randomly choose a negative target sentence t′i during training that is not a translation or paraphrase of si. Our objective is to have source and target sentences be more similar than source

We added a number of features to the code base to improve performance and make it easier to use. First, we added code to support easier inference. Examples of using the code programmatically to embed sentences and score sentence pairs (using cosine similarity) are shown in Figure 1.

and negative target examples by a margin δ:

1 from models import load_model
2

min
θsrc,θtgt i

δ−fθ(si, ti) + fθ(si, t′i))
+

3 text1 = ’This is a test.’
(1) 4 text2 = ’This is another test.’
5
6 # Load English paraphrase model

where the similarity function is deﬁned as:

7 model = load_model(’paraphrase-at-scale/ model.para.lc.100.pt’)

fθ(s, t) = cos g(s; θsrc), g(t; θtgt)

8
(2) 9 # Obtain sentence embedding
10 embeddings = model.embed_raw_text([text1

where g is the sentence encoder with parameters

, text2]) # 2D numpy array of sentence embeddings

for each language θ = (θsrc, θtgt). To select t′i 11 cosine_scores = model.score_raw_text([(

we choose the most similar sentence in some set

text1, text2)]) # list of cosine

scores

according to the current model parameters, i.e.,

the one with the highest cosine similarity. We Figure 1: Usage example of programmatically loading

found we could achieve the strongest performance one of our pretrained models and obtaining sentence

by tying all parameters together for each language, embeddings and scores for two sentences.

more precisely, θsrc and θtgt are the same.

Our code base also supports functionality that

Negative Sampling. Negative examples are se- allows one to read in a list of sentences and pro-

lected from the sentences in the batch from the duce a saved numpy array of the sentence embed-

opposing language when training with bitext and dings. We also included functionality that allows

from any sentence in the batch when using para- one to read in a list of sentence pairs and produce

phrase data. In all cases, we choose the neg- the sentence pairs along with their cosine similar-

ative example with the highest cosine similarity ity scores in an output ﬁle. These scripts allow

to the given sentence s, ensuring that the nega- our models to be used without any programming

tive is not in fact paired with s in the batch. To for the two most common use cases: embedding

select even more difﬁcult negative examples that sentences and scoring sentence pairs. Examples

aid training, we use the mega-batching procedure of their usage with a trained model are shown in

of Wieting and Gimpel (2018), which aggregates Figure 2.

1 python -u embed_sentences.py --sentencefile paraphrase-at-scale/examplesentences.txt --load-file paraphrase -at-scale/model.para.lc.100.pt -output-file sentence_embeds.np
2
3 python score_sentence_pairs.py -sentence-pair-file paraphrase-atscale/example-sentences-pairs.txt -load-file paraphrase-at-scale/model. para.lc.100.pt
Figure 2: Usage example to embed sentences and score sentence pairs. The ﬁrst command is a usage example of scoring a list of sentence pairs. The ﬁle example-sentences-pairs.txt contains a list of sentences, one per line. The output of the script is a saved numpy array of sentence embeddings in the same order of the input sentences. The second command is a usage example of scoring a list of sentence pairs. The ﬁle example-sentences-pairs.txt contains pairs of tab-separated sentences, one per line. The output of the script is a text ﬁle containing the tab separated list of sentences along with their cosine scores in the same order of the input sentences.
Second, we added a training mode for HDF5, allowing training data to remain on disk during training. This leads to a signiﬁcant reduction in RAM usage during training, which is especially true when using more than 10 million training examples. Efﬁcient training can now be done on CPU only using only a few gigabytes of RAM.
1 cd preprocess/bilingual && bash do_all. sh fr-es-de
2 cd ../.. 3 cd preprocess/paranmt && bash do_all.sh
0.7 1.0 0.5
Figure 3: Usage example to download and preprocess bilingual and ParaNMT data. The ﬁrst command downloads and preprocesses (ﬁlters, trains sentencepiece models, tokenizes if language is zh, converts ﬁles to hdf5 format) en-X bilingual data. The third command downloads and preprocesses ParaNMT data. The arguments are used to ﬁlter the data (semantic similarity scores between 0.7 and 1.0 and trigram overlap below 0.5, which have been used in prior papers when generating training data for paraphrase generation (Iyyer et al., 2018; Krishna et al., 2020)).
Lastly, we also added code for preprocessing data, including scripts to download and evaluate on the STS data (English, non-English, and cross-lingual), as well as code to download and process bitext and ParaNMT automatically. For bitext, our scripts download the data, ﬁlter by

en ar de es fr ru tr zh
25.85M 8.23M 6.47M 6.75M 6.46M 9.09M 5.12M 4.18M
Table 1: The number of sentence pairs used to train our models. For English, the data is ParaNMT, and for the other languages, the data is a collection of bitext detailed in Section 5.1.
length,4 lowercase, remove duplicates, train a sentencepiece model, encode the data with the sentencepiece model, shufﬂe the data, and process the data into HDF5 format for efﬁcient use. For ParaNMT, our scripts download the data, use a language classiﬁer to ﬁlter out non-English sentences5 (Joulin et al., 2016), ﬁlter the data by paraphrase score, trigram overlap, and length,6 train a sentencepiece model, encode the data with the sentencepiece model, and process the data into HDF5 format. Examples are shown in Figure 3.
5 Experiments
5.1 Experimental Setup
Data. For our English model, we train on selected sentence pairs from ParaNMT (Wieting and Gimpel, 2017a). We ﬁlter the corpus by only including sentence pairs where the paraphrase score for the two sentences is ≥ 0.4. We additionally ﬁltered sentence pairs by their trigram overlap (Wieting et al., 2017), which is calculated by counting trigrams in the two sentences, and then dividing the number of shared trigrams by the total number in the sentence with fewer tokens. We only include sentence pairs where the trigram overlap score is ≤ 0.7. The paraphrase score is calculated by averaging PARAGRAM-PHRASE embeddings (Wieting et al., 2016b) for the two sentences in each pair and then computing their cosine similarity. The purpose of the lower threshold is to remove noise while the higher threshold is meant to remove paraphrases that are too similar.
Our training data is a mixture of Open Subtitles 20187 (Lison and Tiedemann, 2016), Tanzil corpus8 (Tiedemann), Europarl9 for Spanish, Global
4We remove sentences with the number of tokens (untokenized) smaller than 3 or greater than 100.
5https://fasttext.cc 6We remove sentences with the number of tokens (untokenized) smaller than 5 or greater than 40. 7http://opus.nlpl.eu/OpenSubtitles.php 8http://opus.nlpl.eu/Tanzil.php 9http://opus.nlpl.eu/Europarl.php

Model
BERT (CLS) BERT (Mean) InferSent GenSen USE Sentence-BERT LASER SP

Semantic Textual Similarity (STS) 2012 2013 2014 2015 2016 Avg. 33.2 29.6 34.3 45.1 48.4 38.1 48.8 46.5 54.0 59.2 63.4 54.4 61.1 51.4 68.1 70.9 70.7 64.4 60.7 50.8 64.1 73.3 66.0 63.0 61.4 59.0 70.6 74.3 73.9 67.8 66.9 63.2 74.2 77.3 72.8 70.9 63.1 47.0 67.7 74.9 71.9 64.9 68.7 64.7 78.1 81.4 80.0 74.6

Table 2: Results of our models and models from prior work on English STS. We show results, measured in Pearson’s r × 100, for each year of the STS tasks 2012-2016 as well as the average performance across all years.

Model

Dim. ar-ar ar-en es-es es-en tr-en

LASER

1024 69.3

65.5

79.7

59.7

72.0

Espana-Bonet et al. (2017) 2048 59

44

78

49

76

Chidambaram et al. (2018) 512

-

-

64.2

58.7

-

2017 STS 1st Place 2017 STS 2nd Place 2017 STS 3rd Place

-

75.4

74.9

85.6

83.0

77.1

-

75.4

71.3

85.0

81.3

74.2

-

74.6

70.0

84.9

79.1

73.6

SP

1024 76.2

78.3

85.8

78.4

79.2

Table 3: Comparison of our models with those in the literature on non-English and cross-lingual STS. We also include the top 3 systems for each dataset from the SemEval 2017 STS shared task. Metric is Pearson’s r ×100.

Voices10 (Tiedemann), and the MultiUN corpus11. We follow the same distribution for our languages of interest across data sources as Artetxe and Schwenk (2018) for a fair comparison. One exception, though, is we do not include training data from Tatoeba12 (Tiedemann) as they do, since this domain is also in the bitext mining evaluation set. The amount of data used to train each of our models is shown in Table 1.
Hyperparameters. For all models we ﬁx the batch size to 128, margin δ to 0.4, and the annealing rate to 150.13 We set the size of the sentencepiece vocabulary to 50,000, using a shared vocabulary for the models trained on bitext. We optimize our models using Adam (Kingma and Ba, 2014) with a learning rate of 0.001 and train models for 25 epochs.
For training on the bilingual corpora, we tune each model on the 250 example 2017 English STS task (Cer et al., 2017). We vary dropout on the embeddings over {0, 0.1, 0.3} and the mega-batch size M over {60, 100, 140}.
For training on ParaNMT, we ﬁx the hyperparameters in our model due to the increased data size making tuning more expensive. We use a mega-batch size M of 100 and set the dropout on
10https://opus.nlpl.eu/GlobalVoices.php 11http://opus.nlpl.eu/MultiUN.php 12https://opus.nlpl.eu/Tatoeba.php 13Annealing rate is the number of minibatches that are processed before the megabatch size is increased by 1.

the embeddings to 0.0.
5.2 Evaluation
We evaluate sentence embeddings using the SemEval semantic textual similarity (STS) tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 means they are completely equivalent. As our test set, we report the average Pearson’s r over each year of the STS tasks from 2012-2016.
For cross-lingual semantic similarity and semantic similarity in non-English languages, we evaluate on the STS tasks from SemEval 2017. This evaluation contains Arabic-Arabic, ArabicEnglish, Spanish-Spanish, Spanish-English, and Turkish-English datasets. The datasets were created by translating one or both pairs of an English STS pair into Arabic (ar), Spanish (es), or Turkish (tr).
We also evaluate on the Tatoeba bitext mining task introduced by Artetxe and Schwenk (2018). The dataset consists of up to 1,000 English-aligned sentence pairs for over 100 languages. The aim of the task is to ﬁnd the nearest neighbor for each sentence in the other language according to cosine similarity. Performance is measured by computing the error rate.

Language LASER XLM-R mBART CRISS SP

ar

7.8 52.5

61

22 8.8

de

1.0 11.1 13.2 2.0 1.5

es

2.1 24.3 39.6 3.7 2.4

fr

4.3 26.3 39.6 7.3 5.4

ru

5.9 25.9 31.6 9.7 5.6

tr

2.6 34.3 48.8 7.1 1.4

Table 4: Results on the Tatoeba bitext mining task (Artetxe and Schwenk, 2018). Results are measured in error rate ×100.

6 Results
English Semantic Similarity. The results for our English semantic similarity evaluation are shown in Table 2. Our SP model has the best performance across each year of the task, signiﬁcantly outperforming all prior work.

Cross-Lingual Semantic Similarity. The results for the non-English and cross-lingual semantic similarity evaluation are shown in Table 3. From the results, our model again outperforms all prior work using sentence embeddings. The only systems that have better performance are the top (non-embedding based) systems from SemEval 2017 for Spanish-English.14
Bitext Mining. The results on the Tatoeba bitext mining task from Artetxe and Schwenk (2018) are shown in Table 4. The results show that our embeddings are competitive, but have slightly higher error rates than LASER. The models are so close that the difference in error rate for the two models across the 6 evaluations is 0.23, corresponding to a difference of about 2 mismatched sentence pairs per dataset. We also compare to mBART, XLM-R, and CRISS.15
This bitext mining result is in contrast to the results on cross-lingual semantic similarity, suggesting that our embeddings account for a less literal semantic similarity, making them more adept at detecting paraphrases but slightly weaker at identifying translations. It is also worth noting that LASER was trained on Tatoeba data outside the test sets, which could also account for some of the slight improvement over our model.

7 Speed Analysis

We analyze the speed of our models as well as selected popular sentence embedding models from
14The top systems for this task used supervision and relied on state-of-the-art translation models to ﬁrst translate the nonEnglish sentences to English.
15Results are copied from (Tran et al., 2020).

Model
SP LASER Sentence-Bert InferSent

GPU
13,863 6,033 288 4,445

CPU
12,776 26 2 16

Table 5: Speed as measured in sentences/second on both GPU (Nvidia 1080 TI) and CPU (single core).

prior work. To evaluate inference speed, we measure the time required to embed 120,000 sentences from the Toronto Book Corpus (Zhu et al., 2015). Preprocessing of sentences is not factored into the timing, and each method sorts the sentences by length prior to computing the embeddings to reduce padding and extra computation. We use a batch size of 64 for each model. The number of sentences embedded per second is shown in Table 5.
From the results, we see that our model is easily the fastest on GPU, sometimes by an order of magnitude. Interestingly, using a single core of CPU, we achieve similar speeds to inference on GPU, which is not the case for any other model. Moreover, we repeated the experiment, this time using 32 cores and achieved a speed of 15,316 sentences/second. This is even faster than when using a GPU and indicates that our model can effectively be used at scale when GPUs are not available. It also suggests our model would be appropriate for use on embedded devices.

8 Conclusion
In this paper, we present a system for the learning and inference of paraphrastic sentence embeddings in any language for which there is paraphrase or bilingual parallel data. Additionally, we release our trained sentence embedding models in English, as well as Arabic, German, Spanish, French, Russian, Turkish, and Chinese. These models are trained on tens of million of sentence pairs resulting in models that achieve state-of-theart performance on unsupervised English semantic similarity and are state-of-the-art or competitive on non-English semantic similarity, cross-lingual semantic similarity, and bitext mining.
Moreover, our models are signiﬁcantly faster than prior work owing to their simple architecture. They can also be run on CPU with little to no loss in speed from running them on GPU—-something that no strong models from prior work are able to do. Lastly, we release our code that has been modiﬁed to make training and inference easier, with

support for training on large corpora, preprocessing paraphrase and bilingual corpora and evaluation data, as well as scripts for easy inference that can generate embeddings or semantic similarity scores for sentences supplied in a text ﬁle.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. Proceedings of SemEval, pages 497–511.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In Proceedings of the International Conference on Learning Representations.
Mikel Artetxe and Holger Schwenk. 2018. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. arXiv preprint arXiv:1812.10464.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference.

In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.
Muthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learning cross-lingual sentence representations via a multi-task dual-encoder model. arXiv preprint arXiv:1810.12836.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680, Copenhagen, Denmark.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of COLING.
Cristina Espana-Bonet, Ada´m Csaba Varga, Alberto Barro´n-Ceden˜o, and Josef van Genabith. 2017. An empirical analysis of nmt-derived interlingual embeddings and their use in parallel sentence identiﬁcation. IEEE Journal of Selected Topics in Signal Processing, 11(8):1340–1350.
Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. 2017. Learning generic sentence representations using convolutional neural networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2380–2390, Copenhagen, Denmark.

Francis Gre´goire and Philippe Langlais. 2018. Extracting parallel sentences with bidirectional recurrent neural networks to improve machine translation. arXiv preprint arXiv:1806.05559.
Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith Stevens, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Effective parallel corpus mining using bilingual sentence embeddings. arXiv preprint arXiv:1807.11906.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Jeff Johnson, Matthijs Douze, and Herve´ Je´gou. 2017. Billion-scale similarity search with GPUs. arXiv preprint arXiv:1702.08734.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efﬁcient text classiﬁcation. arXiv preprint arXiv:1607.01759.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.
Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in Neural Information Processing Systems 28, pages 3294–3302.
Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. arXiv preprint arXiv:2010.05700.
Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.
Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Ku¨ttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. Paq: 65

million probably-asked questions and what you can do with them. arXiv preprint arXiv:2102.07033.
Pierre Lison and Jo¨rg Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726–742.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. arXiv preprint arXiv:1703.02507.
Nghia The Pham, Germa´n Kruszewski, Angeliki Lazaridou, and Marco Baroni. 2015. Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084.
Holger Schwenk. 2018. Filtering and mining parallel data in a joint multilingual space. arXiv preprint arXiv:1805.09822.
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzma´n. 2019. Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. arXiv preprint arXiv:1907.05791.
Holger Schwenk and Matthijs Douze. 2017. Learning joint multilingual sentence representations with neural machine translation. arXiv preprint arXiv:1704.04154.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems.
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. 2018. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079.
Jo¨rg Tiedemann. Parallel data, tools and interfaces in OPUS.
Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020. Cross-lingual retrieval for iterative self-supervised training. arXiv preprint arXiv:2006.09526.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016a. Charagram: Embedding words and sentences via character n-grams. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016b. Towards universal paraphrastic sentence embeddings. In Proceedings of the International Conference on Learning Representations.
John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019a. Beyond bleu: Training neural machine translation with semantic similarity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4344–4355.
John Wieting and Kevin Gimpel. 2017a. Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. arXiv preprint arXiv:1711.05732.
John Wieting and Kevin Gimpel. 2017b. Revisiting recurrent networks for paraphrastic sentence embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2078–2088, Vancouver, Canada.
John Wieting and Kevin Gimpel. 2018. ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462. Association for Computational Linguistics.
John Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-Kirkpatrick. 2019b. Simple and effective paraphrastic similarity from parallel translations. Proceedings of the ACL.
John Wieting, Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 274–285, Copenhagen, Denmark.
John Wieting, Graham Neubig, and Taylor BergKirkpatrick. 2019c. A bilingual generative transformer for semantic sentence embedding. arXiv preprint arXiv:1911.03895.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV).

