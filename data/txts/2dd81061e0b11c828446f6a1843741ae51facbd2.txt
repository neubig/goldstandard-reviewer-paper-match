arXiv:2110.14549v1 [q-bio.NC] 27 Oct 2021

Latent Equilibrium: A uniﬁed learning theory for arbitrarily fast computation with arbitrarily slow neurons
Paul Haider1 Benjamin Ellenberger1 Laura Kriener1 Jakob Jordan1 Walter Senn1,* Mihai A. Petrovici1,2,*
1Department of Physiology, University of Bern 2Kirchhoff-Institute for Physics, Heidelberg University
{first_name}.{surname}@unibe.ch
Abstract
The response time of physical computational elements is ﬁnite, and neurons are no exception. In hierarchical models of cortical networks each layer thus introduces a response lag. This inherent property of physical dynamical systems results in delayed processing of stimuli and causes a timing mismatch between network output and instructive signals, thus afﬂicting not only inference, but also learning. We introduce Latent Equilibrium, a new framework for inference and learning in networks of slow components which avoids these issues by harnessing the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. We jointly derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. The resulting model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. We demonstrate successful learning of standard benchmark datasets, achieving competitive performance using both fully-connected and convolutional architectures, and show how our principle can be applied to detailed models of cortical microcircuitry. Furthermore, we study the robustness of our model to spatio-temporal substrate imperfections to demonstrate its feasibility for physical realization, be it in vivo or in silico.
1 Introduction
Physical systems composed of large collections of simple, but intricately connected elements can exhibit powerful collective computational properties. A prime example are animals’ nervous systems, and most prominently the human brain. Its computational prowess has motivated a large, crossdisciplinary and ongoing endeavor to emulate aspects of its structure and dynamics in artiﬁcial substrates, with the aim of being ultimately able to replicate its function. The speed of information processing in such a system depends on the response time of its components; for neurons, for example, it can be the integration time scale determined by their membrane time constant.
*Senior authors.
35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

If we consider hierarchically organized neural networks composed of such elements, each layer in the hierarchy causes a response lag with respect to a changing stimulus. This lag introduces two related critical issues. For one, the inference speed in these systems decreases with their depth. In turn, this induces timing mismatches between instructive signals and neural activity, which disrupts learning. For example, recent proposals for bio-plausible implementations of error backpropagation (BP) [1–4] in the brain all require some form of relaxation, both for inference and during learning [5–11]. Notably, this also affects some purely algorithmic methods involving auxiliary variables [12]. To deal with this inherent property of physical dynamical systems, two approaches have been suggested: either phased plasticity that is active only following a certain relaxation period, or long stimulus presentation times with small learning rates. Both of these solutions entail signiﬁcant drawbacks: the former is challenging to implement in asynchronous, distributed systems such as cortical networks or neuromorphic hardware, while the latter results, by construction, in slow learning. This has prompted the critique that any algorithm requiring such a settling process is too slow to describe complex brain function, particularly when involving real-time responses [13]. To the best of our knowledge this fundamental problem affects all modern models of approximate BP in biological substrates [5–11].
To overcome these issues, we propose a novel framework for fast computation and learning in physical substrates with slow components. As we show below, this framework jointly addresses multiple aspects of neuronal computation, including neuron morphology, membrane dynamics, synaptic plasticity and network structure. In particular, it provides a biologically plausible approximation of BP in deep cortical networks with continuous-time, leaky neuronal dynamics and local, continuous plasticity. Moreover, our model is easy to implement in both software and hardware and is well-suited for distributed, asynchronous systems.
In our framework, inference can be arbitrarily fast (up to ﬁnite simulation resolution or ﬁnite communication speed across physical distances) despite a ﬁnite response time of individual system components; downstream responses to input changes thus become effectively instantaneous. Conversely, responses to instructive top-down input that generate local error signals are also near-instantaneous, thus effectively removing the need for any relaxation phase. This allows truly phase-free learning from signals that change on much faster time scales than the response speed of individual network components.
Similarly to other approaches [5, 6, 9, 14, 15], we derive neuron and synapse dynamics from a joint energy function. However, our energy function is designed to effectively disentangle these dynamics, thus removing the disruptive co-dependencies that otherwise arise during relaxation. This is achieved by introducing a simple, but crucial new ingredient: neuronal outputs that try to guess their future state based on their current information, a property we describe as “prospective” (which should not be confused with the “predictive” in predictive coding, as we also discuss below). Thereby, our framework also constructs an intimate relationship between such “slow” neuronal networks and artiﬁcial neural network (ANN)1, thus enabling the application of various auxiliary methods from deep learning.
2 The problems of slow components
To illustrate the issue with relaxation, we consider two neurons arranged in a chain (Fig. 1a). Biological neuronal membrane potentials u are conventionally modeled as leaky integrators of their input I: Cmu˙ = −glu + I, where the membrane capacitance Cm and leak conductance gl determine the membrane time constant τ m := Cm/gl and thereby its response speed. These dynamics attenuate and delay input, resulting in signiﬁcant differences between neuronal output rates and those expected in an instantaneous system (Fig. 1b). This mismatch increases with every additional layer: a feedforward network with n layers has an effective relaxation time constant of approximately nτ m.
Besides slow inference, this delayed response leads to critical issues during learning from downstream instructive signals. Consider the typical scenario where such a target signal is present in the output layer and plasticity is continuously active (and not phased according to some complicated schedule). If the system fulﬁlls its task correctly, the target signal corresponds to the output of the relaxed system and no learning should take place. However, due to delays in neuronal responses, the output signal differs from the target during relaxation, which causes plasticity to adapt synaptic weights in an effort to better match the target. As a result, the system “overshoots” during early relaxation and has to
1To differentiate between biologically plausible, leaky neurons and abstract neurons with instantaneous response, we respectively use the terms “neuronal” and “neural”.
2

a
𝑟𝑟2 𝑢𝑢2 𝑤𝑤2

b
r2 u2

without prospective coding
𝑢𝑢tgt
𝑢𝑢2P 𝑊𝑊2P,1P

r1

𝑒𝑒1

𝑟𝑟2

u1 𝑢𝑢2

𝑟𝑟1 𝑢𝑢1
𝑤𝑤1
𝑟𝑟in

rin

𝑤𝑤𝜑𝜑 𝑢�𝑢
2

0

𝑒𝑒
𝜏𝜏𝑢𝑢̇ = −𝑢𝑢 +𝑾𝑾𝑾𝑾 + 𝑒𝑒
𝑾𝑾𝑾𝑾
20

𝑢𝑢1I 𝑊𝑊1I,P1

𝐵𝐵1P,P2 𝑊𝑊1P,1I

𝑢𝑢1P

𝑊𝑊1P,0P

40

60

time [ms]

e
80 0

with prospective coding

𝑢𝑢tgt

20

40

time [ms]

𝑢𝑢2P 𝑊𝑊2P,1P

60

80

u2 [a.u.]

c 𝑒𝑒 d f 1

w2 r1

𝑒𝑒

η→0

η=0

0

50

100

150

𝜏𝜏𝑢𝑢̇ = −𝑢𝑢

𝑟𝑟1 ww12 𝜑𝜑 𝑢�𝑢 +𝑾𝑾𝑾𝑾 + 𝑒𝑒

𝑢𝑢1

𝐸𝐸𝑖𝑖 = 𝑒𝑒𝑖2𝑖 𝐵𝐵1P,P2 𝑢𝑢1I 𝑡𝑡3
𝑊𝑊IP 𝑊𝑊1P,1I
1,1

𝑡𝑡2 + d𝑡𝑡 𝐸𝐸𝑖𝑖 2

𝑡𝑡1 + d𝑡𝑡

𝑡𝑡2

𝑢𝑢P

𝐸𝐸𝑖𝑖 1

1 𝑢𝑢𝑗𝑗

w [a.u.]

𝑤𝑤1

0

50

100

150

time [ms𝑟]𝑟in

𝑾𝑾𝑾𝑾

𝑡𝑡1

𝑢𝑢𝑖𝑖

𝑡𝑡0

𝑊𝑊1P,0P
𝐸𝐸𝑖𝑖 0 = 0

Figure 1: Prospective coding solves the relaxation problem. (a) A simple, functionally feedforward network of two neurons. Note the recurrence induced by the propagation of both bottom-up signals and top-down errors. (b) Neuronal ﬁring rates (blue) and membrane potentials (purple) for an input sequence consisting of a quickly but smoothly changing stimulus followed by two constant stimuli. Dashed black lines denote instantaneous (relaxed) rates. Red shading highlights the mismatch between instantaneous and actual ﬁring rates that disrupts both inference and learning. (c) Continuous learning during relaxation without prospective coding in the network from (a). Dotted black: target membrane potential. Solid lines: trajectories for vanishing learning rates. Dashed lines: trajectories for nonzero learning rates. Purple: membrane potentials. Note the overshoot when learning is continuously active. Green/red: presynaptic weights of 1st/2nd neuron. (d) Sketch of neuron model derived from Latent Equilibrium (LE). The membrane potential still reacts slowly to input, but the output ﬁring rate uses the prospective membrane potential u˘m. (e) Same as (b), but with LE. Note the instantaneous reaction to input changes of the second neuron. (f) Sketch of the mismatch energy Ei for a hidden neuron during learning for an input with several jumps (for clarity, we assume the jumps to be upwards for consecutive inputs). In LE (orange), the energy itself jumps with the input, but then remains constant while neuron dynamics (ui, uj) evolve. Without LE (purple), the energy changes transiently and plasticity follows incorrect gradients before relaxation. The trajectories for E = 0 correspond to the scenario from (c). Planes of constant energy are drawn for visual guidance and do not represent constant-energy manifolds.

undo these synaptic changes through further learning in the late relaxation phase (Fig. 1c). Also,
since output errors need inputs to propagate forward through the entire network before propagating backwards themselves, the input layer only observes correct errors after about 2nτ m. We refer to this issue as the “relaxation problem”. In the following, we present a solution for this problem, combining
prospective neuron dynamics with continuously active, local synaptic plasticity.

3 Fast computations in slow substrates

Inspired by the canonical coordinates from classical mechanics, we describe the state of a neuronal network by its position in phase space (u, u˙ ), with the generalized position u and the generalized momentum u˙ . The relation between these two components describes the physics of the system. To
obtain the leaky integrator dynamics that characterize biological neurons, we ﬁrst deﬁne the abstract network state u˘m as

u˘m := u + τ mu˙ .

(1)

3

We next deﬁne an energy function E that characterizes this state:

E(u˘m) := 1 u˘m − Wϕ(u˘m) − b 2 + βL(u˘m) .

(2)

2

Here, W and b represent the weight matrix and bias vector, ϕ is the neuronal activation function

and the loss L is scaled by a constant β; we use bold font for matrices, vectors, and vector-valued

functions. Intuitively speaking, E measures the difference between “what neurons guess that they will be doing in the future” (u˘m) and “what their biases and presynaptic afferents believe they should be doing” (b + W ϕ(u˘m)). Furthermore, for a subset of neurons, it adds a loss L that provides an

external measure of the error for the states of these neurons, determined by instructive signals such as

labels, reconstruction errors or rewards. This formulation of the energy function is very generic and

can, by appropriate choice of parameters, apply to different network topologies, including multilayer

perceptrons, convolutional architectures and recurrent networks. Note that this energy can be written

as a neuron-wise sum over mismatch energies plus, for factorizing loss functions, a local loss term

Ei(u˘mi , ri,pre)

=

1 2

(u˘mi

−

Wiri,pre

−

bi)2

+ βLi (u˘mi ),

where

ri,pre

:=

ϕ(u˘mi,pre)

is

the

presynaptic

input vector for the ith neuron in the network.

We can now derive neuronal dynamics as extrema of the energy function from Eqn. 2:

∇u˘m E = 0 =⇒ τ mu˙ = −u + Wϕ(u˘m) + b + e ,

(3)

with presynaptic bottom-up input Wϕ(u˘m) and top-down error signals

e = ϕ (u˘m)W T [u˘m − Wϕ(u˘m) − b]

(4)

for hidden neurons and e = −β∇u˘m L for neurons which directly contribute to the loss. Plugging Eqn. 3 into Eqn. 4, it is easy to see that in hierarchical networks these errors can be expressed recursively over layers , e = ϕ (u˘m)W T+1e +1, thus instantiating a variant of BP. Eqn. 3 can be interpreted as the dynamics of a structured pyramidal neuron receiving presynaptic, bottom-up input via its basal dendrites and top-down input via its apical tree (Fig. 1d). We provide a more detailed description of our model’s biophysical implementation in Section 5. We note that our approach is in contrast to previous work that introduced neuron dynamics via gradient descent on an energy function, such as [5, 6, 16], whereas we require a stationary energy function with respect to u˘m. Indeed, this difference is crucial for solving the relaxation problem, as discussed below. Since for a given input our network moves, by construction, within a constant-energy manifold, we refer to our model as Latent Equilibrium (LE).
We can now revisit our choice of u˘m from a functional point of view. Instead of the classical output rate ϕ(u), our neurons ﬁre with ϕ(u˘m), which depends on both u and u˙ (Eqn. 1). As neuron membranes are low-pass ﬁlters (Eqn. 3), u˘m can be viewed as a prospective version of u: when ﬁring, the neuron uses its currently available information to forecast the state of its membrane potential after relaxation. The prospective nature of u˘m also holds in a strict mathematical sense: the breve operator ˘m := (1 + τ md/dt) is the exact inverse of an exponential low-pass ﬁlter (see SI). While neuronal membranes continue to relax slowly towards their steady states, neuronal outputs use membrane momenta to compute a correct instantaneous reaction to their inputs, even in the case of jumps (Fig. 1e). Thus, information can propagate instantaneously throughout the network, similarly to an ANN, counterintuitively even when membrane dynamics are never in equilibrium. The activity of the output layer hence reﬂects arbitrarily fast changes in the input – even on time scales smaller than the neuronal time constant τ m – rather than responding with a signiﬁcant time lag and attenuation as in previous, gradient-based models.

The idea of incorporating derivatives into the input-output function of a system has a long history in control theory [17] and also represents a known, though often neglected feature of (single) biological neurons [18, 19]. A related, but different form of neuronal prospectivity has also been considered in other models of bio-plausible BP derived from a stationary action [20, 21]. At the level of neuronal populations with additive Gaussian noise, there exists a long tradition of studying faster-than-τ m responses, both with [22] and without [23] recurrent connectivity. Similar observations also hold for single neurons in the presence of noise [24, 25]. Building on these insights and integrating them into a uniﬁed theory of neuronal dynamics and learning, our model proposes a speciﬁc form of prospective coding that can also be learned by local adaptation mechanisms, as we discuss in Section 6.

We should also stress the difference between the terms “prospective” and “predictive”. Predictive coding, or more generally, predictive processing, is a theory of brain function which proposes that

4

brains maintain internal models of their environment which they update and learn by trying to predict sensory input at the same moment in time. Originally based on a speciﬁc Bayesian model and error communication scheme [14], the notion of predictive coding can be generalized to layers in a hierarchical model predicting the activities of subsequent layers. This principle is instantiated in our networks as well, as will become clear in the following sections. In contrast, prospective coding in our framework refers to the ability of neurons to “look forward in time” using the current state of the membrane potential in phase space (position and velocity), as described above.2 Prospective and predictive coding are thus complementary: our speciﬁc prospective mechanism provides a cure for relaxation in predictive coding networks, thus signiﬁcantly speeding up both learning and inference, as we describe below.
Note that, even for functionally feedforward networks, our resulting network structure is recurrent, with backward coupling induced by error inputs to the apical tree. As a non-linear recurrent network, it cannot settle instantaneously into the correct state; rather, in numerical simulations, it jumps quickly towards an estimated stationary activity state and reaches equilibrium within several such jumps (of inﬁnitesimal duration). In practice, saturating activation functions can help avoid pathological behavior under strong coupling. Moreover, we can introduce a very short exponential low-pass ﬁlter τ s on top-down signals, slightly larger than the temporal resolution of the simulation. Thus, in physical systems operating in continuous time, τ s can effectively become inﬁnitesimal as well and does not affect the speed of information propagation through the network. In particular, as we discuss below, the perpetual consistency between input and output allows our model to continuously learn to reduce the loss, obviating the need for network relaxation phases and the associated global control of precisely timed plasticity mechanisms.

4 Fast learning in slow substrates

Based on our prospective energy function (Eqn. 2), we deﬁne synaptic weight dynamics, i.e., learning, as time-continuous stochastic gradient descent with learning rate ηW :

W˙ ∝ −∇W E =⇒ W˙ = ηW [u˘m − W r − b] rT .

(5)

Thus, weights evolve continuously in time driven by local error signals without requiring any particular schedule. Neuronal biases are adapted according to the same principle. Note that this rule only uses quantities that are available at the locus of the synapse (see also Section 5). Intuitively, this locality is enabled by the recurrent nature of the network: errors in the output units spread throughout the system, attributing credit locally through changes in neuronal membrane potentials. These changes are then used by synapses to update their weight in order to reduce the network loss. However, our learning rule is not an exact replica of BP, although it does approximate it in the limit of inﬁnitely weak supervision β → 0 (often referred to as “nudging”); strictly speaking, it minimizes the energy function E, which implicitly minimizes the loss L. This form of credit assignment can be related to previous models which similarly avoid a separate artiﬁcial backward pass (as necessary in classical BP) by allowing errors to inﬂuence neuronal activity [27]. Plasticity in the weights projecting to output neurons depends on the choice of L; for example, for an L2 loss, plasticity in the output layer corresponds to the classical delta rule [28]: W˙ N = ηW β [rN∗ − rN ] rNT −1.
Despite similarities to previous work, learning in our framework does not suffer from many of the shortcomings that we have already noted. Since activity propagates quasi-instantaneously throughout the network, our plasticity can be continuously active without disrupting learning performance. This is true by construction and most easily visualized for a sequence of (piecewise constant) input patterns: following a change in the input, membrane dynamics take place in a constant-energy manifold (Eqn. 3) across which synaptic weight dynamics remain unchanged, i.e., they equally and simultaneously pull downward on all points of this manifold (Fig. 1f). This disentangling of membrane and synaptic weight dynamics constitutes the crucial difference to previous work, where the neuronal mismatch energies Ei change as dynamics evolve and thus can not represent the true errors in the network before reaching a ﬁxed point. We further note that LE also alleviates the problem of unlearning in these other models: due to the misrepresentation of errors during relaxation, continuous learning

2A similar concept has also been discussed in [26], but with the aim of implementing the future discounted states required for temporal difference learning; this form of prospectivity thus addresses a different problem and results in very different neuronal and synaptic dynamics.

5

validation error [%] membrane pot. [a.u.] validation error [%]

a
1

β = 0, η = 0

β = 0, η = 0

102 b

0

101

−1 teacher u˘∗ u˘m

u
0.0 0.1 0.2 0.3 0.4 0.5 0.6

100

0

20

time [s]

60 c

d e 100
HIGGS (FC)

100 MNIST (ConvNet)

50

BP

LE

BP

80

LE

40

10

60

MNIST (FC)
BP with SGD w/o prosp. coding LE with FA LE τ m → 10 τ m

40

60

epochs

80

100

CIFAR10 (ConvNet)
BP LE

30

40

20

1

20

0

50

100 0

50

100 0

20

40

60

80 100

epochs

epochs

epochs

Figure 2: Learning with LE. (a) Fourier synthesis with fully connected (FC) network of size 50-30-1, receiving
sinusoidal inputs with equidistant frequencies between 10 and 510 Hz. Teaching signal only present for the ﬁrst 0.4 s, plasticity is continuously active. (b) MNIST dataset with FC network of size 784-300-100-10, presentation times Tpres = 1 ms = 0.05 τ m per sample. Since variability for runs with different τ m is statistically indistinguishable, we only show one set of corresponding min/max values. For comparison, we also show the performance with feedback alignment (FA) [31]. (c) HIGGS dataset with FC network of size 28-300-300-300-1. (d) MNIST with convolutional network (ConvNet) LeNet-5 [32]. (e) CIFAR-10 dataset (ConvNet: LeNet-5).
For all examples, standard ANNs with the same topology trained with classical BP are shown for comparison.

changes synaptic weights even in perfectly trained networks (Fig. 1c). This disruption of existing memories is related, but not identical to catastrophic forgetting.
To illustrate time-continuous learning on continuous data streams, we ﬁrst consider a simple Fouriersynthesis task in which a network driven by oscillating inputs learns to approximate a periodic (here: sawtooth) target signal. Despite the wavelengths of both input and target being much smaller than the time constant of the neuron, continuous plasticity in our model allows the output neuron to approximate the target well (Fig. 2a). The remaining differences merely reﬂect the fundamental limitation of a ﬁnite set of input frequencies and hidden units.
We next turn the more traditional paradigm of classiﬁcation tasks to demonstrate the scalability of our model. We train feedforward networks using LE on several standard benchmark datasets (see SI for details) using sample presentation times much shorter than the membrane time constant. We ﬁrst learn MNIST [29] and HIGGS [30] with a fully connected architecture. After 100 epochs, our model reaches classiﬁcation test errors (mean ± std) of (1.98 ± 0.11) % (MNIST) and (27.6 ± 0.4) % (HIGGS), on par with a standard ANN trained using stochastic gradient descent (SGD) and reaching (1.93 ± 0.14) % (MNIST) and (27.8 ± 0.4) % (HIGGS) with the same architecture, i.e., with the same number of learnable parameters (Fig. 2b,c). With feedback alignment (FA, [31]), we achieve test errors of (2.6 ± 0.1) % for MNIST. To illustrate the indifference of LE with respect to neuronal time constants, we repeated the MNIST experiments with signiﬁcantly slower neuronal dynamics, obtaining the same results. In contrast, a network following the classical paradigm without prospective rates performs poorly: for membrane time constants of τ m = 10 ms and presentation times of T = 100 ms per sample, the network does not exceed 90% accuracy on MNIST. For even shorter presentation times, the performance of such models quickly degrades further.
Since our model does not assume any speciﬁc connectivity pattern, we can easily integrate different network topologies. Here we demonstrate this by introducing convolutional architectures on both MNIST and the more challenging CIFAR10 [33] datasets. On these datasets, our LE networks achieve test errors of (1.1 ± 0.1) % (MNIST) and (38.0 ± 1.3) % (CIFAR10), again on par with ANNs with identical structure at (1.08 ± 0.07) % (MNIST) and (39.4 ± 5.6) % (CIFAR10) (Fig. 2d,e). These results show that LE enables time-continuous learning using arbitrarily short presentation times in networks of leaky integrator neurons to achieve results competitive with standard BP in ANNs.
6

5 Fast computation and learning in cortical microcircuits

Due to the simplicity of their implementation, the principles of LE can be applied to models of approximate BP in the brain in order to alleviate the issues discussed above. Here we demonstrate how a network of hierarchically organized dendritic microcircuits [8, 34] can make use of our theoretical advances to signiﬁcantly increase both inference and training speed, thus removing several critical shortcomings towards its viability as a scalable model of cortical processing. The resulting dynamical system represents a detailed and biologically plausible version of BP, with real-time dynamics, and phase-free, continual local learning able to operate on effectively arbitrary sensory timescales.
The fundamental building block of this architecture is a cortical microcircuit model consisting of pyramidal cells and interneurons (Fig. 3a,b). Pyramidal cells have three compartments: a basal dendrite receiving bottom-up input from lower areas, an apical dendrite receiving top-down input from higher areas and lateral input from interneurons, and a somatic compartment that integrates dendritic information and generates the neuronal output. Interneurons consist of two compartments: a basal dendrite receiving input from pyramidal cells in the same layer, and a somatic compartment that receives input from pyramidal cells in higher layers. Pyramidal somatic compartments are leaky integrators of input from neighboring compartments (see SI for the full set of equations):

Cmu˙ siom = gl (El − usiom) + gbas vibas − usiom + gapi viapi − usiom ,

(6)

where i is the neuron index, El the leak potential, vibas and viapi the basal and apical membrane potentials, respectively, and gbas and gapi the dendro-somatic couplings. Due to the conductancebased interaction between compartments, the effective time constant of the soma is τ eff := Cm/(gl + gbas + gapi). For somatic membrane potentials, assuming that apical dendrites encode errors (see
below) and basal dendrites represent the input, this corresponds directly to Eqn. 3. Following [35],
plasticity in basal synapses is driven by the local error signal given by the discrepancy between

output
a outpoututput

b 𝑣𝑣𝑗a𝑗 pi 𝑣𝑣𝑗a𝑗 pi

100
d
80

MC by Sacramento et al. MC with LE

validation error [%]

c inputinput
ihnopruiztontal vertical diagonal

𝑔𝑔api 𝑔𝑔api 𝑟𝑟𝑗𝑗 𝑟𝑟𝑗𝑗 𝑢𝑢𝑗s𝑗 om 𝑢𝑢som
𝑔𝑔bas 𝑗𝑗
bas 𝑔𝑔bas
𝑣𝑣𝑗𝑗
𝑣𝑣𝑗b𝑗 as

𝑣𝑣𝑖a𝑖 pi

𝑣𝑣𝑖a𝑖 pi
𝑔𝑔api

𝑟𝑟𝑖𝑖

𝑢𝑢som

𝑖𝑖

𝑔𝑔bas 𝑔𝑔api

𝑣𝑣𝑖b𝑖 as

𝑟𝑟𝑖𝑖

𝑢𝑢som

𝑖𝑖

𝑔𝑔bas

𝑣𝑣𝑖b𝑖 as

test error [%]

60
40
20
0 0
100

200

Tpres = 1 τeff Tpres = 10 τeff Tpres = 100 τeff

400 600
epochs

800 1000

50

0

10−1

100

101

102

Tpres[τeﬀ ]

Figure 3: LE in cortical microcircuits. (a,b) Model architecture following [8]. Red: pyramidal cells, blue: interneurons; somatic compartments have darker colors. Each soma sends out a single axon that transmits either ϕ(u) (Sacramento et al.) or ϕ(u˘m) (LE). Except for top-down synapses, all synapses are plastic. (c) Synthetic dataset with 8 images grouped in 3 classes used to train a 3-layer network with 9-30-3 pyramidal cells. (d) Model performance during (top) and after (bottom) learning with (blue) and without (orange) LE. Top: Note the similarity in performance gains at the beginning of training, before the disruptive effects of relaxation begin to dominate. For better visualization, ﬂuctuations are smoothed with a sliding window over 10 epochs. Bottom: Model performance (min, max and mean over 10 seeds) after 1000 epochs for different input presentation times.

7

somatic and dendric membrane potentials usiom and vibas:

w˙ ij = ηW ϕ(usiom) − ϕ(αvibas) ϕ(usjom) ,

(7)

which is analogous to Eqn. 5 up to a monotonic transformation on the voltages (see SI).

In this architecture, plasticity serves two purposes. For pyramidal-to-pyramidal feedforward synapses, it implements error-correcting learning as a time-continuous approximation of BP. For pyramidal-tointerneuron synapses, it drives interneurons to mimic their pyramidal partners in the layers above (see also SI). Thus, in a well-trained network, apical compartments of pyramidal cells are at rest, reﬂecting zero error, as top-down and lateral inputs cancel out. When an output error propagates through the network, these two inputs can no longer cancel out and their difference represents the local error ei. This architecture does not rely on the transpose of the forward weight matrix, improving viability for implementation in distributed asynchronous systems. Here, we keep feedback weights ﬁxed, realizing a variant of feedback alignment. In principle, these weights could also be learned in order to further improve the local representation of errors Section 7.

Incorporating the principles of LE is straightforward and requires only that neurons ﬁre prospectively: ϕ(u) → ϕ(u˘eff). While we have already addressed the evidence for prospective neuronal activity, we note that our plasticity also uses these prospective signals, which constitutes an interesting prediction for future in-vivo experiments. We can now compare the behavior and performance of our LEaugmented model to its original archetype. Since large networks using the original model require prohibitively long training times when simulated with full dynamics rather than just their steady state [8], we use small networks and a small synthetic dataset as a benchmark (Fig. 3c).

Our microcircuit model can learn perfect classiﬁcation even for very short presentation times. In
contrast, the original model without the prospective mechanism stagnates at high error rates even for
this simple task. As discussed earlier, this can be traced back to the learning process being disrupted
during relaxation. Without prospective dynamics, the model requires presentation times on the order of 100 τ eff to achieve perfect accuracy (Fig. 3d). In contrast, LE only degrades for presentation times below 0.1 τ eff, which is due to the limited resolution of our numerical integration method. Thus, incorporating LE into cortical microcircuits can bring the required presentation times into biologically
plausible regimes, allowing networks to deal with rich sensory data.

6 Robustness to substrate imperfections
Computer simulations often assume perfectly homogeneous parameters across the network. Models can hence inadvertently rely on this homogeneity, resulting in unpredictable behavior and possibly fatal dysfunction when faced with the physics of analog substrates which are characterized by both heterogeneity in their components as well as temporal perturbation of their dynamics. Therefore, we consider robustness to spatio-temporal noise to represent a necessary prerequisite for any mathematical model aspiring to physical implementation, be it biological or artiﬁcial.
Spatial noise reﬂects the individuality of cortical neurons or the heterogeneity arising from device mismatch in hardware. Here, we focus on the heterogeneity of time constants; in contrast to, for example, variability in synaptic parameters or activation functions, these variations can not be “trained away” by adapting synaptic weights [36–38]. The two time constants that govern neuron dynamics in our model, namely integration (Eqn. 3) and prospective coding (Eqn. 1), previously assumed to be identical, are affected independently by such variability. To differentiate between the two, we assign the prospective dynamics their own time constant: u˘r := u + τ ru˙ . We can now model heterogeneity as independent, multiplicative Gaussian noise on all time constants: τ m/r → (1 + ξ)τ m/r, with ξ ∼ N (0, στ2); we use multiplicative noise to emphasize that our model is agnostic to absolute time scales, so only relative relationships between speciﬁc time constants matter.
Due to the resulting mismatches between the timing of neuronal input and output, neuronal outputs suffer from exponential transients, leading to relaxation issues similar to the ones we already addressed in detail. However, depending on the transmission of top-down signals, the effects on learning performance can be very different. According to the formal theory, backward errors use the correct prospective voltages: e ∝ [u˘m − W ϕ(u˘r)] (Eqns. 4 and 5); this allows robust learning even for relatively large perturbations in the forward signals (Fig. 4a). In contrast, in biophysical implementations such as the microcircuits discussed above, neurons can only transmit a single output signal ϕ(u˘ri), which consequently also affects the errors: e ∝ [u˘r − W ϕ(u˘r)]. Since deviations due

8

test error [%]

102 a

101

στ = 0 (i.e., τim = τir) στ = 0 without adaptation στ = 0 with adaptation

100 10−2

10−1

100

Tpres[τ m]

τ m [ms]

τ m [ms]

b before
40

20

0

40

after

20

0

0

20 40

τ r [ms]

test error [%]

102 c
101 100
10−1

τ s = 2.0 ms τ s = 0.5 ms τ s = 0.1 ms τ s = 0.0 ms

100

101

102

Tpres[ms]

Figure 4: Robustness of LE. We show test errors on MNIST (min, max and mean over several seeds) after 100 training epochs for different input presentation times. (a) Effects of heterogeneity of time constants with (red) and without (orange) adaptation. Baseline with homogeneous time constants in blue for comparison. (b) Integration and prospective time constant pairs (τ m, τ r) for individual neurons within a network before (top) and after (bottom) development. Colors encode the layer to which the neurons belong. (c) Effects of temporal noise of width σr = 0.2 rmax with (solid) and without (dashed) synaptic ﬁltering for different synaptic time constants.

to a mismatch between integration and prospective time constants persist on the time scale of τ m (see SI), even small amounts of mismatch can lead to a signiﬁcant loss in performance (Fig. 4a).
Here we address this issue by introducing a neuron-local adaptation mechanism that corrects the difference between prospective voltages u˘m and u˘r induced by mismatching time constants:

τ˙ m = ητ (u˘r − W r − b) u˙ .

(8)

In biological substrates, this could, for example, correspond to an adaptation of the transmembrane ion channel densities, whereas on many neuromorphic substrates, neuronal time constants can be adapted individually [39]. Before training the network we allow it to go through a “developmental phase” in which individual neurons are not affected by top-down errors and learn to match their prospective ﬁring to their membrane dynamics (Fig. 4b), thus recovering the performance of networks with perfectly matched time constants (Fig. 4a). This developmental phase merely consists of presenting input samples to the network, for a duration that depends on the required matching precision. Here, we achieved mismatches below 1 ‰ within the equivalent of 20 training epochs. Note that neuronal time constants remain diverse after this phase, but are matched in a way that enables fast reaction of downstream areas to sensory stimuli – an ability that is certainly helpful for survival. This aligns well with in-vivo observations of parameters that are highly diverse between neurons and individuals, but are ﬁne-tuned within each neuron in order to reliably produce a desired input-output relationship [40].
We next model additive temporal noise on neuronal outputs as might be induced, for example, by noisy transmission channels: r → r+ξ, with ξ ∼ N (0, σr2). Formally, this turns membrane dynamics into a Langevin process. These perturbations add up over consecutive layers and can also accumulate over time due to recurrent interactions in the network. This introduces noise to the weight updates that can impair learning performance. Due to their slow integration of inputs, traditional neuron models ﬁlter out this noise, but our prospective mechanism effectively removes this ﬁlter. We thus need an additional denoising mechanism, for which we again turn to biology: by introducing synaptic ﬁltering with a short time constant τ s, synaptic input is denoised before reaching the membrane; formally, r = ϕ(u˘m) is replaced by a low-pass-ﬁltered rs in the energy function (Eqn. 2, see also SI). This narrow ﬁlter also mitigates possible numerical instabilities, as discussed in Section 3.
Networks equipped with synaptic ﬁlters learn reliably even in the presence of signiﬁcant noise levels (Fig. 4c). However, introducing this ﬁlter affects the quasi-instantaneity of computation in our networks, which then require longer input presentation times. Even so, these presentation times need only be “long” with respect to the characteristic time constant of relaxation mismatches – in this case, τ s. Thus, for the typical scenario of white noise described above, minuscule τ s on and even below biological time scales (see, e.g., [41–43]) can achieve effective denoising, without signiﬁcantly affecting the advantages conferred by prospective coding. In conclusion, the demonstrated robustness of our model to spatial and temporal substrate imperfections introduced by simple, biologically inspired mechanisms, make it a promising candidate for implementation in analog physical systems.

9

7 Implications and limitations
In this section, we brieﬂy address several interesting questions that arise from physical embeddings of LE, both in vivo and in silico.
Within the context of LE, the microcircuit model used to implement error backpropagation carries several implications for cortical phenomenology beyond speciﬁc connectivity patterns. In particular, we expect that during active, attentive sensory processing, both during and after learning, cortical pyramidal cells and inhibitory interneurons will react simultaneously to changes in the sensory stimulus. Furthermore, we would also expect such synchronization at the level of cross-cortical networks, for example across the ventral visual stream. This should be observable in large-scale activity recordings with sufﬁcient temporal resolution, for example in iEEG data.
A further implication of our framework is that plasticity is, in principle, equanimous about the speed of sensory input changes. In particular, learning should be possible without neurons ever reaching a steady state. We would argue that the ability of mammals to learn from a continuously, and often quickly changing input stream already provides evidence for such quick processing and learning. In fact, stimulus presentation times for humans in a subliminal face-word association paradigm can be as short as 17 ms and still induce association learning [44]. Furthermore, we expect in-vivo plasticity in cortical subnetworks responsible for such forms of pattern recognition to explicitly depend on prospective neuronal states; this would contrast with other paradigms that only propose a dependence on instantaneous (e.g., [35]) or even low-pass-ﬁltered (e.g., [45]) state variables.
By explicitly approximating error backpropagation, our framework inherits its challenges with respect to biological plausibility, in particular the weight transport problem. In part, this is addressed by feedback alignment, as used in our cortical microcircuits. However, these weights can themselves be plastic in order to further boost learning performance [46–49]. The representation of errors by nudging neuronal activity also has the effect of diluting error signals (in addition to the vanishing gradient problem); this could be mitigated by adapting learning rates as a function of layer identity.
One major aspect of biology that our framework does not explicitly address is the spike-based communication between neurons. In silico, this represents less of an obstacle, because pulsed communication packets can easily carry more information than just the pulse arrival times by including additional payload. In vivo, a similar role could be played by inter-spike-intervals within spike doublets or bursts, or by the precise spike timing used in, for example, spike latency codes [50].
With respect to neuromorphic implementation, we should stress that our robustness analysis only represents a starting point and not a quantitatively faithful study of the expected effects in analog/mixedsignal hardware. While the investigated forms of spatiotemporal noise certainly are among the most salient, there are many other substrate-induced distortive effects to be considered, such as neuronal delays, limited bandwidth or limited synaptic real-estate [51, 52]. Ultimately, an actual demonstration in silico will have to be the deﬁnitive arbiter of the neuromorphic feasibility of LE.
8 Conclusion
We have introduced a new framework for inference and learning in physical systems composed of computational elements with ﬁnite response times. Our model rests on four simple axioms: prospective coding (Eqn. 1), neuronal mismatch energy (Eqn. 2), energy conservation under neuronal dynamics (Eqn. 3) and gradient descent on the energy under synaptic plasticity (Eqn. 5). In particular, incorporating the simple, biologically inspired mechanism of prospective coding allows us to avoid critical issues and scalability bottlenecks inherent to many current models of approximate BP in cortex. Furthermore, we have demonstrated robustness of the resulting implementations to substrate imperfections, a prerequisite for deployment in analog neuronal systems, both biological and artiﬁcial.
Our framework carries implications both for neuroscience and for the design of neuromorphic hardware. The prospective mechanism described here would allow biological circuits to respond much faster than previously assumed. Furthermore, our framework suggests that both inference and learning take place on prospective, rather than instantaneous neuronal quantities. From a hardware perspective, this lifts the previously perceived limitations of slow analog components (as compared to digital ones) without relinquishing their power efﬁciency.
10

9 Acknowledgements
This work has received funding from the European Union 7th Framework Programme under grant agreement 604102 (HBP), the Horizon 2020 Framework Programme under grant agreements 720270, 785907 and 945539 (HBP), the Swiss National Science Foundation (SNSF, Sinergia grant CRSII5180316) and the Manfred Stärk Foundation. We acknowledge the use of Fenix Infrastructure resources, which are partially funded from the European Union’s Horizon2020 research and innovation programme through the ICEI project under the grant agreement No. 800858. Furthermore, we thank Mathieu Le Douairon, Reinhard Dietrich and the Insel Data Science Center for the usage and outstanding support of their Research HPC Cluster. A very special thank you goes to Ellis, for his uniquely wholesome inﬂuence during the ﬁnal stretch of this work.
11

References
1. Linnainmaa, S. Taylor Expansion of the Accumulated Rounding Error. BIT Numerical Mathematics 16, 146–160 (1976).
2. Werbos, P. J. in System Modeling and Optimization 762–770 (Springer, 1982). 3. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning Representations by Back-
propagating Errors. Nature 323, 533–536 (1986). 4. Ivakhnenko, A. G. & Lapa, V. G. Cybernetic Predicting Devices (1966). 5. Whittington, J. C. R. & Bogacz, R. An Approximation of the Error Backpropagation Algorithm
in a Predictive Coding Network with Local Hebbian Synaptic Plasticity. Neural Computation 29, 1229–1262 (2017). 6. Scellier, B. & Bengio, Y. Equilibrium propagation: Bridging the Gap between Energy-Based Models and Backpropagation. Frontiers in Computational Neuroscience 11, 24 (2017). 7. Guerguiev, J., Lillicrap, T. P. & Richards, B. A. Towards Deep Learning with Segregated Dendrites. eLife 6, e22901 (2017). 8. Sacramento, J., Ponte Costa, R., Bengio, Y. & Senn, W. Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm. Advances in Neural Information Processing Systems 31, 8721–8732 (2018). 9. Song, Y., Lukasiewicz, T., Xu, Z. & Bogacz, R. Can the Brain do Backpropagation? – Exact Implementation of Backpropagation in Predictive Coding Networks. Advances in Neural Information Processing Systems 33, 22566 (2020). 10. Millidge, B., Tschantz, A. & Buckley, C. L. Predictive Coding Approximates Backprop along Arbitrary Computation Graphs. arXiv: 2006.04182 (2020). 11. Millidge, B., Tschantz, A., Buckley, C. L. & Seth, A. Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain. arXiv: 2009.05359 (2020). 12. Choromanska, A. et al. Beyond Backprop: Online Alternating Minimization with Auxiliary Variables. International Conference on Machine Learning 97, 1193–1202 (2019). 13. Bartunov, S. et al. Assessing the Scalability of Biologically-motivated Deep Learning Algorithms and Architectures. arXiv: 1807.04587 (2018). 14. Rao, R. P. N. & Ballard, D. H. Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects. Nature Neuroscience 2, 79–87 (1999). 15. Bogacz, R. A tutorial on the free-energy framework for modelling perception and learning. Journal of mathematical psychology 76, 198–211 (2017). 16. Hopﬁeld, J. J. Neurons with Graded Response Have Collective Computational Properties like Those of Two-State Neurons. Proceedings of the National Academy of Sciences 81, 3088–3092 (1984). 17. Minorsky, N. Directional stability of automatically steered bodies. Journal of the American Society for Naval Engineers 34, 280–309 (1922). 18. Hodgkin, A. L. & Huxley, A. F. A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve. The Journal of Physiology 117, 500–544 (1952). 19. Platkiewicz, J. & Brette, R. Impact of Fast Sodium Channel Inactivation on Spike Threshold Dynamics and Synaptic Integration. PLOS Computational Biology 7 (2011). 20. Dold, D. et al. Lagrangian dynamics of dendritic microcircuits enables real-time backpropagation of errors. Computational and Systems Neuroscience (Cosyne) (2019). 21. Kungl, A. F., Dold, D., Riedler, O., Petrovici, M. A. & Senn, W. Deep reinforcement learning for time-continuous substrates. Neuro Inspired Computational Elements Conference (2020). 22. Van Vreeswijk, C. & Sompolinsky, H. Chaotic Balanced State in a Model of Cortical Circuits. Neural Computation 10, 1321–1371 (1998). 23. Knight, B. W. Dynamics of Encoding in a Population of Neurons. Journal of General Physiology 59, 734–766 (1972). 24. Plesser, H. E. & Gerstner, W. Escape Rate Models for Noisy Integrate-and-Free Neurons. Neurocomputing 32-33, 219–224 (2000). 25. Köndgen, H. et al. The Dynamical Response Properties of Neocortical Neurons to Temporally Modulated Noisy Inputs In Vitro. Cerebral Cortex 18, 2086–2097 (2008).
12

26. Brea, J., Gaál, A. T., Urbanczik, R. & Senn, W. Prospective Coding by Spiking Neurons. PLOS Computational Biology 12 (2016).
27. Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. & Hinton, G. Backpropagation and the Brain. Nature Reviews Neuroscience 21, 335–346 (2020).
28. Widrow, B. & Hoff, M. E. Adaptive Switching Circuits (1960). 29. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-Based Learning Applied to Document
Recognition. Proceedings of the IEEE 86, 2278–2324 (1998). 30. Baldi, P., Sadowski, P. & Whiteson, D. Searching for Exotic Particles in High-energy Physics
with Deep Learning. Nature Communications 5, 1–9 (2014). 31. Lillicrap, T. P., Cownden, D., Tweed, D. B. & Akerman, C. J. Random Synaptic Feedback
Weights Support Error Backpropagation for Deep Learning. Nature Communications 7, 1–10 (2016). 32. LeCun, Y. et al. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation 1, 541–551 (1989). 33. Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images (2009). 34. Sacramento, J., Costa, R. P., Bengio, Y. & Senn, W. Dendritic error backpropagation in deep cortical microcircuits. arXiv:1801.00062 (2017). 35. Urbanczik, R. & Senn, W. Learning by the Dendritic Prediction of Somatic Spiking. Neuron 81, 521–528 (2014). 36. Wunderlich, T. et al. Demonstrating Advantages of Neuromorphic Computation: A Pilot Study. Frontiers in Neuroscience 13 (2019). 37. Billaudelle, S. et al. Versatile Emulation of Spiking Neural Networks on an Accelerated Neuromorphic Substrate. 2020 IEEE International Symposium on Circuits and Systems (2020). 38. Göltz, J. et al. Fast and Energy-Efﬁcient Neuromorphic Deep Learning with First-Spike Times. Nature Machine Intelligence 3, 823–835. ISSN: 2522-5839 (Sept. 2021). 39. Thakur, C. S. et al. Large-scale Neuromorphic Spiking Array Processors: A Quest to Mimic the Brain. Frontiers in Neuroscience 12, 891 (2018). 40. Marder, E. & Goaillard, J.-M. Variability, compensation and homeostasis in neuron and network function. Nature Reviews Neuroscience 7, 563–574 (2006). 41. Spruston, N, Jonas, P & Sakmann, B. Dendritic Glutamate Receptor Channels in Rat Hippocampal CA3 and CA1 Pyramidal Neurons. The Journal of Physiology 482, 325–352 (1995). 42. Bellingham, M. C., Lim, R. & Walmsley, B. Developmental Changes in EPSC Quantal Size and Quantal Content at a Central Glutamatergic Synapse in Rat. The Journal of Physiology 511, 861–869 (1998). 43. Geiger, J. R. P., Lübke, J., Roth, A., Frotscher, M. & Jonas, P. Submillisecond AMPA ReceptorMediated Signaling at a Principal Neuron–Interneuron Synapse. Neuron 18, 1009–1023 (1997). 44. Duss, S. B., Oggier, S., Reber, T. P. & Henke, K. Formation of semantic associations between subliminally presented face-word pairs. Consciousness and cognition 20, 928–935 (2011). 45. Clopath, C., Büsing, L., Vasilaki, E. & Gerstner, W. Connectivity reﬂects coding: a model of voltage-based STDP with homeostasis. Nature neuroscience 13, 344–352 (2010). 46. Akrout, M., Wilson, C., Humphreys, P. C., Lillicrap, T. & Tweed, D. Deep learning without weight transport. arXiv: 1904.05391 (2019). 47. Guerguiev, J., Kording, K. P. & Richards, B. A. Spike-based causal inference for weight alignment. arXiv:1910.01689 (2019). 48. Amit, Y. Deep Learning with Asymmetric Connections and Hebbian Updates. Frontiers in Computational Neuroscience 13, 18 (2019). 49. Kunin, D. et al. Two routes to scalable credit assignment without weight symmetry. International Conference on Machine Learning, 5511–5521 (2020). 50. VanRullen, R., Guyonneau, R. & Thorpe, S. J. Spike times make sense. Trends in neurosciences 28, 1–4 (2005). 51. Pfeil, T. et al. Six networks on a universal neuromorphic computing substrate. Frontiers in neuroscience 7, 11 (2013). 52. Petrovici, M. A. et al. Characterization and Compensation of Network-Level Anomalies in Mixed-Signal Neuromorphic Modeling Platforms. PLOS ONE 9 (2014).
13

Supplementary Information

10 Relation between low-pass ﬁlter and lookahead

In general, the prospective (or lookahead) voltage u˘r that enters the activation function “looks into the future” with a time horizon of τ r, that is in general different from τ m (cf. Eqn. 1):

u˘r(t) := 1 + τ r d u(t) .

(9)

dt

On the other hand, the exponential low-pass ﬁlter (LPF) with time constant τ x of some time-dependent quantity u(t) is given by the average over the past, weighted with an exponential kernel

ux(t) := 1

t
u(t )e(t −t)/τx dt .

(10)

τ x −∞

Usually, τ x is either the membrane or the synaptic time constant. To see what happens for a certain neuron in the case of heterogeneous time constants, (τ m = τ r), we can compute

d um = d 1

t
u(t )e(t −t)/τm dt

(11)

dt

dt τ m −∞

= 1 u(t )e(t −t)/τm

+ t u(t ) ∂ e(t −t)/τm dt

(12)

τm

t =t −∞

∂t

u(t)

= 1 u(t) − 1

t
u(t )e(t −t)/τm dt

(13)

τm

τ m −∞

um (t)

= 1 u(t) − um(t)

(14)

τm

to obtain the general expression

u˘mr (t) = 1 + τ r d um(t) = um + τ r u(t) − um(t)

(15)

dt

τm

= τ r u(t) + τ m − τ r um(t) .

(16)

τm

τm

As mentioned in Section 6, different prospective and membrane time constants lead to deviations that
persist on the time scale of τ m since the second term in Eqn. 15 is still proportional to the low-pass ﬁltered voltage um, which relaxes with the speciﬁc time constant τ m. On the other hand, in case of equal time constants τ m = τ r it immediately follows that lookahead and LPF are inverse operations:

u˘(t) = 1 + τ d u(t) = u(t) + τ u˙ (t) = u(t) .

(17)

dt

11 Detailed derivation of the neuronal dynamics

Eqn. 3 represents the solution for a stationary energy with respect to the prospective voltage u˘m. In the following, we show the detailed derivation for an arbitrary component i:

∂E = ∂ 1

u˘m − Wjkϕ(u˘m) 2

(18)

∂u˘mi ∂u˘mi 2

j

k

j,k

=

u˘m − Wjkϕ(u˘m) ∂ u˘m − Wjlϕ(u˘m)

(19)

j

k ∂u˘i j

l

j,k,l

=

u˘mj − Wjkϕ(u˘mk ) [δij − δilWjlϕ (u˘ml )]

(20)

j,k,l

= u˘mi − Wikϕ(u˘mk ) − ϕ (u˘mi ) WiTj u˘mj − Wjkϕ(u˘mk )

(21)

k

j,k

14

and therefore

0 = ∂E =⇒ τ mu˙ i = −ui + Wikϕ(u˘m) + ϕ (u˘m) W T u˘m − Wjkϕ(u˘m) (22)

∂u˘mi

k

i

ij j

k

k

j,k

11.1 Neuronal dynamics with synaptic ﬁltering

To include synaptic ﬁltering in our theory, we introduce an additional LPF as in Eqn. 10 with time

constant τ s. For this, it is sufﬁcient to replace ﬁring rates with ﬁltered rates, ϕ → ϕs, in the total

energy E:

E(u˘m) := 1 u˘m − W ϕs(u˘m) − b 2 + βL(u˘m) .

(23)

2

Deriving the neuronal dynamics from a vanishing gradient ∇u˘m E as before now yields

τ mu˙ = −u + W ϕs(u˘m) + b + e ,

(24)

where the error term now reads

e = ϕs(u˘m)W T [u˘m − W ϕs(u˘m) − b] .

(25)

These are essentially the same equations as before, just with the rates replaced with their ﬁltered version.

12 General formulation for arbitrary connectivity functions

Here we consider a generalization of the energy function from the main manuscript that includes arbitrary “connectivity functions” f with parameters θ:

E(u˘m) := 1 u˘m − f (ϕ(u˘m), θ) 2 + βL(u˘m) .

(26)

2

Again, we derive neuron dynamics by requiring ∇u˘m E(u˘m) = 0. For simplicity, we compute it

element wise, shown here for a neuron that does not directly contribute to the loss L:

∂E(u˘m) =u˘m − f (ϕ(u˘m), θ) +

∂u˘mi

i

i

∂ 1 u˘m − fj(ϕ(u˘m), θ) 2 ∂u˘mi 2 j

j

=u˘m − f (ϕ(u˘m), θ) − ∂ϕi ∂fj(ϕ(u˘m), θ) u˘m − f (ϕ(u˘m), θ) (27)

i

i

∂u˘mi

∂ϕi

j

j

j

From this we can, for example, obtain the neuron dynamics described in the main manuscript by choosing a linear connectivity function fi = j wijϕ(u˘mj ) + bi.

13 Simulation details

13.1 Gradient-based model

For Fig. 1c we consider the neuron dynamics from the main manuscript, but replace prospective membrane potentials with their instantaneous version. Furthermore, we consider a squared loss with some ﬁxed target t∗. To avoid artiﬁcially introducing an additional mismatch problem, we add an exponential low-pass ﬁlter to the error terms; this prevents the model to reduce weights to zero in the absence of a teacher due to a continuous mismatch between instantaneous bottom-up predictions and slow neuronal responses. This results in the following dynamics for the two neurons:

τ mu˙ 1 = − u1 + w1rin + ϕ (u1)w2(u2 − w2ϕ(u1))

(28)

τ mu˙ 2 = − u2 + w2ϕ(u1) + β(t∗ − u2)

(29)

As in the main manuscript, plasticity is deﬁned as stochastic gradient descent on the energy; as above, we consider low-pass ﬁltered variants of inputs:

∆wi = η(ui − W ri−1)ri−1

(30)

Here we use linear activation functions and the following parameters: dt = 0.001, τ m = 10ms, β ∈ {0, 0.9}, ηW = 0.0005.

15

13.2 Numerical implementation

In order to carry out our simulations we had to discretize the differential equations, which we state

here again for clarity:

τ mu˙

(t)

=

−u

(t)

+

W

ϕ(u˘m(t))

+

ϕ

(

u˘m

(t

))

W

T +1

u˘m+1(t) − W +1ϕ(u˘m(t))

(31)

W˙ (t) = η u˘m(t) − W ϕ(u˘m−1(t)) ϕ(u˘m−1(t)) .

(32)

First, observe that u˙ appears on both sides of the ﬁrst equation: explicitly on the left hand side, and implicitly on the right, in the deﬁnition of u˘m(t) = u(t) + τ mu˙ (t). To resolve this circular dependency, we deﬁne u˘m(t + dt) = u(t) + τ mu˙ (t), which works well for small enough dt.

We ﬁrst consider the neuronal update and use forward Euler to rewrite the derivative as a ﬁnite difference with time step dt

τ m u (t + dt) − u (t) = −u (t) + W ϕ(u˘m(t)) + e (t)

(33)

dt

with

e

(t)

:=

ϕ

(

u˘

m

(

t))

W

T +1

u˘m+1(t) − W +1ϕ(u˘m(t))

(34)

and solve for u (t + dt) to obtain

u (t + dt) = u (t) + dt∆u (t) ,

(35)

where

∆u (t) = 1 [−u (t) + W ϕ(u˘m(t)) + e (t)] .

(36)

τm

Similarly, we use forward Euler for weight dynamics:

W (t + ddt)t − W (t) = η u˘m(t) − W ϕ(u˘m−1(t)) ϕ(u˘m−1(t)) (37)

and

W (t + dt) = W (t) + dt∆W (t) ,

(38)

with

∆W (t) = η u˘m(t) − W ϕ(u˘m−1(t)) ϕ(u˘m−1(t))

(39)

For both e (t) and ∆W (t) it is crucial to combine the u˘m(t) with the input W ϕ(u˘m−1(t)) that was also used in in computing u˘m(t). Pseudo-code for our vanilla implementation can be found in Algorithm 1.

Algorithm 1 Pseudo-code for the multi-layer implementation of Latent Equilibrium (LE)

1: for all layers from 1 (input) to N (output) do

2:

e

(t)

←

(1

−

δ

N )ϕ

(

u˘m

(t

))

W

T +1

(t

)

u˘m+1(t) − W +1(t)ϕ(u˘m(t))

+ δ N etrg(t)

3: ∆u (t) ← τ −1 [−u (t) + W (t)r −1(t) + e (t)]

4: u (t + dt) ← u (t) + dt ∆u (t)

5: u˘ (t + dt) ← u (t) + τ ∆u (t)

6: if synaptic plasticity then

7:

∆W (t) ← η e (t) · ϕ(u˘m−1(t))T

8:

W (t + dt) ← W (t) + dt ∆W (t)

9: end if

10: end for

14 Microcircuit details

The somatic membrane potential of hidden layer pyramidal cells, interneurons, and top-layer pyramidal cells is described by the following differential equations:

Cmu˙ P = gl El − uP + gbas vbas − uP + gapi vapi − uP ,

(40)

Cmu˙ I = gl El − uI + gden vden − uI + inudge, I ,

(41)

Cmu˙ PN = gl El − uPN + gbas vNbas − uPN + inudge, tgt .

(42)

16

𝑒𝑒

𝜏𝜏𝑢𝑢̇ = −𝑢𝑢

𝜑𝜑 𝑢�𝑢

+𝑾𝑾𝑾𝑾 + 𝑒𝑒

𝑾𝑾𝑾𝑾

a

𝑢𝑢tgt

𝑢𝑢2P 𝑊𝑊2P,1P

𝑢𝑢1I 𝑊𝑊1I,P1

𝐵𝐵1P,P2 𝑊𝑊1P,1I

𝑢𝑢1P 𝑊𝑊1P,0P

prosp. voltage

weight

b
0.0
−0.5 0
c
0.0
−0.2 0

W2P,P1 W1I,P1

B1P,P2 −1 · W1P,I1

5

10

time [ms]

5
time [ms]

u˘P2 u˘I1
10

pros. voltage

d

e

0.0

u˘P2

−0.2

utgt

u˘P2 utgt

−0.4

0

5

time [ms]

f

1

0

0

5

time [ms]

W2P,P1 W1I,P1 W2t,g1t

W1P,P0 W2t,g1t

weight

0

2000

4000

time [ms]

Figure 5: Learning to mimic a teacher microcircuit with LE. (a) Microcircuit architecture following [1]. (b) Learning of the lateral weights W1IP,1 and W1P,I1 to implement the self-predicting state. (c) Prospective membrane voltages during learning of the self-predicting state where (in absence of a target) the top-down activity is matched by the activity of the interneuron. (d, e) Comparison between the prospective membrane voltage u˘P2 of the output pyramidal neuron and the target voltage utgt before (d) and after (e) training. (f) Weight evolution
during learning.

Target signals to interneurons and top-layer pyramidal neurons are modeled as a conductance-based input to the respective somatic compartments:

inudge, I = gnudge, I uP+1 − uI , inudge, tgt = gnudge, tgt utgt − uPN .

(43) (44)

The target signal for the top-layer pyramidal is determined by the training set. For the interneurons, the somatic membrane potentials of the pyramidal neurons in the layer above serve as targets. The membrane potentials of the dendritic compartments instantaneously follow their inputs:

vbas

=

W

PP , −1

ϕ

uP−1

,

(45)

vapi = BP,P +1ϕ

uP+1

+

W

PI ,

ϕ

uI

,

(46)

vden

=

W

IP ,

ϕ

uP

.

(47)

All synapses except the top-down connections are plastic. The learning rules are described by

W˙ PP = ηPP ϕ uP − ϕ

gbas

vbas ϕ uP ,

(48)

, −1

gl + gbas + gapi

−1

W˙ IP = ηIP ϕ uI − ϕ gden vden ϕ uP ,

(49)

,

gl + gden

W˙

PI ,

= ηPI

−vapi

ϕ

uI

.

(50)

Here, the weights of the top-down connections BPP are static and random.

The differential equations for the somatic membrane potentials of all neuron types can be rewritten in a simpler form which also increases their numerical stability:

Cmu˙ = 1 ueff − u ,

(51)

τeff

τeff =

Cm

.

gl + gbas/den + gapi/nudge

(52)

17

Here, ueff is the effective reversal potential deﬁned as:

ueff,P = glEl + gbasvbas + gapivapi , gl + gbas + gapi

ueff,I = glEl + gdenvden + gnudge, IuP+1 , gl + gden + gnudge, I

ueff,P = glEl + gbasvNbas + gnudge, tgtutgt

N

gl + gbas + gnudge, tgt

(53) (54) (55)

if a target is provided. If no target is provided to the top-layer pyramidal neurons we assume utgt = ueNff,P and the above equation simpliﬁes to

ueff,P = glEl + gbasvNbas .

N

gl + gbas

(56)

We include LE in the dendritic microcircuit by two simple modiﬁcations. First, the output rate of the neurons must depend on the prospective voltage: ϕ (u) → ϕ (u˘). Note that this includes also the rates in the calculation of dendritic membrane potentials (Eqns. 45 to 47) as well as the plasticity rules (Eqns. 48 to 50). Secondly, the nudging for the interneurons must depend on the prospective voltage of the pyramidal neurons above:

ueff,I = glEl + gdenvden + gnudgeu˘P+1 . gl + gden + gnudge, I

(57)

For the simulation of the cortical networks shown in Fig. 3 and Fig. 5, we use the Euler integration
method. Similarly to the LE networks without microcircuit connectivity, we break the circular dependency of u˘m in an Euler integration step by deﬁning u˘m as a function of previous time steps (see Section 13.2).

Learning is split into two stages: ﬁrst, the learning of the so-called self-predicting state and afterwards the learning of the actual task. The self-predicting state describes a conﬁguration of weights in which, in the absence of target signals provided to the last layer, apical dendrites are always at rest and the somatic membrane potentials of the interneurons match the membrane potentials of the pyramidal neurons in the layer above. In this state, the network is able to correctly transport errors induced by the target signal to the apical compartments of the lower layer neurons.

Here we demonstrate, for a single microcircuit, the learning of the self-predicting state from a random
initialization of weights, by presenting the network with random inputs, no targets to the output layer and using the learning rules given above with ηPP = 0 and ηPI/IP = 0 (Fig. 5 b, c). After
the self-predicting state is learned, the network is taught to reproduce the input-output relationship
produced by a teacher network (Fig. 5 d-f). For the learning of the task we set the learning rates to ηPI = 0 and ηPP/IP = 0. In the main manuscript (Fig. 3) we initialize the weights with

W IP

gbas =

gl + gden

W PP

and

(58)

1,1 gden (gl + gbas) 2,1

W1P,I1 = −B1P,P2 ,

(59)

thereby skipping the ﬁrst learning stage and initializing the network directly in the self-predicting state. The full set of parameters used in Fig. 3 and Fig. 5 can be found in Section 15.2.

15 Parameters
15.1 Parameters used for classiﬁcation experiments shown in Fig. 2
Table 1 lists all the parameters we used for the experiments shown in Fig. 2. This includes HIGGS and MNIST experiments with fully connected (FC) architectures in Fig. 2b and c as well as MNIST and CIFAR-10 experiments employing convolutional networks (ConvNets).
Standard artiﬁcial neural networks (ANNs) were trained with classical backpropagation (BP) using the same network topologies but with cross-entropy (CE) loss instead of the mean squared error (MSE) loss used for the LE experiments.

18

Symbol

Table 1: Neuron, network and training parameters used to produce the results shown in Fig. 2.

Parameter name

Fig. 2a

Fig. 2b

Fig. 2c

Fig. 2d

Fig. 2e

Neuron parameters

τ m [ms] membrane time constant

10

20

10

10

10

τ r [ms] prospective time constant

10

20

10

10

10

τ s [ms] synaptic time constant

0

0

0

0

0

ϕ

activation

tanh

hard sigmoid1

ϕN

output activation

linear

Network parameters
architecture input size hidden layer size

β L
ηw,b [ms−1]

output layer size nudging strength loss
initial weights & biases learning rate layerwise η factors3

FC 50 30
1
uniform2 0.25 –

FC 784 300 100
10
128 × 0.125 1, .2, .1

FC

LeNet-5

LeNet-5

28

28 × 28 × 1 32 × 32 × 3

300

C (5 × 5) × 20 − MP 24

300

C (5 × 5) × 50 − MP 24

300

500

1

10

0.1

MSE

∼ N (µ = 0, σ = 0.05)

64 × 0.125 128 × 0.125 64 × 0.125

1, .2, .1, .1 1, .2, .1 1, .2, .2, .2, .2, .1

Training parameters

dt [ms] Tpres

temporal resolution presentation time

batch size # training epochs # train samples # validation samples # test samples # seeds

0.001 1 dt = 0.001 ms
1 – – – – –

0.01 100 dt = 1 ms
512
50000 10000 10000
10

0.1 20 dt = 2 ms 128
40000 10000 10000
9

0.1 100 dt = 10 ms
512
100 50000 10000 10000 9

0.1 50 dt = 5 ms 128
342000 18000 40000
9

1 By “hard sigmoid” we mean the piecewise linear function ϕ(x) that is obtained clipping a rectiﬁed linear unit (ReLU) to [0, 1]

 0  ϕ(x) = xθ(x) − (x − 1)θ(x − 1) = 1
x

if x ≤ 0 if x ≥ 1 else

, ϕ (x) = 1 if x ∈ [0, 1] 0 else

where θ(x) denotes the Heaviside step function.
2 PyTorch defaults 3 These factors scale the learning rate η for each layer independently. 4 C and MP indicate convolutional and max pooling layers, respectively.

19

Also, we used a ReLU function for the hidden layer activation of the ANNs instead of the hard sigmoid activation that was used for the LE simulations. Furthermore, the BP results for the HIGGS dataset were produced using different activation functions for both hidden and output layers, namely tanh and sigmoidal, respectively. To perform the simulations without prospective coding shown in Fig. 2b we set τ r = 0 ms, τ m = 10 ms and used a temporal resolution of dt = 1 ms to obtain reasonable presentation times of multiple τ m that allow for relaxation. That is to say Tpres = 200 dt = 20 ms = 20 τ m for the purple curve in Fig. 2 compared to presentation times of Tpres = 1 ms = 0.05 τ m used for the LE simulations that employ the prospective coding allowing for much smaller presentation times.
20

15.2 Parameters used for the experiments shown in Fig. 3 and Fig. 5

Table 2: Neuron, network and training parameters used to produce the results using the microcircuit architecture.

Parameter name

Fig. 3

Fig. 5

Neuron parameters
Cm El gl gbas gapi gden gnudge, I gnudge, tgt
τeff ϕ (x)

[ms−1]1 [ms−1] [ms−1] [ms−1] [ms−1] [ms−1]
[ms]

1 0 0.03 0.1 0.06 0.1 0.06 0.06 5.262 log [1 + exp (x)]

1 0 0.03 0.1 0.06 0.1 0.06 0.06 5.262 log [1 + exp (x)]

Network parameters
size input size hidden layer size output layer selfpred. η1PP selfpred. η2PP selfpred. η1IP selfpred. η1PI training η1PP training η2PP training η1IP training η1PI weight init (uniform)

[ms−1] [ms−1] [ms−1] [ms−1] [ms−1] [ms−1] [ms−1] [ms−1]

9 30 3
−
−
−
− 5dt/Tpres 3 1dt/Tpres 3 2dt/Tpres 3
04 [−1, 1]

1 1 1 0 0 40 50 50 10 20 04 [−1, 1]

Training parameters

start in selfpred. state train biases

delay on target signal

selfpred. epochs

training epochs

dt

[ms]

Tpres

yes
no 1dt5 − 1000 0.1 3 dt − 5000 dt

no
no 1dt5 3 500 0.01 100 dt

1 To keep the other variables unitless, except for dynamical time scales, conduc-
tances and learning rates need to have the unit 1 / ms. 2 The effective time constant is calculated from the neurons conductances: τeff =
. Cm
gl +gbas +gapi
3 Learning rates are scaled with varying Tpres. 4 If the network is starting in or has previously learned the self-predicting state
the weights W PI do not need to be adapted. 5 As the effect of a change in the input signal needs as many timesteps as there
are hidden layers to reach the top layer of the network, the target signal needs
to be delayed relative to the input by this amount of time steps.

15.3 Parameters used for the experiments shown in Fig. 4
Parameters not mentioned here explicitly (batch size, number of training, validation and test samples, learning rates, mean and variance for initial weights and biases) were the same as for the LE experiments shown in Fig. 2b (cf. Table 1).

21

Table 3: Additional parameters needed to reproduce the experiments shown in Fig. 4

Symbol Parameter name

Fig. 4a

Fig. 4c

Training parameters
dt temporal resolution Tpres presentation time τ s synaptic time constant
# seeds

0.002 ms − 2.0 ms 100 dt −
4

0.01 ms, 0.05 ms1 20 dt − 1000 dt 0 ms − 2.0ms
3

Noise parameters
στm/r time constant width2 σξ noise width
target LPF

0 τ m/r, 0.01 τ m/r, 0.2 τ m/r −
−

−
0.2 rmax yes3

1 The smaller value was used for the simulations of the green datapoints while the bigger
value was used to obtain the blue and yellow curves. 2 Time constants were clipped, i.e., τ m/r + ξ ∈ [1, 1000], to exclude the unphysical case of
them to become negative. 3 In case of Fig. 4c, an additional LPF with time constant τ trg = N ×τ s where N = # layers
was applied to the target signal. However, this is just an approximation for the N LPFs
that are being applied to the input signal during a forward pass. Yet it helps to reduce
“wrong learning” during the short relaxation phases introduced by the synaptic ﬁltering and can be neglected in the limit τ s → 0.

16 Broader impact
The physical interpretation of our model not only offers a biologically plausible implementation of continuous-time, continuously active neuro-synaptic dynamics, but also outlines a speciﬁc path towards mixed-signal (analog/digital) in-silico implementation. Even with existing technologies, such neuromorphic systems harbor the potential of surpassing their biological archetypes with respect to both energy efﬁciency and speed [2]. In conjunction with the inherent ability of our framework to support the processing of continuous data streams, the reduced power consumption of such devices makes them a prime candidate for the construction of autonomous, embodied learning machines.
While obviously beneﬁcial for research and commercial deployment, one should be aware that improved training efﬁciency carries the risk of deploying ever more intransparent models [3]. Furthermore, along with its obvious beneﬁts, improved, and in particular autonomous AI entails a plethora of far-reaching societal consequences that are the subject of ongoing academic and public debate [4]. Future progress hence needs to be considered carefully and responsibly, and, in particular, properly reﬂected in public policy.
On the path towards understanding and replicating biological intelligence, a corollary beneﬁt for the scientiﬁc community may emerge. Modern machine learning requires enormous amounts of compute, thus largely limiting cutting-edge developments to institutions with the corresponding resources. The envisioned bio-inspired yet also bio-transcendent hardware systems have the potential to drastically increase the overall efﬁciency of custom-designed computational platforms. The resulting decrease in operating costs could thus signiﬁcantly expedite the democratization of AI research.
References
1. Sacramento, J., Ponte Costa, R., Bengio, Y. & Senn, W. Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm. Advances in Neural Information Processing Systems 31, 8721–8732 (2018).
2. Markovic´, D., Mizrahi, A., Querlioz, D. & Grollier, J. Physics for Neuromorphic Computing. Nature Reviews Physics 2, 499–510 (2020).
3. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–623 (2021).

22

4. Futurium (European Commission). Ethics Guidelines for Trustworthy AI https://ec.europa. eu/futurium/en/ai-alliance-consultation.
23

