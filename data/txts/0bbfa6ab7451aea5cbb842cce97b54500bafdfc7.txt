Uncertain Decisions Facilitate Better Preference Learning

arXiv:2106.10394v2 [stat.ML] 28 Oct 2021

Cassidy Laidlaw University of California, Berkeley cassidy_laidlaw@cs.berkeley.edu

Stuart Russell University of California, Berkeley
russell@cs.berkeley.edu

Abstract
Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human’s environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human’s preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the ﬁrst statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity—the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difﬁcult. It also provides a ﬁrst step towards understanding and improving preference learning methods for uncertain and suboptimal humans.

1 Introduction
The problem of inferring human preferences has been studied for decades in ﬁelds such as inverse reinforcement learning (IRL), preference elicitation, and active learning. However, there are still several shortcomings in existing methods for preference learning. Active learning methods require query access to a human; this is infeasible in many purely observational settings and may lead to inaccuracies due to the description-experience gap [1]. IRL is an alternative preference learning tool which requires only observations of human behavior. However, IRL suffers from underspeciﬁcation, i.e. preferences are not precisely identiﬁable from observed behavior [2]. Furthermore, nearly all IRL methods require that the observed human is optimal or noisily optimal at optimizing for their preferences. However, humans are often systematically suboptimal decision makers [3], and accounting for this makes IRL even more underspeciﬁed, since it is hard to tell suboptimal behavior for one set of preferences apart from optimal behavior for another set of preferences [4].
IRL and preference learning from observational data are generally applied in situations where a human is acting under no uncertainty. Given the underspeciﬁcation challenge, one might expect that adding in the possibility of uncertainty in decision making (known as partial observability) would only make preference learning more challenging. Indeed, Choi and Kim [5] and Chinaei and ChaibDraa [6], who worked to apply IRL to partially observable Markov decision processes (POMDPs,
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

Decisions without uncertainty

Decisions under uncertainty

(a) Should I quarantine a traveler with a 100% Should I quarantine a traveler with some

accurate negative test for a dangerous dis- symptoms of a dangerous disease but no test

ease?

results?

(b) Should a person with irrefutable evidence of Should a person with circumstantial evi-

and confession to a crime be convicted?

dence of a crime be convicted?

Figure 1: One of our key ﬁndings is that decisions made under uncertainty can reveal more preferences than clear decisions. Here we give examples of decisions made with and without uncertainty. (a) In the case without uncertainty, nobody would choose to quarantine the traveler, so we cannot distinguish between different people’s preferences. However, in the case with uncertainty, people might decide differently whether to quarantine the traveler depending on their preferences on the tradeoff between individual freedom and public health. This allows us to identify those preferences by observing decisions. (b) Similarly, observing decisions on whether to convict a person under uncertainty reveals preferences about the tradeoff between convicting innocent people and allowing criminals to go free.

where agents act under uncertainty), remarked that the underspeciﬁcation of IRL combined with the intractability of POMDPs made for a very difﬁcult task.
In this work, we ﬁnd that, surprisingly, observing humans making decisions under uncertainty actually makes preference learning easier (see Figure 1). To show this, we analyze a simple setting, where a human decision maker observes some information and must make a binary choice. This is somewhat analogous to supervised learning, where a decision rule is chosen to minimize some loss function over a data distribution. In our formulation, the goal is to learn the human decision maker’s loss function by observing their decisions. Often, in supervised learning, the loss function is simply the 0-1 loss. However, humans may incorporate many other factors into their implicit “loss functions”; they may weight different types of mistakes unequally or incorporate fairness constraints, for instance. One might call this setting “inverse supervised learning,” but it is better described as inverse decision theory (IDT) [7, 8], since the objective is to reverse-engineer only the human’s decision rule and not any learning process used to arrive at it. IDT can be shown to be a special case of partially observable IRL (see Appendix B) but its restricted assumptions allow more analysis than would be possible for IRL in arbitrary POMDPs. However, we believe that the insights we gain from studying IDT should be applicable to POMDPs and uncertain decision making settings in general. We introduce a formal description of IDT in Section 3.
While we hope to provide insight into general reward learning, IDT is also a useful tool in its own right; even in this binary, non-sequential setting, human decisions can reveal important preferences. For example, during a deadly disease outbreak, a government might pass a law to quarantine individuals with a chance of being sick. The decision rule the government uses to choose who to quarantine depends on the relative costs of failing to quarantine a sick person versus accidentally quarantining an uninfected one. In this way, even human decisions where there is a “right” answer are revealing if they are made under uncertainty. This example could distinguish a preference for saving lives versus one for guaranteeing freedom of movement. These preferences on the tradeoff between costs of mistakes are expressed through the loss function that the decision maker optimizes.
In our main results on IDT in Section 4, we ﬁnd that the identiﬁability of a human’s loss function is dependent on whether the decision we observe them making involves uncertainty. If the human faces sufﬁcient uncertainty, we give tight sample complexity bounds on the number of decisions we must observe to identify their loss function, and thus preferences, to any desired precision (Theorem 4.2). On the other hand, if there is no uncertainty—i.e., the correct decision is always obvious— then we show that there is no way to identify the loss function (Theorem 4.11 and Corollary 4.12). Technically, we show that learning the loss function is equivalent to identifying a threshold function over the space of posterior probabilities for which decision is correct given an observation (Figure 2). This threshold can be determined to precision ǫ in Θ(1/(pcǫ)) samples, where pc is the probability density of posterior probabilities around the threshold. In the case where there is no uncertainty in the decision problem, pc = 0 and we demonstrate that the loss function cannot be identiﬁed.
2

These results apply to optimal human decision makers—that is, those who completely minimize their expected loss. When a decision rule or policy is suboptimal, in general their loss function cannot be learned [4, 9]. However, we show that decisions made under uncertainty are also helpful in this case; under certain models of suboptimality, we can still exactly recover the human’s loss function.
We present two such models of suboptimality (see Figure 3). In both, we assume that the decision maker is restricting themselves to choosing a decision rule h in some hypothesis class H, which may not include the optimal decision rule. This framework is similar to that of agnostic supervised learning [10, 11], but solves the inverse problem of determining the loss function given a hypothesis class and decision samples. If the restricted hypothesis class H is known, we show that the loss function can be learned similarly to the optimal case (Theorem 4.7). Our analysis makes a novel connection between Bayesian posterior probabilities and binary hypothesis classes. However, assuming that H is known is a strong assumption; for instance, we might suspect that a decision maker is ignoring some data features but we may not know exactly which features. We formalize this case by assuming that the decision maker could be considering the optimal decision rule in any of a number of hypothesis classes in some family H. This case is more challenging because we may need to identify which hypothesis class the human is using in order to identify their loss function. We show that, assuming a smoothness condition on H, we can still obtain the decision maker’s loss function (Theorem 4.10).
We conclude with a discussion of our results and their implications in Section 5. We extend IDT to more complex loss functions that can depend on certain attributes of the data in addition to the chosen decision; we show that this extension can be used to test for the fairness of a decision rule under certain criteria which were previously difﬁcult to measure. We also compare the implications of IDT for preference learning in uncertain versus clear decision problems. Our work shows that uncertainty is helpful for preference learning and suggests how to exploit this fact.
2 Related Work
Our work builds upon that of Davies [8] and Swartz et al. [7], who ﬁrst introduced inverse decision theory. They describe how to apply IDT to settings in which a doctor makes treatment decisions based on a few binary test outcomes, but provide no statistical analysis. In contrast, we explore when IDT can be expected to succeed in more general cases and how many observed decisions are necessary to infer the loss function. We also analyze cases where the decision maker is suboptimal for their loss function, which are not considered by Davies or Swartz et al.
Inverse reinforcement learning (IRL) [2, 12, 13, 14, 15], also known as inverse optimal control, aims to infer the reward function for an agent acting in a Markov decision process (MDP). Our formulation of IDT can be considered as a special case of IRL in a partially observable MDP (POMDP) with two states and two actions (see Appendix B). Some prior work explored IRL in POMDPs [5, 6] by reducing the POMDP to a belief-state MDP and applying standard IRL algorithms. Our main purpose is not to present improvements to IRL algorithms; rather, we give an analysis of the difference between observable and partially observable settings for preference learning. We begin with the restricted setting of IDT but hope to extend to sequential decision making in the future. We also consider cases where the human decision maker is suboptimal, which previous work did not explore.
Performance metric elicitation (ME) aims to learn a loss function (aka performance metric) by querying a human [16, 17, 18]. ME and other active learning approaches [19, 20, 21, 22] require the ability to actively ask a user for their preference among different loss or reward functions. In contrast, IDT aims to learn the loss function purely by observing a decision maker. Active learning is valuable for some applications, but there are many cases where it is infeasible. Observed decisions are often easier to obtain than expert feedback. Also, active learning may suffer from the description-experience gap [1]; that is, it may be difﬁcult to evaluate in the abstract the comparisons that these methods give as queries to the user, leading to biased results. In contrast, observing human decision making “in the wild” with IDT could lead to a more accurate understanding of human preferences.
Preference and risk elicitation aim to identify people’s preferences between different uncertain or certain choices. A common tool is to ask a person to choose between a lottery (i.e., uncertain payoff) and a guaranteed payoff, or between two lotteries, varying parameters and observing the resulting choices [23, 24, 25]. In our analysis of IDT, decision making under uncertainty can be
3

(a) Uncertain decision pc

(b) Clear decision

(c) Suboptimal decision
Observed decisions Yˆ = 0 Yˆ = 1

PDF of q(X)

0 cˆ

1 c−ǫ c c+ǫ 1 0

10

1

Probability that the ground truth Y = 1

Figure 2: A visualization of three settings for inverse decision theory (IDT), which aims to estimate
c, the parameter of a decision maker’s loss function, given observed decisions yˆ1, . . . , yˆm ∈ {0, 1}. Here, each decision yˆi is plotted against the probability q(xi) = P(Y = 1 | X = xi) that the ground truth (correct) decision Y is 1 given the decision maker’s observation xi. Lemma 4.1 shows that an optimal decision rule assigns yˆi = 1{q(xi) ≥ c}. (a) For uncertain decision problems, IDT can estimate c as the threshold of posterior probabilities q(xi) where the decision switches from 0 to 1 (Section 4.1). If the distribution of q(X) has probability density at least pc on [c − ǫ, c + ǫ], Theorem 4.2 shows we can learn c to precision ǫ with m ≥ O(1/(pcǫ)) samples. (b) When there is no uncertainty in the decision problem, IDT cannot characterize the loss parameter c because the
threshold between positive and negative decisions could be anywhere between 0 and 1 (Section 4.4).
(c) A suboptimal human decision maker does not use an optimal decision rule for any loss parameter
c, but we can often still estimate their preferences (Sections 4.2 and 4.3).

cast as a natural series of choices between lotteries. If we observe enough different lotteries, the decision maker’s preferences can be identiﬁed. On the other hand, if there is no uncertainty, then we only observe choices between guaranteed payoffs and there is little information to characterize preferences.
3 Problem Formulation
We formalize inverse decision theory using decision theory and statistical learning theory. Let D be a distribution over observations X ∈ X and ground truth decisions Y ∈ {0, 1}. We consider an agent that receives an observation X and must make a binary decision Yˆ ∈ {0, 1}. While many decision problems include more than two choices, we consider the binary case to simplify analysis. However, the results are applicable to decisions with larger numbers of choices; assuming irrelevance from independent alternatives (i.e. the independence axiom [26]), a decision among many choices can be reduced to binary choices between pairs of them. We generally assume that D is ﬁxed and known to both the decision maker and the IDT algorithm. Unless otherwise stated, all expectations and probabilities on X and Y are with respect to the distribution D.
We furthermore assume that the agent has chosen a decision rule (or hypothesis) h : X → {0, 1} from some hypothesis class H that minimizes a loss function which depends only on the decision Yˆ = h(X) that was made and the correct decision Y :
h ∈ arg min E(X,Y )∼D [ℓ(h(X), Y )] .
h∈H
In general, the loss function ℓ might depend on the observation X as well; we explore this extension in the context of fair decision making in Section 5.1. Assuming the formulation above, since Y, Yˆ ∈ {0, 1} we can write the loss function ℓ as a matrix C ∈ R2×2 such that ℓ(yˆ, y) = Cyˆy. We denote by RC (h) = E(X,Y )∼D [ℓ(h(X), Y )] the expected loss or “risk” of the hypothesis h with cost matrix C. This cost matrix has four entries, but the following lemma shows that it effectively has only one degree of freedom. Lemma 3.1 (Equivalence of cost matrices). Any cost matrix C = ( CC0100 CC0111 ) is equivalent to a cost matrix C′ = ( 0c 1−0 c ) where c = C10+CC0110−−CC0000−C11 as long as C10 + C01 − C00 − C11 = 0. That is, there are constants a, b ∈ R such that RC (h) = aRC′ (h) + b for all h.
See Appendix A.1 for this and other proofs. Based on Lemma 3.1, from now on, we assume the cost matrix only has one parameter c, which is the cost of a false positive; 1 − c is the cost of a

4

Optimal decision rule for loss parameter c
Optimal rule for c˜

hc hc˜ All decision rules

Optimal
decision rule in H for c

hc
hc˜ H

Optimal in H for c
Optimal in H for c

hc˜ hc

hc hc˜ H
H

(a) Optimal

(b) Known suboptimal

(c) Unknown suboptimal

Figure 3: We analyze IDT for optimal decision makers and two cases of suboptimal decision makers. (a) In the optimal case (Section 4.1), the decision maker chooses the optimal decision rule h for their loss parameter c from all possible rules. (b) In the known suboptimal case (Section 4.2), the decision maker chooses from a restricted hypothesis class H which may not contain the overall best decision
rule. (c) In the unknown suboptimal case (Section 4.3), the decision maker chooses any of several hypothesis classes H ∈ H and then uses the optimal rule within that class, which may not be the
optimal rule amongst all classes. This case is more difﬁcult than (b) because we often need to identify the hypothesis class H in addition to the loss parameter c.

false negative. Intuitively, high values of c indicate a preference for erring towards the decision Yˆ = 0 under uncertainty while low values indicate a preference for erring towards the decision Yˆ = 1. Finally, we assume that making the correct decision is always better than making an incorrect decision, i.e. C00 < C10 and C11 < C01. This implies that 0 < c < 1.
We write ℓc and Rc to denote the loss and risk functions using this loss parameter c. Thus, we can formally deﬁne a binary decision problem:
Deﬁnition 3.2 (Decision problem). A (binary) decision problem is a pair (D, c), where D is a distribution over pairs of observations and correct decisions (X, Y ) ∈ X × {0, 1} and c ∈ (0, 1) is the loss parameter. The decision maker aims to choose a decision rule h : X → {0, 1} that minimizes the risk Rc(h) = E(X,Y )∼D[ℓc(h(X), Y )].
As a running example, we consider the decision problem where an emergency room (ER) doctor needs to decide whether to treat a patient for a heart attack. In this case, the observation X might consist of the patient’s medical records and test results; the correct decision is Y = 1 if the patient is having a heart attack and Y = 0 otherwise; and the made decision is Yˆ = 1 if the doctor treats the patient and Yˆ = 0 if not. In this case, a higher value of c indicates that the doctor places higher cost on accidentally treating a patient not having a heart attack, while a lower value of c indicates the doctor places higher cost on accidentally failing to treat a patient with a heart attack.
In inverse decision theory (IDT), our goal is to determine the loss function the agent is optimizing, which here is equivalent to the parameter c. We assume access to the true distribution D of observations and labels and also a ﬁnite sample of observations and decisions S = {(x1, yˆ1), . . . , (xm, yˆm)} where xi ∼ D i.i.d. and the decisions are made according to the decision rule, i.e. yˆi = h(xi).
Some of our main results concern the effects on IDT of whether or not a decision is made under uncertainty. We now formally characterize such decision problems.
Deﬁnition 3.3 (Decision problems with and without uncertainty). A decision problem (D, c) has no uncertainty if P(X,Y )∼D(Y = 1 | X) ∈ {0, 1} almost surely. The decision problem has uncertainty otherwise.
That is, if it is always the case that, after observing and conditioning on X, either Y = 1 with 100% probability or Y = 0 with 100% probability, then the decision problem has no uncertainty.

4 Identiﬁability and Sample Complexity
We aim to answer two questions about IDT. First, under what assumptions is the loss function identiﬁable? Second, if the loss function is identiﬁable, how large must the sample S be to estimate c to some precision with high probability? We adopt a framework similar to that of probably approximately correct (PAC) learning [27], and aim to calculate a cˆ such that with probability at least 1 − δ with respect to the sample of observed decisions, |cˆ − c| ≤ ǫ. While PAC learning typically focuses

5

on test or prediction error, we instead focus on the estimation error for c. This has multiple advantages. First, it allows for better understanding and prediction of human behavior across distribution shift or in unseen environments [28]. Second, there are cases where we care about the precise tradeoff the decision maker is optimizing for; for instance, in the ER doctor example, there are guidelines on the tradeoff between different types of treatment errors and we may want to determine if doctors’ behavior aligns with these guidelines [3]. Third, if the decision maker is suboptimal for their loss function (explored in Sections 4.2 and 4.3), we may not want to simply replicate the suboptimal decisions, but ﬁnd a better decision rule according to the loss function.
We consider three settings where we would like to estimate c, illustrated in Figure 3. First, we assume that the decision maker is perfectly optimal for their loss function. This is similar to the framework of Swartz et al. [7]. However, moving beyond their analysis, we present properties necessary for identiﬁability and sample complexity rates. Second, we relax the assumption that the decision maker is optimal, and instead assume that they only consider a restricted set of hypotheses H which is known to us. Finally, we remove the assumption that we know the hypothesis class that the decision maker is considering. Instead, we consider a family of hypothesis classes; the decision maker could choose the optimal decision rule within any class, which is not necessarily the optimal decision rule across all classes.
4.1 Optimal decision maker
First, we assume that the decision maker is optimal. In this case, the form of the optimal decision rule is simply the Bayes classiﬁer [29].
Lemma 4.1 (Bayes optimal decision rule). An optimal decision rule h for a decision problem (D, c) is given by h(x) = 1{q(x) ≥ c} where q(x) = P(X,Y )∼D(Y = 1 | X = x) is the posterior probability of class 1 given the observation x.
That is, any optimal decision rule corresponds to a threshold function on the posterior probability q(x), where the threshold is at the loss parameter c. Thus, the strategy for estimating c from a sample of observations and decisions is simple. For each observation xi, we calculate q(xi). Then, we choose any cˆ such that q(xi) ≥ cˆ ⇔ yˆi = 1; that is, cˆ is consistent with the observed data. From statistical learning theory, we know that a threshold function can be PAC learned in O(log(1/δ)/ǫ) samples. However, such learning only guarantees low prediction error of the learned hypothesis. We need stronger conditions to ensure that cˆ is close to the true loss function parameter c. The following theorem states conditions which allow estimation of c to arbitrary precision.
Theorem 4.2 (IDT for optimal decision maker). Let ǫ > 0 and δ > 0. Say that there exists pc > 0 such that P(q(X) ∈ (c, c + ǫ]) ≥ pcǫ and P(q(X) ∈ [c − ǫ, c)) ≥ pcǫ. Let cˆ be chosen to be consistent with the observed decisions as stated above, i.e. q(xi) ≥ cˆ ⇔ yˆi = 1. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples m ≥ logp(c2ǫ/δ) .
The parameter pc can be interpreted as the approximate probability density of q(X) around the threshold c. For instance, the requirements of Theorem 4.2 are satisﬁed if the random variable q(X) has a probability density of at least pc on the interval [c−ρ, c+ρ] for some ρ ≥ ǫ; the requirements of Theorem 4.2 are more general to allow for cases when q(X) does not have a density. The lower the density pc, and thus the probability of observing decisions close to the threshold c, the more difﬁcult inference becomes. Because of this, Theorem 4.2 requires that the decision problem has uncertainty. If the decision problem has no uncertainty according to Deﬁnition 3.3, then q(X) ∈ {0, 1} always, i.e. the distribution of posterior probabilities has mass only at 0 and 1. In this case, pc = 0 for small enough ǫ and Theorem 4.2 cannot be applied. In fact, as we show in Section 4.4, it is impossible to tell what the true loss parameter c when the decision problem lacks uncertainty. Figure 2(a-b) illustrates these results.
4.2 Suboptimal decision maker with known hypothesis class
Next, we consider cases where the decision maker may not be optimal with respect to their loss function. Our model of suboptimality is that the agent only considers decision rules within some hypothesis class H, which may not include the optimal decision rule. This formulation is similar to that of agnostic PAC learning [10, 11]. It can also be considered a case of a restricted “choice set” as deﬁned in the preference learning literature [30, 31]. It can encompass many types of irrationality
6

or suboptimality. For instance, one could assume that the decision maker is ignoring some of the features in x; then H would consist of only decision rules depending on the remaining features. In the ER doctor example, we might assume that H consists of decision rules using only the patient’s blood pressure and heart rate; this models a suboptimal doctor who is unable to use more data to make a treatment decision.
While there are many possible models of suboptimality, this one has distinct advantages for preference learning with IDT. One alternative model is that the decision maker has small excess risk, i.e. Rc(h) ≤ Rc(h∗) + ∆ for some small ∆ where h∗ is the optimal decision rule. However, this deﬁnition precludes identiﬁability even in the inﬁnite sample limit (see Appendix C). Another form of suboptimality could be that the decision maker chooses a decision rule to minimize a surrogate loss rather than the true loss. However, we show in Appendix F that for reasonable surrogate losses this is no different from minimizing the true loss. A ﬁnal alternative model of suboptimality is that the human is noisily optimal; this assumption underlies models like Boltzmann rationality or the Shephard-Luce choice rule [32, 26, 33, 14]. However, these models assume stochastic decision making and also cannot handle systematically suboptimal humans.
In this section we begin by assuming that the restricted hypothesis class H is known; this requires some novel analysis but the resulting identiﬁability conditions and sample complexity are very similar to the optimal case in Section 4.1. In the next section, we consider cases where we are unsure about which restricted hypothesis class the decision maker is considering. Deﬁnition 4.3. A hypothesis class H is monotone if for any h, h′ ∈ H, either h(x) ≥ h′(x) ∀x ∈ X or h(x) ≤ h′(x) ∀x ∈ X .
Deﬁnition 4.4. The optimal subset of a hypothesis class H for a distribution D is deﬁned as
optD(H) = {h ∈ H | ∃c such that h ∈ arg minh∈H Rc(h)}
In this section, we consider hypothesis classes whose optimal subsets are monotone. That is, changing the parameter c has to either ﬂip the optimal decision rule’s output for some observations from 0 to 1, or ﬂip some decisions from 1 to 0. It cannot both change some decisions from 0 to 1 and some from 1 to 0. This assumption is mainly technical; many interesting hypothesis classes naturally have monotone optimal subsets. Any hypothesis class formed by thresholding a function is monotone, i.e H = {h(x) = 1{f (x) ≥ b} | b ∈ R}. Also, the set of decision rules based on a particular subset of the observed features satisﬁes this criterion, since optimal decision rules in this set are thresholds on the posterior probability that Y = 1 given the subset of features.
For hypothesis classes with monotone optimal subsets, we can prove properties that allow for similar analysis to that we introduced in Section 4.1. Let hc denote a decision rule which is optimal for loss parameter c in hypothesis class H. That is, hc ∈ arg minh∈H Rc(h). A key lemma allows us to deﬁne a value similar to the posterior probability we used for analyzing the optimal decision maker.
Lemma 4.5 (Induced posterior probability). Let optD(H) be monotone and deﬁne
qH(x) sup {c ∈ [0, 1] | hc(x) = 1}∪{0} and qH(x) inf {c ∈ [0, 1] | hc(x) = 0}∪{1} .
Then for all x ∈ X , qH(x) = qH(x). Deﬁne the induced posterior probability of H as qH(x) qH(x) = qH(x).
Corollary 4.6. Let hc be any optimal decision rule in H for loss parameter c. Then for any x ∈ X , hc(x) = 1 if qH(x) > c and hc(x) = 0 if qH(x) < c.
Using Lemma 4.5, the problem of IDT again reduces to learning a threshold; this time, any optimal classiﬁer in H is a threshold function on the induced posterior probability qH(X), as shown in Corollary 4.6. Thus, to estimate cˆ, we calculate an induced posterior probability qH(xi) for each observation xi and choose any estimate cˆ such that qH(xi) ≥ cˆ ⇔ yˆi = 1. This allows us to state a theorem equivalent to Theorem 4.2 for the suboptimal case.
Theorem 4.7 (Known suboptimal decision maker). Let ǫ > 0 and δ > 0, and let optD(H) be monotone. Say that there exists pc > 0 such that P(qH(X) ∈ (c, c + ǫ]) ≥ pcǫ and P(qH(X) ∈ [c − ǫ, c)) ≥ pcǫ. Let cˆ be chosen to be consistent with the observed decisions, i.e. qH(xi) ≥ cˆ ⇔ yˆi = 1. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples m ≥ logp(c2ǫ/δ) .
7

4.3 Suboptimal decision maker with unknown hypothesis class

We now analyze the case when the decision maker is suboptimal but we are not sure in what manner.
We model this by considering a family of hypothesis classes H. We assume that the decision maker considers one of these hypothesis classes H ∈ H and then chooses a rule h ∈ arg minh∈H Rc(h). This case is more challenging because we may need to identify H to identify c.

One natural family H consists of hypothesis classes which depend only on some subset of the features:
Hfeat {HS | S ⊆ {1, . . . , n}} where HS h(x) = f (xS ) | f : R|S| → {0, 1} (1)

where xS denotes only the coordinates of x which are in the set S. This models a situation where we believe the decision maker may be ignoring some features, but we are not sure which features are being ignored. Another possibility for H is thresholded linear combinations of the features in x, i.e.
Hlinear {Hw | w ∈ Rn} where Hw h(x) = 1{w⊤x ≥ b} | b ∈ R .
In this case, we assume that the decision maker chooses some weights w for the features arbitrarily but then thresholds the combination optimally. This could model the decision maker under- or overweighting certain features, or also ignoring some (if wj = 0 for some j).

In the high pressure and hectic environment of the ER example, we might assume that the doctor is using only a few pieces of data to decide whether to treat a patient. Here, Hfeat would consist of a hypothesis class with decision rules that depend only on blood pressure and heart rate, a hypothesis class with decision rules that rely on these and also on an ECG, and so on. The difﬁculty of this setting compared to that of Section 4.2 is that the doctor could be using an optimal decision rule within any of these hypothesis classes. Thus, we may need to identify what data the doctor is using in their decision rule in order to identify their loss parameter c.

Estimating the loss parameter c in the unknown hypothesis class case requires an additional assumption on the family of hypothesis classes H, in addition to the monotonicity assumption from Section 4.2. Deﬁnition 4.8. Consider a family of hypothesis classes H. Let h ∈ H ∈ H and H˜ ∈ H. Then the minimum disagreement between h and H˜ is deﬁned as MD(h, H˜) infh˜∈H˜ P h˜(X) = h(X) . Deﬁnition 4.9. A family of hypothesis classes H and hypothesis hc ∈ H ∈ H such that hc ∈ arg minh∈H Rc(h) is α-MD-smooth if optD(H˜) is monotone for every H˜ ∈ H and
∀H˜ ∈ H ∀c′ ∈ (0, 1) MD(hc′ , optD(H˜)) ≤ (1 + α|c′ − c|)MD(hc, optD(H˜)).

While MD-smoothness is not particularly intuitive at ﬁrst, it is necessary in some cases to ensure identiﬁability of the loss parameter c. We present a case in Appendix D.2 where a lack of MDsmoothness precludes identiﬁability.

Theorem 4.10 (Unknown suboptimal decision maker). Let ǫ > 0 and δ > 0. Suppose we observe
decisions from a decision rule hc which is optimal for loss parameter c in hypothesis class H ∈ H. Let hc and H be α-MD-smooth. Furthermore, assume that there exists pc > 0 such that for any ρ ≤ ǫ, P(qH(X) ∈ (c, c + ρ)) ≥ pcρ and P(qH(X) ∈ (c − ρ, c)) ≥ pcρ. Let d ≥ VCdim (∪H∈HH)
be an upper bound on the VC-dimension of the union of all the hypothesis classes in H.

Let hˆcˆ ∈ arg minhˆ∈Hˆ Rcˆ(hˆ) be chosen to be consistent with the observed decisions, i.e. ˆhcˆ(xi) = yˆi for i = 1, . . . , m. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples

m ≥ O˜

α ǫ

+

1 ǫ2

d+lopgc(1/δ) .

Theorem 4.10 requires more decision samples to guarantee low estimation error |cˆ − c|. Unlike Theorems 4.2 and 4.7, the number of samples needed grow with the square of the desired precision 1/ǫ2. There is also a dependence on the VC-dimension of the hypothesis classes H ∈ H, since we are not sure which one the decision maker is considering.
Since our results in this section are highly general, it may be difﬁcult to see how they apply to concrete cases. In Appendix E, we explore the speciﬁc case of IDT in the unknown hypothesis class setting for Hfeat as deﬁned in (1). We give sufﬁcient conditions for MD-smoothness to hold and show that the sample complexity grows only logaramithically with n, the dimension of the observation space X , if the decision maker is relying on a sparse set of features.

8

4.4 Lower bounds
Is there any algorithm which can always determine the loss parameter c to precision ǫ with high probability using fewer samples than required by Theorems 4.2 and 4.7? We show that the answer is no: our previously given sample complexity rates are minimax optimal up to constant factors. We formalize this by considering any generic IDT algorithm, which we represent as a function cˆ : (X × {0, 1})m → (0, 1). The algorithm maps the sample of observations and decisions S to an estimated loss parameter cˆ(S). The algorithm also takes as input the distribution D and in the suboptimal cases the hypothesis class H or family of hypothesis classes H, but we leave this dependence implicit in our notation. First, we consider the optimal (Theorem 4.2) and known suboptimal (Theorem 4.7) cases; since these are nearly identical, we focus on the optimal case.
Theorem 4.11 (Lower bound for optimal decision maker). Fix 0 < ǫ < 1/4, 0 < δ ≤ 1/2, and 0 < pc ≤ 1/8ǫ. Then for any IDT algorithm cˆ(·), there exists a decision problem (D, c) satisfying the conditions of Theorem 4.7 such that m < log8(p1c/ǫ2δ) implies that P(|cˆ(S) − c| ≥ ǫ) > δ.
Corollary 4.12 (Lack of uncertainty precludes identiﬁability). Fix 0 < ǫ < 1/4 and suppose a decision problem (D, c) has no uncertainty. Then for any IDT algorithm cˆ(·), there is a loss parameter c and hypothesis class H such that for any sample size m, P(|cˆ(S) − c| ≥ ǫ) ≥ 1/2.
Corollary 4.12 shows that a lack of uncertainty in the decision problem means that no algorithm can learn the loss parameter c to a non-trivial precision with high probability. Thus, uncertainty is required for IDT to learn the loss parameter c. Since c represents the preferences of the decision maker, decisions made under certainty do not reveal precise preference information. In Appendix D, we explore lower bounds for the unknown suboptimal case (Section 4.3 and Theorem 4.10).

5 Discussion

Now that we have thoroughly analyzed IDT, we explore its applications, implications, and limitations.

5.1 IDT for ﬁne-grained loss functions with applications to fairness

First, we discuss an extension of IDT to loss functions which depend not only on the chosen decision

Yˆ = h(X) and the ground truth Y , but on the observation X as well. In particular, we extend the

formulation of IDT from Section 3 to include loss functions which depend on the observations via

a “sensitive attribute” A ∈ A. We denote the value of the sensitive attribute for an observation x by

a(x). We again assume that the decision maker chooses the optimal decision rule for this extended

loss function:

h ∈ arg minh E(X,Y )∼D[ℓ(h(X), Y, a(X))].

(2)

This optimal decision rule h ∈ H is equivalent to a set of decision rules for every value of A, each

of which is chosen to minimize the conditional risk for observations with that attribute value:

h(x) = ha(x)(x) where ha ∈ arg min E(X,Y )∼D[ℓ(h(X), Y, a) | a(X) = a].
h
In this formulation, each attribute-speciﬁc decision rule ha minimizes an expected loss which only depends on the made and correct decisions h(X) and Y over a conditional distribution. Thus, we can split a sample of decisions into samples for each value of the sensitive attribute and perform IDT separately. This will result in a loss parameter estimate cˆa for each value of a.

Once we have estimated loss parameters for each value of A, we may ask if the decision maker is applying the same loss function across all such values, i.e. if ca = ca′ for any a, a′ ∈ A. If the loss function is not identical for all values of A, i.e. if ca = ca′ , then one might conclude that the decision maker is unfair or discriminatory against observations with certain values of A. For instance, in the ER example, we might be concerned if the doctor is using different loss functions
for patients with and without insurance. Concepts like these have received extensive treatment in the
machine learning fairness literature, which studies criteria for when a decision rule can be considered
“fair.” One such fairness criterion is that of group calibration, also known as sufﬁciency [34, 35, 36]:
Deﬁnition 5.1. A decision rule h : X → {0, 1} for a distribution (X, Y ) ∼ D satisﬁes the group calibration/sufﬁciency fairness criterion if there is a function r : X → R and threshold t ∈ R such that h(x) = 1{r(x) ≥ t} and r satisﬁes Y ⊥⊥ A | r(X).

9

Testing for group calibration is known to be difﬁcult because of the problem of infra-marginality [37]. While complex Bayesian models have previously been used to perform a “threshold test” for group calibration, we can use IDT to directly test this criterion in an observed decision maker:
Lemma 5.2 (Equal loss parameters imply group calibration). Let h be chosen as in (2) where ℓ(yˆ, y, a) = ca if yˆ = 1 and y = 0, ℓ(yˆ, y, a) = 1 − ca if yˆ = 0 and y = 1, and ℓ(yˆ, y, a) = 0 otherwise. Then h satisﬁes group calibration (sufﬁciency) if ca = ca′ for every a, a′ ∈ A. Conversely, if there exist a, a′ ∈ A such that ca = c′a and P(q(X) ∈ (ca, ca′ )) > 0, then h does not satisfy group calibration.
If we can estimate ca for a decision rule h for each a ∈ A, then Lemma 5.2 allows us to immediately determine if h satisﬁes sufﬁciency. The minimax guarantees on the accuracy of IDT may make this approach more attractive than the Bayesian threshold test in many scenarios.
5.2 Suboptimal decision making with and without uncertainty
We have so far compared the effect of decisions made with and without uncertainty on the identiﬁability of preferences; here, we argue that uncertainty also allows for much more expressive models of suboptimality in decision making. In decisions made with certainty, suboptimality can generally only take two forms: either the decision maker is noisy and sometimes randomly makes incorrect decisions, or the decision maker is systematically suboptimal and always makes the wrong decision. Neither seems realistic in the ER doctor example: we would not expect to the doctor to randomly choose not to treat some patients who are clearly having heart attacks, and certainly not expect them to never treat patients having heart attacks. In contrast, the models of suboptimality we have presented for uncertain decisions allow for much more rich and realistic forms of suboptimal decision making, like ignoring certain data or over-/under-weighting evidence. We expect that there are similarly more rich forms of suboptimality for uncertain sequential decision problems.
5.3 Limitations and future work
While this study sheds signiﬁcant light on preference learning for uncertain humans, there are some limitations that may be addressed by future work. First, while we assume the data distribution D of observations X and ground truth decisions Y is known, this is rarely satisﬁed in practice. However, statistics is replete with methods for estimating properties of a data distribution given samples from it. Such methods are beyond the scope of this work, which focuses on the less-studied problem of inferring a decision maker’s loss function. Our work also lacks computational analysis of algorithms for performing IDT. However, such algorithms are likely straightforward; we decide to focus on the statistical properties of IDT, which are more relevant for preference learning in general. Finally, we assume in this work that the decision maker is maximizing expected utility (EU), or equivalently minimizing expected loss. In reality, human decision making may not agree with EU theory; alternative models of decision making under uncertainty such as prospect theory are discussed in the behavioral economics literature [38]. Some work has applied these models to statistical learning [39], but we leave their implications for IDT to future work.
6 Conclusion and Societal Impact
We have presented an analysis of preference learning for uncertain humans through the setting of inverse decision theory. Our principle ﬁndings are that decisions made under uncertainty can reveal more preference information than obvious ones; and, that uncertainty can alleviate underspeciﬁcation in preference learning, even in the case of suboptimal decision making. We hope that this and other work on preference learning will lead to AI systems which better understand human preferences and can thus better fulﬁll them. However, improved understanding of humans could also be applied by malicious actors to manipulate people or invade their privacy. Additionally, building AI systems which learn from human decisions could reproduce racism, sexism, and other harmful biases which are widespread in human decision-making. Despite these concerns, understanding human preferences is important for the long-term positive impact of AI systems. Our work shows that uncertain decisions can be a valuable source of such preference information.
10

Acknowledgments and Disclosure of Funding
We would like to thank Kush Bhatia for valuable discussions, Meena Jagadeesan, Sam Toyer, and Alex Turner for feedback on drafts, and the NeurIPS reviewers for helping us improve the clarity of the paper. This research was supported by the Open Philanthropy Foundation. Cassidy Laidlaw is also supported by a National Defense Science and Engineering Graduate (NDSEG) Fellowship.

References
[1] Ralph Hertwig and Ido Erev. The Description–Experience Gap in Risky Choice. Trends in Cognitive Sciences, 13(12):517–523, December 2009. ISSN 1364-6613. doi: 10.1016/j.tics.2009.09.004. URL https://www.sciencedirect.com/science/article/pii/S1364661309002125 .

[2] Andrew Y. Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. In ICML, volume 1, page 2, 2000.

[3] Sendhil Mullainathan and Ziad Obermeyer. A Machine Learning Approach to Low-Value Health Care: Wasted Tests, Missed Heart Attacks and Mis-predictions. Technical report, National Bureau of Economic Research, 2019.

[4] Stuart Armstrong and Sören Mindermann. Occam’s Razor is Insufﬁcient to Infer the Preferences of Irrational Agents. Advances in Neural Information Processing Systems, 31, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/d89a66c7c80a29b1bdbab0f2a1a94af8- Abstract.html .

[5] Jaedeug Choi and Kee-Eung Kim. Inverse Reinforcement Learning in Partially Observable Environments. Journal of Machine Learning Research, 12(21):691–730, 2011. ISSN 1533-7928. URL http://jmlr.org/papers/v12/choi11a.html .

[6] Hamid R. Chinaei and Brahim Chaib-Draa. An Inverse Reinforcement Learning Algorithm for Partially Observable Domains with Application on Healthcare Dialogue Management. volume 1, pages 144–149, December 2012. doi: 10.1109/ICMLA.2012.31.

[7] Richard J Swartz, Dennis D Cox, Scott B Cantor, Kalatu Davies, and Michele Follen.

Inverse Decision Theory.

Journal of the American Statistical Association, 101(473):

1–8, March 2006.

ISSN 0162-1459.

doi: 10.1198/016214505000000998.

URL

https://amstat.tandfonline.com/doi/abs/10.1198/016214505000000998 .

Publisher:

Taylor & Francis.

[8] Kalatu Davies. Inverse Decision Theory with Medical Applications. PhD thesis, Rice University, Houston, Texas, May 2005. URL https://scholarship.rice.edu/handle/1911/18756.

[9] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference. In International Conference on Machine Learning, pages 5670–5679. PMLR, 2019.

[10] David Haussler.

Decision Theoretic Generalizations of the PAC Model for Neu-

ral Net and Other Learning Applications.

Information and Computation, 100(1):78–

150, September 1992. ISSN 0890-5401. doi: 10.1016/0890-5401(92)90010-D. URL

https://www.sciencedirect.com/science/article/pii/089054019290010D .

[11] Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward Efﬁcient Agnostic Learning. Machine Learning, 17(2):115–141, November 1994. ISSN 1573-0565. doi: 10.1007/BF00993468. URL https://doi.org/10.1007/BF00993468 .

[12] Pieter Abbeel and Andrew Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 1, 2004.

[13] Deepak Ramachandran and Eyal Amir. Bayesian Inverse Reinforcement Learning. In IJCAI, volume 7, pages 2586–2591, 2007.

[14] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum Entropy Inverse Reinforcement Learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.

[15] Justin Fu, Katie Luo, and Sergey Levine. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning. arXiv preprint arXiv:1710.11248, 2017.

11

[16] Gaurush Hiranandani, Shant Boodaghians, Ruta Mehta, and Oluwasanmi Koyejo. Performance Metric Elicitation from Pairwise Classiﬁer Comparisons. arXiv:1806.01827 [cs, stat], January 2019. URL http://arxiv.org/abs/1806.01827. arXiv: 1806.01827.
[17] Gaurush Hiranandani, Shant Boodaghians, Ruta Mehta, and Oluwasanmi O Koyejo. Multiclass Performance Metric Elicitation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 9356–9365. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9133- multiclass- performance- metric- elicitation.pdf .
[18] Gaurush Hiranandani, Harikrishna Narasimhan, and Oluwasanmi Koyejo. Fair Performance Metric Elicitation. arXiv:2006.12732 [cs, stat], November 2020. URL http://arxiv.org/abs/2006.12732. arXiv: 2006.12732.
[19] Erdem Biyik and Dorsa Sadigh. Batch Active Preference-Based Learning of Reward Functions. In Proceedings of The 2nd Conference on Robot Learning, pages 519–528. PMLR, October 2018. URL https://proceedings.mlr.press/v87/biyik18a.html. ISSN: 2640-3498.
[20] Sören Mindermann, Rohin Shah, Adam Gleave, and Dylan Hadﬁeld-Menell. Active Inverse Reward Design. arXiv:1809.03060 [cs, stat], November 2019. URL http://arxiv.org/abs/1809.03060. arXiv: 1809.03060.
[21] Erdem Bıyık, Malayandi Palan, Nicholas C. Landolﬁ, Dylan P. Losey, and Dorsa Sadigh. Asking Easy Questions: A User-Friendly Approach to Active Reward Learning. arXiv:1910.04365 [cs], October 2019. URL http://arxiv.org/abs/1910.04365. arXiv: 1910.04365.
[22] Kush Bhatia, Peter L. Bartlett, Anca D. Dragan, and Jacob Steinhardt. Agnostic Learning with Unknown Utilities. arXiv:2104.08482 [cs, stat], April 2021. URL http://arxiv.org/abs/2104.08482. arXiv: 2104.08482.
[23] Michele Cohen, Jean-Yves Jaffray, and Tanios Said. Experimental Comparison of Individual Behavior Under Risk and Under Uncertainty for Gains and for Losses. Organizational Behavior and Human Decision Processes, 39(1):1–22, February 1987. ISSN 0749-5978. doi: 10.1016/0749-5978(87)90043-4. URL https://www.sciencedirect.com/science/article/pii/0749597887900434.
[24] Charles A. Holt and Susan K. Laury. Risk Aversion and Incentive Effects. The American Economic Review, 92(5):1644–1655, 2002. ISSN 0002-8282. URL https://www.jstor.org/stable/3083270. Publisher: American Economic Association.
[25] Tamás Csermely and Alexander Rabas. How to Reveal People’s Preferences: Comparing Time Consistency and Predictive Power of Multiple Price List Risk Elicitation Methods. Journal of Risk and Uncertainty, 53(2):107–136, 2016. ISSN 0895-5646. doi: 10.1007/s11166-016-9247-6.
[26] R. Duncan Luce. The Choice Axiom After Twenty Years. Journal of Mathematical Psychology, 15(3):215–233, June 1977. ISSN 0022-2496. doi: 10.1016/0022-2496(77)90032-3. URL https://www.sciencedirect.com/science/article/pii/0022249677900323 .
[27] Leslie G. Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134–1142, 1984. Publisher: ACM New York, NY, USA.
[28] Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying Differences in Reward Functions. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=LwEQnp6CYev .
[29] Luc Devroye, László Györﬁ, and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31. Springer Science & Business Media, 2013.
[30] Hong Jun Jeon, Smitha Milli, and Anca D. Dragan. Reward-Rational (Implicit) Choice: A Unifying Formalism for Reward Learning. arXiv:2002.04833 [cs], December 2020. URL http://arxiv.org/abs/2002.04833. arXiv: 2002.04833.
[31] Rachel Freedman, Rohin Shah, and Anca Dragan. Choice Set Misspeciﬁcation in Reward Inference. arXiv:2101.07691 [cs], January 2021. URL http://arxiv.org/abs/2101.07691. arXiv: 2101.07691.
[32] Roger N. Shepard. Stimulus and Response Generalization: A Stochastic Model Relating Generalization to Distance in Psychological Space. Psychometrika, 22(4):325–345, December 1957. ISSN 1860-0980. doi: 10.1007/BF02288967. URL https://doi.org/10.1007/BF02288967.
12

[33] Chris L. Baker, Joshua B. Tenenbaum, and Rebecca R. Saxe. Goal Inference as Inverse Planning. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29, 2007. Issue: 29.

[34] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent Trade-Offs in the Fair Determination of Risk Scores. arXiv:1609.05807 [cs, stat], November 2016. URL http://arxiv.org/abs/1609.05807. arXiv: 1609.05807.

[35] Lydia T. Liu, Max Simchowitz, and Moritz Hardt. The Implicit Fairness Criterion of Unconstrained Learning. In International Conference on Machine Learning, pages 4051–4060. PMLR, May 2019. URL http://proceedings.mlr.press/v97/liu19f.html. ISSN: 2640-3498.

[36] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019.

[37] Camelia Simoiu, Sam Corbett-Davies, and Sharad Goel.

The Problem of Infra-

Marginality in Outcome Tests for Discrimination.

The Annals of Applied Statis-

tics, 11(3), September 2017. ISSN 1932-6157. doi: 10.1214/17-AOAS1058. URL

https://projecteuclid.org/journals/annals-of-applied-statistics/volume-11/issue-3/The-problem-of-infra-

[38] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk. Econometrica, 47(2):263–291, 1979. ISSN 0012-9682. doi: 10.2307/1914185. URL https://www.jstor.org/stable/1914185. Publisher: [Wiley, Econometric Society].

[39] Liu Leqi, Adarsh Prasad, and Pradeep K Ravikumar. On Human-Aligned Risk Minimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 15055–15064. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9642-on-human-aligned-risk-minimization.pdf.

[40] V. Vapnik. Estimation of Dependences Based on Empirical Data. Information Science and Statistics. Springer-Verlag, New York, 2006. ISBN 978-0-387-30865-4. doi: 10.1007/0-387-34239-7. URL https://www.springer.com/gp/book/9780387308654 .

[41] Anselm Blumer, A. Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the VapnikChervonenkis Dimension. Journal of the ACM, 36(4):929–965, October 1989. ISSN 0004-5411, 1557735X. doi: 10.1145/76359.76371. URL https://dl.acm.org/doi/10.1145/76359.76371.

[42] Jiˇrí Matoušek and Jan Vondrák. The Probablistic Method. Lecture Notes, Charles University, Prague, Czech Republic, March 2008.

[43] D. Angluin and L. G. Valiant.

Fast Probabilistic Algorithms for Hamiltonian Cir-

cuits and Matchings.

Journal of Computer and System Sciences, 18(2):155–193,

April 1979.

ISSN 0022-0000.

doi: 10.1016/0022-0000(79)90045-X.

URL

https://www.sciencedirect.com/science/article/pii/002200007990045X .

[44] Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A General Lower Bound on the Number of Examples Needed for Learning. Information and Computation, 82(3):247–261, 1989. Publisher: Elsevier.

[45] V. Vapnik.

Principles of Risk Minimization for Learning Theory.

Ad-

vances in Neural Information Processing Systems, 4, 1991.

URL

https://proceedings.neurips.cc/paper/1991/hash/ff4d5fbbafdf976cfdc032e3bde78de5- Abstract.html .

[46] Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss functions all the same? Neural Computation, 16(5):1063–1076, May 2004. ISSN 0899-7667. doi: 10.1162/089976604773135104. URL https://doi.org/10.1162/089976604773135104.

13

Appendix

A Proofs

A.1 Proof of Lemma 3.1
Lemma 3.1 (Equivalence of cost matrices). Any cost matrix C = ( CC1000 CC1011 ) is equivalent to a cost matrix C′ = ( 0c 1−0 c ) where c = C10+CC0110−−CC0000−C11 as long as C10 + C01 − C00 − C11 = 0. That is, there are constants a, b ∈ R such that RC (h) = aRC′ (h) + b for all h.

Proof. Let a = C10 + C01 − C00 − C11 and b = P(Y = 0)C00 + P(Y = 1)C11. Then

RC (h)

= P(h(X) = 0 ∧ Y = 0)C00 + P(h(X) = 1 ∧ Y = 0)C10

+ P(h(X) = 0 ∧ Y = 1)C01 + P(h(X) = 1 ∧ Y = 1)C11

= P(h(X) = 1 ∧ Y = 0)(C10 − C00) + P(Y = 0)C00

+ P(h(X) = 0 ∧ Y = 1)(C01 − C11) + P(Y = 1)C11

= P(h(X) = 1 ∧ Y = 0)(C10 − C00) + P(h(X) = 0 ∧ Y = 1)(C01 − C11) + b

= (C10 + C01 − C00 − C11) P(h(X) = 1 ∧ Y = 0)

C10 − C00

C10 + C01 − C00 − C11

+P(h(X) = 0 ∧ Y = 1)

C01 − C11

+b

C10 + C01 − C00 − C11

= a(P(h(X) = 1 ∧ Y = 0)c + P(h(X) = 0 ∧ Y = 1)(1 − c)) + b

= aRC′ (h) + b.

A.2 Proof of Lemma 4.1
Lemma 4.1 (Bayes optimal decision rule). An optimal decision rule h for a decision problem (D, c) is given by h(x) = 1{q(x) ≥ c} where q(x) = P(X,Y )∼D(Y = 1 | X = x) is the posterior probability of class 1 given the observation x.
This result is well-known [29] but we include a proof here for completeness.

Proof. Let h(x) = 1{q(x) ≥ c} and let ˜h : X → {0, 1} be any other decision rule. We will show that not only is h an optimal decision rule, but in fact that if P(h(X) = h˜(X) ∧ q(X) = c) > 0, then Rc(h˜) > Rc(h); that is, h˜ is strictly suboptimal. Thus, any optimal decision rule h∗ must satisfy h(x) = h∗(x) almost surely except where q(x) = c.
First, let’s deﬁne the conditional risk of h at x, denoted by Rc(h | X = x):
Rc(h | X = x) = c P(h(X) = 1 ∧ Y = 0 | X = x) + (1 − c) P(h(X) = 0 ∧ Y = 1 | X = x).
Note that one of the two terms is always zero, depending on whether h(X) is 0 or 1, since h(X) is deterministic given X. The risk of h is the expectation of the conditional risk:
Rc(h) = EX∼Dx [Rc(h | X = x)].
We can bound the conditional risk for the optimal decision rule h:

Rc(h | x) = cP(Y = 0 | X = x)

q(x) ≥ c

(1 − c)P(Y = 1 | X = x) q(x) < c

= c(1 − q(x)) q(x) ≥ c (1 − c)q(x) q(x) < c

≤ c(1 − c).

(3)

Now, consider the conditional risk for the other decision rule ˜h at x. First, suppose h˜(x) = h(x); that is, the decision rule agrees with the optimal one. Then clearly Rc(h˜ | X = x) = Rc(h | X = x) ≤ c(1 − c). Next,

14

suppose q(x) = c and ˜h(x) = h(x). Then

Rc(h˜ | X = x) = cP(Y = 0 | X = x)

q(x) < c

(1 − c)P(Y = 1 | X = x) q(x) > c

= c(1 − q(x)) q(x) < c (1 − c)q(x) q(x) > c

> c(1 − c).

(4)

Finally, suppose q(x) = c; in this case, it is clear that Rc(h˜ | X = x) = c(1 − c) regardless of what h˜(x) is. Putting this together, we can break down the risk of h˜ by conditioning on whether h˜(x) = h(x) or q(x) = c:

Rc(h˜) = E[Rc(h˜ | X = x)] = E[Rc(h˜ | X = x) | h˜(X) = h(X) ∨ q(X) = c] P(h˜(X) = h(X) ∨ q(X) = c) + E[Rc(h˜ | X = x) | h˜(X) = h(X) ∧ q(X) = c] P(h˜(X) = h(X) ∧ q(X) = c)

(i)
>/≥ E[Rc(h˜ | X = x) | ˜h(X) = h(X) ∨ q(X) = c] P(h˜(X) = h(X) ∨ q(X) = c) + E[c(1 − c) | h˜(X) = h(X) ∧ q(X) = c] P(h˜(X) = h(X) ∧ q(X) = c)

(ii)
≥

E[Rc (h

|

X

=

x)

|

h˜ (X )

=

h(X )

∨

q(X )

=

c]

P(h˜ (X )

=

h(X )

∨

q(X )

=

c)

+ E[Rc(h | X = x) | h˜(X) = h(X) ∧ q(X) = c] P(h˜(X) = h(X) ∧ q(X) = c)

= E[Rc(h | X = x)]

= Rc(h).

(i) uses (4) and (ii) uses (3). The above shows that Rc(h˜) ≥ Rc(h) for any decision rule ˜h, demonstrating that h must have the lowest risk achievable. Note that (i) is strictly greater as long as P(h˜(X) = h(X) ∧ q(X) =
c) > 0, validating the claim above that any optimal decision rule must agree with h almost surely except when
q(X) = c.

A.3 Proof of Theorem 4.2
Theorem 4.2 (IDT for optimal decision maker). Let ǫ > 0 and δ > 0. Say that there exists pc > 0 such that P(q(X) ∈ (c, c + ǫ]) ≥ pcǫ and P(q(X) ∈ [c − ǫ, c)) ≥ pcǫ. Let cˆ be chosen to be consistent with the observed decisions as stated above, i.e. q(xi) ≥ cˆ ⇔ yˆi = 1. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples m ≥ logp(c2ǫ/δ) .

Proof. Let h denote the decision maker’s decision rule. From the proof of Lemma 4.1, we know that the optimality of h means that h(X) = 1{q(X) ≥ c} almost surely as long as q(X) = c.
Let E denote the event that we observe xi and xj in the sample such that q(xi) ∈ (c, c + ǫ] and q(xj) ∈ [c − ǫ, c):
E = ∃xi q(xi) ∈ (c, c + ǫ] ∧ ∃xj q(xi) ∈ [c − ǫ, c) .

E1

E2

First, we will lower bound the probability of E1:
P(E1) = 1 − P(∀xi q(xi) ∈/ (c, c + ǫ]) = 1 − (P(q(X) ∈/ (c, c + ǫ]))m = 1 − (1 − P(q(X) ∈ (c, c + ǫ]))m ≥ 1 − (1 − ǫpc)m ≥ 1 − e−mǫpc ≥ 1 − e− log(2/δ) = 1 − δ/2.
Second, we will lower bound the probability of E2:
P(E2) = 1 − P(∀j q(xj) ∈/ [c − ǫ, c)) = 1 − (P(q(X) ∈/ [c − ǫ, c)))m

15

= 1 − (1 − P(q(X) ∈ [c − ǫ, c)))m ≥ 1 − (1 − ǫpc)m ≥ 1 − e−mǫpc
≥ 1 − e− log(2/δ) = 1 − δ/2.
Putting the above together, we can lower bound the probability of E:
P(E) = P(E1 ∧ E2) = 1 − P(¬E1 ∨ ¬E2) ≥ 1 − P(¬E1) − P(¬E2) ≥ 1 − δ.
Finally, we will show that E implies |cˆ− c| ≤ ǫ. Suppose E occurs. Then q(xi) > c, so h(xi) = yˆi = 1. This means that cˆ ≤ q(xi) ≤ c + ǫ. Also, q(xj) < c, so h(xj) = yˆj = 0. This means that cˆ > q(xj) ≥ c − ǫ. Thus
c − ǫ < cˆ ≤ c + ǫ |cˆ − c| ≤ ǫ.
So with probability at least 1 − δ, |cˆ − c| ≤ ǫ.

A.4 Proof of Lemma 4.5
The proof of Lemma 4.5 depends on another lemma, which will also be useful in the unknown hypothesis class setting. This lemma bounds the conditional probability that the correct decision Y = 1 for observations x between the decision boundaries of two optimal decision rules.
Lemma A.1. Suppose optD(H) is monotone and let hc, hc′ ∈ H be optimal decision rules for loss parameters c and c′, respectively, where c < c′. Then for every x ∈ X , hc′ (x) ≤ hc(x). Furthermore, assuming P(hc(X) = hc′ (X)) = P(hc(X) = 1 ∧ hc′ (X) = 0) > 0,
c ≤ P(Y = 1 | hc(X) = 1 ∧ hc′ (X) = 0) ≤ c′.

Proof. We can write the risk of a decision rule h for cost c as Rc(h) = c P(h(X) = 1 ∧ Y = 0) + (1 − c) P(h(X) = 0 ∧ Y = 1) = c P(Y = 0) − P(h(X) = 0 ∧ Y = 0) + (1 − c) P(h(X) = 0 ∧ Y = 1)

= c P(Y = 0) − P(h(X) = 0) − P(h(X) = 0 ∧ Y = 1)

= c P(Y = 0) − c P(h(X) = 0) + c P(h(X) = 0 ∧ Y = 1) + P(h(X) = 0 ∧ Y = 1) − c P(h(X) = 0 ∧ y = 1)
= c P(Y = 0) − c P(h(X) = 0) + P(h(X) = 0 ∧ Y = 1).

Since hc is optimal for c, we have Applying (5) to (6) gives

Rc(hc′ ) − Rc(hc) ≥ 0.

+ (1 − c) P(h(X) = 0 ∧ Y = 1)
(5) (6)

P(hc′ (X) = 0 ∧ Y = 1) − P(hc(X) = 0 ∧ Y = 1) − c P(hc′ (X) = 0) − P(hc(X) = 0) ≥ 0. (7)

Now, suppose the lemma does not hold; that is, there is some x ∈ X such that hc′ (x) > hc(x). Since optD(H) is monotone, this implies

∀x ∈ X hc(x) ≤ hc′ (x).

(⋆)

Assuming (⋆) we have the following two identities: P(hc(X) = 0) − P(hc′ (X) = 0) = P(hc(X) = 0) ∧ hc′ (X) = 1)
P(hc(X) = 0 ∧ Y = 1) − P(hc′ (X) = 0 ∧ Y = 1) = P(hc(X) = 0) ∧ hc′ (X) = 1 ∧ Y = 1). Plugging these in to (5) gives
c P(hc(X) = 0) ∧ hc′ (X) = 1) − P(hc(X) = 0) ∧ hc′ (X) = 1 ∧ Y = 1) ≥ 0 P(hc(X) = 0) ∧ hc′ (X) = 1 ∧ Y = 1) ≤ c P(hc(X) = 0) ∧ hc′ (X) = 1) P(hc(X) = 0) ∧ hc′ (X) = 1 ∧ Y = 1) ≤ c P(hc(X) = 0) ∧ hc′ (X) = 1)

16

P(Y = 1 | hc(X) = 0 ∧ hc′ (X) = 1) ≤ c. This is the ﬁrst claim of the lemma. Now, we can apply the same set of steps to Rc′ (hc) − Rc′ (hc′ ) ≥ 0 (i.e., using (5) and the above identities) to obtain
c′ ≤ P(Y = 1 | hc(X) = 0 ∧ hc′ (X) = 1). Combining these two equations implies c′ ≤ c, but we assumed that c < c′, so this is a contradiction. Thus, (⋆) must be false!

Since optD(H) is monotone, the falsity of (⋆) implies that actually,

∀x ∈ X hc′ (x) ≤ hc(x).

(8)

Now, we can complete the proof by repeating the above steps using (8) instead of (⋆) to obtain c ≤ P(Y = 1 | hc(X) = 1 ∧ hc′ (X) = 0) ≤ c′.

Lemma 4.5 (Induced posterior probability). Let optD(H) be monotone and deﬁne

qH(x) sup {c ∈ [0, 1] | hc(x) = 1} ∪ {0} and qH(x) inf {c ∈ [0, 1] | hc(x) = 0} ∪ {1} .

Then for all x ∈ X , qH(x) = qH(x). Deﬁne the induced posterior probability of H as qH(x) qH(x).

qH(x) =

Proof. Fix x ∈ X . Using Lemma A.1, we have that c < c′ ⇒ hc(x) ≥ hc′ (x).
That is, hc(x) is monotone non-increasing in c. This is enough to show that qH(x) is well-deﬁned. Consider three cases:

1. ∀c, hc(x) = 1. In this case, qH(x) = sup{c ∈ [0, 1] | hc(x) = 1} ∪ {0} = 1 and qH(x) = inf{c ∈ [0, 1] | hc(x) = 0} ∪ {1} = inf ∅ ∪ {1} = 1 so qH(x) = 1.
2. ∀c, hc(x) = 0. In this case, qH(x) = sup{c ∈ [0, 1] | hc(x) = 1} ∪ {0} = sup ∅ ∪ {0} = 0 and qH(x) = inf{c ∈ [0, 1] | hc(x) = 0} ∪ {1} = 0 so qH(x) = 0.
3. ∃ c0, c1 such that hc0 (x) = 0 and hc1 (x) = 1. In this case, neither {c ∈ [0, 1] | hc(x) = 1} nor {c ∈ [0, 1] | hc(x) = 0} is empty so we have qH(x) = sup {c ∈ [0, 1] | hc(x) = 1} qH(x) = inf {c ∈ [0, 1] | hc(x) = 0}. Say qH(x) is not well-deﬁned; that is, sup {c ∈ [0, 1] | hc(x) = 1} = inf {c ∈ [0, 1] | hc(x) = 0}. First, suppose sup {c ∈ [0, 1] | hc(x) = 1} < inf {c ∈ [0, 1] | hc(x) = 0}. Then there exists some c for which hc(x) ∈/ {0, 1}, which is impossible. So sup {c ∈ [0, 1] | hc(x) = 1} > inf {c ∈ [0, 1] | hc(x) = 0}. However, this implies that ∃ c1 ≥ c0 such that hc1 (x) = 1 but hc0 (x) = 0. Since hc(x) is nonincreasing in c, this is a contradiction. Thus qH(x) = qH(x) = qH(x) is welldeﬁned.

Corollary 4.6. Let hc be any optimal decision rule in H for loss parameter c. Then for any x ∈ X , hc(x) = 1 if qH(x) > c and hc(x) = 0 if qH(x) < c.

Proof. Let hc ∈ arg min Rc(h)
h∈H
be an optimal decision rule in H for loss parameter c.

Fix any x ∈ X . If qH(x) = c, we don’t need to prove anything. If qH(x) > c, then suppose hc(x) = 1, i.e.

hc(x) = 0. Then

qH(x) = inf {c′ ∈ [0, 1] | hc′ (x) = 0} ≤ c

since hc(x) = 0. However, this is a contradiction since we assumed qH(x) > c. Thus hc(x) = 1.

Now, if qH(x) < c, suppose hc(x) = 0, i.e. hc(x) = 1. Then qH(x) = sup {c′ ∈ [0, 1] | hc′ (x) = 1} ≥ c.
This is also a contradiction since we assumed qH(x) < c, so hc(x) = 0.

17

A.5 Proof of Theorem 4.7
Theorem 4.7 (Known suboptimal decision maker). Let ǫ > 0 and δ > 0, and let optD(H) be monotone. Say that there exists pc > 0 such that P(qH(X) ∈ (c, c + ǫ]) ≥ pcǫ and P(qH(X) ∈ [c − ǫ, c)) ≥ pcǫ. Let cˆ be chosen to be consistent with the observed decisions, i.e. qH(xi) ≥ cˆ ⇔ yˆi = 1. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples m ≥ logp(c2ǫ/δ) .
Proof. Let h ∈ H denote the decision maker’s decision rule. From Corollary 4.6, we know that h(x) = 1{qH(x) ≥ c} as long as qH(x) = c.
Let E denote the event that we observe xi and xj in the sample such that qH(xi) ∈ (c, c + ǫ] and qH(xj) ∈ [c − ǫ, c). An analogous computation to the proof of Theorem 4.2 (Section A.3) shows that if m ≥ logp(c2ǫ/δ) , then P(E) ≥ 1 − δ.
If E occurs, then h(xi) = 1 and so cˆ ≤ c + ǫ. Also, h(xj) = 0 so cˆ ≥ c − ǫ. Thus, we have P(|cˆ − c| ≤ ǫ) ≥ P(E) ≥ 1 − δ.

A.6 Proof of Theorem 4.10

Theorem 4.10 (Unknown suboptimal decision maker). Let ǫ > 0 and δ > 0. Suppose we observe decisions from a decision rule hc which is optimal for loss parameter c in hypothesis class H ∈ H. Let hc and H be αMD-smooth. Furthermore, assume that there exists pc > 0 such that for any ρ ≤ ǫ, P(qH(X) ∈ (c, c + ρ)) ≥ pcρ and P(qH(X) ∈ (c − ρ, c)) ≥ pcρ. Let d ≥ VCdim (∪H∈HH) be an upper bound on the VC-dimension
of the union of all the hypothesis classes in H.

Let ˆhcˆ ∈ arg minhˆ∈Hˆ Rcˆ(hˆ) be chosen to be consistent with the observed decisions, i.e. hˆcˆ(xi) = yˆi for i = 1, . . . , m. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of samples m ≥

O˜

α ǫ

+

1 ǫ2

d+lopgc(1/δ) .

Proof. Speciﬁcally, we will prove that P(|cˆ − c| ≤ ǫ) ≥ 1 − δ as long as

m ≥ O α + 1 d log(α/(pcǫ)) + log(1/δ) .

(9)

ǫ ǫ2

pc

Throughout the proof, let hc ∈ arg minh∈H Rc(h) be the true decision rule and let hˆcˆ ∈ arg minhˆ∈Hˆ Rcˆ(Hˆ) be the estimated decision rule, i.e. one that agrees with the decisions in the sample of observations S.

First, we use a standard result from PAC learning theory to upper bound the disagreement between the estimated decision rule hˆcˆ and the true decision rule hc. In particular, since this is a case of realizable PAC learning, i.e. the true decision rule hc is in one of the hypothesis classes H ∈ H, we have that

P(hc(X) = hˆcˆ(X)) ≤ O

1

+ α

1

pcǫ pcǫ2

= O min pcǫ , pcǫ2 α

with probability at least 1 − δ over the drawn sample. This bound follows from Vapnik [40] and Blumer et al. [41] since the set of all possible hypotheses ∪H∈HH has VC-dimension at most d, and we observe a sample of m observations xi and decisions yˆi = hc(xi) where m satisﬁes (9). In particular, denote

r = P(hc(X) = hˆcˆ(X)) ≤ min pcǫ , pcǫ2 .

(10)

6α 36

Next, we show that (10) implies that |cˆ − c| ≤ ǫ; since (10) holds with probability at least 1 − δ, this is enough to complete the proof of Theorem 4.10. We will prove that cˆ − c ≤ ǫ given (10). The proof that c − cˆ ≤ ǫ is analogous. We require a technical lemma on probability theory:
Lemma A.2. Let A, B, and C be events in a probability space with P(A) > 0 and P(B) > 0. Then
|P(C | A) − P(C | B)| ≤ P(A ∧ ¬B) + P(¬A ∧ B) . min(P(A), P(B))

Proof of Lemma A.2. To simply the proof of this lemma, we adopt the boolean algebra notation that AB is equivalent to A ∧ B and A¯ is equivalent to ¬A. Then we have
|P(C | A) − P(C | B)|

18

= P(AC) − P(BC) P(A) P(B) P(ABC) + P(AB¯C) P(ABC) + P(A¯BC)
= P(AB) + P(AB¯) − P(AB) + P(A¯B) = |P(ABC)P(A¯B) + P(AB¯C)P(B) − P(ABC)P(AB¯) − P(A¯BC)P(A)|
P(A)P(B)

(i) max P(ABC)P(A¯B) + P(AB¯C)P(B), P(ABC)P(AB¯) + P(A¯BC)P(A) ≤ P(A)P(B)

= max P(ABC)P(A¯B) + P(AB¯C)P(B) , P(ABC)P(AB¯) + P(A¯BC)P(A)

P(A)P(B)

P(A)P(B)

(ii)

P(B)P(A¯B) + P(AB¯)P(B) P(A)P(AB¯) + P(A¯B)P(A)

≤ max

,

P(A)P(B)

P(A)P(B)

P(A¯B) + P(AB¯) P(AB¯) + P(A¯B)

= max

P(A)

,

P(B)

= P(AB¯) + P(A¯B) . min(P(A), P(B))

(i) uses the fact that for positive u and v, |u − v| ≤ max(u, v). (ii) uses the fact that P(E1E2) ≤ P(E1) for any events E1 and E2.

Essentially, Lemma A.2 says that if events A and B have high “overlap,” then the conditional probabilities of another event C given A and B should be close. We next carefully construct two such events with high overlap.

First, let c′ = c + ǫ/2 and let hc′ ∈ arg minh∈H Rc′ (h). Since hc and H are α-MD-smooth, we have that

MD(hc′ , optD(Hˆ)) ≤ (1 + α|c′ − c|)MD(hc, optD(Hˆ))

(11)

≤ (1 + αǫ/2)P(hc(X) = hˆcˆ(X))

≤ (1 + αǫ/2)r.

(12)

Since MD(h, optD(Hˆ)) = infhˆ∈opt (Hˆ) P(h(X) = ˆh(X)), there must be some hypothesis hˆcˆ′ ∈ D
arg minhˆ∈Hˆ Rcˆ′ (hˆ) that matches the minimum disagreement with hc′ plus a small positive number (in case the inﬁmum is not achieved):
P(hˆcˆ′ (X) = hc′ (X)) ≤ MD(hc′ , optD(cˆ)) + r ≤ (2 + αǫ/2)r.

Now, let the events A, B, and C be deﬁned as follows:

A : hc(X) = 1 ∧ hc′ (X) = 0, B : hˆcˆ(X) = 1 ∧ hˆcˆ′ (X) = 0, C : Y = 1.

Using Lemma A.2, we can write the bound

P(Y = 1 | B) ≤ P(Y = 1 | A) + P(A ∧ ¬B ∨ ¬A ∧ B) .

(13)

min(P(A), P(B))

We will establish bounds on each term in (13).

Upper bound on P(A ∧ ¬B ∨ ¬A ∧ B) It is easy to see that

A ∧ ¬B ∨ ¬A ∧ B ⇒ hc(X) = hˆcˆ(X) ∨ hc′ (X) = hˆcˆ′ (X).

Given this implication, it must be that

P(A ∧ ¬B ∨ ¬A ∧ B) ≤ P(hc(X) = hˆcˆ(X) ∨ hc′ (X) = ˆhcˆ′ (X)) ≤ (3 + αǫ/2)r ≤ pcǫ2/12 + pcǫ2/12 = pcǫ2/6

where the inequalities follow from (10) and (12).

Lower bound on min(P(A), P(B)) Since hc is optimal within H for loss parameter c, Corollary 4.6 gives that hc(x) = 1 if qH(x) > c. Similarly, hcˆ(x) = 0 if qH(x) < c. Therefore,
qH(X) ∈ (c, c′) ⇒ hc(X) = 1 ∧ hcˆ(X) = 0 ⇔ A.

19

This implication allows us to lower bound P(A): P(A) ≥ P(qH(X) ∈ (c, c′)) = P(qH(X) ∈ (c, c + ǫ/2)) ≥ pcǫ/2
where the ﬁnal inequality is by assumption. We also need to lower bound P(B) in order to lower bound min(P(A), P(B)):
P(B) = P(A ∧ B) + P(¬A ∧ B) = P(A) − P(A ∧ ¬B) + P(¬A ∧ B)
≥ P(A) − P(A ∧ ¬B) + P(¬A ∧ B) ≥ pcǫ/2 − pcǫ2/6 ≥ pcǫ/3.
We assume that ǫ ≤ 1 to lower bound ǫ ≥ ǫ2, but this is ﬁne since if ǫ > 1 then Theorem 4.10 holds trivially. Thus we have min(P(A), P(B)) ≥ pcǫ/3. Lower bound on P(Y = 1 | B) By Lemma A.1, we have that, since P(B) > 0,
P(Y = 1 | B) = P(Y = 1 | ˆhcˆ(X) = 1 ∧ hˆcˆ′ (X) = 0) ≥ cˆ.
Upper bound on P(Y = 1 | A) Similarly, by Lemma A.1, we have that, since c′ > c and P(A) > 0, P(Y = 1 | A) = P(Y = 1 | hc(X) = 1 ∧ hc′ (X) = 0) ≤ c′ = c + ǫ/2.

Concluding the proof Given all these bounds, we can rewrite (12) as

cˆ ≤ P(Y

= 1 | B) ≤ P(Y = 1 | A) + P(A ∧ ¬B ∨ ¬A ∧ B) min(P(A), P(B))
≤ c + ǫ/2 + pcǫ2/6 pcǫ/3
≤ c + ǫ/2 + ǫ/2 = c + ǫ cˆ − c ≤ ǫ.

This completes the proof that cˆ− c ≤ ǫ with probability at least 1 − δ; the proof that c − cˆ ≤ ǫ is analogous.

A.7 Proof of Theorem 4.11
Theorem 4.11 (Lower bound for optimal decision maker). Fix 0 < ǫ < 1/4, 0 < δ ≤ 1/2, and 0 < pc ≤ 1/8ǫ. Then for any IDT algorithm cˆ(·), there exists a decision problem (D, c) satisfying the conditions of Theorem 4.7 such that m < log8(p1c/ǫ2δ) implies that P(|cˆ(S) − c| ≥ ǫ) > δ.

Proof. Consider a distribution over X ∈ X = [0, 1] where

q(x) = P(Y = 1 | X = x) = x.

Let the distribution DX over X have density pc on the interval (1/2 − 2ǫ, 1/2 + 2ǫ) and let P(X = 0) = P(X = 1) = 1/2 − 2pcǫ.
Let c1 = 1/2 − ǫ and c2 = 1/2 + ǫ. Then clearly, for c ∈ {c1, c2}, the conditions of Theorem 4.2 are satisﬁed:

P(q(X) ∈ [c − ǫ, c)) = P(q(X) ∈ (c, c + ǫ]) = pcǫ.

By Lemma 4.1, the optimal decision rule for loss parameter c1 is hc1 (x) = 1{x ≥ c1} and for c2 it is hc2 (x) = 1{x ≥ c2}.

Now suppose m < log(1/2δ) 8pcǫ
as stated in the theorem. We can bound the probability of the following event E:
P(∀xi ∈ S q(xi) ∈ {0, 1}) = [P(X ∈ {0, 1})]m

E
= (1 − 4pcǫ)m

(i)
≥

e−8pcǫ m

20

= e− log(1/2δ) = 2δ.
(i) uses the fact that 1 − u ≥ e−2u for u ∈ [0, 1/2]. Now, suppose E occurs. In this case, hc1 (xi) = hc2 (xi) for all xi ∈ S. That is, regardless of which loss parameter c ∈ {c1, c2} is used, the distribution of samples will be the same. Let S1 denote the random variable for a sample taken from a decision maker using hc1 and S2 a sample taken from hc2 . Since these have the same distribution under E, they must induce the same probabilities when the IDT algorithm cˆ is applied to them:
p1 = P(cˆ(S1) ≤ 1/2 | E) = P(cˆ(S2) ≤ 1/2 | E), p2 = P(cˆ(S1) > 1/2 | E) = P(cˆ(S2) > 1/2 | E).
Since p1 + p2 = 1, at least one of p1, p2 ≥ 1/2. Suppose WLOG that p1 ≥ 1/2. Then
P(|cˆ(S2) − c2| ≥ ǫ) ≥ P(cˆ(S2) ≤ 1/2) = P(cˆ(S2) ≤ 1/2 | E) P(E) ≥ 1/2(2δ) = δ.
Thus there is a decision problem (D, c2) for which the IDT algorithm cˆ must make an error of at least size ǫ with at least probability δ. This concludes the proof.
Corollary 4.12 (Lack of uncertainty precludes identiﬁability). Fix 0 < ǫ < 1/4 and suppose a decision problem (D, c) has no uncertainty. Then for any IDT algorithm cˆ(·), there is a loss parameter c and hypothesis class H such that for any sample size m, P(|cˆ(S) − c| ≥ ǫ) ≥ 1/2.
Proof. Let the loss parameters c1 = 1/2 − ǫ and c2 = 1/2 + ǫ be deﬁned as in the proof of Theorem 4.11 above. By Lemma 4.1, the optimal decision rule for loss parameter c1 is hc1 (x) = 1{q(x) ≥ c1} and for c2 it is hc2 (x) = 1{q(x) ≥ c2}. Since P(q(x) ∈ {0, 1}) = 1, it is clear that the decision rules make the same decision rules almost surely, i.e. P(hc1 (X) = hc2 (X)) = 1. Thus, letting S1 and S2 denote samples drawn from decision rules hc1 and hc2 , respectively, as above, we have that the distributions of S1 and S2 are indistinguishable. Thus by the same argument as above we can show that (WLOG)
P(|cˆ(S2) − c2| ≥ ǫ) ≥ P(cˆ(S2) ≤ 1/2) ≥ 1/2.

A.8 Proof of Lemma 5.2
Deﬁnition 5.1. A decision rule h : X → {0, 1} for a distribution (X, Y ) ∼ D satisﬁes the group calibration/sufﬁciency fairness criterion if there is a function r : X → R and threshold t ∈ R such that h(x) = 1{r(x) ≥ t} and r satisﬁes Y ⊥⊥ A | r(X).
Lemma 5.2 (Equal loss parameters imply group calibration). Let h be chosen as in (2) where ℓ(yˆ, y, a) = ca if yˆ = 1 and y = 0, ℓ(yˆ, y, a) = 1 − ca if yˆ = 0 and y = 1, and ℓ(yˆ, y, a) = 0 otherwise. Then h satisﬁes group calibration (sufﬁciency) if ca = ca′ for every a, a′ ∈ A.
Conversely, if there exist a, a′ ∈ A such that ca = c′a and P(q(X) ∈ (ca, ca′ )) > 0, then h does not satisfy group calibration.

Proof that equal ca imply group calibration. Assume ca = ca′ = c for every a, a′ ∈ A. Then deﬁne

r(x) = q(x) + h(x).

(14)

That is, r(x) is the posterior probability q(x) = P(Y = 1 | X = x) plus one if the decision rule outputs the decision h(x) = 1. From the proof of Lemma 4.1, we know that h(x) = 1 if q(x) > c and h(x) = 0 if q(x) < c. From this and (14) we can write

h(x) = 1{r(x) ≥ c + 1}.

Now we need to show that Y ⊥⊥ A | r(X). Note that r(X) ∈ [0, c]∪[c+1, 2]. First, we consider r(X) ∈ [0, c]. In this case, for any a ∈ A, we have

P(Y = 1 | A = a, r(X) = r) = P(Y = 1 | A = a, q(X) = r)

=r

= P(Y = 1 | r(X) = r).

Next, say r(X) ∈ [c + 1, 2]. Then

P(Y = 1 | A = a, r(X) = r) = P(Y = 1 | A = a, q(X) = r − 1)

=r−1

= P(Y = 1 | r(X) = r).

So in either case, P(Y = 1 | A = a, r(X) = r) = P(Y = 1 | r(X) = r). Thus Y ⊥⊥ A | r(X).

21

Y = 0 s0
a0 Yˆ = 0
R=0

ox x ∼ X | Y = 0
a1 Yˆ = 1

Y = 1 s1
a0 Yˆ = 0

R = −c

R = −(1 − c)

ox x ∼ X | Y = 1
a1 Yˆ = 1
R=0

Figure 4: A graphical depiction of the POMDP formulation of IDT described in Appendix B. A state in {s0, s1} is randomly selected at each timestep and an observation is generated according to the conditional distribution of X | Y . An action (decision) is taken and the agent receives reward equal to the negative of the loss.

Proof of inverse. Now, assume ∃ a, a′ ∈ A such that ca = ca′ . WLOG, suppose that ca < ca′ . Let r : X → R be any function satisfying h(x) = 1{r(x) ≥ t}. WLOG we can also assume t = 0. From Lemma 4.1, we know that if a(x) = a, then q(x) < ca implies h(x) = 0 and q(x) > ca implies h(x) = 1. Also, if a(x) = a′,
then q(x) < ca′ implies h(x) = 0 and q(x) > ca′ implies h(x) = 1. Therefore,

P(Y = 1 | A = a, r(X) > 0)

= P(Y = 1 | A = a, q(X) > ca)

= P(Y = 1 | q(X) > ca)

= P(Y = 1 | q(X) ∈ (ca, ca′ )) P(q(X) ∈ (ca, ca′ )) P(q(X) > ca)

+ P(Y = 1 | q(X) ≥ ca′ ) P(q(X) ≥ ca′ ) P(q(X) > ca)

(i)
<

ca′

P(q(X )

∈

(ca,

ca′ ))

+ P(Y

= 1 | q(X) ≥ ca′ ) P(q(X) ≥ ca′ )

P(q(X) > ca)

P(q(X) > ca)

(ii)
≤ P(Y = 1 | q(X) ≥ ca′ )

≤ P(Y = 1 | q(X) > ca′ ) = P(Y = 1 | A = a′, q(X) > ca′ ) = P(Y = 1 | A = a′, r(X) > 0).

(i) and (ii) make use of the fact that
P(Y = 1 | q(X) ∈ (ca, ca′ )) = E[q(X) | q(X) ∈ (ca, ca′ )] < ca′ ≤ E[q(X) | q(X) ≥ ca′ ] = P(Y = 1 | q(X) ≥ ca′ ).
(i) also uses the assumption that P(q(X) ∈ (ca, ca′ )) > 0. Thus, we have that P(Y = 1 | A = a, r(X) > 0) = P(Y = 1 | A = a′, r(X) > 0); therefore, Y and A are not independent given r(X), so group calibration is not satisﬁed.

B POMDP Formulation of IDT
As mentioned in the main text, IDT can be seen as a special case of inverse reinforcement learning (IRL) in a partially observable Markov decision process (POMDP) (or equivalently, belief state MDP). Here, we present the equivalent POMDP and discuss connections to to our results. A POMDP is a tuple consisting of seven elements. For an IDT decision problem (D, c) they are:
• The state space consists of two states, each corresponding to a value of Y , the ground truth/correct decision. We call them s0 for Y = 0 and s1 for Y = 1.
• The action space consists of two actions, each corresponding to one of the decisions Yˆ . We equivalently call them a0 for Yˆ = 0 and a1 for Yˆ = 1.

22

q(x) = 0 a0 E[R] = 0 a1
E[R] = −c

...

q(x) = p

a0 E[R] = −p(1 − c) a1

...

q(x) = 1

a0 E[R] = −(1 − c) a1

E[R] = −(1 − p)c

E[R] = 0

Figure 5: A graphical depiction of the belief state MDP formulation of IDT. There is a belief state for each posterior probability q(x) = P(Y = 1 | X = x) ∈ [0, 1]. Observing the agent at a belief state gives a constraint on their reward function [2]. Thus, if q(X) has support on [0, 1], i.e. if there
is a signiﬁcant range of uncertainty in the decision problem, then there can be arbitrarily many such constraints, allowing the loss parameter c to be learned to arbitrary precision.

• The transition probabilities do not depend on the previous state or action; rather, s0 or s1 is randomly selected based on their probabilities under the distribution D:
p(st+1 = s0 | st, at) = PX,Y ∼D(Y = 0), p(st+1 = s1 | st, at) = PX,Y ∼D(Y = 1).

• The reward function is the negative of the loss function described in Section 3:

R(s0, a0) = 0 R(s0, a1) = −c

R(s1, a0) = −(1 − c), R(s1, a1) = 0.

• The observation space includes elements for each X ∈ X . We denote by ox the POMDP observation for x ∈ X .
• The observation probabilities are p(ot = ox | st = sy) = P(X = x | Y = y).
• The discount factor γ is basically irrelevant to IDT, since the decisions are non-sequential. Thus any γ will produce the same behavior.
A graphical depiction of this POMDP is shown in Figure 4. Any decision rule h : X → {0, 1} corresponds to a policy π in this POMDP:
π(at = ayˆ | ot = ox) = 1{h(x) = yˆ}.
Belief state MDP The above POMDP can be equivalently formulated as a belief state MDP. The belief states correspond to values of the posterior probability
P(s = s1 | o = ox) = P(Y = 1 | X = x) = q(x).
A graphical depiction of this belief state reduction is shown in Figure 5. Since the POMDP is non-sequential, these beliefs only depend on the most recent observation ox. The expected reward for action ayˆ at belief state with posterior probability q(x) is
R(q(x), a0) = P(s = s0 | q(x))R(s0, a0) + P(s = s1 | q(x))R(s1, a0) = −q(x)(1 − c), R(q(x), a1) = P(s = s0 | q(x))R(s0, a1) + P(s = s1 | q(x))R(s1, a1) = −(1 − q(x))c.

Thus, observing decision a0 at a belief state q(x) indicates that
R(q(x), a0) ≥ R(q(x), a1) −q(x)(1 − c) ≥ −(1 − q(x))c
c ≥ q(x).

23

Similarly, observing decision a1 at a belief state q(x) indicates that
R(q(x), a0) ≤ R(q(x), a1) −q(x)(1 − c) ≤ −(1 − q(x))c
c ≤ q(x).
Thus, as described in Section 4.1, IDT in this (optimal) case consists of determining the threshold on q(x) where the action switches from a0 to a1 for observations ox.
This formulation gives some additional insight into why uncertainty is helpful for IDT. If q(x) ∈ {0, 1} always, then there are only two belief states corresponding to q(x) = 0 and q(x) = 1. Thus, we only obtain two constraints on the value of c, i.e. 0 ≤ c ≤ 1. However, if q(X) has support on all of [0, 1], then we there belief states corresponding to every q(x) ∈ [0, 1]. Thus we can obtain inﬁnite constraints on the value of c, allowing learning it to arbitrary precision as shown in Section 4.1.

C Alternative Suboptimality Model

As mentioned in Section 4.2, there are many ways to model suboptimal decision making. One possibility is to only require that the decision rule h is close to optimal, i.e.

Rc(h) ≤ Rocpt + ∆ where Rocpt = inf Rc(h∗).

(15)

h∗

However, as we show in the following lemma, this assumption can preclude identiﬁablity of c. The models of suboptimality we present in Sections 4.2 and 4.3, in contrast, still allow exact identiﬁability of the loss parameter.
Lemma C.1 (Loss cannot always be identiﬁed for close-to-optimal decision rules). Fix 0 < ∆ ≤ 1 and 0 < ǫ < 1/4. Then for any IDT algorithm cˆ(·), there is a decision problem (D, c) and a decision rule h which is ∆-close to optimal as in (15) such that

P(|cˆ(S) − c| ≥ ǫ) ≥ 1/2,

where the sample S of any size m is observed from the decision rule h. Furthermore, the distribution D and loss parameter c satisfy the requirements of Theorem 4.2 for when the decision maker is optimal.

Proof. Consider a distribution over X ∈ X = [0, 1] where
q(x) = P(Y = 1 | X = x) = x.
Let the distribution DX have density ∆ on the interval (1/2 − 2ǫ, 1/2 + 2ǫ) and let P(X = 0) = P(X = 1) = 1/2 − 2∆ǫ. Let c1 = 1/2 − ǫ and c2 = 1/2 + ǫ. Then clearly P(q(X) ∈ [c − ǫ, c)) = P(q(X) ∈ (c, c + ǫ]) = ǫ∆ for c ∈ {c1, c2}. Thus either c1 or c2 satisﬁes the conditions of Theorem 4.2. Now deﬁne identical decision rules
h1(x) = h2(x) = 1{x ≥ 1/2 − ǫ}.
From Lemma 4.1, we know that h1 is optimal for c1, so it is certainly ∆-close to optimal. We can show that h2 is ∆-close to optimal for c2 as well:
Rc2 (h2) − Rc2 (x → 1{x ≥ 1/2 + ǫ})
= E ℓ(1{X ≥ 1/2 − ǫ}, Y ) − ℓ(1{X ≥ 1/2 + ǫ}, Y )
= E ℓ(1{X ≥ 1/2 − ǫ}, Y ) − ℓ(1{X ≥ 1/2 + ǫ}, Y ) | X ∈ [1/2 − ǫ, 1/2 + ǫ] P(X ∈ [1/2 − ǫ, 1/2 + ǫ]) ≤ 2P(X ∈ [1/2 − ǫ, 1/2 + ǫ]) = 4ǫ∆ ≤ ∆.
Since h1 and h2 are identical, we must have that for a sample S chosen according to either, at least one of P(cˆ(S) ≥ 1/2) ≥ 1/2 or P(cˆ(S) < 1/2) ≥ 1/2. Thus for some c ∈ {c1, c2},
P(|cˆ(S) − c| ≥ ǫ) ≥ 1/2.

24

D Additional Results for IDT with Suboptimal Decision Maker

D.1 Lower bound for unknown hypothesis class

We give two lower bounds for the sample complexity in the unknown hypothesis class case from Section 4.3.

First,

in

Theorem

D.1,

we

show

that

there

is

an

IDT

problem

such

that

m

=

Ω(

log(1/δ) 2

)

samples

are

required

pc ǫ

√

to estimate c. Second, in Theorem D.2, we show that there is an IDT problem such that m = Ω( pcdǫ ) samples

are required. These lower bounds do not precisely match our upper bound of m = O( pcdǫ2 + logp(c1ǫ/2δ) ) from

Theorem 4.10, and we leave as an open problem the exact minimax sample complexity of IDT in the unknown

hypothesis class case. However, they do show that IDT does become harder as the VC-dimension d increases,

and that in some suboptimal cases a number of samples proportional to 1/ǫ2 is needed to estimate c to precision

ǫ—more than the 1/ǫ needed for an optimal decision maker.

Theorem D.1 (First lower bound for suboptimal decision maker). Fix 0 < ǫ ≤ 1/8, 0 < δ ≤ 1/2, and pc ≤ 1/10. Then there is a decision problem (D, c), hypothesis class family H, and hypothesis class H ∈ H
satisfying the conditions of Theorem 4.10 with the above parameters such that

m < Ω log(1/δ) pcǫ2

implies that P(|cˆ(S) − c| ≥ ǫ) ≥ δ.

Proof. Speciﬁcally, let the sample size

m = log(1/(2δ)) . 40pcǫ2

Deﬁning the distribution First, we deﬁne a joint distribution D over X = (X1, X2) ∈ X = R2 and Y ∈ {0, 1}. The distribution of X has support on 2 line segments in R2 and at a point. It can be summarized as follows:

1.

DX has density

5pc 2

on the line segment from (−1, 0) to (1, 0).

P(Y

=1|X

= (x1, 0)) =

1+x1 2

.

2. DX has density 10pcx1 at points (x1, 1) on the line segment from (0, 1) to (1, 1). P(Y = 1 | X = (x1, 1)) = 1.

3. DX has point mass P(X = (−1, 0)) = 1 − 10pc. P(Y = 1 | X = (−1, 0)) = 0.

Deﬁning the family of hypothesis classes Now, we deﬁne a family of two hypothesis classes:

H1 {h(x) = 1{x1 ≥ b} | b ∈ [3/8, 5/8]}

H2 {h(x) = 1{x1 ≥ b + 2ǫx2} | b ∈ [1/2, 3/4]}

H {H1, H2}.

Let’s analyze H1 ﬁrst. The posterior probability that Y = 1 given that X1 = x1 is

P(Y = 1 | X1 = x1) =

1+x1 2

x1 < 0

(16)

1+9x1 2+8x1

x1 ≥ 0.

It is simple to show that this is increasing in x1; thus, the Bayes optimal decision rule based on X1 for c is

h1c (x) = 1{x1 ≥ 2c − 1} c ≤ 1/2

(17)

1{x1 ≥ 92−c−81c }

c > 1/2.

Now, let’s analyze H2. The posterior probability that Y = 1 given that X1 − 2ǫX2 = b for b ≥ −2ǫ is

P(Y = 1 | X1 − 2ǫX2 = b) = 1 + 9b + 16ǫ .

(18)

2 + 8b + 16ǫ

This can also be shown to be increasing in b, so the Bayes optimal decision rule based on X1 − 2ǫX2 for c >= 1/2 is

h2c (x) = 1 x1 − 2ǫx2 ≥ 2c − 1 − 16ǫ + 16cǫ .

(19)

9 − 8c

For this proof,

we consider

two hypothesis

class and loss parameter

pairs:

c1

=

1/2

for H1

and c2

=

1+16ǫ 2+16ǫ

for H2. These correspond to the decision rules

h1(x) = 1{x1 ≥ 0},

25

h2(x) = 1{x1 − 2ǫx2 ≥ 0} =

x1 ≥ 0 x1 ≥ 2ǫ

x2 = 0 x2 = 1.

It should be clear that these decision rules agree except when x2 = 1 and x1 ∈ [0, 2ǫ).

Another important fact is that

c2 = 1 + 16ǫ = 1 + 4ǫ ≥ 1 + 2ǫ

(20)

2 + 16ǫ 2 1 + 8ǫ 2

since ǫ ≤ 1/8.

We defer to the end of the proof to show that these hypotheses and distribution satisfy the conditions of Theorem 4.10.

Deriving the lower bound E:
P( ∃xi ∈ S

Similarly to the proof of Theorem 4.11, we can bound the probability of an event xi,1 ∈ [0, 2ǫ) ∧ xi,2 = 1) = [1 − P(X1 ∈ [0, 2ǫ) ∧ X2 = 1)]m

E
= (1 − 20pcǫ2)m ≥ e−40pcǫ2 m

= e− log(1/2δ) = 2δ.

Conditional on E, the distributions of samples S1 and S2 for decision rules h1 and h2 are identical: p1 = P(cˆ(S1) ≤ 1/2 + ǫ | E) = P(cˆ(S2) ≤ 1/2 + ǫ | E), p2 = P(cˆ(S1) > 1/2 + ǫ | E) = P(cˆ(S2) > 1/2 + ǫ | E).
Since p1 + p2 = 1, at least one of p1, p2 ≥ 1/2. Suppose WLOG that p1 ≥ 1/2. Then

(i)
P(|cˆ(S2) − c2| ≥ ǫ) ≥ P(cˆ(S2) ≤ 1/2 + ǫ) = P(cˆ(S2) ≤ 1/2 + ǫ | E) P(E) ≥ 1/2(2δ) = δ.
(i) uses the fact shown earlier in (20). Thus, there is a decision problem (D, c2) for which the IDT algorithm cˆ must make an error of at least size ǫ with at least probability δ. This concludes the main proof.

Verifying the requirements of Theorem 4.10 First, we need to show that qH1 (X) has density at least pc on [c1 − ǫ, c1 + ǫ] = [1/2 − ǫ, 1/2 + ǫ]. From (16) and (17), it is clear that

qH1 (x) = g1(x1) =

1+x1 2
1+9x1 2+8x1

x1 < 0 x1 ≥ 0.

We can write the density of qH1 (X) as the density of X1 multiplied by the derivative of the inverse of g1:

p(x1) ddc g1−1(c) ≥ 52pc ddc

2c − 1
2c−1 9−8c

c ≤ 1/2 c > 1/2

= 5pc 2
≥ pc.

2
10 (9−8c)2

c ≤ 1/2 c > 1/2

Next, we need to show that qH2 (X) has density at least pc on [c2 − ǫ, c2 + ǫ] ⊆ [1/2, 1]]. From (18) and (19),

we know that

qH2 (x) = g2(x1 − 2ǫx2) = 21 ++ 89((xx11 −− 22ǫǫxx22)) ++ 1166ǫǫ .

Using the same method as for qH1 (X) and the fact that the density of X1 − 2ǫX2 is at least the density of X1

(i.e.,

5pc 2

),

we

have

that

the

density

of

qH2 (X)

is

at

least

52pc ddc g2−1(c) = 52pc ddc 2c − 1 9−−168ǫc+ 16cǫ = 5pc 10 + 16ǫ 2 (9 − 8c)2

≥ 5pc 2 = pc. 25

26

The only remaining condition of Theorem 4.10 to prove is MD-smoothness. Again, consider H1 ﬁrst:

MD(h1b , H2) = min P h1b (X) = h2b (X)

1

b2 ∈[1/2,3/4]

1

2

= min 5pc |b1 − b2| + 5pc b21 − (b2 + 2ǫ)2 b2∈[1/2,3/4] 2

= 5pc |b1 − b1| + 5pc b21 − (b1 + 2ǫ)2 2
= 20pc|ǫ(b1 + ǫ)|.

From (17), we know that b1 − b′1 ≤ 10(c1 − c′1) where b1 and b′1 are the optimal thresholds for loss parameters c1 and c′1, respectively. So we have that

MD(h1c′ , H2) − MD(h1c , H2) = 20pcǫ(|b′1 + ǫ| − |b1 + ǫ|)

1

1

≤ 20pcǫ|b′1 − b1|

≤ 200pcǫ|c′1 − c1|.

Thus h1 and H are α-MD-smooth with α = 200pcǫ.

Similarly, for H2,

MD(h2b , H1) = min P h1b (X) = h2b (X)

2

b1 ∈[1/2,3/4]

1

2

= min 5pc |b1 − b2| + 5pc b21 − (b2 + 2ǫ)2 b1∈[1/2,3/4] 2
= 5pc |b2 − b2| + 5pc b22 − (b2 + 2ǫ)2 2
= 20pc|ǫ(b2 + ǫ)|.

So we have that

MD(h2c′ , H1) − MD(h1, H1) = 20pcǫ(|b′2 + ǫ| − |b2 + ǫ|) 2 ≤ 20pcǫ|b′2 − b2| ≤ 200pcǫ|c′2 − c2|,

and thus h2 and H are also 200pcǫ-MD-smooth.

Theorem D.2 (Second lower bound for suboptimal decision maker). Let d ≥ 6 such that d ≡ 2 (mod 4). Let ǫ ∈ (0, 64√1d−2 ] and pc ∈ (0, 1]. Then for any IDT algorithm cˆ(·), there is a decision problem (D, c), hypothesis class family H, and hypothesis class H ∈ H satisfying the conditions of Theorem 4.10 with the

above parameters such that

√ m<Ω d
pcǫ

implies that P(|cˆ(S) − c| ≥ ǫ) ≥ 1 . 160

Proof. Speciﬁcally, let

√ m = d − 2.
64pcǫ

Deﬁning the distribution Let n = d − 2 ≥ 1; n is divisible by four. First, we deﬁne a joint distribution D over X ∈ X = Rn+1 and Y ∈ {0, 1}. Let Xj refer to the jth coordinate of the random vector X and let xij refer to the jth coordinate of the ith sample xi. Furthermore, let X1:n refer to the ﬁrst n components of X.
The distribution of X has support on n line segments in Rn+1 and at the origin. In particular, it has density pc/n on each line segment from (0, . . . , Xj = 1, . . . , 0, 0) to (0, . . . , Xj = 1, . . . , 0, 1), where the density is with respect to the Lebesque measure on the line. There is additionally a point mass of probability 1 − pc at the origin. Everywhere on the support of D,
P(Y = 1 | X1:n = x1:n, Xn+1 = xn+1) = xn+1.

Deﬁning the family of hypothesis classes Next, we deﬁne a family of hypothesis classes. Let σ ∈ {−1, 1}n

and deﬁne

f σ(x) = xn+1 − 8ǫ√nσ⊤x1:n.

27

Then we deﬁne 2n hypothesis classes, one for each value of σ: Hσ {h(x) = 1 {f σ(x) ≥ b} | b ∈ [1/4, 3/4]} , H {Hσ | σ ∈ {0, 1}n}.

Now, we can derive the optimal decision rule in hypothesis class Hσ for loss parameter c. Let [f σ(X)]31//44 = max(1/4, min(3/4, f σ(X)) denote the value f σ(X) clamped to the interval [1/4, 3/4]. Then for b ∈ (1/4, 3/4),
P Y = 1 | [f σ(X)]13//44 = b = P Y = 1 | Xn+1 − 8ǫ√nσ⊤X1:n = b
= 1 n P Y = 1 | Xj = 1 ∧ Xn+1 = b + 8ǫ√nσj n j=1 √ 1⊤σ
= b + 8ǫ n n .
where 1 is the all-ones vector. Thus, the Bayes optimal decision rule based on [f σ (X)]31//44 is

hσ(x) = 1

f

σ

(x)

+

√ 8ǫ n

1⊤

σ

≥

c

c

n

=1

f

σ (x)

≥

c

−

√ 8ǫ n

1⊤σ

n

for c − 8ǫ√n 1⊤σ ∈ (1/4, 3/4). The induced posterior probability for Hσ is n

qHσ

(x)

=

f

σ (x)

+

√ 8ǫ n

1⊤σ

.

n

We consider one hypothesis from each hypothesis class Hσ ∈ H. Speciﬁcally, we consider the optimal decision

rule for

cσ = 1 + 8ǫ√n 1⊤σ ,

2

n

which, as shown above is,

hσ(x) = 1 f σ(x) ≥ 1 .

(21)

2

We leave until the end of the proof to show that each of these decision rules hσ for σ ∈ {−1, 1}n satisﬁes the

requirements of Theorem 4.10.

Deriving the lower bound Now, we are ready to derive the lower bound that there is some hσ such that

P(|cˆ(S) −

c|

≥

ǫ)

≥

1 80

.

First,

we

can

rewrite

hσ

from

(21)

as

hσ((0, xj

=

1, 0, xn+1))

=

1{xn+1

−

√ 8ǫ nσj

≥

1/2}

√

= 1{xn+1 ≥ 1/2 + 8ǫ nσj}.

Thus, only decisions made on points where xn+1 ∈ [1/2 − 8ǫ√n, 1/2 + 8ǫ√n] are dependent on σj. Denote

by Ej the event that there is an observed sample that depends on σj:

√

√

Ej

∃xi ∈ S such that xij = 1 ∧ xi,n+1 ∈ [1/2 − 8ǫ n, 1/2 + 8ǫ n].

Suppose we let σj be independently Rademacher distributed, i.e. we assign equal probability 1/2n to each σ ∈ {−1, 1}. Then if Ej does not occur, the sample of decisions S is independent from σj , i.e.
S ⊥⊥ σj | ¬Ej .

Now let F denote the event that more than n/2 of the Ej events occur:

F

|{j ∈ 1, . . . , n | Ej }| > n/2.

We will start by proving a lower bound on P(|cˆ(S) − cσ| ≥ ǫ | ¬F ). If F does not occur, then at least half of the Ej do not occur. Thus at least half of the elements of σ are independent from the sample S. Let I be the set of indices j for which Ej does not occur; thus, σI ⊥⊥ S, and given ¬F , |I| ≥ n/2.

We can decompose cσ into part that depends on σI and part that depends on σIC :

cσ

=

1

+ 8ǫ√n 1⊤σI

+

√ 8ǫ n

1⊤ σI C

.

(22)

2

n

n

28

Note that for each j ∈ I, σj2+1 is 1/2-Bernoulli distributed. Thus Z = 1⊤σI + |I| = σj + 1 ∼ Binom |I|, 1 . 2 j∈I 2 2

We can establish lower bounds on the tails of this given that F occurs:

P Z − |I| ≥ t | ¬F = Z − |I| ≤ −t | ¬F ≥ 1 e−32t2/n.

2

2

15

This lower bound is from Matoušek and Vondrák [42]. Plugging in t = 1 √n, we obtain 8

P Z − |I| ≥ 1 √n | ¬F = Z − |I| ≤ − 1 √n | ¬F ≥ 1

28

2

8

20

P 1⊤σI ≥ 1 √n | ¬F = 1⊤σI ≤ − 1 √n | ¬F ≥ 1 .

(23)

4

4

20

Given S, σIC is completely known (since Ej occurs for each j ∈ IC, revealing σj ). So plugging (23) into (22) gives

P

cσ

−

1

−

√ 8ǫ

n

1⊤

σ

I

C

≥ 2ǫ | ¬F, S

2

n

=P

cσ

−

1

−

√ 8ǫ

n

1⊤

σ

I

C

≤ −2ǫ | ¬F, S

2

n

≥1 20

P (cσ − cσIC ≥ 2ǫ | ¬F, S) = P (cσ − cσIC ≤ −2ǫ | ¬F, S) ≥ 1 . 20

That is, there is at least probability 1/20 that cσ is more than 2ǫ above and below cσIC , given ¬F and the observed sample S.

This

is

enough

to

show

that

P(|cˆ(S) −

cσ |

≥

ǫ

|

¬F, S)

≥

1 40

.

First,

observe

that

P(cˆ(S) ≥ cσIC | ¬F, S) + P(cˆ(S) < cσIC | ¬F, S) = 1,

so one of these probabilities must be at least 1/2. Say WLOG that it is the ﬁrst. Then
P(|cˆ(S) − cσ| ≥ ǫ | ¬F, S) ≥ P(cσ − cσIC ≤ −2ǫ ∧ cˆ(S) ≥ cσIC | ¬F, S)

(=i) P(cσ − cσIC ≤ −2ǫ | ¬F, S) P(cˆ(S) ≥ cσIC | ¬F, S)

≥1 20

1 = 1. 2 40

Here, (i) makes use of the fact that S ⊥⊥ σI | ¬F . Given this, we can ﬁnally derive the lower bound on the unconditional probability that P(|cˆ(S) − cσ| ≥ ǫ):

P(|cˆ(S) − cσ| ≥ ǫ) = P(|cˆ(S) − cσ| ≥ ǫ | F )P(F ) + P(|cˆ(S) − cσ| ≥ ǫ | ¬F )P(¬F ) ≥ P(|cˆ(S) − cσ| ≥ ǫ | ¬F )P(¬F )

≥ P(¬F ) .

(24)

40

So we need to derive a lower bound on P(¬F ). We can√do so by noti√ng that in order for F to occur, there must be at least n/2 samples xi with xi,n+1 ∈ [1/2 − 8ǫ n, 1/2 + 8ǫ n]. The probability of this event for

a particular sample is

√

√

√

P Xn+1 ∈ [1/2 − 8ǫ n, 1/2 + 8ǫ n] = 16pcǫ n.

So at least n/2 of the m samples must have the event with probability 16pcǫ√n occur for F to occur. Let

GE(p, m, r) denote the probability of at least r successes of probability p in m independent trials. Then there

is the following fact from probability theory [43]:

GE(p, m, (1 + γ)mp) ≤ e−γ2mp/3.

Then

√ P(F ) ≤ GE(16pcǫ n, m, n/2)
√ √n = GE 16pcǫ n, 64pcǫ , 2

√n 64pcǫ

√ 16pcǫ n

29

≤ e−n/12 ≤ 3/4
as long as n ≥ 4 as assumed. Thus P(¬F ) > 1/4. So putting this together with (24), we have P(|cˆ(S) − cσ| ≥ ǫ) ≥ 1 . 160
This equation is given with respect to the uniform distribution over σ. But there also must be a particular σ and thus corresponding hσ ∈ Hσ which has the same tails on cˆ(S) − c. Thus we conclude the proof.

Verifying the requirements of Theorem 4.10 Now we show that the distribution and hypothesis class family satisfy the conditions of Theorem 4.10. First, note that all h ∈ H ∈ H are thresholds on linear functions of the observation x. Thus, ∪H∈HH is a subset of the halfspaces in Rn+1 and so it has VC-dimension at most n + 2 = d.

Next, it is clear that for ρ ≤ ǫ,

P(qHσ (X) ∈ (c, c + ρ]) = P

f

σ

(X

)

+

√ 8ǫ n

1⊤

σ

∈

(c, c + ρ]

n

n
=P
j=1

Xj = 1 ∧ Xn+1 − 8ǫ√nσj + 8ǫ√n 1⊤σ ∈ (c, c + ρ] n

n pcρ

=

n = pcρ.

j=1

A similar result can be shown for P(qHσ (X) ∈ [c − ρ, c)).

Finally, we need to show that MD-smoothness holds. Take any hσ and any Hσ˜ . Then the disagreement between hσ and a hypothesis in Hσ˜ with threshold b is

σ

σ˜

pc n 1

√

√

P(h (X) = hb (X) = n

2 + 8ǫ nσj − b − 8ǫ nσ˜j

j=1

pc n =n

1√ 2 + 8ǫ n(σj − σ˜j) − b .

j=1

This is minimized when b is the median of 1 + 8ǫ√n(σj − σ˜j) for j = 1, . . . , n. Thus b ∈ [ 1 − 8ǫ√n, 1 +

8ǫ√n]; since ǫ ≤

2

2

2

1√ , this implies b ∈ [3/8, 5/8]. Suppose now we let c′ ∈ [cσ − 1/8, cσ + 1/8]. Then we

64 n

can let b′ = b + (c′ − cσ) and

MD(hσc′ , Hσ˜ ) ≤ P hσc′ (X) = hσb˜′ (X) = P hσ(X) = hσb˜ (X) = MD(hσ, Hσ˜ ).

Thus for |c′ − cσ| ≤ 1/8, hσ and H are 0-MD-smooth. If |c′ − cσ| > 1/8, then we have

MD(hσc′ , hσ˜ ) ≤ 1 < MD(h8σ, Hσ˜ ) |c′ − cσ|MD(hσ, Hσ˜ ).

Thus overall hσ and H are α-MD-smooth with

α = max

8

.

σ˜=σ MD(hσ , Hσ˜ )

Bibliographic note: we establish dependence on the VC dimension d in Theorem D.2 using a technique similar to that used by Ehrenfeucht et al. [44].

D.2 Necessity of MD-smoothness

The lower bounds given in Section D.1 do not depend on the α parameter from the MD-smoothness assumption made in Theorem 4.3; thus, one may wonder if this assumption is necessary. In the following lemma, we show that it is necessary in some cases by giving an example of an IDT problem where a lack of MD-smoothness precludes identiﬁability of the loss parameter.

Lemma D.3 (No MD-smoothness can prevent identiﬁablity). Let ǫ ∈ (0, 1/10). Then for any IDT algorithm cˆ(·), there is a decision problem (D, c), hypothesis class family H, and hypothesis class H ∈ H satisfying the
conditions of Theorem 4.10 except for MD-smoothness such that

for a sample S of any size m.

P(|cˆ(S) − c| ≥ ǫ) ≥ 1 2

30

X2 P(Y = 1 | X)

1.0

0.5

h1

0.0

−0.5

−1.0 −1.0 −0.5 0.0
X1

1.05

0.90

0.75

0.60

h2

0.45

0.30

0.15

0.00

0.5 1.0

0.75 P(Y = 1 | X1) P(Y = 1 | X2)
0.50 0.25
−1.0 −0.5 0.0 0.5 1.0

Figure 6: A visualization of the distribution and decision rules used in Lemma D.3 to show that a lack
of MD-smoothness can prevent identiﬁability of the loss parameter c. On the left, the distribution over X = (X1, X2) and Y is shown; X has constant density on unit squares in the ﬁrst and third quadrants, and P(Y = 1 | X) varies as shown with the heatmap. We consider two decision rules h1 and h2 which are optimal thresholds of X1 and X2, respectively, for loss parameters c1 = 2/5 and c2 = 3/5, respectively. Since c1 = c2 but P(h1(X) = h2(X)) = 1, it is impossible to identify c reliably. This is because the distribution and decision rules are not MD-smooth, since shifting either
decision rule slightly causes a jump in minimum disagreement with the other hypothesis class from
0 to a positive value.

Proof. Deﬁning the distribution First, we deﬁne a distribution D over X ∈ X = R2 and Y ∈ {0, 1}. DX

has density 1/2 on two squares [−1, 0] × [−1, 0] and [0, 1] × [0, 1], and the distribution of Y | X is deﬁned as

follows:

P(Y = 1 | X = x =

2 3

+

2 15

x

1

+

8 15

x

2

1 3

+

8 15

x

1

+

2 15

x

2

x ∈ [−1, 0] × [−1, 0] x ∈ [0, 1] × [0, 1].

Deﬁning the family of hypothesis classes We consider the two hypothesis classes which are thresholds on one component of the observation x:

H1 = {h(x) = 1{x1 ≥ b} | b ∈ [−1, 1]}, H2 = {h(x) = 1{x2 ≥ b} | b ∈ [−1, 1]}.

That is, H = {H1, H2}. The conditional probabilities for Y = 1 given just one of the observation components

are

qH (x) = P(Y = 1 | X1 = x1) = 2 + 2 x1 + 2 x11{x1 ≥ 0},

1

5 15

5

(25)

qH (x) = P(Y = 1 | X2 = x2) = 3 + 2 x2 + 2 x21{x2 ≤ 0}.

2

5 15

5

We consider the optimal decision rules for c1 = 2/5 and c2 = 3/5 in H1 and H2, respectively, which from the

above can be calculated as

h1(x) = 1{x1 ≥ 0}, h2(x) = 1{x2 ≥ 0}.

The distribution and decision rules are visualized in Figure 6.
Lack of identiﬁability Note that since X only has support where sgn(X1) = sgn(X2), the above decision rules are indistinguishable. Thus, we use the same techniques from Corollary 4.12 and Lemma C.1 to show that for at least one of c ∈ {c1, c2}
P(|cˆ(S) − c| ≥ 1/2(c2 − c1) = 1/10 ≥ ǫ) ≥ 1/2.
Hypothesis classes are not MD-smooth Although this is not required for the proof of the lemma, we will demonstrate that the deﬁned hypothesis classes are not α-MD-smooth for any α. By way of contradiction, assume that there is some α such that h1 and H are MD-smooth. Then for any c′1 ∈ [0, 1],
MD(hc′ , H2) ≤ (1 + α|c′1 − c1|)MD(h1, H2) = 0. 1
Here, MD(h1, H2) since P(h1(X) = h2(X)) = 0, i.e. h1 and h2 do not disagree at all. However, there are clearly values of c′1 such that MD(hc′ , H2) > 0, so we have a contradiction.
1
Verifying the other requirements of Theorem 4.10 Clearly, the family of hypothesis classes deﬁned above have ﬁnite VC-dimension.

31

The densities of qH1 (X) and qH2 (X) can be calculated as the density of X1 or X2 multiplied by the derivative of the inverse of the posterior probability functions. The densities of X1 and X2 are both 1/2 on the interval [−1, 1], and the derivative of the inverse of the equations in (25) is at least 15/8. So the distribution satisﬁes the requirements of Theorem 4.10 other than MD-smoothness with pc ≥ 15/16.

E Feature Subset Hypothesis Class Family

In this section, we work through the application of Theorem 4.10 to a practical example. Theorem 4.10 concerns the case of IDT when the decision maker could be restricting themselves to any suboptimal hypothesis class H ∈ H for some family of hypothesis classes H. In this example, we consider Hfeat as deﬁned in (1) and repeated here:

Hfeat {HS | S ⊆ {1, . . . , n}} where HS h(x) = f (xS ) | f : R|S| → {0, 1} .

(1)

This family can model decision makers that have bounded computational capacity and may only be able to reason based on a few features of the data. An application of structural risk minimization [45] from learning theory shows that the sample complexity of IDT in this case may scale only linearly in the number of features considered and logarithmically in the total feature count:

Lemma E.1. Let a decision maker use a hypothesis class HS ∈ Hfeat as deﬁned in (1) which consists of decision rules depending only on the subset of the features in S. Let s = |S| be the number of such features; neither s nor S is known. Suppose X = Rd, i.e. d is the total number of features. Let assumptions on ǫ, δ, α,
and pc be as in Theorem 4.10.

Let hˆcˆ ∈ arg minhˆ∈HSˆ Rcˆ(hˆ) be chosen to be consistent with the observed decisions, i.e. hˆcˆ(xi) = yˆi, and such that |Sˆ| is as small as possible. Then |cˆ − c| ≤ ǫ with probability at least 1 − δ as long as the number of

samples m satisﬁes

m≥O

α+ 1 ǫ ǫ2

s log d + log(1/δ) . pc

Proof. We prove Lemma E.1 by bounding the VC-dimension of the union of all optimal decision rules in all

HS ∈ Hfeat where |S| ≤ s. An optimal decision rule for loss parameter c in HS is given by the Bayes optimal

classiﬁer:

hSc (x) = 1{P(Y = 1 | XS = xs) ≥ c}.

Now consider a set of observations x1, . . . , xd ∈ X . We will show that for d > 1 + 2s log2(n + 1), this set cannot be shattered by d. To see why, note that decision rules in any particular class HS threshold the posterior probability P(Y = 1 | XS = xs). Thus, each hypothesis class can only produce d + 1 distinct labelings of the set of observations. The number of hypothesis classes HS with |S| ≤ s is

sn

s k

s

s ≤ n ≤ (n + 1) .

k=0

k=0

So the number of distinct labelings assigned by hypotheses in H to the observations must be at most (d+1)(n+ 1)s < 2d if d > 1 + 2s log2(n + 1). Thus this set cannot be shattered, so

VCdim ∪|S|≤sHS ≤ 1 + 2s log2(n + 1) = O(s log n).

Applying Theorem 4.10 with d = O(s log n) completes the proof.

The following lemma states conditions under which α-MD-smoothness holds for Hfeat.
Lemma E.2. Let Hfeat and HS be deﬁned as in (1). Let h ∈ HS . Suppose that there is a ζ > 0 such that for any Sˆ ⊆ {1, . . . , n}, one of the following holds: either (a) P(Y = 1 | X = xS) = P(Y = 1 | X = xSˆ) for all x ∈ Rd, or (b) MD(h, HSˆ) ≥ ζ. Furthermore, suppose that the distribution of qHS (X) is absolutely continuous with respect to the Lebesque measure and that its density is bounded above by M < ∞. Then h and Hfeat are α-MD-smooth with α = M/ζ.
Since α-MD-smoothness is a sufﬁcient condition for identiﬁcation of the loss function parameter c, Lemma E.2 gives conditions under which IDT can be performed. The main requirement is that considering different subsets of the features either gives identical decision rules (case (a)) or decision rules which disagree by some minimum amount (case (b)). If decision rules using a different subset of the features can be arbitrarily close to the true one, it may not be possible to apply IDT.

32

Proof. Consider any Sˆ ⊆ {1, . . . , n}. If (a) holds for Sˆ, then hSc (x) = hScˆ(x) for any c ∈ [0, 1] and x ∈ X . Thus
MD(hSc′ , HSˆ) = 0 ≤ (1 + α|c′ − c|)MD(hSc , HSˆ) = 0 so α-MD-smoothness holds in this case for any α.
If (b) holds, then let hˆ ∈ arg minhˆ∈HSˆ P(h(X) = hˆ(X)). Let c′ ∈ [0, 1]; without loss of generality, we may assume that c′ > c. Denote qS (x) = P(Y = 1 | XS = xs). Then
MD(hSc′ , HSˆ) ≤ P(hSc′ (X) = hˆ(X)) = P qS (X) < c′ ∧ hˆ(X) = 1) + P qS (X) > c′ ∧ hˆ(X) = 0)

≤ P qS (X) ∈ [c, c′) ∧ hˆ(X) = 1 + P qS (X) < c ∧ hˆ(X) = 1 + P qS (X) > c ∧ hˆ(X) = 0)

= P qS (X) ∈ [c, c′) ∧ hˆ(X) = 1

≤ M (c′ − c) + MD(h, HSˆ)

≤ 1 + M (c′ − c) MD(h, H ˆ).

ζ

S

+ MD(h, HSˆ)

So h and H satisfy α-MD-smoothness with α = M/ζ.

F Surrogate Loss Functions

Here, we explore using IDT when the decision maker minimizes a surrogate loss instead of the true loss. So

far, as formulated in Section 3, we have assumed that the decision maker chooses a decision rule h which

minimizes the expected loss E[ℓc(h(X), Y )], where the loss function is deﬁned as



0

yˆ = y

ℓc(yˆ, y) = 1c − c

yˆ = 1 ∧ y = 0 yˆ = 0 ∧ y = 1

= c 1{yˆ = 1}

y=0

(26)

(1 − c) 1{yˆ = 0} y = 1.

However, this loss function is not convex or continuous, so it is difﬁcult to optimize. Thus, we might expect the decision maker to choose their decision rule using a surrogate loss which is convex. In particular, suppose that the decision rule h(·) is calculated by thresholding a function f : X → R:
h(x) = 1{f (x) ≥ 0}.
Then, we can replace the indicator functions in (26) with a surrogate loss V : R → R:

ℓ˜c(w, y) = c V (w)

y=0

(27)

(1 − c) V (−w) y = 1.

Say that the decision maker minimizes this loss ℓ˜c instead of the true loss ℓ:

f ∗ ∈ arg min E[ℓ˜c(f (X), Y )].

(28)

f

The following lemma shows that, for reasonable surrogate losses, if the decision maker is optimal then minimizing the surrogate loss is equivalent to minimizing the true loss. The proof is adapted from Section 4.2 of Rosasco et al. [46]; they show that the hinge loss, squared loss, and logistic loss all satisfy the necessary conditions.
Lemma F.1. Suppose V : R → R is convex and that it is strictly increasing in a neighborhood of 0. Let f ∗ be chosen as in (28), and let h(x) = 1{f ∗(x) ≥ 0}. Then h ∈ arg minh E[ℓc(h(X), Y )]; that is, the threshold of f ∗ is an optimal decision rule for the true cost function.

Proof. We prove the lemma by contradiction; assume that h is not an optimal decision rule for the true loss function. Then by Lemma 4.1,
P(h(X) = 1{q(X) ≥ c} ∧ q(X) = c) > 0.

33

This implies that either

P(h(X) = 0 ∧ q(X) > c) > 0 or P(h(X) = 1 ∧ q(X) < c) > 0,

or equivalently,

P(f ∗(X) < 0 ∧ q(X) > c) > 0 or P(f ∗(X) ≥ 0 ∧ q(X) < c) > 0.

(29)

Without loss of generality, assume the former. Deﬁne

f˜(x) = f0∗(x)

f ∗(x) < 0 ∧ q(x) > c otherwise.

Consider any x which satisﬁes f ∗(x) < 0 and q(x) > c. We can write

E ℓ˜c(f ∗(X), Y ) − ℓ˜c(f˜(X), Y ) | X = x

= P(Y = 0 | X = x) c V (f ∗(x)) − V (f˜(x)) + P(Y = 1 | X = x) (1 − c) = (1 − q(x)) c (V (f ∗(x)) − V (0)) + q(x) (1 − c) (V (−f ∗(x)) − V (0)) = ℓ˜c(f ∗(x) | x) − ℓ˜c(0 | x),

V (−f ∗(x)) − V (−f˜(x))

where we deﬁne

ℓ˜c(w | x) = (1 − q(x)) c V (w) + q(x) (1 − c) V (−w).

ℓ˜c(w | x) satisﬁes two properties:

1. It is convex in w, since it is a sum of two convex functions.
2. It is strictly decreasing in w in a neighborhood of 0. To see why, note that we assumed q(x) > c, so (1 − q(x)) c < (1 − c) c < q(x) (1 − c).
Thus, since the weight on V (−w) is greater than the weight on V (w), and V (w) is strictly increasing about 0, ℓ˜c(w | x) must be strictly decreasing about 0.

Together, these properties imply that ℓ˜c(f ∗(x) | x) − ℓ˜c(0 | x) > 0
since we assumed that f ∗(x) < 0. Thus we have that

E ℓ˜c(f ∗(X), Y ) − ℓ˜c(f˜(X), Y ) | X = x > 0

(30)

for any x where f ∗(x) < 0 and q(x) > c. Now, we analyze the difference in expect loss for f ∗ and f˜. Since these agree on all points except when f ∗(x) < 0 and q(x) > c, we have that
E[ℓ˜(f ∗(X), Y )] − E[ℓ˜(f˜(X), Y )] = E ℓ˜(f ∗(X), Y ) − ℓ˜(f˜(X), Y ) f ∗(X) < 0 ∧ q(X) > c P f ∗(X) < 0 ∧ q(X) > c

(i)

> 0.

(31)

Here, (i) is due to the combination of (30), which implies the ﬁrst term is positive, and the ﬁrst case of (29), which implies the second term is positive.
(31) implies that f˜ has lower expected surrogate loss than f ∗. However, we assumed that f ∗ minimized the expected surrogate loss; thus we have a contradiction.

Lemma F.1 means that all the results for an optimal decision maker (e.g., Theorem 4.2) apply immediately to a decision maker minimizing a reasonable surrogate loss. In the case of decision problems without uncertainty, the decision rule will encounter zero loss and thus must be optimal, so Lemma F.1 also applies in this case for an optimal or suboptimal decision maker (e.g., Corollary 4.12). In the case of a suboptimal decision maker facing uncertainty, different loss functions may lead to different decision rules, so we cannot extend the results in that case to surrogate losses. Table 1 summarizes which results hold equivalently for decision makers minimizing an expected surrogate loss.

34

Setting
IDT for optimal decision maker (Theorem 4.2) IDT for suboptimal decision maker (Theorems 4.7 and 4.10) No identiﬁability for decisions without uncertainty (Corollary 4.12)

True loss
✓ ✓ ✓

Surrogate loss
✓ ✗ ✓

Table 1: An overview of which of our results apply in the setting when the decision maker is minimizing a surrogate loss rather than the true loss.

G Further Comparison to Prior Work
In this section, we compare two prior papers on preference learning to our results. Mindermann et al. [20] and Bıyık et al. [21] both propose methods for active preference learning, i.e. querying a person to learn their preferences. In each method, queries are prioritized which minimize the uncertainty of the person. The authors argue that such queries are easier to answer and thus lead to more effective preference learning. At ﬁrst, these results may seem to contradict our ﬁndings that uncertain decisions make preference learning easier. However, we argue that their results are not in conﬂict with ours. Decisions with more uncertainty are probably more difﬁcult for people to make, and those close to the decision boundary are probably the most difﬁcult. However, our results show that it is necessary to observe such decisions in order to recover the person’s preferences. If we cannot observe decisions made arbitrarily close to the person’s decision boundary, we cannot exactly characterize the loss function they are optimizing. Thus, combining the results of Mindermann et al. [20] and Bıyık et al. [21] with ours suggests that there is a tradeoff between the ease of the decision problem for the human and the identiﬁability of their preferences. That is, uncertainty may make the human’s decision problem more difﬁcult but our problem of identifying preferences easier.

35

