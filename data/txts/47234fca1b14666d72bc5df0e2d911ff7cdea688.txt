1
Hypergraph Spectral Clustering in the Weighted Stochastic Block Model
Kwangjun Ahn, Kangwook Lee, and Changho Suh

arXiv:1805.08956v1 [math.ST] 23 May 2018

Abstract—Spectral clustering is a celebrated algorithm that partitions objects based on pairwise similarity information. While this approach has been successfully applied to a variety of domains, it comes with limitations. The reason is that there are many other applications in which only multi-way similarity measures are available. This motivates us to explore the multi-way measurement setting. In this work, we develop two algorithms intended for such setting: Hypergraph Spectral Clustering (HSC) and Hypergraph Spectral Clustering with Local Reﬁnement (HSCLR). Our main contribution lies in performance analysis of the poly-time algorithms under a random hypergraph model, which we name the weighted stochastic block model, in which objects and multi-way measures are modeled as nodes and weights of hyperedges, respectively. Denoting by n the number of nodes, our analysis reveals the following: (1) HSC outputs a partition which is better than a random guess if the sum of edge weights (to be explained later) is Ω(n); (2) HSC outputs a partition which coincides with the hidden partition except for a vanishing fraction of nodes if the sum of edge weights is ω(n); and (3) HSCLR exactly recovers the hidden partition if the sum of edge weights is on the order of n log n. Our results improve upon the state of the arts recently established under the model and they ﬁrstly settle the order-wise optimal results for the binary edge weight case. Moreover, we show that our results lead to efﬁcient sketching algorithms for subspace clustering, a computer vision application. Lastly, we show that HSCLR achieves the information-theoretic limits for a special yet practically relevant model, thereby showing no computational barrier for the case.
I. INTRODUCTION
T HE problem of clustering is prevalent in a variety of applications such as social network analysis, computer vision, and computational biology. Among many clustering algorithms, spectral clustering is one of the most prominent algorithms proposed by [2] in the context of image segmentation, viewing an image as a graph of pixel nodes, connected by weighted edges representing visual similarities between two adjacent pixel nodes. This approach has become popular, showing its wide applicability in numerous applications, and has been extensively analyzed under various models [3]–[5] .
While the standard spectral clustering relies upon interactions between pairs of two nodes, there are many applications where interaction occurs across more than two nodes. One such application includes a social network with online social
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2015R1C1A1A02036561). This paper was presented in part at the IEEE International Symposium on Information Theory 2017 [1].
Kwangjun Ahn is with the Department of Mathematical Sciences, KAIST (e-mail: kjahnkorea@kaist.ac.kr).
Kangwook Lee and Changho Suh are with the School of Electrical Engineering, KAIST (e-mail: {kw1jjang, chsuh}@kaist.ac.kr).

communities, called folksonomies, in which users attach tags

to resources. In the example, a three-way interaction occurs

across users, resources and annotations [6]. Another appli-

cation is molecular biology, in which multi-way interactions

between distinct systems capture molecular interactions [7].

See [8] and the list of applications therein. Hence, one

natural follow-up research direction is to extend the celebrated

framework of graph spectral clustering into a hypergraph

setting in which edges reﬂect multi-way interactions.

As an effort, in this work, we consider a random weighted

uniform hypergraph model which we call the weighted

stochastic block model, which is a special case of that

considered in [8]. An edge of size d is homogeneous if it

consists of nodes from the same group, and is heterogeneous otherwise.1 Given a hidden partition of n nodes into k groups, a weight is independently assigned to each edge of size d

such that homogeneous edges tend to have higher weights

than heterogeneous edges. More precisely, for some constants

p > q, the expectation of homogeneous edges’ weights is pαn and that of heterogeneous edges’ weights is qαn.2 Here, αn
captures the sparsity level of the weights, which may decay

in n. The task here is to recover the hidden partition from

the weighted hypergraph. In particular, we aim to develop

computationally efﬁcient algorithms that provably ﬁnd the

hidden partition.

Our contributions: By generalizing the spectral clustering

algorithms proposed for the graph clustering, we ﬁrst pro-

pose two poly-time algorithms which we name Hypergraph

Spectral Clustering (HSC) and Hypergraph Spectral Cluster-

ing with Local Reﬁnement (HSCLR). We then analyze their

performances, assuming that the size of hyperedges is d, the number of clusters k is constant, and the size of each group is linear in n. Our main results can be summarized as follows. For some constants c and c′, which depend only on p, q, and k, the following statements hold with high probability:

• Detection: If

n d

αn

≥

c · n,

the

output

of

HSC

is

more

consistent with the hidden partition than a random guess;

• Weak consistency: If

n d

αn

=

ω(n),

HSC

outputs

a

partition which coincides with the hidden partition except

o(n) number of nodes; and

• Strong consistency: If

n d

αn

≥

c′ · n log n,

HSCLR

exactly recovers the hidden partition.

1While edges of a graph are pairs of nodes, edges of a hypergraph (or hyperedges) are arbitrary sets of nodes. Further, the size of an edge is the number of nodes contained in the edge.
2For illustrative purpose, we focus on a symmetric setting. In Sec. V, we will extend our results (to be described later) to a more general setting.

2

TABLE I: Comparison to the state of the arts: “Weak” and “Strong” are for consistency results.

[9] [10] [11] [8] [12] Ours

Model assumption

Multi Weighted

Gr√oups √ √ √

Edges × × √×

√×

√×

Order of

n d

αn

required

for

Detection

Weak

Strong

NA
NA NA NA Ω(n) Ω(n)

nd nd Ω(n(log n)2) Ω(n(log n)2)
NA
ω(n)

NA NA NA NA NA Ω(n log n)

We remark that our main results are the ﬁrst order-wise optimal results for the binary edge weight case (see Proposition 1).

A. Related work

1) Graph Clustering: The problem of standard graph clustering, i.e., d = 2, has been studied in great generality. Here, we summarize some major developments, referring the readers to a recent survey by Abbe [13] for details. The detection problem, whose goal is to ﬁnd a partition that is more consistent with the hidden partition than a random guess, has received a wide attention. A notable work by Decelle et al. [14] ﬁrstly observes phase transition and conjectures the transition limit. Further, they also conjecture that the computational gap exists for the case of k ≥ 4. For the case of k = 2, the phase transition limit is fully settled jointly by [15] and [16], [17]: The impossibility of the detection below the conjectured threshold is established in [15], and it is proved that the conjectured threshold can be achieved via some efﬁcient algorithms in [16], [17]. The limits for the case k ≥ 3 have been studied in [18]–[21], and are settled in [22].
The weak/strong consistency problem aims at ﬁnding a cluster that is correct except a vanishing or zero fraction. The necessary and sufﬁcient conditions for weak consistency have been studied in [23]–[27], and those for strong consistency in [25], [27]–[29]. In particular for strong consistency, both the fundamental limits and computationally efﬁcient algorithms are investigated initially for k = 2 [25], [28], [29], and recently for general k [27]. While most of the works assume that the graph parameters such as p, q, k, and the size of clusters are ﬁxed, one can also study the minimax scenario where the graph parameters are adversarially chosen against the clustering algorithm. In [30], the authors characterize the minimax-optimal rate. Further, [24] shows that the minimaxoptimal rate can be achieved by an efﬁcient algorithm.

2) Hypergraph Clustering: Compared to graph clustering,

the study of hypergraph clustering is still in its infancy. In

this section, we brieﬂy summarize recent developments. For

detection, analogous to the work by Decelle et al. [14], An-

gelini et al. [31] ﬁrstly conjecture phase transition thresholds.

These conjectures have not been settled yet unlike the graph

case. In [8], the authors study a speciﬁc spectral clustering

algorithm, which can be shown to detect the hidden cluster

if

n d

αn

=

Ω(n(log n)2),

while

the

conjectured

threshold

for

detection is

n d

αn

=

c⋆n

for

some

constant

c⋆.

Actually,

this gap is due to the technical challenge that is speciﬁc to

the hypergraph clustering problem: See Remark 7 for details.

In [12], the authors study the bipartite stochastic block model,

and as a byproduct of their results, they show that detection

is possible under some speciﬁc model if

n d

αn

=

Ω(n).

While this guarantee is order-wise optimal, it holds only when

edge weights are binary-valued and the size of two clusters

are equal. Our detection guarantee, obtained by delicately

resolving the technical challenges speciﬁc to hypergraphs, is

also order-wise optimal but does not require such assumptions.

While several consistency results under various models

are shown in [8]–[12], to the best of our knowledge, our

consistency guarantees are the ﬁrst order-wise optimal ones.

We brieﬂy overview the existing results below. In [9], [10],

the authors derive consistency results for the case in which

αn = 1 and weights are binary-valued. In [8], the authors

investigate consistency results of a certain spectral clustering

algorithm under a fairly general random hypergraph model,

called the planted partition model in hypergraphs. Indeed, our

hypergraph model is a special case of the planted partition

model, and hence the algorithm proposed in [8] can be applied

to our model as well. One can show that their algorithm is

weakly consistent if

n d

αn

=

Ω(n(log n)2)

under

our

model.

The case of non-uniform hypergraphs, in which the size of

edges may vary, is studied in [11]. See Table I for a summary.

While most of the existing works focus on analyzing the

performance of certain clustering algorithms, some study the

fundamental limits. In [1], [32], the information-theoretic lim-

its are characterized for speciﬁc hypergraph models. In [33],

the minimax optimal rates of error fraction are derived for

the binary weighted edge case. However, it has not been

clear whether or not a computationally efﬁcient algorithm

can achieve such limits. In this work, we show that HSCLR

achieves the fundamental limit for the model considered in [1].

3) Main innovation relative to [1]: The new algorithms

proposed in this work can be viewed as strict improvements

over the algorithm proposed in our previous work [1]. First,

the algorithm of [1] cannot handle the sparse-weight regime,

i.e.,

n d

αn

=

Θ(n).

In

order

to

address

this,

we

employ

a

preprocessing step prior to the spectral clustering step. It turns

out this can handle the sparse regime; see Lemma 2 for details.

Another limitation of the original algorithm is related to its

reﬁnement step (to be detailed later). The original reﬁnement

step is tailored for a speciﬁc model, which assumes binary-

valued weights and two clusters (see Deﬁnition 7). On the

other hand, our new reﬁnement step can be applied to the

general case with weighted edges and k clusters. Further, the

original reﬁnement step involves iterative updates, and this is

solely because our old proof holds only with such iterations.

However, we observe via experiments that a single reﬁnement

step is always sufﬁcient. By integrating a well-known sample

splitting technique into our algorithm, we are able to prove

that a single reﬁnement step is indeed sufﬁcient.

Apart from the improvements above, we also propose a

sketching algorithm for subspace clustering based on our new

algorithm, and we show that it outperforms existing schemes

in terms of sample complexity as well as computational

complexity.

4) Computer vision applications: The weighted stochastic block model that we consider herein is well-ﬁtted into com-

3

puter vision applications such as geometric grouping and subspace clustering [34]–[36]. The goal of such problems is to cluster a union of groups of data points where points in the same group lie on a common low-dimensional afﬁne space. In these applications, similarity between a ﬁxed number of data points reﬂects how well the points can be approximated by a low-dimensional ﬂat. By viewing these similarities as the weights of edges in a hypergraph, one can relate it to our model. Note that edges connecting the data points from the same low-dimensional afﬁne space have larger weights compared to other edges: See Section VI for detailed discussion.

5) Connection with low-rank tensor completion: Our model

bears strong resemblance to the low-rank tensor comple-

tion. To see this, consider the following model: for each

e = {i1, i2, i3} ∈ E, edge weight of e is generated as We = pXe (where Xe ∼ Bern(αn)) if (i1, i2, i3) are from the

same cluster; We = qXe otherwise. This model generates a

weighted hypergraph, whose weights are either p, q, or 0. Now,

view each weight as an observation of an entry of a hidden

tensor T, whose entries Ti1i2i3 = p if (i1, i2, i3) are from the same cluster; Ti1i2i3 = q otherwise. Here, 0 weight indicates

that the entry is “unobserved”. Then, the knowledge of hidden

partition will directly lead to “completion” of unobserved

entries. This way, one can draw a parallel between hypergraph

clustering and the low-rank tensor completion.3 This connec-

tion allows us to compare our results with the guarantee in the

tensor completion literature. For instance, the sufﬁcient con-

dition for vanishing estimation error, i.e., weak consistency,

derived in [38] reads

n d

αn

=

ω(n3/2 log4 n),

while

ours

reads

n d

αn

=

ω(n).

This

favors

our

approach.

Moreover,

a

more interesting implication arises in computational aspects.

Notice that a na¨ıve lower bound for tensor completion is4

n d

αn

=

Ω(n),

and

the

tensor

completion

guarantee

comes

with an additional Ω(n1/2) factor to the lower bound. Actually

this gap has not been closed in the literature, raising a question

whether this information-computation gap is fundamental.

Interestingly, this gap does not appear in our result, hence

hypergraph clustering can shed new light on the computational

aspects of tensor completion. Recently, a similar observation

has been made independently in [39] for spike-tensor-related

models (see Sec. 4.3. therein).

B. Paper organization
Sec. II introduces the considered model; in Sec. III, our main results are presented along with some implications; in Sec. IV, we provide the proofs of the main theorems; in Sec. V, we discuss as to how our results can be extended and adapted to other models; Sec. VI is devoted to practical applications relevant to our model, and presents the empirical performances of the proposed algorithms; and in Sec. VII, we conclude the paper with some future research directions.

3Here, T is of rank at most k since it admits a CP-decomposition [37] T = q1⊗3 + ki=1(p − q)(Z∗i)⊗3.
4The number of free parameters deﬁning a rank k, d-th order, ndimensional tensor is ndk, which scales like Θ(n) when d and k are ﬁxed.

C. Notations

Let Mi∗ (M∗j) be the ith row (the jth column) of matrix

M. For a positive integer n, [n] := {1, 2, . . . , n}. For a set A

and an integer m,

A m

:= {B ⊂ A : |B| = m}. Let log(·)

denote the natural logarithm. Let I{·} denote the indicator

function. For a function F : A → B and b ∈ B, F −1(b) :=

{i ∈ A : F (i) = b}.

II. THE WEIGHTED STOCHASTIC BLOCK MODEL

We ﬁrst remark that our deﬁnition of the weighted SBM is a generalization of the original model for graphs [40], [41] to a hypergraph setting. For simplicity, we will focus on the following symmetric assortative model in this paper. In Sec. V, we generalized our results to a broader class of graph models.

1) Model: Let V = [n] be the indices of n nodes, and E :=

[n] d

be the set of all possible edges of size d for a ﬁxed integer

d ≥ 2. Let Ψ : V → [k] be the hidden partition function that

maps n nodes into k groups for a ﬁxed integer k. Equivalently,

the membership function can be represented in a matrix form

Z ∈ {0, 1}n×k, which we call the membership matrix, whose

(i, j)th entry takes 1 if j = Ψ(i) and 0 otherwise. We denote

by ni the size of the jth group for j = 1, 2, . . . , k, i.e., nj := |Ψ−1(j)|. Let nmin := minj nj and nmax := maxj nj. An

edge e = {i1, . . . , id} is homogeneous if Ψ(i1) = Ψ(i2) =

· · · = Ψ(id) and heterogeneous otherwise. We now formally

deﬁne the weighted SBM.

Deﬁnition 1 (The weighted SBM(p, q, αn)). A random weight We ∈ [0, 1] is assigned to each edge e independently5: for homogeneous edges, E[We] = pαn; and for heterogeneous
edges, E[We] = qαn.

Note that the weighted SBM does not assume a speciﬁc edge weight distribution but only speciﬁes the expected values. For instance, it can capture the case with a single location family distribution with different parameters as well as the case with two completely different weight distributions.

Example 1 (The unweighted hypergraph case). For homo-
geneous edges, We ∼ Bern(pαn); and for heterogeneous edges, We ∼ Bern(qαn). This is an instance of the weighted SBM(p, q, αn). When d = 2, it captures the standard models
such as planted multisection [3] and the SBM [42].

Example 2 (The weighted hypergraph case). For homoge-
neous edges, We ∼ Bern(0.75); and for heterogeneous edges, We ∼ Unif[0, 1], a uniform distribution on [0, 1]. This model can be seen as an instance of the weighted SBM(0.75, 0.5, 1).

2) Performance metric: Given {We}e∈E and the number of

clusters k, we intend to recover a hidden partition Ψ up to a

permutation. Formally, for any estimator Φ : [n] → [k], we

deﬁne the error fraction as err(Φ) :=

1 n

minΠ∈P

|{i

:

Ψ(i) =

Π(Φ(i))}|, where P is the collection of all permutations of

[k]. We study three types of consistency guarantees [13], [43].

5Our results hold as long as the weights are upper bounded by any ﬁxed positive constant since one can always normalize the edge weights such that they are within [0, 1]. The global upper bound on the edge weights are required for deriving our large deviation results (Lemmas 3 and 5) in the proof.

4

Algorithm 1 HSC

1: Input: A weighted hypergraph H = ([n], {We}e∈E ), the

number of clusters k.

2: Compute the processed similarity matrix A0: Compute

the similarity matrix A where Aij = e: {i,j}⊂e We if i = j; and Aij = 0 otherwise. Then, obtain A0

by zeroing-out row i (and the corresponding column) if

j Aij

>

cthr

1 n

i,j Aij , where cthr > 0 is a constant

depending only on d (e.g., cthr = 6 when d = 2).

3: Apply spectral clustering to A0: Find k largest eigenvec-

tors of A0, stack them side by side to obtain U0 ∈ Rn×k,

and cluster the rows of U0 using the approximate geomet-

ric k-clustering [48] with an approximation rate ǫ > 0.

4: Output: ΦHSC(i) = cluster index of the ith row.

Deﬁnition 2 (Recovery types). An estimator Φ is
• strongly consistent if limn→∞ Pr(err(Φ) = 0) = 1; • weakly consistent if limn→∞ err(Φ) = 0 in prob.; and • is solving detection if it outputs a partition which is more
consistent relative to a random guess.6

III. MAIN RESULTS

A. Hypergraph Spectral Clustering

Hypergraph Spectral Clustering (HSC) is built upon the
spectral relaxation technique [10] and the spectral algo-
rithms [5], [24], [26], [44]–[47]. The ﬁrst step of the algorithm is to compute the processed similarity matrix whose entries
represent similarities between pairs. To this end, we ﬁrst compute the similarity matrix A, where Aij = e: {i,j}⊂e We if i = j; Aij = 0 if i = j. This is inspired by the spectral relaxation technique in [10]. Next, we zero-out every row and column whose sum is larger than a certain threshold, constructing an output A0, which we call the processed similarity matrix. We then apply spectral clustering to the processed similarity matrix. That is, we ﬁrst ﬁnd the k largest eigenvectors U0 ∈ Rn×k of A0, and cluster n rows of U0 using the approximate geometric k-clustering [48]. Note that HSC is non-parametric, i.e., it does not require the knowledge
of model parameters. See Alg. 1 for the detailed procedure.

Remark 1. The zeroing-out procedure, proposed in [44] (see

Sec. 3 therein), is used to remove outlier rows whose sums

are much larger than the average. This is necessary since if

such outliers exist, the eigenvector estimate will be biased,

and hence the spectral clustering will also fail. Note that

this technique is widely adopted in various graph clustering

algorithms [26], [45], [49].

The time complexity of HSC is O nd . As each edge

appears

2

d 2

times during the construction of the similarity

matrix, this step requires 2

d 2

|E |

=

O

nd

time. The ﬁrst

k eigenvectors can be computed via power iterations, which

can be done within O(kn2 log n) time [50]. Geometric k-

clustering can be done in time O(n(log n)k) [48].

6Here we provide an informal deﬁnition for simplicity. See Deﬁnition 7 in [13] for the formal deﬁnition.

Algorithm 2 HSCLR

1: Input: A weighted hypergraph H = ([n], {We}e∈E ), the

number of clusters k, and sample splitting rate β > 0.

2: Randomly split E: for small enough β > 0, include each

edge of E in E1 independently with probability β. Denote

by E2 the complement of E1.

3: Apply Hypergraph Spectral Clustering to H1 =

([n], {We}e∈E1 ) to yield an estimate ΦHSC.

4: Local reﬁnement: for i = 1, 2, . . . , n, ΦHSCLR(i) =

arg maxj∈[k]

1 |E (i) (j)|

e∈E(i)(j) We.

5: Output: ΦHSCLR.

B. Hypergraph Spectral Clustering with Local Reﬁnement

Our second algorithm consists of two stages: HSC and local
reﬁnement. The HSCLR algorithm is inspired by a similar
reﬁnement procedure, which has been proposed for the graph
case [27], [28]. The algorithm begins with randomly splitting edges into two sets E1 and E2. For small β > 0, we assign each edge to E1 independently with probability β. E2 is the complement of E1. Then, we run HSC on H1 = ([n], {We}e∈E1). Next, we do local reﬁnement with E2. For i ∈ [n] and j ∈ [k], deﬁne E(i)(j) to be the set of edges ( ∈ E2) which connect node i with d − 1 nodes from Φ−HS1C(j), i.e., E(i)(j) := e ∈ E2 : i ∈ e, (e \ {i}) ⊂ Φ−HS1C(j) . Then, for each i ∈ [n], we update ΦHSC(i) with

1

arg

max
j∈[k]

|E (i) (j )|

We.

(1)

e∈E(i) (j)

That is, the reﬁnement step ﬁrst measures the ﬁtness of each

node with respect to different clusters, and updates the cluster

assignment of each node accordingly. Note that HSCLR is

also non-parametric. See Alg. 2 for the detailed procedure.

The time complexity of HSCLR is O nd . For each node

i, the local reﬁnement requires

k j=1

|E

(i)

(j)|

ﬂops,

which

is bounded by k|Ei|, where |Ei| is the number of edges

containing node i. As i |Ei| = d|E|, the local reﬁnement step can be done within O (|E|) time.

Remark 2. HSCLR is inspired by the recent paradigm of solving non-convex problems, which ﬁrst approximately estimates the solution, followed by some local reﬁnement. This two-stage approach has been applied to a variety of contexts, including matrix completion [51], [52], phase retrieval [53], [54], robust PCA [55], community recovery [28], [56], EMalgorithm [57], and rank aggregation [58].

C. Theoretical guarantees

Theorem 1. Let ΦHSC be the output of HSC. Suppose that

nmax n

=

O(1).

Then,

there

exist

constants

c0, c1

>

0

(where

min

p and q) such that if n αn ≥ c0n, then,

c1 depends on

d

n3

err(ΦHSC) ≤ c1k d(d − 1)n2 n αn

(2)

min d

w.p. 1 − O(n−1), provided that c1k d(d−1)nn23 (n)α < 1. min d n

Proof: See Sec. IV-A.

5

Note that when d = 2, Thm. 1 recovers [24, Thm. 6].

Remark 3. We remark a technical challenge that arises in proving Thm. 1 relative to the graph case. Actually, the key step in the proof is to derive the sharp concentration bound on a certain matrix spectral norm (to be detailed later). But the bounding technique employed in the graph case does not carry over to the hypergraph case, as the matrix has strong dependencies across entries. We address this challenge by developing a delicate analysis that carefully handles such dependencies. See Remark 7 in Sec. IV for details.

Corollary 1 (Detection). Suppose that nnmmainx = O(1). There

exists a constant c2 depending on p, q and k such that HSC

solves detection if

n d

αn

≥

c2 · n.

Proof: In Thm. 1, err(ΦHSC) = O(1/c) when αn

satisﬁes

n d

αn

≥

cn

for

sufﬁciently

large

c

>

0.

Remark 4. We compare our algorithm to the one proposed

in [31]. To compare, we ﬁrst note that in the graph case, the

threshold for detection [14] is achieved by new methods based

on the non-backtracking operator [17], [22], [59]. In [59], the

spectral analysis based on a plain adjacency matrix is shown

to fail, while the one based on the non-backtracking operator

succeeds. Recently, it is shown that the non-backtracking

based approach can be extended to the hypergraph case, and it

is empirically observed to outperform a spectral method that

is similar to HSC except the preprocessing step [31].

Corollary

2

(Weak

consistency).

Suppose

that

nmax n

=

O(1).

n αn = ω(n).

min

HSC is weakly consistent if d

Proof: By (2),

n d

αn

=

ω(n)

⇒

err(ΦHSC )

=

o(1).

Remark 5. When specialized to weighted stochastic block

model, the weak consistency guarantee of [8] becomes

n d

αn

=

Ω(n(log n)2),

which

comes

with

an

extra

poly-

logarithmic factor gap to ours.

The following theorem provides the theoretical guarantee of HSCLR. See Sec. IV-B for the proof.

Theorem

2

(Strong

consistency).

Suppose that

nmax n

=

O(1).

Then, HSCLR with sampling rate7 β = lolgolgong nmiins strongly

consistent provided that for any ǫ > 0,

(p − q)2 n

(n/nmin)d−1

p

d αn ≥ (8 + ǫ)

d

n log n . (3)

Remark 6. We remark that Thm. 2 characterizes the perfor-

mance of our non-parametric algorithm for any hypergraphs

with (bounded) real-valued weights. Hence, one may obtain a

tighter threshold and a parametric algorithm by focusing on a

more speciﬁc hypergraph model. For instance, in [60], Chien

et al. derive a tighter bound for the binary weight case. As

a concrete example, when d = 3 and k = 2 with two equal-

s(√izepd−cl√usqte)r2s,n3thαe nsu≥fﬁc1i12enntlcoognnd,itiwohniloef Tthhamt o4f.1Tihnm[6. 02] rreeaaddss

(p−q)2 p

n 3

αn

≥

32 3

n

lo

g

n

.

7We note that β can be chosen arbitrarily as long as β = o(1) and β = ω(1/ log n). See Sec. IV-B for detail.

Indeed, by leveraging recent works on phase transition of random hypergraphs [61], [62], we can prove the order-wise optimality of our algorithms for the binary-valued edge case.

Proposition 1. For the binary-valued edge case, there is no

estimator which

• solves detection when

n d

αn

= o(n)

• is weakly consistent when

n d

αn

= O(n);

and

• is strongly consistent when

n d

αn

=

o(n log n).

Proof: If

n d

αn

=

o(n),

the

fraction

of

isolated

nodes

approaches 1, hence detection is infeasible. In [61], the

authors show that if

n d

αn

=

Θ(n),

there

is

no

connected

component of size (1−o(1))n, implying that weak consistency

is infeasible. Lastly, [62] shows that

n d

αn

>

c · n log n

for

some constant c > 0 is required for connectivity, a necessary

condition for strong consistency.

IV. PROOFS

A. Proof of Theorem 1

We ﬁrst outline the proof. Proposition 2 asserts that spectral clustering ﬁnds the exact clustering if E[A] is available instead of A0. We then make use of Lemma 1 to bound the error fraction in terms of A0 − E[A] . Finally, we derive a new concentration bound for the above spectral norm, and combine
it with Lemma 1 to prove the theorem.
Consider two off-diagonal entries Ai,j and Ai′,j′ such that Ψ(i) = Ψ(i′) and Ψ(j) = Ψ(j′). One can see from the deﬁnition that Ai,j is statistically identical to Ai′j′ , so E[Aij ] = E[Ai′j′ ]. Hence, by deﬁning a k × k matrix B such that Bℓ,m = E[Aij ], where i ∈ Ψ−1(ℓ), j ∈ Ψ−1(m), for some i = j, one can verify that P := ZBZT coincides with E[A] except for the diagonal entries. Our model implies that the diagonal entries of B are strictly larger than its offdiagonal entries, so B is of full rank.
Proposition 2. (Lemma 2.1 in [5]) Consider B ∈ Rk×k of full rank and the membership matrix Z ∈ {0, 1}n×k. Let P = ZBZT . Then the matrix U ∈ Rn×k whose columns are the ﬁrst k eigenvectors of P satisﬁes:Ui∗ = Uj∗ whenever Ψ(i) = Ψ(j); Ui∗ and Uj∗ are orthogonal whenever Ψ(i) = Ψ(j). In particular, a clustering algorithm on the rows of U will exactly output the hidden partition.

Proposition 2 suggests that spectral clustering successfully ﬁnds Ψ if P is available. We now turn to the case where A0 is available instead of P. It is developed in [5] a general
scheme to prove error bounds for spectral clustering under an assumption that k-clustering step outputs a “good” solution. To clarify the meaning of “goodness”, we formally describe the k-means clustering problem.

Deﬁnition 3 (k-means clustering problem). The goal is to

cluster the rows of an n×k matrix U. Deﬁne the cost function

of a partition Φ : [n] → [k] as cost(Φ) =

k j=1

Var(Φ−1

(j

)),

2

where Var(A) =

i∈A

Ui∗

−

1 |A|

ℓ∈A Uℓ

. We

say Φ is (1 + ǫ)-approximate if cost(Φ) ≤ (1 +

ǫ) minΦ′:[n]→[k] cost(Φ′) .

6

We now introduce the general scheme to prove error bounds, formally stated in the following lemma.

Lemma 1. Assume that P is deﬁned as in Proposition 2 and σmin(P) is the smallest non-zero singular value of P. Let M be any symmetric matrix and U ∈ Rn×k be the k largest eigenvectors of M. Suppose a (1 + ǫ)-approximate solution Φ for a constant ǫ > 0. Then, for some c3 > 0, err(Φ) ≤ c3k(1 + ǫ) σM min−(PP)22 , provided that c3k(1 + ǫ) σM min−(PP)22 ≤ 1.
Proof: We refer to [5] for the proof. Thm. 1.2. in [48] implies that a (1+ǫ)-approximate solution can be found using the approximate geometric k-clustering.8 Hence, the above lemma implies that one needs to bound A0 − P in order to analyze the error fraction of the spectral clustering. Our technical contribution lies mainly in deriving such concentration bound, formally stated below.

Lemma 2. There exist constants cthr (depending only on

d), c4, c5 > 0 such that the processed similarity matrix A0 with constant cthr (see Alg. 1) satisﬁes A0 − P ≤

c4

n

n−2 d−2

αn

with

probability

exceeding

1 − O(n−1),

pro-

vided

that

n

n−2 d−2

pαn

≥

c5.

Proof: See Appendix A.

Note that this lemma holds for a ﬁxed d. We now conclude

the proof with these lemmas. Let µ :=

n−2 d−2

αn.

We

ﬁrst

estimate σmin(P).

Claim 1. σmin(P) ≤ c6nminµ for some constant c6 > 0.

Proof: By de√ﬁnitio√n, P = √(Z∆−1)∆B∆T (Z∆−1)T , where ∆ = diag( n1, n2, . . . , nk). Since the columns of Z∆−1 are orthonormal, σmin(P) = σmin(∆B∆T ). One can show that σmin(∆B∆T ) ≥ σmin(∆)2σmin(B). Hence, σmin(P) ≥ σmin(∆)2σmin(B) = nminσmin(B). Hence, we calculate σmin(B). By the deﬁnition of B,

Bℓm =

pαn

nℓ −2 d−2

qαn n−2

+ qαn

n−2 d−2

−

nℓ −2 d−2

d−2

 (nℓ−2)

=

µ

·



d−2
( ) n−2 d−2

(p

−

q)

+

q

if ℓ = m,

q

if ℓ = m.

if ℓ = m, if ℓ = m

Thus, B = µ · q11T + µ · diag(f1, f2, . . . , fℓ), where fℓ :=

( ) nℓ−2 d−2

(p

−

q).

As

nmax/nmin

=

O(1),

each

fℓ

converges

to

( ) n−2 d−2

a positive constant, implying that σmin(B) = Θ(µ).

By Lemma 2 and the above claim, c3k(1 + ǫ) σAm0in−(PP)22 ≤

c3k(1+ǫ)c24 n

2

2

holds w.p. 1−O(n−1) for nµ ≥ c5. Choosing

c6

nmin µ

c0 =

c5

,

c1 =

c3 k(1+ǫ)c24
2

completes the proof.

d(d−1)

c6

Remark 7. (Technical novelty relative to the graph case):

Indeed, proving the sharp concentration of a spectral norm has

been a key challenge in the spectral analysis [44], [63]. While

most bounds developed hinge upon the independence between

entries9, the matrix A in HSC has strong dependencies across

8Note that this result holds only for a ﬁxed k [48]. 9For instance, the most studied model, called the Wigner matrix, assumes
independence among entries. See [64] for more details.

entries due to its construction. For instance, the entries A12

and A13 both have a term We for any edge e of the form

{1, 2, 3, j4, j5, . . . , jd}, hence sharing

n−3 d−3

many terms.

One approach to handle this dependency is to use ma-

trix Bernstein inequality [65] on the decomposition A =

e∈E WeSe, where Se :=

i,j∈e

e

i

e

T j

.

See

[8],

[11].

How-

i=j

ever, √this approach provides a bound which comes with an

extra log n factor relative to the bound in Lemma 2, resulting

in a suboptimal consistency guarantee as described in Sec. I-A.

Another approach is a combinatorial method [63], which

counts the number of edges between subsets. The rationale

behind this method is as follows. From the deﬁnition of the spectral norm, one needs to bound the quantity xT (A0 − P)x
for any vector x. It turns out that this quantity has a close

connection to the number of (hyper)edges between two subsets
in a random (hyper)graph. For instance, 1TAA1B is precisely the number of edges between A and B.

Indeed, a technique for estimating the number of hyper-

edges between two arbitrary subsets is developed in [66]. Us-

ing this method, however, one may only obtain a suboptimal

guarantee, which is

n d

αn

=

Ω(n1.5).

On

the

other

hand,

we

show via our analysis that the order-optimal guarantee can

be obtained by improving the standard combinatorial method.

See Appendix A.

B. Proof of Theorem 2

We ﬁrst outline the proof. Using the union bound, we show that it is sufﬁcient to prove Pr(ΦHSCLR(i) = j) = o(n−1) for all 1 ≤ i ≤ n and j = Ψ(i). We then consider the following

events to bound this error probability. The ﬁrst event is that the

average edge weight of the edges between the true community

Ψ(i) and node i is less than a certain threshold, and the other

one is that the average edge weight of the edges between

the wrong community j and node i is greater than the certain

threshold. We will ﬁrst show that if the misclassiﬁcation event

occurs, at least one of these two events must occur. Thus, we

bound the error probability by bounding those of these two

events using Lemma 3 and Lemma 4, respectively.

We consider the boundary case

n d

αn

=

Θ(n log n).

As

β

n d

αn

= Θ(n log log n) = ω(n), Corollary 2 guarantees that

ΦHSC is weakly consistent. Without loss of generality, assume

that the identity permutation is equal to arg minΠ∈P |{i :

Ψ(i)

=

Π(ΦHSC(i))}|.

Then,

|Φ− HS1C (j )∩Ψ−1 (j )|
−1

>

1 − γ,

i.e.,

at

|ΦHSC (j )|

least 1 − γ fraction of the nodes that are classiﬁed as in com-

munity j are correctly classiﬁed. The second stage of HSCLR

reﬁnes the output of the ﬁrst stage ΦHSC, resulting in ΦHSCLR.

By the union bound, we have Pr(err(ΦHSCLR) = 0) ≤

n i=1

j=Ψ(i) Pr(ΦHSCLR(i) = j). Since the total number of

summands is Θ(n), if Pr(ΦHSCLR(i) = j) = o(n−1) for all

1 ≤ i ≤ n and j = Ψ(i), then Pr(err(ΦHSCLR) = 0) = o(1).

By Pr

the reﬁnement rule (1), Pr(ΦHSCLR(i) = j) ≤

< e∈E(i) (Ψ(i)) We
|E (i) (Ψ(i))|

e|∈EE((ii))((jj))|We . For any real numbers

(a, b, t), [a ≥ b] ⊃ [a ≥ t] ∩ [t ≥ b] holds. By taking complements of both sides, we have [a < b] ⊂ [a < t] ∪ [t < b].

Therefore, by the union bound, P (a < b) ≤ P (a < t)+P (t <

7

b) holds for any (a, b, t). Applying this bound, we have

e∈E(i)(Ψ(i)) We

e∈E(i)(j) We

Pr |E(i)(Ψ(i))| < |E(i)(j)|

(4)

e∈E(i)(Ψ(i)) We p + q

≤ Pr |E(i)(Ψ(i))| < 2 αn

(5)

R1

p+q

e∈E(i)(j) We

+ Pr 2 αn < |E(i)(j)|

,

(6)

R2

We ﬁrst interpret R1 and R2. For illustration, assume that ΦHSCLR coincides with Ψ. Under this assumption, observe that e|∈EE((ii))((ΨΨ((ii))))|We is equal to the average edge weight of the homogeneous edges within community Ψ(i). Since the expected value of this term is p2 αn, one can show that the term R1 vanishes. Similarly, e|∈EE((ii))((jj))|We is the average weight of the edges connecting i and the other nodes in community j. Since these edges are heterogeneous, R2 also vanishes.
Indeed, as err(ΦHSC) is not exactly zero, but an arbitrarily
small constant, the above interpretation is not precise. In what
follows, we show that R1 and R2 vanish as well for the case. We begin with bounding R1. Denote by Eh the set of all
homogeneous edges. Recall that edges in E(i)(Ψ(i)), except O(γ) fraction, are homogeneous, so |E(i)(Ψ(i)) ∩ Eh| = (1−O(γ))|E(i)(Ψ(i))|. By restricting the range of summation, R1 ≤ Pr e∈E(i)(Ψ(i))∩Eh We < p+2 q αn|E (i)(Ψ(i))| . Note that We’s are not restricted to Bernoulli random variables. By tweaking the proof of conventional large deviation results [67]
for Bernoulli variables, we obtain the following:

Lemma 3. Let S be the sum of m mutually independent

random variables taking values in [0, 1]. For any δ > 0,

we have Pr (S > (1 + δ)E[S]) ≤ exp − 2δ+2δ E[S] and

Pr (S < (1 − δ)E[S]) ≤ exp

−

δ2 2

E[

S

]

.

Proof: See Appendix D-A. As E[ e∈E(i)(Ψ(i))∩Eh We] = (1 − O(γ))pαn|E (i)(Ψ(i))|,

p+2 q αn|E(i)(Ψ(i))| − 1 = (1 + O(γ)) q − p ,

E[ e∈E(i)(Ψ(i))∩Eh We]

2p

so Lemma 3−2) with δ = (1 + O(γ)) p2−pq gives

R1 ≤ exp − (p − q)2 αn(1 + O(γ))|E(i)(Ψ(i))| . (7) 8p

Next we consider R2. Again, edges in E(i)(j), except O(γ) fraction, are heterogeneous, so |E(i)(j) ∩ Ehc| = (1 − O(γ))|E(i)(j)|. The following lemma says that the contribution due to the O(γ) fraction of edges is marginal:

Lemma 4. For sufﬁciently small γ > 0,





Pr 

We

>

pαn |E (i) (j )| 

=

o(n−1) .

(8)

e∈E (i) (j)∩Eh

log(1/γ)

Proof: See Appendix D-B. Hence, we focus on heterogeneous edges only. Making a similar argument as above, the bound in Lemma 3 becomes

 (p−q)2



R2 ≤ exp −

4q2 p−q

qαn(1

+

O(γ ))|E (i) (j )|

(9)

2 + 2q

(a)
≤ exp

− 1 (p − q)2 αn(1 + O(γ))|E(i)(j)|

,

(10)

8p

where (a) follows since 2+1p2−qq = 32 +12pq = ( 23 pq +1 21 ) pq ≥ 21pq .

Since (p−pq)2

n d

αn

≥ (8 + ǫ) (n/nmdin)d−1 n log n,

a

straight-

forward

calculation

yields

1 8

(p−q)2 p

αn(1

+

O(γ))

nmin −1 d−1

≥

1 + 116 ǫ log n, for sufﬁciently large n. Thus, R1 and R2 are

both o(n−1) from (7) and (10).

V. DISCUSSION

We have shown that our algorithms can achieve the orderoptimal sample complexity for all different recovery guarantees under a symmetric block model. In this section, we show that our main results indeed hold for a broader class of block models. We also show that HSCLR can achieve the sharp recovery threshold for a certain SBM model.

A. Extensions

For the graph case [13], a fairly general model, which subsumes as a special case the asymmetric SBM, has been investigated. Here we extend our model to one such model but in the context of hypergraphs. Speciﬁcally, we consider the following asymmetric weighted SBM.

Deﬁnition 4 (The asymmetric weighted SBM). Let {pe}e∈E be constants such that pe > pe′ holds for any homogeneous edge e and heterogeneous edge e′. A random weight is
assigned to each edge independently as follows: For each edge e ∈ E, E[We] = peαn. Notice that this reduces to the condition of p > q in the symmetric setting.

We ﬁnd that our main results stated in Thm. 1 and 2 readily carry over the above asymmetric setting. The key rationale behind this is that our spectral clustering guarantee hinges only upon the full-rank condition on B (see Sec. III-A for the deﬁnition). Here, what one can easily verify is that the condition above implies the full-rank condition, and hence our results hold even for the asymmetric setting. The only distinction here is that the constants that appear in the theorems depend now on pe’s. Similarly, our technique can cover disassortative SBM in which heterogeneous edges have larger weights than homogeneous edges.

Deﬁnition 5 (The symmetric disassortative weighted SBM). In Deﬁnition 1, we assume instead that 0 < p < q < 1.

Another prominent instance is the planted clique model.

Deﬁnition 6 (The planted clique model). Fix s-subset of

nodes C (s ≤ n). Consider a random hypergraph in which

every d-regular edge e = {i1, i2, . . . , id} appears with proba-

bility

1

if

e⊆C

or

1 2

otherwise.

8

In this model, one wishes to detect the hidden subset C, which is called the clique. Following a similar analysis with a dcaifnfebreendtenteoctitoend oiff ser≥rorc∗fr·a√ctnionfo, ronsoemcaencsohnoswtantthact∗,thwehcilcihquies consistent with the well-known result for d = 2 [68].
B. Sharpness
Recently, sharp thresholds on the fundamental limits are characterized in the graph case [22], [24], [27], [28], [30]. In contrast, such a tight result has been widely open in the hypergraph case. A notable exception is our companion paper [32] which studies a special case of the weighted SBM (considered herein), in which weights are binary-valued.

Deﬁnition 7 (Generalized Censored Block Model with Homogeneity Measurements [32]). Let θ ∈ (0, 1/2) be a ﬁxed constant. Assume that k = 2 and denote erasure by x. If the edge e is homogeneous, We = 1 w.p. αn(1 − θ), We = 0 w.p. αnθ, and We = x w.p. 1 − αn. Otherwise, We = 1 w.p. αnθ, We = 0 w.p. αn(1 − θ), and We = x w.p. 1 − αn.
The information-theoretic limit for strong consistency has been characterized under this model, formally stated below.

Proposition 3. (Thm. 1 in [32]) Under the model in Deﬁ-

nition 7, the maximum likelihood estimator is strongly con-

sistent for any given hidden partition Ψ if

n d

αn

≥

(1 +

ǫ

)

2d− d

2

√ n log √n ( 1−θ− θ)2

for

any constant ǫ

>

0.

Conversely, if

n d

αn

≤

(1 − ǫ) 2dd−2 (√1n−lθo−g √n θ)2 , no algorithm can be

strongly consistent for any given hidden partition Ψ.

Using our results, we can show that there is no computational barrier under this model. We now state the theorem, deferring the proof to Appendix B.
Theorem 3. HSCLR10 achieves the information-theoretic limits characterized in Proposition 3.

VI. APPLICATION
In this section, based on our algorithms, we design a sketching algorithm for subspace clustering.
A. Subspace clustering
It is well known that hypergraph clustering is closely related to computer vision applications such as subspace clustering [35]. In the subspace clustering problem, one is given with n data points x1, x2, . . . , xn ∈ Rℓ in a high dimensional ambient space. The n data points are partitioned into k groups, and data points in the same group approximately lie on the same subspace, each of dimension at most m < ℓ. The goal is to recover the hidden partition of the n data points based on certain measurements. Among various approaches, tensor-based algorithms measure similarities between the data points to recover the cluster [34]–[36]. More speciﬁcally, they construct a weighted d-uniform (d ≥ m + 2) hypergraph ([n], {We}e∈E ), in which each edge weight represents the similarity of the corresponding d points. One typical approach to measure the similarity between d data points is based on

10Indeed, there should be some minor tweaks to make HSCLR better adapted to this model. See Appendix B for details.

the hyperplane ﬁtting. More speciﬁcally, denoting by ﬁt(·) the error of ﬁtting an m-dimensional afﬁne subspace to d data points, one may set We = exp (−ﬁt(xi1 , . . . , xid )) for e = {i1, i2, · · · , id}. Note that We ≃ 1 if the d data points are approximately on the same subspace, and We ≃ 0 if the data points cannot be ﬁt on a single subspace.
Consider a set of d data points of the same cluster, which approximately lie on the same subspace by deﬁnition. The edge weight corresponding to these d data points will be approximately 1, and one may model the edge weight as a random value whose expected value is close to 1. Similarly, one may model the edge weights of heterogeneous edges by a random variable whose expected value is close to 0.11
Clearly, our weighted SBM can precisely capture the above
hypergraph model since our model only assumes that the
average weights of homogeneous edges are larger than those
of heterogeneous edges. We verify this claim using a real
data set. Hopkins 155 is the most widely used dataset for the subspace clustering problem [69]. We ﬁrst set d = 8 and m = 3, and then randomly sample 10000 homogeneous edges and 10000 heterogeneous edges. The empirical distributions of edge weights are shown in Fig. 1a. We can see that the
homogeneous edges have larger weights on average than the
heterogeneous edges, well respecting the weighted SBM.

B. Sketching algorithms for subspace clustering

Modern subspace clustering algorithms involve a large number of data points lying on a high-dimensional space, i.e., n and ℓ are very large. Hence, storing the entire raw data points is prohibitive, and one may have to resort to the sketch of the data set. A sketch can be viewed as a summary of the dataset, containing sufﬁcient information of the data set.
As evidenced by the preceding section, we assume that the weighted hypergraph constructed from the data points follows the model in Sec. II. Under this assumption, subspace clustering can be done by clustering nodes of the weighted hypergraph. The following corollary asserts that one can exactly solve the subspace clustering problem with a sketch consisting of the weights of randomly chosen hyperedges.12 We now state a corollary, a consequence of Thm. 1 and 2.

Corollary 3. Suppose that nmax/nmin = O(1) and αn = 1.

Then, HSC is weakly consistent if

n d

sn

=

ω(n),

and

HSCLR

is strongly consistent if

n d

sn

≥

c8 · n log n

for

some

constant

c8 > 0. Moreover, the computational complexities of HSC,

HSCLR

reduce

to

max{

n d

sn, n(log n)k}.

Remark 8. One can sketch data more aggressively if the subspaces are not similar to each other [70]. This is also captured in Corollary 3 as follows. As a concrete example, consider two subspaces of dimension m and a heterogeneous edge e. When the two subspaces are moving farther away

11Indeed, the edge weight of a heterogeneous edge can be very close to 1, i.e., the ﬁtting error can be close to 0. This may happen when d data points, which are from different subspaces, are well aligned with another single subspace. Such a coincidence, however, happens with very low probability, and hence we simply treat these atypical events as statistical noise.
12We note that one may carefully choose similarity entries in order to achieve a more informative sketch than our random one, at the cost of increased computational complexity for sketch construction.

9

Noise level Average run time (sec)

SSC

SSC-OMP

LRR

TSC

NSN+Sspec 1

0.15

0.15

0.15

0.15

0.15

0.9

0.1

0.1

0.1

0.1

0.1

0.8

0.05

0.05

0.05

0.05

0.05

0.7

0

0

0

0

0

300 400 500 600

300 400 500 600

300 400 500 600

300 400 500 600

0.6 300 400 500 600

TTM

SCC

SGC

Tetris

0.5
Proposed

0.4

0.15

0.15

0.15

0.15

0.15

0.1

0.1

0.1

0.1

0.1

0.3

0.05

0.05

0.05

0.05

0.05

0.2

0

0

0

0

0

0.1

Number of points in each subspace 300 400 500 600

300 400 500 600

300 400 500 600

300 400 500 600

300 400 500 600 0

100 80 60 40

SSC SSC-OMP LRR TSC NSN+Spec TTM Proposed

20

0

0

2000

4000

6000

Number of data points (n)

(a)

(b)

(c)

Fig. 1: (a) Distribution of edge weights for the Hopkins 155 data set. Notice that homogeneous edges have larger weights on average. This implies that the hypergraph constructed from tensor-based approaches respects our model. (b) Fractional error of various algorithms. We report the fractional error of each algorithm for varying n/k and σ (a lighter color implies a lower error fraction). Note that we include iterative algorithms (SCC, SGC, Tetris) although they cannot be utilized in the sketching scenario. We can see that our approach has a comparable performance to the state of the arts. (c) Average run time comparison with prior subspace clustering algorithms. We can observe that our proposed algorithm scales nearly linearly in n while others do not.

from each other, the ﬁtting error of e increases. Thus, We approaches 0, and hence p − q increases. Since the sample complexity is inversely proportional to p − q,13 one can sketch
more aggressively.

Corollary 3 implies that our sketching method can reduce

the storage overhead from O(nℓ) to O(n log n). We now

evaluate our sketching algorithm. The relevant parameters are

n, k, ℓ, m, d, and sn: in an ambient dimension of ℓ = 50, we randomly generate k subspaces each being of dimension of

m = 3; for each subspace, we randomly sample n/k points and perturb every point with Gaussian noise of variance σ2;

we set edge size d = 5 and sampling probability sn. We ﬁrst implement HSCLR in MATLAB14. We then compare HSCLR
with other prior algorithm15, adopting the experimental setups

from [75] and [8].

We ﬁrst measures the performance of various algorithms.

We set k = 3 and

n d

sn

=

5kd−1n log n/d,

and

report

the average fractional errors of each algorithm over 20 trials

for (n/k, σ) ∈ {300, 400, 500, 600} × {0, 0.05, 0.1, 0.15}

in Fig. 1b. Observe that our algorithm matches the state-

of-the-art performance. We also measures the run time

of the algorithms. We set k = 2, σ = 0.025, n ∈

{750, 1500, 3000, 6000},

n d

sn = 5kd−1n log n/d, and report

the average run time over 10 trials. Fig. 1c shows that the

runtime of our proposed algorithm scales nearly linearly in n.

C. Other applications

Apart from subspace clustering, there are many applications in which d-wise similarities can carry more information than pairwise ones. Those include other computer vision applications (such as geometric grouping [34], [36] and highorder matching [77]), tagged social networks [6], biological

networks [7] and co-authorship networks [78]. We remark that while our model assumes equal-sized hyperedges, the HSCLR algorithm is applicable even when the size of hyperedges vary, which is the case for some of these applications. However, the success of the reﬁnement step is contingent upon whether or not the average weight of homogeneous edges is larger than that of heterogeneous edges. While this assumption is shown to hold for the subspace clustering problem, whether or not this assumption holds for the other applications is an interesting future direction.
VII. CONCLUSION
In this paper, we develop two hypergraph clustering algorithms: HSC and HSCLR. Our main contribution lies in performance analysis of them under a new hypergraph model, which we call the weighted SBM. Our results improve upon the state of the arts, and ﬁrstly settle the order-optimal results. Further, we show that HSCLR achieves the informationtheoretic limits of a certain hypergraph model. We also develop a sketching algorithm for subspace clustering based on HSCLR, and empirically show that the new algorithm outperforms the existing ones.
We conclude our paper with future research directions.
• Detection threshold: In [31], a sharp threshold for detection is conjectured. Further, the non-backtracking method is conjectured to be optimal. Proving these conjectures still remains open. The optimality of HSC is also open.
• Consistency threshold: The fundamental limits for weak/strong consistency under the general weighted SBM are unknown. An important open problem is to characterize the general limits in terms of the model parameters (n, d, k, p, q, αn).

13The sufﬁcient condition in Thm. 2 reads (p−q)2 n sn ≥ C for some
pd
quantity C. 14We observe a large constant in the computational complexity of the
geometric k-clustering, and hence we implement HSCLR with an efﬁcient k-means algorithms for the experiments.
15Sparse subspace clustering (SSC) [71], a variant of SSC using
OMP (SSC-OMP) [72], subspace clustering using low-rank representation
(LRR) [73], thresholding-based subspace clustering (TSC) [74], subspace
clustering using nearest neighborhood search (NSN+Spec) [75], and tensor trace maximization (TTM) [8]. Note that SCC [36], SGC [76], and Tetris [8])
are not applicable to the sketching scenario due to their iterative natures.

APPENDIX A

We ﬁrst note that the overall structure of the proof resem-

bles the ones in [44], [63], except that the entries of A are

not independent. This is because each hyperedge’s weight is added to more than one entries of A0 in our case, resulting

in dependency structure between all elements of the matrix.

See Remark 7 for more details.

We begin with some preliminaries: Let ν :=

n−2 d−2

pαn

≥

maxi,j E[Ai,j ]; let B := {x ∈ Rn : x 2 ≤ 1}; let Dδ :=

10

√

x = (x1, x2, . . . , xn) ∈ B :

nxi δ

∈Z

;

for

a

matrix

C,

denC(A, B) := i∈A j∈B Ci,j; and for a matrix C and a subset I, let CI be the matrix obtained from C by zeroing

out all rows and columns in subset I. The following large

deviation results will be frequently used throughout the proof:

Lemma 5. Let S =

n i=1

Xi,

where

0

≤

Xi

≤

b

for

each

i

for b > 0. There exist constants c7 > 0 depending only on b

such that the following holds for any a ≥ E[S] and k ≥ c7:

Pr(S > k · a) ≤ exp − 1 k log k · a . 2b

Proof: See Appendix D-A.

We consider the most challenging case where

n d

αn

=

Θ(n), i.e., ν = Θ(1/n). First, note that P − E[A] is a

dia√gonal matrix whose entries are O(ν). Hence, P−E[A] = O( nν). Thus, it sufﬁces to show that

A0 − E[A]

√ = O( nν) .

(11)

Lemma 6. Let C be a n × n matrix. For 0 < δ < 1,

The following lemma bounds the number of removed rows (and columns).
Lemma 7. For some cthr > 0 (depending only on d), there exists a constant c8 > 0 such that if nν ≥ c8, then w.p. 1 − exp(−Ω(n)), |{i : denA(i, [n]) ≥ cthr · nν}| ≤ (nν)−3n.
Proof: See Appendix D-D. By Lemma 7, for nν ≥ c8, Pr |J | ≥ (nν)−3n ≤ e−Ω(n). As there are at most 2n = en log 2 many subsets of [n], due to the union bound, the proof for (T 1) will be completed after showing that for a ﬁxed |I| ≤ (nν)−3n, Pr(EI ) ≤ O e−2 log 2n . Observe that

AIi,j xixj − xT E[A]x
(i,j)∈Sδ (x)

≤ xT

E[AI ] − E[A]
(E1)

x+

E[AIi,j ]xixj
(i,j)∈Sδ (x)c
(E2)

C ≤ (1 − 3δ)−1 max xT Cx .
x∈Dδ

Proof: See Appendix D-C.

Due to Lemma 6, one can replace (11) with a

more tractable statement at the cos√t of the constant:

supx∈Dδ xT A0 − E[A] x = O( nν). For a vec-

tor x = (x1, x2, . . . , xn) ∈ Dδ, deﬁne Sδ(x) :=

(i, j) : |xixj| < δ2

ν n

for 0 < δ < 1. Then, one has:

sup xT A0 − E[A] x = sup

A0i,j xixj − xT E[A]x

x∈B

x∈B (i,j)

Let (T 1) = supx∈B and (T 2) = supx∈B

(i,j)∈Sδ (x) A0i,j xixj − xT E[A]x (i,j)∈Sδ(x)c A0i,j xixj . Then, the

+

AI
i,j

−

E[AIi,j ]

xixj

,

(i,j)∈Sδ (x)

(E3)

and hence we will show t√hat there exist c13, c14, c15√> 0 such that supx∈Dδ (E1) ≤√c13 nν, supx∈Dδ (E2) ≤ c14 nν, and supx∈Dδ (E3) ≤ c15 nν with probability 1 − O(e−2n log 2), respectively. Having shown these, the proof for (T 1) is completed by taking c12 := c13 + c14 + c15. (i) (E1): As |I| ≤ (nν)−3n,

xT E[AI ] − E[A] x ≤ E[AI ] − E[A]

≤ E[AI ] − E[A] ≤ 2(nν)−3n2 · ν2 = √2(nν)−1/2.

F√

√

Hence, by taking c13 = 2, supx∈Dδ (E1) ≤ c13 nν

holds with probability 1 for nν ≥ 1.

above quantity is bounded above by (√T1) + (T2). We now show that each of (T1) and (T2) is O( nν).
A. Proof of (T 1)
We denote by J the random subset of [n] that corresponds to the removed rows and columns during the processing step (see step 2 of Alg. 1). For a sufﬁciently large constant c12 > 0 (to be chosen later) and I ⊂ [n], deﬁne the event

(ii) (E2): As ν ≥ maxi,j E [Ai,j],

E[AIi,j ]xixj ≤ ν

|xixj |

(i,j)∈Sδc

(i,j)∈Sδ (x)c i=j

=ν

x2i x2j (a) 1 √ ≤ nν

(b)
x2x2 ≤

1 √nν ,

(i,j)∈Sδ (x)c |xixj | δ2

i j δ2
(i,j)∈Sδ (x)c

i=j

i=j

EI =

sup
x∈Dδ

(i,j)∈Sδ (x)

AIi,j xixj −xT E[A]x

√ > c12· nν

Then, it is sufﬁcient to show that Pr(EJ ) → 0. Note that the following upper bound holds for:

Pr(EJ ) =

[Pr(EJ , J = I)]

. (iii)

where (a) is due to the deﬁnition of Sδ(x), and (b)

follows since

x

≤√ 1. Hence,

by

taking c14

=

1 δ2

,

supx∈Dδ (E2) ≤ c14 nν holds with probability 1.

(E3): Let x = (x1, x2, . . . , xn) ∈ Dδ be ﬁxed. We have

(i,j)∈Sδ (x)

AI
i,j

−

E[AIi,j ]

xixj

I ⊂[n]

≤

[Pr(EI , J = I)] +

[Pr(EI , J = I)]

|I |≤(nν )−3 n

|I |≥(nν )−3 n

=

xixjI{i ∈/ I, j ∈/ I}

[We − E[We]]

(i,j)∈Sδ (x) i=j

e∈E {i,j}⊂e

≤

[Pr(EI )] +

Pr(J = I)

|I |≤(nν )−3 n

|I |≥(nν )−3 n

=

[Pr(EI )] + Pr |J | ≥ (nν)−3n .

|I |≤(nν )−3 n

=

(We − E[We])

[xixj I{i ∈/ I, j ∈/ I}] .

e∈E

(i,j)∈Sδ (x)

i=j, {i,j}⊂e

=:Ye

11

Note that {Ye}e∈E is a collection of independent random variables. To apply Bernstein inequality to e∈E Ye, we do some preliminary calculations. First, it easily follows
from the deﬁnition of Sδ that

|Ye| ≤ (We − E[We])

[xixj I{i ∈/ I, j ∈/ I}]

(i,j)∈Sδ (x) i=j, {i,j}⊂e

≤

|xixj | ≤ δ2 ν · 2 d ≤ d2δ2 ν .

(i,j)∈Sδ (x)

n2

n

i=j, {i,j}⊂e

Next, we compute a bound on the sum of variances:

E[Ye2]
e∈E

(a)
≤
e∈E

d2E[We2]

x2i x2j I{i ∈/ I, j ∈/ I}

(i,j)∈Sδ (x) i=j, {i,j}⊂e

(b)
≤ d2
e∈E

E[We]
(i,j) i=j,{i,j}⊂e

x2i x2j

= d2
(i,j) i=j

x2i x2j E[Ai,j ]

≤ d2ν

(c)
x2i x2j ≤ d2ν ,

(i,j) i=j

where (a) is due to (

k i=1

ai)2

≤

k

k i=1

a2i ;

(b)

follows

since We ∈ [0, 1]; (c) follows since x ≤ 1.

Thus, Bernstein inequality yields: Pr

t ≤ 2 exp −

t2/2 √ , we have

d

2

ν

+

1 3

d

2

δ

2

ν n

t

e∈E Ye ≥

√

Pr

Ye ≥ c15 nν

e∈E

≤ 2 exp −

c215

n.

2d2 + 23 d2δ2c15

As |Dδ| = eΘ(n), the union bound yields

√

Pr sup

Ye ≥ c15 nν

x∈Dδ e∈E

≤ eΘ(n) Pr

√ Ye ≥ c15 nν

e∈E

≤ 2eΘ(n) exp

−

c215

n,

2d2 + 32 d2δ2c15

and hence by choosing c15 su√fﬁciently large, one can ensure that supx∈Dδ (E3) ≤ c15 nν w.p. 1−O(e−2n log 2).
Since nν ≥ c8 and nν ≥ 1, nν should be greater than or equal to max{c8, 1}, so one can take c5 = max{c8, 1}.

B. Proof of (T 2)

This case immediately follows from a celebrated combinatorial technique proposed in [63]. We summarize their results.

Deﬁnition 8. We say the bounded density property holds with constants α, β, γ > 0 if the following two hold:
1) For each node u, denA0 (u, [n]) ≤ α · nν.

2) For any two subsets A, B, either denA0(A, B) ≤ β · ν|A||B| or denA0 (A, B) log denνA|A0 (||AB,|B) ≤ γ · max{|A|, |B|} log max{|nA|,|B|} .
Proposition 4 ([44], [63]). If the bounded densit√y property holds with some constant α, β, γ, then (T 2) = O( nν).

Therefore, one only needs to show that the bounded density property holds with high probability to ﬁnish the proof.

Lemma 8. With probability 1 − O(n−1), the bounded density property holds with some constants c9, c10, c11.

Proof: See Appendix D-E.

APPENDIX B

For notational simplicity, as k = 2, we represent partition functions ΦHSC, Ψ by binary vectors X, Z ∈ {0, 1}n. We

deﬁne some notations: Let W = [We]e∈E ; for a vector

V = [Vi]1≤i≤n ∈ {0, 1}n and e = {i1, i2, . . . , id} ∈

[n] d

,

let fe(V) = I{Vi1 = Vi2 = . . . Vid }; let F(V) = [fe(V)]e∈E .

A straightforward calculation yields for any two binary

vectors X and Y, the likelihood of X is greater than that

of Y if and only if d(W, F(X)) < d(W, F(Y)), where

d(X, Y) := |{i ∈ [n] : Xi = Yi}| for any X and Y.

To make HSCLR better adapted to the model, we modify

the algorithm as follows:

1) We apply HSC to ([n], W′), where W′ is obtained from

W by replacing the erasure weights x’s with 0’s.

2) We then employ a likelihood-based reﬁnement rule:

Xi ← Xi

if d(W, F(X)) < d(W, F(X ⊕ ei));

Xi ⊕ 1 otherwise.

Remark 9. Notice that one can employ such a likelihood-based estimator only when edge distributions are fully speciﬁed.

We now begin the main proof. We consider the most

challenging regime where

n d

p

=

Θ(n log n),

and

suppose

n

2d−2 αn ≥ (1 + ǫ)

√

n log n√

(12)

d

d ( 1 − θ − θ)2

for a ﬁxed ǫ > 0. For simplicity, we assume that n is even, and ﬁx the ground truth to be A = (1, . . . , 1, 0, . . . , 0); for

n/2

n/2

other cases, the proof follows similarly.

Let X be the output of the ﬁrst stage. By Thm. 1, one can

see that X is weakly consistent. Without loss of generality,

we assume for an arbitrarily small η > 0 that

X = (0, . . . , 0, 1, 1, 1, . . . , 1, 1, 1, 1, . . . , 1, 0, 0, 0, . . . , 0, 0, 0) .

ηn

n/2−ηn

ηn

n/2−ηn

Indeed, X needs not have the same number of 0’s and 1’s but the other cases can be handled similarly using the same arguments.
As in the proof of Thm. 2 (see Sec. IV-B), due to the union bound, it is enough to show that the probability of having node 1’s afﬁliation incorrect after reﬁnement is o(n−1), i.e.,

Pr (node 1 is incorrect after reﬁnement) = o(n−1) .

12

By the new reﬁnement rule, Pr (node 1 is incorrect after reﬁnement) = Pr 0 < d(W, F(X ⊕ e1)) − d(W, F(X)) .

The following lemma states that the difference of hamming distances can be viewed as the sum of random variables.

Lemma

9.

Pi, Pi′

i.∼i.d.

Bern(αn)

and

Θ

i

,

Θ

′ i

i.∼i.d.

Bern(θ).

Then,

d(W, F(X ⊕ e1)) − d(W, F(X))

( ) 2

n/2−ηn d−1

( ) ( ) 2

n/2 d−1

−2

n/2−ηn d−1

=

Pi(2Θi − 1)) +

Pi′ (1 − 2Θ′i) .

i=1

i=1

Proof: See Appendix D-F.

Let V1 =

n/2 d−1

and V2 =

n/2−ηn d−1

.

By

Lemma

9,

Pr 0 < d(W, F(X ⊕ e1)) − d(W, F(X))

2V1 −2V2

2V2

= Pr −

Pi′ (1 − 2Θ′i) < Pi(2Θi − 1))

i=1

i=1

(a)
≤ Pr

2V1 −2V2

2V2

−

Pi′ < Pi(2Θi − 1))

i=1

i=1





O(η)V1

2V2

(=b) Pr −

Pi′ < Pi(2Θi − 1)) .

i=1

i=1

(13)

where (a) is due to |1 − 2Θ′i| ≤ 1; (b) is due to 2V1 − 2V2 = O(η)V1 .
In view of Lemma 4, one can similarly show that





O(η)V1 ′

V1αn

−1

Pr 

Pi >

 = o(n ) , (14)

i=1

log(1/η)

provided that η is sufﬁciently small. Thus,

(13) ≤ Pr − lVo1gα(1n/η) < 2V2 Pi(2Θi − 1)) + o(n−1)
i=1

Lemma 10. For an integer K > 0, let {Pi}Ki=1 i.∼i.d. Bern(αn) and {Θi}Ki=1 i.∼i.d. Bern(θ). Then, for any ℓ > 0

1−θ K

Pr log θ

Pi(2Θi − 1) ≥ −ℓ

i=1

≤

e

1 2

ℓ

−

K

(αn

(

√ 1−

θ

−

√ θ

)

2

+

O

(

α2n

)

)

.

Proof. See Appendix D-G.

By Lemma 10,

Pr −

V1αn

2V2
< Pi(2Θi − 1))

log(1/η) i=1

≤ e ( ) ( ) . 21 √loglog1(−1θ/θη) V1αn−2V2 αn(√1−θ−√θ)2+O(α2n )

(15)

Note that as αn = o(1),

1 log

1−θ θ

V α − 2V

α (√1 − θ − √θ)2 + O(α2 )

2 log(1/η) 1 n

2n

n

= (1 + o(1))

log

1−θ θ

d

log(1/η)2d n

n αn − 2 d

1 − η d−1 2

·d n nd

√

√

αn( 1 − θ − θ)2 + O(α2n)

= (1 + o(1))

log

1−θ θ

d

log(1/η)2d n

n αn − 2 d

1 − η d−1 2

· nd nd αn(√1 − θ − √θ)2 → − 2d1−2 nd nd αn(√1 − θ − √θ)2

as η → 0+ and n → ∞. Thus, (15) ≤ e−(1+ǫ/2) log n = o(n−1) for sufﬁciently large n and small η.

APPENDIX C

To extend the analysis to the planted clique model, we need another type of error fraction, which is deﬁned as follows:

err′(Φ) := min max 1 |{i ∈ Ψ−1(j) : Π(Φ(i)) = j}| . Π∈P 1≤j≤k nj

Note that err′ characterizes the maximum value of withincluster error fraction over all clusters. Let us denote the smallest singular value of B (deﬁned in Sec. III-A) by σ and the size of smallest cluster by nmin. Then, following [5], one can prove the following result by tweaking the proof of Thm. 1:

Theorem 4. For some c14, c15, the following holds: if

n αn

≥

c14n

and

c15(1

+

ǫ)

( ) kn

n−2 d−2

2

< 1, then w.p.

d

αn nmin σ2

exceeding 1 − O(n−1),

err′(Φ ) ≤ c (1 + ǫ) kn nd−−22 .

(16)

HSC

15

αnn2minσ2

of Wpleanntoedw cdleiqmuoenwsthraetne sho≥wcT∗ h· m√.n4fgour asroamnteeecsonthsetadnet tce∗c.tiToon apply Thm. 4, we need to ﬁrst compute σ of

B=

1 2

s−2 d−2

+

1 2

n−2 d−2

1 n−2 2 d−2

.

1 n−2

1 n−2

2 d−2

2 d−2

Using the fact that the minimum singular value of

a+b a aa

is √ 2b

, we have

4+

(

b a

)

2

+

2+

b a

σ=

4+

2

s−2 d−2

= Θ sd−2 .

(s−2) 2

(s−2)

d−2
( ) n−2 d−2

+ 2 + (ndd− − −222)

Hence, by Thm. 4 (as αn = 1), Thus, whenever s = Ω(√n),

err′(ΦHSC) ≤ c15(1 + ǫ) 2ns2ndσ−−222 = O

nd−1 s2(d−1))

= O(1) .

13

APPENDIX D
A. Proofs of Lemma 3 and Lemma 5
Without loss of generality, we will prove the lemmas assuming that E[Xi] > 0 for all i. We ﬁrst obtain a useful bound on the moment generating function (mgf) of S. For an arbitrary λ > 0,

n

E[exp{λS}] = E exp λ

Xi

i=1

n
≤
i=1

(eλb − 1) 1 + b E[Xi]

≤ 1 + eλb − 1

n i=1

E[Xi

]

n
,

(17)

b

n

where

the

ﬁrst

inequality

holds

since

eλx −1 x

≤

eλb −1 b

holds

for all 0 < x ≤ b, and the second inequality holds due to

the AM-GM inequality. We now prove the lemmas using this

bound.

1) Proof of Lemma 5: Using Markov’s inequality and (17),

Pr(S > x) = Pr(eλS > eλx) ≤ exp{−λx}E[exp{λS}]

eλb − 1 E[S] n

≤ exp{−λx} 1 + b

n.

By taking λ = log (1 + δ), we obtain

Pr (S > (1 + δ)E[S]) ≤ e−(1+δ) log(1+δ)E[S]

≤

e−(1+δ) log(1+δ)E[S]+δE[S]

≤

e−

δ2 2+δ

E[S

]

,

δE[S] n 1+ n
(18)

where the last equality holds since log(1 + δ) ≥ 1+δδ/2 . This completes the proof of the upper bound.

B. Proof of Lemma 4

Assume that |E(i)(j)∩Eh| = c·γ|E(i)(j)| for some constant

c > 0. For simplicity, let us write

c·γ |E (i) (j)| i=1

Wi.

Then

we

get:

e∈E(i)(j)∩Eh We as

Pr We > pαn|E(i)(j)|

e∈E (i) (j)∩Eh

log(1/γ)

= Pr

c·γ |E (i) (j)|

Wi >

i=1

cγ

1

· cγpαn|E(i)(j)| .

log(1/γ)

(19)

From the proof of Lemma 3 (see (18)), one can deduce the following:

Corollary 4. Let S be the sum of m mutually independent random variables taking values in [0, 1]. For any δ > 0, we have

By choosing λ = 1b log 1 + Eb[xS] , i.e., x = (ebλb−1) E[S], we have

Pr(S > x) ≤ exp − x log 1 + bx

b

E[S]

≤ exp − x log 1 + bx

b

E[S]

exp(x)

= exp − x · log 1 + bx − b .

b

E[S]

1+ x n n

By setting x = ka, we have

Pr (S > (1 + δ)E[S]) ≤ e−{(1+δ) log(1+δ)−δ}E[S] . (20)

We will apply Corollary 4 with (1 + δ) = (cγ log(1/γ))−1. As (cγ log(1/γ))−1 → ∞ as γ → 0+,
we may regard δ to be an arbitrarily large constant. Because (1 + δ) log(1 + δ) − δ = (1 + o(1))(1 + δ) log(1 + δ) as
δ → ∞, in what follows, we will replace the upper bound (20) with e−(1+δ) log(1+δ)E[S]:

− √1

cγ pαn |E (i) (j)|

(19) ≤

1

cγ log(1/γ)

ecγ log(1/γ)

Pr(S > k · a) = exp − ka · log 1 + bka − b

b

E[S]

≤ exp −k log (1 + bk) − b · a , b

where the inequality holds since a ≥ E[S]. Since [log(1 +

bk) − b]

∼

log(k),

log(1 + bk) − b

≥

1 2

log(k)

holds

for

all

k ≥ c7, where c7 is some positive constant depending only

− √1

cγ pαn |E (i) (j)|

≤ 1 . cγ log(1/γ)

ecγ log(1/γ)

(21)

Since we consider the regime

n d

αn

=

Θ(n log n),

pαn|E(i)(j)| = c′ · log n for some constant c′ > 0. Hence,

the last term is equal to

1

− √ 1 c′·log n
log(1/γ )

on b. Applying this inequality to the above bound completes the proof.
2) Proof of Lemma 3: Since the proof bears great similarity to the conventional case [67], we only show the upper bound.

ecγ log(1/γ) = exp − log n ·

log(1/γ)

−

1

+

log

c

+

1 2

log(log(1/γ))

.

log(1/γ)

Using Markov’s inequality and (17) with b = 1,

Since the exponent diverges as γ → 0+, we prove the lemma.

Pr (S > (1 + δ)E[S]) = Pr eλS > eλ(1+δ)E[S] ≤ e−λ(1+δ)E[S] 1 + (eλ − 1) E[S] n
n

C. Proof of Lemma 6
WLOG, assume that C = supx∈B xT Cx; the case C = − infx∈B xT Cx follows similarly. Observe that the diameter of each cell resulting from discretization is δ. For a

14

vector d such that d 2 ≤ δ and x ∈ B, (x + d)T C(x + d) − xT Cx = 2dT Cx + dT Cd. Thus, we get:

(x + d)T C(x + d) − xT Cx ≤ (x + d)T C(x + d) − xT Cx ≤ 2 dT Cx + ≤2 d C x + C d 2≤2 d C + C ≤ 3 d C ≤ 3δ C .

dT Cd d2

Let x∗ = arg supx∈B xT Cx. Then, there exists x0 ∈ Dδ such that x0 − x∗ ≤ δ, so
C = (x∗)T Cx∗ ≤ (x0)T Cx0 + 3δ C ≤ sup |xT Cx| + 3δ C .
x∈Dδ
By rearrangement, we get: (1 − 3δ) C ≤ supx∈Dδ |xT Cx|. D. Proof of Lemma 7

Let us say node i is bad if denA(i, [n]) ≥ cthr · nν for some constant cthr to be chosen later. Let δ := (nν)−3.

Pr(there are more than δn bad nodes)

≤

Pr (every node in X is bad)

X ⊂[n]:|X |=δn

≤

Pr (denA(X, [n]) ≥ δn · (cthr · nν)) .

X ⊂[n]:|X |=δn

Note that for any subsets A and B,

Since

(23)

·

(nν)2

→

−

1 2d2

cthr

log cthr

<

0

as

nν

→

∞,

there

exists a constant c8 such that nν ≥ c8 implies (23) < 0. This

completes the proof.

E. Proof of Lemma 8

By taking c9 = cthr, the ﬁrst part of Deﬁnition 8 follows easily by the deﬁnition of A0.
We now turn to the second part of Deﬁnition 8. Without loss of generality, we assume that A ∩ B = ∅ and |A| ≤ |B|.
1) The case where |B| ≥ ne : It follows that ν|A||B| ≥ ν|Ae |n , and since we veriﬁed the ﬁrst part of Deﬁnition 8, we obtain denA0 (A, B) ≤ |A| · c9nν. Hence, denA0 (A, B) ≤ c9eν|A||B|.
2) The case where |B| < ne : It sufﬁces to show the property for the case where A0 is replaced by A due to the fact that denA0 (A, B) ≤ denA(A, B). Because of (22), denA(A, B) is a sum of independent random variables taking values in [0, d2]. As E[denA(A, B)] ≤ ν|A||B| (∵ ν ≥ maxi,j E [Ai,j ]), Lemma 5 ensures that there exist constants c7 > 0 such that

Pr denA(A, B) > k·ν|A||B| ≤ exp − 1 k log k · ν|A||B| 2d2

for any k ≥ c7 regardless of choices of A and B. Claim 2. Let

denA(A, B) =

Ai,j =

We

i∈A j∈B

i∈A j∈B e∈E {i,j}⊂e

=

We

1,

e∈E

(i,j)∈A×B:{i,j}⊂e

(22)

14d2 n ka,b := max min k ≥ 1 : k log k ≥ νa log b , c7 .
Then, with probability 1 − O(n−1), the following holds: For any two subsets A and B,

i.e., denA(A, B) is a sum of independent random variables taking values in [0, d2]. Hence, using the fact that E[denA(X, [n])] ≤ δn2ν (∵ ν ≥ maxi,j E [Ai,j ]) together with Lemma 5 (take b = d2), there exists c7 > 0 such that
Pr denA(X, [n]) ≥ kδn2ν ≤ exp − 1 k log k · δn2ν 2d2
whenever k ≥ c7.

denA(A, B) ≤ k|A|,|B|ν|A||B| .

Proof: It is sufﬁcient to prove the following:

Pr A,B denA(A, B) > k|A|,|B|ν|A||B| = O(n−1).

Note that in the case of |A| = a and |B| = b,

Pr(denA(|A|, |B|) > kA,B · ν|A||B|) is upper bounded

by

e

x

p(−

1 2d

2

ka,b

log

ka,b

·

νab)

as

ka,b

≥

c7.

Hence,

the

union bound yields:

By taking cthr = c7,

Pr denA(X, [n]) ≥ cthrδn2ν

X ⊂[n]:|X |=δn

≤ n exp − 1 cthr log cthr · δn2ν

δn

2d2

(a)
≤ exp

δ log 1 + δ − 1 cthr log cthr · δnν · n .

δ

2d2

where (a) is due to the fact that mn ≤ nme m. Plugging back in δ = (nν)−3, we obtain

δ log 1 + δ − 1 cthr log cthr · δnν

δ

2d2

= −3 log(nν)(nν)−3 + (nν)−3 −

1 2d2 cthr log cthr

· (nν)−2 . (23)





Pr  [denA(A, B) > kA,B · ν|A||B|]
A,B
≤ na nb exp − 21d2 ka,b log ka,b · νab .
a,b

Since there are at most n2 choices for (a, b), it is enough

to show that

n a

n b

exp

−

1 2d2

ka,b

log

ka,b

·

νab

≤

1 n3

for any (a, b).

By the deﬁnition of “ka,b”, we have ka,b log ka,b ≥ 1ν4da2 log nb . Hence,

21d2 ka,b log ka,b · νab ≥ 7b log nb (≥a) a + b + 5b log nb
(b)
≥ a + b + a log na + b log nb + 3 log n ,

15

where (a) follows since a ≤ b ≤ ne ; (b) follows since x log x is increasing on [1, ne ]. Thus, we have

exp − 21d2 ka,b log ka,b · νab

≤ exp −a log n + 1 − b log n + 1 − 3 log n .

a

b

Further, since mn ≤ nme m = exp (m(log n/m + 1)),

n n = exp (a (log n/a + 1) + b (log n/b + 1)) . ab

Thus, na nb e− 2d12 ka,b log ka,b·νab ≤ e−3 log n. By the above claim, we have that for any A and B such that |A| ≤ |B| ≤ ne , either of the following holds:

(i) denA(A, B) ≤ c7ν|A||B| or; 14d2 n
(ii) k|A|,|B| log k|A|,|B| = νa log b .

For (ii), one can derive: denA(A, B) ≤ k|A|,|B| · ν|A||B| = ν|A| lo1g4dk2|A|,|B| log |Bn| · ν|A||B| ≤

log 14d2

n

ν|A| log denA(A,B)

|B|

ν |A||B|

· ν|A||B|, and

denA(A, B) log

denA(A,B) ν|A||B|

≤

14d2|B| log

|Bn| .

hence

Combining the above two cases 1) and 2), the proof is completed by taking c10 = max{c9e, c7} and c11 = 14d2.

F. Proof of Lemma 9

One can easily show that the LHS is equal to

e∈E:We=x [I {fe(X ⊕ e1) = We} − I {fe(X) = We}]. Since the summand is nonzero only if fe(X ⊕ e1) = fe(X),

we count the number of such edges.

First, observe that if 1 ∈/ e, fe(X ⊕ e1) = fe(X). Further, if two (or more) nodes other than node 1 are of different

afﬁliations, then fe(X ⊕ e1) = fe(X) = 0. Thus, e must include 1 and all the other nodes in e must be of the same

afﬁliation: If all the nodes of e other than node 1 are afﬁliated

with community 0, fe(X ⊕ e1) = 1 and fe(X) = 0; and if all the nodes of e other than node 1 are afﬁliated with community

1, fe(X ⊕ e1) = 0 and fe(X) = 1.

Deﬁne the set of edges corresponding to the former case

as E1, and that corresponding to the latter case as E2, i.e., E1 := {e ∈ E : 1 ∈ e and (e \ {1}) ⊂ {ηn + 1, ηn + 2, . . . , ηn + n/2}} and E2 := {e ∈ E : 1 ∈ e and (e \ {1}) ⊂ {2, 3, . . . , ηn, ηn + n/2 + 1, ηn + n/2 +

2, . . . , n}}. Consider all homogeneous edges in E1. The total

contribution of the terms associated with these edges to the

sum is e∈E1 :We=x,e:homogeneous [I {1 = We} − I {0 = We}]. Each term is −1 if observation is not corrupted, and +1

if observation is corrupted. Thus, the total contribution is

|{e∈E1 i=1

:

e is homogeneous}| Pi(2Θi − 1)

=

i(=n/1d2−−1ηn) Pi(2Θi −

1), where Pi i.∼i.d. Bern(αn) and Θi i.∼i.d. Bern(θ). By rewriting

other contributions in a similar way, we complete the proof.

G. Proof of Lemma 10

Let Z := log

1−θ θ

K i=1

Pi(2Θi

−

1)

+

ℓ

and

M(λ)

:=

E[

e

λ

l

og(

1− θ

θ

)

P1

(

2Θ

1

−

1)

]

.

Via

simple

calculation,

we

have

Pr (Z > 0) = Pr

e

1 2

Z

>

1

≤

e

1 2

ℓ

M 1/2

K

=

e

1 2

ℓ

+

K

{−

αn

(

√ 1−

θ

−

√

θ

)

2

+

O

(

α2n

)

}.

.

REFERENCES

[1] K. Ahn, K. Lee, and C. Suh, “Information-theoretic limits of subspace clustering,” in IEEE ISIT, 2017.
[2] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE TPAMI, vol. 22, no. 8, pp. 888–905, 2000.
[3] F. McSherry, “Spectral partitioning of random graphs,” in FOCS. IEEE, 2001, pp. 529–537.
[4] K. Rohe, S. Chatterjee, and B. Yu, “Spectral clustering and the highdimensional stochastic blockmodel,” The Annals of Statistics, 2011.
[5] J. Lei and A. Rinaldo, “Consistency of spectral clustering in stochastic block models,” The Annals of Statistics, vol. 43, no. 1, pp. 215–237, 2015.
[6] G. Ghoshal, V. Zlatic´, G. Caldarelli, and M. Newman, “Random hypergraphs and their applications,” Physical Review E, vol. 79, no. 6, p. 066118, 2009.
[7] T. Michoel and B. Nachtergaele, “Alignment and integration of complex networks by hypergraph-based spectral clustering,” Physical Review E, vol. 86, no. 5, p. 056111, 2012.
[8] D. Ghoshdastidar and A. Dukkipati, “Uniform hypergraph partitioning: Provable tensor methods and sampling techniques,” JMLR, vol. 18, no. 50, pp. 1–41, 2017.
[9] ——, “Consistency of spectral partitioning of uniform hypergraphs under planted partition model,” in NIPS, 2014, pp. 397–405.
[10] ——, “A provable generalized tensor spectral method for uniform hypergraph partitioning.” in ICML, 2015, pp. 400–409.
[11] ——, “Consistency of spectral hypergraph partitioning under planted partition model,” The Annals of Statistics, 45(1), pp. 289-315, 2017.
[12] L. Florescu and W. Perkins, “Spectral thresholds in the bipartite stochastic block model,” COLT, pp. 943–959, 2016.
[13] E. Abbe, “Community detection and stochastic block models: Recent developments,” JMLR, Special Issue, 2017.
[14] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova´, “Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications,” Physical Review E, 2011.
[15] E. Mossel, J. Neeman, and A. Sly, “Reconstruction and estimation in the planted partition model,” Probability Theory and Related Fields, vol. 162, no. 3-4, pp. 431–461, 2015.
[16] L. Massoulie´, “Community detection thresholds and the weak ramanujan property,” in STOC. ACM, 2014, pp. 694–703.
[17] C. Bordenave, M. Lelarge, and L. Massoulie, “Non-backtracking spectrum of random graphs: Community detection and non-regular ramanujan graphs,” in FOCS. IEEE, 2015, pp. 1347–1357.
[18] Y. Chen and J. Xu, “Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices,” JMLR, vol. 17, no. 1, pp. 882–938, 2016.
[19] J. Neeman and P. Netrapalli, “Non-reconstructability in the stochastic block model,” arXiv preprint arXiv:1404.6304, 2014.
[20] A. Montanari, “Finding one community in a sparse graph,” Journal of Statistical Physics, vol. 161, no. 2, pp. 273–299, 2015.
[21] J. Banks, C. Moore, J. Neeman, and P. Netrapalli, “Information-theoretic thresholds for community detection in sparse networks,” in COLT, 2016.
[22] E. Abbe and C. Sandon, “Proof of the achievability conjectures in the general stochastic block model,” CPAM, 2017.
[23] A. A. Amini and E. Levina, “On semideﬁnite relaxations for the block model,” arXiv preprint arXiv:1406.5647, 2014.
[24] C. Gao, Z. Ma, A. Y. Zhang, and H. H. Zhou, “Achieving optimal misclassiﬁcation proportion in stochastic block model,” JMLR, 2017.
[25] E. Mossel, J. Neeman, and A. Sly, “Consistency thresholds for binary symmetric block models,” arXiv preprint arXiv:1407.1591, 2014.
[26] S.-Y. Yun and A. Proutiere, “Accurate community detection in the stochastic block model via spectral algorithms,” arXiv, 2014.
[27] E. Abbe and C. Sandon, “Community detection in general stochastic block models: Fundamental limits and efﬁcient algorithms for recovery,” in FOCS. IEEE, 2015, pp. 670–688.
[28] E. Abbe, A. S. Bandeira, and G. Hall, “Exact recovery in the stochastic block model,” IEEE Transactions on Information Theory, 2016.

16

[29] B. Hajek, Y. Wu, and J. Xu, “Achieving exact cluster recovery threshold via semideﬁnite programming: Extensions,” IEEE Transactions on Information Theory, vol. 62, no. 10, pp. 5918–5937, Oct 2016.
[30] A. Y. Zhang, H. H. Zhou et al., “Minimax rates of community detection in stochastic block models,” The Annals of Statistics, 2016.
[31] M. Angelini, F. Caltagirone, F. Krzakala, and L. Zdeborova, “Spectral detection on sparse hypergraphs,” in Allerton, 2015.
[32] K. Ahn, K. Lee, and C. Suh, “Community recovery in hypergraphs,” ArXiv, 2016.
[33] C.-Y. Lin, C. I, and I.-H. Wang, “On the fundamental statistical limit of community detection in random hypergraphs,” in IEEE ISIT, 2017.
[34] V. M. Govindu, “A tensor decomposition for geometric grouping and segmentation,” in IEEE CVPR, 2005.
[35] S. Agarwal, K. Branson, and S. Belongie, “Higher order learning with graphs,” in ICML. ACM, 2006, pp. 17–24.
[36] G. Chen and G. Lerman, “Spectral curvature clustering (scc),” International Journal of Computer Vision, vol. 81, no. 3, pp. 317–330, 2009.
[37] F. L. Hitchcock, “The expression of a tensor or a polyadic as a sum of products,” Studies in Applied Mathematics, 1927.
[38] B. Barak and A. Moitra, “Noisy tensor completion via the sum-ofsquares hierarchy,” in COLT, 2016, pp. 417–445.
[39] C. Kim, A. S. Bandeira, and M. X. Goemans, “Community detection in hypergraphs, spiked tensor models, and sum-of-squares,” arXiv, 2017.
[40] V. Jog and P.-L. Loh, “Information-theoretic bounds for exact recovery in weighted stochastic block models using the renyi divergence,” arXiv preprint arXiv:1509.06418, 2015.
[41] M. Xu, V. Jog, and P.-L. Loh, “Optimal rates for community estimation in the weighted stochastic block model,” arXiv preprint arXiv:1706.01175, 2017.
[42] P. W. Holland, K. B. Laskey, and S. Leinhardt, “Stochastic blockmodels: First steps,” Social networks, vol. 5, no. 2, pp. 109–137, 1983.
[43] E. Mossel, J. Neeman, and A. Sly, “Consistency thresholds for the planted bisection model,” in STOC. ACM, 2015, pp. 69–75.
[44] U. Feige and E. Ofek, “Spectral techniques applied to sparse random graphs,” Random Structures & Algorithms, 2005.
[45] A. Coja-Oghlan, “Graph partitioning via adaptive spectral techniques,” Combinatorics, Probability and Computing, 2010.
[46] V. Vu, “A simple svd algorithm for ﬁnding hidden partitions,” arXiv preprint arXiv:1404.3918, 2014.
[47] O. Gue´don and R. Vershynin, “Community detection in sparse networks via grothendieck’s inequality,” Probability Theory and Related Fields, vol. 165, no. 3-4, pp. 1025–1049, 2016.
[48] J. Matousˇek, “On approximate geometric k-clustering,” Discrete & Computational Geometry, vol. 24, no. 1, pp. 61–84, 2000.
[49] P. Chin, A. Rao, and V. Vu, “Stochastic block model and community detection in sparse graphs: A spectral algorithm with optimal rate of recovery.” in COLT, 2015, pp. 391–423.
[50] C. Boutsidis, P. Kambadur, and A. Gittens, “Spectral clustering via the power method-provably,” in ICML, 2015, pp. 40–48.
[51] R. H. Keshavan, A. Montanari, and S. Oh, “Matrix completion from a few entries,” IEEE Transactions on Information Theory, vol. 56, no. 6, pp. 2980–2998, 2010.
[52] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in STOC. ACM, 2013, pp. 665–674.
[53] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” in NIPS, 2013, pp. 2796–2804.
[54] E. J. Candes, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” IEEE Transactions on Information Theory, vol. 61, no. 4, pp. 1985–2007, 2015.
[55] X. Yi, D. Park, Y. Chen, and C. Caramanis, “Fast algorithms for robust pca via gradient descent,” in NIPS, 2016, pp. 4152–4160.
[56] Y. Chen, G. Kamath, C. Suh, and D. Tse, “Community recovery in graphs with locality,” in ICML, 2016.
[57] S. Balakrishnan, M. J. Wainwright, and B. Yu, “Statistical guarantees for the em algorithm: From population to sample-based analysis,” 2017.
[58] Y. Chen and C. Suh, “Spectral MLE: Top-k rank aggregation from pairwise comparisons,” in ICML, 2015, pp. 371–380.
[59] F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborova´, and P. Zhang, “Spectral redemption in clustering sparse networks,” PNAS, vol. 110, no. 52, pp. 20 935–20 940, 2013.
[60] I. Chien, C.-Y. Lin, I. Wang et al., “On the minimax misclassiﬁcation ratio of hypergraph community detection,” arXiv, 2018.
[61] A. Coja-Oghlan, C. Moore, and V. Sanwalani, “Counting connected graphs and hypergraphs via the probabilistic method,” Random Structures & Algorithms, vol. 31, no. 3, pp. 288–329, 2007.

[62] J. Nesetril, O. Serra, J. A. Telle, O. Cooley, M. Kang, and C. Koch, “Evolution of high-order connected components in random hypergraphs,” Electronic Notes in Discrete Mathematics, 2015.
[63] J. Friedman, J. Kahn, and E. Szemeredi, “On the second eigenvalue of random regular graphs,” in STOC. ACM, 1989, pp. 587–598.
[64] T. Tao, Topics in random matrix theory. American Mathematical Society Providence, RI, 2012, vol. 132.
[65] J. A. Tropp, “User-friendly tail bounds for sums of random matrices,” Foundations of computational mathematics, 2012.
[66] P. Jain and S. Oh, “Provable tensor factorization with missing data,” in NIPS, 2014, pp. 1431–1439.
[67] N. Alon and J. H. Spencer, The probabilistic method, 2004. [68] N. Alon, M. Krivelevich, and B. Sudakov, “Finding a large hidden clique
in a random graph,” Random Structures and Algorithms, 1998. [69] R. Tron and R. Vidal, “A benchmark for the comparison of 3-d motion
segmentation algorithms,” in IEEE CVPR, 2007. [70] R. Heckel, M. Tschannen, and H. Bo¨lcskei, “Dimensionality-reduced
subspace clustering,” Information and Inference: A Journal of the IMA, vol. 6, no. 3, pp. 246–283, 2017. [71] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,” IEEE TPAMI, vol. 35, no. 11, pp. 2765–2781, 2013. [72] E. L. Dyer, A. C. Sankaranarayanan, and R. G. Baraniuk, “Greedy feature selection for subspace clustering.” JMLR, vol. 14, no. 1, pp. 2487–2517, 2013. [73] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” IEEE TPAMI, 2013. [74] R. Heckel and H. Bo¨lcskei, “Robust subspace clustering via thresholding,” IEEE Transactions on Information Theory, vol. 61, no. 11, pp. 6320–6342, 2015. [75] D. Park, C. Caramanis, and S. Sanghavi, “Greedy subspace clustering,” in NIPS, 2014, pp. 2753–2761. [76] S. Jain and V. Madhav Govindu, “Efﬁcient higher-order clustering on the grassmann manifold,” in IEEE ICCV, 2013, pp. 3511–3518. [77] O. Duchenne, F. Bach, I.-S. Kweon, and J. Ponce, “A tensor-based algorithm for high-order graph matching,” IEEE TPAMI, vol. 33, no. 12, pp. 2383–2395, 2011. [78] J. Yang and J. Leskovec, “Deﬁning and evaluating network communities based on ground-truth,” Knowledge and Information Systems, vol. 42, no. 1, pp. 181–213, 2015.
Kwangjun Ahn received his B.S. degree in the Department of Mathematical Sciences from Korea Advanced Institute of Science and Technology (KAIST) in 2017. He is currently a military police desk clerk in the US Army as a part of Korean Augmentation to the US Army (KATUSA). His research interests lie in applied mathematics.
Kangwook Lee is a postdoctoral researcher in the School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST). He earned his Ph.D. in EECS from UC Berkeley in 2016. He is a recipient of the KFAS Fellowship from 2010 to 2015. His research interests lie in information theory and machine learning.
Changho Suh (S’10–M’12) is an Ewon Associate Professor in the School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST) since 2012. He received the B.S. and M.S. degrees in Electrical Engineering from KAIST in 2000 and 2002 respectively, and the Ph.D. degree in Electrical Engineering and Computer Sciences from UC-Berkeley in 2011. From 2011 to 2012, he was a postdoctoral associate at the Research Laboratory of Electronics in MIT. From 2002 to 2006, he had been with the Telecommunication R&D Center, Samsung Electronics. Dr. Suh received the 2015 Haedong Young Engineer Award from the Institute of Electronics and Information Engineers, the 2013 Stephen O. Rice Prize from the IEEE Communications Society, the David J. Sakrison Memorial Prize from the UCBerkeley EECS Department in 2011, and the Best Student Paper Award of the IEEE International Symposium on Information Theory in 2009.

