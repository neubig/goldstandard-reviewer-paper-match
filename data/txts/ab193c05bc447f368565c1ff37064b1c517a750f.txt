BBQ-Networks: Efﬁcient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems

arXiv:1608.05081v4 [cs.LG] 23 Nov 2017

Zachary Lipton Xiujun Li† Jianfeng Gao† Lihong Li‡∗ Faisal Ahmed† Li Deng§∗

Carnegie Mellon University, Pittsburgh, PA, USA

Amazon AI, Palo Alto, CA, USA

†Microsoft Research, Redmond, WA, USA

‡Google Inc., Kirkland, WA, USA

§Citadel, Seattle, WA, USA

zlipton@cmu.edu, †{xiul,jfgao,fiahmed}@microsoft.com

‡lihongli.cs@gmail.com, §l.deng@ieee.org

Abstract

regions in which the agent is relatively uncertain in action

We present a new algorithm that signiﬁcantly improves the efﬁciency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neu-

selection. Our algorithm, the Bayes-by-Backprop Q-network (BBQN), explores via Thompson sampling, drawing Monte Carlo samples from a Bayesian neural network (Blundell et al. 2015). In order to produce the temporal difference targets

ral network. Our algorithm learns much faster than common

for Q-learning, we must generate predictions from a frozen

exploration strategies such as -greedy, Boltzmann, bootstrap-

target network (Mnih et al. 2015). We show that using the

ping, and intrinsic-reward-based ones. Additionally, we show

maximum a posteriori (MAP) assignments to generate targets

that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.

results in better performance (in addition to being computationally efﬁcient). We also demonstrate the effectiveness of replay buffer spiking (RBS), a simple technique in which we

Introduction

pre-ﬁll the experience replay buffer with a small set of transitions harvested from a na¨ıve, but occasionally successful,

Increasingly, we interact with computers via natural-language dialogue interfaces. Simple question answering (QA) bots

rule-based agent. This technique proves essential for both BBQNs and standard DQNs.

already serve millions of users through Amazon’s Alexa, Ap-

We evaluate our dialogue agents on two variants of a

ple’s Siri, Google’s Now, and Microsoft’s Cortana. These bots

movie-booking task. Our agent interacts with a user to book

typically carry out single-exchange conversations, but we as-

a movie. Success is determined at the end of the dialogue if a

pire to develop more general dialogue agents, approaching

movie has been booked that satisﬁes the user. We benchmark

the breadth of capabilities exhibited by human interlocutors.

our algorithm and baselines using an agenda-based user simu-

In this work, we consider task-oriented bots (Williams and

lator similar to Schatzmann, Thomson, and Young (2007). To

Young 2004), agents charged with conducting a multi-turn

make the task plausibly challenging, our simulator introduces

dialogue to achieve some task-speciﬁc goal. In our case, we

random mistakes to account for the effects of speech recogni-

attempt to assist a user to book movie tickets.

tion and language understanding errors. In the ﬁrst variant,

For complex dialogue systems, it is often impossible to

our environment remains ﬁxed for all rounds of training. In

specify a good policy a priori and the dynamics of an en-

the second variant, we consider a non-stationary, domain-

vironment may change over time. Thus, learning policies

extension environment. In this setting, new attributes of ﬁlms

online and interactively via reinforcement learning (RL)

become available over time, increasing the diversity of dia-

has emerged as a popular approach (Singh et al. 2000;

logue actions available to both the user and the agent. Our

Gasˇic´ et al. 2010; Fatemi et al. 2016). Inspired by RL

experiments on both the stationary and domain-extension

breakthroughs on Atari and board games (Mnih et al. 2015;

environments demonstrate that BBQNs outperform DQNs

Silver et al. 2016), we employ deep reinforcement learn-

using either -greedy exploration, Boltzmann exploration, or

ing (DRL) to learn policies for dialogue systems. Deep Q-

the bootstrap approach introduced by Osband et al. (2016).

network (DQN) agents typically explore via the -greedy

Furthermore, the real user evaluation results consolidate the

heuristic, but when rewards are sparse and action spaces

effectiveness of our approach that BBQNs are more effective

are large (as in dialogue systems), this strategy tends to fail.

than DQNs in exploration. Besides, we also show that all

In our experiments, a randomly exploring Q-learner never

agents only work given replay buffer spiking, although the

experiences success in thousands of episodes.

number of pre-ﬁlled dialogues can be small.

We offer a new, efﬁcient solution to improve the exploration of Q-learners. We propose a Bayesian exploration strat-

Task-Oriented dialogue systems

egy that encourages a dialogue agent to explore state-action

In this paper, we consider goal-oriented dialogue agents,

∗This work was done while ZL, LL & LD were with Microsoft. Copyright c 2018, Association for the Advancement of Artiﬁcial

speciﬁcally one that aims to help users to book movie tickets. Over the course of several exchanges, the agent gathers

Intelligence (www.aaai.org). All rights reserved.

information such as movie name, theater and number of tick-

Language Understanding
Natural Language Generation

State Tracker

Dialog Policy

Figure 1: Components of a dialogue system

ets, and ultimately completes a booking. A typical dialogue pipeline is shown in Figure 1. In every turn of a conversation, the language understanding module converts raw text into structured semantic representations known as dialog-acts, which pass through the state-tracker to maintain a record of information accumulated from previous utterances. The dialogue policy then selects an action (to be deﬁned later) which is transformed to a natural language form by a generation module. The conversation continues until the dialogue terminates. A numerical reward signal is used to measure the utility of the conversation. Details of this process are given below.
Dialog-acts Following Schatzmann, Thomson, and Young (2007), we represent utterances as dialog-acts, consisting of a single act and a (possibly empty) collection of (slot=value) pairs, some of which are informed while others are requested (value omitted). For example, the utterance, “I’d like to see Our Kind of Traitor tonight in Seattle” maps to the structured semantic representation request(ticket, moviename=Our Kind of Traitor, starttime=tonight, city=Seattle).
State tracker Other than information inferred from previous utterances, the state-tracker may also interact with a database, providing the policy with information such as how many movies match the current constraints. It then delexicalizes the dialog-act, allowing the dialogue policy to act upon more generic states. The tracked state of the dialogue, consisting of a representation of the conversation history and several database features, is passed on to the policy to select actions.
Actions Each action is a de-lexicalized dialog-act. In the movie-booking task, we consider a set of 39 actions. These include basic actions such as greeting(), thanks(), deny(), conﬁrm question(), conﬁrm answer(), closing(). Additionally, we add two actions for each slot: one to inform its value and the other to request it. The pipeline then ﬂows back to the user. Any slots informed by the policy are then ﬁlled in by the state tracker. This yields a structured representation such as inform(theater=Cinemark Lincoln Square), which is then mapped by a natural language generation module to a textual utterance, such as “This movie is playing tonight at Cinemark Lincoln Square.”
The conversation process above can be naturally mapped to the reinforcement learning (RL) framework, as follows (Levin, Pieraccini, and Eckert 1997). The RL agent

navigates a Markov decision process (MDP), interacting with

its environment over a sequence of discrete steps (Sutton and

Barto 1998). At step t ∈ {1, 2, . . .}, the agent observes the

current state st, and chooses some action at according to a

policy π. The agent then receives reward rt and observes new

state st+1, continuing the cycle until the episode terminates.

In this work, we assume that the set of actions, denoted A,

is ﬁnite. In our dialogue scenario, the state-tracker produces

states, actions are the de-lexicalized dialog-acts described

earlier, state transitions are governed by the dynamics of the

conversation, and a properly deﬁned reward function is used

to measure the degree of success of a dialogue. In our exper-

iment, for example, success corresponds to a reward of 40,

failure to a reward of −10, and we apply a per-turn penalty

of -1 to encourage pithy exchanges.

The goal of RL is to ﬁnd an optimal policy to maxi-

mize long-term reward. The Q-function measures, for ev-

ery state-action pair (s, a), the maximum expected cumu-

lative discounted reward achieved by choosing a in s and

E then following an optimal policy thereafter: Q∗(s, a) =

maxπ π

∞ i=0

γirt+i

|

st

=

s, at

=

a

,

where

γ

∈

(0, 1)

is a discount factor. Owing to large state spaces, most prac-

tical reinforcement learners approximate the Q-function by

some parameterized model Q(s, a; θ). An example, as we

used in this paper, is a neural network, where θ represents

the set of weights to be learned. Once a good estimate of

θ is found so that Q(·, ·; θ) is a good approximation of

Q(·, ·), the greedy policy, π(s; θ) = arg maxa Q(s, a; θ),

is a near-optimal policy (Sutton and Barto 1998). A popular

way to learn a neural-network-based Q-function is known as

DQN (Mnih et al. 2015); see the appendix for more details.

Bayes-by-Backprop
Bayes-by-Backprop (Blundell et al. 2015) captures uncertainty information from neural networks by maintaining a probability distribution over the weights in the network. For simplicity, we explain the idea for multilayer perceptrons (MLPs). An L-layer MLP for model P (y|x, w) is parameterized by weights w = {Wl, bl}Ll=1: yˆ = WL · φ(WL−1 · ... · φ(W1 · x + b1) + ... + bL−1) + bL , where φ is an activation function such as sigmoid, tanh, or rectiﬁed linear unit (ReLU). In standard neural network training, weights are optimized by SGD to minimize a loss function such as squared error.
With Bayes-by-Backprop, we impose a prior distribution over the weights, p(w), and learn the full posterior distribution, p(w|D) ∝ p(w)p(D|w), given training data D = {xi, yi}Ni=1. In practice, however, computing an arbitrary posterior distribution can be intractable. So, we instead approximate the posterior by a variational distribution, q(w|θ). In this work, we choose q to be a Gaussian with diagonal covariance, i.e., each weight wi is sampled from N (µi, σi2). To ensure that all σi remain strictly positive, we parameterize σi by the softplus function σi = log(1 + exp(ρi)), giving variational parameters θ = {(µi, ρi)}Di=1 for a D-dimensional weight vector w.
We learn these parameters by minimizing variational free energy (Hinton and Van Camp 1993), the KL-divergence be-

tween the variational approximation q(w|θ) and the posterior

µ is initialized to the zero vector 0 and the variational stan-

p(w|D):

dard deviation σ matches the prior σp for each weight. Note

E θ∗ = argminθKL[q(w|θ)||p(w|D)]

that unlike conventional neural networks, we need not assign the weights randomly because sampling breaks symmetry.

= argminθ KL[q(w|θ)||p(w)] − q(w|θ)[log p(D|w)] . As a consequence of this initialization, from the outset, the agent explores uniformly at random. Over the course of train-

When w is sampled from q, the above objective func-

ing, as the experience buffer ﬁlls, the mean squared error

tion can be estimated by its empirical version: f (D, θ) =

starts to dominate the objective function and the variational

log q(w|θ) − log p(w) − log p(D|w). It can be minimized

distribution moves further from the prior.

by SGVB, using the reparametrization trick popularized

Given experiences of the form T = {(s, a, r, s )} con-

by Kingma and Welling (2013). See appendix for more de-

sisting of transitions collected so far, we apply a Q-learning

tails.

approach to optimize the network parameter, in a way sim-

BBQ-networks

ilar to DQN (Mnih et al. 2015). To do so, we maintain a frozen, but periodically updated, copy of the same BBQN,

We are now ready to introduce BBQN, our algorithm for learning dialogue policies with deep learning models. BBQN

whose parameter is denoted by θ˜ = {(µ˜i, ρ˜i)}Di=1. For any transition (s, a, r, s ) ∈ T , this network is used to compute

builds upon the deep Q-network, or DQN (Mnih et al. 2015),

a target value y for Q(s, a; θ), resulting in a regression data

and uses a Bayesian neural network to approximate the Q-

set D = {(x, y)}, for x = (s, a). We then apply the Bayes-

function and the uncertainty in its approximation. Since we work with ﬁxed-length representations of dialogues, we use

by-backprop method described in the previous section to optimize θ, until it converges when θ˜ is replaced by θ. There

an MLP, but extending our methodology to recurrent or con-

are two ways to generate the target value y.

volutional neural networks is straightforward.

The ﬁrst uses a Monte Carlo sample from the frozen

network, w˜ ∼ q(·|θ˜), to compute the target y: y = r +

Action selection A distinct feature of BBQN is that it ex-

γ maxa Q(s , a ; w˜ ). To speed up training, for each mini-

plicitly quantiﬁes uncertainty in the Q-function estimate,

batch, we draw one sample of w˜ for target generation, and

which can be used to guide exploration. In DQN, the Q-

one sample of w for sample-based variational inference (see

function is represented by a network with parameter w.

previous section). With this implementation, the training

BBQN, in contrast, maintains a distribution q over w.

speeds of BBQN and DQN are roughly equivalent.

As described in the previous section, q is a multivariante Gaussian with diagonal covariance, parameterized by θ = {(µi, ρi)}Di=1. In other words, a weight wi has a posterior distribution q that is N (µi, σi2) where σi = log(1 + exp(ρi)).
Given a posterior distribution q over w, a natural and
effective approach to exploration is posterior sampling, or
Thompson Sampling (Thompson 1933; Chapelle and Li 2011;
Osband, Russo, and Roy 2013), in which actions are sampled
according to the posterior probability that they are optimal in the current state. Formally, given a state st and network

The second uses maximum a posterior (MAP) estimate to compute y: y = r + γ maxa Q(s , a ; µ˜). This computationally more efﬁcient choice is motivated by the observation that, since we only require the uncertainty estimates for exploration, it may not be necessary to sample from the frozen network for synthesizing targets. Furthermore, early in training, the predictive distribution of the networks has high variance, resulting in a large amount of noise in target values that can slow down training.

parameter θt in step t, an action a is selected to be at with

the probability Pr(at = a|st, θt) =

BBQN with intrinsic reward Variational Information

1{ Q(st, a; w) > Q(s, a ; w), ∀a = a} · dq(w|θt) . (1)
w
Computing these probabilities is usually difﬁcult, but fortunately all we need is a sample of an action from the corresponding multinomial distribution. To do so, we ﬁrst draw wt ∼ q(·|θt), then set at = arg maxa Q(st, a; wt). It can be veriﬁed that this process samples actions with the same probabilities given in the Equation 1. We have also considered integrating the -greedy approach, exploring by Thompson sampling with probability 1− and uniformly at random with probability . But empirically, uniform random exploration confers no supplementary beneﬁt for our task.

Maximizing Exploration (VIME) (Houthooft et al. 2016a) introduces an exploration strategy based on maximizing the information gain about the agent’s belief of environment dynamics. It adds an intrinsic reward bonus to the reward function, which quantiﬁes the agent’s surprise: r (st, at, st+1) = r(st, at) + ηDKL[p(θ|ξt, at, st+1)||p(θ|ξt)], (where ξt is deﬁned as the history of the agent up until time step t: ξt = {s1, a1, ..., st}), and has demonstrated strong empirical performance. We explore a version of BBQNs that incorporates the intrinsic reward from VIME, terming the approach BBQN-VIME-MC/MAP. The BBQN-VIME variations encourage the agents to explore the state-action regions that are relatively unexplored and in which BBQN is relatively

uncertain in action selection. In our full-domain experiment,

BBQN The BBQN is initialized by a prior distribution p

both BBQN and BBQN-VIME variations achieve similar

over w. It consists of an isotropic Gaussian whose variance

performance with no signiﬁcant difference, but in domain-

σp2 is a single hyper-parameter introduced by our model. We

extension experiments, we observe that BBQN-VIME-MC

initialize the variational parameters to match the prior. So

slightly outperforms BBQN-MAP.

Replay buffer spiking In reinforcement learning, there are multiple sources of uncertainty. These include uncertainty over the parameters of our model and uncertainty over unseen parts of the environment. BBQN addresses parameter uncertainty but it can struggle given extreme reward sparsity. Researchers use various techniques to accelerate learning in these settings. One approach is to leverage prior knowledge, as by reward shaping or imitation learning. Our approach falls into this category. Fortunately, in our setting, it’s easy to produce a few successful dialogues manually. Even though the manual dialogues do not follow an optimal policy, they contain some successful movie bookings, so they indicate the existence of the large (+40) reward signal. Pre-ﬁlling the replay buffer with these experiences dramatically improves performance (Figure 3). For these experiments, we construct a simple rule-based agent that, while sub-optimal (18.3% success rate), achieves success sometimes. In each experiment, we harvest 100 dialogues of experiences from the rule-based agent, adding them to the replay buffer. We ﬁnd that, in on our task, RBS is essential for both BBQN and DQN approaches. Interestingly, performance does not strictly improve with the number pre-ﬁlled dialogues (Figure 3). Note that replay buffer spiking is different from imitation learning. RBS works well with even a small number of warmstart dialogues, suggesting that it is helpful to communicate even the very existence of a big reward. We ﬁnd that even one example of a successful dialogue in the replay buffer could successfully jump-start a Q-learner.
Experiments
We evaluate our methods on two variants of the moviebooking task. In our experiments, we adapt the publicly available1 simulator described in Li et al. (2016). In the ﬁrst, the agent interacts with the user simulator over 400 rounds. Each round consists of 50 simulated dialogues, followed by 2 epochs of training. All slots are available starting from the very ﬁrst episode. In the second, we test each model’s ability to adapt to domain extension by periodically introducing new slots. Each time we add a new slot, we augment both the state space and action space. We start out with only the essential slots: [date, ticket, city, theater, starttime, moviename, numberofpeople, taskcomplete] and train for 40 training rounds up front. Then, every 10 rounds, we introduce a new slot in a ﬁxed order. For each added slot, the state space and action space grow accordingly. This experiment terminates after 200 rounds. In both experiments, quantifying uncertainty in the network weights is important to guide effective exploration.
To represent the state of the dialogue at each turn, we construct a 268 dimensional feature vector, consisting of the following: (i) one-hot representations of the act and slot corresponding to the current user action, with separate components for requested and informed slots; (ii) corresponding representations of the act and slot corresponding to the last agent action; (iii) a bag of slots corresponding to all previously ﬁlled slots over the course of the dialog history; (iv) both a scalar and one-hot representation of the current turn count; and (v) counts representing the number of results
1https://github.com/MiuLab/UserSimulator

Agents
BBQN-VIME-MAP BBQN-VIME-MC
BBQN-MAP BBQN-MC DQN-VIME-MAP DQN-VIME-MC DQN-Bootstrap DQN-Boltzmann
DQN

Full Domain Success Rate Reward

0.4856 0.4941 0.5031 0.4877 0.3893 0.3700 0.2516 0.2658 0.2693

9.8623 10.4268 10.7093 9.9840 5.8616 4.9990 -0.1300 0.4180 0.8660

Domain Extension Success Rate Reward

0.6813 0.7120 0.6852 0.6722 0.3751 0.3675 0.3170 0.2435 0.3503

15.8223 17.6261 17.3230 16.1320 4.9223 4.8270 -0.6820 -3.4640 4.7560

Table 1: Final performance of trained agents on 10k simulated dialogues, averaged over 5 runs.

from the knowledge base that match each presently ﬁlled-in constraint (informed slot) as well as the intersection of all ﬁlled-in constraints. For domain-extension experiments, features corresponding to unseen slots take value 0 until they are seen. When domain is extended, we add features and corresponding weights to input layer, initializing the new weights to 0 (or µi = 0, σi = σprior for BBQN), a trick due to Lipton, Vikram, and McAuley (2015).
Training details For training, we ﬁrst use a naive but occasionally successful rule-based agent for RBS. All experiments use 100 dialogues to spike the replay buffer. We note that experiments showed models to be insensitive to the precise number. After each round of 50 simulated dialogues, the agent freezes the target network parameters θ−, and then updates the Q- function, training for 2 epochs, then re-freezes and trains for another 2 epochs. There are two reasons for proceeding in 50-dialog spurts, rather than updating one minibatch per turn. First, in a deployed system, real-time updates might not be realistic. Second, we train for more batches per new turn than is customary in DQN literatures owing to the economic considerations: computational costs are negligible, while failed dialogues either consume human labor (in testing) or confer opportunity costs (in the wild).
Baseline methods To demonstrate the efﬁcacy of BBQN, we compare against -greedy in a standard DQN. Additionally, we compare against Boltzmann exploration, an approach in which the probability of selecting any action in a given state is determined by a softmax function applied to the predicted Q-values. Here, afﬁnity for exploration is parameterized by the Boltzmann temperature. We also compare to the bootstrapping method of Osband et al. (2016). For the bootstrap experiments, we use 10 bootstrap heads, and assign each data point to each head with probability 0.5. We evaluate all four methods on both the full domain (static) learning problem and on the domain extension problem.
We also tried comparing against Gaussian processes (GP) based approaches. However, in our setting, due to the highdimensional inputs and large number of time steps, we were unable to get good results. In our experiments, the computation and memory requirement grow quadratically over

VXFFHVVUDWH



%%419,0(0$3

%%419,0(0&



%%410$3

%%410&

'419,0(0$3

'419,0(0&



'41%RRWVWUDS

'41%ROW]PDQQ

'41

























URXQG

(a) Full domain (success rate)

VXFFHVVUDWH



%%419,0(0$3

%%419,0(0&



%%410$3

%%410&



'419,0(0$3

'419,0(0&

'41%RRWVWUDS



'41%ROW]PDQQ

'41



















URXQG

(b) Domain extension (success rate)

UHZDUG



%%419,0(0$3

%%419,0(0&



%%410$3

%%410&

'419,0(0$3



'419,0(0&

'41%RRWVWUDS

'41%ROW]PDQQ



'41

























URXQG

(c) Full domain (reward)

UHZDUG



%%419,0(0$3



%%419,0(0&

%%410$3



%%410&

'419,0(0$3

'419,0(0&



'41%RRWVWUDS

'41%ROW]PDQQ



'41



















URXQG

(d) Domain extension (reward)

Figure 2: Training plots with conﬁdence intervals for the full domain (all slots available from start) and domain extension problems (slots added every 10 rounds).

time, and memory starts to explode at the 10th (simulation) round. Limiting data size for GP was not helpful. Furthermore, in contrast to Gasˇic´ et al. (2010) where the state is 3-dimensional, our experiments have 268-dimensional states, making scalability an even bigger challenge. A recent paper (Fatemi et al. 2016) compares deep RL (both policy gradient and Q-learning) to GP-SARSA (Engel, Mannor, and Meir 2005) on a simpler dialogue policy learning problem. In order to make Gaussian processes computationally tractable, they rely on sparsiﬁcation methods (Engel, Mannor, and Meir 2005), gaining computation efﬁciency at the expense of accuracy. Despite this undertaking to make GPs feasible and competitive, they found that deep RL approaches outperform GP-SARSA with respect to ﬁnal performance, regret, and computational expense (by wall-clock). While we consider Gaussian processes to be an evolving area, it is worthwhile to try the Gaussian processes with sparsiﬁcation methods to compare with deep RL approaches as future work.

Architecture details All models are MLPs with ReLU activations. Each network has 2 hidden layers with 256 hidden nodes each. We optimize over parameters using Adam (Kingma and Ba 2015) with a batch size of 32 and initial learning rate of 0.001, determined by a grid search. To avoid biasing the experiments towards our methods, we determine common hyper-parameters using standard DQN. Because BBQN confers regularization, we equip DQN models with dropout regularization of 0.5, shown by Blundell et al. (2015) to confer comparable predictive performance on holdout data.
Each model has additional hyper-parameters. For example, -greedy exploration requires an initial value of and an attenuation schedule. Boltzmann exploration requires a temperature. The bootstrapping-based method of Osband et al. (2016) requires both a number of bootstrap heads and the probability that each data point is assigned to each head. Our BBQN requires that we determine the variance of the Gaussian prior distribution and the variance of the Gaussian error distribution.

Simulation results As shown in Figure 2, BBQN variants perform better than the baselines. In particular, BBQN-MAP performs the best on the full domain setting, BBQN-VIMEMC achieves the best performance on the domain extension setting, with respect to cumulative successes during training and ﬁnal performance of the trained models (Table 1). Note that the domain extension problem becomes more difﬁcult every 10 epochs, so sustained performance corresponds to getting better, while declining performance does not imply the policy becomes worse. On both problems, no method achieves a single success absent RBS. Evaluating our best algorithm (BBQN-MAP) using 0, 100, and 1000 RBS dialogues (Figure 3), we ﬁnd that using 1000 (as compared to 100) dialogues, our agents learn quickly but that their longterm performance is worse. One heuristic to try in the future may be to discard pre-ﬁlled experiences after meeting some performance threshold.

0.6

method

BBQN-MAP Spiked with 1000 Dialogues

BBQN-MAP Spiked with 100 Dialogues

0.5

BBQN-MAP Spiked with 0 Dialogues

0.4

success rate

0.3

0.2

0.1

0.0

50

100

150

200

250

300

350

400

round

15

method

BBQN-MAP Spiked with 1000 Dialogues

10

BBQN-MAP Spiked with 100 Dialogues

BBQN-MAP Spiked with 0 Dialogues

5

0

reward

5

10

15

20

50

100

150

200

250

300

350

400

round

Figure 3: RBS with 100 dialogues improves both success rate (top) and reward (bottom).

We also considered that perhaps some promising trajectories might never be sampled by the BBQN. Thus, we constructed an experiment exploring via a hybridization of the BBQN’s Thompson sampling with the -greedy approach. With probability 1 − , the agent selects an action by Thomp-

son sampling given one Monte Carlo sample from the BBQN and with probability the agent selects an action uniformly at random. However, the uniformly random exploration confers no additional beneﬁt.
Human evaluation We evaluate the agents trained using simulated users against real users, recruited from the authors’ afﬁliation. We conducted the study using the DQN and BBQN-MAP agents. In the full-domain setting, the agents were trained with all the slots. In the domain-extension setting, we ﬁrst picked DQN (b-DQN) and BBQN (b-BBQN) agents before the domain extension at training epoch 40 and the performance of these two agents is tied, nearly 45% success rate. From training epoch 40, we started to introduce new slots, and we selected another two agents (a-DQN and a-BBQN) at training epoch 200. In total, we compare three agent pairs: {DQN, BBQN} for full domain, {b-DQN, b-BBQN} from before domain extension, and {a-DQN, aBBQN} from after domain extension. In the real user study, for each dialogue session, we select one of six agents randomly to converse with a user. We present the user with a user goal sampled from our corpus. At the end of each dialogue session, the user was asked to give a rating on a scale from 1 to 5 based on the naturalness, coherence, and task-completion capability of the agent (1 is the worst rating, 5 is the best). In total, we collected 398 dialogue sessions. Figure 4a presents the performance of these agents against real users in terms of success rate. Figure 4b shows the comparison in user ratings. In the full-domain setting, the BBQN agent is signiﬁcantly better than the DQN agent in terms of success rate and user rating. In the domain-extension setting, before domain extension, the performance of both agents (b-DQN and b-BBQN) is tied; after domain extension, the BBQN (a-BBQN) agent signiﬁcantly outperforms the DQN (a-DQN) in terms of success rate and user rating.
Related work
Our paper touches several areas of research, namely Bayesian neural networks, reinforcement learning with deep Qnetworks, Thompson Sampling, and dialogue systems. This work employs Q-learning (Watkins and Dayan 1992), a popular method for model-free RL. For a broad resource on RL, we point to Sutton and Barto (1998). Recently, Mnih et al. (2015) achieved super-human performance on Atari games using deep Q-learning and incorporating techniques such as experience replay (Lin 1992).
Efﬁcient exploration remains one of the deﬁning challenges in RL. While provably efﬁcient exploration strategies are known for problems with ﬁnite states/actions or problems with nice structures (Kakade 2003; Asmuth et al. 2009; Jaksch, Ortner, and Auer 2010; Li et al. 2011; Osband, Russo, and Roy 2013), less is known for the general case, especially when general nonlinear function approximation is used. The ﬁrst DQN papers relied upon the -greedy exploration heuristic (Mnih et al. 2015). More recently, Stadie, Levine, and Abbeel (2015) and Houthooft et al. (2016a; 2016b) introduced approaches to encourage exploration by perturbing the reward function. Osband et

6XFFHVV5DWH




S 









 

 



 '41

%%41

S 

 





S   
 

E'41 E%%41 D'41 D%%41

(a) Distribution of Success Rate


S 


S 

S 



8VHU5DWH







 '41 %%41 E'41 E%%41 D'41 D%%41

(b) Distribution of User Ratings
Figure 4: Performance of BBQN agent versus DQN agent tested with real users, number of tested dialogues and pvalues are indicated on each bar (difference in mean is significant with p < 0.05).

al. (2016) attempts to mine uncertainty information by training a neural network with multiple output heads. Each head is associated with a distinct subset of the data. This works for some Atari games, but does not confer a beneﬁt for us. Chapelle and Li (2011) empirically examine Thompson sampling, one of the oldest exploration heuristics (Thompson 1933), for contextual bandits, which is later shown to be effective for solving ﬁnite-state MDPs (Strens 2000; Osband, Russo, and Roy 2013).
We build on the Bayes-by-backprop method of Blundell et al. (2015), employing the reparameterization trick popularized by Kingma and Welling (2013), and following a long history of variational treatments of neural networks (Hinton and Van Camp 1993; Graves 2011). After we completed this work, Kirkpatrick et al. (2017) independently investigated parameter uncertainty for deep Qnetworks to mitigate catastrophic forgetting issues. Blundell et al. (2015) consider Thompson sampling for con-

textual bandits, but do not consider the more challenging case of MDPs. Our paper also builds on prior work in task-oriented dialogue systems (Williams and Young 2004; Gasˇic´ et al. 2010; Wen et al. 2016) and RL for learning dialogue policies (Levin, Pieraccini, and Eckert 2000; Singh et al. 2000; Williams and Young 2007; Gasˇic´ et al. 2010; Fatemi et al. 2016). Our domain-extension experiments take inspiration from Gasˇic et al. (2014) and our user simulator is modeled on Schatzmann, Thomson, and Young (2007).
Conclusions
For learning dialogue policies, BBQNs explore with greater efﬁciency than traditional approaches. The results are similarly strong for both static and domain extension experiments in simulation and real human evaluation. Additionally, we showed that we can beneﬁt from combining BBQlearning with other, orthogonal approaches to exploration, such as those work by perturbing the reward function to add a bonus for uncovering surprising transitions, i.e., state transitions given low probability by a dynamics model, or previously rarely seen states (Stadie, Levine, and Abbeel 2015; Houthooft et al. 2016a; Houthooft et al. 2016b; Bellemare et al. 2016). Our BBQN addresses uncertainty in the Q-value given the current policy, whereas curiosity addresses uncertainty of the dynamics of under-explored parts of the environment. Thus there is a synergistic effect of combining the approaches. On the domain extension task, BBQN-VIME proved especially promising, outperforming all other methods. We see several promising paths for future work. Notably, given the substantial improvements of BBQNs over other exploration strategies, we would like to extend this work to popular deep reinforcement learning benchmark tasks (Atari, etc.) and other domains, like robotics, where the cost of exploration is high, to see if it confers a comparably dramatic improvement.
References
[Asmuth et al. 2009] Asmuth, J.; Li, L.; Littman, M. L.; Nouri, A.; and Wingate, D. 2009. A Bayesian sampling approach to exploration in reinforcement learning. In UAI.
[Bellemare et al. 2016] Bellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R. 2016. Unifying count-based exploration and intrinsic motivation. In NIPS.
[Blundell et al. 2015] Blundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra, D. 2015. Weight uncertainty in neural networks. In ICML.
[Chapelle and Li 2011] Chapelle, O., and Li, L. 2011. An empirical evaluation of Thompson sampling. In NIPS.
[Engel, Mannor, and Meir 2005] Engel, Y.; Mannor, S.; and Meir, R. 2005. Reinforcement learning with Gaussian processes. In ICML.
[Fatemi et al. 2016] Fatemi, M.; Asri, L. E.; Schulz, H.; He, J.; and Suleman, K. 2016. Policy networks with two-stage training for dialogue systems. arXiv:1606.03152.
[Gasˇic´ et al. 2010] Gasˇic´, M.; Jurcˇ´ıcˇek, F.; Keizer, S.; Mairesse, F.; Thomson, B.; Yu, K.; and Young, S. 2010.

Gaussian processes for fast policy optimisation of pomdpbased dialogue managers. In SIGDial.
[Gasˇic et al. 2014] Gasˇic, M.; Kim, D.; Tsiakoulis, P.; Breslin, C.; Henderson, M.; Szummer, M.; Thomson, B.; and Young, S. 2014. Incremental on-line adaptation of pomdp-based dialogue managers to extended domains. Interspeech.
[Graves 2011] Graves, A. 2011. Practical variational inference for neural networks. In NIPS.
[Hinton and Van Camp 1993] Hinton, G. E., and Van Camp, D. 1993. Keeping the neural networks simple by minimizing the description length of the weights. In COLT.
[Houthooft et al. 2016a] Houthooft, R.; Chen, X.; Chen, X.; Duan, Y.; Schulman, J.; Turck, F. D.; and Abbeel, P. 2016a. VIME: Variational information maximizing exploration. In NIPS.
[Houthooft et al. 2016b] Houthooft, R.; Chen, X.; Duan, Y.; Schulman, J.; De Turck, F.; and Abbeel, P. 2016b. Curiosity-driven exploration in deep reinforcement learning via Bayesian neural networks. arXiv:1605.09674.
[Jaksch, Ortner, and Auer 2010] Jaksch, T.; Ortner, R.; and Auer, P. 2010. Near-optimal regret bounds for reinforcement learning. JMLR.
[Kakade 2003] Kakade, S. 2003. On the Sample Complexity of Reinforcement Learning. Ph.D. Dissertation, Gatsby Computational Neuroscience Unit, UCL, UK.
[Kingma and Ba 2015] Kingma, D., and Ba, J. 2015. Adam: A method for stochastic optimization. In ICLR.
[Kingma and Welling 2013] Kingma, D. P., and Welling, M. 2013. Auto-encoding variational Bayes. arXiv:1312.6114.
[Kirkpatrick et al. 2017] Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.; Grabska-Barwinska, A.; et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences.
[Levin, Pieraccini, and Eckert 1997] Levin, E.; Pieraccini, R.; and Eckert, W. 1997. Learning dialogue strategies within the Markov decision process framework. In IEEE Workshop on Automatic Speech Recognition and Understanding.
[Levin, Pieraccini, and Eckert 2000] Levin, E.; Pieraccini, R.; and Eckert, W. 2000. A stochastic model of humanmachine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing 8(1):11–23.
[Li et al. 2011] Li, L.; Littman, M. L.; Walsh, T. J.; and Strehl, A. L. 2011. Knows what it knows: A framework for selfaware learning. Machine Learning.
[Li et al. 2016] Li, X.; Lipton, Z. C.; Dhingra, B.; Li, L.; Gao, J.; and Chen, Y.-N. 2016. A user simulator for taskcompletion dialogues. arXiv preprint arXiv:1612.05688.
[Lin 1992] Lin, L.-J. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning.
[Lipton, Vikram, and McAuley 2015] Lipton, Z. C.; Vikram, S.; and McAuley, J. 2015. Capturing meaning in product reviews with character-level generative text models. arXiv:1511.03683.

[Mnih et al. 2015] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D. 2015. Human-level control through deep reinforcement learning. Nature.
[Osband et al. 2016] Osband, I.; Blundell, C.; Pritzel, A.; and Van Roy, B. 2016. Deep exploration via bootstrapped DQN. In NIPS.
[Osband, Russo, and Roy 2013] Osband, I.; Russo, D.; and Roy, B. V. 2013. (More) efﬁcient reinforcement learning via posterior sampling. In NIPS.
[Schatzmann, Thomson, and Young 2007] Schatzmann, J.; Thomson, B.; and Young, S. 2007. Statistical user simulation with a hidden agenda. SIGDial.
[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; van den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.; Grewe, D.; Nham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T.; Leach, M.; Kavukcuoglu, K.; Graepel, T.; and Hassabis, D. 2016. Mastering the game of go with deep neural networks and tree search. Nature.
[Singh et al. 2000] Singh, S. P.; Kearns, M. J.; Litman, D. J.; and Walker, M. A. 2000. Reinforcement learning for spoken dialogue systems. In NIPS.
[Stadie, Levine, and Abbeel 2015] Stadie, B. C.; Levine, S.; and Abbeel, P. 2015. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv:1507.00814.
[Strens 2000] Strens, M. J. A. 2000. A Bayesian framework for reinforcement learning. In ICML.
[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. MIT Press.
[Thompson 1933] Thompson, W. R. 1933. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika.
[Watkins and Dayan 1992] Watkins, C. J., and Dayan, P. 1992. Q-learning. Machine Learning.
[Wen et al. 2016] Wen, T.-H.; Gasic, M.; Mrksic, N.; RojasBarahona, L. M.; Su, P.-H.; Ultes, S.; Vandyke, D.; and Young, S. 2016. A network-based end-to-end trainable task-oriented dialogue system. arXiv:1604.04562.
[Williams and Young 2004] Williams, J. D., and Young, S. J. 2004. Characterizing task-oriented dialog using a simulated ASR channel. In Interspeech.
[Williams and Young 2007] Williams, J. D., and Young, S. J. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language 21(2):393–422.

