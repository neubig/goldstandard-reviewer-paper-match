MULTI-ENCODER MULTI-RESOLUTION FRAMEWORK FOR END-TO-END SPEECH RECOGNITION
Ruizhi Li1, Xiaofei Wang1, Sri Harish Mallidi2, Takaaki Hori3, Shinji Watanabe1, Hynek Hermansky1
1The Johns Hopkins University, 2Amazon, 3Mitsubishi Electric Research Laboratories (MERL)
{ruizhili, xiaofeiwang, shinjiw, hynek}@jhu.edu, mallidih@amazon.com, thori@merl.com

arXiv:1811.04897v1 [cs.CL] 12 Nov 2018

ABSTRACT
Attention-based methods and Connectionist Temporal Classiﬁcation (CTC) network have been promising research directions for end-toend Automatic Speech Recognition (ASR). The joint CTC/Attention model has achieved great success by utilizing both architectures during multi-task training and joint decoding. In this work, we present a novel Multi-Encoder Multi-Resolution (MEMR) framework based on the joint CTC/Attention model. Two heterogeneous encoders with different architectures, temporal resolutions and separate CTC networks work in parallel to extract complimentary acoustic information. A hierarchical attention mechanism is then used to combine the encoder-level information. To demonstrate the effectiveness of the proposed model, experiments are conducted on Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate (WER) reduction of 18.0 − 32.1%. Moreover, the proposed MEMR model achieves 3.6% WER in the WSJ eval92 test set, which is the best WER reported for an end-to-end system on this benchmark.
Index Terms— End-to-End Speech Recognition, Hierarchical Attention Network, Encoder-Decoder, Connectionist Temporal Classiﬁcation, Multi-Encoder Multi-Resolution
1. INTRODUCTION
Recent advancements in deep neural networks enabled several practical applications of automatic speech recognition (ASR) technology. The main paradigm for an ASR system is the so-called hybrid approach, which involves training a DNN to predict context dependent phoneme states (or senones) from the acoustic features. During inference the predicted senone distributions are provided as inputs to decoder, which combines with lexicon and language model to estimate the word sequence. Despite the impressive accuracy of the hybrid system, it requires hand-crafted pronunciation dictionary based on linguistic assumptions, extra training steps to derive context-dependent phonetic models, and text preprocessing such as tokenization for languages without explicit word boundaries. Consequently, it is quite difﬁcult for non-experts to develop ASR systems for new applications, especially for new languages.
End-to-End speech recognition approaches are designed to directly output word or character sequences from the input audio signal. This model subsumes several disjoint components in the hybrid ASR model (acoustic model, pronunciation model, language model) into a single neural network. As a result, all the components of an end-to-end model can be trained jointly to optimize a single objective. Two dominant end-to-end architectures for ASR are Connectionist Temporal Classiﬁcation (CTC) [1, 2, 3] and attention-based encoder decoder [4, 5] models. While CTC efﬁciently addresses sequential problem (speech vectors to word sequence mapping) by

avoiding the alignment pre-construction step using dynamic programming, it assumes conditional independence of label sequence given the input. Attention model does not assume conditional independence of label sequence resulting in a more ﬂexible model. However, attention-based methods encounter difﬁculty in satisfying the speech-label monotonic property. To alleviate this issues, a joint CTC/Attention framework was proposed in[6, 7, 8]. The joint model was shown to provide the state-of-the-art end-to-end results in several benchmark datasets [8].
In end-to-end ASR approaches, the encoder acts as an acoustic model providing higher-level features for decoding. Bi-directional Long Short-Term Memory (BLSTM) has been widely used due to its ability to model temporal sequences and their long-term dependencies as the encoder architecture; Deep convolutional Neural Network (CNN) was introduced to model spectral local correlations and reduce spectral variations in end-to-end framework [7, 9]. The encoder architecture combining CNN with recurrent layers, was suggested to address the limitation of LSTM. While temporal subsampling in RNN and max-pooling in CNN aim to reduce the computational complexity and enhance the robustness, it is likely that subsampling technique results in loss of temporal resolution.
In this work, we propose a Multi-Encoder Multi-Resolution (MEMR) model within the joint CTC/Attention framework. This is strongly motivated by the success of multi-stream paradigm in Hybrid ASR [10, 11, 12] mimicking human speech processing cognitive system. Two parallel encoders with heterogeneous structures, RNN-based and CNN-RNN-based, are mutually complementary in characterizing the speech signal.
Several studies have shown that attention-based model beneﬁts from having multiple attention mechanisms [13, 14, 15, 16, 17, 18]. Inspired by the advances in Hierarchical Attention Network (HAN) in document classiﬁcation [16], multi-modal video description [17] and machine translation [18], we adapt HAN into our MEMR model. The encoder that carries the most discriminate information for the prediction can dynamically receive a stronger weight. Each encoder is associated with a CTC network to guide the frame-wise alignment process for individual encoder.
This paper is organized as follows: section 2 explains the joint CTC/Attention model. The description of the proposed MEMR framework is in section 3. Experiments with results and several analyses are presented in section 4. Finally, in section 5 the conclusion is derived.
2. JOINT CTC/ATTENTION MECHANISM
In this section, we review the joint CTC/attention architecture, which takes advantage of both CTC and attention-based end-to-end ASR approaches during training and decoding.

2.1. Connectionist Temporal Classiﬁcation (CTC)
Following Bayes decision theory, CTC enforces a monotonic mapping from a T -length speech feature sequence, X = {xt ∈ RD|t = 1, 2, ..., T }, to an L-length letter sequence, C = {cl ∈ U |l = 1, 2, ..., L}. Here xt is a D-dimensional acoustic vector at frame t, and cl is at position l a letter from U , a set of distinct letters.
The CTC network introduces a many-to-one function from frame-wise latent variable sequences, Z = {zt ∈ U blank|t = 1, 2, ..., T }, to letter predictions of shorter lengths. Note that the additional “blank” symbol is used to handle the merging of repeating letters. With several conditional independence assumptions, the posterior distribution, p(C|X), is represented as follows:

p(C|X) ≈

p(zt|X) pctc(C|X),

(1)

Zt

where p(zt|X) is a frame-wise posterior distribution, and we also deﬁne the CTC objective function pctc(C|X). CTC preserves the beneﬁts that it avoids the HMM/GMM construction step and preparation of pronunciation dictionary.

During inference, the joint CTC/Attention model performs a
label-synchronous beam search. The most probable letter sequence Cˆ given the speech input X is computed according to

Cˆ = arg max {λ log pctc(C|X) + (1 − λ) log patt(C|X) C∈U ∗

+ γ log plm(C)}

(4)

where external RNN-LM probability log plm(C) is added with a scaling factor γ.

3. PROPOSED MEMR FRAMWORK
The overall architecture is shown in Fig. 1. Two types of encoders with different temporal resolutions are presented in parallel to capture acoustic information in various ways, followed by an attention fusion mechanism together with per-encoder CTC. An external RNN-LM is also involved during the inference step. We will describe the details of each component in the following sections.

2.2. Attention-based EncoRdeNrN-DLMecoder

As one of the most commonly used sequence modeling techniques,

… … the attention-bca1secd2 frameworkcl selectivelcyL−e1nccLodes an audio se-
quence of variable length into a ﬁxed dimension vector representation, which is then consumed by the decoder to produce a distribution over the outputs. We can directly estimate the posterior distribution p(C|X) using the chain rule:

CTC1

Decoder

CTC2

L
p(C|X) = p(cl|c1, ...r, cl l−1, X) patt(C|X), (2)

l=1

Stream

Attention

where patt(C|X tion. Typically, a

)BiLsrS1ldTeMﬁn-ebdasaesd

tehnecoadtteernttrioarnn2l s-bfoarsmeds

objective functhe speech vec-

tors X into framAett-ewnitsioen1hidden vectorAhtttenItfiotnh2e encoder subsamples the input by a factor s, there will be T /s time steps in H =

{h1, ..., hhT11/,sh}12., . .T. h, he1T/l4etter-wise context hv21e,chto22,r. .r.l, his2T/f4ormed as a weighted summation of frame-wise hidden vectors H using content-

based attention mEencchoadneisr1m.

Encoder2

… … In comparison to CTC, not requiring conditional independence
assumptionxs11 isx12one of the axd1Tvantagxe21s xo22f using the axt2Ttention-based
modCeHl.1However, the attention is too ﬂexible to satisfy CmHon2otonic alignment constraint in speech recognition tasks.
Beamforming

2.3. Joint CTC/AAtrtreanyt1ion

Array2

The joint CTC/Attention architecture beneﬁts from both CTC and attention-based models since the attention-based encoder-decoder is trained together with CTC within the Multi-Task Learning (MTL) framework. The encoder is shared across CTC and attention-based encoders. And the objective function to be maximized is a logarithmic linear combination of the CTC and attention objectives, i.e., pctc(C|X) and p†att(C|X):

LMT L = λ log pctc(C|X) + (1 − λ) log p†att(C|X), (3)

where λ is a tunable scalar satisfying 0 ≤ λ ≤ 1. p†att(C|X) is an approximated letter-wise objective where the probability of a predic-
tion is conditioned on previous true labels.

RNNLM

… … c1 c2

cl

cL−1 cL

CTC1

Decoder

CTC2

rl

Stream

Attention

r1l

r2l

Attention1

Attention2

h11

,

h

12,

.

.

.

,

h

1 T

Encoder1 BLSTM

h21

,

h

22,

.

.

.

,

h

2 T/4

Encoder2 VGGBLSTM

x1 x2

xT

…

Fig. 1: The Multi-Encoder Multi-Resolution Architecture.

3.1. Multi-Encoder with Multi-Resolution
We propose a Multi-Encoder Multi-Resolution (MEMR) architecture that has two encoders, RNN-based and CNN-RNN-based. Both encoders take the same input features in parallel operating on different temporal resolutions, aiming to capture complimentary information in the speech.

The RNN-based encoder is designed to model temporal se-
quences with their long-range dependencies. In MEMR, the BLSTM
encoder has only BLSTM layers that extract the frame-wise hidden vector h1t without subsampling in any layer:

h1t = Encoder1(X) BLSTMt(X)

(5)

where the BLSTM decoder is labeled as index 1. The combination of CNN and RNN allows the convolutional
feature extractor applied on the input to reveal local correlations in both time and frequency dimensions. The RNN block on top of CNN makes it easier to learn temporal structure from the CNN output, to avoid modeling direct speech features with more underlying variations. The pooling layer is essential in CNN to reduce the spatial size of the representation to control over-ﬁtting. In MEMR, we use the initial layers of the VGG net architecture [19] followed by BLSTM layers as VGGBLSTM decoder labeled as index 2:

h2t = Encoder2(X) VGGBLSTMt(X).

(6)

The conﬁguration of convolutional layers in VGGBLSTM encoder is the same as in [7].

3.2. Hierarchical Attention

Since the encoders in MEMR describe the speech signal differently by catching acoustic knowledge in their own ways, encoder-level fusion is suitable to boost the network’s ability to retrieve the relevant information. We adapt Hierarchical Attention Network (HAN) in [16] for information fusion. The decoder with HAN is trained to selectively attend to appropriate encoder, based on the context of each prediction in the sentence as well as the higher-level acoustic features from both encoders, to achieve a better prediction.
The letter-wise context vectors, r1l and r2l , from individual encoders are computed as follows:

r1l = T a1lth1t , r2l = T /4a2lth2t ,

(7)

t=1

t=1

where the attention weights are obtained using a scontent-based at-
tention mechanism. Note that since Encoder2 performs downsampling by 4, the summation is till T /4 in Eq. (7).
The fusion context vector rl is obtained as a convex combination of r1l and r2l as illustrated in the following:

rl = βl1r1l + βl2r2l ,

(8)

βli = ContentAttention(ql−1, ril), i = 1, 2.

(9)

The stream-level attention weights βl1 and βl2 are estimated according to the previous decoder state, ql−1, and context vectors, r1l and r2l , from individual encoders as described in Eq. (9). The fusion
context vector is then fed into the decoder to predict the next letter.

3.3. Per-encoder CTC
In the CTC/Attention model with a single encoder, the CTC objective serves as an auxiliary task to speed up the procedure of realizing monotonic alignment and providing a sequence-level objective. In the MEMR framework, we introduce per-encoder CTC where a separate CTC mechanism is active for each encoder stream during training and decoding. Sharing one set of CTC among encoders is a soft constraint that limits the potential of diverse encoders to reveal complimentary information. In the case that both encoders are

with different temporal resolutions and network architectures, perencoder CTC can further align speech with labels in a monotonic order and customize the sequence modeling of individual streams.
During training and decoding steps, we follow Eq. (3) and (4) with a change of the CTC objective log pctc(C|X) in the following way:
1 log pctc(C|X) = 2 λ(log pctc1 (C|X) + log pctc2 (C|X)), (10)
where joint CTC loss is the average of per-encoder CTCs.
4. EXPERIMENTS
4.1. Experimental Setup
We demonstrate our proposed MEMR model using two datasets: WSJ1 [20] (81 hours) and CHiME-4 [21] (18 hours). In WSJ1, we used the standard conﬁguration: “si284” for training, “dev93” for validation, and “eval92” for test. The CHiME-4 dataset is a noisy speech corpus recorded or simulated using a tablet equipped with 6 microphones in four noisy environments: a cafe, a street junction, public transport, and a pedestrian area. For training, we used both “tr05 real” and “tr05 simu” with additional WSJ1 corpora to support end-to-end training. “dt05 multi isolated 1ch track” is used for validation. We evaluated the real recordings with 1, 2, 6-channel in the evaluation set. The BEAMFORMIT method was applied to multichannel evaluation. In all experiments, 80-dimensional mel-scale ﬁlterbank coefﬁcients with additional 3-dimensional pitch features served as the input features.
The Encoder1 contains four BLSTM layers, in which each layer has 320 cells in both directions followed by a 320-unit linear projection layer. The Encoder2 combines the convolution layers with RNN-based network that has the same architecture as Encoder1. A content-based attention mechanism with 320 attention units is used in encoder-level and frame-level attention mechanisms. The decoder is a one-layer unidirectional LSTM with 300 cells. We use 50 distinct labels including 26 English letters and other special tokens, i.e., punctuations and sos/eos.
We incorporated the look-ahead word-level RNN-LM [25] of 1-layer LSTM with 1000 cells and 65K vocabulary, that is, 65Kdimensional output in Softmax layer. In addition to the original speech transcription, the WSJ text data with 37M words from 1.6M sentences was supplied as training data. RNN-LM was trained separately using Stochastic Gradient Descent (SGD) with learning rate = 0.5 for 60 epochs.
The MEMR model is implemented using Pytorch backend on ESPnet. Training procedure is operated using the AdaDelta algorithm with gradient clipping on single GPUs, “GTX 1080ti”. The mini-batch size is set to be 15. We also apply a unigram label smoothing technique to avoid over-conﬁdence predictions. The beam width is set to 30 for WSJ1 and 20 for CHiME-4 in decoding. For model jointly trained with CTC and attention objectives, λ = 0.2 is used for training, and λ = 0.3 for decoding. RNN-LM scaling factor γ is 1.0 for all experiments with the exception of using γ = 0.1 in decoding attention-only models.
4.2. Results
The overall experimental results on WSJ1 and CHiME-4 are shown in Table 1. Compared to joint CTC/Attetion single-encoder models, the proposed MEMR model with per-encoder CTC and HAN achieves relative improvements of 9.6% (28.4% → 26.4%) in

Table 1: Comparison among single-encoder end-to-end models with BLSTM or VGGBSLTM as the encoder, the MEMR model and prior end-to-end models. (WER: WSJ1, CHiME-4)

Table 2: Comparison between the MEMR model and VGGBSLTM single-encoder model with similar network size. (WER: WSJ1, CHiME-4)

Model
BLSTM (Single-Encoder) CTC ATT CTC+ATT
VGGBLSTM (Single-Encoder) CTC ATT CTC+ATT
BLSTM+VGGBLSTM (MEMR) CTC ATT CTC(shared)+ATT CTC(shared)+ATT+HAN CTC(per-enc)+ATT CTC(per-enc)+ATT+HAN
Previous Studies RNN-CTC [2] Eesen [3] Temporal LS + Cov. [22] E2E+regularization[23] Scatt+pre-emp[24] Joint e2e+look-ahead LM[25] RCNN+BLSTM+CLDNN [26] EE-LF-MMI [27]

CHiME-4 WSJ1 et05 real 1ch eval92

62.7

36.4

50.2

20.8

29.2

4.6

50.6

19.1

42.2

17.2

29.6

5.6

49.1

15.2

44.3

18.9

26.8

4.4

26.9

4.3

26.6

4.1

26.4

3.6

-

8.2

-

7.4

-

6.7

-

6.3

-

5.7

-

5.1

-

4.3

-

4.1

CHiME-4 and 21.7% in WSJ1 (4.6% → 3.6%) in terms of WER. We compare the MEMR model with other end-to-end approaches, and it outperforms all of the systems from previous studies. We design experiments with ﬁxed encoder-level attention βl1 = βl2 = 0.5. And the MEMR model with HAN outperforms the ones without parameterized stream attention. Moreover, per-encoder CTC constantly enhances the performance with or without HAN. Specially in WSJ1, the model shows notable decrease (4.3% → 3.6%) in WER with per-encoder CTC. Our results further conﬁrms the effectiveness of joint CTC/Attention architecture in comparison to models with either CTC or attention network.
For fair comparison, we increase the number of BLSTM layers from 4 to 8 in Encoder2 to train a single-encoder model. In Table 2, the MEMR system outperforms the single-encoder model by a signiﬁcant margin with similar amount of parameters, 21.9M v.s. 21.3M. In CHiME-4, we evaluate the model using real test data from 1, 2, 6-channel resulting in an average of 19% relative improvement from all three setups. In WSJ1, we reach 3.6% WER in eval92 in our MEMR framework with relatively 32.1% improvement.
The results in Table 3 shows the contribution of multiple resolution. The WER goes up when increasing subsampling factor s1 closer to s2 = 4 in both datasets. In other words, the fusion works better when two encoders are more heterogeneous which supports our hypothesis. As shown in Table 4, We analyze the average streamlevel attention weight for Encoder2 when we gradually decrease the number of LSTM layers while keeping Encoder1 with the original

Data
CHiME-4 et05 real 1ch et05 real 2ch et05 real 6ch
WSJ1 eval92

Single-Encoder (21.9M)
32.2 26.8 21.7
5.3

Proposed Model (21.3M)
26.4 (18.0%) 21.9 (18.3%) 17.2 (20.8%)
3.6 (32.1%)

Table 3: Effect of Multi-Resolution Conﬁguration (s1, s2), where s1 and s2 are the subsampling factors for Encoder1 and Encoder2, respectively. (WER: WSJ1, CHiME-4)

Data

(4,4) (2,4) (1,4)

CHiME-4 et05 real 1ch 29.1 27.0 26.4

WSJ1 eval92

4.5 4.2 3.6

conﬁguration. It aims to show that HAN is able to attend to the appropriate encoder seeking for the right knowledge. As suggested in the table, more attention goes to Encoder1 from Encoder2 as we intentionally make Encoder2 weaker.
Table 4: Analysis of Hierarchical Attention mechanism when when ﬁxing Encoder1 and changing the number of LSTM layers in Encoder2. (WER: CHiME-4)

# LSTM Layers in VGGBLSTM
0 1 2 3 4

Average Stream Attention for VGGBLSTM
0.27 0.52 0.75 0.82 0.81

WER %
30.6 29.8 28.9 27.8 26.4

5. CONCLUSION
In this work, we present our MEMR framework to build an endto-end ASR system. Higher-level frame-wise acoustic features are carried out from RNN-based and CNN-RNN-based encoders with subsampling only in convolutional layers. Stream fusion selectively attends to each encoder via a content-based attention. We also investigated that assigning a CTC network to individual encoder further enhance the heterogeneous conﬁguration of encoders. The MEMR model outperforms various single-encoder models, reaching the state-of-the-art performance on WSJ among end-to-end systems.

6. REFERENCES
[1] Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber, “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in International Conference on Machine learning (ICML), 2006, pp. 369–376.
[2] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in International Conference on Machine Learning (ICML), 2014, pp. 1764– 1772.
[3] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,” in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167–174.
[4] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
[5] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems (NIPS), 2015, pp. 577–585.
[6] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTCattention based end-to-end speech recognition using multitask learning,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 4835– 4839.
[7] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan, “Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM,” in INTERSPEECH, 2017.
[8] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, “Hybrid ctc/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[9] Yu Zhang, William Chan, and Navdeep Jaitly, “Very deep convolutional networks for end-to-end speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017.
[10] Sri Harish Reddy Mallidi et al., A Practical and Efﬁcient Multistream Framework for Noise Robust Speech Recognition, Ph.D. thesis, Johns Hopkins University, 2018.
[11] Hynek Hermansky, “Multistream recognition of speech: Dealing with unknown unknowns,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1076–1088, 2013.
[12] Sri Harish Mallidi and Hynek Hermansky, “Novel neural network based fusion for multistream asr,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5680–5684.
[13] Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, and Kazuya Takeda, “Multi-head decoder for end-to-end speech recognition,” in Proc. Interspeech 2018, 2018, pp. 801–805.
[14] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J

Weiss, Kanishka Rao, Ekaterina Gonina, et al., “State-of-theart speech recognition with sequence-to-sequence models,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4774–4778.
[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017, pp. 5998–6008.
[16] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy, “Hierarchical attention networks for document classiﬁcation,” in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 1480–1489.
[17] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John R Hershey, Tim K Marks, and Kazuhiko Sumi, “Attention-based multimodal fusion for video description,” in Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017, pp. 4203–4212.
[18] Jindˇrich Libovicky` and Jindˇrich Helcl, “Attention strategies for multi-source sequence-to-sequence learning,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2017, vol. 2, pp. 196–202.
[19] Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[20] Linguistic Data Consortium, “CSR-II (wsj1) complete,” Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.
[21] Emmanuel Vincent, S Watanabe, Jon Barker, and Ricard Marxer, “The 4th chime speech separation and recognition challenge,” 2016.
[22] Jan Chorowski and Navdeep Jaitly, “Towards better decoding and language model integration in sequence to sequence models,” arXiv preprint arXiv:1612.02695, 2016.
[23] Yingbo Zhou, Caiming Xiong, and Richard Socher, “Improved regularization techniques for end-to-end speech recognition,” arXiv preprint arXiv:1712.07108, 2017.
[24] Neil Zeghidour, Nicolas Usunier, Gabriel Synnaeve, Ronan Collobert, and Emmanuel Dupoux, “End-to-end speech recognition from the raw waveform,” arXiv preprint arXiv:1806.07098, 2018.
[25] Takaaki Hori, Jaejin Cho, and Shinji Watanabe, “End-to-end speech recognition with word-based RNN language models,” arXiv preprint arXiv:1808.02608, 2018.
[26] Yisen Wang, Xuejiao Deng, Songbai Pu, and Zhiheng Huang, “Residual convolutional ctc networks for automatic speech recognition,” arXiv preprint arXiv:1702.07793, 2017.
[27] Hossein Hadian, Hossein Sameti, Daniel Povey, and Sanjeev Khudanpur, “End-to-end speech recognition using lattice-free mmi,” Proc. Interspeech 2018, pp. 12–16, 2018.

