TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment

Jianwei Yang Microsoft Research
jianwyan@microsoft.com

Yonatan Bisk Carnegie Mellon University
ybisk@cs.cmu.edu

Jianfeng Gao Microsoft Research
jfgao@microsoft.com

arXiv:2108.09980v1 [cs.CV] 23 Aug 2021

Abstract
Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The Ô¨Årst is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efÔ¨Åcient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we Ô¨Ånetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, setting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.
1. Introduction
Aligning or grounding language to videos is a challenging topic in the context of vision-language (VL) research as it requires the model to understand contents, dynamics, and causality presented in videos [3]. Inspired by the success of BERT [10] in natural language processing, there is a growing interest in applying transformer-based multi-modal models for video-text alignment and representation learning [41, 40, 60, 33, 14, 28]. These models are typically pretrained on large amounts of noisy video-text pairs using contrastive learning [35, 34], and then applied in a zeroshot manner or Ô¨Ånetuned for various downstream tasks, such as text-video retrieval [52], video action step localization [61], video action segmentation [43], video question

Contrastive Loss ùêø3

ùêæ‚Ä≤ hard negatives

Multi-modal Fusion Layers

‚Ä¶ H Hard negative mining

alignment

Contrastive Loss ùêø2
(add, tomatoes, pan, stir)

scores

Contrastive Loss ùêø1

Language Encoder

Video Encoder

Add blended tomatoes to pan and stir.
Text-video pair

‚Ä¶ ùêæ ‚àí 1 negatives

Figure 1: The proposed token-aware cascade contrastive
learning pipeline. We compute three contrastive losses: 1) sentence-level loss L1 over all negative examples; 2) tokenlevel loss L2 on content words (noun, verb) over all negative examples; 3) sentence-level loss L3 over hard negative examples sampled based on L1 and L2 online.

answering [44, 27] and video captioning [58].
In this paper, we present a new variant of contrastive learning, Token-Aware Cascade contrastive learning (TACo) to improve the video-text alignment for both large-scale pretraining and downstream speciÔ¨Åc tasks. As the name indicates, TACo makes two modiÔ¨Åcations to the conventional contrastive learning used in video-language domain. The Ô¨Årst is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that, given a video and its corresponding text, content words, such as nouns and verbs, are more likely than function words to be aligned with (or grounded to) visual contents in the video. Conventional contrastive learning typically compute the loss after aggregating over all the words in the text and frames in the video (loss L1 or L3 in Fig. 1). In contrast, the token-aware contrastive loss is computed using only a subset of words whose syntactic classes belong to a predeÔ¨Åned set (e.g., nouns and verbs), which forces the grounding of individual words to the video (loss L2). For example, we pay particular attention to the words ‚Äúadd‚Äù, ‚Äútomatos‚Äù, ‚Äúpan‚Äù and ‚Äústir‚Äù in Fig. 1.

The second technique we introduce is a cascade sampling method to Ô¨Ånd a small set of hard negative examples for training the multi-modal fusion layers. Consider a batch of K video-text pairs. For each of the video-text pairs, the ideal case is that we use the remaining K ‚àí 1 negative videos or texts to compute the contrastive loss after multi-modal fusion. However, the cost of computing the contrastive loss quickly becomes prohibitive when it is coupled with multi-modal fusion layers, considering its high complexity O(K2 √ó L2) where L is total number of visual and textual tokens. A conventional way to address this is using random sampling to select a small subset of negative pairs. In this paper, instead of random sampling, we propose a cascade sampling method as shown in the top-right of Fig. 1 to efÔ¨Åciently select a small set of hard negative examples on the Ô¨Çy during training. It leverages the videotext alignment scores computed in L1 and L2 before multimodal fusion layers, and helps to learn the multi-modal fusion layers more effectively without any extra overhead.
We perform a comprehensive empirical study to validate the effectiveness of TACo in both pretraining and dataset-speciÔ¨Åc scenarios. We apply TACo and different variants of contrastive losses to train or pretrain and Ô¨Ånetune on various downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet) [58, 52, 12], video action step localization (CrossTask) [61] and action segmentation (COIN) [43]. Our results show that TACo improves the text-video retrieval performance over current state-of-the-art across three benchmarks. Furthermore, the learned multi-modal representation and video representation can be effectively transferred to CrossTask and COIN, and achieve better or comparable performance to current state-of-the-art methods.
2. Related work
Video-language pretraining. Realistic application scenarios around videos have prompted emergence of various video-language tasks, such as text-video retrieval [30, 55, 53], video question answering [21, 27], video captioning [54, 59], etc. Inspired by the success of BERT for largescale pretraining in language domain [10], transformers have been employed in the video-language domain [41, 60, 33, 28] as well as image-language domain [42, 32, 57, 29]. Combined with large scale datasets, e.g. Howto100M [35] this approach has proven to be effective on various downstream tasks. Depending on the tasks of interest, some approaches train a multi-modal transformer using a combination of multiple losses including video-text alignment [41, 60, 33, 28], masked token (words/frames/objects) prediction [41, 60, 33], and frame order prediction [28], etc. Some other approaches exploited various contrastive learning techniques to directly optimize the feature space without multi-modal fusion [35, 34, 31, 14]. In most of previ-

ous works, these two approaches were explored separately. Very recently, an updated version of [33] used two independent alignment losses before and after multi-modal fusion in a single framework. In this paper, however, these two losses cooperate closely with each other during training in that the earlier stage helps to discover the hard negatives while the multi-modal layers with more capacity help to tackle those hard samples particularly.
Video-text alignment. Aligning videos to text requires the model to understand motion and temporal coherence. Some works have relied on attention mechanisms to extract key information from videos [45, 55], while others preserve visual information by composing pairwise joint representation using 3D tensors [53] or use multi-level video encoders to separately encode the spatial and temporal cues [11]. These models usually rely on a rank or margin loss to learn the correct alignment for video-text pairs. Another line of work learns Ô¨Åne-grained or hierarchical alignment between videos and texts [56, 49, 6]. In [49], the authors proposed a Ô¨Åne-grained alignment by extracting the nouns and verbs from action phrase in a sentence and projecting them into a shared space with videos. Alternatively, the authors in [6] extract a hierarchical semantic graph and apply graph reasoning to achieve the alignment at different levels. Similar ideas have been also proposed in the imagetext alignment by decomposing the images and texts into sub-tokens [26, 50]. Thus far, it has not been studied how these task-speciÔ¨Åc architectures can be integrated into largescale pretraining. In this paper, we are the Ô¨Årst to propose a simple yet effective token-aware contrastive loss for Ô¨Ånegrained alignment for pretraining and downstream tasks.
Negative sampling. Key to efÔ¨Åcient contrastive training is a good source of negative examples. Most of current approaches use random sampling strategies for training videotext alignment [60, 33]. However, in the domain of imagetext retrieval, a few works tried hard negative sampling to choose the hardest negatives for training. In [2, 13], the authors computed the alignment scores for all image-text pairs in a mini-batch and use the hardest negative sample to compute the marginal loss. However, this strategy can only be applied without multi-modal fusion. In those models which have multi-modal fusion layers for better representations [32, 8], the authors instead compute the matching score ofÔ¨Çine and then use it to sample hard negatives for Ô¨Ånetuning image-text retrieval model, which however is difÔ¨Åcult for large-scale pretraining due to the high computational cost. In this paper, our cascade hard negative mining is particularly designed to address these issues as we efÔ¨Åciently select the hard negative samples online before multi-modal fusion and send them to the fusion layers for computing the loss. As we will show in our experiments, this technique can be seamlessly applied to both large-scale pretraining and downstream tasks.

2

3. Method
3.1. Framework
As depicted in Fig. 1, our model has three components:
Video encoding module fŒ∏v . It is implemented by a stack of self-attention layers parameterized by Œ∏v. Here, we assume the input video features have been already extracted using some pre-trained models such as 2D CNN (e.g., ResNet [18]) or 3D CNN (e.g., I3D [4], S3D [51]). Given the input video embeddings, video encoder starts with a linear layer to project them to the same dimension d as following self-attention layers. We denote the output of our video encoder for a video clip by a sequence of m features, x = {x1, ..., xm} ‚àà Rm√ód. The number of features m depends on the choice of sampling frame rate and the video feature extractor, which we will discuss in Sec. 4.
Language encoding module fŒ∏t . We use pretrained tokenizer [48] and BERT [10] to tokenize the input texts and extract textual features, respectively. Given a raw sentence, we append a ‚Äú[CLS]‚Äù and ‚Äú[SEP]‚Äù to the beginning and end, respectively. At the top, we can obtain a sequence of n textual features y = {y1, ..., yn} ‚àà Rn√ód. We ensure the output feature dimension of video encoder to be identical to that of language encoder. During training, we update the parameters Œ∏t in our language encoder to adapt to the texts in speciÔ¨Åc domain, e.g., cooking instructions in YouCook2 [58].
Multi-modal fusion module fŒ∏m . It also consists of selfattention layers with learnable parameters Œ∏m. It takes video features x ‚àà Rm√ód and text features y ‚àà Rn√ód from two separate modalities as inputs and output the (m + n) features z = {z1, ..., z(m+n)} ‚àà R(m+n)√ód. To help it to distinguish the video and language tokens, we use a token type embedding layer to learn two embeddings and add them to the visual and textual tokens, separately. Similar to original Transformer [47], we include a positional embedding layer to encode the absolute token positions in the input sequence.
The above three components comprise our video-text alignment model which is then trained with the proposed token-aware cascade contrastive loss. We start with a brief review of conventional contrastive learning and then introduce the proposed technique.

3.2. Contrastive learning: a revisit

Given

a

set

of

N

video-text

pairs

{(

v

i

,

ti

)}

N i=1

,

our

goal

is to learn an optimal scoring function s such that paired

video and text (vi, ti) have higher scores than all the other

unmatched pairs (vj, tk), j = k. From the probabilistic

perspective, aligning vi to ti is equivalent to maximizing

the conditional probability p(vi|ti) while minimizing the

probability for all negative pairs p(vj|ti), j = i. Accord-

ing to [15, 37], p(vj|ti) can be approximated by:

exps(vj ,ti)

p(vj |ti) ‚àº N exps(vk,ti)

(1)

k=1

where s(v, t) is the alignment score between v and t; the denominator is a sum over all possible videos, which is a partition function for normalization. Adding cross-entropy loss on p(vj|ti), we can then derive the NCE loss [15]:

N

Lnce = ‚àí log p(vi|ti)

i=1

(2)

N

exps(vi ,ti )

‚àº ‚àí log

i=1

exps(vi,ti) + k=i exps(vk,ti)

The denominator in Eq. 2 requires a sum over all videos in a dataset, which is intractable in practice. Therefore, we usually compute the NCE loss on a mini-batch of K(K N ) video-text pairs sampled from the whole dataset. Ideally, we want to learn the parameters Œ∏ = {Œ∏v, Œ∏t, Œ∏m} of the model to minimize the above NCE loss, such that ‚àÜ = s(vi, ti) ‚àí s(vj, ti) is maximized over all tuples (ti, vi, vj), j = i. A number of previous works used the above formula for contrastive learning [34, 60]. Meanwhile, there are some variants of computing contrastive loss in video-langauge representation learning. For example, [28, 14] omits the denominator and incorporate a margin s.t. s(vi, ti) > s(vj, ti) + Œ¥, ‚àÄj = i in a mini-batch. [33] optimizes binary cross-entropy (BCE) by assigning (vi, ti) a positive label (1) and other pairs a negative label (0).

3.3. TACo: our approach

The way of using contrastive learning in previous works

has two issues. First, the loss is computed at sentence-

level by taking ‚Äò[CLS]‚Äô token [14] or the maximum over

all tokens [34] in a sentence. Clearly, the content words

(e.g., nouns, verbs) are more likely to align with the visual

contents or concepts in the videos compared with function

words (e.g., stop words). Second, the high computational

cost in multi-modal fusion layers hinder the usage of large

batch of negative samples, which however is essential to

contrastive learning [34, 17, 7]. Motivated by these two is-

sues, we introduce TACo, a simple yet effective method to

improve the contrastive learning. We elaborate below how

these contrastive losses are computed.

Given

the

K

video-text

pairs

{(

v

i

,

ti

)}

K i=1

in

a

mini-

batch, we Ô¨Årst use our video encoder fŒ∏v and lan-

guage encoder fŒ∏t to obtain a batch of video features X = {x1, ..., xK } ‚àà RK√óm√ód and text features Y =
{y1, ..., yK } ‚àà RK√ón√ód, respectively. Then, we average
all tokens of a video clip vi to get x¬Øi ‚àà R1√ód, and take the
Ô¨Årst ‚Äò[CLS]‚Äô token for each text ti to get y¬Øi ‚àà R1√ód. Based

3

on x¬Ø and y¬Ø, we compute the sentence-level contrastive loss:

K

expx¬Øi ¬∑y¬Øi /œÑ1

L1 = ‚àí log

(3)

i=1

expx¬Øi¬∑y¬Øi/œÑ1 + j=i expx¬Øj ¬∑y¬Øi/œÑ1

where œÑ1 is a scalar temperature parameter. In Eq. 3, the computation is simply a number of dot-products between video and text features. Giving such efÔ¨Åciency, we can use all the K ‚àí 1 negative samples in a mini-batch to compute the loss. Through this, we optimize Œ∏v and Œ∏t so as to project the video and text samples into an aligned feature space.
The ‚Äò[CLS]‚Äô token and average of video tokens in Eq. 3 overlooks the differences across tokens and frames, and thus may not provide the pressure to push individual tokens (e.g., nouns and verbs) to ground on the speciÔ¨Åc video contents. To encourage correct alignment, in addition to the sentencelevel loss, we introduce a token-level contrastive loss:

Ô£´

Ô£∂

K

exps(xi ,yip )/œÑ2

L2 = ‚àí

log Ô£¨

p

Ô£∑
p

i=1 p‚ààPi Ô£≠ exps(xi,yi )/œÑ2 + exps(xj ,yi )/œÑ2 Ô£∏

j=i

(4)

where œÑ2 is another scalar temperature parameter; Pi is the indices of tokens of interest in i-th text, and yip is the p-th
token embedding in i-th text. s(¬∑) measures the similarity between video features and speciÔ¨Åc token embedding yip. It Ô¨Årst computes the dot-product between yip ‚àà R1√ód and all m video tokens x ‚àà Rm√ód, and then take the maximum

over m scores to get the Ô¨Ånal alignment score. Through

Eq. 4, the model uses individual tokens as anchors to align

with video, which is complementary to the sentence-level

loss in Eq. 3. Similar to Eq. 3, we can compute this token-

level contrastive loss efÔ¨Åciently, and thus use all the K ‚àí 1

negative samples. As a whole, these two losses are used to

optimize Œ∏v and Œ∏t in a token-aware manner.

Token of interest. In Eq. 4, we need to decide which tokens

should be included in Pi. In this paper, we heuristically

select nouns and verbs as the targets considering they are

more ‚Äúconcrete‚Äù in the videos. In practice, nouns or verbs

usually have different discriminativeness even if they are all

the same type. For example, ‚Äúman‚Äù is a noun but is less in-

formative than ‚Äúgymnast‚Äù. To reÔ¨Çect this, we further assign

different words with different weights by computing their

inverse document frequency (idf) [22]. A higher idf means

it is more unique across the corpus, and hence will weigh

more when computing the token-level contrastive loss. An-

other practical issue for computing the loss is that the tokens

are usually sub-words due to the BERT tokenizer. Hence,

for all tokens that belongs to the same word, we will assign

the same weights accordingly.

After computing the token-aware contrastive loss, we

feed the features from separate modalities to multi-modal

fusion layers to enable more interactions between them two.

Similar to previous work [60], we take the feature corre-

sponding to the ‚Äú[CLS]‚Äù in the (m + n) outputs. We regard

this as the summary of two modalities and then compute the

contrastive loss:

Ô£´

Ô£∂

K

expw¬∑zic,lis

L3 = ‚àí log Ô£≠

cls

cls Ô£∏ (5)

i=1

expw¬∑zi,i + j=i expw¬∑zj,i

where zjc,lis is the multi-modal fusion output for ‚Äú[CLS]‚Äù token taking xj and yi as inputs; w ‚àà R1√ód is the parameter in a linear layer1. Based on Eq. 5, we optimize all parameters in our model Œ∏ = {Œ∏v, Œ∏t, Œ∏m} in collaboration with Eq. 3 and Eq. 4.
In Eq. 5, a practical challenge is that we can hardly use all (K ‚àí 1) negative samples in the mini-batch, due to the high computational and memory cost in the multi-modal fusion. The O(d(m + n)2) complexity of self-attention layer makes it intractable to pass all K √ó K pairs into the multimodal layers. Previous work solved this by performing random sampling to cut the number of negative samples to K . However, randomly choosing negative samples may result in sub-optimal learning since the pairs are scarce. We therefore introduce a cascade sampling strategy to Ô¨Ånd hard negatives instead of random ones. Cascade hard negative sampling. To reduce the computational cost in Eq. 5, we choose among all possible videotext pairs a small subset which are most difÔ¨Åcult. However, computing the alignment scores for all pairs using Eq. 5 and then select the hard negatives is a ‚Äúchicken-and-egg‚Äù problem. Instead, we propose to use the similarities between all video-text pairs computed in Eq. 3 and Eq. 4 as the guidance. SpeciÔ¨Åcally, for each text-video pair (vj, ti), we take their global similarity x¬Øj ¬∑ y¬Øi computed in Eq. 3 and tokenlevel similarity by aggregating p‚ààPi s(xj, yip) for all tokens of interest in ti. Then we sum the two similarities as the alignment score for the given pair. For each text, we choose the top K aligned negative videos and vice versa. The resulting 2K √ó (K + 1) pairs are then fed into the multi-modal fusion layers. Through this strategy, we can effectively select the difÔ¨Åcult negative samples on the Ô¨Çy at no extra cost. Since the multi-modal fusion layers has more capacity (parameters) to distinguish these hard negatives from positive ones, our sampling strategy naturally prompts the cooperation between the three contrastive losses.
Finally, we present a comprehensive comparison to differentiate our model with previous works with respect to the used contrastive learning method in Table 1.

3.4. Objective

The training objective in our method is Ô¨Ånding optimal Œ∏ = {Œ∏v, Œ∏t, Œ∏m} by minimizing the combination of the above three contrastive losses:

N

arg min

(L1 + ŒªtL2 + L3)

(6)

Œ∏v ,Œ∏t,Œ∏m i=1

1for clarity, we omit the bias term in the formula

4

Method

Token-aware Early stage Later stage Cascade Loss

VideoBert [41]



CBT [40]



TJVE [35]



MIL-NCE [34]



ActBert [60]



UniVL [33]



MMT [14]



TACo(Ours)









BCE







NCE





 Margin







NCE







BCE







NCE





 Margin







NCE

Table 1: A comparison of video-language pretraining methods regarding contrastive learning strategies. ‚ÄúEarly stage‚Äù and ‚ÄúLater stage‚Äù mean computing the loss before and after multi-modal fusion, respectively. ‚ÄúCascade‚Äù means using cascade hard negative sampling.

where Œªt is the weight of token-level loss (0.5 by default). During inference, we make the prediction by summing the alignment scores from all the three scoring functions.
4. Experimental setup
4.1. Datasets
In our experiments, we train and evaluate our model on the following established benchmarks: ‚Ä¢ YouCook2 [58] consists of 2k videos about routine cooking activities of 89 recipes. Each video contains multiple video clips annotated with text descriptions by human annotators. Following [35, 34], we train our models on the training split, and report the text-video retrieval performance on around 3.5k validation clips. ‚Ä¢ MSR-VTT [52] contains 10k video clips associated with 200k sentences. There are two validation splits used in previous work. In [31, 14], the training set has 9k clip-text pairs with the remaining 1k pairs for evaluation, which we denote by split1. In [53, 35, 34], 1k clip-text pairs are sampled from the 3k pairs in test set for evaluation, while the original 7k pairs are used for training. We denote this by split2. We report text-video retrieval results using both splits. ‚Ä¢ ActivityNet [25]. It consists of 20K YouTube videos, each of which is associated with multiple human-annotated captions. Following [56, 14], we concatenate all the captions for a video into a paragraph and evaluate the paragraphvideo retrieval on the ‚Äúval1‚Äù split. ‚Ä¢ Howto100M [35]. We compare with previous work under the pretraining protocol on Howto100M [35, 34, 60, 33]. It was collected from YouTube and contains over 1.2M narrated videos associated with automatically generated transcripts. Each video contains over 100 clips on average.
To further verify the transferrability or our learned multimodal representation from Howto100M, we also evaluate the action step localization and action segmentation on CrossTask [61] and COIN [43], respectively.

4.2. Settings
Previous work use a variety of different video and language representations which we Ô¨Ånd signiÔ¨Åcantly affect the Ô¨Ånal performance. We summarize different choices below: ‚Ä¢ Video representations. For 2D CNN, Resnet-152 [18] is used to extract feature map and then globally pooled to 2048-d [35, 33]. For 3D features, commonly used models are I3D [5], R(2+1)D [46] and S3D [51]. In [60], the authors further extract objects from the video clips. In [31, 14], the authors use collaborative experts to extract features from audio, scene, OCR, face, speech, etc. ‚Ä¢ Language representations. There are primarily four variants: 1) GoogleNews pretrained word2vec (w2v) [36] used in [31, 35, 34]; 2) LSTM or Bidirectional LSTM [19]; 3) pretrained BERT [10] used in [41, 60, 33, 14] and 4) OpenAI-GPT [38] used in [31].
In this paper, we use a pretrained BERT-base model for language representation as in [60, 33]. For video features, following [35, 34, 33], we extract 2D CNN features using Resnet-152 (R-152) pretrained on ImageNet [9]. For 3D CNN features, we use I3D (with Resnext-101 backbone) pretrained on Kinetics-400 [23] and S3D [51] pretrained on Howto100M [34]. The off-the-shelf pretrained weights are provided by [16] and [34]. For simplicity, we denote them by I3D-X101 and S3D-HM in the following.
Another discrepancy among different methods is the number of self-attention layers used in the model. In [60], the authors use 12 multi-modal self-attention layers while 6 video encoder layers and 2 multi-modal fusion layers are used in [33]. Differently, 4 multi-modal self-attention layers are used in [14]. In this paper, for all our ablation studies below, we use 1 and 2 self-attention layers for our video encoder and multi-modal fusion, respectively. To compare with previous work on speciÔ¨Åc dataset, we use 2 video encoding layers. While pretraining the model with largescale dataset Howto100M [35], we increase to 4 video encoding layers for comparable model capacity to previous works [60, 33, 14]. Note that this largest model is still smaller than or on par with the aforementioned methods.
4.3. Implementation details
For YouCook2 and MSR-VTT, the maximum number of video and text tokens are set to 48 and 30, respectively. For paragraph-video retrieval on ActivityNet, we set them both to 256. The 2D R-152 feature is extracted for one frame per second, and then globally pooled to 2048-d. For 3D CNN features, we follow [35] to sample video frames at 24 fps and extract an I3D-X101 feature every 16 frames. This results in 1.5 2048-d feature per second. For Eq. 3 and 4, we set the temperatures œÑ1 and œÑ2 both equal to 1. Training on separate datasets. In this setting, we train models from scratch using the training set provided in YouCook2, MSR-VTT and ActivityNet separately. We train

5

YouCook2

MSR-VTT (split1)

Video Representation

R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R10‚Üë MR‚Üì

R-152, Baseline R-152, Ours

4.1 13.2 19.4 81.0 16.4 42.6 55.8 8.0 4.6 14.1 20.4 71.0 18.9 46.2 58.8 7.0

I3D-X101, Baseline I3D-X101, Ours

2.1 8.1 12.7 125.0 14.7 40.83 53.2 9.0 2.6 8.9 13.2 115.0 20.6 44.0 56.9 7.0

R-152+I3D-X101, Baseline 4.2 13.5 20.0 75.0 16.6 45.4 58.5 7.0 R-152+I3D-X101, Ours 4.7 14.3 21.9 68.0 23.1 50.5 64.0 5.0

S3D-HM, Baseline S3D-HM, Ours

13.8 37.2 51.1 10.0 18.7 47.2 62.2 6.0 16.1 40.3 52.2 9.0 23.9 51.4 65.0 5.0

R-152+S3D-HM, Baseline 13.3 35.8 48.9 11.0 21.4 48.1 61.5 6.0

R-152+S3D-HM, Ours

15.8 39.8 52.4 10.0 24.5 52.8 65.5 5.0

Table 2: Text-video retrieval performance on YouCook2 and MSR-VTT with different feature types. S3D pretrained on HowTo100M outperforms others with large margin.

the model for 30k iterations with batch size 128. For each training sample, we use our cascade sampling strategy to sample 8 hard negatives. We use Adam [24] as the optimizer with initial learning rate 1e‚àí4. A linear learning rate decay is applied after 5k warm-up iterations. The weight decay is set to 1e‚àí5. Pretraining and Ô¨Ånetuning. We pretrain our model on Howto100M [35]. Since the original annotated video clips in Howto100M are usually short with a few seconds, we merge the adjacent clips so that the resulted text has at least 10 words. We use Adam [24] as the optimizer with initial learning rate 1e‚àí4. We train the model for 500k iterations with batch size 64, and also sample 8 hard negatives for each sample using our cascade sampling strategy. After pretraining, we Ô¨Ånetune the pretrained models on different datasets using the same setting as above except for a lower initial learning rate 2e‚àí5 and less Ô¨Ånetuning iterations 20k. Evaluation metrics. For text-video retrieval, we use Recalls at different points (Recall@n or Rn, with n as a speciÔ¨Åc number) and Median Rank (MR) as the metrics following previous works [60, 33]. In all tables, we use ‚Üë or ‚Üì to indicate higher or lower is better, respectively.
5. Results
We Ô¨Årst evaluate text-video retrieval performance and then study whether the learned representations can be transferred to other tasks on CrossTask and COIN.
5.1. Text-video retrieval
5.1.1 Comparing with baselines
We Ô¨Årst show the comparisons with baselines to inspect the effects of different components in our model. Video representations. We train our model with different video representations as described above and compare it with the baseline model which has identical architecture but merely trained with L3 as depicted in Eq. 5. The baseline

YouCook2

MSR-VTT (split1)

Losses

Cascade R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R10‚Üë MR‚Üì

L1

n/a

L3

n/a

L1 + L3



L1 + L3



L1 + L2 + L3 

14.1 35.7 48.8 11.0 22.9 49.7 61.7 6.0 13.3 35.8 48.9 11.0 21.4 48.1 61.5 6.0 13.9 37.4 50.7 10.0 22.5 50.8 64.1 5.0 15.0 38.7 51.3 10.0 23.7 51.3 63.9 5.0 15.8 39.8 52.4 10.0 24.5 52.8 65.5 5.0

Table 3: Text-video retrieval performance with different technique ensembles. It shows that using our proposed two techniques produce best results. All experiments use R152+S3D-HM video features.

YouCook2

MSR-VTT (split1)

Token of Interest R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R10‚Üë MR‚Üì

None det+adp noun verb noun+verb

15.0 38.7 51.3 10.0 23.7 51.3 63.9 5.0 14.7 38.5 51.2 10.0 23.3 51.0 63.5 5.0 15.4 39.3 51.8 10.0 24.0 51.8 65.1 5.0 15.3 39.0 51.4 10.0 23.9 52.1 64.8 5.0 15.8 39.8 52.4 10.0 24.5 52.8 65.5 5.0

Table 4: Text-video retrieval performance with different tokens of interest for computing token-level contrastive loss. ‚Äúdet‚Äù means determiner; ‚Äúadp‚Äù means adposition. We use the same video features as in Table 3.

contrastive learning method has been adopted in a number of previous works [60, 33]. This comparison can verify the effectiveness of our proposed contrastive learning method considering two models have exactly the same number of parameters. In Table 2, we can see our proposed method outperforms baseline across all feature types introduced in Sec. 4.2 on both YouCook2 and MSR-VTT. Note that our model uses exactly the same number of parameters to the baseline model. These consistent improvements demonstrate the effectiveness and generalization ability of our proposed method. As mentioned above, we also observe the text-video retrieval performance signiÔ¨Åcantly depends on the feature types. We can Ô¨Ånd 3D features (I3D-X101 and S3D-HM) in general outperform 2D feature (R-152), which is expected since 2D feature does not capture the motions in the videos. Among all three feature types, S3D-HM outperforms the other two with large margin, which demonstrates the potential to learn good video representation by pretraining on large-scale noisy dataset (Howto100M [35]). Because Howto100M mainly contains instructional videos, it is more close to YouCook2 than MSR-VTT, and hence we see more gain on YouCook2. These comparisons indicate video representations matter much to the Ô¨Ånal performance. Component Analysis. In our method, we combine L1, L2, and L3 during training and inference. Here, we study how they perform separately and contribute to the Ô¨Ånal performance. In Table 2, we use R-152+S3D-HM as the video feature and report the results with different loss combina-

6

Model

Lang. Video

YouCook2 R1‚Üë R5‚Üë R10‚Üë MR‚Üì

Random

‚Äì

‚Äì

0.0 0.2 0.3 1675

TVJE [35]

w2v R-152+I3D-X101 4.2 13.7 21.5 65

UniVL(v1) [33] BERT R-152+I3D-X101 3.4 10.8 17.8 76

TACo (Ours) BERT R-152+I3D-X101 4.9 14.7 21.7 63

UniVL(v3) [33] BERT S3D-HM TACo (Ours) BERT S3D-HM

7.7 23.9 34.7 21 16.6 40.3 53.1 9.0

Table 5: Comparing text-video retrieval on YouCook2.

Model

Lang. Video

MSR-VTT R1‚Üë R5‚Üë R10‚Üë MR‚Üì

Random JSFusion [53] JPoSE [49] TVJE [35] UniVL(v1)‚àó [33] TACo (Ours)

‚Äì BiLSTM w2v w2v BERT BERT

‚Äì R-152 TSN+Flow R-152+I-101 R-152+I-101 R-152+I-101

0.1 0.5 1.0 500.0 10.2 31.2 43.2 13.0 14.3 38.1 53.0 9.0 12.1 35.0 48.0 12.0 14.6 39.0 52.6 10.0 19.2 44.7 57.2 7.0

CE [31] MMT [14] TACo (Ours)

GPT BERT BERT

Collaborative Experts 20.9 48.8 62.4 6.0

Collaborative Experts 24.6 54.0 67.1 4.0

R-152+S3D-HM

26.7 54.5 68.2 4.0

Table 6: Comparing text-video retrieval on MSR-VTT. The upper block and bottom block use split2 and split1, respectively. We report them separately for fair comparison.

Model

Lang. Video

ActivityNet R1‚Üë R5‚Üë R10‚Üë MR‚Üì

Random

-

-

0.02 0.1 1.02 2458

DenseCap [25] LSTM C3D

14.0 32.0 65.0 34

FSE [56]

GRU C3D+TSN-Inception 18.2 44.8 89.1 7.0

CE [31]

GPT Collaborative Experts 18.2 47.7 91.4 6.0

MMT [14] BERT Collaborative Experts 22.7 54.2 93.2 5.0

TACo (Ours) BERT R-152+S3D-HM

25.8 56.3 93.8 4.0

Table 7: Comparing text-video retrieval on ActivityNet.

tions. As we can see, solely using L1 (row 1) or L2 (row 2) for contrastive learning results in sub-optimal video-text alignment. Simply combining them together (row 3) improves the performance on two datasets. This implies that different levels of contrastive learning can be complementary to each other, which supports our earlier hypothesis that these two losses are synergistic with each other for a better video-text alignment. When incorporating the hard negative mining via our cascade sampling strategy (row 4), it further improves the performance. Finally, we can see adding token-level contrastive loss L3 can further improve the performance across all settings (row 5).
Tokens of Interest. We further study the effect of different tokens of interest on the model performance. By default, our model uses the noun and verb as the tokens of interest to compute the token-level contrast loss. Here, we vary them to other types such as adposition (adp) and determiner (det) for investigation. In Table 4, we replace ‚Äúnoun+verb‚Äù

Figure 2: Zero-shot performance on YouCook2 and MSRVTT for different settings. score-1-5 correspond to the Ô¨Åve settings in Table 3 from top to bottom.
with ‚Äúdet+adp‚Äù, ‚Äúnoun‚Äù and ‚Äúverb‚Äù and report the numbers on two text-video retrieval datasets. As we can see, using ‚Äúdet+adp‚Äù as the target tokens is worse than the baseline without any token-level contrastive loss. ‚Äúnoun‚Äù and ‚Äúverb‚Äù can both improve the performance while ‚Äúnoun‚Äù is slightly better than ‚Äúverb‚Äù. Finally, combining noun and verb together achieves the best performance. These results align with our intuition to use nouns and verbs as the target token for Ô¨Åne-grained alignment between texts and videos considering they are usually grounded to video contents.
5.1.2 Comparing with state-of-the-art
We compare with previous works under three protocols: 1) training and evaluating on separate datasets; 2) pretraining on Howto100M and evaluating zero-shot performance and 3) Ô¨Ånetuning pretrained model on separate datasets. Results on separate datasets. We separately show the comparisons on YouCook2, MSR-VTT and ActivityNet in Table 5, 6 and 7. For a fair comparison with previous works, we use the same or similar features as listed in the tables. As we can see, our method outperforms all previous work across all datasets. These results validates its effectiveness to learn video-text alignment. Note that previous works either use a variety of loss functions [33, 28] or a collection of multiple features [31, 14]. In contrast, we achieve the best performance using a simpler contrastive learning pipeline with smaller model size. This supports our earlier claim on the efÔ¨Åciency. Comparing the numbers in Table 2, Table 5 and Table 6, we can Ô¨Ånd our model achieves better performance with the same video features when using deeper video encoder (2 layers v.s. 1 layer).

7

Finetuned Zero-shot

Model

Video

YouCook2

MSR-VTT

ActivityNet

R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R50‚Üë MR‚Üì

TJVE [35] ActBERT [60] MIL-NCE [34] TACo (Ours)

R-152+I-101 O-101+ R(2+1)D S3D-HM S3D-HM

6.1 17.3 24.8 46.0 7.5 21.2 29.6 38.0 ‚Äì ‚Äì ‚Äì ‚Äì 9.6 26.7 38.0 19.0 8.6 23.4 33.1 36.0 ‚Äì ‚Äì ‚Äì ‚Äì 15.1 38.0 51.2 10.0 9.9 24.0 32.4 29.5 ‚Äì ‚Äì ‚Äì ‚Äì 19.9 43.2 55.7 8.0 9.8 25.0 33.4 29.0 ‚Äì ‚Äì ‚Äì ‚Äì

TJVE [35]

R-152+I3D-X101

UniVL(v3) [33] S3D-HM

TACo (Ours) S3D-HM

8.2 24.5 35.3 24.0 14.9 40.2 52.8 9.0 ‚Äì ‚Äì ‚Äì ‚Äì 28.9 57.6 70.0 4.0 21.2 49.6 63.1 6.0 ‚Äì ‚Äì ‚Äì ‚Äì 29.6 59.7 72.7 4.0 24.8 52.1 64.0 5.0 28.3 56.8 92.6 4.0

MMT [14] TACo (Ours)

Collaborative Experts ‚Äì ‚Äì ‚Äì ‚Äì 26.6 57.1 69.6 4.0 28.7 61.4 94.5 3.3 R-152+S3D-HM 27.3 56.5 68.8 4.0 28.4 57.8 71.2 4.0 30.4 61.2 93.4 3.0

Table 8: A complete comparison of TACo under zero-shot and Ô¨Ånetuning evaluation protocols. Note that the zero-shot and upper part of Ô¨Ånetuned performance for MSRVTT is on split2, while the bottom is on split1 for fair comparison.

Method
Alayrac et al. [1] Zhukov et al. [61] Supervised [61] NN-Viterbi [39] CBT [40] TVJE [35] MIL-NCE [34] ActBert [60] UniVL(v3) [33]
TACo (Ours)

CrossTask
13.3 22.4 31.6
‚Äì ‚Äì 33.6 40.5 41.4 42.0
42.5

COIN
‚Äì ‚Äì ‚Äì 21.2 53.9 ‚Äì 61.0 57.0 70.0
68.4

Table 9: Action step localization on CrossTask (avg. recall) and action segmentation on COIN (acc.).

Zero-shot and Ô¨Ånetuned performance. In Table 8, we show the comparisons across different models pretrained on Howto100M. In the upper part of the table, we compare the zero-shot performance on YouCook2 and MSR-VTT. We do not evaluate on ActivityNet since it has different number of input video tokens compared with the pretrained model and thus is not directly compatible to the pretrained model. As we can see, TACo outperforms previous works signiÔ¨Åcantly on YouCook2 and slightly on MSR-VTT. Since YouCook2 has closer domain gap to Howto100M than MSR-VTT, the improvement brought by large-scale pretraining is more signiÔ¨Åcant. However, on MSR-VTT, our model still outperforms MIL-NCE [34] which uses the same video features. In Fig. 2, we show the zero-shot performance on YouCook2 and MSR-VTT when pretraining our models with different contrastive losses as listed in Table 3. Accordingly, it shows our proposed contrastive losses gradually improve the performance, and combining all techniques achieves the best performance. Based on the pretrained model, we further Ô¨Ånetune it on speciÔ¨Åc datasets. In our experiments, we use two feature S3D-HM and R-152+S3D-HM, to compare with the methods with the same/similar settings. As we can see, our model using S3D-HM outperforms UniVL [33] using the same feature but more video encoder layers (6). Different from zero-shot results, we observe more improvement on MSR-VTT than YouCook2 after Ô¨Ånetuning. This implies that Ô¨Ånetuning on speciÔ¨Åc datasets can compensate the domain gap to the pretraining datasets. To compare with the methods using features extracted from collaborative experts [14], we enrich our video representation by adding 2D R-152 feature, which achieves better performance on MSR-VTT, and better Recall@1 and Median Rank on ActivityNet. Note that this combination hurts the performance on YouCook2, and we witnessed a similar trend for models without pretraining in Table 2. Finally, comparing with the results without pretraining in Table 5, 6 and 7, we clearly Ô¨Ånd large-scale pretraining and Ô¨Ånetuning brings substantial improvements consistently.

5.2. Other video-related tasks
Following [35, 60, 33], we evaluate action step localization performance on CrossTask dataset [61]. It covers 18 tasks and each video contains multiple video segments annotated with action steps and natural language descriptions. Similar to [35, 60, 33], we use our model to compute the similarity between each frame and the action step descriptions, which results in a score matrix. Using the ofÔ¨Åcial algorithm provided by [61], we can Ô¨Ånd the optimal framewise order of action steps for a video. By comparing it with the ground-truth annotations, we compute the recall for each task and then do the average. According to the results in Table 9, our model achieves the best performance compared with previous works. This indicates that our model can learn good video-language representations.
We further evaluate our pretrained model on action segmentation task on COIN dataset, following [34, 60]. Unlike the above task, action segmentation does not rely on texts, and thus can be used to evaluate the learned video representation. As shown in Table 9, our method signiÔ¨Åcantly outperforms MIL-NCE and ActBert, and achieves comparable performance to UniVL. This indicates that our model is also a good video representation learner.
6. Conclusion
In this paper, we introduced TACo, a simple yet effective contrastive learning method for learning video-text alignment. It is aimed at addressing two existing issues in current contrastive learning pipelines: missing Ô¨Ånegrained alignment and inefÔ¨Åcient sampling for multi-modal fusion. Without introducing any extra parameters, our method achieved promising results on three text-video retrieval benchmarks under various evaluation protocols. We further demonstrated the learned representations can be effectively transferred to other tasks such as action step localization and segmentation. Based on all these encouraging results, we believe TACo is a good alternative to conventional contrastive learning pipeline.

8

References
[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsupervised learning from narrated instruction videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4575‚Äì4583, 2016. 8
[2] Srikar Appalaraju and Vineet Chaoji. Image similarity using deep cnn and curriculum learning. arXiv preprint arXiv:1709.08761, 2017. 2
[3] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. 1
[4] Joao Carreira and Andrew Zisserman. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 05 2017. 3
[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299‚Äì6308, 2017. 5
[6] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10638‚Äì10647, 2020. 2
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. 3
[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, pages 104‚Äì120. Springer, 2020. 2
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255. Ieee, 2009. 5
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 10 2019. 1, 2, 3, 5
[11] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding for zeroexample video retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9346‚Äì9355, 2019. 2
[12] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961‚Äì970, 2015. 2

[13] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017. 2
[14] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In European Conference on Computer Vision (ECCV), volume 5. Springer, 2020. 1, 2, 3, 5, 7, 8, 12
[15] Michael Gutmann and Aapo Hyva¬®rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 297‚Äì304, 2010. 3
[16] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6546‚Äì 6555, 2018. 5
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729‚Äì9738, 2020. 3
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 12 2015. 3, 5
[19] Sepp Hochreiter and Ju¬®rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997. 5
[20] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017. 11
[21] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758‚Äì2766, 2017. 2
[22] Karen Spa¬®rck Jones. A statistical interpretation of term speciÔ¨Åcity and its application in retrieval. Journal of Documentation, 28:11‚Äì21, 1972. 4
[23] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The Kinetics Human Action Video Dataset. arXiv:1705.06950, 05 2017. 5
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6
[25] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706‚Äì715, 2017. 5, 7, 11
[26] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching. In Proceedings of the European Conference on Computer Vision (ECCV), pages 201‚Äì216, 2018. 2
[27] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. 1, 2

9

[28] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200, 2020. 1, 2, 3, 7
[29] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Objectsemantics aligned pre-training for vision-language tasks. arXiv preprint arXiv:2004.06165, 2020. 2
[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla¬¥r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740‚Äì755. Springer, 2014. 2
[31] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019. 2, 5, 7
[32] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In 11 pages, 5 Ô¨Ågures, 08 2019. 2
[33] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou. Univilm: A uniÔ¨Åed video and language pre-training model for multimodal understanding and generation. arXiv:2002.06353, 2020. 1, 2, 3, 5, 6, 7, 8, 12
[34] A. Miech, J. B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9876‚Äì9886, June 2020. 1, 2, 3, 5, 8
[35] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV, 06 2019. 1, 2, 5, 6, 7, 8
[36] Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations in vector space. In International Conference on Learning Representations, 2013. 5
[37] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012. 3
[38] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. 5
[39] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juergen Gall. Neuralnetwork-viterbi: A framework for weakly supervised video learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7386‚Äì7395, 2018. 8
[40] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743, 2019. 1, 5, 8

[41] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid. Videobert: A joint model for video and language representation learning. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 7463‚Äì7472, 2019. 1, 2, 5
[42] Hao Tan and Mohit Bansal. LXMERT: Learning CrossModality Encoder Representations from Transformers. In EMNLP, 08 2019. 2
[43] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1207‚Äì1216, 2019. 1, 2, 5
[44] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through questionanswering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631‚Äì4640, 2016. 1
[45] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning language-visual embedding for movie understanding with natural-language. arXiv preprint arXiv:1609.08124, 2016. 2
[46] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450‚Äì6459, 2018. 5
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 06 2017. 3
[48] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re¬¥mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface‚Äôs transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019. 3
[49] Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen. Fine-grained action retrieval through multiple partsof-speech embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pages 450‚Äì459, 2019. 2, 7
[50] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma. UniÔ¨Åed visual-semantic embeddings: Bridging vision and language with structured meaning representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6609‚Äì6618, 2019. 2
[51] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classiÔ¨Åcation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 305‚Äì321, 2018. 3, 5
[52] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language.

10

In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5288‚Äì5296. IEEE, 2016. 1, 2, 5, 11 [53] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471‚Äì487, 2018. 2, 5, 7 [54] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. Video captioning and retrieval models with semantic attention. arXiv preprint arXiv:1610.02947, 6(7), 2016. 2 [55] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. End-to-end concept word detection for video captioning, retrieval, and question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3165‚Äì3173, 2017. 2 [56] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision (ECCV), pages 374‚Äì390, 2018. 2, 5, 7 [57] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. UniÔ¨Åed vision-language pretraining for image captioning and vqa. In Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelligence, 2019. 2 [58] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In AAAI 2018, 2017. 1, 2, 3, 5, 11 [59] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739‚Äì 8748, 2018. 2 [60] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, 2020. 1, 2, 3, 4, 5, 6, 8, 12 [61] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Crosstask weakly supervised learning from instructional videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3537‚Äì3545, 2019. 1, 2, 5, 8
A. Tokens of interest

Dataset

Noun Verb All

YouCook2 [58] MSR-VTT [52] ActivityNet [25]

378 4,415 2,602

168 1,463 1,021

2,144 15,740 9,059

Table 10: Token statistics for each dataset.

We extract tokens of interest (T.O.I) using the pos-tagger provided by Spacy [20]. In Table 10, we show the statistics of tokens for three datasets. For each token that is tagged at VERB or NOUN, we compute the inverse document frequency (idf) by:

idf (token) = log |D| (7) 1 + |{d ‚àà D : token ‚àà d}|

where D is the full set of corpus, which are the captions in the training set for a dataset; the denominator counts the number of captions which contain a speciÔ¨Åc token. Based on Eq. 7, we can compute the idf for each token of interest. The smaller the idf, the more frequent it appears in the corpus. We do not compute the tf term since usually a token only appears once in a single sentence. The full list of tokens and corresponding idfs can be found in Fig. 4. For a given sentence, we Ô¨Årst assign the computed idfs to its nouns and verbs and then normalize the idfs, which are then used to weigh the token-level contrastive losses.
B. Contribution of three contrastive losses

Loss

R@1 R@5 R@10 MR

Early stage only 14.1 35.7 48.8 11.0 Later stage only 13.3 35.8 48.9 11.0

Early stage Token-level Later stage Fused

15.3 39.3 51.9 10.0 15.0 39.5 51.4 11.0 14.3 38.4 50.6 11.0 15.8 39.8 52.4 10.0

Table 11: Text-video retrieval performance using separate alignment scores on YouCook2.

In this part, we investigate the contributions of three contrastive losses used in our model. After we train the videotext alignment model using all three losses, we report the performance using separate alignment scores in Table 11. For reference, the top two rows are the performance for using early stage only and later stage only contrastive learning to train the model. The bottom four rows are the separate performance at different stages for our model. As we can see, combining three contrastive losses during training can boost the performance for both early and later stage (row 3 v.s. row 1, row 5 v.s. row 2). This indicates that the three losses are synergistic to each other for a better videotext alignment. On the other hand, the early stage alignment achieves better performance than other two (token-level and later stage), while the fused score is the best. We suspect that this is because early stage alignment is trained with all text-video pairs at sentence-level. In contrast, token-level contrast focuses on single tokens and the multi-modal fusion layers merely see a small part of hard text-video pairs.

C. Effect of cascade sampling
The proposed cascade sampling helps the later stage contrastive learning to focus on hard negative samples. As shown in our main submission, adding cascade sampling will improve the performance. We suspect this is because cascade sampling helps learn a better later stage alignment. To verify this, we compare the later stage alignment across three different settings: 1) merely applying later stage contrastive loss; 2) combine early state and later stage con-

11

trastive losses and 3) using cascade sampling for later stage contrastive loss. We report the results on YouCook2 in Table 12. Here, note that we only use the later stage alignment scores for evaluating the performance. As we can see, combining early stage and later stage together slightly improves the performance. This is probably because early stage contrastive loss helps to learn a better video and language encoder, from which the multi-modal module takes better representations for cross-modal fusion. After applying the cascade sampling for the later stage contrastive loss, the performance is further improved. Since our cascade sampling strategy can send more difÔ¨Åcult samples to the later stage, the cross-modal fusion layers can learn more discriminative representations for video-text alignment. These results validate that the hard negative mining through cascade sampling indeed helps to improve the later-stage text-video alignment, and hence the Ô¨Ånal performance.

Setting

R@1 R@5 R@10 MR

Later stage only

13.3 35.7 48.8 11.0

Early stage + Later stage 13.6 35.9 49.1 11.0

Cascade sampling

14.5 38.3 50.7 11.0

Table 12: Text-video retrieval performance on YouCook2 only using later stage alignment score for different settings.

D. Effect of video encoder layers
In our main paper, we noticed the number of video encoder layers affects the Ô¨Ånal performance. To have a more comprehensive study, we use R-152 and S3D-HM as the 2D and 3D features and train the video-text alignment model on YouCook2 with different video encoder layers. As shown in Table 13, using more video encoder layers can signiÔ¨Åcantly boost the text-video retrieval performance. Particularly, when no video encoder layers are used, the model can hardy capture the long-range temporal dynamics, and thus performs poorly. Once we add one video encoder layer, the performance improves signiÔ¨Åcantly. With the increase of encoder layers, the performance is further improved, which is reasonable since more video encoder layers can encode more complicated video contents and dynamics.

#video #params. FLOPs

YouCook2

enc. layers (M) (G) R@1 R@5 R@10 MR

0

126.5 3.86 14.0 35.7 49.5 11.0

1

133.6 4.11 15.8 39.8 52.4 10.0

2

140.7 4.45 15.9 40.5 53.8 9.0

4

154.9 5.14 16.4 40.5 54.3 9.0

Table 13: Text-video retrieval performance on YouCook2 with different video encoder layers using R-152+S3D-HM.

E. Comparing model size and FLOPs
Finally, we attempt to compare the model sizes and computational costs for different methods. Unfortunately, all previous methods did not report FLOPs and only MMT [14] discussed #params. However, the results in Table 13 imply that bigger model can usually achieve better performance. Therefore, it is necessary to have a comparison of model size and computational cost between our model and those from other methods. For other methods which do not report the numbers, we estimate them based on the descriptions in the original paper. Table 14 summarizes the comparisons and also reports the #params and FLOPs (all underlined numbers are estimated based on the descriptions in original papers). As shown, our largest model has comparable size and FLOPs to others.

method

text video mm #params. FLOPs self cross (M) (G)

ActBert [60] 12 12 MMT [14] 12 4 UniVL [33] 12 6 Our largest 12 4

0 24 00 20 20

369.1 133.3 169.0 154.9

13.80 4.63 5.82 5.14

Table 14: Comparison of model size and FLOPs. ‚Äúmm‚Äù means multi-modal fusion, and ‚Äúself‚Äù means self-attention layers while ‚Äúcross‚Äù means cross-modal attention.

F. Visualizations
We visualize the text-video retrieval results by varying the weights for the token-level alignment scores during testing. In Fig. 3, we show two text-video retrieval examples on YouCook (top) and MSR-VTT (bottom). From top to bottom, the Ô¨Åve rows in each block correspond to the top Ô¨Åve retrieved results from the whole test set. As we can see, when we gradually increase the weight for the tokenlevel alignment score, there are more related videos appearing in the top Ô¨Åve candidates. For YouCook2, when we set the weight equal to 0.0, the third and Ô¨Åfth video are not well-aligned with the query since they are both not about ‚Äútomato‚Äù. When we increase the weight to 0.1, we can observe the the fourth video moves to the third place. After we increase the weight to 0.5, we can see all top-5 videos are about cutting tomato. Similarly, for MSR-VTT, we can see the last three videos are not about ‚Äútwo people talking on a table‚Äù. When we increase the weight to 0.1, the Ô¨Åfth video is replaced with a more matched video. Keeping increase the weight to 0.5, we can obtain the top 5 videos all about ‚Äútwo people talking with each other on a table‚Äù. These visualizations demonstrate the efÔ¨Åcacy of our proposed token-level contrastive learning.

12

Query: cut the tomato and put it inside a bowl

cut a tomato into quarters remove the seeds chop finely and add to the bowl

cut a tomato into quarters remove the seeds chop finely and add to the bowl

cut a tomato into quarters remove the seeds chop finely and add to the bowl

chop a tomato into thin slices

chop a tomato into thin slices

chop a tomato into thin slices

chop some red onions red pepper and green pepper into square pieces

chop up the tomatoes

chop up the tomatoes

chop up the tomatoes

chop some red onions red pepper and green pepper into square pieces

slice tomatoes into thin slices

cut the pepperoni in half

cut the pepperoni in half

Weight=0.0

Weight=0.1

Query: two people are talking with each other on a table

cut tomatoes and place them in a bowl
Weight=0.5

a man and a woman trying some sake

a man and a woman trying some sake

there is a woman is talking with two guys

there is a woman is talking with two guys

there is a woman is talking with two guys

a man and another man speak to each other in a room

leonardo dicaprio is portrayed as two different characters in this film

leonardo dicaprio is portrayed as two different characters in this film

a man and a woman trying some sake

a girl in a studio singing

a girl in a studio singing

two men talking about investors on a show

a cartoon man plays a card game with his friends

two men talking about investors on a show

a man and woman arguing about fake arms used in a performance

Weight=0.0

Weight=0.1

Weight=0.5

Figure 3: Text-video retrieval results given a query on YouCook2 (top) and MSR-VTT (bottom). In each block, we show top 5 ranked videos from top to bottom. From left to right, we gradually increase the token-level alignment weight from 0.0 to 0.1 and then 0.5 (default in our main experiments). The change of the top 5 results demonstrate the beneÔ¨Åt of token-level contrast when performing text-video retrieval. Below each video (depicted by three side-by-side frames), we show the associated descriptions provided in the original dataset. Better viewed by enlarging the Ô¨Ågure.

13

Figure 4: Token inverse document frequency (IDF) for noun and verb in YouCook2 and MSR-VTT. For clarity, we evenly sample the tokens and show their IDFs. From left to right, the noun/verb becomes more and more frequent gradually.
14

