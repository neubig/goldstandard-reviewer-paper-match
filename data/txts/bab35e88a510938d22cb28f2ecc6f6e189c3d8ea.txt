Arabic Speech Recognition by End-to-End, Modular Systems and Human
Amir Husseina,b,, Shinji Watanabec, Ahmed Alia
aHBKU, Qatar Computing Research Institute, Doha, Qatar bKanari AI, Pasadena, California cCarnegie Mellon University

arXiv:2101.08454v2 [eess.AS] 29 Jun 2021

Abstract
Recent advances in automatic speech recognition (ASR) have achieved accuracy levels comparable to human transcribers, which led researchers to debate if the machine has reached human performance. Previous work focused on the English language and modular hidden Markov model-deep neural network (HMM-DNN) systems. In this paper, we perform a comprehensive benchmarking for end-toend transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the Arabic language and its dialects. For the HSR, we evaluate linguist performance and lay-native speaker performance on a new dataset collected as a part of this study. For ASR the end-to-end work led to 12.5%, 27.5% , 33.8% WER; a new performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our results suggest that human performance in the Arabic language is still considerably better than the machine with an absolute WER gap of 3.5% on average. Keywords: Dialectal Arabic, End-to-end speech recognition, Human speech recognition, Modern Standard Arabic, Transformer

1. Introduction
Automatic Speech Recognition has shown fast progress recently, thanks to advancements in Deep Neural Network (DNN) which has brought remarkable

Email addresses: amir@kanari.ai (Amir Hussein), shinji@ieee.org (Shinji Watanabe), amali@hbku.edu.qa (Ahmed Ali)

Preprint submitted to Journal of Computer Speech and Language

June 30, 2021

improvements in reaching human-level performance [1]. Traditional ASR systems employ a modular design, with diﬀerent modules for acoustic modeling, pronunciation lexicon, and language modeling that are trained separately. More recently, end-to-end (E2E) models that are trained to convert acoustic features to text transcriptions directly, potentially optimizing all parts for the end task [2]. With the performance of ASR systems reaching closer to that of a human, several eﬀorts embarked to benchmark the performance of state-of-the-art ASR systems against professional transcribers [3, 4]. In [1] the researches showed that E2E ASR can achieve competitive performance in simple speech recognition tasks like reading newspaper. After that, a study by Microsoft [3], suggested that the ASR systems have already reached the level of the professional human transcriber in more diﬃcult ASR tasks like conversational speech. On the other hand, a study by IBM [4] suggested that human parity in conversational speech is still considerably better. The aforementioned studies were conducted on English language and were not explored in morphologically complex languages like Arabic. The Arabic language is the largest Semitic language with a high degree of aﬃxations and derivations, which result in a huge increase in the number of word forms. Remarkably, the 400 million speakers (estimated in 2020) Arabic native speakers use Dialectal Arabic (DA) as their way of communication in the day-to-day speech. DA does not have standard orthographic rules. It can be argued that a language is a dialect with an army and navy [5]. If we take this perspective into consideration, we can describe the diﬀerent Arabic dialects as diﬀerent languages. However, Arabs in general perceive dialects as a deterioration from the classical Arabic, almost using all the same Arabic letters. An objective comparison of the varieties of Arabic dialects could potentially lead to the conclusion that Arabic dialects are historically related, but not synchronically, and are mutually unintelligible languages like English and Dutch. This makes Arabic an excellent choice to highlight the challenges of speech recognition in the wild.
Till today, the state-of-the-art ASR in the Arabic language comes from modular Hidden Markov Model Deep Neural Network (HMM-DNN) systems. Lately,
2

the best ASR results on the Modern Standard Arabic (MSA) data were reported by the Aalto University team [6], with WER of 13.2% on MGB2 test set using a combination of over 30 system. There are three major challenges when developing speech recognition models for the Arabic language:
• Arabic is a consonantal language with most of the available text is nondiacritized. As a result, it is challenging to determine the location of the vowels, which can convey diﬀerent meanings.
• Existence of diﬀerent Arabic dialects with limited labeled data. Each dialect is a native Arabic language that is spoken, but not written, as it does not have standardized orthographic rules.
• Arabic morphological complexity with a high degree of aﬃxation and derivation that makes it challenging to estimate probabilities for the language model and increases the out-of-vocabulary (OOV) rate.
Several attempts have been made to address each of the aforementioned challenges. To address the nondiacritized words ambiguity, researchers in [7] utilized sequence-to-sequence deep learning model, inspired by Neural Machine Translation, to restore the missing diacritics. The proposed approach achieved new state-of-the-art with word error rate of 4.49%. To address the challenge of limited dialect speech data, several transfer learning approaches were proposed [8, 9] that utilize similarity between MSA and dialectal Arabic (DA) speech. Finally, to deal with morphological complexity and OOV, the character-level language model (LM) was suggested by [10]. However, the major limitation with character level LM is that it is diﬃcult to capture the contexts of the word in a sentence. Furthermore, the previous approaches mainly used HMM-DNN models that have several limitations including model complexity (complicated to implement as HMM-DNN systems employ a modular design, with diﬀerent modules for acoustic modeling, pronunciation lexicon, and language modeling, which are trained separately) and requirement of linguistic resources. The core contribution of this study lies in a new comprehensive analysis comparing the
3

E2E transformer ASR, the modular HMM-DNN ASR, and the HSR. To avoid biases in our analysis, we collected a new evaluation set of 3 hours containing news reports and conversational speech with both MSA and DA. To better understand human performance, we hired expert linguists and educated native speakers to perform HSR task on the new hidden set. To the best of our knowledge, this is the ﬁrst work that compares head to head single E2E, modular, and HSR performance. The main question that we address in this work is whether there are major qualitative diﬀerences between the HSR and the state-of-theart machine results in the Arabic ASR as shown in Figure 1. In this work, we develop the ﬁrst E2E transformer ASR for the Arabic language. Furthermore, we provide the best practices for ﬁnetuning the transformer ASR for dialectal Arabic1. The proposed approach advances the Arabic ASR by addressing all the aforementioned HMM-DNN limitations, achieving signiﬁcant improvements over the previous state-of-the-art in both MSA and DA tasks. The main advantage of E2E transformer architecture is that it reduces word ambiguity by learning the context with a self-attention mechanism. In order to address the OOV eﬃciently, we use sub-word tokenization [11]. Furthermore, we benchmark the performance of a single E2E transformer model with the state-of-the-art single HMM-DNN approach [12] pointing out the advantages and disadvantages of each approach. Finally, the proposed eﬀectiveness of pretrained E2E transformer is evaluated on two dialectal datasets MGB3 and MGB5 using ﬁnetuning to highlight the challenges of Arabic ASR in the wild2.
In summary, the key contributions of our work include:
• A comprehensive assessment of the human performance on Arabic speech recognition system, analyzing the type of errors and correlation with the machine.
1The source code has been made publicly available on Espnet Github repository https://github.com/espnet/espnet/tree/master/egs/mgb2/asr1
2In case of acceptance, the code will be made publicly available as part of Espnet oﬃcial recipe
4

Figure 1: High-level illustration of the core study conducted in this paper.
• A new milestone of the Arabic speech recognition performance with E2E transformer architecture for MSA and DA tasks.
• As a part of HSR versus machine study, we provide a new hidden test set combining both MSA and DA to avoid being biased in our analysis, as previous test sets have been made public.
An additional contribution of our work is developing a voice activity detection (VAD) pipeline for the E2E transformer to address the problem of very long speech segments, which is a typical situation in a practical ASR setup. The developed pipeline combines the speech detection precision of InaSpeechSegmenter [13], with the maximum length of the segmentation feature of energy-based VAD[14]. Finally, we plan to provide a new benchmark, including the state-ofthe-art recipe, and pre-trained models and make them publicly available for the
5

community. The rest of the paper is organized as follows: In Section 2, we cover related
work. Section 3 describes the developed ASR models in this study. Section 4 describes the details of the experimental setup and the used datasets. Section 5 presents the results with comparison to the state-of-the-art approaches and human performance. Finally, Section 6 concludes the ﬁndings of our study and discusses future work.
2. Related Work
This section highlights prior work in ASR approaches, which can be grouped into modular HMM-DNN systems and E2E. In addition, we present a subsection about the studies that conducted comparisons between human and machine performance on speech transcription.
2.1. Hybrid HMM-DNN systems For a long time, the HMM with Gaussian Mixture Model (GMM) has been
considered the mainstream model for large vocabulary continuous speech recognition (LVCSR) achieving best recognition results. In [15], authors proposed the ﬁrst HMM-DNN hybrid approach, where GMM was replaced with the deep neural network model. It achieved signiﬁcant performance gains compared to the HMM-GMM legacy system in the LVCSR task. After that, several DNN architectures were explored for the acoustic modeling, including Recurrent Neural Networks (RNNs), Bidirectional RNNs (BDRNNs), and deep conditional random ﬁelds, which showed prominent performance improvement [16, 17]. In [18], the authors proposed a time delay neural network (TDNN) and showed better results in learning wider temporal dependencies compared to DNN and RNN based models. In [12], researchers targeted addressing MSA task using similar architecture to [18], combining three AM models trained with Lattice-free Maximum Mutual Information (LF-MMI) objective function. They combined TDNN, LSTM, and BLSTM using the Minimum Bayes Risk (MBR) decoding
6

criterion and achieved ﬁrst place in the MGB2 Arabic Broadcast Media Recognition challenge with a WER of 14.2%. After that, in MGB3 challenge [19] a team from Aalto University combined over 30 systems using MBR, including two acoustic models (TDNN-BLSTM) and a variety of language models (character, sub-word, and word-based) [6]. Aalto team achieved a WER of 13.2% on MGB2 and 37.5% on MGB3, which is to the best of our knowledge, the state-of-the-art results. One main advantage of the modular HMM-DNN over the current E2E transformer system is that HMM-DNN is streamable while E2E-transformer is not. On the other hand, the main disadvantage of the modular HMM-DNN system is its complexity consisting of various components, including the acoustic model, language model, and pronunciation model. Each component of the system is optimized independently with a diﬀerent objective function, which usually leads to a non optimal global solution. In addition, HMM-DNN systems require linguistic resources, such as handcrafted pronunciation dictionary that is subject to human error.
2.2. End to end ASR systems E2E deep learning models were introduced to simplify the complexity of
HMM-DNN modular models into a single deep network architecture and address the aforementioned limitations. The main issue with E2E sequence to sequence models was data alignment [20]. Several approaches were introduced to address the data alignment problem, including connectionist temporal classiﬁcation (CTC) based model [2] and attention-based sequence to sequence model [21, 22]. The E2E CTC approach uses Markov assumptions to eﬃciently solve sequential problems with dynamic programming. However, the main limitation of CTC is the assumption that the label outputs are conditionally independent of each other. On the other hand, the attention-based E2E model uses an attention mechanism to perform alignment between acoustic frames and label sequence without the independence assumption, yet it can result in nonsequential alignments. In [23], the authors proposed a multitask CTC/attention approach, which eﬀectively utilizes the advantages of both architectures in training and de-
7

coding. The proposed multitask CTC/attention approach is based on long short term memory (LSTM) sequence to sequence model. At this point it is important to note that CTC and attention losses are only surrogates for the actual performance measure which is word error rate, while HMM-DNN systems are trained with objective functions which are close approximations to the word error rate ( MMI or MBR). In addition, attention and CTC objectives are teacher-forcing based likelihood and do not consider beam search with an LM rescoring. On the other hand, MMI and MBR objectives try to optimize ASR beam search decoding (through lattice, n-best in the classical one or lattice-free MMI in the chain model). In the literature, we found only two works that are related to E2E models for Arabic speech data [10, 24]. In [10] the authors proposed an E2E model based on Bidirectional Recurrent Neural Networks (BRNN) with CTC. However, the results in [10] were only reported on the MGB2 developments set with no results on the test set. In [24], the researchers studied the learned internal representations in DeepSpeech2 E2E [1] on phonemes and graphemes classiﬁcation tasks. The transformer-based architecture was ﬁrst introduced as a neural machine translation system [25] to replace the recurrence with the self-attention mechanism. In [26, 27], authors showed superiority performance of transformer-based models compared to state-of-the-art recurrent networks in ASR task. Besides, the E2E transformer-based ASR showed close performance to the state-of-the-art HMM-DNN systems [28]. In this paper, we develop an E2E transformer-based ASR model with a multitask CTC/attention objective function for Arabic speech data.
2.3. Speech recognition by humans and machines Advances in ASR technology produced major improvements reaching the
range of human performance. In two papers by Microsoft [3, 29], authors suggested that the machine has already reached the human performance in English ASR. Microsoft reported that their improved ASR outperformed expert transcriber by 0.1% and 0.2% WER on Switchboard and CallHome datasets, respectively. In their study, an existing Microsoft transcription pipeline was
8

leveraged, in which transcription is conducted on a weekly basis. The transcription was conducted on NIST 2000 test set with two passes. In the ﬁrst pass, a transcriber works from scratch to transcribe the data and, in the second pass, a second listener monitors the data to do error correction. On the other hand, IBM [4] conducted an independent set of human performance measurements to verify the aforementioned claims by Microsoft. IBM found that human performance is considerably better outperforming their state-of-the-art ASR by 0.4% and 3.5% WER on Switchboard and CallHome datasets, respectively. Unlike the previous Microsoft setup, IBM experiment transcribers were aware of the experiment and were actively involved. Three independent transcribers were used for the ﬁrst pass in addition to quality control by a fourth senior transcriber. The ﬁnal performance was chosen based on the lowest transcriber word error rate (WER). Unlike previous studies where several systems were combined, in this paper, we are looking for a practical ASR usage where our focus is to get the best results from a single ASR system. Furthermore, in the present study, best practices of the two aforementioned studies are followed to set the most realistic and unbiased comparison between human and machine as described in Section 4.1.
3. ASR Models Description
3.1. Hybrid HMM-DNN The acoustic model for the modular system is trained using the 1,200 hours
MGB2 [30]. For language modeling, we use the 130M words crawled from the Aljazeera Arabic website from the period 2000 - 2011, as provided for the MGB2 challenge. LM experiments used a grapheme lexicon of 1.3M words. The grapheme-based lexicon has a 1:1 word-to-grapheme mapping, which means the vocabulary size is the same as the lexicon size. Acoustic modeling: In this study, we adopt the architecture proposed by [18], which consists of combining a time delay neural network (TDNN) with long short term memory (LSTM) layers and showed signiﬁcantly better results com-
9

pared to bidirectional LSTM (BLSTM) acoustic modeling. The TDNN-LSTM model consists of 5 hidden layers, each layer containing 1,024 hidden units. Neural networks are trained using lattice-free maximum mutual information (LFMMI) [31]. The input to the modular system is standard 13-dimensional cepstral mean-variance normalized (CMVN) Mel-Frequency Cepstral Coeﬃcients (MFCC) features without energy, and its ﬁrst and second derivatives. For each frame, we also include its neighboring ±4 frames and apply Linear Discriminative Analysis (LDA) transformation to project the concatenated frames to 40 dimensions, followed by Maximum Likelihood Linear Transform. Speaker adaptation is also applied with feature-space Maximum Likelihood Linear Regression (fMLLR). The GMM-HMM model has 100K Gaussians for 5K states. The parameters for language weight and silence penalty that provided the best results on the development set were found 0.8 and 0.0 respectively. Acoustic models are built using Kaldi ASR toolkit [32]. Language modeling: Two n-gram LMs are trained: a big four-gram LM (bLM4), trained using the spoken transcripts and the 130M words background text; and a smaller four-gram LM obtained by pruning bLM4 using pocolm3. The small LM is used for ﬁrst-pass acoustic decoding to generate lattices. These lattices are then rescored using the bLM4. In an attempt to be consistent with the E2E system, we explored using the time-restricted self-attention layer for the acoustic modeling as described in [33]. We also explored the sub-word modeling using 1K sub-words as it has been tuned for the HMM-DNN. Our results showed 1.1% relative reduction in WER on the MGB2 development set and 0.7% relative increase in WER on the MGB2 test set compared to word tokenization. As a result, we decided to drop the sub-word modeling and adopt the stateof-the-art TDNN-LSTM architecture with word tokenization. Throughout the paper we refer to this modular system as HMM-DNN.
3https://github.com/danpovey/pocolm
10

3.2. E2E transformer Transformer is a recent sequence to sequence model that completely replaced
the recurrence in traditional recurrent networks with self-attention mechanism and sinusoidal position information. In this paper, we utilize transformer-based architecture for Arabic ASR, as shown in Figure 2. On a very high level, the transformer consists of an encoder model with M repeating encoder blocks and a decoder model with N repeating decoder blocks. The encoder model mainly maps the input vector to a latent representation. The input to the encoder X is a sequence of 83-dimensional feature frames, 80-dimensional log Mel spectrogram with pitch features [34]. The decoder generates one prediction at a time in an auto-regressive fashion. At each time step, the input to the decoder model is the latent representation for the encoder model and previous decoder predictions.

3.3. Pre-encoder
First, the acoustic feature frames X are transformed into sub-sampled sequence Xs ∈ Rdsub×dmodel with 2D-CNN sampling layer. The dsub is the length of the output sequence and dmodel is the number of input feature dimensions to the Encoder.

3.4. Encoder

Each encoder block consists of sub-layers: a multi-head self-attention mecha-

nism and a position-wise fully connected network. The output of each sub-layer

is followed by a layer normalization [35] with a residual connection from the

sub-layer input [36]. The input to the ﬁrst encoder block is the sub-sampled

sequence input Xs. At the self-attention sub-layer, the Xs is transformed to queries Q = Xs ∗ Wq, keys K = Xs ∗ Wk, and values V = Xs ∗ Wv, where Wq and Wk ∈ Rdmodel×dk and Wv ∈ Rdmodel×dv are learnable weights. The dmodel is the output dimension from the previous attention layer, and dv, dk = dq are

the dimensions of values, keys and queries. After that, a normalized weighted

similarity Z from self-attention is obtained with a softmax as shown in Eq 1.

Q ∗ KT

Self Attention(Q, K, V) = sof tmax( √ ) ∗ V

(1)

dk

11

Figure 2: Illustration of E2E transformer-based ASR architecture. The input to the encoder during the training is a sequence of feature frames. The output of the encoder is fed as input to the decoder in addition to the masked target transcription. The output of the decoder is the prediction of the masked transcription
To deal with multiple attentions, multiple attention sub-layers are used in parallel, usually referred to as multi-head attention (MHA). The MHA is obtained by concatenating all of the self-attention heads at a particular layer.
MHA(Q, K, V) = [Z1, Z2, · · · , Zh] Wh (2)
Zi = SelfAttention(Qi, Ki, Vi) where h is the number of attention heads in a layer and i corresponds to the ith head in the layer. The output from MHA is normalized and then fed to the Feed Forward (FF) sub-layer connected network which is applied to each
12

position separately.

FF(z[t]) = max (0, z[t] ∗ W1 + b1) W2 + b2

(3)

where z[t] is the tth position of the input Z to the FF sub-layer.

3.5. Decoder
The decoder architecture is very similar to the encoder. However, in addition to MHA self-attention and fully connected sub-layers, it has a third masked self-attention layer. The masked self-attention in the decoder is allowing to attend only to earlier positions in the output sequence. The decoder prediction Yˆ [t] at each time step is conditionally dependent on the ﬁnal representation produced by the encoder He and the previous target sequence Y [1 : t − 1]. The conditional dependence is obtained with the multihead attention that calculates the attention between the encoder latent features and the previous decoded sequence. Similar to the encoder, the decoder has residual connections and layer normalization around each sub-layer.

3.6. Positional encoding Positional encoding is added to the input embeddings to reﬂect the positional
context of each word in the sentence. Transformers use sinusoidal positional encoding with diﬀerent frequencies as shown in Equation 4.

P E(n,2i) = sin n/100002i/dmodel (4)
P E(n,2i+1) = cos n/100002i/dmodel
where n is the position of a word in the sentence and i is a position along the
embedding vector dimension.

3.7. Transformer ASR training During training, the acoustic model predicts the posterior probability of the
transcription Y given acoustic features X. The total objective function of the acoustic model Lasr is a multi-task learning objective that combines the E2E

13

decoder loss Ld = −log[Pd(Y|X)] and the CTC loss Lctc = −log[Pctc(Y|X)]. This multi-objective function was proposed to improve model robustness and faster convergence [23].

Lasr = αLctc + (1 − α)Ld

(5)

where Pd are the probabilities predicted by transformer decoder, Pctc are the CTC probabilities, and α is a weighting factor that trades-oﬀ the two losses.

3.8. Transformer ASR inference During the inference mode, a language model (LM) is used to disambiguate
between hypothesised words generated by the decoder. In particular, two types of LM models are used in this paper: long short term memory (LSTM) and transformer-based language model (TLM). The LM model prediction is combined with E2E as shown in Eq. 6.

Yˆ = argmax{λLctc + (1 − λ)Ld + µLlm}

(6)

Y ∈Y ∗

where Llm = Plm(Y) is the language model prediction, Y∗ is a set of hypotheses

generated using beam search over all possible sequences, µ and λ are trade-

oﬀ factors. The audio segmentation during the inference in an experimental

setup is usually assumed to be prepared by an expert transcriber. In Section

5.6, we study the eﬀect of segment duration variability on E2E transformer

performance.

4. Experimental Setup
The proposed E2E transformer approach is benchmarked with state-of-theart approaches on MSA task with (MGB2) data and on dialectal task with (MGB3, MGB5) data. For MSA evaluation, the conventional word error rate (WER) is used. However, for dialectal data evaluation, the multi-reference word error rate (MR-WER) and averaged WER (AV-WER) are adopted from MGB3, MGB5 challenges [19, 37]. The MR-WER was proposed to evaluate
14

dialectal data, which does not have standardized orthography [38]. All models are implemented using Espnet toolkit [39]. We ran our experiments on an HPC node equipped with 4 NVIDIA Tesla V100 GPUs with 16 GB memory, and 20 cores of Xeon(R) E5-2690 CPUs.
4.1. Human transcription experiment Two professional linguist transcribers were hired independently and their
quality transcription was checked by a third senior transcriber with extensive experience in the linguistic annotation. In addition, another three educated native Arabic speakers (not linguists) were hired to transcribe the same data. The transcribers were not aware of the experiment conducted and they performed the transcription as a part of their daily transcription tasks. The same guidelines were provided to all transcribers to ensure the consistency and the quality of the transcription, which included guides for audio segmentation, truncated words, words from other languages, hesitation, etc. There were no restrictions on the number of times to listen to the speech. We found that, on average, transcribers needed to listen between 2-4 times for each sentence. As part of our study, 3 hours of Arabic speech data from Aljazzera news channel in the period from (June - August) 2020 was collected. The data included a variety of conversations, interview programs and reports by journalists from the ﬁeld. Around 10% of the data is in Dialect Arabic, including Egyptian, Gulf, Levantine, and North African. This dataset is also used as a ﬁnal hidden test (hereafter referred to as Hidden Test4) as it was not seen by any model before.
4.2. Model development data In this work, the Arabic Multi-Genre Broadcast (MGB2) corpus [30] was
used for model training. Around 70% of the data is considered Modern Standard Arabic (MSA), with the rest in Dialectal Arabic including Egyptian (EGY),
4As part of our ongoing eﬀort, this dataset is hosted on https://arabicspeech.org/ as the ﬁnal hidden test set for Arabic ASR benchmarking.
15

Gulf (GLF), Levantine (LEV), and North African (NOR). The dataset span is more than 10 years recording, during 2005-2015, from 19 distinct programs, and contains around 1,200 hours. The programs include conversations (63%), interviews (19%), and reports (18%). The conversational speech is the most challenging because it includes overlapping speech with multiple dialects. All programs were aligned using the QCRI Arabic LVCSR system [40], which is grapheme-based with one unique grapheme sequence per word. Moreover, the dataset includes a large corpus of background text that can be used to build a language model. The text corpus consists of over 130 million words crawled from the Aljazeera.net website. BuckWalter5 (BW) mapping format is used for the transcriptions and the background text data. More details about the data can be found in Table 1.
Table 1: Arabic datasets description used in this study.

Dataset

Type Hours Programs #Segments

MGB2

Training Development
Evaluation

1200 10 10

2,214 17 17

370K 5002 5365

Adaptation

4.6

23

2202

MGB3

Development

4.8

24

2181

Evaluation

6

30

5746

MGB5

Adaptation 10.2

Development

1.3

Evaluation

1.4

69

31063

10

1129

14

1055

Hidden Test Evaluation

3

7

1404

5Buckwalter (BW) is a one-to-one mapping allowing non-Arabic speakers to understand Arabic scripts, and it is also left-to-right, making it easy to render on most devices.

16

4.3. Dialectal data In this study, the proposed E2E transformer ASR is benchmarked on two
dialectal real-world datasets; the Egyptian MGB3 [19] and the Moroccan MGB5 [37]. The MGB3 dataset comprises of 16 hours of speech obtained from 80 YouTube videos, while MGB5 consists of 13 hours of speech extracted from 93 YouTube videos. Both datasets are distributed across seven genres: comedy, cooking, family/kids, fashion, drama, sports, and science talks (TEDx).
4.4. Data pre-processing The raw audio segments were ﬁrst augmented with the speed perturbation
approach, which increased the original signal by a factor of three with speed factors of 0.9, 1.0 and 1.1 [41]. Each augmented audio was transformed to a sequence of 83-dimensional feature frames for the E2E model, and an 80dimensional log Mel spectrogram with pitch features [34]. In addition, the resulting mel-spectrogram features were augmented with specaugment approach [42], which consists of three deformations: warping the features in time direction, masking blocks of consecutive frequency channels and masking blocks of utterances in time. As for the text data for the language model development, two sources were considered: the transcription text and the background text of 130 million words 6. The data was cleaned by removing punctuations, diacritics, extra empty spaces, newlines, and single-character words. To overcome the problem of very long sequences, the text was segmented to contain a maximum of 200 words with an overlap of 50 words [43]. The sub-word model [11] was used to tokenize the input text and prepare the vocabulary.
4.5. Default Model Hyperparameters All hyperparameters were obtained using a grid search. The parameters
tuning was performed on a small subset of MGB2 data (250 h). The E2E transformer-based ASR model was trained using Noam optimizer [25] with a
6https://www.aljazeera.net/
17

learning rate of 5. The best values for multi-objective tradeoﬀ weights: α in Equation 5, µ and λ in Equation 6 were found to be 0.3, 0.3 and 0.5, respectively. Table (2) summarizes the best set of parameters that were found for AM and LM transformer architecture7. As for LSTM LM, the best results were obtained with 2 layers and 650 units/layer. The LSTM LM was trained with a batch-size of 512 and a stochastic gradient descent algorithm with a learning rate of 1. The perplexity for E2E-Transformer LM and LSTM LM on the MGB2 entire text data (background text + transcription) after 10 epochs are 36.5 and 54.5 respectively.

Table 2: Values of tuned hyperparameters for E2E AM transformer and LM transformer obtained from grid search.

Input Encoder Decoder dmodel (attention)
FFN

AM Hyperparameters batch-bins: 22000000
12 layers, 8 attention heads/layer 6 layers, 8 attention heads/layer
512 2048

LM Hyperparameters batch-size: 64
12 layers, 4 attention heads/layer 12 layers, 4 attention heads/layer
512 2048

5. Results and Discussion
5.1. ASR benchmarking The developed E2E Transformer (E2E-T) is benchmarked with the state-
of-the-art modular system; [12], Aalto system [6] and the expert linguist. In addition, the contribution of CTC and Attention objectives to the E2E-T overall performance is examined. E2E-T(CTC+Attention) hereafter referred to as just E2E-T. Table 3 summarizes the WER results on the MSA datasets: the MGB2 test set and the Hidden Test set. It worth nothing that the WER results were scored with a global map scoring (GLM) ﬁle to map the numbers from digits
7The source code to reproduce the results is made publicly available on Espnet Github repository https://github.com/espnet/espnet/tree/master/egs/mgb2/asr1
18

form to their verbatim transcription. It can be seen that the proposed E2E
Table 3: WER% performance of E2E transformer (E2E-T) with (CTC, Attention and CTC+Attention) , HMM-DNN and state-of-the-art Aalto approach. The results were obtained on MGB2 test set and the Hidden Test set that was collected as part of this study.

HMM-DNN Aalto [6] E2E-T(CTC) E2E-T(Att) E2E-T(CTC+Att)

MGB2 Test

15.8

13.2

16.9

13.4

Hidden Test

15.9

-

17.7

14.7

12.5 12.6

Transformer with hybrid (CTC+Attention) outperforms the single HMM-DNN and Aalto system by 20% and 5% in relative WER, respectively. In addition, it can be seen that most of the contribution to the overall E2E-T performance is coming from the attention objective achieving WER of 13.4% and 14.7% on MGB2 Test and Hidden Test sets respectively. Adding CTC improves WER by an absolute of 0.9% and 2.1% on MGB2 Test and Hidden Test sets respectively.
5.2. Human and machine error analysis In this section, we analyze in more details the type of errors and correlation
between the expert linguist (Linguist), native speaker (Native), E2E transformer and HMM-DNN ASR systems. Figures (3,4) illustrate the inter-annotation disagreement on the Hidden Test data for raw and normalized text. It can be seen from Figures (3,4) that the inter-annotation disagreement between the two linguists is between 10.4% and 12.6% for raw and normalized transcriptions respectively. We found that the inter-annotation disagreement between the two linguists is mainly caused by the variability in transcribing dialectal speech, which does not have orthography standards. In addition, we observe that the expert linguists tend to pay more attention than the native speaker to linguistic mistakes especially with Alif/Ya/Ta-Marbuta, which are common mistakes in the Arabic language. As a result we reduced the disagreement with the common Alif/Ya/Ta-Marbuta normalization. The normalization removes distinctions within three sets of characters that are often written inconsistently
19

Figure 3: Confusion matrix of inter-annotation disagreement from raw transcription text. The rows represent a hypothesis and columns represent corresponding reference.

in DA and sometimes in MSA: Alif forms ( A = , > = , < = , | = ) , Ya

forms ( y = , Y = , and Ta-Marbuta forms ( p = , h = ). Comparing

Figures (3 4) it can be seen that the inter-annotation disagreement between the

linguist and the native speaker was signiﬁcantly reduced by up to 18% absolute,

which shows that almost half of the disagreement between the linguist, and the

Native is due to linguistic mistakes. For the comparison of inter-annotation dis-

agreement with a group, we deﬁne the inter-annotation disagreement gap G()

(hereafter referred to as gap) between ai a member of group A and a group B

mathematically

as

G(ai, B)

=

1 J +K

J j=1

K k=1

abs(disag

(ai

,

bj

)

−

disag

(bj

,

bk

))

∀j = k, where disag() is the inter-annotation disagreement. Table 4 summarizes

the inter-annotation disagreement gaps of the best machine (E2E-Transformer)

with the Native speaker group and the expert Linguist group. One can observe

that the inter-annotation gap of disagreement between the E2E-Transformer

20

Figure 4: Confusion matrix of inter-annotation disagreement from normalized transcription text.
Table 4: The inter-annotation disagreement gaps from raw and normalized transcriptions. The gaps are computed between the E2E-Transformer and Native speaker group, E2E-Transformer and expert Linguist group, Native and expert Linguist group.

Native Raw Norm

Linguist Raw Norm

E2E-Transformer 6.6

2

3.5 4.7

Native

-

- 13.1 5.4

ASR and the expert linguist is 3.5%, 4.7 % from the raw and normalized transcriptions respectively. Hence the minimum gap for the machine to overcome the expert linguist performance is 3.5%. It noticeable that the gap between the native speaker and the expert linguist compared to the gap between the

21

E2E-Transformer and expert linguist is higher by 9.6%, 0.7 % for the raw and normalized transcriptions respectively. Furthermore, we take a closer look at the most frequent top ten errors in terms of substitutions, deletions, and insertions made by the linguist and E2E transformer, as shown in Tables 5 and 7. Similar errors from both the E2E trans-
Table 5: Most common substitutions for E2E ASR system and expert linguist. The number of times each error occurs is followed by the word in the reference and the corresponding hypothesis.

E2E Transformer

BW (REF / HYP)

REF Translation

12: <nh / >nh

it is

12: AldEwY / AldEwp

lawsuit

7: AljAbry / Aljbr

Aljabry (name)

6: dyAb / AldyAb

Dyab (name)

5: btAEh / btAEt

possessed

5: nHnA / nHn

we

4: Ally / Alty

which

4: AlwA$nTn / wA$nTn Washington

4: l>n / l}n

to

3: lAzAlt / zAlt

still

Linguist

BW (REF / HYP) REF Translation

11: <nh / >nh

it is

6: btAEh / btAEt

possessed

7: lk / lky

your

6: nHnA / nHn

we

4: tfDl / AtfDl

go ahead

4: fy / fyh

in

4: l>n / >n

to

3: AlmAyh / AlmwyA

water

3: ln / lm

did/will not

3: bdt / bd>t

started

former and the linguist are highlighted with the same color. Inspections revealed that the top errors made by both human and machine are substantially similar, especially the insertions and deletions. Looking at the substitutions, one can notice that most of the errors for both the linguist and E2E are on dialectal words. The main diﬀerence between E2E and the linguist is that the E2E tends to make more errors on rare Arabic words like names. In addition, we provide a comprehensive analysis of the substitutions on a semantic level which includes the eﬀect of phonetic8, morphological and syntactic changes illustrated in Table 6. We note that one representative example was picked from Table 5 for each
8https://en.wikipedia.org/wiki/Help:IPA/Arabic

22

Table 6: Linguistic analysis of the most common substitutions from Table 5 for both E2E Transformer and the linguist. The assessment includes phonetic, morphological and syntactic changes

Arabic

BW <nh /

Base Phrases <n+h /

Phonetic Pinnahu /

Meaning / Analysis h = aﬃx, refers to ”him”, both prepositions can be used

>nh AldEwY /

>n+h Al+dEwY /

Pannahu aldaQwa: /

for emphasis, their usage and meaning diﬀer based on the syntax Al = the; dEwY (root: Ada˜EY)= To claim something (in court)

AldEwp AljAbry /

Al+dEwp Al+jAbry /

aldaQwa: alÃa:biri: /

dEwp (root: dEA)= asks or calls (A man asks/calls God for help) Al= the

Aljbr dyAb /

Al+jbr dyAb /

alÃabr di:a:b /

AljAbry (root: jbr) = Algebra (Name of a person) Al= the

AldyAb Ally /

Al+dyAb Ally /

aldi:a:b a:llajj /

dyAb (dialect of *}Ab)= wolves (Name of a person) Ally = which (dialectal form)

Alty l>n /

Alty l+>n /

a:llati: lPan /

Alty = Alty: which l = aﬃx used for explanation; l>n (root: >n) = to

l}n lAzAlt /

l+}n l+zAlt /

lPin la:za:lat /

l}n (root: }n) = if lA = aﬃx used for negation

zAlt

zAlt

za:lat

zAlt (root: zAl) = gone away

case described in Table 6. Phonetic: It can be seen that phonetic changes are mainly related to the dialectal substitutions (nHnA / nHn, Ally / Alty, AlmAyh / AlmwyA), which does not eﬀect the meaning of the words, or with unfamiliar names (AljAbry / Aljbr). In addition, it is noticeable that, unlike the linguist, the model fails to correctly transcribe words that sound the same but written diﬀerently and have diﬀerent meanings (AldEwY / AldEwp). Morphological: morphological substitutions in some cases cause a change in the word meaning like discarding the negation in (lAzAlt / zAlt). However other changes like adding or discarding a deﬁnite article like (al = the), which is commonly used in Arabic language with entities (AlwA$nTn / wA$nTn, dyAb / AldyAb), does not aﬀect the meaning. Syntactic: It can be noted that both the model and the linguist ﬁnd it diﬃcult to correctly transcribe (<nh / >nh) which are often wrongly used interchangeably by native Arabic speakers. Both (<nh / >nh) can be used for emphasis, however their usage and meaning diﬀer based on the syntax. In addition, native speakers articulate the ﬁrst sound (Pi ,Pa) with a sound that is in between which

23

makes it harder to distinguish. On the other hand, the substitution (l>n / l}n) changes the meaning and one can note that this type of mistake is only made by the machine. Finally, from Table 7 it can be seen that the insertion and deletion patterns are similar for both the linguist and E2E: prepositions are the most frequent errors. It can be observed that the deletion rate of E2E compared to the linguist is somewhat higher than the substitution and insertion rates. This makes sense as the linguist is more careful in transcribing everything that is being heard.
Table 7: Most common insertion and deletions for E2E ASR system and expert linguist.

Insertions

E2E Transformer

linguist

BW Translation BW Translation

20: >n

that

16: fy

in

18: >w

or

12: mn

from

13: Alh

his

12: nEm

yes

12: fy

in

10: >n

that

9: lA

no

9: mA

what

8: mA

what

9: w

and

6: <lY

to

6: <lY

to

5: mn

from

6: >w

or

5: hw

him

6: yEny

means

4: >nA

me

5: lA

no

Deletions

E2E Transformer

linguist

BW

Translation

BW

Translation

42: yEny

means

21: nEm

yes

39: nEm

yes

17: fy

in

16: >n

that

13: >n

that

16: fy

in

10: mA

what

16: bn

son

9: lA

no

13: mn

from

6: Al—n

now

13: w

and

6: mn

from

12: lA

no

6: hw

him

10: Tyb

alright

6: yEny

means

11: hw

him

5: yA

(calling)

5.3. Speech recognition by E2E transformer and HMM-DNN In this section, the results of both the HMM-DNN and the E2E transformer
are analyzed, pointing out the advantages and disadvantages of each. The correct transcription in the examples are highlighted in green and the corresponding errors are highlighted in pink.
• Dialectal Arabic: in the case of dialects and overlapped speech, the E2E generates more accurate transcriptions. The E2E transformer is able to learn the context much better with the self-attention mechanism and

24

capture the semantics in both the standard Arabic structure as well as diﬀerent Arabic dialects. REF : REF BW : AltqsyT fy $y’ slby wfy $y’ <yjAby hl> fy $y’ DrwryAt mvlA llbyt . Translation: The installment has negative things and positive things now there are necessities, for example for a house. E2E : fy $y’ slby fy $y’ <yjAby >nA fy $y’ DrwryAt mvlA llbyt HMM-DNN : hy t>Syl b$y’ slby b$y’ <yjAby Drwryp tsll Albyt
• Noise and hesitation: the E2E transformer is more sensitive to noise and hesitation compared to HMM-DNN. For example, when a person says ”aaaah”, or when there is a sound of an ambulance, the E2E model generates random words. We think that HMM-TDNN is more robust to noise and hesitation because it uses the <SIL> symbol in the Viterbi alignment during the training step which was successfully aligned with silences. On the other hand, the CTC loss in E2E uses a similar symbol for the white spaces between the words, however we think that the E2E requires more data or explicit labeling for the noise and hesitation to improve the robustness. This is mainly because the E2E system optimization is mainly driven by the data compared to the HMM-DNN system which includes more expert engineering. REF : <ambulance noise>
REF BW : <ambulance noise> Alfydyw bEd mA Ant$r bhyk srEp yEny >ETAny dAfE <ny >Syr hyk kl >HdAv Translation: The video after it spread with such speed. I mean it gave me an incentive that I would like that all events.
25

E2E : fy $hr mAyw fy $hr mAyw fy bEd mA Ant$r bhyk bsrEp yEny >ETAny dAfE Eny >Syr hyk kl >HdAv HMM-DNN : Alfydyw bEd mA Ant$r bhyk srEp yEny >ETAny dAfE <ny >Syr hyk kl >HdAv 5.4. Eﬀect of data size on ASR performance In this section, the eﬀect of the size of the data on both our proposed E2E Transformer and HMM-DNN modular systems is examined. The conﬁguration used for both E2E Transformer and HMM-DNN modular systems is described in Section 4.5. The size of the training data was chosen from the following points (250h, 550h, and 1200h). For consistency, the development and testing data were kept the same for all training data sizes. The performance of both E2E and modular systems for each data size is illustrated in Figure 5. It can
Figure 5: Performance of HMM-DNN and E2E ASR systems measured in WER for diﬀerent training datasizes.
be seen that the performance of the modular ASR system is much better at datasize lower than around 400h. However, as the datasize increases, the E2E performance improves with much steeper trend compared to the modular system, which indicates that with more data, the E2E model is expected to show further improvements. In addition, the results show that the E2E performance outperforms the modular system after around 400h of data size, which dispel the myth about the need of a very huge amount of private data to match the
26

performance of a modular system [44]. We note that while 400 hours might not seem a lot of data for English, it is still a huge amount of transcribed data for the vast majority of languages and dialects in the world.

5.5. E2E Transformer LM ablation analysis In this section, the eﬀect of the LM on the acoustic E2E transformer per-
formance using the transcription text (TR) and background (BG) text is investigated. The WER% and the real time factor (RT) of the acoustic E2E transformer with RNN LM and Transformer LM architectures are summarized in Table 8. It can be seen that as the beam size decreases, the WER gets worse
Table 8: Comparison (WER% and RT factor) of E2E Transformer (E2E-T) with diﬀerent LM rescoring conﬁguration on MGB2 test set. The language models were trained with the transcription text (TR) and background (BG) text

Method
E2E-T+LSTM-LM (BG+TR) E2E-T+T-LM (BG+TR) E2E-T+T-LM (TR) E2E-T

Beam 20 WER RT
13.4 3.65 12.7 5.87 12.6 5.87 12.5 3.24

Beam 5 WER RT
14.6 0.9 13.1 1.49
13 1.49 13 0.82

Beam 2 WER RT
14.7 0.44 13.5 0.62 13.4 0.62
13 0.39

Table 9: LM standardized confusion pairs

Freq 19

E2E-T lly

18 dA/dh

/

10 dy/hAy

/

3 <HnA

E2E-T+ LM

Alty/Al*y

/

h*A

h*h

nHn

Translation which this this we

and the RT improves, which is expected. A very interesting aspect to note is that the E2E model without LM achieves better WER compared to E2E with LM. Looking at the confusion results, we found that the LM rescoring is in fact standardizing the Arabic dialects text to MSA as shown in Table 9. Although these predictions are linguistically correct, they are counted as errors since they

27

do not match the verbatim transcription, causing the WER with LM to become slightly higher. Overall, the manual inspection showed that the results with LM helps to improve the quality of the transcription without changing its meaning.
Figure 6: Illustration of E2E transformer ASR prediction on very long segment
5.6. Eﬀect of speech segments length The duration of speech segments for ASR has traditionally been controlled
by professional transcribers. In practical ASR applications, the duration variability arises from automatic segmentation. In this section, we investigate the impact of segment duration variability on the E2E transformer (E2E-T) and HMM-DNN system that arises from using voice activity detection (VAD). In a practical setup, we found that automatic VAD like InaSpeechSegmenter (IS) [13] can produce very long segments compared to the training data prepared by the expert human. The change in the segment duration aﬀects the statistical properties of the data and hence causes a shift in the data distribution. An exmaple that shows the eﬀect of a very long segment on E2E transformer transcription is illustrated in Figure 6. In this particular example, the WER was %3 up to 32 seconds. However, after 32 seconds, the model starts to generate random predictions highlighted in red. To address this problem, we propose improved VAD pipeline (Imp IS) that combines beneﬁts of IS, which is accurate in speech
28

detection, and energy-based VAD9 to control the maximum segment duration. The Imp IS is applied by running energy-based VAD and IS in parallel, hence the approach does not introduce any further latency. After that, the resulted segments from energy-based VAD are aligned with the corresponding long segments from IS. In this work, we found that the best results of the proposed Imp IS VAD were obtained with the maximum duration threshold of 25 sec. It is worth noting that the automatic segmentation is applied on the complete audio ﬁles, and hence the entire MGB2 test set with overlapped segments is used in Tables 11,10 compare to the oﬃcial MGB2 test set used previously in Tables 3, 8 that only includes the non-overlapped segments. The WER(%) of E2E-T and HMM-DNN results on human segmentation HS, IS segmentation, and the proposed Imp IS are summarized in Table 10. It is noticeable that

Table 10: E2E transformer (E2E-T) and HMM-DNN WER (%) results on human segmentation (HS), InaSpeech (IS) segmentation , and the proposed improved InaSpeech segmentation (Imp IS) benchmarked on two sets: MGB2 Test, and new Hidden test set (Hidden Test) described in Section 4.3.

MGB2 Test HS IS Imp IS

Hidden Test HS IS Imp IS

E2E-T HMM-DNN

14.7 21.6

42.3 27.3

16.6 28.3

12.6 15.9

32.2 24.8

14.5 26.6

the proposed Imp IS approach improved IS WER by an absolute of 25.7% and 17.7% on MGB2 Test and Hidden Test respectively, reaching ideal human segmentation with an absolute diﬀerence of only 1.9%. On the other hand, the eﬀect of the segment duration on HMM-DNN is 1.4% on average when comparing the IS to Imp IS segmentation WER results. However, the HMM-DNN results are still aﬀected by the automatic segmentation with absolute diﬀerence of 6.3% and 8.9% when comparing HS with Imp IS. Moreover to examine the eﬀect of the segmentation duration the WER results of E2E-T and HMM-DNN
9https://github.com/shammur/pyVAD
29

on the MGB2-Test set with segmentation ranges of 0-15 seconds, 16-30 seconds and 30-more seconds are illustrated in Table 11. It can be observed from
Table 11: E2E transformer (E2E-T) and HMM-DNN WER (%) results on the MGB2-Test set with segmentation ranges of 0-15 seconds, 16-30 seconds and 30-more seconds.

MGB2 Test segmentation 0-15 sec 16-30 sec 30-more sec

E2E-T HMM-DNN

19.4 31.7

18.3 28.3

59.7 29.5

Table 11 that for 30s-more segmentation the WER of E2E-T increases signiﬁcantly by an absolute of 41.4% compared to 16-30 sec segmentation range. On the other hand, the WER of HMM-DNN for 30-more segmentation decreased by only 1% compared to 16-30 sec range. This clearly shows the signiﬁcant impact of very long segments on the E2E-T system compared to the modular ASR system. To further investigate the segmentation duration of the proposed

Figure 7: Segments duration percentage for diﬀerent segmentation including MGB2 Train (MGB2 Train) and Test (MGB2 Test) human segmentation, InaSeg segmentation (MGB2 Test IS), and improved InaSeg segmentation (MGB2 Test Imp IS).
Imp IS segmented MGB2 Test (MGB2 Test Imp IS) compared to the human segmented train (MGB2 Train), human segmented test (MGB2 Test), IS segmented MGB2 Test (MGB2 Test IS), we visualize the percentage of segmen-
30

tation duration with three ranges: 0-15 seconds, 16-30 seconds and 30-more seconds as shown in Figure 7. It can be seen that IS contains 43.6 % of its segments with duration larger than 30 seconds, which is signiﬁcantly higher compared to the MGB2 Train segmentation with only 7.9% of segments larger than 30 seconds. On the other hand, all segments of the proposed Imp IS are less than 30 seconds with 45.4% less than 15 seconds and 54.3% between 16 and 30 seconds. Finally, the MGB2 test set with human segmentation has all of its segment duration below 15 seconds. Moreover, we visualize the segmentation duration statistically we deﬁne the eﬀective segmentation duration as the range within 3 standard deviations that covers 99% of the segments as shown in Figure 8. It can be seen that the eﬀective segmentation duration of MGB2 Test IS is around 60 seconds with maximum duration of around 120 seconds which is much larger than the eﬀective segmentation duration of MGB2 Train, which is only 30 seconds. This conﬁrms our assumption about the shift in the duration distribution and explains why the model fails to make successful predictions after around 30 seconds.
5.7. Dialectal Arabic ASR In this section, the performance of E2E transformer is studied on two di-
alectal Arabic ASR challenges MGB3 and MGB5 [19, 37]. For more details about the data, refer to section 4.3 and Table 1. As DA is lacking standard orthographic rules as well as sizable transcribed data, it is considered to be an excellent choice to highlight the challenges of speech recognition in the wild. The best E2E transformer obtained in Section 5.5 is benchmarked with the MGB3 and the MGB5 state-of-the-art results. The E2E transformer was ﬁnetuned on the MGB3 and the MGB5 adaptation sets independently. The hyper-parameters for the transformer model are similar to what was used for the MGB2 training described in Section 4.5 except that the learning rate is reduced to 0.1 and no warm-up steps. The model is initialized with the E2E transformer parameters pretrained on the MGB2 from Section 5.5. For data preprocessing, the same data augmentation described in Section 4.4 was followed. Each sentence
31

(a)

(b)

(c)

(d)

Figure 8: Duration distributions of the speech segments for: (a) MGB2 Train Hs human segmentation, (b) MGB2 Test Hs human segmentation, (c) MGB2 Test IS InaSeg segmentation and (d) MGB2 Test Imp IS proposed improved InaSeg segmentation.

in the adaptation and the development data were transcribed by four diﬀerent annotators to explore the non-orthographic nature of dialectal Arabic. In our experiments, the transcripts from the four transcribers were combined, which increased the amount of the data four times. The participants in the MGB3 and the MGB5 challenges had the access to the development set which often used in the competitions as part of the model ﬁnetuning. As a result we report two sets of results: 1) Finetune the model using the adaptation set only for 30 epochs and use the development set to monitor the performance and select the model with the best result; 2) Finetune the model on adaptation and the development sets combined. In this case the model is ﬁrst ﬁnetuned for 30 epochs on the

32

adaptation set and then further ﬁnetuned on adaptation set for an additional 5 epochs. The results of both approaches on the MGB3 Test, MGB5 Test sets are illustrated in Table 12. It can be seen from Table 12 that the single E2E-
Table 12: MR-WER% & AV-WER% results on two dialectal datasets MGB3 and MGB5 test sets.

Aalto [6] RDI-CU [37] E2E-Transformer(adapt) E2E-Transformer(adapt+dev)

MGB3 Test

MR-WER AV-WER

29.3

37.5

-

-

29.2

36.0

27.5

34.4

MGB5 Test

MR-WER AV-WER

-

-

37.6

59.4

34.9

57.2

33.8

56.2

Transformer outperforms the state-of-the-art modular HMM-DNN systems in both of the DA challenges. We see about 4% relative reduction in AV-WER while using the adaptation data only, and between 6-8% relative reduction in AV-WER when using both the adaptation and the development data, which signiﬁcantly outperforms the previous state-of-the-arts in DA ASR. This is a new milestone for DA speech recognition.
6. Conclusion
In this paper, we presented the ﬁrst comprehensive study comparing head to head E2E ASR, modular HMM-DNN ASR and HSR on Arabic speech. We provided a comprehensive error analysis comparing the best ASR system performance to the expert linguist and native speaker. It has been found that the machine ASR arguably outperforms the performance of the native speaker, however, the WER gap to reach expert linguist performance is still on average 3.5% on the raw Arabic transcription text. It was noticeable that the machine mistakes showed high similarity with the expert linguist transcription. Additionally, we developed the ﬁrst E2E transformer for the Arabic ASR and its dialects. The proposed E2E transformer signiﬁcantly outperformed prior state-of-the-art on MGB2, MGB3 and MGB5 achieving a new state-of-the-art performance at
33

12.5%, 27.5% and 33.8% respectively. Moreover, it has been found that, in practical ASR, the segment duration has a severe impact on E2E transformer performance. To address the problem of segment duration variability, a new VAD pipeline with a maximum duration threshold was proposed. For future work, we plan to address the gap between human and machine in Arabic ASR and address the low resource challenge in dialectal Arabic, which still shows a high error rate.
References
[1] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., Deep speech 2: End-to-end speech recognition in English and Mandarin, in: International conference on machine learning, 2016, pp. 173–182.
[2] A. Graves, N. Jaitly, Towards end-to-end speech recognition with recurrent neural networks, in: International conference on machine learning, 2014, pp. 1764–1772.
[3] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig, Achieving human parity in conversational speech recognition, arXiv preprint arXiv:1610.05256.
[4] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L.-L. Lim, et al., English conversational telephone speech recognition by humans and machines, Proc. Interspeech (2017) 132–136.
[5] P. Michalowski, The lives of the sumerian language, Margins of writing, origins of cultures (2006) 159–84.
[6] P. Smit, S. R. Gangireddy, S. Enarvi, S. Virpioja, M. Kurimo, Aalto system for the 2017 Arabic multi-genre broadcast challenge, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2017, pp. 338–345.
34

[7] H. Mubarak, A. Abdelali, H. Sajjad, Y. Samih, K. Darwish, Highly eﬀective arabic diacritization using sequence to sequence modeling, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 2390–2395.
[8] A. Das, M. Hasegawa-Johnson, Cross-lingual transfer learning during supervised training in low resource scenarios, in: Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[9] S. Khurana, A. Ali, J. Glass, Darts: Dialectal Arabic transcription system, arXiv preprint arXiv:1909.12163.
[10] A. Ahmed, Y. Hifny, K. Shaalan, S. Toral, End-to-end lexicon free Arabic speech recognition using recurrent neural networks, Computational Linguistics, Speech And Image Processing For Arabic Language 4 (2018) 231.
[11] T. Kudo, Subword regularization: Improving neural network translation models with multiple subword candidates, arXiv preprint arXiv:1804.10959.
[12] S. Khurana, A. Ali, Qcri advanced transcription system (QATS) for the Arabic multi-dialect broadcast media recognition: MGB-2 challenge, in: IEEE Spoken Language Technology Workshop (SLT), IEEE, 2016, pp. 292– 298.
[13] D. Doukhan, E. Lechapt, M. Evrard, J. Carrive, Ina’s mirex 2018 music and speech detection system, in: Music Information Retrieval Evaluation eXchange (MIREX 2018), 2018.
[14] T. Giannakopoulos, pyaudioanalysis: An open-source python library for audio signal analysis, PloS one 10 (12) (2015) e0144610.
[15] G. E. Dahl, D. Yu, L. Deng, A. Acero, Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition, IEEE Transactions on audio, speech, and language processing 20 (1) (2011) 30–42.
35

[16] A. Graves, A.-r. Mohamed, G. Hinton, Speech recognition with deep recurrent neural networks, in: IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE, 2013, pp. 6645–6649.
[17] Y. Hifny, Uniﬁed acoustic modeling using deep conditional random ﬁelds, Transactions on Machine Learning and Artiﬁcial Intelligence 3 (2) (2015) 65–65.
[18] V. Peddinti, D. Povey, S. Khudanpur, A time delay neural network architecture for eﬃcient modeling of long temporal contexts, in: Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[19] A. Ali, S. Vogel, S. Renals, Speech recognition challenge in the wild: Arabic MGB-3, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2017, pp. 316–322.
[20] D. Wang, X. Wang, S. Lv, An overview of end-to-end automatic speech recognition, Symmetry 11 (8) (2019) 1018.
[21] W. Chan, N. Jaitly, Q. Le, O. Vinyals, Listen, attend and spell: A neural network for large vocabulary conversational speech recognition, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2016, pp. 4960–4964.
[22] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, Y. Bengio, Attentionbased models for speech recognition, Advances in neural information processing systems 28 (2015) 577–585.
[23] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, T. Hayashi, Hybrid ctc/attention architecture for end-to-end speech recognition, IEEE Journal of Selected Topics in Signal Processing 11 (8) (2017) 1240–1253.
[24] Y. Belinkov12, A. Ali, J. Glass, Analyzing phonetic and graphemic representations in end-to-end automatic speech recognition, Proc. Interspeech.
36

[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, I. Polosukhin, Attention is all you need, in: Advances in neural information processing systems, 2017, pp. 5998–6008.
[26] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, et al., A comparative study on transformer vs rnn in speech applications, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2019, pp. 449– 456.
[27] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang, et al., Transformer-based acoustic modeling for hybrid speech recognition, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6874– 6878.
[28] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, R. Collobert, End-to-end ASR: from supervised to semi-supervised learning with modern architectures, arXiv preprint arXiv:1911.08460.
[29] A. Stolcke, J. Droppo, Comparing human and machine errors in conversational speech transcription, arXiv preprint arXiv:1708.08615.
[30] A. Ali, P. Bell, J. Glass, Y. Messaoui, H. Mubarak, S. Renals, Y. Zhang, The mgb-2 challenge: Arabic multi-dialect broadcast media recognition, in: IEEE Spoken Language Technology Workshop (SLT), IEEE, 2016, pp. 279–284.
[31] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, S. Khudanpur, Purely sequence-trained neural networks for ASR based on lattice-free MMI, in: Proc. Interspeech, 2016, pp. 2751–2755.
[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, et al., The kaldi speech
37

recognition toolkit, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), no. CONF, IEEE Signal Processing Society, 2011.
[33] D. Povey, H. Hadian, P. Ghahremani, K. Li, S. Khudanpur, A timerestricted self-attention layer for ASR, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 5874–5878.
[34] P. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal, S. Khudanpur, A pitch extraction algorithm tuned for automatic speech recognition, in: IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE, 2014, pp. 2494–2498.
[35] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint arXiv:1607.06450.
[36] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[37] A. Ali, S. Shon, Y. Samih, H. Mubarak, A. Abdelali, J. Glass, S. Renals, K. Choukri, The MGB-5 challenge: Recognition and dialect identiﬁcation of dialectal Arabic speech, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2019, pp. 1026–1033.
[38] A. Ali, W. Magdy, P. Bell, S. Renais, Multi-reference wer for evaluating asr for languages with no orthographic rules, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2015, pp. 576– 580.
[39] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen, et al., Espnet: End-to-end speech processing toolkit, Proc. Interspeech (2018) 2207–2211.
38

[40] A. Ali, Y. Zhang, P. Cardinal, N. Dahak, S. Vogel, J. Glass, A complete kaldi recipe for building arabic speech recognition systems, in: IEEE spoken language technology workshop (SLT), IEEE, 2014, pp. 525–529.
[41] T. Ko, V. Peddinti, D. Povey, S. Khudanpur, Audio augmentation for speech recognition, in: Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[42] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, Q. V. Le, Specaugment: A simple data augmentation method for automatic speech recognition, Proc. Interspeech (2019) 2613–2617.
[43] R. Pappagari, P. Zelasko, J. Villalba, Y. Carmiel, N. Dehak, Hierarchical transformers for long document classiﬁcation, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2019, pp. 838– 844.
[44] A. Zeyer, K. Irie, R. Schlu¨ter, H. Ney, Improved training of end-to-end attention models for speech recognition, arXiv preprint arXiv:1805.03294.
39

