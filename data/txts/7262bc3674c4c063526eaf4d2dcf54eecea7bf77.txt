PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations
John Wieting1 Kevin Gimpel2 1Carnegie Mellon University, Pittsburgh, PA, 15213, USA 2Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA
jwieting@cs.cmu.edu, kgimpel@ttic.edu

arXiv:1711.05732v2 [cs.CL] 20 Apr 2018

Abstract
We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the nonEnglish side of a large parallel corpus, following Wieting et al. (2017). Our hope is that PARANMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARANMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1
1 Introduction
While many approaches have been developed for generating or ﬁnding paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Lan et al., 2017), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), which was created automatically from bilingual text by pivoting over the non-English language (Bannard and Callison-Burch, 2005). PPDB has been used to improve word embeddings (Faruqui et al., 2015; Mrksˇic´ et al., 2016). However, PPDB is less useful for learning sentence embeddings (Wieting and Gimpel, 2017).
In this paper, we describe the creation of a dataset containing more than 50 million sentential
1 Dataset, code, and embeddings are available at https: //www.cs.cmu.edu/˜jwieting.

paraphrase pairs. We create it automatically by scaling up the approach of Wieting et al. (2017). We use neural machine translation (NMT) to translate the Czech side of a large Czech-English parallel corpus. We pair the English translations with the English references to form paraphrase pairs. We call this dataset PARANMT-50M. It contains examples illustrating a broad range of paraphrase phenomena; we show examples in Section 3. PARANMT-50M has the potential to be useful for many tasks, from linguistically controlled paraphrase generation, style transfer, and sentence simpliﬁcation to core NLP problems like machine translation.
We show the utility of PARANMT-50M by using it to train paraphrastic sentence embeddings using the learning framework of Wieting et al. (2016b). We primarily evaluate our sentence embeddings on the SemEval semantic textual similarity (STS) competitions from 2012-2016. Since so many domains are covered in these datasets, they form a demanding evaluation for a general purpose sentence embedding model.
Our sentence embeddings learned from PARANMT-50M outperform all systems in every STS competition from 2012 to 2016. These tasks have drawn substantial participation; in 2016, for example, the competition attracted 43 teams and had 119 submissions. Most STS systems use curated lexical resources, the provided supervised training data with manually-annotated similarities, and joint modeling of the sentence pair. We use none of these, simply encoding each sentence independently using our models and computing cosine similarity between their embeddings.
We experiment with several compositional architectures and ﬁnd them all to work well. We also ﬁnd beneﬁt from making a simple change to learning to better leverage the large training set, namely, increasing the search space of negative

examples. We additionally evaluate on generalpurpose sentence embedding tasks used in past work (Kiros et al., 2015; Conneau et al., 2017), ﬁnding our embeddings to perform competitively.
Lastly, we show that PARANMT-50M is able to be used in paraphrase generation. Recent work (Iyyer et al., 2018) used PARANMT-50M to generate paraphrases that have a speciﬁc syntactic form. In their model, a sentence and its target form (represented as the top two levels of a linearized parse tree) are transformed by the model into a paraphrase with this target structure. We also explore paraphrase generation in this paper, ﬁnding that a basic encoder-decoder model trained on PARANMT-50M has a canonicalization effect and is able to correct grammar and standardize the input sentence.
We release the PARANMT-50M dataset, our trained sentence embeddings, and our code. PARANMT-50M is the largest collection of sentential paraphrases released to date. We hope it can motivate new research directions and be used to create powerful NLP models, while adding a robustness to existing ones by incorporating paraphrase knowledge. Our paraphrastic sentence embeddings are state-of-the-art by a signiﬁcant margin, and we hope they can be useful for many applications both as a sentence representation function and as a general similarity metric.
2 Related Work
We discuss work in automatically building paraphrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions.
Paraphrase discovery and generation. Many methods have been developed for generating or ﬁnding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017).
The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to

ﬁnd lexical and phrasal paraphrases in parallel text. Ganitkevitch et al. (2013) scaled up these techniques to produce the Paraphrase Database (PPDB). Our goals are similar to those of PPDB, which has likewise been generated for many languages (Ganitkevitch and Callison-Burch, 2014) since it only needs parallel text. In particular, we follow the approach of Wieting et al. (2017), who used NMT to translate the non-English side of parallel text to get English-English paraphrase pairs. We scale up the method to a larger dataset, produce state-of-the-art paraphrastic sentence embeddings, and release all of our resources.
Sentence embeddings. Our learning and evaluation setting is the same as that in recent work which seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks (Wieting et al., 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017). They trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. Prior work in learning general sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015; Gan et al., 2017), and other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017; Conneau et al., 2017).
Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrksˇic´ et al., 2016). Several have used NMT architectures and training settings to obtain better embeddings for words (Hill et al., 2014a,b) and words-in-context (McCann et al., 2017). Hill et al. (2016) evaluated the encoders of Englishto-X NMT systems as sentence representations. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations.
3 The PARANMT-50M Dataset
To create our dataset, we used backtranslation (Wieting et al., 2017). We used a Czech-English NMT system to translate Czech sentences from the training data into English. We

Dataset Common Crawl CzEng 1.6 Europarl News Commentary

Avg. Length 24.0±34.7 13.3±19.3 26.1±15.4 25.2±13.9

Avg. IDF 7.7±1.1 7.4±1.2 7.1±0.6 7.5±1.1

Avg. Para. Score 0.83±0.16 0.84±0.16 0.95±0.05 0.92±0.12

Vocab. Entropy 7.2 6.8 6.4 7.0

Parse Entropy 3.5 4.1 3.0 3.4

Size 0.16M 51.4M 0.65M 0.19M

Table 1: Statistics of 100K-samples of Czech-English parallel corpora; standard deviations are shown for averages.

Reference Translation so, what’s half an hour? well, don’t worry. i’ve taken out tons and tons of guys. lots of guys. it’s gonna be ...... classic. greetings, all! but she doesn’t have much of a case. it was good in spite of the taste.

Machine Translation half an hour won’t kill you. don’t worry, i’ve done it to dozens of men. yeah, sure. it’s gonna be great. hello everyone! but as far as the case goes, she doesn’t have much. despite the ﬂavor, it felt good.

Table 2: Example paraphrase pairs from PARANMT-50M, where each consists of an English reference translation and the machine translation of the Czech source sentence (not shown).

paired the translations with the English references to form English-English paraphrase pairs.
We used the pretrained Czech-English model from the NMT system of Sennrich et al. (2017). Its training data includes four sources: Common Crawl, CzEng 1.6 (Bojar et al., 2016), Europarl, and News Commentary. We next discuss how we chose the CzEng corpus from among these to create our dataset. We did not choose Czech due to any particular linguistic properties. Wieting et al. (2017) found little difference among Czech, German, and French as source languages for backtranslation. There were much larger differences due to data domain, so we focus on the question of domain in this section. We leave the question of investigating properties of back-translation of different languages to future work.
3.1 Choosing a Data Source
To assess characteristics that yield useful data, we randomly sampled 100K English reference translations from each data source and computed statistics. Table 1 shows the average sentence length, the average inverse document frequency (IDF) where IDFs are computed using Wikipedia sentences, and the average paraphrase score for the two sentences. The paraphrase score is calculated by averaging PARAGRAM-PHRASE embeddings (Wieting et al., 2016b) for the two sentences in each pair and then computing their cosine similarity. The table also shows the entropies of the vocabularies and constituent parses obtained using the Stanford Parser (Manning et al., 2014).2
2To mitigate sparsity in the parse entropy, we used only the top two levels of each parse tree.

Europarl exhibits the least diversity in terms of rare word usage, vocabulary entropy, and parse entropy. This is unsurprising given its formulaic and repetitive nature. CzEng has shorter sentences than the other corpora and more diverse sentence structures, as shown by its high parse entropy. In terms of vocabulary use, CzEng is not particularly more diverse than Common Crawl and News Commentary, though this could be due to the prevalence of named entities in the latter two.
In Section 5.3, we empirically compare these data sources as training data for sentence embeddings. The CzEng corpus yields the strongest performance when controlling for training data size. Since its sentences are short, we suspect this helps ensure high-quality back-translations. A large portion of it is movie subtitles which tend to use a wide vocabulary and have a diversity of sentence structures; however, other domains are included as well. It is also the largest corpus, containing over 51 million sentence pairs. In addition to providing a large number of training examples for downstream tasks, this means that the NMT system should be able to produce quality translations for this subset of its training data.
For all of these reasons, we chose the CzEng corpus to create PARANMT-50M. When doing so, we used beam search with a beam size of 12 and selected the highest scoring translation from the beam. It took over 10,000 GPU hours to backtranslate the CzEng corpus. We show illustrative examples in Table 2.

3.2 Manual Evaluation
We conducted a manual analysis of our dataset in order to quantify its noise level, and how the noise can be ameliorated with ﬁltering. Two domain experts annotated a sample of 100 examples from each of ﬁve ranges of the Paraphrase Score.3 They annotated both the strength of the paraphrase relationship and the ﬂuency of the back-translation.
Para. Score # Tri. Overlap Paraphrase Fluency Range (M) Mean (Std.) 1 2 3 1 2 3
(-0.1, 0.2] 4.0 0.00±0.0 92 6 2 1 5 94 (0.2, 0.4] 3.8 0.02±0.1 53 32 15 1 12 87 (0.4, 0.6] 6.9 0.07±0.1 22 45 33 2 9 89 (0.6, 0.8] 14.4 0.17±0.2 1 43 56 11 0 89 (0.8, 1.0] 18.0 0.35±0.2 1 13 86 3 0 97
Table 3: Manual evaluation of 100-pair data samples drawn from ﬁve ranges of the automatic paraphrase score (ﬁrst column). Second column shows total count of pairs in that range in PARANMT-50M. Paraphrase strength and ﬂuency were judged on a 1-3 scale and the table shows counts of each score designation.

of the entire dataset. In the highest paraphrase score range, 86% of the pairs possess a strong paraphrase relationship. The annotations suggest that PARANMT-50M contains approximately 30 million strong paraphrase pairs, and that the paraphrase score is a good indicator of quality. With regards to ﬂuency, the vast majority of the backtranslations are ﬂuent, even at the low end of the paraphrase score range. At the low ranges, we inspected the data and found there to be many errors in the sentence alignment in the original bitext.
4 Learning Sentence Embeddings
To show the usefulness of the PARANMT-50M dataset, we will use it to train sentence embeddings. We adopt the learning framework from Wieting et al. (2016b), which was developed to train sentence embeddings from pairs in PPDB. We ﬁrst describe the compositional sentence embedding models we will experiment with, then discuss training and our modiﬁcation (“megabatching”).

To annotate paraphrase strength, we adopted the annotation guidelines used by Agirre et al. (2012). The original guidelines specify 6 classes, which we reduce to 3 for simplicity. We collapse the top two into one category, leave the next alone, and collapse the bottom 3 into our lowest category. Therefore, for a sentence pair to have a rating of 3, the sentences must have the same meaning, but some unimportant details can differ. To have a rating of 2, the sentences are roughly equivalent, with some important information missing or that differs slightly. For a rating of 1, the sentences are not equivalent, even if they share minor details.
For ﬂuency of the back-translation, we use the following: A rating of 3 means it has no grammatical errors, 2 means it has one to two errors, and 1 means it has more than two grammatical errors or is not a natural English sentence.
Table 3 summarizes the annotations. For each score range, we report the number of pairs, the mean trigram overlap score, and the number of times each paraphrase/ﬂuency label was present in the sample of 100 pairs. There is noise in the dataset but it is largely conﬁned in the bottom two ranges which together comprise only 16%
3Since the range of values is constrained to be ≤ 1, and most values are positive, we split it up into 5 evenly spaced segments as shown in Table 3.

Models. We want to embed a word sequence s into a ﬁxed-length vector. We denote the tth word in s as st, and we denote its word embedding by xt. We focus on three model families, though we also experiment with combining them in various ways. The ﬁrst, which we call WORD, simply averages the embeddings xt of all words in s. This model was found by Wieting et al. (2016b) to perform strongly for semantic similarity tasks.
The second is similar to WORD, but instead of word embeddings, we average character trigram embeddings (Huang et al., 2013). We call this TRIGRAM. Wieting et al. (2016a) found this to work well for sentence embeddings compared to other n-gram orders and to word averaging.
The third family includes long short-term memory (LSTM) architectures (Hochreiter and Schmidhuber, 1997). We average the hidden states to produce the ﬁnal sentence embedding. For regularization during training, we scramble words with a small probability (Wieting and Gimpel, 2017). We also experiment with bidirectional LSTMs (BLSTM), averaging the forward and backward hidden states with no concatenation.4
4Unlike Conneau et al. (2017), we found this to outperform max-pooling for both semantic similarity and general sentence embedding tasks.

Training. The training data is a set S of paraphrase pairs s, s and we minimize a marginbased loss (s, s ) =
max(0, δ − cos(g(s), g(s )) + cos(g(s), g(t)))
where g is the model (WORD, TRIGRAM, etc.), δ is the margin, and t is a “negative example” taken from a mini-batch during optimization. The intuition is that we want the two texts to be more similar to each other than to their negative examples. To select t we choose the most similar sentence in some set. For simplicity we use the mini-batch for this set, i.e.,
t = argmax cos(g(s), g(t ))
t : t ,· ∈Sb\{ s,s }
where Sb ⊆ S is the current mini-batch.
Modiﬁcation: mega-batching. By using the mini-batch to select negative examples, we may be limiting the learning procedure. That is, if all potential negative examples in the mini-batch are highly dissimilar from s, the loss will be too easy to minimize. Stronger negative examples can be obtained by using larger mini-batches, but large mini-batches are sub-optimal for optimization.
Therefore, we propose a procedure we call “mega-batching.” We aggregate M mini-batches to create one mega-batch and select negative examples from the mega-batch. Once each pair in the mega-batch has a negative example, the megabatch is split back up into M mini-batches and training proceeds. We found that this provides more challenging negative examples during learning as shown in Section 5.5. Table 6 shows results for different values of M , showing consistently higher correlations with larger M values.
5 Experiments
We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings.
5.1 Evaluation
We evaluate sentence embeddings using the SemEval semantic textual similarity (STS) tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016) and the STS Benchmark (Cer et al., 2017). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different

topics and 5 means they are completely equivalent. As our test set, we report the average Pearson’s r over each year of the STS tasks from 2012-2016. We use the small (250-example) English dataset from SemEval 2017 (Cer et al., 2017) as a development set, which we call STS2017 below.
Section A.1 in the appendix contains a description of a method to obtain a paraphrase lexicon from PARANMT-50M that is on par with that provided by PPDB. In Section A.2 in the appendix, we also evaluate our sentence embeddings on a range of tasks that have previously been used for evaluating sentence representations (Kiros et al., 2015).
5.2 Experimental Setup
For training sentence embeddings on PARANMT50M, we follow the experimental procedure of Wieting et al. (2016b). We use PARAGRAMSL999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix for all models that use word embeddings. We ﬁx the mini-batch size to 100 and the margin δ to 0.4. We train all models for 5 epochs. For optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 0.001. For the LSTM and BLSTM, we ﬁxed the scrambling rate to 0.3.5
5.3 Dataset Comparison
We ﬁrst compare parallel data sources. We evaluate the quality of a data source by using its backtranslations paired with its English references as training data for paraphrastic sentence embeddings. We compare the four data sources described in Section 3. We use 100K samples from each corpus and trained 3 different models on each: WORD, TRIGRAM, and LSTM. Table 4 shows that CzEng provides the best training data for all models, so we use it to create PARANMT-50M and in all remaining experiments.
CzEng is diverse in terms of vocabulary and has highly-diverse sentence structures. It has significantly shorter sentences than the other corpora, and has much more training data, so its translations are expected to be better than those in the other corpora. Wieting et al. (2017) found that sentence length was the most important factor in
5Like Wieting and Gimpel (2017), we found that scrambling signiﬁcantly improves results, even though we use much more training data than they used. But while they used a scrambling rate of 0.5, we found that a smaller rate of 0.3 worked better, presumably due to the larger training set.

Training Corpus Common Crawl CzEng 1.6 Europarl News Commentary

WORD 80.9 83.6 78.9 80.2

TRIGRAM 80.2 81.5 78.0 78.2

LSTM 79.1 82.5 80.4 80.5

Table 4: Pearson’s r × 100 on STS2017 when training on 100k pairs from each back-translated parallel corpus. CzEng works best for all models.

ﬁltering quality training data, presumably due to how NMT quality deteriorates with longer sentences. We suspect that better translations yield better data for training sentence embeddings.

5.4 Data Filtering
Since the PARANMT-50M dataset is so large, it is computationally demanding to train sentence embeddings on it in its entirety. So, we ﬁlter the data to create a training set for sentence embeddings.
We experiment with three simple methods. We ﬁrst try using the length-normalized translation score from decoding. Second, we use trigram overlap ﬁltering as done by Wieting et al. (2017).6 Third, we use the paraphrase score from Section 3.
We ﬁltered the back-translated CzEng data using these three strategies. We ranked all 51M+ paraphrase pairs in the dataset by the ﬁltering measure under consideration and then split the data into tenths (so the ﬁrst tenth contains the bottom 10% under the ﬁltering criterion, the second contains those in the bottom 10-20%, etc.).
We trained WORD, TRIGRAM, and LSTM models for a single epoch on 1M examples sampled from each of the ten folds for each ﬁltering criterion. We averaged the correlation on the STS2017 data across models for each fold. Table 5 shows the results of the ﬁltering methods. Filtering based on PARAGRAM-PHRASE similarity produces the best data for training sentence embeddings.

Filtering Method Trigram Overlap Translation Score Para. Score

Model Avg. 83.1 83.2 83.3

Table 5: Pearson’s r × 100 on STS2017 for the best training fold across the average of WORD, TRIGRAM, and LSTM models for each ﬁltering method.
6Trigram overlap is calculated by counting trigrams in the reference and translation, then dividing the number of shared trigrams by the total number in the reference or translation, whichever has fewer.

We randomly selected 5M examples from the top two scoring folds using PARAGRAM-PHRASE ﬁltering, ensuring that we only selected examples in which both sentences have a maximum length of 30 tokens.7 These resulting 5M examples form the training data for the rest of our experiments. Note that many more than 5M pairs from the dataset are useful, as suggested by our human evaluations in Section 3.2. We have experimented with doubling the training data when training our best sentence similarity model and found the correlation increased by more than half a percentage point on average across all datasets.

5.5 Effect of Mega-Batching
Table 6 shows the impact of varying the megabatch size M when training for 5 epochs on our 5M-example training set. For all models, larger mega-batches improve performance. There is a smaller gain when moving from 20 to 40, but all models show clear gains over M = 1.

M WORD TRIGRAM LSTM

1 82.3

81.5

81.5

20 84.0

83.1

84.6

40 84.1

83.4

85.0

Table 6: Pearson’s r × 100 on STS2017 with different mega-batch sizes M .

sentence: sir, i’m just trying to protect. negative examples: M = 1 i mean, colonel... M = 20 i only ask that the baby be safe. M = 40 just trying to survive. on instinct.

sentence: M =1 M = 20 M = 40

i’m looking at him, you know? they know that i’ve been looking for her. i’m keeping him. i looked at him with wonder.

sentence: M =1 M = 20 M = 40

i’il let it go a couple of rounds. sometimes the ball doesn’t go down. i’ll take two. i want you to sit out a couple of rounds, all right?

Table 7: Negative examples for various megabatch sizes M with the BLSTM model.
Table 7 shows negative examples with different mega-batch sizes M . We use the BLSTM model and show the negative examples (nearest neighbors from the mega-batch excluding the current training example) for three sentences. Using
7Wieting et al. (2017) investigated methods to ﬁlter backtranslated parallel text. They found that sentence length cutoffs were effective for ﬁltering.

Training Data Our Work PARANMT
SimpWiki STS Competitions
Related Work

Model
WORD TRIGRAM LSTM LSTM BLSTM WORD + TRIGRAM (addition) WORD + TRIGRAM + LSTM (addition) WORD, TRIGRAM (concatenation) WORD, TRIGRAM, LSTM (concatenation) WORD, TRIGRAM (concatenation) 1st Place System 2nd Place System 3rd Place System
InferSent (AllSNLI) (Conneau et al., 2017) InferSent (SNLI) (Conneau et al., 2017) FastSent (Hill et al., 2016) DictRep (Hill et al., 2016) SkipThought (Kiros et al., 2015) CPHRASE (Pham et al., 2015) CBOW (from Hill et al., 2016) BLEU (Papineni et al., 2002) METEOR (Denkowski and Lavie, 2014)

Dim. 300 300 300 900 900 300 300 600 900 600
-
-
4096 4096 100 500 4800
500
-

2012 66.2 67.2 67.0 68.0 67.4 67.3 67.1 67.8 67.7 61.8
64.8
63.4
64.1 58.6 57.1
39.2 53.4

2013 61.8 60.3 62.3 60.4 60.2 62.8 62.8 62.7 62.8 58.4
62.0
59.1
58.3 51.5 50.4
29.5 47.6

2014 76.2 76.1 76.3 76.3 76.1 77.5 76.8 77.4 76.9 74.4
74.3
74.2
74.3 67.8 66.2 63 67 29 65 64 42.8 63.7

2015 79.3 79.7 78.5 78.8 79.5 80.1 79.2 80.3 79.8 77.0
79.0
78.0
77.8 68.3 65.2
49.8 68.8

2016 77.5 78.3 76.0 75.9 76.5 78.2 77.0 78.1 76.8 74.0
77.7
75.7
75.7 67.2 63.5
47.4 61.8

Table 8: Pearson’s r × 100 on the STS tasks of our models and those from related work. We compare to the top performing systems from each SemEval STS competition. Note that we are reporting the mean correlations over domains for each year rather than weighted means as used in the competitions. Our best performing overall model (WORD, TRIGRAM) is in bold.

Our Work (Unsupervised) WORD TRIGRAM LSTM WORD + TRIGRAM (addition) WORD + TRIGRAM + LSTM (addition) WORD, TRIGRAM (concatenation) WORD, TRIGRAM, LSTM (concatenation) Related Work (Unsupervised) InferSent (AllSNLI) (Conneau et al., 2017) C-PHRASE (Pham et al., 2015) GloVe (Pennington et al., 2014) word2vec (Mikolov et al., 2013) sent2vec (Pagliardini et al., 2017) Related Work (Supervised) Dep. Tree LSTM (Tai et al., 2015) Const. Tree LSTM (Tai et al., 2015) CNN (Shao, 2017)

Dim.
300 300 300 300 300 600 900
4096
300 300 700

Corr.
79.2 79.1 78.4 79.9 79.6 79.9 79.2
70.6 63.9 40.6 56.5 75.5
71.2 71.9 78.4

Table 9: Pearson’s r × 100 on STS Benchmark test set.

larger mega-batches improves performance, presumably by producing more compelling negative examples for the learning procedure. This is likely more important when training on sentences than prior work on learning from text snippets (Wieting et al., 2015, 2016b; Pham et al., 2015).

Models WORD / TRIGRAM WORD / LSTM TRIGRAM / LSTM

Mean Pearson Abs. Diff. 2.75 2.17 2.89

Table 10: The means (over all 25 STS competition datasets) of the absolute differences in Pearson’s r between each pair of models.

5.6 Model Comparison
Table 8 shows the results on the STS tasks from 2012-2016, and Table 9 shows results on the STS Benchmark.8 Our best models outperform all STS competition systems and all related work of which we are aware on the STS datasets. Note that the large improvement over BLEU and METEOR implies that our embeddings could be useful for evaluating machine translation output.
Overall, our individual models (WORD, TRIGRAM, LSTM) perform similarly. Using 300 dimensions appears to be sufﬁcient; increasing dimensionality does not necessarily improve correlation. When examining particular STS tasks, we found that our individual models showed marked differences on certain tasks. Table 10 shows the mean absolute difference in Pearson’s r over all 25
8Baseline results are from http://ixa2.si.ehu. es/stswiki/index.php/STSbenchmark, except for the unsupervised InferSent result which we computed.

Template
original (SBARQ(ADVP)(,)(S)(,)(SQ)) (S(NP)(ADVP)(VP))
original (S(SBAR)(,)(NP)(VP)) (S(‘‘)(UCP)(’’)(NP)(VP))
original -
original -

Paraphrase
with the help of captain picard, the borg will be prepared for everything. now, the borg will be prepared by picard, will it? the borg here will be prepared for everything.
you seem to be an excellent burglar when the time comes. when the time comes, you’ll be a great thief. “you seem to be a great burglar, when the time comes.” you said.
overall, i that it’s a decent buy, and am happy that i own it. it’s a good buy, and i’m happy to own it.
oh, that’s a handsome women, that is. that’s a beautiful woman.

Table 11: The top two rows in the table show syntactically controlled paraphrases generated by the SCPN. The bottom two rows are examples from our paraphrase model that are able to canonicalize text and even correct grammar mistakes.

datasets. The TRIGRAM model shows the largest differences from the other two, both of which use word embeddings. This suggests that TRIGRAM may be able to complement the other two by providing information about words that are unknown to models that rely on word embeddings.
We experiment with two ways of combining models. The ﬁrst is to deﬁne additive architectures that form the embedding for a sentence by adding the embeddings computed by two (or more) individual models. All parameters are trained jointly just like when we train individual models; that is, we do not ﬁrst train two simple models and add their embeddings. The second way is to deﬁne concatenative architectures that form a sentence embedding by concatenating the embeddings computed by individual models, and again to train all parameters jointly.
In Table 8 and Table 9, these combinations show consistent improvement over the individual models as well as the larger LSTM and BLSTM. Concatenating WORD and TRIGRAM results in the best performance on average across STS tasks, outperforming the best supervised systems from each year. We will release the pretrained model for these “WORD, TRIGRAM” embeddings upon publication. In addition to providing a strong baseline for future STS tasks, our embeddings offer the advantages of being extremely efﬁcient to compute and being robust to unknown words.
We show the usefulness of PARANMT by also reporting the results of training the “WORD, TRIGRAM” model on SimpWiki, a dataset of aligned sentences from Simple English and standard English Wikipedia (Coster and Kauchak, 2011). It has been shown useful for training sentence embeddings in past work (Wieting and Gimpel, 2017). However, Table 8 shows that training on

PARANMT leads to gains in correlation of 3 to 6 points.
6 Paraphrase Generation
Besides creating state-of-the-art paraphrastic sentence embeddings, our dataset is useful for paraphrase generation for purposes of augmenting data and creating adversarial examples. The work is described fully in (Iyyer et al., 2018), where their model, the Syntactically Controlled Paraphrase Network (SCPN), is trained to generate a paraphrase of a sentence whose constituent structure follows a provided parse template. These parse templates are the top two levels of the linearized parse tree (the level immediately below the root along with the root).
We have also found that training an encoderdecoder model on PARANMT-50M can lead to a model that canonicalizes text. For this experiment, we used a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder and a twolayer LSTM decoder with soft attention over the encoded states!(Bahdanau et al., 2014). The attention computation consists of a bilinear product with a learned parameter matrix.
Table 11, shows two examples from each of these models. Notice how the for the SCPN, the transformation preserves the semantics of the sentence while changing its syntax to ﬁt the templates. The latter two examples show the canonicalization effect where the model is able to correct grammatical errors and standardize the output. This canonicalization would be interesting to explore for automatic grammar correction as it does so without any direct supervision. Future work could also use this canonicalization to improve performance of models by standardizing inputs and removing noise from data.

These were the ﬁrst studies using PARANMT50M to generate paraphrases, and we believe that PARANMT-50M and future datasets like it, can be used to generate rich paraphrases that improve the performance and robustness of models on a multitude of NLP tasks and leave this future exploration.
7 Conclusion
We described the creation of PARANMT-50M, a dataset of more than 50M English sentential paraphrase pairs. We showed how to use PARANMT50M to train paraphrastic sentence embeddings that outperform supervised systems on STS tasks, as well as how it can be used for generating paraphrases for purposes of data augmentation, robustness, and even grammar correction.
The key advantage of our approach is that it only requires parallel text. There are hundreds of millions of parallel sentence pairs, and more are being generated continually. Our procedure is immediately applicable to the wide range of languages for which we have parallel text.
We release PARANMT-50M, our code, and pretrained sentence embeddings, which also exhibit strong performance as general-purpose representations for a multitude of tasks.9 We hope that PARANMT-50M, along with our embeddings, can impart a notion of meaning equivalence to improve NLP systems for a variety of tasks. We are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and paraphrase generation for data augmentation and ﬁnding adversarial examples.
Acknowledgments
We thank the developers of Theano (Theano Development Team, 2016), the developers of PyTorch (Paszke et al., 2017), and NVIDIA Corporation for donating GPUs used in this research.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
9We will release code and embeddings under the permissive MIT and CC BY 4.0 licenses. We will work with the CzEng developers to release PARANMT-50M under the most permissive license possible, but as CzEng is only available for non-commercial research purposes, we may be restricted to release PARANMT-50M under the same license as CzEng.

Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. Proceedings of SemEval, pages 497–511.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In Proceedings of the International Conference on Learning Representations.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.
Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th annual meeting on Association for Computational Linguistics, pages 50–57. Association for Computational Linguistics.
Ondˇrej Bojar, Ondˇrej Dusˇek, Tom Kocmi, Jindˇrich Libovicky´, Michal Nova´k, Martin Popel, Roman Sudarikov, and Dusˇan Varisˇ. 2016. CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered. In Text, Speech, and Dialogue:

19th International Conference, TSD 2016, number 9924 in Lecture Notes in Computer Science, pages 231–238, Cham / Heidelberg / New York / Dordrecht / London. Masaryk University, Springer International Publishing.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and D. Christopher Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364.
William Coster and David Kauchak. 2011. Simple english wikipedia: a new text simpliﬁcation task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 665–669. Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language speciﬁc translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics.
William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.
Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retroﬁtting word vectors to semantic lexicons. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. 2017. Learning generic sentence representations using convolutional neural networks. In Proceedings of EMNLP, pages 2380–2390, Copenhagen, Denmark. Association for Computational Linguistics.
Juri Ganitkevitch and Chris Callison-Burch. 2014. The multilingual paraphrase database. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014).

Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In Proceedings of HLT-NAACL.
Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. 2014a. Embedding word similarity with neural machine translation. arXiv preprint arXiv:1412.6448.
Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. 2014b. Not all neural embeddings are born equal. arXiv preprint arXiv:1410.0718.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Felix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4).
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8).
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of NAACL.
Yangfeng Ji and Jacob Eisenstein. 2013. Discriminative improvements to distributional sentence similarity. In EMNLP, pages 891–896.
Youxuan Jiang, Jonathan K. Kummerfeld, and Walter S. Lasecki. 2017. Understanding task design trade-offs in crowdsourced paraphrase collection. In Proceedings of ACL, pages 103–109, Vancouver, Canada. Association for Computational Linguistics.
Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. arXiv preprint arXiv:1506.06726.

Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A denotational and distributional approach to semantics. In SemEval@ COLING, pages 329–334.
Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential paraphrases. In Proceedings of EMNLP, Copenhagen, Denmark. Association for Computational Linguistics.
Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.
Xin Li and Dan Roth. 2002. Learning question classiﬁers. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.
Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. 2017. Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 881–893.
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6297–6308.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.
Nikola Mrksˇic´, Diarmuid O´ Se´aghdha, Blaise Thomson, Milica Gasˇic´, Lina Rojas-Barahona, PeiHao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-ﬁtting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. arXiv preprint arXiv:1703.02507.

Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL, page 271. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL, pages 115–124. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318. Association for Computational Linguistics.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch.
Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2015. Ppdb 2.0: Better paraphrase ranking, ﬁnegrained entailment relations, word embeddings, and style classiﬁcation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 425–430.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2014).
Nghia The Pham, Germa´n Kruszewski, Angeliki Lazaridou, and Marco Baroni. 2015. Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of EMNLP.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. 2017. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444.
Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel La¨ubli, Antonio Valerio Miceli Barone, Jozef Mokry, et al. 2017. Nematus: a toolkit for neural machine translation. arXiv preprint arXiv:1703.04357.
Yang Shao. 2017. Hcti at semeval-2017 task 1: Use convolutional neural network to evaluate semantic textual similarity. In Proceedings of the

11th International Workshop on Semantic Evaluation (SemEval-2017), pages 130–133.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.
Yui Suzuki, Tomoyuki Kajiwara, and Mamoru Komachi. 2017. Building a non-trivial paraphrase corpus using multiple machine translation systems. In Proceedings of ACL 2017, Student Research Workshop, pages 36–42, Vancouver, Canada. Association for Computational Linguistics.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2):165–210.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016a. Charagram: Embedding words and sentences via character n-grams. In Proceedings of EMNLP.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016b. Towards universal paraphrastic sentence embeddings. In Proceedings of ICLR.
John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, and Dan Roth. 2015. From paraphrase database to compositional paraphrase model and back. Transactions of the ACL (TACL).
John Wieting and Kevin Gimpel. 2017. Revisiting recurrent networks for paraphrastic sentence embeddings. In Proceedings of ACL.
John Wieting, Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. In Proceedings of EMNLP.

Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.
Wei Xu, Chris Callison-Burch, and William B Dolan. 2015. SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).
Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics, 2:435–448.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. In Proceedings of IJCAI.
Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classiﬁcation improved by integrating bidirectional lstm with twodimensional max pooling. In Proceedings of COLING, pages 3485–3495, Osaka, Japan. The COLING 2016 Organizing Committee.
A Appendix
A.1 Paraphrase Lexicon
While PARANMT-50M consists of sentence pairs, we demonstrate how a paraphrase lexicon can be extracted from it. One simple approach is to extract and rank word pairs u, v using the crosssentence pointwise mutual information (PMI):
#(u, v)#(·, ·) PMIcross(u, v) = log #(u)#(v)
where joint counts #(u, v) are incremented when u appears in a sentence and v appears in its paraphrase. The marginal counts (e.g., #(u)) are computed based on single-sentence counts, as in ordinary PMI. This works reasonably well but is not able to differentiate words that frequently occur in paraphrase pairs from words that simply occur frequently together in the same sentence. For example, “Hong” and “Kong” have high cross-sentence PMI. We can improve the score by subtracting the ordinary PMI that computes joint counts based on single-sentence co-occurrences. We call the result the adjusted PMI:
PMIadj(u, v) = PMIcross(u, v) − PMI(u, v)
Before computing these PMIs from PARANMT50M, we removed sentence pairs with a paraphrase score less than 0.35 and where either sen-

laughed

PPDB PARANMT-50M

respectful

PPDB PARANMT-50M

giggled, smiled, funny, used, grew, bust, ri, did chortled, guffawed, pealed, laughin, laughingstock, cackled, chuckled, snickered, mirthless, chuckling, jeered, laughs, laughing, taunted, burst, cackling, scoffed, humorless, barked,...
respect, respected, courteous, disrespectful, friendly, respecting, respectable, humble, environmentally-friendly, child-friendly, digniﬁed, respects, compliant, sensitive, abiding,... reverent, deferential, revered, respectfully, awed, respect, respected, respects, respectable, politely, considerate, treat, civil, reverence, polite, keeping, behave, proper, digniﬁed, decent,...

Table 12: Example lexical paraphrases from PPDB ranked using the PPDB 2.0 scoring function and from the paraphrase lexicon we induced from PARANMT-50M ranked using adjusted PMI.

tence is longer than 30 tokens. When computing the ordinary PMI with single-sentence context, we actually compute separate versions of this PMI score for translations and references in each PARANMT-50M pair, then we average them together. We did this because the two sentences in each pair have highly correlated information, so computing PMI on each half of the data would correspond to capturing natural corpus statistics in a standard application of PMI.

Dataset PPDB L PPDB XL PPDB XXL PPDB XXXL PARANMT-50M PARANMT-50M

Score PPDB 2.0 PPDB 2.0 PPDB 2.0 PPDB 2.0 cross-sentence PMI adjusted PMI

ρ × 100 37.97 52.32 60.44 61.47 52.12 61.59

Table 13: Evaluation of scored paraphrase lexicons on SimLex-999, showing Spearman’s ρ × 100.

Table 13 shows an evaluation of the resulting score functions on the SimLex-999 word similarity dataset (Hill et al., 2015). As a baseline, we use the lexical portion of PPDB 2.0 (Pavlick et al., 2015), evaluating its ranking score as a similarity score and assigning a similarity of 0 to unseen word pairs.10 Our adjusted PMI computed from PARANMT-50M is on par with the best PPDB lexicon.
Table 12 shows examples from PPDB and our paraphrase lexicon computed from PARANMT50M. Paraphrases from PPDB are ordered by the PPDB 2.0 scoring function. Paraphrases from our lexicon are ordered using our adjusted PMI scoring function; we only show paraphrases that appeared at least 10 times in PARANMT-50M.

10If both orderings for a SimLex word pair appear in PPDB, we average their PPDB 2.0 scores. If multiple lexical entries are found with different POS tags, we take the ﬁrst instance.

A.2 General-Purpose Sentence Embedding Evaluations
We evaluate our sentence embeddings on a range of tasks that have previously been used for evaluating sentence representations (Kiros et al., 2015). These include sentiment analysis (MR, Pang and Lee, 2005; CR, Hu and Liu, 2004; SST, Socher et al., 2013), subjectivity classiﬁcation (SUBJ; Pang and Lee, 2004), opinion polarity (MPQA; Wiebe et al., 2005), question classiﬁcation (TREC; Li and Roth, 2002), paraphrase detection (MRPC; Dolan et al., 2004), semantic relatedness (SICK-R; Marelli et al., 2014), and textual entailment (SICK-E). We use the SentEval package from Conneau et al. (2017) to train models on our ﬁxed sentence embeddings for each task.11
Table 14 shows results on the general sentence embedding tasks. Each of our individual models produces 300-dimensional sentence embeddings, which is far fewer than the several thousands (often 2400-4800) of dimensions used in most prior work. While using higher dimensionality does not improve correlation on the STS tasks, it does help on the general sentence embedding tasks. Using higher dimensionality leads to more trainable parameters in the subsequent classiﬁers, increasing their ability to linearly separate the data.
To enlarge the dimensionality, we concatenate the forward and backward states prior to averaging. This is similar to Conneau et al. (2017), though they used max pooling. We experimented with both averaging (“BLSTM (Avg., concatenation)”) and max pooling (“BLSTM (Max, concatenation)”) using recurrent networks with 2048-dimensional hidden states, so concatenating them yields a 4096-dimension embedding. These high-dimensional models outperform SkipThought (Kiros et al., 2015) on all tasks except SUBJ and TREC. Nonetheless, the In-

11Available

at

https://github.com/

facebookresearch/SentEval.

Model

Dim. MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E

Unsupervised (Unordered Sentences)

Unigram-TFIDF (Hill et al., 2016)

73.7 79.2 90.3 82.4 - 85.0 73.6/81.7

-

-

SDAE (Hill et al., 2016)

2400 74.6 78.0 90.8 86.9 - 78.4 73.7/80.7

-

-

Unsupervised (Ordered Sentences)

FastSent (Hill et al., 2016)

100 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3

-

-

FastSent+AE (Hill et al., 2016)

71.8 76.7 88.8 81.5 - 80.4 71.2/79.1

-

-

SkipThought (Kiros et al., 2015)

4800 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 85.8

82.3

Unsupervised (Structured Resources)

DictRep (Hill et al., 2016)

500 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8

-

-

NMT En-to-Fr (Hill et al., 2016)

2400 64.7 70.1 84.9 81.5 - 82.8

-

BYTE mLSTM (Radford et al., 2017)

4096 86.9 91.4 94.6 88.5 - - 75.0/82.8 79.2

-

Individual Models (Our Work)

WORD

300 75.8 80.5 89.2 87.1 80.0 80.1 68.6/80.9 83.6

80.6

TRIGRAM

300 68.8 75.5 83.6 82.3 73.6 73.0 71.4/82.0 79.3

78.0

LSTM

300 73.8 78.4 88.5 86.5 80.6 76.8 73.6/82.3 83.9

81.9

LSTM

900 75.8 81.7 90.5 87.4 81.6 84.4 74.7/83.0 86.0

83.0

BLSTM

900 75.6 82.4 90.6 87.7 81.3 87.4 75.0/82.9 85.8

84.4

Mixed Models (Our Work)

WORD + TRIGRAM (addition)

300 74.8 78.8 88.5 87.4 78.7 79.0 71.4/81.4 83.2

80.6

WORD + TRIGRAM + LSTM (addition)

300 75.0 80.7 88.6 86.6 77.9 78.6 72.7/80.8 83.6

81.8

WORD, TRIGRAM (concatenation)

600 75.8 80.5 89.9 87.8 79.7 82.4 70.7/81.7 84.6

82.0

WORD, TRIGRAM, LSTM (concatenation) 900 77.6 81.4 91.4 88.2 82.0 85.4 74.0/81.5 85.4

83.8

BLSTM (Avg., concatenation)

4096 77.5 82.6 91.0 89.3 82.8 86.8 75.8/82.6 85.9

83.8

BLSTM (Max, concatenation)

4096 76.6 83.4 90.9 88.5 82.0 87.2 76.6/83.5 85.3

82.5

Supervised (Transfer)

InferSent (SST) (Conneau et al., 2017)

4096 - 83.7 90.2 89.5 - 86.0 72.7/80.9 86.3

83.1

InferSent (SNLI) (Conneau et al., 2017) 4096 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 88.5

86.3

InferSent (AllNLI) (Conneau et al., 2017) 4096 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 88.4

86.3

Supervised (Direct)

Naive Bayes - SVM

79.4 81.8 93.2 86.3 83.1 -

-

-

-

AdaSent (Zhao et al., 2015)

83.1 86.3 95.5 93.3 - 92.4

-

-

-

BLSTM-2DCNN (Zhou et al., 2016)

82.3 - 94.0

- 89.5 96.1

-

-

-

TF-KLD (Ji and Eisenstein, 2013)

-

-

-

- - - 80.4/85.9

-

-

Illinois-LH (Lai and Hockenmaier, 2014)

-

-

-

- --

-

-

84.5

Dependency Tree-LSTM (Tai et al., 2015)

-

-

-

- --

-

86.8

-

Table 14: General-purpose sentence embedding tasks, divided into categories based on resource requirements.

ferSent (Conneau et al., 2017) embeddings trained on AllNLI still outperform our embeddings on nearly all of these general-purpose tasks.
We also note that on ﬁve tasks (SUBJ, MPQA, SST, TREC, and MRPC), all sentence embedding methods are outperformed by supervised baselines. These baselines use the same amount of supervision as the general sentence embedding methods; the latter actually use far more data overall than the supervised baselines. This suggests that the pretrained sentence representations are not capturing the features learned by the models engineered for those tasks.
We take a closer look of how our embeddings compare to InferSent (Conneau et al., 2017). InferSent is a supervised model trained on a large textual entailment dataset (the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), which consist of nearly 1 million human-

labeled examples).
While InferSent has strong performance across all downstream tasks, our model obtains better results on semantic similarity tasks. It consistently reach correlations approximately 10 points higher than those of InferSent.
Regarding the general-purpose tasks, we note that some result trends appear to be inﬂuenced by the domain of the data. InferSent is trained on a dataset of mostly captions, especially the model trained on just SNLI. Therefore, the datasets for the SICK relatedness and entailment evaluations are similar in domain to the training data of InferSent. Further, the training task of natural language inference is aligned to the SICK entailment task. Our results on MRPC and entailment are signiﬁcantly better than SkipThought, and on a paraphrase task that does not consist of caption data (MRPC), our embeddings are competitive with In-

ferSent. To quantify these domain effects, we performed additional experiments that are described in Section A.2.1.
There are many ways to train sentence embeddings, each with its own strengths. InferSent, our models, and the BYTE mLSTM of Radford et al. (2017) each excel in particular classes of downstream tasks. Ours are specialized for semantic similarity. BYTE mLSTM is trained on review data and therefore is best at the MR and CR tasks. Since the InferSent models are trained using entailment supervision and on caption data, they excel on the SICK tasks. Future work will be needed to combine multiple supervision signals to generate embeddings that perform well across all tasks.
A.2.1 Effect of Training Domain on InferSent
We performed additional experiments to investigate the impact of training domain on downstream tasks. We ﬁrst compare the performance of our “WORD, TRIGRAM (concatenation)” model to the InferSent SNLI and AllNLI models on all STS tasks from 2012-2016. We then compare the overall mean with that of the three caption STS datasets within the collection. The results are shown in Table 15. The InferSent models are much closer to our WORD, TRIGRAM model on the caption datasets than overall, and InferSent trained on SNLI shows the largest difference between its overall performance and its performance on caption data.

Data Overall mean diff. MSRvid (2012) diff. Images (2014) diff. Images (2015) diff.

AllNLI 10.5 5.2 6.4 3.6

SNLI 12.5 4.6 4.8 3.0

Table 15: Difference in correlation (Pearson’s r × 100) between “WORD, TRIGRAM” and InferSent models trained on two different datasets: AllNLI and SNLI. The ﬁrst row is the mean difference across all 25 datasets, then the following rows show differences on three individual datasets that are comprised of captions. The InferSent models are much closer to our model on the caption datasets than overall.

We also compare the performance of these models on the STS Benchmark under several conditions (Table 16). We ﬁrst compare unsupervised results on the entire test set, the subset consisting of captions (3,250 of the 8,628 examples in the test set), and the remainder. We include analogous re-

Model Unsupervised InferSent (AllNLI) InferSent (SNLI) WORD, TRIGRAM Supervised InferSent (AllNLI) InferSent (SNLI)

All Cap. No Cap.
70.6 83.0 56.6 67.3 83.4 51.7 79.9 87.1 71.7
75.9 85.4 64.8 75.9 86.4 63.1

Table 16: STS benchmark results (Pearson’s r × 100) comparing our WORD, TRIGRAM model to InferSent trained on AllNLI and SNLI. Unsupervised results were obtained by simply using cosine similarity of the pretrained embeddings on the test set with no training or tuning. Supervised results were obtained by training and tuning using the training and development data of the STS Benchmark. We report results using all of the data (All), only the caption portion of the data (Cap.), and all of the data except for the captions (No Cap.).

sults in the supervised setting, where we ﬁlter the respective training and development sets in addition to the test sets. Compared to our model, InferSent shows a much larger gap between captions and non-captions, providing evidence of a bias. Note that this bias is smaller for the model trained on AllNLI, as its training data includes other domains.

