arXiv:2203.07206v1 [cs.LG] 14 Mar 2022

Improving State-of-the-Art in One-Class Classiﬁcation by Leveraging Unlabeled Data
Farid Bagirova, Dmitry Ivanova, Aleksei Shpilmana
aJetBrains Research
Abstract When dealing with binary classiﬁcation of data with only one labeled class
data scientists employ two main approaches, namely One-Class (OC) classiﬁcation and Positive Unlabeled (PU) learning. The former only learns from labeled positive data, whereas the latter also utilizes unlabeled data to improve the overall performance. Since PU learning utilizes more data, we might be prone to think that when unlabeled data is available, the go-to algorithms should always come from the PU group. However, we ﬁnd that this is not always the case if unlabeled data is unreliable, i.e. contains limited or biased latent negative data. We perform an extensive experimental study of a wide list of state-of-the-art OC and PU algorithms in various scenarios as far as unlabeled data reliability is concerned. Furthermore, we propose PU modiﬁcations of state-of-the-art OC algorithms that are robust to unreliable unlabeled data, as well as a guideline to similarly modify other OC algorithms. Our main practical recommendation is to use state-of-the-art PU algorithms when unlabeled data is reliable and to use the proposed modiﬁcations of state-of-the-art OC algorithms otherwise. Additionally, we outline procedures to distinguish the cases of reliable and unreliable unlabeled data using statistical tests.1.
1The code is available at https://github.com/jbr-ai-labs/PU-OC

1. Introduction
An input of a supervised binary classiﬁer consists of two sets of examples: positive and negative. However, the access to labeled samples from both classes can be restricted in many realistic scenarios. A particularly well-studied restriction is the absence of labeled negative examples, which appears in malfunction detection, medical anomalies detection, etc. [5]. One of the approaches that deal with this is One-Class (OC) classiﬁcation [41]. Typically, OC algorithms treat available positive examples as normal data and try to separate it from unseen data, that is often referred to as either anomalies or outliers [22, 27, 7, 6, 5]. Further studies noted that OC algorithms always make assumptions about negative data distribution [55, 49, 50]. For example, some methods assume negative distribution to be uniform [60, 56] or concentrated where positive data are rare [59, 47], model negative distribution as a Gaussian [45], or separate positive data from the origin [53]. This may lead to errors if unlabeled data are not distributed according to these assumptions [63].
Another approach to classiﬁcation in the absence of labeled negative data is Positive-Unlabeled (PU) learning [12, 13, 37]. In addition to a labeled positive sample, PU algorithms leverage an unlabeled set of mixed positive and negative examples. In contrast to OC methods, PU methods do not make assumptions about negative data and instead approximate either negative distribution [16, 30], its statistics [15, 14, 33], or its samples [62, 40, 36, 61], by comparing positive and unlabeled data. This approach can even outperform supervised classiﬁcation, given a suﬃcient amount of unlabeled data [43, 33]. Because PU methods make fewer assumptions and have access to more data, they might seem favorable to OC methods whenever unlabeled data are at hand. However, our experiments show that dependence on unlabeled data may hinder PU methods in particularly extreme cases, which we refer to as cases of unreliable unlabeled data. We identify several such cases, including distributional shifts in unlabeled data, scarcity of unlabeled data, and scarcity of latent negative examples in unlabeled data. We present a motivational example of a possible
2

(a) OC (no shift) ROC AUC=0.92

(b) PU (no shift) ROC AUC=0.97

(c) OC (negative shift) (d) PU (negative shift)

ROC AUC=0.91

ROC AUC=0.08

Figure 1: Performance of OC and PU models on synthetic data. Subﬁgures (a) and (b) show the performance of the algorithms in the standard case when the negative distribution does not change from train to test times. Subﬁgures (c) and (d) show the eﬀects of the negative distribution shift. The OC and PU models are OC-SVM and PU-SVM, respectively (section 3.2.1). The OC model is trained on labeled positive examples. The PU model is trained on labeled positive as well as unlabeled examples, i.e. a mixture of positive and negative examples. Black dashed lines represent the decision boundaries of the optimal Bayesian classiﬁer. Overdependence on unreliable unlabeled data causes the PU method to misclassify all anomalies when a shift of the negative distribution occurs.

eﬀect of distributional shifts on PU models in Figure 1. In subplots (a, b) a PU model approximates the separating line more accurately than an OC model. Conversely, in subplots (c, d) a shift of negative distribution causes the PU model to misclassify all negative examples, whereas the OC model is unaﬀected by the shift. While the presented example is synthetic, similar situations can occur in the real world, e.g. in the problem of spam detection where the spam bots constantly improve.
One possible conclusion is to opt for using OC methods when unlabeled data is unreliable, but we aim to ﬁnd a way to construct robust methods that can learn even from unreliable unlabeled data. To this end, we propose modiﬁcations of several modern OC algorithms that leverage unlabeled data, which we refer to as PU-OC models. These modiﬁcations either are based on risk estimation techniques where the negative risk is approximated using positive and unlabeled data [33], or simply replace positive data with unlabeled in algorithm-speciﬁc routines (see Sections 3.2.4, 3.2.5). We ﬁnd that all PU-OC methods beneﬁt

3

from reliable unlabeled data, but only modiﬁcations from the second group are safe to apply to unreliable unlabeled data (i.e. either improve upon or perform on par with the original OC methods). We pinpoint this result to a crucial property: in the absence of latent negative examples, the PU-OC algorithm becomes equivalent to the original OC algorithm. Our main practical recommendation is to use state-of-the-art PU algorithms when reliable unlabeled data is available and to use the PU-OC algorithms that satisfy this property when the robustness is a concern.
A question remains how to identify the cases of unreliable unlabeled data. We ﬁnd that scarcity of latent negatives can be statistically tested by comparing predictions of OC or PU models for positive and unlabeled samples. Similarly, a shift of negative distribution can be tested by comparing predictions for unlabeled data from training and testing distributions, providing the latter is available. We provide a detailed procedure for this in subsection 5.8.
Related Work. There is a line of work in the OC literature that investigates ways to augment OC methods with additional data. Several studies ﬁnd that exposure to a small and possibly biased sample of outliers can improve the performance of OC classiﬁers [25, 48, 50]. [55] show both theoretically and empirically that unlabeled data help a classic machine learning algorithm to detect novelties. Nevertheless, there is a lack of modern literature that views OC and PU learning as diﬀerent solutions to the same problem and gives practical recommendations for diﬀerent scenarios. Our study attempts to ﬁll this gap.
The problem of unreliable unlabeled data is also underexplored in PU literature. One existing direction is concerned with robustness to class prior shift [8]. While this is a useful property, we study a more severe case of arbitrary negative distribution shifts. Furthermore, an increase in proportions of positive class can be seen as a special case of negative distribution shift. We implement the latest PU method robust to class prior shifts as a baseline [42]. Other papers are concerned with arbitrary positive shifts [24] and covariate shifts [51], but unlike us, they assume access to unbiased unlabeled data from the testing dis-
4

tribution. To the best of our knowledge, we address the problem of PU learning with shifted or scarce latent negatives for the ﬁrst time.
Our study is conceptually similar to the ﬁeld of safe semi-supervised learning where the focus is on making unlabeled data never hurt while retaining performance [39, 38, 23]. The diﬀerence is that we do not assume access to labeled negative data.

2. Problem setup
Let x ∈ Rd be a data point. Let y ∈ {0, 1} be a binary label of x that denotes its class. Let s ∈ {0, 1} be a binary label of x denotes whether x is labeled, i.e. if y is known. We view x, y, s as random variables with some joint distribution p(x, y, s). We assume that only positive data can be labeled, i.e. p(s = 1|y = 0) = 0. The probability density functions of positive (y = 1) and negative (y = 0) distributions are given by:

pp(x) := p(x | y = 1, s = 0) (1a)

pn(x) := p(x | y = 0, s = 0) (1b)

Let pu(x) be the probability density function of the unlabeled distribution, i.e. a mixture of positive and negative distributions, and α = p(y = 1|s = 0) be the mixture proportion. We assume that the probability of being labeled p(s = 1 | y = 1) is constant and independent of x, which is known as Selected Completely At Random (SCAR) setting [16]. In this case:

pu(x) := p(x|s = 0) = αpp(x) + (1 − α)pn(x)

(2)

In OC methods we assume that only a labeled sample Xp from pp(x) is available. In PU methods we assume the case-control scenario [3]: two sets of data Xp and Xu are sampled independently from fp(x) and fu(x). Both OC and PU methods output some score function proportional to p(x) := p(y = 1|x, s = 0) that separates positive and negative data. PU methods often additionally

5

estimate α. Since α is generally unidentiﬁable [4], we will focus on estimation of its upper bound α∗, as proposed in [31].

3. Methods
In this section, we ﬁrst describe the PU methods used in this study. We begin by describing Risk Estimation approach, which some of our PU-OC algorithms are based on. Then, we describe DRPU and PAN, which we use for comparison as the most recent and state-of-the-art PU algorithms. Finally, we describe the OC methods used in this study and propose their modiﬁcations that leverage unlabeled data, i.e. the PU-OC algorithms. In Appendix A.1 and Appendix A.2, we verify our choices of the state-of-the-art for both OC and PU by comparing these algorithms with other modern algorithms. Additional details about the methods are reported in Appendix B.

3.1. Positive-Unlabeled methods
3.1.1. Risk Estimation Let h(x) be an arbitrary decision function that estimates y, l(t, y) be the
loss incurred for predicting t when the ground truth is y. Deﬁne Rp+(h) = Ex∼fp (l(h(x), 1)) and Rn−(h) = Ex∼fn (l(h(x), 0)) as the positive and the negative risks. If both positive and negative data are available, the risk of the decision function can be estimated as a weighted sum of positive and negative risks (eq. 3a). In the PU setting, Rn−(h) is unavailable but can be estimated as a diﬀerence between the risks on positive and unlabeled data (eq. 3b), as shown in [15, 14].

Rpn(h) = αRp+(h) + (1 − α)Rn−(h) (3a)

Rpu(h) = αRp+(h) − αRp−(h) + Ru−(h) (3b)

Estimator (3b) is called unbiased risk-estimator and can be improved by introducing a non-negativity constraint to reduce overﬁtting [33]:

6

Rnn(h) = αRp+(h) + max(0, −αRp−(h) + Ru−(h))

(4)

Estimator (4) is called non-negative risk estimator. In practice, the decision function h is parameterized by θ, which can represent weights of a neural network or some other model. The parameters are trained to minimize Rnn(h(x | θ)) for some loss function like double hinge [14] or sigmoid [33]. We use the latter. Notice that α is assumed to be identiﬁed in this method, so in experiments, we additionally estimate it with TIcE [2].
Risk estimation can be applied to modify any OC model to leverage unlabeled data, providing this OC model is based on or can be generalized to the supervised (PN) setting. This can be done by ﬁrst replacing the OC objective with the PN objective, and then applying risk estimation to the PN objective, i.e. evaluating negative risk using positive and unlabeled samples.

3.1.2. DRPU Density Ratio estimation for PU Learning (DRPU) [42] is a new state-of-
the-art PU method that can be viewed as a combination of Risk Estimation techniques and Density Ratio estimation techniques. From the Bayes rule we have:

p(y = 1|x, s = 0) = p(x|y = 1, s = 0)p(y = 1|s = 0) = α pp(x) (5)

p(x|s = 0)

pu(x)

Equation 5 shows that PU learning can be decomposed into the problems of
estimating α (Mixture Proportion estimation) and estimating the ratio r(x) = ppup((xx)) , (Density Ratio estimation). DRPU tackles the latter problem via minimization of the Bregman divergence [57] and does not require the identiﬁed α
during training. As a result, DRPU is stable to the shifts of class proportions in
unlabeled data. After estimating r(x), the class prior α can be approximated on the validation dataset as min 1 . Note that misestimating the class prior only
xv r(xv )
shifts the predictions while retaining their relative order, and therefore does not

7

aﬀect the ROC AUC score. In our study we adapt oﬃcial implementation of DRPU 2.
3.1.3. PAN Predictive Adversarial Network (PAN) [29] is another state-of-the-art PU
algorithm based on a GANs [20]. In this algorithm, two classiﬁers C and D play a minimax game instead, which diﬀers from the generator and the discriminator trained in the standard GAN framework. The objective of the classiﬁer C is to predict the same probabilities on the unlabeled data as the discriminator D. As for the discriminator D, its loss consists of two parts: the negated loss of the classiﬁer C and the negated log-likelihood incurred by the classiﬁcation of the positive data against the unlabeled. As a result, C tries to mimic D, while D tries to both confuse C and classify the given data correctly. In the equilibrium, the two models equivalently classify the unlabeled data while distinguishing the positive data from the negative. We use our implementation of PAN based on the original paper.
3.2. One-Class methods 3.2.1. OC-SVM
OC-SVM [53] is one of the most well-known OC methods. OC-SVM modiﬁes the classical objective of Support Vector Machines and tries to separate the labeled positive data from the origin. Similarly to SVM, OC-SVM can be optimized by solving the dual problem and specifying only dot product in the feature space, i.e. kernel trick. In the case of RBF kernel (and other translationinvariant kernels [54]), OC-SVM tries to envelope the positive data while minimizing the enclosed volume and is equivalent to SVDD [11, 59]. OC-SVM solves the following optimization problem:
2https://github.com/csnakajima/pu-learning
8

12

1N

min w − r +

max(0, r − w · Φ(xi)) − r

(6)

w,r 2

νN

i=0

where Φ is a feature map in some Hilbert space, ν is both a regularization

parameter and a correction for possible data contamination, and w · Φ(xi) − r

is the decision function. In the modern literature, OC-SVM is mainly used as a

benchmark algorithm. However, new methods are still being proposed that are

based on OC-SVM ([47, 6]) or incorporate it as a part of the model ([17, 45]).

We use scikit-learn implementation of OC-SVM in our work. Traditionally,

OC-SVM is used with RBF kernel but in our implementation we use the linear

kernel. During the preliminary experiments, we discovered that such replace-

ment improves ROC AUC of OC-SVM on benchmark datasets. We train all

SVM models on features extracted with an encoder, which was trained as a

part of an autoencoder only on positive data.

PU-OC As shown in [14], linear models can be trained in the PU setting

by minimizing risk deﬁned in (3b). Since soft-margin SVM is a linear model

that minimizes empirical risk with respect to hinge-loss (square brackets in 7)

with regularization term, one can use hinge loss as h(x) and apply risk estimator

techniques to soft-margin SVM. We refer to this approach as PU-SVM.

min λ w 2 + 1 N max(0, 1 − yi(wtx − b)) (7)

w,b

N

i=1

In Appendix B.1.2, we show that similarly to SVM and OC-SVM PU-SVM

with unbiased risk estimator can be optimized by solving the dual problem, but

that all labeled positive examples are support vectors in PU-SVM. Note that

this is a secondary result that can be of independent interest. Since all labeled

examples are considered as support vectors, such solution is computationally

ineﬃcient and does not scale well with data size. Instead, we apply non-negative

risk (4) estimator for the SVM objective and solve it with Stochastic Gradient

Decent (SGD) with w as a parameter. Details of our optimization process are

reported in Appendix B.1.1.

9

3.2.2. OC-CNN OC-CNN [45] is a hybrid model for one class classiﬁcation that consist of
two parts. First, the positive data is processed by a pretrained feature extractor (ResNet, VGG, AlexNet, etc.), which is frozen during both training and inference. After the latent representation of positive data is acquired, OCCNN models the negative distribution as a Gaussian in the latent feature space. Then, a standard binary classiﬁer like a multi-layered perceptron is trained to distinguish positive latent features from the pseudo-negative Gaussian noise. Additionally, the author claim that instead of the binary classiﬁer, OC-SVM can be trained on latent representations, which also yields decent results. We implement OC-CNN according to the original paper. As in the original paper, we use ResNet pretrained on ImageNet as a feature extractor. Since pretraining is done in a supervised manner and classes from CIFAR10 overlap with classes from ImageNet, OC-CNN should not be directly compared with other OC algorithms that do not have access to such additional data. The same logic applies to the proposed PU modiﬁcation.
PU-OC Since OC-CNN assumes a particular negative distribution, it is easy to modify it for PU setup. We replace the pseudo-negative examples in the latent space with unlabeled examples processed through the same feature extractor and then perform the standard risk estimation procedure, i.e. minimize the non-negative risk in the latent space (4).
3.2.3. OC-LSTM OC-LSTM [17] is a hybrid method that can work with text and series data.
This model is based on a combination of the representation power of Long Short-Term Memory [26] and the OC-SVM objective. Speciﬁcally, the data points in a series are processed through an LSTM block and transformed to vector representations, e.g. as last hidden vector or average over all hidden vectors. After that, a single vector per series is acquired and fed into OC-SVM. Authors described two approaches to training. First, the updates of OC-SVM and LSTM can alternate: one model is frozen and the other is updated. Second,
10

both parts can be updated simultaneously via gradient decent (same procedure for OC-SVM as in our PU-SVM optimization). We employ the second approach.
PU-OC OC-LSTM with end-to-end optimization can be easily adapted to the PU setting. Since SVM is updated via SGD, we can replace it with our PU-SVM model and use same learning pipeline.
3.2.4. DROCC DROCC [21] achieves the state-of-the-art performance on several real-world
datasets across diﬀerent domains. The key assumption behind DROCC is that the positive data lie on a low-dimensional locally Euclidean manifold. Based on this assumption, the pseudo-negative examples can be generated from the available positives. The negative examples are approximated with gradient ascent (similarly to adversarial attack) with some tricks. The ascent is performed from the positive point with the current decision function used as a measure of negativity. After that, the parameters of the decision function are optimized to minimize the standard binary classiﬁcation loss between the positive and the pseudo-negative (adversarial) examples. We slightly modify the authors’ implementation3. Originally, the adversarial search is performed in the input space. Because of that, negative examples are often original positive images with some noise. Instead, we search for negative examples in the feature space obtained from the output of the middle hidden layer of the network. In our experiments, we ﬁnd that this small change improves ROC AUC.
PU-OC A major bottleneck of DROCC is the generation of pseudo-negative examples. Since this search requires access to a decision function, it can struggle to ﬁnd good approximation for negatives, especially early in the training. We propose to deal with this bottleneck using unlabeled data. In our PU-DROCC, the pseudo-negative points are generated from the unlabeled rather than the positive examples. This way, the pseudo-negative examples are closer to the real negatives if the unlabeled data contain some negative points, i.e. if α < 1. In the
3https://github.com/microsoft/EdgeML
11

extreme case when the unlabeled data contain only positives, our modiﬁcation is equivalent to the original algorithm.
3.2.5. CSI CSI [58] is an OC method based on two self-supervised techniques: con-
trastive learning and transformation predictions. Contrastive learning works with a modiﬁed dataset where each data point is augmented with one of several transformations. The key idea is to move close the embeddings of diﬀerent augmentations of the same initial objects and vice-versa move far the embeddings of diﬀerent objects. In CSI some of augmentations (e.g. rotations) are also considered as negative examples. In addition to contrastive loss CSI is learned to predict applied transformations. The anomaly score of a data point consist of two parts: contrastive representation score and classiﬁcation score. Contrastive representation score of the datapoint is the distance to the closest train point in embeddings’ space, and classiﬁcation score is an error in the prediction of transformations. Both scores should be low for positive sample. We use the oﬃcial implementation of CSI 4 with a change in neural architecture. While original study uses ResNet, we train a small convolutional network instead for a fair comparison with other algorithms
PU-OC In our PU modiﬁcation of CSI, we additionally contrast positive points with examples from unlabeled data. Like our PU-DROCC, such modiﬁcation will be close to the original model in the extreme case when unlabeled data has no negative examples, i.e. α ∼ 1.
4. Datasets
Here we describe the datasets used in this study and some preprocessing details.
4https://github.com/alinlab/CSI
12

Table 1: Datasets’ details for diﬀerent settings

Setting

Dataset

Pos class

Neg class Pos lab Pos unl Neg unl α

One-vs-all CIFAR-10

any class

all other

2500

2500

2500 0.5

classes

rnd subset

Neg shift

CIFAR-10

{0} or {2} from other 2500 2500

2500 0.5

classes

subset from

Pos modes CIFAR-10

vehicle or

all other

2500

2500

2500 0.5

classes

animal classes

Dependency on |Xu|

CIFAR-10

{0, 1, 8, 9}

all other 2500 vary
classes

vary 0.5

Dependency on α

CIFAR-10

{0, 1, 8, 9}

all other

2500

2500

2500 vary

classes

–

Abnormal

Car

–

576

573

86 0.87

–

PenDigits

–

–

2533 2509

110 0.97

–

Dec. Reviews

–

–

170

154

316 0.33

–

SMS Spam

–

–

1910 1937

609 0.76

–

Twitter

Genuine Social1 bot 423

424

818 0.33

CIFAR-10. CIFAR-10 is a standard for both OC and PU methods benchmark dataset with images from ten animal and vehicle classes [34]. There are around 6000 images for each class and the proportion of negatives and positives depends on the particular experiment.
Abnormal1001. This dataset consists of abnormal images from six classes, including Chair, Car, Airplane, Boat, Sofa, and Motorbike [52]. Normal images come from the respective classes from the PASCAL VOC dataset [18]. An example of images from this dataset is presented in Figure 2. This dataset is challenging for PU methods since the negative data are very scattered and only a few negative examples are available. We perform experiments only with Car class since it has the most examples, with 110 abnormal and 1315 normal images, α = 0.87. We use preprocessed Abnormal1001 dataset, where all images are resized to 200 × 200 resolution. Additionally for SVM-based models, we

13

preprocess both CIFAR-10 and Abnormal1001 with encoders of autoencoders trained only on positive data. For CNN-based models, we apply standard normalization to feed images to ResNet.
Pendigits. Pendigits dataset consists of consecutive pixels sampled from digits handwritten on a tablet [1]. The task is to classify the digits based on these sequences of pixels. We use the processed dataset from ODDS5 with 6870 examples in total and 156 anomalies, α = 0.97.
Text Datasets. We use two diﬀerent datasets with text data. The ﬁrst is standard dataset of deceptive reviews [44]. It consists of 400 deceptive hotel reviews acquired with crowdsourcing service and 400 truthful reviews from TripAdvisor, α = 0.33. The second is SMS Spam dataset [1]. It contains 5572 examples with 747 spam messages considered as anomalies, α = 0.76. In order to preprocess text datasets, we remove stop words and punctuation, cast characters to lower case, and apply Snowball Stemmer from the nltk package. Additionally, we use pretrained Google News embeddings6.
Twitter bots. This dataset [10, 46] provides information about around 7000 accounts, which are divided into genuine users and four diﬀerent types of bots. On this dataset, we treat only one type of bot as available in the unlabeled sample during training. During inference, we have four diﬀerent options for negative data: the remaining three bot classes and all bot classes together. We acquire the data from the repository7, which has small diﬀerences in the number of classes and the account features with the dataset described in the original paper.
Train-test split. For CIFAR-10 we use standard train-test split. Other datasets we split randomly into train and test in proportion 4 : 1.
5http://odds.cs.stonybrook.edu 6https://code.google.com/archive/p/word2vec/ 7https://botometer.osome.iu.edu/bot-repository/datasets.html
14

Figure 2: Examples of abnormal car (left) and normal car (right) from Abnormal1001.
Dataset details. Table 1 presents dataset details for each experimental setting. OC methods use only positive labeled data. PU methods also use unlabeled data but do not know which is positive and which is negative.
5. Experiments
In this section, we describe experimental settings and report results. We repeat each experiment 10 times. Statistical signiﬁcance is veriﬁed via paired Wilcoxon signed-rank test with a 0.05 P-value threshold. We indicate a signiﬁcant diﬀerence in favor of PU-OC or OC method with by underlining the higher ROC AUC value. As noted before (Section 3.2.2), the performance of OC-CNN and PU-CNN should not be directly compared to other models. The selected hyperparameters and tuning procedure are reported in Appendix C. Additional experiments are reported in Appendix A.
5.1. One-vs-all A common experimental setting in OC papers for datasets with multiple
classes is one-vs-all classiﬁcation. In this setting, one class is treated as positive and all other classes constitute negative examples. We ﬁx α in this setting at 0.5, label half of the positive points, and conduct experiments with each class selected as positive. Note that since OC methods have access to only half of all positive points, their performance cannot be directly compared to the results reported in their original papers.
15

Table 2: ROC AUC in one-vs-all setting

Pos class OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

0 0.78 0.79 0.70 0.71 0.76 0.80 0.74 0.91 0.85 0.85

1 0.67 0.70 0.87 0.90 0.74 0.85 0.80 0.94 0.89 0.89

2 0.60 0.62 0.63 0.61 0.64 0.75 0.64 0.87 0.78 0.76

3 0.67 0.68 0.62 0.68 0.61 0.74 0.69 0.86 0.78 0.77

4 0.63 0.65 0.60 0.62 0.71 0.76 0.77 0.90 0.81 0.80

5 0.64 0.72 0.83 0.83 0.67 0.77 0.76 0.90 0.82 0.80

6 0.74 0.74 0.73 0.79 0.75 0.81 0.81 0.94 0.88 0.88

7 0.66 0.70 0.80 0.92 0.71 0.79 0.73 0.92 0.83 0.82

8 0.68 0.77 0.86 0.89 0.78 0.84 0.78 0.95 0.88 0.88

9 0.75 0.82 0.89 0.91 0.79 0.83 0.83 0.93 0.87 0.87

avg 0.68 0.72 0.76 0.79 0.72 0.79 0.76 0.91 0.84 0.83

The ROC AUC metrics for OC and PU methods for all classes are presented in Table 2. PU-OC algorithms consistently signiﬁcantly outperform their OC counterparts with a few exceptions where the methods perform on par. Furthermore, DRPU and PAN outperform other algorithms on most classes, although on classes 7 through 9 our PU-CSI performs exceptionally well.
5.2. Shift of the negative distribution While in the previous setting PU algorithms have shined, that setting is
convenient in that unlabeled data contain reliable latent negative examples. In the next setting, we investigate the eﬀect of negative distribution shift, i.e. when latent negative examples are sampled from diﬀerent distributions at train and test times. For example, this can be relevant if the negative distribution constantly shifts over time and the model cannot be retrained at each time step, or if a large but biased unlabeled sample is available for training, whereas an unlabeled sample used for inference is either too small or for some reason unavailable. Since negative shift only aﬀects unlabeled data, only PU methods are expected to be sensitive to it (Fig. 1). Further, we suspect that the eﬀect of negative shift on PU methods might decrease with the increased modality of the negative distribution due to negative data covering more latent space, so we

16

Table 3: ROC AUC for diﬀerent modalities of negative distribution under random negative shifts on CIFAR-10 with Plane (0) class chosen as positive

Neg modality 1

2

3

4

OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

0.76 0.70 0.68 0.73 0.73 0.73 0.72 0.80 0.74 0.74

0.78 0.78 0.74 0.76 0.74 0.77 0.71 0.78 0.77 0.79

0.80 0.80 0.69 0.70 0.73 0.77 0.74 0.85 0.80 0.81

0.748 0.752 0.69 0.70 0.72 0.75 0.74 0.83 0.81 0.81

Table 4: ROC AUC for diﬀerent modalities of negative distribution under random negative shifts on CIFAR-10 with Bird (2) class chosen as positive

Neg modality 1

2

3

4

OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

0.54 0.54 0.59 0.54 0.69 0.72 0.63 0.60 0.72 0.64

0.59 0.61 0.61 0.59 0.68 0.73 0.58 0.73 0.70 0.65

0.62 0.64 0.58 0.56 0.64 0.71 0.58 0.76 0.70 0.70

0.61 0.60 0.60 0.57 0.64 0.70 0.64 0.81 0.72 0.69

additionally vary the number of negative classes. For this setting, we consider 0 (airplane) or 2 (bird) class from CIFAR-10
as positive, n random classes as negative train, and n other random classes as negative test, where n ∈ {1, 2, 3, 4}. The results are presented in Table 3 and Table 4. Despite the distribution shifts that negatively aﬀect PU methods, most PU-OC algorithms either signiﬁcantly outperform or perform on par with their OC counterparts. The exception is OC-SVM, which outperforms all other algorithms when n = 1. Evidently, even incorrect but data-driven estimates of the negative distribution used by PU algorithm are on average more helpful than the generic assumptions made by OC algorithms. Furthermore, for state-of-theart PU algorithms increasing the number of negative classes partially mitigates the eﬀect of negative shifts.
Although PU-OC models outperform their OC counterparts when the shift is random, there are particular shifts that can signiﬁcantly harm the performance of PU and PU-OC algorithms. We present results of the experiments with nonrandom negative shifts, such as animal-animal, animal-vehicle, vehicle-animal, and vehicle-vehicle, in Table 6 and Table 5. The results vary from algorithm
17

Table 5: ROC AUC for particular shifts with Bird class (2) chosen as positive. We test shifts for unimodal and multimodal negative distribution. We consider following multimodal negative distributions: A3 = {3, 4, 5}, A4 = {6, 7}, V2 = {0, 1}, V3 = {8, 9}

Shift OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

0→3 0.53 0.68 0.70 0.69 0.67 0.63 0.61 0.58 0.53 0.53

3→0 0.77 0.59 0.52 0.55 0.70 0.69 0.77 0.58 0.54 0.54

0→1 0.66 0.72 0.66 0.71 0.78 0.80 0.66 0.83 0.89 0.83

3→4 0.47 0.47 0.63 0.60 0.53 0.58 0.55 0.64 0.54 0.54

V2 → A3 0.75 0.68 0.66 0.69 0.73 0.88 0.78 0.97 0.51 0.52

A3 → V2 0.47 0.57 0.59 0.60 0.63 0.67 0.56 0.79 0.62 0.62

V2 → V3 0.52 0.60 0.51 0.50 0.60 0.59 0.57 0.58 0.91 0.91

A3 → A4 0.72 0.66 0.59 0.58 0.72 0.68 0.71 0.71 0.73 0.73

to algorithm. OC-SVM and in some cases OC-CNN outperform their PU-OC counterparts, highlighting that modiﬁcations based on risk estimation are not robust to negative distribution shifts. PU-CSI shows the same performance as regular CSI. On the other hand, PU-DROCC signiﬁcantly outperforms its OC counterpart in most cases and at least performs on par in other cases. It also outperforms both state-of-the-art PU algorithms in all cases but one. Indeed, this algorithm shows the highest robustness to shifts in the latent negative distribution.
5.3. Number of positive modes In the standard one-vs-all setting the positive distribution is close to uni-
modal. Many OC methods try to envelope positive data and thus greatly beneﬁt from unimodality [19]. However, distributions of the real-world data are often more complex. Therefore, the one-vs-all setting might produce overly optimistic estimates of the performance of OC methods. We present a synthetic example that highlights the potential struggle of OC methods with multimodal positive distributions in the case when the negative points are concentrated between the modes of the positive distribution (Fig. 4 a). In this example, the OC method
18

(a) OC ROC AUC = 0.07

(b) PU ROC AUC = 0.93

Figure 4: Separating lines of OC and PU algorithms on multimodal synthetic data. Positive distribution is a mixture of Gaussians with four diﬀerent centers. The OC and PU models are OC-SVM and PU-SVM, respectively. OC-SVM tries to enclose all modes in a single envelope, whereas PU-SVM correctly identiﬁes the location of negative data.

attempts to enclose all positive modes in a single envelope and assigns high probability of being positive to negative examples. In contrast, PU methods are robust to multimodality and can accurately estimate the decision boundary (Fig. 4 b).
In this setting, we consider a random subset from vehicle (animal) CIFAR-10 classes as positive data and all animal (vehicle) classes as negative data. We study the performance of models with respect to the number classes that form a positive sample. The results are reported in Tables 7, 8. Like in the one-vs-all setting, PU-OC methods consistently outperform their OC counterparts. Further, the performance of all OC methods decreases with the increased number of modes. OC-SVM suﬀers from multimodality the most, which is expected from a method that tries to enclose all positive data in a single envelope. The performance of some PU-OC algorithms also drops due to data becoming more complex, but these drops are slight compared to OC algorithms. The exception is PU-CSI that performs exactly as well as the original CSI on this task. Interestingly, the performance of SOTA PU methods generally increases with

19

the modality. As a result, DRPU and PAN outperform all other algorithms when the modality is high (as in other experiments, we exclude CNN-based models from this comparison due to leveraging a ResNet pretrained on ImageNet). However, when the modality is low, PU-DROCC achieves even better performance.
Table 6: ROC AUC for particular shifts with Airplane class (0) chosen as positive. We test shifts for unimodal and multimodal negative distributions. We consider following multimodal negative distributions: A1 = {2, 3, 4}, A2 = {5, 6, 7}, and V1 = {1, 8, 9}

Shift OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

1→2 0.80 0.77 0.75 0.76 0.74 0.75 0.80 0.63 0.63 0.61

2→1 0.76 0.63 0.54 0.56 0.76 0.79 0.60 0.66 0.59 0.58

1→8 0.51 0.55 0.46 0.47 0.60 0.66 0.50 0.65 0.67 0.66

2→3 0.86 0.79 0.54 0.56 0.80 0.85 0.80 0.92 0.73 0.72

V1 → A1 0.84 0.78 0.75 0.74 0.75 0.77 0.81 0.69 0.92 0.93

A1 → V1 0.64 0.57 0.49 0.49 0.68 0.69 0.56 0.71 0.59 0.62

A1 → A2 0.86 0.85 0.80 0.79 0.79 0.87 0.82 0.96 0.61 0.63

5.4. Size and contamination of unlabeled data Other cases when PU algorithms may struggle are when negative examples
are scarce, i.e. when the positive class proportion is high or the sample size of unlabeled data is small. We study the performance of OC and PU methods with respect to α and the size of the unlabeled sample. We mainly focus on the comparison of the state-of-the-art algorithms as well as some baselines . We choose all vehicles as positive classes and all animals as negative classes. The results are presented in Figure 5. Subplot (b) shows that even as few as 50 unlabeled examples are suﬃcient for PU models to outperform DROCC, but that both versions of CSI outperform other algorithms until the unlabeled sample be-
20

Table 7: ROC AUC for diﬀerent modalities of positive distribution on CIFAR-10 with vehicle classes chosen as positive

Pos modality 1

2

3

4

OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

0.74 0.80 0.85 0.85 0.78 0.92 0.90 0.98 0.88 0.87

0.60 0.80 0.82 0.82 0.79 0.87 0.90 0.98 0.86 0.85

0.58 0.77 0.83 0.83 0.77 0.86 0.81 0.96 0.88 0.88

0.57 0.80 0.88 0.88 0.73 0.83 0.82 0.96 0.93 0.93

comes bigger than 1000 examples. Subplot (a) shows that state-of-the-art PU methods struggle when unlabeled examples are mostly positive. In contrast, PU-DROCC consistently outperforms its OC analog. While both versions of CSI perform similarly, they outperform all other algorithms when α is high. Note that when α = 0.05, the diﬀerence between CSI and PU-CSI is statistically signiﬁcant in favor of the latter. Also note that CSI generally performs much better than in the previous settings, as its performance varies with the selected positive classes. Additionally, we separately compare OC models with their PU-OC variants in this setting. Results are presented in Figure 6 and Figure 7. Figure 6 shows that PU-OC models usually outperform original models or perform on par, even when α is particularly high. The result, however, is compounded with multimodality of positive distribution, which might have a negative eﬀect on some OC models. Figure 7 shows that for most models as few unlabeled examples as 250 are enough to signiﬁcantly improve performance.

21

Table 8: ROC AUC for diﬀerent modalities of positive distribution on CIFAR-10 with animal classes chosen as positive

Pos modality OC-SVM PU-SVM CSI PU-CSI DROCC PU-DROCC OC-CNN PU-CNN DRPU PAN

1 0.60 0.63 0.64 0.66 0.73 0.90 0.76 0.97 0.82 0.82

2 0.64 0.65 0.60 0.60 0.75 0.91 0.81 0.97 0.78 0.77

3 0.69 0.72 0.59 0.59 0.74 0.90 0.70 0.95 0.77 0.77

4 0.66 0.68 0.55 0.53 0.69 0.89 0.78 0.97 0.82 0.81

5 0.64 0.66 0.53 0.52 0.72 0.89 0.74 0.96 0.86 0.85

6 0.67 0.69 0.51 0.51 0.69 0.90 0.74 0.96 0.93 0.93

5.5. Abnormal1001 The results of the experiments on Abnormal are presented in Table 9. Al-
though this dataset is challenging for PU methods due to the scarcity of anomalies and similarity of nominal and anomalous data, PU-OC models still outperform their OC counterparts across all models. Furthermore, PAN and our PU-DROCC perform the best among all methods.
5.6. Sequential data Comparison of LSTM-based models on datasets with sequential data can be
found in Table 10. PU models achieve better scores than OC algorithms on all three datasets. Furthermore, OC-LSTM performs poorly on the Deceptive Reviews dataset, which has very few labeled positive examples.
5.7. Twitter bots Results for experiments on Twitter dataset shown in Table 11. Since the
dataset is tabular, it is diﬃcult to apply CNN-based and CSI-based models,

22

(a)

(b)

Figure 5: ROC AUC with respect to the proportion of positive examples in unlabeled data (a) and the size of unlabeled data (b).

Table 9: mal1001

ROC AUC on Abnor-

Base

OC PU

SVM

0.60 0.64

CSI

0.55 0.64

DROCC 0.60 0.84

CNN

0.53 0.76

DRPU

– 0.78

PAN

– 0.84

Table 10: ROC AUC on sequential data

Model

PenDigits Deceptive SMS Spam

OC-LSTM PU-LSTM

0.96 0.97

0.52 0.77

0.69 0.92

so we only apply the rest. In general, we ﬁnd that PU-OC models outperform original models, especially in a more realistic case when new bots are added to the already existing. In most cases, state-of-the-art PU models achieve the best performance.
It is worth noting that the results of [46] for OC-SVM model diﬀer from ours for some classes. We have discovered that with some seeds it is possible to replicate results of the original paper, and that the authors appear to have used one random seed in their experiments. However, when results are averaged by seed, the quality of OC-SVM model can drop signiﬁcantly. Another possible

23

(a) SVM-based models.

(b) CSI-based models.

(c) CNN-based models.

(d) DROCC-based models.

Figure 6: ROC AUC with respect to proportion of positive examples in unlabeled data.

explanation is the slight diﬀerences in the datasets that we and [46] use.
5.8. Identifying unreliable unlabeled data As our experiments show, some PU algorithms can perform poorly when
available unlabeled data are unreliable. Here, we propose a method to distinguish such cases. Empirically, PU models can struggle when α is too high (α > 0.9), when negative shifts occur, or when only a few unlabeled examples are at hand. We discover that statistical tests can help with the ﬁrst two problems. The situation with the high α can be identiﬁed by measuring the diﬀerence of distributions of outputs of a classiﬁcation model (OC or PU) on unlabeled and positive samples (ﬁg. 8 a). The diﬀerence can be measured with a statistical test, e.g. T-test or Mann-Whitney U-test. This gives us the following procedure:
24

(a) SVM-based models.

(b) CSI-based models.

(c) CNN-based models.

(d) DROCC-based models.

Figure 7: ROC AUC for diﬀerent sizes of available unlabeled data.

1. Train an OC or PU model on the available data. 2. Set the p-value threshold pcrit to 0.1 (standard for hypothesis testing). 3. Compute the outputs of the trained model on the samples from positive
and unlabeled distributions. 4. Calculate the p-value pv of a statistical test on the predictions from the
previous step to check whether the predictions are from the same distribution. If pv > pcrit then the null hypothesis of the distributions being the same is accepted and it is advised to use OC models or their robust PU-OC variants, otherwise it is advised to use PU models.
Similarly, applying Mann-Whitney U-test on the outputs of PU models for train and test unlabeled samples can help to detect distribution shifts. Fig. 8 (b) shows p-values for samples with and without negative shifts. As can be seen,
25

Table 11: ROC AUC for diﬀerent negative test distributions on Twitter dataset.

Negatives OC-SVM PU-SVM DROCC PU-DROCC DRPU PAN

all 0.61 0.86 0.72 0.81 0.94 0.83

social1 0.98 0.71 0.93 0.96 0.94 0.99

social2 0.57 0.89 0.74 0.88 0.96 0.87

social3 0.90 0.77 0.95 0.96 0.93 0.99

traditional1 0.20 0.98 0.28 0.28 0.89 0.45

p-values with shift are signiﬁcantly lower when α is not high. Therefore, one can detect the shift by comparing p-values of statistical tests on two samples from train distribution and on samples from train and test distribution. The procedure for distribution shift detection is similar to that for high α detection:
1. Train a PU model on the available data. 2. Calculate the threshold value for p-value pcrit. Compute the outputs of
the trained model on two subsamples from the labeled data. Set pcrit as the p-value of the statistical test applied to these subsamples (checking if they come from identical distribution). 3. Compute the outputs of the trained model on the samples from unlabeled train and test distributions. 4. Calculate p-value pv of a statistical test on the predictions from the previous step. If pv < pcrit then the distribution shift occurred and it is advised to use OC models or their robust PU-OC variants, otherwise it is advised to use PU models.
Furthermore, we discovered that even randomly initialized networks can be used for testing if α is high (Fig. 9). However, the resulting statistical test is not as sensitive as the statistical test with trained OC/PU models (Fig. 8 a). As a remedy, we can use same methodology as in the distribution shift: instead of setting pcrit to 0.1, pcrit can be estimated as p-value of the statistical test
26

(a)

(b)

Figure 8: P-values of Mann–Whitney U-test for identiﬁcation of a) high α or b) negative shift. We show results for OC-SVM and DROCC as OC models and PU-SVM and PU-DROCC as PU models.

Figure 9: P-values of Mann–Whitney U-test for identiﬁcation of high α with randomly initialized network.
applied to two subsamples from the positive distribution.
6. Conclusion In this paper, we experimentally compare the eﬀectiveness of state-of-the-art
OC and PU algorithms in multiple settings with varied reliability of unlabeled data. We ﬁnd that while PU algorithms often perform better, their performance can be severely hindered if the unlabeled dataset is too small, contains too few latent negative examples, or is aﬀected by particular shifts of the negative distribution. As an alternative, we propose to incorporate unlabeled data in the
27

pipeline of OC algorithms. We refer to this novel group of hybrid algorithms as PU-OC. We investigate several general ways to construct PU-OC algorithms. We ﬁnd that PU-OC algorithms can both beneﬁt from reliable unlabeled data and be robust to unreliable unlabeled data. However, this is only the case if they are speciﬁcally constructed to have a crucial property: in the absence of latent negative examples in the unlabeled data, the PU-OC algorithm should collapse into the original OC algorithm. Finally, we formulate procedures based on statistical tests that help to identify if the unlabeled data is reliable. Depending on the results of these tests, practitioners can opt to use either state-of-the-art PU algorithms (to beneﬁt from reliable unlabeled data the most) or PU-OC algorithms (that are safe to apply to unreliable unlabeled data). We hope that our ﬁndings will motivate future researchers to investigate PU-OC augmentations of their OC algorithms.
To the best of our knowledge, our paper is the ﬁrst to investigate the potential vulnerability of PU algorithms to the cases of unreliable unlabeled data. While we make a progress towards identifying such cases via statistical tests, a more desirable alternative is a PU algorithm that achieves state-of-the-art performance regardless of whether unlabeled data are reliable. We hope that future researchers will consider testing the robustness of their PU algorithms in the settings of unreliable unlabeled data, as well as proposing safe alternatives to their PU algorithms that should be used in such settings.
References
[1] A. Asuncion and D. Newman. Uci machine learning repository, 2007.
[2] J. Bekker and J. Davis. Estimating the class prior in positive and unlabeled data through decision tree induction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[3] J. Bekker and J. Davis. Learning from positive and unlabeled data: A survey. Machine Learning, 109(4):719–760, 2020.
28

[4] G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. The Journal of Machine Learning Research, 11:2973–3009, 2010.
[5] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.
[6] R. Chalapathy, A. K. Menon, and S. Chawla. Anomaly detection using one-class neural networks. arXiv preprint arXiv:1802.06360, 2018.
[7] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1–58, 2009.
[8] N. Charoenphakdee and M. Sugiyama. Positive-unlabeled classiﬁcation under class prior shift and asymmetric error. In Proceedings of the 2019 SIAM International Conference on Data Mining, pages 271–279. SIAM, 2019.
[9] H. Chen, F. Liu, Y. Wang, L. Zhao, and H. Wu. A variational approach for learning from positive and unlabeled data. arXiv preprint arXiv:1906.00642, 2019.
[10] S. Cresci, R. Di Pietro, M. Petrocchi, A. Spognardi, and M. Tesconi. Dnainspired online behavioral modeling and its application to spambot detection. IEEE Intelligent Systems, 31(5):58–64, 2016.
[11] M. David. Tax. One-class classiﬁcation. PhD thesis, PhD thesis, Technische Universiteit Delft, 2001.
[12] F. Denis. Pac learning from positive statistical queries. In International Conference on Algorithmic Learning Theory, pages 112–126. Springer, 1998.
[13] F. Denis, R. Gilleron, and F. Letouzey. Learning from positive and unlabeled examples. Theoretical Computer Science, 348(1):70–83, 2005.
29

[14] M. Du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and unlabeled data. In International conference on machine learning, pages 1386–1394, 2015.
[15] M. C. Du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled data. In Advances in neural information processing systems, pages 703–711, 2014.
[16] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 213–220, 2008.
[17] T. Ergen, A. H. Mirza, and S. S. Kozat. Unsupervised and semisupervised anomaly detection with lstm neural networks. arXiv preprint arXiv:1710.09207, 2017.
[18] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338, 2010.
[19] Z. Ghafoori and C. Leckie. Deep multi-sphere support vector data description. In Proceedings of the 2020 SIAM International Conference on Data Mining, pages 109–117. SIAM, 2020.
[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.
[21] S. Goyal, A. Raghunathan, M. Jain, H. V. Simhadri, and P. Jain. Drocc: Deep robust one-class classiﬁcation. arXiv preprint arXiv:2002.12718, 2020.
[22] F. E. Grubbs. Procedures for detecting outlying observations in samples. Technometrics, 11(1):1–21, 1969.
[23] L.-Z. Guo, Z.-Y. Zhang, Y. Jiang, Y.-F. Li, and Z.-H. Zhou. Safe deep semi-supervised learning for unseen-class unlabeled data. In International Conference on Machine Learning, pages 3897–3906. PMLR, 2020.
30

[24] Z. Hammoudeh and D. Lowd. Learning from positive and unlabeled data with arbitrary positive shift. arXiv preprint arXiv:2002.10261, 2020.
[25] D. Hendrycks, M. Mazeika, and T. Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018.
[26] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[27] V. Hodge and J. Austin. A survey of outlier detection methodologies. Artiﬁcial intelligence review, 22(2):85–126, 2004.
[28] W. Hu, M. Wang, Q. Qin, J. Ma, and B. Liu. Hrn: A holistic approach to one class learning. Advances in Neural Information Processing Systems, 33:19111–19124, 2020.
[29] W. Hu, R. Le, B. Liu, F. Ji, J. Ma, D. Zhao, and R. Yan. Predictive adversarial learning from positive and unlabeled data. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 7806– 7814, 2021.
[30] D. Ivanov. Dedpul: Diﬀerence-of-estimated-densities-based positiveunlabeled learning. In 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 782–790. IEEE, 2020.
[31] S. Jain, M. White, M. W. Trosset, and P. Radivojac. Nonparametric semisupervised learning of class proportions. arXiv preprint arXiv:1601.01944, 2016.
[32] W. Karush. Minima of functions of several variables with inequalities as side constraints. M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago, 1939.
[33] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In Advances in neural information processing systems, pages 1675–1685, 2017.
31

[34] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[35] H. W. Kuhn and A. W. Tucker. Nonlinear programming. In Traces and emergence of nonlinear programming, pages 247–258. Springer, 2014.
[36] X. Li and B. Liu. Learning to classify texts using positive and unlabeled data. In IJCAI, volume 3, pages 587–592, 2003.
[37] X.-L. Li and B. Liu. Learning from positive and unlabeled examples with diﬀerent data distributions. In European conference on machine learning, pages 218–229. Springer, 2005.
[38] Y.-F. Li and D.-M. Liang. Safe semi-supervised learning: a brief introduction. Frontiers Comput. Sci., 13(4):669–676, 2019.
[39] Y.-F. Li and Z.-H. Zhou. Towards making unlabeled data never hurt. IEEE transactions on pattern analysis and machine intelligence, 37(1):175–188, 2014.
[40] B. Liu, W. S. Lee, P. S. Yu, and X. Li. Partially supervised classiﬁcation of text documents. In ICML, volume 2, pages 387–394. Citeseer, 2002.
[41] M. M. Moya, M. W. Koch, and L. D. Hostetler. One-class classiﬁer networks for target recognition applications. NASA STI/Recon Technical Report N, 93:24043, 1993.
[42] S. Nakajima and M. Sugiyama. Positive-unlabeled classiﬁcation under class-prior shift: A prior-invariant approach based on density ratio estimation. arXiv preprint arXiv:2107.05045, 2021.
[43] G. Niu, M. C. du Plessis, T. Sakai, Y. Ma, and M. Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. Advances in neural information processing systems, 29:1199–1207, 2016.
32

[44] M. Ott, Y. Choi, C. Cardie, and J. T. Hancock. Finding deceptive opinion spam by any stretch of the imagination. arXiv preprint arXiv:1107.4557, 2011.
[45] P. Oza and V. M. Patel. One-class convolutional neural network. IEEE Signal Processing Letters, 26(2):277–281, 2018.
[46] J. Rodr´ıguez-Ruiz, J. I. Mata-S´anchez, R. Monroy, O. Loyola-Gonz´alez, and A. L´opez-Cuevas. A one-class classiﬁcation approach for bot detection on twitter. Computers & Security, 91:101715, 2020.
[47] L. Ruﬀ, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Mu¨ller, and M. Kloft. Deep one-class classiﬁcation. In International conference on machine learning, pages 4393–4402, 2018.
[48] L. Ruﬀ, R. A. Vandermeulen, N. G¨ornitz, A. Binder, E. Mu¨ller, K.-R. Mu¨ller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019.
[49] L. Ruﬀ, J. R. Kauﬀmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft, T. G. Dietterich, and K.-R. Mu¨ller. A unifying review of deep and shallow anomaly detection. arXiv preprint arXiv:2009.11732, 2020.
[50] L. Ruﬀ, R. A. Vandermeulen, B. J. Franks, K.-R. Mu¨ller, and M. Kloft. Rethinking assumptions in deep anomaly detection. arXiv preprint arXiv:2006.00339, 2020.
[51] T. Sakai and N. Shimizu. Covariate shift adaptation on learning from positive and unlabeled data. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4838–4845, 2019.
[52] B. Saleh, A. Farhadi, and A. Elgammal. Object-centric anomaly detection by attribute-based reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 787–794, 2013.
33

[53] B. Sch¨olkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and J. C. Platt. Support vector method for novelty detection. In Advances in neural information processing systems, pages 582–588, 2000.
[54] B. Sch¨olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443–1471, 2001.
[55] C. Scott and G. Blanchard. Novelty detection: Unlabeled data deﬁnitely help. In Artiﬁcial intelligence and statistics, pages 464–471. PMLR, 2009.
[56] C. D. Scott and R. D. Nowak. Learning minimum volume sets. The Journal of Machine Learning Research, 7:665–704, 2006.
[57] M. Sugiyama, T. Suzuki, and T. Kanamori. Density-ratio matching under the bregman divergence: a uniﬁed framework of density-ratio estimation. Annals of the Institute of Statistical Mathematics, 64(5):1009–1044, 2012.
[58] J. Tack, S. Mo, J. Jeong, and J. Shin. Csi: Novelty detection via contrastive learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020.
[59] D. M. Tax and R. P. Duin. Support vector data description. Machine learning, 54(1):45–66, 2004.
[60] R. Vert, J.-P. Vert, and B. Sch¨olkopf. Consistency and convergence rates of one-class svms and related algorithms. Journal of Machine Learning Research, 7(5), 2006.
[61] M. Xu, B. Li, G. Niu, B. Han, and M. Sugiyama. Revisiting sample selection approach to positive-unlabeled learning: Turning unlabeled data into positive rather than negative. arXiv preprint arXiv:1901.10155, 2019.
[62] H. Yu, J. Han, and K. C.-C. Chang. Pebl: Positive example based learning for web page classiﬁcation using svm. In Proceedings of the
34

Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 239–248, New York, NY, USA, 2002. ACM. ISBN 1-58113-567-X. doi: 10.1145/775047.775083. URL http://doi.acm.org/10.1145/775047.775083. [63] L. Zhang, M. Goldstein, and R. Ranganath. Understanding failures in out-of-distribution detection with deep generative models. In International Conference on Machine Learning, pages 12427–12436. PMLR, 2021.
35

Table A.12: ROC AUC of PU models for various α

α VPU DRPU PAN EN nnPU nnPU∗ DEDPUL

0.05 0.95 0.90 0.95 0.94 0.64 0.91 0.94

0.25 0.94 0.93 0.94 0.93 0.74 0.92 0.93

0.5 0.92 0.93 0.93 0.93 0.91 0.93 0.92

0.75 0.86 0.91 0.91 0.88 0.91 0.85 0.88

0.95 0.58 0.81 0.69 0.66 0.78 0.72 0.68

Appendix A. Additional Experiments
Appendix A.1. PU models Some modern PU methods moved from ROC AUC as the main metric to
accuracy. Therefore, it is hard to compare diﬀerent methods since in each paper they are optimized for their respective metrics. In this experiment, we try to perform a fair comparison between modern and classic PU methods. We choose vehicle classes from CIFAR-10 as positive and animal classes as negatives. The results are reported in Table A.12. All methods are described in Appendix Appendix B.3 and Section 3.1. In Table A.12, nnPU row represent nnPU with α estimated from DEDPUL, and nnPU∗ uses real value of α. It can be seen that DRPU outperforms other models when α <= 0.75, while PAN outperforms other models when α >= 0.5. Since these two models cover the best performance in all cases, we use them as state-of-the-art for comparison in the main text.
Appendix A.2. OC models We perform additional comparison with HRN, another OC model that was
published concurrently with CSI. The results are reported in Table A.13. Despite using the code provided by the authors, HRN performed poorly in our experiments and signiﬁcantly diﬀers from the results reported in the original

36

paper (reported as HRN∗). Our hypothesis to why this could have happened is that, according to the code, the authors have reported the highest ROC-AUC achieved during training, while we evaluate ROC-AUC after the training is complete for all our methods. Still, even if we use the results reported in the original paper for comparison, CSI and DROCC generally outperform HRN, so we do not include it for comparison in the main text.
Table A.13: ROC AUC in one-vs-all setting for OC models

Pos class OC-SVM CSI DROCC HRN HRN∗

0 0.78 0.70 0.76 0.74 0.73

1 0.67 0.87 0.74 0.5 0.69

2 0.60 0.63 0.64 0.5 0.57

3 0.67 0.62 0.61 0.5 0.63

4 0.63 0.60 0.71 0.5 0.71

5 0.64 0.83 0.67 0.5 0.67

6 0.74 0.73 0.75 0.5 0.77

7 0.66 0.80 0.71 0.5 0.65

8 0.68 0.86 0.78 0.67 0.78

9 0.75 0.89 0.79 0.55 0.77

Appendix B. Methods
Appendix B.1. PU-SVM This subsection is organized as follows. First, we construct PU-SVM from
classic SVM model and describe our implementation that is based on optimization with SGD. Second, we prove that PU-SVM without non-negativity constraint can also be solved via dual problem, albeit less eﬃciently.
Appendix B.1.1. Our implementation The soft-margin SVM objective can be formulated in the following way:

min λ w 2 + E(x,y)∼f lh(y, w · Φ(x) − b)

ω,b

u

(B.1)

where lh(y, x) = max(0, 1 − yx) is hinge loss, λ is regularization parameter, and ω · Φ(x) − b is the decision function. Since the objective (B.1) is a risk with regularization term, we can apply risk estimation techniques to it. However,

37

several changes are required to that end. First, we replace hinge loss with double hinge loss ldh(y, x) = max(−2yx, lh(y, x)). As shown in [14], the unbiased estimate of the objective (B.1) with double hinge-loss is a convex function (B.2).

min λ ω 2 + Ex ∼f ldh(−1, ωΦ(xu) − b)+

ω,b

uu

+αExp∼fp ldh(1, ωΦ(xp) − b) − αExp∼fp ldh(−1, ωΦ(xp) − b)

(B.2)

As we show in Section Appendix B.1.2, the objective (B.2) can be solved via dual problem, but the solution is computationally ineﬃcient. As an alternative, we add an additional non-negative constraint to (B.2) and solve the resulting objective with the stochastic gradient decent (SGD):

min λ ω 2 + αEx ∼f ldh(1, ωΦ(xp) − b)+

ω,b

pp

+ max(0, Exu∼fu ldh(−1, ωΦ(xu) − b) − αExp∼fp ldh(−1, ωΦ(xp) − b))

(B.3)

The non-negativity constraint is motivated by [33]. Since we solve (B.3) with SGD, we need to explicitly construct feature space for x, i.e. Φ(x), which again is ineﬃcient. Instead, we make our second change and replace the dotproduct in the feature space w · Φ(x) with the kernel function K(w, x). Note that now w has same dimension as x, rather than the dimension of the feature space Φ(x). Similar trick is applied in OC-NN [6] and DeepSVDD [19], where the dot-product is replaced with inference of neural network. Finally, we obtain the PU-SVM objective that we use in our study:

min λ ω 2 + αEx ∼f ldh(1, K(ω, x) − b)+

ω,b

pp

+ max(0, Exu∼fu ldh(−1, K(ω, x) − b) − αExp∼fp ldh(−1, K(ω, x) − b)) (B.4)

38

Appendix B.1.2. Dual Problem Here we show how the objective (B.2) can be solved via dual problem. To
this end, we use the following property of double-hinge loss:
ldh(1, x) − ldh(−1, x) = max(−2x, 0, 1 − x) − max(2x, 0, 1 + x) = −2x (B.5)
After replacing the expectations in (B.2) with their empirical estimates and applying the property (B.5), we get the following optimization problem:





min L(ω, b) = min λ ω 2 + 1

2α ldh(−1, ωΦ(xi) − b) + 2αb +

−ωΦ(xi)

ω,b

ω,b

nu i∈u

np i∈p

(B.6)

Similarly to classic SVM, for each unlabeled point we introduce an additional

slack variable ξi = ldh(−1, ωΦ(xi) − b), such that:

Or, in another form:

ξi ≥ 0 ξi ≥ 1 − b + ωΦ(xi) ξi ≥ 2(ωΦ(xi) − b)

(B.7) (B.8) (B.9)

−ξi ≤ 0 1 − ξi − b + ωΦ(xi) ≤ 0 −ξi − 2(b − ωΦ(xi)) ≤ 0

(B.10) (B.11) (B.12)

According to Karush–Kuhn–Tucker conditions [32, 35], the optimization problem (B.6) can be solved with the following Lagrangian function:

L(ω, b, ξ, A, B, C) = λ ω 2 − α w

1 Φ(xi) + 2αb +

ξi − Aiξi

np i∈p

nu i∈u i∈u

− Bi(b − ωΦ(xi) − 1 + ξi) − Ci(ξi − 2(ωΦ(xi) − b))

i∈u

i∈u

(B.13)

39

As KKT suggests, the optimal vector for the problem above satisﬁes the following conditions:

Aiξi = 0
Bi(b − ωΦ(xi) − 1 + ξi) = 0
Ci(ξi − 2(ωΦ(xi) − b)) = 0 ∂L ∂L ∂L = = =0 ∂ω ∂b ∂ξi
Tacking a closer look at each derivative yields:

(B.14) (B.15) (B.16) (B.17)

∂L

α

= 2ω −

∂ω

np

Φ(xi) + (Bi + 2Ci)Φ(xi)

i∈p

i∈u

∂L

∂b = 2α − Bi − 2 Ci

i∈u

i∈u

∂L 1 ∂ξi = nu − Ai − Bi − Ci

Deﬁne τi as:



α τi =  2np

−

1 2

Bi

−

Ci

Then, ω can be rewritten as:

,i ∈ p ,i ∈ u

ω=

τiΦ(xi)

i∈p,u

If (B.22) is substituted into the Lagrangian, we ﬁnally get:

(B.18) (B.19) (B.20)
(B.21) (B.22)

α

L=λ

τiτjΦ(xi)Φ(xj) − np

τi Φ(xi )Φ(xj )

i∈p,u j∈p,u

i∈u,p j∈p

+b (2α − Bi − 2 Ci) + ξi

i∈u

i∈u

i∈u

0

1 nu − Ai − Bi − Ci
0

+ Bi +

τiBjΦ(xi)Φ(xj) + 2

τi Cj Φ(xi )Cj Φ(xj )

i∈u

i∈p,u j∈u

i∈p,u j∈u

(B.23)

40

Subject to:

Ai, Bi, Ci ≥ 0

(B.24)

2α − Bi − 2 Ci = 0

i∈u

i∈u

1 nu − Ai − Bi − Ci = 0



α τi =  2np

−

1 2

Bi

−

Ci

,i ∈ p ,i ∈ u

(B.25) (B.26) (B.27)

We can apply the kernel trick to (B.23) and get a quadratic optimization problem with linear constraints (B.24-B.27):

α

L(τ, B, C) = λ

τiτj K(xi, xj ) − np

τiK(xi, xj)

i∈p,u j∈p,u

i∈u,p j∈p

+ Bi +

τiBjK(xi, xj) + 2

τiCj K(xi, xj )

i∈u

i∈p,u j∈u

i∈p,u j∈u

(B.28)

Similarly to the soft-margin SVM, each point with non-zero τi is a support vector. Since, all labeled positive examples have positive τi, they are all support vectors. Because of large number of support vectors, this approach is too computationally demanding and time consuming, so we opt for optimization with SGD described in Appendix Appendix B.1.1.

Appendix B.2. HRN
HRN [28] trains a neural classiﬁer on the log-likelihood loss with a special regularization term w n. The proposed term is referred as holistic regularization or H-regularization and essentially is an analogue of lasso or ridge regularization of a higher degree. The paper proposes to set n = 12. Holistic regularization helps the model to remove feature bias and to prevent it from collapsing into a constant solution. We adapt the oﬃcial implementation of HRN 8.

8https://github.com/morning-dews/HRN
41

Appendix B.3. PU methods Appendix B.3.1. VPU
VPU [9] is based on variational inference that allows to estimate a variational upper bound of the KL-divergence between the real posterior distribution and its estimate, parameterized by a neural network. A major advantage of VPU is that variational upper bound can be computed without knowing prior probability α, which most modern PU models rely on. Our implementation is based on the pseudo-algorithm provided in the original paper.
Appendix B.3.2. EN EN [16] is a classic PU algorithm. It consists of two steps. At the ﬁrst step,
a biased classiﬁer is trained to naively distinguish positive data from unlabeled. At the second step, the output of the trained classiﬁer is calibrated in order to make unbiased predictions.
Appendix B.3.3. DEDPUL DEDPUL [30] is a two-stage algorithm. At the ﬁrst stage, it trains a biased
classiﬁer to distinguish positive data from unlabeled and obtains predictions of this classiﬁer for all examples. This is similar to the ﬁrst stage of EN. At the second stage, it estimates the probability density functions of positive and unlabeled data in the space of predictions. Using the Bayes rule, these densities can estimate the posterior probability p(x), whereas the prior probability α is chosen such that it equals the expected posteriors. DEDPUL achieves state-ofthe-art performance in mixture proportion estimation and can improve accuracy of any PU algorithm.
We use the original implementation of DEDPUL9 without any changes.
Appendix B.4. TIcE In all our experiments, we use TIcE estimator of α for PU-SVM and PU-
CNN models. We borrow TIcE implementation from DEDPUL repository5.
9https://github.com/dimonenka/DEDPUL
42

Appendix C. Hyperparameters
Hyperprameters for all models can be found in Tables C.14, C.15, C.16, C.17, C.18, C.19. First, we tune hyperparameters on one train-test split via grid search for all models except DROCC. Initial hyperparameters for DROCC we take from [21]. After that, we manually ﬁne-tune hyperparameters in the neighborhood of the initial parameters found with grid search. For SVM-based methods we search on the following grid: kernel ∈ {rbf, linear}, ν ∈ {0.1, 0.5, 0.9}, γ ∈ {0.01, 0.1, 1}, λ ∈ {0.01, 0.1, 1}, lr ∈ {10−3, 10−4}, num epochs ∈ {50, 100, 200}. For LSTM-based methods we search on the following grid: lstm dim ∈ {4, 16}, ν ∈ {0.1, 0.5, 0.9}, batch size∈ {128, 256}, lr ∈ {10−2, 10−3, 10−4},num epochs ∈ {50, 100, 200}. For CNN-based methods we search on the following grid: γ ∈ {0.9, 1}, lr ∈ {10−3, 10−4}, num epochs ∈ {10, 20, 40}. For CSI-based methods we search on the following grid: λ ∈ {0.01, 0.1, 0.5}, lr ∈ {10−5, 10−4, 10−3}, temperature∈ {0.01, 0.05, 0.1, 0.5}, batch size∈ {32, 64, 128}, γ ∈ {0.9, 0.96, 0.99}. For DRPU we search on the following grid: α ∈ {0.001, 0.01, 0.1}, lr ∈ {10−4, 10−3}, γ ∈ {0.96, 0.99}, num epochs ∈ {10, 20, 50}. For PAN we search on the following grid: λ ∈ {0.001, 0.01, 0.1}, lr ∈ {10−4, 10−3}, γ ∈ {0.96, 0.99}, num epochs ∈ {10, 20, 50}.
Table C.14: Hyperparameters for SVM-based models

Hyperparameter kernel ν γ λ num epochs lr lr decay

OC-SVM

linear 0.5 2e-3 –

–

–

–

PU-SVM

linear – 1 0.01

100

5e-3 0.995

Table C.15: Hyperparameters for CNN-based models

Hyperparameter num epochs lr batch size γ

OC-CNN

10

1e-4

256

1

PU-CNN

10

1e-4

256

1

43

Table C.16: Hyperparameters for LSTM-based models

Hyperparameter num epochs lr batch size ν lstm dim

OC-LSTM

200

1e-4

128

0.5

16

PU-LSTM

100

5e-3

256

0.5

16

Table C.17: Hyperparameters for state-of-the-art PU models

Hyperparameter α

λ num epochs lr γ

DRPU

0.001 –

60

1e-3 0.99

PAN

– 0.001

20

1e-3 0.99

Table C.18: DROCC Hyperparameters

Model

DROCC

PU-DROCC

Pos data

0 1 2 3 4 5 6 7 8 9 vehicles Abnormal

All

λ

0.5 0.5 0.5 0.5 0.5 0.2 0.5 0.5 0.5 0.5 0.5

0.5

0.5

radius

24 24 32 28 32 36 32 28 28 28

28

30

2

γ

1.5 1.1 1.5 1.1 1.5 1.5 1.5 1.1 1.1 1.1 1.1

1.1

2

learning rate

1e-3 1e-3 1e-3 1e-3 1e-3 5e-3 1e-3 1e-3 1e-3 1e-3 1e-3

1e-3

5e-4

ascent step size 1e-2 1e-2 1e-2 1e-3 1e-4 1e-3 1e-2 1e-2 1e-2 1e-2 1e-3

1e-3

1e-5

ascent num steps 40 60 60 60 20 40 60 50 50 50

60

50

10

γlr

1111111111

1

0.99

0.96

num epochs

30 30 30 30 30 30 30 30 30 30

30

15

20

batch size

128 128 128 128 128 128 128 128 128 128 128

128

256

Table C.19: Hyperparameters for CSI-based models

model CSI

Parameter λ
batch size temp γ lr

0 1 2 3 4 5 6 7 8 9 vehicles Abnormal

0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.5 0.5 0.1

0.1

64 32 32 32 32 32 32 32 32 32

64

64

0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.1 0.5 0.5

0.1

0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99

0.99

1e-4 1e-3 1e-3 1e-3 1e-4 1e-3 1e-3 1e-3 1e-3 1e-3 1e-4

1e-3

λ

0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.5 0.5 0.1

0.1

batch size 64 32 64 32 32 32 32 32 32 32

96

64

batch size unl 16 16 16 16 16 16 16 16 16 16

32

32

PU-CSI

temp

0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.1 0.5 0.07

0.01

γ

0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99

0.99

lr

1e-3 1e-3 1e-3 1e-3 1e-4 1e-3 1e-3 1e-3 1e-3 1e-3 5e-3

1e-4

44

