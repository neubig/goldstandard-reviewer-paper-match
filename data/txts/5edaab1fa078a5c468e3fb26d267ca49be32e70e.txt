A Cost-Effective Framework for Preference Elicitation and Aggregation

arXiv:1805.05287v2 [cs.LG] 7 Jul 2018

Zhibing Zhao, Haoming Li, Junming Wang RPI Troy, NY, USA
{zhaoz6,lih14,wangj33}@rpi.edu

Jeffrey O. Kephart, Nicholas Mattei, Hui Su
IBM Research Yorktown, NY, USA {kephart,n.mattei,huisuibmres}@us.ibm.com

Lirong Xia RPI
Troy, NY, USA xial@cs.rpi.edu

Abstract
We propose a cost-effective framework for preference elicitation and aggregation under the Plackett-Luce model with features. Given a budget, our framework iteratively computes the most cost-effective elicitation questions in order to help the agents make a better group decision.
We illustrate the viability of the framework with experiments on Amazon Mechanical Turk, which we use to estimate the cost of answering different types of elicitation questions. We compare the prediction accuracy of our framework when adopting various information criteria that evaluate the expected information gain from a question. Our experiments show carefully designed information criteria are much more efﬁcient, i.e., they arrive at the correct answer using fewer queries, than randomly asking questions given the budget constraint.
1 INTRODUCTION
Consider the hiring decision problem [Bhattacharjya and Kephart, 2014]. With the aid of an intelligent system, a group of people (the key group) faces a hiring decision about many candidates who are characterized by attributes, such as experiences, technical skills, communication skills, etc. The goal is to help the key group make a group decision without directly eliciting their full preferences over all candidates, which is often infeasible given the vast number of candidates. Instead, the intelligent system may ask fellow employees (the regular group) about their preferences in order to learn about the key group’s preferences. How can the intelligent system decide which member in the regular group to ask and

which questions to ask? Note that we discuss the presence of two groups but our framework is applicable when there is only one group of decision makers as well.
This example illustrates the preference elicitation problem, which has been widely studied in the ﬁeld of recommender systems [Loepp et al., 2014], healthcare [Chajewska et al., 2000, Weernink et al., 2014, Erdem and Campbell, 2017], marketing [Huang and Luo, 2016], stable matching [Drummond and Boutilier, 2014, Rastegari et al., 2016], combinatorial auctions [Sandholm and Boutilier, 2006], etc. Most previous works studied a special case of the aforementioned scenario, in which the regular group is the key group. The objective of preference elicitation is to achieve some goal using as few samples (data) as possible. A common approach is to adaptively ask questions that maximize expected information gain, measured by some information criteria.
Moreover, most previous work focused on speciﬁc types of elicitation questions, e.g. pairwise comparisons. In this paper, we consider a more general framework that asks a variety of elicitation questions and can accommodate one or more groups. The diversity of elicitation questions enables us to query cost-effectively. Intuitively, an agent’s preference order over 10 alternatives tells us more about her preference in general than just her top choice among the 10; however, it may take her longer to do so. The key question we want to answer in this paper is:
How can we compute the most cost-effective questions for preference elicitation under resource constraints?
1.1 OUR CONTRIBUTIONS
We propose a ﬂexible cost-effective preference elicitation and aggregation framework to predict a single agent’s preference or help make a group decision. The main inputs include a budget W , a set of designs (i.e. questions to ask) H, a cost function w, a randomized

voting rule and an information criterion. We model nondeterministic preferences using the Plackett-Luce model with features.
Cost-effectiveness. We propose a ﬂexible, cost-effective preference elicitation framework that accommodates all randomized voting rules, ranking models, and information criteria. This iterative framework leverages the optimal design technique. In each iteration, we choose the question that provides the most information per unit cost. The response is then recorded as a data point, leading to an update of the posterior distribution of the parameter, which is treated as the prior for the next iteration. In any iteration, the posterior estimate of the parameter can be used to compute a winner distribution using a randomized voting rule. This procedure is illustrated in Figure 1.

(MPC) criterion, extended from the information criterion by Azari Souﬁani et al. [2013], which maximizes the improvement of the least certain pairwise comparison. Other commonly-used information criteria include D-optimality [Wald, 1943, Mood et al., 1946] and Eoptimality [Ehrenfeld, 1955], as well as asking a question uniformly at random. All these information criteria are based on the information of the posterior distribution of the model parameter, which is approximated by its asymptotic distribution, a multivariate Gaussian computed based on the composite marginal likelihood method [Pauli et al., 2011].
Empirical Studies & Experiments. We carry out Amazon Mechanical Turk experiments to estimate the cost of answering various types of questions for a target domain of ranking hotels. We compare the performances of MPC, D-optimality and E-optimality with simulations and observe that these criteria have similar performance in terms of prediction accuracy, and we observe that all of them signiﬁcantly outperform random elicitation questions.

Figure 1: Illustration of the proposed framework.
Randomized Voting Rules. We use randomized voting rules to compute the winning alternatives of a group decision, which outputs the distribution of winners (see Section 4 for details). The probability for each alternative to be the winner is proportional to its score based on the voting rule. These probability estimates are more informative than only recommending a winner as it provides a distribution over the candidates as well.
We prove that when people have non-deterministic preferences, the probability of an alternative to be the winner is proportional to the total expected score of this alternative for all agents (Theorem 1). This means the randomized counterpart of any scoring rule can be used in our framework as long as the expected score of each alternative for a single agent is easy to compute. Then we prove that under the Plackett-Luce model, the winner distributions of probabilistic plurality and probabilistic Borda are easy to compute (Corollary 1 and Theorem 2).
Information Criteria. An information criterion plays a key role in determining the next elicitation question by measuring the information in the distribution of a parameter. We propose the minimum pairwise certainty

1.2 RELATED WORK AND DISCUSSIONS
Our work is related to cost-effective experimental designs, which were investigated by Wright et al. [2010], Volkov [2014] in the context of aquatic toxicology and drug development, respectively. Volkov [2014] modeled cost-effectiveness as different types of optimization problems, e.g., minimize cost under information constraints. We take a greedy approach, similar to an algorithm proposed by Wright et al. [2010], and choose the design (elicitation question) that maximizes the expected information gain per unit cost. Our cost varies depending on the type of questions and is estimated empirically, similar to the idea in Volkov [2014]. To our best knowledge, this paper is the ﬁrst work to apply cost-effective experimental design to preference elicitation.
The greedy approach is also called one-step-lookahead policy, which can be arbitrarily worse than optimal tstep-lookahead (t-step myopic active search) policies for t ≥ 2 [Garnett et al., 2012]. Arbitrary t-step myopic active search is hard to compute, as was shown by Jiang et al. [2017], which also proved that nonmyopic active search is computationally hard even to approximate and proposed an efﬁcient searching algorithm. This algorithm is potentially useful in the preference elicitation context and is an interesting future direction.
Most previous works in preference elicitation assumed that people’s preferences are deterministic. For example, Bhattacharjya and Kephart [2014] proposed an even swap algorithm to reveal a single decision maker’s most

preferred alternative; Lu and Boutilier [2011a], Kalech et al. [2011] elicited preferences from a group of people in order to make a group decision under a (deterministic) voting rule. In contrast, we consider non-deterministic preferences of people, which is often the case in realworld. Moreover, we use randomized voting rules, which output the probability of each alternative to be the winner. These probabilities, which can be viewed as normalized scores over all alternatives, provide a quantitative measure of the quality of each alternative. For example, an alternative that wins with probability 0.8 can be seen as being much better than other alternatives.
Non-deterministic preferences were modeled by general random utility models by Azari Souﬁani et al. [2013]. They proposed a preference elicitation framework for personalized choice and social choice (aggregated preference). We use the Plackett-Luce model with features, which is a special case of general random utility models but has easy-to-compute probabilities. More importantly, we use randomized voting rules for aggregation, which is very different from parametric modeling of social choices employed by [Azari Souﬁani et al., 2013].
Pairwise elicitation questions may be the most widely explored in the literature due to their simplicity [Branke et al., 2017, Eric et al., 2008, Houlsby et al., 2012, Lu and Boutilier, 2011a, Pfeiffer et al., 2012]. In contrast, Azari Souﬁani et al. [2013] focused on elicitation of full rankings, though their proposed framework also allows for partial orders. Drummond and Boutilier [2014], Lu and Boutilier [2011b] studied a larger set of queries, which includes asking a person to rank her top k choices over all alternatives. In this paper, we consider an even broader set of queries, asking an agent to rank her top k choices over a subset of l alternatives (k < l). This enables us to elicit preferences in a more cost-effective manner.
As a key role in preference elicitation, information criteria have been widely investigated for different applications. Standard information criteria include Doptimality (used in [Houlsby et al., 2011, 2012, Pfeiffer et al., 2012]) and E-optimality. Drummond and Boutilier [2014] and Lu and Boutilier [2011a] use minimax-regretbased criterion for stable matching and aggregation respectively. Azari Souﬁani et al. [2013] proposed yet another criterion, deﬁned on the certainty of the least certain pairwise comparison over the intrinsic utilities (part of the parameter of their general random utility models) of all alternatives. Our MPC criterion extends the criterion by Azari Souﬁani et al. [2013]. To predict a single agent’s top k preference, we search over a subset of all pairwise comparisons (see Section 3.3). To help make a group decision, we search over all pairwise comparisons

of all agents in the key group to ﬁnd the least certain pairwise comparison (Equation (3)).

2 PRELIMINARIES
Let A = {a1, a2, . . . , am} denote a set of m alternatives and {1, . . . , n1, n1 + 1, . . . , n1 + n2} denote n1 + n2 agents, where the ﬁrst n1 agents belong to the key group, who will be making a group decision. The remaining n2 agents belong to the regular group. For all i = 1, . . . , m, ai is characterized by a real-valued column vector of K attributes zi. For all j = 1, . . . , n1 + n2, agent j is characterized by a real-valued column vector of L attributes xj. A full ranking R is often denoted by ai1 ai2 . . . aim , where “ ” means “is preferred over”. We denote the budget by W , where the money is used to pay the agents for answering elicitation questions.
For n1 = 1, we want to predict the single key agent’s full or top k ranking with as much certainty as possible given a budget W . For n1 ≥ 2, the goal is to predict the winning alternative of the key group by eliciting preferences from the regular group in the most cost-effective way. More concretely, given W , we want to output a distribution of winning alternatives, w.r.t. a randomized voting rule, which will be deﬁned in Section 4.

2.1 THE PLACKETT-LUCE MODEL WITH FEATURES

Let the parameter B = [bκι]K×L be a matrix of realvalued coefﬁcients, transforming features to utilities. Each value bκι corresponds to the κ-th attribute from an alternative and ι-th attribute from an agent. The parameter space Θ is a set of all real-valued K × L matrices. Then the utility of an alternative ai to an agent j is

uji = xj Bzi.

(1)

For any agent j and any full ranking Rj = ai1 ai2 . . . aim , the probability of Rj is

Pr(Rj) =

exp(uji1 ) ×

m q=1

exp(ujiq

)

exp(uji2 ) ×

m q=2

exp(ujiq

)

···×

exp(ujim−1 ) .

exp(ujim−1 ) + exp(ujim )

Given the Plackett-Luce model with features, the prob-

ability of alternative ai1 to be ranked at the top among

{ai1 , . . . , ail } by agent j is

exp(uji1 ) . Speciﬁcally,

l q=1

exp(ujiq

)

for any two alternatives a1 and a2, the probability of

a1 a2 by agent j is exp(uejx1p)+(uejx1p)(uj2) .

2.2 ONE-STEP BAYESIAN EXPERIMENTAL DESIGN

Given any probabilistic model parameterized by B ∈ Θ and any prior distribution π(B), a one-step Bayesian experimental design consists of two parts: (i) a set of designs H, where each h ∈ H is composed of an agent and a question; (ii) an information measure G(·), which maps any distribution of B over Θ to a real-valued scalar: a measure of information in this distribution.

For any design h ∈ H, the distribution of responses can be computed using the ground truth parameter B∗. We use D to denote the set of all possible responses. Given a ground truth parameter B∗, the probability of any data d ∈ D can be computed as Pr(d|h). Further, we can compute the posterior distribution of parameter π(B|d, h) over the parameter space Θ and the corresponding information criterion G(π(B|d, h)). The expected information is

E[G(π(B|h))] = G(π(B|d, h)) Pr(d|h),
d∈D

where the expectation is taken over all possible responses. The goal is to ﬁnd the design h that maximize the expected information gain, which is E[G(π(B|h)] − G(π(B)), per unit cost. Let w(h) denote the cost function, which maps the 2-tuple (agent, question) to a positive cost. Given the cost function w(h), we can compute the optimal design h∗ that maximizes the expected information gain per unit cost by

h∗

=

arg

max

E[G(π(B|h))]

−

G(π(B)) .

(2)

h

w(h)

3 COST-EFFECTIVE PREFERENCE ELICITATION

In our proposed framework, we iteratively adapt the onestep experimental design by querying the most costeffective question in each iteration. At any iteration t, the prior distribution of B is the posterior distribution given data Dt, i.e. π(Bt|Dt). Given this posterior, we ﬁnd the most cost-effective design ht, which consists of one agent and one question, and query ht. The response is combined with Dt to form Dt+1. Then the budget W and the set of designs H are updated before going to the next iteration. Finally, when n1 = 1, we compute the predicted preference of this agent; when n1 ≥ 2, we compute the distribution of winners based on a randomized voting rule. This framework is formally illustrated in Algorithm 1.
For the rest of this section, we will explain how to approximate the posterior distribution π(Bt|Dt) and how G(π(B)) is computed.

Algorithm 1 Cost-Effective Preference Elicitation
Input: Budget W , randomize voting rule r, cost function w(h), information criterion G(π(B)), the set of designs H where for any h ∈ H, w(h) ≤ W . Output: A predicted preference when n1 = 1 or a distribution of winning alternatives for group decision when n1 ≥ 2. Initialization: Randomly initialize data D1.
while H is not empty do Compute/approximate π(Bt|Dt); Compute ht ∈ H using (2); Implement ht (query an agent a question). Let Rt denote her answer. Then Dt+1 ← Dt ∪ {Rt}, H ← H − ht, W ← W − wt; Remove all h ’s from H where w(h ) > W .
end while Compute the predicted preference when n1 = 1 or a distribution of winning alternatives according to the voting rule r when n1 ≥ 2.

3.1 APPROXIMATION OF POSTERIOR DISTRIBUTION

For any prior π(B) and data D, the posterior distribution is given by π(B|D) = Pr(D|B)π(B) accord-
Θ Pr(D|B)π(B)dB
ing to Bayes’ rule. This posterior is often hard to com-
pute. A commonly-used approach is to approximate it by
its asymptotic distribution, which is a multivariate Gaus-
sian distribution characterized by the composite marginal
likelihood (CML) method [Pauli et al., 2011].

For convenience we vectorize B as a column vector, denoted by β = vec(B). The composite marginal likelihood method [Lindsay, 1988, Zhao and Xia, 2018] computes the estimate of the ground truth parameter from marginal events, e.g., pairwise comparisons. Let {E1, . . . , Eq} denote q selected marginal events. Then the composite marginal likelihood method computes the estimate βCML by

q

βCML = arg max CLL(β) = arg max ln Pr(Eλ|β),

β∈Θ

β∈Θ λ=1

where CLL(β) denotes the composite log-likelihood function. Under our Plackett-Luce model with features, CLL(β) is twice differentiable for all β ∈ Θ, i.e. J(β) = −∇2 CLL(β) exists. From Pauli et al. [2011], asymp-
β
totically, π(β|D) is a multivariate Gaussian distribution, whose mean is βCML and covariance matrix is J−1(β).
Computing J(β) requires computation of second order partial derivatives of ln Pr(Eλ|β) for all λ. We will show the close-form second order partial derivative formula for

any response from an agent.

3.2 THE SET OF DESIGNS

Each design h ∈ H is a combination of an agent and a

question about her preferences. The agent can be anyone

from {1, . . . , n1 + n2}. In this paper, for simplicity, we

consider the case where only the agents from the regular

group {n1 + 1, . . . , n1 + n2} are queried. For any inte-

gers k < l ≤ m, we may ask an agent to rank her top

k alternatives over a subset of l alternatives. When k =

1, l = 2, the question is a pairwise comparison; when

k = 1, l > 2, the question is to query an agent’s top al-

ternative among a subset of alternatives; when k = l − 1,

we are asking a full ranking over a subset of alternatives.

The advantage of this type of questions is that the prob-

abilities of responses of these questions are easy to com-

pute, as well as their partial derivatives. W.l.o.g. let Rj =

a1 a2 . . . ak others be the answer from agent

j. Then we have Pr(Rj|B) =

k p=1

exp(uj p )
l

, and

exp(u )

i=p

ji

ln Pr(Rj|B) =

kp=1(ujp − ln

l i=p

exp(uji

)).

For any 1 ≤ κ ≤ K and 1 ≤ ι ≤ L, let bκι be the (κ, ι) entry of B. We have

∂ ln Pr(Rj|B) k ∂ujp

=( −

∂bκι

p=1 ∂bκι

li=p exp(uji) ∂∂ubκjιi ),

l i=p

exp(ujp

)

where ∂∂ubκjpι and ∂∂ubκjιi are constants (products of an agent’s attribute and an alternative’s attribute) by deﬁ-
nition. Therefore, for diagonal entries, the second order
partial derivatives are given by

∂2 ln Prj(R|B) k

∂b2κι

= ((

p=1

li=p exp(uji) ∂∂ubκjιi )2

l i=p

exp(uji

)

− li=p exp(uji)( ∂∂ubkjpl )2 ),

l p=1

exp(ujp

)

and for non-diagonal entries, we have

∂2 ln Prj(R|B)

∂bκ1ι1 ∂bκ2ι2

k( =(
p=1

li=p exp(uji) ∂∂bκu1jιi1 )( li=p exp(uji) ∂∂bκu2jιi2 ) ( m i=p euji )2

− li=p exp uji( ∂∂bκu1jιi1 )( ∂∂bκu2jιi2 ) ).

l i=p

exp(uji

)

3.3 INFORMATION CRITERIA
An information criterion maps the distribution of a parameter to a real-valued quality. Standard information

criteria are mostly directly computed from the covariance matrix J−1(β) or its inverse J(β). For example, D-optimality [Wald, 1943, Mood et al., 1946] computes the determinant of J(β); E-optimality [Ehrenfeld, 1955] computes the minimum eigenvalue of J(β). We propose the following minimum pairwise certainty (MPC) criterion by extending the criterion from [Azari Souﬁani et al., 2013] to our domain.
MPC for Case n1 = 1. We consider two types of purposes: predicting the agent’s (unordered) top k alternatives and predicting the agent’s ranked top k alternatives. We note that the criterion by Azari Souﬁani et al. [2013] only applies to full rankings, which is a special case of our ranked top k. The intuition of this criterion is to maximize the certainty of the least certain pairwise comparison among a subset of pairwise comparisons. Formally, let Ak denote the set of predicted top k alternatives for this key agent.
• Unordered top-k where 1 ≤ k < m:

G(π(β)) = min |mean(u1i1 − u1i2 )| . i1∈Ak,i2∈Ak std(u1i1 − u1i2 )

• Ranked top-k where 1 < k < m:

G(π(β)) = min |mean(u1i1 − u1i2 )| . i1∈Ak,i2=i1 std(u1i1 − u1i2 )

In the above equations, mean(uji1 − uji2 ) is computed using βCML and std(uji1 − uji2 ) is computed using the approximated covariance matrix J−1(β) as follows.
Because uji1 − uji2 is linear with β (see Equation (1) and recall that β is the vectorization of B), we write it as uji1 − uji2 = κ,ι cκιbκι, where cκι’s are constants computed from attributes of ai1 , ai2 and agent j. Then we have std(uji1 − uji2 ) =
(κ1,ι1),(κ2,ι2) cκ1ι1 cκ2ι2 Cov(bκ1,ι1 , bκ2,ι2 ). When κ1 = κ2 = κ and ι1 = ι2 = ι, Cov(bκ1,ι1 , bκ2,ι2 ) reduces to Var(bκι). Both Cov(bκ1,ι1 , bκ2,ι2 ) and Var(bκι) are entries of J−1(β).
MPC for Case n1 ≥ 2. Our MPC for this case is different from the criterion by Azari Souﬁani et al. [2013] in that we ﬁnd the least certain pairwise comparison across all agents in the key group. Formally, our MPC for n1 ≥ 2 is

G(π(β)) =

min

|mean(uji1 − uji2 )| , (3)

j∈{1,...,n1},i2=i1 std(uji1 − uji2 )

where the computation of mean(uji1 − uji2 ) and std(uji1 − uji2 ) are similar to the n1 = 1 case.

4 RANDOMIZED VOTING RULES

We use randomized voting rules to aggregate the key group’s preferences. A randomized voting rule computes the distribution of winners given the preferences of the agents from the key group. Under non-deterministic preferences, this distribution can be computed from the parameter of the model. This section shows that under the Plackett-Luce model with features, probabilistic plurality and probabilistic Borda are easy to compute.

A randomized voting rule assigns a probability for each alternative to be the winner according to the data, usually based on a scoring function. For example, the probabilistic plurality rule, which is equivalent to random dictatorship [Gibbard, 1977], samples a winner from a distribution where the probability of each alternative being the winner is proportional to the plurality score of this alternative. Other randomized voting rules can be deﬁned similarly, including probabilistic Borda [Heckelman, 2003]. The voting rule must have scores associated with it, but this is a very mild restriction because many commonly-studied voting rules including all positional scoring rules, Copeland, range voting, and approval voting, have randomized counterparts.

Recall that n1 agents are making a group decision among

m alternatives. Let P denote the preference proﬁle that

consists of n1 full rankings over m alternatives from the

key group. Let sr(ai, P ) denote the score of alterna-

tive ai under voting rule r and Prr(ai|P ) be the prob-

ability for ai to win under the randomized analogy of r

given P . Then Prr(ai|P ) is computed by Prr(ai|P ) =

. sr(ai,P )

m i=1

sr

(ai ,P

)

Example 1 Suppose the set of alternatives is {a1, a2, a3} and the votes are {a1 a2 a3, a1 a3 a2, a2 a1 a3}. The plurality and Borda scores are shown in Table 1. Under probabilistic plurality rule, a1 wins with probability 2/3 and a2 wins with probability 1/3. Under probabilistic Borda, a1, a2, a3 win with probabilities 5/9, 3/9, 1/9 respectively.

a1 a2 a3 plurality 2 1 0 Borda 5 3 1

Table 1: Scores under plurality and Borda

We consider non-deterministic preferences from agents,
where the preferences from the key agents are indepen-
dent of each other. Because each agent has m! possible rankings, there are (m!)n1 possible preference proﬁles. Then we have Prr(ai) = (qm=1!)n1 Pr(Pq) Prr(ai|Pq),

where Pq denotes the q-th possible preference proﬁle.
Given a voting rule r, let Xji be the score of ai for agent j. Xji is a random variable due to the uncertainty of agent j’s preference. The following theorem shows that the probability of ai being the winner is proportional to the sum of expected score of ai for each agent.

Theorem 1 For any 1 ≤ i ≤ m, Prr(ai) ∝

n1 j

EXji.

Proof: It sufﬁces to prove Prr(a1) ∝

n1 j

EXj1.

By deﬁnition, Prr(a1) = (qm=1!)n1 Pr(Pq) Prr(a1|Pq). Let S denote the score of a1 under rule r. Then S is a random variable deﬁned over the (m!)n1 cases. Let sq
denote the value that S takes for case q. In any case q, we
have Prr(a1|Pq) ∝ sq by the deﬁnition of randomized voting rules. We re-write it as Prr(a1|Pq) = M sq , where M is the normalization factor. Observe that across all the (m!)n1 cases, M does not change because the voting
rule r and the set of agents does not change. So we have

Prr(a1) =

q(m=1!)n1 Pr(Pq)sq M

(m!)n1

∝

Pr(Pq)sq = ES.

(4)

q=1

Since S =

n1 j=1

Xj1,

due

to

linearity

of

expectation,

we have ES = E[

n1 j=1

Xj1]

=

n1 j

EXj1.

By (4),

we have Prr(a1) ∝

n1 j

EXj1.

For probabilistic plurality, the expected score of ai for agent j is exactly the probability of ai being ranked at the top by agent j. So we have the following corollary:

Corollary 1 Let paj i be the probability of ai being

ranked at the top by agent j. For any 1 ≤ i ≤ m,

Prplurality(ai) = n11

n1 j

paj i .

Theorem 2 Let paj i ai denote the probability for agent

j to prefer alternative ai over ai . Then for any 1 ≤ i ≤

m, PrBorda(ai) ∝

n1 j

i =i paj i ai .

Proof: For any i ∈ {1, . . . , m}, we have PrBorda(ai) ∝

n1 j

EXji

by

Theorem

1,

where

Xji

here

denotes

the

score of ai for agent j under Borda. We only need to prove EXji = i =i paj i ai . This is a known result,

but we were not able to ﬁnd a formal proof in literature,

except a proof for three alternatives by Chen and Heck-

elman [2005], which is easy to be extended for arbitrary

number of alternatives. For completeness we provide a

short proof.

By deﬁnition of Borda, we have EXji = m k=−11(m − k) R:ai at kth position of R Prj (R), where R is any full

ranking over the m alternatives and Prj(R) is the probability of R by agent j. Imagine m−1 bins, each of which is labeled with ai ai for all the remaining m − 1 ai ’s. Observe that there are m − k copies of Prj(R) for all R where ai beats exactly m − k other alternatives. We can distribute the m − k copies to the m − k bins (one in each) for all ai ’s that are ranked after ai. We do this for all possible rankings and in the end, each bin labeled by ai ai gets the probabilities of all rankings compatible with ai ai . This ﬁnishes the proof. We note that for the Plackett-Luce model, paj i ’s and paj i ai ’s are easy to compute (see Section 2.1).
5 EXPERIMENTS
We ﬁrst introduce an example of empirically estimating the cost of asking different types of questions on MTurk. Then, we show the result of a simulation of cost-effective preference elicitation using synthetic data.
Figure 3: The user interface for a Turker to submit her ranked top 4 over 10 alternatives. The attributes are average ratings, prices per night, time to Times Square, and time to the nearest airport.
5.1 ESTIMATING w(h) We recall that a question is deﬁned by a pair of parameters (k, l), where l is the number of alternatives that are presented to an agent and k is the number of alternatives that the agent is asked to rank at the top k positions.

In order to map the question types to the time to answer them, we run 2 experiments with multiple tasks on MTurk. Each task required MTurk workers to report their preferences over a set of hotels. We recorded the time they spent on each task, in order to learn such mapping in the following two cases: • k, l ∈ [2, 10], k = l − 1: full rankings; • k ∈ [1, 10], l = 10: ranked top k alternatives over 10.
Experiment Setting. For the ﬁrst case, we looked for information on the ﬁrst 54 Hotels in New York City in alphabetical order. We split the 54 randomly into 9 sets, each containing 2, 3, ..., 10 hotels. We then showed the 9 sets to MTurk workers, randomizing the order of the 9 sets as well as the initial display order of alternatives within each set, and asked them to rearrange by dragand-dropping the alternatives according to their preferences. The alternatives were anonymous and represented by 4 attributes: average guest rating on a popular travel website, price per night, time to Times Square and time to the nearest airport.
For the second case, a separate experiment is run with another 10 sets of NYC hotels, drawn randomly again from the ﬁrst 100 hotels in NYC in alphabetical order. In each task, we placed a horizontal green bar underneath the alternative above which are the k alternatives of interest. We instructed the MTurk workers before the experiment started that only the alternatives above the green bar would count, i.e. only needed to rank-order top-k. All of these were done with goal of minimizing the overhead time for workers to understand the instruction so that the recorded time accurately reﬂect the time of decision-making. An example of the UI is show in Figure 3, where we asked Turkers to rank-order her top4 favorite hotels over a set of 10.
The following analysis is made possible by responses from 408 MTurk workers (202 for the ﬁrst case and 206 for the second).
Experiment Results. Although Volkov [2014] considered both linear and quadratic cost function and argued for the superiority of the latter, for simplicity, we perform a linear regression on the dataset to obtain a linear cost function. We regress the time to rearrange the alternatives and submit a full ranking on the number of alternatives in the set. We ﬁnd that, on average, the time a Turker spent on rank-ordering a full ranking over l alternatives is tfull−l = 5.33l (Figure 2 left), and that on rank-ordering her top-k alternatives over 10 alternatives is ttop-k = 1.38k + 32.25 (Figure 2 right). In addition, the 408 workers spent an average of 341.5 seconds on the tasks and were each paid $0.3. Therefore, the monetary cost of elicitation on average is w = 0.00088t, which correspond to an hourly wage of $3.16.

Figure 2: The left subﬁgure shows the average time a user spent to submit a full ranking over 2, . . . , 10 alternatives; the right subﬁgure shows the average time a user spent to give her ranked top 1, . . . , 10 alternatives when 10 alternatives were proposed.

Combining these two functions with the hourly wage, we propose the following cost functions, which estimates the cost (in USD) of elicitation about hotel preferences given 4 alternative attributes: wfull−l = 0.0047l and wtop−k = 0.0012k + 0.028. We observe that the time a user spent is not very sensitive to k. This is sensible, as when a MTurk worker ranks her top k choices, she may follow the following procedure: 1. read the descriptions of all hotels, 2. form their preferences, and 3 choose top k. Step 1 and 2 do not depend on k and dominates the time for step 3, as illustrated in the right ﬁgure of Figure 2. This suggests that when a ﬁxed number of alternatives is proposed to an agent, it’s likely that the most cost-effective question to ask is a full ranking, as we will see in the next subsection.
5.2 COST-EFFECTIVE PREFERENCE ELICITATION
We demonstrate the viability of our cost-effective framework and compare performances of different information criteria on synthetic data.
Synthetic Data. We randomly generated 10 alternatives, each of which has 3 attributes, independently normally distributed N (0, 1). We then randomly generated 5 agents that forms the key group and 20 the regular group. Each agent also has 3 attributes, independently normally distributed N (0, 1). B was generated from Dirichlet distribution Dir(1). The result is averaged over 400 trials.
To echo the motivating example from the beginning of this paper, we simulated the process of eliciting key group’s preference by asking agents in the regular group questions. For simplicity, we consider 3 types of ques-

tions, represented in (k, l): (1, 2), (1, 10) and (9, 10). We run Algorithm 1 using 3 different information criteria: D-Optimality, E-Optimality, and the proposed MPC. The three elicitation processes utilized the cost function estimated in Section 5.1. They were initialized with the same set of 50 randomly generated pairwise comparisons and was given a $0.9 budget. Agents’ answers to the elicitation questions are generated from the Plackett-Luce model.

Metrics. We use total variation distance to measure the

difference between the winner distributions computed

from the ground truth parameter and the estimates, de-

noted by ψ∗ and ψ respectively. The total variance dis-

tance is deﬁned as δ(ψ∗, ψ) = 21

m i=1

|ψ∗

(ai

)

−

ψ(ai

)|.

To plot the results, at each cost w, we used the data point

that is below w but closest to w in each trial. These points

were averaged over all trials.

Observations. We observe that for both probabilistic plurality and probabilistic Borda, the performances of MPC, D-optimality, and E-optimality are similar, all of which signiﬁcantly outperforms random elicitation questions (see Figures 4). For example, for probabilistic plurality and probabilistic Borda, at the budget of 0.85 dollars, MPC achieves 15% less total variation distance than that of random elicitation questions. As another example, to achieve the total variation distance of 0.064 under randomized plurality (respectively, randomized Borda), MPC uses 20% (23.5%) less money than that of random elicitation questions.

We also observe that D-optimality almost always choose a full ranking as the most cost-effective question, while MPC tends to choose more full rankings than pairwise comparisons at early stages (see Figure 5). Due to the budget limit, many trials ﬁnish after 19 iterations because

Figure 4: Total variation distance for probabilistic plurality (left) and probabilistic Borda (right).

Figure 5: Types of questions chosen by the MPC (left) and D-optimality (right). The legend “Full Ranking”, “Top Choice”, and “Pairwise” correspond to (k = 9, l = 10), (k = 1, l = 10) and (k = 1, l = 2) respectively.

they only query full rankings. Others ﬁnish at different iterations. The distribution of types of questions for Eoptimality is similar to MPC. Under all criteria except random, l = 10, k = 1 questions were rarely asked.
Discussions. The meaning of cost-effectiveness in this paper is twofold: (1) in the preference elicitation procedure, we ask elicitation questions that is expected to provide more information per unit cost; and (2) the presence of regular group gives us a belief on the key group’s preferences inexpensively. As we have seen in our experiments, a budget of $0.9 gives us a reasonably good estimate of the key group’s preferences by querying the regular group.
6 CONCLUSIONS AND FUTURE WORK
We proposed a ﬂexible and cost-effective framework for preference elicitation that can be adapted for any ranking

model, any information criterion, and any set of questions. We used randomized voting rules to help make group decisions and proposed MPC for both prediction of one agent’s preference and aggregation of a group of agents’ preferences. Experiments show that MPC and other commonly-used information criteria work better than asking random elicitation questions. For future work we will explore better information criteria for group decisions. We also plan to extend this framework to multiple types of resource constraints.
Acknowledgements
We thank all anonymous reviewers for helpful comments and suggestions. This work is supported by NSF #1453542 and ONR #N00014-17-1-2621.

References
Hossein Azari Souﬁani, David C. Parkes, and Lirong Xia. Preference elicitation for general random utility models. In Proceedings of Uncertainty in Artiﬁcial Intelligence (UAI), Bellevue, Washington, USA, 2013.
Debarun Bhattacharjya and Jeffrey Kephart. Bayesian interactive decision support for multi-attribute problems with even swaps. In Proceedings of the Thirtieth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-14), Corvallis, Oregon, 2014.
Juergen Branke, Salvatore Corrente, Salvatore Greco, and Walter Gutjahr. Efﬁcient pairwise preference elicitation allowing for indifference. Computers & Operations Research, 88:175–186, 2017.
Urszula Chajewska, Daphne Koller, and Ron Parr. Making rational decisions using adaptive utility elicitation. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), pages 363–369, Austin, TX, USA, 2000.
Frederick H Chen and Jac C Heckelman. Winning probabilities in a pairwise lottery system with three alternatives. Economic Theory, 26(3):607–617, 2005.
Joanna Drummond and Craig Boutilier. Preference elicitation and interview minimization in stable matchings. In AAAI, pages 645–653, 2014.
Sylvain Ehrenfeld. On the efﬁciency of experimental designs. The annals of mathematical statistics, 26(2): 247–255, 1955.
Seda Erdem and Danny Campbell. Preferences for public involvement in health service decisions: a comparison between best-worst scaling and trio-wise stated preference elicitation techniques. The European Journal of Health Economics, 18(9):1107–1123, 2017.
Brochu Eric, Nando D Freitas, and Abhijeet Ghosh. Active preference learning with discrete choice data. In Advances in neural information processing systems, pages 409–416, 2008.
Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, and Richard Mann. Bayesian optimal active search and surveying. In Proceedings of the 29th International Conference on Machine Learning, 2012.
Allan Gibbard. Manipulation of schemes that mix voting with chance. Econometrica, 45:665–681, 1977.
Jac C Heckelman. Probabilistic Borda rule voting. Social Choice and Welfare, 21(3):455–468, 2003.
Neil Houlsby, Ferenc Husza´r, Zoubin Ghahramani, and Ma´te´ Lengyel. Bayesian active learning for classiﬁcation and preference learning. stat, 1050:24, 2011.

Neil Houlsby, Jose Miguel Hernandez-Lobato, Ferenc Huszar, and Zoubin Ghahramani. Collaborative Gaussian processes for preference learning. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 2105–2113, Lake Tahoe, NV, USA, 2012.
Dongling Huang and Lan Luo. Consumer preference elicitation of complex products using fuzzy support vector machine active learning. Marketing Science, 35(3):445–464, 2016.
Shali Jiang, Gustavo Malkomes, Geoff Converse, Alyssa Shofner, Benjamin Moseley, and Roman Garnett. Efﬁcient nonmyopic active search. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1714–1723, Sydney, Australia, 06–11 Aug 2017. PMLR.
Meir Kalech, Sarit Kraus, Gal A Kaminka, and Claudia V Goldman. Practical voting rules with partial information. Autonomous Agents and Multi-Agent Systems, 22(1):151–182, 2011.
Bruce G Lindsay. Composite likelihood methods. Contemporary Mathematics, 80, 1988.
Benedikt Loepp, Tim Hussein, and Ju¨ergen Ziegler. Choice-based preference elicitation for collaborative ﬁltering recommender systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 3085–3094. ACM, 2014.
Tyler Lu and Craig Boutilier. Robust approximation and incremental elicitation in voting protocols. In Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 287–293, Barcelona, Catalonia, Spain, 2011a.
Tyler Lu and Craig Boutilier. Vote elicitation with probabilistic preference models: Empirical estimation and cost tradeoffs. In International Conference on Algorithmic DecisionTheory, pages 135–149. Springer, 2011b.
Alexander M Mood et al. On hotelling’s weighing problem. The Annals of Mathematical Statistics, 17(4): 432–446, 1946.
Francesco Pauli, Walter Racugno, and Laura Ventura. Bayesian composite marginal likelihoods. Statistica Sinica, pages 149–164, 2011.
Thomas Pfeiffer, Xi Alice Gao, Andrew Mao, Yiling Chen, and David G. Rand. Adaptive polling and information aggregation. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), pages 122–128, Toronto, Canada, 2012.
Baharak Rastegari, Paul Goldberg, and David Manlove. Preference elicitation in matching markets via inter-

views: A study of ofﬂine benchmarks. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 1393– 1394, 2016.
Tuomas Sandholm and Craig Boutilier. Preference elicitation in combinatorial auctions. In Peter Cramton, Yoav Shoham, and Richard Steinberg, editors, Combinatorial Auctions, chapter 10, pages 233–263. MIT Press, 2006.
Oleg Volkov. Optimal Relaxed Designs of Experiments, with Pharmaceutical Applications. PhD thesis, Queen Mary University of London, 2014.
Abraham Wald. On the efﬁcient design of statistical investigations. The annals of mathematical statistics, 14 (2):134–140, 1943.
Marieke GM Weernink, Sarah IM Janus, Janine A van Til, Dennis W Raisch, Jeannette G van Manen, and Maarten J IJzerman. A systematic review to identify the use of preference elicitation methods in healthcare decision making. Pharmaceutical medicine, 28 (4):175–185, 2014.
Stephen E Wright, Belle M Sigal, and A John Bailer. Workweek optimization of experimental designs: exact designs for variable sampling costs. Journal of Agricultural, Biological and Environmental Statistics, 15(4):491–509, 2010.
Zhibing Zhao and Lirong Xia. Composite marginal likelihood methods for random utility models. In Proceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, 2018.

