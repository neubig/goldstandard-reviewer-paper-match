arXiv:2112.10645v1 [math.OC] 20 Dec 2021

Distributed and Stochastic Optimization Methods with Gradient Compression and Local Steps
Eduard Gorbunov Supervisors:
D.Sc., Professor at MIPT, Alexander Gasnikov Ph.D., Professor at KAUST, Peter Richtárik
A Thesis Submitted for the Degree of Doctor of Philosophy Phystech School of Applied Mathematics and Informatics Moscow Institute of Physics and Technology 2021

To my parents ii

Acknowledgments
I express my deepest gratitude to my supervisors Alexander Gasnikov and Peter Richtárik. I have learned a lot from both of you about various aspects of being a researcher. Thank you a lot for your guidance, encouragement, and opportunities that you provided. This all allowed me to realize my potential. Next, I am grateful to Pavel Dvurechensky for all his help and guidance, especially during the work on our ﬁrst joint papers. I thank all my co-authors for their work, fruitful discussions and great impact on my research (in the order of appearance of joint works): Evgeniya Vorontsova, Dmitry Kovalev, Elnur Gasanov, Ahmed Mohammed, Elena Chernousova, Konstantin Mishchenko, Martin Takáč, El Houcine Bergou, Darina Dvinskikh, César A. Uribe, Filip Hanzely, Adel Bibi, Ozan Sener, Sergey Guz, Maksim Shirobokov, Egor Shulgin, Aleksandr Beznosikov, Marina Danilova, Dmitry Makarenko, Alexander Rogozin, Sergey Guminov, Dmitry Kamzolov, Innokentiy Shibaev, Konstantin Burlachenko, Zhize Li, Max Ryabinin, Vsevolod Plokhotnyuk, Gennady Pekhimenko, Alexander Borzunov, Michael Diskin, Ilyas Fatkhullin, Igor Sokolov, Gauthier Gidel, Nicolas Loizou, and Hugo Berard. I also express my gratitude to Artem Babenko, Francis Bach, Aymeric Dieuleveut, Samuel Horváth, Praneeth Karimireddy, Eric Moulines, Anton Osokin, Alexander Panin, Liudmila Prokhorenkova, Adrien Taylor, and Aleksei Ustimenko for fruitful discussions. Further, I owe a great thanks to my internship advisor Gauthier Gidel and to Nicolas Loizou, who I actively collaborated with during my internship. I learned a lot from you! Finally, it is hard to express how much I appreciate all the support I received from MIPT and, in particular, from Andrei M. Raigorodskii and the Phystech School of Applied Mathematics and Informatics.
iii

Contents

List of Figures

ix

List of Tables

xii

1 Introduction

1

1.1 Stochastic First-Order Methods

1

1.2 Centralized Distributed Stochastic Optimization

3

1.2.1 Communication Compression

4

1.2.2 Local Updates

6

1.2.3 Non-Convex Distributed Optimization with Compression

7

1.3 Distributed Optimization Without a Central Server

8

1.4 Scientiﬁc Novelty

10

1.5 Presentations and Validation of Research Results

11

1.6 Publications

11

1.6.1 Excluded Papers

12

1.7 Thesis Structure

12

2 A Uniﬁed Theory of SGD: Variance Reduction, Sampling, Quantization

and Coordinate Descent

13

2.1 Introduction

13

2.2 The Many Faces of Stochastic Gradient Descent

14

2.3 Contributions

16

2.4 Main Result

17

2.4.1 Key Assumption

17

2.4.2 Main Theorem

19

2.5 The Classic, The Recent and The Brand New

21

2.6 Special Cases

25

2.6.1 Proximal SGD for Stochastic Optimization

25

2.6.2 SGD-SR

26

2.6.3 SGD-MB

28

2.6.4 SGD-star

30

2.6.5 SAGA

31

2.6.6 N-SAGA

33

2.6.7 SEGA

35

2.6.8 N-SEGA

36

2.6.9 SVRG

37

2.6.10 L-SVRG

38

iv

2.6.11 DIANA

40

2.6.12 Q-SGD-SR

43

2.6.13 VR-DIANA

44

2.6.14 JacSketch

47

2.6.15 Interpolation Between Methods

49

2.7 Experiments

52

2.7.1 Experiments on SGD-MB

52

2.7.2 Experiments on SGD-star

54

2.7.3 Experiments on N-SEGA

55

2.8 Discussion

56

3 Linearly Converging Error Compensated SGD

57

3.1 Introduction

57

3.2 Summary of Contributions

59

3.3 Main Result

64

3.4 Further Notation

66

3.5 SGD as a Special Case

67

3.6 Special Cases: SGD

68

3.6.1 DIANA with Arbitrary Sampling and Double Quantization

68

3.6.2 Recovering Tight Complexity Bounds for VR-DIANA

76

3.7 Distributed SGD with Compression and Error Compensation

78

3.8 Special Cases: Error Compensated Methods

80

3.8.1 EC-SGDsr

80

3.8.2 EC-SGD

83

3.8.3 EC-GDstar

88

3.8.4 EC-SGD-DIANA

90

3.8.5 EC-SGDsr-DIANA

95

3.8.6 EC-LSVRG

100

3.8.7 EC-LSVRGstar

105

3.8.8 EC-LSVRG-DIANA

108

3.9 Numerical Experiments

114

4 Local SGD: Uniﬁed Theory and New Eﬃcient Methods

117

4.1 Introduction

117

4.1.1 Our Contributions

119

4.2 Our Framework

120

4.3 Data Similarity and Local Loop

123

4.4 Local Stochastic Direction

125

4.4.1 Unbiased Local Gradient Estimator aki

125

4.4.2 Local Shift bki

126

4.4.3 Parameters of Assumption 4.2.3

127

v

4.5 Special Cases

127

4.5.1 Local-SGD

132

4.5.2 Local-SVRG

146

4.5.3 S*-Local-SGD

153

4.5.4 SS-Local-SGD

156

4.5.5 S*-Local-SGD*

166

4.5.6 S-Local-SVRG

170

4.6 Experiments

176

4.7 Conclusions and Future Work

176

5 MARINA: Faster Non-Convex Distributed Learning with Compression 178

5.1 Introduction

178

5.1.1 Contributions

180

5.1.2 Related Work

183

5.1.3 Preliminaries

184

5.2 MARINA: Compressing Gradient Diﬀerences

184

5.2.1 Convergence Results for Generally Non-Convex Problems

184

5.2.2 Convergence Results Under Polyak-Łojasiewicz Condition

186

5.3 MARINA and Variance Reduction

187

5.3.1 Finite Sum Case

187

5.3.2 Online Case

189

5.4 MARINA and Partial Participation

192

5.5 Numerical Experiments

193

5.5.1 Binary Classiﬁcation with Non-Convex Loss

193

5.5.2 Image Classiﬁcation

197

6 Moshpit SGD: Communication-Eﬃcient Decentralized Training on Hetero-

geneous Unreliable Devices

201

6.1 Introduction

201

6.2 Related Work

202

6.2.1 Data Parallel Training

202

6.2.2 Communication-Eﬃcient All-Reduce

203

6.2.3 Distributed Training in Unstable Conditions

204

6.2.4 Decentralized Training

204

6.3 Method Description

205

6.3.1 Moshpit Averaging

205

6.3.2 Convergence Analysis

207

6.3.3 Implementation Details

210

6.4 Experiments

210

6.4.1 Decentralized Averaging

210

6.4.2 ImageNet Training

211

vi

6.4.3 Masked Language Model Training

212

6.5 Conclusion

213

References

214

A Basic Facts, Technical Lemmas, and Auxiliary Results

235

A.1 Standard Deﬁnitions from Optimization Theory

235

A.2 Compression and Quantization Operators

236

A.3 Basic Inequalities

237

A.4 Identities and Inequalities Involving Random Variables

237

A.5 Auxiliary Results and Technical Lemmas

238

B Appendix for Chapter 3

244

B.1 Missing Plots

245

B.1.1 Compressing Stochastic Gradients

245

B.1.2 Compressing Full Gradients

246

B.2 Compression Operators: Extra Commentary

249

B.2.1 Unbiased Compressors

249

B.2.2 Biased Compressors

250

B.3 Proofs for Section 4.2

251

B.3.1 A Lemma

251

B.3.2 Proof of Theorem 3.3.4

252

B.4 Distributed SGD with Compression and Error Compensation: Missing Proofs

253

B.5 SGD with Delayed Updates

260

B.6 Special Cases: Delayed Updates Methods

265

B.6.1 D-SGD

265

B.6.2 D-QSGD

270

B.6.3 D-QSGDstar

274

B.6.4 D-SGD-DIANA

278

B.6.5 D-SGDsr

281

B.6.6 D-LSVRG

283

B.6.7 D-QLSVRG

286

B.6.8 D-QLSVRGstar

291

B.6.9 D-LSVRG-DIANA

294

C Appendix for Chapter 4

299

C.1 Table of Frequently Used Notation

300

C.2 Extra Experiments

301

C.2.1 Missing Details from Section 4.6 and an Extra Figure

301

C.2.2 The Eﬀect of Local Shift/Drifts

302

C.3 Missing Proofs for Section 4.2

303

C.3.1 Proof of Theorem 4.2.4

307

vii

C.3.2 Corollaries

308

C.4 Missing Proofs and Details for Section 4.3

310

C.4.1 Constant Local Loop

310

C.4.2 Random Local Loop

319

C.5 Missing Parts from Section 4.4

329

C.5.1 Proof of Lemma 4.4.3

330

D Appendix for Chapter 5

334

D.1 Missing Proofs for MARINA

334

D.1.1 Generally Non-Convex Problems

334

D.1.2 Convergence Results Under Polyak-Łojasiewicz Condition

337

D.2 Missing Proofs for VR-MARINA

340

D.2.1 Finite Sum Case

340

D.2.2 Online Case

349

D.3 Missing Proofs for PP-MARINA

358

D.3.1 Generally Non-Convex Problems

358

D.3.2 Convergence Results Under Polyak-Łojasiewicz Condition

362

E Appendix for Chapter 6

365

E.1 GPU Instance Costs

365

E.2 Additional Related Work

366

E.2.1 Decentralized Training

367

E.2.2 Compressed Communication

367

E.2.3 Multiple Local Steps

368

E.2.4 Asynchronous Methods

368

E.2.5 Distributed Hash Tables

369

E.3 Proofs of Mixing Properties of Moshpit All-Reduce

369

E.3.1 Computing Exact Average in a Full Grid

370

E.3.2 Proof of Theorem 6.3.1

371

E.3.3 Proof of Theorem 6.3.2

373

E.3.4 Additional Guarantees For Moshpit Averaging

376

E.4 Convergence Proofs of Moshpit SGD

382

E.4.1 Convex Case

382

E.4.2 Non-Convex Case

389

E.5 Training with a Dynamic Number of Peers

396

E.6 Load Balancing via Linear Programming

396

E.7 Detailed Experimental Setup

397

E.7.1 ImageNet Training

397

E.7.2 ALBERT Training

398

E.8 Additional Averaging Experiments

400

E.9 Additional Image Classiﬁcation Experiments

400

viii

List of Figures

2.1 SGD-MB and independent SGD applied on LIBSVM [27] datasets with regularization parameter λ = 10−5. Axis y stands for relative suboptimality, i.e. ff((xxkk))−−ff((xx∗0)) . Title label “unif” corresponds to probabilities chosen by 1 while label “imp” corresponds

to probabilities chosen by 2. Lastly, legend label “r” corresponds to “replacement”

with value “True” for SGD-MB and value “False” for independent SGD.

53

2.2 Comparison of SGD-star, SGD and SAGA on least squares problem.

54

2.3 N-SEGA applied on constrained least squares problem with noised partial derivative

oracle. Legend labels stand for the magnitude σ2 of the oracle noise.

55

3.1 Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to

solve logistic regression problem with 20 workers.

115

3.2 Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solve logistic regression

problem with 20 workers.

115

4.1 Comparison of standard Local-SGD (Alg. 27) and our Local-SVRG (Alg. 28) for

varying γ. Logistic regression applied on LibSVM [27]. Other parameters: L = 1, µ =

10−4, τ = 40. Parameter n chosen as per Tbl. C.2 in the appendix.

177

5.1 Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-

convex loss (5.10) with LibSVM data [27]. Parameter n is chosen as per Table C.2

(n = 5). Stepsizes for the methods are chosen according to the theory. In all cases,

we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

195

5.2 Comparison of VR-MARINA with VR-DIANA on binary classiﬁcation problem involving

non-convex loss (5.10) with LibSVM data [27]. Parameter n is chosen as per Table C.2

(n = 5). Stepsizes for the methods are chosen according to the theory and the

batchsizes are ∼ m/100. In all cases, we used the RandK sparsiﬁcation operator with

K ∈ {1, 5, 10}.

196

5.3 Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-

convex loss (5.10) with mushrooms dataset and n = 20 workers. Stepsizes for the

methods are chosen according to the theory. In all cases, we used the RandK

sparsiﬁcation operator with K ∈ {1, 5, 10}.

197

5.4 Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset.

Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes

are ∼ m/50. In all cases, we used the RandK sparsiﬁcation operator, the approximate

values of K are given in the legends (d is dimension of the problem).

198

ix

5.5 Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset.

Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes

are ∼ m/50. We used the RandK sparsiﬁcation operator, the approximate values

of K are given in the legends (d is dimension of the problem). We also show the

performance of VR-MARINA and VR-DIANA without compression.

200

6.1 A schematic illustration of Butterﬂy All-Reduce.

203

6.2 Example averaging order for 16 peers in 2 rounds. On each round, peers are split

into 4 groups that run All-Reduce in parallel.

205

6.3 Averaging error for Moshpit All-Reduce.

211

6.4 (Left, Middle) ResNet-50 top-1 validation accuracy for ImageNet as a function of

training time (left) and epochs (middle). (Right) Full training objective (MLM +

SOP) of ALBERT-large on BookCorpus as a function of training time.

212

B.1 Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to

solving logistic regression problem with 20 workers. EC-SGD identical corresponds

to SGD with error compensation with the identity compression operator C(x) = x, i.e.,

it is just parallel SGD.

245

B.2 Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solving logistic regression

problem with 20 workers.

246

B.3 Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solving logistic regression

problem with 100 workers.

247

B.4 Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solving logistic

regression problem with 20 workers.

248

B.5 Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solve logistic regres-

sion problem with 100 workers.

249

C.1 Comparison of standard Local-SGD (Algorithm 27), and Local-SVRG (Algorithm 28)

with various stepsizes γ. Logistic regression applied on LibSVM data [27] with

heterogenously splitted data. Other parameters: L = 1, µ = 10−4, τ = 40. Parameter

n chosen as per Table C.2. (Same as Figure 4.1, but with the heterogenous data split)301

C.2 Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27

with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and

S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 0 (see

Table C.3).

304

C.3 Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27

with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and

S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 1 (see

Table C.3).

305

x

C.4 Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27

with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and

S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 2 (see

Table C.3).

306

C.5 Comparison of the following noiseless algorithms: Local-SGD (LGD, Algorithm 27

with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and

S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 3 (see

Table C.3).

307

E.1 Averaging error of Moshpit All-Reduce as a function of the iteration number for

diﬀerent conﬁgurations and failure rates.

400

E.2 ResNet-50 top-1 validation accuracy on ImageNet when training on a single node

with 8× V100-PCIe GPUs. (Left) Convergence in terms of training time, (Right)

Convergence in terms of training epochs

401

xi

List of Tables

2.1 List of speciﬁc existing (in some cases generalized) and new methods which ﬁt our

general analysis framework. VR = variance reduced method, AS = arbitrary sampling,

Quant = supports gradient quantization, RCD = randomized coordinate descent

type method. a Special case of SVRG with 1 outer loop only; b Special case of DIANA

with 1 node and quantization of exact gradient.

22

2.2 The parameters for which the methods from Table 2.1 (special cases of (2.5)) satisfy

Assumption 2.4.1. The meaning of the expressions appearing in the table, as well as

their justiﬁcation is deﬁned in detail in Section 4.5.

23

2.3 Four types of least squares.

55

3.1 Complexity of Error-Compensated SGD methods established in this chapter. Sym-

bols: ε = error tolerance; δ = contraction factor of compressor C; ω = variance

parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 =

average of the uniform bounds for the variances of stochastic gradients of workers.

EC-GDstar, EC-LSVRGstar and EC-LSVRG-DIANA are the ﬁrst EC methods with a

linear convergence rate without assuming that ∇fi(x∗) = 0 for all i. EC-LSVRGstar

and EC-LSVRG-DIANA are the ﬁrst EC methods with a linear convergence rate which

do not require the computation of the full gradient ∇fi(xk) by all workers in each iter-

ation. Out of these three methods, only EC-LSVRG-DIANA is practical. †EC-GD-DIANA

is a special case of EC-SGD-DIANA where each worker i computes the full gradient

∇fi(xk).

61

3.2 Error compensated methods developed in this work. In all cases, vik = C(eki + γgik).

The full descriptions of the algorithms are included in Section 3.8.

62

3.3 Summary of datasets: N = total # of data samples; d = # of features.

114

4.1 The eﬀect of data similarity and local loop on Assumption 4.2.3. Constant factors

are ignored. Homogeneous data are recovered as a special case of ζ-heterogeneous

data with ζ = 0. Heterogeneous case is slightly loose in light of Remark 4.2.6. If one

replaces the bound on the second moments (4.8) with a analogous bound on variance

squared expectation (see Assumption C.4.1), the bounds on γ, D3 and H will have

(τ

− 1)

times

better

dependence

on

the

variance

parameters

(or

1−p p

times

for

the

random loop). See Section C.4.1 and C.4.2 of appendix for more details.

124

xii

4.2 A selection of methods that can be analyzed using our framework, which we detail

in the appendix. A choice of aki , bki and lik is presented along with the established complexity bounds (= number of iterations to ﬁnd such xˆ that E[f (xˆ) − f (x∗)] ≤ ε)

and a speciﬁc setup under which the methods are analyzed. For Algorithms 1-4 we

suppress

constants

and

log

1 ε

factors.

Since

Algorithms

5

and

6

converge

linearly,

we

suppress

constants

only

while

keeping

log

1 ε

factors.

All

rates

are

provided

in

the

strongly convex setting. UBV stands for the “Uniform Bound on the Variance”

of local stochastic gradient, which is often assumed when fi is of the form (5.5).

ES stands for the “Expected Smoothness” [63], which does not impose any extra

assumption on the objective/noise, but rather can be derived given the sampling

strategy and the smoothness structure of fi. Consequently, such a setup allows

us to obtain local methods with importance sampling. Next, the simple setting is

a special case of ES when we uniformly sample a single index on each node each

iteration. ♣: Local-SGD methods have never been analyzed under ES assumption.

Notation: σ2 – averaged (within nodes) uniform upper bound for the variance of local

stochastic gradient, σ∗2 – averaged variance of local stochastic gradients at the solution,

ζ∗2

d=ef

1 n

n i=1

∇fi(x∗) 2, max Lij – the worst smoothness of fi,j, i ∈ [n], j ∈ [m], L –

the worst ES constant for all nodes.

130

4.3

A

selection

of

methods

that

can

be

analyzed

using

our

framework.

A

choice

of

a

k i

,

bki

and lik is presented along with the established complexity bounds (= number of

iterations to ﬁnd such xˆ that E[f (xˆ) − f (x∗)] ≤ ε) and a speciﬁc setup under which

the methods are analyzed. For all algorithms we suppress constants factors. All

rates are provided in the weakly convex setting. UBV stands for the “Uniform

Bound on the Variance” of local stochastic gradient, which is often assumed when

fi is of the form (5.5). ES stands for the “Expected Smoothness” [63], which does

not impose any extra assumption on the objective/noise, but rather can be derived

given the sampling strategy and the smoothness structure of fi. Consequently, such

a setup allows us to obtain local methods with importance sampling. Next, the

simple setting is a special case of ES when we uniformly sample a single index on

each node each iteration. ♣: Local-SGD methods have never been analyzed under

ES assumption. Notation: σ2 – averaged (within nodes) uniform upper bound for

the variance of local stochastic gradient, σ∗2 – averaged variance of local stochastic

gradients

at

the

solution,

ζ∗2

d=ef

1 n

n i=1

∇fi(x∗) 2, max Lij – the worst smoothness

of fi,j, i ∈ [n], j ∈ [m], L – the worst ES constant for all nodes, R0 d=ef x0 − x∗ –

distance of the starting point x0 from the closest solution x∗, ∆0 d=ef f (x0) − f (x∗). 131

xiii

5.1 Summary of the state-of-the-art results for ﬁnding an ε-stationary point for the

problem (5.1), i.e., such a point xˆ that E ∇f (xˆ) 2 ≤ ε2. Dependences on the

numerical constants, “quality” of the starting point, and smoothness constants are

omitted in the complexity bounds. Abbreviations: “PP” = partial participation;

“Communication complexity” = the number of communications rounds needed to ﬁnd

an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order

oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization

parameter (see Def. A.2.1); n = the number of nodes; m = the size of the local

dataset; r = (expected) number of clients sampled at each iteration; b = the batchsize

for VR-MARINA at the iterations with compressed communication. To simplify the

bounds, we assume that the expected density ζQ of the quantization operator Q (see

Def. A.2.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and 2-quantization,

see [20]). We notice that [67] and [32] contain also better rates under diﬀerent

assumptions on clients’ similarity.

180

5.2 Summary of the state-of-the-art results for ﬁnding an ε-solution for the problem (5.1)

satifying Polyak-Łojasiewicz condition (see As. 5.2.4), i.e., such a point xˆ that

E [f (xˆ) − f (x∗)] ≤ ε. Dependences on the numerical constants and log(1/ε) factors

are omitted and all smoothness constanst are denoted by L in the complexity bounds.

Abbreviations: “PP” = partial participation; “Communication complexity” = the

number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle

complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-

stationary point. Notation: ω = the quantization parameter (see Def. A.2.1); n = the

number of nodes; m = the size of the local dataset; r = (expected) number of clients

sampled at each iteration; b = the batchsize for VR-MARINA at the iterations with

compressed communication. To simplify the bounds, we assume that the expected

density ζQ of the quantization operator Q (see Def. A.2.1) satisﬁes ω + 1 = Θ(d/ζQ)

(e.g., this holds for RandK and 2-quantization, see [20]). We notice that [67] and

[32] contain also better rates under diﬀerent assumptions on clients’ similarity.

182

5.3 Summary of the datasets and splitting of the data among clients (Figure 5.1).

194

5.4 Summary of the parameters used in the experiments presented in Fig. 5.4 and

Fig. 5.5. Stepsizes were tuned, batchsize = 256 on each worker, other parameters were

picked according to the theory, except the last line, where p for VR-MARINA without

compression was picked as for VR-MARINA with RandK, K = 100 000 compression

operator.

198

xiv

B.1 Complexity of SGD methods with delayed updates established in this chapter. Symbols:

ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter

of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average

of the uniform bounds for the variances of stochastic gradients of workers; M2,q =

(ω + 1)σ2 + ωζ∗2; σq2 = (1 + ω)

1

+

ω n

σ2. †D-QGDstar is a special case of D-QSGDstar

where each worker i computes the full gradient ∇fi(xk); ‡D-GD-DIANA is a special

case of D-SGD-DIANA where each worker i computes the full gradient ∇fi(xk).

266

B.2 The parameters for which the methods from Tables 3.1 and B.1 satisfy Assump-

tion 3.3.3. The meaning of the expressions appearing in the table, as well as their

justiﬁcation is deﬁned in details in the Sections 3.8 and B.6. Symbols: ε = error

tolerance; δ = contraction factor of compressor C; ω = variance parameter of compres-

sor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform

bounds for the variances of stochastic gradients of workers.

298

C.1 Summary of frequently used notation.

300

C.2 Number of clients per dataset (Figures 4.1 and C.1).

301

C.3 Instances of (C.1).

303

C.4 The parameters for which the methods from Table 4.2 satisfy Assumption 4.2.3/C.4.1.

Absolute constants were omitted. The meaning of the expressions appearing in

the table, as well as their justiﬁcation, is detailed in Section 4.5. UBV stands for

the “Uniform Bound on the Variance” of local stochastic gradient, which is often

assumed when fi is of the form (5.5). ES stands for the “Expected Smoothness”

inequality [63], which does not impose any extra assumption on the objective/noise,

but rather can be derived given the sampling strategy and the smoothness structure

of fi. Consequently, such a setup allows us to obtain local methods with importance

sampling. Next, the simple setting is a special case of ES when we uniformly sample

a single index on each node each iteration.

333

E.1 Cloud and marketplace GPU instance pricing for short-term usage.

366

E.2 Heterogeneous setup for ImageNet training.

398

E.3 Heterogeneous setup for ALBERT training.

399

xv

Chapter 1
Introduction

In1 this chapter, we give a general introduction with an overview of the developed results in this thesis. All subsequent chapters also contain their own detailed introductions.

1.1 Stochastic First-Order Methods

Stochastic optimization [200, 109] is a young but rapidly developing branch of optimization. Stochastic optimization methods are at the heart of various applications of statistics [204] and machine learning [47, 197]. Sometimes the use of stochasticity is dictated by the nature of the optimization problem, in other situations, people artiﬁcially introduce stochasticity to solve the problem faster, e.g., in randomized coordinate-wise methods [151, 181, 102] and stochastic derivative-free approaches [154, 54, 38, 16, 49].

Due to their practical eﬃciency and simplicity in implementation, stochastic ﬁrst-order methods

are the most popular stochastic optimization methods. The simplest and brightest example of

such a method is Stochastic Gradient Descent (SGD) [182]. In its basic form, SGD applied to the

unconstrained minimization problem

min f (x)
x∈Rd

(1.1)

has the update rule

xk+1 = xk − γkgk,

(1.2)

where {xk}k≥0 is the sequence of optimization variables, {γk}k≥0 is the sequence of stepsizes, and

{gk}k≥0 are stochastic gradients – the key ingredient in SGD. In a nutshell, stochastic gradient gk

is a random vector that, in some sense, approximates the true gradient ∇f (xk) of the objective

function f at the point xk. Of course, in each particular situation, it should be clariﬁed in what

sense gk approximates ∇f (xk). Typically, this means that gk is an unbiased estimate of ∇f (xk)

for ﬁxed xk:

E gk | xk = ∇f (xk).

(1.3)

1The work on this thesis was partially supported by RFBR 19-31-51001 and was partially supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) 075-00337-20-03, project no. 0714-2020-0005.

1

Although this assumption is natural, it is not enough to ensure the convergence of SGD to some solution of the problem (1.1). Therefore, it is necessary to introduce additional assumptions on the stochastic gradient. Moreover, before that, it needs to be clariﬁed what we mean by the “convergence” of a stochastic method.
As in the majority of papers on stochastic optimization, in this thesis, we focus on the convergence in expectation, i.e., we study the convergence rates of the considered methods to achieve a desired accuracy of the solution (in terms of functional suboptimality/squared distance to the solution/squared norm of the gradient) in expectation. In many real-world problems, “inexpectation” convergence guarantees are in good correspondence with behavior of the method during a particular run and they are often easier to derive than their high-probability counterparts. However, we emphasize that for a deeper understanding of the stochastic methods, it is also highly important to analyze their high-probability convergence rates [149, 44, 45, 144, 33, 52, 53, 30], as well as limit distributions [167, 76, 66, 224] and almost-surely convergence guarantees [23, 245, 156, 137, 194, 160].
The classical convergence guarantees for SGD [182, 149] rely on the bounded second moment assumption:
E gk 2 | xk ≤ G2
for some constant G > 0. Although this assumption is reasonable for convex non-smooth objectives, it does not hold for strongly convex problems and for several smooth convex problems. To resolve this issue for smooth problems, one can analyze [46] SGD assuming only boundedness of the variance:
E gk − ∇f (xk) 2 | xk ≤ σ2
for some σ ≥ 0. Next, if stochastic realizations of the objective function f are smooth, then one can relax this assumption even further [156, 63]. Moreover, taking into account some structural properties of the problem one can construct gk in such a way that it will satisfy certain inequalities needed to derive the convergence of the resulting method. For example, in ﬁnite-sum optimization, one can consider variance reduced methods [184, 82, 35], in distributed optimization, one might be interested in designing parallel stochastic methods with communication compression [4, 196], and, when the dimension of the problem is an issue, one can use coordinate-wise randomization [151].
As a result, a lot of diﬀerent stochastic methods appeared in the literature and were analyzed under various assumptions. However, a large group of SGD methods have update rules of the form (1.2) with gradient estimates satisfying (1.3). Therefore, it is important to have a clean systematic way to analyze all of them, i.e., have a general theoretical framework that provides tight analysis for all of these methods.
2

The First Contribution: Uniﬁed Theory of SGD
Our ﬁrst contribution is a general analysis of SGD in the strongly convex case with proximable regularization. That is, we propose a uniﬁed assumption on the stochastic gradients and the problem that covers various existing methods in diﬀerent settings. Whenever we recover a known method, our general theorem provides the tightest know rate for this method. Moreover, inspired by the proposed theoretical framework, we generalize several existing methods and develop new stochastic methods.
Chapter 2 is devoted to the ﬁrst contribution of this thesis and is based on the following paper:
[55] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A Uniﬁed Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 680–690. PMLR, 26–28 Aug 2020.

1.2 Centralized Distributed Stochastic Optimization

As we mentioned earlier, stochastic optimization methods are widely used in machine learning applications. With the growth of data and complexity of models it became inevitable to consider ways of solving the problems in a parallel/distributed way. Indeed, training modern deep neural networks would take a prohibitively long time (e.g., days or even years of computations) if executed on a single machine, even if this machine is a top-of-the-line GPU server [113]. Therefore, distributed stochastic methods are usually applied in such problems [64, 236], where parallel computations help to reduce the training time signiﬁcantly. Moreover, distributed methods are the natural choice when the data is private and/or distributed across multiple devices, e.g., in federated learning [100, 134].

In its general form, distributed unconstrained optimization problem can be deﬁned in the following way: n devices/peers/workers/nodes/machines solve the minimization problem

1n

min f (x) :=

fi(x) ,

x∈Rd

n i=1

(1.4)

where function fi is known for worker i only but not necessarily to other workers, meaning that worker i can compute some speciﬁc quantities such as functional value or (stochastic) gradient of fi but other workers do not necessarily have an access to this information. For example, in federated learning, the information about function fi is privately stored on device i, and f1, . . . , fn are naturally heterogeneous. In large-batch training of deep neural networks, all functions fi can be equal to f .

3

Perhaps the simplest SGD variant for solving (1.4) is Parallel SGD [247]:

xk+1 = xk − γkgk = xk − γnk n gik,
i=1

(1.5)

where gik is a stochastic gradient of function fi at point xk. That is, at each iteration of Parallel SGD, workers ﬁrst compute stochastic gradients gik, and, after that, vectors gik for i = 1, . . . , n are aggregated and new point xk+1 is computed. Here the following natural question arises: how
are the stochastic gradients aggregated?

The classical and historically ﬁrst way of gradients aggregation is to use the Parameter Server architecture [114]. In this approach, workers cannot communicate between each other directly, and instead are only allowed to communicate with a dedicated machine: a server or master. Therefore, to update xk+1 via (1.5), workers need to send the gradients gik to the server. After that, the server averages the received vectors, computes xk+1, and broadcasts the result back to the workers.

Despite its simplicity, this idea works quite well in practice. However, Parallel SGD has a signiﬁcant issue that rapidly becomes evident with the growth of the number of workers n and/or growth of the dimension of the problem d. This issue is called communication bottleneck. It means that for large enough n or d, communication may take much more time than computation. This happens because of several reasons: 1) stochastic gradients gik can be dense and huge-dimensional, 2) workers communicate at each iteration of the method, and 3) a single machine (server) is responsible for aggregating a large amount of information at each iteration. In this thesis, we address all these three problems separately.

1.2.1 Communication Compression

The natural way of addressing the communication bottleneck is to use communication compression [196, 215], which is based on applying compression to the gradient vectors or tensors that workers need to send to the master. For example, one can modify Parallel SGD in the following way [4]:

xk+1 = xk − γnk n C(gik),
i=1

(1.6)

where C : Rd → Rd is some (possibly randomized) operator called compression operator. This
method is usually called Compressed or Quantized SGD (QSGD). In this scheme, instead of sending gik, the workers send the compressed message C(gik) to the server. Therefore, the operator C is designed in such a way that transmitting C(gik) requires much less time than transmitting gik. For example, one can use the so-called RandK operator that picks K components of the input
uniformly at random and scales the result to ensure unbiasedness:

RandK(x) = d xiei. K i∈S

4

Here (e1, e2, . . . , ed) is a standard basis in Rd, x = (x1, . . . , xd) ∈ Rd, and S is a random set uniformly distributed on the family of K-element subsets of {1, 2, . . . , d}. When K d, the per-iteration communication cost of QSGD is signiﬁcantly smaller than for Parallel SGD.
Moreover, in the (strongly) convex case, one can prove that QSGD converges to the solution with any predeﬁned accuracy if the operator C satisﬁes

E [C(x)] = x, E C(x) − x 2 ≤ ω x 2

(1.7)

with some ω ≥ 0 for all x ∈ Rd. Compression operators satisfying (1.7) are usually called unbiased compressors. Although inequality (1.7) is satisﬁed for a wide range of compression operators, it does not cover several practically important biased compression operators such as the TopK compression operator that picks K components of the input with the largest absolute values. Usually, when the compression operator C is biased, it is assumed that

E C(x) − x 2 ≤ (1 − δ) x 2

(1.8)

with some δ ∈ (0, 1] for all x ∈ Rd. Interestingly, Compressed SGD (1.6) with biased compression C may diverge exponentially fast even for strongly convex problems [20]. To circumvent this issue, one can use the so-called error compensation mechanism [196]. The resulting method is usually called Error Compensated SGD (EC-SGD), and has the following update rule:

xk+1 = xk − n1 n vik,
i=1

vik = C(γkgik + eki ),

eki +1 = γkgik + eki − vik.

(1.9)

Here, each worker i “memorizes” the unsent information eki +1 = γkgik + eki − vik in order to use it during the next iterations.

EC-SGD was analyzed in many papers under diﬀerent assumptions [208, 209, 20]. However, before this thesis, there were several important gaps in the theory of stochastic methods with error compensation in the (strongly) convex case. In particular, there were no full-gradient methods (gik = ∇fi(xk)) with error compensation that have linear convergence in the strongly convex case. Moreover, there were no variance reduced variants of EC-SGD and variants with arbitrary sampling was never analyzed.

The Second Contribution: Uniﬁed Theory of Error Compensated Methods
Our second contribution in this thesis can be seen as an extension of the ﬁrst contribution to the class of methods with error compensation. That is, we propose a new uniﬁed theoretical framework for the analysis of stochastic ﬁrst-order methods supporting error compensation. Using this framework, we develop new eﬃcient error-compensated methods. In particular, we develop the ﬁrst full-gradient methods with error compensation that have linear convergence in the strongly convex case and the ﬁrst variance reduced method with error compensation that also enjoys linear convergence on strongly convex problems. Moreover, our framework

5

covers methods with delayed updates. Overall, using this new framework we develop 16 new optimization methods.
Chapter 3 is devoted to the second contribution of this thesis, and is based on the following paper:
[57] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly Converging Error Compensated SGD. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20889–20900. Curran Associates, Inc., 2020.

1.2.2 Local Updates

Another popular way of addressing communication bottleneck is to use more computations locally on workers between two sequential communication rounds. For example, workers can perform several (τ ≥ 1) SGD steps between two neighboring communications rounds rather than a single (τ = 1) step. Formally, the update of resulting method can be written in the form:



xki − γkgik,

xki +1 =

n
1 (xk − γ gk),

 n

i

ki

i=1

if k + 1 if k + 1

mod τ = 0, mod τ = 0,

(1.10)

where xki denotes the local iterate stored on node i ∈ {1, . . . , n} at iteration k. This method is known as Local-SGD/Federated Averaging (FedAvg) [100, 134, 205]. Local-SGD and its diﬀerent variants gained a lot of attention and were studied in a number of papers [247, 135, 205, 125, 123, 231, 87, 89, 229]. However, several promising directions, such as better understanding of so-called local shifts, more sophisticated local gradient estimators allowing importance sampling, variance reduction or coordinate descent, variable number of local steps, and general theory supporting diﬀerent data similarity types, were unexplored in the previous works.

The Third Contribution: Uniﬁed Theory of Methods with Local Updates
Motivated by the ﬁrst two contributions, we propose yet another uniﬁed theoretical framework, this time for the analysis of Local-SGD-type methods, in the regime when the objective function is (strongly) convex. We recover multiple known local optimizers as a special case of our general framework, along with their convergence rates (up to small constant factors). To demonstrate the strengths of our approach we develop a new method called S-Local-SVRG ﬁtting our general framework. Moreover, using our general theorem we prove that S-Local-SVRG converges linearly even when the local loss functions are arbitrarily heterogeneous. This is the ﬁrst variance reduced linearly converging Local-SGD method. Moreover, to obtain this result, we did not need to rely on any restrictive assumptions such as gradient boundedness or gradients similarity.
Chapter 4 is devoted to the third contribution of this thesis and based on the following paper:
[56] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. Local SGD: Uniﬁed Theory and

6

New Eﬃcient Methods. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3556–3564. PMLR, 13–15 Apr 2021.
1.2.3 Non-Convex Distributed Optimization with Compression
In the previous sections, we focus on (quasi-strongly) convex problems. However, there are many practically important problems that are non-convex, including training deep neural networks [47], and matrix completion and recovery [130, 21]. Clearly, it is important to design eﬃcient SGD-type methods for solving non-convex problems [31].
Nowadays one of the most popular example of non-convex optimization problems is training of deep neural networks. As we mentioned before, some of these tasks are so computationally hard that even top-of-the-line GPU servers [113] may require years of computations to solve them. Therefore, such problems are necessarily solved in a distributed manner.
As is the case in the convex regime, communication bottleneck appears in non-convex distributed optimization too, and one can handle this issue using communication compression. The optimization and machine learning communities have exerted considerable eﬀort in recent years to design distributed methods supporting compressed communication. From the many methods proposed, we emphasize here VR-DIANA [79], FedCOMGATE [67], and FedSTEPH [32] because they are supported by the state-of-the-art theoretical complexity results in the setup when the local loss functions are allowed to be arbitrarily heterogeneous.
The Fourth Contribution: Faster Methods for Non-Convex Distributed Optimization with Compression
We develop and analyze MARINA: a new communication eﬃcient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient diﬀerences that is reminiscent of but diﬀerent from the strategy employed in the DIANA method [139]. Unlike virtually all competing distributed ﬁrstorder methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. The communication complexity bounds we prove for MARINA are evidently better than those of all previous ﬁrst-order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The ﬁrst method is designed for the case when the local loss functions owned by clients are either of a ﬁnite sum or of an expectation form, and the second method allows for a partial participation of clients – a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of oracle/communication complexity. Finally, we provide a convergence analysis of all methods for problems satisfying the Polyak-Łojasiewicz condition.
Chapter 5 is devoted to the fourth contribution of this thesis, and is based on the following paper:
7

[51] Eduard Gorbunov, Konstantin P. Burlachenko, Zhize Li, and Peter Richtárik. MARINA: Faster Non-Convex Distributed Learning with Compression. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 3788–3798. PMLR, 18–24 Jul 2021.

1.3 Distributed Optimization Without a Central Server

In situations when it is possible to engineer the network that deﬁnes communication links among the machines, one can handle communication bottleneck even without compressed communications and local updates. As we explained in the previous sections, in the parameterserver architecture the communication bottleneck arises mainly because of the existence of a machine (server) that aggregates a lot of data at each iteration. To alleviate this issue, one can change the communication protocol in such a way that no machine is required to aggregate too much data at any iteration.

One of the most popular decentralized communication protocols is gossip [24, 218, 122]. For any given network structure, and initial vectors x01, x02, . . . , x0n ∈ Rd, gossip generates the sequence of points {xki }k≥0 on each worker i = 1, . . . , n such that

n
xki +1 = Mij xkj ,
j=1

(1.11)

where Mij is the i, j-th element of a mixing matrix M. The key property of a mixing matrix is

that Mi,j = 0 iﬀ i = j and (i, j) ∈ E, where E denotes the set of edges in the communication

network. Further, for (i, j) ∈ E it satisﬁes Mij > 0 and Mii > 0 for all i = 1, . . . , n. Moreover,

it is usually assumed that M is symmetric M = M , M1 = 1, where 1 = (1, . . . , 1) ∈ Rn,

and λ2(M) < 1, where λ2(M) is the absolute value of the second largest (in absolute value)

eigenvalue of M [59]. Under these assumptions gossip converges linearly to the exact average of

x

0 1

,

x02

,

.

.

.

,

x

0 n

as

follows:

Xk − X 2 ≤ (λ2(M))k X0 − X 2,

where

Xk

=

[

xk1

,

xk2

,

.

.

.

,

x

k n

]

∈

Rd×n

and

X

=

[x, x, . . . , x]

∈

Rd×n,

x

=

1 n

n i=1

x0i .

That

is,

gossip

ﬁnds approximate average on nodes with accuracy Xk−X 2 ≤ ε after O (1 − λ2(M))−1 log(ε−1)

iterations. The quantity η = 1 − λ2(M) is called the spectral gap of the mixing matrix M, and

η−1 is typically a polynomial of the total number of nodes n when the maximal degree of the

node is O(1). For example, for uniformly averaging M one can show that η−1 = O(n2) for the

ring topology (node degree 2), η−1 = O(n) for the two-dimensional torus topology (node degree

2), and η−1 = O(1) for the fully connected graph (node degree n − 1) [3].

One or several steps of gossip can be used in distributed optimization algorithms as an alternative to aggregation through the central server, e.g., in Parallel SGD. Choosing the communication graph in such way that there are no “overloaded” nodes, i.e., each node has a degree O(1), one can

8

signiﬁcantly reduce the cost of one communication round in comparison to the parameter-server architecture. However, the communication complexity of gossip-based decentralized optimization methods often has multiplicative dependence on either O(η−1) (see [233] and references therein) or O(η−1/2) [190, 219, 39, 105], which is not improvable for gossip-based methods [9, 191]. Since in the practically interesting cases we have η = Ω(n), it means that the overall number of communication rounds needed to achieve the desired accuracy of the solution grows with the number of workers n as Ω(n) or Ω(√n).
As an alternative to gossip, many practical distributed training systems perform averaging with All-Reduce [64, 138, 203, 236]. This name refers to a collection of protocols originally developed for HPC applications. Workers can follow these protocols to collectively compute the average gradient more eﬃciently than with a central server. The simplest variant of All-Reduce is known as Butterﬂy All-Reduce [159]. Each of n participants splits its local vector into n chunks. Then, the i-th worker aggregates the i-th chunk of data from all peers and sends back the averaged chunk. As long as the vector size s is greater than n, this protocol uses O s × n−n 1 total bandwidth on each worker. However, it requires all-to-all communication, which is not always practical for the HPC infrastructure. Real-world systems typically use Ring or Tree All-Reduce, where each worker only communicates with a small subset of its peers. These protocols enable highly eﬃcient and scalable averaging with O(1) or O(log N ) total communication per worker.
As a result, All-Reduce Parallel SGD enjoys the beneﬁts of two worlds: the number of communication rounds does not grow with n, and each worker handles O(s) amount data only, where s is the size of one vector. However, All-Reduce protocols share a common drawback: they cannot tolerate node failures or network instability. If any single participant fails to execute its part or takes long to respond, this paralyzes all other workers. In contrast, gossip-based algorithms are more robust to such changes, which makes them applicable to time-varying networks [145, 146, 147, 183] and federated learning [173, 234, 239].
The Fifth Contribution: Fault-Tolerant and Communication-Eﬃcient Decentralized Optimization Method
In this thesis, we lift the above restrictions by proposing Moshpit All-Reduce — an iterative averaging protocol that exponentially converges to the global average even with unreliable communication-constrained devices. According to our analysis, this method has exponential convergence independent of the network topology. Armed with this averaging protocol, we develop Moshpit SGD for distributed optimization. We derive convergence rates for this algorithm and establish its equivalence to Centralized (Local) SGD for (strongly) convex and non-convex problems.
Chapter 6 is devoted to the ﬁfth contribution of this thesis, and is based on the following paper:
[185] Max Ryabinin*, Eduard Gorbunov*, Vsevolod Plokhotnyuk, and Gennady Pekhimenko (*equal contribution). Moshpit SGD: Communication-Eﬃcient Decentralized Training on
9

Heterogeneous Unreliable Devices. Advances in Neural Information Processing Systems, volume 34 (accepted), 2021.
1.4 Scientiﬁc Novelty
All results are new. They are summarized as follows:
• We propose new general analysis of SGD in the strongly convex case with proximable regularization. Our approach covers various existing methods in diﬀerent settings. Whenever we recover a known method, our general theorem provides the tightest know rate for this method. Moreover, inspired by the proposed theoretical framework, we develop new stochastic methods (SGD-MB, SGD-star, N-SEGA, N-SAGA, Q-SGD-SR).
• We propose a new uniﬁed theoretical framework for the analysis of stochastic ﬁrst-order methods with error compensation and delayed updates. Using this framework, we develop 16 new methods. In particular, we develop the ﬁrst full-gradient methods with error compensation that have linear convergence in the strongly convex case (EC-SGD-DIANA) and the ﬁrst variance reduced method with error compensation that also enjoys linear convergence on strongly convex problems (EC-LSVRG-DIANA).
• We develop a new uniﬁed theoretical framework for the analysis of Local-SGD-type methods when the objective function is (strongly) convex. We recover multiple known local optimizers as a special case of our general framework, along with their convergence rates (up to small constant factors). To demonstrate the strengths of our approach, we develop a new method called S-Local-SVRG ﬁtting our general framework. Moreover, using our general theorem we prove that S-Local-SVRG converges linearly even when the local loss functions are arbitrarily heterogeneous. That is, we propose the ﬁrst variance reduced linearly converging method without any restrictive assumptions.
• We develop and analyze MARINA: a new communication eﬃcient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient diﬀerences. Unlike virtually all competing distributed ﬁrst-order methods, ours is based on a carefully designed biased gradient estimator. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The ﬁrst method is designed for the case when the local loss functions owned by clients are either of a ﬁnite sum or of an expectation form, and the second method allows for partial participation of clients. The proposed methods are superior to previous state-of-the-art methods in terms of oracle/communication complexity. Finally, we provide a convergence analysis of all methods for problems satisfying the Polyak-Łojasiewicz condition.
• We develop Moshpit All-Reduce — an iterative averaging protocol that exponentially converges to the global average even with unreliable communication-constrained devices.
10

According to our analysis, this method has exponential convergence independent of the network topology. Armed with this averaging protocol, we develop Moshpit SGD for distributed optimization. We derive convergence rates for this algorithm and establish its equivalence to Centralized (Local) SGD for (strongly) convex and non-convex problems.
1.5 Presentations and Validation of Research Results
The results of this thesis were presented at the following conferences and seminars. • Neural Information Processing Systems 34 (NeurIPS 2021), “Moshpit SGD: CommunicationEﬃcient Decentralized Training on Heterogeneous Unreliable Devices”, online, 10 December, 2021. • 38th International Conference on Machine Learning (ICML 2021), “MARINA: Faster Non-Convex Distributed Learning with Compression”, online, 21 July, 2021. • 24th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2021), “Local SGD: Uniﬁed Theory and New Eﬃcient Methods”, online, 14 April, 2021. • Federated Learning One-World Seminar, “MARINA: Faster Non-Convex Distributed Learning with Compression”, online, 10 March, 2021. • Neural Information Processing Systems 33 (NeurIPS 2020), “Linearly Converging Error Compensated SGD”, online, 9 December, 2020. • Federated Learning One-World Seminar and All-Russian Optimization Seminar, “Linearly Converging Error Compensated SGD”, online, 7 October, 2020. • 23rd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2020), “A Uniﬁed Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent”, online, 26–28 August, 2020.
1.6 Publications
Chapters 2-6 are based on the following papers, respectively: Published papers: [55] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A Uniﬁed Theory of SGD: Variance
Reduction, Sampling, Quantization and Coordinate Descent. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 680–690. PMLR, 26–28 Aug 2020. [57] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly Converging Error Compensated SGD. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
11

Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20889–20900. Curran Associates, Inc., 2020. [56] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. Local SGD: Uniﬁed Theory and New Eﬃcient Methods. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3556–3564. PMLR, 13–15 Apr 2021. [51] Eduard Gorbunov, Konstantin P. Burlachenko, Zhize Li, and Peter Richtárik. MARINA: Faster Non-Convex Distributed Learning with Compression. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 3788–3798. PMLR, 18–24 Jul 2021. In print: [185] Max Ryabinin*, Eduard Gorbunov*, Vsevolod Plokhotnyuk, and Gennady Pekhimenko (*equal contribution). Moshpit SGD: Communication-Eﬃcient Decentralized Training on Heterogeneous Unreliable Devices. Advances in Neural Information Processing Systems, volume 34, 2021. Appendix B contains extra plots, some missing proofs, and the results for the methods with delayed updates from [57] (Chapter 3). Extra experiments and missing proofs of the general results from Chapter 4 are deferred to Appendix C. Finally, missing proofs and additional technical details from Chapters 5 and 6 are given in Appendices D and E, respectively.
1.6.1 Excluded Papers
During my PhD studies, I was also fortunate to co-author two papers on stochastic optimization with heavy-tailed noise in stochastic gradients [52, 53], a paper on Byzantine-tolerant distributed optimization without parameter server [50], two review-papers on non-convex optimization [31] and decentralized distributed optimization [59], a paper on extensions of modern error feedback [41], a paper on the last-iterate convergence analysis of Extragradient method [58], and a paper on new analysis of its stochastic versions [48].
1.7 Thesis Structure
The thesis consists of an introduction, 5 main chapters, list of 247 references, and 5 chapters in the Appendix with technical details, some proofs, and auxiliary results.
12

Chapter 2
A Uniﬁed Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent

2.1 Introduction
In this chapter, we are interested in the optimization problem

min f (x) + R(x),
x∈Rd

(2.1)

where f is convex, diﬀerentiable with Lipschitz gradient, and R : Rd → R∪{+∞} is a proximable (proper closed convex) regularizer. In particular, we focus on situations when it is prohibitively expensive to compute the gradient of f , while an unbiased estimator of the gradient can be computed eﬃciently. This is typically the case for stochastic optimization problems, i.e., when

f (x) = Eξ∼D [fξ(x)] ,

(2.2)

where ξ is a random variable, and fξ : Rd → R is smooth for all ξ. Stochastic optimization problems are of key importance in statistical supervised learning theory. In this setup, x represents a machine learning model described by d parameters (e.g., logistic regression or a deep neural network), D is an unknown distribution of labelled examples, fξ(x) represents the loss of model x on datapoint ξ, and f is the generalization error. Problem (2.1) seeks to ﬁnd the model x minimizing the generalization error. In statistical learning theory one assumes that while D is not known, samples ξ ∼ D are available. In such a case, ∇f (x) is not computable, while ∇fξ(x), which is an unbiased estimator of the gradient of f at x, is easily computable.

Another prominent example, one of special interest in this chapter, are functions f which arise as averages of a very large number of smooth functions:

1n

f (x) =

fi(x).

n i=1

(2.3)

This problem often arises by approximation of the stochastic optimization loss function (2.2) via Monte Carlo integration, and is in this context known as the empirical risk minimization (ERM) problem. ERM is currently the dominant paradigm for solving supervised learning problems

13

[197]. If index i is chosen uniformly at random from [n] d=ef {1, 2, . . . , n}, ∇fi(x) is an unbiased estimator of ∇f (x). Typically, ∇f (x) is about n times more expensive to compute than ∇fi(x).

Lastly, in some applications, especially in distributed training of supervised models, one considers

problem (2.3), with n being the number of machines, and each fi also having a ﬁnite sum structure,

i.e.,

1m

fi(x) =

fij (x),

(2.4)

m j=1

where m corresponds to the number of training examples stored on machine i.

2.2 The Many Faces of Stochastic Gradient Descent

Stochastic gradient descent (SGD) [182, 149, 221] is a state-of-the-art algorithmic paradigm for solving optimization problems (2.1) in situations when f is either of structure (2.2) or (2.3). In its generic form, (proximal) SGD deﬁnes the new iterate by subtracting a multiple of a stochastic gradient from the current iterate, and subsequently applying the proximal operator of R:

xk+1 = proxγR(xk − γgk).

(2.5)

Here, gk is an unbiased estimator of the gradient (i.e., a stochastic gradient),

E gk | xk = ∇f (xk),

(2.6)

and

proxγR(x)

d=ef

argminu{γR(x)

+

1 2

u − x 2}. However, and this is the starting point of our

journey in this paper, there are inﬁnitely many ways of obtaining a random vector gk satisfying

(2.6). On the one hand, this gives algorithm designers the ﬂexibility to construct stochastic

gradients in various ways in order to target desirable properties such as convergence speed,

iteration cost, parallelizability and generalization. On the other hand, this poses considerable

challenges in terms of convergence analysis. Indeed, if one aims to, as one should, obtain the

sharpest bounds possible, dedicated analyses are needed to handle each of the particular variants

of SGD.

Vanilla1 SGD. The ﬂexibility in the design of eﬃcient strategies for constructing gk has led to a creative renaissance in the optimization and machine learning communities, yielding a large number of immensely powerful new variants of SGD, such as those employing importance sampling [243, 148], and mini-batching [99]. These eﬀorts are subsumed by the recently developed and remarkably sharp analysis of SGD under arbitrary sampling paradigm [63], ﬁrst introduced in the study of randomized coordinate descent methods by [180]. The arbitrary sampling paradigm covers virtually all stationary mini-batch and importance sampling strategies in a uniﬁed way,

1In this thesis, by vanilla SGD we refer to SGD variants with or without importance sampling and mini-batching, but excluding variance-reduced variants, such as SAGA [35] and SVRG [82].

14

thus making headway towards theoretical uniﬁcation of two separate strategies for constructing

stochastic gradients. For strongly convex f , the SGD methods analyzed in [63] converge linearly

to a neighbourhood of the solution x∗ = arg minx f (x) for a ﬁxed stepsize γk = γ. The size of the

neighbourhood is proportional to the second moment of the stochastic gradient at the optimum

(σ2 d=ef n1

n i=1

∇fi(x∗) 2), to the stepsize (γ), and inversely proportional to the modulus of

strong convexity. The eﬀect of various sampling strategies, such as importance sampling and

mini-batching, is twofold: i) improvement of the linear convergence rate by enabling larger

stepsizes, and ii) modiﬁcation of σ2. However, none of these strategies2 is able to completely

eliminate the adverse eﬀect of σ2. That is, SGD with a ﬁxed stepsize does not reach the optimum,

unless one happens to be in the overparameterized case characterized by the identity σ2 = 0.

Variance reduced SGD. While sampling strategies such as importance sampling and minibatching reduce the variance of the stochastic gradient, in the ﬁnite-sum case (2.3) a new type of variance reduction strategies has been developed over the last few years [184, 35, 82, 198, 170, 157, 103]. These variance-reduced SGD methods diﬀer from the sampling strategies discussed before in a signiﬁcant way: they can iteratively learn the stochastic gradients at the optimum, and in so doing are able to eliminate the adverse eﬀect of the gradient noise σ2 > 0 which, as mentioned above, prevents the iterates of vanilla SGD from converging to the optimum. As a result, for strongly convex f , these new variance-reduced SGD methods converge linearly to x∗, with a ﬁxed stepsize. At the moment, these variance-reduced variants require a markedly diﬀerent convergence theory from the vanilla variants of SGD. An exception to this is the situation when σ2 = 0 as then variance reduction is not needed; indeed, vanilla SGD already converges to the optimum, and with a ﬁxed stepsize. We end the discussion here by remarking that this hints at a possible existence of a more uniﬁed theory, one that would include both vanilla and variance-reduced SGD.

Distributed SGD, quantization and variance reduction. When SGD is implemented in a distributed fashion, the problem is often expressed in the form (2.3), where n is the number of workers/nodes, and fi corresponds to the loss based on data stored on node i. Depending on the number of data points stored on each node, it may or may not be eﬃcient to compute the gradient of fi in each iteration. In general, SGD is implemented in this way: each node i ﬁrst computes a stochastic gradient gik of fi at the current point xk (maintained individually by each node). These gradients are then aggregated by a master node [199, 101], in-network by a switch [189], or a diﬀerent technique best suited to the architecture used. To alleviate the communication bottleneck, various lossy update compression strategies such as quantization [196, 65, 240], sparsiﬁcation [101, 5, 226] and dithering [4] were proposed. The basic idea is for each worker to apply a randomized transformation Q : Rd → Rd to gik, resulting in a vector which is still an unbiased estimator of the gradient, but one that can be communicated with fewer bits. Mathematically, this amounts to injecting additional noise into the already noisy stochastic gradient gik. The ﬁeld of quantized SGD is still young, and even some basic questions remained

2Except for the full batch strategy, which is prohibitively expensive.

15

open until recently. For instance, there was no distributed quantized SGD capable of provably solving (2.1) until the DIANA algorithm [139] was introduced. DIANA applies quantization to gradient diﬀerences, and in so doing is able to learn the gradients at the optimum, which makes it able to work for any regularizer R. DIANA has some structural similarities with SEGA [69]—the ﬁrst coordinate descent type method which works for non-separable regularizers—but a more precise relationship remains elusive. When the functions of fi are of a ﬁnite-sum structure as in (4.3), one can apply variance reduction to reduce the variance of the stochastic gradients gik together with quantization, resulting in the VR-DIANA method [79]. This is the ﬁrst distributed quantized SGD method which provably converges to the solution of (2.1)+(4.3) with a ﬁxed stepsize.
Randomized coordinate descent (RCD). Lastly, in a distinctly separate strain, there are SGD methods for the coordinate/subspace descent variety [151]. While it is possible to see some RCD methods as special cases of (2.5)+(2.6), most of them do not follow this algorithmic template. First, standard RCD methods use diﬀerent stepsizes for updating diﬀerent coordinates [169], and this seems to be crucial to their success. Second, until the recent discovery of the SEGA method, RCD methods were not able to converge with non-separable regularizers. Third, RCD methods are naturally variance-reduced in the R = 0 case as partial derivatives at the optimum are all zero. As a consequence, attempts at creating variance-reduced RCD methods seem to be futile. Lastly, RCD methods are typically analyzed using diﬀerent techniques. While there are deep links between standard SGD and RCD methods, these are often indirect and rely on duality [198, 29, 61].
2.3 Contributions
As outlined in the previous section, the world of SGD is vast and beautiful. It is formed by many largely disconnected islands populated by elegant and eﬃcient methods, with their own applications, intuitions, and convergence analysis techniques. While some links already exist (e.g., the uniﬁcation of importance sampling and mini-batching variants under the arbitrary sampling umbrella), there is no comprehensive general theory. It is becoming increasingly diﬃcult for the community to understand the relationships between these variants, both in theory and practice. New variants are yet to be discovered, but it is not clear what tangible principles one should adopt beyond intuition to aid the discovery. This situation is exacerbated by the fact that a number of diﬀerent assumptions on the stochastic gradient, of various levels of strength, is being used in the literature.
The main contributions of this work include:
• Uniﬁed analysis. In this work we propose a unifying theoretical framework which covers all of the variants of SGD outlined in Section 2.2. As a by-product, we obtain the ﬁrst uniﬁed analysis of vanilla and variance-reduced SGD methods. For instance, our analysis covers as special cases vanilla SGD methods from [156] and [63], variance-reduced SGD methods such as SAGA [35], L-SVRG [77, 103] and JacSketch [62]. Another by-product is the uniﬁed analysis of SGD methods
16

which include RCD. For instance, our theory covers the subspace descent method SEGA [69] as a special case. Lastly, our framework is general enough to capture the phenomenon of quantization. For instance, we obtain the DIANA and VR-DIANA methods in special cases. • Generalization of existing methods. An important yet relatively minor contribution of our work is that it enables generalization of knowns methods. For instance, some particular methods we consider, such as L-SVRG (Alg 10) [103], were not analyzed in the proximal (R = 0) case before. To illustrate how this can be done within our framework, we do it here for L-SVRG. Further, most3 of the methods we analyze can be extended to the arbitrary sampling paradigm. • Sharp rates. In all known special cases, the rates obtained from our general theorem (Theorem 2.4.4) are the best known rates for these methods. • New methods. Our general analysis provides estimates for a possibly inﬁnite array of new and yet-to-be-developed variants of SGD. One only needs to verify that Assumption 2.4.1 holds, and a complexity estimate is readily furnished by Theorem 2.4.4. Selected existing and new methods that ﬁt our framework are summarized in Table 2.1. This list is for illustration only, we believe that future work by us and others will lead to its rapid expansion. • Experiments. We show through extensive experimentation that some of the new and generalized methods proposed here and analyzed via our framework have some intriguing practical properties when compared against appropriately selected existing methods.
2.4 Main Result
We ﬁrst introduce the key assumption on the stochastic gradients gk enabling our general analysis (Assumption 2.4.1), then state our assumptions on f (Assumption 2.4.2), and ﬁnally state and comment on our uniﬁed convergence result (Theorem 2.4.4).
2.4.1 Key Assumption
Our ﬁrst assumption is of key importance. It is mainly an assumption on the sequence of stochastic gradients {gk} generated by an arbitrary randomized algorithm. Besides unbiasedness (see (2.7)), we require two recursions to hold for the iterates xk and the stochastic gradients gk of a randomized method. We allow for ﬂexibility by casting these inequalities in a parametric manner.
Assumption 2.4.1. Let {xk} be the random iterates produced by proximal SGD (Algorithm
3Our analysis allows for arbitrary sampling of all methods except of those using partial derivatives such as SEGA or N-SEGA. We shall note that arbitrary sampling for SEGA was developed concurrently in [71]. Note that [71] proposes many novel variance reduced algorithms, for some of which we can obtain best rates. A detailed discussion and comparison to [71] is provided in Remark 2.6.38 in the Appendix
17

in Eq (2.5)). We ﬁrst assume that the stochastic gradients gk are unbiased

E gk | xk = ∇f (xk),

(2.7)

for all k ≥ 0. Further, we assume that there exist non-negative constants A, B, C, D1, D2, ρ and a (possibly) random sequence {σk2}k≥0 such that the following two relations holda

E gk − ∇f (x∗) 2 | xk ≤ 2ADf (xk, x∗) + Bσk2 + D1,

(2.8)

E σk2+1 | σk2 ≤ (1 − ρ)σk2 + 2CDf (xk, x∗) + D2, The expectation above is with respect to the randomness of the algorithm.

(2.9)

aFor convex and L-smooth f , one can show that ∇f (x) − ∇f (y) 2 ≤ 2LDf (x, y). Hence, Df can be used as a measure of proximity for the gradients.

The unbiasedness assumption (2.7) is standard. The key innovation we bring is inequality (2.8) coupled with (2.9). We argue, and justify this statement by furnishing many examples in Section 2.5, that these inequalities capture the essence of a wide array of existing and some new SGD methods, including vanilla, variance reduced, arbitrary sampling, quantized and coordinate descent variants. Note that in the case when ∇f (x∗) = 0 (e.g., when R = 0), the inequalities in Assumption 2.4.1 reduce to

E gk 2 | xk ≤ 2A(f (xk) − f (x∗)) + Bσk2 + D1,

(2.10)

E σk2+1 | σk2 ≤ (1 − ρ)σk2 + 2C(f (xk) − f (x∗)) + D2.

(2.11)

Similar inequalities can be found in the analysis of stochastic ﬁrst-order methods. However, this is the ﬁrst time that such inequalities are generalized, equipped with parameters, and elevated to the status of an assumption that can be used on its own, independently from any other details deﬁning the underlying method that generated them.

To give a further intuition about inequalities (2.8) and (2.9), we shall note that sequence σk usually represents the portion of noise that can gradually decrease over the course of optimization while constants D1, D2 represent a static noise. On the other hand, constants A, C are usually related to some measure of smoothness of the objective. For instance, the parameters for (deterministic) gradient descent can be chosen as A = L, B = C = D1 = D2 = σk2 = ρ = 0. For an overview of parameter choices for speciﬁc instances of (2.5), see Table C.4. Note also that the choice of parameters of (2.8) and (2.9) is not unique, however this has no impact on convergence rates we provide.

18

2.4.2 Main Theorem
For simplicity, we shall assume throughout that f is (µ, x∗)-strongly quasi-convex, which is a generalization of µ-strong convexity.

Assumption 2.4.2 ((µ, x∗)-strong quasi-convexity). There exists µ > 0 such that f : Rd → R satisﬁes the following inequality for all x ∈ Rd:

f (x∗) ≥ f (x) + ∇f (x), x∗ − x + µ x∗ − x 2 . 2

(2.12)

We are now ready to present the key lemma of this paper which states per iteration recurrence to analyze (2.5).

Lemma 2.4.3. Let Assumptions 2.4.1 and 2.4.2 be satisﬁed. Then the following inequality holds for all k ≥ 0:

E xk+1 − x∗ 2 + M γ2E σk2+1

≤ (1 − γµ)E xk − x∗ 2 + 1 − ρ + MB M γ2E σk2 −2γ (1 − γ(A + CM )) E Df (xk, x∗) +(D1 + M D2)γ2.

Proof. We start with estimating the ﬁrst term of the Lyapunov function. Let rk = xk − x∗. Then

rk+1 2 = ≤ =

proxγR(xk − γgk) − proxγR(x∗ − γ∇f (x∗)) 2

xk − x∗ − γ(gk − ∇f (x∗)) 2

rk

2 − 2γ rk, gk − ∇f (x∗)

+ γ2

gk − ∇f (x∗)

2
.

Taking expectation conditioned on xk we get

E rk+1 2 | xk

=

rk

2
− 2γ

rk, ∇f (xk) − ∇f (x∗)

+ γ2E

gk − ∇f (x∗) 2 | xk

(2.12)
≤

(1 − γµ) rk 2 − 2γDf (xk, x∗) + γ2E gk − ∇f (x∗) 2 | xk

(2.7)≤+(2.8) (1 − γµ) rk 2 + 2γ (Aγ − 1) Df (xk, x∗) + Bγ2σk2 + γ2D1.

19

Using this we estimate the full expectation of V k+1 in the following way:
E xk+1 − x∗ 2 + M γ2Eσk2+1 (2≤.9) (1 − γµ)E xk − x∗ 2 + 2γ (Aγ − 1) E Df (xk, x∗) +(1 − ρ)M γ2Eσk2 + 2CM γ2E Df (xk, x∗) +Bγ2Eσk2 + (D1 + M D2)γ2
= (1 − γµ)E xk − x∗ 2 + 1 + MB − ρ M γ2Eσk2 +2γ (γ(A + CM ) − 1) E Df (xk, x∗) +(D1 + M D2)γ2.

It remains to rearrange the terms.

Using recursively Lemma 2.4.3, we obtain the convergence rate of proximal SGD, which we state as Theorem 2.4.4.
Theorem 2.4.4. Let Assumptions 2.4.1 and 2.4.2 be satisﬁed. Choose constant M such that M > Bρ . Choose a stepsize satisfying

0 < γ ≤ min 1 , 1 . µ A + CM

(2.13)

Then the iterates {xk}k≥0 of proximal SGD (Algorithm (2.5)) satisfy

E V k ≤ max

(1 − γµ)k,

B

k

1+ M −ρ

V 0 + (D1 + M D2)γ2 , min γµ, ρ − MB

(2.14)

where the Lyapunov function V k is deﬁned by V k d=ef xk − x∗ 2 + M γ2σk2.

Proof. Note ﬁrst that due to (2.13) we have 2γ (1 − γ(A + CM )) EDf (xk, x∗) > 0, thus we can omit the term.

20

Unrolling the recurrence from Lemma 2.4.3 and using the Lyapunov function notation gives us

EV k

≤

max

(1 − γµ)k,

B

k

1+ −ρ

V0

M

k−1
+(D1 + M D2)γ2 max
l=0

(1 − γµ)l,

B

l

1+ M −ρ

≤

max

(1 − γµ)k,

B

k

1+ −ρ

V0

M

∞
+(D1 + M D2)γ2 max
l=0

(1 − γµ)l,

B

l

1+ M −ρ

≤

max

(1 − γµ)k,

B

k

1+ −ρ

V 0 + (D1 + M D2)γ2 .

M

min γµ, ρ − MB

This theorem establishes a linear rate for a wide range of proximal SGD methods up to a certain oscillation radius, controlled by the additive term in (2.14), and namely, by parameters D1 and D2. As we shall see in Section 4.5 (refer to Table C.4), the main diﬀerence between the vanilla and variance-reduced SGD methods is that while the former satisfy inequality (2.9) with D1 > 0 or D2 > 0, which in view of (2.14) prevents them from reaching the optimum x∗ (using a ﬁxed stepsize), the latter methods satisfy inequality (2.9) with D1 = D2 = 0, which in view of (2.14) enables them to reach the optimum.
2.5 The Classic, The Recent and The Brand New
In this section we deliver on the promise from the introduction and show how many existing and some new variants of SGD ﬁt our general framework (see Table 2.1).
An overview. As claimed, our framework is powerful enough to include vanilla methods ( in the “VR” column) as well as variance-reduced methods ( in the “VR” column), methods which generalize to arbitrary sampling ( in the “AS” column), methods supporting gradient quantization ( in the “Quant” column) and ﬁnally, also RCD type methods ( in the “RCD” column).

21

Table 2.1: List of speciﬁc existing (in some cases generalized) and new methods which ﬁt our
general analysis framework. VR = variance reduced method, AS = arbitrary sampling, Quant = supports gradient quantization, RCD = randomized coordinate descent type method. a Special case of SVRG with 1 outer loop only; b Special case of DIANA with 1 node and quantization of
exact gradient.

Problem

Method Alg # Citation VR? AS? Quant? RCD? Section Result

(2.1)+(2.2)

SGD

Alg 1

[156]









2.6.1 Cor 2.6.2

(2.1)+(2.3)

SGD-SR Alg 2

[63]









2.6.2 Cor 2.6.5

(2.1)+(2.3)

SGD-MB Alg 3 NEW 







2.6.3 Cor 2.6.9

(2.1)+(2.3)

SGD-star Alg 4 NEW  





2.6.4 Cor 2.6.12

(2.1)+(2.3)

SAGA

Alg 5

[35]









2.6.5 Cor 2.6.15

(2.1)+(2.3)

N-SAGA Alg 6 NEW 







2.6.6 Cor 2.6.17

(2.1)

SEGA

Alg 7

[69]









2.6.7 Cor 2.6.19

(2.1) (2.1)+(2.3)

N-SEGA Alg 8 NEW 





SVRGa

Alg 9

[82]









2.6.8 Cor 2.6.21



2.6.9 Cor 2.6.23

(2.1)+(2.3)

L-SVRG Alg 10

[77]









2.6.10 Cor 2.6.25

(2.1)+(2.3) (2.1)+(2.3)

DIANA

Alg 11

[139]







DIANAb

Alg 12

[139]









2.6.11 Cor 2.6.28



2.6.11 Cor 2.6.29

(2.1)+(2.3)

Q-SGD-SR Alg 13 NEW









2.6.12 Cor 2.6.31

(2.1)+(2.3)+(4.3) VR-DIANA Alg 14

[79]









2.6.13 Cor 2.6.34

(2.1)+(2.3)

JacSketch Alg 15

[62]

 





2.6.14 Cor 2.6.37

For existing methods we provide a citation; new methods developed in this paper are marked accordingly. We provide a link to the appropriate section for easy navigation. While these details are important, the main message of this chapter, i.e., the generality of our approach, is captured by Table 2.1. The “Result” column of Table 2.1 points to a corollary of Theorem 2.4.4; these corollaries state in detail the convergence statements for the various methods. In all cases where known methods are recovered, these corollaries of Theorem 2.4.4 recover the best known rates.
Parameters. From the point of view of Assumption 2.4.1, the methods listed in Table 2.1 exhibit certain patterns. To shed some light on this, in Table C.4 we summarize the values of these parameters.

22

Table 2.2: The parameters for which the methods from Table 2.1 (special cases of (2.5)) satisfy Assumption 2.4.1. The meaning of the expressions appearing in the table, as well as their justiﬁcation is deﬁned in detail in Section 4.5.

Method SGD
SGD-SR SGD-MB SGD-star
SAGA N-SAGA
SEGA N-SEGA SVRGa L-SVRG DIANA DIANAb Q-SGD-SR VR-DIANA JacSketch

A

2L

2L
A +L(τ −1) τ
2L

2L

2L

2dL

2dL

2L

2L

1

+

2ω n

L

(1 + 2ω) L

2(1 + ω)L

1

+

4ω+2 n

L

2L1

B
0
0
0
0
2
2
2d
2d
2
2
2ω n
2ω
0
2(ω+1) n
2λmax n

ρ 1 1 1 1 1/n 1/n 1/d 1/d 0 p α α 1 α λmin

C

D1

D2

0

2σ2

0

0

2σ2

0

0

D τ

0

0

0

0

L/n

0

0

L/n

2σ2 σn2

L/d

0

0

L/d 2dσ2 σd2

0

0

0

Lp

0

0

Lα (1+nω)σ2 ασ2

Lα

0

0

0

2(1 + ω)σ2 0

1 m

+

4α

L

0

0

L2 n

0

0

Note, for example, that for all methods the parameter A is non-zero. Typically, this a multiple of an appropriately deﬁned smoothness parameter (e.g., L is the Lipschitz constant of the gradient of f , L and L1 in SGD-SR4, SGD-star and JacSketch are expected smoothness parameters). In the three variants of the DIANA method, ω captures the variance of the quantization operator Q. That is, one assumes that EQ(x) = x and E Q(x) − x 2 ≤ ω x 2 for all x ∈ Rd. In view of (2.13), large A means a smaller stepsize, which slows down the rate. Likewise, the variance ω also aﬀects the parameter B, which in view of (2.14) also has an adverse eﬀect on the rate. Further, as predicted by Theorem 2.4.4, whenever either D1 > 0 or D2 > 0, the corresponding method converges to an oscillation region only. These methods are not variance-reduced. All symbols used in Table C.4 are deﬁned in the appendix, in the same place where the methods are described and analyzed.
Five new methods. To illustrate the usefulness of our general framework, we develop 5 new variants of SGD never explicitly considered in the literature before (see Table 2.1). Here we brieﬂy
4SGD-SR is ﬁrst SGD method analyzed in the arbitrary sampling paradigm. It was developed using the stochastic reformulation approach (whence the “SR”) pioneered in [181] in a numerical linear algebra setting, and later extended to develop the JacSketch variance-reduction technique for ﬁnite-sum optimization [62].
23

motivate them; details can be found in the corresponding sections.

• SGD-MB (Algorithm 3). This method is speciﬁcally designed for functions of the ﬁnite-sum structure (4.3). As we show through experiments, this is a powerful mini-batch SGD method, with mini-batches formed with replacement as follows: in each iteration, we repeatedly (τ times) and independently pick i ∈ [n] with probability pi > 0. Stochastic gradient gk is then formed by averaging the stochastic gradients ∇fi(xk) for all selected indices i (including each i as many times as this index was selected). This allows for a more practical importance mini-batch sampling implementation than what was until now possible (see Remark 2.6.10 for more details and experiment in Figure 2.1).

• SGD-star (Algorithm 4). This new method forms a bridge between vanilla and variance-

reduced SGD methods. While not practical, it sheds light on the role of variance reduction.

Again, we consider functions of the ﬁnite-sum form (4.3). This methods answers the following

question: assuming that the gradients ∇fi(x∗), i ∈ [n] are known, can they be used to design a

more powerful SGD variant? The answer is yes, and SGD-star is the method. In its most basic

form, SGD-star constructs the stochastic gradient via gk = ∇fi(xk) − ∇fi(x∗) + ∇f (x∗), where

i ∈ [n] is chosen uniformly at random. Inferring from Table C.4, where D1 = D2 = 0, this

method converges to x∗, and not merely to some oscillation region. Variance-reduced methods

essentially work by iteratively constructing increasingly more accurate estimates of ∇fi(x∗).

Typically, the term σk2 in the Lyapunov function of variance reduced methods will contain a

term of the form

i

hki − ∇fi(x∗)

2
,

with

hki

being

the

estimators

maintained

by

the

method.

Remarkably, SGD-star was never explicitly considered in the literature before.

• N-SAGA (Algorithm 6). This is a novel variant of SAGA [35], one in which one does not have access to the gradients of fi, but instead only has access to noisy stochastic estimators thereof (with noise σ2). Like SAGA, N-SAGA is able to reduce the variance inherent in the ﬁnite sum structure (4.3) of the problem. However, it necessarily pays the price of noisy estimates of ∇fi, and hence, just like vanilla SGD methods, is ultimately unable to converge to x∗. The oscillation region is governed by the noise level σ2 (refer to D1 and D2 in Table C.4). This method will be of practical importance for problems where each fi is of the form (2.2), i.e., for problems of the “average of expectations” structure. Batch versions of N-SAGA would be well suited for distributed optimization, where each fi is owned by a diﬀerent worker, as in such a case one wants the workers to work in parallel.

• N-SEGA (Algorithm 8). This is a noisy extension of the RCD-type method SEGA, in complete analogy with the relationship between SAGA and N-SAGA. Here we assume that we only have noisy estimates of partial derivatives (with noise σ2). This situation is common in derivative-free optimization, where such a noisy estimate can be obtained by taking (a random) ﬁnite diﬀerence approximation [152]. Unlike SEGA, N-SEGA only converges to an oscillation region the size of which is governed by σ2.

• Q-SGD-SR (Algorithm 13). This is a quantized version of SGD-SR, which is the ﬁrst SGD method

24

analyzed in the arbitrary sampling paradigm. As such, Q-SGD-SR is a vast generalization of the celebrated QSGD method [4].
2.6 Special Cases
2.6.1 Proximal SGD for Stochastic Optimization
Algorithm 1 SGD Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ
for k = 0, 1, 2, . . . do Sample ξ ∼ D gk = ∇fξ(xk) xk+1 = proxγR(xk − γgk)
end for

We start with stating the problem, the assumptions on the objective and on the stochastic gradients for SGD [156]. Consider the expectation minimization problem

min f (x) + R(x), f (x) d=ef ED [fξ(x)]
x∈Rd

(2.15)

where ξ ∼ D, fξ(x) is diﬀerentiable and L-smooth almost surely in ξ.
Lemma 2.6.1 shows that the stochastic gradient gk = ∇fξ(xk) satisﬁes Assumption 2.4.1. The corresponding choice of parameters can be found in Table C.4.

Lemma 2.6.1 (Generalization of Lemmas 1,2 from [156]). Assume that fξ(x) is convex in x for every ξ. Then for every x ∈ Rd

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4L(Df (x, x∗)) + 2σ2,

(2.16)

where σ2 d=ef Eξ ∇fξ(x∗) 2 . If further f (x) is µ-strongly convex with possibly non-convex fξ, then for every x ∈ Rd

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4Lκ(Df (x, x∗)) + 2σ2,

(2.17)

where κ = Lµ .

Corollary 2.6.2. Assume that fξ(x) is convex in x for every ξ and f is µ-strongly quasi-convex.

Then

SGD

with

γ

≤

1 2L

satisﬁes

E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γσ2 . µ

(2.18)

25

If we further assume that f (x) is µ-strongly convex with possibly non-convex fξ(x), SGD with

γ

≤

1 2Lκ

satisﬁes

(2.18)

as

well.

Proof. It suﬃces to plug parameters from Table C.4 into Theorem 2.4.4.

Proof of Lemma 2.6.1

The proof is a direct generalization to the one from [156]. Note that

1 2 ED

∇fξ(x) − ∇f (x∗) 2 − ED ∇fξ(x∗) − ∇f (x∗) 2

1 = 2 ED
(A.9)
≤ ED

∇fξ(x) − ∇f (x∗) 2 − ∇fξ(x) − ∇fξ(x∗) 2

∇fξ(x∗) − ∇f (x∗) 2

≤ 2LDf (x, x∗).

It remains to rearrange the above to get (2.16). To obtain (2.17), we shall proceed similarly:

1 2 ED

∇fξ(x) − ∇f (x∗) 2 − ED ∇fξ(x∗) − ∇f (x∗) 2

1 = 2 ED
(A.9)
≤ ED

∇fξ(x) − ∇f (x∗) 2 − ∇fξ(x) − ∇fξ(x∗) 2

∇fξ(x∗) − ∇f (x∗) 2

≤ L2 x − x∗ 2 ≤ 2 Lµ2 Df (x, x∗).

Again, it remains to rearrange the terms.

2.6.2 SGD-SR

In this section, we recover convergence result of SGD under expected smoothness property from [63]. This setup allows obtaining tight convergence rates of SGD under arbitrary stochastic reformulation of ﬁnite sum minimization5.

The stochastic reformulation is a special instance of (2.15):

min f (x) + R(x),
x∈Rd

f (x) = ED [fξ(x)] ,

def 1 n

fξ(x) =

ξifi(x)

n i=1

(2.19)

where ξ is a random vector from distribution D such that for all i: ED [ξi] = 1 and fi (for all i) is smooth, possibly non-convex function. We next state the expextes smoothness assumption. A speciﬁc instances of this assumption allows to get tight convergence rates of SGD, which we recover in this section.

5For technical details on how to exploit expected smoothness for speciﬁc reformulations, see [63]

26

Algorithm 2 SGD-SR
Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ] is vector of ones for k = 0, 1, 2, . . . do Sample ξ ∼ D gk = ∇fξ(xk) xk+1 = proxγR(xk − γgk) end for

Assumption 2.6.3 (Expected smoothness). We say that f is L-smooth in expectation with respect to distribution D if there exists L = L(f, D) > 0 such that

ED ∇fξ(x) − ∇fξ(x∗) 2 ≤ 2LDf (x, x∗),

(2.20)

for all x ∈ Rd. For simplicity, we will write (f, D) ∼ ES(L) to say that (2.20) holds.
Next, we present Lemma 2.6.4 which shows that choice of constants for Assumption 2.4.1 from Table C.4 is valid. Lemma 2.6.4 (Generalization of Lemma 2.4, [63]). If (f, D) ∼ ES(L), then

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4LDf (x, x∗) + 2σ2.

(2.21)

where σ2 d=ef ED ∇fξ(x∗) − ∇f (x∗) 2 .

A direct consequence of Theorem 2.4.4 in this setup is Corollary 2.6.5.

Corollary 2.6.5. Assume that f (x) is µ-strongly quasi-convex and (f, D) ∼ ES(L). Then

SGD-SR

with

γk

≡

γ

≤

1 2L

satisﬁes

E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γσ2 . µ

(2.22)

Proof of Lemma 2.6.4
Here we present the generalization of the proof of Lemma 2.4 from [63] for the case when ∇f (x∗) = 0. In this proof all expectations are conditioned on xk.

E ∇fξ(x) − ∇f (x∗) 2

=
(A.11)
≤
(2.20)
≤

E ∇fξ(x) − ∇fξ(x∗) + ∇fξ(x∗) − ∇f (x∗) 2 2E ∇fξ(x) − ∇fξ(x∗) 2 + 2E ∇fξ(x∗) − ∇f (x∗) 2 4LDf (x, x∗) + 2σ2.

27

2.6.3 SGD-MB

In this section, we present a speciﬁc practical formulation of (2.19) which was not considered in [63]. The resulting algorithm (Algorithm 3) is novel; it was not considered in [63] as a speciﬁc instance of SGD-SR. The key idea behind SGD-MB is constructing unbiased gradient estimate via with-replacement sampling.

Consider random variable ν ∼ D such that P(ν = i) = pi;

n
pi = 1.
i=1

(2.23)

Notice that if we deﬁne

def 1 ψi(x) = fi(x),
npi

i = 1, 2, . . . , n,

(2.24)

then

1n

n (2.24)

(2.23)

f (x) =

fi(x) =

piψi(x) = ED [ψν(x)] .

n i=1 i=1

(2.25)

So, we have rewritten the ﬁnite sum problem (2.3) into the equivalent stochastic optimization

problem

min ED [ψν(x)] .
x∈Rd

(2.26)

We are now ready to describe our method.

At

each

iteration

k

we

sample

νik

,

.

.

.

,

ν

k τ

∼

D

independently (1 ≤ τ ≤ n), and deﬁne gk d=ef τ1

τ i=1

∇ψν

k

(xk

).

Further, we use gk as a

i

stochastic gradient, resulting in Algorithm 3.

Algorithm 3 SGD-MB

Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ν such that (2.23) holds.

for k = 0, 1, 2, . . . do

Sample

νik

,

.

.

.

,

ν

k τ

∼

D

independently

gk

=

1 τ

τ i=1

∇ψν

k

(xk

)

i

xk+1 = xk − γgk

end for

To remain in full generality, consider the following Assumption.

Assumption 2.6.6. There exists constants A > 0 and D ≥ 0 such that

ED ∇ψν(x) 2 ≤ 2A (f (x) − f (x∗)) + D

(2.27)

for all x ∈ Rd.
Note that it is suﬃcient to have convex and smooth fi in order to satisfy Assumption 2.6.6, as Lemma 2.6.7 states.

28

Lemma 2.6.7. Let σ2 d=ef ED ∇ψν(x∗) 2 . If fi are convex and Li-smooth, then Assumption 2.6.6 holds for A = 2L and D = 2σ2, where

L ≤ max Li . i npi

(2.28)

If moreover ∇fi(x∗) = 0 for all i, then Assumption 2.6.6 holds for A = L and D = 0.

Next, Lemma 2.6.8 states that Algorithm 3 indeed satisﬁes Assumption 2.4.1.

Lemma 2.6.8. Suppose that Assumption 2.6.6 holds. Then gk is unbiased; i.e. ED gk = ∇f (xk). Further,

ED gk 2 ≤ 2A + 2L(τ − 1) (f (xk) − f (x∗)) + D .

τ

τ

Thus, parameters from Table C.4 are validated. As a direct consequence of Theorem 2.4.4 we get Corollary 2.6.9.

Corollary

2.6.9.

As

long

as

0<γ

≤

A

τ +L(τ −1)

,

we

have

E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + γD . µτ

(2.29)

Remark 2.6.10. For τ = 1, SGD-MB is a special of the method from [63], Section 3.2. However, for τ > 1, this is a diﬀerent method; the diﬀerence lies in the with-replacement sampling. Note that with-replacement trick allows for eﬃcient and implementation of independent importance sampling a with complexity O(τ log(n)). In contrast, implementation of without-replacement importance sampling has complexity O(n), which can be signiﬁcantly more expensive to the cost of evaluating i∈S ∇fi(x).
aDistribution of random sets S for which random variables i ∈ S and j ∈ S are independent for j = i.

Proof of Lemma 2.6.8 Notice ﬁrst that
ED gk

(2=.24)
= (2=.23)
=

1τ τ ED
i=1

1 ∇f k (xk) npνk νi
i

ED 1 ∇fν (xk) npν

n
p

1 ∇f (xk)

i npi i

i=1

∇f (xk).

29

So, gk is an unbiased estimator of the gradient ∇f (xk). Next,

ED gk 2

=
=
=
=
(2.27)
≤

 1τ

2
k

ED  τ

∇ψνk (x )  i

i=1



1

τ

τ 2 ED 

i=1

∇ψνik (xk) 2 + 2
i<j



∇ψνk (xk), ∇ψνk (xk) 

i

j

1 τ ED

∇ψν (xk) 2 + 2 τ2

ED ∇ψνk (xk) , ED ∇ψνk (xk)

i

j

i<j

1 τ ED

∇ψν (xk) 2 + τ − 1 ∇f (xk) 2 τ

2A (f (xk) − f (x∗)) + D + 2L(τ − 1)(f (xk) − f (x∗)) .
τ

Proof of Lemma 2.6.7 Let L = L(f, D) > 0 be any constant for which

Eξ∼D ∇φξ(x) − ∇φξ(x∗) 2 ≤ 2L(f (x) − f (x∗))

(2.30)

holds for all x ∈ Rd. This is the expected smoothness property (for a single item sampling) from [63]. It was shown in [63, Proposition 3.7] that (2.30) holds, and that L satisﬁes (2.28). The claim now follows by applying [63, Lemma 2.4].

2.6.4 SGD-star
Consider problem (2.19). Suppose that ∇fi(x∗) is known for all i. In this section we present a novel algorithm — SGD-star — which is SGD-SR shifted by the stochastic gradient in the optimum. The method is presented under Expected Smoothness Assumption (2.20), obtaining general rates under arbitrary sampling. The algorithm is presented as Algorithm 4.
Algorithm 4 SGD-star Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ]
is vector of ones for k = 0, 1, 2, . . . do
Sample ξ ∼ D gk = ∇fξ(xk) − ∇fξ(x∗) + ∇f (x∗) xk+1 = proxγR(xk − γgk) end for

Suppose that (f, D) ∼ ES(L). Note next that SGD-star is just SGD-SR applied on objec-
tive Df (x, x∗) instead of f (x) when ∇f (x∗) = 0. This careful design of the objective yields (Df (·, x∗), D) ∼ ES(L) and ED ∇xDf (x, x∗) 2 | x = x∗ = 0, and thus Lemma (2.6.4) be-
ξ
comes

30

Lemma 2.6.11 (Lemma 2.4, [63]). If (f, D) ∼ ES(L), then ED gk − ∇f (x∗) 2 ≤ 4LDf (xk, x∗).

(2.31)

A direct consequence of Corollary (thus also a direct consequence of Theorem 2.4.4) in this setup is Corollary 2.6.12.

Corollary

2.6.12.

Suppose

that

(f, D) ∼ ES(L).

Then

SGD-star

with

γ

=

1 2L

satisﬁes

E xk − x∗ 2 ≤ 1 − µ k x0 − x∗ 2 . 2L

(2.32)

Remark 2.6.13. Note that results from this section are obtained by applying results from 2.6.2. Since Section 2.6.3 presets a speciﬁc sampling algorithm for SGD-SR, the results can be thus extended to SGD-star as well.

Proof of Lemma 2.6.11 In this proof all expectations are conditioned on xk.

ED gk − ∇f (x∗) 2

= ED ∇fξ(xk) − ∇fξ(x∗) 2
(2.20)
≤ 4LDf (xk, x∗).

2.6.5 SAGA

In this section we show that our approach is suitable for SAGA [35] (see Algorithm 5). Consider the ﬁnite-sum minimization problem

1n

f (x) =

fi(x) + R(x),

n i=1

(2.33)

where fi is convex, L-smooth for each i and f is µ-strongly convex.

Algorithm 5 SAGA [35]

Input: learning rate γ > 0, starting point x0 ∈ Rd

Set φ0j = x0 for each j ∈ [n] for k = 0, 1, 2, . . . do

Sample j ∈ [n] uniformly at random

Set φkj +1 = xk and φki +1 = φki for i = j

n

gk

=

∇fj (φkj +1)

−

∇fj(φkj )

+

1 n

∇fi(φki )

i=1

xk+1 = proxγR xk − γgk

end for

31

Lemma 2.6.14. We have E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2

(2.34)

and E σk2+1 | xk ≤

n
where σ2 = 1

∇fi(φk) − ∇fi(x∗)

2
.

kn

i

i=1

1− 1 n

σk2 + 2nL Df (xk, x∗),

(2.35)

Clearly, Lemma 2.6.14 shows that Algorithm 5 satisﬁes Assumption 2.4.1; the corresponding parameter choice can be found in Table C.4. Thus, as a direct consequence of Theorem 2.4.4 with M = 4n we obtain the next corollary.

Corollary

2.6.15.

SAGA

with

γ

=

1 6L

satisﬁes

EV k ≤ 1 − min µ , 1 6L 2n

k
V 0.

(2.36)

Proof of Lemma 2.6.14

Note that Lemma 2.6.14 is a special case of Lemmas 3,4 from [140] without prox term. We reprove it with prox for completeness.

Let all expectations be conditioned on xk in this proof. Note that L-smoothness and convexity of fi implies

1 ∇fi(x) − ∇fi(y) 2 ≤ fi(x) − fi(y) − ∇fi(y), x − y , ∀x, y ∈ Rd, i ∈ [n]. 2L

(2.37)

By deﬁnition of gk we have

E gk − ∇f (x∗) 2

=
(A.11)
≤


k+1

k 1n

k

2
∗

E  ∇fj(φj ) − ∇fj(φj ) + n ∇fi(φi ) − ∇f (x ) 

i=1

2E ∇fj(xk) − ∇fj(x∗) 2 | xk

+2E ∇fj(x∗) − ∇fj(φkj ) − E ∇fj(x∗) − ∇fj(φkj ) 2

(A.14)+(2.37)
≤
=

4nL n Dfi (xk, x∗) + 2E
i=1

∇fj(x∗) − ∇fj(φkj ) 2 | xk

4LDf (xk, x∗) + 2 n1 n

∇fi(φki ) − ∇fi(x∗)

2
.

i=1

σk2

32

To proceed with (2.35), we have

E σk2+1

= =
(2.37)
≤
=

1n nE
i=1

∇fi(φki +1) − ∇fi(x∗) 2

1n n i=1

n −n 1 ∇fi(φki ) − ∇fi(x∗) 2 + n1

1− 1 n

n1 n ∇fi(φki ) − ∇fi(x∗) 2
i=1

+ 2nL2 n Dfi (xk, x∗)
i=1

1 − n1 σk2 + 2nL Df (xk, x∗).

∇fi(xk) − ∇fi(x∗) 2

2.6.6 N-SAGA

Algorithm 6 Noisy SAGA (N-SAGA)

Input: learning rate γ > 0, starting point x0 ∈ Rd

Set ψj0 = x0 for each j ∈ [0]

for k = 0, 1, 2, . . . do

Sample j ∈ [n] uniformly at random and ζ

Set gjk+1 = gj(xk, ξ) and gik+1 = gik for i = j

n

gk

=

gj (xk ,

ξ)

−

gjk

+

1 n

gik

i=1

xk+1 = proxγR(xk − γgk)

end for

Note that it can in practice happen that instead of ∇fi(x) one can query gi(x, ζ) such that Eξgi(·, ξ) = ∇fi(·) and Eξ gi(·, ξ) 2 ≤ σ2. This leads to a variant of SAGA which only uses noisy
estimates of the stochastic gradients ∇i(·). We call this variant N-SAGA (see Algorithm 6).

Lemma 2.6.16. We have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2 + 2σ2,

(2.38)

and E σk2+1 | xk ≤

where σ2 d=ef 1 n

gk − ∇fi(x∗)

2
.

kn

i

i=1

1− 1 n

σk2 + 2nL Df (xk, x∗) + σn2 ,

(2.39)

33

Corollary 2.6.17. Let γ = 61L . Then, iterates of Algorithm 6 satisfy

EV k ≤ 1 − min µ , 1 6L 2n

k
V0+

σ2

.

L

min(µ,

3L n

)

Analogous results can be obtained for L-SVRG.

Proof of Lemma 2.6.16

Let all expectations be conditioned on xk. By deﬁnition of gk we have

E gk − ∇f (x∗) 2

≤
=
(A.11)
≤


k

k 1n k

2
∗

E  gj(x , ζ) − gj + n gi − ∇f (x ) 

i=1


k

∗

∗

k 1n k

2
∗

E  gj(x , ζ) − ∇fj(x ) + ∇fj(x ) − gj + n gi − ∇f (x ) 

i=1

2E gj(xk, ζ) − ∇fj(x∗) 2

+2E ∇fj(x∗) − gjk − E ∇fj(x∗) − gjk 2

(A.14)
≤ 2E
= 2E
(A.14)
≤ 2E

gj(xk, ζ) − ∇fj(x∗) 2 + 2E ∇fj(x∗) − gjk 2 gj(xk, ζ) − ∇fj(x∗) 2 + 2 n1 n gik − ∇fi(x∗) 2
i=1 σk2
∇fj(xk) − ∇fj(x∗) 2 + 2σ2 + 2σk2

(2.37)
≤ 4LDf (xk, x∗) + 2σk2 + 2σ2

For the second inequality, we have

E σk2+1

=
=
≤
(2.37)
≤

1n nE
i=1

gik+1 − ∇fi(x∗) 2

1n n i=1

n −n 1 gik − ∇fi(x∗) 2 + n1 E

gi(xk, ζ) − ∇fi(x∗) 2

1n n i=1

n −n 1 gik − ∇fi(x∗) 2 + n1 ∇fi(xk) − ∇fi(x∗) 2 + σn2

1 − n1 σk2 + 2nL Df (xk, x∗) + σn2 .

34

2.6.7 SEGA
Algorithm 7 SEGA [69]
Input: learning rate γ > 0, starting point x0 ∈ Rd Set h0 = 0 for k = 0, 1, 2, . . . do Sample j ∈ [d] uniformly at random Set hk+1 = hk + ei(∇if (xk) − hki ) gk = dei(∇if (xk) − hki ) + hk xk+1 = proxγR(xk − γgk) end for

We show that the framework recovers the simplest version of SEGA (i.e., setup from Theorem D1 from [69]) in the proximal setting6.
Lemma 2.6.18. (Consequence of Lemmas A.3., A.4. from [69]) We have
E gk − ∇f (x∗) | xk 2 ≤ 2d ∇f xk − ∇f (x∗) 2 + 2dσk2

and E σk2+1 | xk = 1 − d1 σk2 + d1 ∇f xk − ∇f (x∗) 2 ,
where σk2 d=ef hk − ∇f (x∗) 2.

Given that we have from convexity and smoothness ∇f (xk) − ∇f (x∗) 2 ≤ 2LDf (xk, x∗), Assumption 2.4.1 holds the parameter choice as per Table C.4. Setting further M = 4d2, we get the next corollary.

Corollary

2.6.19.

SEGA

with

γ

=

1 6dL

satisﬁes

EV k ≤ 1 − µ

k
V 0.

6dL

6General version for arbitrary gradient sketches instead of partial derivatives can be recovered as well, however, we omit it for simplicity

35

2.6.8 N-SEGA
Algorithm 8 Noisy SEGA (N-SEGA)
Input: learning rate γ > 0, starting point x0 ∈ Rd Set h0 = 0 for k = 0, 1, 2, . . . do Sample i ∈ [d] uniformly at random and sample ξ Set hk+1 = hk + ei(gi(x, ξ) − hki ) gk = dei(gi(x, ξ) − hki ) + hk xk+1 = xk − γgk end for

Here we assume that gi(x, ζ) is a noisy estimate of the partial derivative ∇if (x) such that Eζ gi(x, ζ) = ∇if (x) and Eζ |gi(x, ζ) − ∇if (x)|2 ≤ σd2 .
Lemma 2.6.20. The following inequalities hold:
E gk − ∇f (x∗) 2 ≤ 4dLDf (xk, x∗) + 2dσk2 + 2dσ2,

where σk2 =

E σk2+1 ≤

hk − ∇f (x∗)

2
.

1− 1 d

σk2 + 2dL Df (xk, x∗) + σd2 ,

Corollary 2.6.21. Let γ = 6L1d . Applying Theorem 2.4.4 with M = 4d2, iterates of Algo-

rithm 8 satisfy

EV k ≤ 1 − µ

k
V0+

σ2

.

6dL

Lµ

Proof of Lemma 2.6.20 Let all expectations be conditioned on xk. For the ﬁrst bound, we write

gk − ∇f (x∗) = hk − ∇f (x∗) − dhki ei + d∇if (x∗)ei + dgi(xk, ξ)ei − d∇if (x∗)ei .

a

b

Let us bound the expectation of each term individually. The ﬁrst term can be bounded as

E a 2 = E I − deiei (hk − ∇f (x∗)) 2
2
= (d − 1) hk − ∇f (x∗) 2

≤

d

hk − ∇f (x∗)

2
.

36

The second term can be bounded as

E b 2 = EiEξ dgi(x, ξ)ei − d∇fi(x∗)ei 2

=

EiEξ

dgi(xk, ξ)ei − d∇if (xk)ei

2
+ Ei

d∇if (xk)ei − d∇fi(x∗)ei

2

≤ dσ2 + d ∇f (xk) − ∇f (x∗) 2

≤ dσ2 + 2LdDf (xk, x∗),

where in the last step we used L–smoothness of f . It remains to combine the two bounds.

For the second bound, we have

E hk+1 − ∇f (x∗) 2 = E hk + gi(xk, ξ)ei − hki − ∇f (x∗) 2

= E I − eiei hk + gi(xk, ξ)ei − ∇f (x∗) 2

=

E

I − eiei

(hk − ∇f (x∗))

2
+E

gi(xk, ξ)ei − ∇if (x∗)ei

2

=

1− 1

hk − ∇f (x∗)

2
+E

gi(xk, ξ)ei − ∇if (xk)ei

2

d

+E ∇if (xk)ei − ∇if (x∗)ei 2

=

1− 1

hk − ∇f (x∗)

2
+

σ2

+

1

∇f (xk) − ∇f (x∗) 2

d

dd

≤ 1 − d1 hk − ∇f (x∗) 2 + σd2 + 2dL Df (xk, x∗).

2.6.9 SVRG

Algorithm 9 SVRG [82]

Input: learning rate γ > 0, epoch length m, starting point x0 ∈ Rd

φ = x0

for s = 0, 1, 2, . . . do

for k = 0, 1, 2, . . . , m − 1 do

Sample i ∈ {1, . . . , n} uniformly at random

gk = ∇fi(xk) − ∇fi(φ) + ∇f (φ)

xk+1 = proxγR(xk − γgk)

end for

φ

=

x0

=

1 m

m k=1

xk

end for

Let σk2 d=ef n1 n ∇fi(φ) − ∇fi(x∗) 2. We will show that Lemma 2.4.3 recovers per-epoch analysis
i=1
of SVRG in a special case.

37

Lemma 2.6.22. For k mod m = 0 we have E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2
and E σk2+1 | xk = σk2+1 = σk2.

(2.40) (2.41)

Proof. The proof of (2.40) is identical to the proof of (2.34). Next, (2.41) holds since σk does not depend on k.

Thus, Assumption 2.4.1 holds with parameter choice as per Table C.4 and Lemma 2.4.3 implies the next corollary.

Corollary 2.6.23.

E xk+1 − x∗ 2 + γ(1 − 2γL)EDf (xk, x∗) ≤ (1 − γµ)E xk − x∗ 2 + 2γ2Eσk2.

(2.42)

Recovering SVRG rate

Summing (2.42) for k = 0, . . . , m − 1 using σk = σ0 we arrive at

m

2

E xm − x∗ 2 + γ(1 − 2γL)EDf (xk, x∗) ≤ (1 − γµ)E x0 − x∗ + 2mγ2Eσ02

k=1

≤ 2 µ−1 + 2mγ2L Df (x0, x∗) .

Since Df is convex in the ﬁrst argument, we have

mγ(1 − 2γL)Df

1 m xk, x∗ m k=1

m
≤ xm − x∗ 2 + γ(1 − 2γL)Df (xk, x∗)
k=1

and thus

Df m1 m xk, x∗ ≤ 2 mµ−γ1(1+−22mγγL2)L Df (x0, x∗),
k=1

which recovers rate from Theorem 1 in [82].

2.6.10 L-SVRG

In this section we show that our approach also covers L-SVRG analysis from [77, 103] (see Algorithm 10) with a minor extension – it allows for proximable regularizer R. Consider the ﬁnite-sum minimization problem

1n

f (x) =

fi(x) + R(x),

n i=1

(2.43)

38

where each fi convex and L-smooth for each i and f is µ-strongly convex.
Algorithm 10 L-SVRG ([77, 103])
Input: learning rate γ > 0, probability p ∈ (0, 1], starting point x0 ∈ Rd w0 = x0 for k = 0, 1, 2, . . . do Sample i ∈ {1, . . . , n} uniformly at random gk = ∇fi(xk) − ∇fi(wk) + ∇f (wk) xk+1 = xk − γgk wk+1 = xk with probability p wk with probability 1 − p end for

Note that the gradient estimator is again unbiased, i.e. E gk | xk = ∇f (xk). Next, Lemma 2.6.24 provides with the remaining constants for Assumption 2.4.1. The corresponding choice is stated in Table C.4.

Lemma 2.6.24 (Lemma 4.2 and Lemma 4.3 from [103] extended to prox setup). We have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2

(2.44)

and E σk2+1 | xk ≤ (1 − p)σk2 + 2LpDf (xk, x∗),
where σk2 d=ef n1 n ∇fi(wk) − ∇fi(x∗) 2.
i=1

Next,

applying

Theorem

2.4.4

on

Algorithm

10

with

M

=

4 p

we

get

Corollary

2.6.25.

Corollary

2.6.25.

L-SVRG

with

γ

=

1 6L

satisﬁes

EV k ≤ 1 − min

µp ,

6L 2

k
V 0.

(2.45) (2.46)

39

Proof of Lemma 2.6.24 Let all expectations be conditioned on xk. Using deﬁnition of gk

E gk − ∇f (x∗) 2

Alg=. 10 E ∇fi(xk) − ∇fi(wk) + ∇f (wk) − ∇f (x∗) 2

(A.11)
≤

2E ∇fi(xk) − ∇fi(x∗) 2

+2E ∇fi(x∗) − ∇fi(wk) − E ∇fi(x∗) − ∇fi(wk) | xk 2

(2.37),(A.14)

≤

4LDf (xk, x∗) + 2E

∇fi(wk) − ∇fi(x∗) 2

=

4LDf (xk, x∗) + 2σk2.

For the second bound, we shall have

E σk2+1

Alg=. 10
(2.37)
≤

(1 − p)σk2 + np n ∇fi(xk) − ∇fi(x∗) 2
i=1
(1 − p)σk2 + 2LpDf (xk, x∗).

2.6.11 DIANA

In this section we consider a distributed setup where each function fi from (2.3) is owned by i-th machine (thus, we have all together n machines).

We show that our approach covers the analysis of DIANA from [139, 79]. DIANA is a speciﬁc algorithm for distributed optimization with quantization – lossy compression of gradient updates, which reduces the communication between the server and workers7.

In particular, DIANA quantizes gradient diﬀerences instead of the actual gradients. This trick allows for the linear convergence to the optimum once the full gradients are evaluated on each machine, unlike other popular quantization methods such as QSGD [4] or TernGrad [227]. In this case, DIANA behaves as variance reduced method – it reduces a variance that was injected due to the quantization. However, DIANA also allows for evaluation of stochastic gradients on each machine, as we shall further see.

First of all, we introduce the notion of quantization operator.

Deﬁnition 2.6.26 (Quantization). We say that ∆ˆ is a quantization of vector ∆ ∈ Rd and

write ∆ˆ ∼ Q(∆) if

E∆ˆ = ∆, E ∆ˆ − ∆ 2 ≤ ω ∆ 2

(2.47)

7It is a well-known problem in distributed optimization that the communication between machines often takes more time than actual computation.
40

for some ω > 0.

Algorithm 11 DIANA [139, 79]

Input:

learning

rates

α>0

and

γ

> 0,

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

and

h0

=

1 n

1: for k = 0, 1, . . . do

2: Broadcast xk to all workers

3: for i = 1, . . . , n in parallel do

4:

Sample gik such that E[gik | xk] = ∇fi(xk)

5:

∆ki = gik − hki

6:

Sample ∆ˆ ki ∼ Q(∆ki )

7:

hki +1 = hki + α∆ˆ ki

8:

gˆik = hki + ∆ˆ ki

9: end for

10:

∆ˆ k = n1

n i=1

∆ˆ ki

11:

gk

=

1 n

n i=1

gˆik

=

hk

+

∆ˆ k

12: xk+1 = proxγR xk − γgk

13:

hk+1

=

1 n

n i=1

hki +1

=

hk

+

α∆ˆ k

14: end for

n i=1

h0i

The aforementioned method is applied to solve problem (2.1)+(2.3) where each fi is convex and L-smooth and f is µ-strongly convex.
Lemma 2.6.27 (Lemma 1 and consequence of Lemma 2 from [79]). Suppose that α ≤ 1+1ω . For all iterations k ≥ 0 of Algorithm 11 it holds

E gk | xk E gk − ∇f (x∗) 2 | xk
E σk2+1 | xk

= ∇f (xk),

(2.48)

≤ 1 + 2nω n1 n ∇fi(xk) − ∇fi(x∗) 2
i=1

+ 2ωσk2

+

(1

+

ω)σ2 ,

n

n

(2.49)

≤ (1 − α)σk2 + αn n ∇fi(xk) − ∇fi(x∗) 2 + ασ2. (2.50)
i=1

n
where σ2 = 1

hk − ∇fi(x∗) 2 and σ2 is such that 1 n E

kn

i

n

i=1

i=1

gik − ∇fi(xk) 2 | xk ≤ σ2.

Bounding further n1 ni=1 ∇fi(xk) − ∇fi(x∗) 2 ≤ 2LDf (xk, x∗) in the above Lemma, we see that Assumption 2.4.1 as per Table C.4 is valid. Thus, as a special case of Theorem 2.4.4, we
obtain the following corollary.

Corollary 2.6.28. Assume that fi is convex and L-smooth for all i ∈ [n] and f is µ strongly

41

convex,

α

≤

ω+1 1 ,

γ

≤

1
(1+ 2ω )L+M Lα

where

M

>

n2ωα .

Then

the

iterates

of

DIANA

satisfy

n

E V k ≤ max

(1 − γµ)k,

1 + 2ω − α k nM

V 0 + 1+nω + M α σ2γ2 , min γµ, α − n2Mω

(2.51)

where the Lyapunov function V k is deﬁned by V k d=ef xk − x∗ 2 + M γ2σk2. For the particular

choice

α

=

ω+1 1 ,

M

=

4ω(ωn+1) ,

γ

=

(

1+

1
6ω

)

L

,

then

DIANA

converges

to

a

solution

neighborhood

n

and the leading iteration complexity term is

max

11

, γµ α −

2ω

nM

= max κ + κ 6ω , 2(ω + 1) , n

(2.52)

where κ = Lµ .

As mentioned, once the full (deterministic) gradients are evaluated on each machine, DIANA converges linearly to the exact optimum. In particular, in such case we have σ2 = 0. Corollary 2.6.29 states the result in the case when n = 1, i.e. there is only a single node 8. For completeness, we present the mentioned simple case of DIANA as Algorithm 12.

Algorithm 12 DIANA: 1 node & exact gradients [139, 79]
Input: learning rates α > 0 and γ > 0, initial vectors x0, h0 ∈ Rd 1: for k = 0, 1, . . . do 2: ∆k = ∇f (xk) − hk 3: Sample ∆ˆ k ∼ Q(∆k) 4: hk+1 = hk + α∆ˆ k 5: gk = hk + ∆ˆ k 6: xk+1 = proxγR xk − γgk 7: end for

Corollary 2.6.29. Assume that fi is µ-strongly convex and L-smooth for all i ∈ [n], α ≤ ω+1 1 ,

γ

≤

1 (1+2ω)L+M Lα

where

M

>

2αω .

Then

the

stochastic

gradient

gˆk

and

the

objective

function

f

satisfy Assumption 2.4.1 with A = (1 + 2ω) L, B = 2ω, σk2 = hk − h∗ 2 , ρ = α, C = Lα, D1 =

0, D2 = 0 and

E V k ≤ max (1 − γµ)k, 1 + 2ω − α k V 0, M

(2.53)

where the Lyapunov function V k is deﬁned by V k d=ef xk − x∗ 2 + M γ2σk2. For the particular

choice

α

=

ω+1 1 ,

M

=

4ω(ω + 1),

γ

=

1 (1+6ω)L

the

leading

term

in

the

iteration

complexity

8node = machine
42

bound is where κ = Lµ .

11

max

, γµ

α

−

2ω

M

= max {κ + 6κω, 2(ω + 1)} ,

(2.54)

2.6.12 Q-SGD-SR
In this section, we consider a quantized version of SGD-SR.
Algorithm 13 Q-SGD-SR Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ]
is vector of ones for k = 0, 1, 2, . . . do
Sample ξ ∼ D gk ∼ Q(∇fξ(xk)) xk+1 = proxγR(xk − γgk) end for

Lemma 2.6.30 (Generalization of Lemma 2.4, [63]). If (f, D) ∼ ES(L), then ED gk − ∇f (x∗) 2 ≤ 4L(1 + ω)Df (xk, x∗) + 2σ2(1 + ω).

(2.55)

where σ2 d=ef ED ∇fξ(x∗) 2 .

A direct consequence of Theorem 2.4.4 in this setup is Corollary 2.6.31.

Corollary 2.6.31. Assume that f (x) is µ-strongly quasi-convex and (f, D) ∼ ES(L). Then

Q-SGD-SR

with

γk

≡

γ

≤

1 2(1+ω)L

satisﬁes

E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γ(1 + ω)σ2 . µ

(2.56)

Proof of Lemma 2.6.30 In this proof all expectations are conditioned on xk. First of all, from Lemma 2.6.4 we have
ED ∇fξ(xk) − ∇f (x∗) 2 ≤ 4LDf (xk, x∗) + 2σ2.
The remaining step is to understand how quantization of ∇fξ(xk) changes the above inequality if we put gk ∼ Q(∇fξ(xk)) instead of ∇fξ(xk). Let us denote mathematical expectation with respect randomness coming from quantization by EQ [·]. Using tower property of mathematical

43

expectation we get

E gk − ∇f (x∗) 2

=
(A=.14)
(2.55)
≤

ED EQ gk − ∇f (x∗) 2 E gk − ∇fξ(xk) 2 + E

∇fξ(xk) − ∇f (x∗) 2

E gk − ∇fξ(xk) 2 + 4LDf (xk, x∗) + 2σ2.

Next, we estimate the ﬁrst term in the last row of the previous inequality

E gk − ∇fξ(xk) 2

(2.47)
≤
(A.11)
≤
≤

ωE ∇fξ(xk) 2
2ωE ∇fξ(xk) − ∇fξ(x∗) 2 + 2ωE 4ωLDf (xk, x∗) + 2ωσ2.

∇fξ(x∗) 2

Putting all together we get the result.

2.6.13 VR-DIANA
Corollary 2.6.28 shows that once each machine evaluates a stochastic gradient instead of the full gradient, DIANA converges linearly only to a certain neighborhood. In contrast, VR-DIANA [79] uses a variance reduction trick within each machine, which enables linear convergence to the exact solution. In this section, we show that our approach recovers VR-DIANA as well.

44

Algorithm 14 VR-DIANA based on L-SVRG (Variant 1), SAGA (Variant 2), [79]

Input:

learning

rates

α>0

and

γ

> 0,

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

,

h0

=

1 n

1: for k = 0, 1, . . . do

n i=1

h0i

2:

Sample random uk =

1,

with

probability

1 m

0,

with

probability

1

−

1 m

3: Broadcast xk, uk to all workers

only for Variant 1

4: for i = 1, . . . , n in parallel do

5:

Pick random jik ∼u.a.r. [m]

m

6:

µki

=

1 m

∇fij (wikj )

j=1

Worker side

7: gik = ∇fijik (xk) − ∇fijik (wikjik ) + µki

8:

∆ˆ ki = Q(gik − hki )

9:

hki +1 = hki + α∆ˆ ki

10:

for j = 1, . . . , m do

11:

wk+1 = xk, if uk = 1 Variant 1 (L-SVRG): update epoch gradient if uk = 1

ij

wikj, if uk = 0

12:

wk+1 = xk, j = jik

ij

wikj , j = jik

13:

end for

Variant 2 (SAGA): update gradient table

14: end for n

15:

hk

+1

=

hk

+

α n

∆ˆ ki

i=1

16: gk = 1 n (∆ˆ k + hk)

n

i

i

i=1

17: xk+1 = xk − γgk

Gather quantized updates

18: end for

The aforementioned method is applied to solve problem (2.1)+(2.3) where each fi is also of a ﬁnite sum structure, as in (4.3), with each fij(x) being convex and L-smooth, and fi(x) being µ-strongly convex. Note that ∇f (x∗) = 0 and, in particular, Df (x, x∗) = f (x) − f (x∗) since the problem is considered without regularization.
Lemma 2.6.32 (Lemmas 3, 5, 6 and 7 from [79]). Let α ≤ ω+1 1 . Then for all iterates k ≥ 0 of Algorithm 14 the following inequalities hold:

E gk | xk E Hk+1 | xk E Dk+1 | xk E gk 2 | xk

= ∇f (xk),

(2.57)

≤ (1 − α) Hk + 2α Dk + 8αLn f (xk) − f (x∗) , m
≤ 1 − 1 Dk + 2Ln f (xk) − f (x∗) , m

(2.58) (2.59)

≤ 2L 1 + 4ω + 2 n

f (xk) − f (x∗) + 2ω Dk + 2(ω + 1) Hk, (2.60)

n2 m

n2

45

n

2

nm

2

where Hk = hki − ∇fi(x∗) and Dk =

∇fij(wikj) − ∇fij(x∗) .

i=1

i=1 j=1

Corollary 2.6.33. Let α ≤ min 31m , ω+1 1 . Then stochastic gradient gˆk (Algorithm 18) and

the objective function f satisfy Assumption 2.4.1 with A = 1 + 4ωn+2 L, B = 2(ωn+1) , ρ =

α, C = L

1 m

+

4α

, D1 = 0, D2 = 0 and

σk2 = Hnk + nDmk = n1 n hki − ∇fi(x∗) 2 + n1m n m ∇fij(wikj) − ∇fij(x∗) 2 .

i=1

i=1 j=1

Proof. Indeed, (2.7) holds due to (3.30). Inequality (2.8) follows from (3.33) with A =

1

+

4ω+2 n

L, B

=

2(ω+1) n

,

D1

=

0, σk2

=

Hk n

+ nDmk

if

we

take

into

account

that

2ω n2

Dk m

+ 2(ωn+2 1) H k

≤

2(ωn+1) nDmk + Hnk . Finally, summing inequalities (3.31) and (3.32) and using α ≤ 31m

E σk2 | xk

=
(3.31)+(3.32)
≤
≤

1 E

Hk+1 | xk

+

1 E Dk+1 | xk

n

nm

(1 − α) Hk + 1 + 2α − 1 Dk + 2L 1 + 4α

n

m nm

m

(1 − α) σk2 + 2L m1 + 4α f (xk) − f (x∗)

f (xk) − f (x∗)

we get (2.9) with ρ = α, C = L

1 m

+

4α

, D2 = 0.

Corollary 2.6.34. Assume that fi is µ-strongly convex and fij is convex and L-smooth for

all i ∈ [n], j ∈ [m], α ≤ min

31m , ω+1 1

,

γ

≤

1
(1+ 4ω+2 )L+M L( 1 +4α)

where

M

>

2(ωn+α1) .

Then

n

m

the iterates of VR-DIANA satisfy

E V k ≤ max (1 − γµ)k, 1 + 2(ω + 1) − α k V 0, nM

(2.61)

where the Lyapunov function V k is deﬁned by V k d=ef xk − x∗ 2+M γ2σk2. Further, if we set α =

min

31m , ω+1 1

,

M

=

4(ωn+α1) ,

γ

=

(

1+

20

ω

1
+18

+

4

ω

+4

)L

,

then

to

achieve

precision

E

n

nαm

xk − x∗ 2 ≤

εV 0 VR-DIANA needs O max κ + κ ω+n 1 + κ (ω+1) mnaxm{m,ω+1} , m, ω + 1 log 1ε iterations, where

κ = Lµ .

Proof. Using Corollary 2.6.33 we apply Theorem 2.4.4 and get the result.

Remark 2.6.35. VR-DIANA can be easily extended to the proximal setup in our framework.

46

2.6.14 JacSketch

In this section, we show that our approach covers the analysis of JacSketch from [62]. JacSketch is a generalization of SAGA in the following manner. SAGA observes every iteration ∇fi(x) for random index i and uses it to build both stochastic gradient as well as the control variates on the stochastic gradient in order to progressively decrease variance. In contrast, JacSketch observes every iteration the random sketch of the Jacobian, which is again used to build both stochastic gradient as well as the control variates on the stochastic gradient.

For simplicity, we do not consider proximal setup, since [62] does not either.

We ﬁrst introduce the necessary notation (same as in [62]). Denote ﬁrst the Jacobian the

objective

∇F(x) d=ef [∇f1(x), . . . , ∇fn(x)] ∈ Rd×n.

(2.62)

Every iteration of the method, a random sketch of Jacobian ∇F (xk)S (where S ∼ D) is observed. Then, the method builds a variable Jk, which is the current Jacobian estimate, updated using so-called sketch and project iteration [60]:

Jk+1 = Jk(I − ΠSk ) + ∇F(xk)ΠSk ,

where ΠS is a projection under W norm9 (W ∈ Rn×n is some positive deﬁnite weight matrix) deﬁned as ΠS d=ef S(S WS)†S W10.

Further, in order to construct unbiased stochastic gradient, an access to the random scalar θS

such that

ED [θSΠS] e = e,

(2.63)

where e is the vector of all ones.

Next, the simplest option for the choice of the stochastic gradient is ∇fS(x) – an unbiased estimate of ∇f directly constructed using S, θS:

∇fS(x) = θS ∇F(x)ΠSe. n

(2.64)

However, one can build a smarter estimate ∇fS,J(x) via control variates constructed from J:

∇fS,J(x) = θS (∇F(x) − J)ΠSe + 1 Je.

n

n

(2.65)

The resulting algorithm is stated as Algorithm 15.

9Weighted Frobenius norm of matrix X ∈ Rn×n with a positive deﬁnite weight matrix W ∈ Rn×n is deﬁned as X W−1 d=ef Tr (XW−1X ).
10Symbol † stands for Moore-Penrose pseudoinverse.

47

Algorithm 15 JacSketch [62]
Input: (D, W, θS), x0 ∈ Rd, Jacobian estimate J0 ∈ Rd×n, stepsize γ > 0 1: for k = 0, 1, 2, . . . do 2: Sample a fresh copy Sk ∼ D 3: Jk+1 = Jk(I − ΠSk ) + ∇F(xk)ΠSk 4: gk = ∇fSk,Jk (xk) 5: xk+1 = xk − γgk
6: end for

Next we present Lemma 2.6.36 which directly justiﬁes the parameter choice from Table 2.1.

Lemma 2.6.36 (Lemmas 2.5, 3.9 and 3.10 from [62]). Suppose that there are constants L1, L2 > 0 such that

ED

∇fS(x) − ∇fS(x∗)

2 2

ED

(∇F(x) − ∇F(x∗))ΠS

2 W−1

≤ 2L1(f (x) − f (x∗)), ≤ 2L2(f (x) − f (x∗)),

∀x ∈ Rd ∀x ∈ Rd,

Then

ED Jk+1 − ∇F(x∗) 2 ≤ (1 − λmin) Jk − ∇F(x∗) 2 + 2L2(f (xk) − f (x∗)), (2.66)

W−1

W−1

ED gk 2 ≤ 4L1(f (xk) − f (x∗)) + 2 λmax Jk − ∇F(x∗) 2 ,

2

n2

W−1

where λmin = λmin (ED [ΠS]) and λmax = λmax W1/2 ED θS2 ΠSee ΠS − ee ther, ED [∇fS,J(x)] = ∇f (x).

(2.67) W1/2 . Fur-

Thus, as a direct consequence of Theorem 2.4.4, we obtain the next corollary.

Corollary 2.6.37. Consider the setup from Lemma 2.6.36. Suppose that f is µ-strongly

convex and choose γ ≤ min satisfy

µ1 ,

1
L2

2L1+M n

where M > 2nλλmmainx . Then the iterates of JacSketch

E V k ≤ max (1 − γµ)k, 1 + 2nλMmax − λmin k V 0.

(2.68)

Remark 2.6.38. We shall note that concurrently with this work, a more general version of JacSketch with reﬁned analysis was proposed in [71], obtaining many new methods in special case (such as LSVRG, SEGA and several new ones), with best known rate in each special case. As mentioned in the main body of the paper, the rates from [71] for methods that have randomness in partial derivatives and non-uniform smoothness are better to what can Theorem 2.4.4 achieve. On the other hand, [71] only focuses on variance reduced methods,

48

while this paper analyzes also methods with extra noise.

2.6.15 Interpolation Between Methods
Given that a set of stochastic gradients satisfy Assumption 2.4.1, we show that an any convex combination of the mentioned stochastic gradients satisfy Assumption 2.4.1 as well.

Lemma 2.6.39. Assume that sequences of stochastic gradients {g1k}k≥0, . . . , {gmk }k≥0 at the

common iterates {xk}k≥0 satisfy the Assumption 2.4.1 with parameters A(j), B(j), {σk2(j)}k≥0,

C(j), ρ(j), D1(j), D2(j), j ∈ [m] respectively. Then for any vector τ = (τ1, . . . , τm) such as

m

m

τj = 1 and τj ≥ 0, j ∈ [m] stochastic gradient gτk = τjgjk satisﬁes the Assumption 2.4.1

j=1

j=1

with parameters:

m
Aτ = τjA(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τjσk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τjC(j)B(j), Dτ,1 = τjD1(j), Dτ,2 = τjD2(j)B(j).

j=1

j=1

j=1

(2.69)

Furthermore,

if

stochastic

gradients

g

k 1

,

.

.

.

,

g

k m

are

independent

for

all

k,

Assumption

2.4.1

is

satisﬁed with parameters

m
Aτ = L + τj2A(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τj2σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τj2C(j)B(j), Dτ,1 = τj2D1(j), Dτ,2 = τj2D2(j)B(j).

j=1

j=1

j=1

(2.70)

What is more, instead of taking convex combination one can choose stochastic gradient at random. Lemma 2.6.40 provides the result.

Lemma 2.6.40. Assume that sequences of stochastic gradients {g1k}k≥0, . . . , {gmk }k≥0 at the common iterates {xk}k≥0 satisfy the Assumption 2.4.1 with parameters A(j), B(j), {σk2(j)}k≥0,
C(j), ρ(j), D1(j), D2(j), j ∈ [m] respectively. Then for any vector τ = (τ1, . . . , τm) such
m
as τj = 1 and τj ≥ 0, j ∈ [m] stochastic gradient gτk which equals gjk with probability τj
j=1
satisﬁes the Assumption 2.4.1 with parameters:

m
Aτ = τjA(j),
j=1

Bτ = 1,

m
στ2,k = τjB(j)σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τjB(j)C(j), Dτ,1 = τjD1(j), Dτ,2 = B(j)τjD2(j).

j=1

j=1

j=1

(2.71)

Furthermore,

if

stochastic

gradients

g

k 1

,

.

.

.

,

g

k m

are

independent

for

all

k,

Assumption

2.4.1

is

49

satisﬁed with parameters

m
Aτ = L + τj2A(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τj2σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τj2C(j)B(j), Dτ,1 = τj2D1(j), Dτ,2 = τj2D2(j)B(j).

j=1

j=1

j=1

(2.72)

Example 2.6.41 (τ -L-SVRG). Consider the following method — τ -L-SVRG — which interpo-
lates between vanilla SGD and L-SVRG.
Algorithm 16 τ -L-SVRG
Input: learning rate γ > 0, probability p ∈ (0, 1], starting point x0 ∈ Rd, convex combination parameter τ ∈ [0, 1] w0 = x0 for k = 0, 1, 2, . . . do Sample i ∈ {1, . . . , n} uniformly at random gLk−SV RG = ∇fi(xk) − ∇fi(wk) + ∇f (wk) Sample j ∈ {1, . . . , n} uniformly at random gSkGD = ∇fj (xk) gk = τ gSkGD + (1 − τ )gLk −SV RG xk+1 = xk − γgk wk+1 = xk with probability p wk with probability 1 − p end for

When τ = 0 the Algorithm 16 becomes L-SVRG and when τ = 1 it is just SGD with uniform sampling. Notice that Lemmas 2.6.24 and 2.6.4 still hold as they does not depend on the update rule for xk+1.

Thus, sequences {gSkGD}k≥0 and {gLk−SV RG}k≥0 satisfy the conditions of Lemma 2.6.39 and, as a consequence, stochastic gradient gk from τ -L-SVRG meets the Assumption 2.4.1 with the
following parameters:

Aτ = L + 2τ 2L + 2(1 − τ )2L,

Bτ = 1,

στ2,k = 2 (1 −nτ )2 n

∇fi(wk) − ∇fi(x∗)

2
,

i=1

ρτ = p, Cτ = 2(1 − τ )2Lp, Dτ,1 = 2τ 2σ2, Dτ,2 = 0.

Remark 2.6.42. Similar interpolation with the analogous analysis can be considered between SGD and SAGA, or SGD and SVRG.

50

Proof of Lemma 2.6.39

Indeed, (2.7) holds due to linearity of mathematical expectation. Next, summing inequalities

(2.8)

for

g

k 1

,

.

.

.

,

g

k m

and

using

convexity

of

· 2 we get

E gτk − ∇f (x∗) 2 | xk

m

2

≤

τjE gjk − ∇f (x∗) | xk

j=1

(2.8)

m

m

m

≤ 2 τjA(j)Df (xk, x∗) + B(j)τjσk2(j) + τjD1(j),

j=1

j=1

j=1

m

m

which implies (2.8) for gτk with Aτ = τjA(j), Bτ = 1, στ2,k = τjB(j)σk2(j), Dτ,1 =

j=1

j=1

m

τj D1 (j ).

Finally,

summing

(2.9)

for

g1k

,

.

.

.

,

g

k m

gives

us

j=1

(2.9)
E στ2,k+1 | στ2,k ≤

1 − min ρ(j)
j∈[m]

m

m

στ2,k + 2 τjB(j)C(j)Df (xk, x∗) + τjB(j)D2(j),

j=1

j=1

m

m

which is exactly (2.9) for στ2,k with ρ = min ρ(j), Cτ = τjC(j), Dτ,2 = τjD2(j).

j∈[m]

j=1

j=1

Next, for independent gradients we have

E gτk − ∇f (x∗) 2 | xk

m

=

τj2E

j=1

gjk − ∇f (x∗) 2 | xk

+2 τiτjE gjk − ∇f (x∗), gik − ∇f (x∗)
i<j

m

=

τj2E

j=1

gjk − ∇f (x∗) 2 | xk + 2 τiτj ∇f (xk) − ∇f (x∗) 2
i<j

m

≤

τj2E

j=1

gjk − ∇f (x∗) 2 | xk

 m 2 +  τj
j=1

∇f (xk) − ∇f (x∗) 2

m

=

τj2E

j=1

gjk − ∇f (x∗) 2 | xk + ∇f (xk) − ∇f (x∗) 2

m

≤

τj2E

j=1

gjk − ∇f (x∗) 2 | xk + 2LDf (xk, x∗).

(2.73)

and further the bounds follow.

51

Proof of Lemma 2.6.40

Indeed, (2.7) holds due to linearity and tower property of mathematical expectation. Next, using

tower

property

of

mathematical

expectation

and

inequalities

(2.8)

for

g

k 1

,

.

.

.

,

g

k m

we

get

E gτk − ∇f (x∗) 2 | xk

2

m

2

= E Eτ gτk − ∇f (x∗) | xk = τjE gjk − ∇f (x∗) | xk

j=1

(2.8)

m

m

m

≤ 2 τjA(j)Df (xk, x∗) + B(j)τjσk2(j) + τjD1(j),

j=1

j=1

j=1

m

m

which implies (2.8) for gτk with Aτ = τjA(j), Bτ = 1, στ2,k = τjB(j)σk2(j), Dτ,1 =

j=1

j=1

m

τj D1 (j ).

Finally,

summing

(2.9)

for

g1k

,

.

.

.

,

g

k m

gives

us

j=1

(2.9)
E στ2,k+1 | στ2,k ≤

1 − min ρ(j)
j∈[m]

m

m

στ2,k + 2 τjB(j)C(j)Df (xk, x∗) + τjB(j)D2(j),

j=1

j=1

m

m

which is exactly (2.9) for στ2,k with ρ = min ρ(j), Cτ = τjB(j)C(j), Dτ,2 = τjB(j)D2(j).

j∈[m]

j=1

j=1

To show (2.72), it suﬃces to combine above bounds with the trick (2.73).

Remark 2.6.43. Recently, [217] demonstrated in that the convex combination of SGD and SARAH [157] performs very well on non-convex problems.

2.7 Experiments

2.7.1 Experiments on SGD-MB

In Section 2.6.3, we describe in detail the SGD-MB method already outlined before. The main advantage of SGD-MB is that the sampling procedure it employs can be implemented in just O(τ log n) time. In contrast, even the simplest without-replacement sampling which selects each function into the minibatch with a prescribed probability independently (we will refer to it as independent SGD) requires n calls of a uniform random generator. We demonstrate numerically that SGD-MB has essentially identical iteration complexity to independent SGD in practice. We consider logistic regression with Tikhonov regularization:

1n n log 1 + exp ai x · bi
i=1

+ λ x 2, 2

(2.74)

For a ﬁxed expected sampling size τ , consider two options for the probability of sampling the i-th function:
1. nτ , or

52

2. δ+aiai2+2+λλ , where δ is such that11 ni=1 δ+aiai2+2+λλ = 1. The results can be found in Figure 2.1, where we also report the choice of stepsize γ and the choice of τ in the legend and title of the plot, respectively.

Relative suboptimality

Dataset: a1a, tau: 10, unif
100

10−1

10−2

10−3
10−4 0
100 10−1 10−2

r: True, gamma: 0.65864 r: True, gamma: 0.06586 r: False, gamma: 0.65864 r: False, gamma: 0.06586

5000

10000

15000 20000 25000
Iteration

30000

35000

40000

Dataset: w1a, tau: 10, unif
r: True, gamma: 0.98214 r: True, gamma: 0.09821 r: False, gamma: 0.98214 r: False, gamma: 0.09821

10−3

10−4

10−5 0

10000

20000

30000

40000

Iteration

50000

60000

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 10, imp

100

r: True, gamma: 0.90940

r: True, gamma: 0.09094

r: False, gamma: 0.90940

10−1 r: False, gamma: 0.09094

10−2

10−3 0
100

5000

10000

15000 20000 25000
Iteration

30000

35000

40000

Dataset: w1a, tau: 10, imp

10−1

10−2

10−3 10−4
0

r: True, gamma: 2.26848 r: True, gamma: 0.22685 r: False, gamma: 2.26848 r: False, gamma: 0.22685

10000

20000

30000

40000

Iteration

50000

60000

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 50, unif
100

10−1

10−2

10−3 10−4 10−5
0
100 10−1 10−2

r: True, gamma: 0.98879 r: True, gamma: 0.09888 r: False, gamma: 0.98879 r: False, gamma: 0.09888

5000

10000

15000 20000 25000
Iteration

30000

35000

40000

Dataset: w1a, tau: 50, unif
r: True, gamma: 2.64488 r: True, gamma: 0.26449 r: False, gamma: 2.64488 r: False, gamma: 0.26449

10−3

10−4

10−5 0

10000

20000

30000

40000

Iteration

50000

60000

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 50, imp

100

r: True, gamma: 1.06421

r: True, gamma: 0.10642

10−1 r: False, gamma: 1.06421 r: False, gamma: 0.10642

10−2

10−3

10−4 0
100 10−1

5000

10000

15000 20000 25000
Iteration

30000

35000

40000

Dataset: w1a, tau: 50, imp
r: True, gamma: 3.39828 r: True, gamma: 0.33983 r: False, gamma: 3.39828 r: False, gamma: 0.33983

10−2

10−3

10−4 0

10000

20000

30000

40000

Iteration

50000

60000

Relative suboptimality

Figure 2.1: SGD-MB and independent SGD applied on LIBSVM [27] datasets with regularization parameter λ = 10−5. Axis y stands for relative suboptimality, i.e. ff((xxkk))−−ff((xx∗0)) . Title label “unif” corresponds to probabilities chosen by 1 while label “imp” corresponds to probabilities chosen by 2. Lastly, legend label “r” corresponds to “replacement” with value “True” for SGD-MB and value “False” for independent SGD.
Indeed, iteration complexity of SGD-MB and independent SGD is almost identical. Since the cost of each iteration of SGD-MB is cheaper12, we conclude superiority of SGD-MB to independent SGD.
11An RCD version of this sampling was proposed in [70]; it was shown to be superior to uniform sampling both in theory and practice.
12The relative diﬀerence between iteration costs of SGD-MB and independent SGD can be arbitrary, especially
53

2.7.2 Experiments on SGD-star

In this section, we study SGD-star and numerically verify claims from Section 2.6.4. In particular,

Corollary 2.6.12 shows that SGD-star enjoys linear convergence rate which is constant times

better to the rate of SAGA (given that problem condition number is high enough). We compare

3 methods – SGD-star, SGD and SAGA. We consider simple and well-understood least squares

problem

minx

1 2

Ax − b

2 where elements of A, b were generated (independently) from standard

normal distribution. Further, rows of A were normalized so that Ai: = 1. Thus, denoting

fi(x)

=

1 2

(Ai:

x

−

bi)2,

fi

is

1-smooth.

For

simplicity,

we

consider

SGD-star

with

uniform

serial

sampling, i.e. L = 1.

Next,

for

both

SGD-star

and

SGD

we

use

stepsize

γ

=

1 2

(theory

supported

stepsize

for

SGD-star),

while

for

SAGA

we

set

γ

=

1 5

(almost

theory

supported

stepsize).

Figure

2.2

shows

the

results.

Figure 2.2: Comparison of SGD-star, SGD and SAGA on least squares problem.
Note that, as theory predicts, SGD-star is always faster to SAGA, although only constant times. Further, in the cases where d ≥ n, performance of SGD seems identical to the performance of SGD-shift. This is due to a simple reason: if d ≥ n, we must have ∇fi(x∗) = 0 for all i, and
for the case when cost of evaluating ∇fi(x) is cheap, n is huge and n τ . In such case, cost of one iteration of SGD-MB is τ Cost(∇fi) + τ log(n) while the cost of one iteration of independent SGD is τ Cost(∇fi) + n.
54

thus SGD and SGD-shift are in fact identical algorithms.

2.7.3 Experiments on N-SEGA
In this experiment we study the eﬀect of noise on N-SEGA. We consider unit ball constrained least squares problem: min x ≤1 f (x) where f (x) = Ax − b 2. and we suppose that there is an oracle providing us with noised partial derivative gi(x, ζ) = ∇if (x) + ζ, where ζ ∼ N (0, σ2). For each problem instance (i.e. pair A, b), we compare performance of N-SEGA under various noise magnitudes σ2.
The speciﬁc problem instances are presented in Table 2.3. Figure 2.3 shows the results.

Type

A

b

1

Aij ∼ N (0, 1) (independently)

vector of ones

2

Same as 1, but scaled so that λmax(A A) = 1

vector of ones

3 Aij = ij j ∀i, j : ij, j ∼ N (0, 1) (independently) vector of ones

4

Same as 3, but scaled so that λmax(A A) = 1

vector of ones

Table 2.3: Four types of least squares.

We shall mention that this experiment serves to support and give a better intuition about the results from Section 2.6.8 and is by no means practical. The results show, as predicted by theory, linear convergence to a speciﬁc neighborhood of the objective. The eﬀect of the noise varies, however, as a general rule, the larger strong convexity µ is (i.e. problems 1,3 where scaling was not applied), the smaller the eﬀect of noise is.

Figure 2.3: N-SEGA applied on constrained least squares problem with noised partial derivative oracle. Legend labels stand for the magnitude σ2 of the oracle noise.
55

2.8 Discussion
Although our approach is rather general, we still see several possible directions for future extensions, including: • Recently, our results were extended to generally convex [90] and non-convex functions [89, 120]. • It would be further interesting to unify our theory with biased gradient estimators. If this was possible, one could recover methods as SAG [184] in special cases, or obtain rates for the zero-order optimization. We have some preliminary results in this direction already. • Although our theory allows for non-uniform stochasticity, it does not recover the best known rates for RCD type methods with importance sampling. It would be thus interesting to provide a more reﬁned analysis capable of capturing importance sampling phenomena more accurately. • An extension of Assumption 2.4.1 to iteration dependent parameters A, B, C, D1, D2, ρ would enable an array of new methods, such as SGD with decreasing stepsizes. Such an extension is rather very straightforward. • It would be interesting to provide a uniﬁed analysis of stochastic methods with acceleration and momentum. In fact, [108] provide (separately) a uniﬁcation of some methods with and without variance reduction. Hence, an attempt to combine our insights with their approach seems to be a promising starting point in these eﬀorts.
56

Chapter 3
Linearly Converging Error Compensated SGD

3.1 Introduction

We1 consider distributed optimization problems of the form

1n

min f (x) =

fi(x) ,

x∈Rd

n i=1

(3.1)

where n is the number of workers/devices/clients/nodes. The information about function fi is stored on the i-th worker only. Problems of this form appear in the distributed or federated training of supervised machine learning models [199, 100]. In such applications, x ∈ Rd describes the parameters identifying a statistical model we wish to train, and fi is the (generalization or empirical) loss of model x on the data accessible by worker i. If worker i has access to data with distribution Di, then fi is assumed to have the structure

fi(x) = Eξi∼Di [fξi (x)] .

(3.2)

Dataset Di may or may not be available to worker i in its entirety. Typically, we assume that worker i has only access to samples from Di. If the dataset is fully available, it is typically ﬁnite, in which case we can assume that fi has the ﬁnite-sum form2:

1m

fi(x) =

fij (x).

m j=1

(3.3)

Communication bottleneck. The key bottleneck in practical distributed [64] and federated [100, 83] systems comes from the high cost of communication of messages among the clients needed to ﬁnd a solution of suﬃcient qualities. Several approaches to addressing this communication bottleneck have been proposed in the literature.
In the very rare situation when it is possible to adjust the network architecture connecting the
1Part of the work was done while I was a research intern at KAUST. 2The implicit assumption that each worker contains exactly m data points is for simplicity only; all our results have direct analogues in the general setting with mi data points on worker i.

57

clients, one may consider a fully decentralized setup [19], and allow each client in each iteration to communicate to their neighbors only. One can argue that in some circumstances and in a certain sense, decentralized architecture may be preferable to centralized architectures [122]. Another natural way to address the communication bottleneck is to do more meaningful (which typically means more expensive) work on each client before each communication round. This is done in the hope that such extra work will produce more valuable messages to be communicated, which hopefully results in the need for fewer communication rounds. A popular technique of this type which is particularly relevant to Federated Learning is based in applying multiple local updates instead of a single update only. This is the main idea behind Local-SGD [210]; see also [15, 68, 86, 89, 97, 209, 229]. However, in this chapter, we contribute to the line work which aims to resolve the communication bottleneck issue via communication compression. That is, the information that is normally exchanged—be it iterates, gradients or some more sophisticated vectors/tensors—is compressed in a lossy manner before communication. By applying compression, fewer bits are transmitted in each communication round, and one hopes that the increase in the number of communication rounds necessary to solve the problem, if any, is compensated by the savings, leading to a more eﬃcient method overall.

Error-feedback framework. In order to address these issues, in this chapter we study a broad class of distributed stochastic ﬁrst order methods for solving problem (3.1) described by the iterative framework

xk+1 = xk − n1 n vik,
i=1
eki +1 = eki + γgik − vik,

i = 1, 2, . . . , n.

(3.4) (3.5)

In this scheme, xk represents the key iterate, vik is the contribution of worker i towards the

update in iteration k, gik is an unbiased estimator of ∇fi(xk) computed by worker i, γ > 0 is a

ﬁxed stepsize and eki is the error accumulated at node i prior to iteration k (we set to e0i = 0 for

all i). In order to better understand the role of the vectors vik and eki , ﬁrst consider the simple

special case with vik ≡ γgik. In this case, eki = 0 for all i and k, and method (3.4)–(3.5) reduces

to distributed SGD:

xk+1 = xk − nγ n gik.
i=1

(3.6)

However, by allowing to chose the vectors vik in a diﬀerent manner, we obtain a more general update rule than what the SGD update (3.6) can oﬀer. [209], who studied (3.4)–(3.5) in the n = 1 regime, show that this ﬂexibility allows to capture several types of methods, including those employing i) compressed communication, ii) delayed gradients, and iii) minibatch gradient updates. While our general results apply to all these special cases and more, in order to not dilute the focus of the chapter, in the main body of this chapter we concentrate on the ﬁrst use case—compressed communication—which we now describe.

Error-compensated compressed gradient methods. Note that in distributed SGD (3.6),

58

each

worker

needs

to

know

the

aggregate

gradient

gk

=

1 n

n i=1

gik

to

form

xk+1,

which

is

needed before the next iteration can start. This can be achieved, for example, by each worker

i communicating their gradient gik to all other workers. Alternatively, in a parameter server

setup, a dedicated master node collects the gradients from all workers, and broadcasts their

average gk to all workers. Instead of communicating the gradient vectors gik, which is expensive

in distributed learning in general and in federated learning in particular, and especially if d is

large, we wish to communicate other but closely related vectors which can be represented with

fewer bits. To this eﬀect, each worker i sends the vector

vik = C(eki + γgik), ∀i ∈ [n]

(3.7)

instead, where C : Rd → Rd is a (possibly randomized, and in such a case, drawn independently of all else in iteration k) compression operator used to reduce communication. We assume throughout that there exists δ ∈ (0, 1] such that the following inequality holds for all x ∈ Rd

E C(x) − x 2 ≤ (1 − δ) x 2.

(3.8)

For any k ≥ 0, the vector eki +1 =

k t=0

γgit

−

vit

captures

the

error

accumulated

by

worker

i

up

to iteration k. This is the diﬀerence between the ideal SGD update

k t=0

γ

git

and

the

applied

update

k t=0

vit.

As

we

see

in

(3.7),

at

iteration

k

the

current

error

eki

is

added

to

the

gradient

update γgik—this is referred to as error feedback—and subsequently compressed, which deﬁnes

the update vector vik. Compression introduces additional error, which is added to eki , and the

process is repeated.

Compression operators. For a rich collection of speciﬁc operators satisfying (3.8), we refer the reader to [209] and [20]. These include various unbiased or contractive sparsiﬁcation operators such as RandK and TopK, respectively, and quantization operators such as natural compression and natural dithering [78]. Several additional comments related to compression operators are included in Section B.2.

3.2 Summary of Contributions
We now summarize the key contributions of this chapter.
General theoretical framework. In this work we propose a general theoretical framework for analyzing a wide class of methods that can be written in the the error-feedback form (3.4)-(3.5). We perform complexity analysis under µ-strong quasi convexity (Assumption 3.3.1) and L-smoothness (Assumption 4.2.2) assumptions on the functions f and {fi}, respectively. Our analysis is based on an additional parametric assumption (using parameters A, A , B1, B1, B2, B2, C1, C2, D1, D1, D2, D3, η, ρ1, ρ2, F1, F2, G) on the relationship between the iterates xk, stochastic gradients gk, errors ek and a few other quantities (see Assumption 3.3.3, and the stronger Assumption 3.3.2). We prove a single theorem (Theorem 3.3.4) from which

59

all our complexity results follow as special cases. That is, for each existing or new speciﬁc method, we prove that one (or both) of our parametric assumptions holds, and specify the parameters for which it holds. These parameters have direct impact on the theoretical rate of the method. A summary of the values of the parameters for all methods developed in this chapter is provided in Table C.4 in the appendix. We remark that the values of the parameters A, A , B1, B1, B2, B2, C1, C2 and ρ1, ρ2 inﬂuence the theoretical stepsize.
Sharp rates. For existing methods covered by our general framework, our main convergence result (Theorem 3.3.4) recovers the best known rates for these methods up to constant factors.
Eight new error-compensated (EC) methods. We study several speciﬁc EC methods for solving problem (3.1). First, we recover the EC-SGD method ﬁrst analyzed in the n = 1 case by [209] and later in the general n ≥ 1 case by [20]. More importantly, we develop eight new methods: EC-SGDsr, EC-GDstar, EC-SGD-DIANA3, EC-SGDsr-DIANA, EC-GD-DIANA, EC-LSVRG, EC-LSVRGstar and EC-LSVRG-DIANA. Some of these methods are designed to work with the expectation structure of the local functions fi given in (3.2), and others are speciﬁcally designed to exploit the ﬁnite-sum structure (3.3). All these methods follow the error-feedback framework (3.4)–(3.5), with vik chosen as in (3.7). They diﬀer in how the gradient estimator gik is constructed (see Table 3.2 for a compact description of all these methods; formal descriptions can be found in the appendix). As we shall see, the existing EC-SGD method uses a rather naive gradient estimator, which renders it less eﬃcient in theory and practice when compared to the best of our new methods. A key property of our parametric assumption described above is that it allows for the construction and modeling of more elaborate gradient estimators, which leads to new EC methods with superior theoretical and practical properties when compared to prior state of the art.
3Inspired by personal communication with D. Kovalev in November 2019 who shared a key algorithm and preliminary results of our work, [207] studied almost the same algorithm and also other related methods and independently derived convergence rates. Our work was ﬁnalized and submitted to NeurIPS 2020 in June 2020, while the results in [207] were obtained in Summer 2020 and appeared on arXiv in September 2020. Moreover, in our work, we obtain tighter rates (see Table 3.1 for the details).
60

Table 3.1: Complexity of Error-Compensated SGD methods established in this chapter. Symbols:
ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for the variances of stochastic gradients of workers. EC-GDstar, EC-LSVRGstar and EC-LSVRG-DIANA are the ﬁrst EC methods with a linear convergence rate without assuming that ∇fi(x∗) = 0 for all i.
EC-LSVRGstar and EC-LSVRG-DIANA are the ﬁrst EC methods with a linear convergence rate which do not require the computation of the full gradient ∇fi(xk) by all workers in each iteration. Out of these three methods, only EC-LSVRG-DIANA is practical. †EC-GD-DIANA is a special case of EC-SGD-DIANA where each worker i computes the full gradient ∇fi(xk).

Problem Method (3.1)+(3.3) EC-SGDsr (3.1)+(3.2) EC-SGD (3.1)+(3.3) EC-GDstar (3.1)+(3.2) EC-SGD-DIANA

Alg # Citation Sec #

Alg 19

new

3.8.1

Alg 20 Alg 21

[209] new

3.8.2 3.8.3

Alg 22

new

3.8.4

(3.1)+(3.3) EC-SGDsr-DIANA Alg 23

(3.1)+(3.2) (3.1)+(3.3) (3.1)+(3.3) (3.1)+(3.3)

EC-GD-DIANA† EC-LSVRG EC-LSVRGstar EC-LSVRG-DIANA

Alg 22 Alg 24 Alg 25 Alg 26

new
new new new new

3.8.5
3.8.4 3.8.6 3.8.7 3.8.8

Rate (constants ignored)

O

√
L + L+ δLL +

σ∗2

+

µ

δµ

nµε

L(σ∗2 +ζ∗2/δ) √
µ δε

O κ + σ∗2 +
δ nµε

L(σ∗2 +ζ∗2/δ) √
δµ ε

Opt. I: O

O κδ log 1ε
√
ω + κ + σ2 + L√σ2
δ nµε δµ ε

Opt. II: Opt. I: O Opt. II: O

√

O 1+ω + κ + σ2 + √Lσ2

δ

δ √

nµε

µ √δε

ω + L + LL + σ∗2 + L√σ∗2

µ

δµ √

nµε δµ√ ε

1+ω + L + LL + σ∗2 + √Lσ∗2

δ

µ

δµ

nµε µ δε

O ω + κ log 1 δ √ε
O m + κ + L√ζ∗2
δ δµ ε

O m + κδ log 1ε O ω + m + κδ log 1ε

61

Table 3.2: Error compensated methods developed in this work. In all cases, vik = C(eki + γgik). The full descriptions of the algorithms are included in Section 3.8.

Problem (3.1) + (3.3)

Method EC-SGDsr

(3.1) + (3.2) (3.1)

EC-SGD EC-GDstar

(3.1) + (3.2) EC-SGD-DIANA

gik
m
m1 ξij ∇fij (xk)
j=1
∇fξi (xk) ∇fi(xk) − ∇fi(x∗)
gˆik − hki + hk

(3.1) + (3.3) EC-SGDsr-DIANA

∇fξk (xk) − hki + hk i

(3.1) + (3.3)

EC-LSVRG

(3.1) + (3.3) EC-LSVRGstar

∇fil(xk) − ∇fil(wik) + ∇fi(wik)
∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗)

(3.1) + (3.3) EC-LSVRG-DIANA

gˆik − hki + hk
where gˆik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

Comment
E [ξij ] = 1
EDi ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)
known ∇fi(x∗) ∀i
E gˆik = ∇fi(xk) Ek gˆik − ∇fi(xk) 2 ≤ D1,i
hki +1 = hki + αQ(gˆik − hki )
n
hk = n1 hki
i=1
E ∇fξk (xk) = ∇fi(xk) i
EDi ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)
hki +1 = hki + αQ(∇fξk (xk) − hki ) ni hk = n1 hki i=1 l chosen uniformly from [m] k+1 xk, with prob. p,
wi = wik, with prob. 1 − p
l chosen uniformly from [m] k+1 xk, with prob. p, wi = wik, with prob. 1 − p
hki +1 = hki + αQ(gˆik − hki )
n
hk = n1 hki
i=1
l chosen uniformly from [m] k+1 xk, with prob. p, wi = wik, with prob. 1 − p

First linearly converging EC methods. The key theoretical consequence of our gen-

eral framework is the development of the ﬁrst linearly converging error-compensated SGD-type

methods for distributed training with biased communication compression. In particular, we

design four such methods: two simple but impractical methods, EC-GDstar and EC-LSVRGstar,

with rates O

κ δ

ln

1 ε

and O

m

+

κ δ

ln

1 ε

, respectively, and two practical but more elab-

orate methods, EC-GD-DIANA, with rate O

ω

+

κ δ

ln

1 ε

,

and

EC-LSVRG-DIANA,

with

rate

O

ω

+

m

+

κ δ

ln

1 ε

. In these rates, κ = L/µ is the condition number, 0 < δ ≤ 1 is the

contraction parameter associated with the compressor C used in (3.7), and ω is the variance

62

parameter associated with a secondary unbiased compressor4 Q which plays a key role in the construction of the gradient estimator gik. The complexity of the ﬁrst and third methods does not depend on m as they require the computation of the full gradient ∇fi(xk) for each i. The remaining two methods only need to compute O(1) stochastic gradients ∇fij(xk) on each worker i.
The ﬁrst two methods, while impractical, provided us with the intuition which enabled us to develop the practical variant. We include them in this chapter due to their simplicity, because of the added insights they oﬀer, and to showcase the ﬂexibility of our general theoretical framework, which is able to describe them. EC-GDstar and EC-LSVRGstar are impractical since they require the knowledge of the gradients {∇fi(x∗)}, where x∗ is an optimal solution of (3.1), which are obviously not known since x∗ is not known.
The only known linear convergence result for an error compensated SGD method is due to [20], who require the computation of the full gradient of fi by each machine i (i.e., m stochastic gradients), and the additional assumption that ∇fi(x∗) = 0 for all i. We do not need such assumptions, thereby resolving a major theoretical issue with EC methods.
Results in the convex case. Our theoretical analysis goes beyond distributed optimization and recovers the results from [55, 90] (without regularization) in the special case when vik ≡ γgik. As we have seen, in this case eki ≡ 0 for all i and k, and the error-feedback framework (3.4)–(3.5) reduces to distributed SGD (3.6). In this regime, the relation (3.17) in Assumption 3.3.3 becomes void, while relations (3.13) and (3.14) with σ22,k ≡ 0 are precisely those used by [55] to analyze a wide array of SGD methods, including vanilla SGD [182], SGD with arbitrary sampling [63], as well as variance reduced methods such as SAGA [35], SVRG [82], LSVRG [77, 103], JacSketch [62], SEGA [69] and DIANA [139, 79]. Our theorem recovers the rates of all the methods just listed in both the convex case µ = 0 [90] and the strongly-convex case µ > 0 [55] under the more general Assumption 3.3.3.
DIANA with bi-directional quantization. To illustrate how our framework can be used even in the case when vik ≡ γgik, eki ≡ 0, we develop analyze a new version of DIANA called DIANAsr-DQ that uses arbitrary sampling on every node and double quantization5, i.e., unbiased compression not only on the workers’ side but also on the master’s one.
Methods with delayed updates. Following [206], we also show that our approach covers SGD with delayed updates [1, 11, 43] (D-SGD), and our analysis shows the best-known rate for this method. Due to the ﬂexibility of our framework, we are able develop several new variants of D-SGD with and without quantization, variance reduction, and arbitrary sampling. Again,
4We assume that EQ(x) = x and E Q(x) − x 2 ≤ ω x 2 for all x ∈ Rd. 5In the concurrent work (which appeared on arXiv after we have submitted our paper to NeurIPS) a similar method was independently proposed under the name of Artemis [165]. However, our analysis is more general, see all the details on this method in the appendix. This footnote was added to the paper during the preparation of the camera-ready version of our paper.
63

due to space limitations, we put these methods together with their convergence analyses in the appendix.

3.3 Main Result

In this section we present the main theoretical result of our chapter. First, we introduce our assumption on f , which is a relaxation of µ-strong convexity (see also Assumption 2.4.2).

Assumption 3.3.1 (µ-strong quasi-convexity). Assume that function f has a unique minimizer x∗. We say that function f is strongly quasi-convex with parameter µ ≥ 0 if for all x ∈ Rd

f (x∗) ≥ f (x) + ∇f (x), x∗ − x + µ x − x∗ 2. 2

(3.9)

We allow µ to be zero, in which case f is sometimes called weakly quasi-convex (see [206] and references therein).

We now introduce our key parametric assumption on the stochastic gradient gk. This is a generalization of the assumption introduced by [55] for the particular class of methods described covered by the EF framework (3.4)–(3.5).

Assumption 3.3.2. For all k ≥ 0, the stochastic gradient gk is an average of stochastic gradients gik such that

gk = n1 n gik,
i=1

E gk | xk = ∇f (xk).

(3.10)

Moreover, there exist constants A, A, A , B1, B2, B1, B2, B1, B2, C1, C2, G, D1, D1, D1, D2, D3 ≥ 0, and ρ1, ρ2 ∈ [0, 1] and two sequences of (probably random) variables {σ1,k}k≥0 and {σ2,k}k≥0, such that the following recursions hold:

1n nE
i=1

1 n g¯k 2 ≤ 2A(f (xk) − f (x∗)) + B σ2 + B σ2 + D ,

n

i

1 1,k

2 2,k

1

i=1

gk − g¯k 2 | xk

i

i

≤ 2A(f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1,

E gk 2 | xk ≤ 2A (f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1,

(3.11) (3.12) (3.13)

E σ12,k+1 | σ12,k, σ22,k ≤ (1 − ρ1)σ12,k + 2C1 f (xk) − f (x∗) + Gρ1σ22,k + D2,(3.14)

E σ22,k+1 | σ22,k ≤ (1 − ρ2)σ22,k + 2C2 f (xk) − f (x∗) ,

(3.15)

where g¯ik = E gik | xk .

Let us brieﬂy explain the intuition behind the assumption and the meaning of the introduced parameters. First of all, we assume that the stochastic gradient at iteration k is conditionally

64

unbiased estimator of ∇f (xk), which is a natural and commonly used assumption on the stochastic gradient in the literature. However, we explicitly do not require unbiasedness of gik, which is very useful in some special cases. Secondly, let us consider the simplest special case when gk ≡ ∇f (xk) and f1 = . . . = fn = f , i.e., there is no stochasticity/randomness in the method and the workers have the same functions. Then due to ∇f (x∗) = 0, we have that
(A.4)
∇f (xk) 2 ≤ 2L(f (xk) − f (x∗)),

which implies that Assumption 3.3.2 holds in this case with A = A = L, A = 0 and B1 = B2 = B1 = B2 = B1 = B2 = C1 = C2 = D1 = D1 = D1 = D2 = 0, ρ = 1, σ12,k ≡ σ22,k ≡ 0.
In general, if gk satisﬁes Assumption 3.3.3, then parameters A, A and A are usually connected with the smoothness properties of f and typically they are just multiples of L, whereas terms B1σ12,k, B2σ22,k, B1σ12,k, B2σ22,k, B1σ12,k, B2σ22,k and D1, D1, D1 appear due to the stochastic nature of gik. Moreover, {σ12,k}k≥0 and {σ22,k}k≥0 are sequences connected with variance reduction processes and for the methods; without any kind of variance reduction these sequences contains only zeros. Parameters B1 and B2 are often 0 or small positive constants, e.g., B1 = B2 = 2, and D1 characterizes the remaining variance in the estimator gk that is not included in the ﬁrst two terms.
Inequalities (3.14) and (3.15) describe the variance reduction processes: one can interpret ρ1 and ρ2 as the rates of the variance reduction processes, 2C1(f (xk) − f (x∗)) and 2C2(f (xk) − f (x∗)) are “optimization” terms and, similarly to D1, D2 represents the remaining variance that is not included in the ﬁrst two terms. Typically, σ12,k controls the variance coming from compression and σ22,k controls the variance taking its origin in ﬁnite-sum type randomization (i.e., subsampling) by each worker. In the case ρ1 = 1 we assume that B1 = B1 = C1 = G = 0, D2 = 0 (for ρ2 = 1 analogously), since inequality (3.14) becomes superﬂuous.
However, in our main result we need a slightly diﬀerent assumption.

Assumption 3.3.3. For all k ≥ 0, the stochastic gradient gk is an unbiased estimator of

∇f (xk):

E gk | xk = ∇f (xk).

(3.16)

Moreover, there exist non-negative constants A , B1, B2, C1, C2, F1, F2, G, D1, D2, D3 ≥ 0, ρ1, ρ2 ∈ [0, 1] and two sequences of (probably random) variables {σ1,k}k≥0 and {σ2,k}k≥0 such that inequalities (3.13), (3.14) and (3.15) hold and

3L K wkE ek 2 ≤ 14 K wkE f (xk) − f (x∗) + F1σ12,0 + F2σ22,0 + γD3WK (3.17)

k=0

k=0

65

for

all

k, K

≥

0,

where

ek

=

1 n

n i=1

eki

and

{WK }K≥0

and

{wk }k≥0

are

deﬁned

as

WK = K wk, wk = (1 − η)−(k+1), η = min γ2µ , ρ41 , ρ42 .
k=0

(3.18)

This assumption is more ﬂexible than Assumption 3.3.2 and helps us to obtain a uniﬁed analysis of all methods falling in the error-feedback framework. We emphasize that in this assumption we do not assume that (3.11) and (3.12) hold explicitly. Instead of this, we introduce inequality (3.17), which is the key tool that helps us to analyze the eﬀect of error-feedback and comes from the analysis from [209] with needed adaptations connected with the ﬁrst three inequalities. As we show in the appendix, this inequality can be derived for SGD with error compensation and delayed updates under Assumption 3.3.2 and, in particular, using (3.11) and (3.12). As before, D3 hides a variance that is not handled by variance reduction processes and F1 and F2 are some constants that typically depend on L, B1, B2, ρ1, ρ2 and γ.

We now proceed to stating our main theorem.

Theorem 3.3.4. Let Assumptions 3.3.1, 4.2.2 and 3.3.3 be satisﬁed and γ ≤ 1/4(A +C1M1+C2M2). Then for all K ≥ 0 we have
E f (x¯K ) − f (x∗) ≤ (1 − η)K 4(T 0 + γF1σγ12,0 + γF2σ22,0) + 4γ D1 + M1D2 + D3 (3.19)

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF1σγ1K2,0 + γF2σ22,0) + 4γ D1 + M1D2 + D3

(3.20)

when µ = 0, where η = min {γµ/2, ρ1/4, ρ2/4}, T k d=ef M1 = 43Bρ11 , M2 = 4(B23+ρ234 G) .

x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

All the complexity results summarized in Table 3.1 follow from this theorem; the detailed proofs of the main results are included in the appendix. Furthermore, in the appendix we include similar results but for methods employing delayed updates.

3.4 Further Notation

In what follows it will be useful to denote

vk d=ef n1 vik,
i

gk d=ef n1 gik,
i

ek d=ef n1 eki .
i

66

By aggregating identities (3.5) across all i, we get ek+1 = ek + γgk − vk. In our proofs we also use the perturbed iterates technique [112, 131] based on the analysis of the following sequence

x˜k = xk − ek.

(3.21)

This sequence satisﬁes very useful for the analysis relation: x˜k+1 (3=.21) xk+1 − ek+1 (3.4)=,(3.5) xk − vk − (ek + γgk − vk) = xk − ek − γgk (3=.21) x˜k − γgk. (3.22)

3.5 SGD as a Special Case
In this section we want to show that our approach is general enough to cover many existing methods of SGD type. Consider the following situation:

vk = γgk, e0 = 0.

(3.23)

It implies that ek = 0 for all k ≥ 0 and the updates rules (3.4)-(3.5) gives us a simple SGD:

xk+1 = xk − γgk.

(3.24)

The following lemma formally shows that SGD under general enough assumptions satisﬁes Assumption 3.3.3.
Lemma 3.5.1. Let Assumptions 3.3.1 and 4.2.2 be satisﬁes and inequalities (3.16), (3.13), (3.14) and (3.15) hold. Then for the method (3.24) inequality (3.17) holds with F1 = F2 = 0 and D3 = 0 for all k ≥ 0.

Proof. Since ek = 0 and f (xk) ≥ f (x∗) for all k ≥ 0 we get

3L K wkE ek 2 = 0 ≤ 14 K wkE f (xk) − f (x∗)

k=0

k=0

which concludes the proof.

It implies that all methods considered in Chapter 2 ﬁt our framework. Moreover, using Theorem 3.3.4 we derive the following result.

Theorem 3.5.2. Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, inequalities (3.16), (3.13),

(3.14), (3.15) hold and γ ≤ 1/4(A +C1M1+C2M2). Then for the method (3.24) for all K ≥ 0 we

have

E f (x¯K ) − f (x∗) ≤ 1 − min γµ , ρ1 , ρ2 2 44

K 4T 0 γ + 4γ D1 + M1D2 ,

67

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4γTK0 + 4γ D1 + M1D2

when µ = 0, where T k d=ef xk − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and M1 = 43Bρ11 , M2 = 4(B23+ρ234 G) .

In particular, if σ22,k ≡ 0, then our assumption coincides with the key assumption from [55] and our theorem recovers the same rates as in [55] when µ > 0. The case when µ = 0 was not considered in [55], while in our analysis we get it for free.

3.6 Special Cases: SGD
To illustrate the generality of our approach, we develop and analyse a new special case of SGD without error-feedback and show that in some cases, our framework recovers tighter rates than the framework from [55].

3.6.1 DIANA with Arbitrary Sampling and Double Quantization

In this section we consider problem (3.1) with f (x) being µ-quasi strongly convex and fi(x) satisfying (3.3) where functions fij(x) are diﬀerentiable, but not necessary convex. Following [63] we construct a stochastic reformulation of this problem:

f (x) = ED [fξ(x)] ,

1n fξ(x) = n fξi(x),
i=1

1m fξi (x) = m ξijfij(x),
j=1

(3.25)

where ξ = (ξ1 , . . . , ξn ), ξi = (ξi1, . . . , ξim) is a random vector with distribution Di such that EDi[ξij] = 1 for all i ∈ [n], j ∈ [m] and the following assumption holds.

Assumption 3.6.1 (Expected smoothness). We assume that functions f1, . . . , fn are L-smooth

in expectation w.r.t. distributions D1, . . . , Dn, i.e., there exists constant L = L(f, D1, . . . , Dn)

such that

EDi ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)

(3.26)

for all i ∈ [n] and x ∈ Rd.

To solve this problem, we consider DIANA [139, 79] — a distributed stochastic method using unbiased compressions or quantizations for communication between workers and master. We start with the formal deﬁnition of quantization. In [139, 79] DIANA was analyzed under the assumption that stochastic gradients gik have uniformly bounded variances which is not very practical.
Therefore, we consider a slightly diﬀerent method called DIANAsr-DQ which works with the stochastic reformulation (3.25) of problem (3.1)+(3.3), see Algorithm 17. Moreover, to illustrate the ﬂexibility of our approach, we consider compression not only on the workers’ side but also on the master side. To perform an update of DIANAsr-DQ master needs to gather quantized

68

Algorithm 17 DIANAsr with Double Compression (DIANAsr-DQ)

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1:

Set

h0

=

1 n

n i=1

h0i

2: for k = 0, 1, . . . do

3: Broadcast gk−1 to all workers

If k = 0, then broadcast x0

4: for i = 1, . . . , n in parallel do

5:

xk = xk−1 − γgk−1

Ignore this line if k = 0

6:

Sample gik,1 = ∇fξk (xk) satisfying Assumption 3.6.1 independtently from other

workers

i

7:

∆ˆ ki = gik,1 − hki

8:

Sample ∆ki ∼ Q1(∆ˆ ki ) indepently from other workers

9:

gik,2 = hki + ∆ki

10:

hki +1 = hki + α∆ki

11: end for

12:

gk,2

=

1 n

n i=1

gik,2

=

hk

+

1 n

n i=1

∆ki

n

n

13:

hk+1

=

1 n

hki +1 = hk + α n1

∆ki

i=1

i=1

14: Sample gk ∼ Q2(gk,2)

15: xk+1 = xk − γgk−1

16: end for

gradient diﬀerences ∆ki and the to broadcast quantized stochastic gradient gk to all workers. Clearly, in this case, only compressed vectors participate in communication.
In the concurrent work [165] the same method was independently proposed under the name of Artemis. However, our analysis is slightly more general: it is based on Assumption 3.6.1 while in [165] authors assume L-cocoercivity of stochastic gradients almost surely. Next, a very similar approach was considered in [216], where authors present a method with error compensation on master and worker sides. Moreover, recently another method called DORE was developed in [127], which uses DIANA-trick on the worker side and error compensation on the master side. However, in these methods, compression operators are the same on both sides, despite the fact that gathering the information often costs much more than broadcasting. Therefore, the natural idea is in using diﬀerent quantization for gathering and broadcasting, and it is what DIANAsr-DQ does. Moreover, we do not assume uniform boundedness of the second moment of the stochastic gradient like in [216], and we also do not assume uniform boundedness of the variance of the stochastic gradient like in [127]. Assumption 3.6.1 is more natural and always holds for the problems (3.1)+(3.3) when fij are convex and L-smooth for each i ∈ [n], j ∈ [m]. In contrast, in the same setup, there exist such problems that the variance of the stochastic gradients is not uniformly upper bounded by any ﬁnite constant.
We assume that Q1 and Q2 satisfy (A.6) with parameters ω1 and ω2 respectively.

69

Lemma 3.6.2. Let Assumption 3.6.1 be satisﬁed. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), ≤ 2L(1 + ω2) 2 + 3ω1
n

f (xk) − f (x∗)

(3.27) + 3ω1(1n+ ω2) σk2 + D1,(3.28)

where

σk2

=

1 n

n hk − ∇f (x∗) 2 and D = (2+3ω1)(1+ω2) n ED

i=1 i

1

n2

i

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Proof. First of all, we show inbiasedness of gk:

E gk | xk

(A.15=),(A.6) (A.15=),(A.6)
=

E gk,2 | xk = hk + n1 n E ∆ki | xk
i=1

hk + n1 n E ∆ˆ ki | xk
i=1

hk + 1 n n i=1

∇fi(xk) − hki

= ∇f (xk).

Next, to denote mathematical expectation w.r.t. the randomness coming from quantizations

Q1 and Q2 at iteration k we use EQk [·] and EQk [·] respectively. Using these notations and the

1

2

deﬁnition of quantization we derive

EQk [ gk 2] 2

(A.14=),(A.6)
(A.6)
≤

gk,1 2 + EQk gk,2 − gk,1 2 2
(1 + ω2) gk,1 2.

Taking the conditopnal mathematical expectation EQk [·] from the both sides of previous inequality 1

70

and

using

the

independence

of

∆1i

,

.

.

.

,

∆

n i

we

get

EQk1 ,Qk2 gk 2

(A=.15) (A=.14)
=
(A.11),(A.6)
≤
(A.11)
≤

(1 + ω2)EQk 1

gk,1 2

 = (1 + ω2)EQk 
1

1n k

2
k

n (hi + ∆i ) 

i=1

1n (1 + ω2)
n

hki + ∆ˆ ki

i=1

2 + (1 + ω2)EQk1  n1 n (∆ki − ∆ˆ ki ) 2
i=1

1n

k

∗

2

∗

∗

(1 + ω2) n

∇fξk (x ) − ∇fξk (x ) + ∇fξk (x ) − ∇fi(x )

i

i

i

i=1

(1 + ω2) n

+ n2

EQk1

i=1

∆ki − ∆ˆ ki 2

2(1 + ω2) n n

∇fξk (xk) − ∇fξk (x∗) 2

i

i

i=1

1n

∗

2 ∗

+2(1 + ω2) n

∇fξk (x ) − ∇fi(x ) i

i=1

ω1(1 + ω2) n + n2

∇fξk (xk) − hki 2 i

i=1

2(1 + ω2) n n

∇fξk (xk) − ∇fξk (x∗) 2

i

i

i=1

1n

∗

2 ∗

+2(1 + ω2) n

∇fξk (x ) − ∇fi(x ) i

i=1

3ω1(1 + ω2) n + n2

∇fξk (xk) − ∇fξk (x∗) 2

i

i

i=1

3ω1(1 + ω2) n + n2

∇fξk (x∗) − ∇fi(x∗) 2 i

i=1

3ω1(1 + ω2) n + n2

hki − ∇fi(x∗) 2.

i=1

Finally, we take conditional mathematical expectation E[· | xk] from the both sides of the

71

inequality above and use the independece of ξ1k, . . . , ξnk:

E gk 2 | xk

(3.26)
≤
=

2L(1 + ω2) 2 + 3nω1 (f (xk) − f (x∗)) + 3ω1(1n+ ω2) σk2



1n +2(1 + ω2)E 
n

∇fξk (x∗) − ∇fi(x∗) i

i=1

2 | xk

3ω1(1 + ω2) n

+ n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2

2L(1 + ω2) 2 + 3nω1 (f (xk) − f (x∗)) + 3ω1(1n+ ω2) σk2

(1 + ω2)(2 + 3ω1) n

+

n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Lemma 3.6.3. Let fi be convex and L-smooth, Assumption 3.6.1 holds and α ≤ 1/(ω1+1). Then, for all k ≥ 0 we have

E σk2+1 | xk ≤ (1 − α)σk2 + 2α(3L + 4L)(f (xk) − f (x∗)) + D2,

(3.29)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D2

=

3α n

n
i=1 EDi

∇fξi (x∗) − ∇fi(x∗) 2 .

Proof. For simplicity, we introduce new notation: h∗i d=ef ∇fi(x∗). Using this we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

= E hki − h∗i + α∆ki 2 | xk

(A=.6)
(A.6),(A.15)
≤

hki − h∗i

2

+

2α

hki

−

h

∗ i

,

∇

fi

(

x

k

)

−

hki

+ α2E

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω1 + 1)E ∇fξk (xk) − hki 2 | xk .
i

∆ki 2 | xk

72

Using variance decomposition (A.14) and α ≤ 1/(ω1+1) we get

α2(ω1 + 1)EDi ∇fξk (xk) − hki 2 i
Putting all together we obtain

(A=.14)
(A.11)
≤
(A.4),(3.26)
≤

α2(ω1 + 1)EDi ∇fξk (xk) − ∇fi(xk) 2 i +α2(ω1 + 1) ∇fi(xk) − hki 2

3αEDi ∇fξk (xk) − ∇fξk (x∗) 2

i

i

+3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 i
+3α ∇fi(xk) − ∇fi(x∗) 2

+α ∇fi(xk) − hki 2

6α(L + L)Dfi(xk, x∗) + α ∇fi(xk) − hki 2 +3αEDi ∇fξk (x∗) − ∇fi(x∗) 2
i

E hki +1 − h∗i 2 | xk

≤
(A=.8)
(A.4)
≤

hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i +6α(L + L)Dfi (xk, x∗) + 3αEDi ∇fξk (x∗) − ∇fi(x∗) 2
i
hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2 +6α(L + L)Dfi (xk, x∗) + 3αEDi ∇fξk (x∗) − ∇fi(x∗) 2
i
(1 − α) hki − h∗i 2 + α(6L + 8L)Dfi(xk, x∗) +3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 .
i

Summing up the above inequality for i = 1, . . . , n we derive

1n nE
i=1

hki +1 − h∗i 2 | xk

≤ 1 −n α n hki − h∗i 2 + α(6L + 8L)(f (xk) − f (x∗))
i=1

3α n

+

EDi

n i=1

∇fξk (x∗) − ∇fi(x∗) 2 . i

Theorem 3.6.4. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n, f (x) is µ-quasi strongly convex and Assumption 3.6.1 holds. Then DIANAsr-DQ satisﬁes Assumption 3.3.3

73

with

A = L(1 + ω2) 2 + 3nω1 , B1 = 3ω1(1n+ ω2) ,

(2 + 3ω1)(1 + ω2) n

D1 =

n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k = σk2 = n1 n hki − ∇fi(x∗) 2,
i=1

B2 = 0,

σ22,k ≡ 0,

ρ1 = α,

ρ2 = 1,

C1 = α(3L + 4L),

C2 = 0,

3α n

D2 =

EDi

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

G = 0, F1 = F2 = 0, D3 = 0,

with γ and α satisfying

γ≤ 1 ,

4(1 + ω2)

L

2

+

15ω1 n

+

16Lω1 n

α≤ 1 , ω+1

M1 = 4ω1(1 + ω2) , nα

M2 = 0

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4T 0 γ + 4γ D1 + M1D2 ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4γTK0 + 4γ D1 + M1D2

when µ = 0, where T k d=ef xk − x∗ 2 + M1γ2σ12,k.

In other words, if

γ= 1 ,

4(1 + ω2)

L

2

+

15ω1 n

+

16Lω1 n

α= 1 ω+1

and D1 = 0, i.e., ∇fξk (xk) = ∇fi(xk) almost surely, DIANAsr-DQ converges with the linear rate i

O ω1 + L (1 + ω2) 1 + ω1 ln 1

µ

n

ε

to the exact solution. Applying Lemma A.5.3 we establish the rate of convergence to εsolution.
Corollary 3.6.5. Let the assumptions of Theorem 3.6.4 hold and µ > 0. Then after K

74

iterations of DIANAsq-DQ with the stepsize

γ0 = 1

4(1 + ω2)

L

2

+

15ω1 n

+

16Lω1 n

 ln max 2, µ2K2( xD0−1x+∗M21+DM2 1γ02σ12,0)

γ = min γ0,



µK









 
,

M1 = 4ω1(1 + ω2)



nα





and

α

=

1 ω+1

we

have

E f (x¯K ) − f (x∗) = O A x0 − x∗ 2 exp − min µ , 1 K + D1 + M1D2 .

A ω1

µK

That is, to achive E f (x¯K) − f (x∗) ≤ ε DIANAsq-DQ requires

O ω1 + L 1 + ωn1µ (1 + ω2) + (1 + ωn12)µ(1ε+ ω2) n EDi ∇fξi (x∗) − ∇fi(x∗) 2
i=1
Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

iterations.

Corollary 3.6.6. Let the assumptions of Theorem 3.6.4 hold and µ = 0. Then after K iterations of DIANAsq-DQ with the stepsize

γ0 = 1

4(1 + ω2)

L

2

+

15ω1 n

+

16Lω1 n

γ = min γ0,

x0 − x∗ 2 ,

x0 − x∗ 2 , M1 = 4ω1(1 + ω2)

M1σ12,0

(D1 + M1D2)K

nα

and

α

=

1 ω+1

we

have

E

f (x¯K ) − f (x∗)

of order

 O  LR02(1 + ω2) 1 + ωn1
K

√ R0σ1,0(1 + ω1) 1 + ω2

R0

+ √nK +



(1 + ω1)(1 + ω2)Dopt

√



nK

n

where R0 =

x0 − x∗

2, Dopt =

1 n

EDi ∇fξi(x∗) − ∇fi(x∗) 2. That is, to achive

i=1

E f (x¯K) − f (x∗) ≤ ε DIANAsq-DQ requires

LR02(1 + ω2) 1 + ω1

√ R0σ1,0(1 + ω1) 1 + ω2

R02(1 + ω1)(1 + ω2)Dopt

O

n+

√

+

ε

nε

nε2

iterations.

75

Algorithm 18 VR-DIANA based on LSVRG (Variant 1), SAGA (Variant 2), [79]

Input:

learning

rates

α>0

and

γ

> 0,

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

,

h0

=

1 n

1: for k = 0, 1, . . . do

n i=1

h0i

2:

Sample random uk =

1,

with

probability

1 m

0,

with

probability

1

−

1 m

3: Broadcast xk, uk to all workers

only for Variant 1

4: for i = 1, . . . , n in parallel do

5:

Pick jik uniformly at random from [m]

m

6:

µki

=

1 m

∇fij (wikj )

j=1

Worker side

7: gik = ∇fijik (xk) − ∇fijik (wikjik ) + µki

8:

∆ˆ ki = Q(gik − hki )

9:

hki +1 = hki + α∆ˆ ki

10:

for j = 1, . . . , m do

11:

wk+1 = xk, if uk = 1 Variant 1 (L-SVRG): update epoch gradient if uk = 1

ij

wikj, if uk = 0

12:

wk+1 = xk, j = jik

ij

wikj , j = jik

13:

end for

Variant 2 (SAGA): update gradient table

14: end for n

15:

hk

+1

=

hk

+

α n

∆ˆ ki

i=1

16: gk = 1 n (∆ˆ k + hk)

n

i

i

i=1

17: xk+1 = xk − γgk

Gather quantized updates

18: end for

3.6.2 Recovering Tight Complexity Bounds for VR-DIANA
In this section we consider the same problem (3.1)+(3.3) and variance reduced version of DIANA called VR-DIANA [79], see Algorithm 18. For simplicity we assume that each fij is convex and L-smooth and fi is additionally µ-strongly convex.
Lemma 3.6.7 (Lemmas 3, 5, 6 and 7 from [79]). Let α ≤ ω+1 1 . Then for all iterates k ≥ 0 of Algorithm 18 the following inequalities hold:

E gk | xk E Hk+1 | xk E Dk+1 | xk E gk 2 | xk

= ∇f (xk),

(3.30)

≤ (1 − α) Hk + 2α Dk + 8αLn f (xk) − f (x∗) , m
≤ 1 − 1 Dk + 2Ln f (xk) − f (x∗) , m

(3.31) (3.32)

≤ 2L 1 + 4ω + 2 n

f (xk) − f (x∗) + 2ω Dk + 2(ω + 1) Hk, (3.33)

n2 m

n2

76

n

nm

where Hk = hki − ∇fi(x∗) 2 and Dk =

∇fij(wikj) − ∇fij(x∗) 2.

i=1

i=1 j=1

This lemma shows that VR-DIANA satisﬁes (3.13), (3.14) and (3.15). Applying Theorem 3.5.2 we get the following result.

Theorem 3.6.8. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n and fi(x) is µ-strongly convex for all i = 1, . . . , n. Then VR-DIANA satisﬁes Assumption 3.3.3 with

4ω + 2

2(ω + 1)

A = L 1 + n , B1 = n , D1 = 0,

σ12,k = Hk = n1 n hki − ∇fi(x∗) 2,
i=1

2ω

B2 =

, n

σ22,k = Dk = n1m n m ∇fij (wikj ) − ∇fij (x∗) 2,
i=1 j=1

ρ1 = α,

ρ2 = 1 , m

C1 = 4αL, C2 = L , D2 = 0, G = 2, F1 = F2 = 0, D3 = 0, m

with γ and α satisfying

γ≤ 3 ,

L

41 3

+

52ω+35 n

α≤ 1 , ω+1

8(ω + 1) M1 = 3nα ,

8ωm 32m M2 = 3n + 9

and for all K ≥ 0

E f (x¯K) − f (x∗) ≤ 1 − min γµ , α , 1 2 4 4m

K 4T 0 ,
γ

when µ > 0 and

E

f (x¯K ) − f (x∗)

4T 0 ≤

γK

when µ = 0, where T k d=ef xk − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k.

In other words, if µ > 0 and

γ= 3 ,

L

41 3

+

52ω+35 n

α= 1 , ω+1

then VR-DIANA converges with the linear rate

O ω + m + κ 1 + ω ln 1

n

ε

to the exact solution which coincides with the rate obtained in [79]. We notice that the framework from [55] establishes slightly worse guarantee:

O ω + m + κ 1 + ω max{m, ω + 1} ln 1

n

m

ε

77

This guarantee is strictly worse than our bound when m ≤ 1 + ω. The key tool that helps us to improve the rate is two sequences of {σ12,k}k≥0, {σ22,k}k≥0 instead of one sequence {σk2}k≥0 as in [55].

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary 3.6.9. Let the assumptions of Theorem 3.6.8 hold and µ = 0. Then after K iterations of VR-DIANA with the stepsize

γ0 = 3

L

41 3

+

52ω+35 n

x0 − x∗ 2

8(ω + 1)

8ωm 32m

γ = min γ0, M1σ2 + M2σ2 , M1 = 3nα , M2 = 3n + 9

1,0

2,0

and

α

=

1 ω+1

we

have

E

f (x¯K ) − f (x∗)

of order

 O  LR02 1 + ωn
K

R0 +

(1+ω)2 n

σ12,0

+

K

1

+

ω n

 mσ22,0


where R0 = x0 − x∗ 2. That is, to achive E f (x¯K) − f (x∗) ≤ ε VR-DIANA requires

 O  LR02 1 + ωn
ε

R0 +

(1+ω)2 n

σ12,0

+

ε

1

+

ω n

 mσ22,0


iterations.

3.7 Distributed SGD with Compression and Error Compensation

In this section we consider the scenario when compression and error-feedback is applied in order to reduce the communication cost of the method, i.e., we consider SGD with error compensation and compression (EC-SGD) which has updates of the form (3.4)-(3.5) with

gk = n1 n gik
i=1

vk =

1n k n vi ,
i=1

vik = C(eki + γgik)

ek =

1n k n ei ,
i=1

eki +1 = eki + γgik − vik = eki + γgik − C(eki + γgik).

(3.34) (3.35)

Moreover, we assume that e0i = 0 for i = 1, . . . , n.

78

Lemma 3.7.1. Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, Assumption 3.3.2 holds anda













 

δ

δ

 

γ ≤ min 4µ ,

, 2C2 2B2 +B2

 96L 2δA + A + 1−2ρ1 Cρ11 + ρ22(G1−Cρ22) 2Bδ 1 + B1 + ρ2(1δ−ρ2) 

(3.36) where M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) . Then EC-SGD satisﬁes Assumption 3.3.3, i.e., inequality
(3.17) holds with the following parameters:

24Lγ2 F1 = δρ1(1 − η)

2B1 + B1 , δ D3 = 6Lγ δ

24Lγ2 F2 = δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 δ

D2 2B1 + B1 + 2D1 + D1 .

ρ1 δ

δ

+ 2B2 + B2 , δ (3.37)
(3.38)

aWhen ρ1 = 1 and ρ2 = 1 one can always set the parameters in such a way that B1 = B1 = B2 = B2 =

C1 = C2 = 0, D2 = 0. In this case we assume that 1−2ρ1 Cρ11 + ρ22(G1−Cρ22)

2Bδ 1 + B1

+ 2C2 2Bδ 2 +B2 ρ2 (1−ρ2 )

= 0.

That is, Assumption 3.3.2 implies Assumption 3.3.3 in the case of error compensation. As a direct application of Lemma 3.7.1 and Theorem 3.3.4 we get the following result.

Theorem 3.7.2. Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, Assumption 3.3.2 holds and

γ≤

1,

4(A + C1M1 + C2M2)













 

δ

δ

 

γ ≤ min 4µ ,

, 2C2 2B2 +B2

 96L 2δA + A + 1−2ρ1 Cρ11 + ρ22(G1−Cρ22) 2Bδ 1 + B1 + ρ2(1δ−ρ2) 

where M1 = 43Bρ11 and M2 = 4(B23+ρ234 G) . Then for all K ≥ 0 we have E f (x¯K ) − f (x∗) ≤ (1 − η)K 4(T 0 + γF1σγ12,0 + γF2σ22,0) + 4γ D1 + M1D2 + D3 ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF1σγ1K2,0 + γF2σ22,0) + 4γ D1 + M1D2 + D3

when µ = 0, where η = min {γµ/2, ρ1/4, ρ2/4}, T k d=ef x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

24Lγ2 F1 = δρ1(1 − η)

2B1 + B1 , δ

24Lγ2 F2 = δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 + 2B2 + B2 ,

δ

δ

79

Algorithm 19 EC-SGDsr

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi(xk)

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

10: xk+1 = xk − vk

11: end for

D3 = 6Lγ D2 2B1 + B1 + 2D1 + D1 .

δ ρ1 δ

δ

3.8 Special Cases: Error Compensated Methods

3.8.1 EC-SGDsr

In this section we consider the same setup as in Section 3.6.1 and assume additionally that f1, . . . , fn are L-smooth.

Lemma 3.8.1. For all k ≥ 0 we have

1n

k

n E gi

i=1

1n nE
i=1

gik − g¯ik

2 | xk 2 | xk

E gk 2 | xk

≤ 4L f (xk) − f (x∗) + n2 n ∇fi(x∗) 2,
i=1

≤ 6(L + L) f (xk) − f (x∗) + n3 n ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

≤ 4L f (xk) − f (x∗) + n22 n ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Proof. Applying straightforward inequality a + b 2 ≤ 2 a 2 + 2 b 2 for a, b ∈ Rd we get

1 n g¯k 2

n

i

i=1

=
(A.11)
≤
(A.4)
≤

n1 n ∇fi(xk) − ∇fi(x∗) + ∇fi(x∗) 2
i=1

n1 n ∇fi(xk) − ∇fi(x∗) 2 + n2 n ∇fi(x∗) 2

i=1

i=1

4L f (xk) − f (x∗) + 2 n n

∇fi(x∗) 2.

i=1

(3.39)

80

Similarly we obtain

1n nE
i=1

gik − g¯ik 2 | xk

=
(A.11)
≤
(A.4),(3.26)
≤

1n n ED
i=1

∇fξi (xk) − ∇fi(xk) 2

3n n ED
i=1

∇fξi (xk) − ∇fξi (x∗) 2

3n

+

ED

n i=1

∇fξi (x∗) − ∇fi(x∗) 2

+3 n n

∇fi(x∗) − ∇fi(xk) 2

i=1

6(L + L) f (xk) − f (x∗)

3n

+

ED

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Next,

using

the

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

we

derive

E gk 2 | xk

=
(A.11)
≤
(3.26)
≤



1n E n

∇fξk (xk) − ∇fξk (x∗) + ∇fξk (x∗) − ∇fi(x∗)

i

i

i

i=1

2 | xk

2n nE
i=1

∇fξik (xk) − ∇fξik (x∗) 2 | xk



1n +2E 
n

∇fξk (x∗) − ∇fi(x∗) i

i=1

2 | xk

4L f (xk) − f (x∗) + n22 n EDi
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Applying Theorem 3.7.2 we get the following result.
Theorem 3.8.2. Assume that f (x) is µ-quasi strongly convex, f1, . . . , fn are L-smooth and Assumption 3.6.1 holds. Then EC-SGDsr satisﬁes Assumption 3.3.2 with

A = 2L, A = 3(L + L), A = 2L, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 =
n

∇fi(x∗) 2,

i=1

3n

D1 =

ED

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

D3 = 6Lγ δ

2D1 + D1 , δ

81

with γ satisfying and for all K ≥ 0

γ ≤ min 1 , δ 8L 4 6L (4L + 3δ(L + L))

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4 x0K−γx∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

when µ = 0.

In other words, EC-SGDsr converges with linear rate O

√

L µ

+

L+ δLL µδ

ln

1 ε

to the neighbour-

hood of the solution when µ > 0. Applying Lemma A.5.3 we establish the rate of convergence

to ε-solution.

Corollary 3.8.3. Let the assumptions of Theorem 3.8.2 hold and µ > 0. Then after K iterations of EC-SGDsr with the stepsize





 

1

ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3

 

δ

D1



6L(2D1/δ+D1)



γ = min ,

,

 8L 4 6L (4L + 3δ(L + L))

µK











we have E f (x¯K) − f (x∗) of order



√

O  L + L + δLL

δ







x0 − x∗ 2 exp − + Lµ+√δLL K + µDK1 + L(Dδµ1 2+KD21/δ)  .

L

δ

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr requires



√

O  L + L + δLL + D1 +

µ

δµ

µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0. Corollary 3.8.4. Let the assumptions of Theorem 3.8.2 hold and µ = 0. Then after K

82

Algorithm 20 EC-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi(xk) independently from other

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

10: xk+1 = xk − vk

11: end for

workers

iterations of EC-SGDsr with the stepsize

1

δ

γ0 = min 8L , 4 6L (4L + 3δ(L + L))

γ = min γ0,

x0 − x∗ 2

x0 − x∗ 2δ

,3

D1K

6L(2D1/δ + D1)K

we have E f (x¯K) − f (x∗) of order

√

 R02

L

+

L+

δLL δ

O K +

 R02KD1 + 3 LR(04δ(K2D21/)1δ/3+ D1) 

where R0 = x0 − x∗ 2. That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr requires

 R02 O

√

L

+

L+

δLL δ

ε

+ R02D1 + R02 ε2



L(2D1/δ + D1)

√



δε3

iterations.

3.8.2 EC-SGD
In this section we consider problem (3.1) with fi(x) satisfying (3.2) where functions fξi(x) are diﬀerentiable and L-smooth almost surely in ξi, i = 1, . . . , n.
Lemma 3.8.5 (See also Lemmas 1,2 from [156]). Assume that fξi(x) are convex in x for every

83

ξi, i = 1, . . . , n. Then for every x ∈ Rd and i = 1, . . . , n

n1 n ∇fi(x) 2 ≤ 4L (f (x) − f (x∗)) + n2 n ∇fi(x∗) 2,

i=1

i=1

n1 n Eξi∼Di ∇fξi (x) − ∇fi(x) 2
i=1

≤

12L (f (x) − f (x∗)) + 3

n
E

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

1n

2

∗

2

Eξ1,...,ξn n ∇fξi (x) ≤ 4L (f (x) − f (x )) + n2

i=1

E ∇fξi (x∗) − ∇fi(x∗) 2 .

If further f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi, then for every x ∈ Rd and i = 1, . . . , n

n1 n ∇fi(x) 2 ≤ 4Lκ (f (x) − f (x∗)) + n2 n ∇fi(x∗) 2,

i=1

i=1

n1 n Eξi∼Di ∇fξi(x) − ∇fi(x) 2 ≤ 12Lκ (f (x) − f (x∗))
i=1

+3 n E n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

1n

2

∗

Eξ1,...,ξn n ∇fξi (x) ≤ 4Lκ (f (x) − f (x ))

i=1

2n + n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

where κ = Lµ .

Proof. We start with the case when functions fξi(x) are convex in x for every ξi. The ﬁrst inequality follows from (3.39). Next, we derive

n1 n Eξi∼Di ∇fξi (x) − ∇fi(x) 2
i=1

(A.11)
≤
(A.4)
≤

n3 n Eξi∼Di ∇fξi (x) − ∇fξi (x∗) 2
i=1

+ n3 n Eξi∼Di ∇fξi (x∗) − ∇fi(x∗) 2
i=1

+3 n n

∇fi(x∗) − ∇fi(x) 2

i=1

12L (f (x) − f (x∗)) + n3 n E ∇fξi(x∗) 2.
i=1

84

Due

to

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

we

get

Eξ1,...,ξn

1n

2

n ∇fξi (x)

i=1

=
(A.11)
≤
(A.4)
≤

1n

2

∗

∗

∗

Eξ1,...,ξn n (∇fξi (x) − ∇fξi (x ) + ∇fξi (x ) − ∇fi(x ))

i=1

2n n Eξi∼Di
i=1

∇fξi (x) − ∇fξi (x∗) 2

+2Eξ1,...,ξn

1n

∗

2 ∗

n (∇fξi(x ) − ∇fi(x ))

i=1

4L (f (x) − f (x∗)) + 2 n2

E ∇fξi (x∗) − ∇fi(x∗) 2 .

Next, we consider the second case: f (x) is µ-strongly convex with possibly non-convex fi, fξi. In this case

n1 n ∇fi(x) 2
i=1

(A.11)
≤
(A.1)
≤
≤

n2 n ∇fi(x) − ∇fi(x∗) 2 + n2 n ∇fi(x∗) 2

i=1

i=1

2L2 x − x∗ 2 + 2 n n

∇fi(x∗) 2

i=1

4L2 (f (x) − f (x∗)) + 2 n

µ

n

∇fi(x∗) 2

i=1

where the last inequality follows from µ-strong convexity of f . Similarly, we get

1n n Eξi∼Di
i=1

∇fξi(x) − ∇fi(x) 2

(A.11)
≤
(A.1)
≤
≤

3n n Eξi∼Di
i=1

∇fξi (x) − ∇fξi (x∗) 2

3n + n Eξi∼Di
i=1

∇fξi (x∗) − ∇fi(x∗) 2

+3 n n

∇fi(x∗) − ∇fi(x) 2

i=1

6L2 x − x∗ 2

3n + n Eξi∼Di
i=1

∇fξi (x∗) − ∇fi(x∗) 2

12L2 (f (x) − f (x∗)) µ

3n + n Eξi∼Di
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

85

Finally,

using

independence

of

ξ1k

,

.

.

.

,

ξ

k n

we

derive

Eξ1,...,ξn

1n

2

n ∇fξi (x)

i=1

=
(A.11)
≤
(A.1)
≤ ≤

1n

2

∗

∗

∗

Eξ1,...,ξn n (∇fξi (x) − ∇fξi (x ) + ∇fξi (x ) − ∇fi(x ))

i=1

2n n Eξi∼Di
i=1

∇fξi (x) − ∇fξi (x∗) 2

+2Eξ1,...,ξn

1n

∗

2 ∗

n (∇fξi(x ) − ∇fi(x ))

i=1

2L2 x − x∗ 2 + 2 E n2

4L2 (f (x) − f (x∗)) + 2

µ

n2

∇fξi (x∗) − ∇fi(x∗) 2 E ∇fξi (x∗) − ∇fi(x∗) 2 .

Applying Theorem 3.7.2 we get the following result.
Theorem 3.8.6. Assume that fξ(x) is convex and L-smooth in x for every ξ and f (x) is µ-quasi strongly convex. Then EC-SGD satisﬁes Assumption 3.3.2 with

A = A = 2L, A = 6L, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 =
n

∇fi(x∗) 2,

i=1

2n

D1 =

E

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

D3 = 6Lγ δ

2D1 + D1 , δ

with γ satisfying and for all K ≥ 0

γ ≤ √δ 8L 6 + 9δ

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4 x0K−γx∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

when µ = 0. If further f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi,

86

then EC-SGD satisﬁes Assumption 3.3.2 with

A = A = 2Lκ, A = 6Lκ, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 =
n

∇fi(x∗) 2,

i=1

2n

D1 =

E

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

D3 = 6Lγ δ

2D1 + D1 , δ

with γ satisfying and for all K ≥ 0

γ ≤ min 1 , δ 8κL 8L 3κ(2 + 3δ)

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1 .

In other words, EC-SGD converges with linear rate O

κ δ

ln

1 ε

to the neighbourhood of the solution

when fξ(x) are convex for each ξ and µ > 0. Applying Lemma A.5.3 we establish the rate of

convergence to ε-solution.

Corollary 3.8.7. Let the assumptions of Theorem 3.8.6 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of EC-SGD with the stepsize





 

δ

ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3

 

D1



6L(2D1/δ+D1)



γ = min √

,

 8L 6 + 9δ

µK











we have

E f (x¯K ) − f (x∗) = O

L x0 − x∗ 2 exp

δµ −K

+ D1 + L(D1 + D1/δ)

.

δ

L

µK

δ µ2 K 2

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD requires

 O  L + D1 +
δµ µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Corollary 3.8.8. Let the assumptions of Theorem 3.8.6 hold and f (x) is µ-strongly convex

87

with µ > 0 and possibly non-convex fi, fξi. Then after K iterations of EC-SGD with the stepsize





 

1

ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3

 

δ

D1



6L(2D1/δ+D1)



γ = min

,

,

 8κL 8L 3κ(2 + 3δ)

µK











we have E f (x¯K) − f (x∗) of order

√

O

Lκ + L κ

x0 − x∗ 2 exp − min

δµ 1 √,

K + D1 + L(D1 + D1/δ) .

δ

L κ κ2

µK

δ µ2 K 2

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD requires

 O κ2 + κ3/2 + D1 +
δ µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary 3.8.9. Let the assumptions of Theorem 3.8.6 hold, fξ(x) are convex for each ξ and µ = 0. Then after K iterations of EC-SGD with the stepsize

γ = min √ δ , 8L 6 + 9δ

x0 − x∗ 2

x0 − x∗ 2δ

,3

D1K

6L(2D1/δ + D1)K

we have E f (x¯K) − f (x∗) of order

 O  LR02 +
δK

 R02KD1 + 3 LR(04δ(K2D21/)1δ/3+ D1) 

where R0 = x0 − x∗ 2. That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD requires

iterations.

 O  LδRε02 + R02εD2 1 + R02



L(2D1/δ + D1)

√



δε3

3.8.3 EC-GDstar
We assume that i-th node has access to the gradient of fi at the optimality, i.e., to the ∇fi(x∗). It is unrealistic scenario but it gives some insights that we will use next in order to design the method that converges asymptotically to the exact solution.
Assume that f (x) is µ-quasi strongly convex and each fi is convex and L-smooth. By deﬁnition

88

Algorithm 21 EC-GDstar (see also [55])

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

gik = ∇fi(xk) − ∇fi(x∗)

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

10: xk+1 = xk − vk

11: end for

of gik it trivially follows that

gk = n1 n gik = n1 n ∇fi(xk) − ∇fi(x∗) = ∇f (xk) − ∇f (x∗) = ∇f (xk),

i=1

i=1

gik = g¯ik, and

1 n k2 n gi
i=1
gk 2

=
(A.4)
≤
=

n1 n ∇fi(xk) − ∇fi(x∗)
i=1

2L n n

fi(xk) − fi(x∗) −

i=1

2
∇fi(x∗), xk − x∗

(A.4)
∇f (xk) 2 ≤ 2L f (xk) − f (x∗) .

= 2L f (xk) − f (x∗) ,

Applying Theorem 3.7.2 we get the following result.
Theorem 3.8.10. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then EC-GDstar satisﬁes Assumption 3.3.2 with

A = A = L, A = 0, B1 = B2 = B1 = B2 = B1 = B2 = 0, D1 = D1 = D1 = 0, σ12,k ≡ σ22,k ≡ 0,
ρ1 = ρ2 = 1, C1 = C2 = 0, G = 0, D2 = 0, F1 = F2 = 0, D3 = 0,

with γ satisfying

δ γ≤ √
8L 3

89

Algorithm 22 EC-SGD-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2:

Set

h0

=

1 n

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Sample gˆik such that E[gˆik | xk] = ∇fi(xk) and E gˆik − ∇fi(xk) 2 | xk ≤ D1,i

independently from other workers

7:

gik = gˆik − hki + hk

8:

vik = C(eki + γgik)

9:

eki +1 = eki + γgik − vik

10:

hki +1 = hki + αQ(gˆik − hki )

11: end for

12:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

hki ) 13: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n

n

n i=1

vik

,

hk+1

=

1 n

hki +1 = hk +α n1

Q(gˆik −

i=1

i=1

14: end for

and for all K ≥ 0 when µ > 0 and when µ = 0.

E f (x¯K ) − f (x∗) ≤

1 − γµ

K 4 x0 − x∗ 2 ,

2

γ

E

f (x¯K ) − f (x∗)

4 x0 − x∗ ≤

2

Kγ

In other words, EC-GDstar converges with linear rate O

κ δ

ln

1 ε

to the exact solution when

µ > 0 removing the drawback of EC-SGD and EC-GD. If µ = 0 then the rate of convergence is

O L x0δ−εx∗ 2 . However, EC-GDstar relies on the fact that i-th node knows ∇fi(x∗) which is

not realistic.

3.8.4 EC-SGD-DIANA
In this section we present a new method that converges to the exact optimum asymptotically but does not need to know ∇fi(x∗) and instead of this it learns the gradients at the optimum. This method is inspired by another method called DIANA (see [139, 79]).
We notice that master needs to gather only C(eki + γgik) and Q(gˆik − hki ) from all nodes in order to perform an update.
Lemma 3.8.11. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all

90

k ≥ 0 we have

where

D1

=

1 n

1n nE
i=1

E gk | xk

1 n g¯k 2

n

i

i=1

gik − g¯ik 2 | xk

E gk 2 | xk

= ∇f (xk), ≤ 4L f (xk) − f (x∗) + 2σk2,
≤ D1, ≤ 2L f (xk) − f (x∗) + D1
n

n i=1

D1,i

and

σk2

=

1 n

n i=1

hki − ∇f (x∗) 2.

(3.40) (3.41) (3.42) (3.43)

Proof. First of all, we show unbiasedness of gk:

E gk | xk

= n1 n E gik | xk
i=1

=1 n n i=1

∇fi(xk) − hki + hk

= ∇f (xk).

Next, we derive the upper bound for g¯ik 2:

g¯ik 2 = ∇fi(xk) − hki − hk 2 (A≤.11) 2 ∇fi(xk) − ∇fi(x∗) 2 + 2 hki − ∇fi(x∗) − hk + ∇f (x∗) 2

(A.4)
≤ 4L fi(xk) − ∇fi(x∗) − ∇fi(x∗), xk − x∗

+2 hki − ∇fi(x∗) − hk + ∇f (x∗)

2
.

Summing up previous inequality for i = 1, . . . , n we get

1 n g¯k 2

n

i

i=1

≤
(A.14)
≤

4L(f (xk) − f (x∗)) + 2 n n

hki − ∇fi(x∗) −

i=1

4L f (xk) − f (x∗) + 2 n n

hki − ∇f (x∗) 2.

i=1

1n k

2 ∗

n (hi − ∇fi(x ))

i=1

(3.44)

Using the unbiasedness of gˆik we derive

1n nE
i=1

gik − g¯ik 2 | xk

=1 n E n i=1

gˆk − ∇f (xk) 2 | xk ≤ 1 n D = D .

i

i

n

1,i

1

i=1

Finally, we obtain the upper bound for the second moment of gk using the independence of

91

gˆ1k, . . . , gˆnk: E gk 2 | xk

(A=.14)
(A.4)
≤
=
≤

∇f (xk) 2 + E gk − ∇f (xk) 2



k

∗

1n k

2

k

k

2L(f (x ) − f (x )) + E  n (gˆi − ∇fi(x )) | x 

i=1

2L(f (xk) − f (x∗)) + 1

n
E

n2 i=1

gˆik − ∇fi(xk) 2 | xk

2L(f (xk) − f (x∗)) + n12 n D1,i.
i=1

Lemma 3.8.12. Let assumptions of Lemma 3.8.11 hold and α ≤ 1/(ω+1). Then, for all k ≥ 0

we have

E σk2+1 | xk ≤ (1 − α)σk2 + 2Lα(f (xk) − f (x∗)) + α2(ω + 1)D1,

(3.45)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D1

=

1 n

n i=1

D1,i.

Proof. For simplicity, we introduce new notation: h∗i d=ef ∇fi(x∗). Using this we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

=

E hk − h∗ + αQ(gˆk − hk) 2 | xk

i

i

i

i

(A=.6)
(A.6),(A.15)
≤

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2E Q(gˆik − hki ) 2 | xk
hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω + 1)E gˆik − hki 2 | xk .

Using variance decomposition (A.14) and α ≤ 1/(ω+1) we get

α2(ω + 1)E gˆik − hki 2 | xk

(A=.14) ≤

α2(ω + 1)E gˆik − ∇fi(xk) 2 | xk +α2(ω + 1) ∇fi(xk) − hki 2
α2(ω + 1)D1,i + α ∇fi(xk) − hki 2.

92

Putting all together we obtain

E hki +1 − h∗i 2 | xk

≤
(A=.8)
(A.4)
≤

hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i + α2(ω + 1)D1,i hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2 + α2(ω + 1)D1,i (1 − α) hki − h∗i 2 + 2Lα fi(xk) − fi(x∗) − ∇fi(x∗), xk − x∗
+α2(ω + 1)D1,i.

Summing up the above inequality for i = 1, . . . , n we derive

1n nE
i=1

k+1 ∗ 2 k 1 − α n hi − hi | x ≤ n

hki − h∗i 2 + 2Lα(f (xk) − f (x∗)) + α2(ωn+ 1) n D1,i.

i=1

i=1

Applying Theorem 3.7.2 we get the following result.

Theorem 3.8.13. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then EC-SGD-DIANA satisﬁes Assumption 3.3.2 with

A = 2L,

A = 0,

A = L,

B1 = 2,

1n

D1 =

D1,i,

n i=1

σ12,k = σk2 = n1 n hki − ∇fi(x∗) 2,
i=1

B1 = B2 = B2 = B1 = B2 = 0, σ22,k ≡ 0, ρ1 = α, ρ2 = 1, C1 = Lα, C2 = 0, D1 = 0,

D2 = α2(ω + 1)D1, D1 = D1 , G = 0, n

96Lγ2

6Lγ 4α(ω + 1)

F1 = δ2α 1 − min γµ , α , F2 = 0, D3 = δ

δ + 1 D1,

24

with γ and α satisfying

√

1 δ 1−α

1

γ ≤ min 4L , 8L 6(3 − α) , α ≤ ω + 1 , M1 = M2 = 0

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4( x0 − x∗γ2 + γF1σ02) + 4γ D1 + D3 ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4( x0 − xγ∗ K2 + γF1σ02) + 4γ D1 + D3

when µ = 0.

93

In other words, if

√ γ = min 1 , δ 1 − α ,
4L 8L 6(3 − α)

α = min 1 , 1 ω+1 2

and D1 = 0, i.e., gˆik = ∇fi(xk) almost surely (this is the setup of EC-GD-DIANA), EC-SGD-DIANA

converges with the linear rate

O ω + κ ln 1 δε

to the exact solution. Applying Lemma A.5.3 we establish the rate of convergence to ε-solution in the case when µ > 0.

Corollary 3.8.14. Let the assumptions of Theorem 3.8.13 hold and µ > 0. Then after K iterations of EC-SGD-DIANA with the stepsize

√

γ0 = min 1 , δ 1 − α

,

R0 = x0 − x∗ ,

F˜1

=

784Lγ2 ,

4L 8L 6(3 − α)

7δ2α

 ln max 2, min n(R02+F˜1γ0σ12,0)µ2K2 , δ(R02+F˜1γ0σ12,0)µ3K3 

 γ = min γ0,

D1

6LD1 (4α(ω+1)/δ+1)

 ,



µK











and

α

≤

1 ω+1

we

have

E f (x¯K ) − f (x∗) = O Lδ R02 exp − min δLµ , α K + nDµK1 + LD1 (αδ(µω2+K1)2/δ + 1) .

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD-DIANA requires

 O  1 + L + D1 +
α δµ nµε



LD1 (α(ω+1)/δ + 1)

√

 iterations.

µ δε

In particular, if α = ω+1 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD-DIANA requires





O ω + L + D1 + L√D1  iterations, δµ nµε δµ ε

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires





O  ω + 1 + L + D1 + √LD1  iterations. δ δµ nµε µ δε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

94

Corollary 3.8.15. Let the assumptions of Theorem 3.8.13 hold and µ = 0. Then after K iterations of EC-SGD-DIANA with the stepsize

√ γ0 = min 1 , δ 1 − α , R0 = x0 − x∗ ,
4L 8L 6(3 − α)





γ

=

 min γ

,

3

R02δ2α

1 − min

γ02µ , α4

,

nR02 , 3

δR02

 ,

0

96Lσ02

D1K 6LD1 4α(ω+1) + 1 K 


δ

and

α

≤

1 ω+1

we

have

E

f (x¯K ) − f (x∗)

of order

  LR02 3 LR04σ02 O + √ +  δK K 3 δ2α



R02D1 3 LR04D1 α(ωδ+1) + 1 

nK +

δK2

. 

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD-DIANA requires



LR2 3 LR04σ02 R2D1 R02

O

 

0+ √

+0 +

 δε

ε 3 δ2α

nε2



LD1 α(ωδ+1) + 1

√

 

δε3



iterations. requires

In particular, if α = ω+1 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGD-DIANA

 LR02

3 LR04(ω + 1)σ02

R02D1

 R02 LD1

O  δε +

√ ε 3 δ2

+ nε2 +

√ δ ε3

 iterations,

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires

 LR02

3 LR04(ω + 1)σ02

R02D1

 R02 LD1

O  δε +

δε

+ nε2 +

√ δε3

 iterations.

3.8.5 EC-SGDsr-DIANA
In this section we consider the same setup as in Section 3.6.1 and consider EC-SGD-DIANA adjusted to this setup. The resulting algorithm is called EC-SGDsr-DIANA, see
Lemma 3.8.16. Let Assumption 3.6.1 be satisﬁed and fi be convex and L-smooth for all

95

Algorithm 23 EC-SGDsr-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2:

Set

h0

=

1 n

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Sample gˆik = ∇fξk (xk) satisfying Assumption 3.6.1 independtently from other workers

i

7:

gik = gˆik − hki + hk

8:

vik = C(eki + γgik)

9:

eki +1 = eki + γgik − vik

10:

hki +1 = hki + αQ(gˆik − hki )

Q(·) is calculated independtly from other workers

11: end for

12:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

hki ) 13: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n

n

n i=1

vik

,

hk+1

=

1 n

hki +1 = hk +α n1

Q(gˆik −

i=1

i=1

14: end for

i ∈ [n]. Then, for all k ≥ 0 we have

1n nE
i=1

E gk | xk

1 n g¯k 2

n

i

i=1

gik − g¯ik 2 | xk

E gk 2 | xk

= ∇f (xk), ≤ 4L f (xk) − f (x∗) + 2σk2, ≤ 6(L + L) f (xk) − f (x∗) + D1, ≤ 4L f (xk) − f (x∗) + D1

where

σk2

=

1 n

n i=1

hki − ∇f (x∗)

2,

D1

=

3 n

n

D1

=

2 n2

EDi ∇fξi (x∗) − ∇fi(x∗) 2 .

i=1

n
i=1 EDi

∇fξi (x∗) − ∇fi(x∗) 2 and

Proof. First of all, we show unbiasedness of gk:

E gk | xk

= n1 n E gik | xk
i=1

=1 n n i=1

∇fi(xk) − hki + hk

= ∇f (xk).

(3.46) (3.47) (3.48) (3.49)

96

Following the same steps as in the proof of (3.44) we derive (3.47). Next, we establish (3.48):

1n nE
i=1

gik − g¯ik 2 | xk

=
(A.11)
≤
(A.4),(3.26)
≤

1n n EDi
i=1

∇fξk (xk) − ∇fi(xk) 2 i

3n n EDi
i=1

∇fξk (xk) − ∇fξk (x∗) 2

i

i

3n

+

EDi

n i=1

∇fξk (x∗) − ∇fi(x∗) 2 i

+3 n n

∇fi(x∗) − ∇fi(xk) 2

i=1

6(L + L) f (xk) − f (x∗)

3n

+

EDi

n i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Finally, we obtain the upper bound for the second moment of gk using the independence of

ξ

k 1

,

.

.

.

,

ξ

k n

:

E gk 2 | xk

=
(A.11)
≤
(3.26)
≤



1n

k

∗

2

∗

∗

k

E n

(∇fξk (x ) − ∇fξk (x ) + ∇fξk (x ) − ∇fi(x )) | x 

i

i

i

i=1

2n nE
i=1

∇fξk (xk) − ∇fξk (x∗) 2 | xk

i

i



1n

∗

2

∗

k

+2E  n

(∇fξk (x ) − ∇fi(x )) | x  i

i=1

4L f (xk) − f (x∗) + n22 n EDi
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Lemma 3.8.17. Let fi be convex and L-smooth, Assumption 3.6.1 holds and α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E σk2+1 | xk ≤ (1 − α)σk2 + 2α(3L + 4L)(f (xk) − f (x∗)) + D2,

(3.50)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗) 2 and D2 = α2(ω + 1)D1.

Proof. The proof is identical to the proof of Lemma 3.6.3 up to the following changes in the notation: ω1 = ω, ∆ki = Q(gˆik − hki ) and ∆ˆ ki = gˆik − hki .
Applying Theorem 3.7.2 we get the following result.

97

Theorem 3.8.18. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n, f (x) is µ-quasi strongly convex and Assumption 3.6.1 holds. Then EC-SGDsr-DIANA satisﬁes Assumption 3.3.2 with

A = 2L, σ12,k

A = 3(L + L), A = 2L, = σk2 = n1 n hki − ∇fi(x∗)
i=1

B1 = 2,

3n

D1 =

EDi

n i=1

2, D1 = 0, D1 = 32n D1,

∇fξi (x∗) − ∇fi(x∗) 2 , D2 = α2(ω + 1)D1

B1 = B1 = B2 = B2 = B2 = 0, σ22,k ≡ 0, ρ1 = α, ρ2 = 1, C1 = 2α(3L + 4L), C2 = 0,

96Lγ2

6Lγ 4α(ω + 1)

G = 0, F1 = δ2α 1 − min γµ , α , F2 = 0, D3 = δ

δ + 1 D1,

24

with γ and α satisfying









γ ≤ min  1 , δ  ,

 4L 4 6L 4L + 3δ(L + L) + 16(31L−+α4L) 

α≤ 1 , ω+1

M1 = M2 = 0.

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4( x0 − x∗γ2 + γF1σ02) + 4γ D1 + D3 ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4( x0 − xγ∗ K2 + γF1σ02) + 4γ D1 + D3

when µ = 0.

Applying Lemma A.5.3 we establish the rate of convergence to ε-solution in the case when µ > 0.

Corollary 3.8.19. Let the assumptions of Theorem 3.8.18 hold and µ > 0. Then after K

98

iterations of EC-SGDsr-DIANA with the stepsize









 

1

δ

 

γ0 = min 4L ,

,
16(3L+4L)

  

4 6L 4L + 3δ(L + L) + 1−α

  

R0 =

x0 − x∗ , F˜1 =

96Lγ02 ,

δ2α 1 − min γ02µ , α4


 

ln max 2, min 3n(R02+F˜1γ0σ12,0)µ2K2 , δ(R02+F˜1γ0σ12,0)µ3K3


 

 
γ = min γ0,

2D1

6LD1 4α(ωδ+1) +1  ,



µK















and

α

≤

1 ω+1

we

have

E

f (x¯K ) − f (x∗)

of order



√

O  L + LL

δ


2

 µ

 

D1

LD1 α(ωδ+1) + 1 

R0

exp − min L

+

√ LL

,α K 

+

nµK

+

δ µ2 K 2



δ

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires

 √
O  1 + L + LL + D1 +  α µ δµ nµε



LD1 α(ωδ+1) + 1

√

 

iterations.

µ δε



In particular, if α = ω+1 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√



O ω + L + LL + D1 + L√D1  iterations,

µ δµ nµε δµ ε

and if α = ω+δ 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√



O  ω + 1 + L + LL + D1 + √LD1  iterations.

δ µ δµ nµε µ δε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0. Corollary 3.8.20. Let the assumptions of Theorem 3.8.18 hold and µ = 0. Then after K

99

iterations of EC-SGDsr-DIANA with the stepsize









 

1

δ

 

γ0 = min 4L ,

,
16(3L+4L)

  

4 6L 4L + 3δ(L + L) + 1−α

  

R0 = x0 − x∗ ,





γ

=

 min γ

,

3

R02δ2α

1 − min

γ02µ , α4

,

3nR02 , 3

δR02

 ,

0

96Lσ02

2D1K 6LD1 4α(ω+1) + 1 K 


δ

and

α

≤

1 ω+1

we

have

E

f (x¯K ) − f (x∗)

of order



√

LR2

LLR2

3 LR04σ02

O

 

0+

0+ √ +

K

δK

K 3 δ2α



R02D1 3 LR04D1 α(ωδ+1) + 1 

nK +

δK2

. 

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√

LR2

LLR2

3 LR04σ02 R2D

R02

O

 

0+

0+ √

+ 0 1+

ε

δε

ε 3 δ2α

nε2



LD1 α(ωδ+1) + 1

√

 

δε3



iterations. In particular, if α = ω+1 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√

LR2

LLR2

3 LR04(ω + 1)σ02 R2D

 R2 LD1

O 0 +

0+

√

+ 0 1 + 0 √  iterations,

ε

δε

ε 3 δ2

nε2

δ ε3

and if α = ω+δ 1 , then to achive E f (x¯K) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√

LR2

LLR2

3 LR04(ω + 1)σ02 R2D

 R2 LD1

O  ε 0 + δε 0 + δε + n0ε21 + 0√δε3  iterations.

3.8.6 EC-LSVRG
In this section we consider problem (3.1) with f (x) being µ-quasi strongly convex and fi(x) satisfying (3.3) where functions fij(x) are convex and L-smooth. For this problem we propose a new method called EC-LSVRG which takes for the origin another method called LSVRG (see [77, 103]).

Lemma 3.8.21. For all k ≥ 0, i ∈ [n] we have

g¯ik = E gik | xk = ∇fi(xk)

(3.51)

100

Algorithm 24 EC-LSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

7:

vik = C(eki + γgik)

8:

eki +1 = eki + γgik − vik

9:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

10: end for

11:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

and

1n nE
i=1

1 n g¯k 2

n

i

i=1

gik − g¯ik 2 | xk

E gk 2 | xk

≤ 4L f (xk) − f (x∗) + D1, ≤ 12L f (xk) − f (x∗) + 3σk2, ≤ 4L f (xk) − f (x∗) + 2σk2

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗)

2

and

D1

=

2 n

n i=1

∇fi(x∗) 2.

Proof. First of all, we derive unbiasedness of gik: E gik | xk = m1 m ∇fij(xk) − ∇fij(wik) + ∇fi(wik) = ∇fi(xk).
j=1

n

Next,

we

get

an

upper

bound

for

1 n

g¯ik 2:

i=1

1 n g¯k 2

n

i

i=1

=
(A.11)
≤
(A.4)
≤

n1 n ∇fi(xk) 2
i=1

n2 n ∇fi(xk) − ∇fi(x∗) 2 + n2 n ∇fi(x∗) 2

i=1

i=1

4L f (xk) − f (x∗) + 2 n n

∇fi(x∗) 2.

i=1

(3.52) (3.53) (3.54)

101

Using (3.51) we establish the following inequality:

1n nE
i=1

gik − g¯ik 2 | xk

(A.11)
≤
(A.4),(A.14)
≤

3n nE
i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

2 | xk

+3 n E n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+3 n n

∇fi(x∗) − ∇fi(xk) 2

i=1

12L f (xk) − f (x∗) + n3m n m ∇fij(wik) − ∇fij(x∗) 2.
i=1 j=1

Finally, we derive (3.54):

E gk 2 | xk



1n

k

2

k

k

∗

k

=

E  n ∇fil(x ) − ∇fil(wi ) + ∇fi(wi ) − ∇fi(x ) | x 

i=1

(A.11)
≤

2n nE
i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+2 n E n i=1

∇fil(wik) − ∇fil(x∗) −

∇fi(wik) − ∇fi(x∗)

2 | xk

2

= n2m n m ∇fij(wik) − ∇fij(x∗) − m1 m ∇fij(wik) − ∇fij(x∗)

i=1 j=1

j=1

+ n2m n m ∇fij(xk) − ∇fij(x∗) 2
i=1 j=1

(A.4),(A.14)
≤

4L f (xk) − f (x∗)

+ n2m n m ∇fij(wik) − ∇fij(x∗) 2 .
i=1 j=1

Lemma 3.8.22. For all k ≥ 0, i ∈ [n] we have

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(3.55)

102

Proof. By deﬁnition of wik+1 we get

E σk2+1 | xk

=
=
(A.4)
≤
=

1 nm

nm

E

i=1 j=1

∇fij (wik+1) − ∇fij (x∗) 2 | xk

1n−mp n m ∇fij(wik) − ∇fij(x∗) 2 + npm n m ∇fij(xk) − ∇fij(x∗) 2

i=1 j=1

i=1 j=1

(1 − p)σk2 + 2nLmp n m Dfij (xk, x∗)
i=1 j=1

(1 − p)σk2 + 2Lp f (xk) − f (x∗) .

Applying Theorem 3.7.2 we get the following result.

Theorem 3.8.23. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then EC-LSVRG satisﬁes Assumption 3.3.2 with

A = 2L,

A = 12L,

A = 2L,

B1 = B1 = B1 = B2 = 0,

2n D1 =
n

∇fi(x∗) 2,

i=1

D1 = D1 = 0, B2 = 3, B2 = 2, σ12,k ≡ 0, C1 = 0,

σ22,k = σk2 = n1m n m ∇fij (wik) − ∇fij (x∗) 2,
i=1 j=1

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

72Lγ2

12Lγ

G = 0, F1 = 0, F2 = δp 1 − min γµ , p , D3 = δ2 D1,

24

with γ satisfying









 

1

δ

 

4

γ ≤ min 24L ,

, M2 = p .

  

8L

3 2 + 3δ

2

+

1 1−p

  

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K

4(T 0

+

γF2σ02)

+

48Lγ2 D1

γ

δ2

when µ > 0 and

E

f (x¯K ) − f (x∗)

≤

4(T 0

+

γF2σ02)

+

48Lγ2 D1

γK

δ2

when µ = 0, where T k d=ef xk − x∗ 2 + M2γ2σk2.

In other words, EC-LSVRG converges with linear rate O

1 p

+

√κ δ 1−p

ln

1 ε

to the neighbourhood

of

the

solution.

If

m

≥

2

then

taking

p

=

1 m

we

get

that

in

expectation

the

sample

complexity

of

103

one iteration of EC-LSVRG is O(1) gradients calculations per node as for EC-SGDsr with standard

sampling and the rate of convergence to the neighbourhood becomes O

m

+

κ δ

ln

1 ε

.

We

notice that the size of this neighbourhood is typically smaller than for EC-SGDsr, but still the

method fails to converge to the exact solution with linear rate. Applying Lemma A.5.3 we

establish the rate of convergence to ε-solution in the case when µ > 0.

Corollary 3.8.24. Let the assumptions of Theorem 3.8.23 hold and µ > 0. Then after K iterations of EC-LSVRG with the stepsize









 

1

δ

 

γ0 = min 24L ,

,

  

8L

3 2 + 3δ

2

+

1 1−p

  

T˜0 =

x0 − x∗ 2 + M2γ2σ2, F˜2 =

72Lγ02 ,

00

δp 1 − min γ02µ , p4

 

ln max 2, δ2 T˜0+F˜2γ0σ02 µ3K3

 

 

48LD1

 

γ = min γ0, µK  ,









and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) = O Lδ T˜0 + F˜2γ0σ02 exp − min δLµ , m1 K + δ2LµD2K1 2 .

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-LSVRG requires

√ O m + L + L√D1
δµ δµ ε

iterations.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary 3.8.25. Let the assumptions of Theorem 3.8.23 hold and µ = 0. Then after K iterations of EC-LSVRG with the stepsize









 

1

γ0 = min

,

δ

 
, R0 = x0 − x∗ ,

 24L 8L 3 2 + 3δ 2 + 1−1 p 


 γ = min γ0,


R02p 3 R02δp 1 − min γ02µ , p4

4σ2 ,

72Lσ2

0

0



,3

δ2R02

 ,

12LD1K 

104

Algorithm 25 EC-LSVRGstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗)

7:

vik = C(eki + γgik)

8:

eki +1 = eki + γgik − vik

9:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

10: end for

11:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 O  LR02 +
δK

mR02σ02 K+

3 LR04mσ02 3 LR04 1 n

√

+

3

3 δK

(δK)2/3 n i=1

 ∇fi(x∗) 2 .

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-LSVRG requires

iterations.

 O  LR02 +
δε

mR02σ02 3 LR04mσ02 R02

+√

+

ε

3 δε

δε3/2

 Ln
∇fi(x∗) 2 n i=1

3.8.7 EC-LSVRGstar
In the setup of Section 3.8.6 we now assume that i-th node has an access to the ∇fi(x∗). Under this unrealistic assumption we construct the method called EC-LSVRGstar that asymptotically converges to the exact solution.

Lemma 3.8.26. For all k ≥ 0, i ∈ [n] we have

E gk | xk = ∇f (xk)

(3.56)

and

1 n g¯k 2 ≤ 2L f (xk) − f (x∗) ,

(3.57)

n

i

i=1

105

where

σk2

=

1 nm

1n nE
i=1

gik − g¯ik 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2,

E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2,

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(3.58) (3.59)

Proof. First of all, we derive unbiasedness of gk:

E gk | xk

= n1 n E ∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗) | xk
i=1

= n1m n m ∇fij(xk) − ∇fij(wik) + ∇fi(wik) − ∇fi(x∗)
i=1 j=1

= ∇f (xk) + 1 n n

−∇fi(wik) + ∇fi(wik) − ∇f (x∗) = ∇f (xk).

i=1

n

Next,

we

get

an

upper

bound

for

1 n

g¯ik 2:

i=1

1n

g¯k 2 = 1 n

(A.4)
∇f (xk) − ∇f (x∗) 2 ≤ 2L f (xk) − f (x∗) .

n

i

n

i

i

i=1

i=1

Since the variance of random vector is not greater than its second moment we obtain:

1n nE
i=1

gik − g¯ik 2 | xk

(A.14)
≤
(A.11)
≤
(A.4),(A.14)
≤

1n nE
i=1

gik 2 | xk

2n nE
i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

2 | xk

+2 n E n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

4L f (xk) − f (x∗) + n2m n m ∇fij(wik) − ∇fij(x∗) 2.
i=1 j=1

Inequality (3.59) trivially follows from the inequality above by Jensen’s inequality and convexity of · 2.

Lemma 3.8.27. For all k ≥ 0, i ∈ [n] we have

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(3.60)

Proof. The proof of this lemma is identical to the proof of Lemma 3.8.22.

106

Applying Theorem 3.7.2 we get the following result.
Theorem 3.8.28. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then EC-LSVRGstar satisﬁes Assumption 3.3.2 with

A = L, A = A = 2L, B1 = B1 = B1 = B2 = 0, B2 = B2 = 2, D1 = D1 = 0,

σ12,k ≡ 0, ,

C1 = 0,

σ22,k = σk2 = n1m n m ∇fij (wik) − ∇fij (x∗) 2,
i=1 j=1

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

G = 0,

F1 = 0,

48Lγ2(2 + p)

F2 =

,

δp

D3 = 0,

with γ satisfying









 

3

δ

 

8

γ ≤ min 56L ,

, M2 = 3p .

  

8L

3 1+δ

1

+

2 1−p

  

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK

when µ = 0, where T k d=ef xk − x∗ 2 + M2γ2σk2.

In other words, EC-LSVRGstar converges with linear rate O

1 p

+

√κ δ 1−p

ln

1 ε

exactly to the

solution when µ > 0.

If m ≥ 2 then taking p =

1 m

we get that in expectation the sample

complexity of one iteration of EC-LSVRGstar is O(1) gradients calculations per node as for

EC-SGDsr with standard sampling and the rate of convergence becomes O

m

+

κ δ

ln

1 ε

.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary 3.8.29. Let the assumptions of Theorem 3.8.28 hold and µ = 0. Then after K iterations of EC-LSVRGstar with the stepsize









 

3

γ0 = min

,

δ

 
, R0 = x0 − x∗ ,

 56L 8L 3 1 + δ 1 + 1−2 p 





 γ = min γ ,

3pR02 , 3 R02δp 1 − min γ02µ , p4

 ,

0


8σ02

72Lσ02



107

Algorithm 26 EC-LSVRG-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2:

Set

h0

=

1 n

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Pick l uniformly at random from [m]

7:

Set gˆik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

8:

gik = gˆik − hki + hk

9:

vik = C(eki + γgik)

10:

eki +1 = eki + γgik − vik

11:

hki +1 = hki + αQ(gˆik − hki )

12:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

13: end for

14:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

hki ) 15: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n

n

n i=1

vik

,

hk+1

=

1 n

hki +1 = hk +α n1

Q(gˆik −

i=1

i=1

16: end for

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 O  LR02 +
δK

R02mσ02

3

LR04

mσ

2 0



+√

.

K

3 δK

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-LSVRGstar requires

 O  LR02 +
δε

R02mσ02

3

LR04

mσ

2 0



+√



ε

3 δε

iterations.
However, such convergence guarantees are obtained under very restrictive assumption: the method requires to know vectors ∇fi(x∗).

3.8.8 EC-LSVRG-DIANA
In the setup of Section 3.8.6 we construct a new method called EC-LSVRG-DIANA which does not require to know ∇fi(x∗) and has linear convergence to the exact solution. As in EC-SGD-DIANA the master needs to gather only C(eki + γgik) and Q(gˆik − hki ) from all nodes in order to perform an update.

108

Lemma 3.8.30. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m. Then, for all k ≥ 0 we have

1n nE
i=1

E gk | xk

1 n g¯k 2

n

i

i=1

gik − g¯ik 2 | xk

E gk 2 | xk

= ∇f (xk), ≤ 4L f (xk) − f (x∗) + 2σ12,k, ≤ 6L f (xk) − f (x∗) + 3σ12,k + 3σ22,k, ≤ 4L f (xk) − f (x∗) + 2σ22,k

(3.61) (3.62) (3.63) (3.64)

where

σ12,k = n1 n hki − ∇f (x∗) 2,
i=1

σ22,k = n1m n m ∇fij(wik) − ∇fij(x∗) 2.
i=1 j=1

Proof. First of all, we show unbiasedness of gk:

E gk | xk

= 1 n E gˆk − hk + hk | xk

n

i

i

i=1

= n1m n m ∇fij(xk) − ∇fij(wik) + ∇fi(wik) − hki + hk = ∇f (xk).
i=1 j=1

n

Next,

we

derive

the

upper

bound

for

1 n

g¯ik 2:

i=1

1 n g¯k 2

=

1 n ∇f (xk) − hk + hk 2

n

i

n

i

i

i=1

i=1

(A.11)
≤

n2 n ∇fi(xk) − ∇fi(x∗) 2 + n2 n hki − ∇fi(x∗) − hk − ∇f (x∗) 2

i=1

i=1

(A.4),(A.14)
≤

4L f (xk) − f (x∗)

+2 n n

hki − ∇fi(x∗) 2.

i=1

109

Since the variance of random vector is not greater than its second moment we obtain:

1n nE
i=1

gik − g¯ik 2 | xk

(A.14)
≤ =
(A.11)
≤
(A.4),(A.14)
≤

1n nE
i=1

gik 2 | xk

1n nE
i=1

∇fil(xk) − ∇fil(wik) + ∇fi(wik) − hki + hk 2 | xk

3n nE
i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

2 | xk

+3 n E n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+ n3 n hki − ∇fi(x∗) − hk − ∇f (x∗) 2
i=1

6L f (xk) − f (x∗) + n3m n m ∇fij(wik) − ∇fij(x∗) 2
i=1 j=1

+3 n n

hki − ∇fi(x∗)

2
.

i=1

Finally, we obtain an upper boud for the second moment of gk:

E gk 2 | xk



1n

k

2

k

k

∗

k

=

E  n ∇fil(x ) − ∇fil(wi ) + ∇fi(wi ) − ∇fi(x ) | x 

i=1

(A.11)
≤

2n nE
i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+2 n E n i=1

∇fil(wik) − ∇fil(x∗) −

∇fi(wik) − ∇fi(x∗)

2 | xk

2

= n2m n m ∇fij(wik) − ∇fij(x∗) − m1 m ∇fij(wik) − ∇fij(x∗)

i=1 j=1

j=1

+ n2m n m ∇fij(xk) − ∇fij(x∗) 2
i=1 j=1

(A.4),(A.14)
≤

4L f (xk) − f (x∗)

+ n2m n m ∇fij(wik) − ∇fij(x∗) 2 .
i=1 j=1

Lemma 3.8.31. Assume that α ≤ 1/(ω+1). Then, for all k ≥ 0 we have E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k, E σ22,k+1 | xk ≤ (1 − p)σk2,2 + 2Lp f (xk) − f (x∗)

(3.65) (3.66)

110

where

σ12,k

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij(wik) − ∇fij(x∗) 2.

Proof. First of all, we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

=

E hk − h∗ + αQ(gˆk − hk) 2 | xk

i

i

i

i

(A=.6)
(A.6),(A.15)
≤

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2E Q(gˆik − hki ) 2 | xk
hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω + 1)E gˆik − hki 2 | xk .

Using variance decomposition (A.14) and α ≤ 1/(ω+1) we get

α2(ω + 1)E gˆik − hki 2 | xk

(A=.14)
≤
(A.11)
≤

α2(ω + 1)E gˆik − ∇fi(xk) 2 | xk + α2(ω + 1) ∇fi(xk) − hki 2 αE gˆik − ∇fi(xk) 2 | xk + α ∇fi(xk) − hki 2 2αE ∇fil(xk) − ∇fil(x∗) − ∇fi(xk) − ∇fi(x∗) 2 | xk

+2αE ∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

(A.14)
≤

+α ∇fi(xk) − hki 2 2αE ∇fil(xk) − ∇fil(x∗) 2 | xk

(A.4)
≤

+2αE ∇fil(wik) − ∇fil(x∗) 2 | xk + α ∇fi(xk) − hki 2 4LαDfi (xk, x∗) + 2mα m ∇fij(wik) − ∇fij(x∗) 2
j=1
+α ∇fi(xk) − hki 2

Putting all together we obtain

E hki +1 − h∗i 2 | xk

≤
(A=.8)
(A.4)
≤

hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i +4LαDfi (xk, x∗) + 2mα m ∇fij(wik) − ∇fij(x∗) 2
j=1
hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2 +4LαDfi (xk, x∗) + 2mα m ∇fij(wik) − ∇fij(x∗) 2
j=1

(1 − α) hki − h∗i 2 + 6LαDfi (xk, x∗)

+ 2α m m

∇fij(wik) − ∇fij(x∗) 2.

j=1

111

Summing up the above inequality for i = 1, . . . , n we derive

E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k.

Similarly to the proof of Lemma 3.8.22 we get

E σ22,k+1 | xk

= =
(A.4)
≤ =

1 nm

nm

E

i=1 j=1

∇fij (wik+1) − ∇fij (x∗) 2 | xk

1n−mp n m ∇fij(wik) − ∇fij(x∗) 2
i=1 j=1

+ npm n m ∇fij(xk) − ∇fij(x∗) 2
i=1 j=1

(1 − p)σ22,k + 2nLmp n m Dfij (xk, x∗)
i=1 j=1

(1 − p)σ22,k + 2Lp f (xk) − f (x∗) .

Applying Theorem 3.7.2 we get the following result.
Theorem 3.8.32. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m and f (x) is µ-quasi strongly convex. Then EC-LSVRG-DIANA satisﬁes Assumption 3.3.2 with

A = A = 2L, B1 = B2 = 0, B1 = B2 = 2,

A = 3L,

B1 = B2 = 3,

σ12,k = n1 n
i=1

σ22,k = n1m n m ∇fij(wik) − ∇fij(x∗) 2,
i=1 j=1

D1 = D1 = D1 = D2 = D3 = 0, hki − ∇fi(x∗) 2, ρ1 = α,
ρ2 = p, C1 = 3Lα, C2 = Lp,

24Lγ2

4 δ

+

3

24Lγ2

4 1−α

4 δ

+

3

+3

G = 2, F1 = δα 1 − min γµ , α , p , F2 = δp 1 − min γµ , α , p ,

2 44

2 44

with γ and α satisfying









γ ≤ min  9 , δ  ,

 296L 4L 6 4 + 3δ + 1−2α 3 + 1−4 p (4 + 3δ) + 16−δp 

α≤ 1 ω+1

with

M1

=

0

and

M2

=

8 3p

+

32 9p

and

for

all

K

≥

0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α p ,,

2 44

K 4(T 0 + γF1σ12,0 + γF2σ22,0) ,
γ

112

when µ > 0 and

E

f (x¯K ) − f (x∗)

4(T 0 + γF1σ12,0 + γF2σ22,0) ≤

Kγ

when µ = 0, where T k d=ef xk − x∗ 2 + M2γ2σ22,k.

In other words, if p = 1/m, m ≥ 2 and









γ = min  9 , δ  ,

 296L 4L 6 4 + 3δ + 1−2α 3 + 1−4 p (4 + 3δ) + 16−δp 

α = min 1 , 1 , ω+1 2

then EC-LSVRG-DIANA converges with the linear rate

O ω + m + κ ln 1 δε

to the exact solution when µ > 0. Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary 3.8.33. Let the assumptions of Theorem 3.8.32 hold and µ = 0. Then after K iterations of EC-LSVRG-DIANA with the stepsize









 

9

δ

 

γ0 = min 296L ,

,

  

4L

6

4

+

3δ

+

2 1−α

3

+

4 1−p

(4

+

3δ)

+

6δ 1−p

  

R0 = x0 − x∗ ,





  
γ = min γ0,
  

9pR02 , 56σ22,0 3

R02

  

24L( 4 +3)

2 24L( 4 ( 4 +3)+3) 2 ,

δα(1−min{δγ02µ , α4 , p4 }) σ1,0 + δp(1−m1−inα{ γδ02µ , α4 , p4 }) σ2,0 

and p = m1 , m ≥ 2, α = min ω+1 1 , 12 we have E f (x¯K ) − f (x∗) of order

 O  LR02 +
δK

R02mσ22,0 3 LR04((ω + 1)σ12,0 + mσ22,0)  K + δ2/3K  .

That is, to achive E f (x¯K) − f (x∗) ≤ ε EC-LSVRG-DIANA requires

 O  LR02 +
δε

R02mσ22,0 3 LR04((ω + 1)σ12,0 + mσ22,0) 

ε+

δ2/3ε



iterations.

113

3.9 Numerical Experiments

To justify our theory, we conduct several numerical experimentson logistic regression problem with 2-regularization:

min

f (x) = 1

N

µ

log (1 + exp (−y · (Ax) )) +

x2

,

x∈Rd

N

i

i

2

i=1

(3.67)

where N is a number of features, x ∈ Rd represents the weights of the model, A ∈ RN×d is a feature matrix, vector y ∈ {−1, 1}N is a vector of labels and (Ax)i denotes the i-th component of vector Ax. Clearly, this problem is L-smooth and µ-strongly convex with L = µ + λmax(A A)/4N, where λmax(A A) is a largest eigenvalue of A A. The datasets were taken from LIBSVM library [27], and the code was written in Python 3.7 using standard libraries. Our code is available at https://github.com/eduardgorbunov/ef_sigma_k.

We simulate parameter-server architecture using one machine with Intel(R) Core(TM) i7-9750 CPU 2.60 GHz in the following way. First of all, we always use such N that N = n · m and consider n = 20 and n = 100 workers. The choice of N for each dataset that we consider is stated in Table 3.3.
Table 3.3: Summary of datasets: N = total # of data samples; d = # of features.

a9a

w8a gisette mushrooms madelon phishing

N 32, 000 49, 700 6, 000

8, 000

2, 000 11, 000

d 123

300

5, 000

112

500

68

Next, we shuﬄe the data and split in n groups of size m. To emulate the work of workers, we use a single machine and run the methods with the parallel loop in series. Since in these experiments we study sample complexity and number of bits used for communication, this setup is identical to the real parameter-server setup in this sense.
In all experiments we use the stepsize γ = 1/L and 2-regularization parameter µ = 10−4λmax(A A)/4N. The starting point x0 for each dataset was chosen so that f (x0) − f (x∗) ∼ 10. In experiments with stochastic methods we used batches of size 1 and uniform sampling for simplicity. For LSVRG-type methods we choose p = 1/m.
Compressing stochastic gradients. The results for a9a, madelon and phishing can be found in Figure 3.1 (included here) and for w8a, mushrooms and gisette in Figure B.1 (in the Appendix). We choose number of components for TopK operator of the order max{1, d/100}. Clearly, in these experiments we see two levels of noise. For some datasets, like a9a, phishing or mushrooms, the noise that comes from the stochasticity of the gradients dominates the noise coming from compression. Therefore, methods such as EC-SGD and EC-SGD-DIANA start to oscillate around a larger value of the loss function than other methods we consider. EC-LSVRG reduces the largest source of noise and, as a result, ﬁnds a better approximation of the solution.
114

However, at some point, it reaches another level of the loss function and starts to oscillate there due to the noise coming from compression. Finally, EC-LSVRG-DIANA reduces the variance of both types, and as a result, ﬁnds an even better approximation of the solution. In contrast, for the madelon dataset, both noises are of the same order, and therefore, EC-LSVRG and EC-SGD-DIANA behave similarly to EC-SGD. However, EC-LSVRG-DIANA again reduces both types of noise eﬀectively and ﬁnds a better approximation of the solution after a given number of epochs. In the experiments with w8a and gisette datasets, the noise produced by compression is dominated by the noise coming from the stochastic gradients. As a result, we see that the DIANA-trick is not needed here.

f(xk) f(x * ) f(x0) f(x * )

10 1 10 3

a9a, 20 workers
EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7

0.00 0.25N0u.5m0b0e.r7o5f 1b.i0ts0p1e.2r 5w1o.r5k0er1.75 21.0e70

a9a, 20 workers

EC-SGD top-1

10 1

EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant

EC-L-SVRG top-1

10 3

EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7 0 Numb2e0r of pas4s0es throu6g0h the d8a0ta

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers
EC-SGD top-5

10 1

EC-SGD-DIANA top-5 rand-5 EC-SGD-DIANA top-5 l2-quant

10 2

EC-L-SVRG top-5 EC-L-SVRG-DIANA top-5 rand-5

10 3

EC-L-SVRG-DIANA top-5 l2-quant

10 4

10 5

10 6

0
100 10 1 10 2 10 3 10 4 10 5 10 6

500N0u0m0 b1e00r0o0f0b0i1ts50p0e0r00w2o0r0k0e0r00 2500000 madelon, 20 workers
EC-SGD top-5 EC-SGD-DIANA top-5 rand-5 EC-SGD-DIANA top-5 l2-quant EC-L-SVRG top-5 EC-L-SVRG-DIANA top-5 rand-5 EC-L-SVRG-DIANA top-5 l2-quant

0 Num20ber o4f 0passe6s0throu8g0h th1e0d0ata120

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 20 workers
EC-SGD top-1

10 1

EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant

10 2

EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1

10 3

EC-L-SVRG-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

1000N00u0m2b0e00r0o00f b3i0t0s00p0e0r 4w00o0r0k00er5000000 phishing, 20 workers
EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

0 Numb5e0r of pas1s0e0s throu1g5h0the da2t0a0

f(xk) f(x * ) f(x0) f(x * )

Figure 3.1: Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to solve logistic regression problem with 20 workers.

Compressing full gradients.

f(xk) f(x * ) f(x0) f(x * )

a9a, 20 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 100000N0u20m00b00e0r30o00f0b00it4s000p0e00r5w00o00r0k0e60r00000 7000000

a9a, 20 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7 0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers
EC-GD top-5

EC-GD top-10

10 2

EC-GD-star top-5 EC-GD-DIANA top-5 rand-5

10 4

EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0N.5umbe1r .o0f bits1p.e5r wor2ke.0r 21.5e7 madelon, 20 workers
EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4 10 6 10 8 10 10 10 12

phishing, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0.0 0.2Num0.b4er 0of.6bits0p.8er w1o.0rker1.2 11.e47 phishing, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

f(xk) f(x * ) f(x0) f(x * )

Figure 3.2: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solve logistic regression problem with 20 workers.

115

In order to show the eﬀect of DIANA-type variance reduction itself, we consider the case when all workers compute the full gradients of their functions, see Figure 3.2 (included here) and Figures B.2–B.5 (in the Appendix). Clearly, for all datasets except mushrooms, EC-GD with constant stepsize converges to a neighborhood of the solution only, while EC-GDstar and EC-GD-DIANA converge with linear rate asymptotically to the exact solution. EC-GDstar always show the best performance, however, it is impractical: we used a very good approximation of the solution to apply this method. In contrast, EC-DIANA converges slightly slower and requires more bits for communication; but it is practical and shows better performance than EC-GD. On the mushrooms datasets, EC-GD does not reach the oscillation region after the given number of epochs, therefore, it is preferable there.
116

Chapter 4
Local SGD: Uniﬁed Theory and New Eﬃcient Methods

4.1 Introduction

In this chapter1, we are interested in a centralized distributed optimization problem of the form

1n

min f (x) =

fi(x),

x∈Rd

n i=1

(4.1)

where n is the number of devices/clients/nodes/workers. We assume that fi can be represented either as a) an expectation, i.e.,

fi(x) = Eξi∼Di [fξi (x)] ,

(4.2)

where Di describes the distribution of data on device i, or b) as a ﬁnite sum, i.e.,

1m

fi(x) =

fij (x).

m j=1

(4.3)

While our theory allows the number of functions m to vary across the devices, for simplicity of exposition, we restrict the narrative to this simpler case.
Federated learning (FL)—an emerging subﬁeld of machine learning [135, 100, 134]—is traditionally cast as an instance of problem (6.6) with several idiosyncrasies. First, the number of devices n is very large: tens of thousands to millions. Second, the devices (e.g., mobile phones) are often very heterogeneous in their compute, connectivity, and storage capabilities. The data deﬁning each function fi reﬂects the usage patterns of the device owner, and as such, it is either unrelated or at best related only weakly. Moreover, device owners desire to protect their local private data, and for that reason, training needs to take place with the data remaining on the devices. Finally, and this is of key importance for the development in this work, communication among the workers, typically conducted via a trusted aggregation server, is very expensive.

1Part of this work was done while I was a research intern at KAUST.

117

Communication bottleneck. There are two main directions in the literature for tackling the communication cost issue in FL. The ﬁrst approach consists of algorithms that aim to reduce the number of transmitted bits by applying a carefully chosen gradient compression scheme, such as quantization [4, 17, 139, 79, 174, 178], sparsiﬁcation [2, 126, 5, 226, 225, 140], or other more sophisticated strategies [88, 209, 230, 223, 20, 57]. The second approach—one that we investigate in this chapter—instead focuses on increasing the total amount of local computation in between the communication rounds in the hope that this will reduce the total number of communication rounds needed to build a model of suﬃcient quality [199, 241, 177, 115, 161]. These two approaches, communication compression and local computation, can be combined for a better practical performance [15].

Local ﬁrst-order algorithms. Motivated by recent development in the ﬁeld [247, 135, 205, 125, 123, 231, 86, 89, 229], in this chapter we perform an in-depth and general study of local ﬁrst-order algorithms. Contrasted with zero or higher order local methods, local ﬁrst order methods perform several gradient-type steps in between the communication rounds. In particular, we consider the following family of methods:



xki − γgik,

xki +1 =

n 1

k

k

 n xi − γgi ,

i=1

if ck+1 = 0, if ck+1 = 1,

(4.4)

where xki represents the local variable maintained by the i-th device, gik represents local ﬁrst order direction2 and (possibly random) sequence {ck}k≥1 with ck ∈ {0, 1} encoding the times
when communication takes place.

Both the classical Local-SGD/FedAvg [135, 205, 89, 229] and shifted local SGD [123, 86] methods fall into this category of algorithms. However, most of the existing methods have been analyzed with limited ﬂexibility only, leaving many potentially fruitful directions unexplored. The most important unexplored questions include i) better understanding of the local shift that aims to correct the ﬁxed point of local methods, ii) support for more sophisticated local gradient estimators that allow for importance sampling, variance reduction, or coordinate descent, iii) variable number of local steps, and iv) general theory supporting multiple data similarity types, including identical, heterogeneous and partially heterogeneous (ζ-heterogeneous - deﬁned later).

Consequently, there is a need for a single framework unifying the theory of local stochastic ﬁrst order methods, ideally one capable of pointing to new and more eﬃcient variants. This is what we do in this work.

Uniﬁcation of stochastic algorithms. There have been multiple recent papers aiming to unify the theory of ﬁrst-order optimization algorithms. The closest to our work is the uniﬁcation of

2Vector gik can be a simple unbiased estimator of ∇fi(xki ), but can also involve a local “shift” designed to correct the (inherently wrong) ﬁxed point of local methods. We elaborate on this point later.

118

(non-local) stochastic algorithms in [55] that proposes a relatively simple yet powerful framework for analyzing variants of SGD that allow for minibatching, arbitrary sampling,3 variance reduction, subspace gradient oracle, and quantization. We recover this framework as a special case in a non-local regime. Next, a framework for analyzing error compensated or delayed SGD methods was recently proposed in [57]. Another relevant approach covers the uniﬁcation of decentralized SGD algorithms [97], which is able to recover the basic variant of Local-SGD as well. While our framework matches their rate for basic Local-SGD, we cover a broader range of local methods in this work as we focus on the centralized setting.
4.1.1 Our Contributions
In this chapter, we propose a general framework for analyzing a broad family of local stochastic gradient methods of the form (4.4). Given that a particular local algorithm satisﬁes a speciﬁc parametric assumption (Assumption 4.2.3) in a certain scenario, we provide a tight convergence rate of such a method.
Let us give a glimpse of our results and their generality. A local algorithm of the form (4.4) is allowed to consist of an arbitrary local stochastic gradient estimator (see Section 4.4 for details), a possible drift/shift to correct for the non-stationarity of local methods4 and a ﬁxed or random local loop size. Further, we provide a tight convergence rate in both the identical and heterogeneous data regimes for strongly (quasi) convex and convex objectives. Consequently, our framework is capable of:
• Recovering known optimizers along with their tight rates. We recover multiple known local optimizers as a special case of our general framework, along with their convergence rates (up to small constant factors). This includes FedAvg/Local-SGD [135, 205] with currently the best-known convergence rate [89, 229, 97, 228] and SCAFFOLD [86]. Moreover, in a special case we recover a general framework for analyzing non-local SGD method developed in [55], and consequently we recover multiple variants of SGD with and without variance reduction, including SAGA [35], L-SVRG [103], SEGA [69], gradient compression methods [139, 79] and many more.
• Filling missing gaps for known methods. Many of the recovered optimizers have only been analyzed under speciﬁc and often limiting circumstances and regimes. Our framework allows us to extend known methods into multiple hitherto unexplored settings. For instance, for each (local) method our framework encodes, we allow for a random/ﬁxed local loop size, identical/heterogeneous/ζ-heterogeneous data (introduced soon), and convex/strongly convex objective.
• Extending the established optimizers. To the best of our knowledge, none of the known
3A tight convergence rate given any sampling strategy and any smoothness structure of the objective. 4Basic local algorithms such as FedAvg/Local-SGD or FedProx [115] have incorrect ﬁxed points [161]. To eliminate this issue, a strategy of adding an extra “drift” or “shift” to the local gradient has been proposed recently [123, 86].
119

local methods have been analyzed under arbitrary smoothness structure of the local objectives5 and consequently, our framework is the ﬁrst to allow for the local stochastic gradient to be constructed via importance (possibly minibatch) sampling. Next, we allow for a local loop with a random length, which is a new development contrasting with the classical ﬁxed-length regime. We discuss advantages of of the random loop in Section 4.3.
• New eﬃcient algorithms. Perhaps most importantly, our framework is powerful enough to point to a range of novel methods. A notable example is S-Local-SVRG, which is a local variance reduced SGD method able to learn the optimal drift. This is the ﬁrst time that local variance reduction is successfully combined with an on-the-ﬂy learning of the local drift. Consequently, this is the ﬁrst method which enjoys a linear convergence rate to the exact optimum (as opposed to a neighborhood of the solution only) without any restrictive assumptions and is thus superior in theory to the convergence of all existing local ﬁrst order methods. We also develop another linearly converging method: S*-Local-SGD*. Albeit not of practical signiﬁcance as it depends on the a-priori knowledge of the optimal solution x∗, it is of theoretical interest as it enabled us to discover S-Local-SVRG. See Table 4.2 which summarizes all our complexity results.
Notation. Due to its generality, our chapter is heavy in notation. For the reader’s convenience, we present a notation table in Section C.1 of the appendix.

4.2 Our Framework

In this section we present the main result of the chapter. Let us ﬁrst introduce the key assumptions that we impose on our objective (6.6). We start with a relaxation of µ-strong convexity (see also Assumptions 2.4.2 and 3.3.1).

Assumption 4.2.1 ((µ, x∗)-strong quasi-convexity). Let x∗ be a minimizer of f . We assume that fi is (µ, x∗)-strongly quasi-convex for all i ∈ [n] with µ ≥ 0, i.e. for all x ∈ Rd:

fi(x∗) ≥ fi(x) + ∇fi(x), x∗ − x + µ x − x∗ 2. 2

(4.5)

Next, we require classical L-smoothness6 of local objectives, or equivalently, L-Lipschitzness of their gradients.

Assumption 4.2.2 (L-smoothness). Functions fi are L-smooth for all i ∈ [n] with L ≥ 0,

i.e.,

∇fi(x) − ∇fi(y) ≤ L x − y , ∀x, y ∈ Rd.

(4.6)

5By this we mean that function fi,j from (4.3) is Mi,j -smooth with Mi,j ∈ Rd×d, Mi,j 0, i.e., for all x, y ∈ Rd we have fi,j(x) ≤ fi,j(y) + ∇fi,j(y), x − y + 12 (x − y) Mi,j(x − y). As an example, logistic regression possesses naturally such a structure with matrices Mi,j of rank 1.
6While we require L-smoothness of fi to establish the main convergence theorem, some of the parameters of Assumption 4.2.3 can be tightened considering a more complex smoothness structure of the local objective.

120

In order to simplify our notation, it will be convenient to introduce the notion of virtual iterates

xk deﬁned as a mean of the local iterates [209]: xk d=ef n1

n i=1

xki

.

Despite

the

fact

that

xk

is

being physically computed only for k for which ck = 1, virtual iterates are a very useful tool

facilitating the convergence analysis. Next, we shall measure the discrepancy between the local

and

virtual

iterates

via

the

quantity

Vk

deﬁned

as

Vk

d=ef

1 n

n i=1

xki − xk

2.

We are now ready to introduce the parametric assumption on both stochastic gradients gik and function f . This is a non-trivial generalization of the assumption from [55] to the class of local stochastic methods of the form (4.4), and forms the heart of this work.7

Assumption 4.2.3 (Key parametric assumption). Assume that for all k ≥ 0 and i ∈ [n], local stochastic directions gik satisfy

n1 n Ek gik = n1 n ∇fi(xki ),

i=1

i=1

(4.7)

where Ek[·] deﬁnes the expectation w.r.t. randomness coming from the k-th iteration only. Fur-
ther, assume that there exist non-negative constants A, A , B, B , C, C , F, F , G, H, D1, D1, D2, D3 ≥ 0, ρ ∈ (0, 1] and a sequence of (possibly random) variables {σk2}k≥0 such that

1n nE
i=1

gik 2 ≤2AE f (xk) − f (x∗) + BE σk2 + F E [Vk] + D1,

 1n

2
k

k

∗

2

E  n gi  ≤2A E f (x ) − f (x ) + B E σk + F E [Vk] + D1,

i=1

E σk2+1 ≤(1 − ρ)E σk2 + 2CE f (xk) − f (x∗) + GE [Vk] + D2,

2L K wkE[Vk] ≤ 12 K wkE f (xk) − f (x∗) + 2LHEσ02 + 2LD3γ2WK ,

k=0

k=0

(4.8) (4.9) (4.10) (4.11)

where sequences {WK }K≥0, {wk}k≥0 are deﬁned as

K
WK d=ef wk,
k=0

def

1

wk = 1 − min γµ, ρ

4

k+1 ,

(4.12)

Admittedly, with its many parameters (whose meaning will become clear from the rest of the chapter), Assumption 4.2.3 is not easy to parse on ﬁrst reading. Several comments are due at this point. First, while the complexity of this assumption may be misunderstood as being problematic, the opposite is true. This assumption enables us to prove a single theorem (Thm. 4.2.4) capturing the convergence behavior, in a tight manner, of all local ﬁrst-order methods described by our framework (4.4). So, the parametric and structural complexity of this assumption is paid for by the uniﬁcation aspect it provides. Second, for each speciﬁc method we

7Recently, the assumption from [55] was generalized in a diﬀerent way to cover the class of the methods with error compensation and delayed updates [57].

121

consider in this work, we prove that Assumption 4.2.3 is satisﬁed, and each such proof is based on much simpler and generally accepted assumptions. So, Assumption 4.2.3 should be seen as a “meta-assumption” forming an intermediary and abstract step in the analysis, one revealing the structure of the inequalities needed to obtain a general and tight convergence result for local ﬁrst-order methods. We dedicate the rest of the chapter to explaining these parameters and to describing the algorithms and the associate rates their combination encodes. We are now ready to present our main convergence result.

Theorem 4.2.4. Let Assumption 4.2.1, 4.2.2 and 4.2.3 be satisﬁed and assume the stepsize

satisﬁes 0 < γ ≤ min 2(A +14CB ) , F +L4GB

3ρ

3ρ

. Deﬁne xK d=ef W1K

K k=0

wk

xk

,

Φ0 d=ef 2 x0−x∗ 2+ 83Bρ γγ2Eσ02+4LHγEσ02 and Ψ0 d=ef 2 D1 + 43Bρ D2 + 2LγD3 . min γµ, ρ4 . Then if µ > 0, we have

Let θ d=ef 1 −

E f (xK ) − f (x∗) ≤θK Φ0 + γΨ0,

(4.13)

and in the case when µ = 0, we have

E

f (xK )

−

f (x∗)

Φ0 ≤

+

γΨ0.

K

(4.14)

As already mentioned, Thm. 4.2.4 serves as a general, uniﬁed theory for local stochastic gradient algorithms. The strongly convex case provides a linear convergence rate up to a speciﬁc neighborhood of the optimum. On the other hand, the weakly convex case yields an O(K−1) convergence rate up to a particular neighborhood. One might easily derive O(K−1) and O(K−1/2) convergence rates to the exact optimum in the strongly and weakly convex case, respectively, by using a particular decreasing stepsize rule. The next corollary gives an example of such a result in the strongly convex scenario, where the estimate of D3 does not depend on the stepsize γ. A detailed result that covers all cases is provided in Section C.3.2 of the appendix.

Corollary

4.2.5.

Consider

the

setup

from

Thm.

4.2.4

and

by

1 ν

denote

the

resulting

upper

bound on γ.a Suppose that µ > 0 and D3 does not depend on γ. Let

γ = min  1 , ln max 2, min Υ1Υµ22K2 , Υ1Υµ33K3  ,

ν

µK



where

Υ1

=

2

x0 − x∗

2+

8B Eσ02 3ν2ρ

+

4LHνEσ02 ,

Υ2

=

2D1 +

4B D2 3ρ

,

Υ3

=

4LD3.

procedure (4.4) achieves

E f (xK ) − f (x∗) ≤ ε

Then, the

as long as

K ≥O

1 + ν log νΥ1 + Υ2 +

ρµ

ε

µε

Υ3 µ2ε .

122

aIn order to get tight estimate of D3 and H, we will impose further bounds on γ (see Tbl. 4.1). Assume that these extra bounds are included in parameter h.
Remark 4.2.6. Admittedly, Thm. 4.2.4 does not yield the tightest known convergence rate in the heterogeneous setup under Assumption 4.2.4. Speciﬁcally, the neighborhood to which Local-SGD converges can be slightly smaller [97]. While we provide a tighter theory that matches the best-known results, we have deferred it to the appendix for the sake of clarity. In particular, to get the tightest rate, one shall replace the bound on the second moment of the stochastic direction (4.8) with two analogous bounds – ﬁrst one for the variance and the second one for the squared expectation. See Assumption C.4.1 for details. Fortunately, Thm. 4.2.4 does not need to change as it does not require parameters from (4.8); these are only used later to derive D3, H, γ based on the data type. Therefore, only a few extra parameters should be determined in the speciﬁc scenario to get the tightest rate.
Remark 4.2.7. As we show in the appendix when looking at particular special cases, local gradient methods are only as good as their non-local counterparts (i.e., when τ = 1) in terms of the communication complexity in the fully heterogeneous setup. Furthermore, the non-local methods outperform local ones in terms of computation complexity. While one might think that this observation is a byproduct of our analysis, our observations are supported by ﬁndings in recent literature on this topic [86, 89]. To rise to the defense of local methods, we remark that they might be preferable to their non-local cousins in the homogeneous data setup [229] or for personalized federated learning [72].
The parameters that drive both the convergence speed and the neighborhood size are determined by Assumption 4.2.3. In order to see through the provided rates, we shall discuss the value of these parameters in various scenarios. In general, we would like to have ρ ∈ (0, 1] as large as possible, while all other parameters are desired to be small so as to make the inequalities as tight as possible.
Let us start with studying data similarity and inner loop type as these can be decoupled from the type of the local direction that the method (4.4) takes.
4.3 Data Similarity and Local Loop
We now explain how our framework supports ﬁxed and random local loop, and several data similarity regimes.
Local loop. Our framework supports local loop of a ﬁxed length τ ≥ 1 (i.e., we support local methods performing τ local iterations in between communications). This option, which is the de facto standard for local methods in theory and practice [135], is recovered by setting caτ = 1 for all non-negative integers a and ck = 0 for k that are not divisible by τ in (4.4). However,
123

Table 4.1: The eﬀect of data similarity and local loop on Assumption 4.2.3. Constant factors

are ignored. Homogeneous data are recovered as a special case of ζ-heterogeneous data with

ζ = 0. Heterogeneous case is slightly loose in light of Remark 4.2.6. If one replaces the bound

on the second moments (4.8) with a analogous bound on variance squared expectation (see

Assumption C.4.1), the bounds on γ, D3 and H will have (τ − 1) times better dependence on

the

variance

parameters

(or

1−p p

times

for

the

random

loop).

See

Section

C.4.1

and

C.4.2

of

appendix for more details.

Data het

Loop ﬁxed

ζ-het ﬁxed

het random ζ-het radnom

Extra upper bounds on γ

τ1µ ,
τ

1

,

F + ρ(B1−Gρ) τ

τ1µ ,

1

,

τ F + ρ(B1−Gρ)

√

p , √ p , √p ρ(1−ρ) ,

µ

(1−p)F

BG(1−p)

µp , F (1p−p) , BpρG((11−−ρp)) ,

1 2L A+ ρ(B1−Cρ)
1 Lτ A+ ρ(B1−Cρ)
p L(1−p) A+ ρ(B1−Cρ)
p L(1−p) A+ ρ(B1−Cρ)

D3 (τ − 1)2 D1 + BDρ 2

(τ − 1) D1 + γζµ2 + BDρ 2

(1−p) D1+ BD ρ 2 p2

(1−p) p

D1 + γζµ2 + BDρ 2

H
B(τ −1)2γ2 ρ
B(τ −1)γ2 ρ
B(1−p)γ2 p2 ρ
B(1−p)γ2 pρ

our framework also captures the very rarely considered local loop with a random length. We recover this when ck are random samples from the Bernoulli distribution Be(p) with parameter p ∈ (0, 1].

Data similarity. We look at various possible data similarity regimes. The ﬁrst option we consider is the fully heterogeneous setting where we do not assume any similarity between the local objectives whatsoever. Secondly, we consider the identical data regime with f1 = . . . = fn. Lastly, we consider the ζ-heterogeneous data setting, which bounds the dissimilarity between the full and the local gradients [228] (see Def. 4.3.1).

Deﬁnition 4.3.1 (ζ-heterogeneous functions). We say that functions f1, . . . , fn are ζ-heterogeneous for some ζ ≥ 0 if the following inequality holds for all x ∈ Rd:
n1 n ∇fi(x) − ∇f (x) 2 ≤ ζ2.
i=1

(4.15)

The ζ-heterogeneous data regime recovers the heterogeneous data for ζ = ∞ and identical data for ζ = 0.
In Section C.4 of the appendix, we show that the local loop type and the data similarity type aﬀect parameters H and D3 from Assumption 4.2.3 only. However, in order to obtain an eﬃcient bound on these parameters, we impose additional constraints on the stepsize γ. While we do not have space to formally state our results in the main body, we provide a comprehensive summary in Tbl. 4.1.
Methods with a random loop communicate once per p−1 iterations on average, while the ﬁxed

124

loop variant communicates once every τ iterations. Consequently, we shall compare the two loop types for τ = p−1. In such a case, parameters D3 and H and the extra conditions on stepsize γ match exactly, meaning that the loop type does not inﬂuence the convergence rate. Having said that, random loop choice provides more ﬂexibility compared to the ﬁxed loop. Indeed, one might want the local direction gik to be synchronized with the communication time-stamps in some special cases. However, our framework does not allow such synchronization for a ﬁxed loop since we assume that the local direction gik follows some stationary distribution over stochastic gradients. The random local loop comes in handy here; the random variable that determines the communication follows a stationary distribution, thus possibly synchronized with the local computations.

4.4 Local Stochastic Direction

This section discusses how the choice of gik allows us to obtain the remaining parameters from

Assumption 4.2.3 that were not covered in the previous section. To cover the most practical

scenarios,

we

set

gik

to

be

a

diﬀerence

of

two

components

a

k i

,

bki

∈

Rd,

which

we

explain

next.

We stress that the construction of gik is very general: we recover various state-of-the-art methods

along with their rates while covering many new interesting algorithms. We will discuss this in

more detail in Section 4.5.

4.4.1 Unbiased Local Gradient Estimator aki
The ﬁrst component of the local direction that the method (4.4) takes is aki – an unbiased, possibly variance reduced, estimator of the local gradient, i.e., Ek[aki ] = ∇fi(xki ). Besides the unbiasedness, aki is allowed to be anything that satisﬁes the parametric recursive relation from [55], which tightly covers many variants of SGD including non-uniform, minibatch, and variance reduced stochastic gradient. The parameters of such a relation are capable of encoding both the general smoothness structure of the objective and the gradient estimator’s properties that include a diminishing variance, for example. We state the adapted version of this recursive relation as Assumption 4.4.1.
Assumption 4.4.1. Let the unbiased local gradient estimator aki be such that

Ek aki − ∇fi(x∗) 2 ≤ 2AiDfi (xki , x∗) + Biσi2,k + D1,i, Ek σi2,k+1 ≤ (1 − ρi)σi2k + 2CiDfi (xki , x∗) + D2,i

for Ai ≥ 0, Bi ≥ 0, D1,i ≥ 0, 0 ≤ ρi ≤ 1, Ci ≥ 0, D2,i ≥ 0 and a non-negative sequence {σi2,k }∞ k=0 .a

aBy Dfi (xki , xk) we mean Bregman distance between xki , xk deﬁned as Dfi (xki , xk) d=ef fi(xki ) − fi(xk) −

∇f

i

(x

k

)

,

x

k i

− xk

.

Note that the parameters of Assumption 4.4.1 can be taken directly from [55] and oﬀer a broad

125

range of unbiased local gradient estimators aki in diﬀerent scenarios. The most interesting setups covered include minibatching, importance sampling, variance reduction, all either under the classical smoothness assumption or under a uniform bound on the stochastic gradient variance.
Our next goal is to derive the parameters of Assumption 4.2.3 from the parameters of Assumption 4.4.1. However, let us ﬁrst discuss the second component of the local direction – the local shift bki .

4.4.2 Local Shift bki

The local update rule (4.4) can include the local shift/drift bki allowing us to eliminate the

infamous non-stationarity of the local methods. The general requirement for the choice of bki

is so that it sums up to zero (

n i=1

bki

= 0)

to

avoid

unnecessary

extra

bias.

For

the

sake

of

simplicity (while maintaining generality), we will consider three choices of bki – zero, ideal shift

(= ∇fi(x∗)) and on-the-ﬂy shift via a possibly outdated local stochastic non-variance reduced

gradient estimator that satisﬁes a similar bound as Assumption 4.4.1.

Assumption 4.4.2. Consider the following choices:

Case I: bki = 0,

Case II: bki = ∇fi(x∗),

Case

III:

bki

=

hki

−

1 n

n i=1

hki

where

hki

∈

Rd

is

a

delayed

local

gradient

estimator

deﬁned

recursively as



hk+1 = hki

i

lk

i

with probability 1 − ρi , with probability ρi

where 0 ≤ ρi ≤ 1 and lik ∈ Rd is an unbiased non-variance reduced possibly stochastic gradient estimator of ∇fi(xk) such that for some Ai, D3,i ≥ 0 we have

Ek lik − ∇fi(x∗) 2 ≤ 2AiDfi (xki , x∗) + D3,i.

(4.16)

Let us look closer at Case III as this one is the most interesting. Note that what we assume about lik (i.e., (4.16)) is essentially a variant of Assumption 4.4.2 with σi2,k parameters set to zero. This is achievable for a broad range of non-variance reduced gradient estimators that includes minibatching and importance sampling [63]. An intuitive choice of lik is to set it to aki given that aki is not variance reduced. In such a case, the scheme (4.4) reduces to SCAFFOLD [86] along with its rate.
However, our framework can do much more beyond this example. First, we cover the local variance reduced gradient aki with lik constructed as its non-variance reduced part. In such a case, the neighborhood of the optimum from Thm. 4.2.4 to which the method (4.4) converges shrinks. There is a way to get rid of this neighborhood, noticing that lik is used only once in a while. Indeed, the combination of the full local gradient lik together with the variance reduced aki leads to a linear rate in the strongly (quasi) convex case or O(K−1) rate in the weakly convex

126

case. We shall remark that the variance reduced gradient might require a sporadic computation of the full local gradient – it makes sense to synchronize it with the update rule for hki . In such a case, the computation of lik is for free. We have just described the S-Local-SVRG method (Algorithm 32).

4.4.3 Parameters of Assumption 4.2.3
We proceed with a key lemma that provides us with the remaining parameters of Assumption 4.2.3 that were not covered in Section 4.3. These parameters will be chosen purely based on the selection of aki and bki discussed earlier.
Lemma 4.4.3. For all i ∈ [n] suppose that aki satisﬁes Assumption 4.4.1, while bki was chosen as per Assumption 4.4.2. Then, (4.8), (4.9) and (4.10) hold with

A = 4 max Ai, B = 2, F = 4L max Ai,

i

i


2
D1 =  n 2
n

n i=1

D1,i +

n i=1

D1,i

∇fi(x∗) 2

Case I, Case II, III,

B = n1 , F = 2L mnaxi Ai + 2L2, D1 = n12 n D1,i
i=1
A = 2 maxi Ai + L, G = CL/2, n


mini ρi ρ=
mini min {ρi, ρi}

Case I, II, Case III,

n

 n2 BiD2,i,

D2 =

i=1 n

 n1 (2BiD2,i + ρiD3,i)

i=1

Case I, II, Case III,


4 maxi{BiCi} C=
4 maxi{BiCi} + 4 maxi{ρiAi}

Case I, II, Case III.

We have just broken down the parameters of Assumption 4.2.3 based on the optimization objective and the particular instance of (4.4). However, it might still be hard to understand particular rates based on these choices. In the appendix, we state a range of methods and decouple their convergence rates. A summary of the key parameters from Assumption 4.2.3 is provided in Tbl. C.4.

4.5 Special Cases
Our theory covers a broad range of local stochastic gradient algorithms. While we are able to recover multiple known methods along with their rates, we also introduce several new methods along with extending the analysis of known algorithms. As already mentioned, our theory

127

covers convex and strongly convex cases, identical and heterogeneous data regimes. From the algorithmic point of view, we cover the ﬁxed and random loop, various shift types, and arbitrary local stochastic gradient estimator. We stress that our framework gives a tight convergence rate under any circumstances.
While we might not cover all of these combinations in a deserved detail, we thoroughly study a subset of them in the following subsections. An overview of these methods is presented in Tbl. 4.2 together with their convergence rates in the strongly convex case (see Tbl. 4.3 for the rates in the weakly convex setting). Next, we describe a selected number of special cases of our framework.
• Non-local stochastic methods. Our theory recovers a broad range of non-local stochastic methods. In particular, if n = 1, we have Vk = 0, and consequently we can choose A = A , B = B , D1 = D1, F = F = G = H = D3 = 0. With such a choice, our theory matches8 the general analysis of stochastic gradient methods from [55] for τ = 1. Consequently, we recover a broad range of algorithms as a special case along with their convergence guarantees, namely SGD [182] with its best-known rate on smooth objectives [156, 63], variance reduced ﬁnite sum algorithms such as SAGA [35], SVRG [82], L-SVRG [77, 103], variance reduced subspace descent methods such as SEGA/SVRCD [69, 71], quantized methods [139, 79] and others.
• “Star”-shifted local methods. As already mentioned, local methods have inherently incorrect ﬁxed points [161]; and one can ﬁx these by shifting the local gradients. Star-shifted local methods employ the ideal stationary shift using the local gradients at the optimum bki = ∇fi(x∗) (i.e., Case II from Assumption 4.4.2) and serve as a transition from the plain local methods (Case I from Assumption 4.4.2) to the local methods that shift using past gradients such as SCAFFOLD (Case III from Assumption 4.4.2). In the appendix, we present two such methods: S*-Local-SGD (Algorithm 29) and S*-Local-SGD* (Algorithm 31). While being impractical in most cases since ∇fi(x∗) is not known, star-shifted local methods give new insights into the role and eﬀect of the shift for local algorithms. Speciﬁcally, these methods enjoy superior convergence rate when compared to methods without local shift (Case I) and methods with a shift constructed from observed gradients (Case III), while their rate serves as an aspiring goal for local methods in general. Fortunately, in several practical scenarios, one can match the rate of star methods using an approach from Case III, as we shall see in the next point.
• Shifted Local SVRG (S-Local-SVRG). As already mentioned, local SGD suﬀers from convergence to a neighborhood of the optimum only, which is credited to i) inherent variance of the local stochastic gradient, and ii) incorrect ﬁxed point of local GD. We propose a way to correct both issues. To the best of our knowledge, this is the ﬁrst time that on-device variance reduction was combined with the trick for reducing the non-stationarity of local methods. Speciﬁcally, the latter is achieved by selecting bki as a particular instance of Case III from Assumption 4.4.2 such that lik is the full local gradient, which in turns yields D1,i = 0, Ai = L. In order to not waste
8Up to the non-smooth regularization/proximal steps and small constant factors.
128

local computation, we synchronize the evaluation of lik with the computation of the full local gradient for the L-SVRG [77, 103] estimator, which we use to construct aki . Consequently, some terms cancel out, and we obtain a simple, fast, linearly converging local SGD method, which we present as Algorithm 32. We believe that this is remarkable since only a very few local methods converge linearly to the exact optimum.9
9A linearly converging local SGD variant can be recovered from stochastic decoupling [142], although this was not considered therein. Besides that, FedSplit [161] achieves a linear rate too, however, with a much stronger local oracle.
129

Table 4.2: A selection of methods that can be analyzed using our framework, which we detail

in the appendix. A choice of aki , bki and lik is presented along with the established complexity bounds (= number of iterations to ﬁnd such xˆ that E[f (xˆ) − f (x∗)] ≤ ε) and a speciﬁc setup

under

which

the

methods

are

analyzed.

For

Algorithms

1-4

we

suppress

constants

and

log

1 ε

factors. Since Algorithms 5 and 6 converge linearly, we suppress constants only while keeping

log

1 ε

factors.

All rates are provided in the strongly convex setting.

UBV stands for the

“Uniform Bound on the Variance” of local stochastic gradient, which is often assumed when fi is

of the form (5.5). ES stands for the “Expected Smoothness” [63], which does not impose any

extra assumption on the objective/noise, but rather can be derived given the sampling strategy

and the smoothness structure of fi. Consequently, such a setup allows us to obtain local methods

with importance sampling. Next, the simple setting is a special case of ES when we uniformly

sample a single index on each node each iteration. ♣: Local-SGD methods have never been

analyzed under ES assumption. Notation: σ2 – averaged (within nodes) uniform upper bound

for the variance of local stochastic gradient, σ∗2 – averaged variance of local stochastic gradients

at

the

solution,

ζ∗2

d=ef

1 n

n i=1

∇fi(x∗) 2, max Lij – the worst smoothness of fi,j, i ∈ [n], j ∈ [m],

L – the worst ES constant for all nodes.

Method Local-SGD Alg. 27, [228] Local-SGD Alg. 27, [97] Local-SGD Alg. 27, [89]♣
Local-SGD Alg. 27, [89]♣
Local-SVRG Alg. 28, (NEW)
Local-SVRG Alg. 28, (NEW)
S*-Local-SGD Alg. 29, (NEW) SS-Local-SGD
Alg. 30, [86] SS-Local-SGD Alg. 30, (NEW)
S*-Local-SGD* Alg. 31, (NEW)
S-Local-SVRG Alg. 32, (NEW)

aki , bki , lik

f

ξ

i

(x

k i

)

,

0,

−

f

ξ

i

(x

k i

)

,

0,

−

f

ξ

i

(x

k i

)

,

0,

−

f

ξ

i

(x

k i

)

,

0,

−

∇fi,ji (xki ) − ∇fi,ji (yik) +∇fi (yik ), 0, −
∇fi,ji (xki ) − ∇fi,ji (yik) +∇fi (yik ), 0, −

fξi (xki ), ∇fi(x∗), −

fξi (xki ), hki

−

1 n

∇

f

ξ˜k

(y

k i

)

i

fξi (xki ), hki

−

1 n

∇

f

ξ˜k

(y

k i

)

i

ni=1 hki , ni=1 hki ,

∇fi,ji (xki ) − ∇fi,ji (x∗) +∇fi(x∗), ∇fi(x∗), −

∇fi,ji (xki ) − ∇fi,ji (yk)

+∇fi (yk ),

hki − n1

n i=1

hki

,

∇fi(yk

)

Complexity

L + σ2 +
µ nµε

Lτ (σ2+τ ζ2) µ2 ε

τL + σ2 +

µ

nµε

L(τ −1)(σ2+(τ −1)ζ∗2) µ2 ε

√

L+L/n+ (τ −1)LL + σ∗2

µ

nµε

+ Lζ2(τ −1) + L(τ −1)(σ∗2+ζ∗2)

µ2ε √

µ2 ε

Lτ +L/n+ (τ −1)LL + σ∗2

µ

nµε

m+

+ L(τ −1)(σµ∗22+ε(τ −1)ζ∗2) √
L+max Lij/n+ (τ −1)L max Lij µ

+ Lζ2µ(2τε−1) +

L(τ −1)ζ∗2 µ2 ε

m+

√
Lτ +max Lij/n+ (τ −1)L max Lij µ
+ L(τ µ−21ε)2ζ∗2

τL + σ2 +

µ

nµε

L(τ −1)σ2 µ2 ε

L + σ2 +
pµ nµε

L(1−p)σ2 pµ2 ε

√

L + L + LL(1−p)

pµ nµ

pµ

+ σ∗2 +
nµε

L(1−p)σ∗2 pµ2 ε

τ L + max Lij

µ

n√µ

+ (τ −1)L max Lij

µ

log 1ε

m + L + max Lij pµ √ nµ + L max Lij (1−p)
pµ

log 1ε

Setting UBV, ζ-Het UBV, Het ES, ζ-Het
ES, Het

Sec 4.5.1 4.5.1 4.5.1 4.5.1

simple, ζ-Het

4.5.2

simple, Het
UBV, Het UBV, Het ES, Het
simple, Het

4.5.2 4.5.3 4.5.4 4.5.4 4.5.5

simple, Het

4.5.6

130

Table

4.3:

A

selection

of

methods

that

can

be

analyzed

using

our

framework.

A

choice

of

a

k i

,

bki

and lik is presented along with the established complexity bounds (= number of iterations to ﬁnd

such xˆ that E[f (xˆ) − f (x∗)] ≤ ε) and a speciﬁc setup under which the methods are analyzed.

For all algorithms we suppress constants factors. All rates are provided in the weakly convex

setting. UBV stands for the “Uniform Bound on the Variance” of local stochastic gradient, which

is often assumed when fi is of the form (5.5). ES stands for the “Expected Smoothness” [63],

which does not impose any extra assumption on the objective/noise, but rather can be derived

given the sampling strategy and the smoothness structure of fi. Consequently, such a setup

allows us to obtain local methods with importance sampling. Next, the simple setting is a

special case of ES when we uniformly sample a single index on each node each iteration. ♣:

Local-SGD methods have never been analyzed under ES assumption. Notation: σ2 – averaged

(within nodes) uniform upper bound for the variance of local stochastic gradient, σ∗2 – averaged

variance

of

local

stochastic

gradients

at

the

solution,

ζ∗2

d=ef

1 n

n i=1

∇fi(x∗) 2, max Lij – the

worst smoothness of fi,j, i ∈ [n], j ∈ [m], L – the worst ES constant for all nodes, R0 d=ef x0 − x∗

– distance of the starting point x0 from the closest solution x∗, ∆0 d=ef f (x0) − f (x∗).

Method Local-SGD Alg. 27, [228] Local-SGD Alg. 27, [97] Local-SGD Alg. 27, [89]♣
Local-SGD Alg. 27, [89]♣
Local-SVRG Alg. 28, (NEW)
Local-SVRG Alg. 28, (NEW)
S*-Local-SGD Alg. 29, (NEW) SS-Local-SGD
Alg. 30, [86]

aki , bki , lik

f

ξ

i

(x

k i

),

0,

−

f

ξ

i

(x

k i

),

0,

−

f

ξ

i

(x

k i

),

0,

−

f

ξ

i

(x

k i

),

0,

−

∇fi,ji (xki ) − ∇fi,ji (yik) +∇fi (yik ), 0, −
∇fi,ji (xki ) − ∇fi,ji (yik) +∇fi (yik ), 0, −

f

ξ

i

(x

k i

)

,

∇

f

i

(

x

∗

),

−

fξi (xki ), hki

−

1 n

ni=1 hki ,

∇

f

ξ˜k

(y

k i

)

i

SS-Local-SGD Alg. 30, (NEW)

fξi (xki ), hki

−

1 n

ni=1 hki ,

∇

f

ξ˜k

(y

k i

)

i

S*-Local-SGD* Alg. 31, (NEW)
S-Local-SVRG Alg. 32, (NEW)

∇fi,ji (xki ) − ∇fi,ji (x∗) +∇fi(x∗), ∇fi(x∗), −

∇fi,ji (xki ) − ∇fi,ji (yk)

+∇fi (yk ),

hki − n1

n i=1

hki

,

∇fi(yk

)

Complexity

LR02 + σ2R02

ε

nε2 √

+

R

2 0

Lτ (σ2+τ ζ2) ε3/2

τ LR02 + σ2R02

ε

nε√2

+

R

2 0

√

L(τ −1)(σ2+(τ −1)ζ∗2) ε3/2

L+L/n+ (τ −1)LL R02 σ2R2

ε

√

+

∗0 nε2

+ Lζ2(µτε−21)√R02 + R02

L(τ −1)(σ∗2+ζ∗2) ε3/2

Lτ +L/n+ (τ −1)LL R02 σ2R2

√ε

+

∗0 nε2

+

R

2 0

√

L(τ −1)(σ∗2+(τ −1)ζ∗2)
√ ε3/2

L+max Lij m/n+ (τ −1)L max Lij R02

√

ε

3 (τ −1)mLε max√Lij R02 + Lζ2(µτε−21)R02

+

R

2 0

L(τ −1)ζ∗2 ε3/2

√√

Lτ +max Lij m/n+ (τ −1)L max Lij R02

√

ε

√

3 (τ −1)mL max Lij R02 + R02
ε

L(τ −1)2ζ∗2 ε3/2

√

τ LR02 + σ2R02 + R02 L(τ −1)σ2

ε

nε2

ε3/2

√

LR02 + σ2R02 + R02 L(1−p)σ2

pε

nε2

p1/2 ε3/2

√
L+pL/n+ p(1−p)LL R02

pε

3 (1−p)L(L+pL)R04∆0

+

pε

3 (1−p)Lσ∗2 R04 σ2 R2

+

p2/√3 ε

+

∗0 nε2

+ R02 p1L/2(1ε−3/p2)σ∗2

(Lτ +max Lij/n+√(τ −1)L max Lij )R02

ε

L+pL√m/n+√(1−p)L max Lij R02

pε

R02 3 L max L2ij

+

p2/3 ε

Setting UBV, ζ-Het UBV, Het
ES, ζ-Het
ES, Het

Sec 4.5.1 4.5.1 4.5.1 4.5.1

simple, ζ-Het

4.5.2

simple, Het
UBV, Het UBV, Het

4.5.2 4.5.3 4.5.4

ES, 4.5.4
Het

simple, Het
simple, Het

4.5.5 4.5.6

131

4.5.1 Local-SGD

We start with the analysis of Local-SGD (see Algorithm 27) under diﬀerent assumptions of stochastic gradients and data similarity.

Algorithm 27 Local-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd, communication period τ ≥ 1

1: for k = 0, 1, . . . do

2: for i = 1, . . . , n in parallel do

3:

Sample gik = ∇fξk (xki ) independently from other nodes

i

4:

if k + 1 mod τ = 0 then

n

5:

xki +1

=

xk+1

=

1 n

xki − γgik

i=1

6:

else

7:

xki +1 = xki − γgik

8:

end if

9: end for

10: end for

averaging local update

Uniformly Bounded Variance

In this section we assume that fi has a form of expectation (see (5.5)) and stochastic gradients

∇fξi(x) satisfy Eξi ∇fξi(x) − ∇fi(x) 2 ≤ D1,i, ∀ x ∈ Rd, ∀ i ∈ [n].

(4.17)

We also introduce the average variance σ2 and the parameter of heterogeneity at the solution ζ∗2

in the following way:

σ2 = n1 n D1,i,
i=1

ζ∗2 = n1 n ∇fi(x∗) 2.
i=1

Lemma 4.5.1. Assume that functions fi are convex and L-smooth for all i ∈ [n]. Then

n1 n ∇fi(xki ) 2 ≤ 6L f (xk) − f (x∗) + 3L2Vk + 3ζ∗2
i=1

(4.18)

and

1n

2 k

k

∗

2

n ∇fi(xi ) ≤ 4L f (x ) − f (x ) + 2L Vk.

(4.19)

i=1

132

Proof. First, to show (4.18) we shall have

n1 n ∇fi(xki ) 2
i=1

(A.11)
≤
(4.6),(C.3)
≤ =

n3 n ∇fi(xki ) − ∇fi(xk) 2 + n3 n ∇fi(xk) − ∇fi(x∗) 2

i=1

i=1

+3 n n

∇fi(x∗) 2

i=1

3L2 n n

xki − xk 2 + 6nL n Dfi (xk, x∗) + 3ζ∗2

i=1

i=1

6L f (xk) − f (x∗) + 3L2Vk + 3ζ∗2.

Next, to establish (4.19), we have

1n

2 k

n ∇fi(xi )

i=1

=
(A.11)
≤
(4.6),(C.3)
≤
=

1n

k

2 ∗

n ∇fi(xi ) − ∇fi(x )

i=1

n2 n ∇fi(xki ) − ∇f (xk) 2 + n2 n ∇fi(xk) − ∇f (x∗) 2

i=1

i=1

2L2 n n

xki − xk 2 + 4nL n Dfi (xk, x∗)

i=1

i=1

4L f (xk) − f (x∗) + 2L2Vk.

Lemma 4.5.2. Let fi be convex and L-smooth for all i ∈ [n]. Then for all k ≥ 0

1n nE
i=1

gik 2 | xk

≤ 6L f (xk) − f (x∗) + 3L2Vk + σ2 + 3ζ∗2,

1n nE
i=1

gik − g¯ik 2 | xk

≤ σ2,

 1n

2 k


k

k

∗

2

σ2

E  n gi | x  ≤ 4L f (x ) − f (x ) + 2L Vk + n ,

i=1

where E[· | xk] d=ef E[· | xk1, . . . , xkn].

(4.20) (4.21) (4.22)

Proof. First of all, we notice that g¯ik = E gik | xk = ∇fi(xki ). Using this we get

1n nE
i=1

gik − g¯ik

1n

k

n E gi

i=1

2 | xki 2 | xki

= n1 n Eξik ∇fξik (xki ) − ∇fi(xki ) 2 (6≤.7) n1 n D1,i,

i=1

i=1

(A=.14)

n1 n Eξik ∇fξik (xki ) − ∇fi(xki ) 2 + n1 n ∇fi(xki ) 2

i=1

i=1

(6.7),(4.18)
≤

6L f (xk) − f (x∗)

+ 3L2Vk + n1 n

D1,i + 3 ∇fi(x∗) 2 .

i=1

133

Finally,

using

independence

of

g1k

,

g2k

,

.

.

.

,

g

k n

we

obtain

 1n

2 k


k

E  n gi | x 

i=1

(A.14)
≤
=
(6.7),(4.19)
≤



1n E n

gik − ∇fi(xki )

i=1

2 | xk +

1n

2 k

n ∇fi(xi )

i=1

1n n2 E
i=1

k

k2 k

1n

2 k

gi − ∇fi(xi ) | xi + n ∇fi(xi )

i=1

4L f (xk) − f (x∗) + 2L2Vk + n12 n D1,i.
i=1

Heterogeneous Data Applying Corollary C.4.3 and Lemmas 4.5.1 and 4.5.2 we get the following result.
Theorem 4.5.3. Assume that fi(x) is µ-strongly convex and L-smooth for every i ∈ [n]. Then Local-SGD satisﬁes Assumption C.4.1 with

A = 3L, A = 0, B = B = 0, F = 3L2, F = 0, D1 = 3ζ∗2, D = σ2,

A = 2L,

B = 0,

F = 2L2,

σ2

D1 =

, n

σk2 ≡ 0,

ρ = 1,

C = 0,

G = 0,

D2 = 0,

H = 0, D3 = 2e(τ − 1) 3(τ − 1)ζ∗2 + σ2

with γ satisfying

γ ≤ min 1 , √ 1 . 4L 4 6e(τ − 1)L

and for all K ≥ 0

E f (xK ) − f (x∗) ≤ 2 x0 − x∗ 2 + 2γ σ2/n + 4eL(τ − 1)γ σ2 + 3(τ − 1)ζ2 .

γWK

∗

In particular, if µ > 0 then

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 γ
+2γ σ2/n + 4eL(τ − 1)γ σ2 + 3(τ − 1)ζ∗2

(4.23)

and when µ = 0 we have E f (xK ) − f (x∗) ≤ 2 x0γ−Kx∗ 2 + 2γ σ2/n + 4eL(τ − 1)γ σ2 + 3(τ − 1)ζ∗2 (.4.24)
The theorem above together with Lemma A.5.3 implies the following result.

134

Corollary 4.5.4. Let assumptions of Theorem 4.5.3 hold with µ > 0. Then for

1

1

γ

=

min

4L ,

√ 4 6e(τ

−

, 1)L

γK

,

ln max 2, min x0−x∗ 2nµ2K2/σ2, x0−x∗ 2µ3K3/4eL(τ −1)(σ2+3(τ −1)ζ∗2) γK =
µK

for all K such that

either or

µγK ≤ 1 min 1 , √ 1
4L 4 6e(τ − 1)L

≤ γK

we have that E f (xK) − f (x∗) equals

O

τ L x0 − x∗ 2 exp

µ −K

+

σ2

+ L(τ − 1) σ2 + (τ − 1)ζ∗2

.

τL

nµK

µ2 K 2

(4.25)

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 τL σ2
O + + µ nµε



L(τ − 1) (σ2 + (τ − 1)ζ∗2)

µ2ε



iterations/oracle calls per node and τ times less communication rounds.

Now we consider some special cases. First of all, if D1,i = 0 for all i ∈ [n], i.e. gik = ∇fi(xki ) almost surely, then our result implies that for Local-SGD it is enough to perform

 OτL +
µ


L(τ − 1)2ζ∗2 µ2ε 

iterations in order to achieve E f (xK) − f (x∗) ≤ ε. It is clear that for this scenario the optimal choice for τ is τ = 1 which recovers10 the rate of Gradient Descent.
Secondly, if τ = 1 then we recover the rate of parallel SGD:

O L + σ2 µ nµε

communication rounds/oracle calls per node

in order to achieve E f (xK) − f (x∗) ≤ ε. Finally, our result gives a negative answer to the following question: is Local-SGD always worse

10We notice that for this particular case our analysis doesn’t give extra logarithmical factors if we apply (4.23) instead of (4.25).

135

then Parallel Minibatch SGD (PMSGD) for heterogeneous data? To achieve E f (xK) − f (x∗) ≤ ε Local-SGD requires

 τL σ2
O + + µ nµε



L(τ − 1) (σ2 + (τ − 1)ζ∗2)

µ2ε



oracle calls per node.

It means that if

σ2

≥ 1 for given τ > 1 and ε and σ2 are such that the ﬁrst

n L(τ −1)(σ2+(τ −1)ζ∗2)ε

term in the complexity bound is dominated by other terms, then the second term corresponding

to the complexity of PMSGD dominates the third term. Informally speaking, if the variance is

large or ε is small then Local-SGD with τ > 1 has the same complexity bounds as PMSGD.

Combining Theorem 4.5.3 and Lemma A.5.6 we derive the following result for the convergence of Local-SGD in the case when µ = 0.

Corollary 4.5.5. Let assumptions of Theorem 4.5.3 hold with µ = 0. Then for

 γ = min  1 , √ 1 ,
 4L 4 6e(τ − 1)L



nR02 , 3

R02

 ,

σ2K 4eL(τ − 1) (σ2 + (τ − 1)ζ∗2) K 

where R0 = x0 − x∗ , we have that

 E f (xK ) − f (x∗) = O  τ LR02 +
K

R2σ2

3

LR04(τ

−

1) (σ2

+

(τ

−

 1)ζ∗2)

0+ nK

K2/3  .

(4.26)

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

O τ LR02 + R02σ2 + R02 L(τ − 1) (σ2 + (τ − 1)ζ∗2)

ε

nε2

ε3/2

iterations/oracle calls per node and τ times less communication rounds.

Homogeneous Data

In this case we modify the approach a little bit and apply the following result.

Lemma 4.5.6 (Lemma 1 from [89]). Under the homogeneous data assumption for Local-SGD

we have

E [Vk] ≤ (τ − 1)γ2σ2

(4.27)

for all k ≥ 0.

Using this we derive the following inequality for the weighted sum of Vk:

K

K

2L wkE[Vk] ≤ 2L(τ − 1)γ2σ2 wk = 2L(τ − 1)γ2σ2WK .

k=0

k=0

136

Together with Lemmas 4.5.1 and 4.5.2 and Theorem 4.2.4 it gives the following result.

Theorem 4.5.7. Assume that f (x) is µ-strongly convex and L-smooth and f1 = . . . = fn = f . Then Local-SGD satisﬁes Assumption 4.2.3 with

A = 3L, B = 0, F = 3L2, D1 = σ2, A = 2L, σk2 ≡ 0, ρ = 1, C = 0, G = 0, D2 = 0,

B = 0, H = 0,

F = 2L2,

σ2

D1 =

, n

D3 = (τ − 1)σ2

with γ satisfying
and for all K ≥ 0 E f (xK ) − f (x∗)
In particular, if µ > 0 then

γ ≤ 1. 4L
≤ 2 x0 − x∗ 2 + 2γ σ2/n + 2L(τ − 1)γσ2 . γWK

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 + 2γ σ2/n + 2L(τ − 1)γσ2 γ

and when µ = 0 we have

E f (xK ) − f (x∗)

≤ 2 x0 − x∗ 2 + 2γ σ2/n + 2L(τ − 1)γσ2 . γK

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.8. Let assumptions of Theorem 4.5.7 hold with µ > 0. Then for

γ = min

1 ln (max {2, min { x0−x∗ 2nµ2K2/σ2, x0−x∗ 2µ3K3/2L(τ −1)σ2}}) 4L , µK

(4.28) (4.29)

for all K such that

either or

ln (max {2, min { x0−x∗ 1 ≤ ln (max {2, min { 4L

2nµ2K2/σ2, x0−x∗
K x0−x∗ 2nµ2K2/σ2,
µK

2µ3K3/2L(τ −1)σ2}}) ≤ 1 x0−x∗ 2µ3K3/2L(τ −1)σ2}})

we have that

E f (xK ) − f (x∗) = O

L x0 − x∗ 2 exp

µ −K

+

σ2

+ L(τ − 1)σ2

.

L

nµK

µ2 K 2

(4.30)

137

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 O  L ln
µ

L x0 − x∗ 2 ε

+ σ2 + nµε

 L(τ − 1)σ2
µ2ε 

iterations/oracle calls per node and τ times less communication rounds.

It means that if nσ2L2 ε ≥ 1, τ ≤ 1 + nσ2L2 ε and ε and σ2 are such that the ﬁrst term in the complexity bound is dominated by other terms, then the second term corresponding to the complexity of PMSGD dominates the third term. Informally speaking, if the variance is large or ε is small then Local-SGD with τ > 1 has the same complexity bounds as PMSGD.

Combining Theorem 4.5.7 and Lemma A.5.6 we derive the following result for the convergence of Local-SGD in the case when µ = 0.

Corollary 4.5.9. Let assumptions of Theorem 4.5.7 hold with µ = 0. Then for

 γ = min  1 ,
 4L



nR02 , 3

R02

 ,

σ2K 2L(τ − 1)σ2K 

where R0 = x0 − x∗ , we have that

 E f (xK ) − f (x∗) = O  LR02 +
K

R2σ2

3

LR04(τ

−

 1)σ2

0+ nK

K2/3  .

(4.31)

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

O LR02 + R02σ2 + R02 L(τ − 1)σ2

ε

nε2

ε3/2

iterations/oracle calls per node and τ times less communication rounds.

ζ-Heterogeneous Data

In this setup we also use an external result to bound E[Vk].

Lemma 4.5.10 (Lemma 8 from [228]). If f1, f2, . . . , fn are ζ-heterogeneous then for Local-SGD

we have

E [Vk] ≤ 3τ γ2σ2 + 6τ 2γ2ζ2

(4.32)

for all k ≥ 0.

138

Using this we derive the following inequality for the weighted sum of Vk:

K

K

2L wkE[Vk] ≤ 6τ Lγ2 σ2 + 2τ ζ2

wk = 6τ Lγ2 σ2 + 2τ ζ2 WK .

k=0

k=0

Together with Lemmas 4.5.1 and 4.5.2 and Theorem 4.2.4 it gives the following result.

Theorem 4.5.11. Assume that f1, . . . , fn are ζ-heterogeneous, µ-strongly convex and Lsmooth functions. Then Local-SGD satisﬁes Assumption 4.2.3 with

A = 3L,

B = 0,

F = 3L2,

D1 = σ2 + 3ζ∗2,

A = 2L,

B = 0,

F = 2L2,

σ2

D1 =

, n

σk2 ≡ 0, ρ = 1, C = 0, G = 0, D2 = 0, H = 0, D3 = 3τ σ2 + 2τ ζ2

with γ satisfying γ ≤ 1. 4L
and for all K ≥ 0 E f (xK ) − f (x∗) ≤ 2 x0 − x∗ 2 + 2γ σ2/n + 6Lτ γ σ2 + 2τ ζ2 . γWK
In particular, if µ > 0 then

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 + 2γ σ2/n + 6Lτ γ σ2 + 2τ ζ2 γ

and when µ = 0 we have

E f (xK ) − f (x∗) ≤ 2 x0 − x∗ 2 + 2γ σ2/n + 6Lτ γ σ2 + 2τ ζ2 . γK

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.12. Let assumptions of Theorem 4.5.11 hold with µ > 0. Then for

γ = min

1 ln (max {2, min { x0−x∗ 2nµ2K2/σ2, x0−x∗ 2µ3K3/6Lτ (σ2+2τ ζ2)}}) 4L , µK

(4.33) (4.34)

for all K such that

either or

ln (max {2, min { x0−x∗ 1 ≤ ln (max {2, min { 4L

2nµ2K2/σ2, x0−x∗
K x0−x∗ 2nµ2K2/σ2,
µK

2µ3K3/6Lτ (σ2+2τ ζ2)}}) ≤ 1 x0−x∗ 2µ3K3/6Lτ (σ2+2τ ζ2)}})

139

we have that

E f (xK ) − f (x∗) = O

L x0 − x∗ 2 exp

µ −K

+

σ2

+ Lτ (σ2 + τ ζ2)

.

L

nµK

µ2 K 2

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 O  L ln
µ

L x0 − x∗ 2 ε

+ σ2 + nµε

 Lτ (σ2 + τ ζ2)
µ2ε 

(4.35)

iterations/oracle calls per node and τ times less communication rounds.

Combining Theorem 4.5.11 and Lemma A.5.6 we derive the following result for the convergence of Local-SGD in the case when µ = 0.

Corollary 4.5.13. Let assumptions of Theorem 4.5.11 hold with µ = 0. Then for

 γ = min  1 ,
 4L



nR02 , 3

R02

 ,

σ2K 6Lτ (σ2 + 2τ ζ2)K 

where R0 = x0 − x∗ , we have that

 E f (xK ) − f (x∗) = O  LR02 +
K

R2σ2

3

LR04τ (σ2

+

 τ ζ2)

0+ nK

K2/3  .

(4.36)

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

O LR02 + R02σ2 + R02 Lτ (σ2 + τ ζ2)

ε

nε2

ε3/2

iterations/oracle calls per node and τ times less communication rounds.

Expected Smoothness and Arbitrary Sampling

In this section we continue our consideration of Local-SGD but now we make another assumption on stochastic gradients ∇fξi(x).

Assumption 4.5.14 (Expected Smoothness). We assume that for all i ∈ [n] stochastic

gradients ∇fξi(x) are unbiased estimators of ∇fi(x) and there exists such constant L > 0 that

∀x, y ∈ Rd

Eξi∼Di ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)

(4.37)

where Dfi(x, y) d=ef fi(x) − fi(y) − ∇fi(y), x − y .

In particular, let us consider the following special case. Assume that fi(x) has a form of ﬁnite

140

sum (see (4.3)) and consider the following stochastic reformulation:

fi(x) = Eξi [fξi (x)] ,

1m fξi (x) = m ξi,jfi,j(x),
j=1

(4.38)

where E[ξi,j] = 1 and E[ξi2,j] < ∞. In this case, Eξi[∇fξi] = ∇fi(x). If each fi,j(x) is Li,j-smooth then there exists such L ≤ maxj∈[m] Li,j that Assumption 4.5.14 holds. Clearly, L depends on the sampling strategy and in some cases one can make L much smaller than maxj∈[m] Li,j via good choice of this strategy. Our analysis works for an arbitrary sampling strategy that satisﬁes Assumption 4.5.14.

Lemma 4.5.15. Let fi be convex and L-smooth for all i ∈ [n]. Then for all k ≥ 0

1n

k

n E gi

i=1

1n nE
i=1

gik − g¯ik

2 | xk 2 | xk

≤ 8L f (xk) − f (x∗) + 4LLVk + 2σ∗2 + 2ζ∗2, ≤ 8L f (xk) − f (x∗) + 4LLVk + 2σ∗2,

(4.39) (4.40)

 1n

2 k


k

k

∗

E  n gi | x  ≤ 4 (2L/n + L) (f (x ) − f (x )) + 2L (2L/n + L) Vk

i=1

+

2σ

2 ∗

,

n

(4.41)

where

σ∗2

=

1 n

x

k 1

,

.

.

.

,

x

k n

].

n
i=1 E

∇fξi (x∗) − ∇fi(x∗)

2,

ζ∗2

=

1 n

n i=1

∇fi(x∗) 2 and E[· | xk] d=ef E[· |

Proof. First of all, we notice that g¯ik = E gik | xk = ∇fi(xki ). Using this we get

1n nE
i=1

gik 2 | xk

(A.11)
≤
(4.37),(A.14)
≤
(C.47)
≤

n2 n Eξik ∇fξik (xki ) − ∇fξik (x∗) 2 + n2 n Eξik ∇fξik (x∗) 2

i=1

i=1

4nL n Dfi (xki , x∗) + n2 n Eξi

i=1

i=1

∇fξi (x∗) − ∇fi(x∗) 2

+2 n n

∇fi(x∗) 2

i=1

8L f (xk) − f (x∗) + 4LLVk + 2σ∗2 + 2ζ∗2

141

and

1n nE
i=1

gik − g¯ik 2 | xk

=
(A.14)
≤
(A.11)
≤
(4.37)
≤
(C.47)
≤

n1 n Eξik ∇fξik (xki ) − ∇fi(xki ) 2
i=1
n1 n Eξik ∇fξik (xki ) − ∇fi(x∗) 2
i=1
n2 n Eξik ∇fξik (xki ) − ∇fξik (x∗) 2
i=1
+ n2 n Eξik ∇fξik (x∗) − ∇fi(x∗) 2
i=1
4nL n Dfi (xki , x∗) + 2σ∗2
i=1
8L f (xk) − f (x∗) + 4LLVk + 2σ∗2.

(4.42)

Finally,

using

independence

of

ξ1k

,

ξ2k

,

.

.

.

,

ξ

k n

we

obtain

 1n

2 k


k

E  n gi | x 

i=1

(A=.14)
=
(4.42),(4.19)
≤



1n

k

2
k

Eξik  n

(∇fξk (xi ) − ∇fi(xi ))  + i

i=1

1n

2 k

n ∇fi(xi )

i=1

1n n2 Eξik
i=1

k

k2

1n

2 k

∇fξk (xi ) − ∇fi(xi ) i

+n

∇fi(xi )

i=1

4 (2L/n + L) (f (xk) − f (x∗)) + 2L (2L/n + L) Vk + 2nσ∗2 .

Heterogeneous Data Applying Corollary C.4.3 and Lemmas 4.5.1 and 4.5.15 we get the following result.
Theorem 4.5.16. Assume that fi(x) is µ-strongly convex and L-smooth for i ∈ [n]. Let Assumption 4.5.14 holds. Then Local-SGD satisﬁes Assumption C.4.1 with

A = 3L,

A = 4L, B = B = 0, F = 3L2, F = 4LL, D1 = 3ζ∗2,

A = 4L + 2L, n σk2 ≡ 0,

B = 0,

F = 4LL + 2L2, n

D1 = 2nσ∗2 ,

ρ = 1, C = 0, G = 0, D2 = 0,

H = 0, D3 = 2e(τ − 1) 2σ∗2 + 3(τ − 1)ζ∗2

D1 = 2σ∗2

with γ satisfying

γ ≤ min 1 , 1 . 8L/n + 4L 4 2eL(τ − 1) (3L(τ − 1) + 4L)

142

and for all K ≥ 0

E f (xK ) − f (x∗) ≤ 2 x0 − x∗ 2 + 2γ 2σ∗2/n + 4eL(τ − 1)γ 2σ2 + 3(τ − 1)ζ2 .

γWK

∗

∗

In particular, if µ > 0 then

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 γ
+2γ 2σ∗2/n + 4eL(τ − 1)γ 2σ∗2 + 3(τ − 1)ζ∗2

(4.43)

and when µ = 0 we have

E f (xK ) − f (x∗)

2 x0 − x∗ 2 ≤
γK

+2γ 2σ∗2/n + 4eL(τ − 1)γ 2σ∗2 + 3(τ − 1)ζ∗2 .

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.17. Let assumptions of Theorem 4.5.16 hold with µ > 0. Then for

(4.44)

1

1

γ0 = min 8L/n + 4L , 4 2eL(τ − 1) (3L(τ − 1) + 4L) ,

ln max 2, min n x0−x∗ 2µ2K2/2σ∗2, x0−x∗ 2µ3K3/4eL(τ −1)γ(2σ∗2+3(τ −1)ζ∗2) γ = min γ0,
µK

for all K such that

either or

ln max 2, min n x0−x∗ 2µ2K2/2σ∗2, x0−x∗ 2µ3K3/4eL(τ −1)γ(2σ∗2+3(τ −1)ζ∗2) K

≤1

ln max 2, min n x0−x∗ 2µ2K2/2σ∗2, x0−x∗ 2µ3K3/4eL(τ −1)γ(2σ∗2+3(τ −1)ζ∗2) γ0 ≤
µK

we have that E f (xK) − f (x∗) is of the order

O Lτ + L/n + (τ − 1)LL R2 exp −

µ

K

0

Lτ + L/n + (τ − 1)LL

+ σ∗2 + L(τ − 1) σ∗2 + (τ − 1)ζ∗2 ,

nµK

µ2 K 2

where R0 = x0 − x∗ . That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 O  Lτ + L +
µ nµ

(τ − 1)LL + σ∗2 +

µ

nµε



L(τ − 1) (σ∗2 + (τ − 1)ζ∗2)

µ2ε



143

iterations/oracle calls per node and τ times less communication rounds.
Combining Theorem 4.5.16 and Lemma A.5.6 we derive the following result for the convergence of Local-SGD in the case when µ = 0.

Corollary 4.5.18. Let assumptions of Theorem 4.5.16 hold with µ = 0. Then for

1

1

γ0 = min 8L/n + 4L , 4 2eL(τ − 1) (3L(τ − 1) + 4L) ,


 γ = min γ0,




nR02 , 3

R02

 ,

2σ∗2K 4eL(τ − 1) (2σ∗2 + 3(τ − 1)ζ∗2) K 

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) equals

 Lτ + L/n +

(τ − 1)LL R02

O K +

R2σ2

3

LR04(τ

−

1)

(σ

2 ∗

+

(τ

−

 1)ζ∗2)

0 ∗+ nK

K2/3  .

(4.45)

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 Lτ + L/n +
O

(τ − 1)LL R02 R02σ∗2 R02

ε

+ nε2 +



L(τ − 1) (σ∗2 + (τ − 1)ζ∗2)

ε3/2



iterations/oracle calls per node and τ times less communication rounds.

ζ-Heterogeneous Data Applying Corollary C.4.5 and Lemma 4.5.15 we get the following result.

Theorem 4.5.19. Assume that fi(x) is L-smooth for i ∈ [n] and f1, . . . , fn are ζ-heterogeneous and µ-strongly convex. Let Assumption 4.5.14 holds. Then Local-SGD satisﬁes Assumption 4.2.3 with

A = 4L, B = 0, F = 4LL, D1 = 2σ∗2 + 2ζ∗2,

A = 4L + 2L, n σk2 ≡ 0,

B = 0,

F = 4LL + 2L2, n

D1 = 2nσ∗2 ,

ρ = 1, C = 0, G = 0, D2 = 0,

H = 0, D3 = 2(τ − 1) 2σ∗2 + 2ζ∗2 + γζµ2

with γ satisfying

γ ≤ min 1 , 1 . 8L/n + 4L 8 2LL(τ − 1)

144

and for all K ≥ 0

E f (xK ) − f (x∗) ≤ 2 x0 − x∗ 2 + 2γ 2σ∗2 + 4Lζ2(τ − 1) + 8L(τ − 1)γ σ2 + ζ2 .

γWK

n

µ

∗∗

In particular, if µ > 0 then

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 γ
+2γ 2nσ∗2 + 4Lζ2(µτ − 1) + 8L(τ − 1)γ σ∗2 + ζ∗2

(4.46)

and when µ = 0 we have E f (xK ) − f (x∗)

2 x0 − x∗ 2 ≤
γK +2γ 2nσ∗2 + 4Lζ2(µτ − 1) + 8L(τ − 1)γ σ∗2 + ζ∗2

. (4.47)

The theorem above together with Lemma A.5.3 implies the following result. Corollary 4.5.20. Let assumptions of Theorem 4.5.19 hold with µ > 0. Then for

1

1

γ0 = min 8L/n + 4L , 8 2LL(τ − 1) ,

ln max 2, min x0−x∗ /( ), 2µ2K2 2σ∗2/n+4Lζ2(τ−1)/µ x0−x∗ 2µ3K3/8L(τ −1)(σ∗2+ζ∗2)

γK =

,

µK

γ = min {γ0, γK }

for all K such that either µγK ≤ 1 or γ0 ≤ γK we have that E f (xK) − f (x∗) is of the order

O L + L/n + (τ − 1)LL R2 exp −

µ

K

0

L + L/n + (τ − 1)LL

+ σ∗2 + Lζ2(τ − 1) + L(τ − 1) σ∗2 + ζ∗2 ,

nµK

µ2K

µ2 K 2

where R0 = x0 − x∗ . That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 OL + L +
µ nµ

(τ − 1)LL + σ∗2 + Lζ2(τ − 1) +

µ

nµε

µ2ε



L(τ − 1) (σ∗2 + ζ∗2)

µ2ε



iterations/oracle calls per node and τ times less communication rounds.
Combining Theorem 4.5.19 and Lemma A.5.6 we derive the following result for the convergence of Local-SGD in the case when µ = 0.

145

Corollary 4.5.21. Let assumptions of Theorem 4.5.19 hold with µ = 0. Then for

1

1

γ0 = min 8L/n + 4L , 8 2LL(τ − 1) ,


 γ = min γ0,




R02

,3

R02

 ,

(2σ∗2/n + 4Lζ2(τ−1)/µ) K 8L(τ − 1) (σ∗2 + ζ∗2) K 

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) equals

 L + L/n +

(τ − 1)LL R02

O K +

R2 (σ∗2/n + Lζ2(τ −1)/µ)

3

LR04(τ

−

1)

(σ

2 ∗

+

 ζ∗2)

0 K + K2/3  .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SGD requires

 L + L/n +
O

(τ − 1)LL R02 (σ∗2/n + Lζ2(τ −1)/µ) R02 R02

ε

+

ε2

+



L(τ − 1) (σ∗2 + ζ∗2)

ε3/2



iterations/oracle calls per node and τ times less communication rounds.

4.5.2 Local-SVRG
As an alternative to Local-SGD when the local objective is of a ﬁnite sum structure (4.3), we propose L-SVRG [77, 103] stochastic gradient as a local direction instead of the plain stochastic gradient. Speciﬁcally, we consider

aki d=ef ∇fi,ji (xki ) − ∇fi,ji (wik) + ∇fi(wik),

bki = 0,

where index 1 ≤ ji ≤ m is selected uniformly at random and wik is a particular iterate from the local history updated as follows:



wk+1 = xki

i

wk

i

w.p. q w.p. 1 − q.

Next, we will assume that the local functions fi,j are max Lij-smooth.11 Lastly, we will equip the mentioned method with the ﬁxed local loop. The formal statement of the described instance of (4.4) is given as Algorithm 28.
Let us next provide the details on the convergence rate. In order to do so, let us identify the parameters of Assumption 4.4.1.
11It is easy to see that we must have max Lij ≥ L ≥ m1 max Lij.

146

Algorithm 28 Local-SVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd, communication period τ ≥ 1

1: for k = 0, 1, . . . do

2: for i = 1, . . . , n in parallel do

3:

Choose ji uniformly at random, independently across nodes

4:

gik = ∇fi,ji (xki ) − ∇fi,ji (wik) + ∇fi(wik)

5:

wk+1 = xki w.p. q

i

wik w.p. 1 − q

6:

if k + 1 mod τ = 0 then

n

7:

xki +1

=

xk+1

=

1 n

xki − γgik

i=1

8:

else

9:

xki +1 = xki − γgik

10:

end if

11: end for

12: end for

averaging local update

Proposition 4.5.22 (see [55]). Gradient estimator aki satisﬁes Assumption 4.4.1 with pa-

rameters Ai = 2 max Lij, Bi = 2, D1,i = 0, ρi = q, Ci = max Lijq, D2,i = 0, and σi2,k =

m

1 m

∇fij(wik) − ∇fij(x∗) 2.

j=1

ζ-Heterogeneous Data
It remains to use Lemma 4.4.3 along with Corollary C.4.5 to recover all parameters of Assumption 4.2.3 and obtain a convergence rate of Algorithm 28 in ζ-heterogeneous case.

Theorem 4.5.23. Assume that fi(x) is µ-strongly convex and L-smooth for i ∈ [n] and f1, . . . , fn are ζ-heterogeneous, convex and max Lij-smooth. Then Local-SVRG satisﬁes Assumption 4.2.3 with

A = 8 max Lij, B = 2, F = 8L max Lij, D1 = 2ζ∗2,

A = 4 max Lij + L, B = 1 ,

n

n

σk2 = n4m n m ∇fij(wik) − ∇fij(x∗) 2,
i=1 j=1

F = 4L mnax Lij + 2L2, D1 = 0, ρ = q, C = 8q max Lij, G = 4qL max Lij,

D2 = 0,

H = 8(τ − 1)(2 + q)γ2 , q

D3 = 2(τ − 1) 2ζ∗2 + γζµ2

147

with γ satisfying





γ ≤ min  1 , 1  .  2 (44 max Lij/n + L) 16 L max Lij(τ − 1) (1 + 4/(1−q)) 

and for all K ≥ 0 E f (xK ) − f (x∗)

≤ Φ0 + 8L(τ − 1)γ ζ2 + 2γζ2 ,

γWK

µ

∗

where Φ0 = 2

x0 − x∗

2

+

3n8q γ2σ02

+

32L(τ

−1)(2+q q

)γ

3

σ02

.

In particular, if µ > 0 then

E f (xK ) − f (x∗) ≤

1 − min γµ, q 4

K Φγ0 + 8L(τ − 1)γ ζµ2 + 2γζ∗2

(4.48)

and when µ = 0 we have

E f (xK ) − f (x∗) ≤ γΦK0 + 8L(τ − 1)γ ζµ2 + 2γζ∗2 .

(4.49)

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.24. Let assumptions of Theorem 4.5.23 hold with µ > 0. Then for







1

1



γ0 = min  2 (44 max Lij/n + L) , 16 L max Lij(τ − 1) (1 + 4/(1−q))  ,

Φ0 = 2 x0 − x∗ 2 + 3n8q γ02σ02 + 32L(τ − 1q)(2 + q)γ03 σ02,

 

ln max 2, min Φ0µ3K2/8Lζ2(τ −1), Φ0µ3K3/16L(τ −1)ζ∗2

γ = min γ0, µK

q= 1, m
 
, 

m > 1,

for all K such that

either or

ln max 2, min Φ0µ3K2/8Lζ2(τ −1), Φ0µ3K3/16L(τ −1)ζ∗2 K

≤1 m

ln γ0 ≤

max

2, min

Φ0µ3K2/8Lζ2(τ −1), Φ0µ3K3/16L(τ −1)ζ∗2 µK

we have that E f (xK) − f (x∗) is of the order

O Φ0 exp − min m−1, γ0µ K + ζ2L(τ − 1) + L(τ − 1)ζ∗2 .

γ0

µ2K

µ2 K 2

148

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SVRG requires



O m + L + max Lij +

µ

nµ

(τ − 1)L max Lij Lζ2(τ − 1)

µ

+ µ2ε +


L(τ − 1)ζ∗2 µ2ε 

iterations/oracle calls per node and τ times less communication rounds.

Combining Theorem 4.5.23 and Lemma A.5.6 we derive the following result for the convergence of Local-SVRG in the case when µ = 0.

Corollary 4.5.25. Let assumptions of Theorem 4.5.23 hold with µ = 0. Then for







1

1



γ0 = min  2 (44 max Lij/n + L) , 16 L max Lij(τ − 1) (1 + 4/(1−q))  ,

q= 1, m

m > 1,


 γ = min γ0,


3nR02 , 3

R02

,

4mσ02 16Lm(τ − 1)(2 + 1/m)σ02



µR02

,3

R02

 ,

4Lζ2(τ − 1)K 8L(τ − 1)ζ∗2K 

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) is of the order

(L + max Lij/n + O

(τ − 1)L max Lij)R02 + K

mσ02R02/n + 3 Lm(τ − 1)σ02R04 √
+ LR02ζµ2K(τ −1) + 3 LRK04(2τ/3−1)ζ∗2 .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SVRG requires

(L + max Lij/n + O

(τ − 1)L max Lij)R02 + ε

mσ02R02/n + 3 Lm(τ − 1)σ02R04

√

+ Lζ2(µτε−21)R02 + R02

L(τ −1)ζ∗2 ε3/2

iterations/oracle calls per node and τ times less communication rounds.

Remark 4.5.26. To get the rate from Tbl. 4.3 it remains to apply the following inequality: σ02 = n4m n m ∇fij (x0) − ∇fij (x∗) 2 (4≤.6) 4 max L2ij x0 − x∗ 2.
i=1 j=1

Heterogeneous Data First of all, we need the following lemma.

149

Lemma 4.5.27. Assume that fi(x) is L-smooth for i ∈ [n] and fij is convex and max Lijsmooth for i ∈ [n], j ∈ [m]. Then for Local-SVRG we have

1n nE
i=1

g¯ik 2

1n nE
i=1

gk − g¯k 2

i

i

≤ 6LE f (xk) − f (x∗) + 3L2E[Vk] + 3ζ∗2,

(4.50)

≤ 8 max LijE f (xk) − f (x∗) + 21 E[σk2] + 4L max LijE[Vk],(4.51)

nm

where

σk2

=

4 nm

∇fij(wik) − ∇fij(x∗) 2.

i=1 j=1

Proof. Inequality (4.50) follows from g¯ik = E gik | xk = ∇fi(xki ) and inequality (4.18). Next, using Young’s inequality we derive

1n nE
i=1

gk − g¯k 2

i

i

(A.14)
≤
(A.11)
≤
(A=.15)
(C.3),(A.14)
≤
(C.47)
≤

1n nE
i=1

gik − ∇fi(x∗) 2

2n nE
i=1

∇fiji (wik) − ∇fiji (x∗) − (∇fi(wik) − ∇fi(x∗)) 2

+2 n E n i=1

∇fiji (xki ) − ∇fiji (x∗) 2

2 nm

nm

E

i=1 j=1

∇fij(wik) − ∇fij(x∗) − (∇fi(wik) − ∇fi(x∗)) 2

+ 2 n mE nm i=1 j=1

∇fij(xki ) − ∇fij(x∗) 2

4 manx Lij n E Dfi (xki , x∗)
i=1

+ 2 n mE nm i=1 j=1

∇fij(wik) − ∇fij(x∗) 2

8 max LijE f (xk) − f (x∗) + 12 E[σk2] + 4L max LijE[Vk].

Applying Corollary C.4.3, Lemma 4.5.27, Proposition 4.5.22 and Lemma 4.4.3 we get the following result.
Theorem 4.5.28. Assume that fi(x) is µ-strongly convex and L-smooth for i ∈ [n] and fij is convex and max Lij-smooth for i ∈ [n], j ∈ [m]. Then Local-SVRG satisﬁes Assumption C.4.1

150

with

A = 3L,

A = 4 max Lij,

B = 0,

B = 1, 2

F = 3L2,

F = 4L max Lij,

D1 = 3ζ∗2,

D1 = 0, A = 4 manx Lij + L, B = n1 , F = 4L mnax Lij + 2L2, D1 = 0,

σk2 = n4m n m ∇fij(wik) − ∇fij(x∗) 2,
i=1 j=1

ρ = q,

C = 8q max Lij,

G = 4qL max Lij,

D2 = 0,

H = 2e(τ − 1)(2 + q)γ2 , q

D3 = 6e(τ − 1)2ζ∗2

with γ satisfying





γ ≤ min  1 , 1  .  2 (44 max Lij/n + L) 4 2eL(τ − 1) (3L(τ − 1) + 4 max Lij + 8 max Lij/(1−q)) 

and for all K ≥ 0

E f (xK ) − f (x∗)

≤ Φ0 + 24eL(τ − 1)2ζ2γ2,

γWK

∗

where Φ0 = 2

x0 − x∗

2

+

3n8q γ2σ02

+

8eL(τ

−1)(2+q q

)γ

3

σ02

In particular, if µ > 0 then

E f (xK ) − f (x∗) ≤

1 − min γµ, q 4

K Φγ0 + 24eL(τ − 1)2ζ∗2γ2

(4.52)

and when µ = 0 we have

E f (xK ) − f (x∗) ≤ γΦK0 + 24eL(τ − 1)2ζ∗2γ2.

(4.53)

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.29. Let assumptions of Theorem 4.5.28 hold with µ > 0. Then for







1

1



γ0 = min  2 (44 max Lij/n + L) , 4 2eL(τ − 1) (3L(τ − 1) + 4 max Lij + 8 max Lij/(1−q))  ,

Φ0 = 2 x0 − x∗ 2 + 3n8q γ02σ02 + 8eL(τ − 1q)(2 + q)γ03 σ02,

 

ln max 2, Φ0µ3K3/24eL(τ −1)2ζ∗2

 

γ = min γ0, µK  ,

q= 1, m

m > 1,

for all K such that

ln either

max

2, Φ0µ3K3/24eL(τ −1)2ζ∗2 K

≤ 1 or γ0 ≤ ln m

max

2, Φ0µ3K3/24eL(τ −1)2ζ∗2 µK

151

we have that E f (xK) − f (x∗) is of the order

O Φ0 exp − min m−1, γ0µ K + L(τ − 1)2ζ∗2 .

γ0

µ2 K 2

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SVRG requires



O m + Lτ + max Lij +

µ

nµ

(τ − 1)L max Lij µ+


L(τ − 1)2ζ∗2 µ2ε 

iterations/oracle calls per node and τ times less communication rounds.

Combining Theorem 4.5.28 and Lemma A.5.6 we derive the following result for the convergence of Local-SVRG in the case when µ = 0.

Corollary 4.5.30. Let assumptions of Theorem 4.5.28 hold with µ = 0. Then for q = m1 , m > 1 and







1

1



γ0 = min  2 (44 max Lij/n + L) , 4 2eL(τ − 1) (3L(τ − 1) + 4 max Lij + 8 max Lij/(1−q))  ,


 γ = min γ0,




3nR02 , 3

R02

,3

R02

 ,

4mσ02 4eLm(τ − 1)(2 + 1/m)σ02 12eL(τ − 1)2ζ∗2K 

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) is of the order

(Lτ + max Lij/n + O

(τ − 1)L max Lij)R02 + K

mσ02R02/n + 3 Lm(τ − 1)σ02R04 3 LR04(τ − 1)2ζ∗2
+ K2/3 .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case Local-SVRG requires

(Lτ + max Lij/n + O

(τ − 1)L max Lij)R02 + ε

mσ02R02/n + 3 Lm(τ − 1)σ02R04

+ R02

L(τ − 1)2ζ∗2 ε3/2

iterations/oracle calls per node and τ times less communication rounds.

152

Remark 4.5.31. To get the rate from Tbl. 4.3 it remains to apply the following inequality: σ02 = n4m n m ∇fij (x0) − ∇fij (x∗) 2 (4≤.6) 4 max L2ij x0 − x∗ 2.
i=1 j=1

4.5.3 S*-Local-SGD
In this section we consider the same settings as in Section 4.5.1 and our goal is to remove one of the main drawbacks of Local-SGD in heterogeneous case which in the case of µ-strongly convex fi with µ > 0 converges with linear rate only to the neighbourhood of the solution even in the full-gradients case, i.e. when D1,i = 0 for all i ∈ [n]. However, we start with unrealistic assumption that i-th node has an access to ∇fi(x∗) for all i ∈ [n]. Under this assumption we present a new method called Star-Shifted Local-SGD (S*-Local-SGD, see Algorithm 29).

Algorithm 29 S*-Local-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd, communication period τ ≥ 1

1: for k = 0, 1, . . . do

2: for i = 1, . . . , n in parallel do

3:

Sample gˆik = ∇fξk (xki ) independently from other nodes

i

4:

gik = gˆik − ∇fi(x∗)

5:

if k + 1 mod τ = 0 then

n

6:

xki +1

=

xk+1

=

1 n

xki − γgik

i=1

7:

else

8:

xki +1 = xki − γgik

9:

end if

10: end for

11: end for

averaging local update

Lemma 4.5.32. Let fi be convex and L-smooth for all i ∈ [n]. Then for all k ≥ 0

1n

kk

n E gi | xi

i=1

= n1 n ∇fi(xki ),
i=1

1 n g¯k 2 ≤ 4L f (xk) − f (x∗) + 2L2V ,

n

i

k

i=1

1n nE
i=1

gik − g¯ik 2 | xki

≤ σ2,

 1n

2 k


k

k

∗

2

σ2

E  n gi | x  ≤ 4L f (x ) − f (x ) + 2L Vk + n ,

i=1

where σ2 d=ef n1

n i=1

D1,i

and

E[·

|

xk ]

d=ef

E[·

|

xk1

,

.

.

.

,

x

k n

].

(4.54) (4.55) (4.56) (4.57)

153

Proof. First of all, we notice that E gik | xki = ∇fi(xki ) − ∇fi(x∗) and

1n

kk

n E gi | xi

i=1

=1 n n i=1

∇fi(xki ) − ∇fi(x∗)

= n1 n ∇fi(xki ).
i=1

Using this we get

1 n g¯k 2

n

i

i=1

=
(C.47)
≤

1n n

∇fi(xki ) − ∇fi(x∗) 2 (C≤.3) 2nL n Dfi (xki , x∗)

i=1

i=1

4L f (xk) − f (x∗) + 2L2Vk

and

1n nE
i=1

gik − g¯ik 2 | xki

=1 n E n i=1

∇fξk (xki ) − ∇fi(xki ) 2 i

(6≤.7) n1 n D1,i =: σ2.
i=1

Finally,

using

independence

of

g1k

,

g2k

,

.

.

.

,

g

k n

and

1 n

n i=1

∇fi(x∗)

=

∇f (x∗)

=

0

we

obtain

 1n

2 k


k

E  n gi | x 

i=1

(A.14=),(4.54)
=
=
(6.7),(4.19)
≤



1n E n

gik − ∇fi(xki )

i=1

2 | xk +

1n

2 k

n ∇fi(xi )

i=1



1n E n

∇fξk (xki ) − ∇fi(xki ) i

i=1

2 | xk +

1n

2 k

n ∇fi(xi )

i=1

1n n2 Eξik
i=1

k

k2

1n

2 k

∇fξk (xi ) − ∇fi(xi ) i

+n

∇fi(xi )

i=1

4L f (xk) − f (x∗) + 2L2Vk + σn2 .

Applying Corollary C.4.3 and Lemma 4.5.32 we get the following result.

Theorem 4.5.33. Assume that fi(x) is µ-strongly convex and L-smooth for every i ∈ [n]. Then S*-Local-SGD satisﬁes Assumption C.4.1 with

A = 2L, A = 0, A = 2L, B = 0,

B = B = 0,

F = 2L2,

F = 0,

D1 = 0,

D1 = σ2 := n1 n D1,i
i=1

F = 2L2,

σ2

D1 =

, n

σk2 ≡ 0,

ρ = 1,

C = 0,

G = 0,

D2 = 0,

H = 0, D3 = 2e(τ − 1)σ2.

154

Consequently, if we have for µ > 0

γ ≤ min 1 , √ 1 . 4L 8 e(τ − 1)L

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 + 2γ σ2 + 4eL(τ − 1)γσ2

γ

n

and when µ = 0 we have

E f (xK ) − f (x∗)

≤ 2 x0 − x∗ 2 + 2γ σ2 + 4eL(τ − 1)γσ2 .

γK

n

In the special case when ∇fξk (xki ) = ∇fi(xki ) for all i ∈ [n] and k ≥ 0 we obtain S*-Local-GD i
which converges with O τ κ ln 1ε rate when µ > 0 and with O Lτ x0ε−x∗ 2 rate when µ = 0 to the exact solution asymptotically.
The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.34. Let assumptions of Theorem 4.5.33 hold with µ > 0. Then for

1

1

γ = min 4L , 8√e(τ − 1)L , γK ,

ln (max {2, min { x0−x∗ 2nµ2K2/σ2, x0−x∗ 2µ3K3/4eL(τ −1)σ2}}) γK =
µK

for all K such that

1

1

either µγK ≤ 1 or min 4L , 8√e(τ − 1)L ≤ γK

we have that

E f (xK ) − f (x∗) = O

τ L x0 − x∗ 2 exp

µ −K

+

σ2

+ L(τ − 1)σ2

.

τL

nµK

µ2 K 2

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case S*-Local-SGD requires

 τL σ2
O + + µ nµε

 L(τ − 1)σ2
µ2ε 

iterations/oracle calls per node and τ times less communication rounds.
Combining Theorem 4.5.33 and Lemma A.5.6 we derive the following result for the convergence of S*-Local-SGD in the case when µ = 0.

155

Corollary 4.5.35. Let assumptions of Theorem 4.5.33 hold with µ = 0. Then for

 γ = min  1 , √ 1 ,
 4L 8 e(τ − 1)L



nR02 , 3

R02

 ,

σ2K 4eL(τ − 1)σ2K 

where R0 = x0 − x∗ , we have that

 E f (xK ) − f (x∗) = O  τ LR02 +
K

R2σ2

3

LR04(τ

−

 1)σ2

0+ nK

K2/3  .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case S*-Local-SGD requires

O τ LR02 + R02σ2 + R02 L(τ − 1)σ2

ε

nε2

ε3/2

iterations/oracle calls per node and τ times less communication rounds.

4.5.4 SS-Local-SGD

Uniformly Bounded Variance

In this section we consider the same settings as in Section 4.5.1

Algorithm 30 Stochastically Shifted Local-SGD (SS-Local-SGD)

Input: learning rate γ > 0, initial vector x0 ∈ Rd, probability of communication p ∈ (0, 1],

probability of the shift’s update q ∈ (0, 1], batchsize r for computing shifts

1: y0 = x0

2: For i ∈ [n] compute r independent samples ∇f 0 (y0), ∇f 0 (y0), . . . , ∇f 0 (y0), set

ξi,1

ξi,2

ξi,r

∇f 0 (y0)
ξ

=

1 r

i

rj=1 ∇fξ0

(y0) and ∇f 0(y0) =
ξ

1 n

i,j

ni=1 ∇fξ0i (y0)

3: for k = 0, 1, . . . do

4: for i = 1, . . . , n in parallel do

5:

Sample ∇fξk (xki ) independently from other nodes

i

6: gik = ∇fξik (xki ) − ∇fξik (yk) + ∇fξk (yk), where ∇fξki (yk) = 1r

∇f k (yk)
ξ

=

1 n

ni=1 ∇fξki (yk)

rj=1 ∇fξki,j (yk) and

7:

xk+1 = xk+1,

w.p. p,

n
where xk+1 = 1 (xk − γgk)

i

xki − γgik, w.p. 1 − p,

n

i

i

i=1

8: yk+1 = xykk,, ww..pp.. q1,− q, and for all i ∈ [n], j ∈ [r] ξki,+j 1 is

a fresh sample, equal to ξki,j, 9: end for
10: end for

if yk+1 = yk, otherwise.

The main algorithm in this section is Stochastically Shifted Local-SGD (SS-Local-SVRG, see

156

Algorithm 30). We notice that the updates for xki +1 and yk+1 can be dependent, e.g., one can take p = q and update yk+1 as xk every time xki +1 is updated by xk+1. Moreover, with probability q line 8 implies a round of communication and computation of new stochastic gradient
by each worker.

We emphasize that in expectation yk is updated only once per 1/q iterations. Therefore, if r = O (1/q) and q ≤ p, then up to a constant numerical factor the overall expected number of oracle calls and communication rounds are the same as for Local-SGD with either the same probability p of communication or with constant local loop length τ = 1/p .

Finally,

we

notice

that

due

to

independence

of

ξ

k i,

1

,

ξ

k i,

2

,

.

.

.

,

ξ

k i,r

we

have

E

∇f

k (yk) − ∇fi(yk)

(6.7)
2≤

D1,i .

ξi

r

(4.58)

Lemma 4.5.36. Let fi be convex and L-smooth for all i ∈ [n]. Then for all k ≥ 0

1n

k

n Ek gi

i=1

= n1 n ∇fi(xki ),
i=1

1n nE
i=1

g¯ik 2

≤ 8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk] + 2σr 2 ,

1n nE
i=1

gik − g¯ik 2

≤ σ2,

 1n

2
k

k

∗

2

σ2

E  n gi  ≤ 4LE f (x ) − f (x ) + 2L E [Vk] + n ,

i=1

where σk2 d=ef n1 n ∇fi(yk) − ∇fi(x∗) 2 and σ2 d=ef n1
i=1

n i=1

D1,i.

(4.59) (4.60) (4.61) (4.62)

Proof. We start with unbiasedness:

1n

k

n Ek gi

i=1

=

1n E

∇f (xk) − ∇f (yk) + ∇f (yk)

n

k

ξik i

k
ξi

ξk

i=1

=

1n E

∇f (xk) + E

∇f

(yk) − 1

n
∇f

(yk)

=1

n
∇f (xk).

n

k

ξik i

k

ξk

n

k
ξi

n

ii

i=1

i=1

i=1

157

Using this we get

1n nE
i=1

g¯ik 2

(A.11)
≤
(C.3),(A.14)
≤
(C.47),(A.14)
≤
(4.58)
≤

2n nE
i=1

∇fi(xki ) − ∇fi(x∗) 2

+ 2 n E ∇f (yk) − ∇f (x∗) − ∇f (yk) − ∇f (x∗) 2

n

k
ξi

i

ξk

i=1

4nL n E Dfi (xki , x∗) + n2 n E

i=1

i=1

2
∇f k (yk) − ∇fi(x∗)
ξi

8LE f (xk) − f (x∗) + 4L2E[Vk] + n2 n E
i=1

∇fi(yk) − ∇fi(x∗) 2

+2 n E n i=1

2
∇f k (yk) − ∇fi(yk)
ξi

8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk] + 2σr 2

and

1n E

gk − g¯k 2 = 1 n E

(6.7)
∇f (xk) − ∇f (xk) 2 ≤ σ2.

n

i

i

n

ξik i

ii

i=1

i=1

Finally, we use independence of ∇fξk (xk1), . . . , ∇fξk (xkn) and derive

1

n

 1n

2
k

E  n gi 

i=1

=
(A=.14)
(4.19)
≤
(6.7)
≤

 1n

2
k

E n

∇fξk (xi )  i

i=1

 1n

2
k

 1n

k

2
k

E  n ∇fi(xi )  + E  n

∇fξk (xi ) − ∇fi(xi )  i

i=1

i=1

4LE f (xk) − f (x∗) + 2L2E[Vk] + n12 n E
i=1

∇fξk (xki ) − ∇fi(xki ) 2 i

4LE f (xk) − f (x∗) + 2L2E [Vk] + σn2

which ﬁnishes the proof.

Lemma 4.5.37. Let fi be convex and L-smooth for all i ∈ [n]. Then for all k ≥ 0 E σk2+1 ≤ (1 − q)E σk2 + 2LqE f (xk) − f (x∗)
where σk2 d=ef n1 n ∇fi(yk) − ∇fi(x∗) 2.
i=1

(4.63)

158

Proof. By deﬁnition of yk+1 we have

E σk2+1 | xk1, . . . , xkn

= 1 −n q n ∇fi(yk) − ∇fi(x∗) 2 + nq n ∇fi(xk) − ∇fi(x∗) 2

i=1

i=1

(C.3)
≤ (1 − q)σk2 + 2Lq(f (xk) − f (x∗)).

Taking the full mathematical expectation on both sides of previous inequality and using the tower property (A.15) we get the result.

Using Corollary C.4.7 we obtain the following theorem.

Theorem 4.5.38. Assume that fi(x) is µ-strongly convex and L-smooth for every i ∈ [n]. Then SS-Local-SGD satisﬁes Assumption C.4.1 with

A = 4L, A = 0, B = 2, B = 0, F = 4L2, F = 0,

D1 = σ2 = n1 n D1,i,
i=1

A = 2L,

B = 0,

F = 2L2,

2σ2 D1 = ,
r

σ2

D1 =

, n

σk2 = n1 n

∇fi(yk) − ∇fi(x∗)

2
,

i=1

ρ = q,

C = Lq,

G = 0,

D2 = 0,

128(1 − p)(2 + p)(2 + q)γ2

H=

3p2q

,

8(1 − p) D3 = p2

2(p + 2)σ2 + pσ2 r

under assumption that √
γ ≤ min 1 , p 3 . 4L 32L 2(1 − p)(2 + p) (1 + 1/(1−q))

Moreover, for µ > 0 we have

E f (xK ) − f (x∗) ≤

1 − min γµ, q 4

K Φ0 γ

σ2 16L(1 − p) +2γ n + γ p2

2(p + 2)σ2 + pσ2 r

and when µ = 0 we have E f (xK ) − f (x∗)

≤ Φ0 + 2γ σ2 + γ 16L(1 − p) 2(p + 2)σ2 + pσ2

γK

n

p2

r

where Φ0 = 2 x0 − x∗ 2 + 512L(1−p)(32p+2pq)(2+q)γ3σ02 . The theorem above together with Lemma A.5.3 implies the following result.

159

Corollary 4.5.39. Let assumptions of Theorem 4.5.38 hold with µ > 0. Then for

√

1

p3

γ0 = min 4L , 32L 2(1 − p)(2 + p) (1 + 1/(1−q)) ,

Φ0 = 2 x0 − x∗ 2 + 512L(1 − p)(2 + p)(2 + q)γ03σ02 , 3p2q

q = p,

 

ln max 2, min nΦ0µ2K2/2σ2, pΦ0µ3K3/32L(1−p)(3p+4)σ2

γ = min γ0, µK



, r = 1 ,



p

for all K such that

either or

ln max 2, min nΦ0µ2K2/2σ2, pΦ0µ3K3/32L(1−p)(3p+4)σ2 ≤p
K

ln γ0 ≤

max

2, min

nΦ0µ2K2/2σ2, pΦ0µ3K3/32L(1−p)(3p+4)σ2 µK

we have that E f (xK) − f (x∗) is of the order

Φ0

1

σ2 L(1 − p)σ2

O γ0 exp − min p , γ0µ K + nµK + pµ2K2 .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case SS-Local-SGD requires

 L σ2
O + + pµ nµε

 L(1 − p)σ2
pµ2ε 

iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.

Combining Theorem 4.5.38 and Lemma A.5.6 we derive the following result for the convergence of SS-Local-SGD in the case when µ = 0.

Corollary 4.5.40. Let assumptions of Theorem 4.5.38 hold with µ = 0. Then for q = p, r = 1/p and

√

1

p3

γ0 = min 4L , 32L 2(1 − p)(2 + p) (1 + 1/(1−q)) ,



 γ = min γ0, 3

3p3R02

,



256L(1 − p)(2 + p)2σ02



nR02 , 3

pR02

 ,

σ2K 16L(1 − p)(3p + 4)σ2K 

160

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) is of the order

 LR02 + 3 L(1 − p)σ02R04 O  pK +

σ2R2

3

LR04(1

−

 p)σ2

0+ nK

p1/3K2/3  .

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case SS-Local-SGD requires

 LR02 + 3 L(1 − p)σ02R04

σ2R02

 R02 L(1 − p)σ2

O pε

+ nε2 +

p1/2ε3/2



iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.

Remark 4.5.41. To get the rate from Tbl. 4.3 it remains to apply the following inequality:

σ02 = n1 n

(4.6)
∇fi(x0) − ∇fi(x∗) 2 ≤ L2 x0 − x∗ 2.

i=1

Expected Smoothness and Arbitrary Sampling

In this section we consider the same method SS-Local-SGD, but without assumption that the stochastic gradient has a uniformly bounded variance. Instead of this we consider the same setup as in Section 4.5.1, i.e. we assume that each worker i ∈ [n] at any point x ∈ Rd has an access to the unbiased estimator ∇fξi(x) of ∇fi(x) satisfying Assumption 4.5.14.

Lemma 4.5.42. Let fi be convex and L-smooth for all i ∈ [n]. Let Assumption 4.5.14 holds. Then for all k ≥ 0

1n

k

n Ek gi

i=1

1n nE
i=1

g¯ik 2

1n nE
i=1

gik − g¯ik 2

= n1 n ∇fi(xki ),
i=1
≤ 8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk],
≤ 8LE f (xk) − f (x∗) + 4LLE[Vk] + 2σ∗2,

(4.64) (4.65) (4.66)

 1n

2
k

2L

k

∗

2L

E  n gi  ≤ 4 n + L E f (x ) − f (x ) + 2L n + L E[Vk]

i=1

+

2

σ

2 ∗

,

n

(4.67)

where σ2 d=ef 1 n

2
∇f k (yk) − ∇fi(x∗) and σ2 d=ef 1

kn i=1

ξi

∗n

n
i=1 Eξi

∇fξi (x∗) − ∇fi(x∗)

2.

Proof. First of all, (4.64) follows from (4.59). Next, using g¯ik = ∇fi(xki ) − ∇fξki (yk) + ∇fξk (yk)

161

we get

1n nE
i=1

g¯ik 2

(A.11)
≤
(C.3),(A.14)
≤
(C.47)
≤

2n nE
i=1

∇fi(xki ) − ∇fi(x∗) 2

+2 n E n i=1

2

∇f k (yk) − ∇fi(x∗) − (∇f k (yk) − ∇f (x∗))

ξi

ξ

4nL n E Dfi (xki , x∗) + n2 n E

i=1

i=1

∇f k (yk) − ∇fi(x∗) 2
ξi

8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk]

and

1n nE
i=1

gik − g¯ik 2

=
(A.14)
≤
(A.11)
≤
(4.37)
≤
(C.47)
≤

1n nE
i=1

∇fξk (xki ) − ∇fi(xki ) 2 i

1n nE
i=1

∇fξk (xki ) − ∇fi(x∗) 2 i

2n nE
i=1

∇fξk (xki ) − ∇fξk (x∗) 2

i

i

+2 n E n i=1

∇fξk (x∗) − ∇fi(x∗) 2 i

4nL n E Dfi (xki , x∗) + 2σ∗2
i=1

8LE f (xk) − f (x∗) + 4LLE[Vk] + 2σ∗2.

(4.68) (4.69)

Finally,

we

use

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

and

derive

 1n

2
k

E  n gi 

i=1

=
(A.15)=,(A.14)
=
(4.69),(4.19)
≤

 1n

2
k

E n

∇fξk (xi )  i

i=1



1n

k

2
k

 1n

2
k

E n

(∇fξk (xi ) − ∇fi(xi )) i

+E n

∇fi(xi ) 

i=1

i=1

1n n2 E
i=1

k

k2

 1n

2
k

∇fξk (xi ) − ∇fi(xi ) i

+E n

∇fi(xi ) 

i=1

4 2nL + L E f (xk) − f (x∗) + 2L 2nL + L E[Vk] + 2nσ∗2

which ﬁnishes the proof.

Lemma 4.5.43. Let fi be convex and L-smooth for all i ∈ [n] and Assumption 4.5.14 holds.

162

Then for all k ≥ 0

E σk2+1 ≤ (1 − q)E σk2 + 2q 2rL + L E f (xk) − f (x∗) + 2qrσ∗2

(4.70)

where σ2 d=ef 1 n

2
∇f k (yk) − ∇fi(x∗) and σ2 d=ef 1

kn i=1

ξi

∗n

n
i=1 Eξi

∇fξi (x∗) − ∇fi(x∗)

2.

Proof. By deﬁnition of yk+1 we have

E σk2+1 | xk1, . . . , xkn

= (A=.14)

1−q n n

∇f k (yk) − ∇fi(x∗) 2
ξi

i=1

qn

+n

E k+1
ξi

i=1

∇f k+1 (xk) − ∇fi(x∗) 2
ξi

(1 − q)σk2 + nq n ∇fi(xk) − ∇fi(x∗) 2
i=1

qn

+n

E k+1
ξi

i=1

∇f k+1 (xk) − ∇fi(xk) 2 .
ξi

Next,

we

use

independence

of

ξ

k+1 i,1

,

ξ

k+1 i,2

,

.

.

.

,

ξ

k+1 i,r

for

all

i

∈

[n]

and

derive

E σk2+1 | xk1, . . . , xkn

=
(C.3),(A.14)
≤
(A.11)
≤
(4.37)
≤

(1 − q)σk2 + nq n ∇fi(xk) − ∇fi(x∗) 2
i=1

q nr

+ nr2

E k+1
ξi,j

i=1 j=1

∇f k+1 (xk) − ∇fi(xk) 2
ξi,j

(1 − q)σk2 + 2Lq f (xk) − f (x∗)

q nr

+ nr2

E k+1
ξi,j

i=1 j=1

∇f k+1 (xk) − ∇fi(x∗) 2
ξi,j

(1 − q)σk2 + 2Lq f (xk) − f (x∗)

2q n r

+ nr2

E k+1
ξi,j

i=1 j=1

∇f k+1 (xk) − ∇f k+1 (x∗) 2

ξi,j

ξi,j

2q n r

+ nr2

E k+1
ξi,j

i=1 j=1

∇f k+1 (x∗) − ∇fi(x∗) 2
ξi,j

(1 − q)σk2 + 2q 2rL + L

f (xk) − f (x∗) + 2qσ∗2 . r

Taking the full mathematical expectation on both sides of previous inequality and using the tower property (A.15) we get the result.

Using Corollary C.4.7 we obtain the following theorem.

163

Theorem 4.5.44. Assume that fi(x) is µ-strongly convex and L-smooth for every i ∈ [n]. Let Assumption 4.5.14 holds. Then SS-Local-SGD satisﬁes Assumption C.4.1 with

A = 4L, A = 4L, B = 2, B = 0, F = 4L2, F = 4LL, D1 = 0, B = 0,

D1 = 2σ∗2 = n2 n Eξi ∇fξi (x∗) − ∇fi(x∗) 2,
i=1

A = 2 2L + L , n

D1 = 2nσ∗2 ,

σk2 = n1 n

2
∇f k (yk) − ∇fi(x∗) ,
ξi

i=1

ρ = q,

C =q

F = 2L 2L + L , n
2L + L , G = 0, r

D2 = 2qσ∗2 , , r

128(1 − p)(2 + p)(2 + q)γ2

H=

3p2q

,

8(1 − p) D3 = p2

2pσ∗2 + 32(2 3+r p)σ∗2

under assumption that







√



γ ≤ min  1 , p 3  .

 4 2nL + L 32 2L(1 − p) (2 + p)L + pL + (2+p()1(−2Lq/)r+L) 

Moreover, for µ > 0 we have E f (xK ) − f (x∗) ≤

1 − min γµ, q 4

K Φ0 γ

+2γ 2σ∗2 + γ 16L(1 − p)

n

p2

2pσ∗2 + 32(2 3+r p)σ∗2

and when µ = 0 we have E f (xK ) − f (x∗) ≤ γΦK0 + 2γ 2nσ∗2 + γ 16L(p12− p) 2pσ∗2 + 32(2 3+r p)σ∗2

where Φ0 = 2 x0 − x∗ 2 + 512L(1−p)(23+pp2)q(2+q)γ3E[σ02] .

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.45. Let assumptions of Theorem 4.5.44 hold with µ > 0. Then for







√



 

1

p3

 

γ0 = min

,

,

 4 2nL + L 32 2L(1 − p) (2 + p)L + pL + (2+p()1(−2Lq/)r+L) 

Φ0 = 2 x0 − x∗ 2 + 512L(1 − p)(2 + p)(2 + q)γ03E[σ02] , p2q

q = p,

r= 1 , p

 

ln max 2, min nΦ0µ2K2/4σ∗2, pΦ0µ3K /3 64L(1−p)(1+32(2+p)/3)σ∗2

 

γ = min γ0, µK  ,

164

for all K such that

either or

ln max 2, min nΦ0µ2K2/4σ∗2, pΦ0µ3K /3 64L(1−p)(1+32(2+p)/3)σ∗2 K

≤p

ln γ0 ≤

max

2, min

nΦ0µ2K2/4σ∗2, pΦ0µ3K /3 64L(1−p)(1+32(2+p)/3)σ∗2 µK

we have that E f (xK) − f (x∗) is of the order

O Φ0 exp − min 1 , γ0µ K + σ∗2 + L(1 − p)σ∗2 .

γ0

p

nµK

pµ2 K 2

That is, to achieve E f (xK) − f (x∗) ≤ ε in this case SS-Local-SGD requires

 O L + L +
pµ nµ

LL(1 − p)

√

+

σ∗2

+

pµ

nµε


L(1 − p)σ∗2 pµ2ε 

iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.

Combining Theorem 4.5.44 and Lemma A.5.6 we derive the following result for the convergence of SS-Local-SGD in the case when µ = 0.

Corollary 4.5.46. Let assumptions of Theorem 4.5.44 hold with µ = 0. Then for q = p, r = 1/p and







√



 

1

p3

 

γ0 = min

,

,

 4 2nL + L 32 2L(1 − p) (2 + p)L + pL + (2+p()1(−2Lq/)r+L) 



 γ = min γ0, 3

p3R02

,



256L(1 − p)(2 + p)2E[σ02]



nR02 , 3

pR02

 ,

2σ∗2K 32L(1 − p) (1 + 32(2+p)/3) σ∗2K 

where R0 = x0 − x∗ , we have that E f (xK ) − f (x∗) is of the order


L + pL/n + O


p(1 − p)LL R02 + 3 L(1 − p)E[σ02]R04 pK +



σ∗2R02 +

3 LR04(1 − p)σ∗2 .

nK

p1/3K 2/3



That is, to achieve E f (xK) − f (x∗) ≤ ε in this case SS-Local-SGD requires


L + pL/n + O


p(1 − p)LL R02 + 3 L(1 − p)E[σ02]R04 σ∗2R02 R02

pε

+ nε2 +



L(1 − p)σ∗2 

p1/2ε3/2



iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.

165

Remark 4.5.47. To get the rate from Tbl. 4.3 it remains to apply the following inequality:

E[σ02]

=
(A=.14)
(C.3)
≤
(A.14)
≤
(A.11)
≤
r= 1/p ,(4.37)
≤

1n n Eξ0i
i=1

∇fξ0i (x0) − ∇fi(x∗) 2

1 n ∇f (x0) − ∇f (x∗) 2 + 1 n E

n

i

i

0

n

ξi

i=1

i=1

∇fξ0i (x0) − ∇fi(x0) 2

2L(f (x0) − f (x∗)) + 1 n nr2

r
E0
ξi,j

i=1 j=1

∇fξ0i,j (x0) − ∇fi(x0) 2

2L(f (x0) − f (x∗)) + n1r n Eξi
i=1

∇fξi (x0) − ∇fi(x∗) 2

2L(f (x0) − f (x∗)) + n2r n Eξi
i=1

∇fξi (x0) − ∇fξi (x∗) 2

2n + nr Eξi
i=1

∇fξi (x∗) − ∇fi(x∗) 2

2 (L + 2pL) (f (x0) − f (x∗)) + 2pσ∗2.

4.5.5 S*-Local-SGD*
In this section we present doubly idealized algorithm for solving problem (6.6)+(4.3). Speciﬁcally, we choose bki to the optimal shift ∇fi(x∗) as per Case II, while aki is selected as SGD-star gradient estimator [55], i.e.,
aki = ∇fi,ji (xki ) − ∇fi,ji (x∗) + ∇fi(x∗), bki = ∇fi(x∗).

Note that now aki serves as an ambitious target for the local variance reduced estimators, while bki serves as an ambitious goal for the local shift. The resulting instance of (4.4) is presented as Algorithm 31 and called Star-Shifted Local-SGD-star (S*-Local-SGD*).

Algorithm 31 S*-Local-SGD*

Input: learning rate γ > 0, initial vector x0 ∈ Rd, communication period τ ≥ 1

1: for k = 0, 1, . . . do

2: for i = 1, . . . , n in parallel do

3:

Set gik = ∇fi,ji(xki ) − ∇fi,ji(x∗) where 1 ≤ ji ≤ m is sampled independently from all

nodes

4:

if k + 1 mod τ = 0 then

n

5:

xki +1

=

xk+1

=

1 n

xki − γgik

i=1

6:

else

7:

xki +1 = xki − γgik

8:

end if

averaging local update

9: end for

10: end for

166

Let us next provide the details on the convergence rate. In order to do so, let us identify the parameters of Assumption 4.4.1.

Lemma 4.5.48. Let fi be convex and L-smooth and fi,j be convex and max Lij-smooth for all i ∈ [n], j ∈ [m]. Then for all k ≥ 0

1n

k

n Ek gi

i=1

1n nE
i=1

g¯ik 2

1n nE
i=1

gik − g¯ik 2

= n1 n ∇fi(xki ),
i=1
≤ 4LE f (xk) − f (x∗) + 2L2E[Vk],
≤ 4 max LijE f (xk) − f (x∗) + 2L max LijE[Vk],

 1n

2
k

max Lij

k

∗

E  n gi  ≤ 4 n + L E f (x ) − f (x )

i=1

+2L max Lij + L E[Vk]. n

(4.71) (4.72) (4.73)
(4.74)

Proof. First of all,

1n

k

n Ek gi

i=1

= n1m n m ∇fi,j(xki ) − ∇fi,j(x∗) = n1 n ∇fi(xki )

i=1 j=1

i=1

and, in particular, g¯ik = Ek gik = ∇fi(xki ) − ∇fi(x∗). Using this we derive

1n nE
i=1

g¯ik 2

=
(C.3)
≤

1n nE
i=1
2L n nE
i=1

∇fi(xki ) − ∇fi(x∗) 2
(C.47)
Dfi (xki , x∗) ≤ 4LE

f (xk) − f (x∗)

+ 2L2E[Vk]

and

1n nE
i=1

gik − g¯ik 2

(A.14)
≤
=
(C.3)
≤
(C.47)
≤

1n nE
i=1

gik 2

n1m n m ∇fi,j (xki ) − ∇fi,j (x∗) 2
i=1 j=1

2 manx Lij n E Dfi (xki , x∗)
i=1

4 max LijE f (xk) − f (x∗) + 2L max LijE[Vk].

(4.75)

167

Finally, due to the independence of j1, j2, . . . , jn we have

 1n

2
k

E  n gi 

i=1

(A.14)=,(A.15)
=
(A.14)
≤
(4.75),(C.47)
≤



1n

k

2

k

∗

E n

∇fi,ji (xi ) − ∇fi,ji (x∗) − (∇fi(xi ) − ∇fi(x )) 

i=1



1n

k

2
∗

+E  n ∇fi(xi ) − ∇fi(x ) 

i=1

1n n2 E
i=1

∇fi,ji (xki ) − ∇fi,ji (x∗) − (∇fi(xki ) − ∇fi(x∗)) 2

 1n

2
k

+E  n ∇fi(xi ) 

i=1

1 nm

k

∗2

 1n

2
k

n2m

∇fi,j(xi ) − ∇fi,j(x ) + E  n ∇fi(xi ) 

i=1 j=1

i=1

4 max Lij + L E f (xk) − f (x∗) + 2L max Lij + L E[Vk].

n

n

Using Corollary C.4.3 we obtain the following theorem.
Theorem 4.5.49. Assume that fi(x) is µ-strongly convex and L-smooth and fi,j is convex and max Lij-smooth for every i ∈ [n], j ∈ [m]. Then S*-Local-SGD* satisﬁes Assumption C.4.1 with

A = 2L, A = 2 max Lij, B = B = 0, F = 2L2, F = 2L max Lij,

A = 2 max Lij + L , B = 0, F = 2L max Lij + L

n

n

D1 = 0, σk2 ≡ 0, ρ = 1, C = 0, G = 0, D2 = 0, H = 0,

D1 = D1 = 0, , D3 = 0

under assumption that





γ ≤ min  1 , 1  .  4 maxnLij + L 8 eL(τ − 1) (L(τ − 1) + max Lij) 

Moreover, for µ > 0 we have

E f (xK ) − f (x∗)

≤ (1 − γµ)K 2 x0 − x∗ 2 γ

and when µ = 0 we have

E f (xK ) − f (x∗)

2 x0 − x∗ 2

≤

.

γK

168

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.50. Let assumptions of Theorem 4.5.49 hold with µ > 0. Then for





γ = min  1 , 1   4 maxnLij + L 8 eL(τ − 1) (L(τ − 1) + max Lij) 

and for all K ≥ 1 we have E f (xK) − f (x∗) of order

 O  Lτ + max Lij +
n

(τ − 1)L max Lij

 R02 exp − Lτ + max Lij +
n


µ K 
(τ − 1)L max Lij

with R0 = x0 − x∗ . That is, to achieve E f (xK) − f (x∗) ≤ ε in this case S*-Local-SGD* requires

 

O  Lτ + max Lij +

µ

nµ

 (τ − 1)L max Lij

Lτ + maxnLij +

µ  log

 (τ − 1)L max Lij R02


ε



iterations/oracle calls per node and τ times less communication rounds.

Next, we derive the following result for the convergence of S*-Local-SGD* in the case when µ = 0.

Corollary 4.5.51. Let assumptions of Theorem 4.5.49 hold with µ = 0. Then for





γ = min  1 , 1  ,  4 maxnLij + L 8 eL(τ − 1) (L(τ − 1) + max Lij) 

we have that E f (xK) − f (x∗) is of the order


Lτ + max Lij/n + O


(τ − 1)L max Lij K

 R02
, 

where R0 = x0 − x∗ . That is, to achieve E f (xK) − f (x∗) ≤ ε in this case S*-Local-SGD* requires


Lτ + max Lij/n + O


 (τ − 1)L max Lij R02


ε



iterations/oracle calls per node and τ times less communication rounds.

169

Algorithm 32 Shifted Local SVRG (S-Local-SVRG) for minimizing local ﬁnite sums

Input: learning rate γ > 0, initial vector x0 ∈ Rd, probability of communication p ∈ (0, 1], probability of local full gradient computation q ∈ (0, 1], initialization y0 = x0

1: for k = 0, 1, . . . do

2: for i = 1, . . . , n in parallel do

3:

Choose ji uniformly at random from [m]

4:

gik = ∇fi,ji (xki ) − ∇fi,ji (yk) + ∇f (yk)

5:

xk+1 = xk+1,

w.p. p,

n
where xk+1 = 1 (xk − γgk)

i

xki − γgik, w.p. 1 − p,

n

i

i

i=1

6:

yk+1 = xk, w.p. q,

yk, w.p. 1 − q

7: end for

8: end for

4.5.6 S-Local-SVRG
In this section we are interested in problem (6.6)+(4.3). To solve this problem we propose a new method called Shifted Local-SVRG (S-Local-SVRG, see Algorithm 32).
We note that our analysis works even when updates in lines 5,6 are not independent. Moreover, in order for S-Local-SVRG to be eﬃcient, we shall require q ≤ p.
Remark 4.5.52. Unlike all other special cases, the rate of S-Local-SVRG can not be directly obtained from the theory of the local stochastic solver described in Section 4.4. Speciﬁcally, we construct the sequence lik using yk in contrast to xki used in Section 4.4. While we could construct lik from the local iterate sequences, setting it as the virtual iterates yields a tighter rate. We remark that such a choice is rather poor in general; we can implement it eﬃciently thanks to the speciﬁc structure of S-Local-SVRG.
Lemma 4.5.53. Let fi be convex and L-smooth and fi,j be convex and max Lij-smooth for

170

all i ∈ [n], j ∈ [m]. Then for all k ≥ 0

1n

k

n Ek gi

i=1

1n nE
i=1

g¯ik 2

1n nE
i=1

gik − g¯ik 2

= n1 n ∇fi(xki ),
i=1
≤ 8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk],

(4.76) (4.77)

≤ 8 max LijE f (xk) − f (x∗) + 2E[σk2] + 4L max LijE[Vk],(4.78)

 1n

2
k

2 max Lij

k

∗

22

E  n gi  ≤ 4

n + L E f (x ) − f (x ) + n E[σk]

i=1

+2L 2 max Lij + L E[Vk], n

(4.79)

where σk2 d=ef n1m n m ∇fi,j (yk) − ∇fi,j (x∗) 2 + n1 n ∇fi(yk) − ∇fi(x∗) 2.

i=1 j=1

i=1

Proof. First of all, we have

1n

k

n Ek gi

i=1

= n1 n Ek ∇fi,jk (xki ) − ∇fi,ji (yk) + ∇f (yk)
i=1
= n1m n m ∇fi,j(xki ) − ∇fi,j(yk) + ∇f (yk)
i=1 j=1
= n1 n ∇fi(xki )
i=1

and, in particular, g¯ik = Ek[gik] = ∇fi(xki ) − ∇fi(yk) + ∇f (yk). Using this we get

1n nE
i=1

g¯ik 2

(A.11)
≤
(C.3),(A.14)
≤
(C.47)
≤

2n nE
i=1

∇fi(xki ) − ∇fi(x∗) 2

+2 n E n i=1

∇fi(yk) − ∇fi(x∗) − (∇f (yk) − ∇f (x∗)) 2

4nL n E Dfi (xki , x∗) + n2 n E

i=1

i=1

∇fi(yk) − ∇fi(x∗) 2

8LE f (xk) − f (x∗) + 2E[σk2] + 4L2E[Vk]

171

and 1n nE
i=1

gik − g¯ik 2

=
(A.14)
≤
(A.11)
≤
(C.3)
≤
(C.47)
≤

1n nE
i=1

∇fi,ji (xki ) − ∇fi,ji (yk) − (∇fi(xki ) − ∇fi(yk)) 2

1n nE
i=1

∇fi,ji (xki ) − ∇fi,ji (yk) 2

2 nm

nm

E

i=1 j=1

∇fi,j (xki ) − ∇fi,j (x∗) 2

+ 2 n mE nm i=1 j=1

∇fi,j(yk) − ∇fi,j(x∗) 2

4 manx Lij n E Dfi (xki , x∗) + 2E[σk2]
i=1

8 max LijE f (xk) − f (x∗) + 2E[σk2] + 4L max LijE[Vk]. (4.80)

Finally, using independence of j1, j2, . . . , jn we derive

 1n

2
k

E  n gi 

i=1

(A.14=),(4.76)
=
(4.19),(4.80)
≤

 1n

2
k

E  n ∇fi(xi ) 

i=1



1n

k

2

k

k

k

+E  n (∇fi,ji(xi ) − ∇fi,ji(y ) − (∇fi(xi ) − ∇fi(y ))) 

i=1

 1n

2
k

E  n ∇fi(xi ) 

i=1

1n + n2 E
i=1

(∇fi,ji (xki ) − ∇fi,ji (yk) − (∇fi(xki ) − ∇fi(yk))) 2

4 2 manx Lij + L E f (xk) − f (x∗) + n2 E[σk2] +2L 2 max Lij + L E[Vk]. n

Lemma 4.5.54. Let fi be convex and L-smooth and fi,j be convex and max Lij-smooth for all i ∈ [n], j ∈ [m]. Then for all k ≥ 0

E σk2+1 ≤ (1 − q) E σk2 + 2(L + max Lij)qE f (xk) − f (x∗)

(4.81)

where σk2 d=ef n1m n m ∇fi,j (yk) − ∇fi,j (x∗) 2 + n1 n ∇fi(yk) − ∇fi(x∗) 2.

i=1 j=1

i=1

172

Proof. First of all, we introduce new notations: σk2,1 d=ef n1m n m ∇fi,j (yk) − ∇fi,j (x∗) 2 ,
i=1 j=1

σk2,2 = n1 n

∇fi(yk) − ∇fi(x∗)

2
.

i=1

Secondly, by deﬁnition of yk+1 we have

E σk2+1,1 | xk1, . . . , xkn

=
(C.3)
≤

1n−mq n m ∇fi,j(yk) − ∇fi,j(x∗) 2
i=1 j=1
+ nqm n m ∇fi,j(xk) − ∇fi,j(x∗) 2
i=1 j=1
(1 − q)σk2,1 + 2q max Lij(f (xk) − f (x∗)),

hence

E σk2+1,1 ≤ (1 − q)E σk2,1 + 2q max LijE f (xk) − f (x∗) .

Next, the deﬁnition of yk+1 implies

(4.82)

E σk2+1,2 | xk1, . . . , xkn

= 1 −n q n ∇fi(yk) − ∇fi(x∗) 2 + nq n ∇fi(xk) − ∇fi(x∗) 2

i=1

i=1

(C.3)
≤ (1 − q)σk2 + 2Lq(f (xk) − f (x∗)),

hence

E σk2+1,2 ≤ (1 − q)E σk2,2 + 2LqE f (xk) − f (x∗) .

Finally, we combine obtained inequalities and get

(4.83)

E [σk+1]

=
(4.82),(4.83)
≤
=

E σk2+1,1 + E σk2+1,2
(1 − q) E σk2,1 + E σk2,2 + 2(L + max Lij)qE f (xk) − f (x∗) (1 − q) E σk2 + 2(L + max Lij)qE f (xk) − f (x∗) ,

which concludes the proof.

Using Corollary C.4.7 we obtain the following theorem. Theorem 4.5.55. Assume that fi is µ-strongly convex and L-smooth and fi,j is convex and

173

max Lij-smooth for all i ∈ [n], j ∈ [m]. Then S-Local-SVRG satisﬁes Assumption C.4.1 with

A = 4L, A = 4 max Lij, B = B = 2, F = 4L2, F = 4L max Lij D1 = D1 = 0,

A = 4 manx Lij + 2L, B = n2 , F = 2L 2 manx Lij + L , D1 = 0,

σk2 = n1m n m ∇fi,j(yk) − ∇fi,j(x∗) 2 + n1 n ∇fi(yk) − ∇fi(x∗) 2 ,

i=1 j=1

i=1

ρ = q,

C = (L + max Lij)q,

G = 0,

D2 = 0,

256(1 − p2)(2 + q)γ2

H=

3p2q

,

D3 = 0

under assumption that







√



 

1

p3

 

γ ≤ min 56 max Lij + 4L + 32L ,

.
4(L+max Lij )(1+p)



 

3n

3n 32 2L(1 − p) L(2 + p) + p max Lij +

(1−q)

  

Moreover, for µ > 0 we have

E f (xK ) − f (x∗) ≤

1 − min γµ, q 4

K 2 x0 − x∗ 2 + 16γn2qσ02 + 1024L(1−3pp22)q(2+q)γ3σ02 γ

and when µ = 0 we have

E f (xK ) − f (x∗)

≤ 2 x0 − x∗ 2 + 16γn2qσ02 + 1024L(1−3pp22)q(2+q)γ3σ02 . γK

The theorem above together with Lemma A.5.3 implies the following result.

Corollary 4.5.56. Let assumptions of Theorem 4.5.55 hold with µ > 0. Then for q = 1/m, m ≥ 1/p,







√



 

1

p3

 

γ = min 56 max Lij + 4L + 32L ,

4(L+max Lij )(1+p)



 

3n

3n 32 2L(1 − p) L(2 + p) + p max Lij +

(1−q)

  

and for all K ≥ 1 we have E f (xK) − f (x∗) of order



O  L + max Lij +

p

n





(1 − p)L max Lij  Φ0 exp − min Λ, 1 K  ,

p

m

where Φ0 = 2 x0 − x∗ 2 + 16γn2qσ02 + 1024L(1−3pp22)q(2+q)γ3σ02 , Λ = L

max Lij

√µ

. That is,

(1−p)L max Lij

p+ n +

p

174

to achieve E f (xK) − f (x∗) ≤ ε in this case S-Local-SVRG requires





O m + L + max Lij +



pµ

nµ

 (1 − p)L max Lij
pµ  log

√

Lp + maxnLij +

(1−p)L max Lij p

ε

 Φ0




iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.
That is, S-Local-SVRG is the ﬁrst implementable linearly converging stochastic method with local updates with a convergence guarantee in terms of the number of communications that is not worse than that of GD even in the arbitrary heterogeneous data regime.
Next, we derive the following result for the convergence of S-Local-SVRG in the case when µ = 0.

Corollary 4.5.57. Let assumptions of Theorem 4.5.55 hold with µ = 0. Then for q = 1/m, m ≥ 1/p and







√



 

1

p3

 

γ0 = min 56 max Lij + 4L + 32L ,

4(L+max Lij )(1+p)



 

3n

3n 32 2L(1 − p) L(2 + p) + p max Lij +

(1−q)

  

γ = min γ0,

nR02 , 3

3p2R02

8mσ02 512L(1 − p2)(2m + 1)σ02

we have that E f (xK) − f (x∗) is of the order

 O


L + p max Lij/n +

(1 − p)L max Lij pK

R02 +

 mσ02R02 3 Lmσ02R04 √nK + p2/3K  ,

where R0 = x0 − x∗ . That is, to achieve E f (xK) − f (x∗) ≤ ε in this case S-Local-SVRG requires

 L + p max Lij/n +
K =O 

(1 − p)L max Lij pε

R02 +



mσ02R02 3 Lmσ02R04

√nε +

 p2/3ε 

iterations/oracle calls per node (in expectation) and 1/p times less communication rounds.

175

Remark 4.5.58. To get the rate from Tbl. 4.3 it remains to apply the following inequality:

σ02 = n1m n m ∇fi,j(x0) − ∇fi,j(x∗) 2 + n1 n ∇fi(x0) − ∇fi(x∗) 2

i=1 j=1

i=1

(4.6)
≤ 2 max L2ij + L2

x0 − x∗ 2.

4.6 Experiments
We perform multiple experiments to verify the theoretical claims of this chapter. Due to space limitations, we only present a single experiment in the main body; the rest can be found in Section C.2 of the appendix.
We demonstrate the beneﬁt of on-device variance reduction, which we introduce in this chapter. For that purpose, we compare standard Local-SGD (Algorithm 27) with our Local-SVRG (Algorithm 28) on a regularized logistic regression problem with LibSVM data [27]. For each problem instance, we compare the two algorithms with the stepsize γ ∈ {1, 0.1, 0.01} (we have normalized the data so that L = 1). The remaining details for the setup are presented in Section C.2.1 of the appendix.
Our theory predicts that both Local-SGD and Local-SVRG have identical convergence rate early on. However, the neighborhood of the optimum to which Local-SVRG converges is smaller comparing to Local-SGD. For both methods, the neighborhood is controlled by the stepsize: the smaller the stepsize is, the smaller the optimum neighborhood is. The price to pay is a slower rate at the beginning.
The results are presented in Figure 4.1. As predicted, Local-SVRG always outperforms Local-SGD as it converges to a better neighborhood. Figure 4.1 also demonstrates that one can trade the smaller neighborhood for the slower convergence by modifying the stepsize.

4.7 Conclusions and Future Work
This chapter develops a uniﬁed approach to analyzing and designing a wide class of local stochastic ﬁrst order algorithms. While our framework covers a broad range of methods, there are still some types of algorithms that we did not include but desire attention in future work. First, it would be interesting to study algorithms with biased local stochastic gradients; these are popular for minimizing ﬁnite sums; see SAG [193] or SARAH [157]. The second hitherto unexplored direction is including Nesterov’s acceleration [155] in our framework. This idea is gaining traction in the area of local methods already [161, 237]. However, it is not at all clear how this should be done and several attempts at achieving this uniﬁcation goal failed. The third direction is allowing for a regularized local objective, which has been under-explored in the FL community so far. Other compelling directions that we do not cover are the local higher-order or proximal methods [115, 161] and methods supporting partial participation [135].

176

Relative suboptimality

Relative suboptimality

100 10-1 10-2 10-3 10-4 10-5 10-6
0
100

w2a

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

madelon

10000

10-1

10-2 10-3 10-4
0
100 10-2 10-4

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

duke

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

10-6

10-8 0

2000

4000

6000

8000

Rounds of communication

10000

Relative suboptimality

Relative suboptimality

Relative suboptimality

a1a
100

10-1

10-2

10-3 10-4 10-5 10-6
0
10-1 10-3 10-5 10-7 10-9 10-11 10-13
0
10-1 10-3 10-5 10-7 10-9 10-11 10-13
0

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

mushrooms

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

phishing

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

Relative suboptimality

Figure 4.1: Comparison of standard Local-SGD (Alg. 27) and our Local-SVRG (Alg. 28) for varying γ. Logistic regression applied on LibSVM [27]. Other parameters: L = 1, µ = 10−4, τ =
40. Parameter n chosen as per Tbl. C.2 in the appendix.

177

Chapter 5
MARINA: Faster Non-Convex Distributed Learning
with Compression
5.1 Introduction
Non-convex1 optimization problems appear in various applications of machine learning, such as training deep neural networks [47] and matrix completion and recovery [130, 21]. Because of their practical importance, these problems gained much attention in recent years, which led to a rapid development of new eﬃcient methods for non-convex optimization problems [31], and especially the training of deep learning models [214].
Training deep neural networks is notoriously computationally challenging and time-consuming. In the quest to improve the generalization performance of modern deep learning models, practitioners resort to using increasingly larger datasets in the training process, and to support such workloads, it is imperative to use advanced parallel and distributed hardware, systems, and algorithms. Distributed computing is often necessitated by the desire to train models from data naturally distributed across several edge devices, as is the case in federated learning [100, 134]. However, even when this is not the case, distributed methods are often very eﬃcient at reducing the training time [64, 236]. Due to these and other reasons, distributed optimization has gained immense popularity in recent years.
However, distributed methods almost invariably suﬀer from the so-called communication bottleneck: the communication cost of information necessary for the workers to jointly solve the problem at hand is often very high, and depending on the particular compute architecture, workload, and algorithm used, it can be orders of magnitude higher than the computation cost. A popular technique for resolving this issue is communication compression [196, 100, 215], which is based on applying a lossy transformation/compression to the models, gradients, or tensors to be sent over the network to save on communication. Since applying a lossy compression generally decreases the utility of the exchanged messages, such an approach will typically lead to an increase in the number of communications, and the overall usefulness of this technique manifests
1The results from this chapter were obtained while I was a research intern at KAUST. We thank Konstantin Mishchenko (KAUST) for a suggestion related to the experiments, Elena Bazanova (MIPT) for the suggestions about improving the text, and Slavomír Hanzely (KAUST) and Egor Shulgin (KAUST) for spotting the typos.
178

itself in situations where the communication savings are larger compared to the increased need for the number of communication rounds [78]. The optimization and machine learning communities have exerted considerable eﬀort in recent years to design distributed methods supporting compressed communication. From many methods proposed, we emphasize VR-DIANA [79], FedCOMGATE [67], and FedSTEPH [32] because these papers contain the state-of-the-art results in the setup when the local loss functions can be arbitrary heterogeneous.
179

Table 5.1: Summary of the state-of-the-art results for ﬁnding an ε-stationary point for the problem (5.1), i.e., such a point xˆ that E ∇f (xˆ) 2 ≤ ε2. Dependences on the numerical constants, “quality” of the starting point, and smoothness constants are omitted in the complexity bounds. Abbreviations: “PP” = partial participation; “Communication complexity” = the number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization parameter (see Def. A.2.1); n = the number of nodes; m = the size of the local dataset; r = (expected) number of clients sampled at each iteration; b = the batchsize for VR-MARINA at the iterations with compressed communication. To simplify the bounds, we assume that the expected density ζQ of the quantization operator Q (see Def. A.2.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and 2-quantization, see [20]). We notice that [67] and [32] contain also better rates under diﬀerent assumptions on clients’ similarity.

Setup

Method

Citation

Communication Complexity Oracle Complexity

(5.1)
(5.1), (5.4)
(5.1), (5.5)

DIANA
FedCOMGATE (1) FedSTEPH, r = n
MARINA (Alg. 33)
DIANA VR-DIANA VR-MARINA (Alg. 34)
b = 1(2) DIANA (3) FedCOMGATE (3) VR-MARINA (Alg. 34) b =1

[139] [79] [120] [67] [32] Thm. 5.2.1 Cor. 5.2.2
[120]
[79] Thm. 5.3.2 Cor. 5.3.3
[139] [120] [67] Thm. 5.3.6 Cor. 5.3.7

√
1+(1+ω) ω/n ε2

1+ω ε2
1+ω/n ε4
√ 1+ω/ n
ε2

√

1+(1+ω) ε2

ω/√n + 1n+ε4ω

m2/3+ω 1+ω/n

ε2
1+max{ω,√(1+ω)m}/√n
ε2

√ 1+(1+εω2) ω/n + 1n+ε4ω

1+ω ε2

√

√

1+ωε2/ n + n1ε+3ω

√
1+(1+ω) ω/n ε2

1+ω nε4 1+ω/n ε4
√ 1+ω/ n
ε2

√

1+(1+ω) ε2

ω/√n + 1n+ε4ω

m2/3+ω 1+ω/n

ε2
1+max{ω,√(1+ω)m}/√n
ε2

√ 1+(1+εω2) ω/n + 1n+ε4ω

1+ω nε4

√

√

1+ωε2/ n + n1ε+3ω

VR-MARINA (Alg. 34)

b =Θ

1 nε2

Thm. 5.3.6 Cor. 5.3.7

√ 1+ω/ n
ε2

√
1+nωε/4 n + 1n+ε3ω

PP, (5.1)

FedSTEPH PP-MARINA (Alg. 36)

[32] Thm. 5.4.1 Cor. 5.4.2

1+rεω4/n + (1r+(nω−)(1n)−ε4r)
√ 1+(1+ω) n/r
ε2

1+rεω4/n + (1r+(nω−)(1n)−ε4r)
√ 1+(1+ω) n/r
ε2

(1) The results for FedCOMGATE are derived under assumption that for all vectors x1, . . . , xn ∈ Rd the quantization operator Q satisﬁes E n1 ni=1 Q(xj ) 2 − Q n1 ni=1 xj 2 ≤ G for some constant G ≥ 0. In fact, this
assumption does not hold for classical quantization operators like RandK and 2-quantization on Rd. The counterexample: n = 2 and x1 = −x2 = (t, t, . . . , t) with arbitrary large t > 0. (2) One can even further improve the communication complexity by increasing b . (3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.

5.1.1 Contributions

We propose several new distributed optimization methods supporting compressed communication, speciﬁcally focusing on smooth but nonconvex problems of the form

1n

min f (x) =

fi(x) ,

x∈Rd

n i=1

(5.1)

180

where n workers/devices/clients/peers are connected in a centralized way with a parameter-server, and client i has an access to the local loss function fi only. We establish strong complexity rates for them and show that they are better than previous state-of-the-art results.

• MARINA. The main contribution of our chapter is a new distributed method supporting communication compression called MARINA (Alg 33). In this algorithm, workers apply an unbiased compression operator to the gradient diﬀerences at each iteration with some probability and send them to the server that performs aggregation by averaging. Unlike all known methods operating with unbiased compression operators, this procedure leads to a biased gradient estimator. We prove convergence guarantees for MARINA, which are strictly better than previous state-of-the-art methods (see Table 5.1). For example, MARINA’s rate O( 1+ωε2/√n ) is O(√ω) times better than that of the state-of-the-art method DIANA [139], where ω is the variance parameter associated with the deployed compressor. For example, in the case of the Rand1 sparsiﬁcation compressor,
√ we have ω = d − 1, and hence we get an improvement by the factor O( d). Since the number d of features can be truly very large when training modern models, this is a substantial improvement that can even amount to several orders of magnitude.

• Variance Reduction on Nodes. We generalize MARINA to VR-MARINA, which can handle the

situation when the local functions fi have either a ﬁnite-sum (each fi is an average of m functions)

or an expectation form, and when it is more eﬃcient to rely on local stochastic gradients rather

than on local gradients. When compared with MARINA, VR-MARINA additionally performs local

variance reduction on all nodes, progressively removing the variance coming from the stochastic

approximation, leading to a better oracle complexity than previous state-of-the-art results (see
√
Table 5.1). When no compression is used (i.e., ω = 0), the rate of VR-MARINA is O( √nmε2 ), while

the

rate

of

the

state-of-the-art

method

VR-DIANA

is

O

(

m2/3
2

).

This

is

an

improvement

by

the

√

ε

factor O( nm1/6). When much compression is applied, and ω is large, our method is faster

by the factor O( mm1/22/+3+ω1ω/2 ). In the special case, when there is just a single node (n = 1), and

no compression is used, VR-MARINA reduces to the PAGE method of [118]; this is an optimal

ﬁrst-order algorithm for smooth non-convex ﬁnite-sum/online optimization problems.

• Partial Participation. We develop a modiﬁcation of MARINA allowing for partial participation of the clients, which is a feature critical in federated learning. The resulting method, PP-MARINA, has superior communication complexity to the existing methods developed for this settings (see Table 5.1).

• Convergence Under the Polyak-Łojasiewicz Condition. We analyze all proposed methods for problems satisfying the Polyak-Łojasiewicz condition [166, 129]. Again, the obtained results are strictly better than previous ones (see Table 5.2). Statements and proofs of all these results are in the Appendix.

• Simple Analysis. The simplicity and ﬂexibility of our analysis oﬀer several extensions. For example, one can easily generalize our analysis to the case of diﬀerent quantization operators and diﬀerent batch sizes used by clients. Moreover, one can combine the ideas of VR-MARINA and

181

PP-MARINA and obtain a single distributed algorithm with compressed communications, variance
reduction on nodes, and clients’ sampling. We did not do this to keep the exposition simpler.
Table 5.2: Summary of the state-of-the-art results for ﬁnding an ε-solution for the problem (5.1) satifying Polyak-Łojasiewicz condition (see As. 5.2.4), i.e., such a point xˆ that E [f (xˆ) − f (x∗)] ≤ ε. Dependences on the numerical constants and log(1/ε) factors are omitted and all smoothness constanst are denoted by L in the complexity bounds. Abbreviations: “PP” = partial participation; “Communication complexity” = the number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization parameter (see Def. A.2.1); n = the number of nodes; m = the size of the local dataset; r = (expected) number of clients sampled at each iteration; b = the batchsize for VR-MARINA at the iterations with compressed communication. To simplify the bounds, we assume that the expected density ζQ of the quantization operator Q (see Def. A.2.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and 2-quantization, see [20]). We notice that [67] and [32] contain also better rates under diﬀerent assumptions on clients’ similarity.

Setup (5.1)

Method DIANA FedCOMGATE (1)
MARINA (Alg. 33)

(5.1), (5.4)
(5.1), (5.5)

DIANA
VR-DIANA VR-MARINA (Alg. 34)
b = 1(2) DIANA (3) FedCOMGATE (3) VR-MARINA (Alg. 34) b =1

Citation
[120] [67] Thm. 5.2.5 Cor. D.1.4
[120]
[120] Thm. D.2.4 Cor. D.2.5
[139] [120] [67] Thm. D.2.9 Cor. D.2.10

Communication Complexity √
L(1+(1+ω) ω/n)
µ L(1+ω)
µ

√
ω + L(1+µω/ n)

√ L(1+(1+ω) ω/n) +
µ
+ L(n1+µω√) Lµ + 1ε
L m2/3+ω 1+ω/n

µ

ω + m+

L(1+max{ω,√(1+ω)m}/√n)

+

µ

√ 1+(1+εω2) ω/n + 1n+ε4ω

L(1+ω) µ

√

√

ω + L(1+µω/ n) + L nµ1+ε ω

Oracle Complexity √
L(1+(1+ω) ω/n)
µ L(1+ω)
nµε

√
ω + L(1+µω/ n)

√ L(1+(1+ω) ω/n) +
µ
+ L(n1+µω√) Lµ + 1ε
L m2/3+ω 1+ω/n

µ

ω + m+

L(1+max{ω,√(1+ω)m}/√n)

+

µ

√ 1+(1+εω2) ω/n + 1n+ε4ω

L(1+ω) nµε

√

√

ω + L(1+µω/ n) + L nµ1+ε ω

PP, (5.1)

VR-MARINA (Alg. 34) b = Θ n1µε FedSTEPH (4)
PP-MARINA (Alg. 36)

Thm. D.2.9 Cor. D.2.10
[32] Thm. D.3.3 Cor. D.3.4

√
ω + L(1+µω/ n)
L 3/2 µ
√
(ω+r1)n + L(1+(1+µω) n/r)

√

1+ω nµε

+

L(1+ω/ nµ2 ε

n)

+

L(1+√ω) nµ2 ε

L 3/2 µ
√
(ω+r1)n + L(1+(1+µω) n/r)

(1) The results for FedCOMGATE are derived under assumption that for all vectors x1, . . . , xn ∈ Rd the quantization operator Q satisﬁes E n1 ni=1 Q(xj ) 2 − Q n1 ni=1 xj 2 ≤ G for some constant G ≥ 0. In fact, this assumption does

not hold for classical quantization operators like RandK and 2-quantization on Rd.
x1 = −x2 = (t, t, . . . , t) with arbitrary large t > 0. (2) One can even further improve the communication complexity by increasing b . (3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.

The counterexample:

n = 2 and

(4) The rate is derived under assumption that r = Ω((1 + ω) L/µ log(1/ε)).

182

5.1.2 Related Work
Non-Convex Optimization. Since ﬁnding a global minimum of a non-convex function is, in general, an NP-hard problem [143], many researchers in non-convex optimization focus on relaxed goals such as ﬁnding an ε-stationary point. The theory of stochastic ﬁrst-order methods for ﬁnding ε-stationary points is well-developed: it contains lower bounds for expectation minimization without smoothness of stochastic realizations [8] and for ﬁnite-sum/expectation minimization [40, 118] as well as optimal methods matching the lower bounds (see [31, 118] for the overview). Recently, distributed variants of such methods were proposed [213, 201, 91].
Compressed Communications. Works on distributed methods supporting communication compression can be roughly split into two large groups: the ﬁrst group focuses on methods using unbiased compression operators (which refer to as quantizations in this chapter), such as RandK, and the second one studies methods using biased compressors such as TopK. One can ﬁnd a detailed summary of the most popular compression operators in [188, 20].
Unbiased Compression. In this line of work, the ﬁrst convergence result in the non-convex case was obtained by [4] for QSGD, under assumptions that the local loss functions are the same for all workers, and the stochastic gradient has uniformly bounded second moment. After that, [139] proposed DIANA (and its momentum version) and proved its convergence rate for non-convex problems without any assumption on the boundedness of the second moment of the stochastic gradient, but under the assumption that the dissimilarity between local loss functions is bounded. This restriction was later eliminated by [79] for the variance reduced version of DIANA called VR-DIANA, and the analysis was extended to a large class of unbiased compressors. Finally, the results for QSGD and DIANA were recently generalized and tightened by [120] in a unifying framework that included many other methods as well.
Biased Compression. Biased compression operators are less “optimization-friendly” than unbiased ones. Indeed, one can construct a simple convex quadratic problem for which distributed SGD with Top1 compression diverges exponentially fast [20]. However, this issue can be resolved using error compensation [196]. The ﬁrst analysis of error-compensated SGD (EC-SGD) for nonconvex problems was obtained by [88] for homogeneous problems under the assumption that the second moment of the stochastic gradient is uniformly bounded. The last assumption was recently removed from the analysis of EC-SGD by [209, 20], while the ﬁrst results without the homogeneity assumption were obtained by [96] for Choco-SGD, but still under the assumption that the second moment of the stochastic gradient is uniformly bounded. This issue was resolved by [20]. In general, the current understanding of optimization methods with biased compressors is far from complete: even in the strongly convex case, the ﬁrst linearly converging [57] and accelerated [168] error-compensated stochastic methods were proposed just recently.
Other Approaches. Besides communication compression, there are also diﬀerent techniques aiming to reduce the overall communication cost of distributed methods. The most popular ones are based on decentralized communications and multiple local steps between communication
183

rounds, where the second technique is very popular in federated learning [100, 83]. One can ﬁnd the state-of-the-art distributed optimization methods using these techniques and their combinations in [122, 86, 117, 97]. Moreover, there exist results based on the combinations of communication compression with either decentralized communication, e.g., Choco-SGD [96], or local updates, e.g., Qsparse-Local-SGD [15], FedCOMGATE [67], FedSTEPH [32], where in [15] the convergence rates were derived under an assumption that the stochastic gradient has uniformly bounded second moment and the results for Choco-SGD, FedCOMGATE, FedSTEPH were described either earlier in the text, or in Table 5.1.

5.1.3 Preliminaries
We will rely on two key assumptions throughout the text.
Assumption 5.1.1 (Uniform lower bound). There exists f∗ ∈ R such that f (x) ≥ f∗ for all x ∈ Rd.

Assumption 5.1.2 (L-smoothness). We assume that fi is Li-smooth for all i ∈ [n] = {1, 2, . . . , n} meaning that the following inequality holds ∀x, y ∈ Rd, ∀i ∈ [n]:

∇fi(x) − ∇fi(y) ≤ Li x − y .

This

assumption

implies

that

f

is

Lf -smooth

with

L2f

≤

L2

=

1 n

n i=1

L2i .

(5.2)

5.2 MARINA: Compressing Gradient Diﬀerences
In this section, we describe the main algorithm of this work: MARINA (see Algorithm 33). At each iteration of MARINA, each worker i either sends to the server the dense vector ∇fi(xk+1) with probability p, or it sends the quantized gradient diﬀerence Q ∇fi(xk+1) − ∇fi(xk)) with probability 1 − p. In the ﬁrst situation, the server just averages the vectors received from workers and gets gk+1 = ∇f (xk+1), whereas in the second case, the server averages the quantized diﬀerences from all workers and then adds the result to gk to get gk+1. Moreover, if Q is identity quantization, i.e., Q(x) = x, then MARINA reduces to Gradient Descent (GD).
However, for non-trivial quantizations, we have E[gk+1 | xk+1] = ∇f (xk+1) unlike all other distributed methods using exclusively unbiased compressors we know of. That is, gk+1 is a biased stochastic estimator of ∇f (xk+1). However, MARINA is an example of a rare phenomenon in stochastic optimization when the bias of the stochastic gradient helps to achieve better complexity.

5.2.1 Convergence Results for Generally Non-Convex Problems
We start with the following result.

184

Algorithm 33 MARINA

1: Input: starting point x0, stepsize γ, probability p ∈ (0, 1], number of iterations K

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p)

5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk



8:

Set gk+1 = ∇fi(xk+1),

if ck = 1,

i

gk + Q ∇fi(xk+1) − ∇fi(xk)) , if ck = 0

9: end for

10:

gk+1

=

1 n

n i=1

gik+1

11: end for

12: Return: xˆK chosen uniformly at random from {xk}Kk=−01

Theorem 5.2.1. Let Assumptions 5.1.1 and 5.1.2 be satisﬁed. Then, after

K = O ∆0L 1 + ε2

(1 − p)ω pn

iterations

with

∆0

=

f (x0)

−

f∗,

L2

=

1 n

n i=1

L2i

and

the

stepsize

γ≤ L

1 1 + (1−pnp)ω

MARINA produces point xˆK for which E[ ∇f (xˆK) 2] ≤ ε2.
One can ﬁnd the full statement of the theorem together with its proof in Section D.1.1 of the Appendix.
The following corollary provides the bounds on the number of iterations/communication rounds and estimates the total communication cost needed to achieve an ε-stationary point in expectation. Moreover, for simplicity, throughout the chapter we assume that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.
Corollary 5.2.2. Let the assumptions of Theorem 5.2.1 hold and p = ζQ/d. If
γ≤ 1 , L 1 + ωn ζdQ − 1

185

then MARINA requires

O ∆0L 1 + ω d − 1

ε2

n ζQ

iterations/communication rounds in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).

Let us clarify the obtained result. First of all, if ω = 0 (no quantization), then ζQ = 0 and the rate coincides with the rate of Gradient Descent (GD). Since GD is optimal among ﬁrst-order methods in terms of reducing the norm of the gradient [26], the dependence on ε in our bound cannot be improved in general. Next, if n is large enough, i.e., n ≥ ω(d/ζQ − 1), then2 the iteration complexity of MARINA (method with compressed communications) and GD (method with dense communications) coincide. This means that in this regime, MARINA is able to reach a provably better communication complexity than GD!

Remark 5.2.3. When p = 1/(ω+1) the complexity bound for MARINA becomes

O

∆0L

ω 1+ √

.

ε2

n

Since the deﬁnition of quantization (Deﬁnition A.2.1) covers uniform coordinate-wise randomization and directional derivative oracle (directions are sampled from the uniform distribution on the unit Euclidean sphere) with ω = d − 1, the dependence on ω cannot be improved in general. One can prove this using the standard results for derivative-free methods from [150] that multiplicative dependence on O(d) is unavoidable and approximating partial or directional derivative oracle using ﬁnite diﬀerences. Similar arguments hold for the methods from the next sections as well.

5.2.2 Convergence Results Under Polyak-Łojasiewicz Condition

In this section, we provide a complexity bounds for MARINA under the Polyak-Łojasiewicz (PŁ) condition.

Assumption 5.2.4 (PŁ condition). Function f satisﬁes Polyak-Łojasiewicz (PŁ) condition

with parameter µ, i.e.,

∇f (x) 2 ≥ 2µ (f (x) − f (x∗)) .

(5.3)

holds for x∗ = arg minx∈Rd f (x) and for all x ∈ Rd.

Under this and previously introduced assumptions, we derive the following result.

2For 2-quantization this requirement is satisﬁed when n ≥ d.

186

Theorem 5.2.5. Let Assumptions 5.1.1, 5.1.2 and 5.2.4 be satisﬁed. Then, after

K = O max 1 , L 1 + (1 − p)ω

pµ

pn

log ∆0 ε

iterations

with

∆0

=

f (x0)

−

f (x∗),

L2

=

1 n

n i=1

L2i

and

the

stepsize


   γ ≤ min
 L

1 1 + 2(1p−np)ω





p

 

, 2µ 





MARINA produces a point xK for which E[f (xK) − f (x∗)] ≤ ε.
One can ﬁnd the full statement of the theorem together with its proof in Section D.1.2 of the Appendix.

5.3 MARINA and Variance Reduction

Throughout this section, we assume that the local loss on each node has either a ﬁnite-sum form

(ﬁnite sum case),

1m

fi(x) =

fij (x),

m j=1

(5.4)

or an expectation form (online case),

fi(x) = Eξi∼Di [fξi (x)].

(5.5)

5.3.1 Finite Sum Case

In this section, we generalize MARINA to problems of the form (5.1)+(5.4), obtaining VR-MARINA (see Algorithm 34). At each iteration of VR-MARINA, devices are to compute the full gradients ∇fi(xk+1) and send them to the server with probability p. Typically, p ≤ 1/m and m is large, meaning that workers compute full gradients rarely (once per ≥ m iterations in expectation). At other iterations, workers compute minibatch stochastic gradients evaluated at the current and previous points, compress them using an unbiased compression operator, i.e., quantization/quantization operator, and send the resulting vectors gik+1 − gk to the server. Moreover, if Q is the identity quantization, i.e., Q(x) = x, and n = 1, then MARINA reduces to the optimal method PAGE [118].
In this part, we will rely on the following average smoothness assumption.

Assumption 5.3.1 (Average L-smoothness). For all k ≥ 0 and i ∈ [n] the minibatch stochastic

gradients

diﬀerence

∆ki

=

1 b

j∈I (∇fij(xk+1) − ∇fij(xk)) computed on the i-th worker

i,k

187

Algorithm 34 VR-MARINA: ﬁnite sum case

1: Input: starting point x0, stepsize γ, minibatch size b , probability p ∈ (0, 1], number of

iterations K

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p)

5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk



8:

Set gk+1 = ∇fi(xk+1),

if ck = 1, where I is

i

gk + Q

1 b

j∈I (∇fij(xk+1) − ∇fij(xk)) , if ck = 0,

i,k

i,k

the set of the indices in the minibatch, |Ii,k| = b

9: end for

10:

gk+1

=

1 n

n i=1

gik+1

11: end for

12: Return: xˆK chosen uniformly at random from {xk}Kk=−01

satisﬁes E ∆ki | xk, xk+1 = ∆ki and

E ∆ki − ∆ki 2 | xk, xk+1 ≤ Lb2i xk+1 − xk 2

(5.6)

with some Li ≥ 0, where ∆ki = ∇fi(xk+1) − ∇fi(xk).
This assumption is satisﬁed in many standard minibatch regimes. In particular, if Ii,k = {1, . . . , m}, then Li = 0, and if Ii,k consists of b i.i.d. samples from the uniform distributions on {1, . . . , m} and fij are Lij-smooth, then Li ≤ maxj∈[m] Lij.
Under this and the previously introduced assumptions, we derive the following result.

Theorem 5.3.2. Consider the ﬁnite sum case (5.1)+(5.4). Let Assumptions 5.1.1, 5.1.2 and 5.3.1 be satisﬁed. Then, after

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

iterations

with

∆0

=

f (x0)

−

f∗,

L2

=

1 n

n i=1

L2i ,

L2

=

1 n

n i=1

L2i

and

the

stepsize

γ≤ L+

1 1p−np ωL2 + (1+bω)L2

VR-MARINA produces such a point xˆK that E[ ∇f (xˆK) 2] ≤ ε2. One can ﬁnd the full statement of the theorem together with its proof in Section D.2.1 of the

188

Appendix.

Corollary 5.3.3. Let the assumptions of Theorem 5.3.2 hold and p = min {ζQ/d, b /(m+b )},

where b ≤ m. If

γ≤ L+

1, max{d/ζQn−1,m/b } ωL2 + (1+bω)L2

then VR-MARINA requires

  O  ∆ε20 L 1 +

 ω max {d/ζQ − 1, m/b }  + L
n

 (1 + ω) max {d/ζQ − 1, m/b }
 nb

iterations/communication rounds and O (m + b K) stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).
First of all, when workers quatize diﬀerences of the full gradients, then Ii,k = {1, . . . , m} for all i ∈ [n] and k ≥ 0, implying L = 0. In this case, the complexity bounds for VR-MARINA recover the ones for MARINA. Next, when ω = 0 (no quantization) and n = 1, our bounds for iteration and oracle complexities for VR-MARINA recover the bounds for PAGE [120], which is optimal for ﬁnite-sum smooth non-convex optimization. This observation implies that the dependence on ε and m in the complexity bounds for VR-MARINA cannot be improved in the class of ﬁrst-order stochastic methods. Next, we notice that up to the diﬀerences in smoothness constants, the iteration and oracle complexities for VR-MARINA beneﬁt from the number of workers n. Finally, as Table 5.1 shows, the rates for VR-MARINA are strictly better than ones for the previous state-of-the-art method VR-DIANA [79].
We provide the convergence results for VR-MARINA in the ﬁnite-sum case under the PolyakŁojasiewicz condition, together with complete proofs, in Section D.2.1 of the Appendix.

5.3.2 Online Case

In this section, we focus on problems of type (5.1)+(5.5). For this type of problems, we

consider a slightly modiﬁed version of VR-MARINA. That is, we replace line 8 in Algorithm 34

with the following update rule:

gik+1

=

1 b

j∈Ii,k ∇fξk (xk+1) if ck = 1, and gik+1 = gk +

ij

Q b1 j∈I (∇fξk (xk+1) − ∇fξk (xk)) otherwise, where Ii,k, Ii,k are the sets of the indices in

i,k

ij

ij

the minibatches, |Ii,k| = b, |Ii,k| = b , and ξikj is independently sampled from Di for i ∈ [n],

j ∈ [m] (see Algorithm 35).

Before we provide our convergence results in this setup, we reformulate Assumption 5.3.1 for the online case.

Assumption 5.3.4 (Average L-smoothness). For all k ≥ 0 and i ∈ [n] the minibatch stochastic

189

Algorithm 35 VR-MARINA: online case

1: Input: starting point x0, stepsize γ, minibatch sizes b, b < b, probability p ∈ (0, 1], number

of iterations K

2:

Initialize g0

=

1 nb

n i=1

j∈Ii,0 ∇fξ0 (xk+1), where Ii,0 is the set of the indices in the minibatch,

ij

|Ii,0| = b, and ξi0j is independently sampled from Di for i ∈ [n], j ∈ [m]

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p) 5: Broadcast gk to all workers

6: for i = 1, . . . , n in parallel do

7:

xk+1 = xk − γgk

1 b

j∈I

∇fξk (xk+1),

if ck = 1,

8: Set gik+1 = gk + Qi,kb1 jij∈Ii,k (∇fξikj (xk+1) − ∇fξikj (xk)) , if ck = 0, where

Ii,k, Ii,k are the sets of the indices in the minibatches, |Ii,k| = b, |Ii,k| = b , and ξikj is independently sampled from Di for i ∈ [n], j ∈ [m]

9: end for

10:

gk+1

=

1 n

11: end for

n i=1

gik+1

12: Return: xˆK chosen uniformly at random from {xk}Kk=−01

gradients

diﬀerence

∆ki

=

1 b

j∈I (∇fξk (xk+1) − ∇fξk (xk)) computed on the i-th worker

i,k

ij

ij

satisﬁes E ∆ki | xk, xk+1 = ∆ki and

E ∆ki − ∆ki 2 | xk, xk+1 ≤ Lb2i xk+1 − xk 2

(5.7)

with some Li ≥ 0, where ∆ki = ∇fi(xk+1) − ∇fi(xk).
Moreover, we assume that the variance of the stochastic gradients on all nodes is uniformly upper bounded.

Assumption 5.3.5. We assume that for all i ∈ [n] there exists such constant σi ∈ [0, +∞) that for all x ∈ Rd

Eξi∼Di

Eξi∼Di [∇fξi (x)] = ∇fi(x), ∇fξi (x) − ∇fi(x) 2 ≤ σi2.

(5.8) (5.9)

Under these and previously introduced assumptions, we derive the following result. Theorem 5.3.6. Consider the online case (5.1)+(5.5). Let Assumptions 5.1.1, 5.1.2, 5.3.4 and

190

6.3.3 be satisﬁed. Then, after

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

iterations

with

∆0

=

f (x0)

−

f∗,

L2

=

1 n

n i=1

L2i ,

L2

=

1 n

n i=1

L2i ,

the

stepsize

γ≤ L+

1, 1p−np ωL2 + (1+bω)L2

and

b

=

Θ (σ2/(nε2)) ,

σ2

=

1 n

E[ ∇f (xˆK ) 2] ≤ ε2.

n i=1

σi2,

VR-MARINA

produces

a

point

xˆK

for

which

One can ﬁnd the full statement of the theorem, together with its proof, in Section D.2.2 of the Appendix.

Corollary 5.3.7. Let the assumptions of Theorem 5.3.6 hold and choose p = min {ζQ/d, b /(b+b )}, where b ≤ b, b = Θ (σ2/(nε2)). If

γ≤ L+

1, max{d/ζQn −1,b/b } ωL2 + (1+bω)L2

then VR-MARINA requires

O ∆0 L 1 + ε2

ω

d

σ2

n max ζQ − 1, nb ε2

(1 + ω)

d

σ2

+ L nb max ζQ − 1, nb ε2

iterations/communication rounds and O(ζQK + σ2/(nε2)) stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is O(d + ζQK).
Similarly to the ﬁnite-sum case, when ω = 0 (no quantization) and n = 1, our bounds for iteration and oracle complexities for VR-MARINA recover the bounds for PAGE [120], which is optimal for online smooth non-convex optimization as well. That is, the dependence on ε in the complexity bound for VR-MARINA cannot be improved in the class of ﬁrst-order stochastic methods. As previously, up to the diﬀerences in smoothness constants, the iteration and oracle complexities for VR-MARINA beneﬁt from an increase in the number of workers n.
We provide the convergence results for VR-MARINA in the online case under the Polyak-Łojasiewicz condition, together with complete proofs, in Section D.2.2 of the Appendix.

191

5.4 MARINA and Partial Participation

Finally, we propose another modiﬁcation of MARINA. In particular, we prove an option for

partial participation of the clients - a feature important in federated learning. The resulting

method is called PP-MARINA (see Algorithm 36). At each iteration of PP-MARINA, the server

receives the quantized gradient diﬀerences from r clients with probability 1 − p, and aggregates

full gradients from all clients with probability p, i.e., PP-MARINA coincides with MARINA up

to the following diﬀerence:

gik+1

= ∇fi(xk+1), gk+1

=

1 n

n i=1

gik+1

if

ck

=

1,

and

gik+1

=

gk + Q ∇fi(xk+1) − ∇fi(xk)) , gk+1 = 1r ik∈Ik gikk+1 otherwise, where Ik is the set of r i.i.d.

samples from the uniform distribution over {1, . . . , n}. That is, if the probability p is chosen to

be small enough, then with high probability the server receives only quantized vectors from a

subset of clients at each iteration.

Algorithm 36 PP-MARINA

1: Input: starting point x0, stepsize γ, probability p ∈ (0, 1], number of iterations K, clients-

batchsize r ≤ n

2: Initialize g0 = ∇f (x0)

3: for k = 0, 1, . . . , K − 1 do

4: Sample ck ∼ Be(p)

5: Choose Ik = {1, . . . , n} if ck = 1, and choose Ik as the set of r i.i.d. samples from the

uniform distribution over {1, . . . , n} otherwise

6: Broadcast gk to all workers

7: for i = 1, . . . , n in parallel do

8:

xk+1 = xk − γgk



9:

Set gk+1 = ∇fi(xk+1)

if ck = 1,

i

gk + Q ∇fi(xk+1) − ∇fi(xk) if ck = 0.

10: end for 

∇f (xk+1)



11: Set gk+1 = gk + 1r

Q ∇fik (xk+1) − ∇fik (xk)



ik ∈Ik

12: end for

13: Return: xˆK chosen uniformly at random from {xk}Kk=−01

if ck = 1, if ck = 0.

Below, we provide a convergence result for PP-MARINA for smooth non-convex problems. Theorem 5.4.1. Let Assumptions 5.1.1 and 5.1.2 be satisﬁed. Then, after

K = O ∆0L 1 + ε2

(1 − p)(1 + ω) pr

iterations

with

∆0

=

f (x0)

−

f∗,

L2

=

1 n

n i=1

L2i

and

the

stepsize

γ≤ L 1+

1
(1−p)(1+ω) pr

192

PP-MARINA produces a point xˆK for which E[ ∇f (xˆK) 2] ≤ ε2.
One can ﬁnd the full statement of the theorem together with its proof in Section D.3.1 of the appendix.

Corollary 5.4.2. Let the assumptions of Theorem 5.4.1 hold and choose p = ζQr/(dn), where

r ≤ n. If

γ≤ 1 , L 1 + 1+b ω ζdQnr − 1

then PP-MARINA requires

O ∆0L 1 + ε2

1+ω r

dn − 1 ζQr

iterations/communication rounds to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost is O (dn + ζQrK).
When r = n, i.e., all clients participate in communication with the server at each iteration, the rate for PP-MARINA recovers the rate for MARINA under the assumption that (1 + ω)(d/ζQ − 1) = O(ω(d/ζQ − 1)), which holds for a wide class of quantization operators, e.g., for identical quantization, RandK, and p-quantization. In general, the derived complexity is strictly better than previous state-of-the-art one (see Table 5.1).
We provide the convergence results for PP-MARINA under the Polyak-Łojasiewicz condition, together with complete proofs, in Section D.3.2 of the Appendix.

5.5 Numerical Experiments

5.5.1 Binary Classiﬁcation with Non-Convex Loss
We conduct several numerical experiments3 on binary classiﬁcation problem involving non-convex loss [242] (used for two-layer neural networks) with LibSVM data [27] to justify the theoretical claims of the chapter. That is, we consider the following optimization problem:

1N xm∈iRnd f (x) = N t=1 (at x, yi) ,

(5.10)

where {at} ∈ Rd, yi ∈ {−1, 1} for all t = 1, . . . , N , and the function : Rd → R is deﬁned as

(b, c) = 1 − 1 2 . 1 + exp(−bc)

3Our code is available at https://github.com/burlachenkok/marina.

193

The datasets were taken from LibSVM [27] and split into ﬁve equal parts among ﬁve clients (we excluded N − 5 · N/5 last datapoints from each dataset), see the summary in Table C.2.
Table 5.3: Summary of the datasets and splitting of the data among clients (Figure 5.1).

Dataset n N (# of datapoints) d (# of features)

mushrooms 5

8 120

112

w8a

5

49 745

300

phishing 5

11 055

69

a9a

5

32 560

124

The code was written in Python 3.8 using mpi4py to emulate the distributed environment and then was executed on a machine with 48 cores, each is Intel(R) Xeon(R) Gold 6246 CPU 3.30GHz.
In our experiments, we compare MARINA with the full-batch version of DIANA, and then VR-MARINA with VR-DIANA. We exclude FedCOMGATE and FedPATH from this comparison since they have signiﬁcantly worse oracle complexities (see Table 5.1). Since one of the main goals of our experiments is to justify the theoretical ﬁndings of the chapter, in the experiments, we used the stepsizes from the corresponding theoretical results for the methods (for DIANA and VR-DIANA the stepsizes were chosen according to [79, 120]). Next, to compute the stochastic gradients, we use batchsizes = max{1, m/100} for VR-MARINA and VR-DIANA.
The results for the full-batched methods are reported in Figure 5.1, and the comparison of VR-MARINA and VR-DIANA is given in Figure 5.2. Clearly, in both cases, MARINA and VR-MARINA show faster convergence than the previous state-of-the-art methods, DIANA and VR-DIANA, for distributed non-convex optimization with compression in terms of ∇f (xk) 2 and f (xk) decrease w.r.t. the number of communication rounds, oracle calls per node and the total number of transferred bits from workers to the master.

194

f(xk)

jjrf(xk)jj2

mushrooms

10-1

10-2
0

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms

10-1

10-2 10-3
0
10-1

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

mushrooms

10-1 10-2

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

10-1

w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0
10-1 10-2 10-3

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
w8a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

w8a

10-1

MARINA (K=1) MARINA (K=5)

MARINA (K=10)

10-2

DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2 × 10-1
10-1
6 × 10-2

phishing
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds phishing

10-3 10-4

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-5
0
2 × 10-1
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
phishing
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

6 × 10-2

0.0 10-3 10-4

0.2

0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

phishing

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-5

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1
1.4 × 10-1

a9a

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

1.2 × 10-1

10-1 0
10-1 10-2 10-3

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
a9a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-4

0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
a9a
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

1.2 × 10-1

10-1 0.0
10-1 10-2 10-3

0.2

0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

a9a

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

10-4

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

f(xk)

jjrf(xk)jj2

Figure 5.1: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (5.10) with LibSVM data [27]. Parameter n is chosen as per Table C.2 (n = 5). Stepsizes for the methods are chosen according to the theory. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

195

f(xk)

jjrf(xk)jj2

f(xk)

mushrooms

10-1 10-2 0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms

10-1 10-2

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0
10-1

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-2 0
10-1 10-2

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
mushrooms
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
mushrooms

10-1 10-2 0

100

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
200 300 400 500 600 # of Epochs
mushrooms

10-1 10-2

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-3 0

100 200 300 400 500 600 # of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

w8a

2 × 10-1

10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1
10-2
10-3

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds w8a

2 × 10-1

10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1
10-2
10-3

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

2000000

4000000 6000000
#bits/n w8a

8000000

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
2 × 10-1
10-1
6 × 10-2 4 × 10-2 3 × 10-2
0
10-1 10-2 10-3

2000000

4000000 6000000
#bits/n

8000000

w8a

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

100 200 300 400 500 600
# of Epochs w8a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0

100 200 300 400 500 600

# of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

10-1

phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2

2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds phishing

10-3 10-4 10-5
0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds

phishing

10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-3 10-4

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n
phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-5
0

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n

phishing

10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-3 10-4 10-5
0

100 200 300 400 500 600
# of Epochs phishing
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
100 200 300 400 500 600
# of Epochs

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

f(xk)

2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1
1.4 × 10-1
1.2 × 10-1

a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds a9a

10-2

10-4 10-6 10-8
0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1
1.2 × 10-1

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0
10-2 10-4 10-6

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n

a9a

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

10-8
0
2.6 × 10-1 2.4 × 10-1 2.2 × 10-1
2 × 10-1 1.8 × 10-1 1.6 × 10-1 1.4 × 10-1
1.2 × 10-1

1000000 2000000 3000000 4000000 5000000 6000000 7000000
#bits/n a9a
VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)

0 100 200 300 400 500 600 # of Epochs a9a

10-2

10-4 10-6 10-8
0

VR-MARINA (K=1) VR-MARINA (K=5) VR-MARINA (K=10) VR-DIANA (K=1) VR-DIANA (K=5) VR-DIANA (K=10)
100 200 300 400 # of Epochs

500

600

jjrf(xk)jj2

f(xk)

jjrf(xk)jj2

Figure 5.2: Comparison of VR-MARINA with VR-DIANA on binary classiﬁcation problem involving non-convex loss (5.10) with LibSVM data [27]. Parameter n is chosen as per Table C.2 (n = 5). Stepsizes for the methods are chosen according to the theory and the batchsizes are ∼ m/100. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

We also tested MARINA and DIANA on mushrooms dataset with a bigger number of workers (n = 20). The results are reported in Figure 5.3. Similarly to the previous numerical tests, MARINA shows its superiority to DIANA with n = 20 as well.

196

f(xk)

mushrooms,n=20

10-1

10-2
0
10-1

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms,n=20
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

jjrf(xk)jj2

mushrooms,n=20

10-1

10-2 10-3
0
10-1 10-2

MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)
2500 5000 7500 10000 12500 15000 17500 20000
Communication Rounds
mushrooms,n=20
MARINA (K=1) MARINA (K=5) MARINA (K=10) DIANA (K=1) DIANA (K=5) DIANA (K=10)

jjrf(xk)jj2

f(xk)

10-2

10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

0.0 0.2 0.4 0.6 0.8 1.0 1.2

#bits/n

1e7

Figure 5.3: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (5.10) with mushrooms dataset and n = 20 workers. Stepsizes for the methods are chosen according to the theory. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

5.5.2 Image Classiﬁcation
We also compared the performance of VR-MARINA and VR-DIANA on the training ResNet-18 [75] at CIFAR100 [106] dataset. Formally, the optimization problem is

1N

min f (x) =

(p(f (ai, x)), yi) ,

x∈Rd

N i=1

(5.11)

where {(ai, yi)}Ni=1 encode images and labels from CIFAR100 dataset, f (ai, x) is the output of ResNet-18 on image ai with weights x, p is softmax function, and (·, ·) is cross-entropy loss. ResNet-18 has d = 11 689 512 parameters to train and CIFAR100 contains N = 50 000 colored images. The dataset is split into 5 parts among 5 workers in such a way that the ﬁrst four workers get 10 112 samples and the ﬁfth one get 9 552 samples. The code was written in Python 3.9 using PyTorch 1.7 and then was executed on a machine with NVIDIA GPU Geforce RTX 2080 Ti with 11 GByte onboard global GPU memory.
In all experiments, we use batchsize = 256 on each worker and tune the stepsizes for each method separately. That is, for each method and for each choice of K for RandK operator we run the method with stepsize γ ∈ {10−6, 0.1, 0.2, 0.5, 1.0, 5.0} to ﬁnd the interval containing the best stepsize. Next, the obtained interal is split into ∼ 10 equal parts and the method is run with corresponding stepsizes. Other parameters of the methods are chosen according to the theory. The summary of used parameters is given in Table 5.4.

197

Table 5.4: Summary of the parameters used in the experiments presented in Fig. 5.4 and Fig. 5.5. Stepsizes were tuned, batchsize = 256 on each worker, other parameters were picked according to the theory, except the last line, where p for VR-MARINA without compression was picked as for VR-MARINA with RandK, K = 100 000 compression operator.

Method

RandK, K =

γ

p

VR-MARINA

100 000

0.95 0.008554

VR-MARINA

500 000

0.95 0.024691

VR-MARINA

1 000 000

0.95 0.024691

VR-DIANA

100 000

0.15 0.025316

VR-DIANA

500 000

0.35 0.025316

VR-DIANA

1 000 000

0.35 0.025316

VR-MARINA 11 689 512 (K = d) 3.5 0.024691

VR-DIANA 11 689 512 (K = d) 2.5 0.025316

VR-MARINA 11 689 512 (K = d) 3.5 0.008554

The results are presented in Fig. 5.4. Again, VR-MARINA converges signiﬁcantly faster than VR-DIANA both in terms of the oracle complexity and the total number of transmitted bits to achieve the given accuracy.

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-2 10-3

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-2 10-3

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

jjrf(x)jj2

f(x)

jjrf(x)jj2

f(x)

5 × 100

10-4

5 × 100

10-4

0 500 1000 1500 2000 2500 3000 3500
Communication Rounds

10-5 0
7 × 100
6 × 100

500 1000 1500 2000 2500 3000 3500
Communication Rounds
Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-2 10-3

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4

#bits/n

1e11

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d)

10-5

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4

#bits/n

1e11

jjrf(x)jj2

f(x)

5 × 100

10-4

0

50 100 150 200 250

Epochs

10-5 0

50

100 150 200 250

Epochs

Figure 5.4: Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset. Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes are ∼ m/50. In all cases, we used the RandK sparsiﬁcation operator, the approximate values of K are given in the legends (d is dimension of the problem).

To emphasize the eﬀect of compression we also run VR-MARINA and VR-DIANA without compression, 198

see the results in Fig. 5.5. First of all, one con notice that the methods do beneﬁt from compression: VR-MARINA and VR-DIANA with compression converge much faster than their non-comressed versions in terms of the total number of transmitted bits to achieve given accuracy. Moreover, as Fig. 5.4 shows, VR-MARINA with K = 100 000 converges faster than VR-MARINA with larger K in terms of the epochs. That is, the method with more aggresive compression requires less oracle calls to achieve the same accuracy. The reason of such an unusual behavior is the choice of p: when K = 100 000 the theoretical choice of p is much smaller than for K = 500 000 and K = 1 000 000. Therefore, in VR-MARINA with K = 100 000, the workers compute the full gradients more rarely than in the case of larger K. As the result, it turns out, that the total number of oracle calls needed to achieve given accuracy also smaller for K = 100 000 than for larger K. Moreover, we see this phenomenon even without applying compression: VR-MARINA without compression and with p as in the experiment with VR-MARINA with K = 100 000 converges faster than VR-MARINA without compression and with theoretical choice of p, which is the same as in the case when K = 500 000, 1 000 000, see Table 5.4.
199

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

7 × 100 6 × 100

Training ResNet-18 @ CIFAR100
VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

f(x)

f(x)

5 × 100

5 × 100

0 7 × 100
6 × 100

500

1000

1500

2000

2500

3000

3500

Communication Rounds

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

0.0 10-2
10-3

10-4

0.2

0.4

0.6

0.8

1.0

#bits/n

Training ResNet-18 @ CIFAR100

1.2 1e12

jjrf(x)jj2

f(x)

5 × 100
0 10-2 10-3

50

100

150

200

250

Epochs

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

10-5 10-6 10-2 10-3

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

0

500

1000

1500

2000

2500

3000

3500

Communication Rounds

Training ResNet-18 @ CIFAR100

VR-MARINA (K ¼ 0.009d) VR-MARINA (K ¼ 0.043d) VR-MARINA (K ¼ 0.086d) VR-DIANA (K ¼ 0.009d) VR-DIANA (K ¼ 0.043d) VR-DIANA (K ¼ 0.086d) VR-MARINA (no compr.) VR-MARINA (no compr.p ¼ 0:009) VR-DIANA (no compr.)

10-4

10-4

jjrf(x)jj2

jjrf(x)jj2

10-5

10-5

10-6

10-6

0.0

0.2

0.4

0.6

0.8

1.0

1.2

#bits/n

1e12

0

50

100

150

200

250

Epochs

Figure 5.5: Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset. Number of workers equals 5. Stepsizes for the methods were tuned and the batchsizes are ∼ m/50. We used the RandK sparsiﬁcation operator, the approximate values of K are given in the legends (d is dimension of the problem). We also show the performance of VR-MARINA and VR-DIANA without compression.

200

Chapter 6
Moshpit SGD: Communication-Eﬃcient
Decentralized Training on Heterogeneous Unreliable
Devices
6.1 Introduction
Many1 recent inﬂuential discoveries in deep learning were enabled by the trend of scaling model and dataset size. Over the last decade, computer vision has grown from training models with 60 million parameters [107] on 1.3 million images [36] to 15 times more parameters [95] and 200 times more training data [212]. In natural language processing, the state-of-the-art language models [25] with 175 billion parameters are trained on over 570GB of texts, and even this does not saturate the model quality [84]. Training these large models can take years even with a top-of-the-line GPU server [113]. As a result, researchers and practitioners often have to run distributed training with multiple machines [132].
The dominant approach to distributed deep learning is data-parallel training [220], where each worker processes a fraction of the training batch and then exchanges its gradients with peers. If done naïvely, the gradient exchange can overload the network as the number of workers increases. To combat this issue, modern distributed training algorithms take advantage of communication-eﬃcient protocols, such as all-reduce [159]. These protocols allow workers to collectively compute the global average gradient with a constant communication overhead, regardless of the total number of peers. However, this eﬃciency makes the protocols more fragile: if any single participant fails or takes too long to process its batch, all other nodes will be stalled.
Therefore, scaling all-reduce protocols beyond a couple of servers requires specialized infrastructure with dedicated ultra-high bandwidth networking [132]. This kind of infrastructure is notoriously expensive compared to regular GPU servers or preemptible cloud VMs (see Appendix E.1). Hence, it is tempting to consider distributed training with cheap unreliable instances as a cost-eﬃcient alternative. A similar scenario arises in federated learning [134],
1We would like to thank Anastasia Koloskova, Liudmila Prokhorenkova and Anton Osokin for helpful feedback and discussions. Finally, we would like to thank Dmitry Afanasiev, Vladimir Aliev, Anand Jayarajan and Michael Solotky for their suggestions on the technical aspects of our study. The computational resources for the experiments were provided by the Amazon Research Awards program and Yandex.
201

where one must run distributed training with heterogeneous devices due to privacy concerns.
In both scenarios, participants use a shared network, where both latency and bandwidth can vary drastically due to interference from other users [163]. Furthermore, compute nodes are also subject to failure (or preemption) caused by factors beyond the protocol’s control.
Running large-scale distributed training in these circumstances requires fault- and latencytolerant algorithms [122, 13]. Most of these algorithms replace all-reduce averaging with gossip: each participant periodically downloads the latest parameters from his neighbors in a sparsely connected communication graph and averages the results. The updates gradually propagate through the graph over multiple rounds of averaging. However, the communication required to perform gossip grows linearly with the number of neighbors. Hence, when scaling to hundreds of peers, decentralized SGD has to keep the communication graph sparse, slowing down the convergence.
In this work, we propose an alternative approach. Instead of relying on a predeﬁned communication graph, participants dynamically organize themselves into groups using a fully decentralized matchmaking algorithm which we call Moshpit All-Reduce. This strategy allows us to use communication-eﬃcient all-reduce protocols that signiﬁcantly reduce the network load compared to gossip-based averaging, while still being able to operate in unreliable hardware and network conditions.
Our contributions can be summarized as follows: • We propose Moshpit All-Reduce — a novel decentralized averaging protocol for large-scale training with unreliable communication-constrained devices. According to our analysis, this method has exponential convergence independent of network topology.
• Armed with this averaging protocol, we develop Moshpit SGD for distributed optimization. We derive convergence rates for this algorithm and establish its equivalence to Centralized (Local) SGD in terms of iteration complexity under realistic assumptions.
• Our experiments demonstrate that Moshpit All-Reduce is signiﬁcantly more eﬃcient under network latency. In particular, we train ResNet-50 on ImageNet to 75% accuracy 1.3 times faster than existing decentralized training algorithms and train ALBERT-large from scratch 1.5 times faster on preemptible cloud VMs.
• We release the reference implementation of Moshpit SGD and the code for all experiments.2
6.2 Related Work
6.2.1 Data Parallel Training
The most popular way to accelerate neural network training with multiple devices is data-parallel training [220, 64, 236]. On each optimization step, this strategy splits the training batch among
2github.com/yandex-research/moshpit-sgd
202

participants. Each participant then runs forward and backward passes to obtain gradients of the objective function on their part of the training batch. After that, we can aggregate the gradients from workers and perform an optimization step. There are two main strategies for this aggregation.
Historically, the ﬁrst solution to gradient aggregation was to use Parameter Server (PS) [114]: a separate process or a dedicated server that keeps track of model parameters and optimizer statistics. After each round, the PS accumulates the gradients from each worker and updates the model parameters using SGD or any other optimizer, such as Adam [94]. Finally, the server distributes the updated model parameters to workers.
This strategy is robust and easy to implement, but it requires the server to regularly download full model gradients from every single worker. As a result, the parameter server can quickly become a bottleneck for large-scale training [6]. Since the original PS, researchers have proposed several modiﬁcations that reduce the communication load: accumulating multiple batches [247], compression [126, 98], server sharding [34, 81]. A more detailed overview is given in Appendix E.2.
In turn, many practical distributed training systems have instead switched to averaging with All-Reduce [64, 138, 203, 236]. This name refers to a collection of protocols originally developed for HPC applications. Workers can follow these protocols to collectively compute the average3 gradient more eﬃciently than with a central server.

6.2.2 Communication-Eﬃcient All-Reduce

There are several all-reduce protocols optimized for diﬀerent network topologies. The simplest one is known as Butterﬂy All-Reduce [159]. Each of n participants splits its local vector into n chunks. Then, i-th worker aggregates i-th chunk of data from all peers and sends back the averaged chunk.

device 1

split a 1

x1

xb 11

c 1

scatter

sum

Σ

a avg

all-gather

xa avg
b a1vg c avg

a 2

device 2 x2

xb 12

c 2

Σ

b avg

xa avg
b a1vg c avg

a 3

device 3 x3

xb 13

c 3

Σ

c avg

xa avg
b a1vg c avg

Figure 6.1: A schematic illustration of Butterﬂy All-Reduce.

As long as the vector size s is greater than n, this protocol uses O s × n−n 1 total bandwidth on each worker. However, it requires all-to-all communication, which is not always practical for the HPC infrastructure. Real-world systems typically use Ring or Tree All-Reduce, where each worker only communicates with a small subset of its peers.

These protocols enable highly eﬃcient and scalable averaging with O(1) or O(log n) total

3All-Reduce works with any commutative associative operation, such as min, max, or product.

203

communication per worker, but they also share a common drawback: they cannot tolerate node failures or network instability. If any single participant fails to execute its part or takes long to respond, this paralyzes all other workers.
6.2.3 Distributed Training in Unstable Conditions
Some distributed training applications must deal with unstable network bandwidth and/or unreliable workers. This issue is most prevalent in federated learning [134, 195, 22]. When dealing with privacy-sensitive data distributed across multiple actors, such as hospital servers [202, 116] or mobile phones [73, 235], one must train the model using whichever hardware and network available to those actors.
Another important motivational factor is cost: HPC-grade infrastructure can be prohibitively expensive, pushing researchers and practitioners towards commodity servers or preemptible cloud VMs that are signiﬁcantly cheaper (see Appendix E.1). Another solution is to use volunteer computing [93, 186] with abundant, but even less reliable, compute resources.
Training under these conditions requires specialized strategies. At a small scale, one can deploy one or a few reliable parameter servers to aggregate the updates from workers. This strategy can tolerate individual node failures [74], but scales poorly due to the reasons discussed in Section 6.2.1.
6.2.4 Decentralized Training
If there are too many participants for PS, it can be advantageous to use decentralized SGD via gossip-based averaging [24, 218, 122]. In this scenario, participants form a sparse graph: each worker periodically downloads parameters from its neighbors and mixes them with local parameters.
In essence, gossip-based averaging removes the communication bottlenecks of PS at the cost of using diﬀerent local parameters on each peer. That said, gossip-based optimization algorithms can match, and sometimes even outperform, their centralized counterparts in terms of training speed [191, 192, 190, 122, 13]. However, the convergence properties of gossip averaging and gossip-based optimization methods signiﬁcantly depend on the communication graph through the spectral properties of the mixing matrix [232, 190] or the Laplacian matrix of the network [136, 219].
Consequently, as the number of peers increases, gossip-based averaging has to either increase the number of neighbors (hence more communication) or accept slower convergence speed. Because of this, gossip is less communication-eﬃcient than all-reduce algorithms reviewed in Section 6.2.2. However, gossip-based algorithms are more robust to the changes, which makes them applicable to time-varying networks [145, 146, 147, 183] and federated learning [173, 234, 239].
204

6.3 Method Description

Large-scale training with unreliable participants requires a protocol that is both communicationeﬃcient and fault-tolerant. Unfortunately, existing methods have only provide one of these properties. To better address our conditions, we propose Moshpit All-Reduce — a fully decentralized averaging protocol that combines the eﬃciency of all-reduce and the fault tolerance of gossip-based averaging.
The rest of this section is organized as follows: • Section 6.3.1 describes the protocol and proves its correctness and communication eﬃciency;
• Section 6.3.2 provides the analysis of the proposed protocol and proves exponential convergence rate for averaging and linear convergence rate for optimization;
• Section 6.3.3 contains implementation details for training with heterogeneous compute nodes.
6.3.1 Moshpit Averaging
The core idea of Moshpit All-Reduce is that workers perform averaging in small independent groups. That way, a single failed participant would only aﬀect his current group. In turn, the composition of each group should be chosen dynamically to converge in the least number of steps. Ideally, if there are 16 peers with local parameters x, we can average them in 2 rounds, as demonstrated in Figure 6.2.

Algorithm 37 Moshpit All-Reduce (for i-th peer)

Figure 6.2: Example averaging order for 16 peers in 2 rounds. On each round, peers are split into 4 groups that run All-Reduce in parallel.

Input: parameters {xj}nj=1, number of

peers n, N , M , number of iterations T , peer

index i

x0i := xi Ci0 := get_initial_index(i)

for t ∈ 1 . . . T do

DHT

[

C

t−1 i

,

t].add

(address

i

)

/* wait for peers to assemble */

peerst := DHT.get([Cit−1, t])

xti, cti := AllReduce(xti−1, peerst)

Cit

:=

(C

t− i

1

[1:],

cti

)

//

same

as

eq.

(1)

end for

Return xTi

To achieve this in a decentralized system, we use Distributed Hash Tables (DHT) — a decentralized key-value storage; section E.2 contains a more detailed description. On each averaging round:
• Each worker computes his group key Ci;

• Workers add their network addresses to the DHT key corresponding to Ci;

205

• Each worker can now fetch a full list of peers that have the same Ci and run All-Reduce with those peers.
Unfortunately, the averaging structure from Figure 6.2 is impossible to maintain when participants are constantly joining, leaving, and failing. However, we can achieve equivalent results without global structure using a simple rule: if two peers were in the same group in round t, they must choose diﬀerent groups in round t+1.

A natural way to enforce this rule is to take advantage of the chunk indices from Butterﬂy All-Reduce (see Figure 6.1). Recall that each worker accumulates a unique chunk of parameters deﬁned by an index ci. By setting Ci := ci, we can guarantee that any workers that were in the same group at a round t will have diﬀerent group indices in round t+1.

This averaging scheme can be generalized to more than two dimensions in order to ﬁt a larger

number of peers or reduce the group size. For a N -dimensional hypercube, nodes should ﬁnd

groups of peers that they have not communicated with during N −1 previous rounds. To that

end, we deﬁne Ci as tuples containing chunk indices from N −1 previous rounds (t denotes the

communication round):

Cit := (cti−N+1, cti−N+2, . . . , cti).

(6.1)

The above intuition can be formalized with Algorithm 37. Here, n peers form a virtual N dimensional grid with M peers per row and average their parameters xi over T rounds. DHT[·] is a shortcut for using the DHT to add or retrieve values for a given key. In turn, AllReduce denotes running all-reduce to compute the average x in a given group of peers. The get_initial_index function takes the peer index i and returns N −1 integers in range [0, M ) such as the size of initial groups does not exceed M . That way, the groups formed on all subsequent rounds will also have at most M participants. One possible strategy is:

get_initial_index(i) =

i/M N−1 mod M
j∈{1, ..., N }

(6.2)

If n=M N and there are no node/network failures, Algorithm 37 is equivalent to Torus AllReduce [187], achieving the exact average after N rounds of communication (see Appendix E.3.1). However, our typical use case is far from this perfect scenario; for example, some groups can have less than M members. Furthermore, a peer might fail during all-reduce, causing its groupmates to skip a round of averaging. Still, Moshpit All-Reduce is applicable even in these conditions:
Theorem 6.3.1 (Correctness). If all workers have a non-zero probability of successfully running a communication round and the order of peerst is random, then all local vectors xti converge to the global average with probability 1:

∀i, xti − n1

2
x0i −−−→ 0.
2 t→∞

i

(6.3)

206

Proof (sketch, complete in Appendix E.3.2). Running all-reduce with a subset of peers preserves

the

invariant

1 n

i xti

=

1 n

i xit−1 and reduces the deviation of xti from the overall average.

Complexity. The matchmaking protocol is implemented over Kademlia DHT [133], meaning that each read and write operation needs at most O(log n) requests and O(M ) bandwidth to load peerst.

After the matchmaking is over, each group runs a single all-reduce round to compute the

average. In principle, Moshpit Averaging can use any general-purpose all-reduce protocol.

We opted for a butterﬂy-like version (Figure 6.1), as it is simpler than Ring All-Reduce

while still being communication-eﬃcient. The communication complexity of this algorithm

is O

max(s,

M)

×

M −1 M

, where s is the size of vector x. Thus, the total time complexity of

Algorithm 37 becomes:

M −1 O T × log2 n + M + max(s, M ) × M .

(6.4)

This compares favorably to gossip, where network load grows linearly with the number of neighbors.

6.3.2 Convergence Analysis

Mixing Properties of Moshpit Averaging

As stated in the previous section, Moshpit All-Reduce computes the exact average when n = M N , which cannot be guaranteed in practice. Therefore, additional analysis is needed to establish how quickly Moshpit Averaging approximates the actual average of n vectors stored on peers.
In the following theorem, we provide such analysis for a simpliﬁed version of Moshpit Averaging. One can ﬁnd the full proof in Appendix E.3.3.

Theorem 6.3.2. Consider a modiﬁcation of Moshpit All-Reduce that works as follows: at

each

iteration

k

≥

1,

1)

peers

are

randomly

split

in

r

disjoint

groups

of

sizes

M

k 1

,

.

.

.

,

M

k r

in such a way that

r i=1

Mik

=

n

and

Mik

≥

1

for

all

i

=

1, . . . , r

and

2)

peers

from

each

group compute their group average via All-Reduce. Let x1, . . . , xn be the input vectors of this

procedure

and

xT1

,

.

.

.

,

x

T n

be

the

outputs

after

T

iterations.

Also,

let

x

=

1 n

n i=1

xi

Then,

1n En

xTi − x 2 =

i=1

r−1 r n + n2

T1 n n i=1

xi − x 2.

(6.5)

In particular, this result implies that even if workers are randomly split into pairs at each iteration, the simpliﬁed version of Moshpit Averaging makes the average distortion (the left-hand side of Equation 6.5) less than ε in expectation after O (log(1/ε)) iterations. That is, this algorithm ﬁnds ε-accurate average on each node with the rate that does not depend on the spectral properties of the communication graph. Since Moshpit Averaging prevents two peers

207

Algorithm 38 Moshpit SGD

1: Input: starting point x0, learning rate γ > 0, communication period τ ≥ 1

2: for k = 0, 1, . . . do

3: for each peer i ∈ Pk+1 in parallel do

4:

Compute the stochastic gradient gik at the current point xki

5:

if k + 1 mod τ = 0 then

6: xki +1 = Moshpit All-Reducej∈Pk+1(xkj − γgjk) for i-th peer (Algorithm 37)

7:

else

8:

xki +1 = xki − γgik

9:

end if

10: end for

11: end for

from participating in the same groups during successive iterations, the actual algorithm should ﬁnd ε-accurate averages on participating peers even faster than Equation 6.5 predicts. Moreover, in Appendix E.3.3 we explain how this result can be generalized to the case when {Mik}ni=1 and r depends on k or even is random. In Appendix E.3.4, we also provide the guarantees measuring how fast Algorithm 37 reduces the variance when averaging random vectors.

Moshpit SGD

We consider a classical distributed optimization problem

1n

min f (x) =

fi(x) ,

x∈Rd

n i=1

(6.6)

where n is the number of workers and worker i has access only to the function fi.

We propose a new algorithm called Moshpit SGD to solve this problem (see Algorithm 38). In this algorithm, workers perform independent local SGD steps and periodically synchronize their parameters xki with other peers using Moshpit All-Reduce. Moreover, we deﬁne the indices of participating nodes at iteration k as Pk+1 (P0 = {1, . . . , n}) allowing peers to vanish.

First of all, we list the key assumptions that we use in the convergence analysis of Moshpit SGD.

Assumption 6.3.3 (Bounded variance). We assume that for all k ≥ 0 and i = 1, . . . , n stochastic gradients gik satisfy E gik | xki = ∇fi(xki ) and

E gik − ∇fi(xki ) 2 | xki ≤ σ2.

(6.7)

This assumption is classical in the stochastic optimization literature [149, 46]. We notice that our analysis can be generalized to the settings when the stochastic gradients satisfy less restrictive assumptions such as expected smoothness [63] or have more sophisticated structure similar to [86] using the theoretical framework from [56].

The following assumption controls the averaging properties and the eﬀect of the peers’ vanish-

208

ing.

Assumption 6.3.4 (Averaging quality & peers’ vanishing). We assume that the vanishing of peers does not change the global average of the iterates of Moshpit SGD too much, i.e., Pk+1 ⊆ Pk and |Pk| ≥ nmin for all k ≥ 0, |Paτ | ≤ 2|Pa(τ+1)| for all non-negative integers a ≥ 0, and there exist such x ∈ Rd and a sequence of non-negative numbers {∆kpv}k≥0 that ∀k ≥ 0

E

xk+1 − xk+1, xk+1 + xk+1 − 2x

≤

∆

k pv

,

f

convex;

E ∇f (xk), xk+1 − xk+1 + L xk+1 − xk+1 2 ≤ ∆kpv, f non-convex, L-smooth,

(6.8) (6.9)

where nk = |Pk|, xk+1 = nk1+1 i∈Pk+1 xki +1, and xk+1 = n1k i∈Pk (xki − γgik) for k ≥ 0. Moreover, we assume that for some δaq ≥ 0 and for all non-negative integers a ≥ 0





1 E  naτ

xai τ − xaτ

2


≤

γ 2 δa2q .

i∈Paτ

(6.10)

If Pk = Pk+1 = {1, . . . , n} for all k ≥ 0, i.e., peers do not vanish, then xk = xk and properties (6.8, 6.9) hold with ∆kpv ≡ 0 for all k ≥ 0. Moreover, according to the mixing properties of Moshpit Averaging established in Theorem 6.3.2, inequality 6.10 holds after O (log (1/γ2δa2q)) iterations of Algorithm 37. Therefore, the assumption above is natural and well-motivated.
Under these assumptions, we derive the convergence rates both for convex and non-convex problems. The full statements and complete proofs are deferred to Appendix E.4.

Theorem 6.3.5 (Convex case). Let f1 = . . . = fn = f , function f be µ-strongly convex (Def. A.1.2) and L-smooth (see Def. A.1.1), and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2 and x = x∗, where x∗ ∈ argminx∈Rd f (x) and δpv,1 ∈ [0, 1), δpv,2 ≥ 0. Then there exists a choice of γ such that E f (xK) − f (x∗) ≤ ε after K iterations of Moshpit SGD, where K equals



L

δp2v,2 +σ2/nmin

O

+

+

(1−δpv,1)µ (1 − δpv,1)µε

 L((τ −1)σ2 +δa2q)
(1−δpv,1)2µ2ε , µ > 0;

(6.11)

 LR02

R02(δp2v,2 +σ2/nmin)

R02 L((τ −1)σ2 +δa2q)

O ε +

ε2

+

ε3/2

, µ = 0,

(6.12)

K

where

xK

=

1 W

1 n

wkxki , wk = (1 − γµ)−(k+1), WK =

K k=0 k i∈Pk

O(·) hides constant and log(1/ε) factors.

K k=0

wk ,

R0

=

x0 − x∗

and

That is, if δpv,1 ≤ 1/2, nmin = Ω(n), δp2v,2 = O(σ2/nmin), and δa2q = O((τ − 1)σ), then Moshpit SGD has the same iteration complexity as Local-SGD in the homogeneous case [89, 229]. However, the averaging steps of Moshpit SGD are much faster than those of the parameter-server architecture when the number of peers is large. Also, unlike the state-of-the-art convergence guarantees

209

for Decentralized Local-SGD [97], our bounds do not depend on the spectral properties of the communication graph.

Theorem 6.3.6 (Non-convex case). Let f1 = . . . = fn = f , function f be L-smooth and
bounded from below by f∗, and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γE[ ∇f (xk) 2]+ Lγ2δp2v,2, δpv,1 ∈ [0, 1/2), δpv,2 ≥ 0. Then there exists such choice of γ that E ∇f (xKrand) 2 ≤ ε2 after K iterations of Moshpit SGD, where K equals

δ2 +σ2/n

√
(1−2δ

)(δ2 +(τ −1)σ2)

O

L∆0 (1−2δ )2ε2

1+τ

1−2δpv,1 + pv,2 ε2

min +

pv,1 aq
ε

,

pv,1

∆0 = f (x0) − f (x∗) and xKrand is chosen uniformly from {x0, x1, . . . , xK−1} deﬁned in As. 6.3.4.
Again, if δpv,1 ≤ 1/3, nmin = Ω(n), δp2v,2 = O(σ2/nmin), and δa2q = O((τ − 1)σ), then the above theorem recovers the state-of-the-art results in the non-convex case for Local-SGD [117, 97].

6.3.3 Implementation Details
Training on heterogeneous unreliable hardware also poses a number of engineering challenges. The most obvious one is that the system must be able to recover from node failures. To address these challenges, we use a fully decentralized infrastructure where all information is replicated. When a new worker joins midway through training, it can download the latest model parameters and metadata from any other peer (see section E.5). Another challenge arises when devices in a group have uneven network bandwidth. In that case, we dynamically adjust the communication load of each peer to avoid being bottlenecked. More information on this procedure can be found in section E.6.
6.4 Experiments
In this section, we ﬁrst check the theoretical properties of Moshpit All-Reduce in a controlled setup (Section 6.4.1). Then, we compare Moshpit SGD with other distributed methods on practical tasks of image classiﬁcation and masked language model pretraining (Sections 6.4.2 and 6.4.3).
6.4.1 Decentralized Averaging
We aim to verify the convergence and fault tolerance properties proven in Section 6.3.2. To achieve this, we initialize vectors of 512−1024 peers with standard Gaussian noise and run Moshpit Averaging for up to 18 steps. We report the average squared diﬀerence between the worker parameters and the true average parameters for a 32×32 grid with varying density and failure rate. We simulate failures by randomly shutting down peers with probability p. Failed peers return in the next round of averaging.
The results in Figure 6.3 outperform the theoretical estimate (Theorem 6.3.2) in all but one scenario: when n=1024, the algorithm ﬁnds the exact average (within 32-bit precision) in 2 steps.

210

Mean squared error

10 1

N=1024

N=900

10 5

N=512

10 9 NN==11002244,, pp==00..000014

10 13

N=900, p=0.001

0

5

10

15

20

25

Moshpit All-Reduce steps

Figure 6.3: Averaging error for Moshpit All-Reduce. We also veriﬁed that despite worker failures, the global average vector among all participants

remains constant throughout each run. We report additional grid conﬁgurations in Appendix E.8.

6.4.2 ImageNet Training
Here, we evaluate the performance of Moshpit SGD in distributed training. More speciﬁcally, we train ResNet-50 [75] on the ILSVRC [36] dataset, following the training protocol of [64]. Trainers use SGD with Nesterov momentum with a batch size of 256 and 32-bit precision regardless of the GPU type4. We evaluate the following training strategies:
• All-Reduce SGD (AR-SGD) — traditional distributed training with all-reduce gradient averaging;
• Asynchronous Decentralized Parallel SGD (AD-PSGD) — parallel SGD that runs gossip communication in a cycle: each worker averages parameters with 2 neighbors. Communication rounds are performed in background while the algorithm trains;
• Stochastic Gradient Push (SGP) — a more advanced algorithm with an exponential communication graph and push-based communication [13].
• Moshpit SGD — similar to SGP, but with 1 round of Moshpit Averaging instead of PushSum.
We report top-1 validation accuracy as a function of training time in two experimental setups:
• Homogeneous: 16 servers with a single Tesla V100-PCIe GPU, 6 CPU cores, and 64GB RAM.
• Heterogeneous: a total of 81 GPUs (V100, 1080Ti, and P40) across 64 servers and workstations.5
All servers and workstations communicate over the network with 1Gb/s Ethernet (non-dedicated symmetric bandwidth). The machines are located in two data centers and one oﬃce within 300 km of one another. The communication latency is 1–6ms depending on the location. To

4For GPUs that cannot ﬁt this into memory, we accumulate gradients over 2 batches of 128 examples. 5We provide a detailed conﬁguration in Appendix E.7.

211

Top-1 validation accuracy Top-1 validation accuracy
Training loss

75% 9.8 12.8 17.118.5 26.8 75%

50%

AR-SGD, homog.

AD-PSGD, homog.

SGP, homog.

25%

Moshpit SGD, homog.

AD-PSGD, heterog.

SGP, heterog.

0%

Moshpit SGD, heterog.

0h 4h 8h 12h 16h 20h 24h 28h 32h Time (hours)

50%

AR-SGD, homog.

AD-PSGD, homog.

SGP, homog.

25%

Moshpit SGD, homog.

AD-PSGD, heterog.

SGP, heterog.

0%

Moshpit SGD, heterog.

0 15 30 45 60 75 90 105 120 135 150 Epochs

AR-SGD, homog.

10

Moshpit SGD, heterog.

8

6

4

2 0h 30h 60h 90h 120h 150h 180h Time (hours)

Figure 6.4: (Left, Middle) ResNet-50 top-1 validation accuracy for ImageNet as a function of training time (left) and epochs (middle). (Right) Full training objective (MLM + SOP) of ALBERT-large on BookCorpus as a function of training time.

simulate shared usage, at the beginning of each communication round we inject additional latency sampled from the exponential distribution [211] with the mean of 100ms.
For Moshpit SGD, we use a two-dimensional “grid” with 4 and 8 groups for homogeneous and heterogeneous setups respectively. For AD-PSGD, we attempt to compensate for slow convergence by training for 60 more epochs without changing the learning rate schedule. Finally, we only report AR-SGD in the ﬁrst setup, as it is unsuitable for heterogeneous hardware.
The results in Figure 6.4 (Left) demonstrate that the two most eﬃcient strategies for our setting are Moshpit SGD and SGP. In the homogeneous setup, Moshpit is only slightly more eﬃcient than SGP, likely due to higher eﬃciency of all-reduce. This advantage increases to over 30% for the heterogeneous setup with 64 servers. In turn, AR-SGD demonstrates the best performance per iteration, but its training time is by far the longest due to network latency (1.5× of Moshpit SGD). Finally, AD-PSGD predictably shows the best throughput (time per epoch), but achieves lower accuracy even after training for 150 epochs. We report results for smaller setups in Appendix E.9.

6.4.3 Masked Language Model Training
Finally, we evaluate Moshpit All-Reduce training performance in the wild with preemptible cloud instances. For this experiment, we perform one of the most resource-demanding tasks in modern deep learning — unsupervised pretraining of Transformers [37, 128, 171, 25]. We opt for the ALBERT model [110] to make better use of communication-constrained devices. This model has fewer trainable parameters due to layer-wise weight sharing.
Speciﬁcally, we train ALBERT-large (18M parameters) on the BookCorpus [246] dataset, following the training setup from the original paper. We minimize the masked language modeling loss (MLM) along with the sentence order prediction loss (SOP) using the LAMB optimizer [236] with a global batch size of 4096 and sequence length 512. We measure convergence in terms of full training loss [124, 42]. Similarly to Section 6.4.2, we use two training setups:
• Homogeneous: a single cloud instance with 8 Tesla V100-PCIe GPUs and 56 vCPUs;
• Heterogeneous: a total of 66 preemptible GPUs, 32 of which are cloud T4, and the

212

remaining 34 are various devices rented on a public marketplace.
Despite the fact that the latter setup has almost 3× more raw compute6, its hourly rent costs less than the homogeneous setup due to relying on preemptible instances7. This instance type is much cheaper than regular cloud instances, but it can be interrupted at any time. As a sideeﬀect, the participants in heterogeneous setup are also spread across 3 continents with uneven network bandwidth, ranging from 100Mb/s to 1500Mb/s per worker. These limitations make it impractical to deploy conventional all-reduce protocols. By contrast, the fully decentralized nature of Moshpit SGD allows it to operate on unreliable nodes.
In this setup, the participants accumulate gradients over multiple local batches and use DHT to track the global batch size. Once the swarm collectively accumulates gradients over 4096 training samples, it runs 2 rounds of Moshpit All-Reduce with M =8 and N =2. Unfortunately, training with simple parameter averaging does not converge, likely due to diverging LAMB statistics. To mitigate this issue, workers recover “pseudo-gradients” [176, 28] after averaging to update the optimizer statistics.
Figure 6.4 (right) demonstrates that Moshpit SGD with a fully preemptible ﬂeet of machines trains 1.5 times faster than the traditional data-parallel setup. The ﬁnal loss achieved by two training strategies is the same within the margin of error. A closer investigation reveals that this speedup is entirely explained by the reduced iteration time. An interesting observation is that the iteration time of Moshpit SGD varies between 10–22 seconds, while AR-SGD consistently spends 25s per step. This can be explained by natural variation in the preemptible ﬂeet size: there were 30–66 active participants depending on resource availability.
6.5 Conclusion
In this work, we propose Moshpit All-Reduce — a decentralized averaging protocol intended for distributed optimization. It has favorable theoretical properties when compared to gossip-based approaches and achieves considerable distributed training speedups for image classiﬁcation and masked language modeling.
Our approach was primarily designed for cloud-based training and federated learning, as well as for distributed training on unreliable instances; future work might explore additional settings, such as collaborative training of neural networks. Another perspective research direction is to study the combination of the proposed protocol with other techniques that aim for communication eﬃciency in distributed optimization, such as gradient compression.

6Based on oﬃcial performance benchmarks [158]. 7Please refer to Appendix E.7 for full experimental setups.

213

References
[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems, pages 873–881, 2011.
[2] Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021, 2017.
[3] David Aldous and James Allen Fill. Reversible markov chains and random walks on graphs, 2002. unﬁnished monograph, recompiled 2014, 2002.
[4] Dan Alistarh, Demjan Grubic, Jerry Z Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: communication-eﬃcient sgd via gradient quantization and encoding. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1707–1718, 2017.
[5] Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information Processing Systems, pages 5973–5983, 2018.
[6] Salem Alqahtani and Murat Demirbas. Performance analysis and comparison of distributed machine learning systems. 07 2019.
[7] Erling D. Andersen and Knud D. Andersen. The mosek interior point optimizer for linear programming: An implementation of the homogeneous algorithm. In Applied Optimization, pages 197–232. Springer US, 2000.
[8] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
[9] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. Advances in neural information processing systems, 28:1756–1764, 2015.
[10] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. arXiv preprint arXiv:1806.10188, 2018.
[11] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. In Algorithmic Learning Theory, pages 111–132. PMLR, 2020.
[12] Mahmoud Assran, Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson, and Michael G Rabbat. Advances in asynchronous parallel and distributed optimization. Proceedings of the IEEE, 108(11):2013–2031, 2020.
[13] Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for distributed deep learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
214

editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 344–353. PMLR, 09–15 Jun 2019.
[14] Hari Balakrishnan, M Frans Kaashoek, David Karger, Robert Morris, and Ion Stoica. Looking up data in p2p systems. Communications of the ACM, 46(2):43–48, 2003.
[15] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with quantization, sparsiﬁcation and local computations. In Advances in Neural Information Processing Systems, pages 14668–14679, 2019.
[16] El Houcine Bergou, Eduard Gorbunov, and Peter Richtárik. Stochastic three points method for unconstrained smooth minimization. SIAM Journal on Optimization, 30(4):2726–2749, 2020.
[17] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD: Compressed optimisation for non-convex problems. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 560–569, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.
[18] Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. SignSGD with majority vote is communication eﬃcient and fault tolerant. In ICLR, 2019.
[19] Dimitri P Bertsekas and John N Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliﬀs, NJ, 1989.
[20] Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
[21] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster semi-deﬁnite optimization. In Conference on Learning Theory, pages 530–582. PMLR, 2016.
[22] K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé M Kiddon, Jakub Konečný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In SysML 2019, 2019. To appear.
[23] Léon Bottou. Stochastic learning. In Summer School on Machine Learning, pages 146–168. Springer, 2003.
[24] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE transactions on information theory, 52(6):2508–2530, 2006.
[25] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
215

[26] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for ﬁnding stationary points i. Mathematical Programming, pages 1–50, 2019.
[27] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
[28] Xiangyi Chen, Xiaoyun Li, and Ping Li. Toward communication eﬃcient adaptive gradient method. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference, FODS ’20, page 119–128, New York, NY, USA, 2020. Association for Computing Machinery.
[29] Dominik Csiba and Peter Richtárik. Coordinate descent face-oﬀ: primal or dual? In JMLR Workshop and Conference Proceedings, The 29th International Conference on Algorithmic Learning Theory, 2018.
[30] Ashok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization with heavy tails. arXiv preprint arXiv:2106.14343, 2021.
[31] Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov, Sergey Guminov, Dmitry Kamzolov, and Innokentiy Shibaev. Recent theoretical advances in non-convex optimization. arXiv preprint arXiv:2012.06188, 2020.
[32] Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S Dhillon. Improved convergence rates for non-convex federated learning with compression. arXiv preprint arXiv:2012.04061, 2020.
[33] Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang. From low probability to high conﬁdence in stochastic convex optimization. Journal of Machine Learning Research, 22(49):1–38, 2021.
[34] Jeﬀrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc Le, and Andrew Ng. Large scale distributed deep networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25, pages 1223–1231. Curran Associates, Inc., 2012.
[35] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646–1654, 2014.
[36] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
[37] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.
[38] Pavel Dvurechensky, Eduard Gorbunov, and Alexander Gasnikov. An accelerated directional derivative method for smooth stochastic convex optimization. European Journal of Operational Research, 290(2):601–621, 2021.
216

[39] Alireza Fallah, Mert Gurbuzbalaban, Asu Ozdaglar, Umut Simsekli, and Lingjiong Zhu. Robust distributed accelerated stochastic gradient methods for multi-agent networks. arXiv preprint arXiv:1910.08701, 2019.
[40] C Fang, CJ Li, Z Lin, and T Zhang. Near-optimal non-convex optimization via stochastic path integrated diﬀerential estimator. Advances in Neural Information Processing Systems, 31:689, 2018.
[41] Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter Richtárik. Ef21 with bells & whistles: Practical algorithmic extensions of modern error feedback. arXiv preprint arXiv:2110.03294, 2021.
[42] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity, 2021.
[43] Hamid Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson. An asynchronous minibatch algorithm for regularized stochastic optimization. IEEE Transactions on Automatic Control, 61(12):3740–3754, 2016.
[44] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469–1492, 2012.
[45] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 23(4):2061–2089, 2013.
[46] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.
[47] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.
[48] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. arXiv preprint arXiv:2111.08611, 2021.
[49] Eduard Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou, and Peter Richtarik. A stochastic derivative free optimization method with momentum. In International Conference on Learning Representations, 2020.
[50] Eduard Gorbunov, Alexander Borzunov, Michael Diskin, and Max Ryabinin. Secure distributed training at scale. arXiv preprint arXiv:2106.11257, 2021.
[51] Eduard Gorbunov, Konstantin P. Burlachenko, Zhize Li, and Peter Richtarik. Marina: Faster non-convex distributed learning with compression. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,
217

volume 139 of Proceedings of Machine Learning Research, pages 3788–3798. PMLR, 18–24 Jul 2021.
[52] Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15042–15053. Curran Associates, Inc., 2020.
[53] Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and Alexander Gasnikov. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise. arXiv preprint arXiv:2106.05958, 2021.
[54] Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for derivative-free smooth stochastic convex optimization. arXiv preprint arXiv:1802.09022, 2018.
[55] Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. A uniﬁed theory of sgd: Variance reduction, sampling, quantization and coordinate descent. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 680–690. PMLR, 26–28 Aug 2020.
[56] Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. Local sgd: Uniﬁed theory and new eﬃcient methods. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3556–3564. PMLR, 13–15 Apr 2021.
[57] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtarik. Linearly converging error compensated sgd. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20889–20900. Curran Associates, Inc., 2020.
[58] Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: O(1/k) lastiterate convergence for monotone variational inequalities and connections with cocoercivity. arXiv preprint arXiv:2110.04261, 2021.
[59] Eduard Gorbunov, Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, and Alexander Gasnikov. Recent theoretical advances in decentralized distributed convex optimization. arXiv preprint arXiv:2011.13259, 2020.
[60] Robert M Gower and Peter Richtárik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690, 2015.
[61] Robert M Gower and Peter Richtárik. Stochastic dual ascent for solving linear systems. arXiv:1512.06890, 2015.
218

[62] Robert M Gower, Peter Richtárik, and Francis Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018.
[63] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Sgd: General analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209. PMLR, 2019.
[64] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017.
[65] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, pages 1737–1746. JMLR.org, 2015.
[66] Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In International Conference on Machine Learning, pages 3964–3975. PMLR, 2021.
[67] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Uniﬁed analysis and sharp guarantees. arXiv preprint arXiv:2007.01154, 2020.
[68] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019.
[69] Filip Hanzely, Konstantin Mishchenko, and Peter Richtárik. SEGA: Variance reduction via gradient sketching. In Advances in Neural Information Processing Systems, pages 2082–2093, 2018.
[70] Filip Hanzely and Peter Richtárik. Accelerated coordinate descent with arbitrary sampling and best rates for minibatches. In Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages 304–312. PMLR, 16–18 Apr 2019.
[71] Filip Hanzely and Peter Richtárik. One method to rule them all: variance reduction for data, parameters and many new methods. arXiv preprint arXiv:1905.11266, 2019.
[72] Filip Hanzely and Peter Richtárik. Federated learning of a mixture of global and local models. arXiv preprint arXiv:2002.05516, 2020.
[73] Andrew Hard, Chloé M Kiddon, Daniel Ramage, Francoise Beaufays, Hubert Eichner, Kanishka Rao, Rajiv Mathews, and Sean Augenstein. Federated learning for mobile keyboard prediction, 2018.
[74] Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R. Ganger, and Phillip B. Gibbons. Proteus: Agile ml elasticity through tiered reliability in dynamic resource markets. In Proceedings of the Twelfth European Conference on Computer Systems, EuroSys ’17, page 589–604, New York, NY, USA, 2017. Association for Computing Machinery.
219

[75] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[76] Liam Hodgkinson and Michael Mahoney. Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning, pages 4262–4274. PMLR, 2021.
[77] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.
[78] Samuel Horváth, Chen-Yu Ho, Ľudovít Horváth, Atal Narayan Sahu, Marco Canini, and Peter Richtárik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019.
[79] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
[80] Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and Gennady Pekhimenko. Priority-based parameter propagation for distributed dnn training. In A. Talwalkar, V. Smith, and M. Zaharia, editors, Proceedings of Machine Learning and Systems, volume 1, pages 132–145, 2019.
[81] Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo. A uniﬁed architecture for accelerating distributed DNN training in heterogeneous gpu/cpu clusters. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 463–479. USENIX Association, November 2020.
[82] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315–323, 2013.
[83] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
[84] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
[85] Seymour Kaplan. Application of programs with maximin objective functions to problems of optimal resource allocation. Operations Research, 22(4):802–807, 1974.
[86] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich,
220

and Ananda Theertha Suresh. Scaﬀold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132–5143. PMLR, 2020.
[87] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Scaﬀold: Stochastic controlled averaging for federated learning. arXiv preprint arXiv:1910.06378, 2019.
[88] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback ﬁxes signsgd and other gradient compression schemes. In International Conference on Machine Learning, pages 3252–3261. PMLR, 2019.
[89] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4519–4529. PMLR, 2020.
[90] Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M Gower, and Peter Richtárik. Uniﬁed analysis of stochastic gradient methods for composite convex and smooth optimization. arXiv preprint arXiv:2006.11573, 2020.
[91] Prashant Khanduri, Pranay Sharma, Swatantra Kaﬂe, Saikiran Bulusu, Ketan Rajawat, and Pramod K Varshney. Distributed stochastic non-convex optimization: Momentumbased variance reduction. arXiv preprint arXiv:2005.00224, 2020.
[92] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
[93] Ekasit Kijsipongse, Apivadee Piyatumrong, and Suriya U-ruekolan. A hybrid gpu cluster and volunteer computing platform for scalable deep learning. The Journal of Supercomputing, 04 2018.
[94] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.
[95] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020.
[96] Anastasia Koloskova, Tao Lin, Sebastian U. Stich, and Martin Jaggi. Decentralized deep learning with arbitrary communication compression. ICLR, page arXiv:1907.09356, 2020.
[97] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A uniﬁed theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pages 5381–5393. PMLR, 2020.
[98] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and gossip algorithms with compressed communication. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Ma-
221

chine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3478–3487. PMLR, 09–15 Jun 2019.
[99] Jakub Konečný, Jie Lu, Peter Richtárik, and Martin Takáč. Mini-batch semi-stochastic gradient descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):242–255, 2016.
[100] Jakub Konečný, H. Brendan McMahan, Felix Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication eﬃciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.
[101] Jakub Konečný and Peter Richtárik. Randomized distributed mean estimation: accuracy vs communication. Frontiers in Applied Mathematics and Statistics, 4(62):1–11, 2018.
[102] Dmitry Kovalev, Eduard Gorbunov, Elnur Gasanov, and Peter Richtárik. Stochastic spectral and conjugate descent methods. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 3362–3371, 2018.
[103] Dmitry Kovalev, Samuel Horváth, and Peter Richtárik. Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop. In Proceedings of the 31st International Conference on Algorithmic Learning Theory, 2020.
[104] Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly convergent algorithm for decentralized optimization: Sending less bits for free! In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 4087–4095. PMLR, 13–15 Apr 2021.
[105] Dmitry Kovalev, Adil Salim, and Peter Richtárik. Optimal and practical algorithms for smooth and strongly convex decentralized optimization. Advances in Neural Information Processing Systems, 33, 2020.
[106] Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[107] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.
[108] Andrei Kulunchakov and Julien Mairal. Estimate sequences for variance-reduced stochastic composite optimization. arXiv preprint arXiv:1905.02374, 2019.
[109] Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer Nature, 2020.
[110] Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
222

Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.
[111] Rémi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Asaga: asynchronous parallel saga. In Artiﬁcial Intelligence and Statistics, pages 46–54. PMLR, 2017.
[112] Rémi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Improved asynchronous parallel optimization analysis for stochastic incremental methods. The Journal of Machine Learning Research, 19(1):3140–3207, 2018.
[113] Chuan Li. Demystifying gpt-3 language model: A technical overview, 2020. "https: //lambdalabs.com/blog/demystifying-gpt-3".
[114] Mu Li. Scaling distributed machine learning with the parameter server. In Proceedings of the 2014 International Conference on Big Data Science and Computing, BigDataScience ’14, New York, NY, USA, 2014. Association for Computing Machinery.
[115] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
[116] Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M. Jorge Cardoso, and Andrew Feng. Privacy-Preserving Federated Brain Tumour Segmentation, pages 133–141. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics). SPRINGER, January 2019. 10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019 ; Conference date: 13-10-2019 Through 13-10-2019.
[117] Xiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Communication eﬃcient decentralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 5, 2019.
[118] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. arXiv preprint arXiv:2008.10898, 2020.
[119] Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik. Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning, pages 5895–5904. PMLR, 2020.
[120] Zhize Li and Peter Richtárik. A uniﬁed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020.
[121] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic
223

gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737–2745, 2015.
[122] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5330–5340, 2017.
[123] Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance reduced local SGD with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019.
[124] Jiahuang Lin, Xin Li, and Gennady Pekhimenko. Multi-node bert-pretraining: Costeﬃcient approach, 2020.
[125] Tao Lin, Sebastian Urban Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches, use local SGD. ICLR, page arXiv:1808.07217, 2020.
[126] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In ICLR 2018 International Conference on Learning Representations, 2018.
[127] Xiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan. A double residual compression algorithm for eﬃcient distributed learning. arXiv preprint arXiv:1910.07561, 2019.
[128] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.
[129] Stanislaw Łojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les équations aux dérivées partielles, 117:87–89, 1963.
[130] Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion. In International Conference on Machine Learning, pages 3345–3354. PMLR, 2018.
[131] Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM Journal on Optimization, 27(4):2202–2229, 2017.
[132] Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim Hazelwood, Andrew Hock, Xinyuan Huang, Bill Jia, Daniel Kang, David Kanter, Naveen Kumar, Jeﬀery Liao, Guokai Ma, Deepak Narayanan, Tayo Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St. John, Carole-Jean Wu, Lingjie Xu, Cliﬀ Young, and Matei Zaharia.
224

MLPerf Training Benchmark. In Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys’20), 2020.
[133] Petar Maymounkov and David Mazieres. Kademlia: A peer-to-peer information system based on the xor metric. In International Workshop on Peer-to-Peer Systems, pages 53–65. Springer, 2002.
[134] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. Communication-eﬃcient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017.
[135] H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep networks using model averaging. arXiv preprint arXiv:1602.05629, 2016.
[136] Russell Merris. Laplacian matrices of graphs: a survey. Linear algebra and its applications, 197:143–176, 1994.
[137] Panayotis Mertikopoulos, Nadav Hallak, Ali Kavis, and Volkan Cevher. On the almost sure convergence of stochastic gradient descent in non-convex problems. Advances in Neural Information Processing Systems, 33, 2020.
[138] Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki Tanaka, and Yuichi Kageyama. Massively distributed sgd: Imagenet/resnet-50 training in a ﬂash, 2019.
[139] Konstantin Mishchenko, Eduard Gorbunov, Martin Takáč, and Peter Richtárik. Distributed learning with compressed gradient diﬀerences. arXiv preprint arXiv:1901.09269, 2019.
[140] Konstantin Mishchenko, Filip Hanzely, and Peter Richtárik. 99% of worker-master communication in distributed optimization is not needed. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 979–988. PMLR, 2020.
[141] Konstantin Mishchenko, Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini. A delaytolerant proximal-gradient algorithm for distributed learning. In International Conference on Machine Learning, pages 3587–3595. PMLR, 2018.
[142] Konstantin Mishchenko and Peter Richtárik. A stochastic decoupling method for minimizing the sum of smooth and non-smooth functions. arXiv preprint arXiv:1905.11535, 2019.
[143] KG Murty and SN Kabadi. Some np-complete problems in quadratic and nonlinear programming. Mathematical programming, 39(2):117–129, 1987.
[144] Aleksandr Viktorovich Nazin, AS Nemirovsky, Aleksandr Borisovich Tsybakov, and AB Juditsky. Algorithms of robust stochastic optimization based on mirror descent method. Automation and Remote Control, 80(9):1607–1627, 2019.
[145] Angelia Nedić and Alex Olshevsky. Distributed optimization over time-varying directed graphs. IEEE Transactions on Automatic Control, 60(3):601–615, 2014.
225

[146] Angelia Nedić and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-varying directed graphs. IEEE Transactions on Automatic Control, 61(12):3936– 3947, 2016.
[147] Angelia Nedić, Alex Olshevsky, and Michael G Rabbat. Network topology and communication-computation tradeoﬀs in decentralized optimization. Proceedings of the IEEE, 106(5):953–976, 2018.
[148] Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. Mathematical Programming, 155(1– 2):549–573, 2015.
[149] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[150] Arkadij Semenovič Nemirovskij and David Borisovich Yudin. Problem complexity and method eﬃciency in optimization. 1983.
[151] Yurii Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.
[152] Yurii Nesterov. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–566, 2017.
[153] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[154] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527–566, April 2017. First appeared in 2011 as CORE discussion paper 2011/16.
[155] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate O(1/k2). In Dokl. Akad. Nauk SSSR, volume 269, pages 543–547, 1983.
[156] Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richtárik, Katya Scheinberg, and Martin Takáč. SGD and Hogwild! convergence without the bounded gradients assumption. In International Conference on Machine Learning, pages 3750–3758, 2018.
[157] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2613–2621. JMLR. org, 2017.
[158] NVIDIA. Nvidia data center deep learning product performance. "https:// developer.nvidia.com/deep-learning-performance-training-inference", accessed at 2021.02.03.
226

[159] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J. Parallel Distrib. Comput., 69(2):117–124, February 2009.
[160] Vivak Patel. Stopping criteria for, and strong convergence of, stochastic gradient descent on bottou-curtis-nocedal functions. arXiv preprint arXiv:2004.00475, 2020.
[161] Reese Pathak and Martin J Wainwright. FedSplit: An algorithmic framework for fast federated optimization. arXiv preprint arXiv:2005.05238, 2020.
[162] Zhimin Peng, Yangyang Xu, Ming Yan, and Wotao Yin. Arock: an algorithmic framework for asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 38(5):A2851–A2879, 2016.
[163] V. Persico, P. Marchetta, A. Botta, and A. Pescape. On network throughput variability in microsoft azure cloud. In 2015 IEEE Global Communications Conference (GLOBECOM), pages 1–6, 2015.
[164] Valerio Persico, Pietro Marchetta, Alessio Botta, and Antonio Pescapè. Measuring network throughput in the cloud: The case of amazon ec2. Computer Networks, 93:408 – 422, 2015. Cloud Networking and Communications II.
[165] Constantin Philippenko and Aymeric Dieuleveut. Artemis: tight convergence guarantees for bidirectional compression in federated learning. arXiv preprint arXiv:2006.14591, 2020.
[166] Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864–878, 1963.
[167] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838–855, 1992.
[168] Xun Qian, Peter Richtárik, and Tong Zhang. Error compensated distributed sgd can be accelerated. arXiv preprint arXiv:2010.00091, 2020.
[169] Zheng Qu and Peter Richtárik. Coordinate descent with arbitrary sampling I: Algorithms and complexity. Optimization Methods and Software, 31(5):829–857, 2016.
[170] Zheng Qu, Peter Richtárik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with arbitrary sampling. In Advances in Neural Information Processing Systems 28, pages 865–873, 2015.
[171] Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[172] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016.
[173] S Sundhar Ram, A Nedić, and Venugopal V Veeravalli. Asynchronous gossip algorithms for stochastic optimization. In Proceedings of the 48h IEEE Conference on Decision and
227

Control (CDC) held jointly with 2009 28th Chinese Control Conference, pages 3581–3586. IEEE, 2009.
[174] Ali Ramezani-Kebrya, Fartash Faghri, and Daniel M Roy. NUQSGD: Improved communication eﬃciency for data-parallel SGD via nonuniform quantization. arXiv preprint arXiv:1908.06077, 2019.
[175] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693–701, 2011.
[176] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021.
[177] Sashank J Reddi, Jakub Konečný, Peter Richtárik, Barnabás Póczós, and Alex Smola. AIDE: Fast and communication eﬃcient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.
[178] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-eﬃcient federated learning method with periodic averaging and quantization. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2021–2031, 2020.
[179] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, and Ramtin Pedarsani. An exact quantized decentralized gradient descent algorithm. IEEE Transactions on Signal Processing, 67(19):4934–4947, 2019.
[180] Peter Richtárik and Martin Takáč. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, 10(6):1233–1243, 2016.
[181] Peter Richtárik and Martin Takác. Stochastic reformulations of linear systems: algorithms and convergence theory. SIAM Journal on Matrix Analysis and Applications, 41(2):487–524, 2020.
[182] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400–407, 1951.
[183] Alexander Rogozin and Alexander Gasnikov. Projected gradient method for decentralized optimization over time-varying networks. arXiv preprint arXiv:1911.08527, 2019.
[184] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems, pages 2663–2671, 2012.
[185] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-eﬃcient decentralized training on heterogeneous unreliable devices. arXiv preprint arXiv:2103.03239, 2021.
228

[186] Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks using decentralized mixture-of-experts. In Advances in Neural Information Processing Systems, 2020.
[187] Paul Sack and William Gropp. Collective algorithms for multiported torus networks. ACM Trans. Parallel Comput., 1(2), February 2015.
[188] Mher Safaryan, Egor Shulgin, and Peter Richtárik. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. arXiv preprint arXiv:2002.08958, 2020.
[189] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, and Peter Richtárik. Scaling distributed machine learning with in-network aggregation. arXiv preprint ArXiv:1903.06701, 2019.
[190] Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Lee, and Laurent Massoulié. Optimal convergence rates for convex distributed optimization in networks. Journal of Machine Learning Research, 20:1–31, 2019.
[191] Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Tat Lee, and Laurent Massoulié. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In International Conference on Machine Learning, pages 3027–3036, 2017.
[192] Kevin Scaman, Francis Bach, Sébastien Bubeck, Laurent Massoulié, and Yin Tat Lee. Optimal algorithms for non-smooth distributed optimization in networks. In Advances in Neural Information Processing Systems, pages 2740–2749, 2018.
[193] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017.
[194] Othmane Sebbouh, Robert M Gower, and Aaron Defazio. Almost sure convergence rates for stochastic gradient descent and stochastic heavy ball. In Conference on Learning Theory, pages 3935–3971. PMLR, 2021.
[195] Aaron Segal, Antonio Marcedone, Benjamin Kreuter, Daniel Ramage, H. Brendan McMahan, Karn Seth, K. A. Bonawitz, Sarvar Patel, and Vladimir Ivanov. Practical secure aggregation for privacy-preserving machine learning. In CCS, 2017.
[196] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
[197] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
[198] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. Journal of Machine Learning Research, 14(1):567–599, 2013.
229

[199] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-eﬃcient distributed optimization using an approximate Newton-type method. In Proceedings of the 31st International Conference on Machine Learning, PMLR, volume 32, pages 1000–1008, 2014.
[200] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczyński. Lectures on stochastic programming: modeling and theory. SIAM, 2014.
[201] Pranay Sharma, Swatantra Kaﬂe, Prashant Khanduri, Saikiran Bulusu, Ketan Rajawat, and Pramod K Varshney. Parallel restarted spider–communication eﬃcient distributed nonconvex optimization with optimal computation complexity. arXiv preprint arXiv:1912.06036, 2019.
[202] Micah J. Sheller, Brandon Edwards, G. Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou, Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka R. Colen, and Spyridon Bakas. Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. Scientiﬁc Reports, 10(1):12598, Jul 2020.
[203] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.
[204] Vladimir Spokoiny et al. Parametric estimation. ﬁnite sample theory. The Annals of Statistics, 40(6):2877–2909, 2012.
[205] Sebastian U Stich. Local SGD converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018.
[206] Sebastian U Stich. Uniﬁed optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019.
[207] Sebastian U Stich. On communication compression for distributed optimization on heterogeneous data. arXiv preprint arXiv:2009.02388, 2020.
[208] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed sgd with memory. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 4452–4463, 2018.
[209] Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:1–36, 2020.
[210] Sebastian Urban Stich. Local SGD converges fast and communicates little. International Conference on Learning Representations (ICLR), page arXiv:1805.09767, 2019.
[211] Andrei M Sukhov, MA Astrakhantseva, AK Pervitsky, SS Boldyrev, and AA Bukatov. Generating a function for network delay. Journal of High Speed Networks, 22(4):321–333, 2016.
230

[212] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable eﬀectiveness of data in deep learning era. In ICCV, 2017.
[213] Haoran Sun, Songtao Lu, and Mingyi Hong. Improving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking. In International Conference on Machine Learning, pages 9217–9228. PMLR, 2020.
[214] Ruoyu Sun. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.
[215] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329–3337. PMLR, 2017.
[216] Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In International Conference on Machine Learning, pages 6155–6165, 2019.
[217] Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920, 2019.
[218] John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.
[219] César A Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedić. A dual approach for optimal algorithms in distributed optimization over networks. Optimization Methods and Software, pages 1–40, 2020.
[220] Leslie G Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1990.
[221] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron. In 22nd International Conference on Artiﬁcial Intelligence and Statistics, volume 89 of PMLR, pages 1195–1204, 2019.
[222] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan S. Rellermeyer. A survey on distributed machine learning. ACM Comput. Surv., 53(2), March 2020.
[223] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient compression for distributed optimization. In Advances in Neural Information Processing Systems, pages 14259–14268, 2019.
231

[224] Hongjian Wang, Mert Gürbüzbalaban, Lingjiong Zhu, Umut Şimşekli, and Murat A Erdogdu. Convergence rates of stochastic gradient descent under inﬁnite noise variance. arXiv preprint arXiv:2102.10346, 2021.
[225] Hongyi Wang, Scott Sievert, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. Atomo: Communication-eﬃcient learning via atomic sparsiﬁcation. arXiv preprint arXiv:1806.04090, 2018.
[226] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiﬁcation for communication-eﬃcient distributed optimization. In Advances in Neural Information Processing Systems, pages 1299–1309, 2018.
[227] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: ternary gradients to reduce communication in distributed deep learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1508–1518, 2017.
[228] Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous distributed learning. arXiv preprint arXiv:2006.04735, 2020.
[229] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334–10343. PMLR, 2020.
[230] Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized SGD and its applications to large-scale distributed optimization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5325–5333, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.
[231] Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variancereduced stochastic gradient descent with robustness to byzantine attacks. arXiv preprint arXiv:1912.12716, 2019.
[232] Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):65–78, 2004.
[233] Jinming Xu, Ye Tian, Ying Sun, and Gesualdo Scutari. Distributed algorithms for composite optimization: Uniﬁed and tight convergence analysis. arXiv preprint arXiv:2002.11534, 2020.
[234] Feng Yan, Shreyas Sundaram, SVN Vishwanathan, and Yuan Qi. Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties. IEEE Transactions on Knowledge and Data Engineering, 25(11):2483–2493, 2012.
[235] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong,
232

Daniel Ramage, and Françoise Beaufays. Applied federated learning: Improving google keyboard query suggestions, 2018.
[236] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020.
[237] Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural Information Processing Systems, 33, 2020.
[238] Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. arXiv preprint arXiv:2011.08474, 2020.
[239] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835–1854, 2016.
[240] Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 4035–4043, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
[241] Yuchen Zhang and Xiao Lin. DiSCO: Distributed optimization for self-concordant empirical loss. In International Conference on Machine Learning, pages 362–370, 2015.
[242] Lei Zhao, Musa Mammadov, and John Yearwood. From convex to nonconvex: a loss function analysis for binary classiﬁcation. In 2010 IEEE International Conference on Data Mining Workshops, pages 1281–1288. IEEE, 2010.
[243] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning, PMLR, volume 37, pages 1–9, 2015.
[244] Shen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016.
[245] Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. Advances in Neural Information Processing Systems, 30:7040–7049, 2017.
[246] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.
233

[247] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. In J. Laﬀerty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23, pages 2595–2603. Curran Associates, Inc., 2010.
234

Appendix A
Basic Facts, Technical Lemmas, and Auxiliary Results

A.1 Standard Deﬁnitions from Optimization Theory
In this section, we provide the most frequently used deﬁnitions and simple facts from optimization theory. The proofs of the facts mentioned below are given in [153].
Notation. We use the following notation. x, y d=ef i xiyi is the standard Euclidean inner product, and x d=ef x, x 1/2 is the induced 2 norm. For simplicity we assume that (2.1) has a unique minimizer, which we denote x∗. Let Df (x, y) denote the Bregman divergence associated with f : Df (x, y) d=ef f (x) − f (y) − ∇f (y), x − y . We often write [n] d=ef {1, 2, . . . , n}.
Deﬁnition A.1.1 (L-smoothness). A function f : Rn → R is called L-smooth if for all x, y ∈ Rn, the following inequality holds:

∇f (x) − ∇f (y) ≤ L x − y . If the function f is L-smooth, then for all x, y ∈ Rn
f (y) ≤ f (x) + ∇f (x), y − x + L y − x 2. 2
Next, if f is additionally lower bounded by f∗, then for all x ∈ Rd ∇f (x) 2 ≤ 2L (f (x) − f∗) .

(A.1) (A.2) (A.3)

Finally, if f is additionally convex, then for all x, y ∈ Rd

∇f (x) − ∇f (y) 2 ≤ 2LDf (x, y).

(A.4)

Deﬁnition A.1.2 (µ-strong convexity). A diﬀerentiable function f : Rn → R is called

235

µ-strongly convex if there exists a constant µ ≥ 0 such that for all x, y ∈ Rn f (y) ≥ f (x) + ∇f (x), y − x + µ y − x 2. 2

(A.5)

A.2 Compression and Quantization Operators
Deﬁnition A.2.1 (Quantization). We say that a stochastic mapping Q : Rd → Rd is a quantization operator/quantization if there exists ω > 0 such that for any x ∈ Rd , we have

E [Q(x)] = x, E Q(x) − x 2 ≤ ω x 2.

(A.6)

For the given quantization operator Q(x), we deﬁne the the expected density as ζQ = supx∈Rd E [ Q(x) 0] , where y 0 is the number of non-zero components of y ∈ Rd.

Notice that the expected density is well-deﬁned for any quantization operator since Q(x) 0 ≤ d. Below we enumerate some classical compression and quantization operators (see more in [20]).
1. TopK sparsiﬁcation. This compression operator is deﬁned as follows:

K
C(x) = x(i)e(i)
i=1

where |x(1)| ≥ |x(2)| ≥ . . . ≥ |x(d)| are components of x sorted in the decreasing order of

their absolute values, e1, . . . , ed is the standard basis in Rd and K is some number from

[d]. Clearly, TopK is a biased compression operator. One can show that TopK satisﬁes

(3.8)

with

δ

=

K d

[20].

2. RandK sparsiﬁcation operator is deﬁned as

Q(x) = d xiei K i∈S

where S is a random subset of [d] sampled from the uniform distribution on the all subset

of [d] with cardinality K. RandK is an unbiased compression operator satisfying (A.6)

with

ω

=

d K

.

3. p-quantization. By 2-quantization we mean the following random operator:

Q(x) = x psign(x) ◦ ξ

where x p =

d i=1

|xi|p

1/p
is an

p-norm of vector x, sign(x) is a component-wise sign

of vector x, a ◦ b deﬁnes a component-wise product of vectors a and b and ξ = (ξ1, . . . , ξd)

236

is a random vector such that

 1, ξi = 0,

with probability |xxi|p , with probability 1 − |xxi|p .

One can show that this operator satisﬁes (A.6). In particular, if p = 2 it satisﬁes (A.6)

√

√

with ω =

d−1

and

if

p

=

∞,

then

ω

=

1+ 2

d

−1

(see

[139]).

We assume that C is any operator which enjoys the following contractive property: there exists a constant 0 < δ ≤ 1 such that

E x − C(x) 2 ≤ (1 − δ) x 2, ∀x ∈ Rd.

A.3 Basic Inequalities

For all a, b, x1, . . . , xn ∈ Rd, β > 0 and p ∈ (0, 1] the following inequalities hold

a, b ≤ a 2 + β b 2 ,

2β

2

a − b, a + b = a 2 − b 2,

1 a 2 − b 2 ≤ a + b 2, 2 a + b 2 ≤ (1 + β) a 2 + (1 + 1/β) b 2,

n

2

n

xn ≤ n

xi 2,

i=1

i=1

p −1 1 − 2 ≤ 1 + p,

1 + p (1 − p) ≤ 1 − p .

2

2

(A.7) (A.8) (A.9) (A.10) (A.11)
(A.12) (A.13)

A.4 Identities and Inequalities Involving Random Variables
Variance decomposition. For a random vector ξ ∈ Rd and any deterministic vector x ∈ Rd the variance can be decomposed as

E ξ − Eξ 2 = E ξ − x 2 − Eξ − x 2

(A.14)

Tower property of mathematical expectation. For random variables ξ, η ∈ Rd we have

E [ξ] = E [E [ξ | η]]

(A.15)

237

under assumption that all expectations in the expression above are well-deﬁned.
A.5 Auxiliary Results and Technical Lemmas
The next lemma is used in the analysis of methods with delayed gradients (see Section B.5).

Lemma A.5.1 (Lemma 14 from [209]). For any τ vectors a1, . . . , aτ ∈ Rd and ξ1, . . . , ξτ zero-mean random vectors in Rd, each ξt conditionally independent of {ξi}ti=−11 for all 1 ≤ t ≤ τ the following inequality holds


τ

2

τ

E  (at + ξt)  ≤ τ

τ
at 2 + E ξt 2.

t=1

t=1

t=1

(A.16)

However, the above lemma is not applicable in the analysis of methods with local steps. To overcome this issue, we propose a generalized version of this result.

Lemma A.5.2. For any τ random vectors ξ1, . . . , ξτ ∈ Rd such that for all t = 2, . . . , τ random

vector ξt depends on ξ1, . . . , ξt−1 and does not depend on ξt+1, . . . , ξτ the following inequality

holds


τ

2

τ

τ

E  ξt  ≤ eτ E Et[ξt] 2 + e E ξt − Et[ξt] 2 ,

(A.17)

t=1

t=1

t=1

where Et[·] denotes the conditional expectation E[· | ξt−1, . . . , ξ1].

Proof. First of all, if τ = 1 then (A.16) immediately follows from variance decompostion (A.14). Otherwise (τ > 1) for all l = 1, . . . , τ we have


l

2

El  ξt 

t=1

(A=.14)

(A.10)
≤

l−1 2
El[ξl] + ξt + El
t=1

ξl − El[ξl] 2

1+ 1 τ −1

l−1 2
ξt + τ
t=1

El[ξl] 2 + El

ξl − El[ξl] 2 .

Taking full mathematical expectation and using tower property (A.15) we derive


l

2

E  ξt  ≤

t=1

1+ 1 τ −1


l−1

2

E  ξt  + τ E

t=1

El[ξl] 2 + E

ξl − El[ξl] 2

 l 2 for all l = 1, . . . , τ . Unrolling the recurrence for E  ξt  we obtain
t=1


τ

2

τ

1 τ −t

E  ξt  ≤ τ

1+ τ −1

E

t=1

t=1

Et[ξt] 2

τ
+
t=1

1+ 1 τ −1

τ −t
E

ξt − Et[ξt] 2 .

238

Since 1 + τ−1 1 τ−t ≤ 1 + τ−1 1 τ−1 ≤ e for all t = 1, . . . , τ we get (A.16).

We use the following lemma to derive the ﬁnal complexity results from Chapter 3 in the strongly convex case.

Lemma A.5.3 (see also Lemma 2 from [206]). Let {rk}k≥0 satisfy

rK ≤ a + c1γ + c2γ2 γWK

(A.18)

for all K ≥ 0 with some constants a, c2 ≥ 0, c1 ≥ 0 where {wk}k≥0 and {WK }K≥0 are deﬁned in (3.18), γ ≤ d1 . Then for all K such that

either or

ln (max{2, min{aµ2K2/c1, aµ3K3/c2}}) ≤ min{ρ1, ρ2}
K

1 ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

≤

h

µK

and we have that

γ = min

1 ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

,

d

µK

rK = O da exp − min

µ , ρ1, ρ2 K

+ c1 +

c2

.

d

µK µ2K2

(A.19) (A.20)

Proof. Since WK ≥ wK = (1 − η)−(K+1) we have

rK ≤ (1 − η)K+1 a + c1γ + c2γ2 ≤ a exp (−η(K + 1)) + c1γ + c2γ2.

γ

γ

(A.21)

Next we consider two possible situations.
1. If d1 ≥ ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) then we choose γ = ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) and get that

rK

(A.25)
≤

a exp (−η(K + 1)) + c1γ + c2γ2

γ

ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

= O aµK exp − min ρ1, ρ2,

K

K

+O c1 + c2 . µK µ2K2

239

Since ln(max{2,min{aµ2KK2/c1,aµ3K3/c2}}) ≤ min{ρ1, ρ2} we have

aµ2K2 aµ3K3

rK = O aµK exp − ln max 2, min

,

c1

c2

+O c1 + c2 µK µ2K2

= O c1 + c2 . µK µ2K2

2. If d1 ≤ ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) then we choose γ = d1 which implies that

rK

(A.25)
≤

da exp

− min

µ , ρ1 , ρ2

(K + 1)

+ c1 + c2

d4 4

d d2

=

O da exp − min

µ , ρ1, ρ2 K

+ c1 +

c2

.

d

µK µ2K2

Combining the obtained bounds we get the result.

In Chapter 4, we apply slightly diﬀerent result in the strongly convex case.

Lemma A.5.4 (see also Lemma 2 from [206]). Let {rk}k≥0 satisfy rK ≤ a + c1γ + c2γ2 γWK

(A.22)

for all K ≥ 0 with some constants a, c2 ≥ 0, c1 ≥ 0 where {wk}k≥0 and {WK }K≥0 are deﬁned in (4.12), γ ≤ h1 . Then for all K such that

either or

ln (max{2, min{aµ2K2/c1, aµ3K3/c2}}) ≤ρ
K

1 ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

≤

h

µK

and we have that

γ = min

1 ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

,

h

µK

rK = O ha exp − min

µ ,ρ K

+ c1 +

c2

.

h

µK µ2K2

(A.23) (A.24)

Proof. Since WK ≥ wK = (1 − η)−(K+1) we have

rK ≤ (1 − η)K+1 a + c1γ + c2γ2 ≤ a exp (−η(K + 1)) + c1γ + c2γ2.

γ

γ

Next we consider two possible situations.

(A.25)

240

1. If h1 ≥ ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) then we choose γ = ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) and get that

rK

(A.25)
≤

a exp (−η(K + 1)) + c1γ + c2γ2

γ

= O aµK exp − min ρ, ln (max{2, min{aµ2K2/c1, aµ3K3/c2}}) K K

+O c1 + c2 . µK µ2K2

Since ln(max{2,min{aµ2KK2/c1,aµ3K3/c2}}) ≤ ρ we have

aµ2K2 aµ3K3

rK = O aµK exp − ln max 2, min

,

c1

c2

+O c1 + c2 µK µ2K2

= O c1 + c2 . µK µ2K2

2. If h1 ≤ ln(max{2,min{aµµ2KK2/c1,aµ3K3/c2}}) then we choose γ = h1 which implies that

rK

(A.25)
≤

ha exp

− min

µρ ,

(K + 1)

+ c1 + c2

h4

h h2

=

O ha exp − min

µ ,ρ K

+ c1 +

c2

.

h

µK µ2K2

Combining the obtained bounds we get the result.

In the analysis of Moshpit-SGD, we also use the following lemma that follows from the previous one.

Lemma A.5.5. Let {rk}k≥0 satisfy rK ≤ a + c1γ + c2γ2 γWK

for all K ≥ 0 with some constants a, c2 ≥ 0, c1 ≥ 0, where wk = (1 − γµ(1 − δpv,1))−(k+1),

WK =

K k=0

wk

,

µ

>

0,

δpv,1

∈

[0, 1)

and

γ

≤

γ0

for

some

γ0

>

0,

γ0

≤

1/µ(1−δpv,1).

Then,

for

all K such that

either ln (max {2, min {aµ2(1−δpv,1)2K2/c1, aµ3(1−δpv,1)3K3/c2}}) ≤ 1 K

ln (max {2, min {aµ2(1−δpv,1)2K2/c1, aµ3(1−δpv,1)3K3/c2}})

or γ0 ≤

(1 − δpv,1)µK

241

and γ = min
we have that

ln (max {2, min {aµ2(1−δpv,1)2K2/c1, aµ3(1−δpv,1)3K3/c2}})

γ0,

(1 − δpv,1)µK

rK = O a exp (−γ0µ(1 − δpv,1)K) +

c1

+

c2

.

γ0

(1 − δpv,1)µK (1 − δpv,1)2µ2K2

To establish the complexity bounds in the convex case, we apply the lemma below.

Lemma A.5.6. Let {rk}k≥0 satisfy

rK ≤ a + b1γ + b2γ2 + c1γ + c2γ2 γK K K

(A.26)

for all K ≥ 0 with some constants a > 0, b1, b2, c1, c2 ≥ 0 where γ ≤ γ0. Then for all K and

we have that

γ = min γ0,

aa ,3 ,
b1 b2

a

a

,3

c1K c2K

rK = O

√

√

a + ab1 + 3 a2b2 +

γ0K K

K

√

ac1 + 3 a2c2 .

K

K 2/3

(A.27)

Proof. We have

rK ≤ a + b1γ + b2γ2 + c1γ + c2γ2 γK K K

≤ min γ0,

a ba1 , 3 ba2 ,

c1aK , 3 c2aK

+ b1 · KK

a + b2 · 3 a b1 K b2

a

a2

+c1 ·

+ c2 3

c1K

c2K

√

√

√

= O a + ab1 + 3 a2b2 + ac1 + 3 a2c2 .

γ0K K

K

K

K 2/3

Next, we use the following result in the analysis of methods presented in Chapter 5.

Lemma A.5.7 (Lemma 2 from [118]). Assume that function f is L-smooth and xk+1 = xk − γgk. Then

f (xk+1) ≤ f (xk) − γ ∇f (xk) 2 − 1 − L

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2. 2

(A.28)

Finally, in the analysis of Moshpit-SGD, we use the following classical result establishing contractiveness of the gradient descent step.

242

Lemma A.5.8 (Lemma 6 from [86]). For any L-smooth and µ-strongly convex function f : Rn → R, points x, y ∈ Rn, and stepsize γ ∈ (0, 1/L], the following inequality holds:

x − γ∇f (x) − y + γ∇f (y) 2 ≤ (1 − γµ) x − y 2.

(A.29)

243

Appendix B
244

Appendix for Chapter 3

B.1 Missing Plots

B.1.1 Compressing Stochastic Gradients

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2 10 3

w8a, 20 workers
EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0.0
100 10 1 10 2 10 3

0.N5umb1e.r0of bi1t.s5per w2.o0rker 2.5 1e37.0 w8a, 20 workers
EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6

0 Num5ber o1f0passe1s5thro2u0gh th2e5data30

100

w8a, 20 workers
EC-SGD identical

10 1

EC-SGD top-3 EC-SGD-DIANA top-3 rand-3

10 2

EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3

EC-L-SVRG-DIANA top-3 rand-3

10 3

EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0.0
100 10 1 10 2 10 3

0.2Nu0m.4ber0o.f6bits0.p8er w1.o0rke1r.2 11.4e9 w8a, 20 workers
EC-SGD identical EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0 Num5ber o1f0passe1s5thro2u0gh th2e5data30

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

mushrooms, 20 workers

10 1

EC-SGD top-1 EC-SGD-DIANA top-1 rand-1

EC-SGD-DIANA top-1 l2-quant

10 3

EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1

EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7

10 9

0.0
10 1 10 3 10 5

0N.u5mber 1o.f0bits pe1r.5worker2.0 1e7 mushrooms, 20 workers
EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0 Nu1m0b0er o2f0p0asse3s00thro4u0g0h the50d0ata 600

mushrooms, 20 workers

10 1

EC-SGD identical EC-SGD top-1

EC-SGD-DIANA top-1 rand-1

10 3

EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1

EC-L-SVRG-DIANA top-1 rand-1

10 5

EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0.0
10 1 10 3 10 5

0.2Num0b.e4r of b0i.t6s pe0r .w8orke1r.0 11.e29 mushrooms, 20 workers
EC-SGD identical EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0 Nu1m0b0er o2f0p0asse3s00thro4u0g0h the50d0ata 600

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2

gisette, 20 workers
EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10

5
0.00

100 10 1 10 2

0.25Nu0m.5b0er0o.f7b5its1.p0e0r w1o.2rk5er1.50 11.e785 gisette, 20 workers
EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10 5 0 20 40 60 80

Number of passes through the data

100

gisette, 20 workers
EC-SGD identical

EC-SGD top-50

10 1

EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant

10 2

EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50

EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10 5 0
100 10 1 10 2 10 3

Nu2mber of 4bits per w6orker 8 1e9 gisette, 20 workers
EC-SGD identical EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 4

10 5 0 20 40 60 80 Number of passes through the data

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure B.1: Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to solving logistic regression problem with 20 workers. EC-SGD identical corresponds to SGD with error compensation with the identity compression operator C(x) = x, i.e., it is just parallel SGD.

245

B.1.2 Compressing Full Gradients

f(xk) f(x * ) f(x0) f(x * )

100

w8a, 20 workers EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0.00 0.25N0u.m50be0r.7o5f b1it.0s0pe1r.2w5or1k.5er0 1.7512e.700

100

w8a, 20 workers
EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

mushrooms, 20 workers
EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

10 2

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 3

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

Number of bits per worker 1000000 2000000 3000000 4000000 5000000 6000000 7000000 mushrooms, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

gisette, 20 workers
EC-GD top-50

EC-GD top-100

10 2

EC-GD-star top-50 EC-DIANA top-50 l2-quant

EC-DIANA top-50 rand-50

10 4

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.5Num1b.0er of1b.5its p2e.r0wor2k.e5r 3.01e8 gisette, 20 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

Figure B.2: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solving logistic regression problem with 20 workers.

246

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 100 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 100000N0u20m00b00e0r30o00f0b00it4s000p0e00r5w00o00r0k0e60r00000 7000000

a9a, 100 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 100 workers EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0.00 0.25N0u.m50be0r.7o5f b1it.0s0pe1r.2w5or1k.5er0 1.7512e.700

100

w8a, 100 workers
EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 100 workers
EC-GD top-5

EC-GD top-10

10 2

EC-GD-star top-5 EC-GD-DIANA top-5 rand-5

10 4

EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0N.5umbe1r .o0f bits1p.e5r wor2ke.0r 21.5e7 madelon, 100 workers
EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 100 workers
EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

10 2

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 3

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

Number of bits per worker 1000000 2000000 3000000 4000000 5000000 6000000 7000000 mushrooms, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4

phishing, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0.0 0.2Num0.b4er 0of.6bits0p.8er w1o.0rker1.2 11.e47 phishing, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000 gisette, 100 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.5Num1b.0er of1b.5its p2e.r0wor2k.e5r 3.01e8 gisette, 100 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure B.3: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solving logistic regression problem with 100 workers.

247

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 20 workers

GD

10 1

EC-GD top-1 EC-GD top-2

EC-GD-star top-1

10 3

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7 0.0
10 1 10 3

0.5 Nu1m.0be1r.o5f b2it.0s pe2r.5wo3rk.0er 3.5 14e.80 a9a, 20 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 20 workers
GD

EC-GD top-3

10 2

EC-GD top-6 EC-GD-star top-3

EC-GD-DIANA top-3 rand-3

10 4

EC-GD-DIANA top-3 l2-quant

10 6

10 8

0.0
100 10 2 10 4

0N.u2mber0o.f4bits p0e.r6worke0r.8 1e19.0 w8a, 20 workers
GD EC-GD top-3 EC-GD top-6 EC-GD-star top-3 EC-GD-DIANA top-3 rand-3 EC-GD-DIANA top-3 l2-quant

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers
GD

EC-GD top-5

10 2

EC-GD top-10 EC-GD-star top-5

10 4

EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0.2Num0b.4er of0b.6its p0e.r8wor1ke.0r 1.21e9 madelon, 20 workers
GD EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 20 workers
GD

10 1

EC-GD top-1 EC-GD top-2

10 2

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0.0 100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

0.5Nu1m.0ber1o.f5bits2.p0er w2.o5rke3r.0 31.5e8 mushrooms, 20 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 20 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu1mber of2bits per w3orker 4 1e8

100

phishing, 20 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

100

gisette, 20 workers
GD

EC-GD top-50

10 2

EC-GD top-100 EC-GD-star top-50

EC-DIANA top-50 l2-quant

10 4

EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.2 N0u.m4be0r.6of b0i.t8s p1e.r0wo1r.k2er 1.4 11e1.60 gisette, 20 workers
GD EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure B.4: Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solving logistic regression problem with 20 workers.

248

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 100 workers

GD

10 1

EC-GD top-1 EC-GD top-2

EC-GD-star top-1

10 3

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0.0
10 1 10 3

0.5 Nu1m.0be1r.o5f b2it.0s pe2r.5wo3rk.0er 3.5 14e.80 a9a, 100 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 100 workers
GD

EC-GD top-3

10 2

EC-GD top-6 EC-GD-star top-3

EC-GD-DIANA top-3 rand-3

10 4

EC-GD-DIANA top-3 l2-quant

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0N.u2mber0o.f4bits p0e.r6worke0r.8 1e19.0 w8a, 100 workers
GD EC-GD top-3 EC-GD top-6 EC-GD-star top-3 EC-GD-DIANA top-3 rand-3 EC-GD-DIANA top-3 l2-quant

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 100 workers
GD

EC-GD top-5

10 2

EC-GD top-10 EC-GD-star top-5

10 4

EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0.2Num0b.4er of0b.6its p0e.r8wor1ke.0r 1.21e9 madelon, 100 workers
GD EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 100 workers
GD

10 1

EC-GD top-1 EC-GD top-2

10 2

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0.0 100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

0.5Nu1m.0ber1o.f5bits2.p0er w2.o5rke3r.0 31.5e8 mushrooms, 100 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 100 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu1mber of2bits per w3orker 4 1e8

100

phishing, 100 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

100

gisette, 100 workers
GD

EC-GD top-50

10 2

EC-GD top-100 EC-GD-star top-50

EC-DIANA top-50 l2-quant

10 4

EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.2 N0u.m4be0r.6of b0i.t8s p1e.r0wo1r.k2er 1.4 11e1.60 gisette, 100 workers
GD EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure B.5: Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solve logistic regression problem with 100 workers.

B.2 Compression Operators: Extra Commentary
Communication eﬃcient distributed SGD methods based on the idea of communication compression exists in two distinct varieties: i) methods based on unbiased compression operators, and ii) methods based on biased compression operators. The ﬁrst class of methods is much mire developed than the latter since it is easier to theoretically analyze unbiased operators. The subject of this chapter is the study of the latter and dramatically less developed and understood class.
B.2.1 Unbiased Compressors
By unbiased compression operators we mean randomized mappings Q : Rd → R satisfying the relations
EQ(x) = x and E Q(x) − x 2 ≤ ω x 2, ∀x ∈ Rd

249

for some ω ≥ 0. While operators satisfying the above relations are often in the literature called quantization operators, this class includes compressors which perform sparsiﬁcation as well.
Among the ﬁrst methods using unbiased compressors developed in this ﬁeld are QSGD [4], TernGrad [227] and DQGD [92]. The ﬁrst analysis of QSGD and TernGrad without bounded gradients assumptions was proposed in [139], which contains the best known results for QSGD and TernGrad. However, existing guarantees in the strongly convex case for QGSD, TernGrad, and DQGD establish linear convergence to some neighborhood of the solution only, even if the workers quantize the full gradients of their functions. This problem was resolved by [139], who proposed the ﬁrst method, called DIANA, which uses quantization for communication and enjoys the linear rate of convergence to the exact optimum asymptotically in the strongly convex case when workers compute the full gradients of their functions in each iteration. Unlike all previous approaches, DIANA is based on the quantization of gradient diﬀerences rather than iterates or gradients. In essence, DIANA is a technique for reducing the variance introduced by quantization. [79] generalized the DIANA method to the case of more general quantization operators. Moreover, the same authors developed a new method called VR-DIANA specially designed to solve problems (3.1) with the individual functions having the ﬁnite sum structure (3.3).
B.2.2 Biased Compressors
By biased compressors we mean (possibly) randomized mappings C : Rd → R satisfying the average contraction relation
E C(x) − x 2 ≤ (1 − δ) x 2, ∀x ∈ Rd
for some δ > 0.
Perhaps the most popular biased compression operator is TopK, which takes vector x as input and substitutes all coordinates of x by zero except the k components with the largest absolute values. However, such a greedy approach applied to simple distributed SGD and even distributed GD can break the convergence of the method even when applied to simple functions in small dimensions, and may even lead to exponential divergence [20]. The error-feedback framework described in [88, 209, 208] and studies in this chapter can ﬁx this problem, and it remains the only known mechanism that does so for all compressors described in (3.8). This is one of the main motivations for the study of the error-feedback mechanism. For instance, error feedback can ﬁx convergence issues with methods like sign-SGD [18]. The analysis of error feedback by [88, 209, 208] works either under the assumption that the second moment of the stochastic gradient is uniformly bounded or only for the single-worker case. Recently Beznosikov et al. [20] proposed the ﬁrst analysis of SGD with error feedback for the general case of multiple workers without bounded second moment assumption. There is another line of works [98, 96] where authors apply arbitrary compressions in the decentralized setup. This approach has better potential than a centralized one in terms of reducing the communication cost. However, in this chapter, we study only centralized architecture.
250

B.3 Proofs for Section 4.2

B.3.1 A Lemma
Lemma B.3.1 (See also Lemma 8 from [209]). Let Assumptions 3.3.1, 3.3.3 and 4.2.2 be satisﬁed and γ ≤ 1/4(A +C1M1+C2M2). Then for all k ≥ 0 we have

γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2,

(B.1)

where T k d=ef x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and M1 = 43Bρ11 , M2 = 4(B23+ρ243 G) .

Proof. We start with the upper bound for E x˜k+1 − x∗ 2. First of all, by deﬁnition of x˜k we have

x˜k+1 − x∗ 2

(3=.22) = =

x˜k − x∗ − γgk 2 x˜k − x∗ 2 − 2γ x˜k − x∗, gk + γ2 gk 2 x˜k − x∗ 2 − 2γ xk − x∗, gk + γ2 gk 2 + 2γ xk − x˜k, gk .

Taking conditional expectation E · | xk from the both sides of the previous inequality we get

E x˜k+1 − x∗ 2 | xk

(3.16),(3.13)
≤
(4.5)
≤

x˜k − x∗ 2 − 2γ xk − x∗, ∇f (xk)

+γ2 2A (f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1 +2γ xk − x˜k, ∇f (xk)

x˜k − x∗ 2 − γµ xk − x∗ 2 − γ(2 − 2A γ)(f (xk) − f (x∗))

+γ2B1σ12,k + γ2B2σ22,k + γ2D1

+2γ xk − x˜k, ∇f (xk) .

(B.2)

Next,

−

xk − x∗

2 = − x˜k − x∗ + xk − x˜k

2

(A.9)
≤

−1

x˜k − x∗

2+

xk − x˜k

2.

2

(B.3)

Using Fenchel-Young inequality we derive an upper bound for the inner product from (B.2):

xk − x˜k, ∇f (xk)

(A.7)
≤L

xk − x˜k

2+

1

∇f (xk)

(A.4)
2≤L

xk − x˜k

2 + 1 (f (xk) − f (x∗)).

(B.4)

4L

2

Combining previous three inequalities we get

E x˜k+1 − x∗ 2 | xk

(B.2)−(B.4)
≤

1 − γµ 2

x˜k − x∗ 2 − γ 1 − 2A γ

+γ2B1σ12,k + γ2B2σ22,k + γ2D1

+γ(2L + µ) xk − x˜k 2.

(f (xk) − f (x∗)) (B.5)

251

Taking into account that T k = x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k with M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) , using the tower property (A.15) of mathematical expectation together with γ ≤
4(A +C1M11+C2M2) , we conclude

E T k+1

(B.5)
≤

(3.14),(3.15)
≤

≤

1 − γ2µ E x˜k − x∗ 2 − γ 1 − 2A γ E f (xk) − f (x∗) + M1γ2E σ12,k+1 M2γ2E σ22,k+1 + γ2B1σ12,k + γ2B2σ22,k + γ2D1 + γ(2L + µ)E xk − x˜k 2

1 − γ2µ E x˜k − x∗ 2 + 1 + MB11 − ρ1 M1γ2E σ12,k

+ 1 + B2 + M1Gρ1 − ρ2 M2γ2E σ2 + γ2(D + M1D2)

M2

2,k

1

−γ 1 − 2(A + C1M1 + C2M2)γ E f (xk) − f (x∗)

+γ(2L + µ)E xk − x˜k 2
1 − γ2µ E x˜k − x∗ 2 + 1 − ρ41 M1γ2E σ12,k + 1 − ρ42 M2γ2E σ22,k − γ2 E f (xk) − f (x∗) + γ(2L + µ)E xk − x˜k 2 + γ2(D1 + M1D2).

Since L ≥ µ, x˜k = xk − ek and η d=ef min{ γ2µ , ρ41 , ρ42 } the last inequality implies γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2,

which concludes the proof.

B.3.2 Proof of Theorem 3.3.4

Proof. Form Lemma B.3.1 we have γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2.
Summing up these inequalities for k = 0, . . . , K with weights wk = (1 − η)−(k+1) we get

12 K wkE f (xk) − f (x∗)
k=0

≤
(3.17),(3.18)
≤

K wk(1γ− η) ET k − wγk ET k+1 + γ(D1 + M1D2) K wk

k=0

k=0

K
+3L wkE ek 2

k=0

K wkγ−1 ET k − wγk ET k+1 + F1σ12,0 + F2σ22,0
k=0

+γ2(D1 + M1D2 + D3)WK

+ 14 K wkE f (xk) − f (x∗) .
k=0

252

Rearranging the terms and using x¯K = W1K obtain

K k=0

wk

xk

together

with

Jensen’s

inequality

we

E f (x¯K ) − f (x∗)

4(T 0 + γF1σ12,0 + γF2σ22,0)

≤

γWK

+ 4γ D1 + M1D2 + D3 .

Finally, using the deﬁnition of the sequences {WK }K≥0 and {wk}k≥0 we derive that if µ >, then WK ≥ wK ≥ (1 − η)−K and we get (3.19). In the case when µ = 0 we have wk = 1 and WK = K which implies (3.20).

B.4 Distributed SGD with Compression and Error Compensation: Missing Proofs

Lemma B.4.1 (Lemma 3.7.1). Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, Assumption 3.3.2 holds anda


  γ ≤ min  δ ,  4µ   

96L

2δA + A + 1−2ρ1

δ Cρ11 + ρ22(G1−Cρ22)

2B1 δ

+

B1

2C2 2B2 +B2

+

δ
ρ2(1−ρ2)


   
, (B.6)
   

where M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) . Then EC-SGD satisﬁes Assumption 3.3.3, i.e., inequality (3.17) holds with the following parameters:

24Lγ2 F1 = δρ1(1 − η)

2B1 + B1 , δ D3 = 6Lγ δ

24Lγ2 F2 = δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 δ

D2 2B1 + B1 + 2D1 + D1 .

ρ1 δ

δ

+ 2B2 + B2 , δ (B.7)
(B.8)

aWhen ρ1 = 1 and ρ2 = 1 one can always set the parameters in such a way that B1 = B1 = B2 = B2 =

C1 = C2 = 0, D2 = 0. In this case we assume that 1−2ρ1 Cρ11 + ρ22(G1−Cρ22)

2Bδ 1 + B1

+ 2C2 2Bδ 2 +B2 ρ2 (1−ρ2 )

= 0.

Proof. First of all, we derive an upper bound for the second moment of eki +1:

E eki +1 2

(3.35)=,(A.15)
(3.8)
≤
(A.15)=,(A.14)
(A.10)
≤

E E eki + γgik − C(eki + γgik) 2 | eki , gik

(1 − δ)E eki + γgik 2

(1 − δ)E eki + γg¯ik 2 + (1 − δ)γ2E gik − g¯ik 2

(1 − δ)(1 + β)E ek 2 + (1 − δ) 1 + 1 γ2E g¯k 2

i

β

i

+(1 − δ)γ2E gik − g¯ik 2.

253

Summing up these inequalities for i = 1, . . . , n we get

n1 n E eki +1 2 ≤ (1 − δ)(1 + β) n1 n E eki 2

i=1

i=1

+(1 − δ)

1+ 1 β

γ2 1 n E g¯k 2

n

i

i=1

+(1 − δ)γ2 1 n E gk − g¯k 2.

n

i

i

i=1

(B.9)

Consider β = 2(1δ−δ) . For this choice of β we have

(1 − δ)(1 + β) = (1 − δ) 1 + δ

=1− δ

2(1 − δ)

2

(1 − δ) 1 + 1 β

= (1 − δ) 1 + 2(1 − δ) = (1 − δ)(2 − δ) ≤ 2(1 − δ) .

δ

δ

δ

Using this we continue our derivations:

1n

k+1 2

n E ei

≤

i=1

(3.11),(3.12)
≤

1− δ 2
+(1 − 1− δ
2 +γ2(1 +γ2(1

1 n E ek 2 + 2γ2(1 − δ) 1 n E g¯k

n

i

δn

i

i=1

i=1

δ)γ2 1 n E gk − g¯k 2

n

i

i

i=1

n1 n E eki 2 + 2γ2(1 − δ)
i=1

2A + A δ

− δ) 2Bδ 1 + B1 Eσ12,k + γ2(1 − δ)

− δ) 2D1 + D1 . δ

2
E f (xk) − 2B2 + B2
δ

f (x∗) Eσ22,k (B.10)

Unrolling the recurrence above we get

1n

k+1 2

n E ei

i=1

(B.10)
≤

2γ2(1 − δ) 2A + A k δ l=0
+γ2(1 − δ) 2B1 + B1 δ
+γ2(1 − δ) 2B2 + B2 δ
+γ2(1 − δ) 2D1 + D1 δ

1− δ

k−l
E f (xl) − f (x∗)

2

k 1 − 2δ k−l Eσ12,l
l=0

k 1 − 2δ k−l Eσ22,l
l=0

k

δ k−l

1− 2

l=0

(B.11)

254

which implies

K
3L wkE ek 2
k=0

(3=.35)
(B.11)
≤

K
3L wkE
k=0

1n

2 (A.11) k

K

1n

n ei ≤ 3L wk n E

i=1

k=0

i=1

eki 2

6Lγ2(1 − δ)

1

−

δ 2

2A + A δ

Kk
wk
k=0 l=0

1− δ

k−l
E f (xl) − f (x∗)

2

3Lγ2(1 − δ) + 1− δ
2

2B1 + B1 δ

Kk
wk
k=0 l=0

1 − 2δ k−l Eσ12,l

3Lγ2(1 − δ) + 1− δ
2

2B2 + B2 δ

Kk
wk
k=0 l=0

1 − 2δ k−l Eσ22,l

3Lγ2(1 − δ) + 1− δ
2

2D1 + D1 δ

Kk
wk
k=0 l=0

δ k−l 1− 2 .

(B.12)

In the remaining part of the proof we derive upper bounds for three terms in the right-hand side of the previous inequality. First of all, recall that wk = (1 − η)−(k+1) and η = min γ2µ , ρ41 , ρ42 . It implies that for all 0 ≤ i < k we have

(A.12)
wk = (1 − η)−(k−j+1) (1 − η)−j ≤ wk−j (1 + 2η)j

≤ wk−j (1 + γµ)j (3≤.36) wk−j 1 + 4δ j ,

(A.12)
wk = (1 − η)−(k−j+1) (1 − η)−j ≤ wk−j (1 + 2η)j

min{ρ1, ρ2} j

≤ wk−j 1 +

2

.

(B.13) (B.14)

For simplicity, we introduce new notation: rk d=ef E f (xk) − f (x∗) . Using this we get

Kk
wk
k=0 l=0

δ k−l 1 − 2 rl

(B.13)
≤
(A.13)
≤
≤

Kk
wlrl
k=0 l=0 Kk
wlrl
k=0 l=0 K
wk rk
k=0

δ k−l

δ k−l

1+ 4

1− 2

δ k−l 1− 4

∞

δk

1− 4

k=0

4K

=

wk rk .

δ k=0

Next, we apply our assumption on σ22,k and derive that

(B.15)

Eσ22,k+1

(3.15)
≤
≤

(1 − ρ2)Eσ22,k + 2C2 E f (xk) − f (x∗)
rk k
(1 − ρ2)k+1σ22,0 + 2C2 (1 − ρ2)k−lrl,
l=0

(B.16)

255

hence

K k wk 1 − 2δ k−l Eσ22,l ≤ K k wk 1 − 2δ k−l (1 − ρ2)lσ22,0

k=0 l=0

k=0 l=0

2C2 K k l

+ 1 − ρ2

wk

k=0 l=0 t=0

1− δ 2

k−l
(1 − ρ2)l−trt.

Using this and

wk 1 − 2δ k−l (1 − ρ2)l−t

(B.13)
≤

wl 1 + 4δ k−l 1 − 2δ k−l (1 − ρ2)l−t

(A.13),(B.14)
≤

1 − 4δ k−l 1 + ρ22 l−t (1 − ρ2)l−twt

(A.13)
≤

δ k−l

ρ2 l−t

1− 4

1 − 2 wt

we derive

K k wk 1 − 2δ k−l Eσ22,l ≤ K k wk 1 − 4δ k−l 1 − ρ22 l w0σ22,0

k=0 l=0

k=0 l=0

2C2 K k l

δ k−l

ρ2 l−t

+ 1 − ρ2

1− 4

1− 2

wtrt

k=0 l=0 t=0

≤ w0σ22,0 ∞ 1 − 4δ k
k=0

∞ 1 − ρ2 k k=0 2

2C2 1 − ρ2

K
wk rk
k=0

∞

δk

1− 4

k=0

∞ 1 − ρ2 k k=0 2

8σ22,0

16C2 K

= δρ2(1 − η) + δρ2(1 − ρ2) wkrk.

k=0

(B.17)

256

Similarly, we estimate σ12,k:

Eσ12,k+1

(3.14)
≤ ≤
≤
=

(1 − ρ1)Eσ12,k + 2C1 E f (xk) − f (x∗) +Gρ1Eσ22,k + D2

rk

k

k

(1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

k
+D2 (1 − ρ1)l

l=0

k

k

(1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

∞

+D2 (1 − ρ1)l

l=0

k

k

(1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

+ D2 . ρ1

(B.18)

Using this we get

K k wk 1 − 2δ k−l Eσ12,l ≤ σ12,0 K k wk 1 − 2δ k−l (1 − ρ1)l

k=0 l=0

k=0 l=0

2C1 K k l

+ 1 − ρ1

wk

k=0 l=0 t=0

1− δ 2

k−l
(1 − ρ1)l−trt

Gρ1 K k l

+ 1 − ρ1

wk

k=0 l=0 t=0

1− δ 2

k−l
(1 − ρ1)l−tEσ22,t

D2 K k l

+

wk

ρ1 k=0 l=0 t=0

1− δ 2

k−l
(1 − ρ1)l−t.

(B.19)

Moreover,

wk 1 − 2δ k−l (1 − ρ1)l−t

(B.13)
≤

wl 1 + 4δ k−l 1 − 2δ k−l (1 − ρ1)l−t

(A.13),(B.14)
≤

1 − 4δ k−l 1 + ρ21 l−t (1 − ρ1)l−twt

(A.13)
≤

δ k−l

ρ1 l−t

1− 4

1 − 2 wt,

257

hence

Kk
wk
k=0 l=0

1 − 2δ k−l Eσ12,l

(B.19)
≤
≤
=

Kk
w0σ12,0

1 − δ k−l 1 − ρ1 l

4

2

k=0 l=0

2C1 K k l

δ k−l

ρ1 l−t

+ 1 − ρ1

1− 4

1− 2

wtrt

k=0 l=0 t=0

+ Gρ1 K k l 1 − ρ1

1 − 4δ k−l 1 − ρ21 l−t wtEσ22,t

k=0 l=0 t=0

+ D2 ρ1

K
wk
k=0

∞

δk

1− 2

k=0

∞
(1 − ρ1)k
k=0

w0σ12,0 ∞ 1 − 4δ k
k=0

∞ 1 − ρ1 k k=0 2

+ 2C1 1 − ρ1

K
wk rk
k=0

∞

δk

1− 4

k=0

∞ 1 − ρ1 k k=0 2

+ Gρ1 1 − ρ1

K
wk Eσ22,k
k=0

∞

δk

1− 4

k=0

∞ 1 − ρ1 k k=0 2

+ 2D2 WK δρ1

8σ12,0 + 16C1

K w r + 8G

K
w Eσ2

δρ1(1 − η) δρ1(1 − ρ1)

k k δ(1 − ρ1)

k 2,k

k=0

k=0

+ 2D2 WK . δρ1

(B.20)

258

For the third term in the right-hand side of previous inequality we have

8G K

2

δ(1 − ρ1) wkEσ2,k

k=0

(B.16)
≤
(B.14)
≤
(A.13)
≤
≤ =

8Gσ22,0

K
w (1 − ρ )k

δ(1 − ρ1)

k

2

k=0

+ 16GC2 K δ(1 − ρ1)(1 − ρ2)

k
wk(1 − ρ2)k−lrl

k=0 l=0

8Gσ22,0w0 K δ(1 − ρ1) k=0

1 + ρ2 2

k
(1 − ρ2)k

+ 16GC2

Kk

δ(1 − ρ1)(1 − ρ2) k=0 l=0

1 + ρ2 2

k−l
(1 − ρ2)k−lwlrl

8Gσ22,0w0 ∞ 1 − ρ2 k δ(1 − ρ1) k=0 2

+ 16GC2

Kk

δ(1 − ρ1)(1 − ρ2) k=0 l=0

1 − ρ2 2

k−l
wlrl

16Gσ22,0w0 +

16GC2

δρ2(1 − ρ1) δ(1 − ρ1)(1 − ρ2)

K
wk rk
k=0

∞ 1 − ρ2 k k=0 2

16Gσ22,0

32GC2

K

δρ2(1 − ρ1)(1 − η) + δρ2(1 − ρ1)(1 − ρ2) wkrk

k=0

(B.21)

Combining inequalities (B.20) and (B.21) we get

Kk
w

1− δ

k−l
Eσ2

≤

8σ12,0 + 16

C1 + 2GC2

k

2

1,l

δρ1(1 − η) δ(1 − ρ1) ρ1 ρ2(1 − ρ2)

K
wk rk

k=0 l=0

k=0

16Gσ22,0

2D2

+ δρ2(1 − ρ1)(1 − η) + δρ1 WK

(B.22)

Finally, we estimate the last term in the right-hand side of (B.12):

Kk

δ k−l

wk 1 − 2

≤

k=0 l=0

K
wk
k=0

∞

δk

1− 2

k=0

= 2 WK . δ

(B.23)

Plugging inequalities (B.15), (B.17), (B.22), (B.23) and

1−δ
δ

≤ 1 in

(B.12) we obtain

1− 2

K

24L

2A δ

+

A

+

2 1−ρ

C1 ρ

+

2GC2 ρ (1−ρ

)

3L wkE ek 2 ≤

1

1

2

2

k=0 δ

2B1 δ

+

B1

2C2 2B2 +B2

+

δ
ρ2(1−ρ2)

+ 24Lγ2 2B1 + B1 σ2

δρ1(1 − η) δ

1,0

+ 24Lγ2 δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 δ

+ 2Bδ 2 + B2 σ22,0

6Lγ2 D2 2B1

2D1

+

+ B1 +

+ D1 WK .

δ ρ1 δ

δ

γ2 K wk rk
k=0

259

Taking into account that γ ≤

96L

2δA +A+ 1−2ρ1

δ Cρ11 + ρ22(G1−Cρ22)

2B

2C2

2B2 δ

+B2

1 +B1 + ρ (1−ρ )

δ

2

2

δρ214(L1γ−2η) 2Bδ 1 + B1 , F2 = δρ224(L1γ−2η) 1−2Gρ1 2Bδ 1 + B˜1 D3 = 6Lδγ Dρ12 2Bδ 1 + B1 + 2Dδ 1 + D1 we get

+

2B2 δ

+

B2

and

, F1 =

3L K wkE ek 2 ≤ 41 K wkrk + F1σ12,0 + F2σ22,0 + γD3.

k=0

k=0

B.5 SGD with Delayed Updates

In this section we consider the SGD with delayed updates (D-SGD) [1, 121, 43, 10, 209]. This method has updates of the form (3.4)-(3.5) with

gk = n1 n gik
i=1

vk =

1n k n vi ,
i=1

 vik = γgik−τ ,
0,

if t ≥ τ, if t < τ

ek =

1n k n ei ,
i=1

τ
eki +1 = eki + γgik − vik = γ gik+1−t,
t=1

(B.24) (B.25)

where the summation is performed only for non-negative indices. Moreover, we assume that e0i = 0 for i = 1, . . . , n.
For convenience we also introduce new constant:

Aˆ = A + Lτ.

(B.26)

Lemma B.5.1. Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, inequalities (3.13), (3.14) and (3.15) hold anda









γ ≤ min  1 , 1  ,

 2τ µ 8 Lτ Aˆ + ρ12(B11−Cρ11) + ρ22(B12−Cρ22) + ρ2(14−Bρ11G)(C12−ρ2) 

(B.27)

where M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) . Then D-SGD satisﬁes Assumption 3.3.3, i.e., inequality

260

(3.17) holds with the following parameters:

F1 = 6γ2LB1τ (2 + ρ1) , ρ1

6γ2τ L(2 + ρ2) F2 =
ρ2

2B1G + B , 1 − ρ1 2

(B.28)

D3 = 3γτ L D + 2B1D2 .

1

ρ1

(B.29)

aWhen ρ1 = 1 and ρ2 = 1 one can always set the parameters in such a way that B1 = B1 = B2 = B2 = C1 = C2 = 0, D2 = 0. In this case we assume that ρ12(B11−Cρ11) = ρ22(B12−Cρ22) = 0.

Proof. First of all, we derive an upper bound for the second moment of eki :

E ek 2

(B=.25)
(A.16)
≤
(A.14)
≤
(3.13),(A.4)
≤


τ

2

γ2E  gk−t 

t=1

τ

2

τ

γ2τ E ∇f (xk−t) + γ2 E

t=1
τ
γ2τ E

t=1

2

τ

∇f (xk−t) + γ2 E

t=1

t=1

τ

2γ2 (A + Lτ ) E f (xk−t) − f (x∗)

Aˆ

t=1

τ

+γ2B2 Eσ22,k−t + γ2τ D1

t=1

gk−t − ∇f (xk−t) 2
gk−t 2
τ
+ γ2B1 Eσ12,k−t
t=1

which implies

(B.30)

K
3L wkE ek 2
k=0

(B.30)
≤

Kτ

6γ2LAˆ

wkE f (xk−t) − f (x∗)

k=0 t=1

Kτ

+3γ2LB1

wk Eσ12,k−t

k=0 t=1

Kτ

+3γ2LB2

wkEσ22,k−t + 3γ2τ LD1WK

k=0 t=1

(B.31)

In the remaining part of the proof we derive upper bounds for four terms in the right-hand side of the previous inequality. First of all, recall that wk = (1 − η)−(k+1) and η = min γ2µ , ρ41 , ρ42 . It implies that for all 0 ≤ i < k and 0 ≤ t ≤ τ we have

(A.12)
wk = (1 − η)−(k−t+1) (1 − η)−t ≤ wk−t (1 + 2η)t

≤ wk−t (1 + γµ)t (B≤.27) wk−t 1 + 21τ t ≤ wk−t exp 2tτ ≤ 2wk−t,

(B.32)

−(k−j+1)

−j (A.12)

j

min{ρ1, ρ2} j

wk = (1 − η)

(1 − η) ≤ wk−j (1 + 2η) ≤ wk−j 1 +

2

(. B.33)

261

For simplicity, we introduce new notation: rk d=ef E f (xk) − f (x∗) . Using this we get

Kτ
wk rk−t
k=0 t=1

(B.32)
≤

Kτ

K

2wk−trk−t ≤ 2τ wkrk

k=0 t=1

k=0

(B.34)

Similarly, we estimate the second term in the right-hand side of (B.33):

Kτ
wk Eσ12,k−t
k=0 t=1

≤
(B.18)
≤

Kτ

K

2wk−tEσ12,k−t ≤ 2τ wkEσ12,k

k=0 t=1

k=0

2τ σ2 K w (1 − ρ )k + 4C1τ K

1,0

k

1

1 − ρ1

k
wk(1 − ρ1)k−lrl

k=0

k=0 l=0

+ 2Gρ1τ K 1 − ρ1

k wk(1 − ρ1)k−lEσ22,l + 2τρD2 WK .

k=0 l=0

(B.35)

For the ﬁrst term in the right-hand side of previous inequality we have

K
2τ σ12,0 wk(1 − ρ1)k
k=0

(B.33)
≤
(A.13)
≤
≤

K
2τ σ12,0
k=0

1 + ρ1 2

k+1
(1 − ρ1)k

2τ 1 + ρ21 σ12,0 K

1 − ρ1 k 2

k=0

∞
τ (2 + ρ1) σ12,0
k=0

1 − ρ1 2

k 2τ (2 + ρ1) σ12,0

≤

.

ρ1

(B.36)

The second term in the right-hand side of (B.35) can be upper bounded in the following way:

4C1τ K 1 − ρ1

k
wk(1 − ρ1)k−lrl

k=0 l=0

(B.33)
≤
(A.13)
≤
≤
≤

4C1τ K k

1 − ρ1

wlrl

k=0 l=0

1 + ρ1 2

k−l
(1 − ρ1)k−l

4C1τ K k

ρ1 k−l

1 − ρ1

wlrl 1 − 2

k=0 l=0

4C1τ 1 − ρ1

K
wk rk
k=0

∞ 1 − ρ1 k k=0 2

8C1τ K ρ1(1 − ρ1) wkrk.
k=0

(B.37)

262

Repeating similar steps we estimate the third term in the right-hand side of (B.35):

2Gρ1τ K

k
w (1 − ρ )k−lEσ2

≤

4Gτ K w Eσ2

1 − ρ1

k

1

2,l

1 − ρ1

k 2,k

k=0 l=0

k=0

(B.16)
≤

4Gτ σ22,0 K w (1 − ρ )k

1 − ρ1

k

2

k=0

+ 8GC2 K (1 − ρ1)(1 − ρ2)

k
wk(1 − ρ2)k−lrl

k=0 l=0

(B.33)
≤

4Gτ σ22,0 K 1 − ρ1 k=0

1 + ρ2 2

k+1
(1 − ρ2)k

+ 8GC2τ

Kk

(1 − ρ1)(1 − ρ2) k=0 l=0

1 + ρ2 2

k−l
(1 − ρ2)k−lwlrl

(A.13)
≤

2Gτ (2 + ρ2)σ22,0 ∞ 1 − ρ2 k 1 − ρ1 k=0 2

+ 8GC2τ

Kk

(1 − ρ1)(1 − ρ2) k=0 l=0

1 − ρ2 2

k−l
wlrl

4Gτ (2 + ρ2)σ22,0 ≤
ρ2(1 − ρ1)

+ 8GC2τ (1 − ρ1)(1 − ρ2)

K
wk rk
k=0

∞ 1 − ρ2 k k=0 2

= 4Gτ (2 + ρ2)σ22,0 ρ2(1 − ρ1)

16GC2τ

K

+ ρ2(1 − ρ1)(1 − ρ2) wkrk

k=0

(B.38)

Combining inequalities (B.35), (B.36), (B.37) and (B.38) we get

Kτ
w Eσ2

≤ 2τ (2 + ρ1) σ12,0 + 8τ C1 + 2GC2

k 1,k−t

ρ1

1 − ρ1 ρ1 ρ2(1 − ρ2)

K
wk rk

k=0 t=1

k=0

4Gτ (2 + ρ2)σ22,0 2τ D2 + ρ2(1 − ρ1) + ρ WK .

Next, we derive

(B.39)

Kτ
wk Eσ22,k−t
k=0 t=1

≤
(B.16)
≤

Kτ

K

2wk−tEσ22,k−t ≤ 2τ wkEσ22,k

k=0 t=1

k=0

K
2τ σ22,0 wk(1 − ρ1)k

k=0

+ 4C2τ K 1 − ρ2

k
wk(1 − ρ2)k−lrl.

k=0 l=0

(B.40)

263

For the ﬁrst term in the right-hand side of previous inequality we have

K
2τ σ22,0 wk(1 − ρ2)k
k=0

(B.33)
≤
(A.13)
≤
≤

K
2τ σ22,0
k=0

1 + ρ2 2

k+1
(1 − ρ2)k

2τ 1 + ρ22 σ22,0 K

1 − ρ2 k 2

k=0

∞
τ (2 + ρ2) σ22,0
k=0

1 − ρ2 2

k 2τ (2 + ρ2) σ22,0

≤

.

ρ2

The second term in the right-hand side of (B.40) can be upper bounded in the following way:

4C2τ K 1 − ρ2

k
wk(1 − ρ2)k−lrl

k=0 l=0

(B.33)
≤
(A.13)
≤
≤
≤

4C2τ K k

1 − ρ2

wlrl

k=0 l=0

1 + ρ2 2

k−l
(1 − ρ2)k−l

4C2τ K k

ρ2 k−l

1 − ρ2

wlrl 1 − 2

k=0 l=0

4C2τ 1 − ρ2

K
wk rk
k=0

∞ 1 − ρ2 k k=0 2

8C2τ K ρ2(1 − ρ2) wkrk,
k=0

hence

Kτ
wk Eσ22,k−t
k=0 t=1

(B.40)
≤

2τ (2 + ρ2) σ22,0

8C2τ K

ρ2

+ ρ2(1 − ρ2) wkrk.

k=0

(B.41)

Plugging inequalities (B.34), (B.39) and (B.41) in (B.31) we obtain

K
3L w E ek 2 ≤ 12γ2Lτ Aˆ +

2B1C1

+

2B2C2

+

4B1GC2

k

ρ1(1 − ρ1) ρ2(1 − ρ2) ρ2(1 − ρ1)(1 − ρ2)

K
wk rk

k=0

k=0

+ 6γ2LB1τ (2 + ρ1) σ2 + 6γ2τ L(2 + ρ2) 2B1G + B σ2

ρ1

0

ρ2

1 − ρ1

2 2,0

+3γ2τ L D1 + 2Bρ1D2 WK .

Taking into F2 = 6γρ22τ L

account that γ 2B11G−(2ρ+1 ρ2) + B2

≤
4

4Lτ

1 Aˆ+ 2B1C1 + 2B2C2 + 4B1GC2
ρ1(1−ρ1) ρ2(1−ρ2) ρ2(1−ρ1)(1−ρ2)

and D3 = 3γτ L

D1

+

2B1D2 ρ

we get

, F1 = 6γ2LB1ρτ1(2+ρ1) ,

3L K wkE ek 2 ≤ 41 K wkrk + F1σ12,0 + F2σ22,0 + γD3.

k=0

k=0

264

As a direct application of Lemma B.5.1 and Theorem 3.3.4 we get the following result.

Theorem B.5.2. Let Assumptions 3.3.1 and 4.2.2 be satisﬁed, inequalities (3.13), (3.14) and (3.15) hold and









γ ≤ min  1 , 1 , 1  ,

 4(A + C1M1 + C2M2) 2τ µ 8 Lτ Aˆ + ρ12(B11−Cρ11) + ρ22(B12−Cρ22) + ρ2(14−Bρ11G)(C12−ρ2) 

where M1 = 43Bρ11 and M2 = 4(B23+ρ234 G) . Then for all K ≥ 0 we have E f (x¯K ) − f (x∗) ≤ (1 − η)K 4(T 0 + γF1σγ12,0 + γF2σ22,0) + 4γ D1 + M D2 + D3
when µ > 0 and E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF1σγ1K2,0 + γF2σ22,0) + 4γ D1 + M D2 + D3

when µ = 0, where η = min {γµ/2, ρ1/4, ρ2/4}, T k d=ef x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

F1 = 6γ2LB1τ (2 + ρ1) , ρ1

6γ2τ L(2 + ρ2) F2 =
ρ2

2B1G + B , 1 − ρ1 2

D3 = 3γτ L D + 2B1D2 .

1

ρ1

B.6 Special Cases: Delayed Updates Methods

B.6.1 D-SGD
In this section we consider the same setup as in Section 3.8.2. We notice that vectors eki appear only in the analysis and there is no need to compute them. Moreover, we use ∇fi(x∗) in the deﬁnition of gik which is problematic at the ﬁrt glance. Indeed, workers do not know ∇fi(x∗). However, since 0 = ∇f (x∗) = n1 ∇fi(x∗) and master node uses averages of gik for the updates one can ignore ∇fi(x∗) in gik in the implementation of D-SGD and get exactly the same method. We deﬁne gik in such a way only for the theoretical analysis.

Lemma B.6.1 (see also Lemmas 1,2 from [156]). Assume that fξi(x) are convex in x for every ξi, i = 1, . . . , n. Then for every x ∈ Rd and i = 1, . . . , n

E gk 2 | xk ≤ 4L(f (xk) − f (x∗)) + n22 n Var [∇fξi(x∗)] .
i=1

(B.42)

265

Table B.1: Complexity of SGD methods with delayed updates established in this chapter.

Symbols: ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter

of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic

gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for

the variances of stochastic gradients of workers; M2,q = (ω + 1)σ2 + ωζ∗2; σq2 = (1 + ω)

1

+

ω n

σ2.

†D-QGDstar is a special case of D-QSGDstar where each worker i computes the full gradient

∇fi(xk); ‡D-GD-DIANA is a special case of D-SGD-DIANA where each worker i computes the full

gradient ∇fi(xk).

Problem (3.1)+(3.3) (3.1)+(3.2) (3.1)+(3.2) (3.1)+(3.2) (3.1)+(3.2) (3.1)+(3.2) (3.1)+(3.2) (3.1)+(3.3) (3.1)+(3.3) (3.1)+(3.3) (3.1)+(3.3)

Method D-SGDsr D-SGD D-QSGD D-QSGDstar D-QGDstar† D-SGD-DIANA D-GD-DIANA‡ D-LSVRG D-QLSVRG D-QLSVRGstar D-LSVRG-DIANA

Alg # Alg 43 Alg 39 Alg 40 Alg 41 Alg 41 Alg 42 Alg 42 Alg 44 Alg 45 Alg 46 Alg 47

Citation new [209] new new new new new new new new new

Sec # B.6.5 B.6.1 B.6.2 B.6.3 B.6.3 B.6.4 B.6.4 B.6.6 B.6.7 B.6.8 B.6.9

Rate (constants ignored)

√

√

O L+ L2τ 2+LLτ + σ∗2 +

Lτ σ∗2 √

µ

n√µε

µ nε

O τ κ + σ∗2 +

Lτ σ∗2 √

nµε

µ √nε

O κ τ + ω + M2,q +

Lτ M2,q √

n

nµε

µ nε

√

O κ τ + ω + σ2 + L√τσ2

n

nµε

µ nε

O κ τ + ωn log 1ε √

O ω + κ τ + ω + σ2 +

Lτ σq2 √

n

nµε

µ nε

O ω + κ τ + ωn log 1ε

O (m + κτ ) log 1ε √

O m + κ τ + ω + ζ∗2 +

Lτ ζ∗2 √

n

nµε

µ nε

O m + κ τ + ωn log 1ε O ω + m + κ τ + ωn log 1ε

If further f (x) is µ-quasi strongly convex with possibly non-convex fi, fξi and µ > 0, then for every x ∈ Rd and i = 1, . . . , n

E gk 2 | xk ≤ 4Lκ(f (xk) − f (x∗)) + n22 n Var [∇fξi(x∗)] ,
i=1

(B.43)

where κ = Lµ .

266

Algorithm 39 D-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi(xk) − ∇fi(x∗)

6:

vk = γgik−τ , if k ≥ τ,

i

0,

if k < τ

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

1 n

10: xk+1 = xk − vk

11: end for

n i=1

∇fξi

(xk−τ

)

Proof. By deﬁnition of gk we have

E gk 2 | xk

=
(A.11)
≤
(A.11)
≤



1n E n

∇fξi (xk) − ∇fξi (x∗) + ∇fξi (x∗) − ∇fi(x∗)

i=1

 2E 

1n n i=1

∇fξi (xk) − ∇fξi (x∗)

2 | xk



1n

∗

2
∗

+2 E  n (∇fξi(x ) − ∇fi(x )) 

i=1

n
Var n1 ∇fξi (x∗)
i=1

2n nE
i=1

∇fξi (xk) − ∇fξi (x∗) 2 | xk

2n + n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 , Var[∇fξi (x∗)]

2 | xk
(B.44)

where in the last inequality we use independence of ∇fξi(x∗), i = 1, . . . , n. Using this we derive inequality (B.42):

E gk 2 | xk

(B.44),(A.4)
≤
=
=

4nL n E Dfξi (xk, x∗) | xk + n22 n Var [∇fξi (x∗)]

i=1

i=1

4nL n Dfi (xk, x∗) + n22 n Var [∇fξi (x∗)]

i=1

i=1

4L f (xk) − f (x∗) + n22 n Var [∇fξi(x∗)] .
i=1

267

Next, if f (x) is µ-quasi strongly convex, but fi, fξi are not necessary convex, we obtain

E gk 2 | xk

(B.44),(A.1)
≤
(4.5)
≤

2L2 n n

xk − x∗ 2 + n22 n Var [∇fξi (x∗)]

i=1

i=1

4Lµ2 f (xk) − f (x∗) + n22 n Var [∇fξi(x∗)] .
i=1

Theorem B.6.2. Assume that fξ(x) is convex in x for every ξ. Then D-SGD satisﬁes Assumption 3.3.3 with

A = 2L,

B1 = B2 = 0,

D1 = n22 n Var [∇fξi(x∗)] ,
i=1

σ12,k ≡ σ22,k ≡ 0

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0

F1 = F2 = 0,

D3 = 6γnτ2L n Var [∇fξi(x∗)]
i=1

with γ satisfying and for all K ≥ 0

γ≤ 1 8L 2τ (τ + 2)

E f (x¯K ) − f (x∗) ≤

1 − γµ 2

K 4 x0 −γ x∗ 2 + 8nγ2 (1 + 3Lγτ ) n Var [∇fξi(x∗)]
i=1

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + 8nγ2 (1 + 3Lγτ ) n Var [∇fξi(x∗)]
i=1

when µ = 0. If further fi(x) are µ-strongly convex with possibly non-convex fξi and µ > 0, then D-SGD satisﬁes Assumption 3.3.3 with

A = 2κL,

B1 = B2 = 0,

D1 = n22 n Var [∇fξi(x∗)] ,
i=1

σ12,k ≡ σ22,k ≡ 0,

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0,

F1 = F2 = 0,

D3 = 6γnτ2L n Var [∇fξi(x∗)]
i=1

with γ satisfying

γ ≤ min 1 , 1 8κL 8L 2τ (τ + 2κ)

268

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − γµ 2

K 4 x0 −γ x∗ 2 + 8nγ2 (1 + 3Lγτ ) n Var [∇fξi(x∗)] .
i=1

In other words, D-SGD converges with linear rate O

τ

κ

ln

1 ε

to the neighbourhood of the solution

when µ > 0. Applying Lemma A.5.3 we establish the rate of convergence to ε-solution.

Corollary B.6.3. Let the assumptions of Theorem B.6.2 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of D-SGD with the stepsize

γ = min  1 , ln max 2, min x0−xD∗ 12µ2K2 , x0−3xτ∗LD2µ13K3 

 8L 2τ (τ + 2)

µK







we have

E f (x¯K ) − f (x∗) = O

Lτ x0 − x∗ 2 exp

µ −K

+ D1 + Lτ D1

.

τL

µK µ2K2

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGD requires

 O  τ L + D1 +
µ µε

 Lτ D1 µ√ε  iterations.

Corollary B.6.4. Let the assumptions of Theorem B.6.2 hold and f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi. Then after K iterations of D-SGD with the stepsize



 

1

1

ln max 2, min x0−xD∗ 2µ2K2 , x0−Lxτ∗D2µ3K3 

γ = min

,

,

1

1

 8κL 8L 2τ (τ + 2κ)

µK







we have E f (x¯K) − f (x∗) of order

√ O L κ+τ κ

x0 − x∗ 2 exp − min

µ1 √,

K + D1 + Lτ D1 .

τ L κ κ2

µK µ2K2

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGD requires

 O κ2 + τ κ3/2 + D1 +
µε

 Lτ D1 µ√ε  iterations.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

269

Corollary B.6.5. Let the assumptions of Theorem B.6.2 hold, fξ(x) are convex for each ξ and µ = 0. Then after K iterations of D-SGD with the stepsize

γ = min

1,

8L 2τ (τ + 2)

x0 − x∗ 2 , 3 x0 − x∗ 2

D1K

3Lτ D1K

we have E f (x¯K) − f (x∗) of order

 O  τ LR02 +
K

R2τ D 3 LR04τ D1 

0 1+ K

K2/3 

where R0 = x0 − x∗ . That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGD requires

 τ LR2 R2D

R02

 Lτ D1

O  ε 0 + 0ε2 1 + ε3/2 

iterations.

B.6.2 D-QSGD

In this section we show how one can combine delayed updates with quantization using our scheme.

Algorithm 40 D-QSGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n do

5:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ | xk−τ ] = ∇fi(xk−τ )

and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

6:

gik−τ = Q(gˆik−τ ) − ∇fi(x∗) (quantization is performed independently from other

nodes)

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9: end for

10:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

γ n

n i=1

gik−τ

=

γ n

n i=1

Q(gˆik−τ

)

11: xk+1 = xk − vk

12: end for

Lemma B.6.6. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all

270

k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), ≤ 2L 1 + 2ω
n

where

D

=

1 n

n i=1

Di.

f (xk) − f (x∗) + (ω + 1)D + 2ω n

n

n2

∇fi(x∗) 2

i=1

Proof. First of all, we show unbiasedness of gk:

E gk | xk

= (A=.6)

1

n
E

gk | xk

=1

n
E

E

Q(gˆk) − ∇f (x∗) | xk

n

i

n

Q

i

i

i=1

i=1

n1 n ∇fi(xk) − ∇fi(x∗) = ∇f (xk),
i=1

where EQ [·] denotes mathematical expectation w.r.t. the randomness coming only from the quantization. Next, we derive the upper bound for the second moment of gk:

EQ gk 2



1n

k

2
∗

= EQ  n Q(gˆi ) − ∇fi(x ) 

i=1

(A.14)

 1n

k

2
k

1n k

2 ∗

= EQ  n Q(gˆi ) − gˆi  + n gˆi − ∇fi(x ) . (B.45)

i=1

i=1

Since Q(gˆ1k), . . . , Q(gˆnk) are independent quantizations, we get

EQ gk 2

(B.45) 1 n

k

k2

1n k

2 ∗

≤ n2 EQ Q(gˆi ) − gˆi + n gˆi − ∇fi(x )

i=1

i=1

(A.6) ω n k 2 1 n k

2 ∗

≤ n2 gˆi + n gˆi − ∇fi(x ) .

i=1

i=1

Taking conditional expectation E · | xk from the both sides of the previous inequality we obtain

E gk 2 | xk

≤
(A.14)
≤

ωn n2 E
i=1



gˆk 2 | xk + E 1 n gˆk − ∇f (x∗)

i

 n

i

i

i=1

2 | xk

ωn n2

∇fi(xk) 2 + nω2 n E

i=1

i=1

gˆik − ∇fi(xk) 2 | xk

+ 1n n

∇fi(xk) − ∇fi(x∗)

i=1

2

 1n k

k

+E  n gˆi − ∇fi(x )

i=1

∇f (xk)−∇f (x∗) 2

2 | xk .

271

It remains to estimate terms in the second and the third lines of the previous inequality:

ωn n2

∇fi(xk) 2

i=1

ωn nE
i=1

gˆik − ∇fi(xk) 2 | xk

∇f (xk) − ∇f (x∗) 2



E 1 n gˆk − ∇f (xk)

 n

i

i

i=1

2 | xk

(A.11)
≤
(A.4)
≤
≤
(A.4)
≤
=
≤

2ω n n2

∇fi(xk) − ∇fi(x∗) 2 + 2nω2 n

∇fi(x∗) 2

i=1

i=1

4ωL f (xk) − f (x∗) + 2ω n

n

n2

∇fi(x∗) 2,

i=1

ωn

ωD

n2 Di = n ,

i=1

2L f (xk) − f (x∗) ,

1n n2 E
i=1

gˆik − ∇fi(xk) 2 | xk

1n

D

n2 Di = n .

i=1

Putting all together we get

E gk 2 | xk

≤ 2L 1 + 2ω n

f (xk) − f (x∗) + (ω + 1)D + 2ω n

n

n2

∇fi(x∗) 2.

i=1

Theorem B.6.7. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then D-QSGD satisﬁes Assumption 3.3.3 with

A = L 1 + 2nω , B1 = B2 = 0, D1 = (ω +n1)D + 2nω2 n ∇fi(x∗) 2,
i=1

σ12,k ≡ σ22,k ≡ 0, ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0

F1 = F2 = 0,

G = 0,

D3 = 3γτ L n

(ω + 1)D + 2ω n n

∇fi(x∗) 2

i=1

with γ satisfying and for all K ≥ 0

γ ≤ min 1 , 1 4L(1 + 2ω/n) 8L 2τ (τ + 1 + 2ω/n)

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + γ D1 + D3

when µ > 0 and when µ = 0.

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + γ D1 + D3

272

In other words, D-QSGD converges with the linear rate

O κ 1 + ω + κ τ τ + ω ln 1

n

n

ε

to the neighbourhood of the solution when µ > 0. Applying Lemma A.5.3 we establish the rate of convergence to ε-solution.
Corollary B.6.8. Let the assumptions of Theorem B.6.7 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of D-QSGD with the stepsize

γ0 = min 1 , 1

, R0 = x0 − x∗ ,

4L(1 + 2ω/n) 8L 2τ (τ + 1 + 2ω/n)

 

ln max 2, min R02µ2K2 , R02µ3K3

 

 = min γ0,

D1

3τ LD1



γ 

µK







we have E f (x¯K) − f (x∗) of order

 O LR02 1 + ωn + τ τ + ωn


exp −  L

µ

1

+

ω n

+

τ

τ

+

ω n





K + D1 + Lτ D1  .  µK µ2K2 

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QSGD requires





O  L 1 + ω + L τ τ + ω + D1 + L√τ D1  iterations.

µ

nµ

n µε µ ε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.
Corollary B.6.9. Let the assumptions of Theorem B.6.7 hold and µ = 0. Then after K iterations of D-QSGD with the stepsize

1

1

γ0 = min 4L(1 + 2ω/n) , 8L 2τ (τ + 1 + 2ω/n) ,

γ = min γ0,

x0 − x∗ 2 , 3 x0 − x∗ 2

D1K

3Lτ D1K

we have E f (x¯K) − f (x∗) of order

O  LR02 1 + ωn + LR02 τ τ + ωn +

K

K

R2D 3 LR04τ D1 

0 1+ K

K2/3 

273

where R0 = x0 − x∗ . That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QSGD requires

 O  LR02 1 + ωn
ε

LR02 +

τ

τ

+

ω n

ε

R2D

R02

 Lτ D1

+ 0ε2 1 + ε3/2 

iterations.

B.6.3 D-QSGDstar

As we saw in Section B.6.2 D-QSGD fails to converge to the exact optimum asymptotically even if gˆik = ∇fi(xk) for all i = 1, . . . , n almost surely, i.e., all Di = 0 for all i = 1, . . . , n. As for EC-GDstar we assume now that i-th worker has an access to ∇fi(x∗). Using this one can construct the method with delayed updates that converges asymptotically to the exact solution
when the full gradients are available.

Algorithm 41 D-QSGDstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n do

5:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ | xk−τ ] = ∇fi(xk−τ )

and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

6:

gik−τ = Q(gˆik−τ − ∇fi(x∗)) (quantization is performed independently from other

nodes)

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9: end for

10:

ek

=

1 n

∇fi(x∗))

n i=1

eki

,

gk

=

1 n

11: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

γ n

n i=1

gik−τ

=

γ n

n i=1

Q(gˆik−τ

−

12: end for

Lemma B.6.10. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

where

D

=

1 n

n i=1

Di.

= ∇f (xk), ≤ 2L 1 + ω
n

f (xk) − f (x∗) + (ω + 1)D n

(B.46) (B.47)

274

Proof. First of all, we show unbiasedness of gk:

E gk | xk

= (A=.6)

1

n
E

gk | xk

=1

n
E

E

Q(gˆk − ∇f (x∗)) | xk

n

i

n

Q

i

i

i=1

i=1

n1 n ∇fi(xk) − ∇fi(x∗) = ∇f (xk),
i=1

where EQ [·] denotes mathematical expectation w.r.t. the randomness coming only from the quantization. Next, we derive the upper bound for the second moment of gk:

EQ gk 2



1n

k

2
∗

= EQ  n Q gˆi − ∇fi(x ) 

i=1



(A.14)

1n

k

∗

k

2
∗

= EQ  n Q gˆi − ∇fi(x ) − gˆi − ∇fi(x ) 

i=1

1n k

2 ∗

+ n gˆi − ∇fi(x ) .

i=1

(B.48)

Since Q gˆ1k − ∇f1(x∗) , . . . , Q gˆnk − ∇fn(x∗) are independent quantizations, we get

EQ gk 2

(B.48)
≤

1n E

Q gˆk − ∇f (x∗) − gˆk − ∇f (x∗) 2

n2

Q

i

i

i

i

i=1

1n k

2 ∗

+ n gˆi − ∇fi(x )

i=1

(A.6) ω n k

∗ 2 1n k

2 ∗

≤ n2 gˆi − ∇fi(x ) + n gˆi − ∇fi(x ) .

i=1

i=1

Taking conditional expectation E · | xk from the both sides of the previous inequality we obtain

E gk 2 | xk

≤
(A.14)
≤

ωn n2 E
i=1



gˆk − ∇f (x∗) 2 | xk + E 1 n gˆk − ∇f (x∗)

i

i

 n

i

i

i=1

2 | xk

ωn n2

∇fi(xk) − ∇fi(x∗) 2 + nω2 n E

i=1

i=1

gˆik − ∇fi(xk) 2 | xk

+ 1n n

∇fi(xk) − ∇fi(x∗)

i=1

2

 1n k

k

+E  n gˆi − ∇fi(x )

i=1

2 | xk .

∇f (xk)−∇f (x∗) 2

275

It remains to estimate terms in the second and the third lines of the previous inequality:

ωn
n2 i=1 ωn nE
i=1

∇fi(xk) − ∇fi(x∗) 2 gˆik − ∇fi(xk) 2 | xk

(A.4)
≤
≤

2ωL f (xk) − f (x∗) , n

ωn

ωD

n2 Di = n ,

i=1

(A.4)
∇f (xk) − ∇f (x∗) 2 ≤ 2L f (xk) − f (x∗) ,

 1n k

2

k

k

1n

k

k2 k

E  n gˆi − ∇fi(x ) | x  = n2 E gˆi − ∇fi(x ) | x

i=1

i=1

1n

D

≤ n2 Di = n .

i=1

Putting all together we get

E gk 2 | xk

≤ 2L 1 + ω n

f (xk) − f (x∗) + (ω + 1)D . n

Theorem B.6.11. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then D-QSGDstar satisﬁes Assumption 3.3.3 with

A =L

1+ ω , n

B1 = B2 = 0,

(ω + 1)D

D1 =

, n

σ12,k ≡ σ22,k ≡ 0,

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0,

F1 = F2 = 0, D3 = 3γτ L(ω + 1)D n

with γ satisfying and for all K ≥ 0

γ ≤ min 1 , 1 . 4L(1 + ω/n) 8L τ (τ + 1 + ω/n)

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + D3

when µ > 0 and when µ = 0.

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + 4γ D1 + D3

In other words, D-QSGDstar converges with the linear rate

O τ + κ 1 + ω + κ τ τ + ω ln 1

n

n

ε

276

to the exact solution when µ > 0 and D = 0, i.e., gˆik = ∇fi(xk) for all i = 1, . . . , n almost surely. Applying Lemma A.5.3 we establish the rate of convergence to ε-solution.

Corollary B.6.12. Let the assumptions of Theorem B.6.11 hold and µ > 0. Then after K iterations of D-QSGDstar with the stepsize

γ0 = min 1 , 1

, R0 = x0 − x∗ ,

4L(1 + ω/n) 8L τ (τ + 1 + ω/n)

 

ln max 2, min nR02µ2K2 , nR02µ3K3

 

 = min γ0,

D

3τ LD



γ 

µK







we have E f (x¯K) − f (x∗) of order

 O LR02 1 + ωn + τ τ + ωn


exp −  L

µ

1

+

ω n

+

τ

τ

+

ω n





D

Lτ D

K 

+

nµK

+

nµ

2

K

2

 

.

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QSGDstar requires

√

L

ωL

ω

D

Lτ D

O

1+ + τ τ + + + √

µ

nµ

n nµε µ nε

iterations.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary B.6.13. Let the assumptions of Theorem B.6.11 hold and µ = 0. Then after K iterations of D-QSGDstar with the stepsize

1

1

γ0 = min 4L(1 + 2ω/n) , 8L τ (τ + 1 + ω/n) ,


 γ = min γ0,




n x0 − x∗ 2 3 n x0 − x∗ 2 

, DK

3Lτ DK 

we have E f (x¯K) − f (x∗) of order

O  LR02 1 + ωn + LR02 τ τ + ωn +

K

K

R2D 3 LR04τ D  n0K + n1/3K2/3 

where R0 = x0 − x∗ . That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QSGDstar requires

 O  LR02 1 + ωn
ε

LR02 +

τ

τ

+

ω n

ε

√ + Rn02εD2 + R√02 nLε3τ/2D 

277

iterations.

B.6.4 D-SGD-DIANA

In this section we present a practical version of D-QSGDstar: D-SGD-DIANA.

Algorithm 42 D-SGD-DIANA

Input:

learning

rates

γ

> 0, α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2:

Set

h0

=

1 n

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk−τ to all workers

5: for i = 1, . . . , n do

6:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ | xk−τ ] = ∇fi(xk−τ )

and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

7:

∆ˆ ki −τ = Q(gˆik−τ − hki −τ ) (quantization is performed independently from other nodes)

8:

gik−τ = hki −τ + ∆ˆ ik−τ

9:

vik = γgik−τ

10:

eki +1 = eki + γgik − vik

11:

hki −τ +1 = hki −τ + α∆ˆ ik−τ

12: end for

13:

hk−τ

=

1 n

n i=1

hki −τ

,

ek

=

1 n

n i=1

eki

,

gk

=

1 n

γhk−τ

+

γ n

n i=1

∆ˆ ki −τ

14: xk+1 = xk − vk

15:

hk−τ +1

=

hk−τ

+

α n

n i=1

∆ˆ ki −τ

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

γ n

n i=1

gik−τ

=

16: end for

Lemma B.6.14 (Lemmas 1 and 2 from [79]). Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk E σk2+1 | xk

= ∇f (xk),

≤ 2L 1 + 2ω n

f (xk) − f (x∗) + 2ωσk2 + (ω + 1)D

n

n

≤ (1 − α)σk2 + 2Lα f (xk) − f (x∗) + αD

(B.49) (B.50) (B.51)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D=

1 n

n i=1

Di.

Theorem B.6.15. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is

278

µ-quasi strongly convex. Then D-SGD-DIANA satisﬁes Assumption 3.3.3 with

A = L 1 + 2nω , B1 = 2nω , D1 = (ω +n1)D , σ12,k = σk2 = n1 n hki − ∇fi(x∗) 2,
i=1

α(ω + 1)D

B2 = 0, ρ1 = α, ρ2 = 1, C1 = Lα, C2 = 0, D2 =

n , G = 0,

12γ2Lωτ (2 + α)

4ω (ω + 1)D

F1 =

, F2 = 0, D3 = 3γτ L 1 +

nα

n

n

with γ and α satisfying

1

1

1

8ω

γ ≤ min

, 4L(1 + 14ω/3n) 8L 2τ (1 + τ + 2ω/n + 4ω/n(1−α))

,

α ≤ ω + 1,

M1 = 3nα

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4(T 0 +γγF1σ02) + 4γ D1 + M1D2 + D3

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 +γKγF1σ02) + 4γ D1 + M1D2 + D3

when µ = 0, where T k d=ef x˜k − x∗ 2 + M1γ2σk2. In other words, if

γ ≤ min 1 , 1

, α ≤ min 1 , 1

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)

ω+1 2

then D-SGD-DIANA converges with the linear rate

O ω + κ 1 + ω + κ τ τ + ω ln 1

n

n

ε

to the exact solution when µ > 0. Applying Lemma A.5.3 we establish the rate of convergence to ε-solution.
Corollary B.6.16. Let the assumptions of Theorem B.6.15 hold and µ > 0. Then after K

279

iterations of D-SGD-DIANA with the stepsize

γ0 = min 1 , 1

, R0 = x0 − x∗ ,

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)

F˜1 =

12Lωτ (2 + α)γ02 , nα

T˜0 = R02 + M1γ02σ02,

  

 

  

ln

max

 2,

min

 (T˜0+γ0F˜1σ02)µ2K2

,

(T˜0 +γ0 F˜1 σ02 )µ3 K 3



  

 

D1+M1D2

3τ L D + 2B1D2

 
 









1

α



γ = min γ0, µK 

























and α ≤ min ω+1 1 , 12 we have E f (x¯K ) − f (x∗) of order

 O LR02

1+ ω + n





τ τ+ω n

exp − min  µ   L 1 + ωn + τ τ + ωn

 D1 + M1D2

τL

D1

+

B1 D2 α



+O  µK +

µ2 K 2

.

  , 1  K
1 + ω   

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGD-DIANA requires



O ω + L 1 + ω + L τ τ + ω + (ω + 1) 1 + ωn D +

µ

nµ

n

nµε

Lτ (ω + 1)

1

+

ω n

 D

√



µ nε

iterations. Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary B.6.17. Let the assumptions of Theorem B.6.15 hold and µ = 0. Then after K iterations of D-SGD-DIANA with the stepsize

γ0 = min 1 , 1

, R0 = x0 − x∗ ,

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)





  γ = min γ0,
 

R02 , 3

R02nα

,

M1σ02 12Lωτ (2 + α)σ02

R02

,3

(D1 + M1D2)K 3τ L

R02

D1

+

2B1D2 α

 
K 

280

Algorithm 43 D-SGDsr

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik−τ = ∇fξi (xk−τ ) − ∇fi(x∗)

6:

vik = γgik−τ

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

1 n

10: xk+1 = xk − vk

11: end for

we have E f (x¯K) − f (x∗) of order

n i=1

∇fξi

(xk−τ

)

 O  L 1 + ωn R02 + L
K

τ

τ

+

ω n

R02

K+

R02ω(1 + ω)σ02

3

R04Lτ ω(1

+

ω

)

σ

2 0



√

+

√



nK

3 nK

 +O 

(1 + ω) 1 + ωn R02D + 3 R04τ L(1 + ω) 1 + ωn D  .

nK

n1/3K 2/3

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGD-DIANA requires

 O  L 1 + ωn R02 + L
ε

τ

τ

+

ω n

R02

ε+

R02ω(1 + ω)σ02

3

R04Lτ ω(1

+

ω

)

σ

2 0



√

+

√



nε

3 nε



(1 + ω)

1

+

ω n

R02D

R02

+O 

nε2

+

τ L(1 + ω)

1

+

ω n

 D

n1/2ε3/2



iterations.

B.6.5 D-SGDsr
In this section we consider the same settings as in Section 3.8.1, but this time we consider delayed updates. Moreover, in this section we need slightly weaker assumption.

Assumption B.6.18 (Expected smoothness). We assume that function f is L-smooth in expectation w.r.t. distribution D, i.e., there exists constant L = L(f, D) such that

ED ∇fξ(x) − ∇fξ(x∗) 2 ≤ 2L (f (x) − f (x∗))

(B.52)

for all i ∈ [n] and x ∈ Rd.

281

Lemma B.6.19. For all k ≥ 0 we have E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2ED ∇fξ(x∗) 2 .

(B.53)

Proof. Applying straightforward inequality a + b 2 ≤ 2 a 2 + 2 b 2 for a, b ∈ Rd we get

E gk 2 | xk

=
(A.11)
≤
(B.52)
≤



1n E n

∇fξi (xk) − ∇fi(x∗)

i=1

2 | xk

2ED ∇fξ(xk) − ∇fξ(x∗) 2 + 2ED ∇fξ(x∗) − ∇f (x∗) 2

4L f (xk) − f (x∗) + 2ED ∇fξ(x∗) 2 .

Theorem B.6.20. Assume that f (x) is µ-quasi strongly convex, L-smooth and Assumption B.6.18 holds. Then D-SGDsr satisﬁes Assumption 3.3.3 with

A = 2L, B1 = B2 = 0, D1 = 2ED ∇fξ(x∗) 2, σ12,k ≡ σ22,k ≡ 0 ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0, F1 = F2 = 0, D3 = 6γτ LED ∇fξ(x∗) 2

with γ satisfying and for all K ≥ 0

γ ≤ min 1 , 1 8L 8 Lτ (Lτ + 2L)

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 8γ(1 + 3γτ L)ED ∇fξ(x∗) 2

when µ > 0 and

E

f (x¯K ) − f (x∗)

4 x0 − x∗ ≤

2
+ 8γ(1 + 3γτ L)ED

∇fξ(x∗) 2

γK

when µ = 0.

In other words, D-SGDsr converges with linear rate O

√
Lµ + LLτµ+L2τ2 ln 1ε to the neighbour-

hood of the solution when µ > 0. Applying Lemma A.5.3 we establish the rate of convergence

to ε-solution.

Corollary B.6.21. Let the assumptions of Theorem B.6.20 hold and µ > 0. Then after K

282

iterations of D-SGDsr with the stepsize

γ0 = min 1 , 1

, R0 = x0 − x∗ ,

8L 8 Lτ (Lτ + 2L)

 

ln max 2, min R02µ2K2 , R02µ3K3

 

 = min γ0,

D1

3τ LD1



γ 

µK







we have E f (x¯K) − f (x∗) of order

O R02 L + L2τ 2 + LLτ exp − τµL K + ED ∇µfKξ(x∗) 2 + Lτ EDµ∇2Kfξ2(x∗) 2 .

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGDsr requires

√ L + L2τ 2 + LLτ

ED ∇fξ(x∗) 2

O

+

+

µ

µε

Lτ ED ∇fξ(x∗) 2  µ√ε  iterations.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.
Corollary B.6.22. Let the assumptions of Theorem B.6.20 hold and µ = 0. Then after K iterations of D-SGDsr with the stepsize

γ = min 1 , 1 , 8L 8 Lτ (Lτ + 2L)

x0 − x∗ 2 , 3 x0 − x∗ 2

D1K

3Lτ D1K

we have E f (x¯K) − f (x∗) of order



√

O  LR02 + L2τ 2 + LLτ R02 +

K

K

R02τ ED ∇fξ(x∗) 2 3 LR04τ ED ∇fξ(x∗) 2 

+



K

K 2/3

where R0 = x0 − x∗ . That is, to achive E f (x¯K) − f (x∗) ≤ ε D-SGDsr requires

 LR2

√ L2τ 2 + LLτ R2

R2ED ∇fξ(x∗) 2

R02

O  ε 0 + ε 0 + 0 ε2 +

Lτ ED ∇fξ(x∗) 2 

ε3/2



iterations.

B.6.6 D-LSVRG
In the same settings as in Section 3.8.6 we now consider a new method called D-LSVRG which is another modiﬁcation of LSVRG that works with delayed updates.

283

Algorithm 44 D-LSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

10: end for

11:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

Lemma B.6.23. For all k ≥ 0, i ∈ [n] we have

E gik | xk = ∇fi(xk)

and E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

Proof. First of all, we derive unbiasedness of gik: E gik | xk = m1 m ∇fij(xk) − ∇fij(wik) + ∇fi(wik) = ∇fi(xk).
j=1

(B.54) (B.55)

284

Next, we estimate the second moment of gk:

E gk 2 | xk

=
=
(A.11)
≤
(A.14)
≤
(A.4)
≤ =



1n

k

2

k

k

E  n ∇fil(x ) − ∇fil(wi ) + ∇fi(wi ) 

i=1



1n

k

2

k

k

∗

E  n ∇fil(x ) − ∇fil(wi ) + ∇fi(wi ) − ∇fi(x ) 

i=1

2n nE
i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+2 n E n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

2 | xk

n2m n m ∇fij(xk) − ∇fij(x∗) 2 + n2 E
i=1 j=1

∇fil(wik) − ∇fil(x∗) 2 | xk

n4mL n m Dfij (xk, x∗) + n2m n m ∇fij(wik) − ∇fij(x∗) 2

i=1 j=1

i=1 j=1

4L f (xk) − f (x∗) + 2σk2.

Lemma B.6.24. For all k ≥ 0, i ∈ [n] we have

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(B.56)

Proof. The proof is identical to the proof of Lemma 3.8.22.

Theorem B.6.25. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then D-LSVRG satisﬁes Assumption 3.3.3 with

A = 2L,

B1 = 0,

B2 = 2,

D1 = 0,

σ22,k = σk2 = n1m n m ∇fij (wik) − ∇fij (x∗) 2,
i=1 j=1

σ12,k ≡ 0, ρ1 = 1, ρ2 = p, C1 = 0, C2 = Lp, D2 = 0,

G = 0,

F1 = 0,

12γ2Lτ (2 + p)

F2 =

,

p

D3 = 0

with γ satisfying

3

1

8

γ ≤ min

, 56L 8L τ (2 + τ + 4/(1−p))

,

M2 = 3p

285

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK

when µ = 0, where T k d=ef x˜k − x∗ 2 + M2γ2σk2.

In other words, D-LSVRG converges with linear rate O

1 p

+

κ

τ

τ

+

1 (1−p)

ln

1 ε

to the

exact solution when µ > 0.

If

m

≥

2

then

taking

p

=

1 m

we

get

that

in

expectation

the

sample complexity of one iteration of D-LSVRG is O(1) gradients calculations per node as for

D-SGDsr with standard sampling and the rate of convergence to the exact solution becomes

O

(m

+

κτ

)

ln

1 ε

.

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary B.6.26. Let the assumptions of Theorem B.6.25 hold and µ = 0. Then after K iterations of D-LSVRG with the stepsize

γ = min 3 , 1 , 56L 8L τ (2 + τ + 4/(1−p))

x0 − x∗ 2

x0 − x∗ 2p

M2σ2 , 3 12Lτ (2 + p)σ2

0

0

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 O  Lτ R02 +
K

R02mσ02

3

R04

Lτ

σ

2 0



K+ K

where R0 = x0 − x∗ . That is, to achive E f (x¯K) − f (x∗) ≤ ε D-LSVRG requires

 O  Lτ R02 +
ε

R02mσ02

3

R04

Lτ

σ

2 0



ε+

 ε

iterations.

B.6.7 D-QLSVRG
In this section we add a quantization to D-LSVRG. Lemma B.6.27. For all k ≥ 0, i ∈ [n] we have
E gik | xk = ∇fi(xk)

286

Algorithm 45 D-QLSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

Set gik−τ = Q(gˆik−τ ) (quantization is performed independently

8:

vik = γgik−τ

9:

eki +1 = eki + γgik − vik

10:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

11: end for

12:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

13: xk+1 = xk − vk

14: end for

from

other

nodes)

and E gk 2 | xk ≤ 4L 1 + 2ω n

f (xk) − f (x∗) + 2

1 + 2ω n

σk2 + 2nω2 n ∇fi(x∗) 2,
i=1

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

Proof. First of all, we derive unbiasedness of gik:

E gik | xk

(A=.15) =

E EQ 1m m j=1

Q(gˆik) | xk (A=.6) E gˆik | xk ∇fij(xk) − ∇fij(wik) + ∇fi(wik)

= ∇fi(xk).

Next, we estimate the second moment of gk:

EQ gk 2

= (A=.14)

 EQ 
 EQ 

1n

2
k

n Q(gˆi ) 

i=1

1 n Q(gˆk) − gˆk

n

i

i

i=1

2 +

1n

2 k

n gˆi .

i=1

Since quantization on nodes is performed independently we can decompose the ﬁrst term from

287

the last row of the previous inequality into the sum of variances:

EQ gk 2

=
(A.6)
≤
(A.11)
≤

1n n2 EQ
i=1

ω n gˆk

n2

i

i=1

1 + 2ω n

Q(gˆk) − gˆk

2
+

i

i

1n

2 k

n gˆi

i=1

2 1n k

2 ∗

+ n gˆi − ∇fi(x )

i=1

1 n gˆk − ∇f (x∗) 2 + 2ω n

n

i

i

n2

i=1

i=1

∇fi(x∗) 2.

Taking conditional mathematical expectation E · | xk from the both sides of previous inequality we get

E gk 2 | xk

≤
≤
(A.4)
≤ =

1 + 2ω n

2n nE
i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

+ 1 + 2ω n

2n nE
i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

2 | xk

2ω n + n2

∇fi(x∗) 2

i=1

1 + 2ω n

n2m n m ∇fij(xk) − ∇fij(x∗) 2
i=1 j=1

+ 1 + 2ω n

2n nE
i=1

∇fil(wik) − ∇fil(x∗) 2 | xk + 2nω2 n ∇fi(x∗) 2
i=1

1 + 2ω n

4L n nm

m
Dfij (xk, x∗)

i=1 j=1

+ 1 + 2ω n

n2m n m ∇fij(wik) − ∇fij(x∗) 2 + 2nω2 n ∇fi(x∗) 2

i=1 j=1

i=1

4L 1 + 2ω n

f (xk) − f (x∗) + 2

1 + 2ω n

σk2 + 2nω2 n ∇fi(x∗) 2.
i=1

Lemma B.6.28. For all k ≥ 0, i ∈ [n] we have

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(B.57)

Proof. The proof is identical to the proof of Lemma 3.8.22.

Theorem B.6.29. Assume that f (x) is µ-quasi strongly convex and functions fij are convex

288

and L-smooth for all i ∈ [n], j ∈ [m]. Then D-QLSVRG satisﬁes Assumption 3.3.3 with

2ω

2ω

A = 2L 1 + n , B1 = 0, B2 = 2 1 + n

σ22,k = σk2 = n1m n m ∇fij (wik) − ∇fij (x∗) 2,
i=1 j=1

, D1 = 2nω2 n ∇fi(x∗) 2,
i=1
ρ1 = 1, ρ2 = p, C2 = Lp,

σ12,0 ≡ 0, D2 = 0,

C1 = 0,

G = 0,

F1 = 0,

12γ2Lτ

1

+

2ω n

(2 + p)

F2 =

, p

6γτ Lω n D3 = n2

∇fi(x∗) 2

i=1

with γ satisfying

3

1

8

1

+

2ω n

γ ≤ min

, 56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

,

M2 =

3p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 +γγF2σ02) + 4γ D1 + D3

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 +γKγF2σ02) + 4γ D1 + D3

when µ = 0, where T k d=ef x˜k − x∗ 2 + M2γ2σk2. In other words, D-QLSVRG converges with linear rate

O 1 + κ 1 + ω + κ τ τ + 1 + ω 1 + 1 ln 1

p

n

n

(1 − p)

ε

to

neighbourhood

the

solution

when

µ

>

0.

If

m

≥

2

then

taking

p

=

1 m

we

get

that

in

expectation

the sample complexity of one iteration of D-QLSVRG is O(1) gradients calculations per node as

for D-QSGDsr with standard sampling and the rate of convergence to the neighbourhood of the

solution becomes

O m + κ 1 + ω + κ τ τ + ω ln 1 .

n

n

ε

Applying Lemma A.5.3 we establish the rate of convergence to ε-solution.

Corollary B.6.30. Let the assumptions of Theorem B.6.29 hold, fξ(x) are convex for each ξ

289

and µ > 0. Then after K iterations of D-QLSVRG with the stepsize

γ0 = min 3 , 1

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

 

ln max 2, min R02µ2K2 , R02µ3K3

 

 = min γ0,

D1

3τ LD1



γ 

µK







and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 O LR02 1 + ωn + τ τ + ωn


exp −  L

µ

1

+

ω n

+

τ

τ

+

ω n





K + D1 + Lτ D1  .  µK µ2K2 

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QLSVRG requires





O  L 1 + ω + L τ τ + ω + D1 + L√τ D1  iterations.

µ

nµ

n µε µ ε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary B.6.31. Let the assumptions of Theorem B.6.29 hold and µ = 0. Then after K iterations of D-QLSVRG with the stepsize

γ0 = min 3 , 1

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))


 γ = min γ0,


R02 , 3 M2σ02 12Lτ

R02p

1

+

2ω n

, (2 + p)

 R02 , 3 R02  D1K 3Lτ D1K 

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 LR02
O 

1

+

ω n

+

K

τ

τ

+

ω n



R02m

1

+

ω n

σ02

3 R04Lτ m

1

+

ω n

+

+



K

K



 +O 

R2D 3 LR04τ D1 

0 1+ K

K2/3  .

290

Algorithm 46 D-QLSVRGstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

Set gik−τ = Q(gˆik−τ − ∇fi(x∗)) (quantization is performed

nodes)

8:

vik = γgik−τ

9:

eki +1 = eki + γgik − vik

10:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

11: end for

12:

ek

=

1 n

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

13: xk+1 = xk − vk

14: end for

independently

from

other

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QLSVRG requires



LR02

1

+

ω n

+

O



ε

τ

τ

+

ω n



R02m

1

+

ω n

σ02

3 R04Lτ m

1

+

ω n

+

+



ε

ε



 R2D

R02

 Lτ D1

+O  0ε2 1 + ε3/2 

iterations.

B.6.8 D-QLSVRGstar
Now we assume that i-th node has an access to ∇fi(x∗) and modify D-QLSVRG in order to get convergence asymptotically to the exact optimum.
Lemma B.6.32. For all k ≥ 0, i ∈ [n] we have

E gk | xk = ∇f (xk)

(B.58)

and

E gk 2 | xk ≤ 2L 1 + ω n

f (xk) − f (x∗) + 2 1 + ωn σk2,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(B.59)

291

Proof. First of all, we derive unbiasedness of gik:

E gk | xk

(A=.15) E E

1

n
Q(gˆk − ∇f (x∗))

| xk

(A=.6) E

1

n

gˆk − ∇f (x∗) | xk

Qn

i

i

n

i

i

i=1

i=1

= n1m n m ∇fij(xk) − ∇fij(wik) + ∇fi(wik) = ∇f (xk).
i=1 j=1

Next, we estimate the second moment of gk:

EQ gk 2

= (A=.14)

 EQ 
 EQ 

1n

k

2
∗

n Q(gˆi − ∇fi(x )) 

i=1

1 n Q(gˆk − ∇f (x∗)) −

n

i

i

i=1

gˆik − ∇fi(x∗)

2 +

1n k

2 ∗

n gˆi − ∇fi(x ) .

i=1

Since quantization on nodes is performed independently we can decompose the ﬁrst term from the last row of the previous inequality into the sum of variances:

EQ gk 2

=
(A.6)
≤
(A.11)
≤

1n E

Q(gˆk − ∇f (x∗)) − gˆk − ∇f (x∗)

2
+

n2

Q

i

i

i

i

i=1

ωn k

∗ 2 1n k

2 ∗

n2 gˆi − ∇fi(x ) + n gˆi − ∇fi(x )

i=1

i=1

1+ ω n

1 n gˆk − ∇f (x∗) 2.

n

i

i

i=1

1n k

2 ∗

n gˆi − ∇fi(x )

i=1

Taking conditional mathematical expectation E · | xk from the both sides of previous inequality and using the bound

1n nE
i=1

gˆik − ∇fi(x∗) 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2

implicitly obtained in the proof of Lemma B.6.27 we get (B.59).

Lemma B.6.33. For all k ≥ 0, i ∈ [n] we have

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

where

σk2

=

1 nm

n i=1

n j=1

∇fij(wik) − ∇fij(x∗) 2.

(B.60)

Proof. The proof is identical to the proof of Lemma 3.8.22.

Theorem B.6.34. Assume that f (x) is µ-quasi strongly convex and functions fij are convex

292

and L-smooth for all i ∈ [n], j ∈ [m]. Then D-QLSVRGstar satisﬁes Assumption 3.3.3 with

2ω A = 2L 1 + n , B1 = 0, B2 = 2 σ22,k = σk2 = n1m n m ∇fij (wik) − ∇fij (x∗) 2,
i=1 j=1

1 + 2nω , D1 = 0, σ12,0 ≡ 0, ρ1 = 1, ρ2 = p, C2 = Lp, D2 = 0,

C1 = 0,

G = 0,

F1 = 0,

12γ2Lτ

1

+

2ω n

(2 + p)

F2 =

, p

D3 = 0

with γ satisfying

3

1

8

1

+

2ω n

γ ≤ min

, 56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

,

M2 =

3p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK

when µ = 0, where T k d=ef x˜k − x∗ 2 + M2γ2σk2.

In other words, D-QLSVRGstar converges with linear rate

O 1 + κ 1 + ω + κ τ τ + 1 + ω 1 + 1 ln 1

p

n

n

(1 − p)

ε

to

the

exact

solution

when

µ

>

0.

If

m

≥

2

then

taking

p

=

1 m

we

get

that

in

expectation

the

sample complexity of one iteration of D-QLSVRGstar is O(1) gradients calculations per node as

for D-QSGDsr with standard sampling and the rate of convergence to the exact solution becomes

O m + κ 1 + ω + κ τ τ + ω ln 1 .

n

n

ε

Applying Lemma A.5.6 we get the complexity result in the case when µ = 0. Corollary B.6.35. Let the assumptions of Theorem B.6.34 hold and µ = 0. Then after K

293

iterations of D-QLSVRGstar with the stepsize

γ0 = min 3 , 1

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))


 γ = min γ0,


R02 , 3 M2σ02 12Lτ

R02p

1

+

2ω n

 
(2 + p) 

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

 LR02
O 

1

+

ω n

+

K

τ

τ

+

ω n



R02m

1

+

ω n

σ02

3 R04Lτ m

1

+

ω n

+

+

.

K

K



That is, to achive E f (x¯K) − f (x∗) ≤ ε D-QLSVRGstar requires



LR02

1

+

ω n

+

O



ε

τ

τ

+

ω n



R02m

1

+

ω n

σ02

3 R04Lτ m

1

+

ω n

+

+



ε

ε



iterations.
However, such convergence guarantees are obtained under very restrictive assumption: the method requires to know vectors ∇fi(x∗).

B.6.9 D-LSVRG-DIANA
In the setup of Section B.6.6 we construct a new method with delayed updates and quantization called D-LSVRG-DIANA which does not require to know ∇fi(x∗) and has linear convergence to the exact solution.
Lemma B.6.36. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), ≤ 4L 1 + 2ω
n

f (xk) − f (x∗) + 2nω σ12,k + 2

1 + 2ω n

(B.61) σ22,k (B.62)

where

σ12,k

=

1 n

n i=1

hki − ∇f (x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij(wik) − ∇fij(x∗) 2.

Proof. First of all, we show unbiasedness of gk:

E gk | xk

(A=.15) =

hk + 1 n E E

∆ˆ k

| xk

(A=.6) hk + 1

n
E gˆk − hk | xk

n

Qi

n

i

i

i=1

i=1

1 nm nm i=1 j=1

∇fij(xk) − ∇fij(wik) + ∇fi(wik)

= ∇f (xk).

294

Algorithm 47 D-LSVRG-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2:

Set

h0

=

1 n

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk−τ to all workers

5: for i = 1, . . . , n in parallel do

6:

Pick l uniformly at random from [m]

7:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

8:

∆ˆ ki −τ = Q(gˆik−τ − hki −τ ) (quantization is performed independently from other nodes)

9:

gik−τ = hki −τ + ∆ˆ ik−τ

10:

vik = γgik−τ

11:

eki +1 = eki + γgik − vik

12:

hki −τ +1 = hki −τ + α∆ˆ ik−τ

13: end for

14: ek = 1 n ek, gk = 1 n gk = hk + 1 n ∆ˆ k, vk = 1 n vk = γhk−τ + γ n ∆ˆ k−τ

n i=1 i

n i=1 i

n

i

n i=1 i

n i=1 i

i=1

15: hk−τ +1 = n1 n hki −τ +1 = hk−τ + α n1 n ∆ˆ ki −τ

i=1

i=1

16: xk+1 = xk − vk

17: end for

Next, we derive the upper bound for the second moment of gk:

EQ gk 2

= (A=.14)

 EQ 
 EQ 

hk + 1

n

2 ∆ˆ k

n

i

i=1

1 n ∆ˆ k − gˆk + hk

n

i

i

i

i=1

2 +

1n

2 k

n gˆi .

i=1

Since quantization on nodes is performed independently we can decompose the ﬁrst term from the last row of the previous inequality into the sum of variances:

EQ gk 2

≤
(A.6),(A.11)
≤
(A.11)
≤

1n E

∆ˆ k − gˆk + hk 2 + 1 n gˆk − ∇f (x∗) 2

n2

Q

i

i

i

n

i

i

i=1

i=1

ω n gˆk − hk 2 + 1 n gˆk − ∇f (x∗) 2

n2

i

i

n

i

i

i=1

i=1

1 + 2ω n

1 n gˆk − ∇f (x∗) 2 + 2ω n hk − f (x∗) 2.

n

i

i

n2

i

i

i=1

i=1

Taking mathematical expectation E · | xk from the both sides of the previous inequality and using the bound

1n nE
i=1

gˆk − ∇f (x∗) 2 | xk ≤ 4L f (xk) − f (x∗) + 2

nm
∇f (wk) − ∇f (x∗) 2

i

i

nm

ij i

ij

i=1 j=1

implicitly obtained in the proof of Lemma B.6.27 we get (B.62).

295

Lemma B.6.37. Assume that α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k,

where

σ12,k

=

1 n

E σ22,k+1 | xk ≤ (1 − p)σk2,2 + 2Lp f (xk) − f (x∗)

n i=1

hki − ∇fi(x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij(wik) − ∇fij(x∗) 2.

Proof. The proof is identical to the proof of Lemma 3.8.31.

Theorem B.6.38. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m and f (x) is µ-quasi strongly convex. Then D-LSVRG-DIANA satisﬁes Assumption 3.3.3 with

2ω

2ω

2ω

A = 2L 1 + n

,

B1 =

, n

B2 = 2 1 + n

,

D1 = 0,

σ12,k = n1 n hki − ∇fi(x∗) 2,
i=1

σ22,k = n1m n m ∇fij (wik) − ∇fij(x∗) 2,
i=1 j=1

ρ1 = α, ρ2 = p, C1 = 3Lα, C2 = Lp, D2 = 0, G = 2,

12γ2Lωτ (2 + α)

12γ2τ L(2 + p) 4ω

2ω

F1 = nα , F2 = p

n(1 − α) + 1 + n , D3 = 0

with γ and α satisfying









γ ≤ min  1 , 1  ,

 8L 397 + 234nω 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 

and for all K ≥ 0

α≤ 1 , ω+1

8ω M1 = 3nα ,

8

7

+

6ω n

M2 = 9p .

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α p ,,

2 44

K 4(T 0 + γF1σ12,0 + γF2σ22,0) γ

when µ > 0 and

E

f (x¯K ) − f (x∗)

4(T 0 + γF1σ12,0 + γF2σ22,0) ≤

γK

when µ = 0, where T k d=ef x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k.

296

In other words, if m ≥ 2, p = 1/m, α = min ω+1 1 , 12 and









γ ≤ min  1 , 1  ,

 8L 397 + 234nω 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 

D-LSVRG-DIANA converges with the linear rate

O ω + m + κ 1 + ω + κ τ τ + ω ln 1

n

n

ε

to the exact solution when µ > 0. Applying Lemma A.5.6 we get the complexity result in the case when µ = 0.

Corollary B.6.39. Let the assumptions of Theorem B.6.38 hold and µ = 0. Then after K iterations of D-LSVRG-DIANA with the stepsize









 

1

1

 

γ0 = min

,

,

 8L 397 + 234nω 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 





 γ = min γ0,

R02

,3

R02

 ,

 M1σ12,0 + M2σ22,0 12τ L ω(2n+αα) + 2+p p 1 + 2nω + n(14−ωα) 

where R0 = x0 − x∗ , α = min ω+1 1 , 12 order

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of

 LR02
O 

1

+

ω n

+

K

τ

τ

+

ω n



R02ω(ω + 1)σ12,0

R02m

1

+

ω n

σ22,0

+

√

+



nK

K



 3 R04τ Lω(ω + 1)σ12,0

3 R04τ Lm

1

+

ω n

σ22,0 

+O 

√

+



3 nK

K

That is, to achive E f (x¯K) − f (x∗) ≤ ε D-LSVRG-DIANA requires



LR02

1

+

ω n

+

O



ε

τ

τ

+

ω n



R02ω(ω + 1)σ12,0

R02m

1

+

ω n

σ22,0

+

√

+



nε

ε



 3 R04τ Lω(ω + 1)σ12,0

3 R04τ Lm

1

+

ω n

σ22,0 

+O 

√

+



3 nε

ε

iterations.

297

Table B.2: The parameters for which the methods from Tables 3.1 and B.1 satisfy Assumption 3.3.3. The meaning of the expressions appearing in the table, as well as their justiﬁcation is deﬁned in details in the Sections 3.8 and B.6. Symbols: ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for the variances of stochastic gradients of workers.
298

Method

A

EC-SGDsr

2L

EC-SGD

2L

EC-GDstar

L

EC-SGD-DIANA

L

EC-SGDsr-DIANA

2L

EC-LSVRG

2L

EC-LSVRGstar

2L

EC-LSVRG-DIANA

2L

D-SGDsr D-SGD D-QSGD
D-QSGDstar D-QGDstar D-SGD-DIANA
D-LSVRG D-QLSVRG

2L 2L L 1 + 2nω L 1 + ωn L 1 + ωn L 1 + 2nω
2L 2L 1 + 2nω

D-QLSVRGstar D-LSVRG-DIANA

2L 1 + 2nω 2L 1 + 2nω

B1

B2

0

0

0

0

0

0

0

0

0

0

0

2

0

2

0

2

0

0

0

0

0

0

0

0

0

0

2nω 0

0

2

0 2 1 + 2nω

0 2 1 + 2nω 2nω 2 1 + 2nω

ρ1 ρ2

C1

C2

1

1

0

0

1

1

0

0

1

1

0

0

α

1

Lα

0

α

1 2α(3L + 4L)

0

1

p

1

p

0

Lp

0

Lp

α

p

3Lα

Lp

1

1

1

1

1

1

1

1

1

1

α

1

1

p

1

p

1

p

α

p

0

0

0

0

0

0

0

0

0

0

Lα

0

0

Lp

0

Lp

0

Lp

3Lα

Lp

F1, F2 0, 0

0, 0
0, 0 96Lγ2 δ2α(1−η) , 0
96Lγ2 δ2α(1−η) , 0 0, δ7p2(L1−γ2η)
0, 48δLpγ2 24Lγ2 4 +3
δ δα(1−η) , 24Lγ2 1−4α δ4 +3 +3 δp(1−η)
0, 0
0, 0
0, 0

0, 0 0, 0

12γ2 Lωτ (2+α)

nα

,0

12γ2Lτ (2+p)

0,

np

12γ2 Lτ 1+ 2nω τ (2+p)

0,

p

12γ2L 1+ 2nω τ (2+p)

0,

p

12γ2 Lωτ (2+α)

nα

,

12γ2τ L(2+p)

2ω(3−α)

p

1 + n(1−α)

G

D1, D2, D3

0

2σ∗2 , 0, 6Lγ 4ζ∗2 + 3σ2

n

δ

δ

∗

0

2σ∗2 , 0, 12Lγ 2ζ∗2 + σ2

n

δ

δ

∗

0

0, 0, 0

σ2 , α2(ω + 1)σ2,

0

n

6Lγ 4α(ω+1)

2

δ

δ

+1 σ

2σ∗2 , α2(ω + 1)σ2 ,

0

n

∗

18Lγ 4α(ω+1) + 1 σ2

δ

δ

∗

0 0, 0, 24δL2 γ ζ∗2

0

0, 0, 0

2

0, 0, 0

0

2σ∗2 , 0, 6Lτ γσ∗2

n

n

0

2σ∗2 , 0, 6Lτ γσ∗2

n

n

(ω+1)σ2 + 2ωζ∗2 , 0,

0

n

n

3γτ L n

(ω + 1)σ2 + 2ωζ∗2

(ω+1)σ2

3γτ L(ω+1)σ2

0

n , 0,

n

0

0, 0, 0

(ω+1)σ2

α(ω+1)σ2

0

n,

n

,

2

3γτ L 1 + 4nω

(ω+1)σ n

0

0, 0, 0

2ωζ∗2 , 0,

0

n2

6γτ Lωζ∗

n

0

0, 0, 0

0

0, 0, 0

Appendix C
299

Appendix for Chapter 4

C.1 Table of Frequently Used Notation

Table C.1: Summary of frequently used notation.

f : Rd → R fi : Rd → R
x∗ d n xki gik γ ck µ L xk Vk xK ζ τ p aki bki hki lik L max Lij σ2 σ∗2 ζ∗2
A, A , B, B , C, C , F, F , G, H, D1, D1, D2, D3, ρ Ai, Bi D1,i, ρi, Ci, D2,i
Ai, D3,i σk2, σi2,k
E[·] E · | xk Dh(x, y)

Main notation

Objective to be minimized

Local objective owned by device/worker i

Global optimum of (6.6); x∗ ∈ Rd

Dimensionality of the problem space

Number of clients/devices/nodes/workers

Local iterate; xki ∈ Rd Local stochastic direction; gik ∈ Rd Stepsize/learning rate; γ ≥ 0

Indicator of the communication; ck ∈ {0, 1}

Strong quasi-convexity of the local objective; µ ≥ 0

Smoothness of the local objective; L ≥ µ

Virtual iterate; xk ∈ Rd Discrepancy between local and virtual iterates; V k ≥ 0

Weighted average of historical iterates; xK ∈ Rd

Heterogeneity parameter; ζ ≥ 0

Size of the ﬁxed local loop τ ≥ 0

Probability of aggregation ﬁxed for the random local loop p ∈ [0, 1]

Unbiased local gradient; aki ∈ Rd Local shift; bki ∈ Rd Delayed local gradient estimator used to construct bki ; hki ∈ Rd Unbiased local gradient estimator used to construct bki ; lik ∈ Rd
Expected smoothness of local objectives; L ≥ 0

Smoothness constant of local summands; max Lij ≥ 0

Averaged upper bound for the variance of local stochastic gradient

Averaged variance of local stochastic gradients at the solution

def
=

1

n

ni=1 ∇fi(x∗) 2

Parametric Assumptions

(6.6) (5.5) or (4.3)
(6.6) (6.6) (4.4) (4.4) (4.4) (4.4) (4.5) (4.6) Sec 4.2 Sec 4.2 Thm 4.2.4 (4.15) Sec 4.3 Sec 4.3 Sec 4.4 Sec 4.4 Sec 4.4 Sec 4.4 (4.37) Sec (4.5.2) Tab (C.3) Tab (C.3) Tab (C.3)

Parameters of Assumption 4.2.3

Parameters of Assumption 4.4.1 Parameters of Assumption 4.4.2 Possibly random non-negative sequences from Assumptions 4.2.3, 4.4.1, C.4.1 Standard

Expectation

def
=E

· | xk1 , . . . , xkn

; expectation conditioned on k-th local iterates

def
= h(x) − h(y) − ∇h(y), x − y ; Bregman distance of x, y w.r.t. h

As 4.4.1

300

C.2 Extra Experiments

C.2.1 Missing Details from Section 4.6 and an Extra Figure

In Section 4.6 we study the eﬀect of local variance reduction on the communication complexity of local methods. We consider the regularized logistic regression objective, i.e., we choose

def 1 m

fi(x) =

log 1 + exp

m j=1

a(i−1)m+j , x · b(i−1)m+j

+ µ x 2, 2

where aj ∈ Rd, bj ∈ {−1, 1} for j ≤ nm are the training data and labels.

Number of the clients. We select a diﬀerent number of clients for each dataset in order to capture a variety of scenarios. See Table C.2 for details.
Table C.2: Number of clients per dataset (Figures 4.1 and C.1).

Dataset n # datapoints (= mn) d

a1a

5

mushrooms 12

phishing 11

madelon 50

duke

4

w2a

10

1 605 8 124 11 055 2 000
44 3 470

123 112 68 500 7 129 300

Relative suboptimality

100 10-1 10-2 10-3 10-4 10-5
0
10-1 10-3 10-5 10-7 10-9 10-11
0

w2a

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

mushrooms

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

Relative suboptimality

Relative suboptimality

100 10-1 10-2
0
10-1 10-3 10-5 10-7
0

a1a
1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

duke

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

Relative suboptimality

Relative suboptimality

madelon
100

10-1
10-2 0
10-1 10-3 10-5

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

2000

4000

6000

8000

Rounds of communication

10000

phishing

1_SGD 0.1_SGD 0.01_SGD 1_SVRG 0.1_SVRG 0.01_SVRG

10-7

10-9 0

2000

4000

6000

8000

Rounds of communication

10000

Relative suboptimality

Figure C.1: Comparison of standard Local-SGD (Algorithm 27), and Local-SVRG (Algorithm 28) with various stepsizes γ. Logistic regression applied on LibSVM data [27] with heterogenously splitted data. Other parameters: L = 1, µ = 10−4, τ = 40. Parameter n chosen as per Table C.2. (Same as Figure 4.1, but with the heterogenous data split)

301

Data split. The experiment from Figure 4.1 in the main body of the paper splits the data among the clients uniformly at random (i.e., split according to the the order given by a random permutation). However, in a typical FL scenario, the local data might signiﬁcantly diﬀer from the population average. For this reason, we also test on a diﬀerent split of the data: we ﬁrst sort the data according to the labels, and then split them among the clients. Figure C.1 shows the results. We draw a conclusions identical to Figure 4.1. We see that Local-SVRG was at least as good as Local-SGD for every stepsize choice and every dataset. Further, the prediction that the smaller stepsize yields the smaller of the optimum neighborhood for the price of slower convergence was conﬁrmed.

Environment. All experiments were performed in a simulated environment on a single machine.

C.2.2 The Eﬀect of Local Shift/Drifts
The experiment presented in Section 4.6 examined the eﬀect of the noise on the performance of local methods and demonstrated that control variates can be eﬃciently employed to reduce that noise. In this section, we study the second factor that inﬂuences the neighborhood to which Local-SGD converges: non-stationarity of Local-GD.
We have already shown that the mentioned non-stationarity of Local-GD can be ﬁxed using a carefully designed idealized/optimal shift that depends on the solution x∗ (see Algorithm 29). Furthermore, we have shown that this idealized shift can be learned on-the-ﬂy at the small price of slightly slower convergence rate (see Algorithm 30 – SS-Local-SGD/SCAFFOLD).1
In this experiment, we therefore compare Local-SGD, S*-Local-SGD and SCAFFOLD. In order to decouple the local variance with the non-stationarity of the local methods, we let each algorithm access the full local gradients. Next, in order to have a full control of the setting, we let the local objectives to be artiﬁcially generated quadratic problems. Speciﬁcally, we set

fi(x) = µ2 x 2 + 1 −2 µ (x − zi∗)





m

 aiai  (x − zi∗),

j=1

(C.1)

where ai are mutually orthogonal vectors of norm 1 with m < d (generated by orthogonalizing Gaussian vectors), zi∗ are Gaussian vectors and µ = 10−3. We consider four diﬀerent instances of (C.1) given by Table C.1. Figures C.2, C.3, C.4, C.5 show the result.
Through most of the plots across all combinations of type, τ , n, we can see that Local-SGD suﬀers greatly from the fact that it is attracted to an incorrect ﬁxed point and as a result, it never converges to the exact optimum. On the other hand, both S*-Local-SGD and SCAFFOLD converge to the exact optimum and therefore outperform Local-SGD in most examples. We

1In fact, SCAFFOLD can be coupled together with Local-SVRG given that the local objectives are of a ﬁnite-sum structure, resulting in Algorithm 32.

302

Table C.3: Instances of (C.1).

Type m

zi∗

0 1 ∼ N (0, I)

1 10 ∼ N (0, I)

2 1 ∼ N (0, I)

3 10 ∼ N (0, I)

shall note that the rate of SCAFFOLD involves slightly worse constants than those in Local-SGD and S*-Local-SGD, and therefore it sometimes performs worse in the early stages of the optimization process when compared to the other methods. Furthermore, notice that our method S*-Local-SGD always performed best.
To summarize, our results demonstrate that
(i) the incorrect ﬁxed point of used by standard local methods is an issue not only theory but also in practice, and should be addressed if better performance is required,
(ii) the theoretically optimal shift employed by S*-Local-SGD is ideal from a performance perspective if it was available (however, this strategy is impractical to implement as the optimal shift presumes the knowledge of the optimal solution), and
(iii) SCAFFOLD/SS-Local-SGD is a practical solution to ﬁxing the incorrect ﬁxed point problem – it converges to the exact optimum at a price of a slightly worse initial convergence speed.

C.3 Missing Proofs for Section 4.2

Let us ﬁrst state some well-known consequences of L-smoothness. Speciﬁcally, if fi is L-smooth, we must have

fi(y) ≤ fi(x) + ∇fi(x), y − x + L x − y 2, 2

∀x, y ∈ Rd.

(C.2)

If in addition to this we assume that fi is convex, the following bound holds:

∇fi(x) − ∇fi(y) 2 ≤ 2L(fi(x) − fi(y) − ∇fi(y), x − y ) d=ef 2LDfi(x, y), ∀x, y ∈ Rd (C.3)

We next proceed with the proof of Theorem 4.2.4. Following the technique of virtual iterates from [209, 89], notice that the sequence {xk}k≥0 satisﬁes the recursion

xk+1 = xk − nγ n gik.
i=1

(C.4)

This observation forms the backbone of the key lemma of our paper, which we present next.

303

Relative suboptimality

Relative suboptimality

10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 0, tau: 5, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000 Type: 0, tau: 5, n: 50
SCAFFOLD LGD LGD*
10000Round2s00o0f0comm30u0n00ication40000 50000 Type: 0, tau: 5, n: 500
SCAFFOLD LGD LGD* 10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

100 10 1 10 2 10 3 10 4 10 5 10 6
0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 0, tau: 20, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 0, tau: 20, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000 Type: 0, tau: 20, n: 500
SCAFFOLD LGD LGD*
10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

Type: 0, tau: 100, n: 5

100

SCAFFOLD

LGD

LGD*

10 1

10 2

0 100 10 1

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 0, tau: 100, n: 50

SCAFFOLD LGD LGD*

10 2

10 3 0
100
10 1

10000Round2s00o0f0 comm30u0n00ication40000 50000
Type: 0, tau: 100, n: 500 SCAFFOLD LGD LGD*

10 2

10 3 0

10000Round2s00o0f0 comm30u0n00ication40000 50000

Relative suboptimality

Figure C.2: Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27 with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 0 (see Table C.3).

Lemma C.3.1. Let Assumption 4.2.1, 4.2.2 and 4.2.3 be satisﬁed and γ ≤ min {1/2(A +MC), L/(F +MG)}, where M = 43Bρ . Let η d=ef min γµ, ρ4 . Then for all k ≥ 0 we have

γE f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M D2) + 2LγEVk, (C.5)

where η d=ef min γµ, ρ4 , T k d=ef xk − x∗ 2 + M γ2σk2.

Proof. First of all, to simplify the proofs we introduce new notation: gk d=ef n1 this and (C.4) we get

n i=1

gik

.

Using

xk+1 − x∗ 2

(C=.4) =

xk − x∗ − γgk 2 xk − x∗ 2 − 2γ xk − x∗, gk + γ2 gk 2.

Taking conditional mathematical expectation Ek[·] = E[· | xk] d=ef E[· | xk1, . . . , xkn] on both sides of the previous inequality we get
E xk+1 − x∗ 2 | xk (5=.8) xk − x∗ 2 − 2nγ n xk − x∗, ∇fi(xki ) + γ2E gk 2 | xk ,
i=1

304

Relative suboptimality

Relative suboptimality

10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 1, tau: 5, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 1, tau: 5, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 1, tau: 5, n: 500

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7
0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 1, tau: 20, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 1, tau: 20, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 1, tau: 20, n: 500

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

100
10 1
10 2
10 3
10 4 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 1, tau: 100, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 1, tau: 100, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000
Type: 1, tau: 100, n: 500 SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Figure C.3: Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27 with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 1 (see Table C.3).

hence

E xk+1 − x∗ 2

(A.15)
≤
(4.8)
≤

E xk − x∗ E xk − x∗
+2A γ2E

2 − 2γ n E n i=1
2 − 2γ n E n i=1
f (xk) − f (x∗)

xk − x∗, ∇fi(xki ) + γ2E gk 2

xk − x∗, ∇fi(xki ) + B γ2E σk2

+ F γ2E [Vk] + γ2D1.

(C.6)

Next, we derive an upper bound for the second term on the right-hand side of the previous inequality:

2γ n −
n

xk − x∗, ∇fi(xki )

i=1

=
(4.5),(C.2)
≤
(A.11)
≤

2γ n n i=1 2γ n n i=1
+ 2γ n

x∗ − xki , ∇fi(xki ) + xki − xk, ∇fi(xki )
fi(x∗) − fi(xki ) − µ2 xki − x∗ 2 n fi(xki ) − fi(xk) + L2 xk − xki 2
i=1

−2γ f (xk) − f (x∗) − µγ xk − x∗ 2 + LγVk. (C.7)

305

Relative suboptimality

Relative suboptimality

10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 2, tau: 5, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000 Type: 2, tau: 5, n: 50
SCAFFOLD LGD LGD*
10000Round2s00o0f0comm30u0n00ication40000 50000 Type: 2, tau: 5, n: 500
SCAFFOLD LGD LGD* 10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

Type: 2, tau: 20, n: 5

100

SCAFFOLD

LGD

10 1

LGD*

10 2

10 3

10 4

10 5

0

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 2, tau: 20, n: 50
100

10 1

10 2

10 3

10 4

10 5

10 6

SCAFFOLD LGD

10 7

LGD*

0

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 2, tau: 20, n: 500

10 1

10 3

10 5

10 7

10 9

SCAFFOLD

10 11

LGD

LGD*

10 13 0

10000 20000 30000 40000 50000

Rounds of communication

Relative suboptimality

Relative suboptimality

Relative suboptimality

Type: 2, tau: 100, n: 5

100

SCAFFOLD

LGD

LGD*

10 1

10 2

0 100 10 1

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 2, tau: 100, n: 50

SCAFFOLD LGD LGD*

10 2

10 3 0
100
10 1

10000Round2s00o0f0 comm30u0n00ication40000 50000
Type: 2, tau: 100, n: 500 SCAFFOLD LGD LGD*

10 2

10 3 0

10000Round2s00o0f0 comm30u0n00ication40000 50000

Relative suboptimality

Figure C.4: Comparison of the following noiseless algorithms Local-SGD (LGD, Algorithm 27 with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 2 (see Table C.3).
Plugging (C.7) in (C.6), we obtain

E xk+1 − x∗ 2

(C.6),(C.7)
≤

(1 − γµ)E xk − x∗ 2 − 2γ 1 − A γ E f (xk) − f (x∗)

+B γ2E σk2 + γ L + F γ E [Vk] + γ2D1.

(C.8)

It implies that

ET k+1

=
(C.8),(4.10)
≤

E xk+1 − x∗ 2 + M γ2E σk2+1 (1 − γµ)E xk − x∗ 2 + 1 + MB − ρ M γ2Eσk2
−2γ 1 − A + M C γ E f (xk) − f (x∗) +γ L + (F + M G)γ EVk + γ2 D1 + M D2 .

306

Relative suboptimality

Relative suboptimality

10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 3, tau: 5, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 3, tau: 5, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 3, tau: 5, n: 500

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7
0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 3, tau: 20, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 3, tau: 20, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Type: 3, tau: 20, n: 500

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Relative suboptimality

Relative suboptimality

100
10 1
10 2
10 3
10 4 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 0

Type: 3, tau: 100, n: 5

SCAFFOLD LGD LGD*

10000Round2s00o0f0 comm30u0n00ication40000 50000

Type: 3, tau: 100, n: 50

SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000
Type: 3, tau: 100, n: 500 SCAFFOLD LGD LGD*

10000Round2s00o0f0comm30u0n00ication40000 50000

Relative suboptimality

Figure C.5: Comparison of the following noiseless algorithms: Local-SGD (LGD, Algorithm 27 with no local noise) and SCAFFOLD [86] (Algorithm 30 without “Loopless”) and S*-Local-SGD (LGD*, Algorithm 29). Quadratic minimization, problem type 3 (see Table C.3).

Since

M

=

4B 3ρ

,

η

=

min

γµ, ρ4

and γ ≤ min {1/2(A +MC), L/(F +MG)}, we get

ET k+1 ≤ (1 − γµ)E xk − x∗ 2 + 1 − ρ4 M γ2Eσk2 − γE f (xk) − f (x∗) +2LγEVk + γ2 D1 + M D2
≤ (1 − η)ET k − γE f (xk) − f (x∗) + 2LγEVk + γ2 D1 + M D2 .

Rearranging the terms we get (C.5).

Using the above lemma we derive the main complexity result.

C.3.1 Proof of Theorem 4.2.4
From Lemma C.3.1 we have that γE f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M D2) + 2LγEVk.

307

Summing up previous inequalities for k = 0, . . . , K with weights wk deﬁned in (4.12) we derive

K
γ wkE f (xk) − f (x∗)
k=0

≤
(4.12),(4.11)
≤

K
wk(1 − η)ET k − wkET k+1 + γ2(D1 + M D2)WK
k=0
K
+2Lγ wkEVk
k=0
K
wk−1ET k − wkET k+1 + γ2 D1 + M D2 WK
k=0
+ γ2 K wkE f (xk) − f (x∗) + 2LHγEσ02 + 2Lγ3D3WK .
k=0

Relations T k ≥ 0 and w−1 = 1 imply that

γ2 K wkE f (xk) − f (x∗)
k=0

≤ T 0 + 2LHγEσ02 + γ2 D1 + M D2 + 2LγD3 WK .

Using the deﬁnition of xK and convexity of f , we get

E f (xK ) − f (x∗)

≤ 2T 0 + 4LHγEσ02 + 2γ D + M D2 + 2LγD3 .

γWK

1

(C.9)

It remains to consider two cases: µ > 0 and µ = 0. If µ > 0 we have WK ≥ wK ≥ (1 − η)−K, where η d=ef min γµ, ρ4 which implies (4.13). Finally, when µ = 0, we have wk = 1 for all k ≥ 0,
which implies WK = K + 1 ≥ K and (4.14).

C.3.2 Corollaries

We state the full complexity results that can be obtained from Theorem 4.2.4. These results can be obtained as a direct consequence of Lemmas A.5.4 and A.5.6.

Corollary

C.3.2.

Consider

the

setup

from

Theorem

4.2.4

and

denote

1 h

to

be

the

resulting

upper bound on γa and µ > 0.

1. If D3 does not depend on γ, then for all K such that

either or

ln (max{2, min{aµ2K2/c1, aµ3K3/c2}}) ≤ρ
K

1 ln (max{2, min{aµ2K2/c1, aµ3K3/c2}})

≤

,

h

µK

308

a=2

x0 − x∗

2

+

8B Eσ02 3h2ρ

+

4LHhEσ02 ,

c1

=

2D1

+

4B D2 3ρ

,

c2

=

4LD3

and

γ = min 1 , γK , h

ln max 2, min aµ2K2 , aµ3K3

=

c1

c2

,

γK µK

we haveb

E f (xK) − f (x∗) = O ha exp − min

µ ,ρ K

+ c1 +

c2

.

h

µK µ2K2

That is, to achieve E f (xK) − f (x∗) ≤ ε, the method requiresc:

K =O

1 + h log ha + c1 +

ρµ

ε µε

c2 µ2ε .

2.

If

D3

= D3,1 +

Dγ3,2 ,

then

the

same

bounds

hold

with

c1

= 2D1 +

4B D2 3ρ

+ 2LD3,2

and

c2 = 4LD3,1.

aIn order to obtain tight estimate of parameters D3 and H, we shall impose further bounds on γ (see Section 4.3 and Table 4.1 therein).
bO hides numerical constants and logarithmical factors depending on K and parameters of the problem.
cIf c1 = c2 = 0, then one can replace O by O.

Corollary

C.3.3.

Let

assumptions

of

Theorem

4.2.4

be

satisﬁed

with

any

γ

≤

1 h

and

µ = 0.

1. If D3 does not depend on γ, then for all K and

γ = min 1 , a , 3 a ,

a

a

,3

,

h b1 b2 c1K c2K

where a = 2

x0 − x∗

2, b1 = 4LHEσ02, b2 =

8B3Eρσ02 ,

c1

=

2D1

+

4B D2 3ρ

,

c2

=

4LD3,

we

have

E f (xK ) − f (x∗) = O

√

√

ha + ab1 + 3 a2b2 +

KK

K

√

ac1 + 3 a2c2 .

K

K 2/3

That is, to achieve E f (xK) − f (x∗) ≤ ε, the method requires

ha

√ ab1

√ 3 a2b2

ac1

√ a c2

K = O ε + ε + ε + ε2 + ε3/2 .

2.

If

D3

= D3,1 +

Dγ3,2 ,

then

the

same

bounds

hold

with

c1

= 2D1 +

4B D2 3ρ

+ 2LD3,2

and

c2 = 4LD3,1.

309

C.4 Missing Proofs and Details for Section 4.3
C.4.1 Constant Local Loop
In this section we show how our results can be applied to analyze (4.4) in the case when
 1, if k mod τ = 0, ck = 0, if k mod τ = 0,
where τ is number of local steps between two neighboring rounds of communications. This corresponds to the setting in which the local loop size on each device has a ﬁxed length.

Heterogenous Data

First of all, we need to assume more about gik.

Assumption C.4.1. We assume that inequalities (4.8)-(4.10) hold and additionally there exist such non-negative constants A, A, B, B, F , F , D1, D1 that for all k ≥ 0

1n nE
i=1

g¯ik 2

1n nE
i=1

gik − g¯ik 2

≤ 2AE f (xk) − f (x∗) + BE σk2 + F E [Vk] + D1, ≤ 2AE f (xk) − f (x∗) + BE σk2 + F E [Vk] + D1,

(C.10) (C.11)

where g¯ik = E gik | xk1, . . . , xkn .

We notice that inequalities (C.10)-(C.11) imply (4.8) and vice versa. Indeed, if (C.10)-(C.11) hold then inequality (4.8) holds with A = A + A, B = B + B, F = F + F , D1 = D1 + D1 due to variance decomposition formula (A.14), and if (4.8) is true then (C.10)-(C.11) also hold with A = A = A, B = B = B, F = F = F , D1 = D1 = D1.

We start our analysis without making any assumption on homogeneity of data that workers have an access to. Next lemma provides an upper bound for the weighted sum of EVk.

Lemma C.4.2. Let Assumption 4.2.1, 4.2.2 and C.4.1 hold anda

















γ ≤ min  1 , 1  ,

 4(τ − 1)µ

2G(B(τ −1)+B) 

   

2 e(τ − 1) F (τ − 1) + F + ρ(1−ρ)

   

γ≤ 1 4 2eL(τ − 1) A(τ − 1) + A + 2C(Bρ((τ1−−ρ1))+B)

310

Then (4.11) holds with
H = 4e(τ − 1)(B(τ − 1) + B)(2 + ρ)γ2 , ρ
D3 = 2e(τ − 1) D1(τ − 1) + D1 + 2D2(B(τ − 1) + B) . ρ

(C.12)

aWhen ρ = 1 one can always set the parameters in such a way that B = B = C = G = 0, D2 = 0. In this case we assume that ρ2(1B−Cρ) = ρ2(1B−Cρ) = ρ2(1B−Gρ) = ρ2(1B−Gρ) = 0.

Proof. Consider some integer k ≥ 0. There exists such integer t ≥ 0 that τ t ≤ k ≤ τ (t + 1) − 1. Using this and Lemma A.5.2 we get

E[Vk]

(4.4)=,(C.4)



1n

k−1

k−1 2

E  xτi t − γ gil − xτt + γ gl 

n i=1

l=τ t

l=τ t

γ2 n


k−1

2

=

E

gil − gl 

n i=1

l=τ t

(A.16)
≤

eγ2(k − τ t) n k−1 E

g¯l − g¯l 2

+ eγ2

n

k−1
E

gl − g¯l − gl − g¯l

2

n

i

n

ii

i=1 l=τ t

i=1 l=τ t

(A.14)
≤

eγ2(τ − 1) n k−1

n

E

i=1 l=τ t

g¯l 2
i

eγ2 n k−1 +n E
i=1 l=τ t

gl − g¯l 2 ,
ii

n

where

g¯k

=

1 n

g¯ik. Applying Assumption C.4.1, we obtain

i=1

(C.10),(C.11)

k−1

k−1

EVk

≤

2e A(τ − 1) + A γ2 E f (xl) − f (x∗) + e B(τ − 1) + B γ2 Eσl2

l=τ t

l=τ t

k−1
+e F (τ − 1) + F γ2 EVl + e(τ − 1) D1(τ − 1) + D1 γ2,

l=τ t

hence

k

k j−1

wjEVj ≤ 2e A(τ − 1) + A γ2

wjE f (xl) − f (x∗)

j=τ t

j=τ t l=τ t

k j−1

+e B(τ − 1) + B γ2

wj Eσl2

j=τ t l=τ t

k j−1

+e F (τ − 1) + F γ2

wj EVl

j=τ t l=τ t

k
+e(τ − 1) D1(τ − 1) + D1 γ2 wj.
j=τ t

(C.13)

311

Recall that wk = (1 − η)−(k+1) and η = min γµ, ρ4 . Together with our assumption on γ it implies that for all 0 ≤ i < k, 0 ≤ j ≤ τ − 1 we have

(A.12)
wk = (1 − η)−(k−j+1) (1 − η)−j ≤ wk−j (1 + 2η)j ≤ wk−j (1 + 2γµ)j ≤ wk−j 1 + 2(τ 1− 1) j ≤ wk−j exp 2(τ j− 1) 1 ≤ wk−j exp 2 ≤ 2wk−j,
wk = (1 − η)−(k−i+1) (1 − η)−i (A≤.12) wk−i (1 + 2η)i ≤ wk−i 1 + ρ2 i , wk (A≤.12) (1 + 2η)k+1 ≤ 1 + ρ2 k+1 .

(C.14) (C.15) (C.16)

For simplicity, we introduce new notation: rk d=ef E f (xk) − f (x∗) . Using this we get

k j−1
wj rl
j=τ t l=τ t
k j−1
wj Eσl2
j=τ t l=τ t
k j−1
wj EVl
j=τ t l=τ t

(C.14)
≤
(C.14)
≤
(C.14)
≤

k j−1

k

k

2wlrl ≤ 2(k − τ t) wjrj ≤ 2(τ − 1) wjrj,

j=τ t l=τ t

j=τ t

j=τ t

k j−1

k

k

2wlEσl2 ≤ 2(k − τ t) wjEσj2 ≤ 2(τ − 1) wjEσj2,

j=τ t l=τ t

j=τ t

j=τ t

k j−1

k

k

2wlEVl ≤ 2(k − τ t) wjEVj ≤ 2(τ − 1) wjEVj.

j=τ t l=τ t

j=τ t

j=τ t

Plugging these inequalities in (E.22) we derive

k

k

k

wjEVj ≤ 4e(τ − 1)(A(τ − 1) + A)γ2 wjrj + 2e(τ − 1)(B(τ − 1) + B)γ2 wjEσj2

j=τ t

j=τ t

j=τ t

k

k

+2e(τ − 1)(F (τ − 1) + F )γ2 wjEVj + e D1(τ − 1) + D1 γ2 wj.

j=τ t

j=τ t

Since Vτt = 0 for all integer t ≥ 0 we obtain

K

K

K

wkEVk ≤ 4e(τ − 1)(A(τ − 1) + A)γ2 wkrk + 2e(τ − 1)(B(τ − 1) + B)γ2 wkEσk2

k=0

k=0

k=0

K
+2e(τ − 1)(F (τ − 1) + F )γ2 wkEVk

k=0

K
+e D1(τ − 1) + D1 γ2 wk

(C.17)

k=0

It remains to estimate the second term in the right-hand side of the previous inequality. First of

312

all,

Eσk2+1

(4.10)
≤
≤ ≤ =

(1 − ρ)Eσk2 + 2C E f (xk) − f (x∗) +GEVk + D2

rk

k

k

k

(1 − ρ)k+1Eσ02 + 2C (1 − ρ)k−lrl + G (1 − ρ)k−lEVl + D2 (1 − ρ)l

l=0

l=0

l=0

k

k

∞

(1 − ρ)k+1Eσ02 + 2C (1 − ρ)k−lrl + G (1 − ρ)k−lEVl + D2 (1 − ρ)l

l=0

l=0

l=0

(1 − ρ)k+1Eσ02 + 2C k (1 − ρ)k−lrl + G k (1 − ρ)k−lEVl + Dρ2 .

l=0

l=0

(C.18)

It implies that

K
wk Eσk2
k=0

(C.18)
≤
(C.15),(C.16)
≤
(A.13)
≤
≤
=

Eσ02 K wk(1 − ρ)k + 12−Cρ K

k
wk(1 − ρ)k−lrl

k=0

k=0 l=0

GK +1 − ρ

k wk(1 − ρ)k−lEVl + D2ρWK

k=0 l=0

Eσ02 1 + ρ2 K 1 + ρ2 k (1 − ρ)k
k=0

2C K k

+1 − ρ

wl

k=0 l=0

1+ ρ 2

k−l
(1 − ρ)k−lrl

G Kk

+1 − ρ

wl

k=0 l=0

1+ ρ 2

k−l (1 − ρ)k−lEVl + D2ρWK

Eσ02 1 + ρ2 K 1 − ρ2 k + 12−Cρ K k wlrl 1 − ρ2 k−l

k=0

k=0 l=0

G Kk

+1 − ρ

wlEVl

k=0 l=0

1− ρ 2

k−l + D2WK ρ

Eσ02 1 + ρ2

∞ 1 − ρ k + 2C

2

1−ρ

K
wk rk

k=0

k=0

∞

ρ

1− 2

l=0

+G 1−ρ

K
wk EVk
k=0

∞ 1 − ρ l + D2WK l=0 2 ρ

Eσ02(2ρ + ρ) + ρ(14C− ρ) K wkrk + ρ(12G− ρ) K wkEVk

k=0

k=0

+ D2WK . ρ

l
(C.19)

313

Plugging this inequality in (C.17) we get

K wkEVk ≤ 4e(τ − 1)γ2 A(τ − 1) + A + 2C(Bρ((τ1−−1ρ))+ B)

K
wk rk

k=0

k=0

+ 2e(τ − 1)(B(τ − 1) + B)Eσ02(2 + ρ)γ2 ρ

+2e(τ − 1)γ2

F (τ − 1) + F + 2G(B(τ − 1) + B) ρ(1 − ρ)

K
wk EVk
k=0

+e(τ − 1)γ2 D1(τ − 1) + D1 + 2D2(B(τ − 1) + B) WK . ρ

Our choice of γ implies

4e(τ − 1)γ2 A(τ − 1) + A + 2C(B(τ − 1) + B) ≤ 1

ρ(1 − ρ)

8L

and 2e(τ − 1)γ2 F (τ − 1) + F + 2G(B(τ − 1) + B) ρ(1 − ρ)
Using these inequalities we continue our derivations

≤ 1. 2

12 K wkEVk ≤ 81L K wkrk + 2e(τ − 1)(B(τ − 1)ρ+ B)Eσ02(2 + ρ)γ2

k=0

k=0

+e(τ − 1)γ2 D1(τ − 1) + D1 + 2D2(B(τ − 1) + B) WK . ρ

Multiplying both sides by 4L we get the result.

Clearly, this lemma and Theorem 4.2.4 imply the following result.

Corollary C.4.3. Let the assumptions of Lemma C.4.2 are satisﬁed. Then Assumption 4.2.3 holds and, in particular, if







1

L

γ ≤ min  2 A + 4B3ρC , F + 4B3ρG  ,

















γ ≤ min  1 , 1  ,

 4(τ − 1)µ

2G(B(τ −1)+B) 

   

2 e(τ − 1) F (τ − 1) + F + ρ(1−ρ)

   

γ≤

1,

4 2eL(τ − 1) A(τ − 1) + A + 2C(Bρ((τ1−−ρ1))+B)

314

then for all K ≥ 0 we have E f (xK ) − f (x∗)

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

≤

γWK

+2γ D1 + 4B3ρD2 + 2LγD3 ,

(C.20)

where xK d=ef W1K Kk=0 wkxk and
H = 4e(τ − 1)(B(τ − 1) + B)(2 + ρ)γ2 , ρ
D3 = 2e(τ − 1) D1(τ − 1) + D1 + 2D2(B(τ − 1) + B) . ρ

Moreover, if µ > 0, then

E f (xK ) − f (x∗) ≤

1 − min γµ, ρ 4

K

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

γ

+2γ D1 + 4B3ρD2 + 2LγD3 ,

(C.21)

and in the case when µ = 0, we have

E f (xK ) − f (x∗)

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

≤

γK

+2γ D1 + 4B3ρD2 + 2LγD3 .

(C.22)

ζ-Heterogeneous Data

In this section we assume that f1, f2, . . . , fn are ζ-heterogeneous (see Deﬁnition 4.3.1). Moreover, we additionally assume that E gik | xki = ∇fi(xki ) and that the functions fi for i ∈ [n] are µ-strongly convex,

fi(x) ≥ fi(y) + ∇fi(y), x − y + µ x − y 2 2

∀x, y ∈ Rd

(C.23)

which implies (e.g., see [153])

∇fi(x) − ∇fi(y), x − y ≥ µ x − y 2 ∀x, y ∈ Rd.

(C.24)

315

Lemma C.4.4. Let Assumption 4.2.2 be satisﬁed, inequalities (5.8)-(4.10) hold anda









γ ≤ min  1 , 1 , 1  .

 4(τ − 1)µ 2 (τ − 1) F + ρ2(1B−Gρ)

4

2L(τ − 1)

A

+

2BC ρ(1−ρ)

  

Moreover, assume that f1, f2, . . . , fn are ζ-heterogeneous and µ-strongly convex, and E gik | xki = ∇fi(xki ) for all i ∈ [n]. Then (4.11) holds with

4B(τ − 1)γ2(2 + ρ)

ζ2 2BD2

H=

, D3 = 2(τ − 1) D1 + +

.

ρ

γµ ρ

(C.25)

aWhen ρ = 1 one can always set the parameters in such a way that B = C = G = 0, D2 = 0. In this case we assume that ρ2(1B−Cρ) = ρ2(1B−Gρ) = 0.

Proof. First of all, if k mod τ = 0, then Vk = 0 by deﬁnition. Otherwise, we have

Vk (4.4)=,(C.4) n1 n xki −1 − xk−1 − γgik−1 + γgk−1 2
i=1

=

1 n xk−1 − xk−1 2 + 2γ n xk−1 − xk−1, gk−1 − gk−1 + γ2 n gk−1 − gk−1 2

n

i

n

i

i

n

i

i=1

i=1

i=1

= Vk−1 + 2γ n1 n xki −1 − xk−1, gk−1 + 2nγ n xk−1 − xki −1, gik−1

i=1

i=1

γ2 n +

gk−1 − gk−1 2

n

i

i=1

= Vk−1 + 2nγ n xk−1 − xki −1, gik−1 + γn2 n gik−1 − gk−1 2.

i=1

i=1

Next, we take the conditional expectation E · | xk−1 d=ef E · | xk1−1, . . . , xkn−1 on both sides of the obtained inequality and get

E Vk | xk−1

=
(A.14)
≤

2γ n Vk−1 +
n i=1

xk−1 − xik−1, ∇fi(xki −1)

γ2 n +n E
i=1

gik−1 − gk−1

2γ n Vk−1 +
n i=1

xk−1 − xik−1, ∇fi(xki −1) − ∇fi(xk−1)

+ 2γ n n i=1

xk−1 − xki −1, ∇fi(xk−1)

γ2 n +n E
i=1

gik−1 2 | xk−1 .

2 | xk−1

316

Since

1 n

n i=1

xk−1

− xki −1, ∇f (xk−1)

= 0, we can continue as follows:

E Vk | xk−1

(C.24)
≤
(A.7)
≤
(4.15)
≤

2γµ n Vk−1 − n

xk−1 − xk−1 2 + γ2 n E

i

n

i=1

i=1

gik−1 2 | xk−1

+ 2γ n n

xk−1 − xki −1, ∇fi(xk−1) − ∇f (xk−1)

i=1

γ2 n (1 − 2γµ)Vk−1 + n E
i=1

gik−1 2 | xk−1

+ 2γ n n i=1

µ2 xk−1 − xki −1 2 + 21µ ∇fi(xk−1) − ∇f (xk−1) 2

γ2 n (1 − γµ)Vk−1 + n E
i=1

gik−1 2 | xk−1 + γµζ2 .

Taking full expectation on both sides of previous inequality, we obtain

EVk (A≤.15) E [Vk−1] + γn2 n E gik−1 2 + γµζ2 .
i=1

Let t be a non-negative integer for which τ t ≤ k < τ (t + 1). Using this and Vτt = 0, we unroll the recurrence and derive

E[Vk] ≤ γn2 k−1 n E gil 2 + γζ2(kµ− τ t)
l=τ t i=1
(4≤.8) γ2 k−1 2AE f (xl) − f (x∗) + BE[σl2] + F E[Vl] + D1 + γζ2(kµ− τ t) ,
l=τ t

whence

k

k j−1

k j−1

wjEVj ≤ 2Aγ2

wjE f (xl) − f (x∗) + Bγ2

wj Eσl2

j=τ t

j=τ t l=τ t

j=τ t l=τ t

k j−1

+F γ2

wjEVl + (τ − 1)

j=τ t l=τ t

γ2D1 + γζ2 µ

k
wj .
j=τ t

If we substitute A with e(A(τ − 1) + A), B with e(B(τ − 1) + B), F with e(F (τ − 1) + F ), and γ2D1 + γµζ2 with eγ2(D1(τ − 1) + D1) in the inequality above, we will get inequality (E.22).
Following the same steps as in the proof of Lemma C.4.2, we get

K wkEVk ≤ 4(τ − 1)γ2 A + ρ(21B−Cρ) K wkrk + 2BEσ02(2 +ρρ)(τ − 1)γ2

k=0

k=0

+2(τ − 1)γ2

F + 2BG ρ(1 − ρ)

K
wkEVk + (τ − 1)γ2
k=0

ζ2 2BD2 D1 + +
γµ ρ

WK .

317

Our choice of γ implies that

4(τ − 1)γ2 A + 2BC ≤ 1 and 2(τ − 1)γ2 F + 2BG ≤ 1 .

ρ(1 − ρ) 8L

ρ(1 − ρ) 2

Using these inequalities we continue our derivations

12 K wkEVk ≤ 81L K wkrk + 2BEσ02(2 +ρρ)(τ − 1)γ2

k=0

k=0

+(τ − 1)γ2

ζ2 2BD2 D1 + +

WK .

γµ ρ

Multiplying both sides by 4L we get the result.

Clearly, this lemma and Theorem 4.2.4 imply the following result.

Corollary C.4.5. Let the assumptions of Lemma C.4.4 be satisﬁed. Then Assumption 4.2.3 holds and, in particular, if

γ ≤ min

1 , L , M = 4B ,

2(A + CM ) F + GM

3ρ









γ ≤ min  1 , 1 , 1  ,

 4(τ − 1)µ 2 (τ − 1) F + ρ2(1B−Gρ)

4

2L(τ − 1)

A

+

2BC ρ(1−ρ)

  

then for all K ≥ 0 we have

E f (xK ) − f (x∗)

≤ 2T 0 + 4LHγEσ02 + 2γ D + M D2 + 2LγD3 ,

γWK

1

where xK d=ef W1K Kk=0 wkxk and

4B(τ − 1)γ2(2 + ρ)

ζ2 2BD2

H=

, D3 = 2(τ − 1) D1 + +

.

ρ

γµ ρ

(C.26)

Moreover, if µ > 0, then E f (xK ) − f (x∗) ≤

1 − min γµ, ρ 4

K 2T 0 + 4LHγEσ02 γ

+2γ D1 + M D2 + 2LγD3 ,

(C.27)

and in the case when µ = 0, we have E f (xK ) − f (x∗) ≤ 2T 0 + 4γLKHγEσ02 + 2γ D1 + M D2 + 2LγD3 .

(C.28)

318

C.4.2 Random Local Loop
In this section we show how our results can be applied to analyze (4.4) in the case when

 1, ck = 0,

with probability p, with probability 1 − p,

where p encodes the probability of initiating communication. This choice in eﬀect leads to a method using a random-length local loop on all devices.

Heterogeneous Data

As in Section C.4.1, our analysis of (4.4) with random length of the local loop relies on Assumption C.4.1. Next lemma provides an upper bound for the weighted sum of E [Vk] in this case.
Lemma C.4.6. Let Assumptions 4.2.1, 4.2.2 and C.4.1 be satisﬁed anda









γ ≤ min  p , p , p 3ρ(1 − ρ)  ,

 16µ 2 (1 − p)((2 + p)F + pF ) 8 2G(1 − p) (p + 2)B + pB 

γ≤

√ p3 .

16 2L(1 − p) (2 + p)A + pA + 2C (pρ+(12−)Bρ)+pB

Then (4.11) holds with

64(1 − p) (p + 2)B + pB (2 + ρ)γ2

H=

3p2ρ

,





8(1 − p)

8D2 (p + 2)B + pB

D3 = p2 (p + 2)D1 + pD1 +

3ρ  .

(C.29)

aWhen ρ = 1 one can always set the parameters in such a way that B = B = C = G = 0, D2 = 0. In this case we assume that ρ2(1B−Cρ) = ρ2(1B−Cρ) = ρ2(1B−Gρ) = ρ2(1B−Gρ) = 0.

Proof. First of all, we introduce new notation:

E[·

|

xk, gk]

d=ef

E[·

|

x

k 1

,

.

.

.

,

x

k n

,

g1k

,

.

.

.

,

g

k n

],

319

E[·

|

xk ]

d=ef

E[·

|

x

k 1

,

.

.

.

,

x

k n

].

By

deﬁnition

of

Vk ,

we

have

E Vk+1 | xk

(A=.15) =
(A=.14)
(A.10),(A.14)
≤
(A.13),(A.14)
≤

1n n EE
i=1

xki +1 − xk+1 2 | xk, gk | xk

1−p n

n

E

i=1

xki − xk − γgik + γgk 2 | xk

1 − p n xk − xk − γg¯k + γg¯k 2

n

i

i

i=1

+ (1 − p)γ2 n E n i=1

gik − g¯ik − (gk − g¯k)

2 | xk

(1 − p) 1 + p n

(1 − p)

1

+

2 p

2

xki − xk 2 +

n i=1 n

+ (1 − p)γ2 n E n i=1

gik − g¯ik 2 | xk

1− p 2

V + (1 − p)(2 + p)γ2 n g¯k 2

k

pn

i

i=1

+ (1 − p)γ2 n E n i=1

gik − g¯ik 2 | xk ,

γ2 n
i=1

g¯ik − g¯k 2

where g¯k = E[gk | xk]. Taking the full expectation we derive

E [Vk+1]

≤

(C.10),(C.11)
≤

1− p 2

(1 − p)(2 + p)γ2 n

E [Vk] +

E

pn i=1

+ (1 − p)γ2 n E n i=1

gik − g¯ik 2

g¯k 2
i

1 − p E [Vk] + 2(1 − p)γ2 2 + p A + A E f (xk) − f (x∗)

2

p

+(1 − p)γ2 2 +p p B + B Eσk2 + 2 +p p F + F EVk

+(1 − p)γ2 2 + p D1 + D1 . p

This inequality together with γ ≤

p

imply

2 (1−p)((2+p)F +pF )

E [Vk+1] ≤

1 − p E [Vk] + 2(1 − p)γ2 2 + p A + A E f (xk) − f (x∗)

4

p

+(1 − p)γ2 2 +p p B + B Eσk2 + (1 − p)γ2 2 +p p D1 + D1 .

320

Unrolling the recurrence, we obtain

E [Vk+1] ≤ 2(1 − p)γ2 2 +p p A + A k 1 − p4 k−l E f (xl) − f (x∗)
l=0
+(1 − p)γ2 2 +p p B + B k 1 − p4 k−l Eσl2
l=0
+(1 − p)γ2 2 +p p D1 + D1 k 1 − p4 k−l .
l=0
As a consequence, we derive

K

2(1 − p) (2 + p)A + pA γ2 K k

p k−l

wkE [Vk] ≤

p 1− p

1 − 4 wkrl

k=0

4

k=0 l=0

(1 − p) +

(2 + p)B + pB

p

1

−

p 4

γ2 K k
k=0 l=0

1− p 4

k−l
wkE σl2

(1 − p) +

(2 + p)D1 + pD1 p

γ2 K k−1
k=0 l=0

1− p 4

k−1−l
wk ,

(C.30)

where we use new notation: rl = E f (xl) − f (x∗) . Recall that wk = (1 − η)−(k+1) and η = min γµ, ρ4 . Together with our assumption on γ it implies that for all 0 ≤ i < k we have

(A.12)
wk = (1 − η)−(k−i+1) (1 − η)−i ≤ wk−i (1 + 2η)i ≤ wk−i (1 + 2γµ)i ≤ wk−i 1 + p8 i ,
wk = (1 − η)−(k−i+1) (1 − η)−i (A≤.12) wk−i (1 + 2η)i ≤ wk−i 1 + ρ2 i , wk (A≤.12) (1 + 2η)k+1 ≤ 1 + ρ2 k+1 .
Having these inequalities in hand we obtain

(C.31) (C.32) (C.33)

Kk k=0 l=0

p k−l 1 − 4 wkrl

(C.31)
≤
(A.13)
≤
=

Kk k=0 l=0

1− p 4

Kk k=0 l=0

1− p 8

8K p wkrk,
k=0

k−l

p

1+ 8

k−l
wlrl ≤

k−l
wlrl
K
wk rk
k=0

∞

pk

1− 8

k=0

321

Kk k=0 l=0

1− p 4

k−l
wkE σl2

(C.31)
≤
(A.13)
≤
≤

Kk k=0 l=0

1− p 4

Kk k=0 l=0

1− p 8

K
wkE σk2
k=0

k−l 1 + p8 k−l wlE σl2
k−l
wlE σl2

∞

pk

1− 8

k=0

= p8 K wkE σk2 ,
k=0

and

K k−1

p k−1−l

1− 4

wk ≤

k=0 l=0

K
wk
k=0

∞

pk

1− 4

k=0

= 4WK . p

Plugging

these

inequalities

together

with

1−

p 4

≥

3 4

in

(C.30),

we

derive

K

64(1 − p) (2 + p)A + pA γ2 K

wkE [Vk] ≤

3p2

wk rk

k=0

k=0

32(1 − p) (2 + p)B + pB γ2 K

+

3p2

wkE σk2

k=0

4(1 − p) (2 + p)D1 + pD1 γ2

+

p2

WK .

(C.34)

It remains to estimate the second term on the right-hand side of this inequality. We notice that an analogous term appears in the proof of Lemma C.4.2. In particular, in that proof inequality (C.19) was shown via inequalities (4.10), (C.32), (C.33) and (A.13) which hold in this case too. Therefore, we get that

K
wkE σk2
k=0

(C.19)
≤

Eσ02(2ρ + ρ) + ρ(14C− ρ) K wkrk + ρ(12G− ρ) K wkEVk + D2ρWK ,

k=0

k=0

whence

K
wkE [Vk]
k=0

(C.34)
≤

64(1 − p)γ2

(2 + p)A + pA + 2C (pρ+(12−)Bρ)+pB 3p2

K
wk rk
k=0

32(1 − p) (p + 2)B + pB (2 + ρ)γ2Eσ02

+

3p2ρ

64G(1 − p) (p + 2)B + pB γ2 K

+

3p2ρ(1 − ρ)

wkE [Vk]

k=0



4(1 − p)γ2

8D2 (p + 2)B + pB

+ p2 (p + 2)D1 + pD1 +

3ρ

  WK .

322

Our assumptions on γ imply

64(1 − p)γ2

(2 + p)A + pA + 2C (pρ+(12−)Bρ)+pB 3p2

Next, we introduce new notation as follows:

≤ 1, 8L

64G(1 − p) (p + 2)B + pB γ2 1

3p2ρ(1 − ρ)

≤ 2.

64(1 − p) (p + 2)B + pB (2 + ρ)γ2

H=

3p2ρ

,





8(1 − p)

8D2 (p + 2)B + pB

D3 = p2 (p + 2)D1 + pD1 +

3ρ  .

Putting all together, we get

21 K wkE [Vk] ≤ 81L K wkrk + H2 Eσ02 + D23 γ2WK ,

k=0

k=0

which concludes the proof.

This lemma and Theorem 4.2.4 imply the following result.

Corollary C.4.7. Let the assumptions of Lemma C.4.6 be satisﬁed. Then Assumption 4.2.3 holds and, in particular, if









 

1

L

p

p 3ρ(1 − ρ)

 

γ ≤ min  2 A + 4B3ρC , F + 4B3ρG , 16µ , 8 2G(1 − p) (p + 2)B + pB  ,





  

√

  

γ ≤ min  p , p 3  ,

 2 (1 − p)((2 + p)F + pF ) 16 2L(1 − p) (2 + p)A + pA + 2C (pρ+(12−)Bρ)+pB 

then for all K ≥ 0 we have E f (xK ) − f (x∗)

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

≤

γWK

+2γ D1 + 4B3ρD2 + 2LγD3 ,

(C.35)

323

where xK d=ef W1K Kk=0 wkxk and

64(1 − p) (p + 2)B + pB (2 + ρ)γ2

H=

3p2ρ

,





8(1 − p)

8D2 (p + 2)B + pB

D3 = p2 (p + 2)D1 + pD1 +

3ρ  .

Moreover, if µ > 0, then

E f (xK ) − f (x∗) ≤

1 − min γµ, ρ 4

K

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

γ

+2γ D1 + 4B3ρD2 + 2LγD3 ,

(C.36)

and in the case when µ = 0, we have

E f (xK ) − f (x∗)

2

x0 − x∗

2

+

8B 3ρ

γ2Eσ02

+

4LH γ Eσ02

≤

γK

+2γ D1 + 4B3ρD2 + 2LγD3 .

(C.37)

ζ-Heterogeneous Data

In this section we assume that f1, f2, . . . , fn are ζ-heterogeneous (see Deﬁnition 4.3.1). Moreover, we additionally assume that E gik | xki = ∇fi(xki ) and we also assume µ-strong convexity of the functions fi for i ∈ [n].

Lemma C.4.8. Let Assumption 4.2.2 be satisﬁed, inequalities (5.8)-(4.10) hold anda

 γ ≤ min  p ,
 8µ 

p 2F (1 − p) ,

pρ(1 − ρ) , 32BG(1 − p)



p

 

.

128L(1 − p)

A

+

2BC ρ(1−ρ)

 

Moreover, assume that f1, f2, . . . , fn are ζ-heterogeneous and µ-strongly convex, and E gik | xki = ∇fi(xki ) for all i ∈ [n]. Then (4.11) holds with

16B(1 − p)(2 + ρ)γ2

4(1 − p)

ζ2 4BD2

H=

, D3 =

D1 + +

.

pρ

p

γµ ρ

(C.38)

aWhen ρ = 1 one can always set the parameters in such a way that B = C = G = 0, D2 = 0. In this case we assume that ρ2(1B−Cρ) = ρ2(1B−Gρ) = 0.

Proof. First of all, we introduce new notation: E[· | xk, gk] d=ef E[· | xk1, . . . , xkn, g1k, . . . , gnk]. By

324

deﬁnition of Vk for all k ≥ 1 we have

E[Vk | xk−1, gk−1]

(4.4)=,(C.4) =
= =

1 − p n xk−1 − xk−1 − γgk−1 + γgk−1 2

n

i

i

i=1

1−p n n i=1

xk−1 − xk−1 2 + 2γ(1 − p) n

i

n

i=1

xki −1 − xk−1, gk−1 − gik−1

+ γ2(1 − p) n gk−1 − gk−1 2

n

i

i=1

(1 − p)Vk−1 + 2γ(1 − p)

1 n k−1 k−1 k−1 n xi − x , g
i=1

+ 2γ(1 − p) n n i=1

xk−1 − xki −1, gik−1

+ γ2(1 − p) n n i=1

gik−1 − gk−1 2

2γ(1 − p) n (1 − p)Vk−1 +
n

xk−1 − xki −1, gik−1

i=1

+ γ2(1 − p) n gk−1 − gk−1 2.

n

i

i=1

Next, we take the conditional expectation E · | xk−1 d=ef E · | xk1−1, . . . , xkn−1 on both sides of the obtained inequality and get

E Vk | xk−1

=
(A.14)
≤

2γ(1 − p) n (1 − p)Vk−1 +
n

xk−1 − xki −1, ∇fi(xki −1)

i=1

+ γ2(1 − p) n E n i=1

gik−1 − gk−1 2 | xk−1

2γ(1 − p) n (1 − p)Vk−1 +
n

xk−1 − xki −1, ∇fi(xki −1) − ∇fi(xk−1)

i=1

+ 2γ(1 − p) n n

xk−1 − xki −1, ∇fi(xk−1)

i=1

+ γ2(1 − p) n E n i=1

gik−1 2 | xk−1 .

325

Since

1 n

n i=1

xk−1

− xki −1, ∇f (xk−1)

= 0, we can continue as follows:

E Vk | xk−1

(C.24)
≤
(A.7)
≤
(4.15)
≤

2γµ(1 − p) n (1 − p)Vk−1 −
n

xk−1 − xki −1 2

i=1

+ 2γ(1 − p) n n

xk−1 − xki −1, ∇fi(xk−1) − ∇f (xk−1)

i=1

+ γ2(1 − p) n E n i=1

gik−1 2 | xk−1

γ2(1 − p) n

(1 − p)(1 − 2γµ)Vk−1 +

E

n i=1

gik−1 2 | xk−1

+ 2γ(1 − p) n n i=1

µ2 xk−1 − xki −1 2 + 21µ ∇fi(xk−1) − ∇f (xk−1) 2

γ2(1 − p) n

(1 − p)(1 − γµ)Vk−1 +

E

n i=1

gik−1 2 | xk−1 + (1 −µp)γζ2 .

Taking full mathematical expectation on both sides of previous inequality and using 1 − γµ ≤ 1 we obtain
EVk (A≤.15) (1 − p)E [Vk−1] + γ2(1n− p) n E gik−1 2 + (1 −µp)γζ2
i=1 (4.8)
≤ (1 − p)E[Vk−1] + (1 − p)γ2 2AE[f (xk−1) − f (x∗)] + BE[σk2] + F E[Vk−1] + D1 + (1 − p)γζ2 . µ

Since γ ≤

p 2F (1−p)

we

have

(1 − p)γ2F

≤

p 2

and

EVk ≤ 1 − p2 E[Vk−1] + (1 − p)γ2 2AE[f (xk−1) − f (x∗)] + BE[σk2] + D1 + γζµ2 .

Unrolling the recurrence we obtain

E [Vk] ≤ (1 − p)γ2 k−1 1 − p2 k−1−l 2AE f (xl) − f (x∗) + BE σl2 + D1 + γζµ2 .
l=0

As a consequence, we derive

K

2A(1 − p)γ2 K k

p k−l

wkE [Vk] ≤

1− p

1 − 2 wkrl

k=0

2

k=0 l=0

B(1 − p)γ2 K k + 1− p
2 k=0 l=0

1− p 2

k−l
wkE σl2

+ D1 + γζµ2 (1 − p)γ2 K k−1 1 − p2 k−1−l wk,
k=0 l=0

(C.39)

326

where we use new notation: rl = E f (xl) − f (x∗) . Recall that wk = (1 − η)−(k+1) and η = min γµ, ρ4 . Together with our assumption on γ it implies that for all 0 ≤ i < k we have

(A.12)
wk = (1 − η)−(k−i+1) (1 − η)−i ≤ wk−i (1 + 2η)i ≤ wk−i (1 + 2γµ)i ≤ wk−i 1 + p4 i ,
wk = (1 − η)−(k−i+1) (1 − η)−i (A≤.12) wk−i (1 + 2η)i ≤ wk−i 1 + ρ2 i , wk (A≤.12) (1 + 2η)k+1 ≤ 1 + ρ2 k+1 .
Having these inequalities in hand we obtain

(C.40) (C.41) (C.42)

Kk k=0 l=0

p k−l 1 − 2 wkrl

(C.40)
≤
(A.13)
≤
=

Kk k=0 l=0

1− p 2

Kk k=0 l=0

1− p 4

4K p wkrk,
k=0

k−l

p

1+ 4

k−l
wlrl ≤

k−l
wlrl
K
wk rk
k=0

∞

pk

1− 4

k=0

Kk k=0 l=0

1− p 2

k−l
wkE σl2

(C.40)
≤
(A.13)
≤
≤

Kk k=0 l=0

1− p 2

Kk k=0 l=0

1− p 4

K
wkE σk2
k=0

k−l 1 + p4 k−l wlE σl2
k−l
wlE σl2

∞

pk

1− 4

k=0

= p4 K wkE σk2 ,
k=0

and

K k−1

p k−1−l

1− 2

wk ≤

k=0 l=0

K
wk
k=0

∞

pk

1− 2

k=0

= 2WK . p

Plugging

these

inequalities

together

with

1−

p 2

≥

1 2

in

(C.39)

we

derive

K wkE [Vk] ≤ 16A(1p− p)γ2 K wkrk + 8B(1 p− p)γ2 K wkE σk2

k=0

k=0

k=0

2 D1 + ζ2 (1 − p)γ2

+

γµ

WK .

p

(C.43)

It remains to estimate the second term in the right-hand side of this inequality. We notice that an analogous term appear in the proof of Lemma C.4.2. In particular, in that proof inequality (C.19) was shown via inequalities (4.10), (C.32), (C.33) and (A.13) which hold in this case too.

327

Therefore, we get that

K
wkE σk2
k=0

(C.19)
≤

Eσ02(2ρ + ρ) + ρ(14C− ρ) K wkrk + ρ(12G− ρ) K wkEVk + D2ρWK ,

k=0

k=0

hence

K
wkE [Vk]
k=0

(C.34)
≤

16(1 − p)γ2

A

+

2BC ρ(1−ρ)

p

K
wk rk
k=0

+ 8B(1 − p)(p2ρ+ ρ)γ2Eσ02 + 16BpρG((11−−ρp))γ2 K wkE [Vk]
k=0

2(1 − p)γ2

ζ2 4BD2

+

D1 + +

WK .

p

γµ ρ

Our assumption on γ imply

16(1 − p)γ2

A

+

2BC ρ(1−ρ)

p

≤ 1, 8L

16BG(1 − p)γ2 1 pρ(1 − ρ) ≤ 2 .

Next, we introduce new notation as follows:

16B(1 − p)(2 + ρ)γ2

4(1 − p)

ζ2 4BD2

H=

, D3 =

D1 + +

.

pρ

p

γµ ρ

Putting all together we get

12 K wkE [Vk] ≤ 81L K wkrk + H2 Eσ02 + D23 γ2WK

k=0

k=0

which concludes the proof.

This lemma and Theorem 4.2.4 imply the following result.

Corollary C.4.9. Let the assumptions of Lemma C.4.8 are satisﬁed. Then Assumption 4.2.3 holds and, in particular, if

γ ≤ min

1 , L , p , M = 4B ,

2(A + CM ) F + GM 8µ

3ρ





 
γ ≤ min

p ,

 2F (1 − p)



pρ(1 − ρ) , 32BG(1 − p)

p

 

,

128L(1 − p)

A

+

2BC ρ(1−ρ)

 

then for all K ≥ 0 we have E f (xK ) − f (x∗)

≤ 2T 0 + 4LHγEσ02 + 2γ D + M D2 + 2LγD3 ,

γWK

1

(C.44)

328

where xK d=ef W1K Kk=0 wkxk and

16B(1 − p)(2 + ρ)γ2

4(1 − p)

ζ2 4BD2

H=

, D3 =

D1 + +

.

pρ

p

γµ ρ

Moreover, if µ > 0, then E f (xK ) − f (x∗) ≤

1 − min γµ, ρ 4

K 2T 0 + 4LHγEσ02 γ

+2γ D1 + M D2 + 2LγD3 ,

and in the case when µ = 0 we have E f (xK ) − f (x∗) ≤ 2T 0 + 4γLKHγEσ02 + 2γ D1 + M D2 + 2LγD3 .

(C.45) (C.46)

C.5 Missing Parts from Section 4.4

Let us start with an useful Lemma that bounds the Bregman distance between the local iterate xki and the optimum x∗ by the Bregman distance between the virtual iterate xk and the optimum.
Lemma C.5.1. Assume fi is L-smooth for all i ∈ [n]. Then

Dfi (xki , x∗) ≤ 2Dfi (xk, x∗) + L xki − xk 2 ∀i ∈ [n].

(C.47)

Proof. Using corollaries of L-smoothness and Young’s inequality, we derive

D

f

i

(

x

k i

,

x∗

)

(C.2)
≤
(A.7)
≤
(4.6)
≤

Dfi (xk, x∗) + ∇fi(xk) − ∇fi(x∗), xki − xk + L2 xki − xk 2 Dfi (xk, x∗) + 21L ∇fi(xk) − ∇fi(x∗) 2 + L xki − xk 2
2Dfi (xk, x∗) + L xki − xk 2.

329

C.5.1 Proof of Lemma 4.4.3

n

Let

us

bound

1 n

Ek

i=1

gik 2 ﬁrst:

1n n Ek
i=1

gik 2

= = ≤ ≤
(C.47)
≤
≤

1n n Ek
i=1

aki − bki 2

1n n Ek
i=1

aki − ∇fi(x∗) − (bki − ∇fi(x∗)) 2

2n n Ek
i=1

aki − ∇fi(x∗) 2 + bki − ∇fi(x∗) 2

2n n

2AiDfi (xki , x∗) + Biσi2,k + D1,i + Ek

i=1

bki − ∇fi(x∗) 2

2n n

4AiDfi (xk, x∗) + 2AiL xki − xk 2 + Biσi2,k

i=1

+2 n n i=1

D1,i + Ek

bki − ∇fi(x∗) 2

8 max{Ai}(f (xk) − f (x∗)) + 4 max{Ai}LVk

i

i

+ n2 n Biσi2,k + D1,i + Ek bki − ∇fi(x∗) 2 .
i=1

Taking the full expectation, we arrive at

1n nE
i=1

gik 2

≤ 8 max{Ai}E(f (xk) − f (x∗)) + 4 max{Ai}LEVk

i

i

+2 n n

BiEσi2,k + D1,i + E bki − ∇fi(x∗) 2 .

i=1

(C.48)

Next, we have

330

 1n

2
k

 1n k

2
k

Ek  n gi  = Ek  n ai − bi 

i=1

i=1

 1n k

2
∗

= Ek  n ai − ∇fi(x ) 

i=1

1n k

∗

1n

k

2 ∗

= Var n ai − ∇fi(x ) + n ∇fi(xi ) − ∇fi(x )

i=1

i=1

≤ Var n1 n aki − ∇fi(x∗) + n1 n ∇fi(xki ) − ∇fi(x∗) 2

i=1

i=1

≤ Var n1 n aki − ∇fi(x∗) + 2nL n Dfi (xki , x∗)

i=1

i=1

= n12 n Var aki − ∇fi(x∗) + 2nL n Dfi (xki , x∗)

i=1

i=1

≤ n12 n Ek aki − ∇fi(x∗) 2 + 2nL n Dfi (xki , x∗)

i=1

i=1

≤ n12 n 2AiDfi (xki , x∗) + Biσi2,k + D1,i + 2nL n Dfi (xki , x∗)

i=1

i=1

≤ 1 n 2 max{A } + nL D (xk, x∗) + B σ2 + D

n2

i i

fi i

i i,k

1,i

i=1

(C.47)
≤
=

4 maxi{Ai} + 2L Df (xk, x∗) n

1n + n2
i=1

2(max{Ai}L + nL2) xki − x∗ 2 + Biσi2,k + D1,i
i

4 maxi{Ai} + 2L n

f (xk) − f (x∗) + 2 maxi{Ai}L + L2 Vk n

1n + n2

Biσi2,k + D1,i .

i=1

Further, we deﬁne

ωk2 d=ef n2 n Biσi2,k
i=1

(C.49)

331

and consequently, we get

E ωk2+1

=
≤
(C.47)
≤
≤

2n

2

n BiE σi,k+1

i=1

(1 − ρ)ωk2 + n2 n BiCiDfi (xki , x∗) + n2
i=1

(1 − ρ)ωk2 + n4 n BiCiDfi (xk, x∗) + n2
i=1

(1 − ρ)ωk2 + 4 max{BiCi}Df (xk, x∗) +
i

n
BiD2,i
i=1 n
BiCiL
i=1

xki − xk

2 max{BiCi}LVk +
i

2 + n2 n BiD2,i
i=1
2n n BiD2,i.
i=1

We will provide a bound on E bki − ∇fi(x∗) 2 based on the choices of bki :

Case I. The choice bki = 0 yields E bki − ∇fi(x∗) 2 = ∇fi(x∗) 2.

Case II. The choice bki = ∇fi(x∗) yields E bki − ∇fi(x∗) 2 = 0. Overall, for both Case I and II we have

Eσ2 ≤ (1 − ρ)Eσ2 + 4 max{B C }D (xk, x∗) + 2 max{B C }LV + 2 n B D

k+1

k

i ii f

i

ii k n

i 2,i

i=1

as desired, where σk = ωk.

Case

III.

The

choice

bki

= hki −

1 n

n i=1

hki

yields

1n k

∗ 2 1n k 1n k

2 ∗

1n

k

∗2

n

bi − ∇fi(x ) = n

hi − n

hi − ∇fi(x )

≤ n

hi − ∇fi(x )

i=1

i=1

i=1

i=1

where

Ek hki +1 − ∇fi(x∗) 2

=
(4.16)
≤

(1 − ρi) hki − ∇fi(x∗) 2 + ρiEk lik − ∇fi(x∗) 2 (1 − ρi) hki − ∇fi(x∗) 2 + 2ρiAiDfi (xki , x∗) + ρiD3,i.

Next, set σk2 d=ef ωk2 + hki − ∇fi(x∗) 2 for this case. Consequently, we have

Ekσk2+1 ≤ (1 − ρ)σk2 + 4(max{BiCi} + max{ρiAi})Df (xk, x∗) + 2(max{BiCi}

i

i

i

1n

+

max{ρiAi})LVk
i

+

n

2BiD2,i + ρiD3,i ,

i=1

where ρ = mini min{ρi, ρi}. It remains to plug everything back to (4.8), (4.9) and (4.10).

332

Table C.4: The parameters for which the methods from Table 4.2 satisfy Assumption 4.2.3/C.4.1. Absolute constants were omitted. The meaning of the expressions appearing in the table, as well as their justiﬁcation, is detailed in Section 4.5. UBV stands for the “Uniform Bound on the Variance” of local stochastic gradient, which is often assumed when fi is of the form (5.5). ES stands for the “Expected Smoothness” inequality [63], which does not impose any extra assumption on the objective/noise, but rather can be derived given the sampling strategy and the smoothness structure of fi. Consequently, such a setup allows us to obtain local methods with importance sampling. Next, the simple setting is a special case of ES when we uniformly sample a single index on each node each iteration.
333

Method, Setting Local-SGD UBV, ζ-Het. Local-SGD UBV, Het. Local-SGD ES, ζ-Het. Local-SGD ES, Het. Local-SVRG
simple, ζ-Het. Local-SVRG simple, Het.
S*-Local-SGD UBV, Het.
SS-Local-SGD UBV, Het.,
p = q, r = 1/p SS-Local-SGD
ES, Het., p = q, r = 1/p S*-Local-SGD*
simple, Het. S-Local-SVRG simple, Het., q = m1 , m ≥ p1

A, A, A, A L, −, −, L

B, B, B, B 0, −, −, 0

−, L, 0, L

−, 0, 0, 0

L, −, −, Ln + L 0, −, −, 0

−, L, L, Ln + L −, 0, 0, 0

max Lij , −, −, maxnLij + L
−, L, max Lij , maxnLij + L

1, −, −, n1 −, 0, 1, n1

−, L, 0, L

−, 0, 0, 0

−, L, 0, L

−, 1, 0, 0

−, L, L, Ln + L −, 1, 0, 0

−, L, max Lij , maxnLij + L

−, 0, 0, 0

−, L, max Lij , maxnLij + L

−, 1, 1, n1

ρ

C

1

0

F, F, F, F L2, −, −, L2

G

D1, D1, D1, D1, D2, D3

0

σn2 , σ2 + ζ∗2, −, −, 0,

τσ2 + τ2ζ2

1

0

−, L2, 0, L2

0

1 0 LL, −, −, LnL + L2 0

σn2 , −, ζ∗2, σ2, 0, (τ − 1)σ2 + (τ − 1)2ζ∗2
σn∗2 , σ∗2 + ζ∗2, −, −, 0, (τ − 1) σ∗2 + ζ∗2 + γζµ2

1 0 −, L2, LL, LnL + L2 0

q max Lij q

max LijL, −, −, maxnLij L + L2

max Lij Lq

σn∗2 , −, ζ∗2, σ∗2, 0, (τ − 1)σ∗2 + (τ − 1)2ζ∗2
0, ζ∗2, −, −, 0, (τ − 1) ζ∗2 + γζµ2

q max Lij q

−, L2, max Lij L, maxnLij L + L2

max Lij Lq

0, −, ζ∗2, 0, 0, (τ − 1)2ζ∗2

1

0

−, L2, 0, l2

0

σ2 , −, 0, σ2, (τ − 1)σ2

n

p

Lp

−, L2, 0, L2

0

σ2 , −, pσ2, σ2, 0, (1−p)σ2

n

p

p Lp + Lp2 −, L2, LL, LnL + L2 0 σn∗2 , −, 0, σ∗2, p2σ∗2, (1−pp)σ∗2

−, L2, max Lij L,

p

0

L max Lij + L2

0

n

1

L+max Lij

−, L2, max Lij L,

m

m

L max Lij + L2

0

n

0, −, 0, 0, 0, 0 0, −, 0, 0, 0, 0

Appendix D
Appendix for Chapter 5

D.1 Missing Proofs for MARINA

D.1.1 Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 5.2.1 together with the proof of this result.

Theorem D.1.1 (Theorem 5.2.1). Let Assumptions 5.1.1 and 5.1.2 be satisﬁed and

γ≤ 1 , L 1 + (1−pnp)ω

(D.1)

where

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

MARINA

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 , γK

(D.2)

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is,

after

K = O ∆0L 1 + ε2

(1 − p)ω pn

(D.3)

iterations MARINA produces such a point xˆK that E[ ∇f (xˆK) 2] ≤ ε2. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O d + ∆0L 1 + ε2

(1 − p)ω (pd + (1 − p)ζQ) , pn

(D.4)

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof of Theorem 5.2.1. The scheme of the proof is similar to the proof of Theorem 1 from [118].

334

From Lemma A.5.7, we have

E[f (xk+1)] ≤ E[f (xk)]− γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2 + γ E gk − ∇f (xk) 2 .

2

2γ 2

2

(D.5)

Next, we need to derive an upper bound for E gk+1 − ∇f (xk+1) 2 . By deﬁnition of gk+1, we

have



∇f (xk+1)

gk+1 =

n

gk 

+

1 n

Q

i=1

∇fi(xk+1) − ∇fi(xk)

with probability p, with probability 1 − p.

Using this, variance decomposition (A.14) and tower property (A.15), we derive:

E gk+1 − ∇f (xk+1) 2

(A=.15) (A.15)=,(A.14)


k

1n

k+1

k

2
k+1

(1 − p)E  g +

Q ∇fi(x ) − ∇fi(x ) − ∇f (x ) 

n i=1

 1n

k+1

k

2

k+1

k

(1 − p)E 

Q ∇fi(x ) − ∇fi(x ) − ∇f (x ) + ∇f (x ) 

n i=1

+(1 − p)E gk − ∇f (xk) 2 .

Since Q ∇f1(xk+1) − ∇f1(xk) , . . . , Q ∇fn(xk+1) − ∇fn(xk) are independent random vectors for ﬁxed xk and xk+1 we have

E gk+1 − ∇f (xk+1) 2

 1n

k+1

k

2

k+1

k

= (1 − p)E 

Q ∇fi(x ) − ∇fi(x ) − ∇fi(x ) + ∇fi(x ) 

n i=1

+(1 − p)E gk − ∇f (xk) 2

= 1 n−2 p n E Q ∇fi(xk+1) − ∇fi(xk) − ∇fi(xk+1) + ∇fi(xk) 2
i=1
+(1 − p)E gk − ∇f (xk) 2

(A≤.6) (1 −n2p)ω n E ∇fi(xk+1) − ∇fi(xk) 2 + (1 − p)E gk − ∇f (xk) 2 .
i=1

Using L-smoothness (5.2) of fi together with the tower property (A.15), we obtain

E gk+1 − ∇f (xk+1) 2

≤ (1 −n2p)ω n L2i E xk+1 − xk 2 + (1 − p)E gk − ∇f (xk) 2
i=1

=

(1 − p)ωL2 E

xk+1 − xk 2 + (1 − p)E

gk − ∇f (xk) 2 (.D.6)

n

Next, we introduce a new notation:

Φk

=

f (xk) − f∗

+

γ 2p

gk − ∇f (xk) 2.

Using this and

335

inequalities (D.5) and (D.6), we establish the following inequality:

E [Φk+1] ≤ E f (xk) − f∗ − γ ∇f (xk) 2 − 1 − L xk+1 − xk 2 + γ gk − ∇f (xk) 2

2

2γ 2

2

+ γ E (1 − p)ωL2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

2p

n

= E [Φk] − γ2 E ∇f (xk) 2 + γ(1 −2ppn)ωL2 − 21γ + L2 E xk+1 − xk 2

(D.1)

γ

≤ E [Φk] − 2 E

∇f (xk) 2 ,

(D.7)

where in the last inequality, we use γ(1−2ppn)ωL2 − 21γ + L2 ≤ 0 following from (D.1). Summing up inequalities (D.7) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 KE
k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤

(E[Φk] − E[Φk+1]) =

=,

γK k=0

γK

γK

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (A.15) and the deﬁnition of xˆK, we obtain (D.2) that implies (D.3) and (D.4).

Corollary D.1.2 (Corollary 5.2.2). Let the assumptions of Theorem 5.2.1 hold and p = ζdQ , where ζQ is the expected density of the quantization (see Def. A.2.1). If
γ≤ 1 , L 1 + ωn ζdQ − 1

then MARINA requires

K = O ∆0L 1 + ω d − 1

ε2

n ζQ

iterations/communication rounds to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is





O d + ∆ε02L ζQ +

 ωζQ (d − ζQ)
n

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 5.2.2. The choice of p = ζdQ implies

1 − p = d − 1,

p

ζQ

pd + (1 − p)ζQ ≤ ζQ + 1 − ζQ · ζQ ≤ 2ζQ. d

336

Plugging these relations in (D.1), (D.3), and (D.4), we get that if
γ≤ 1 , L 1 + ωn ζdQ − 1

then MARINA requires

K = O ∆0L 1 + ε2
= O ∆0L 1 + ε2

(1 − p)ω pn
ω d −1 n ζQ

iterations/communication rounds in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

d + K(pd + (1 − p)ζQ) = O d + ∆0L 1 + ε2

(1 − p)ω (pd + (1 − p)ζQ) pn





= O d + ∆ε02L ζQ +

 ωζQ (d − ζQ)
n

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.1.2 Convergence Results Under Polyak-Łojasiewicz Condition

In this section, we provide the full statement of Theorem 5.2.5 together with the proof of this result.

Theorem D.1.3 (Theorem 5.2.5). Let Assumptions 5.1.1, 5.1.2 and 5.2.4 be satisﬁed and


   γ ≤ min
 L

1 1 + 2(1p−np)ω





p

 

, 2µ  ,





(D.8)

where

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

MARINA

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(D.9)

where ∆0 = f (x0) − f (x∗). That is, after

K = O max 1 , L 1 + (1 − p)ω

pµ

pn

log ∆0 ε

(D.10)

iterations MARINA produces such a point xK that E[f (xK) − f (x∗)] ≤ ε. Moreover, under

337

an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O d + max 1 , L 1 + (1 − p)ω

pµ

pn

(pd + (1 − p)ζQ) log ∆0 , ε

(D.11)

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof of Theorem 5.2.5. The proof is very similar to the proof of Theorem 5.2.1. From Lemma A.5.7 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2

2

2γ 2

+ γ E gk − ∇f (xk) 2 2

(5.3)
≤ (1 − γµ)E f (xk) − f (x∗) −

1 − L E xk+1 − xk 2

2γ 2

+ γ E gk − ∇f (xk) 2 . 2

(D.12)

Using the same arguments as in the proof of (D.6), we obtain

E gk+1 − ∇f (xk+1) 2

≤

(1 − p)ωL2 E

xk+1 − xk 2 + (1 − p)E

gk − ∇f (xk) 2 .

n

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk) − f (x∗) +

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1] ≤ E (1 − γµ)(f (xk) − f (x∗)) − 1 − L xk+1 − xk 2 + γ gk − ∇f (xk) 2

2γ 2

2

+ γ E (1 − p)ωL2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

p

n

= E (1 − γµ)(f (xk) − f (x∗)) + γ + γ (1 − p) gk − ∇f (xk) 2 2p

+ γ(1 − p)ωL2 − 1 + L E xk+1 − xk 2

pn

2γ 2

(D.8)
≤ (1 − γµ)E[Φk],

where in the last inequality, we use γ(1−ppn)ωL2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (D.8). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 = (1 − γµ)K (f (x0) − f (x∗))

that implies (D.10) and (D.11).

338

Corollary D.1.4. Let the assumptions of Theorem 5.2.5 hold and p = ζdQ , where ζQ is the expected density of the quantization (see Def. A.2.1). If


   γ ≤ min
 L 1 +

1 2nω ζdQ − 1





p

 

, 2µ  ,





then MARINA requires

K = O max d , L 1 + ω d − 1

ζQ µ

n ζQ

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is





L O d + max d, ζQ +
µ





ωζQ (d − ζQ) log ∆0 

n

ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = ζdQ implies

1 − p = d − 1,

p

ζQ

pd + (1 − p)ζQ ≤ ζQ + 1 − ζQ · ζQ ≤ 2ζQ. d

Plugging these relations in (D.8), (D.10), and (D.11), we get that if


   γ ≤ min
 L 1 +

1 2nω ζdQ − 1





p

 

, 2µ  ,





then MARINA requires

K = O max 1 , L 1 + (1 − p)ω

pµ

pn

log ∆0 ε

= O max d , L 1 + ω d − 1

ζQ µ

n ζQ

log ∆0 ε

iterations/communication rounds in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected

339

total communication cost per worker is

d + K(pd + (1 − p)ζQ) = O d + max 1 , L 1 + (1 − p)ω

pµ

pn

(pd + (1 − p)ζQ) log ∆0 ε





L = O d + max d, ζQ +
µ





ωζQ (d − ζQ) log ∆0 

n

ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.2 Missing Proofs for VR-MARINA

D.2.1 Finite Sum Case

Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 5.3.2 together with the proof of this result.

Theorem D.2.1 (Theorem 5.3.2). Consider the ﬁnite sum case (5.1)+(5.4). Let Assumptions 5.1.1, 5.1.2 and 5.3.1 be satisﬁed and

γ≤ L+

1, 1p−np ωL2 + (1+bω)L2

(D.13)

where

L2

=

1 n

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 , γK

(D.14)

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is,

after

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

(D.15)

iterations VR-MARINA produces such a point xˆK that E[ ∇f (xˆK) 2] ≤ ε2, and the expected total number of stochastic oracle calls per node equals





m+K(pm+2(1−p)b ) = O m + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pm + (1 − p)b ) .

pn

b

(D.16)

Moreover, under an assumption that the communication cost is proportional to the number

of non-zero components of transmitted vectors from workers to the server, we have that the

340

expected total communication cost per worker equals





d + K(pd + (1 − p)ζQ) = O d + ∆ε20 L +

1 − p ωL2 + (1 + ω)L2

pn

b

where ζQ is the expected density of the quantization (see Def. A.2.1).





 (pd + (1 − p)ζQ) ,

(D.17)

Proof of Theorem 5.3.2. The proof of this theorem is a generalization of the proof of Theorem 5.2.1. From Lemma A.5.7, we have

E[f (xk+1)] ≤ E[f (xk)]− γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2 + γ E gk − ∇f (xk) 2 .

2

2γ 2

2

Next, we need to derive an upper bound for E

(D.18)

n

gk+1 − ∇f (xk+1)

2

.

Since

gk+1 =

1 n

gik+1,

i=1

we get the following representation of gk+1:



∇f (xk+1) 

 gk+1 =


n



gk

+

1 n

Q  b1

(∇fij(xk+1) − ∇fij(xk))

 i=1 j∈Ii,k

with probability p, with probability 1 − p.

Using this, variance decomposition (A.14) and tower property (A.15), we derive:

E gk+1 − ∇f (xk+1) 2







2

(A=.15) (1 − p)E  gk + 1 n Q  1



n

b

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) 





i=1

j∈Ii,k







2

(A.15),(A.14)

1n

1

= (1 − p)E 

Q

n

b

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk) 





i=1

j∈Ii,k

+(1 − p)E gk − ∇f (xk) 2 .

Next,

we

use

the

notation:

∆ki

=

1 b

(∇fij(xk+1) − ∇fij(xk)) and ∆ki = ∇fi(xk+1) − ∇fi(xk).

j∈Ii,k

These vectors satisfy E ∆ki | xk, xk+1

= ∆ki

for

all

i ∈ [n].

Moreover,

Q(

∆k1

),

.

.

.

,

Q(∆

k n

)

are

341

independent random vectors for ﬁxed xk and xk+1. These observations imply

E gk+1 − ∇f (xk+1) 2

= = (A.15)=,(A.14) (A.15=),(A.6) (A.15)=,(A.14)

 1n

2

k

k

(1 − p)E  n Q(∆i ) − ∆i 

i=1

+(1 − p)E gk − ∇f (xk) 2

1−p n

n2

E

i=1

Q(∆ki ) − ∆ki + ∆ki − ∆ki 2

+(1 − p)E gk − ∇f (xk) 2

1−p n

n2

E

i=1

+(1 − p)E

Q(∆ki ) − ∆ki 2 + E gk − ∇f (xk) 2

∆ki − ∆ki 2

1−p n

n2

ωE

i=1

∆ki 2 + E

∆ki − ∆ki 2

+(1 − p)E gk − ∇f (xk) 2

1−p n

n2

ωE

i=1

∆ki 2 + (1 + ω)E

+(1 − p)E gk − ∇f (xk) 2 .

∆ki − ∆ki 2

Using L-smoothness (5.2) and average L-smoothness (5.6) of fi together with the tower property (A.15), we get

E gk+1 − ∇f (xk+1) 2

1−p n ≤ n2

ωL2i + (1 +bω)L2i E xk+1 − xk 2

i=1

+(1 − p)E gk − ∇f (xk) 2

= 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E gk − ∇f (xk) 2 .

(D.19)

Next,

we

introduce

new

notation:

Φk

=

f

(

x

k

)

−

f∗

+

γ 2p

gk −∇f (xk)

2. Using this and inequalities

342

(D.18) and (D.19), we establish the following inequality:

E [Φk+1]

≤
=
(D.13)
≤

E f (xk) − f∗ − γ ∇f (xk) 2 − 1 − L

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

2p n

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

E [Φk] − γ E ∇f (xk) 2 2

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2

2pn

b

2γ 2

E [Φk] − γ E ∇f (xk) 2 , 2

(D.20)

where in the last inequality, we use γ(21p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 following from (D.13). Summing up inequalities (D.20) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 KE
k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤

(E[Φk] − E[Φk+1]) =

=,

γK k=0

γK

γK

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (A.15) and the deﬁnition of xˆK, we obtain (D.14) that implies (D.15), (D.16), and (D.17).

Remark D.2.2 (About batchsizes dissimilarity). We notice that our analysis can be easily

extended to handle the version of VR-MARINA with diﬀerent batchsizes b1, . . . , bn on diﬀerent

workers,

i.e.,

when

|Ii,k |

=

bi

and

∆ki

=

1 b

j∈I (∇fij(xk+1) − ∇fij(xk)). In this case, the

i

i,k

statement of Theorem 5.3.2 remains the same with the small modiﬁciation: instead of Lb2 the

complexity

bounds

will

have

1 n

n i=1

Lb2i .

i

Corollary D.2.3 (Corollary 5.3.3). Let the assumptions of Theorem 5.3.2 hold and p =

min ζdQ , mb+b , where b ≤ m and ζQ is the expected density of the quantization (see

Def. A.2.1). If

γ≤ L+

1, max{d/ζQn−1,m/b } ωL2 + (1+bω)L2

then VR-MARINA requires

  O  ∆ε20 L 1 +

 ω max {d/ζQ − 1, m/b }  + L
n

 (1 + ω) max {d/ζQ − 1, m/b }
 nb

343

iterations/communication rounds,





O m + ∆ε20 L b +

 ω max {(d/ζQ − 1)(b )2, mb } + L
 n

 (1 + ω) max {(d/ζQ − 1)b , m}
 n

stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is





O d + ∆ε02ζQ L 1 +

 ω max {d/ζQ − 1, m/b }  + L
n

 (1 + ω) max {d/ζQ − 1, m/b }
 nb

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 5.3.3. The choice of p = min ζdQ , mb+b implies

1 − p = max d − 1, m ,

p

ζQ

b

pm + (1 − p)b ≤ 2mb ≤ 2b , m+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (D.13), (D.15), (D.16) and (D.17) and using a + b ≤ a + b, we

get that if

γ≤ L+

1, max{d/ζQn−1,m/b } ωL2 + (1+bω)L2

then VR-MARINA requires

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

 = O  ∆ε20 L +



L2 ω max {d/ζQ − 1, m/b } + L2 (1 + ω) max {d/ζQ − 1, m/b } 

n

nb

  = O  ∆ε20 L 1 +

 ω max {d/ζQ − 1, m/b }  + L
n

 (1 + ω) max {d/ζQ − 1, m/b }
 nb

344

iterations/communication rounds and





m + K(pm + 2(1 − p)b ) = O m + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pm + (1 − p)b )

pn

b

= O m + ∆0 L 1 + ε2

ω max {d/ζQ − 1, m/b } n

+L (1 + ω) max {d/ζQ − 1, m/b } b nb

= O m + ∆0 L b + ε2

ω max {(d/ζQ − 1)(b )2, mb } n

+L (1 + ω) max {(d/ζQ − 1)b , m} n

stochastic oracle calls per node in expectation in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is





d + K(pd + (1 − p)ζQ) = O d + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pd + (1 − p)ζQ)

pn

b

= O d + ∆0ζQ L 1 + ε2

ω max {d/ζQ − 1, m/b } n

+L (1 + ω) max {d/ζQ − 1, m/b } nb

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide an analysis of VR-MARINA under the Polyak-Łojasiewicz condition in the ﬁnite sum case.

Theorem D.2.4. Consider the ﬁnite sum case (5.1)+(5.4). Let Assumptions 5.1.1, 5.1.2, 5.3.1 and 5.2.4 be satisﬁed and



  

γ ≤ min

  

L

+

2(1−p) pn

1 ωL2 + (1+bω)L2





p

 

, 2µ  ,





(D.21)

where

L2

=

1 n

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(D.22)

345

where ∆0 = f (x0) − f (x∗). That is, after



  1

L+

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆

log

0 

µ

 ε





(D.23)

iterations VR-MARINA produces such a point xK that E f (xK) − f (x∗) ≤ ε, and the expected total number of stochastic oracle calls per node m + K(pm + 2(1 − p)b ) equals





  1

L+

O m + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pm + (1 − p)b ) log

0

 

.

µ



ε





(D.24)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server we have that the expected total communication cost per worker d + K(pd + (1 − p)ζQ) equals





  1

L+

O d + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pd + (1 − p)ζQ) log

0

 

,

µ



ε





(D.25)

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof. The proof is very similar to the proof of Theorem 5.3.2. From Lemma A.5.7 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2

2

2γ 2

+ γ E gk − ∇f (xk) 2 2

(5.3)
≤ (1 − γµ)E f (xk) − f (x∗) −

1 − L E xk+1 − xk 2

2γ 2

+ γ E gk − ∇f (xk) 2 . 2

Using the same arguments as in the proof of (D.19), we obtain

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E gk − ∇f (xk) 2 .

346

Putting

all

together

we

derive

that

the

sequence

Φk

=

f (xk)

−

f (x∗)

+

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤
=
(D.21)
≤

E (1 − γµ)(f (xk) − f (x∗)) − 1 − L 2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

pn

b

xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

E (1 − γµ)(f (xk) − f (x∗)) + γ + γ (1 − p) 2p

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L

pn

b

2γ 2

gk − ∇f (xk) 2 E xk+1 − xk 2

(1 − γµ)E[Φk],

where in the last inequality we use γ(1p−n p) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤

(1

−

γ

µ)

γ p

following

from

(D.21).

Unrolling

the

recurrence

and

using

g0

=

∇f (x0),

we

obtain

E f (xk+1) − f (x∗) ≤ E[Φk+1] ≤ (1 − γµ)k+1Φ0 = (1 − γµ)k+1(f (x0) − f (x∗))

that implies (D.23), (D.24), and (D.25).

Corollary D.2.5. Let the assumptions of Theorem D.2.4 hold and p = min ζdQ , mb+b , where b ≤ m and ζQ is the expected density of the quantization (see Def. A.2.1). If



  

γ ≤ min

  

L

+

1
2 max{d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2





p

 

, 2µ  ,





then VR-MARINA requires

  1 L
O max , 1 + p µ



ω max {d/ζQ − 1, m/b }  + L

n

µ





(1 + ω) max {d/ζQ − 1, m/b }  log ∆0 

nb

ε

iterations/communication rounds,

 O m + max b , L b +
pµ

 ω max {(d/ζQ − 1)(b )2, mb }
 n

+ Lµ (1+ω) max{(nd/ζQ−1)b ,m} log ∆ε0

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the

347

expected total communication cost per worker is

O d + ζQ max

 1 , L 1 + pµ

 ω max {d/ζQ − 1, m/b }
 n

+ Lµ (1+ω) max{ndb/ζQ−1,m/b } log ∆ε0

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = min ζdQ , mb+b implies

1 − p = max d − 1, m ,

p

ζQ

b

pm + (1 − p)b ≤ 2mb ≤ 2b , m+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (D.21), (D.23), (D.24) and (D.25) and using a + b ≤ a + b, we

get that if





  

γ ≤ min

  

L

+

1
2 max{d/ζQ−1,m/b }
n

ωL2 + (1+bω)L2



p

 

, 2µ  ,





then VR-MARINA requires



  1

L+

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆

log

0 

µ

 ε







  1

L+

= O max ,

 p









L2 ω max{d/ζnQ−1,m/b } + L2 (1+ω) max{ndb/ζQ−1,m/b }  ∆0

log

 

µ

 ε





 = O max 1 , L 1 +
pµ

 ω max {d/ζQ − 1, m/b }
 n

+ L (1 + ω) max {d/ζQ − 1, m/b } log ∆0

µ

nb

ε

348

iterations/communication rounds and

m + K(pm + 2(1 − p)b )





 

L+



= O m + max p1 ,













1−p ωL2+ (1+ω)L2

 

pn

b



µ

(pm

+

(1

−

p)b

)

log

∆0 ε

 









= O m + max p1 , Lµ 1 +

ω max{d/ζQ−1,m/b }
n

+ Lµ (1+ω) max{ndb/ζQ−1,m/b } b log ∆ε0

= O m + max bp , Lµ b +

ω max{(d/ζQ−1)(b )2,mb }
n

+ Lµ (1+ω) max{(nd/ζQ−1)b ,m} log ∆ε0

stochastic oracle calls per node in expectation in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is

d + K(pd + (1 − p)ζQ)





 

L+



= O d + max p1 ,













1−p ωL2+ (1+ω)L2

 

pn

b



µ

(pd

+

(1

−

p)ζQ)

log

∆0 ε

 









= O d + ζQ max p1 , Lµ 1 +

ω max{d/ζQ−1,m/b }
n

+ Lµ (1+ω) max{ndb/ζQ−1,m/b } log ∆ε0

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.2.2 Online Case
Generally Non-Convex Problems In this section, we provide the full statement of Theorem 5.3.6 together with the proof of this result. Theorem D.2.6 (Theorem 5.3.6). Consider the ﬁnite sum case (5.1)+(5.5). Let Assump-

349

tions 5.1.1, 5.1.2 and 5.3.4 be satisﬁed and

γ≤ L+

1, 1p−np ωL2 + (1+bω)L2

(D.26)

where

L2

=

1 n

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 + σ2 , γK nb

(D.27)

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is,

after

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

(D.28)

iterations with b = Θ( nσε22 ) VR-MARINA produces such a point xˆK that E[ ∇f (xˆK ) 2] ≤ ε2, and the expected total number of stochastic oracle calls per node b + K(pb + 2(1 − p)b ) equals





σ2 ∆0 O  nε2 + ε2 L +





1−p

(1 + ω)L2

σ2

ωL2 +

pn

b

 p nε2 + (1 − p)b  .

(D.29)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server we have that the expected total communication cost per worker d + K(pd + (1 − p)ζQ) equals





O d + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pd + (1 − p)ζQ) ,

pn

b

(D.30)

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof of Theorem 5.3.6. The proof follows the same steps as the proof of Theorem 5.3.2. From Lemma A.5.7, we have

E[f (xk+1)] ≤ E[f (xk)]− γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2 + γ E gk − ∇f (xk) 2 .

2

2γ 2

2

Next, we need to derive an upper bound for E

(D.31)

n

gk+1 − ∇f (xk+1)

2

.

Since

gk+1 =

1 n

gik+1,

i=1

we get the following representation of gk+1:

n

1  nb

∇fξk (xk+1)

  

i=1 j∈Ii,k

ij

gk+1 =


n



gk + n1 i=1 Q  b1 j∈I (∇fξikj (xk+1) − ∇fξikj (xk))

i,k

with probability p, with probability 1 − p.

Using this, variance decomposition (A.14), tower property (A.15), and independence of ξikj for

350

i ∈ [n], j ∈ Ii,k, we derive:

E gk+1 − ∇f (xk+1) 2

 



2

(A=.15) (1 − p)E  gk + n1 n Q  b1

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1)  

i=1

j∈Ii,k

 n 2

+ n2pb2 E 

∇fξk (xk+1) − ∇f (xk+1) 

i=1 j∈Ii,k

ij

 



2

(A.15)=,(A.14) (1 − p)E  n1 n Q  b1

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk)  

i=1

j∈Ii,k

+(1 − p)E

gk − ∇f (xk) 2

n

+

p
22

E

nb

i=1 j∈Ii,k

∇fξikj (xk+1) − ∇f (xk+1) 2

 



2

(A.15=),(5.9) (1 − p)E  n1 n Q  b1

(∇fij(xk+1) − ∇fij(xk)) − ∇f (xk+1) + ∇f (xk)  

i=1

j∈Ii,k

+(1 − p)E gk − ∇f (xk) 2 + pnσb2 ,

where

σ2

=

1 n

obtain

n i=1

σi2.

Applying

the

same

arguments

as

in

the

proof

of

inequality

(D.19),

we

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E

gk − ∇f (xk) 2 + pσ2 . nb

(D.32)

Next,

we

introduce

new

notation:

Φk

=

f

(

x

k

)

−

f∗

+

γ 2p

gk −∇f (xk)

2. Using this and inequalities

(D.31) and (D.32), we establish the following inequality:

E [Φk+1]

≤
=
(D.26)
≤

E f (xk) − f∗ − γ ∇f (xk) 2 − 1 − L

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

2p n

b

xk+1 − xk 2

+γE

(1 − p)

gk − ∇f (xk)

2
+

pσ2

2p

nb

E [Φk] − γ E ∇f (xk) 2 2

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L E xk+1 − xk 2 + γσ2

2pn

b

2γ 2

2nb

E [Φk] − γ2 E ∇f (xk) 2 + γ2nσb2 ,

(D.33)

where in the last inequality, we use γ(21p−np) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 following from (D.26).

351

Summing up inequalities (D.33) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 KE
k=0

∇f (xk) 2

2 K−1

σ2

≤

(E[Φk] − E[Φk+1]) +

γK k=0 nb

= 2 (E[Φ0] − E[ΦK ]) + σ2 = 2∆0 + σ2 ,

γK

nb γK nb

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (A.15) and the deﬁnition of xˆK, we obtain (D.27) that implies (D.28), (D.29), and (D.30).

Remark D.2.7 (About batchsizes dissimilarity). Similarly to the ﬁnite sum case, our analysis

can be easily extended to handle the version of VR-MARINA with diﬀerent batchsizes b1, . . . , bn

and b1, . . . , bn on diﬀerent workers, i.e., when |Ii,k| = bi, |Ii,k| = bi for i ∈ [n]. In this case,

the statement of Theorem 5.3.6 remains the same with the small modiﬁciation: instead of Lb2

the

complexity

bounds

will

have

1 n

n i=1

L2i b

,

and

instead

of

the

requirement

b

=

Θ

σ2 nε

it will

i

have

1 n2

n i=1

σi2 b

=

Θ(ε2).

i

Corollary D.2.8 (Corollary 5.3.7). Let the assumptions of Theorem 5.3.6 hold and p =

min ζdQ , b+b b , where b ≤ b, b = Θ (σ2/(nε2)) and ζQ is the expected density of the quantization

(see Def. A.2.1). If

γ≤ L+

1, max{d/ζQn −1,b/b } ωL2 + (1+bω)L2

then VR-MARINA requires

O ∆0 L 1 + ε2

ω

d

σ2

n max ζQ − 1, nb ε2

(1 + ω)

d

σ2

+ L nb max ζQ − 1, nb ε2

iterations/communication rounds and

σ2 ∆0Lb ∆0L O nε2 + ε2 + ε2

ωb max n

d

σ2

ζQ − 1 b , nε2

+

∆0L ε2

1+ω n

max

ζdQ − 1 b , nσε22

stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost per worker is

O d + ∆0ζQ L 1 + ε2

ω

d

σ2

n max ζQ − 1, nb ε2

1+ω

d

σ2

+ L nb max ζQ − 1, nb ε2

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

352

Proof of Corollary 5.3.3. The choice of p = min ζdQ , b+b b implies

1 − p = max d − 1, b ,

p

ζQ

b

pm + (1 − p)b ≤ 2mb ≤ 2b , m+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (D.26), (D.28), (D.29) and (D.30) and using a + b ≤ a + b, we

get that if

γ≤ L+

1, max{d/ζQn −1,b/b } ωL2 + (1+bω)L2

then VR-MARINA requires

 K = O  ∆ε20 L +



1 − p ωL2 + (1 + ω)L2 

pn

b

 = O  ∆ε20 L +



L2 ω max {d/ζQ − 1, b/b } + L2 (1 + ω) max {d/ζQ − 1, b/b } 

n

nb

= O ∆0 L 1 + ε2

ω

d

σ2

n max ζQ − 1, nb ε2

(1 + ω)

d

σ2

+ L nb max ζQ − 1, nb ε2

iterations/communication rounds and





b + K(pb + 2(1 − p)b ) = O b + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pb + (1 − p)b )

pn

b

= O b + ∆0 L 1 + ε2

ω max {d/ζQ − 1, b/b } n

+L (1 + ω) max {d/ζQ − 1, b/b } b nb

σ2 ∆0 = O nε2 + ε2 L b +

ωb max n

d

σ2

ζQ − 1 b , nε2

+L 1 + ω max n

d

σ2

ζQ − 1 b , nε2

stochastic oracle calls per node in expectation to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected

353

total communication cost per worker is





d + K(pd + (1 − p)ζQ) = O d + ∆ε20 L +





1 − p ωL2 + (1 + ω)L2  (pd + (1 − p)ζQ)

pn

b

= O d + ∆0ζQ L 1 + ε2

ω

d

σ2

n max ζQ − 1, nb ε2

1+ω

d

σ2

+L nb max ζQ − 1, nb ε2

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide an analysis of VR-MARINA under Polyak-Łojasiewicz condition in the online case.

Theorem D.2.9. Consider the ﬁnite sum case (5.1)+(5.5). Let Assumptions 5.1.1, 5.1.2, 5.3.4, 5.2.4 and 6.3.3 be satisﬁed and



  

γ ≤ min

  

L

+

2(1−p) pn

1 ωL2 + (1+bω)L2





p

 

, 2µ  ,





(D.34)

where

L2

=

1 n

n i=1

L2i

and

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

VR-MARINA,

we

have

E

f (xK ) − f (x∗)

≤ (1 − γµ)K ∆0 +

σ2 ,

nbµ

(D.35)

where ∆0 = f (x0) − f (x∗). That is, after



  1

L+

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆

log

0 

µ

 ε





(D.36)

iterations with b = Θ nσµ2ε VR-MARINA produces such a point xK that E f (xK ) − f (x∗) ≤ ε, and the expected total number of stochastic oracle calls per node b + K(pb + 2(1 − p)b ) equals





  1

L+

O m + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pb + (1 − p)b ) log

0

 

.

µ



ε





(D.37)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the

354

expected total communication cost per worker d + K(pd + (1 − p)ζQ) equals





  1

L+

O d + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pd + (1 − p)ζQ) log

0

 

,

µ



ε





(D.38)

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof. The proof is very similar to the proof of Theorem 5.3.6. From Lemma A.5.7 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2

2

2γ 2

+ γ E gk − ∇f (xk) 2 2

(5.3)
≤ (1 − γµ)E f (xk) − f (x∗) −

1 − L E xk+1 − xk 2

2γ 2

+ γ E gk − ∇f (xk) 2 . 2

Using the same arguments as in the proof of (D.32), we obtain

E gk+1 − ∇f (xk+1) 2

≤ 1 − p ωL2 + (1 + ω)L2 E xk+1 − xk 2

n

b

+(1 − p)E

gk − ∇f (xk) 2 + pσ2 . nb

(D.39)

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk) − f (x∗) +

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤
=
(D.21)
≤

E (1 − γµ)(f (xk) − f (x∗)) − 1 − L 2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E 1 − p ωL2 + (1 + ω)L2

pn

b

xk+1 − xk 2

+γE

(1 − p)

gk − ∇f (xk)

2
+

pσ2

p

nb

E (1 − γµ)(f (xk) − f (x∗)) + γ + γ (1 − p) 2p

+ γ(1 − p) ωL2 + (1 + ω)L2 − 1 + L

pn

b

2γ 2

gk − ∇f (xk) 2 + γσ2 nb
E xk+1 − xk 2

γσ2 (1 − γµ)E[Φk] + ,
nb

where in the last inequality we use γ(1p−n p) ωL2 + (1+bω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤

355

(1

−

γ

µ)

γ p

following

from

(D.34).

Unrolling

the

recurrence

and

using

g0

=

∇f (x0),

we

obtain

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 + γnσb2 K−1(1 − γµ)k
k=0

≤

(1 − γµ)K (f (x0) − f (x∗)) + γσ2

∞
(1 − γµ)k

nb k=0

≤ (1 − γµ)K (f (x0) − f (x∗)) + σ2 . nbµ

Together with b = Θ nσµ2ε it implies (D.36), (D.37), and (D.38).

Corollary D.2.10. Let the assumptions of Theorem D.2.9 hold and p = min ζdQ , b+b b , where b ≤ b and ζQ is the expected density of the quantization (see Def. A.2.1). If



  

γ ≤ min

  

L

+

1
2 max{d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2





p

 

, 2µ 





and then VR-MARINA requires

b = Θ nσµ2ε , σ2 = n1 n σi2,
i=1

O max 1 , L 1 + pµ

ω max d − 1, σ2

n

ζQ

nb µ

+ L 1 + ω max d − 1, σ2

µ nb

ζQ

nb µ

log ∆0 ε

iterations/communication rounds,

O σ2 + max b , L b +

nµε

pµ

ωb max n

d − 1 b , σ2

ζQ

nµε

+ Lµ

1+ω n

max

ζdQ − 1 b , nσµ2ε

log

∆0 ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is

O d + ζQ max 1 , L 1 + pµ

ω max d − 1, σ2

n

ζQ

nb µ

+ Lµ 1n+bω max ζdQ − 1, nσb2µ

log

∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero

356

components of transmitted vectors from workers to the server.

Proof. The choice of p = min ζdQ , b+b b implies

1 − p = max d − 1, b ,

p

ζQ

b

pm + (1 − p)b ≤ 2bb ≤ 2b , b+b

pd + (1 − p)ζQ ≤ ζQ · d + 1 − ζQ · ζQ ≤ 2ζQ.

d

d

√

√√

Plugging these relations in (D.34), (D.36), (D.37) and (D.38) and using a + b ≤ a + b, we

get that if





  

γ ≤ min

  

L

+

1
2 max{d/ζQ−1,b/b }
n

ωL2 + (1+bω)L2



p

 

, 2µ  ,





then VR-MARINA requires



  1

L+

K = O max ,

 p









1p−np ωL2 + (1+bω)L2  ∆

log

0 

µ

 ε







  1

L+

= O max ,

 p









L2 ω max{d/ζnQ−1,b/b } + L2 (1+ω) max{ndb/ζQ−1,b/b }  ∆0

log

 

µ

 ε





= O max 1 , L 1 + pµ

ω max d − 1, σ2

n

ζQ

nb µ

+ L 1 + ω max d − 1, σ2

µ nb

ζQ

nb µ

log ∆0 ε

iterations/communication rounds and





  1

L+

b + K(pb + 2(1 − p)b ) = O b + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pb + (1 − p)b ) log

0 

µ



ε





= O b + max 1 , L 1 + pµ

ω max {d/ζQ − 1, b/b } n

+ L (1 + ω) max {d/ζQ − 1, b/b } b log ∆0

µ

nb

ε

= O σ2 + max b , L b +

nµε

pµ

ωb max n

d − 1 b , σ2

ζQ

nµε

+ L 1 + ω max µn

d − 1 b , σ2

ζQ

nµε

log ∆0 ε

357

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker is





  1

L+

d + K(pd + (1 − p)ζQ) = O d + max ,



p









1p−np ωL2 + (1+bω)L2 

∆

(pd + (1 − p)ζQ) log

0 

µ



ε





= O d + ζQ max 1 , L 1 + pµ

ω max d − 1, σ2

n

ζQ

nb µ

+ L 1 + ω max d − 1, σ2

µ nb

ζQ

nb µ

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.3 Missing Proofs for PP-MARINA

D.3.1 Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 5.4.1 together with the proof of this result.

Theorem D.3.1 (Theorem 5.4.1). Let Assumptions 5.1.1 and 5.1.2 be satisﬁed and

γ≤ L 1+

1,
(1−p)(1+ω)
pr

(D.40)

where

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

PP-MARINA,

we

have

E ∇f (xˆK ) 2 ≤ 2∆0 , γK

(D.41)

where xˆK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is,

after

K = O ∆0L 1 + ε2

(1 − p)(1 + ω) pr

(D.42)

iterations PP-MARINA produces such a point xˆK that E[ ∇f (xˆK) 2] ≤ ε2. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost (for all workers) equals

dn + K(pdn + (1 − p)ζQr) = O dn + ∆0L 1 + ε2

(1 − p)(1 + ω) pr

(pdn + (1 − p)ζQr) , (D.43)

358

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof of Theorem 5.4.1. The proof is very similar to the proof of Theorem 5.3.2. From Lemma A.5.7, we have

E[f (xk+1)] ≤ E[f (xk)]− γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2 + γ E gk − ∇f (xk) 2 .

2

2γ 2

2

(D.44)

Next, we need to derive an upper bound for E gk+1 − ∇f (xk+1) 2 . By deﬁnition of gk+1, we

have



∇f (xk+1)



gk+1 = gk + 1

Q

 

r



ik ∈Ik

∇fik (xk+1) − ∇fik (xk)

with probability p, with probability 1 − p.

Using this, variance decomposition (A.14) and tower property (A.15), we derive:

E gk+1 − ∇f (xk+1) 2

 (A=.15) (1 − p)E  gk + 1r Q
ik ∈Ik

∇fik (xk+1) − ∇fik (xk)

2 − ∇f (xk+1) 


 (A.15)=,(A.14) (1 − p)E 


1r Q
ik ∈Ik

∇fik (xk+1) − ∇fik (xk)

2 − ∇f (xk+1) + ∇f (xk) 


+(1 − p)E gk − ∇f (xk) 2 .

Next, we use the notation: ∆ki = ∇fi(xk+1) − ∇fi(xk) for i ∈ [n] and ∆k = ∇f (xk+1) − ∇f (xk). These vectors satisfy E ∆kik | xk, xk+1 = ∆k for all ik ∈ Ik. Moreover, Q(∆kik ) for ik ∈ Ik are

359

independent random vectors for ﬁxed xk and xk+1. These observations imply

E gk+1 − ∇f (xk+1) 2



2

= (1 − p)E  1r Q(∆kik ) − ∆k 
ik ∈Ik

+(1 − p)E gk − ∇f (xk) 2

= 1 −r p E Q(∆kik ) − ∆kik + ∆kik − ∆k 2 +(1 − p)E gk − ∇f (xk) 2

(A.15)=,(A.14)

1−p rE

Q(∆kik ) − ∆kik 2 + E

+(1 − p)E gk − ∇f (xk) 2

∆kik − ∆k 2

(A.15=),(A.6)

1 − p ωE r

∆kik 2 + E

∆kik − ∆k 2

+(1 − p)E gk − ∇f (xk) 2

(A.15)=,(A.14)

(1 − p)(1 + ω)

r

E

∆kik 2 + (1 − p)E

gk − ∇f (xk) 2 .

Using L-smoothness (5.2) of fi together with the tower property (A.15), we get

E gk+1 − ∇f (xk+1) 2

(1 − p)(1 + ω) n 2

k+1

k2

≤ nr

Li E x − x

i=1

+(1 − p)E gk − ∇f (xk) 2

=

(1 − p)(1 + ω)L2 E

xk+1 − xk 2

r

+(1 − p)E gk − ∇f (xk) 2 .

(D.45)

Next,

we

introduce

new

notation:

Φk

=

f

(

x

k

)

−

f∗

+

γ 2p

gk −∇f (xk)

2. Using this and inequalities

(D.44) and (D.45), we establish the following inequality:

E [Φk+1]

≤
=
(D.40)
≤

E f (xk) − f∗ − γ ∇f (xk) 2 − 1 − L

2

2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)(1 + ω)L2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

2p

r

E [Φk] − γ2 E ∇f (xk) 2 + γ(1 − p2)(p1n+ ω)L2 − 21γ + L2 E xk+1 − xk 2

E [Φk] − γ E ∇f (xk) 2 , 2

(D.46)

where in the last inequality we use γ(1−p2)(p1n+ω)L2 − 21γ + L2 ≤ 0 following from (D.40). Summing

360

up inequalities (D.20) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

1 K−1 KE
k=0

∇f (xk) 2

2 K−1

2 (E[Φ0] − E[ΦK ]) 2∆0

≤

(E[Φk] − E[Φk+1]) =

=,

γK k=0

γK

γK

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (A.15) and the deﬁnition of xˆK, we obtain (D.41) that implies (D.42) and (D.43).

Corollary D.3.2 (Corollary 5.4.2). Let the assumptions of Theorem 5.4.1 hold and p = ζdQnr , where r ≤ n and ζQ is the expected density of the quantization (see Def. A.2.1). If
γ≤ 1 , L 1 + 1+rω ζdQnr − 1

then PP-MARINA requires

K = O ∆0L 1 + ε2

1+ω r

dn − 1 ζQr

iterations/communication rounds to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost is
O dn + ∆0L ζQr + (1 + ω)ζQ (dn − ζQr) ε2
under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof of Corollary 5.4.2. The choice of p = ζdQnr implies

1 − p = dn − 1,

p

ζQr

pdn + (1 − p)ζQr ≤ ζQr + 1 − ζQr · ζQr ≤ 2ζQr. dn

Plugging these relations in (D.40), (D.42), and (D.43), we get that if

γ≤ 1 , L 1 + 1+rω ζdQnr − 1

361

then PP-MARINA requires

K = O ∆0L 1 + ε2
= O ∆0L 1 + ε2

(1 − p)(1 + ω) pr
1 + ω dn − 1 r ζQr

iterations/communication rounds in order to achieve E[ ∇f (xˆK) 2] ≤ ε2, and the expected total communication cost is

dn + K(pdn + (1 − p)ζQr) = O dn + ∆0L 1 + (1 − p)(1 + ω) (pdn + (1 − p)ζQr)

ε2

pr

= O dn + ∆0L ζQr + (1 + ω)ζQ (dn − ζQr) ε2

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

D.3.2 Convergence Results Under Polyak-Łojasiewicz Condition

In this section, we provide an analysis of PP-MARINA under Polyak-Łojasiewicz condition.

Theorem D.3.3. Let Assumptions 5.1.1, 5.1.2 and 5.2.4 be satisﬁed and


   γ ≤ min
 L 1 +

1
2(1−p)(1+ω) pr





p

 

, 2µ  ,





(D.47)

where

L2

=

1 n

n i=1

L2i .

Then

after

K

iterations

of

PP-MARINA,

we

have

E f (xK ) − f (x∗) ≤ (1 − γµ)K ∆0,

(D.48)

where ∆0 = f (x0) − f (x∗). That is, after

K = O max 1 , L 1 + (1 − p)(1 + ω)

pµ

pr

log ∆0 ε

(D.49)

iterations PP-MARINA produces such a point xK that E[f (xK) − f (x∗)] ≤ ε. Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server, we have that the expected total communication cost (for all workers) dn + K(pdn + (1 − p)ζQr) equals

O dn + max 1 , L 1 + (1 − p)(1 + ω)

pµ

pr

(pdn + (1 − p)ζQr) log ∆0 , ε

(D.50)

362

where ζQ is the expected density of the quantization (see Def. A.2.1).

Proof. The proof is very similar to the proof of Theorem 5.4.1. From Lemma A.5.7 and PŁ condition we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] − γ E ∇f (xk) 2 − 1 − L E xk+1 − xk 2

2

2γ 2

+ γ E gk − ∇f (xk) 2 2

(5.3)
≤ (1 − γµ)E f (xk) − f (x∗) −

1 − L E xk+1 − xk 2

2γ 2

+ γ E gk − ∇f (xk) 2 . 2

Using the same arguments as in the proof of (D.45), we obtain

E gk+1 − ∇f (xk+1) 2

≤

(1 − p)(1 + ω)L2 E

xk+1 − xk 2 + (1 − p)E

gk − ∇f (xk) 2 .

r

Putting

all

together,

we

derive

that

the

sequence

Φk

=

f (xk) − f (x∗) +

γ p

gk − ∇f (xk)

2 satisﬁes

E [Φk+1]

≤
=
(D.47)
≤

E (1 − γµ)(f (xk) − f (x∗)) − 1 − L 2γ 2

xk+1 − xk 2 + γ gk − ∇f (xk) 2 2

+ γ E (1 − p)(1 + ω)L2 xk+1 − xk 2 + (1 − p) gk − ∇f (xk) 2

p

r

E (1 − γµ)(f (xk) − f (x∗)) + γ + γ (1 − p) 2p

gk − ∇f (xk) 2

+ γ(1 − p)(1 + ω)L2 − 1 + L E xk+1 − xk 2

pr

2γ 2

(1 − γµ)E[Φk],

where in the last inequality we use γ(1−p)p(r1+ω)L2 − 21γ + L2 ≤ 0 and γ2 + γp (1 − p) ≤ (1 − γµ) γp following from (D.47). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

E f (xK ) − f (x∗) ≤ E[ΦK ] ≤ (1 − γµ)K Φ0 = (1 − γµ)K (f (x0) − f (x∗))

that implies (D.49) and (D.50).

Corollary D.3.4. Let the assumptions of Theorem D.3.3 hold and p = ζdQnr , where r ≤ n and ζQ is the expected density of the quantization (see Def. A.2.1). If


   γ ≤ min
 L 1 +

1
2(1+ω) r

ζdQnr − 1





p

 

, 2µ  ,





363

then PP-MARINA requires

K = O max dn L 1 + ζQr µ

1+ω r

dn − 1 ζQr

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

O dn + max dn, L ζQr + (1 + ω)ζQ (dn − ζQr) µ

log ∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

Proof. The choice of p = ζdQnr implies

1 − p = dn − 1,

p

ζQr

pdn + (1 − p)ζQr ≤ ζQr + 1 − ζQr · ζQr ≤ 2ζQr. dn

Plugging these relations in (D.47), (D.49), and (D.50), we get that if


   γ ≤ min
 L 1 +

1
2(1+ω) r

ζdQnr − 1





p

 

, 2µ  ,





then PP-MARINA requires

K = O max 1 , L 1 + (1 − p)(1 + ω)

pµ

pr

log ∆0 ε

= O max dn L 1 + ζQr µ

1+ω r

dn − 1 ζQr

log ∆0 ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

dn + K(pdn + (1 − p)ζQr) = O dn + max p1 , Lµ 1 +

(1−p)(1+ω) pr

(pdn

+

(1

−

p)ζQr)

log

∆0 ε

= O dn + max dn, Lµ ζQr +

(1 + ω)ζQ (dn − ζQr)

log

∆0 ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server.

364

Appendix E
Appendix for Chapter 6
E.1 GPU Instance Costs
This section provides a brief cost analysis of typical deep learning compute resources both in the cloud and on-premises. For brevity, we limit this analysis to the popular GPUs available at the time of submission. Note that the exact costs will depend on a variety of factors such as the cloud provider, the region, electricity costs, and market ﬂuctuations. Therefore, we warn the reader to consider this analysis only as a rough estimate.
Speciﬁcally, we estimate the compute costs for the occasional usage scenario: running a single set of experiments over several weeks or conducting infrequent experiments. This scenario covers most research scientists and small organizations. The most straightforward way to provision a GPU server in such a scenario is to rent it from a cloud provider (e.g., GCP or AWS) or a public marketplace (e.g., Vast.ai or Golem).
While the exact server speciﬁcations vary from one provider to another, there are two broad categories of GPU machines: regular and preemptible. Regular instance types typically oﬀer 1–8 GPUs per node with tight uptime guarantees (typically 99.99%) and a high-bandwidth network (tens of Gb/s). In turn, preemptible instances provide the same resource type at a signiﬁcant discount with the condition that the machine can be terminated at any time after short notice.
To account for individual variations, we report the average rent price over three popular cloud providers. We consider three popular instance types: two high-end instances with 8 Tesla V100 or A100 GPUs and a low-end instance with a single Tesla T4 GPU. We also describe several low-end servers and workstations available on a public marketplace. Unlike cloud VMs, these instances are hosted on non-curated hardware with less uptime guarantees (typically 95% – 99.9%), slower network and signiﬁcant variation in performance. However, marketplace instances are the cheapest in terms of cost per TFLOPS. To quantify this, we report the average over three most aﬀordable instances that ﬁt the chosen minimum requirements.
As a point of comparison, we also measure each system’s training performance for BERTLarge [37] ﬁne-tuning on SQuAD v1.1 [172] in PyTorch with mixed precision. We follow the oﬃcial benchmarking protocol by [158] and reuse the oﬃcial performance results for V100, A100, and T4 instances. The only exception is GTX 1080Ti, where we use full 32-bit precision because
365

that device does not support eﬃcient half-precision operations. Table E.1: Cloud and marketplace GPU instance pricing for short-term usage.

GPU

Minimum system speciﬁcations

CPU cores

CPU type

RAM, GB

Average cost, $/hour Regular Preemptible

BERT-Large training samples/s

Cloud instances

8× V100

64

Intel Xeon Broadwell

480

23.47

7.13

354

8× A100

96

AMD Epyc ROME

960

30.65

10.18

755

1× T4

4

Intel Xeon Cascade Lake

16

0.46

0.18

18

Marketplace instances

6× 3090

32

AMD Epyc Rome

480

5.04

4.17

154

4× 2080Ti

16

Intel Xeon Haswell

240

0.96

0.84

83.4

1× RTX 1080Ti

8

Intel Xeon Haswell

16

0.22

0.16

12

Table E.1 shows two main tendencies. First, preemptible cloud instances are, on average, three times cheaper than their non-preemptible counterparts1. Second, the high-end HPC-grade servers that oﬀer the highest raw performance are less cost-eﬀective than lower-tier servers and marketplace instances. In theory, one could match the raw ﬂoating-point performance of a 8×V100 instance at a fraction of its cost using multiple lower-tier workstations, such as 4× RTX 2080Ti, with a smaller total cost. However, in practice, running distributed training with these workstations is challenging due to their unreliability and slow network connection.
Note that this analysis does not represent the cloud costs for sustained GPU usage. If an organization plans to constantly use GPU resources over a period of multiple years, they can reduce the costs by deploying their own compute infrastructure or relying on the sustained usage discounts reaching up to 60–70%. Thus, the long-term compute costs are much harder to analyze and depend on a number of additional factors, such as local electricity prices for on-premise infrastructure. However, this scenario oﬀers similar trade-oﬀs: HPC-grade infrastructure oﬀers greater interconnectivity, but requires expensive network interface cards, high-end switches and a more complex setup process.

E.2 Additional Related Work
In this section, we review some of the papers relevant to our work, but omitted from the main part due to space constraints.
1The cost can be up to 11× cheaper for some instance types, e.g. Azure V100 instances in the central US region at the time of writing.

366

E.2.1 Decentralized Training
In this subsection, we give additional details about the dependence of gossip-based optimization methods on the spectral properties on the communication graph through the spectral properties of the mixing matrix [232, 190] or the Laplacian matrix [136, 219] of the network. That is, gossip ﬁnds approximate average on nodes with accuracy ε after O (1 − λ2(M))−1 log(ε−1) iterations, where M is the mixing matrix and λ2(M) is the second largest eigenvalue of M when sorted by absolute value. The quantity η = 1 − λ2(M) is called the spectral gap of the mixing matrix M, and η−1 is typically a polynomial of the total number of nodes n when the maximal degree of the node is O(1). For example, for uniformly averaging M one can show that η−1 = O(n2) for the ring topology (node degree 2), η−1 = O(n) for the two-dimensional torus topology (node degree 2), and η−1 = O(1) for the fully connected graph (node degree n − 1); one can ﬁnd more examples in [3]. Similarly, the communication complexity of decentralized optimization methods often has multiplicative dependence on either O(η−1) (see [233] and references therein) or O(η−1/2) [190, 219, 39, 105], which is not improvable for gossip-based methods [9, 191].
Contrary to this, Moshpit All-Reduce does not depend on a ﬁxed communication graph and the properties of its mixing matrix. However, it depends on the number of averaging groups and the total number of peers (see Theorem 6.3.2), which can be viewed as properties of a time-varying random communication graph. Fortunately, this dependence is often much better than in gossip: as we mentioned in the main part of the paper, even if workers are randomly split into pairs at each iteration, the simpliﬁed version of Moshpit All-Reduce makes the average distortion (the left-hand side of Equation 6.5) at least 2 times smaller after each round on average.
E.2.2 Compressed Communication
Another popular approach to addressing the communication bottleneck is communication compression [196, 4, 215]: before sending any information (e.g., iterates, gradients, Hessians or more sophisticated data) over the network, peers compress this information by applying some (possibly random) transformation. As the result, peers send fewer bits for each communication round, but the total number of communication rounds needed to achieve the predeﬁned accuracy of the solution increases. However, communication compression is very useful in the situations when the reduction in communication costs of one round is more important than the increase in the number of these rounds [78].
There are two distinct groups of works on distributed training with compressed communication: ones that focus on unbiased compression operators (e.g., Rand-K, p-quantization) and ones studying algorithms with biased compressors (e.g., Top-K); see a detailed summary of popular compression operators in [20]). Quantized SGD (QSGD) [4] and TernGrad [227] were among the ﬁrst compression methods with convergence guarantees. Next, the convergence analysis of these methods was generalized and tightened in the (strongly) convex case in [139]. Moreover, the authors of [139] proposed a modiﬁcation of QSGD called DIANA: this algorithm is based on the
367

quantization of gradients’ diﬀerences, which helps it achieve linear convergence in the strongly convex case when peers compute full gradients. Next, DIANA was generalized to arbitrary unbiased compression in [79], where authors also developed and analyzed the variance-reduced version of DIANA. After that, several further modiﬁcations, such as Accelerated DIANA [119] and DIANA with bidirectional compression [57, 165], were proposed. Finally, we refer the reader to [120, 67, 32] for state-of-the-art results for distributed methods with unbiased compression in the non-convex case.
However, naïve application of biased compression operators can lead to signiﬁcantly worse performance in practice. For instance, as it was shown recently in [20], parallel SGD with Top-1 compression can diverge exponentially fast. Therefore, biased compressors are used jointly with so-called error-compensation [196]. The ﬁrst analysis of Error-Compensated SGD (EC-SGD) was proposed in [208, 88] which then was generalized and tightened in [20]. Next, several further improvements, such as an accelerated version of EC-SGD [168] and linearly converging EC-SGD [57], were recently proposed. However, current theory does not show any superiority of distributed methods with biased compressors to the ones with unbiased compression operators. In addition, one can combine decentralized communication with compression. Such combinations with unbiased compression operators were studied in [179, 104] and with biased operators in [98, 96]. In this paper, we do not study the interaction of diﬀerent compression methods and Moshpit Averaging, leaving this promising direction to future work.
E.2.3 Multiple Local Steps
Alternatively, to reduce the impact of the communication bottleneck, it is possible to perform several local optimization steps on each peer between the communication rounds. This approach is based on the idea that the increased computational load of peers will decrease the number of communication rounds required to obtain the optimal parameters; it is frequently used in federated learning [100, 83]. In particular, one of the most popular methods with multiple local steps is called Local-SGD or Federated Averaging [100, 210]. The ﬁrst results on its convergence were given in [210, 125], and later they were tightened and generalized both for homogeneous [89, 229] and heterogeneous cases [89, 228]. Recently, further modiﬁcations of Local-SGD were proposed and analyzed: these modiﬁcations include acceleration [237], variance reduction [56], communication compression [15, 67, 32], decentralization [117, 97], adaptive and proximal methods [176, 238], and resistance to client drift [86]. Moshpit SGD can perform multiple local gradient steps before synchronization by design, as shown in Algorithm 38.
E.2.4 Asynchronous Methods
In the previous subsections, we mostly discussed synchronous distributed methods, since they are more widespread and better studied than asynchronous ones. Mainly, this is because asynchronous methods are more diﬃcult to implement, debug and analyze under general assumptions. However, such methods can be more eﬃcient in terms of using computational
368

resources, which leads to faster wall-clock convergence [12]. In recent years, several asynchronous stochastic methods [175, 244, 111], methods with no shared memory [162, 141], and methods with delayed updates [1, 43, 11, 57] were proposed and analyzed. One can ﬁnd more details in a recent survey of asynchronous distributed methods [12]. Moshpit SGD belongs to this family of asynchronous approaches as well, because the averaging steps happen in smaller groups and can be interleaved with local parameter updates.
E.2.5 Distributed Hash Tables
In this work, we set out to improve distributed averaging with a dynamic matchmaking protocol. Without a central server, this protocol relies on decentralized data structures to organize peers. The main data structure we use is the Distributed Hash Table, or DHT. On a high level, DHT is a distributed fault-tolerant “dictionary” that can be accessed by every participant. Each key-value pair is stored on a subset of peers determined by the hash function of the key.
Each participant has a unique identiﬁer (ID) sampled uniformly from the hash function output range. When storing a (key, value) pair, one must ﬁnd k peers whose IDs are nearest to hash(key) according to a chosen metric. After that, the participant requests each of those peers to store (key, value). When retrieving a value for a key, one should compute hash(key), search for peers with IDs nearest to that hash value and request the value from those peers.
Speciﬁc DHT versions, such as Chord [14] or Kademlia [133], employ diﬀerent hash types and algorithms for ﬁnding nearest peers. For instance, Kademlia DHT sorts peers based on the XOR distance function: d(x, y) = int(x ⊕ y).
In DHT, each participant is directly aware of only a small subset of peers. When storing or retrieving a key, the participant requests additional peers from its neighbors in a semi-greedy search, minimizing the XOR distance until it ﬁnds k nearest peers. In Kademlia, nodes form a special navigable graph structure that lets them ﬁnd nearest peers in at most O(k + log n) requests to other peers, where n is the total number of participants. Due to their scalability and fault-tolerance, DHTs found numerous applications including BitTorrent, Ethereum, I2P and even deep learning with Mixtures-of-Experts [186].
E.3 Proofs of Mixing Properties of Moshpit All-Reduce
Here we formally state the theorems about mixing properties of Moshpit Averaging along with their proofs.
Notation. Throughout the following sections, we use the standard notation from the literature on stochastic optimization. That is, for any n-dimensional vectors x = (x1, . . . , xn) , y = (y1, . . . , yn) ∈ Rd we use x, y to denote the standard inner product: x, y = x1y1 + . . . + xnyn. Next, we use x to denote the 2=norm of x ( x = x, x ), E[ξ] to denote an expectation of a random variable ξ, E[ξ | η] is used for the conditional expectation of ξ given η, and P{E} denotes the probability of an event E.
369

E.3.1 Computing Exact Average in a Full Grid

As discussed in Section 6.3.1, Moshpit All-Reduce obtains the exact average of parameter vectors from n peers arranged in a grid with N coordinates and M positions per coordinate when n ≡ M N . That is, when the grid is full and each step averages M parameter values along a single grid coordinate without repetitions, the algorithm needs only N steps to compute the actual average across all nodes. In this section, we give a proof of this fact.
First, let us formally deﬁne the setting and the averaging steps of Moshpit All-Reduce in this speciﬁc case. Let xi1i2...iN be the parameter vector of the worker with coordinates i1, i2, . . . , iN ; each coordinate ik takes values from 1 to M , because the hypercube of peers is completely full (thus, due to the pigeonhole principle, there are no unoccupied coordinates). Next, arrange the coordinates of these vector according to the order of averaging iterations: namely, at iteration 1

x1i1i2...iN = M1 M xj1i2...iN ,
j1=1

i1 ∈ {1, . . . , M },

(E.1)

which means that for the ﬁrst iteration, we take the average across the ﬁrst axis x1 and replicate it across all M resulting vectors regardless of their index i1. The next averaging steps can be expressed similarly with a simple recurrence relation:

xti1i2...iN = M1 M xti1−..1.it−1jtit+1...iN .
jt=1

(E.2)

Given this formal deﬁnition, we can now state and prove the exact averaging result:

Theorem E.3.1 (Exact average in a full N -dimensional hypercube after N steps). Assume that M N peers are arranged in a N -dimensional hypercube with M positions in each dimension. Also, assume that each peer fully participates in every averaging step and M -sized groups for each averaging iteration are determined based on the hypercube coordinates. Then, if Moshpit All-Reduce is ran in the above setup for N iterations without repeating groups (i.e. averaging across each dimension exactly once), its result for each participant is the average value of x across all M N peers.

Proof. We can directly obtain the expression for the average by expanding the recurrence and

370

rearranging the sums:

=1 M





xNi1i2...iN = M1 M xNi1.−..i1N−1jN = M1 M  M1 M xi1i2...jN−1jN  = . . .

jN =1

jN =1

jN −1 =1

M jN =1

1M

M

...

M jN−1=1 j2=1

1M

M

xj1...jN

j1=1

1M M

MM

= MN

...

xj1...jN =

jN =1 jN−1=1 j2=1 j1=1

N summations

1

M

= MN

xj1...jN .

j1,...,jN =1

But this is exactly the global average of all x, since there are M N participants and each vector is represented in the sum because of summation over all possible indices.

Notice that for a given grid of peers, if some of its indices do not have corresponding parameter

vectors, Equation (E.2) may result in diﬀerent average vectors on diﬀerent workers due to

diﬀerent numbers of peers along a coordinate for diﬀerent indices. For example, running two

iterations of Moshpit Averaging with N = 2, M = 2 and three parameter vectors x11, x21, x22

results

in

x11 +x21 2

on

the

ﬁrst

worker

and

x11 +x21 4

+ x22

on

other

workers,

so

neither

of

the

values

is equal to the global average. However, the variance of the averaged vectors does decrease,

which is formally proven in Section E.3.3.

E.3.2 Proof of Theorem 6.3.1

Below we provide the complete proof of Theorem 6.3.1. For the readers’ convenience, we restate the theorem.

Theorem E.3.2 (Theorem 6.3.1). If all workers have non-zero probability of successfully
running a communication round in Moshpit Averaging and the order of peerst is random, then all local vectors xti converge to the global average with probability 1:

∀i = 1, . . . , n

t

1n

2 0

xi − n

xi −−−→ 0.
t→∞

i=1

(E.3)

Proof of Theorem 6.3.1. First of all, we notice that (E.3) is equivalent to

∀i = 1, . . . , n, ∀j = 1, . . . , n

t

1n 0

2

xi(j) − n

xi (j) −−−→ 0,
t→∞

i=1

(E.4)

where xti(j) denotes j-th component of xti. Consider an arbitrary component j ∈ {1, . . . , n} and

the

sequence

of

intervals

{Ij,t}t≥0

where

Ij,t

=

conv

{x

t 1

(

j

),

xt2

(j

)

,

.

.

.

,

x

t n

(

j

)}

.

Then,

{Ij,t}t≥0

is

a sequence of nested intervals (Ij,t+1 ⊆ Ij,t∀t ≥ 0), since averaging in groups does not expand

the

convex

hull

of

{

x

t 1

,

xt2

,

.

.

.

,

x

t n

}.

For convenience, we specify the bounds of the intervals:

371

Ij,t = [aj,t, bj,t]. Using the Cantor’s intersection theorem, we conclude that

∞
Ij,t = Ij = [aj, bj],
t=0

where

x(j)

=

1 n

n i=1

x0i (j)

∈

[aj ,

bj ].

If

[aj, bj] = {x(j)}

with

probability

1,

then

(E.4)

holds

with probability 1 as well. Suppose the opposite: there exist such j ∈ {1, . . . , n}, [a, b] and

δ, ∆ > 0 that x(j) ∈ [a, b], b − a = ∆ and

∞

∞

P [a, b] ⊆ Ij,t = δ > 0 and ∀ε > 0 P [a − ε, b + ε] ⊆ Ij,t < δ.

t=0

t=0

E

Eε

This implies that for all ε > 0 there exists such Tε > 0 that

P ∀t ≥ Tε aj,t ∈ [a − ε, a], bj,t ∈ [b, b + ε]
Eε

= δε > 0.

Consider

ε=

∆ (2n+100)2n

and

assume

that

the

event

Eε

holds.

Next,

we

introduce

new

notation:

Jlteft = {i ∈ {1, . . . , n} | xti(j) ∈ [a − ε, a]} and Jrtight = {i ∈ {1, . . . , n} | xti(j) ∈ [b, b + ε]}. Since

Eε holds the sets Jlteft and Jrtight are non-empty for all t ≥ Tε with probability δε > 0:

P ∀t ≥ Tε Jlteft = ∅ and Jrtight = ∅ = δε > 0.

(E.5)

We notice that every pair of workers i1, i2 has a non-zero probability of taking part in the
averaging inside the common group at each iteration since all workers have a non-zero probability
of successfully running a communication round and the order of peerst is random. This implies that every pair of workers i1, i2 with probability 1 take part in the averaging inside the common
group inﬁnitely many times when t goes to the inﬁnity.
Next, we choose some t0 ≥ Tε. Let Jlte0ft = {il,1, . . . , il,ql } and Jrti0ght = {ir,1, . . . , ir,qr }. Consider the event Eε,0 ⊆ Eε such that in Eε,0 peer il,1 computes an average in the group containing any peer from Jrti0ght at some iteration t1 > t0. Our observations above imply that P{Eε,0} = P{Eε} = δε > 0. Then, xtil1,1 (j) ≥ n−n 1 (a−ε)+ n1 b = a−ε+ n1 (∆+ε) = a− (2n+∆100)2n + n1 ∆ + (2n+∆100)2n > a + 2∆n , i.e., xitl1,1(j) ∈ (a, b] meaning that il,1 ∈ Jlte1ft. The last part of the proof shows that for any t ≥ t1, the peer il,1 will never be the part of Jlteft and after a ﬁnite number of iterations Jlteft = ∅ with probability δε > 0 when Eε,0 holds, implying the contradiction with (E.5).
To show that, we consider the following set of peers: Jlte1ft = {i ∈ {1, . . . , n} | ∃t ≥ t1 : xti(j) ∈ [a − ε, a + 2∆n )}. Next, we consider the event Eε,1 ⊆ Eε,0 such that in Eε,1 peer il,1 computes an average in the group containing some peer il,avg,1 from Jlte1ft at some iteration t2 > t1 (and t2 is the ﬁrst such moment after t1). Again, our observations imply P{Eε,1} = P{Eε,0} = δε > 0. Then, xtil2,1 (j) = xtil2,avg,1 (j) > n−n 1 (a − ε) + n1 a + 2∆n = a + 2∆n2 − n(2(nn+−110)∆0)2n > a + 4∆n2 . After that, we consider the event Eε,2 ⊆ Eε,1 such that in Eε,2 peer il,1 or il,avg,1 computes an

372

average in the group containing a peer il,avg,2 = il,avg,1 from Jlte1ft at an iteration t3 > t2 (and

t3 is the ﬁrst such moment after t2). Then, xtil3,1 (j), xtil3,avg,1 (j) and xtil3,avg,2 (j) are greater than

n−n 1 (a

−

ε)

+

1 n

a

+

∆ 4n2

= a + 4∆n3 − n(2(nn+−110)∆0)2n > a + 8∆n3 .

Therefore, after at least n − 1 of such averaging iterations, with probability δε all xti(j) will be

greater

than

a+

∆ (2n)n

>

a

while

Eε

holds.

This

contradicts

(E.5).

Therefore,

∞
Ij,t = {x(j)}
t=0

with probability 1, which concludes the proof.

E.3.3 Proof of Theorem 6.3.2

In this section, we provide the complete proof of Theorem 6.3.2. For convenience, we restate the theorem below.

Theorem E.3.3 (Theorem 6.3.2, averaging convergence rate). Consider the modiﬁcation of

Moshpit All-Reduce that works as follows: at each iteration k ≥ 1 1) peers are randomly

split

into

r

disjoint

groups

of

sizes

M

k 1

,

.

.

.

,

M

k r

in

such

a

way

that

r i=1

Mik

=

n

and

Mik

≥

1 ∀i = 1, . . . , r and 2) peers from each group compute their group average via All-Reduce.

Let

x1, . . . , xn

be

the

input

vectors

of

this

procedure

and

x

T 1

,

.

.

.

,

x

T n

be

the

outputs

after

T

iterations. Then,

1n En

xTi − x 2 =

i=1

r−1 r T 1 n

n + n2

· n

xi − x 2,

i=1

(E.6)

where

x

=

1 n

n i=1

xi.

Proof. First of all, let us clarify the procedure of random splitting of peers in r groups. We assume

that at iteration k of the modiﬁed algorithm we generate a random permutation πk = (π1k, . . . , πnk)

of 1, . . . , n.

Next,

J1k

=

{π

k 1

,

.

.

.

,

π

k M

k

}

form

the

indices

of

the

ﬁrst

group

of

workers,

J2k

=

1

{πMk k+1, . . . , πMk k } are the indices of the second group, and Jrk = {πMk k+Mk+...+Mk

+1

,

.

.

.

,

π

k n

}

1

2

1

2

r−1

are the indices of group r. In other words, we generate a random permutation and take contiguous

subgroups of indices corresponding to predeﬁned group sizes Mik, starting from the ﬁrst group.

By deﬁnition, we have

r i=1

Jik

=

{1, 2, . . . , n},

where

deﬁnes the disjoint union operator.

Moreover,

notice

that

group

sizes

M

k 1

,

.

.

.

,

M

k r

can

depend

on

k

and

even

be

random:

for

our

analysis, it is suﬃcient that the randomness deﬁning the permutation is independent from

M

k 1

,

.

.

.

,

M

k r

.

Next,

vectors

x

k 1

,

.

.

.

,

x

k n

are

obtained

by

the

following

formula:

∀j = 1, . . . , n,

xk = 1 j Mik

xkt −1,
k

t∈Ji

where Jik is the group for which j ∈ Jik.

Using this, we show that the average of vectors {xki }ni=1 remains the same throughout the

373

iterations of Moshpit All-Reduce:

1

n xk = 1

r
Mk ·

1

n

j

j=1

n i=1 i Mik

xkt −1 = n1 r

k

i=1

xkt −1 = n1 n xkj −1.

k

j=1

t∈Ji

t∈Ji

Therefore,

the

quantity

1 n

n j=1

xkj − x 2 (average distortion) measures the quality of averaging.

For this quantity, we can derive the following expression:

2

1 n xk − x 2 = 1 r M k 1

n

j

j=1

n i=1 i Mik

xkt −1 − x
k

t∈Ji





= 1r 1

xk−1 − x 2 + 2

xk−1 − x, xk−1 − x  .

n i=1 Mik  k t

t
k

l



t∈Ji

t,l∈Ji ,t<l

Taking the expectation Eπk [·] with respect to the randomness coming from the choice of πk we get

 1n
Eπk  n
j=1





xkj − x 2 = n1 r M1k Eπk 

i=1 i

t∈Jik







xtk−1 − x 2+2Eπk 

xtk−1 − x, xkl −1 − x  .

t,l∈Jik ,t<l

Since ∀j, j1, j2 ∈ {1, . . . , n}, j1 = j2 and for all i = 1, . . . , r

P j ∈ Jik = Mnik , P j1, j2 ∈ Jik = Mik(Mn2ik − 1) ,

374

we have





1n Eπk  n

xkj − x

2


=

1r 1 n Mk

Mik n n

xjk−1 − x 2

j=1

i=1 i

j=1

+2 Mik(Mik − 1) n2

xkj1−1 − x, xjk2−1 − x

1≤j1<j2≤n

= r n xk−1 − x 2 + 2 n − r

xk−1 − x, xk−1 − x

n2

j

n3

j1

j2

j=1

1≤j1<j2≤n

=

r n−r −

n
xk−1 − x 2

n2 n3

j

j=1

 n−r n + n3 
j=1



xkj −1 − x 2 + 2

xkj1−1 − x, xkj2−1 − x 

1≤j1<j2≤n

2

= n(r − 1) + r n

xk−1 − x 2 + n − r

n
(xk−1 − x)

n3

j

n3

j

j=1

j=1

= r − 1 + r · 1 n xk−1 − x 2.

n n2 n

j

j=1

nx−nx 2=0

Finally, we take the full expectation from the both sides of the above equation and apply the tower property E [Eπk [·]] = E [·]:





1n En

xkj − x

2


=

j=1

r−1 r n + n2





1 n k−1

2

E  n xj − x  .

j=1

Unrolling the recurrence for k = T , we establish (E.6).

Remark E.3.4. The result implies that increasing the group size α > 1 times implies almost α times faster convergence to the average.

Remark E.3.5. Our analysis can be easily generalized to the case when number of groups r can depend on k and be a random variable independent from the choice of permutations and the number of groups at previous steps. In this case, (E.6) transforms into

1n En

xTi − x 2 = n1 n

T
xi − x 2 ·

E[rk] − 1 + E[rk] ,

n

n2

i=1

i=1

k=1

where rk is the number of groups at iteration k.

(E.7)

375

E.3.4 Additional Guarantees For Moshpit Averaging
In this section, we derive the result measuring the rate of variance reduction when averaging random vectors with Algorithm 37. We start with the following technical lemma:

Lemma E.3.6. Let ξ ∼ Binom(M, p) have a binomial distribution with parameters M (number of trials) and p (probability of success for each trial). Then

m1(M, p) := E min 1 , 1 ξ 1
m2(M, p) := E min ξ2 , 1

= (1 − p)M + M 1 (1 − p)M−i − (1 − p)M , i=1 i

(E.8)

= (1 − p)M + M 1 (1 − p)M−i − (1 − p)M M 1 .(E.9)

i=1 i

j=i j

Proof. We start with the proof of (E.8). By deﬁnition of the expectation, we have

E min 1 , 1 ξ

= (1 − p)M + M 1 pi(1 − p)M−i M . i=1 i i

For simplicity of further derivations, we introduce the following notation: m1(M, p) = E min

1 ξ

,

1

and m2(M, p) = E min

1 ξ2

,

1

. Taking the derivative of m1(M, p) by p, we obtain

m1(M, p) = −M (1 − p)M−1 + M pi−1(1 − p)M−i Mi − M M i− i pi(1 − p)M−i−1 Mi

i=1

i=1

=

−M (1 − p)M−1 + 1

M
−(1 − p)M + pi(1 − p)M−i

M

p i=0 i

M −

M 1 pi(1 − p)M−i M

1 − p i=1 i i

+1 1−p

M
−(1 − p)M + pi(1 − p)M−i

M

i=0 i

= −M (1 − p)M−1 + 1 1 − (1 − p)M − M m1(M, p) − (1 − p)M

p

1−p

+ 1 1 − (1 − p)M 1−p

1

(1 − p)M−1 M

= p(1 − p) −

p

− 1 − p m1(M, p).

Rearranging the terms, we get the following linear ﬁrst-order ODE

M

1

(1 − p)M−1

m1(M, p) + 1 − p m1(M, p) = p(1 − p) −

. p

(E.10)

To solve it, we consider the following homogeneous ODE:

M m1(M, p) + 1 − p m1(M, p) = 0.

The solution of this ODE is m1(M, p) = C(1 − p)M , where C ∈ R is an arbitrary real constant.

376

Next, we go back to the initial ODE (E.10) and try to ﬁnd a solution of the form m1(M, p) = C(p)(1 − p)M , where C(p) : R → R is a diﬀerentiable function:

C(p)(1 − p)M

+ M C(p)(1 − p)M = 1−p ⇓ C (p)(1 − p)M =
⇓ C (p) =

1

(1 − p)M−1

p(1 − p) − p

1

(1 − p)M−1

p(1 − p) − p

1

1

p(1 − p)M+1 − p(1 − p) .

Since

1

1

1

x(1 − x)k+1 = x(1 − x)k + (1 − x)k+1

for all x ∈ {0, 1} and all non-negative integers k, we have

11

1

1

11

C (p) = p + 1 − p + (1 − p)2 + . . . + (1 − p)M+1 − p − 1 − p

⇓

M
C (p) = (1 − p)−i−1,

i=1

hence

C(p) = Cˆ + M 1 (1 − p)−i, i=1 i

where Cˆ is a real constant. Putting all together, we obtain

(E.11)

m1(M, p) = C(p)(1 − p)M = Cˆ(1 − p)M + M 1i (1 − p)M−i.
i=1

Taking m1(M, 0) = 1 into account, we conclude that Cˆ = 1 −

M i=1

1 i

and

obtain

(E.8).

Using a similar technique, we derive (E.9). By deﬁnition of the expectation, we have

m2(M, p) = (1 − p)M + M i12 pi(1 − p)M−i Mi .
i=1

377

Taking the derivative of m2(M, p) by p, we obtain

m2(M, p) = −M (1 − p)M−1 + M 1i pi−1(1 − p)M−i Mi − M Mi2− i pi(1 − p)M−i−1 Mi

i=1

i=1

= −M (1 − p)M−1 + 1 M 1 pi(1 − p)M−i M p i=1 i i

M −

M 1 pi(1 − p)M−i M

1 − p i=1 i2 i

+ 1 M 1 pi(1 − p)M−i M 1 − p i=1 i i

= −M (1 − p)M−1 + 1 m1(M, p) − (1 − p)M p

+ 1 −M m2(M, p) + M (1 − p)M + m1(M, p) − (1 − p)M 1−p

m1(M, p) (1 − p)M−1 M

= p(1 − p) −

p

− 1 − p m2(M, p).

Rearranging the terms, we get the following linear ﬁrst-order ODE

M

m1(M, p) (1 − p)M−1

m2(M, p) + 1 − p m2(M, p) = p(1 − p) −

. p

(E.12)

To solve this ODE, we consider the homogeneous ODE:

M m2(M, p) + 1 − p m2(M, p) = 0.

The solution of this ODE is m2(M, p) = C(1 − p)M , where C ∈ R is an arbitrary real constant. Next, we go back to the initial ODE (E.12) and try to ﬁnd a solution of the form m2(M, p) = C(p)(1 − p)M , where C(p) : R → R is a diﬀerentiable function:

C(p)(1 − p)M

+ M C(p)(1 − p)M = 1−p ⇓ C (p)(1 − p)M = ⇓ C (p) =

m1(M, p) − (1 − p)M−1

p(1 − p)

p

m1(M, p) − (1 − p)M−1

p(1 − p)

p

m1(M, p)

1

p(1 − p)M+1 − p(1 − p) .

378

Using (E.11) and (E.8), we derive

C (p)

(E=.8) = (E=.11) =

M

M

1 i

1 i

(1

−

p)M

−i

− p(i1=1− p) + i=p1(1 − p)M+1

M

1

M

1

− ip(1 − p) + ip(1 − p)i+1

i=1

i=1

M1 −
i=1 i

1+ 1 p 1−p

+M 1 i=1 i

11

1

1

p + 1 − p + (1 − p)2 + . . . + (1 − p)i+1

M1 i=1 i

1

1

(1 − p)2 + . . . + (1 − p)i+1

M

1

M1

=

(1 − p)i+1

, j

i=1

j=i

hence

C(p) = Cˆ + M 1 (1 − p)−i M 1 ,

i=1 i

j=i j

where Cˆ is a real constant. Putting all together, we obtain

m2(M, p) = C(p)(1 − p)M = Cˆ(1 − p)M + M 1i (1 − p)M−i M 1j .

i=1

j=i

Taking m2(M, 0) = 1 into account, we conclude that Cˆ = 1−

M1 i=1 i

M j=i

1 j

and

obtain

(E.9).

Using this lemma, we derive the following result:

Theorem E.3.7. Assume that peers participating in Moshpit Averaging have independent

random vectors x1, . . . , xn with means x1, . . . , xn and variances bounded by σ2 before the

averaging.

Let

x

T 1

,

.

.

.

,

x

T n

be

the

outputs

of

Moshpit

Averaging

after

T

iterations.

Finally,

we

assume that each peer from the grid can be dropped out for the whole averaging process before

averaging independently from other peers, i.e., n ∼ Binom(M N , p). Then, for all i = 1, . . . , n

we have

E xTi − Ex xTi 2 ≤ M T −1σ2m1(M − 1, p) (m2(M − 1, p))T −1 ,

(E.13)

where functions m1(M, p) and m2(M, p) are deﬁned in (E.8) and (E.9) respectively, and Ex [·]

denotes

the

expectation

w.r.t.

the

randomness

from

x1, . . . , xn.

Moreover,

if

p

≥

2 3

and

M

≥

11,

then

m1(M

− 1, p) ≤

2 M

,

m2(M

− 1, p) ≤

3 M2

and

T

T2

2σ2

E xi − Ex xi

≤ M (M/3)T −1 .

(E.14)

Proof. First of all, we recall an equivalent formulation of Moshpit Averaging. Consider a hypercube {1, . . . , M }N . One can consider the elements of this hypercube as hyperindices and assign a unique hyperindex to each peer so that peers can be viewed as vertices in the hypercube.

379

Then, during the k-th iteration of Moshpit All-Reduce, each worker computes the average among those peers that have hyperindices with the same values except the k-th index; in other words, peers compute averages along the k-th dimension of the hypercube. Next, if n = 0, we assume that xTi = Ex xTi and (E.13) holds for free. Therefore, to derive (E.13), we assume that n > 0.

More formally, we use the following notation: xCi = xi for all i = 1, . . . , n, where Ci =

(ci1

,

ci2

,

.

.

.

,

c

i N

),

cij

∈

{1, . . . , M }

for

all

j

=

1, . . . , M ,

and

Ci

=

Ck

for

i

=

k.

Let

C

be

the

set

of

hyperindices corresponding to all peers. Next, we use xtCi to deﬁne the vector stored on i-th

peer after t iterations of Moshpit Averaging. Then, for all i = 1, . . . , n we have x0Ci = xCi and

for all t = 1, . . . , N

xt = 1 Ci bi,t

xtC−k1,

k∈Ji,t

where

Ji,t

=

{k

∈

n

|

Ck

=

(ck1

,

.

.

.

,

c

k N

)

∈

C

and

ckj

=

cij

∀j

=

t}

and

bi,t

=

|Ji,t|.

Using

this,

we

derive the following formula for xtCi:

xT ≡ xT = 1

i

Ci bi,T

1 bi1,T −1

1 bi2,T −2

... 1 biT −1,1

xiT .

i1 ∈Ji,T

i2∈Ji1,T −1

i3∈Ji2,T −1

iT ∈JiT −1,1

Taking the expectation w.r.t. x1, . . . , xn, we get

Ex xTi

1

1

1

1

= bi,T

bi ,T −1

bi ,T −2

... bi ,1

xiT .

i1∈Ji,T 1

i2∈Ji1,T −1 2

i3∈Ji2,T −1

T −1 iT ∈JiT −1,1

Using the independence of x1, . . . , xn, we derive

Ex xTi − Ex xTi 2



2

= Ex 

...

xiT − xiT 


i1∈Ji,T i2∈Ji1,T −1

iT ∈JiT −1,1 bi,T bi1,T −1 . . . biT −1,1 

=

. . . Ex xiT − xiT 2

i1∈Ji,T i2∈Ji1,T −1 iT ∈JiT −1,1 b2i,T b2i1,T −1 . . . b2iT −1,1

σ2

≤

...

b2 b2 . . . b2

i1∈Ji,T i2∈Ji1,T −1 iT ∈JiT −1,1 i,T i1,T −1

iT −1,1

σ2

=

...

b2 b2

. . . b2

. bi ,1

i1∈Ji,T i2∈Ji1,T −1 iT −1∈JiT −2,2 i,T i1,T −1

iT −2,2 T −1

Next, taking the full expectation from the both sides of the previous inequality and using the tower property, we obtain





E

xTi − Ex xTi

2
≤ E



σ2

...

b2 b2

. . . b2

 . (E.15)

bi


,1

i,T i1,T −1

iT −2,2 T −1

i1∈Ji,T i2∈Ji1,T −1

iT −1∈JiT −2,2

Notice that Jik,T −k ∩ Jik+1,T −k−1 = {ik+1} for all k = 0, . . . , T − 1, where i0 = i. Moreover, for

380

k1, k2 ∈ {0, 1, . . . , T }, k1 < k2 either Jik1 ,T −k1 ∩ Jik2 ,T −k2 = {k2} or Jik1 ,T −k1 ∩ Jik2 ,T −k2 = ∅. The ﬁrst situation is possible iﬀ ik1 = ik1+1 = . . . ik2−1.
Taking these observations about sets Jik,T −k into account, we consider the sets Jik,T −k = Jik,T −k \ {ik} for k = 0, 1, . . . , T − 1. These sets are pairwise disjoint and their cardinalities bik,T −k = |Jik,T −k| satisfy the following relations: bik,T −k = 1 + bik,T −k ≥ max{1, bik,T −k} =: ˆbik,T −k for k = 1, 2, . . . , T − 1. Moreover, bi,T , bi1,T −1, . . . , biT−1,1 are independent random variables from the binomial distribution Binom(M − 1, p). Finally, we notice that the number of terms in (E.15) is upper-bounded by M T −1, since |Ji,t| ≤ M for all i = 1, . . . , n and t = 0, . . . , T .
Putting all together, we obtain

E xTi − Ex xTi 2





σ2

≤ E 

...



ˆb2 ˆb2

. . . ˆb2 ˆb



i1∈Ji,T i2∈Ji1,T −1 iT −1∈JiT −2,2 i,T i1,T −1

iT −2,2 iT −1,1

≤ M T −1σ2E

1

ξˆ12

ξˆ22

.

.

.

ξˆ2
T

−1

ξˆT

= M T −1σ2E 1 E 1 . . . E 1 E 1 ,

ξˆ12

ξˆ22

ξˆ2
T −1

ξˆT

where ξˆk2 = max{1, ξ12} for k = 1, . . . , T and ξ1, . . . , ξT are i.i.d. random variables having the binomial distribution Binom(M − 1, p). Then one can simplify the inequality above using Lemma E.3.6 and get

E xTi − Ex xTi 2 ≤ M T −1σ2m1(M − 1, p) (m2(M − 1, p))T −1 ,

where functions m1(M, p) and m2(M, p) are deﬁned in (E.8) and (E.9) respectively.
Next, we simplify the obtained upper bound under the assumption that M and p are not too small; speciﬁcally, M ≥ 11 and p ≥ 2/3. From (E.8), we have

Since we have

m1(M − 1, p) = (1 − p)M−1 + M−1 1i (1 − p)M−1−i − (1 − p)M−1
i=1

M −1
≤ (1 − p)M−1

1

.

i=1 i(1 − p)i

1

k(1 − p)k

k

1

(k + 1)(1 − p)k+1 ·

1

=

−−−→

≥ 3,

(k + 1)(1 − p) k→∞ 1 − p

M −1
(1 − p)M−1

1

= x (1 − p)M ·

1

=x 1 .

i=1 i(1 − p)i

M (1 − p)M

M

381

Using simple algebra, one can prove that for M ≥ 11 and p ≥ 2/3 the following inequality holds:

m1(M − 1, p) ≤ (1 − p)M−1 M−1 i(1 −1 p)i ≤ M2 .
i=1

Similarly, we analyze m2(M − 1, p):

m2(M − 1, p) = (1 − p)M−1 + M−1 1i (1 − p)M−1−i − (1 − p)M−1 M−1 1j

i=1

j=i

M −1
≤ (1 − p)M−1

1

M−1 1 .

i=1 i(1 − p)i j=i j

Since

M −1

M −1

1

1

k(1−p)k

j

(k − 1)

1 j

j=k

=

j=k

M −1

1

1

(k−1)(1−p)k−1

j

M −1

k(1 − p)

1 k−1

+

1 j

j=k−1

j=k

≥

3(k

−

1)

·

1 k

= 3(k − 1)2 −−−→ 3 ,

k

1 k−1

+

1 k

k(2k − 1) k→∞ 2

we have

M −1
(1 − p)M−1

1

M−1 1 = x

i=1 i(1 − p)i j=i j

(1 − p)M ·

1

M 2(1 − p)M

1 = x M2 .

Next, one can prove with simple algebra that for M ≥ 11 and p ≥ 2/3 the following inequality

holds:

m2(M − 1, p) ≤ (1 − p)M−1 M−1 i(1 −1 p)i M−1 1j ≤ M32 .

i=1

j=i

Plugging the obtained upper bounds for m1(M − 1, p) and m2(M − 1, p) in (E.13), we obtain (E.14).

E.4 Convergence Proofs of Moshpit SGD
In this section, we provide the complete statements of the theorems establishing the convergence of Moshpit SGD together with the full proofs. First, we introduce all necessary deﬁnitions, basic inequalities and auxiliary lemmas; then we prove the convergence in strongly convex and convex cases; lastly, we provide the proofs for the non-convex case.
E.4.1 Convex Case
In this section, we give the full proof of Theorem 6.3.5 about the convergence of Moshpit SGD for convex and strongly convex problems. The scheme of the proof follows the similar steps as in the state-of-the-art analysis of Local-SGD [89, 229, 56]. We start with the following lemma:

382

Lemma E.4.1. Let f1 = . . . = fn = f , function f be µ-strongly convex (Def. A.1.2) and L-smooth (see Def. A.1.1), and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2 and x = x∗, where x∗ ∈ argminx∈Rd f (x) and δpv,1 ∈ [0, 1), δpv,2 ≥ 0. Then, for any k ≥ 0 the iterates produced by Moshpit SGD with γ ≤ 1/4L satisfy

γE f (xk) − f (x∗)

≤ (1 − γµ(1 − δpv,1))E xk − x∗ 2 − E

+ 3Lγ E[V ] + γ2 σ2 + δ2 ,

2

k

nmin pv,2

xk+1 − x∗ 2

(E.16)

where Vk = n1k i∈Pk xki − xk 2 and xk = n1k i∈Pk xki .

Proof. Recall that Assumption 6.3.4 with ∆kpv = δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2 and x = x∗ states

E xk+1 − xk+1, xk+1 + xk+1 − 2x∗ ≤ δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2,

(E.17)

where xk+1 = n1k

i∈Pk (xki − γgik). Next, the deﬁnition of xk+1 implies

xk+1 = 1 nk

xk − γ i nk

gik = xk − γgk,

i∈Pk

i∈Pk

where gk = n1k i∈Pk gik. Using this, we derive

xk+1 − x∗ 2 = = =

xk+1 − x∗ 2 + 2 xk+1 − xk+1, xk+1 − x∗ + xk+1 − xk+1 2 xk − x∗ − γgk 2 + xk+1 − xk+1, xk+1 + xk+1 − 2x∗ xk − x∗ 2 − 2γ xk − x∗, gk + γ2 gk 2 + xk+1 − xk+1, xk+1 + xk+1 − 2x∗ .

Taking the conditional expectation E · | xk := E · | Pk, xki , i ∈ Pk from the both sides of the previous equation and using Assumption 6.3.3, we obtain



2

E xk+1 − x∗ 2 | xk = xk − x∗ 2 − 2γ xk − x∗, 1 nk

∇f (xk) + γ2E  1

i

 nk

gik | xk

i∈Pk

i∈Pk

+E xk+1 − xk+1, xk+1 + xk+1 − 2x∗ | xk .

(E.18)

383

Next, we estimate the second and the third terms in the right-hand side of (E.18). First,

−2γ

xk − x∗, 1 nk

∇f (xki )

i∈Pk

=
(A.5),(C.3)
≤
(A.11)
≤

2γ nk i∈Pk

x∗ − xki , ∇f (xki ) + xki − xk, ∇f (xki )

2γ nk i∈Pk

f (x∗) − f (xki ) − µ2 xki − x∗ 2

+ 2γ nk i∈Pk

f (xki ) − f (xk) + L2 xki − xk 2

2γ f (x∗) − f (xk) − γµ xk − x∗ 2 +LγVk,

(E.19)

where Vk = n1k i∈Pk xki − xk 2. Secondly, since stochastic gradients {gik}i∈Pk are computed independently, we get

 γ2E 


1

gk

nk

i

i∈Pk

2 | xk 

(A=.14)
(A.11)
≤
(A.11),(A.3),(6.7)
≤
(A.1)
≤

2

γ2 1 nk

∇f (xki )

i∈Pk



2

+γ2E  1  nk

(gik − ∇f (xki ))

i∈Pk

| xk 

2

2γ2 1 nk

(∇f (xki ) − ∇f (xk))

i∈Pk

+ 2γ2 ∇f (xk) 2

γ2 + n2 E
k i∈Pk

gik − ∇f (xki ) 2 | xk

2γ2

∇f (xk) − ∇f (xk) 2

nk

i

i∈Pk

+4Lγ2 f (xk) − f (x∗) + γ2σ2 nk

2L2γ2

k

k2

nk

xi − x

i∈Pk

2L2γ2Vk
+4Lγ2 f (xk) − f (x∗)

+ γ2σ2 . nmin

(E.20)

Plugging (E.19) and (E.20) in (E.18), we obtain

E xk+1 − x∗ 2 | xk

≤ (1 − γµ) xk − x∗ 2 − 2γ (1 − 2Lγ) f (xk) − f (x∗)
γ2σ2 +Lγ (1 + 2Lγ) Vk +
nmin +E xk+1 − xk+1, xk+1 + xk+1 − 2x∗ | xk ,

384

and E xk+1 − x∗ 2

(E.17)
≤
≤

(1 − γµ(1 − δpv,1))E xk − x∗ 2 − 2γ (1 − 2Lγ) E f (xk) − f (x∗)

+Lγ (1 + 2Lγ) E[Vk] + γ2

σ2 + δ2 nmin pv,2

(1 − γµ(1 − δpv,1))E xk − x∗ 2 − γE f (xk) − f (x∗)

+ 3Lγ E[V ] + γ2 σ2 + δ2 ,

2

k

nmin pv,2

where in the last inequality we use γ ≤ 1/4L.

Next, we estimate the term E[Vk] measuring the expected dissimilarity between local iterates and their global average at iteration k.

Lemma E.4.2. Let f1 = . . . = fn = f , function f be µ-strongly convex (Def. A.1.2) and L-smooth (see Def. A.1.1), and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2 and x = x∗, where x∗ ∈ argminx∈Rd f (x) and δpv,1 ∈ [0, 1), δpv,2 ≥ 0. Then, for any k ≥ 0 the iterates produced by Moshpit SGD with γ ≤ 1/4L satisfy

E[Vk] ≤ 2γ2 4δa2q + (τ − 1)σ2 ,

(E.21)

where Vk = n1k i∈Pk xki − xk 2 and xk = n1k i∈Pk xki .

Proof. First of all, if k = aτ for some integer a ≥ 0, then (E.21) follows from Assumption 6.3.4 (eq. (6.10)). Therefore, we consider such k that k = aτ + t for some t ∈ (0, τ ). Then, for any i, j ∈ Pk, i = j

E xki − xkj 2 | xk−1

= (A=.14)

E xki −1 − γgik−1 − xjk−1 + γgjk−1 2 | xk−1
xki −1 − γ∇f (xki −1) − xkj −1 + γ∇f (xjk−1) 2 +γ2E gik−1 − ∇f (xki −1) + gjk−1 − ∇f (xkj −1) 2 | xk−1 .

Using Lemma A.5.8 and independence of gik−1 and gjk−1 for given xki −1, xkj −1, i = j we derive

E xki − xkj 2 | xk−1

(A.29)
≤
(6.7)
≤

(1 − γµ) xki −1 − xkj −1 2 + γ2E gik−1 − ∇f (xki −1) 2 | xk−1 +γ2E gjk−1 − ∇f (xjk−1) 2 | xk−1
(1 − γµ) xki −1 − xkj −1 2 + 2γ2σ2,

from which we get the following:

Eg xki − xkj 2 ≤ (1 − γµ)Eg xik−1 − xkj −1 2 + 2γ2σ2 ≤ Eg xki −1 − xkj −1 2 + 2γ2σ2.

385

Here, Eg[·] denotes the expectation conditioned on {Pk}(ka=+a1τ)τ−1. Unrolling the recurrence, we get

Eg xki − xkj 2

≤ Eg ≤ Eg

xai τ − xaj τ 2 + 2(k − aτ )γ2σ2 xai τ − xaj τ 2 + 2(τ − 1)γ2σ2.

(E.22)

Using this, we estimate Eg[Vk]:

Eg [Vk ]

=
(E.22)
≤
(A.10)
≤
=
≤
≤

 1 nk Eg 
i∈Pk

xk − 1

xk

i nk

j

j∈Pk

2

(A.11) 1

 

≤

n2

Eg

k i,j∈Pk

xki − xkj 2

1 n2k i,j∈Pk Eg

xai τ − xaj τ 2 + 2(τ − 1)γ2σ2

2 n2k i,j∈Pk Eg

xai τ − xaτ 2 + Eg

xaj τ − xaτ 2

4

nk

Eg

i∈Pk

xai τ − xaτ 2 + 2(τ − 1)γ2σ2

4 · naτ Eg naτ nk i∈Paτ

xai τ − xaτ 2 + 2(τ − 1)γ2σ2

 8
Eg  naτ
i∈Paτ

xai τ − xaτ



2


+

2(τ

−

1)γ2σ2,

+ 2(τ − 1)γ2σ2

where in the last inequality we use 2n(a+1)τ = 2|P(a+1)τ | ≥ |Paτ | = naτ and |nk| ≤ |nk−1| following from Assumption 6.3.4. Finally, we take the full expectation from the previous inequality and derive





(A.15)

1

E[Vk] ≤ 8E 

naτ

(6.10)

xai τ − xaτ

2


+

2(τ

−

1)γ2σ2

≤

2γ2

4δa2q + (τ − 1)σ2

,

i∈Paτ

which ﬁnishes the proof.

Combining Lemmas E.4.1 and E.4.2, we get the following result:

Theorem E.4.3 (Theorem 6.3.5, convergence in the convex case). Let f1 = . . . = fn = f be µ-strongly convex (Def. A.1.2) and L-smooth (see Def. A.1.1), and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γµE[ xk − x∗ 2] + γ2δp2v,2 and x = x∗, where x∗ ∈ argminx∈Rd f (x) and δpv,1 ∈ [0, 1), δpv,2 ≥ 0. Then, for any K ≥ 0, the iterates produced by Moshpit SGD with γ ≤ 1/4L satisfy

E f (xK ) − f (x∗) ≤ (1 − γµ(1 − δpv,1))K R02 + γ γ

σ2 + δ2 + 3Lγ nmin pv,2

4δa2q + (τ − 1)σ2 , (E.23)

386

when µ > 0, and

E f (xK ) − f (x∗) ≤ R02 + γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2 ,

γK

nmin pv,2

aq

(E.24)

when µ = 0, where R0 = x0 − x∗ , xK = W1K Kk=0 wkxk = W1K Kk=0 wnkk i∈Pk xki , wk = (1 −

γµ(1 − δpv,1))−(k+1), and WK =

K k=0

wk

.

That is, Moshpit

SGD achieves E[f (xK)−f (x∗)] ≤ ε

after



L

σ2

δp2v,2

K =O

+

+

+

(1 − δpv,1)µ nmin(1 − δpv,1)µε (1 − δpv,1)µε

 L((τ − 1)σ2 + δa2q)
(1 − δpv,1)2µ2ε  (E.25)

iterations with

 

ln max 2, min R02µ2(1−δpv,1)2K2 , R02µ3(1−δpv,1)3K3

 

γ = min  1 ,

(δp2v,2+σ2/nmin) 3L(4δa2q+(τ −1)σ2)

 

 4L

(1 − δpv,1)µK











when µ > 0, and after

 LR02

R02σ2

R02δp2v,2 R02

K = O  ε + nminε2 + ε2 +

L((τ − 1)σ2 + δa2q) 

ε3/2



(E.26)

iterations with
 γ = min  1
 4L

R0

(δ2 + σ2/n )K ,

pv,2

min

3

3L

R02 4δa2q + (τ − 1)σ2

 
K

when µ = 0.

Proof. Plugging the result of Lemma E.4.2 in inequality (E.16) from Lemma E.4.1, we obtain

γE f (xk) − f (x∗)

≤ (1 − γµ(1 − δpv,1))E xk − x∗ 2 − E xk+1 − x∗ 2

+3Lγ3 4δ2 + (τ − 1)σ2 + γ2 σ2 + δ2 .

aq

nmin pv,2

Next, we sum up these inequalities for k = 0, . . . , K with weights wk = (1 − γµ(1 − δpv,1))−(k+1)

387

and divide both sides by γWK, where WK =

K k=0

wk

:

1

K
w E f (xk) − f (x∗)

WK

k

k=0

≤

1

K
(1 − γµ(1 − δ

))w E xk − x∗ 2

γWK

pv,1 k

k=0

1K

−

wk E

γWK k=0

xk+1 − x∗ 2

+γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2

nmin pv,2

aq

1K wk
WK k=0

= 1 K w E xk − x∗ 2 − w E xk+1 − x∗ 2

γWK

k−1

k

k=0

+γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2

nmin pv,2

aq

w−1 x0 − x∗ 2 − wK E xK+1 − x∗ 2 = γWK

+γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2

nmin pv,2

aq

≤ x0 − x∗ 2 + γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2 .

γWK

nmin pv,2

aq

Since f is convex, we apply the Jensen’s inquality

f 1 K wkxk WK k=0
to the previous result and get

≤

1

K
w f (xk)

WK

k

k=0

E f (xK ) − f (x∗) ≤ R02 + γ σ2 + δ2 + 3Lγ 4δ2 + (τ − 1)σ2 ,

γWK

nmin pv,2

aq

where R0 = x0 − x∗ and xK = W1K Kk=0 wkxk = W1K Kk=0 wnkk i∈Pk xki . If µ > 0, then WK ≥ wK ≥ (1 − γµ(1 − δpv,1))−K , implying (E.23). Next, wk = 1 and WK = K when µ = 0
gives (E.24). It remains to estimate the total number of iterations K required by Moshpit SGD to ﬁnd an ε-solution, i.e., to achieve E[f (xK) − f (x∗)] ≤ ε. Applying Lemma A.5.5 to (E.23),
we get the following result: if µ > 0 and

 

ln max 2, min R02µ2(1−δpv,1)2K2 , R02µ3(1−δpv,1)3K3

 

γ = min  1 ,

δp2v,2+σ2/nmin 3L(4δa2q+(τ −1)σ2)  ,

 4L

(1 − δpv,1)µK











388

then

 E f (xK ) − f (x∗) = O LR02 exp

− µ (1 − δpv,1)K L

δp2v,2 + / σ2 nmin L δa2q + (τ − 1)σ2  + (1 − δpv,1)µK + (1 − δpv,1)2µ2K2  ,

implying (E.25). Similarly, we apply Lemma A.5.6 to (E.24) and get that for µ = 0 and

 γ = min  1
 4L

R0

(δ2 + σ2/n )K ,

pv,2

min

3

3L

R02 4δa2q + (τ − 1)σ2


 ,
K

 E f (xK ) − f (x∗) = O  LR02 +
K



R2(δ2 + σ2/nmin) 3 R04L δa2q + (τ − 1)σ2

0 pv,2

+

 ,

K

K 2/3



implying (E.26).

E.4.2 Non-Convex Case
In this section, we give the full proof of Theorem 6.3.6 about convergence of Moshpit SGD for general non-convex problems. The proof follows the similar steps as in the state-of-the-art analysis of Local-SGD in non-convex case [117, 97]. We start with the following lemma:

Lemma E.4.4. Let f1 = . . . = fn = f , function f be L-smooth and bounded from below by f∗, and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γE[ ∇f (xk) 2]+Lγ2δp2v,2, δpv,1 ∈ [0, 1/2), δpv,2 ≥ 0. Then, for any K ≥ 0 the iterates produced by Moshpit SGD with γ ≤ (1−2δpv,1)/8L satisfy

(1 − 2δpv,1)γ K−1

4

E

k=0

∇f (xk) 2

K −1
≤ f (x0) − f∗ + γL2 E[Vk]
k=0
+KLγ2 σ2 + δ2 , nmin pv,2

(E.27)

where Vk = n1k i∈Pk xki − xk 2 and xk = n1k i∈Pk xki .

Proof. Recall that Assumption 6.3.4 with ∆kpv = δpv,1γE[ ∇f (xk) 2] + Lγ2δp2v,2 states

E ∇f (xk), xk+1 − xk+1 + L xk+1 − xk+1 2 ≤ δpv,1γE[ ∇f (xk) 2] + Lγ2δp2v,2, (E.28)

where xk+1 = n1k implies

i∈Pk (xki − γgik). As for the convex case, we notice that the deﬁnition of xk+1

xk+1 = 1 nk

xk − γ i nk

gik = xk − γgk,

i∈Pk

i∈Pk

389

where gk = n1k i∈Pk gik. Using this and L-smoothness of f , we derive

f (xk+1) − f (xk)

(C.3)
≤
(A.10)
≤
=

∇f (xk), xk+1 − xk + L xk+1 − xk 2 2
∇f (xk), xk+1 − xk + ∇f (xk), xk+1 − xk+1 + L xk+1 − xk 2 +L xk+1 − xk+1 2
−γ ∇f (xk), gk + Lγ2 gk 2 + ∇f (xk), xk+1 − xk+1 +L xk+1 − xk+1 2,

from which it follows that E f (xk+1) − f (xk) | xk



2

≤ −γ ∇f (xk), 1

∇f (xk) + Lγ2E  1

gk

nk

i

 nk

i

i∈Pk

i∈Pk

| xk 

+E ∇f (xk), xk+1 − xk+1 + L xk+1 − xk+1 2 | xk , (E.29)

where E · | xk := E · | Pk, xki , i ∈ Pk . Next, we estimate the second and third terms in the right-hand side of (E.29). First of all,

−γ ∇f (xk), 1 nk

∇f (xki )

i∈Pk

=
(A.7)
≤
(A.11)
≤
(A.1)
≤

−γ ∇f (xk) 2 − γ

∇f (xk), 1 nk

∇f (xki ) − ∇f (xk)

i∈Pk

−γ ∇f (xk) 2 + γ ∇f (xk) 2 2

2

+γ 1 2 nk

(∇f (xki ) − ∇f (xk))

i∈Pk

γ −

∇f (xk) 2 +

γ

2

2nk

∇f (xki ) − ∇f (xk) 2

i∈Pk

− γ2 ∇f (xk) 2 + γL2 2 Vk,

(E.30)

where Vk = n1k i∈Pk xki − xk 2. Secondly, since the stochastic gradients {gik}i∈Pk are computed

390

independently, we derive



2

Lγ2E  1

gk

 nk

i

i∈Pk

| xk 

(A=.14)
(A.11)
≤
(A.11),(6.7)
≤
(A.1)
≤

2

Lγ2 1 nk

∇f (xki )

i∈Pk



2

+Lγ2E  1  nk

(gik − ∇f (xki ))

i∈Pk

| xk 

2Lγ2

2

1 (∇f (xk) − ∇f (xk))

nk

i

i∈Pk

+2Lγ2 ∇f (xk) 2 + γ2L E n2k i∈Pk

gik − ∇f (xki ) 2 | xk

2γ2L nk

∇f (xki ) − ∇f (xk) 2 + 2Lγ2 ∇f (xk) 2

i∈Pk

+ γ2Lσ2 nk

2L3γ2 nk

xki − xk 2 +2Lγ2 ∇f (xk) 2

i∈Pk

2L3 γ 2 Vk
+ γ2Lσ2 . nmin

(E.31)

Plugging (E.30) and (E.31) in (E.29), we obtain

E f (xk+1) − f (xk) | xk

≤ − γ (1 − 4Lγ) ∇f (xk) 2 + γL2 (1 + 4Lγ) V + Lγ2σ2

2

2

k nmin

+E ∇f (xk), xk+1 − xk+1 + L xk+1 − xk+1 2 | xk .

Next, we take the full expectation from the both sides of the above inequality, apply the tower property (A.15) and take into account that γ ≤ (1−2δpv,1)/8L:

E f (xk+1) − f (xk)

≤

− γ (1 − 4Lγ) E

∇f (xk) 2

γL2

Lγ2σ2

+ (1 + 4Lγ) E[V ] +

2

2

k

nmin

+E ∇f (xk), xk+1 − xk+1 + L xk+1 − xk+1 2

(E.28)
≤

γ − 2 (1 − 2δpv,1 − 4Lγ) E

+Lγ2

σ2 + δ2 nmin pv,2

∇f (xk) 2 + γL2 2 (1 + 4Lγ) E[Vk]

≤ − (1 − 2δpv,1)γ E ∇f (xk) 2 + γL2E[V ] + Lγ2 σ2 + δ2 .

4

k

nmin pv,2

391

Summing up the obtained inequalities for k = 0, . . . , K − 1 and rearranging the terms, we derive

(1 − 2δpv,1)γ K−1

4

E

k=0

∇f (xk) 2

K −1

K −1

≤

E f (xk) − f (xk+1) + γL2 E[Vk]

k=0

k=0

+K Lγ 2

σ2 + δ2 nmin pv,2

K −1
= f (x0) − E[f (xK )] + γL2 E[V ] + KLγ2

σ2 + δ2

k

nmin pv,2

k=0

K −1
≤ f (x0) − f + γL2 E[V ] + KLγ2

σ2 + δ2

,

∗

k

nmin pv,2

k=0

where f∗ is a uniform lower bound for f .

The next step towards completing the proof of Theorem 6.3.6 gives the upper bound for

K −1 k=0

E[Vk]

that

appeared

in

(E.27).

Lemma E.4.5. Let f1 = . . . = fn = f be L-smooth and bounded from below by f∗, and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γE[ ∇f (xk) 2] + Lγ2δp2v,2, δpv,1 ∈ [0, 1/2), δpv,2 ≥ 0. Then, for any K ≥ 0 the iterates produced by Moshpit SGD with γ ≤ 1/(4√eL(τ−1))
satisfy

K −1

K −1

E[Vk] ≤ 8eγ2(τ − 1)2 E[ ∇f (xk) 2] + 4γ2K 2δa2q + e(τ − 1)σ2 , (E.32)

k=0

k=0

where Vk = n1k i∈Pk xki − xk 2 and xk = n1k i∈Pk xki .

Proof. First of all, consider k such that k = aτ + t for some t ∈ [0, τ ). Let Eg[·] denote the expectation conditioned on {Pt}(t=a+aτ1)τ−1. Then

Eg [Vk ]

=
=
(A.10)
≤

1

nk

Eg

i∈Pk

xki − xk 2



1

Eg xaτ − xaτ

nk

i

i∈Pk

2

nk

Eg

i∈Pk

xai τ − xaτ

(A.14) 1

≤

Eg

nk i∈P

k

k−1 2 − γ git 

t=aτ

 2 + 2γ2 Eg 
nk i∈Pk

xki − xaτ 2
k−1 2 git  .
t=aτ

(E.33)

392

Next, we estimate the second term in the right-hand side of (E.33) using Lemma A.5.2:

2γ2


k−1

2

Eg 

git 

nk i∈Pk

t=aτ

(A.17)
≤
(A.10),(6.7)
≤
(A.1)
≤
≤

2eγ2(k − aτ ) nk

k−1
Eg[ ∇f (xti) 2]

i∈Pk t=aτ

+ 2eγ2 nk

k−1
Eg[ git − ∇f (xti) 2]

i∈Pk t=aτ

k−1
4eγ2(τ − 1) Eg[ ∇f (xt) 2]
t=aτ

k−1
+4eγ2(τ − 1)

1

nk

Eg[ ∇f (xti) − ∇f (xt) 2]

t=aτ i∈Pk

+2eγ2(k − aτ )σ2

k−1
4eγ2(τ − 1) Eg[ ∇f (xt) 2]
t=aτ

k−1
+4eγ2L2(τ − 1)

nt

·

1

nk nt

Eg[ xti − xt 2]

t=aτ

i∈Pt

+2eγ2(τ − 1)σ2

k−1

k−1

4eγ2(τ − 1) Eg[ ∇f (xt) 2] + 8eγ2L2(τ − 1) Eg[Vt]

t=aτ

t=aτ

+2eγ2(τ − 1)σ2,

where in the last two inequalities we use nk = |Pk| ≤ |Pk−1| = nk−1 for all k ≥ 1 and naτ ≤ 2n(a+1)τ for all integer a ≥ 0. Plugging this inequality in (E.33) and taking the full expectation from the result, we get





E[Vk] ≤ 2E  1 nk

k−1

xai τ − xaτ

2


+

4eγ2(τ

−

1)

E[ ∇f (xt) 2]

i∈Pk

t=aτ

k−1
+8eγ2L2(τ − 1) E[Vt] + 2eγ2(τ − 1)σ2

t=aτ





≤ 4E  1 naτ

k−1

xai τ − xaτ

2


+

4eγ2(τ

−

1)

E[ ∇f (xt) 2]

i∈Paτ

t=aτ

k−1
+8eγ2L2(τ − 1) E[Vt] + 2eγ2(τ − 1)σ2
t=aτ

(6.10)

k−1

k−1

≤ 4eγ2(τ − 1) E[ ∇f (xt) 2] + 8eγ2L2(τ − 1) E[Vt] + 2γ2 2δa2q + e(τ − 1)σ2 ,

t=aτ

t=aτ

where in the second inequality we also use nk = |Pk| ≤ |Pk−1| = nk−1 for all k ≥ 1 and naτ ≤ 2n(a+1)τ for all integer a ≥ 0. Summing up the obtained inequalities for k = aτ, aτ + 1, . . . , K

393

for some K ∈ [aτ, (a + 1)τ − 1] we derive

K

K k−1

K k−1

E[Vk] ≤ 4eγ2(τ − 1)

E[ ∇f (xt) 2] + 8eγ2L2(τ − 1)

E[Vt]

k=aτ

k=aτ t=aτ

k=aτ t=aτ

+2γ2(K − aτ + 1) 2δa2q + e(τ − 1)σ2

K

K

≤ 4eγ2(τ − 1)2 E[ ∇f (xk) 2] + 8eγ2L2(τ − 1)2 E[Vk]

k=aτ

k=aτ

+2γ2(K − aτ + 1) 2δa2q + e(τ − 1)σ2

≤ 4eγ2(τ − 1)2 K E[ ∇f (xk) 2] + 21 K E[Vk]

k=aτ

k=aτ

+2γ2(K − aτ + 1) 2δa2q + e(τ − 1)σ2 ,

where in the last inequality we use γ ≤ 1/(4√eL(τ−1)). Rearranging the terms, we get that for K ≥0

K

K

E[Vk] ≤ 8eγ2(τ − 1)2 E[ ∇f (xk) 2] + 4γ2(K − aτ + 1) 2δa2q + e(τ − 1)σ2 ,

k=aτ

k=aτ

where a ≥ 0 is an integer such that aτ ≤ K ≤ (a+1)τ −1. Summing up the obtained inequalities for K = τ − 1, 2τ − 1, . . . , τ (K−1)/τ − 1, K − 1, we derive (E.32).

Combining Lemmas E.4.4 and E.4.5, we get the following result:

Theorem E.4.6 (Theorem 6.3.6). Let f1 = . . . = fn = f , function f be L-smooth and bounded

from below by f∗, and Assumptions 6.3.3 and 6.3.4 hold with ∆kpv = δpv,1γE[ ∇f (xk) 2] +

Lγ2δp2v,2, δpv,1 ∈ [0, 1/2), δpv,2 ≥ 0. Then, for any K ≥ 0 the iterates produced by Moshpit SGD

with

γ ≤ min

1 − 2δpv,1 1 − 2δpv,1 ,√

8L 8 eL(τ − 1)

satisfy

E ∇f (xKrand) 2

≤ 8∆0 (1 − 2δpv,1)Kγ

+ 8Lγ 1 − 2δpv,1

σ2 + δ2 + 4γL 2δ2 + e(τ − 1)σ2

nmin pv,2

aq

,(E.34)

where ∆0 = f (x0) − f∗ and xKrand is chosen uniformly at random from {x0, x1, . . . , xK−1}. That

394

is, Moshpit SGD achieves E ∇f (xKrand) 2 ≤ ε2 after

K =O

L∆0

1 + (τ − 1)

(1 − 2δpv,1)2ε2

δp2v,2 + / σ2 nmin

1 − 2δpv,1 +

ε2

√ + (1−2δpv,1)(εδa2q+(τ −1)σ2)

(E.35)

iterations with



γ = min  1 − 2δpv,1 ,

1 − 2δpv,1

√

,

 8L 8 eL(τ − 1)





∆0 , 3

∆0  .

LK δp2v,2 + / σ2 nmin

4L2

2δa2q + e(τ − 1)σ2

 

Proof of Theorem 6.3.6. Plugging the result of Lemma E.4.5 in the inequality (E.27) from Lemma E.4.4, we obtain

(1 − 2δpv,1)γ K−1

4

E

k=0

∇f (xk) 2

K −1
≤ f (x0) − f∗ + 8eγ3L2τ (τ − 1) E[ ∇f (xk) 2]

k=0

+K Lγ 2

σ2 + δ2 + 4γL 2δ2 + e(τ − 1)σ2

nmin pv,2

aq

≤ f (x0) − f∗ + (1 − 28δpv,1)γ K−1 E
k=0

∇f (xk) 2

+KLγ2 σ2 + δ2 + 4γL 2δ2 + e(τ − 1)σ2 .

nmin pv,2

aq

Next,

1K KE
k=0

∇f (xk) 2

≤ 8∆0 (1 − 2δpv,1)Kγ

+ 8Lγ

σ2 + δ2 + 4γL 2δ2 + e(τ − 1)σ2 ,

1 − 2δpv,1 nmin pv,2

aq

where ∆0 = f (x0) − f∗. Since xKrand is chosen uniformly at random from {x0, x1, . . . , xK−1}, we

have

E ∇f (xKrand) 2 (A=.15) K1 K E ∇f (xk) 2
k=0

and (E.34) holds. Applying Lemma A.5.6 to (E.34), we get the following result: if



γ = min  1 − 2δpv,1 ,

1 − 2δpv,1

√

,

 8L 8 eL(τ − 1)





∆0 , 3

∆0  ,

LK δp2v,2 + / σ2 nmin

4L2

2δa2q + e(τ − 1)σ2

 

395

then E ∇f (xKrand) 2 is of the order



O  L∆0 1 + (τ − 1) 1 − 2δpv,1 +



(1 − 2δpv,1)2K

L∆0 δp2v,2 + / σ2 nmin (1 − 2δpv,1)2K



3 L2∆20(δa2q + (τ − 1)σ2)

+

,

(1 − 2δpv,1)K2/3



which implies the desired convergence result from (E.35).

E.5 Training with a Dynamic Number of Peers
Many practical setups with unreliable devices allow peers to join or leave at any time, which can produce undesirable side-eﬀects. For instance, consider a participant that joins the “swarm” midway through the training process. If this participant starts with the initial model parameters, it can undo some of the progress made by other peers.
To circumvent this issue, we require each new participant to download the latest parameters from a random up-to-date peer discovered through DHT. The same technique is used to synchronize the optimizer statistics and the learning rate schedule. This protocol is also triggered if a peer becomes desynchronized with others, e.g., after a network freeze.

E.6 Load Balancing via Linear Programming
When running Moshpit Averaging on heterogeneous devices, one must regularly perform Butterﬂy All-Reduce among peers with uneven network bandwidth. In order to speed up the protocol, we can make low-throughput peers receive, average, and send smaller partitions of the averaged vector; conversely, the high-throughput peers can process greater fractions of the input vector. To compute the optimal partitioning, peers must solve an optimization problem that minimizes the total time spent on communication during all-reduce.
Consider a group of M peers with network bandwidths b1, ..., bM , deﬁned for simplicity as the minimum of the upload and download speed for each peer. Our objective is to ﬁnd wi — a fraction of all input vectors to be processed by the i-th peer.
In Butterﬂy All-Reduce, each peer i splits its vector into parts and sends these parts to corresponding peers. Since there is no need to send wi to itself, i-th peer will upload a total of 1 − wi of the vector to its peers. On the receiving side, peer i will average wi of the vector from all peers in its group. To do so, it must download M − 1 vector parts of size wi from all other peers. After that, peers distribute the averaged parts by running the same procedure in reverse (see Figure 6.1).
Thus, the communication time for each peer is proportional to ti = (1 − wi + (M − 1)wi) · b1i and the total runtime of Butterﬂy All-Reduce is the maximum communication time over all peers: T = maxi ti = maxi(1 − wi + (M − 1)wi) · b1i . Formally, we minimize T with respect to wi with

396

two constraints on the fraction weights:

min
w
subject to

max(1 − wi+(M − 1)wi) · 1

i

bi

M

wi = 1

i=1

wi ≥ 0

∀i = 1, . . . , M

Because the functions being maximized and the constraints are linear in wi, this problem can be reduced to linear programming [85]. Namely, we can minimize a surrogate variable ξ such that ∀i, ξ ≥ (1 − wi + (M − 1) · wi) · b1i . The resulting linear program is formulated as follows:

min
w,ξ
subject to

ξ
M
wi = 1
i=1
wi ≥ 0 ξ ≥ (1−wi + (M − 1)wi) · 1 bi

∀i = 1, . . . , M ∀i = 1, . . . , M

We solve this problem using the interior point method [7] implemented as part of the SciPy package (scipy.optimize.linprog). Note that depending on the conditions given by participant bandwidth, optimal weights of speciﬁc peers might be equal to 0 in some cases. In essence, this allows our method to smoothly interpolate between data parallelism [220], parameter server [114] and sharded parameter server [34] in manner similar to BytePS [81].

E.7 Detailed Experimental Setup
In this section, we provide the detailed hardware conﬁguration of servers used for each of our distributed training experiments.
E.7.1 ImageNet Training
Both homogeneous and heterogeneous training setups for ImageNet are provisioned in our on-premise infrastructure across multiple data centers and an oﬃce space (for the heterogeneous setup only).
Homogeneous. For the homogeneous setup, we use 16 identical instances with the following speciﬁcations:
• GPU: V100-PCIe,

397

• CPU: 6 vCPUs (Xeon E5-2650v4), • RAM: 64GB.

Heterogeneous. In turn, the heterogeneous setup contains multiple instance types listed in Table E.2:
Table E.2: Heterogeneous setup for ImageNet training.

Instances
4 17 7 16 20

GPUs
1 2 1 1 1

GPU type
V100-PCIe GTX 1080Ti GTX 1080Ti
P40 M40-24GB

Cores
6 8 4 4 4

RAM, GB
64 64 32 32 32

CPU type
E5-2650v4 E5-2650v4 E5-2650v4 E5-2667v2 E5-2667v2

E.7.2 ALBERT Training
Homogeneous. For the homogeneous setup, we use a single virtual machine with the following speciﬁcations:
• GPU: 8× V100-PCIe,
• CPU: 48 vCPUs (Xeon E5-2650v4),
• RAM: 488GB.
At the time of writing, the cloud rent cost for this instance is $24.48 per hour.
Heterogeneous. Our heterogeneous setup is composed of two parts: AWS EC2 Spot instances and crowdsourced machines from the Vast.ai marketplace. For spot instances, we picked the smallest suitable instance size available from the cloud provider and further limited their bandwidth to 1Gb/s2. As for marketplace instances, we report the hardware speciﬁcations for each worker gathered 1 hour after the start of ALBERT training.
Since both cloud and marketplace instances are preemptible, the actual cost of the server ﬂeet will vary based on the current price. For simplicity, we report the maximum hourly price we ended up paying for this instance (enforced via maximum bid). Finally, some marketplace instances have missing speciﬁcations, such as unknown CPU type. This is likely caused by non-standard virtualization conﬁgured by the device owner. The resulting ﬂeet conﬁguration, shown in Table E.3, costs up to $15.43/hour, depending on the number of active instances.
2We use tc qdisc Linux utility to artiﬁcially limit the network throughput, similarly to [80]

398

Table E.3: Heterogeneous setup for ALBERT training.

GPU

Cores RAM, GB

CPU type

Download, Mb/s Upload, Mb/s Cost, $/hour

Preemptible g4dn.xlarge instances (32×)

T4

4

16

Xeon Platinum 8259CL

1000

1000

0.1578

Marketplace instances

GTX 1070Ti 6

16

E5-2640

425

GTX 1070Ti 6

16

i3-6100T

121

GTX 1080Ti 4

20

i3-6096P

817

GTX 1080Ti 20

129

E5-2630v4

660

GTX 1080Ti 1

16

i7-7700K

245

GTX 1080Ti 48

97

Xeon Platinum 8124

583

GTX 1080Ti 10

16

Unknown

n/a

GTX 1080Ti 4

16

Xeon Gold 6149

98

GTX 1080Ti 4

16

Xeon Gold 6149

99

GTX 1080Ti 4

16

Xeon Gold 6149

99

GTX 1080Ti 4

16

Xeon Gold 6149

99

RTX 2070S

24

32

E5-2620v2

199

RTX 2070S

32

97

E5-2650

162

RTX 2080

6

16

E5-2620v3

271

RTX 2080

24

32

E5-2630v3

199

RTX 2080S

4

32

E5-2697v4

101

RTX 2080S

4

32

E5-2697v4

93

RTX 2080S

4

32

E5-2697v4

94

RTX 2080S

4

32

E5-2697v4

94

RTX 2080S

4

32

E5-2697v4

100

RTX 2080Ti

4

16

Ryzen Threadripper 3960x

279

RTX 2080Ti

8

129

E5-2670v3

616

RTX 2080Ti

6

32

E5-2620v3

217

RTX 2080Ti

8

16

E5-2697v2

100

RTX 2080Ti

8

21

E5-2697v2

145

RTX 2080Ti 12

32

Unknown

111

RTX 2080Ti 12

64

E5-2690v3

205

RTX 3080

16

16

i7-10700K

69

RTX 3090

14

32

E5-2695v3

93

RTX 3090

16

32

Ryzen 9 3950X

338

Titan RTX

4

32

Xeon W-3223

321

Titan RTX

4

32

Xeon Gold 6149

99

Titan V

8

32

i7-7700K

97

V100-FHHL

8

60

Xeon Gold 6148

544

255

0.036

36

0.06

308

0.101

475

0.182

210

0.302

539

0.217

n/a

0.15

100

0.2

98

0.2

99

0.2

99

0.2

25

0.199

64

0.285

287

0.25

25

0.302

99

0.292

99

0.292

98

0.292

98

0.292

99

0.292

271

0.35

672

0.201

61

0.22

58

0.3

49

0.243

92

0.326

61

0.549

49

0.462

37

0.498

38

0.511

115

1

100

0.702

50

0.282

584

0.39

Total hourly cost (as listed):

15.43

399

E.8 Additional Averaging Experiments

In this section, we evaluate the averaging precision with the same methodology as in 6.4.1, but for diﬀerent worker conﬁgurations. In Figure E.1, plots 1–5 explore several combinations of grid sizes and failure rates, whereas plot 6 (bottom right) demonstrates a setup with the same number of peers (106) arranged into several diﬀerent grid sizes and its relation to convergence. Note that M =32 outperforms the alternatives only for the speciﬁc failure rate of 0.001.

Mean squared error

10 1 10 3 10 5 10 7 10 9 10 11 10 13 10 15
0
10 1 10 3 10 5 10 7 10 9 10 11 10 13 10 15
0

Grid 8x8

N=50, p=0 N=50, p=0.005

10 1

N=50, p=0.01 N=50, p=0.02

10 3

10 5

10 7

10 9

10 11

10 13

10 15

5

10

15

20

25

0

Grid 32x32x32

N=29 000, p=0 N=29 000, p=0.001

10 1

N=29 000, p=0.002 N=29 000, p=0.004

10 3

10 5

10 7

10 9

10 11

10 13

10 15

5

10

15

20

25

0

Moshpit All-Reduce steps

Grid 8x8x8

N=470, p=0 N=470, p=0.001

10 1

N=470, p=0.0025 N=470, p=0.0075

10 3

N=470, p=0.01

10 5

10 7

10 9

10 11

10 13

10 15

5 10 15 20 25 30

0

Grid 256x256

N=29 000, p=0 N=29 000, p=0.001

10 1

N=29 000, p=0.002 N=29 000, p=0.004

10 3

10 5

10 7

10 9

10 11

10 13

10 15

5

10

15

20

25

0

Moshpit All-Reduce steps

Grid 8x8x8x8
N=3700, p=0 N=3700, p=0.001 N=3700, p=0.005 N=3700, p=0.01
5 10 15 20 25 30
Varying grid size
Grid 1024^2, N=10^6, p=0.001 Grid 32^4, N=10^6, p=0.001 Grid 4^10, N=10^6, p=0.001

20

40

60

80

Moshpit All-Reduce steps

Mean squared error

Figure E.1: Averaging error of Moshpit All-Reduce as a function of the iteration number for diﬀerent conﬁgurations and failure rates.

E.9 Additional Image Classiﬁcation Experiments
Aside from the two evaluation scenarios provided in 6.4.2, we also measure the performance of Moshpit-SGD in a non-distributed setup, i.e. on a single server with multiple GPUs. We conduct this experiment on the same 8× V100 machine that was used in the homogeneous setup for training ALBERT (see Appendix E.7.2).
As Figure E.2 demonstrates, Moshpit SGD is slower than AR-SGD by approximately 25%. This result is expected, since our implementation of Moshpit All-Reduce is more general and communicates over a TCP connection, whereas AR-SGD uses direct peer-to-peer GPU communication over PCIe. On average, this incurs a slowdown of 27% in terms of training time.

400

Top-1 validation accuracy

42.8

53.2

75%

50%

25%

AR-SGD, local

0%

Moshpit SGD, local

0h 8h 16h 24h 32h 40h 48h 56h 64h Training time (hours)

75%

50%

25%

AR-SGD, local Moshpit SGD, local 0%

0

15

30

45

60

75

90

Training epochs

Figure E.2: ResNet-50 top-1 validation accuracy on ImageNet when training on a single node with 8× V100-PCIe GPUs. (Left) Convergence in terms of training time, (Right) Convergence in terms of training epochs

401

