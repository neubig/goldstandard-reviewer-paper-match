arXiv:2010.03680v2 [cs.CL] 11 Dec 2020

ADAPTIVE SELF-TRAINING FOR FEW-SHOT NEURAL SEQUENCE LABELING
Yaqing Wang†, Subhabrata Mukherjee∗, Haoda Chu , Yuancheng Tu , Ming Wu , Jing Gao†, Ahmed Hassan Awadallah∗
†SUNY Buffalo, ∗Microsoft Research AI, Microsoft AI {yaqingwa, jing}@buffalo.edu,
{Subhabrata.Mukherjee, haochu, yuantu, mingwu, hassanam}@microsoft.com
ABSTRACT
sequence labeling is an important technique employed for many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), slot tagging for dialog systems and semantic parsing. Large-scale pre-trained language models obtain very good performance on these tasks when ﬁne-tuned on large amounts of task-speciﬁc labeled data. However, such large-scale labeled datasets are difﬁcult to obtain for several tasks and domains due to the high cost of human annotation as well as privacy and data access constraints for sensitive user applications. This is exacerbated for sequence labeling tasks requiring such annotations at tokenlevel. In this work, we develop techniques to address the label scarcity challenge for neural sequence labeling models. Speciﬁcally, we develop self-training and meta-learning techniques for training neural sequence taggers with few labels. While self-training serves as an effective mechanism to learn from large amounts of unlabeled data – meta-learning helps in adaptive sample re-weighting to mitigate error propagation from noisy pseudo-labels. Extensive experiments on six benchmark datasets including two for massive multilingual NER and four slot tagging datasets for task-oriented dialog systems demonstrate the effectiveness of our method. With only 10 labeled examples for each class for each task, our method obtains 10% improvement over state-of-the-art systems demonstrating its effectiveness for the low-resource setting.
1 INTRODUCTION
Motivation. Deep neural networks typically require large amounts of training data to achieve stateof-the-art performance. Recent advances with pre-trained language models like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and RoBERTa (Liu et al., 2019) have reduced this annotation bottleneck. In this paradigm, large neural network models are trained on massive amounts of unlabeled data in a self-supervised manner. However, the success of these large-scale models still relies on ﬁne-tuning them on large amounts of labeled data for downstream tasks. For instance, our experiments show 27% relative improvement on an average when ﬁne-tuning BERT with the full training set (2.5K-705K labels) vs. ﬁne-tuning with only 10 labels per class. This poses several challenges for many real-world tasks. Not only is acquiring large amounts of labeled data for every task expensive and time consuming, but also not feasible in many cases due to data access and privacy constraints. This issue is exacerbated for sequence labeling tasks that require annotations at token- and slot-level as opposed to instance-level classiﬁcation tasks. For example, an NER task can have slots like B-PER, I-PER, O-PER marking the beginning, intermediate and out-of-span markers for person names, and similar slots for the names of location and organization. Similarly, language understanding models for dialog systems rely on effective identiﬁcation of what the user intends to do (intents) and the corresponding values as arguments (slots) for use by downstream applications. Therefore, fully supervised neural sequence taggers are expensive to train for such tasks, given the requirement of thousands of annotations for hundreds of slots for the many different intents.
Semi-supervised learning (SSL) (Chapelle et al., 2010) is one of the promising paradigms to address labeled data scarcity by making effective use of large amounts of unlabeled data in addition to task-speciﬁc labeled data. Self-training (ST, (III, 1965)) as one of the earliest SSL approaches
1

Unlabeled data

(1) Fine-tune

(3) Labeled data adaptive acquisition

Few-shot Labeled data
Teacher

(2) Assign Pseudo labels
Pseudo-labeled data

Labeled mini-batch data

(4) Re-weighting
Reweighted Pseudo-labeled
data

Student
(5) Train

Assign student as a new teacher

Student training iterations

Figure 1: MetaST framework.

has recently shown state-of-the-art performance for tasks like image classiﬁcation (Li et al., 2019; Xie et al., 2020) performing at par with supervised systems while using very few training labels. In contrast to such instance-level classiﬁcation tasks, sequence labeling tasks have dependencies between the slots demanding different design choices for slot-level loss optimization for the limited labeled data setting. For instance, prior work (Ruder & Plank, 2018) using classic self-training techniques for sequence labeling did not ﬁnd much success in the low-data regime with 10% labeled data for the target domain. Although there has been some success with careful task-speciﬁc data selection (Petrov & McDonald, 2012) and more recently for distant supervision (Liang et al., 2020) using external resources like knowledge bases (e.g., Wikipedia). In contrast to these prior work, we develop techniques for self-training with limited labels and without any task-speciﬁc assumption or external knowledge.
For self-training, a base model (teacher) is trained on some amount of labeled data and used to pseudo-annotate (task-speciﬁc) unlabeled data. The original labeled data is augmented with the pseudo-labeled data and used to train a student model. The student-teacher training is repeated until convergence. Traditionally in self-training frameworks, the teacher model pseudo-annotates unlabeled data without any sample selection. This may result in gradual drifts from self-training on noisy pseudo-labeled instances (Zhang et al., 2017). In order to deal with noisy labels and training set biases, Ren et al. (2018) propose a meta-learning technique to automatically re-weight noisy samples by their loss changes on a held-out clean labeled validation set. We adopt a similar principle in our work and leverage meta-learning to re-weight noisy pseudo-labeled examples from the teacher. While prior techniques for learning to re-weight examples have been developed for instance-level classiﬁcation tasks, we extend them to operate at token-level for discrete sequence labeling tasks. To this end, we address some key challenges on how to construct an informative held-out validation set for token-level re-weighting. Prior works (Ren et al., 2018; Shu et al., 2019) for instance classiﬁcation construct this validation set by random sampling. However, sequence labeling tasks involve many slots (e.g. WikiAnn has 123 slots over 41 languages) with variable difﬁculty and distribution in the data. In case of random sampling, the model oversamples from the most populous category and slots. This is particularly detrimental for low-resource languages in the multilingual setting. To this end, we develop an adaptive mechanism to create the validation set on the ﬂy considering the diversity and uncertainty of the model for different slot types. Furthermore, we leverage this validation set for token-level loss estimation and re-weighting pseudo-labeled sequences from the teacher in the meta-learning setup. While prior works (Li et al., 2019; Sun et al., 2019; Bansal et al., 2020) on meta-learning for image and text classiﬁcation leverage multi-task learning to improve a target classiﬁcation task based on several similar tasks, in this work we focus on a single sequence labeling task – making our setup more challenging altogether.
Our task and framework overview. We focus on sequence labeling tasks with only a few annotated samples (e.g., K = {5, 10, 20, 100}) per slot type for training and large amounts of task-speciﬁc unlabeled data. Figure 1 shows an overview of our framework with the following components: (i) Self-training: Our self-training framework leverages a pre-trained language model as a teacher and co-trains a student model with iterative knowledge exchange (ii) Adaptive labeled data acquisition for validation: Our few-shot learning setup assumes a small number of labeled training samples per slot type. The labeled data from multiple slot types are not equally informative for the student

2

model to learn from. While prior works in meta-learning randomly sample some labeled examples for held-out validation set, we develop an adaptive mechanism to create this set on the ﬂy. To this end, we leverage loss decay as a proxy for model uncertainty to select informative labeled samples for the student model to learn from in conjunction with the re-weighting mechanism in the next step. (iii) Meta-learning for sample re-weighting: Since pseudo-labeled samples from the teacher can be noisy, we employ meta-learning to re-weight them to improve the student model performance on the held-out validation set obtained from the previous step. In contrast to prior work (Ren et al., 2018) on sample re-weighting operating at instance-level, we incorporate the re-weighting mechanism at token-level for sequence labeling tasks. Here the token-level weights are determined by the student model loss on the above validation set. Finally, we learn all of the above steps jointly with end-toend learning in the self-training framework. We refer to our adaptive self-training framework with meta-learning based sample re-weighting mechanism as MetaST.
We perform extensive experiments on six benchmark datasets for several tasks including multilingual Named Entity Recognition and slot tagging for user utterances from task-oriented dialog systems to demonstrate the generalizability of our approach across diverse tasks and languages. We adopt BERT and multilingual BERT as encoder and show that its performance can be signiﬁcantly improved by nearly 10% for low-resource settings with few training labels (e.g., 10 labeled examples per slot type) and large amounts of unlabeled data. In summary, our work makes the following contributions. (i) Develops a self-training framework for neural sequence tagging with few labeled training examples. (ii) Leverages an acquisition strategy to adaptively select a validation set from the labeled set for meta-learning of the student model. (iii) Develops a meta-learning framework for re-weighting pseudo-labeled samples at token-level to reduce drifts from noisy teacher predictions. (iv) Integrates the aforementioned components into an end-to-end learning framework and demonstrates its effectiveness for neural sequence labeling across six benchmark datasets with multiple slots, shots, domains and languages.

2 BACKGROUND

Sequence labeling and slot tagging. This is the task identifying the entity span of several slot types (e.g., names of person, organization, location, date, etc.) in a text sequence. Formally, given a sentence with N tokens X = {x1, ..., xN }, an entity or slot value is a span of tokens s = [xi, ..., xj](0 ≤ i ≤ j ≤ N ) associated with a type. This task assumes a pre-deﬁned tagging policy like BIO (Tjong et al., 1999), where B marks the beginning of the slot, I marks an intermediate token in the span, and O marks out-of-span tokens. These span markers are used to extract multi-token values for each of the slot types with phrase-level evaluation for the performance.

Self-training. Consider f (·; θtea) and f (·; θstu) to denote the teacher and student models respec-

tively in the self-training framework. The role of the teacher model (e.g., a pre-trained language

model) is to assign pseudo-labels to unlabeled data that is used to train a student model. The

teacher and student model can exchange knowledge and the training schedules are repeated till

convergence. The success of self-training with deep neural networks in recent works (He et al.,

2019; Xie et al., 2020) has been attributed to a number of factors including stochastic regularization

with dropouts and data regularization with unlabeled data. Formally, given m-th unlabeled sen-

tence

with

N

tokens

Xmu

=

{

x

u 1,m

,

...,

x

u N

,m

}

and

C

pre-deﬁned

labels,

consider

the

pseudo-labels

Yˆm(t)

=

[yˆm(t),1,

...,

yˆ(t) ]
m,N

generated

by

the

teacher

model

at

the

t-th

iteration

where,

yˆm(t),n = arg max fn,c(xum,n; θt(et)a).

(1)

c∈C

The pseudo-labeled data set, denoted as (Xu, Yˆ (t)) = {(Xmu , Yˆm(t))}M m , is used to train the student model and learn its parameters as:

θˆ(t) = arg min 1

M
l(Yˆ (t), f (Xu ; θ(t−1))),

(2)

stu

θM

m

m stu

m=1

where l(·, ·) can be modeled as the cross-entropy loss.

3 ADAPTIVE SELF TRAINING
Given a pre-trained language model (e.g., BERT (Devlin et al., 2019)) as the teacher, we ﬁrst ﬁnetune it on the small labeled data to make it aware of the underlying task. The ﬁne-tuned teacher

3

model is now used to pseudo-label the large unlabeled data. We consider the student model as another instantiation of the pre-trained language model that is trained over the pseudo-labeled data. However, our few-shot setting with limited labeled data results in a noisy teacher. A naive transfer of teacher knowledge to the student results in the propagation of noisy labels limiting the performance of the student model. To address this challenge, we develop an adaptive self-training framework to re-weight pseudo-labeled predictions from the teacher with a meta-learning objective that optimizes the token-level loss from the student model on a held-out labeled validation set. This held-out set is adaptively constructed via labeled data acquisition which selects labeled samples with high uncertainty for efﬁcient data exploration.

3.1 ADAPTIVE LABELED DATA ACQUISITION
In standard meta-learning setup for instance-level classiﬁcation tasks, the held-out validation set is usually constructed via random sampling (Ren et al., 2018; Shu et al., 2019). Sequence labeling tasks involve many slot types with variable difﬁculty and distribution in the data. For instance, NER tasks over WikiAnn operate over 123 slot types from 41 languages with additional complexity from variable model performance across different languages. A random sampling leads to oversampling instances with the most populous categories and slot types in the data. Therefore, we propose a novel labeled data acquisition strategy to construct the validation set for effective data exploration. We demonstrate its beneﬁt over classic meta-learning approaches from prior works in experiments.

In general, data acquisition strategies for prior works in meta-learning and active learning broadly leverage random sampling (Ren et al., 2018; Shu et al., 2019), easy (Kumar et al., 2010) and hard example mining (Shrivastava et al., 2016) or uncertainty-based methods (Chang et al., 2017a). These strategies have been compared in prior works (Chang et al., 2017a; Gal et al., 2017) that show uncertainty-based methods to have better generalizability across diverse settings. There are several approaches to uncertainty estimation including error decay (Konyushkova et al., 2017; Chang et al., 2020), Monte Carlo dropouts (Gal et al., 2017) and predictive variance (Chang et al., 2017a). We follow a similar principle of error decay to ﬁnd samples that the model is uncertain about and can correspondingly beneﬁt from knowing their labels (similar to active learning settings). To this end, we leverage stochastic loss decay from the model as a proxy for the model uncertainty to generate validation set on the ﬂy. This is used for estimating token-level weights and re-weighting pseudo labeled data in Section 3.2.

Consider the loss of the student model with parameters θs(tt)u on the labeled data (Xml , Ym) in the

t-th

iteration

as

l

(Ym

,

f

(Xml

;

θ

(t) stu

)).

To

measure

the

loss

decay

value

at

any

iteration,

we

use

the

difference between the current and previous loss values. Considering these values may ﬂuctuate

across iterations, we adopt the moving average of the loss values for (Xml , Ym) in the latest R iterations as a baseline lbm for loss decay estimation. Baseline measure lbm is calculated as follows:

m 1R

l (t−r)

lb

= R

l(Ym, f (Xm; θstu )).

(3)

r=1

Since the loss decay values are estimated on the ﬂy, we want to balance exploration and exploitation.
To this end, we add a smoothness factor δ to prevent the low loss decay samples (i.e. samples with
low uncertainty) from never being selected again. Considering all of the above factors, we obtain the sampling weight of labeled data (Xml , Yml ) as follows:

Wm ∝ max(lbm − l(Ym, f (Xml ; θs(tt)u)), 0) + δ.

(4)

The smoothness factor δ needs to be adaptive since the training loss is dynamic. Therefore, We adopt the maximum of the loss decay value as the smoothness factor δ to encourage exploration.

The aforementioned acquisition function is re-estimated after a ﬁxed number of steps to adapt to
model changes. With labeled data acquisition, we rely on informative uncertain samples to improve learning efﬁciency. The sampled mini-batches of labeled data {Bsl } are used as a validation set for the student model in the next step for re-weighting pseudo-labeled data from the teacher model. We
demonstrate its impact via ablation study in experiments. Note that the labeled data is only used to
compute the acquisition function and not used for explicit training of the student model in this step.

3.2 RE-WEIGHTING PSEUDO-LABELED DATA
To mitigate error propagation from noisy pseudo-labeled sequences from the teacher, we leverage meta-learning to adaptively re-weight them based on the student model loss on the held-out valida-

4

tion set obtained via labeled data acquisition from the previous section. In contrast to prior work focusing on instance-level tasks like image classiﬁcation – sequence labeling operates on discrete text sequences as input and assigns labels to each token in the sequence. Since teacher predictions vary for different slot labels and types, we adapt the meta-learning framework to re-weight samples at a token-level resolution.

Token

Re-weighting.

Consider

the

pseudo-labels

{Yˆm(t)

=

[yˆm(t),1

,

...,

yˆ(t)
m,N

]}M m=1

from

the

teacher

in the t-th iteration with m and n indexing the instance and a token in the instance, respectively. In

classic self-training, we update the student parameters leveraging pseudo-labels as follows:

θˆ(t) = θˆ(t−1) − α

1

M
l(Yˆ (t), f (Xu ; θ(t−1))) .

(5)

stu

stu

M

m

m stu

m=1

Now, to downplay noisy token-level labels, we leverage meta-learning to re-weight the pseudolabeled data. To this end, we follow a similar analysis from (Koh & Liang, 2017) and (Ren et al., 2018) to perturb the weight for each token in the mini-batch by . Weight perturbation is used to discover data points that are most important to improve the model performance on a held-out validation set (Koh & Liang, 2017) where the sample importance is given by the magnitude of the the negative gradients. We extend prior techniques to obtain token-level perturbations as:

θˆ(t) ( ) = θˆ(t−1) − α

11 M N [

· l(yˆ(t) , f (xu ; θˆ(t−1)))] .

(6)

stu

stu

MN

m,n

m,n

m,n stu

m=1 n=1

The token weights are obtained by minimizing the student model loss on the held-out validation set.
Here, we employ the labeled data acquisition strategy from Eq. 4 to sample informative mini-batches of labeled data Bsl locally at step t. To obtain a cheap estimate of the meta-weight at step t, we take a single gradient descent step for the sampled labeled mini-batch Bsl :

|Bsl | N

∂ um,n,s = −

11

[l(ym,n, f (xl ; θˆ(t) ( ))] |

=0

(7)

∂ m,n,s |Bsl | N m=1 n=1

m,n stu

m,n,s

We set the token weights to be proportional to the negative gradients to reﬂect the importance of
pseudo-labeled tokens in the sequence. Since sequence labeling tasks have dependencies between
the slot types and tokens, it is difﬁcult to obtain a good estimation of the weights based on a single mini-batch of examples. Therefore, we sample S mini-batches of labeled data {B1l , ..., BSl } with the adaptive acquisition strategy and calculate the mean of the gradients to obtain a robust gradient estimate. Note that S is a constant number that is the same for each token and the proportional sign
in Eq. 8. Since a negative weight indicates a pseudo-label of poor quality that would potentially degrade the model performance, we set such weights to 0 to ﬁlter them out. The impact of S is
investigated in the experiments (refer to Appendix A.1). The overall meta-weight of pseudo-labeled token (xum,n, yˆm,n) is obtained as:

S

wm,n ∝ max( um,n,s, 0)

(8)

s=1

To further ensure the stability of the loss function in each mini-batch, we normalise the weight wm,n. Finally, we update the student model parameters while accounting for token-level re-weighting as:

θˆ(t) = θˆ(t−1) − α

11 M N [w

· l(yˆ(t) , f (xu ; θˆ(t−1)))] .

(9)

stu

stu

MN

m,n

m,n

m,n stu

m=1 n=1

We demonstrate the impact of our re-weighting mechanism with an ablation study in experiments.
3.3 TEACHER MODEL ITERATIVE UPDATES
At the end of every self-training iteration, we assign the student model as a new teacher model (i.e., θtea = θs(Ttu)) . Since the student model uses the labeled data only as a held-out validation set for meta-learning, we further utilize the labeled data (Xl, Y ) to ﬁne-tune the new teacher model

5

f (·, θt(et)a) with standard supervised loss minimization. We explore the effectiveness of this step with an ablation study in experiments. The overall training procedure is summarized in Algorithm 1.

Algorithm 1: MetaST Algorithm.
Input: Labeled sequences (Xl, Y ); Unlabeled sequences (Xu); Pre-trained BERT model with randomly initialized token classiﬁcation layer f (·; θ(0)); Batches S; Number of self-training iterations T .
Initialize teacher model θtea = θ(0) while not converged do
Fine-tune teacher model on small labeled data (Xl, Y ); Initialize the student model θs(0tu) = θ(0); Generate hard pseudo-labels Yˆ (t) for unlabeled samples Xu with model f (·, θtea); for t ← 1 to T do
Compute labeled data acquisition function according to Eq. 4; Sample S mini-batches of labeled examples {B1l , ..., BSl } from (Xl, Y ) based on labeled data acquisition function; Randomly sample a batch of pseudo-labeled examples Bu from (Xu, Yˆ (t)) ; Compute token-level weights in Bu based on the loss on {B1l , ..., BSl } according to Eq. 8; Train model f (·, θs(tt)u) on weighted pseudo-labeled sequences Bu and update parameters θs(tt)u ; end
Update the teacher: θtea = θs(Ttu) end

4 EXPERIMENTS

Encoder. Pre-trained language models like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and RoBERTa (Liu et al., 2019) have shown state-of-the-art performance for various natural language processing tasks. In this work we adopt one of them as a base encoder by initializing the teacher with pre-trained BERT-base model and a randomly initialized token classiﬁcation layer.

Datasets. We perform large-scale experiments with six different datasets including

Dataset

# Slots # Train # Test # Lang

user utterances for task-oriented dialog sys- Email

20

2.5K 1k

EN

tems and multilingual Named Entity Recog- SMNITIPMS ovie

39

13K 0.7K EN

12

8.8K 2.4K EN

nition tasks as summarized in Table 1. (a) MIT Restaurant

8

6.9K 1.5K EN

Email. This consists of natural language Wikiann (EN)

3

user utterances for email-oriented user actions CoNLL03 (EN)

4

20K 10K EN 15K 3.6K EN

like sending, receiving or searching emails with attributes like date, time, topics, peo-

CoNLL03 Wikiann

16

38K 15K 4

123

705K 329K 41

ple, etc. (b) SNIPS is a public benchmark

Table 1: Dataset summary.

dataset (Coucke et al., 2018) of user queries from multiple domains including music, media, and

weather. (c) MIT Movie and Restaurant corpus (Liu et al., 2013) consist of similar user utterances

for movie and restaurant domains. (d) CoNLL03 (Sang & Meulder, 2003) and Wikiann (Pan et al.,

2017) are public benchmark datasets for multilingual Named Entity Recognition. CoNLL03 is a

collection of news wire articles from the Reuters Corpus from 4 languages with manual annotations,

whereas Wikiann comprises of extractions from Wikipedia articles from 41 languages with auto-

matic annotation leveraging meta-data for different entity types like ORG, PER, LOC etc. For every

dataset, we sample K ∈ {5, 10, 20, 100} labeled sequences for each slot type from the Train data,

and add the remaining to the unlabeled set while ignoring their labels – following standard setups

for semi-supervised learning. We repeatedly sample K labeled instances three times for multiple

runs to report average performance with standard deviation across the runs.

Baselines. The ﬁrst baseline we consider is the fully supervised BERT model trained on all available training data which provides the ceiling performance for every task. Each of the other models are trained on K training labels per slot type. We adopt several state-of-the-art semi-supervised methods as baselines: (1) CVT (Clark et al., 2018) is a semi-supervised sequence labeling method based on cross-view training; (2) SeqVAT (Chen et al., 2020) incorporates adversarial training with conditional random ﬁeld layer for semi-supervised sequence labeling; (3) Mean Teacher (MT) (Tarvainen & Valpola, 2017) averages model weights to obtain an aggregated teacher; (4) VAT (Miyato et al., 2018) adopts virtual adversarial training to make the model robust to noise; (5) classic ST (III, 1965) is simple self-training method with hard pseudo-labels; (6) BOND (Liang et al., 2020) is the most recent work on self-training for sequence labeling with conﬁdence-based sample selection and forms a strong baseline for our work. We implement our framework in Pytorch and use Tesla V100 gpus for experiments. Hyper-parameter conﬁgurations with model settings presented in Appendix.

6

Neural sequence labeling performance with few training labels. Table 2 shows the performance comparison among different models with K=10 labeled examples per slot type. The fully supervised BERT trained on thousands of labeled examples provides the ceiling performance for the few-shot setting. We observe our method MetaST to signiﬁcantly outperform all methods across all datasets including the models that also use the same BERT encoder as ours like MT, VAT, Classic ST and BOND with corresponding average performance improvements as 14.22%, 14.90%, 8.46% and 8.82%. Non BERT models like CVT and SeqVAT are consistently worse than other baselines.

Method

SNIPS

Email

Movie

# Slots

39

20

12

Full-supervision

BERT

95.80

94.44

87.87

Few-shot supervision (10 labels per slot)

BERT

79.01

87.85

69.50

Few-shot supervision (10 labels per slot) + unlabeled data

CVT

78.23

78.24

62.73

SeqVAT

78.67

72.65

67.10

MT

79.48

89.53

67.62

VAT

79.08

89.71

70.17

Classic ST

83.26

90.70

71.88

BOND

83.54

89.75

70.91

MetaST

88.23

92.18

77.67

(0.04;↑12%) (0.47;↑4.93%) (0.10;↑11.76%)

Restaurant 8
78.95
54.06
42.57 51.55 51.75 53.34 56.80 55.78 63.83 (1.62;↑18.07%)

CoNLL03 (EN) 4
92.40
71.15
54.31 67.21 68.67 65.03 70.99 69.56 76.65 (0.73;↑7.73%)

Wikiann (EN) 3
84.04
45.61
27.89 35.16 41.43 38.81 46.15 48.73 56.61 (0.4;↑24.12%)

Table 2: F1 score comparison of models for sequence labeling on different datasets. All models (except CVT and SeqVAT) use the same BERT encoder. F1 score of our model for each task is followed by standard deviation and percentage improvement (↑) over BERT with few-shot supervision.

We also observe variable performance of the models across different tasks. Speciﬁcally, the performance gap between the best few-shot model and the fully supervised model varies signiﬁcantly. MetaST achieves close performance to the fully-supervised model in some datasets (e.g. SNIPS and Email) but has bigger room for improvement in others (e.g. CoNLL03 (EN) and Wikiann (EN)). This can be attributed to the following factors. (i) Labeled training examples and slots. The total number of labeled training instances for our K-shot setting is given by K × #Slots. Therefore, for tasks with higher number of slots and consequently more training labels, most of the models perform better including MetaST. Task-oriented dialog systems with more slots and inherent dependency between the slot types beneﬁt more than NER tasks. (ii) Task difﬁculty: User utterances from task-oriented dialog systems for some of the domains like weather, music and emails contain predictive query patterns and limited diversity. In contrast, Named Entity Recognition datasets are comparatively diverse and require more training labels to generalize well. Similar observations are also depicted in Table 3 for multilingual NER tasks with more slots and consequently more training labels from multiple languages as well as richer interactions across the slots from different languages.

Dataset
CoNLL03 Wikiann

#Lang
4 41

#Slots
16 123

Full Sup. BERT
87.67 87.17

Few-shot Sup. BERT
70.77 79.67

MT
68.34 80.23

Few-shot supervision + unlabeled data

VAT Classic ST BOND

MetaST

67.63 78.82

72.69 80.24

72.79 76.41 (0.47) (↑ 7.97%) 79.57 81.61 (0.14) (↑ 2.42%)

Table 3: F1 score comparison of models for sequence labeling on multilingual datasets using the same BERT-Multilingual-Base encoder. F1 score of MetaST for each task is followed by standard deviation in parentheses and percentage improvement (↑) over BERT with few-shot supervision.
Controlling for the total amount of labeled data. In order to control for the variable amount of training labels across different datasets, we perform another experiment where we vary the number of labels for different slot types while keeping the total number of labeled instances for each dataset similar (ca. 200). Results are shown in Table 4. To better illustrate the effect of the number of training labels, we choose tasks with lower performance in Table 2 for this experiment. Comparing the results in Tables 2 and 4, we observe the performance of MetaST to improve with more training labels for all the tasks .
Effect of varying the number of labels K per slot. Table 5 shows the improvement in the performance of MetaST when increasing the number of labels for each slot type in the SNIPS dataset.

7

Dataset
MIT Movie MIT Restaurant CoNLL03 (EN) Wikiann (EN)
Average

BERT (Full Supervision)
87.87 78.95 92.40 84.04
85.82

BERT (Few-shot Supervision)
75.81 60.12 77.48 62.04
68.86

MetaST ( %Improvement )
80.33 (↑ 5.96%) 67.86 (↑ 12.87%) 81.61 (↑ 5.33%) 71.27 (↑ 14.88%)
75.27 (↑ 9.31%)

Table 4: F1 scores of different models with 200 labeled samples for each task. The percentage improvement (↑) is over the BERT model with few-shot supervision.

Similar trends can be found on other datasets (results in Appendix). As we increase the amount of labeled training instances, the performance of BERT also improves, and correspondingly the margin between MetaST and these baselines decreases although MetaST still improves over all of them. In the self-training framework, given the ceiling performance for every task and the improved performance of the teacher with more training labels, there is less room for (relative) improvement of the student over the teacher model. Consider SNIPS for example. Our model obtains 12% and 2% improvement over the few-shot BERT model for the 10-shot and 100-shot setting with F1-scores as 88.22% and 95.39%, respectively. The ceiling performance for this task is 95.8% on training BERT on the entire dataset with 13K labeled examples. This demonstrates that MetaST is most impactful for low-resource settings with few training labels for a given task.

#Slots
5 10 20 100

Few-shot Supervision
BERT
70.63 79.01 86.81 93.90

CVT
69.82 78.23 88.04 94.61

SeqVAT
69.34 78.67 85.05 91.46

Few-shot supervision + unlabeled data MT VAT Classic ST BOND MetaST (%Improvement)

70.85 79.48 87.31 94.26

71.34 79.08 88.19 94.53

72.59 83.26 88.32 93.92

72.85 83.54 88.93 94.22

81.56 (↑15%) 88.22 (↑12%) 91.99 (↑6%) 95.39 (↑2%)

Table 5: Variation in model performance on varying K labels / slot on SNIPS dataset with 39 slots. The percentage improvement (↑) is relative to the BERT model with few-shot supervision.

Ablation analysis. Table 6 demonstrates the impact of different MetaST components with ablation analysis. We observe that soft pseudo-labels hurt the model performance compared to hard pseudolabels, as also shown in recent work (Kumar et al., 2020). Such a performance drop may be attributed to soft labels being less informative compared to sharpened ones. Removing the iterative teacher ﬁne-tuning step (Section 3.1) also hurts the overall performance.

Method

Datasets SNIPS CoNLL03

BERT w/ Continued Pre-training + Few-shot Supervision

83.96 69.84

Classic ST Classic ST w/ Soft Pseudo-Labels

83.26 81.17

70.99 71.87

MetaST (ours) w/ Hard Pseudo-Labels MetaST w/ Soft Pseudo-Labels

88.23 86.16

76.65 75.84

MetaST w/o Iterative Teacher Fine-tune 85.64

MetaST w/o Labeled Data Acq.

86.63

72.74 75.02

Pseudo-labeled Data Re-weighting

MetaST w/o Re-weighting

85.48 73.02

MetaST (Easy)

85.56 74.53

MetaST (Difﬁcult)

86.34 68.06

Figure 2: Visualization of MetaST re-

weighting on CoNLL03 (EN). Table 6: Ablation analysis of our framework MetaST with

10 labeled examples per slot on SNIPS and CoNLL03 (EN).

Continued pre-training v.s. self-training. To contrast continued pre-training with self-training, we further pre-train BERT on in-domain unlabeled data and then ﬁne-tune it with few labeled examples denoted as “BERT (Continued Pre-training + Few-shot Supervision)”. The pre-training step improves the BERT performance over the baseline on SNIPS but degrades the performance on CoNLL03. This indicates that continued pre-training can improve the performance of few-shot supervised BERT on specialized tasks (e.g., SNIPS) with different data distribution than the original pre-training data (e.g., Wikipedia), but may not help for general domain ones like CoNLL03 with overlapping data from Wikipedia. In contrast to the above baseline, MetaST brings signiﬁcant im-

8

provements on both datasets. This demonstrates the generality and ﬂexibility of self-training over pre-training as also observed in contemporary work (Zoph et al., 2020) on image classiﬁcation. Adaptive labeled data acquisition. We perform an ablation study by removing adaptive labeled data acquisition from MetaST (denoted as “MetaST w/o Labeled Data Acq.”). Removing this component leads to around 2% performance drop on an average demonstrating the impact of labeled data acquisition. Moreover, the performance drop on SNIPS (39 slots) is larger than that on CoNLL03 (4 slots). This demonstrates that adaptive acquisition is more helpful for tasks with more slot types – where diversity and data distribution necessitate a better exploration strategy in contrast to random sampling employed in prior meta-learning works. Re-weighting strategies. To explore the role of token-level re-weighting for pseudo-labeled sequences (discussed in Section 3.2), we replace our meta-learning component with different sample selection strategies based on the model conﬁdence for different tokens. One sampling strategy chooses samples uniformly without any re-weighting (referred to as “MetaST w/o Re-weighting”). The sampling strategy with weights proportional to the model conﬁdence favors easy samples (referred to as “MetaST-Easy”), whereas the converse favors difﬁcult ones (referred to as “MetaSTDifﬁcult”).We observe the meta-learning based re-weighting strategy to perform the best. Interestingly, MetaST-Easy outperforms MetaST-Difﬁcult signiﬁcantly on CoNLL03 (EN) but achieves slightly lower performance on SNIPS. This demonstrates that difﬁcult samples are more helpful when the quality of pseudo-labeled data is relatively high. On the converse, the sample selection strategy focusing on difﬁcult samples introduces noisy examples with lower pseudo-label quality. Therefore, sampling strategies may need to vary for different datasets, thereby, demonstrating the necessity of adaptive data re-weighting as in our framework MetaST. Moreover, MetaST signiﬁcantly outperforms classic self-training strategies with hard and soft pseudo-labels demonstrating the effectiveness of our design.
Analysis of pseudo-labeled data re-weighting. To visually explore the adaptive re-weighting mechanism, we illustrate token-level re-weighting of MetaST on CoNLL03 (EN) dataset with K=10 shot at step 100 in Fig. 2. We include the re-weighting visualisation on SNIPS in Appendix A.1. We observe that the selection mechanism ﬁlters out most of the noisy pseudo-labels (colored in blue) even those with high teacher conﬁdence as shown in Fig. 2.
5 RELATED WORK
Semi-supervised learning has been widely used for consistency training (Bachman et al., 2014; Rasmus et al., 2015; Laine & Aila, 2017; Tarvainen & Valpola, 2017; Miyato et al., 2018), latent variable models (Kingma et al., 2014) for sentence compression (Miao & Blunsom, 2016) and code generation (Yin et al., 2018). More recently, methods like UDA (Xie et al., 2019) leverage consistency training for few-shot learning of instance-classiﬁcation tasks leveraging auxiliary resources like paraphrasing and back-translation (BT) (Sennrich et al., 2016).
Sample selection. Curriculum learning (Bengio et al., 2009) techniques are based on the idea of learning easier aspects of the task ﬁrst followed by the more complex ones. Prior work leveraging self-paced learning (Kumar et al., 2010) and more recently self-paced co-training (Ma et al., 2017) leverage teacher conﬁdence to select easy samples during training. Sample selection for image classiﬁcation tasks have been explored in recent works with meta-learning (Ren et al., 2018; Li et al., 2019) and active learning (Panagiota Mastoropoulou, 2019; Chang et al., 2017b). However, all of these techniques rely on only the model outputs applied to instance-level classiﬁcation tasks.
Semi-supervised sequence labeling. Miller et al. (2004); Peters et al. (2017) leverage large amounts of unlabeled data to improve token representation for sequence labeling tasks. Another line of research introduces latent variable modeling (Chen et al., 2019; Zhou & Neubig, 2017) to learn interpretable and structured latent representations. Recently, adversarial training based model SeqVAT (Chen et al., 2020) and cross-view training method CVT (Clark et al., 2018) have shown promising results for sequence labeling tasks.
6 CONCLUSIONS
In this work, we develop an adaptive self-training framework MetaST that leverages self-training and meta-learning for few-shot training of neural sequence taggers. We address the issue of error propagation from noisy pseudo-labels from the teacher in the self-training framework by adaptive sample selection and re-weighting with meta-learning. Extensive experiments on six benchmark datasets
9

and different tasks including multilingual NER and slot tagging for task-oriented dialog systems demonstrate the effectiveness of the proposed method particularly for low-resource settings.
REFERENCES
Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 3365–3373, 2014.
Trapit Bansal, Rishikesh Jha, and Andrew McCallum. Learning to few-shot learn across diverse natural language classiﬁcation tasks, 2020.
Yoshua Bengio, Je´roˆme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Andrea Pohoreckyj Danyluk, Le´on Bottou, and Michael L. Littman (eds.), Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pp. 41–48. ACM, 2009. doi: 10.1145/1553374.1553380.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 1002–1012. Curran Associates, Inc., 2017a.
Haw-Shiuan Chang, Erik G. Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 1002–1012, 2017b.
Haw-Shiuan Chang, Shankar Vembu, Sunil Mohan, Rheeya Uppaal, and Andrew McCallum. Using error decay prediction to overcome practical issues of deep active learning for named entity recognition. Machine Learning, 109(9):1749–1778, 2020.
Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-supervised learning. 2010.
Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua Lu. Seqvat: Virtual adversarial training for semi-supervised sequence labeling. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8801–8811, 2020.
Mingda Chen, Qingming Tang, Karen Livescu, and Kevin Gimpel. Variational sequential labelers for semi-supervised learning. arXiv preprint arXiv:1906.09535, 2019.
Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised sequence modeling with cross-view training. arXiv preprint arXiv:1809.08370, 2018.
Alice Coucke, Alaa Saade, Adrien Ball, The´odore Bluche, Alexandre Caulier, David Leroy, Cle´ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Mae¨l Primet, and Joseph Dureau. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. In Privacy in Machine Learning and Artiﬁcial Intelligence workshop, ICML2018, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.
10

Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1183–1192. PMLR, 2017.
Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. Revisiting self-training for neural sequence generation, 2019.
H. J. Scudder III. Probability of error of some adaptive pattern-recognition machines. IEEE Trans. Inf. Theory, 11(3):363–371, 1965. doi: 10.1109/TIT.1965.1053799.
Diederik P. Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semisupervised learning with deep generative models. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 3581–3589, 2014.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. arXiv preprint arXiv:1703.04730, 2017.
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In Advances in Neural Information Processing Systems, pp. 4225–4235, 2017.
Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. arXiv preprint arXiv:2002.11361, 2020.
M. P. Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 1189–1197. Curran Associates, Inc., 2010.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems 32, pp. 10276–10286. Curran Associates, Inc., 2019.
Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. Bond: Bert-assisted open-domain named entity recognition with distant supervision. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1054–1064, 2020.
J. Liu, Panupong Pasupat, D. Cyphers, and James R. Glass. Asgard: A portable architecture for multilingual dialogue systems. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 8386–8390, 2013.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.
Fan Ma, Deyu Meng, Qi Xie, Zina Li, and Xuanyi Dong. Self-paced co-training. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2275–2284, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
Yishu Miao and Phil Blunsom. Language as a latent variable: Discrete generative models for sentence compression. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 319–328. The Association for Computational Linguistics, 2016. doi: 10.18653/v1/d16-1031.
11

Scott Miller, Jethran Guinness, and Alex Zamanian. Name tagging with word clusters and discriminative training. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pp. 337– 342, 2004.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1946–1958, Vancouver, Canada, July 2017. Association for Computational Linguistics.
Emmeleia Panagiota Mastoropoulou. Enhancing deep active learning using selective self-training for image classiﬁcation. Master’s thesis, KTH, School of Electrical Engineering and Computer Science (EECS), 2019.
Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108, 2017.
Slav Petrov and Ryan McDonald. Overview of the 2012 shared task on parsing the web. 2012.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 3546–3554, 2015.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning, pp. 4334–4343, 2018.
Sebastian Ruder and Barbara Plank. Strong baselines for neural semi-supervised learning under domain shift. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1044–1054, 2018.
Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, 2003.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-1009.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 761–769, 2016.
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weightnet: Learning an explicit mapping for sample weighting. In Advances in Neural Information Processing Systems, pp. 1919–1930, 2019.
Q. Sun, Y. Liu, T. Chua, and B. Schiele. Meta-transfer learning for few-shot learning. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 403–412, 2019.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017.
12

Erik F Tjong, Kim Sang, and Jorn Veenstra. Representing text chunks. In Ninth Conference of the European Chapter of the Association for Computational Linguistics, 1999.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training, 2019.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. Structvae: Tree-structured latent variable models for semi-supervised semantic parsing. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 754–765. Association for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1070.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Chunting Zhou and Graham Neubig. Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction. arXiv preprint arXiv:1704.01691, 2017.
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. Advances in Neural Information Processing Systems, 33, 2020.
13

A APPENDIX
A.1 EXPLORATIONS ON UNLABELED DATA AND MINI-BATCH S
Variation in model performance with unlabeled data. Table 12 shows the improvement in model performance as we inject more unlabeled data with diminishing returns after a certain point.
Variation in model performance with mini-batch S. We set the value of S in Eq. 8 to {1, 3, 5} respectively to explore its impact on the re-weighting mechanism. From Figure 3 we observe that the model is not super sensitive to hyper-parameter S but can achieve a better estimate of the weights of the pseudo-labeled data with increasing mini-batch values.

Ratio of Unlabeled Data
5% 25% 75%

Datasets SNIPS CoNLL03

84.47 87.10 87.50

72.92 76.46 76.56

Table 7: Varying proportion of unlabeled data for MetaST with 10 labels per slot.

F1

90 S=1 S=2
85 S=3
80
75
70 CONLL03 SNIPS
Figure 3: Varying S mini-batch labeled data for re-weighting.

A.2 ANALYSIS OF RE-WEIGHTING ON SNIPS AND CONLL03
Analysis of pseudo-labeled data re-weighting. To visually explore the adaptive re-weighting mechanism, we illustrate token re-weighting of MetaST on CoNLL03 and SNIPS datasets with K=10 shot at step 100 in Fig. 4. Besides the observation in the experimental section, we observe that many difﬁcult and correct pseudo-labeled samples (low teacher conﬁdence) are selected according to Fig. 4a.

(a) SNIPS

(b) CoNLL03

Figure 4: Visualization of MetaST re-weighting examples on SNIPS and CoNLL03 (EN).

A.3 K-SHOTS
Effect of varying the number of few-shots K. We show the performance changes with respect to varying number of few-shots K {5, 10, 20, 100} on Wikiann (en), MIT movie, MIT Restaurant, CoNLL2003 (En), Multilingual CoNLL and Multilingual Wikiann in Table 9-13. Since the number of labeled examples for some slots in Email dataset is around 10, we only show 5 and 10 shots for Email dataset in Table 8.

14

Table 8: Email Dataset.

Method

Shots

5

10

Full-supervision BERT

0.9444

Few-shot Supervision

BERT

0.8211

0.8785

Few-shot Supervision + unlabeled data

CVT

67.44

78.24

SeqVAT

64.67

72.65

Mean Teacher 84.10

89.53

VAT

83.24

89.71

Classic ST 86.88

90.70

BOND

84.92

89.75

MetaST

89.21

92.18

Method

Shots (3 Slot Types)

5

10

20

Full-supervision BERT

84.04

Few-shot Supervision

BERT

37.01

45.61

Few-shot Supervision + unlabeled data

CVT

16.05

27.89

SeqVAT

21.11

35.16

Mean Teacher 30.92

41.43

VAT

24.72

38.81

Classic ST 32.72

46.15

BOND

34.22

48.73

MetaST

55.04

56.61

54.53
46.42 42.26 50.61 50.15 54.41 52.45
60.38

100
67.87
66.36 62.37 67.16 66.31 68.64 68.89 73.20

Table 9: Wikiann (En) Dataset.

Method

Shots (8 Slot Types)

5

10

20

Full-supervision BERT

78.95

Few-shot Supervision

BERT

41.39

54.06

Few-shot Supervision + unlabeled data

CVT

33.74

42.57

SeqVAT

41.94

51.55

Mean Teacher 40.37

51.75

VAT

41.29

53.34

Classic ST 44.35

56.80

BOND

43.01

55.78

MetaST

53.02

63.83

60.12
51.33 56.15 57.34 59.68 60.28 59.96
67.86

100
72.24
70.84 71.39 72.40 72.65 73.13 73.60 75.25

Table 10: MIT Restaurant Dataset.

Method

Shots (4 Slot Types)

5

10

20

Full-supervision BERT

87.67

Few-shot Supervision

BERT

64.80

70.77

Few-shot Supervision + unlabeled data

Mean Teacher 64.55

68.34

VAT

64.97

67.63

Classic ST 67.95

72.69

BOND

69.42

72.79

MetaST

73.34

76.65

73.89
73.87 74.26 73.79 76.02 77.01

100
80.61
79.21 80.70 81.82 80.62 82.11

Table 12: Multilingual CoNLL03.

Method

Shots (12 Slot Types)

5

10

20

Full-supervision BERT

87.87

Few-shot Supervision

BERT

62.80

69.50

Few-shot Supervision + unlabeled data

CVT

57.48

62.73

SeqVAT

60.94

67.10

Mean Teacher 58.92

67.62

VAT

60.75

70.17

Classic ST 63.39

71.88

BOND

62.50

70.91

MetaST

72.57

77.67

75.81
70.20 74.15 75.24 75.41 76.58 75.52
80.33

100
82.49
81.82 82.73 82.20 82.39 83.06 82.65 84.35

Figure 5: MIT Movie Dataset.

Method

Shots (4 Slot Types)

5

10

20

Full-supervision BERT

92.40

Few-shot Supervision

BERT

63.87

71.15

Few-shot Supervision + unlabeled data

CVT

51.15

54.31

SeqVAT

58.02

67.21

Mean Teacher 59.04

68.67

VAT

57.03

65.03

Classic ST 64.04

70.99

BOND

62.52

69.56

MetaST

71.49

76.65

73.57
66.11 74.15 72.62 72.69 74.65 74.19
78.54

100
84.36
81.99 82.20 84.17 84.43 84.93 83.87 85.77

Table 11: CoNLL2003 (EN)

Method

Shots (3 Slot Types × 41 languages)

5

10

20 100

Full-supervision BERT

87.17

Few-shot Supervision

BERT

77.68

79.67

Few-shot Supervision + unlabeled data

Mean Teacher 77.09

80.23

VAT

74.71

78.82

Classic ST 76.73

80.24

BOND

78.81

79.57

MetaST

79.10

81.61

82.33
82.19 82.60 82.39 82.19 83.14

85.70
85.34 85.82 86.08 86.14 85.57

Table 13: Multilingual Wikiann

15

A.4 IMPLEMENTATIONS AND HYPER-PARAMETER
We do not perform any hyper-parameter tuning for different datasets. The batch size and maximum sequence length varies due to data characteristics and are as shown in Tbale 14. The hyperparameters are as shown in Table 14.
Also, we retain parameters from original BERT implementation from https://github.com/ huggingface/transformers.
We implement SeqVAT based on https://github.com/jiesutd/NCRFpp.

Dataset
SNIPS Email Movie Restaurant CoNLL03 (EN) Wikiann (EN) CoNLL03 (multilingual) Wikiann (multilingaul)

Sequence Length
64 64 64 64 128 128 128 128

Batch Size
16 16 16 16 16 16 16 16

Labeled data sample size |B|
32 32 32 16 8 8 32 32

Unlabeled Batch Size
32 32 32 32 32 32 32 32

BERT Encoder
BERT-base-uncased BERT-base-cased BERT-base-uncased BERT-base-uncased BERT-base-cased BERT-base-cased BERT-multilingual-base-cased BERT-multilingual-base-cased

Table 14: Batch size, sequence length and BERT encoder choices across datasets

BERT attention dropout BERT hidden dropout Latest Iteration R in labeled data acquisition BERT output hidden size h
Steps for ﬁne-tuning teacher model on labeled data Steps T for self-training model on unlabeled data Mini-batch S Re-initialize Student Pseudo-label Type Warmup steps learning rate α Weight decay
Table 15: Hyper-parameters.

0.3 0.3 5 768
2000 3000 5 Y Hard 20 5e−5 5e−6

16

