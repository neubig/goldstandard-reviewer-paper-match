Bandits for Correlated Markovian Environments

arXiv:1803.04008v2 [cs.LG] 1 Mar 2019

Multi-Armed Bandits for Correlated Markovian Environments with Smoothed Reward Feedback

Tanner Fiez Department of Electrical and Computer Engineering University of Washington Seattle, WA 98195-4322, USA
Shreyas Sekar Harvard University Cambridge, MA 02138, USA
Lillian J. Ratliﬀ Department of Electrical and Computer Engineering University of Washington Seattle, WA 98195-4322, USA

fiezt@uw.edu ssekar@hbs.edu ratliffl@uw.edu

Editor:

Abstract
We study a multi-armed bandit problem in a dynamic environment where arm rewards evolve in a correlated fashion according to a Markov chain. Diﬀerent than much of the work on related problems, in our formulation a learning algorithm does not have access to either a priori information or observations of the state of the Markov chain and only observes smoothed reward feedback following time intervals we refer to as epochs. We demonstrate that existing methods such as UCB and ε–greedy can suﬀer linear regret in such an environment. Employing mixing-time bounds on Markov chains, we develop algorithms called EpochUCB and EpochGreedy that draw inspiration from the aforementioned methods, yet which admit sublinear regret guarantees for the problem formulation. Our proposed algorithms proceed in epochs in which an arm is played repeatedly for a number of iterations that grows linearly as a function of the number of times an arm has been played in the past. We analyze these algorithms under two types of smoothed reward feedback at the end of each epoch: a reward that is the discount-average of the discounted rewards within an epoch, and a reward that is the time-average of the rewards within an epoch. Keywords: online learning, multi-armed bandits, regret minimization, markovian rewards, upper conﬁdence bounds
1. Introduction
Online learning and the theory of multi-armed bandits play a key role in shaping how digital platforms actively engage with users. A common theme underlying these platforms is the presence of mechanisms that target individuals with personalized options chosen from a pool of available actions. Tools from the theory of multi-armed bandits provide a systematic framework for synthesizing algorithms where a decision-maker interacts with an agent, by balancing exploration (learning how agents react to new alternatives) with exploitation (repeatedly oﬀering the best performing options to an agent). Typical applications where
1

Fiez, Sekar, and Ratliff
such trade-oﬀs arise include recommender systems (Li, Chu, Langford, and Schapire, 2010; Li, Karatzoglou, and Gentile, 2016), crowdsourcing (Liu and Liu, 2017; Tran-Thanh et al., 2014), and incentive design in e-commerce and physical retail1 (Chakrabarti et al., 2008). A rich stream of literature has investigated multi-armed bandit algorithms for such scenarios, obtaining near-optimal performance guarantees for a multitude of settings (see, e.g., Bubeck and Cesa-Bianchi, 2012; Lattimore and Szepesv´ari, forthcoming; Slivkins, forthcoming).
Despite their popularity, most of the bandit algorithms in this line of research fail to take into account a crucial component of human interaction prevalent in the aforementioned applications. Notably, the type or state of an agent at any given point in time depends primarily on their underlying beliefs, opinions, and preferences; as these states evolve, so do the rewards for pursuing each distinct action. For example, there is mounting empirical evidence that humans make decisions by comparing to evolving reference points such as the status quo, expectations about the future, or past experiences (Kahneman and Tversky, 1984). Against this backdrop, we illuminate three critical challenges that motivate our work and render current bandit approaches ineﬀective in a dynamic setting.
Correlated Evolution of Rewards: The evolution of an agent’s beliefs or reference point is inextricably tied to the mechanisms that they interact with. Hence, the preferences of an agent evolve in a correlated fashion, and consequently, so do the rewards for each action a decision-maker can select. As a result, standard techniques that ignore such correlations may misjudge the rewards of each action. It is worth noting that several previous works studying online decision-making problems have identiﬁed similar dynamic feedback loops in applications such as click-through rate prediction (Graepel et al., 2010) and rating systems (Herbrich et al., 2007). Lack of State Information: Digital platforms that engage with users often observe only their actions (e.g., click or no click) that can act as a proxy for a reward, and rarely have access to their underlying beliefs, opinions, or preferences that induce responses. Thus, it is imperative to design learning policies that are unaware of the agent’s state, yet which remain cognizant of the general fact that rewards are drawn from evolving distributions over the agent’s state. Batch Feedback: In real world systems, it is often the case that reward feedback is processed in batches to update a learning policy because of various runtime constraints (see Chapter 8 of Slivkins, forthcoming). Moreover, immediate feedback may not reﬂect the long-term mean reward of an action when agent preferences are rapidly evolving. In such scenarios, frequently adapting a policy can further complicate the task of distinguishing between the underlying value of actions.
The objective of the work in this paper is to develop a principled approach to solving bandit problems in environments where: (i ) future rewards are correlated with past actions as a result of a dependence on the agent’s underlying state, (ii ) the decision-maker does not have a priori information regarding the state nor do they observe the state during their interaction with the agent, and (iii ) reward feedback cannot be collected immediately to update a learning policy. In order to capture these features, we consider a bandit setting
1. Although we motivate our work with applications pertaining to interactions between a decision-maker and an agent, an example being digital platforms that actively engage with users, our framework applies to any correlated Markovian environment, e.g., spectrum access applications (Tekin and Liu, 2012).
2

Bandits for Correlated Markovian Environments
with m arms or actions and an underlying state θ ∈ Θ. The policy class is restricted such that a ﬁxed action must be played within each epoch—a time interval consisting of a number of iterations that grows linearly as a function of the number of times an arm has been played in the past. The observed feedback at the end of an epoch is a smoothed reward. The smoothed reward can be modeled as a discount-averaged reward or a timeaveraged reward of the instantaneous rewards at each iteration within an epoch. In the discount-averaged reward model, rewards carry more weight toward the end of an epoch, while in the time-averaged reward model, rewards are given equal weight within an epoch. The reward for an action at each iteration within an epoch is drawn from a distribution that depends on θ. Moreover, the state θ evolves in each iteration according to a Markov chain whose transition matrix depends on the arm selection.
Although our setup is a generalization of the classic bandit problem, popular approaches such as UCB and ε–greedy (Auer et al., 2002a) perform poorly here, often converging to suboptimal arms; we demonstrate this in Example 1 of Section 2. The failure of existing algorithms stems from the correlation between actions: since the evolution of θ depends on the action played, observed rewards are a function of past actions. Finally, it is worth noting that problems with Markovian rewards are traditionally studied through the lens of reinforcement learning (Azar et al., 2013; Jaksch et al., 2010). However, all of the algorithms in this domain assume that the decision-maker has full or partial information on an agent’s state. We believe that the state-agnostic bandit algorithms developed in this work will serve as a bridge between the traditional bandit theory and techniques from reinforcement learning.
1.1 Contributions and Organization
Given a multi-armed bandit problem where the arm rewards evolve in a correlated fashion according to a Markov chain, the fundamental question studied in this work is the following: Can we design an algorithm that provably guarantees sublinear regret as a function of the time horizon in the absence of state observations and knowledge of the way in which the reward distributions are evolving?
The contributions of this work are now summarized. To demonstrate the necessity of our work, we show that traditional bandit algorithms such as UCB and ε–greedy can misidentify the optimal arm and suﬀer linear regret as a result of underestimating the value of the optimal arm in a correlated Markovian environment. We then develop a general framework for analyzing epoch-based bandit algorithms for problems with correlated Markovian rewards. We join this framework with theory of Markov chain mixing times and existing bandit principles to design algorithms capturing the uncertainty in the empirical mean rewards arising from the unobserved, evolving state distribution and the stochasticity of the reward distributions. This gives us the central contributions of the paper, bandit algorithms called EpochUCB and EpochGreedy. We prove O(log(n)) gap-dependent regret bounds for each algorithm under discount-averaged and time-averaged reward feedback. Given that no online learning algorithm can obtain sublogarithmic regret bounds, even in the case of independent and identically distributed rewards, our algorithms produce regret bounds that are asymptotically tight. Moreover, we obtain an O( n log(n)) gap-independent regret bound for EpochUCB under discount-averaged and time-averaged reward feedback.
3

Fiez, Sekar, and Ratliff
Finally, we present a number of simulations comparing the empirical and theoretical performance of our proposed algorithms. This is augmented with a set of illustrative experiments demonstrating that our proposed algorithms empirically outperform existing bandit and reinforcement learning algorithms in correlated Markovian environments.
We now brieﬂy characterize the challenges involved in designing online learning algorithms when the rewards evolve in a correlated fashion and outline our techniques to overcome them. Unlike a typical bandit problem, there are two sources of uncertainty in the empirical rewards: (i ) uncertainty regarding the state, and (ii ) uncertainty from the stochasticity of the distributions from which rewards are drawn. As we demonstrate with illustrative examples, misjudging the expected reward of arm as an artifact of failing to take into account the multiple sources of uncertainty can lead to signiﬁcant regret. Moreover, the distribution from which an arm’s reward is drawn can change even when that arm is not selected since rewards depend on the evolving state. To overcome these obstacles, our proposed algorithms leverage techniques from the theory of mixing times for Markov chains, while retaining the spirit of the UCB and ε–greedy algorithms developed in Auer et al. (2002a). Speciﬁcally, by characterizing the mixing times, we are able to obtain estimates of the empirical mean rewards that closely approximate the expected stationary distribution rewards and maintain accurate conﬁdence windows representing the uncertainty in these estimates. The execution of each algorithm in epochs of growing length ensures that the uncertainty in the empirical mean reward estimates for each arm eventually dissipates.
Following a formalization of the problem we study in Section 2, we present our proposed EpochUCB and EpochGreedy algorithms and analyze their regret in Section 3. In Section 4, we present simulation results. We conclude in Section 5 with a discussion of open questions and comments on future work. Finally, to promote readability, an appendix comes after the primary presentation of our work. In Appendix A, we put forward a notation table that includes the most important and frequently used notation in the paper. The majority of the proofs backing our theoretical results are contained in Appendix B. However, proof sketches are provided immediately following the statements of our main results. In Section C, further details are given on the existing algorithms that we empirically compare to our proposed algorithms.
1.2 Background and Related Work
The two distinct features separating our model from previous work on multi-armed bandits with Markov chains are that the rewards evolve in a correlated fashion and the decisionmaker is fully unaware of the agent’s state. These features are missing in the related rested and restless multi-armed bandit problems.
In the rested (Anantharam et al., 1987; Tekin and Liu, 2010) and restless (Ortner et al., 2014; Tekin and Liu, 2012) bandit literature, there is an independent Markov chain tied to each arm and the reward for an arm depends on the state of the Markov chain for that arm. In each model, when an arm is selected the state of the Markov chain for that arm is observed and transitions. The distinguishing characteristic between the problems is the behavior of the Markov chains tied to arms that are not selected. In the rested bandit model, the Markov chains associated with unselected arms remain unaltered; however, in
4

Bandits for Correlated Markovian Environments
the the restless bandit model, the Markov chains associated with unselected arms evolve precisely as they would had they been played.
Hence, as is true in our model, in the restless bandit problem the reward distribution on an arm can evolve even when the arm is not being played. However, a key point of distinction is that the evolution of the reward distribution on an arm is independent of the reward distribution for every other arm. In contrast, the problem setting we study is such that there is a shared Markov chain between arms so that the evolution of the rewards on each arm is correlated. Moreover, in the setting under consideration there are no observations of the Markov chain state or distribution. Although these distinctions may appear subtle, the correlation between present actions and future rewards, along with the absence of state observations, results in a number of technical diﬃculties.
An exception to the formulations of the rested and restless bandit problems is the manuscript of Mazumdar et al. (2017), which proposes a UCB-inspired strategy for expert selection in a Markovian environment. Our proposed UCB-inspired algorithm, called EpochUCB, improves upon the regret bound in that work. Moreover, we consider more general reward feedback structures and propose an ε–greedy inspired algorithm called EpochGreedy. Another work along this same vein is that of Azar et al. (2013); in this work, an expert selection strategy is proposed for ﬁnding policies in Markov decision processes with partial state feedback.
The problem we study bears some conceptual similarity to the traditional principalagent model from contract theory (Bolton et al., 2005; Laﬀont and Martimort, 2009). The standard principal-agent model is a one-shot interaction: a principal selects a signal j ∈ [m] to send to an agent with a type variable θ, and then the self-interested agent pursues an action depending on (θ, j). The reward the principal obtains is a function of the action of the agent, and consequently, the parameters (θ, j). Typically, there exists an information asymmetry between the principal and the agent. Prominent examples include adverse selection (agent type is unobservable to the principal) and moral hazard (agent action is unobservable to the principal). Recently, repeated principal-agent problems have begun to be studied as bandit problems where each round corresponds to the standard principal-agent formulation with adverse selection and moral hazard. A notable example of such a formulation is the work of Ho et al. (2016). The caveat of this work is that in each round a brand-new agent arrives with a type variable sampled from an i.i.d. distribution; following an interaction with the principal, an agent exits the system forever. In contrast, the problem we study can be viewed as a repeated principal-agent problem where a unique agent continuously interacts with the principal. However, while our formulation is an example of adverse selection since the state is unobserved and dynamically evolves, we do not model the strategic nature of the agent.
Finally, we remark that our setting is general enough to model a number of existing works, which we describe below:
1. Bandits with Positive Externalities: The state θ could represent the agent’s bias toward actions that have yielded high reward in the past as in Shah et al. (2018). The decision-maker receives a higher expected reward by selecting actions that the agent is positively predisposed toward.
2. Bandits Tracking Arm Selection History: In recharging bandits (Immorlica and Kleinberg, 2018), the reward on an arm is an increasing function of the time since it was
5

Fiez, Sekar, and Ratliff
last selected. The state θ could track such a time period. Along the same lines, in rotting bandits (Levine et al., 2017), the reward on an arm is a decreasing function of the number of times it has been played in the past. The state θ could track the number of plays of each arm.
2. Preliminaries
We now formalize the problem we study, detail technical challenges, and present an example demonstrating the insuﬃciency of existing algorithms that necessitates our work.
2.1 Problem Statement
Consider a decision-maker that faces the problem of repeatedly choosing an action to impact an agent over a ﬁnite time horizon. We refer to the set of possible actions as arms and use the notation [m] = {1, . . . , m} to index them. The agent is modeled as having state θ ∈ Θ, where Θ is a ﬁnite set, that evolves in time according to a Markov chain whose transition matrix depends on the arm selected by the decision-maker. In turn, the agent’s state inﬂuences the rewards the decision-maker receives. The goal of the decision-maker is to construct a policy that sequentially selects arms in order to maximize the cumulative reward over a ﬁnite horizon.
We restrict the decision-maker’s policy class to a speciﬁc type of multi-armed bandit algorithm that we refer to as an epoch mixing policy. Such a policy α is executed over epochs indexed by [n] = {1, . . . , n}. In each epoch k ∈ [n], the policy selects an arm α(k) ∈ [m] and repeatedly ‘plays’ this arm for τkα > 0 iterations within this epoch—where we use the superscript α to indicate that the epoch length depends on the arm selected— before receiving feedback in the form of a reward. When the decision-maker makes an arm choice at an epoch k ∈ [n], the state of the agent θ evolves for τkα iterations. The reward the decision-maker observes at the end of the epoch is a function of the rewards at each iteration within the epoch. In short, an epoch mixing policy proceeds on two time scales: each selection of an arm corresponds to an epoch k ∈ [n] that begins at time tk and ends at time tk+1 = tk + τkα following τkα iterations. Our motivation for restricting the policy class in this way stems from the inherent challenges online platforms face to process immediate feedback in order to update learning algorithms frequently, and the necessity of observing feedback based upon periods of near-stationarity to obtain meaningful regret bounds.
Returning to the agent model, the agent’s state θ is modeled as the state of a Markov chain. Let βtk : Θ → [0, 1] denote the state distribution at time tk and θtk denote the random variable representing the agent’s state at time tk having distribution βtk . Given that arm α(k) = j is selected at epoch k ∈ [n], let the arm-speciﬁc transition matrix of the Markov chain be denoted as Pj : Θ × Θ → [0, 1]. Then, the state distribution on each θ ∈ Θ evolves between epochs k and k + 1 as follows:
βtk+1 (θ) = θ ∈Θ Pjτkα (θ , θ)βtk (θ ),
where Pjτkα(θ , θ) is the probability of the state transition from θ to θ in τkα iterations—that is, Pjτkα is the τkα composition of Pj. Observe that the agent model we adopt captures the
6

Bandits for Correlated Markovian Environments

fact that the agent’s preferences depend on past interactions with the decision-maker since the state distribution when an epoch begins depends on previous epochs.

Assumption 1. For each arm j ∈ [m], the transition matrix Pj is irreducible and aperiodic.

Irreducible and aperiodic Markov chains are ergodic. Assumption 1 implies that for each
j ∈ [m], the Markov chain characterized by Pj has a unique positive stationary distribution that we denote by πj : θ → (0, 1]. Furthermore, let P˜j denote the time reversal of Pj, deﬁned as P˜j(θ, θ ) = πj(θπ)jP(jθ()θ ,θ) ∀ θ, θ ∈ Θ. The time reversal matrix P˜j is also irreducible and aperiodic with unique positive stationary distribution πj. Deﬁne the multiplicative reversiblization of Pj to be M (Pj) = PjP˜j, which is itself a reversible transition matrix.
The eigenvalues of M (Pj) are real and non-negative so that the second largest eigenvalue λ2(M (Pj)) ∈ [0, 1] (Fill, 1991).

Assumption 2. For each arm j ∈ [m], the transition matrix Pj is such that M (Pj) is irreducible.

This is a standard assumption in the bandit literature with Markov chains (see Tekin and Liu, 2012, and the references therein) that implies λ2(M (Pj)) ∈ [0, 1). Recall that Assumption 1 on the transition matrices ensures λ2(M (Pj)) ∈ [0, 1]. Hence, Assumption 2 on the transition matrices only restricts the boundary case when λ2(M (Pj)) = 1. The assumption is necessary to derive meaningful bounds on the deviation between the expected
reward feedback and the expected stationary distribution reward of an arm.

Remark 1. A permissive suﬃcient condition on an ergodic transition matrix P that ensures M (P ) is irreducible is P (θ, θ) > 0 ∀ θ ∈ Θ (Tekin and Liu, 2012). This condition holds naturally for a large class of applications. A much more restrictive, yet still relevant suﬃcient condition on an ergodic transition matrix P certifying that M (P ) is irreducible would be that P is also reversible. We emphasize that each of the suﬃcient conditions are not necessary conditions for Assumption 2.

Reward Models: The reward the decision-maker receives is dependent on the state of the agent. Let θk = {θt}tt=k+tk1−1 be the sequence of random state variables in epoch k ∈ [n] and let rθαk(k),k be the observed reward at the end of the epoch, where α(k) and θk denote the dependence on the arm selected and the state, respectively. Similarly, let rαθt(k),t denote the instantaneous reward at an iteration t ∈ [tk, tk+1 − 1] during epoch k ∈ [n]. While a number
of diﬀerent functions of the instantaneous rewards in each epoch could be considered, we restrict our attention to the case where rθαk(k),k is a smoothed reward over the epoch. We consider the observed smoothed reward to be a discount-averaged or time-averaged reward
of the instantaneous rewards in an epoch. For a discount factor γ ∈ (0, 1) selected by the
decision-maker, the discount-averaged reward is deﬁned as

rθk

=

1
α

α(k),k γk

tt=k+tk1 −1 (γ )tk+1 −1−t rαθt(k),t ,

where

γαk = tt=k+tk1−1(γ)tk+1−1−t

7

Fiez, Sekar, and Ratliff

denotes the sum of the discount factors in the epoch. In the special case that the discount factor γ = 1, the observed reward is a time-averaged reward:

rθk

=

1
α

α(k),k γk

tt=k+tk1−1 γtk+1−1−trαθt(k),t = τ1kα

tt=k+tk1−1 rαθt(k),t.

The rewards at an iteration rαθt(k),t are assumed bounded; without loss of generality, rαθt(k),t ∈ [0, 1]. Moreover, they are stochastic with stochastic kernel Tr(θ, α(k)) ∈ P([0, 1]) such that rαθt(k),t ∼ Tr(θt, α(k)) and where P([0, 1]) denotes the space of probability distributions on
[0, 1].

Remark 2. Observe that when γ ∈ (0, 1), the discount factors are growing within an epoch so that rewards are given more weight toward the end of an epoch, and when γ = 1, the rewards within an epoch are given equal weight. This general framework allows us to model a variety of objectives. For instance, agents are likely to have recency bias and hence, if a decision-maker’s instantaneous reward depends on some measure of agent happiness or satisfaction, then discounting rewards over the epoch is reasonable. On the other hand, if the decision-maker’s instantaneous reward measures revenue or proﬁt, then equally weighting all rewards accrued in an epoch is reasonable.

Given Assumption 1, if an arm j ∈ [m] is chosen at every iteration, then the Markov chain would eventually converge toward its stationary distribution πj. This would, in turn, give rise to a ﬁxed reward distribution. We deﬁne the expected stationary distribution reward µj for arm j ∈ [m] to be

µj = E θ∈Θ rjθπj(θ) ,
where the expectation is with respect to Tr(θ, j). Likewise, we deﬁne the optimal arm, indexed by j∗ ∈ [m], and denoted as ∗ when used in a subscript, to be the arm that yields the highest expected reward µj from its stationary distribution πj. Hence, the expected reward of the optimal arm j∗ is

µ∗ = maxj E θ∈Θ rjθπj(θ) .

We use a notion of regret as a performance metric that compares the cumulative expected reward over a ﬁnite horizon of a benchmark policy and that of the policy α. The benchmark policy we compare to is the best ﬁxed arm in hindsight on the stationary distribution rewards. That is, we compare to the policy that plays the optimal arm j∗ at each epoch and receives rewards drawn from its stationary distribution.

Deﬁnition 1 (Cumulative Regret). The cumulative regret after n epochs of policy α is

given by

Rα(n) = nµ∗ − E

n k=1

rθαk(k),k

,

(1)

where the expectation is with respect to the random draw of the rewards through Tr(θ, α(k)), arms selected by the decision-maker using α, and the state θ.

8

Bandits for Correlated Markovian Environments
2.1.1 Discussion of Regret Notion
Let us brieﬂy comment on the regret notion we consider. It is worth noting that the benchmark policy being compared to is the optimal policy within the policy class that is restricted to a ﬁxed arm being played. In general, however, the globally optimal policy for a given problem instance may not always play a ﬁxed arm at each epoch. Simply put, the globally optimal policy may select an arm dependent on the state of the Markov chain. In fact, we would expect the globally optimal policy to be a deterministic policy in each state. Meaning that, conditioning on the state, the globally optimal policy would play the best arm for that state. This is because in the full information case, where the decision-maker observes the initial state distribution, the dynamics, etc., the decision-maker simply faces a Markov decision process—which are known to have deterministic state-dependent optimal policy (Bellman, 1957). Of course, since in our problem the state is fully unobserved and no prior on the distribution is available, ﬁnding such a policy is infeasible. Owing to this basis, measuring the regret with respect to the best ﬁxed arm in hindsight on the stationary distribution rewards is standard in multi-armed bandit problems with Markov chains (see, e.g., Gai et al., 2011; Tekin and Liu, 2010, 2012). The regret notion we adopt—comparing to the best ﬁxed arm in hindsight when the globally optimal policy may not always play a ﬁxed arm—is often referred to as weak regret (Auer et al., 2002b).
2.2 Details on Technical Challenges and Insuﬃciency of Existing Methods
The key technical challenge stems from the dynamic nature of the reward distributions. Indeed, the rewards depend on an underlying state distribution which is common across arms; the initial distribution of the Markov chain when an arm is pulled is the distribution at the end of the preceding arm pull. The consequences of the evolving nature of the state distribution are two-fold: (i ) the reward distribution on any given arm can evolve even when the arm is not being played by the algorithm and (ii ) the fashion in which the reward distribution on each arm evolves when not being played depends on the current arm being played by the algorithm. That is, future reward distributions on each arm are correlated with the present actions of an algorithm. The manner in which the reward distributions evolve is precisely where the problem deviates from the rested and restless bandit problems and becomes more challenging.
Since rewards are dependent on the algorithm, i.i.d. reward assumptions, such as those found in the stochastic bandit problem, fail to hold. Despite this, a natural question may be whether or not naively employing algorithms from this literature, such as UCB and ε– greedy, is suﬃcient in a correlated Markovian environment. We consider a simple example that indicates it is not.
Example 1 (Failure of UCB and ε–greedy). Consider a problem instance with two arms [m] = {1, 2} and two states Θ = {θ1, θ2}. The state transition matrix and reward structure for each arm are depicted in Figure 1a. We assume that > 0 is a suﬃciently small constant. The stationary distribution for arm 1 is given by π1(θ1) = /( + 1) ≈ 0 and π1(θ2) = 1/(1 + ) ≈ 1, meaning at the stationary distribution of arm 1 the state is θ2 almost surely, and vice-versa for arm 2. The deterministic reward rjθi for each (arm, state) pair (j, θi) with j ∈ [m] and i ∈ {1, 2} is provided under the state. Clearly, the optimal strategy is to play arm 1 repeatedly to obtain a per arm selection reward of nearly one.
9

Fiez, Sekar, and Ratliff

0

1−

Arm 1

θ1

1

r1θ1 = 0 1− 1

θ2
r1θ2 = 1 0

Arm 2

θ1

θ2

r2θ1 = 0.5 (a)

r2θ2 = 0.5

Regret

5000 4000 3000 2000 1000
0 0

Cumulative Regret
EpochUCB EpochGreedy UCB ε-Greedy

2500 5000 7500 Epochs
(b)

10000

Figure 1: (a) State transition diagram and reward structure for each arm. (b) Regret for UCB and ε–greedy versus our proposed algorithms EpochUCB and EpochGreedy.

Suppose that the initial state distribution is given by β1 = [1, 0]. Every time UCB and ε–greedy play arm 1, the agent is in state θ1 with high probability; consequently the reward for arm 1 is estimated to be close to zero. Therefore, UCB and ε–greedy underestimate the reward for arm 1 and misidentify arm 2 as the optimal arm since the agent almost always remains in state θ1 as a result of the induced Markov chain. Simulations support this ﬁnding as demonstrated in Figure 1b. Indeed, UCB and ε–greedy converge to the suboptimal arm and suﬀer linear regret. In contrast, our proposed algorithms, EpochUCB (Section 3.3) and EpochGreedy (Section 3.4), identify the optimal arm rapidly and incur only sublinear regret.
3. Regret Analysis
We begin this section by deriving a general framework for analyzing the regret of any multi-armed bandit policy interacting with a correlated Markovian environment in which the observed feedback is a smoothed (discount-averaged or time-averaged) reward over an epoch. We then introduce our proposed EpochUCB algorithm and present gap-dependent and gap-independent regret bounds for both discount-averaged and time-averaged reward feedback. This is followed by an exposition of our proposed EpochGreedy algorithm. For EpochGreedy, we prove a gap-dependent regret bound for both discount-averaged and timeaveraged reward feedback.
3.1 Regret Decomposition
In this section, we derive a regret bound for a generic policy α in terms of the expected number of plays of each suboptimal arm. While such regret decompositions are typical in the bandit literature, our bound is novel. This is owing to the fact that we need to employ results on the mixing times of Markov chains to decompose the regret into components arising from the selection of a suboptimal arm and that coming from the misalignment of the agent’s state distribution with the stationary distribution.
10

Bandits for Correlated Markovian Environments

Deﬁne Tjα(n) =

n k=1

I {α(k)

=

j}

to

be

the

random

variable

representing

the

number

of epochs in which arm j ∈ [m] was selected by algorithm α in the initial n epochs. We use

I{·} to denote the indicator function, meaning that I{α(k) = j} = 1 when α(k) = j and

I{α(k) = j} = 0 when α(k) = j. Moreover, observe that j∈[m] Tjα(n) = n. Our goal is to relate Eα[Tjα(n)], where Eα emphasizes the randomness in the algorithm and the rewards, to the regret Rα(n). Toward this end, deﬁne ∆j = µ∗ − µj for each j ∈ [m] to be the reward

gap. We can add and subtract j∈[m] Eα[Tjα(n)]µj into (1) to obtain

Rα(n) = nµ∗ − j∈[m] Eα[Tjα(n)]µj + j∈[m] Eα[Tjα(n)]µj − E

n k=1

rθαk(k),k

= j∈[m] Eα[Tjα(n)](µ∗ − µj) + Eα

n k=1

j∈[m] I{α(k) = j}µj

−E

n k=1

j∈[m] I{α(k) = j}rθj,kk

= j=j∗ Eα[Tjα(n)]∆j + E nk=1 j∈[m] I{α(k) = j}(µj − rθj,kk) . (2)

Compared to the regret decomposition for the stochastic multi-armed bandit problem, which
has the form Rα(n) = j=j∗ Eα[Tjα(n)]∆j, the dynamic nature of the reward distributions in the problem leads any algorithm to incur an additional regret penalty through the term

E

n k=1

j∈[m] I{α(k) = j}(µj − rθj,kk) .

(3)

Intuitively, this regret term is capturing the fact that the expected reward for selecting an arm at any given epoch can potentially deviate from the expected stationary distribution reward of the arm in an unfavorable way when the state distribution is not at the stationary distribution.

Example 2 (Markovian Regret Penalty). Consider an optimal arm j∗ ∈ [m] with two states Θ = {θ1, θ2}, stationary distribution given by π∗(θ1) = and π∗(θ2) = 1 − where > 0 is a small constant, and deterministic state-dependent rewards given by r∗θ1 = 0 and r∗θ2 = 1, so that the expected stationary distribution reward for the arm is nearly one. Moreover,
suppose that the initial state distribution of a problem instance is given by β1 = [1, 0]. The expected reward of arm j∗ is close to zero in the initial epoch for this problem instance, implying that the regret penalty for selecting arm j∗ in the epoch is almost one despite the reward gap being zero. This example highlights precisely what (3) elucidates in the regret
decomposition: an arm selection can yield regret beyond the reward gap ∆j when the state distribution departs from the stationary distribution of the chosen arm. We often refer to
the regret term in (3) as the Markovian regret penalty.

In order to bound the Markovian regret penalty, we need some technical machinery. Let Rjθ,i be the reward received when arm j ∈ [m] is chosen for the i–th time, where we include θ in the subscript to note the state dependence of the random reward. For each arm j ∈ [m],
deﬁne the i–th ﬁltration:

Fj,i = σ(Rjθ,1, . . . , Rjθ,i, θt1 , . . . , θti ),

j

j

where tij is the time instance at which arm j ∈ [m] is chosen for the i-th time. That is,

Fj,i is the smallest σ-algebra generated by the random variables (Rjθ,1, . . . , Rjθ,i, θt1, . . . , θti ).

j

j

11

Fiez, Sekar, and Ratliff

From the tower property of expectation, we have E

n k=1

j∈[m] I{α(k) = j}(µj − rθj,kk)

= Eα nk=1 j∈[m] I{α(k) = j}E µj − rθj,kk Fj,Tjα(k)−1 ≤ Eα nk=1 j∈[m] I{α(k) = j} E µj − rθj,kk Fj,Tjα(k)−1 . (4)

Prior to continuing to bound the Markovian regret penalty, we introduce the epoch

sequence {τkα}nk=1 considered in this work. Recall that α(k) represents the arm selected by the policy α at the beginning of epoch k ∈ [n] and Tαα(k)(k − 1) is the number of times this arm has been selected in previous epochs. At each epoch k ∈ [n], the policy-dependent

epoch length is

τkα = τ0 + ζTαα(k)(k − 1),

(5)

where τ0, ζ ∈ Z+ are constants selected by the decision-maker. We also use the notation τjα,k = τ0 + ζTjα(k − 1) to denote the epoch length when α(k) = j at epoch k ∈ [n]. It is important to recognize that the length of each epoch is random owing to the dependence

on not only the epoch index, but also on the identity of the arm selected in the epoch. This

is a reasonable model since a learning strategy should only be altered for an arm as a result of acquiring more information about the arm. The sequence {τkα}nk=1 ensures that as an arm is repeatedly selected and the conﬁdence in the expected stationary reward of the arm

grows, so does the length of each epoch when the arm is selected. Consequently, once highly

suboptimal arms are discarded, each epoch contains suﬃciently many iterations to guarantee

that the observed rewards closely approximate the stationary distribution rewards—this is

crucial for discriminating between the optimal arm and nearly optimal arms. We remark

that epochs of a ﬁxed duration would lead to a regret bound that is linear in the time

horizon under our analysis. This provides theoretical justiﬁcation beyond Example 1 on

the insuﬃciency of existing bandit algorithms for this problem. Informally, this is because

algorithms that do not play arms an increasing number of times by design may never

push the state distribution toward a stationary distribution and the rewards drawn from

a distribution misaligned with a stationary distribution could be highly suboptimal. This

observation serves as further motivation for the feedback model we study apart from relevant

applications. More detail is provided on this point in Appendix D.

We now return to deriving a bound on the Markovian regret penalty. To do so, we

adopt tools from the theory of Markov chains. Indeed, we need a classic result about the

convergence rates of Markov chains.

Proposition 1 (Fill (1991)). Let P be an irreducible and aperiodic transition matrix on
a ﬁnite state space Θ and π be the stationary distribution. Deﬁne the chi-squared distance from stationary at time n as χ2n = θ(πn(θ) − π(θ))2/π(θ), where πn = θ P n(θ, ·)π0(θ) and π0 is the initial distribution of the Markov chain. Then, 4 πn − π 2 ≤ χ20(λ2(M (P )))n. Furthermore,

maxπ0∈P(Θ) θ P n(θ, ·)π0(θ) − π(·) 2 ≤ 41 1 + (1−mmininθππ(θ(θ)))2 (λ2(M (P )))n, (6) θ

where P(Θ) is the space of probability distributions on Θ.
Noting that χ20 is always bounded above by 1+(1−minθ π(θ))2(minθ π(θ))−1, Equation 6 is easily derived.

12

Bandits for Correlated Markovian Environments

Remark 3. Proposition 1 provides a bound certifying that the state distribution of a Markov chain with an ergodic transition matrix P will converge toward its stationary distribution at least at a geometric rate in λ2(M (P )) when λ2(M (P )) ∈ [0, 1). Recall that when M (P ) is irreducible, λ2(M (P )) ∈ [0, 1).
The ensuing lemma translates Proposition 1 into a bound on the deviation of the expected reward of an arm selection from the expected stationary distribution reward of the arm; the deviation decays geometrically as a function of the epoch length. Beforehand, for each arm j ∈ [m], deﬁne the following constants:
λj = (λ2(M (Pj)))1/2, ηj = min{γ, λj}, φj = max{γ, λj}, ψj = ηj/φj.

Lemma 1 (Convergence of Expected Reward to Expected Stationary Reward). Suppose Assumptions 1 and 2 hold and α(k) = j ∈ [m] at epoch k ∈ [n]. Then,

E[µj − rθj,kk Fj,Tjα(k)−1

≤ , Cj υjγ (τjα,k)
γ αk

where

Cj = 1/2(1 + (1 − minθ πj(θ))2/ minθ πj(θ))1/2

and υjγ(τjα,k) is deﬁned as follows depending on the type of reward feedback: 1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

 τα −1

τα

γα

 (φj ) j,k (1−(ψj ) j,k ) , if γ = λj

υj (τj,k) =

(1−ψj )
α

.

(7)

(φj )τj,k−1τjα,k,

otherwise

2. Time-Averaged Reward Feedback: γ = 1.
τα
υjγ (τjα,k) = 1−1(λ−jλ)jj,k . (8)

The proof of Lemma 1 is primarily a consequence of Proposition 1 and can be found in Appendix B.2. Observe that since Proposition 1 holds for any initial state distribution, Lemma 1 holds irrespective of the state distribution at the beginning of an epoch, and hence, is independent of the algorithm.
Remark 4. Lemma 1 contains a discount factor dependent deﬁnition for υjγ(τjα,k) under discount-averaged reward feedback. To be precise, the deﬁnition of υjγ(τjα,k) depends on if γ = λj. The deﬁnition of υjγ(τjα,k) provided for the case that γ = λj holds even when γ = λj. However, the bound speciﬁed for when γ = λj is tighter than that speciﬁed for when γ = λj. More generally, each bound we give in this paper for discount-averaged reward feedback contains similar discount factor dependent deﬁnitions; it will always be the case that the bounds provided for the event in which γ = λj for some j ∈ [m] hold when γ = λj for each j ∈ [m], but the latter bounds are stronger.
13

Fiez, Sekar, and Ratliff

Returning to the regret decomposition, we apply Lemma 1 to (4) and obtain

n

n

Eα[ I{α(k) = j}|E[µj − rθj,kk|Fj,Tjα(k)−1]|] ≤ Eα[ I{α(k) = j}υjγ(τjα,k)]. (9)

k=1 j∈[m]

k=1 j∈[m]

We now derive a bound on (9) dependent on the type of reward feedback. Building on Lemma 1, we need to consider several cases: 1) γ ∈ (0, 1) and γ = λj for all j ∈ [m], 2) γ ∈ (0, 1) and γ = λj for some j ∈ [m], and 3) γ = 1.

Case 1. γ ∈ (0, 1) and γ = λj ∀ j ∈ [m].

τ α −1

τα

Eα nk=1 j∈[m] I{α(k) = j}υjγ (τjα,k) = Eα nk=1 j∈[m] I{α(k) = j} Cj(φj) jγ,kαk (1−(1ψ−j()ψj) j,k )

≤ Eα

Cj j∈[m] 1−ψj

τ α −1

n k=1

I {α(k)

=

j} (φj )γjα,k

k

≤ Eα

Cj j∈[m] 1−ψj

n k=1

I {α(k)

=

j}(φj )τjα,k−1

= Eα

Cj j∈[m] 1−ψj

iT=jα1(n)(φj )τ0+ζ(i−1)−1

≤ j∈[m] 1−Cψj j ni=1(φj )τ0+ζ(i−1)−1

(10)

= j∈[m] Cj φ(φjj−)ητ0j 11−−((φφjj))ζζn

Observe that as n → ∞, the inner sum found in (10) approaches the constant given as (φj)τ0(φj − ηj)−1(1 − (φj)ζ )−1 since it is a geometric series.

Case 2. γ ∈ (0, 1) and γ = λj for some j ∈ [m].

Eα

n k=1

j∈[m] I{α(k) = j}υjγ (τjα,k)

= Eα ≤ Eα

nk=1 j∈[m] I {α(k) = j} Cj (φj )τγjααk,k−1τjα,k

j∈[m] Cj

n k=1

I {α(k)

=

j}(φj )τjα,k−1τjα,k

= Eα j∈[m] Cj Ti=jα1(n)(φj )τ0+ζ(i−1)−1(τ0 + ζ(i − 1))

≤ j∈[m] Cj ni=1(φj )τ0+ζ(i−1)−1(τ0 + ζ(i − 1))

(11)

= j∈[m] Cj (φj )τ0−1 τ0−(φ1j−)ζ(φn(τ)ζ0+ζn) + ζ(φ(j1)−ζ ((1φ−()φζ )j2)ζn)

j

j

The ﬁnal equality follows from recognizing that the inner sum contained in (11) is an
arithmetico-geometric series and substituting the expression for the ﬁnite sum. Observe that as n → ∞, the arithmetico-geometric series in (11) approaches the constant given as τ0(1 − (φj)ζ)−1 + ζ(φj)ζ((1 − (φj)ζ)2)−1. For more details on this series, see Appendix B.3.

14

Bandits for Correlated Markovian Environments

Case 3: γ = 1.

τα
Eα nk=1 j∈[m] I{α(k) = j}υjγ (τjα,k) = Eα nk=1 j∈[m] I{α(k) = j} Cjτ(jα1,k−((1λ−jλ) jj),k )

≤ Eα

Cj j∈[m] 1−λj

n k=1

I {α(k)

=

j}

1 τα

j,k

= Eα

Cj j∈[m] 1−λj

Tjα(n)

1

i=1 τ0+ζ(i−1)

≤ j∈[m] 1−Cλj j ni=1 τ0+ζ1(i−1)

(12)

≤ j∈[m] 1−Cλj j τ10 + ζ1 log 1 + ζτn0

The ﬁnal inequality is obtained from the observation that the inner sum found in (12) is a harmonic sum that can be bound with standard techniques. We include the derivation in Appendix B.1.
The bounds we just derived give our ﬁnal bounds on the Markovian regret penalty. Hence, plugging the bounds on the Markovian regret penalty back into the initial expression for the regret decomposition found in (2) gives rise to the following proposition.

Proposition 2 (Regret Decomposition). Suppose Assumptions 1 and 2 hold. Then, for any given algorithm α with corresponding epoch length sequence {τkα}nk=1 as given in (5):
Rα(n) ≤ j=j∗ Eα Tjα(n) ∆j + j∈[m] Lγj (n),
where Lγj (n) is deﬁned as follows depending on the type of reward feedback: 1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

Lγ (n) = Cj φ(φjj−)ητ0j
j

11−−((φφjj))ζζn ,
τ −(φ )ζn(τ +ζn)

ζ(φ )ζ (1−(φ )ζn)

Cj (φj )τ0−1 0 1j−(φj )ζ0

+ (j1−(φj )ζ )j2 ,

2. Time-Averaged Reward Feedback: γ = 1.

if γ = λj ∀ j ∈ [m] .
otherwise
(13)

Lγj (n) = 1−Cλj j τ10 + ζ1 log 1 + ζτn0 .

(14)

Remark 5. The type of reward feedback (discount-averaged or time-averaged) for which the Markovian regret penalty of j∈[m] Lγj (n) is not as costly depends on the precise discount factor under discount-averaged reward feedback, the Markov chain statistics (Cj, λj) for each j ∈ [m], and the time horizon n. Typically however, the Markovian regret penalty will be smaller under discount-averaged reward feedback than under time-averaged reward feedback. In most cases, this is to be expected since the rewards are given increased weight as the state distribution tends closer to a stationary distribution.
3.1.1 Discussion of Regret Decomposition
The dynamic and evolving reward structure present in the problem we study leads any algorithm to incur regret beyond the usual penalty for playing suboptimal arms via, what
15

Fiez, Sekar, and Ratliff
we refer to as, the Markovian regret penalty (see 3). Leveraging classic results on mixing of Markov chains and the construction of the epoch length sequence {τkα}nk=1 considered in this work, we bounded the Markovian regret penalty with j∈[m] Lγj (n). In essence, this bound limits the regret arising from the rewards on an arm being drawn from an evolving distribution to a term that quickly approaches a constant as the time horizon grows in the case of discount-averaged reward feedback and a term that grows only logarithmically in the time horizon in the case of time-averaged reward feedback. The regret decomposition allows us to now focus soley on the selection of suboptimal arms.
3.2 Preliminaries for Algorithm–Based Regret Bounds
Given Proposition 2, in order to obtain a bound on the regret for a particular algorithm α, we need to limit Eα[Tjα(n)] for each j = j∗. To do so, it is important to characterize the uncertainty in the empirical mean reward of each arm as a function of the number of times the arm has been pulled. Fundamentally, there are two sources of uncertainty in the observed rewards:
1. The reward distribution on each arm is dynamic owing to the dependence on the unobserved and evolving state distribution.
2. The observed rewards derive from stochastic reward distributions.
Hence, in contrast to the conventional stochastic multi-armed bandit problem, where the stochasticity of the observed rewards is the only source of uncertainty, we must also carefully consider how much uncertainty arises from the dynamic nature of the reward distributions as an artifact of the unobserved and evolving state distribution.
From Lemma 1, we can observe that the upper bound on the deviation between the expected reward of an arm selection and the expected stationary distribution reward for that arm decays as a function of the number of times the arm has been selected—since epochs grow linearly in the number of times an arm has been pulled in the past. Consequently, the mean of these deviations vanishes as the number of times the arm has been pulled grows. Using this observation, the following lemma delineates the maximum amount of uncertainty in the empirical mean reward of an arm arising from the dynamic nature of the reward distribution on the arm from that coming out of the stochasticity of the rewards. Precisely, Lemma 2 provides a bound on the deviation between the expected mean reward and the expected stationary distribution reward for an arm j ∈ [m] after it has been selected Tj times. Lemma 2 (Convergence of Expected Mean Reward to Expected Stationary Reward). Suppose Assumptions 1 and 2 hold. Then, after an arm j ∈ [m] has been played Tj times by an algorithm α with corresponding epoch length sequence {τkα}nk=1 as given in (5),
µj − T1j Ti=j 1 E[Rjθ,i|Fj,i−1] ≤ LγjT(jTj ) .
The proof of Lemma 2 follows from manipulating the expression that needs to be bounded into a sum over terms that can each be bounded using Lemma 1 and then applying similar analysis to that which was used to bound (9) when deriving Proposition 2. The full proof can be found in Appendix B.3.
16

Bandits for Correlated Markovian Environments

Remark 6. In a similar manner to how we were able to limit the Markovian regret penalty, Lemma 2 limits the amount of uncertainty stemming from the dynamic nature of the reward distribution on an arm to a term that tends toward zero quickly as a function of the number of times the arm has been pulled.
Given that Lemma 2 characterizes the maximum amount of uncertainty coming solely from the evolution of the reward distributions in time, we are left to identify the uncertainty arising from the stochasticity in the rewards. To do so, we need a concentration inequality that does not require independence in the observed rewards of an arm since the underlying Markov chain that generates the rewards is common across the arms. On that account, an important technical tool for our impending algorithm-based regret analysis is the AzumaHoeﬀding inequality.
Proposition 3 (Azuma-Hoeﬀding Inequality (Azuma, 1967; Hoeﬀding, 1963)). Suppose (Zi)i∈Z+ is a martingale with respect to the ﬁltration (Fi)i∈Z+ and there are ﬁnite, nonnegative constants ci, such that |Zi − Zi−1| < ci almost surely for all i ≥ 1. Then for all
>0 P (Zn − E[Zn] ≤ − ) ≤ exp − 2 ni=2 1 c2i .

To apply the Azuma-Hoeﬀding inequality, we need to formulate our problem as a Martingale diﬀerence sequence. Toward this end, deﬁne the random variables

Xj,i = Rjθ,i − E[Rjθ,i|Fj,i−1],

where the expectation is taken with respect to Tr(θ, j), and

Yj,Tj =

Tj i=1

Xj,i,

(15)

where Tj denotes number of times arm j has been played. Note that Yj,Tj is a martingale; indeed, since Yj,Tj is Fj,Tj –measurable by construction,

E[Yj,Tj+1|Fj,Tj ] = E[Xj,Tj+1|Fj,Tj ] + E[Yj,Tj |Fj,Tj ] = Yj,Tj

and E[|Yj,Tj |] < ∞ since rewards are bounded. Moreover, the boundedness of the rewards also implies the martingale Yj,Tj has bounded diﬀerences: |Yj,Tj − Yj,Tj−1| = |Xj,Tj | ≤ 1 almost surely since rewards are normalized to be on the interval [0, 1], without loss of generality.
The remainder of this section is devoted to presenting our proposed EpochUCB and EpochGreedy algorithms along with the regret bound guarantees we obtain for each of these algorithms. The environment simulation procedure for the algorithms is given in Algorithm 1. To derive the algorithm-based regret bounds, we make use of the techniques we have developed to reason about the uncertainty in the empirical mean reward of each arm in conjunction with the proof techniques developed to analyze the UCB and ε–greedy algorithms.

17

Fiez, Sekar, and Ratliff

Algorithm 1 Environment Implementation for Pulling an Arm

1: function pullarm(i, k, γ, tk, τkα)

2: rθi,kk ← 0, γαk ← 0

3: for t ∈ [tk, tk + τkα) do

4:

riθ,tt ∼ Tr(θt, i), θt ∼ βt

5:

rθi,kk ← rθi,kk + (γ)tk+τkα−1−triθ,tt

6:

γαk ← γαk + (γ)tk+τkα−1−t

7:

βt+1(θ) = θ ∈Θ Pi(θ , θ)βt(θ ) ∀ θ ∈ Θ

8: end for

9:

rθi,kk ← rθi,kk/γαk

10: return rθi,kk

11: end function

Pull arm i

3.3 EpochUCB Algorithm Analysis

In this section, we analyze the regret of EpochUCB (Algorithm 2). At a high level, EpochUCB plays the arm that maximizes the sum of the empirical mean reward and the conﬁdence window at each epoch for a time period that grows linearly as a function of the number of times the arm selection has been chosen in the past. More formally, for each arm j ∈ [m], deﬁne the empirical mean reward after k − 1 epochs to be

R¯j,Tjα(k−1) = Tjα(1k−1) Ti=jα1(k−1) Rjθ,i,

and the conﬁdence window at epoch k ∈ [n] to be

cj,k(Tjα(k − 1)) = LγjT(jTαjα(k(−k−1)1)) +

6 log(k)
α

.

Tj (k−1)

(16)

Following an initialization round in which each arm is played once, the algorithm selects the arm α(k) at epoch k ∈ [n] such that:

α(k) = arg maxj∈[m] R¯j,Tjα(k−1) + cj,k(Tjα(k − 1)).

(17)

Our algorithm bears conceptual similarity to the UCB2 algorithm introduced in Auer et al. (2002a). However, the crucial ingredients in our method are the careful choice of the conﬁdence window that captures multiple sources of uncertainty, and the way we exploit the linearly increasing epoch length sequence to ensure that the window of uncertainty is in fact diminishing after multiple plays of an arm. The following theorem provides an upper bound on the number of times any suboptimal arm will be played by the EpochUCB algorithm.

Theorem 1 (Bound on Suboptimal Plays for EpochUCB). Suppose Assumptions 1 and 2
hold. Let α be the EpochUCB algorithm with corresponding epoch length sequence {τkα}nk=1 as given in (5). Then, for each suboptimal arm j ∈ [m],

Eα[Tjα(n)]

≤

4 ∆2

ργj +

6 log(n) 2 + 3 + 2 log(n),

j

18

Bandits for Correlated Markovian Environments

Algorithm 2 EpochUCB

1: procedure EpochUCB(τ0, ζ, γ)

2: t1 ← 0, Tj ← 1 & R¯j,Tj ← 0 ∀ j ∈ [m]

3: for 1 ≤ k ≤ m do

4:

R¯j,Tj ← pullarm(k, k, γ, tk, τ0)

5:

tk+1 ← tk + τ0

6: end for

7: while k > m do

8:

i = arg maxj∈[m] R¯j,Tj + cj,k(Tj )

9:

τkα ← τ0 + ζTi

10:

rθi,kk ← pullarm(i, k, γ, tk, τkα)

11:

R¯i,Ti+1 ← R¯i,Ti + (rθi,kk − R¯i,Ti )/Ti

12:

Ti ← Ti + 1, tk+1 ← tk + τkα, k ← k + 1

13: end while

14: end procedure

Pull each arm once Algorithm 1 Equation 5
EpochUCB Equation 17
Equation 5 Algorithm 1

where ργj is a time-invariant constant deﬁned as follows depending on the type of reward feedback:
1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

 Cj

(φj )τ0

1,

ργ =

φj −ηj 1−(φj )ζ

ζ

j Cj (φj )τ0−1 1−(τφ0j )ζ + (1−ζ((φφjj))ζ )2 ,

if γ = λj ∀ j ∈ [m] .
otherwise

(18)

2. Time-Averaged Reward Feedback: γ = 1.

ργj = √ζτ0C(1j−λj ) 1 + τζ0 .

(19)

The complete proof can be found in Appendix B.4. Proof (sketch.) The intuition behind the proof is that the algorithm can play a suboptimal arm when: (i ) the conﬁdence bounds on the stationary distribution rewards for the optimal arm or a suboptimal arm fail, or (ii ) the optimal arm and a suboptimal arm have been sampled insuﬃciently to distinguish between the respective stationary distribution rewards. The crux of the proof is the derivation of the conﬁdence window, which must capture the maximum amount of uncertainty in the observed rewards. Since Lemma 2 provides an upper bound on the uncertainty from the dynamic nature of the reward distributions, we are only left to characterize the uncertainty from the stochasticity in the rewards.
We previously formulated the arm-based reward observations as a Martingale diﬀerence sequence so that we can apply the Azuma-Hoeﬃng inequality (Proposition 3) to bound the uncertainty arising from the stochasticity in the rewards. Toward this end, suppose an arm j ∈ [m] has been played Tj times prior to an epoch k ∈ [n]. We need to relate the martingale Yj,Tj as deﬁned in (15) to the deviation between the empirical mean reward and the expected stationary distribution reward for the arm, so that we can derive a bound on this quantity of interest and obtain our conﬁdence window.
19

Fiez, Sekar, and Ratliff

To do so, deﬁne the event ω = {µj − R¯j,Tj ≥ } for some arbitrary equivalently express this event as

> 0. We can

ω= =

µj − T1j µj − T1j

Ti=j 1 E[Rjθ,i|Fj,i−1] + T1j

Tj i=1

E[Rjθ,i|Fj,i−1]

−

R¯j,Tj

≥

Tj i=1

E[Rjθ,i|Fj,i−1]

−

Yj,Tj T

≥

.

j

This representation is obtained from adding and subtracting T1j Ti=j 1 E[Rjθ,i|Fj,i−1] into the event ω and invoking the deﬁnition of Yj,Tj found in (15). From Lemma 2, we obtain

ω⊂

Lγj (Tj ) − Yj,Tj ≥

Tj

Tj

= Yj,Tj ≤ Lγj (Tj ) − .

Tj

Tj

Hence, applying Proposition 3,

P (µj − R¯j,Tj ≥ ) ≤ P YjT,jTj ≤ LγjT(jTj ) −

≤ exp − T2j

− Lγj (Tj ) 2 .
Tj

Deﬁning =

2 log 1 + Lγj (Tj) , we determine that for any ﬁxed δ > 0,

Tj

δ

Tj

P µj − R¯j,Tj ≥

2 log 1 + Lγj (Tj) ≤ δ.

Tj

δ

Tj

Selecting δ(k) = k−3, we recover the deﬁnition of the conﬁdence window found in (16) and get the ensuing conﬁdence bounds on the expected stationary distribution reward of an arm j ∈ [m] that hold with growing probability as the algorithm proceeds:

P (µj ≤ R¯j,Tj + cj,k(Tj)) ≥ 1 − k−3 and P (µj ≥ R¯j,Tj − cj,k(Tj)) ≥ 1 − k−3.

Since the probability of the conﬁdence bounds failing diminishes as the algorithm progresses, we can show that the number of times a suboptimal arm is played as an artifact of the conﬁdence bounds failing grows only logarithmically in the time horizon. The details of this argument are found in our full proof.
When the conﬁdence bounds hold, a suboptimal arm can be only played when the optimal arm and a suboptimal arm have been sampled insuﬃciently to distinguish between the respective stationary distribution rewards. To distinguish between the stationary distribution rewards of the optimal arm j∗ and that of a suboptimal arm j, it is adequate to ﬁnd the smallest integer representing the number of samples of the arm, such that

Lγ ( )
µ∗ − µj − 2cj,k( ) = ∆j − 2 j +

6 log(k) > 0

holds for every epoch k ∈ [n]. Indeed, we do so for each function Lγj ( ) can adopt dependent on the type of reward feedback, and show that the number of times each suboptimal arm
must be sampled to identify the optimal arm grows only logarithmically in the time horizon.
This component of the proof is is detailed in our full proof.

The following corollary is a direct consequence of Proposition 2 and Theorem 1. 20

Bandits for Correlated Markovian Environments
Corollary 1 (Gap-Dependent Regret Bound for EpochUCB). Under the assumptions of Theorem 1,
REpochUCB(n) ≤ j=j∗ ∆4j ργj + 6 log(n) 2 + 3∆j + 2 log(n)∆j + j∈[m] Lγj (n).
This gap-dependent regret bound is O(log(n)). Thus, although our problem is more general than many other bandit problems considered previously, we still obtain an gapdependent regret bound of the same asymptotic gap-dependent order and inverse dependence on the gaps.
The following corollary gives a gap-independent regret bound for EpochUCB of order O( n log(n)).
Corollary 2 (Gap-Independent Regret Bound for EpochUCB). Under the assumptions of Theorem 1,
REpochUCB(n) ≤ n j∈[m] 4(ργj )2 + 8ργj 6 log(n) + 26 log(n) + 2 + j∈[m] Lγj (n).
The proof of Corollary 2 follows from bounding j=j∗ Eα Tjα(n) ∆j using the CauchySchwarz inequality, Theorem 1, and ∆j being bounded in [0, 1]. We defer the full proof to Appendix B.5.
3.3.1 Discussion of Regret Bounds
The regret bounds we present have an intuitive and necessary dependence on the Markov chain statistics (Cj, λj) for each arm j ∈ [m]. Such a dependence is commonly found in regret bounds for bandit problems with Markov chains. In fact, we are unaware of any papers in the related rested and restless multi-armed bandit literature (e.g., see the work of Tekin and Liu, 2012, and the references therein) that do not have a similar dependence. Indeed, the statistics (Cj, λj) are directly tied to the mixing times of the Markov chains characterized by the transition matrix for each arm j ∈ [m], and the regret bounds that can be obtained necessarily depend on the mixing times.
Recall from our regret decomposition that there is a penalty for selecting suboptimal arms and for the state distribution deviating from a stationary distribution. Clearly, the regret deriving from the state distribution converging to the stationary distribution of an arm must scale proportionally to the mixing time of that Markov chain. Similarly, when arms converge toward their respective stationary distributions promptly, the observed reward feedback can be exploited early on to conﬁdently identify the expected stationary distribution reward of each arm. Conversely, as the mixing times of the Markov chains grow, any learning algorithm would require many samples to conﬁdently estimate the expected stationary distribution reward of each arm since the feedback observed early on cannot be ensured to closely approximate rewards drawn from the stationary distributions of the arms. Hence, the dependence on the Markov chain statistics in our regret bounds should be seen as capturing a natural measure of instance-dependent complexity stemming from the Markov chains that complements the standard measure of instance-dependent complexity characterized by the reward gap.
21

Fiez, Sekar, and Ratliff

Finally, we remark that there exists large classes of transition matrices P for which the second largest eigenvalue of the multiplicative reversiblization λ2(M (P )) is bounded away from 1. Moreover, a signiﬁcant body of work that has identiﬁed suﬃcient conditions for such instances (e.g., see Corollary 2.2 in Kirkland, 2009). In practice, we also ﬁnd that λ2(M (P )) is bounded away from 1. For example, if we sample a transition matrix P with 10 states from uniform and standard normal distributions (subject to normalization) 1000 times each, we ﬁnd λ2(M (P )) has respective means of 0.32 and 0.7 with 95th percentiles of 0.38 and 0.83. Thus, for the vast majority of problem instances the size of the constant factors in our regret bounds tied to the Markov chain statistics will be reasonable.

3.4 EpochGreedy Algorithm Analysis

In this section, we analyze the regret of EpochGreedy (Algorithm 3). The EpochGreedy
algorithm is a simple bandit policy that plays the arm with maximum empirical mean reward with probability 1 − εk and an arm selected uniformly at random with probability εk at an epoch k ∈ [n]. Formally, the policy at epoch k ∈ [n] with α taken as the EpochGreedy algorithm is

arg maxj∈[m] R¯j,T α(k−1), w.p. 1 − εk

α(k) =

j

.

(20)

j,

w.p.

εk m

∀

j

∈ [m]

Algorithm 3 EpochGreedy

1: procedure EpochGreedy(τ0, ζ, γ, c, d) 2: t1 ← 0, Tj ← 0 & R¯j,Tj ← 0 ∀ j ∈ [m], k ← 1

c ≥ c ν2, 0 < d ≤ ∆min

3: while k > 0 do

EpochGreedy

4:

ik = arg maxj R¯j,Tj

Find arm with maximum empirical mean reward

5:

εk

=

min{1,

cm d2k

}

Random Exploration Probability

6:

If rand(·) ≥ εk: i ← ik else: i ← randint(m)

Equation 20

7:

τkα ← τ0 + ζTi

8:

rθi,kk ← pullarm(i, k, γ, tk, τkα)

Equation 5 Algorithm 1

9:

R¯i,Ti+1 ← R¯i,Ti + (rθi,kk − R¯i,Ti )/Ti

10:

Ti ← Ti + 1, tk+1 ← tk + τkα, k ← k + 1

11: end while

12: end procedure

EpochGreedy is a variant of the ε–greedy algorithm presented in Auer et al. (2002a). The novelty in our algorithm and analysis is the construction of the random exploration probability sequence {εk}nk=1 to suit our problem and obtain strong regret guarantees. The challenge in devising this sequence and analyzing the resulting algorithm stems from the need to bear in mind the multiple sources of uncertainty present in our problem. Consequently, critical tools in our analysis include our technique to quantify the convergence of the expected mean reward of each arm to an expected stationary distribution reward (Lemma 2) and our formulation of the observed reward feedback on each arm as a Martingale diﬀerence sequence to conform with the Azuma-Hoeﬀding inequality (Proposition 3).

22

Bandits for Correlated Markovian Environments

The random exploration probability sequence {εk}nk=1 in our algorithm decays linearly as a function of the time horizon—just as is the case in the ε–greedy algorithm of Auer

et al. (2002a). However, a fundamental distinction of the random exploration probability

sequence we construct is the dependence of the constant factors found in the sequence on

the Markov chain statistics (Cj, λj) for each arm j ∈ [m]. This dependence shows up

since the Markov chain statistics act as a measure of instance-dependent complexity that

complements the standard measure of instance-dependent complexity characterized by the

reward gap. For a detailed discussion on this point, refer back to Section 3.3.1.

Toward formally posing the random exploration probability sequence we construct, ﬁx

constants c and d such that 0 ≤ d ≤ ∆min wher√e ∆min = minj=j∗ ∆j and c ≥ c ν2 where c > 8, ν ≥ max{κ, √dc }, and κ = min{κ > 0 : κ i ≥ Lγj (i) ∀ i ∈ [n], j ∈ [m]}. Recall that Lγj (·) depends on the Markov chain statistics (Cj, λj) and is deﬁned in (13) for discount-

averaged reward feedback and in (14) for time-averaged reward feedback. Finally, deﬁne

{εk }nk=1

to

be

a

sequence

with

εk

=

min{1,

cm d2k

}

for

each

epoch

k

∈

[n]

so

that

for

k

≥

cm d2

,

εk =

cm d2k

.

The following theorem provides an upper bound on the probability that any

suboptimal arm j ∈ [m] will be played by the EpochGreedy algorithm at an epoch k ≥

cm d2

.

Theorem 2 (Bound on Probability of EpochGreedy Playing a Suboptimal Arm). Suppose

Assumptions 1 and 2 hold. Fix constant c such that c ≥ c ν2 where c > 8, ν = max{κ, √d },

and

κ

=

min{κ

>

0

:

√ κi

≥

Lγ (i)

∀

i

∈

[n], j

∈

[m]}.

Deﬁne c

= (4c )(

c
c /2 − 2)−2.

j

Moreover, ﬁx constant d such that 0 ≤ d ≤ ∆min where ∆min = minj=j∗ ∆j. Let α be

the EpochGreedy algorithm with epoch length sequence {τk}nk=1 as given in (5) and random

exploration

probability

sequence

{εk }nk=1

where

εk

=

min{1,

cm d2k

}

for

each

epoch

k

∈

[n].

Then, at any epoch k ≥

cm d2

and for each suboptimal arm j ∈ [m],

P (α(k)

=

j)

≤

c d2k

+

+

d22c log (k−1)d2cmexp(1/2)

cm (k−1)d2 exp(1/2)

2c exp(1) d2

(k−1)d2cmexp(1/2) c/c .

c/(5d2)

The complete proof can be found in Appendix B.6.

Proof (sketch.) The proof hinges on the fact that the algorithm can play a suboptimal arm

as a result of random exploration or when the empirical mean reward of a suboptimal arm

is greater than the empirical mean reward of the optimal arm. Indeed, the probability that

a suboptimal arm j ∈ [m] is chosen at epoch k ≥

cm d2

is given by

P (α(k) = j) = εmk + (1 − εk)P (R¯j,Tjα(k−1) ≥ R¯∗,T∗α(k−1)),

which we bound as

P (α(k) = j) ≤ d2ck + P (R¯j,Tjα(k−1) ≥ R¯∗,T∗α(k−1)).

(21)

Moreover,

P (R¯j,T α(k) ≥ R¯∗,T α(k)) ≤ P

R¯j,T α(k)

≥

µj

+

∆j 2

+P

R¯∗,T α(k)

≤

µ∗

−

∆j 2

.

j

∗

j

∗

23

Fiez, Sekar, and Ratliff

Deﬁning x0 = 21 (

k i=1

mεi )

and

using

techniques

found

in

Auer

et

al.

(2002a),

we

can

show

P R¯j,Tjα(k) ≥ µj + ∆2j

x0

≤

P

Tjα(k)

=

i

R¯j,i

≥

µj

+

∆j 2

i=1

k

+

P

i= x0 +1

≤ x0 exp −5x0 + ki= x0 +1 P R¯j,i ≥ µj + ∆2j .

R¯j,i

≥

µj

+

∆j 2

(22)

We can also conclude that x0 ≥ 2dc2 and x0 ≥ dc2 log( d2k ecxmp(1/2) ). In this proof sketch, we focus on bounding the sum present in (22) since this is where
our analysis deviates from that found in Auer et al. (2002a) most signiﬁcantly. To do so, consider the event ω = {R¯j,i − µj ≥ ∆2j }, which we can express as

ω= =

R¯j,i

−

1 i

Yji,i + 1i

i l=1

E[Rjθ,l|Fj,l−1]

+

1 i

i l=1

E[Rjθ,l|Fj,l−1]

−

µj

≥

∆j 2

i l=1

E[Rjθ,l|Fj,l−1]

−

µj

≥

∆j 2

,

when we add and subtract the random variable 1i

i l=1

E[Rjθ,l

|Fj,l−1]

into

the

event

ω

and

invoke the deﬁnition of Yj,i from (15). From Lemma 2, we obtain

ω⊂

Yj,i + Lγj (i) ≥ ∆j .

i

i

2

(23)

Moreover, 2dc2 ≥ 2∆c ν2m2in since by construction c ≥ c ν2 and 0 ≤ d ≤ ∆min, so that x0 ≥ 2∆c ν2m2in .

This implies

√∆j ≥ √∆min ≥ √ν ≥ Lγj (x0) .

c /2

c /2

x0

x0

Hence, √∆cj/2 ≥ Lγji(i) for all i ≥ x0 + 1 so that, recalling (23),

γ

√

ω ⊂ Yji,i + Lji(i) ≥ ∆2j ⊂ Yji,i ≥ ∆2j − √∆cj/2 = Yji,i ≥ ∆j ( √c2c/2−2) .

Now, we apply the Azuma–Hoeﬀding inequality from Proposition 3 to ω to get that

P

R¯j,i

≥

µj

+

∆j 2

≤P

√ Yji,i ≥ ∆j ( √c2c/2−2)

≤ exp

√ − 2i ∆j ( √c2c/2−2) 2

= exp

− ic∆2j .

Finally, we can show that

k

exp − i∆2j ≤ c exp − ∆2j x0 . Plugging this bound

i= x0 +1

c

∆2j

c

into (22) and then relating that bound back to (21) yields our ﬁnal result.

Remark 7. Theorem 2 gives a bound on the probability of a suboptimal arm being selected

at an epoch k ≥

cm d2

that is equivalent for discount-averaged reward feedback and time-

averaged reward feedback. However, the type of reward feedback does impact the bound in

Theorem 2 as a consequence of the size of the constant c, which derives from the constant κ that depends on Lγj (·), and hence, the type of reward feedback. While the bound for each
type of reward feedback is of identical order, when the constant c grows with the constant κ through Lγj (·) it is likely that in practice the empirical performance will degrade for benign
problem instances since the algorithm will explore for a longer period of time. Refer back to Remark 5 for a discussion of how Lγj (·) depends on the discount factor and the type of
reward feedback.

24

Bandits for Correlated Markovian Environments

Theorem 2 gives an instantaneous bound on the probability of selecting a suboptimal

arm. To obtain a bound on the number of times a suboptimal arm will be played, we can

sum the probability of the arm being played at each epoch k ∈ [n]. Observe that when

c

>

c

,

for

k

∈

[n]

suﬃciently

large

enough,

p(α(k)

=

j)

≤

c d2k

+ o( k1 )

for

any

suboptimal

arm j ∈ [m]. Hence, Eα[Tjα(n)] =

n k=1

p(α(k)

=

j)

≤

c d2

O

(log

(n

)).

This

implies

that

when

we relate back to the regret proposition from Proposition 2, we can obtain a gap-dependent

regret bound.

Corollary 3 (Gap-Dependent Regret Bound for EpochGreedy). Under the assumptions of

Theorem 2,

REpochGreedy(n)

≤

O(log(n))

c d2

j=j∗ ∆j +

j∈[m] Lγj (n).

Remark 8. The gap-dependent regret bound of EpochGreedy is of equal asymptotic order as the gap-dependent regret bound we obtained for EpochUCB in Corollary 1. However, as opposed to having a dependence on the reward gaps of the form j=j∗ ∆−j 1 as in the regret bound for EpochUCB, the dependence on the reward gaps found in the regret bound for EpochGreedy is of the form d−2 j=j∗ ∆j, which can be signiﬁcantly worse for certain problem instances. Moreover, Theorem 2 relies on knowledge of a lower bound on the minimum reward gap given by the constant d that would typically not be available a priori. That being said, EpochGreedy is simple and near-greedy approaches often perform well in practice.
4. Experiments
In this section, we present a set of illustrative experiments to enhance our theoretical results. To begin, we describe the class of problem instances we run our experiments on. Following this, we show theoretical and empirical regret comparisons between EpochUCB and EpochGreedy and examine each quantity as a function of the discount factor that governs the reward feedback. Finally, we compare the empirical performance of EpochUCB and EpochGreedy with a variety of alternative algorithms existing in the literature.
Before doing so however, it is worth revisiting Example 1 to understand why popular bandit approaches fail to achieve good performance in a correlated Markovian environment, and to elucidate how our approach overcomes the issues plaguing conventional methods.
Revisiting Example 1: Why do EpochUCB and EpochGreedy work? First, suppose τ0, ζ = 1. Then, EpochUCB and EpochGreedy initially estimate the empirical mean reward of arm 1 to be zero in Example 1. However, during the exploration phase, arm 1 is played again for an epoch length > 1 and the state transitions from θ1 to θ2 within the epoch. Therefore, the observed reward feedback tends closer to stationary distribution reward, and the empirical mean reward for arm 1 increases. This continues as the epoch length increases so that eventually the empirical mean reward for arm 1 exceeds that of arm 2 and each algorithm correctly identiﬁes arm 1 as the optimal arm prior to moving from exploration to exploitation. We can recall that UCB and ε–greedy fell victim to under-estimating the stationary distribution reward of the optimal arm since the state distribution was never allowed to converge toward the stationary distribution. Eﬀectively, UCB and ε–greedy were estimating the stationary distribution reward of the optimal arm based on a reward
25

Fiez, Sekar, and Ratliff

Stationary Distributions 0.8 0.6 0.4

Arm 1 Arm 2 Arm 3 Arm 4

Mean Reward Distributions 0.8 0.6 0.4

Arm 1 Arm 2 Arm 3 Arm 4

Probability Reward

0.2

0.2

0.0 1

2

3

4

State Indexes

(a)

0.0 1

2

3

4

State Indexes

(b)

Figure 2: (a) Stationary distributions indexed by the state for each arm in a sample 4-arm, 4-state instance; (b) Mean of the reward distributions indexed by the state for each arm for the same instance as (a).

distribution, which was arising from a state distribution deviating signiﬁcantly from the stationary distribution, that had much worse expected reward.
4.1 Problem Generation
For our experiments, we consider a class of problem instances exhibiting interesting structures, i.e., the arms are suﬃciently unique but strongly anti-correlated. We believe this to be reasonably representative of actual instances where an agent has vastly diﬀerent preferences depending the underlying state and the presented arm. We generate the problem instances with the following criteria on the transition matrices and the reward distributions.
Transition Matrices: We consider a scenario in which the transition matrix for the optimal arm is such that the state is constrained to be in a subset of the state space with high probability. Moreover, the transition matrices of each suboptimal arm are such that the state enters this subset of the state space with low probability. An illustration of the stationary distribution for a sample instance having this property is given in Figure 2a. Each transition matrix in our experiments was inspected to ensure Assumptions 1 and 2 were satisﬁed.
Reward Distributions: The reward distribution for each arm, state pair is randomly chosen to be a beta, Bernoulli, or uniform distribution. The mean of each distribution is randomly selected with the caveat that the means are increasing in the stationary distribution probability of the states. Figure 2b shows the mean reward for each reward distribution corresponding to an arm, state pair for a sample instance.
4.2 EpochUCB vs. EpochGreedy
We now compare the theoretical gap-dependent regret bounds and empirical performance of EpochUCB and EpochGreedy. In this section, we set τ0 = 40, ζ = 1 and for EpochGreedy let the constant c = c ν2 where c was selected to minimize the cumulative regret subject to the constraint c > 8. Moreover, for each simulation we present the mean results over
26

Bandits for Correlated Markovian Environments

Cumulative Regret Cumulative Regret Cumulative Regret

Cumulative Regret Bound

8000 6000 4000 2000
0 0

EpochUCB EpochGreedy
25000 50000 75000 100000 Epochs
(a)

Cumulative Regret Bound

4000 3000 2000 1000
0 0

γ =1.00 γ =0.90 γ =0.89 γ =0.88 γ =0.85
25000 50000 75000 100000 Epochs
(b)

Cumulative Regret Bound

8000 6000 4000 2000
0 0

γ =1.00 γ =0.90 γ =0.89 γ =0.88 γ =0.85
25000 50000 75000 100000 Epochs
(c)

Figure 3: (a) Mean theoretical gap-dependent regret bounds under time-averaged reward feedback for EpochUCB and EpochGreedy over 5 problem instances from the class described in Section 4.1 with 4 arms and 4 states; (b–c) Mean theoretical gap-dependent regret bounds for EpochUCB (Figure 3b) and EpochGreedy (Figure 3c) under discount-averaged reward feedback over the problem instances in (a) with various discount factors.

5 problem instances sampled from the class we consider as described in Section 4.1 with 4 arms (m = 4) and 4 states (|Θ| = 4).
4.2.1 Theoretical Comparison
In Figure 3a we compare the theoretical gap-dependent regret bounds of EpochUCB and EpochGreedy under time-averaged reward feedback. As our theoretical results indicate, each regret bound grows logarithmically and EpochUCB’s regret bound is tighter than EpochGreedy’s owing to the superior dependence on the reward gaps and the sharper constants. In Figures 3b and 3c we examine the theoretical gap-dependent regret bounds of EpochUCB and EpochGreedy under discount-averaged reward feedback as a function of the discount factor on the rewards. For these problem instances, and as we would expect to be the case generally since the majority of weight is given to rewards as the state distribution tends closer to a stationary distribution (see Remark 5 for further discussion on this point), the gap-dependent theoretical regret decays as the discount factor decays until the improvement begins to saturate.
4.2.2 Empirical Comparison
In Figure 4a we compare the empirical performance of EpochUCB and EpochGreedy under time-averaged reward feedback and observe that EpochUCB signiﬁcantly outperforms EpochGreedy. However, in Figure 4b we show that EpochGreedy’s empirical performance improves dramatically and tends toward the empirical performance of EpochUCB as we decay the constant parameter c from that which was selected to minimize the theoretical regret. This phenomenon matches the empirical conclusions drawn with respect to the ε– greedy algorithm in work of Auer et al. (2002a). In Figures 5a and 5b, we examine the
27

Fiez, Sekar, and Ratliff

Cumulative Regret Cumulative Regret

5000 4000 3000 2000 1000
0 0

Cumulative Regret
EpochUCB EpochGreedy

20000 Epochs
(a)

40000

5000 4000 3000 2000 1000
0 0

Cumulative Regret

20000 Epochs

40000

(b)

c = c∗/1 c = c∗/2 c = c∗/4 c = c∗/8 c = c∗/16

Figure 4: (a) Mean empirical regret for EpochUCB and EpochGreedy under time-averaged reward feedback over the problem instances in Figure 3a; (b) Mean empirical regret for EpochGreedy under time-averaged reward feedback over the problem instances in (a) as the theoretically optimal constant c∗ to minimize the regret is decayed.

Cumulative Regret Cumulative Regret

600 400 200
0

Cumulative Regret

20000 Epochs
(a)

γ =1.00 γ =0.90 γ =0.89 γ =0.88 γ =0.85
40000

5000 4000 3000 2000 1000
0 0

Cumulative Regret

20000 Epochs

40000

(b)

γ =1.00 γ =0.90 γ =0.89 γ =0.88 γ =0.85

Figure 5: (a–b) Mean empirical regret for EpochUCB (Figure 5a) and EpochGreedy (Figure 5b) under discount-averaged reward feedback over the problem instances in Figure 3a for various discount factors.

empirical performance of EpochUCB and EpochGreedy under discount-averaged reward feedback as a function of the discount factor. Similar to as with the theoretical regret, we ﬁnd that the cumulative regret decays as a function of the discount factor.
4.3 Comparison to Existing Algorithms
To conclude our simulations, we compare the performance of EpochUCB and EpochGreedy with UCB and ε–greedy, as well as several other well-studied algorithms from the literature that we now detail.
28

Bandits for Correlated Markovian Environments

Cumulative Regret Cumulative Regret

4000 3000 2000 1000
0 0

Cumulative Regret

10000 20000 30000 Epochs
(a)

40000

EpochUCB EpochGreedy UCB UCB+ ε-Greedy EXP3 RL

5000 4000 3000 2000 1000
0 0

Cumulative Regret

10000 20000 30000 Epochs
(b)

40000

EpochUCB EpochGreedy UCB UCB+ ε-Greedy EXP3 RL

Figure 6: Mean empirical regret under time-averaged reward feedback for each of the algorithms over 5 problem instances from the class described in Section 4.1 with 4 arms and 4 states in (a) and 4 arms and 8 states in (b).

Variance-Tuned UCB: Following Auer et al. (2002a), we consider a tuned version of UCB where the conﬁdence windows is replaced with a conﬁdence window on the variance. We refer to this variant of UCB as UCB+. This variant is known to often perform well in practice for many instances.
EXP3: We compare our proposed algorithms to the EXP3 algorithm of Auer et al. (2002b) for adversarial bandits. EXP3 has been proven to omit sublinear regret against an oblivious adversary—meaning that an adversary can select the sequence of rewards with knowledge of the algorithm but this must be done a priori. However, since the reward feedback in the problem we study depends on the history of actions, our model can be seen as a type of adaptive adversary and therefore these bounds do not necessarily hold for the problem being considered. In fact, the regret can be linear for EXP3 against an adaptive adversary (Dekel et al., 2012).
Continuous Reinforcement Learning (RL): We compare to Q-learning with linear function approximation as expressed in Melo et al. (2008) and Geramifard et al. (2013). We allow the reinforcement learning algorithm to observe the state distribution prior to each decision point and use the state distribution as the features of the linear model. Hence, the RL algorithm has access to a form of partial state observation that is not available to the bandit algorithms since the state is drawn from the observed state distribution. Linear function approximation is a natural choice given that the expected stationary distribution reward is a linear combination of the stationary distribution rewards and the stationary distribution. For the simulations, we decay the rando√mness of the ε–greedy policy exponentially and for the weight updates use a step size of 1/ k to satisfy the conditions of Robbins and Monro (1985). For further details on the algorithm and implementation, see Appendix C.
In this section, we again set τ0 = 40, ζ = 1, but motived by Figure 4b we tune the constant c for EpochGreedy to yield favorable empirical performance. We show the mean performance of each of the algorithms under time-averaged reward feedback over 5 problem instances sampled from the class we consider as described in Section 4.1 with 4 arms and 4 states in Figure 6a and 4 arms and 8 states in Figure 6b. To allow for a fair comparison,
29

Fiez, Sekar, and Ratliff
we run each algorithm for equal number of total iterations and convert the regret from the algorithms that select actions at each iteration to epochs. As expected, EpochUCB and EpochGreedy demonstrate the sublinear convergence rates we proved. On the other hand, each of the algorithms we compare to performs poorly and suﬀers linear regret. Given Example 1, this is unsurprising for UCB, UCB+, and ε–greedy. For Exp3, we attribute the poor performance to the adaptive nature of the rewards, as it is known that EXP3 has linear regret against an adaptive adversary (Dekel et al., 2012), e.g., when the reward in each round depends on the actions selected in previous rounds. Furthermore, although the RL algorithm has more information than the bandit algorithms, its performance is also poor. This is partly due to the continuous state space, the fact that the algorithm only has access to a distribution over states and not the exact state, and that it switches between actions too rapidly.
Finally, in settings that do not have strong correlation, EpochUCB and EpochGreedy may converge slower than some of the alternatives. However, as opposed to the algorithms we compare to, EpochUCB and EpochGreedy will always perform adequately asymptotically, meaning that the optimal arm will still be identiﬁed correctly.
5. Conclusion and Future Work
We study a multi-armed bandit problem in which a decision-maker repeatedly faces an agent with an unobserved state variable that inﬂuences the reward distribution on each arm and this state variable evolves with a Markov chain whose transition matrix depends on the action of the decision-maker. This work is motivated by applications pertaining to interactions between a decision-maker and an agent, a general example being digital platforms that actively engage with users, where the agent’s underlying state is not static. The novelty and technical challenges in this problem formulation stem from the decisionmaker obtaining no information about the state or distribution of the Markov chain and the fact that observed rewards are correlated with past actions. Despite the generality of the problem formulation, we developed algorithms called EpochUCB and EpochGreedy with sublinear regret guarantees. Concretely, we proved O(log(n)) gap-dependent regret bounds for each of our proposed algorithms as well as an O( n log(n)) gap-independent regret bound for EpochUCB. Moreover, our simulations empirically validate our methods and demonstrate the insuﬃciency of existing bandit and reinforcement learning algorithms for the problem at hand.
As is standard in bandit problems with Markovian rewards, we focus on the weak regret measure. An interesting and challenging question for future work is whether it is possible to obtain sublinear regret guarantees under stronger notions of regret. A clear example is when the benchmark policy being compared to is the optimal policy within the class of state dependent policies. However, it may be of interest to pursue dynamic regret measures that could be more attainable. A well-known dynamic regret measure is that considered in Garivier and Moulines (2011). This regret measure, proposed for bandit problems where the reward distributions can change abruptly, compares against the arm that yields the maximum reward at each time along the horizon. Unfortunately, there is not a clear translation of this regret measure to the problem we study since the optimal arm at each time depends on the history of arm selections and state variables, and hence, is not ﬁxed a priori.
30

Bandits for Correlated Markovian Environments
A promising direction in identifying suitable dynamic regret measures in bandit problems with Markovian rewards is the dynamic path-dependent regret measure proposed in Cortes et al. (2017) for rested bandit problems. This regret measure compares at each time index to the optimal action given the history of actions and feedback until that time index. An adaptation of this concept to the problem formulation we consider is worth exploring.
Finally, given that we derived a general framework for analyzing the regret of any multi-armed bandit policy interacting with a correlated Markovian environment in which the observed feedback is a smoothed reward over an epoch, there is potential to extend alternative existing algorithms from the bandit literature to the problem formulation of this paper. Of potential interest may be algorithms with weaker theoretical guarantees, but which are known to perform well in practice, such as variance-tuned algorithms.
Acknowledgments
This work is supported by National Science Foundation (NSF). Tanner Fiez was also supported in part by a National Defense Science and Engineering Graduate (NDSEG) Research Fellowship.
31

Fiez, Sekar, and Ratliff

Appendix A. Notation Table
This appendix contains the most important and frequently used notation in the paper.

Notation
[m]
[n]
α(k) Tjα(n) Rα(n)
τ0, ζ τkα τjα,k τji tk tij θtk
θk βtk πj Pj Pjτkα (θ, θ ) γ
γ αk Tr(θ, j) rjθ rjθ,tt rθj,kk Rjθ,i R¯j,Tj
µj ∆j, ∆min, d Fj,i
Yj,Tj λj, ηj, φj, ψj Cj υjγ(·), Lγj (·), ργj
cj,k (·) εk c, c , c
ν, κ

Meaning

Set of arms: {1, . . . , m} Epoch horizon: {1, . . . , n}

Arm pulled at epoch k under policy α

Number of times arm j is pulled in n epochs under policy α

Cumulative regret after n epochs under policy α

Initial epoch size and linear growth term in epoch length sequence
# of iterations in epoch k under policy α: τ0 + ζTαα(k)(k − 1) # of iterations in epoch k when α(k) = j: τ0 + ζTjα(k − 1) # of iterations in the epoch arm j is pulled for i–th time: τ0 + ζ(i − 1)

Iteration index at the start of epoch k

Iteration index at the start of the epoch arm j is pulled for i–th time

State variable at iteration tk Sequence of state variables in epoch k: {θt}tt=k+tk1−1 State distribution at iteration tk
Unique and positive stationary distribution of arm j

Transition probability matrix for arm j

Probability of moving from θ to θ in τkα iterations when arm j is pulled Discount factor on rewards ∈ (0, 1] Sum of discount factors in epoch k under policy α: tt=k+tk1−1(γ)tk+1−1−t Stochastic reward kernel for state θ and arm j

Stationary reward of arm j in state θ

Stochastic reward of arm j at iteration t drawn from Tr(θt, j)

Reward in epoch k for arm j:

1
α

γk

tt=k+tk1 −1 (γ )tk+1 −1−t rjθ,tt

Reward in the epoch arm j is pulled for the i–th time

Mean epoch rewards for arm j after Tj selections: T1j Ti=j 1 Rjθ,i Expected stationary distribution reward for arm j: E[ θ∈Θ rjθπj(θ)]

∆j = µ∗ − µj, ∆min = minj∈[m] ∆j, 0 ≤ d ≤ ∆min

Smallest σ-algebra generated by (Rjθ,1, . . . , Rjθ,i, θt1, . . . , θti )

j

j

Martingale for arm j pulled Tj times: see (15)

(λ2(M (Pj)))1/2, ηj = min{γ, λj}, φj = max{γ, λj}, and ψj = ηj/φj

1/2(1 + (1 − minθ πj(θ))2/ minθ πj(θ))1/2

Discount-averaged reward feedback: see (7), (13), (18)

Time-averaged reward feedback: see (8), (14), (19)

EpochUCB conﬁdence window for arm j at epoch k: see (16)

EpochGreedy

random

exploration

probability

in

epoch

k:

min{1,

cm d2k

}

c ≥ c ν2, c > 8, c = 4c ( c /2 − 2)√−2 ν = max{κ, √d }, κ = min{κ > 0 : κ i ≥ Lγj (i) ∀ i ∈ [n], j ∈ [m]}

c

32

Bandits for Correlated Markovian Environments

Appendix B. Proofs
This appendix contains proofs that were not included in the main body of the paper.

B.1 Proof of Harmonic Bound Proof (Proof of Harmonic Bound ). For any a > 0 and positive integer n, we have that

a+n 1 1

n

i ≤ a + log 1 + a . (24)

i=a

Indeed, rewrite the summation in (24) as

a+n 1 1 a+n 1 =+
i=a i a i=a+1 i

and apply the fundamental inequality (i)−1 ≤ ii−1(x)−1dx, which holds for any i > 1 and is simply a consequence of (i)−1 being a decreasing function, repeatedly for i = a + 1, a + 2, . . . , a + n so that we have a telescoping summation of integrals—i.e.,

a+n 1 1 a+n 1 1

a+n 1

1

n

=+

≤+

dx = + log 1 + .

i=a i a i=a+1 i a a x

a

a

Thus, for τ0 and ζ in Z+,

n

1

1 τ0/ζ+n−1 1 1 1 τ0/ζ+n−1 1

τ0 + ζ(i − 1) = ζ i = τ0 + ζ i

i=1

i=τ0/ζ

i=τ0 /ζ +1

and

1 1 τ0/ζ+n−1 1 1 1 τ0/ζ+n−1 1

11

ζ(n − 1)

+

≤+

dx = + log 1 +

.

τ0 ζ i=τ0/ζ+1 i τ0 ζ τ0/ζ x τ0 ζ τ0

Finally, we upper bound the preceding result by replacing n − 1 with n to simplify our analysis in the proof of Theorem 1, which gives

n

1

11

ζn

τ0 + ζ(i − 1) ≤ τ0 + ζ log

1+ τ0

.

i=1

33

Fiez, Sekar, and Ratliff

B.2 Proof of Lemma 1
Proof (Proof of Lemma 1). Suppose α(k) = j ∈ [m] at epoch k ∈ [n] and thus the epoch contains τjα,k iterations. Then, noting that µj = θ rjθπj(θ), we have the following:

tk+1 −1

E rjθπj(θ) − 1

(γ)tk+1−1−trjθt Fj,T α(k)−1

θ γk t=tk j

tk+1 −1

≤1

(γ)tk+1−1−t |πj (θ) − βt(θ)|

γk t=tk θ

tk+1 −1

1 =

(γ )tk+1 −1−t

γk t=tk θ

πj(θ) − Pjt−tk (θ , θ)βtk (θ )
θ

tk+1 −1

1 =

(γ)tk+1−1−t πj (·) −

γk t=tk θ

Pjt−tk (θ , ·)βtk (θ ) 1

tk+1 −1

≤ Cj

(γ)tk+1−1−t(λj )t−tk .

γk t=tk

The ﬁrst inequality follows from the triangle inequality, the rewards being bounded in [0, 1], and Fubini’s theorem (Folland, 2007, Theorem 2.37). The second inequality is a direct application of Proposition 1. Now recall that we deﬁned the following constants for arm j:

ηj = min{γ, λj}, φj = max{γ, λj}, ψj = ηj/φj.

This gives

Cj tk+1−1(γ)tk+1−1−t(λj )t−tk = Cj (φj )τjα,k−1 tk+1−1(ψj )t−tk .

γk t=tk

γk t=tk

We can bound the equation overhead dependent on the discount factor γ. Case 1: γ ∈ (0, 1) and γ = λj.

Cj (φj )τjα,k−1 tk+1−1

t−t

Cj (φj )τjα,k−1(1 − (ψj )τjα,k )

(ψj) k =

. γk(1 − ψj)

γk t=tk

Case 2: γ ∈ (0, 1) and γ = λj.

Cj (φj )τjα,k−1 tk+1−1

t−t

Cj (φj )τjα,k−1τjα,k

(ψj) k =

γk

t=t

. γk

k

Case 3: γ = 1.

Cj (φj )τjα,k−1 tk+1−1 t−t Cj (1 − (λj )τjα,k ) (ψj) k = γ (1 − λ) .
γk t=tk k

34

Bandits for Correlated Markovian Environments

Combining each of the cases gives

E µj − rθj,kk Fj,Tjα(k)−1

≤ Cj υjγ (τjα,k) , γk

where υjγ(τjα,k) is deﬁned as follows depending on the type of reward feedback: 1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

 τα −1

τα

γα

 (φj ) j,k (1−(ψj ) j,k ) ,

υj (τj,k) =

(1−ψj )
α

(φj )τj,k−1τjα,k,

if γ = λj . otherwise

2. Time-Averaged Reward Feedback: γ = 1.
τα
υjγ (τjα,k) = 1−1(λ−jλ)jj,k .

B.3 Proof of Lemma 2

Proof (Proof of Lemma 2). We manipulate the expression and use the triangle inequality to convert the quantity to a sum of terms that can be bounded with Lemma 1 as follows:

Tj

Tj

µj − 1 Tj

E[Rθ |Fj,i−1] = 1

j,i

Tj

(µj − E[Rjθ,i|Fj,i−1])

i=1

i=1

Tj

1 =
Tj

E[µj

−

r

θ j,k

i

|F

j,i−

1

]

j

i=1

Tj

≤1 Tj

E[µj

−

r

θ j,k

i

|F

j,i−

1

]

j

i=1

Cj Tj υjγ (τji)

≤ Tj
i=1

γi ,
j

(25)

where kji is the epoch in which arm j is pulled for the i–th time and τji is number of iterations in that epoch. We now bound (25) for each function υjγ(τji) takes on dependent upon the discount factor γ.
Case 1: γ ∈ (0, 1) and γ = λj ∀ j ∈ [m].

Cj Tj υjγ (τji) Cj Tj (φj )τji−1(1 − (ψj )τji ) Cj

Tj i=1 γij = Tj i=1

γ

i j

(1

−

ψj

)

≤ Tj

1 1 − ψj

Tj (φj )τji−1 i=1 γij

≤ Cj Tj

1 1 − ψj

Tj
(φj )τji−1 = Cj i=1 Tj

1 1 − ψj

Tj
(φj )τ0+ζ(i−1)−1
i=1

= Cj Tj

(φj )τ0−1 1 − ψj

1 − (φj )ζTj 1 − (φj)ζ

= Cj Tj

(φj )τ0 φj − ηj

1 − (φj )ζTj 1 − (φj)ζ .

35

Fiez, Sekar, and Ratliff

Case 2: γ ∈ (0, 1) and γ = λj for some j ∈ [m].

Cj Tj υjγ (τji)

Cj Tj τji(φj )τji−1

Tj
Cj

τ i−1 i

Tj i=1 γij = Tj i=1 γij ≤ Tj i=1 (φj ) j τj

Tj
= Cj (φj )τ0+ζ(i−1)−1(τ0 + ζ(i − 1)) Tj i=1

= Cj (φj )τ0−1 Tj (φj )ζ(i−1)(τ0 + ζ(i − 1)) Tj i=1

Cj (φj )τ0−1 τ0 − (φj )ζTj (τ0 + ζTj ) ζ(φj )ζ (1 − (φj )ζTj )

= Tj

1 − (φj)ζ

+ (1 − (φj)ζ )2 .

The ﬁnal equality follows from recognizing that

Tj i=1

(φj

)ζ

(i−1)

(τ0

+ζ

(i−1))

is

an

arithmetico-

geometric series and then substituting the expression for the ﬁnite sum. An arithmetico-

geometric series has the following general form for constants |r| < 1 and a, d ∈ R, which can

equivalently be expressed in terms of the ﬁnite sum that is bounded by the inﬁnite sum:

n ri−1(a + d(i − 1)) = a − rn(a + dn) + dr(1 − rn) ≤ a +

dr .

1−r

(1 − r)2 1 − r (1 − r)2

i=1

Case 3: γ = 1.

Cj Tj υjγ (τji) Cj Tj (1 − (λj )τji ) Cj Tj i=1 γij = Tj i=1 γij (1 − λj ) ≤ Tj

1 1 − λj

= Cj Tj

1 1 − λj

Tj 1 i=1 τ0 + ζ(i − 1)

≤ Cj 1 Tj 1 − λj

1 + 1 log 1 + ζTj

τ0 ζ

τ0

Tj 1 i=1 γij
.

The ﬁnal inequality is a direct application of harmonic bound from Section B.1. Combining each of the cases gives

Tj

E µj − 1 Tj

E[Rjθ,i|Fj,i−1]

i=1

≤ Lγj (Tj) , Tj

where Lγj (Tj) is deﬁned as follows depending on the type of reward feedback: 1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

 Cj

(φj )τ0

Lγj (Tj) =

φj −ηj

11−−(φ(φj )jζ)Tζj ,
ζTj

ζ

ζTj

Cj (φj )τ0−1 τ0−(φj1)−(φj(τ)ζ0+ζTj ) + ζ(φj()1−(1(φ−j()φζj))2 ) ,

if γ = λj ∀ j ∈ [m] .
otherwise

2. Time-Averaged Reward Feedback: γ = 1. Lγj (n) = 1−Cλj j τ10 + ζ1 log 1 + ζτn0 .

36

Bandits for Correlated Markovian Environments

B.4 Proof of Theorem 1
Proof (Proof of Theorem 1.) The EpochUCB policy plays the arm with the maximum upper conﬁdence bound on the empirical mean reward at each epoch. The policy at an epoch k ∈ [n] with α taken as the EpochUCB algorithm is then

α(k) = arg max R¯j,T α(k−1) + cj,k(Tjα(k − 1)).

j∈[m]

j

Recall that we use the notation

Tjα(k−1)

R¯ α

=

1

Rθ

j,Tj (k−1) Tjα(k − 1)

j,i

i=1

and

α

Lγj (Tjα(k − 1))

6 log(k)

cj,k(Tj (k − 1)) = Tjα(k − 1) + Tjα(k − 1)

to respectively denote the empirical mean reward and conﬁdence window at epoch k ∈ [n] for arm j ∈ [m] when it has been pulled Tjα(k − 1) times prior to epoch k.
Given the above notation, we upper bound Tjα(n) for each arm j ∈ [m]. Note that we replace the arm index j with ∗ when referencing the optimal arm. For an arbitrary positive
integer , we have that

n

Tjα(n) = 1 +

I{α(k) = j}

≤+

k=m+1 n
I{α(k) = j, Tjα(k − 1) ≥ }

k=m+1

n

≤+

I{R¯∗,T α(k−1) + c∗,k(T∗α(k − 1)) ≤ R¯j,T α(k−1) + cj,k(Tjα(k − 1)), Tjα(k − 1) ≥ }

∗

j

k=m+1

n

≤+

I min R¯∗,s + c∗,k(s) ≤ max R¯j,wj + cj,k(wj)

k=m+1 0<s<k

≤wj <k

n k−1 k−1

≤+

I{R¯∗,s + c∗,k(s) ≤ R¯j,wj + cj,k(wj)}.

k=1 s=1 wj =

Now, suppose that all three of the following are false:

 R¯∗,s

≤ µ∗ − c∗,k(s)

(26)



 R¯j,wj

≥ µj + cj,k(wj)

(27)



 µ∗

< µj + 2cj,k(wj)

(28)

Then,

R¯∗,s + c∗,k(s) > µ∗ ≥ µj + 2cj,k(wj) > R¯j,wj + cj,k(wj).

37

Fiez, Sekar, and Ratliff

Hence, if R¯∗,s +c∗,k(s) ≤ R¯j,wj +cj,k(wj), then at least one of (26)–(28) holds. We bound the probability of events (26) and (27) using the Azuma-Hoeﬀding inequality given in Proposi-
tion 3 and ﬁnd a positive integer such that (28) is always false.
Toward this end, suppose an arm j has been played Tj times. We apply Proposition 3 to
the martingale (Yj,Tj )Tj∈Z+. Note that by the law of conditional expectations, E[Yj,Tj ] = 0 so that Proposition 3 implies that for each arm j ∈ [m] and any > 0, P (Yj,Tj ≤ − ) ≤ exp(− 2/(2Tj)). We need to relate the random variable Yj,Tj to the diﬀerence between the empirical mean reward and the expected stationary distribution reward for each arm so that we can bound this diﬀerence. Consider the event ω = {µj − R¯j,Tj ≥ }, and equivalently:

ω= =

Tj

Tj

µj − 1 Tj

E[Rθ |Fj,i−1] + 1

j,i

Tj

E[Rjθ,i|Fj,i−1] − R¯j,Tj ≥

i=1

i=1

1 Tj

Yj,T

µj −

E[Rjθ,i|Fj,i−1] − j ≥ .

Tj i=1 Tj

This representation follows from adding and subtracting the random variable given by T1j Ti=j 1 E[Rjθ,i|Fj,i−1] into the event ω and the deﬁnition of Yj,Tj given in (15). By Lemma 2,

ω ⊂ Lγj (Tj ) − Yj,Tj ≥

Tj

Tj

= Yj,Tj ≤ Lγj (Tj ) − .

Tj

Tj

Hence, applying Proposition 3,

P (µj − R¯j,Tj ≥

)≤P

Yj,Tj ≤ Lγj (Tj ) −

Tj

Tj

≤ exp − Tj 2

− Lγj (Tj) 2 . Tj

Deﬁning

2

1 Lγj (Tj)

=

log +

,

Tj

δ

Tj

we have, for any ﬁxed δ > 0,

P µ − R¯ ≥ 2 log 1 + Lγj (Tj) ≤ δ.

(29)

j

j,Tj

Tj

δ

Tj

Taking δ = k−3, and relating (29) to (26) and (27), we get that P (R¯∗,s ≤ µ∗ − c∗,t(s)) ≤ k−3

and P (R¯j,wj ≥ µj + c∗,t(wj)) ≤ k−3.

This implies that (26) and (27) hold with high probability. Now, we choose to be the smallest integer such that (28) is always false—that is,
choose it such that

Lγj ( )

6 log(k)

µ∗ − µj − 2cj,k(wj) > µ∗ − µj − 2

+

> 0.

(30)

38

Bandits for Correlated Markovian Environments

We ﬁnd for each function that Lγj ( ) can take on depending on the discount factor γ.

Case 1: γ ∈ (0, 1) and γ = λj ∀ j ∈ [m].

Plugging Lγj ( ) into (30), we need to ﬁnd to satisfy the following:

∆j − 2 Cj (φj)τ0 1 − (φj)ζ + 6 log(k) > 0.

(31)

φj − ηj 1 − (φj)ζ

Since

(1

−

(φj )ζ

)(1

−

(φj )ζ )−1

≤

(1

−

(φj )ζ )−1

and

1/x

<

√ 1/ x

on

[1, ∞),

we

have

1 (φj )τ0 φj − ηj

1 − (φj)ζ 1 − (φj)ζ

≤ √1

(φj )τ0 φj − (ηj)ζ

1 1 − (φj)ζ .

Thus, (31) reduces to ﬁnding the smallest integer such that

∆j − 2 √Cj

(φj )τ0 φj − ηj

1 1 − (φj)ζ +

6√log(k) > 0.

Rearranging and squaring terms, we get the following condition:

4

(φj )τ0

> ∆2j Cj φj − ηj

1 1 − (φj)ζ +

2
6 log(k) .

Case 2: γ ∈ (0, 1) and γ = λj for some j ∈ [m].

Plugging Lγj ( ) into (30), we need to ﬁnd to satisfy

∆j − 2 Cj(φj)τ0−1 τ0 − (φj)ζ (τ0 + ζ ) + ζ(φj)ζ (1 − (φj)ζ ) +

1 − (φj)ζ

(1 − (φj)ζ )2

=S

6 log(k) > 0, (32)

where S is the ﬁnite sum of an arithmetico-geometric series. Since the ﬁni√te sum of a series is upper bounded by the inﬁnite sum, implying S ≤ S∞, and 1/x < 1/ x on [1, ∞), we have

1 τ0 − (φj)ζ (τ0 + ζ ) ζ(φj)ζ (1 − (φj)ζ )

1 − (φj)ζ

+ (1 − (φj)ζ )2

< √1

τ0 + ζ(φj)ζ

.

1 − (φj)ζ (1 − (φj)ζ )2

Consequently, satisfying Equation (32) reduces to ﬁnding the smallest integer such that

∆j − 2 Cj (φ√j )τ0−1

τ0 + ζ(φj)ζ

+

1 − (φj)ζ (1 − (φj)ζ )2

6√log(k) > 0.

Rearranging and squaring terms, we get the following condition:

4

τ −1

τ0

ζ (φj )ζ

> ∆2 Cj (φj ) 0
j

1 − (φj)ζ + (1 − (φj)ζ )2 +

2
6 log(k) .

39

Fiez, Sekar, and Ratliff

Case 3: γ = 1. Plugging Lγj ( ) into (30), we need to ﬁnd to satisfy the following equation:

∆j − 2 Cj

11

ζ

6 log(k)

+ log 1 +

+

> 0.

(33)

(1 − λj) τ0 ζ

τ0

Let ˜ = ζ/τ0, so that the condition needing to be satisﬁed in (33) is equivalently expressed

as

∆j − 2

Cj

1 ζ + 1 log(1 + ˜) +

τ0(1 − λj) ˜ τ0 ˜

6 log(k) > 0.

Since 1/x < 1/√x and (1/x) log(1 + x) < 1/√x on [1, ∞), we have the inequality

1 ζ + 1 log(1 + ˜) <

1

ζ +

1 .

˜ τ0 ˜

˜ τ0

˜

Hence, an

such that the following equation holds will satisfy (33):

√

√

∆j − 2

Cj

√τ0 ζ + √τ0 +

τ0(1 − λj) ζ τ0

ζ

6√log(k) > 0.

Rearranging and squaring terms, we get the following condition:

4 >

√

Cj

ζ 1+ +

∆2j ζτ0(1 − λj)

τ0

2
6 log(k) .

Combining each of the cases, (28) is false for

= 4 ργ + ∆2j j

6 log(n) 2 ,

and for all

wj ≥ 4 ργ + ∆2j j

6 log(n) 2,

where the constant ργj is deﬁned as follows depending on the type of reward feedback: 1. Discount-Averaged Reward Feedback: γ ∈ (0, 1).

 Cj

(φj )τ0

1,

ργ =

φj −ηj 1−(φj )ζ

ζ

j Cj (φj )τ0−1 1−(τφ0j )ζ + (1−ζ((φφjj))ζ )2 ,

if γ = λj ∀ j ∈ [m] .
otherwise

2. Time-Averaged Reward Feedback: γ = 1. ργj = √ζτ0C(1j−λj ) 1 + τζ0 .

40

Bandits for Correlated Markovian Environments

Hence,

Eα[Tjα(n)] ≤

n k−1 k−1
+
k=1 s=1 wj =

P (R¯∗,s ≤ µ∗ − c∗,k(s)) + P (R¯j,wj ≥ µj + cj,k(wj))

≤ 4 ργ + ∆2j j

6 log(n) 2

nk
+

k
2k−3

k=1 s=1 wj =1

≤ 4 ργ + ∆2j j

6 log(n) 2 + 3 + 2 log(n).

Note that the ﬁnal inequality follows from (24).

B.5 Proof of Corollary 2

Proof (Proof of Corollary 2). The regret decomposition given in Proposition 2 says that the

regret of any algorithm α with epoch length sequence {τkα}nk=1 as deﬁned in (5) is bounded

as follows:

Rα(n) ≤ Eα[Tjα(n)]∆j +

Lγj (n).

j=j∗

j∈[m]

To obtain a gap-independent regret bound for EpochUCB, it is suﬃcient to derive a gapindependent bound on the term j=j∗ Eα[Tjα(n)]∆j since j∈[m] Lγj (n) has no dependence on the reward gaps. We derive such a bound using the Cauchy-Schwarz inequality, the
upper bound on the number of times a suboptimal arm is played by EpochUCB given in
Theorem 1, and the fact that ∆j ∈ [0, 1]. Indeed,

Eα[Tjα(n)]∆j =

Eα[Tjα(n)]∆j =

j=j∗

j∈[m]

j∈[m]

Eα[Tjα(n)]

Eα[Tjα(n)]∆j

≤

Eα[Tjα(n)]

Eα[Tjα(n)]∆2j

j∈[m]

j∈[m]

=n

Eα[Tjα(n)]∆2j

j∈[m]

≤n

4 ργ + ∆2j j

6 log(n) 2 + 3 + 2 log(n) ∆2j

j∈[m]

=n

4 ργj + 6 log(n) 2 + 3∆2j + 2∆2j log(n)

j∈[m]

≤n

4 ργj + 6 log(n) 2 + 3 + 2 log(n)

j∈[m]

=n

4(ργj )2 + 8ργj 6 log(n) + 26 log(n) + 3 .

j∈[m]

41

Fiez, Sekar, and Ratliff

Substituting the preceding result into Proposition 2, we have

REpochUCB(n) ≤

n

4(ργj )2 + 8ργj

j∈[m]

6 log(n) + 26 log(n) + 3 + Lγj (n).
j∈[m]

B.6 Proof of Theorem 2
Proof (Proof of Theorem 2). The EpochGreedy policy plays the arm with maximum empirical mean reward with probability 1 − εk and an arm selected uniformly at random with probability εk at an epoch k ∈ [n]. Formally, the policy at epoch k ∈ [n] with α taken as the EpochGreedy algorithm is

α(k) =

arg maxj∈[m] R¯j,T α(k−1), j
j,

w.p. 1 − εk .

w.p.

εk m

∀

j

∈ [m]

Deﬁne ∆min = minj=j∗ ∆j and ﬁx constants c and d√such that 0 ≤ d ≤ ∆min and c > c ν2 where c > 8, ν ≥ max{κ, √d }, and κ = min{κ > 0 : κ i ≥ Lγj (i) ∀ i ∈ [n], j ∈ [m]}. Recall

c

that Lγj (·) depends on the Markov chain statistics (Cj, λj) and is deﬁned in (13) for discount-

averaged reward feedback and in (14) for time-averaged reward feedback. Let {εk}nk=1 be

a

sequence

with

εk

=

min{1,

cm d2k

}

for

each

epoch

k

∈

[n]

so

that

for

k

≥

cm d2

, εk

=

cm d2k

.

Moreover, let x0 = 21 (

k i=1

mεi ),

and

observe

that

the

value

in

the

parenthesis

is

the

expected

number of times an arm will be pulled from random exploration. Given the above notation,

our objective is to upper bound the probability of a suboptimal arm j ∈ [m] being selected

at an epoch k ≥

cm d2

.

The probability that a suboptimal arm j ∈ [m] is chosen at epoch k ≥

cm d2

is given by

P (α(k) = j) = εk + (1 − εk)P R¯j,T α(k−1) ≥ R¯∗,T α(k−1) ,

m

j

∗

which we bound as

P (α(k) = j) ≤ c + P R¯j,T α(k−1) ≥ R¯∗,T α(k−1) .

d2k

j

∗

(34)

For notational simplicity, we proceed bounding P (R¯j,T α(k) ≥ R¯∗,T α(k)) and obtain a bound

j

∗

on P (R¯j,T α(k−1) ≥ R¯∗,T α(k−1)) from merely swapping the epoch index in our ﬁnal result.

j

∗

From a union bound over the events that result in the event {R¯j,T α(k) ≥ R¯∗,T α(k)}, we can

j

∗

bound P (R¯j,T α(k) ≥ R¯∗,T α(k)). Indeed,

j

∗

P R¯j,T α(k) ≥ R¯∗,T α(k) ≤ P R¯j,T α(k) ≥ µj + ∆j

j

∗

j

2

+ P R¯∗,T α(k) ≤ µ∗ − ∆j .

∗

2

(35)

42

Bandits for Correlated Markovian Environments

We expand P (R¯j,Tjα(k) ≥ µj + ∆2j ) using the conditional probability to factor out the randomness of Tjα(k) from R¯j,Tjα(k) as follows:

P R¯j,T α(k) ≥ µj + ∆j

j

2

= k P Tjα(k) = i, R¯j,i ≥ µj + ∆2j
i=1
= k P Tjα(k) = i R¯j,i ≥ µj + ∆2j P R¯j,i ≥ µj + ∆2j
i=1 x0
= P Tjα(k) = i R¯j,i ≥ µj + ∆2j P R¯j,i ≥ µj + ∆2j
i=1
+ k P Tjα(k) = i R¯j,i ≥ µj + ∆2j P R¯j,i ≥ µj + ∆2j . (36)
i= x0 +1

Now, consider the event ω = {R¯j,i − µj ≥ ∆2j }, and equivalently:

ω=

R¯j,i − 1i i E[Rjθ,l|Fj,l−1] + 1i i E[Rjθ,l|Fj,l−1] − µj ≥ ∆2j

l=1

l=1

= Yij,i + 1i i E[Rjθ,l|Fj,l−1] − µj ≥ ∆2j .
l=1

This representation follows from adding and subtracting 1i

i l=1

E[Rjθ,l|Fj,l−1]

into

the

event

ω and the deﬁnition of Yj,i from (15). From Lemma 2, we obtain

ω ⊂ Yj,i + Lγj (i) ≥ ∆j . (37)

i

i

2

Equation 37 holds for discount-averaged and time-averaged reward feedback. Fundamen-

tally, the type of reward feedback impacts the ﬁnal result as a consequence of the lower

bound on the constant c, which derives from the constant κ that depends on the functional

form Lγj (·) adopts as a consequence of the type of reward feedback.

For a positive integer k ≥ k

=

cm d2

and εk =

cm d2k

,

we

have

that

1k

1

x0 = 2m εi = 2m

i=1

k

k

cc

k

εi +

εi ≥ 2d2 + d2 log k

i=1

i= k

≥

c .

2d2

We also conclude that

c

d2k exp(1/2)

x0 ≥ d2 log

cm

.

(38)

Moreover, 2dc2 ≥ 2∆c ν2m2in since by construction c ≥ c ν2 and 0 ≤ d ≤ ∆min, so that x0 ≥ 2∆c ν2m2in .

This implies

∆j

∆min

ν

Lγj (x0)

≥

≥√ ≥

.

c /2

c /2

x0

x0

43

Fiez, Sekar, and Ratliff

Hence, √∆cj/2 ≥ Lγji(i) for all i ≥ x0 + 1 so that, recalling (37),

Yj,i Lγj (i) ∆j

Yj,i ∆j

∆j

Yj,i ∆j( c /2 − 2)

ω⊂

+

≥

⊂

≥−

=

≥

√

.

i

i

2

i2

c /2

i

2c

Now, we apply the Azuma-Hoeﬀding inequality from Proposition 3 to ω to get that

P R¯j,i ≥ µj + ∆j 2
Thus,

≤ P Yj,i ≥ ∆j( √c /2 − 2)

i

2c

≤ exp

−i

∆j( √c /2 − 2)

2
.

(39)

2

2c

P R¯j,T α(k) ≥ µj + ∆j

j

2

x0
≤P
i=1

Tjα(k) = i R¯j,i ≥ µj + ∆2j

k

+

P

i= x0 +1

R¯j,i ≥ µj + ∆j 2 (40)

≤ x0 P Tjα(k) = i R¯j,i ≥ µj + ∆2j + k exp − 2i ∆j( √c2/c2 − 2) 2 (41)

i=1

i= x0 +1

≤ x0 P Tjα(k) = i R¯j,i ≥ µj + ∆2j + ∆c 2 exp − ∆2jc x0 , (42)

i=1

j

where in (40) we upper bound the terms that were dropped from (36) by one, in (41) we apply the inequality of (39), and in (42) we deﬁne c = 4c ( c /2 − 2)−2 and employ the inequality

n

i∆2j

∞

i∆2j

c

∆2j x0

exp − c ≤

exp − c ≤ ∆2 exp − c .

i= x0 +1

i= x0 +1

j

The choice of which term to keep in each sum of (40) stems from the fact that the concen-
tration bound on the empirical mean will be the dominating factor when i ≤ x0 and will tend toward zero for i ≥ x0 + 1.
Deﬁning TjR(k) to be the number of epochs arm j has been chosen from random exploration by the EpochGreedy algorithm in k epochs, we have

P R¯j,T α(k) ≥ µj + ∆j

j

2

x0
≤P
i=1

Tjα(k) ≤ i R¯j,i ≥ µj + ∆2j

c + ∆2 exp
j

x0
≤P
i=1

TjR(k) ≤ i R¯j,i ≥ µj + ∆2j

c + ∆2 exp
j

≤ x0P TjR(k) ≤ x0 + ∆c 2 exp − ∆2jc x0 .
j

− ∆2j x0 c
− ∆2j x0 c

The ﬁnal inequality follows from recognizing that epochs consisting of random exploration are independent of the empirical mean rewards. To determine a bound on P (TjR(k) ≤ x0) we need the following Bernstein inequality.

44

Bandits for Correlated Markovian Environments

Proposition 4 (Bernstein Inequality (Uspensky, 1937)). Let Z1, . . . , Zn be independent

random variables with range [0, 1]. Deﬁne Sn =

n i=1

Xi

and

σ2

=

Var(Sn)

=

n i=1

Var(Xi

).

Then for all ≥ 0,

2

P (Sn − E[Sn] ≤ − ) ≤ exp − 2σ2 + .

We can express TjR(k) as a sum of indicator variables IjR,i for i ∈ [k] given by TjR(k) =

k i=1

IjR,i.

Each

indicator

variable

IjR,i

represents

the

event

arm

j

is

pulled

at

an

epoch

i

∈

[k]

due to random exploration. Moreover, each indicator variable IjR,i is a Bernoulli random

variable

with

parameter

p

=

εi m

for

i

∈

[k],

so

that

E[TjR(k)]

=

k i=1

εi m

and

E[TjR(k)]

=

2x0.

The variance of a Bernoulli random variable is upper bounded by the expected value,

implying that Var(TjR(k)) ≤ E[TjR(k)] = 2x0. Leveraging the preceding analysis, we apply

the Bernstein inequality from Proposition 4 to P (TjR(k) ≤ x0). This gives

P TjR(k) ≤ x0 = P TjR(k) ≤ 2x0 − x0 = P TjR(k) − E[TjR(k)] ≤ −x0 ≤ exp − x50 .

Hence,

P R¯j,Tjα(k) ≥ µj + ∆2j ≤ x0 exp − x50 + ∆c 2 exp − ∆2jc x0 .
j

Equivalent analysis can be used to show that

P R¯∗,T∗α(k) ≤ µ∗ − ∆2j ≤ x0 exp − x50 + ∆c 2 exp − ∆2jc x0 .
j

Relating back to (35), we obtain

P R¯j,Tjα(k−1) ≥ R¯∗,T∗α(k−1) ≤ 2x0 exp − x50 + 2∆c2 exp − ∆2jc x0 .
j

Substituting the above inequality into (34), we have

c

x0 2c

∆2j x0

P (α(k) = j) ≤ d2k + 2x0 exp − 5 + ∆2 exp − c .

j

Finally, the lower bound constructed for x0 in (38) yields

P (α(k) = j) ≤ c + d2k +

2c (k − 1)d2 exp(1/2)

d2 log

cm

cm (k − 1)d2 exp(1/2)

2c exp(1) d2

cm

c/c

(k − 1)d2 exp(1/2) .

c/(5d2)

45

Fiez, Sekar, and Ratliff

Appendix C. Details of Comparison Algorithms

In this section, we provide details on the continuous reinforcement learning approach that we

compare with our proposed algorithms. For a complete treatment, we refer the reader to the

works of Melo et al. (2008) and Geramifard et al. (2013). The Q-function is approximated

by Q(x, a) = wT φ(x, a) where w is the weight vector, x is the feature vector, and a is the

action.

The

loss

function

is

then

given

by

L(w)

=

1 2

(

Q+

(x,

a)

−

Q(

s,

a))2

where

Q+(x, a)

=

r + γ maxa Q(x , a ). To update the weights, gradient descent is used on the loss function

so that we have wk+1 = wk − α∇L(w) where α is the step size. We allow the reinforcement

learning algorithm to use the state distribution βtk as the features for the model and φ(βtk , a) is a vector of dimension |Θ|m which maps βtk into coordinates reserved for action a and

ma√kes each other coordinate have value zero. For our simulations, we use a step size of 1/ k where k is the round. The policy that the reinforcement learning algorithm follows is

an ε–greedy policy, meaning that at each iteration maxa Q(x, a) is played with probability

1 − ε and a random action is played with probability ε. We exponentially decayed ε to give

suﬃcient opportunity to explore and then exploit. Speciﬁcally, we decayed ε so that it was

< .05 after half of the horizon.

There are several reasons why we consider comparing to this form of reinforcement learning. To begin with, we note that it would be an extremely unfair comparison between discrete Q-learning that was able to observe the precise state—as opposed to state distribution—and the bandit algorithms. This is because the discrete Q-learning algorithm would then just be solving a Markov decision process and converge to the optimal statedependent policy in the limit. For this reason we were interested in a reinforcement learning approach that can handle partially observable problems. Indeed, we can consider the state distribution as a partial state observation since the state will be drawn from this distribution, but the reinforcement learning algorithm is not privy to the exact state. Linear function approximation is a natural choice given that the expected stationary distribution reward is a linear combination of the stationary distribution rewards and the stationary distribution.

Appendix D. Supplemental Discussion

In this section, we revisit our discussion from Section 3 on why studying algorithms em-

ploying constant length epochs would be problematic for bandit problems in a correlated

Markovian environment. The purpose of this discussion is to provide reasoning beyond

motivating applications for analyzing algorithms based on an arm-dependent linearly in-

creasing epoch length sequence, as well as to give theoretical justiﬁcation for the failure of

traditional bandit algorithms that ostensibly employ epochs of a ﬁxed duration equal to

one. In the ﬁnal step to obtain a bound on the Markovian regret penalty for each regime of

the discount factor γ, we were able to evaluate or bound the sums in (10)–(12) to terms that

approach a constant or grow only logarithmically in the time horizon. However, a constant

epoch length would make the summands of (10)–(12) constant. Consequently, each respec-

tive sum would be of the form

n i=1

c

where

c

>

0

represents

the

constant

summand,

so

that each respective sum would have a linear dependence on the time horizon. This means

that the bounds on the Markovian regret penalty appearing in Proposition 2 would grow

46

Bandits for Correlated Markovian Environments
linearly in the time horizon and we could no longer hope to obtain sublinear regret bounds regardless of the algorithm. Intuitively, this is to be expected: since the regret benchmark is with respect to the stationary distribution rewards and, owing to the correlated nature of the problem, any algorithm that is not guaranteed to select an arm repeatedly may never guide the state distribution toward a stationary distribution. In a similar manner, epochs of ﬁxed duration would lead to problems when considering speciﬁc choices of algorithms and attempting to bound either the probability of playing each suboptimal arm or the expected number of times each suboptimal arm is played. This is because if the observed rewards are not guaranteed to approach the stationary distribution rewards, then attempting to discriminate between the stationary distribution reward of each arm would be infeasible.
References
V. Anantharam, P. Varaiya, and J. Walrand. Asymptotically eﬃcient allocation rules for the multiarmed bandit problem with multiple plays–Part II: Markovian rewards. IEEE Transactions on Automatic Control, 32(11):977–982, 1987.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2):235–256, 2002a.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2002b.
M. G. Azar, A. Lazaric, and E. Brunskill. Regret bounds for reinforcement learning with policy advice. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 97–112, 2013.
K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 19(3):357–367, 1967.
R. Bellman. Dynamic programming, princeton university pres. Princeton. New Jersey, 1957.
P. Bolton, M. Dewatripont, et al. Contract theory. MIT press, 2005.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
D. Chakrabarti, R. Kumar, F. Radlinski, and E. Upfal. Mortal multi-armed bandits. In Neural Information Processing Systems, pages 273–280, 2008.
C. Cortes, G. DeSalvo, V. Kuznetsov, M. Mohri, and S. Yang. Discrepancy-based algorithms for non-stationary rested bandits. arXiv preprint arXiv:1710.10657, 2017.
O. Dekel, A. Tewari, and R. Arora. Online bandit learning against an adaptive adversary: from regret to policy regret. In International Conference on Machine Learning, pages 1747–1754, 2012.
47

Fiez, Sekar, and Ratliff
J. A. Fill. Eigenvalue bounds on convergence to stationarity for nonreversible Markov chains, with an application to the exclusion process. The Annals of Applied Probability, 1(1):62–87, 1991.
G. Folland. Real Analysis. Wiley, 2nd edition, 2007.
Y. Gai, B. Krishnamachari, and M. Liu. On the combinatorial multi-armed bandit problem with Markovian rewards. In Global Communications Conference, pages 1–6, 2011.
A. Garivier and E. Moulines. On upper-conﬁdence bound policies for switching bandit problems. In International Conference on Algorithmic Learning Theory, pages 174–188. Springer, 2011.
A. Geramifard, T. J. Walsh, S. Tellex, G. Chowdhary, N. Roy, and J. P. How. A tutorial on linear function approximators for dynamic programming and reinforcement learning. Foundations and Trends in Machine Learning, 6(4):375–451, 2013.
T. Graepel, J. Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft’s Bing search engine. In International Conference on Machine Learning, pages 13–20, 2010.
R. Herbrich, T. Minka, and T. Graepel. Trueskill: a Bayesian skill rating system. In Neural Information Processing Systems, pages 569–576, 2007.
C.-J. Ho, A. Slivkins, and J. W. Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. Journal of Artiﬁcial Intelligence Research, 55:317–359, 2016.
W. Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963.
N. Immorlica and R. D. Kleinberg. Recharging bandits. In Foundations of Computer Science, 2018.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.
D. Kahneman and A. Tversky. Choices, values, and frames. American Psychologist, 1984.
S. Kirkland. Subdominant eigenvalues for stochastic matrices with given column sums. Electronic Journal of Linear Algebra, 18:784–800, 2009.
J.-J. Laﬀont and D. Martimort. The theory of incentives: the principal-agent model. Princeton university press, 2009.
T. Lattimore and C. Szepesv´ari. Bandit algorithms. Cambridge University Press, forthcoming. URL http://downloads.tor-lattimore.com/banditbook/book.pdf.
N. Levine, K. Crammer, and S. Mannor. Rotting bandits. In Neural Information Processing Systems, pages 3077–3086, 2017.
48

Bandits for Correlated Markovian Environments
L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In International Conference on World Wide Web, pages 661–670, 2010.
S. Li, A. Karatzoglou, and C. Gentile. Collaborative ﬁltering bandits. In International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 539–548, 2016.
Y. Liu and M. Liu. An online learning approach to improving the quality of crowd-sourcing. IEEE/ACM Transactions on Networking, 25(4):2166–2179, 2017.
E. Mazumdar, R. Dong, V. R. Royo, C. Tomlin, and S. S. Sastry. A multi-armed bandit approach for online expert selection in markov decision processes. arXiv preprint arXiv:1707.05714, 2017.
F. S. Melo, S. P. Meyn, and M. I. Ribeiro. An analysis of reinforcement learning with function approximation. In International Conference on Machine learning, pages 664– 671, 2008.
R. Ortner, D. Ryabko, P. Auer, and R. Munos. Regret bounds for restless markov bandits. Theoretical Computer Science, 558:62–76, 2014.
H. Robbins and S. Monro. A stochastic approximation method. In Herbert Robbins Selected Papers, pages 102–109. Springer, 1985.
V. Shah, J. Blanchet, and R. Johari. Bandit learning with positive externalities. Neural Information Processing Systems, 2018.
A. Slivkins. Introduction to multi-armed bandits. Foundations and Trends in Machine Learning, forthcoming. URL http://slivkins.com/work/MAB-book.pdf.
C. Tekin and M. Liu. Online algorithms for the multi-armed bandit problem with Markovian rewards. In Allerton Conference on Communication, Control, and Computing, pages 1675–1682, 2010.
C. Tekin and M. Liu. Online learning of rested and restless bandits. IEEE Transactions on Information Theory, 58(8):5588–5611, 2012.
L. Tran-Thanh, S. Stein, A. Rogers, and N. R. Jennings. Eﬃcient crowdsourcing of unknown experts using bounded multi-armed bandits. Artiﬁcial Intelligence, 214:89–111, 2014.
J. Uspensky. Introduction to Mathematical Probability. McGraw-Hill Book Company, 1937.
49

