Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document
Shaden Shaar,1 Firoj Alam,1 Giovanni Da San Martino,2 Preslav Nakov1
1 Qatar Computing Research Institute, HBKU, Doha, Qatar 2 University of Padova, Italy
{sshaar, falam, pnakov}@hbku.edu.qa, dasan@math.unipd.it

arXiv:2109.07410v1 [cs.CL] 14 Sep 2021

Abstract
Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very timeconsuming, human fact-checkers can beneﬁt from tools that can support them and make them more efﬁcient. Here, we focus on building a system that could provide such support. Given an input document, it aims to detect all sentences that contain a claim that can be veriﬁed by some previously factchecked claims (from a given database). The output is a reranked list of the document sentences, so that those that can be veriﬁed are ranked as high as possible, together with corresponding evidence. Unlike previous work, which has looked into claim retrieval, here we take a document-level perspective. We create a new manually annotated dataset for the task, and we propose suitable evaluation measures. We further experiment with a learning-to-rank approach, achieving sizable performance gains over several strong baselines. Our analysis demonstrates the importance of modeling text similarity and stance, while also taking into account the veracity of the retrieved previously fact-checked claims. We believe that this research would be of interest to fact-checkers, journalists, media, and regulatory authorities.
1 Introduction
Recent years have brought us a proliferation of false claims, which spread fast online, especially in social media; in fact, much faster than the truth (Vosoughi, Roy, and Aral 2018). To deal with the problem, a number of fact-checking initiatives have been launched, such as FactCheck, FullFact, PolitiFact, and Snopes, where professional fact-checkers verify claims (Nakov et al. 2021a). Yet, manual fact-checking is very time-consuming and tedious, and checking a single claim can take many hours, even days. Thus, automatic factchecking has been proposed as a possible alternative (Li et al. 2016; Shu et al. 2017; Rashkin et al. 2017; Hassan et al. 2017; Vo and Lee 2018; Lee, Wu, and Fung 2018; Li et al. 2018; Thorne and Vlachos 2018; Lazer et al. 2018; Vosoughi, Roy, and Aral 2018; Zhang et al. 2020b), and it is useful in many scenarios, as it scales much better and can yield results much faster. Yet, automated methods lag behind in terms of credibility, transparency, and explainability, and they cannot rival the quality that manual fact-checking can offer.
Thus, manual and automatic fact-checking will likely coexist in the near future, and they will beneﬁt from each

Figure 1: The architecture of our system. Given an input document, it aims to detect all sentences that contain a claim that can be veriﬁed by some previously fact-checked claims (from a given database). The output is a re-ranked list of the document sentences, so that those that can be veriﬁed are ranked as high as possible, together with corresponding evidence.
other as automatic methods are trained on data that human fact-checkers produce, while human fact-checkers can be assisted by automatic tools. A middle ground between manual and automatic fact-checking is to verify an input claim by ﬁnding a previously fact-checked claim that allows us to make a true/false judgment on the veracity of the input claim. This is the problem we will explore below.
Previous work has approached the problem at the sentence level: given an input sentence/tweet, produce a ranked list of relevant previously fact-checked claims that can verify it (Shaar et al. 2020a). However, here we target a more challenging reformulation at the document level, where the system needs to prioritize the sentences to bring to the user’s attention, by producing an appropriate ranking. For example, a US presidential debate has 1,300 sentences on average, but only a small fraction of them would be veriﬁable using previously fact-checked claims.
In our problem formulation, given an input document, the system needs to detect all sentences that contain a claim that

can be veriﬁed by a previously fact-checked claim (from a given database of such claims). The output is a re-ranked list of the document sentences, so that those that can be veriﬁed are ranked as high as possible, as illustrated in Figure 1. The system could optionally further provide a corresponding claim (or a list of claims) from the database as evidence. Note that we are interested in returning claims that would not just be relevant when fact-checking the claims in the input sentence, but such that would be enough to decide on its factuality.
This is a novel formulation of the problem, which was not studied before. It would be of interest to fact-checkers not only when they are facing a new document to analyze, but also when they want to check whether politicians keep repeating claims that have been previously debunked, so that they can be approached for comments. It would also be of interest to journalists, as it could bring them a tool that can allow them to put politicians and public ofﬁcials on the spot, e.g., during a political debate, a press conference, or an interview, by showing the journalist in real time which claims have been previously fact-checked and found false. Finally, media outlets would beneﬁt from such tools for self monitoring and quality assurance, and so would regulatory authorities such as Ofcom.1
Our contributions can be summarized as follows:
• We introduce a new challenging real-world task formulation to assist fact-checkers, journalists, media, and regulatory authorities in ﬁnding which claims in a long document have been previously fact-checked.
• We develop a new dataset for this task formulation, which consists of seven debates, 5,054 sentences, 16,636 target veriﬁed claims to match against, and 75,810 manually annotated sentence–veriﬁed claim pairs.
• We deﬁne new evaluation measures (variants of MAP), which are better tailored for our task setup.
• We address the problem using a learning-to-rank approach, and we demonstrate sizable performance gains over strong baselines.
• We offer analysis and discussion, which can facilitate future research, and we release our data and code at http://anonymous
2 Related Work
Disinformation, misinformation, and “fake news” thrive in social media. See (Lazer et al. 2018) and (Vosoughi, Roy, and Aral 2018) for a general discussion on the science of “fake news” and the process of proliferation of true and false news online. There have also been several interesting surveys, e.g., Shu et al. (2017) studied how information is disseminated and consumed in social media. Another survey by Thorne and Vlachos (2018) took a fact-checking perspective on “fake news” and related problems. Yet another survey (Li et al. 2016) covered truth discovery in general.
More relevant to the present work, a recent survey has studied what AI technology can offer to assist the work
1http://www.ofcom.org.uk/

of professional fact-checkers (Nakov et al. 2021a), and has pointed out to the following research problems: (i) identifying claims worth fact-checking, (ii) detecting relevant previously fact-checked claims, (iii) retrieving relevant evidence to fact-check a claim, and (iv) actually verifying the claim.
The vast majority of previous work has focused on the latter problem, i.e., claim veriﬁcation, while the other three problems remain understudied, even though there is an awareness that they are integral steps of an end-to-end automated fact-checking pipeline (Vlachos and Riedel 2014; Hassan et al. 2017). This situation is gradually changing, and the research community has recently started paying more attention to all four problems, in part thanks to the emergence of evaluation campaigns that feature all steps such as the CLEF CheckThat! lab (Nakov et al. 2018; Elsayed et al. 2019; Barrón-Cedeño et al. 2020; Nakov et al. 2021b).
Here we focus on direction (ii), i.e., detecting relevant previously fact-checked claims, which is the least studied of the above problems. Shaar et al. (2020a) proposed a claimfocused task formulation, and released two datasets: one based on PolitiFact (i.e., claims from political debates and speeches), and another one based on Snopes (i.e., claims from tweets). They had a ranking formulation: given a claim, they asked to retrieve a ranked list of previously factchecked claims from a given database of such claims; the database included the veriﬁed claims together with corresponding articles. One can argue that this formulation falls somewhere between (ii) detecting relevant previously factchecked claims and (iii) retrieving relevant evidence to factcheck a claim. The same formulation was adopted at the CLEF CheckThat! lab in 2020, where the focus was on tweets, and in 2021, which featured both tweets and political debates (Barrón-Cedeño et al. 2020; Shaar et al. 2020b; Nakov et al. 2021b). A similar formulation was also explored in (Miranda et al. 2019).
Experiments with these datasets and task formulations have shown that one can achieve sizable performance gains when matching not only against the target claim, but also using the full text of the associated article that fact-checkers wrote to explain their verdict. Thus, in a follow-up work, Shaar et al. (2021) focused on modeling the context when checking an input sentence from a political debate, both on the source side and on the target side, e.g., by looking at neighboring sentences and using co-reference resolution.
There has been also an extension of the tweet formulation: Vo and Lee (2020) looked into multimodality. They focused on tweets that discuss images and tried to detect the corresponding veriﬁed claim by matching both the text and the image against the images in the veriﬁed claim’s article. Finally, the task was also addressed in a reverse formulation, i.e., given a database of fact-checked claims (e.g., a short list of common misconceptions about COVID-19), ﬁnd social media posts that make similar claims (Hossain et al. 2020).
Unlike the above work, our input is a document, and the goal is to detect all sentences that contain a claim that can be veriﬁed by some previously fact-checked claim (from a given database).

No. Input Sentence

Veriﬁed Claim

Label & Date

Stance

1 But the Democrats, by the way, are Donald Trump: The weak illegal immi- False, stated on April agree

very weak on immigration.

gration policies of the Obama Admin. al- 18, 2017

lowed bad MS 13 gangs to form in cities

across U.S. We are removing them fast!

Verdict Unknown

2

ICE we’re getting MS13 out by the Donald Trump: Says of MS13 gang Mostly-False, stated on agree

False

thousands.

members, "We are getting them out of our May 15, 2018

country by the thousands."

3

ICE we’re getting MS13 out by the Donald Trump: I have watched ICE lib- False, stated on June agree

Unknown

thousands.

erate towns from the grasp of MS13.

30, 2018

4 We have one of the highest business Barack Obama: "There are so many Half-True, stated on disagree Unknown tax rates anywhere in the world, push- loopholes ... our businesses pay effectively September 26, 2008 ing jobs and wealth out of our country. one of the lowest tax rates in the world."

Table 1: Example sentences from Donald Trump’s Interview with Fox and Friends on June 6th, 2018.

3 Task Deﬁnition
We deﬁne the task as follows (see also Figure 1): Given an input document and a database of previously fact-checked claims, produce a ranked list of its sentences, so that those that contain claims that can be veriﬁed by a claim from the database are ranked as high as possible. We further want the system to be able to point to the database claims that verify a claim in an input sentence.
Note that we want the Input sentence to be veriﬁed as true/false, and thus we want to skip matches against Veriﬁed claims with labels of unsure veracity such as halftrue. Note also that solving this problem requires going beyond stance, i.e., whether a previously fact-checked claim agree/disagree with the input sentence (Miranda et al. 2019). In certain cases, other factors might also be important, such as, (i) whether the two claims express the same degree of speciﬁcity, (ii) whether they are made by the same person and during the same time period, (iii) whether the veriﬁed claim is true/false or is of mixed factuality, etc. Table 1 shows some examples of Input and Veriﬁed claims, stance, and verdict for the Input–Veriﬁed claim pair.
4 Dataset
4.1 Background
We construct a dataset based on fact-checked claims from PolitiFact,2 an organization of journalists that focuses on claims made by politicians. For each fact-checked claim, they have a factuality label and an article explaining the reason for assigning that label.
PolitiFact further publishes commentaries that highlight some of the claims made in a debate or speech, with links to fact-checking articles about these claims from their website. These commentaries were used in previous work as a way to obtain a mapping from Input sentences in a debate/speech to Veriﬁed claims. For example, Shaar et al. (2020a) collected 16,636 Veriﬁed claims, and 768 Input–Veriﬁed claim pairs from 70 debates and speeches, together with the transcript of the target event. For each Veriﬁed claim, they released the following:
2http://www.politifact.com/

• VeriﬁedStatement: the text of the claim, which is a normalized version of the original claim, as the human factcheckers typically reformulate it, e.g., to make it clearer, context-independent, and self-contained;
• TruthValue: the label of the claim {Pants-on-Fire!, False, Mostly-False, Half-True, Mostly-True, True};
• Title: the title of the article that discusses the claim; • Body: the body of the article that discusses the claim.
The above dataset has high precision, and it is suitable for their formulation of the task: given a sentence (one of the 768 ones), identify the correct claim that veriﬁes it (from the set of 16,636 Veriﬁed claims). However, it turned out not to be suitable for our purposes due to recall issues: missing links between Input sentences in the debate/speech and the set of Veriﬁed claims. This is because PolitiFact journalists were not interested in making an exhaustive list of all possible correct mappings between Input sentences and Veriﬁed claims in their database; instead, they only pointed to some such links, which they wanted to emphasize. Moreover, if the debate made some claim multiple times, they would include a link for only one of these instances (or they would skip the claim altogether). Moreover, if the claims made in a sentence are veriﬁed by multiple claims in the database, they might only include a link to one of these claims (or to none).
As we have a document-level task, where identifying sentences that can be veriﬁed using a database of fact-checked claims is our primary objective (while returning the matching claims is secondary), we need not only high precision, but also high recall for the Input–Veriﬁed claims pairs.
4.2 Our Dataset
We manually checked and re-annotated seven debates from the dataset of Shaar et al. (2020a) by linking Veriﬁed claims from PolitiFact to the Input sentences in the transcript. This includes 5,054 sentences, and ideally, we would have wanted to compare each of them against each of the 16,636 Veriﬁed claims, which would have resulted in a huge and very imbalanced set of pairs: 5, 054 × 16, 636 = 84, 078, 344. Thus, we decided to pre-ﬁlter the Input sentences and the Input– Veriﬁed claim pairs. The process is sketched in Figure 2 and described in more detail below.

Figure 2: Data preparation pipeline.
Phase 1: Input Sentence Filtering Not all sentences in a speech/debate contain a veriﬁable factual claim, especially when uttered in a live setting. In speeches, politicians would make a claim and then would proceed to provide numbers and anecdotes to emphasize and to create an emotional connection with the audience. In our case, we only need to focus on claims. We also know that not all claims are important enough to be fact-checked. Thus, we follow (Konstantinovskiy et al. 2021) to keep only Input sentences that are worth fact-checking. Based on this deﬁnition, positive examples include, but are not limited to (a) stating a deﬁnition, (b) mentioning a quantity in the present or in the past, (c) making a veriﬁable prediction about the future, (d) referencing laws, procedures, and rules of operation, or (e) implying correlation or causation (such correlation/causation needs to be explicit). Negative examples include personal opinions and preferences, among others. In this step, three annotators independently made judgments about the Input sentences for check-worthiness (i.e., check-worthy vs. not check-worthy), and we only rejected a sentence if all three annotators judged it to be not check-worthy. As a result, we reduced the number of input sentences to check from 5,054 down to 700.
Phase 2: Generating Input–Veriﬁed Pairs Next, we used BM25 to retrieve 15 Veriﬁed claims per Input sentence. As a result, we managed to reduce the number of pairs to check from 700 × 16, 636 = 11, 645, 200 to 700 × 15 = 10, 500.
Phase 3: Input–Veriﬁed Pairs Filtering We manually went through the 10,500 Input–Veriﬁed pairs, and we ﬁltered out the ones that were incorrectly retrieved by the BM25 algorithm. Again, we were aiming for high recall, and thus we only rejected a pair if all three out of the three annotators independently chose to reject it. As a result, the ﬁnal number of pairs to check went down to 1,694.
Phase 4: Stance and Verdict Annotation Again, three annotators manually annotated the 1,694 Input–Veriﬁed pairs with stance and verdict using the following labels:
• stance: agree, disagree, unrelated, not–claim;
• verdict: true, false, unknown, not–claim.
The label for stance is agree if the Veriﬁed claim agrees with the Input claim, disagree if it opposes it, and unrelated if there is no agree/disagree relation (this includes truly unrelated claims or related but without agreement/disagreement, e.g., discussing the same topic).

The verdict is true/false if the Input sentence makes a claim whose veracity can be determined to be true/false based on the paired Veriﬁed claim and its veracity label; it is unknown otherwise. The veracity can be unknown for various reasons, e.g., (i) the Veriﬁed claim states something (a bit) different; (ii) the two claims are about different events; (iii) the veracity label of the Veriﬁed claim is ambiguous. We only need the verdict annotation to determine whether the Input sentence is veriﬁable; yet, we use the stance to construct suitable Input–Veriﬁed claim pairs.
Final Dataset Our ﬁnal dataset consists of 5,054 Input sentences, and 75,810 Input–Veriﬁed claim pairs. This includes 125 Input sentences that can be veriﬁed using a database of 16,663 fact-checked claims, and 198 Input– Veriﬁed claim pairs where the Veriﬁed can verify the Input sentence (as some Input sentences can be veriﬁed by more than one Veriﬁed). See Table 2 for more detail.
Annotation and Inter-Annotator Agreement Each Input–Veriﬁed claim pair was annotated by three annotators: one male and two female, with BSc and PhD degrees. The disagreements were resolved by majority voting, and, if not possible, in a discussion with additional consolidators. We measured the inter-annotator agreement on phase 4 (phases 1 and 3 aimed for high recall rather than agreement). We obtained a Fleiss Kappa (κ) of 0.416 for stance and of 0.420 for the verdict, both corresponding to moderate agreement.
Statistics Table 2 reports some statistics about each transcript, as well as overall (last row). Shown are (i) the number of sentences per transcript, (ii) total number of sentences with top 15 veriﬁed claim pairs, (iii) the number of input sentences for which there is a Veriﬁed claim with an agree or a disagree stance (column Stance-Input), (iv) the number of pairs with an agree or a disagree stance (column: Stancepairs), (v) the number of input sentences for which there is a true/false verdict (column Verdict-Input), and (vi) the number of pairs with a true/false verdict (column: Verdict-pairs).
5 Evaluation Measures
Given a document, the goal is to rank its sentences, so that those that those that can be veriﬁed (i.e., with a true/false verdict; Verdict-Input in Table 2) are ranked as high as possible, and also to provide a relevant Veriﬁed claim (i.e., one that could justify the verdict; Verdict-pairs in Table 2). This is a (double) ranking task, and thus we use ranking evaluation measures based on Mean Average Precision (MAP). First, let us recall the standard AP:
AP = nk=1 P1(k) × rel(k) , (1) rel.sentences
where P1(k) is the precision at a cut-off k in the list, rel(k) is 1 if the k-th ranked sentence is relevant (i.e., has either a true or a false verdict), and rel. sentences is the number of Input sentences that can be veriﬁed in the transcript.
We deﬁne more strict AP measures, APHr , AP0r, and AP0r.5, which only give credit for an Input sentence with a known verdict, if also a corresponding Veriﬁed claim is correctly identiﬁed:

Date
2017-08-03 2017-08-22 2018-04-26 2018-05-25 2018-06-12 2018-06-15 2018-06-28
Total

Event
Rally Speech Rally Speech Interview Naval Grad. Speech North Korea Summit Speech Interview Rally Speech

# Topic
3-4 5+ 5+ 1-2 1-2 3-4 5+

Sent.
291 792 597 279 1,245 814 1,036
5,054

Sent.-Var. Pairs
4,365 11,880
8,955 4,185 18,675 12,210 15,540
75,810

# Stance-Input
34 50 28 14 29 24 49
228

# Stance-pairs
62 116
52 19 45 36 82
412

# Verdict-Input
20 23 17
4 15 11 35
125

# Verdict-pairs
32 40 32 5 15 17 57
198

Table 2: Statistics about our dataset: number of sentences in each transcript, and distribution of clear stance (agree + disagree) and clear verdict (true + false) labels. The number of topics were manually decided by looking at the keywords detected in each transcript. Sent.: number of input sentences, Sent.-Var. Pairs: number of input sentences with top 15 veriﬁed claims pairs.

r

n k=1

P1r

(k)

×

relHr

(k)

APH = rel.sentences

(2)

where relHr (k) is 1 if the k-th ranked Input sentence is rele-

vant and at least one relevant Veriﬁed claim was retrieved in

the top-r Veriﬁed claim list.

r

n k=1

P0r

(k)

×

rel(k)

AP0 = rel. sentences

(3)

r

n k=1

P0r.5

(k)

×

rel(k)

AP0.5 = rel. sentences

(4)

where Pmr (k), is precision at cut-off k, so that it increments by m, if none of the relevant Veriﬁed claim was retrieved in

the top-r Veriﬁed claim list; otherwise, it increments by 1.3

We compute M AP , M APHr , M AP0r, and M AP0r.5 by averaging AP , APHr , AP0r, and AP0r.5, respectively, over the test transcripts.

We also compute M APinner by averaging the APinner

on the Veriﬁed claims: we compute APinner for a given In-

put sentence, by scoring the rankings of the retrieved Veriﬁed

claims as in the task presented in (Shaar et al. 2020a).

Experiment
BERTScore (F1) on VeriﬁedStatement NLI (Entl) on VeriﬁedStatement NLI (Neut) on VeriﬁedStatement NLI (Contr) on VeriﬁedStatement NLI (Entl+Contr) on VeriﬁedStatement SimCSE on Title SimCSE on VeriﬁedStatement SimCSE on Body SBERT on Title SBERT on VeriﬁedStatement SBERT on Body BM25 on VeriﬁedStatement BM25 on Body BM25 on Title

MAPinner
0.638 0.574 0.112 0.025 0.553 0.220 0.451 0.576 0.165 0.531 0.649 0.316 0.892 0.145

Table 3: Veriﬁed Claim retrieval experiments on the annotations obtained from the PolitiFact dataset and the manually annotated pairs with agree or disagree stance.

3The simple AP can also be represented as AP1r, as it increments by 1 regardless of whether a relevant Veriﬁed claim is in the top-r Veriﬁed claim list.

6 Model
The task we are trying to solve has two subtasks. The ﬁrst sorts the Input sentences in the transcript in a way, so that the Input sentences that can be veriﬁed using the database are on top. The second one consists of retrieving a list of matching Veriﬁed claims for a given Input sentence. While we show experiments for both subtasks, our main focus is on solving the ﬁrst one.
6.1 Input–Veriﬁed Pair Representation
In order to rank the Input sentences from the transcript, we need to ﬁnd ways to represent them, so that we would have information about whether the database of Veriﬁed claims can indeed verify some claim from the Input sentence. To do that, we propose to compute multiple similarity measures between all possible Input–Veriﬁed pairs, where we can match the Input sentence against the VeriﬁedStatement, the Title, and the Body of the veriﬁed claims’ fact-checking article in PolitiFact.
• BM25: These are BM25 scores when matching the Input sentence against the VeriﬁedStatement, the Title, and the Body, respectively (3 features);
• NLI Score (Nie et al. 2020): These are posterior probabilities for NLI over the labels {entailment, neutral, contradiction} between the Input sentence and the VeriﬁedStatement (3 features);
• BERTScore (Zhang et al. 2020a): F1 score from the BERTScore similarity scores between the Input sentence and the VeriﬁedStatement (1 feature);
• Sentence-BERT (SBERT) (Reimers and Gurevych 2019): Cosine similarity for sentence-BERT-large embedding of the Input sentence as compared to the embedding for the VeriﬁedStatement, the Title, and the Body. Since the Body is a longer piece of text, we obtain the cosine similarity between the Input sentence vs. each sentence from the Body, and we only keep the four highest scores (6 features);
• SimCSE (Gao, Yao, and Chen 2021): Similarly to SBERT, we compute the cosine similarity between the SimCSE embeddings of the Input sentence against the VeriﬁedStatement, the Title, and the Body. Again, we use the top-4 scores when matching against the Body sentences (6 features: 1 from the VeriﬁedStatement + 1 from the Title + 4 from the Body).

Politifact Truth Value
Pants on Fire! FALSE Mostly–False Half–True Mostly–True TRUE

True/False
24 76 44 2 42 11

Unknown
191 382 312 260 227 85

Table 4: Distribution: Input–Veriﬁed pairs with a true/false verdict vs. the TruthValue for Veriﬁed claim from PolitiFact.

6.2 Single-Score Baselines
Each of the above scores, e.g., SBERT, can be calculated for each Input–Veriﬁed claim pair. For a given Input sentence, this makes 16,663 scores (one for each Veriﬁed from the database), and as a baseline, we assign to the Input sentence a score that is the maximum over these scores. Then, we sort the sentences of the input document based on these scores, and we evaluate the resulting ranking.
6.3 Re-ranking Models
We performed preliminary experiments looking into how the above measures work for retrieving the correct Veriﬁed for an Input sentence for which there is at least one match in the Veriﬁed claims database. This corresponds to the sentencelevel task of (Shaar et al. 2020a), but on our dataset, where we augment the matching Input–Veriﬁed pairs from their dataset with all the Input–Veriﬁed pairs with a stance of agree or disagree. The results are shown in Table 3. We can see that BM25 on Body yields the best overall MAP score, which matches the observations in (Shaar et al. 2020a).
RankSVM for Veriﬁed Claim Retrieval Since now we know that the best Veriﬁed claim retriever uses the BM25 on Body, we use it to retrieve the top-N Veriﬁed claims for a given Input sentence, and then we calculate the 19 similarity measures described above for each candidate in this top-N list. Afterwards, we concatenate the scores for these top-N candidates. Thus, we create a feature vector of size 19 × N for each Input sentence. For example, a top-3 experiment uses for each Input sentence a feature vector of size 19×3 = 57, which represents each similarity measure based on the top-3 Veriﬁed claims retrieved by BM25 on Body. Then, we train a RankSVM using this feature representation.
RankSVM–Max Instead of concatenating the 19dimensional vectors for the top-N candidates, this time we take the maximum over these candidates for each feature, thus obtaining a new 19-dimensional vector. The hypothesis here is that the further apart these scores are, the more conﬁdent we can be that the Input sentence can be veriﬁed by the top retrieved Veriﬁed claim (Yang, Zhang, and Lin 2019). Then, we train a RankSVM like before.
RankSVM–Max with Skipping Table 4 shows us that almost all Input–Veriﬁed pairs with the TruthValue of the Veriﬁed claim being Half–True result in an Input sentence for which we cannot determine the verdict. Therefore, we further experiment with a variant of RankSVM–Max that skips scores belonging to a Half–True Veriﬁed claim.

7 Experiments and Evaluation
We performed a 7-fold cross-validation, where we used 6 out of the 7 transcripts for training and the remaining one for testing. We started by computing the 19 similarity measures described in Section 6. Then, we used these representations to test the baselines and to train pairwise learning-to-rank models. The results are shown in Table 5.
Baselines Table 5 shows that Sentence-BERT and SimCSE, computed on the Veriﬁed claims, perform best.
An interesting observation can be made by comparing Table 3 and Table 5. From Table 3, we see that the best Veriﬁed claim retriever uses BM25 on Body; however, we see poor results when we use this measure for Input sentences ranking. Moreover, while the best model in Table 5 is SBERT on VeriﬁedStatement, the Veriﬁed retriever using the same model performs poorly as seen in Table 3. This is because SBERT tends to always yield high scores to Veriﬁed claims, even when there is no relevant Veriﬁed claim.
RankSVM for Veriﬁed Claims Retrieval We trained a RankSVM on the 19 similarity measures computed for the top-N retrieved Veriﬁed claims, according to BM25, the best system on Body. We can see from Table 5 that using the RankSVM on the 19 measures improves the scores by up to 10 MAP points absolute. Moreover, the best model achieves a MAP score of 0.404.
RankSVM–Max We see a huge jump in performance when we use max-pooling instead of BM25-retrieved Veriﬁed claims. The MAP increases from 0.404 to 0.491 using the RankSVM on the Top-10 scores from all the 19 metrics.
A high improvement can be observed when we consider MAP30, MAP30.5 and MAP3H from RankSVM for Veriﬁed claims retrieval. Note that, since there is a max over each metric independently, we no longer have a uniﬁed Veriﬁed suggestion, which is required to compute MAP0, MAP0.5, and MAPH . Therefore, for computing these evaluation measures, we use the best Veriﬁed claim retriever from Table 3, i.e., BM25 on Body.
RankSVM–Max with Skipping The highest MAP score, 0.522, is achieved by the RankSVM that uses the top-5 scores from each measure while skipping the Half–True Veriﬁed claim scores. We can also conclude by looking at the other variants of the MAP score, e.g., MAPH , that we can identify the Input sentences that need to be fact-checked and detect the correct Veriﬁed claims in the top-3 ranks.
Ablation Experiments We performed an ablation study for the best model from Table 5 removing one of the features at a time. We also excluded all scores based on Title, VeriﬁedStatement and Body. The results are shown in Table 6. We can see that the largest drops, and therefore the most important features, are the VeriﬁedStatement and Body scores, whereas without Title scores the model performs almost identically to the original. We also notice that although the NLI Score did not perform very well by itself (see the baselines in Table 5), it yields a signiﬁcant drop, from 0.522 to 0.475 MAP points, when it is removed, which shows its importance.

Experiment

MAP MAP10 MAP30 Baselines: Single Scores

MAP10.5

MAP30.5

BERTScore (F1) on VeriﬁedStatement NLI (Entl) on VeriﬁedStatement NLI (Neut) on VeriﬁedStatement NLI (Contr) on VeriﬁedStatement NLI (Entl+Contr) on VeriﬁedStatement SimCSE on VeriﬁedStatement SimCSE on Title SimCSE on Body SBERT on VeriﬁedStatement SBERT on Title SBERT on Body BM25 on vclaimstatement BM25 on Title BM25 on Body

0.076 0.035 0.036 0.051 0.041 0.287 0.242 0.068 0.303 0.117 0.033 0.146 0.084 0.155

0.046 0.025 0.001 0.001 0.005 0.249 0.144 0.041 0.245 0.044 0.016 0.107 0.047 0.130

0.050 0.029 0.003 0.001 0.007 0.259 0.213 0.048 0.284 0.082 0.021 0.122 0.049 0.144

0.061 0.030 0.019 0.026 0.023 0.268 0.193 0.055 0.274 0.080 0.025 0.127 0.066 0.143

0.063 0.032 0.020 0.026 0.024 0.273 0.227 0.058 0.294 0.099 0.027 0.134 0.067 0.150

RankSVM for Retrieved Veriﬁed Claims (using BM25 on Body)

Top-1 Top-3 Top-5 Top-10 Top-20 Top-30

0.382 0.345 0.362 0.404 0.400 0.357

0.357 0.318 0.335 0.364 0.346 0.310

0.373 0.336 0.353 0.391 0.377 0.339

0.369 0.332 0.349 0.384 0.373 0.333

0.378 0.341 0.357 0.398 0.388 0.348

RankSVM–Max

Top-1 Top-3 Top-5 Top-10 Top-20 Top-30

0.411 0.449 0.482 0.491 0.488 0.486

0.299 0.328 0.349 0.394 0.381 0.377

0.390 0.429 0.464 0.473 0.470 0.468

0.355 0.389 0.416 0.443 0.434 0.432

0.401 0.439 0.473 0.482 0.479 0.477

RankSVM–Max with Skipping Half-True Veriﬁed claims

Top-1 Top-3 Top-5 Top-10 Top-20 Top-30

0.467 0.507 0.522 0.515 0.504 0.493

0.353 0.370 0.379 0.401 0.350 0.376

0.442 0.485 0.501 0.494 0.481 0.468

0.410 0.438 0.451 0.458 0.427 0.435

0.455 0.496 0.512 0.505 0.493 0.481

MAP1H
0.034 0.017 0.000 0.000 0.002 0.208 0.093 0.025 0.203 0.019 0.008 0.086 0.031 0.107
0.310 0.278 0.292 0.313 0.291 0.260
0.253 0.273 0.291 0.320 0.310 0.304
0.287 0.306 0.316 0.323 0.293 0.301

MAP3H
0.038 0.023 0.001 0.000 0.003 0.223 0.172 0.034 0.251 0.060 0.012 0.100 0.034 0.132
0.352 0.319 0.335 0.368 0.352 0.318
0.364 0.400 0.436 0.445 0.439 0.435
0.417 0.454 0.468 0.465 0.447 0.433

Table 5: Verdict Experiments: Baseline and re-ranking experiments on the PolitiFact dataset.

Experiment
RankSVM–Max on Top-5 with Skipping
w/o BERTScore (F1) w/o NLI Score (E, N, C) w/o SimCSE w/o SBERT w/o BM25 w/o scores on Title w/o scores on VeriﬁedStatement w/o scores on Body

MAP
0.522
0.499 0.475 0.511 0.498 0.497 0.522 0.311 0.444

MAP10
0.379
0.376 0.330 0.353 0.381 0.343 0.369 0.242 0.295

MAP30
0.501
0.480 0.451 0.486 0.481 0.473 0.501 0.293 0.427

MAP10.5
0.451
0.437 0.402 0.432 0.440 0.420 0.445 0.276 0.370

MAP30.5
0.512
0.489 0.463 0.499 0.490 0.485 0.511 0.302 0.435

MAP1H
0.316
0.313 0.279 0.295 0.308 0.287 0.308 0.198 0.249

MAP3H
0.468
0.450 0.423 0.454 0.452 0.441 0.468 0.268 0.398

Table 6: Verdict Experiments: Ablation experiments on the best model from Table 5, RankSVM with Top-5 scores from all metrics while skipping half-true Veriﬁed claims.

8 Conclusion and Future Work
We introduced a new challenging real-world task formulation to assist fact-checkers, journalists, media, and regulatory authorities in ﬁnding which claims in a long document have been previously fact-checked. Given an input document, we aim to detect all sentences containing a claim that can be veriﬁed by some previously fact-checked claims (from a given database). We developed a new dataset for this task formulation, consisting of seven debates, 5,054 sentences, 16,636 target veriﬁed claims to match against, and 75,810 manually annotated sentence–veriﬁed claim pairs.

We further deﬁned new evaluation measures (variants of MAP), which are better tailored for our task setup. We addressed the problem using a learning-to-rank approach, and we demonstrated sizable performance gains over strong baselines. We offered analysis and discussion, which can facilitate future research, and we released our data and code.
In future work, we plan to focus more on detecting the matching claims, which was our second objective here. We also plan to explore other transformers and novel ranking approaches such as multi-stage document ranking using monoBERT and duoBERT (Yates, Nogueira, and Lin 2021).

References
Barrón-Cedeño, A.; Elsayed, T.; Da San Martino, G.; Hasanain, M.; Suwaileh, R.; Haouari, F.; and Nakov, P. 2020. CheckThat! at CLEF 2020: Enabling the Automatic Identiﬁcation and Veriﬁcation of Claims on Social Media. In Proceedings of the European Conference on Information Retrieval, ECIR ’20, 499–507.
Barrón-Cedeño, A.; Elsayed, T.; Nakov, P.; Da San Martino, G.; Hasanain, M.; Suwaileh, R.; Haouari, F.; Babulkov, N.; Hamdan, B.; Nikolov, A.; Shaar, S.; and Sheikh Ali, Z. 2020. Overview of CheckThat! 2020: Automatic Identiﬁcation and Veriﬁcation of Claims in Social Media. In Proceedings of the 11th International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction, CLEF ’2020, 215–236.
Elsayed, T.; Nakov, P.; Barrón-Cedeño, A.; Hasanain, M.; Suwaileh, R.; Atanasova, P.; and Da San Martino, G. 2019. CheckThat! at CLEF 2019: Automatic Identiﬁcation and Veriﬁcation of Claims. In Proceedings of the 41st European Conference on Information Retrieval, ECIR ’19, 309–315.
Gao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821.
Hassan, N.; Zhang, G.; Arslan, F.; Caraballo, J.; Jimenez, D.; Gawsane, S.; Hasan, S.; Joseph, M.; Kulkarni, A.; Nayak, A. K.; Sable, V.; Li, C.; and Tremayne, M. 2017. ClaimBuster: The First-ever End-to-end Fact-checking System. Proc. VLDB Endow., 10(12): 1945–1948.
Hossain, T.; Logan IV, R. L.; Ugarte, A.; Matsubara, Y.; Young, S.; and Singh, S. 2020. COVIDLies: Detecting COVID-19 Misinformation on Social Media. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020.
Konstantinovskiy, L.; Price, O.; Babakar, M.; and Zubiaga, A. 2021. Towards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection. Digital Threats: Research and Practice, 2(2): 1–16.
Lazer, D. M.; Baum, M. A.; Benkler, Y.; Berinsky, A. J.; Greenhill, K. M.; Menczer, F.; Metzger, M. J.; Nyhan, B.; Pennycook, G.; Rothschild, D.; Schudson, M.; Sloman, S. A.; Sunstein, C. R.; Thorson, E. A.; Watts, D. J.; and Zittrain, J. L. 2018. The Science of Fake News. Science, 359(6380): 1094–1096.
Lee, N.; Wu, C.-S.; and Fung, P. 2018. Improving LargeScale Fact-Checking using Decomposable Attention Models and Lexical Tagging. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, ACL ’18, 1133–1138.
Li, S.; Zhao, S.; Cheng, B.; and Yang, H. 2018. An Endto-End Multi-task Learning Model for Fact Checking. In Proceedings of the First Workshop on Fact Extraction and VERiﬁcation (FEVER), 138–144.
Li, Y.; Gao, J.; Meng, C.; Li, Q.; Su, L.; Zhao, B.; Fan, W.; and Han, J. 2016. A Survey on Truth Discovery. SIGKDD Explor. Newsl., 17(2): 1–16.

Miranda, S. a.; Nogueira, D.; Mendes, A.; Vlachos, A.; Secker, A.; Garrett, R.; Mitchel, J.; and Marinho, Z. 2019. Automated Fact Checking in the News Room. In The World Wide Web Conference, WWW ’19, 3579–3583.
Nakov, P.; Barrón-Cedeño, A.; Elsayed, T.; Suwaileh, R.; Màrquez, L.; Zaghouani, W.; Atanasova, P.; Kyuchukov, S.; and Da San Martino, G. 2018. Overview of the CLEF-2018 CheckThat! Lab on Automatic Identiﬁcation and Veriﬁcation of Political Claims. In International Conference of the Cross-Language Evaluation Forum for European Languages, 372–387.
Nakov, P.; Corney, D.; Hasanain, M.; Alam, F.; Elsayed, T.; Barrón-Cedeño, A.; Papotti, P.; Shaar, S.; and Da San Martino, G. 2021a. Automated Fact-Checking for Assisting Human Fact-Checkers. In Proceedings of the 30th International Joint Conference on Artiﬁcial Intelligence, IJCAI ’21, 4551– 4558.
Nakov, P.; Da San Martino, G.; Elsayed, T.; Barrón-Cedeño, A.; Míguez, R.; Shaar, S.; Alam, F.; Haouari, F.; Hasanain, M.; Babulkov, N.; Nikolov, A.; Shahi, G. K.; Struß, J. M.; and Mandl, T. 2021b. The CLEF-2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News. In Proceedings of the 43rd European Conference on Information Retrieval, 639–649.
Nie, Y.; Williams, A.; Dinan, E.; Bansal, M.; Weston, J.; and Kiela, D. 2020. Adversarial NLI: A New Benchmark for Natural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL ’20.
Rashkin, H.; Choi, E.; Jang, J. Y.; Volkova, S.; and Choi, Y. 2017. Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP ’17, 2931–2937.
Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), EMNLP-IJCNLP ’19, 3982–3992.
Shaar, S.; Alam, F.; Da San Martino, G.; and Nakov, P. 2021. The Role of Context in Detecting Previously Fact-Checked Claims. Arxiv/2104.07423.
Shaar, S.; Babulkov, N.; Da San Martino, G.; and Nakov, P. 2020a. That is a Known Lie: Detecting Previously FactChecked Claims. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL ’20, 3607–3618.
Shaar, S.; Nikolov, A.; Babulkov, N.; Alam, F.; BarrónCedeño, A.; Elsayed, T.; Hasanain, M.; Suwaileh, R.; Haouari, F.; Da San Martino, G.; and Nakov, P. 2020b. Overview of CheckThat! 2020 English: Automatic Identiﬁcation and Veriﬁcation of Claims in Social Media. In Proceedings of the 11th International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction, CEUR Workshop Proceedings. CEUR-WS.org.

Shu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017. Fake News Detection on Social Media: A Data Mining Perspective. SIGKDD Explor. Newsl., 19(1): 22–36.
Thorne, J.; and Vlachos, A. 2018. Automated Fact Checking: Task Formulations, Methods and Future Directions. In COLING.
Vlachos, A.; and Riedel, S. 2014. Fact Checking: Task definition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, 18–22.
Vo, N.; and Lee, K. 2018. The rise of guardians: Factchecking url recommendation to combat fake news. In SIGIR, 275–284.
Vo, N.; and Lee, K. 2020. Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP ’20, 7717–7731.
Vosoughi, S.; Roy, D.; and Aral, S. 2018. The spread of true and false news online. Science, 359(6380): 1146–1151.
Yang, W.; Zhang, H.; and Lin, J. 2019. Simple Applications of BERT for Ad Hoc Document Retrieval. arXiv:1903.10972.
Yates, A.; Nogueira, R.; and Lin, J. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 1154–1156.
Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020a. BERTScore: Evaluating Text Generation with BERT. In International Conference on Learning Representations.
Zhang, W.; Deng, Y.; Ma, J.; and Lam, W. 2020b. AnswerFact: Fact Checking in Product Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), ACL ’20, 2407– 2417.

