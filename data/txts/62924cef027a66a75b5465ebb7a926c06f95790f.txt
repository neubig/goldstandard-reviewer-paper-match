Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment

Yifan Wu Carnegie Mellon University
yw4@cs.cmu.edu
Divyansh Kaushik Carnegie Mellon University
dkaushik@cs.cmu.edu

Ezra Winston Carnegie Mellon University
ewinston@cs.cmu.edu
Zachary Lipton Carnegie Mellon University
zlipton@cmu.edu

arXiv:1903.01689v2 [cs.LG] 11 Mar 2019

Abstract
Domain adaptation addresses the common problem when the target distribution generating our test data drifts from the source (training) distribution. While absent assumptions, domain adaptation is impossible, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-adversarial approaches consist of aligning source and target encodings, often motivating this approach as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, e.g. they can break down under shifting label distributions. We propose asymmetrically-relaxed distribution alignment, a new approach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize precise assumptions under which our algorithm is theoretically principled and demonstrate empirical beneﬁts on both synthetic and real datasets.
1 Introduction
Despite breakthroughs in supervised deep learning across a variety of challenging tasks, current techniques depend precariously on the i.i.d. assumption. Unfortunately, real-world settings often demand not just generalization to unseen examples but robustness under a variety of shocks to the data distribution. Ideally, our models would leverage unlabeled test data, adapting in real time to produce improved predictions. Unsupervised domain adaptation formalizes this problem as learning a classiﬁer from labeled source domain data and unlabeled data from a target domain, to maximize performance on the target distribution.
Without further assumptions, guarantees of target-domain accuracy are impossible (BenDavid et al., 2010b). However, well-chosen assumptions can make possible algorithms with non-vacuous performance guarantees. For example, under the covariate shift assumption (Heckman, 1977; Shimodaira, 2000), although the input marginals can vary between source and target (pS(x) = pT (x)), the conditional distribution of the labels (given features) exhibits invariance across domains (pS(y|x) = pT (y|x)). Some consider the reverse setting label shift (Saerens et al., 2002; Zhang et al., 2013; Lipton et al., 2018), where although the label distribution shifts (pS(y) = pT (y)), the class-conditional input distribution is invariant (pS(x|y) = pT (x|y)). Traditional approaches to both problems require the source distributions’
1

support to cover the target support, estimating adapted classiﬁers via importance-weighted risk minimization (Shimodaira, 2000; Huang et al., 2007; Gretton et al., 2009; Yu & Szepesva´ri, 2012; Lipton et al., 2018).
Problematically, assumptions of contained support are violated in practice. Moreover, most theoretical analyses do not guaranteed target accuracy when the source distribution support does not cover that of the target. A notable exception, Ben-David et al. (2010a) leverages capacity constraints on the hypothesis class to enable generalization to out-of-support samples. However, their results (i) do not hold for high-capacity hypothesis classes, e.g., neural networks; and (ii) do not provide intuitive interpretations on what is suﬃcient to guarantee a good target domain performance.
A recent sequence of deep learning papers have proposed empirically-justiﬁed adversarial training schemes aimed at practical problems with non-overlapping supports (Ganin et al., 2016; Tzeng et al.). Example problems include generalizing from gray-scale images to colored images or product images on white backgrounds to photos of products in natural settings. While importance-weighting solutions are useless here (with non-overlapping support, weights are unbounded), domain-adversarial networks (Ganin et al., 2016) and subsequently-proposed variants report strong empirical results on a variety of image recognition challenges.
The key idea of domain-adversarial networks is to simultaneously minimize the source error and align the two distributions in representation space. The scheme consists of an encoder, a label classiﬁer, and a domain classiﬁer. During training, the domain classiﬁer is optimized to predict each image’s domain given its encoding. The label classiﬁer is optimized to predict labels from encodings (for source images). The encoder weights are optimized for the twin objectives of accurate label classiﬁcation (of source data) and fooling the domain classiﬁer (for all data).
Although Ganin et al. (2016) motivate their idea via theoretical results due to Ben-David et al. (2010a), the theory is insuﬃcient to justify their method. Put simply, Ben-David et al. (2010a) bound the test error by a sum of three terms. The domain-adversarial objective minimizes two among these, but this minimization may cause the third term to increase. This is guaranteed to happen when the label distribution shifts between source and target. Consider the case of cat-dog classiﬁcation with non-overlapping support. Say that the source distribution contains 50% dogs and 50% cats, while the target distribution contains 25% dogs and 75% cats. Successfully aligning these distributions in representation space requires the classiﬁer to predict the same fraction of dogs and cats on source and target. If one achieves 100% accuracy on the source data, then target accuracy will be at most 75% (Figure 1(a)).
In this paper, we propose asymmetrically-relaxed distribution alignment, a relaxed distance for aligning data across domains that can be minimized without requiring latent-space distributions to match exactly. The new distance is minimized whenever the density ratios in representation space from target to source are upper bounded by a certain constant, such that the target representation support is contained in the source representation’s. The relaxed distribution alignment need not lead to a poor classiﬁer on the target domain under label distribution mismatch (Figure 1(b)). We demonstrate theoretically that the relaxed alignment is suﬃcient for a good target domain performance under a concrete set of assumptions on the data distributions. Further, we propose several practical ways to achieve the relaxed distribution alignment, translating the new distance into adversarial learning objectives. Empirical results on synthetic and real datasets show that incorporating our relaxed distribution alignment loss into adversarial domain adaptation gives better classiﬁcation
2

Source Target
φ:X →Z

Latent Space Z

Source Target
φ:X →Z

Latent Space Z

− Source

+

−

Input Space X

(a) Exact matching

+ Target

− Source

+

−

Input Space X

(b) Relaxed matching

+ Target

Figure 1: (a) In order to match the latent space distributions exactly, a model must map some elements of positive class in the target domain to some elements of negative class in the source domain. (b) A better mapping is achieved by requiring only that the source covers the target in the latent space.

performance on the target domain. We make the following key contributions:
• We propose an asymmetrically relaxed distribution matching objective, overcoming the limitation of standard objectives under label distribution shift.
• We provide theoretical analysis demonstrating that under a clear set of assumptions, the asymmetrically relaxed distribution alignment can provide target-domain performance guarantees.
• We propose several distances that satisfy the desired properties and are optimizable by adversarial training.
• We empirically show that our asymmetrically relaxed distribution matching losses improve target performance when there is a label distribution shift in the target domain, and perform comparably otherwise.

2 Preliminaries
We use subscripts S and T to distinguish between source and target domains, e.g., pS and pT , and employ the notation U for statements that are true for any domain U ∈ {S, T }. For simplicity, we dispense with some rigorousness in notating probability measures. For example, we use the terms measure and distribution interchangeably and assume that a density function exists when necessary without explicitly stating the base measure and required regularity conditions. We use a single lowercase letter, e.g. p, to denote both the probability measure function and the probability density function: p(x) is a density when the input x is a single point while p(C) is a probability when the input C is a set. We will use Supp(p) to denote the support of distribution p, i.e., the set of points where the density is positive. Similarly, for a function mapping φ, φ(x) denotes an output if x is a point and φ(C) denotes the image if C is a set. The inverse mapping φ−1 always outputs a set (the inverse image) regardless of whether its input is a point or a set. We will also be less careful about the use of sup
v.s. max, inf v.s. min and “everywhere” v.s. “almost everywhere”. 1 {·} is used as the
3

indicator function for statements that output 1 if the statement is true and 0 otherwise. For two functions f and g we use f ≡ g to denote that f (x) = g(x) for every input x.

Unsupervised domain adaptation For simplicity, we address the binary classiﬁcation
scenario. Let X be the input space and f : X → {0, 1} be the (domain-invariant) ground
truth labeling function. Let pS and pT be the input distributions over X for source and target domain respectively. Let Z be a latent space and Φ denote a class of mappings from X to Z. For a domain U , let pφU (·) be the induced probability distribution over Z such that pφU (C) = pU (φ−1(C)) for any C ⊂ Z. Given z ∈ Z let φU (·|z) be the conditional distribution induced by pU and φ such that dzpφU (z)φU (x|z) = pU (x) holds for all x ∈ X . Deﬁne H to be a class of predictors over the latent space Z, i.e., each h ∈ H maps from Z to {0, 1}.
Given a representation mapping φ ∈ Φ, classiﬁer h ∈ H, and input x ∈ X , our prediction is
h(φ(x)). The risk for a single input x can be written as |h(φ(x)) − f (x)| and the expected
risk for a domain U is

EU (φ, h) = dxpU (x) |h(φ(x)) − f (x)|

=. dzpφU (z) h(z) − fUφ(z) =. dzpφU (z)rU (z; φ, h) (1)

where we deﬁne a domain-dependent latent space labeling function fUφ(z) = and the risk for a classiﬁer h as rU (z; φ, h) = h(z) − fUφ(z) ∈ [0, 1].

dxφU (x|z)f (x)

We are interested in bounding the classiﬁcation risk of a (φ, h)-pair on the target domain:

ET (φ, h) = dzpφT (z)rT (z; φ, h) = ES(φ, h)

+ dzpφT (z)rT (z; φ, h) − dzpφS(z)rS(z; φ, h)

= ES(φ, h) + dzpφT (z) (rT (z; φ, h) − rS(z; φ, h))

+ dz pφT (z) − pφS(z) rS(z; φ, h) .

(2)

The second term in (2) becomes zero if the latent space labeling function is domain-invariant. To see this, we apply
rT (z; φ, h) − rS(z; φ, h) = h(z) − fTφ(z) − h(z) − fSφ(z)
≤ fTφ(z) − fSφ(z) . (3)
The third term in (2) is zero when pφT and pφS are the same. In the unsupervised domain adaptation setting, we have access to labeled source data (x, f (x)) for x ∼ pS and unlabeled target data x ∼ pT , from which we can calculate1 the ﬁrst and
1In this work we focus on how domain adaption are able to generalize across distributions with diﬀerent supports so we will not talk about ﬁnite-sample approximations.

4

third term in (2). For x ∈ Supp(pT ) \ Supp(pS), we have no information about its true label f (x) and thus fTφ(z) becomes inaccessible when z = φ(x) for such x. So the second term in (2) is not directly controllable.

Domain-adversarial learning Domain-adversarial approaches focus on minimizing the ﬁrst and third term in (2) jointly. Informally, these approaches minimize the source domain classiﬁcation risk and the distance between the two distributions in the latent space:

min ES(φ, h) + λD(pφS, pφT ) + Ω(φ, h) ,

(4)

φ,h

where D is a distance metric between distributions and Ω is a regularization term. Standard choices of D such as a domain classiﬁer (Jensen-Shannon (JS) divergence 2 ) (Ganin et al.,
2016), Wasserstein distance (Shen et al., 2018) or Maximum Mean Discrepancy (Huang et al., 2007) have the property that D(pφS, pφT ) = 0 if pφS ≡ pφT and D(pφS, pφT ) > 0 otherwise. In the next section, we will show that minimizing (4) with such D will lead to undesirable performance and propose an alternative objective to align pφS and pφT instead of driving them to be identically distributed.

3 A Motivating Scenario
To motivate our approach, we formally show how exact distribution matching can lead to undesirable performance. More speciﬁcally, we will lower bound ET (φ, h) when both ES(φ, h) and D(pφS, pφT ) are zero with respect to the shift in the label distribution. Let ρS and ρT be the proportion of data with positive label, i.e., ρU = dxpU (x)f (x). We formalize the result as follows. Proposition 3.1. If D(pφS, pφT ) = 0 if and only if pφS ≡ pφT , ES(φ, h) = D(pφS, pφT ) = 0 indicates ET (φ, h) ≥ |ρS − ρT |.
The proof follows the intuition of Figure 1(a): If ρS < ρT , the best we can do is to map ρT − ρS proportion of positive samples from the target inputs to regions of latent space corresponding to negative examples from the source domain while maintaining the label consistency for remaining ones. Switching the term positive/negative gives a similar argument for ρT < ρS. Proposition 3.1 says that if there is a label distribution mismatch ρT = ρS, minimizing the objective (4) to zero imposes a positive lower bound on the target error. This is especially problematic in cases where a perfect pair φ, h may exist, achieving zero error on both source and target data (Figure 1(b)).
Asymmetrically-relaxed distribution alignment It may appear contradictory that minimizing the ﬁrst and third term of (2) to zero guarantees a positive ET (φ, h) and thus a positive second term when there exists a pair of φ, h such that ET (φ, h) = 0 (all three terms are zero). However, this happens because although D(pφS, pφT ) = 0 is a suﬃcient condition for the third term of (2) to be zero, it is not a necessary condition. We now examine the third term of (2):
dz pφT (z) − pφS(z) rS(z; φ, h)
2Per (Nowozin et al., 2016), there is a slight diﬀerence between JS-divergence and the original GAN objective (Goodfellow et al., 2014). We will use the term JS-divergence for the GAN objective.
5

pφ (z)

≤ sup Tφ − 1 ES(φ, h).

(5)

z∈Z pS (z)

This expression (5) shows that if the source error ES(φ, h) is zero then it is suﬃcient to say the third term of (2) is zero when the density ratio pφT (z)/pφS(z) is upper bounded by some constant for all z. Note that it is impossible to bound pφT (z)/pφS(z) by a constant that is smaller than 1 so we write this condition as supz∈Z pφT (z)/pφS(z) ≤ 1 + β for some β ≥ 0. Note that this is a relaxed condition compared with pφT (z) ≡ pφS(z), which is a special case with β = 0.
Relaxing the exact matching condition to the more forgiving bounded density ratio condition makes it possible to obtain a perfect target domain classiﬁer in many cases where the stricter condition does not, by requiring only that the (latent space) target domain support is contained in the source domain support, as shown in Figure 1(b). The following proposition states that our relaxed matching condition does not suﬀer from the previously-described problems concerning shifting label distributions (Proposition 3.1), and provides intuition regarding just how large β may need to be to admit a perfect target domain classiﬁer. Proposition 3.2. For every ρS, ρT , there exists a construction of (pS, pT , φ, h) such that ES (φ, h) = 0, ET (φ, h) = 0 and supz∈Z pφT (z)/pφS (z) ≤ max ρρTS , 11−−ρρTS .
Given this motivation, we propose relaxing from exact distribution matching to bounding the density ratio in the domain-adversarial learning objective (4). We call this asymmetricallyrelaxed distribution alignment since we aim at upper bounding pφT /pφS (but not pφS/pφT ). We now introduce a class of distances between distributions that can be minimized to achieve the relaxed alignment: Deﬁnition 3.3 (β-admissible distances). Given a family of distributions deﬁned on the same space Z, a distance metric Dβ between distributions is called β-admissible if Dβ(p, q) = 0 when supz∈Z p(z)/q(z) ≤ 1 + β and Dβ(p, q) > 0 otherwise.
Our proposed approach is to replace the typical distribution distance D in the domainadversarial objective (4) with a β-admissible distance Dβ so that minimizing the new objective does not necessarily lead to a failure under label distribution shift. However, it is still premature to claim the justiﬁcation of our approach due to the following issues: (i) We may not be able get a perfect source domain classiﬁer with ES(φ, h) = 0. This also indicates a trade-oﬀ in selecting β as (a) higher β will increase the upper bound (βES(φ, h) according to (5)) on the third term in (2) (b) lower β will make a good target classiﬁer impossible under label distribution shift. (ii) Minimizing Dβ(pφT , pφS) as part of an objective does not necessarily mean that we will obtain a solution with Dβ(pφT , pφS) = 0. There may still be some proportion of samples from the target domain lying outside the support of source domain in the latent space Z. In this case, the density ratio pφT /pφS is unbounded and (5) becomes vacuous. (iii) Even when we are able optimize the objective perfectly, i.e., ES(φ, h) = Dβ(pφS, pφT ) = 0, with a proper choice of β such that there exists φ, h such that ET (φ, h) = 0 holds simultaneously (e.g. Figure 1(b), Proposition 3.2), it is still not guaranteed that such φ, h is learned (e.g. Figure 2(a)), as the second term of (2) is unbounded and changes with φ. Put simply, the problem is that although there may exist alignments perfect for prediction, there also exist other alignments that satisfy the objective but predict poorly (on target data). To our knowledge this problem eﬀects all domain-adversarial methods proposed in the literature, and how to theoretically guarantee that the desired alignment is

6

learned remains an open question.
Next, we theoretically study the target classiﬁcation error under asymmetrically-relaxed distribution alignment. Our analysis resolves the above issues by (i) working with imperfect source domain classiﬁer and relaxed distribution alignment; and (ii) providing concrete assumptions under which a good target domain classiﬁer can be learned.
4 Bounding the Target Domain Error
In a manner similar to (2), Ben-David et al. (2007, 2010a) bound the target domain error by a sum of three terms: (i) the source domain error (ii) an H-divergence between pφS and pφT (iii) the best possible classiﬁcation error that can be achieved on the combination of pφS and pφT . We motivate our analysis by explaining why their results are insuﬃcient to give a meaningful bound for domain-adversarial learning approaches. From a theoretical upper bound, we may desire to make claims in the following pattern:
Let MA be a set of models that satisfy a set of properties A (e.g. with low training error), and B be a set of assumptions on the data distributions (pS, pT , f ). For any given model M ∈ MA, its performance can be bounded by a certain quantity, i.e. ET (M ) ≤ A,B.
Ideally, A should be observable on available data information (i.e. without knowing target labels), and assumptions B should be model-independent (independent of which model M = (φ, h) is learned among MA). In the results of Ben-David et al. (2007, 2010a), terms (i) and (ii) are observable so A can be set as achieving low quantities on these two terms. Since term (iii) is unobservable we may want to make assumptions on it. This term, however, is model-dependent when φ is learned jointly. To make a model-independent assumption on term (iii), we need to take the supremum over all (φ, h) ∈ MA, i.e., all possible models that achieve low values on (i) and (ii). This supremum can be vacuous without further assumptions as a cross-label mapping may also achieve low source error and distribution alignment (e.g. Figure 2(a) v.s. Figure 1(b)). Moreover, when H contains all possible binary classiﬁers, the H-divergence is minimized only if the two distributions are the same, thus suﬀering the same problem as Proposition 3.1 and is therefore not suitable for motivating a learning objective.
To overcome these limitations, we propose a new theoretical bound on the target domain error which (a) treats the diﬀerence between pφS and pφT asymmetrically and (b) bounds the label consistency (second term in 2) by exploiting the Lipschitz-ness of φ as well as the separation and connectedness of data distributions. Our result can be interpreted as a combination of observable model properties and unobservable model-independent assumptions while being non-vacuous: it is able to guarantee correct classiﬁcation for (some fraction of) data points from the target domain even where the source domain has zero density.
4.1 A general bound
We introduce our result with the following construction: Construction 4.1. The following statements hold simultaneously:
7

1. (Lipschitzness of representation mapping.) φ is L-Lipschitz: dZ (φ(x1), φ(x2)) ≤ LdX (x1, x2) for any x1, x2 ∈ X .

2. (Imperfect asymmetrically-relaxed distribution alignment.) For some β ≥ 0, there exist a set B ⊂ Z such that ppφTφ((zz)) ≤ 1 + β holds for all z ∈ B and pφT (B) ≥ 1 − δ1.
S
3. (Separation of source domain in the latent space.) There exist two sets C0, C1 ⊂ X that satisfy:

(a) C0 ∩ C1 = ∅

(b) pS(C0 ∪ C1) ≥ 1 − δ2.

(c) For i ∈ {0, 1}, f (x) = i for all x ∈ Ci.

(d) infz0∈φ(C0),z1∈φ(C1) dZ (z0, z1) ≥ ∆ > 0.

Note that this construction does not require any information about target domain labels so the statements [1-3] can be viewed as observable properties of φ. We now introduce our model-independent assumption: Assumption 4.2. (Connectedness from target domain to source domain.) Given constants (L, β, ∆, δ1, δ2, δ3), assume that, for any BS, BT ⊂ X with pS(BS) ≥ 1 − δ2 and pT (BT ) ≥ 1 − δ1 − (1 + β)δ2, there exists CT ⊂ BT that satisﬁes the following conditions:

1. For any x ∈ CT , there exists x ∈ CT ∩ BS such that one can ﬁnd a sequence of points

x0, x1, ..., xm

∈ CT

with

x0

= x,

xm

=x,

f (x) = f (x )

and

dX (xi−1, xi) <

∆ L

for

all

i = 1, ..., m.

2. pT (CT ) ≥ 1 − δ3.

We are ready to present our main result: Theorem 4.3. Given a L-Lipschitz mapping φ ∈ Φ and a binary classiﬁer h ∈ H, if φ satisﬁes the properties in Construction 4.1 with constants (L, β, ∆, δ1, δ2), and Assumption 4.2 holds with the same set of constants plus δ3, then the target domain error can be bounded as

ET (φ, h) ≤ (1 + β)ES(φ, h) + 3δ1 + 2(1 + β)δ2 + δ3 .

Notice that it is always possible to make Construction 4.1 by adjusting the constants L, β, ∆, δ1, δ2. Given these constants, Assumption 4.2 can always be satisﬁed by adjusting δ3. So Theorem 4.3 is a general bound. The key challenge in bounding ET (φ, h) is to bound the second term in (2) by identifying suﬃcient conditions that prevent cross-label mapping (e.g. Figure 2(a)). To resolve this challenge, we exploit the fact that if there exist a path from a target domain sample to a source domain sample in the input space X and all samples along the path are mapped into two separate regions in the latent space (due to distribution alignment), then these two connected samples cannot be mapped to diﬀerent regions, as shown in Figure 2(b).
4.2 Example of a perfect target domain classiﬁer
To interpret our result, we construct a simple situation where ET (φ, h) = 0 is guaranteed when the domain adversarial objective with relaxed distribution alignment is minimized to
8

− Source Target

Latent Space Z +

φ:X →Z

− Source

+

−

Input Space X

(a) Failure case

+ Target

− Source Target

Latent Space Z +

φ:X →Z

φ is not continuous

Target Source
− Input Space X +
(b) Failure impossible

Figure 2: (a) Label consistency is broken even if φ satisﬁes the relaxed distribution aligning requirement. (b) The main idea of our analysis: A continuous mapping cannot project a connected region into two regions separated by a margin. So label consistency is preserved for a region that is connected to the source domain.

zero, exploiting pure data-dependent assumptions: Assumption 4.4. Assume the target support consists of disjoint clusters Supp(pT ) = ST,0,1 ∪ ... ∪ ST,0,m0 ∪ ST,1,1 ∪ ... ∪ ST,1,m1 , where any cluster ST,i,j is connected and its labels are consistent: f (x) = i for all x ∈ ST,i,j. Moreover, each of these cluster overlaps with source distribution. That is, for any i ∈ {0, 1} and j ∈ {1, ..., mi}, ST,i,j ∩ Supp(pS) = ∅. Corollary 4.5. If Assumption 4.4 holds and there exists a continuous mapping φ such that (i) supz∈Z pφT (z)/pφS(z) ≤ 1 + β for some β ≥ 0; (ii) for any pair x0, x1 ∈ Supp(pS) such that f (x0) = 0 and f (x1) = 1, we have dZ (φ(x0), φ(x1)) ≥ ∆ > 0, then ES(φ, h) = 0 indicates ET (φ, h) = 0.
Proof follows directly by observing that a construction of δ1 = δ2 = δ3 = 0 exists in Theorem 4.3. A simple example that satisﬁes Assumption 4.4 is Figure 2(b). For a real world example, consider the cat-dog classiﬁcation problem. Say that source domain contains small-to-medium cats and dogs while target domain contains medium-to-large cats and dogs. The target domain consists of clusters (e.g. cats and dogs, or multiple sub-categories) and each of them overlaps with the source domain (the medium ones).

5 Asymmetrically-relaxed distances
So far, we have motivated the use of asymmetrically-relaxed distribution alignment which aims at bounding pφT /pφS by a constant instead of driving towards pφS ≡ pφT . More speciﬁcally, we propose to use a β-admissible (Deﬁnition 3.3) distance Dβ in objective (4) to align the source and target encodings rather than the standard distances corresponding an adversarial domain classiﬁer. In this section, we derive several β-admissible distance metrics that can be practically minimized with adversarial training. More speciﬁcally, we propose three types of distances (i) f-divergences; (ii) modiﬁed Wasserstein distance; (iii) reweighting distances; and demonstrate how to optimize them by adversarial training.

9

5.1 f -divergence

Given a convex and continuous function f which satisﬁes f (1) = 0, the f -divergence between two distributions p and q can be written as Df (p, q) = dzp(z)f pq((zz)) . According to
Jensen’s inequality Df (p, q) ≥ f dzp(z) pq((zz)) = 0. Standard choices of f (see a list in Nowozin et al. (2016)) are strictly convex thus Df (p, q) = 0 if and only if p ≡ q when f is strictly convex. To derive a β-adimissible variation for each standard choice of f , we linearize f (u) where u ≥ 1+1β . If and only if pq((zz)) ≤ 1 + β for all z, f becomes a linear function with respect to all q(z)/p(z) and thus Jensen’s inequality holds with equality.
Given a convex, continuous function f : R+ → R with f (1) = 0 and some β ≥ 0, we introduce the partially linearized f¯β as follows

f¯β(u) =

f (u) + Cf,β f ( 1+1β )u − f ( 1+1β )

if u ≤ 1+1β , if u > 1+1β .

where Cf,β = −f ( 1+1β ) + f ( 1+1β ) 1+1β − f ( 1+1β ).

It can be shown that f¯β is continuous, convex and f¯β(1) = 0. As we already explained,

Df¯

(p, q)

=

0

if

and

only

if

p(z) q(z)

≤

1+β

for

all

z.

Hence

is

Df¯

is β-admissible.

β

β

Adversarial training According to Nowozin et al. (2016), adversarial training (Goodfellow et al., 2014) can be viewed as minimizing the dual form of f -divergences

Df (p, q) = sup Ez∼q [T (z)] − Ez∼p [f ∗(T (z))]
T :Z→dom(f ∗)

where f ∗ is the Fenchel Dual of f with f ∗(t) = supu∈dom(f) {ut − f (u)}. Applying the same derivation for f¯β we get3

Df¯ (p, q) = sup Ez∼q [T (z)] − Ez∼p [f ∗(T (z))]

(6)

β

T :Z→dom(f¯∗)

β

where dom(f¯β∗) = dom(f ∗) ∩ − ∞, f ( 1+1β ) . Plugging in the corresponding f for JS-divergence gives

Df¯β (p, q)

g(z)

g(z)

= g:Zs→up(0,1] Ez∼q log 2 + β + Ez∼p log 1 − 2 + β , (7)

where g(z) can be parameterized by a neural network with sigmoid output as typically used in adversarial training.

5.2 Wasserstein distance
The idea behind modifying the Wasserstein distance is to model the optimal transport from p to the region where distributions have 1 + β maximal density ratio with respect to q. We
3We are omitting some additive constant term.

10

deﬁne the relaxed Wassertein distance as Wβ(p, q) = inf E(z1,z2)∼γ [ z1 − z2 ] ,
γ∈ β (p,q)
where β(p, q) is deﬁned as the set of joint distributions γ over Z × Z such that

∀z1 dzγ(z1, z) = p(z1) ; ∀z2 dzγ(z, z2) ≤ (1 + β)q(z2) .

Wβ is β-admissible since no transportation is needed if p already lies in the qualiﬁed region with respect to q.
Adversarial training Following the derivation for the original Wasserstein distance, the dual form becomes

Wβ(p, q) = sup Ez∼p [g(z)] − (1 + β)Ez∼q [g(z)]

(8)

g

s.t. ∀z ∈ Z , g(z) ≥ 0 ,

∀z1, z2 ∈ Z , g(z1) − g(z2) ≤ z1 − z2 ,

Optimization with adversarial training can be done by parameterizing g as a non-negative function (e.g. with soft-plus output log(1 + ex) or RELU output max(0, x)) and following Arjovsky
et al. (2017); Gulrajani et al. (2017) to enforce its Lipschitz continuity approximately.

5.3 Reweighting distance

Given any distance metric D, a generic way to make it β-admissible is to allow reweighting for one of the distances within a β-dependent range. The relaxed distance is then deﬁned as the minimum achievable distance by such reweighting.
Given a distribution q over Z and a reweighting function w : Z → [0, ∞). The reweighted distribution qw is deﬁned as qw(z) = dqz(zq)(wz)(wz)(z) . Deﬁne Wβ,q to be a set of β-qualiﬁed reweighting with respect to q:

1 Wβ,q = w : Z → [0, 1], dzq(z)w(z) = 1 + β .

Then the relaxed distance can be deﬁned as

Dβ(p, q) = min D(p, qw) .

(9)

w∈Wβ,q

Such Dβ is β-admissible since the set {qw : w ∈ Wβ,q} is exactly the set of p such that supz∈Z p(z)/q(z) ≤ 1 + β.
Adversarial training We propose an implicit-reweighting-by-sorting approach to optimize Dβ without parameterizing the function w when D can be optimized by adversarial training. Adversarially trainable D shares a general form as

D(p, q) = sup Ez∼p [f1(g(z))] − Ez∼q [f2(g(z))] ,
g∈G

11

where f1 and f2 are monotonically increasing functions. According to (9), the relaxed distance can be written as

Dβ(p, q) = min sup Ez∼p [f1(g(z))] − Ez∼qw [f2(g(z))] ,
w g∈G

1

s.t. w : Z → [0, 1] , dzq(z)w(z) =

.

(10)

1+β

One step of alternating minimization on Dβ, could consist of ﬁxing p, q, g and optimizing w. Then the problem becomes

max dzq(z)w(z)f2(g(z)) .

(11)

w∈Wβ,q

Observe that the optimal solution to (11) is to assign w(z) = 1 for the 1+1β fraction of z from distribution q, where f2(g(z)) take the largest values. Based on this observation, we propose to do the following sub-steps when optimizing (11) as an alternating minimization step: (i)
Sample a minibatch of z ∼ q; (ii) Sort these z in descending order according to f2(g(z)); (iii) Assign w(z) = 1 to the ﬁrst 1+1β fraction of the list. Note that this optimization procedure is not justiﬁed in principle with mini-batch adversarial training but we found it to work well in
our experiments.

Source, y=0 Source, y=1 Target, y=0 Target, y=1

Source, y=0 Source, y=1 Target, y=0 Target, y=1

Source, y=0 Source, y=1 Target, y=0 Target, y=1

(a) raw (synthetic) data

(b) latent representations (DANN) (c) latent representations (ours)

Figure 3: Domain-adversarial training under label distribution shift on a synthetic dataset.

6 Experiments
To evaluate our approach, we implement Domain Adversarial Neural Networks (DANN), (Ganin et al., 2016) replacing the JS-divergence (domain classiﬁer) with our proposed βadmissible distances (Section 5). Our experiments address the following questions: (i) Does DANN suﬀer the limitation as anticipated (Section 3) when faced with label distribution shift? (ii) If so, do our β-admissible distances overcome these limitations? (iii) Absent shifting label distributions, is our approach comparable to DANN?
We implement adversarial training with diﬀerent β-admissible distances (Section 5) and compare their performance with vanilla DANN. We name diﬀerent implementations as follows. (a) Source: source-only training. (b) DANN: JS-divergence (original DANN). (c) WDANN: original Wasserstein distance. (d) fDANN-β: β-admissible f -divergence, JS-version (7). (e) sDANN-β: reweighting JS-divergence (10), optimized by our proposed
12

implicit-reweighting-by-sorting. (f) WDANN1-β: β-admissible Wasserstein distance (8) with soft-plus on critic output. (g) WDANN2-β: β-admissible Wasserstein distance (8) with RELU on critic output. (h) sWDANN-β: reweighting Wasserstein distance (10), optimized by implicit-reweighting-by-sorting. Adversarial training on Wasserstein distances follows Gulrajani et al. (2017) but uses one-sided gradient-penalty. We always perform adversarial training with alternating minimization (see Appendix for details).

Synthetic datasets We create a mixture-of-Gaussians binary classiﬁcation dataset where each domain contains two Gaussian distributions, one per label. For each label, the distributions in source and target domain have a small overlap, validating the assumptions in our analysis. We create a label distribution shift with balanced source data (50% 0’s v.s. 50% 1’s) and imbalanced target data (10% 0’s v.s. 90% 1’s) as shown in Figure 3(a). Table 1 shows the target domain accuracy for diﬀerent approaches. As expected, vanilla DANN fails under label distribution shift because a proportion of samples from the target inputs are mapped to regions of latent space corresponding to negative samples from the source domain (Figure 3(b)). In contrast, with our β-admissible distances, domain-adversarial networks are able to adapt successfully (Figure 3(c)), improving target accuracy from 89% (source-only) to 99% accuracy (with adaptation), except the cases where β is too small to admit a good target domain classiﬁer (in this case we need β ≥ 0.9/0.5 − 1 = 0.8). We also experiment with label-balanced target data (no label distribution shift). All approaches except source-only achieve an accuracy above 99%, so we do not present these results in a separate table.

Table 1: Classiﬁcation accuracy on target domain with label distribution shift on a synthetic dataset.

method

accuracy%

Source DANN

89.4±1.1 59.1±5.1

WDANN 50.8±32.1

β

0.5

2.0

4.0

fDANN-β sDANN-β WDANN1-β WDANN2-β sWDANN-β

66.0± 41.6 99.9± 0.1 45.7± 41.5 97.6± 1.2 79.0± 5.9

99.9± 0.0 99.9± 0.0 66.4± 41.1 99.7± 0.2 99.9± 0.0

99.8±0.0 99.9±0.0 99.9±0.0 99.5±0.3 99.9±0.0

Real datasets We experiment with the MNIST and USPS handwritten-digit datasets. For both directions (MNIST → USPS and USPS → MNIST), we experiment both with and without label distribution shift. The source domain is always class-balanced. To simulate label distribution shift, we sample target data from only half of the digits, e.g. [0-4] or [5-9]. Tables 2 and 3 show the target domain accuracy for diﬀerent approaches with/without label distribution shift. As on synthetic datasets, we observe that DANN performs much worse than source-only training under label distribution shift. Compared to the original DANN, our approaches fair signiﬁcantly better while achieving comparable performance absent label distribution shift.

13

Table 2: Classiﬁcation accuracy on target domain with/without label distribution shift on MNIST-USPS.

target labels
Source DANN
fDANN-1 fDANN-2 fDANN-4 sDANN-1 sDANN-2 sDANN-4

[0-4] Shift
74.3±1.0 50.0±1.9
71.6±4.0 74.3±2.5 75.9±1.6 71.6±3.7 76.4±3.1 81.0±1.6

[5-9] Shift
59.5±3.0 28.2±2.8
67.5±2.3 61.9±2.9 64.4±3.6 49.1±6.3 48.7±9.0 60.8±7.5

[0-9] No-Shift
66.7±2.1 78.5±1.6
73.7±1.5 72.6±0.9 72.3±1.2 81.0±1.3 81.7±1.4 82.0±0.4

Table 3: Classiﬁcation accuracy on target domain with/without label distribution shift on USPS-MNIST.

target

[0-4]

[5-9]

[0-9]

labels

Shift

Shift No-Shift

Source DANN

69.4±2.3 57.6±1.1

30.3±2.8 37.1±3.5

49.4±2.1 81.9±6.7

fDANN-1 fDANN-2 fDANN-4 sDANN-1 sDANN-2 sDANN-4

80.4±2.0 86.6±4.9 77.6±6.8 68.2±2.7 78.6±3.6 83.5±2.7

40.1±3.2 41.7±6.6 34.7±7.1 45.4±7.1 36.1±5.2 41.1±6.6

75.4±4.5 70.0±3.3 58.5±2.2 78.8±5.3 77.4±5.7 75.6±6.9

7 Related work
Our paper makes distinct theoretical and algorithmic contributions to the domain adaptation literature. Concerning theory, we provide a risk bound that explains the behavior of domainadversarial methods with model-independent assumptions on data distributions. Existing theories without assumptions of contained support (Ben-David et al., 2007, 2010a; BenDavid & Urner, 2014; Mansour et al., 2009; Cortes & Mohri, 2011) do not exhibit this property since (i) when applied to the input space, their results are not concerned with domain-adversarial learning as no latent space is introduced, (ii) when applied to the latent space, their unobservable constants/assumptions become φ-dependent, which is undesirable as explained in Section 4. Concerning algorithms, several prior works demonstrate empirical success of domain-adversarial approaches, (Tzeng et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016; Tzeng et al.; Hoﬀman et al., 2017; Shu et al., 2018). Among those, Cao et al. (2018a,b) deal with the label distribution shift scenario through a heuristic reweighting scheme. However, their re-weighting presumes that they have a good classiﬁer in the ﬁrst place, creating a cyclic dependency.

14

8 Conclusions
We propose to use asymmetrically-relaxed distribution distances in domain-adversarial learning objectives, replacing standard ones which seek exact distribution matching in the latent space. While overcoming some limitations of the standard objectives under label distribution mismatch, we provide a theoretical guarantee for target domain performance under assumptions on data distributions. As our connectedness assumptions may not cover all cases where we expect domain adaptation to work in practice, (e.g. when the two domains are completely disjoint), providing analysis under other type of assumptions might be of future interest.
Acknowledgments
This work was made possible by a generous grant from the Center for Machine Learning and Health, a joint venture of Carnegie Mellon University, UPMC, and the University of Pittsburgh, in support of our collaboration with Abridge AI to develop robust models for machine learning in healthcare. We are also supported in this line of research by a generous faculty award from Salesforce Research.
References
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Ben-David, S. and Urner, R. Domain adaptation–can quantity compensate for quality? Annals of Mathematics and Artiﬁcial Intelligence, 70(3):185–202, 2014.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. Analysis of representations for domain adaptation. In Advances in neural information processing systems, pp. 137–144, 2007.
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from diﬀerent domains. Machine learning, 79(1-2):151–175, 2010a.
Ben-David, S., Lu, T., Luu, T., and P´al, D. Impossibility theorems for domain adaptation. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 129–136, 2010b.
Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. Domain separation networks. In Advances in Neural Information Processing Systems, pp. 343–351, 2016.
Cao, Z., Long, M., Wang, J., and Jordan, M. I. Partial transfer learning with selective adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2724–2732, 2018a.
Cao, Z., Ma, L., Long, M., and Wang, J. Partial adversarial domain adaptation. In European Conference on Computer Vision, pp. 139–155. Springer, 2018b.
Cortes, C. and Mohri, M. Domain adaptation in regression. In International Conference on Algorithmic Learning Theory, pp. 308–323. Springer, 2011.
15

Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014.
Gretton, A., Smola, A. J., Huang, J., Schmittfull, M., Borgwardt, K. M., and Sch¨olkopf, B. Covariate shift by kernel mean matching. Journal of Machine Learning Research, 2009.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767–5777, 2017.
Heckman, J. J. Sample selection bias as a speciﬁcation error (with an application to the estimation of labor supply functions), 1977.
Hoﬀman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017.
Huang, J., Gretton, A., Borgwardt, K. M., Scho¨lkopf, B., and Smola, A. J. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pp. 601–608, 2007.
Lipton, Z. C., Wang, Y.-X., and Smola, A. Detecting and correcting for label shift with black box predictors. arXiv preprint arXiv:1802.03916, 2018.
Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Nowozin, S., Cseke, B., and Tomioka, R. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271–279, 2016.
Saerens, M., Latinne, P., and Decaestecker, C. Adjusting the outputs of a classiﬁer to new a priori probabilities: a simple procedure. Neural computation, 14(1):21–41, 2002.
Shen, J., Qu, Y., Zhang, W., and Yu, Y. Wasserstein distance guided representation learning for domain adaptation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
Shimodaira, H. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000.
Shu, R., Bui, H. H., Narui, H., and Ermon, S. A dirt-t approach to unsupervised domain adaptation. arXiv preprint arXiv:1802.08735, 2018.
Tzeng, E., Hoﬀman, J., Saenko, K., and Darrell, T. Adversarial discriminative domain adaptation.
Tzeng, E., Hoﬀman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
Yu, Y. and Szepesv´ari, C. Analysis of kernel mean matching under covariate shift. arXiv preprint arXiv:1206.4650, 2012.
16

Zhang, K., Scho¨lkopf, B., Muandet, K., and Wang, Z. Domain adaptation under target and conditional shift. In International Conference on Machine Learning, pp. 819–827, 2013.
17

A Proofs

Derivation of (1).

EU (φ, h) = dxpU (x) |h(φ(x)) − f (x)|

= dx dzpφU (z)φU (x|z) |h(φ(x)) − f (x)|
= dzpφU (z) dxφU (x|z) |h(z) − f (x)|
= dzpφU (z) h(z) − dxφU (x|z)f (x) =. dzpφU (z) h(z) − fUφ(z) =. dzpφU (z)rU (z; φ, h)
where we use the following fact: For any ﬁxed z, h(z) ∈ {0, 1}, if h(z) = 0 then |h(z) − f (x)| = f (x) − h(z) for all x. Similarly, when h(z) = 1, we have |h(z) − f (x)| = h(z) − f (x) for all x. Thus we can move the integral over x inside the absolute operation.

Proof of Proposition 3.1. First we have ρU = dxpU (x)f (x) = dx dzpφU (z)φU (x|z)f (x) = dzpφU (z)fUφ(z) .
When ES(φ, h) = 0 we have dzpφS(z)h(z) − ρS = dzpφS(z)h(z) − dzpφS(z)fSφ(z) ≤ dzpφS(z) h(z) − fSφ(z) = ES(φ, h) = 0
thus dzpφS(z)h(z) = ρS. Applying the fact that pφS(z) = pφT (z) for all z ∈ Z,
ET (φ, h) = dzpφT (z) h(z) − fTφ(z) ≥ dzpφT (z)h(z) − dzpφT (z)fTφ(z)

= dzpφS(z)h(z) − which concludes the proof.

dzpφT (z)fTφ(z) = |ρS − ρT | ,

Proof of Proposition 3.2. Let pS be the uniform distribution over [0, 1] and pT be the uniform distribution over [2, 3]. The labeling function f is set as f (x) = 1 iﬀ x ∈ [0, ρS] ∪ [2, 2 + ρT ] such that the deﬁnition of ρS and ρT is preserved. We construct the following mapping φ: For x ∈ [0, 1] φ(x) = x. For x ∈ [2, 2 + ρT ] φ(x) = (x − 2)ρS/ρT . For x ∈ [2 + ρT , 3] φ(x) = 1 − (3 − x)(1 − ρS)/(1 − ρT ). φ maps both source and target data into [0, 1] with pφS to be uniform over [0, 1] and pφT (z) = ρT /ρS when z ∈ [0, ρS] and pφT (z) = (1 − ρT )/(1 − ρS) when z ∈ [ρS, 1]. Since pφS(z) = 1 for all z ∈ [0, 1] we can conclude that supz∈Z pφT (z)/pφS(z) ≤ max ρρTS , 11−−ρρTS .

18

Proof of Theorem 4.3. Instead of working with Assumption 4.2 we ﬁrst extend Construction 4.1 with the following addition

Construction A.1. (Connectedness from target domain to source domain.) Let CT ⊂ X be a set of points in the raw data space that satisfy the following conditions:

1. φ(CT ) ⊂ φ(C0 ∪ C1).

2. For any x ∈ CT , there exists x ∈ CT ∩ (C0 ∪ C1) such that one can ﬁnd a sequence of

points

x0, x1, ..., xm

∈ CT

with

x0

= x,

xm

=x,

f (x) = f (x )

and

dX (xi−1, xi) <

∆ L

for all i = 1, ..., m.

3. pT (CT ) ≥ 1 − δ3.

We now proceed to prove bound based on Constructions 4.1 and A.1. Later on we will show that Assumption 4.2 indicates the existence of Construction A.1 so that the bound holds with a combination of Constructions 4.1 and Assumption 4.2.
The third term of (2) can be written as

dzpφS (z)

pφT (z) − 1 pφS (z)

rS(z; φ, h)

≤ inf dzpφ (z)

B⊆Z B

S

pφT (z) − 1 pφS (z)

rS(z; φ, h) + dzpφS(z)
Bc

pφT (z) − 1 pφS (z)

rS(z; φ, h)

≤ inf
B⊆Z

sup pφT (z) − 1 z∈B pφS (z)

dzpφS(z)rS(z; φ, h) + dzpφT (z)rS(z; φ, h)

B

Bc

≤ inf
B⊆Z

sup pφT (z) − 1 z∈B pφS (z)

ES(φ, h) + pφT (Bc)

≤ βES(φ, h) + δ1 .

(12)

For the second term of (2), plugging in rU (z; φ, h) = h(z) − fUφ(z) gives

dzpφT (z) (rT (z; φ, h) − rS(z; φ, h))

= dzpφT (z) h(z) − fTφ(z) − h(z) − fSφ(z)

= dzpφT (z) fTφ(z) − fSφ(z)

= dzpφT (z) fTφ(z) − fSφ(z) (1 {z ∈ φ(C0)} + 1 {z ∈ φ(C1)} + 1 {z ∈ (φ(C0) ∪ φ(C1))c})

= dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ φ(C0)} + dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ φ(C1)}

+ dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ (φ(C0) ∪ φ(C1))c}

(13)

19

Applying fTφ(z) − fSφ(z) ≤ fTφ(z) + fSφ(z) to the ﬁrst part of (13) gives

dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ φ(C0)}

≤ dzpφT (z)fTφ(z)1 {z ∈ φ(C0)} + dzpφT (z)fSφ(z)1 {z ∈ φ(C0)}

= dzpφT (z) dxφT (x|z)f (x)1 {z ∈ φ(C0)} + dzpφT (z)fSφ(z)1 {z ∈ φ(C0)}

= dxf (x) dzpφT (z)φT (x|z)1 {z ∈ φ(C0)} + dzpφT (z)fSφ(z)1 {z ∈ φ(C0)}

= dxf (x)pT (x)1 {φ(x) ∈ φ(C0)} + dzpφT (z)fSφ(z)1 {z ∈ φ(C0)}

= dxpT (x)1 {f (x) = 1, φ(x) ∈ φ(C0)} + dzpφT (z)fSφ(z)1 {z ∈ φ(C0)}

(14)

Similarly, applying fTφ(z) − fSφ(z) = (1 − fTφ(z)) − (1 − fSφ(z)) ≤ (1 − fTφ(z)) + (1 − fSφ(z)) to the second part of (13) gives

dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ φ(C1)}

≤ dzpφT (z)(1 − fTφ(z))1 {z ∈ φ(C1)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

= dzpφT (z) 1 − dxφT (x|z)f (x) 1 {z ∈ φ(C1)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

= dx(1 − f (x)) dzpφT (z)φT (x|z)1 {z ∈ φ(C1)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

= dx(1 − f (x))pT (x)1 {φ(x) ∈ φ(C1)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

= dxpT (x)1 {f (x) = 0, φ(x) ∈ φ(C1)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

(15)

Combining the second part of (14) and the second part of (15)

dzpφT (z)fSφ(z)1 {z ∈ φ(C0)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)} = dz ppφTφ ((zz)) pφS(z)fSφ(z)1 {z ∈ φ(C0)} (1 {z ∈ B} + 1 {z ∈ Bc})
S
+ dz ppφTφ ((zz)) pφS(z)(1 − fSφ(z))1 {z ∈ φ(C1)} (1 {z ∈ B} + 1 {z ∈ Bc}) S
≤ (1 + β) dzpφS(z)fSφ(z)1 {z ∈ φ(C0)} + (1 + β) dzpφS(z)(1 − fSφ(z))1 {z ∈ φ(C1)}
+ dzpφT (z)1 {z ∈ Bc} (1 {z ∈ φ(C0)} + 1 {z ∈ φ(C1)})
≤ (1 + β) dxpS(x)1 {f (x) = 1, φ(x) ∈ φ(C0)} + (1 + β) dxpS(x)1 {f (x) = 0, φ(x) ∈ φ(C1)} + pT (Bc)

20

≤ (1 + β) dxpS(x) (1 {f (x) = 1, φ(x) ∈ φ(C0) ∨ f (x) = 0, φ(x) ∈ φ(C1)}) + δ1

(16)

For i ∈ {0, 1} if x ∈ Ci then f (x) = i and φ(x) ∈ Ci. So if f (x) = 1, φ(x) ∈ φ(C0) or f (x) = 0, φ(x) ∈ φ(C1) holds we must have x ∈/ C0 ∪ C1. Therefore, following (16) gives

dzpφT (z)fSφ(z)1 {z ∈ φ(C0)} + dzpφT (z)(1 − fSφ(z))1 {z ∈ φ(C1)}

≤ (1 + β) dxpS(x)1 {x ∈/ C0 ∪ C1} + δ1

= (1 + β)(1 − pS(C0 ∪ C1)) + δ1

≤ (1 + β)δ2 + δ1

(17)

Now looking at the ﬁrst part of (14) and the ﬁrst part of (15)

dxpT (x)1 {f (x) = 1, φ(x) ∈ φ(C0)} + dxpT (x)1 {f (x) = 0, φ(x) ∈ φ(C1)}

= dxpT (x)1 {f (x) = 1, φ(x) ∈ φ(C0), x ∈ CT } + dxpT (x)1 {f (x) = 1, φ(x) ∈ φ(C0), x ∈/ CT }

+ dxpT (x)1 {f (x) = 0, φ(x) ∈ φ(C1), x ∈ CT } + dxpT (x)1 {f (x) = 0, φ(x) ∈ φ(C1), x ∈/ CT }

≤ dxpT (x) (1 {f (x) = 1, φ(x) ∈ φ(C0), x ∈ CT } + 1 {f (x) = 0, φ(x) ∈ φ(C1), x ∈ CT }) + pT (CTc )

≤ dxpT (x)1 {x ∈ CT } 1 {f (x) = 1, φ(x) ∈ φ(C0) ∨ f (x) = 0, φ(x) ∈ φ(C1)} + δ3 . (18)

Next we show that the ﬁrst part of (18) is 0. Recall that φ(CT ) ⊂ φ(C0 ∪ C1) and if x ∈ CT

there exists x ∈ CT ∩ (C0 ∪ C1) with a sequence of points x0, x1, ..., xm ∈ CT such that

x0 = x, xm = x , f (x) = f (x ) and dX (xi−1, xi) <

∆ L

for all i = 1, ..., m.

So for x ∈ CT

and f (x) = i, we pick such x . Since φ is L-Lipschitz and φ(CT ) ⊂ φ(C0 ∪ C1) we have

φ(x0), φ(x1), ..., φ(xm) ∈ φ(C0 ∪ C1) and dZ (φ(xi−1), φ(xi)) < ∆ for all i = 1, ..., m. Applying

the fact that infz0∈φ(C0),z1∈φ(C1) dZ (z0, z1) ≥ ∆ > 0 we know that if φ(x) = φ(x0) ∈ φ(Cj )

for some j ∈ {0, 1} then φ(x ) = φ(xm) ∈ φ(Cj). From x ∈ C0 ∪ C1 and f (x ) = f (x) = i

we have φ(x ) ∈ φ(Ci). Since C0 ∩ C1 = ∅ we can conclude i = j and thus φ(x) ∈ φ(Ci)

if f (x) = i for any x ∈ CT . Therefore, if x ∈ CT , neither f (x) = 1, φ(x) ∈ φ(C0) nor

f (x) = 0, φ(x) ∈ φ(C1) can hold. Hence the ﬁrst part of (18) is 0.

So far by combining (17) and (18) we have shown that the sum of (14) and (15) (which are the ﬁrst two parts of (13)) can be upper bounded by δ1 + (1 + β)δ2 + δ3. For the third part of (13) we have

dzpφT (z) fTφ(z) − fSφ(z) 1 {z ∈ (φ(C0) ∪ φ(C1))c}

≤ dzpφT (z)1 {z ∈ (φ(C0) ∪ φ(C1))c}
= dz ppφTφ ((zz)) pφS(z)1 {z ∈ (φ(C0) ∪ φ(C1))c} (1 {z ∈ B} + 1 {z ∈ Bc}) S
≤ dz ppφTφ ((zz)) pφS(z)1 {z ∈ (φ(C0) ∪ φ(C1))c} 1 {z ∈ B} + dzpφT (z)1 {z ∈ Bc} S

21

≤ (1 + β) dzpφS(z)1 {z ∈ (φ(C0) ∪ φ(C1))c} + δ1 = (1 + β) 1 − dzpφS(z)1 {z ∈ φ(C0) ∪ φ(C1)} + δ1

= (1 + β) 1 − dxpS(x)1 x ∈ φ−1 (φ(C0) ∪ φ(C1)) + δ1

= (1 + β) 1 − pS φ−1 (φ(C0) ∪ φ(C1)) + δ1

≤ (1 + β) (1 − pS (C0 ∪ C1)) + δ1

≤ (1 + β)δ2 + δ1 .

(19)

Putting (19) into (13) gives

dzpφT (z) (rT (z; φ, h) − rS(z; φ, h)) ≤ 2δ1 + 2(1 + β)δ2 + δ3 .

(20)

Plugging (12) and (20) into (2) gives the result of Theorem 4.3 under Constructions 4.1 and A.1. It remains to show that Assumption 4.2 implies the existence of a Construction A.1. To prove this, we ﬁrst write φ(CT ) ⊂ φ(C0 ∪ C1) as CT ⊂ φ−1(φ(C0 ∪ C1)). By Construction 4.1 we have pS(C0 ∪ C1) ≥ 1 − δ2. From (19) we have
pT φ−1(φ(C0 ∪ C1)) = dxpT (x)1 x ∈ φ−1(φ(C0 ∪ C1))
= dzpφT (z)1 {z ∈ φ(C0 ∪ C1)} ≥ (1 + β)δ2 + δ1 .
Setting BS = C0 ∪ C1 and BT = φ−1(φ(C0 ∪ C1) in Assumption 4.2 gives a construction of Construction A.1, thus concluding the proof.

Proof of Corollary 4.5. Based on the statement of Corollary 4.5 it is obvious that Construction 4.1 can be made with δ1 = 0, δ2 = 0 and a ﬁnitely large L. (Here we implicitly assume that φ is bounded on X ). It remains to show that Assumption 4.2 holds with δ3 = 0. As δ1 = δ2 = 0, any BS and BT will be supersets of Supp(pS) and Supp(pT ) respectively. So it suﬃcies to consider BS = Supp(pS) and BT = Supp(pT ).
Now we verify that CT = Supp(pT ) satisﬁes the requirements in Assumption 4.2. According to Assumption 4.4, for any x ∈ Supp(pT ), there must exist ST,i,j such that x ∈ ST,i,j, ST,i,j is connected, f (x ) = i for all x ∈ ST,i,j and ST,i,j ∩ Supp(pS) = ∅. Pick x ∈ ST,i,j ∩ Supp(pS). Such x satisﬁes x ∈ CT ∩ BS with our choice of CT and BS. Since ST,i,j is connected we can ﬁnd a sequence of points x0, ..., xm ∈ ST,i,j with x0 = 0, xm = x and dX (xi−1, xi) < for any > 0. As ST,i,j is label consistent we have f (x) = f (x ). Picking = ∆L concludes the fact that CT = Supp(pT ) satisﬁes the requirements in Assumption 4.2.
Since pT (Supp(pT )) = 1 we have δ3 = 0. As a result, ET (φ, h) ≤ (1 + β)ES(φ, h) holds according to Theorem 4.3, which concludes the proof of Corollary 4.5.

22

Derivation of (6). The Fenchel Dual of f¯β(u) can be written as

f¯β∗(t) = = =

tf −1(t) − f¯β(f −1(t)) +∞

if t ≤ f ( 1+1β ) , if t > f ( 1+1β ) .

tf −1(t) − f (f −1(t)) + C +∞

if t ≤ f ( 1+1β ) , if t > f ( 1+1β ) .

f ∗(t) + Cf,β +∞

if t ≤ f ( 1+1β ) , , if t > f ( 1+1β ) .

where Cf,β = f ( 1+1β ) − f ( 1+1β ) 1+1β + f ( 1+1β ). Therefore, the modiﬁed f¯β-divergence can be written as

Df,β(p, q) =

sup

Ez∼q [T (z)] − Ez∼p [f ∗(T (z))] − Cf,β .

T :Z→dom(f ∗)∩(−∞,f ( 1+1β )]

Derivation of (7). According to Nowozin et al. (2016), the GAN objecitve uses f (u) = u log u−(1+u) log(1+u). Hence f ∗(t) = − log(1−et), f (u) = log u+u 1 and f ( 1+1β ) = log 2+1β . So we need to parameterize T : Z → − ∞, log 2+1β . T (z) = log 2g+(zβ) with g(z) ∈ (0, 1] satisﬁes the range constraint for T . Plugging T (z) = log 2g+(zβ) into (6) gives the result of (7).

B Experiment Details
Synthetic datasets For source distribution, we sample class 0 from N ([−1, −0.3], diag(0.1, 0.4)) and class 1 from N ([1, 0.3], diag(0.1, 0.4)). For target distribution, we sample class 0 from N ([−0.3, −1], diag(0.4, 0.1)) and class 1 from N ([0.3, 1], diag(0.4, 0.1)). For label classiﬁer, we use a fully-connect neural net with 3 hidden layers (50, 50, 2) and the latent space is set as the last hidden layer. For domain classiﬁer (critic) we use a fully-connect neural net with 2 hidden layers (50, 50).
Image datasets For MNIST we subsample 2000 data points and for USPS we subsample 1800 data points. The subsampling process depends on the given label distribution (e.g. shift or no-shift). For label classiﬁer, we use LeNet and the latent space is set as the last hidden layer. For domain classiﬁer (critic) we use a fully-connect neural net with 2 hidden layers (500, 500).
In all experiments, we use λ = 1 in the objective (4) and ADAM with learning rate 0.0001 and β1 = 0.5 as the optimizer. We also apply a l2-regularization on the weights of φ and h with coeﬃcient 0.001.
More discussion on synthetic experiments. The only unexcepted failure is WDANN1-2, which achieves only 20% accuracy in 2-out-of-5 runs. Looking in to the low accuracy runs we found that the l2-norm of the encoder weights is clearly higher than the successful runs. Large
23

l2-norm of weights in φ likely results in a high Lipschitz constant L, which is undesirable according to our theory. We only implemented l2-regularization to encourage Lipschitz continuity of the encoder φ, which might be insuﬃcient. How to enforce Lipschitz continuity of a neural network is still an open question. Trying more sophisticated approaches for Lipschitz continuity can a future direction. Choice of β. Since a good value of β may depend on the knowledge of target label distribution which is unknown, we experiment with diﬀerent values of β. Empirically we did not ﬁnd any clear pattern of correlation between value of β and performance as long as it is big enough to accommodate label distribution shift so we would leave it as an open question. In practice we suggest to use a moderate value such as 2 or 4, or estimate based on prior knowledge of target label distribution.
24

