Joint part-of-speech and dependency projection from multiple sources
Anders Johannsen Zˇ eljko Agic´ Anders Søgaard Center for Language Technology, University of Copenhagen, Denmark
anders@johannsen.com

Abstract
Most previous work on annotation projection has been limited to a subset of IndoEuropean languages, using only a single source language, and projecting annotation for one task at a time. In contrast, we present an Integer Linear Programming (ILP) algorithm that simultaneously projects annotation for multiple tasks from multiple source languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another.
1 Introduction
Cross-language annotation projection for unsupervised POS tagging and syntactic parsing was introduced ﬁfteen years ago (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015).
Despite the maturity of the ﬁeld, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which covers languages spoken in the European parliament, annotation projection is
1All previous work that we are aware of—with the possible exception of McDonald et al. (2011); but see Sections 2 and 5—uses only a single source (m = 1), but in our experiments, we use multiple source languages.

typically limited to the subset of Indo-European languages that have treebanks.
Previous work is also limited in another respect. While treebanks typically contain multiple layers of annotation, previous work has focused on projecting data for a single task.
We go signiﬁcantly beyond previous work in two ways: 1) by considering multi-source projection across languages in parallel corpora that are available for hundreds of languages, including many non-Indo-European languages; and 2) by jointly projecting annotation for two mutually dependent tasks, namely POS tagging and dependency parsing. Using multiple source languages makes our projections denser. In single source projection, the source language may not contain all syntactic phenomena of the target language; we combat this by transferring syntactic information from multiple source languages. Our work also differs from previous work on annotation projection in projecting soft rather than hard constraints, i.e., scores rather than labels and edges.
Contributions We present a novel ILP-based algorithm for jointly projecting POS labels and dependency annotations across word-aligned parallel corpora. The performance of our algorithm compares favorably to that of a state-of-the-art projection algorithm, as well as to multi-source delexicalized transfer. Our experiments include between 23 and 27 languages using two parallel corpora that are available for hundreds of languages, namely a collection of Bibles and Watchtower periodicals. Finally, we make both the parallel corpora and the code publicly available.2
2 Projection algorithm
The projection algorithm is divided into two distinct steps. First, we project potential syntactic
2https://bitbucket.org/lowlands/ release

561
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 561–566, Berlin, Germany, August 7-12, 2016. c 2016 Association for Computational Linguistics

edges and POS tags from all source languages into an intermediate target graph, which is left deliberately ambiguous. In the second step, we decode the target graph by solving a constrained optimisation problem, which simultaneously resolves all ambiguities and produces a single dependency tree with a ﬁxed set of POS tags. Below we describe both steps in more detail.
2.1 Cross-language sentence
The input to our projection algorithm is a crosslanguage sentence, a data structure that ties together a collection of aligned sentences from a parallel corpus, i.e., sentences in many different languages that are determined to be translation equivalents. One sentence of the set is designated as the target while the rest are sources. We project syntactic information from the sources to the target.
All source sentences are automatically parsed with a graph-based dependency parser and labeled with parts of speech. Instead of using the single best dependency tree output by the parser, we extract its scoring matrix, an ambiguous structure that assigns a numeric score to each potential dependency edge. The target sentence is not parsed or POS-tagged. In fact, our approach is explicitly designed to work for target languages where no such resources are available. Only unsupervised word alignments couple the target sentence with each source sentence.
More formally, a cross-language sentence may be represented as a graph G = (V, E), where each vertex is a POS-tagged token of a sentence in some language. With one target and n source languages, the total set of tagged word vertices V can be written as the union of sentence vertices: V = V0 ∪ . . . ∪ Vn. The target sentence is Vt = V0, while source sentences are Vs = V1 ∪ . . . ∪ Vn.
Two kinds of weighted edges connect the graph. Edges that go between tagged tokens of a sentence Vi represent potential dependency edges. Thus, for the sentence i, the induced subgraph G[Vi] is the (ambiguous) dependency graph. Edges connecting a source vertex to target vertex represent word alignments. The set of alignment edges is A ⊆ Vs × Vt.
To account for POS we introduce a vertex labeling function l : V → Σ, where Σ is the POS vocabulary. The source sentences are automatically tagged, and for any source vertex the label function simply returns this tag. For the target sentence

the POS labels are unknown, which is to say that every target token is ambiguous between |Σ| POS tags. We represent this ambiguity in the graph by creating a vertex for each possible combination of target word and POS. Concretely, if a source sentence i has n tokens, and the target sentence has m tokens, then |Vi| = n, and |Vs| = m|Σ|.
Alignments are constrained such that an alignment (u, v) ∈ Vs × Vt only exists if the source and target token were linked by the automatic aligner and l(u) = l(v), i.e., the POS tags match. This ﬁlters out potential source relations with dissimilar syntax, a luxury that we are allowed in a multiple source language setup.
2.2 Projecting to ambiguous target graph
The target graph G[Vt] starts out empty and is populated with edges in the following way. We go through the source sentences, looking for potential dependency edges where both endpoints are aligned to the target sentence, and transferring the edge whenever we ﬁnd one. Technically, for every source sentence i and for each edge in the source graph (us, vs) ∈ G[Vi], we create an edge (ut, vt) in the target iff both (us, ut) and (vs, vt) ∈ A. The edge weight is the source edge score (as determined by an automatic parser) weighted by the joint alignment probability of (us, ut) and (vs, vt):
d(ut, vt) = max a(us, vs) a(us, ut) d(vs, vt).
us,vs
For clarity, d refers to weights of dependency edges, and a to alignment edge weights. Multiple source sentences may project the same edge to the target graph. When this happens we update the target edge weight only if the new weight is larger than the existing. The weight then reﬂects the strongest evidence found for a given syntactic relation across all source languages.
2.3 Decoding the target graph
We are now ready to decode the target graph. The result of decoding is a dependency tree as well as a labeling of the target sentences with POS tags. Labeling with POS corresponds to selecting a subset of the vertices V˜ ⊂ Vt, such that exactly one vertex is chosen for each token. Similarly the decoded dependency tree is a subset of the projected target edges with the constraint that it must form a tree over the vertices of V˜ . The joint optimization objective is to simultaneously select a set of vertices V˜ and edges E˜ to maximize the score of

562

the decoded tree. We solve this constrained optimization problem by casting it as an integer linear programming (ILP) problem.
The full speciﬁcation of the ILP model is displayed as Figure 1. The model is optimized over two types of binary decision variables mapping directly to the target graph representation discussed in the previous section, plus additional ﬂow variables that enforce tree structure. An edge variable ei,k,j,l represents a target edge (i, j) where the POS of i is k and the POS of j is l. For instance, the variable e2,V,1,N represents a directed edge from the second token (a verb) to the ﬁrst (a noun). An active vertex variable vi,k indicates that the POS of token i is chosen as k.
Following Martins (2012), we constrain the search space to spanning trees by using a singlecommodity-ﬂow construction. In the commodityﬂow analogy, we imagine the root as a factory that produces n commodities (for an n token sentence) which are distributed along the edges of the tree. Each token is a consumer that must receive and pass on all except one commodity to its dependents, i.e., the difference between incoming and outgoing ﬂow should be 1. Since all commodities must be consumed, the outgoing ﬂow for a leaf node will be zero. Together with the requirement that each token must have exactly one head, this ensures all tokens are connected to the root in the tree structure.
The last two constraint groups enforce edge and POS consistency, and the selection of single POS per token. Both are new to this work.
3 Data sources
Our projection requires parallel text, ideally spanning a large number of languages, and dependency treebanks for the sources.
Treebanks To train the source-side taggers and dependency parsers, and to evaluate the crosslingual taggers and parsers, we use the Universal Dependencies (UD) version 1.2 treebanks with the corresponding test sets.3
Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While
3http://hdl.handle.net/11234/1-1548 4https://www.jw.org/

ILP model

Edges Vertices
Flow

ei,k,j,l vi,k φi,k,j,l

∈ {0, 1}
∈ {0, 1} ∈ R+

Maximize

ei,k,j,l wi,k,j,l
i,k,j,l

One parent per token

ei,k,j,l = 1

∀j = 0

i,k,l

The root token (index 0) sends n ﬂow

φ0,0,j,l = n
j,l
Each token consumes one unit of ﬂow

φi,k,x,l − φx,k,j,l = 1

i,k,l

k,j,l

One POS per token

∀x = 0

vi,k = 1
k
Active edges choose token POS

∀i = 0

vi,k ≥ ei,k,j,l vi,l ≥ ei,k,j,l

∀i = 0, j, k, l ∀i, j, k, l

Above, i, j, and x are token indices, while k and l refer to POS. Quantiﬁcation over these symbols in the equations are always with respect to a given target graph.
Figure 1: Speciﬁcation of the ILP model. We list, in order, the decision variables, the objective, and the ﬁve groups of constraint templates.
the two collections span more than 100 languages, we focus on the subsets that overlap with the UD languages to facilitate evaluation. For EBC, that amounts to 27 languages, and 23 for WTC.
Preprocessing We use simple sentence splitting and tokenization models to segment the parallel corpora.5 To sentence- and word-align the individual language pairs, we use a Gibbs samplingbased IBM1 alignment model called efmaral (O¨ stling, 2015). IBM1 has been shown to lead to more robust alignments across typologically distant language pairs (O¨ stling, 2015). We modify
5https://github.com/bplank/ multilingualtokenizer

563

the aligner to output alignment probabilities. All the source-side texts are POS-tagged and dependency parsed using TnT (Brants, 2000) and TurboParser (Martins et al., 2013). We use our own fork of the arc-factored TurboParser to output the edge weight matrices.6
4 Experiments
4.1 Setup
In our experiments, as in the preprocessing, we use the TnT tagger and the arc-factored TurboParser, which we train on the EBC and WTC texts with projected and decoded annotations. We randomly sample up to 20k sentences per training ﬁle in both tagging and parsing. This 20k sampling limit applies to all systems.
We compare two cross-lingual projection-based parsing systems, and one baseline system.
ILP The ILP-based joint projection algorithm we presented in Section 2.
DCA Our implementation of the de facto standard annotation projection algorithm of Hwa et al. (2005), as reﬁned by Tiedemann (2014). In contrast to our ILP approach, it uses heuristics to ensure dependency tree constraints on a sourcetarget sentence pair basis. We gather all the pairwise projections into a target sentence graph and then perform maximum spanning tree decoding following Sagae and Lavie (2006).
DELEX The multi-source direct delexicalized transfer baseline of McDonald et al. (2011). Each source is represented by an approximately equal number of sentences.
4.2 Results
Table 1 provides a summary of dependency parsing scores. We report UAS scores over predicted and gold POS. The predicted tags come from our cross-lingual taggers. Our ILP approach consistently outperforms DCA on both by a large margin of 3-5 points UAS using predicted POS, and 5-10 points on gold POS. Note that DELEX is trained on gold POS and therefore has an advantage in this
6https://github.com/andersjo/ TurboParser
8We do not include DELEX in the comparison for the gold POS scenario only. In this particular scenario, DELEX is also trained on gold POS, and thus biased: the cross-lingual taggers do not have gold POS available for training, and the same holds for DELEX and projected POS.

Predicted POS EBC WTC
Gold POS EBC WTC

ILP 51.62 (18) 53.58 (20)

Approach
DCA 48.39 (8) 48.40 (0)

65.43 (25) 59.94 (2) 66.51 (23) 55.73 (0)

DELEX 42.44 (1) 47.35 (3)
64.13 (–) 66.68 (–)

Table 1: Macro-averaged UAS scores summarizing our evaluation. EBC: Edinburgh Bible corpus, WTC: Watchtower corpus. Numbers of languages with top performance per system are reported in brackets. All parsers use their respective EBC or WTC taggers.8

setting. Relying on predicted POS and WTC data, our ILP approach beats DCA for all the test languages. With EBC, we outperform DCA on 19 out of 27 languages.
In Table 2, we split the scores across the test languages and parallel data sources, and we also report the POS tagging accuracies. Our WTC taggers are on average 3.5 points better than EBC taggers, yielding the top score for 16/23 languages from the overlap. Notably, on several non-IndoEuropean languages, we observe signiﬁcant improvements. For example, on Indonesian, DCA improves over DELEX by 12 points UAS, while ILP adds 6 more points on top. We observe a similar pattern for Arabic and Estonian. We note that DELEX tops ILP and DCA on only 1 EBC and 3 WTC languages, and by a narrow margin.
Analysis A projected parse is allowed to be a composite of edges from many source languages. To ﬁnd out to what degree this actually happens, we analyze all projections into English and German on the WTC corpus.
For German the top four source languages are Czech, Norwegian, French, and English, contributing between 16% and 7% of all edges. For English the top languages are Norwegian, Italian, Indonesian, and Swedish. Here, the top language Norwegian is responsible for 42% of the edges, while Swedish accounts for 13%. Only the language projecting the highest scoring edge is counted. On average, a German sentence has edges from 4.1 source languages. The same number for English is slightly higher, at 4.5.
Manually annotated data We annotate a small number of sentences in English from EBC and

564

Language Arabic Basque
Bulgarian Croatian
Czech Danish English * Estonian
Farsi Finnish French German * Greek Hebrew
Hindi * Hungarian
Indonesian Italian * Latin
Norwegian Polish
Portuguese * Romanian
* Serbian Slovene Spanish Swedish
Average Best for

POS tagging

EBC 39.54 43.43 76.45 72.83 70.81 76.43 71.90 75.55 64.94 70.41 74.25 74.36 56.52 43.65 59.99 71.57 63.30 79.28 83.41 77.00 73.36 78.41 71.56 74.07 75.68 76.72 78.26

WTC 53.91
– 68.27 76.18 78.49 86.36 79.2 73.98 25.67 67.44 79.23 68.36 75.75
– 48.86 71.42 75.61 83.82
– 85.31 73.68 83.67 76.34
– 78.11 85.69 84.80

69.40 73.05

7

16

ILP 36.59 22.77 50.6 54.19 52.67 61.14 55.76 62.9 23.53 43.66 53.52 45.02 62.59 30.25 18.26 49.74 51.99 63.13 68.65 65.04 62.94 63.75 57.74 49.15 59.17 63.63 65.24
51.62 18

Dependency parsing

EBC

WTC

DCA 37.70 17.38 60.03 45.08 41.44 53.22 50.64 56.95 42.37 44.51 53.11 50.21 37.73 39.76 35.59 44.97 23.53 58.66 68.45 58.32 59.27 60.45 56.73 49.38 53.66 52.20 55.21

DELEX 13.17 27.85 57.83 42.34 40.99 49.65 48.04 49.32 28.93 41.18 48.97 49.36 37.11 19.06 21.03 43.07 31.18 53.94 41.42 53.46 53.33 52.91 45.73 47.06 50.55 47.6 50.85

ILP 37.41
– 49.68 55.16 53.09 61.78 58.70 63.85 20.34 42.59 55.69 43.99 62.43
– 15.95 44.17 58.01 64.88
– 66.54 63.74 64.62 58.76
– 59.79 64.93 66.15

DCA 32.14
– 37.18 50.56 44.36 58.64 57.12 58.41 12.26 35.6 51.47 36.7 52.95
– 10.77 42.33 52.29 63.57
– 64.37 55.4 63.16 54.78
– 54.8 61.90 62.45

48.39 8

42.44 1

53.58 48.40

20

0

DELEX 21.15
– 48.37 45.49 47.99 55.96 53.87 48.48 19.48 41.52 51.53 45.79 54.90
– 21.04 46.66 39.67 58.06
– 60.11 54.87 56.99 51.23
– 52.53 55.87 57.48
47.35 3

Table 2: Tagging and parsing (UAS) accuracy. Scores are macro-averaged, and all parsers use predicted POS from respective EBC or WTC taggers. *: True target languages, not used as sources.

WTC, which gives us a way to directly evaluate the projections without training parsers. On this small test set of 2 × 50 sentences, we obtain UAS scores of 68% (WTC) and 62% (EBC). The POS accuracies are 79% and 80%. All ﬁgures are comparable to the results from the indirect projection evaluation.
5 Related work
In recent years, we note an increased interest for work in cross-lingual processing, and particularly in POS tagging and dependency parsing of lowresource languages.
Yarowsky et al. (2001) proposed the idea of inducing NLP tools via parallel corpora. Their contribution started a line of work in annotation projection. Das and Petrov (2011) used graph-based label propagation to yield competitive POS taggers, while Hwa et al. (2005) introduced the projection of dependency trees. Tiedemann (2014) further improved this approach to single-source projection in the context of synthesizing dependency treebanks (Tiedemann and Agic´, 2016). The current state of the art in cross-lingual dependency parsing also involves exploiting large parallel corpora (Ma and Xia, 2014; Rasooli and

Collins, 2015). Transferring models by training parsers with-
out lexical features was ﬁrst introduced by Zeman and Resnik (2008). McDonald et al. (2011) and Søgaard (2011) coupled delexicalization with contributions from multiple sources, while McDonald et al. (2013) were the ﬁrst to leverage uniform representations of POS and syntactic dependencies in cross-lingual parsing.
Even more recently, Agic´ et al. (2015) exposed a bias towards closely related Indo-European languages shared by most previous work on annotation projection, while introducing a bias-free projection algorithm for learning 100 POS taggers from multiple sources. Their line of work is non-trivially extended to multilingual dependency parsing by Agic´ et al. (2016).
The work in annotation projection for crosslingual NLP invariably treats mutually dependent layers of annotation separately. Our contribution is distinct from these works by implementing the ﬁrst approach to joint projection of POS and dependencies, while maintaining the outlook on processing truly low-resource languages.
6 Conclusion
In our contribution, we addressed tagging and parsing for low-resource languages through joint cross-lingual projection of POS tags and syntactic dependencies from multiple source languages. Our novel approach to transferring the annotations via word alignments is based on integer linear programming, more speciﬁcally on a commodityﬂow formalization for spanning trees.
In our experiments with 27 treebanks from the Universal Dependencies (UD) project, our approach compared very favorably to two competitive cross-lingual systems: we provided the best cross-lingual taggers and parsers for 18/27 and 20/23 languages, depending on the parallel corpora used. We made no unrealistic assumptions as to the availability of parallel texts and preprocessing tools for the target languages. Our code and data is freely available.9
Acknowledgements This research is partially funded by the ERC Starting Grant LOWLANDS (#313695).
9https://bitbucket.org/lowlands/ release

565

References
Zˇ eljko Agic´, Dirk Hovy, and Anders Søgaard. 2015. If All You Have is a Bit of the Bible: Learning POS Taggers for Truly Low-Resource Languages. In ACL.
Zˇ eljko Agic´, Anders Johannsen, Barbara Plank, He´ctor Mart´ınez Alonso, Natalie Schluter, and Anders Søgaard. 2016. Multilingual Projection for Parsing Truly Low-Resource Languages. TACL, 4.
Thorsten Brants. 2000. TnT – A Statistical Part-ofSpeech Tagger. In ANLP.
Christos Christodouloupoulos and Mark Steedman. 2014. A Massively Parallel Corpus: The Bible in 100 Languages. Language Resources and Evaluation, 49(2).
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections. In ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping Parsers via Syntactic Projection Across Parallel Texts. Natural Language Engineering, 11(3).
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit.
Xuezhe Ma and Fei Xia. 2014. Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization. In ACL.
Andre´ F. T. Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the Turbo: Fast ThirdOrder Non-Projective Turbo Parsers. In ACL.
Andre´ F. T. Martins. 2012. The Geometry of Constrained Structured Prediction: Applications to Inference and Learning of Natural Language Syntax. Ph.D. thesis, Carnegie Mellon University and Instituto Superior Tecnico.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-Source Transfer of Delexicalized Dependency Parsers. In EMNLP.
Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Ta¨ckstro¨m, Claudia Bedini, Nu´ria Bertomeu Castello´, and Jungmee Lee. 2013. Universal Dependency Annotation for Multilingual Parsing. In ACL.
Robert O¨ stling. 2015. Word Order Typology through Multilingual Word Alignment. In ACL.
Mohammad Sadegh Rasooli and Michael Collins. 2015. Density-Driven Cross-Lingual Transfer of Dependency Parsers. In EMNLP.

Kenji Sagae and Alon Lavie. 2006. Parser Combination by Reparsing. In NAACL.
Anders Søgaard. 2011. Data Point Selection for CrossLanguage Adaptation of Dependency Parsers. In ACL.
Jo¨rg Tiedemann and Zˇ eljko Agic´. 2016. Synthetic Treebanking for Cross-Lingual Dependency Parsing. Journal of Artiﬁcial Intelligence Research, 55.
Jo¨rg Tiedemann. 2014. Rediscovering Annotation Projection for Cross-Lingual Parser Induction. In COLING.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing Multilingual Text Analysis Tools via Robust Projection Across Aligned Corpora. In NAACL.
Daniel Zeman and Philip Resnik. 2008. CrossLanguage Parser Adaptation between Related Languages. In IJCNLP Workshop on NLP for Less Privileged Languages.

566

