Good Classiﬁcation Measures and How to Find Them

arXiv:2201.09044v1 [cs.LG] 22 Jan 2022

Martijn Gösgens Eindhoven University of Technology
Eindhoven, The Netherlands research@martijngosgens.nl

Anton Zhiyanov Yandex Research, HSE University
Moscow, Russia zhiyanovap@gmail.com

Alexey Tikhonov Yandex
Berlin, Germany altsoph@gmail.com

Liudmila Prokhorenkova Yandex Research, HSE University, MIPT
Moscow, Russia ostroumova-la@yandex.ru

Abstract
Several performance measures can be used for evaluating classiﬁcation results: accuracy, F-measure, and many others. Can we say that some of them are better than others, or, ideally, choose one measure that is best in all situations? To answer this question, we conduct a systematic analysis of classiﬁcation performance measures: we formally deﬁne a list of desirable properties and theoretically analyze which measures satisfy which properties. We also prove an impossibility theorem: some desirable properties cannot be simultaneously satisﬁed. Finally, we propose a new family of measures satisfying all desirable properties except one. This family includes the Matthews Correlation Coefﬁcient and a so-called Symmetric Balanced Accuracy that was not previously used in classiﬁcation literature. We believe that our systematic approach gives an important tool to practitioners for adequately evaluating classiﬁcation results.
1 Introduction
Classiﬁcation is a classic machine learning task that is used in countless applications. To evaluate classiﬁcation results, one has to compare the predicted labeling of a given set of elements with the actual (true) labeling. For this, performance measures are used, and there are many well-known ones like accuracy, F-measure, and so on [13, 15]. The fact that different measures behave differently is known throughout the literature [10, 13, 20, 21]. For instance, accuracy is known to be biased towards the majority class. Thus, different measures may lead to different evaluation results, and it is important to choose an appropriate measure. While there are attempts to compare performance measures and describe their properties [3, 6, 13, 14, 26, 28], the problem still lacks a systematic approach, and our paper aims at ﬁlling this gap.1 Our research is particularly motivated by a recent paper [12] providing a systematic analysis of evaluation measures for the clustering task. We transfer many proposed properties to the classiﬁcation problem and extend the research by adding more properties, new measures, and novel theoretical results.
To provide a systematic comparison of performance measures, we formally deﬁne a list of properties that are desirable across various classiﬁcation tasks. The proposed properties can be applied both to binary and multiclass problems. Some properties are intuitive and straightforward, like symmetry, while others are more tricky. A particularly important property is called constant baseline. It requires a measure not to be biased towards particular predicted class sizes. For each measure and each
1We describe related research in detail in Appendix A.
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

property, we formally prove or disprove that the property is satisﬁed. We believe that this analysis is essential for better understanding the differences between the performance measures.
Then, we analyze relations between different properties in the binary case and prove an impossibility theorem: it is impossible for a performance measure to be linearly transformable to a metric and simultaneously have the constant baseline property. This means that at least one of these properties has to be discarded. If we relax the set of properties by discarding the distance requirement, the remaining ones can be simultaneously satisﬁed. In fact, we propose a family of measures called Generalized Means (GM), satisfying all the properties except distance and generalizing the well-known Matthews Correlation Coefﬁcient (CC). In addition to CC, this class also contains another intuitive measure that we name Symmetric Balanced Accuracy. To the best of our knowledge, this measure has not been previously used for classiﬁcation evaluation.2 If we instead discard the constant baseline (but keep its approximation), then the arccosine of CC is a measure satisfying all the properties.
We also demonstrate through a series of experiments that different performance measures can be inconsistent in various situations. We notice that measures having more desirable properties are usually more consistent with each other.
We hope that our research will motivate further studies analyzing the properties of performance measures for classiﬁcation and other problems since there are still plenty of questions to be answered.

2 Performance measures for classiﬁcation

In this section, we deﬁne measures that are commonly used for evaluating classiﬁcation results. Classiﬁcation problems can be divided into binary, multiclass, and multilabel. In this paper, we focus on binary and multiclass and leave multilabel for future research. There are several types of performance measures: threshold measures assume that predicted labels deterministically assign each element to a class (e.g., accuracy); probability measures assume that the predicted labels are soft and compare these probabilities with the actual outcome (e.g., the cross-entropy loss); ranking measures take into account the relative order of the predicted soft labels, i.e., quantify whether the elements belonging to a class have higher predicted probabilities compared to other elements (e.g., area under the ROC curve or average precision). Our research focuses on threshold measures.

Now we introduce notation needed to formally deﬁne binary and multiclass threshold measures.3

Let n > 0 be the number of elements in the dataset and let m ≥ 2 denote the number of classes. We

assume that there is true labeling classifying elements into m classes and also predicted labeling. Let

C be the confusion matrix: each matrix element cij denotes the number of elements with true label i

and predicted label j. For binary classiﬁcation, c11 is true positive (TP), c00 is true negative (TN),

c10 is false negative (FN), and c01 is false positive (FP). We use the notation ai =

m−1 j=0

cij , bi

=

m−1 j=0

cji

for

the

sizes

of

i-th

class

in

the

true

and

predicted

labelings,

respectively.

Finally,

we

denote classiﬁcation measures by M (C) or M (A, B), where A and B are true and predicted labelings,

and write M (c11, c10, c01, c00) for binary ones.

Table 1 (above the line) lists several widely used classiﬁcation measures. The most well-known is accuracy which is the fraction of correctly classiﬁed elements. Accuracy is known to be biased towards the majority class, so it is not appropriate for unbalanced problems. To overcome this, Balanced Accuracy re-weights the terms to treat all classes equally. Cohen’s Kappa uses a different approach to overcome this bias: it corrects the number of correctly classiﬁed samples by the expected value obtained by a random classiﬁer [5]. Matthews Correlation Coefﬁcient is the Pearson correlation coefﬁcient between true and predicted labelings for binary classiﬁcation [11]. For the multiclass case, covariance is computed for each class, and the obtained values are averaged before computing the correlation coefﬁcient. Finally, Confusion Entropy computes the entropy of the misclassiﬁcation distribution for each class and combines the obtained values, see Table 1 and [30] for the details.4

Some measures are exclusively deﬁned for binary classiﬁcation. In this case, the classes are often referred to as ‘positive’ and ‘negative’. Jaccard measures the fraction of correctly detected positive

2For clustering evaluation, there is an analog known as Sokal&Sneath’s measure [12]. 3For convenience, we list the notation used in the paper in Table 7 in Appendix. 4There can be cases when a class is not present in the predicted labels. Then, some measures may contain
division by zero. A proper way to ﬁll in such singularities is discussed in Appendix B.

2

Table 1: Commonly used (above the line) and novel (below the line) validation measures

F-measure (Fβ) Jaccard (J) Matthews Coefﬁcient (CC)
Accuracy (Acc) Balanced Accuracy (BA) Cohen’s Kappa (κ) Confusion Entropy (CE)

Binary

Multiclass

(1+β 2 )·c11 (1+β 2 )·c11 +β 2 ·c10 +c01
c11 c11 +c10 +c01
c√11 c00 −c01 c10 b1 ·a1 ·b0 ·a0

micro / macro / weighted

micro / macro / weighted

n

m−1 i=0

cii −

m−1 i=0

bi ai

(n2 −

m−1 i=0

b2i )(n2−

m−1 i=0

a2i )

m−1 i=0

cii

n

1 m−1 cii m i=0 ai

n

m−1 i=0

cii −

m−1 i=0

ai bi

n2 −

m−1 i=0

ai bi

− 21n i,j:i=j

cji

log2m−2

cji a +b

+

cij

log2m−2

cij a +b

jj

jj

Symmetric Balanced Accuracy (SBA) Generalized Means (GM) Correlation Distance (CD)

1 2m

n c11−a1b1

r

1 2

(ar1

ar0

+br1

br0

)

m−1 i=0

+ cii

cii

ai

bi

micro / macro / weighted

π1 arccos(CC)

examples among all positive ones (both in true and predicted labelings). F-measure is the (possibly weighted) harmonic mean of Recall (c11/a1) and Precision (c11/b1). For measures that do not have a natural multiclass variant, there are several universal extensions obtained via averaging the results for m one-vs-all binary classiﬁcations [17]. For each one-vs-all classiﬁcation, a particular class i is considered positive while all other classes are grouped to a negative class.

Micro averaging sums up all binary confusion matrices corresponding to m one-vs-all classiﬁcations.

Formally, it sets true positive as

m−1 i=0

cii,

false

negative

and

false

positive

as

n

−

m−1 i=0

cii,

true

negative as (m − 2)n +

m−1 i=0

cii.

Then,

a

given

binary

measure

is

applied

to

the

obtained

matrix.

Macro averaging computes the measure values for m binary classiﬁcation sub-problems and then

averages the results: m1

m−1 i=0

M

(cii,

ai

−

cii,

bi

−

cii,

n

−

ai

−

bi

+

cii),

where

M

(·)

is

a

given

binary measure. Note that macro averaging gives equal weights to all one-vs-all binary classiﬁcations.

In contrast, weighted averaging weights one-vs-all binary classiﬁcations according to the sizes of the

corresponding classes: n1

m−1 i=0

ai

·

M

(cii,

ai

−

cii,

bi

−

cii,

n

−

ai

−

bi

+

cii).

3 Properties of validation measures

As clearly seen from the above discussion, there are many options for classiﬁcation validation. In this section, we propose a formal approach that allows for a better understanding the differences between the measures and for making an informed decision among them for a particular application. For this, we propose properties of validation measures that can be useful across various applications and formally check which measures satisfy which properties. In this regard, we follow the approach proposed in [12] for comparing validation measures for clustering tasks.
First, we observe that some theoretical results from [12] are related to binary classiﬁcation measures. Indeed, a popular subclass of clustering validation measures are pair-counting ones. Such measures are deﬁned in terms of the values N11, N10, N01, N00 that essentially deﬁne a confusion matrix for binary classiﬁcation on element pairs. Thus, replacing Nij in pair-counting clustering measures by cij, results in binary classiﬁcation measures. We refer to Table 8 in Appendix B for the correspondence of some classiﬁcation and clustering measures. In particular, Accuracy is equivalent to Rand, while Cohen’s Kappa corresponds to Adjusted Rand. This equivalence allows us to transfer some of the results from [12] to the context of binary classiﬁcation. However, an important contribution of our work is the extension of the properties and analysis to the multiclass case. We also prove an impossibility theorem stating that some of the desirable properties cannot be simultaneously satisﬁed and develop a new family of measures having all properties except one.

3

Table 2: Properties of validation measures and averagings, / indicates that property is satisﬁed only in binary case

Measure

Max Min CSym Sym Dist Mon SMon CB ACB

F1 (binary)  



J (binary)





CC

 / 

Acc





BA





κ





CE

















  / /  













 



 /















SBA





GM (binary)  



CD

 / 



 



 

  / /  

Preserving properties by various averaging types

Micro Macro Weighted



































Similarly to [12], we note that all the discussed properties are invariant under linear transformations and interchanging true and predicted labelings. Hence, we may restrict to measures for which higher values indicate higher similarity between classiﬁcations.
Table 2 summarizes our ﬁndings: for each measure, we mathematically prove or disprove each desirable property. Further in this section, we refer only to known measures (above the line), while the remaining ones will be deﬁned and analyzed in Section 4. In addition to individual measures, we also analyze the properties of micro, macro, and weighted multiclass averagings: for each averaging, we analyze whether it preserves a given property, assuming the binary classiﬁcation measure satisﬁes it. All the proofs can be found in Appendix C. Let us now deﬁne and motivate each property.

3.1 Maximal and minimal agreement

These properties make the upper and lower range of a performance measure interpretable. The maximal agreement property requires the measure to have an upper bound that is only achieved when the compared labelings are identical.
Deﬁnition 1. We say that a measure M satisﬁes maximal agreement if there exists a constant cmax such that for all C, M (C) ≤ cmax with equality iff C is diagonal.

Also, for a given true labeling, there are several “worst” predictions, i.e., labelings that are wrong everywhere. This leads to the following property.
Deﬁnition 2. We say that a measure M satisﬁes minimal agreement if there exists a constant cmin such that for all C, M (C) ≥ cmin with equality iff the diagonal of C is zero, i.e., cii = 0 for all i.

These properties allow for an easy and intuitive interpretation of the measure’s values. While all of

the measures in Table 2 do satisfy maximal agreement, there are popular measures such as Recall

(c11/a1) and Precision (c11/b1) that do not satisfy this property as the maximum can also be achieved

when the compared classiﬁcations are not identical. For minimal agreement, many performance

measures violate it. For example, Cohen’s Kappa is obtained from accuracy by subtracting the

expected value of accuracy and normalizing the result. As a result of the particular normalization

used, it has minimal value −

m−1 i=0

aibi

/

n2 −

m−1 i=0

aibi

, which is clearly not constant.

If a binary measure satisﬁes maximal agreement, then its multiclass variant obtained via micro, macro, or weighted averaging also satisﬁes this property as each one-vs-all binary classiﬁcation agrees maximally. However, this does not hold for minimal agreement: though each one-vs-all binary classiﬁcation will have zero true positives, the number of true negatives may still be positive.

4

3.2 Symmetry
Deﬁnition 3. We say that a measure M is symmetric if M (C) = M (CT ) holds for all C.
In other words, we require symmetry with respect to interchanging predicted and true labels. This property is often desirable since similarity is usually understood as a symmetric concept. However, in some speciﬁc applications, there may be reasons to treat the true and predicted labelings differently and thus use an asymmetric measure. An example of an asymmetric measure is Balanced Accuracy.
Let us also introduce class-symmetry, i.e., invariance to permuting the classes. Deﬁnition 4. We say that a measure M is class-symmetric if, for any permutation π of the classes {1, . . . , m} and any confusion matrix C, M (C) = M (C˜) holds, where C˜ is given by c˜ij = cπ(i),π(j).
Note that known multiclass measures are all class-symmetric, while in binary classiﬁcation tasks, there can be an asymmetry between ‘positive’ and ‘negative’ classes. Examples of well-known class-asymmetric binary classiﬁcation measures are Jaccard and F1.
3.3 Distance
In some applications, it is desirable to have a distance interpretation of a measure: whenever a labeling A is similar to B, while B is similar to C, it should intuitively hold that A is also somewhat similar to C. For instance, it can be the case that the actual labels are unknown, and the labeling A is only an approximation of the truth. Then, we would want the similarity between predicted labels and A to be not too different from the similarity between predicted and the actual true labels. This would be guaranteed if the measure is a distance. Deﬁnition 5. A measure has distance property if it can be linearly transformed to a metric distance.
A function d(A, B) is a metric distance if it is symmetric, nonnegative, equals zero only when A = B, and satisﬁes the triangle inequality d(A, C) ≤ d(A, B) + d(B, C). Note that the ﬁrst requirement is equivalent to symmetry (Deﬁnition 3), while the second and third imply maximal agreement (Deﬁnition 1). Furthermore, note that if d is a distance, then c · d is also a distance for any c > 0. Therefore, we can conclude that M is a distance if and only if M satisﬁes symmetry and maximal agreement while cmax − M (A, B) satisﬁes the triangle inequality.
While most measures cannot be linearly transformed to a distance, some measures do satisfy this property. For example, the Jaccard measure can be transformed to the Jaccard distance 1 − J(A, B). Similarly, Accuracy can be transformed to a distance by 1 − Acc(A, B).
3.4 Monotonicity
Monotonicity is one of the most important properties of a similarity measure: intuitively, changing one labeling such that it becomes more similar to the other ought to increase the similarity score. Then, to formalize monotonicity, we need to determine what changes make the classiﬁcations A and B more similar to each other. The simplest option is to take one element on which A and B disagree and resolve this disagreement. Deﬁnition 6. A measure M is monotone if M (C) < M (C˜) for any confusion matrices C and C˜ such that C˜ is obtained from C by decrementing an off-diagonal entry cab and incrementing caa or cbb and none of the row- or column-sums of C equal n.
The condition on C is equivalent to neither A nor B labeling all elements to the same class. We need this to prevent contradictions with the constant baseline property that will be deﬁned in Section 3.5.
Deﬁnition 6 deﬁnes a partial ordering over confusion matrices with the same total number of elements. However, we can relax the latter restriction and obtain the following, stronger notion of monotonicity that deﬁnes a partial ordering across different numbers of elements. Deﬁnition 7. A measure M is strongly monotone if M (C) < M (C˜) for any confusion matrices C and C˜ such that C˜ is obtained from C by either increasing a diagonal entry or decreasing an off-diagonal entry. Here we require that none of the row- or column-sums of C equal n and that C and C˜ are not simultaneously diagonal or zero-diagonal matrices.
5

The last condition is needed since otherwise the deﬁnition would contradict the maximal (or minimal) agreement properties as M (C) = cmax ≥ M (C˜) holds when C is diagonal.
All measures in Table 2 except for CE and multiclass κ, CC and CD satisfy monotonicity from Deﬁnition 6. Strong monotonicity is violated by many measures: for instance, the widely used F1, Jaccard and Cohen’s Kappa do not satisfy this intuitive property.
3.5 Constant baseline
The constant baseline is perhaps the most important non-trivial property. On the one hand, it ensures that a measure is not biased towards labelings with particular class sizes b1, . . . , bm. On the other hand, it also ensures some interpretability for ‘mediocre’ predictions.
Intuitively, if predicted labels are drawn at random and independently of the true labels, we would expect them to have a low similarity with the true labels. Then, if another prediction has a similarly low score, we can say that it is roughly as bad as a random guess. However, this is only possible when such random classiﬁcations achieve similar scores, independent of their class sizes. To formalize this, let U (b1, . . . , bm) denote the uniform distribution over labelings with class sizes b1, . . . , bm. We say that the class sizes b1, . . . , bm are unary if bi = n for some i ∈ {1, . . . , m}. That is, if all elements get classiﬁed to the same class, so that U (b1, . . . , bm) is a constant distribution. Deﬁnition 8. We say that a measure M has a constant baseline property if there exists cbase(m) that does not depend on n but may depend on m, such that for any true labeling A and non-unary class sizes b1, . . . , bm, it holds that EB∼U(b1,...,bm)[M (A, B)] = cbase(m).
Note that we need to require the class sizes to be non-unary: if these class sizes are unary, we will have contradictions with maximal and minimal agreement when the class sizes of A are also unary. Many popular measures such as F1, Accuracy, and Jaccard do not have a constant baseline. Furthermore, some measures that do have a constant baseline were deliberately designed to have one. For example, Cohen’s Kappa was obtained from accuracy by correcting it for chance. While our deﬁnition of the constant baseline does allow for a baseline cbase(m) that depends on the number of classes m, some measures such as the Matthews Coefﬁcient and Cohen’s Kappa have a baseline that is constant w.r.t. m.
All of the measures that satisfy constant baseline turn out to be linear functions of cii for ﬁxed class sizes a1, . . . , am and b1, . . . , bm. For such measures, linearity of the expectation can be utilized to easily compute the baseline by substituting the expected values EB∼U(b1,...,bm)[cii] = ainbi . Thus, we also propose the following relaxation of the constant baseline property. Deﬁnition 9. A measure M is said to have an approximate constant baseline if there exists a function cbase(m) that does not depend on n but may depend on m such that for any class sizes a1, . . . , am and any non-unary b1, . . . , bm, M (C¯) = cbase(m), where c¯ij = ainbj .
The advantage of this relaxation is that it allows us to non-linearly transform measures while still maintaining an approximate constant baseline. Take for example the Matthews Correlation Coefﬁcient: it cannot be linearly transformed to a distance while the transformations CD(A, B) = π1 arccos(CC(A, B)) and 2(1 − CC(A, B)) do yield distances. Because Correlation Coefﬁcient has a constant baseline, these non-linear transformations have an approximate constant baseline, see Section 4 for more details.
As can be seen from Table 2, there is no measure satisfying all the properties. In particular, there is no measure having both distance and constant baseline. In the next section, we show why this is not a coincidence.
4 Impossibility theorem for classiﬁcation
In this section, we focus on binary classiﬁcation and more deeply analyze the relations between the properties discussed above. Unfortunately, it turns out that the properties introduced in the previous section cannot all be satisﬁed simultaneously. Theorem 1. There is no binary classiﬁcation measure that simultaneously satisﬁes the monotonicity, distance, and constant baseline properties.
6

Proof. Let A be a labeling with a single positive and n − 1 negatives. Let B1 be a random labeling with a single positive and let B2 be a random labeling with two positives. The constant baseline requires E[M (A, B1)] = E[M (A, B2)], which gives

1

n−1

2

n−2

n cmax + n M (0, 1, 1, n − 2) = n M (1, 0, 1, n − 2) + n M (0, 1, 2, n − 3),

which we rewrite to

2M (1, 0, 1, n − 2) − cmax = (n − 1)M (0, 1, 1, n − 2) − (n − 2)M (0, 1, 2, n − 3). (1)

Now, we consider a labeling C with a single positive that does not coincide with the positive of A and a labeling B that has two positives which are the positives of A and C. The triangle inequality tells us that

cmax−M (0, 1, 1, n−2) ≤ 2cmax−M (1, 1, 0, n−2)−M (1, 0, 1, n−2) = 2(cmax−M (1, 1, 0, n−2)),

where the last step follows from symmetry (implied by distance). This is rewritten to

2M (1, 1, 0, n − 2) − cmax ≤ M (0, 1, 1, n − 2).

(2)

Combining (1) and (2), we obtain

(n − 1)M (0, 1, 1, n − 2) − (n − 2)M (0, 1, 2, n − 3) ≤ M (0, 1, 1, n − 2).

We rewrite this to M (0, 1, 1, n − 2) ≤ M (0, 1, 2, n − 3), which clearly contradicts monotonicity.

Thus, we have to discard one of these properties. Obviously, discarding monotonicity would be highly undesirable since higher values would then not necessarily indicate higher similarity. For this reason, we analyze what happens if we discard either distance or constant baseline. All the results stated below are proven in Appendix D.

Discarding distance Assuming some additional smoothness conditions that are, however, satisﬁed by all measures discussed in this paper, we prove the following result.
Theorem 2. All binary measures that satisfy all properties except distance must be of the form

s a0a1 , b0b1 · nc11 − a1b1 ,

n2 n2

n2

where the normalization factor s(a, b) needs to satisfy some additional properties listed in Theorem 3.

This class of measures is quite wide and contains many unelegant measures. An interesting subclass can be obtained if we normalize by the generalized mean, i.e., take s(a, b)−1 = ( 12 ar + 21 br)1/r.
Deﬁnition 10. For r ∈ R, we deﬁne Generalized Means measures as

GMr = n c11 − a1b1 .

r

1 2

(ar1ar0

+

br1br0)

Statement 1. For any r ∈ R, the measure GMr satisﬁes all properties except for being a distance.

We also show that the Generalized Means measures contain two interesting special cases.
Statement 2. If r → 0 (corresponding to the geometric mean), GMr(C) → CC(C). If r = −1 (corresponding to the harmonic mean), GM−1(C) = BA(C) + BA(C ) − 1 .
Thus, for r = −1 Generalized Means is equivalent to the measure 12 BA(C) + BA(C ) that we call Symmetric Balanced Accuracy (SBA). To the best of our knowledge, this measure has not been used in the classiﬁcation literature. However, in the clustering literature, a similar measure is known as Sokal&Sneath’s measure [1, 12]. Interestingly, SBA preserves its properties for the multiclass case.
Statement 3. SBA satisﬁes all properties except for being a distance for any m ≥ 2.

7

Discarding (exact) constant baseline Note that Theorem 1 only proves an impossibility for the exact constant baseline, but not the approximate constant baseline. Statement 4. The measures CD(A, B) := π1 arccos(CC(A, B)) and CD (A, B) :=
2(1 − CC(A, B)) satisfy all properties except the exact constant baseline, but including the approximate constant baseline.
Following [12], we call the measure π1 arccos(CC(A, B)) Correlation Distance (CD). We prove the following result (see Appendix D.2 for the details).
Statement 5. CD approximates a constant baseline with one order of precision better than CD .
Essentially, this is a consequence of the fact that the transformation π1 arccos(CC) is a symmetric function around the constant baseline CC = 0 while 2(1 − CC) is not. In more detail, we show that the leading error term of CD is of the order E[CC(A, B)2] while the leading error term for CD is of the order E[CC(A, B)3]. Currently, we are not aware of other distance measures for which the constant baseline is approximated up to the same order of precision as CD. We thus argue that for binary classiﬁcation tasks where a distance interpretation is desirable, Correlation Distance is the most suitable measure.

5 Inconsistency of measures in practice

In this section, we conduct several experiments that demonstrate how often performance measures
may disagree in practice in different scenarios. These experiments demonstrate the importance of the
problem considered in this paper and show which measures are usually more consistent than others. For binary classiﬁcation, we consider all measures from Table 1. For F-measure, we take β = 1, for Generalized Means, we consider r = 1. Recall that SBA and CC are also instances of GM with r = −1 and r → 0, respectively. Furthermore, Jaccard is a monotone transformation of F1, and CD is a monotone transformation of CC. Therefore, we omit CD and Jaccard from all inconsistency tables. The code for our experiments can be found on GitHub.5

5.1 Binary measures

Distinguishing measures for small datasets First, we construct simple examples showing the inconsistency of all pairs of binary classiﬁcation measures. We say that two measures M1 and M2 are consistent on a triplet of classiﬁcations (A, B1, B2) if M1(A, B1) ∗ M1(A, B2) implies M2(A, B1)∗M2(A, B2), where ∗ ∈ {>, <, =}. Otherwise, we say that the measures are inconsistent. We took n ∈ {2, 3, . . . , 10} and went through all the possible triplets (A, B1, B2) of binary labelings of n elements (we additionally require that all labelings contain both classes). For each triplet, we
check which pairs of measures are inconsistent. We say that a pair of measures is indistinguishable for a given n if it is consistent on all triplets.

Table 3 lists all measures that are indistinguishable for a given n. For instance, for n = 2, all measures are always consistent. For n = 4, we can distinguish Acc, F1, and CE from other measures and each other. Interestingly, the remaining measures are those having the constant baseline property. Importantly, the most consistent measures are CC, SBA, and GM1 — these measures have the best properties according to our analysis. This supports our intuition that “good” measures agree with each other better than those having fewer desired properties. Additionally, in Appendix E.1, we list six triplets (A, B1, B2) with n = 10 that discriminate all pairs of different measures.

Table 3: Indistinguishable measures
n measures
2 [Acc, BA, F1, κ, CE, GM1, CC, SBA]
3 [Acc, BA, κ, GM1, CC, SBA] 4-5 [BA, κ, GM1, CC, SBA] 6-7 [GM1, CC, SBA] 8 [CC, SBA] 9-10 —

Experiment within a weather forecasting service In this experiment, we aim at understanding whether the differences between measures may affect the decisions made while designing real systems. For this purpose, we conduct an experiment within the Yandex.Weather service.
5https://github.com/yandex-research/classiﬁcation-measures

8

There is a model that predicts the presence/absence of precipitation at a particular location [18]. The prediction is made for 12 prediction intervals (horizons): from ten minutes to two hours. The original model returns the probability of precipitation, which can be converted to binary labels via a threshold. There are six thresholds used in this experiment, which lead to six different models. The measures were logged for 12 days. To sum up, for each threshold (model), each day, and each horizon, we have a confusion matrix that can be used to compute a performance measure.

For each pair of measures, we

compute how often they are in- Table 4: Inconsistency of binary measures for rain prediction, %

consistent according to the def-

inition above. For this, we ag-

Acc BA F1 κ CE GM1 CC SBA

gregate the results over all days Acc — 96.5 41.0 37.5 3.1 38.7 44.3 55.9

and horizons. Table 4 shows that BA 96.5 — 55.6 58.9 99.7 57.7 52.0 40.4

there are pairs of measures with F1 41.0 55.6 — 3.3 44.2 2.2 3.4 15.0

substantial disagreement: e.g., κ

37.5 58.9 3.3 — 40.7 1.1 6.7 18.3

accuracy and Balanced Accuracy CE 3.1 99.7 44.2 40.7 — 41.9 47.5 59.1

almost always disagree. This can GM1 38.7 57.7 2.2 1.1 41.9 — 5.5 17.1

be explained by the fact that ac- CC 44.3 52.0 3.4 6.7 47.5 5.5 — 11.4

curacy has a bias towards the ma- SBA 55.9 40.4 15.0 18.3 59.1 17.1 11.4 —

jority class, so it prefers a higher

threshold, while Balanced Accuracy weighs true positives more heavily, so it prefers a lower threshold.

In contrast, GM1, CC, κ, and F1 agree with each other much better. In Appendix E.1 we conduct a

more detailed analysis. In particular, we separately consider the ten-minute and two-hour prediction

horizons and show that the behavior and consistency of measures signiﬁcantly depend on the horizon

as the horizon deﬁnes the balance between true positives, true negatives, false positives, and false

negatives. We also observe that CC and SBA perfectly agree for the ten-minute horizon but have

noticeable disagreement for two hours.

5.2 Multiclass measures
In this section, we analyze multiclass measures. For all measures that are deﬁned for the multiclass problems, we consider their standard expressions (if not stated otherwise). For other measures (F1, Jaccard, GM1), we use macro averaging.
Image classiﬁcation We conduct an experiment on ImageNet [24], a classic dataset for image classiﬁcation. For this, we take the top-10 algorithms that are considered to be state-of-the-art at the moment of submission.6 We check whether the leaderboard based on accuracy is consistent with the leaderboards based on other measures. Thus, we apply the models to the test set, compute the confusion matrices, and compare all measures deﬁned in Table 1.
Notably, the ImageNet dataset is balanced. This makes all measures more similar to each other. For instance, accuracy and BA are equal in this scenario. Also, the constant baseline property discussed in Section 3.5 is especially important for unbalanced datasets. Thus, measures are more consistent on balanced data. Nevertheless, we notice that the ranking can be inconsistent starting from the algorithm ranked ﬁfth on the leaderboard.
The (partial) results are shown in Table 5. Here we compare EfﬁcientNet-B7 NoisyStudent [31] and Swin-B Transformer (patch size 4x4, window size 12x12, image size 3842) [19] that are the ﬁfth and sixth models in the leaderboard. One can see that the measures inconsistently rank the algorithms: Confusion Entropy, Jaccard, and SBA disagree with accuracy and other measures. Interestingly, while Jaccard and F1 always agree for binary problems, they may disagree after the macro averaging, as we see in this case. Also, for one measure, different multiclass extensions may be inconsistent, as we see with macro averaging versus the standard deﬁnition of the multiclass Correlation Coefﬁcient. More detailed results can be found in Appendix E.2.

Sentiment analysis In the previous experiment, we noticed that despite several disagreements, the measures usually rank the algorithms similarly. This can be caused by the fact that the test set of ImageNet is balanced: all classes have equal sizes. However, in practical applications, we
6https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv (May 8, 2021).

9

Table 5: Inconsistent results on ImageNet, % (ﬁfth and sixth models in the leaderboard)

Acc/BA F1

J

κ 1−CE GM1 CC CCmacro SBA

Efﬁcientnet Swin

86.46 86.43

86.30 86.27

77.525 77.531

86.44 86.42

93.41 93.51

86.28 86.26

86.44 86.42

86.419 86.423

86.57 86.61

Table 6: Ranking algorithms according to different measures on SST-5: from 1 (best) to 7 (worst)

Acc BA F1 J κ CE GM1 CC CCmacro SBA

Flair+ELMo 1 1 1 1 1 1

1

1

1

1

Flair+BERT 2 4 5 5 4 2

5

2

2

2

SVM

3 3 3 33 5

3

3

4

4

Logistic

4 5 4 45 3

4

5

5

3

FastText

5 2 2 22 6

2

4

3

5

VADER

6 6 6 66 7

6

6

6

7

TextBlob

7 7 7 77 4

7

7

7

6

rarely encounter balanced data. Thus, we also consider an unbalanced classiﬁcation task. In this experiment, we take the 5-class Stanford Sentiment Treebank (SST-5) dataset [27]. We compare the following algorithms: TextBlob, VADER, Logistic Regression, SVM, FastText, Flair+ELMo, and Flair+BERT [23]. Table 6 shows that different measures rank the algorithms differently. Among the measures shown in the table, the only consistent rankings are the one provided by κ and BA and the second given by F1, GM, and Jaccard. Note that the latter ranking signiﬁcantly disagrees with the ranking by accuracy.
Appendix E.2 contains an additional experiment with an unbalanced multiclass dataset, where we show the inconsistency rates of the considered measures and different multiclass extensions.
6 Conclusion and future work
In this paper, we propose a systematic approach to the analysis of classiﬁcation performance measures: we propose several desirable properties and theoretically check each property for a list of measures. We also prove an impossibility theorem: some desirable properties cannot be simultaneously satisﬁed, so either distance or exact constant baseline has to be discarded.
Based on the properties we analyzed in this paper, we come to the following practical suggestions. If the distance requirement is needed, Correlation Distance seems to be the best option: it satisﬁes all the properties except for the exact constant baseline, which is still approximately satisﬁed. Otherwise, we suggest using one of Generalized Means, including Correlation Coefﬁcient and Symmetric Balanced Accuracy — they satisfy all the properties except distance. For binary classiﬁcation, CC is a natural choice as it can be non-linearly transformed to a distance. For multiclass problems, Symmetric Balanced Accuracy has an additional advantage: among the considered measures, only this one preserves its good properties in the multiclass case. Finally, we do not advise using averagings, but if needed, then macro averaging preserves more properties.
There are still many open questions and promising directions for future research. First, we would like to see whether one could construct a set of desirable properties that can be used as axioms to uniquely deﬁne one good measure (or a parametrized group of measures). Secondly, it is an open problem whether Generalized Means measures in general (or SBA in particular) can be converted to a distance via a continuous transformation. Finally, our work does not cover ranking and probability-based measures. Thus, we leave aside such widely used measures as cross-entropy and AUC. Formalizing and analyzing their properties is an important direction for future research.
Broader impact Our work may help towards reducing certain biases in research. For instance, some measures (e.g., accuracy) are biased towards the majority class. Thus, the bias towards the majority class could be even ampliﬁed with the poor metric selection. Our work could provide some clues on how to avoid such a situation.

10

Acknowledgments and Disclosure of Funding
Part of this work was done while Martijn Gösgens was visiting Yandex and Moscow Institute of Physics and Technology (Russia). The work of Martijn Gösgens is supported by the Netherlands Organisation for Scientiﬁc Research (NWO) through the Gravitation NETWORKS grant no. 024.002.003.
The authors would like to thank Alexander Ganshin, Pert Vytovtov, and Eugenia Elistratova for providing the weather forecasting data.
References
[1] A. N. Albatineh, M. Niewiadomska-Bugaj, and D. Mihalko. On similarity indices and correction for chance agreement. Journal of Classiﬁcation, 23(2):301–313, 2006.
[2] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann. The balanced accuracy and its posterior distribution. In 2010 20th international conference on pattern recognition, pages 3121–3124. IEEE, 2010.
[3] D. Chicco and G. Jurman. The advantages of the matthews correlation coefﬁcient (mcc) over f1 score and accuracy in binary classiﬁcation evaluation. BMC genomics, 21(1):1–13, 2020.
[4] S.-S. Choi, S.-H. Cha, and C. C. Tappert. A survey of binary similarity and distance measures. Journal of systemics, cybernetics and informatics, 8(1):43–48, 2010.
[5] J. Cohen. A coefﬁcient of agreement for nominal scales. Educational and psychological measurement, 20(1):37–46, 1960.
[6] C. Cortes and M. Mohri. Auc optimization vs. error rate minimization. Advances in neural information processing systems, 16(16):313–320, 2004.
[7] R. Delgado and J. D. Núñez-González. Enhancing confusion entropy (cen) for binary and multiclass classiﬁcation. PloS one, 14(1):e0210264, 2019.
[8] R. Delgado and X.-A. Tibau. Why cohen’s kappa should be avoided as performance measure in classiﬁcation. PloS one, 14(9):e0222916, 2019.
[9] D. Dua and C. Graff. UCI machine learning repository, 2017.
[10] C. Ferri, J. Hernández-Orallo, and R. Modroiu. An experimental comparison of performance measures for classiﬁcation. Pattern Recognition Letters, 30(1):27–38, 2009.
[11] J. Gorodkin. Comparing two k-category assignments by a k-category correlation coefﬁcient. Computational biology and chemistry, 28(5-6):367–374, 2004.
[12] M. Gösgens, L. Prokhorenkova, and A. Tikhonov. Systematic analysis of cluster similarity indices: How to validate validation measures. International Conference on Machine Learning (ICML), 2021.
[13] M. Hossin and M. Sulaiman. A review on evaluation metrics for data classiﬁcation evaluations. International Journal of Data Mining & Knowledge Management Process, 5(2):1, 2015.
[14] J. Huang and C. X. Ling. Using auc and accuracy in evaluating learning algorithms. IEEE Transactions on Knowledge and Data Engineering, 17(3):299–310, 2005.
[15] N. Japkowicz and M. Shah. Evaluating learning algorithms: a classiﬁcation perspective. Cambridge University Press, 2011.
[16] S. Kosub. A note on the triangle inequality for the jaccard distance. Pattern Recognition Letters, 120:36–38, 2019.
[17] O. Koyejo, N. Natarajan, P. Ravikumar, and I. S. Dhillon. Consistent multilabel classiﬁcation. In NIPS, volume 29, pages 3321–3329, 2015.
[18] V. Lebedev, V. Ivashkin, I. Rudenko, A. Ganshin, A. Molchanov, S. Ovcharenko, R. Grokhovetskiy, I. Bushmarinov, and D. Solomentsev. Precipitation nowcasting with satellite imagery. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2680–2688, 2019.
[19] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.
11

[20] A. Luque, A. Carrasco, A. Martín, and A. de las Heras. The impact of class imbalance in classiﬁcation performance metrics based on the binary confusion matrix. Pattern Recognition, 91:216–231, 2019.

[21] D. Powers. Evaluation: From precision, recall and f-measure to roc, informedness, markedness & correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.

[22] D. M. W. Powers. The problem with kappa. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 345–355, 2012.

[23] P. Rao. Fine grained sentiment classiﬁcation. fine-grained-sentiment, 2021.

https://github.com/prrao87/

[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.

[25] Scikit-learn. Clustering algorithms. https://scikit-learn.org/stable/supervised_ learning.html, 2021.

[26] F. Sebastiani. An axiomatically derived measure for the evaluation of classiﬁcation algorithms. In Proceedings of the 2015 international conference on the theory of information retrieval, pages 11–20, 2015.

[27] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.

[28] M. Sokolova and G. Lapalme. A systematic analysis of performance measures for classiﬁcation tasks. Information processing & management, 45(4):427–437, 2009.

[29] V. Van Asch. Macro-and micro-averaged evaluation measures. Technical report, 2013.

[30] J.-M. Wei, X.-J. Yuan, Q.-H. Hu, and S.-Q. Wang. A novel measure for evaluating classiﬁers. Expert Systems with Applications, 37(5):3799–3809, 2010.

[31] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687–10698, 2020.

12

A Related work
Properly choosing an evaluation measure is a signiﬁcant problem that attracted much attention in recent and long-standing research. In this section, we cover some related papers. In summary, while there are many related studies, the ﬁeld lacks systematic approaches. Some papers focus on particular advantages and ﬂaws of particular measures, while others suggest some informal properties. Our paper suggests a uniﬁed analysis that generalizes and extends the existing research.
A work conceptually related to ours is [26]. In this paper, the authors deﬁne a list of properties (they refer to them as axioms). Some properties are similar to ours: MON is our monotonicity, FIX is somewhat similar (but not the same) to our maximal and minimal agreement, CHA is the constant baseline, and SYM is our class-symmetry. The properties CON and SDE/WDE are related to singularities. In the current paper, we do not focus on singularities since measures are naturally extended to such cases, as we discuss in Section B. Another property is called Robustness to Imbalance (IMB). This property requires a constant classiﬁer that classiﬁes all elements to either the positive or the negative class to get a constant similarity score k1 or k2, respectively. One can see that our constant baseline thus implies IMB with k1 = k2. On the other hand, having k1 = k2 may lead to bias towards a particular class, which does not seem to be desired. The authors show that several known measures do not satisfy some of the properties and propose K measure, which is a shifted version of Balanced Accuracy with singularities properly resolved. Let us also note that the authors advocate against CC largely because they do not use this same straightforward resolution to the singularities for this measure. Our work differs in the following aspects. First, we consider more comprehensive lists of measures and properties and check each property for each popular measure. In particular, our properties include symmetry (in terms of interchanging labelings), distance, and approximate constant baseline. We show that in terms of the extended list of properties, there are better variants than the K measure (which we refer to as Balanced Accuracy). We also provide a deep theoretical analysis of properties and propose a new family of ‘good’ measures. In addition, we rigorously analyze the multiclass scenario, including the properties of aggregation schemes. To sum up, while there are methodological similarities, there are signiﬁcant differences in the analysis and outcomes.
With some similarities to our research, the authors of [13] formulate a list of (informal) properties that are argued to be desirable for an evaluation measure. These properties include having a natural extension to the multiclass case, low complexity and computational cost, distinctiveness and discriminability, informativeness, and favoring the minority class. While informativeness seems to be an informal analog of our constant baseline, the properties are not formally deﬁned, and thus no systematic analysis of measures with respect to the properties can be given.
Another work related to our research [28] deﬁnes a list of properties by describing several transformations of the confusion matrix that should not change the measure value. As a result, the authors provide a table listing which measures are invariant under which transformations. This analysis includes our symmetry and also scale invariance which we discuss further in Appendix D. However, the discussed properties are quite simple, and the work does not cover the most important and complex ones like constant baseline, monotonicity, or distance.
There are papers focusing on properties of a particular measure, for instance, Cohen’s Kappa [8, 22], Confusion Entropy [7], or Balanced Accuracy [2]. Some papers go beyond the threshold measures considered in our paper. For instance, [6] theoretically analyzes how the area under the ROC curve (AUC) relates to accuracy. Another work focusing on AUC and accuracy is [14]. This paper formally deﬁnes two properties: degree of consistency and degree of discriminancy. The degree of consistency is not a property of a measure but rather a property of a pair of measures. In our experiments on synthetic and real data, we compute such degrees of (in)consistency. The degree of discriminancy, in turn, can be reformulated as the number of different values that a measure has (in a given domain).
There are studies advocating using the Matthews correlation instead of some other popular measures. For instance, the authors of [8] compare CC to Cohen’s Kappa and show that the latter may have undesirable behavior in some scenarios. Essentially, these scenarios show that Cohen’s Kappa does not satisfy our strong monotonicity requirement. A recent paper [3] advocates using CC over F1 and accuracy based on several intuitive use cases, where it is clear that the performance is poor, but only CC can correctly detect that in all cases. We note that all the use cases are related to our constant baseline property. Similarly to the above research, we conclude that CC should be preferred over F1,
13

Variable

n

m

cij

Ai

Bi

C = (cij)

ai =

m−1 j=0

cij

bi =

m−1 j=0

cji

pA

=

an1 ,

pB

=

b1 n

pAB

=

c11 n

M (C), M (A, B), M (pAB, pA, pB)

Table 7: Notation
Deﬁnition number of elements number of classes number of elements of class i that are predicted as j elements with true label i elements with predicted label i m × m confusion matrix size of i-th class in the true labeling
size of i-th class in the predicted labeling fraction of positive entries (for binary classiﬁcation) fraction of agreeing positives (for binary classiﬁcation) classiﬁcation validation measure

accuracy, and Cohen’s kappa. Importantly, our conclusion is based on a rigorous analysis and formal properties.
Numerous studies empirically compare different classiﬁcation measures [4, 10]; some of them speciﬁcally focus on imbalanced data [20]. Going beyond particular measures, some studies compare the properties of micro- and macro- averagings [29]. However, to the best of our knowledge, our work is the ﬁrst one giving a formal approach to the problem.
Finally, as we discuss in the main text in more detail, our work is motivated by a recent study [12] that analyzes properties of cluster validation measures. We refer to this paper for an overview of related work in cluster analysis.

B More on classiﬁcation validation measures

Notation For convenience, Table 7 lists notation frequently used throughout the text.

Resolving singularities When some of the classes are not present in the predicted (or, more rarely, true) labelings, some measures from Table 1 may not be deﬁned. Let us discuss how to resolve such singularities appropriately.

For some measures, singularities can only occur when the measures maximally or minimally agree with each other. For example, the denominator of Jaccard is only zero if a1 = b1 = 0, in which case A = B must hold so that the singularity is easily resolved by maximal agreement, leading to J(A, B) = 1.

For measures such as Matthews Coefﬁcient, singularities can be resolved using constant baseline. For

CC, a singularity can only occur whenever either n2 =

n m=1

a2i

or

n2

=

n m=1

b2i .

This

implies

that either A or B classiﬁes all elements to the same class. If both A and B classify all elements to

the same class, then the singularity can be resolved by maximal agreement (if they classify to the

same class) or minimal agreement (otherwise). If one of A and B classiﬁes all elements to the same

class, then the constant baseline tells us that M (A, B) = 0 should hold.

Similarly, some measures, e.g., BA and SBA, contain terms cii/ai (or cii/bi) that may have singularities. In cases where ai = 0, these singularities can be algebraically resolved by cii = 0 = ainbi . This leads to caiii = bni and ensures that such singularities will not lead to violations of constant baseline.

Correspondence with pair-counting cluster validation measures As discussed in the main text, there is a correspondence between pair-counting cluster validation measures and binary classiﬁcation validation measures. We refer to Table 8 for some corresponding pairs.

14

Table 8: Correspondence of binary classiﬁcation measures and pair-counting clustering measures

Classiﬁcation
F1 Jaccard Matthews Correlation Coefﬁcient Accuracy Cohen’s Kappa
Symmetric Balanced Accuracy Correlation Distance

Clustering
Dice Jaccard Pearson Correlation Coefﬁcient Rand Adjusted Rand
Sokal&Sneath Correlation Distance

C Checking the properties
Table 2 lists which measures satisfy the discussed properties and which averaging schemes preserve them. In this section, we formally prove all the results. Recall that if a measure does not have a natural extension to the multiclass case, then we analyze its binary variant. Additionally, if a property is violated in the binary case, then we do not check it in the multiclass case.

Using existing analysis of cluster validation indices As discussed in the previous section, there is
a correspondence between some pair-counting clustering evaluation measures and classiﬁcation ones. Recall that a pair-counting clustering measure is a function of N11, N10, N01, and N00, where N11 is the number of element-pairs belonging to the same cluster in both partitions, N00 is the number of pairs belonging to different clusters in both partitions, N10 is the number of pairs belonging to the same cluster in the true partition but to different clusters in the predicted partition, and N01 is the number of pairs belonging to different clusters in the true partition but to the same cluster in
the predicted partition. Thus, pair-counting clustering measures are functions of TP, TN, FP, and FN deﬁned for classifying element-pairs into “intra-cluster” and “inter-cluster” pairs. So, replacing Nij by cij we naturally get a binary classiﬁcation measure. Some classiﬁcation evaluation indices have been theoretically analyzed in [12]. Using Table 8, we can adopt some of these results for
classiﬁcation measures.

C.1 Maximal and minimal agreement

To check whether a measure has the maximal or minimal agreement properties, we substitute the entries of a diagonal matrix or a matrix with zero diagonal into the expression: we need either a strict upper or a strict lower bound for the measure values. Note that for measures having the monotonicity property (i.e., for all considered measures except CE and multiclass κ, CC, CD), it is sufﬁcient to check that we obtain constant values for diagonal and non-diagonal matrices. Indeed, each confusion matrix can be monotonically transformed to a diagonal (or a zero-diagonal) one.

By substituting a diagonal confusion matrix, we get the maximal agreement for F1, J, CC, Acc, BA, κ, SBA, and GMr with cmax = 1. For −CD, the maximal agreement holds with cmax = 0. Finally, CE = 0 if C is diagonal and otherwise there exists a pair (i, j) such that cij > 0, ai > 0, bj > 0, so we get −CE < 0.

The minimal agreement for accuracy, Balanced Accuracy, and Symmetric Balanced Accuracy clearly holds with cmin = 0. Substituting a zero-diagonal confusion matrix into GMr, we get cmin = −1.

For binary measures F1 and Jaccard, the minimal agreement does not hold: these measures equal zero not only for zero-diagonal matrices, but also when c11 = 0 and c00 > 0.

In the binary case, the minimal agreement of CC is satisﬁed with cmin = −1. However, this property

010

010

is violated if m > 2. For instance, consider the confusion matrices C1 = 0 0 1 and C2 = 1 0 1 .

200

010

We have CC(C1) = CC(C2) (-0.5 and -0.6, respectively), while C1 and C2 are both zero-diagonal.

Note that CD is a monotone transformation of CC, so CD inherits the same properties.

15

For

CE,

the

minimal

agreement

does

not

hold

even

in

the

binary

case

[7]:

let

C1

=

(

0 6

6 0

)

and

C2

=

(

1 5

5 1

)

.

Then,

we have CE(C1)

=

1 and CE(C2)

>

1.

This contradicts both the minimal

agreement and monotonicity properties.

Finally, substituting a zero-diagonal matrix into Cohen’s Kappa, we get n−2− i iaaibiibi which is clearly non-constant.

C.2 Symmetry

Class-symmetry Almost all considered measures are class-symmetric: they do not change after interchanging class labels. The only exceptions are F1 and Jaccard. Class-symmetry of GM follows

from the fact that it can be rewritten as (c11c00 − c01c10) /

r

1 2

(ar1ar0

+

br1br0)

.

Symmetry This property is easily veriﬁed by swapping ai with bi and cij with cji. Thus, all measures except BA are symmetric.

C.3 Distance
We refer to [16] for the proof that Jaccard satisﬁes this requirement. To show that accuracy has this property, we need to show that 1 − Acc is a distance, which is true since n(1 − Acc) is the Hamming distance.
Now, we need to prove that CD is a distance since it was previously known only for the binary case. Lemma 1. The Correlation Distance CD = π1 arccos(CC) is a distance for any m ≥ 2.

Proof. Let us represent a classiﬁcation by a matrix via one-hot encoding, i.e., A = (aij)i∈[n],j∈[m],
where aij = 1{A(i) = j}, and deﬁne aj = i aij. Note that for two labelings A and B, the
Frobenius inner product is given by A, B = cjj,
j
where cjj is the j-th diagonal entry of the confusion matrix for A and B. Next, we deﬁne
A¯ := aij − anj i∈[n],j∈[m] .

Then, for two labelings A and B, the Frobenius inner product of these mappings is given by

A¯, B¯ =
j

cjj − aj bj . n

And the squared length equals

A¯ 2 = n −

j a2j .
n

Therefore, we get

A¯, B¯ CC(C) = A¯ · B¯ ,

so that its arccosine is indeed the angle between A¯ and B¯, which is a metric distance.

Let us now prove that the remaining measures cannot be linearly transformed to metric distances. According to Theorem 1, a measure that satisﬁes monotonicity and constant baseline cannot have the distance property. This proves that CC, BA, κ, SBA, and GMr cannot be linearly transformed to a distance (note that BA is also not symmetric). To show that CE does not have this property, we take A = (1, 1, 0), B = (1, 1, 1), C = (1, 0, 1). Note that CE (A, C) = 1 and CE (A, B) = CE (B, C) ≈ 0.387. Hence, CE (A, C) > CE (A, B) + CE (B, C) that disproves the distance property. Finally, the counter-example for F1 is given in [12] since F1 is equivalent to the Dice index.

16

C.4 Monotonicity

Strong monotonicity F1 and Jaccard are constant w.r.t. c00, so they are not strongly monotone.

Cohen’s

Kappa

also

violates

this

property

[12]:

we

have

κ

(

1 1

2 0

)

<

κ

(

1 1

3 0

).

Then,

CE

is

not

strongly

monotone since it is not monotone (see below).

The fact that CC is strongly monotone in the binary case is proven in [12] (for general binary vectors). In contrast to the binary case, CC is not strongly monotone if m ≥ 3 since it is not monotone. CD inherits monotonicity properties from CC.

To prove that accuracy is strongly monotone, we use the inequality (a + x)/(b + x) > a/b for b > a > 0 and x > 0. So, accuracy increases if we simultaneously increment cii (for some i) and n. If we increment n and cij for i = j, then accuracy decreases, which proves strong monotonicity. Similar reasoning works for BA and SBA.

Finally, let us prove that GMr is strongly monotone for any r. Lemma 2. GMr is strongly monotone.

Proof. Note that r → 0 corresponds to CC. Since this measure is considered above, we may assume that r = 0.

Due to the symmetry of GM, we only need to prove that the measure is strongly monotone w.r.t. c11 and c10. Moreover, GM ﬂips the sign if we invert the labels in one classiﬁcation. Hence, we only need to prove that it is increasing in c11. Considering GM as a function of independent variables c11, c00, c01, c10, we calculate

∂GMr = (n + c11 − b1 − a1) ∂c11

1 rr

−1/r rr

2 (a1a0 + b1b0)

− 21r ar1−1ar0r + b1r−1br0r (nc11 − a1b1)

1 rr

−1/r−1 rr

2 (a1a0 + b1b0)

.

Simplifying the expression, we note that it has the same sign as the following sum

(n + c11 − b1 − a1) (ar1ar0 + br1br0) − ar1−1ar0 + b1r−1br0 (nc11 − a1b1) = ar0ar1−1 −nc11 + a1b1 + a1n + a1c11 − b1a1 − a21 + br0br1−1 −nc11 + a1b1 + b1n + b1c11 − b1a1 − b21 = ar0a1r−1 · a0c10 + br0br1−1 · b0c01 ≥ 0.
Note that the last expression is strictly positive if the classiﬁcations A and B do not coincide and are not constant.

Monotonicity First, we note that monotonicity of Acc, BA, SBA, and GM follows from their strong monotonicity. Monotonicity of F1 and Jaccard follows from their deﬁnitions, see also [12].

Monotonicity of CC in the binary case follows from its strong monotonicity. However, for m ≥ 3, CC

100

100

is not monotone. Indeed, consider C1 = 6 1 0 , C2 = 7 0 0 and note that CC(C2) > CC(C1).

001

001

The fact that Cohen’s Kappa is monotone follows from [12] (the proof for Adjusted Rand applies to

general binary vectors). Similarly to CC, for m ≥ 3, monotonicity is violated. Consider, for example,

012

102

C1 = 0 0 0 , C2 = 0 0 0 and note that κ(C1) > κ(C2).

100

100

Finally, the example from Section C.1 disproves the monotonicity of CE.

C.5 Constant baseline
Approximate constant baseline Substituting cij = aibj/n into CC, CD, BA, κ, SBA, and GM, we get values that do not depend on ai, bi. Thus, these measures have the approximate constant baseline property.
Substituting cij = aibj/n into CE, we get that the result depends on ai and bj. For instance, taking (a0, a1) = (2, 1), (b0, b1) = (1, 2) and (a0, a1) = (0, 3), (b0, b1) = (1, 2) we get different values of

17

CE that disproves approximate constant baseline. Similarly, F1, Jaccard, and accuracy do not have this property.

Exact constant baseline We will use the following lemma.
Lemma 3. Suppose that the ﬁxed true labeling A has class-sizes a1, . . . , am, while the predicted labeling B ∼ U (b1, . . . , bm) is random. Then, EB∼U(b1,...,bm)cij = aibj /n.

Proof. To prove this equality, we simply note that

EB∼U(b ,...,b )cij =

E 1{x ∈ Bj} = ai P (x˜ ∈ Bj) = ai E

1{x˜ = y} = aibj ,

1

m

n

x∈Ai

y∈Bj

where x˜ is an arbitrary element of Ai.

Now, let us prove that all measures that have the exact constant baseline property also have the approximate constant baseline.
Lemma 4. If a measure M (C) is scale-invariant (see Deﬁnition 11), continuous, and has the constant baseline property, then it also has the approximate constant baseline.

Proof. Let us ﬁx non-negative numbers {ai}m i=−01, {bi}m i=−01 such that

m−1 i=0

ai

=

m−1 i=0

bi

=

n.

Then, consider a ﬁxed classiﬁcation AN with class sizes N a1, . . . , N am and a random classiﬁcation

BN taken from U (N b1, . . . , N bm).

Let cNij denote entries of the confusion matrix for AN and BN . Let us prove that for any i, j ∈
{1, . . . m}, the random variable cNij /N converges to aibj/n in L2 as N → ∞. From Lemma 3, we
have E (cij/N ) = aibj/n. Let us compute Var (cij). Recall that cij = x∈AN 1{x ∈ BjN }, then i

Var (cij) =

Cov 1{x ∈ BjN }, 1{y ∈ BjN } .

x,y∈AN i

It remains to compute Cov 1{x ∈ BjN }, 1{y ∈ BjN } for x = y and x = y. For this, note that

P x ∈ BjN = bj/n and P x, y ∈ BjN = NNbnj((NN nbj−−11)) for x = y.

Then,

Cov 1{x ∈ BjN }, 1{y ∈ BjN } = P x, y ∈ BjN − P x ∈ BjN 2 = O(1/N ).

Thus, we get that Var (cij/N ) = O(N )/N 2 = O(1/N ) and prove L2-convergence.

Now we are ready to prove the lemma. Let M be a scale-invariant, continuous measure that has constant baseline. Then,

CN

cbase = EBN ∼U(Nb1,...,Nbm)M (CN ) = EM

N

−−−−→ M (C) ,
N →∞

where CN is the confusion matrix for AN and BN and C is the confusion matrix for A and B. Here EM CN /N → M (C) holds since the L2-convergence of cNij to aibj/n implies convergence in distribution.

From this lemma, we get that F1, Jaccard, Acc, and CE do not have constant baseline since they violate the approximate constant baseline property.
Assume that a measure M (C) is linear in cii for ﬁxed ai and bj. Then, using the linearity of expectation, we note that approximate constant baseline implies exact constant baseline for such measures. This observation gives that CC, BA, κ, SBA, and GMr have the constant baseline property.
Finally, we note that CD violates the constant baseline property as it has both monotonicity and distance properties (in binary case), while Theorem 1 states that all three properties cannot be simultaneously satisﬁed.

18

C.6 Preserving properties by averagings

Micro averaging Recall that for micro averaging, we sum up the binary confusion matrices

corresponding to m one-vs-all classiﬁcations. Formally, we set TP :=

m−1 i=0

cii,

FN

:=

FP

=

n−

m−1 i=0

cii,

TN

:=

(m

−

2)n

+

m−1 i=0

cii.

Then,

we

compute

the

binary

measure.

First, it is easy to see that this averaging preserves symmetry and class-symmetry.

Let us prove that micro averaging preserves the maximal agreement property. If a confusion matrix

C is diagonal, then n −

m−1 i=0

cii

=

0

and

FP

=

FN

=

0.

Substituting

these

values

in

a

binary

measure M , we get cmax. If C is not diagonal, then FP = FN = n −

m−1 i=0

cii

>

0

and

the

result

of

the averaging will be strictly lower than cmax. On the other hand, minimal agreement is not preserved

since TN = (m − 2)n > 0 for zero-diagonal confusion matrices. As a simple example, consider a
measure 1{TP + TN > 0} satisfying the minimal agreement property. Then, after micro averaging,

this measure is constant, thus violating minimal agreement.

Also, micro averaging preserves monotonicity: increasing cii for ﬁxed n leads to increased TP and

TN, leaving TP+FP, TP+FN, TN+FP, TN+FN unchanged. On the other hand, strong monotonicity

can be violated: incrementing cij for i = j we increase n, so TN = (m − 2)n +

m−1 i=0

cii

also

increases and the averaged measure may increase. For example, consider a strongly monotone binary

measure TP + TN − FP − FN. Then, after micro averaging, it reduces to nm, which violates strong

monotonicity.

To prove that micro averaging preserves the distance property, we ﬁrst note that it preserves maximal
agreement and symmetry. To show that the triangle inequality is also preserved, we consider micro
averaging as a result of the following procedure. First, we use one-hot encoding to map each class to a binary vector. Then, we map a classiﬁcation vector A of size n to the binary vector Aˆ of size nm consisting of one-hot encoded binary vectors. Finally, for two classiﬁcations A and B, we compute the binary measure for Aˆ and Bˆ. It is easy to see that this procedure is equivalent to micro averaging. Thus, for any multiclass labelings A, B, C, there exist binary labelings Aˆ, Bˆ, Cˆ with
confusion matrices corresponding to the result of micro averaging. Hence, the triangle inequality for
micro averaged matrices follows from the binary property.

Finally, approximate constant baseline can be violated after micro averaging. Indeed, let us take

cii = aibi/n. Then, after the averaging, we get TP =

m−1 i=0

aibi/n,

which

is

not

necessary

equal

to (TP + FN)(TP + FP)/(mn) = n/m. As an example, we can consider a measure TP − (TP +

FP)(TP + FN)/(TP + FP + FN + TN) having constant baseline. Thus, the averaged measure is

m−1 i=0

cii

−

n/m,

which

does

not

have

an

approximate

constant

baseline.

Consequently,

the

constant

baseline property is also violated.

Macro averaging As for the micro averaging, symmetry and class-symmetry are clearly satisﬁed.

Let us check the maximal agreement. Consider a binary measure M having this property. If C is diagonal, then the result of the averaging is m1 i M (cii, 0, 0, n − cii) = cmax. If C is not diagonal, then one of ai − cii > 0 and the averaged measure is strictly lower than cmax. In contrast, the

minimal agreement property can be violated, since for a zero-diagonal confusion matrix the result of the averaging is m1 i M (0, ai, bi, n − ai − bi). Since we may have n − ai − bi > 0, the minimal
agreement can be violated. For instance, consider the measure 1{TP + TN > 0} satisfying the

010

001

minimal agreement property in the binary case. Then, taking C1 = 0 0 1 and C2 = 0 0 1 we get

110

110

that the averaging has different values on these matrices (1 and 2/3, respectively), thus the minimal

agreement property does not hold.

It is easy to see that monotonicity is preserved by macro averaging. However, strong monotonicity

can be violated. Indeed, assume that cij increases. Then, for k ∈/ {i, j}, the values ckk, ak, bk do

not change while n increases. To show that this can break strong monotonicity, consider the same

counterexample as for the micro averaging: TP + TN − FP − FN. Then, after macro averaging, we

get the measure n(m − 4) + 4

m−1 i=0

cii

/m that is not strongly monotone.

Let us prove that macro averaging preserves the distance property. As for the micro averaging, it remains to check the triangle inequality. Let A, B, and C be multiclass classiﬁcations with n

19

elements and m classes. Then, for all i ∈ {1, . . . , m}, we can build the binary labelings Ai, Bi, Ci corresponding to one-vs-all binary classiﬁcations. Triangle inequality holds for each Ai, Bi, Ci. Thus, summing up these inequalities over all i ∈ {1, . . . , m}, we prove the triangle inequality for the macro-averaged measure.
Finally, approximate and exact constant baseline are preserved by the macro averaging due to the linearity of expectation.

Weighted averaging Similar reasoning as above, allows one to show that weighted averaging preserves the maximal agreement, class-symmetry, monotonicity, exact and approximate constant baseline.

For the minimal agreement, the counterexample used for macro averaging also works in this case.

Clearly, weighted averaging is not symmetric: we normalize by the class sizes ai. Therefore, the distance property is not preserved as it requires symmetry.

Finally, as a counterexample to strong monotonicity, we can take M = TP + TN − FP − FN and

011

011

C1 = 1 0 0 , C2 = 1 0 1 , Then, M (C1) = −2 < −9/5 = C2).

100

100

D Theoretical analysis

In this section, we perform a theoretical analysis of binary classiﬁcation measures. First, we generalize the deﬁnition of constant baseline and theoretically compare the two non-linear distancetransformations of the Matthews Correlation Coefﬁcient. Then, we derive the class of measures that satisfy all properties except distance.

D.1 Higher-order approximate constant baseline

Before we generalize our deﬁnition of constant baseline, let us introduce some additional properties. These properties differ from the properties introduced in the main text in the sense that they are not desirable in themselves but are rather instrumental for the analysis of other desirable properties.
Deﬁnition 11. A measure M is scale-invariant if, for any scalar α > 0 and confusion matrix C, M (αC) = M (C).

We remark that all measures of Table 1 are scale-invariant.

Note that any binary classiﬁcation measure can be written as a function of the four variables c11, a1, b1, n as c10 = a1 − c11, c01 = b1 − c11, and c00 = n − a1 − b1 + c11. Therefore, any scale-invariant binary classiﬁcation measure can be written as a function of the three fractions pAB = c11/n, pA = a1/n, and pB = b1/n. Hence, we will use the shorthand notation M (C) = M (pAB, pA, pB) for the remainder of this analysis. We will write PAB instead of pAB whenever B is random. Note that for B ∼ U (pBn, (1 − pB)n), it holds that EB∼U(pBn,(1−pB)n)[PAB] = pApB. Thus, it can readily be seen that the approximate constant baseline is satisﬁed whenever M (pApB, pA, pB) = cbase. We introduce one additional property that ensures that the measure is a well-behaved function in terms of
these variables.

Deﬁnition 12. A scale-invariant measure M is smooth if, for any pA, pB ∈ (0, 1), the Taylor series of M (pAB, pA, pB) around the point pAB = pApB converges absolutely on the interval pAB ∈ [0, min{pA, pB}]. That is, for all pA, pB ∈ (0, 1) and pAB ∈ [0, min{pA, pB}], we have

∞ (pAB − pApB)k ∂k

k!

∂pk M (pApB, pA, pB) < ∞.

k=0

AB

Note that such absolute convergence implies that the Taylor series converges to M (pAB, pA, pB).

We remark that all constant-baseline measures of Table 1 are linear functions in pAB for ﬁxed pA, pB.

Thus, each of these is smooth. Furthermore, because CC is linear in pAB, we have that for any

transformation f (CC), the Taylor expansion of f (CC) is given by substituting CC in the Taylor

expansion

of

f.

Thus,

since

the

Taylor

expansion

of

f1(x)

=

1 π

arccos(x)

and

f2(x)

=

2(1 − x)

around x = 0 converges for x ∈ [−1, 1], we have that CD= f1(CC) and CD = f2(CC) are also

smooth measures.

20

This allows us to express the expected value of a measure in terms of the central moments of PAB:

∞ (PAB − pApB)k ∂k

E[M (PAB, pA, pB)] = E

k!

∂pk M (pApB, pA, pB)

k=0

AB

∞ E[(PAB − pApB)k] ∂k

=

k!

∂pk M (pApB, pA, pB).

k=0

AB

Here, the absolute convergence helps bound the term inside the expectation so that the Dominated Convergence Theorem allows us to interchange summation and expectation. In this expression, the ﬁrst-order term vanishes as E[PAB] = pApB. Thus, we have

∞ E[(PAB − pApB)k] ∂k

E[M (PAB, pA, pB)] = M (pApB, pA, pB) +

k!

∂pk M (pApB, pA, pB).

k=2

AB

Note that for large numbers of items, PAB is highly concentrated around pApB. Thus, the contribution of the higher-order central moments is relatively small. This leads to the following generalization of the constant baseline.
Deﬁnition 13. A smooth measure M has a k-th order approximate constant baseline, if there exists a constant cbase such that M (pApB, pA, pB) = cbase, while for all ∈ {2, . . . , k}, it holds that

∂ M (pApB, pA, pB) = 0.
∂pAB

Thus, ﬁrst-order constant baseline is equivalent to the approximate constant baseline. Furthermore, note that ∞-th order approximate constant baseline implies exact constant baseline since then
E[M (PAB, pA, pB)] = M (pApB, pA, pB) = cbase.
While it seems likely that the exact constant baseline also implies ∞-th order constant baseline, we were not able to formally prove this. However, all constant-baseline measures of Table 1 also satisfy ∞-th order constant baseline. For this reason, we will use ∞-th order constant baseline as a substitute for the exact constant baseline when deriving measures from properties.

D.2 Constant baseline order of distance transformations
We now show that the constant baseline of CD = π1 arccos(CC) is one order higher than CD = 2(1 − CC).
Statement 6. CD = π1 arccos(CC) has a second-order approximate constant baseline while CD = 2(1 − CC) only has a ﬁrst-order approximate constant baseline.

Proof. The Matthews Correlation Coefﬁcient is given by

CC(pAB, pA, pB) =

pAB − pApB , pA(1 − pA)pB(1 − pB)

so that it is indeed a linear function in pAB for ﬁxed pA, pB. Therefore, the Taylor expansions of CD and CD are obtained by simply substituting CC into the Taylor expansions of π1 arccos(x) and
2(1 − x) respectively. We have

1 arccos(x) = π − ∞ (2k)!x2k+1 and

π

2

4k(k!)2(2k + 1)

k=0

√ √ ∞ 2 2k 2(1 − x) = 2 − 2
k=0 k + 1 k

x k+1 .
4

Thus, we see that 2(1 − x) we have a quadratic term, which we do not have for π1 arccos(x). This shows that CD has a second-order constant baseline while CD only has a ﬁrst-order constant
baseline.

21

D.3 Deriving measures satisfying all properties except distance Let us derive a class of measures satisfying all properties from Table 2 except distance. We will use ∞-th order constant baseline instead of the exact constant baseline as this property is easier to analyze while it implies exact constant baseline and coincides with it for all measures of Table 1. Theorem 3. Let M be a smooth binary classiﬁcation measure that satisﬁes the following properties:
1. ∞-th order constant baseline with constant 0; 2. symmetry; 3. class-symmetry; 4. maximal agreement with constant 1; 5. minimal agreement with constant −1; 6. strong monotonicity.
Then, it is of the following form: M (pAB, pA, pB) = s(pA, pB)(pAB − pApB),
where s satisﬁes the following properties:
1. s(pB, pA) = s(pA, pB) = s(1 − pA, 1 − pB); 2. s(pA, pA) = s(pA, 1 − pA) = pA(11−pA) ; 3. s(pA, pB ) < max pA1pB , (1−pA)1(1−pB) for pB = 1 − pA;
4. s(pA, pB ) < max pA(11−pB) , (1−p1A)pB for pB = pA;
5. 1s pA ∂p∂A + pB ∂p∂B s ∈ min −2, −1 − (1−ppAA)(p1B−pB) , max 21p−Bp−B1 , 21p−Ap−A1 ; 6. 1s (1 − pA) ∂p∂A − pB ∂p∂B s ∈ min 2 − p1A , 2 − 1−1pB , max 1 + ppB A((11−−ppBA)) , 2 .

Proof. From the deﬁnition of ∞-th order constant baseline, we have that M (pAB, pA, pB) must be a linear function in pAB for ﬁxed pA, pB. Thus, it must be of the form

M (pAB, pA, pB) = cbase + (pAB − pApB)s(pA, pB) = (pAB − pApB)s(pA, pB)

for some function s(·, ·).

Now, symmetry requires M (pAB, pB, pA) = M (pAB, pA, pB) which leads to s(pB, pA) = s(pA, pB). Then, class-symmetry requires M (pAB, pA, pB) = M (1−pA−pB+pAB, 1−pA, 1−pB), leading to s(1 − pA, 1 − pB) = s(pA, pB).
For maximal agreement, we have M (pAB, pA, pB) ≤ 1 with equality only if pAB = pA = pB, i.e., M (pA, pA, pA) = 1, leading to s(pA, pA) = pA(11−pA) . Furthermore, M (pAB, pA, pB) ≤ M (min{pA, pB}, pA, pB) < 1 for pA = pB is satisﬁed by

1

1

s(pA, pB) < min{pA, pB} − pApB = min{pA(1 − pB), (1 − pA)pB}

1

1

= max

,

.

pA(1 − pB) (1 − pA)pB

Minimal agreement requires M (pAB, pA, pB) ≥ −1 with equality only if pAB = 0, pB = 1 − pA. For equality, we need
1 s(pA, 1 − pA) = pA(1 − pA) .

22

While for pB = 1 − pA, we need M (pAB, pA, pB) ≥ M (max{0, pA + pB − 1}, pA, pB) > −1, leading to

1

1

1

s(pA, pB) < min{pApB, (1 − pA)(1 − pB)} = max pApB , (1 − pA)(1 − pB) .

For the remainder of the proof, we will derive that strong monotonicity is satisﬁed when the last two conditions of Theorem 3 hold. The ﬁrst one will be derived from the increasingness of M in N00 while the second one will be derived from decreasingness in N10. Increasingness in N11 and decreasingness in N01 will then follow from class-symmetry and symmetry, respectively.

We rewrite the condition dNd00 M to

d M
dN00

N11 , N11 + N10 , N11 + N01 N11 + N10 + N01 + N00 N11 + N10 + N01 + N00 N11 + N10 + N01 + N00

1

∂

∂

∂

= − N pAB ∂pAB + pA ∂pA + pB ∂pB M (pAB, pA, pB).

Since we want dNd00 M > 0, we need

∂

∂

∂

pAB ∂pAB + pA ∂pA + pB ∂pB M (pAB, pA, pB) < 0.

We compute the partial derivatives of M :

∂ M = s,
∂pAB

∂

∂

∂pA M = −pBs + (pAB − pApB) ∂pA s, (3)

∂

∂

∂pB M = −pAs + (pAB − pApB) ∂pB s.

Thus, we need

∂

∂

(pAB − 2pApB) · s + (pAB − pApB) pA ∂pA + pB ∂pB s < 0

for all pAB ∈ [max{pA + pB − 1, 0}, min{pA, pB}]. Since the left-hand side is linear in pAB, we only need to check the upper and lower limit. Substituting pAB = min{pA, pB} leads to

pA ∂ + pB ∂ s < 2pApB − min{pA, pB} s

∂pA

∂pB

min{pA, pB} − pApB

= pApB − 1 s min{pA(1 − pB), pB(1 − pA)}

= max pB − 1, pA − 1 s

1 − pB

1 − pA

= max 2pB − 1 , 2pA − 1 s. 1 − pB 1 − pA

Substituting pAB = max{0, pA + pB − 1} gives a lower bound

pA ∂ + pB ∂ s > − 2pApB − max{0, pA + pB − 1} s

∂pA

∂pB

pApB − max{0, pA + pB − 1}

= − 1 + pApB s min{pApB, (1 − pA)(1 − pB)}

= − max 2, 1 + pApB s. (1 − pA)(1 − pB)

Combining this, we conclude that increasingness in N00 is satisﬁed whenever it holds that

1

∂

∂

pA

+ pB

s ∈ min −2, −1 −

pApB

, max 2pB − 1 , 2pA − 1 ,

s

∂pA

∂pB

(1 − pA)(1 − pB)

1 − pB 1 − pA

23

as required.

The condition for decreasingness in N10 is obtained similarly. The condition dNd10 M < 0 can be rewritten to

∂

∂

∂

−pAB ∂pAB + (1 − pA) ∂pA − pB ∂pB M (pAB, pA, pB) < 0.

Substituting the partial derivatives from (3) gives

∂

∂

s · (−pAB − (1 − pA)pB + pApB) + (pAB − pApB) (1 − pA) ∂pA − pB ∂pB

s < 0.

Again, this linear inequality should hold for all pAB ∈ [max{pA + pB − 1, 0}, min{pA, pB}] and we only need to test the extremes. For pAB = min{pA, pB}, we ﬁnd the upper bound

1

∂

∂

(1 − pA) − pB

s < min{pA, pB} + (1 − pA)pB − pApB)

s

∂pA

∂pB

min{pA, pB} − pApB

= min{pA(1 − pB) + pB(1 − pA), 2pB(1 − pA)} min{pA(1 − pB), pB(1 − pA)}

= max 1 + pB(1 − pA) , 2 . pA(1 − pB)

Substituting pAB = max{0, pA + pB − 1}, we get the following upper bound

1

∂

∂

(1 − pA) − pB

s > max{0, pA + pB − 1} + (1 − pA)pB − pApB)

s

∂pA

∂pB

max{0, pA + pB − 1} − pApB

= − max{pB(1 − 2pA), pA + 2pB − 1 − 2pApB} min{pApB, (1 − pA)(1 − pB)}

1

1

= min 2 − , 2 −

.

pA

1 − pB

Combined, we obtain the desired condition

1

∂

∂

1

1

(1 − pA) − pB

s ∈ min 2 − , 2 −

, max 1 + pB(1 − pA) , 2 .

s

∂pA

∂pB

pA

1 − pB

pA(1 − pB)

D.4 Generalized Means measure
The Generalized Means measure GMr corresponds to s(pA, pB) = Mr(pA(1 − pA), pB(1 − pB))−1, where Mr is the generalized mean with exponent r. Lemma 5. s(pA, pB) = Mr(pA(1 − pA), pB(1 − pB))−1 satisﬁes all the conditions of Theorem 3.

Proof. The proof follows from Section C, where it is shown that GMr indeed satisﬁes all the required properties. Let us also demonstrate the conditions explicitly.

The ﬁrst four conditions can be easily veriﬁed by substituting this s(pA, pB). Verifying the last two conditions require a bit more work. The partial derivatives of s(pA, pB) are given by

∂1

r1

− r1 r

∂pA 2 (pA(1 − pA)) + 2 (pB(1 − pB))

1

r 2

(pA(1

−

pA))r−1

(1

−

2pA)

=−

r+1

r 12 (pA(1 − pA))r + 12 (pB(1 − pB))r r

2pA − 1

(pA(1 − pA))r

= pA(1 − pA) · (pA(1 − pA))r + (pB(1 − pB))r · s,

24

and similarly

∂

2pB − 1

(pB(1 − pB))r

∂pB s = pB(1 − pB) · (pA(1 − pA))r + (pB(1 − pB))r · s.

Substituting this into the condition for N00-monotonicity, we get

1

∂

∂

s pA ∂pA + pB · ∂pB s

2pA − 1

(pA(1 − pA))r

2pB − 1

(pB(1 − pB))r

= 1 − pA · (pA(1 − pA))r + (pB(1 − pB))r + 1 − pB · (pA(1 − pA))r + (pB(1 − pB))r .

Note that the two large fractions sum to 1, so that we recognize this as the weighted average of
(2pA − 1)/(1 − pA) and (2pB − 1)/(1 − pB), which are exactly the two terms in the maximum of the upper bound of the N00-monotonicity condition. Furthermore, note that both these terms are larger than −1, so that the lower bound is also satisﬁed.

Similarly, for the condition corresponding to N10-monotonicity, we get

1

∂

∂

s (1 − pA) ∂pA − pB ∂pB s

1

(pA(1 − pA))r

=

2− pA

(pA(1−pA))r + 1 (pB(1−pB))r +

2

1 2−
1−pB

(pB(1 − pB))r (pA(1−pA))r + (pB(1−pB))r .

Again, we recognize this as the weighted average of 2 − p−A1 and 2 − (1 − pB)−1, which are the terms in the minimum of the required lower bound, so that this is always satisﬁed. Finally, the corresponding
upper bound is always satisﬁed since 2 − p−A1 and 2 − (1 − pB)−1 can both be upper-bounded by 1. We thus conclude that GMr indeed lies inside this class of measures for all r.

Proof of Statement 2 Finally, let us show that Generalized Means generalizes both CC and SBA.

Recall that

GMr =

nc11 − a1b1 1 .

1 2

(ar1ar0

+

br1br0)

r

Taking r = −1, we obtain:

1 1 + GM−1 = 1 + 2

nc11 + nc11 − b1 − a1 a0a1 b0b1 a0 b0

= 1 c11(a0 + a1) + c11(b0 + b1) − b1 − a1 + 2

2

a0a1

b0b1

a0 b0

= 1 c11 + c11 + n − a1 − b1 + c11 + n − a1 − b1 + c11

2 a1 b1

a0

b0

= 2 · SBA .

1

Now, let us conﬁrm that taking r → 0 we get CC. Let X := b0b1/(a0a1), then

1 2

(ar1ar0

+

br1br0)

r

can be rewritten to

1

a0a1

1 (1 + Xr)

r
= a0a1 exp

1 ln

1 (1 + Xr)

.

2

r2

We take the limit of the exponent and use l’Hôpital to ﬁnd that

lim ln 12 (1 + Xr) = lim ln(X)Xr = 1 ln X.

r→0

r

r→0 1 + Xr 2

Hence, as r → 0, the denominator of GMr converges to

a0a1 · exp

1 ln X
2

√ = a0a1 X =

a0a1b0b1

and we obtain CC.

25

Table 9: Examples of triplets discriminating all pairs of different measures: the upper table lists the triplets, the lower table speciﬁes which triplet discriminates a particular pair

Triplet 1 Triplet 2 Triplet 3 Triplet 4 Triplet 5 Triplet 6

A
(1, 1, 1, 0, 1, 1, 0, 1, 1, 0) (0, 1, 1, 1, 1, 0, 1, 1, 0, 1) (0, 0, 0, 0, 1, 1, 1, 0, 1, 0) (0, 1, 1, 1, 1, 0, 1, 1, 0, 1) (0, 0, 0, 0, 1, 1, 1, 0, 1, 0) (1, 1, 1, 1, 1, 1, 1, 1, 0, 1)

B1
(1, 1, 1, 0, 1, 0, 1, 1, 1, 1) (1, 0, 0, 1, 0, 1, 0, 1, 1, 0) (1, 1, 1, 1, 1, 1, 1, 1, 0, 1) (1, 1, 1, 1, 1, 1, 1, 1, 0, 1) (0, 1, 1, 0, 0, 1, 0, 0, 0, 1) (1, 1, 1, 0, 1, 1, 0, 1, 1, 0)

B2
(1, 0, 0, 1, 0, 1, 0, 1, 1, 0) (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) (0, 1, 1, 1, 1, 0, 1, 1, 0, 1) (0, 1, 0, 1, 1, 1, 1, 1, 0, 1) (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) (0, 1, 1, 0, 0, 1, 0, 0, 0, 1)

Acc BA F1 κ CE GM1 CC SBA

Acc — 1 2 6 6 1 5 5

BA 1 — 1 1 1 3 3 1

F1

2 1 —2 2 1 2 2

κ

6 1 2—4 1 3 3

CE

6 1 2 4— 1

33

GM1 1 3 1 1 1 — 5 1

CC 5 3 2 3 3 5 — 4

SBA 5 1 2 3 3 1 4 —

E Additional experimental results

E.1 Binary measures
Distinguishing binary measures Let us show triplets of labelings (A, B1, B2) discriminating all pairs of measures in the binary classiﬁcation case. Each triplet consists of the true labeling A and two predicted labelings B1 and B2. We say that two measures are strictly inconsistent if, according to the ﬁrst one, B1 is closer to A, while, according to the second one, B2 is closer to A (comparing to the main text, here we consider only strict inequalities). Table 9 lists six triplets, where all labelings are of size n = 10. It also speciﬁes which triplet discriminates each pair of measures.

Experiment within a weather forecasting service In this section, we provide a detailed analysis of the precipitation prediction task discussed in Section 5.1.

In Figure 1, we show the dependence of measures on the threshold that is used to convert soft predictions to binary labels. This is done separately for two prediction horizons: ten minutes and two hours. We make the following observations. For the ten-minute horizon, most of the measures agree that the optimal threshold is 0.9. However, Confusion Entropy favors the largest threshold, while Balanced Accuracy favors the smallest one. Interestingly, the behavior of measures signiﬁcantly differs for the two-hour prediction interval. In this case, many of the measures favor either 0.6, 0.7, or 0.77. However, accuracy and CE prefer the largest threshold, while BA and SBA prefer the smallest one. Interestingly, this is the only experiment where we observe that SBA has such a noticeable disagreement with GM1 and CC.

To better understand the differences between the measures, let us list average confusion matrices for the ten-minute and two-hour prediction horizons depending on a threshold (in increasing order). Here we show the relative values in percentages.

For ten minutes:

(

93.55 0.22

1.12 5.11

)

(

93.76 0.29

0.91 5.04

)

(

93.84 0.33

0.83 5.01

)

(

93.91 0.36

0.76 4.97

)

(

94.10 0.49

0.57 4.85

)

(

94.33 0.75

0.34 4.59

)

For two hours:

(

90.41 1.47

4.25 3.87

)

(

91.32 1.74

3.34 3.60

)

(

91.67 1.87

2.99 3.47

)

(

91.96 1.99

2.70 3.35

)

(

92.94 2.51

1.72 2.83

)

(

93.98 3.43

0.68 1.91

)

Consider, for instance, the two smallest thresholds for the ten-minute horizon. It is easy to see that accuracy grows from 98.66% to 98.80%. In contrast, for Balanced Accuracy, the difference between

26

BA, 2 h

Acc, 10 min

κ, 2 h

F1, 10 min

98.95 98.9
98.85 98.8
98.75 98.7
98.65 0.3 0.4

90.2 90
89.8 89.6 89.4 89.2
89 88.8 88.6 88.4 88.2
0.3 0.4

93.3

93.2 10 min

93.1

2 h

93

92.9

92.8

92.7

92.6

92.5

92.4

92.3

92.2

0.3 0.4

89.6 89.4 89.2
89 88.8 88.6 88.4 88.2
88 87.8
0.3 0.4

10 min 2 h
0.5 0.6 0.7 0.8 0.9 threshold

96 95.8 95.6 95.4 95.2 95 94.8 94.6 94.4 94.2 1

10 min 2 h
0.5 0.6 0.7 0.8 0.9 threshold

60 58 56 54 52 50 48 46 1

0.5 0.6 0.7 0.8 0.9 threshold

85 84 83 82 81 80 79 78 77 1

10 min 2 h
0.5 0.6 0.7 0.8 0.9 threshold

56 55 54 53 52 51 50 49 48 1

CC, 2 h SBA, 10 min

1-CE, 2 h GM1, 10 min

F1, 2 h κ, 10 min

Acc, 2 h BA, 10 min

97.5

97

96.5

96

95.5

95

94.5

94

93.5 10 min

93

2 h

92.5

0.3 0.4 0.5 0.6 0.7 0.8 0.9

threshold

84 82 80 78 76 74 72 70 68 66 1

89.6

89.4

89.2

89

88.8

88.6

88.4

88.2

88

87.8

10 min

87.6

2 h

87.4

0.3 0.4 0.5 0.6 0.7 0.8 0.9

threshold

56 54 52 50 48 46 44 1

89.6

89.4

89.2

89

88.8

88.6

88.4

88.2

88

87.8

10 min

87.6

2 h

87.4

0.3 0.4 0.5 0.6 0.7 0.8 0.9

threshold

56 55 54 53 52 51 50 49 48 47 46 45 1

94.8

94.7

94.6

94.5

94.4

94.3

94.2

94.1

10 min 2 h

94

0.3 0.4 0.5 0.6 0.7 0.8 0.9

threshold

78.5 78 77.5 77 76.5 76 1

GM1, 2 h

1-CE, 10 min

SBA, 2 h

CC, 10 min

Figure 1: Dependence of measures on thresholds, for ten-minute and two-hour prediction horizons, the values are multiplied by 100

27

Table 10: Inconsistency of binary measures for rain prediction, horizon 10 minutes, %

Acc BA F1 κ CE GM1 CC SBA

Acc

— 93.3 14.4 14.4 3.3 14.4 15.0 15.0

BA 93.3 — 78.9 78.9 96.7 78.9 78.3 78.3

F1 14.4 78.9 — 0.0 17.8 0.0 0.6 0.6

κ

14.4 78.9 0.0 — 17.8 0.0 0.6 0.6

CE

3.3 96.7 17.8 17.8 — 17.8 18.3 18.3

GM1 14.4 78.9 0.0 0.0 17.8 — 0.6 0.6

CC 15.0 78.3 0.6 0.6 18.3 0.6 — 0.0

SBA 15.0 78.3 0.6 0.6 18.3 0.6 0.0 —

Table 11: Inconsistency of binary measures for rain prediction, horizon 2 hours, %

Acc BA F1 κ CE GM1 CC SBA

Acc

— 98.3 63.3 58.3 1.7 61.1 72.2 91.7

BA 98.3 — 35.0 39.4 100 37.2 25.6 6.1

F1 63.3 35.0 — 4.4 65.0 2.2 8.9 28.3

κ

58.3 39.4 4.4 — 60.0 2.2 13.3 32.8

CE

1.7 100 65.0 60.0 — 62.8 73.9 93.3

GM1 61.1 37.2 2.2 2.2 62.8 — 11.1 30.6

CC 72.2 25.6 8.9 13.3 73.9 11.1 — 18.9

SBA 91.7 6.1 28.3 32.8 93.3 30.6 18.9 —

the values can be written as:

∆BA = ∆c00 + ∆c11 ≈ 0.21 + −0.07 < 0.

a0

a1 94.67 5.33

So, Balanced Accuracy favors the smallest threshold. This can be explained by the fact that BA normalizes true positives (c11) by a much smaller value, so that the impact of c11 is much higher.

More interesting is the fact that for the ten-minute horizon, SBA agrees with most of the measures and strongly disagrees with BA. This can be explained by the fact that SBA also takes into account the distribution of predicted labels. For instance, for the two smallest thresholds, the difference becomes:

0.21 −0.07 93.76 93.55

5.04 5.11

∆SBA ≈

+

+

−

+

−

> 0.

94.67 5.33

94.05 93.77

5.95 6.23

Here the difference between the last two terms is positive and dominates all other differences. This happens because the false positive rate becomes signiﬁcantly smaller.

Tables 10 and 11 summarize inconsistency between different measures for the ten-minute and twohour horizons. In particular, we can see that SBA and CC always agree for the ten-minute horizon, while they have almost 20% disagreement for two hours.

E.2 Multiclass measures
Image classiﬁcation The extended results are shown in Table 12. The models are the following:7
1. tf_efﬁcientnet_l2_ns 2. tf_efﬁcientnet_l2_ns_475 3. swin_large_patch4_window12_384 4. tf_efﬁcientnet_b7_ns 5. tf_efﬁcientnet_b6_ns 6. swin_base_patch4_window12_384 7. swin_large_patch4_window7_224 7https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv

28

Table 12: Extended results for ImageNet, the values are multiplied by 100, inconsistencies are highlighted

Acc/BA F1

J

κ 1−CE GM1 CC CCmac SBA

1 88.33 88.21 80.43 88.32 94.42 88.19 88.32 88.31 88.44 2 88.23 88.08 80.25 88.21 94.38 88.07 88.22 88.20 88.35 3 87.15 87.01 78.63 87.13 93.86 87.00 87.13 87.14 87.30 4 86.83 86.64 78.08 86.82 93.64 86.63 86.82 86.78 86.95 5 86.46 86.30 77.525 86.44 93.41 86.28 86.44 86.419 86.57 6 86.43 86.27 77.531 86.42 93.51 86.26 86.42 86.423 86.60 7 86.32 86.17 77.311 86.30 93.37 86.16 86.30 86.31 86.48 8 86.31 86.12 77.314 86.29 93.41 86.10 86.30 86.28 86.47 9 86.08 85.89 76.97 86.06 93.21 85.87 86.07 86.02 86.19 10 85.72 85.55 76.51 85.70 93.05 85.53 85.70 85.70 85.89

Table 13: Inconsistency of multiclass measures on the Yeast dataset, %

Acc BA F1

J

κ CE GM1 CC SBA

Acc

— 11.8 13.7 11.1 4.6 47.7 11.1 3.3 17.0

BA 11.8 — 9.8 8.5 7.2 52.9 7.2 8.5 11.8

F1 13.7 9.8 — 2.6 10.5 48.4 5.2 10.5 4.6

J

11.1 8.5 2.6 — 9.2 49.7 6.5 9.2 7.2

κ

4.6 7.2 10.5 9.2 — 49.7 7.8 1.3 13.7

CE 47.7 52.9 48.4 49.7 49.7 — 51.0 48.4 45.1

GM1 11.1 7.2 5.2 6.5 7.8 51.0 — 7.8 8.5

CC

3.3 8.5 10.5 9.2 1.3 48.4 7.8 — 13.7

SBA 17.0 11.8 4.6 7.2 13.7 45.1 8.5 13.7 —

8. dm_nfnet_f6
9. tf_efﬁcientnet_b5_ns
10. dm_nfnet_f5
Note that the dataset is balanced, so accuracy coincides with BA, and weighted average coincides with macro average.
Inconsistency for Yeast dataset In this experiment, we consider the Yeast dataset8 from the UCI repository [9]. The task is to predict protein localization sites among ten possible variants. The class sizes are {463, 429, 244, 163, 51, 44, 35, 30, 20, 5}, so the dataset is highly unbalanced.
To this dataset, we apply the following algorithms from the scikit-learn library [25]: DecisionTree, ExtraTree, ExtraTreesEnsemble, NearestNeighbors, RadiusNeighbors, RandomForest, BernoulliNB, GaussianNB, LabelSpreading, QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis, NearestCentroid, MLPClassiﬁer, LogisticRegression, LogisticRegressionCV, RidgeClassiﬁer, RidgeClassiﬁerCV, LinearSVC. Thus, there are 18 algorithms giving 153 possible pairs. For each pair of measures and each pair of algorithms, we check whether the measures are consistent. Aggregating the results over all pairs of algorithms, we obtain Table 13.
We can see that for some measures the disagreement can be signiﬁcant. For example, inconsistency is particularly high for Confusion Entropy, which does not satisfy most of the properties. Interestingly, the best agreement is achieved by CC and κ.
Finally, Table 14 shows inconsistency of different averagings.

8https://archive.ics.uci.edu/ml/datasets/Yeast 29

Table 14: Inconsistency of averagings on the Yeast dataset

F1mic F1mac F1wgt

F1mic
— 13.73 3.27

F1mac
13.73 —
10.46

F1wgt
3.27 10.46
—

Jmic Jmac
Jwgt

Jmic
— 11.11 2.61

Jmac
11.11 — 8.50

Jwgt
2.61 8.50 —

CC CCmic CCmac CCwgt

CC

— 3.27 0.00 0.65

CCmic 3.27 —

0.00 0.65

CCmac 0.00 0.00

—

0.65

CCwgt 0.65 0.65

0.65

—

CD CDmic CDmac CDwgt

CD

— 3.27

0.00

0.65

CDmic 3.27

—

0.00 0.65

CDmac 0.00 0.00

—

0.65

CDwgt 0.65 0.65

0.65

—

GMm 1 ic GMm 1 ac GMw1 gt

GMm 1 ic
— 11.76 7.19

GMm 1 ac
11.76 — 7.19

GMw1 gt
7.19 7.19 —

30

