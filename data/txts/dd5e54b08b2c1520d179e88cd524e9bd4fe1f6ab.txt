Sample Complexity of Sinkhorn Divergences

arXiv:1810.02733v2 [math.ST] 5 Feb 2019

Aude Genevay DMA,
ENS Paris

Lénaic Chizat INRIA

Francis Bach INRIA and
DI, ENS Paris

Marco Cuturi Google and
CREST ENSAE

Gabriel Peyré CNRS and DMA,
ENS Paris

Abstract

1 Introduction

Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on Sinkhorn divergences (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength ε, between OT (ε = 0) and MMD (ε = ∞). Although the tradeoﬀ induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively O(n3 log n), O(n2) and n2 operations given a sample size n), much less is known in terms of their sample complexity, namely the gap between these quantities, when evaluated using ﬁnite samples vs. their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes√, O(1/n1/d) for OT in dimension d and O(1/ n) for MMD, that for SDs has only been studied empirically. In this paper, we (i) derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer ε, (ii) prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and (iii) provide the ﬁrst sample complexity bound for SDs, obtained,by reformulating SDs as a maximization proble√m in a RKHS. We thus obtain a scaling in 1/ n (as in MMD), with a constant that depends however on ε, making the bridge between OT and MMD complete.
Preliminary work. Under review by AISTATS 2019. Do not distribute.

Optimal Transport (OT) has emerged in recent years as a powerful tool to compare probability distributions. Indeed, Wasserstein distances can endow the space of probability measures with a rich Riemannian structure (Ambrosio et al., 2006), one that is able to capture meaningful geometric features between measures even when their supports do not overlap. OT has been, however, long neglected in data sciences for two main reasons, which could be loosely described as computational and statistical : computing OT is costly since it requires solving a network ﬂow problem; and suﬀers from the curse-of-dimensionality, since, as will be made more explicit later in this paper, the Wasserstein distance computed between two samples converges only very slowly to its population counterpart.
Recent years have witnessed signiﬁcant advances on the computational aspects of OT. A recent wave of works have exploited entropic regularization, both to compare discrete measures with ﬁnite support (Cuturi, 2013) or measures that can be sampled from (Genevay et al., 2016). Among the many learning tasks performed with this regularization, one may cite domain adaptation (Courty et al., 2014), text retrieval (Kusner et al., 2015) or multi-label classiﬁcation (Frogner et al., 2015). The ability of OT to compare probability distributions with disjoint supports (as opposed to the Kullback-Leibler divergence) has also made it popular as a loss function to learn generative models (Arjovsky et al., 2017; Salimans et al., 2018; Beaumont et al., 2002).
At the other end of the spectrum, the maximum mean discrepancy (MMD) (Gretton et al., 2006) is an integral probability metric (Sriperumbudur et al., 2012) on a reproducing kernel Hilbert space (RKHS) of test functions. The MMD is easy to compute, and has also been used in a very wide variety of applications, including for instance the estimation of generative models (Li et al., 2015; ?, 2017).
OT and MMD diﬀer, however, on a fundamental aspect: their sample complexity. The deﬁnition of sample

Manuscript under review by AISTATS 2019

complexity that we choose here is the convergence rate of a given metric between a measure and its empirical counterpart, as a function of the number of samples. This notion is crucial in machine learning, as bad sample complexity implies overﬁtting and high gradient variance when using these divergences for parameter estimation. In that context, it is well known that the sample complexity of MMD is independent of the dimension, scaling as √1n (Gretton et al., 2006) where n is the number of samples. In contrast, it is well known that standard OT suﬀers from the curse of dimensionality (Dudley, 1969): Its sample complexity is exponential in the dimension of the ambient space. Although it was recently proved that this result can be reﬁned to consider the implicit dimension of data (Weed and Bach, 2017), the sample complexity of OT appears now to be the major bottleneck for the use of OT in high-dimensional machine learning problems.
A remedy to this problem may lie, again, in regularization. Divergences deﬁned through regularized OT, known as Sinkhorn divergences, seem to be indeed less prone to over-ﬁtting. Indeed, a certain amount of regularization seems to improve performance in simple learning tasks (Cuturi, 2013). Additionally, recent papers (Ramdas et al., 2017; Genevay et al., 2018) have pointed out the fact that Sinkhorn divergences are in fact interpolating between OT (when regularization goes to zero) and MMD (when regularization goes to inﬁnity). However, aside from a recent central limit theorem in the case of measures supported on discrete spaces (Bigot et al., 2017), the convergence of empirical Sinkhorn divergences, and more generally their sample complexity, remains an open question.
Contributions. This paper provides three main contributions, which all exhibit theoretical properties of Sinkhorn divergences. Our ﬁrst result is a bound on the speed of convergence of regularized OT to standard OT as a function of the regularization parameter, in the case of continuous measures. The second theorem proves that the optimizers of the regularized optimal transport problem lie in a Sobolev ball which is independent of the measures. This allows us to rewrite the Sinkhorn divergence as an expectation maximization problem in a RKHS ball and thus justify the use of kernel-SGD for regularized OT as advocated in (Genevay et al., 2016). As a consequence of this reformulation, we provide as our third contribution a sample complexity result. We focus on how the sample size and the regularization parameter aﬀect the convergence of the empirical Sinkhorn divergence (i.e., computed from samples of two continuous measures) to the continuous Sinkhorn divergence. We show that the Sinkhorn divergence beneﬁts from the same sample complexity as MMD, scaling in √1n but with a constant that depends

on the inverse of the regularization parameter. Thus sample complexity worsens when getting closer to standard OT, and there is therefore a tradeoﬀ between a good approximation of OT (small regularization parameter) and fast convergence in terms of sample size (larger regularization parameter). We conclude this paper with a few numerical experiments to asses the dependence of the sample complexity on ε and d in very simple cases.
Notations. We consider X and Y two bounded subsets of Rd and we denote by |X | and |Y| their respective diameter sup{||x − x |||x, x ∈ X (resp.Y)}. The space of positive Radon measures of mass 1 on X is denoted M1+(X ) and we use upper cases X, Y to denote random variables in these spaces. We use the notation ϕ = O(1 + xk) to say that ϕ ∈ R is bounded by a polynomial of order k in x with positive coeﬃcients.

2 Reminders on Sinkhorn Divergences

We consider two probability measures α ∈ M1+(X ) and β on M1+(Y). The Kantorovich formulation (1942) of optimal transport between α and β is deﬁned by

W (α, β) d=ef. min

c(x, y)dπ(x, y), (P)

π∈Π(α,β) X ×Y

where the feasible set is composed of probability distributions over the product space X × Y with ﬁxed marginals α, β:

Π(α, β) d=ef. π ∈ M1+(X × Y) ; P1 π = α, P2 π = β ,

where P1 π (resp. P2 π) is the marginal distribution of π for the ﬁrst (resp. second) variable, using the projection maps P1(x, y) = x; P2(x, y) = y along with the push-forward operator .

The cost function c represents the cost to move a unit

of mass from x to y. Through this paper, we will

assume this function to be C∞ (more speciﬁcally, we

need

it

to

be

C

d 2

+1

).

When

X

=

Y

is

endowed

with

a

distance dX , choosing c(x, y) = dX (x, y)p where p 1

yields the p-Wasserstein distance between probability

measures.

We introduce regularized optimal transport, which consists in adding an entropic regularization to the optimal transport problem, as proposed in (Cuturi, 2013). Here we use the relative entropy of the transport plan with respect to the product measure α ⊗ β following (Genevay et al., 2016):

Wε(α, β) d=ef. min
π∈Π(α,β)

c(x, y)dπ(x, y)
X ×Y
+ εH(π | α ⊗ β),

(Pε)

Manuscript under review by AISTATS 2019

where

H(π | α ⊗ β) d=ef.

log

X ×Y

dπ(x, y) dα(x)dβ(y)

dπ(x, y). (1)

Choosing the relative entropy as a regularizer allows to express the dual formulation of regularized OT as the maximization of an expectation problem, as shown in (Genevay et al., 2016)

Wε(α, β) = max

u(x)dα(x) + v(y)dβ(y)

u∈C(X ),v∈C(Y) X

Y

u(x)+v(y)−c(x,y)

−ε

e

ε

dα(x)dβ(y) + ε

X ×Y

= max Eα⊗β fεXY (u, v) + ε
u∈C(X),v∈C(Y )

where

fεxy(u, v)

=

u(x)

+

v(y)

−

εe . u(x)+v(y)−c(x,y) ε

This

reformulation as the maximum of an expectation will

prove crucial to obtain sample complexity results. The

existence of optimal dual potentials (u, v) is proved in

the appendix. They are unique α− and β−a.e. up to

an additive constant.

To correct for the fact that Wε(α, α) = 0, (Genevay et al., 2018) propose Sinkhorn divergences, a natural normalization of that quantity deﬁned as

W¯ ε(α,

β)

=

Wε(α,

β)

−

1 (Wε(α,

α)

+

Wε(β,

β)).

(2)

2

This normalization ensures that W¯ ε(α, α) = 0, but also has a noticeable asymptotic behavior as mentioned

in (Genevay et al., 2018). Indeed, when ε → 0 one re-

covers the original (unregularized) OT problem, while

choosing ε → +∞ yields the maximum mean discrep-

ancy associated to the kernel k = −c/2, where MMD

is deﬁned by:

M M Dk(α, β) = Eα⊗α[k(X, X )] + Eβ⊗β[k(Y, Y )] − 2Eα⊗β[k(X, Y )].

In the context of this paper, we study in detail the sample complexity of Wε(α, β), knowing that these results can be extended to W¯ ε(α, β).

3 Approximating Optimal Transport with Sinkhorn Divergences
In the present section, we are interested in bounding the error made when approximating W (α, β) with Wε(α, β). Theorem 1. Let α and β be probability measures on X and Y subsets of Rd such that |X | = |Y| D and assume that c is L-Lipschitz w.r.t. x and y. It holds
0 Wε(α, β) − W (α, β) 2εd log e2√·L·D (3)
d·ε
∼ε→0 2εd log(1/ε). (4)

Proof. For a probability measure π on X × Y, we denote by C(π) = c dπ the associated transport cost and by H(π) its relative entropy with respect to the product measure α ⊗ β as deﬁned in (1). Choosing π0 a minimizer of minπ∈Π(α,β) C(π), we will build our upper bounds using a family of transport plans with ﬁnite entropy that approximate π0. The simplest approach consists in considering block approximation. In contrast to the work of Carlier et al. (2017), who also considered this technique, our focus here is on quantitative bounds.
Deﬁnition 1 (Block approximation). For a resolution ∆ > 0, we consider the block partition of Rd in hypercubes of side ∆ deﬁned as
{Q∆k = [k1 · ∆, (k1 + 1) · ∆[ × . . . [kd · ∆, (kd + 1) · ∆[ ; k = (k1, . . . , kd) ∈ Zd}.

To simplify notations, we introduce Q∆ij d=ef. Q∆i × Q∆j , α∆i d=ef. α(Q∆i ), β∆j d=ef. β(Q∆j ). The block approximation of π0 of resolution ∆ is the measure π∆ ∈ Π(α, β)
characterized by

∆

π0(Q∆ij )

π |Q∆ ij = α∆i · β∆j (α|Q∆ i ⊗ β|Q∆ j )

for all (i, j) ∈ (Zd)2, with the convention 0/0 = 0.

π∆ is nonnegative by construction. Observe also that for any Borel set B ⊂ Rd, one has

∆

d

π0(Q∆ij )

∆

∆

π (B × R ) = (i,j)∈(Zd)2 α∆i · β∆j · α(B ∩ Qi ) · βj

= α(B ∩ Q∆i ) = α(B),
i∈Zd

which proves, using the symmetric result in β, that π∆ belongs to Π(α, β). As a consequence, for any ε > 0 one has Wε(α, β) C(π∆) + εH(π∆). Recalling also that the relative entropy H is nonnegative over the set of probability measures, we have the bound

0 Wε(α, β)−W (α, β) (C(π∆)−C(π0)) +εH(π∆).

We can now bound the terms in the right-hand side, and choose a value for ∆ that minimizes these bounds.
The bound on C(π∆) − C(π0) relies on the Lipschitz regularity of the cost function. Using the fact that π∆(Q∆ij ) = π0(Q∆ij ) for all i, j, it holds

C(π∆) − C(π0) =

π0(Q∆ij ) sup c(x, y)

(i,j )∈(Zd )2

x,y∈Q∆ ij

√ 2L∆ d,

− inf c(x, y)
x,y∈Q∆ ij

Manuscript under review by AISTATS 2019

where L is the Lipsc√hitz constant of the cost (separately in x and y) and ∆ d is the diameter of each set Q∆i .
As for the bound on H(π∆), using the fact that π0(Q∆ij ) 1 we get

H(π∆) =

log

(i,j )∈(Zd )2

π0(Q∆ij ) α∆i · β∆j

π0(Q∆ij )

log(1/α∆i

)

+

log(1/β

∆ j

)

(i,j )∈(Zd )2

= −H∆(α) − H∆(β),

π0(Q∆ij )

where we have deﬁned H∆(α) = i∈Zd α∆i log(α∆i ) and similarly for β. Note that in case α is a discrete measure with ﬁnite support, H∆(α) is equal to (minus)
the discrete entropy of α as long as ∆ is smaller than
the minimum separation between atoms of α. However, if α is not discrete then H∆(α) blows up to −∞ as ∆
goes to 0 and we need to control how fast it does so. Considering α∆ the block approximation of α with constant density α∆i /∆d on each block Q∆i and (minus) its diﬀerential entropy HLd (α∆) = Rd α∆(x) log α∆(x)dx, it holds H∆(α) = HLd (α∆) − d · log(1/∆). Moreover, using the convexity of HLd , this can be compared with the diﬀerential entropy of the uniform probabil-
ity on a hypercube containing X of size 2D. Thus it holds HLd (α∆) −d log(2D) and thus H∆(α) −d · log(2D/∆).
Summing up, we have for all ∆ > 0
√ Wε(α, β) − W (α, β) 2L∆ d + 2εd · log(2D/∆).

T√he above bound is convex in ∆, minimized with ∆ = 2 d · ε/L. This yields

Wε(α, β) − W (α, β)

L·D

4εd + 2εd log √

.

d·ε

4 Properties of Sinkhorn Potentials

We prove in this section that Sinkhorn potentials are bounded in the Sobolev space Hs(Rd) regardless of the marginals α and β. For s > d2 , Hs(Rd) is a reproducing kernel Hilbert space (RKHS): This property will be
crucial to establish sample complexity results later on,
using standard tools from RKHS theory.
Deﬁnition 2. The Sobolev space Hs(X ), for s ∈ N∗, is the space of functions ϕ : X ⊆ Rd → R such that for every multi-index k with |k| s the mixed partial derivative ϕ(k) exists and belongs to L2(X ). It is
endowed with the following inner-product

ϕ, ψ Hs(X ) =

ϕ(k)(x)ψ(k)(x)dx. (5)

|k| s X

Theorem 2. When X and Y are two compact sets of Rd and the cost c is C∞, then the Sinkhorn poten-
tials (u, v) are uniformly bounded in the Sobolev space Hs(Rd) and their norms satisfy

1 ||u||Hs = O 1 + εs−1

1 and ||v||Hs = O 1 + εs−1 ,

with constants that only depend on |X | (or |Y| for v),d,

and c(k) ∞ for k = 0, . . . , s. In particular, we get the following asymptotic behavior in ε: ||u||Hs = O(1) as

ε

→

+∞

and

||u||Hs

=

O

(

1 εs−1

)

as

ε

→

0.

To prove this theorem, we ﬁrst need to state some regularity properties of the Sinkhorn potentials.
Proposition 1. If X and Y are two compact sets of Rd and the cost c is C∞, then

• u(x) ∈ [miny v(y)−c(x, y), maxy v(y)−c(x, y)] for all x ∈ X

• u is L-Lipschitz, where L is the Lipschitz constant of c

• u ∈ C∞(X ) and

u(k)

∞

=

O(1

+

1 εk−1

)

and the same results also stand for v (inverting u and v in the ﬁrst item, and replacing X by Y).

Proof. The proofs of all three claims exploit the optimality condition of the dual problem:

−u(x)

exp

=

ε

v(y) − c(x, y)

exp

β(y)dy. (6)

ε

−u(x)
Since β is a probability measure, e ε is a convex

v(x)−c(x,y)

−u(x)

combination of ϕ : x → e ε and thus e ε ∈

[miny ϕ(y), maxy ϕ(y)]. We get the desired bounds by

taking the logarithm. The two other points use the

following lemmas:

Lemma 1. The derivatives of the potentials are given by the following recurrence

u(n)(x) = gn(x, y)γε(x, y)β(y)dy,

(7)

where

u (x) − c (x, y)

gn+1(x, y) = gn(x, y) +

ε gn(x, y),

g1(x, y) = c (x, y) and γε(x, y) = exp( u(x)+v(yε)−c(x,y) ).

Lemma 2. The sequence of auxiliary functions

(gk)k=0... veriﬁes u(k) ∞ gk ∞. Besides, for all

j = 0, . . . , k, for all k = 0, . . . , n − 2, g(j)

is

n−k ∞

bounded by a polynomial in 1ε of order n − k + j − 1.

Manuscript under review by AISTATS 2019

The detailed proofs of the lemmas can be found in the appendix. We give here a sketch in the case where d = 1. Lemma 1 is obtained by a simple recurrence, consisting in diﬀerentiating both sides of the dual optimality condition. Diﬀerentiating under the integral is justiﬁed with the usual domination theorem, bounding the integrand thanks to the Lipschitz assumption on c, and this bound is integrable thanks to the marginal constraint. Diﬀerentiating once and rearranging terms gives:

u (x) = c (x, y)γε(x, y)β(y)dy.

(8)

where γε is deﬁned in Lemma 1. One can easily see

that

γε(x, y)

=

u

(x)−c ε

(x,y) γε(x, y)

and

this

allows

to

conclude the recurrence, by diﬀerentiating both sides

of the equality. From the primal constaint, we have

that Y γε(x, y)β(y)dy = 1. Thus thanks to Lemma 1 we immediately get that u(n) ∞ gn ∞. For n = 1, since g1 = c we get that u ∞ = c ∞ = L and this proves the second point of Proposition 1. For higher

values of n, we need the result from Lemma 2. This

property is also proved by recurrence, but requires a

bit more work. To prove the induction step, we need

to go from bounds on gn(i−) k, for k = 0, . . . , n − 2 and i = 0, . . . , k to bounds on gn(i+) 1−k, for k = 0, . . . , n − 1 and i = 0, . . . , k. Hence only new quantities that we

need to bound are gn(k+)1−k, k = 0, . . . , n − 1. This is done by another (backwards) recurrence on k which

involves some tedious computations, based on Leibniz

formula, that are detailed in the appendix.

Combining the bounds of the derivatives of the potentials with the deﬁnition of the norm in Hs, is enough
to complete the proof of Theorem 2.

Proof. (Theorem 2) The norm of u in Hs(X ) is

1



2

||u||Hs = 

(u(k))2

|k| s X

1



2

|X | 

2
u(k)  .

∞ |k| s

From Proposition 1 we have that ∀k, u(k) ∞ = O(1 +

1 εk−1

)

and

thus

we

get

that

||u||Hs

=

O(1 +

1 εs−1

).

We

just proved the bound in Hs(X ) but we actually want

to have a bound on Hs(Rd). This is immediate thanks

to the Sobolev extension theorem (Calderón, 1961)

which guarantees that ||u||Hs(Rd) C||u||Hs(X ) under

the assumption that X is a bounded Lipschitz domain.

This result, aside from proving useful in the next section to obtain sample complexity results on the Sinkhorn divergence, also proves that kernel-SGD can be used to solve continuous regularized OT. This idea introduced in Genevay et al. (2016) consists in assuming the

potentials are in the ball of a certain RKHS, to write them as a linear combination of kernel functions and then perform stochastic gradient descent on these coefﬁcients. Knowing the radius of the ball and the kernel associated with the RKHS (here the Sobolev or Matérn kernel) is crucial to obtain good numerical performance and ensure the convergence of the algorithm.

5 Approximation from Samples

In practice, measures α and β are only known through

a ﬁnite number of samples. Thus, what can be actu-

ally computed in practice is the Sinkhorn divergence

between

the

empirical

measures

αˆn

d=ef.

1 n

n i=1

δXi

and βˆn

d=ef.

1 n

n i=1

δYi

,

where

(X1, . . . , Xn)

and

(Y1, . . . , Yn) are n-samples from α and β, that is

n

n

Wε(αˆn, βˆn) = max u(Xi) + v(Yi)

u,v

i=1

i=1

n

u(Xi) + v(Yi) − c(Xi, Yi)

− ε exp

+ε

i=1 ε

1n

= max

fεXiYi (u, v) + ε,

u,v n

i=1

where (Xi, Yi)ni=1 are i.i.d random variables distributed according to α ⊗ β. On actual samples, these quantities
can be computed using Sinkhorn’s algorithm (Cuturi,
2013).

Our goal is to quantify the error that is made by approximating α, β by their empirical counterparts αˆn, βˆn,
that is bounding the following quantity:

|Wε(α, β) − Wε(αˆn, βˆn)| =

1n

|EfεXY (u∗, v∗) −

f

X ε

i

Y

i

(

uˆ,

vˆ)|

,

(9)

n

i=1

where (u∗, v∗) are the optimal Sinkhorn potentials associated with (α, β) and (uˆ, vˆ) are their empirical counterparts.

Theorem 3. Consider the Sinkhorn divergence be-
tween two measures α and β on X and Y two bounded subsets of Rd, with a C∞, L-Lipschitz cost c. One has

E|W (α, β) − W (αˆ , βˆ )| = O

e

κ ε

√

1+

1

ε

εnn

n

ε d/2

where κ = 2L|X | + c ∞ and constants only depend on |X |,|Y|,d, and c(k) ∞ for k = 0 . . . d/2 . In particular, we get the following asymptotic behavior in

ε:

E|Wε(α, β) − Wε(αˆn, βˆn)| =

e

κ ε

O ε d/2 √n

as ε → 0

E|Wε(α, β) − Wε(αˆn, βˆn)| =

O

1 √

n

as ε → +∞.

Manuscript under review by AISTATS 2019

We have:

An interesting feature from this theorem is the fact when ε is large enough, the convergence rate does not depend on ε anymore. This means that at some point, increasing ε will not substantially improve convergence. However, for small values of ε the dependence is critical.
We prove this result in the rest of this section. The main idea is to exploit standard results from PAC-learning in RKHS. Our theorem is an application of the following result from Bartlett and Mendelson (2002) ( combining Theorem 12,4) and Lemma 22 in their paper):
Proposition 2. (Bartlett-Mendelson ’02) Consider α a probability distribution, a B-lipschitz loss and G a given class of functions. Then

1n

Eα sgu∈pG Eα (g, X) − n

(g, Xi)

i=1

2BEαR(G(X1n))

where R(G(X1n)) is the Rademacher complexity of class

G

deﬁned

by

R(G(X1n))

=

supg∈G

Eσ

1 n

n i=1

σi

g(Xi

)

where (σi)i are iid Rademacher random variables. Be-

sides, when G is a ball of radius λ in a RKHS with

kernel k the Rademacher complexity is bounded by

R(Gλ(X1n))

λn n k(Xi, Xi).
i=1

EfεXY (u∗, v∗) − EfεXY (uˆ, vˆ)

1n

fεXY (u∗, v∗) −

fεXiYi (u∗, v∗)

E

n

i=1

1n

1n

+

fεXiYi (u∗, v∗) −

fεXiYi (uˆ, vˆ)

n

n

i=1

i=1

1n

+

fεXiYi (uˆ, vˆ) − EfεXY (uˆ, vˆ)

n

i=1

(10) (11) (12)

Both (10) and (12) can be bounded by

sup(u,v)∈(Hs )2 |EfεXY (u, v)

−

1 n

n i=1

fεXi Yi

(u,

v)|

λ

while (11) is non-positive since (uˆ, vˆ) is the maximizer

of n1

n i=1

fεXi Yi

(·,

·).

To apply Proposition 2 to Sinkhorn divergences we need to prove that (a) the optimal potentials are in a RKHS and (b) our loss function f ε is Lipschitz in the potentials.
The ﬁrst point has already been proved in the previous section. The RKHS we are considering is Hs(Rd) with s = d2 + 1. It remains to prove that f ε is Lipschitz in (u, v) on a certain subspace that contains the optimal potentials.
Lemma 4. Let A = {(u, v) | u ⊕ v 2L|X | + c ∞}. We have:

Our problem falls in this framework thanks to the following lemma:
Lemma 3. Let Hλs d=ef. {u ∈ Hs(Rd) | ||u||Hs(Rd) λ}, then there exists λ such that:

|Wε(α, β) − Wε(αˆn, βˆn)|

1n

3 sup |EfεXY (u, v) −

f

X ε

i

Y

i

(

u,

v

)|

.

(u,v )∈(Hsλ )2

n i=1

Proof. Inserting EfεXY (uˆ, vˆ) and using the triangle inequality in (9) gives

|Wε(α, β)−Wε(αˆn, βˆn)| |EfεXY (u∗, v∗)−EfεXY (uˆ, vˆ)|

1n

+ |EfεXY (uˆ, vˆ) −

fεXiYi (uˆ, vˆ)|.

n

i=1

From Theorem 2, we know that the all the dual

potentials are bounded in Hs(Rd) by a constant λ

which doesn’t depend on the measures. Thus the

second term is bounded by sup(u,v)∈(Hs )2 |Efε(u, v) −

λ

1 n

n i=1

fε(u,

v)|

.

The ﬁrst quantity needs to be broken down further.

Notice that it is non-negative since (u∗, v∗) is the maxi-

mizer of Efε(·, ·) so we can leave out the absolute value.

(i) the pairs of optimal potentials (u∗, v∗) such that u∗(0) = 0 belong to A,
(ii) f ε is B-Lipschitz in (u, v) on A with B 1 + exp(2 L|X |+ε c ∞ ).
Proof. Let us prove that we can restrict ourselves to a subspace on which f ε is Lipschitz in (u, v).
f ε(u, v, x, y) = u(x)+v(y)−ε exp u(x) + v(y) − c(x, y) ε
∇f ε(u, v) = 1 − exp u + v − c . ε
To ensure that f ε is Lipschitz, we simply need to ensure that the quantity inside the exponential is upperbounded at optimality and then restrict the function to all (u, v) that satisfy that bound.
Recall the bounds on the optimal potentials from Proposition 1. We have that ∀x ∈ X , y ∈ Y,
u(x) L|x| and v(y) max u(x) − c(x, y).
x
Since we assumed X to be a bounded set, denoting by |X | the diameter of the space we get that at optimality

Manuscript under review by AISTATS 2019

Figure 1: W¯ ε(αˆn, αˆn) as a function of n in log-log space : Inﬂuence of ε for ﬁxed d on two uniform distributions on the hypercube with quadratic cost.

Figure 2: W¯ ε(αˆn, αˆn) as a function of n in log-log space : Inﬂuence of d for ﬁxed ε on two uniform distributions on the hypercube with quadratic cost.

∀x ∈ X , y ∈ Y u(x) + v(y)

2L|X | + c ∞ .

Let us denote A = {(u, v) ∈ (Hs(Rd))2 | u ⊕ v 2L|X | + c ∞}, we have that ∀(u, v) ∈ A,

|∇f ε(u, v)|

L|X | + 1 + exp(2

c ∞ ).

ε

We now have all the required elements to prove our sample complexity result on the Sinkhorn loss, by applying Proposition 2.

Proof. (Theorem 3) Since fε is Lipschitz and we are optimizing over Hs(Rd) which is a RKHS, we can apply
Proposition 2 to bound the sup in Lemma 3. We get:

E|Wε(α, β) − Wε(αˆn, βˆn)|

2Bλ 3nE

n
k(Xi, Xi)
i=1

where B 1 + exp(2 L|X |+ε c ∞ ) (Lemma 4), λ =

O(max(1, εd1/2 )) (Theorem 2). We can further bound

n i=1

k(Xi

,

Xi

)

by

n maxx∈X k(x, x) where k is

the kernel associated to Hs(Rd) (usually called

Matern or Sobolev kernel) and thus maxx∈X k(x, x) =

k(0, 0) := K which doesn’t depend on n or ε. Combin-

ing all these bounds, we get the convergence rate in

√1n with diﬀerent asymptotic behaviors in ε when it is

large or small.

Using similar arguments, we can also derive a concentration result: Corollary 1. With probability at least 1 − δ,

|Wε(α, β) − Wε(αˆn, βˆn)|

λK 6B √ + C
n

2 log 1δ n

where B, λ, K are deﬁned in the proof above, and C = κ + ε exp( κε ) with κ = 2L|X | + c ∞.

Proof. We apply the bounded diﬀerences (Mc Diarmid)

inequality to g : (x1, . . . , xn) → supu,v∈Hs (EfεXY −

λ

1 n

f

X ε

i

,Yi

)

.

From

Lemma

4

we

get

that

∀x, y,

f

xy ε

(

u,

v

)

κ + εeκ/ε d=ef. C, and thus, changing one of the variables

in g changes the value of the function by at most 2C/n.

Thus the bounded diﬀerences inequality gives

P (|g(X1, . . . , Xn) − Eg(X1, . . . , Xn)| > t)

t2n 2 exp( 2C2 )

Choosing t = C least 1 − δ

2 log δ1 yields that with probability at
n

g(X1, . . . , Xn)

Eg(X1, . . . , Xn) + C

2 log 1δ n

and from Theorem 3 we already have

Eg(X1, . . . , Xn) = E sup s (EfεXY − n1 fεXi,Yi )
u,v∈Hλ

2BλK √. n

Manuscript under review by AISTATS 2019

Figure 3: W¯ ε(αˆn, αˆn) as a function of n in log-log space - cost c(x, y) = ||x − y||1 with uniform distributions (two leftmost ﬁgures) and quadratic cost c(x, y) = ||x − y||22 with standard normal distributions (right ﬁgure).

6 Experiments

We conclude with some numerical experiments on the

sample complexity of Sinkhorn Divergences. Since

there are no explicit formulas for Wε in general, we

consider

W¯ ε(αˆn, αˆn)

where

αˆn

d=ef.

1 n

n i=1

δXi,

αˆn

d=ef.

1 n

n i=1

δXi

and (X1, . . . , Xn) and (X1, . . . , Xn) are

two independent n-samples from α. Note that we use

in this section the normalized Sinkhorn Divergence as deﬁned in (2), since we know that W¯ ε(α, α) = 0 and thus W¯ ε(αˆn, αˆn) → 0 as n → +∞ .

Each of the experiments is run 300 times, and we plot the average of W¯ ε(αˆn, αˆn) as a function of n in log-log
space, with shaded standard deviation bars.

First, we consider the uniform distribution over a hypercube with the standard quadratic cost c(x, y) = ||x−y||22, which falls within our framework, as we are dealing with a C∞ cost on a bounded domain. Figure 1 shows the inﬂuence of the dimension d on the convergence, while Figure 2 shows the inﬂuence of the regularization ε on the convergence for a given dimension. The inﬂuence of ε on the convergence rate increases with the dimension: the curves are almost parallel for all values of ε in dimension 2 but they get further apart as dimension increases. As expected from our bound, there is a cutoﬀ which happens here at ε = 1. All values of ε 1 have similar convergence rates, and the dependence on 1ε becomes clear for smaller values. The same cutoﬀ appears when looking at the inﬂuence of the dimension on the convergence rate for a ﬁxed ε. The curves are parallel for all dimensions for ε 1 but they have very diﬀerent slopes for smaller ε.

We relax next some of the assumptions needed in our theorem to see how the Sinkhorn divergence behaves empirically. First we relax the regularity assumption on the cost, using c(x, y) = ||x−y||1. As seen on the two left images in ﬁgure 3 the behavior is very similar to the quadratic cost but with a more pronounced inﬂuence of ε, even for small dimensions. The fact that the convergence rate gets slower as ε gets smaller is already

very clear in dimension 2, which wasn’t the case for the quadratic cost. The inﬂuence of the dimension for a given value of ε is not any diﬀerent however.
We also relax the bounded domain assumption, considering a standard normal distribution over Rd with a quadratic cost. While the inﬂuence of ε on the convergence rate is still obvious, the inﬂuence of the dimension is less clear. There is also a higher variance, which can be expected as the concentration bound from Corollary 1 depends on the diameter of the domain.
For all curves, we observe that d and ε impact variance, with much smaller variance for small values of ε and high dimensions. From the concentration bound, the dependency on ε coming from the uniform bound on fε is of the form ε exp(κ/ε), suggesting higher variance for small values of ε. This could indicate that our uniform bound on fε is not tight, and we should consider other methods to get tighter bounds in further work.
7 Conclusion
We have presented two convergence theorems for SDs: a bound on the approximation error of OT and a sample complex√ity bound for empirical Sinkhorn divergences. The 1/ n convergence rate is similar to MMD, but with a constant that depends on the inverse of the regularization parameter, which nicely complements the interpolation property of SDs pointed out in recent papers. Furthermore, the reformulation of SDs as the maximization of an expectation in a RKHS ball also opens the door to a better use of kernel-SGD for the computation of SDs.
Our numerical experiments suggest some open problems. It seems that the convergence rate still holds for unbounded domains and non-smooth cost functions. Besides, getting tighter bounds in our theorem might allow us to derive a sharp estimate on the optimal ε to approximate OT for a given n, by combining our two convergence theorems together.

Manuscript under review by AISTATS 2019

References
L. Ambrosio, N. Gigli, and G. Savaré. Gradient ﬂows in metric spaces and in the space of probability measures. Springer, 2006.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3 (Nov):463–482, 2002.
M. A. Beaumont, W. Zhang, and D. J. Balding. Approximate bayesian computation in population genetics. Genetics, 162(4):2025–2035, 2002.
J. Bigot, E. Cazelles, and N. Papadakis. Central limit theorems for sinkhorn divergence between probability distributions on ﬁnite spaces and statistical applications. arXiv preprint arXiv:1711.08947, 2017.
A. Calderón. Lebesgue spaces of diﬀerentiable functions. In Proc. Sympos. Pure Math, volume 4, pages 33–49, 1961.
G. Carlier, V. Duval, G. Peyré, and B. Schmitzer. Convergence of entropic schemes for optimal transport and gradient ﬂows. SIAM Journal on Mathematical Analysis, 49(2):1385–1418, 2017.
Y. Chen, T. Georgiou, and M. Pavon. Entropic and displacement interpolation: a computational approach using the hilbert metric. SIAM Journal on Applied Mathematics, 76(6):2375–2396, 2016.
N. Courty, R. Flamary, and D. Tuia. Domain adaptation with regularized optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274–289. Springer, 2014.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Adv. in Neural Information Processing Systems, pages 2292–2300, 2013.
R. Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical Statistics, 40(1): 40–50, 1969.
C. Frogner, C. Zhang, H. Mobahi, M. Araya, and T. Poggio. Learning with a Wasserstein loss. In Adv. in Neural Information Processing Systems, pages 2044–2052, 2015.
A. Genevay, M. Cuturi, G. Peyré, and F. Bach. Stochastic optimization for large-scale optimal transport. In D. D. Lee, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Proc. NIPS’16, pages 3432–3440. Curran Associates, Inc., 2016.
A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1608–1617, 2018.

A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. Smola. A kernel method for the two-sampleproblem. In Adv. in Neural Information Processing Systems, pages 513–520, 2006.
L. Kantorovich. On the transfer of masses (in Russian). Doklady Akademii Nauk, 37(2):227–229, 1942.
M. Kusner, Y. Sun, N. Kolkin, and K. Q. Weinberger. From word embeddings to document distances. In Proc. of the 32nd Intern. Conf. on Machine Learning, pages 957–966, 2015.
C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. MMD GAN: Towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584, 2017.
Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Conference on Machine Learning, pages 1718–1727, 2015.
A. Ramdas, N. G. Trillos, and M. Cuturi. On wasserstein two-sample testing and related families of nonparametric tests. Entropy, 19(2):47, 2017.
T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. In International Conference on Learning Representations, 2018.
B. Sriperumbudur, K. Fukumizu, A. Gretton, B. Schoelkopf, and G. Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550–1599, 2012.
J. Weed and F. Bach. Sharp asymptotic and ﬁnitesample rates of convergence of empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.

Manuscript under review by AISTATS 2019

Appendix
Proof. (Lemma 1) For better clarity, we carry out the computations in dimension 1 but all the arguments are valid in higher dimension and we will clarify delicate points throughout the proof. Diﬀerentiating both sides of the optimality condition (6) and rearranging yields

u (x) = c (x, y)γε(x, y)β(y)dy.

(13)

Notice that γε(x, y) =

u

(x)−c ε

(x,y)

γε(x,

y).

Thus by immediate recurrence (diﬀerentiating both sides of the

equality again) we get that

u(n)(x) = gn(x, y)γε(x, y)β(y)dy,

(14)

where

gn+1(x,

y)

=

gn(x,

y)

+

u

(x)−c ε

(x,y) gn(x,

y)

and

g1(x,

y)

=

c

(x,

y)

To extend this ﬁrst lemma to the d-dimensional case, we need to consider the sequence of indexes σ = (σ1, σ2, . . . ) ∈ {1, . . . , d}N which corresponds to the axis along which we successively diﬀerentiate. Using the same reasoning as above, it is straightforward to check that

∂ku =
∂xσ1 . . . ∂xσk

gσ,k γε

where gσ,1 = ∂x∂σc1 and gσ,k+1 = ∂∂gxσσ,kk++11 + 1ε

− ∂u
∂ xσk+1

∂c ∂ xσk+1

gσ,k+1

Proof. (Lemma 2) The proof is made by recurrence on the following property : Pn : For all j = 0, . . . , k, for all k = 0, . . . , n − 2, gn(j−)k is bounded by a polynomial in 1ε of order n − k + j − 1.
∞
Let us initialize the recurrence with n = 2

u −c

g2 = g1 + ε g1

(15)

g2 ∞

g1 ∞ + u ∞ +ε c ∞ g1 ∞

(16)

Recall that u ∞ = g1 ∞ = c ∞. Let C = maxk c(k) ∞, we get that g2 ∞ required form.

C + C+ε C C which is of the

Now assume that Pn is true for some n 2. This means we have bounds on gn(i−) k, for k = 0, . . . , n − 2 and i = 0, . . . , k. To prove the property at rank n + 1 we want bounds on gn(i+) 1−k, for k = 0, . . . , n − 1 and i = 0, . . . , k. The only new quantity that we need to bound are gn(k+)1−k, k = 0, . . . , n − 1. Let us start by bounding g2(n−1) which corresponds to k = n − 1 and we will do a backward recurrence on k. By applying Leibniz formula for the
successive derivatives of a product of functions, we get

u −c g2 = g1 + ε g1

(n−1)

(n) n−1 n − 1 u(p+1) − c(p+1) (n−1−p)

g2

= g1 +

p

ε

g1

p=0

g2(n−1)
∞

g1(n)

n−1
+
∞ p=0

n−1 p

u(p+1) ∞ + ε

c(p+1) ∞

g1(n−1−p)
∞

n−1
C+
p=0

n−1 p

gp+1 ∞ + C C ε

(17) (18) (19) (20)

Manuscript under review by AISTATS 2019

Thanks to Pn we have that gp ∞

p i=0

ai,p

1 εi

,

p

=

1, . . . , n

so

the

highest

order

term

in

ε

in

the

above

inequality

is

1 εn

.

Thus

we

get

g2(n−1)

∞

n+1 i=0

ai,2,n−1

1 εi

which

is

of

the

expected

order

Now assume gn(j+)1−j are bounded with the appropriate polynomials for j < k n − 1. Let us bound gn(k+)1−k

g(k)
n+1−k ∞

(k+1)

kk

gn−k

+

∞

p

p=0

(k+1)

kk

gn−k

+

∞

p

p=0

u(p+1) ∞ + ε

c(p+1) ∞

g(k−p)
n−k ∞

gp+1 ∞ + C ε

g(k−p)
n−k ∞

(21) (22)

The ﬁrst term g(k+1) is bounded with a polynomial of order 1 by recurrence assumption. Regarding the

n−k ∞

εn+1

terms in the sum, they also have all been bounded and

gp+1 ∞ gn(k−−kp)
∞

p1 ai,p+1 εi
i=0

n−p 1 ai,n−k,k−p εi
i=0

n1 a˜i εi
i=0

So g(k)
n+1−k ∞

n+1 i=0

ai,n+1−k,k

1 εi

To extend the result in Rd, the recurrence is made on the the following property

g(j)
σ,n−k ∞

n−k+|j|−1 1 ai,n−k,j,σ εi
i=0

∀j | |j| = 0, . . . , k ∀k = 0, . . . , n − 2 ∀σ ∈ {1, . . . , d}N (23)

where j is a multi-index since we are dealing with multi-variate functions, and gσ,n−k is deﬁned at the end of the previous proof. The computations can be carried out in the same way as above, using the multivariate version of Leibniz formula in (18) since we are now dealing with multi-indexes.

