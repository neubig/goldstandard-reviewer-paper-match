arXiv:1802.09022v3 [math.OC] 20 Sep 2020

AN ACCELERATED METHOD FOR DERIVATIVE-FREE SMOOTH STOCHASTIC CONVEX OPTIMIZATION∗
EDUARD GORBUNOV† , PAVEL DVURECHENSKY‡ , AND ALEXANDER GASNIKOV§
Abstract. We consider an unconstrained problem of minimizing a smooth convex function which is only available through noisy observations of its values, the noise consisting of two parts. Similar to stochastic optimization problems, the ﬁrst part is of stochastic nature. The second part is additive noise of unknown nature, but bounded in absolute value. In the two-point feedback setting, i.e. when pairs of function values are available, we propose an accelerated derivative-free algorithm together with its co√mplexity analysis. The complexity bound of our derivative-free algorithm is only by a factor of n larger than the bound for accelerated gradient-based algorithms, where n is the dimension of the decision variable. We also propose a non-accelerated derivative-free algorithm with a complexity bound similar to the stochastic-gradient-based algorithm, that is, our bound does not have any dimension-dependent factor except logarithmic. Notably, if the diﬀerence between the starting point and the solution is a sparse vector, for both our algorithms, we obtain a better complexity bound if the algorithm uses an 1-norm proximal setup, rather than the Euclidean proximal setup, which is a standard choice for unconstrained problems
Key words. Derivative-Free Optimization, Zeroth-Order Optimization, Stochastic Convex Optimization, Smoothness, Acceleration
AMS subject classiﬁcations. 90C15, 90C25, 90C56

1. Introduction. Derivative-free or zeroth-order optimization [58, 34, 16, 63, 24] is one of the oldest areas in optimization, which constantly attracts attention from the learning community, mostly in connection to online learning in the bandit setup [17] and reinforcement learning [60, 23, 35, 22]. We study stochastic derivative-free optimization problems in a two-point feedback situation, considered by [1, 30, 62] in the learning community and by [55, 64, 41, 42, 40] in the optimization community. Two-point setup allows one to prove complexity bounds, which typically coincide with the complexity bounds for gradient-based algorithms up to a small-degree polynomial of n, where n is the dimension of the decision variable. On the contrary, problems with one-point feedback are harder and complexity bounds for such problems either have worse dependence on n, or worse dependence on the desired accuracy of the solution, see [52, 57, 36, 2, 45, 61, 49, 5, 18] and the references therein.
More precisely, we consider the following optimization problem

(1.1)

min f (x) := Eξ[F (x, ξ)] = F (x, ξ)dP (x) ,

x∈Rn

X

where ξ is a random vector with probability distribution P (ξ), ξ ∈ X , and the function f (x) is closed and convex. Note that F (x, ξ) can be non-convex in x with positive probability. Moreover, we assume that, almost sure w.r.t. distribution P , the function

∗Submitted to the editors 30 April, 2019. Funding: The work of Eduard Gorbunov in Section 2.3. was supported by the Ministry of
Science and Higher Education of the Russian Federation (Goszadaniye) No. 075-00337-20-03, project No. 0714-2020-0005.
†Moscow Institute of Physics and Technology; National Research University Higher School of Economics (eduard.gorbunov@phystech.edu, https://eduardgorbunov.github.io/).
‡Weierstrass Institute for Applied Analysis and Stochastics; Institute for Information Transmission Problems RAS; National Research University Higher School of Economics (pavel.dvurechensky@wias-berlin.de).
§Moscow Institute of Physics and Technology; Institute for Information Transmission Problems RAS; National Research University Higher School of Economics (gasnikov@yandex.ru)
1

2

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

F (x, ξ) has gradient g(x, ξ), which is L(ξ)-Lipschitz continuous with respect to the Euclidean norm. We assume that we know a constant L2 0 such that EξL(ξ)2 ≤ L2 < +∞. Under these assumptions, Eξg(x, ξ) = ∇f (x) and f is L2-smooth, i.e. has L2-Lipschitz continuous gradient with respect to the Euclidean norm. Also we assume that, for all x,

(1.2)

Eξ[ g(x, ξ) − ∇f (x) 22] σ2,

where · 2 is the Euclidean norm. We emphasize that, unlike [30], we do not as-

sume that Eξ

g(x, ξ)

2 2

is bounded since it is not the case for many unconstrained

optimization problems, e.g. for deterministic quadratic optimization problems.

Finally, we assume that we are in the two-point feedback setup, which is also con-

nected to the common random numbers assumption, see [48] and references therein. Speciﬁcally, an optimization procedure, given a pair of points (x, y) ∈ R2n, can obtain

a pair of noisy stochastic realizations (f (x, ξ), f (y, ξ)) of the objective value f , which

we refer to as oracle call. Here

(1.3)

f (x, ξ) = F (x, ξ) + η(x, ξ), |η(x, ξ)| ∆, ∀x ∈ Rn, a.s. in ξ,

and there is a possibility to obtain an iid sample ξ from P . This makes our problem

more complicated than problems studied in the literature. Not only do we have

stochastic noise in problem (1.1), but also an additional noise η(x, ξ), which can be

adversarial.

Our model of the two-point feedback oracle is pretty general and covers deter-

ministic exact oracle or even speciﬁc types of one-point feedback oracle. For example,

if the function F (x, ξ) is separable, i.e. F (x, ξ) = f (x) + h(ξ), where Eξ [h(ξ)] = 0,

|h(ξ)|

≤

∆ 2

for

all

ξ

and

the

oracle

gives

us

F (x, ξ)

at

a

given

point

x,

then

for

all

ξ1, ξ2

we can deﬁne f (x, ξ1) = F (x, ξ1) and f (y, ξ2) = F (y, ξ2) = F (y, ξ1) + h(ξ2) − h(ξ1). Since |h(ξ2) − h(ξ1)| ≤ |h(ξ2)| + |h(ξ1)| ≤ ∆ we can use representation (1.3) omitting dependence of η(x, ξ1) on ξ2. Moreover, such an oracle can be encountered in practice as rounding errors can be modeled as a process of adding a random bit modulo 2

to the last or several last bits in machine number representation format (see [37] for

details).

As it is known [47, 26, 31, 38], if a stochastic approximation g(x, ξ) for the gradient

of f is available, an accelerated gradient method has oracle complexity bound (i.e. the

total number of stochastic ﬁrst-order oracle calls) O max

L2

R

2 2

/ε,

σ

2

R22

/ε

2

,

where ε is the target optimization error in terms of the objective residual, the goal being to ﬁnd such xˆ that Ef (xˆ) − f ∗ ≤ ε. Here f ∗ is the global optimal value of f , R2 is such that x0 − x∗ 2 ≤ R2 with x∗ being some solution. The question, to which
we give a positive answer in this paper, is as follows.

Is it possible to solve a stochastic optimization problem with the same ε-dependence

in the iteration and sample complexity and only noisy observations of the objective

value?

Many existing ﬁrst- and zero-order methods are based on so-called proximal setup

(see [9] and Subsection 2.1 for the precise deﬁnition). This includes a choice of some norm in Rn and a corresponding prox-function, which is strongly convex with respect

to this norm. Standard gradient method for unconstrained problems such as (1.1) is

obtained when one chooses the Euclidean · 2-norm as the norm and squared Euclidean norm as the prox-function. We go beyond this conventional path and consider
· 1-norm in Rn and corresponding prox-function given in [9]. Yet this proximal setup

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 3

is described in the textbook, we are not aware of any particular examples where it is
used for unconstrained optimization problems. Notably, as we show in our analysis,
this choice can lead to better complexity bounds. In what follows, we character-
ize these two cases by the choice of · p-norm with p ∈ {1, 2} and its conjugate q ∈ {2, ∞}, given by the identity p1 + 1q = 1.

1.1. Related Work. Online optimization with two-point bandit feedback was

considered in [1], where regret bounds were obtained. Non-smooth determinis-

tic and stochastic problems in the two-point derivative-free oﬄine optimization set-

ting were considered in [55].1 Non-smooth stochastic problems were considered in

[62] and independently in [7], the latter paper considering also problems with ad-

ditional noise of an unknown nature in the objective value. The authors of [30]

consider smooth stochastic optimization problems, yet under additional quite restric-

tive assumption Eξ

g(x, ξ)

2 q

< + ∞.

Their bound was improved in [40, 39] for

the problems with non-Euclidean proximal setup and noise in the objective value.

Strongly convex problems with diﬀerent smoothness assumptions were considered in

[39, 7]. Smooth stochastic convex optimization problems, without the assumption

that

E

g(x, ξ)

2 2

<

+∞,

were

studied

in

[42,

41]

for

the

Euclidean

case.

Accelerated

and non-accelerated derivative-free method for smooth but deterministic problems

were proposed in [55] and extended in [14, 33] for the case of additional bounded

noise in the function value.

Table 1 presents a detailed comparison of our results and most close results in the

literature on two-point feedback derivative-free optimization and assumptions, under

which they are obtained. The ﬁrst row corresponds to the non-smooth setting with the

assumption that Eξ

g(x, ξ)

2 2

≤ M22, which mostly restricts the scope to constrained

optimization problems on a convex set with the diameter Rp measured by · p-norm.

This setting is very well understood with the proposed methods being able to solve

stochastic optimization problems with additional bounded noise in the objective value

and to use non-Euclidean proximal setup. Importantly, non-Euclidean proximal setup

corresponding to p = 1, q = ∞ allows one to obtain a complexity bound with only

logarithmic dependence on the dimension n.

Rows 2-6 of Table 1 correspond to smooth problems with L2-Lipschitz continu-

ous gradient, which makes possible to apply Nesterov’s acceleration and obtain better

complexity bounds. In this case stochastic optimization problems are characterized

by the variance σ2 of the stochastic gradient, see (1.2). For the smooth setting the full

picture is not completely understood in the literature, and our goal is to obtain meth-

ods, which provide the full picture similarly to the non-smooth setting by combining

stochastic optimization setup, additional bounded noise in the objective value, accel-

eration, and better complexity bounds achievable owing to the use of non-Euclidean

proximal setup corresponding to p = 1, q = ∞. Previous works for the smooth

case consider only Euclidean case and either deterministic problems with additional

bounded noise [55, 14, 33] or stochastic problems without additional bounded noise

[42, 41].

We also mention the works [52, 57, 56, 28, 36, 59, 25, 39, 2, 49, 8, 18, 61, 45, 44,

5, 45, 6, 50, 3] where derivative-free optimization with one-point feedback is studied

in diﬀerent settings, and works [54, 4] on coupling non-accelerated methods to obtain

acceleration, which inspired our work. After our paper appeared as a preprint, the

papers [10, 15] studied derivative-free quasi-Newton methods for problems with noisy

1We list the references in the order of the date of the ﬁrst appearance, but not in the order of the date of oﬃcial publication.

4

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

Method MD
[30, 40, 39, 37] [62, 7]
RSGF
[42, 41]
RS
[55, 14]
RDFDS [This paper]
AccRS
[55, 33]
ARDFDS [This paper]

Assumptions bound. gr. bound. var.
bound. var.
bound. var.

Oracle complexity, O (·)
2 n q M22Rp2
ε2

p = 1 σ2 ∆ √ √√

max

, nL2 R22 ε

nσ2 R22 ε2

nL2 R22 ε

2

2



max  n q Lε2Rp2 , n q εσ22Rp2 





√

×

×

√ ××

√ √√

n L2R22 ε

√ ××



2



1 1 max n 2 + q

L2Rp2 , n q σ2Rp2 

√

√√

ε

ε2





Table 1

Comparison of oracle complexity (total number of zero-order oracle calls) of diﬀerent methods

with two-point feedback for convex optimization problems. Rp is such that x0 − x∗ p ≤ Rp with x∗

being some solution. In the column “Assumptions” we use “bound. gr.” for Eξ

g(x, ξ)

2 2

≤ M22

and “bound.

var.”

for Eξ

g(x, ξ) − ∇f (x)

2 2

σ2. Column “p = 1” corresponds to the support of

non-Euclidean proximal setup, column “σ2” to the support of stochastic optimization problems, “∆”

corresponds to the support of additional bounded noise of unknown nature. All the rows except the

ﬁrst one assume that f is L2-smooth. O(·) notation means O(·) up to logarithmic factors in n, ε.

function values, and the paper [11] reported theoretical and empirical comparison of diﬀerent gradient approximations for zero-order methods. The authors of [21] combine accelerated derivative-free optimization with accelerated variance reduction technique for ﬁnite-sum convex problems. For a recent review of derivative-free optimization see [48]. We extend the proposed algorithms for a more general setting of inexact directional derivative oracle as well as for strongly convex problems in [32]. Mixed ﬁrst-order/zero-order setting is considered in [12] and zero-order methods for nonsmooth saddle-point problems are developed in [13].

1.2. Our Contributions. As our main contribution, we propose an accelerated method for smooth stochastic derivative-free optimization with two-point feedback, which we call Accelerated Randomized Derivative-Free Directional Search (ARDFDS). Our method has the complexity bound

(1.4)

  1+1
O max n 2 q 

2  L2εRp2 , n q εσ22Rp2  ,


where O hides logarithmic factor of the dimension, Rp is such that x0 − x∗ p ≤ Rp with x∗ being an arbitrary solution to (1.1) and x0 being the starting point of the algorithm. We underline that our bounds hold for any solution. Thus, to obtain the
best possible bound, one can consider the closest solution to the starting point. In the
Euclidean case p = q = 2, the ﬁrst term in the above bound has better dependence
on ε, L2 and R2 than the ﬁrst term in the bound in [42, 41]. Unlike these papers, our bound also covers the non-Euclidean case p = 1, q = ∞ and, due to that, allows to ob-
tain better complexity bounds. To illustrate this, let us consider an arbitrary solution x∗ to (1.1), start method from a point x0 and deﬁne the sparsity s of the vector x0−x∗,

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 5

i.e. x0−x∗ 1 ≤ s· x0−x∗ 2 and 1 ≤ s ≤ √n. Then the complexity of our method for

p = 1, q = ∞ is O max

, ns2 L2

x0 −x∗

2 2

ε

s2 σ 2

x0 −x∗

2 2

ε2

, which is always no worse

than the complexity for p = q = 2, which is O max

, n2 L2

x0 −x∗

2 2

ε

nσ2

x0 −x∗

2 2

ε2

√

and allows to gain up to n if s is close to 1. Notably, this is done automatically,

without any prior knowledge of s. An example of this situation can be a typical

compressed sensing problem [19, 29] of recovering a sparse signal x∗ from noisy ob-

servations of a linear transform of x∗ via solving an optimization problem. In this

case, if x0 = 0 then x0 − x∗ is sparse by the problem assumption. Moreover, since our bounds hold for arbitrary solution x∗, to get better complexity estimate, one can use

the bound obtained using the sparsest solution.

Unlike previous works, we consider additional, possibly adversarial noise η(x, ξ) in

the objective value and analyze how this noise aﬀects the convergence rate estimates.

If the noise can be controlled and ∆ can be made arbitrarily small, e.g. if the objective

is calculated by an auxiliary procedure, we estimate how ∆ should depend on the

target accuracy ε to ensure ﬁnding an ε-solution. If the noise is uncontrolled, e.g. we

only have an estimate for the noise level ∆ and we cannot make ∆ arbitrarily small,

we can run our algorithms and guarantee that they generate a point with expected

objective residual bounded by a quantity dependent on ∆. This is important when

the objective is given as a solution to some auxiliary problem, which can not be

solved exactly, e.g. in bi-level optimization or reinforcement learning. It should also

be mentioned that our assumption Eξ[L(ξ)2] ≤ L2 for some L2 is weaker than the

assumption that there is L2 s.t. L(ξ) ≤ L2 a.s. in ξ, which is used in [42, 41].

As our second contribution, we propose a non-accelerated Randomized Derivative-

Free Directional Search (RDFDS) method with the complexity bound

(1.5)

 2

2 

O max  n q Lε2Rp2 , n q εσ22Rp2  ,





where, unlike [42, 41], the non-Euclidean case p = 1, q = ∞ is also covered with the
gain in the complexity of up to the factor of n in comparison to the case p = q = 2.
Notably, in the non-Euclidean case, we obtain a nearly dimension independent (O
hides logarithmic factor of the dimension) complexity bound despite we use only
noisy function value observations.
Why is it important to improve the ﬁrst term under the maximum?
1. Acceleration when n is large. The ﬁrst term under the maximum dom-
3 1−1√
inates the second term when σ2 ≤ ε 2 n 2 q L2 in the accelerated case and
Rp
when σ2 ≤ εL2 in the non-accelerated case, which could be met in practice if ε, L2 and n are large enough compared to Rp. For example, if p = 1, q = ∞ and we would like to ﬁnd an ε-solution with ε = 10−3 and L2 = 100, Rp = 10, n = 10000 (or larger), and the variance satisﬁes mild assumption σ2 ≤ 10−1,
then the complexity of ARDFDS is better than that of RDFDS.
2. Better dimension dependence in the deterministic case. We underline
that even in the deterministic case with σ = 0 and without additive noise,
both our non-accelerated and accelerated complexi√ty bounds for p = 1 are new. Moreover, disregarding ln n factors, for s ∈ [1, n], the existing bounds [55] are n/s2 and n/s times worse than our new bounds respectively in non-

6

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

accelerated and accelerated cases. Importantly, in the non-accelerated case
our bound is dimension-independent up to a ln n factor.
3. Parallel computation of mini-batches makes acceleration reasonable when σ2 is not small. Even when the second term in (1.4) is dom-
inating and, thus, the total computation time is proportional to the second
term, using parallel computations we can force the total computation time
to be proportional to the ﬁrst term, underlining the importance of making it
smaller via accelerating the method. The idea is to use parallel computations
of mini-batches as follows. Instead of sampling one ξ in each iteration of the
algorithm one can consider a mini-batch of size r, i.e. sample r iid realiza-
tions of ξ and average r ﬁnite-diﬀerence approximations for the gradient to reduce the variance of this approximation from σ2 to σr2 . If one can have an access to at least r processors, in each iteration all processors simulta-
neously in parallel can make a call to the zeroth-order oracle and calculate
ﬁnite-diﬀerence approximation for the gradient. Then a processor chosen to
be central calculates the average of these r approximations, which gives a
mini-batch approximation of the gradient. Since this work is done in parallel,
it takes nearly the same amount of time as using a mini-batch of size 1 in the
standard approach. By choosing suﬃciently large r, one can make the second term in (1.4) (which is now proportional to σr2 ) smaller than the ﬁrst term. Hence, the total computation time will be proportional to the ﬁrst term under
the maximum in (1.4). Such an acceleration can be achieved by a reasonable amount of processors. For example, if σ2 = 1, which is not small, n = 10000, Rp = 10, ε = 10−3 and L2 = 100, then it is suﬃcient to have r = 102.5 ≈ 316 processors which is a small number compared to modern supercomputers and clusters that often have ∼ 105 − 106 processors.

2. Algorithms for Stochastic Convex Optimization.

2.1. Preliminaries. p-norm proximal setup. Let p ∈ [1, 2] and x p be the

n

·

p-norm in Rn deﬁned as

x

p p

=

|xi|p. Further, let · q be its dual, deﬁned by

i=1

g q = max g, x , x p ≤ 1 , where q ∈ [2, ∞] is the conjugate number to p, given
x
by p1 + 1q = 1, and, for q = ∞, by deﬁnition, x ∞ = max |xi|. We also use x 0
i=1,...,n
to denote the number of non-zero components of x ∈ Rn. We choose a prox-function

d(x), which is continuousand 1-strongly convex on Rn with respect to · p, i.e., for any x, y ∈ Rn, d(y) − d(x) − ∇d(x), y − x ≥ 21 y − x 2p. Without loss of generality, we assume that min d(x) = 0. We deﬁne also the corresponding Bregman divergence
x∈Rn
V [z](x) = d(x) − d(z) − ∇d(z), x − z , for x, z ∈ Rn. Note that, by the 1-strong

convexity of d(·),

(2.1)

V [z](x) ≥ 12 x − z 2p, ∀ x, z ∈ Rn.

For p = 1, we choose the prox-function (see [9]) d(x) = x 2κexp(1)n(κ−1)(2−κ)/κ ln n ,
2
where κ = 1 + ln1n and, for the case p = 2, we choose the prox-function to be d(x) = 12 x 22. Main technical lemma. In our proofs of complexity bounds, we rely on the following
lemma. The proof is rather technical and is provided in the appendix.

Lemma 2.1. Let e ∈ RS2(1), i.e. be a random vector uniformly distributed on the

surface

of

the

unit

Euclidean

sphere

in

Rn,

p

∈

[1, 2]

and

q

be

given

by

1 p

+

1 q

=

1.

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 7

Deﬁne ρn = min{q − 1, 16 ln n − 8}n2/q−1. Then, for n 8,

(2.2) (2.3)

Ee

e

2 q

≤

ρn,

and

Ee

s, e

2

e

2 q

≤ 6ρnn

s 22,

∀s ∈ Rn.

Stochastic approximation of the gradient. Based on the noisy observations (1.3) of the objective value, we form the following stochastic approximation of ∇f (x)

(2.4)

∇mf t(x) = 1 m f (x + te, ξi) − f (x, ξi) e,

m

t

i=1

where e ∈ RS2(1), ξi, i = 1, ..., m are independent realizations of ξ, m is the mini-batch size, t is some small positive parameter, which we call smoothing parameter.

2.2. Accelerated Randomized Derivative-Free Directional Search. The method is listed as Algorithm 2.1. Following [55, 42, 41] we assume that L2 is known. The possible choice of the smoothing parameter t and mini-batch size m are discussed below. Note that at every iteration the algorithm requires to solve an auxiliary minimization problem. As it is shown in [9], for both cases p = 1 and p = 2 this minimization can be made explicitly in O(n) arithmetic operations.

Algorithm 2.1 Accelerated Randomized Derivative-Free Directional Search

(ARDFDS)

Input: x0 – starting point; N – number of iterations; L2 – smoothness constant; m ≥ 1 – mini-batch size; t > 0 – smoothing parameter; V (·, ·) – Bregman divergence.

1: y0 ← x0, z0 ← x0.
2: for k = 0, . . . , N − 1. do 3: Generate ek+1 ∈ RS2 (1) independently from previous iterations and ξik+1,
i = 1, ..., m – independent realizations of ξ, which are also independent from

previous iterations.

4: τk ← k+2 2 , xk+1 ← τkzk + (1 − τk)yk.

5: Calculate ∇mf t(xk+1) using (2.4) with e = ek+1 and set yk+1 ← xk+1 −

2L12 ∇mf t(xk+1).

6:

αk

←

k+1 96n2ρ L

, zk+1 ← argmin

αk+1n

∇mf t(xk+1), z − zk

+ V [zk] (z)

.

n2

z∈Rn

7: end for

8: return yN

Theorem 2.2. Let ARDFDS be applied to solve problem (1.1), x∗ be an arbitrary solution to (1.1) and Θp be such that V [z0](x∗) ≤ Θp. Then, for all n ≥ 8,

E[f (yN )] − f (x∗) (2.5)

√

+ + 384n2ρnL2Θp
N2

384N σ2 nL2 m

12 2nΘp N2

L22t + 2t∆

+ 6LN

L22t2

+

16∆2 t2

+ 24nNρ2 L

L22t2

+

16∆2 t2

.

2

n2

Before we prove Theorem 2.2 in the next subsection, let us discuss its result. In the simple case ∆ = 0, all the terms in the r.h.s. of (2.5) can be made smaller than ε for any ε 0 by an appropriate choice of N, m, t. Thus, we consider a more interesting case and assume that the noise level satisﬁes 0 < ∆ L2Θpn3ρ2n/2. The second inequality is non-restrictive since by the L2-smoothness f (x0) − f ∗ L2Θp and it is

8

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

N (ε) m(ε) ∆(ε) t(ε)
N (ε)m(ε)

p=1

max min

n ln nL2Θ1 ε

1, εσ3/22 ·

Θ1 ln n nL2

, √ ε3/2
L2Θ1n ln n

ε2 nL2 Θ1

min

√ , ε3/4
4 L32Θ1n ln n

√ε L2 nΘ1

max

, n ln nL2Θ1 σ2Θ1 ln n

ε

ε2

p=2

max min

n2 L2 Θ2 ε

1, εσ3/22 · ΘL22

, √ε3/2
n L2Θ2

ε2 nL2 Θ2

min

√ , ε3/4
4 n2L32Θ2

√ε L2 nΘ2

max

, n2L2Θ2 σ2Θ2n

ε

ε2

Table 2 Summary of the values for N, m, ∆, t and the total number of function value evaluations N m guaranteeing for the cases p = 1 and p = 2 that Algorithm 2.1 outputs yN satisfying E [f (yN )] − f (x∗) ≤ ε. Numerical constants are omitted for simplicity.

natural to assume that the oracle error is smaller than the initial objective residual.

In

order

to

minimize

the

terms

with

L22t2

+

16∆2 t2

in

the

r.h.s

of

(2.5),

we

set

t

as

2 L∆2 . Substituting this into the r.h.s. of (2.5) and using that, by our assumption

on ∆, Θpn2ρnL2 2nL2Θp∆, we obtain the following inequality

(2.6)

E[f (yN )] − f (x∗)

408L2NΘ2pn2ρn + 3n8L4N2 σm2 + 48N ∆ + 3Nn2ρ∆n

First, we consider the situation of controlled noise level ∆ which can be made arbitrarily small. For example, the value of f is deﬁned as a solution of some auxiliary problem, which can be solved numerically with arbitrarily small accuracy ∆. Then we have control over parameters N, m, ∆ in the r.h.s of (2.6) and can choose these parameters to make it smaller than ε. First, we choose N to make the ﬁrst term to be smaller than ε. After that we choose m to make the second term smaller than ε. Finally, we choose ∆ to make all the other terms smaller than ε. The resulting values of these parameters up to constants are given in Table 2. As a summary, we have the following corollary of Theorem 2.2.
Corollary 2.3. Assume that the value of ∆ can be controlled and satisﬁes 0 < ∆ L2Θpn3ρ2n/2. Assume that for a given accuracy ε 0 the values of the parameters N (ε), m(ε), t(ε), ∆(ε) satisfy relations stated in Table 2 and ARDFDS is applied to solve problem (1.1). Then the output point yN satisﬁes E [f (yN )] − f (x∗) ε. Moreover, the overall number of oracle calls is N (ε)m(ε) given in the same table.
Note that in the case of uncontrolled noise level ∆, the values of this parameter stated in Table 2 can be seen as the maximum value of the noise level which can be tolerated by the method still allowing it to achieve E [f (yN )] − f (x∗) ε.
Next, we consider the case of uncontrolled noise level ∆ and estimate the smallest expected objective residual which can be guaranteed in theory. First, we focus on the following three terms in the r.h.s. of (2.6), for simplicity disregarding the numerical constants,

(2.7)

L2ΘNpn2 2ρn + N ∆ + Nnρ2n∆ ,

and consider two cases a) N nρn and b) N nρn. In the case a), we have that the third term in (2.7) is dominated by the second one. Minimizing then in N the

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 9

t(∆) N (∆)

min

p=1

∆/L2

, L2Θ1n 1/3
∆

L2Θ1n 1/4 ∆

min

p=2

∆/L2

, L2Θ2n2 1/3
∆

L2Θ2n3 1/4 ∆

m(∆) ε(∆)

min σ2 ,

σ2

nL2 ∆

√ (n4 L52 Θ1 ∆3 )1/4
1/

max L2Θ1n∆2 3 , nL2Θ1∆

min σ2 ,

σ2

nL2 ∆

√ (n3 L52 Θ2 ∆3 )1/4
1/

max L2Θ2n2∆2 3 , nL2Θ2∆

Nm

min σ2

Θ1

1/3
,

σ2

n2 L22 ∆4

nL2 ∆

min σ2

Θ2

1/3
,

σ2

nL22 ∆4

L2 ∆

Table 3 Summary of the values for N, m, t and the total number of function value evaluations N m guaranteeing for the cases p = 1 and p = 2 that Algorithm 2.1 outputs yN with minimal possible expected objective residual ε if the oracle noise level ∆ is uncontrolled. Numerical constants and logarithmic factors in n are omitted for simplicity.

upper bound L2ΘNpn22ρn +N ∆ for (2.7), we obtain the optimal number Na) of steps and minimal possible value εa) of this upper bound. Moreover inequality Na) nρn turns out to be equivalent to ∆ L2Θp . In the case b) the second term in (2.7) is dominated
nρ2n
by the third one. Minimizing then in N the upper bound L2ΘNpn22ρn + Nnρ2n∆ for (2.7), we obtain the optimal number Nb) of steps and minimal possible value εb) of this upper bound. Moreover inequality Nb) nρn turns out to be equivalent to ∆ Ln2ρΘ2p . Now
n
we can choose ma) = nNLa2) εσa2) and mb) = nNLb)2 εσb2) to make the second term in the r.h.s. of (2.6) to be of the same order as the smallest achievable error εa) or εb) in the case a) or b) respectively. Finally, we check that ∆ L2Θp is equivalent to the case a) and
nρ2n
inequalities εa) εb), Na) Nb), ma) mb). This means that the smallest possible error is max{εa), εb)} and it is achieved in the number of iterations min{Na), Nb)} with batch size min{ma), mb)}. The corresponding values of the parameters are given in Table 3 and we summarize the result as follows.
Corollary 2.4. Assume that ∆ is known and satisﬁes 0 < ∆ L2Θpn3ρ2n/2, the parameters N (∆), m(∆), t(∆) satisfy relations stated in Table 3 and ARDFDS is applied to solve problem (1.1). Then the output point yN satisﬁes E [f (yN )] − f (x∗) ε(∆), where ε(∆) satisﬁes the corresponding relation in the same table. Moreover,
the overall number of oracle calls is N (∆)m(∆) given in the same table.
Using an additional “light-tail” assumption that Eξ[exp( g(x, ξ)−∇f (x) 22/σ2)] exp(1) and techniques of [43] our algorithm and analysis can be extended to obtain
results in terms of probability of large deviations. For example, in the case of con-
trolled noise level ∆ this means that our algorithm outputs a point yN which satisﬁes P{f (yN ) − f (x∗) ε} 1 − δ, where δ ∈ (0, 1) is the conﬁdence level, for the price of extra ln 1δ factor in N and m.
In the several next subsections we provide the full proof of Theorem 2.2 consisting
of the four following parts. We start with the technical result providing us with
inequalities relating the approximation (2.4) with the stochastic gradient g(x, ξ) and
full gradient ∇f (x). The next two parts are in the spirit of Linear Coupling method of
[4]. Namely, we analyze the progress of the Gradient Descent step (line 5 of ARDFDS)
and estimate the progress of the Mirror Descent step (line 6 of ARDFDS). In the ﬁnal
fourth part, we combine all previous parts and ﬁnish the proof of the theorem. We

10

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

emphasize that in the last part we use a careful analysis of the recurrent inequalities for E[ x∗ − zk p] (see Lemma B.1, proved in Appendix B) in order to bound the terms related to the noise in the objective values.

2.2.1. Inequalities for Gradient Approximation. The proof of the main theorem relies on the following technical result, which connects ﬁnite-diﬀerence approximation (2.4) of the stochastic gradient with the stochastic gradient itself and also with ∇f . This lemma plays a central role in our analysis providing correct dependence of the complexity bounds on the dimension.
Lemma 2.5. For all x, s ∈ Rn, we have

(2.8)

Ee

∇mf t(x)

2 q

(2.9)

Ee

∇mf t(x)

2 2

(2.10)

Ee ∇mf t(x), s

(2.11)Ee

∇f (x), e

e − ∇mf t(x)

2 2

m

12ρn n

gm(x, ξm)

2 2

+

ρn t2 m

L(ξi)2 + 16ρtn2∆2 ,

i=1

m

1 2n

gm(x, ξm)

2 2

−

t2 2m

L(ξi)2 − 8t∆22 ,

i=1

m

n1 gm(x, ξm), s − 2tms√pn L(ξi) − 2∆t√sn p ,

i=1

m

2 n

∇f (x) − gm(x, ξm)

2 2

+

t2 m

L(ξi)2 + 16t∆2 2 ,

i=1

m

where

gm(x, ξm)

:=

1 m

g(x, ξi), ∆ is deﬁned in (1.3), L(ξ) is the Lipschitz constant

i=1

of g(·, ξ), which is the gradient of F (·, ξ).

Proof. First of all, we rewrite ∇mf t(x) as follows

∇mf t(x) =

m
gm(x, ξm), e + m1 θ(x, ξi, t, e) e,
i=1

where

θ(x, ξi, t, e)

=

F (x+te,ξi)−F (x,ξi) t

−

g(x, ξi), e

+ η(x+te,ξit)−η(x,ξi) , i = 1, . . . , m.

By the L(ξ)-smoothness of F (·, ξ) and (1.3), we have

(2.12) Proof of (2.8).

|θ(x, ξi, t, e)|

≤

L(ξ)t 2

+

2t∆ .

(2.13)

Ee

∇mf t(x)

2 q

= Ee
x
2Ee

m

2

gm(x, ξm), e + m1 θ(x, ξi, t, e) e

i=1

q

m

2

gm(x, ξm), e

e

2 q

+

2Ee

1 m

θ(x, ξi, t, e)e

i=1

q

y 12ρn n
12ρn n

gm(x, ξm) gm(x, ξm)

m

2

2 2

+

2ρn m

L(ξ2i)t + 2t∆

i=1

2 + ρnt2 m L(ξ )2 + 16ρn∆2 ,

2

m

i

t2

i=1

where x holds since

x+y

2 q

2

x

2 q

+

2

y

2 q

,

∀x,

y

∈

Rn;

y

follows

from

inequal-

ities (2.2), (2.3), (2.12) and the fact that, for any a1, a2, . . . , am > 0, it holds that

m

2

ai

m
m a2i .

i=1

i=1

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 11

Proof of (2.9).

(2.14)

Ee

∇mf t(x)

2 2

m

2

= Ee gm(x, ξm), e + m1 θ(x, ξi, t, e) e

i=1

2

x

m

1 Ee gm(x, ξm), e e 2 − 1

2
L(ξi)t + 2∆

2

2m

2

t

i=1

y

2m

2

1 2n

gm(x, ξm)

2 2

−

t 2m

L(ξi)2

−

8∆ t2

,

i=1

where x follows from (2.12) and inequality

x+y

2 2

1 2

x

2 2

−

y

2 2

,

∀x,

y

∈

Rn;

y follows from e ∈ RS2(1) and Lemma B.10 in [14], stating that, for any s ∈ Rn,

E

s, e

2

=

1 n

s

22.

Proof of (2.10).

(2.15)

Ee ∇mf t(x), s

m

= Ee

gm(x, ξm), e e, s

+

Ee

1 m

θ(x, ξi, t, e) e, s

i=1

x

m

1 gm(x, ξm), s − 1

L(ξi)t + 2∆ Ee| e, s |

n

m

2

t

i=1

y n1 gm(x, ξm), s − 2tms√pn m L(ξi) − 2∆t√sn p

i=1

where x follows from Ee[n g, e e] = g, ∀g ∈ Rn and (2.12); y follows from Lemma B.10 in [14], since E| s, e | ≤ E s, e 2, and the fact that x 2 x p for p 2.
Proof of (2.11).

(2.16)

Ee

∇f (x), e

e − ∇mf t(x)

2 2

m

2

= Ee

∇f (x), e e −

gm(x, ξm), e

e

−

1 m

θ(x, ξi, t, e)e

i=1

2

x

2

m

2

2Ee

∇f (x) − gm(x, ξm), e e

+ 2Ee

1 m

θ(x, ξi, t, e)e

2

i=1

2

y

2m

2

2 n

∇f (x) − gm(x, ξm)

2 2

+

t m

L(ξi)2

+

16∆ t2

,

i=1

where x holds since

x+y

2 2

2

x

2 2

+

2

y

22, ∀x, y ∈ Rn;

y follows from e ∈ S2(1)

and Lemma B.10 in [14], and (2.12).

2.2.2. Progress of the Gradient Descent Step. The following lemma estimates the progress in step 5 of ARDFDS, which is a gradient step.

Lemma 2.6. Assume that y = x − 2L12 ∇mf t(x). Then, (2.17)

m

gm(x, ξm)

2 2

≤ 8nL2(f (x)−Eef (y))+8

∇f (x)−gm(x, ξm)

2 2

+

5nt m

2

L(ξi)2+ 80nt2∆2 ,

i=1

where gm(x, ξm) is deﬁned in Lemma 2.5, ∆ is deﬁned in (1.3), L(ξ) is the Lipschitz constant of g(·, ξ), which is the gradient of F (·, ξ).
Proof. Since ∇mf t(x) is collinear to e, we have that, for some γ ∈ R, y − x = γe. Then, since e 2 = 1,

∇f (x), y − x = ∇f (x), e γ = ∇f (x), e e, y − x = ∇f (x), e e, y − x .

12

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

From this and L2-smoothness of f , we obtain

f (y)

f (x) +

∇f (x), e e, y − x

+

L2 2

||y

−

x||22

= f (x) + ∇mf t(x), y − x

−

L2 2

||y

−

x||22

x f (x) + ∇mf t(x), y − x

+ L2||y − x||22 + ∇f (x), e e − ∇mf t(x), y − x + L2||y − x||22 + 2L12 ∇f (x), e e − ∇mf t(x) 22,

where x follows from the Fenchel inequality

s, z

− ζ2

z

2 2

≤

1 2ζ

s 22.

Using y =

x − 2L12 ∇mf t(x), we get

4L12 ∇mf t(x) 22 f (x) − f (y) + 2L12 ∇f (x), e e − ∇mf t(x) 22.

Taking the expectation in e we obtain

2m

2 (2.9)

1

1

gm(x, ξm)

2 2

−

t 2m

L(ξi)2

−

8∆ t2

1 4L

Ee

∇mf t(x)

2 2

4L2 2n

2

i=1

f (x) − Eef (y) + 2L12 Ee ∇f (x), e e − ∇mf t(x) 22

(2.11)

f (x)

−

Eef (y)

+

1 2L

2m

2

2 n

∇f (x) − gm(x, ξm)

2 2

+

t m

L(ξi)2

+

16∆ t2

.

2 i=1

Rearranging the terms, we obtain the statement of the lemma.
2.2.3. Progress of the Mirror Descent Step. The following lemma estimates the progress in step 6 of ARDFDS, which is a Mirror Descent step.
Lemma 2.7. For z+ = argmin αn ∇mf t(x), u − z + V [z] (u) we have
u∈Rn

(2.18)

α gm(x, ξm), z − u

6α2nρn

gm(x, ξm)

2 2

+

V

[z](u)

−

Ee[V

[z+](u)

+ α2n2ρn
2

2m

2

t m

L(ξi)2

+

16∆ t2

i=1

+α√n z − u p 2m t m L(ξi) + 2t∆ ,

i=1

where gm(x, ξm) is deﬁned in Lemma 2.5, ∆ is deﬁned in (1.3), L(ξ) is the Lipschitz constant of g(·, ξ), which is the gradient of F (·, ξ).
Proof. For all u ∈ Rn, we have

(2.19)

αn ∇mf t(x), z − u = αn ∇mf t(x), z − z+ + αn ∇mf t(x), z+ − u

x
αn ∇mf t(x), z − z+ + −∇V [z](z+), z+ − u

=y αn ∇mf t(x), z − z+ + V [z](u) − V [z+](u) − V [z](z+)

z

αn ∇mf t(x), z − z+

− 12

z − z+

2 p

+ V [z](u) − V [z+](u)

{ α2n2 ∇mf t(x) 2 + V [z](u) − V [z+](u),

2

q

where x follows from the deﬁnition of z+, whence ∇V [z](z+) + αn∇mf t(x), u −

z+ 0 for all u ∈ Rn; y follows from the “magic identity” Fact 5.3.2 in [9] for the

Bregman divergence; z follows from (2.1); and { follows from the Fenchel inequality

ζ s, z

− 21

z

2 p

≤

ζ2 2

s 2q.

Taking expectation in e, applying (2.10) with s = z − u

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 13

and (2.8), we get

(2.20)

m

αn n1 gm(x, ξm), z − u − t 2zm−√unp L(ξi) − 2∆ tz√−nu p

i=1

αnEe ∇mf t(x), z − u

α2 n2
2 Ee

∇mf t(x)

2 q

+

V

[z](u)

−

Ee[V

[z+](u)]

α2n2 12ρn gm(x, ξm) 2 + ρnt2 m L(ξi)2 + 16ρn∆2 + V [z](u) − Ee[V [z+](u)].

2

n

2

m

t2

i=1

Rearranging the terms, we obtain the statement of the lemma.

2.2.4. Proof of Theorem 2.2. First, we prove the following lemma, which estimates the per-iteration progress of the whole algorithm.

Lemma 2.8. Let {xk, yk, zk, αk, τk}, k all u ∈ Rn,

0 be generated by ARDFDS. Then, for

(2.21)

48n2ρnL2αk2+1Ee,ξ[f (yk+1) | Ek, Ξk] − (48n2ρnL2αk2+1 − αk+1)f (yk)

−V [zk](u) + Ee,ξ[V [zk+1](u) | Ek, Ξk] − Rk+1 αk+1f (u),

(2.22)

R = + k+1

48α2k+1 nρn σ 2 m

61α2k+1 n2 ρn 2

L22t2

+

16∆2 t2

+ αk+1√n zk − u p L22t + 2t∆ ,

where ∆ is deﬁned in (1.3), Ek and Ξk denote the history of realizations of e1, . . . , ek

and

ξ11

,

.

.

.

,

ξ

1 m

,

.

.

.

,

ξ

k 1

,

.

.

.

,

ξ

k m

respectively,

up

to

the

step

k.

Proof. Combining (2.17) and (2.18), we obtain

(2.23)

α

g

m

(x

k

+1

,

ξ

k+1 m

),

z

−

u

48α2n2ρnL2(f (xk+1) − Eef (yk+1))

+V [zk](u) − Ee[V [zk+1](u)] + 48α2nρn

∇f (xk+1) − gm(xk+1, ξmk+1)

2 2

+ 61α22n2ρn

t2 m L(ξk+1)2 + 16∆2

m

i

t2

√ + α n zk − u p

m

t 2m

L(ξik+1)

+

2∆ t

,

i=1

i=1

where gm(x, ξm) is deﬁned in Lemma 2.5 and the expectation in e is conditional on

Ek. By the deﬁnition of gm(x, ξm) and (1.2), for all x ∈ Rn, Eξgm(x, ξm) = ∇f (x)

and Eξ

∇f (x) − gm(x, ξm)

2 2

≤

σm2 .

Using these two facts and taking the expectation

in ξmk+1 conditional on Ξk, we obtain

(2.24)

αk+1 ∇f (xk+1), zk − u

48αk2+1n2ρnL2 (f (xk+1) − Ee,ξ[f (yk+1) | Ek, Ξk])

+V [zk](u) − Ee,ξ[V [zk+1](u) | Ek, Ξk] + Rk+1.

14

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

Further,

αk+1

(f (xk+1) − f (u)) αk+1 ∇f (xk+1), xk+1 − u
= αk+1 ∇f (xk+1), xk+1 − zk + αk+1 ∇f (xk+1), zk − u
=x (1−τkτk)αk+1 ∇f (xk+1), yk − xk+1 + αk+1 ∇f (xk+1), zk − u y (1−τkτk)αk+1 (f (yk) − f (xk+1)) + αk+1 ∇f (xk+1), zk − u
(2.24)
(1−τkτk)αk+1 (f (yk) − f (xk+1)) +48αk2+1n2ρnL2 (f (xk+1) − Ee,ξ[f (yk+1) | Ek, Ξk]) +V [zk](u) − Ee,ξ[V [zk+1](u) | Ek, Ξk] + Rk+1
=z (48αk2+1n2ρnL2 − αk+1)f (yk) −48αk2+1n2ρnL2Ee,ξ[f (yk+1) | Ek, Ξk] + αk+1f (xk+1) +V [zk](u) − Ee,ξ[V [zk+1](u) | Ek, Ξk] + Rk+1.

Here x is since xk+1 := τkzk + (1 − τk)yk ⇔ τk(xk+1 − zk) = (1 − τk)(yk − xk+1), y follows from the convexity of f and inequality 1 − τk 0, and z is since τk = 48αk+11n2ρnL2 . Rearranging the terms, we obtain the statement of the lemma.
Proof of Theorem 2.2. Note that 48n2ρnL2αk2+1 − αk+1 + 192n21ρnL2 = 48n2ρnL2αk2 since

48n2ρnL2αk2+1

−αk+1 + 192n21ρnL2 = 19(2kn+2ρ2)n2L2 − 96nk2+ρ2nL2 + 192n21ρnL2

= k2+149k2+n42−ρ2kL−4+1 = 19(2kn+2ρ1)2L = 48n2ρnL2αk2 .

n2

n2

Taking, for any 1 l N , the full expectation E[·] = Ee1,...,eN ,ξ1,...,ξ1 ,...,ξN ,...,ξN [·] in

1

m

1

m

both sides of (2.21) for k = 0, . . . , l − 1 and telescoping the obtained inequalities2, we

have,

(2.25)

l−1
48n2ρnL2αl2E[f (yl)] + 192n21ρnL2 E[f (yk)] − V [z0](u)
k=1

l−1

l−1

+E[V [zl](u)] − ζ1 αk+1E[ u − zk p] − ζ2 αk2+1

k=0

k=0

l−1
αk+1f (u),
k=0

where we denoted

(2.26)

ζ1 := √n L22t + 2t∆ ,

ζ2 := 48nρn σm2 + 61n22ρn

L22t2

+

16∆2 t2

.

Since u in (2.25) is arbitrary, we set u = x∗, where x∗ is a solution to (1.1), use the

inequality Θp V [z0](x∗), and deﬁne Rk := E[ x∗ − zk p]. Also, from (2.1), we have

√

l−1

2Θpζ1 . To simplify the notation, we deﬁne Bl := ζ2 αk2+1 + Θp +

that ζ1α1R0 ≤ 48n2ρnL2

√

k=0

2Θp ζ1

l−1

l(l+3)

. Since αk+1 = 192n2ρ L and, for all i = 1, . . . , N , f (yi)

48n2 ρn L2

n2

f (x∗), we

k=0

get from (2.25) that

192(ln+21ρ)n2L2 E[f (yl)]

f (x∗) 192(nl+23ρ)nlL2 − 192nl−2ρ1nL2 + Bl

(2.27)

l−1
−E[V [zl](x∗)] + ζ1 αk+1Rk,
k=1

0 192(ln+21ρ)n2L2 (E[f (yl)] − f (x∗))

l−1
Bl − E[V [zl](x∗)] + ζ1 αk+1Rk,
k=1

2Note that α1 = 96n22ρnL2 = 48n21ρnL2 and therefore 48n2ρnL2α21 − α1 = 0.

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 15

which gives E[V [zl](x∗)]

l−1
Bl + ζ1 αk+1Rk and
k=1

(2.28)

1 2

(E[

zl − x∗

p])2

12 E[ zl − x∗ 2p]

E[V [zl](x∗)]

l−1
Bl + ζ1 αk+1Rk,
k=1

√

l−1

whence, Rl 2 · Bl + ζ1 αk+1Rk. This recurrent sequence of Rl’s is analyzed

k=1

√

separately

in

Appendix

B.

Applying

Lemma

B.1

with

a0

=

ζ2α12

+

Θp

+

2Θp ζ1 48n2ρ L

, ak

=

n2

ζ2αk2+1, b = ζ1 for k = 1, . . . , N − 1, we obtain

(2.29)

l−1
Bl + ζ1 αk+1Rk
k=1

√√

2

2

Bl +

2ζ1 ·

l 96n2 ρ

L

, l = 1, . . . , N

n2

Since V [z](x∗) 0, by inequality

(2.30)

19(2Nn+2ρ1n)2L2 (E[f (yN )] − f (x∗))

x
2BN + 4ζ2 ·

N4

1 (96n2ρnL2)2

(2.27), for l = N and the deﬁnition

√

√

2

2

BN +

2ζ1 ·

N 96n2 ρ

L

l−1

n√ 2

= 2ζ2

αk2+1

+

2Θp

+

2Θp ζ1 24n2ρ L

+

k=0 √

n2

y 2Θp + 24n22ΘρnpζL12 + (926ζ2n(2Nρn+L12)3)2 +

of Bl, we have
4ζ12N 4 (96n2 ρn L2 )2
4ζ12N 4 (96n2 ρn L2 )2

where x is due to the fact that, ∀a, b ∈ R, (a + b)2 2a2 + 2b2 and y is be-

N −1

N +1

cause αk2+1 = (96n2ρ1nL2)2 k2

k=0

k=2

(96n2ρ1nL2)2 · (N +1)(N +62)(2N +3)

· 1
(96n2 ρn L2 )2

(N+1)2(N6+1)3(N+1) = (96(nN2+ρn1L)32)2 . Dividing (2.30) by 19(2Nn+2ρ1n)2L2 and substituting ζ1, ζ2

from (2.26), we obtain

E[f (yN )] − f (x∗)

√

384(ΘNp+n21ρ)2nL2 + 1(2N√+21Θ)2p ζ1 + (39864n(2Nρ+nL1)2ζ)22 + 12n2ρnNL42ζ(12N +1)2

+ 384Θp n2 ρn L2
N2

12 2nΘp N2

L22t + 2t∆

+ 3n8L4N2 σm2

+ 6LN

L22t2

+

16∆2 t2

+ 24nNρ2 L

L22t2

+

16∆2 t2

.

2

n2

2.3. Randomized Derivative-Free Directional Search. Our non-accelerated method is listed as Algorithm 2.2. Following [55, 42, 41] we assume that L2 is known. The possible choice of the smoothing parameter t and mini-batch size m are discussed below. Note that at every iteration the algorithm requires to solve an auxiliary minimization problem. As it is shown in [9], for both cases p = 1 and p = 2 this minimization can be made explicitly in O(n) arithmetic operations.
Theorem 2.9. Let RDFDS be applied to solve problem (1.1), x∗ be an arbitrary solution to (1.1), and Θp be such that V [z0](x∗) ≤ Θp. Then

E[f (x¯N )] − f (x∗) (2.31)

384nρNn√L2Θp + L22σm2 + 6Ln2 + 3LN2ρn

+ L22 t2
2

8∆2 t2

+ 8 N2nΘp L22t + 2t∆ ,

∀n ≥ 8.

16

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

Algorithm 2.2 Randomized Derivative-Free Directional Search (RDFDS)

Input: x0 – starting point; N – number of iterations; L2 – smoothness constant; m 1 – mini-batch size; t > 0 – smoothing parameter; α = 48nρ1nL2 – stepsize;
V (·, ·) – Bregman divergence.

1: for k = 0, . . . , N − 1. do 2: Generate ek+1 ∈ RS2 (1) independently from previous iterations and ξik+1,
i = 1, ..., m – independent realizations of ξ, which are also independent from

previous iterations. 3: Calculate ∇mf t(xk) using (2.4) with e = ek+1.

4: xk+1 ← argmin αn ∇mf t(xk), x − xk + V [xk] (x) .

x∈Rn

5: end for

N −1

6:

return

x¯N

←

1 N

xk .

k=0

Proof of Theorem 2.9. The proof of this result is rather similar to the proof of

Theorem 2.2. First of all,

(2.32)

αn ∇mf t(xk), xk − x∗

= αn ∇mf t(xk), xk − xk+1 + αn ∇mf t(xk), xk+1 − x∗

x
αn ∇mf t(xk), xk − xk+1 + −∇V [xk](xk+1), xk+1 − x∗

=y αn ∇mf t(xk), xk − xk+1 + V [xk](x∗) − V [xk+1](x∗) − V [xk](xk+1)

z

αn ∇mf t(xk), xk − xk+1

− 21

xk − xk+1

2 p

+ V [xk](x∗) − V [xk+1](x∗)

α2 n2 2

∇mf t(xk

2 q

+

V

[xk ](x∗ )

−

V

[xk+1](x∗),

where x follows from ∇V [xk](xk+1) + αn∇mf t(xk), x − xk+1 0 for all x ∈ Rn,

y follows from “magic identity” Fact 5.3.2 in [9] for Bregman divergence, and z is

since V [x](y)

1 2

x−y

2p.

Taking conditional expectation Ee[ · | Ek] in both sides of

(2.32) we get

(2.33)

αnEe[ ∇mf t(xk), xk − x∗ | Ek]

α22n2 Ee[

∇mf t(xk)

2 q

|

Ek ]

+V [xk](x∗) − Ee[V [xk+1](x∗) | Ek]

From (2.33), (2.8) and (2.10) for s = xk − x∗, we obtain

gm(xk, ξmk+1), xk − x∗

24α2nρnL2(f (xk) − f (x∗))

+12α2nρ ∇f (x ) − gm(x , ξ k+1) 2 + α2n2ρ · t2 m L (ξk+1)2 + 8α2n2ρn∆2

n

k

km

2

n 2m

2i

t2

i=1

+α√n xk − x∗ p · 2m t m L2(ξik+1) + 2α∆√n txk−x∗ p

i=1

+V [xk](x∗) − Ee[V [xk+1](x∗) | Ek].

Taking conditional expectation Eξ[ · | Ξk] in the both sides of the previous inequality and using the convexity of f and (1.2), we have (2.34)
(α − 24α2nρnL2) (f (xk) − f (x∗)) 12α2nρn σm2 + α2n2ρn L222t2 + 8t∆22
α/√4 +α n xk − x∗ p L22t + 2t∆ + V [xk](x∗) − Ee,ξ[V [xk+1](x∗) | Ek, Ξk],

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 17

since α = 48nρ1nL2 . Denote

(2.35)

ζ1

=

L2 t 2

+

2t∆ ,

ζ2 = L222t2 + 8t∆22 .

Note that (2.36)

ζ12 = L22t + 2t∆ 2 2 · L224t2 + 2 · 4t∆22 = ζ2.

Taking for any 1 l N , the full expectation E[·] = Ee1,...,eN ,ξ1,...,ξ1 ,...,ξN ,...,ξN [·] in

1

m

1

m

both sides of inequalities (2.34) for k = 0, . . . , l − 1 and summing them, we get

(2.37)

0

Nα 4

(E[f

(x¯l)]

−

f

(x∗))

l · 12α2nρn σm2 + lα2n2ρnζ2

√ l−1 +α nζ1 E[

xk − x∗

p]+V [x0](x∗) − E[V [xl](x∗)],

k=0

l−1

where

x¯l

=

1 l

xk. From the previous inequality, since V [z0](x∗)

k=0

Θp, we get

(2.38)

1 2

(E[

xl − x∗

p])2

12 E[ xl − x∗ 2p]

E[V [xl](x∗)]

Θp + l · 12α2nρn σ2

+

lα2n2ρnζ2

+

√ α nδζ1

l−1

E[

xk − x∗

p],

m

k=0

whence, ∀l N , we obtain (2.39)

E[ xl − x∗ p]

√

2

√ l−1

2

Θp

+

l

·

12α2nρn

σ m

+ lα2n2ρnζ2

+α

nζ1

E[ xk − x∗ p].

k=0

Denote Rk = E[ x∗ − xk p] for k = 0, . . . , N . The recurrent sequence of√Rk’s is analyzed separately in Appendix B. Applying Lemma B.2 with a0 = Θp + α nζ1E[ x0 − x∗ p] Θp + α 2nΘpζ1, ak = 12α2nρn σm2 + α2n2ρnζ2, b = √nζ1 for k = 1, . . . , N − 1 we have for l = N

Nα 4

(E[f

(x¯N

)]

−

f

(x∗))

2

√

2

Θp

+N

·

12α2

nρn

σ m

+ N α2n2ρnζ2

+α

2nΘpζ1 +

2nζ1αN

x 2Θp + 24N α2nρn σ2 + 2N α2n2ρnζ2 + 2α 2nΘpζ1 + 4nζ2α2N 2,

m

1

whence E[f (x¯N )] − f (x∗)

(2.36)

√

384nρNnL2Θp + L22σm2 + 6nLζ22 + 8

+ 2nΘp ζ1
N

ζ2 N 3L2 ρn

(2.35)
384nρnL2Θp +

2σ2

+

√

N

L2 m

+ 8 N2nΘp L22t + 2t∆ ,

6Ln2 + 3LN2ρn

+ L22 t2
2

8∆2 t2

where we used also that α = 48nρ1nL2 . Similarly to the discussion above concerning the ARDFDS and its convergence theorem, we can formulate corollaries for the RDFDS in the case of controlled and uncontrolled noise level ∆. In the simple case ∆ = 0, all the terms in the r.h.s.

18

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

N (ε) m(ε) ∆(ε)

p=1
ln nL2Θ1 ε
max 1, Lσ22ε
min nε , nLε22Θ1

p=2
nL2 Θ2 ε
max 1, Lσ22ε
min nε , nLε22Θ2

t(ε) N (ε)m(ε)

min max

ε ,√ ε

nL2

nL22 Θ1

, L2Θ1 ln n σ2Θ1 ln n

ε

ε2

min max

ε ,√ ε

nL2

nL22 Θ1

, nL2Θ2 nσ2Θ2

ε

ε2

Table 4 Summary of the values for N, m, ∆, t and the total number of function value evaluations N m guaranteeing for the cases p = 1 and p = 2 that Algorithm 2.2 outputs x¯N satisfying E [f (x¯N )] − f (x∗) ≤ ε. Numerical constants are omitted for simplicity.

of (2.31) can be made smaller than ε for any ε 0 by an appropriate choice of N, m, t. Thus, we consider a more interesting case and assume that the noise level satisﬁes 0 < ∆ L2Θpnρ2n/2, the second inequality being non-restrictive. In order to minimize the term with L222t2 + 8t∆22 in the r.h.s of (2.31), we set t = 2 L∆2 . Substituting this into the r.h.s. of (2.31) and using that, by our assumption on ∆,
2nL2Θp∆ nρnL2Θp, we obtain an upper bound for E[f (x¯N )] − f (x∗). Following the same steps as in the proof of Corollaries 2.3 and 2.4, we obtain the following results for RDFDS.
Corollary 2.10. Assume that the value of ∆ can be controlled and satisﬁes 0 < ∆ L2Θpnρ2n/2. Assume that for a given accuracy ε 0 the values of the parameters N (ε), m(ε), t(ε), ∆(ε) satisfy the relations stated in Table 4 and RDFDS is applied to solve problem (1.1). Then the output point x¯N satisﬁes E [f (x¯N )] − f (x∗) ≤ ε. Moreover, the overall number of oracle calls is N (ε)m(ε) given in the same table.
Note that in the case of uncontrolled noise level ∆, the values of this parameter stated in Table 4 can be seen as the maximum value of the noise level which can be tolerated by the method still allowing it to achieve E [f (x¯N )] − f (x∗) ε. For a more general case of uncontrolled noise level ∆, we obtain the following Corollary.
Corollary 2.11. Assume that ∆ is known and satisﬁes 0 < ∆ L2Θpnρ2n/2, the parameters N (∆), m(∆), t(∆) satisfy relations stated in Table 5 and RDFDS is applied to solve problem (1.1). Then the output point x¯N satisﬁes E [f (x¯N )] − f (x∗) ε(∆), where ε(∆) satisﬁes the corresponding relation in the same table. Moreover, the overall number of oracle calls is N (∆)m(∆) given in the same table.
Similarly to ARDFDS, RDFDS and its analysis can be extended to obtain convergence in terms of probability of large deviations under additional “light-tail” assumption.
2.4. Role of the algorithms parameters. Role of ∆ and t. We would like to mention that there is no need to know the noise level ∆ to run our algorithms. As it can be seen from (2.5), the ARDFDS method is robust in the sense of [51] to the choice of the smoothing parameter t. Namely, if we under/overestimate ∆ by a constant factor, the corresponding terms in the convergence rate will increase only by a constant factor. Similar remark holds for the assumption that L2 is known.
Our Theorems 2.2 and 2.9 are applicable in two situations, the noise being a) controlled and b) uncontrolled.

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 19

t(∆) N (∆)

min

p=1 ∆/L2

Ln2∆Θ1 ,

L2 Θ1 n∆

min

p=2 ∆/L2

L2∆Θ2 ,

nL2 Θ2 ∆

m(∆) ε(∆) N (∆)m(∆)

min σ2 , √ σ2

nL2 ∆
√

nL32 Θ1 ∆

max n∆, nL2Θ1∆

min

, σ 2 Θ1
n2 ∆2

σ2 nL2 ∆

min σ2 , √ σ2

nL2 ∆
√

nL32 Θ2 ∆

max n∆, nL2Θ2∆

min σn2∆Θ22 , Lσ22∆

Table 5 Summary of the values for N, m, t and the total number of function value evaluations N m

guaranteeing for the cases p = 1 and p = 2 that Algorithm 2.2 outputs x¯N with minimal possible expected objective residual ε if the oracle noise ∆ is uncontrolled. Numerical constants and logarithmic

factors in n are omitted for simplicity.

a) Our assumptions on the noise level in Tables 2 and 4 can be met in practice.

For example, in [14], the objective function is deﬁned by some auxiliary prob-

lem and its value can be calculated with accuracy ∆ at the cost proportional

to ln ∆1 , which would result in only a ln 1ε factor in the total complexity of our methods in this paper combined with the method in [14] for approximating

the function value.

b) The minimum guaranteed accuracy ε(∆) in Tables 3 and 5 can not be arbi-

trarily small, which is reasonable: one can not solve the problem with better

accuracy than the accuracy of the available information. Interestingly, the

minimal possible accuracy for the accelerated method could be larger than

for the non-accelerated method, which means that accelerated methods are

less robust to noise (cf. full gradient methods [27, 46]). To illustrate this, let

us, for simplicity neglect the√numerical constants and consider a case with σ = 0, L2 = 1, Θp = 1, t = 2 ∆, and large N nρn. Then the main terms

in the r.h.s. of (2.5) are n2ρn + N2∆ . Minimizing in N , we have the minimal

√ N2

nρn

accuracy of the order n∆. Similarly, the main terms in the r.h.s. of (2.31)

are nρn + N∆ . Minimizing in N , we have the minimal accuracy of the order

N

ρn √

∆1/2n1/2ρ1n/2 < n∆, which is smaller than for the accelerated method.

Role of σ2. Although, all the related works, which we are aware of, assume σ2 to

be known, adaptivity to the variance σ2 is a very important direction of future work.

Note that similarly to the robustness to ∆, our method is robust to σ2.

3. Experiments. We performed several numerical experiments to illustrate our theoretical results. In particular, we compared our methods with the Euclidean and 1-norm proximal setups and the RSGF method from [41] applied to two problems: minimizing Nesterov’s function and logistic regression problem. For all the results reported below we tuned parameters αk and α for ARDFDS and RDFDS respectively and the stepsize parameter for RSGF. We use E and NE in the plots to refer to the methods with 2-norm and 1-norm proximal setups respectively and RSGF to refer to the method from [41]. The code was written in Python using standard libraries, see the details at https://github.com/eduardgorbunov/ardfds.

20

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

3.1. Experiments with Nesterov’s function. We tested our methods on the problem of minimizing Nesterov’s function [53] deﬁned as:

f (x) = L42

n−1
12 (x1)2 + (xi − xi+1)2 + (xn)2 − x1 ,
i=1

where xi is i-th component of vector x ∈ Rn. f is convex, L2-smooth w.r.t. the

Euclidean norm and attains its minimal value f ∗ = L82 −1 + n+1 1 at the point

x∗ = (x∗,1, . . . , x∗,n) such that x∗,i = 1 − n+i 1 . Moreover, the lower complexity bound for ﬁrst-order methods in smooth convex optimization is attained [53] on this

function.

We add stochastic noise to this function and consider F (x, ξ) = f (x) + ξ a, x ,

where ξ is Gaussian random variable with mean µ = 0 and variance σ2, a ∈ Rn is

some vector in the unit Euclidean sphere, i.e.

a

2 2

=

1.

This implies that f (x) =

Eξ [F (x, ξ)] and F (x, ξ) is L2-smooth in x w.r.t. the Euclidean norm since g(x, ξ) −

g(y, ξ) = ∇f (x)−∇f (y). Moreover, Eξg(x, ξ) = ∇f (x) and Eξ

g(x, ξ) − ∇f (x)

2 2

=

a 22Eξ ξ2 = σ2 for all x ∈ Rn. Also we introduce an additive noise η(x) =

∆ sin

x − x∗

−2 2

.

It is clear that |η(x)| ≤ ∆ for all x ∈ Rn.

Overall, we are in

the setting described in Introduction with f (x, ξ) = F (x, ξ) + η(x) = f (x) + ξ a, x +

∆ sin

x − x∗

−2 2

.

We compare our methods with the Euclidean and 1-norm proximal setups as well

as the RSGF method from [41] applied to this problem for diﬀerent sparsity levels

of x0 − x∗ and diﬀerent values of n, σ and ∆. For all tests we use L2 = 10, adjust starting point x0 such that f (x0) − f (x∗) ∼ 102 and choose t = max{10−8, 2 ∆/L2}.

The second term under the maximum in the deﬁnition of t corresponds to the optimal

choice of t for given ∆ and L2, i.e., it minimizes the right-hand sides of (2.5) and (2.31),

and the ﬁrst term under the maximum is needed to prevent unstable computations

when t is too small.

3.1.1. Experiments with diﬀerent sparsity levels. In this set of experi-
ments we considered diﬀerent choices of the starting point x0 with diﬀerent sparsity levels of x0 − x∗, i.e., for n = 100, 500, 1000 we picked such starting points x0 that vector x0 − x∗ has 1, n/10, n/2 non-zero components. In particular, we shift ﬁrst 1, n/10, n/2 components of x∗ by some constant to obtain x0. In order to isolate the eﬀect of the sparsity from eﬀects coming from the stochastic nature of f (x, ξ) and noise η(x) we
choose σ = ∆ = 0. Our results are reported in Figure 1. As the theory predicts, our
methods with p = 1 work increasingly better than our methods with p = 2 as n is growing when x0 − x∗ 0 is small.
3.1.2. Experiments with diﬀerent variance. In this subsection we report the numerical results with diﬀerent valu√es of σ2. For each choice of the dimension n we used two values of σ2: σs2mall = εx3/02−xn∗L21 and σb2ig = 10000σs2mall with ε = 10−3. As one can see from Table 2, when σ2 = σs2mall the ﬁrst term under the maximum in the complexity bound is dominating (up to logarithmic factors). This implies that
ARDFDS with p = 1 is guaranteed to ﬁnd an ε-solution even with the mini-batch size m = 1 (up to logarithmical factors). We choose ε = 10−3, ∆ = 0 and x0 such that it diﬀers from x∗ only in the ﬁrst component and run the experiments for n = 100, 500, 1000 and σ2 = σs2mall, σb2ig, see Figures 2 and 3. We see in Figure 2 that for σ2 = σs2mall it is suﬃcient to use mini-batches of the size m = 1 to reach accuracy ε = 10−3 and the overall picture is very similar to the one presented in Figure 1.

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 21

n = 100, 2 = = 0, ||x0 x * ||0 = 1

10 1

ARDFDS_E ARDFDS_NE

RDFDS_E

10 3

RDFDS_NE RSGF

10 5

10 7

10 9

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000 n = 100, 2 = = 0, ||x0 x * ||0 = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

100 n = 100, 2 = = 0, ||x0 x * ||0 A=RD5F0DS_E

10 1

ARDFDS_NE RDFDS_E

10 2

RDFDS_NE RSGF

10 3

10 4

10 5 10 6

10 7 0

50000 10N00u00m1b50e00r0 o20f0o00r0a2c50le000ca30l0l0s00 350000 400000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

n = 500, 2 = = 0, ||x0 x * ||0 = 1

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7

100 10 1 10 2 10 3 10 4 10 5 10 6
100 10 1 10 2

0 2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000

n = 500, 2 =

= 0, ||x0

x * ||0 = 50 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 500, 2 =

= 0, ||x0

x * ||0 = 250 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

10 3

10 4

10 5 0

250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

n = 1000, 2 = = 0, ||x0 x * ||0 = 1

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7

100 10 1 10 2 10 3 10 4 10 5
100
10 1

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 1000, 2 =

= 0, ||x0

x * ||0 = 100 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0 500000 10N000u00m15b00e00r0 o20f00o00r0a2c50l0e000ca30l0l0s000 3500000 4000000

n = 1000, 2 =

= 0, ||x0

x * ||0 = 500 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

10 2

10 3

0 500000 10N000u00m15b00e00r0 o20f00o00r0a2c50l0e000ca30l0l0s000 3500000 4000000

f(xk) f(x * ) f(x0) f(x * )

Fig. 1. Numerical results for minimizing Nesterov’s function for diﬀerent sparsity levels of x0 − x∗ and dimensions n of the problem.

In contrast, when σ2 = σb2ig (Figure 3) and m = 1 the methods fail to reach the target accuracy. In these tests accelerated methods show higher sensitivity to the noise and, as a consequence, we see that for n = 500, 1000 and m = 10 RDFDS NE reaches the accuracy ε = 10−3 faster than competitors justifying the following insight that we have from our theory: when the variance is large, non-accelerated methods require smaller mini-batch size m and are able to ﬁnd ε-solution faster than their accelerated counterparts.

3.1.3. Experiments with diﬀerent noise level of the oracle. Here we

present the numerical experiments with diﬀerent values of ∆. To isolate the eﬀect of

the non-stochastic noise, we set σ = 0 for all tests reported in this subsection. We run

the methods for problems with n = 100, 500, 1000 and chose the starting point in the

same way as in Subsection 3.1.2. For each choice of the dimension n we used three

values of ∆: ∆small = min

√ , ε3/2√2

L2

x0 −x∗

2 1

n

ln

n

2ε2

nL2

x0 −x∗

2 1

, ∆medium = 103 · ∆small

and ∆large = 106 ·∆small with ε = 10−3. As one can see from Table 2 when ∆ = ∆small

ARDFDS with p = 1 is guaranteed to ﬁnd an ε-solution. The results are reported in

Figure 4. We see that for larger values of ∆ accelerated methods achieve worse accu-

racy than for small values of ∆. However, in all experiments our methods succeeded

to reach ε-solution with ε = 10−3 meaning that the noise level ∆ in practice can be

much larger than it is prescribed by our theory.

3.1.4. Experiment with large dimension. In Figure 5 we report the experi-

ment results for n = 5000, σ2 = σ2 = ε3/2√nL2 ,

small

x0−x∗ 1

∆ = ∆small = min

√ , ε3/2√2

L2

x0 −x∗

2 1

n

ln

n

2ε2

nL2

x0 −x∗

2 1

, and ε = 10−3. The obtained

results are in a good agreement with our theory and experiment results for smaller

22

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

n = 100, small 2, = 0, m = 1

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7
0
10 1 10 3

25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000 n = 100, small 2, = 0, m = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

10 5 10 7

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

100

n = 100, small 2, = 0, m = 100 ARDFDS_E

ARDFDS_NE

10 2

RDFDS_E RDFDS_NE

RSGF

10 4

10 6

10 8

0.00 0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e07

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

n = 500, small 2, = 0, m = 1 ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

10 2

RDFDS_NE RSGF

10 3

10 4 10 5 10 6

10 7
0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000 n = 500, small 2, = 0, m = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 11.e07

n = 500, small 2, = 0, m = 100

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7

0.0 0.2Numb0e.r4of ora0c.l6e calls0.8 11.e08

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7
0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

n = 1000, small 2, = 0, m = 1 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000 n = 1000, small 2, = 0, m = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0.00 0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e07

n = 1000, small 2, = 0, m = 100

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7

0.00 0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e08

f(xk) f(x * ) f(x0) f(x * )

Fig. 2. Numerical results for minimizing Nesterov’s function with noisy stochastic oracle having σ2 = σs2mall for diﬀerent sizes of mini-batch m and dimensions of the problem n.

dimensions.
3.2. Experiments with logistic regression. In this subsection we report the numerical results for our methods applied to the logistic regression problem:

(3.1)

min
x∈Rn

1M

f (x) = M

fi(x) ,

i=1

fi(x) = log (1 + exp (−yi · (Ax)i)) .

Here fi(x) is the loss on the i-th data point, A ∈ RM×n is a matrix of instances, y ∈ {−1, 1}M is a vector of labels and x ∈ Rn is a vector of parameters (or weights).
It can be e√asily shown that f (x) is convex and L2-smooth w.r.t. the Euclidean norm with L2 = λmax(A A)/4M where λmax(A A) denotes the maximal eigenvalue of A A. Moreover, problem (3.1) is a special case of (1.1) with ξ being a random variable with
the uniform distribution on {1, . . . , M }.
For our experiments we use the data from LIBSVM library [20], see also Table 6 summarizing the information about the datasets we used. In all test we chose t = 10−8

Size M Dimension n

heart diabetes a9a

270

768

32561

13

8

123

Table 6 Summary of used datasets.

phishing 11055 68

w8a 49749 300

and the starting point x0 such that it diﬀers from x∗ only in the ﬁrst component and f (x0) − f (x∗) ∼ 10. We use standard solvers from scipy library to obtain a very good approximation of a solution x∗ and use it to measure the quality of the approximations

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 23

n = 100, big 2, = 0, m = 1

100

ARDFDS_E

ARDFDS_NE

10 1

RDFDS_E RDFDS_NE

RSGF

10 2

10 3

10 4

0
100 10 1 10 2

25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000 n = 100, big 2, = 0, m = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

10 3

10 4

10 5 0

250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 100, big 2, = 0, m = 100

100

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

10 2

RDFDS_NE RSGF

10 3

10 4

10 5

10 6 0.00 0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e07

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7
0.00

n = 100, big 2, = 0, m = 1000 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e08

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

n = 500, big 2, = 0, m = 1

100

ARDFDS_E

ARDFDS_NE

10 1

RDFDS_E RDFDS_NE

RSGF

10 2

10 3

10 4

0
100 10 1 10 2 10 3 10 4 10 5
0.0
100 10 1 10 2 10 3 10 4 10 5 10 6 0.0
100 10 1 10 2 10 3 10 4 10 5 10 6
0.0

2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000 n = 500, big 2, = 0, m = 10
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
0.2Numb0e.r4of ora0c.l6e calls0.8 11.e07 n = 500, big 2, = 0, m = 100
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
0.2Numb0e.r4of ora0c.l6e calls0.8 11.e08 n = 500, big 2, = 0, m = 1000
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
0.2Numb0e.r4of ora0c.l6e calls0.8 11.e09

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

n = 1000, big 2, = 0, m = 1

100

ARDFDS_E

ARDFDS_NE

10 1

RDFDS_E RDFDS_NE

RSGF

10 2

10 3

10 4

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 1000, big 2, = 0, m = 10

100

ARDFDS_E

ARDFDS_NE

10 1

RDFDS_E RDFDS_NE

10 2

RSGF

10 3

10 4

10 5
0.00
100 10 1 10 2

0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e07 n = 1000, big 2, = 0, m = 100
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

10 3

10 4

10 5

10 6 0.00
100 10 1 10 2 10 3 10 4 10 5 10 6
0.00

0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e08 n = 1000, big 2, = 0, m = 1000
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF
0.25 0N.5u0m0b.7e5r o1f.0o0ra1c.l2e5ca1l.l5s0 1.75 21.0e09

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Fig. 3. Numerical results for minimizing Nesterov’s function with noisy stochastic oracle having σ2 = σb2ig for diﬀerent sizes of mini-batch m and dimensions of the problem n.

by other algorithms. The results for the batch (and hence deterministic) methods with m = M and mini-batch stochastic methods are presented in Figures 6 and 7 respectively. In all cases methods with the 1-norm proximal setup show the best or comparable with the best results.
4. Conclusion. In this paper, we propose two new algorithms for stochastic smooth derivative-free convex optimization with two-point feedback and inexact function values oracle. Our ﬁrst algorithm is an accelerated one and the second one is a non-accelerated one. Notably, despite the traditional choice of 2-norm proximal setup for unconstrained optimization problems, our analysis has yielded better complexity bounds for the method with 1-norm proximal setup than the ones with 2-norm proximal setup. This is also conﬁrmed by numerical experiments.
REFERENCES
[1] A. Agarwal, O. Dekel, and L. Xiao, Optimal algorithms for online convex optimization with multi-point bandit feedback, in COLT 2010 - The 23rd Conference on Learning Theory, 2010.
[2] A. Agarwal, D. P. Foster, D. J. Hsu, S. M. Kakade, and A. Rakhlin, Stochastic convex optimization with bandit feedback, in Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, eds., Curran Associates, Inc., 2011, pp. 1035–1043.

f(xk) f(x * ) f(x0) f(x * )

24

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8
0
10 1 10 3 10 5 10 7

n = 100,

2 = 0, small

, m=1 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

n = 100,

2 = 0, medium

, m=1 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0 25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

n = 100, 2 = 0, large , m = 1

100

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 2

RSGF

10 3

10 4

10 5

10 6 0

25000 5N000u0m7b50e0r0 o10f0o00r0a1c25le000ca15l0l0s00 175000 200000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

10 1 10 3 10 5 10 7
0
10 1 10 3 10 5 10 7
0
100 10 1 10 2 10 3 10 4 10 5 10 6
0

n = 500,

2 = 0, small

, m=1 ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000 n = 500, 2 = 0, medium , m = 1
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000 n = 500, 2 = 0, large , m = 1
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

2000N00um4b0e00r0o0f o6r0a0c0le00ca8ll0s0000 1000000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

n = 1000, 2 = 0, small , m = 1 ARDFDS_E

ARDFDS_NE

10 2

RDFDS_E RDFDS_NE

RSGF

10 4

10 6

10 8

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 1000, 2 = 0, medium , m = 1

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

RDFDS_NE

10 3

RSGF

10 5

10 7

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

n = 1000, 2 = 0, large , m = 1

100

ARDFDS_E

10 1

ARDFDS_NE RDFDS_E

10 2

RDFDS_NE RSGF

10 3

10 4

10 5

10 6

0 250000 Number 500000 750000 o10f00o00r0a1c25l0e000ca15l0l0s000 1750000 2000000

Fig. 4. Numerical results for minimizing Nesterov’s function with noisy stochastic oracle having σ2 = 0 for diﬀerent ∆ and dimensions of the problem n.

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 0.0

n = 5000, small

2, small

, m=1
ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

0.2Numb0e.r4of ora0c.l6e calls0.8 11.e07

Fig. 5. Numerical results for minimizing Nesterov’s function with noisy stochastic oracle having σ2 = σs2mall and ∆ = ∆small for the dimension of the problem n = 5000.

[3] A. Akhavan, M. Pontil, and A. B. Tsybakov, Exploiting higher order smoothness in derivative-free optimization and continuous bandits, arXiv:2006.07862, (2020).
[4] Z. Allen-Zhu and L. Orecchia, Linear coupling: An ultimate uniﬁcation of gradient and mirror descent, arXiv:1407.1537, (2014).
[5] F. Bach and V. Perchet, Highly-smooth zero-th order online optimization, in 29th Annual Conference on Learning Theory, V. Feldman, A. Rakhlin, and O. Shamir, eds., vol. 49 of Proceedings of Machine Learning Research, Columbia University, New York, New York, USA, 23–26 Jun 2016, PMLR, pp. 257–283.
[6] P. L. Bartlett, V. Gabillon, and M. Valko, A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assumption, in Proceedings of the 30th International Conference on Algorithmic Learning Theory, A. Garivier and S. Kale, eds.,

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 25

100 10 2 10 4 10 6 10 8 10 10
0

heart, M = 270, n = 13, full batch ARDFDS_E ARDFDS_NE RDFDS_E RDFDS_NE RSGF

100

diabetes, M = 768, n = 8, full batch ARDFDS_E

ARDFDS_NE

10 2

RDFDS_E RDFDS_NE

10 4

RSGF

100

a9a, M = 32561, n = 123, full batch ARDFDS_E

ARDFDS_NE

RDFDS_E

RDFDS_NE

10 1

RSGF

10 6

10 8

10 2

10 10

1 Numb2er of or3acle cal4ls 5 1e8

0 1Number2of oracle3calls 4 1e8

0 1 Num2 ber o3f orac4le calls5

100

phishing, M = 11055, n = 68, full batch ARDFDS_E

100

w8a, M = 49749, n = 300, full batch ARDFDS_E

ARDFDS_NE

ARDFDS_NE

10 1

RDFDS_E RDFDS_NE

RDFDS_E RDFDS_NE

10 2

RSGF

10 1

RSGF

6 1e8

f(xk) f(x * ) f(x0) f(x * ) f(xk) f(x * ) f(x0) f(x * )

10 3

10 2

10 4

0.0 0.N5umber1o.0f oracle1c.5alls 2.0 1e9

0 1 Num2ber of3oracle4calls 5 16e9

Fig. 6. Numerical results for solving logistic regression problem (3.1) for diﬀerent datasets using batch methods with m = M .

100

a9a, M = 32561, n = 123, m = 100 ARDFDS_E

ARDFDS_NE

RDFDS_E

10 1

RDFDS_NE RSGF

f(xk) f(x * ) f(x0) f(x * )

10 2

10 3

0.0 0.2 Nu0m.4ber0o.f6ora0c.le8 cal1ls.0 1.21e7

100

w8a, M = 49749, n = 300, m = 100 ARDFDS_E

ARDFDS_NE

RDFDS_E

10 1

RDFDS_NE RSGF

10 2

10 3 0.0 0.5 Num1.b0er o1f .o5racle2.c0alls 2.5 13e.07

f(xk) f(x * ) f(x0) f(x * )

Fig. 7. Numerical results for solving logistic regression problem (3.1) for diﬀerent datasets using mini-batch stochastic methods.

vol. 98 of Proceedings of Machine Learning Research, Chicago, Illinois, 22–24 Mar 2019, PMLR, pp. 184–206. [7] A. Bayandina, A. Gasnikov, and A. Lagunovskaya, Gradient-free two-points optimal method for non smooth stochastic convex optimization problem with additional small noise, Automation and remote control, 79 (2018). arXiv:1701.03821. [8] A. Belloni, T. Liang, H. Narayanan, and A. Rakhlin, Escaping the local minima via simulated annealing: Optimization of approximately convex functions, in Proceedings of The 28th Conference on Learning Theory, P. Gru¨nwald, E. Hazan, and S. Kale, eds., vol. 40 of Proceedings of Machine Learning Research, Paris, France, 03–06 Jul 2015, PMLR, pp. 240–265. [9] A. Ben-Tal and A. Nemirovski, Lectures on Modern Convex Optimization (Lecture Notes), Personal web-page of A. Nemirovski, 2020, https://www2.isye.gatech.edu/∼nemirovs/ LMCOLN2020WithSol.pdf . [10] A. S. Berahas, R. H. Byrd, and J. Nocedal, Derivative-free optimization of noisy functions via quasi-Newton methods, SIAM Journal on Optimization, 29 (2019), pp. 965–993, https: //doi.org/10.1137/18M1177718. [11] A. S. Berahas, L. Cao, K. Choromanski, and K. Scheinberg, A theoretical and empirical comparison of gradient approximations in derivative-free optimization, arXiv:1905.01332, (2019). [12] A. Beznosikov, E. Gorbunov, and A. Gasnikov, Derivative-free method for composite opti-

26

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

mization with applications to decentralized distributed optimization, IFAC-PapersOnLine, (2020). Accepted, arXiv:1911.10645. [13] A. Beznosikov, A. Sadiev, and A. Gasnikov, Gradient-free methods for saddle-point problem, in Mathematical Optimization Theory and Operations Research 2020, A. Kononov and et al., eds., Cham, 2020, Springer International Publishing. accepted, arXiv:2005.05913. [14] L. Bogolubsky, P. Dvurechensky, A. Gasnikov, G. Gusev, Y. Nesterov, A. M. Raigorodskii, A. Tikhonov, and M. Zhukovskii, Learning supervised pagerank with gradientbased and gradient-free optimization methods, in Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, eds., Curran Associates, Inc., 2016, pp. 4914–4922. arXiv:1603.00717. [15] R. Bollapragada and S. M. Wild, Adaptive sampling quasi-Newton methods for derivativefree stochastic optimization, arXiv:1910.13516, (2019). [16] R. Brent, Algorithms for Minimization Without Derivatives, Dover Books on Mathematics, Dover Publications, 1973. [17] S. Bubeck and N. Cesa-Bianchi, Regret analysis of stochastic and nonstochastic multi-armed bandit problems, Foundations and Trends in Machine Learning, 5 (2012), pp. 1–122, https: //doi.org/10.1561/2200000024. [18] S. Bubeck, Y. T. Lee, and R. Eldan, Kernel-based methods for bandit convex optimization, in Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, New York, NY, USA, 2017, ACM, pp. 72–85. arXiv:1607.03084. [19] E. J. Candes, J. K. Romberg, and T. Tao, Stable signal recovery from incomplete and inaccurate measurements, Communications on Pure and Applied Mathematics, 59 (2006), pp. 1207–1223, https://doi.org/10.1002/cpa.20124. [20] C.-C. Chang and C.-J. Lin, Libsvm: A library for support vector machines, ACM transactions on intelligent systems and technology (TIST), 2 (2011), pp. 1–27. [21] Y. Chen, A. Orvieto, and A. Lucchi, An accelerated DFO algorithm for ﬁnite-sum convex functions, in Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research, PMLR, 2020. (accepted), arXiv:2007.03311. [22] K. Choromanski, A. Iscen, V. Sindhwani, J. Tan, and E. Coumans, Optimizing simulations with noise-tolerant structured exploration, in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 2970–2977. [23] K. Choromanski, M. Rowland, V. Sindhwani, R. Turner, and A. Weller, Structured evolution with compact architectures for scalable policy optimization, in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, eds., vol. 80 of Proceedings of Machine Learning Research, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018, PMLR, pp. 970–978. [24] A. R. Conn, K. Scheinberg, and L. N. Vicente, Introduction to Derivative-Free Optimization, Society for Industrial and Applied Mathematics, 2009, https://doi.org/10.1137/1. 9780898718768. [25] O. Dekel, R. Eldan, and T. Koren, Bandit smooth convex optimization: Improving the biasvariance tradeoﬀ, in Advances in Neural Information Processing Systems 28, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds., Curran Associates, Inc., 2015, pp. 2926–2934. [26] O. Devolder, Stochastic ﬁrst order methods in smooth convex optimization, CORE Discussion Paper 2011/70, (2011). [27] O. Devolder, F. Glineur, and Y. Nesterov, First-order methods of smooth convex optimization with inexact oracle, Mathematical Programming, 146 (2014), pp. 37–75. [28] J. Dippon, Accelerated randomized stochastic optimization, Ann. Statist., 31 (2003), pp. 1260– 1281, https://doi.org/10.1214/aos/1059655913. [29] D. L. Donoho, Compressed sensing, IEEE Transactions on Information Theory, 52 (2006), pp. 1289–1306. [30] J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono, Optimal rates for zeroorder convex optimization: The power of two function evaluations, IEEE Trans. Information Theory, 61 (2015), pp. 2788–2806. arXiv:1312.2139. [31] P. Dvurechensky and A. Gasnikov, Stochastic intermediate gradient method for convex problems with stochastic inexact oracle, Journal of Optimization Theory and Applications, 171 (2016), pp. 121–145, https://doi.org/10.1007/s10957-016-0999-6. [32] P. Dvurechensky, A. Gasnikov, and E. Gorbunov, An accelerated directional derivative method for smooth stochastic convex optimization, arXiv:1804.02394, (2018). [33] P. Dvurechensky, A. Gasnikov, and A. Tiurin, Randomized similar triangles method: A unifying framework for accelerated randomized optimization methods (coordinate descent, directional search, derivative-free method), arXiv:1707.08486, (2017).

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 27
[34] V. Fabian, Stochastic approximation of minima with improved asymptotic speed, Ann. Math. Statist., 38 (1967), pp. 191–200, https://doi.org/10.1214/aoms/1177699070.
[35] M. Fazel, R. Ge, S. Kakade, and M. Mesbahi, Global convergence of policy gradient methods for the linear quadratic regulator, in Proceedings of the 35th International Conference on Machine Learning, J. Dy and A. Krause, eds., vol. 80 of Proceedings of Machine Learning Research, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018, PMLR, pp. 1467–1476.
[36] A. D. Flaxman, A. T. Kalai, and H. B. McMahan, Online convex optimization in the bandit setting: Gradient descent without a gradient, in Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05, Philadelphia, PA, USA, 2005, Society for Industrial and Applied Mathematics, pp. 385–394.
[37] A. Gasnikov, P. Dvurechensky, and Y. Nesterov, Stochastic gradient methods with inexact oracle, Proceedings of Moscow Institute of Physics and Technology, 8 (2016), pp. 41–91. In Russian, ﬁrst appeared in arXiv:1411.4218.
[38] A. V. Gasnikov and P. E. Dvurechensky, Stochastic intermediate gradient method for convex optimization problems, Doklady Mathematics, 93 (2016), pp. 148–151.
[39] A. V. Gasnikov, E. A. Krymova, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko, Stochastic online optimization. single-point and multi-point non-linear multiarmed bandits. convex and strongly-convex case, Automation and Remote Control, 78 (2017), pp. 224–234, https://doi.org/10.1134/S0005117917020035. arXiv:1509.01679.
[40] A. V. Gasnikov, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko, Gradientfree proximal methods with inexact oracle for convex stochastic nonsmooth optimization problems on the simplex, Automation and Remote Control, 77 (2016), pp. 2018–2034, https://doi.org/10.1134/S0005117916110114. arXiv:1412.3890.
[41] S. Ghadimi and G. Lan, Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming, SIAM Journal on Optimization, 23 (2013), pp. 2341–2368. arXiv:1309.5549.
[42] S. Ghadimi, G. Lan, and H. Zhang, Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization, Mathematical Programming, 155 (2016), pp. 267– 305, https://doi.org/10.1007/s10107-014-0846-1. arXiv:1308.6594.
[43] E. Gorbunov, D. Dvinskikh, and A. Gasnikov, Optimal decentralized distributed algorithms for stochastic convex optimization, arXiv preprint arXiv:1911.07363, (2019).
[44] E. Hazan and K. Levy, Bandit convex optimization: Towards tight bounds, in Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds., Curran Associates, Inc., 2014, pp. 784–792.
[45] K. G. Jamieson, R. Nowak, and B. Recht, Query complexity of derivative-free optimization, in Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds., Curran Associates, Inc., 2012, pp. 2672–2680.
[46] D. Kamzolov, P. Dvurechensky, and A. V. Gasnikov, Universal intermediate gradient method for convex problems with inexact oracle, Optimization Methods and Software, 0 (2020), pp. 1–28, https://doi.org/10.1080/10556788.2019.1711079. arXiv:1712.06036.
[47] G. Lan, An optimal method for stochastic composite optimization, Mathematical Programming, 133 (2012), pp. 365–397. Firs appeared in June 2008.
[48] J. Larson, M. Menickelly, and S. M. Wild, Derivative-free optimization methods, Acta Numerica, 28 (2019), p. 287404, https://doi.org/10.1017/S0962492919000060.
[49] T. Liang, H. Narayanan, and A. Rakhlin, On zeroth-order stochastic convex optimization via random walks, arXiv:1402.2667, (2014).
[50] A. Locatelli and A. Carpentier, Adaptivity to smoothness in X-armed bandits, in Proceedings of the 31st Conference On Learning Theory, S. Bubeck, V. Perchet, and P. Rigollet, eds., vol. 75 of Proceedings of Machine Learning Research, PMLR, 06–09 Jul 2018, pp. 1463–1492.
[51] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation approach to stochastic programming, SIAM Journal on Optimization, 19 (2009), pp. 1574– 1609, https://doi.org/10.1137/070704277.
[52] A. Nemirovsky and D. Yudin, Problem Complexity and Method Eﬃciency in Optimization, J. Wiley & Sons, New York, 1983.
[53] Y. Nesterov, Introductory Lectures on Convex Optimization: a basic course, Kluwer Academic Publishers, Massachusetts, 2004.
[54] Y. Nesterov, Smooth minimization of non-smooth functions, Mathematical Programming, 103 (2005), pp. 127–152, https://doi.org/10.1007/s10107-004-0552-5.
[55] Y. Nesterov and V. Spokoiny, Random gradient-free minimization of convex functions, Found. Comput. Math., 17 (2017), pp. 527–566, https://doi.org/10.1007/ s10208-015-9296-2. First appeared in 2011 as CORE discussion paper 2011/16.
[56] B. T. Polyak and A. B. Tsybakov, Optimal order of accuracy of search algorithms in stochas-

28

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

tic optimization, Problemy Peredachi Informatsii, 26 (1990), pp. 45–53. [57] V. Y. Protasov, Algorithms for approximate calculation of the minimum of a convex function
from its values, Mathematical Notes, 59 (1996), pp. 69–74. [58] H. H. Rosenbrock, An automatic method for ﬁnding the greatest or least value of a function,
The Computer Journal, 3 (1960), pp. 175–184, https://doi.org/10.1093/comjnl/3.3.175. [59] A. Saha and A. Tewari, Improved regret guarantees for online smooth convex optimization
with bandit feedback, in Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, G. Gordon, D. Dunson, and M. Dudk, eds., vol. 15 of Proceedings of Machine Learning Research, Fort Lauderdale, FL, USA, 11–13 Apr 2011, PMLR, pp. 636–642. [60] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, Evolution strategies as a scalable alternative to reinforcement learning, arXiv:1703.03864, (2017). [61] O. Shamir, On the complexity of bandit and derivative-free stochastic convex optimization, in Proceedings of the 26th Annual Conference on Learning Theory, S. Shalev-Shwartz and I. Steinwart, eds., vol. 30 of Proceedings of Machine Learning Research, Princeton, NJ, USA, 12–14 Jun 2013, PMLR, pp. 3–24. [62] O. Shamir, An optimal algorithm for bandit and zero-order convex optimization with two-point feedback, Journal of Machine Learning Research, 18 (2017), pp. 52:1–52:11. First appeared in arXiv:1507.08752. [63] J. C. Spall, Introduction to Stochastic Search and Optimization, John Wiley & Sons, Inc., New York, NY, USA, 1 ed., 2003. [64] S. U. Stich, C. L. Muller, and B. Gartner, Optimization of convex functions with random pursuit, SIAM Journal on Optimization, 23 (2013), pp. 1284–1309.

Appendix A. Proof of Lemma 2.1. In this appendix we prove that, for e ∈ RS2 (1), q 2, and n 8,

(A.1) (A.2)

E[ e 2q] E[ s, e 2 e 2q]

min{q − 1, 16 ln n − 8}n 2q −1, 6 s 22 min{q − 1, 16 ln n − 8}n 2q −2.

Throughout this appendix, to simplify the notation, we denote by E the expectation w.r.t. random vector e ∈ RS2 (1).
We start by proving the following inequality, which could be not tight for large q:

(A.3)

E[ e 2q] (q − 1)n 2q −1, 2 q < ∞.

We have (A.4)



n

E[ e 2q] = E 

|ek |q

k=1

2
qx


n

E

|ek |q

k=1

2

q

=y

(nE[|e2 |q ])

2 q

,

2
where x is due to probabilistic version of Jensen’s inequality (function ϕ(x) = x q is concave, because q 2) and y is because expectation is linear and components of
the vector e are identically distributed. We also denote by ek the k-th component of e. In particular, e2 is the second component.
By the Poincare lemma, e has the same distribution as √ ξ , where ξ is
ξ12 +···+ξn2
the standard Gaussian random vector with zero mean and identity covariance matrix.
Then

E[|e2 |q ]

=E

|ξ2 |q q

(ξ

2 1

+...+ξ

2 n

)

2

n

= ··· |x2|q

x2k

Rn

k=1

q −2
·

1 n

· exp

(2π) 2

n
− 12 x2k
k=1

dx1 . . . dxn.

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 29

For the transition to the spherical coordinates

x1 = r cos ϕ sin θ1 . . . sin θn−2, x2 = r sin ϕ sin θ1 . . . sin θn−2, x3 = r cos θ1 sin θ2 . . . sin θn−2, x4 = r cos θ2 sin θ3 . . . sin θn−2,
... xn = r cos θn−2, r > 0, ϕ ∈ [0, 2π), θi ∈ [0, π], i = 1, ..., n − 2

the Jacobian satisﬁes

det ∂(r,ϕ∂,(θx11,θ,.2..,,.x..n,θ)n−2) = rn−1 sin θ1(sin θ2)2 . . . (sin θn−2)n−2.

In the new coordinates we have

E[|e2 |q ]

=

···

rn−1| sin ϕ|q| sin θ1|q+1| sin θ2|q+2 . . . | sin θn−2|q+n−2

r>0, ϕ∈[0,2π),

θi∈[0,π], i=1,...,n−2

r2
· exp (−n2 ) dr . . . dθn−2 =

1 n Ir · Iϕ · Iθ1 · Iθ2 · . . . · Iθn−2 ,

(2π) 2

(2π) 2

+∞
where Ir = rn−1exp − r22 dr,
0

2π

π

π

Iϕ = | sin ϕ|qdϕ = 2 | sin ϕ|qdϕ, Iθi = | sin θi|q+idθi for i = 1, ..., n − 2. Next we

0

0

0

calculate these integrals starting with Ir:

+∞ n−1

r2

r=√2t +∞ n −1

n −1 n

Ir = r exp − 2 dr =

(2t) 2 exp (−t)dt = 2 2 Γ( 2 ).

0

0

To compute the other integrals, we consider the following integral for α > 0:

π
| sin ϕ|αdϕ
0

π

π

2

2

α

= 2 | sin ϕ|αdϕ = 2 (sin2 ϕ) 2 dϕ

0

0

t=sin2 ϕ
=

1 α−1

1

t 2 (1 − t)− 2 dt = B( α+1 ,

2

0

12 ) =

Γ( α+2 1 )Γ( 21 ) Γ( α+2 2 )

√ Γ( α+1 )

=

π

2 α+2

.

Γ( 2 )

This gives

(A.5)

=

1 n

(2π) 2

E[|e2|q] = 1 n Ir · Iϕ · Iθ1 · Iθ2 · . . . · Iθn−2

(2π) 2

·

2

n 2

−1Γ(

n

)

·

√ 2π

Γ(

q+1 2

)

· √π Γ( q+2 2 )

· . . . · √π Γ( q+n2−1 )

=

√1

· Γ( n2 )Γ( q+2 1 ) .

2 Γ( q+2 2 ) Γ( q+2 3 )

q+n Γ( 2 )

π Γ( q+2n )

The next step is to show that, for all q 2,

(A.6)

√1 · Γ( n2 )Γ( q+2 1 )
π Γ( q+2n )

q
q−n1 2 .

First we show that (A.6) holds for q = 2 and arbitrary n:

√1 · Γ( n2 )Γ( 2+2 1 ) − 1 = √1 · Γ( n2 )· 21 Γ( 12 ) − 1 = 1 − 1 = 0 0.

π Γ( 2+2n )

n π n2 Γ( n2 )

n nn

30

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

Next, we consider the function f (q) = √1

· Γ( n2 )Γ( q+2 1 ) −

q−1

q
2 , where q

n

π

Γ( q+n )

n

2

2, and

digamma function ψ(x) = d(ln(dΓx(x))) with scalar argument x > 0. For the gamma

function it holds that Γ(x + 1) = xΓ(x), x > 0. Taking natural logarithm in both

sides and derivative w.r.t. x, we get d(ln(Γd(xx+1))) = d(ln(dΓx(x))) + x1 , meaning that ψ(x + 1) = ψ(x) + x1 . To prove that the digamma function monotonically increases

for x > 0, we show that

(A.7)

(Γ (x))2 < Γ(x)Γ (x).

Indeed,

(Γ (x))2

+∞

2

=

exp(−t) ln t · tx−1dt

0

x +∞

x−1 2

+∞

x−1

2

<

exp(− t )t 2 dt ·

exp(− t )t 2 ln t dt

2

2

0

0

+∞

+∞

= exp(−t)tx−1dt · exp(t)tx−1 ln2 tdt = Γ(x)Γ (x),

0

0

where x follows from the Cauchy-Schwartz inequality and we have strict inequality

x−1

x−1

t )t 2 and exp(− t )t 2 ln t are linearly independent. From

since functions exp(− 2

2

(A.7) it follows that d2(ldnxΓ2(x)) = ΓΓ((xx))

Γ (x) (Γ (x))2 (A.7) = Γ(x) − (Γ(x))2 > 0, i.e. digamma

function is increasing.

Now we show that fn(q) decreases on the interval [2, +∞). To that end, we

consider ln(fn(q))

ln(fn(q))
d(ln(fn (q ))) dq

= ln = 12 ψ

Γ( n ) √2 π
q+1 2

+ ln − 12 ψ

Γ q+2 1 − ln Γ q+2n − 2q (ln(q − 1) − ln n) , q+2n − 12 ln(q − 1) − 2(qq−1) + 21 ln n

and show that d(ln(fdnq (q))) < 0 for q 2. Let k = n2 (the largest integer which is no greater than n2 ). Then ψ q+2n > ψ k − 1 + q+2 1 and ln n ln(2k + 1), whence,

d(ln(fn (q ))) dq

< 12 ψ q+2 1 − ψ k − 1 + q+2 1 − 12 ln(q − 1) − 2(qq−1) + 12 ln(2k + 1)

k−1

= 21 ψ q+2 1 −

1 q+1

− ψ q+2 1

i=1 2 +k−i−1

x − 1 k−1 2 − 1 + 1 ln 2k+1

2

q−1+2k−2i q−1 2

q−1

i=1

− 2(qq−1) + 12 ln 2qk−+11

= − 12 q−2 1 + q+2 1 + q+2 3 + . . . + q+22k−3 + 12 ln 2qk−+11

y
<

−

1

ln

q+2k−1

+ 1 ln

2k+1

2

q−1

2

q−1

z − 1 ln 2k+1 + 1 ln 2k+1

2

q−1

2

q−1

= 0,

where x and z are since q

2,

y

follows

from

an

estimate

of

the

integral

of

1 x

by

the

integral of the constant functions gi(x) = q−11+2i , x ∈ [q − 1 + 2i, q − 1 + 2i + 2], i =

q+2k−1

0, ..., 2k − 1: q−2 1 + q+2 1 + q+2 3 + . . . + q+22k−3 >

x1 dx = ln q+q2−k1−1 .

q−1

Thus, we have shown that d(ln(fdnq (q))) < 0 for q 2 and an arbitrary natural

number n. Therefore, for any ﬁxed number n, the function fn(q) decreases as q

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 31

increases, which means that fn(q) fn(2) = 0, i.e., (A.6) holds. From this, (A.4), and (A.5) we obtain (A.3), i.e. that, for all 2 q < ∞,

(A.8)

E[||e||2q] (A.4) (nE[|e2|q]) 2q (A.5),(A.6) (q − 1)n 2q −1.

Next, we analyze separately the case of large q, in particular, q = ∞. We consider

the r.h.s. of (A.8) as a function of q and ﬁnd its minimum for q 2. Denote

hn(q) = ln(q − 1) + 2q − 1 ln n, which is the logarithm of the r.h.s. of (A.8). The

derivative

of

hn(q)

is

dhn (q ) dq

=

q

1 −1

−

2

ln q2

n

,

which

implies

that

the

ﬁrst-order

optimality

condition

is

1 q−1

−

2

ln n q2

=

0,

or

equivalently

q2 − 2q ln n + 2 ln n

=

0.

If

n

8, then the

function hn(q) attains its minimum on the set [2, +∞) at q0 = ln n 1 + 1 − ln2n

(for the case n 7 the optimal point is q0 = 2 and without loss of generality, we assume n 8). Therefore, for all q > q0, including q = ∞, we have

(A.9)

E[||e||2q ]

x
<

E[||e||2

]

(A.8)

(q0

− 1)n q20 −1

y

(2

ln

n

−

1)n

2 ln n

−1

q0

= (2 ln n − 1) exp(2) (16 ln n − 8) 1 (16 ln n − 8)n 2q −1,

n

n

where x is since e q < e q0 for q > q0, y follows from q0 2 ln n, q0 ln n. Combining estimates (A.8) and (A.9), we obtain (A.1).
It remains to prove (A.2). First, we estimate E[ e 4q]. By the probabilistic Jensen’s inequality, for q 2,

 E[||e||4q] = E 

2

n

2q

|ek |q





k=1

2

n

2q

E

|ek |q

k=1

x
E

n
n |ek|2q
k=1

(A.5),(A.6) 4
nq

2q−1 n

2

qy
=

n2 E[|e2 |2q ]

2 q

2

2q q

4

2 = (2q − 1)2n q −2,

where x is since

n

2

xk

n
n x2k for x1, x2, . . . , xn ∈ R and y follows from the

k=1

k=1

linearity of expectation and the components of the random vector e being identically

distributed. From this we obtain

(A.10)

2 −1
E[||e||4q] (2q − 1)n q .

Next, we consider the r.h.s. of (A.10) as a function of q and ﬁnd its minimum for

q 2. The logarithm of the r.h.s. of (A.10) is hn(q) = ln(2q − 1) + 2q − 1 ln n with

the derivative dhdnq(q) = 2q2−1 − 2 qln2n , which implies the ﬁrst-order optimality condition

2 2q−1

−

2 ln n q2

=

0,

or

equivalently

q2 − 2q ln n + ln n

=

0.

If

n

3, the point where the

function hn(q) attains its minimum on the set [2, +∞) is q0 = ln n 1 + 1 − ln1n

(for the case n 2 the optimal point is q0 = 2 and without loss of generality we

32

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

assume that n 3). Therefore for all q > q0, including q = ∞,

(A.11)

E[ e 4q]

x
<

E[

e

4

(A.10)

2 −1

]

(2q0 − 1)n q0

y

(4

ln

n

−

1)n

2 ln n

−1

q0

= (4 ln n − 1) expn(2) (32 ln n − 8) n1 (32 ln n − 8)n 2q −1,

where x is since e q < e q0 for q > q0, y follows from q0 2 ln n, q0 ln n. Combining the estimates (A.10) and (A.11), we get the inequality

(A.12)

2 −1
E[ e 4q] min{2q − 1, 32 ln n − 8}n q .

The next step is to estimate E[ s, e 4], where s ∈ Rn is some ﬁxed vector. Let

Sn(r) be the surface area of n-dimensional Euclidean sphere with radius r and dσ(e)

be unnormalized uniform measure on n-dimensional Euclidean sphere. Then Sn(r) =

Sn(1)rn−1, SSn− n(11()1) = nn√−π1 ΓΓ(( nn++2 21 )) . Let ϕ be the angle between s and e. Then
2

(A.13)

π

E[ s, e 4] = Sn1(1) s, e 4dσ(ϕ) = Sn1(1)

s

4 2

cos3

ϕSn−1

(sin

ϕ)dϕ

S

0

=

π
s 4 Sn−1(1) cos4 ϕ sinn−2 ϕdϕ =

s

4·

n−1 Γ( n+2 2 ) √

π
cos4 ϕ sinn−2 ϕdϕ.

2 Sn(1) 0

2 n π Γ( n+2 1 ) 0

Further, denoting the Beta function by B(·, ·),

π

π

π
cos4 ϕ sinn−2 ϕdϕ = 2

2

cos4

ϕ

sinn−2

ϕdϕ

t=sin2
=

ϕ

2 n−3

3

t 2 (1 − t) 2 dt

0

0

0

5 n−1

3 1 1 n−1

√ n−1

= B( n−1 , 5 ) = Γ( 2 )Γ( 2 ) = 2 · 2 Γ( 2 )Γ( 2 ) = 3 · πΓ( 2 ) .

22

Γ( n+2 4 )

n+2 2 ·Γ( n+2 2 )

n+2 2Γ( n+2 2 )

From this and (A.13), we obtain

(A.14)

E[ s, e 4]

n+2

√ n−1

= s 4 · n√−1 Γ( 2 ) · 3 · πΓ( 2 )

2 n π Γ( n+2 1 ) n+2 2Γ( n+2 2 )

= s 4 · 3(n−1) · Γ( n−2 1 ) = 3 s 42 x 3 s 42 .

2 2n(n+2) n−2 1 Γ( n−2 1 ) n(n+2) n2

To prove (A.2), it remains to use (A.12), (A.14) and the Cauchy-Schwartz inequality (E[XY ])2 E[X2] · E[Y 2]:

E[ s, e 2||e||2q]

E[ s, e 4] · E[ e 4]

√ 3

s

2

min{2q

−

1,

32

ln

n

−

8}n

2 q

−2.

q

2

Appendix B. Technical Results on Recurrent Sequences. Lemma B.1. Let a0, . . . , aN−1, b, R1, . . . , RN−1 be non-negative numbers and

(B.1)

√ Rl 2 ·

l−1

l−1

ak + b αk+1Rk

k=0

k=1

l = 1, . . . , N,

where αk+1 = 96nk2+ρ2nL2 for all k ∈ N. Then, for l = 1, . . . , N ,

(B.2)

l−1

l−1

ak + b αk+1Rk

k=0

k=1



2

l−1

√

2

 ak + 2b · 96n2lρnL2  .

k=0

AN ACCEL. METHOD FOR DER.-FREE SMOOTH STOCH. CONVEX OPTIMIZATION 33
Proof. For l = 1 the inequality is trivial. Next we assume that (B.2) holds for some l < N and prove this inequality for l + 1. From the induction assumption and (B.1) we obtain

(B.3)

√ Rl 2

l−1

√

2

ak +

2b ·

l 96n2 ρ

L

,

k=0

n2

whence

l

l

l−1

l−1

ak + b αk+1Rk = ak + b αk+1Rk + al + bαl+1Rl

k=0

k=1

k=0

k=1

2

x

l−1

√

2

√

ak +

2b ·

l 96n2 ρ

L

+ al + 2bαl+1

k=0

n2

l−1

√

2

ak +

2b ·

l 96n2 ρ

L

k=0

n2

l
= ak + 2
k=0

l−1

2√

42

√

ak

·

l · 2b 96n2ρ L

+

l ·2b (96n2ρ L

)2

+

2bαl+1

k=0

n2

n2

l−1

2√

ak

+

l · 2b 96n2ρ L

k=0

n2

l
= ak + 2
k=0
yl
ak + 2
k=0

l−1

√

ak · 2b

k=0

+ l2
96n2 ρn L2

αl+1 2

l

2√

42

(l+1) ·

2b +

(l+1) ·2b

2

2

(96n ρ L )

ak 96n2ρnL2

n2

k=0

+ 2b2

+ · l4
(96n2 ρn L2 )2

αl+1 l2 96n2 ρn L2

2

l

√

(l+1)2

=

ak + 2b · 96n2ρ L ,

k=0

n2

l−1
where x holds by the induction assumption and (B.3), y is since ak
k=0

+ = l2
96n2 ρn L2

αl+1 2

2l2 +l+2 192n2 ρn L2

(96n2lρ4nL2)2 + αl+1 · 96n2lρ2nL2

, (l+1)2
96n2 ρn L2 l4 +(l+2)l2
(96n2 ρn L2 )2

. (l+1)4
(96n2 ρn L2 )2

l
ak and
k=0

Lemma B.2. Let α, a0, . . . , aN−1, b, R1, . . . , RN−1 be non-negative numbers and

(B.4)

√ Rl 2 ·

l−1

l−1

ak + bα Rk

k=0

k=1

l = 1, . . . , N.

Then, for l = 1, . . . , N ,

(B.5)

l−1

l−1

ak + bα Rk

k=0

k=1



2

l−1

√



ak + 2bαl .

k=0

Proof. For l = 1 the inequality is trivial. Next we assume that (B.5) holds for some l < N and prove it for l + 1. By the induction assumption and (B.4) we obtain

(B.6)

√ Rl 2

l−1

√

ak + 2bαl ,

k=0

34

E. GORBUNOV, P. DVURECHENSKY, AND A. GASNIKOV

whence

l

l

l−1

l−1

ak + bα Rk = ak + bα Rk + al + bαRl

k=0

k=1

k=0

k=1

2

x

l−1

√

√

ak + 2bαl + al + 2bα

l−1

√

ak + 2bαl

k=0

k=0

l
= ak + 2
k=0

l−1

√

√

ak · 2bαl + 2b2α2l2 + 2bα

k=0

l−1

√

ak + 2bαl

k=0

l
= ak + 2
k=0

l−1

√

ak · 2bα l + 21 + 2b2α2 l2 + l

k=0

yl
ak + 2
k=0

l

√

ak · 2bα(l + 1) + 2(bα(l + 1))2 =

k=0

2

l

√

ak + 2bα(l + 1) ,

k=0

l−1
where x is by the induction assumption and (B.6), y is since ak
k=0

l
ak .
k=0

