PREPRINT VERSION

1

arXiv:1804.03986v1 [math.OC] 9 Apr 2018

Dynamic Sensor Subset Selection for Centralized Tracking of a Stochastic Process

Arpan Chattopadhyay & Urbashi Mitra

Abstract—Motivated by the Internet-of-things and sensor networks for cyberphysical systems, the problem of dynamic sensor activation for the centralized tracking of an i.i.d. time-varying process is examined. The tradeoff is between energy efﬁciency, which decreases with the number of active sensors, and ﬁdelity, which increases with the number of active sensors. The problem of minimizing the time-averaged mean-squared error over inﬁnite horizon is examined under the constraint of the mean number of active sensors. The proposed methods artfully combine Gibbs sampling and stochastic approximation for learning, in order to create a high performance, energy efﬁcient tracking mechanisms with active sensor selection. Centralized tracking of i.i.d. process with known distribution as well as an unknown parametric distribution are considered. For an i.i.d. process with known distribution, convergence to the global optimal solution with high probability is proved. The main challenge of the i.i.d. case is that the process has a distribution parameterized by a known or unknown parameter which must be learned; one key theoretical result proves that the proposed algorithm for tracking an i.i.d. process with unknown parametric distribution converges to local optima. Numerical results show the efﬁcacy of the proposed algorithms and also suggest that global optimality is in fact achieved in some cases.
I. INTRODUCTION
Controlling and monitoring physical processes via sensed data are integral parts of internet-of-things (IOT) and cyberphysical systems, and also have applications in industrial process monitoring and control, localization, tracking of mobile objects, environmental monitoring, system identiﬁcation and disaster management. In such applications, sensors are simultaneously resource constrained (power and/or bandwdith) and tasked to achieve high performance sensing, control, communication, and tracking. Wireless sensor networks must further contend with interference and fading. One strategy for balancing resource use with performance is to activate a subset of the total possible number of sensors to limit both computation as well as bandwidth use.
Herein, we address the fundamental problem of optimal dynamic sensor subset selection for tracking a time-varying stochastic process. We ﬁrst examine the centralized tracking of an i.i.d. process with a known distribution, which is a precursor to the centralized tracking of an i.i.d. process with an unknown, parametric distribution. For the known prior distribution case, optimality of the proposed algorithm is proven. For the proposed algorithm for centralized tracking of an i.i.d. process with parameter learning, results on almost sure convergence to local optima are proven. The algorithms
The authors are with the Department of Electrical Engineering, University of Southern California. Email: {achattop,ubli}@usc.edu
This work was funded by the following grants: ONR N00014-15-1-2550, NSF CNS-1213128, NSF CCF-1718560, NSF CCF-1410009, NSF CPS1446901, AFOSR FA9550-12-1-0215
Some parts of this paper have previously been accepted in conferences [1], [2].

are numerically validated to demonstrate their efﬁcacy against competetive algorithms and natural heuristics.
Optimal sensor subset selection problems can be broadly classiﬁed into two categories: (i) optimal sensor subset selection for static data with known prior distribution, but unknown realization, and (ii) dynamic sensor subset selection to track a time-varying stochastic process. There have been several recent attempts to solve the ﬁrst problem; see [3] for sensor network applications and [4] for mobile crowdsensing applications. This problem poses two major challenges: (i) computing the estimation error given the observations from a subset of sensors, and (ii) ﬁnding the optimal sensor subset from exponentially many number of subsets. In [3], a tractable lower bound on performance addressed the ﬁrst challenge and a greedy algorithm addressed the second. In our current paper, we use Gibbs sampling to solve the problem of tracking an i.i.d. time varying process via active sensing. While estimation of static data and tracking i.i.d. time-varying process are the same problems mathematically, herein we provide a provably optimal alternative approach to that of [3]; in case the distribution is unknown and learnt over time, Gibbs sampling also yields a low-complexity sensor subset selection scheme, thereby eliminating the need for running a greedy algorithm whose complexity scales with the number of sensors.
There have been several related works on the problem of dynamic sensor subset selection to track a time-varying stochastic process; see [5]–[10]. In [7], the problem of selecting a single sensor node to track a Markov chain is addressed; [7] assumes the availability of a centralized controller which has knowledge of the latest observation made by the selected sensor. This problem was extended to sensor subset selection (by a centralized controller) in [5], and energy efﬁciency issues were incorporated in [6]. These two papers considered sequential decision making over a ﬁnite time horizon. The existence of an optimal policy for the centralized optimal dynamic sensor subset selection problem for inﬁnite time horizon is proved in [8]; the structure of the optimal policy for the special case of linear quadratic Gaussian (LQG) problem is also provided. The paper [9] addresses the problem of selecting a single sensor at each time, with the assumption that the observation of the sensor is shared among all sensors. Thompson sampling, in [4], solved the problem of centralized tracking of a linear Gaussian process (with unknown noise statistics) via active sensing.
Herein, we consider the problem of dynamically choosing the optimal sensor subset for centralized tracking of an i.i.d. time-varying process with an unknown parametric distribution, using tools from Gibbs sampling (see [11]) and stochastic approximation (see [12]). To the best of our knowledge, this problem has not been solved in prior work. Our work accommodates energy constraint in the network by imposing

PREPRINT VERSION

2

Active sensors
Inactive sensors

!" ($) !( ($) !) ($) !* ($)

&'($) Sink

Figure 1. Centralized estimation. Sensors send their observations {Z1(t), · · · , ZN (t)} to the sink, and the fusion center estimates Xˆ (t).
a constraint on the number of active sensors. In this paper, we make the following contributions:
1) In Section III, a centralized tracking and learning algorithm for an i.i.d. process with a known distribution is developed, in order to minimize time-average estimation error subject to a constraint on the mean number of active sensors. In particular, Gibbs sampling minimizes computational complexity for a relaxed version of the problem, along with stochastic approximation that is employed to iteratively update a Lagrange multiplier to achieve the mean number of activated sensors constraint. Desired almost sure convergence to the optimal solution is proved. A challenge we overcome in the analysis, is handling updates at different time scales that given rise to several technical issues that need to be addressed.
2) In Section IV, a centralized tracking and learning algorithm for an i.i.d. process with an unknown, but parametric distribution is developed. In addition to Gibbs sampling and stochastic approximation as used in Section III, simultaneous perturbation stochastic approximation (SPSA) is employed for parameter estimation obviating the need for expectation-maximization.
3) Numerical results show that the proposed algorithms outperform simple greedy algorithms. Numerical results also demonstrate a tradeoff between performance and computational cost for learning. Furthermore, the numerical results show that sometimes global optima are achieved in tracking i.i.d. process with unknown parametric distribution.
The rest of the paper is organized as follows. The system model is described in Section II. Tracking of an i.i.d. process with known distribution is described in Section III. Section IV deals with the tracking of an i.i.d. process with unknown, parametric distribution. Numerical results are presented in Section V, followed by the conclusion in Section VI. All mathematical proofs are provided in the appendices.
II. SYSTEM MODEL
We consider a connected single-hop wireless sensor network (see Figure 1) where sensor nodes communicate directly with the fusion center; the fusion center is responsible for all

control or estimation operations in the network. The sensor nodes are denoted by the index set N = {1, 2, · · · , N }. While our methods can be adapted to consider multihopped communication via relays, we do not treat this case herein.
The physical process under measurement is denoted by {X(t)}t≥0, where t is a discrete time index and X(t) ∈ Rq×1. {X(t)}t≥0 is an i.i.d. process. The distribution of X(t) may be known, or X(t) might have a parametric distribution pθ0 (·), where the unknown parameter vector θ0 needs to be be learnt via the measurements. The parameter vector θ0 lies inside the interior of a compact subset Θ ⊂ Rd.
At time t, if a sensor k is used to sense the process, then the observation at sensor k is provided by a r-dimensional column vector
zk(t) = f0(X(t)) + vk(t),
where vk(t) is a Gaussian random vector (observation noise) which is independent across k and i.i.d. across t.
Let B(t) ∈ {0, 1}1×N := B be a vector where the kth entry Bk(t) = 1 if the kth sensor is activated at time, t and Bk(t) = 0, if it is inactive. The decision to activate any sensor for sensing and communicating the observation is taken by the fusion center. We denote by B =. {0, 1}N the set of all possible conﬁgurations (i.e., sensor activation vectors) in the network, and by B a generic conﬁguration. Clearly, B(t) ∈ B. Each conﬁguration represents a unique set of activated sensors. The notation B−j ∈ {0, 1}N−1 is used to represent the conﬁguration B with its j-th entry removed. We denote by (B−j, 0) another conﬁguration which agrees with B at all coordinates other than the j-th coordinate, where (B−j, 0) has a value 0 as the j-th entry (i.e., the j-th sensor is not activated); a similar deﬁnition holds for (B−j, 1).
The observation made by .sensor k at time t is Zk(t) = Bk(t)zk(t). We deﬁne Z(t) = {Zk(t) : 1 ≤ k ≤ N }}.

A. Problem framework
Our sensor network seeks to achieve two goals: develop a sensing strategy, B(t) and compute an estimate of X(t) at the fusion center which is denoted by Xˆ (t) (see Figure 1). For the case of unknown distribution paramter, the fusion center also computes an estimate of those parameters, θˆ(t). To compute these three functions we deﬁne two distinct information structures:
Hc(t) = {B(τ ), Xˆ (τ − 1), Z(τ ), θˆ(τ − 1), ∀ τ ≤ t}(1) Hp(t) = {B(τ ), Xˆ (τ ), Z(τ ), θˆ(τ ), ∀ τ ≤ t − 1} (2)

The corresponding functions are then given as follows:

B(t) = µ1(Hp(t))

(3)

Xˆ (t) = µ2(Hc(t))

(4)

(5)

We observe the sequential nature in applying the functions µi, that is, the activation vector B(t) determines the observations
Z(t) which in turn are used to compute the tracked process, Xˆ (t). For unknown θ0, we compute θˆ(t) = µ3(Hc(t)). For an i.i.d. time varying process, Hc(t) is sufﬁcient to estimate

PREPRINT VERSION

3

Xˆ (t). However, in order to optimally decide B(t) when θ0

is unknown, the fusion center needs knowledge about the

performance of all past conﬁgurations. Hence, Hp(t) and

Hc(t) have two different information structures. However, we

will see that, our Gibbs sampling algorithm determines B(t)

by using only a sufﬁcient statistic (which captures the past

history) calculated iteratively in each slot.

We deﬁne a policy µ = (µ1, µ2, µ3) as a tuple of mappings, where µ1(Hp(t)) = B(t), µ2(Hc(t)) = Xˆ (t) and µ3(Hc(t)) = θˆ(t) as discussed earlier. The policy µ may be randomized, where the quantities B(t), θˆ(t) and Xˆ (t) are

chosen according to random distributions deﬁned by µ; that

is, µ1(Hp(t)), µ2(Hc(t)) and µ3(Hc(t)) are three probability distributions for B(t), Xˆ (t) and θˆ(t), respectively. In the

sequel, we will investigate Gibbs sampling strategies for sensor

selection; therein, B(t) will be random.

Our goal is to solve the following centralized problem

of minimizing the time-average mean squared error (MSE)

subject to a constraint on the mean number of active sensors

per unit time:

1t

µ∗ = arg min lim sup

Eµ||X(τ ) − Xˆ (τ )||2

µ t→∞ t τ =1

s.t. lim sup 1t t Eµ||B(τ )||1 ≤ N¯ (P1)
t→∞ τ =1

where Eµ is the expectation under policy µ, and the expectation is taken over the randomness in the process as well as any possible randomness in the policy µ.

III. IID PROCES WITH KNOWN DISTRIBUTION
In this section, we provide an algorithm for solving the centralized problem (P1) when {X(t)}t≥0 is i.i.d. with known distribution. This algorithm is developed as a precursor to the algorithms for tracking an i.i.d. process having a parametric distribution with an unknown parameter θ0.

optimizer B∗ ∈ B (not necessarily unique) for the problem (P2); if the conﬁguration B∗ is chosen at each t, the minimum cost of (P2) can be achieved (follows from the law of large numbers, since the cost incurred over time for a given µ2 constitutes an i.i.d. sequence whose mean is the optimal cost for (P2)). Hence, (P2) can be written as:
arg min Eµ ,B||X(τ ) − Xˆ (τ )||2 +λ||B||1 (P3)
B∈B 2 :=f (B)

:=h(B)

Here f (B) (for any B ∈ B) is the MSE under estimation policy µ2 when the sensor activation vector is B; f (B) becomes equal to the MMSE under conﬁguration B if µ2(Hc(τ )) = E(X(τ ))|Hc(τ )). Our results in this paper will hold for MMSE or any other general estimator.
The following result tells us how to choose the optimal λ∗
to solve (P1).

Theorem 1. Consider problem (P1) and its relaxed version

(P3). If there exists a Lagrange multiplier λ∗ ≥ 0 and a B∗ ∈

B, such that an optimal conﬁguration for (P3) under λ = λ∗

is B∗, and the constraint in (P1) is satisﬁed with equality

under the pair (B∗, λ∗), then B∗ is an optimal conﬁguration

for (P1).

In general, if there exist multiple conﬁgurations

B1∗, B2∗, · · · , Bm∗ , a multiplier λ∗ ≥ 0, and a probability

mass function (p1, p2, · · · , pm) such that (i) each of

B

∗ 1

,

B2∗

,

·

·

·

, Bm∗

is

optimal

for

problem

(P3)

under

λ∗,

and

(ii)

m i=1

pi||Bi∗||1

=

N¯ ,

then

an

optimal

solution

for

(P1)

is

to select one Bi∗ independently according to the probability

mass function noted above.

Proof. See Appendix A.

Theorem 1 allows us to obtain a solution for (P1) from the solution of (P3) by choosing an appropriate λ∗; the existence of λ∗ will be discussed in Section III-E.

A. Relaxing the constrained problem
In order to solve the constrained problem (P1), we ﬁrst relax (P1) by using a Lagrance multiplier λ, and obtain the following unconstrained problem:

1t

µ∗ = arg min lim sup

Eµ

µ t→∞ t τ =1

||X(τ )−Xˆ (τ )||2+λ||B(τ )||1 (P2)

The multiplier λ ≥ 0 can be viewed as the cost incurred for

activating a sensor at any time instant. We will see later that

solution of the unconstrained problem (P2) will be used to

solve the constrained problem (P1).

We observe that, at time τ , for the chosen sensor subset

B(τ ) and the corresponding collected observations Z(τ ), the

minimum mean squared error (MMSE) estimate of X(τ ) is given by Xˆ (τ ) = E(X(τ )|Hc(τ )); hence, we ﬁx the

estimation policy µ2 and solve (P2) only over the sensor subset

selection policy µ1 (since the distribution of X(τ ) is known,

µ3 has no relevance here). Since (P2) is an unconstrained

problem and X(τ ) is i.i.d. across τ , there exists at least one

B. Solving (P2) and (P3) for known distribution

Finding the optimal solution of (P2) and (P3) requires us to search over 2N possible conﬁgurations and to compute
the MSE for each conﬁguration. Hence, we propose Gibbs sampling based algorithms to avoid this O(2N ) computation.
Let us deﬁne a probability distribution πβ(·) over B as (with a parameter β > 0):

.

e−βh(B)

. e−βh(B)

πβ (B) = B ∈B e−βh(B ) = Zβ . (6)

Following the terminology in statistical physics, we call β the inverse temperature, and Zβ the partition function. The quantity h(B) is viewed as the energy under conﬁguration B. It is straightforward to see that limβ↑∞ B∈arg minA∈B h(A) πβ (B) = 1. Hence, if a conﬁguration B(t) is selected at each time t with probability distribution πβ(·) for sufﬁciently large β > 0, then B(t) will belong to the set of minimizers of (P3) with high probability; if ∆1 := minB ∈B:h(B )=minB∈B h(B) h(B ) − minB∈B h(B), then, for a unique minimizer B∗, πβ(B∗) ≥ 1+(2N −11)e−β∆1

PREPRINT VERSION

4

(which goes to 1 as β → ∞). However, computing Zβ requires 2N addition operations; hence, we use a sequential

subset selection algorithm based on Gibbs sampling (see [11,

Chapter 7]) in order to avoid explicit computation of Zβ while

picking B(t) ∼ πβ(·).

Below we introduce the Basic Gibbs (BG) algorithm.

BG algorithm: Start with an initial conﬁguration B(0). At

time t, pick a random sensor jt uniformly from the set of

all sensors. Choose Bjt (t) = 1 with probability p(t) :=

and choose B (t) = 0 e−βh(B−jt (t−1),1)

e−βh(B−jt (t−1),1)+e−βh(B−jt (t−1),0)

jt

with probability (1 − p(t)). For k = jt, choose Bk(t) =

Bk(t − 1). Activate the sensors according to B(t).

Note that, in this algorithm, it is sufﬁcient to maintain

Hp(t) = B(t − 1) due to the i.i.d. nature of X(t).

Theorem 2. Under the BG algorithm, {B(t)}t≥0 is a reversible, ergodic, time-homogeneous Markov chain with stationary distribution πβ(·).

Proof. Follows from the theory in [11, Chapter 7]). The proof can be done by verifying the detailed balance equation for the Markov chain B(t).

Theorem 2 tells us that if the fusion center runs BG and

reaches the steady state distribution of the Markov chain
{B(t)}t≥0, then the conﬁguration chosen by the algorithm will have distribution πβ(·). Also, by the ergodicity of {B(t)}t≥0,
the time-average occurence rates of all conﬁgurations match
the distribution πβ(·) almost surely. For very large β > 0, if one runs {B(t)}t≥0 for a
sufﬁciently long, ﬁnite time T0, then the terminal state B(T0) will belong to arg minB∈B h(B) with high probability. We have already shown that, for a unique minimizer B∗, πβ(B∗) ≥ 1+(2N −11)e−β∆1 (which goes to 1 as β → ∞). In Section III-D, we will provide an upper bound on dV (π(t), πβ)
which is the total variation distance between the distribution
of B(t) under BG algorithm and the distribution πβ; the upper bound is dV (π(t), πβ ) ≤ dV (π(0), πβ )(1 − e−NβNN∆ ) Nt , where ∆ := maxB∈B,A∈B |h(B) − h(A)|. Hence, for a large but ﬁnite time T0, we can derive the following bound:

π(T0)(B∗) ≥ πβ (B∗) − 2dV (π(T0), πβ ) 1
≥ 1 + (2N − 1)e−β∆1

T0

(0)

e−βN ∆ N

−2dV (π , πβ) 1 − N N

C. The exact solution
BG is operated with a ﬁxed β, but the optimal solution of the unconstrained problem (P2) can only be obtained with β ↑ ∞; this is done by updating β at a slower time-scale than the iterates of BG algorithm. The quantity β(t) is increased logarithmically with time in order to maintain the necessary timescale difference between Gibbs sampling and β(t) update. We call this new algorithm Adaptive Basic Gibbs or ABG.
ABG algorithm: This algorithm is same as BG except that at time t, we use β(t) := β(0) log(1 + t) to compute the update probabilities, where β(0) > 0, β(0)N ∆ < 1, and ∆ := maxB∈B,A∈B |h(B) − h(A)|.

Theorem 3. Under the ABG algorithm, the Markov chain

{B(t)}t≥0 is strongly ergodic, and the limiting probability

distribution satisﬁes limt→∞ A) = 1.

A∈arg minC∈B h(C) P(B(t) =

Proof. See Appendix C. We have used the notion of weak and strong ergodicity of time-inhomogeneous Markov chains from [11, Chapter 6, Section 8]), which is provided in Appendix B. The proof is similar to the proof of [13, Theorem 2], but is given here for completeness.

Theorem 3 shows that we can solve (P2) exactly if we run ABG for inﬁnite time, in contrast to BG which provides an approximate solution.
For i.i.d. time varying {X(t)}t≥0 with known joint distribution, we can either: (i) ﬁnd the optimal conﬁguration B∗ using ABG off-line and use B∗ for ever, or (ii) run ABG at the same timescale as t, and use the current conﬁguration B(t) for sensor activation; both schemes will minimize the cost in (P2). By the strong ergodicity of {B(t)}t≥0, optimal cost will be achieved for (P2) under ABG.

D. Convergence rate of BG and ABG
Let π(t) denote the probability distribution of B(t) under BG. Let us consider the transition probability matrix P of the Markov chain {Y (l)}l≥0 with Y (l) = B(lN ), under BG. Let us recall the deﬁnition of the Dobrushin’s ergodic coefﬁcient δ(P ) from [11, Chapter 6, Section 7] for the matrix P ; using a method similar to that of the proof of Theorem 3, we can show that δ(P ) ≤ (1 − e−NβNN∆ ). Then, by [11, Chapter 6, Theorem 7.2], we can say that under BG, we have
l
dV (π(lN), πβ ) ≤ dV (π(0), πβ ) 1 − e−NβNN∆ . We can prove
similar bounds for any t = lN + k, where 0 ≤ k ≤ N − 1.
Clearly, under the BG algorithm, the convergence rate decreases as β increases. Hence, there is a trade-off between convergence rate and accuracy of the solution in this case. Also, the rate of convergence decreases with N .
Such a closed-form convergence rate bound for ABG is not easily available. For the ABG algorithm, the convergence rate is expected to decrease with time, since the value of β(t) increases to ∞.

E. Gibbs sampling and stochastic approximation for (P1)
In Section III-B and Section III-C, we presented Gibbs sampling based algorithms for the unconstrained problem (P2). Now we provide an algorithm that updates λ with time in order to meet the constraint in (P1) with equality, and thereby solves (P1) (see Theorem 1) by solving the unconstrained problem.
Let us denote the optimal conﬁguration for (P3) under a given estimation strategy µ2 (which could be the MMSE estimator) by B∗.
Lemma 1. For the unconstrained problem (P3), the optimal mean number of active sensors, Eµ2 ||B∗||1, decreases with λ. Similarly, the optimal error, Eµ2 f (B∗), increases with λ.
Proof. See Appendix D.

PREPRINT VERSION

5

Lemma 1 provides intuition as to how to update λ in BG or in ABG in order to solve (P1). We seek to provide one algorithm which updates λ(t) at each time instant, based on the number of active sensors in the previous time instant. In order to maintain the necessary timescale difference between the {B(t)}t≥0 process and the λ(t) update process, we use stochastic approximation ( [12]) based update rules for λ(t).
The optimal mean number of active sensors, Eµ2 ||B∗||1, for the unconstrained problem (P3) is a decreasing staircase function of λ, where each point of discontinuity is associated with a change in the optimizer B∗(λ). Hence, the optimal solution of the constrained problem (P1) requires us to randomize between two values of λ (and therefore between two conﬁgurations) in case the optimal λ∗ as in Theorem 1 belongs to the set of such discontinuities. However, this randomization will require us to update a randomization probability at another timescale; having stochastic approximations running in multiple timescales leads to slow convergence. Hence, instead of using a varying β(t), we use a ﬁxed, but large β and update λ(t) in an iterative fashion using stochastic approximation; BG itself is a randomized subset selection algorithm and hence no further randomization is required to meet the constraint in (P1) with equality. This observation is formalized in the following lemma. This lemma will be crucial in the convergence proof of the Gibbs Learn (GL) algorithm proposed later.

Lemma 2. Under BG, Eµ2 ||B(t)||1 is a Lipschitz continuous and decreasing function of λ.

Proof. See Appendix E.

We make the following feasibility assumption for (P1), under BG with the chosen β > 0.

Assumption 1. There exists λ∗ ≥ 0 such that the constraint in (P1) under λ∗ and BG is met with equality.

Note that, by Lemma 2, Eµ2 ||B||1 continuously decreases in λ. Hence, if N¯ is feasible, then such a λ∗ must exist by the

intermediate value theorem. Our proposed Gibbs Learn (GL)

algorithm updates λ(t) iteratively in order to solve (P1). Let

us deﬁne: hλ(t)(B) := f (B) + λ(t)||B||1 (recall the notation

from (P3)). Now, we formally describe the GL algorithm to

solve the constrained problem (P1).

GL algorithm:

1) Choose any initial B(0) ∈ {0, 1}N and λ(0) ≥ 0.

2) At each discrete time instant t = 0, 1, 2, · · · , pick a

random sensor jt ∈ N independently and uniformly.

For sensor jt, choose Bjt (t) = 1 with probabil-

ity p :=

e−βhλ(t)(B−jt (t−1),1) e−βhλ(t)(B−jt (t−1),1)+e−βhλ(t)(B−jt (t−1),0)

and

choose Bjt (t) = 0 with probability (1 − p). For

k = jt, we choose Bk(t) = Bk(t − 1).

3) Update λ(t) at each node as follows:

λ(t + 1) = [λ(t) + a(t)(||B(t − 1)||1 − N¯ )]cb
The stepsize {a(t)}t≥1 constitutes a positive sequence such that ∞ t=1 a(t) = ∞ and ∞ t=1 a2(t) < ∞. The nonnegative projection boundaries b and c are such that λ∗ ∈ (b, c) where λ∗ is deﬁned in Assumption 1.

We next make two observations on the GL algorithm: • If ||B(t−1)||1 is more than N¯ , then λ(t) is increased with
the hope that this will reduce the number of active sensors in subsequent time slots, as suggested by Lemma 2. • The B(t) and λ(t) processes run on two different timescales; B(t) runs in the faster timescale whereas λ(t) runs in a slower timescale. This can be understood from the fact that the stepsize in the λ(t) update process decreases with time t. Here the faster timescale iterate will view the slower timescale iterate as quasi-static, while the slower timescale iterate will view the faster timescale as almost equilibriated. This is reminiscent of two-timescale stochastic approximation (see [12, Chapter 6]). Let πβ|λ∗ (·) denote πβ(·) under λ = λ∗.
Theorem 4. Under GL and Assumption 1, we have λ(t) → λ∗ almost surely, and the limiting distribution of {B(t)}t≥0 is πβ|λ∗ (·).
Proof. See Appendix F.
Theorem 4 says that GL produces a conﬁguration from the distribution πβ|λ∗ (·) under steady state. Hence, GL meets the sensor activation constraint in (P1) with equality and offers a near-optimal time-average mean squared error for the constrained problem; the gap from the optimal MSE can be made arbitrarily small by choosing β large enough.

F. A hard constraint on the number of activated sensors

Let us consider the following modiﬁed constrained problem (recall notation from (P3)):

min f (B) s.t. ||B||1 ≤ N¯

(P4)

B∈B

It is easy to see that (P4) can be easily solved using similar Gibbs sampling algorithms as in Section III, where the Gibbs sampling algorithm runs only on the set of conﬁgurations which activate N¯ number of sensors. Thus, as a by-product, we have also proposed a methodology for the problem in [3], though our framework is more general than [3].
Note that, the constraint in (P1) is weaker than (P4). Also, if we choose β very large, then the number of sensors activated by GL will have very small variance. This allows us to meet the constraint in (P4) with high probability.

IV. IID PROCESS WITH PARAMETRIC DISTRIBUTION: UNKNOWN θ0
In Section III, we described algorithms for centralized tracking of an i.i.d. process {X(t)}t≥0 with known distributions. In this section, we will deal with the centralized tracking of an i.i.d. process {X(t)}t≥0 where X(t) ∼ pθ0 (·) with an unknown parameter θ0 ∈ Θ; in this case, θ0 has to be learnt over time through observations.
The algorithm described in this section will be an adaptation of the GL algorithm discussed in Section III-E. However, when θ0 is unknown, we have to update its estimate θ(t) over time using the sensor observations. In order to solve the constrained problem (P1), we still need to update λ(t) over time so as to attain the optimal λ∗ of Theorem 1

PREPRINT VERSION

6

iteratively. However, f (B) (MSE under conﬁguration B) in (P3) is unknown since θ0 is unknown, and its estimate f (t)(B) has to be learnt over time using the sensor observations; as a result h(t)(B) := f (t)(B) + λ(t)||B||1 is also iteratively updated for all B ∈ B. Hence, we combine the Gibbs sampling algorithm with update schemes for f (t)(B), λ(t) and θ(t) using multi-timescale stochastic approximation (see [12]). The Gibbs sampling runs in the fastest timescale and the θ(t) update runs in the slowest timescale.
Since the algorithm has several steps (such as Gibbs sampling, θ(t) update, f (t)(B) update and λ(t) update) and each of these steps needs detailed explanation, we ﬁrst describe in details some key features and steps of the algorithm in Section IV-A, Section IV-B, Section IV-C, Section IV-D and Section IV-E, and then provide a brief summary of the algorithm in Section IV-F.
The proposed Gibbs Parameter Learning (GPL) algorithm also requires a sufﬁciently large positive number A0 and a large integer T as input. The need of these parameters will be clear as we proceed through the algorithm description.
Let J (t) denote the indicator that time t is an integer multiple of T . Deﬁne ν(t) := tτ=0 J (τ ) to be the number of time slots till time t when all sensors are activated.

A. Step size sequences

For the stochastic approximation updates of f (t)(B), λ(t)

and θ(t), the algorithm uses four nonnegative sequences

{a(t)}t≥0, {b(t)}t≥0, {c(t)}t≥0, {d(t)}t≥0. Let {s(t)}t≥0 be

a generic sequence where s ∈ {a, b, c}. Then we require

the following two conditions: (i) ∞ t=0 s2(t) < ∞,.

∞ t=0 s(t) = ∞, and (ii)

In addition, we have the following speciﬁc assumptions:

(iii) limt→∞ d(t) = 0, (iv) ∞ t=0 dc22((tt)) < ∞, (v) limt→∞ ab((tt)) = limt→∞ c(b(Ttt) ) = 0.
Let us recall from the GL algorithm that we had one

stochastic approximation update for λ(t) and a single stepsize

sequence {a(t)}t≥0. However, in the GPL algorithm, we will have three stochastic approximation updates and hence there

are three step size sequences a(t), b(t) and c(t) in the sequel;

the stepsize d(t) is used in estimate update θ(t).

Conditions (i) and (ii) are standard requirements for stochas-

tic approximation step sizes. Conditions (iii) and (iv) are

additional requirements for the θ(t) update scheme described

later. Condition (v) ensures that the three update equations run

on separate timescales. The stepsize c(t) corresponds to the

slowest timescale and a(t) corresponds to the fastest timescale

among these step size sequences; this can be understood from

the fact that any iteration involving c(t) as the stepsize will

vary very slowly, and the iteration involving a(t) will vary

fast due to the large step sizes. In condition (v), we have
c( Tt ) instead of c(t) because θ(t) in our proposed GPL algorithm will be updated only once in every T time slots,
and hence a step size c( Tt ) will be used to update θ(t) by using observations from all sensors whenever J (t) = 1.

B. Gibbs sampling step for sensor subset selection

The algorithm also maintains a running estimate h(t)(B)

of h(B) for all B ∈ B. At time t, it selects a random sensor

jt ∈ N uniformly and independently, and sets Bjt (t) = 1 with

probability p(t) :=

e−βh(t)(B−jt (t−1),1)

and

e−βh(t)(B−jt (t−1),1)+e−βh(t)(B−jt (t−1),0)

Bjt (t) = 0 with probability (1 − p(t)). For k = jt, it sets

Bk(t) = Bk(t − 1). 1 The sensors are activated according to

B(t), and the observations ZB(t)(t) := {zk(t) : Bk(t) = 1} are collected. Then the algorithm declares Xˆ (t) = µ2(Hc(t)).

C. Parameter estimate update θ(t)
If J (t) = 1, the fusion center reads all sensors and obtains Z(t). This is required primarily because we seek to update θ(t) iteratively and reach a local maximum of the function:
g(θ) = EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ)
g(θ) is the expected log-likelihood of Z(t) given θ, when all sensors are active and X(t) ∼ pθ0 (·). Note that, maximizing g(θ) minimizes the KL divergence D(p(Z(t)|θ0)||p(Z(t)|θ)). If we use only the sensor observations corresponding to the activation vector B(t) obtained from Gibbs sampling, then the estimate will be biased by the reading of the sensors which are chosen more frequently by Gibbs sampling. However, we will later see that the additional amount of sensing and communication can be made arbitrarily small by choosing T large enough.
Since we seek to reach a local maximum of g(θ) = EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ), a gradient ascent scheme is employed. The gradient of g(θ) along any coordinate can be computed by perturbing θ in two opposite directions along that coordinate and evaluating the difference of g(·) at those two perturbed values. However, if θ0 is highdimensional, then estimating this gradient along all coordinates is computationally intensive. Moreover, evaluating g(θ) for any θ requires us to compute an expectation over the distribution pθ0 (·) and the distribution of the observation noise at all sensors, which might also be expensive. Hence, we perform a noisy gradient estimation for g(θ) by simultaneous perturbation stochastic approximation (SPSA) as in [14]. Our algorithm generates ∆(t) ∈ {1, −1}d uniformly over all sequences, and perturbs the current estimate θ(t) by a random vector d(ν(t))∆(t) (recall that ν(t) := tτ=0 J (τ )) in two opposite directions to obtain θ(t) + d(ν(t))∆(t) and θ(t) − d(ν(t))∆(t), and estimates each component of the gradient from the following difference:
log p(Z(t)|θ(t)+d(ν(t))∆(t))−log p(Z(t)|θ(t)−d(ν(t))∆(t))
This estimate is noisy because (i) Z(t) and ∆(t) are random, and (ii) d(ν(t)) > 0 while ideally it should be inﬁnitesimally small.
The k-th component of θ(t) is updated as follows:
1This randomization operation can even be repeated multiple times in each time slots to achieve faster convergence results.

PREPRINT VERSION

7

θk(t + 1)

log p(Z(t)|θ(t) + d(ν(t))∆(t))

= θk(t) + c(ν(t))J (t)

2d(ν(t))∆k(t)

log p(Z(t)|θ(t) − d(ν(t))∆(t))

−

(7)

2d(ν(t))∆k(t)

Θ

The iterates are projected onto the compact set Θ to ensure boundedness.
Note that, (7) is a stochastic gradient ascent iteration performed once in every T slots with step size c(ν(t)). On the other hand, the sequence {d(t)}t≥0 is the perturbation sequence used in gradient estimation, and hence it need not behave like a standard stochastic approximation step size sequence. However, d(t) should converge to 0 as t → ∞, in order to ensure that the gradient estimate is asymptotically unbiased. The conditions (iii) and (iv) in Section IV-A are technical conditions required for the convergence of (7).

YB(t) is expensive (since it requires us to evaluate an expectation over the conditional distribution of X(t) given ZB(t) and θ(t)), one can obtain an unbiased estimate of YB(t) by drawing a random sample of X(t) from the distribution pθ(t)(·), and scaling the sample squared error ||X(t) − XˆB(t)||2 by the ratio of the two distributions p(·|θ(t), ZB(t)) and pθ(t)(·); this technique is basically importance sampling (see [15]) with
only one sample.
Using YB(t), the following update is made for all B ∈ B:

f (t+1)(B) = [f (t)(B) + J (t)a(ν(t))(YB(t) − f (t)(B))]A0 0 (10)

The iterates are projected onto a compact interval

[0, A0] to ensure boundedness. The goal here is that, if θ(t) → θ∗, then f (t)(B) will converge

to

EZB(t)∼p(·|θ0)EX(t)∼p(·|θ∗,ZB(t))(||X (t)

−

XˆB(t)||2|ZB(t), θ∗), which is equal to f (B) under θ∗ = θ0.

We will later argue that this occasional O(2N ) computation

for all B ∈ B can be avoided, but convergence will be slow.

D. Weight update λ(t)

λ(t) is updated as follows:

λ(t + 1) = [λ(t) + b(t)(||B(t)||1 − N¯ )]A0 0 .

(8)

The intuition here is that, if ||B(t)||1 > N¯ , the sensor activation cost λ(t) needs to be increased to prohibit activating large
number of sensors in future; this is motivated by Lemma 2 and is similar to the λ(t) update equation in GL algorithm. The goal is to converge to λ∗ as deﬁned in Theorem 1.

E. MSE estimate update f (t)(B)
Since pθ0 (·) is not known initially, the true value of f (B) (i.e., the MSE under conﬁguration B and known distribution of X(t)) is not known; hence, the proposed GPL algorithm updates an estimate f (t)(B) using the sensor observations. If J (t) = 1, the fusion center obtains Z(t) by reading all sensors. The goal is to obtain a random sample YB(t) of the MSE under a conﬁguration B, by using these observations, and update f (t)(B) using YB(t).
However, since θ0 is unknown and only θ(t) is available, as an alternative to the MSE under conﬁguration B, the fusion center uses the trace of the conditional covariance matrix of X(t) given ZB(t), assuming that X(t) ∼ p(·|θ(t), ZB(t)). Hence, we deﬁne a random variable:
YB(t) := EX(t)(||X(t) − XˆB(t)||2|ZB(t), θ(t)) (9)
for each B ∈ B, where XˆB(t) is the MMSE estimate declared by µ2 for conﬁguration B and given the observation ZB(t) made by active sensors determined by B, under the assumption that X(t) ∼ pθ(t)(·). Clearly, YB(t) is a random variable with the randomness coming from two sources: (i) randomness of θ(t), and (ii) randomness of ZB(t) which has a distribution p(ZB(t)|θ0) since the original X(t) process that yields ZB(t) has a distribution pθ0 (·). Computation of YB(t) is simple for Gaussian X(t) and the MMSE estimator, since closed form expressions are available for YB(t). In case the computation of

F. The GPL algorithm
A summary of all the steps of the GPL algorithm is provided below. We will show in Theorem 5 the almost sure convergence of this algorithm.
GPL algorithm: Initialize all iterates arbitrarily. For any time t = 0, 1, 2, · · · , do the following:

1) Sensor activation and process estimation: Perform
the Gibbs sampling step as in Section IV-B and
obtain the activation vector B(t). Activate the sensors
{k ∈ 1, 2, · · · , N : Bk(t) = 1} and obtain the corresponding observations ZB(t)(t) at the fusion center. Estimate Xˆ (t) using the observations ZB(t)(t) and the current parameter estimate θ(t). If J (t) = 1,
read all sensors and obtain Z(t). Compute or estimate
YB(t) (deﬁned in (9)) for all B ∈ B. 2) f (t)(B) update: If J (t) = 1, update f (t)(B) for
all B ∈ B using (10) and also update h(t)(B) = f (t)(B) + λ(t)||B||1 for all B ∈ B. 3) θ(t) update: If J (t) = 1, update θ(t) using (7). If
J (t) = 0, then θ(t + 1) = θ(t).
4) λ(t) update: Update λ(t) according to (8).

Note that, in the GPL algorithm, it is sufﬁcient to consider

Hp(t) = {λ(t); B(t − 1); f (t)(B)∀B ∈ B}, and Hc(t) =

{B(t); Z(t); θ(t)}. The Gibbs sampling step tries to minimize

a running estimate h(t)(·) of the unconstrained cost function

over the space of conﬁgurations B.

Multiple timescales: GPL has multiple iterations running

in multiple timescales (see [12, Chapter 6]). The {B(t)}t≥0

process runs ar the fastest timescale, whereas the {θ(t)}t≥0

update scheme runs at the slowest timescale. The basic idea is

that a faster timescale iterate views a slower timescale iterate

as quasi-static, whereas a slower timescale iterate views a

faster timescale iterate as almost equilibriated. For example,

since

limt→∞

c(t) a(t)

=

0,

the

θ(t)

iterates

will

vary

very

slowly

compared to f (t)(B) iterates; as a result, f (t)(B) iterates will

view quasi-static θ(t). In other words, the iterates will behave

PREPRINT VERSION

8

as if a slower timescale iterate varies in a slow outer loop, and a faster timescale iterate varies in an inner loop.

G. Complexity of GPL and reducing the complexity

Sampling and communication complexity: Since all sen-

sors are activated when J (t) = 1, the mean number of addi-
tional active sensors per unit time is O( NT ); these additional O( NT ) observations need to be communicated to the fusion center. However, the additional O( NT ) sensing can be made large enough by choosing a very large T .

Computational complexity: The computation of YB(t) in (10) for all B ∈ B requires O(2N ) expectation computa-

tions whenever J (t) = 1. However, if one chooses large

T (e.g., O(4N )), then this additional computation per unit

time will be small. However, if one wants to avoid that

computation also, then, when J (t) = 1, one can simply

compute YB(t)(t) and update f (t)(B(t)) instead of doing it

for all conﬁgurations B ∈ B. However, the stepsize sequence

a(ν(t)) cannot be used; instead, a stepsize a(νB(t)) has to

be used when B(t) = B and f (t)(B) is updated using

(10), where νB(t) :=

t τ

=0

J

(τ

)I(B

(τ

)

=

B).

In

this

case, the convergence result (Theorem 5) on GPL will still

hold; however, the proof will require a technical condition

lim inft→∞

νB (t) t

>

0

almost

surely

for

all

B

∈

B,

which

will

be satisﬁed by the Gibbs sampler using ﬁnite β and bounded

h(t)(B). However, we discuss only (10) update in this paper

for the sake of simplicity in the convergence proof, since

technical details of asynchrounous stochastic approximation

required in the variant mentioned in this subsection are not

the main theme of this paper.

When J (t) = 1, one can avoid computation of h(t+1)(B)

for all B ∈ B in Step 2 of GPL. Instead, the fusion center can

update only h(t)(B(t)), h(t)(B−jt (t−1), 1) and h(t)(B−jt (t−

1), 0) at time t, since only these iterates are required in the

Gibbs sampling.

H. Convergence of GPL
We will ﬁrst list a few assumptions that will be crucial in the convergence proof of GPL.
Assumption 2. The distribution pθ(·) and the mapping µ2 as deﬁned before are Lipschitz continuous in θ ∈ Θ.
Assumption 3. µ2 is known to the fusion center.
Assumption 3 allows us to focus only on the sensor subset selection problem rather than the problem of estimating the process given the sensor observations.
Assumption 4. Let us consider YB(t) with θ(t) = θ∀t ∈ {0, 1, 2, · · · } ﬁxed in GPL. Suppose that, one uses BG to solve the unconstrained problem (P2) for a given λ, but with the MSE ||X(t) − Xˆ (t)||2 replaced by YB(t)(t) (under a ﬁxed θ) in the objective function of (P2), and then ﬁnds the λ∗(θ) as in Theorem 1 to meet the constraint N¯ with equality. We assume that, for the given β and N¯ , and for each θ ∈ Θ, there exists λ∗(θ) ∈ [0, A0) such that, the optimal Lagrange multiplier to relax this new unconstrained problem is λ∗(θ) (Theorem 1). Also, λ∗(θ) is Lipschitz continuous in θ ∈ Θ.

Assumption 4 makes sure that the λ(t) iteration (8) con-
verges, and the constraint is met with equality. Let us deﬁne the function Γ¯θ(φ) := limδ↓0 [θ+δφδ]Θ−θ ; this
function is parameterized by θ ∈ Θ, and calculates the gradient
of the projection function at θ.
Assumption 5. Consider the function g(θ) =
EX(t)∼pθ0 (·),B(t)=[1,1,··· ,1] log p(Z(t)|θ); this is the expected conditional log-likelihood function of Z(t) conditioned
on θ, given that X(t) ∼ pθ0 (·) and B(t) = [1, 1, · · · , 1]. We assume that the ordinary differential equation (ODE) θ˙(τ ) = Γ¯θ(τ)(∇g(θ(τ ))) has a globally asymptotically stable solution θ∗ in the interior of Θ. Also, ∇g(θ) is Lipschitz
continuous in θ.
One can show that the θ(t) iteration (7) asymptotically tracks the ordinary differential equation (ODE) θ˙(τ ) = ∇g(θ(τ )) inside the interior of Θ. In fact, Γ¯θ(τ)(∇g(θ(τ )) = ∇g(θ(τ )) when θ(τ ) lies inside the interior of Θ. The globally asymptotically stable equilibrium condition on θ˙(τ ) = Γ¯θ(τ)(∇g(θ(τ ))) is required to make sure that the iteration does not converge to some unwanted point on the boundary of Θ due to the forced projection. The assumption on θ∗ makes sure that the θ(t) iteration converges to θ∗. However, if there
does not exist such a globally asymptotically stable equilib-
rium, θ(t) converges almost surely to the set of stationary
points of the ODE.
The following result tells us that the iterates of GPL almost
surely converge to the desired values.

Theorem 5. Under Assumptions 2, 3, 4, 5 and the GPL algorithm, we have limt→∞ θ(t) = θ∗ almost surely. Correspondingly, λ(t) → λ∗(θ∗) almost surely. Also, f (t)(B) → EZB(t)∼p(·|θ0)EX(t)∼p(·|θ∗,ZB(t))(||X(t) − XˆB(t)||2|ZB(t), θ∗) =: fθ∗ (B) almost surely for all B ∈
B. The B(t) process reaches the steady-state distribution
πβ,fθ∗ ,λ∗(θ∗),θ∗ (·) which can be obtained by replacing h(B) in (6) by fθ∗ (B) + λ∗(θ∗)||B||1 where fθ∗ (B) is the MSE under conﬁguration B if the true parameter is θ∗.
In case there does not exist a globally asymptotically stable equilibrium θ∗, the θ(t) iteration almost surely converges to the stationaly points of the ODE θ˙(τ ) = Γ¯θ(τ)(∇g(θ(τ ))).

Proof. See Appendix G.

Now we make a few observations. If θ(t) → θ∗, but

the constraint in (P1) is satisﬁed with λ = 0 and policy

µ2(·; ·; θ∗),2 then λ(t) → 0, i.e., λ∗(θ∗) = 0, and the constraint becomes redundant. If θ∗ exists, then, under the

above assumptions, we will have θ∗ = θ0, and the algorithm

reaches the global optimum.

If all sensors are not read when J (t) = 1, then one

has to update θ(t) based on the observations ZB(t)(t)

collected from the sensors determined by B(t). In that case,

θ(t) will converge to a stationary point θ1 of g1(θ) :=

limt→∞ EX(t)∼pθ (·),B(t)∼πβ,f ,λ∗(θ),θ log p(ZB(t)(t)|θ),

which

will

be

0
different

fromθ θ∗

of

Theorem

5

in

general.

However, in the numerical example in Section V, we observe

numerically that θ1 = θ∗ can be possible.

2The other two arguments of µ2 are B(t) and ZB(t)(t)

PREPRINT VERSION

9

Figure 2. Comparison among OPT, BG under steady state (T0 = ∞), BG with ﬁnite iterations, GREEDY1 and GREEDY2, for solving problem (P2). For each β, BG with ﬁnite iterations stops after T0 = 100 iterations. The results of BG with ﬁnite iterations are averaged over 100 independent sample paths. Details are provided in Section V-A.
V. NUMERICAL RESULTS
A. Performance of BG algorithm
For the sake of illustration, we consider N = 10 sensors which are supposed to sense X = {X1, X2, · · · , X10}, where X is a jointly Gaussian random vector with covariance matrix M . Sensor k has access only to Xk. The matrix M is chosen as follows. We generated a random N × N matrix A whose elements are uniformly and independently distributed over the interval [−1, 1], and set M = AT A as the covariance matrix of X. We set sensor activation cost λ = 2, and seek to solve (P2). We assume that sensing at each node is perfect, and that the fusion center estimates Xˆ from the observation {Xi}i∈S =: XS as E(X|XS), where S is the set of active sensors. Under such an estimation scheme, the conditional distribution of XSc is still a jointly Gaussian random vector with mean E(XSc |XS) and the covariance matrix M (Sc, Sc) − M (Sc, S)M (S, S)−1M (S, Sc) (see [16, Proposition 3.4.4]), where M (S, Sc) is the restriction of M to the rows indexed by S and the columns indexed by Sc. The trace of this covariance matrix gives the MMSE when the subset S of sensors are active.
In Figure 2, we compare the cost for ﬁve algorithms:
• OPT: Here we consider the minimum cost for (P2). • BG under steady state: Here the conﬁguration B ∈ B
is chosen according to the distribution πβ(·) deﬁned in Section III, which can be obtained by running BG for T0 = ∞ iterations. This is done for several values of β. • BG with ﬁnite iteration: Here we run BG algorithm for T0 = 100 iterations. This is done independently for several values of β, where for each β the iteration starts from an independent random conﬁguration. Note that, we have simulated 100 independent sample paths of BG for each β, and averaged the result over these sample paths. • GREEDY1: Start with an empty set S, and ﬁnd the cost if this subset of sensors are activated. Then compare this cost with the cost in case sensor 1 is added to this set. If it turns out that adding sensor 1 to this set S reduces the cost, then add sensor 1 to the set S; otherwise, remove

Figure 3. Comparison among OPT, BG under steady state (T0 = ∞), and GREEDY2, for solving problem (P4). Details are provided in Section V-B.
sensor 1 from set S. Do this operation serially for all sensors, and activate the sensors given by the ﬁnal set S. • GREEDY2: Start with an empty set S, and ﬁnd the cost if this subset of sensors are activated. Then ﬁnd the sensor j1 which, when added to S, will result in the minimum cost. If the cost for S ∪ {j1} is less than that of S, then do S = S ∪ {j1}. Now ﬁnd the sensor j2 which, when added to S, will result in the minimum cost. If the cost for S ∪ {j2} is less than that of S, then do S = S ∪{j2}. Repeat this operation N times, and activate the set of sensors given by the ﬁnal set S. This algorithm is adapted from [3].
It turns out that, under the optimal conﬁguration, 5 sensors are activated and the optimal cost is 13.9184. GREEDY1 activates 7 sensors and incurred a cost of 15.7881. On the other hand, GREEDY2 activates 6 sensors and incurs a cost of 15.1234. However, we are not aware of any monotonicity or supermodularity property of the objective function in (P2); hence, we cannot provide any constant approximation ratio guarantee for GREEDY1 and GREEDY2 algorithms for the problem (P2). On the other hand, we have already proved that BG performs near optimally for large β. Hence, we choose to investigate the performance of BG, though it might require more number of iterations compared to N = 10 iterations for GREEDY1 or O( N(N2−1) ) iterations for GREEDY2. It is important to note that, (P2) is NP-hard, and BG allows us to avoid searching over 2N possible conﬁgurations.
In Figure 2, we can see that for β ≥ 3, the steady state distribution πβ(·) of BG achieves better expected cost than GREEDY1 and GREEDY2, and the cost becomes closer to the optimal cost as β increases. On the other hand, for each β ≥ 5, BG after 100 iterations yielded a conﬁguration that achieves near-optimal cost. Hence, BG with reasonably small number of iterations can be used to ﬁnd the optimal subset of active sensors. Note that, in this numerical example, BG with 100 iterations need to compute the cost for 200 conﬁgurations, while GREEDY1 and GREEDY2 need to compute the cost for 10 and 45 conﬁgurations respectively; but this additional amount of computation (which is much less that 2N ) can signiﬁcantly reduce the cost. However, the real advantage of Gibbs sampling based subset selection over the greedy subset selection is that, when an unknown distribution is learnt over

PREPRINT VERSION

10
starting from λ(0) = 4 and and using the stepsize sequence a(t) = 1t , the iterate λ(t) becomes very close to λ∗ = 2 within 100 iterations. Thus, our numerical illustration shows that GL algorithm has reasonably fast convergence rate for practical active sensing. We will later demonstrate convergence of the mean number of active sensors per slot to N¯ for GPL, and hence do not show it here.

Figure 4. Illustration for convergence speed of λ(t) (averaged over 50 independent sample paths) in the GL algorithm.

D. Performance of GPL

time, the greedy algorithm has to be re-run each time with O(N ) or O(N 2) complexity, whereas Gibbs sampling can be run in each slot iteratively with minimal computational cost while achieving near-optimal performance under large β.
B. Performance of BG applied to problem (P4) Here we seek to solve problem (P4) with N¯ = 4 under
the same setting as in Section V-A except that a new sample of the covariance matrix M is chosen. Here we compare the MSE for the following three cases:
• OPT: Here we choose an optimal subset for (P4). • BG under steady state: Here we assume that the conﬁgu-
ration B is chosen according to the steady-state distribution πβ(·), but restricted only to the set {B ∈ B : ||B||1 = N¯ }. This is done by putting h(B) = E||X − Xˆ ||2 if ||B||1 = N¯ and h(B) = ∞ otherwise. This is done for several values of β. • GREEDY2: Start with an empty set S, and ﬁnd the MSE if this subset of sensors are activated. Then ﬁnd the sensor j1 which, when added to S, will result in the minimum MSE. If the MSE for S ∪ {j1} is less than that of S, then do S = S∪{j1}. Now ﬁnd the sensor j2 which, when added to S, will result in the minimum MSE. If the MSE for S ∪ {j2} is less than that of S, then do S = S ∪ {j2}. Repeat this until we have |S| = N¯ , and activate the set of N¯ sensors given by the ﬁnal set S. A similar greedy algorithm is used in [3].
The performances for these three cases are shown in Figure 3. BG outperforms GREEDY2 for β ≥ 3, and becomes very close to OPT performance for β ≥ 5.
C. Convergence speed of GL algorithm
We consider a setting similar to that of Section V-A, except that we ﬁx β = 5, and choose an M which is different from that in Section V-A. Under this setting, for λ∗ = 2, BG algorithm yields the MMSE 2.4303, and the expected number of sensors activated by BG algorithm becomes 6.5247. Now, let us consider problem (P1) with the constraint value N¯ = 6.5247. Clearly, if GL algorithm is employed to ﬁnd out the solution of problem (P1) with N¯ = 6.5247, then λ(t) should converge to λ∗ = 2.
The evolution of λ(t) (averaged over 50 independent sample paths) under GL is shown in Figure 4. We can see that,

Now we demonstrate the performance of GPL to solve (P1).

We consider the following parameter values: N = 10, N¯ = 4,

a(t)

=

0.1 t0.6

,

b(t)

=

0.1 t0.8

,

c(t)

=

0t.1 ,

d(t)

=

0.1 t0.1

,

T

=

50,

λ(0) = 0.05, β = 1000. Gibbs sampling is run 10 times per

slot.

For illustration purpose, we assume that X(t) ∼ N (θ0, (1− θ0)2) scalar, and zk(t) = X(t) + wk(t), where θ0 = 0.5 and wk(t) is zero mean i.i.d. Gaussian noise independent across k. Standard deviation of wk(t) is chosen uniformly and independently from the interval [0, 0.5], for each k ∈ {1, 2, · · · , N }.
Initial estimate θ(0) = 0.2, Θ = [0, 0.8].

We consider three possible algorithms and cases: (i) GPL in
its basic form (all sensors are read when J (t) = 1, and θ(t) and f (t)(B) are updated for all B ∈ B when J (t) = 1), (ii)
a low-complexity variation of GPL called GPL-L where all sensors are not read when J (t) = 1, and f (t)(B(t)) and θ(t)
updates are done every T slots, and (iii) the OPT case where N¯ sensors with smallest observation noise variances are used
for MMSE estimation in each slot, with a perfect knowledge
of θ0 = 0.5.

The time-average MSE per slot, mean number of active

sensors per slot, λ(t) and θ(t) are plotted against t in Fig-

ure 5. MSE of all these three algorithms are much smaller

than V ar(X(t)) = (1 − θ0)2 (this is MMSE without any

observation). We notice that GPL and GPL-L perform close

to OPT in terms of time-average MSE; this shows the power of

Gibbs sampling and learning θ(t) over time. We also observe

that, GPL converges faster than GPL-L, at the expense of ad-

ditional computation and communication; but both algorithms

asymptotically offer the same MSE per unit time. We have

plotted only one sample path since the algorithms converge

almost surely to the global optimum in this case, as observed

in the simulation. We observe that 1t

t τ

=1

||B

(τ

)||1

→

N

and θ(t) → θ0 almost surely for both algorithms (veriﬁed by

simulating multiple sample paths). It is interesting to note that

θ∗ = θ1 = θ0 in this numerical example (recall Theorem 5 and

the observations after that), i.e., both algorithms converge to

the true parameter value θ0. Convergence rate will vary with

stepsize and other parameters, and hence is not discussed here.

Note that, we have already shown performance improvement by the use of BG against GREEDY1 and GREEDY2 algorithms for known distribution; see Section V-A. Hence, we do not consider asymptotic performance improvement of

PREPRINT VERSION

11

Figure 5. Performance of GPL for centralized tracking of the i.i.d process.

GPL against those two algorithms. 3
VI. CONCLUSIONS
We have proposed low-complexity centralized learning algorithms for dynamic sensor subset selection for tracking i.i.d. time-varying processes. We ﬁrst provided algorithms based on Gibbs sampling and stochastic approximation for i.i.d. time-varying data with known distribution, and later provided learning algorithms for unknown, parametric distribution, and proved almost sure convergence. Numerical results demonstrate the efﬁcacy of the algorithms against simple algorithms without learning. In future, we seek to develop distributed tracking algorithms for i.i.d. process and Markov chains with known and unknown dynamics.
REFERENCES
[1] Arpan Chattopadhyay and Urbashi Mitra. Optimal sensing and data estimation in a large sensor network. In IEEE Global Communications Conference (GLOBECOM), pages –. IEEE, 2017.
[2] Arpan Chattopadhyay and Urbashi Mitra. Optimal active sensing for process tracking. In International Symposium on Information Theory (ISIT), pages –. IEEE, 2018.
3However, it is important to note that the OPT performance for the speciﬁc numerical example in Figure 5 can be achieved by GREEDY2 because of the simple model and known θ0, but this is not true in general as observed in the numerical example of Section V-A. In order to use GREEDY1 and GREEDY2 when θ0 is unknown, one has to run these two algorithms each time θ(t) is updated, and for this the MSE for all B ∈ B need to be recomputed for each new value of θ(t). On the contrary, performing one or a few steps of Gibbs sampling will be much easier in a particular slot.

[3] D. Wang, J. Fisher III, and Q. Liu. Efﬁcient observation selection in

probabilistic graphical models using bayesian lower bounds. In Pro-

ceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial

Intelligence (UAI), pages 755–764. ACM, 2016.

[4] F. Schnitzler, J.Y. Yu, and S. Mannor. Sensor selection for crowdsensing

dynamical systems. In International Conference on Artiﬁcial Intelligence

and Statistics (AISTATS), pages 829–837, 2015.

[5] D.S. Zois, M. Levorato, and U. Mitra. Active classiﬁcation for pomdps:

A kalman-like state estimator. IEEE Transactions on Signal Processing,

62(23):6209–6224, 2014.

[6] D.S. Zois, M. Levorato, and U. Mitra. Energy-efﬁcient, heterogeneous

sensor selection for physical activity detection in wireless body area

networks. IEEE Transactions on Signal Processing, 61(7):1581–1594,

2013.

[7] V. Krishnamurthy and D.V. Djonin. Structured threshold policies for dynamic sensor schedulingâA˘Tˇa partially observed markov decision pro-

cess approach. IEEE Transactions on Signal Processing, 55(10):4938–

4957, 2007.

[8] W. Wu and A. Arapostathis. Optimal sensor querying: General marko-

vian and lqg models with controlled observations. IEEE Transactions

on Automatic Control, 53(6):1392–1405, 2008.

[9] V. Gupta, T.H. Chung, B. Hassibi, and R.M. Murray. On a stochastic

sensor selection algorithm with applications in sensor scheduling and

sensor coverage. Automatica, 42:251–260, 2006.

[10] A. Bertrand and M. Moonen. Efﬁcient sensor subset selection and link

failure response for linear mmse signal estimation in wireless sensor

networks. In European Signal Processing Conference (EUSIPCO), pages

1092–1096. EURASIP, 2010.

[11] P. Bremaud. Markov Chains, Gibbs Fields, Monte Carlo Simulation,

and Queues. Springer, 1999.

[12] Vivek S. Borkar. Stochastic approximation: a dynamical systems

viewpoint. Cambridge University Press, 2008. [13] A. Chattopadhyay and B. BÅC´ aszczyszyn.

Gibbsian on-line

distributed content caching strategy for cellular networks.

https://arxiv.org/abs/1610.02318, 2016.

[14] J.C. Spall. Multivariate stochastic approximation using a simultaneous

perturbation gradient approximation. IEEE Transactions on Automatic

Control, 37(3):332–341, 1992.

PREPRINT VERSION

12

[15] Rajan Srinivasan. Importance sampling: Applications in communications and detection. Springer Science & Business Media, 2013.
[16] B. Hajek. An Exploration of Random Processes for Engineers. Lecture Notes for ECE 534, 2011.
[17] A. Chattopadhyay, M. Coupechoux, and A. Kumar. Sequential decision algorithms for measurement-based impromptu deployment of a wireless relay network along a line. IEEE/ACM Transactions on Networking, longer version available in http://arxiv.org/abs/1502.06878, 24(5):2954– 2968, 2016.
APPENDIX A PROOF OF THEOREM 1
We will prove only the ﬁrst part of the theorem where there exists one B∗. The second part of the theorem can be proved similarly. Let us denote the optimizer for (P1) by B, which is possibly different from B∗. Then, by the deﬁnition of B∗, we have f (B∗) + λ∗||B∗||1 ≤ f (B) + λ∗||B||1. But ||B||1 ≤ K (since B is a feasible solution to the constrained problem) and ||B∗||1 = K (by assumption). Hence, f (B∗) ≤ f (B). This completes the proof.

Hence,
= ≥ ≥ = ≥ =

∞

(1 − δ(Pl))

l=0 ∞

inf

min{Pl(B , B), Pl(B

l=0 B ,B ∈B B∈B

∞

N −1

2N

l=0

k=0

e−β(0) log(1+lN +k)×∆ 2N

∞ N −1 e−β(0) log(1+lN +N )×∆

l=0 k=0
1∞

N 1

NN

(1 + lN )β(0)N∆

l=1

1

∞

1

N N+1

(1 + i)β(0)N∆

i=N +1

∞

, B)} (11)

APPENDIX B WEAK AND STRONG ERGODICITY
Consider a discrete-time Markov chain (possibly not timehomogeneous) {B(t)}t≥0 with transition probability matrix (t.p.m.) P (m; n) between t = m and t = n. We denote by D the collection of all possible probasbility distributions on the state space. Let dV (·, ·) denote the total variation distance between two distributions in D. Then {B(t)}t≥0 is called weakly ergodic if, for all m ≥ 0, we have limn↑∞ supµ,ν∈D dV (µP (m; n), νP (m; n)) = 0. The Markov chain {B(t)}t≥0 is called strongly ergodic if there exists π ∈ D such that, limn↑∞ supµ∈D dV (µT P (m; n), π) = 0 for all m ≥ 0.

APPENDIX C PROOF OF THEOREM 3

We will ﬁrst show that the Markov chain {B(t)}t≥0 in

weakly ergodic.

Let us deﬁne ∆ := maxB∈B,A∈B |h(B) − h(A)|.

Consider the transition probability matrix (t.p.m.) Pl for

the inhomogeneous Markov chain {Y (l)}l≥0 (where Y (l) :=

B(lN )). The Dobrushin’s ergodic coefﬁcient δ(Pl) is given

by (see [11, Chapter 6, Section 7] for deﬁnition) δ(Pl) =

1 − infB ,B ∈B B∈B min{Pl(B , B), Pl(B , B)}. A sufﬁcient condition for the Markov chain {B(t)}t≥0 to be weakly ergodic is ∞ l=1(1 − δ(Pl)) = ∞ (by [11, Chapter 6, Theorem 8.2]).

Now, with positive probability, activation states for all nodes

are updated over a period of N slots. Hence, Pl(B , B) > 0

for all B , B ∈ B. Also, once a node jt for t = lN + k is

chosen in ABG algorithm, the sampling probability for any activation state in a slot is greater than e−β(lN2 +k)∆ . Hence, for independent sampling over N slots, we have, for all pairs

B , B:

N −1 e−β(lN +k)∆

Pl(B , B) >

>0 2N

k=0

Here the ﬁrst inequality uses the fact that the cardinality of B is 2N . The second inequality follows from replacing k by

N in the numerator. The third inequality follows from lower-

bounding

1 (1+lN )β(0)N∆

by

1 N

ilN=l+NN −1 (1+i)β1(0)N∆ . The last

equality follows from the fact that

∞1 i=1 ia

diverges

for

0<

a < 1.

Hence, the Markov chain {B(t)}t≥0 is weakly ergodic.

In order to prove strong ergodicity of {B(t)}t≥0, we

invoke [11, Chapter 6, Theorem 8.3]. We denote the t.p.m. of {B(t)}t≥0 at a speciﬁc time t = T0 by Q(T0), which is

a given speciﬁc matrix. If {B(t)}t≥0 evolves up to inﬁnite

time with ﬁxed t.p.m. Q(T0), then it will reach the stationary

distribution πβ (B) = e−βT0 h(B) . Hence, we can claim that

T0

ZβT0

Condition 8.9 of [11, Chapter 6, Theorem 8.3] is satisﬁed.

Next, we check Condition 8.10 of [11, Chapter 6, Theo-

rem 8.3]. For any B ∈ arg minB ∈B h(B ), we can argue
that πβT0 (B) increases with T0 for sufﬁciently large T0; this can be veriﬁed by considering the derivative of πβ(B) w.r.t.

β. For B ∈/ arg minB ∈B h(B ), the probability πβT0 (B) decreases with T0 for large T0. Now, using the fact that

any monotone, bounded sequence converges, we can write ∞ T0=0 B∈B |πβT0+1 (B) − πβT0 (B)| < ∞.
Hence, by [11, Chapter 6, Theorem 8.3], the Markov chain

{B(t)}t≥0 is strongly ergodic. It is straightforward to verify

the claim regarding the limiting distribution.

APPENDIX D PROOF OF LEMMA 1
Let λ1 > λ2 > 0, and the corresponding optimal error and mean number of active sensors under these multiplier values be (f1, n1) and (f2, n2), respectively. Then, by deﬁnition, f1+ λ1n1 ≤ f2 + λ1n2 and f2 + λ2n2 ≤ f1 + λ2n1. Adding these two inequalities, we obtain λ1n1 + λ2n2 ≤ λ1n2 + λ2n1, i.e., (λ1 − λ2)n1 ≤ (λ1 − λ2)n2. Since λ1 > λ2, we obtain n1 ≤ n2. This completes the ﬁrst part of the proof. The second part of the proof follows using similar arguments.

PREPRINT VERSION

13

APPENDIX E PROOF OF LEMMA 2

1) Convergence in the fastest timescale: Let us denote the probability distribution of B(t) under GPL by π(t) (a column

Let us denote Eµ2 ||B(t)||1 =: g(λ) =

. B∈B ||B||1e−βh(B)
Zβ

It is straightforward to see that Eµ2 ||B(t)||1 is continuously differentiable in λ. Let us denote Zβ by Z for simplicity. The

derivative of g(λ) w.r.t. λ is given by:

vector indexed by the coﬁgurations from B), and the corresponding transition probability matrix (TPM) by A(t); i.e., (π(t+1))T = (π(t))T A(t) = (1−1)×(π(t))T +1×(π(t))T A(t). This form is similar to a standard stochastic approximation scheme as in [12, Chapter 2] except that the step size sequence

for π(t) iteration is a constant sequence. Also, if f (t)(B), λ(t)
g (λ)

−Zβ

||B||2 e−β(f (B)+λ||B||1) −

||B|| e−β(f(B)+λ||B||1) dZand θ(t) are constant with time t, then A(t) = A will also

=

B∈B

1

B∈B

1

Z2

dλ
be constant with time t, and the stationary distribution for

Now, it is straightforward to verify that ddZλ = −βZg(λ). Hence,

the TPM A will exist and will be Lipschitz continuous in all (constant) slower timescale iterates. Hence, by using similar argument as in [12, Chapter 6, Lemma 1], one can show the

g (λ) following for all B ∈ B:

lim |π (B) − π (B)| = 0 a.s. −Zβ B∈B ||B||21e−β(f(B)+λ||B||1) + B∈B ||B||1e−β(f(B)+λ||B||1)βZg(λ)

(t)

=

Z2

t→∞

β,f (t),λ(t),θ(t)

(13)

Now, g (λ) ≤ 0 is equivalent to

g(λ) ≤

B∈B ||B||21e−β(f (B)+λ||B||1) B∈B ||B||1e−β(f (B)+λ||B||1)

Noting that E||B||1 =: g(λ) and dividing the numerator and
denominator of R.H.S. by Z, the condition is reduced to E||B||1 ≤ EE||||BB||||121 , which is true since E||B||21 ≥ (E||B||1)2. Hence, E||B||1 is decreasing in λ for any β > 0. Also, it is easy to verify that |g (λ)| ≤ (β + 1)N 2. Hence, g(λ) is
Lipschitz continuous in λ.

APPENDIX F PROOF OF THEOREM 4
Let the distribution of B(t) under GL be π(t)(·). Since limt→∞ a(t) = 0, it follows that limt→∞ dV (π(t), πβ|λ(t)) = 0 (where dV (·, ·) is the total variation distance), and limt→∞(Eπ(t) ||B(t)||1 −Eπβ|λ(t)||B(t)||1) := limt→∞ e(t) = 0. Now, we can rewrite the λ(t) update equation as follows:
λ(t + 1) = [λ(t) + a(t)(Eπβ|λ(t)||B(t)||1 − N¯ + Mt + et)]cb (12)
Here Mt := ||B(t)||1 − Eπ(t) ||B(t)||1 is a Martingale difference noise sequence, and limt→∞ et = 0. It is easy to see that the derivative of Eπβ|λ||B(t)||1 w.r.t. λ is bouned for λ ∈ (b, c); hence, Eπβ|λ||B(t)||1 is a Lipschitz continuous function of λ. It is also easy to see that the sequence {Mt}t≥0 is bounded. Hence, by the theory presented in [12, Chapter 2] and [12, Chapter 5, Section 5.4], λ(t) converges to the unique zero of Eπβ|λ||B(t)||1 − N¯ almost surely. Hence, λ(t) → λ∗ almost surely. Since limt→∞ dV (π(t), πβ|λ(t)) = 0 and πβ|λ is continuous in λ, the limiting distribution of B(t) becomes πβ|λ∗ .

APPENDIX G PROOF OF THEOREM 5
The proof involves several steps, and these steps are provided one by one.

where πβ,f(t),λ(t),θ(t)(·) can be obtained by replacing h(B) in (6) by f (t)(B) + λ(t)(θ(t))||B||1

2) Convergence of iteration (10): Note that, (10) depends

on θ(t) and not on B(t) and λ(t); the iteration (10) depends

on θ(t) through the estimation function µ2. Now, f (t)(B) is

updated at a faster timescale compared to θ(t). Let us consider

the iterations (10) and (7); they constitute a two-timescale

stochastic approximation.

Note that, for a given θ, the iteration (10) remains bounded

inside a compact set independent of θ; hence, using [12,

Chapter 2, Theorem 2] with additional modiﬁcation as sug-

gested in [12, Chapter 5, Section 5.4] for projected stochastic

approximation, we can claim that limt→∞ f (t)(B) → fθ(B)

almost surely for all B ∈ B, if θ(t) is kept ﬁxed at a value θ.

Also, since µ2 is Lipschitz continuous in θ, we can claim that

fθ(B) is Lipschitz continuous in θ for all B ∈ B. We also

have

limt→∞

c(ν(t)) a(ν(t))

=

0.

Hence, by using an analysis similar to that in [17, Ap-

pendix E, Section C.2] (which uses [12, Chapter 6, Lemma 1]),

one can claim that:

lim |f (t)(B) − fθ(t)(B)| = 0 a.s. ∀B ∈ B

(14)

t→∞

This proves the desired convergence of the iteration (10). 3) Convergence of λ(t) iteration: The λ(t) iteration will
view θ(t) as quasi-static and B(t), f (t)(·) iterations as equi-
libriated. Let us assume that θ(t) is kept ﬁxed at θ. Then, by (13)
and (14), we can work with πβ,fθ,λ(t),θ in this timescale. Under this situation, (8) asymptotically tracks the iteration λ(t + 1) = [λ(t) + b(t)( B∈B πβ,fθ,λ(t),θ(B)||B||1 − N¯ + Mt)]A0 where {Mt}t≥0 is a Martingale differenece sequence. Now, πβ,fθ,λ(t),θ(B) is Lipschitz continuous in θ and λ(t) (using Assumption 2, Assumption 4 and a little algebra on
the expression (6)). If A0 is large enough, then, by the theory of [12, Chapter 2, Theorem 2] and [12, Chapter 5, Section 5.4], one can claim that λ(t) → λ∗(θ) almost surely, and λ∗(θ) is
Lipschitz continuous in θ (by Assumption 4).
Hence, by using similar analysis as in [17, Appendix E,
Section C.2] (which uses [12, Chapter 6, Lemma 1]), we can
say that, under iteration (8):

lim |λ(t) − λ∗(θ(t))| = 0 a.s.

(15)

t→∞

PREPRINT VERSION

14

4) Convergence of the θ(t) iteration: Note that, (7) is the
slowest timescale iteration and hence it will view all other
there iterations (at three different timescales) as equilibriated.
However, this iteration is not affected by other iterations.
Hence, this iteration is an example of simultaneous perturba-
tion stochastic approximation as in [14], but with a projection
operation applied on the iterates. Hence, by combining [14,
Proposition 1] and the discussion in [12, Chapter 5, Section 5.4], we can say that limt→∞ θ(t) = θ∗ almost surely in case Assumption 5 holds.
If there does not exist a globally asymptotically stable equilibrium θ∗ (as assumed in Assumption 5), then, by using
the same techniques as in [17, Appendix E, Section C], one
can claim that the θ(t) iteration almost surely converges to the stationary points of the ODE θ˙(τ ) = Γ¯θ(τ)(∇g(θ(τ ))).
5) Completing the proof: We have seen that limt→∞ θ(t) = θ∗ almost surely. Hence, by (15), limt→∞ λ(t) = λ∗(θ∗) almost surely. By (14), limt→∞ f (t)(B) = fθ∗ (B) almost surely for all B ∈ B. Then, by (13), limt→∞ π(t)(B) = πβ,fθ∗ ,λ∗(θ∗),θ∗ (B) almost surely. Hence, Theorem 5 is proved.

