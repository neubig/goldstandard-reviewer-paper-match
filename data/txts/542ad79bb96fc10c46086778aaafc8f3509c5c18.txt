Global Convergence and Variance-Reduced Optimization for a Class of Nonconvex-Nonconcave Minimax Problems

Junchi Yang ∗

Negar Kiyavash † February 25, 2020

Niao He ∗

arXiv:2002.09621v1 [math.OC] 22 Feb 2020

Abstract
Nonconvex minimax problems appear frequently in emerging machine learning applications, such as generative adversarial networks and adversarial learning. Simple algorithms such as the gradient descent ascent (GDA) are the common practice for solving these nonconvex games and receive lots of empirical success. Yet, it is known that these vanilla GDA algorithms with constant step size can potentially diverge even in the convex setting. In this work, we show that for a subclass of nonconvex-nonconcave objectives satisfying a so-called two-sided Polyak-Lojasiewicz inequality, the alternating gradient descent ascent (AGDA) algorithm converges globally at a linear rate and the stochastic AGDA achieves a sublinear rate. We further develop a variance reduced algorithm that attains a provably faster rate than AGDA when the problem has the ﬁnite-sum structure.

1 Introduction

We consider minimax optimization problems of the forms

min max f (x, y) E[F (x, y; ξ)],

(1)

x∈Rd1 y∈Rd2

and

1n

min max f (x, y)

fi(x, y),

(2)

x∈Rd1 y∈Rd2

n

i=1

where ξ is a random vector with support Ξ, and f (x, y) is a possibly nonconvex-nonconcave function. Minimax problems have been widely studied in game theory and operations research. Recent emerging applications in machine learning have further stimulated a surge of interest in these problems. For example, generative adversarial networks (GANs) [Goodfellow et al., 2016] can be viewed as a two-player game between a generator that produces synthetic data and a discriminator that diﬀerentiates between true data and synthetic data. In reinforcement learning, solving Bellman equations can also be reformulated as minimax optimization problems [Chen and Wang, 2016, Dai et al., 2017, 2018]. Other applications include robust optimization [Namkoong and Duchi, 2016, 2017], adversarial machine learning [Sinha et al., 2017, Madry et al., 2017], unsupervised learning [Xu et al., 2005], and so on.

∗Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, IL 61801, USA
(junchiy2@illinois.edu, niaohe@illinois.edu). †School of Management of Technology, E´cole Polytechnique F´ed´erale de Lausanne, Switzerland (negar.kiyavash@epfl.ch).

1

The most natural and frequently used methods for solving minimax problems (1) and (2) are the gradient descent ascent (GDA) algorithms (or their stochastic variants), with either simultaneous or alternating updates of the primal-dual variables, referred to as SGDA and AGDA, respectively, throughout the paper. While these algorithms have received much empirical success especially in adversarial training, it is known that these GDA algorithms with constant stepsizes could fail to converge for general smooth function [Mescheder et al., 2018], even for the bilinear games [Gidel et al., 2019]; even when they do converge, the stable limit point may not be a local Nash equilibrium [Daskalakis et al., 2018, Mazumdar and Ratliﬀ, 2018]. On the other hand, GDA algorithms can converge linearly to the saddle point for strongly-convex-strongly-concave functions [Facchinei and Pang, 2007]. Moreover, for many simple nonconvex-nonconcave objective functions, such as, f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y, we also observe that GDA algorithms with constant stepsizes indeed converge to the global Nash equilibrium (or saddle point), at a linear rate (see Figure 1). This also holds true for their stochastic variants, albeit at a sublinear rate. These facts naturally raise a question: Is there a general condition under which GDA algorithms converge to the global optima?

1020

105
SGDA

Stoc-SGDA

AGDA

Stoc-AGDA

50

0

-50

-100

-150 10

5

6

4

0

2

0

-5

-2

-4

-10 -6

100 100
10-20

10-40 0

200 400 600 800 1000 Iterations

10-5 0

200 400 600 800 1000 Iterations

(a) Surface plot of f (x, y) (b) Deterministic GDA algo-(c) Stochastic GDA algorithms

(d) Trajectories

rithms

Figure 1: (a) Surface plot of the nonconvex-nonconcave function f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y ; (b) Convergence of

SGDA and AGDA; (c) Convergence of stochastic SGDA and stochastic AGDA; (d) Trajectories of four algorithms

Furthermore, the use of variance reduction techniques has played a prominent role in improving the convergence over stochastic or batch algorithms for both convex and nonconvex minimization problems, which have been extensively studied in the past few years; see, e.g., [Johnson and Zhang, 2013, Reddi et al., 2016a,b, Xiao and Zhang, 2014], just to name a few. However, when it comes to the minimax problems, there are limited results, except under convex-concave setting [Palaniappan and Bach, 2016, Du and Hu, 2019]. This leads to another open question: Can we improve GDA algorithms for nonconvex-nonconcave minimax problems?

1.1 Our contributions
In this paper, we address these two questions and speciﬁcally focus on the alternating gradient descent ascent, namely AGDA. This is due to several considerations. First of all, it has been recently shown that alternating updates of GDA are more stable than simultaneous updates [Gidel et al., 2019, Bailey et al., 2019]. Note that for a convex-concave matrix game, SGDA may diverge while AGDA is proven to always have bounded iterates [Gidel et al., 2019]. See Figure 2 for a simple illustration. Secondly, in general, it is much more challenging to analyze AGDA than SGDA. There is a lack of discussion on the convergence of AGDA for general minimax problems in the literature. Our contributions are summarized as follows.
First, we identity a general condition that relaxes the convex-concavity requirement of the objective function while still guaranteeing global convergence of AGDA and stochastic AGDA (Stoc-AGDA). We call this the two-sided PL condition, which requires that both players’ utility functions satisfy Polyak-Lojasiewicz
2

7 6.5
6 5.5
5
4.5
4 0

y y

10

9

8

8

6

7

4
6
2

0

5

-2
-4 4

SGDA

-6

AGDA

-8

SGDA

AGDA

3

Initial

100

200

300

400

500

-10

-10 -8

-6

-4

-2

0

2

4

6

8

10

0

Iterations

x

10

8

6

4

2

0

-2

-4

SGDA

-6

AGDA

-8

SGDA AGDA Initial

50

100

150

200

250

300

-10

-10 -8

-6

-4

-2

0

2

4

6

8

10

Iterations

x

(a) τ = 0.01

(b) τ = 0.01

(c) τ = 0.025

(d) τ = 0.025

Figure 2: Consider the objective f (x, y) = log (1 + ex) + 3xy − log (1 + ey): (a) Convergence of AGDA and SGDA under the stepsize

τ = 0.01; (b) Trajectories of two algorithms under the stepsize τ = 0.01; (c) Convergence of AGDA and SGDA under stepsize τ = 0.025;

(d) Trajectories of two algorithms with stepsize τ = 0.025;

(PL) inequality [Polyak, 1963]. Such conditions indeed hold true for several applications, including robust least square, generative adversarial imitation learning for linear quadratic regulator (LQR) dynamics [Cai et al., 2019], and potentially many others in adversarial learning [Du et al., 2019], robust phase retrieval [Sun et al., 2018, Zhou et al., 2016], robust control [Fazel et al., 2018], and etc. We show that under the two-sided PL condition, AGDA with proper constant stepsizes converges globally to a saddle point at a linear rate of O(1 − κ−3)t, while Stoc-AGDA with proper diminishing stepsizes converges to a saddle point at a sublinear rate of O(κ5/t), where κ is the underlying condition number.
Second, for minimax problems with the ﬁnite sum structure, we introduce a variance-reduced AGDA algorithm (VR-AGDA) that leverages the idea of stochastic variance reduced gradient (SVRG) [Johnson and Zhang, 2013, Reddi et al., 2016a] with the alternating updates. We prove that VR-AGDA achieves the complexity of O((n2/3κ3 log(1/ )) in the region n ≤ κ9 and O(n + κ9) log(1/ )) in the region n ≥ κ9, where n is the number of component functions. This greatly improves over the O nκ3 log 1 complexity of AGDA when applied to the ﬁnite sum minimax problems. We summarize the results of these algorithms in Table 1. Our numerical experiments further demonstrate that VR-AGDA performs signiﬁcantly better than AGDA and Stoc-AGDA, especially for problems with large condition numbers. To our best knowledge, this is the ﬁrst work to provide a variance reduced algorithm and theoretical guarantees in the nonconvex-nonconcave regime of minimax optimization.

Algorithms Complexity

AGDA O nκ3 log 1

Stoc-AGDA O µκ25

VR-AGDA

O

n

2 3

κ3

log

1

, n ≤ κ9

O (n + κ9) log 1

Table 1: Complexities of three algorithms for the ﬁnite-sum problem (2), where κ l/µ1 is condition number, l is Lipschtiz gradient constant, µ1 and µ2 are the two-side PL constants with µ1 ≤ µ2. See Section 4 for more details.

1.2 Related work
Nonconvex minimax problems. There has been a recent surge in research on solving minimax optimization beyond the convex-concave regime [Sinha et al., 2017, Chen et al., 2017, Qian et al., 2019, Thekumparampil et al., 2019, Lin et al., 2018, Nouiehed et al., 2019, Abernethy et al., 2019], but they diﬀer from our work from various perspectives. For example, Chen et al. [2017], Sinha et al. [2017], Lin et al. [2019], Thekumparampil et al. [2019] considered the minimax problem when the objective function is nonconvex in x
3

but concave in y and focused on achieving convergence to stationary points. Their algorithms require solving the inner maximization or some sub-problems with high accuracy at every iteration, which are diﬀerent from AGDA. Lin et al. [2018] considered a general class of weakly-convex weakly-concave minimax problems and proposed an inexact proximal point method to ﬁnd an -stationary point. Their convergence result relies on assuming the existence of a solution to the corresponding Minty variational inequality, which is often hard to verify. Abernethy et al. [2019] recently showed the linear convergence of a second-order iterative algorithm, called Hamiltonian gradient descent (HGD), for a subclass of “suﬃciently bilinear” functions. Compared with their work, the PL condition we consider in this paper is easier to verify and GDA algorithms are much simpler.
PL condition. Recently, Nouiehed et al. [2019] studied a class of minimax problems where the objective only satisﬁes a one-sided PL condition and introduced the GDmax algorithm, which takes multiple ascent steps at every iteration. Our work diﬀers from [Nouiehed et al., 2019] in two aspects: (i) we consider the two-sided PL condition which guarantees global convergence 1; (ii) we consider AGDA which takes one ascent step at every iteration. Another closely related work is Cai et al. [2019]. The authors considered a speciﬁc application in generative adversarial imitation learning with linear quadratic regulator dynamics. This is a special example that falls under the two-sided PL condition.
Variance-reduced minimax optimization. There exists a few works that apply variance reduction techniques to minimax optimization. Palaniappan and Bach [2016], Luo et al. [2019] provided linearconvergent algorithms for strongly-convex-strongly-concave objectives, based on simultaneous updates. Du and Hu [2019] extended the result to convex-strongly-concave objectives with full-rank coupling bilinear term. In contrast, we are dealing with a much broader class of objectives that are possibly nonconvex-nonconcave. We point out that Luo et al. [2020] recently introduced a variance-reduced algorithm for ﬁnding the stationary point of nonconvex-strongly-concave problems, which is again diﬀerent from our setting.
The rest of this paper is organized as follows. In Section 2, we introduce the two-sided PL condition and show the equivalence of three min-max optimality criteria under this condition. In Section 3, we describe deterministic and stochastic AGDA algorithms, and provide convergence analyses of those algorithms under the two-sided PL condition. In Section 4, we introduce the variance-reduced AGDA algorithm and establish its convergence results. In Section 5, we provide numerical performance of these algorithms for robust least square and imitation learning for LQR.
2 Global optima and two-sided PL condition
Throughout this paper, we assume that the function f (x, y) in (1) is continuously diﬀerentiable and has Lipschitz gradient. We state it as a basic assumption. Here · is used to denote the Euclidean norm.
Assumption 1 (Lipschitz gradient). There exists a positive constant l > 0 such that
∇xf (x1, y1) − ∇xf (x2, y2) ≤ l[ x1 − x2 + y1 − y2 ], ∇yf (x1, y1) − ∇yf (x2, y2) ≤ l[ x1 − x2 + y1 − y2 ],
holds for all x1, x2 ∈ Rd1 , y1, y2 ∈ Rd2 .
1We also show that AGDA can ﬁnd −stationary point for minimax problems under the one-sided PL condition within O(1/ 2) iterations in Appendix D.
4

We now deﬁne three notions of optimality for minimax problems. The most direct notion of optimality is global minimax point, at which x∗ is an optimal solution to the function g(x) := maxy f (x, y) and y∗ is an optimal solution to maxy f (x∗, y). In the two-player zero-sum game, the notion of saddle point is also widely used [Von Neumann et al., 2007, Nash, 1953]. For a saddle point (x∗, y∗), x∗ is an optimal solution to minx f (x, y∗) and y∗ is an optimal solution to maxy f (x∗, y).
Deﬁnition 1 (Global optima).

1. (x∗, y∗) is a global minimax point, if for any (x, y) :

f (x∗, y) ≤ f (x∗, y∗) ≤ max f (x, y ).

(3)

y

2. (x∗, y∗) is a saddle point, if for any (x, y) :

f (x∗, y) ≤ f (x∗, y∗) ≤ f (x, y∗).

(4)

3. (x∗, y∗) is a stationary point, if :

∇xf (x∗, y∗) = ∇yf (x∗, y∗) = 0.

(5)

For general nonconvex-nonconcave minimax problems, these three notions of optimality are not necessarily equivalent. A stationary point may not be a saddle point or a global minimax point; a global minimax point may not be a saddle point or a stationary point. Note that generally speaking, for minimax problems, a saddle point or a global minimax point may not always exist. However, since our goal in this paper is to ﬁnd global optima, in the remainder of the paper, we assume that a saddle point always exists.
Assumption 2 (Existence of saddle point). The objective function f has at least one saddle point. We also assume that for any ﬁxed y, minx∈Rd1 f (x, y) has a nonempty solution set and a optimal value, and for any ﬁxed x, maxy∈Rd2 f (x, y) has a nonempty solution set and a ﬁnite optimal value.
For unconstrained minimization problems: minx∈Rn f (x), Polyak [1963] proposed Polyak-Lojasiewicz (PL) condition, which is suﬃcient to show global linear convergence for gradient descent without assuming convexity. Speciﬁcally, a function f (·) satisﬁes PL condition if it has a nonempty solution set and a ﬁnite optimal value f ∗, and there exists some µ > 0 such that 12 ∇f (x) 2 ≥ µ(f (x) − f ∗), ∀x. As discussed in Karimi et al. [2016], PL condition is weaker, or not stronger, than other well-known conditions that guarantee linear convergence for gradient descent, such as error bounds (EB) [Luo and Tseng, 1993], weak strong convexity (WSC) [Necoara et al., 2018] and restricted secant inequality (RSI) [Zhang and Yin, 2013].
We introduce a straightforward generalization of the PL condition to the minimax problem: function f (x, y) satisﬁes the PL condition with constant µ1 with respect to x, and -f satisﬁes PL condition with constant µ2 with respect to y. We formally state this in the following deﬁnition.
Deﬁnition 2 (Two-sided PL condition). A continuously diﬀerentiable function f (x, y) satisﬁes the two-sided PL condition if there exist constants µ1, µ2 > 0 such that:
∇xf (x, y) 2 ≥ 2µ1[f (x, y) − min f (x, y)], ∀x, y,
x
∇yf (x, y) 2 ≥ 2µ2[max f (x, y) − f (x, y)], ∀x, y.
y

The two-sided PL condition does not imply convexity-concavity, and it is a much weaker condition than strong-convexity-strong-concavity. In Lemma 2.1, we show that three notions of optimality are equivalent under the two-sided PL condition. Note that they may not be unique.

5

Lemma 2.1. If the objective function f (x, y) satisﬁes the two-sided PL condition, then the following holds true:
(saddle point) ⇔ (global minimax) ⇔ (stationary point).
Below we give some examples that satisfy this condition. Example 1. The nonconvex-nonconcave function in the introduction, f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y satisﬁes the two-sided PL condition with µ1 = 1/16, µ2 = 1/11 (see Appendix A).
Example 2. f (x, y) = F (Ax, By), where F (·, ·) is strongly-convex-strongly-concave and A and B are arbitrary matrices, satisﬁes the two-sided PL condition.
Example 3. The generative adversarial imitation learning for LQR can be formulated as minK minθ m(K, θ), where m is strongly-concave in terms of θ and satisﬁes PL condition in terms of K (see [Cai et al., 2019] for more details), thus satisfying the two-sided PL condition.
Under the two-sided PL condition, the function g(x) := maxy f (x, y) can be shown to satisfy PL condition with µ1 (see Appendix A). Moreover, it holds that g is also L-smooth with L := l + l2/µ2 [Nouiehed et al., 2019]. Finally, we denote µ = min(µ1, µ2) and κ = µl , which represents the condition number of the problem.

3 Global convergence of AGDA and Stoc-AGDA

In this section, we establish the convergence rate of the stochastic alternating gradient descent ascent (Stoc-AGDA) algorithm, which we present in Algorithm 1, under the two-sided PL condition. Stoc-AGDA updates variables x and y sequentially using stochastic gradient descent/ascent steps. Here we make standard assumptions about stochastic gradients Gx(x, y, ξ) and Gy(x, y, ξ).
Assumption 3 (Bounded variance). Gx(x, y, ξ) and Gy(x, y, ξ) are unbiased stochastic estimators of ∇xf (x, y) and ∇yf (x, y) and have variances bounded by σ2 > 0.

Algorithm 1 Stoc-AGDA
1: Input: (x0, y0), step sizes {τ1t}t > 0, {τ2t}t > 0 2: for all t = 0, 1, 2, ... do
3: Draw two i.i.d. samples ξt1, ξt2 ∼ P (ξ) 4: xt+1 ← xt − τ1tGx(xt, yt, ξt1) 5: yt+1 ← yt + τ2tGy(xt+1, yt, ξt2) 6: end for

Note that Stoc-AGDA with constant stepsizes (i.e., τ1t = τ1 and τ2t = τ2) and noiseless stochastic gradient (i.e., σ2 = 0) reduces to AGDA:

xt+1 = xt − τ1∇xf (xt, yt),

(6)

yt+1 = yt − τ2∇yf (xt+1, yt).

(7)

We will measure the inaccuracy of (xt, yt) through the potential function

Pt := at + λ · bt,

(8)

where at = E[g(xt) − g∗], bt = E[g(xt) − f (xt, yt)] and λ > 0 to be speciﬁed later in the theorems. Recall that g(x) := maxy f (x, y) and g∗ = minx g(x). This metric is driven by the deﬁnition of minimax point, because g(x) − g∗ and g(x) − f (x, y) are non-negative for any (x, y), and both equal to 0 if and only if (x, y) is a
minimax point.

6

Stoc-AGDA with constant stepsizes We ﬁrst consider Stoc-AGDA with constant stepsizes. We show that {(xt, yt)}t will converge linearly to a neighbourhood of the optimal set.

Theorem 3.1. Suppose Assumptions 1, 2, 3 hold and f (x, y) satisﬁes the two-sided PL condition with µ1

and

µ2.

Deﬁne

Pt

:= at +

110 bt.

If

we

run

Algorithm

1

with

τ2t

= τ2

≤

1 l

and

τ1t

= τ1

≤

, µ22 τ2
18l2

then

Pt ≤(1 − 1 µ1τ1)tP0 + δ,

(9)

2

where δ = (1−µ2τ2)(L+l)τ12+lτ22+10Lτ12 σ2.
10µ1 τ1

Remark 1. In the theorem above, we choose τ1 smaller than τ2, τ1/τ2 ≤ µ22/(18l2), because our potential function is not symmetric about x and y. Another reason is because we want yt to approach y∗(xt) ∈ arg maxy f (xt, y) faster so that ∇xf (xt, yt) is a better approximation for ∇g(xt) (∇g(x) = ∇xf (x, y∗(x)), see Nouiehed et al. [2019]). Indeed, it is common to use diﬀerent learning rates for x and y in GDA algorithms
for nonconvex minimax problems; see e.g., Jin et al. [2019] and Lin et al. [2019]. Note that the ratio between
these two learning rates is quite crucial here. We also observe empirically when the same learning rate is
used, even if small, the algorithm may not converge to saddle points.

Remark 2. When t → ∞, Pt → δ. If τ1 → 0 and τ22/τ1 → 0, the error term δ will go to 0. When using smaller stepsizes, the algorithm reaches a smaller neighbour of the saddle point yet at the cost of a slower
rate, as the contraction factor also deteriorates.

Linear convergence of AGDA Setting σ2 = 0, it follows immediately from the previous theorem that AGDA converges linearly under the two-sided PL condition. Moreover, we have

Theorem 3.2. Suppose Assumptions 1, 2 hold and f (x, y) satisﬁes the two-sided PL condition with µ1 and µ2. Deﬁne Pt := at + 110 bt. If we run AGDA with τ1 = 1µ822l3 and τ2 = 1l , then

µ1µ22 t

Pt ≤ 1 − 36l3 P0.

(10)

Furthermore, {(xt, yt)}t converges to some saddle point (x∗, y∗), and

∗2

∗2

µ1µ22 t

xt − x + yt − y ≤ α 1 − 36l3 P0,

(11)

where α is a constant depending on µ1, µ2 and l.

The above theorem implies that the limit point of {(xt, yt)}t is a saddle point and the distance to the saddle point decreases in the order of O (1 − κ−3)t . Note that in the special case when the objective is strongly-convex-strongly-concave, it is known that SGDA (GDA with simultaneous updates) achieves an O(κ2 log(1/ )) iteration complexity (see, e.g., Facchinei and Pang [2007]) and this can be further improved to O(κ log(1/ )) by extragradient methods [Korpelevich, 1976], Nesterov’s dual extrapolation [Nesterov and Scrimali, 2006] or accelerated proximal point algorithm [Lin et al., 2020]. However, these result relies heavily on the strong monotonicity of the corresponding variational inequality. For the general two-sided PL condition, we may not achieve the same dependency on κ.

Stoc-AGDA with diminishing stepsizes While Stoc-AGDA with constant stepsizes only converges linearly to a neighbourhood of the saddle point, Stoc-AGDA with diminishing stepsizes converges to the saddle point but at a sublinear rate O(1/t).

7

Theorem 3.3. Suppose Assumptions 1, 2, 3 hold and f (x, y) satisﬁes the two-sided PL condition with µ1

and

µ2.

Deﬁne

Pt

= at + 110 bt.

If

we

run

algorithm

1

with

stepsizes

τ1t

=

β γ+t

and

τ2t

=

18l2 β µ2 (γ +t)

for

some

2

β > 2/µ1 and γ > 0 such that τ11 ≤ min{1/L, µ22/18l2}, then we have

ν Pt ≤ γ + t , (12)

where ν :=

(L + l)β2 + 182l5β2/µ42 + 10Lβ2 σ2

max γP0,

.

10µ1β − 20

Remark 3. Note the rate is aﬀected by ν, and the ﬁrst term in the deﬁnition of ν is controlled by the initial
point. In practice, we can ﬁnd a good initial point by running Stoc-AGDA with constant stepsizes so that only the second term in the deﬁnition of ν matters. Then by choosing β = 3/µ1, we have ν = O µl521σµ242 . Thus, the convergence rate of Stoc-AGDA is O κ5µσt2 .

4 Stochastic variance reduced algorithm

In this section, we study the minimax problem in (2) with the ﬁnite-sum structure:

1n

min max f (x, y) =

xy

n

fi(x, y),

i=1

which arises ubiquitously in machine learning. We are especially interested in the case when n is large. We assume the overall objective function f (x, y) still satisﬁes the two-sided PL condition with µ1 and µ2, but we do not assume each fi to satisfy the two-sided PL condition. Instead of Assumption 1, we now assume each component fi has Lipschitz gradients.

Assumption 4. Each fi has l-Lipschitz gradients.

If we run AGDA with full gradients to solve the ﬁnite-sum minimax problem, the total complexity for ﬁnding an -optimal solution is O(nκ3 log(1/ )) by Theorem 3.2. Despite the linear convergence, the per-iteration cost is high and the complexity can be huge when the number of components n and condition number κ are large. Instead, if we run Stoc-AGDA, this leads to the total complexity O κµ52σ2 by Remark 3, which has worse dependence on .
Motivated by the recent success of stochastic variance reduced gradient (SVRG) technique [Johnson and Zhang, 2013, Reddi et al., 2016a, Palaniappan and Bach, 2016], we introduce the VR-AGDA algorithm (presented in Algorithm 2), that combines AGDA with SVRG so that the linear convergence is preserved while improving the dependency on n and κ. VR-AGDA can be viewed as the applying SVRG to AGDA with restarting: at every epoch k, we restart the SVRG subroutine (with T outer iterations, N inner steps) by initializing it with (xk, yk), which is randomly selected from previous SVRG subroutine. This is partly inspired by the GD-SVRG algorithm for minimizing PL functions [Reddi et al., 2016a]. Notice when T = 1, VR-AGDA reduces to a double-loop algorithm which is similar to the SVRG for saddle point problems proposed by Palaniappan and Bach [2016], except for several notable diﬀerences: (i) we are using the alternating updates rather than simultaneous updates, (ii) as a result, we require to sample two independent indices rather than one at each iteration, and (iii) most importantly, we are dealing with possibly nonconvex-nonconcave objectives that satisfy the two-sided PL condition.
The following two theorems capture the convergence of VR-AGDA.

8

Algorithm 2 VR-AGDA

1: input: (x˜0, y˜0), stepsizes τ1, τ2, iteration numbers N, T

2: for all k = 0, 1, 2, ... do

3: for all t = 0, 1, 2, ...T − 1 do

4:

xt,0 = x˜t, yt,0 = y˜t,

5:

compute

∇xf (x˜t, y˜t)

=

1 n

n i=1

∇x

fi

(x˜t

,

y˜t

)

6:

compute

∇yf (x˜t, y˜t)

=

1 n

n i=1

∇y

fi

(x˜t

,

y˜t

)

7:

for all j = 0 to N − 1 do

8:

sample i.i.d. indices i1j , i2j uniformly from [n]

9:

xt,j+1 = xt,j − τ1[∇xfi1 (xt,j , yt,j ) − ∇xfi1 (x˜t, y˜t) + ∇xf (x˜t, y˜t)]

j

j

10:

yt,j+1 = yt,j + τ2[∇yfi2 (xt,j+1, yt,j ) − ∇yfi2 (x˜t, y˜t) + ∇yf (x˜t, y˜t)]

j

j

11:

end for

12:

x˜t+1 = xt,N , y˜t+1 = yt,N

13: end for
14: choose (xk, yk) from {{(xt,j , yt,j )}jN=−01}Tt=−01 uniformly at random 15: x˜0 = xk, y˜0 = yk

16: end for

Theorem 4.1. Suppose Assumptions 2 and 4 hold and f (x, y) satisﬁes the two-sided PL condition with µ1 and µ2. Deﬁne Pk = ak + 210 bk, where ak = E[g(xk) − g∗] and bk = E[g(xk) − f (xk, yk)]. If we run VR-AGDA with τ1 = β/(28κ8l), τ2 = β/(lκ6), N = αβ−2/3κ9(2 + 4β1/2κ−3)−1 and T = 1, where α, β are constants irrelevant to l, n, µ1, µ2, then Pk+1 ≤ 12 Pk. This further implies a total complexity of
O (n + κ9) log(1/ )
for VR-AGDA to achieve an -optimal solution.
Theorem 4.2. Under the same assumptions in Theorem 4.1 and further assuming n ≤ κ9 , if we run VR-AGDA with τ1 = β/(28κ2ln2/3), τ2 = β/(ln2/3), N = αβ−2/3n(2 + 4β1/2n−1/3)−1 , and T = κ3n−1/3 , where α, β are constants irrelevant to l, n, µ1, µ2, then Pk+1 ≤ 12 Pk. This further implies a total complexity of
O n2/3κ3 log(1/ )
for VR-AGDA to achieve an -optimal solution.
Remark 4. Theorems 4.1 and 4.2 are diﬀerent in their choices of stepsizes and iteration numbers, which gives rise to diﬀerent complexities. Another diﬀerence is that Theorem 4.2 only works in the regime where the number of components n is not “too large” compared to the condition number, i.e., n ≤ κ9, which naturally guarantees T = κ3n−1/3 ≥ 1.
Remark 5. Since AGDA has complexity O nκ3 log(1/ ) , VR-AGDA with the setting in Theorem 4.1 is better than AGDA when n ≥ κ6. With the setting in Theorem 4.2, VR-AGDA outperforms AGDA as long as the assumption n ≤ κ9 holds. As a result of these two theorems, VR-AGDA always improves over AGDA. Furthermore, VR-AGDA with the second setting has a lower complexity than the ﬁrst setting in the regime n ≤ κ9, although the ﬁrst setting allows a simpler double-loop algorithm. Figure 3 summarizes the performance of VR-AGDA compared to AGDA in diﬀerent regimes of n and κ.

9

Complexity in

AGDA VR-AGDA1 VR-AGDA2
Regime
Figure 3: Comparison of complexities of AGDA and VR-AGDA, where VR-AGDA1, VR-AGDA2 correspond to the two settings in Theorems 4.1 and 4.2. In the regime n ≤ κ9, VR-AGDA2 performs best; in the regime n ≥ κ9, VR-AGDA1 performs best.

5 Experiments
In the introduction, we already presented the convergence results of AGDA on a two-dimensional nonconvexnonconcave function that satisﬁes the two-sided PL condition. In this section, we will present numerical experiments on machine learning applications: robust least square and imitation learning for linear quadratic regulators (LQR). Particularly, we focus on the comparison between AGDA, Stoc-AGDA, and VR-AGDA.

5.1 Robust least square

We consider the least square problems with coeﬃcient matrix A ∈ Rn×m and noisy vector y0 ∈ Rn. We assume that y0 is subject to bounded deterministic perturbation δ. Robust least square (RLS) minimizes the worst case residual, and can be formulated as [El Ghaoui and Lebret, 1997]:

min max Ax − y 2, where δ = y0 − y.
x δ: δ ≤ρ

We consider RLS with soft constraint:

min max F (x, y) :=

Ax − y

2 M

−

λ

y − y0

2M ,

(13)

xy

where we also adopt the general M-(semi-)norm in (13):

x

2 M

=

xT M x

and

M

is

positive

semi-deﬁnite.

F (x, y) satisﬁes the two-sided PL condition when λ > 1, because it can be written as the composition of

a strongly-convex-strongly-concave function and an aﬃne function (Example 2). However, F (x, y) is not

strongly convex about x, and when M is not full-rank, it is not strongly concave about y.

Datasets. We use three datasets in the experiments, and two of them are generated in the same way as in Du and Hu [2019]. We generate the ﬁrst dataset with n = 1000 and m = 500 by sampling rows of A from a Gaussian N (0, In) distribution and setting y0 = Ax∗ + with x∗ from Gaussian N (0, 1) and from Gaussian N (0, 0.01). We set M = In and λ = 3. The second dataset is the rescaled aquatic toxicity dataset by Cassotti et al. [2014], which uses 8 molecular descriptors of 546 chemicals to predict quantitative acute aquatic toxicity towards Daphnia Magna. We use M = I and λ = 2 for this dataset. The third dataset is generated with A ∈ R1000×500 from Gaussian N (0, Σ) where Σi,j = 2−|i−j|/10, M being rank-deﬁcit with positive eigenvalues sampled from [0.2, 1.8] and λ = 1.5. These three datasets represent cases with low, median, and high condition numbers, respectively.

10

105 105

100 100
100

10-5 10-10 10-15
0

AGDA Stoc-AGDA VR-AGDA

50

100

#grad/n

10-5

10-10

150

0

AGDA Stoc-AGDA VR-AGDA

20

40

60

#grad/n

10-10

AGDA Stoc-AGDA VR-AGDA

80

100

0

1000

2000

3000

4000

5000

#grad/n

105 100 10-5 10-10 10-15
0

(a) Dataset 1

AGDA Stoc-AGDA VR-AGDA

50

100

#grad/n

(b) Dataset 2
105
100

(c) Dataset 3
1010
105
100

10-5 10-10

AGDA Stoc-AGDA VR-AGDA

10-5 10-10

AGDA Stoc-AGDA VR-AGDA

150

0

20

40

60

80

100

#grad/n

0

1000

2000

3000

4000

5000

#grad/n

(d) Dataset 1

(e) Dataset 2

(f) Dataset 3

Figure 4: Comparison of the convergences of AGDA, Stoc-AGDA and SVRG-AGDA on three datasets based on two inaccuracy measures: (i) xt − x∗ 2 + yt − y∗ 2 ( as shown in the ﬁrst row), and (ii) Pt = (g(xt) − g∗) + (g(xt) − f (xt, yt)) (as shown in the
second row).

Evaluation. For each dataset, we compare three algorithms: AGDA, Stoc-AGDA, and VR-AGDA. We tune the stepsizes of all algorithms to achieve the best convergence. For Stoc-AGDA, we choose constant stepsizes to form a fair comparison with the other two. We report the potential function value, i.e., Pt described in our theorems, and distance to the limit point (xt, yt) − (x∗, y∗) 2. These errors are plotted against the number of gradient evaluations normalized by n (i.e., number of full gradients). Results are reported in Figure 4. We observe that VR-AGDA and AGDA both exhibit linear convergence, and the speedup of VR-AGDA is fairly signiﬁcant when the condition number is large, whereas Stoc-AGDA progresses fast at the beginning and stagnates later on. These numerical results clearly validate our theoretical ﬁndings.

5.2 Generative adversarial imitation learning for LQR

The optimal control problem for LQR can be formulated as:

minimize
πt
such that

∞
Ex0∼D xt Qxt + ut Rut
t=0
xt+1 = Axt + But, ut = πt(xt)

where xt ∈ Rd is a state, ut ∈ Rk is a control, D is the distribution of initial state x0, and πt is a policy. It is known that the optimal policy is linear: ut = −K∗xt, where K∗ ∈ Rk×d. If we parametrize the policy in the
linear form, ut = −Kxt, the problem can be written as:

min C(K; Q, R) := Ex0∼D
K

∞
xt Qxt + ut Rut
t=0

11

105 100 10-5 10-10 10-15 10-20
0

105
AGDA VR-AGDA
100

105
AGDA VR-AGDA
100

AGDA VR-AGDA

10-5

20

40

60

80

100

0

#grad/n

50

100

150

#grad/n

10-5

200

0

100

200

300

400

500

#grad/n

(a) d = 3, k = 2

(b) d = 20, k = 10

(c) d = 30, k = 20

Figure 5: AGDA and VR-AGDA on generative adversarial learning for LQR

where the trajectory is induced by LQR dynamics and policy K. In generative adversarial imitation learning for LQR, the trajectories induced by an expert policy KE are observed and part of the goal is to learn the cost function parameters Q and R from the expert. This can be formulated as a minimax problem [Cai et al., 2019]:
min max m(K, Q, R)
K (Q,R)∈Θ
where m(K, Q, R) := C(K; Q, R) − C(KE; Q, R) − Φ(Q, R), Θ = {(Q, R) : αQI Q βQI, αRI R βRI} and Φ is a strongly-convex regularizer. We sample n initial points x(01), x(02), ..., x(0n) from D and approximate C(K; Q, R) by sample average

1n Cn(K; Q, R) := n
i=1

∞
xt Qxt + ut Rut
t=0

.
x0 =x(0i)

We then consider

mn(K, Q, R) = Cn(K; Q, R) − Cn(KE; Q, R) − Φ(Q, R).

Note that mn satisﬁes the PL condition in terms of K [Fazel et al., 2018], and mn is strongly-concave in terms of (Q, R), so the function satisﬁes the two-sided PL condition.

In our experiment, we use Φ(Q, R) = λ( Q − Q¯ 2 + R − R¯ 2) for some Q¯, R¯ and λ = 1. We generate

three datasets with diﬀerent dimensions: (1) d = 3, k = 2; (2) d = 20, k = 10; (3) d = 30, k = 20. The initial

distribution D is N (0, Id) and we sample n = 100 initial points. The exact gradients can be computed based

on the compact forms established in Fazel et al. [2018], Cai et al. [2019]. We compare AGDA and VR-AGDA

under ﬁne-tuned stepsizes, and track their errors in terms of

Kt − K∗ 2 +

Qt − Q∗

2 F

+

Rt − R∗

2 F

.

The

result is reported in Figure 5, which again indicates that VR-AGDA signiﬁcantly outperforms AGDA.

6 Conclusion
In this paper, we identify a subclass of nonconvex-nonconcave minimax problems, represented by the the so-called two-side PL condition, for which AGDA and Stoc-AGDA can converge to global saddle points. We also propose the ﬁrst linearly-convergent variance-reduced AGDA algorithm that is provably always faster than AGDA, for this subclass of minimax problems . We hope this work can shed some light on the understanding of nonconvex-nonconcave minimax optimization: (1) diﬀerent learning rates for two players are essential in GDA algorithms with alternating updates; (2) convexity-concavity is not a watershed to
12

guarantee global convergence of GDA algorithms; (3) the complexity of solving minimax problems under PL conditions may have high-order dependence on the condition number in contrast to problems with strong convex-concavity conditions. It remains interesting to explore whether similar results apply to GDA algorithms with simultaneous updates and whether these algorithms can be further accelerated with momentum or catalyst schemes.
References
Jacob Abernethy, Kevin A Lai, and Andre Wibisono. Last-iterate convergence rates for min-max optimization. arXiv preprint arXiv:1906.02027, 2019.
James P Bailey, Gauthier Gidel, and Georgios Piliouras. Finite regret and cycles with ﬁxed step-size via alternating gradient descent-ascent. arXiv preprint arXiv:1907.04392, 2019.
Qi Cai, Mingyi Hong, Yongxin Chen, and Zhaoran Wang. On the global convergence of imitation learning: A case for linear quadratic regulator. arXiv preprint arXiv:1901.03674, 2019.
Matteo Cassotti, Davide Ballabio, Viviana Consonni, Andrea Mauri, Igor V Tetko, and Roberto Todeschini. Prediction of acute aquatic toxicity toward daphnia magna by using the ga-k nn method. Alternatives to Laboratory Animals, 42(1):31–41, 2014.
Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-convex objectives. In Advances in Neural Information Processing Systems, pages 4705–4714, 2017.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions via dual embeddings. In Artiﬁcial Intelligence and Statistics, pages 1458–1467, 2017.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. SBEED: Convergent reinforcement learning with nonlinear function approximation. 80:1125–1134, 10–15 Jul 2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. In International Conference on Learning Representations (ICLR 2018), 2018.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of deep neural networks. In International Conference on Machine Learning, pages 1675–1685, 2019.
Simon S Du and Wei Hu. Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 196–205, 2019.
Laurent El Ghaoui and Herv´e Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal on matrix analysis and applications, 18(4):1035–1064, 1997.
Francisco Facchinei and Jong-Shi Pang. Finite-dimensional variational inequalities and complementarity problems. Springer Science & Business Media, 2007.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Conference on Machine Learning, pages 1467–1476, 2018.
13

Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1802–1811, 2019.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? arXiv preprint arXiv:1902.00618, 2019.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315–323, 2013.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795–811. Springer, 2016.
GM Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Matecon, 12: 747–756, 1976.
Qihang Lin, Mingrui Liu, Hassan Raﬁque, and Tianbao Yang. Solving weakly-convex-weakly-concave saddlepoint problems as successive strongly monotone variational inequalities. arXiv preprint arXiv:1810.10207, 2018.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.
Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization. arXiv preprint arXiv:2002.02417, 2020.
Luo Luo, Cheng Chen, Yujun Li, Guangzeng Xie, and Zhihua Zhang. A stochastic proximal point algorithm for saddle-point problems. arXiv preprint arXiv:1909.06946, 2019.
Luo Luo, Haishan Ye, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvexstrongly-concave minimax problems. arXiv preprint arXiv:2001.03724, 2020.
Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent methods: a general approach. Annals of Operations Research, 46(1):157–178, 1993.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Eric Mazumdar and Lillian J Ratliﬀ. On the convergence of gradient-based learning in continuous games. ArXiv e-prints, 2018.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International Conference on Machine Learning, pages 3481–3490, 2018.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Advances in Neural Information Processing Systems, pages 2208–2216, 2016.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems, pages 2971–2980, 2017.
14

John Nash. Two-person cooperative games. Econometrica: Journal of the Econometric Society, pages 128–140, 1953.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of ﬁrst order methods for non-strongly convex optimization. Mathematical Programming, pages 1–39, 2018.
Yurii Nesterov and Laura Scrimali. Solving strongly monotone variational and quasi-variational inequalities. Available at SSRN 970903, 2006.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative ﬁrst order methods. In Advances in Neural Information Processing Systems, pages 14905–14916, 2019.
Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. In Advances in Neural Information Processing Systems, pages 1416–1424, 2016.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 3(4):643–653, 1963.
Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over multiple domains. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4739–4746, 2019.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314–323, 2016a.
Sashank J Reddi, Suvrit Sra, Barnab´as P´oczos, and Alex Smola. Fast incremental method for smooth nonconvex optimization. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 1971–1977. IEEE, 2016b.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with principled adversarial training. stat, 1050:29, 2017.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational Mathematics, 18(5):1131–1198, 2018.
Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Eﬃcient algorithms for smooth minimax optimization. In Advances in Neural Information Processing Systems, pages 12659–12670, 2019.
John Von Neumann, Oskar Morgenstern, and Harold William Kuhn. Theory of games and economic behavior (commemorative edition). Princeton university press, 2007.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.
Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In Advances in neural information processing systems, pages 1537–1544, 2005.
Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under weaker conditions. arXiv preprint arXiv:1303.4645, 2013.
Yi Zhou, Huishuai Zhang, and Yingbin Liang. Geometrical properties and accelerated gradient solvers of non-convex phase retrieval. In 2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 331–335. IEEE, 2016.
15

Appendix

A Proofs for Section 2

We ﬁrst present several key lemmas.
Lemma A.1 (Karimi et al. [2016]). If f (·) is l-smooth and it satisﬁes PL with constant µ, then it also satisﬁes error bound (EB) condition with µ, i.e.
∇f (x) ≥ µ xp − x , ∀x,
where xp is the projection of x onto the optimal set, also it satisﬁes quadratic growth (QG) condition with µ, i.e.
f (x) − f ∗ ≥ µ xp − x 2, ∀x. 2
Conversely, if f (·) is l-smooth and it satisﬁes EB with constant µ, then it satisﬁes PL with constant µ/l.

From the above lemma, we easily derive that l ≥ µ.
Lemma A.2 (Nouiehed et al. [2019]). In the minimax problem, when −f (x, ·) satisﬁes PL condition with constant µ2 for any x and f satisﬁes Assumption 1, then the function g(x) := maxy f (x, y) is L-smooth with L := l + l2/µ2 and ∇g(x) = ∇xf (x, y∗(x)) for any y∗(x) ∈ arg maxy f (x, y).
Lemma A.3. In the minimax problem 1, when the objective function f satisﬁes Assumption 1 (Lipschitz gradient) and the two-sided PL condition with constant µ1 and µ2, then function g(x) := maxy f (x, y) satisﬁes the PL condition with µ1.

Proof. From Lemma A.2,

∇g(x) 2 = ∇xf (x, y∗(x)) 2.

Since f (·, y) satisﬁes PL condition with constant µ1, we get

∇g(x) 2 ≥ 2µ1[f (x, y∗(x)) − min f (x , y∗(x))].

(14)

x

Also,

f (x , y∗(x)) ≤ max f (x , y) =⇒ min f (x , y∗(x)) ≤ min max f (x , y) = g∗.

(15)

y

x

xy

Combining equation (14) and (15), we obtain,

∇g(x) 2 ≥ 2µ1(g(x) − g∗).

The following lemma states that stochastic gradient descent converges linearly to the neighbourhood of the optimal set under PL condition. The proof is based on [Karimi et al., 2016]. Lemma A.4. Consider the optimization problem minx f (x) = E[F (x; ξ)], where f is l-smooth and satisﬁes PL condition with constant µ. Using the stochastic gradient descent with step size τ ≤ 1/l,
xt+1 = xt − τ G(xt, ξt),
16

where then we have

E[G(x, ξ) − ∇f (x)] = 0, E[ G(x, ξ) − ∇f (x) 2] ≤ σ2, E[f (xt+1) − f ∗] ≤ (1 − µτ )E[f (xt) − f ∗] + lτ 2 σ2. 2

Proof. By smoothness of f we have

f (xt+1) − f ∗ ≤ f (xt) +

∇f (xt), xt+1 − xt

l +

xt+1 − x 2 − f ∗

2

= f (xt) − τ ∇f (xt), G(xt, ξt) + lτ 2 G(xt, ξt) 2 − f ∗. 2

Taking expectation of both sides, we get

E[f (xt+1) − f ∗] ≤E[f (xt) − f ∗] − τ E[ ∇f (xt) 2] + lτ 2 E[ G(xt, ξt) 2] 2

=E[f (xt) − f ∗] − τ E[ ∇f (xt) 2] + lτ 2 E[ ∇f (xt) 2] 2

+ lτ 2 E[ ∇f (xt) − G(xt, ξt) 2] 2

≤E[f (xt) − f ∗] − τ E[ ∇f (xt) 2] + lτ 2 σ2

2

2

≤(1 − µτ )E[f (xt) − f ∗] + lτ 2 σ2, 2

where in the equality we use E[G(xt, ξt)] = ∇f (xt), in the second inequality we use τ ≤ 1/l, and we use PL condition in the last inequality.

Proof for Lemma 2.1.

Proof. • (stationary point) =⇒ (saddle point): From the deﬁnition of PL condition, if (x∗, y∗) is a stationary point,

max f (x∗, y) − f (x∗, y∗) ≤ 1

y

2µ2

f (x∗, y∗) − min f (x, y∗) ≤ 1

x

2µ1

∇yf (x∗, y∗) 2 = 0, ∇xf (x∗, y∗) 2 = 0,

so maxy f (x∗, y) = f (x∗, y∗) = minx f (x, y∗), and therefore f (x∗, y∗) is a saddle point.

• (saddle point) =⇒ (global minimax point): Follow from deﬁnitions.

• (global minimax point) =⇒ (stationary point): If (x∗, y∗) is a global minimax point, then by deﬁnition,

y∗ ∈ arg max f (x∗, y∗), x∗ ∈ arg min g(x),

y

x

Then by ﬁrst order necessary condition, we have,

∇yf (x∗, y∗) = 0, ∇g(x∗) = 0,

17

Further with Lemma A.2, Thus, (x∗, y∗) is a stationary point.

∇g(x∗) = ∇xf (x∗, y∗) = 0

Proposition 1. The function
f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y,
satisﬁes the two-sided PL condition with µ1 = 1/16, µ2 = 1/14.
Proof. It is not hard to derive that arg minx f (x, y) = 0, ∀y, and arg maxy f (x, y) = 0, ∀x, i.e. x∗(y) = y∗(x) = 0, ∀x, y. Therefore, (0, 0) is the only saddle point. Then compute the gradients:
∇xf (x, y) = 2x + 3 sin2(y) sin(2x), ∇yf (x, y) = −8y + 3 sin2(x) sin(2y) − 10 sin(2y).
and
|∇2xf (x, y)| = |2 + 6 sin2(y) cos(2x)| ≤ 8, |∇2yf (x, y)| = | − 8 + 6 sin2(x) cos(2y) − 20 cos(2y)| ≤ 28.
so f (·, y) is L1-smooth with L1 = 8 for any x and f (x, ·) is L2-smooth with L2 = 28 for any y. Then note that:
|∇xf (x, y)| |∇xf (x, y)| |2x + 3 sin2(y) sin(2x)| 1 |x − x∗(y)| = |x| = |x| ≥ 2 , |∇yf (x, y)| |∇yf (x, y)| | − 8y + 3 sin2(x) sin(2y) − 10 sin(2y)| |y − y∗(x)| = |y| = |y| ≥ 2 So f (·, y) satisﬁes EB with µEB1 = 1/2, and -f (x, ·) satisﬁes EB with µEB2 = 2. By Lemma A.1, we have f (·, y) satisﬁes PL with constant µ1 = 1/16 and -f (x, ·) satisﬁes PL with constant µ1 = 1/14.

B Proofs for Section 3

Before we step into proofs for Theorem 3.1, 3.2 and 3.3, we ﬁrst present a contraction theorem for each iteration.

Theorem B.1. Assume Assumption 1, 2, 3 hold and f (x, y) satisﬁes the two-sided PL condition with µ1 and µ2. Deﬁne at = E[g(xt) − g∗] and bt = E[g(xt) − f (xt, yt)]. If we run one iteration of Algorithm 1 with τ1t = τ1 ≤ 1/L (L is speciﬁed in Lemma A.2) and τ2t = τ2 ≤ 1/l, then
at+1 + λbt+1 ≤ max{k1, k2}(at + λbt) + λ(1 − µ2τ2) L 2+ l τ12σ2 + 2l λτ22σ2 + L2 τ12σ2,
where

k1 := 1 − µ1 τ1 + λ(1 − µ2τ2)τ1 − λ(1 + β)(1 − µ2τ2)(2τ1 + lτ12) ,

(16)

l2τ1

l2

1

l2

2

k2 := 1 − µ2τ2 + µ2λ + (1 − µ2τ2) µ2 τ1 + (1 + β )(1 − µ2τ2) µ2 (2τ1 + lτ1 ),

(17)

and λ, β > 0 such that k1 ≤ 1.

18

Proof. Because g is L-smooth by Lemma A.2, we have

g(xt+1) − g∗ ≤g(xt) − g∗ +

∇g(xt), xt+1 − xt

L +

xt+1 − xt

2

2

=g(xt) − g∗ − τ1 ∇g(xt), Gx(xt, yt, ξt1) + L2 τ12 Gx(xt, yt, ξt1) 2.

Taking expectation of both side and use Assumption 3, we get

E[g(xt+1) − g∗] ≤E[g(xt) − g∗] − τ1E[ ∇g(xt), ∇xf (xt, yt) ] + L2 τ12E[ Gx(xt, yt, ξt1) 2] ≤E[g(xt) − g∗] − τ1E[ ∇g(xt), ∇xf (xt, yt) ] + L2 τ12E[ ∇xf (xt, yt) 2] + L2 τ12σ2 ≤E[g(xt) − g∗] − τ1E[ ∇g(xt), ∇xf (xt, yt) ] + τ21 E[ ∇xf (xt, yt) 2] + L2 τ12σ2 ≤E[g(xt) − g∗] − τ21 E ∇g(xt) 2 + τ21 E ∇xf (xt, yt) − ∇g(xt) 2 + L2 τ12σ2, (18)

where in the second inequality we use Assumption 3, and in the third inequality we use τ1 ≤ 1/L. Because −f (xt+1, y) is l-smooth and µ1-PL, by Lemma A.4, when τ1 ≤ 1/l we have

E[g(xt+1) − f (xt+1, yt+1)] ≤ (1 − µ2τ2)E[g(xt+1) − f (xt+1, yt)] + 2l τ22σ2 ≤ (1 − µ2τ2)E[g(xt) − f (xt, yt) + f (xt, yt) − f (xt+1, yt) + g(xt+1) − g(xt)] + 2l τ22σ2 (19)

Because of lipschitz continuity of the gradient, we can bound f (xt, yt) − f (xt+1, yt) as

f (xt, yt) − f (xt+1, yt) ≤ − ∇xf (xt, yt), xt+1 − xt

l +

xt+1 − xt 2

2

≤ τ1 ∇xf (xt, yt), Gx(xt, yt, ξt1) + 2l τ12 Gx(xt, yt, ξt1) 2.

Taking expectation of both side and use Assumption 3,

E[f (xt, yt) − f (xt+1, yt)] ≤ (τ1 + 2l τ12)E ∇xf (xt, yt) 2 + 2l τ12σ2. (20) Also from (18) ,

E[g(xt+1) − g(xt)] ≤ − τ21 E ∇g(xt) 2 + τ21 E ∇xf (xt, yt) − ∇g(xt) 2 + L2 τ12σ2. (21) Combining (19), (20) and (21),

E[g(xt+1) − f (xt+1, yt+1)] ≤(1 − µ2τ2)E[g(xt) − f (xt, yt)] + (1 − µ2τ2)(τ1 + 2l τ12)E ∇xf (xt, yt) 2−

(1 − µ2τ2) τ1 E ∇g(xt) 2 + (1 − µ2τ2) τ1 E ∇xf (xt, yt) − ∇g(xt) 2+

2

2

(1 − µ2τ2) L 2+ l τ12σ2 + 2l τ22σ2. (22)

19

Combining (18) and (22), we have for ∀λ > 0

at+1 + λbt+1 ≤at − τ1 + λ(1 − µ2τ1) τ1 E ∇g(xt) 2 + λ(1 − µ2τ2)bt+

2

2

τ1 + λ(1 − µ2τ2) τ1 E ∇xf (xt, yt) − ∇g(xt) 2 + λ(1 − µ2τ2)

2

2

τ1 + 2l τ12

E ∇xf (xt, yt) 2+

λ(1 − µ2τ2) L 2+ l τ12σ2 + 2l λτ22σ2 + L2 τ12σ2

≤at − τ1 + λ(1 − µ2τ1) τ1 − λ(1 + β)(1 − µ2τ2)

2

2

τ1 + 2l τ12

E ∇g(xt) 2+

λ(1 − µ2τ2)bt + τ21 + λ(1 − µ2τ2) τ21 + λ 1 + β1 (1 − µ2τ2) τ1 + 2l τ12 E ∇xf (xt, yt) − ∇g(xt) 2+

λ(1 − µ2τ2) L 2+ l τ12σ2 + 2l λτ22σ2 + L2 τ12σ2, (23)

where in the second inequality we use Young’s Inequality and β > 0. Now it suﬃces to bound g(xt) 2 and ∇xf (xt, yt) − ∇g(xt) 2 by at and bt. With Lemma A.2, we have:

∇xf (xt, yt) − ∇g(xt) 2 = ∇xf (xt, yt) − ∇xf (xt, y∗(xt)) 2 ≤ l2 y∗(xt) − yt 2,

(24)

for any y∗(xt) ∈ arg maxy f (xt, y). Now we ﬁx y∗(xt) to be the projection of yt on the the set arg maxy f (xt, y). Because −f (xt, ·) satisﬁes PL condition with µ2, and Lemma A.1 therefore indicates it also satisﬁes quadratic growth condition with µ2, i.e.

y∗(xt) − yt

2≤

2 [g(xt) − f (xt, yt)],

(25)

µ2

along with (24), we get ∇xf (xt, yt) − ∇g(xt) 2 ≤ 2l2 [g(xt) − f (xt, yt)]. (26) µ2
Because g satisﬁes PL condition with µ1 by Lemma A.3,

∇g(xt) 2 ≥ 2µ1[g(xt) − g∗].

(27)

Plug (26) and (27) into (23), we can get

at+1 + λbt+1 ≤ 1 − µ1 τ1 + λ(1 − µ2τ2)τ1 − λ(1 + β)(1 − µ2τ2)(2τ1 + lτ12) at+

l2τ1

l2

1

l2

2

λ 1 − µ2τ2 + µ2λ + (1 − µ2τ2) µ2 τ1 + (1 + β )(1 − µ2τ2) µ2 (2τ1 + lτ1 ) bt+

λ(1 − µ2τ2) L 2+ l τ12σ2 + 2l λτ22σ2 + L2 τ12σ2. (28)

Proof of Theorem 3.1
Proof. In the setting of Theorem 1, τ1t = τ1 and τ2t = τ2, ∀t. By Thoerem B.1, We only need to choose τ1, τ2, λ and β to let k1, k2 < 1. Here we ﬁrst choose β = 1 and λ = 1/10. Then
k1 =1 − µ1 τ1 + λ(1 − µ2τ2)τ1 − λ(1 + β)(1 − µ2τ2)(2τ1 + lτ12) 1
≤1 − µ1 τ1 − λ(1 − µ2τ2)τ1[(1 + β)(2 + lτ1) − 1] ≤ 1 − 2 τ1µ1, (29)
20

where in the last inequality we just plug in β and λ and use lτ1 ≤ 1. Also,

l2τ1

l2

1

l2

2

k2 =1 − µ2τ2 + µ2λ + (1 − µ2τ2) µ2 τ1 + (1 + β )(1 − µ2τ2) µ2 (2τ1 + lτ1 )

≤1 − l2τ1 µ2

µ22τ2 1

1

τ1l2 − λ − (1 − µ2τ2) 1 +

1+ β

(2 + lτ1)

≤1 − l2τ1 , (30) µ2

where in the last inequality we plug in β and λ and we use µτ122lτ22 ≤ 18 by our choice of τ1. Note that 12 τ1µ1 < l2µτ21 , because 12 τ1µ1 / l2µτ21 = µ21lµ22 < 1. Deﬁne Pt := at + 110 bt, and by Theorem B.1,

Pt+1 ≤ 1 − 1 τ1µ1 Pt + (1 − µ2τ2)(L + l)τ12 σ2 + lτ22 σ2 + Lτ12 σ2.

2

20

20

2

With some simple computation,

Pt ≤ (1 − 1 µ1τ1)tP0 + (1 − µ2τ2)(L + l)τ12 + lτ22 + 10Lτ12 σ2.

2

10µ1τ1

We verify that τ1 ≤ 1/L by noting: τ1 ≤ µ1228τl22 ≤ 1µ822l3 ≤ 2µl22 and L = l + µl22 ≤ 2µl22 .

Proof of Theorem 3.2

Proof. The ﬁrst part of Theorem 3.2 is a direct corollary of Theorem 3.1 by setting σ = 0. We show the second part by noting that

xt+1 − xt 2 = τ12 ∇xf (xt, yt) 2 , and yt+1 − yt 2 = τ22 ∇yf (xt+1, yt) 2 .

(31)

Also,

∇yf (xt+1, yt) 2 ≤ ∇yf (xt, yt) 2 + ∇yf (xt+1, yt) − ∇yf (xt, yt) 2

≤ ∇yf (xt, yt) − ∇yf (xt, y∗(xt)) 2 + l2 xt+1 − xt 2

≤l2 yt − y∗(xt) 2 + l2 xt+1 − xt 2

2l2

2

2 2l2

22

2

≤ µ2 bt + l xt+1 − xt = µ2 bt + l τ1 ∇xf (xt, yt) ,

(32)

where in the second inequality y∗(xt) is the projection of yt on the the set arg maxy f (xt, y) and ∇yf (xt, y∗(xt)) = 0, in the third inequality we use lipschtiz continuity of gradient, and in the last in-
equality we use quadratic growth condition. Also,

∇xf (xt, yt) 2 ≤ ∇g(xt) 2 + ∇xf (xt, yt) − ∇g(xt) 2

= ∇g(xt) − ∇g(x∗) 2 + ∇xf (xt, yt) − ∇g(xt) 2

≤L2 xt − x∗ 2 + l2 y∗(xt) − yt 2

2L2

2l2

≤ µ1 at + µ2 bt, (33)

21

where in the ﬁrst equality x∗ is the projection of xt on the set arg minx g(x) and ∇g(x∗) = 0, in the second inequality y∗(xt) is the projection of yt on the the set arg maxy f (xt, y) and ∇g(xt) = ∇xf (xt, yt), and in
the last inequality we use quadratic growth condition. Therefore with (32) and (33),

xt − x∗ 2 +

yt − y∗

2 ≤τ12 ∇xf (xt, yt) 2 + τ22 ∇yf (xt+1, yt) 2

22 2

2 2l2 2

≤(1 + τ2 l )τ1 ∇xf (xt, yt) + µ2 τ2 bt

≤ 2(1 + τ22l2)τ12L2 at + 2(1 + τ22l2)τ12l2 + 2l2τ22 bt

µ1

µ2

≤ 2(1 + τ22l2)τ12L2 + 20(1 + τ22l2)τ12l2 + 20l2τ22

µ1

µ2

P0ct,

where c = 1 − µ316µl322 . Letting α1 =

2(1+τ22µl12)τ12L2 + 20(1+τ22l2)µτ212l2+20l2τ22 P0, we have √
xt+1 − xt + yt+1 − yt ≤ 2α1ct/2.

For n ≥ t,

n−1

√∞

√2α ct/2

xn − xt + yn − yt ≤

xi+1 − xi + yi+1 − yi ≤ 2α1 ci/2 ≤

1√ ,

1− c

i=t

i=t

so {(xt, yt)}t converges and by ﬁrst part of this theorem the limit (x∗, y∗) must be a saddle point. Thus we

have

xt − x∗ 2 + yt − y∗ 2 ≤ 2α√1 ct = αctP0, (1 − c)2

with α = 2

+ 2(1+τ22 l2 )τ12 L2
µ1

20(1+τ22 l2 )τ12 l2 +20l2 τ22 µ2

/(1 − √c)2.

Proof of Theorem 3.3

Proof. First note that since τ t ≤ µ2/18l2, τ t = 18l2β = 18l2τ1t ≤ 1 . Similar to the proof of Theorem 3.1,

1

2

2 µ22(γ+t)

µ22

l

by choosing β = 1 and λ = 1/10 in the Theorem B.1, we have min{k1, k2} = 12 µ1τ1t. We prove the theorem

by induction. When t = 1, it is naturally satisﬁed by deﬁnition of ν. We assume that Pt ≤ γ+ν t . Then by

Theorem B.1,

Pt+1 ≤ 1 − 12 µ1τ1 Pt + λ(1 − µ2τ2t) L 2+ l (τ1t)2σ2 + 2l λ(τ2t)2σ2 + L2 (τ1t)2σ2

γ + t − 12 µ1β ν

(L + l)β2

182l5β2

Lβ2

2

≤

γ+t

+ γ+t

20(γ + t)2 + 20µ4(γ + t)2 + 2(γ + t)2

σ

2

γ+t−1

1 2

µ1β

−

1

(L + l)β2

182l5β2

Lβ2

2

≤ (γ + t)2 ν − (γ + t)2 ν + 20(γ + t)2 + 20µ4(γ + t)2 + 2(γ + t)2 σ

(34)

2

ν

≤

,

γ+t+1

where in the second inequality we plug in τ1t and τ2t, in the last inequality we use (γ + t + 1)(γ + t − 1) ≤ (γ + t)2 and the fact that sum of last two terms in (34) is no greater than 0 by our choice of ν.

22

C Proofs for Section 4

Proof of Theorem 4.1

Proof. Because the proof is long, we break the proof into three parts for the convenience of understanding the intuition behind it.
Part 1.
Consider in one outer loop k. Deﬁne at,j = E[g(xt,j)−g∗], bt,j = E[g(xt,j)−f (xt,j, yt,j)], a˜t = E[g(x˜t)−g∗] and ˜bt = E[g(x˜t) − f (x˜t, y˜t)]. We omit the subscript t for now. We denote the stochastic gradients as

Gx(xj, yj) = ∇xfij (xj, yj) − ∇xfij (x˜, y˜) + ∇xf (x˜, y˜), Gy(xj , yj+1) = ∇yfij (xj+1, yj ) − ∇yfij (x˜, y˜) + ∇yf (x˜, y˜).

Note that these are unbiased stochastic gradients. Similar to the proof of Theorem B.1 (replace σ2 in (18) ), with τ1 ≤ 1/L, we have
aj+1 ≤ aj − τ21 E ∇g(xj) 2 + τ21 E ∇xf (xj, yj) − ∇g(xj) 2 + L2 τ12E Gx(xj, yj) − ∇xf (xj, yj) 2 (35) By Lemma A.4, with τ2 ≤ 1/l,
bj+1 ≤ E[g(xj+1) − f (xj+1, yj )] − τ22 E ∇yf (xj+1, yj ) 2 + 2l τ22E Gy(xj+1, yj ) − ∇yf (xj+1, yj ) 2 (36)

Furthermore, we bound the distance to the x˜ = x0 as

E xj+1 − x˜ 2 = E xj − τ1Gx(xj , yj ) − x˜ 2

= E xj − x˜ 2 + 2E xj − x˜, τ1∇xf (xj, yj) + τ12E ∇xf (xj, yj) 2 + τ12E Gx(xj, yj) − ∇xf (xj, yj) 2

≤ (1 + τ1β1)E xj − x˜ 2 +

τ 2 + τ1 1 β1

E ∇xf (xj , yj ) 2 + τ12E Gx(xj , yj ) − ∇xf (xj , yj ) 2, (37)

where in the last inequality we use Young’s inequality to the inner product and β1 > 0 is a constant which we will determine later. Similarly,

E yj+1 − y˜ 2 ≤ (1 + τ2β2)E yj − y˜ 2 + τ 2 + τ2 2 β2

E ∇yf (xj+1, yj ) 2 + τ22E Gy(xj+1, yj ) − ∇yf (xj+1, yj ) 2 (38)

where in the last inequality we use Young’s inequality to the inner product and β2 > 0 is a constant. We are

going to construct a potential function

Rj = aj + λbj + cj xj − x˜ 2 + dj yj − y˜ 2,

(39)

and we will determine λ, cj and dj later. Combine (35), (36) and (38),

Rj+1 ≤aj − τ21 E ∇g(xj) 2 + τ21 E ∇xf (xj, yj) − ∇g(xj) 2 + L2 τ12E Gx(xj, yj) − ∇xf (xj, yj) 2+

λE[g(xj+1) − f (xj+1, yj)] −

λτ2 E

∇yf (xj+1, yj )

2+

2

cj+1E xj+1 − x˜ 2 +

λl dj+1 + 2

τ22E Gy(xj+1, yj ) − ∇yf (xj+1, yj ) 2+

dj+1(1 + τ2β2)E yj − y˜ 2 + dj+1

τ 2 + τ2 2 β2

E ∇yf (xj+1, yj ) 2

(40)

23

Then we bound the variance of the stochastic gradients,

E Gy(xj+1, yj ) − ∇yf (xj+1, yj ) 2 = E ∇yfij (xj+1, yj ) − ∇yfij (x˜, y˜) + ∇yf (x˜, y˜) − ∇yf (xj+1, yj ) 2 ≤ E ∇yfij (xj+1, yj ) − ∇yfij (x˜, y˜) 2 ≤ l2E xj+1 − x˜ 2 + l2E yj − y˜ 2 (41)

where in the ﬁrst inequality we use E[∇yfij (xj+1, yj) − ∇yfij (x˜, y˜)] = ∇yf (xj+1, yj) − ∇yf (x˜, y˜). Similarly,

E Gx(xj, yj) − ∇xf (xj, yj) 2 ≤ l2E xj − x˜ 2 + l2E yj − y˜ 2.

(42)

Plugging (41) into (40),

Rj+1 ≤aj − τ21 E ∇g(xj) 2 + τ21 E ∇xf (xj, yj) − ∇g(xj) 2 + L2 τ12E Gx(xj, yj) − ∇xf (xj, yj) 2+

λE[g(xj+1) − f (xj+1, yj)] −

λτ2 E

∇yf (xj+1, yj )

2+

2

cj+1 +

λl dj+1 + 2

l2τ22 E xj+1 − x˜ 2+

dj+1(1 + τ2β2) +

λl dj+1 + 2

l2τ22 E yj − y˜ 2 + dj+1

τ 2 + τ2 2 β2

E ∇yf (xj+1, yj ) 2.

(43)

Then we plug in (37) and rearrange,

Rj+1 ≤aj − τ21 E ∇g(xj ) 2 + cj+1 + dj+1 + λ2l l2τ22

τ 2 + τ1 1 β1

E

∇xf (xj, yj)

2+

τ1 E

∇xf (xj, yj) − ∇g(xj)

2+

2

λE[g(xj+1) − f (xj+1, yj )] − λτ2 − dj+1 τ 2 + τ2

2

2 β2

E ∇yf (xj+1, yj ) 2+

cj+1 + dj+1 + λ2l l2τ22 (1 + τ1β1)E xj − x˜ 2 + dj+1(1 + τ2β2) + dj+1 + λ2l l2τ22 E yj − y˜ 2+

L2 + cj+1 + dj+1 + λ2l l2τ22 τ12E Gx(xj , yj ) − ∇xf (xj , yj ) 2 (44)

Consider the second line. Using PL condition ∇yf (xj+1, yj) 2 ≥ 2µ2[g(xj+1) − f (xj+1, yj)] and assuming λ ≥ dj+1(τ2 + 1/β2), which we will justify later by our choices of dj+1 and β2, we have

λ the second line ≤λ 1 − τ2µ2 + dj+1

τ 2 + τ2

µ2 E[g(xj+1) − f (xj+1, yj )]

2

2 β2

λ ≤λ 1 − τ2µ2 + dj+1

τ 2 + τ2

µ2

2

2 β2

bj + E f (xj , yj ) − f (xj+1, yj ) + (aj+1 − aj )

λ ≤λ 1 − τ2µ2 + dj+1

τ 2 + τ2

µ2

2

2 β2

bj + τ1 + 2l τ12 E ∇xf (xj , yj ) 2+

2l τ12E Gx(xj, yj) − ∇xf (xj, yj 2 − τ21 E ∇g(xj) 2+ τ21 E ∇xf (xj, yj) − ∇g(xj) 2 + L2 τ12E Gx(xj, yj) − ∇xf (xj, yj) 2

24

where in the last inequality we use (35) and (20). Now we plug this into Rj+1,

Rj+1

≤aj − τ1 (1 + λζ)E ∇g(xj) 2 + 2

λl cj+1 + dj+1 + 2

τ1 (1 + λζ)E ∇xf (xj, yj) − ∇g(xj) 2 + λζbj+ 2

cj+1 + dj+1 + λ2l l2τ22 (1 + τ1β1)E xj − x˜ 2 +

l2τ22

τ 2 + τ1 1 β1

dj+1(1 + τ2β2) +

+ λζ τ1 + 2l τ12 dj+1 + λ2l l2τ22

L2 + cj+1 + dj+1 + λ2l l2τ22 + λζ L 2+ l τ12E Gx(xj , yj ) − ∇xf (xj , yj ) 2,

E ∇xf (xj, yj) 2+
E yj − y˜ 2+ (45)

where we deﬁne ζ = 1 − τ2µ2 + λ2 dj+1 2 ∇g(xj) − ∇xf (xj, yj) 2,

τ22 + βτ22

µ2 and ψ = 1 − ζ. With

∇xf (xj, yj) 2 ≤ 2 ∇g(xj) 2 +

Rj+1 ≤aj − τ21 (1 + λζ) − 2 cj+1 + dj+1 + λ2l l2τ22

τ 2 + τ1 1 β1

− 2λζ

τ1 + 2l τ12

E ∇g(xj) 2+

λζbj + τ21 (1 + λζ) + 2 cj+1 + dj+1 + λ2l l2τ22

τ 2 + τ1 1 β1

− 2λζ

τ1 + 2l τ12

E ∇xf (xj, yj) − ∇g(xj) 2+

cj+1 + dj+1 + λ2l l2τ22 (1 + τ1β1)E xj − x˜ 2 + dj+1(1 + τ2β2) + dj+1 + λ2l l2τ22 E yj − y˜ 2+

L2 + cj+1 + dj+1 + λ2l l2τ22 + λζ L 2+ l τ12E Gx(xj, yj) − ∇xf (xj, yj) 2. (46)

Then plugging in (26), (27) and (42), we get

Rj+1 ≤aj − τ1(1 + λζ) − 4 cj+1 + dj+1 + λ2l l2τ22

τ 2 + τ1 1 β1

− 4λζ

τ1 + 2l τ12

µ1aj +

1

l2τ1

4l2

λl 2 2 2 τ1

4l2

l2

λbj − λ λ λψ − µ2 (1 + λζ) − µ2 cj+1 + dj+1 + 2 l τ2

τ1 + β1

− λζ µ2

τ1 + 2 τ1

bj +

cj+1 + dj+1 + λ2l l2τ22 (1 + τ1β1) + L2 + cj+1 + dj+1 + λ2l l2τ22 + λζ L 2+ l τ12l2 E xj − x˜ 2+

dj+1(1 + τ2β2) + dj+1 + λ2l l2τ22 + L2 + cj+1 + dj+1 + λ2l l2τ22 + λζ L 2+ l τ12l2 E yj − y˜ 2.

(47)

Now we are ready to deﬁne sequences {cj}j and {dj}j. Let cN = dN = 0, and

cj = dj =

cj+1 +

λl dj+1 + 2

l2τ22 (1 + τ1β1) +

dj+1(1 + τ2β2) +

λl dj+1 + 2

l2τ22 +

L 2 + cj+1 + L 2 + cj+1 +

λl dj+1 + 2
λl dj+1 + 2

l2τ22 + λζ L 2+ l l2τ22 + λζ L 2+ l

τ12l2, τ12l2.

We further deﬁne

m1j :=τ1(1 + λζ) − 4 cj+1 + dj+1 + λ2l l2τ22

τ 2 + τ1 − 4λζ τ1 + l τ 2 ,

1 β1

21

21

l2τ1

4l2

λl 2 2

mj

:= λ

λψ − (1 + λζ) −

µ2

µ2

cj+1 +

dj+1 + 2

l τ2

τ 2 + τ1 1 β1

4l2 − λζ
µ2

τ1 + 2l τ12

(48) . (49)

25

Then we can write (47) as

Rj+1 ≤ Rj − m1j aj − λm2j bj

(50)

Now we bring back the subscript t. Summing the equation from 0 to N − 1,

N −1

R0 − RN at,0 + λbt,0 − at,N − λbt,N a˜t + λ˜bt − a˜t+1 − λ˜bt+1

at,j + λbt,j ≤ N γ = N γ = N γ ,

j=0

(51)

where

γ

:=

minj

{m

1 j

,

m2j

}

,

and

the

ﬁrst

equality

is

due

to

cN

=

dN

=

0

and

(xt,0, yt,0)

=

(x˜t, y˜t).

Summing

t

from 0 to T − 1, we get

1 T −1 N −1

a˜0 + λ˜b0 ak + λbk

N T at,j + λbt,j ≤ N T γ = N T γ .

t=0 j=0

(52)

The left hand side is exactly ak+1 + λbk+1, because (xk, yk) is sampled uniformly from {{(xt,j, yt,j)}jN=−01}Tt=−01. Part 2.

It suﬃces to choose proper τ1, τ2, N and T such that N T γ > 1. Driven by the proof, we choose

k1 τ1 = κ2l ,

β1 = k2κ2l,

τ2 = k3 , l

β2 = lk4.

We will choose k1, k2, k3 and k4 later and we let k1, k2, k3, k4 ≤ 1. Plug back to cj and dj, we have

k12 cj = 1 + k1k2 + κ4

2

k12k32

k12

cj+1 + k3(1 + k1k2) + κ4 + (L + l) κ4

k32 + k3 l2 l2k4

λ2 lk32(1 + k1k2) + 2Lκ4 k12 + 2λκ4 lk12k32 + 2λκ4 (L + l)k12(1 − k3k4)

k12

2

12

2

l2

≤ 1 + k1k2 + κ4 cj+1 + 3k3 + 3 κ3 k1 dj+1 + 2λlk3 + (1 + 2λ) κ3 k1,

µ2 dj+1+

(53)

where in the last inequality we assume k32 + kk34 ≤ 1.

k12

2

k12

dj = κ4 cj+1 + 1 + k3k4 + k3 + (L + l) κ4

k32 + k3 l2 l2k4

µ2 + κ14 k12k32 dj+1+

λ2 lk32 + 2Lκ4 k12 + 2λκ4 lk12k32 + 2λκ4 (L + l)k12(1 − k3k4)

k12

2 32

2

l2

≤ κ4 cj+1 + 1 + k3k4 + 2k3 + κ3 k1 dj+1 + λlk3 + (1 + 2λ) κ3 k1.

(54)

We deﬁne ej = max{cj, dj}. Then combining (53) and (54), we easily get

ej ≤ 1 + k1k2 + k3k4 + 3k32 + κ43 k12 ej+1 + 2λlk32 + (1 + 2λ) κl3 k12.

As eN = 0, we have

e0 ≤ 2λlk32 + (1 + 2λ) κl3 k12

1 + k1k2 + k3k4 + 3k32 +

4 κ3

k12

N

−1

k k + k k + 3k2 + 4 k2

,

12

34

3 κ3 1

(55)

26

and note that ej > ej+1 so ej ≤ e0, ∀j. Then we want to lower bound γ. Rearrange (48),

m1j =µ1 τ1(1 + λ − λτ2µ2) − 2λl3τ22

τ 2 + τ1 1 β1

− 4λ τ1 + 2l τ12

(1 − τ2µ2)−

−2τ1

τ 2 + τ2 2 β2

µ2 + 4

τ 2 + τ1 1 β1

l2τ22 + 8

τ1 + 2l τ12

τ 2 + τ2 2 β2

µ2 dj+1−

4 τ 2 + τ1 1 β1

cj+1

≥ 12 τ1µ1 − κ44 k32

k2 + k1 1 k2

10µ2 + κ2l k1

k2 + k3 3 k4

µ1

4

l2 dj+1 − κ4

k2 + k1 1 k2

µ1 l2 cj+1,

(56)

where in the inequality, we use λ = 1/20 and assume that κ12 k32(k1 + k12 ) ≤ 10. Rearranging (49),

2

l2τ1

mj =τ2µ2 − µ2

1

2l5

λ + 1 − τ2µ2 − µ2

τ 2 + τ1 1 β1

2 4l2 τ2 − µ2

τ1 + 2l τ12 (1 − τ2µ2)−

2 2 τ2

22

2 τ2

4 l4 2 2 τ1

8l2

l2

λ

τ2 + β2

µ2 + λ l τ1

τ2 + β2

+ λ µ2 τ2

τ1 + β1

+ λµ2

τ1 + 2 τ1

τ 2 + τ2 2 β2

4 l2 λ µ2

τ 2 + τ1 1 β1

cj+1

l2τ1

2 k3

≥

−

2 min{µ1, µ2}

200

k3 + k4

80 + κ2

k2 + k1 1 k2

µ2

80

l2 dj+1 − κ2

k2 + k1 1 k2

µ2 l2 cj+1,

µ2 dj+1− (57)

where

in

the

inequality

we

use

λ

=

1/20

and

assume

k1

≤

k3/28

and

1 κ2

k32

12 τ1µ1 = 2µκ12l k1 and 2 minl{2µτ11,µ2} = 2κ2 minl{µ1,µ2} k1. Then we have

k1 + k12

≤ 1/4. Note that

m1j ≥ κ13

21 k1 − κ42 k32

k2 + k1 1 k2

+ 10µ2 k1 l

k2 + k3 3 k4

dj+1 − 4

l

κ2

k2 + k1 1 k2

cj+1 , l

m2j ≥ κ1

1

80

2 k1 − κ2

k2 + k1 1 k2

+ 200 k2 + k3 3 k4

dj+1 − 80

l

κ2

k2 + k1 1 k2

cj+1 . l

(58) (59)

Letting k1/k2 = k3/k4 and k1 = 218 k3, we have

1 γ≥

1 k3 − 360

k2 + k3

e0 ,

(60)

κ3 56

3 k4 l

where we use cj, dj ≤ e0, ∀j. By plugging in k1 = k3/28 and λ = 1/20 into (55), we have

e0 ≤ l (1 + 2k3k4 + 4k32)N − 1 . (61) k4/k3 + 3

Plugging this into (60), we have

1 k3

(1 + 2k3k4 + 4k32)N − 1 2

γ ≥ κ3 56 − 360

k4/k3 + 3

k3 + k3/k4 .

(62)

We choose k4 = k31/2, then

1 k3

3/2

2N

k32 + k31/2

N T γ ≥ κ3

− 360 56

(1 + 2k3

+ 4k3)

−1

k−1/2 + 3

NT.

(63)

3

27

Part 3. We choose T = 1, k3 = βκ−6 and N = α(2k33/2 + 4k32)−1 ≥ α2 k2−3/2, where α, β is irrelevant to n, l, µ1, µ2. Then since (1 + 2k33/2 + 4k32)N ≤ eα, after plugging in N and k3, we have
N T γ ≥ κ13 5k63 − 360(eα − 1)(2k3) α2 k2−3/2 ≥ 12 516 − 2 × 360(eα − 1) αβ−1/2. (64)

Therefore, for choosing α small enough and β small enough, we have N T γ ≥ 2. Now it remains to verify several assumptions we made in the proof. The ﬁrst is kk34 + k32 ≤ 1. Since kk34 + k32 = k31/2 + k32, this assumption
easily holds when β ≤ 1/4. The second assumption we want to verify is κ12 k32 k1 + k12 ≤ 1/4. Note that

1 k2

1 k1 +

κ2 3

k2

= 1 k2 k1 + k3

κ2 3

k4k1

= κ12 k32

1

−1/2

28 k3 + 28k3

.

So this assumption can also be easily satisﬁed when β is small. The last assumption we need to verify is λ ≥ dj+1 τ2 + β12 . Because dj+1 ≤ e0 and (61),

dj+1

1 τ2 + β2

≤ l (1 + 2k3k4 + 4k32)N − 1 k4/k3 + 3
≤ (1 + 2k3k4 + 4k32)N − 1 ≤ 2(eα − 1)k3.

k3 + 1 l k4l
k32 + k31/2 k3−1/2 + 3

So this assumption holds when α and β are small.

Proof of Theorem 4.2

Proof. We start from Part 3 of the proof of Theorem 4.1. We now choose k3 = βn−2/3, N = α(2k33/2 + 4k32)−1,

and T = κ3n−1/3 then

1 NTγ ≥

1 − 2 × 360(eα − 1) αβ−1/2

(65)

2 56

Therefore, for choosing α small enough and β small enough, we have N T γ ≥ 2. Other assumptions can be easily veriﬁed by the same way as in the proof of Theorem 4.1.

D AGDA for minimax problems under one-sided PL condition
We are here to show that if −f (x, ·) satisﬁes PL condition with constant µ and f (·, y) may be nonconvex (referred to as PL game by Nouiehed et al. [2019]), AGDA as presented in Algorithm 3 can ﬁnd -stationary point of g(x) := maxy f (x, y) within O( −2) iterations. Note that GDmax has complexity O( −2 log(1/ )) on minimax problems under the one-sided PL condition [Nouiehed et al., 2019]; SGDA has complexity O( −2) on nonconvex-strongly-concave minimax problems [Lin et al., 2019]. Here we deﬁne condition number κ = µl and L is still deﬁned the same as before. The proof is based on our previous analysis and Lin et al. [2019].
Deﬁnition 3. x is -stationary point of a diﬀerential function f if E ∇f (x) ≤ .

28

Algorithm 3 AGDA
1: Input: (x0, y0), step sizes τ1 > 0, τ2t > 0 2: for all t = 0, 1, 2, ..., T − 1 do 3: xt+1 ← xt − τ1∇fx(xt, yt) 4: yt+1 ← yt + τ2∇fy(xt+1, yt) 5: end for 6: choose (xT , yT ) uniformly from {(xt, yt)}Tt=0

Theorem D.1. Suppose Assumption 1 holds and −f (x, ·) satisﬁes PL condition with constant µ for any x.

If

we

run

Algorithm

3

with

τ1

=

1 20κ2 l

and

τ2

=

1l ,

then

E ∇g(xT ) 2 ≤ 8 [10κ2la0 + κ2lb0],

(66)

T +1

where a0 = g(x0) − g∗ and b0 = g(x0) − f (x0, y0).

Proof. For convenience, we still deﬁne bt = g(xt) − f (xt, yt). Since it can be easily veriﬁed that τ1 ≤ 1/L, by

(18) and (26), we have

g(xt+1) ≤ g(xt) − τ1 ∇g(xt) 2 + τ1l2 bt. (67)

2

µ2

By (22), we have

bt+1 ≤(1 − µ2τ2)bt + (1 − µ2τ2) τ1 + 2l τ12 ∇xf (xt, yt) 2−

(1 − µ2τ2) τ1 ∇g(xt) 2 + (1 − µ2τ2) τ2 ∇xf (xt, yt) − ∇g(xt) 2

2

2

≤(1 − µ2τ2)bt + 2(1 − µ2τ2) τ1 + 2l τ12 − (1 − µ2τ2) τ22 ∇g(xt) 2+

2(1 − µ2τ2) τ1 + 2l τ12 + (1 − µ2τ2) τ22 ∇xf (xt, yt) − ∇g(xt) 2

2 l2

3

2

2

≤(1 − µ2τ2) 1 + 5τ1 + 2lτ1 µ2 bt + (1 − µ2τ2) 2 τ1 + lτ1 ∇g(xt) ,

(68)

where in the second inequality we use Young’s inequality, and in third inequality we use (26). We write

bt+1 = αbt + β ∇g(xk) 2

(69)

with

2 l2 α = (1 − µ2τ2) 1 + 5τ1 + 2lτ1 µ2 ,

β = (1 − µ2τ2) 23 τ1 + lτ12 .

Then

t−1
bt ≤ αtb0 + β αt−1−k ∇g(xk) 2,
k=0

t ≥ 1.

Plugging into (67), we have

g(xt+1) ≤ g(xt) − τ1 ∇g(xt) 2 + τ1l2 αtb0 + τ1l2β t−1 αt−1−k ∇g(xk) 2, 2 µ2 µ2 k=0

t ≥ 1.

(70)

29

Telescoping and rearranging,

τ1 T 2 t=0

∇g(xt) 2 − τ1l2β T t−1 αt−1−k µ2 t=1 k=0

Considering the left hand side of (71),

∇g (xk )

2 ≤ g(x0) − g(xT +1) + τ1l2 b0 T αt ≤ a0 + τ1l2 b0

µ2 t=0

µ2(1 − α)

(71)

T t−1

T −1 T

T −1 1

αt−1−k ∇g(xk) 2 =

αt−1−k ∇g(xk) 2 ≤

∇g(xk) 2,

1−α

t=1 k=0

k=0 t=k+1

k=0

(72)

and therefore,

τ1 T 2 t=0

∇g(xt) 2 − τ1l2β T t−1 αt−1−k ∇g(xk) 2 ≥ T µ2 t=0 k=0 t=0

1

l2β

−

τ1 ∇g(xt) 2.

2 µ2(1 − α)

(73)

We note that β = (1 − µ2τ2)

3 2

τ1

+

lτ12

≤ 52 τ1 because l/τ1 ≤ 1 by our choice of τ1. Also,

2 l2

τ1l2 1

1 − α = µ2τ2 − (1 − µ2τ2) 5τ1 + 2lτ1

µ2 ≥ µ2τ2 − 7(1 − µ2τ2) µ2

≥, 2κ

(74)

where in the last inequality we use µ2τ2 = 1/κ and (1 − µ2τ2) τµ12l2 = (1 − 1/κ)/(20κ) ≤ 1/(20κ). Plugging

into (73),

τ1 T 2 t=0

∇g(xt) 2 − τ1l2β T t−1 αt−1−k µ2 t=1 k=0

∇g(xk) 2 ≥ τ1 T 4
t=0

∇g(xt) 2.

(75)

Combining with (71), we have

1 T ∇g(xt) 2 ≤ 4 a0 + τ1l2 b0 ≤ 8 [10κ2la0 + κ2lb0], (76)

T + 1 t=0

(T + 1)τ1

µ2(1 − α)

T +1

where in the inequality we use 1 − α ≥ 1/(2κ) again.

30

