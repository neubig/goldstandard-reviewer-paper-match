Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence

Nihar B. Shah Sivaraman Balakrishnan Joseph Bradley Abhay Parekh Kannan Ramchandran Martin J. Wainwright UC Berkeley

nihar@eecs.berkeley.edu sbalakri@berkeley.edu
joseph.kurata.bradley@gmail.com parekh@berkeley.edu
kannanr@eecs.berkeley.edu wainwrig@berkeley.edu

arXiv:1505.01462v1 [cs.LG] 6 May 2015

Keywords: Pairwise comparisons, inference, ranking, topology, crowdsourcing
Abstract
Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector w∗ ∈ Rd that represents the “qualities” of the d items being compared; this class of models includes the two most widely used parametric models– the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector w∗ under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared via its Laplacian spectrum. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors.
1. Introduction
In an increasing range of applications, it is of interest to elicit judgments from non-expert humans. For instance, in marketing, elicitation of preferences of consumers about products, either directly or indirectly, is a common practice (Green et al., 1981). The gathering of this and related data types has been greatly facilitated by the emergence of “crowdsourcing” platforms such as Amazon Mechanical Turk: they have become powerful, low-cost tools for collecting human judgments (Khatib et al., 2011; Lang and Rio-Ross, 2011; von Ahn et al., 2008). Crowdsourcing is employed not only for collection of consumer preferences, but also for other types of data, including counting the number of malaria parasites in an image of a blood smear (Luengo-Oroz et al., 2012); rating responses of an online search engine to search queries (Kazai, 2011); or for labeling data for training machine learning algorithms (Hinton et al., 2012; Raykar et al., 2010; Deng et al., 2009). In a diﬀerent domain, competitive sports can be understood as a mechanism for sequentially performing comparisons between individuals or teams (Ross, 2007; Herbrich et al., 2007). Finally, peer-grading in massive open online courses (MOOCs) (Piech et al., 2013) can be viewed as another form of elicitation.
A common method of elicitation is through pairwise comparisons. For instance, the decision of a consumer to choose one product over another constitutes a pairwise comparison between the two products. Workers in a crowdsourcing setup are often asked to compare pairs of items: for
1

!%

!%

!%

%tagline%for%a% are%plaMorm%
but sure cure” /%10%

%Which%image%is%more%relevant% How%rHeolewvarenlte%ivs%atnhtisi%simthaigsei%mfoarg%%e for for%the%search%query%‘INTERNET’?% the%stehaercshe%aqrucehryq%u'IeNrTyE'RINNTEET'R?%NET'?

!%

!%

(a) Asking for a pairwise comparison.

/%100/%100
(b) Asking for a numeric score.

Figure 1. An example of eliciting judgments from people: rating the relevance of the result of a search query.

instance, they might be asked to identify the better of two possible results of a search engine, as shown in Figure 1a. Competitive sports such as chess or basketball also involve sequences of pairwise comparisons. From a modeling point of view, we can think of pairwise comparisons as a means of estimating the underlying “qualities” or “weights” of the items being compared (e.g., skill levels of chess players, relevance of search engine results, etc.). Each pairwise comparison can be viewed as a noisy sample of some function of the underlying pair of (real-valued) weights. Noise can arise from a variety of sources. When objective questions are posed to human subjects, noise can arise from their diﬀering levels of expertise. In a sports competition, many sources of randomness can inﬂuence the outcome of any particular match between a pair of competitors. Thus, one important goal is to estimate the latent qualities based on noisy data in the form of pairwise comparisons. A related problem is that of experimental design: assuming that we can choose the subset of pairs to be compared (e.g., in designing a chess tournament), what choice will allow for the most accurate estimation? Characterizing the fundamental diﬃculty of estimating the weights will allow us to make this choice judiciously. These tasks are the primary focus of this paper.
In more detail, the focus of this paper is the aggregation from pairwise comparisons in a fairly broad class of parametric models. This class includes as special cases the two most popular models for pairwise comparisons—namely, the Thurstone (Case V) (Thurstone, 1927) and the Bradley-TerryLuce (BTL) (Bradley and Terry, 1952; Luce, 1959) models. The Thurstone (Case V) model has been used in a variety of both applied (Swets, 1973; Ross, 2007; Herbrich et al., 2007) and theoretical papers (Bramley, 2005; Krabbe, 2008; Nosofsky, 1985). Similarly, the BTL model has been popular in both theory and practice (e.g., (Nosofsky, 1985; Atkinson et al., 1998; Koehler and Ridpath, 1982; Heldsinger and Humphry, 2010; Loewen et al., 2012; Green et al., 1981; Khairullah and Zionts, 1987)).
1.1 Some past work
There is a vast literature on the Thurstone and BTL models, and we focus on those most closely related to our own work. Negahban et al. (2012) provide minimax bounds for the BTL model in the special case of comparisons chosen uniformly at random. They focus on this case in order to complement their analysis of an algorithm based on a random walk. In their analysis, there is a gap between the achievable rate of the MLE and the lower bound. In contrast, our analysis eliminates this discrepancy and shows that MLE is an optimal estimator (up to constant factors) and achieves the minimax rate. In independent and concurrent work, Hajek et al. (2014) consider the problem of estimation in the Plackett-Luce model, which extends the BTL model to comparisons of two or more items. They derive bounds on the minimax error rates under this model which are tight up
2

to logarithmic factors. In contrast, our results are tight up to constants and, as we emphasize in the following section, provide deeper insights into the role of the topology of the comparison graph. Jagabathula and Shah (2008) design an algorithm for aggregating ordinal data when the underlying distribution over the permutations is assumed to be sparse. Ammar and Shah (2011) employ a diﬀerent, maximum entropy approach towards parameterization and inference from partially ranked data. Rajkumar and Agarwal (2014) study the statistical convergence properties of several rank aggregation algorithms.
Our work assumes a ﬁxed design setup. In this setup, the choice of which pairs to compare and the number of times to compare them is chosen ahead of time in a non-adaptive fashion. There is a parallel line of literature on “sorting” or “active ranking” from pairwise comparisons. For instance, Braverman and Mossel (2008) assume a noise model where the outcome of a pairwise comparison depends only on the relative ranks of the items being compared, and not on their actual ranks or values. On the other hand, Jamieson and Nowak (2011) consider the problem of ranking a set of items assuming that items can be embedded into a smaller-dimensional Euclidean space, and that the outcomes of the pairwise comparisons are based on the relative distances of these items from a ﬁxed reference point in the Euclidean space.
A recent line of work considers a variant of the BTL and the Thurstone models where the comparisons may depend on some auxiliary unknown variable in addition to the items being compared; for instance, the accuracy of the individual making the comparison in an objective task. Chen et al. (2013) consider a crowdsourcing setup where the outcome depends on the worker’s expertise. They present algorithms for inference under such a model and present empirical evaluations. Yi et al. (2013) consider a problem in the spirit of collaborative ﬁltering where certain unknown preferences of a certain user must be predicted based on the preferences of other users as well as of that user over other items. Lee et al. (2011) consider the inverse problem of measuring the expertise of individuals based on the rankings submitted by them, and the proposed algorithms assume an underlying Thurstone model.
1.2 Our contributions
Both the Thurstone (Case V) and BTL models involve an unknown vector w∗ ∈ Rd corresponding to the underlying qualities of d items, and in a pairwise comparison between items j and k, the probability of j being ranked above k is some function F of the diﬀerence wj∗ − wk∗. The Thurstone (Case V) and BTL are based on diﬀerent choices of F , and both belong to the broader class of models analyzed in this paper, in which F is required only to be strongly log-concave.
With this context, the main contributions of this paper are to provide some answers to the following questions:
• How does the minimax error for estimating the weight vector w∗ in various norms scale with the problem dimension (the number of items) and the number of observations?
– We derive upper and lower bounds on the minimax estimation rates under the model described above. Our upper/lower bounds on the estimation error agree up to constant factors: to the best of our knowledge, despite the voluminous literature on these two models, this provides the ﬁrst sharp characterization of the associated minimax rates. Moreover, our error guarantees provide guidance to the practitioner in assessing the number of pairwise comparisons to be made in order to guarantee a pre-speciﬁed accuracy.
• Given a budget of n comparisons, which pairs of items should be compared?
– The bounds that we derive depend on the comparison graph induced by the subset of pairs that are compared. Our theoretical analysis reveals that the spectral gap of a certain scaled version
3

of the graph Laplacian plays a fundamental role, and provides guidelines for the practitioner on how to choose the subset of comparisons to be made.
• When is it better to elicit pairwise comparisons versus numeric scores?
– When eliciting data, one often has the liberty to ask for either cardinal values (Figure 1b) or for pairwise comparisons (Figure 1a) from the human subjects. One would like to adopt the approach that would lead to a better estimate. One may be tempted to think that cardinal elicitation methods are superior, since each cardinal measurement gives a real-valued number whereas an ordinal measurement provides at most one bit of information. Our bounds show, however, that the scaling of the error in the cardinal and ordinal settings is identical up to constant pre-factors. As we demonstrate, this result allows for a comparison of cardinal and ordinal data elicitation methods in terms of the per-measurement noise alone, independent of the number of measurements and the number of items. A priori, there is no obvious reason for the relative performance to be independent of the number of measurements and items.
Notation: For any symmetric matrix M of size (m × m), we will let λ1(M ) ≤ λ2(M ) ≤ · · · ≤ λm(M ) denote its ordered eigenvalues. We will use the notation DKL(P1 P2) to denote the KullbackLeibler divergence between the two distributions P1 and P2. For any integer m, we will let [m] denote the set {1, . . . , m}.

2. Problem formulation
We begin with some background followed by a precise formulation of the problem.

2.1 Generative models for ranking

Given a collection of d items to be evaluated, we suppose that each item has a certain numeric quality
score, and a comparison of any pair of items is generated via a comparison of the two quality scores in the presence of noise. We represent the quality scores as a vector w∗ ∈ Rd, so item j ∈ [d] has quality score wj∗. Now suppose that we make n pairwise comparisons: if comparison i ∈ [n] pertains to comparing item ai with item bi, then it can be described by a diﬀerencing vector xi ∈ Rd, with entry ai equal to one, entry bi equal to −1, and the remaining entries set to 0.
With this notation, we study the problem of estimating the weight vector w∗ based on observing
a collection of n independent samples yi ∈ {−1, 1} drawn from the distribution

P yi = 1|xi, w∗ = F

xi, w∗ σ

for i ∈ [n],

(Ordinal)

where F is a known function taking values in [0, 1]. Since the probability of item ai dominating bi should be independent of the order of the two items being compared, we require throughout that F (x) = 1 − F (−x).
In any model of the general form (Ordinal), the parameter σ > 0, assumed to be known, plays the role of a noise parameter, with a higher value of σ leading to more uncertainty in the comparisons. Moreover, we assume that F is strongly log-concave in a neighborhood of the origin, meaning that there is some curvature parameter γ > 0 such that

d2

dt2 (− log F (t)) ≥ γ for all t ∈ [−2B/σ, 2B/σ].

(1)

Here the known parameter B denotes a bound on the ∞-norm of the weight vector, namely

w∗ ∞ ≤ B.

4

As our analysis shows, a bound of this form is fundamental: the minimax error for estimating w∗
will diverge to inﬁnity if we are allowed to consider models in which B is arbitrarily large (see
Proposition 17 in Appendix G). Informally, this behavior is related to the diﬃculty of estimating very small (or very large) probabilities that can arise in the two models for large w∗ ∞. Note that any model of the form (Ordinal) is invariant to shifts in w∗, that is, it does not diﬀerentiate between the vector w∗ and the shifted vector w∗ +1, where 1 denotes the vector of all ones. Therefore, in order to ensure identiﬁability of w∗, we assume throughout that 1, w∗ = 0. We will use the notation WB to denote the set of permissible quality score vectors

WB : = w ∈ Rd | w ∞ ≤ B, and 1, w = 0 .

(2)

Both the Thurstone (Case V) model with Gaussian noise (Thurstone, 1927) and Bradley-TerryLuce (BTL) models (Bradley and Terry, 1952; Luce, 1959) are special cases of this general set-up, as we now describe.

Thurstone (Case V): This model is is a special case of the family (Ordinal), obtained by setting

F (t) =

t

1 √

e−u2/2du,

(3)

−∞ 2π

corresponding to the CDF of the standard normal distribution. Consequently, the Thurstone model can alternatively be written as making n i.i.d. observations of the form

yi = sign xi, w∗ + i , for i ∈ [n],

(Thurstone)

where i ∼ N (0, σ2) is observation noise. It can be veriﬁed that the Thurstone model is strongly log-concave (e.g., see (Tsukida and Gupta, 2011)).

Bradley-Terry-Luce: The Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952; Luce, 1959) is another special case in which

1 F (t) = 1 + e−t ,

and hence

P yi = 1|xi, w∗ = 1 + exp 1− xi, w∗
σ

for i ∈ [n].

It can also be veriﬁed that the BTL model is strongly log-concave.

(BTL)

Cardinal observation models: While our primary focus is on the pairwise-comparison setting, for comparison purposes we also analyze analogous cardinal settings where each observation is real valued. In particular, we consider the following two cardinal analogues of the Thurstone model. In the Cardinal model we consider, each observation i ∈ [n] consists of a numeric evaluation yi ∈ R of a single item,

yi = ui, w∗ + i for i ∈ [n],

(Cardinal)

where ui in this case is a coordinate vector with one of its entries equal to 1 and remaining entries equal to 0, and i is independent Gaussian noise N (0, σ2). One may alternatively elicit cardinal
values of the diﬀerences between pairs of items

yi = xi, w∗ + i for i ∈ [n],

(Paired Cardinal)

where i are i.i.d. N (0, σ2). We term this model the Paired Cardinal model.

5

2.2 Fixed design and the graph Laplacian

We analyze the estimation error when a ﬁxed subset of pairs is chosen for comparison. Of interest
to us will be the comparison graph deﬁned by these chosen pairs, with each pair inducing an edge
in the graph. Edge weights are determined by the fraction of times a given pair is compared. The
analysis in the sequel reveals the central role played by the Laplacian of this weighted graph. Note
that we are operating in a ﬁxed-design setup where the graph is constructed oﬄine and does not
depend on the observations. In the ordinal models, the ith measurement is related to the diﬀerence between the two items
being compared, as deﬁned by the measurement vector xi ∈ Rd. We let X ∈ Rn×d denote the measurement matrix with the vector xTi as its ith row. The Laplacian matrix L associated with this diﬀerencing matrix is given by

1T

1n

T

L := X X =

n

n

xixi .

(4)

i=1

By construction, for any vector v ∈ Rd, we have vT Lv = j=k Ljk(vj −vk)2, where Ljk is the fraction of the measurement vectors {xi}ni=1 in which items (j, k) are compared.
The Laplacian matrix is positive semideﬁnite, and has at least one zero-eigenvalue, corresponding
to the all-ones eigenvector. The Laplacian matrix induces a graph on the vertex set {1, . . . , d}, in
which a given pair (j, k) is included as an edge if and only if Ljk = 0, and the weight on an edge (j, k) equals Ljk. We emphasize that throughout our analysis, we assume that the comparison graph is connected, since otherwise, the quality score vector w∗ is not identiﬁable. Note that the Laplacian matrix L induces a semi-norm1 on Rd, given by

u − v L : = (u − v)T L(u − v).

(5)

We study optimal rates of estimation in this semi-norm, as well as the usual 2-norm. As will be clearer in the sequel the L semi-norm is a natural metric in our setup, and estimation in this induced metric can be done at a topology independent rate. The estimation error in the L semi-norm is closely related to the prediction risk in generalized linear models. It arises naturally when one is interested in predicting the probability of a certain outcome for a new comparison.

3. Bounds on the minimax risk
In this section, we state the main results of the paper, and discuss some of their consequences.

3.1 Minimax rates in the squared L semi-norm

Our ﬁrst main result provides bounds on the minimax risk under the squared L semi-norm (5) in the pairwise comparison models introduced earlier. In all of the statements, we use c1, c2, etc. to denote positive numerical constants, independent of the sample size n, number of items d and other problem-dependent parameters.
Apart from the parameter γ, the bounds presented subsequently will depend on F through a second parameter ζ, deﬁned as

max F (x)

x∈[0,2B/σ]

ζ :=

.

(6)

F (2B/σ)(1 − F (2B/σ))

In the BTL and the Thurstone models, we have ζ : = F (2B/σ)F(1−(0F) (2B/σ)) .

1. A semi-norm diﬀers from a norm in that the semi-norm of a non-zero element is allowed to be zero.

6

Theorem 1 (Bounds on minimax rates in L semi-norm) (a) For a sample size n ≥ c1σζ2Btr2(L†) , any estimator w based on n samples from the Ordinal model has Laplacian squared error lower bounded as
d sup E w − w∗ 2L ≥ cζ1 σ2 nd . (7a)
w∗∈WB

(b) For any instance of the Ordinal model with γ-strong log-concavity and any w∗ ∈ WB, the maximum likelihood estimator satisﬁes the bound

∗ 2 cζ2σ2 d

−t

P wML − w L > t γ2 n ≤ e for all t ≥ 1,

and consequently

sup E
w∗∈WB

wML − w∗ 2L ≤ c1γuζ σ2 nd .

(7b)

The results of Theorem 1 characterize the minimax risk in the squared L semi-norm up to constant factors. The upper bounds follow from an analysis of the maximum likelihood estimator, which turns out to be a convex optimization problem. On the other hand, the lower bounds are based on a combination of information-theoretic techniques and carefully constructed packings of the parameter set WB. The main technical diﬃculty is in constructing a packing in the semi-norm induced by the Laplacian L. See Appendix A for the full proof.

3.2 Minimax rates in the squared 2-norm
Let us now turn to optimizing the minimax risk under the squared Euclidean norm. Theorem 2 below presents upper and lower bounds on this quantity.

Theorem 2 (Bounds on minimax rates in 2-norm) (a) For a sample size n ≥ c2σζ2Btr2(L†) , any estimator w based on n samples from the Ordinal model has squared Euclidean error lower
bounded as

sup E
w∗∈WB

∗2

σ2

w−w

2

≥ c2

max n

d
d2, max
d ∈{2,...,d} i= 0.99d

1 .
λi(L)

(8a)

(b) For any instance of the Ordinal model with γ-strong log-concavity and any w∗ ∈ WB, the maximum likelihood estimator satisﬁes the bound

sup E
w∗∈WB

w − w∗ 2 ≤ c2uζ σ2 d .

ML

2

γ λ2(L)n

(8b)

See Appendix B for the proof of this theorem. As we describe in the next section, the upper and
lower bounds on minimax risk from Theorem 2 to identify the comparison graph(s) that lead to the
best possible minimax risk over all possible graph topologies.
Figure 2 depicts results from simulations under the Thurstone model, depicting the squared 2 error for the maximum likelihood estimator for various values of n and d. In the simulations, the true vector w∗ is generated by ﬁrst drawing a d-length vector uniformly at random from [−1, 1]d, followed by a scale and shift to ensure w∗ ∈ WB. The n pairs are chosen uniformly (with replacement) at random from the set of d2 possible pairs of items. The value of σ and B are both ﬁxed to

7

Error
|| ˆw−w ∗ ||22
Rescaled error
|| ˆw−w ∗ ||22 n/d2

24

22

20

2-2

2-4

d = 8

2-6

d = 16 d = 32

2-8 27

d = 64
28 29

210 211 212 213

Number of samples n

24

22

20

2-2

2-4

d = 8

2-6

d = 16 d = 32

2-8 27

d = 64
28 29

210 211 212 213

Number of samples n

(a) Error

(b) Rescaled error

Figure 2. Simulation results under the Thurstone model. The comparison topology chosen here is the complete graph.

be 1. Given the n samples, inference is performed via the maximum likelihood estimator for the

Thurstone model. Each point in the plots is an average of 20 such trials.

The error in Figure 2 reduces linearly with n, exactly as predicted by our Theorem 2. For the

complete graph, λ21(L) = d−2 1 . Theorem 2 thus predicts a quadratic increase in the error with d. As

predicted,

the

error

when

normalized

by

1 d2

in

Figure

2

converges

to

the

same

curve

for

all

values

of

d.

Before concluding this section, we also look at the Paired Cardinal model (Section 2.1), the

cardinal analogue of the Thurstone model.

Theorem 3 (Bounds on minimax rates in 2-norm) For the Paired Cardinal model, the minimax risk is sandwiched as

2 tr(L†)

∗2

2 tr(L†)

c3 σ n ≤ inwf w∗s∈uWp∞ E w − w 2 ≤ c3u σ n . (9)

The proof of Theorem 3 is available in Appendix C.
We conjecture that the dependence of the squared 2 minimax risk under the Ordinal models on the problem parameters n, d and the graph topology is identical to that derived in Theorem 3 for the Paired Cardinal model, i.e., is proportional to tr(nL†) .

3.3 Extension to m-ary comparisons
Suppose instead of eliciting pairwise comparisons, one can instead ask the workers to make comparisons between more than two options. In particular, we assume that each sample is a selection of the item with the largest perceived quality among some m presented items. The setting of pairwise comparisons is a special case with m = 2. Recall from Theorem 2 that the minimum squared 2 minimax risk in the pairwise comparison setting is of the order dn2 . Our goal in this section is to bring the concept of multiple-item comparison under the same framework as the pairwise case, and via a generalization of our earlier theoretical analysis, understand how the error exponent depends on m.

8

Consider d items, where every item j ∈ [d] has a certain underlying quality score wj∗ ∈ [−B, B]. You obtain n samples, with each sample being a selection of the item with the largest perceived value
among some m presented items.
Consider (d × m) matrices E1, . . . , En such that for each i ∈ [n], the m columns of Ei are distinct unit vectors. The positions of the non-zero elements in the m columns of Ei represent the identities of the m items compared in the ith sample. One can visualize the choices of the items compared as a
hyper-graph, with d vertices representing the d items and hyper-edge i ∈ [n] containing the m items
compared in observation i.
Let R1, . . . , Rm be (m × m) permutation matrices representing m cyclic shifts in an arbitrary (but ﬁxed) direction. Consider the observation model

P(yi = j|w∗, Ei) = F ((w∗)T EiRj)

for all j ∈ [m], where F : [−B, B]m → [0, 1] represents the probability of choosing the ﬁrst among the m items presented. For every x ∈ [−B, B]m, F (x) is assumed to satisfy:

• Shift-invariance: the probabilities depend only on the diﬀerences in the weights of the items presented, i.e, F (x) depends only on {xi − xj}i,j∈[m].

• Strong log-concavity: ∇2(− log F (x)) 0.

H for some (m×m) symmetric matrix H with λ2(H) >

Note that the shift-invariance assumption implies 1 ∈ nullspace(∇2(− log F (x))), thereby neces-
sitating nullspace(H) = span(1) and λ1(H) = 0. One can also verify that the model proposed here reduces to the Ordinal model of Section 2.1 when m = 2.
For any hope of inferring the true weights w∗, we must ensure that the comparison hyper-graph
is “connected”, i.e., for every pair of items i, j ∈ [d], there must exist a path connecting item i and
item j in the comparison hyper-graph. We assume this condition is satisﬁed. We also continue to assume that w∗ ∈ WB : = {w ∈ Rd | w ∞ ≤ B, w, 1 = 0}.
The popular Plackett-Luce model falls in this class, as illustrated below.

Example 1 (Plackett-Luce model (Plackett, 1975; Luce, 1959)) The Plackett-Luce model con-

cerns the process of choosing an item from a given set. Speciﬁcally, given m items with quality scores

w

∗ 1

,

.

.

.

,

w

∗ m

respectively,

the

likelihood

of

choosing

item

i

∈

[m]

under

this

model

is

given

by

ewi∗

∗

∗

=: F ([w1, . . . , wm]).

mj=1 ewj∗

Every choice is made independent of all other choices. It is easy to verify that the Plackett-Luce model satisﬁes shift invariance. We now show that it
also satisﬁes strong log-concavity. A little algebra gives

2

ex1

x

x

x xT

∇ (− log F (x)) = ( ex, 1 )4 e , 1 diag(e ) − e (e ) ,

where ex : = [ex1 · · · exm]T . We will now derive a lower bound for the expression above. An application of the Cauchy-Schwarz inequality yields that for any vector v ∈ Rm,

vT (ex(ex)T )v ≤ vT diag(ex) ex, 1 v,

with equality if and only if v ∈ span(1). It follows that λ2(∇2(− log F (x))) > 0 for all x ∈ [−B, B]m.

Deﬁning

the

scalar

β

:

=

minx∈[−B,B]m

λ2( (

ex1 ex, 1

)4

ex, 1 diag(ex)−ex(ex)T ), on can see that setting

H = σ(I − 11T ) satisﬁes the strong log-concavity conditions.

9

Our goal is to capture the scaling of the minimax error with respect to the number of observations n, the dimension d of the problem, and the choice of the subsets compared {Ei}i∈[n]. It is well understood (Miller, 1956; Kiger, 1984; Shiﬀrin and Nosofsky, 1994; Saaty and Ozdemir, 2003) that humans have a limited information storage and processing capacity, which makes it diﬃcult to compare more than a small number of items. For instance, Saaty and Ozdemir (2003) recommend eliciting preferences over no more than seven options. Thus in this work we will restrict our attention to m = O(1). Moreover, the amount of noise in the selection process also depends on the number of items m presented at a time: the higher the number, the greater the noise. We will thus not use a ‘noise parameter σ’ in this setting, and assume the noise to be incorporated in the function F which itself is a function of m.
Our results involve the Laplacian of the comparison graph, deﬁned for the m-wise comparison setting as follows. Let L be an (d × d) matrix that depends on the choice of the comparison topology as

1n

TT

L := n

Ei(mI − 11 )Ei .

(10)

i=1

We will call L the Laplacian of the comparison hyper-graph. One can verify that when applied to the special case of m = 2, the matrix L deﬁned in (10) reduces to the Laplacian of the pairwisecomparison graph deﬁned earlier in (4).
The following theorem presents our main results for the m-wise comparison setting.

Theorem 4 For the m-wise model, the minimax risk is sandwiched as

infz F (z)

d

c3 m2λm(H) supz ∇F (z) 2H† n ≤ inwf w∗s∈uWpB E

in the squared L semi-norm and as

w − w∗

2 L

m2 supz

∇ log F (z)

2 2

d

≤ c3u

λ2 (H )2

, n

infz F (z)

d2

c4 m2λm(H) supz ∇F (z) 2H† n ≤ inwf w∗s∈uWpB E

w − w∗

2 2

m2 supz

∇ log F (z)

2 2

d2

≤ c4u

λ2 (H )2

, λ2(L)n

in the squared 2 norm. Here we assume n ≥ c5 tr(L†) infz F (z) 2 for both the lower bounds, and
B2λm(H) supz ∇F (z) H†
where the suprema and inﬁma with respect to the parameter z are taken over the set [−B, B]m.

The proof of Theorem 4 is provided in Appendix D. Our results establish that the dependence of the squared L semi-norm and squared Euclidean minimax error on m occurs only as multiplicative pre-factors, and the error exponent is independent of m. Thus, if one follows the standard recommendation in the psychology literature Miller (1956); Kiger (1984); Shiﬀrin and Nosofsky (1994); Saaty and Ozdemir (2003)—namely to choose m = O(1)—then the best possible scaling of the squared L semi-norm minimax risk with respect to d and n is always nd , that of the squared Euclidean minimax risk is always dn2 , and evenly spreading the samples across all possible choices of m items is optimal. Nevertheless, a more reﬁned modeling and analysis is required to understand the precise tradeoﬀs governing the choice of the number m of items presented to the user.

4. Role of graph topology
We now return to the setting of pairwise comparisons. In certain applications, one may have the liberty to decide which pairs are compared. The results of the previous section demonstrated the role played by the Laplacian of the comparison graph in the estimation error. We now employ these results

10

to derive guidelines towards designing the comparison graph. Let us focus on the estimation error in the squared 2 norm in the ordinal setting. As discussed earlier, we assume that the graph induced by the comparisons is connected. An application of Theorem 2 lets us identify good topologies for pairwise comparisons in the ﬁxed-design setup.
A popular class of comparison topologies is that of evenly distributed samples on an unweighted graph (e.g., (Negahban et al., 2012)). Consider any ﬁxed, unweighted graph G = (V, E). We assume that the samples are distributed evenly along the edges E of G, and that the sample size n is suﬃciently large. Using standard matrix concentration inequalities, it is straightforward to extend our analysis to the setting of random chosen comparisons from a ﬁxed graph (see, for instance, Oliveira (2009)). Let L denote the Laplacian of G. We deﬁne the scaled Laplacian of G as
1 L := L.
|E|
One can verify that the matrix L deﬁned here is identical to what was deﬁned in (4) in a more general context. In order to diﬀerentiate from L, we will term L as the regular Laplacian of the graph G.

4.1 Analytical results
Consider the Ordinal model and the squared 2-norm as the metric of interest. We claim that in order to determine whether a given comparison graph achieves minimax risk (up to a constant pre-factor), it suﬃces to examine the eigen-spectrum of the scaled Laplacian matrix. In particular, we claim that:
• If the scaled Laplacian has a second smallest eigenvalue that scales as λ21(L) = Θ(d), then the comparison graph is optimal, and leads to the smallest possible minimax risk, in particular one that scales as dn2 .
• Conversely, if the scaled Laplacian matrix has an eigen-spectrum satisfying


d
d2 = o  max
d ∈{2,...,d} i= 0.99d

 1
, λi(L)

(11)

then the associated estimation error is strictly larger than the minimax risk. In particular, this sub-optimality holds whenever d2 = o( λ21(L) ).
In order to verify these claims, we note that by deﬁnition (4) of the Laplacian matrix, we have

1n

T

tr(L) = n

tr(xixi ) = 2.

i=1

It follows that λ2(L) ≤ d−2 1 , i.e., that λ21(L) = Ω(d). As we will see shortly, several classes of graphs satisfy λ21(L) = Θ(d). Comparing the lower bound of Ω( dn2 ) on the minimax risk (8a) with the upper bound (8b) gives the suﬃcient condition of λ21(L) = Θ(d) for optimality, and the smallest minimax risk as Θ( dn2 ). The lower bound (8a) now also gives the claimed condition for strict sub-optimality.
In order to illustrate these claims, let us consider a few canonical classes of graphs, and study how
the estimation error under the squared Euclidean norm scales in the Ordinal model. The spectra
of the regular Laplacian matrices of these graphs can be found in various standard texts on spectral
graph theory (e.g., Brouwer and Haemers (2011)).

11

• Complete graph. A complete graph has one edge between every pair of nodes. The spectrum

of the regular Laplacian of the complete graph is 0, d, . . . , d, and hence the spectrum of the scaled

Laplacian L is 0, d−2 1 , . . . , d−2 1 .

Substituting λ2(L) =

2 d−1

in Theorem 2b gives an upper bound

of Θ( dn2 ) on the minimax risk, and Theorem 2 gives a matching lower bound. The suﬃciency

condition discussed above proves optimality.

• Constant-degree expander. The spectrum of the regular Laplacian is 0, Θ(d), Ω(d), . . . , Ω(d).
Since the number of edges is Θ(d), the spectrum of the scaled Laplacian equals 0, Θ( d1 ), Ω( d1 ), . . . , Ω( d1 ). The evaluation of this class of graphs with respect to the minimax risk is identical to that of complete graphs, giving a lower and upper bound of Θ( dn2 ) on the minimax risk, and guaranteeing optimality.

• Complete bipartite. The d nodes are partitioned into two sets comprising, say, m1 and m2 nodes. There is an edge between every pair of nodes in diﬀerent sets, and there are no edges between any two nodes in the same set. The eigenvalues of the regular Laplacian of this graph are 0, m2, . . . , m2, m1, . . . , m1, m1 + m2. Since the total number of edges is m1m2, the scaled Laplacian

m1 −1

m2 −1

L has a spectrum 0, m11 ,..., m11 , m12 ,..., m12 , m11 + m12 . Suppose without loss of generality that m1 ≥ m2.

m1 −1

m2 −1

Also suppose that m2 > 1 (the case of m2 = 1 is the star graph discussed below). Then we have

m11 ≤ m12 ≤ m11 + m12 and that d > m1 ≥ d2 . Furthermore since m2 > 1, the multiplicity of m11 in the spectrum of the scaled Laplacian is at least 1. Thus we have λ2(L) = Θ( d1 ). Theorem 2 then gives lower and upper bounds on the minimax risk as Θ( dn2 ) and the suﬃciency condition

discussed above guarantees its optimality.

• Star. A star graph has one central node with edges to every other node. It is a special case
of the complete bipartite graph with m1 = d − 1 and m2 = 1. The spectrum of the regular
Laplacian is 0, 1, . . . , 1, d. Since there are (d − 1) edges, the spectrum of the scaled Laplacian is 0, d−1 1 , . . . , d−1 1 , d−d 1 . Theorem 2 and the suﬃciency condition discussed above imply that this class of graphs is optimal and is associated to a minimax risk of Θ( dn2 ).

• Path. A path graph is associated to an arbitrary ordering of the d nodes with edges between
pairs j and (j + 1) for every j ∈ {1, . . . , d − 1}. The spectrum of the regular Laplacian is given by 2 1 − cos πdi , i ∈ {0, . . . , d − 1}, and that of the scaled Laplacian is thus d−2 1 1 − cos πdi , i ∈ {0, . . . , d − 1}. The relation (1 − cos x) = sin2 x2 and the approximation sin x ≈ x for values of x close to zero gives λ2(L) = Θ( d13 ). The minimax risk is thus upper bounded as O( dn4 ) and lower bounded as Ω( dn3 ). This class of graphs is thus strictly suboptimal.

• Cycle. A cycle is identical to a path except for an additional edge between node d and node 1.

The spectrum of the regular Laplacian is given by 2 1 − cos 2dπi , i ∈ {0, . . . , d − 1}, and that of

the scaled Laplacian is thus d2 1 − cos 2dπi , i ∈ {0, . . . , d − 1}. The relation (1 − cos x) = sin2 x2

and

the

approximation

sin x

≈

x

for

values

of

x

close

to

zero

gives

λ2(L)

=

Θ(

1 d3

).

The

minimax

risk is thus upper bounded as O( dn4 ) and lower bounded as Ω( dn3 ). This class of graphs is thus

strictly suboptimal.

• Barbell. The nodes are partitioned into two sets of d2 nodes each, and there is an edge between

every pair of nodes within each set. In addition, there is exactly one edge across the sets. The spec-

trum of the regular Laplacian can be computed as 0, Θ( d1 ), Θ(d), . . . , Θ(d). Since there are Θ(d2)

edges,

the

spectrum

of

the

scaled

Laplacian

turns

out

to

become

0

,

Θ(

1 d3

)

,

Θ(

1 d

),

.

.

.

,

Θ(

1 d

),

Ω(

1 d

).

12

Applying the results derived earlier in the paper, we get that a lower bound of Ω( dn3 ) and an upper bound of O( dn4 ) on the minimax risk, thereby also establishing the sub-optimality of this class of graphs.
• 2D Lattice. An (m1 × m2) lattice has d = m1m2 vertices arranged as a (m1 × m2) grid. Assume m1 = Θ(d) and m2 = Θ(d). This class of graphs can be written as a Cartesian product of a path graph of length m1 and a second path graph of length m2. As a result, the spectrum of the scaled Laplacian is d2 2 − cos mπi1 − cos mπj2 , ... i∈{0,...,m1−1},j∈{0,...,m2−1}. Again, using the small angle approximation of the sinusoid, one can compute an upper bound on the minimax risk as O( dn3 ) and a lower bound of Ω( dn2 ). We do not know at this point whether the 2D lattice minimizes the minimax risk.
• Hypercube. Assume d = 2m for some integer m. Representing each node as a distinct m-length binary vector, an edge exists between the nodes corresponding to any pair of vectors within a Hamming distance of one. The hypercube is an m-fold Cartesian product of a path with two nodes, and hence the regular Laplacian has an eigenvalue of 2i with multiplicity mi , for i ∈ {0, . . . , m}. The scaled Laplacian has an eigenvalue of d l2oig d with multiplicity mi , for i ∈ {0, . . . , m}. A lower bound on the minimax risk is Ω( dn2 ) and an upper bound is O( d2 lnog d ). We do not know if the hypercube is optimal, our bounds do tell us that any sub-optimality is bounded by at most a logarithmic factor.
Observe that the degree-k expander requires n ≥ kd samples while the complete graph requires n ≥ d2 samples, so in practical applications at least for small sample sizes we should prefer a low-degree expander.
Finally, if the conjecture in Section 3.2 were true, namely that the 2 minimax risk scales as σ2tr(L†)/n, then the condition tr(L†) = Θ(d2) would be necessary and suﬃcient for optimality of a comparison graph with the scaled Laplacian L. Observe that the graphs designated as ‘optimal’ in the discussion above indeed satisfy this condition. On the other hand, the graphs established as strictly suboptimal have tr(L†) = Ω(d3).

4.2 Experiments and simulations

This section evaluates the dependence of the squared 2-error on the topology of the comparison

graph. We consider the following ﬁve topologies: path, barbell, complete, expander and 2D-lattice.

In order to form an expander graph, we used the Gabber-Galil construction (Gabber and Galil,

1981). For any chosen graph topology, the n diﬀerence vectors are selected as one edge each chosen

uniformly at random (with replacement) from the comparison graph. Recall that our theory predicts

that the complete and expander graphs will perform the best, and that the line and dumbbell graphs

will fare the worst. Also recall that our theory predicts the error will scale as

w∗ − w

2 2

scales with

n as 1/n in the complete and expander topologies.

4.2.1 Experiments on synthetic data
This section describes simulations using data generated synthetically from the Thurstone model. In the simulations, we ﬁrst generate a quality score vector w∗ ∈ WB using one of the procedures described below. Once w∗ is chosen, the n pairwise comparisons for any given topology are generated as follows. An edge is selected uniformly (with replacement) at random from the underlying graph, and the chosen edge determines the pair of items compared. The outcome of the comparison is generated as per the Thurstone model with the chosen w∗ as the underlying quality score. Finally,

13

the maximum likelihood estimator for the Thurstone model is employed to estimate w∗. Every point in the plots is an average across 40 trials.
The following six procedures are employed to generated the true quality score vector w∗ in the six respective subﬁgures of Figure 3.
(a) Gaussian: w∗ is drawn from the standard normal distribution N (0, I).
(b) Uniform: w∗ is drawn uniformly at random from the set [−1, 1]d.
(c) Packing set for the path graph: We ﬁrst choose a vector z as by setting a value of 0 in the ﬁrst coordinate, a value −1 in d2 of the other coordinates chosen uniformly at random, and a value 1 in the remaining coordinates. Letting L = U T ΛU denote the eigen-decomposition of the Laplacian matrix of the path graph, w∗ is set as U T Λ†z, where Λ† is the Moore-Penrose pseudoinverse of Λ. This generation process mimics a construction used to prove the lower bound in Theorem 2, and tailors the construction for the path graph.
(d) Packing set for the barbell graph: The procedure is identical to that in (c), except that the Laplacian matrix used is that of the barbell graph.
(e) Packing set for the complete graph: The procedure is identical to that in (c), except that the Laplacian matrix used is that of the complete graph.
(f) Packing set for the star graph: The procedure is identical to that in (c), except that the Laplacian matrix used is that of the star graph.
The vector w∗ generated in this procedure is then scaled and shifted to ensure w∗ ∈ WB. The value of B and σ are set as 1.
Figure 3 plots the estimation error under various topologies of the comparison graph. Observe in the ﬁgure that the error is the lowest under the complete and the star graphs, and the highest under the barbell and the path graphs. In particular, the error consistently varies as Θ(d2/n) for the complete and star graphs – this phenomenon holds even in plots (e) and (f) where the procedure to choose w∗ forms the worst case for the complete and star graphs respectively according to the proof of Theorem 2. On the other hand, the minimax error varies as Ω(d3/n) in the worst case for the path and the barbell graphs. Finally, observe that in the simulations, the (constant) multiplicative factors to the term dn2 in the error turn out to be rather small, in the range of 0 to 9.
4.2.2 Experiments on MTurk
In this section, we describe the results of experiments conducted on the popular Amazon Mechanical Turk (https://www.mturk.com/; henceforth referred to as “MTurk”) commercial crowdsourcing platform, evaluating the eﬀects of the choice of the topology. MTurk is an online platform where individuals or businesses can put up a task, and any individual can log in and complete the tasks in exchange for a payment that is speciﬁed along with the task. In our experiments, each worker was oﬀered 20 cents per completed task. A worker was allowed to do no more than one task in an experiment. Workers were required to answer all the questions in a task. Only those workers who had 100 or more prior approved works and an approval rate of 95% or higher were allowed. Workers from any country were allowed to participate, except for the task of estimating distances between cities (for which only USA-based workers were permitted since all questions involved American cities).
We conducted three experiments that required the workers to make ordinal choices.
(a) Estimating areas of circles: In each question, the worker was shown a circle in a bounding box (Figure 5a), and the worker was required to identify the fraction of the box’s area that the circle occupied.
14

Rescaled error
|| ˆw−w ∗ ||22 n/d2

Rescaled error
|| ˆw−w ∗ ||22 n/d2

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

Rescaled error
|| ˆw−w ∗ ||22 n/d2

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

(a) Gaussian

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

Rescaled error
|| ˆw−w ∗ ||22 n/d2

(b) Uniform

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

(c) Packing set for the path graph

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

Rescaled error
|| ˆw−w ∗ ||22 n/d2

(d) Packing set for the barbell graph

30

25

Complete graph Star graph

20

Path graph Barbell graph

15

10

5

00 10 20 30 40 50 60 70
Number of items d

(e) Packing set for the complete graph

(f) Packing set for the star graph

Figure 3. Estimation error under diﬀerent topologies for diﬀerent generative processes in the synthetic simulations.

Rescaled error
|| ˆw−w ∗ ||22 n/d2

15

(a) Area of circle

(b) Age from photograph

(c) City distances

Figure 4: Estimation error under diﬀerent topologies in the experiments conducted on MTurk.

(b) Estimating age of people from photographs: The worker was shown photographs of people (Figure 5b) and was asked to estimate their ages.
(c) Estimating distances between pairs of cities: Pairs of cities were listed (Figure 5c) and for each pair, the worker had to estimate the distance between them.
For each experiment, we recruited 140 workers on MTurk, and assigned them to one of the ﬁve topologies uniformly at random. In this experiment and others involving aggregation of ordinal data from MTurk, the aggregation procedure follows maximum likelihood estimation under the Thurstone model, and the estimator is supplied the best-ﬁtting value of σ obtained via 3-fold cross-validation. Each run of the estimation procedure employs the data provided by ﬁve randomly chosen workers from the pool of workers who performed that task. The entire data pertaining to these experiments is available on the ﬁrst author’s website.
Figure 4 plots the squared 2 estimation error for the three experiments under the ﬁve topologies considered. We see that the relative errors are generally consistent with our theory, with the complete graph exhibiting the best performance and the path graph faring the worst. On real datasets, model misspeciﬁcation can in some cases cause the outcomes to diﬀer from our theoretical predictions. Understanding the eﬀect of model misspeciﬁcation, especially on topology considerations, is an important question we hope to address in future work.
5. Cardinal versus ordinal measurements
In this section, we compare two approaches towards eliciting data: a score-based “cardinal” approach and a comparison-based “ordinal” approach. In a cardinal approach, evaluators directly enter numeric scores as their answers (Figure 1b), while an ordinal approach involves comparing (pairs of) items (Figure 1a).
There are obvious advantages and disadvantages associated with either approach. On one hand, the cardinal approach allows for very ﬁne measurements. For instance, the cardinal measurements in Figure 1 can take any value between 0 and 100, whereas an ordinal measurement is binary. One might be tempted to go even further and argue that ordinal measurements necessarily give less information, for one can always convert a set of cardinal measurements into ordinal, simply by ordering the measurements by value. If this conversion were valid, the data processing inequality (Cover and Thomas, 2012), would then guarantee that estimators based on ordinal data can never outperform estimators based on cardinal data. However, this conversion assumes that cardinal and ordinal measurements suﬀer from the same type of statistical ﬂuctuation. The following set of experiments show this assumption is false.
5.1 Raw data from MTurk
We conducted seven diﬀerent experiments on MTurk to investigate the possibility of a “dataprocessing inequality” between the elicited cardinal and ordinal responses: Are responses elicited
16

or%a% rm%
cure”
%OLDER?%
!%

healthcare%plaMorm%

between%these%ci.es?%

“Simple, fast but sure cure”

But that is the begin% ning of a new stor
sStoaryn$oFf rthaengcriasdcuaol $arennedua$Al oufsa.mna

/%10%

!!% %

!!% %

miles% story of his gradual regeneration, of his
from one world into another, of his intiat a new unknown life. That might be the

%Which%circle%is%BIGGER?%%

How%many%words%are%misspelled% in%this%paragraph?%%

of a new story, but our present story is e
words%are%misspell

Which%pair%of%ci.es%is%farther%

Which circle is BIGGER?But tWhaht ois%dthoe%yboeug%tinhnininkg%iso%fOaLDneEwR?s%toWryh-atth%aiesw%tahye%%fdriostman%ecaec%bhe%otwtheeern?%%% Which%tone%corresponds%to

story of the gradual reneual of a mtahne, %fthoellowing%pairs%of%ci4es?%
story of his gradual regeneration, of his pasing

HIGHE%RW%nhuimchb%iemr%aogne%a%%ips%hmoonree%k%ree

!%

!%

from one world into another, of his intiatioSnainnC$tFohraaarnnlcodi2$s$ceo%$$$anSda$nA$uFasr.nandn%$c%$%isco$$ for%the%search%query%‘INT

¢

¢

aofnaenweuwnsktno!oryw,%nbulitfeo.uTrhpartesme!ingt%hst tboerythiseesnudbejedc.Bt oston$$ milesA%us.n%%%

!%

!%

!%

!%

(a)

word(sb%)are%misspelled%

(c)

How many words are
What%ism%thissep%delilestdainnctheis%%paragraph?

between%these%ci.es? But that is the beginning of a story of the gradual rene% ual

new story of a man,

the the

San$Francisco$and$Aus.n%%% stor%y of his gradual regeneration, of his pasing from one world into another, of his intiation into a new unknown life. That might be the subject

miles% of a new story, but our present story is ended.
words are misspelled

(d)

Which%sound%has%a%% HIGHER%frequency?%

!%

!%

(e)

%WRahticeh%%tihmiasg%tea%igs%lminoer%ef%orerl%eav%ant% for%htheea%sltehacrcahr%equ%pelrayM%‘IoNrTmER%NET’?% “Simple, fast but sure cure”
/%10%

(f )

!%

!%

How%r!%eWle% vhaicnht%i%sc%itrhcisle%im%isa%gB
the%search%query%'INTERN
!%
/%100%

Figure 5. Screenshots of the tasks presented to the subjects. For each task, only one version (cardinal

or ordinal) is shown here.

Who%do%you%think%is%OLDER?% What%is%the%distance

%Which%image%is%more%relevant% How%relevant%is%this%image%for%%

the%following%pairs%o
%

foiprna%tiohrrsed%oisnfeaialtrefcomhrsm%?queOqeuruyirv%a‘eIlNxepnTteErtRiomNdeEnaTttas’?ol%ebattdhaieun%sesdetaobryccohﬁn%rqcstluueedlreicyi%tt'hIiNnagtTEctahRriNsdiEinsTa'gl?er%nesepraolnlysens oatndthtehecnasseu: bctorancvtSeiarntng-$Francisco$and$

ing cardinally collected data into ordinal (by subtracting pairs of respo!ns% es) often le!ad%s to a higher

mile

amount of noise as compared to that in data that is elicited directly in ordinal form.

The tasks were selected to have a broad coverage of several important subjective judgment

paradigms such as preference elicitation, knowledge elicitation, audio and visual perception and

skill utilization.
In!ad% dition to

the

thre!e e%xperiments

described

in

/%100%
Section

4.2.2,

we

conducted

the

following

fo%uWr hich%image%is%m

experiments.

for%the%search%quer

(d) Finding spelling mistakes in text: The worker had to identify the number of words that were misspelled in each paragraph shown (Figure 5d).

(e) Identifying sounds: The worker was presented with audio clips, each of which was the sound of a single key on a piano (which corresponds to a single frequency). The worker had to estimate !% the frequency of the sound in each audio clip (Figure 5e).

(f) Rating tag-lines for a product: A product was described and tag-lines for this product were shown (Figure 5f). The worker had to rate each of these tag-lines in terms of its originality, clarity and relevance to this product.

(g) Rating relevance of the results of a search query: Results for the query ‘Internet’ for an image search were shown (Figure 1) and the worker had to rate the relevance of these results with respect to the given query.

Note that the data collected for (a)–(c) here was diﬀerent and independent of the data collected for these tasks in Section 4.2.2.
The number of items d in the experiments ranged from 10 to 25. For each of the seven experiments, we recruited 100 workers, and assigned each worker to either the ordinal or the cardinal version of the task at random. Upon obtaining the data, we ﬁrst reduced the cardinal data obtained from the experiments into ordinal form by comparing answers given by the subjects to consecutive questions. For ﬁve of the experiments ((a) through (e)), we had access to the “ground truth” solutions, using

17

Task Error in Ordinal
Std. dev. Error in Cardinal
Std. dev. Time in Ordinal
Std. dev. Time in Cardinal
Std. dev.

Circle 6% .23 17% .31 98s 21.1 181s 39.9

Age 13% .33 17% .38 31s 14.3
70s 33.1

Distance 17% .38 20% .38 84s 62.1 144s 56.2

Spelling 40% .49 42% .46 316s 33.2 525s 46.0

Audio 20% .40 29% .43 66s 11.1 134s 12.4

Tagline 44% .47 42% .46 251s 28.1 342s 44.6

Relevance 31% .44 35% .44 105s 13.1 185s 28.2

Table 1. Comparison of the average amount of error when ordinal data is collected directly versus when cardinal data is collected and converted to ordinal. Also tabulated is the median time (in seconds) taken to complete a task by a subject in either type of task.

which we computed the fraction of answers that were incorrect in the ordinal and the cardinalconverted-to-ordinal data (any tie in the latter case was counted as half an error). For the two remaining experiments ((f) and (g)) for which there is no ground truth, we computed the ‘error’ as the fraction of (ordinal or cardinal-converted-to-ordinal) answers provided by the subjects that disagreed with each other. It is important to note that in the experiments in this section, we did not run any estimation procedure on the data: we only measured the noise in the raw responses. The entire data pertaining to these experiments, including the interface seen by the workers and the data obtained from their work, is available on the ﬁrst author’s website.
The results are summarized in Table 1. If the cardinal measurements could always be converted to ordinal ones with the same noise level as directly eliciting ordinal responses, then it would be unlikely for the amount of error in the ordinal setting to be smaller than that in the cardinal setting. Table 1 shows that converting cardinal data to an ordinal form very often results in a higher (and sometimes signiﬁcantly higher) per-sample error in the (raw) responses than direct elicitation of ordinal evaluations. Such an outcome may be explained by the argument that the inherent evaluation process in humans is not the same in the cardinal and ordinal cases: humans do not perform an ordinal evaluation by ﬁrst performing cardinal evaluations and then comparing them (Barnett, 2003; Stewart et al., 2005). One can also see from Table 1 that the amount of time required for cardinal evaluations was typically (much) higher than for ordinal evaluations. One can thus assume that we will typically have the per-observation error in the ordinal case lower than that in the cardinal case. In particular, if we consider the Thurstone and the Cardinal models (introduced in Section 2.1), we can assume that σ < σc.
5.2 Analytical comparison of Cardinal versus Ordinal
As discussed earlier, while cardinal measurements allow more ﬂexibility in the range of responses, ordinal measurements contain a lower per-sample error. Ordinal measurements have additional beneﬁts in that they avoid calibration issues that are frequently encountered in cardinal measurements (Tsukida and Gupta, 2011), such as the evaluators’ inherent (and possibly time-varying) biases, or tendencies to give inﬂated or conservative evaluations. Ordinal measurements are also recognized to be easier or faster for humans to make (Barnett, 2003; Stewart et al., 2005), allowing for more evaluations with the same amount of time, eﬀort and cost.
The lack of clarity regarding when to use a cardinal versus an ordinal approach forms the motivation of this section. Can we make as reliable estimates from paired comparisons as from numeric scores? How much lower does the noise have to be for comparative measurements to be preferred
18

over cardinal measurements? The answers to these questions will help in determining how responses
should be elicited.
In order to compare the cardinal and ordinal methods of data elicitation, we focus on a setting
with evenly budgeted measurements. In accordance with the ﬁxed-design setup assumed throughout
the paper, we choose the vectors xi a priori. Suppose that n is large enough, and that in the ordinal case we compare each pair n/ d2 times. In the cardinal case suppose that we evaluate the quality of each item n/d times. We consider the Gaussian-noise models Thurstone and Cardinal introduced earlier in Section 2.1. In order to capture the fact that the amount of noise is diﬀerent in the cardinal
and ordinal settings, we will denote the standard deviation of the noise in the cardinal setting as σc, and retain our notation of σ for the noise in the ordinal setting. In order to bring the two models
on the same footing, we measure the error in terms of the squared 2-norm. Let γG and ζG denote the parameters γ and ζ (deﬁned in (1) and (6) respectively) specialized to
the Gaussian distribution. Deﬁne b (σ, B) : = ζG(cB2 ,σ) , bu(σ, B) : = c2γuGζG(B(B,σ,)σ) and b(σ, B) : = ζcG2σB22 . Observe that b , bu and b are independent of the parameters n and d.
With these preliminaries in place, we now compare the minimax error in the estimation under
the cardinal and ordinal settings.

Proposition 5 Given a sample size n that is a multiple of d(d − 1)b(σ, B), suppose that we observe each coordinate n/d times under the Cardinal model. Then the minimax risk is given by

inf sup E
w w∗∈WB

w − w∗ 22 = σc2 nd .

(12a)

Similarly, if we observe each pair n/ d2 times in the Thurstone model, then the minimax risk is sandwiched as

σ2b

d (σ, B)

≤

inf

sup

E

w − w∗ 2

≤ σ2bu(σ, B) d .

n w w∗∈W

2

n

B

(12b)

In the cardinal case, when each coordinate is measured the same number of times, the Cardinal model reduces to the well-studied normal location model, for which the MLE is known to be the minimax estimator and its risk is straightforward to characterize (see Lehmann and Casella (1998) for instance). In the ordinal case, the result follows from the general treatment in Section 3.
Let us now return to the question deciding between the cardinal and the ordinal methods of data elicitation. Suppose that we believe the Gaussian-noise models to be reasonably correct, and the per-observation errors σ and σc under the two settings are known or can be separately measured. Proposition 5 shows that the scaling of the minimax error in the cardinal and ordinal settings is identical in terms of the problem parameters n and d. As an important consequence, our result thus allows for the choice to be made based only on the parameters (σ, σc, B), and independent of n and d: the ordinal approach incurs a lower minimax error when bu(σ, B)σ2 < σc2 while the cardinal approach is better oﬀ in terms of minimax error whenever b (σ, B)σ2 > σc2. Establishing the exact decision boundary would require tightening the constants in the bounds, a task we leave for future work.

5.3 Aggregate Estimation Error in Experiments on MTurk
For the sake of completeness, we also computed the estimation error in the cardinal and ordinal settings. We consider data from the three experiments (c), (d) and (e).2 We normalize the true
2. We restrict attention to these three experiments for the following reasons. There is no ground truth for experiments (f) and (g). In experiment (a), the size of each circle in each question is chosen independently from a continuous distribution, making all questions diﬀerent and preventing aggregation. Experiment (b) employs a disconnected topology.

19

vector to have w∗ ∞ = 1 and set B = 1. For each of the three experiments, we execute 100 iterations of the following procedure. Select ﬁve workers from the cardinal and ﬁve from the ordinal pool of workers uniformly at random. (The number ﬁve is inspired by practical systems (Wang et al., 2011; Piech et al., 2013).) We run the maximum-likelihood estimator of the Cardinal model on the data from the ﬁve workers selected from the cardinal pool, and the maximum-likelihood estimator of the Thurstone model on the data from the ﬁve workers of the ordinal pool. Note that unlike Section 5.1, the cardinal data here is not converted to ordinal.

Task w∗−d w 22 in Ordinal w∗−d w 22 in Cardinal Kendall-tau coeﬃcient in Ordinal
Kendall-tau coeﬃcient in Cardinal

Spelling
0.358 ± 0.035
0.350 ± 0.045 0.277 ± 0.049 0.129 ± 0.046

Distance
0.168 ± 0.026
0.330 ± 0.028 0.547 ± 0.034 0.085 ± 0.038

Audio
0.444 ± 0.055
0.508 ± 0.053 0.513 ± 0.047 0.304 ± 0.049

Table 2: Evaluation of the inferred solution from the data received from multiple workers.
The results are tabulated in Table 2. To put the results in perspective of the rest of the paper, let us also recall the per-sample errors in these experiments from Table 1. Observe that among these three experiments, the per-sample noise in the cardinal data was closest to that in the ordinal data in the experiment on identifying the number of spelling mistakes. The gap was larger in the two remaining experiments. This fact is reﬂected in the results of Table 2 where the estimator on the cardinal data incurs a lower 2-error than the estimator on the ordinal data in the experiment on identifying the number of spelling mistakes, whereas the outcome goes the other way in the two remaining experiments. Our theory needs to tighten the constants in order to address this regime.

6. Conclusions
In this paper, we presented topology-aware minimax error bounds under a broad class of preferenceelicitation models. We demonstrated the utility of these results in guiding the selection of comparisons and in guiding the choice of the elicitation paradigm (cardinal versus ordinal) when these options are available. One potential direction for future work would be to investigate improved data collection mechanisms, for instance adaptive schemes where we focus our eﬀort on the most noisy comparisons. A second direction would be to characterize the precise thresholds for making the choice between the cardinal and ordinal approaches. Finally, the Thurstone and BTL models are parametric idealizations that have proved useful in a wide variety of applications. In future work we would like to investigate more ﬂexible semi-parametric and non-parametric pairwise comparison models (see, for instance, Chatterjee (2014); Braverman and Mossel (2008)).
Acknowledgments
This work was partially supported by Oﬃce of Naval Research MURI grant N00014-11-1-0688, MURI grant 96045-23800, and National Science Foundation Grants CIF-31712-23800, DMS-1107000 and CIF-81652-23800. The work of N.S. was also partially supported by a Microsoft Research PhD fellowship.

Appendix A. Proof of Theorem 1
The following two sections prove the lower and upper bounds (respectively) on the minimax risk of Ordinal model under the squared L semi-norm.

20

A.1 Lower bound
Our lower bounds are based on the Fano argument, which is a standard method in minimax analysis (see for instance Tsybakov (2008)). Suppose that our goal is to bound the minimax risk of estimating a parameter w over an indexed class of distributions P = {Pw | w ∈ W} in the square of a pseudometric ρ. Consider a collection of vectors {w1, . . . , wM } contained within W such that

min ρ wj, wk ≥ δ and
j,k∈[M ]
j=k

1

M

DKL(Pwj Pwk ) ≤ β.

2 j,k∈[M ]

j=k

We refer to any such subset as an (δ, β)-packing set.

Lemma 6 (Pairwise Fano minimax lower bound) Suppose that we can construct a (δ, β)-packing with cardinality M . Then the minimax risk is lower bounded as

inf sup E ρ(w, w∗)2 ≥ δ2 1 − β + log 2 . (13)

w w∗∈W

2

log M

In order to apply Lemma 6, we need to a construct a suitable packing set. Given a scalar α ∈ (0, 14 ) whose value will be speciﬁed later, deﬁne the integer

d

M (α) : = exp log 2 + 2α log 2α + (1 − 2α) log(1 − 2α) .

(14)

2

We require the following two auxiliary lemmas:
Lemma 7 For any α ∈ (0, 41 ), there exists a set of M (α) binary vectors {z1, . . . , zM(α)} ⊂ {0, 1}d such that

αd ≤

zj − zk

2 2

≤

d

e1, zj = 0

for all j = k ∈ [M (α)], and for all j ∈ [M (α)],

(15a) (15b)

where e1 denotes the ﬁrst canonical basis vector.

This result is a straightforward consequence of the Gilbert-Varshamov bound (Gilbert, 1952; Varshamov, 1957).

Lemma 8 For any pair of quality score vectors wj and wk, and for

max F (x)

x∈[0,2B/σ]

ζ :=

,

F (2B/σ)(1 − F (2B/σ))

we have DKL(Pwj Pwk ) ≤ nσζ2 (wj − wk)T L(wj − wk). (16)

We prove this lemma at the end of this section.

Taking these two lemmas as given for the moment, consider the set {z1, . . . , zM(α)} of d-dimensional binary vectors given by Lemma 7. The Laplacian L of the comparison graph is symmetric and

21

positive-semideﬁnite, and so has a diagonalization of the form L = U T ΛU where U ∈ Rd×d is an orthonormal matrix, and Λ is a diagonal matrix of nonnegative eigenvalues.
Letting Λ† denote the Moore-Pen√rose pseudo-inverse of Λ, consider the collection {w1, . . . , wM(α)} of vectors given by wj : = √δ U T Λ†zj for each j ∈ [M (α)]. Since 1 ∈ nullspace(L), we are
d√ guaranteed that 1, wj = √δ 1T U T Λ†zj = 0. On the other hand,
d

(wj

−

wk)T L(wj

−

wk)

≤

δ2 (zj

−

√

√

zk)T Λ†U LU T Λ†(zj

−

zk)

d

=

δ2 (zj

−

√√ zk) Λ†Λ Λ†(zj

−

zk)

d

δ2 j k 2

= z −z d

2,

Here the last step makes use of the fact that the ﬁrst coordinate of each vector zj and zk is zero. It

follows that αδ2 ≤

wj − wk

2 L

≤

δ2.

Setting δ2 : = 0.01 σn2ζd , we ﬁnd that

wj ∞ ≤ √δ √Λ†zj 2 (≤i) √δ

d

d

tr(Λ†)

(ii)
=

δ √

d

(iii)
tr(L†) ≤ B,

where inequality (i) follows from the fact that zj has entries in {0, 1}; equation (ii) follows since L† = U T Λ†U by deﬁnition; and inequality (iii) follows from our choice of δ and our assumption n ≥ cσ2ζtBr(2L†) on the sample size with c = 0.01. We have thus veriﬁed that each vector wj also satisﬁes the boundedness constraint wj ∞ ≤ B required for membership in WB. Finally, observe
that

nζ δ 2

j

k2

2

max DKL(Pwj Pwk ) ≤
j=k

σ2 ,

and

min w − w
j=k

L ≥ αδ .

We have thus constructed a suitable packing set for applying Lemma 6, which yields the lower bound

α

δ2ζ2n + log 2

[ w − w∗ 2L] ≥ δ2 1 − σ

.

E

2

log M (α)

Substituting our choice of δ and setting α = 0.01 proves the claim for d > 9.

In order to handle the case d ≤ 9, we consider the set of the three d-length vectors given by

z1 = [0 · · · 0 − 1], z2 = [0 · · · 0 1] and z3 = [0 · · · 0 0]. Construct the packing set {w1, w2, w3}

from these three vectors {z1, z2, z3} as done above for the case of d > 9. From the calculations made

for the general case above, we have for all pairs minj=k

wj −wk

2 L

≥

δ2 9

and

maxj,k

wj −wk

2 L

≤

4δ2,

and as a result maxj,k DKL(Pwj Pwk ) ≤ 4nσζ2δ2 . Choosing δ2 = σ28lnoζg 2 and applying Lemma 6 proves

the theorem.

The only remaining detail is to prove Lemma 8.

Proof of Lemma 8: For any pair of quality score vectors wj and wk, the KL divergence between the distributions Pwj and Pwk is given by

n j

F ( wj, xi /σ)

j

1 − F ( wj, xi /σ)

DKL(Pwj Pwk ) =

F(

w

,

xi

/σ) log

F(

wk,

xi

/σ)

+ (1 − F (

w

,

xi

/σ)) log

1 − F(

wk,

xi

. /σ)

i=1

22

For any a, b ∈ (0, 1), we have the elementary inequality a log ab ≤ (a − b) ab . Applying this inequality to our expression above gives

DKL(Pwj

n j

k

F ( wj, xi /σ)

Pwk ) ≤

(F (

w

,

xi

/σ) − F (

w

,

xi

/σ)) F(

wk,

xi

/σ)

i=1

j

k

1 − F ( wj, xi /σ)

− F ( w , xi /σ)) − F ( w , xi /σ) 1 − F ( wk, xi /σ)

n (F ( wj, xi /σ) − F ( wk, xi /σ))2

≤

F(

wk,

xi

/σ)(1 − F (

wk,

xi

. /σ))

i=1

Since max{ wj ∞, wk ∞} ≤ B, and since F is a non-decreasing function, we have

n (F ( wj, xi /σ) − F ( wk, xi /σ))2

DKL(Pwj Pwk ) ≤

. F (2B/σ)(1 − F (2B/σ))

i=1

Finally, applying the mean value theorem and recalling the deﬁnition of ζ (from (6)) yields

n j

k

2

nζ j

kT

j

k

DKL(Pwj Pwk ) ≤ ζ( w , xi /σ − w , xi /σ) = σ2 (w − w ) L(w − w ),

i=1

as claimed.

A.2 Upper bound

For the Ordinal model, the MLE is given by wˆ ∈ arg min (w), where
w∈WB

1n

xi, w

xi, w

(w) = − n

1[yi = 1] log F σ + 1[yi = −1] log 1 − F σ

, and (17a)

i=1

WB : = w ∈ Rd | 1, w = 0, and w ∞ ≤ B .

(17b)

Our goal is to bound the estimation error of the MLE in the squared semi-norm

v

2 L

=

vT

Lv.

For the purposes of this proof (as well as subsequent ones), let us state and prove an auxiliary

lemma that applies more generally to M -estimators that are based on minimizing an arbitrary convex

and diﬀerentiable function over some subset W of the set W∞ : = {w ∈ Rd | 1, w = 0}. The MLE

under consideration here is a special case. This lemma requires that is diﬀerentiable and strongly

convex at w∗ with respect to the semi-norm · L, meaning that there is some constant κ > 0 such

that

(w∗ + ∆) −

(w∗) −

∇ (w∗), ∆

≥κ

∆

2 L

(18)

for all perturbations√∆ ∈ Rd such that (w∗ + ∆) ∈ W. Finally, it is also convenient to introduce the semi-norm u L† = uT L†u, where L† is the Moore-Penrose pseudo-inverse of L.

Lemma 9 (Upper bound for M -estimators) Consider the M -estimator

w ∈ arg min (w), where W is any subset of W∞,

(19)

w∈W

and is a diﬀerentiable cost function satisfying the κ-strong convexity condition (18) at some w∗ ∈ W. Then

w − w∗

1 L≤

∇ (w∗)

L† .

(20)

κ

23

Proof Since w and w∗ are optimal and feasible, respectively, for the original optimization problem, we have (w) ≤ (w∗). Deﬁning the error vector ∆ = w − w∗, adding and subtracting the quantity ∇ (w∗), ∆ yields the bound
(w∗ + ∆) − (w∗) − ∇ (w∗), ∆ ≤ − ∇ (w∗), ∆ .
By the κ-convexity condition, the left-hand side is lower bounded by κ ∆ 2L. As for the righthand side, note that ∆ satisﬁes the constraint 1, ∆ = 0, and thus is orthogonal to the nullspace of the Laplacian matrix L. Therefore, by Lemma 16 (in Appendix F), we have | ∇ (w∗), ∆ | ≤
∇ (w∗) L† ∆ L. Combining the pieces yields the claimed inequality (20).

In order to apply Lemma 9 to the MLE for the Ordinal model, we need to verify that the negative log likelihood (17a) satisﬁes the strong convexity condition, and we need to bound the random variable ∇ (w∗) L† deﬁned in the dual norm · L†.

Verifying strong convexity: By chain rule, the Hessian of is given by

2

1n

∇ (w) = nσ2

i=1

1[yi = 1]Ti1 + 1[yi = −1]Ti2

x

i

x

T i

,

where

F ( w, xi )2 − F ( w, xi )F ( w, xi )

F ( w, xi )2 + (1 − F ( w, xi ))F ( w, xi )

Ti1 : =

σ

σ

σ , and Ti2 : =

σ

σ

σ.

F ( w,σxi )2

(1 − F ( w,σxi ))2

Observe that the term Ti1 is simply the second derivative of log F evaluated at w,σxi , and hence the strong log-concavity of F implies Ti1 ≥ γ. On the other hand, the term Ti2 is the second derivative of log(1 − F ). Since F (−x) = 1 − F (x) for all x, it follows that the function x → 1 − F (x) is also strongly log-concave with parameter γ and hence Ti2 ≥ γ. Putting together the pieces, we conclude that
vT ∇2 (w)v ≥ nγσ2 Xv 22 for all v, w ∈ WB,

where X ∈ Rn×d has the diﬀerencing vector xi ∈ Rd as its ith row. Thus, if we introduce the error vector ∆ : = w − w∗, then we may conclude that

(w∗ + ∆) − (w∗) − ∇ (w∗), ∆ ≥ nγσ2 X∆ 22 = σγ2 ∆ 2L,

showing that is strongly convex around w∗ with parameter κ = σγ2 . An application of Lemma 9

then gives

∆

2 L

≤

σ4 γ2

∇ (w∗)

2L† .

Bounding the dual norm: In order to obtain a concrete bound, it remains to control the quantity ∇ (w∗)T L†∇ (w∗). Observe that the gradient takes the form

∗ −1 n

F ( w∗, xi /σ)

F ( w∗, xi /σ)

∇ (w ) = nσ

1[yi = 1] F ( w∗, xi /σ) − 1[yi = −1] 1 − F ( w∗, xi /σ) xi.

i=1

Deﬁne a random vector V ∈ Rn with independent components as

Vi =

F ( w∗, xi /σ) F ( w∗, xi /σ) −F ( w∗, xi /σ) 1−F ( w∗, xi /σ)

w.p. w.p.

F ( w∗, xi /σ) 1 − F ( w∗, xi /σ).

24

With this notation, we have ∇ (w∗) = − n1σ XT V . One can verify that E[V ] = 0 and

F (z) F (z)

F (z)

|Vi| ≤ z∈[−2Bsu/pσ,2B/σ] max F (z) , 1 − F (z) ≤ z∈[−2Bsu/pσ,2B/σ] F (z)(1 − F (z)) ≤ ζ, (21)

where ζ is as deﬁned in (6). Deﬁning the n-dimensional square matrix M : = γσ2n22 XL†XT , our

deﬁnitions and previous bounds imply that

∆

2 L

≤ V TMV .

Consequently, our problem has been reduced to controlling the ﬂuctuations of the quadratic form

V T M V ; in order to do so, we apply the Hanson-Wright inequality (see Lemma 13 in Appendix E).

A straightforward calculation yields

2

σ4

σ2

|||M |||fro = (d − 1) γ4n2 and |||M |||op = γ2n ,

where we have used the fact that L = n1 XT X. Moreover, since the components of V are independent and of zero mean, a straightforward calculation yields that E[V T M V ] ≤ E[ V 2∞tr(M )] ≤ ζγ2σ2n2d .
Since |Vi| ≤ ζ, the variables are ζ-sub-Gaussian, and hence the Hanson-Wright inequality implies
that

T

ζ2σ2d

t2γ4n2 tγ2n

P V M V − γ2n > t ≤ 2exp − c min{ ζ4(d − 1)σ4 , ζ2σ2 }

for all t > 0.

Consequently, after some simple algebra, we conclude that

2 cζ2σ2 d

−t

P ∆ L > t γ2 n ≤ e for all t ≥ 1,

for some universal constant c. Integrating this tail bound yields the bound on the expectation.

Appendix B. Proof of Theorem 2
The following two sections prove the upper and lower bounds (respectively) on the minimax risk in the squared Euclidean norm for Ordinal model. We prove the lower bound in two parts corresponding to the two components of the “max” in the statement of the theorem.

B.1 Upper bound

The proof of the upper bound under the Euclidean norm follows directly from the upper bound

under the L semi-norm proved in Theorem 1. From the setting described in Section 2, we have that

the nullspace of the matrix L is given by the span of the all ones vector. Furthermore, we have

w∗ − w, 1

= 0, and

w∗ − w

2 L

≥

λ2(L)

w∗ − w

22.

Substituting this inequality into the upper

bound (7b) gives the desired result.

B.2 Lower bound: Part I
Since the Laplacian L of the comparison graph is symmetric and positive-semideﬁnite. By diagonalization, we can write L = U T ΛU where U ∈ Rd×d is an orthonormal matrix, and Λ is a diagonal matrix of nonnegative eigenvalues with Λjj = λj(L).
We ﬁrst use the Fano method (Lemma 6) to prove that the minimax risk is lower bounded as cσ2 dn2 . For scalars α ∈ (0, 41 ) and δ > 0 whose values will be speciﬁed later, recall the set {z1, . . . , zM(α)} of vectors in the Boolean hypercube {0, 1}d given by Lemma 7. We then deﬁne a

25

second set {wj, j ∈ [M (α)]} via wj : = √δ U T P zj, where P is a permutation matrix to be speciﬁed
d

momentarily. At this point, the only constraint imposed on P is that it keeps the ﬁrst coordinate

constant. By construction, for each j = k, we have

wj − wk

2 2

=

δ2 d

zj − zk

2 2

≥

αδ2,

where

the

ﬁnal inequality follows from the fact that the set {z1, . . . , zM(α)} comprises binary vectors with a

minimum Hamming distance at least αd.

Consider any distinct j, k ∈ [M (α)]. Then, for some {i1, . . . , ir} ⊆ {2, . . . , d} with αd ≤ r ≤ d, it

must be that

j

k 2 δ2 T j

T k 2 δ2 j k 2 δ2 r

w − w L = d U P z − U P z L = d z − z Λ = d λim(L).

m=1

It follows that for some non-negative numbers a2, . . . , ad such that αd ≤

d i=2

ai

≤

d,

1

j

k 2 δ2 d

M (α)

w − w L = d aiλi(L).

2 j=k

i=2

We choose the permutation matrix P such that the last (d − 1) coordinates are permuted to have a2 ≥ · · · ≥ ad and the dth coordinate remains ﬁxed. With this choice, we get

1

j

k 2 δ2 d

2δ2

M (α)

w −w

L≤

d

tr(L) ≤ d−1

tr(L). d

2 j=k

Lemma (14) (Appendix F) gives the trace constraint tr(L) = 2, which in turn guarantees that

1
(M2(α))

j=k wj − wk 2L ≤ 4dδ2 . For the choice of P speciﬁed above, we have for every j ∈ [M (α)],

1, wj = √δd eT1 P zj = eT1 zj = 0,

where the ﬁnal equation employed the property (15b).

Setting δ2 = 0.01 σ42ndζ2 , we have wj ∞ ≤ √δd zj 2 (≤i) δ (≤ii) B, where inequality (i) follows from the fact that zj has entries in {0, 1}; inequality (ii) follows from our choice of δ and our assumption
n ≥ cσ2ζtBr(2L†) on the sample size with c = 0.002, where Lemma 14 guarantees n ≥ c4σζ2Bd22 . We have thus veriﬁed that each vector wj also satisﬁes the boundedness constraint wj ∞ ≤ B required for

membership in WB.

From the proof of Theorem 1, we have that for any distinct DKL(Pwj

Pwk )

≤

nζ σ2

wj − wk

2L, and

hence

1

nζ 4δ2

M (α)

DKL(Pwj Pwk ) ≤ σ2 d = 0.01 d,

2 j=k

where we have substituted our previous choice of δ. Applying Lemma 6 with the packing set {w1, . . . , wM(α)} gives

αδ2 0.01d + log 2 Mn θ(P); ρ ≥ 2 1 − log M (α) .

Substituting our choice ofδ and setting α = 0.01 proves the claim for d > 9.

For the case of d ≤ 9, consider the set of the three d-length vectors z1 = [0 · · · 0 − 1],

z2 = [0 · · · 0 1] and z3 = [0 · · · 0 0]. Construct the packing set {w1, w2, w3} from these three

vectors {z1, z2, z3} as done above for the case of d > 9. From the calculations made for the general

case above, we have for all pairs minj=k

wj − wk

2 2

≥

δ2 9

and

maxj,k

wj − wk

2 L

≤

4δ2,

and

as

a

result maxj,k DKL(Pwj

Pwk )

≤

4nσζ2δ2 .

Choosing

δ2

=

σ2 log 2 8nζ

and

applying

Lemma

6

yields

the

claim.

26

B.3 Lower bound: Part II

Given an integer d ∈ {2, . . . , d}, and scalars α ∈ (0, 41 ) and δ > 0, deﬁne the integer

d

M (α) : = exp log 2 + 2α log 2α + (1 − 2α) log(1 − 2α) .

(22)

2

Applying Lemma 7 with d as the dimension yields a subset {z1, . . . , zM (α)} of the Boolean hypercube {0, 1}d with the stated properties. We then deﬁne a set of d-length vectors {w1, . . . , wM (α)} via

wj = [0 (zj)T 0 · · · 0]T for each j ∈ [M (α)].
√ For each j ∈ [M (α)], let us deﬁne wj : = √δ U T Λ†wj. Now, letting e1 ∈ Rd denote the ﬁrst
d√ standard basis vector, we have 1, wj = √δ 1T U T Λ†wj = 0. where we have used the fact that
d
1 ∈ nullspace(L). Furthermore, for any j = k, we have

j

k 2 δ2 j

kT † j

k δ2

d

1

w −w

2=

(w d

−w

)

Λ (w

−w

)≥

d

. λi

i= (1−α)d

Thus, setting δ2 = 0.01 σn2ζd yields

wj

∞

≤

δ √

√Λ†wj 2 (≤i) √δ

d

d

tr(Λ†)

(ii)
=

δ √

d

(iii)
tr(L†) ≤ B,

where ine√quality (i)√follows from the fact that zj has entries in {0, 1}; step (ii) follows because the matrices Λ† and L† have the same eigenvalues; and inequality (iii) follows from our choice of δ and our assumption n ≥ cσ2ζtBr(2L†) on the sample size with c = 0.01. We have thus veriﬁed that each vector wj also satisﬁes the boundedness constraint wj ∞ ≤ B required for membership in WB.
Furthermore, for any pair of distinct vectors in this set, we have

j

k 2 δ2 j k 2 2

w −w L = d z −z 2 ≤δ .

From the proof of Theorem 1, we DKL(Pwj

Pwk )

≤

nζ σ2

wj − wk

2 L

≤

0.01d

.

Applying

Lemma

6

with

the packing set {w1, . . . , wM(α)} gives

Mn w(P); · 22 ≥ α2δ2 1 − 0l.o0g1M+ l(oαg)2 .

Substituting our choice of δ and setting α = 0.01 proves the claim for d > 9. For the case of d ≤ 9, we will show a lower bound of cσn2 λ29(L) for a universal constant c > 0. This
quantity is at lea√st as large as the claimed lower bound. Consider the packing set of three d-length vectors w1 = δU Λ†[0 1 0 · · · 0]T , w2 = −w1 and w3 = [0 · · · 0]T for some δ > 0. Then for every j = k, one can verify that wj − wk 2L ≤ 4δ2, wj − wk 22 ≥ λ2δ(2L) . Choosing δ2 = σ28lnoζg 2 and applying Lemma 6 proves the claim for d ≤ 9.
Finally, taking the maximum over all values of d ∈ {2, . . . , d} gives the claimed lower bound.

Appendix C. Proof of Theorem 3
We now turn to the proof of Theorem 3 on the minimax rate for the Paired Cardinal model. Recall that this observation model takes the standard linear model, y = Xw∗ + , where y ∈ Rn, w ∈ Rd and ∼ N (0, σ2I).

27

C.1 Upper bound under the squared L semi-norm

The maximum likelihood estimate in the Paired Cardinal model is a special case of the general

M -estimator (19) with (w) : = 21n

n i=1

yi −

xi, w

2. For this quadratic objective function, it is

easy to verify that the γ-convexity condition holds with γ = 1. (In particular, note that the Hessian

of is given by L = XT X/n.)

Given the result of Lemma 9, it remains to upper bound ∇ (w∗) L†. A straightforward com-

putation yields

∇ (w∗)

2 L†

=

σε T Q σε

where Q : =

nσ22 XL†XT .

Consequently, the random variable

∇

(w∗)

2 L†

is

quadratic

form

in

the

standard

Gaussian

random

vector

σε .

An application of Lemma 15

(Appendix F) gives tr(Q) = σn2 d − 1 and |||Q|||op = σn2 , and then applying a known tail bound on

Gaussian quadratic forms (see Lemma 12 in Appendix E) yields

∇

(w∗)

2 L†

P

σ2

≥

dδ +√

2 ≤ e− δ22

nn

for all δ ≥ 0.

Since d ≥ 2, we have σ nd + √σn δ 2 ≤ 2σ2ndδ2 for all δ ≥ 4, which yields

∗2

4σ2d

−t

P ∇ (w ) L† ≥ t n ≤ e

for all t ≥ 8.

Integrating this tail bound yields that E

∇

(w∗)

2 L†

≤ cσ2 nd , from which the claim follows.

C.2 Lower bound under the squared L semi-norm

Based on the pairwise Fano lower bound previously stated in Lemma 6, we need to construct a suitable (δ, β)-packing, where the semi-norm ρ(wj, wk) = wj − wk L is deﬁned by the Laplacian. Given the additive Gaussian noise observation model, we also have

DKL(Pwj Pwk ) = 2nσ2 wj − wk 2L,

(23)

The construction of the packing and the remainder of the proof proceeds in a manner identical to
the proof of the lower bound in Theorem 1, except for the absence of the requirement of wj ∞ ≤ B on the elements {wj} of the packing set.

C.3 Upper bound under the squared Euclidean norm

The upper bound follows by direct analysis of the (unconstrained) least-squares estimate, which has the explicit form w = n1 L†XT y, and thus

E w − w∗ 22 = E n1 L†XT 22 = σ2tr( n12 L†XT XL†)

where we have used the fact that ∼ N (0, σ2In). Since L = XT X/n by deﬁnition, we conclude that

E

w − w∗

2 2

=

σ2tr(L†) n

as claimed.

C.4 Lower bound under the squared Euclidean norm
We obtain the lower bound by computing the Bayes risk with respect to a suitably deﬁned (proper) prior distribution over the weight vector w∗. In particular, if we impose the prior w∗ ∼ N (0, σn2 L†), Bayes’ rule then leads to the posterior distribution
P w | y; X ∝ exp 2−σ12 y − Xw 22 exp 2−σn2 wT Lw 1{ w, 1 = 0}.

28

Thus conditioned on y, w is distributed as N (XT X + nL)−1XT y, σ22 L† . By applying iterated

expectations, the Bayes risk is given by E

w − 12 L†XT y

2 2

=

σ22 tr(L†), which completes the proof.

Appendix D. Proof of Theorem 4
This section presents the proof of Theorem 4 for the setting of m-wise comparisons. We ﬁrst state some simple properties of the model introduced in Section 3.3, which we will use subsequently in the proofs of the results.
Lemma 10 The Laplacian of the underlying pairwise-comparison graph satisﬁes the trace constraints nullspace(L) = 1, λ2(L) > 0 and tr(L) = m(m − 1).
Lemma 11 For any j ∈ [m], i ∈ [n] and any vector v ∈ Rm, we have
λ2m(H) vT (mI − 11T )v ≤ vT RjHRjT v ≤ λmamx(H) vT (mI − 11T )v. See Section D.2 for the proof of these auxiliary lemmas.

D.1 Upper bound under the squared L semi-norm
We prove this upper bound by applying Lemma 9. In this case, the rescaled negative log likelihood takes the form
(w) = − 1 n m 1[yi = j] log F wT EiRj , n
i=1 j=1

and the MLE is obtained by constrained minimization over the set WB : = w ∈ Rd | 1, w =
0, and w ∞ ≤ B . As in our proof of the upper bound in Theorem 1, we need to verify the κ-strong convexity condition, and to control the dual norm ∇ (w∗) L†.

Verifying strong convexity: The gradient of the negative log likelihood is

1n m

∇ (w) = − n

1[yi = j]EiRj ∇ log F (v) v=wT EiRj .

i=1 j=1

The Hessian of the negative log likelihood can be written as

2

1n m

2

TT

∇ (w) = n

1[yi = j]EiRj ∇ log F (v) v=wT EiRj Rj Ei .

i=1 j=1

Using our strongly log-concave assumption on F , we have that for any vector z ∈ Rd,

zT ∇2

(w)z

1n m

T

2

TT

=− n

1[yi = j]z EiRj ∇ log F (v) v=wT EiRj Rj Ei z

i=1 j=1

1n m

T

TT

≥ n

1[yi = j]z EiRjHRj Ei z

i=1 j=1

λ2(H) 1 n m

T

TT

≥ mn

1[yi = j]z Ei(mI − 11 )Ei z,

i=1 j=1

29

where the last step follows from Lemma 11. The deﬁnition (10) of L implies that

zT ∇2 (w)z ≥ λ2m(H) zT Lz = λ2m(H) z 2L.

Consequently, the κ-convexity condition holds around w∗ with κ = λ2m(H) . An application of Lemma 9 then yields

∗2

m2

∗2

m2

∗T †

∗

wML − w L ≤ λ2(H)2 ∇ (w ) L† = λ2(H)2 ∇ (w ) L ∇ (w ).

(24)

Controlling the dual norm: The gradient of the negative log likelihood can then be rewritten

as ∇ (w∗) = − n1

n i=1

EiVi,

where

each

index

i

∈

[n],

the

random

vector

vector

Vi

∈

Rm

is

given

by Vi : =

m j=1

1[yi

= j] Rj ∇ log F (

w∗, Ei

Rj ).

Now

observe

that

the

matrix

M

: = I − m1 11T

is

symmetric and positive semi-deﬁnite with rank (m − 1), eigenvalues {1, . . . , 1, 0}, its nullspace equals

the span of the all-ones vector, and that M † = M . Using this matrix, we deﬁne the transformed

vector

Vi

:=

(

M

†

)

1 2

Vi

for

each

i

∈

[n].

Consider a vector x and its shifted version x + t1, where t ∈ R and 1 denotes the vector of all

ones. By the shift invariance property, the function g(t) = F (x + t1) − F (x) is constant, and hence

g (0) = ∇F (x), 1 = 0, and g (0) = 1, ∇2F (x) 1 = 0,

(25)

which implies that 1 ∈ nullspace(∇2F (x)). Furthermore, we have ∇ log F (x), 1 = F (1x) ∇F (x), 1 = 0. Consequently, Vi, 1 = 0 = Vi, nullspace(M ) . This allows us to write

∗

1n

1

∗T †

∗

1 n n T 1 T†

1

∇ (w ) = − n

EiM 2 Vi, and ∇ (w ) L ∇ (w ) = n2

Vi M 2 Ei L E M 2 V .

i=1

i=1 =1

By deﬁnition, for every pair i = ∈ [n], Vi is independent of V . Moreover, for every i ∈ [n],

m

E[Vi]

=

E[(M

†)

1 2

1[yi = j]Rj ∇ log F (v) v=(w∗)T EiRj ]

j=1

m
= (M †) 12 F ((w∗)T EiRj )Rj ∇ log F (v) v=(w∗)T EiRj
j=1

m
= (M †) 12 Rj ∇F (v) v=(w∗)T EiRj .
j=1

In order to further evaluate this expression, deﬁne a function g : Rm → R as g(z) =

m j=1

F

(z

T

Rj

).

Then by deﬁnition we have g(z) = 1. Taking derivatives, we get 0 = ∇g(z) =

m j=1

Rj

∇F

(z

T

Rj

).

It follows that E[Vi] = 0, and hence that

∗T †

∗

1

nn T 1 T†

1

E[∇ (w ) L ∇ (w )] = n2 E[

Vi M 2 Ei L E M 2 V ]

i=1 =1

1

n T 1 T†

1

= n2 E[ Vi M 2 Ei L EiM 2 Vi]

i=1

1 ≤ E[ sup V
n ∈[n]

2 1n 1 T†

1

2]tr( n M 2 Ei L EiM 2 ).

i=1

30

Since L = mn V

n i=1

EiM EiT ,

we

have

tr( n1

n i=1

M

1 2

EiT

L†

Ei

M

1
2)

=

dm−1 ,

as

well

as

m
22 = 1[yi = j](∇ log F (v) v=(w∗)T EiRj )T RjT M Rj ∇ log F (v) v=(w∗)T EiRj .
j=1

Recalling the previously deﬁned matrix M , observe that since Rj is simply a permutation matrix, we have RjT M Rj = M for every j ∈ [m]. By chain rule, we have ∇ log F (v), 1 = F (1v) ∇F (v), 1 = 0, where the last step follows from our previous calculation. It follows that

E ∇ (w∗), L†∇ (w∗)

d

≤

sup

∇ log F (v) 2.

n v∈[−B,B]m

2

Substituting this bound into equation (24) yields the claim.

D.1.1 Lower bound under the squared L semi-norm
For any pair of quality score vectors wj and wk, the KL divergence between the distributions Pwj and Pwk is given by

nm jT

F (wjT EiRl)

DKL(Pwj Pwk ) = i=1 l=1 F (w EiRl) log F (wkT EiRl) .

Applying the inequality log x ≤ x − 1, valid for x > 0, we ﬁnd that

nm jT

F (wjT EiRl)

DKL(Pwj Pwk ) ≤ i=1 l=1 F (w EiRl) F (wkT EiRl) − 1 .

Now employing the fact that

m l=1

F

(wj T

EiRl)

=

m l=1

F (wkT EiRl)

=

1

gives

DKL(Pwj

nm
Pwk ) ≤
i=1 l=1

F (wjT EiRl)2

jT

kT

F (wkT EiRl) − 2F (w EiRl) + F (w EiRl) .

n m (F (wjT EiRl) − F (wkT EiRl))2

=
i=1 l=1

F (wkT EiRl)

1 ≤

nm
(F (wjT EiRl) − F (wkT EiRl))2

F (−B, B, . . . , B)

i=1 l=1

1 ≤

nm
( ∇F (zil), wjT EiRl − wkT EiRl )2,

F (−B, B, . . . , B)

i=1 l=1

for some zil ∈ [−B, B]m. Letting ζ = supz∈[F−(B−,BB],mB,.∇..,FB()z) 2H† and applying Lemma 16 (noting that wjT EiRl, nullspace(H) = 0 for all i, j, l) gives

DKL(Pwj

n
Pwk ) ≤

m

ζ

wj T EiRl − wkT EiRl

2 H

i=1 l=1

nm

≤ ζ(wj − wk)T

E

i

Rl

H

R

T l

EiT

i=1 l=1

≤ ζλm(H)n wj − wk 2L,

(wj − wk)

(26)

31

where the ﬁnal step is a result of Lemma 11. Consider the pair of scalars α ∈ (0, 41 ) and δ > 0 whose values will be speciﬁed later. Let M (α)
be as deﬁned in (14). Consider the packing set {w1, . . . , wM(α)} constructed in Appendix A.1. Each of these vectors is of length d, satisﬁes wj, 1 = 0, and furthermore, each pair from this set satisﬁes αδ2 ≤ wj − wk 2L ≤ δ2. Setting δ2 = 0.01 nζλmd (H) yields

DKL(Pwj Pwk ) ≤ 0.01d.

Every element from the packing set also satisﬁes
to the class WB. Applying Lemma 6 yields the lower bound

wj ∞ ≤ B when n ≥ 0ζ.0B12σλ2mtr((HL†)) , and thus belongs

w − w∗

2

≥

α 0.01

d

0.01d + log 2

1−

.

L 2 nζλm(H)

log M (α)

Setting α = 0.01 proves the claim for d > 9.

For the case of d ≤ 9, consider the set of the three d-length vectors z1 = [0 · · · 0 − 1],

z2 = [0 · · · 0 1] and z3 = [0 · · · 0 0]. Construct the packing set {w1, w2, w3} from these three

vectors {z1, z2, z3} as done above for the case of d > 9. From the calculations made for the general

case above, we have for all pairs minj=k

wj − wk

2 L

≥

δ2 9

and

maxj,k

wj − wk

2 L

≤

4δ2,

and

as

a

result maxj,k DKL(Pwj Pwk ) ≤ 4nζλm(H)δ2. Choosing δ2 = 8nζlλogm2(H) and applying Lemma 6 proves

the claim.

D.1.2 Upper bound under the squared Euclidean norm
The upper bound under the squared 2-norm follows directly from the upper bound under the squared L semi-norm in Theorem 4: noting that (w∗ − w) ⊥ nullspace(L), we get that
(w∗ − w)T L(w∗ − w) ≥ λ2(L) w∗ − w 22.
Substituting this inequality in the upper bound on the minimax risk under the squared L semi-norm in Theorem 4 gives the desired result.

D.1.3 Lower bound under the squared Euclidean norm

supz∈[−B,B]m ∇F (z) 2 †

Deﬁne ζ =

F (−B,B,...,B) H .

wj, wk ∈ WB,

Equation (26) in Appendix D.1.1 shows that for any vectors

DKL(Pwj Pwk ) ≤ ζλm(H)n wj − wk 2L,

Consider the pair of scalars α ∈ (0, 41 ) and δ > 0 whose values will be speciﬁed later. Let M (α) be as deﬁned in (14). In Appendix B.2 we constructed a set {w1, . . . , wM(α)} of vectors of length d that

satisfy wj, 1 = 0 for every j ∈ [M (α)], and for every pair of vectors in this set,

and 1 (M2(α))

j=k wj − wk 2L ≤ 2dδ2 tr(L). Applying Lemma 10 gives

wj − wk

2 2

≥

αδ2

1

j

k 2 2δ2

M (α)

w −w

L≤

m(m − 1). d

2 j=k

Setting δ2 = 0.005 nζλm(Hd)2m(m−1) yields

DKL(Pwj Pwk ) ≤ 0.01d.

32

In a manner similar to Lemma 14 in the pairwise comparison case, one can show that in the general setting of this section, tr(L†) ≥ 4m(dm2−1) . Then, every element from the packing set also satisﬁes wj ∞ ≤ B when δ ≤ B, which holds true under our assumption of n ≥ ζcBσ22λtrm(L(H†)) ≥ 4m(m−c1σ)ζ2Bd22λm(H) with c = 0.01. Each element of our packing set thus belongs to the class WB. Applying Lemma 6
yields the lower bound

∗2 α

d2

0.01d + log 2

w−w

L

≥

0.01 2 nζλm(H)m(m − 1)

1−

log M (α)

.

Setting α = 0.01 proves the claim for d > 9.

For the case of d ≤ 9, consider the set of the three d-length vectors z1 = [0 · · · 0 − 1],

z2 = [0 · · · 0 1] and z3 = [0 · · · 0 0]. Construct the packing set {w1, w2, w3} from these three

vectors {z1, z2, z3} as done above for the case of d > 9. From the calculations made for the general

case above, we have for all pairs minj=k

wj − wk

2 2

≥

δ2 9

and

maxj,k

wj − wk

2 L

≤

4δ2,

and

as

a

result maxj,k DKL(Pwj Pwk ) ≤ 4nζλm(H)δ2. Choosing δ2 = 8nζlλogm2(H) and applying Lemma 6 proves

the claim.

D.2 Some implied properties of the model In this section, we prove the two auxiliary lemmas stated at the start of this appendix.

D.2.1 Proof of Lemma 10 From the deﬁnition (10) of L, have

1n

TT

1n

T

L1 = n

Ei(mI − 11 )Ei 1 = n

Ei(mI − 11 )1 = 0,

i=1

i=1

showing that 1 ∈ nullspace(L).
Now consider any non-zero vector v : = [v1, . . . , vd]T ∈ Rd such that v ∈/ span(1). Then there must exist some i, j ∈ [d] such that vi = vj. We know that there exists some path from item i to j in the comparison hyper-graph. Thus there must exist some hyper-edge in this path with two items, say i , j , such that vi = vj . Suppose that hyper-edge corresponds to sample ∈ [n]. Let v : = ET v. Then v ∈/ span(1). The Cauchy-Schwarz inequality v , v 1, 1 > ( v , 1 )2 thus implies

vT E (mI − 11T )ET v > 0.

Furthermore, for any v ∈ Rm, the Cauchy-Schwarz inequality v , v 1, 1 > ( v , 1 )2 implies that for any i ∈ [n], we have vT Ei(mI − 11T )EiT v ≥ 0. Overall we conclude that have vT Lv > 0 for every v ∈/ span(1), and hence, nullspace(L) = 1 and λ2(L) > 0.
Finally, we have

1n

TT

1n

T

TT

tr(L) = n

tr(Ei(mI − 11 )Ei ) = n

mtr(EiEi ) − tr(Ei11 Ei ) .

(27)

i=1

i=1

By

the

deﬁnition

of

the

matrices

{Ei}i∈[n],

tr(EiEiT )

=

m

and

tr(

Ei

11T

E

T i

)

=

m.

Substituting

these

values in (27) gives the desired result tr(L) = m(m − 1).

33

D.2.2 Proof of Lemma 11 Let h1, . . . , hm denote the m eigenvectors of H, with h1 = √1m 1. Then for any vector v ∈ Rm,

m

m

v T Hv = λi(H) v , hi 2 ≥ λ2(H) v , hi 2 = λ2(H)

i=2

i=2

m v , hi 2 − 1 v , 1 2 m
i=1

= λ2(H)v T (I − 1 11T )v , m

where the ﬁnal step employed the property similar argument gives

m i=1

hihTi

=I

of the eigenvectors h1, . . . , hm

of H.

A

v THv

m
= λi(H) v , hi
i=2

m
2 ≤ λmax(H)
i=2

v , hi

2 = λmax(H)

m v , hi 2 − 1 m
i=1

= λmax(H)v T (I − 1 11T )v . m

v,1 2

Setting v

=

R

T j

v

gives

λ2(H)vT Rj(I − m1 11T )RjT v ≤ vT RjHRjT v ≤ λmax(H)vT Rj(I − m1 11T )RjT v.

Observe that the matrix I − m1 11T is invariant to permutation of the coordinates, and hence Rj(I −

1 m

11T

)RjT

=

I

−

m1 11T .

This

gives

λ2m(H) vT (mI − 11T )v ≤ vT RjHRjT v ≤ λmamx(H) vT (mI − 11T )v.

Appendix E. Some useful tail bounds
In this appendix, we collect a few useful tail bounds for quadratic forms in Gaussian and sub-Gaussian random variables.

Lemma 12 (Tail bound for Gaussian quadratic form) For any positive semideﬁnite matrix Q and standard Gaussian vector g ∼ N (0, Id), we have

P gT Qg ≥ tr(Q) + |||Q|||op δ 2 ≤ e−δ/2.

(28)

valid for all δ ≥ 0.

√

√

Proof Note that the function g → Qg 2 is Lipschitz with constant ||| Q|||op. Consequently,

by con√centration for Lipschitz functions of Gaussian vectors (Ledoux, 2001), the random variable Z = Qg 2 satisﬁes the upper bound

t2 P Z ≥ E[Z] + t ≤ exp − 2|||√Q|||2
op

t2

= exp −

.

2|||Q|||op

√ By Jensen’s inequality, we have E[Z] = E[ Qg 2] ≤ completes the proof.

E[gT Qg] =

tr(Q). Setting t =

|||Q|||op δ

34

Lemma 13 ((Hanson and Wright, 1971; Rudelson and Vershynin, 2013)) Let V ∈ Rd be a random vector with independent zero-mean components that are sub-Gaussian with parameter K, and let M ∈ Rd×d be an arbitrary matrix. Then there is a universal constant c > 0 such that

T

T

t2

t

P V M V − E[V M V ] > t ≤ 2 exp −c min K4|||M |||2 , K2|||M |||

fro

op

for all t > 0. (29)

Appendix F. Properties of Laplacian matrices

By construction, the Laplacian L of the comparison graph is symmetric and positive-semideﬁnite. By the singular value decomposition, we can write L = U T ΛU where U ∈ Rd×d is an orthonormal
matrix, and Λ is a diagonal matrix of nonnegative eigenvalues with Λjj = λj(L) for every j ∈ [d]. Given our assumption of λ1(L) ≤ · · · ≤ λd(L), we also have Λ11 ≤ · · · ≤ Λdd. Also recall that L† denotes the Moore-Penrose pseudo-inverse of L. In terms of the notation introduced, the MoorePenrose pseudo-inverse is then given by L† = U T Λ†U , where Λ† is a diagonal matrix with entries

Λ†jj =

(Λ−jj1) 0

if Λjj > 0 otherwise.

The following pair of lemmas establish some useful properties about L.

Lemma 14 The Laplacian matrix (4) satisﬁes the trace constraints

tr(L) = 2, and tr(L†) ≥ d2 . 4

Proof From the deﬁnition (4) of the matrix L, we have tr(L) = n1

n i=1

tr(xi

xTi

)

=

2.

We also

know that λ1(L) = 0, and hence dj=2 λj(L) = 2. Given the latter constraint, the sum dj=2 λj1(L)

is minimized when λ2(L) = · · · = λd(L). Some simple algebra now gives the claimed result.

Lemma 15 For the matrix L deﬁned in (4), and for a (n × d) matrix X with xTi as its ith row,

tr( n1 xT L†x) = d − 1, ||| n1 xT L†x|||fro = d − 1, and ||| n1 xT L†x|||op = 1.

Proof Let Q = 1 xT L†x. Since L = 1 XT X = U T ΛU , the diagonal entries of Λ are the squared

n√

n

sing√ular va√lues of X/ n. Consequently, there√must √exist an orthonormal matrix V such that

X/ n = V ΛU T , and thus√we ca√n write Q = V Λ Λ† Λ V T . By deﬁnition of the Moore-Penrose

pseudo-inverse, the matrix Λ Λ† Λ is a diagonal matrix; since the Laplacian graph is connected,

its diagonal contains (d − 1) ones and a single zero. Noting that V is an orthonormal matrix gives

the claimed result.

For future reference, we state and prove a lemma showing that these two semi-norms satisfy a restricted form of the Cauchy-Schwarz inequality:

Lemma 16 For any two vectors u and v such that u ⊥ nullspace(L) or/and v ⊥ nullspace(L), we have

| u, v | ≤ u L† v L.

(30)

35

Proof Since L = U T ΛU and L† = U T Λ†U , we have

√√

√

√

vT Lv uT L†u = vT U T ΛU v uT U T Λ†U u = v 2 u 2 ≥ | v, u |,

√

√

where we have deﬁned v : = ΛU v and u : = Λ†U u. Continuing on,

√√ v, u = vT U T Λ Λ†U u = vT U U T u,

where we have used the fact that u or/and v are orthogonal to the null space of L. Since U is orthonormal, we conclude that v, u = v, u , which completes the proof.

Appendix G. Minimax risk without assumptions on quality scores
The setting considered throughout the paper imposes two restrictions (2) on the quality score vector w∗. The ﬁrst condition is that of shift invariance, that is, w∗, 1 = 0. The necessity of this condition for identiﬁability under the Ordinal model is easy to verify. The second condition is that the quality score vectors are B-bounded, that is, w∗ ∞ ≤ B for some ﬁnite B. In this section, for the sake of completeness, we show that the minimax risk is inﬁnite in the absence of this condition.
Proposition 17 Any estimator w based on n samples from the Ordinal model (with unbounded quality score vectors) has error lower bounded as

sup

E

w − w∗

2 2

=

sup

E

w − w∗

2 L

= ∞.

w∗∈W∞

w∗∈W∞

The remainder of this section is devoted to the formal proof of Proposition 17. Consider the event

where for every comparison, the item with the higher quality score in w∗ wins. For any w∗ ∈ W∞\{0},

this

event

occurs

with

a

probability

at

least

1 2n

.

Under

this

event,

the

true

w∗

is

indistinguishable

from the quality score vector cw∗ ∈ W∞ for every c ≥ 0, and the error is also unbounded. Since the

probability of this event is strictly bounded away from zero, the expected error is also unbounded.

References
Ammar Ammar and Devavrat Shah. Ranking: Compare, don’t score. In Allerton Conference on Communication, Control, and Computing, pages 776–783, 2011.
Donald R Atkinson, Bruce E Wampold, Susana M Lowe, Linda Matthews, and Hyun-Nie Ahn. Asian American preferences for counselor characteristics: Application of the Bradley-Terry-Luce model to paired comparison data. The Counseling Psychologist, 26(1):101–123, 1998.
William Barnett. The modern theory of consumer behavior: Ordinal or cardinal? The Quarterly Journal of Austrian Economics, 6(1):41–65, 2003.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, pages 324–345, 1952.
Tom Bramley. A rank-ordering method for equating tests by expert judgment. Journal of Applied Measurement, 6(2):202–223, 2005.
Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Symposium on Discrete Algorithms, pages 268–276, 2008.

36

Andries E Brouwer and Willem H Haemers. Spectra of graphs. Springer, 2011.
Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177–214, 2014.
Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In International Conference on Web Search and Data Mining, pages 193–202, 2013.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.
Ofer Gabber and Zvi Galil. Explicit constructions of linear-sized superconcentrators. Journal of Computer and System Sciences, 22(3):407–420, 1981.
Edgar N Gilbert. A comparison of signalling alphabets. Bell System Technical Journal, 31(3): 504–522, 1952.
Paul E Green, J Douglas Carroll, and Wayne S DeSarbo. Estimating choice probabilities in multiattribute decision making. Journal of Consumer Research, pages 76–84, 1981.
Bruce Hajek, Sewoong Oh, and Jiaming Xu. Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, pages 1475–1483, 2014.
David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, pages 1079–1083, 1971.
Sandra Heldsinger and Stephen Humphry. Using the method of pairwise comparison to obtain reliable teacher assessments. The Australian Educational Researcher, 37(2):1–19, 2010.
Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill: A Bayesian skill rating system. In Advances in Neural Information Processing Systems, volume 19, page 569, 2007.
Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97, 2012.
Srikanth Jagabathula and Devavrat Shah. Inferring rankings under constrained sensing. In Advances in Neural Information Processing Systems, pages 753–760, 2008.
Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. In Advances in Neural Information Processing Systems, pages 2240–2248, 2011.
Gabriella Kazai. In search of quality in crowdsourcing for search engine evaluation. In Advances in Information Retrieval, pages 165–176. Springer, 2011.
Zahid Y Khairullah and Stanley Zionts. An approach for preference ranking of alternatives. European journal of operational research, 28(3):329–342, 1987.
37

Firas Khatib, Frank DiMaio, Seth Cooper, Maciej Kazmierczyk, Miroslaw Gilski, Szymon Krzywda, Helena Zabranska, Iva Pichova, James Thompson, Zoran Popovi´c, Mariusz Jaskolski, and David Baker. Crystal structure of a monomeric retroviral protease solved by protein folding game players. Nature structural & molecular biology, 18(10):1175–1177, 2011.
John I Kiger. The depth/breadth trade-oﬀ in the design of menu-driven user interfaces. International Journal of Man-Machine Studies, 20(2):201–213, 1984.
Kenneth J Koehler and Harold Ridpath. An application of a biased version of the Bradley-Terry-Luce model to professional basketball results. Journal of Mathematical Psychology, 25(3), 1982.
Paul FM Krabbe. Thurstone scaling as a measurement method to quantify subjective health outcomes. Medical care, 46(4):357–365, 2008.
ASID Lang and Joshua Rio-Ross. Using Amazon Mechanical Turk to transcribe historical handwritten documents. The Code4Lib Journal, 2011.
M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001.
Michael D Lee, Mark Steyvers, Mindy De Young, and Brent J Miller. A model-based approach to measuring expertise in ranking tasks. In Proceedings of the 33rd annual conference of the cognitive science society, 2011.
E.L. Lehmann and G. Casella. Theory of Point Estimation. Springer Texts in Statistics, 1998.
Peter John Loewen, Daniel Rubenson, and Arthur Spirling. Testing the power of arguments in referendums: A Bradley–Terry approach. Electoral Studies, 31(1):212–221, 2012.
R Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. New York: Wiley, 1959.
Miguel Angel Luengo-Oroz, Asier Arranz, and John Frean. Crowdsourcing malaria parasite quantiﬁcation: an online game for analyzing images of infected thick blood smears. Journal of medical Internet research, 14(6), 2012.
George A Miller. The magical number seven, plus or minus two: some limits on our capacity for processing information. Psychological review, 63(2):81, 1956.
Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, pages 2474–2482, 2012.
Robert M Nosofsky. Luce’s choice model and Thurstone’s categorical judgment model compared: Kornbrot’s data revisited. Attention, Perception, & Psychophysics, 37(1):89–91, 1985.
Roberto Imbuzeiro Oliveira. Concentration of the adjacency matrix and of the laplacian in random graphs with independent edges. arXiv preprint arXiv:0911.0600, 2009.
Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and Daphne Koller. Tuned models of peer assessment in MOOCs. In International Conference on Educational Data Mining, 2013.
Robin L Plackett. The analysis of permutations. Applied Statistics, pages 193–202, 1975.
38

Arun Rajkumar and Shivani Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In Proceedings of the 31st International Conference on Machine Learning, pages 118–126, 2014.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. The Journal of Machine Learning Research, 99: 1297–1322, 2010.
Daniel Ross. Arpad Elo and the Elo rating system, 2007. http://en.chessbase.com/post/ arpad-elo-and-the-elo-rating-system.
Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration. Electronic Communications in Probability, 18:1–9, 2013.
Thomas L Saaty and Mujgan S Ozdemir. Why the magic number seven plus or minus two. Mathematical and Computer Modelling, 38(3):233–244, 2003.
Richard M Shiﬀrin and Robert M Nosofsky. Seven plus or minus two: a commentary on capacity limitations. Psychological Review, 1994.
Neil Stewart, Gordon DA Brown, and Nick Chater. Absolute identiﬁcation by relative judgment. Psychological review, 112(4):881, 2005.
John Swets. The relative operating characteristic in psychology. Science, 182(4116), 1973. Louis L Thurstone. A law of comparative judgment. Psychological Review, 34(4):273, 1927. Kristi Tsukida and Maya R Gupta. How to analyze paired comparison data. Technical report, DTIC
Document, 2011. A.B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics, 2008. RR Varshamov. Estimate of the number of signals in error correcting codes. In Dokl. Akad. Nauk
SSSR, volume 117, pages 739–741, 1957. Luis von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel Blum. Recaptcha:
Human-based character recognition via web security measures. Science, 321(5895):1465–1468, 2008. Jing Wang, Panagiotis G Ipeirotis, and Foster Provost. Managing crowdsourcing workers. In Winter Conference on Business Intelligence, pages 10–12, 2011. Jinfeng Yi, Rong Jin, Shaili Jain, and Anil Jain. Inferring users’ preferences from crowdsourced pairwise comparisons: A matrix completion approach. In AAAI Conference on Human Computation and Crowdsourcing, 2013.
39

