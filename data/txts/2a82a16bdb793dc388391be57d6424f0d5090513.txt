Integrating Rankings into Quantized Scores in Peer Review

Yusha Liu Carnegie Mellon University
yushal@cs.cmu.edu

Yichong Xu Microsoft Cognitive Services Research
yichong.xu@microsoft.com

Nihar B. Shah Carnegie Mellon University
nihars@cs.cmu.edu

Aarti Singh Carnegie Mellon University
aarti@cs.cmu.edu

arXiv:2204.03505v1 [cs.IR] 5 Apr 2022

Abstract
In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a signiﬁcant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workﬂows, thereby leading to inefﬁciencies.
We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workﬂows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and ﬁnd that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.
1 Introduction
Many applications involve people evaluating a large number of items in a distributed fashion. An important and prominent such application, which is the focus of this paper, is peer review of papers in scientiﬁc conferences. A typical way of collecting reviews is through quantized scores, that is, where reviewers are asked to provide scores from a constant number of quantization levels. For example, reviewers for conferences are often asked to provide their opinions on papers in the format of ﬁve-level or ten-level Likert items, which are used to evaluate the qualities of submitted papers.
A drawback of such quantized scores is that there exist a large number of ties in such quantized scores, due to the constant number of quantization levels as well as the respondents’ tendency to give equal values in the absence of prompt for relative relationships [1]. For example, a recent study [2] of peer-review data from the NeurIPS 2016 conference found that among all instances where a reviewer reviewed a pair of papers, the pair of review scores provided by the reviewer were tied in more than 30% of such instances. Such a large proportion of ties were found to exist in the scores for all four criteria elicited from reviewers, and the number of ties was even higher when restricting attention to only the top and middle-quality papers. Such ties result in a loss of information, thereby contributing to the difﬁculty in making decisions regarding the acceptance of papers.
1

Despite the apparent drawbacks, there are strong reasons that quantized scores are a widely used format instead of continuous-valued scores to elicit the opinions of reviewers. This is because of the limited ability of humans to describe stimuli with numerical values [3]. Psychometric studies have discussed the appropriate number of response alternatives when eliciting responses from humans and suggested that it remain a small constant [4, 5].
An alternative form of evaluation comprises rankings. Ranking information alone has been demonstrated to be a robust way to collect information [6–9]. For example, crowdsourcing experiments [9] demonstrate the reliability of answers in form of pairwise comparisons, which incur a lower per-sample error as compared to eliciting numerical values.
Scientiﬁc conferences, which face an increasing number of submissions each year, have made attempts to collect additional information by asking reviewers to report rankings in addition to scores [10]. Among recent computer science conferences, the NeurIPS 2016 conference asked each reviewer to also rank the papers they were reviewing [2]. This was an experiment that provided a sanity check about rankings in peer review and also identiﬁed several beneﬁts of collecting rankings. The ICML 2021 conference collected scores as well as rankings from reviewers on their assigned papers. Note that during the review process, a majority of the reviewers’ time is spent on reading and evaluating the papers. As a result, generating rankings in addition to the traditional scores may take only little additional time.
In Figure 1a we illustrate the standard interface used by Area Chairs in peer review, augmented with the ranking information (in the “rankings” column). An important aspect of the standard interface, extensively used by chairs in their workﬂow, is the ability to sort papers via the minimum, maximum, or average received scores or via the spread of the scores. The augmented ranking information shows the (partial or total) rankings provided by reviewers, where papers not being handled by that Area Chair are replaced with a ∗ symbol (as done by ICML 2021). There are multiple challenges of such an interface. First, such ranking information is incompatible with commonly used workﬂow elements such as sorting according to scores, so any such operations will omit ranking information. Second, it is not clear how to efﬁciently extract information from the rankings under such an cluttered interface. As a consequence, the varying use or lack of use of ranking information by different Area Chairs can add to the arbitrariness in peer review. Thus, while the elicitation of ranking evaluations introduces another kind of information for chairs to use, the question of how to judiciously use this data has remained open.
Our goal is to allow the chairs to use the ranking information in addition to the scores while not disrupting their workﬂow. To this end, we design a method to integrate the reviewer-provided rankings into the scores. One may think of this approach as dequantizing the scores. In our problem formulation (Section 3), the output is a real-valued score for each review, where these real-valued scores combine the reviewer-provided rankings and (quantized) scores. We refer to these real-valued outputs as the “dequantized scores”. We illustrate the interface with these dequantized scores in Figure 1b. As shown in Figure 1b, these dequantized scores can now be incorporated in the standard workﬂow for Area Chairs or Program Chairs, allowing them to seamlessly perform the tasks that they conventionally perform on the original quantized scores (such as sorting by the average or the spread of the scores for each paper).
Our contributions. The main contributions of this paper are as follows. (1) We identify and formulate the problem of combining rankings and quantized scores, focusing speciﬁcally on
dequantization of the scores for every review, rather than aggregating the quantized scores and/or rankings given by reviewers to estimate a score for each paper. We provide detailed motivations for dequantizing the scores in Section 4.1. Our method allows that each reviewer is only assigned a small subset of the papers and, further, may only return a partial ranking of the assigned papers. For concreteness, we focus on the application of peer-review, while noting that the problem and solution may also apply to other settings where both rankings and quantized scores are available.
(2) We propose a computationally-efﬁcient algorithm that outputs a real-valued dequantized score for every review in the assignment.
• Our approach is based on a set of design principles we outline subsequently. We make no parametric assumptions on the data generation process, nor the existence of ground truth quality scores and global rankings.
• A part of our approach is inspired by isotonic regression, and we also provide connections to estimation under the Thurstone model and the balanced-rank-estimation algorithm [11, Section 4.1].
• Our algorithm includes a purely data-driven “quantization validation (QV)” method for hyperparameter selection due to the absence of the ground truth.
2

Submissions Users

Settings

Help Center Select Your Role : Chair

Nihar Shah

Chair Console
Tip: you can hover over column headers and click "Hide" to remove columns.
1 - 25 of 8966 «« « 1 2 3 4 5 » »» Show: 25

50

100

All

Clear All Filters

Actions

Overall Evaluation (Scores)

Paper ID

Title

Reviewers

Scores

Rankings

Min

Max

Avg

Spread

e.g. <3

ﬁlter...

Clear

ﬁlter...
Clear

ﬁlter...
Clear

ﬁlter...
Clear

e.g. <3

e.g. <3

e.g. <3

e.g. <3

Clear

Clear

Clear

Clear

Clear

The Hobbit 134

R1; R3; R5

6; 7; 7

[R1]: * > 134 [R4]: * > 134,   * > 6390

6

7

6.67

0.47

699

The Fellowship of the Ring

Submissions Users

Settings

1209

The Two Towers

R1; R5; R8 R2; R5; R7

5; 5; 5 6; 6; 6

[R8]: * > * > 699 [R2]: 1209 > *

Help Center

5

5

Select Your Role :

6

6

5 Chair
6

0 Nihar Shah
0

Chair Console The Return of the King

6390

R4; R5; R6

Tip: you can hover over column headers and click "Hide" to remove columns.

7; 6; 7

[R4]: * > 134,   * > 6390 [R5]: 6390 > * [R6]: * > * > 6390 > *

6

7

6.67

0.47

1 - 25 of 8966

««
(a)

«12
Interface

3
with

45»
quantized

»s»corSehsowa:nd

25

50

rankings.

100

All

Clear All Filters

Actions

Overall Evaluation (Dequantized Scores)

Paper ID

Title

Reviewers

Original Scores Dequantized Scores

Min

Max

Avg

Spread

e.g. <3

ﬁlter...
Clear

ﬁlter...

ﬁlter...

ﬁlter...

Clear

Clear

Clear

Clear

134

The Hobbit

R1; R3; R5

6; 7; 7

5.77; 6.54; 6.85

© 2022 Microsoft Corporation About CMT | Docs | Terms of Use | Privacy & Cookies | Request Free Site

699

The Fellowship of the Ring

R1; R5; R8

5; 5; 5

4.86; 5.17; 5.10

1209

The Two Towers

R2; R5; R7

6; 6; 6

6.27; 6.18; 6.48

6390

The Return of the King

R4; R5; R6

7; 6; 7

7.10; 6.02; 7.01

e.g. <3
Clear
5.77 4.68 6.18 6.02

e.g. <3
Clear
6.85 5.17 6.48 7.10

e.g. <3
Clear
6.39 4.98 6.31 6.71

e.g. <3
Clear
0.46 0.22 0.13 0.49

(b) Interface with dequantized scores.

Figure 1: An illustration of the envisaged interfaces for the Area Chairs, a conference management system (Microsoft CMT) used commonly by computer science conferences for peer-review. The top ﬁgure shows the interface with separate quantized scores and rankings reported by reviewers. The rankings in the top ﬁgure are processed in the same manner as in the ICML 2021 conference, precisely, papers outside the scope of the Area Chair are replaced with symbols for conﬁdentiality. The bottom ﬁgure shows the interface with our proposed output format of dequantized scores. Each row represents a paper under the scope of the Area Chair, while the columns correspond to relevant in©f2o0r22mMaictrioosonft CoofrpothrateionpapAeboru.t CMT | Docs | Terms of Use | Privacy & Cookies | Request Free Site

(3) We evaluate the empirical performance of our proposed algorithm on both synthetic data as well as real-world peer review data from the ICLR 2017 conference. As motivated subsequently in Section 4.2, we use the (Kendall-tau) ranking error as the metric of interest. We compare the proposed algorithm to two baselines: quantized scores and BRE-adjusted-scores, which are formally introduced in Section 5.2. We ﬁnd that our algorithm incurs about 30% lower error as compared to the best performing baseline in the ICLR 2017 data.
2 Related Work
There is a long line of literature [9, 11–21] in the domain of ranking from pairwise comparisons with various modeling assumptions. Methods from this domain take comparison outcomes between pairs of items as input, and output estimates of an underlying global ranking or a comparison probabilities matrix. The problems investigated in this domain, however, are fundamentally different from ours with major distinctions in both the input and output of the problem. In our setting, the algorithm needs to effectively incorporate scores in addition to rankings, while the above methods cannot be trivially adapted to take score values as input. In settings such as peer review, scores cannot be neglected, as the ranking information is very limited and sparse. Additionally, the number of comparisons per item needed in most prior work in ranking from pairwise comparisons grows with the number of items (sometimes logarithmically but often linearly). In practice, however, there are often only 3 to 6 reviews per paper in most peer-reviewed conferences, and the number of comparisons per paper is a small constant. Mao et al. [16]
3

allow for the number of comparisons to be constant, but still use at least tens of comparisons per item in their simulations, which remains impractical for the peer-review setting. Furthermore, unlike their goals, we make a deliberate design choice to not aim to output global ranking or comparisons probabilities of the papers, but instead focus on estimation of dequantized scores for reviewer-paper pairs. This design choice is motivated in Section 4.1.
Some recent works [22–29] develop approaches to use ranking information in addition to labels (scores) in supervised tasks such as classiﬁcation, regression, and optimization. However, there are crucial differences between their settings and ours, such as the preservation of distinct reviewer evaluations, instead of pooling the data together,
which prohibit any direct translations of those works to our setting. These works consider the generalization setting by building general predictive models from training data that then apply to incoming test data. Whereas our work is in the transduction setting, where we derive the output scores directly from the input quantized scores and rankings.
The kind of data considered in the paper by Ailon [30] is closer to our setting despite their goal of deriving a global ranking, which considers two kinds of input: a total ranking for only a few top-ranked items and quantized scores. However, the method then only uses the partial ranking induced by the quantized scores and discards the actual values of the scores. This step thus leads to a signiﬁcant loss of information. For instance, a reviewer giving a ‘strong accept’ to two papers is very different from the reviewer giving a ‘strong reject’ to two papers, but their algorithm will not distinguish these two cases. In our setting, such data is equivalent to having only partial rankings but no scores.
The paper most closely related to our setting is a concurrent and independent work by Pearce and Erosheva [31], which also considers a transduction setting with both ranking and score data. They propose a model termed the Mallows-Binomial model, parametric model that jointly captures the scores and rankings. They estimate the model parameters via maximum likelihood estimation and propose two computationally-efﬁcient algorithms based on A* tree search. They present theoretical results including properties of the Maximum likelihood estimator (MLE) for model parameters. On the empirical front, they ﬁt the model on a real-world grant panel review dataset where 6 judges each scored all of the 18 proposals and ranked their top 6. They examine the results manually in absence of ground truth and show that the estimated model parameters successfully capture information from both scores and rankings. The work of Pearce and Erosheva [31], however, differs from ours in several critical ways. First, their method and analysis require that each reviewer reviews the entire set of objects to be evaluated. While this condition may be met in small grant proposal panels such as the American Institute of Biological Sciences grant proposal review studied in [31], this is impractical in conference peer-review that has thousands of submitted papers, which is the primary focus of our work. Second, they assume that each reviewer provides a top-K ranking. We allow reviewers to provide comparisons between any arbitrary subsets of assigned papers (Section 3), which eventually constitutes a partial ranking different from that in [31]. Third, the Mallows-Binomial model assumes the existence of a true underlying quality score for each paper and consequently of a global ranking of the papers, whereas our approach deliberately aims to avoid assumptions about the ground truth scores and about ranking over the papers. Fourth, their model assumes that rankings and quantized scores are independent conditioned on the model parameters. On the contrary, we assume that the rankings provided by any reviewer are consistent – and hence strongly dependent – with the quantized scores provided by that reviewer. Our setting occurs more naturally in peer-review, where reviewers report the two types of information at the same time as their ﬁnal evaluations. Fifth, they pool the reviewers’ data together to obtain a ﬁnal quality score for each paper. On the other hand, our approach seeks to provide ﬂexibility to the Area Chairs and Program Chairs on aggregating the reviews for any paper, and hence we output a score for each review.
Finally, there has been a ﬂurry of works that address various other problems in peer review, such as bias [32–34], miscalibration [35–38], subjectivity [39, 40], dishonest behavior [41–46], and others. See [47] for a survey. In particular, Noothigattu et al. [40] address the problem of reviewer subjectivity, where their proposed output is similar to that we consider here – an updated score pertaining to each review.
3 Problem Setting
Consider a set of P papers indexed by p ∈ [P ], and a set of R reviewers indexed by r ∈ [R].1 Let A denote the set of assigned reviewer-paper pairs: A = {(r, p) ∈ [R] × [P ] | reviewer r reviews paper p}. For every paper p assigned to reviewer r, the reviewer reports a quantized score zrp ∈ Z bounded by a pre-speciﬁed interval [a, b] : a, b ∈ Z. We
1For any positive integer m, we use the standard notation [m] = {1, 2, . . . m}.
4

assume that the quantization process maps a value in the interval [z − 0.5, z + 0.5] to integer z. In our experiments, both the number of papers assigned to a reviewer and the number of reviews received by a paper are constants, which is the case in practice in peer-review, as opposed to scaling with P or R.
In addition, the reviewer reports a partial ranking πr, which is a partial ordering over the set of papers that is assigned to this reviewer. For a pair of papers (p, p ) reviewed by reviewer r, we use p r p to mean that reviewer r ranks paper p above paper p in the partial ranking πr provided by reviewer r. The partial ranking πr can be then represented as the set πr = {(p, p ) : (r, p) ∈ A, (r, p ) ∈ A, p r p }. We assume that for every reviewer, the reported rankings are consistent with the scores, that is, the reviewer may give the ranking p r p only when the reported scores satisfy zrp ≥ zrp .
Observe that under our problem formulation, reviewers can choose to report a total ranking of assigned papers, but also have the freedom to not compare some pairs of papers. In peer review, it is not always reasonable to ask for a binary comparison between every pair of papers. For example, as pointed out in [48], it can be difﬁcult to make a comparison in a situation with “incomplete evaluation in one borderline paper vs narrow applicability of another”. Allowing partial orderings prevents reviewers from being forced to make such “apples-to-oranges” comparisons [48].
We now describe our objectives given the inputs of quantized scores and rankings. As introduced in Section 1 and detailed in Section 4.1, given the quantized scores and rankings from the reviewers, our goal is to merge the two sources of information into a single source, in a manner that can seamlessly assist the human experts. Our designed algorithm thus aggregates the reviewer-provided sources of information into dequantized and continuous scores. We denote the output dequantized scores as {yrp}(r,p)∈A.
4 Main Results
We ﬁrst describe our three design principles in Section 4.1. Then in Section 4.2, we present our proposed algorithm designed based on these principles. In Section 4.3, we draw interesting connections between our algorithm and simpler algorithms under some extreme cases, as well as a connection to maximum likelihood estimation under the Thurstone model. Finally, we present our data-driven ‘quantization validation’ method for hyperparameter selection in Section 4.4.
4.1 Approach and Design Principles
In this section, we introduce three principles we follow in designing our algorithm. The ﬁrst principle pertains to our approach towards the goal of the algorithm. Unlike many prior works analyzing
scores and ranking information which focus on estimation of the “true” underlying qualities of each item [9, 17, 21, 49, 50] or the ranking of such assumed “true” qualities [14–16, 18, 49, 51], we deliberately focus on obtaining a dequantized numerical output corresponding to each review. This is because, in the actual peer-review process, the ﬁnal aggregation depends on various additional factors including the review and rebuttal text, reviewer discussions, and Program/Area Chairs’ preferences on how to aggregate individual reviews. The reviewer-provided scores are usually used heavily in the peer-review workﬂow for sorting or initial judgments but do not fully determine the ﬁnal decision. Furthermore, we deliberately do not want to base our algorithm on the assumption of the existence of some objective true qualities, especially in applications such as peer review that has a signiﬁcant amount of subjective opinion [52]. Thus with our goal of helping the Program Chairs and Area Chairs seamlessly make their decisions in the real world, we focus on producing a uniﬁed representation in form of dequantized scores {yrp}(r,p)∈A. This approach of computing updated scores for each review is also taken by Noothigattu et al. [40] in their work on mitigating subjectivity in peer review.
Design Principle 1 (Dequantization of scores and not ﬁnal decisions). The goal is to produce estimated dequantized scores {yrp}(r,p)∈A, and not the ﬁnal decisions on the set of papers.
There are two ways to interpret the dequantized scores {yrp}(r,p)∈A, one under a model-based setting and another that does not rely on modeling assumptions.
(1) Under a model-based setting, outcomes in form of partial rankings and scores are generated from latent and real-valued variables. For example in the commonly-used Thurstone model [53], the latent variable yrp, (r, p) ∈ A represents the inherent opinion of reviewer r for paper p and is drawn from a normal distribution. Under this modeling
5

assumption, {yrp}(r,p)∈A can be interpreted as estimations of ground-truth values {yrp}(r,p)∈A. In other words, it is the scores that reviewers would have given without the quantization process, which reﬂect the “true” opinion of the reviewers.
(2) The assumption that reviewers generate some unquantized scores in their mind before giving a (quantized) score, as assumed by models such as the Thurstone model, may not hold in practice. As discussed earlier in Section 1, people may not be capable of evaluating items with ﬁned-grained measure scales [3–5, 9]. One can alternatively consider the dequantized scores {yrp}(r,p)∈A as a set of continuous variables that integrate the information from quantized scores and rankings without assuming the existence of ground-truth scores. The values of these variables are still compatible with the traditionally score-based decision rules, and other workﬂow elements used by Area and Program Chairs (Figure 1).
With this motivation, we now present our second principle which requires the estimates to be consistent with the input.
Design Principle 2 (Consistency). The values of estimation output {yrp}(r,p)∈A must be consistent with (i) reviewerprovided quantized scores, that is, based on the assumption of the quantization process in Section 3, zrp − 0.5 ≤ yrp ≤ zrp + 0.5 for every (r, p) ∈ A; and (ii) reviewer-provided rankings, that is, the ordering of {yrp}p:(r,p)∈A strictly follows πr, for every reviewer r.
The above design principles leave us with many possible choices for yrp, (r, p) ∈ A. For instance, consider a scores-only setting where we observe the quantized score zrp. For a reviewer-paper pair (r, p) ∈ A, the quantized score zr,p deﬁnes only the range of possible values for the dequantized score yr,p. Based on only the principles we have established so far, all output values yrp ∈ [zrp − 0.5, zrp + 0.5] are equally good. Consequently, we establish another design principle to break ties within this interval.
We use the consensus value among reviewers as the additional signal to break ties for quantized scores. The consensus value for a paper is deﬁned as the average of the scores provided by the reviewers for that paper. Based on only the information from the quantized scores given by other reviewers for that paper, it is natural to envisage that the dequantized score lies closer to the consensus value than away from it. In other words, the consensus signal indicates part of the interval [zr,p − 0.5, zr,p + 0.5] in which the dequantized score should lie. For example, without any ranking information, if a reviewer r gave a score of 7 and all the other reviewers gave 7 as well for paper p, then the consensus signal from other reviewers offers no additional information within the interval [6.5, 7.5]. However, if the other reviewers all gave quantized scores 4, then their consensus signal indicates that the dequantized score yr,p should lie within [4, 7] ∩ [7 − 0.5, 7 + 0.5], which is [6.5, 7].
Design Principle 3 (Consensus). We break ties in scores due to quantization in the direction of reviewer consensus.
How much does the dequantized score move in the direction of consensus? This is governed by a hyperparameter in our algorithm, whose value is chosen in a data-dependent manner.

4.2 Proposed algorithm

Our three design principles then lead to our proposed algorithm, Algorithm 1. The algorithm takes both reviewer-

provided rankings and quantized scores as inputs, and outputs dequantized scores {yrp}(r,p)∈A (Design Principle 1). The

algorithm solves a constrained convex optimization problem. The two constraints ensure that the output values are consis-

tent with the reviewer-provided information (Design Principle 2). The term

yrp

−

|{r

:(r

1 ,p)∈A}|

2
r :(r ,p)∈A yr p

in the objective measures disagreement between reviewers, therefore capturing the consensus (Design Principle 3). This term and the term (yrp − zrp)2 together capture the move towards the consensus, and the amount of movement is

determined by a hyperparameter λ > 0. In Section 4.4, we introduce a proposed cross-validation-like procedure called

quantization-validation for selecting λ.

It remains to specify the parameter , which can simply be chosen to be a small positive constant. This parameter

is only meant to ensure that any partial ranking given by a reviewer is strictly followed. We pick = 0.05 in our

subsequent experiments while noting that our results are robust to the choice of small (Appendix A.4). Our algorithm

6

Algorithm 1 Proposed algorithm
Inputs: Quantized scores {zrp}(r,p)∈A and rankings {πr}r∈[R] given by reviewers, hyperparameter λ, small value Output: Solution of the following constrained convex optimization program:

arg min
{yrp }(r,p)∈A


 
p∈[P ]

 yrp − |{r
r:(r,p)∈A

2



1 yr p + λ (yrp − zrp)2 ,

: (r , p) ∈ A}|



r :(r ,p)∈A

(r,p)∈A

such that yrp ≥ yrp + whenever (r, p) ∈ A, (r, p ) ∈ A, and p r p ; zrp − 0.5 ≤ yrp ≤ zrp + 0.5 ∀ (r, p) ∈ A.

is inspired in part by isotonic regression [54]: if = 0, then the term (yrp − zrp)2 in the objective and the resulting constraints yrp ≥ yrp ∀ (r, p) ∈ A, (r, p ) ∈ A, simply represent isotonic regression to incorporate the rankings.2
The optimization problem. When λ > 0, the objective is strictly convex. Note that the convexity of the ﬁrst term follows from the fact that this term is a sum over functions, where each function is a composite of a convex function and an afﬁne function. The set of constraints are linear inequalities, therefore the optimization problem yields a unique solution. Speciﬁcally, it is a convex quadratic programming (QP) problem, and therefore can be solved in time polynomial in the number of reviewers and papers.
Using the outputs: Scores or percentiles. We can extract two kinds of information from the output of the algorithm to present to the Program Chairs and/or Area Chairs. The dequantized scores for all reviewer-paper pairs in the assignment {yrp, (r, p) ∈ A} themselves provide a convenient and easy-to-use interface that is much cleaner than showing the quantized scores together with raw rankings, as demonstrated in Figure 1. The scores produced by the proposed algorithm lie within the quantization intervals provided by quantized scores (Design Principle 2). Within the intervals, the scores are in arbitrary scale because determining their exact values would essentially require more stringent modeling assumptions than we make in this work. The use of arbitrary scales is not new and is employed in various other applications to express measurements in absence of absolute values [55–58].
An alternative useful type of information that can be derived from this data is the ranking or percentile of each score yrp, which represents the relative position of evaluation given by r to p among the entire pool of all review scores (across all assigned reviewer-paper pairs). In the interface in Figure 1b, the percentiles can be shown in place of the dequantized scores and enjoy the same compatibility as dequantized scores with the workﬂow of the chairs. Either of these types of information can then be provided to an Area Chair or Program Chair to help inform decisions across a wider scope of papers that are not accessible to individual reviewers.
In the sequel, we primarily focus on evaluating the performance in terms of ranking error of the scores for all reviewer-paper pairs. The Kendall-tau ranking error we use is formally deﬁned in Section 5.1.
4.3 Analysis of the proposed algorithm in special cases
In this section, we analyze several properties of Algorithm 1. First, we draw a connection to Balanced Rank Estimation (BRE) algorithm [11] (Section 4.3.1) under certain assumptions. We then characterize the output by giving its analytical solution when only scores are present (Section 4.3.2). We also make a connection to the Thurstone model with quantization under the score-only case (Section 4.3.3).
4.3.1 Connection to Balanced Rank Estimation (BRE) when λ = ∞
Balanced Rank Estimation (BRE) [11, Section 4.1] is an algorithm for rank recovery with pairwise comparisons which enjoys optimal theoretical guarantees under the standard comparison-only setting, under three assumptions: there is a
2Note that isotonic regression itself cannot break ties between quantized scores for each reviewer.
7

ground-truth global ranking, the pairs compared are chosen independently and uniformly at random, and each pairwise comparison has an identical noise distribution. In a nutshell, BRE ﬁrst estimates a score for each item. The estimated score is proportional to the difference between the number of items preceding and succeeding this item in the given pairwise comparisons. Then, BRE uses these estimated scores to induce a global ranking of all items and the global ranking is the ﬁnal output of their method.
In what follows, we show the connection between our algorithm and BRE under a certain setting. In this setting: (i) every reviewer gives a total ranking of all the papers assigned to them, and (ii) λ = ∞ in our algorithm, that is, we retain only the second term in the objective in Algorithm 1. Without the ﬁrst term in its objective, the optimization problem in Algorithm 1 can be solved separately and in closed-form for each reviewer, and simply corresponds to the problem of breaking ties amongst papers with the same score using comparisons. For a reviewer-paper pair (r, p) ∈ A, the output yrp equals the sum of quantized score zrp given by the reviewer and an arbitrary-scale value generated from the ranking πr. We next describe the connection between this ranking-induced value and the output of the BRE algorithm. Consider the set of pairwise comparisons indicated by the total ranking πp, and further restrict our attention to the subset of comparisons between paper p and papers with the same quantized scores as p. The ranking-induced value is then proportional to the difference between the number of papers preceding and succeeding p in this subset of comparisons. This is the same way that the BRE algorithm estimates the score for paper p given the subset of comparisons as input.
The following proposition characterizes the behavior of the algorithm in the limit of λ = ∞.
Proposition 1. Assume that reviewers give total rankings of assigned papers, and that is a small constant such that the program in Algorithm 1 is feasible. When λ = ∞, our algorithm generates scores separately across different reviewers. For each reviewer, our algorithm adds to the quantized scores values that are proportional to the estimated scores from the Balanced Rank Estimation (BRE) [11] algorithm.
Therefore, we refer to the special case which our algorithm reduces to when λ = ∞ and reviewers provide total rankings of assigned papers as BRE-adjusted-scores. We provide further details of this reduction and the full algorithmic description of BRE-adjusted-scores (Algorithm 3) in Appendix B.1. This reduction is of interest since it shows that when λ = ∞, the proposed algorithm still makes reasonable use of the provided rankings to incorporate into quantized scores.

4.3.2 Analytical solution when only scores are available

While the solution of Algorithm 1 can be complicated to analyze, it is much easier when we only use scores. Without the ranking information, the only constraints on the output scores are zrp − 0.5 ≤ yrp ≤ zrp + 0.5, ∀(r, p) ∈ A. We can then get an analytical solution for Algorithm 1 as below.

Proposition 2. Suppose reviewers only provide quantized scores and no rankings, and each paper receives µ scores. Then for any reviewer-paper pair (r, p) the scores {yrp}(r,p)∈A output by Algorithm 1 can be written in closed form as

1 + µλ

1

y = µ(1 + λ) zrp +

µ(1 + λ) zr p

r :(r ,p)∈A,r =r

 y 
yrp = zrp − 0.5
zrp + 0.5

if y ∈ [zrp − 0.5, zrp + 0.5] if y < zrp − 0.5 if y > zrp + 0.5.

The full proof is provided in Appendix B.2. The output {yrp}(r,p)∈A is a convex combination of scores for paper p, where the weights depend on the constant µ as well as the hyperparameter λ > 0. Precisely, the relative weight on the quantized score given by reviewer r is 1 + µλ whereas the weights on every other reviewer are 1 (note that the number of reviews per paper µ is typically a small constant). Assume that the scores of paper p are not the same across all reviewers. Then, as the hyperparameter λ increases and the weight on consensus decreases, we can see from the reduction in Proposition 2 that the estimated score {yrp} approaches {zrp} as one should expect. As a further special case, when the scores received by any paper p from all reviewers are identical to, say, zp, the estimates yrp also reduce to zp as one may expect. When λ = ∞, the output is simply zrp. This result proves reasonable behavior from the proposed algorithm when only score information is available.

8

4.3.3 Connection to Thurstone model with quantization when only scores are available
We now draw a connection between the consensus objective used in Algorithm 1 and the Thurstone model with quantization. In this section, we retain the assumption in Proposition 2 that only scores are provided and focus on the consensus objective. The Thurstone model [53] is a widely used statistical model. For example, it is used for modeling peer grading in MOOCS in [59]. In the Thurstone model with quantization, scores are generated by the following process. Each paper is assumed to have an underlying true quality score x∗p. The latent evaluation score yrp given by reviewer r to paper p is generated from a normal distribution, whose mean is the underlying true score of paper p. That is, yrp ∼ N (x∗p, σ2), for some value σ that represents the standard deviation. Our setting involves quantization, we then assume the observed scores {zrp}(r,p)∈A are quantized scores obtained by quantizing the latent scores {yrp}(r,p)∈A to integers, that is, zrp = yrp , (r, p) ∈ A. 3 We focus on analysis of variables {yrp}(r,p)∈A. Since the quantization process from y to z is itself deterministic, we consider the joint likelihood P(z, y; x∗). To maximize the likelihood, we take the maximization over both x∗, y. We focus on the solutions for y, which we aim to connect our output to. The connection between the consensus objective (which uses scores) and the log-likelihood under the Thurstone model with quantization is made in the following proposition.

Proposition 3. The maximizer {yrp}(r,p)∈A of the consensus objective has the largest likelihood under the Thustone model with quantization, that is,

arg min
{yrp }(r,p)∈A

p∈[P ]

r:(r,p)∈A

 yrp − |{r

2

1

: (r , p) ∈ A}|

yr p

r :(r ,p)∈A

= arg max max log P({zrp}(r,p)∈A, {yrp}(r,p)∈A; {x∗p}p∈[P ])
{yrp}(r,p)∈A {x∗p}p∈[P ]

such that zrp − 0.5 ≤ yrp ≤ zrp + 0.5 ∀ (r, p) ∈ A.

The proof of this proposition is provided in Appendix B.3. We thus see that our consensus principle (Design Principle 3) also follows if we consider a standard parametric model.

4.4 Hyperparameter selection via Quantization Validation (QV)
In this section, we introduce a novel cross-validation-like method to choose the appropriate value of hyperparameter λ > 0 in Algorithm 1. The selected value of λ is then given as an input to the algorithm. In a hypothetical situation where we were to observe ground truth values yrp for some pairs of (r, p) ∈ A, it would be possible to perform traditional cross-validation where we choose the value of λ that achieves the best performance in estimation of the observed values of yrp on a holdout set. However, in practice, we only have access to the quantized scores. This motivates us to design a procedure to select λ, which performs validation on a dataset constructed by further quantization of the observations. We call this procedure Quantization Validation (QV).
Algorithm 2 Quantization-validation (QV)
Require: Inputs: observations of quantized scores {zrp}(r,p)∈A, rankings {πr}r∈[R], set of possible values of the hyperparameter Λ, quantization function q, loss function loss.
1: Calculate further-quantized scores zrp = q(zrp), ∀(r, p) ∈ A. 2: Calculate rankings
πr = {(p, p ) : zrp > zrp , (r, p) ∈ A, (r, p ) ∈ A}. 3: for λ ∈ Λ do 4: Obtain solution {zrp}(r,p)∈A by calling Algorithm 1 with λ, and inputs {zrp}(r,p)∈A and {πr}r∈[R]. 5: Compute the validation error eλ = loss(z, z), where z is the vector of {zrp}(r,p)∈A and z of {zrp}(r,p)∈A. 6: end for 7: Output selected value of hyperparameter λ ∈ arg minλ∈Λ eλ. (Ties are broken in favor of the smallest value.)
3Let · be the mapping from a number to the nearest integer.

9

We present our QV procedure in Algorithm 2. In words, given a set of possible values for λ, we ﬁrst construct
a validation set by further coarsening the observed quantized scores using fewer quantization bins. Then we select
the best value of λ that achieves the lowest loss in the recovery of the original quantized scores, via a function of our choice loss : R|A| × R|A| → R. We use Kendall-tau ranking error as the loss function, which is deﬁned formally in the experiments Section 5.1. Speciﬁcally, we select a quantization function q : R → R , to convert the quantized score zrp to zrp, ∀(r, p) ∈ A that has fewer quantization levels. The rankings are then re-computed from the quantized scores {zrp}(r,p)∈A as {πr}r∈[R]. We validate on the dataset consisting of {zrp}(r,p)∈A and {πr}r∈[R], where the goal is to recover {zrp}(r,p)∈A. Given a pre-speciﬁed set Λ of candidate values of λ, we compare the algorithm output {zrp}(r,p)∈A under each value of λ with its ground truth {zrp}(r,p)∈A, and compute the ranking error. Finally, we select the value of λ as the one that induces the smallest ranking error in the validation process, where ties are broken in
favor of the smallest value.

5 Experiments
We evaluate the empirical performance of our proposed algorithm on simulated data as well as on real-world data collected from the peer review process of the ICLR 2017 conference. The code for our algorithms and results is available online at https://github.com/MYusha/rankings_and_quantized_scores.

5.1 Implementation details
For each experiment, we run 20 trials and plot the mean and standard error of the mean. We treat differences less than 10−4 in values of yrp as ties.

Proposed algorithm. As discussed in Section 4.2, Algorithm 1 solves a strictly convex optimization problem with linear inequalities. We use CVXPY to obtain the solutions: The solver we used is CVXOPT with the tolerance for feasibility conditions (feastol) set as 10−6. The constant which enforces the strict inequality constraints is set as 0.05. Note that the algorithm is robust to the choice of (as long as it is small, such that the problem in Algorithm 1 is feasible): we present experiments demonstrating its robustness to the choice of in Appendix A.4.

Quantization validation. We use an exponential grid for candidate values of λ in Quantization validation: Λ = {exp(t/4) : 0 ≤ t < 40, t ∈ Z}. The quantization function is set to q(·) = 2· .

Performance measure. We use the (normalized) Kendall-tau ranking error as the loss function. Intuitively, given two rankings on the same set of elements, the Kendall-tau ranking error measures the number of pairs of items whose relationships are reversed in the two rankings. More formally, we deﬁne y to be the vector of {yrp}(r,p)∈A and y to be the vector of {yrp}(r,p)∈A. Then the normalized Kendall-tau ranking error between y and y is deﬁned as the following.

1 |{((r, p), (r , p )) : yrp > yr p }| (r,p),(r ,p ):yrp>yr p

1 I(yrp ≺ yr p )+ 2 I(yrp = yr p ) .

Note that the pairs ((r, p), (r , p )) which are tied in y are omitted from the computation above. In addition to the ranking error, we also measure the 2-error between the output values {yrp}(r,p)∈A and the ground truth values {yrp}(r,p)∈A as
r,p:(r,p)∈A |yr,p − yr,p|2.

5.2 Baseline methods
We compare the dequantized scores output by our algorithm with the outputs of the following two natural baseline methods.
1. Quantized scores: We simply use the observed scores {zrp}(r,p)∈A as the ﬁnal output.

10

(a) Varying standrad deviation σ.

(b) Varying number of papers per reviewer. (c) 2-error. Varying number of papers per reviewer.

Figure 2: Experiment results on synthetic data with varying levels of noise (standard deviation σ) and number of papers assigned to each reviewer.

2. BRE-adjusted-scores (Algorithm 3): In Section 4.3, we have shown that this algorithm is equivalent to our proposed algorithm when λ → ∞ and reviewers provide total rankings of assigned papers. In ICLR 2017 dataset, reviewers may provide partial rankings instead. We make simple adjustments to the baseline to accommodate, which are explained in Appendix C. For simplicity, we refer to this baseline in the ICLR 2017 dataset as BRE-adjusted-scores as well, throughout this section.

5.3 Synthetic dataset
We evaluate the performance of the proposed algorithm when the data is generated from a Thurstone model. For P papers to evaluate, their true scores are drawn independently from the uniform distribution x∗p ∼ Unif[1, 9] ∀p ∈ [P ]. The latent unquantized score yrp that is given by reviewer r to paper p is drawn from the normal distribution: yrp ∼ N (x∗p, σ2), where the parameter σ represents the standard deviation of the model, then clipped by [0, 10]. The quantized scores are then generated by rounding {yrp}(r,p)∈A to the nearest integer such that 0 ≤ zrp ≤ 10, zrp ∈ Z, ∀(r, p) ∈ A. For simplicity, we assign the same number of papers to each reviewer in the experiments. Similarly, we give the same number of reviewers (scores) to each paper. Given ﬁxed numbers of papers per reviewer and reviewers per paper, the reviewer assignment A = {(r, p) where r reviews p, r ∈ [R], p ∈ [P ]} is generated uniformly at random from all possible assignments. The default setting is set to P = 60, σ = 0.5, each paper gets 4 reviewers and each reviewer is assigned 4 papers. We examine the performance of our proposed algorithm under various settings where we change the parameters individually.
Varying levels of noise. We obtain solutions with data generated from the Thurstone model with different values of the standard deviations and show the results in normalized Kendall-tau error in Figure 2a. Note that under the setting with very small noise, the performances of the two baselines are slightly worse than those under the setting with a larger noise. This is caused by a larger number of ties between the quantized scores under a smaller noise level. Speciﬁcally, when σ = 1.0, the average percentage of ties in the output of the quantized scores and the BRE-adjusted-scores are 11.1% and 5.8% respectively. However, when σ decreases to 0.1, the average percentage of ties in their output increases to 12.4% and 6.3%. By considering both consensus between reviewers and the provided rankings in dequantizing the scores, our proposed algorithm has less than 1.5% ties in its output and outperforms the baseline methods under all three settings. We further show the distribution of selected λ over the 20 trials in Figure 6 in Appendix A. As the noise level increases, the distribution of λ selected by quantization-validation moves toward the larger end, which is an indication of decreasing weight on the consensus term in the objective in Algorithm 1.
Varying loads. We also consider settings with varying numbers of assigned papers to each reviewer and varying numbers of reviews for each paper. While varying one of the two variables, we ﬁx the other to be the same as the
11

(a) Varying number of papers per reviewers.

(b) 2-error. Varying number of papers per reviewers.

Figure 3: Experiment results on ICLR 2017 data.

default setting. The error rates with a varying number of papers assigned to each reviewer are shown in Figure 2b. As the number of assigned papers increases, the error of BRE-adjusted-scores baseline decreases, similar to the proposed algorithm. This is because the simple comparison injection employed by the BRE-adjusted-scores baseline lets the number of papers per reviewer directly dictate the number of possible values for dequantized scores. Results show that changes in the number of reviewers per paper do not affect the performance signiﬁcantly so we defer the corresponding plot of error rates to Figure 5 in Appendix A. In all settings with varying loads, our proposed algorithm consistently incurs smaller errors than the baselines.
2 error. In addition to the ranking error, we also report 2 errors on the synthetic dataset. The 2 errors are calculated with the same set of dequantized scores for which we report the ranking errors. Despite its primary focus on recovering the ranking among yrp(r,p)∈A, the proposed algorithm shows a small advantage in 2 error as well, compared to the baselines. The 2 errors with varying numbers of papers per reviewer are displayed in Figure 2c. The results on the 2 error for varying the noise and loads are qualitatively similar in that our method is no worse and offers a small improvement. The plots are deferred to Figure 7 in Appendix A.3.

5.4 Real-world dataset from ICLR 2017
We conduct experiments on data from the peer-review process of the ICLR 2017 conference [60]. There are P = 427 papers and every paper receives at least 3 reviews. For simplicity (so that number of papers is a multiple of the reviewer load), we keep P = 426 papers and retain 3 reviews for each paper by discarding some reviews (43 out of 1321 reviews are discarded). Since the reviewers are all anonymous in this dataset, we generate the reviewer assignments A in a random manner, subject to the constraint that a ﬁxed number of papers are assigned to every reviewer.
Each review score comprises an integer from 1 to 10, which we treat as the ground-truth values for unquantized scores {yrp}(r,p)∈A. We generate the quantized scores by putting the original review scores in fewer quantization levels, via the quantization procedure: zrp = yrp/2 , such that 1 ≤ zrp ≤ 5, zrp ∈ Z, ∀(r, p) ∈ A. The reviewer-provided partial rankings are generated from the original review scores as πr = {(p, p ) : yrp > yrp , (r, p) ∈ A, (r, p ) ∈ A}, since the ICLR 2017 conference did not collect rankings directly from reviewers.
Figure 3a plots the normalized Kendall-tau error of our algorithm and the baselines under varying numbers of papers per reviewer (note that we cannot control the noise level σ in the ICLR 2017 dataset as we do in the synthetic dataset.) We observe that our algorithm incurs a lower error as compared to the baselines. Furthermore, the error decreases as the number of papers per reviewer increases. As this number decreases from 6 to 2, the percentage of ties in the output of quantized scores remains at 32.2%, while the percentage in the output of BRE-adjusted-scores increases from 8.0% to 23.2%. Our algorithm outputs less than 3.4% percent of ties in all three settings.
12

Similar to the synthetic dataset, we report the 2 errors on the ICLR 2017 dataset in Figure 3b. Again, we observe that our estimator slightly outperforms the baselines in terms of the 2 error. More details can be found in Appendix A.3.

(a) Synthetic dataset 1: QV

(c) Synthetic dataset 2: QV

(e) ICLR 2017 dataset: QV

(b) Synthetic dataset 1: original data

(d) Synthetic dataset 2: original data

(f) ICLR 2017 dataset: original data

Figure 4: The ranking error with varying λ in synthetic and ICLR datasets. Top row: Error on the quantization validation set. Red circles mark the value selected by QV which achieves the smallest errors on the quantization-validation set. Bottom row: Error curves on the original dataset. The circles indicate performance under the hyperparameter value selected by QV (from the top row). This is compared to the performance of optimal value marked by red diamonds, which achieves the smallest error on the original dataset.

5.5 Hyperparameter λ selection via QV
We present results that shed light on the Quantization Validation (QV) hyperparameter selection process introduced in Section 4.4, as well as the effect of hyperparameter λ on the performance of the proposed algorithm. Figure 4 presents plots comparing the performance of different values of hyperparameter λ, the value chosen by QV, and the best value of λ chosen by a hypothetical oracle that has access to ground truth data. The plots compare these choices on two synthetic datasets (with parameters set as defaults speciﬁed in Section 5.3) and the ICLR 2017 dataset (with 6 reviewers per paper). The errors are shown in log-scale for clarity for ICLR 2017 data, note that y-axes may not start at 0 to be able to zoom in on the relevant parts. Our experiments reveal a strong performance of the quantization-validation process: We observe that it can select a good λ close to the optimal ideal value λ∗, and incurs an error close to that incurred by λ∗.
6 Discussion
We address the problem of aggregating quantized scores and rankings evaluations in the form of dequantized scores, applicable to important settings such as peer review. An aspect to keep in mind regarding any such adjustment that uses global data is that of privacy in peer review [44, 61, 62]. By providing Area Chairs the information aggregated across all
13

the reviewers who reviewed their assigned papers, we need to ensure that it should not inadvertently reveal the review information of paper(s) outside their scope. Another direction of future work is that of global versus subgroup accuracy. For example, some subgroups of papers might have a larger inter-reviewer disagreement, or fewer reviewer-provided comparisons than other papers, because of their ﬁelds. This phenomenon might affect the ranking errors in these subgroups through the consensus objective in our algorithm. In this work, we consider the Kendall-tau ranking error which is a global metric across all papers. It is also of interest to analyze and/or modify our proposed algorithm to ensure comparable error rates across various subgroups of papers. It is also possible that reviewers might take malicious/adversarial behaviors against our proposed algorithm to affect the ﬁnal outcome. For example, to improve the acceptance chance of his/her paper, a reviewer who is also a paper author might intentionally give low ratings to assigned papers. Therefore, making our algorithm strategyproof is also an important future direction. Finally, it is of interest to prove strong theoretical guarantees about the proposed algorithm including our quantization-validation method, or design new algorithms for this problem that have strong theoretical guarantees with good empirical performance.
7 Acknowledgements
This work was supported in parts by NSF CAREER award 1942124, NSF CIF 1763734, and a Google Research Scholar Award.
References
[1] Norman T Feather. The measurement of values: Effects of different assessment procedures. Australian Journal of Psychology, 25(3):221–231, 1973.
[2] Nihar B Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike Von Luxburg. Design and analysis of the nips 2016 review process. The Journal of Machine Learning Research, 19(1):1913–1946, 2018.
[3] George A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956.
[4] Petra Lietz. Research into questionnaire design: A summary of the literature. International journal of market research, 52(2):249–272, 2010.
[5] W Paul Jones and Scott A Loe. Optimal number of questionnaire response categories: More may not be better. Sage Open, 3(2):2158244013489691, 2013.
[6] William L Rankin and Joel W Grube. A comparison of ranking and rating procedures for value system measurement. European Journal of Social Psychology, 10(3):233–246, 1980.
[7] John R Douceur. Paper rating vs. paper ranking. ACM SIGOPS Operating Systems Review, 43(2):117–121, 2009.
[8] Nihar B Shah, Joseph K Bradley, Abhay Parekh, Martin Wainwright, and Kannan Ramchandran. A case for ordinal peerevaluation in moocs. In NIPS Workshop on Data Driven Education, pages 1–8, 2013.
[9] Nihar B Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, and Martin J Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. The Journal of Machine Learning Research, 17(1):2049–2095, 2016.
[10] David Soergel, Adam Saunders, and Andrew McCallum. Open scholarship and peer review: a time for experimentation. 2013.
[11] Fabian Wauthier, Michael Jordan, and Nebojsa Jojic. Efﬁcient ranking from pairwise comparisons. In International Conference on Machine Learning, pages 109–117, 2013.
[12] Brian Eriksson. Learning to top-k search using pairwise comparisons. In Artiﬁcial Intelligence and Statistics, pages 265–273, 2013.
[13] Mark Braverman, Jieming Mao, and S Matthew Weinberg. Parallel algorithms for select and partition with noisy comparisons. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 851–862, 2016.
14

[14] Nihar Shah, Sivaraman Balakrishnan, Aditya Guntuboyina, and Martin Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. In International Conference on Machine Learning, pages 11–20, 2016.
[15] Nihar B Shah and Martin J Wainwright. Simple, robust and optimal ranking from pairwise comparisons. The Journal of Machine Learning Research, 18(1):7246–7283, 2017.
[16] Cheng Mao, Jonathan Weed, and Philippe Rigollet. Minimax rates and efﬁcient algorithms for noisy sorting. arXiv preprint arXiv:1710.10388, 2017.
[17] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank centrality: Ranking from pairwise comparisons. Operations Research, 65(1):266–287, 2017.
[18] Ashwin Pananjady, Cheng Mao, Vidya Muthukumar, Martin J Wainwright, and Thomas A Courtade. Worst-case vs average-case design for estimation from ﬁxed pairwise comparisons. arXiv preprint arXiv:1707.06217, 2017.
[19] Arpit Agarwal, Prathamesh Patil, and Shivani Agarwal. Accelerated spectral ranking. In International Conference on Machine Learning, pages 70–79. PMLR, 2018.
[20] Rahul Makhijani and Johan Ugander. Parametric models for intransitivity in pairwise rankings. In The World Wide Web Conference, pages 3056–3062, 2019.
[21] Jingyan Wang, Nihar Shah, and R Ravi. Stretching the effectiveness of mle from accuracy to bias for pairwise comparisons. In International Conference on Artiﬁcial Intelligence and Statistics, pages 66–76. PMLR, 2020.
[22] Max Hopkins, Daniel M Kane, and Shachar Lovett. The power of comparisons for actively learning linear classiﬁers. arXiv preprint arXiv:1907.03816, 2019.
[23] Shiwei Zeng and Jie Shen. Learning halfspaces with pairwise comparisons: Breaking the barriers of query complexity via crowd wisdom. arXiv preprint arXiv:2011.01104, 2020.
[24] Max Hopkins, Daniel Kane, Shachar Lovett, and Gaurav Mahajan. Noise-tolerant, reliable active classiﬁcation with comparison queries. In Conference on Learning Theory, pages 1957–2006. PMLR, 2020.
[25] Yichong Xu, Sivaraman Balakrishnan, Arthur Dubrawski, and Aarti Singh. Regression with comparisons: Escaping the curse of dimensionality with ordinal information. Journal of machine learning research, 2020.
[26] Yichong Xu, Hongyang Zhang, Kyle Miller, Aarti Singh, and Artur Dubrawski. Noise-tolerant interactive learning using pairwise comparisons. In Neural Information Processing Systems, NIPS, 2017.
[27] Yichong Xu, Xi Chen, Aarti Singh, and Artur Dubrawski. Thresholding bandit problem with both duels and pulls. In Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2020.
[28] Yichong Xu, Aparna Joshi, Aarti Singh, and Artur Dubrawski. Zeroth order non-convex optimization with dueling- choice bandits. In Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2020.
[29] Thane Somers, Nicholas RJ Lawrance, and Geoffrey A Hollinger. Efﬁcient learning of trajectory preferences using combined ratings and rankings. In Proc. Robotics: Science and Systems Conference Workshop on Mathematical Models, Algorithms, and Human-Robot Interaction (RSS), Boston, MA, 2017.
[30] Nir Ailon. Aggregation of partial rankings, p-ratings and top-m lists. Algorithmica, 57(2):284–300, 2010.
[31] Michael Pearce and Elena A Erosheva. A uniﬁed statistical learning model for rankings and scores with application to grant panel review. arXiv preprint arXiv:2201.02539, 2022.
[32] Andrew Tomkins, Min Zhang, and William D Heavlin. Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713, 2017.
[33] Ivan Stelmakh, Nihar Shah, and Aarti Singh. On testing for biases in peer review. Advances in Neural Information Processing Systems, 32:5286–5296, 2019.
[34] Emaad Manzoor and Nihar B Shah. Uncovering latent biases in text: Method and application to peer review. In AAAI, 2021.
15

[35] Peter A. Flach, Sebastian Spiegler, Bruno Golénia, Simon Price, John Guiver, Ralf Herbrich, Thore Graepel, and Mohammed J. Zaki. Novel tools to streamline the conference review process: Experiences from SIGKDD’09. SIGKDD Explor. Newsl., 11(2):63–67, May 2010.
[36] Magnus Roos, Jörg Rothe, and Björn Scheuermann. How to calibrate the scores of biased reviewers by quadratic programming. In AAAI Conference on Artiﬁcial Intelligence, 2011.
[37] Jingyan Wang and Nihar B Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS, 2019.
[38] Sijun Tan, Jibang Wu, Xiaohui Bei, and Haifeng Xu. Least square calibration for peer reviews. Advances in Neural Information Processing Systems, 34, 2021.
[39] Carole J Lee. Commensuration bias in peer review. Philosophy of Science, 82(5):1272–1283, 2015.
[40] Ritesh Noothigattu, Nihar Shah, and Ariel Procaccia. Loss functions, axioms, and peer review. Journal of Artiﬁcial Intelligence Research, 2021.
[41] T. N. Vijaykumar. Potential organized fraud in ACM/IEEE computer architecture conferences. https://medium.com/ @tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d, 2020.
[42] Michael L Littman. Collusion rings threaten the integrity of computer science research. Communications of the ACM, 64(6):43–44, 2021.
[43] Ruihan Wu, Chuan Guo, Felix Wu, Rahul Kidambi, Laurens van der Maaten, and Kilian Q Weinberger. Making paper reviewing robust to bid manipulation attacks. arXiv preprint arXiv:2102.06020, 2021.
[44] Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In NeurIPS, 2020.
[45] Yichong Xu, Han Zhao, Xiaofei Shi, and Nihar Shah. On strategyproof conference review. In IJCAI, 2019.
[46] Komal Dhull, Steven Jecmen, Pravesh Kothari, and Nihar B Shah. Strategyprooﬁng peer assessment via partitioning: The price in terms of evaluators’ expertise. arXiv preprint arXiv:2201.10631, 2022.
[47] Nihar B Shah. An overview of challenges, experiments, and computational solutions in peer review. Communications of the ACM (to appear). Preprint available at http://bit.ly/PeerReviewOverview, July 2021.
[48] Anna Rogers and Isabelle Augenstein. What can we do to improve peer review in NLP? arXiv preprint arXiv:2010.03863, 2020.
[49] Maksims N Volkovs and Richard S Zemel. A ﬂexible generative model for preference aggregation. In Proceedings of the 21st international conference on World Wide Web, pages 479–488, 2012.
[50] Bruce Hajek, Sewoong Oh, and Jiaming Xu. Minimax-optimal inference from partial rankings. arXiv preprint arXiv:1406.5638, 2014.
[51] Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. arXiv preprint arXiv:0707.1051, 2007.
[52] Corinna Cortes and Neil D Lawrence. Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. arXiv preprint arXiv:2109.09774, 2021.
[53] Louis L Thurstone. A law of comparative judgment. Psychological review, 34(4):273, 1927.
[54] RE Barlow, DJ Bartholomew, JM Bremner, and HD Brunk. The theory and application of isotonic regression, 1972.
[55] Valerio Adinolﬁ and Edward H Sargent. Photovoltage ﬁeld-effect transistors. Nature, 542(7641):324–327, 2017.
[56] Matthias Bucher, Antonios Bazigos, François Krummenacher, Jean-Micehl Sallese, and Christian Enz. Ekv3. 0: An advanced charge based mos transistor model. a design-oriented mos transistor compact model. In Transistor Level Modeling for Analog/RF IC Design, pages 67–95. Springer, 2006.
16

[57] Prashant V Kamat. Absolute, arbitrary, relative, or normalized scale? how to get the scale right, 2019. [58] RL Hoffman. Zno-channel thin-ﬁlm transistors: Channel mobility. Journal of Applied Physics, 95(10):5813–5819, 2004. [59] Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and Daphne Koller. Tuned models of peer assessment
in moocs. arXiv preprint arXiv:1307.2579, 2013. [60] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy
Schwartz. A dataset of peer reviews (peerread): Collection, insights and NLP applications. In Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), New Orleans, USA, June 2018. [61] Wenxin Ding, Nihar B. Shah, and Weina Wang. On the privacy-utility tradeoff in peer-review data analysis. In AAAI Privacy-Preserving Artiﬁcial Intelligence (PPAI-21) workshop, 2020. [62] Wenxin Ding, Gautam Kamath, Weina Wang, and Nihar B. Shah. Calibration with privacy in peer review. arXiv 2201.11308, 2022.
Appendices
A Additional experimental results
In this appendix, we present additional experimental results supplementing those in Section 5.
A.1 Performance with varying numbers of reviewers per paper
In Figure 5, we show ranking error rates on the synthetic dataset with varying numbers of reviewers per paper. Change of this number does not affect the algorithm’s performance signiﬁcantly.
A.2 Distribution of hyperparameters selected by QV
In Figure 6, we show the distribution of hyperparameter value chosen by the quantization-validation procedure (Algorithm 2) on synthetic datasets. The distribution of values changes most signiﬁcantly as the noise standard deviation σ increases. This is because increasing σ makes scores more divergent for each paper. The QV effectively captures this and as a result, chooses larger λ as the noise level σ increases, which leads to decreasing weight on the consensus term in the objective of the optimization problem in Algorithm 1. The change in the number of papers per reviewer, or the number of reviewers per paper does not signiﬁcantly affect the range of hyperparameter values selected by the QV.
Figure 5: Synthetic data with varying numbers of reviewers per paper.
17

(a) Noise level σ = 0.1

(b) Noise level σ = 1.0

(c) Number of assigned papers per reviewer = 2

(d) Number of assigned papers per reviewer = 6

(e) Number of scores per paper = 2

(f) Number of scores per paper = 6

Figure 6: Distributions of λ selected by QV in synthetic dataset with varying parameters.

18

(a) Varying noise level σ.

(b) Varying number of reviewers per paper.

Figure 7: Additional experiment results on 2 errors on synthetic data.

(a) Experimental results on synthetic data.

(b) Experimental results on ICLR 2017 data.

Figure 8: Experimental results with varying .

This indicates that these two parameters have no signiﬁcant effect on how much the algorithm relies on consensus, as opposed to the noise level σ.
A.3 Performance in 2 errors
We display the additional results in 2 error on synthetic data in Figure 7. For the ICLR 2017 dataset, where the 2 errors are shown in Figure 3b, we provide additional details as follows. Given the data-generation process, we ﬁrst project the dequantized scores back to integers in [1, 10] by the function yr,p = 2yr,p − 0.5 , (r, p) ∈ A. For example, yrp in the interval [1, 1.5) is projected to 2, and yrp in the interval [1.5, 2) is projected to 3. For synthetic dataset, we provide the additional results of performance in 2 error with varying levels of noise σ and numbers of reviews per paper. The results under these settings are shown in Figure 7.
A.4 Performance with varying
We show that the proposed algorithm is robust to several choices of small . We vary the value of for our proposed algorithm and obtained results on both synthetic and ICLR 2017 data, which are shown in Figure 8. For synthetic data, the parameters are set to default (Section 5.3). For the ICLR 2017 data, the only parameter which is the number of papers per reviewer is set to 6. Performances of the baselines are also plotted as a reference.
19

B Proofs of Propositions
In this section, we present the proofs for propositions from Section 4.3.

B.1 Proof of Proposition 1
For clarity, we ﬁrst present the full procedure of BRE-adjusted-scores here. It is also one of the baseline methods that we evaluate in Section 5, for synthetic data and with a simple adjustment for the ICLR 2017 data. For conciseness, we ﬁx the number of assigned papers across reviewers and denote this constant as κ.

Algorithm 3 BRE-adjusted-scores

Require: The ranking and quantized scores provided by each reviewer: {zrp}(r,p)∈A and {πr}r∈[R]. For simplicity,

assume πr is total ranking among papers πr = {pκ pκ−1 · · · p1}. A small constant .

1: for r ∈ {reviewers} do

2: Divide all the reviewed papers by their quantized scores, denote the set of quantization bins as B.

3: for B ∈ B do

4:

Deﬁne the ranked papers in B as {pu pu−1 · · · pv}, where u ≥ v and u − v + 1 represents number of

papers in this quantization bin. Denote the score value which is the same for all papers in B as zB.

5:

for t = v . . . u do

6:

Set yr,pt = zB + (t − v) × .

7:

end for

8:

Adjust the values yr,p = yr,p −

1 |{p:p∈B}|

p:p∈B yr,p − zB for p ∈ B.

9: end for

10: end for

11: Output {yrp}(r,p)∈A

We ﬁrst introduce the notion of quantization bins, which is deﬁned separately across different reviewers. For a reviewer r, a quantization bin is a group of papers with the same quantized scores given by r. Each reviewer can have at least 1 quantization bin and at most κ quantization bins in the assigned papers. In Algorithm 3, the quantized score zrp is incremented by a value according to the rank of p inside its quantization bin, then “centered” to form the output whose mean value inside each quantization bin remains the same. Precisely, yrp can be explicitly written as follows. Note that in Algorithm 3, the choice of does not change the ranking of output {yrp}(r,p)∈A and for implementation, we choose to be the same as in the proposed algorithm, which is 0.05.
yrp = zrp + πr(p) − 1 − Prp , (1) 22
where Prp denotes the total number of papers in the bin that p belongs to, within the scope of reviewer r, and πr(p) denotes the ranking of p inside its quantization bin and within the scope of papers reviewed by r. For example, if reviewer r reviews only two papers p and p , and gives identical scores to the two papers, and ranks them as p p , then we have πr(p) = 2, πr(p ) = 1.
Having established the procedure in Algorithm 3, we now study (i) the reduction from our proposed algorithm as λ = ∞ to BRE-adjusted-scores, and (ii) the connection between BRE-adjusted-scores and BRE.

B.1.1 Reduction from proposed algorithm to BRE-adjusted-scores Recall that, as λ = ∞, the proposed algorithm reduces to the following optimization problem which does not have the objective term that captures reviewer consensus. yr is the vector of {yr,p}(r,p)∈A and yr, zr are that of
20

{yr,p}(r,p)∈A, {zr,p}(r,p)∈A.

yr = arg min yr − zr 2,

(2)

yr ∈Rκ

such that yrp ≥ yrp + whenever (r, p) ∈ A, (r, p ) ∈ A, and p r p

zrp − 0.5 ≤ yrp ≤ zrp + 0.5 ∀ (r, p) ∈ A.

(3)

Recall that we assume to be a small constant such that the feasible set is not empty. For example, in practice we
choose to be 0.05 (Section 5). Given the problem in (2), the solutions yrp are different from zrp only when there
exist tied scores given by a reviewer. Precisely, with the assumption that reviewers report total rankings, consider the problem of ﬁnding the vector x = [x1 . . . xm] ∈ Rm such that x1 = x2 + = · · · = xm + (m − 1) that minimizes the objective function x − c 22, c = [c, c, . . . , c] ∈ Rm where c is some constant. Let us set the derivative of the objective function with respect to xm to ﬁnd the minimizer:

m−1

m−1

xm = arg min (x + i − c)2 → xm = c −

.

x

2

i=0

Therefore xi = xm + (m − i) = c + (m − i) − m2−1 . In the setting of (2), x corresponds to the vector of scores of papers with tied scores for a ﬁxed reviewer. c is the score value, m is the total number of papers in the quantization
bin. Therefore, we have the solution to the problem in (2) as:

yrp = zrp +

m−1

(m − i) − 2

= zrp +

πr(p) − 1 − Prp − 1 2

= zrp + πr(p) − 1 − Prp . (4) 22

It is easy to see the equivalence between (1) and (4), indicating that under the assumption that each reviewer provides total ranking πr among all assigned papers, the proposed algorithm reduces to Algorithm 3 as λ = ∞.

B.1.2 Relationship between BRE-adjusted-scores and BRE

Given possibly noisy, randomly collected pairwise comparisons where each pair is compared with a certain probability, the BRE estimates a score for each item (paper). The ﬁnal goal in their work is global ranking, which is revealed by the estimated scores. In their setting, each pair can only be compared once, and the score of an item is calculated as the relative number of items preceding and succeeding it. Using our notations, let us denote the output score of paper p as x∗p, since they do not distinguish between different reviewers. In BRE algorithm, the estimated scores for a paper is

x∗p ∝ |p ∈ [P ] : p = p, p ≺ p| − |p ∈ [P ] : p = p, p p| .

(5)

Recall the adjustment to scores performed by BRE-adjusted-scores in (1). For a pair of reviewer and paper (r, p), let Br(p) denote the set of papers reviewed by r and are in the same quantization bin as p. We can write that:

Prp = |p : p ∈ Br(p), p ≺ p| + |p : p ∈ Br(p), p πr(p) = |p : p ∈ Br(p), p ≺ p| + 1.

p| + 1,

Plugging the above back in (1) gives us:

yrp = zrp + 2 (|p : p ∈ Br(p), p ≺ p| − |p : p ∈ Br(p), p p|) (6)
It is easy to see that the adjustment amount conditioned on the quantized score in (6) is proportional to the relative difference between the number of papers preceding and succeeding paper p in its quantization bin. The multiplicative factor is simply the small constant /2, since the output scores are arbitrary-scale (Section 4.2).
Combining B.1.1 and B.1.2, we proved the following in the special case where (i) reviewers report total rankings of assigned paper and (ii) λ = ∞ in Algorithm 1: Algorithm 1 adds to each quantized score a value that is proportional to the estimated score from BRE, when BRE is given the ranking information within the quantization bin.

21

B.2 Proof of Proposition 2
Recall that µ is the number of scores received by each paper. Without ranking information, {yrp} can be solved separately for each paper p ∈ [P ]. For a ﬁxed p, the proposed algorithm reduces to the following optimization problem.

arg min

(yrp − y¯p)2 + λ

(yrp − zrp)2.

(7)

{yrp}(r,p)∈E reviewers r:(r,p)∈A

(r,p)∈A

such that zrp − 0.5 ≤ yrp ≤ zrp + 0.5 ∀ (r, p) ∈ A.

When λ > 0, the objective is a strictly multivariate convex function. For every reviewer r that reviews paper p, the partial derivative of objective in (7) is as follows. For simplicity, let us denote the objective in (7) as fp.





∂fp = 2 yrp − 1

yrp + λ(yrp − zrp) .

∂yrp

µ
r:(r,p)∈A

When yrp = µ1(+1+µλλ) zrp + r =r µ(11+λ) zr p, the convex multivariate objective function fp achieves local minimum since its derivatives achieve 0 for all variables. Since the function is strictly convex, the local minimum is the global

minimum. If the global minimum is inside the feasible set deﬁned by the linear inequalities, then the solution to the

problem in (7) is

1 + µλ

1

yrp = µ(1 + λ) zrp + µ(1 + λ) zr p. (8)

r =r

In other words, {yrp} is the weighted average of scores for paper p, where the weights are dependent on constant µ (number of reviews for each paper) and the hyperparameter λ. If the global minimum is outside the feasible set, observe that the objective function in (7) is convex to each variable yrp if other variables are ﬁxed. Therefore, in this case, the solution {yrp} is the closest point in [zrp − 0.5, zrp + 0.5] to the right-hand side in (8). To summarize, the solution to (7) is as follows.







1 + µλ

1

yrp = min max  µ(1 + λ) zrp + µ(1 + λ) zr p, zrp − 0.5 , zrp + 0.5 . (9)

r =r

B.3 Proof of Proposition 3

The likelihood for all r, p : (r, p) ∈ A which we study can be expressed as

P

{zr

p

},

{

yr

p

}|{x

∗ p

}

∝P

{zrp} | {yrp}, {x∗p}

P

{yrp} | {x∗p}

We thus have the following equivalence for taking maximization over the latent ys and x∗s. Note that we study the dequantized scores, so focusing on the solutions for yr,p.

arg max max P({zrp}, {yrp} | x∗p) = arg max max P({zrp} | {yrp}, x∗p) P({yrp} | x∗p)

(10)

{yrp} {x∗p}

{yrp} {x∗p}

Given observations of {zrp}(r,p)∈A, the likelihood P {zrp}(r,p)∈A | {yrp}(r,p)∈A, {x∗p}p∈[P ] equals 1 only when such consistency is satisﬁed: yrp ∈ [zrp − 0.5, zrp + 0.5], ∀(r, p) ∈ A and equals 0 otherwise, due to the deterministic nature of the quantization process. If consistency is satisﬁed, we then consider optimization of the second likelihood on
the right-hand side by taking the logarithm of it.

∗

(yrp − x∗p)2

1

log P({yrp} | xp) =

− 2σ2

log( √ ) σ 2π

r:(r,p)∈A

∝−

(yrp − x∗p)2.

(11)

r:(r,p)∈A

22

For a ﬁxed paper p, we can ﬁnd the maximizer x∗p of (11) by setting the derivative to 0, which yields x∗p =

1 |{r :(r ,p)∈A}|

r:(r,p)∈A yrp for all p ∈ [P ]. Plugging x∗p, ∀p ∈ [P ] back into (10) and we have that:

arg max max log P({zrp}, {yrp}; x∗) = arg min (yrp −

1

yrp)2.

{yrp} {x∗p}

p {yrp} r

|{r : (r , p) ∈ A}|
r:(r,p)∈A

s.t. zrp − 0.5 ≤ yrp ≤ zrp + 0.5

Therefore, the maximizers {yrp}r,p of likelihood function under the Thurstone model with quantization is the same as the maximizer of the consensus objective in Algorithm 1, with the constraints that yrp’s are consistent with the quantized scores zrp’s.

C Baseline method for ICLR 2017 data

In Algorithm 3 in Section B.1, we show the baseline method when reviewers provide total rankings of assigned papers. However, in the ICLR dataset (Section 5.4), we derive the reviewer-reported rankings from their original review scores, since ranking information was not collected directly from reviewers. The original scores are integers in 1 ∼ 10, and the rankings are derived as: πr = {(p, p ) : yrp > yrp , (r, p) ∈ A, (r, p ) ∈ A}. Therefore, the reviewer-reported rankings are not total rankings over assigned papers, whenever there exist ties in the original review scores {yrp}(r,p)∈A. Instead, they can be seen as a total ranking over groups of paper. Precisely, the rankings observed in this dataset are a subset of partial rankings that can be expressed as πr = {GT GT −1 . . . G1}, where each Gi, i ∈ [T ] represents a group of paper with tied scores, p p if p ∈ Gu, p ∈ Gv and u, v : u > v, u ∈ [T ], v ∈ [T ]. For input to algorithms, if yrp = yrp , then p, p ∈ Gi for some i ∈ [T ].
For the ICLR 2017 dataset with partial rankings, we employ a baseline algorithm that can be seen as a generalization of Algorithm 3. The baseline method for the ICLR 2017 dataset is deﬁned in Algorithm 4. The difference is that output scores are now adjusted from the quantized scores in groups instead of individually. Precisely, the score adjustment of an item is not dependent on the number of items preceding and succeeding it, but on the number of groups preceding and succeeding its group. When reviewers provide total rankings of assigned papers, each group only contains one paper and consequently, Algorithm 4 reduces to Algorithm 3.
In the following algorithm, the value of remains 0.05.

Algorithm 4 Partial-rankings-adjusted-scores

Require: The ranking and scores provided by each reviewer: {zrp}(r,p)∈A and {πr}r∈[R]. πr is partial ranking among

papers πr = {GT GT −1 . . . G1}. A small constant .

1: for r ∈ {reviewers} do

2: Divide all the reviewed papers by their quantized scores, denote the set of quantization bins as B.

3: for B ∈ B do

4:

if papers {p}p∈B belong to more than one group then

5:

Deﬁne the set of groups in B as {Gu Gu−1 · · · Gv}, where u ≥ v and u − v + 1 represents the

number of groups in this quantization bin. Denote the score value for bin B as zB.

6:

for t = v . . . u do

7:

Set yr,p = zB + (t − v) × , for all p ∈ Gt.

8:

end for

9:

Adjust the values yr,p = yr,p −

1 |{p:p∈B}|

p:p∈B yr,p − zB for p ∈ B.

10:

end if

11: end for

12: end for

13: Output {yrp}(r,p)∈A

23

