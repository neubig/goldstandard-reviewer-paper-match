arXiv:2011.06741v3 [cs.LG] 27 Oct 2021

Rebounding Bandits for Modeling Satiation Eﬀects
Liu Leqi∗1, Fatma Kılınç-Karzan2, Zachary C. Lipton1,2, and Alan L. Montgomery1,2
1Machine Learning Department 2Tepper School of Business Carnegie Mellon University
October 27, 2021
Abstract
Psychological research shows that enjoyment of many goods is subject to satiation, with short-term satisfaction declining after repeated exposures to the same item. Nevertheless, proposed algorithms for powering recommender systems seldom model these dynamics, instead proceeding as though user preferences were ﬁxed in time. In this work, we introduce rebounding bandits, a multi-armed bandit setup, where satiation dynamics are modeled as time-invariant linear dynamical systems. Expected rewards for each arm decline monotonically with consecutive exposures to it and rebound towards the initial reward whenever that arm is not pulled. Unlike classical bandit settings, methods for tackling rebounding bandits must plan ahead and model-based methods rely on estimating the parameters of the satiation dynamics. We characterize the planning problem, showing that the greedy policy is optimal when the arms exhibit identical deterministic dynamics. To address stochastic satiation dynamics with unknown parameters, we propose Explore-Estimate-Plan (EEP), an algorithm that pulls arms methodically, estimates the system dynamics, and then plans accordingly.
1 Introduction
Recommender systems suggest such diverse items as music, news, restaurants, and even job candidates. Practitioners hope that by leveraging historical interactions, they might provide services better aligned with their users’ preferences. However, despite their ubiquity in application, the dominant learning framework suﬀers several conceptual gaps that can result in misalignment between machine behavior and human preferences. For example, because human preferences are seldom directly observed, these systems are typically trained on the available observational data (e.g., purchases, ratings, or clicks) with the objective of predicting customer behavior [4, 27]. Problematically, such observations tend to be confounded (reﬂecting exposure bias due to the current recommender system) and subject to censoring (e.g., users with strong opinions are more likely to write reviews) [41, 16].
Even if we could directly observe the utility experienced by each user, we might expect it to depend, in part, on the history of past items consumed. For example, consider the task of automated (music) playlisting. As a user is made to listen to the same song over and over again, we might expect that the utility derived from each consecutive listen would decline [35]. However, after listening to other music for some time, we might expect the utility associated with that song to bounce back towards its baseline level. Similarly, a diner served pizza for lunch might feel diminished pleasure upon eating pizza again for dinner.
The psychology literature on satiation formalizes the idea that enjoyment depends not only on
∗Corresponding author: leqil@cs.cmu.edu.
1

one’s intrinsic preference for a given product but also on the sequence of previous exposures and the time between them [3, 6]. Research on satiation dates to the 1960s (if not earlier) with early studies addressing brand loyalty [42, 28]. Interestingly, even after controlling for marketing variables like price, product design, promotion, etc., researchers still observe brand-switching behavior in consumers. Such behavior, referred as variety seeking, has often been explained as a consequence of utility associated with the change itself [25, 17]. For a comprehensive review on hedonic decline caused by repeated exposure to a stimulus, we refer the readers to [11].
In this paper, we introduce rebounding bandits, a multi-armed bandits (MABs) [37] framework that models satiation via linear dynamical systems. While traditional MABs draw rewards from ﬁxed but unknown distributions, rebounding bandits allow each arm’s rewards to evolve as a function of both the per-arm characteristics (susceptibility to satiation and speed of rebounding) and the historical pulls (e.g., past recommendations). In rebounding bandits, even if the dynamics are known and deterministic, selecting the optimal sequence of T arms to play requires planning in a Markov decision process (MDP) whose state space scales exponentially in the horizon T . When the satiation dynamics are known and stochastic, the states are only partially observable, since the satiation of each arm evolves with (unobserved) stochastic noises between pulls. And when the satiation dynamics are unknown, learning requires that we identify a stochastic dynamical system.
We propose Explore-Estimate-Plan (EEP) an algorithm that (i) collects data by pulling each arm repeatedly, (ii) estimates the dynamics using this dataset; and (iii) plans using the estimated parameters. We provide guarantees for our estimators in § 6.2 and bound EEP’s regret in § 6.3.
Our main contributions are: (i) the rebounding bandits problem (§3), (ii) analysis showing that when arms share rewards and (deterministic) dynamics, the optimal policy pulls arms cyclically, exhibiting variety-seeking behavior (§4.1); (iii) an estimator (for learning the satiation dynamics) along with a sample complexity bound for identifying an aﬃne dynamical system using a single trajectory of data (§6.2); (iv) EEP, an algorithm for learning with unknown stochastic dynamics that achieves sublinear w-step lookahead regret [34] (§6); and (v) experiments demonstrating EEP’s eﬃcacy (§7).
2 Related Work
Satiation eﬀects have been addressed by such diverse disciplines as psychology, marketing, operations research, and recommendation systems. In the psychology and marketing literatures, satiation has been proposed as an explanation for variety-seeking consumer behavior [11, 25, 26]. In operations research, addressing continuous consumption decisions, [3] propose a deterministic linear dynamical system to model satiation eﬀects. In the recommendation systems community, researchers have used semi-Markov models to explicitly model two states: (i) sensitization— where the user is highly interested in the product; and (ii) boredom—where the user is not engaged [18].
The bandits literature has proposed a variety of extensions where rewards depend on past exposures, both to address satiation and other phenomena. [14, 21, 39] tackle settings where each arm’s expected reward grows (or shrinks) monotonically in the number of pulls. By contrast, [19, 2, 7] propose models where rewards increase as a function of the time elapsed since the last pull. [34] model the expected reward as a function of the time since the last pull drawn from a Gaussian Process with known kernel. [43] propose a model where rewards are linear functions of the recent history of actions and [29] model the reward as a function of a context that evolves according to known deterministic dynamics. In rested bandits [12], an arm’s rewards changes only when it is played, and in restless bandits [44] rewards evolve independently from the play of
2

012345 012345

Satiation Reward Pull

Satiation Reward Pull

0

5

10

15

20

25

30

Time (t)

0

5

10

15

20

25

30

Time (t)

(a) γk = .5, λk = 3, bk = 3

(b) γk = .8, λk = 1.5, bk = 3

Figure 1: These plots illustrate the satiation level and reward of an arm from time 1 to 30. The two plots are generated with the same pull sequence, base rewards bk = 3 and realized noises with variance σz = .1. In Figure 1a, γk = .5 and λk = 3. In Figure 1b, γk = .8 and λk = 1.5. In both cases, the arm has started with 0 as its base satiation level. Black dashed line: the satiation level. Red solid line: the reward. Blue dots: time steps where the arm is pulled.

each arm.

Key Diﬀerences This may be the ﬁrst bandits paper to model evolving rewards through continuous-state linear stochastic dynamical systems with unknown parameters. Our framework captures several important aspects of satiation: rewards decline by diminishing amounts with consecutive pulls and rebound towards the baseline with disuse. Unlike models that depend only on ﬁxed windows or the time since the last pull, our model expresses satiation more organically as a quantity that evolves according to stochastic dynamics and is shocked (upward) by pulls. To estimate the reward dynamics, we leverage recent advances in the identiﬁcation of linear dynamical systems [40, 38] that rely on the theory of self-normalized processes [33, 1] and block martingale conditions [40].

3 Rebounding Bandits Problem Setup

Consider the set of K arms [K] := {1, . . . , K} with bounded base rewards b1, . . . , bK. Given a horizon T , a policy π1:T := (π1, . . . , πT ) is a sequence of actions, where πt ∈ [K] depends on past actions and observed rewards. For any arm k ∈ [K], we denote its pull history from 0 to T as the binary sequence uk,0:T := (uk,0, . . . , uk,T ), where uk,0 = 0 and for t ∈ [T ], uk,t = 1 if πt = k and uk,t = 0 otherwise. The subsequence of uk,0:T from t1 to t2 (including both endpoints) is denoted by uk,t1:t2 .
At time t, each arm k has a satiation level sk,t that depends on a satiation retention factor γk ∈ [0, 1), as follows

sk,t := γk(sk,t−1 + uk,t−1) + zk,t−1, ∀t > tk0,

(1)

where tk0 := mint{t : uk,t = 1} is the ﬁrst time arm k is pulled and zk,t−1 is independent and identically distributed noise drawn from N (0, σz2), accounting for incidental (uncorrelated) factors in the satiation dynamics. Because satiation requires exposure, arms only begin to have nonzero satiation levels after their ﬁrst pull, i.e., sk,0 = . . . = sk,tk = 0.
0
At time t ∈ [T ], if arm k is played with a current satiation level sk,t, the agent receives reward µk,t := bk − λksk,t, where bk is the base reward for arm k and λk ≥ 0 is a bounded exposure inﬂuence factor. We use satiation inﬂuence to denote the product of the exposure inﬂuence factor λk and the satiation level sk,t. In Figure 1, we show how rewards evolve in response to both pulls and the stochastic dynamics under two sets of parameters. The expected reward of

3

arm k (where the expectation is taken over all noises associated with the arm) monotonically decreases by diminishing amounts with consecutive pulls and increases with disuse by diminishing amounts.
Remark 1 (negative expected reward). We note that there exist choices of bk, γk, λk for which the expected reward of arm k can be negative. In the traditional bandits setup, one must pull an arm at every time step. Thus, what matters are the relative rewards and the problem is mathematically identical, regardless of whether the expected rewards range from −10 to 0 or 0 to 10. In addition, one might construct settings where negative expected rewards are reasonable. For example, when one of the arms corresponds to no recommendation with 0 being its expected reward (e.g., bk = 0, λk = 0), then the interpretation of negative expected reward would be that the corresponding arm (item) is less preferred relative to not being recommended.
Given horizon T ≥ 1, we seek an optimal pull sequence π1:T , where πt depends on past rewards and actions (π1, µπ1,1, . . . , πt−1, µπt−1,t−1) and maximizes the expected cumulative reward:

GT (π1:T ) := E

T t=1

µπt,t

.

(2)

Additional Notation Let γ := maxk∈[K] γk and λ := maxk∈[K] λk. We use a for some positive constant C.

b when a ≤ Cb

4 Planning with Known Dynamics

Before we can hope to learn an optimal policy with unknown stochastic dynamics, we need to establish a procedure for planning when the satiation retention factors, exposure inﬂuence factors, and base rewards are known. We begin by presenting several planning strategies and analyzing them under deterministic dynamics, where the past pulls exactly determine each arm’s satiation level, i.e., sk,t = γk(sk,t−1 + uk,t−1), ∀t > tk0. With some abuse of notation, at time t ≥ 2, given a pull sequence uk,0:t−1, we can express the satiation and the expected1 reward of each arm as

sk,t(uk,0:t−1) = γk (sk,t−1 + uk,t−1) = γk (γk (sk,t−2 + uk,t−2)) + γkuk,t−1 =

µk,t(uk,0:t−1) = bk − λk

t−1 i=1

γkt−iuk,i

.

t−1 i=1

γkt−iuk,i,

(3)

At time t = 1, we have that sk,1(uk,0:0) = 0 and µk,1(uk,0:0) = bk for all k ∈ [K]. Since the arm parameters {λk, γk, bk}Kk=1 are known, our goal (2) simpliﬁes to ﬁnding a pull sequence that solves the following bilinear integer program:


KT

t−1

K



 max

uk,t bk − λk γt−iuk,i : uk,t = 1, ∀t ∈ [T ],

 (4)

uk,t

k

k=1

k=1 t=1

i=0

uk,t ∈ {0, 1}, uk,0 = 0, ∀k ∈ [K], ∀t ∈ [T ]

where the objective maximizes the expected cumulative reward associated with the pull sequence and the constraints ensure that at each time period we pull exactly one arm. Note that (4) includes products of decision variables uk,t leading to bilinear terms in the objective. In Appendix A, we provide an equivalent integer linear program.
1We use “expected reward” to emphasize that all results in this section also apply to settings where the satiation dynamics are deterministic but the rewards are stochastic, i.e., µk,t = bk − λksk,t + ek,t for independent mean-zero noises ek,t.

4

4.1 The Greedy Policy
At each step, the greedy policy πg picks the arm with the highest instantaneous expected reward. Formally, at time t, given the pull history {uk,0:t−1}Kk=1, the greedy policy picks
πtg ∈ arg max µk,t(uk,0:t−1).
k∈[K ]
In order to break ties, when all arms have the same expected reward, the greedy policy chooses the arm with the lowest index.
Note that the greedy policy is not, in general, optimal. Sometimes, we are better oﬀ allowing the current best arm to rebound even further, before pulling it again.
Example 1. Consider the case with two arms. Suppose that arm 1 has base reward b1, satiation retention factor γ1 ∈ (0, 1), and exposure inﬂuence factor λ1 = 1. For any ﬁxed time horizon T > 2, suppose that arm 2 has b2 = b1 + γ12−−γγ22T where γ2 ∈ (0, 1) and λ2 = 1. The greedy policy π1g:T will keep pulling arm 2 until time T − 1 and then play arm 1 (or arm 2) at time T . This is true because if we keep pulling arm 2 until T − 1, at time T , we have µ2,T (u2,0:T −1) = b1 = µ1,T (u1,0:T −1). However, the policy π1n:T , where πtn = 2 if t ≤ T − 2, πTn−1 = 1, and πTn = 2, obtains a higher expected cumulative reward. In particular, the diﬀerence GT (π1n:T ) − GT (π1g:T ) will be γ2 − γ2T −1.
4.2 When is Greedy Optimal?
When the satiation retention factors γk = 0 for all k ∈ [K], i.e., when the satiation eﬀect is always 0, we know that the greedy policy (which always plays the arm with the highest instantaneous expected reward) is optimal. However, when satiation can be nonzero, it is less clear under what conditions the greedy policy performs optimally. This question is of special interest when we consider human decision-making, since we cannot expect people to solve large-scale bilinear integer programs every time they pick music to listen to.
In this section, we show that when all arms share the same properties (γk, λk, bk are identical for k ∈ [K]), the greedy policy is optimal. In this case, the greedy policy exhibits variety-seeking behavior as it plays the arms cyclically. Interestingly, this condition aligns with early research that has motivated studies on satiation [42, 28]: when controlling for marketing variables (e.g., the arm parameters γk, λk, bk), researchers still observe variety-seeking behaviors of consumers (e.g., playing arms in a cyclic order).
Assumption 1. γ1 = . . . = γK = γ, λ1 = . . . = λK = λ, and b1 = . . . = bK = b.
We start with characterizing the greedy policy when Assumption 1 holds.
Lemma 1 (Greedy Policy Characterization). Under Assumption 1 and the tie-breaking rule that when all arms have the same expected reward, the greedy policy chooses the one with the lowest arm index, the sequence of arms pulled by the greedy policy forms a periodic sequence: π1 = 1, π2 = 2, · · · , πK = K, and πt+K = πt, ∀t ∈ N+.
In this case, the greedy policy is equivalent to playing the arms in a cyclic order. All proofs for the paper are deferred to the Appendices. Theorem 1. Under Assumption 1, given any horizon T , the greedy policy π1g:T is optimal. Remark 2. Theorem 1 suggests that when the (deterministic) satiation dynamics and base rewards are identical across arms, planning does not require knowledge of those parameters.
5

Lemma 1 and Theorem 1 lead us to conclude the following result: when recommending items that share the same properties, the best strategy is to show the users a variety of recommendations by following the greedy policy.
On a related note, Theorem 1 also gives an exact Max K-Cut of a complete graph KT on T vertices, where the edge weight connecting vertices i and j is given by e(i, j) = λγ|j−i| for i = j. The Max K-Cut problem partitions the vertices of a graph into K subsets P1, . . . PK, such that the sum of the edge weights connecting the subsets are maximized [10]. Mapping the Max K-Cut problem back to our original setup, each vertex represents a time step. If vertex i is assigned to subset Pk, it suggests that arm k should be played at time i. The edge weights e(i, j) = λγ|j−i| for i = j can be seen as the reduction in satiation inﬂuence achieved by not playing the same arm at both time i and time j. The goal (4) is to maximize the total satiation inﬂuence reduction.
Proposition 2 (Connection to Max K-Cut). Under Assumption 1, an optimal solution to (4) is given by a Max K-Cut on KT , where KT is a complete graph on T vertices with edge weights e(i, j) = λγ|j−i| for all i = j.
Using Lemma 1 and Theorem 1, we obtain that an exact Max K-Cut of KT is given by ∀k ∈ [K], Pk = {t ∈ [T ] : t ≡ k (mod K)}, which may be a result of separate interest.

4.3 The w-lookahead Policy

To model settings where the arms correspond to items with diﬀerent characteristics (e.g., we
can enjoy tacos on consecutive days but require time to recover from a trip to the steakhouse)
we must allow the satiation parameters to vary across arms. Here, the greedy policy may
not be optimal. Thus, we consider more general lookahead policies (the greedy policy is a
special case). Given a window of size w and the current satiation levels, the w-lookahead policy
picks actions to maximize the total reward over the next w time steps. Let l denote T /w . Deﬁne ti = min{iw, T } for i ∈ [l] and t0 = 0. More formally, the w-lookahead policy π1w:T is deﬁned as follows: for any i ∈ [l], given the previously chosen arms’ corresponding pull histories {uwk,0:ti−1 }Kk=1 where uwk,0 = 0 and uwk,t = 1 if (and only if) πtw = k, the next w (or T mod w) actions πtwi−1+1:ti are given by

  ti

uk,0:ti−1 = uwk,0:ti−1 , ∀k ∈ [K], 

max

µπt,t(uπt,0:t−1) :

K k=1

uk,t

=

1,

∀t ∈ [T ],

(5)

 πti−1+1:ti t=ti−1+1

uk,t ∈ {0, 1}, ∀k ∈ [K], t ∈ [ti] 

In the case of a tie, one can pick any of the sequences that maximize (5). We recover the greedy policy when the window size w = 1, and ﬁnding the w-lookahead policy for the window size w = T is equivalent to solving (4).

Remark 3. Another reasonable lookahead policy, which requires planning ahead at every time step, would be the following: at every time t, plan for the next w actions and follow them for a single time step. Studying the performance of such a policy is of future interest. To lighten the computational load, we adopt the current w-lookahead policy which only requires planning every w time steps.

For the rest of the paper, we use Lookahead({λk, γk, bk}Kk=1, {uwk,0:ti−1}Kk=1, ti−1, ti) to refer to the solution of (5), where the arm parameters are {λk, γk, bk}Kk=1, the historical pull sequences of all arms till time ti−1 are given by {uwk,0:ti−1}Kk=1, and the solution corresponds to the actions that should be taken for the next ti − ti−1 time steps.

Theorem 2. Given any horizon T , let π1∗:T be a solution to (4). For a ﬁxed window size w ≤ T ,

we have that

∗

w

λγ(1 − γT −w)

GT (π1:T ) − GT (π1:T ) ≤ (1 − γ)2 T /w .

6

Remark 4. Note that when w = T , the w-lookahead policy by deﬁnition is the optimal policy and in such case, the upper bound for the optimality gap of w-lookahead established in Theorem 2 is also 0. In contrast to the optimal policy, the computational beneﬁt of the w-lookahead policy becomes apparent when the horizon T is large since it requires solving for a much smaller program (5). In general, the w-lookahead policy is expected to perform much better than the greedy policy (which corresponds to the case of w = 1) √at the expense of a higher computational c√ost. Finally, we note that for the window size of w = T , we obtain GT (π1∗:T ) − GT (π1w:T ) ≤ O( T ).
5 Learning with Unknown Dynamics: Preliminaries
When the satiation dynamics are unknown and stochastic (σz > 0), the learner faces a continuousstate partially observable MDP because the satiation levels are not observable. To set the stage, we ﬁrst introduce our state representation (§ 5.1) and a regret-based performance measure (§ 5.2). In the next section, we will introduce EEP, our algorithm for rebounding bandits.
5.1 State Representation
Following [32], at any time t ∈ [T ], we deﬁne a state vector xt in the state space X to be xt = (x1,t, n1,t, x2,t, n2,t, . . . , xK,t, nK,t), where nk,t ∈ N is the number of steps at time t since arm k was last selected and xk,t is the satiation inﬂuence (product of λk and the satiation level) as of the most recent pull of arm k. Since the most recent pull happens at t − nk,t, we have xk,t = bk − µk,t−nk,t = λksk,t−nk,t . Recall that µk,t−nk,t is the reward collected by pulling arm k at time t − nk,t. Note that bk is directly observed when arm k is pulled for the ﬁrst time because there is no satiation eﬀect. The state at the ﬁrst time step is x1 = (0, . . . , 0). Transitions between two states xt and xt+1 are deﬁned as follows: If arm k is chosen at time t, and reward µk,t is obtained, then the next state xt+1 will satisfy (i) for the pulled arm k, nk,t+1 = 1 and xk,t+1 = bk − µk,t; (ii) for other arms k = k, nk ,t+1 = nk ,t + 1 if nk ,t = 0, nk ,t+1 = 0 if nk ,t = 0, and the satiation inﬂuence remains the same xk ,t+1 = xk ,t.
Given {γk, λk, bk}Kk=1, the reward function r : X × [K] → R represents the expected reward of pulling arm k under state xt: If nk,t = 0, then r(xt, k) = bk. If nk,t ≥ 1, r(xt, k) = bk − γknk,t xk,t − λkγknk,t , which equals E[µk,t|xt], where the expectation is taken over the noises in between the current pull and the last pull of arm k. See Appendix C.1 for the full description of the MDP setup (including the transition kernel and value function deﬁnition) of rebounding bandits.
5.2 Evaluation Criteria: w-step Lookahead Regret
In reinforcement learning (RL), the performance of a learner is often measured through a regret that compares the expected cumulative reward obtained by the learner against that of an optimal policy in a competitor class [20]. In most episodic (e.g., ﬁnite horizon) RL literature [31, 15], regrets are deﬁned in terms of episodes. In such cases, the initial state is reset (e.g., to a ﬁxed state) after each episode ends, independent of previous actions taken by the leaner. Unlike these episodic RL setups, in rebounding bandits, we cannot restart from the initial state because the satiation level cannot be reset and user’s memory depends on past received recommendations. Instead, [34] proposed a version of w-step lookahead regret that divides the T time steps into T /w episodes where each episode (besides the last) consists of w time steps. At the beginning of each episode, the initial state is reset but depends on how the learner has interacted with the user previously. In particular, at the beginning of episode i + 1 (at time t = iw + 1), given that the learner has played π1:iw with corresponding pull sequence uk,0:iw for k ∈ [K], we reset the initial state to be xi = (µ1,iw+1(u1,0:iw), n1,iw+1, . . . , µK,iw+1(uK,0:iw), nK,iw+1) where µk,t(·) is deﬁned in (3) and nk,iw+1 is the number of steps since arm k is last pulled by the learner as of
7

time iw + 1. Then, given the learner’s policy π1:T , where πt : X → [K], the w-step lookahead regret, against a competitor class Cw (which we deﬁne later), is deﬁned as follows:

Regw(T ) =

T /w i=0

−1 maxπ˜1:w∈Cw

E

min{w,T −iw} j=1

r(xiw+j , π˜j (xiw+j ))

xiw+1

=

xi

−E

min{w,T −iw} j=1

r(xiw+j , πiw+j (xiw+j ))

xiw+1

=

xi

,

(6)

where the expectation is taken over xiw+2, . . . , xmin{(i+1)w,T }.
The competitor class Cw that we have chosen consists of policies that depend on time steps, i.e., Cw = {π˜1:w : π˜t = π˜t(xt) = π˜t(xt), π˜t ∈ [K], ∀t ∈ [w], xt, xt ∈ X } . We note that Cw subsumes many traditional competitor classes in bandits literature, including the class of ﬁxed-action policies considered in adversarial bandits [20] and the class of periodic ranking policies [7]. In our paper, the w-lookahead policy (including the T -lookahead policy given by (4)) is a time-dependent policy that belongs to Cw, since at time t, it will play a ﬁxed action by solving (5) using the true reward parameters {λk, γk, bk}Kk=1. The time-dependent competitor class Cw diﬀers from a state-dependent competitor class which includes all measurable functions π˜t that map from X to [K]. The state-dependent competitor class contains the optimal policy π∗ where πt∗(xt) depends on not just the time step but also the exact state xt. Finding the optimal state-dependent policy requires optimal planning for a continuous-state MDP, which relies on state space discretizion [31] or function approximation (e.g., approximate dynamic programming algorithms [30, 9, 36]). In Appendix C, we provide discussion and analysis on an algorithm compared against the optimal state-dependent policy. We proceed the rest of the main paper with Cw deﬁned above.
When w = 1, the 1-step lookahead regret is also known as the instantaneous regret, which is commonly used in restless bandits literature and some nonstationary bandits papers including [29]. Note that low instantaneous regret does not imply high expected cumulative reward in the longterm, i.e., one may beneﬁt more by waiting for certain arms to rebound. When w = T , we recover the full horizon regret. As we have noted earlier, ﬁnding the optimal competitor policy in this case is computationally intractable because the number of states, even when the satiation dynamics are deterministic, grows exponentially with the horizon T . Finally, we note that the w-step lookahead regret can be obtained for not just policies designed to look w steps ahead but any given policy. For a more comprehensive discussion on these notions of regret, see [34, Section 4].

6 Explore-Estimate-Plan

We now present Explore-Estimate-Plan (EEP), an algorithm for learning in rebounding bandits with stochastic dynamics and unknown parameters, that (i) collects data by pulling each arm a ﬁxed number of times; (ii) estimates the model’s parameters based on the logged data; and then (iii) plans according to the estimated model. Finally, we analyze EEP’s regret.
Because each arm’s base reward is known from the ﬁrst pull, whenever arm k is pulled at time t and nk,t = 0, we measure the satiation inﬂuence λksk,t, which becomes the next state xk,t+1:

xk,t+1 = λksk,t = λkγknk,t sk,t−nk,t + λkγknk,t + λk

nk,t −1 i=0

γki zk,t−1−i

= γknk,t xk,t+1−nk,t + λkγknk,t + λk

nk,t −1 i=0

γki

zk,t−1−i.

(7)

We note that the current state xk,t equals xk,t+1−nk,t , since xk,t+1−nk,t is the last observed satiation inﬂuence for arm k and nk,t is the number of steps since arm k was last pulled.

8

6.1 The Exploration Phase: Repeated Pulls
We collect a dataset Pkn by consecutively pulling each arm n+1 times, in turn, where n ≥ T 2/3/K (Line 4-7 of Algorithm 1). Speciﬁcally, for each arm k ∈ [K], the dataset Pkn contains a single trajectory of n+1 observed satiation inﬂuences x˜k,1, . . . , x˜k,n+1, where x˜k,1 = 0 and x˜k,j (j > 1) is the diﬀerence between the ﬁrst reward and the j-th reward from arm k. Thus, for x˜k,j, x˜k,j+1 ∈ Pkn, using (7) with nk,t = 1 (because pulls are consecutive), it follows that

x˜k,j+1 = γkx˜k,j + dk + z˜k,j ,

(8)

where dk = λkγk and z˜k,j are independent samples from N (0, σz2,k) with σz2,k = λ2kσz2. In Appendix E.2, we discuss other exploration strategies (e.g., playing the arms cyclically) for EEP
and their regret guarantees.

6.2 Estimating the Reward Model and Satiation Dynamics

For all k ∈ [K], given the dataset Pkn, we estimate Ak = (γk, dk) estimator :
Ak ∈ arg min Yk − XkA 22,
A∈R2

using the ordinary least squares

where Yk ∈ Rn is an n-dimensional vector whose j-th entry is x˜k,j+1 and Xk ∈ Rn×2 takes as its j-th row the vector xk,j = (x˜k,j, 1) , i.e., x˜k,j+1 is treated to be the response to the covariates xk,j. This suggests that

γk

−1

Ak =

= Xk Xk Xk Yk,

(9)

dk

and we take λk = |dk/γk|.
The diﬃculty in analyzing the ordinary least squares estimator (9) for identifying an aﬃne dynamical system (8) using a single trajectory of data comes from the fact that the samples are not independent. Asymptotic guarantees of the ordinary least squares estimators in this case have been studied previously in the control theory and time series communities [13, 22]. Recent work on system identiﬁcations for linear dynamical systems focuses on the sample complexity [40, 38]. Adapting the proof of [40, Theorem 2.4], we derive the following theorem for identifying our aﬃne dynamical system (8).
Theorem 3. Fix δ ∈ (0, 1). For all k ∈ [K], there exists a constant n0(δ, k) such that if the dataset Pkn satisﬁes n ≥ n0(δ, k), then

P Ak − Ak 2

1/(ψn) ≤ δ,

where ψ =

min

, . σz2,k(1−γk)2
16d2k (1−γk2 )+(1−γk )2 σz2,k

σz2,k 4(1−γk2)

As shown in Theorem 3, when dk = λkγk gets larger, the convergenc√e rate for Ak gets slower. G√iven a single trajectory of suﬃcient length, we obtain |γk − γk| ≤ O(1/ n√) and |dk − dk| ≤ O(1/ n). In Corollary 4, we show that the estimator of λk also achieves O(1/ n) estimation error.
√ Corollary 4. Fix δ ∈ (0, 1). Suppose that for all k ∈ [K], we have P( Ak − Ak 2 1/ n) ≤ δ and γk > 0. Then, with probability 1 − δ, we have that for all k ∈ [K],

1

1

|γk − γk| ≤ O √ , |λk − λk| ≤ O √ .

n

n

9

Algorithm 1: w-lookahead Explore-Estimate-Plan

Input: Lookahead window size w, Number of arms K, Horizon T

1 Initialize t = 1, π1:T to be an empty array of length T and T = T 2/3 + w − (T 2/3 mod w).

2 for k = 1, . . . , K do

3 Set t = t and initialize an empty array Pkn.

4 for c = 0, . . . , T /K do

5

Play arm k to obtain reward µk,t +c and add µk,t − µk,t +c to Pkn.

6

Set πt = k and increase t by 1.

7 end

8 Obtain γk, dk using the estimator (9), set λk = |dk/γk| and bk = µk,t .

9 end

10 Let t0 = T , set πt:t0 = (1, . . . , T − t + 1), and play πt:t0.
11 for i = 1, . . . , T −wt0 do 12 Set ti = min{ti−1 + w, T }.

13

Obtain

πti−1+1:ti

=

Lookahead

(

{λ

k

,

γ

k

,

bk

}

K k=1

,

{

uk

,0:ti

−1

}

K k=1

,

ti

−

1

,

ti

)

where

{uk,0:ti−1 }Kk=1 are the arm pull histories correspond to π1:ti−1 .

14

Play πti−1+1,ti .

15 end

6.3 Planning and Regret Bound
In the planning stage of Algorithm 1 (Line 11-15), at time ti−1 + 1, the next w arms to play are obtained through the Lookahead function deﬁned in (5) based on the estimated parameters from the estimation stage (Line 8). Using the results in Corollary 4, we obtain the following sublinear regret bound for w-lookahead EEP.
Theorem 5. There exists a constant T0 such that for all T > T0 and w ≤ T 2/3, the w-step lookahead regret of w-lookahead Explore-Estimate-Plan satisﬁes
Regw(T ) ≤ O(K1/2T 2/3 log T ).
Remark 5. The fact that EEP incurs a regret of order O(T 2/3) is expected for two reasons: First, EEP can be viewed as an explore-then-commit (ETC) algorithm that ﬁrst explores then exploits. The regret of EEP resembles the O(T 2/3) regret of the ETC algorithm in the classical K-armed bandits setting [20]. In rebounding bandits, the fundamental obstacle to mixing the exploration and exploitation stages is the need to estimate the satiation dynamics. When the rewards of each arm are not observed periodically, the obtained satiation inﬂuences can no longer be viewed as samples from the same time-invariant aﬃne dynamical system, since the parameters of the system depend on the duration between pulls. In practice, one may utilize the maximum likelihood estimator to obtain estimates of the reward parameters but obtaining the sample complexity of such an estimator with dependent data is diﬃcult. Second, it has been shown in [5] that when the rewards of the arms have temporal variation that depends on the horizon T , the worst case instantaneous regret has a lower bound Ω(T 2/3). On the other hand, in K-armed bandits, the regret (following the classical deﬁnition [20]) is lower bounded by Ω(T 1/2), and can be attained by methods like the upper conﬁdence bound algorithm [20]. Precisely characterizing the regret lower bound for rebounding bandits is of future interest.

7 Experiments
We now evaluate the performance of EEP experimentally, separately investigating the sample eﬃciency of our proposed estimators (9) for learning the satiation and reward models (Figure 2) and
10

Cumulative Reward
log Regw

(a) log |γk − γk| v.s. log n

(b) log |λk − λk| v.s. log n

Figure 2: Figure 2a and 2b are the log-log plots of absolute errors of γk and λk with respect to

the number of samples n in a single trajectory. The results are averaged over 30 random runs,

where the shaded area represents one standard deviation.

116605

115505

114405 135

T-lookahead w-lookahead

130 1 2 3 4 5 6 7 w8 9 10 11 12 13 14 15

56..7050

55..2550

w = 2, Slope = 0.57

45..7050

w = 5, Slope = 0.61 w = 8, Slope = 0.66

44..2550

w = 10, Slope = 0.71

4.00 4.25 4.50 4.75 5lo.0g0T5.25 5.50 5.75 6.00

(a) T = 30

(b) log Regw v.s. log T

Figure 3: Figure 3a shows the expected cumulative reward collected by the T -lookahead policy (red line) and w-lookahead policy (blue dots) when T = 30. Figure 3b shows the log-log plot of the w-step lookahead regret of w-lookahead EEP under diﬀerent T averaged over 20 random runs.

the computational performance of the w-lookahead policies (5) (Figure 3a). For the experimental setup, we have 5 arms with satiation retention factors γ1 = γ2 = .5, γ3 = .6, γ4 = .7, γ5 = .8, exposure inﬂuence factors λ1 = 1, λ2 = λ3 = 3, λ4 = λ5 = 2, base rewards b1 = 2, b2 = 3, b3 = 4, b4 = 2, b5 = 10, and noise with variance σz = 0.1.

Parameter Estimation We ﬁrst evaluate our proposed estimator for using a single trajectory per arm to estimate the arm parameters γk, λk. In Figure 2, we show the absolute error (averaged over 30 random runs) between the estimated parameters and the true parameters for each arm. Aligning with our theoretical guarantees (Corollary 4), the log-log plots show that the convergence rate of the absolute error is on the scale of O(n−1/2).

w-lookahead Performance To evaluate w-lookahead policies, we solve (5) using the true reward parameters and report expected cumulative rewards of the obtained w-lookahead policies (Figure 3a). Recall that the greedy policy is precisely the 1-lookahead policy. In order to solve the resulting integer programs, we use Gurobi 9.1 [23] and set the number of threads for solving the problem to 10. When T = 30, the T -lookahead policy (expected cumulative rewards given by the red line in Figure 3a) solved through (4) is obtained in 1610s. On the other hand, all w-lookahead policies (expected cumulative rewards given by the blue dots in Figure 3a) for w in between 1 and 15 are solved within 2s. We provide the results when T = 100 in Appendix G. Despite using signiﬁcantly lower computational time, w-lookahead policies achieve a similar expected cumulative reward to the T -lookahead policy.

EEP Performance We evaluate the performance of EEP when T ranges from 60 to 400. For each horizon T , we examine the w-step lookahead regret of w-lookahead EEP where w = 2, 5, 8, 10. All results are averaged over 20 random runs. As T increases, the exploration stage of EEP becomes longer, which results in collecting more data for estimating the reward parameters and lower variance of the parameter estimators. We ﬁt a line for the regrets with the same

11

lookahead size w to examine the order of the regret with respect to the horizon T . The slopes of the lines (see Figure 3b’s legend) are close to 2/3, which aligns with our theoretical guarantees (Theorem 5), i.e., the regrets are on the order of O(T 2/3). In Appendix G, we present additional experimental setups and results.
8 Conclusions
While our work has taken strides towards modeling the exposure-dependent evolution of preferences through dynamical systems, there are many avenues for future work. First, while our satiation dynamics are independent across arms, a natural extension might allow interactions among the arms. For example, a diner sick of pizza after too many trips to Di Fara’s, likely would also avoid Grimaldi’s until the satiation eﬀect wore oﬀ. On the system identiﬁcation side, we might overcome our reliance on evenly spaced pulls, producing more adaptive algorithms (e.g., optimism-based algorithms) that can reﬁne their estimates, improving the agent’s policy even past the pure exploration period. Finally, our satiation model captures just one plausible dynamic according to which preferences might evolve in response to past recommendations. Characterizing other such dynamics (e.g., the formation of brand loyalty where the rewards of an arm increase with more pulls) in bandits setups is of future interest.
Acknowledgement
LL is generously supported by an Open Philanthropy AI Fellowship. The authors would like to thank David Childers, Biswajit Paria, Eyan P. Noronha, Sai Sandeep and Max Simchowitz for very helpful discussions, and Stephen Tu for his insightful suggestions on system identiﬁcation of aﬃne dynamical systems.
References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312–2320, 2011.
[2] Soumya Basu, Rajat Sen, Sujay Sanghavi, and Sanjay Shakkottai. Blocking bandits. In Advances in Neural Information Processing Systems, pages 4785–4794, 2019.
[3] Manel Baucells and Rakesh K Sarin. Satiation in discounted utility. Operations research, 55(1):170–181, 2007.
[4] James Bennett, Stan Lanning, et al. The netﬂix prize. In Proceedings of KDD cup and workshop, volume 2007, page 35. New York, 2007.
[5] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration–exploitation in a multiarmed bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319–337, 2019.
[6] Felipe Caro and Victor Martínez-de Albéniz. Product and price competition with satiation eﬀects. Management Science, 58(7):1357–1373, 2012.
[7] Leonardo Cella and Nicolò Cesa-Bianchi. Stochastic bandits with delay-dependent payoﬀs. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1168–1177, 2020.
[8] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.
12

[9] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6(Apr):503–556, 2005.
[10] Alan Frieze and Mark Jerrum. Improved approximation algorithms for max k-cut and max bisection. Algorithmica, 18(1):67–81, 1997.
[11] Jeﬀ Galak and Joseph P Redden. The properties and antecedents of hedonic decline. Annual review of psychology, 69:1–25, 2018.
[12] John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society: Series B (Methodological), 41(2):148–164, 1979.
[13] James Hamilton. Time series analysis. Princeton University Press, Princeton, N.J, 1994.
[14] Hoda Heidari, Michael J Kearns, and Aaron Roth. Tight policy regret bounds for improving and decaying bandits. In IJCAI, pages 1562–1570, 2016.
[15] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(4), 2010.
[16] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. Unbiased learning-to-rank with biased feedback. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 781–789, 2017.
[17] Barbara E Kahn. Consumer variety-seeking among goods and services: An integrative review. Journal of retailing and consumer services, 2(3):139–148, 1995.
[18] Komal Kapoor, Karthik Subbian, Jaideep Srivastava, and Paul Schrater. Just in time recommendations: Modeling the dynamics of boredom in activity streams. In Proceedings of the eighth ACM international conference on web search and data mining, pages 233–242, 2015.
[19] Robert Kleinberg and Nicole Immorlica. Recharging bandits. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 309–319. IEEE, 2018.
[20] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
[21] Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In Advances in neural information processing systems, pages 3074–3083, 2017.
[22] Lennart Ljung. System identiﬁcation. Wiley encyclopedia of electrical and electronics engineering, pages 1–19, 1999.
[23] Gurobi Optimization LLC. Gurobi optimizer reference manual, 2021.
[24] Nikolai Matni and Stephen Tu. A tutorial on concentration bounds for system identiﬁcation. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 3741–3749. IEEE, 2019.
[25] Leigh McAlister. A dynamic attribute satiation model of variety-seeking behavior. Journal of Consumer Research, 9(2):141–150, 1982.
[26] Leigh McAlister and Edgar Pessemier. Variety seeking behavior: An interdisciplinary review. Journal of Consumer research, 9(3):311–322, 1982.
[27] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In Proceedings of the 22nd international conference on World Wide Web, pages 897–908. ACM, 2013.
[28] J Douglas McConnell. The development of brand loyalty: an experimental study. Journal of Marketing Research, 5(1):13–19, 1968.
13

[29] Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, and Yoshimi Fukuoka. Nonstationary bandits with habituation and recovery dynamics. Operations Research, 68(5):1493–1516, 2020.
[30] Rémi Munos. Performance bounds in p-norm for approximate value iteration. SIAM journal on control and optimization, 46(2):541–561, 2007.
[31] Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning. In Advances in Neural Information Processing Systems, pages 1763–1771, 2012.
[32] Ronald Ortner, Daniil Ryabko, Peter Auer, and Rémi Munos. Regret bounds for restless markov bandits. In International Conference on Algorithmic Learning Theory, pages 214–228. Springer, 2012.
[33] Victor H Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and Statistical Applications. Springer, 2009.
[34] Ciara Pike-Burke and Steﬀen Grunewalder. Recovering bandits. In Advances in Neural Information Processing Systems, pages 14122–14131, 2019.
[35] Rebecca K Ratner, Barbara E Kahn, and Daniel Kahneman. Choosing less-preferred experiences for the sake of variety. Journal of consumer research, 26(1):1–15, 1999.
[36] Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data eﬃcient neural reinforcement learning method. In European Conference on Machine Learning, pages 317–328. Springer, 2005.
[37] Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527–535, 1952.
[38] Tuhin Sarkar and Alexander Rakhlin. Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical systems. In International Conference on Machine Learning, pages 5610– 5618, 2019.
[39] Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko. Rotting bandits are no harder than stochastic ones. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 2564–2572, 2019.
[40] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identiﬁcation. In Conference On Learning Theory, pages 439–473, 2018.
[41] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pages 814–823, 2015.
[42] William T Tucker. The development of brand loyalty. Journal of Marketing research, 1(3):32–35, 1964.
[43] Romain Warlop, Alessandro Lazaric, and Jérémie Mary. Fighting boredom in recommender systems with linear reinforcement learning. In Advances in Neural Information Processing Systems, pages 1757–1768, 2018.
[44] Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied probability, 25(A):287–298, 1988.
14

Contents (Appendix)

A Integer Linear Programming Formulation

16

B Proofs and Discussion of Section 4

17

B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

B.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

B.3 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

B.4 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

C More Discussion on Learning with Unknown Dynamics

23

C.1 MDP Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

C.2 Exploration and Estimation of the Reward Model . . . . . . . . . . . . . . . . . . 24

C.2.1 Estimation using Multiple Trajectories . . . . . . . . . . . . . . . . . . . . 24

C.2.2 Estimation using a Single Trajectory . . . . . . . . . . . . . . . . . . . . . 25

C.3 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

C.3.1 Time-dependent Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

C.3.2 State-dependent Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

D Proofs of Section 6.2 and Appendix C.2.2

29

D.1 Proof of Theorem 3 and Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . 29

D.2 Proof of Corollary 4 and Corollary 9 . . . . . . . . . . . . . . . . . . . . . . . . . 34

E Additional Proofs and Discussion of Section 6

35

E.1 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

E.2 Exploration Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

F Additional Proofs of Appendix C

39

F.1 Proof of Corollary 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F.2 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F.3 Proof of Proposition 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

F.4 Proof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

G Additional Experimental Details and Results

43

15

A Integer Linear Programming Formulation

The bilinear integer program of (4) admits the following equivalent linear integer programming formulation:

t−1

max

bkuk,t − λk γkt−izk,t,i

uk,t ,zk,t,i

k∈[K] t∈[T ]

i=0

s.t.

uk,t = 1,

k∈[K ]

∀t ∈ [T ],

zk,t,i ≤ uk,i, zk,t,i ≤ uk,t, uk,i + uk,t − 1 ≤ zk,t,i, ∀k ∈ [K], t ∈ [T ], i ∈ {0, . . . , t − 1},

uk,t ∈ {0, 1}, uk,0 = 0, ∀k ∈ [K], t ∈ [T ],

zk,t,i ∈ {0, 1}, ∀k ∈ [K], t ∈ [T ], i ∈ {0, . . . , t − 1}.

16

B Proofs and Discussion of Section 4

B.1 Proof of Lemma 1
Proof. When the expected rewards of all arms are the same, we know that the arm with the lowest index will be chosen and thus the ﬁrst K pulls will be π1 = 1, . . . , πK = K. We will complete the proof through induction. Suppose that the greedy pull sequence is periodic with π1 = 1, . . . , πK = K and πt+K = πt until time h > K. We deﬁne k to be h mod K and n to be (h − k )/K. We will show that πh+1 = 1 if πh = K and πh+1 = πh + 1 otherwise. When k = 0 (i.e., πh = K), all arms have been pulled exactly n times as of time h. By the induction assumption, we know that u1,1:h−K = u2,2:h−K+1 = . . . = uK,K:h, which implies that last time when each arm is pulled, all of them have the same expected rewards, i.e.,

µ1,h−K+1(u1,0:h−K ) = µ2,h−K+2(u2,0:h−K+1) = · · · = µK,h(uK,0:h−1). Moreover, u1,h−K+1:h = (1, 0, · · · 0), u2,h−K+1:h = (1, 0, · · · 0 ), · · · , uK,h:h = (1).

K times

K-1 times

Therefore, by (3), at time h + 1, arm 1 has the highest expected reward and will be chosen. In
the case where k > 0 (i.e., πh = k ), we let h := h − k . We have that µ1,h −K+1(u1,0:h −K ) = . . . = µK,h(uK,0:h −1) and s = s1,h −K+1(u1,0:h −K ) = . . . = sK,h (uK,0:h −1) ≤ 1−γKγK . Then, at time h + 1, the satiation level for the arms will be sk,h+1(uk,0:h) = γk −k+1 1 + γK s for all k ≤ k and sk,h+1(uk,0:h) = γK−k+k +1s for all k > k . Thus, the arm with the lowest satiation level will be πh+1 = k + 1 = πh + 1, since sk +1,h+1(uk +1,0:h) < s1,h+1(u1,0:h). Consequently, the greedy policy will select arm πh + 1 at time h + 1.

B.2 Proof of Theorem 1

Proof. First, when T ≤ K, greedy policy is optimal since its cumulative expected reward is T b.

So, we consider the case of T > K. Assume for contradiction that there exists another policy π1o:T

that is optimal and is not greedy, i.e., ∃t ∈ [T ], πto ∈/ arg maxk∈[K] b − λsok,t where sok,t denotes

the

satiation

level

of

arm

k

at

time

t

under

the

policy

π

o 1:T

.

We

will

construct

a

new

policy

π1n:T

that

obtains

a

higher

cumulative

expected

reward

than

π

o 1:T

.

Throughout

the

proof,

we

use

snk,t

to denote the satiation levels for the new policy.

We ﬁrst note two illustrative facts to give the intuition of the proof.

Fact 1: Any policy π1o:T that does not pick the arm with the lowest satiation level (i.e., highest expected reward) at the last time step T is not optimal.
Proof of Fact 1: In this case, the policy π1n:T = (π1o, . . . , πTo −1, πT ) where πT ∈ arg maxk∈[K] b − λsok,T will obtain a higher cumulative expected reward.

Fact 2: If a policy π1o:T picks the lowest satiation level for the ﬁnal pull πTo but does not pick the arm

with

the

lowest

satiation

level

at

time

T −1,

we

claim

that

π1n:T

=

(

π1o

,

.

.

.

,

π

o T

−2

,

πTo

,

πTo

−

1

)

=

π1o:T

obtains a higher cumulative expected reward.

Proof of Fact 2: First, note that πTo −1 = πTo because otherwise πTo −1 is the arm with the lowest

satiation level at T − 1. Moreover, at time T − 1, πTo ∈ arg mink sok,T −1 has the smallest satiation,

since if not, then there exists another arm k = πTo and k = πTo −1 that has a smaller satiation

level than πTo at time T − 1. In that case, πTo will not be the arm with the lowest satiation at

time T , which is a contradiction. Then, we deduce soπo ,T −1 > soπo ,T −1. Combining this with

T −1

T

πTo −1 = πTo , we arrive at

GT (π1n:T ) − GT (π1o:T ) = λ(1 − γ) soπo ,T −1 − soπo ,T −1 > 0.

T −1

T

17

For the general case, given any policy π1o:T that is not a greedy policy, we construct the new policy π1n:T that has a higher cumulative expected reward through the following procedure:

1. Find t∗ ∈ [T ] such that for all t > t∗, πto ∈ arg maxk∈[K] b − λsok,t and πto∗ ∈/ arg maxk∈[K] b − λsok,t∗. Further, we know that πto∗+1 ∈ arg maxk∈[K] b − λsok,t∗, using the same reasoning as the above example, i.e., otherwise πto∗+1 ∈/ arg maxk∈[K] b − λsok,t∗+1. To ease the notation, we use k1 to denote πto∗ and k2 to denote πto∗+1.

2. For the new policy, we choose π1n:t∗+1 = (π1o, . . . , πto∗−1, k2, k1). Let Aot1,t2 denote the set

{t : t∗ + 2 ≤ t ≤ t2, πto = πto1}. Aot1,t2 contains a set of time indices in between t∗ + 2

and

t2

when

arm

πto1

is

played

under

policy

π

o 1:T

.

We

construct

the

following

three

sets

TA

:=

{t

:

t∗

+

2

≤

t

≤

T

,

|

A

o t∗

,t

|

<

|Aot∗+1,t|},

TB

:=

{t

:

t∗

+

2

≤

t

≤

T

,

|A

o t∗

,t

|

>

|Aot∗+1,t|}

and TC := {t : t∗ + 2 ≤ t ≤ T, |Aot∗,t| = |Aot∗+1,t|}. For time t ≥ t∗ + 2, we consider the

following three cases:

Case I. TB = ∅, which means that at any time t in between t∗ +2 and T , arm k1 is played more than arm k2 from t∗ + 2 to t. In this case, the new policy follows πtn∗+2:T = πto∗+2:T .

Case II. TA = ∅, which means that at any time t in between t∗ + 2 and T , arm k2 is played more than arm k1 from t∗ + 2 to t. In this case, the new policy satisﬁes: for all t ≥ t∗ + 2, 1) πtn = πto if πto = k1 and πto = k2; 2) πtn = k2 if πto = k1; and 3) πtn = k1 if πto = k2.

Case III. TA = ∅ and TB = ∅. Then, starting from t∗ + 2, if t ∈ TA, πtn follows the new policy construction in Case I, i.e., πtn = πto. If t ∈ TB, πtn follows the new policy construction

in Case II. Finally, for all t ∈ TC , deﬁne tA,t = maxt ∈TA: t and tB,t = maxt ∈TB: t .

t <t

t <t

If tA,t > tB,t, then πtn follows the new policy construction as Case I. If tA,t < tB,t,

πtn follows the new policy construction as Case II. We note that tA,t = tB,t since

TA ∩ TB = ∅.

When

TA

=

∅

and

TB

=

∅,

we

know

that

k1

and

k2

are

not

played

in

π

o t∗

+2:

T

.

In

this

case,

the

new policy construction can follow either Case I or Case II. To complete the proof, we state some

facts ﬁrst:

• From t∗, the expected rewards collected by the policies π1o:T and π1n:T only diﬀer at times when arm k1 or arm k2 is played.

• π1n:t∗+1 obtains a higher cumulative expected reward than π1o:t∗+1.

• At time t∗ +2, the new policy follows that snk1,t∗+2 = γ +γ2sok1,t∗ and snk2,t∗+2 = γ2 +γ2sok2,t∗ . On the other hand, the old policy has sok1,t∗+2 = γ2 + γ2sok1,t∗ and sok2,t∗+2 = γ + γ2sok2,t∗ .

Let Nk1 := {t : t∗ + 2 ≤ t ≤ T, πto = k1} and Nk2 := {t : t∗ + 2 ≤ t ≤ T, πto = k2} denote the sets

of

time

steps

when

k1

and

k2

are

played

in

π

o 1:T

.

For a given satiation level x at time t

together

with the time steps the arm is pulled Nk, we have that at time t ≥ t , the arm has satiation level

gNk (x, t, t ) = γt−t x + Nk,i<t γt−Nk,i where Nk,i is the i-th smallest element in Nk.

In Case I, the diﬀerence of the cumulative expected rewards between the two policies satisﬁes:

|Nk2 |

GT (π1n:T ) − GT (π1o:T ) >

−λgNk2 (snk2,t∗+2, Nk2,i, t∗ + 2) + λgNk2 (sok2,t∗+2, Nk2,i, t∗ + 2)

i=1

|Nk1 |
+ −λgNk1 (snk1,t∗+2, Nk1,j , t∗ + 2) + λgNk1 (sok1,t∗+2, Nk1,j , t∗ + 2)
j=1

18

=λ

sok2,t∗+2 − snk2,t∗+2

|Nk2 |
γNk2,i−(t∗+2) + λ
i=1

sok1,t∗+2 − snk1,t∗+2

|Nk1 |
γNk1,j −(t∗+2) > 0,
j=1

where we have used the fact that sok2,t∗+2 − snk2,t∗+2 = − sok1,t∗+2 − snk1,t∗+2 and for all j ∈ [|Nk1|], Nk2,j < Nk1,j. In Case II, similarly, we have that

> 0, |Nk2 | ≥ |Nk1 |

|Nk1 |

GT (π1n:T ) − GT (π1o:T ) >

−λgNk1 (snk2,t∗+2, Nk1,j , t∗ + 2) + λgNk1 (sok1,t∗+2, Nk1,j , t∗ + 2)

j=1

|Nk2 |
+ −λgNk2 (snk1,t∗+2, Nk2,i, t∗ + 2) + λgNk2 (sok2,t∗+2, Nk2,i, t∗ + 2)
i=1

=λ

sok1,t∗+2 − snk2,t∗+2

|Nk1 |
γNk1,j −(t∗+2) + λ
j=1

sok2,t∗+2 − snk1,t∗+2

|Nk2 |
γNk2,i−(t∗+2) > 0,
i=1

since sok1,t∗+2 − snk2,t∗+2 = − Nk1,i < Nk2,i.

sok2,t∗+2 − snk1,t∗+2

> 0, |Nk2| ≤ |Nk1| and for all i ∈ [|Nk2|],

Finally, for Case III, the new policy construction is a mix of Case I and Case II. We represent the time interval [t∗ + 2, T ] to be [t∗ + 2, T ] = [ti1,s1 , ti1,e1 ] ∪ [ti2,s2 , ti2,e2 ] ∪ · · · ∪ [tiM ,sM , tiM ,eM ] where t∗ + 2 = ti1,s1 ≤ . . . ≤ tiM ,sM = T , ∩M m=1[tim,sm , tim,em ] = ∅ and M − 1 is the number of new policy construction switches happen in between t∗ + 2 and T . We say that a new policy
construction switch happens at time t if the policy construction follows Case I at time t − 1 but
follows Case II at time t or vice versa. Each im = im−1 can take values I or II, representing
which policy construction rule is used between the time period tim,sm and tim,em. For any time index set V , we use the notation V [tim,sm , tim,em ] := {t ∈ V : tim,sm ≤ t ≤ tim,em }.

We notice that at any switching time tim,sm, the number of previous pulls of arm k1 and k2 from
time tim−1,sm−1 to tim−1,em−1 are equivalent, which is denoted by lm = |Nk1 [tim,sm , tim,em ]| =
|Nk2[tim,sm, tim,em]| for all m < M . From our analysis of Case I and Case II, we know that to show that π1n:T obtains a higher cumulative expected reward, it suﬃces to prove: for all m < M such that

sok2,tim,sm − snk2,tim,sm = − sok1,tim,sm − snk2,tim,sm = −

sok1,tim,sm − snk1,tim,sm sok2,tim,sm − snk1,tim,sm

> 0, > 0,

we have

s − s = − o
k2 ,tim+1 ,sm+1

n k2 ,tim+1 ,sm+1

s − s = − o
k1 ,tim+1 ,sm+1

n k2 ,tim+1 ,sm+1

s − s o
k1 ,tim+1 ,sm+1

n k1 ,tim+1 ,sm+1

s − s o
k2 ,tim+1 ,sm+1

n k1 ,tim+1 ,sm+1

> 0, > 0.

We will establish these facts in Lemma 3. Finally, we note that the above required conditions are held at time ti1,s1 = t∗ + 2.

Lemma 3. Let Nk[ts, te] denote the set of time steps when arm k is pulled in between (and

including)

time

ts

and

te

under

policy

π

o 1:T

.

Let

sok,t

and

snk,t

represent

the

satiation

level

of

arm

k

at

time

t

when

following

the

policy

π1o:T

and

π

n 1:T

,

respectively.

For

two

diﬀerent

arms

k1

and

k2, suppose that at time ts we have

sok2,ts − snk2,ts = − sok1,ts − snk1,ts > 0,

19

sok1,ts − snk2,ts = − sok2,ts − snk1,ts > 0.
Further, suppose that from time ts to te, π1n:T follows either Case I (or Case II) of new policy construction (see proof of Theorem 1 for their deﬁnitions); and at time ts = te + 1, the new policy construction for π1n:T has switched to Case II (or Case I if Case II is used from ts to te). Then at time ts, we have that

sok2,ts − snk2,ts = − sok1,ts − snk2,ts = −

sok1,ts − snk1,ts sok2,ts − snk1,ts

> 0, > 0.

Proof of Lemma 3. Following the deﬁnition in the proof of Theorem 1, given that at time ts, arm k has satiation s, let gNk[ts,te](s, ts, ts) denote the satiation level of arm k at time ts after being pulled at the time steps in the set Nk[ts, te]. Let Nk,i[ts, te] be the i-th smallest element in the set Nk[ts, te]. From the deﬁnition of the new policy construction given in the proof of Theorem 1, we also know that (1) N := |Nk1[ts, te]| = |Nk2[ts, te]|; (2) if Case I is applied in between ts and te, we have that for all i ∈ [N ], Nk2,i[ts, te] < Nk1,i[ts, te]; and (3) if Case II is applied in between ts and te, we have that for all i ∈ [N ], Nk2,i[ts, te] > Nk1,i[ts, te].
We ﬁrst consider the setting when Case I new policy construction is applied, then at time ts, we can show that

sok1,ts − snk2,ts =gNk1 [ts,te] sok1,ts , ts, ts − gNk2 [ts,te] snk2,ts , ts, ts

=γ ts −ts

sok1,ts − snk2,ts

l

+

γts−Nk1,i[ts,te] − γts−Nk2,i[ts,te]

i=1

=γts−ts snk1,ts − sok2,ts =snk1,ts − sok2,ts > 0,

l

+

γts−Nk1,i[ts,te] − γts−Nk2,i[ts,te]

i=1

where the last inequality has used the fact that when we use Case I construction, we have Nk2,i[ts, te] < Nk1,i[ts, te]. Meanwhile, we also have that

sok2,ts − snk2,ts =gNk2 [ts,te] sok2,ts , ts, ts − gNk2 [ts,te] snk2,ts , ts, ts =γts−ts sok2,ts − snk2,ts = −γts−ts sok1,ts − snk1,ts
= − sok1,ts − snk1,ts > 0.

When Case II new policy construction is applied, then at time ts, we get

sok1,ts − snk2,ts =gNk1 [ts,te] sok1,ts , ts, ts − gNk1 [ts,te] snk2,ts , ts, ts =γts−ts sok1,ts − snk2,ts = −γts−ts sok2,ts − snk1,ts
= − sok2,ts − snk1,ts > 0,

since sok1,ts − snk2,ts > 0. On the other hand, we have that

sok2,ts

− snk2,ts

=gNk [ts,te] sok2,ts , ts, ts 2
=γts−ts sok2,ts − snk2,ts

− gNk [ts,te] snk2,ts , ts, ts 1

l

+

γts−Nk2,i[ts,te] − γts−Nk1,i[ts,te]

i=1

20

=γts−ts snk1,ts − sok1,ts =snk1,ts − sok1,ts > 0,

l

+

γts−Nk2,i[ts,te] − γts−Nk1,i[ts,te]

i=1

where the last inequality is true because when Case II new policy construction is applied, we have Nk1,i[ts, te] < Nk2,i[ts, te].

B.3 Proof of Proposition 2

Proof. If T ≤ K, a Max K-Cut of KT is ∀k ∈ [T ], Pk = {k}, which is the same as an optimal solution to (4). Let 1{·} denote the indicator function. When T > K, the integer program in (4) is equivalent to

K

KT

max

buk,1 +

uk,t∈{0,1}: k=1

k=1 t=2

∀t∈[T ], k uk,t=1

t−1
buk,t − λ γt−iuk,iuk,t
i=1

K

KT

= max

b1{1 ∈ Pk} +

P1,...,PK ⊆[T ]: ∪kPk=[T ], k=1

k=1 t=2

∀k=k ,Pk∩Pk =∅

t−1
b1{t ∈ Pk} − λ γt−i1{i ∈ Pk}1{t ∈ Pk}
i=1

K

= max T b −

P1,...,PK ⊆[T ]: ∪kPk=[T ],

k=1

∀k=k ,Pk∩Pk =∅

λγt−i
t,i∈Pk : i<t

T t−1

K−1 K

=T b −

λγt−i +

max

λγt−i,

t=2 i=1

P1,...,PK ⊆[T ]:

∪kPk=[T ], k=1 k =k+1 t∈Pk,

∀k=k ,Pk∩Pk =∅

i∈Pk : i<t

where the second equality uses the fact

K k=1

1{t

∈

Pk }

=

1

for

all

t

∈

[T

]

and

the

third

equality

is true because for any P1, . . . PK such that ∀k = k , Pk ∩ Pk = ∅ and ∪kPk = [T ], we have

T t−1

Total Edge Weights of KT =

e(t, i) =

e(t, i)

t=2 i=1

t,i∈[T ]:i<t, ∃k∈[K ],i,t∈Pk

+

e(t, i).

t,i∈[T ]:i<t, ∀k∈[K ],i,t∈/ Pk

B.4 Proof of Theorem 2

Proof.

Given

π1∗:T

and

π

w 1:T

,

deﬁne

a

set

of

new

policies

{π˜

i 1:T

}li−=11

such

that

for

all

i,

π˜1i :T

=

(π1w:iw, πi∗w+1:T ). Based on this, we have the following decomposition





l−2
GT (π1∗:T )−GT (π1w:T ) = GT (π1∗:T ) − GT (π˜11:T ) +  GT (π˜1i :T ) − GT (π˜1i+:T1)+GT (π˜1l−:T1) − GT (π1w:T ) .

A0

i=1

Ai

Al−1

To distinguish the past pull sequences of each arm under diﬀerent policies, we use the following

notations: µk,t(uk,0:t−1; π ) gives the expected reward of arm k at time t by following pull sequence

π1:t−1.

By

the

deﬁnition

of

π

w 1:T

,

we

have

that

w

T

A0 =

µπ∗,t(uπ∗,0:t−1; π∗) − µπw,t(uπw,0:t−1; πw) +

t

t

t

t

µπ∗,t(uπ∗,0:t−1; π∗) − µπ∗,t(uπ∗,0:t−1; π˜1)

t

t

t

t

t=1

t=w+1

21

T

≤

µπ∗,t(uπ∗,0:t−1; π∗) − µπ∗,t(uπ∗,0:t−1; π˜1),

t

t

t

t

t=w+1

where the inequality follows from the fact that π1w:w is optimal for (4) when T = w. Similarly, we obtain that for all i ∈ [l − 2],

iw

(i+1)w

Ai =

µπw,t(uπw,0:t−1; πw) − µπw,t(uπw,0:t−1; πw) +

t

t

t

t

µπ∗,t(uπ∗,0:t−1; π˜i) − µπw,t(uπw,0:t−1; πw)

t

t

t

t

t=1 =0

t=iw+1 ≤0

T

+

µπ∗,t(uπ∗,0:t−1; π˜i) − µπ∗,t(uπ∗,0:t−1; π˜i+1)

t

t

t

t

t=(i+1)w+1

T

≤

µπ∗,t(uπ∗,0:t−1; π˜i) − µπ∗,t(uπ∗,0:t−1; π˜i+1).

t

t

t

t

t=(i+1)w+1

Finally, we have Al−1 =

T t=(l−1)w+1

µπ∗,t(uπ∗,0:t−1;

π˜l−1)−µπw

,t(uπw

,0:t−1;

πw

)

≤

0.

To complete

t

t

t

t

the proof, it suﬃces to use the fact that for all i ∈ {1, . . . , l − 1},

T

T −iw−1 t γ

λγ(1 − γT −iw)

max
π1:T ,π1:T :

µπt,t(uπt,0:t−1; π) − µπt,t(uπt,0:t−1; π ) ≤

λγ

≤

1−γ

t=iw+1

t=0

(1 − γ)2

πiw+1:T =πiw+1:T

λγ(1 − γT −w) ≤ (1 − γ)2 ,

where the ﬁrst inequality holds because for any arm, the maximum satiation level discrepancy under two pull sequences (after iw time steps) is γ/(1 − γ) and from time iw + 1 till time T , the objective will be maximized when the arm with the maximum satiation discrepancy is played all the time.

22

C More Discussion on Learning with Unknown Dynamics
As we have noted in Section 5, when the learner makes a decision on which arm to pull, the learner does not observe the hidden satiation level the user has for the arms. The POMDP the learner faces can be cast as a fully observable MDP (Appendix C.1) where the estimated reward model (Appendix C.2) can be used for planning (Appendix C.3). In addition to policies that are time-dependent (actions taken by time-dependent policies only depend on the time steps at which they are taken) considered in Section 6, we also consider state-dependent policies where the states are continuous.

C.1 MDP Setup
We begin with describing the full MDP setup of rebounding bandits, including the state representation and reward function deﬁned in Section 5.1. Following [32], at any time t ∈ [T ], we deﬁne our state vector to be xt = (x1,t, n1,t, x2,t, n2,t, . . . , xK,t, nK,t), where nk,t ∈ N is the number of steps since arm k is last selected and xk,t is the satiation inﬂuenceas of the most recent pull of arm k. Since the most recent pull happens at t − nk,t, we have xk,t = bk − µk,t−nk,t = λksk,t−nk,t . We note that bk can be obtained when arm k is pulled for the ﬁrst time since the satiation eﬀect is 0 if an arm has not been pulled before. The initial state is xinit = (0, . . . , 0). Transitions between two states xt and xt+1 are deﬁned as follows: If arm k is chosen at time t, i.e., πt = k, and reward µk,t is obtained, then the next state xt+1 will be:
A.1 For the pulled arm k, nk,t+1 = 1 and xk,t+1 = bk − µk,t.
A.2 For other arms k = k, nk ,t+1 = nk ,t + 1 if nk ,t = 0 and nk ,t+1 = 0 if nk ,t = 0. The satiation inﬂuence remains the same, i.e., xk ,t+1 = xk ,t.
For all xt ∈ X and k ∈ [K], we have that E[xk,t] ≤ λγ/(1 − γ) and Var[xk,t] ≤ λ2σz2/(1 − γ2). Hence, for any δ ∈ (0, 1), P (maxk,t |xk,t| ≥ B(δ)) ≤ δ, where

λγ

2 log(2KT /δ)

B(δ) := 1 − γ + λσz

1 − γ2 .

(10)

The MDP the learner faces can be described as a tuple M := xinit, [K], {γk, λk, bk}Kk=1, T of the initial state xinit, actions (arms) [K], the horizon T and parameters {γk, λk, bk}Kk=1. Let ∆(·) denote the probability simplex. Given {γk, λk, bk}Kk=1, the expected reward r : X × [K] → R and transition functions p : X × [K] × [T ] → ∆(X ) are deﬁned as follows:

1. r : X ×[K] → R gives the expected reward of pulling arm k conditioned on xt, i.e., r(xt, k) = E[µk,t|xt].2 If nk,t = 0, then r(xt, k) = bk. If nk,t ≥ 1, r(xt, k) = bk − γknk,t xk,t − λkγknk,t .

2. When pulling arm k at time t and state xt, p (xt+1|xt, k, t) = 0 if xt+1 does not satisfy A.1

or A.2. When xt+1 fulﬁlls both A.1 and A.2, we consider two cases of xt. If nk,t = 0,

then the transition function p (xt+1|xt, k, t) is given by the Gaussian density with mean

γknk,t (xk,t + λk) and variance λ2kσz2

nk,t −1 i=0

γk2i,

as

illustrated

in

(11).

If nk,t = 0, then

p(xt+1|xt, k, t) = 1 since for the ﬁrst pull of arm k, the obtained reward µk,t = bk.

At time t, the learner follows an action πt : X → [K] that depends on the state. We use Vtπ,M : X → R to denote the value function of policy π1:T at time t under MDP M: Vtπ,M(xt) = r(xt, πt(xt)) + Ext+1∼p(·|xt,πt(xt),t)[Vtπ+1,M(xt+1)] and VTπ+1,M(x) = 0 for all x ∈ X . To restate our goal (2) in terms of the value function: for an MDP M, we would like to ﬁnd a policy π1:T

2By conditioning on xt, we mean conditioning on the σ-algebra generated by past actions and observed rewards.

23

that maximizes

T

V1π,M(xinit) = E

r(xt, πt(xt)) x1 = xinit .

t=1

To simplify the notation, we use π to refer to a policy π1:T . Given an MDP M, we denote its optimal policy by πM ∗ and the value function for the optimal policy by Vt∗,M, i.e., Vt∗,M(x) := Vtπ,M M ∗ (x).

C.2 Exploration and Estimation of the Reward Model

As we have discussed in § 6.1, based on our satiation and reward models, the satiation inﬂuence xk,t of arm k forms a dynamical system where we only observe the value of the system when arm k is pulled. When arm k is pulled at time t and nk,t = 0, we observe the satiation inﬂuence λksk,t which becomes the next state xk,t+1, i.e.,

nk,t −1

xk,t+1 = λksk,t = λkγknk,t sk,t−nk,t + λkγknk,t + λk

γki zk,t−1−i

i=0

nk,t −1

= γknk,t xk,t+1−nk,t + λkγknk,t + λk

γki zk,t−1−i.

i=0

(11)

We note that the current state xk,t equals to xk,t+1−nk,t since xk,t+1−nk,t is the last observed satiation inﬂuence for arm k and nk,t is the number of steps since arm k is last pulled.

Exploration Settings Depending on the nature of the recommendation domain, we consider two types of exploration settings: one where the users only interact with the recommendation systems for a short time after they log in to the service (Appendix C.2.1) and the other where the users tend to interact with the system for a much longer time, e.g., automated music playlisting (Appendix C.2.2). In the ﬁrst case, the learner collects multiple (n) short trajectories of user utilities, while in the second case, similar to § 6.2, the learner obtains a single trajectory of user utilities that has length n. In both settings, we obtain th√at under some mild conditions, the estimation errors of our estimators for γk and λk are O(1/ n).

Exploration Strategies Generalizing from the case where arms are pulled repeatedly, we explore by pulling the same arm at a ﬁxed interval m. In particular, when m = 1, the exploration strategy is the same as repeatedly pulling the same arm for multiple times, which is the exploration strategy used in § 6.1. When m = K, the exploration strategy is to pull the arms in a cyclic order. We present the estimator for γk, λk using the dataset collected by this exploration strategy in both the multiple trajectory and single trajectory settings.

C.2.1 Estimation using Multiple Trajectories

For each arm k ∈ [K], we use Dkn,m to denote a dataset containing n trajectories of evenly spaced observed satiation inﬂuences that are collected by our exploration phase. The time
interval between two pulls of an arm is denoted by m. Each trajectory is of length at least
Tmin + 1 for Tmin > 1. For trajectory i ∈ [n], the observed satiation inﬂuences are denoted by x˜(ki,)1, . . . , x˜(ki,)Tmin+1, . . ., where x˜(ki,)1 = 0 is the initial satiation inﬂuence and the rest of the satiation inﬂuences x˜(ki,)j (j > 1) is the diﬀerence between the ﬁrst received reward, i.e., the base reward bk, and the reward from the j-th pull of arm k. In other words, for x˜(ki,)j, x˜(ki,)j+1 ∈ Dkn,m, it follows that

x˜(ki,)j+1

=

ak x˜(ki,)j

+

dk

+

z˜(i) ,
k,j

(12)

24

where

ak

=

γkm,

dk

=

λk γkm

and

z˜(i)
k,j

are

the

independent

samples

from

N

σz2,k = λ2kσz2(1 − γk2m)/(1 − γk2).

0, σz2,k

with

To

estimate

dk ,

we

use

the

estimator

dk

=

1 n

ni=1 x˜(ki,)2

=

dk

+

1 n

n i=1

z˜k(i,)1.

By the standard

Gaussian tail bound, we obtain that for δ ∈ (0, 1), with probability 1 − δ,

2σz2,k log(2/δ)

|dk − dk| ≤

=: d(n, δ, k).

(13)

n

When estimating ak, we ﬁrst take the diﬀerence between the ﬁrst Tmin+1 entries of two trajectories i and 2i for i ∈ n/2 and obtain a new trajectory y˜k(i,)1, . . . , y˜k(i,)Tmin+1 where y˜k(i,)j = x˜(ki,)j − x˜(k2,ji) for j ∈ [Tmin + 1]. We note that the new trajectory forms a linear dynamical system without the
bias term dk, i.e.,

y˜(i)
k,j+1

=

ak y˜k(i,)j

+

w˜k(i,)j ,

where

w˜k(i,)j

are

samples

from

N

(0,

2σ

2 z,k

).

We

use

the

ordinary

least

squares

estimator

to

estimate

ak :

n/2

ak = arg min

a

i=1

(i)

(i)

2

y˜k,Tmin+1 − ay˜k,Tmin

n/2 y˜(i) y˜(i)

= i=1 k,Tmin k,Tmin+1 .

n/2 (i)

2

i=1 y˜k,Tmin

(14)

Theorem 6. [24, Theorem II.4] Fix δ ∈ (0, 1). Given n ≥ 64 log(2/δ), with probability 1 − δ, we have that

2 log(4/δ)

|ak − ak| ≤ 4 n Tmin a2t =: a(n, δ, k).

(15)

t=0 k

We notice that as the minimum length of the trajectory gets greater, the upper bound of the estimation error of ak gets smaller. Using our estimators for ak and dk, we estimate γk and λk through γk = |ak|1/m and λk = |dk/ak|.
Corollary 7. Fix δ ∈ (0, 1). Suppose that for all k ∈ [K], we are given Dkn,m where n ≥ 64 log(2/δ) and ak > 0 where ak is deﬁned in (14). Then, with probability 1 − δ, we have that for all k ∈ [K],

|γk − γk| ≤ a(n, δ/K, k) = O

1 √

γkm−1

n

1 and |λk − λk| ≤ O √ .
n

The proof of Corollary 7 can be found in Appendix F.1. In the case where we are have collected n trajectories of evenly spaced user utilities for ea√ch arm, when the sample size n is suﬃcient large, the estimation errors of γk and λk are O(1/ n).

C.2.2 Estimation using a Single Trajectory
In the case where the learner gets to interact with the user for a long period of time (which is the setting considered in § 5 and § 6), we collect a single trajectory of evenly spaced arm pulls for each arm: for each arm k ∈ [K], we use Pkn,m to denote a dataset containing a single

25

trajectory of n + 1 observed satiation inﬂuences x˜k,1, . . . , x˜k,n+1, where similar to the multiple
trajectories case, x˜k,1 = 0, x˜k,j (j > 1) is the diﬀerence between the ﬁrst received reward and
the j-th received reward and the time interval between two consecutive pulls is m. Thus, for x˜k,j, x˜k,j+1 ∈ Pkn,m, it follows that

x˜k,j+1 = akx˜k,j + dk + z˜k,j ,

(16)

where ak, dk and z˜k,j are deﬁned the same as the ones in (12). For all k ∈ [K], given Pkn,m, we use the following estimators to estimate Ak = (ak, dk) ,

Ak = ak = (Xk Xk)−1Xk Yk,

(17)

dk

where Yk ∈ Rn is an n-dimensional vector whose j-th entry is x˜k,j+1 and Xk ∈ Rn×2 has its j-th row to be the vector xk,j = (x˜k,j, 1) . Finally, we take γk = |ak|1/m and λk = |dk/ak|. We note that Ak = arg minAk∈R2 Yk − XkAk 22, i.e., it is the ordinary least squares estimator for Ak given the dataset that treats x˜k,j+1 to be the response of the covariates xk,j.
As we have noted earlier (§ 6.2), unlike the multiple trajectories setting, in the single trajectory case, the diﬃculty in analyzing the ordinary least squares estimator (17) comes from the fact that the samples are not independent. Asymptotic guarantees of the ordinary least squares estimators in this case have been studied previously in control theory and time series community [13, 22]. The recent work on system identiﬁcations for linear dynamical systems focuses on studying the sample complexity of the problem [40, 38]. Adapting the proof of [40, Theorem 2.4], we derive the following theorem for identifying our aﬃne dynamical system (16).
Theorem 8. Fix δ ∈ (0, 1). For all k ∈ [K], there exists a constant n0(δ, k) such that if the dataset Pkn,m satisﬁes n ≥ n0(δ, k), then

P Ak − Ak 2

1/(ψn) ≤ δ,

where ψ =

min

, . σz2,k(1−ak)2
16d2k (1−a2k )+(1−ak )2 σz2,k

σz2,k 4(1−a2k )

As shown in Theorem 8, when dk = λkγkm gets larger, the rates of convergence for√Ak gets slower. Given tha√t we have a single trajectory of suﬃcient length, |ak − ak| ≤ O(1/ n) and |dk − dk| ≤ O(1/ n). Similar to the mul√tiple trajectories case, as shown in Corollary 9, the estimators of γk and λk also achieve O(1/ n) estimation error.
√ Corollary 9. Fix δ ∈ (0, 1). Suppose that for all k ∈ [K], we have P( Ak − Ak 2 1/ n) ≤ δ and ak > 0 where Ak and ak are deﬁned in (17). Then, with probability 1 − δ, we have that for
all k ∈ [K],

1 |γk − γk| ≤ O √
n

1 and |λk − λk| ≤ O √ .
n

In the next section, we assume that the satiation and reward models are estimated using the dataset collected by the proposed exploration strategies and estimators for multiple trajectories or a single trajectory of user utilities. We will show that performing planning based on these estimated models will give us policies that perform well for the true MDP.

26

C.3 Planning
For a continuous-state MDP, planning can be done through either dynamic programming with a discretized state space or approximate dynamic programming that uses function approximations. In Appendix C.3.2, we consider the case where we are given a continuous-state MDP planning oracle and provide guarantees of the optimal state-dependent policy planned under the estimated satiation dynamics and reward model. Within the state-dependent policies, we also consider a set of policies that only depend on time (Appendix C.3.1), i.e., the time-dependent competitor class deﬁned in § 5.2. In addition to not requiring discretization of the state space to solve the planning problem, such policies can be deployed to settings where user utilities are hard to attain after the exploration stage. We will show that using the dataset (collected by our exploration strategy in Appendix C.2) with suﬃcient trajectories (or a suﬃcient long trajectory) to estimate {γk, λk}Kk=1, the optimal policy πM ∗ for M = x1, [K], {γk, λk, bk}Kk=1, T also performs well in the original MDP M. We note that bk is known exactly since it is the same as the ﬁrst observed reward for arm k, as discussed in Appendix C.2.

C.3.1 Time-dependent Policy
We ﬁrst show that ﬁnding the optimal time-dependent policy is equivalent to solving the bilinear program (4).
Lemma 4. Consider a policy π that depends only on the time step t but not the state xt, i.e., π satisﬁes πt = πt(xt) = πt(xt) for all t ∈ [T ] and xt, xt ∈ X . Then, we have

T
V1π,M(xinit) = µπt,t(uπt,0:t−1),
t=1

where uπt,0:t−1 is the corresponding pull sequence of arm πt under policy π and µk,t is deﬁned in (3).

Remark 6. We denote the policy obtained by solving (4) using model parameters in M by πM T .

Because solving (4) is equivalent to maximizing

T t=1

µπt,t(uπt,0:t−1),

Lemma

4

suggests

that,

for MDP M, the best policy π that depends only on the time step t but not the exact state xt

(which we refer as time-dependent policies), is πM T .

Proposition 5. Fix δ ∈ (0, 1). Suppose that for all k ∈ [K], we are given Dkn,m such that n ≥ 64 log(2/δ) and ak ∈ (a, a) for some 0 < a < a < 1 almost surely where ak is deﬁned
in (14). Consider a policy π that depends on only the time step t but not the state xt. Then, with

probability 1 − δ, we have that

|V1π,M(xinit) − V1π,M(xinit)| ≤ O √Tn .

Remark 7. Proposition 5 applies to time-dependent policies. Such policies can be constructed

from an optimal solution to (4) or the w-lookahead policy (√5). From these results, we deduce

that when the historical trajectory is of size n = O(T ), the T -lookahead policy πw obtained

√

M

from solving (5) with the parameters from the estimated MDP M will be O( T )-separated from

the optimal time-dependent policy πM T obtained by solving (4) with the true parameters of M.

That is,

πT

πw

πT

πT

πT

πT

0

≤

V1,M M (xinit)

−

V1,M M (xinit)

=

V1,M M (xinit)

−

V M (xinit)
1,M

+

V M (xinit)
1,M

−

V M (xinit)
1,M

πT

πw

πw

πw

+

V M (xinit)
1,M

−

V M (xinit)
1,M

+

V M (xinit)
1,M

−

V1,M M (xinit)

27

πT

πT

πT

πw

πw

πw

≤

|V1,M M (xinit)

−

V M (xinit)|
1,M

+

|V M (xinit)
1,M

−

V M (xinit)|
1,M

+

|V M (xinit)
1,M

−

V1,M M (xinit)|

√

≤ O( T ),

πT

πT

where the second inequality follows from the fact that V M (xinit) − V M (xinit) ≤ 0 (since for the

1,M

1,M

MDP M, πT is the optimal time-dependent policy), and the third (last) inequality is derived by
M
applying Proposition 5 twice and using Remark 4.

C.3.2 State-dependent Policy

In Proposition 6, we show that the diﬀerence between the value of the optimal state-dependent

policy πM ∗ , and the value of the optimal state-dependent policy π∗ planned under the estimated

√

M

M is of order O(T 2/ n) where n is the number of historical trajectories if we use multiple

trajectories to estimate γk and λk.

Proposition 6. Fix δ ∈ (0, 1). Suppose that for all k ∈ [K], we are given Dkn,m such that n ≥ 64 log(2/δ) and ak ∈ (a, a) for some 0 < a < a < 1 almost surely where ak is deﬁned in (14).
Then, with probability 1 − δ,

|V1∗,M(xinit) − V1π,M M ∗ (xinit)| ≤ O √T n2 .

Remark 8. The assumptions in Proposition 5 and 6 correspond to the case where we use multiple trajectories to estimate the satiation dynamics and reward model. They can be replaced by conditions on single trajectory datasets when one uses a single trajectory to estimate the parameters.
In summary, as Proposition 6 suggests, when giv√en a continuous-state MDP planning oracle, our algorithm obtain a policy πM ∗ that is O(T 2/ n) away from the optimal policy πM ∗ under the true MDP M where the size of the exploration stage for our algorithm (EEP) is O(Kn) and the horizon of the exploitation/planning stage is T . We also note that the optimal statedependent policy πM ∗ is the optimal competitor policy when the competitor class (§ 5.2) contains all measurable functions from X to [K].

28

D Proofs of Section 6.2 and Appendix C.2.2

D.1 Proof of Theorem 3 and Theorem 8

We notice that Theorem 3 is a consequence of Theorem 8 when m = 1. More speciﬁcally, the

dataset Pkn and the parameter Ak = (γk, λkγk) in Theorem 3 is a special case of the dataset

Pkn,m

and

parameter

Ak

=

(

γ

m k

,

λk

γ

m k

)

considered in Theorem 8 by taking m = 1. Thus, below

we directly present the proof of Theorem 8 where we use the notation from Theorem 8 (and

Appendix C.2.2), i.e., ak = γkm and dk = λkγkm.

We begin with presenting some key results from [40]; we utilize these results in establishing the sample complexity of our estimator for identifying an aﬃne dynamical system in Appendix C.2.2.

Deﬁnition 1. [40, Deﬁnition 2.1] Let {φt}t≥1 be an {Ft}t≥1-adapted random process taking

values in R. We say (φt)t≥1 satisﬁes the (k, ν, p)-block martingale small-ball (BMSB) condition

if, for any j ≥ 0, one has k1

k i=1

P(|φj+i|

≥

ν |Fj )

≥

p

almost

surely.

Given

a

process

(Xt)t≥1

taking values in Rd, we say that it satisﬁes the (k, Γsb, p)-BMSB condition for Γsb 0 if for any

ﬁxed w in the unit sphere of Rd, the process φt := w, Xt satisﬁes (k, w Γsbw, p)-BMSB.

Proposition 7. [40, Proposition 2.5] Fix a unit vector w ∈ Rd, deﬁne φt = w Xt. If the scalar process {φt}t≥1 satisﬁes the (l, w Γsbw, p)-BMSB condition for some Γsb ∈ Rd×d, then

n 2

w Γsbwp2

P

φt ≤

l T /l 8

t=1

T /l p2

≤ exp −

.

8

Theorem 10. [40, Theorem 2.4] Fix δ ∈ (0, 1), T ∈ N and 0 ≺ Γsb Γ. Then if (Xt, Yt)t≥1 ∈

(Rd×Rn)n is a random sequence such that (a) Yt = AXt+ηt, where Ft = σ(η1, . . . , ηt) and ηt|Ft−1

is σ2-sub-Gaussian and mean zero, (b) X1, . . . , XT satisﬁes the (l, Γsb, p)-BMSB condition, and

(c) P(

n t=1

XtXt

T Γ) ≥ δ. Then if

10l T≥

log (1/δ) + 2d log(10/p) + log det(ΓΓ−1)

,

p2

sb

we have that for A = arg minA∈Rn×d

T t=1

Yt − AXt

22,

 90σ
P  A − A op > p

 n + d log(10/p) + log det ΓΓ−sb1 + log(1/δ)
 ≤ 3δ. T λmin(Γsb)

We note that in the proof of Theorem 10 in [40], condition (b) is used through applying Proposition 7 to ensure that for any unit vector w ∈ Rd,

P T w, Xt 2 ≤ (w Γsbw)p2 l T /l ≤ exp − T /l p2 . (18)

8

8

t=1

To apply Theorem 10 in our setting to obtain Theorem 8, we verify condition (a) and (c). For condition (b), we show a result similar to (18). The below technical lemmas are used in our proof of Theorem 8.

Lemma 8. Let a, b be scalars with b > 0. Suppose that X ∼ N (a, b). Then for any θ ∈ [0, 1],

P(|X| ≥

θ(a2 + b)) ≥ (1 − θ)2 . 9

29

Proof. By the Paley-Zygmund inequality,

P(|X| ≥

θE[X2]) = Pr(X2 ≥ θE[X2]) ≥ (1 − θ)2 E[X2]2 . E[X 4 ]

Using the mean and variance of non-central chi-squared distributions, we obtain that

E[X2] = a2 + b, E[X4] = a4 + 6a2b + 3b2 = (a2 + 3b)2 − 6b2.

Plugging them back to the Paley-Zygmund inequality, we have that

P(|X| ≥

θ(a2 + b) ≥ (1 − θ)2 , 9

where the last inequality uses the fact that E[X4] ≤ (a2 + 3b)2 ≤ 9(a2 + b)2 = 9E[X2]2.

Lemma 9. Let {φt}t≥1 be a scalar process satisfying that

1l l P(|φt+i| ≥ νt|Ft) ≥ p,
i=1

for νt depending on Ft. If P(mint νt ≥ ν) ≥ 1 − δ for ν > 0 that depends on δ, then

T 2

ν 2 p2

P

φt ≤

l T /l 8

t=1

3 T /l p

≤ exp −

+ δ.

4

Proof. We begin with partitioning Z1, . . . , ZT into S := T /l blocks of size l. Consider the random variables

Bj = 1

l 2

νj2lpk

φjl+i ≥ 2

i=1

, for 0 ≤ j ≤ S − 1.

We observe that

T 2

ν 2 p2

P

φt ≤

l T /l 8

t=1

T 2

ν 2 p2

=P

φt ≤

l T /l 8

∩ {min νt ≥ ν}
t

t=1

T 2

ν 2 p2

+P

φt ≤

l T /l 8

∩ {min νt < ν}
t

t=1

T

ν2 p2

≤P

φ2t ≤ t/l l lS ∩ {min νt ≥ ν} + P(min νt < ν)

8

t

t

t=1

T

ν2 p2

2 ≤ t/l l kS + δ.

≤P

φt

8

t=1

Using Chernoﬀ bound, we obtain that







T 2

ν2t/l lp2

S−1 l 2

νj2lp2

S−1 l 2

νj2lp2

P

φt ≤ 8 kS ≤ P 

φjl+i ≤ 8 lS = P 

φjl+i ≤

lS 8

t=1

j=0 i=1

j=0 i=1

30





S−1

p

− pS λ

≤ P  Bj ≤ S ≤ inf e 4 E[e

4

λ≤0

j=0

S−1 j=0

Bj

],

where the second to the last inequality uses the fact that νj22lpl Bj ≤ that

l i=1

φ2jl+i

Further,

we

have

l 2

νj2lpl

1l

p

E[Bj|Fjl] = P

φjl+i ≥ 2 Fjl ≥ P l 1 {|φjl+i| ≥ νjl} ≥ 2 Fjl

i=1

i=1

p ≥,
2

where

the

ﬁrst

inequality

uses

the

fact

that

1 ν2

φ2jl+i

≥

1{φjl+i|

≥

νjl}

and

the

last

inequality

jl

uses the fact that for a random variable X supported on [0, 1] almost surely such that E[X] ≥ p

for some p ∈ (0, 1), then for all t ∈ [0, p], P (X ≥ t) ≥ p1−−tt . This is true because

1

1

1

t

P (X ≥ t) = dP(x) ≥ xdP(x) = xdP(x) − xdP(x) = p − t (1 − P (X ≥ t)) .

t

t

0

0

In our case, E

1 l

l i=1

1

{|φjl+i|

≥

νjl}

Fjl

= 1l

obtain that for λ ≤ 0, i.e., eλ ≤ 1,

l
i=1 P

|φjl+i| ≥ νjl Fjl

≥ p. Thus, we

E[eλBj |Fjl] = eλP

Bj = 1 Fjl

+

P (Bj

=

0)

=

(eλ

−

1)E[Bj |Fjl ]

+

1

≤

(eλ

−

p 1)

+

1.

2

By law of iterated expectation, we obtain that

E[eλ

S−1 j=0

Bj ]

=

E

eλ

S−2 j=0

Bj

E[eλBj

|F(S

−1)k

]

≤

(eλ

−

p 1)

+

1

E

eλ

S−2 j=0

Bj

≤

(eλ

−

p 1)

+

1

S
.

2

2

Finally, we need to ﬁnd

inf e−pS/4

(eλ

−

p 1)

+

1

S
.

λ≤0

2

We can see that λ∗ = −∞, which gives that

inf e−pS/4

(eλ

−

p 1)

+

1

S
= e−pS/4

p 1−

S
≤ e−pS/4e−pS/2 = e−3pS/4,

λ≤0

2

2

where we have used the fact that 1 + x ≤ ex for all real-valued x.

To apply Theorem 10, we ﬁrst recall that the aﬃne dynamical system we aim to identify is as follows:

x˜k,j+1 = akx˜k,j + dk + z˜k,j ,

where x˜k,1 = 0, ak ∈ (0, 1) and z˜k,j ∼ N (0, σz2,k). We deﬁne the following quantities

j−1
Γk,j := σz2,k a2ki,
i=0

j−1
dk,j := ajkdk,
i=0

and Γk,∞ = σz2,k

∞ a2i = σz2,k . We notice that for all t ∈ [T ], j ≥ 1,

i=0 k

1−a2k

x˜k,t+j |x˜k,t ∼ N ajkx˜k,t + dk,j , Γk,j .

31

Lemma 10. Fix t ≥ 0 and j ≥ 1. Recall that xk,t := (x˜k,t, 1) ∈ R2. Fix a unit vector w ∈ R2. For any ∈ (0, 1), we have

P | w, xk,t+j | ≥ √12 min 1 − , Γk,j − 1 − 1 (ajkx˜k,t + dk,j)2

1 ≥
36

Proof. By Lemma 8, we have that for any unit vector w ∈ R2,

1 P | w, xk,t+j | ≥ √
2

w1

aj x˜k,t + dk,j

2
+ w2 + w2Γk,j

xk,t

1 ≥.

k

1

36

For all ∈ (0, 1), we have

((w1(ajkx˜k,t + dk,j ) + w2)2 + w12Γk,j = w1 ajkx˜k,t + dk,j 2 + w22 + 2w2w1 ajkx˜k,t + dk,j + w12Γk,j

≥ (1 − )w22 − 1 − 1

w1 ajkx˜k,t + dk,j

2
+ w12Γk,j

≥ min 1 − , Γk,j − 1 − 1 (ajkx˜k,t + dk,j)2 .

Lemma 11. Fix δ ∈ (0, 1). {xk,t}nt=1 satisfy that for any unit vector w ∈ R2,

P n w, xk,t 2 ≤ ψ2p2 j n/j 16
t=1

3 n/j p

≤ exp −

+δ

4

with p = 1/72,

2Γk,∞ log(n/δ)

√

j := max − logak 1 + (1 − ak)

dk

, − logak 2 ,

ψ :=







Γk,∞

Γk,∞ 

min  16d2k 2 + Γk,∞ , 4 .

(1−ak )

Proof. Fix δ ∈ (0, 1). Recall that from Lemma 9, we have shown that for all t ≥ 0 and k ≥ 1, given a unit vector w ∈ R2, for any ∈ (0, 1), we have

P | w, xk,t+j | ≥ √12 min 1 − , Γk,j − 1 − 1 (ajkx˜k,t + dk,j)2

1 ≥.
36

Denote qt,j = ajkx˜k,t + dk,j where x˜k,t ∼ N (dk,t, Γk,t). Fix δ ∈ (0, 1). Using the standard Gaussian tail bound and the union bound, we have that with probability 1 − δ,

max qt,j ≤ ajk
t∈[T ]

dk + 1 − ak

2Γ∞ log(n/δ) + dk . 1 − ak

32

When j ≥ j , Γk,j ≥ Γk,∞/2, and with probability 1 − δ, maxt∈[T ] qt,j ≤ 12−dakk . Thus, for j ≥ j , and

4d2k

=

(1−ak )2

,

(1−4da2kk)2 + Γ∞/4

we have

νt2,j := min

1 − ε, Γk,j −

1 −1

qt2,j

ε

≥ min

1 − ε, Γk,∞/2 −

1 −1
ε

4d2k (1 − ak)2





 ≥ min

Γk,∞

, Γk,∞  = ψ2.

 (11−6adk2k)2 + Γk,∞ 4 

Putting it altogether, we have

1 2j

√

2j P | w, xk,t+j | ≥ νt,j/ 2|Ft

j=1

Further, we have

1 2j

√

1

≥ 2j

Pr(| w, xk,t+j | ≥ νt,j / 2|Ft) ≥ 72 .

j=j

P min νt2,j ≥ ψ2 ≥ 1 − δ.
t∈[T ]

Applying Lemma 9, we have that for p = 712 , P n w, xk,t 2 ≤ ψ2p2 j n/j 16
t=1

3 n/j p

≤ exp −

+ δ.

4

Proof of Theorem 8. Based on our setup, condition (a) of Theorem 10 is satisﬁed. For any n, using Lemma 11 with δ = exp(−n), we have that

∀w ∈ R2, P n w, xk,t 2 ≤ ψ2p2 j n/j 16
t=1

3 n/j p

3 n/j p

≤ exp −

+ δ ≤ 2 exp −

,

4

4

with p = 1/72,

2Γk,∞(log(n) + n)

√

j := max − logak 1 + (1 − ak)

dk

, − logak 2 ,

ψ :=







Γk,∞

Γk,∞ 

min  16d2k 2 + Γk,∞ , 4 .

(1−ak )

Thus, we have provided a similar result to (18), which is what condition (b) of Theorem 10 is used for. In this case, we have Γsb = ψI where I is a 2 × 2 identity matrix. Finally, to verify condition (c), we notice that we have

 b2k(1−ajk−1)2 + σz2,k(1−a2kj−2)

Γk,j := E[xk,j xk,j ] =  (1−ak)2

1−a2k
j−1

(1−ak )bk

1−ak

(1−ajk−1)bk  1−ak  .
1

33

and we denote

00 Γ := Γk,n + 0 1 + Γsb,

which gives that 0 ≺ Γsb ≺ Γ and for all j ≥ 1, 0 Γk,j ≺ Γ. Then, we have that

P Xk Xk

2n Γ
δ

=P

λmax

(nΓ)−1/2Xk Xk(nΓ)−1/2

2 ≥

δ

δ ≤ 2E
δ ≤ 2E

λmax (nΓ)−1/2Xk Xk(nΓ)−1/2 tr (nΓ)−1/2Xk Xk(nΓ)−1/2 ≤ δ,

where the last inequality is true since E Xk Xk =

n j=1

Γk,j

nΓ (for all j ∈ [n], trace(Γ −

Γk,j) > 0 and det(Γ − Γk,j) > 0). Following Theorem 10, for δ ∈ (0, 1), when the number of samples satisfy that

n 10 ≥

log (1/δ) + 4 log(10/p) + log det(ΓΓ−1)

,

j p2

sb

we have that
 P  Ak − Ak 2 > 90σz,k
p

 1 + 2 log(10/p) + log det ΓΓ−sb1 + log(1/δ)
 ≤ 3δ. nψ

D.2 Proof of Corollary 4 and Corollary 9
Similar to Appendix D.1, Corollary 4 is a special case of Corollary 9 when m = 1. Hence, we directly present the proof of Corollary 9 below.

Proof of Coro√llary 9. Fix δ ∈ (0, 1). We have that with probability 1 − δ, (n, δ, k) := Ak −

Ak 2 ≤ O(1/ √

n).

With

probability

at

least

1

−

δ K

,

ak := |ak −ak| ≤

Ak −Ak 2 = √

(n, δ/K, k) =

O(1/ n) and dk := |dk − bk| ≤ Ak − Ak 2 = (n, δ/K, k) = O(1/ n). When m = 1, then

|γk − γk| = ||ak| − ak| ≤ |ak − ak| = ak ≤ (n, δ/K, k). When m ≥ 2, since γk = 0, we have that

|ak| − ak

|ak − ak|

|γk − γk| = |ak|(m−1)/m + |ak|(m−2)/mγk + . . . + γkm−1 ≤ γkm−1 .

On the other hand, we obtain that

|λk − λk| =

dk − dk ≤ dk − dk + dk − dk ≤ dk + λk ak ≤ O

1 √

.

ak ak

ak ak ak ak ak

ak

n

The proof completes as follows:

√

√

K

δ

P ∀k ∈ [K], |γk − γk| ≤ O(1/ n), |λk − λk| ≤ O(1/ n) ≥

1− K

k=1

where the last inequality follows from Bernoulli’s inequality.

≥ 1 − δ,

34

E Additional Proofs and Discussion of Section 6

E.1 Proof of Theorem 5
Lemma 12. Consider any episode i + 1 (from time ti + 1 to ti+1) where the initial state xi = (µ1,ti+1(u1,0:ti ), n1,ti+1, . . . , µK,ti+1(uK,0:ti ), nK,ti ) and {uk,0:ti }Kk=1 are the past pull sequences of the proposed policy π1:ti. For all π˜ti+1:ti+1 such that π˜t = π˜t(xt) = π˜t(xt), π˜t ∈ [K], ∀t ∈ [ti + 1, ti+1], xt, xt ∈ X , we have that

ti+1
Exti+2,...,xti
t=ti+1

r(xt, π˜t(xt))|xti+1 = xi

ti+1

=

µk,t(uk,0:t−1),

t=ti+1

where {uk,ti+1:ti+1 }Kk=1 is the arm pull sequence of π˜ti+1:ti+1 .

Proof. Let k denote π˜t where t ∈ {ti + 1, . . . , ti+1}. Recall that we use uk,0:t−1 to denote the
pull sequence of arm k under policy π˜1:ti+1 = (π1:ti, π˜ti+1:ti+1). If k has not been pulled before time t by π˜1:ti+1 , then Exti+2,...,xti+1 r(xt, π˜t)|xti+1 = xi = bπt = µπt,t(uπt,0:t−1). If k has been pulled before, then let q1, . . . , qn denote the time steps that arm k has been pulled before time
t by π˜1:ti+1, i.e., uk,qi = 1 for i ∈ [n] and uk,t = 0 for t ∈/ {q1, . . . , qn}. We have that for t ∈ {ti + 1, . . . , ti+1},

Exti+2,...,xti+1 r(xt, π˜t)|xti+1 = xi =bk − Exti+2,...,xti+1−1 Exti+1 γknk,ti+1 xk,ti+1 + λkγknk,ti+1

|xti+1 = xi

=bk − Exti+2,...,xqn Exqn+1 γknk,ti+1 xk,qn+1 + λkγknk,ti+1 |xti+1 = xi

=bk − Exti+2,...,xqn γknk,ti+1 γknk,qn xk,qn + λkγknk,qn + λkγknk,ti+1 |xti+1 = xi

= . . . = bk − λk γknk,ti+1 + γknk,ti+1 +nk,qn + . . . + γknk,ti+1+nk,qn +...nk,q1

=µk,t(uk,0:t−1),

where the second equality is true because when arm k is not pulled for example at time ti+1 − 1,
the state for arm k at time ti+1 − 1 will satisfy that xk,ti+1 = xk,ti+1−1 and nk,ti+1 = nk,ti+1−1 + 1 with probability 1. In this case, we have that

Exti+1

γknk,ti+1 xk,ti+1 + λkγknk,ti+1 xti+1−1

= γknk,ti+1−1+1xk,ti+1−1 + λkγknk,ti+1−1+1 = γknk,ti+1 xk,ti+1−1 + λkγknk,ti+1 .

The third equality is true since when arm k is pulled for example at time qn, then we have that

Eqn+1∼pM(·|xqn ,k,qn) γknk,ti+1 xk,qn+1 + λkγknk,ti+1 =γknk,ti+1 γknk,qn xk,qn + λkγknk,qn + λkγknk,ti+1 ,

where pM is given in Appendix C.1. The second to last last equality holds because xk,ti+1 = µk,ti+1(uk,0:ti ) where µk,t(·) is deﬁned in (3).

Lemma 13. For any episode i + 1 (from time ti + 1 to ti+1), given the past arm pull sequences {uk,0:ti}Kk=1 of the proposed policy π1:ti, the optimal time-dependent competitor policy π˜ti+1:ti+1, where π˜t = π˜t(xt) = π˜t(xt), π˜t ∈ [K], ∀t ∈ [ti + 1, ti+1], xt, xt ∈ X , for this episode is given by Lookahead({λk, γk, bk}Kk=1, {uk,0:ti }Kk=1, ti, ti+1) where {λk, γk, bk}Kk=1 are the true reward parameters for the rebounding bandits instance.

35

Proof. By Lemma 12, we have that the optimal time-dependent competitor policy π˜ti+1:ti+1 max-

imizes tti=+t1i+1 µk,t(uk,0:t−1), by choosing uk,ti+1:ti+1 . Thus, by the deﬁnition of Lookahead (5), given our proposed policy π1:ti, the optimal time-dependent competitor policy is given by

Lookahead

({

λ

k

,

γk

,

bk

}

K k=1

,

{uk

,0:

ti

}Kk=1

,

ti

,

ti

+1

).

Proof of Theorem 5. Using Lemma 13, we have that given our policy π1:T and its corresponding

pull sequence uk,0:t−1 for k ∈ [K], t ∈ [T ], the optimal competitor policy for episode i+1 where i ∈

{0, . . . , T /w } (episode i+1 ranges from time ti+1 = iw+1 to ti+1 = min{iw+w, T }) is given by

Lookahead

({

λk

,

γk

,

bk

}

K k=1

,

{uk

,0:

ti

}Kk=1

,

ti

,

ti

+1

).

We

use

M({

λ

k

,

γk

,

bk

}Kk=1

,

{

uk

,0:ti

}

K k=1

,

ti

,

ti

+1

)

to denote the (optimal) objective value of (5) given by Lookahead({λk, γk, bk}Kk=1, {uk,0:ti}Kk=1, ti, ti+1).

Denote b = maxk bk and b = mink bk.

Exploration Stage Recall that in Algorithm 1, we have deﬁned T = T 2/3 + w − (T 2/3 mod w)
which is a multiple of w. For the ﬁrst T time steps, as deﬁned in Algorithm 1, our policy π1:T is a time-dependent policy, i.e., it satisﬁes that πt = πt(xt) = πt(xt), πt ∈ [K], ∀t ∈ [1, T ], xt, xt ∈ X . Using 12, we obtain that the regret for the ﬁrst T /w episodes is given by

T˜/w−1


w



max E  r(xiw+j, π˜j(xiw+j)) xiw+1 = xi

π˜1:w ∈C w

i=0

j=1

T˜/w−1

−

E r(xiw+j , πiw+j (xiw+j )) xiw+1 = xi

i=0

T˜/w−1

≤

M({λk, γk, bk}Kk=1, {uk,0:iw}Kk=1, iw, iw + w) − T

i=0

λγ b−
1−γ

λγ ≤T b − b +
1−γ

T T 2/3.

since T ≤ T 2/3 + w and by assumption, w ≤ T 2/3.

Estimation Stage By Theorem 3 and Corollary 4, we have that for any δ ∈ (0, 1) and
n ≥ n0(δ, k) where n0(δ, k) depends on δ logarithmically, with probability 1 − δ, for all k ∈ [K] |γk − γk| ≤ Cγk l√ogn(1/δ) and |λk − λk| ≤ Cλk l√ogn(1/δ) when γk > 0.

We deﬁne two numbers T0 := minT {T : (

K k=1

n0(k,

T

−1/3))3/2

=

C1K(log T )3/2

<

T}

and

T0 := minT T : maxk γk + √TC2γ/k3/K < 1 . These two numbers exist as T can be chosen to

be arbitrarily large. Take T0 = max{T0, T0 }. Then for all T ≥√ T0, with probability 1 − δ

where δ = T −1/3, we have that ∀k ∈ [K], |γk − γk| ≤ γ = O( KT −1/3 log T ), |λk − λk| ≤

√ λ = O( KT −1/3 log T ) and

λ 1−γkγk + γ (1−γk)λ(1−γk)

√ ≤ O( KT −1/3 log T ) since γk ≤

γk +

Cγk < 1 and γk ≤ γ < 1.
T02/3 /K

For any pull sequence uk,0:t−1, using our obtained estimated parameters {γk, λk, bk}Kk=1, we deﬁne

the estimated reward function: for t ≥ 2, µk,t(uk,0:t−1) = bk − λk

t−1 i=1

γkt−iuk,i

, and for t = 1,

µk,1(uk,0:1) = bk = µk,1(uk,0:1), where we note that bk = bk since it is the reward of the ﬁrst pull of arm k. Given t ≥ 2, we have that

|µk,t(uk,0:t−1) − µk,t(uk,0:t−1)|

36

= λk

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

= λk

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

+ λk

≤|λk − λk| γk + λ γk − γk

1 − γk

1 − γk 1 − γk

≤ λ γk + γ λ .

1 − γk

(1 − γk)(1 − γk)

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

(19)

Planning Stage Given our policy π1:T (along with its pull sequence {uk,0:T }Kk=1), starting from

time T +1, for any episode i+1 ≥ T /w, we denote the optimal competitor policy to be πt∗i+1:ti+1 =

Lookahead

({

λ

k

,

γk

,

bk

}Kk=1

,

{

uk

,0:ti

−1

}

K k=1

,

ti

,

ti

+1

)

where

ti

=

iw

and

ti+1

=

min{iw + w, T }.

The

cumulative expected reward collected by πt∗i+1:ti+1 and πti+1:ti+1 has the diﬀerence

M({λk, γk, bk}Kk=1, {uk,0:ti−1 }Kk=1, ti, ti+1) − M({λk, γk, bk}Kk=1, {uk,0:ti−1 }Kk=1, ti, ti+1)

ti+1

ti+1

=

µπ∗,t(u∗π∗,0:t−1) −

µπt,t(uπt,0:t−1)

t

t

t=ti+1

t=ti+1

ti+1

ti+1

=

µπ∗,t(u∗π∗,0:t−1) −

µπ∗,t(u∗π∗,0:t−1)

t

t

t

t

t=ti+1

t=ti+1

ti+1

ti+1

+

µπ∗,t(u∗π∗,0:t−1) −

µπt,t(uπt,0:t−1)

t

t

t=ti+1

t=ti+1

ti+1

ti+1

+

µπt,t(uπt,0:t−1) −

µπt,t(uπt,0:t−1)

t=ti+1

t=ti+1

ti+1

ti+1

≤

µπ∗,t(u∗π∗,0:t−1) −

µπ∗,t(u∗π∗,0:t−1)

t

t

t

t

t=ti+1

t=ti+1

ti+1

ti+1

+

µπt,t(uπt,0:t−1) −

µπt,t(uπt,0:t−1).

t=ti+1

t=ti+1

where u∗π∗,0:t−1 is the corresponding pull sequence of arm πt∗ under policy π1∗:t = (π1:ti , πt∗i+1:t), t

and

the

last

inequality

holds

because

πti+1:ti+1

=

Lookahead

({

λk

,

γ

k

,

bk

}

K k=1

,

{uk

,0:

ti

}Kk=1

,

ti

,

ti

+1

)

is the optimal solution under the estimated parameters {λk, γk, bk}Kk=1 and π’s previous past pull sequence {uk,0:ti}Kk=1. Further, using (19) and the fact that ti − ti−1 ≤ w, we obtain that

ti+1

ti+1

µπ∗,t(u∗π∗,0:t−1) −

t

t

µπ∗,t(u∗π∗,0:t−1)

t

t

t=ti+1

t=ti+1

ti+1

ti+1

+

µπt,t(uπt,0:t−1) −

µπt,t(uπt,0:t−1)

t=ti+1

t=ti+1

≤2w max λ γk + γ λ .

k

1 − γk

(1 − γk)(1 − γk)

Finally, putting it altogether, we have obtained that for all T ≥ T0,

Regw(T ) =

T /w i=0

−1 maxπ˜1:w∈Cw

E

min{w,T −iw} j=1

r(xiw+j , π˜j (xiw+j ))

xiw+1

=

xi

37

−E

min{w,T −iw} j=1

r(xiw+j , πiw+j (xiw+j ))

xiw+1

=

xi





T /w −1

√

λγ

≤O(T 2/3) + (1 − T −1/3) 

2wO( KT −1/3 log T ) + T −1/3 T b − b +

1−γ

i=T /w

√

≤O(T 2/3) + (T − T 2/3)O( KT −1/3 log T ) + O(T 2/3)

√

≤O( KT 2/3 log T ),

which we notice that with probability δ = T −1/3, the cumulative expected reward from time T to T between the optimal competitor policy and our policy π is at most T b − b + 1λ−γγ . This completes the proof.

E.2 Exploration Strategies
In the exploration phase of Algorithm 1 (from time 1 to T ), in addition to playing each arm repeatedly for T /K times, in general, we could explore by playing each arm at a ﬁxed interval, i.e., the time interval between two consecutive pulls of arm k should be a constant mk. For example, this includes playing the arms cyclically with the cylce being 1, 2, . . . , K or playing the ﬁrst two arms in an alternating fashion from time 1 to 2T /K, then the next two arms, etc. As shown in Theorem 8 and Corollary 9, using the datasets (of size n) collected by these exploration strateg√ies, we can obtain estimators γk and λk with the estimation error being on the order of O(1/ n). Using these results (in replacement of Theorem 3 and Corollary 4 in the estimation stage of the proof of Theorem 5), we can obtain that there exists T0 such that for√all T ≥ T0, the regret upper bound of EEP under these exploration strategies are of order O( KT 2/3 log T ).

38

F Additional Proofs of Appendix C

F.1 Proof of Corollary 7

Proof.

Fix δ ∈ (0, 1).

By

Theorem

6,

for

all

k

∈

[K ],

with

probability

1

−

δ 2K

,

we

have

the

following: When m = 1, then |γk − γk| = ||ak| − ak| ≤ |ak − ak| ≤

a(n,

δ 2K

,

k

)

.

When

m

≥

2,

we

have that

|ak| − ak

|ak − ak|

|γk − γk| = |ak|(m−1)/m + |ak|(m−2)/mγk + . . . + γkm−1 ≤ γkm−1 .

On the other hand, given that |ak − ak| ≤

a(n,

δ 2K

,

k),

we

have

that

with

probability

1

−

δ 2K

,

|λ − λ | =

dk − dk ≤ dk − dk + dk − dk ≤ d(n, 2δK , k) + λk a(n, 2δK , k) ≤ O

1 √

.

k

k

ak ak

ak ak ak ak

ak

ak

n

The proof completes as follows:

|ak − ak|

d(n,

δ 2K

,

k)

λk

a(n,

δ 2K

,

k)

P ∀k, |γk − γk| ≤ γm−1 , |λk − λk| ≤

ak

+

ak

k

where the last inequality follows from Bernoulli’s inequality.

K
≥
k=1

δ 1−
2K

2
≥ 1 − δ,

F.2 Proof of Lemma 4
Proof. Let π1:T denote the sequence that policy π will take from time 1 to T . By the deﬁnition of the value function, we have that

T
V1π,M(xinit) = bπ1 + Ex2,...,xt [r(xt, πt)] ,
t=2

where xt ∼ pM(·|xt−1, πt−1, t − 1) is a state vector drawn from the transition distribution deﬁned in Section C.1. Let k denote πt and uk,0:t−1 denote the past pull sequence for arm k under policy π. If k has not been pulled before time t, then Ex2,...,xt [r(xt, πt)] = bπt = µπt,t(uπt,0:t−1). If k has been pulled before, then let t1, . . . , tn denote the time steps that arm k has been pulled before time t. We have that

Ex2,...,xt [r(xt, k)] = bk − = bk − = bk −

Ex2,...,xt−1 Ext∼pM(·|xt−1,k,t−1) γknk,t xk,t + λkγknk,t Ex2,...,xtn Extn+1∼pM(·|xtn ,k,tn) γknk,t xk,tn+1 + λkγknk,t Ex2,...,xtn γknk,t γknk,tn xk,tn + λkγknk,tn + λkγknk,t

= . . . = bk − λk γknk,t + γknk,t+nk,tn + . . . + γknk,t+nk,tn +...nk,t1

= µk,t(uk,0:t−1),

where we note that the second equality is true because when arm k is not pulled for exam-

ple at time t − 1, the state for arm k at time t − 1 will satisfy that xk,t = xk,t−1 and nk,t =

nk,t−1 + 1 with probability 1. In this case, we have that Ext∼pM(·|xt−1,k,t−1) γknk,t xk,t + λkγknk,t =

γknk,t−1+1xk,t−1

+

λk

γ nk,t−1 +1
k

=

γknk,t xk,t−1 + λkγknk,t .

The

third

equality

is

true

since

when

arm

k

is pulled for example at time t − 1, then we have that Ext∼pM(·|xt−1,k,t−1) γknk,t xk,t + λkγknk,t =

γknk,t γknk,t−1 xk,t−1 + λkγknk,t−1 +λkγknk,t . The proof completes by summing over Ex2,...,xt [r(xt, πt)]

for all t ≥ 2.

39

F.3 Proof of Proposition 5

Proof. Fix δ ∈ (0, 1). Let E1 be the event that

1

∀k ∈ [K], |γk − γk| = γk ≤ O

√ n

√ , |λk − λk| = λk ≤ O 1/ n .

From Corollary 7, we have that P(E1) ≥ 1 − δ. Let π1:T denote the sequence that policy π will take from time 1 to T . From Lemma 4, we have that

|V1π,M(xinit) − V1π,M(xinit)| =

T
µπt,t(uπt,0:t−1) − µπt,t(uπt,0:t−1) ,
t=1

where uπt,0:t−1 is the past pull sequence for arm πt under policy π before time t and µk,t(uk,0:t−1) =

bk − λk

t−1 i=1

γkt−iuk,i

for t ≥ 2 and µk,1(uk,0:1) = bk = µk,1(uk,0:1). Given t ≥ 2, let k denote

πt, we have that

|µk,t(uk,0:t−1) − µk,t(uk,0:t−1)|

= λk

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

= λk

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

+ λk

t−1
γkt−iuk,i
i=1

− λk

t−1
γkt−iuk,i
i=1

≤|λk − λk| γk + λ γk − γk

1 − γk

1 − γk 1 − γk

≤ γk λk +

λ γk

1 − γk (1 − γk)(1 − γk)

√ Since γk < 1 (ak√∈ (a, a)) almost surely and with probability 1−δ, for all k ∈ [K], γk ≤ O (1/ n) and λk ≤ O (1/ n). We have that with probability 1 − δ,

T

T

T

µπt,t(uπt,0:t−1) − µπt,t(uπt,0:t−1) ≤ |µπt,t(uπt,0:t−1) − µπt,t(uπt,0:t−1)| ≤ √n .

t=1

t=1

F.4 Proof of Proposition 6

Proof. Fix δ ∈ (0, 1). Let E1 be the event that

1

√

∀k ∈ [K], |γk − γk| = γk ≤ O

√ n

, |λk − λk| = λk ≤ O 1/ n .

From Corollary 9, we have that P(E1) ≥ 1 − δ/2. Let λ := maxk λk . Let E2 denote the event that ∀t ∈ [T ], k ∈ [K], |xk,t| ≤ B(δ/2) (10). We know that P(E2) ≥ 1 − δ/2. When E1 and E2
happen, we ﬁrst observe that for all positive integer n and k ∈ [K],

|γn − γn| ≤ |γ − γ | n max(γn−1, γn−1) ≤

|γk − γk|

√ = O(1/ n),

k

k

kk

k

k

max(γk, γk) ln (1/ max(γk, γk))

whereand the second inequality uses the assumption that ak, γk are bounded away from 0 and 1.
To continue, we ﬁrst bound the distance between the transition function in M and M. At any any time t and state xt = (x1,t, n1,t, . . . , xK,t, nK,t), when we pull arm πt = k, the next state xt+1

40

is updated by: (i) for arm k, nk,t+1 = 1 and (ii) for all other arms k = k, nk,t+1 = nk,t + 1 if nk = 0, nk,t+1 = 0 if nk,t = 0, and xk ,t+1 = xk ,t. Then, by [8, Theorem 1.3], we have that when nπt,t = 0,

pM (xt+1|xt, πt, t) − pM (xt+1|xt, πt, t) 1

(≤∗) 3|λ2k

ni=k,0t−1 γk2i − λ2k ni=k,0t−1 γk2i| |γknk,t xk,t + λkγknk,t − γknk,t xk,t − λkγknk,t |

λ2 nk−1 γ2i

+

nk,t−1 2i

k i=0 k

λk

i=0 γk

3|λ2k =

ni=k,0t−1 γk2i −

ni=k,0t−1 γk2i + (λ2k − λ2k)

λ2k

nk −1 i=0

γk2i

ni=k,0t−1 γk2i|

+ |γknk,t − γknk,t |B(δ/2) + |λkγknk,t − λkγknk,t |

λk

ni=k,0t−1 γk2i

(∗∗)
≤3

nk,t−1

nk,t−1 

nk,t −1

λ2k 

γk2i −

γk2i + (λ2k − λ2k)

γk2i

i=0

i=0

i=0

+ |γknk,t − γknk,t | (B(δ/2) + λk)

+ |λkγknk,t − λkγknk,t |

≤3 (λk + λ)2

1

1

−

+ |λk − λk|(2λk + λ) + |γnk,t − γnk,t | (B(δ/2) + λk) + |λk − λk|

1 − γk2 1 − γk2

1 − γk2

k

k

=3 (λ1k −+ γλ2)2(|1γk2−−γγ2)k2| + |λk − λ1k−|(2γλ2k + λ) + |γknk,t − γknk,t | (B(δ/2) + λk) + |λk − λk|

k

k

k

1 =: P = O √ ,
n

where (∗) holds since pM (xt+1|xt, πt, t) is a Gaussian density with mean γknk,txk,t + λkγknk,t and

variance λ2k

nk −1 i=0

γk2i

and

(∗∗)

uses

the

fact

that

λ2k

nk −1 i=0

γk2i

≥

λ2k

≥

1.

When

nπt,t

=

0

and

condition (i) and (ii) are fulﬁlled, we have that pM (xt+1|xt, πt, t) − pM (xt+1|xt, πt, t) 1 = 0. Otherwise, that is, if condition (i) or (ii) is not satisﬁed, we also have that pM (xt+1|xt, πt, t) − pM (xt+1|xt, πt, t) 1 = 0 since pM (xt+1|xt, πt, t) = pM (xt+1|xt, πt, t) = 0. Next, we examine the diﬀerence of the expected reward obtained by pulling arm k at state xt at time t in MDP M

and M; when nk,t = 0, this is given by

|r(xt, k)] − r(xt, k)]| = |γknk,t xk,t + λkγknk,t − γknk,t xk,t − λkγknk,t | ≤ |xk,t| · |γnk,t − γnk,t | + |λkγknk,t − λkγknk,t + λkγknk,t − λkγknk,t |
≤ (B(δ/2) + λk) |γknk,t − γknk,t | + |λk − λk| =: R = O √1n ,

where r(xt, k) is the expected reward of pulling arm k at state xt in MDP M. Putting it altogether, we have that for any deterministic policy π,

V1π,M(xinit) − V1π,M(xinit) = r(xinit, π1(xinit)) − r(xinit, π1(xinit)) + Ex2∼pM(·|x1,π,1)[V2π,M(x2)]

− Ex2∼pM(·|x1,π,1)[V2π,M(x2)]

≤ R + Ex2∼pM(·|x1,π,1)[V2π,M(x2)] − Ex2∼p (·|x1,π,1)[V2π,M(x2)] M + Ex2∼pM(·|x1,π,1)[V2π,M(x2)] − Ex2∼pM(·|x1,π,1)[V2π,M(x2)]

T
≤ T R + EM,π
t=1

Ext+1∼pM(·|xt,π,t)[Vtπ+1,M(xt+1)]

41

− Ext+1∼p (·|xt,π,t)[Vtπ+1,M(xt+1)] M
≤ T R + T 2 P max bk,
k

where pM(·|xt, π, t) denotes p(·|xt, πt(xt), t) in MDP M and the last inequality uses the fact that pM(·|xt, π, t) − pM(·|xt, π, t), Vtπ+1,M ≤ pM(·|xt, π, t) − pM(·|xt, π, t) 1 Vtπ+1,M ∞ ≤ P T maxk bk. Finally, we have that

V1∗,M(xinit) − V1π,M M ∗ (xinit) = V1π,M M ∗ (xinit) − V1π,M M ∗ (xinit) + V1π,M M ∗ (xinit) − V1π,M M ∗ (xinit)

π∗

π∗

+ V M (xinit) − V M (xinit) ≤ 2T

R + 2T 2

P max bk,

1,M

1,M

k

where the equation follows from the fact that V1∗,M(xinit) = V1π,M M ∗ (xinit) and rearranging the terms, and the inequality follows from applying the bound of V1π,M(xinit) − V1π,M(xinit) ≤ T R + T 2 P maxk bk that was derived above for π = πM ∗ and π = πM ∗ and using the fact that the policy πM ∗ is optimal for MDP M. Let E3 denote the event that V1∗,M(xinit) − V1π,M M ∗ (xinit) ≤ O(T 2/√n). Putting it altogether, we have that P(E3) ≥ P(E2, E1) = 1 − P(E2c ∪ E1c) ≥ 1 − δ.

42

Cumulative Reward
log Regw

560

540

520

500

480

460 440

T-lookahead (24h, 50 threads) Upper Bound (24h, 50 threads)

420

w-lookahead

400 1 2 3 4 5 6 7 w8 9 10 11 12 13 14 15

(a) T = 100

6.5 w = 2, Slope = 0.59

6.0

w = 5, Slope = 0.65 w = 8, Slope = 0.66

5.5 w = 10, Slope = 0.71

5.0

4.5

4.04.0 4.5 5.0 log5T.5 6.0 6.5

(b) log Regw v.s. log T

Figure 4: Figure 4a shows the cumulative expected reward collected by and w-lookahead policy (blue dots) when T = 100. When solving for the T -lookahead policy ((4) with T = 100), after 24 hours, Gurobi 9.1 obtains an objective value of 491.3 (red solid line) with an upper bound 555.3 (red dotted line) and an absolute optimality gap 64.0 (13.0%). The true cumulative expected reward for T -lookahead policy for this problem lies in between the solid and dotted red lines. Figure 4b shows the log-log plot of the w-step lookahead regret of w-lookahead EEP (averaged over 5 random runs) under diﬀerent T .

G Additional Experimental Details and Results

In this appendix, we present additional experimental details and results.

w-lookahead Performance When evaluating the performance of w-lookahead policies, in addition to the case where T = 30 (Figure 3a), we have also run the experiments with T = 100 (Figure 4a). When solving for the 100-lookahead policy, we have increased the number of threads to 50 to solve for (4) and stopped the program at a time limit of 24 hours. In such settings, we obtain an upper bound on the absolute optimality gap of 64.0 (percentage optimality gap of 13.0%). When solved for w-lookahead policies with w in between 1 and 15 using 10 threads, Gurobi ends up solving (5) within 40s for all diﬀerent w values. Thus, despite using signiﬁcantly lower computational time, w-lookahead policies achieve a similar cumulative expected reward to the T -lookahead policies (see Figures 3a and 4a).

EEP Performance Figure 3b is the log-log plot of the w-step lookahead regret of w-lookahead EEP against the horizon T when T = 60, 80, 100, 150, 200, 300, 400 (averaged over 20 random runs) and Figure 4b is the log-log plot when T = 60, 80, 100, 150, 200, 300, 400, 600, 800 (averaged over 5 random runs), under the experimental setup provided in § 7.

43

