Proceedings of Machine Learning Research vol 75:1–29, 2018

31st Annual Conference on Learning Theory

arXiv:1801.03265v3 [cs.LG] 7 Jun 2018

More Adaptive Algorithms for Adversarial Bandits

Chen-Yu Wei University of Southern California
Haipeng Luo University of Southern California

CHENYU.WEI@USC.EDU HAIPENGL@USC.EDU

Editors: Sebastien Bubeck, Vianney Perchet and Philippe Rigollet

Abstract
We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the ﬁrst-order path-length of only the best arm; 3) a regret bound depending on the sum of the ﬁrst-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss and logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a ﬁxed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free.
The main idea of our algorithm is to apply the optimism and adaptivity techniques to the wellknown Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule. Keywords: multi-armed bandit, semi-bandit, adaptive regret bounds, optimistic online mirror descent, increasing learning rate
1. Introduction
The adversarial Multi-Armed Bandits (MAB) problem (Auer et al., 2002) is a classic online learning problem with partial information feedback. In this problem, at each round the learner selects one of the K arms while simultaneously the adversary decides the loss of each arm, then the learner suffers and observes (only) the loss of the picked arm. The goal of the learner is to minimize the regret, that iasl,gothreithdmiffe(Areunecreebt eatlw.,e2e0n02h)eractohtiaelvelossas raengdretthebotuontadl olofsosrodferthO˜e(b√esTt Kﬁx)eadftaerrmT. rTohuendcsla,1sswichiEcxhpi3s worst-case optimal up to logarithmic factors.
There are several existing works on deriving more adaptive bandit algorithms, replacing the dependence on T in the regret bound by some data-dependent quantity that is O(T ) in the worst-case but could be potentially much smaller in benign environments. Examples of such data-dependent quantities include the loss of the best arm (Allenberg et al., 2006; Foster et al., 2016) or the empirical variance of all arms (Hazan and Kale, 2011a; Bubeck et al., 2017). Extensions to more general settings such as semi-bandit, two-point bandit, and graph bandit have also been studied (Neu,
1. Throughout the paper we use the notation O˜(·) to suppress factors that are poly-logarithmic in T and K.
c 2018 C.-Y. Wei & H. Luo.

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

2015; Chiang et al., 2013; Lykouris et al., 2017). These adaptive algorithms not only enjoy better performance guarantees, but also have important applications for other areas such as game theory (Foster et al., 2016).
In this work, we propose a novel and generic bandit algorithm in the more general semi-bandit setting (formally deﬁned in Section 2). By instantiating this generic algorithm differently, we obtain various adaptive algorithms with new data-dependent expected regret bounds that improve previous work. When speciﬁed to the MAB setting with ℓt,i ∈ [−1, 1] denoting the loss of arm i at time t (and ℓ0,i 0), these bounds replace the dependence on T by (also see Table 1 for a summary):

•

Tt=1(ℓt,i⋆

−

1 T

T s=1

ℓs,i⋆ )2,

that

is,

the

(unnormalized)

variance

of

the

best

arm

i⋆.

Similar

existing bounds of (Hazan and Kale, 2011a,b; Bubeck et al., 2017) replace T by the average

of the variances of all arms. In general these two are incomparable. However, note that

the variance of the best arm is always bounded by K times the average variance, while it is

possible that the latter is of order Θ(T ) and the former is only O(1). (Section 3.1)

•K

T t=1

|ℓt,i⋆

−

ℓt−1,i⋆ |,

that

is,

(K

times)

the

ﬁrst-order

path-length

of

the

best

arm.

(Sec-

tion 3.2)

•

K i=1

T t=1

|ℓt,i

−

ℓt−1,i|,

that

is,

the

sum

of

the

ﬁrst-order

path-lengths

of

all

arms.

Impor-

tantly, there is also an additional negative term in the regret similar to the one of (Syrgkanis et al.,

2015)

for

the

full

information

setting.

This

implies

a

fast

convergence

rate

of

order

1/T

3 4

for

several game playing settings with bandit feedback. (Sections 4.1)

• A new quantity in terms of some second-order excess loss (see Eq. (9) for the exact form).

While the bound is not easy to interpret on it own, it in fact automatically and simultane-

ously implies the so-called “small-loss” bound O˜

K

T t=1

ℓt,i⋆

,2 and logarithmic regret

O( K ∆ln T ) if there is an arm whose expected loss is always smaller than those of other arms by a ﬁxed gap ∆ (e.g. the classic i.i.d. MAB setting (Lai and Robbins, 1985)). (Section 4.2)

These bounds are incomparable in general. All of them have known counterparts in the full information setting (see for example (Steinhardt and Liang, 2014) and (De Rooij et al., 2014)), but are novel in the bandit setting to the best of our knowledge. Note that for the ﬁrst two results that depend on some quantities of only the best arm, we require tuning a learning rate parameter in terms of these (unknown) quantities. Obtaining the same results with parameter-free algorithms remains open, even for the full information setting. However, for the other results, we indeed provide parameter-free algorithms based on a variant of the doubling trick.
Our general algorithm falls into the Online Mirror Descent (OMD) framework (see for example (Hazan et al., 2016)) with the “log-barrier” as the regularizer, originally proposed in (Foster et al., 2016). However, to obtain our results, two extra crucial ingredients are needed:

• First, we adopt the ideas of optimism and adaptivity from (Steinhardt and Liang, 2014), which roughly speaking amounts to incorporating a correction term as well as an optimistic predic-
tion into the loss vectors. In (Steinhardt and Liang, 2014), this technique was developed in the Follow-the-Regularized-Leader (FTRL) framework,3 but it is in fact crucial here to re-derive

2. Assuming that losses are non-negative in this case as it is common for small-loss bounds. 3. Although it was confusingly referred as OMD in (Steinhardt and Liang, 2014).

2

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

it in the OMD framework (due to the next ingredient). The challenges here are to come up with the right correction terms and optimistic predictions.

• Second, we apply an individual and increasing learning rate schedule for one of the pathlength results. Such increasing learning rate schedule was originally proposed in (Bubeck et al., 2016) and also recently used in (Agarwal et al., 2017), but for different purposes.

Although most algorithmic techniques we use in this work have been studied before, combining all of them, in the general semi-bandit setting, requires novel and non-trivial analysis. The use of log-barrier in the semi-bandit setting is also new as far as we know.

Related work. There is a rich literature in deriving adaptive algorithms and regret bounds for on-

line learning with full information feedback (see recent work (Luo and Schapire, 2015; Koolen and Van Erven,

2015; van Erven and Koolen, 2016; Orabona and Pa´l, 2016; Cutkosky and Boahen, 2017) and refer-

ences therein), as well as the stochastic bandit setting (such as (Garivier and Cappe´, 2011; Lattimore,

2015; Degenne and Perchet, 2016)). Similar results for the adversarial bandit setting, however,

are relatively sparse and have been mentioned above. While obtaining regret bounds that de-

pend on the quality of the best action is common in the full information setting, it is in fact much

more challenging in the bandit setting, and the only existing result of this kind is the “small-loss”

bound (Allenberg et al., 2006; Foster et al., 2016). We hope that our work opens up more possibili-

ties in obtaining these results, despite some recent negative results discovered by Gerchinovitz and Lattimore

(2016).

Chiang et al. (2013) proposed bandit algorithms with second-order path-length bounds, but their

work requires stronger two-point feedback. The implication of path-length regret bounds on faster

convergence rate for computing equilibriums was studied in (Syrgkanis et al., 2015). Other exam-

ples of adaptive online learning leading to faster convergence in game theory include (Rakhlin and Sridharan,

2013b; Daskalakis et al., 2015; Foster et al., 2016).

There e√xist several bandit algorithms that achieve almost optimal regret in both the adversarial

setting (O( T K)) and the i.i.d. setting (O(

i:∆i=0

ln T ∆

)

where

∆i

is

the

gap

between

the

expected

i

loss of arm i and the one of the optimal arm) (Bubeck and Slivkins, 2012; Seldin and Slivkins, 2014;

Auer and Chiang, 2016; Seldin and Lugosi, 2017). Our results in Section 4.2 have slightly weaker

guarantee for the i.i.d. setting (at most K times worse speciﬁcally) since it essentially replaces all

∆i by mini:∆i=0 ∆i. On the other hand, however, our results have several advantages compared

to previous work. First, our guarantee for the adversarial setting is stronger since it replaces the

dependence on T by the loss of the best arm. Second, our logarithmic regret result applies to not just

the simple i.i.d. setting, but the more general setting mentioned above where neither independence

nor identical distributions is required. Our dependence on ln T is also better than previous works,

resolving an open problem raised by Seldin and Lugosi (2017). Finally, our algorithm and analysis

are also arguably much simpler, without performing any stationarity detection or gap estimation.

Indeed, the result is in some sense algorithm-independent and solely through a new adaptive regret

bound Eq. (9), similar to the results in the full-information setting such as (Gaillard et al., 2014).

Using a self-concordant barrier as regularizer was proposed in the seminal work of (Abernethy et al.,

2008) for general linear bandit problems. The log-barrier is technically not a barrier for the decision

set of the semi-bandit problem, but still it exhibits many similar properties as shown in our proofs.

Optimistic FTRL/OMD was developed in (Chiang et al., 2012; Rakhlin and Sridharan, 2013a). As

pointed out in (Steinhardt and Liang, 2014), incorporating correction terms in the loss vectors can

3

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

also be viewed as using adaptive regularizers, which was studied in several previous works, mostly for the full information setting (see (McMahan, 2017) for a survey).

2. Problem Setup and Algorithm Overview

We consider the combinatorial bandit problem with semi-bandit feedback, which subsumes the
classic multi-armed bandit problem. The learning process proceeds for T rounds. In each round,
the learner selects a subset of arms, denoted by a binary vector bt from a predeﬁned action set X ⊆ {0, 1}K , and suffers loss b⊤t ℓt, where ℓt ∈ [−1, 1]K is a loss vector decided by an adversary. The feedback received by the learner is the vector (bt,1ℓt,1, . . . , bt,K ℓt,K ), or in other words, the loss
of each chosen arm. For simplicity, we assume that the adversary is oblivious and the loss vectors
ℓ1, . . . , ℓT are decided ahead of time independent of the learner’s actions.
The learner’s goal is to minimize the regret, which is the gap between her accumulated loss and that of the best ﬁxed action b∗ ∈ X . Formally the regret is deﬁned as

RegT

T

T

b⊤t ℓt − b∗⊤ℓt, where b∗

t=1

t=1

T
min b⊤ℓt.
b∈X t=1

In the special case of multi-armed bandit, the action set X is {e1, e2, . . . , eK } where ei denotes

the i-th standard basis vector. In other words, in each round the learner picks one arm it ∈ [K]

{1, 2, . . . , K} (corresponding to bt = eit), and receives the loss ℓt,it. We denote the best arm by

i∗

mini∈[K ]

T t=1

ℓt,i.

Notation. For a convex function ψ deﬁned on a convex set Ω, the Bregman divergence of two

points u, v ∈ Ω with respect to ψ is deﬁned as Dψ(u, v) ψ(u)−ψ(v)− ∇ψ(v), u − v . The logbarrier used in this work is of the form ψ(u) = Ki=1 η1i ln u1i for some learning rates η1, . . . , ηK ≥

0 and u ∈ conv(X ), the convex hull of X . With h(y) y − 1 − ln y, the Bregman divergence with

respect to the log-barrier is: Dψ(u, v) = Ki=1 η1i ln uvii + uiv−ivi = Ki=1 η1i h uvii .

The all-zero and all-one vector are denoted by 0 and 1 respectively. ∆K represents the (K − 1)-

dimensional simplex. For a binary vector b we write i ∈ b if bi = 1. Denote by K0 = maxb∈X b 0

the maximum number of arms an action in X can pick. Note that for MAB, K0 is simply 1.

We deﬁne ℓ0 = 0 for notational convenience. At round t, for an arm i we denote its accumulated

loss by Lt,i

t s=1

ℓs,i,

its

average

loss

by

µt,i

1t Lt,i, its (unnormalized) variance by Qt,i

ts=1(ℓs,i − µt,i)2, and its ﬁrst-order path-length by Vt,i

t s=1

|ℓs,i

−

ℓs−1,i|.

For MAB, we

deﬁne αi(t) to be the most recent time when arm i is picked prior to round t , that is, αi(t) =

max{s < t : is = i} (or 0 if the set is empty).

2.1. Algorithm Overview
As mentioned our algorithm falls into the OMD framework that operates on the set Ω = conv(X ). The vanilla OMD formula for the bandit setting is wt = argminw∈Ω{ w, ℓˆt−1 + Dψ(w, wt−1)} for some regularizer ψ and some (unbiased) estimator ℓˆt−1 of the true loss ℓt−1. The learner then picks an action bt randomly such that E[bt] = wt, and constructs the next loss estimator ℓˆt based on the bandit feedback. Our algorithm, however, requires several extra ingredients. The generic update

4

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Algorithm 1 Barrier-Regularized with Optimism and ADaptivity Online Mirror Descent (BROAD-OMD)

Deﬁne: Ω = conv(X ), ψt(w) = Ki=1 η1t,i ln w1i . Initialize: w1′ = argminw∈Ω ψ1(w).

for t = 1, 2, . . . , T do

wt = argminw∈Ω w, mt + Dψt (w, wt′ ) .

Draw bt ∼ wt, suffer loss b⊤t ℓt, and observe {bt,iℓt,i}Ki=1.

Construct ℓˆt as an unbiased estimator of ℓt.

6ηt,iwt,i(ℓˆt,i − mt,i)2, (Option I) Let at,i =

0.

(Option II)

wt′+1 = argminw∈Ω end

w, ℓˆt + at + Dψt (w, wt′ ) .

Table 1: Different conﬁgurations of BROAD-OMD and regret bounds for MAB. See Section 2 and the corresponding sections for the meaning of notation. For the last two rows, to obtain parameter-free algorithms one needs to apply a doubling trick to decrease the learning rate.

Sec. Option

3.1

I

3.2

I

4.1 II

4.2 II

mt,i µ˜t−1,i ℓαi (t),i ℓαi (t),i ℓt,it

ℓˆt,i
(ℓt,i−mwt,ti,)i½{it=i} + mt,i (ℓt,i−mwt¯,ti,)i½{it=i} + mt,i (ℓt,i−mwt,ti,)i½{it=i} + mt,i
ℓt,i½{it=i}
wt,i

ηt,i ﬁxed increasing ﬁxed ﬁxed

E[RegT ] in O˜

K QT ,i∗

K VT,i∗

K

K i=1

VT

,i

min{

K LT ,i∗ ,

K ∆

}

rule is

wt = argmin w, mt + Dψt(w, wt′ ) ,

(1)

w∈Ω

wt′+1 = argmin w, ℓˆt + at + Dψt (w, wt′ ) .

(2)

w∈Ω

Here, we still play randomly according to wt, which is now updated to minimize its loss with respect to mt ∈ [−1, 1]K , an optimistic prediction of the true loss vector ℓt, penalized by a Bregman divergence term associated with a time-varying regularizer ψt. In addition, we maintain a sequence of auxiliary points wt′ that is updated using the loss estimator ℓˆt and an extra correction term at.
When at = 0, this is studied in (Rakhlin and Sridharan, 2013a) under the name optimistic OMD. When at = 0, the closest algorithm to this variant of OMD is its FTRL version studied by Steinhardt and Liang (2014). However, while ψt is ﬁxed for all t in (Steinhardt and Liang, 2014),4 some of our results crucially rely on using time-varying ψt (which corresponds to time-
varying learning rate) and also the OMD update form instead of FTRL.
It is well known that the classic Exp3 algorithm falls into this framework with mt = at = 0 and ψt being the (negative) entropy. To obtain our results, ﬁrst, it is crucial to use the log-barrier

4. Steinhardt and Liang (2014) also uses the notation ψt, but it corresponds to putting at into a ﬁxed regularizer.

5

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

as the regularizer instead, that is, ψt(w) = Ki=1 η1t,i ln w1i for some individual and time-varying learning rates ηt,i. Second, we focus on two options of at. For results that depend on some quantity of only the best arm, we use a sophisticated choice of at that we explain in details in Section 3. For the other results we simply set at = 0. With the choices of mt, ℓˆt, and ηt open, we present this generic framework in Algorithm 1 and name it BROAD-OMD (short for Barrier-Regularized with
Optimism and ADaptivity Online Mirror Descent).
In Section 3 and 4 respectively, we prove general regret bounds for BROAD-OMD with Option
I and Option II, followed by speciﬁc applications in the MAB setting achieved via speciﬁc choices of mt, ℓˆt, and ηt. The results and the corresponding conﬁgurations of the algorithm are summarized in Table 1.
Computational efﬁciency. The sampling step bt ∼ wt can be done efﬁciently as long as Ω can be described by a polynomial number of constraints. The optimization problems in the update rules of wt and wt′ are convex and can be solved by general optimization methods. For many special cases, however, these two computational bottlenecks have simple solutions. Take MAB as an example, wt directly speciﬁes the probability of picking each arm, and the optimization problems can be solved
via a simple binary search (Agarwal et al., 2017).

3. BROAD-OMD with Option I
In this section we focus on BROAD-OMD with Option I. We ﬁrst show a general lemma that update rules (1) and (2) guarantee, no matter what regularizer ψt is used and what at, mt, and ℓˆt are.

Lemma 1 For the update rules (1) and (2), if the following condition holds:

wt − wt′+1, ℓˆt − mt + at ≤ wt, at ,

(3)

then for all u ∈ Ω, we have

wt − u, ℓˆt ≤ Dψt (u, wt′ ) − Dψt (u, wt′+1) + u, at − At,

(4)

where At Dψt (wt′+1, wt) + Dψt (wt, wt′ ) ≥ 0.

The important part of bound (4) is the term u, at , which allows us to derive regret bounds
that depend on only the comparator u. The key is now how to conﬁgure the algorithm such that
condition (3) holds, while leading to a reasonable bound (4) at the same time.
In the work of (Steinhardt and Liang, 2014) for full-information problems, at can be deﬁned as at,i = ηt,i(ℓt,i − mt,i)2, which sufﬁces to derive many interesting results. However, in the bandit setting this is not applicable since ℓt is unknown. The natural ﬁrst attempt is to replace ℓt by ℓˆt, but one would quickly realize the common issue in the bandit literature: ℓˆt,i is often constructed via inverse propensity weighting, and thus (ℓˆt,i − mt,i)2 can be of order 1/wt2,i, which is too large.
Based on this observation, our choice for at is at,i = 6ηt,iwt,i(ℓˆt,i − mt,i)2 (the constant 6 is
merely for technical reasons). The extra term wt,i can then cancel the aforementioned large term 1/wt2,i in expectation, similar to the classic trick done in the analysis of Exp3 (Auer et al., 2002).
Note that with a smaller at, condition (3) becomes more stringent. The entropy regularizer used
in (Steinhardt and Liang, 2014) no longer sufﬁces to maintain such a condition. Instead, it turns out
that the log-barrier regularizer used by BROAD-OMD addresses the issue, as shown below.

6

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Theorem 2 If the following three conditions hold for all t, i: (i) ηt,i ≤ 1162 , (ii) wt,i|ℓˆt,i − mt,i| ≤

3, (iii)

K i=1

ηt,iwt2,i(ℓˆt,i

−

mt,i)2

≤

118 , then BROAD-OMD with at,i

=

6ηt,iwt,i(ℓˆt,i − mt,i)2

guarantees condition (3). Moreover, it guarantees for any u ∈ Ω (recall h(y) = y − 1 − ln y ≥ 0),

T w − u, ℓˆ ≤ K  ln wu1′i,i + T



1 − 1 h ui

T
+ u, a . (5)

t t=1

t i=1  η1,i

t=1 ηt+1,i ηt,i

wt′+1,i  t=1

t

The three conditions of the theorem are usually trivially satisﬁed as we will show. Note that h(·) is always non-negative. Therefore, if the sequence {ηt,i}Tt=+11 is non-decreasing for all i,5 the

term

T

1

t=1 η

−

1 η

h

ui w′

in bound (5) is non-positive. For some results we can simply

t+1,i

t,i

t+1,i

discard this term, while for others, this term becomes critical. On the other hand, the term ln wu1′i,i appears to be inﬁnity if we want to compare with the best ﬁxed action (where ui = 0 for some i).

However, this can be simply resolved by comparing with some close neighbor of the best action in

Ω instead, similar to (Foster et al., 2016; Agarwal et al., 2017). One can now derive different results using Theorem 2 with speciﬁc choices of ℓˆt and mt. As

an example, we state the following corollary by using a variance-reduced importance-weighted estimator ℓˆt as in (Rakhlin and Sridharan, 2013a).

Corollary 3 BROAD-OMD with at,i = 6ηt,iwt,i(ℓˆt,i − mt,i)2, any mt,i ∈ [−1, 1], ℓˆt,i = (ℓt,i−mwt,ti,)i½{i∈bt} + mt,i, and ηt,i = η ≤ 1621K0 enjoys the following regret bound:

E [RegT ] = E

T
bt − b∗, ℓt
t=1

≤ K ln T + 6ηE η

T
(ℓt,i − mt,i)2
t=1 i:i∈b∗

+ O(K0).

One can see that the expected regret in Corollary 3 only depends on the squared estimation error of mt for the actions that b∗ chooses! This is exactly the counterpart of results in (Steinhardt and Liang, 2014), but for the more challenging combinatorial semi-bandit problem. Note that our dependence on K0 is also optimal (Audibert et al., 2013).
In the following subsections, we invoke Theorem 2 with different choices of ℓˆt and mt to obtain various more concrete adaptive bounds. For simplicity, we state these results only in the MAB
setting, but they can be straightforwardly generalized to the semi-bandit case.

3.1. Variance Bound
Our ﬁrst application of BROAD-OMD is an adaptive bound that depends on the variance of the best arm, that is, a bound of order O˜ KQT,i∗ = O˜ K Tt=1(ℓt,i∗ − µT,i∗)2 . According to Corollary 3, if we were able to use mt = µT , with a best-tuned η the bound is obtained immediately. The issue is of course that µT is unknown ahead of time. In fact, even setting mt = µt−1 is infeasible due to the bandit feedback.
Fortunately this issue was already solved by Hazan and Kale (2011a) via the “reservoir sampling” technique. The high level idea is that one can spend a small portion of time on estimating
5. One might notice that ηT +1,i is not deﬁned here. Indeed this term is artiﬁcially added only to make the analysis of Section 3.2 more concise, and ηT +1,i can be any positive number. In Algorithm 2 we give it a concrete deﬁnition.

7

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

µt on the ﬂy. More precisely, by performing uniform exploration with probability min

1,

MK t

at time t for some parameter M , one can obtain an estimator µ˜t of µt such that E[µ˜t] = µt and

Var[µ˜t,i]

≤

Qt,i Mt

(see (Hazan and Kale, 2011a) for details).

Then we can simply pick mt

=

µ˜t−1

and prove the following result.

Theorem 4 BROAD-OMD with reservoir sampling (Hazan and Kale, 2011a), at,i = 6ηt,iwt,i(ℓˆt,i− mt,i)2, mt,i = µ˜t−1,i, ℓˆt,i = (ℓt,i−mwt,ti,)i½{it=i} + mt,i, and ηt,i = η ≤ 1612 guarantees

E [RegT ] = O K lηn T + ηQT,i∗ + K(ln T )2 .

With the optimal tuning of η, the regret is thus of order O˜ KQT,i∗ + K .

3.2. Path-length Bound

Our second application is to obtain path-length bounds. The counterpart in the full-information set-
ting is a bound in terms of the second-order path-length Tt=1(ℓt,i∗ −ℓt−1,i∗)2 (Steinhardt and Liang, 2014). Again, in light of Corollary 3, if we were able to pick mt = ℓt−1 the problem would be

solved. The difﬁculty is again that ℓt−1 is not fully observable.

While it is still not clear how to achieve such a second-order path-length bound or whether

it is possible at all, we propose a way to obtain a slightly weaker ﬁrst-order path-length bound

O˜ K VT,i∗ = O˜ K

T

√ |ℓt,i∗ − ℓt−1,i∗ | . Note that in the worst case this is K times

t=1√

worse than the optimal regret O˜( T K).

The idea is to set mt,i to be the most recent observed loss of arm i, that is, mt,i = ℓαi(t),i, where αi(t) is deﬁned in Section 2. While the estimation error (ℓt,i − ℓαi(t),i)2 could be much larger than (ℓt,i − ℓt−1,i)2, the quantity we aim for, observe that if t − αi(t) is large, it means that arm i has bad

performance before time t so that the learner seldom draws arm i. In this case, the learner might

have accumulated negative regret with respect to arm i, which can potentially be used to compensate

the large estimation error.

To formalize this intuition, we go back to the bound in Theorem 2 and examine the key term Tt=1 u, at after plugging in u = ei for some arm i, mt,i = ℓαi(t),i, and ℓˆt,i = (ℓt,i−mwt,ti,)i½{it=i} + mt,i. We assume ηt,i = η for simplicity and also use the fact wt,i|ℓˆt,i − mt,i| ≤ 2. We then have

T
u, at
t=1

T
= 6η w (ℓˆ − ℓ

T
)2 ≤ 12η |ℓˆ − ℓ

| = 12η

|ℓt,i − ℓαi(t),i|

t,i t,i t=1

αi(t),i

t,i t=1

αi(t),i

t:it=i wt,i

t s=α

(t)+1 |ℓs,i

−

ℓs−1,i|

1

≤ 12η

i

≤ 12η max

VT ,i .

(6)

t:it=i

wt,i

t∈[T ] wt,i

Therefore, the term

T t=1

u, at

is close to the ﬁrst-order path-length but with an extra factor

maxt∈[T ] w1t,i . To cancel this potentially large factor, we adopt the increasing learning rate schedule

recently used in (Agarwal et al., 2017).

The idea is that the term h

ui w′

in Eq. (5) is close to

t+1,i

wt+11,i if ui is close to 1. If we increase the learning rate whenever we encounter a large wt+11,i ,

8

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Algorithm 2 BROAD-OMD+ (specialized for MAB)

Deﬁne:

κ

=

e

l

1 nT

,

ψt(w)

=

Ki=1 ηt1,i ln w1i .

Initialize: w1′ ,i = 1/K, ρ1,i = 2K for all i ∈ [K].

for t = 1, 2, . . . , T do

wt = argminw∈∆K w, mt + Dψt (w, wt′ ) .

w¯t

=

(1

−

1 T

)wt

+

1 KT

1.

Draw it ∼ w¯t, suffer loss ℓt,it , and let ℓˆt,i = (ℓt,i−mwt¯,ti,)i½{it=i} + mt,i.

Let at,i = 6ηt,iwt,i(ℓˆt,i − mt,i)2. wt′+1 = argminw∈∆K w, ℓˆt + at + Dψt (w, wt′ ) . for i = 1, . . . , K do

if w¯1t,i > ρt,i then ρt+1,i = w¯2t,i , ηt+1,i = κηt,i.

else ρt+1,i = ρt,i, ηt+1,i = ηt,i.

end

end

then

1 η

−

1 η

h

ui w′

becomes

a

large

negative

term

in

terms

of

−1 w

, which exactly

t+1,i

t,i

t+1,i

t+1,i

compensates the term

T t=1

u, at

.

To avoid the learning rates increased by too much, similarly to (Agarwal et al., 2017) we use

some individual threshold (ρt,i) to decide when to increase the learning rate and update these thresh-

olds in some doubling manner. Also, we mix wt with a small amount of uniform exploration to

further ensure that it cannot be too small. The ﬁnal algorithm, call BROAD-OMD+, is presented in

Algorithm 2 (only for the MAB setting for simplicity). We prove the following theorem.

Theorem 5

BROAD-OMD+

with

mt,i

=

ℓαi(t),i

and

η1,i

=

η

≤

1 810

guarantees

2K ln T

−1

E [RegT ] ≤ η + E[ρT +1,i∗] 40η ln T + 90ηVT,i∗ + O (1)

when T ≥ 3. Picking η = min 8110 , 60√VT1,i∗ ln T E [RegT ] = O˜ K VT,i∗ + K .

so that the second term is non-positive leads to

4. BROAD-OMD with Option II
In this section, we move on to discuss BROAD-OMD with Option II, that is, at = 0. We also ﬁx ηt,i = η, although in the doubling trick discussed later, different values of η will be used for different runs of BROAD-OMD. Again we start with a general lemma that holds no matter what regularizer ψt is used and what mt and ℓˆt are.
Lemma 6 For the update rules (1) and (2) with at = 0, we have for all u ∈ Ω,
wt − u, ℓˆt ≤ Dψt (u, wt′ ) − Dψt (u, wt′+1) + wt − wt′+1, ℓˆt − mt − At,
where At Dψt (wt′+1, wt) + Dψt (wt, wt′ ) ≥ 0.

9

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

The proof is standard as in typical OMD analysis. The next theorem then shows how the term wt − wt′+1, ℓˆt − mt is further bounded when ψt is the log-barrier as in BROAD-OMD.

Theorem 7 If the following three conditions hold for all t, i: (i) η ≤ 1162 , (ii) wt,i|ℓˆt,i − mt,i| ≤ 3,

(iii) η

K i=1

wt2,i(ℓˆt,i

−

mt,i)2

≤

1 18

(same

as

those

in

Theorem

2),

then

BROAD-OMD

with

at

=

0

guarantees for any u ∈ Ω,

T

K ln w1′ ,i

TK

T

wt − u, ℓˆt ≤

ui + 3η

wt2,i(ℓˆt,i − mt,i)2 − At.

(7)

t=1 i=1 η t=1 i=1 t=1

For MAB, the last term can further be lower bounded by

T t=1

At

≥

1 48η

T t=2

K i=1

. (wt,i−wt−1,i)2
w2

t−1,i

In bound

(7), the ﬁrst term can again be bounded

by

K ln T η

via picking

an appropriate

u.

The

last negative term is useful when we use the algorithm to play games, which is discussed in Sec-

tion 4.1.1. The second term is the key term, which, compared to the key term

T t=1

u, at

in Eq. (5)

for BROAD-OMD with Option I, has an extra wt,i and is in terms of all arms instead of the arms that u picks. As a comparison to Corollary 3, if we pick ℓˆt,i = (ℓt,i−mwt,ti,)i½{i∈bt} + mt,i, we obtain an ex-

pected regret bound in terms of E

T t=1

i∈bt (ℓt,i − mt,i)2 = E

T t=1

K i=1

wt,i(ℓt,i

−

mt,i)2

,

which is not as easy to interpret as the bound in Corollary 3. However, in the following subsections

we will discuss in details how to apply bound (7) to obtain more concrete results.

Before that, we point out that since the bound is now in terms of all arms, we can in fact apply a

doubling trick to make the algorithm parameter-free! The idea is that as long as the observable term

3η

t s=1

K i=1

ws2,i(ℓˆs,i

−

ms,i)2

becomes larger than

K ln T η

at some round t, we half the learning

rate η and restart the algorithm. This avoids the need for optimal tuning done in Section 3. We

formally present the algorithm in Algorithm 3 (in Appendix G) and show its regret bound below.

Theorem 8 If conditions (ii) and (iii) in Theorem 7 hold, then Algorithm 3 guarantees

 E[RegT ] = O 

(K ln T )E

TK
wt2,i(ℓˆt,i − mt,i)2
t=1 i=1

 + K0K ln T  .

In the following subsections, we instantiate Theorem 7 or 8 with different mt and ℓˆt. Again, for simplicity we only focus on the MAB setting.

4.1. Another Path-length Bound

If we conﬁgure BROAD-OMD with Option II in the same way as in Section 3.2, that is, mt,i = ℓαi(t),i and ℓˆt,i = (ℓt,i−mwt,ti,)i½{it=i} + mt,i. Then the key term in Eq. (7) can be bounded as follows:

TK

TK

K

wt2,i(ℓˆt,i − mt,i)2 =

(ℓt,i − ℓαi(t),i)2½{it = i} =

(ℓt,i − ℓαi(t),i)2

t=1 i=1

t=1 i=1

i=1 t:it=i

K

K

t

K

≤2

|ℓt,i − ℓαi(t),i| ≤ 2

|ℓs,i − ℓs−1,i| ≤ 2 VT,i.

(8)

i=1 t:it=i

i=1 t:it=i s=αi(t)+1

i=1

10

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Unlike Eq. (6), this is bounded even without the help of negative regret, but the price is that now the regret depends on the sum of all arms’ path-length. With this calculation, we obtain the following corollary.

Corollary 9 BROAD-OMD with at,i = 0, mt,i = ℓαi(t),i, ℓˆt,i = (ℓt,i−mwt,ti,)i½{it=i} + mt,i, and

ηt,i

=

η

≤

1 162

guarantees

E [RegT ] ≤ O

K ln T η

K
+ 6η VT,i − E
i=1

T K (wt,i − wt−1,i)2 t=2 i=1 48ηwt2−1,i

≤O

K ln T

K

η + η VT,i .

i=1

Using the doubling trick (Algorithm 3), we achieve expected regret O˜

K

K i=1

VT ,i

+

K

.

√ T√his new path-length bound could be K times better than the one in Section 3.2 in some cases, but T times larger in others. The extra advantage, however, is the negative term in the regret,6 explicitly spelled out in Corollary 9, which we discuss next.

4.1.1. FAST CONVERGENCE IN BANDIT GAMES

It is well-known that in a repeated two-player zero-sum game, if both players play according to some

no-regret algorithms, then their average strategies converge to a Nash equilibrium (Freund and Schapire,

1999). Similar results for general multi-player games have also been discovered. The convergence

rate of these results is governed by the regret bounds of the learning algorithms, and several recent

works (such as those mentioned in the√introduction) have developed adaptive algorithms with regret much smaller than the worst case O( T ) b√y exploiting the special structure in this setup, which translates to convergence rates faster than 1/ T in computing equilibriums.

One way to obtain such fast rates is exactly via path-length regret bounds as shown in (Rakhlin and Sridharan,

2013b; Syrgkanis et al., 2015). In these works, the convergence rate 1/T is achieved when the play-

ers have full-information feedback. We generalize their results to the√bandit setting, and show

that convergence

rate

of

1/T

3 4

can be obtained.

Though faster than 1/

T , it is still slower than

1/T compared to the full-information setting, which is due to the fact that in bandit we only have

ﬁrst-order instead of second-order path-length bound. We detail the proofs and the remaining open

problems in Appendix I.

4.2. Adapting to Stochastic Bandits
Our last application is to obtain an algorithm that simultaneously enjoys near optimal regret in both adversarial and stochastic setting. Speciﬁcally, the stochastic setting we consider here is as follows: there exists an arm a∗ and some ﬁxed gap ∆ > 0 such that Eℓt [ℓt,i − ℓt,a∗ |ℓ1, . . . , ℓt−1] ≥ ∆ for all i = a∗ and t ∈ [T ]. In other words, arm a∗’s expected loss is always smaller than those of other arms by a ﬁxed amount. The classic i.i.d. MAB (Lai and Robbins, 1985) is clearly a special case of ours. Unlike the i.i.d. setting, however, we require neither independence nor identical distributions.
6. In fact, similar negative term, coming from the term At in Lemma 1, also exists (but is omitted) in the bound of Theorem 5. However, it is not clear to us how to utilize it in the same way as in Section 4.1.1 if we also want to exploit the other negative term coming from increasing learning rates.

11

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Note that a∗ can be different from the empirically best arm i∗ deﬁned in Section 2. The expected regret in this setting is still with respect to i∗ and further takes into consideration the randomness

over losses. In other words, we care about Eℓ1,...,ℓT [Ei1,...,iT [RegT ]], abbreviated as E[RegT ] still. We invoke BROAD-OMD with at = 0, ℓˆt,i = ℓt,i½w{ti,ti=i} being the typical importance-weighted
unbiased estimator, and a somewhat special choice of mt: mt,i = ℓt,it for all i. This choice
of mt is seemingly invalid since it depends on it, which is drawn after we have constructed wt

based on mt itself. However, note that because mt now has identical coordinates, we have wt =

argminw∈∆K

w, mt + Dψt (w, wt′ )

= argminw∈∆K

Dψt

(w

,

w

′ t

)

= wt′, independent of the

actual value of mt. Therefore, the algorithm is still valid and is in fact equivalent to the vanilla log-barrier OMD of (Foster et al., 2016). Also note that we cannot deﬁne ℓˆt as in previous sections

(in terms of mt) since it is not an unbiased estimator of ℓt anymore (due to the randomness of mt).

Although the algorithm is the same, using our analysis framework we actually derive a tighter

bound in terms of the following quantity based on Theorem 7:

T t=1

K i=1

wt2,i(ℓˆt,i

− ℓt,it )2

=

T t=1

Ki=1(ℓt,i½{it = i} − wt,iℓt,it)2. It turns out that based on this quantity alone, one can derive

both a “small-loss” bound for the adversarial setting and a logarithmic bound for the stochastic

setting as shown below. We emphasize that the doubling trick of Algorithm 3 is essential to make

the algorithm parameter-free, which is another key difference from (Foster et al., 2016).

Theorem 10 BROAD-OMD with at = 0, mt,i = ℓt,it , ℓˆt,i = ℓt,i½w{ti,ti=i} , and the doubling trick (Algorithm 3), guarantees





TK

E [RegT ] = O  (K ln T )E

(ℓt,i½{it = i} − wt,iℓt,it )2 + K ln T  .

(9)

t=1 i=1

This bound implies that in the stochastic setting, we have E [RegT ] = O

K ln T ∆

, while in the ad-

versarial setting, we have E [RegT ] = O KLT,i∗ ln T + K ln T assuming non-negative losses.

5. Conclusions and Discussions
In this work we develop and analyze a general bandit algorithm using techniques such as optimistic mirror descent, log-barrier regularizer, increasing learning rate, and so on. We show various applications of this general framework, obtaining several more adaptive algorithms that improve previous works. Future directions include 1) improving the dependence on K for the path-length results; 2) obtaining second-order path-length bounds; 3) generalizing the results to the linear bandit problem.
Acknowledgement. CYW is grateful for the support of NSF Grant #1755781. The authors would like to thank Chi-Jen Lu for posing the problem of bandit path-length, and to thank Chi-Jen Lu and Yi-Te Hong for helpful discussions in this direction.

References
Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm for bandit linear optimization. In Conference on Learning Theory, pages 263–274, 2008.

12

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In Conference on Learning Theory, pages 12–38, 2017.
Chamy Allenberg, Peter Auer, Laszlo Gyorﬁ, and Gyo¨rgy Ottucsa´k. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In Algorithmic Learning Theory, volume 4264, pages 229–243. Springer, 2006.
Jean-Yves Audibert, Se´bastien Bubeck, and Ga´bor Lugosi. Regret in online combinatorial optimization. Mathematics of Operations Research, 39(1):31–45, 2013.
Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In Conference on Learning Theory, pages 116–120, 2016.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002.
Se´bastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial bandits. In Conference on Learning Theory, pages 42–1, 2012.
Se´bastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex optimization. arXiv preprint arXiv:1607.03084, 2016.
Se´bastien Bubeck, Michael B. Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multiarmed bandits. arXiv preprint arXiv:1711.01037, 2017.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.
Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds. In Conference on Learning Theory, pages 210–227, 2013.
Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In Conference on Learning Theory, 2017.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. Games and Economic Behavior, 92:327–348, 2015.
Steven De Rooij, Tim Van Erven, Peter D Gru¨nwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15(1):1281–1316, 2014.
Re´my Degenne and Vianney Perchet. Anytime optimal algorithms in stochastic multi-armed bandits. In International Conference on Machine Learning, pages 1587–1595, 2016.
Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems, pages 4734–4742, 2016.
Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1-2):79–103, 1999.
13

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS
Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In Conference on Learning Theory, pages 176–196, 2014.
Aure´lien Garivier and Olivier Cappe´. The kl-ucb algorithm for bounded stochastic bandits and beyond. In Conference On Learning Theory, pages 359–376, 2011.
Se´bastien Gerchinovitz and Tor Lattimore. Reﬁned lower bounds for adversarial bandits. In Advances in Neural Information Processing Systems, pages 1198–1206, 2016.
Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning Research, 12(Apr):1287–1311, 2011a.
Elad Hazan and Satyen Kale. A simple multi-armed bandit algorithm with optimal variationbounded regret. In Proceedings of the 24th Annual Conference on Learning Theory, pages 817– 820, 2011b.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):157–325, 2016.
Wouter M Koolen and Tim Van Erven. Second-order quantile methods for experts and combinatorial games. In Conference on Learning Theory, pages 1155–1175, 2015.
Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.
Tor Lattimore. Optimally conﬁdent ucb: Improved regret for ﬁnite-armed bandits. arXiv preprint arXiv:1507.07880, 2015.
Haipeng Luo and Robert E Schapire. Achieving all with no parameters: Adanormalhedge. In Conference on Learning Theory, pages 1286–1304, 2015.
Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Small-loss bounds for online learning with partial information. arXiv preprint arXiv:1711.03639, 2017.
H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. Journal of Machine Learning Research, 18(90):1–50, 2017.
Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Conference on Learning Theory, pages 1360–1375, 2015.
Francesco Orabona and Da´vid Pa´l. Coin betting and parameter-free online learning. In Advances in Neural Information Processing Systems, pages 577–585, 2016.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on Learning Theory, pages 993–1019, 2013a.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Advances in Neural Information Processing Systems, pages 3066–3074, 2013b.
Yevgeny Seldin and Ga´bor Lugosi. An improved parametrization and analysis of the exp3++ algorithm for stochastic and adversarial bandits. In Conference on Learning Theory, 2017.
14

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In International Conference on Machine Learning, pages 1287–1295, 2014.
Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In International Conference on Machine Learning, pages 1593–1601, 2014.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems, pages 2989–2997, 2015.
Tim van Erven and Wouter M Koolen. Metagrad: Multiple learning rates in online learning. In Advances in Neural Information Processing Systems, pages 3666–3674, 2016.

Appendix A. Proof of Lemma 1
Proof of Lemma 1. We ﬁrst state a useful property used in typical OMD analysis. Let Ω be a convex compact set in RK, ψ be a convex function on Ω, w′ be an arbitrary point in Ω, and x ∈ RK. If w∗ = argminw∈Ω{ w, x + Dψ(w, w′)}, then for any u ∈ Ω,
w∗ − u, x ≤ Dψ(u, w′) − Dψ(u, w∗) − Dψ(w∗, w′).

This is by the ﬁrst-order optimality condition of w∗ and direct calculations. Applying this to update rule (2) we have

wt′+1 − u, ℓˆt + at ≤ Dψt (u, wt′ ) − Dψt (u, wt′+1) − Dψt (wt′+1, wt′ );

(10)

while applying it to update rule (1) and picking u = wt′+1 we have

wt − wt′+1, mt ≤ Dψt (wt′+1, wt′ ) − Dψt (wt′+1, wt) − Dψt (wt, wt′ ).

(11)

Now we bound the instantaneous regret as follows:

wt − u, ℓˆt

= wt − u, ℓˆt + at − wt, at + u, at

= wt − wt′+1, ℓˆt + at − wt, at + wt′+1 − u, ℓˆt + at + u, at

= wt − wt′+1, ℓˆt + at − mt − wt, at + wt′+1 − u, ℓˆt + at + wt − wt′+1, mt + u, at

≤ Dψt (u, wt′ ) − Dψt (u, wt′+1) − Dψt (wt′+1, wt) − Dψt (wt, wt′ ) + u, at ,

(12)

where last inequality is by the condition wt − wt′+1, ℓˆt + at − mt − wt, at ≤ 0, Eq. (10), and Eq. (11).

15

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Appendix B. Lemmas for Log-barrier OMD
In this section we establish some useful lemmas for update rules (1) and (2) with log-barrier regularizer, which are used in the proofs of other theorems. We start with some deﬁnitions.

Deﬁnition 11 For any h ∈ RK, deﬁne norm h t,w = h⊤∇2ψt(w)h =

K1 i=1 η

h2i w2

and

t,i i

its dual norm

h

∗ t,w

=

h⊤∇−2ψt(w)h =

K i=1

ηt,iwi2h2i .

For some radius r

>

0, deﬁne

ellipsoid Et,w(r) = u ∈ RK : u − w t,w ≤ r .

Lemma 12

If w′

∈

Et,w(1) and ηt,i

≤

1 81

for all i, then wi′

∈

0.9 h t,w ≤ h t,w′ ≤ 1.2 h t,w for any h ∈ RK .

21 wi,

3 2

wi

for all i, and also

Proof w′ ∈ Et,w(1) implies

K 1 (wi′−wi)2

i=1 η

w2

≤ 1.

Thus for every i, we have |wi′w−wi|

≤

t,i

i

i

√ηt,i ≤ 19 , implying wi′ ∈

8 9

wi,

10 9

wi

⊂

12 wi, 32 wi . Therefore,

h t,w′ =

≥ K 1 h2i
i=1 ηt,i wi′2

Ki=1 ηt1,i ( 190hw2i i)2 = 0.9 h t,w. Similarly, we have h t,w′ ≤ 1.2 h t,w.

Lemma 13

Let

wt, wt′+1

follow

(1)

and

(2)

where

ψt

is

the

log-barrier

with

ηt,i

≤

1 81

for

all

i.

If

ℓˆt − mt + at ∗ ≤ 13 , then wt′+1 ∈ Et,wt (1).

t,wt

Proof

Deﬁne Ft(w) =

w, mt

+

Dψt

(w

,

w

′ t

)

and

Ft′+1(w)

=

w, ℓˆt + at

+ Dψt (w, wt′ ). Then

by deﬁnition we have wt = argminw∈Ω Ft(w) and wt′+1 = argminw∈Ω Ft′+1(w). To show wt′+1 ∈

Et,wt(1), it sufﬁces to show that for all u on the boundary of Et,wt(1), Ft′+1(u) ≥ Ft′+1(wt).

Indeed, using Taylor’s theorem, for any u ∈ ∂Et,wt(1), there is an ξ on the line segment between

wt and u such that (let h u − wt)

Ft′+1(u) = Ft′+1(wt) + ∇Ft′+1(wt)⊤h + 21 h⊤∇2Ft′+1(ξ)h

= Ft′+1(wt) + (ℓˆt − mt + at)⊤h + ∇Ft(wt)⊤h + 21 h⊤∇2ψt(ξ)h

≥ Ft′+1(wt) + (ℓˆt − mt + at)⊤h + 12 h 2t,ξ

(by the optimality of wt)

≥ Ft′+1(wt) + (ℓˆt − mt + at)⊤h + 12 × 0.92 h 2t,wt

(by Lemma 12)

≥ Ft′+1(wt) − ℓˆt − mt + at ∗t,wt h t,wt + 13 h 2t,wt

= Ft′+1(wt) − ℓˆt − mt + at ∗t,wt + 31

( h t,wt = 1)

≥ Ft′+1(wt).

(by the assumption)

16

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Lemma 14

Let

wt, wt′+1

follow

(1)

and

(2)

where

ψt

is

the

log-barrier

with

ηt,i

≤

1 81

for

all

i.

If

ℓˆt − mt + at ∗ ≤ 13 , then wt′+1 − wt t,w ≤ 3 ℓˆt − mt + at ∗ .

t,wt

t

t,wt

Proof Deﬁne Ft(w) and Ft′+1(w) to be the same as in Lemma 13. Then we have

Ft′+1(wt) − Ft′+1(wt′+1) = (wt − wt′+1)⊤(ℓˆt − mt + at) + Ft(wt) − Ft(wt′+1)

≤ (wt − wt′+1)⊤(ℓˆt − mt + at)

(optimality of wt)

≤ wt − wt′+1 t,wt ℓˆt − mt + at ∗t,wt . (13)

On the other hand, for some ξ on the line segment between wt and wt′+1, we have by Taylor’s theorem and the optimality of wt′+1,

Ft′+1(wt) − Ft′+1(wt′+1) = ∇Ft′+1(wt′+1)⊤(wt − wt′+1) + 12 (wt − wt′+1)⊤∇2Ft′+1(ξ)(wt − wt′+1) ≥ 12 wt − wt′+1 2t,ξ . (14)
Since the condition in Lemma 13 holds, wt′+1 ∈ Et,wt(1), and thus ξ ∈ Et,wt(1). Using again Lemma 12, we have

12 wt − wt′+1 2t,ξ ≥ 31
Combining (13), (14), and (15), we have wt − wt′+1 which leads to the stated inequality.

wt − wt′+1 2t,wt . t,wt ℓˆt − mt + at

∗ ≥1
t,wt 3

(15) wt − wt′+1 2t,wt ,

Lemma 15 When the three conditions in Theorem 2 hold, we have either at,i = 6ηt,iwt,i(ℓˆt,i − mt,i)2 or at,i = 0.

ℓˆt − mt + at ∗ ≤ 1 for

t,wt

3

Proof For at,i = 6ηt,iwt,i(ℓˆt,i − mt,i)2, we have

ℓˆt − mt + at

∗2

K
= η w2

ℓˆ

−m

+ 6η w (ℓˆ

− m )2 2

t,i t,i t,i

t,i

t,i t,i t,i

t,i

t,wt i=1

K
= ηt,iwt2,i(ℓˆt,i − mt,i)2 + 12ηt2,iwt3,i(ℓˆt,i − mt,i)3 + 36ηt3,iwt4,i(ℓˆt,i − mt,i)4

i=1

K
≤ ηt,iwt2,i(ℓˆt,i − mt,i)2(1 + 36ηt,i + 324ηt2,i)

(condition (ii))

i=1

K
≤ 2 ηt,iwt2,i(ℓˆt,i − mt,i)2

(condition (i))

i=1
≤ 2 × 1 = 1. 18 9

(condition (iii))

17

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

For at,i = 0, we have

ℓˆt − mt + at ∗2 =
t,wt

ℓˆ − m

∗2

K
=η

w2 (ℓˆ

−m

)2 ≤ 1 < 1 .

t

t

t,wt

t,i t,i t,i

t,i

18 9

i=1

(condition (iii))

Lemma 16 If the three conditions in Theorem 2 hold, BROAD-OMD (with either Option I or II) satisﬁes 12 wt,i ≤ wt′+1,i ≤ 23 wt,i.
Proof This is a direct application of Lemmas 15, 13, and 12.

Lemma 17 For the MAB problem, if the three conditions in Theorem 2 hold, BROAD-OMD (with either Option I or II) satisﬁes 12 wt,i ≤ wt′,i ≤ 23 wt,i.

Proof It sufﬁces to prove wt′ ∈ Et,wt(1) by Lemma 12. Since we assume that the three conditions

in Theorem 2 hold and wt ∈ ∆K, we have mt ∗t,wt =

K i=1

ηt,iwt2,im2t,i

≤

1 162

K i=1

wt2,i

≤

1 162

<

13 .

This

implies

wt′

∈

Et,wt (1)

by

a

similar

arguments

as

in

the

proof

of

Lemma

13

(one

only needs to replace Ft′+1(w) there by G(w) Dψt(w, wt′ ) and note that wt′ = argminw∈∆K G(w)).

Appendix C. Proof of Theorem 2 and Corollary 3

Proof of Theorem 2. We ﬁrst prove Eq. (3) holds: by Lemmas 15 and 14, we have

wt − wt′+1, ℓˆt − mt + at

≤ wt − wt′+1 t,wt ℓˆt − mt + at ∗t,wt ≤ 3 ℓˆt − mt + at ∗2
t,wt
K
≤ 3 ηt,iwt2,i(ℓˆt,i − mt,i)2(1 + 36ηt,i + 324ηt2,i)
i=1
K
≤ 6 ηt,iwt2,i(ℓˆt,i − mt,i)2 = wt, at ,
i=1

where the last two inequalities are by the same calculations done in the proof of Lemma 15. Since Eq. (3) holds, using Lemma 1 we have (ignoring non-positive terms −At’s),

T
wt − u, ℓˆt
t=1

T
≤
t=1

Dψt (u, wt′ ) − Dψt (u, wt′+1)

T
+ u, at
t=1

18

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

T

T

≤ Dψ1 (u, w1′ ) +

Dψt+1(u, wt′+1) − Dψt (u, wt′+1) + u, at . (16)

t=1

t=1

In the last inequality, we add a term DψT+1 (u, wT′ +1) ≥ 0 artiﬁcially. As mentioned, ψT +1, deﬁned in terms of ηT +1,i, never appears in the BROAD-OMD algorithm. We can simply pick any ηT +1,i >
0 for all i here. This is just to simplify some analysis later. The ﬁrst term in (16) can be bounded by the optimality of w1′ :

Dψ1 (u, w1′ ) = ψ1(u) − ψ1(w1′ ) − ∇ψ1(w1′ ), u − w1′

K
≤ ψ (u) − ψ (w′ ) =

1 ln w1′ ,i .

1 1 1 i=1 η1,i ui

The second term, by deﬁnition, is

TK

1

1

ui

t=1 i=1 ηt+1,i − ηt,i h wt′+1,i .

Plugging the above two terms into (16) ﬁnishes the proof.

Proof of Corollary 3. We ﬁrst check the three conditions in Theorem 2 under our choice of
ηt,i and ℓˆt,i: ηt,i = η = 1621K0 ≤ 1612 ; wt,i|ℓˆt,i − mt,i| = |ℓt,i − mt,i|½{i ∈ bt} ≤ 2 < 3; Ki=1 ηt,iwt2,i(ℓˆt,i − mt,i)2 = 1621K0 Ki=1(ℓt,i − mt,i)2½{i ∈ bt} ≤ 1642 < 118 . Applying Theo-
rem 2 we then have

T
wt − u, ℓˆt
t=1

≤ K ln wu1′i,i + T u, at . i=1 η t=1

As mentioned, if we let u = b∗, then ln wu1′i,i becomes inﬁnity for those i ∈/ b∗. Instead, we let

u=

1

−

1 T

b∗

+

1 T

w1′

.

With

this

choice

of

u,

we

have

w1′ ,i u

≤

w1′ ,i
1′

= T . Plugging u into the

i

T w1,i

above inequality and rearranging, we get

T

w − b∗, ℓˆ

K ln T T

≤

+

b∗, a

+ B,

(17)

t

t

η

t

t=1

t=1

where B

1 T

T t=1

−b∗

+ w1′ , ℓˆt

+ at

.

Now note that Ebt[at,i] = 6η(ℓt,i − mt,i)2 = O(η) and Ebt[ℓˆt,i] = ℓt,i = O(1) for all i. Thus,

E[B] = E

1 T

T t=1

−b∗

+ w1′ , Ebt [ℓˆt

+ at]

≤E

1 T

T t=1

−b∗ + w1′ 1

Ebt [ℓˆt + at]

=

∞

O(K0). Taking expectation on both sides of (17), we have

E

T

T

b⊤ℓ − b∗⊤ℓ

≤ K ln T + 6ηE

T

K
(ℓ

− m )2 + O(K ).

tt

t

η

t,i

t,i

0

t=1

t=1

t=1 i∈b∗

19

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Appendix D. Proof of Theorem 4
Proof of Theorem 4. As in Hazan and Kale (2011a), for the rounds we perform uniform sampling we do not update wt′. Let S be the set of rounds of uniform sampling. Then for the other rounds we can apply Corollary 3 to arrive at









E  ℓt,it − ℓt,i∗  ≤ K lηn T + 6ηE  (ℓt,i∗ − µ˜t−1,i∗ )2 + O(1). (18)

t∈[T ]\S

t∈[T ]\S

The second term can be bounded as follows:





E

(ℓt,i∗ − µ˜t−1,i∗ )2 ≤ E

t∈[T ]\S

T
(ℓt,i∗ − µ˜t−1,i∗ )2
t=2

T

T

≤ 3 (ℓt,i∗ − µt,i∗ )2 + 3 (µt,i∗ − µt−1,i∗ )2 + 3E

T
(µt−1,i∗ − µ˜t−1,i∗ )2 .

t=2

t=2

t=2

(19)

The ﬁrst and the third terms in (19) can be bounded using Lemma 10 and 11 of (Hazan and Kale,

2011a) respectively, and they are both of order O(QT,i∗ + 1) if we pick M = Θ(ln T ). The second

term in (19) can be bounded by a constant by Lemma 18. Thus second term in (18) can be bounded

by O (η(QT,i∗ + 1)). Finally, note that E

T t=1

ℓt,it

−

ℓt,i∗

≤E

t∈[T ]\S ℓt,it − ℓt,i∗ +2E[|S|]

and that E[|S|] = O get

T MK t=1 t

= O (M K ln T ) = O K(ln T )2 . Combining everything, we

E T ℓt,it − ℓt,i∗ = O K lηn T + ηQT,i∗ + K(ln T )2 .
t=1

Lemma 18 For any i, Tt=2(µt,i − µt−1,i)2 = O(1).

Proof

By deﬁnition, |µt,i − µt−1,i|

=

1t ℓt,i

−

1 t(t−1)

t−1 s=1

ℓs,i

≤

1t ℓt,i +

1 t(t−1)

T t=2

4 t2

=

O(1).

t−1 s=1

ℓs,i

1 t

t s=1

ℓs,i

−

1 t−1

t−1 s=1

ℓs,i

=

≤ 2t . Therefore, Tt=2(µt,i − µt−1,i)2 ≤

Appendix E. Proof of Theorem 5
We ﬁrst state a useful lemma.
Lemma 19 Let ni be such that ηT +1,i = κni η1,i, i.e., the number of times the learning rate of arm i changes in BROAD-OMD+. Then ni ≤ log2 T , and ηt,i ≤ 5η1,i for all t, i.

20

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Proof Let t1, t2, . . . , tni ∈ [T ] be the rounds the learning rate for arm i changes (i.e., ηt+1,i = κηt,i for t = t1, . . . , tni). By the algorithm, we have

KT ≥ 1 > ρt ,i > 2ρt ,i > · · · > 2ni−1ρt ,i = 2niK.

w¯tn ,i

ni

ni −1

1

i

Therefore,

ni

≤

log2

T.

And

we

have

ηt,i

≤

κlog2

T η1,i

=

e log2 T ln T

η1,i

≤

5η1,i.

Proof of Theorem 5. Again, we verify the three conditions stated in Theorem 2. By Lemma 19,

ηt,i ≤ 5η ≤ 5 × 8110 = 1162 ; also, wt,j ℓˆt,j − mt,j = wt,j (ℓt,j−mwt¯,tj,)j½{it=j} ≤ wt,j wt,j(21− T1 ) ≤

3 because we assume T ≥ 3; ﬁnally,

1 162

× 32

=

118 .

Kj=1 ηt,j wt2,j (ℓˆt,j − mt,j )2 = ηt,it wt2,it (ℓˆt,it − mt,it )2 ≤

Let τj denote the last round the learning rate for arm j is updated, that is, τj max{t ∈

[T ] : ηt+1,j = κηt,j}. We assume that the learning rate is updated at least once so that τj is well

deﬁned, otherwise one can verify that the bound is trivial. For any arm i to compete with, let

u=

1

−

1 T

with B

1 T

ei + T1 w1′ = 1 − T1 ei + K1T 1, which guarantees wu1′i,i ≤ T . Applying Theorem 2,

T t=1

−ei

+ w1′ , ℓˆt

+ at

we have

T

w , ℓˆ − ℓˆ

K ln T T

≤

+

K

1 − 1 h uj + T a + B

tt

t,i

t=1

η

t=1 j=1 ηt+1,j ηt,j

wt′+1,j

t,i t=1

K ln T

1

1

ui

T

≤ η + ητi+1,i − ητi,i h wτ′ i+1,i + t=1 at,i + B

≤ K ln T + 1 − κ h

η

ητi +1,i

ui wτ′ i+1,i

T
+ at,i + B
t=1

K ln T

1

ui

T

≤ η − 5η ln T h w′

+ at,i + B,

(20)

τi +1,i

t=1

where

the

last

inequality

is

by

Lemma

19

and

the

fact

κ

−

1

≥

1 ln T

.

Now

we

bound

the

second

and

the third term in (20) separately.

1. For the second term, by Lemma 16 and T ≥ 3 we have

ui

≥

1

−

1 T

≥

wτ′ i+1,i 32 wτi,i

1 − T1 2 = 32 w¯τi,i

1 − T1 2 ρT +1,i ρT +1,i 4K 3 × 2 ≥ 8 ≥ 8 ≥ 1.
2

Noting that h(y) is an increasing function when y ≥ 1, we thus have

h

ui

≥ h ρT +1,i = ρT +1,i − 1 − ln ρT +1,i ≥ ρT +1,i − 1 − ln KT .

wτ′ i+1,i

8

8

8

8

4

(21)

21

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

2. For the third term, we proceed as

T

T

T

at,i = 6 ηt,iwt,i(ℓˆt,i − mt,i)2 ≤ 90η |ℓˆt,i − mt,i|

t=1

t=1

t=1

1T

≤ 90η max t∈[T ] w¯t,i

|ℓt,i − ℓt−1,i| ≤ 90ηρT +1,iVT,i,

(22)

t=1

where in the ﬁrst inequality, we use wt,i|ℓˆt,i − mt,i| ≤ 3 and ηt,i ≤ 5η; in the second inequality, we do a similar calculation as in Eq. (6) (only replacing wt,i by w¯t,i); and in the last inequality, we use the fact w¯1t,i ≤ ρT +1,i for all t ∈ [T ] by the algorithm.

Combining Eq. (21) and Eq. (22) and using the fact 1+5lnl(nKT4T ) ≤ K ln T , we continue from Eq. (20) to arrive at

T w , ℓˆ − ℓˆ ≤ 2K ln T + ρ

−1 + 90ηV + B,

(23)

tt

t,i

η

T +1,i 40η ln T

T ,i

t=1

We are almost done here, but note that the left-hand side of (23) is not the desired regret. What we would like to bound is

T

T

T

T

w¯t, ℓˆt − ℓˆt,i = w¯t − wt, ℓˆt +

wt, ℓˆt − ℓˆt,i ,

(24)

t=1

t=1

t=1

t=1

where the second summation on the right-hand side is bounded by Eq. (23). The ﬁrst term can

be written as

T t=1

− T1

wt

+

1 KT

1,

ℓˆt

.

Note that T1

T t=1

−wt, ℓˆt

≤

1 T

Tt=1| wt, ℓˆt − mt | +

1 T

Tt=1| wt, mt | ≤ 3 + 1 = 4, and E

1 T

T t=1

1 K

1,

ℓˆt

=

1 T

T t=1

1 K

1,

ℓ

t

≤ 1. Therefore,

taking expectation on both sides of (24), we get

T

T

2K ln T

−1

E

ℓt,it − ℓt,i ≤ η + E[ρT +1,i] 40η ln T + 90ηVT,i + O(1),

t=1

t=1

because E[B] is also O(1) as proved in Corollary 3.

Appendix F. Proofs of Lemma 6 and Theorem 7
Proof of Lemma 6. By the same arguments as in the proof of Lemma 1, we have wt′+1 − u, ℓˆt ≤ Dψt (u, wt′ ) − Dψt (u, wt′+1) − Dψt (wt′+1, wt′ );
and wt − wt′+1, mt ≤ Dψt (wt′+1, wt′ ) − Dψt (wt′+1, wt) − Dψt (wt, wt′ ).
Therefore, by expanding the instantaneous regret, we have wt − u, ℓˆt
22

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

= wt − wt′+1, ℓˆt − mt + wt′+1 − u, ℓˆt + wt − wt′+1, mt ≤ wt − wt′+1, ℓˆt − mt + Dψt (u, wt′ ) − Dψt (u, wt′+1) − Dψt (wt′+1, wt) − Dψt (wt, wt′ ).

Proof of Theorem 7. Applying Lemma 6, we have

T
wt − u, ℓˆt
t=1

T
≤

Dψt (u, wt′ ) − Dψt (u, wt′+1) + wt − wt′+1, ℓˆt − mt

t=1

≤ K ln wu1′i,i i=1 η

T
+ wt − wt′+1, ℓˆt − mt
t=1

− At.

For the second term, using Lemma 15 and 14 we bound wt − wt′+1, ℓˆt − mt by

− At

wt − wt′+1 t,wt

ℓˆt − mt ∗ ≤ 3
t,wt

∗2

K

ℓˆt − mt

= 3η wt2,i(ℓˆt,i − mt,i)2

t,wt

i=1

Finally

we

lower

bound

At

for

the

MAB

case.

Note

h(y)

=

y − 1− ln y

≥

(y−1)2 6

for

y

∈

[

1 2

,

2].

By Lemma 16 and 17, wwt′+t,1i,i and wwtt′,,ii both belong to [ 21 , 2]. Therefore,

A = D (w′ , w ) + D (w , w′ ) = 1 K h wt′+1,i + h wt,i t ψt t+1 t ψt t t η i=1 wt,i wt′,i

1K ≥ 6η
i=1

(wt′+1,i − wt,i)2 (wt,i − wt′,i)2

w2

+

w′2

t,i

t,i

1 K (wt′+1,i − wt,i)2 (wt,i − wt′,i)2

≥ 24η

w2

+ w2

,

i=1

t,i

t−1,i

and

T

1 T K (wt′,i − wt−1,i)2 T K (wt,i − wt′,i)2

1 T K (wt,i − wt−1,i)2

At ≥ 24η

w2

+

w2

≥ 48η

w2

.

t=1

t=2 i=1

t−1,i

t=2 i=1

t−1,i

t=2 i=1

t−1,i

Appendix G. Doubling Trick

We include the version of our algorithm with the doubling trick in Algorithm 3. For simplicity we
still assume the time horizon T is known; the extension to unknown horizon is straightforward. Proof of Theorem 8. Let u = 1 − T1 b∗ + T1 w1′ so that ln wu1′i,i ≤ ln T . At some epoch β, by Theorem 7, the break condition, and condition (iii) we have with ηβ 1622−Kβ 0 ,

Tβ+1 wt − u, ℓˆt
t=Tβ +1

≤ K ln T + 3η

Tβ+1

K
w2 (ℓˆ − m )2

ηβ β t=Tβ +1 i=1 t,i t,i t,i

23

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Algorithm 3 Doubling trick for BROAD-OMD with at = 0 Initialize: η = 1621K0 , T0 = 0, t = 1. for β = 0, 1, . . . do
wt′ = argminw∈Ω ψ1(w) (restart BROAD-OMD). while t ≤ T do
Update wt, sample bt ∼ wt, and update wt′+1 as in BROAD-OMD with Option II. if ts=Tβ+1 Ki=1 ws2,i(ℓˆs,i − ms,i)2 ≥ K3lηn2T then
η ← η/2, Tβ+1 ← t, t ← t + 1. break. end
t ← t + 1. end
end

≤ 2K ln T + 3η

K
w2

(ℓˆ

−m

)2 = O K ln T .

ηβ β i=1 Tβ+1,i Tβ+1,i Tβ+1,i ηβ

Suppose that at time T , the algorithm is at epoch β = β∗. Then we have

T
wt − u, ℓˆt
t=1

β∗
≤O
β=0

K ln T ηβ

β∗
≤ O 2βK0K ln T ≤ O 2β∗K0K ln T .
β=0

It remains to bound β∗. If β∗ = 0 (no restart ever happened), then trivially O(K0K ln T ). Otherwise, because epoch β∗ − 1 ﬁnishes, we have

T t=1

wt

− u, ℓˆt

=

Tβ∗

K
w2 (ℓˆ − m )2 ≥

K ln T

= Ω(22β∗K2K ln T ).

t,i t,i

t,i

3(ηβ∗−1)2

0

t=Tβ∗−1+1 i=1

Combining them, we have

T
wt − u, ℓˆt
t=1

≤O

2β∗ K0K ln T

 ≤ O


Tβ∗
(K ln T )


K
wt2,i(ℓˆt,i − mt,i)2

t=Tβ∗−1+1 i=1





TK

≤ O  (K ln T )

wt2,i(ℓˆt,i − mt,i)2 ,

(25)

t=1 i=1

Combining both cases we have





T

TK

wt − u, ℓˆt ≤ O  K ln T

wt2,i(ℓˆt,i − mt,i)2 + K0K ln T  .

(26)

t=1

t=1 i=1

24

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Now substituting u by its deﬁnition and taking expectations, with B

1 T

T t=1

−b∗ + w1′ , ℓˆt

we

arrive at

T

E

bt − b∗, ℓt

t=1

 ≤ O E 

TK





K ln T

wt2,i(ℓˆt,i − mt,i)2 + K0K ln T  + E[B]

t=1 i=1

 ≤ O

K ln T E

TK
wt2,i(ℓˆt,i − mt,i)2
t=1 i=1

 + K0K ln T  ,

where the last inequality uses the fact E[B] = O(K) and Jensen’s inequality.

Appendix H. Proofs of Corollary 9 and Theorem 20

Proof

of

Corollary

9.

We

ﬁrst

verify

the

three

conditions

in

Theorem

7:

η

≤

1 162

by

assumption;

wt,i ℓˆt,i − mt,i = (ℓt,i − ℓαi(t),i)½{it = i} ≤ 2 < 3; η Ki=1 wt2,i(ℓˆt,i − mt,i)2 = ηwt2,it (ℓˆt,it −

mt,it )2 ≤ 1692 = 118 . Let u = 1 − T1 ei∗ + T1 w1′ , which guarantees wu1′i,i ≤ T . By Theorem 7 and some rearrangement, we have

T
wt − ei∗ , ℓˆt
t=1

K ln T

T

≤

+ 3η

K

T

w2 (ℓˆ − m )2 − A + B,

η

t,i t,i

t,i

t

t=1 i=1

t=1

where B

1 T

replace

T t=1

Theorem 7.

T t=1

−ei∗ + w1′ , ℓˆt

.

To get the stated bound, just note that E[B]

=

O(1), and

K i=1

wt2,i(ℓˆt,i

−

mt,i)2

by

the

upper

bound

at

(8)

and

At

by

the

lower

bound

in

Appendix I. Omitted Details in Section 4.1.1

Although the generalization to multi-player games is straightforward, for simplicity we only con-
sider two-player zero-sum games. We ﬁrst describe the protocol of the game. The game is deﬁned by an unknown matrix G ∈
[−1, 1]M×N where entry G(i, j) speciﬁes the loss (or reward) for Player 1 (or Player 2) if Player 1 picks row i while Player 2 picks column j. The players play the game repeatedly for T rounds. At round t, Player 1 randomly picks a row it ∼ xt for some xt ∈ ∆M while Player 2 randomly picks a column jt ∼ yt for some yt ∈ ∆N . In (Syrgkanis et al., 2015), the feedbacks they receive are the vectors Gyt and x⊤t G respectively. As a natural extension to the bandit setting, we consider a setting where the feedbacks are the scalar values e⊤it Gyt and x⊤t Gejt respectively, that is, the expected loss/reward for the players’ own realized actions (over the opponent’s randomness).
It is clear that each player is essentially facing an MAB problem and thus can employ an MAB
algorithm. Speciﬁcally, if both players ap√ply Exp3 for example, their expected average strategies converge to a Nash equilibrium at rate 1/ T . However, if instead Player 1 applies BROAD-OMD
conﬁgured as in Corollary 9, then her regret has a path-length term that can be bounded as follows:

KT
e⊤i Gyt − e⊤i Gyt−1
i=1 t=2

KT
≤
i=1 t=2

e⊤i G
∞

T
yt − yt−1 1 ≤ K
t=2

yt − yt−1 1,

25

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

which is closely related to the negative regret term in Corollary 9 for Player 2 if she also employs the same BROAD-OMD. The cancellation of these terms then lead to faster convergence rate.

Theorem 20 For the setting described above, if both players run BROAD-OMD conﬁgured as

in

Corollary

9

except

that

ηt,i

=

η

=

(M

+

N

)−

1 4

T

− 41

,

then

their

expected

average

strategies

converge to Nash equilibriums at the rate of O˜

(M

+

N

)

5 4

/T

3 4

, that is,

max

E[x¯]⊤Gy

≤

Val

+

O˜((M

+

5
N)4

/T

3
4)

and

min

x⊤GE[y¯] ≥ Val − O˜((M

5

3

+ N ) 4 /T 4 ),

y∈∆N

x∈∆M

where

x¯

=

1 T

T t=1

xt,

y¯

=

1 T

T t=1

yt

and

Val

=

min

max x⊤Gy = max

min

x⊤Gy.

x∈∆M y∈∆N

y∈∆N x∈∆M

Proof As mentioned, Player 1’s VT,i is

T

T

T

|ℓt,i − ℓt−1,i| = |e⊤i Gyt − e⊤i Gyt−1| ≤

t=1

t=1

t=1

e⊤i G
∞

T
yt − yt−1 1 ≤
t=1

yt − yt−1 1

due to the assumption |G(i, j)| ≤ 1. Therefore, by Corollary 9, Player 1’s (pseudo) regret is

max
x∈∆M

T

T

x⊤t Gyt − x⊤Gyt

t=1

t=1

≤O

M ln T η

T

1 T M (xt,i − xt−1,i)2

+ E 6ηM

yt − yt−1 1 − 48η

x2

,

t=1

t=2 i=1

t−1,i

while Player 2’s (pseudo) regret is

max
y∈∆N

T

T

x⊤T Gy − x⊤t Gyt

t=1

t=1

≤O

N ln T η

T

1 T N (yt,i − yt−1,i)2

+ E 6ηN

xt − xt−1 1 − 48η

y2

.

t=1

t=2 i=1

t−1,i

Summing

up

the

above

two

bounds,

and

using

the

following

fact

(by

the

inequality

a

−

b

≤

a2 4b

):

N

(yt,i − yt−1,i)2

6ηM |yt,i − yt−1,i| − 48ηy2

i=1

t−1,i

N
≤ 432η3M 2 yt2−1,i ≤ 432η3M 2,
i=1

we get

max E[x¯]⊤Gy − min x⊤GE[y¯] = O (M + N ) ln T + η3(M 2 + N 2) .

y∈∆N

x∈∆M

Tη

With η = Θ˜

(M

+

N

)−

1 4

T

−

1 4

then gives

the above bound becomes O˜

(M

+

N

)

5 4

T

−

3 4

. Rearranging

max E[x¯]⊤Gy ≤

min

x⊤GE[y¯]

+

O˜((M

+

N

)

5 4

T

−

3 4

),

y∈∆N

x∈∆M

26

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

≤

min

max

x⊤Gy

+

O˜((M

+

N

)

5 4

T

−

3 4

)

=

Val

+

O˜((M

+

N

)

5 4

T

−

3 4

),

x∈∆M y∈∆N

and similarly

min

x⊤GE[y¯] ≥

max

E[x¯]⊤Gy − O˜((M

+

N

)

5 4

T

−

3 4

)

x∈∆M

y∈∆N

≥ max

min

x⊤Gy − O˜((M

+

N

)

5 4

T

−

3 4

)

=

Val

−

O˜((M

+

N

)

5 4

T

−

3 4

),

y∈∆N x∈∆M

completing the proof. √
As shown by the theorem, we obtain convergence rate faster than 1/ T , but still slower than the 1/T rate compared to the full-information setup of (Rakhlin and Sridharan, 2013b; Syrgkanis et al., 2015), due to the fact that we only have ﬁrst-order instead of second-order path-length bound.
Note that Rakhlin and Sridharan (2013b) also studies two-player zero-sum games with bandit feedback but with an unnatural restriction that in each round the players play the same strategy for four times. Foster et al. (2016) greatly weakened the restriction, but their algorithm only converges to some approximation of Val. For further comparisons, the readers are referred to the comparisons to (Syrgkanis et al., 2015) in (Foster et al., 2016). We also point out that the question raised in (Rakhlin and Sridharan, 2013b) remains open: if the players only receive the realized loss/r√eward e⊤it Gejt as feedback (a more natural setup), can the convergence rate to Val be faster than 1/ T ?

Appendix J. Proof of Theorem 10
Proof of Theorem 10. We ﬁrst verify conditions (ii) and (iii) in Theorem 8 hold for ℓˆt,i = ℓt,i½w{ti,ti=i}
and mt,i = ℓt,it. Indeed, condition (ii) holds since wt,i|ℓˆt,i − mt,i| = |ℓt,i½{it = i} − wt,iℓt,it| ≤
2 < 3. Other the other hand, condition (iii) also holds because

K

K

η wt2,i(ℓˆt,i − mt,i)2 = η (ℓt,i½{it = i} − wt,iℓt,it )2

i=1

i=1

K
= η (ℓ2t,i½{it = i} − 2ℓt,iwt,iℓt,it ½{it = i} + wt2,iℓ2t,it )
i=1

≤1

K

ℓ2 − 2w ℓ2 +

w2 ℓ2

162 t,it

t,it t,it

t,i t,it

i=1

≤ 1 (1 + 0 + 1) < 1 .

162

18

Thus, by Theorem 8, we have





T

TK

E

ℓt,it − ℓt,i∗ = O  (K ln T )E

wt2,i(ℓˆt,i − ℓt,it )2 + K ln T  .

(27)

t=1

t=1 i=1

Now we consider the stochastic setting. In this case, we further take expectations over ℓ1, . . . , ℓT on both sides of (27). The left-hand side of (27) can be lower bounded by

T

T

T

T

T

E

ℓt,it − ℓt,i∗ = E

ℓt,it − min ℓt,j ≥ E

ℓt,it − ℓt,a∗

j

t=1

t=1

t=1

t=1

t=1

27

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

TK





T

T

=E

wt,i(ℓt,i − ℓt,a∗ ) ≥ E 

wt,i∆ = ∆E (1 − wt,a∗ ) .

t=1 i=1

t=1 i=a∗

t=1

(28)

On the other hand,

Eit∼wt

K
wt2,i(ℓˆt,i − ℓt,it )2
i=1

= Eit∼wt

K
wt2,i
i=1

ℓt,i½{it = i}

2

wt,i

− ℓt,it

= Eit∼wt

K
(ℓt,i½{it = i} − wt,iℓt,it )2

i=1


K
= wt,i (ℓt,i − wt,iℓt,i)2 +

 wt,j (wt,iℓt,j)2

i=1

j=i


K


K

≤ wt,i (1 − wt,i)2 + wt,j wt2,i = wt,i(1 − wt,i)

i=1

j=i

i=1

≤ (1 − wt,a∗ ) + wt,i = 2(1 − wt,a∗ ).

(29)

i=a∗

Therefore, the ﬁrst term on the right-hand side of (27) can be upper bounded by

TK

T

(K ln T )E

wt2,i(ℓˆt,i − ℓt,it )2 ≤ (K ln T )E

2(1 − wt,a∗ ) .

(30)

t=1 i=1

t=1

Let H = E Tt=1(1 − wt,a∗ ) . Combining (28), (30), and (27), we have

H∆ ≤ O (K ln T )H + K ln T ,

which implies H = O

K ln T ∆2

. Therefore, the expected regret is upper bounded by

O (K ln T )H + K ln T = O K ln T . ∆

For the adversarial setting, we continue from an intermediate step of (29):

Eit∼wt

K
wt2,i(ℓˆt,i − ℓt,it )2
i=1





K

= wt,i(1 − wt,i)2ℓ2t,i + wt,j wt2,iℓ2t,j 

i=1

j=i

K

K

K

K

≤ wt,iℓ2t,i +

wt,j wt2,iℓ2t,j ≤ wt,iℓ2t,i + wt,j ℓ2t,j = 2Eit∼wt ℓ2t,it

i=1

j=1 i=j

i=1

j=1

28

MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS

Assuming ℓt,i ∈ [0, 1], we thus have ℓ2t,it ≤ ℓt,it and

T

E

ℓt,it

t=1


T
− ℓt,i∗ = O 
t=1

(K ln T )E

T
ℓt,it
t=1

 + K ln T  .

Solving for E

T t=1

ℓt,it

and rearranging then give

T

E

ℓt,it

t=1


T
− ℓt,i∗ = O 
t=1


T
(K ln T ) ℓt,i∗ + K ln T  = O
t=1

KLT,i∗ ln T + K ln T .

29

