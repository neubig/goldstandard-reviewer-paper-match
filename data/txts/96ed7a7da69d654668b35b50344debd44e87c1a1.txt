LOW-RESOURCE CONTEXTUAL TOPIC IDENTIFICATION ON SPEECH
Chunxi Liu†, Matthew Wiesner†, Shinji Watanabe†, Craig Harman†, Jan Trmal†,‡, Najim Dehak†, Sanjeev Khudanpur†,‡
†Center for Language and Speech Processing, The Johns Hopkins University, USA ‡Human Language Technology Center of Excellence, The Johns Hopkins University, USA

arXiv:1807.06204v2 [cs.CL] 28 Sep 2018

ABSTRACT
In topic identiﬁcation (topic ID) on real-world unstructured audio, an audio instance of variable topic shifts is ﬁrst broken into sequential segments, and each segment is independently classiﬁed. We ﬁrst present a general purpose method for topic ID on spoken segments in low-resource languages, using a cascade of universal acoustic modeling, translation lexicons to English, and English-language topic classiﬁcation. Next, instead of classifying each segment independently, we demonstrate that exploring the contextual dependencies across sequential segments can provide large improvements. In particular, we propose an attention-based contextual model which is able to leverage the contexts in a selective manner. We test both our contextual and non-contextual models on four LORELEI languages, and on all but one our attention-based contextual model signiﬁcantly outperforms the context-independent models.
Index Terms— Topic identiﬁcation, universal acoustic modeling, recurrent neural networks, attention
1. INTRODUCTION
Storing and digitizing vast amounts of audio data such as broadcast news, telephone conversations, meetings, and lectures is now commonplace. However, to search, organize and analyze these large audio collections requires the development of new computational tools. Topic identiﬁcation (topic ID) from speech is one such human language technology that aims to identify topics or themes present in a speech recording.
Since audio data lacks the paragraphs and punctuation markings that naturally deﬁne semantically coherent chunks of text, long audio recordings of varying topic shifts must ﬁrst be segmented, often manually. Then the standard approach to topic ID from speech is to (i) develop automatic speech recognition (ASR) systems to decode each speech segment into word sequences, (ii) produce intermediate vector representations of the hypothesized word sequences for each segment, and (iii) learn a classiﬁer from text/topic pairs and apply it to the vector representation of each segment independently. However, such standard approach has many drawbacks, especially in a low-resource scenario: building ASR and topic classiﬁers in a new language requires a large amount of transcribed speech and topic-labeled text in the language, neither of which may be present. Furthermore, accurate topic inference or language understanding in general may require interpretation from adjacent segments. For instance, tasks such as anaphora resolution or entity disambiguation critically depend on contextual clues.
This work was supported by DARPA LORELEI Grant No HR001115-2-0024. The authors thank Zhongqiang Huang at the Rayth¯ eon BBN Technologies, USA, for his help with the machine translation systems.

To study these challenges, we evaluate our topic ID performance in the DARPA LORELEI (Low Resource Languages for Emergent Incidents) Program framework. The program’s goal is to develop human language technologies to support humanitarian assistance and disaster relief operations in locations where a low-resource language is spoken, also referred to as an incident language (IL) in the LORELEI terminology [1, 2]. To provide situational awareness based on IL sources, one component task in LORELEI, called the Situation Frame (SF) task, involves building systems to provide meta-data for text and speech documents. These documents and associated metadata are collectively referred to as situation frames (SFs) and consist of the following items:
• situation type, also simply referred to as topic, • geographic localization, • status (temporal, resolution or urgency) of the situation.
An SF system should automatically identify all the SFs covered in the text or speech collection of IL. In this paper we focus on building topic ID technology to enable situation type identiﬁcation from speech.
In order to simulate realistic disaster scenarios, the LORELEI speech corpora are divided into IL corpora – corpora which typically contain unlabeled data in a low-resource language pertaining to one or more emergent disasters – and related language corpora for which annotated data, possibly from high-resource languages, is provided. In both cases the audio data is collected “in the wild”, and for a diverse set of languages. These data are collected, manually segmented, and annotated by APPEN [3] for the LORELEI program. We refer to each unsegmented audio ﬁle as one spoken document. Since audio ﬁle segmentations are provided, each document consists of a sequence of segments, and each segment lasts around one minute on average and no more than 2 minutes. There are 11 predeﬁned topics chosen according to the LORELEI program scope, as shown in Table 2. Any speech segment categorized by at least one of these topics is deﬁned as in-domain data, otherwise as out-of-domain that can be viewed as the 12th topic label. Table 1 shows an example spoken document that is split into 7 segments with varying topic.
In this paper, we focus particularly on the IL scenario for which the only annotated data are from related (development) languages in addition to a very small amount of IL topic labeled data or IL transcribed speech (minutes rather than hours) which may be obtained.
2. RELATED WORK
Prior work of topic ID on speech [4, 5, 6, 7] has focused on conversational telephone speech such as LDC’s Fisher and Switchboard collections, where topic ID was performed for each whole conversation. Since the two participants of each conversation were prompted to speak on one single topic, no conversation segmentation was needed. Furthermore, since each conversation contains a single topic and

Table 1. An example of a single spoken document that consists of seven spoken segments in LORELEI US English corpus.

Document ID USE 080
USE 080 USE 080
USE 080 USE 080
USE 080
USE 080

Segment ID USE 080 001
USE 080 002 USE 080 003
USE 080 004 USE 080 005
USE 080 006
USE 080 007

Sampled sentences from spoken segment transcript
turning to Tennessee where eleven people have now died in historic wildﬁres ... hundreds of buildings have been torched ... yeah you have a number of people missing but we don’t know the exact number ... ... and he said that the search and rescue effort yesterday ended and now today it is search and recovery ... ... so many homes damaged destroyed ... ... just looking at the devastation now . because we saw a few homes and you know a few cars, it is really bad ... ... but people in town it sounds like now are questioning how fast they were notiﬁed to get out .... ... since they were forced to evacuate so a lot of them will be seeing their homes and properties for the ﬁrst time tonight ...

Topic label Shelter
Out-of-domain Urgent Rescue
Shelter Shelter
Evacuation
Evacuation, Shelter

Table 2. Topic labels deﬁned in the LORELEI Speech SF task.

Topic scope In-domain Out-of-domain

Topic label (Situation Type)
Evacuation Food Supply Urgent Rescue Utilities, Energy, or Sanitation Infrastructure Medical Assistance Shelter Water Supply Civil Unrest or Wide-spread Crime Elections and Politics Terrorism or other Extreme Violence Out-of-domain

lasts 5-10 minutes, the classiﬁcation task is relatively straightforward. The LORELEI collections, however, provide a more challenging and realistic scenario, where wildly collected audio recordings can be extremely long, of varying length, and contain multiple topic shifts at variable locations in the audio. For this reason each audio document in the LORELEI data is ﬁrst segmented by APPEN [3], and then topic ID is required on the much shorter resulting segments.
To solve the LORELEI task, prior work [8] used a mismatched ASR to directly decode IL speech, while [9] proposed sharing common phonemic representation among languages and transferring acoustic models trained on higher-resource (potentially related) language(s). After ASR, [8, 9] translated both development (dev) and incident languages into English words, used the translated dev language data along with the given topic label annotations to learn English-language topic models and then classify the translated IL data. Additionally, instead of using ASR to convert speech into sequences of words, [10, 11, 9] also investigated unsupervised techniques to automatically discover and decode IL speech segments into phone-like units via acoustic unit discovery (AUD), or into wordlike units via unsupervised term discovery (UTD). However, only small amount of IL topic labels might be available to learn classiﬁers based on AUD/UTD tokenized segments, though [9] showed marginal gains by combining them with the above cascade approach that implemented ASR, machine translation (MT) and operated on English words.
Extensive work on text classiﬁcation has also been explored to date. For example, each word can be mapped to word embedding vector and concatenated as inputs to convolutional neural network [12] or recursive/recurrent neural networks [13, 14], with a ﬁnal softmax classiﬁcation layer. [15] demonstrated improved classiﬁcation

performance by using hierarchical attention networks to learn both word- and sentence-level attentions, enabling the models to attend differentially to more and less important words/sentences.
However, all the above work is focused on classifying each data instance (i.e. each single sentence, conversation, or document) individually, and independently from the rest of data instances. Data instances in close proximity to each other may incorporate contextual information that can be exploited by contextual modeling. Thus we note that our LORELEI topic ID task, which is formulated as multi-label classiﬁcation for each speech segment in a spoken document, is similar to the domain or intent classiﬁcation in a multi-turn spoken language understanding (SLU) component of a dialog system [16, 17, 18]. One conversation session between user and dialog system, which can be viewed as one spoken document, may include multiple turns, and the user query in each turn is a spoken segment; thus, each segment needs to be classiﬁed into one of the supported domains or user intents, as classiﬁed into topic(s). [16, 17, 18] have shown that SLU may require contextual interpretation from the dialog history, and the SLU models that incorporate the semantic contexts of preceding user utterances and system outputs outperform those without context. Therefore, in this paper, we investigate if the propagation of contextual information across spoken segments can improve topic ID, although the spoken segment that is one minute long on average in our case is often much longer and more semantically self-contained than the typical utterance of a few words in SLU systems.
3. UNIVERSAL PHONE SET ASR
We attempt to provide language universal acoustic models by training on many languages sharing a common phonemic representation. We then transfer these models to a new language via a pronunciation lexicon with the same phonemic representation as used in training. We refer to this ASR as universal phone set ASR and we use the same approach as in [9]. Following [9], for experiments on Tigrinya and Oromo, we use the same selection of 10 BABEL languages for ASR training (∼600h). For Russian, we use 10h subsets of 21 BABEL languages (∼200h) in training (all except Haitian, Vietnamese, Amharic, Georgian). This reduces training time, provides better phoneme coverage, and performs as well or better in word error rate (WER) as the 10 language ASR on the BABEL Haitian, Amharic and Georgian dev sets. The ﬁnal acoustic models are time-delay neural networks (TDNNs, [19]) trained with the lattice-free version of the maximum mutual information (LF-MMI) criterion [20].
During a LORELEI evaluation we also have access to a few hours (2-10) of consultation with a native informant (NI), a native speaker of the IL. From these interactions we collected an additional 15-30

minutes of IL speech data in both Tigrinya and Oromo. We use this data to adapt the ASR for both languages using the same weight transfer approach as in [21]. Since the source languages and ILs use the same phoneset, all layers of the seed neural network (trained on the source languages), including the ﬁnal layer, were transferred and trained for one epoch on the IL transcribed data.
4. TOPIC IDENTIFICATION
To leverage the supervised topic annotations of speech segments in multiple dev languages, we represent each speech segment in all languages as a bag of English words. We derive this representation by building ASR systems to decode the speech and then translate each decoded word into its most likely English translations1. Support vector machine (SVM) or neural network (NN) based topic classiﬁers can then be learned by using these English word representations of speech segments in foreign languages along with their associated topic labels. Thus, using only a translation lexicon, we can always perform topic ID on an IL without its transcribed or topic-labeled speech by using the unadapted universal phone set ASR to decode and translate its speech segments into English words.
4.1. Learning spoken segment representations
Since English word sequences generated using translation tables lack proper syntax, we represent speech segments using a bag-ofwords model over the generated English words. Each speech segment is represented by a vector of unigram occurrence counts over the generated English word sequences and scaled to produce a term frequency-inverse document frequency (tf-idf) feature, which is then normalized to 2 norm unit length. Latent Semantic Analysis (LSA) [22] transformation can then be learned from the tf-idf features. This transformation effectively merges the dimensions corresponding to words with similar meanings, and maps the high-dimensional tf-idf vectors to a much smaller dimension vector space.
We can also append other auxiliary features to the tf-idf or LSA representations of speech segments. Since our datasets contain segments with music, many of which are out-of-domain, we found that features indicating the substantial presence of music are particularly useful. To generate these features, we build music detectors from the MUSAN dataset [23] and for each speech segment the music detector produces a posterior probability that a substantial portion of music is present. Denoting the tf-idf/LSA vector as x ∈ Rd, the music posterior as δ ∈ (0, 1), and the vector concatenation operation as ⊕, our new representation can be created as x ⊕ δ.
4.2. Non-contextual modeling using SVM and NN
Since each speech segment is represented by a vector x and can be associated with one or multiple topics, we perform topic ID by doing multi-label classiﬁcations. The baseline approach is the binary relevance method, which independently trains one binary SVM classiﬁer for each label, and a segment is evaluated by each classiﬁer to determine if the respective label applies. We use stochastic gradient descent (SGD) based linear SVMs with hinge loss and 2 norm regularization [24, 25].
Another approach based on feedforward NN2 is to use an output layer with sigmoid output nodes, one for each label, and train the NN
1We use the probabilistic bilingual translation tables employed in the MT systems. We use such bilingual lexicons rather than full-blown MT systems to relax the dependency on fully developed IL-to-English MT pipeline that could be unavailable for very-low-resource languages.
2We simply use NN to refer to multi-layer perceptron in following sections.

to minimize the binary cross entropy loss deﬁned as
K
L(Θnn; x, y) = − (yk log ok + (1 − yk) log(1 − ok)) (1)
k=1
where Θnn denotes the NN parameters, y is the target binary vector of topic labels, ok and yk are the output and the target for label k, and the number of unique labels K = 12.

4.3. Contextual modeling using RNN

We explore using recurrent neural network (RNN) to capture the
dependencies between context segments. Different RNN variants can
be used such as the Elman RNN, long short-term memory (LSTM), or
gated recurrent unit (GRU). We denote an RNN simply as a mapping φ : Rd × Rd → Rd that takes a d dimensional input vector x and a d dimensional state vector h and outputs a new d dimensional state vector h = φ(x, h).
Consider a spoken document that consists of n spoken segments, as exempliﬁed in Table 1. For each i = 1 . . . n, the segment i is represented by a vector xi ∈ Rd. The document is represented as X = [x1 . . . xn]. We encode X using a bidirectional RNN (BiRNN), and the model parameters Θrnn associated with this BiRNN layer are φf , φb : Rd × Rd → Rd . Thus the segment representation vectors are encoded by forward and backward RNNs as
fj = φf (xj , fj−1) ∀j = 1 . . . n (2)
bj = φb(xj , bj+1) ∀j = n . . . 1

We assume zero initial state vectors f0 and bn+1. And a contextual

representation is induced as

hi = fi ⊕ bi ∀i = 1 . . . n.

We denote the entire operation as a mapping BiRNNΘrnn :

(h1 . . . hn) ← BiRNNΘrnn (x1 . . . xn).

Therefore, instead of the non-contextual xi, the contextual hi is

used as input to the feedforward fully connected layers for ﬁnal

classiﬁcation:

oi ← NNΘnn (hi) ∀i = 1 . . . n

where oi denotes the ﬁnal output vector. The joint loss

L(Θrnn, Θnn) =

n i=1

L(Θnn;

hi,

yi)

is calculated by Eq. 1.

4.4. Contextual modeling using attention

Consider a spoken document X as above. For each target segment xi, RNNs implicitly encode its context segments as fi−1/bi+1, but

the RNN non-linear transformations make it hard to control the in-

teraction between segments. Instead, we explicitly perform a convex

combination of the target and context segments using an attention
mechanism [26]. For each i = 1 . . . n, now consider classifying xi. We aim to produce a new contextual vector representation ci to replace xi, by combining xi and its contexts X \ xi. Then each ci is followed by fully connected layers for ﬁnal classiﬁcation as in Section 4.2. To do so, let zi be a categorical latent variable with sample space {1 . . . n}, which encodes the desired selection among X based on a query qi. We let the query be xi itself, i.e., qi = xi, since xi has been produced speciﬁcally to encode the semantic information pertaining to segment i. Then we assume the

source position to be selected and attended to follows a distribution,
zi ∼ p(zi = j|qi, X), ∀j = 1 . . . n, and therefore the contextual representation ci is deﬁned as an expenctation:

ci = Ezi∼p(zi|X,qi)[xzi ] = p(zi = j|qi, X) xj

j=1

n

(3)

= αij xj

j=1

Softmax

ααα2,22,3
2,1
%%%2,22,3
2,1

x2 x2 x2 x3
x2 x1

y1

y2

y3

y4

"$&

"&&

"'&

"#&

"$$

"&$

"'$

"#$

c1 
α1,2
α1,1

c2 
α2,3 α2,1 α2,2

c3 
α3,3 α3,1 α3,2

c4 
α4,1 α4,2

x1

x2

x3

x4

Fig. 1. Illustration of the proposed contextual modeling using attention, which operates on a spoken document of 4 segments, and leverages each 1-nearest left and right context segments to classify the target xi, for each i = 1 . . . 4.

The weight αij of each xj is computed by

exp(eij )

αij =

n

, exp(eik )

∀j = 1 . . . n

(4)

k=1

where eij = f (qi, xj), called an alignment model [26] that scores how important the segment j is to help classify the query segment i. We parameterize it with a single-layer NN,
eij = wT σ(W1qi + W2xj + b1) + b2 (5)
= wT σ(W1xi + W2xj + b1) + b2, ∀j = 1 . . . n

where σ is an activation function, and W1, W2 ∈ Rd ×d, w, b1 ∈ Rd , b2 ∈ R1 are the weight matrices and jointly learned with all the other NN parameters. Note that to classify the target xi, the contexts close to xi can be more relevant than the distant ones, so we can also use a truncated context window and only consider its L/R nearest left/right contexts, i.e., for each j = max(0, i − L) . . . min(i + R, n) in Eq. 3, 4 and 5. The complete modeling framework is illustrated in Figure 1, which uses the 1-nearest left and right contexts (i.e. when L = R = 1).
The intuition behind such process is that, although the overall feature vector xi may not be salient enough to produce high posteriors for the correct topic labels, certain feature dimensions in xi are indicative of the correct topics, so that the alignment model of Eq. 5 can still capture those informative feature dimensions and give the useful context segments higher scores eij and higher weights αij. The weights are used in a convex combination of Eq. 3 such that the useful context features are explicitly combined to produce a contextual representation ci.
In contrast with the deterministic RNN mapping, the attention mechanism allows for selectively using the contexts in a dynamic manner. Consider that, given the left contexts of xi, the forward RNN produces a context vector fi−1 as in Eq. 2, and the context vector fi−1 is used in a deterministic function φf (xi, fi−1) regardless of whatever the xi is. However, given different xi, the attention model is able to produce different context weights given different input query vector qi (since qi = xi in Eq. 5); i.e., the contexts will be weighted accordingly for different xi, so that any context can only be effectively used when the attention model detects its relevance and gives it a high weight by Eq. 4 and 5. The alignment model (Eq. 5) is explicitly learned as a selector to dynamically detect relevant and useful contexts over irrelevant ones.
However, as yet, given a ﬁxed input query qi, the alignment model of Eq. 5 equally considers the other input features xj, for each j = 1 . . . n, in the attention computation, remaining unaware of that

the segment i is being the target one to classify. Therefore, inspired by the position-based gating procedure in [27], the scores eij can be penalized based on the relative position of the context segment j and target i before being normalized to weight αij:

d(i, j) exp(eij)

αij =

n

, d(i, k) exp(eik)

∀j = 1 . . . n

(6)

k=1

where d(i, j) is a gating function of one hidden layer NN and logistic sigmoid output ([0, 1]):

1,

j=i

d(i, j) = σ2(w2σ1(w1|i − j| + b1) + b2), ∀j = i (7)

where σ1 is an activation function (tanh), σ2 a sigmoid function, and w1, w2, b1, b2 ∈ R1. Such additional gating procedure helps favor the weight of target xi and penalize the effects of any contexts far
from the target, so that it can presumably prevent ci (Eq. 3) from
being overwhelmed by context segments regardless of the target xi.

5. EXPERIMENTS

5.1. Experimental setup
5.1.1. Data
Language Dataset Pack
Turkish Arabic Spanish Dev US English Mandarin DEV Tigrinya DEV Oromo DEV
Mandarin EVAL Eval Russian
Tigrinya EVAL Oromo EVAL

|Ddoc|
212 47 198 154 77 130 241
119 136 116
46

|Dseg |
2095 1025
393 842 100 159 364
724 787 1095 709

Topic ASR Label Corpora

LDC LDC LDC LDC NI NI NI

BABEL [28] GALE [29] HUB4 [30]
– GALE [31] Universal Universal

LDC GALE [31] LDC Universal LDC Universal LDC Universal

Table 3. LORELEI speech data description. |Ddoc| denotes the number of documents. |Dseg| denotes the number of segments. Manual transcripts are provided for US English corpus.
The dev and eval datasets we used are as shown in Table 3. For Turkish, Arabic, Spanish and English3, each language is a single dataset and seen as dev set. Their topic label annotations for all segments are given, and used for training the topic ID classiﬁers.
For Mandarin, Tigrinya and Oromo4, each language has one DEV and EVAL set respectively; true topic labels on these DEV sets are unavailable, so we selected some segments, collected their hypothesized topic labels from NI, and included them into the classiﬁer training. Also on these DEV sets, we selected some segments for the NI to transcribe and used them for ASR adaptation5. More NI details can be found in [9]. The EVAL sets of these three languages, in addition to the single Russian dataset6, are provided with true topic annotations and are used for evaluating the system performance.

3Turkish (LDC2016E109), Arabic (LDC2016E123), Spanish (LDC2016E127), and US English (LDC2017E50). Since Spanish set is overwhelmed by the segments of topic “Elections and Politics”, we ﬁltered out all segments that include that topic.
4Mandarin DEV (LDC2016E108), Mandarin EVAL (LDC2016E115), Tigrinya DEV (LDC2017E35), Tigrinya EVAL (LDC2017E37), Oromo DEV (LDC2017E36), and Oromo EVAL (LDC2017E38)
5The total given NI session for consultation was 2 hours for Mandarin, 10 hours each for Tigrinya and Oromo. Only on Tigrinya and Oromo DEV sets, we collected transcribed speech from the NI, 27 mins and 18 mins respectively.
6Russian (LDC2016E111)

In sum, when evaluating on Mandarin EVAL or the Russian dataset, the training data for learning topic ID models consists of Turkish, Arabic, Spanish, US English and Mandarin DEV. When evaluating on Tigrinya EVAL or Oromo EVAL, we use the same training data in addition to Tigrinya DEV or Oromo DEV, respectively.
5.1.2. Evaluation metrics
Under the LORELEI Speech SF evaluation framework as described in [2], topic ID system outputs are evaluated in two layers using average precision (AP, equal to the area under the precision-recall curve). The Relevance layer is to separate the segments with at least 1 in-domain topic from non-relevant out-of-domain segments. Speciﬁcally, each segment is given 11 posteriors over each in-domain topic, and the Relevance scorer takes the maximum one as the indomain posterior, and compares it against the true binary label to compute the AP. The Type layer is to detect all present in-domain topics. Type scorer computes the micro-averaged precision and recall across 11 in-domain topics, and then compute the AP.
5.1.3. ASR
Audio transcripts exist only for the LORELEI English speech dataset. For the Turkish, Arabic, Spanish and Mandarin datasets, we used preexisting transcribed speech corpora, as shown in Table 3, to train ASR systems with Kaldi [32], and then decoded the LORELEI datasets using the appropriate ASR. For Russian, Tigrinya and Oromo, transcribed speech corpora were unavailable and we used the universal phone set ASR to decode each corpus. For the Tigrinya and Oromo the pronunciation lexicons were obtained as described in [9]. For Russian, we used wikt2pron [33] to generate a seed lexicon by scraping Wiktionary for XSAMPA pronunciations of all Russian words found in the provided monolingual text and then proceeded as in [9]. We also ﬁltered out all words not written in Cyrillic, and to discard apparent misspellings, we used only the 600k most frequent remaining words. Note that speech segment lengths vary between 5 seconds and 2 minutes, with an average duration of about one minute. Since ASR systems have difﬁculty decoding long segments, we further segmented the audio using either the overlapped segmentation approach as in [34], or voice-activity-detection (VAD) again as in [9]. For the overlapped segmentation, we used chunks 15 seconds long repeated every 10 seconds and then ﬁltered the transcripts by removing words whose midpoints were within 2.5 seconds to the chunk edge before combining them into a single transcript.
In addition, we trained two Gaussian mixture models (GMMs) on the speech and music portions of MUSAN [23]. Each speech segment is split into 15 second chunks but without overlap. Then for each chunk, two average frame-level log-likelihoods were calculated by the music and speech GMMs respectively, to further produce a musicto-speech log-likelihood ratio γ. γ went through a sigmoid function and produced a posterior score. Finally for each speech segment, we used the maximum posterior score over all chunks as the music posterior feature δ for that segment, which was then concatenated to the LSA features (Section 4.1).
5.1.4. MT
Supervised topic label information in various languages can all be projected into English topic classiﬁers through bilingual (i.e., foreign language to English) translation lexicons. Each bilingual MT table was derived from the parallel training data with words aligned automatically by the GIZA++ [35] and Berkeley aligner [36], independently under the MT effort. Any preexisting training data can be used in addition to the data provided by the LORELEI program.

Table 4. Differing topic ID model parameters.

Eval language

Russian Mandarin Tigrinya Oromo

LSA dimension

300

900

SVM 2 regularization 0.001

0.0001

# hidden layers in NN

1

2

# hidden layers in RNN-based 0

1

# hidden layers in Attn-based 1

2

Dropout rate

0.5

0.25

We translated each foreign word in the ASR transcript into its four most likely English translations. Then we mapped any unicode data into their nearest ASCII characters, and ﬁltered stop words using the lists from [25, 37], and any words with three or fewer characters.

5.1.5. Topic ID models
First, the tf-idf or LSA features were learned as described in Section 4.1. For the four eval languages overall, we found LSA dimensions over {300, 600, 900} can generally produce improvements over tf-idf features, and the ones we ﬁnally used are shown in Table 4.
The non-contextual SVM and NN were learned as in Section 4.2. Contextual RNN and attention based models are described in Section 4.3 and 4.4 respectively. Also, validation data is needed for model parameter tuning and during NN training. While evaluating Mandarin, we left a small portion out of the training data as validation data. While evaluating Tigrinya, Oromo and Russian, we used the Mandarin EVAL dataset as validation data. The performance of SVMs did not vary much after 30 SGD epochs. While each NNbased model was trained for up to 50 epochs, the model with the best accuracy on the validation data was used for evaluation on the eval data. For each experiment, we repeated it 5 times, and the means are reported in Table 5 (standard deviation is omitted for clarity).
Some parameters were tuned and shared for all languages. SVMs used 2 regularization constant 0.001 on tf-idf features. All NNbased models had hidden layer size 512 and rectiﬁed linear unit (ReLU) nonlinearities, and were trained with Adam optimizer [38]. Non-contextual NN used mini-batch size of 256 spoken segments. Contextual RNN or attention based models used the mini-batch size of 6 spoken documents. For RNN-based models, we found GRU slightly outperformed the conventional Elman RNN or LSTM, and we used the GRU layer that took the LSA features as inputs.
The remaining parameters were the same when evaluating Mandarin, Tigrinya and Oromo, but differed for Russian, as shown in Table 4. When evaluating Russian, we found using SVM 2 regularization constant 0.001 on LSA features, one NN hidden layer and dropout rate 0.5 gave much better results instead; presumably because the universal phone set ASR for Russian was unadapted, the resulting transcripts were more noisy and required stronger regularization. Also, we used one GRU layer directly followed by the output layer. Each contextual vector ci (Section 4.4) was followed by one hidden layer instead of two. Note that we used the above model parameters different from other three eval languages to obtain optimal results for both Russian non-contextual and contextual models, so that the comparisons between the two are fair. In other words, within each eval language, we focus on drawing fair comparisons between its optimal non-contextual and contextual models.

5.2. Non-contextual topic ID results
Table 5 ﬁrst shows the results based on non-contextual model SVM and NN. The LSA transformation on tf-idf features substantially

Table 5. Topic Identiﬁcation average precision results on LORELEI speech datasets. Attention1 or Attention2 are to use 1 or 2 nearest context segments, respectively. POS denotes that the additional position-based gating procedure is enabled. Last row shows 10-fold cross-validation results on each eval set using ASR transcripts and true topic labels (without using MT or any other dev set), as oracle results for comparison.

Non-contextual Contextual Non-contextual

Model
tf-idf + SVM LSA + SVM LSA + Music + SVM LSA + Music + NN LSA + Music + RNN LSA + Music + NN + Attention1 LSA + Music + NN + Attention1 + POS LSA + Music + NN + Attention2 LSA + Music + NN + Attention2 + POS
LSA + SVM, 10-fold CV

Mandarin Type Rel

0.458 0.505 0.510 0.519 0.525 0.544 0.542 0.537 0.543

0.702 0.739 0.742 0.743 0.737 0.741 0.744 0.742 0.746

0.576 0.843

Russian Type Rel

0.382 0.386 0.408 0.415 0.430 0.466 0.449 0.461 0.448

0.854 0.856 0.870 0.881 0.894 0.888 0.884 0.892 0.887

0.444 0.838

Tigrinya Type Rel

0.371 0.392 0.422 0.451 0.389 0.407 0.455 0.365 0.444

0.554 0.561 0.600 0.625 0.578 0.597 0.618 0.557 0.611

0.574 0.719

Oromo Type Rel

0.382 0.409 0.423 0.436 0.467 0.495 0.482 0.494 0.491

0.772 0.782 0.822 0.819 0.820 0.828 0.830 0.838 0.831

0.419 0.750

Average Type Rel

0.398 0.423 0.441 0.455 0.453 0.478 0.482 0.464 0.482

0.721 0.735 0.759 0.767 0.757 0.764 0.769 0.757 0.769

0.503 0.788

improved performance across the board, and also mapped the highdimensional tf-idf vectors (around 25k) to a dimension small enough for the LSA features to be used as inputs to NN-based models. Additionally, appending auxiliary music posteriors (Section 4.1) to the LSA features can produce large gains, except on Mandarin; we found for the Mandarin dataset music was less indicative of out-of-domain topics. Finally, feedforward NNs were generally more competitive than linear SVMs when using the same input LSA features.
5.3. Contextual topic ID results
Table 5 further shows the results of our experiments using the proposed contextual RNN and attention models. The GRU-based contextual models outperformed the best non-contextual NN models on Russian and Oromo, but not on Mandarin or Tigrinya. For Mandarin, we had a high-performing ASR system trained on about 600 hrs of transcribed speech, so the Mandarin transcripts were much more accurate than other languages, which presumably made it more difﬁcult to improve the non-contextual baseline results; inference from contexts might be helpful to recover the ASR errors in the target segment, and thus better ASR transcripts often allow for conﬁdent classiﬁcation without having to consider additional contexts. For Tigrinya EVAL set, we found around 72% of the segments were out-of-domain; i.e., if a target segment is mostly surrounded by out-of-domain segments, using its contexts can give adverse effects, and the overall results can be worse than the context-independent counterparts.
We further experimented with contextual attention based models, using the contexts of 1 or 2 nearest left and right segments, i.e. when L = R = 1 or L = R = 2 in Section 4.4. The attention-based models outperformed the non-contextual models, except on Tigrinya, due to the overwhelming amount of out-of-domain segments, as discussed above. However, we can match the performance of the noncontextual models on Tigrinya, with only a small performance loss in the other languages, by using the additional gating mechanism in Eq. 6. The gating mechanism partially penalizes the context effects and makes the model aware of the target segment location. Note that, the attention-based models consistently outperformed the RNNbased models, and it demonstrates the efﬁcacy of the gated attention mechanism that dynamically selects and uses more relevant contexts instead of receiving contexts in a deterministic manner.
Overall, with respect to the best context-independent models, the contextual attention based models produced comparable performance on Tigrinya, and produced signiﬁcant performance improvements on the rest three eval languages. Also, the results of using wider contexts, i.e., 2-nearest left and right segments, were comparable to those of

using 1-nearest only. In addition, the attention function we used in Eq. 5 is also called additive attention, and we found it outperformed the dot-product (multiplicative) attention [39]. We also experimented with multi-head attention [39] and component (or multi-dimensional) attention [40], but none of these techniques can give us better results, presumably due to the small size of our topic model training data.
5.4. Ten-fold cross validation analysis
So far, we have only used English translations of each dev and eval language to resolve the language mismatch, but the training and eval datasets can be severely mismatched. An oracle result against which we can compare is the 10-fold cross validation (CV) performance on each eval set itself, where each experiment uses part of the true eval set topic labels for training. For each eval language, we split the corresponding eval set into 10 folds, used the extracted LSA features over the raw ASR transcripts (without translation or any data from other language), completed 10 monolingual supervised SVM classiﬁcations with true topic labels, and reported the average of each 10 experiments as shown in the last row of Table 5.
For each language, such 10-fold CV results give us estimates of the topline numbers we could achieve with around 700 in-domain training exemplars. First, the gap between each topline number and the full accuracy (i.e. AP = 1) indicates the given ASR quality and the intrinsic difﬁculty of each eval dataset. Next, comparing our crosslingual approach with such monolingual topline, we found using the above contextual topic ID approach had reduced the gap on Mandarin, and surpassed the topline on Russian and Oromo, while falling behind on Tigrinya (due to the train-test discrepancy in the amount of out-ofdomain segment occurrences as discussed in Section 5.3).
6. CONCLUSIONS
Audio collected in the wild may contain many topic shifts, and we need to perform topic ID on a sequence of segmented audio. Each resulting speech segment is of reasonable length and semantically self-contained, such that each of them can be independently classiﬁed. However, we have performed comprehensive experiments on the LORELEI datasets in a realistic low-resource scenario, and have found that exploiting the context segments can provide signiﬁcant topic ID performance improvements over the context-independent models. Finally, comparing our contextual frameworks, we demonstrate that the proposed attention modeling which leverages context segments in a selective approach can consistently outperform the RNN-based alternatives.

7. REFERENCES
[1] Stephanie Strassel and Jennifer Tracey, “LORELEI language packs: Data, tools, and resources for technology development in low resource languages,” in Proc. LREC, 2016.
[2] Nikolaos Malandrakis, Ondˇrej Glembek, and Shrikanth Narayanan, “Extracting situation frames from non-English speech: Evaluation framework and pilot results,” in Proc. Interspeech, 2017.
[3] “APPEN,” http://appen.com.
[4] Timothy J Hazen, Fred Richardson, and Anna Margolis, “Topic identiﬁcation from audio recordings using word and phone recognition lattices,” in Proc. ASRU, 2007.
[5] Mark Dredze, Aren Jansen, Glen Coppersmith, and Ken Church, “NLP on spoken documents without ASR,” in Proc. EMNLP, 2010.
[6] Jonathan Wintrode and Sanjeev Khudanpur, “Limited resource term detection for effective topic identiﬁcation of speech,” in Proc. ICASSP, 2014.
[7] Chandler May, Francis Ferraro, Alan McCree, Jonathan Wintrode, Daniel Garcia-Romero, and Benjamin Van Durme, “Topic identiﬁcation and discovery on text and speech,” in Proc. EMNLP, 2015.
[8] Pavlos Papadopoulos, Ruchir Travadi, Colin Vaz, Nikolaos Malandrakis, Ulf Hermjakob, Michael Pust Pourdamghani, Boliang Zhang, Xiaoman Pan, Di Lu, Ying Lin, et al., “Team ELISA system for DARPA LORELEI speech evaluation 2016,” in Proc. Interspeech, 2017.
[9] Matthew Wiesner, Chunxi Liu, Lucas Ondel, Craig Harman, Vimal Manohar, Jan Trmal, Zhongqiang Huang, Najim Dehak, and Sanjeev Khudanpur, “Automatic speech recognition and topic identiﬁcation for almost-zero-resource languages,” in Proc. Interspeech, 2018.
[10] Chunxi Liu, Jinyi Yang, Ming Sun, Santosh Kesiraju, Alena Rott, Lucas Ondel, Pegah Ghahremani, Najim Dehak, Luka´sˇ Burget, and Sanjeev Khudanpur, “An empirical evaluation of zero resource acoustic unit discovery,” in Proc. ICASSP, 2017.
[11] Chunxi Liu, Jan Trmal, Matthew Wiesner, Craig Harman, and Sanjeev Khudanpur, “Topic identiﬁcation for speech without ASR,” in Proc. Interspeech, 2017.
[12] Yoon Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv preprint arXiv:1408.5882, 2014.
[13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts, “Recursive deep models for semantic compositionality over a sentiment treebank,” in Proc. EMNLP, 2013.
[14] Duyu Tang, Bing Qin, and Ting Liu, “Document modeling with gated recurrent neural network for sentiment classiﬁcation,” in Proc. EMNLP, 2015.
[15] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy, “Hierarchical attention networks for document classiﬁcation,” in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 1480–1489.
[16] Puyang Xu and Ruhi Sarikaya, “Contextual domain classiﬁcation in spoken language understanding systems using recurrent neural network,” in Proc. ICASSP, 2014.

[17] Chunxi Liu, Puyang Xu, and Ruhi Sarikaya, “Deep contextual language understanding in spoken dialogue systems,” in Proc. Interspeech, 2015.
[18] Chiori Hori, Takaaki Hori, Shinji Watanabe, and John R Hershey, “Context sensitive spoken language understanding using role dependent LSTM layers,” in Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction, 2015.
[19] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, “A time delay neural network architecture for efﬁcient modeling of long temporal contexts,” in Proc. Interspeech, 2015.
[20] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahremani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur, “Purely sequence-trained neural networks for ASR based on lattice-free MMI,” in Proc. Interspeech, 2016.
[21] Vimal Manohar, Daniel Povey, and Sanjeev Khudanpur, “JHU Kaldi System for Arabic MGB-3 ASR challenge using diarization, audio-transcript alignment and transfer learning,” in Proc. ASRU, 2017.
[22] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman, “Indexing by latent semantic analysis,” Journal of the American society for information science, vol. 41, no. 6, pp. 391, 1990.
[23] David Snyder, Guoguo Chen, and Daniel Povey, “Musan: A music, speech, and noise corpus,” arXiv preprint arXiv:1510.08484, 2015.
[24] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro, “Pegasos: Primal estimated sub-gradient solver for SVM,” in Proc. ICML, 2007.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.
[26] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” Proc. ICLR, 2015.
[27] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “End-to-end continuous speech recognition using attention-based recurrent NN: First results,” arXiv preprint arXiv:1412.1602, 2014.
[28] Jan Trmal, Guoguo Chen, Dan Povey, Sanjeev Khudanpur, Pegah Ghahremani, Xiaohui Zhang, Vimal Manohar, Chunxi Liu, Aren Jansen, Dietrich Klakow, et al., “A keyword search system using open source software,” in Proc. SLT, 2014.
[29] Sameer Khurana and Ahmed Ali, “QCRI advanced transcription system (QATS) for the Arabic multi-dialect broadcast media recognition: MGB-2 challenge,” in Proc. SLT, 2016.
[30] “1997 Spanish Broadcast News Speech (HUB4-NE),” https: //catalog.ldc.upenn.edu/LDC98S74.
[31] “GALE Phase 2 Chinese Broadcast News Speech,” https: //catalog.ldc.upenn.edu/LDC2013S08.
[32] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka´sˇ Burget, Ondˇrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motl´ıcˇek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.

[33] “Wiktionary pronunciation collector,” https://github. com/abuccts/wikt2pron.
[34] Vijayaditya Peddinti, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Reverberation robust acoustic modeling using ivectors with time delay neural networks,” in Proc. Interspeech, 2015.
[35] Franz Josef Och and Hermann Ney, “A systematic comparison of various statistical alignment models,” Computational Linguistics, vol. 29, no. 1, pp. 19–51, 2003.
[36] Percy Liang, Ben Taskar, and Dan Klein, “Alignment by agreement,” in Proc. NAACL HLT, 2006.
[37] Steven Bird, Ewan Klein, and Edward Loper, Natural language processing with Python: analyzing text with the natural language toolkit, O’Reilly Media Inc., 2009.
[38] Diederik P Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” in The International Conference on Learning Representations (ICLR), 2015.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems (NIPS), 2017.
[40] Amit Das, Jinyu Li, Rui Zhao, and Yifan Gong, “Advancing connectionist temporal classiﬁcation with attention modeling,” Proc. ICASSP, 2018.

