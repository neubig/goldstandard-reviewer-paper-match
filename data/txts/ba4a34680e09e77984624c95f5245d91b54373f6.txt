XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization

arXiv:2003.11080v5 [cs.CL] 4 Sep 2020

Junjie Hu * 1 Sebastian Ruder * 2 Aditya Siddhant 3 Graham Neubig 1 Orhan Firat 3 Melvin Johnson 3

Abstract
Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark1 to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.
1. Introduction
In natural language processing (NLP), there is a pressing urgency to build systems that serve all of the world’s approximately 6,900 languages to overcome language barriers and enable universal information access for the world’s citizens (Ruder et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). At the same time, building NLP systems for
*Equal contribution 1Carnegie Mellon University 2DeepMind 3Google Research. Correspondence to: Junjie Hu <junjieh@cs.cmu.edu>, Melvin Johnson <melvinp@google.com>.
1The benchmark is publicly available at https://sites. research.google/xtreme. The codes used for downloading data and training baseline models are available at https: //github.com/google-research/xtreme.

most of these languages is challenging due to a stark lack of data. Luckily, many languages have similarities in syntax or vocabulary, and multilingual learning approaches that train on multiple languages while leveraging the shared structure of the input space have begun to show promise as ways to alleviate data sparsity. Early work in this direction focused on single tasks, such as grammar induction (Snyder et al., 2009), part-of-speech (POS) tagging (Ta¨ckstro¨m et al., 2013), parsing (McDonald et al., 2011), and text classiﬁcation (Klementiev et al., 2012). Over the last few years, there has been a move towards general-purpose multilingual representations that are applicable to many tasks, both on the word level (Mikolov et al., 2013; Faruqui & Dyer, 2014; Artetxe et al., 2017) or the full-sentence level (Devlin et al., 2019; Lample & Conneau, 2019). Despite the fact that such representations are intended to be general-purpose, evaluation of them has often been performed on a very limited and often disparate set of tasks—typically focusing on translation (Glavasˇ et al., 2019; Lample & Conneau, 2019) and classiﬁcation (Schwenk & Li, 2018; Conneau et al., 2018b)— and typologically similar languages (Conneau et al., 2018a).
To address this problem and incentivize research on truly general-purpose cross-lingual representation and transfer learning, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics.2 In addition, we introduce pseudo test sets as diagnostics that cover all 40 languages by automatically translating the English test set of the natural language inference and question-answering dataset to the remaining languages.
XTREME focuses on the zero-shot cross-lingual transfer scenario, where annotated training data is provided in English but none is provided in the language to which systems must transfer.3 We evaluate a range of state-of-the-art machine
2By typologically diverse, we mean languages that span a wide set of linguistic phenomena such as compounding, inﬂection, derivation, etc. which occur in many of the world’s languages.
3This is done both for efﬁciency purposes (as it only requires testing, not training, on each language) and practical considerations (as annotated training data is not available for many languages).

XTREME: A Benchmark for Evaluating Cross-lingual generalization

translation (MT) and multilingual representation-based approaches to performing this transfer. We ﬁnd that while stateof-the-art models come close to human performance in English on many of the tasks we consider, performance drops signiﬁcantly when evaluated on other languages. Overall, performance differences are highest for syntactic and sentence retrieval tasks. Further, while models do reasonably well in most languages in the Indo-European family, we observe lower performance particularly for Sino-Tibetan, Japonic, Koreanic, and Niger-Congo languages.
In sum, our contributions are the following: (i) We release a suite of 9 cross-lingual benchmark tasks covering 40 typologically diverse languages. (ii) We provide an online platform and leaderboard for the evaluation of multilingual models. (iii) We provide a set of strong baselines, which we evaluate across all tasks, and release code to facilitate adoption. (iv) We provide an extensive analysis of limitations of state-of-the-art cross-lingual models.
2. Related Work
Cross-lingual representations Early work focused on learning cross-lingual representations using either parallel corpora (Gouws et al., 2015; Luong et al., 2015) or a bilingual dictionary to learn a linear transformation (Mikolov et al., 2013; Faruqui & Dyer, 2014). Later approaches reduced the amount of supervision required using self-training (Artetxe et al., 2017) and unsupervised strategies such as adversarial training (Conneau et al., 2018a), heuristic initialisation (Artetxe et al., 2018), and optimal transport (Zhang et al., 2017). Building on advances in monolingual transfer learning (McCann et al., 2017; Howard & Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pretrained encoders have recently been shown to be effective for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu & Dredze, 2019; Lample & Conneau, 2019; Siddhant et al., 2020).
Cross-lingual evaluation One pillar of the evaluation of cross-lingual representations has been translation, either on the word level (bilingual lexicon induction) or on the sentence level (machine translation). In most cases, evaluation has been restricted to typologically related languages and similar domains; approaches have been shown to fail in less favorable conditions (Glavasˇ et al., 2019; Vulic´ et al., 2019; Guzma´n et al., 2019). Past work has also reported issues with common datasets for bilingual lexicon induction (Czarnowska et al., 2019; Kementchedjhieva et al., 2019) and a weak correlation with certain downstream tasks (Glavasˇ et al., 2019). Translation, however, only covers one facet of a model’s cross-lingual generalization ability. For instance, it does not capture differences in classiﬁcation performance that are due to cultural differences (Mohammad et al., 2016; Smith et al., 2016).

On the other hand, cross-lingual approaches have been evaluated on a wide range of tasks, including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), document classiﬁcation (Schwenk & Li, 2018), and question answering (Artetxe et al., 2020; Lewis et al., 2019). Evaluation on a single task is problematic as past work has noted potential issues with standard datasets: MLDoc (Schwenk & Li, 2018) can be solved by matching keywords (Artetxe et al., 2020), while MultiNLI, the dataset from which XNLI (Conneau et al., 2018b) was derived, contains superﬁcial cues that can be exploited (Gururangan et al., 2018). Evaluation on multiple tasks is thus necessary to fairly compare cross-lingual models. Benchmarks covering multiple tasks like GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) have arguably spurred research in monolingual transfer learning. In the cross-lingual setting, such a benchmark not only needs to cover a diverse set of tasks but also languages. XTREME aims to ﬁll this gap.
3. XTREME
3.1. Design principles
Given XTREME’s goal of providing an accessible benchmark for the evaluation of cross-lingual transfer learning on a diverse and representative set of tasks and languages, we select the tasks and languages that make up the benchmark based on the following principles:
Task difﬁculty Tasks should be sufﬁciently challenging so that cross-language performance falls short of human performance.
Task diversity Tasks should require multilingual models to transfer their meaning representations at different levels, e.g. words, phrases and sentences. For example, while classiﬁcation tasks require sentence-level transfer of meaning, sequence labeling tasks like part-of-speech (POS) tagging or named entity recognition (NER) test the model’s transfer capabilities at the word level.
Training efﬁciency Tasks should be trainable on a single GPU for less than a day. This is to make the benchmark accessible, in particular to practitioners working with lowresource languages under resource constraints.
Multilinguality We prefer tasks that cover as many languages and language families as possible.
Sufﬁcient monolingual data Languages should have sufﬁcient monolingual data for learning useful pre-trained representations.
Accessibility Each task should be available under a permissive license that allows the use and redistribution of the

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 1. Characteristics of the datasets in XTREME for the zero-shot transfer setting. For tasks that have training and dev sets in other languages, we only report the English numbers. We report the number of test examples per target language and the nature of the test sets (whether they are translations of English data or independently annotated). The number in brackets is the size of the intersection with our selected languages. For NER and POS, sizes are in sentences. Struct. pred.: structured prediction. Sent. retrieval: sentence retrieval.

Task

Corpus

|Train| |Dev|

|Test| Test sets |Lang.| Task

Metric Domain

Classiﬁcation XNLI PAWS-X

392,702 49,401

2,490 2,000

5,010 translations 2,000 translations

15 NLI 7 Paraphrase

Acc.

Misc.

Acc.

Wiki / Quora

Struct. pred.

POS NER

21,253 3,974 20,000 10,000

47-20,436 1,000-10,000

ind. annot. 33 (90) POS ind. annot. 40 (176) NER

F1

Misc.

F1

Wikipedia

XQuAD

87,599 34,726

1,190 translations

11 Span extraction F1 / EM Wikipedia

QA

MLQA

4,517–11,590 translations

7 Span extraction F1 / EM Wikipedia

TyDiQA-GoldP 3,696 634 323–2,719 ind. annot.

9 Span extraction F1 / EM Wikipedia

Retrieval

BUCC Tatoeba

-

- 1,896–14,330

-

-

1,000

-

5 Sent. retrieval F1

- 33 (122) Sent. retrieval Acc.

Wiki / news misc.

data for research purposes.
3.2. Tasks
XTREME consists of nine tasks that fall into four different categories requiring reasoning on different levels of meaning. We give an overview of all tasks in Table 1, and describe the task details as follows.
XNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018b) asks whether a premise sentence entails, contradicts, or is neutral toward a hypothesis sentence. Crowd-sourced English data is translated to ten other languages by professional translators and used for evaluation, while the MultiNLI (Williams et al., 2018) training data is used for training.
PAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling (Yang et al., 2019) dataset requires to determine whether two sentences are paraphrases. A subset of the PAWS dev and test sets (Zhang et al., 2019) was translated to six other languages by professional translators and is used for evaluation, while the PAWS training set is used for training.
POS We use POS tagging data from the Universal Dependencies v2.5 (Nivre et al., 2018) treebanks, which cover 90 languages. Each word is assigned one of 17 universal POS tags. We use the English training data for training and evaluate on the test sets of the target languages.
NER For NER, we use the Wikiann (Pan et al., 2017) dataset. Named entities in Wikipedia were automatically annotated with LOC, PER, and ORG tags in IOB2 format using a combination of knowledge base properties, crosslingual and anchor links, self-training, and data selection. We use the balanced train, dev, and test splits from Rahimi et al. (2019).
XQuAD The Cross-lingual Question Answering Dataset

(Artetxe et al., 2020) requires identifying the answer to a question as a span in the corresponding paragraph. A subset of the English SQuAD v1.1 (Rajpurkar et al., 2016) dev set was translated into ten other languages by professional translators and is used for evaluation.
MLQA The Multilingual Question Answering (Lewis et al., 2019) dataset is another cross-lingual question answering dataset similar to XQuAD. The evaluation data for English and six other languages was obtained by automatically mining target language sentences that are parallel to sentences in English from Wikipedia, crowd-sourcing annotations in English, and translating the question and aligning the answer spans in the target languages. For both XQuAD and MLQA, we use the SQuAD v1.1 training data for training and evaluate on the test data of the corresponding task.
TyDiQA-GoldP We use the gold passage version of the Typologically Diverse Question Answering (Clark et al., 2020) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simpliﬁed version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3× and 2× less lexical overlap compared to XQuAD and MLQA respectively. We use the English training data for training and evaluate on the test sets of the target languages.
BUCC The goal of the second and third shared task of the workshop on Building and Using Parallel Corpora (Zweigenbaum et al., 2017; 2018) is to extract parallel sentences from a comparable corpus between English and four other languages. The dataset provides train and test splits for each language. For simplicity, we evaluate representations on the test sets directly without ﬁne-tuning and calculate similarity

XTREME: A Benchmark for Evaluating Cross-lingual generalization

using cosine similarity.4
Tatoeba We use the Tatoeba dataset (Artetxe & Schwenk, 2019), which consists of up to 1,000 English-aligned sentence pairs covering 122 languages. We ﬁnd the nearest neighbour using cosine similarity and calculate error rate.
3.3. Languages
As noted in Section 3.1, we choose our target languages based on availability of monolingual data, and typological diversity. We use the number of articles in Wikipedia as a proxy for the amount of monolingual data available online. In order to strike a balance between language diversity and availability of monolingual data, we select all languages out of the top 100 Wikipedias5 with the most articles as of December 2019.6 We ﬁrst select all languages that appear in at least three of our benchmark datasets. This leaves us with 19 languages, most of which are Indo-European or major world languages. We now select 21 additional languages that appear in at least one dataset and come from less represented language families. Wherever possible, we choose at least two languages per family.7
In total, XTREME covers the following 40 languages (shown with their ISO 639-1 codes for brevity) belonging to 12 language families and two isolates: af, ar, bg, bn, de, el, en, es, et, eu, fa, ﬁ, fr, he, hi, hu, id, it, ja, jv, ka, kk, ko, ml, mr, ms, my, nl, pt, ru, sw, ta, te, th, tl, tr, ur, vi, yo, and zh. We provide a detailed overview of these languages in terms of their number of Wikipedia articles, linguistic features, and coverage in XTREME in the appendix.
While XTREME covers these languages in the sense that there is gold standard data in at least one task in each language, this does not mean that it covers all aspects of each language that are necessary for transfer. Languages may reveal different characteristics based on the task, domain, and register in which they are used. XTREME thus only serves as a glimpse into a model’s true cross-lingual generalization capability.
3.4. Pseudo test data for analyses
XTREME covers 40 languages overall. Evaluation across the majority of languages is only possible for a subset of tasks, i.e. POS, NER, and Tatoeba. As additional diagnositics and to enable a broader comparison across languages for a more
4Results can be improved using more sophisticated similarity metrics (Artetxe & Schwenk, 2019).
5https://meta.wikimedia.org/wiki/List_of_ Wikipedias
6This also has the beneﬁt that they are covered by state-of-theart methods such as mBERT and XLM.
7For the Austro-Asiatic, Kartvelian, and Kra-Dai families as well as for isolates, we only obtain one language.

diverse set of tasks, we automatically translate the English portions of a representative classiﬁcation and QA task to the remaining languages using an in-house translation system.8 We choose XNLI and XQuAD as both have test sets that are translations of the English data by professional translators.
We ﬁrst verify that performance on the translated test sets is a good proxy for performance on the gold standard test sets. We report the detailed results in the appendix. For XQuAD, the automatically translated test sets underestimate mBERT’s true performance by 3.0 F1 / 0.2 EM points, similar to the 2.6 F1 points reported by Agic´ & Schluter (2018) when translating the test data to other languages.9 For XNLI, the automatically translated test sets overestimate the true prediction accuracy by 2.4 points. In order to measure the translation quality between the human-translated test data and our pseudo test data, we compute the BLEU score, and the chrF score (Popovic´, 2015), which is suitable for measuring the translation quality of some languages such as Chinese and Russian. For the 14 languages in XNLI, we obtain average scores of 34.2 BLEU and 58.9 chrF scores on our pseudo test data compared to the reference translations, which correlate with a Pearson’s ρ of 0.57 and 0.28 respectively with mBERT performance.
Translating the English data to the remaining languages yields 40-way parallel pseudo test data that we employ for analyses in Section 5.
4. Experiments
4.1. Training and evaluation setup
XTREME focuses on the evaluation of multilingual representations. We do not place any restriction on the amount or nature of the monolingual data used for pretraining multilingual representations. However, we request authors to be explicit about the data they use for training, in particular any cross-lingual signal. In addition, we suggest authors should not use any additional labelled data in the target task beyond the one that is provided.
For evaluation, we focus on zero-shot cross-lingual transfer with English as the source language as this is the most common setting for the evaluation of multilingual representations and as many tasks only have training data available in English. Although English is not generally the best source language for cross-lingual transfer for all target languages (Lin et al., 2019), this is still the most practically useful setting. A single source language also facilitates evaluation as models only need to be trained once and can be evaluated
8Details of our translation system are provided in the appendix. 9Note that even human translated test sets may underestimate a model’s true cross-lingual generalization ability as such translationese has been shown to be less lexically diverse than naturally composed language (Koppel & Ordan).

XTREME: A Benchmark for Evaluating Cross-lingual generalization

on all other languages.10
Concretely, pretrained multilingual representations are ﬁnetuned on English labelled data of an XTREME task. The model is then evaluated on the test data of the task in the target languages.
4.2. Baselines
We evaluate a number of strong baselines and state-ofthe-art models. The approaches we consider learn multilingual representations via self-supervision or leverage translations—either for representation learning or for training models in the source or target language. We focus on models that learn deep contextual representations as these have achieved state-of-the-art results on many tasks. For comparability among the representation learning approaches, we focus on models that learn a multilingual embedding space between all languages in XTREME. We encourage future work to focus on these languages to capture as much language diversity as possible. We report hyperparameters in the appendix. All hyper-parameter tuning is done on English validation data. We encourage authors evaluating on XTREME to do the same.
mBERT Multilingual BERT (Devlin et al., 2019) is a transformer model (Vaswani et al., 2017) that has been pretrained on the Wikipedias of 104 languages using masked language modelling (MLM).
XLM XLM (Lample & Conneau, 2019) uses a similar pretraining objective as mBERT with a larger model, a larger shared vocabulary, and trained on the same Wikipedia data covering 100 languages.
XLM-R XLM-R Large (Conneau et al., 2020) is similar to XLM but was trained on more than a magnitude more data from the web covering 100 languages.
MMTE The massively multilingual translation encoder is part of an NMT model that has been trained on in-house parallel data of 103 languages extracted from the web (Arivazhagan et al., 2019). For transfer, we ﬁne-tune the encoder of the model (Siddhant et al., 2020).
Translate-train For many language pairs, an MT model may be available, which can be used to obtain data in the target language. To evaluate the impact of using such data, we translate the English training data into the target language using our in-house MT system. We then ﬁne-tune mBERT on the translated data. We provide details on how we align answer spans in the source and target language for the QA tasks in the appendix. We do not provide translation-based baselines for structured prediction tasks due to an abun-
10Future work may also consider multi-source transfer, which is interesting particularly for low-resource languages, and transfer to unknown languages or unknown language-task combinations.

dance of in-language data and a requirement for annotation projection.
Translate-train multi-task We also experiment with a multi-task version of the translate-train setting where we ﬁne-tune mBERT on the combined translated training data of all languages jointly.
Translate-test Alternatively, we train the English BERTLarge (Devlin et al., 2019) model on the English training data and evaluate it on test data that we translated from the target language to English using our in-house MT system.
In-language model For the POS, NER, and TyDiQAGoldP tasks where target-language training data is available, we ﬁne-tune mBERT on monolingual data in the target language to estimate how useful target language labelled data is compared to labelled data in a source language.
In-language few-shot In many cases, it may be possible to procure a small number of labelled examples in the target language (Eisenschlos et al., 2019). To evaluate the viability of such an approach, we additionally compare against an mBERT model ﬁne-tuned on 1,000 target language examples for the tasks where monolingual training data is available in the target languages.
In-language multi-task For the tasks where monolingual training data is available, we additionally compare against an mBERT model that is jointly trained on the combined training data of all languages.
Human performance For XNLI, PAWS-X, and XQuAD, we obtain human performance estimates from the English datasets they are derived from, MNLI, PAWS-X, and SQuAD respectively (Nangia & Bowman, 2019; Zhang et al., 2019; Rajpurkar et al., 2016).11 For TyDiQA-GoldP, we use the performance estimate of Clark et al. (2020). For MLQA, as answers are annotated using the same format as SQuAD, we employ the same human performance estimate. For POS tagging, we adopt 97% as a canonical estimate of human performance based on Manning (2011). We are not able to obtain human performance estimates for NER as annotations have been automatically generated and for sentence retrieval as identifying a translation among a large number of documents is too time-consuming.
4.3. Results
Overall results We show the main results in Table 2. XLMR is the best-performing zero-shot transfer model and generally improves upon mBERT signiﬁcantly. The improvement is smaller, however, for the structured prediction tasks. MMTE achieves performance competitive with mBERT on most tasks, with stronger results on XNLI, POS, and BUCC.
11Performance may differ across languages due to many factors but English performance still serves as a reasonable proxy.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 2. Overall results of baselines across all XTREME tasks. Translation-based baselines are not meaningful for sentence retrieval. We provide in-language baselines where target language training data is available. Note that for the QA tasks, translate-test performance is not directly comparable to the other scores as a small number of test questions were discarded and alignment is measured on the English data.

Model

Avg Pair sentence Structured prediction

XNLI PAWS-X POS

NER

Question answering

Sentence retrieval

XQuAD MLQA TyDiQA-GoldP BUCC Tatoeba

Metrics

Acc. Acc. F1

F1

F1 / EM F1 / EM

F1 / EM

F1

Acc.

Cross-lingual zero-shot transfer (models are trained on English data)

mBERT XLM XLM-R Large MMTE

59.8 65.4 81.9 71.5

62.2

64.5 / 49.4 61.4 / 44.2 59.7 / 43.9

56.7 38.7

55.7 69.1 80.9 71.3

61.2

59.8 / 44.3 48.5 / 32.6 43.6 / 29.1

56.8 32.6

68.2 79.2 86.4 73.8

65.4

76.6 / 60.8 71.6 / 53.2 65.1 / 45.0

66.0 57.3

59.5 67.4 81.3 73.5

58.3

64.4 / 46.2 60.3 / 41.4 58.1 / 43.8

59.8 37.9

Translate-train (models are trained on English training data translated to the target language)

mBERT mBERT, multi-task

- 74.6 86.3

-

- 75.1 88.9

-

-

70.0 / 56.0 65.6 / 48.0 55.1 / 42.1

-

-

-

72.4 / 58.3 67.6 / 49.8 64.2 / 49.3

-

-

Translate-test (models are trained on English data and evaluated on target language data translated to English)

BERT-large

- 76.8 84.4

-

-

76.3 / 62.1 72.9 / 55.3 72.1 / 56.0

-

-

In-language models (models are trained on the target language training data)

mBERT, 1000 examples -

-

-

87.6

77.9

-

-

58.7 / 46.5

-

-

mBERT

-

-

-

89.8

88.3

-

-

74.5 / 62.7

-

-

mBERT, multi-task

-

-

-

91.5

89.1

-

-

77.6 / 68.0

-

-

Human

- 92.8 97.5 97.0

-

91.2 / 82.3 91.2 / 82.3

90.1 / -

-

-

If a strong MT system is available, translating the training sets provides improvements over using the same model with zero-shot transfer. Translating the test data provides similar beneﬁts compared to translating the training data and is particularly effective for the more complex QA tasks, while being more expensive during inference time. While using an MT system as a black box leads to strong baselines, the MT system could be further improved in the context of data augmentation.
For the tasks where in-language training data is available, multilingual models trained on in-language data outperform zero-shot transfer models. However, zero-shot transfer models nevertheless outperform multilingual models trained on only 1,000 in-language examples on the complex QA tasks as long as more samples in English are available. For the structured prediction tasks, 1,000 in-language examples enable the model to achieve performance that is similar to being trained on the full labelled dataset, similar to ﬁndings for classiﬁcation (Eisenschlos et al., 2019). Finally, multitask learning on the Translate-train and In-language setting generally improves upon single language training.
Cross-lingual transfer gap For a number of representative models, we show the cross-lingual transfer gap, i.e. the difference between the performance on the English test set and all other languages in Table 3.12 While powerful models
12This comparison should be taken with a grain of salt, as scores across languages are not directly comparable for the tasks where test sets differ, i.e. POS, NER, MLQA, and TyDiQA-GoldP and differences in scores may not be linearly related.

Table 3. The cross-lingual transfer gap (lower is better) of different models on XTREME tasks. The transfer gap is the difference between performance on the English test set and the average performance on the other languages. A transfer gap of 0 indicates perfect cross-lingual transfer. For the QA datasets, we only show EM scores. The average gaps are computed over the sentence classiﬁcation and QA tasks.

Model

XNLI PAWS-X XQuAD MLQA TyDiQA-GoldP Avg POS NER

mBERT

16.5 14.1 25.0 27.5

22.2

XLM-R

10.2 12.4 16.3 19.1

13.3

Translate-train 7.3 9.0 17.6 22.2

24.2

Translate-test 6.7 12.0 16.3 18.3

11.2

21.1 25.5 23.6 14.3 24.3 19.8 16.1 - 12.9 - -

such as XLM-R reduce the gap signiﬁcantly compared to mBERT for challenging tasks such as XQuAD and MLQA, they do not have the same impact on the syntactic structured prediction tasks. On the classiﬁcation tasks, the transfer learning gap is lowest, indicating that there may be less headroom for progress on these tasks. The use of MT reduces the gap across all tasks. Overall, a large gap remains for all approaches, which indicates much potential for work on cross-lingual transfer.
5. Analyses
We conduct a series of analyses investigating the limitations of state-of-the-art cross-lingual models.
Best zero-shot model analysis We show the performance of the best zero-shot transfer model, XLM-R Large broken down by task and language in Figure 1. The ﬁgure illus-

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 4. Accuracy of mBERT on POS tag trigrams and 4-grams in the target language dev data that appeared and did not appear in the English training data. We show the performance on English, the average across all other languages, and their difference.

trigram, trigram, 4-gram, 4-gram,

seen

unseen seen

unseen

en

90.3

63.0

88.1

67.5

avg w/o en 50.6

12.1

44.3

18.3

difference 39.7

50.9

43.7

49.2

Figure 1. An overview of XLM-R’s performance on the XTREME tasks across all languages in each task. We highlight an estimate of human performance, performance on the English test set, the average of all languages excluding English, and the family of each language. Performance on pseudo test sets for XNLI and XQuAD is shown with slightly transparent markers.
trates why it is important to evaluate general-purpose multilingual representations across a diverse range of tasks and languages: On XNLI, probably the most common standard cross-lingual evaluation task, and PAWS-X, scores cluster in a relatively small range—even considering pseudo test sets for XNLI. However, scores for the remaining tasks have signiﬁcantly wider spread, particularly as we include pseudo test sets. For TyDiQA-GoldP, English performance is lowest in comparison; the high performance on members of the Austronesian and Uralic language families (Indonesian and Finnish) may be due to less complex Wikipedia context passages for these languages. Across tasks, we generally observe higher performance on Indo-European languages and lower performance for other language families, particularly for Sino-Tibetan, Japonic, Koreanic, and Niger-Congo languages. Some of these difﬁculties may be due to tokenisation and an under-representation of ideograms in the joint sentencepiece vocabulary, which has been shown to be important in a cross-lingual model’s performance (Artetxe et al., 2020; Conneau et al., 2020). We observe similar trends for mBERT, for which we show the same graph in the appendix.
Correlation with pretraining data size We calculate the Pearson correlation coefﬁcient ρ of the model performance and the number of Wikipedia articles (see appendix) in each language and show results in Figure 2.13 For mBERT, which was pretrained on Wikipedia, we observe a high correlation for most tasks (ρ ≈ 0.8) except for the structured prediction tasks where ρ ≈ 0.35. We observe similar trends for XLM and XLM-R, with lower numbers for XLM-R due to the
13We observe similar correlations when using the number of tokens in Wikipedia instead.

different pretraining domain (see the appendix). This indicates that current models are not able to fully leverage the information extracted from the pretraining data to transfer to syntactic tasks.
Analysis of language characteristics We analyze results based on different language families and writing scripts in Figure 3. For mBERT, we observe the best transfer performance on branches of the Indo-European language family such as Germanic, Romance and Slavic languages. In contrast, cross-lingual transfer performance on low-resource language families such as Niger-Congo and Kra-Dai is still low. Looking at scripts, we ﬁnd that the performance on syntactic tasks differs among popular scripts such as Latin and ideogram scripts. For example in the NER task, mBERT performs better on data in Latin script than that in Chinese or Japanese ideograms. This indicates that the current models still have difﬁculty transferring word-level syntactic information across languages written in different scripts.
Errors across languages For XNLI and XQuAD where the other test sets are translations from English, we analyze whether approaches make the same type of errors in the source and target languages. To this end, we explore whether examples that are correctly and incorrectly predicted in English are correctly predicted in other languages. On the XNLI dev set, mBERT correctly predicts on average 71.8% of examples that were correctly predicted in English. For examples that were misclassiﬁed, the model’s performance is about random. On average, predictions on XNLI are consistent between English and another language for 68.3% of examples. On the XQuAD test set, mBERT correctly predicts around 60% of examples that were correclty predicted in English and 20% of examples that were incorrectly predicted. While some of these are plausible spans, more work needs to focus on achieving consistent predictions across languages.
Generalization to unseen tag combinations and entities We analyze possible reasons for the less successful transfer on structured prediction tasks. The Universal Dependencies dataset used for POS tagging uses a common set of 17 POS tags for all languages, so a model is not required to

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Figure 2. Performance of mBERT across tasks and languages in comparison to the number of Wikipedia articles for each language. We show tasks with a Pearson correlation coefﬁcient ρ > 0.7 on the left and others on the right. Numbers across tasks are not directly comparable. We remove the x axis labels of overlapping languages for clarity. We additionally plot the linear ﬁt for each task (curved due to the logarithmic scale of the x axis).

Figure 3. Performance of mBERT across tasks grouped by language families (left) and scripts (right). The number of languages per group is in brackets and the groups are from low-resource to high-resource on the x-axis. We additionally plot the 3rd order polynomial ﬁt for the minimum and maximum values for each group.

generalize to unseen tags at test time. However, a model may be required to generalize to unseen tag combinations at test time, for instance due to differences in word order between languages. We gauge how challenging such generalization is by computing a model’s accuracy for POS tag n-grams in the target language dev data that were not seen in the English training data. We calculate values for tag trigrams and 4-grams and show accuracy scores for mBERT in Table 4. We observe the largest differences in performance for unseen trigrams and 4-grams, which highlights that existing cross-lingual models struggle to transfer to the syntactic characteristics of other languages. For NER, we estimate how well models generalize to unseen entities at test time. We compute mBERT’s accuracy on entities in the target language dev data that were not seen in the English training data. We observe the largest difference between performance on seen and unseen entities for Indonesian and Swahili. Isolating for confounding factors such as entity length, frequency, and Latin script, we ﬁnd the largest differ-

ences in performance for Swahili and Basque. Together, this indicates that the model may struggle to generalize to entities that are more characteristic of the target language. We show the detailed results for both analyses in the appendix.
6. Conclusions
As we have highlighted in our analysis, a model’s crosslingual transfer performance varies signiﬁcantly both between tasks and languages. XTREME is a ﬁrst step towards obtaining a more accurate estimate of a model’s crosslingual generalization ability. While XTREME is still inherently limited by the data coverage of its constituent tasks for many low-resource languages, XTREME nevertheless provides signiﬁcantly broader coverage and more ﬁne-grained analysis tools to encourage research on cross-lingual generalization ability of models. We have released the code for XTREME and scripts for ﬁne-tuning models on tasks in XTREME, which should be to catalyze future research.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Acknowledgements
We’d like to thank Jon Clark for sharing with us the TyDiQA Gold Passage data and for valuable feedback. We would also like to thank Sam Bowman, Sebastian Goodman, and Tal Linzen for their feedback. JH and GN are sponsored by the Air Force Research Laboratory under agreement number FA8750-19-2-0200.
References
Agic´, Zˇ . and Schluter, N. Baselines and test data for crosslingual inference. In Proceedings of LREC 2018, 2018.
Aharoni, R., Johnson, M., and Firat, O. Massively Multilingual Neural Machine Translation. In Proceedings of NAACL 2019, 2019.
Arivazhagan, N., Bapna, A., Firat, O., Lepikhin, D., Johnson, M., Krikun, M., Chen, M. X., Cao, Y., Foster, G., Cherry, C., Macherey, W., Chen, Z., and Wu, Y. Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges. arXiv preprint arXiv:1907.05019, 2019.
Artetxe, M. and Schwenk, H. Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond. Transactions of the ACL 2019, 2019.
Artetxe, M., Labaka, G., and Agirre, E. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of ACL 2017, pp. 451–462, 2017.
Artetxe, M., Labaka, G., and Agirre, E. A robust selflearning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018, pp. 789–798, 2018.
Artetxe, M., Ruder, S., and Yogatama, D. On the Crosslingual Transferability of Monolingual Representations. In Proceedings of ACL 2020, 2020.
Barnes, J., Klinger, R., and Schulte im Walde, S. Bilingual sentiment embeddings: Joint projection of sentiment across languages. In Proceedings of ACL 2018, pp. 2483–2493, Melbourne, Australia, 2018. Association for Computational Linguistics.
Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. In Transactions of the Association of Computational Linguistics, 2020.
Conneau, A., Lample, G., Ranzato, M., Denoyer, L., and Je´gou, H. Word translation without parallel data. In Proceedings of ICLR 2018, 2018a.

Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk, H., and Stoyanov, V. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of EMNLP 2018, pp. 2475–2485, 2018b.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzma´n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of ACL 2020, 2020.
Czarnowska, P., Ruder, S., Grave, E., Cotterell, R., and Copestake, A. Don’t forget the long tail! a comprehensive analysis of morphological generalization in bilingual lexicon induction. In Proceedings of EMNLP 2019, pp. 973–982, 2019.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL 2019, 2019.
Eisenschlos, J., Ruder, S., Czapla, P., Kadras, M., Gugger, S., and Howard, J. MultiFiT: Efﬁcient Multi-lingual Language Model Fine-tuning. In Proceedings of EMNLP 2019, 2019.
Eriguchi, A., Johnson, M., Firat, O., Kazawa, H., and Macherey, W. Zero-shot cross-lingual classiﬁcation using multilingual neural machine translation. arXiv preprint arXiv:1809.04686, 2018.
Faruqui, M. and Dyer, C. Improving vector space word representations using multilingual correlation. In Proceedings of EACL 2014, pp. 462–471, 2014.
Glavasˇ, G., Litschko, R., Ruder, S., and Vulic´, I. How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions. In Proceedings of ACL 2019, 2019.
Gouws, S., Bengio, Y., and Corrado, G. BilBOWA: Fast bilingual distributed representations without word alignments. In Proceedings of ICML 2015, pp. 748–756, 2015.
Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S. R., and Smith, N. A. Annotation Artifacts in Natural Language Inference Data. In Proceedings of NAACL-HLT 2018, 2018.
Guzma´n, F., Chen, P.-J., Ott, M., Pino, J., Lample, G., Koehn, P., Chaudhary, V., and Ranzato, M. The FLoRes Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English. In Proceedings of EMNLP 2019, pp. 6100–6113, 2019.
Howard, J. and Ruder, S. Universal language model ﬁnetuning for text classiﬁcation. In Proceedings of ACL 2018, pp. 328–339, 2018.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Hsu, T.-y., Liu, C.-l., and Lee, H.-y. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model. In Proceedings of EMNLP 2019, pp. 5935–5942, 2019.
Kementchedjhieva, Y., Hartmann, M., and Søgaard, A. Lost in evaluation: Misleading benchmarks for bilingual dictionary induction. In Proceedings of EMNLP 2019, pp. 3327–3332, 2019.
Klementiev, A., Titov, I., and Bhattarai, B. Inducing Crosslingual Distributed Representations of Words. In Proceedings of COLING 2012, 2012.
Koppel, M. and Ordan, N. Translationese and its dialects. In Proceedings of ACL 2011, pages=1318–1326, year=2011, organization=Association for Computational Linguistics.
Lample, G. and Conneau, A. Cross-lingual Language Model Pretraining. In Proceedings of NeurIPS 2019, 2019.
Lee, K., Yoon, K., Park, S., and Hwang, S. W. Semisupervised training data generation for multilingual question answering. In Proceedings of LREC 2018, pp. 2758– 2762, 2018.
Lewis, P., Ouz, B., Rinott, R., Riedel, S., and Schwenk, H. MLQA: Evaluating Cross-lingual Extractive Question Answering. arXiv preprint arXiv:1910.07475, 2019.
Lin, Y.-H., Chen, C.-Y., Lee, J., Li, Z., Zhang, Y., Xia, M., Rijhwani, S., He, J., Zhang, Z., Ma, X., Anastasopoulos, A., Littell, P., and Neubig, G. Choosing Transfer Languages for Cross-Lingual Learning. In Proceedings of ACL 2019, 2019.
Luong, T., Pham, H., and Manning, C. D. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pp. 151–159, 2015.
Manning, C. D. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In International conference on intelligent text processing and computational linguistics, pp. 171–189. Springer, 2011.
McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Proceedings of NIPS 2017, pp. 6294–6305, 2017.
McDonald, R., Petrov, S., and Hall, K. Multi-source transfer of delexicalized dependency parsers. In Proceedings of EMNLP 2011, pp. 62–72, 2011.
Mikolov, T., Le, Q. V., and Sutskever, I. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013.

Mohammad, S. M., Salameh, M., and Kiritchenko, S. How translation alters sentiment. Journal of Artiﬁcial Intelligence Research, 55:95–130, 2016.
Nangia, N. and Bowman, S. R. Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark. In Proceedings of ACL 2019, pp. 4566–4575, 2019.
Nivre, J., Abrams, M., Agic´, Zˇ ., Ahrenberg, L., Antonsen, L., Aranzabe, M. J., Arutie, G., Asahara, M., Ateyah, L., Attia, M., et al. Universal dependencies 2.2. 2018.
Pan, X., Zhang, B., May, J., Nothman, J., Knight, K., and Ji, H. Cross-lingual name tagging and linking for 282 languages. In Proceedings of ACL 2017, pp. 1946–1958, 2017.
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Proceedings of NAACL 2018, pp. 2227–2237, 2018.
Pires, T., Schlinger, E., and Garrette, D. How multilingual is Multilingual BERT? In Proceedings of ACL 2019, 2019.
Popovic´, M. chrF: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392–395, Lisbon, Portugal, 2015.
Rahimi, A., Li, Y., and Cohn, T. Massively Multilingual Transfer for NER. In Proceedings of ACL 2019, 2019.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of EMNLP 2016, 2016.
Ruder, S., Vulic´, I., and Søgaard, A. A Survey of Crosslingual Word Embedding Models. Journal of Artiﬁcial Intelligence Research, 65:569–631, 2019.
Schuster, T., Ram, O., Barzilay, R., and Globerson, A. Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing. In Proceedings of NAACL 2019, 2019.
Schwenk, H. and Li, X. A Corpus for Multilingual Document Classiﬁcation in Eight Languages. In Proceedings of LREC 2018, 2018.
Siddhant, A., Johnson, M., Tsai, H., Arivazhagan, N., Riesa, J., Bapna, A., Firat, O., and Raman, K. Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation. In Proceedings of AAAI 2020, 2020.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Smith, L., Giorgi, S., Solanki, R., Eichstaedt, J., Schwartz, H. A., Abdul-Mageed, M., Buffone, A., and Ungar, L. Does well-being translate on twitter? In Proceedings of EMNLP 2016, pp. 2042–2047, 2016.
Snyder, B., Naseem, T., and Barzilay, R. Unsupervised multilingual grammar induction. In Proceedings of ACL 2009, pp. 73–81, 2009.
Ta¨ckstro¨m, O., Das, D., Petrov, S., McDonald, R., and Nivre, J. Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging. In Transactions of the Association for Computational Linguistics, 2013.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention Is All You Need. In Proceedings of NIPS 2017, 2017.
Vulic´, I., Glavasˇ, G., Reichart, R., and Korhonen, A. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings? In Proceedings of EMNLP 2019, 2019.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Superglue: A stickier benchmark for general-purpose language understanding systems. In Proceedings of NeurIPS 2019, 2019a.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of ICLR 2019, 2019b.
Williams, A., Nangia, N., and Bowman, S. R. A BroadCoverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of NAACL-HLT 2018, 2018.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
Wu, S. and Dredze, M. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of EMNLP 2019, 2019.
Yang, Y., Zhang, Y., Tar, C., and Baldridge, J. PAWS-X: A cross-lingual adversarial dataset for paraphrase identiﬁcation. In Proceedings of EMNLP 2019, pp. 3685–3690, 2019.
Zhang, M., Liu, Y., Luan, H., and Sun, M. Earth mover’s distance minimization for unsupervised bilingual lexicon induction. In Proceedings of EMNLP 2017, pp. 1934– 1945, 2017.

Zhang, Y., Baldridge, J., and He, L. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of NAACL 2019, pp. 1298–1308, 2019.
Zweigenbaum, P., Sharoff, S., and Rapp, R. Overview of the second bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pp. 60–67, 2017.
Zweigenbaum, P., Sharoff, S., and Rapp, R. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th Workshop on Building and Using Comparable Corpora, pp. 39–42, 2018.

XTREME: A Benchmark for Evaluating Cross-lingual generalization
A. Languages
We show a detailed overview of languages in the crosslingual benchmark including interesting typological differences in Table 5. Wikipedia information is taken from Wikipedia14 and linguistic information from WALS Online15. XTREME includes members of the Afro-Asiatic, Austro-Asiatic, Austronesian, Dravidian, Indo-European, Japonic, Kartvelian, Kra-Dai, Niger-Congo, Sino-Tibetan, Turkic, and Uralic language families as well as of two isolates, Basque and Korean.

B. Hyper-parameters
Table 6 summarizes the hyper-parameters of baseline and state-of-the-art models. We refer to XLM-100 as XLM, and XLM-R-large as XLM-R in our paper to simplify the notation.
mBERT We use the cased version, which covers 104 languages, has 12 layers, 768 hidden units per layer, 12 attention heads, a 110k shared WordPiece vocabulary, and 110M parameters.16 The model was trained using Wikipedia data in all 104 languages, oversampling low-resource languages with an exponential smoothing factor of 0.7. We generally ﬁne-tune mBERT for two epochs, with a training batch size of 32 and a learning rate of 2e-5. For training BERT models on the QA tasks, we use the original BERT codebase. For all other tasks, we use the Transformers library (Wolf et al., 2019).
XLM and XLM-R We use the XLM and XLM-R Large versions that cover 100 languages, use a 200k shared BPE vocabulary, and that have been trained with masked language modelling.17 We ﬁne-tune both for two epochs with a learning rate of 3e-5 and an effective batch size of 16. In contrast to XLM, XLM-R does not use language embeddings. We use the Transformers library for training XLM and XLM-R models on all tasks.
C. Translations for QA datasets
We use an in-house translation tool to obtain translations for our datasets. For the question answering tasks (XQuAD and MLQA), the answer span is often not recoverable if the context is translated directly. We experimented with enclosing the answer span in the English context in quotes (Lee et al., 2018; Lewis et al., 2019) but found that quotes were often dropped in translations (at different rates depending
14https://meta.wikimedia.org/wiki/List_of_ Wikipedias
15https://wals.info/languoid 16https://github.com/google-research/bert/ blob/master/multilingual.md 17https://github.com/facebookresearch/XLM

Figure 4. An overview of mBERT’s performance on the XTREME tasks for the languages of each task. We highlight an estimate of human performance, performance on the English test set, the average of all languages excluding English, and the family of each language. Performance on pseudo test sets for XNLI and XQuAD is shown with slightly transparent markers.
on the language). We found that enclosing the answer span in HTML tags (e.g. <b> and </b>) worked more reliably. If this fails, as a back-off we fuzzy match the translated answer with the context similar to (Hsu et al., 2019). If the minimal edit distance between the closest match and the translated answer is larger than min(10, answer len/2), we drop the example. On the whole, using this combination, we recover more than 97% of all answer spans in training and test data.
D. Performance on translated test sets
We show results comparing the performance of mBERT and translate-train (mBERT) baselines on the XQuAD test sets with automatically translated test sets in Table 7. Performance on the automatically translated test sets underestimates the performance of mBERT by 2.9 F1 / 0.2 EM points but overestimates the performance of the translatetrain baseline by 4.0 F1 / 6.7 EM points. The biggest part of this margin is explained by the difference in scores on the Thai test set. Overall, this indicates that automatically translated test sets are useful as a proxy for cross-lingual performance but may not be reliable for evaluating models that have been trained on translations as these have learnt to exploit the biases of translationese.
E. mBERT performance across tasks and languages
We show the performance of mBERT across all tasks and languages of XTREME in Table 4.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 5. Statistics about languages in the cross-lingual benchmark. Languages belong to 12 language families and two isolates, with Indo-European (IE) having the most members. Diacritics / special characters: Language adds diacritics (additional symbols to letters). Compounding: Language makes extensive use of word compounds. Bound words / clitics: Function words attach to other words. Inﬂection: Words are inﬂected to represent grammatical meaning (e.g. case marking). Derivation: A single token can represent entire phrases or sentences.

Language

ISO 639-1 code

# Wikipedia articles (in millions)

Script

Language family

Diacritics / special characters

Extensive compounding

Bound words / clitics

Inﬂection

Derivation

# datasets with language

Afrikaans

af

Arabic

ar

Basque

eu

Bengali

bn

Bulgarian

bg

Burmese

my

Dutch

nl

English

en

Estonian

et

Finnish

ﬁ

French

fr

Georgian

ka

German

de

Greek

el

Hebrew

he

Hindi

hi

Hungarian hu

Indonesian id

Italian

it

Japanese

ja

Javanese

jv

Kazakh

kk

Korean

ko

Malay

ms

Malayalam ml

Mandarin

zh

Marathi

mr

Persian

fa

Portuguese pt

Russian

ru

Spanish

es

Swahili

sw

Tagalog

tl

Tamil

ta

Telugu

te

Thai

th

Turkish

tr

Urdu

ur

Vietnamese vi

Yoruba

yo

0.09

Latin

IE: Germanic

1.02

Arabic

Afro-Asiatic

X

0.34

Latin

Basque

X

0.08

Brahmic

IE: Indo-Aryan

X

0.26

Cyrillic

IE: Slavic

X

0.05

Brahmic

Sino-Tibetan

X

1.99

Latin

IE: Germanic

5.98

Latin

IE: Germanic

0.20

Latin

Uralic

X

0.47

Latin

Uralic

2.16

Latin

IE: Romance

X

0.13

Georgian

Kartvelian

2.37

Latin

IE: Germanic

0.17

Greek

IE: Greek

X

0.25

Hebrew

Afro-Asiatic

0.13

Devanagari

IE: Indo-Aryan

X

0.46

Latin

Uralic

X

0.51

Latin

Austronesian

1.57

Latin

IE: Romance

X

1.18

Ideograms

Japonic

0.06

Brahmic

Austronesian

X

0.23

Arabic

Turkic

X

0.47

Hangul

Koreanic

0.33

Latin

Austronesian

0.07

Brahmic

Dravidian

X

1.09

Chinese ideograms Sino-Tibetan

0.06

Devanagari

IE: Indo-Aryan

0.70

Perso-Arabic

IE: Iranian

1.02

Latin

IE: Romance

X

1.58

Cyrillic

IE: Slavic

1.56

Latin

IE: Romance

X

0.05

Latin

Niger-Congo

0.08

Brahmic

Austronesian

X

0.12

Brahmic

Dravidian

X

0.07

Brahmic

Dravidian

X

0.13

Brahmic

Kra-Dai

X

0.34

Latin

Turkic

X

0.15

Perso-Arabic

IE: Indo-Aryan

X

1.24

Latin

Austro-Asiatic

X

0.03

Arabic

Niger-Congo

X

X

3

X

X

7

X

X

X

3

X

X

X

X

3

X

X

4

X

1

X

3

9

X

X

X

3

X

X

3

X

6

X

X

2

X

X

8

X

X

5

X

3

X

X

X

X

6

X

X

X

4

X

X

X

4

X

3

X

X

4

X

1

X

X

1

X

X

X

5

X

X

2

X

X

X

2

X

8

X

X

3

X

2

X

3

X

7

X

7

X

X

X

3

X

X

1

X

X

X

X

3

X

X

X

X

4

4

X

X

X

5

X

X

X

X

4

6

1

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 6. Hyper-parameters of baseline and state-of-the-art models. We do not use XLM-15 and XLM-R-Base in our experiments.

Model

Parameters Langs Vocab size Layers

BERT-large 364,353,862 1

28,996

24

mBERT

178,566,653 104 119,547

12

MMTE

191,733,123 103 64,000

6

XLM-15

346,351,384 15

95,000

12

XLM-100

827,696,960 100 200,000

12

XLM-R-Base 470,295,954 100 250,002

12

XLM-R-Large 816,143,506 100 250,002

24

F. Correlation with pretraining data size
We show the Pearson correlation coefﬁcient ρ of mBERT, XLM, and XLM-R with the number of Wikipedia articles in Table 9. XLM and mBERT were pretrained on Wikipedia, while XLM-R was pretrained on data from the web.
G. Generalization to unseen tag combinations
We show the performance of mBERT on POS tag trigrams and 4-grams that were seen and not seen in the English training data in Table 10.
H. Generalization to unseen entities
We show the performance of mBERT on entities in the target language NER dev data that were seen and not seen in the English NER training data in Table 11. For simplicity, we count an entity as occurring in the English training data if a subset of at least two tokens matches with an entity in the English training data. As most matching entities in the target language data only consist of up to two tokens, are somewhat frequent, and consist only of Latin characters, we provide the performance on all entities ﬁtting each criterion respectively for comparison. For all target languages in the table except Spanish, entities that appeared in the English training data are more likely to be tagged correctly than ones that did not. The differences are largest for two languages that are typologically distant to English, Indonesian (id) and Swahili (sw). For most languages, entities that appear in the English training data are similarly likely to be correctly classiﬁed as entities that are either frequent, appear in Latin characters, or are short. However, for Swahili and Basque (eu), mBERT does much better on entities that appeared in the English training data compared to the comparison entities. Another interesting case is Georgian (ka), which uses a unique script. The NER model is very good at recognizing entities that are written in Latin script but performs less well on entities in Georgian script.

Figure 5. Comparison of mBERT’s sentence representations by averaging word embeddings in each layer in the BUCC task.
I. Sentence representations across all layers
For sentence retrieval tasks, we analyze whether the multilingual sentence representations obtained from all layers are well-aligned in the embedding spaces. Without ﬁne-tuning on any parallel sentences at all, we explore three ways of extracting the sentence representations from all the models: (1) the embeddings of the ﬁrst token in the last layer, also known as [CLS] token; (2) the average word embeddings in each layer; (3) the concatenation of the average word embeddings in the bottom, middle, and top 4 layers, i.e., Layer 1 to 4 (bottom), Layer 5 to 8 (middle), Layer 9 to 12 (top). Figure 5 shows the F1 scores of the average word embeddings in each layer of mBERT in the BUCC task. We observe that the average word embeddings in the middle layers, e.g., Layer 6 to 8, perform better than that in the bottom or the top layers. In Table 14, we show the performance of these three types of sentence embeddings in the BUCC task. The embeddings of the CLS token perform relatively bad in cross-lingual retrieval tasks. We conjecture that the CLS embeddings highly abstract the semantic meaning of a sentence, while they lose the token-level information which is important for matching two translated sentences in two languages. With respect to the concatenation of average word embeddings from four continuous layers, We also observe that embeddings from the middle layers perform better than that from the bottom and top layers. Average word embeddings in the middle individual layer perform comparative to the concatenated embeddings from the middle four layers.
I.1. Language Families and Scripts
We also report the performance of XLM-R in all the tasks across different language families and writing scripts in Figure 6.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 7. Comparison of F1 and EM scores of mBERT and translate-train (mBERT) baselines on XQuAD test sets (gold), which were translated by professional translators and automatically translated test sets (auto).

Test set

es

de

el

ru

tr

ar

vi

th

zh

hi

avg

mBERT

gold 75.6 / 56.9 70.6 / 54.0 62.6 / 44.9 71.3 / 53.3 55.4 / 40.1 61.5 / 45.1 69.5 / 49.6 42.7 / 33.5 58.0 / 48.3 59.2 / 46.0 62.6 / 47.2 auto 76.1 / 58.7 64.3 / 49.9 57.9 / 42.5 68.3 / 51.8 55.6 / 42.9 62.1 / 48.6 68.6 / 54.3 41.1 / 32.6 48.5 / 47.7 54.1 / 40.9 59.7 / 47.0

translate-train gold 80.2 / 63.1 75.6 / 60.7 70.0 / 53.0 75.0 / 59.7 68.9 / 54.8 68.0 / 51.1 75.6 / 56.2 36.9 / 33.5 66.2 / 56.6 69.6 / 55.4 68.7 / 54.6 auto 80.7 / 66.0 71.1 / 58.9 69.3 / 54.5 75.7 / 61.5 71.2 / 59.1 74.3 / 60.7 76.8 / 64.0 79.5 / 74.8 59.3 / 58.0 69.1 / 55.2 72.7 / 61.3

Table 8. Comparison of accuracy scores of mBERT baseline on XNLI test sets (gold), which were translated by professional translators and automatically translated test sets (auto) in 14 languages. BLEU and chrF scores are computed to measure the translation quality between gold and automatically translated test sets.

Languages zh

es

de

ar

ur

ru

bg

el

fr

hi

sw

th

tr

vi avg

auto Acc. 69.1 74.7 72.8 66.5 64.5 71.6 70.2 67.7 74.3 65.1 50.2 54.5 60.0 72.7 66.7 gold Acc. 67.8 73.5 70.0 64.3 57.2 67.8 68.0 65.3 73.4 58.9 49.7 54.1 60.9 69.3 64.3

BLEU chrF

40.92 43.46 30.94 32.35 20.13 22.62 45.04 60.29 47.91 29.55 31.25 10.65 15.39 56.93 34.82 35.96 67.92 60.28 59.64 48.21 50.38 67.52 75.34 69.58 53.85 59.84 54.89 51.46 69.37 58.87

Figure 6. Performance of XLM-R across tasks grouped by language families (left) and scripts (right). The number of languages per group is in brackets and the groups are from low-resource to high-resource on the x-axis. We additionally plot the 3rd order polynomial ﬁt for the minimum and maximum values for each group.

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 9. Pearson correlation coefﬁcients (ρ) of zero-shot transfer

performance and Wikipedia size across datasets and models.

XNLI PAWS-X POS NER XQuAD MLQA TyDiQA-GoldP BUCC Tatoeba

mBERT 0.79

0.81 0.36 0.35 0.80

0.87

0.82

0.95 0.68

XLM

0.80

0.76 0.32 0.29 0.74

0.73

0.52

0.61 0.68

XLM-R 0.75

0.79 0.22 0.27 0.50

0.76

0.14

0.36 0.49

J. Results for each task and language
We show the detailed results for all tasks and languages in Tables 12 (XNLI), 15 (PAWS-X), 20 (POS), 21 (NER), 17 (XQuAD), 19 (MLQA), 18 (TyDiQA-GoldP), 16 (BUCC), and 13 (Tatoeba).

Table 10. Accuracy of mBERT on the target language dev data on POS tag trigrams and 4-grams that appeared and did not appear in the English training data. We show the average performance across all non-English languages and the difference of said average compared to the English performance on the bottom.

trigram, trigram, 4-gram, 4-gram,

seen

unseen seen

unseen

en 90.3

63.0

88.1

67.5

af 68.1

8.2

64.1

24.2

ar 22.0

0.7

14.9

4.6

bg 63.1

14.6

56.1

23.9

de 77.8

47.2

73.0

48.7

el 59.6

9.1

52.5

14.2

es 68.6

10.6

62.4

24.9

et 60.7

14.4

53.1

31.9

eu 32.8

7.1

28.7

8.1

he 52.7

35.7

44.0

27.4

hi 38.7

13.0

32.6

12.5

hu 55.5

28.8

46.9

23.7

id 60.8

16.6

54.7

21.6

it 75.5

12.8

71.8

23.5

ja 16.3

0.0

12.3

1.0

ko 22.0

2.9

14.7

3.8

mr 31.7

0.0

25.5

3.3

nl 75.5

24.1

71.0

37.8

pt 76.2

14.9

71.2

30.6

ru 69.1

4.8

63.8

20.6

ta 30.3

0.0

24.5

4.2

te 57.8

0.0

48.7

24.7

tr 41.2

6.2

33.9

10.1

ur 30.6

18.3

22.3

10.9

zh 29.0

0.0

21.7

3.9

avg 50.6

12.1

44.3

18.3

diff 39.7

50.9

43.7

49.2

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 11. Comparison of accuracies for entities in the target language NER dev data that were seen in the English NER training data (a); were not seen in the English NER training data (b); only consist of up to two tokens (c); only consist of Latin characters (d); and occur at least twice in the dev data (e). We only show languages where the sets (a–e) contain at least 100 entities each. We show the difference between (a) and (b) and the minimum difference between (a) and (c-e).

af de el en es et eu ﬁ fr he hu id it ka ms nl pt ru sw tr vi

(a) Seen (b) Not seen

94.7 88.3 91.4 91.9 76.3 88.3 83.6 85.3 90.5 78.2 90.7 89.4 88.4 92.3 88.6 93.5 88.6 83.9 96.3 85.2 91.4 82.1 80.2 74.8 84.6 80.4 78.9 69.4 79.8 80.1 56.5 78.3 58.0 81.5 70.2 75.0 82.9 82.3 68.5 66.6 73.7 73.4

(a) − (b)

12.6 8.1 16.5 7.2 -4.1 9.4 14.1 5.5 10.4 21.7 12.3 31.5 6.9 22.1 13.6 10.6 6.4 15.4 29.7 11.6 18.0

(c) Short (d) Latin (e) Freq

86.5 82.9 80.3 88.2 86.6 81.7 72.5 83.9 88.6 66.3 83.7 85.8 87.2 72.5 89.1 87.6 87.8 78.0 65.7 83.1 84.6 83.6 81.2 87.5 86.2 80.0 79.5 70.3 80.3 81.1 77.2 79.9 61.8 82.6 89.6 76.3 84.2 83.0 83.8 70.0 75.0 74.9 87.3 80.6 81.9 91.6 83.4 79.4 68.8 85.7 77.3 66.8 86.0 56.5 88.8 74.3 81.3 87.1 84.4 76.5 49.1 81.9 78.6

min((a) − (c–e)) 7.4 5.4 3.9 0.3 3.7 6.6 11.0 0.4 1.9 1.0 4.7 3.6 0.4 2.7 0.5 5.9 0.8 0.1 26.4 2.2 6.8

Model

Table 12. XNLI accuracy scores for each language. en ar bg de el es fr hi ru sw th tr ur vi zh avg

mBERT XLM XLMR MMTE

80.8 64.3 68.0 70.0 65.3 73.5 73.4 58.9 67.8 49.7 54.1 60.9 57.2 69.3 67.8 65.4 82.8 66.0 71.9 72.7 70.4 75.5 74.3 62.5 69.9 58.1 65.5 66.4 59.8 70.7 70.2 69.1 88.7 77.2 83.0 82.5 80.8 83.7 82.2 75.6 79.1 71.2 77.4 78.0 71.7 79.3 78.2 79.2 79.6 64.9 70.4 68.2 67.3 71.6 69.5 63.5 66.2 61.9 66.2 63.6 60.0 69.7 69.2 67.5

Translate-train 81.9 73.8 77.6 77.6 75.9 79.1 77.8 70.7 75.4 70.5 70.0 74.3 67.4 77.0 77.6 75.1 (multi-task) Translate-train 80.8 73.6 76.6 77.4 75.7 78.1 77.4 71.9 75.2 69.4 70.9 75.3 67.2 75.0 74.1 74.6 Translate-test 85.9 73.1 76.6 76.9 75.3 78.0 77.5 69.1 74.8 68.0 67.1 73.5 66.4 76.6 76.3 76.8

Table 13. Tatoeba results (Accuracy) for each language
Lang. af ar bg bn de el es et eu fa ﬁ fr he hi hu id it ja BERT 42.7 25.8 49.3 17 77.2 29.8 68.7 29.3 25.5 46.1 39 66.3 41.9 34.8 38.7 54.6 58.4 42 XLM 43.2 18.2 40 13.5 66.2 25.6 58.4 24.8 17.1 32.2 32.2 54.5 32.1 26.5 30.1 45.9 56.5 40 XLMR 58.2 47.5 71.6 43 88.8 61.8 75.7 52.2 35.8 70.5 71.6 73.7 66.4 72.2 65.4 77 68.3 60.6
jv ka kk ko ml mr nl pt ru sw ta te th tl tr ur vi zh
BERT 17.6 20.5 27.1 38.5 19.8 20.9 68 69.9 61.2 11.5 14.3 16.2 13.7 16 34.8 31.6 62 71.6 XLM 22.4 22.9 17.9 25.5 20.1 13.9 59.6 63.9 44.8 12.6 20.2 12.4 31.8 14.8 26.2 18.1 47.1 42.2 XLMR 14.1 52.1 48.5 61.4 65.4 56.8 80.8 82.2 74.1 20.3 26.4 35.9 29.4 36.7 65.7 24.3 74.7 68.3

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 14. Three types of sentence embeddings from mBERT in BUCC tasks: (1) CLS token embeddings in the last layer; (2) Average word embeddings in the middle layers, i.e., Layer 6, 7, 8; (3) the concatenation of average word embeddings in the continuous four layers, i.e., Layer 1-4 (bottom layers), Layer 5-8 (middle layers), Layer 9-12 (top layers).

Type

de

fr

zh

ru

CLS Layer 6 Layer 7 Layer 8 Layer 1-4 Layer 5-8 Layer 9-12

3.88 51.29 62.51 64.32
6.98 63.12 53.97

4.73 56.32 62.62 62.46
12.3 63.42 52.68

0.89 41.38 49.99 50.49 12.05 52.84 44.18

2.15 38.81 51.84 53.58
4.33 51.67 43.13

Table 15. PAWS-X accuracy scores for each language.

Model

en de es fr ja ko zh avg

mBERT XLM XLMR MMTE

94.0 85.7 87.4 87.0 73.0 69.6 77.0 81.9 94.0 85.9 88.3 87.4 69.3 64.8 76.5 80.9 94.7 89.7 90.1 90.4 78.7 79.0 82.3 86.4 93.1 85.1 87.2 86.9 72.0 69.2 75.9 81.3

Translate-train 94.0 87.5 89.4 89.6 78.6 81.6 83.5 86.3 Translate-train 94.5 90.5 91.6 91.7 84.4 83.9 85.8 88.9 (multi-task) Translate-test 93.5 88.2 89.3 87.4 78.4 76.6 77.6 84.4

Table 16. BUCC results (F1 scores) for each languages.
Model de fr ru zh avg
BERT 62.5 62.6 51.8 50.0 56.7 XLM 56.3 63.9 60.6 46.6 56.8 XLMR 67.5 66.5 73.5 56.7 66.0 MMTE 67.9 63.9 54.3 53.3 59.8

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Model
mBERT XLM XLMR MMTE
Translate-train Translate-train (multi-task) Translate-test

en
83.5 / 72.2 74.2 / 62.1 86.5 / 75.7 80.1 / 68.1
83.5 / 72.2
86.0 / 74.5
87.9 / 77.1

ar
61.5 / 45.1 61.4 / 44.7 68.6 / 49.0 63.2 / 46.2
68.0 / 51.1
71.0 / 54.1
73.7 / 58.8

Table 17. XQuAD results (F1 / EM) for each language.

de

el

es

hi

ru

th

tr

70.6 / 54.0 66.0 / 49.7 80.4 / 63.4 68.8 / 50.3

62.6 / 44.9 57.5 / 39.1 79.8 / 61.7 61.3 / 35.9

75.5 / 56.9 68.2 / 49.8 82.0 / 63.9 72.4 / 52.5

59.2 / 46.0 56.6 / 40.3 76.7 / 59.7 61.3 / 47.2

71.3 / 53.3 65.3 / 48.2 80.1 / 64.3 68.4 / 45.2

42.7 / 33.5 35.4 / 24.5 74.2 / 62.8 48.4 / 35.9

55.4 / 40.1 57.9 / 41.2 75.9 / 59.3 58.1 / 40.9

75.6 / 60.7 70.0 / 53.0 80.2 / 63.1 69.6 / 55.4 75.0 / 59.7 36.9 / 33.5 68.9 / 54.8

78.8 / 63.9 74.2 / 56.1 82.4 / 66.2 71.3 / 56.2 78.1 / 63.0 38.1 / 34.5 70.6 / 55.7

79.8 / 66.7 79.4 / 65.5 82.0 / 68.4 74.9 / 60.1 79.9 / 66.7 64.6 / 50.0 67.4 / 49.6

vi
69.5 / 49.6 65.8 / 47.6 79.1 / 59.0 70.9 / 50.1
75.6 / 56.2
78.5 / 58.8
76.3 / 61.5

zh
58.0 / 48.3 49.7 / 39.7 59.3 / 50.0 55.8 / 36.4
66.2 / 56.6
67.7 / 58.7
73.7 / 59.1

avg
64.5 / 49.4 59.8 / 44.3 76.6 / 60.8 64.4 / 46.2
70.0 / 56.0
72.4 / 58.3
76.3 / 62.1

Model
mBERT XLM XLM-R MMTE
Translate-train Translate-train (multi-task) Translate-test
Monolingual Monolingual few-shot Joint monolingual

en 75.3 / 63.6 66.9 / 53.9 71.5 / 56.8 62.9 / 49.8 75.3 / 63.6 73.2 / 62.5 75.9 / 65.9 75.3 / 63.6 63.1 / 50.9
77.6 / 69.3

Table 18. TyDiQA-GoldP results (F1 / EM) for each language.

ar

bn

ﬁ

id

ko

ru

sw

62.2 / 42.8 59.4 / 41.2 67.6 / 40.4 63.1 / 39.2

49.3 / 32.7 27.2 / 15.0 64.0 / 47.8 55.8 / 41.9

59.7 / 45.3 58.2 / 41.4 70.5 / 53.2 53.9 / 42.1

64.8 / 45.8 62.5 / 45.8 77.4 / 61.9 60.9 / 47.6

58.8 / 50.0 14.2 / 5.1 31.9 / 10.9 49.9 / 42.6

60.0 / 38.8 49.2 / 30.7 67.0 / 42.1 58.9 / 37.9

57.5 / 37.9 39.4 / 21.6 66.1 / 48.1 63.1 / 47.2

61.5 / 44.1 31.9 / 31.9 62.6 / 49.0 68.6 / 52.0 53.2 / 41.3 53.1 / 33.9 61.9 / 45.5

71.8 / 54.2 49.7 / 36.3 68.1 / 53.6 72.3 / 55.2 58.6 / 47.8 64.3 / 45.3 66.8 / 48.9

68.8 / 49.6 66.7 / 48.1 72.0 / 56.6 76.8 / 60.9 69.2 / 55.7 71.4 / 54.3 73.3 / 53.8

80.5 / 67.0 71.1 / 60.2 75.6 / 64.1 81.3 / 70.4 59.0 / 49.6 72.1 / 56.2 75.0 / 66.7

61.3 / 44.8 58.7 / 49.6 51.4 / 38.1 70.4 / 58.1 45.4 / 38.4 56.9 / 42.6 55.4 / 46.3

82.7 / 69.4 79.6 / 69.9 79.2 / 67.8 68.9 / 72.7 68.9 / 59.4 75.8 / 59.2 81.9 / 74.3

te 49.6 / 38.4 15.5 / 6.9 70.1 / 43.6 54.2 / 45.8 27.4 / 17.5 53.3 / 40.2 75.1 / 59.2 80.2 / 66.4 65.2 / 49.6
83.4 / 70.3

avg 59.7 / 43.9 43.6 / 29.1 65.1 / 45.0 58.1 / 43.8 55.1 / 42.1 64.2 / 49.3 72.1 / 56.0 74.5 / 62.7 58.7 / 46.5
77.6 / 68.0

Model
mBERT XLM XLM-R MMTE
Translate-train Translate-train (multi-task) Translate-test

Table 19. MLQA results (F1 / EM) for each language.

en

ar

de

es

hi

vi

80.2 / 67.0 68.6 / 55.2 83.5 / 70.6
78.5 / –

52.3 / 34.6 42.5 / 25.2 66.6 / 47.1
56.1 / –

59.0 / 43.8 50.8 / 37.2 70.1 / 54.9
58.4 / –

67.4 / 49.2 54.7 / 37.9 74.1 / 56.6
64.9 / –

50.2 / 35.3 34.4 / 21.1 70.6 / 53.1
46.2 / –

61.2 / 40.7 48.3 / 30.2 74 / 52.9
59.4 / –

80.2 / 67.0 55.0 / 35.6 64.4 / 49.4 70.0 / 52.0 60.1 / 43.4 65.7 / 45.5

80.7 / 67.7 58.9 / 39.0 66.0 / 51.6 71.3 / 53.7 62.4 / 45.0 67.9 / 47.6

83.8 / 71.0 65.3 / 46.4 71.2 / 54.0 73.9 / 55.9 71.0 / 55.1 70.6 / 54.0

zh 59.6 / 38.6 40.5 / 21.9 62.1 / 37.0
58.3 / –
63.9 / 42.7
66.0 / 43.9
67.2 / 50.6

avg 61.4 / 44.2 48.5 / 32.6 71.6 / 53.2 60.3 / 41.4
65.6 / 47.9
67.6 / 49.8
71.9 / 55.3

Table 20. POS results (Accuracy) for each language

Lang.

af ar bg de el en es et eu fa ﬁ fr he hi hu id it

mBERT 86.6 56.2 85.0 85.2 81.1 95.5 86.9 79.1 60.7 66.7 78.9 84.2 56.2 67.2 78.3 71.0 88.4 XLM 88.5 63.1 85.0 85.8 84.3 95.4 85.8 78.3 62.8 64.7 78.4 82.8 65.9 66.2 77.3 70.2 87.4 XLMR 89.8 67.5 88.1 88.5 86.3 96.1 88.3 86.5 72.5 70.6 85.8 87.2 68.3 76.4 82.6 72.4 89.4 MMTE 86.2 65.9 87.2 85.8 77.7 96.6 85.8 81.6 61.9 67.3 81.1 84.3 57.3 76.4 78.1 73.5 89.2

ja kk ko mr nl pt ru ta te th tl tr ur vi yo zh avg

mBERT 49.2 70.5 49.6 69.4 88.6 86.2 85.5 59.0 75.9 41.7 81.4 68.5 57.0 53.2 55.7 61.6 71.5 XLM 49.0 70.2 50.1 68.7 88.1 84.9 86.5 59.8 76.8 55.2 76.3 66.4 61.2 52.4 20.5 65.4 71.3 XLMR 15.9 78.1 53.9 80.8 89.5 87.6 89.5 65.2 86.6 47.2 92.2 76.3 70.3 56.8 24.6 25.7 73.8 MMTE 48.6 70.5 59.3 74.4 83.2 86.1 88.1 63.7 81.9 43.1 80.3 71.8 61.1 56.2 51.9 68.1 73.5

XTREME: A Benchmark for Evaluating Cross-lingual generalization

Table 21. NER results (F1 Score) for each language

Lang.

en af ar bg bn de el es et eu fa ﬁ fr he hi hu id it ja jv

mBERT 85.2 77.4 41.1 77.0 70.0 78.0 72.5 77.4 75.4 66.3 46.2 77.2 79.6 56.6 65.0 76.4 53.5 81.5 29.0 66.4 XLM 82.6 74.9 44.8 76.7 70.0 78.1 73.5 74.8 74.8 62.3 49.2 79.6 78.5 57.7 66.1 76.5 53.1 80.7 23.6 63.0 XLMR 84.7 78.9 53.0 81.4 78.8 78.8 79.5 79.6 79.1 60.9 61.9 79.2 80.5 56.8 73.0 79.8 53.0 81.3 23.2 62.5 MMTE 77.9 74.9 41.8 75.1 64.9 71.9 68.3 71.8 74.9 62.6 45.6 75.2 73.9 54.2 66.2 73.8 47.9 74.1 31.2 63.9

ka kk ko ml mr ms my nl pt ru sw ta te th tl tr ur vi yo zh

mBERT 64.6 45.8 59.6 52.3 58.2 72.7 45.2 81.8 80.8 64.0 67.5 50.7 48.5 3.6 71.7 71.8 36.9 71.8 44.9 42.7 XLM 67.7 57.2 26.3 59.4 62.4 69.6 47.6 81.2 77.9 63.5 68.4 53.6 49.6 0.3 78.6 71.0 43.0 70.1 26.5 32.4 XLMR 71.6 56.2 60.0 67.8 68.1 57.1 54.3 84.0 81.9 69.1 70.5 59.5 55.8 1.3 73.2 76.1 56.4 79.4 33.6 33.1 MMTE 60.9 43.9 58.2 44.8 58.5 68.3 42.9 74.8 72.9 58.2 66.3 48.1 46.9 3.9 64.1 61.9 37.2 68.1 32.1 28.9

