arXiv:1809.05161v1 [cs.GT] 13 Sep 2018

An Incentive Mechanism for Crowd Sensing with Colluding Agents
Susu Xu, Weiguang Mao, Yue Cao, Hae Young Noh, Nihar B. Shah Carnegie Mellon University
{susux, weiguanm, yuec1}@andrew.cmu.edu, noh@cmu.edu, nihars@cs.cmu.edu
Abstract
Vehicular mobile crowd sensing is a fast-emerging paradigm to collect data about the environment by mounting sensors on vehicles such as taxis. An important problem in vehicular crowd sensing is to design payment mechanisms to incentivize drivers (agents) to collect data, with the overall goal of obtaining the maximum amount of data (across multiple vehicles) for a given budget. Past works on this problem consider a setting where each agent operates in isolation—an assumption which is frequently violated in practice. In this paper, we design an incentive mechanism to incentivize agents who can engage in arbitrary collusions. We then show that in a “homogeneous” setting, our mechanism is optimal, and can do as well as any mechanism which knows the agents’ preferences a priori. Moreover, if the agents are non-colluding, then our mechanism automatically does as well as any other non-colluding mechanism. We also show that our proposed mechanism has strong (and asymptotically optimal) guarantees for a more general “heterogeneous” setting. Experiments based on synthesized data and real-world data reveal gains of over 30% attained by our mechanism compared to past literature.
1 Introduction
Mobile Crowd Sensing (MCS) is a new and fast-rising community sensing paradigm to fulﬁll the increasing demand of diverse urban sensing data. The recent proliferation of mobile smart devices (e.g., smart phones, smart watches, etc.) provides increasingly capable sources of sensors (e.g., GPS, gyroscope, accelerometer, camera, etc.) to MCS applications. In MCS, crowdsoucer engages a crowd of participants (called agents) to collect sensory data using existing pervasive mobile devices (Ganti et al., 2011; Lane et al., 2010), allowing for collection of diverse data at scale. In an MCS system, after obtaining data from agents, the crowdsoucer cleans and analyzes the data for various applications such as smart city management (Thiagarajan et al., 2009; Dutta et al., 2009) and social applications (Eisenman et al., 2009; Zheng et al., 2013). Vehicular MCS is a type of MCS that employs vehicles such as taxis as agents. It is gaining increasing popularity in recent times particularly for applications such as air/noise pollution monitoring (Zheng et al., 2013; Devarakonda et al., 2013), transportation monitoring (Thiagarajan et al., 2009; Wan et al., 2016), and infrastructure monitoring (Hull et al., 2006). In vehicular MCS, a crowdsoucer employs monetary incentives to motivate agents (drivers) to share spatial-temporal information collected along with their trajectories. Here, agents ﬁrst “bid” the amount of monetary remunerations they wish to receive from the crowdsourcer for the task of collecting and reporting data. The bid by an agent is supposed to reﬂect the “threshold” of remuneration beyond which they will accept to do the task and below which they will not. The crowdsourcer then makes a monetary offer to each agent, which the agent may accept and perform the task. The overall goal of the crowdsourcer is to get the maximum possible number of tasks done (that is, obtain the maximum amount of data) for a given budget, and the goal of the agents is to maximize the monetary remunerations they receive.
Various past works have studied and developed incentive mechanisms to motivate agents to bid truthfully in crowdsensing applications; see (Duan et al., 2012; Yang et al., 2012; Zhao et al., 2014; Jin et al., 2017) and references therein. The methods proposed in past literature, however, operate under the assumption that the agents (drivers) never communicate or collude with each other. This assumption of non-collusion is known to be frequently violated in practice. For example, taxi drivers can discuss and collaborate via their radio systems or in person. Such collusions present signiﬁcant challenges to the design of crowd sensing systems and the collection of quality data with an efﬁcient utilization of the crowdsourcer’s budget. For instance, the recent paper (Ji and Chen, 2017) warns that collusions
1

among agents may result in “serious problems for the platform, such as losing system-wide utility and deteriorating other users’ enthusiasm to participate.” A number of other research studies (Wang et al., 2013, 2014) have shown that with the increasing of colluding agents, it becomes more difﬁcult to eliminate the effects of collusion attacks on the crowd sensing system. A second approach to combat collusions – at least in crowdsourcing applications – is to assume knowledge of some “gold standard” ground truth (Gneiting and Raftery, 2007; Lambert and Shoham, 2009; Shah and Zhou, 2016a; Shah et al., 2015; Shah and Zhou, 2016b) and design proper scoring rules. However, this is a very strong assumption in our crowdsensing setting, and we will not make any such assumption in this paper.
In this paper, we propose a novel incentive mechanism – termed “Insensetive Mechanism”1 – for the crowd sensing problem with colluding agents. We prove that in a setting where the thresholds of agents are “homogeneous”, our Insensetive Mechanism is optimal in that truth-telling is optimal for the agents, and the number of tasks is maximized for the crowdsourcer when agents report truthfully. We then move on to a more general “heterogeneous” threshold setting where we ﬁrst prove the impossibility of any mechanism that can ensure truth-telling; we then show that our proposed Insensetive Mechanism continues to fare quite well even in this setting. In numerical evaluations, we ﬁnd that our proposed mechanism consistently outperforms the state-of-the-art and can lead to gains of 30% or higher in the amount of data collected under a given budget.
The rest of the paper is organized as follows. We formally introduce the problem in Section 2. We then present our main results including our mechanism, theoretical guarantees, and numerical evaluations, in Section 3. We conclude the paper with a discussion in Section 4. This paper also includes proofs of our main theoretical results in appendix.
2 Problem formulation
The system consists of a crowdsourcer with budget B ą 0, and M ě 2 agents, whom we index as 1, . . . , M . Each agent has some private threshold Tm˚ ě 0 such that the agent is incentivized to collect data in that round if and only if the remuneration provided by the crowdsourcer is at least Tm˚ . This threshold captures the willingness of the agent to carry out the data collection task, and is assumed to be constant across the N rounds. This assumption is reasonable as the duration of each round and the entire game are usually short (few hours), and consequently the thresholds of drivers vary little across the rounds (Devarakonda et al., 2013; Hasenfratz et al., 2015). The threshold is known to that agent, but unknown to the crowdsourcer. The entire process comprises N ě 1 rounds of data collection. At the beginning of each round (say, round n P t1, . . . , N u), the agents may communicate with each other, and then each agent (say agent m) bids a number Tm,n to the crowdsourcer. This bid is supposed to represent the true threshold of that agent; however, the agents may be strategic and report a larger number on purpose. We allow for every agent to be cognizant of each others’ true thresholds as well as bids. The crowdsourcer then offers each agent (say, agent m) a reward Rm,n if the agent agrees to collect data in the current round n. Each agent may then accept or reject the reward. If an agent accepts, then the agent collects and reports data to the crowdourcer in that round and obtains the reward Rm,n; otherwise the agent does not collect data or receive the reward. We term the data obtained by the crowdsourcer from each agent for each round as one data point.
The overall goal of the crowdsourcer is to maximize the number of data points obtained across N rounds, subject to the budget constraint. As we assume that the entire incentive mechanism employed is publically known, it is possible for the agents to collude and exploit the mechanism to maximize their own utility. We formulate the objective of the crowdsourcer and the agents in the sections below.
2.1 Objective of the crowdsourcer
The objective of the crowdsourcer is to design an appropriate incentive mechanism to ensure that after N rounds, the budget is best utilized to collect as much data as possible. The objective function refers to the amount of accepted tasks, which equals to the ﬁnal amount of collected data. We formulate the optimization problem for crowdsourcer in
1The name captures the fact that it is an incentive mechanism to crowd sense, and also highlights its primary feature that it is insensitive to collusions among agents.
2

Equation (1) below:

max
tRm,n um“1...M,n“1...N

NM
ÿ ÿ IpRm,n ě Tm˚ q
n“1 m“1

NM

ÿÿ

such that

Rm,n ď B

n“1 m“1

(1a) (1b)

Here I denotes the indicator function. The objective of the crowdsourcer is to maximize the total number of data
points collected (1a), or in other words, the total number of tasks accepted by the agents. Note that any agent m during round n will agree to perform the task if and only if Rm,n ě Tm˚ . The threshold Tn˚ represents the minimum monetary incentive required for the agent motivate herself/himself to accept a task (Angelopoulos et al., 2014). We also note that
the crowdsourcer cannot simply solve the problem (1) as a standard optimization problem because it does not know the true thresholds tTm˚ uM m“1 of the agents.
In this paper, we will measure the efﬁciency of any mechanism in terms of its regret deﬁned as follows. We let α˚
denote the optimal value of the objective (1) that the crowdsourcer can achieve in the hypothetical situation that all agents always report their thresholds truthfully. In this case, the values of thresholds tTm˚ uM m“1 become known to the crowdsourcer, and the problem (1) then turns out to be a simple packing problem. Then for any proposed mechanism
to elicit the thresholds as bids from the agents, we denote the (expected) value of the objective (1a) achieved by the
mechanism as α. We then deﬁne the regret β of this mechanism as r

β :“ α˚ ´ α. r

The goal is to design incentive mechanisms that have the smallest regret β possible.

2.2 Objective of agents

The objective of an agent is to maximize the total remuneration they earn across the N rounds. Speciﬁcally, any agent m will report thresholds tTm,nuNn“1 that maximize the objective

N

ÿ Rm,n IpRm,n ě Tm˚ q.

(2)

n“1

Here, tRm,nuNn“1 is the set of remunerations provided by the crowdsourcer’s mechanism for the reported bids of the agents.2 Finally, if there exist multiple strategies to obtain the maximum proﬁt, the agent will follow the strategy that requires accepting the smallest number of tasks.

2.3 Equilibrium of colluding agents
In this section we discuss the idea of equilibrium among agents that might collude in price rigging. We begin by noting that because of the possible collusion, the widely used Nash equilibrium is inappropriate in our context. Hence we instead employ the notion of perfect cooperative equilibrium (PCE) (Rong and Halpern, 2014) that is better suited to model colluding agents.
In order to formally deﬁne PCE, we ﬁrst introduce the concepts of best response and best utility. Consider an M -agent game G with a set of agents denoted as rM s. Let Si be the set of strategies for any agent i P rM s, and si P Si the strategy that agent i will take, s´i the (vector of) strategies that agents in rM sztiu take in game Gsi . The utility obtained by agent i taking strategy si and others taking s´i is denoted as Uipsi, s´iq.
The notion of best response in the game G is then deﬁned as such: Let Gsi be the (M -1)-agent game among agents in the set rM sztiu when agent i takes strategy si. The strategy si is called a best response in game G if si maximizes agent i’s expected utility Uipsi, s´iq given that the other agents are playing a Nash Equilibrium s´i P N EGpsiq in Gsi .
2One may additionally want to subtract out any costs incurred by the agents from their objective. However, in the practical setting of crowd sensing from taxi drivers, the agents suffer only a negligible cost (Zhang et al., 2016) as they only need to perform a few operations on their phone.

3

Given the ﬁxed budget B, Ui˚ is the upper bound of utility agent i can obtain for any possible strategy si when other agents reach a Nash Equilibrium N EGpsiq in Gsi correspondingly. We deﬁne this upper bound, Ui˚ as the best utility.

Ui˚ :“

sup

Uipsi, s´iq.

siPSi,s´iPN EGpsiq

With these preliminaries, we are now ready to deﬁne PCE.

Deﬁnition 1 (Perfect cooperative equilibrium, PCE). A strategy proﬁle s is a perfect cooperative equilibrium (PCE) in an M -agent game if for every agent i P rM s, we have

Uipsi, s´iq ě Ui˚.

The deﬁnition of PCE coincides with the intuition if a strategy generates greater or equal utility for an agent than the utility obtained by her/him acting selﬁshly, then it is stable and will be favored by all agents.

3 Main results
In this section, we present our proposed Insensetive Mechanism and provide associated guarantees.
3.1 Insensetive Mechanism
We begin with an intuition of our proposed Insensetive Mechanism. To achieve the objective of the crowdsourcer, we need an incentive mechanism that prevents agents from bid rigging. The key idea here is to differentiate the agents so that for any potential collusion, there exist some disadvantaged agents who are incentivized to betray and compete. Compared to conventional incentive mechanisms (Prelec, 2004; Kamble et al., 2015; Jin et al., 2017; Zheng et al., 2017) which treat players symmetrically, Insensetive Mechanism is asymmetric, in the sense that it selects and pays the agents depending on their (arbitrary yet ﬁxed) indices. As a simple example to illustrate this point, consider two agents with indices i ă j, who agree to always bid at the same price. If we can only afford to choose one agent for each round, then our Insensetive Mechanism always selects agent i. A second key idea in the construction of Insensetive Mechanism is to identify and address two extremal strategies that could be excuted by the colluding agents, and suitably smooth out the resulting mechanism to handle all non-extremal strategies as well. The ﬁrst extremal strategy we identify is when every agent bids very high initially hoping that the mechanism will be fooled into believing that their true thresholds are very high and thereby paying a large amount of budget for fewer tasks. In order to circumvent this attack, our algorithm imposes a carefully chosen upper bound on the reward paid in each round, and this upper bound depends on the budget and the number of rounds that remains. The second extremal strategy we identify is when every agent bids low initially and rejects the rewards from initial rounds so that the mechanism may pay a higher amount (per task) for later tasks due to a large leftover budget. We foil this attack by ensuring that the budget is distributed across different rounds in a relatively uniform manner.
With these intuitions, we are now ready to formally present the Insensetive Mechanism in Algorithm 1. We parse through the details of the mechanism in the remainder of this subsection. The mechanism ﬁrst calibrates current estimation of each agent’s threshold based on their bids in the current round and the estimated thresholds from the previous round. After updating the thresholds of all the agents, R, the amount of incentive given for completing one task in the current round, is calculated by taking the minimum among T1,n, ¨ ¨ ¨ , TM,n. Notice that R is upper bounded by pB ´ Busedq{pN ´ n ` 1q, which helps even out the amount of budget spent in each round. This also implies that if minptTm,nuM q ą B ´ Bused , then we still try to pay one agent pB ´ Busedq{pN ´ n ` 1q instead of not paying
m“1 N ´ n ` 1 anyone, as a strategy to elicit a potentially lower threshold from some dishonest agents. Based on the amount of reward R for a single task, we decide K, the number of agents we select in this round.
The selection process is done by stable sorting the list of all agents based on their bids. The original list is ordered by agent’s index, so that 1) agents with smaller bids are prioritized over those with larger bids, and 2) agents with smaller indices are always preferred over those with larger indices. This stable sort step is known to all agents, and intentionally creates a differentiation among agents. Thus in any potential collusion, there exists a most disadvantaged

4

Algorithm 1: Insensetive Mechanism

Input :Total budget B, Number of rounds N , Number of agents M , Set of agents rM s “ tmuM m“1 Output : incentives collected tRm,n|m P rM s, n P N u 1 initialize Rm,n Ð 0 for each agent m, round n 2 initialize thresholds Tm,0 Ð 8 for each agent m 3 initialize allocated budget Bused Ð 0 4 broadcast B, N, M 5 for n Ð 1 to N do 6 for m Ð 1 to M do
// Thresholds update

7

collect submitted bid of agent m as Tm,n

8

Tm,n Ð minpTm,n, Tm,n´1q

9 end

// Calculate incentive

´

B ´ Bused ¯

10 R Ð min T1,n, ¨ ¨ ¨ , TM,n,

N ´n`1

// Select agents

11 K Ð Y B ´ Bused ] R ˆ pN ´ n ` 1q

12 stable sort the list of agents tmuM m“1 in ascending order according to their thresholds tTm,nuM m“1, choose top

K agents as selected

// Task allocation

13 for m P selected do

14

assign task to m with incentive R

15

if m accepts task then

16

Rm,n Ð R // Assign incentive

17

Tm,n Ð Rm,n // Calibrate threshold

18

Bused Ð Bused ` Rm,n

19

end

20 end

21 end

agent who is always the last to be selected and can only collect the leftover compared to other group members. Under our Insensetive Mechanism, this disadvantaged agent knows that she/he can only gain more if 1) she/he is prioritized in the selection step, which means she/he has to bid lower than others, or 2) there is more budget left when she/he gets selected, which means that agents with smaller indices must get fewer payment per round. In any case, this agent is incentivized to defect and bring down the bids.
Finally, if every agent bids higher than the upper bound for each round in order to save budget for later rounds, we will try to pay R to see if some of the agents will accept it. If the task is accepted with R, we will update Bused and our estimation of agent’s threshold Tm,n.
3.2 Optimality with homogeneous thresholds
In this section, we present the optimality guarantee for our proposed Insensetive Mechanism for a setting with “homogeneous” thresholds. In this homogeneous-threshold setting, we assume that all agents have the same threshold, that is, T1˚ “ ¨ ¨ ¨ “ TM˚ “ T ˚. The following results show that our mechanism is optimal in this homogeneous setting.
Theorem 1. In the homogeneous threshold setting, under the Insensetive Mechanism, the honest strategy is a perfect cooperative equilibrium (PCE) for all agents, and the Insensetive Mechanism achieves a zero regret.
This result shows that when the thresholds are homogeneous, the Insensetive Mechanism can surprisingly achieve
5

the same performance as if the thresholds were known exactly to the algorithm. Consequently, even if the agents did not collude, the Insensetive Mechanism performs at least as well as any algorithm designed speciﬁcally to exploit the non-colluding assumptions. The proof of Theorem 1 is provided in Appendix A.

3.3 Bounded regret with heterogeneous thresholds
In this section, we consider a more general situation where different agents can have different thresholds. We present both negative and positive results for this setting.

3.3.1 Suboptimality of any mechanism
Recall that in the homogeneous threshold setting, our Insensetive Mechanism achieves a zero regret. Given this result, a natural question that arises is whether there exists an algorithm that can provide a zero regret guarantee for the heterogeneous setting as well. In this section we show that unfortunately no mechanism can provide such a strong guarantee in this general setting.
Theorem 2. In the heterogeneous threshold setting, no mechanism can ensure that honest strategy is a perfect cooperative equilibrium (PCE).
The proof of this result is provided in Appendix B.

3.3.2 Insensetive Mechanism in heterogeneous situation

We now provide an upper bound on the regret of our Insensetive Mechanism in the general heterogeneous threshold setting.

Theorem 3. The regret β attained by Insensetive Mechanism in the heterogeneous setting is upper bounded as

$ ˚ ´ iN
’α

’

’ ’’&α˚

´

´ iN

`

Y

B

´iN

T

˚ i`1

]¯

βď

Ti˚`1

QU

’α˚ ´

B
˚

’ ’

T2

’

’%α˚ ´ M N

if iTi˚ ă B{N ď iTi˚`1 if iTi˚`1 ă B{N ď pi ` 1qTi˚`1
if B{N ď T2˚ if B{N ą M TM˚ ,

where

M
˚ÿ

´Y

B

´

N

ři
k“1

Tk‹

]

¯

α “ min

T‹

,N .

i“0

i`1

Moreover, for every value of N , M and heterogeneous thresholds, the algorithm attains β “ 0 when B is large enough.

The proof of this result is presented in Appendix C.
3.3.3 Numerical experiments
We now present some numerical simulations that compare the performance of our Insensetive Mechanism with the state of the art. As discussed earlier, past literature focuses on a non-colluding setting, and here we compare with BEACON (Zheng et al., 2017) which is the well-known mechanism for the non-colluding setting.
Figure 1 compares the regret β of our Insensetive Mechanism with BEACON for a 5-round 5-agent game in the heterogeneous setting where agents have true thresholds as 20, 40, 50, 70, 100. The budget ranges from 20 to 3, 000. Overall, our mechanism collects 82.94% of the total number of data points compared to α˚, whereas BEACON attains only 46.63% of α˚ on average. Compared to BEACON, our Insensetive Mechanism achieves an improvement of 36.31%.

6

15

Insensetive Mechanism

BEACON

10

Regret

5

0

1000

2000

3000

Budget

Figure 1: A heterogeneous threshold scenario where the game has 5 rounds and 5 vehicles with real thresholds of 20, 40, 50, 70 and 100. The regret β of our Insensetive Mechanism (solid blue line) is consistently and signiﬁcantly smaller as compared to the prior state-of-the-art BEACON (dotted brown line).

150 Insensetive Mechanism BEACON
100

Regret

50

0

2000

3000

4000

5000

6000

Budget

Figure 2: A real-world scenario where the game has 6 rounds and 50 vehicles with real heterogeneous thresholds. With different amounts of budget, the regret β of our Insensetive Mechanism (solid blue line) is signiﬁcantly smaller than the baseline method BEACON (dotted brown line).

3.3.4 Experiments on real-world data
We collected the real-time surge rates, which reﬂect how many times of the base fare drivers expect to get after riding passengers, from the real-world ride-sharing companies. We used these heterogeneous surge rates to represent the real heterogeneous thresholds of different cars. Therefore, the real threshold of each car was estimated by combining the base fare during a ﬁxed length of time, which was a constant for all cars, and its individual surge rate. We collected the data of around 5,000 cars during 11 days. The data showed that the surge rates of most cars slightly vary with time and location.
Figure 2 compares the regret β of our Insensetive Mechanism with BEACON for a 6-round 50-agent game with different budgets. The budget ranges from 1,500 to 6,000. Compared to α˚, our Insensetive Mechanism manages to collect 96.21% of the total data points , while BEACON only acquires 63.08% of α˚ on average. Our mechanism improves the performance of 33.13% compared to BEACON.
Figure 3 compares the regret β of our Insensetive Mechanism with BEACON for a 6-round game with a ﬁxed budget of 3, 000 and the number of cars ranges from 20 to 80. As the number of cars increases, the regret of BEACON increases accordingly. However, the regret of our Insensetive Mechanism ﬁrst achieves the peak with 50 cars and then decreases, which demonstrates the robustness of our mechanisms. Given a ﬁxed budget, our mechanism can collect 95.86% of the total number of points, which achieves an improvement of 33.52% on average compared to BEACON.

7

150

Regret

100 Insensetive Mechanism BEACON
50

0 20

40

60

80

Number of Cars

Figure 3: A real-world scenario where the game has 6 rounds and a ﬁxed budget of 3, 000. As the number of vehicles varies, the regret β of our Insensetive Mechanism (solid blue line) consistently outperforms the baseline method BEACON (dotted brown line).

4 Discussion
Vehicular crowd sensing is a fast emerging paradigm to collect data for various modern applications. In this paper, we design the Insensetive Mechanism to incentivize agents to collect and report data, with a focus on addressing the challenge of collusion among agents. We show that if the setting is homogeneous in nature, our mechanism performs as well as the best algorithm when the agents’ preferences are known (that is, achieves a zero regret). We also provide negative and positive results for the heterogeneous setting: no mechanism can ensure a zero regret, while our mechanism performs reasonably well.
A key idea behind our Insensetive Mechanism that helps to combat collusion is the introduction of a bias in the choice of the agents. Interestingly, while biases are often undesirable in applications such as statistical learning (Nie et al., 2017; Wang and Shah, 2018), we ﬁnd that a deliberate introduction of bias is beneﬁcial in this game-theoretic setting.
In future work, we wish to incorporate the learning of the spatial-temporal distribution of thresholds of agents from their history. The goal is to design incentive mechanisms that can exploit this “prior” knowledge in order to achieve a lower regret in the heterogeneous setting.
Acknowledgments. This work was supported in part by Carnegie Mellon University’s Mobility21 National University Transportation Center (grant number 69A3551747111), which is sponsored by the US Department of Transportation, the Dowd Fellowship from the College of Engineering at Carnegie Mellon University, Intel, Google, and NSF grants CRII: CIF: 1755656 and CCF: 1763734.

A Proof of Theorem 1

In our proposed mechanism, given B ď M N T ˚, the honest strategy produces a deterministic reward for each agent:

U1psq

“

´Y min N,

B

]¯ T˚

(3a)

T˚

U2psq

“

min

´ N,

Y

B

´

U1

]¯ T

˚

(3b)

T˚

...

UM

psq

“

Y

B

´

U1psq

´

.

.

.

´

UM ´1 psq

] T

˚

(3c)

T˚

The Equation (3a), (3b) and (3c) naturally follow the mechanism that when all agents bid the same, agents with smaller indices always have a higher priority.
Lemma 1. In our incentive mechanism, for any round n P t1, 2, 3, . . . , N ´ 1u, the number of selected agents in the round n is not larger than the number of selected agents in the round n ` 1.

8

Proof. If this theorem does not hold, then there must exists a n1 P t1, 2, . . . , N u, where the number of selected agents

at round n1, Kn1 , is larger than the number of selected agents at round n ` 1, denoted as Kn1`1. That means there must

be at least one agent who is selected in round n1 but not selected in round n1 ` 1. Because in each round n1 we select

Kn1

“

B´Bused N ´n1`1

agents.

If

we

denote

the

budget

left

in

round

n1

as

Bn1 ,

and

the

reward

for

each

selected

agents

in

round n1 and n1 ` 1 as Tn1 , Tn1`1, this situation can be described as

Bn1

Bn1 ´ Kn1 Tn1

N ´ n1 ` 1 ě Kn1 Tn1 , N ´ pn1 ` 1q ` 1 ă Kn1`1Tn1`1

Kn1 ą Kn1`1,Tn1 ě Tn1`1

Expand the inequalities we have

Bn1 ě Kn1 pN ´ n1 ` 1qTn1 , Bn1 ă Kn1 pN ´ n1 ` 1qTn1 which leads to a contradiction. Therefore the lemma holds.

With this lemma, we prove that the honest strategy s is a PCE by splitting the problem into 3 cases. Case 1. B ď N T ˚: For the ﬁrst agent, if she/he bids honestly throughout the game, then the best response of other agents will have no effect on the her/his total reward tB{T ˚uT ˚. Otherwise, if she/he bids T1,1 ą T ˚ in the ﬁrst round, then the Nash Equilibrium for the remaining m ´ 1 agents is that all agents bid honestly as T ˚. If the ﬁrst agent bids T ˚, but increase her/his bid in the round n ą 1, then the mechanism will use her/his previous bid T ˚ as her/his reward
and she/he would accept the reward to ﬁnish the task according to our assumption. In this case, the total reward is still tB{T ˚uT ˚. Therefore, we have U1˚ “ maxp0, tB{T ˚uT ˚q “ tB{T ˚uT ˚ . The reward of agent 1 is upper bounded by the reward gained adopting the honest strategy U1psq “ tB{T ˚uT ˚. Therefore, we have U1psq “ U1˚. For the other agents i P rM szt1u and any strategy s2, the Nash Equilibrium is still that all these agents bid honestly as T ˚. In this case, the ﬁrst agent receives tB{T ˚uT ˚ and other agents receive 0. As B ď N T ˚, we know that Uipsq “ Ui˚ “ 0. Since for any i, we have Uipsq ě Ui˚, the honest strategy s is a PCE when B ď N T ˚.
Case 2. B ě M N T ˚: In this case, the "truth-telling" strategy for crowdsoucer is that at least one agent bids in the range of T P rT ˚, B{M N s. It is obvious that for any agent i, her/his total reward is maximized when she/he bids B{M T ˚ in each round, and collecting a reward of B{M . If she/he ever attempts to bid higher, the Nash Equilibrium of
the other agents will make her/him no gain in this round, and thus in the whole game. Adopting the honest strategy s where each agent bids B{M T ˚, we have Uipsq “ Ui˚ “ B{M for any i. Hence, s is a PCE when B ě M N T ˚.
Case 3. iN T ˚ ă B ă pi ` 1qN T ˚ @i P t1, ¨ ¨ ¨ , M ´ 1u: This is the scenario where the budget is enough to
select i agents during all N rounds, but not enough to pick up i ` 1 agents during all N rounds. According to Eq. 3,
there is

U1psq “ ¨ ¨ ¨ “ Uipsq “ N T ˚

Y

B

´

ři
j“1

Uj psq

]

˚

Y B ´ iN T ˚ ] ˚

Ui`1psq “

T˚ T “ T˚ T

Ui`2psq “ ¨ ¨ ¨ “ UM psq “ 0

According to the Lemma 1, there exists a transition round 1 ď n ď N and we also know that the i ` 1 agent is selected from the N ´ t B´TiN˚T ˚ u ` 1 round.
For any agent j ă i ` 1, if she/he sticks to the honest strategy sj “ s, then Ujpsjq “ N T ˚ no matter what other
agents bid. If any of agent j ă i ` 1 chooses to give a higher bid, we denote n as the ﬁrst round where she/he does so. If n ą 1 then the outcome does not change. But if n “ 1, i.e., she/he bids Tj,1 ą T ˚ from the ﬁrst round, then the agent i ` 1’s best response is to bid Ti`1,1 “ T ˚. When agent i ` 1 takes the best response to bid T ˚, according to the Lemma 1, she/he will receive an extra reward of T ˚. Therefore the upper bound of the reward for agent j is N T ˚.
Hence Ujpsq “ Uj˚ “ N T ˚. For agent i ` 1, similarly, if she/he bids higher than T ˚ from the round N ´ t B´TiN˚T ˚ u ` 1, then the best response
of any agent j ą i ` 1, is to bid T ˚ to get extra reward. Therefore we have Uipsq “ Ui˚ “ t B´TiN˚T ˚ uT ˚.

9

For any agent j ą i ` 1, the best strategy of any agent j1 ď i ` 1 is to bid honestly as T ˚. Therefore, Uj˚ “ 0. Then we have

$

’’Ujpsq ě Uj˚ “ N T ˚

& Uj psq

ě

Uj˚

“

Y B´iN T ˚
T˚

] T˚

’ ’%Ujpsq ě Uj˚ “ 0

if j ă i ` 1 if j “ i ` 1 if j ą i ` 1

In summary, the honest strategy is a PCE in all three cases. If the real threshold T ˚ satisﬁes T ˚ ě B{N M , at most tB{T ˚u agents can be actuated in the game. If each agent
bids T ˚ honestly, based on the mechanism, the payment for each agent in each round will stay at T ˚. By the end of the
game, according to the mechanism all budget would have been spent to actuate as many agents as possible. Therefore the objective for the crowdsourcer is achieved. If the real threshold T ˚ ă B{N M , the budget is large enough to pay each agent in every round, as long as all agents bid the same value between T ˚ and B{N M inclusively. The number of agents selected in one round, denoted as K, is equal to M , and the total number of agents actuated is N M .

B Proof of Theorem 2
Given two selected agents i, j P rM s and an 1-round game, without the loss of generality, we assume Ti˚ ă Tj˚ holds. Suppose that there exists an optimal incentive mechanism: the mechanism should optimize the utilization of the budget to ensure β “ 0 in any setup of B and real thresholds. that is, the mechanism should always pay real threshold to each agent when budget is not enough to support all agents in all rounds.
We divide all incentive mechanisms into two categories: 1) fair incentive mechanism and 2) unfair incentive mechanism. A mechanism is fair if any agents i, j P rM s bid the same Ti “ Tj, agents i and j will be selected with equal probability, which doesn’t hold for the unfair case.
If the above optimal mechanism is fair, then the utility of agent i by taking strategy s1 of bidding as Tj˚ will be
Uips1q “ Uj psq “ Tj˚ ą Ti˚.
If the above optimal mechanism is unfair, since the real threshold is unknown, without the loss of generality, if agent i has a higher priority than agent j, then the utility of agent i by bidding Tj is
Uips1q “ Tj˚ ą Ti˚.
However, according to PCE condition, we have Uipsq “ Ti˚ ě Uips1q “ Tj˚ which leads to a contradiction.
C Proof of Theorem 3
Case 1. B{N ď T2˚: In this case, since the budget can afford at most 2 agents. The budget can afford agent 1 for all rounds, or agent 2 for several rounds. We assume T1˚ ă T2˚ and agent 1 comes with a higher priority P1 ą P2. Since B{N ď T2˚, there must exist a strategy for agent 1 to obtain full reward B, which is also the best strategy for agent 1.
Lemma 2. When B{T2˚ ą tB{T2˚u, the agent 1 needs to take at least rB{T2˚s tasks to get the full reward B.
Proof. Assume agent 1 can get reward B by taking number of tasks k1 ă rB{T2˚s, equivalently k1 ď tB{T2˚u, the average reward for each round would be B{k1 ě B{tB{T2˚u ą T2˚ according to B{T2˚ ą tB{T2˚u. This means that the average reward exceeds T2˚, then there must exist some rounds that agent 2 can bid T2˚ to get some reward such that agent 1 can not get the full reward B, which leads to a contradiction.
When B{T2˚ “ tB{T2˚u “ rB{T2˚s, the best strategy for agent 1 would be to bid T2˚ every time to get full reward B, which needs to take rB{T2˚s tasks. Therefore, given the Lemma 2 and above statement, we know that for the upper bound for regret is α˚ ´ rB{T2˚s.

10

Case 2. iTi˚`1 ă B{N ď pi ` 1qTi˚`1: In this case, budget B can support the ﬁrst i agents for N rounds with bidding Ti˚`1 but not enough to support the ﬁrst i ` 1 agents. For the ﬁrst i agents, this scenario is the same as the homogeneous threshold case 3: iN T ˚ ă B ă pi ` 1qN T ˚. Therefore, at least iN accepted tasks can be obtained. Meanwhile, since we have B{N ą iTi˚`1, there are budget left for the agent i ` 1 in some rounds. Based on our mechanism, the best strategy for agent i ` 1 would be to bid Ti˚`1. Therefore, in this cases, we can ﬁnally achieve the upper bound for regret α˚ ´ iN ´ Y B´TiN˚Ti˚`1 ].
i`1
Case 3. iTi˚ ă B{N ď iTi˚`1: In this case, the budget can support the ﬁrst i agents for N rounds with bids Ti˚, but not enough to support all agents for bidding Ti˚`1. Therefore, in this case, there is no chance for agent i ` 1 to get any reward if the ﬁrst i agents achieve a PCE. This case is equivalent to the homogeneous threshold case 2: B ą iN T ˚ where T “ Ti˚. Based on the proof in Appendix A, the PCE would be that all of ﬁrst i agents bid B{iN . Thus, each one will obtain the reward of B{i and ﬁnish N tasks. The upper bound for regret would be α˚ ´ iN .
Case 4. B{N ą M TM˚ : This case is the same as the homogeneous threshold case: B ą M N T ˚ where T ˚ “ TM˚ . Based on the proof shown in Appendix A, the upper bound for regret would be α˚ ´ M N .
References
Angelopoulos, C. M., Nikoletseas, S., Raptis, T. P., and Rolim, J. D. (2014). Characteristic utilities, join policies and efﬁcient incentives in mobile crowdsensing systems. In Wireless Days (WD), pages 1–6. IEEE.
Devarakonda, S., Sevusu, P., Liu, H., Liu, R., Iftode, L., and Nath, B. (2013). Real-time air quality monitoring through mobile sensing in metropolitan areas. In Proceedings of the 2nd ACM SIGKDD international workshop on urban computing, page 15. ACM.
Duan, L., Kubo, T., Sugiyama, K., Huang, J., Hasegawa, T., and Walrand, J. (2012). Incentive mechanisms for smartphone collaboration in data acquisition and distributed computing. In INFOCOM, pages 1701–1709. IEEE.
Dutta, P., Aoki, P. M., Kumar, N., Mainwaring, A., Myers, C., Willett, W., and Woodruff, A. (2009). Common sense: participatory urban sensing using a network of handheld air quality monitors. In Proceedings of the 7th ACM conference on embedded networked sensor systems, pages 349–350. ACM.
Eisenman, S. B., Miluzzo, E., Lane, N. D., Peterson, R. A., Ahn, G.-S., and Campbell, A. T. (2009). Bikenet: A mobile sensing system for cyclist experience mapping. ACM Transactions on Sensor Networks (TOSN), 6(1):6.
Ganti, R. K., Ye, F., and Lei, H. (2011). Mobile crowdsensing: current state and future challenges. IEEE Communications Magazine, 49(11).
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359–378.
Hasenfratz, D., Saukh, O., Walser, C., Hueglin, C., Fierz, M., Arn, T., Beutel, J., and Thiele, L. (2015). Deriving high-resolution urban air pollution maps using mobile sensor nodes. Pervasive and Mobile Computing, 16:268–285.
Hull, B., Bychkovsky, V., Zhang, Y., Chen, K., Goraczko, M., Miu, A., Shih, E., Balakrishnan, H., and Madden, S. (2006). Cartel: a distributed mobile sensor computing system. In Proceedings of the 4th international conference on Embedded networked sensor systems, pages 125–138. ACM.
Ji, S. and Chen, T. (2017). On designing collusion-resistant incentive mechanisms for mobile crowdsensing systems. In Trustcom/BigDataSE/ICESS, 2017 IEEE, pages 162–169. IEEE.
Jin, H., Su, L., and Nahrstedt, K. (2017). Theseus: Incentivizing truth discovery in mobile crowd sensing systems. arXiv preprint arXiv:1705.04387.
Kamble, V., Marn, D., Shah, N., Parekh, A., and Ramachandran, K. (2015). Truth serums for massively crowdsourced evaluation tasks. arXiv preprint arXiv:1507.07045.
11

Lambert, N. and Shoham, Y. (2009). Eliciting truthful answers to multiple-choice questions. In Proceedings of the 10th ACM conference on Electronic commerce, pages 109–118. ACM.
Lane, N. D., Miluzzo, E., Lu, H., Peebles, D., Choudhury, T., and Campbell, A. T. (2010). A survey of mobile phone sensing. IEEE Communications magazine, 48(9).
Nie, X., Tian, X., Taylor, J., and Zou, J. (2017). Why adaptively collected data have negative bias and how to correct for it. arXiv preprint arXiv:1708.01977.
Prelec, D. (2004). A bayesian truth serum for subjective data. science, 306(5695):462–466.
Rong, N. and Halpern, J. Y. (2014). Cooperative equilibrium: A solution predicting cooperative play. arXiv preprint arXiv:1412.6722.
Shah, N. B. and Zhou, D. (2016a). Double or nothing: Multiplicative incentive mechanisms for crowdsourcing. Journal of Machine Learning Research, 17:1–52.
Shah, N. B. and Zhou, D. (2016b). No oops, you won’t do it again: mechanisms for self-correction in crowdsourcing. In International Conference on Machine Learning.
Shah, N. B., Zhou, D., and Peres, Y. (2015). Approval voting and incentives in crowdsourcing. In International Conference on Machine Learning.
Thiagarajan, A., Ravindranath, L., LaCurts, K., Madden, S., Balakrishnan, H., Toledo, S., and Eriksson, J. (2009). Vtrack: accurate, energy-aware road trafﬁc delay estimation using mobile phones. In Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems, pages 85–98. ACM.
Wan, J., Liu, J., Shao, Z., Vasilakos, A. V., Imran, M., and Zhou, K. (2016). Mobile crowd sensing for trafﬁc prediction in internet of vehicles. Sensors, 16(1):88.
Wang, J. and Shah, N. B. (2018). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. arXiv preprint arXiv:1806.05085.
Wang, X. O., Cheng, W., Mohapatra, P., and Abdelzaher, T. (2013). Artsense: Anonymous reputation and trust in participatory sensing. In INFOCOM, 2013 Proceedings IEEE, pages 2517–2525. IEEE.
Wang, X. O., Cheng, W., Mohapatra, P., and Abdelzaher, T. (2014). Enabling reputation and trust in privacy-preserving mobile sensing. IEEE Transactions on Mobile Computing, 13(12):2777–2790.
Yang, D., Xue, G., Fang, X., and Tang, J. (2012). Crowdsourcing to smartphones: Incentive mechanism design for mobile phone sensing. In Proceedings of the 18th annual international conference on Mobile computing and networking, pages 173–184. ACM.
Zhang, X., Yang, Z., Sun, W., Liu, Y., Tang, S., Xing, K., and Mao, X. (2016). Incentives for mobile crowd sensing: A survey. IEEE Communications Surveys & Tutorials, 18(1):54–67.
Zhao, D., Li, X.-Y., and Ma, H. (2014). How to crowdsource tasks truthfully without sacriﬁcing utility: Online incentive mechanisms with budget constraint. In INFOCOM, pages 1213–1221. IEEE.
Zheng, Y., Liu, F., and Hsieh, H.-P. (2013). U-air: When urban air quality inference meets big data. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1436–1444. ACM.
Zheng, Z., Wu, F., Gao, X., Zhu, H., Tang, S., and Chen, G. (2017). A budget feasible incentive mechanism for weighted coverage maximization in mobile crowdsensing. IEEE Transactions on Mobile Computing, 16(9):2392–2407.
12

