arXiv:2103.02827v2 [cs.LG] 5 Mar 2021

On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk
Audrey Huang1, Liu Leqi1, Zachary C. Lipton1, and Kamyar Azizzadenesheli2
audreyh@andrew.cmu.edu,leqil@cs.cmu.edu, zlipton@cmu.edu,kamyar@purdue.edu 1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Purdue University
Abstract
In order to model risk aversion in reinforcement learning, an emerging line of research adapts familiar algorithms to optimize coherent risk functionals, a class that includes conditional valueat-risk (CVaR). Because optimizing the coherent risk is diﬃcult in Markov decision processes, recent work tends to focus on the Markov coherent risk (MCR), a time-consistent surrogate. While, policy gradient (PG) updates have been derived for this objective, it remains unclear (i) whether PG ﬁnds a global optimum for MCR; (ii) how to estimate the gradient in a tractable manner. In this paper, we demonstrate that, in general, MCR objectives (unlike the expected return) are not gradient dominated and that stationary points are not, in general, guaranteed to be globally optimal. Moreover, we present a tight upper bound on the suboptimality of the learned policy, characterizing its dependence on the nonlinearity of the objective and the degree of risk aversion. Addressing (ii), we propose a practical implementation of PG that uses state distribution reweighting to overcome previous limitations. Through experiments, we demonstrate that when the optimality gap is small, PG can learn risk-sensitive policies. However, we ﬁnd that instances with large suboptimality gaps are abundant and easy to construct, outlining an important challenge for future research.
Keywords: policy gradient, gradient dominance, coherent risk, Markov coherent risk.
1 Introduction
As reinforcement learning (RL) has emerged as a prospective technology in consequential and safety-critical domains, a burgeoning body of research seeks to optimize objectives that incorporate notions of risk sensitivity [Garcıa and Fernández, 2015], e.g., reducing variance or mitigating the worst case returns. In particular, coherent risk functionals [Artzner et al., 1999] satisfy several natural and desirable properties, subsuming the expected return, conditional value-at-risk (CVaR), mean upper semideviation, and spectral risk functionals [Acerbi, 2002]. However, when directly addressing the coherent risk in Markov decision process (MDPs), owing to time inconsistency, it can be diﬃcult to construct dynamic programming formulations or to derive the corresponding Bellman operators. Thus many researchers instead focus on the Markov coherent risk (MCR), a time-consistent surrogate [Ruszczyński and Shapiro, 2006, Tamar et al., 2015a].
1

While recent papers derive policy gradient (PG) updates for MCR [Tamar et al., 2015a], PG for this objective has not been proven to enjoy the same guarantees of global convergence as it has for the expected return [Agarwal et al., 2019, Bhandari and Russo, 2019]. Moreover, while folk wisdom suggests that it should converge to a stationary point of the MCR, this local convergence viewpoint leaves open the critical question of how far this point can be from the global optimum.
In this paper, we analyze the global convergence of PG algorithms for optimizing the MCR, which, to the best of our knowledge, is the ﬁrst investigation into its theoretical convergence. First, we prove that the MCR is not gradient dominated and thus, in general, PG may not converge to a global optimum (Section 5). We upper bound the suboptimality of the policy using a scalar notion of ﬁrst-order stationarity plus necessary problem-dependent residuals reﬂecting the nonlinearity and risk sensitivity of the objective. Moreover, we design MDP problem instances that illustrate how these residuals are inevitable, and thus that our upper bound√is tight. Next, we show that projected gradient descent of the MCR objective converges in O(1/ T ) iterations to a policy with suboptimality gap quantiﬁed by the same residuals (Section 5.3). We later show that the mentioned suboptimality gap vanishes under the expected return, which is an MCR objective, recovering the convergence results of Agarwal et al. [2019].
On the practical side, we provide methods to estimate the gradient of the MCR, a task that proves diﬃcult because it involves an expectation over a reweighted state distribution of the underlying MDP (Theorem 4.1). The weights are determined by the solution to the maximization problem in the MCR Bellman operator (Section 4.1), unique to the coherent risk functional and dependent on the current policy. In Tamar et al. [2015a], the authors make the strong assumption that the agent can sample from the reweighted distribution of the underlying MDP, which rarely holds in online settings. To relax this assumption, in the second part of our paper, we leverage recent advances in oﬀ-policy evaluation to propose an algorithm using state distribution correction [Liu et al., 2018] to tractably estimate the gradient, which can then be plugged into any policy gradient algorithm. This permits, to our knowledge, the ﬁrst evaluations of PG optimization of MCR in MDPs. In our experiments (Section 7), we integrate this method into an actor-critic algorithm to demonstrate its eﬀectiveness in optimizing MCR in a stochastic version of the Cliﬀwalk environment [Sutton and Barto, 2018].
2 Related Work
Risk functionals, including exponential utility [Fei et al., 2020], the mean-variance risk functional [Di Castro et al., 2012, Prashanth and Ghavamzadeh, 2013], the conditional value-at-risk (CVaR) [Tamar et al., 2015b, Chow and Ghavamzadeh, 2014] and cumulative prospect risks [Prashanth et al., 2016] are increasingly studied in the RL literature. They have been explored empirically for a number of Atari games [Bellemare et al., 2017], and in the context of such diverse applications as autonomous driving [Mavrin et al., 2019] and healthcare [Keramati et al., 2019]. Many (but not all) of these risk functionals are subsumed under the class of coherent risks [Shapiro et al., 2009], which satisfy several desirable theoretical properties (Section 3).
In the MDP setting, researchers have considered the dynamic coherent risk [Chow and Ghavamzadeh, 2014], which applies the coherent risk functional directly on the (random) return over entire trajectories and the MCR objective [Tamar et al., 2015a], which applies it in a nested manner at each timestep. Pﬂug and Pichler [2016] shows that the two formulations are generally not equal except for the expected return and max-risk due to a property called time consistency. Time consistent
2

risk measures satisfy the property that if a policy is risk-optimal for T timesteps onward, it is also risk-optimal for t timesteps onward, where t ≤ T . Leveraging time consistency, one can rewrite MCR objectives in dynamic programming form using a Bellman operator, as derived by Ruszczyński and Shapiro [2006] and studied in the PG setting by Tamar et al. [2015a]. By contrast, for the dynamic coherent risk, the Bellman optimality operator has been derived only for CVaR [Chow et al., 2015]. Even for CVaR, the dynamic coherent risk is diﬃcult to work with, requiring that we augment the state space with the continuous CVaR level α (CVaRα(Z) gives the expected value of the lower α-quantile of the random variable Z), and learn the optimal value for all CVaR levels. The MCR upper bounds the dynamic coherent risk, and though not always explicitly named, has seen wide usage in RL literature [Tamar et al., 2015b, Bellemare et al., 2017].
In the primary work on PG for MCR objectives, Tamar et al. [2015a] studies the general class of coherent risk functionals, introducing a (statistically) consistent sampling-based estimate of the policy gradient. However, they do not investigate convergence of the PG algorithm to local or global optima. Further, while they present an actor-critic algorithm for optimizing the MCR, they make strong sampling assumptions.
Our work makes advances on both of these fronts. Inspired by Agarwal et al. [2019], Azizzadenesheli et al. [2018], Bhandari and Russo [2019], who demonstrate the global optimality and convergence rate of PG for the expected return, our convergence analysis characterizes the suboptimality of PG for MCR. Our proposed policy gradient algorithm is inspired by the algorithm presented in Tamar et al. [2015a], utilizing state distribution correction methods ﬁrst proposed by Liu et al. [2018] for oﬀ-policy evaluation and later adapted in Liu et al. [2020] for oﬀ-policy optimization.

3 Preliminaries

Let (Ω, F, P ) denote a probability space where Ω is a ﬁnite set of outcomes, F is the σ-algebra over Ω, and P ∈ B is a probability measure over F and B = ξ : ω∈Ω ξ(ω) = 1, ξ(ω) ≥ 0 is a set of probability measures. Let Z be the space of real-valued random variables Z : Ω → R deﬁned over (Ω, F, P ). A risk functional ρ : Z → R maps a random variable Z to a value on the extended real line R := R ∪ {−∞, ∞}. We call ρ coherent if it satisﬁes the following four properties [Artzner et al., 1999]:
1. Monotonicity: ρ(Z1) ≤ ρ(Z2) whenever Z1 ≤ Z2 almost surely;
2. Convexity: ρ(tZ1 + (1 − t)Z2) ≤ tρ(Z1) + (1 − t)ρ(Z2) for t ∈ [0, 1];
3. Translation equivariance: ρ(Z + c) = ρ(Z) + c, ∀c ∈ R;

4. Positive homogeneity: ρ(tZ) = tρ(Z) for t > 0.
Importantly, coherent risk functionals can be uniquely expressed using the following dual representation:

Theorem 3.1. [Shapiro et al., 2009, Theorem 6.4] A risk functional ρ : Z → R is coherent if and only if there exists a convex bounded and closed set of feasible duals U depending on P , such that

ρ(Z) = sup E[ξZ],

(1)

ξ∈U (P )

3

where the risk envelope U (P ) = {ξ ∈ dom(ρ∗) : ω∈Ω ξ(ω)dP (ω) = 1, ξ ≥ 0} and dom(ρ∗) is the domain of the Fenchel conjugate of ρ.

Because the set of subdiﬀerentials is always a convex and compact set, following Tamar et al. [2015a], we write U using the general form below, which includes additional assumptions on smoothness:
Assumption 3.2 (General Form of Risk Envelope). The risk envelope U(P ) from Theorem 3.1 can be written as

U (P ) = {ξ : ge(ξ, P ) = 0 ∀e ∈ E, fi(ξ, P ) ≤ 0 ∀i ∈ I, ξ ≥ 0, EP [ξ] = 1},

(2)

where E, I denote a set of equality and inequality constraints, respectively. Each constraint ge is aﬃne in ξ, each constraint fi is convex in ξ, and there exists a strictly feasible point ξ. Further, for any given ξ, both ge(ξ, P ) and fi(ξ, P ) are twice diﬀerentiable in P and there exists M > 0 such that for all e ∈ E and i ∈ I,

dge(ξ, P ) ≤ M and dP op

dfi(ξ, P ) ≤ M. dP op

Assumption 3.2 is satisﬁed by the CVaR, mean semideviation, and spectral risk functionals, and implies that the risk envelope U is known in explicit form. The dual representation in Theorem 3.1 indicates that coherent risk measures can be seen as the eifjcctrdujltgunddiuvvjvﬀvldfukdnueiejjrtik worst-case expectation of Z over probability distributions reweighted by ξ, which must be chosen from the set of measures in the risk envelope U [Chow et al., 2015].

Conditional Value-at-Risk (CVaR) CVaR, which arises frequently in the portfolio optimization

and (more recently) RL literatures, corresponds to the expected return over the worst α fraction

of outcomes. Formally, the CVaR at a level α ∈ [0, 1] of a random variable Z is deﬁned as

ρCVaRα = inft∈R t + α1 E[(Z − t)+] . It can equivalently be expressed in the dual formulation

(Theorem 3.1) using the risk envelope UCVaRα(P ) = ξ : ξ ∈ 0, α1 , EP [ξ] = 1 , for which the

solution

ξ∗

to

the

maximization

problem

in

(1)

can

be

calculated

in

closed

form

as

ξ∗(ω)

=

1 α

when

Z(ω) > λ∗ and as ξ∗(ω) = 0 when Z(ω) < λ∗, where λ∗ is any (1 − α)-quantile of Z [Shapiro et al.,

2009, Chapter 6].

4 Problem Setting
In this paper, we focus on the inﬁnite horizon discounted Markov Decision Process (MDP) setting. An MDP is a tuple M = (S, A, C, P, γ, s0), where S is the state space, A is the action space, C : S → [0, 1] is the cost function, P (·|s, a) is the transition kernel, γ ∈ [0, 1) is the discount factor, and s0 ∈ S is the starting state. A stationary Markov policy πθ : S → ∆(A) parameterized by θ ∈ Rd maps each state s ∈ S to a probability measure over actions A, where ∆(·) denotes the probability simplex.
Given an MDP M and policy πθ, consider a ﬁltration F = {Ft}t≥0 such that the observations sequence (s0, a0, c0, . . . , st, at, ct) is Ft-measurable. Let C(st) be the Ft-measurable random cost at time t. In this paper, we consider the Markov coherent risk (MCR) objective deﬁned over {C(st)}t≥0.

4

Deﬁnition 1 (Markov Coherent Risk Objective). Given a sequence of random costs {C(st)}t≥0, the Markov coherent risk objective is deﬁned as

ρθ(s0) = C(s0) + γρ C(s1) + ... + γρ C(st) + . . . ; πθ .

The MCR is a nested discounted sum of the risk functional ρ applied at each timestep to the randomness of the cost C(st) given the previous state st−1, which arises from the stochastic policy πθ and transitions of the MDP. We are interested in solving the following optimization problem, which seeks the parameter θ that minimizes the MCR (3),

min ρθ(s0)

(3)

θ

using policy gradient methods.

4.1 Value Function and Gradient
The risk-sensitive value function for an MDP M under the policy πθ is deﬁned as Vθ(s) = ρθ(s). As shown in Ruszczyński [2010] and Tamar et al. [2015a], the Bellman operator for Vθ(s) is

Vθ(s) = C(s) + γ max Pθ(s |s)ξ(s )Vθ(s ),

(4)

ξ∈U (Pθ) s

where U is the risk envelope deﬁned in Theorem 3.1 and Pθ(s |s) := Assumption 3.2, the value function can be re-written as

a∈A πθ(a|s)P (s |s, a). Under

Vθ(s) = C(s) + γ max min Lθ,s(ξ, λP , λE , λI ),
ξ λP ,λE ,λI

where Lθ,s is the corresponding Lagrangian

Lθ,s(ξ, λP , λE , λI ) = Pθ(s |s)ξ(s )Vθ(s ) − λP

Pθ(s |s)ξ(s ) − 1

s

s

− λE (e)ge(ξ, Pθ) − λI (i)fi(ξ, Pθ).

(5)

e∈E

i∈I

For each state s ∈ S, let (ξθ,s, λPθ,s, λEθ,s, λIθ,s) denote a saddle point of (5), which is guaranteed to exist under Assumption 3.2. The gradient of the value ∇θVθ(s0) = ∇θρθ(s0) is calculated using the
saddle points (ξθ,s, λPθ,s, λEθ,s, λIθ,s), and was derived in Tamar et al. [2015a]. We restate this theorem below with proof in Appendix A.

Theorem 4.1. [Tamar et al., 2015a, Theorem 5.2] Under Assumption 3.2 and assuming the likelihood ratio ∇θ log πθ(a|s) is well-deﬁned and bounded for all (s, a),

∇θVθ(s) = Eξθ,s

∞
γt∇θ log πθ(at|st)hθ(st, at)|s0 = s ,
t=0

5

where Eξθ,s refers to expectation with respect to trajectories generated by a Markov chain with transition probabilities Pθ(·|s)ξθ,s(·), and

hθ(s, a) = γ P (s |s, a) ξθ,s(s ) Vθ(s ) − λPθ,s

s

− λI (i) dfi(ξθ,s, Pθ) − λE (e) dge(ξθ,s, Pθ) .

(6)

θ,s

dPθ

θ,s

dPθ

i

e

Notation We now introduce some notation key to our analysis of policy gradient in the following

sections. First, deﬁne the inﬁnite discounted state visitation distribution under a policy πθ and

the starting state s0 to be dπθ (s) := (1 − γ)

∞ t=0

γ

tPπθ

(st

=

s|s0),

where

Pπθ (st

=

s|s0)

is

the

probability that st = s after executing πθ starting from state s0. We adopt the shorthand for these

terms as dθ and Pθ, respectively, and d∗ and P∗ for the optimal policy π∗. Without loss of generality,

we assume that that the starting state is deterministic and thus remove s0 from our notation to

avoid clutter. We next deﬁne the MDP transitions induced by πθ and reweighted by {ξs}s∈S to

be Pθξ. Under this reweighting, the transitions are given by Pθξ(s |s) := Pθ(s |s)ξs(s ). The state

visitation distribution under these reweighted transitions is dξθ(s) := (1 − γ)

∞ t=0

γ t Pθξ (st

=

s|s0).

5 Global Optimality
In this section, we investigate the global optimality and convergence of PG for the MCR objective (3) and consider policies directly parameterized by θ ∈ R|S|×|A|, i.e. πθ(a|s) = θs,a. We begin with an illustrative example demonstrating that risk-sensitive MCR exhibits local minima for even simple problems, which suggests that unlike the expected return, it is not gradient dominated. We formalize this result in Lemma 5.3, which states that the value optimality gap Vθ − V∗ under the MCR is upper bounded by the magnitude of the gradient plus residual terms. As a result, stationary points of the MCR objective are not guaranteed to be global optima. The residuals are a consequence of the fact that, unlike in the expected return, the MCR Bellman operator (4) is nonlinear in the policy πθ due to the maximization over U, which is dependent on πθ. We show that the residuals vanish under the expected return, which is a Markov coherent risk objective, demonstrating that Lemma 5.3 is a generalization of the gradient domination lemma previously developed in Agarwal et al. [2019] for expected return. Moreover, we prove that there exist problem instances for which the gradient domination inequality holds with equality, demonstrating the tightness of our result. Finally, using the gradient domination lemma, we establish the convergence rate of projected gradient descent for the MCR objective.

5.1 Motivating Example
Consider a 2-armed bandit problem where the cost of Arm 1 is a Bernoulli random variable which assigns probability 0.8 to receiving cost 1, and the cost of Arm 2 is always 0.5. Suppose we work with a directly parameterized policy, where the probability of playing arm 1 is given by θ and the probability of playing arm 2 is given by 1 − θ. This problem can equivalently be written as a ﬁve-state MDP, with transitions given in Figure 1a.
We use the MCR objective with the CVaRα risk functional, and in Figure 1b, plot the optimization landscape for this problem, i.e., the MCR objective over the range of possible policies θ, at diﬀerent

6

C =0

.2θ

1

1 − θ C = 0.5 1 1
.8θ
C =1

(a)

(b)

Figure 1: (a) The MDP for the bandit problem given in Section 5.1. (b) The optimization landscape for MCR using CVaR at diﬀerent α levels in this MDP.

risk sensitivity levels α. It is clear that, for a range of α < 1 levels, two stationary points exist at the boundary θ = 0 and θ = 1. Which one PG converges to depends on the initial value of θ. Only when CVaR α = 1, which corresponds to the expected return, will the stationary point be unique and globally optimal. This matches previous results [Agarwal et al., 2019] demonstrating that the expected return is gradient dominated.
In addition, this counterexample exposes another source of poor convergence and suboptimality in MCR objectives. Because certain coherent risk functionals, such as CVaRα, consider a worst-case reweighted fraction of the cost distribution, they are more vulnerable to vanishing gradients than the expected return. This problem is exacerbated for higher degrees of risk sensitivity, e.g., lower α for CVaR. As can be seen from the α = 0.1 line in Figure 1b, the gradient is zero for a signiﬁcant fraction of random initializations of θ, while expected value (α = 1) has no such problem.

5.2 Gradient Domination

We formalize the intuition of our motivating example using a property called gradient domination. Any stationary point of a gradient dominated function is also globally optimal. Even if a gradient dominated function is highly non-convex, any optimization algorithm that converges to a stationary point will also converge to a global optimum. The expected return has previously been shown to be gradient dominated. We follow the notion of gradient domination introduced by Bhandari and Russo [2019] and formally articulated in Deﬁnition 2.

Deﬁnition 2. For θ ∈ Rd, a function f is (c, µ)- gradient dominated over Θ if there exists constants c > 0 and µ ≥ 0 such that for all θ ∈ Θ,

min f (θ ) ≥ f (θ) + min

c ∇f (θ), θ − θ

µ +

θ−θ

2.

(7)

θ ∈Θ

θ ∈Θ

2

2

Deﬁnition 2 has two important implications. First, when ∇f (θ), θ − θ ≥ 0 for all θ , which occurs when θ is a stationary point, we have minθ ∈Θ f (θ ) ≥ f (θ), implying that θ is globally optimal. Second, for any θ, the optimality gap f (θ) − minθ ∈Θ f (θ ) can be upper bounded by the minimization problem on the right hand side, which is a measure of how far θ is from stationary.

7

We demonstrate through Lemma 5.3 that for MCR although the ﬁrst statement does not hold, we can still expressively bound the optimality gap with a similar optimization problem.
We ﬁrst deﬁne the two residuals presented in Lemma 5.3, which arise from two sources of nonlinearity in the Markov coherent risk value function: (i) the dependence of the saddle point solutions ξθ,s and λθ,s on πθ; and (ii) in the constraints fi and ge which, depending on the coherent risk measure, may be highly nonlinear in πθ. The ﬁrst source of nonlinearity contributes to our analysis is the diﬀerence between the primal and dual solutions of πθ and π∗, which we upper bound by L in Assumption 5.1. The second source of nonlinearity contributes a residual which we upper bound by U in Assumption 5.2. The residual U is related to the diﬀerence between the constraint terms in the Lagrangian (5) and their ﬁrst-order approximation under πθ in the gradient update, and can be interpreted as a measure of how nonlinear the constraints are in the policy πθ.
Assumption 5.1 (Policy Residual). Given a policy θ and any other policy π,

L(θ, π) := γ dξθθ (s) λPθ,s − λPπ,s

Pθ − Pπ (s |s)ξθ,s(s )

(8)

s

s

Suppose that for all θ and π, L(θ, π) ≤ L. Assumption 5.2 (Constraint Residual). Given a policy θ and any policy π,

U (θ, π) :=γ dξθθ (s)
s

λπ,s(i)∆fi + λπ,s(e)∆ge

i

e

+ πθ − π

λθ,s(i)∇θfi(ξθ, Pθ) + λθ,s(e)∇θge(ξθ, Pθ)

(9)

i

e

where ∆fi = fi(ξθ, Pπ) − fi(ξθ, Pθ) and ∆ge is deﬁned similarly. Suppose that for all θ and π, U (θ, π) ≤ U .
We ﬁrst derive a performance diﬀerence lemma for the MCR (Appendix Lemma B.1), which upper bounds the optimality gap using an MCR advantage function. We then use it to establish the gradient domination bound in Lemma 5.3.

Lemma 5.3 (Gradient Domination). The optimality gap Vθ(s0) − V∗(s0) is upper bounded as
dξ∗θ Vθ(s0) − V∗(s0) ≤ dξθθ ∞ π∈m∆a(Ax)S(π − πθ) ∇θVθ(s0) + L(θ, π∗) + U (θ, π∗) .
Lemma 5.3 demonstrates that it is not suﬃcient for the gradient of the value to be small for the policy to be nearly optimal. As with previous results for expected return, the ξθ-reweighted state visitation distribution dξθθ must adequately cover the support of the reweighted state visitation distribution dξ∗θ under the optimal policy. Whenever the fraction d∗/dθ ∞, which is present in the gradient domination bound for the expected return [Agarwal et al., 2019] is ﬁnite, the term
dξ∗θ /dθξθ ∞ is also ﬁnite by deﬁnition (the converse is not necessarily true). Further, even if θ is a stationary point, i.e., (π − πθ) ∇θVθ(s0) ≤ 0 for all π, the residuals
L(θ, π∗) and U (θ, π∗) can be nonzero and global optimality is not guaranteed. The residuals capture the distance between πθ and an optimal policy π∗ in terms of the Lagrangian primal and dual

8

variables that are used to calculate the objective function. In general, an optimal policy for a given

MDP is not known a priori, but we can upper bound the optimality gap at a stationary point θ

using maxπ L(θ, π) + U (θ, π) ≤ L + U from Assumptions 5.1 and 5.2. The upper bound is

problem-dependent and reﬂects the magnitude of risk aversion of the chosen coherent risk functional,

and degree of nonlinearity in the policy arising from the risk envelope U in the Bellman operator (4).

The residual L has a larger upper bound when ξ is permitted to be large, e.g., smaller fractions of the cost distribution are given a high weight. The residual U is a measure of the error of a ﬁrst-order approximation of the constraints fi and ge, and is larger when the constraints are highly nonlinear in πθ. Roughly, the magnitude of L is small when the saddle points of πθ are close to the saddle points of the optimal policy π∗, and the residual U is small if the constraints are additionally linear in πθ.
The expected value objective is equivalently formulated as the MCR objective for CVaR with

α = 1, and its risk envelope is a singleton of UCVaR1(Pθ) = {ξ : ξ = 1}. As a result, ξθ,s = 1 for all θ, s and the expected value Bellman operator is linear in the policy. Then by deﬁnition, the residuals

U = L = 0 for expected value, and Lemma 5.3 is a generalization of the gradient domination lemma

for expected return, i.e., (Lemma 4.1 in Agarwal et al. [2019]). For CVaRα, we have U = 0 as the

inequality

constraints

0

≤

ξ

≤

1 α

are

independent

of

πθ ,

and

L ∝ α1 . As a result, the upper bound

on the optimality gap is larger when α is smaller and the objective is more risk sensitive.

Moreover, Theorem 5.4 guarantees that the bound in Lemma 5.3 is in fact tight. We provide a

proof sketch, with formal argument in Appendix B.

Theorem 5.4. There exists an MDP M and πθ such that

dξ∗θ Vθ(s0) − V∗(s0) = dξθθ ∞ π∈m∆a(Ax)S(π − πθ) ∇θVθ(s0) + L(θ, π∗) + U (θ, π∗) .
Proof Sketch. We give an informal argument on the existence of the lower bound and the necessity of the residual terms. Take the CVaR at level α objective for example, and consider a 2-action MDP, where one action always returns a set high cost, and the other action returns a set low cost most of the time and the high cost the rest. It is easy to see that the optimal policy will always choose the latter action. However, there is a large fraction of policies in this MDP where, even if the policy chooses both actions with reasonable probabilities, the highest α-quantile of the return distribution may include only the high cost. Thus, even though the policy may be arbitrarily suboptimal, it will have a gradient of 0. The suboptimality of the policy can be quantiﬁed exactly using the residuals terms L(θ, π∗) and U (θ, π∗).

Remark 5.5. Previous studies on the global optimality of PG for the expected return have taken into account two sources of suboptimality: vanishing gradients Agarwal et al. [2019] and closure under policy improvement, which Bhandari and Russo [2019] used as a condition for why some problems with constrained policy classes are gradient dominated and others are not. Our results demonstrate that neither of these is suﬃcient to explain suboptimality in the case of coherent risk objectives. For the latter, it is clear in our motivating example that the policy class θ ∈ [0, 1] is closed under policy improvement.

9

5.3 Convergence Rates
We consider the convergence rate of projected gradient descent (PGD) on the MCR under direct policy parameterization, where updates take the following for:

θ ← Proj∆(A)S θ − η∇θVθ(s0) ,

(10)

where Proj∆(A)S is a projection on the probability simplex ∆(A)S in the Euclidean norm and η is the learning rate. Using the gradient domination lemma, we can provide the following iteration complexity bound on PGD, with proof in Appendix B.3:
Proposition 5.6. If Vθ is β-smooth in θ, then the PGD algorithm in (10) on Vθ(s0) with step size η = β1 satisﬁes

32|S |β

min Vθt(s0) − V∗(s0) ≤ D
t

(1 − γ)T + L + U ,

where D = maxt dξ∗θt /dξθθtt ∞.
This guarantee is provided for the best policy over T rounds, as is standard in the noncon√vex optimization literature. This lemma demonstrates that gradient descent converges in O(1/ T ) iterations, but is upper bounded by the same residuals present in the gradient domination lemma (Lemma 5.3). Even if T → ∞ and the ﬁrst term vanishes, it is possible for the policy to be suboptimal because the MCR objective is not gradient dominated. This phenomenon is evident in the motivating example of Section 5.1 and the optimization landscape in Figure 1b. However, if U and L are additionally small, which occurs when θ is near π∗ or the MCR Bellman operator is (close to) linear in the policy, Proposition 5.6 demonstrates that θ is near-optimal as well. Finally, as shown previously, when the Bellman operator is linear in the policy (as is the case for expected value), the residuals vanish and we recover the convergence guarantees for PGD from Agarwal et al. [2019] for expected value.
Coherent risk functionals are not, in general, β-smooth. CVaR, for example, is considered a non-smooth optimization problem [Alexander et al., 2006], and in optimizing this objective, the authors in Tamar et al. [2015b] make the assumptions that ∂ρ∂θ(θZ) and ∂λ∂θθ(ω) exist and are bounded. Under similar assumptions, we can guarantee β-smoothness of Vθ and applicability of the bound in Proposition 5.6 (proof in Appendix B.3):

Corollary 5.7. If the gradients of the primal and dual solutions of the MCR value function (4) exist and are bounded everywhere, i.e.,

dξθ ≤ c and dθ ∞

dλθ ≤ c dθ ∞

and the second derivative of the constraints exist and are bounded, i.e., for all e ∈ E and i ∈ I

d2ge(ξ, P ) dP 2 ≤ M and

d2fi(ξ, P ) dP 2 ≤ M

then there exists β such that the value Vθ is β-smooth for starting state s0, and Proposition 5.6 holds for Vθ.

10

6 Gradient Estimation

In our analysis of the global optimality of PG for MCR, we assumed access to the gradient (4.1).

In practice however, the gradient is diﬃcult to estimate because it is calculated with respect to an

expectation over reweighted transitions Pθξθ . Tamar et al. [2015a] propose an actor-critic method for optimizing the MCR, but require the ability to sample from an MDP with reweighted transitions Pθξθ ,

which is often unavailable in practice. Absent such a resource, one naive method for optimizing the

MCR is to draw samples from dθ and multiply each sample by its importance weight

t−1 j=1

ξθ,sj−1

(sj

),

shown in Algorithm 1.

Algorithm 1: Importance Sampling Gradient Estimation

Input:

for episode k = 1...K do

Generate

N

trajectories

τk

=

{s

(n 0

)

,

a(0n)

,

c(0n

)

,

...,

s

(n) T

,

a(Tn

)

,

cT(n)

}

N n=1

;

Solve

the

maximization

problem

in

(5)

to

obtain

ξk

,

λ

P k

,

λEk

,

λIk

for

all

(s, s

);

Compute hk(s, a) in (6);

Compute ∇θVθ =

N n=1

T t=0

γt

t−1 j=1

ξk,sj−1

(sj

)

hk(st, at);

Update θk+1 ← θk − η∇θVθ;

end

However, as demonstrated by Liu et al. [2018], importance sampling suﬀers from variance that
is exponential in the time horizon. This especially concerning for smaller CVaR levels α, as the maximum per-step importance weight is α1 , which can cause the magnitude of the gradient to become intractably large in longer horizons. Instead, we propose a practical method for optimizing the MCR
that leverages recent advances in oﬀ-policy evaluation and optimization. Moreover, we empirically
demonstrate its improved stability and eﬃcacy.

6.1 State Distribution Reweighting

Note that, using the stationary state distribution, we can write the gradient in (4.1) as

∇θVθ = dξθθ (s) πθ(a|s)∇θ log πθ(a|s)hθ(s, a)

s

a

=

dθ(s)wξθ (s) πθ(a|s)∇θ log πθ(a|s)hθ(s, a),

(11)

s:dθ (s)>0

a

where wξθ (s) := dξθθ (s)/dθ(s), which is well-deﬁned because dξθθ is always 0 whenever dθ is 0 by deﬁnition. If we can estimate the ratio wξθ , then we can calculate the gradient by sampling from the original stationary state distribution dθ. We now state a theorem that can be used to derive the
state distribution correction factor wξθ , inspired by Theorem 4 of Liu et al. [2018].

Theorem 6.1. For any γ ∈ (0, 1) and reweighting {ξs}s∈S where ξs : S → R, deﬁne

L(w, f ) := γEs,s ∼dθ ∆(w; s, a, s )f (s ) + (1 − γ)Es∼d0 1 − w(s) f (s) ,

(12)

11

where ∆(w; s, a, s ) := w(s)ξs(s ) − w(s ). Then w(s) equals wξ(s) := dξθ(s)/dθ(s) if and only if L(w, f ) = 0 for any measurable test function f .
This theorem suggests a method for estimating the MCR gradient using samples collected from dθ, by ﬁrst solving the maximization in (4) to obtaining ξθ, then using it to solve (12) to obtain the state distribution correction wξθ . The gradient can then be calculated using a sample-based estimate of (11). In high-dimensional settings, an RKHS kernel can be used for f [Liu et al., 2018].
Now that we have a method for estimating the the policy gradient of the MCR, we can use it with any PG algorithm. In Algorithm 2, we propose an actor-critic algorithm for optimizing the MCR. In episode k, we draw trajectories using policy πθk and use them to calculate the solutions (ξk, λk) to the min-max Lagrangian problem in (5). We then derive the state distribution corrections wk from (12) using ξk and calculate the gradient according to Theorem 4.1. The state distribution corrections w and Lagrangian variables ξ, λ can also be learned using neural networks (see discussion in Appendix C.4).

Algorithm 2: State Distribution Correction Actor-Critic

Input: Risk envelope U, learning rate η, MDP M.

for episode k = 1...K do

Generate

N

trajectories

τk

=

{s

(n 0

)

,

a(0n)

,

c(0n

)

,

...}

N n=1

using

πθk

in

M;

Pad τk if necessary;

Solve the maximization problem in Eq. (4) to obtain ξk, λPk , λEk , λIk for all (s, s ); Solve the optimization problem in Eq. (12) to obtain wk for all s;

Compute hk(s, a) in (6);

Compute ∇θVθ = a,s∼τk wk(s)∇θ log πθk (a|s)hk(s, a); Update θk+1 ← θk − η∇θVθ;

end

6.2 Convergence Result
We provide a convergence result for Algorithm 2 to a stationary point of the MCR objective. First, we state the following assumption for our convergence guarantee: Assumption 6.2. For all state-action pairs (s, a) and θ ∈ Θ, suppose that
1. ∂πθ∂(θa|s) ≤ G
2. Vθ(s), hθ(s, a) ≤ Cmax 3. σw2 := Edθ [wξθ (s)2] < ∞ 4. the MCR is L-Lipschitz and β-smooth (see Corollary 5.7) We have previously discussed the validity of Assumption 4 in Corollary 5.7, and Assumption 3 was previously made in Liu et al. [2020]. Assumption 1 can be achieved by an appropriate policy parameterization, such as through a linear or diﬀerentiable neural network function approximation
12

(a)

(b)

Figure 2: (a) CVaRα policies for α = 0.25 and α = 1 learned using Algorithm 2. (b) Average cost and success (from 1000 trajectories) over training. While the learned policy for α = 1 always takes the shortest path, it fails to consistently reach the goal when the environment is stochastic.

with softmax output. Assumption 2 follows from the boundedness of Vθ, an assumption common

in the literature, and boundedness of the Lagrangian solutions (ξθ, λ) present in the calculation of

hθ (6). Boundedness of ξ is guaranteed by the constraints in U . The MCR objective with CVaRα

satisﬁes

Assumption

2,

for

example,

because

|ξθ |

≤

1 α

by

deﬁnition,

λPθ

is

bounded

whenever

the

value is bounded because it is any (1 − α) quantile of the distribution of returns, and the remainder

of the terms in hθ are 0. Following this, for Algorithm 2, we can make the following convergence

guarantee to a stationary point, with proof in Appendix C.2.

Theorem 6.3. Suppose Algorithm 2 at episode k and parameters θk is provided with critic esti-

mates Vk, Lagrangian variable estimates ξk and λk, and distribution ratio estimates wk satisfying

Es∼d(wθk (s) − wk(s))2 ≤

2 w,k

and

Es∼d(hθk (s, a) − hθk (s, a))2

≤

2 h,k

for

all

episodes

k.

Then if

Assumption 6.2 is satisﬁed,

1K KE
k=1

2 2βCmax 1 K

∇θ Vθk

≤√ + KK

k=1

√6K ( 2w,kCm2 ax + 2h,k(σw2 + 2w,k))G2

+ 2L ( w,kCmax + h,k σw + w,k)G

Theorem 6.3 demonstrates that when Assumptions 6.2 hold, the average gradient magnitude

over the K episodes is upper bounded by three terms. The ﬁrst two terms vanish as K → ∞, but

the third does not. However, since the third term is in terms of h and w estimation errors, given

estimators h and w with small average error h,k, w,k across the episodes, the third term vanishes

as K → ∞ as well. Under such conditions, Theorem 6.3 shows that Algorithm 2 will converge to

an approximate stationary point. Estimators for h and w with small average error can be achieved

with

a

reasonable

online

critic

and

w, ξ, λ

learning

algorithms.

For

CVaRα,

Cmax

∝

1 α

due

to

the

13

presence of ξ in the calculation of hθ (6), implying that with higher risk sensitivity, convergence to stationary points occurs more slowly.
7 Experiments
In this section, we examine the empirical eﬃcacy of Algorithm 2 with two primary questions in mind:
• Does the state distribution correction reduce the variance of PG on MCR in MDPs?
• Does increasing the risk envelope set U lead to more risk-sensitive behavior?
Baseline and Implementation. To answer these questions, we optimize the MCR with the CVaRα objective at diﬀerent levels α, where smaller α corresponds to higher risk aversion. To optimize this objective, we run Algorithm 2 and compare its performance against importance sampling baselines (Algorithm 1). For both, we use a softmax tabular policy and tabular critic. Full implementation details and a function approximation version of this algorithm are provided in Appendix C.3.
Simulation Domain. We compare the algorithms on the classic 4 × 12 tabular Cliﬀwalk environment, where the agent needs to travel from a start to a goal state while incurring as little cost as possible. Each action incurs 1 cost, but the shortest path lies next to a cliﬀ and entering the cliﬀ corresponds to a cost of 100. Because the classic Cliﬀwalk environment is deterministic, we also compare our algorithms on a stochastic version, where the row of cells above the cliﬀ is “slippery", and induce a transition from these cells into the cliﬀ with probability p. Although our algorithm is intended for an inﬁnite-horizon MDP, the horizon is ﬁxed to a maximum of 500 timesteps for computational feasibility.
Results. Figure 2a displays the learned policies for diﬀerent CVaR levels α in the deterministic and stochastic environments. Interestingly, the α = 0.25 policy ﬁnds a conservative policy and takes a longer path in both deterministic and stochastic Cliﬀwalk, even though the optimal policy in the deterministic environment is to take the shortest path for all α levels. In the stochastic cliﬀwalk, the α = 1 policy learns to take the greedy path but risks falling into the cliﬀ, resulting in failure to consistently succeed at the task. We display the learned state distribution correction and its discussion in Appendix C.3. In both environments, the gradient of the naive importance sampling method diverged, and learning a policy was not possible in either case. In our experiments, a policy in only a truncated Cliﬀwalk of size 2 × 4 was learnable via importance sampling.
8 Discussion
In this paper, we answered several open questions concerning PG for coherent risk functionals. First, we demonstrated that MCR objectives are not in general gradient dominated, giving tight upper bounds on the suboptimality of the learned policy. Second, we proposed an algorithm based on stationary state distribution reweighting to relax stringent assumptions of previous work and tractably estimate the MCR gradient. We demonstrated that our algorithm is guaranteed to converge to stationary points, and demonstrated the stability and eﬃcacy of our algorithm on the
14

CliﬀWalk environment, in which importance sampling cannot learn policies due to exploding gradient magnitudes.
While coherent risk objectives are often used in practice, our results highlight the challenges of obtaining global optimality guarantees for the ﬁnal policy. One important direction of future work lies in improving these convergence results through better characterization of the residual terms over the learning process, and expanding it to other PG algorithms and settings. Further, the lower bound in Theorem 5.4 indicates that poorly conditioned starting states or initial policies also cause convergence to suboptimal policies. In the future, we will investigate how diﬀerent exploration strategies, such using a combination of expected value and coherent risk, may help to mitigate this issue both in theory and practice.
Acknowledgements
Liu Leqi is generously supported by an Open Philanthropy AI Fellowship. Zachary Lipton thanks Amazon AI, Salesforce Research, the Block Center, the PwC Center, Abridge, UPMC, the NSF, DARPA, and SEI for supporting ACMI lab’s research on robust and socially aligned machine learning.
References
Carlo Acerbi. Spectral measures of risk: A coherent representation of subjective risk aversion. Journal of Banking & Finance, 26(7):1505–1518, 2002.
Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift, 2019.
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018.
Siddharth Alexander, Thomas F Coleman, and Yuying Li. Minimizing cvar and var for a portfolio of derivatives. Journal of Banking & Finance, 30(2):583–605, 2006.
Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk. Mathematical ﬁnance, 9(3):203–228, 1999.
Kamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy gradient in partially observable environments: Approximation and convergence. arXiv e-prints, pages arXiv–1810, 2018.
Amir Beck. First-order methods in optimization. SIAM, 2017.
Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. arXiv preprint arXiv:1707.06887, 2017.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv preprint arXiv:1906.01786, 2019.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. In Advances in neural information processing systems, pages 3509–3517, 2014.
15

Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decisionmaking: a cvar optimization approach. In Advances in Neural Information Processing Systems, pages 1522–1530, 2015.
Dotan Di Castro, Aviv Tamar, and Shie Mannor. Policy gradients with variance related risk criteria. arXiv preprint arXiv:1206.6404, 2012.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.
Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoﬀ in regret, 2020.
Tanner Fiez and Lillian Ratliﬀ. Gradient descent-ascent provably converges to strict local minmax equilibria with a ﬁnite timescale separation, 2020.
Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437–1480, 2015.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59–99, 2016.
Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be conservative: Quickly learning a cvar policy. arXiv preprint arXiv:1911.01546, 2019.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Inﬁnitehorizon oﬀ-policy estimation. arXiv preprint arXiv:1810.12429, 2018.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Oﬀ-policy policy gradient with stationary distribution correction. In Uncertainty in Artiﬁcial Intelligence, pages 1180–1190. PMLR, 2020.
Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional reinforcement learning for eﬃcient exploration. arXiv preprint arXiv:1905.06125, 2019.
Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2): 583–601, 2002.
Georg Ch Pﬂug and Alois Pichler. Time-consistent decisions and temporal decomposition of coherent risk functionals. Mathematics of Operations Research, 41(2):682–699, 2016.
LA Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive mdps. In Advances in neural information processing systems, pages 252–260, 2013.
LA Prashanth, Cheng Jie, Michael Fu, Steve Marcus, and Csaba Szepesvári. Cumulative prospect theory meets reinforcement learning: Prediction and control. In International Conference on Machine Learning, pages 1406–1415, 2016.
Andrzej Ruszczyński. Risk-averse dynamic programming for markov decision processes. Mathematical programming, 125(2):235–261, 2010.
16

Andrzej Ruszczyński and Alexander Shapiro. Optimization of convex risk functions. Mathematics of operations research, 31(3):433–452, 2006.
A Shapiro, D Dentcheva, and A Ruszczynski. Lectures on stochastic programming: Modeling and theory, mos-siam ser. Optim., SIAM, Philadelphia, 2009.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for
coherent risk measures. In Advances in Neural Information Processing Systems, pages 1468–1476, 2015a. Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, pages 2993–2999, 2015b.
17

Appendix

A Proofs for Section 4.1

Proof of Theorem 4.1. The proof of this theorem is given in Appendix D of Tamar et al. [2015a], and we repeat it here for completeness. The value Vθ is deﬁned as:

Vθ(s) = C(s) + γ max Pθ(s |s)ξ(s )Vθ(s )
ξ∈U (Pθ) s
= C(s) + γ max min Lθ,s(ξ, λP , λI , λE )
ξλ

Using the envelope theorem [Milgrom and Segal, 2002],

∇θVθ(s) = γ∇θ max min Lθ,s(ξ, λP , λI , λE )
ξλ
= γ∇θLθ,s(ξ, λP , λI , λE )|(ξθ,λPθ ,λEθ ,λIθ )

The derivative of the Lagrangian evaluated at the saddle point (ξθ, λPθ , λEθ , λIθ ) is:

∇θLθ,s(ξθ,s, λPθ,s, λEθ,s, λIθ,s)

= ξθ,s(s ) ∇θπθ(a|s)P (s |s, a)Vθ(s ) + Pθ(s |s)ξθ,s(s )∇θVθ(s )

s

a

s

− λPθ,s ξθ,s(s ) ∇θπθ(a|s)P (s |s, a)

s

a

− λIθ,s(i) dfi(dξpθ(,ss,)Pθ) ∇θπθ(a|s)P (s |s, a) − λEθ,s(e) dged(ξpθ(,ss,)Pθ) ∇θπθ(a|s)P (s |s, a)

i∈I

a

e∈E

a

Then we have

∇θVθ(s) = γ Pθ(s |s)ξθ,s(s )∇θVθ(s ) + ∇θπθ(a|s) γ

s

a

s

− λIθ,s(i) dfi(dξpθ(,ss,)Pθ) − λEθ,s(e) dged(ξpθ(,ss,)Pθ)

i∈I

e∈E

P (s |s, a) ξθ,s Vθ(s ) − λPθ,s

= ∇θπθ(a|s)hθ(s, a) + γ Pθ(s |s)ξθ,s(s )∇θVθ(s )

a

s

with hθ(s, a) as deﬁned in (6). Unfolding the recursion, this gives us

∇θVθ(s) = ∇θπθ(a0|s0)hθ(s0, a0) + γ Pθ(s1|s0)ξθ,s0 (s1) ∇θπθ(a1|s1)hθ(s1, a1)

a

s1

+ γ Pθ(s2|s1)ξθ,s1 (s2)∇θVθ(s2)
s2

and unfolding the recursion further gives the result.

18

B Proofs for Section 5

B.1 Proof of Lemma 5.3

We ﬁrst establish the performance diﬀerence lemma (Lemma B.1) for MCR, which upper bounds the optimality gap using an advantage function. This lemma holds for all policies, not only the directly parameterized policies we consider in Section 5.

Lemma B.1 (Performance Diﬀerence Lemma for Markov Coherent Risk). For all policies π and

starting state s0

Vπ(s0) − V∗(s0) ≤ Es∼dξ∗π Ea∼π∗ Aπ(s, a) + γbπ,s

(13)

where

Aπ(s, a) = uπ(s) − tπ(s, a)

tπ(s, a) = γ P (s |s, a)ξπ,s(s ) Vπ(s ) − λ∗,s
s
uπ(s) = π(a|s)tπ(s, a)
a
bπ,s = λI∗,s(i) fi(ξπ,s, P∗) − fi(ξπ,s, Pπ)
i∈I

+ λE∗,s(e) ge(ξπ,s, P∗) − ge(ξπ,s, Pπ)
e∈E

Proof. For organizational purposes, we denote

fπ,s := λI∗,s(i)fi(ξπ,s, Pπ) and f∗,s := λI∗,s(i)fi(ξπ,s, P∗),

i∈I

i∈I

gπ,s :=

λ

E ∗,s

(

e)

ge

(

ξπ

,s

,

Pπ

)

and

g∗,s :=

λ

E ∗,s

(

e)

ge

(ξ

π

,s

,

P∗

)

.

e∈E

e∈E

Then

bπ,s = f∗,s − fπ,s + g∗,s − gπ,s

First we expand V∗(st) using the deﬁnition of the Lagrangian in (5), where (ξ∗,st, λP∗,st, λI∗,st, λE∗,st) =

arg max arg min L∗,st(ξ, λP , λI , λE )

ξ

λP ,λI ,λE

V∗(s0) − Vπ(s0) = C(s0) + γL∗,s0 (ξ∗,s0 , λP∗,s0 , λI∗,s0 , λE∗,s0 ) − Vπ(s0) Because ξ∗,s0 maximizes the Lagrangian L∗,s0, we can lower bound using ξπ,s0, which maximizes Lπ,s0 :
≥ C(s0) + γL∗,s0 (ξπ,s0 , λP∗,s0 , λI∗,s0 , λE∗,s0 ) − Vπ(s0)
Expanding L∗,s0 using (5),

= π∗(a0|s0) C(s0) − γλ∗,s0 P (s1|s0, a0)ξπ,s0 (s1) − γλ∗,s0 − γf∗,s0 − γg∗,s0

a0

s1

19

+ γ P (s1|s0, a0)ξπ,s0 (s1)V∗(s1) − Vπ(s0)
s1
We can apply this Lagrangian lower bound to every nested value V∗(st) and expand accordingly.

≥ π∗(a0|s0) C(s0) − γλ∗,s0 P (s1|s0, a0)ξπ,s0 (s1) − γλ∗,s0 − γf∗,s0 − γg∗,s0

a0

s1

+ γ P (s1|s0, a0)ξπ,s0 (s1)
s1

π∗(a1|s1) C(s1) + ...
a1

− Vπ(s0)

Then adding and subtracting Vπ(st) at each timestep t, and rearranging:

= π∗(a0|s0) C(s0) + Vπ(s0) − Vπ(s0) − γλ∗,s0 P (s1|s0, a0)ξπ,s0 (s1)

a0

s1

− γλ∗,s0 − γf∗,s0 − γg∗,s0 + γ P (s1|s0, a0)ξπ,s0 (s1)
s1

π∗(a1|s1) C(s1)
a1

+ Vπ(s1) − Vπ(s1) + ... − Vπ(s0)

= π∗(a0|s0) C(s0) − Vπ(s0) − γλ∗,s0 P (s1|s0, a0)ξπ,s0 (s1)

a0

s1

− γλ∗,s0 − γf∗,s0 − γg∗,s0 + γ P (s1|s0, a0)ξπ,s0 (s1)Vπ(s1)
s1

+ γ P (s1|s0, a0)ξπ,s0 (s1)
s1

π∗(a1|s1) C(s1) − Vπ(s1) + ...
a1

Using the deﬁnition of dξ∗π (s),

= dξ∗π (s) π∗(a|s) C(s) + γ P (s |s, a)ξπ,s(s )Vπ(s )

s

a

s

− γλ∗,s P (s |s, a)ξπ,s(s ) − γλ∗,s − γf∗,s − γg∗,s − Vπ(s)
s

Then substituting (5) for Vπ(s),

= dξ∗π (s) π∗(a|s) C(s) + γ P (s |s, a)ξπ,s(s )Vπ(s )

s

a

s

− γλ∗,s P (s |s, a)ξπ,s(s ) − γλ∗,s0 − γf∗,s − γg∗,s
s

− C(s) + γLπ,s(ξπ,s, λPπ,s, λIπ,s, λEπ,s)

20

Because λπ,s minimizes the Lπ,s, we lower bound using λ∗,s:

≥ dξ∗π (s) π∗(a|s) C(s) + γ P (s |s, a)ξπ,s(s )Vπ(s )

s

a

s

− γλ∗,s P (s |s, a)ξπ,s(s ) − γλ∗,s0 − γf∗,s − γg∗,s
s

− C(s) + γLπ,s(ξπ,s, λP∗,s, λI∗,s, λE∗,s)

Finally, expanding the Lagrangian and grouping terms,

= dξ∗π (s) π∗(a|s) γ P (s |s, a)ξπ,s(s )Vπ(s ) − γλ∗,s P (s |s, a)ξπ,s(s )

s

a

s

s

− γλ∗,s − γf∗,s − γg∗,s − uπ(s) − γλ∗,s − γfπ,s − γgπ,s

= dξ∗π (s) π∗(a|s) tπ(s, a) − uπ(s) − γbπ,s

s

a

From the above, ﬂipping the direction of the inequality we obtain

Vπ(s0) − V∗(s0) ≤ dξ∗π (s) π∗(a|s) uπ(s) − tπ(s, a) + γbπ,s

s

a

Rearranging and using the deﬁnition of Aπ(s, a) gives the result.

Proof of Lemma 5.3. We now give a proof for the gradient domination lemma for directly parameterized policies, and going forward we interchangeably refer to the policy as π and θ. From the performance diﬀerence lemma B.1, we have

Vθ(s0) − V∗(s0) ≤ dξ∗θ (s)
s
Using the fact that a πθ(a|s)Aθ(s, a) = 0,

π∗(a|s)Aθ(s, a) + γbθ,s
a

Since

Vθ(s0) − V∗(s0) ≤ dξ∗θ (s)

s

a

π∗(a|s) − πθ(a|s) Aθ(s, a) + γbθ,s

a(π(a|s) − π (a|s))uθ(s) = 0 for any two policies π, π

= dξ∗θ (s)

s

a

πθ(a|s) − π∗(a|s) tθ(s, a) + γbθ,s

Under direct parameterization where θs,a = πθ(a|s), the gradient from Theorem (4.1) is

∂Vθ(s0) = dξθ (s)hθ(s, a)

∂θs,a

θ

21

Due to the nonlinearity in the objective, while tθ contains λ∗,s, hθ is deﬁned using λθ,s. Further, hθ involves the derivatives of the terms in bθ,s. Rearranging the previous inequality,

Vθ(s0) − V∗(s0) ≤ dξ∗θ (s)

s

a

πθ(a|s) − π∗(a|s) tθ(s, a) + γbθ,s

≤ max

dξ∗θ (s)

π∈∆(A)S s

a

πθ(a|s) − π(a|s) tθ(s, a) + γbθ,s

dξ∗θ ≤ξ

max

dξ∗θ (s)

dθθ ∞ π∈∆(A)S s

a

πθ(a|s) − π(a|s) tθ(s, a) + γbθ,s

dξ∗θ ≤ξ

max

dξ∗θ (s)

dθθ ∞ π∈∆(A)S s

a

πθ(a|s) − π(a|s) hθ(s, a) + L(θ, π∗) +

dξ∗θ =ξ

max

dξ∗θ (s) πθ − π

dθθ ∞ π∈∆(A)S s

∇θVθ + L(θ, π∗) + U (θ, π∗)

U (θ, π∗)

where the second to last line uses the residual deﬁnitions (8) and (9), and the last line uses the deﬁnition of the gradient.

B.2 Proof of Theorem 5.4

s0

s1

s2

sT

Figure 3: The MDP for the proof of Theorem 5.4
Consider the MDP of Figure 3. The MDP has the following transitions:
P (s2|s0, a1) = 1 P (s1|s0, a2) = 0.9 P (s2|s0, a2) = 0.1 P (sT |s1) = P (sT |s2) = 1
and the following costs:
C(s0) = 0, C(s1) = 0, C(s2) = 1, C(sT ) = 0
There are 4 states, with sT being absorptive, and only 2 actions. The policy πθ aﬀects only the transitions from the starting state s0 to either of the next states s1 or s2. Note that this MDP follows the same dynamics as a binomial 2-armed bandit, where arm 1 always returns cost 1 and

22

arm 2 returns a cost of 0 90% of the time and 10% of the time returns cost 1. Suppose we work with direct parameterization, where πθ(a1|s) = θa1,s and πθ(a2, s) = 1 − θa2,s. We limit our consideration to θa1,s0 = θ and θa2,s0 = 1 − θ to avoid overparameterizing our policy, as the policy does not aﬀect the cost accrued after s0. Let us consider α = 0.5, the expectation over the highest 50% of the distribution of the total discounted returns. We will show that there exists θ in this MDP such that the gradient domination bound in Lemma 5.3 is tight, thus giving Theorem 5.4.
We can deterministically calculate the Markov coherent risk value of states s1, s2 using the deﬁnition (4):
V MC(s1) = 0
V MC(s2) = 1

Optimal Policy θ∗: The optimal policy in the setting is θ = 0, which always takes action a2.

Policy θ: Set θ = 94 . The value of the initial state V MC can then be calculated using (4). The maximization problem
can be solved by either using the analytical solution to the CVaR risk envelope or by convex programming solver, which gives us the primal solution ξs0 and dual solution λs0 for θ∗ and θ, which are shown in the table below:

ξs0 (s1) ξs0 (s2) λP V MC(s0)

θ

0

2

1

1

θ∗ 8/9

2

0

0.2

The optimality gap is

Vθ(s0) − V∗(s0) = 0.8γ

We will show that the bound in Theorem 5.3 is tight for θ, and proceed by determining the terms on the RHS of the bound.
First, we calculate the gradient ∇θVθ(s0) using Theorem 4.1. Under direct parameterization in our MDP, we have that
∂Vθ(s0) = hθ(s0, a). dθs0,a
Using the Lagrangian solutions from the table, we obtain that

hθ(s0, a1) = hθ(s0, a2) = 0

which indicates that the magnitude of the gradient is 0. Next, we calculate the residual term res using the Lagrangian solutions:

γ λ∗θ,,sP0 − λ∗∗,,Ps0

θs0,a − θ∗,s0,a

P (s |s, a)ξθ,s0(s ) = 0.8γ.

a

s

The last residual term satisﬁes U = 0 because the constraint expressions fi are independent of θ. Then we have the desired result, i.e.,

Vθ(s0) − V∗(s0) = L(θ, π∗) = 0.8γ.

23

B.3 Proof of Proposition 5.6
First, we restate Proposition B.1 from Agarwal et al. [2019], built upon results from Ghadimi and Lan [2016], without proof, which we will later use in the proof of our theorem.
Proposition B.2 (Proposition B.1 from Agarwal et al. [2019]). Let Vθ(s0) be β-smooth in θ and the update rule is θ+ = θ − ηGη(θ). If V η 2 ≤ then

max (π − π) ∇θVθ(s0) ≤ ηβ + 1
π∈∆(A)|S|

Proof of Proposition 5.6. Let

Gη(θ) = 1 θ − P∆(A)S (θ − η∇θVθ(s0)) η

Let θt, Vt respectively be the parameter and value parameterized by θt at time t. Using our

assumption

that

Vθ (s0 )

is

β-smooth,

and

from

Beck

[2017]

we

have

that

for

step

size

η

=

1 β

,

after

T

steps of PGD,

min Gη(θt) 2 ≤
t

2β(V0(s0) − V∗(s0)) T

Then using Proposition B.2 with η = 2β(V0(s0T)−V∗(s0)) ,

min

max

δ ∇θVθt+1 (s0) ≤ (ηβ + 1)

t θt+δ∈∆(A)S , δ 2≤1

2β(V0(s0) − V∗(s0)) T

Because π − π 2 ≤ 2 |S|,

max (π − π) ∇θVθ(s0) = 2
π∈∆(A)|S|
≤2

|S| max
π∈∆(A)|S|

1 (π − π) ∇θVθ(s0)
2|S |

|S |

max

δ ∇θVθ(s0)

θ+δ∈∆(A)S , δ 2≤1

Then using the gradient domination lemma (Lemma 5.3) and the fact that ηβ = 1,

dξ∗θt

2β(V0(s0) − V∗(s0))

mtin Vt(s0) − V∗(s0) ≤ mtax dξθt ∞4 |S|

+ max

T

t

θt

dξ∗θt

≤ max
t

ξθt ∞

dθt

32|S |β (1 − γ)T + L + U

L(θt, π∗) + U (θt, π∗)

where

the

last

line

uses

Assumptions

5.1

and

5.2

and

the

fact

that

V0(s0)

−

V∗(s0)

≤

1

1 −γ

.

Lemma B.3 (Smoothness of Vθ for Direct Parameterization). If the gradients of the primal and

dual solutions of the Markov coherent risk value function (4) exist and are bounded everywhere, that

is

dξθ ≤ c and dλθ ≤ c

dθ ∞

dθ ∞

24

and the second derivative of the constraints exist and are bounded, that is for all e ∈ E and i ∈ I

d2ge(ξ, P ) dP 2 ≤ M and

d2fi(ξ, P ) dP 2 ≤ M

then there exists β such that the value Vθ is β-smooth for starting state s0, that is

∇θVθ(s0) − ∇θVθ (s0) 2 ≤ β θ − θ 2
Proof. Let πα := πθ+αu where u is a unit vector in Rd, and let Vα denote the value function under policy πα. Note that our value function is

Vα(s) = C(s) + max Pα(s |s)ξ(s )Vα(s )
ξ∈U (Pα) s
= C(s) + max min Lα,s(ξ, λP , λE , λI )
ξ λP ,λE ,λI

and ∀s ∈ S, let (ξα,s, λPα,s, λEα,s, λIα,s) denote the saddle points of the Lagrangian Lα,s under policy πα.We will prove that the second derivative dd22Vαα is bounded above by the smoothness parameter β. Let P (α) ∈ R|S|×|S| be the state-state transition matrix under πα

[P (α)]s→s = πα(a|s)P (s |s, a)
a
Further, deﬁne the ξα-reweighted transition to be

[Pξ(α)]s→s = [P (α)ξα]s→s = ξα,s(s ) πα(a|s)P (s |s, a).
a

For notation simplicity, let

yα(s) = λEα,s(e) dge(ξdαP,s, Pα) + λIα,s(i) dfi(ξdαP,s, Pα)

e

i

Under our assumptions, we have that

1.

dπα(a|s) a dα

≤ C1

α=0

2.

d2πα(a|s) a d2α

≤ C2

α=0

3. ξα ∞ ≤ C3 (for CVaRα, C3 = 1/α)

4. λα ∞ ≤ C4 (for CVaR, C4 = 1)

5.

dξα dα

∞ ≤ C5

6. ddλα∗α ∞ ≤ C6 for all dual solutions λα

25

7.

dyα dα

∞ ≤ C7 as a consequence of 6.

and the assumption that the second derivative of the

constraints is bounded.

We have that for an arbitrary vector x that

We also have

max
u 2=1

dP (α) dα x s ≤ C1 x ∞

max
u 2=1

d2P (α) dα2 x ≤ C2 x ∞
s

max
u 2=1

dPξ(α) x = max

dα

s

u 2=1 a,s

dπα(a|s ) ξα,s(s ) + πα(a|s ) dξα,s(s )

P (s |s, a)xa,s

dα

dα

α=0

≤ dπα(a|s) ξα,s P (s |s, a)|xs ,a| + πα(a|s) dξα,s(s ) P (s |s, a)|xs,a|

a,s dα α=0

a,s dα α=0

≤ (C1C3 + C5) x ∞

and

max
u 2=1

dP (α) dξα x = max

dπα(a|s ) dξα,s(s ) P (s |s, a)xa,s

dα dα s

u 2=1 a,s

dα

dα α=0

≤ dπα(a|s)

dξα,s(s )

P (s |s, a)|xs,a

a,s dα α=0 dα α=0

≤ C1C5 x ∞.

We can write Vα(s0) as

−1
Vα(s0) = es0 I − γPξ(α) C,

where C ∈ R|S| is a vector whose s-th entry is the cost of state s. Using the envelope theorem,

d

d

dα Vα(s) = dα Lα,s(ξ, λ)|(ξα,s,λα,s)

= γ dPα(s |s) ξα,s(s )Vα(s ) +

dα

s

s

− λPα,s
s

dPα(s |s) ξα,s(s ) − dα
s

Pα(s |s)ξα,s(s ) dVα(s ) dα
dPα(s |s) yα(s) . dα

Writing this in matrix form and substituting in our previous expression for Vα, we get

dVα(s) = γes

−1
I − γPξ(α)

dP (α) ξα

I − γPξ(α)

−1C − λPα dP (α) ξα − dP (α) yα

.

dα

dα

dα

dα

26

Next, we take the second derivative using the chain rule:

d2Vα(s) dα2
= γ2es

I − γPξ(α)

−1 dPξ(α) dα

−1
I − γPξ(α)

dP (α) ξα

I − γPξ(α)

−1C − λPα dP (α) ξα − dP (α) yα

dα

dα

dα

−1 d2P (α)

−1 dP (α) dξα

−1

+ γes I − γPξ(α)

dα2 ξα I − γPξ(α) C + dα dα I − γPξ(α) C

dP (α)

−1 dPξ(α)

−1

+γ

ξα I − γPξ(α)

I − γPξ(α) C

dα

dα

dλPα dP (α)

P d2P (α)

dP (α) dξα

− dα

dα ξα − λα

dα2 ξα + dα dα

d2P (α)

dP (α) dyα

− dα2 yα − dα dα

Let M (α) = as

−1
I − γPξ(α) . By using power series expansion of matrix inverse, we can write M (α)

∞

n

M (α) = γn P (α)ξα .

n=0

This

implies

that

M (α)

≥

0

componentwise

and

each

row

of

M (α)

sums

to

1 1−

γ

,

that

is

M (α)1

=

1 1−γ

1.

This

implies

1 mu a2=x1 M (α)x ∞ ≤ 1 − γ x ∞

Then, we have

max dVα(s)

dP (α)

≤ γ M (α)

ξαM (α)C

+ γ M (α)yα

+ γ λP M (α)ξα

u 2=1 dα α=0

dα

∞

∞α ∞

γ

dP (α)

γ

γ

≤ (1 − γ)2 Cmax dα ξα ∞ + 1 − γ yα ∞ + 1 − γ C4 ξα ∞

Note that by Assumption 3.2, yα is upper bounded by |E| + |I| M C4,

γ

γ

γ

≤ (1 − γ)2 C1C3Cmax + 1 − γ |E| + |I| M C4 + 1 − γ C1C3.

27

Similarly,

d2Vα(s)

2

dPξ (α)

dP (α)

2

dPξ (α)

max dα2 α=0 ≤ γ M (α) dα M (α) dα ξαM (α)C ∞ + γ M (α) dα M (α)yα ∞

u 2=1

+ γ2 M (α) dPdξα(α) M (α)λPα M (α)ξα ∞

d2P (α)

dP (α) dξα

+ γ M (α)

dα2

ξαM (α)C + γ M (α)
∞

dα

M (α)C

dα

∞

+ γ2 M (α) dP (α) ξαM (α) dPξ(α) M (α)C

dα

dα

∞

dλPα dP (α)

P d2P (α)

dP (α) dξα

+ γ M (α) dα

dα

ξα + γ M (α)λα
∞

dα2 ξα + dα dα ∞

d2P (α)

dP (α) dfα

+ γ M (α) dα2 yα ∞ + γ M (α) dα dα ∞

γ2

γ2

≤

2 (1

−

γ)3

C1C4(C1C3

+

C5)Cmax

+

(1

−

γ)2

(C1C3

+

C5)

|E| + |I|

M C4

γ2

γ

+ (1 − γ)3 (C1C3 + C5)C3C4 + (1 − γ)2 Cmax C2C3 + C1C5

γ + 1 − γ C1C3C6 + C2C3C4 + C1C5 + C2 |E| + |I| M C4 + C1C7

= β.

Under direct parameterization, C1 ≤ |A| and C2 = 0. The result follows using β as the smoothness parameter.

C Proofs for Section 6

C.1 Proof of Theorem 6.1
Proof of Theorem 6.1. Deﬁne the starting state distribution as d0(s) = 1[s = s0]. Then given
reweighting {ξs}s∈S : S → R and transitions Pθ induced by policy πθ, we establish the relation for any γ ∈ (0, 1] that

∞

dξθ(s ) = (1 − γ) γtPθξ(st = s )

t=0

∞

= (1 − γ)d0(s ) + (1 − γ) γtPθξ(st = s |s0)

t=1

∞

= (1 − γ)d0(s ) + γ(1 − γ) γtPθξ(st+1 = s )

t=0

∞

= (1 − γ)d0(s ) + γ(1 − γ) γt Pθ(s |s)ξs(s )Pθξ(st = s)

t=0

s

28

∞

= (1 − γ)d0(s ) + γ Pθ(s |s)ξs(s ) (1 − γ) γtPθξ(st = s)

s

t=0

= (1 − γ)d0(s ) + γ Pθ(s |s)ξs(s )dξθ(s)

(14)

s

Using the same lines of reasoning, for γ ∈ (0, 1]

dθ(s ) = (1 − γ)d0(s ) + γ Pθ(s |s)dθ(s)

(15)

s

With this, dθ can be seen as the invariant distribution of an induced Markov chain which follows Pθ with probability γ and restarts from the initial state s0 with probability 1 − γ. Similarly, dξθ can be seen as the invariant distribution of an induced Markov chain which follows Pθξ with probability γ
and restarts from the initial state s0 with probability 1 − γ.

Multiplying both sides of (15) by f (s ) and summing over s , we have that

dθ(s )f (s ) = (1 − γ) d0(s )f (s ) + γ Pθ(s |s)dθ(s)f (s ).

(16)

s

s

s,s

Similarly,

dξθ(s )f (s ) = (1 − γ) d0(s )f (s ) + γ Pθ(s |s)ξs(s )dξθ(s)f (s )

s

s

s,s

wξ(s )dθ(s )f (s ) = (1 − γ) d0(s )f (s ) + γ Pθ(s |s)wξ(s )dθ(s)f (s )

(17)

s

s

s,s

where the last line uses the deﬁnition wξ(s) := dξθ(s)/dθ(s). The relation in (16) is equivalent to

0 = E(s,s )∼dθ [γf (s ) − f (s)] + (1 − γ)Es∼d0 [f (s)]

(18)

Then using w(s)f (s) for f (s),

= E(s,s )∼dθ [γw(s )f (s ) − w(s)f (s)] + (1 − γ)Es∼d0[w(s)f (s)].

(19)

Plugging (19) into (12),

L(w, f ) = γE(s,s )∼dθ [ξs(s )w(s)f (s )] − Es∼dθ [w(s))f (s)] + (1 − γ)Es∼d0[f (s)]

(20)

Thus when L(w, f ) = 0, (20) is equivalent to (16), which means that w(s) = wξ(s).

C.2 Proof of Theorem 6.3

First, we state and prove an upper bound on the magnitude of gradients using the bias and variance of a gradient oracle in Theorem C.1. Suppose we have a function f : Rd → R which is diﬀerentiable,
L-Lipschitz, and β-smooth, with an inﬁmum at f∗. Suppose have access to a noisy gradient oracle which returns a vector ∇θf (x) ∈ Rd given a query point x. The vector is said to be σ, B-accurate for parameters σ, B ≥ 0 if for all x ∈ Rd, the quantity δ(x) := ∇θf (x) − ∇θf (x) satisﬁes

E[δ(x)|x] ≤ B and E[ δ(x) 2|x] ≤ 2(σ2 + B2)

(21)

Then we have the following guarantee for convergence to a stationary point under stochastic gradient descent with the update x ← x − η∇θf (x):

29

Theorem C.1. Suppose f is diﬀerentiable, L-Lipschitz, and β-smooth, and the approximate gradient
oracle satisﬁes (21) with parameters (σk, Bk) for all iterations k. Then after K iterations, stochastic gradient descent with initial point x1 and stepsize η = √1 satisﬁes
βK

1K

2 2β

K2

2

2 2L

K

E[ ∇f (xk) ] ≤ √ (f (x1) − f∗) + K

K√K (σk + Bk) + K Bk

(22)

k=1

k=1

Additionally, suppose there exists constants B, σ such that Bk ≤ B and σk ≤ σ for all k. Then

1K

2 2β

22

2

E[ ∇f (xk) ] ≤ √ (f (x1) − f∗) + √ (σ + B ) + 2LB.

(23)

K
k=1

K

K

Proof. Since f is β-smooth, we have

f (xk+1) ≤ f (xk) +

∇f (xk), xk+1 − xk

β +

xk+1 − xk

2

2

= f (xk) − η ∇f (xk), ∇f (xk) + βη2 ∇f (xk) 2 2

= f (xk) − η ∇f (xk), δ(xk) + ∇f (xk) + βη2 δ(xk) + ∇f (xk) 2 2

βη2 = f (xk) + 2 − η

∇f (xk) 2 − (η − βη2) ∇f (xk), δ(xk) + βη2 δ(xk) 2 2

where the second step follows from deﬁnition of ∇f (x) and the third step uses the deﬁnition of δ(x). Taking the expectations of both sides and using the properties of (21),

βη2

2

2

22

2

E[f (xk+1)] ≤ E[f (xk)] + 2 − η E[ ∇f (xk) ] + (η − βη )LBk + βη (σk + Bk),

Then summing over iterations k = 1, ...K,

E[f (xK+1)] ≤ f (x1) +

βη2 −η
2

K

K

E[ ∇f (xk) 2] + (η − βη2)LBk + βη2(σk2 + Bk2),

k=1

k=1

Using the fact that f (xK+1) ≥ f∗ and rearranging,

βη2 η−
2

K

K

E[ ∇f (xk) 2] ≤ f (x1) − f∗ + (η − βη2)LBk + βη2(σk2 + Bk2),

k=1

k=1

Now

choosing

η

=

√1 ,
Kβ

note

that

η −βη2

≤

√1 Kβ

and

η − 12 βη2

≥

√1 .
2 Kβ

Plugging

these

inequalities

in,

1K

2

KL

12

2

√ 2 Kβ

E[ ∇f (xk) ] ≤ f (x1) − f∗ +

√ Kβ

Bk

+

βK

(σk

+

Bk )

k=1

k=1

30

Rearranging,

1K

2 2β

K 2L

2

2

2

K

E[ ∇f (xk) ] ≤ √ (f (x1) − f∗) + K

K

Bk

+

√ KK

(σk

+

Bk )

k=1

k=1

giving the ﬁrst statement in the theorem. To achieve the second, use the upper bound B ≥ Bk and σ ≥ σk for all k:

1K

2 2β

22

2

E[ ∇f (xk) ] ≤ √ (f (x1) − f∗) + √ (σ + B ) + 2LB

K
k=1

K

K

Next, we use Theorem C.1 to prove our convergence result in Theorem 6.3:
Proof of Theorem 6.3. We determine σk, Bk for Algorithm 2. For short, ﬁrst deﬁne pθ(s, a) = ∂ log∂πθθ(a|s) . Then the true gradient ∇θV is given by

∇θV = Es,a∼dξθ pθ(s, a)hθ(s, a) = Edθ w(s)pθ(s, a)hθ(s, a) θ

and the estimated gradient is ∇θV : ∇θV = wˆ(s)pθ(s, a)hˆθ(s, a)

We can bound the bias as

E[∇θV − ∇θV |θ] = Edθ w(s)pθ(s, a)hθ(s, a) − wˆ(s)pθ(s, a)hˆθ(s, a)

≤ Edθ (w − wˆ)pθhθ + Edθ wˆpθ h − hθ

≤ Edθ |w − wˆ| pθ |hθ| + Edθ wˆ pθ |h − hθ|

≤ G w Edθ [h2θ] + G h Edθ [wˆ2]

where the last two inequalities follow from Cauchy-Schwarz, and the last inequality uses Assumption 6.2. We can upper bound the last term as

Edθ [wˆ2] ≤ Edθ [w2] + Edθ [(w − wˆ))2] ≤ σw2 +

2 w

which gives us

E[∇θV − ∇θV |θ]

Similarly, the variance is bounded as

≤ GCmax w + G h

σw2 +

2 w

E[ ∇θV − ∇θV 2|θ] ≤ 2Edθ (w − wˆ)pθhθ 2 + Edθ wˆpθ h − hθ 2

≤ 2G2Cm2 ax

2 w

+ 2G2

2h(σw2

+

2w )

Plugging the bias and variance into Theorem C.1 gives the result.

31

C.3 Experiment Details

Hyperparameters We use a separate neural network for the policy and critic, both optimized using the Adam optimizer. For solving the optimization problems (5) and (12), we use CVXPY [Diamond and Boyd, 2016, Agrawal et al., 2018]. For our experiments, we used the hyperparameters in Table 1.

Hyperparameters
γ learning rate (actor) learning rate (critic) batch size (actor) batch size (critic) number of iterations (actor) number of iterations (critic) number of trajectories

1.0 0.001 0.001 all 512 1 10 200

Table 1: Hyperparameters for CliﬀWalk experiments.

Learned State Distribution Correction. The learned state distribution correction w(s) for the α = 0.25 policy in the stochastic Cliﬀwalk from Figure 2 is shown in Figure 4 for each of the 4 × 12 Cliﬀwalk states. Initially, high state correction weights are given to states with high ξ weights (Figure 5). After training converges to the deterministic policy within 5K episodes, the agent takes only a single path along the topmost squares of the Cliﬀwalk. As a result, each of the states along this path has equal weight w(s) = 1/(length of path), and the interior states that the policy does not travel to have w(s) = 0.
Learned Lagrangian Variables. The ξs(s ) for the α = 0.25 policy in the stochastic Cliﬀwalk from Figure 2 is shown in Figure 5 for each of the 4 × 12 Cliﬀwalk states. For the start state (bottom left grid), initially ξs0(scliﬀ) corresponding to walking right and falling into the cliﬀ is high. After the policy learns a deterministic path around 3-4K episodes, the ξs(s ) corresponding to the deterministic transition s such that Pθ(s |s) = 1 for each s must also be 1 due to the constraint that EPθ [ξ] = 1. For s where Pθ(s |s) = 0, the ξs(s ) are free variables and do not aﬀect the value.
C.4 Function Approximation for w, ξ, λ
Finally, in Algorithm 3 we present a method for learning w, ξ, λ in the function approximation setting, such as large or continuous state spaces, where the use of convex solvers may become diﬃcult. In such cases, w and ξ, λ can be learned using neural networks for a total of ﬁve networks–the actor, critic, w, ξ, and λ. For CVaRα, for example, only λP needs to be learned since the constraint that ξ ∈ [0, α1 ] can naturally be implemented by using a Softmax activation in the ﬁnal layer of the ξ network, and multiplying the output by α1 . The λP network controls the constraint that EPθ [ξ] = 1.
The w network seeks to minimize L(w, f ) in Theorem 6.1, an algorithm for which is given in Algorithm 2 of Liu et al. [2018]. In practice, for discrete state spaces the delta kernel can be used
32

Figure 4: Learned state distribution correction w(s) over 10k training episodes from the α = 0.25 policy in the stochastic Cliﬀwalk in Figure 2. Each square corresponds to one state in the grid, e.g. the start state is on the bottom left.
Figure 5: Learned state distribution correction ξs(s ) over 10k training episodes from the α = 0.25 policy in the stochastic Cliﬀwalk in Figure 2. Each square corresponds to one state in the grid, e.g. the start state is on the bottom left. For each state s, the color of the line corresponds to next state s resulting from each of the 4 actions taken (see legend).
33

for f , and in continuous state spaces an RBF kernel with radius set to be the median of distances between states in the batch can be used for f [Liu et al., 2020]. The ξ, λ networks seek to maximize and minimize the MCR objective (4.1), respectively. The gradients of the MCR objective with respect to ξ and λ are given in Lemma C.2. We recommend that a higher learning rate is using for the λ network and a lower learning rate is used for the ξ network. The Lagrangian can be cast as a two-player zero sum game with a bilinear term between ξ and λ, and min-max gradient descent for such objectives has been proven to converge only with ﬁnite timescale separation [Fiez and Ratliﬀ, 2020]. We do not include experiments as it is beyond the scope of our paper, but hope that algorithms such as Algorithm 3 can serve as a basis for future work in scaling coherent risk policy gradient algorithms to larger or continuous state spaces.

Algorithm 3: Stationary State Distribution Reweighting Actor-Critic with Function Approximation

Input: Risk envelope U , learning rates {ηw, ηc, ηξ, ηλ, η}, batch sizes {Bw, Bc, Bl}, updates

{Nw, Nc, Nl}.

Initialize: Policy network θ, critic network θc, w network θw, ξ network φ, λ network ψ;

for episode k = 1...K do

Generate

N

trajectories

τk

=

{s

(n 0

)

,

a(0n)

,

c(0n

)

,

...}

N n=1

using

πθ ;

Pad τk if necessary;

for state ratio updates i = 1...Nw do

Sample a minibatch Bw ∼ τk;

Perform one update to θw according to Algorithm 4 with stepsize ηw;

end

for critic updates i = 1...Nc do Sample a minibatch Bc ∼ τk;
Calculate lθc = |B1c| (s,s )∼Bc

Vθc (s) −

C(s) + Lθ,s(ξφ, λψ)

2
with Lθ,s from (5);

θc ← θc + ηc ∂∂lθθcc ; end

for ξ, λ updates i = 1...Nl do
Sample a minibatch Bl ∼ τk; Calculate gradients ∂∂θVξ and ∂∂θVξ according to Lemma C.2; θξ ← θξ + ηξ ∂∂θVξ ; θλ ← θλ + ηλ ∂∂θVλ ; end

Compute h(s, a) in (6);

Compute ∇θVθ = s,a,s ∼τk wθw (s)∇θ log πθk (a|s)h(s, a); Update θ ← θ − η∇θVθ;

end

34

Algorithm 4: Single w Update (adapted from Algorithm 2 in Liu et al. [2018])
Input: w network parameters θw, learning rate ηw, batch of transitions Bw, weightings ξ, discount factor γ ∈ (0, 1], kernel k.
Compute L(w) = |B1w| i,j∈Bw ∆(w; si, si)∆(w; sj, sj)k(si, sj); where ∆(w, s, s ) = w(s)ξs(s ) − w(s ).;
Update the parameter by θw ← θw − ηw∇θL(wθw /zwθw ); where zwθw is a normalization constant zwθw = |B1w| i∈Bw wθw (si). ;

Lemma C.2 (Gradient of MCR w.r.t ξ, λ. ). Let the policy network be parameterized by θ, the ξ-network parameterized by φ, and the λ-network be parameterized by ψ. Then the gradient of the objective Vθ,φ,ψ with respect to φ is

∇φVθ,φ,ψ(s) = Eξφ

∞
γthφ(st)|s0 = s; πθ
t=0

(24)

where

hφ(s) = γ
s

Pθ(s |s)∇φξφ(s |s)

Vθ,φ,ψ(s )−λψ(p|s)−
e

λψ(e|s) dge(ξφ, Pθ) − dξ(s ) i

λψ(i|s) dfi(ξφ, Pθ) dξ(s ) (25)

The gradient with respect to ψ is

∇ψVθ,φ,ψ(s) = Eξφ

∞
γthψ(st)|s0 = s; πθ
t=0

(26)

where

hψ(s) = −γ ∇ψλψ(p|s)

Pθ(s |s)ξψ(s |s)−1 + ∇ψλψ(e|s)ge(ξφ, Pθ)+ ∇ψλψ(i|s)fi(ξφ, Pθ)

s1

e

i
(27)

Proof. The proof follows from applying the chain rule to the Lagrangian formulation of the objective (5) using the same techniques as the proof of Theorem 4.1 in Appendix A.

35

