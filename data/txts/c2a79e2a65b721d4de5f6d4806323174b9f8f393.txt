Towards Zero-Label Language Learning
Zirui Wang Adams Wei Yu Orhan Firat Yuan Cao Google AI
{ziruiw,adamsyuwei,orhanf,yuancao}@google.com

arXiv:2109.09193v1 [cs.CL] 19 Sep 2021

Abstract
This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of fewshot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages fewshot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-speciﬁc models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark1.
1 Introduction
It is well-known that deep learning models are datahungry. In natural language processing, language model pre-training has become a successful transfer learning approach to effectively reduce the requirement for task-speciﬁc labeled data (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Radford et al., 2019; Raffel et al., 2019; Brown et al., 2020). Via training on unsupervised large-scale text corpus, bi-directional language models such as BERT and XLNet are able to learn contextualized text representations that can then be ﬁne-tuned on downstream tasks with small training data sizes, which have pushed the state of the art on a variety of natural language understanding benchmarks.
More recently, gigantic language models (GLM) such as GPT3 (Brown et al., 2020) have been
1Notably, our method is also the ﬁrst to surpass human performance as of Dec 20, 2020.

Model Human Previous SOTA T5+UDG GPT3 UDG

Setting
Supervised Few-Shot Unsupervised

SuperGLUE Avg. 89.8 89.3 90.4 71.8 78.1

Table 1: SuperGLUE summary.

shown to be effective few-shot learners. As unsupervised training corpus and model size scaling up, the model is able to generate answers for an unseen NLP task with few-shot inference, based on a manually crafted input prompt consist of a task description and a few examples. Despite no ﬁne-tuning is involved, the language model performs competitively against ﬁne-tuned baselines on a wide range of tasks, whose success suggests a new paradigm of transfer learning in NLP. Yet the gaps between few-shot inference and state-of-theart ﬁne-tuned methods are still large on many tasks (for example 17.5 below prior state-of-the-art on SuperGLUE as shown in Table 1), urging for exploration of applications of giant language models beyond few-shot inference.
Inspired by the few-shot capability of GPT3, we shift our focus towards utilizing GLMs for example creation instead of direct inference, and ﬁnd that language models are also excellent few-shot generators. Similar to the few-shot inference paradigm, we query the model with a prompt with a few examples and a description of the desired label, and the model generates examples aligned with the label while resembling the given samples. Interestingly, we ﬁnd no supervision is required for high-quality data creation and thus we only need to use unlabeled examples in our prompts. The dataset created by the model can then used to ﬁne-tune any off-the-shelf model. This approach can therefore be treated as a zero-label learning procedure, in which no human label is required throughout the whole process. It differs from the unsupervised

Figure 1: Illustration of the UDG framework.

learning procedure in that the downstream models still need to be trained with synthetic data, however the training example creation requires no human labor.
Following this procedure, we are able to establish a system trained using unlabeled training data only, and thus we refer to it as Unsupervised Data Generation (UDG). Experiments show that our unsupervised system performs competitively with strong supervised baselines and achieves new stateof-the-art few-shot learning results on text classiﬁcation and the SuperGLUE language understanding benchmarks. The synthesized data can further be used for data augmentation purpose. When combined with existing labeled data we are able to achieve the ﬁrst super-human SuperGLUE scores. These results suggest that few-shot training data creation is a promising alternative to few-shot inference with powerful language models.
2 Related Work
Data augmentation has traditionally been a popular technique for NLP model quality improvement, especially in low-resource regimes (Yu et al., 2018; Wei and Zou, 2019) While traditionally simple heuristics like token-level modiﬁcation has been applied to diversify training samples, more recently generative data augmentation has gained popularity due to the progress made in language modeling (Anaby-Tavor et al., 2019; Papanikolaou and Pierleoni, 2020; Juuti et al., 2020; Lee et al., 2021; Kumar et al., 2021). However, they often require labeled examples to ﬁnetune generative models and heavy postprocessing for data cleaning. On the other hand, our method generates data in a fully unsupervised manner without ﬁnetuning the lan-

guage model, showcasing a new zero-label learning paradigm.
Our approach is also closely related to knowledge retrieval from large language models. These models are known to be good at memorizing facts from training data and capable of performing as open knowledge bases (Petroni et al., 2019; Wang et al., 2020; Roberts et al., 2020; Carlini et al., 2021). The high quality of training examples created by our approach is to a large part guaranteed by the model’s strong knowledge retrieval ability, which reduces the chance of erratic hallucinations irrelevant to the provided labels.
3 Method
3.1 Background: Few-shot Inference
Given a set of labeled data L = {(xi, yi)}ni=1 for a speciﬁc downstream task, the most common approach in recent years has been ﬁne-tuning that updates the weights of a pre-trained model according to L (Devlin et al., 2018; Yang et al., 2019; Raffel et al., 2019). While obtaining state-of-theart performance on a wide range of tasks, ﬁnetuning requires extra update steps and non-trivial amounts of labeled data in the target task. On the other hand, few-shot inference is a more resourceefﬁcient paradigm exhibited in the latest gigantic language models such as GPT3 (Radford et al., 2019; Brown et al., 2020). The idea is to utilize the language model to infer the correct label based on the task description and a few sample inputlabel pairs. In particular, the input to the model M is a handcrafted ordered prompt consisted of a task description T , a small set of K examples Lfew = {(xi, yi)}Ki=1 ⊆ L, and the query example

XLNet BERTLARGE UDA Few-shot Inf. UDG + NLA

Supervised Few-Shot Unsupervised

IMDb
96.80 95.49 95.80 90.38 95.95 96.29

Yelp-2
98.63 98.11 97.95 88.79 98.22 98.38

Yelp-5
72.95 70.68 67.92 48.75 69.05 69.31

Amazon-2
97.89 97.37 96.50 92.63 97.02 97.24

Amazon-5
68.33 65.83 62.88 44.21 64.54 64.88

DBpedia
99.40 99.36 98.91 82.46 96.47 99.21

Avg.
89.00 87.81 86.66 74.54 86.88 87.55

Table 2: Comparison of methods on text classiﬁcation datasets (Accuracy). Results for XLNet are obtained from (Yang et al., 2019) while results for BERTLARGE and UDA are from (Xie et al., 2019). The best result for semi-supervised/few-shot setup is bolded while underline signiﬁes the overall best.

xq, and the model is expected to infer the correct label yq as the most probable next text sequence to the input prompt:
yq = argmax PM (y|[T, Lfew, xq]). (1)
y
Since taking the argmax is intractable, yq is usually obtained through greedy decoding or beam search. Using much less task-speciﬁc data and no gradient update, few-shot inference can obtain performance comparable to ﬁne-tuning methods (e.g. GPT3 performs similarly to ﬁne-tuned BERT on SuperGLUE in Table 4). In its extreme format, giant language models can also perform one-shot (K=1) or even zero-shot (K=0) inference.
3.2 Unsupervised Data Generation
Despite these interesting ﬁndings, few-shot inference using giant language models still underperforms state-of-the-art ﬁne-tuned models on many tasks. In Table 4, for instance, T5 largely outperforms GPT3 (89.3 vs 71.8) despite being much smaller in model sizes (11B vs 175B). One potential limitation is that a language model is never explicitly trained to directly conduct inference. Instead, it is trained as a text generator on unsupervised web corpus where inputs (X) and labels (Y ) happen to coexist. Consequently, the few-shot inference method ﬁnds the proper prompt that ‘forces’ the model to generate next text sequence Xnext which happens to be the label Y. However, this could be suboptimal since the labels often emerge prior to the inputs in real-world web documents. For example, in sentiment classiﬁcation of IMDb movie reviews (Maas et al., 2011), the actual review contexts appear after their corresponding rating scores. Therefore, few-shot inference can force the language model to generate on text distributions that are inconsistent with its training data.

To this end, we propose to utilize language models to perform few-shot generation. Instead of generating and predicting the label Y, we let the model to generate the input X instead, decoupling generation from prediction. We aim to formulate the input prompts that are more likely to naturally exist in the training corpus. Speciﬁcally, the model is queried to generate xg corresponding to a pseudo label yˆg with a prompt consisted of a small set of K unlabeled examples U = {xi}Ki=1 and a description of the desired label:

xg ∼ PM (x|[T, U , Des(yˆg)]),

(2)

where Des(·) is a task-speciﬁc transformation func-

tion that maps a label class to natural language de-

scriptions, as illustrated in Figure 1. Different from

few-shot inference, our method only requires unsu-

pervised few-shot examples, a zero-label learning

setting. In addition, we use top-k sampling instead

of search-based decoding to sample text from the

language model. This allows us to generate a syn-

thetic

labeled

dataset

Lsyn

=

{

(x

i g

,

yˆgi

)}

ns i=1

with

controllable size ns. We then train task-speciﬁc

models utilizing this synthetic dataset, either as

standalone training data or additional auxiliary data.

Unlike existing synthetic data generation systems,

our method requires no ﬁne-tuning step of the gen-

erative model and uses unsupervised data only, and

therefore we refer to it as Unsupervised Data Gen-

eration to emphasize its resource efﬁciency. We

also hope to emphasize that it is not our intention to

leverage the language model to perform generative

tasks, but just to take advantage of it to synthesize

“labeled” examples for downstream model training.

4 Experiments
4.1 Unsupervised Text Classiﬁcation We ﬁrst apply the proposed UDG method on standard text classiﬁcation tasks.

IMDb Acc. Yelp-2 Acc. Amz-5 Acc.

K=0 64.21 67.34 47.35

K=1 91.34 90.27 58.79

K=4 95.86 98.22 62.14

K=32 96.29 98.38 64.88

Table 3: Ablation of number of examples in each prompt.

Experimental Setups. We use six popular text

classiﬁcation benchmark datasets (Maas et al.,

2011; Zhang et al., 2015), including IMDb, Yelp-2,

Yelp-5, Amazon-2 and Amazon-5 sentiment clas-

siﬁcation and DBPedia topic classiﬁcation. We

mainly follow the experimental settings in Xie et al.

(2019) and use the corresponding unlabeled data

for each task. We apply similar preprocessing steps

to clean noisy web texts and truncate the input to

512 subword tokens. For each prompt, we sample

K = 32 unlabeled examples from the unlabeled

data and ﬁt as many examples as allowed by the

length of the language model’s context window (de-

tailed templates shown in Figure 1 and Appendix

C).

This

process

is

then

repeated

nc

=

ns # Class

times

for each label class, where we set nc = 10K for

sentiment classiﬁcation tasks and 1000 for topic

classiﬁcation. We then utilize the language model

to generate one example for each prompt, resulting

in a synthetic labeled dataset of size ns. We use an in-house language model, which is a variant of

the one in (Adiwardana et al., 2020) but trained

with larger data. We exploit top-k sampling with

K=40 and temperature=1.0, and only apply basic

post-processing to ﬁlter generated examples that

are too short/long.

Once we obtain the generated synthetic dataset

Lsyn, it can be utilized as labeled training data for any task-speciﬁc training framework. Here, we

choose the state-of-the-art semi-supervised learn-

ing framework Unsupervised Data Augmentation

(UDA) (Xie et al., 2019) as the backbone. We use

BERTLarge as our base model and follow the training protocol as described in the UDA paper to tune

our hyper-parameters. In our experiment, we ﬁnd

some generated examples are noisy adn thus we

additionally implement a Noisy Label Annealing

(NLA) technique to ﬁlter these examples during the

training process (See Appendix A for details).

Results. We compare models of trained using fully

supervised, semi-supervised/few-shot and unsuper-

vised settings in Table 2. We ﬁrst compare few-shot

inference using our giant language model with ﬁne-

Figure 2: Ablation of number of examples generated per label class.
tuned methods. Despite requiring no additional training costs, the few-shot inference paradigm performs signiﬁcantly worse than supervised or even semi-supervised UDA, which utilizes similar amounts of labeled data. The gap is more evident on multi-way classiﬁcation tasks such as Yelp-5 or DBpedia, where the model is required to predict complex labels beyond simple answers such as ‘True/False’. In contrast, the proposed few-shot generation paradigm obtains strong performance while using less supervision. When combined with NLA, our UDG framework consistently outperforms UDA and few-shot inference on all six tasks, achieving new state-of-the-art few-shot learning results. Besides, without using any label, our method outperforms fully supervised BERTLARGE on IMDb and Yelp-2 and is also competitive on other tasks. Since both UDA and our method rely on BERTLARGE, we expect using XLNet may further boost our unsupervised performance, which we choose to leave for future work.
Analysis. We ﬁrst examine the effect of data noisiness on model performance. As is the case for other data augmentation methods, few-shot generation using giant language models can produce examples that are inaccurate to the desired labels. To reduce the negative impact of these noisy labels, we utilize a simple NLA technique to ﬁlter out examples when the task-speciﬁc models disagree with the synthetic labels with high conﬁdence levels. As shown in Table 2, NLA robustly improves UDG performance on all tasks, especially ones that are sensitive to noise such as DBpedia.
A crucial difference distinguishing our work from existing data generation methods is that we directly query the pretrained language model without any ﬁne-tuning nor supervision. To achieve this, the model needs to not only infer correct knowledge corresponding to the input pseudo label but also generate text with similar styles of the sam-

BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Avg.

Human

89.0 95.8/98.9 100.0 81.8/51.9 91.7/91.3 93.6 80.0 100.0 89.8

BERT++a

RoBERTab

T5c

Sup.

DeBERTad

79.0 84.8/90.4 73.8 70.0/24.1 72.0/71.3 71.7 69.6 64.4 71.5 87.1 90.5/95.2 90.6 84.4/52.5 90.6/90.0 88.2 69.9 89.0 84.6 91.2 93.9/96.8 94.8 88.1/63.3 94.1/93.4 92.5 76.9 93.8 89.3 90.4 94.9/97.2 96.8 88.2/63.7 94.5/94.1 93.2 76.4 95.9 89.9

T5 + UDG

91.4 95.8/97.6 98.0 88.3/63.0 94.2/93.5 93.0 77.9 96.6 90.4

GPT3e

76.4 52.0/75.6 92.0 75.4/30.5 91.1/90.2 69.0 49.4 80.1 71.8

iPETf

Few-Shot 81.2 79.9/88.8 90.8 74.1/31.7 85.9/85.4 70.8 49.3 88.4 75.4

ADAPETg

80.0 82.3/92.0 85.4 76.2/35.7 86.1/85.5 75.0 53.5 85.6 76.0

UDG

Unsup. 81.0 86.2/92.4 80.4 81.1/47.1 82.8/81.8 80.7 67.5 79.5 78.1

Table 4: Comparison of single-model methods on SuperGLUE test scores. Results obtained from the ofﬁcial SuperGLUE leaderboard2. The best result for semi-supervised/few-shot setup is underlined while bold signiﬁes the overall best. Model references: aDevlin et al. (2018) bLiu et al. (2019) cRaffel et al. (2019) dDevlin et al. (2018) eBrown et al. (2020) fSchick and Schütze (2020) gTam et al. (2021)

ple unsupervised examples. Thus, we compare the results when the language model uses different amounts of in-context examples in Table 3. The model fails to generate high-quality data when no sample is given, indicating the importance of fewshot generation. On the other hand, including more unsupervised examples does improve the quality of synthetic dataset which leads to better performance.
Finally, we evaluate the impact of the synthetic data sizes in Figure 2. Despite there is a diminishing return trend, we ﬁnd the ﬁnal performance to continuously improve with more generated data, showing that the language model can generate diverse examples. In addition, one key beneﬁt of our method is that we can sample as much data as needed with no additional cost or supervision. This is particularly useful for tasks from low-resource domains with limited unsupervised data available.
4.2 Unsupervised Language Understanding
To evaluate the proposed framework in a more challenging and comprehensive setting, we extend it to perform on complex language understanding tasks. Experimental Setups. We use the SuperGLUE benchmark (Wang et al., 2019) for general-purpose language understanding in English, which consists of 8 natural language understanding tasks. Tasks cover textual entailment (CB and RTE), question answering (BoolQ, MultiRC and ReCoRD), common sense reasoning (COPA), word sense disambiguation (WiC), and coreference resolution (WSC). We mainly follow the same generation protocol as described in the previous sections, with some minor changes in prompt templates and data

post-processing steps for speciﬁc tasks. As before, we use K=32 unlabeled examples and generate using the same language model. For each task, we use all original labeled data as unsupervised examples for training data creation.
For the downstream model, we use T5 (Raffel et al., 2019) for ﬁne-tuning on the created data. Different from the released T5 checkpoints that are pretrained on multi-task data, we pretrain our own models on unsupervised Colossal Clean Crawled Corpus (C4) data only and thus the combined framework remains unsupervised. For fair comparison with existing models, we pretrain and then ﬁne-tune a T5-Large model using the created data set. Following Raffel et al. (2019), we use a ﬁnetuning batch size of 8 with 512 sequence length. Results. We compare models trained under different settings in Table 4. The GPT3 model (Brown et al., 2020) using the few-shot inference method outperform BERT++ with less supervision and no ﬁne-tuning. However, despite containing much more model parameters, it performs worse than other ﬁne-tuned fully supervised models and fewshot methods. On the other hand, our unsupervised framework using few-shot generation outperforms all few-shot learning systems without using any label, and thus it achieves new state-of-the-art results on this benchmark for methods that exploit littleto-no supervision. In particular, our performance gains largely come from natural language entailment tasks (CB and RTE) as well as word sense disambiguation, where GPT3 performs similarly to random guessing. This indicates that language
2https://super.gluebenchmark.com/ leaderboard

models do contain language knowledge that fewshot inference fails to leverage.
4.3 UDG as Data Augmentation
In previous sections we only use the created examples as pseudo supervision to explore the limits of transfer learning using language models. Nonetheless, the synthetic data can be also treated as augmented data and combined with existing labeled data. To this end, we ﬁne-tune the public T5-XXL checkpoint using both labeled data and generated data. As shown in Table 4, our method combines well with existing labeled data and brings substantial improvements. This is particularly the case for tasks with small data sizes such as COPA and WSC. Moreover, the combined model outperforms not only prior methods but also the human baselines for the ﬁrst time on this important NLP benchmark, setting a new milestone for natural language understanding with machine learning models.
5 Conclusion
In this paper, we propose a “zero-label” training procedure and show that language models are also few-shot example creators in that they can be used to generate high-quality synthetic data in a fully unsupervised manner. Through this, we demonstrate that NLP models can obtain strong results without any human annotated label. Our work illustrate a promising direction for future transfer learning research in NLP.
References
Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2019. Not enough data? deep learning to the rescue!
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Mika Juuti, Tommi Gröndahl, Adrian Flanagan, and N. Asokan. 2020. A little goes a long way: Improving toxic language classiﬁcation despite data scarcity. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2991– 3009, Online. Association for Computational Linguistics.
Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2021. Data augmentation using pre-trained transformer models.
Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150.
Yannis Papanikolaou and Andrea Pierleoni. 2020. Dare: Data augmented relation extraction with gpt2.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2019. Language models as knowledge bases?
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics.
Timo Schick and Hinrich Schütze. 2020. It’s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.

Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537.
Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs.
Jason W. Wei and Kai Zou. 2019. EDA: easy data augmentation techniques for boosting performance on text classiﬁcation tasks. In EMNLP-IJCNLP.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. In ICLR.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. arXiv preprint arXiv:1509.01626.

None 0.9→0.8 0.9→0.7 0.9→0.6 0.9→0.5 95.95 96.03 96.08 96.17 96.29

Table 5: Comparison of different annealing thresholds on IMBd classiﬁcation. We observe performance improves as we ﬁlter more aggresively.

A Noisy Label Annealing

Noisiness is a common issue for synthetic data gen-

eration. To mitigate this issue, prior work [CITE]

utilize extensive ﬁltering methods to select clean

generated examples. While one key beneﬁt of our

method being high-quality synthetic data with mini-

mal ﬁltering, we do ﬁnd some regularization during

ﬁnetuning to be helpful for better performance, es-

pecially on tasks sensitive to noises. In particular,

we obverse that the generated examples of the lan-

guage model may be misaligned with the desired

label class. Thus, we introduce a new training tech-

nique called Noisy Label Annealing (NLA), which

gradually ﬁlter out noisy training signals as training

progresses. Intuitively, we remove a speciﬁc train-

ing example if our model disagrees with its label

with high conﬁdence. Mathematically, at training step t, a given example (xig, yˆgi ) is considered noisy

and removed, if (1) the model’s predicted proba-

bility P (y|xig) is higher than a threshold µt, and (2) the prediction yi = argmaxy P (y|xig) differs from the synthetic label yi = yˆgi . We set the ini-

tial threshold µ0 to 0.9 and gradually anneal it to

1 K

where

K

is

the

number

of

classes.

Intuitively,

the model is less accurate at the early stage of the

ﬁnetuning process and thus we demand a very high

conﬁdence level to ﬁlter noises, whereas we can

safely decrease the “bar” as the model gets better

trained. We explore different ﬁnal annealing val-

ues in Table 5 and ﬁnd a more aggressive strategy

works often better.

B Finetuning Details

For text classiﬁcations, we mainly follow the experimental setups in (Xie et al., 2019). We truncate the input to 512 subwords using BERT’s vocabulary, keeping the last tokens. For the ﬁnetuning process, we search the learning rate in {1e-5, 2e-5, 5e-5} and batch size in {32, 64, 128}. We also tune the number of epochs based on the size of generated data, ranging from 5 to 30. As with (Xie et al., 2019), we also ﬁne-tune the BERT model on in-domain unsupervised data prior to the ﬁnal training stage. For UDA hyperparameters, we tune

the batch size and weight for both unsupervised and generated data, as well as different strategies of Training Signal Annealing (TSA). Notice that TSA is orthogonal to our NLA technique and thus we can apply them at the same time. Experiments are conducted on 32 v3 TPUs.
For tasks in SuperGLUE, we follow the pretraining and ﬁnetuning setups in (Raffel et al., 2019). For the unsupervised setting, we pretrain our own T5-Large model using the exact same settings and hyperparameters as the original paper, except that we do not utilize the original downstream tasks data for multi-task pretraining. For all ﬁnetuning, we use a batch size of 8 and a sequence leangth of 512, and utilize the dev scores to pick the best checkpoint for submission to the test server for ﬁnal evaluation. For more details, we refer readers to the T5 paper (Raffel et al., 2019).

C Prompts & Generated Examples
C.1 Text Classiﬁcation C.1.1 IMDb Input Prompt: Sample Movie Review: This is the most saccharine romance I ever sat through. The perfect film for an idle housewife in kerchief, housedress, and ostrich-trimmed high-heeled mules to watch in the afternoon, lying on the couch eating bonbons. In fact, bonbons play a prominent role in the movie. The only reason I was able to watch to the end, is that I finally was able to gaze at Keanu Reeves’ dreamy face in almost every scene. In most of his films, he moves too fast to get a good look. The only rapid action in this show is Giancarlo Giannini waving his hands with Latin emotionality - more Italian than Mexican, really.
Negative Movie Review:
Generated Output: This remake of the 1966 French film by François Truffaut and Claude Chabrol is just plain awful. The only positive I can say about the film is that the casting was just about perfect. Otherwise, the movie has no redeeming qualities whatsoever. The plot is a mess and is filled with numerous inconsistencies and continuity flaws throughout. It is as if the director just made it up on the spot and made it up bad!
C.1.2 Yelp-2 Input Prompt: Sample Business Review: I had the most delicious burger ever! I also ordered the truffle fries, which were amazing! The sauce they served with the fries was also very good! Definitely come here!! There was great service and wonderful food!
Positive Business Review:
Generated Output: We had our wedding at The Grand in October of 2011. The Grand and their staff are absolutely wonderful to work with. We had over 100 people at the wedding and the room was packed for the reception. We had a great time and the service, food, and atmosphere were fantastic. I can not say enough good things about working with and having our wedding at The Grand. We highly recommend The Grand for your wedding or special event.
C.1.3 Yelp-5 Input Prompt: Yelp Review Content: I waited FOR 35 MINUTES tonight just for 2 brisket meals to go. They weren’t even hot when I arrived home and I live less than 1 minute away. It was still a fine meal, saved by the amazing cornbread!! But, I have no intention of going back any time soon.

Yelp Review Rating: 3/5 Content:
Generated Output: I came here to eat after the ridiculous long lines died down a bit. I must say, I don’t get the hype about this place and felt a bit disappointed. I suggested coming here with a group of friends. I was pretty excited to eat here and found that nothing impressed me and the price tag could not be justified. Nothing stood out for me that would make me want to come here again. I think the highlight of my meal was the ginseng chicken soup. All other items, I’ve had better at other buffets. I didn’t think their selection was that impressive.
C.1.4 Amazon-2 Input Prompt: Sample Product Review Title: Nice to have Content: My dog loves this bed. I don’t like to have my dog sleep on the floor. I know I spoiled my dog. I put a huge pillow on top of this bed to give her the extra comfort. My dog loves sleeping on something soft. Now I have a happy dog that sleeps comfortably every night. Money well spend.
Negative Product Review Title:
Generated Output: Not worth it Content: I am so very disappointed. I bought this for my granddaughter for Christmas. I have a few concerns, but first and foremost, the box that the doll comes in says that it must be assembled by an adult. The instructions are very confusing. My mom and I put it together for her Christmas Eve. Then we realized that the doll has a small hole in the back of the head that is on the end of the seam. I don’t know where the hole came from, but since she is a collectible, I can’t return it.
C.1.5 Amazon-5 Input Prompt: Product Review Title: Good Book Content: Good book, easy to read, with a lot of suggestions about where read the exposure. Also if the book is more than 150 pages, there are a lot of big photos, everyone with a comment; instead of this I would have prefereed more photos with comment also if they would have been smaller. So, I read this book in too little time. My rate for this book is 3.5.
Product Review

Rating: 3/5 Title:
Generated Output: An ok game Content: This game is good, but I wouldn’t buy this game unless you are a star wars fan. The game has lots of levels and it takes quite a while to beat. If you don’t know anything about star wars, you would be lost on the story. There are lots of good things and bad things about this game.
C.1.6 DBpedia Input Prompt: Wikipedia Article Title: University of Georgia School of Law Article: The University of Georgia School of Law (also referred to as Georgia Law) is a graduate school of the University of Georgia.
Wikipedia Article Category: Educational Institution Title:
Generated Output: Vanung University Article: Vanung University (VNU) is a private university in Zhongli City Taoyuan County Taiwan. It is the first private university in Taiwan to be accredited by AACSB.
C.2 SuperGLUE C.2.1 BoolQ Input Prompt: Article: Russian Blue -- Russian Blues are plush short-haired, shimmering pale blue-gray cats with emerald green eyes. Guard hairs are distinctly silver-tipped giving the cat a silvery sheen or lustrous appearance. They have been used on a limited basis to create other breeds such as the Havana Brown or alter existing breeds such as the Nebelung. They are being used in Italy as a way to make Oriental Shorthairs healthier and more robust called RUS4OSH in FIFe. Sample statement based on the article: do all russian blue cats have green eyes.
Article: Okinawa Churaumi Aquarium -- The Okinawa Churaumi Aquarium (Okinawa Churaumi Suizokukan) is located within the Ocean Expo Park in Okinawa, Japan. It welcomed its 20 millionth visitor on 30 March 2010 and is a member of the Japanese Association of Zoos and Aquariums (JAZA). It was the largest aquarium in the world until it was surpassed by the Georgia Aquarium in 2005. The aquarium has the theme of ‘‘Encounter the Okinawan Sea’’. False statement based on the article:
Generated Output:

is okinawa churaumi aquarium the largest aquarium in the world.
C.2.2 CB Input Prompt: Input: Jim waited. He waited a long time, and when the young doctor finally came out, it was almost dark. Jim could nonetheless tell by his anxious face that something was wrong. Output: Something was wrong.
Input: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara’s eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? Output: Contradicting this,
Generated Output: Tara was only a temporary housekeeper.
C.2.3 COPA Input Prompt: Input: My body cast a shadow over the grass. Output: The sun was rising.
Input: My computer screen went blank. Connection: Because Output:
Generated Output: The power went out.
C.2.4 MultiRC Input Prompt: Article: On a day it celebrated 50 years of giving free legal advice, Southern Arizona Legal Aid announced it has received two grants totaling more than 7 million ... Question: Who will the Web site benefit? Answer: Help bridge the "digital divide" that keeps the poor from using the Internet as a resource
Article: While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful ... Question:
Generated Output: Which country did President Bill Clinton visit on March 25, 2000? Correct Answer: Pakistan
C.2.5 ReCoRD Input Prompt: Article: The harrowing stories of women and children locked up for so-called ’moral crimes’ in Afghanistan’s notorious female prison have been revealed after cameras were allowed inside. Mariam has been in Badam Bagh prison for three months after she shot a man who just

raped her at gunpoint and then turned the weapon on herself - but she has yet to been charged. Nuria has eight months left to serve of her sentence for trying to divorce her husband. She gave birth in prison to her son and they share a cell together. Scroll down for video Nuria was jailed for trying to divorce her husband. Her son is one of 62 children living at Badam Bagh prison. Keyword: Badam Bagh prison ### Relevant Fact: The women and children are jailed in Badam Bagh prison.
Article: China’s space agency today distributed fresh new pictures of the Moon’s surface from its lunar lander and rover. The Jade Rabbit rover can be seen trundling across the pockmarked surface of Earth’s satellite in pictures released today by the Chinese Academy of Sciences. The successful soft-landing of the Chang’e 3 lander in December made China the third country to complete a lunar rover mission after the United States and Russia. Chasing the horizon: The sky over the Moon appears inky black in this picture showing the Jade Rabbit rover trundling across the lunar surface taken from the Chang’e 3 lander and released today by Chinese scientists. Keyword: China ### Relevant Fact:
Generated Output:
Knock off or not, there’s no denying the ambition of the China’s Moon mission, which on December 14 achieved the first lunar soft landing in nearly four decades and aims at a manned mission by 2025.
C.2.6 RTE Input Prompt:
Input: No Weapons of Mass Destruction Found in Iraq Yet. Output: Weapons of Mass Destruction Found in Iraq.
Input: CBS newsman Harry Reasoner is returning to his Iowa hometown to get married Saturday. Output: In other words,
Generated Output:
Harry Reasoner is getting married.
C.2.7 WiC Input Prompt:
Word: meet Example: The company agrees to meet the cost of any repairs.
Word: skin Definition 1:
Generated Output:
the outer covering of the body of a vertebrate animal. Example: The skin of a snake.

C.2.8 WSC Input Prompt: Noun: Sam ### Pronoun: he ### Sample Sentence: Sam took French classes from Adam , because he was eager to speak it fluently.
Noun: Ann ### Pronoun: She ### Sentence where She refers to Ann:
Generated Output: It seems that Ann has had a run of bad luck. She never wins at cards.

