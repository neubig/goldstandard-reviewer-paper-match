RECENT DEVELOPMENTS ON ESPNET TOOLKIT BOOSTED BY CONFORMER
Pengcheng Guo1,4, Florian Boyer2,3, Xuankai Chang4, Tomoki Hayashi5, Yosuke Higuchi6 Hirofumi Inaguma7, Naoyuki Kamo8, Chenda Li9, Daniel Garcia-Romero4, Jiatong Shi4
Jing Shi4,10, Shinji Watanabe4, Kun Wei1, Wangyou Zhang9, Yuekai Zhang4
1Northwestern Polytechnical University, 2LaBRI, University of Bordeaux, 3 Airudit 4Johns Hopkins University, 5Human Dataware Lab. Co., Ltd.
6Waseda University, 7Kyoto University, 8NTT Corporation 9Shanghai Jiao Tong University 10Institute of Automation, Chinese Academy of Sciences

arXiv:2010.13956v2 [eess.AS] 29 Oct 2020

ABSTRACT
In this study, we present recent developments on ESPnet: End-toEnd Speech Processing toolkit, which mainly involves a recently proposed architecture called Conformer, Convolution-augmented Transformer. This paper shows the results for a wide range of endto-end speech processing applications, such as automatic speech recognition (ASR), speech translations (ST), speech separation (SS) and text-to-speech (TTS). Our experiments reveal various training tips and signiﬁcant performance beneﬁts obtained with the Conformer on different tasks. These results are competitive or even outperform the current state-of-art Transformer models. We are preparing to release all-in-one recipes using open source and publicly available corpora for all the above tasks with pre-trained models. Our aim for this work is to contribute to our research community by reducing the burden of preparing state-of-the-art research environments usually requiring high resources.
Index Terms— Conformer, Transformer, End-to-End Speech Processing
1. INTRODUCTION
Transformer architecture has drawn immense interest recently and became the dominated model due to its effectiveness across various sequence-to-sequence tasks, like machine translation, language modeling (LM), and automatic speech recognition (ASR) [1–6]. One reason for the success of Transformer model is that the multihead self-attention layers can learn long-range global context better than the recurrent neural networks (RNNs). However, for speech processing tasks, not only the global context, but also the local information is crucial to capture some particular properties of speech, like coarticulation and monotonicity. Convolution neural networks (CNNs), on the other hand, are good at extracting ﬁne-grained local feature patterns. Recently, Gulati et al. [7] proposed a novel architecture with combination of self-attention and convolution in ASR models, which is named Conformer. With this proposed design, the selfattention layer learns the global context while the convolution module efﬁciently captures the local correlations synchronously.
In addition to the ASR task, other speech processing tasks can also have such beneﬁts and obtain improvement when giving local information. In this study, we aim to explore the efﬁciency of Conformer on various end-to-end speech processing applications, including automatic speech recognition (ASR), speech translation (ST), speech separation (SS), and text-to-speech (TTS). We provide intensive comparisons of Conformer with Transformer on lots

of publicly available corpora and try our best to share the practical guides (e.g., learning rate, hyper-parameters, network structure) on the use of Conformer. We also prepare to release the reproducible recipes and state-of-the-art setups to the community to succeed our exciting outcomes.
The contributions of this study include:
• We extend the Conformer architecture to various end-to-end speech processing applications and conduct comparative experiments with Transformer.
• We share our practical guides for the training of Conformer, like learning rate, kernel size of Conformer block, and model architectures, etc.
• We provide reproducible benchmark results, recipes, setups and well-trained models on a large number of publicly available corpora1 in our open source toolkit ESPnet [8–10].

2. CONFORMER
Our Conformer model consists of a Conformer encoder proposed in [7] and a Transformer decoder. The encoder is a multi-blocked architecture and each block is stacked by a positionwise feed-forward (FFN) module, a multihead self-attention (MHSA) module, a convolution (CONV) module, and another FFN module in the end. We apply layer normalization (LN) before each module and dropout followed by a residual connection afterward (pre-norm), as in [5, 11]. This section describes the details of each module in the encoder.

2.1. Multihead self-attention module

The idea of MHSA module is to learn an alignment in which each token in the sequence learns to gather from other tokens [12, 13]. For each single head h, the output of attention computation can be formulated as:

QhKTh

Att(Qh, Kh, Vh) = Softmax √

Vh,

(1)

datt

where Qh = Whq X, Kh = WhkX, and Vh = Vhq X are query, key, and value linear transformations applied on the input sequence X ∈ RT ×datt . Whq , Whk, Whv ∈ Rdatt× dHatt are the projecting weight

1Due to the page limitation, we are not able to cite all references. Instead, corresponding links are embed in the corpora names.

matrices, datt is the dimension of attention, and H refers to the total number of attention heads. The term √1datt is used to scale the dot product result to avoid a very large magnitude caused by the di-
mension of attention. In order to jointly attend to information from
different representation subspaces, the outputs of each head are con-
catenated together and fed into a fully-connected layer, as follows:

MHSA(Q, K, V) = Concat(head1, ..., headH )Wo, (2)

headh = Att(Qh, Kh, Vh),

(3)

where Wo ∈ Rdatt×datt is an output linear projecting matrix. Besides, Conformer also integrates a position encoding scheme
from TransformerXL [3] to generate better position information for the input sequence with various lengths, named relative positional encodings. For an input sequence X, the computational procedure can be summarized as:

X = X + Dropout(MHSA(LN(X))).

(4)

2.2. Convolution module
Figure 1 illustrates the details of CONV module. The CONV module starts with a 1-D pointwise convolution layer and a gated linear units (GLU) activation [14]. The 1-D pointwise convolution layer doubles the input channels, while the GLU activation splits the input along the channel dimension and conducts an element-wise product. After that, it is followed by a 1-D depthwise convolution layer, a batch normalization (BN) layer, a Swish activation, and another 1-D pointwise convolution layer.

Input

1D Depthwise

BN

Conv

GLU

Swish

1D Pointwise Conv

1D Pointwise Conv

LN

Dropout

+

Ouput

Fig. 1. Details of the CONV module. All convolution operations are performed on the time domain.

2.3. Pointwise feed-forward module
The FFN module in original Transformer is composed of two linear transformations with a ReLU activation in between, as follows:

FFN(X) = W2ReLU(W1X + b1) + b2,

(5)

where W1 ∈ Rdatt×dff , W2 ∈ Rdff×datt are linear projecting matrix and datt denotes the hidden dimension of linear layer.
Different from Transformer, Conformer introduces another FFN module and replaces the ReLU activation with the Swish activation. Besides, inspired by Macaron-Net [15], the two FFN modules

Input

FFN1

MHSA

CONV

FFN2

Ouput

Fig. 2. An overview of Conformer block.
are following a half-step scheme and sandwiching the MHSA and CONV modules. Mathematically, for input X, the output is:
X = X + 1 × Dropout(FFN(LN(X))). (6) 2
2.4. Conformer block
Figure 2 shows how to combine each module together. The difference between the Conformer block and Transformer block include: the relative positional encoding, the integrated CONV module, and a pair of FNN modules in the Macaron-Net style.
3. SPEECH APPLICATIONS
In ASR tasks, the Conformer model predicts a target sequence Y of characters or byte-pair-encoding (BPE) tokens2 from an input sequence X of 80 dimensional log-mel ﬁlterbank features with/without 3-dimensional pitch features. X is ﬁrst sub-sampled in a convolutional layer by a factor of 4, as in [4], and then fed into the encoder and decoder to compute the cross-entropy (CE) loss. The encoder output is also used to compute a connectionist temporal classiﬁcation (CTC) loss [17] for joint CTC-attention training and decoding [18]. During inference, token-level or word-level language model (LM) [19] is combined via shallow fusion.
ST tasks adopt the same framework deﬁned in ASR. It directly maps speech from a source language to the corresponding translation in the target language. In order to eliminate the serious under-ﬁtting problem, we initialize the ST encoder by a pre-trained ASR encoder and start the ST decoder from a pre-trained machine translation (MT) decoder, as in [9].
For the SS tasks, the Conformer model is optimized to estimate the time-frequency mask for each individual speaker given a speech mixture. The model is trained with utterance-level permutation invariant loss (uPIT) [20]. Different from the ASR system, the Conformer model here only contains the encoder, followed by an additional linear layer and activation function to predict the masks.
TTS tasks use the Conformer encoder for non-autoregressive TTS models [21–23], which generates a sequence of log-mel ﬁlterbank features from a phoneme or character sequence in cooperation with the duration predictor [21]. The whole model is optimized to minimize the L1 loss for the target features and the mean square error (MSE) loss for the durations.
4. SPEECH RECOGNITION EXPERIMENTS
4.1. Setups
To evaluate the effectiveness of our Conformer model, we conduct experiments on a total of 25 ASR corpora, including various recording environments (clean, noisy, far-ﬁeld, mixed speech), languages (English, Mandarin, Japanese, Spanish, low-resource languages), and sizes (10 - 960 hours). Most of the corpora are followed the
2SentencePiece toolkit [16] is used to generate the BPE tokens.

Table 1. CER/WER results on various open source ASR corpora. Both Transformer and Conformer models are implemented based on ESPnet toolkit. ∗ marks ESPnet2 results. † and ‡ indicate only w/ speed or only w/ SpecAugment, respectively. § denotes w/o any data augmentation.

Dataset
AIDATATANG AISHELL-1 AISHELL-2 AURORA4 CSJ CHiME4
Fisher-CallHome HKUST JSUT
LibriSpeech REVERB Switchboard TEDLIUM2 TEDLIUM3 VoxForge
WSJ WSJ-2mix

Vocab
Char Char Char Char Char Char BPE Char Char BPE Char BPE BPE BPE Char BPE Char

Metric
CER CER CER WER CER WER WER CER CER WER WER WER WER WER CER WER WER

Evaluation Sets
dev / test dev / test android / ios / mic dev 0330 (A / B / C / D) eval{1, 2, 3} {dt05, et05} {simu, real} dev / dev2 / test / devtest / evltest
dev our split {dev, test} {clean, other} et near / et far eval2000 (callhm/ swbd) dev / test dev / test our split dev93/ eval92
tt

Transformer
(†) 5.9 / 6.7 (†) 6.0 / 6.7 (†) 8.9 / 7.5 / 8.6 3.3 / 6.0 / 4.5 / 10.6 (∗) 4.7 / 3.7 / 3.9 (†) 9.6 / 8.2 / 15.7 / 14.5 22.1 / 21.5 / 19.9 / 38.1 / 38.2
(†) 23.5 (†) 18.7 2.1 / 5.3 / 2.5 / 5.5 (†) 13.1 / 15.4 17.3 / 8.5 9.3 / 8.1 10.8 / 8.4 (§) 9.4 / 9.1 (‡) 7.4 / 4.9 (§) 12.6

Conformer
4.3 / 5.0 (*) 4.4 / 4.7 7.6 / 6.8 / 7.4 4.3 / 6.0 / 5.4 / 9.3 (∗) 4.5 / 3.3 / 3.6 9.1 / 7.9 / 14.2 / 13.4 21.5 / 21.1 / 19.4 / 37.4 / 37.5
(†) 22.2 14.5
1.9 / 4.9 / 2.1 / 4.9 (†) 10.5 / 13.9 15.0 / 7.1 8.6 / 7.2 9.6 / 7.6 (§) 8.7 / 8.2 (‡) 7.7 / 5.3 (§) 11.9

Table 2. WER results on dev/test sets of low-resource language corpora. BPE tokens are used as the output units.

Dataset
Yoloxo´ chitl-Mixtec Puebla-Nahuat
Commonvoice-Czech Commonvoice-Welsh Commonvoice-Russian Commonvoice-Italian Commonvoice-Persian Commonvoice-Polish

Transformer
23.0 / 23.2 27.9 / 26.0 38.2 / 44.3 32.0 / 21.8 22.0 / 27.3 31.8 / 33.7 8.5 / 10.2 24.1 / 15.1

Conformer + Data Augmentation
16.0 / 16.1 23.5 / 21.7 15.3 / 20.6 20.0 / 14.2 6.9 / 8.5 15.6 / 17.0 1.4 / 2.1 8.8 / 2.6

same data preparation procedure as in Kaldi [24]. Optionally, we also use speed perturbation [25] at ratio 0.9, 1.0, 1.1 and SpecAugment [26] for the data augmentation in some corpora.
For each corpus, the detail conﬁgurations of our Conformer model are same as ESPnet Transformer recipes [27] (Enc = 12, Dec = 6, dff = 2048, H = 4, datt = 256). Particularly, the number of attention heads and attention dimensions are different for Librispeech (H = 8, datt = 512). The convolution subsampling layer has 2-layer CNN with 256 channels, stride with 2, and kernel size with 3. For different corpora, we train 20-100 epochs and average the last 10 best checkpoints as the ﬁnal model. We tune the learning rate coefﬁcient (e.g., 1-10) and the kernel size of CONV module (e.g., 5-31) on the corresponding development sets to obtain the best results. Detail setups can be referred to ESPnet recipes3.
4.2. Results
Table 1 shows the character and word error rate (CER/WER) results on each corpus. It can be seen that Conformer model outperforms Transformer on 14/17 corpora in our experiments and even achieves state-of-the-art results on several corpora, like AIDATATANG and AISHELL-1. Instead of the single-speaker speech, it also brings about 7% relative improvement compared with Transformer on the multi-speaker WSJ-2mix data. Besides, we also conduct experiments to investigate the generalization of Conformer models on low-
3https://github.com/espnet/espnet

Table 3. CER/WER results of pure CTC models. Vocabularies, error metrics and evaluations sets are same as Table 1. We only use greedy-search without LM rescoring during inference.

Dataset
CSJ TEDLIUM2
VoxForge WSJ

Transformer-CTC
6.0 / 4.2 / 4.8 16.7 / 16.6 14.0 / 14.1 19.4 / 15.5

Conformer-CTC
4.8 / 3.7 / 3.8 9.3 / 8.7 9.2 / 8.4
12.9 / 10.9

Table 4. CER results of different Transducer models on the dev/test set of the VIVOS corpus. All experiments w/o data augmentation.

Model

dev test

Transformer-Transducer

17.2 17.1

Conformer-Transducer

13.7 14.0

TDNN-Conformer-Transducer 11.6 13.1

resource language corpora, as shown in Table 2. Conformer achieves more than 15% relative improvements in all 8 different languages compared with Transformer model.
Since our Conformer model uses the same decoder framework as Transformer, the performance gains may come from the additional local information provided by the CONV module. Thus, we study the effects of the CONV module by training a pure CTC model or a Transducer model with the Conformer encoder. Table 3 summaries the CER/WER results of two pure CTC models, while Table 4 shows the CER results of different Transducer models. We use a single-LSTM layer decoder in all Transducer models. Detail setups can be referred to ESPnet recipes.3. Both ConformerCTC and TDNN-Conformer-Transducer models show consistent improvement and the Conformer-CTC model even achieves competitive results over Transformer with a decoder. From above results, we can conclude that Conformer shows superior performance in various types of ASR corpora, even in the challenging far-ﬁeld, mixed speech, and low-resource language scenarios.

4.3. Discussion
Following are some training tips from our experiments:
• When Conformer occurs a sudden accuracy drop on the training set, decreasing the learning rate can lead to more stable training. We use the learning rate in {1, 2, 5, 10} for different corpora.
• The kernel size of the CONV module is related to the input sentence length in the corpora. We use the kernel size in {5, 7, 15, 31} for different corpora.
• In addition to the warmup training strategy [1], the OneCycleLR [28] learning scheduler can also give a stable training of self-attention based models.
5. SPEECH TRANSLATION EXPERIMENTS
The conﬁguration of all our ST models are same as ASR systems described in Sec 4.1. During the training, we initialized the model parameters with the pre-trained encoder and decoder optimized on ASR and MT parallel data involved in each corpus, respectively. We conduct the ST experiment on the Fisher-CallHome Spanish corpus, and evaluate on ﬁve common test sets. We use the Fisher-dev set as the development set. The input speech feature is same as the ASR system and the output tokens are 1k BPE tokens. Same data augmentation techniques are used to improve the performance.
The Conformer model achieves about 10% relative improvement over the baseline Transformer model in the ST task as well. To validate the gains did not come from just increasing the model parameters with additional CONV and FFN modules, we also train a Conformer-small model by decreasing dff from 2048 to 1024 to keep the parameter budget for a fair comparison. Although the BLEU score is slightly decreased by halving dff, our Conformersmall model still signiﬁcantly outperforms the Transformer model.

Table 6. Signal-to-Distortion Ratio (SDR) results of different models on the WSJ0-2mix cv/tt sets using uPIT.

Model

cv tt

BLSTM-uPIT [20] 9.5 9.5

BLSTM-uPIT (ours) 10.4 10.3 Transformer-uPIT 9.3 8.8 Conformer-uPIT 10.5 10.2

7. TTS EXPERIMENTS
In TTS experiments, we evaluate with three common corpora: LJSpeech (22.05 kHz, English, 24 hours), JSUT (24 kHz, Japanese, 10 hours), and CSMSC (24 kHz, Mandarin, 12 hours), all of which consist of single female speaker speech. We compare the Conformer-based non-autoregressive models with Transformerbased models, including Transformer-TTS [30], FastSpeech (FS) [21], and FastSpeech2 (FS2) [22]. We used datt = 368, dﬀ = 1536, and H = 2. The number of encoder and decoder blocks was set to six for FS, and four for FS2. For FS2, instead of the quantized pitch and energy prediction, we use the token-average pitch and energy prediction introduced in [23] to avoid overﬁtting. As for the nerual vocoder, we use an open-source implementation4 of Parallel WaveGAN [31]. All of the models are implemented with ESPnet2 except for the neural vocoder. Training conﬁgurations, generated samples, and pre-trained models are available on Github3.
Table 7 shows mel-cepstral distortion (MCD), which was calculated with 0-34 order mel-cepstrum and dynamic time warping (DTW) to match the length between the groundtruth and the prediction. The result demonstrates that the Conformer-based models always bring consistent improvement for all corpora, achieving the best performance among the compared models.

Table 5. BLEU scores on Fisher-CallHome Spanish corpus. 1k SentencePiece tokens are used as the output units.

Model
Transformer Conformer-small
Conformer

#Param [M]
33.3 30.9 43.5

dev
48.70 49.96 51.14

Fisher dev2
48.56 50.80 51.59

test
47.95 50.23 51.03

CallHome devtest evltest

18.53 19.51 19.97

18.61 19.19 20.44

6. SPEECH SEPARATION EXPERIMENTS
For the SS task, we compare our Conformer model with Transformer and bidirectional long short-term memory (BLSTM) on WSJ0-2mix corpus. Both models are trained with uPIT [20] based on Phase Sensitive Masks (PSM) and ReLU activation function. The input features are 129-dimensional short-time Fourier transform (STFT) magnitude spectra computed with a sampling frequency of 8 kHZ, a frame size of 32 ms, and a 16 ms frame shift. The BLSTM-uPIT model has 3 BLSTM layers (d = 896), while the Transformer-uPIT and Conformer-uPIT model consist of 3 blocks (dff = 896, datt = 1024, H = 8).
Table 6 summaries the Signal-to-Distortion Ratio (SDR) [29] results of different models on the WSJ0-2mix sets, the current benchmark dataset to validate monaural speech separation. The results show that our Conformer-uPIT model gets competitive results compared with the BLSTM-uPIT model and achieves a signiﬁcant improvement over the Transformer-uPIT model.

Table 7. MCD [dB] results on various open source TTS corpora. All models used a phoneme sequence as the input.

Model
Transformer-TTS Transformer-FS Conformer-FS Transformer-FS2 Conformer-FS2

LJSpeech
7.26 6.91 6.88 6.85 6.79

JSUT
7.10 6.75 6.69 6.69 6.56

CSMSC
6.61 6.27 6.21 6.30 6.25

8. CONCLUSION
We conducted comparative studies of the Conformer model in various speech applications with a large number of publicly available corpora. Speciﬁcally, the experiments were conducted on 25 ASR corpora (17 common sets + 8 low-resource sets), 1 ST corpus, 1 SS corpus, and 3 TTS corpora. From the experiments, our Conformerbased models achieved signiﬁcant improvements in many ASR, ST and TTS tasks and competitive results in SS tasks. We believe that the various benchmark results, reproducible recipes, well-trained models and training tips described in this paper will accelerate the Conformer research on speech applications. Our aim for this activity is to ﬁll out the gap between high-resource research environments in big players and those in the academia or small-scale research groups by providing these up-to-date research environments.
4https://github.com/kan-bayashi/ParallelWaveGAN

9. REFERENCES
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, et al., “Attention is all you need,” in Proc. NeurIPS, 2017, pp. 5998–6008.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. ACL, 2018, pp. 4171–4186.
[3] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, et al., “Transformer-XL: Attentive language models beyond a ﬁxed-length context,” in Proc. ACL, 2019, pp. 2978–2988.
[4] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-Transformer: A no-recurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP. IEEE, 2018, pp. 5884–5888.
[5] Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schlu¨ter, and Hermann Ney, “A comparison of Transformer and LSTM encoder decoder models for ASR,” in Proc. ASRU. IEEE, 2019, pp. 8–15.
[6] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, et al., “A comparative study on Transformer vs RNN in speech applications,” in Proc. ASRU. IEEE, 2019, pp. 449–456.
[7] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, et al., “Conformer: Convolution-augmented transformer for speech recognition,” arXiv preprint arXiv:2005.08100, 2020.
[8] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. INTERSPEECH, 2018, pp. 2207–2211.
[9] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Enrique Yalta Soplin, et al., “ESPnet-ST: All-in-one speech translation toolkit,” in Proc. ACL: System Demonstrations, 2020, pp. 302–311.
[10] Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, Takenori Yoshimura, Shinji Watanabe, Tomoki Toda, Kazuya Takeda, Yu Zhang, and Xu Tan, “ESPnet-TTS: Uniﬁed, reproducible, and integratable open source end-to-end text-tospeech toolkit,” in Proc. ICASSP, 2020, pp. 7654–7658.
[11] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, et al., “Learning deep Transformer models for machine translation,” in Proc. ACL, 2019, pp. 1810–1822.
[12] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2014.
[13] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler, “Efﬁcient Transformers: A survey,” arXiv preprint arXiv:2009.06732, 2020.
[14] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier, “Language modeling with gated convolutional networks,” in Proc. ICML, 2017, pp. 933–941.
[15] Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, et al., “Understanding and improving Transformer from a multi-particle dynamic system point of view,” in Proc. ICLR, 2020.
[16] Taku Kudo and John Richardson, “Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,” in Proc. EMNLP, 2018, pp. 66–71.

[17] Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber, “Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICLR, 2006, pp. 369–376.
[18] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTCattention based end-to-end speech recognition using multi-task learning,” in Proc. ICASSP. IEEE, 2017, pp. 4835–4839.
[19] Takaaki Hori, Jaejin Cho, and Shinji Watanabe, “End-to-end speech recognition with word-based RNN language models,” in Proc. SLT. IEEE, 2018, pp. 389–396.
[20] Morten Kolbæk, Dong Yu, Zheng-Hua Tan, and Jesper Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM TASLP, vol. 25, no. 10, pp. 1901–1913, 2017.
[21] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, et al., “Fastspeech: Fast, robust and controllable text to speech,” in Proc. NeurIPS, 2019, pp. 3171–3180.
[22] Yi Ren, Chenxu Hu, Tao Qin, Sheng Zhao, Zhou Zhao, et al., “FastSpeech 2: Fast and high-quality end-to-end textto-speech,” arXiv preprint arXiv:2006.04558, 2020.
[23] Adrian Lan´cucki, “Fastpitch: Parallel text-to-speech with pitch prediction,” arXiv preprint arXiv:2006.06873, 2020.
[24] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, et al., “The Kaldi speech recognition toolkit,” in Proc. ASRU. IEEE, 2011.
[25] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, “Audio augmentation for speech recognition,” in Proc. INTERSPEECH, 2015, pp. 3586–3589.
[26] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, et al., “SpecAugment: A simple data augmentation method for automatic speech recognition,” in Proc. INTERSPEECH, 2019, pp. 2613–2617.
[27] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, et al., “Improving Transformer-based end-to-end speech recognition with connectionist temporal classiﬁcation and language model integration,” in Proc. INTERSPEECH, 2019, pp. 1408–1412.
[28] Leslie N Smith and Nicholay Topin, “Super-convergence: Very fast training of neural networks using large learning rates,” in Artiﬁcial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics, 2019, vol. 11006, p. 1100612.
[29] Emmanuel Vincent, Re´mi Gribonval, and Ce´dric Fe´votte, “Performance measurement in blind audio source separation,” IEEE/ACM TASLP, vol. 14, no. 4, pp. 1462–1469, 2006.
[30] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu, “Neural speech synthesis with Transformer network,” in Proc. AAAI, 2019, vol. 33, pp. 6706–6713.
[31] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim, “Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,” in Proc. ICASSP. IEEE, 2020, pp. 6199–6203.

