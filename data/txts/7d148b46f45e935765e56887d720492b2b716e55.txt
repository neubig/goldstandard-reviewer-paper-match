Inferring network connectivity from event timing patterns
Jose Casadiego1,2, Dimitra Maoutsa2, Marc Timme1,2,3,4
1Chair for Network Dynamics, Institute of Theoretical Physics and Center for Advancing Electronics Dresden (cfaed), Technical University of Dresden, 01062 Dresden, Germany
2Network Dynamics, Max Planck Institute for Dynamics and Self-Organization (MPIDS), 37077 Göttingen, Germany 3Bernstein Center for Computational Neuroscience (BCCN), 37077 Göttingen, Germany
4Advanced Study Group, Max Planck Institute for the Physics of Complex Systems, 01069 Dresden, Germany
Reconstructing network connectivity from the collective dynamics of a system typically requires access to its complete continuous-time evolution although these are often experimentally inaccessible. Here we propose a theory for revealing physical connectivity of networked systems only from the event time series their intrinsic collective dynamics generate. Representing the patterns of event timings in an event space spanned by inter-event and cross-event intervals, we reveal which other units directly inﬂuence the inter-event times of any given unit. For illustration, we linearize an event space mapping constructed from the spiking patterns in model neural circuits to reveal the presence or absence of synapses between any pair of neurons as well as whether the coupling acts in an inhibiting or activating (excitatory) manner. The proposed model-independent reconstruction theory is scalable to larger networks and may thus play an important role in the reconstruction of networks from biology to social science and engineering.

arXiv:1803.09974v2 [q-bio.NC] 29 Mar 2018

The topology of interactions among the units of network dynamical systems fundamentally underlies their systemic function. Current approaches to reveal interaction patterns of a network from the collective nonlinear dynamics it generates [1–9] rely on directly sampling the trajectories of the collective time evolution. Such sampling requires experimental access to the continuously ongoing dynamics of the network units.
For a range of systems, however, direct access to the units’ internal states is not granted, but only times of events are available. Prominent examples include the times of messages initiated or forwarded in online social networks and distributed patterns of action potentials (spikes) emitted by the neurons of brain circuits, both reﬂecting the respective network structure in a nontrivial way [10–15]. Reconstruction of physical network connectivity from such timing information has been attempted for speciﬁc settings for neural circuits or online social contacts. Often it is limited to small networks (102 units), by large computational eﬀorts including high-performance parallel computers up to about 103 units, or to knowing speciﬁc system models in advance [13, 14, 16–20]. For instance, recent eﬀorts on reconstructing spiking neural circuit connectivity [19] show that combining stochastic mechanisms for spike generation and linear kernels for spike integration enables reconstruction of larger networks (103 neurons), if the spike integration model closely matches the original simulated systems. Alternatively, in systems of pulse-coupled units [20] reconstruction is feasible if the units are all intrinsic oscillators. Besides such speciﬁc solutions, a general model-independent theory based on timing information generated by the collective network dynamics is as yet unknown.
In this Letter, we propose a general theory for reconstructing physical network connectivity based only on the

a

b

Neuron 1

Neuron 2

Neuron 3

Neuron N

Figure 1. Representing timing patterns in event space. a, Schematics of an event for a sample unit (i = 1) formed by the inter-event time of unit 1 and all cross-event intervals that may inﬂuence the subsequent event of unit 1. An additional counting index m is dropped for simplicity b, In the event space for neuron i, each event ei,m is represented by a point on the manifold deﬁned through (3).
event timing patterns generated by the collective spiking dynamics. The theory reveals existence and absence of interactions, their activating or deactivating nature, and enables reliable network reconstruction from regular as well as irregular timing patterns, even if some (hidden) units cannot be observed. The proposed reconstruction theory is model-independent (thereby, purely datadriven) and trivially parallelizable because linearized mappings for diﬀerent units are computationally independent of each other.
Mapping timing patterns to physical connections? To present the proposed theory consistently, we focus on a setting (and notation) of networks of spiking neurons. Alternate applications work in qualitatively the same way and are discussed towards the end of this article. Here the units are individual neurons, a physical connection is a synapse from one neuron to another and the events observed for each unit are the electrical action potentials

or spikes emitted by the neuron. Speciﬁcally, inter-event intervals are inter-spike intervals (ISIs) and cross-event intervals cross-spike intervals (CSIs). The theory stays unchanged (up to notation) for all kinds of event times observed from and originally generated by any speciﬁc collective network dynamics that are typically unknown but coordinated via the network interactions.
Thus, consider a network of N units i ∈ {1, . . . , N } generating spatio-temporal spike patterns (Fig. 1) deﬁned by the sets of times ti,m, where m ∈ N counts the spike times. An inter-spike interval (ISI)

∆Ti,m := ti,m − ti,m−1 > 0,

(1)

measures the duration of time between two consecutive events, the (m − 1)-st and the m-th spike times ti,m−1 and ti,m of neuron i. Similarly, the cross-spike intervals (CSIs)

wji,k,m := tj,p − ti,m−1 > 0,

(2)

measure the duration between the p-th spike generated by neuron j and the previous ((m − 1)st) spike generated by i with tj,p < ti,m (all superscripts throughout this article denote indices, not powers). We index the CSIs by integers k ≡ k(ti,m−1, tj,p) starting with k = 1 for the ﬁrst spike at tj,p = minp′ {tj,p′ | tj,p′ > ti,m−1} of unit j after ti,m−1 and sequentially increasing k by one by counting through the sequence of tj,p forwards in time. Figure 1 illustrates this deﬁnition for one given ISI, where the indices m − 1 and m are suppressed. Now consider that some ﬁnite number Ki of spike times (for each of the other neurons j) preceding ti,m inﬂuences the time ti,m in a relevant way.
As a core conceptual step, we propose that the ISIs of each neuron i are approximately given by some unknown, locally smooth function hi : RN×Ki → R of the Ki relevant cross-spike intervals (see Figure 1b) such that

∆Ti,m = hi ΛiWmi

(3)

for all m. Here the explicit dependency matrix Λi ∈ {0, 1}N×N , cf. [21], is a diagonal matrix (to be deter-
mined) indicating whether there is a physically active (synaptic) connection from neuron j to i (Λijj = 1) or not (Λijj = 0). The matrix

 w1i ,1,m  w2i ,1,m Wmi :=  ...

w1i ,2,m w2i ,2,m
...

. . . w1i ,Ki,m 

. . . w2i ,Ki,m 

i

. . . ...  ∈ RN×K ,

wNi ,1,m wNi ,2,m . . . wNi ,Ki,m (4)
collects the Ki CSIs generated by each neuron j until

just before the m-th spike generated by neuron i. We

refer to the k-th column

i
wk,m

:=

w1i ,k,m, w2i ,k,m, . . . , wNi ,k,m T ∈ RN ,

(5)

2

of Wmi as the k-th presynaptic proﬁle of neuron i before its m-th spike time. It indicates when presynaptic neurons ﬁred for the ﬁrst time, second time, and so on until Ki-th time, before ti,m. For illustration purposes, we here speciﬁcally constrain Ki to take into account only those spike times within the currently considered ISI such that tj,p ∈ [ti,m−1, ti,m]. If a presynaptic neuron does not spike within this interval, its CSIs are set to zero, wji,k,m := 0, in (4). From a biological perspective, the function hi is determined by the intrinsic properties of neuron i including its spike generation mechanism as well as its pre- and postsynaptic processes. The function hi is in general unknown.
In particular, equation (3) assigns a speciﬁc ISI to a speciﬁc collection of timings of presynaptic inputs. Therefore, we may represent such ISI-CSIs tuple in a higher dimensional event space Ei ⊂ R(NKi+1), where each realization of neuron i’s dynamics is given by events deﬁned as
ei,m := vec Wmi , ∆Ti,m T ∈ R(NKi+1), (6)
where the operator vec : RS×R → RSR stands for vectorization of a matrix and it transforms a matrix into a column vector [22].
So, how can events ei,m in the event space Ei help us determine the synaptic inputs to neuron i? Consider a local sample of M + 1 events in Ei as follows (Figure 2a). Selecting a reference event

ei,r := arg min

ei,m − ei,s 2 ,

(7)

ei,s

m

closest to all other events in the sample, with m, s ∈ {1, 2 . . . , M + 1}, yields an approximation

∆Ti,m =. ∆Ti,r + tr

∂hi ∂W i

T
Λi Wmi − Wri

, (8)

of (3) linear in the diﬀerences Wmi − Wri around ei,r . Here ∂hi/∂W i ≡ ∂hi/∂W i Λi Wri ∈ RN×Ki is a matrix derivative and tr(·) is the trace operation. Rewrit-
ing (8) yields

.

Ki

ii

i

∆Ti,m = ∆Ti,r + ∇hi,kΛ wk,m − wk,r , (9)

k=1

where ∇hi,k := ∂hi , ∂hi , . . . , ∂hi ∈ RN is the

∂W1ik ∂W2ik

∂WNi k

gradient of the function at the k-th presynaptic proﬁle.

Thus, given m ∈ {1, 2, . . . , M } diﬀerent events (in addi-

tion to the reference event r), ﬁnding the synaptic con-

nectivity becomes solving the linear regression problem

(9) for the unknown parameters ∇hi,kΛi. In particu-

lar, system (9) may be computationally solved in paral-

lel for diﬀerent units i. We here solve such linear system

3

a

b

a

b

inhibitory

Regular

excitatory

Intermediate Irregular

Figure 2. Slopes in event space yield excitatory and inhibitory synaptic interactions. a, Schematics of a local sampling in the event space. Local samplings may be taken from regular spiking patterns or subsets of irregular spike patterns such that events are close-by in event space. Taking a reference event (red dot) and linearly approximating all other events (gray dot) through (9) constrains existence and sign of interactions without knowing a system model. b, Reconstruction of inhibitory (green), excitatory (orange) and absent (blue) synaptic interactions of a neuron in a random network of N = 100 LIF neurons having δ-synapses with Nexc = 50 excitatory and, Ninh = 50 inhibitory neurons and a connection probability p = 0.1. See [26] for further parameters. The reddashed lines indicate the optimal thresholds (as calculated via Otsu’s method [27]) to distinguish excitatory and inhibitory from absent interactions.

Figure 3. Revealing synaptic connections from regular to irregular spiking patterns. a, Schematic representation of spike trains with diﬀerent degrees of irregularity. To establish test statistics, the spike trains were sampled from blue random networks of N = 100 inhibitory LIF neurons interacting via δ-synapses with connection probability p = 0.1. All simulations were performed using identical time intervals of 500 s each (see [26] for further settings). b, AUC scores for two diﬀerent sampling conditions: random and closest. In the random sampling paradigm, events are randomly drawn from the uniform distribution across the spike trains, while in a closest sampling paradigm, the same number of events closest to a reference event are jointly considered for reconstruction. Reconstruction quality decreases with spiking pattern irregularity for random sampling (dark gray) yet stays consistently high for closest sampling. Gray dashed line indicates random guessing.

via least squares minimization, compare [8, 23]. Blocksparse regression algorithms may also be employed, especially if one proposes higher-order approximations than (9) [21, 24].
Revealing synapses in networks of spiking neurons. To validate the predictive power of our theory, we revealed the synaptic connectivity (i) of model systems with current-based and conductance-based synapses, (ii) of excitatory and inhibitory synapses, (iii) with both instantaneous and temporally extended responses, (iv) of moderately larger number of neurons (compared to the state-of-the-art), (v) for regular and irregular spiking patterns and (vi) under conditions where a subset of units is hidden or unavailable to observation (Figures 2, 3, and 4). We quantify performance of reconstruction by the Area Under the Receiver-Operating-Characteristic Curve (AUC) score [25], that equals 1 for perfect reconstruction and 1/2 for predictions as good as random guessing.
We start illustrating successful reconstruction for simple networks of leaky integrate-and-ﬁre (LIF) [28] neurons with mixed inhibitory and excitatory current-based δ-synapses (see Supplemental Material [26]). As Figure 2b shows by example, the method of event-space mappings does not only reveal the existence and absence of synapses between pairs of neurons in the network; in particular, the signs and magnitudes of the derivatives ∂hi/∂Wji1 already indicate whether a synapse acts in an inhibitory or excitatory way: indeed, inhibitory inputs to neuron i retard its subsequent spike and thereby extend the duration of an ISI, whereas excitatory inputs

shorten the duration. Thus, positive derivatives indicate (eﬀectively) inhibitory and negative derivatives excitatory interactions, compare Figure 2b.
Reconstruction from irregular event patterns? What if the spiking patterns are not regular as often the case for real data (from neurophysiological recordings as well as from observations in any other discipline) and thus not all events are located suﬃciently close to one reference event (7)? If the induced events ei,m are distributed in event space Ei less locally, with the ei,m located on diﬀerent, possibly non-adjacent patches in Ei, we systematically collect only those events that are located close to selected reference events ei,r [29]. To check performance, we systematically varied how irregular spike timing patterns are by varying the overall coupling strength by a factor of 30, eﬀectively interpolating between regimes close to regular dynamics (locking) [30] for small coupling and close to irregular balanced states [12, 31–33] for large coupling. If suﬃciently many events (ISI-CSI combinations) that are close in event space occur in the network’s spike timing patterns, sampling the events in some local region (or, alternatively, local regions) in event space may be compensated by longer recording times or collecting several patches of events. With increasing inhibition, the total number of spikes in each recording decreases whereas the spike sequence irregularity increases. Thereby, to ensure a suﬃcient number of events locally in event space across inhibition levels, we simulated all networks for 500 s each. We indeed ﬁnd that such closest sampling paradigm enables consistent high-quality reconstruction for regular,

a

b

4



b

c

c

d

Figure 4. Model-independence and robustness against hidden units. Quality of reconstruction vs. the number of events M considered for blue random networks of N = 100 blue and p = 0.1 a, LIF neurons with α-synapses (ESL, CCorr, MI, STA and RG stand for event-space linearization (our approach, marked in blue), cross-correlations (green), mutual information (orange), spike-triggered average (rose) and random guessing (gray), respectively); b, HodgkinHuxley neurons with conductance-based synapses; c, LIF neurons coupled with δ-synapses where only 80 neurons are observed. d, Systematic reconstructions versus the fraction of observed neurons. Network parameters are Nexc = 50 and Ninh = 50, see [26] for further settings.
intermediate and irregular spike timing patterns alike (Fig. 3).
We reiterate that approximation (9) requires no a priori information about circuit or neuron models and parameters, instead, only spike timing data are necessary to reveal synapses. The same event-space/linearization (ESL) method proposed performs robustly across diﬀerent circuits of neurons (low- and higher-dimensional neuron models and diﬀerent synaptic models), compares favorably to alternative approaches of model-free reconstruction and even yields reasonable estimates if some units are hidden (see [26] for more details), see Figure 4. Figure panels 4a and b illustrate the performance for networks of simple LIF neurons with currentbased synapses as well as for more biophysically detailed Hodgkin-Huxley neurons coupled via conductance-based synapses (see [26]) explicating that the theory is insensitive to changes of types of coupling and neurons. The ESL method outperforms predictions by statistical dependencies such as cross-correlations, mutual information and spike-triggered averages. In addition, if some neurons are inaccessible (hidden units), synapses among

Figure 5. Reconstructing large networks is computationally feasible. Reconstruction of random networks of N = 2000 LIF neurons with δ-synapses and p = 0.1. See [26] for more details. a, Connectivity matrix excitatory (red) and inhibitory (blue) synapses. Inset shows a close up. b, Distribution of reconstructed excitatory (orange), absent (blue) and inhibitory (green) synapses of a single postsynaptic neuron. c, Quality of reconstruction for individual neurons for M = 8000. ESL, CCorr, MI and STA stand for the approaches introduced here, using cross-correlations, mutual information and spike-triggered averages, respectively. The gray-dashed line stands for random guessing.
accessible neurons may still be recovered (see [26]), Figure 4c-d. Furthermore, a systematic study shows that ESL accurately determines synaptic links in the presence of external inputs emulating neurons ﬁring with Poisson statistics, at the expense of requiring to record a larger number of events (see Supplementary Material [26] for a detailed study). As explained above, recording a larger number of events promotes denser samplings in the event space, which in turn aids in ﬁltering out dynamical effects related to such unobserved inputs when solving the regression problem.
Finally, reconstructing larger networks seems computationally feasible and reconstruction quality compares favorably to other general, model-free approaches. For instance, Figure 5 illustrates successful reconstruction of the presynaptic pattern of a random unit i in a network of N = 2000 neurons exhibiting regular dynamics that was computed within about 500 seconds (∼ 10 minutes per neuron) on a single machine [34]. Generally, the computational (time) complexity of our reconstruction theory resutls from the computation of event-space distances, which scales as O (M N K)2 (see [26]). The approach separates not only existing from absent but also excitatory from inhibitory synapses. As before, a systematic comparison for equal number of events shows that our approach again outperforms predictions by commonly employed statistical dependency quantiﬁers, Figure 5c.
Summary and Conclusions. We presented a general, model-independent theory for reconstructing the topology of a network’s direct interactions from observed patterns of event timings only. A key advance is the proposal that the interevent intervals are given by some unknown,

suﬃciently smooth function, thereby not requiring access to a event generating system model or even the full state vector of the dynamical system generating the events. We illustrated core performance aspects including robustness (against changes in unit dynamics and coupling schemes), computational performance (rapid analysis on single machines and parallelizability) and suitability even if some units are hidden by example of spiking neural networks. By representing inter-event intervals as functions of crossevent intervals, we mapped the problem to event spaces yielding linear equations that enable robust least squares solutions for the topology. Thereby, the approach may be transferred to other systems generating event time series.
We remark that ∇hi,kΛi in (9) maps exactly those physical connections that are used for transmitting signals that actually directly inﬂuence the timings ti,m and thus the inter-event times. This has distinct consequences in practice. For instance, a presynaptic neuron j may generate its only potentially relevant spike during the refractory period of neuron i (or may simply not generate any spike) during the observation period of the experiment recording the timing pattern, thus no synapse from unit j will be indicated even if an anatomical one exists. At the same time, if for any reason (e.g. measurement error or data corruption) an event of unit i is not recorded at all or the ISI recorded with a large error, it becomes easy to spot this event as an outlier as it would lie far above or below the other events with nearby CSIs. As only the presence (and sign) of the above derivatives play a role for reconstruction, a straightforward generalization is to collect events close to several references ei,r , ei,r′ , etc. and concatenate local (and generally diﬀerent) approximations of the form (9).
A recent work [35] studying excitatory-to-inhibitory CA1 synapses in vivo focused on predicting exclusively the strongest interactions from excitatory to inhibitory neurons using a statistical GLM-based method combined with cross-correlograms, which results in highcomputational demands when applied on large networks. The clearest advantages of our theory, beyond its simplicity, are its rapid computational performance and its generality and model-independence, compare to a recent alternative approach [20]. Moreover, our ansatz by construction generalizes to systems beyond spiking neural networks (used here as a systematic case study for illustration) and may be applied, for instance, for revealing friendship networks in online social networks [13, 14] or to determine who communicates with whom in hidden web or wireless services [36, 37] – from the timing of local unit activities alone. Taken together, the results illustrated above suggest the power of systematically generalizing a state space perspective for representing the full trajectories to an event space perspective for representing collectively coordinated event timing patterns.
Acknowledgements. We thank Viola Priesemann, Juan Florez-Weidinger, Mantas Gabrielaitis and Stayko Popov

5
for valuable discussions. We gratefully acknowledge support from the Federal Ministry of Education and Research (BMBF Grant No. 03SF0472F), the Max Planck Society and the German Research Foundation (DFG) through the Cluster of Excellence Center for Advancing Electronics Dresden (cfaed). J. Casadiego and D. Maoutsa contributed equally to this work.

[1] M. K. Yeung, J. Tegner, and J. J. Collins,

Proc. Natl. Acad. Sci. U. S. A. 99, 6163 (2002).

[2] T. S. Gardner, D. di Bernardo, D. Lorenz, and J. J.

Collins, Science 301, 102 (2003).

[3] D. Yu, M. Righero,

and L. Kocarev,

Phys. Rev. Lett. 97, 188701 (2006).

[4] M. Timme, Phys. Rev. Lett. 98, 224101 (2007).

[5] D. Yu and U. Parlitz, Phys. Rev. E 82, 026108 (2010).

[6] W.-X. Wang, Y.-C. Lai,

and C. Grebogi,

Phys. Rep. 644, 1 (2016).

[7] B. Barzel and A. L. Barabási, Nat. Biotechnol. 31, 720

(2013).

[8] M.

Timme

and

J.

Casadiego,

J. Phys. A Math. Theor. 47, 343001 (2014), 1408.2963.

[9] M. Nitzan, J. Casadiego,

and M. Timme,

Sci. Adv. 3, e1600396 (2017).

[10] R. M. Memmesheimer and M. Timme, Physica D 224,

182 (2006).

[11] R. M. Memmesheimer and M. Timme, Phys. Rev. Lett.

97, 188101 (2006).

[12] S. Jahnke, R. M. Memmesheimer, and M. Timme, Phys.

Rev. Lett. 100, 048102 (2008).

[13] T. Aoki, T. Takaguchi, R. Kobayashi, and R. Lambiotte,

Phys. Rev. E 94, 042313 (2016).

[14] R. Kobayashi and R. Lambiotte, in

Tenth International AAAI Conference on Web and Social Media

(2016).

[15] C. Kirst, M. Timme,

and D. Battaglia,

Nat. Commun. 7, 11061 (2016).

[16] V. A. Makarov, F. Panetsos, and O. De Feo,

J. Neurosci. Methods 144, 265 (2005).

[17] F. Van Bussel, B. Kriener, and M. Timme,

Front. Comput. Neurosci. 5, 3 (2011).

[18] F. Gerhard, T. Kispersky, G. J. Gutier-

rez, E. Marder, M. Kramer, and U. Eden,

PLoS Comput. Biol. 9, e1003138 (2013).

[19] Y. V. Zaytsev, A. Morrison, and M. Deger, J. Comput.

Neurosci. 39, 77 (2015).

[20] R.

Cestnik

and

M.

Rosenblum,

Phys. Rev. E 96, 012209 (2017).

[21] J. Casadiego, M. Nitzan, S. Hallerberg, and M. Timme,

Nat. Commun. 8, 2192 (2017).

[22] J. Magnus and H. Neudecker, Technometrics, Vol. 31

(1989) p. 501.

[23] S.

G.

Shandilya

and

M.

Timme,

New J. Phys. 13, 013004 (2011).

[24] Y.

C.

Eldar

and

M.

Mishali,

IEEE Trans. Inf. Theory 55, 5302 (2009).

[25] T. Fawcett, Pattern Recognit. Lett. 27, 861 (2006).

[26] See Supplemental Material for more details on neuron

network models, simulation parameters and additional

results.

[27] N. Otsu, IEEE Trans. Syst. Man. Cybern. 9, 62 (1979).

[28] N. Brunel, Comput. Neurosci. 8, 183 (2000).

[29] One may alternatively collect events close to several ref-

erences ei,r , ei,r′ , etc. and concatenate local (and gen-

erally diﬀerent) approximations of the form (9) yielding

constraints of the same (linear) type and solve the regres-

sion problem via block-sparse regression [21].

[30] C. van Vreeswijk, Phys. Rev. E 54, 5522 (1996).

[31] C. van Vreeswijk and H. Sompolinsky,

Science 274 (1996).

[32] M. Timme, F. Wolf,

and T. Geisel,

Phys. Rev. Lett. 89, 258701 (2002).

[33] D. Hansel and G. Mato, J. Neurosci. 33, 133 (2013).

6

[34] Reconstructions were performed on a single core

of a desktop computer with Intel® Core™ i7-4770

CPU@3.40GHz octocore processor and 16 GB of RAM

memory . Most of execution time is exhausted in the com-

putation of distances among the events. Reconstruction

time is further determined by the speed of the numerical

package chosen to perform least squares optimization.

[35] D. F. English, S. McKenzie, T. Evans, K. Kim, E. Yoon,

and G. Buzsáki, Neuron 96, 505 (2017).

[36] C.

Bettstetter

and

Christian,

in

Proc. 3rd ACM Int. Symp. Mob. ad hoc Netw. Comput. - MobiHoc ’0

(ACM Press, New York, New York, USA, 2002) p. 80.

[37] J. Klinglmayr, C. Kirst, C. Bettstetter, and M. Timme,

New J. Phys. 14, 073031 (2012).

