Spherical convolutions on molecular graphs for protein model quality assessment

arXiv:2011.07980v2 [q-bio.QM] 6 Jan 2021

Ilia Igashov∗ Moscow Institute of Physics and Technology
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France
igashov.is@phystech.edu

Nikita Pavlichenko∗ Moscow Institute of Physics and Technology Dolgoprudny, Moscow region, 141700, Russia
pavlichenko.nv@phystech.edu

Sergei Grudinin Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France
sergei.grudinin@inria.fr

Abstract
Processing information on 3D objects requires methods stable to rigid-body transformations, in particular rotations, of the input data. In image processing tasks, convolutional neural networks achieve this property using rotation-equivariant operations. However, contrary to images, graphs generally have irregular topology. This makes it challenging to deﬁne a rotation-equivariant convolution operation on these structures. In this work, we propose Spherical Graph Convolutional Network (S-GCN) that processes 3D models of proteins represented as molecular graphs. In a protein molecule, individual amino acids have common topological elements. This allows us to unambiguously associate each amino acid with a local coordinate system and construct rotation-equivariant spherical ﬁlters that operate on angular information between graph nodes. Within the framework of the protein model quality assessment problem, we demonstrate that the proposed spherical convolution method signiﬁcantly improves the quality of model assessment compared to the standard message-passing approach. It is also comparable to state-of-the-art methods, as we demonstrate on Critical Assessment of Structure Prediction (CASP) benchmarks. The proposed technique operates only on geometric features of protein 3D models. This makes it universal and applicable to any other geometric-learning task where the graph structure allows constructing local coordinate systems. We will make the method available at https://team.inria.fr/nano-d/software/s-gcn/.
1 Introduction
Prediction of protein three-dimensional (3D) structure is an important problem in structural biology and structural bioinformatics. Despite tremendous progress in this ﬁeld [Greener et al., 2019, Xu, 2019, Senior et al., 2020, Kryshtafovych et al., 2019], particularly in light of the recent CASP14 results [Callaway, 2020], the accuracy of the predicted structures tends to vary signiﬁcantly depending on the availability of additional information, and the number of homologous structures and sequences in the databases [Abriata et al., 2019, Senior et al., 2019, Zheng et al., 2019, Hou et al., 2019]. Therefore, estimation of reliability of the predicted models, and also the assessment of the local structural fragments, is crucial for the practical application of these predictions.
∗These authors contributed equally.
Preprint.

The problem of protein model quality assessment (MQA) has been recognized by the protein structure modeling community and became one of the subchallenges of CASP, the Critical Assessment of protein Structure Prediction community-wide challenge [Cheng et al., 2019, Won et al., 2019]. All of the state-of-the-art MQA methods use, to a certain extent, supervised or unsupervised machine learning. Initially, statistical potentials [Olechnovicˇ and Venclovas, 2017], shallow neural networks [Wallner and Elofsson, 2003], regression methods, and support vector machines [Ray et al., 2012, Uziela et al., 2016] were widely used. More recently, this problem has also got attention from the machine-learning community. This triggered the development of more advanced approaches, such as deep learning-based techniques [Derevyanko et al., 2018, Pagès et al., 2019, Conover et al., 2019, Hiranuma et al., 2020, Jing et al., 2020, Eismann et al., 2020] and graph convolutional networks (GCN) [Baldassarre et al., 2020, Sanyal et al., 2020, Igashov et al., 2020]. The latter methods operate on a molecular graph representation of protein models.
In this work, we propose to capture the 3D structure of a molecular graph using convolution operation based on spherical harmonics. The main idea of our approach is to learn spatial ﬁlters in a reference orientation of each graph node. Indeed, proteins are chained molecules, with a repeated topology of the backbone. Thus, using local coordinate frames constructed on the protein’s backbone, we can build rotational-equivariant spherical ﬁlters. We then incorporate these ﬁlters into a message-passing framework and design a new method called Spherical Graph Convolutional Network (S-GCN), which signiﬁcantly outperforms the classical GCN architecture.
Most of the protein MQA methods operate on the atom-level representation of a protein molecule [Uziela et al., 2016, Olechnovicˇ and Venclovas, 2017, Karasikov et al., 2019, Pagès et al., 2019, Igashov et al., 2020]. At the same time, the state-of-the-art methods use various types of features that often include information about the evolution of molecules or other biological characteristics. On the contrary, S-GCN works with the residue-level protein representation, which signiﬁcantly reduces computational costs and the number of parameters. Also, our method processes only geometric information, i.e. the input feature vector of each amino acid contains only three geometric features and a one-hot vector representing the type of the amino acid. This work demonstrates that the state-of-the-art quality of the protein model assessment task can be achieved using only geometric properties of protein models without any chemo-physical prior information, which often requires additional expensive computations.
The main results of our work can be summarized as follows:
• We propose a new message-passing method based on trainable rotational-equivariant spherical ﬁlters.
• The proposed method signiﬁcantly improves the quality of model assessment as compared to a classical GCN approach, applied to the same input conﬁguration.
• Despite the residue-level representation and only geometric input features, the results of the proposed method are comparable to the state of the art.
2 Related work
Structural bioinformatics. Quality assessment of protein models is a classical problem in protein structure prediction community. There have been multiple approaches developed over last 30 years. These include physics-based techniques [Randall and Baldi, 2008, Faraggi and Kloczkowski, 2014], statistical and unsupervised methods, such as DFIRE [Zhou and Zhou, 2002], DOPE [Shen and Sali, 2006], GOAP [Zhou and Skolnick, 2011], RWplus [Zhang and Zhang, 2010], ORDER_AVE [Liu et al., 2014], VoroMQA [Olechnovicˇ and Venclovas, 2014] and more, classical ML-approaches ModelEvaluator [Wang et al., 2009], ProQ2 [Ray et al., 2012], Wang_SVM [Liu et al., 2016], Qprob [Cao and Cheng, 2016], SBROD [Karasikov et al., 2019], a learning-to-rank technique [Jing et al., 2016], deep learning methods [Derevyanko et al., 2018, Pagès et al., 2019, Conover et al., 2019, Sato and Ishida, 2019, Jing and Xu, 2020, Hiranuma et al., 2020], neural [Wallner and Elofsson, 2003], and graph neural networks [Baldassarre et al., 2020, Sanyal et al., 2020, Igashov et al., 2020].
Graph neural networks (GNNs) for molecular graphs. In the last years, various GNNs were proposed to address the problem of learning on molecular graphs. Starting with the message-passing paradigm in the molecular graph domain [Gilmer et al., 2017], further multiple approaches elaborated on this idea [Schütt et al., 2017, Thomas et al., 2018, Chen et al., 2019, Nachmani and Wolf, 2020,
2

Sun et al., 2020, Klicpera et al., 2020]. All of them were designed to operate on small molecules. For example, in QM9 [Rupp et al., 2012, Blum and Reymond, 2009], a popular benchmark that is used to evaluate these methods, molecules consist of up to 23 atoms. On the contrary, a protein molecule can contain thousands of atoms, and this fact requires different approaches that take into account the size of the data. Recently, GNNs have also been started to be applied to protein graphs for solving various problems such as protein design [Ingraham et al., 2019], protein docking [Fout et al., 2017, Cao and Shen, 2020], classiﬁcation [Weiler et al., 2018, Zamora-Resendiz and Crivelli, 2019], and quality assessment [Sanyal et al., 2020, Baldassarre et al., 2020, Igashov et al., 2020].
Equivariance. Processing information in 3D must be stable against rigid-body transformations of the input data. This stability can be achieved using equivariant operations, which is a very active research topic, especially regarding rotational equivariance. For example, rotation-equivariant CNNs were proposed for spherical images using correlations on a sphere [Cohen et al., 2018] and then extended to fully Fourier-space architectures [Kondor et al., 2018, Anderson et al., 2019]. Similar architectures can be constructed for rigid-body motions using tensor ﬁeld rotation- and translation-equivariant networks [Thomas et al., 2018, Weiler et al., 2018]. Spherical harmonics kernels have also been applied to point-cloud data [Poulenard et al., 2019]. Alternatively, for some types of volumetric data, rotation-equivariant representation can be constructed with oriented local coordinate frames [Pagès et al., 2019]. The same idea can be applied to the protein graph representation, where the spatial relation between local frames can be encoded using spatial edge features [Ingraham et al., 2019] or additional edge descriptors [Sanyal et al., 2020]. For general molecular graphs, the problem is more difﬁcult. Still, there has been signiﬁcant progress using, e.g. the message-passing formalism with messages containing radial and directional information about neighboring graph nodes [Klicpera et al., 2020].

3 Proposed Method
3.1 Protein graph
A protein molecule is a chain of amino acids, or residues, folded in a 3D space. We construct a graph G of the protein molecule by splitting the surrounding space into cells using the Voronoi tessellation method Voronota [Olechnovicˇ and Venclovas, 2014]. Nodes of the resulting graph correspond to the protein residues and edges are associated with the pairs of residues whose Voronoi cells have a non-zero contact surface. Figure 1 schematically shows the graph construction.
Each node v of the graph G contains a feature vector xv associated with the corresponding protein residue. These features include one of 20 amino-acid types encoded with the one-hot representation, the solvent-accessible surface area for each residue, the volume of residue’s Voronoi cell, and the “buriedness” of the residue, which is a topological distance in the graph G to the nearest solventaccessible node. We represent the whole set of nodes as a feature matrix X ∈ RN×d where N is a number of residues and d = 23 is the size of the feature vector. To describe the edges of the graph G, we will use the following notations. Let A ∈ RN×N be the symmetric binary adjacency matrix of the graph. For any pair of residues v and u, the two corresponding entries of the matrix A equal 1 if u and v have an edge, and zero otherwise. In our settings, the graph G does not have self-loops, hence the main diagonal elements of the matrix A are zeros. In order to refer to neighbors of a node v in the graph G, i.e. those nodes that have a common edge with v, we will use notation N (v).
3.2 Spherical harmonics
Let us consider a complex square-integrable function f (θ, ϕ) deﬁned on a unit sphere S1. This function can be expanded in a polynomial basis using spherical harmonics as the basis functions,

∞l

f (θ, ϕ) =

wlmYlm(θ, ϕ),

(1)

l=0 m=−l

3

RA
Cα

N
C O
O C

R Cα
C O
N

N

'< l a t e x i ts h a 1 _ b a s e 6 4 = " i c 4 o M s 9 V s M 1 D Q U 4 Q 9 D y A a i c 9 q W y i E V F v s i + 2 b 8 Q Q J y V f 2 b Q W T f q B R 8 c k q F i T l F o J B c C 2 x l E e I k o 2 v p 8 g Y = " > A A A B 7 X i c d V B D N L S 8 s N A E F J 3 U 4 r W 1 L e q V t r q X V 1 j a 1 U O 6 g X W g x S 0 y V J C w 4 p F C 5 Z k C K l k 0 a t W T m 3 W Z s 3 X r g c e h O P e O F P y Y L B w R W d h M b L C a b G E S 8 P i p Z T m b 6 u L a 2 f Q m t d X 0 O b s p j 0 m Z H E h u m d 5 U 1 t m M C h o C h o f C T 0 7 / R 9 C X A i j D w y c o u V e V r P N / U z 4 v 6 f + C b D / u 0 4 8 O b e d t f 3 x O 6 + n D C u P i G + j 0 G 4 V 0 Y V e P V L T D w B R 3 w A O x 8 O c y 9 O 8 G 5 M W 9 O b z V m L M B v a Q f c l c f n E 5 U C s l a N n N b W S m W N 5 d z F a b 3 W s t 9 b g / K s Y 6 7 3 t x M r d p 6 2 v x 9 F m / 3 d Y a v P K D u b 0 3 x t v d H 7 2 x p d v Y v U P e o D K y G + S x w a m c h n 3 P g M E l k p C 4 7 P F I x J T D h o w 6 g W O V n W 5 Q S U B z x L a Q y A X l Z z l K P E N P U a c U S U 4 e U 8 V p R F 5 R M 1 U c E j h U 7 p B U + w Q 1 G Q w n F f L A D a W 3 D M 3 y t 2 e I x X L M C R q b 5 F 0 d U + x v p v E k j n C Y Z y m 6 O Q m r N t O I U b j s X x o a U X p L 6 I C I Q R P E x 0 a E I y b D N M T 1 o k O K O 5 V K N l k s t d E q J x 9 S H K q r R z F b t 8 J G q m I O P h 7 V n b L S r Z X 1 c r R u F q 0 k N n m G Z b n r Z J r T s 1 d r p d 1 B 0 J l q z O t 6 p E 5 W y 4 a n h X j X L U c V r 0 r 1 c j 0 S i q v Z 0 V u i c u 1 7 X 3 u J Y K u W b V K N 6 M N B M a t K o W r t g X w u 7 o U p v 6 r o T T V d e Z I A 8 S a / B D 3 Z Z 2 T L e o 7 N Q 3 b n 2 o H B F S j V n F + V J 7 d Q / J x R 9 o i V p n B w E j r e q G E V U K m X M h d y N t R N K l Z l x s J 7 Z 9 Y c h S p o 5 m R W m j d I h I y d K x F I k 7 o P o Z a p m N Z e V I a T m 7 g m q E S K z Q o w J g i w X H i 5 Y M w 1 0 7 j I Q X B + B 1 d I N G p O G Z Y a 8 q A t I o z 9 k 5 L a 2 P a j F E U u R 6 V g 0 N v M 4 1 0 q + D p 0 A U S W + O u D Y i G 6 K O W F h m l i c u y W r u g 3 4 6 i U Y Q L 6 y 9 x H P U Z 6 s C r 7 i c C S 3 Q c F H h / e I / G F y W u I j p 3 N m l C O b a r 2 i o 8 h K h / c / e i u 3 T V P T 1 x T U L V z 6 J W + D 8 T l 8 q o D v i M D j W C j J z S a y C m h d i P o E U / V k j k U s t 4 a G X M q i E Y 4 m c K J q U 5 R w l g Z N t V D f D M k O J c e H D / M J w 3 i z I h 4 r R V I f B C K C E Y s t J 2 M J 0 t o Q E L R m p h o W Q a x f E E W L Z t 4 Y E + 4 A h G + T J R 9 0 Q T g F 3 k j z V p P d h b A P t h I j f 6 u n h r 6 K V L 9 N / P u i 4 e e f q e / s Y E 0 r c C m 0 n m z A 3 X K d t Z B J 3 g s A O 6 B r D L J A t C c S H s h Y k C z Q T i N f B H Q M 4 A 7 A h C S 8 h c I D A j g R O 2 v E 4 U B g Q n i B u Z 1 D 4 c Z Q g S G Q q A b 1 C w H t A 6 3 A 8 A Y V P j 6 D c X 0 A b Z c a i e u 3 w W B K a m O u P R Z x G E t P / O f g q G B N k 6 W / + d G 0 8 T z b V h F N z 0 a B x P D P t x m g j c = P O = H w < 0 Q / z 8 l r Y a k L t e x s 8 x = p i < y t / p > l N a R t < e / x l i a t > e x i t >

Cα

✓< l a t e x i ts h a 1 _ b a s e 6 4 = " 3 / f U n p x v 1 w 1 8 o i B i / + / 0 p C L R c e y j N U B C M q R e 0 + 4 7 D s Q x z E B T V 5 W E d m Z i A s u 4 c / 5 1 J A c G u A t h 8 A w c x k Q = " > A A A B 7 H i c d V D B L N S g 8 N B A E O J N y z 3 N 1 U r G x d 1 h d q f X / U 1 q Y K h 9 O 6 e X 9 B w L o S B P B b g 4 B a W U d n 0 k Z h N D K i 4 a d m 9 k 4 J c O b x Y w I s Y A C v X L H j x C x w q H j Y M m W A 2 5 U l I k A Q I 2 Z s 2 i 5 0 e N 2 T J 7 Z M d M m v z b P s 2 B g w 7 5 U k l y Q e v o I E o Y J f S b / A 8 B n g i + x w D c F P V g K r 4 p / p 4 X E g f b 8 / H 4 / b 8 t O C 2 b L k f + F O j F E Z X k O 0 U N w V g 8 L o H S o h g W v o N h a B p j R l q V 5 p 3 Q q X S v R q b 1 4 j + A 6 7 s X H v Q g 6 6 C M D m E j z p v 7 r V 3 6 m V x p h u p Y b e X W W F + V p X 1 e t L W y b c s 2 7 e t u 2 m 3 d e v j s 5 c b B 2 m 9 t 1 f 7 v C J b o 7 + q u Z 5 7 3 J V d M d h U Z o u Q o b o S R x S R m V C s m S s d y k R F p i x F K D q S F + L B l Z I R 8 + z o Z K J B U n x J j w e I E Q 6 4 s 6 G B W S B N s k 2 F S P Y g F s Q a G 7 e Q D B l a L 8 Y 3 w X v Z i B r 7 H + 8 5 + 2 z G 3 O z s / m c N F c 6 U O + / V Z i F V G l D 5 F w i 4 K l h L z e 2 I O E / Y S I d s O w 2 O I 5 Y 6 A h C h p E i w V E S f 0 M c 1 I E B p 3 G A q S h r 4 A d 0 E x O Z y j V J B m F i u 2 Q W s X 6 z r T n 5 p H g 2 9 W o k 5 + p 7 l u r g o l y V M 7 g o T i u d j t C u i 7 1 3 f 6 H p J O F 1 / q 7 b T R K m X c 5 k h N Z 1 C K s 2 F m S c 1 s / a R F B 1 j G 3 m l b V q m n h y K M J 6 2 y 9 + Z m D k C 0 5 f Y v x J A 0 z a 5 S D t I a Z W z 6 A b + x 6 V 1 f E e O q x / b F 2 5 L I t A J p h g e 4 E D i L n k g R 1 Q A T q K r i n l V g u N s v u 0 E x r 2 Y g J + 0 l i y Q m x h R l Y A N J K A J n p P g G s k b 2 3 O 0 N + E T p 8 q j 1 J G j Y J y T m k O P Z i L 0 Y z w D 4 F t u l o G b 5 h 0 P j 5 t T A P g E K u T C j H I O X e Z T J p L 1 5 O B 2 0 U S U D I H L 6 2 t P q T 0 l v S j E D w z 9 e m S R J p 1 l k B J I U p k C F c z g / t e X T 6 7 v M x E C I y E Q k z G J U W l b o J 2 8 o D E h 3 v / 3 u Q 4 X G M 1 F C M I w f z h 6 U d t b P z 2 + c 8 d T m x / m / u g J W G a 1 L C E 6 f x X a h J 9 4 R z 3 m k Z A S k E H M c C Y Q V I 6 8 s Z p j I P k t N d F c / g M U 0 g Q R p S g i j i p M P g j D m s h d k N u I z t T n b i k M j l Z O E 0 U Q Y E S 0 S M 0 j o I U l g 8 V C J x o k n 6 J x h W X K x R 9 I E P F J 8 W K f V G / o g E H q p 2 5 Q T s x q t T J 0 / 3 7 i x 6 6 d 2 l i a P v u x 8 3 P n 6 a 9 K n a o m J 0 7 V y b s O L Z k u w G X L n T u 6 q 1 B 6 A z S t Z h o y w j E X l F S y t c a d A B I C 8 U H d 6 c o y A C E w P M u k U l I 3 K B a A d E C V H F 0 9 1 T I C m S F q e G o j j T c C q R 4 i h 6 N j A F t r 6 4 d M g o 1 E D 2 c t V r 0 s b y u 0 V 6 T j t G N J r H + f K t F M 5 g f 0 P M Z M N q 5 4 y b + P F A m 4 a f z G 2 P T 4 m j Q T 1 e 3 E s 0 = l A < w 8 / / b l 5 z a X B t p + e E l x O 5 i < k t / o > l I a = t < e / x l i a t > e x i t >

R Cα
C O

R R
Cα N

B N
R Cα

C O
N C
O

R Cα

v<latexitsha1_base64="W v O+ P JI B Ey I 3Y M u+ w 6k R YD s fy K Oo h LU A zy J ED 3 Vi o h1 2 vW B 8j W LX b fh X 7U O Nw K y2 0 TA em i w8 k ZH M zI E o=">AAAB53icbZ VC B7 NTg s 8J NB AF EI E Jb X 3P H i4 Fg R +3 X Ix C Xh K 6l z tr w Ha 6 LT G RC o mQ s Lm I iV im S bT q RX y eR a 1q Y B0 C uk K Js S Fb B 4G R +E Qx J WB h CU I GS m BI k hG R mR I y2 r XO W KA m as / xj G ss y t7 Z 8O L 3b 1 um 2 lV t ok p QS d 03 R /P w AA p IE W vN v Hh o tZ C Rq G 4b A 9H h S0 A 9H t 5n P 88 0 9T D +O F 40 l Qj 9 Ad B 8x R Ku 8 vB i mQ d SK s S/ H ls g /k U dk E mX r M/ r j7 T M/ S vn 0 TM b Aw 0 X5 z XJ 2 x0 p ng k WE J /1 E n8 s dZ 6 L1 U Gv d 55 p tz x bc P 2y K Tu 7 nr e ma w 3+ u sk L rZ S d+ 8 /s k c7 l HC 8 h1 t Uv r Pb K TO 1 57 v 5V b 19 G Ew 4 m/ V mu t GN 3 PN d ox u sq V Eh J Yj x n6 K qL i hB h Fa 6 Sx N ja e 4g S BR w JU b 9o A w+ V 4A H 3S I Af m Tc U qO B qN P Qw M xE 8 qa 2 Hi x Ak dE U ja g iB k +w U mH c /o C vw x tu H CJ v S7 Q rk v N9 x ES n /E lq 9 gz g pW F iN K k5 x Ga W M0 F RY z 1J r Kt Y Hi Y nP J Fa + Gk R jz L Zz q Vk C aj h kB Y 3p w 6r S 11 b 5Y a ta z bd aY o Bs F ck 0 gt s 6u O 81 W QO V pR n SZ I gf v wD s Lm v NU u fL D vr M W6 o re n NP b 0i / hu f YA f FE V qC 2 M1 + 0U 7 T/ W Fx X Cs V td uW v 5O G 6W j bR 1 mi Yg l CN p nE G y1 q nT H Ar Q mp l cu B Fc O bm l pp m Zp q R1 6 pR TZ a yT D sg 8 ZT j 0O U iC j F6 P 10 K LU c Jo V Y0 R 1J o RZ p BQ /P o na T iw Q 06 v BZ u mF k 5S i sS 0 MP 2 qU D A7 g RW k Iw S m6 o y6 / JJ G Qi w 1c y ZW 6 qK M Ld g 8L + nw N cl E hj 7 pZ H rJ D Pw m Y2 N 1Z p Du n 2r t x8 D l7 2 TM xh P 9p 3 Kp Z rP k 3Y Z lo F zC I 8W q zx W +l E tR U m0 m J9 M re q oL I J2 6 cc J iT 6 78 a TL z2 8 Ku b Bm kJ f yr 1 0x k Vo z RZ 1 J1 e ow G hm p Jq n yU z PH G xJ R rZ p Mh B u+ o AF Kq n mS X RA 4 Fm U TJ p Sp t yO z ht W TS s 3Z T Nc 3 5r e KZ 2E u Ia 4 gM w qL i yF V oC T zm z Nu o pJ Q m2 F JV C Ds J 8D T F5 O ZV z fl 2 XB r il R f7 H +m J V4 K fI H 29 a 2g 3 7r Ke Z X4 g c8 j Wj u sL D M4 O tZ r 0+ / iX w jL X Ds v Gl O Zd P zz y DS W Jx d XU m hX t wZ O Ds a Qr W 2D K 4E A hR 1 yz P bD l 4K Y wX R Ah 8 Dw O hD 4 Gh A VW h 74 c hg O zS I Xr E l4 K yw XA E pD E xh V 3E P 5Z K 27 C Ph A Zx c Wb A nl + K3 P Kn 8 mp G Vx T PX d 45 W A2 g +1 / fW zm s Bn / 0P U rm y vP L jY c Lf 1 gw Z =R s <8 5 /7 5 l7 d aD + t7 C ex XL r ij 9 t0 R >Y t= s< P/ pl Ca Kt <e /x li at> exit>

xv < l a t e x i t s h a 1 _ b a s e 6 4 = " t C n j w A h w C U G V y c B P r p 8 D g 7 B R 4 e 6 n Y M b D 2 G M P O S T X V 0 X b T J p 3 n v B a 7 i u r 7 u s O u 4 p T V 2 X f Q F t p J u O o E c = " > A A A B + H i c b V C 9 T s M w G P H x T S K / X k y r l 5 / C A y S z C Q x W y s F W E o F R s U K I S i T E a F x l X V K C w W A g I m J C w s t V F K L S g I y t x M j F r K I U x r R F o a S p q G T Y 0 Y 0 W K i O j x 4 u 3 7 O F R 2 a W V q n h 4 T 0 4 i 7 d y s 2 n p U Y 2 5 o K F q K F 6 u f p q V u b N w s W M D B A A g B A i Y M 5 u b V V D H R w Y 2 D e F D B h w u 4 B c F G N p w g y v O 2 P 0 A g n 7 t G S N T c 2 5 Z g d P J l a d 0 T 9 9 L 8 3 J v 3 / n y u C + v j a k D K 8 O U + l U X H a 5 e V g f d y b 9 q 8 r q s T 2 q j t r f b a F 2 y m x u l r l 1 W d b + W c U 1 b N 9 O y Y 7 t 3 b y 7 2 Z + z m w u V f 7 r 2 e 4 / 2 d b d G + 3 j w z E b 9 r 0 4 k S / k m u 1 c F C T M O E i C x k C 4 5 9 I h k J 4 L 2 W s Y D a 6 A K Q h c I O p o y d x F T y S T 4 h T m F H u F P q O a G P y W E y m 0 T n 1 F k I S q 5 c A 1 h k U p Y E N q p x Q S z V E f D j l W L n 8 T 5 K D 3 v 4 Q z X q U X V i h U t I l 0 n H d 3 B E Q 7 K 0 / i 4 Q r z 4 6 J r C U R G 6 C n m + x L p E G z 9 Y 2 Q E K n a E 9 y b O M a Y F S Y t K q j F S 0 t N I d F B y N Y P h / B 2 U Y L N j x D V Q F O s X z U 8 J u T U P Q / E L T X J s P o J n H R 9 d a J d F B f g r d F O m d H j t G u W i F p V s O u 0 V B y P V 8 A p O Z Q a e o n L V W O j S Y 5 b H r V 8 f + N D 1 I w P k D q H Q m y C b h O 4 K 3 2 a P a b Z c i m K x B w / U l 9 3 h i 3 C K U u B y M s 7 U 4 R N R d K w Y d j a T R k w l Y 3 S 4 1 q n U u T G s U O 6 G p q u K f a a Z I Y z Z k J U D m m X M l F 8 m j p y E H m w 1 S q D I v U j U 5 x y E R Q f F 5 d O R I E f x h z l 6 5 K p D O N E u 0 q o Z C R p C w Q f l I J R x p P U 8 n K 4 5 a p N k 0 P 2 a k o J E 0 n V Z h 3 6 o 9 Q l v 5 g F n r D D G h t Q R T 5 Z N n T X A g f N z G p m z + W l r A K v 9 V j U B R s H w t O l e T q K C g f Z h 7 I n D J 9 9 h X S O M i k d V B X 4 2 / j r k / R F e K S Z 8 7 1 T N M / c 9 v 0 y G 5 6 Q D m s n 4 H P 0 o I s p 0 / z 0 p j 4 z r T z R j N A 2 N Z U O Q O F 0 9 4 o j 9 Y D l p l C I a c S O M z E a U g e N F G T L Y k G C w q K l A Z w E y Z Y q U r N E I D a C U z E F Y t Y 2 M U B t p G C P F m V J r w T Y Q V U M a p k I w E B l d 0 7 a g + m i 8 r S I E r o 1 y p Z L w F 1 V n v O X 8 C L 8 u v j / X L j e x l a z Z 7 2 e Z t K u X d q N 1 j a 6 T 9 5 q p q c y b w r s C i u m G U c G 0 g c Q w p A u w z X D l 0 C M I d E Y w x 7 B O 6 h A I 5 F A c n H Q w B B A C 1 U b u X w o D Q A K E 5 / + u w Y Q Z B h j N X B 4 W E A i 7 I / z M V A R u q e f 5 A S V B x b P G z 4 a 9 N 1 a n b K 1 9 r c T U w 4 f x b r / L x Y V X H n q 3 z d + n j A U Z M P a r 4 s + A + k + c / v 4 M j h = B + < 1 A / f P l A r a l / t d Q e g e x = n i < R t / Z > l e a U t < e / x l i a t > e x i t >

u<latexitsha1_base64="B x q5 x Uw f m8 N CJ V QR v 5v 8 GJ p 4I b 3N P Mg x Hm f 3W i 6l G ZS x kT N iK 7 mn c fr P 5C p hZ z xY D kZ Di N jP 4 Ec m RY 8 4=">AAAB53icbZ VC B7 NT Ss g 8M N Jw B AF EI JZ b 3P iu x FJ l +Z u Iy M XK t 6z 3 tC j Hy p LW b RF A mR a LI D iT Y iF B bX V RC 2 eA b xk d Bx T uU K JY g Fm I 4F W +s W QJ C WU b CI g Gr m Bt k hV C mH x yl h Xu d KC n ae I xt 2 sq G te T 8N 3E 7 ut Y 1o WN Z QU W 0o i /j E A4 u IB e vC w Hw M tM Z Rg C 4V x 9t d S6 Y 9B X 5J s 82 P 9F J +j J 45 7 QE C A1 x 8w 9 KL E vw y mO e S0 X S/ Q lJ h /K N dl / mT G M/ P j9 j M/ 4 vj / Tn 3 Az O XO Y XC c xV 4 nP 6 WB f /t C nX K dH 6 Ld 0 GL b 52 X td 9 bp Z 2e u TW Y nV X m1 F 3b p sb e r2 W dw V /U / cd G Hz p hc h U2 b Pt X T7 1 5Z j 5L c 0e 6 E3 u mt 4 m3 v G+ X Po Ok j sU 4 Ew l Yx Q n9 y ql d ho F lh k SE s jN Y 4U t BO n Jq w 9U q wX U 4C L 3J B Av I bu 3 qF Q qG 1 QY 1 xD w qN I HV b AS i TO U jN Q iQ a 5Y + nC g fM L uc r dX f JI v 13 9 Sz i ax l Jj N /0 f Lq v ez U TR C FN o M5 e MY R4 9 jY d qp 6 SB k Pj K OH A Kt X MS 0 GR m i5 7 ux E 1R A so 8 02 6 G1 o6 N 5l tm Y bn t dV b BH R cY d gr L 67 d 8k t QR k pk e SE i gb 8 ww y KZ D tl M QC 4 f8 X W/ S rH + Pr c 06 f xr 9 YA 9 FF + qD X Mr 7 0l X TD l F7 Z Cb t3 F eY z 5R 9 6l bM n mU Z ir i CD l nB I yN U nW a A6 a m5 C cb a Fm p bp UC 0 ZX 7 xK E pq TD 7 yG W ic V ZC U 0R a h8 s DV 4 12 E Lp D Jj g YG u 1l t Rb V BE G /B F n7 C i2 W 0L Z BI 9 mo 2 5a sY W Mw k q6 w Qy o RC i Ie E mD q yj L Js x Qi s 1x P Zd O qb i Lo S 8k H nS x cp u hR m p9 Q r0 I Pp J YC b 1J m D+ R 27 Z xs q lj M Tp 3 M7 d 9H 8 aW d rw G 3z Q li 2 z0 V 8l G zT o +E S t1 + lf q JT Q m2 y of p Ej 7 Oc q Z3 n d/ Z ps b Zl GZ T Cm + yo l 5v z aM V Ig T o5 H Ez Z 8L x QN 6 kD G ZE Y Po + 42 S 1/ V GS G Xj P KK E FB J zD h IE 8 iJ F pG q JW S Z9 A QN 6 pu J bl q mw O 8h t lM S b2 Y EJ d wo L Vg ZT F cL o ZF M m7 D Ua F 7y A EE m h9 u ea Z Km si V vz s rN B xj 6 Pb V /F l qO G n0 l 5R z Tv m 9P 4 9m I pV 5 uF g r8 j eE O k/ 7 Wr 8 aV j Zx y TU 4 hv x DL + Mp W 7b z hr s Er l jo O yw z 4V S hQ x iE U bO b c4 J QQ sh r tO D 8w P YI hM z Dz A wq E DM T KI h /1 w1 A 5M h jA W wH 4 6B g Lg i 8i q 6P 4 78 wA 7w D Fv h sz A Lp Z T3 7 nz g F5 2 zL b Cw q n6 1 8b H g9 q fP 0S X 5J 6 AW 3 0f V lW S sc m jw r LB O c/ m =5 P <L b /z v l/ w aA R tL 9 er b xI b ij D t0 2 >U q= 7< k/ Il ka =t <e /x li at> exit> xu < l a t e x i t s h a 1 _ b a s e 6 4 = " O i r N s + P A z S 6 g 0 w j B Y q G c n D r 0 l E o z F L v t f Z z G Q F T u b j f 5 i P l h R 9 1 I o H p E Q 7 M 0 c L 4 t s H h T I 6 9 h F t J s Z E s = " > A A A B + H i c b V C 9 T s M w G P H x T S K / X k y r l 5 / C A y S z C Q x W y s F W F o h s R U K I S i T E a F x l X V K C w W A g I m J C w s t V F K L S g I y t x M j F r K I U x r R F o a S p q G T Y 0 Y 0 W K i O j x 4 u 3 7 O F R 2 a W V q n h 4 T 0 4 i n d y s 2 n p U Y 2 5 o K F q K F 6 u f p q V u b N w s W M D B A A g B A i Y M 5 u b V V D H R w Y 2 D e F D B h w u 4 B c F G N p w g y v O 2 P 0 A g n 7 t G S N T c 2 5 Z g d P J l a d 0 T 9 9 L 8 3 J v 3 / n y u C + v j c k D K 8 O U + l M X H 6 5 e U g f d y b 5 q 8 r q s T 2 q j t r f b a F 2 y m x u l r l 1 W d b + W c U 1 b N 9 O y Y 7 t 3 b y 7 2 Z + z m w u V f 7 r 2 e 4 / 2 d b d G + 3 j w z E b 9 r 1 4 k K / k M u 1 k F C m M O o i C R k C x 5 9 K h k e 4 L y W s H D a a A K I h c F O p e y d V F T M S T U h T E F H 8 F P z O a z P y W E y k 0 T 7 1 F l I S R 5 c T 1 h H U p I E N a p x e S z t E f c j l H L n h T 5 T D 3 + 4 Q K X q 0 X V R h i l t I Y 0 n o d 3 l E Q 4 K 0 k i 4 6 r z P 4 6 U r C + R r 6 H n m u x L C E G 9 Y Y Q E j n a x 9 y G O M s Y F j Y t R q j T S 0 Y N I d F B j N Y d h / M 2 U e L N K x D T Q F G s X s U 8 b u T n P Q y E L x X 8 0 P s m n B Q 9 u B J O X M i b g 1 V C n q u D T + r l r R T U K o n 3 G C I X n o i g 0 z M o k n U m H I 1 n f p s t F L T o 8 8 / G f a k g n S C 3 B A T 5 R t Q m 8 l B Z N / H N d M O q 7 F O A a E / q Z e 7 D 5 z E H q V A Z m W 9 n U g C K s s 4 V x N M g S d w O 1 p c U 1 z u X h 3 W q n d D W V X G P N p s E G 5 M O k J p z 1 p E U i e u R 0 p i r m a 6 i V h K X S q U Z 6 D I o 3 i 8 K v o c A t d Q 5 Q 9 8 w U h W n k O X 6 q U M f I 0 L 4 q z S E a o Y f v i I x H J 8 N O m S j n T V w W K C a h T q H 4 7 q 0 8 J S 3 N w C E V p d h y o I N c M F 7 p V i y / G Z b U T + / L Q X A 4 3 e V R q k o W U 4 W 4 S v M V F x Y P l Q / j z P k + V w r C n m L S O X A r i 7 / H X y + o C 5 F p 3 e V U J m y / u 3 X a b j U v f o 0 T W c 8 D y a 8 b W U S f 5 T U R F 5 1 N 5 q B m g Z m s g n / A Y 1 8 U M e o y U 5 j m 0 O J g G 5 o N l o Q N a C D U F M M j J Q X H k A M J i Z Z E q V P C k D N h c R C F s U E b m M g 2 p r M C T V k k J M Q i C G v W W E 4 m A y G y 1 h Q 9 T R e V 1 J s q d W a 5 U s F 4 i 6 C i 7 5 n r + B l e X 3 Z f j l z v 4 y N 5 M 9 7 v k W H 3 r O V m a R t 1 h s c 1 0 1 O Z t 4 V 2 B z X T D K O D a w G I Y U A V 4 Z L h y a h u M M A 4 Y d A n d x I M c A u z D o 4 A A I h 6 I N 3 L 4 0 U h A l A c d P 3 4 C o s I A w z k r g 8 r C g x V M X 7 E q g I 3 1 P c I h q A a t n j Z 8 N + G a v z N l a e 1 O p q Y + P Y N 1 / l Y s q r D z 3 5 5 O / z R A i k M H v l 8 W 7 g f k f O / X c I x Q = A / < 1 I / Y H l 9 1 a l / t d g e c O x = l i < w t / p > l e a T t < e / x l i a t > e x i t >

Figure 1: Schematic representation of a molecular graph. (A) 3D protein structure is partitioned into
Voronoi cells, shown with the dashed lines. The central amino acid has the associated coordinate system, which is built according to the topology of its backbone (atoms C, Cα, N ) with the center at the position of the Cα atom. R symbols denote amino acid residues. The spherical angles ϕ and θ of the neighboring residues are computed with respect to the local coordinate system of the central residue. (B) Graph corresponding to the Voronoi tessellation, v is the central node, u is its neighbor, xv and xu are the corresponding feature vectors, which are also shown with colored boxes.

where wlm are the expansion coefﬁcients, and Ylm(θ, ϕ) are the spherical harmonics [Hobson, 1955],

Y m(θ, ϕ) = (2l + 1) (l − m)! P m(cos θ)eimϕ.

(2)

l

4π (l + m)! l

Here, Plm(cos θ) are the associated Legendre polynomials [Hobson, 1955]. We should also note that a real function on a unit sphere can be decomposed in a polynomial basis more compactly using real spherical harmonics as the basis functions. Below we will be using this real basis, which is speciﬁed in Supplementary Materials.

3.3 Local coordinate system
The protein backbone consists of atom repetitions C, Cα, N , O. This allows us to unambiguously associate each residue with a local coordinate system. Indeed, for each residue we can deﬁne the normalized Cα–N vector as the x-axis, the unit vector lying in the C–Cα–N plane, orthogonal to x, and having positive dot product with Cα–C as the y-axis, and the vector product of x with y as the z-axis. Then, given a node v, we can associate each neighbor u ∈ N (v) with a pair of spherical angles Ωuv = (θvu, ϕuv ). They specify the angular position of the projection of the node u onto a unit sphere in the local coordinate system of v. An example of a local coordinates system is schematically shown in Figure 1A. Now, having an unambiguous orientation for each node in the graph, we can construct a rotation-equivariant convolution operation.

3.4 Spherical convolution

We can approximate the expansion (3) of the function f (θ, ϕ) by cutting the series at the maximum expansion order L,

Ll

f (θ, ϕ) ≈ fˆ(θ, ϕ) =

w

m l

Ylm

(θ

,

ϕ

).

(3)

l=0 m=−l

The same approximation can be obtained for a matrix function F : S1 → Rd1×d2 , d1, d2 ∈ N,

Ll

F (θ, ϕ) ≈ Fˆ (θ, ϕ) =

W

m l

Ylm

(

θ

,

ϕ

),

(4)

l=0 m=−l

4

where

matrices

W

m l

denote

expansion

coefﬁcients

of

the

function

F

in

the

Ylm

basis.

Finally,

we

can introduce the spherical convolution operation for the vertex v in the following way,

F ◦v=

Fˆ

(θ

u v

,

ϕuv

)x

v

.

(5)

u∈N (v)

Considering

matrices

W

m l

to

be

optimized

parameters,

we

will

thus

learn

a

spherical

ﬁlter.

We

should speciﬁcally emphasize that matrices Wm l are rotation-equivariant by construction.

3.5 Neural network

The distinctive feature of convolutional networks built on spatial graphs is the way the graph nodes exchange information by passing messages to each other. On each layer of the network, nodes’ feature vectors are combined and updated using the information from the neighboring nodes [Scarselli et al., 2009, Kipf and Welling, 2017]. In our implementation, for the information exchange, we use the proposed spherical convolution operation (5).

Let AΩ ∈ RN×N be a matrix of local angular coordinates for each node’s neighbor in the adjacency

matrix A. This means, for any pair of graph nodes v and u connected with an edge, the corresponding

entry

of

matrix

AΩ

is

a

pair

Ωuv

=

(

θ

u v

,

ϕuv

)

of

angular

coordinates

of

u

with

respect

to

the

local

coordinate system of v. We also denote Ylm(AΩ) ∈ RN×N as a result of the elementwise application

of the spherical harmonics Ylm to the matrix AΩ. We should note that the main diagonal elements

of matrices AΩ and Ylm(AΩ) are zeros, and, opposed to the adjacency matrix A, matricesAΩ and

Ylm(AΩ) are not symmetric. Then, the kth layer of the spherical graph convolutional network can be

expressed as follows,





L

Hk = σ 

Ylm

(A

Ω

)

H

k

−

1

W

m l

+

H k−1W

+

b ,

(6)

l,m

where Hk−1 ∈ RN×dk−1 and Hk ∈ RN×dk are nodes’ feature matrices before and after applying

the layer, H0

≡

X

is

the

input

feature

matrix,

W

m l

∈

Rdk−1 ×dk

and W

∈

Rdk−1 ×dk

are trainable

parameters, b ∈ Rdk is a trainable bias vector, and σ is a nonlinear activation function. If we

let the maximum expansion order L = 0, we can see that the operation (6) reduces to a standard

message-passing form,

Hk = σ

AH

k

−

1

W

0 0

+

H k−1W

+

b

,

(7)

where for each node v, the ﬁrst term transforms features from v’s neighbors N (v), the second term transforms and aggregates features from v itself, and the last terms provides a bias for the activation function σ.

In Supplementary Materials, we also provide a modiﬁcation of the proposed spherical convolution layer (6), which explicitly uses information about contact surface areas between Voronoi cells, and discuss the corresponding network architecture.

4 Experiment
The main purpose of the model quality assessment task is to evaluate the deviation between a generated model of a protein molecule and its native, or target, structure. If the target structure is known, the quality of the model can be calculated by computing one of speciﬁcally designed metrics, e.g., CAD-score [Olechnovicˇ et al., 2012], lDDT [Mariani et al., 2013], or GDT-TS [Zemla et al., 1999]. Most often, however, experimental protein structures are unknown, and thus there is a need for protein structure prediction and model quality assessment. In this section, we report the results of the protein model quality assessment task obtained by our spherical architectures and the GCN baseline. We trained all of our networks using local per-residue CAD-scores as the ground truth. They have been shown to be a more informative and stable metric compared to other MQA measures with respect to local structural perturbations of a protein molecule [Olechnovicˇ et al., 2019]. To retrieve the global per-model scores, we averaged the predicted local scores.

5

We provide results of S-GCN along with several state-of-the-art MQA methods that are described in detail below. We would also like to emphasize that for several reasons, in this work, we do not compare S-GCN with recent GNNs designed for small molecules. First of all, we attempted to train tensor ﬁeld networks [Thomas et al., 2018] and DimeNet [Klicpera et al., 2020] on protein molecules but did not get any adequate results. Secondly, we are unable to test S-GCN on established ML benchmarks such as QM9 [Rupp et al., 2012, Blum and Reymond, 2009] due to the speciﬁcity of the graph representation in our method.
4.1 Datasets
For our experiments, we collected data from the Critical Assessment of Structure Prediction (CASP) benchmarks [Moult et al., 1995, Kryshtafovych et al., 2019]. They contain experimentally obtained native protein structures and the corresponding 3D models predicted by the CASP challenge participants.
For training, we used data from CASP[8-11] stage2 submissions. For each target, we additionally generated 50 near-native models [Hoffmann and Grudinin, 2017] in order to enrich the training dataset with high-quality examples. For validation and selection of hyperparameters, we used data from CASP12 stage2 submissions. All models that we used for training and validation were initially ﬁltered and preprocessed. More precisely, we excluded targets that had only models with low CAD-scores and preprocessed all models by removing residues that were not present in the target structure. More details and the list of all targets are available in Supplementary Materials. In total, we had 333 target structures and 73,418 models from CASP[8-11] for training and 39 targets and 5,411 models from CASP12 for validation. Finally, to test our architectures, we used unreﬁned data from CASP13 (73 target structures, 10,882 models) and unreﬁned data from CASP12 (38 target structures, 5471 models).
For each model, we precomputed matrices Ylm(AΩ) up to the 10th expansion order. These matrices were the most space-consuming part of our dataset, as a spherical harmonic expansion of order L requires the storage of L2 coefﬁcients for each pair of adjacent nodes in a graph.
4.2 Metrics
For the evaluation of the methods, we chose z-scores, MSE, determination coefﬁcient R2, Pearson, and Spearman correlations, as it is described in more detail below. We used global CAD-scores as the ground truth for the assessment 2 . We computed z-scores for the top-predicted protein models for each target and then averaged them over all targets, as explained in more detail in Supplementary Materials. For the MSE, R2, and correlations, we used two different ways of calculation: per-target and global. In the per-target approach, we computed the metrics separately within each protein target and then averaged results over all targets (we averaged correlations using the Fisher transformation [Fisher, 1915]). In the global approach, we stacked scores of all protein models into one vector and calculated the metrics on this vector. For each metric, we also computed bootstrapped means and conﬁdence intervals. For the global metrics, a bootstrapped sample is chosen from the whole set of models. For the per-target metrics, a bootstrapped sample is a sample of targets and their models, respectively. These results are available in Supplementary Materials.
4.3 Baseline architecture
For the baseline, we built a standard graph neural network based on the message-passing operation described in eq. (7). The structure of the proposed architecture can be split into three main parts. The encoder is a set of fully-connected layers that transform the residues’ features into a high-dimensional space. The message-passing part is a set of graph convolution layers (7) that capture the structure of a protein graph and work as a feature extractor. Finally, the scorer is a set of fully-connected layers with a sigmoid at the end. They form a multilayer perceptron and use the obtained features to predict the scores of each protein residue. As a result, we obtain three main design parameters – the number of encoder, message-passing, and scoring layers. We performed a grid search on the values of
2Although we mainly focus on experiments with CAD-score as the ground truth, we also evaluated the quality of our method on the same data with lDDT and GDT-TS as the ground truth. The results are available in Supplementary Materials.
6

Table 1: Architectures of our baseline and spherical graph convolutional networks. SCL and GCL are the spherical and graph convolution layers, correspondingly. FC is a fully-connected layer with ELU activation. The parameters in the parentheses are the sizes of the input and the output feature vectors, correspondingly. BN is the batch normalization layer.

Network Baseline
S-GCN S-GCNs

Architecture
Encoder: FC(23, 32) → Dropout → FC(32, 64) → Dropout → FC(64, 128) → Dropout → Message-passing: GCL(128, 113) → Dropout → GCL(113, 98) → Dropout → GCL(98, 83) → Dropout → GCL(83, 68) → Dropout → GCL(68, 53) → Dropout → GCL(53, 38) → Dropout → GCL(38, 23) → Dropout →GCL(23, 8) → Dropout Scorer: FC(8, 16) → Dropout → FC(16, 32) → Dropout → FC(32, 64) → Dropout → FC(64, 32) → Dropout → FC(32, 16) → Dropout → FC(16, 1) → Sigmoid
SCL(23, 20) → Dropout → SCL(20, 16) → BN → Dropout → SCL(16, 8) → Dropout → SCL(8, 4) → BN → Dropout → SCL(4, 1) → Sigmoid
Spherical part: SCL(23, 20) → Dropout → SCL(20, 16) → BN → Dropout → SCL(16, 14) → Dropout → SCL(14, 12) → BN → Dropout → SCL(12, 8)→ Scorer: FC(8, 128) → D → FC(128, 64) → D → FC(64, 1) → Sigmoid

these parameters and found out that the optimal architecture had 3 encoder layers, 8 message passing layers, and 3 scoring layers. For each layer, we used the ELU activation function and the dropout rate set to 0.3, as we detected it to be optimal. In total, our baseline network contains 339,053 trainable parameters. Table 1 brieﬂy lists the ﬁnal architecture.
Training. We trained this network on CASP[8-11] datasets for 40 iterations. We tuned hyperparameters on CASP12 (preprocessed) dataset. For training, we used the Adam optimizer [Kingma and Ba, 2015] and the Mean Squared Error (MSE) of local scores as the loss function. On each iteration, we trained the network in 4 parallel processes feeding 512 models to each process. One training iteration took ≈21 min. on Intel®Xeon®CPU E5-2630 v4 @ 2.20GHz, and ≈1 min. 10 sec. on NVIDIA GTX 1080 GPU.
Hyperparameters. The learning rate was 0.001, the batch size 1, the dropout rate 0.3, and the L2-regularization coefﬁcient of 10−5.
4.4 Spherical graph convolutional network architectures
While constructing a spherical graph convolutional network, we considered multiple expansion orders in the range from 3 to 10 and ﬁnally chose orders of 5 and 10. We also experimented with the number of layers, the batch normalization layers [Ioffe and Szegedy, 2015] and the batch size, dropouts, the regularization parameters, and the output dimensionality of the spherical convolution layers. The details of these experiments are available in Supplementary Materials.
Finally, we settled upon two architectures. The ﬁrst architecture, S-GCN, represents a sequence of spherical convolution layers (6) combined with dropout and batch normalization layers. In the second architecture, S-GCNs, we added three fully-connected layers to the end of the network following the idea used in the baseline. S-GCN with the expansion order of 5 contains 24, 675 trainable parameters, and S-GCN with the order of 10 contains 95, 475 trainable parameters. Respectively, S-GCNs with the order of 5 contains 42, 625 trainable parameters, and S-GCNs with the order of 10 contains 137, 725 trainable parameters. Table 1 brieﬂy describes these conﬁgurations.
Training. For training, we used the Adam optimizer and the Mean Squared Error of local scores as the loss function. We trained the networks on the shufﬂed data and split the whole training process into equal iterations. Within each iteration, we trained each network in 4 parallel processes feeding 2048 models to each of them. We stored and processed the adjacency matrices in a sparse format. One training iteration takes on average 10 to 20 minutes, depending on the order of expansion on Intel®Xeon®CPU E5-2650 v2 @ 2.60GHz, and ≈12 minutes on NVIDIA GTX 1080 GPU for the 5th order S-GCN. We trained 5th-order networks for 40 iterations and 10th-order networks for 60 iterations.
7

Table 2: Comparison of S-GCN and S-GCNs with the baseline network and the state-of-the-art MQA methods on the unreﬁned CASP12 stage2 dataset. Parameters in parentheses correspond to the order of the spherical harmonic expansion.

Method
SBROD[Karasikov et al., 2019] VoroMQA[Olechnovicˇ and Venclovas, 2017] ProQ3[Uziela et al., 2016] Ornate[Pagès et al., 2019] VoroCNN[Igashov et al., 2020]
Baseline S-GCN(5) S-GCN(10) S-GCNs(5) S-GCNs(10)

z-score
1.282 1.410 1.670 1.780 1.871
1.025 1.704 1.665 1.609 1.303

MSE
0.961 0.051 0.035 0.007 0.007
0.011 0.010 0.005 0.015 0.006

Global metrics

R2

Pearson, r

−81.899 −3.426 −2.036 0.424 0.370

0.552 0.675 0.795 0.813 0.818

0.065 0.157 0.573 −0.272 0.492

0.658 0.854 0.812 0.872 0.803

Spearman, ρ
0.531 0.700 0.806 0.805 0.803
0.666 0.831 0.789 0.853 0.790

MSE
0.961 0.051 0.035 0.007 0.007
0.011 0.010 0.005 0.015 0.006

Per-target metrics

R2

Pearson, r

−427.838 −19.762 −16.572 −1.101 −1.380

0.762 0.803 0.801 0.828 0.817

−2.641 −1.890 −0.831 −3.866 −0.917

0.677 0.797 0.710 0.816 0.738

Spearman, ρ
0.685 0.766 0.750 0.781 0.774
0.604 0.738 0.680 0.762 0.683

Table 3: Comparison of S-GCN and S-GCNs with the baseline network and the state-of-the-art MQA methods on the unreﬁned CASP13 stage2 dataset. Parameters in parentheses correspond to the order of the spherical harmonic expansion.

Method
SBROD[Karasikov et al., 2019] VoroMQA[Olechnovicˇ and Venclovas, 2017] ProQ3[Uziela et al., 2016] Ornate[Pagès et al., 2019] VoroCNN[Igashov et al., 2020]
Baseline S-GCN(5) S-GCN(10) S-GCNs(5) S-GCNs(10)

z-score
1.453 1.369 1.459 1.403 1.516
0.865 1.362 1.247 1.582 1.281

MSE
0.050 0.038 0.035 0.009 0.007
0.017 0.013 0.007 0.020 0.008

Global metrics

R2

Pearson, r

−3.234 −2.197 −1.969 0.193 0.368

0.417 0.659 0.726 0.786 0.764

−0.424 −0.118 0.422 −0.668 0.336

0.465 0.806 0.774 0.801 0.779

Spearman, ρ
0.433 0.688 0.728 0.799 0.767
0.491 0.808 0.783 0.799 0.785

MSE
0.051 0.038 0.035 0.009 0.007
0.017 0.013 0.007 0.020 0.008

Per-target metrics

R2

Pearson, r

−22.455 −15.930 −17.519 −2.326 −1.962

0.805 0.804 0.775 0.814 0.811

−6.375 −3.459 −1.348 −6.415 −1.760

0.648 0.789 0.722 0.820 0.742

Spearman, ρ
0.761 0.768 0.737 0.786 0.771
0.619 0.744 0.694 0.773 0.702

Hyperparameters. The learning rate was set to 0.001, the batch size 64, the dropout rate 0.2 and we used L2-regularization with the coefﬁcient of 0.003 for the network of order 5. We used the dropout rate of 0.1 and L2-regularization with the coefﬁcient of 0.001 for the network of order 10.
4.5 Results
We compared S-GCN and S-GCNs with our baseline network architecture, and also with the stateof-the-art single-model [Cheng et al., 2019] quality assessment methods SBROD [Karasikov et al., 2019], VoroMQA [Olechnovicˇ and Venclovas, 2017], ProQ3 [Uziela et al., 2016], Ornate [Pagès et al., 2019], and VoroCNN [Igashov et al., 2020]. SBROD is a regression-based method operating on 4D geometric descriptors, VoroMQA uses statistics from Voronoi 3D tessellation, ProQ3 is a neural-network-based method with precomputed descriptors of various origin, Ornate uses deep convolutional networks to process volumetric data in local coordinate frames, and, ﬁnally, VoroCNN is a graph convolutional network built on an atom-level molecular graph. We downloaded the results of VoroMQA and ProQ3 for CASP[12-13] and the results of SBROD for CASP13 from the ofﬁcial CASP archive at predictioncenter.org. To obtain the results of SBROD on CASP12 and Ornate and VoroCNN on CASP[12-13], we ran these methods locally. We should emphasize that the main results we report in this work were obtained on the CASP13 dataset, which was not used during training and validation. However, to give a complete picture, we also provide the results obtained on the unreﬁned CASP12 dataset. Table 2 lists the results for CASP12 and Table 3 lists the results for CASP13.
First of all, we can see a huge performance gap between the baseline network and the other methods. This can be explained by the fact that the baseline approach uses neither the 3D structure of the graph nor additional chemo-physical or biological features that are widely accepted by the state-of-the-art methods. At the same time, we would like to emphasize that our spherical graph convolutional networks, which explicitly use the 3D structure of the data, managed to achieve a similar or better quality of predictions compared to the state-of-the-art methods. Figure 2A shows some of the spherical ﬁlters learned by the 5th and the 10th order S-GCNs. We can see their rather complex shape, which is difﬁcult to interpret solely from physico-chemical considerations.
8

A

predicted CAD-score predicted CAD-score

B

S-GCN(5)
0.9 0.8 R2 = 0.118 0.7 r = 0.806
= 0.808 0.6
0.5
0.4
0.3

S-GCN(10)
R2 = 0.422 r = 0.774
= 0.783

S-GCNs(5)
0.9 0.8 R2 = 0.668 0.7 r = 0.801
= 0.799 0.6
0.5
0.4
0.3

S-GCNs(10)
R2 = 0.336 r = 0.779
= 0.785

0.2 0.4 0.6 0.8

0.2 0.4 0.6 0.8

ground-truth CAD-score ground-truth CAD-score

Figure 2: (A) Examples of spherical ﬁlters learned by S-GCN of order 5 (top row) and S-GCN of order 10 (bottom row). The distance to the center is proportional to the absolute function value. The red color corresponds to the positive values, the blue color – to the negative ones. (B) Histograms comparing the ground-truth scores and the predictions of spherical graph convolutional networks on the CASP13 dataset.

Tables 2 and 3 also demonstrate that using a higher order of the spherical harmonic expansion improves the MSE and R2 metrics. At the same time, the order 5 seems to outperform the 10th order in correlation and z-score metrics. This behaviour becomes clearer if we look closer at the absolute values of the predictions. Indeed, Figure 2B illustrates that the predictions of the 10th order network are closer to the diagonal, thus improving MSE and R2. It also explains that even though some methods can have high correlation metrics, their predictions are shifted with respect to the main diagonal, which results in negative R2. Thus, we can conclude that a higher polynomial order of the network allows us to better predict the absolute values of protein scores.
Regarding the correlation metrics, the 5th order S-GCN performs better than the others. We can conclude that it should be the method of choice for ranking protein models and selecting the best model from a given set. Also, taking into account the fact that the 10th order S-GCN has considerably more trainable parameters, and takes 4 times more disk space than the 5th order S-GCN, it makes more sense to use the latter for practical tasks. We can also see that the last scoring layer improves the correlation metrics. S-GCNs also demonstrate a better prediction quality on z-scores, but, as we show in Supplementary Materials, these metrics are not stable and we can not conﬁdently say that one method outperforms another because they all have intersecting conﬁdent intervals.
One ﬁnal remark that we can make after comparing the 5th and the 10th order S-GCNs is that the 10th order architecture may require signiﬁcantly more training data. Therefore, the current CASP training set may not be very well suited for higher-order architectures. As an alternative, one can consider training on Rosetta-generated decoys [Hiranuma et al., 2020] or using other methods for protein structure prediction.
5 Conclusion
In this work, we applied spherical convolutions to capture the 3D structure of a protein graph. The results demonstrate that our method gives a signiﬁcant improvement in the quality of predictions compared to the baseline without orientational relations between the graph nodes. The spherical convolution method can also be combined with other approaches for the protein model quality assessment, and can also potentially use more input features. Thus, we believe it will be possible to achieve even higher prediction results adding biological and chemical information to the input graphs. In addition, we would like to notice that the idea of spherical convolutions is universal and can be applied to various types of graph-learning tasks, provided that the graph structure permits us to deﬁne an equivariant coordinate systems for each graph node.
6 Acknowledgements
We would like to thank Kliment Olechnovic from Vilnius University for his help on graph construction and active discussions during the project. We would also like to thank Elodie Laine from Sorbonne Université for the discussions during the study and proof-reading the manuscript, and Jinbo Xu from

9

Toyota Technological Institute at Chicago for his helpful comments on the manuscript. This work was partially supported by the Inria International Partnership program BIOTOOLS.
Broader Impact
Our work will likely stimulate the development of new representation-learning methods applied to 3D graphs. The latter may represent molecules, such as proteins or nucleic acids. However, these graphs can also describe more general 3D data from other research domains, e.g., from astronomy or earth science.
From the application point of view, we believe our method will be useful in the structural bioinformatics and structural biology communities. Indeed, there has been a very rapid improvement of methods for 3D protein structure prediction, mostly owing to novel developments in deep learning and in algorithms extracting coevolution signals from sequence data. However, the question of how to assess the quality of the predicted models, and which parts of the predicted structures are likely to be less accurate, is still open. Thus, we hope that the proposed approach will help bioinformaticians and structural biologists to better use and analyze available computational data.
From a more general perspective, proteins are responsible for the main cellular functions in any organism. They maintain the shape of the cells, control chemical catalysis, play the role of cellular motors, and regulate vital processes. This makes the study of protein structures and interactions an important part of molecular biology. Understanding protein structure is also crucial for therapeutic purposes toward the development of new drugs, and the improvement of existing ones. To conclude, gaining knowledge of proteins and their functions contribute to a better understanding of the life machinery and organization, which has a signiﬁcant social impact.
References
Luciano A Abriata, Giorgio E Tamò, and Matteo Dal Peraro. A further leap of improvement in tertiary structure prediction in CASP13 prompts new routes for future assessments. Proteins: Structure, Function, and Bioinformatics, 87(12):1100–1112, 2019.
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. In Advances in Neural Information Processing Systems, pages 14510–14519, 2019.
Federico Baldassarre, David Menéndez Hurtado, Arne Elofsson, and Hossein Azizpour. GraphQA: Protein model quality assessment using graph convolutional network. Bioinformatics, btaa714 2020.
L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the chemical universe database GDB-13. Journal of the American Chemical Society, 131:8732, 2009.
Ewen Callaway. “It will change everything”: DeepMind’s AI makes gigantic leap in solving protein structures. Nature, 588(7837):203–204, Nov 2020. ISSN 1476-4687. doi: 10.1038/ d41586-020-03348-4. URL http://dx.doi.org/10.1038/d41586-020-03348-4.
Renzhi Cao and Jianlin Cheng. Protein single-model quality assessment by feature-based probability density functions. Scientiﬁc Reports, 6:23990, 2016. doi: 10.1038/srep23990. URL http: //www.ncbi.nlm.nih.gov/pmc/articles/PMC4819172/.
Yue Cao and Yang Shen. Energy-based graph convolutional networks for scoring protein docking models. Proteins: Structure, Function, and Bioinformatics, page In Press, 2020.
Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564–3572, 2019.
Jianlin Cheng, Myong-Ho Choe, Arne Elofsson, Kun-Sop Han, Jie Hou, Ali HA Maghrabi, Liam J McGufﬁn, David Menéndez-Hurtado, Kliment Olechnovicˇ, Torsten Schwede, et al. Estimation of model accuracy in CASP13. Proteins: Structure, Function, and Bioinformatics, 87(12):1361–1377, 2019.
10

Taco S Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In Sixth International Conference on Learning Representations, 2018.
Matthew Conover, Max Staples, Dong Si, Miao Sun, and Renzhi Cao. AngularQA: Protein model quality assessment with LSTM networks. Computational and Mathematical Biophysics, 7(1):1–9, feb 2019. doi: 10.1101/560995.
Georgy Derevyanko, Sergei Grudinin, Yoshua Bengio, and Guillaume Lamoureux. Deep convolutional networks for quality assessment of protein folds. Bioinformatics, 34(23):4046–4053, jun 2018. doi: 10.1093/bioinformatics/bty494.
Stephan Eismann, Patricia Suriana, Bowen Jing, Raphael JL Townshend, and Ron O Dror. Protein model quality assessment using rotation-equivariant, hierarchical neural networks. arXiv preprint arXiv:2011.13557, 2020.
Eshel Faraggi and Andrzej Kloczkowski. A global machine learning based scoring function for protein structure prediction. Proteins: Structure, Function, and Bioinformatics, 82(5):752–759, 2014. doi: 10.1002/prot.24454. URL http://dx.doi.org/10.1002/prot.24454.
Ronald A Fisher. Frequency distribution of the values of the correlation coefﬁcient in samples from an indeﬁnitely large population. Biometrika, 10(4):507–521, 1915.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263–1272. JMLR. org, 2017.
Joe G Greener, Shaun M Kandathil, and David T Jones. Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints. Nature communications, 10(1):3977–3977, 2019.
Naozumi Hiranuma, Hahnbeom Park, Ivan Anishchanka, Minkyung Baek, Justas Dauparas, and David Baker. Improved protein structure reﬁnement guided by deep learning based accuracy estimation. bioRxiv, 2020.
Ernest William Hobson. The Theory of Spherical and Ellipsoidal Harmonics. New York, Chelsea Pub. Co., 1955.
Alexandre Hoffmann and Sergei Grudinin. NOLB: Nonlinear rigid block normal-mode analysis method. Journal of chemical theory and computation, 13(5):2123–2134, 2017.
Jie Hou, Tianqi Wu, Renzhi Cao, and Jianlin Cheng. Protein tertiary structure modeling driven by deep learning and contact distance prediction in CASP13. Proteins: Structure, Function, and Bioinformatics, 87(12):1165–1178, 2019.
Ilia Igashov, Kliment Olechnovic, Maria Kadukova, Ceslovas Venclovas, and Sergei Grudinin. VoroCNN: Deep convolutional neural network built on 3D Voronoi tessellation of protein structures. bioRxiv 2020.04.27.063586, doi: https://doi.org/10.1101/2020.04.27.063586 2020.
John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graphbased protein design. In Advances in Neural Information Processing Systems, pages 15794–15805, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In 32nd International Conference on Machine Learning, 2015.
Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.
Xiaoyang Jing and Jinbo Xu. Improved protein model quality assessment by integrating sequential and pairwise features using deep learning. bioRxiv, 2020.
11

Xiaoyang Jing, Kai Wang, Ruqian Lu, and Qiwen Dong. Sorting protein decoys by machine-learning-to-rank. Scientiﬁc Reports, 6:31571, aug 2016. URL http://dx.doi.org/10.1038/srep31571http://10.0.4.14/srep31571http: //www.nature.com/articles/srep31571{#}supplementary-information.
Mikhail Karasikov, Guillaume Pagès, and Sergei Grudinin. Smooth orientation-dependent scoring function for coarse-grained protein quality assessment. Bioinformatics, 35(16):2801–2808, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Third International Conference for Learning Representations, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In Fifth International Conference on Learning Representations, 2017.
Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In Eighth International Conference on Learning Representations, 2020.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–Gordan nets: a fully Fourier space spherical convolutional neural network. In Advances in Neural Information Processing Systems, pages 10117–10126, 2018.
Andriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult. Critical assessment of methods of protein structure prediction (CASP)–Round XIII. Proteins: Structure, Function, and Bioinformatics, 87(12):1011–1020, 2019.
Tong Liu, Yiheng Wang, Jesse Eickholt, and Zheng Wang. Benchmarking Deep Networks for Predicting Residue-Speciﬁc Quality of Individual Protein Models in CASP11. Scientiﬁc Reports, 6:19301, jan 2016. URL http://dx.doi.org/10.1038/srep19301http://10.0.4.14/srep19301.
Yufeng Liu, Jianyang Zeng, and Haipeng Gong. Improving the orientation-dependent statistical potential using a reference state. Proteins, 82(10):2383–2393, oct 2014. ISSN 1097-0134 (Electronic). doi: 10.1002/prot.24600.
Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics, 29(21):2722–2728, 2013.
John Moult, Jan T Pedersen, Richard Judson, and Krzysztof Fidelis. A large-scale experiment to assess protein structure prediction methods. Proteins: Structure, Function, and Bioinformatics, 23 (3):ii–iv, 1995.
Eliya Nachmani and Lior Wolf. Molecule property prediction and classiﬁcation with graph hypernetworks. arXiv preprint arXiv:2002.00240, 2020.
Kliment Olechnovicˇ and Cˇ eslovas Venclovas. Voronota: A fast and reliable tool for computing the vertices of the voronoi diagram of atomic balls. Journal of computational chemistry, 35(8): 672–681, 2014.
Kliment Olechnovicˇ and Cˇ eslovas Venclovas. VoroMQA: Assessment of protein structure quality using interatomic contact areas. Proteins: Structure, Function, and Bioinformatics, 85(6):1131– 1145, 2017.
Kliment Olechnovicˇ, Eleonora Kulberkyte˙, and Cˇ eslovas Venclovas. CAD-score: A new contact area difference-based function for evaluation of protein structural models. Proteins: Structure, Function, and Bioinformatics, 81(1):149–162, sep 2012. doi: 10.1002/prot.24172.
Kliment Olechnovicˇ, Bohdan Monastyrskyy, Andriy Kryshtafovych, and Cˇ eslovas Venclovas. Comparative analysis of methods for evaluation of protein models against native structures. Bioinformatics, 35(6):937–944, 2019.
Guillaume Pagès, Benoit Charmettant, and Sergei Grudinin. Protein model quality assessment using 3D oriented convolutional neural networks. Bioinformatics, 35(18):3313–3319, 2019.
12

Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, and Maks Ovsjanikov. Effective rotationinvariant point CNN with spherical harmonics kernels. In 2019 International Conference on 3D Vision (3DV), pages 47–56. IEEE, 2019.
Arlo Randall and Pierre Baldi. SELECTpro: effective protein model selection using a structure-based energy function resistant to BLUNDERs. BMC Structural Biology, 8(1):52, 2008. ISSN 1472-6807. doi: 10.1186/1472-6807-8-52. URL http://dx.doi.org/10.1186/1472-6807-8-52.
Arjun Ray, Erik Lindahl, and Björn Wallner. Improved model quality assessment using ProQ2. BMC Bioinformatics, 13(1):224, 2012. doi: 10.1186/1471-2105-13-224.
M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. von Lilienfeld. Fast and accurate modeling of molecular atomization energies with machine learning. Physical Review Letters, 108:058301, 2012.
Soumya Sanyal, Ivan Anishchenko, Anirudh Dagar, David Baker, and Partha Talukdar. ProteinGCN: Protein model quality assessment using graph convolutional networks. BioRxiv 2020.04.06.028266, doi: https://doi.org/10.1101/2020.04.06.028266 2020.
Rin Sato and Takashi Ishida. Protein model accuracy estimation based on local structure quality assessment using 3d convolutional neural network. PLOS One, 14(9), 2019.
F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, jan 2009. doi: 10.1109/tnn. 2008.2005605.
Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions. In Advances in neural information processing systems, pages 991–1001, 2017.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Protein structure prediction using multiple deep neural networks in the 13th critical assessment of protein structure prediction (CASP13). Proteins: Structure, Function, and Bioinformatics, 87(12):1141–1148, 2019.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using potentials from deep learning. Nature, 577:706–710, 2020.
Min-yi Shen and Andrej Sali. Statistical potential for assessment and prediction of protein structures. Protein Science, 15(11):2507–2524, 2006. ISSN 1469-896X. doi: 10.1110/ps.062416606. URL http://dx.doi.org/10.1110/ps.062416606.
Mengying Sun, Sendong Zhao, Coryandar Gilvary, Olivier Elemento, Jiayu Zhou, and Fei Wang. Graph convolutional networks for computational drug development and discovery. Brieﬁngs in bioinformatics, 21(3):919–935, 2020.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor ﬁeld networks: Rotation-and translation-equivariant neural networks for 3D point clouds. arXiv preprint arXiv:1802.08219, 2018.
Karolis Uziela, Nanjiang Shu, Björn Wallner, and Arne Elofsson. ProQ3: Improved model quality assessments using rosetta energy terms. Scientiﬁc Reports, 6(1), oct 2016. doi: 10.1038/srep33509.
Björn Wallner and Arne Elofsson. Can correct protein models be identiﬁed? Protein Science, 12(5): 1073–1086, may 2003. doi: 10.1110/ps.0236803.
Zheng Wang, Allison N Tegge, and Jianlin Cheng. Evaluating the absolute quality of a single protein model using structural features and support vector machines. Proteins: Structure, Function and Bioinformatics, 75(3):638–647, may 2009. ISSN 1097-0134 (Electronic). doi: 10.1002/prot.22275.
13

Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D steerable CNNs: Learning rotationally equivariant features in volumetric data. In Advances in Neural Information Processing Systems, pages 10381–10392, 2018.
Jonghun Won, Minkyung Baek, Bohdan Monastyrskyy, Andriy Kryshtafovych, and Chaok Seok. Assessment of protein model structure accuracy estimation in CASP13: Challenges in the era of deep learning. Proteins: Structure, Function, and Bioinformatics, 87(12):1351–1360, 2019.
Jinbo Xu. Distance-based protein folding powered by deep learning. Proceedings of the National Academy of Sciences, 116(34):16856–16865, 2019.
Rafael Zamora-Resendiz and Silvia Crivelli. Structural learning of proteins using graph convolutional neural networks. bioRxiv 610444, doi: https://doi.org/10.1101/610444 2019.
Adam Zemla, Cˇ eslovas Venclovas, John Moult, and Krzysztof Fidelis. Processing and analysis of CASP3 protein structure predictions. Proteins: Structure, Function, and Bioinformatics, 37(S3): 22–29, 1999.
Jian Zhang and Yang Zhang. A Novel Side-Chain Orientation Dependent Potential Derived from Random-Walk Reference State for Protein Fold Selection and Structure Prediction. PLOS One, 5 (10):e15386, 2010. URL http://dx.doi.org/10.1371{%}2Fjournal.pone.0015386.
Wei Zheng, Yang Li, Chengxin Zhang, Robin Pearce, SM Mortuza, and Yang Zhang. Deep-learning contact-map guided protein structure prediction in CASP13. Proteins: Structure, Function, and Bioinformatics, 87(12):1149–1164, 2019.
Hongyi Zhou and Jeffrey Skolnick. GOAP: A Generalized Orientation-Dependent, All-Atom Statistical Potential for Protein Structure Prediction. Biophysical Journal, 101(8):2043–2052, oct 2011. ISSN 0006-3495. doi: 10.1016/j.bpj.2011.09.012. URL http://www.ncbi.nlm.nih. gov/pmc/articles/PMC3192975/.
Hongyi Zhou and Yaoqi Zhou. Distance-scaled, ﬁnite ideal-gas reference state improves structurederived potentials of mean force for structure selection and stability prediction. Protein Science : A Publication of the Protein Society, 11(11):2714–2726, nov 2002. ISSN 0961-8368. URL http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2373736/.
14

