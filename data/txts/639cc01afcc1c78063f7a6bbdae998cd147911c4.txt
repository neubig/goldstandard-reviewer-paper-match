A Robust Utility Learning Framework via Inverse Optimization
Ioannis C. Konstantakopoulos*, Lillian J. Ratliff*, Ming Jin, S. Shankar Sastry, Costas J. Spanos

arXiv:1704.07933v1 [cs.GT] 25 Apr 2017

Abstract—In many smart infrastructure applications, ﬂexibility in achieving sustainability goals can be gained by engaging endusers. However, these users often have heterogeneous preferences that are unknown to the decision-maker tasked with improving operational efﬁciency. Modeling user interaction as a continuous game between non–cooperative players, we propose a robust parametric utility learning framework that employs constrained feasible generalized least squares estimation with heteroskedastic inference. To improve forecasting performance, we extend the robust utility learning scheme by employing bootstrapping with bagging, bumping, and gradient boosting ensemble methods. Moreover, we estimate the noise covariance which provides approximated correlations between players which we leverage to develop a novel correlated utility learning framework. We apply the proposed methods both to a toy example arising from Bertrand-Nash competition between two ﬁrms as well as to data from a social game experiment designed to encourage energy efﬁcient behavior amongst smart building occupants. Using occupant voting data for shared resources such as lighting, we simulate the game deﬁned by the estimated utility functions to demonstrate the performance of the proposed methods.
Index Terms—Game Theory, Inverse Optimization, Smart Building Energy Efﬁciency
I. INTRODUCTION
Due to pervasive utilization of Internet of Things and CyberPhysical Systems sensing/actuating platforms, we are increasingly observing human decision-makers being integrated into operational and managerial decisions in infrastructure systems. Their actions can be leveraged to increase both resilience and sustainability thereby making smart infrastructure a worthwhile investment. Smart buildings, being no exception, are a fundamental component of emerging smart cities; their efﬁcient design and operation enables ﬂexibility—e.g., by automatically shifting or curtailing demand during peak hours— in making urban spaces sustainable. More abstractly, in many infrastructure systems there is often an entity acting as a planner (e.g., facility managers, departments of transportation, etc.) that introduces incentives or control policies to coordinate autonomously acting agents in the system (e.g., selﬁsh human decision-makers) so that their collective behavior leads to system-level efﬁciency gains.
One approach to designing such policies is to leverage game-theoretic models of decision-making in an optimization framework to produce policies that encourage or induce behavior that optimizes an objective [1], [2]. Often the planner has at best a prior on the decision-making model of the
*Authors contributed equally I. Konstantakopoulos, M. Jin, S. Sastry, and C. Spanos are with the Electrical Engineering and Computer Sciences Department, University of California, Berkeley, Berkeley, CA 94720. email: {ioanniskon, jinming, spanos, sastry}@eecs.berkeley.edu L. Ratliff is with the Electrical Engineering Department, University of Washington, Seattle, WA 98195. email: ratliffl@uw.edu

individual agents. Such information asymmetries lead to inefﬁciencies [1], [3]. In this paper, we propose a framework for estimating decision-making models of self-interested decisionmakers consuming a shared resource (e.g., lighting in a smart building) that can be leveraged in control or incentive design to aid in closing the efﬁciency gap.
To concretize ideas, consider a smart building—an example we will return to throughout the text. A facilities manager may be incentivized or even tasked to encourage energy efﬁcient behavior if they are accountable for energy costs or are required, e.g., to maintain an operational excellence measure (see, e.g., [4], [5]). At the same time, the facilities manager generally must also ensure user comfort and productivity [6]. Beyond these motivations, demand response (DR) programs are being rolled out by utility companies and third-party solution providers with the goal of correcting for improper load forecasting. Participating consumers decide to change their consumption when DR events are called [7]. The facilities manager may be required to keep this schedule.
Smart building technologies enable new avenues for facilities managers to keep such a prescribed schedule via automation or integrating the end-user. Yet, in ofﬁce buildings the occupants, as employees, typically are not responsible for paying for the energy resources they consume. Hence, there is often a misalignment between the incentives of the facilities manager and the occupants. Social games are a means to engage the occupants to address these inefﬁciencies. In Section VI, we describe one such social game that we designed and implemented on the UC Berkeley campus, aimed at incentivizing energy efﬁcient consumption of shared resources by leveraging building automation.
The broader purpose of this paper is to present a general framework that leverages game-theoretic concepts to learn models of players’ decision-making in competitive environments such as the building energy social game described above. The framework supports learning agents’ preferences over shared resources as well as understanding how preferences change as a function of external stimuli such as physical control or incentives. Such a framework can be used in the design of incentive mechanisms that realign agents’ preferences with those of the planner—which often represent system-level performance criteria—through fair compensation.
More concretely, we model decision-making agents as utility maximizers and, using inverse optimization and game-theoretic techniques, we derive a robust scheme to infer their utility functions. At the core of our approach is the fact that we model the agents as non-cooperative players in a game playing according to a Nash equilibrium strategy. From this point of view, agents are strategic entities that make decisions based on their own preferences despite others. The game-theoretic framework both allows for qualitative insights to be made

about the outcome of such selﬁsh behavior—more so than a simple prescriptive model—and, more importantly, can be leveraged in designing mechanisms for incentivizing agents.
We assume a parametric form of utility function for each player that is dependent on the decisions of others. Correlations between players’ decisions are not known a priori. Assuming observations are approximately Nash equilibria, we use ﬁrst– and second–order conditions on player utility functions to construct a constrained regression model. The result is as a constrained Generalized Least Squares (cGLS) problem with non-spherical noise error terms. Using constrained Feasible Generalized Least Squares (cFGLS), an implementable version of cGLS, we utilize heteroskedastic inference to approximate the correlated errors.
Noting that data sets of observed decisions often may be small relative to the number of model parameters in practice, we employ bootstrapping to generate pseudo-data from which we learn additional estimators. The bootstrapping process allows us to derive an asymptotic approximation of the bias and standard error of an estimator. We utilize ensemble methods such as bagging, bumping, and gradient boosting to extract an estimator from the pseudo-data generated estimators that results in a reduced forecasting error. The ensemble methods are robust under noise and autocorrelated error terms. We apply the robust utility learning framework to a model of Bertrand-Nash competition between ﬁrms in order to illustrate the framework and its performance.
Building on the robust utility learning framework, we use the approximated standard error to derive an innovative utility learning method in which we modify players’ utility functions to create a correlated game. The resulting correlated utility learning method leverages correlations between players and the ensemble estimators to minimize the estimation error by optimizing scaling coefﬁcients that appear in the correlated game utility functions. Applying this method results in a signiﬁcant improvement over the constrained Ordinary Least Squares (cOLS) estimations and outperforms many of the ensemble methods. It also provides insights into how players interact with one another and indicates which players are potentially forming coalitions. Moreover, this technique is amenable to online implementation after an initial training period so that by using cOLS estimators in the correlated utility learning framework, our adaptive incentive design schemes, introduced in [3], [8], can be made robust.
To demonstrate the efﬁcacy of both the robust and correlated utility learning frameworks, we apply them to data generated from the smart building social game experiment we conducted. We show that estimating the players’ utility functions via the proposed methods results in a predictive model that outperforms several other standard techniques such as Ordinary Least Squares (OLS).
The rest of the paper is organized as follows. We describe the abstracted game framework for modeling the interaction of agents as well as deﬁne equilibrium concepts in Section II. In Section III, we formulate the robust utility learning framework and provide an algorithm for implementing it. Section IV contains the Bertrand-Nash competition example and we present the correlated utility learning framework in Section V. In

Section VI, we describe the social game experimental setup on the UC Berkeley campus within the CREST center1, provide a brief literature review, and present the results of both proposed utility learning methods applied to data from the social game. We make concluding remarks and discuss future directions in Section VIII.
II. GAME FRAMEWORK In this section, we abstract the agents’ decision-making processes in a game–theoretic framework.

A. Agent Decision-Making Model
Consider p agents2—i.e. decision-making entities—indexed by the set I = {1, . . . , p}. Each agent is modeled as a utility maximizer that seeks to select xi ∈ R by optimizing

fi(xi, x−i) = finom(xi, x−i) + fiinc(xi, x−i).

(1)

where

f

nom i

(x

i

,

x−

i

)

and

f

inc i

(

xi

,

x−

i

)

are

the

nominal

and

incentive components, respectively, of agent i’s utility function

and where x−i = (x1, . . . , xi−1, xi+1, . . . , xn) ∈ Rn−1 is the collective choices of all agents excluding the i–th agent3.

The choice xi abstracts the agent’s decision; it could rep-

resent, e.g., how much of a particular resource they choose

to consume. The nominal component of fi captures the

agent’s individual preferences over xi and may depend on

the decisions of others x−i. The incentive component models

the portion of the agent’s utility that can be designed by the

planner; it also may depend on the decisions of other agents.

Agent i’s optimization problem is also subject to constraints;

the constraint set is given by Ci = {xi| hi,j(xi) ≥ 0, j =

1, . . . , i} where each hi,j is assumed to be a concave func-

tion of xi. Such constraints may encode cyber or physical

constraints arising from the underlying system—in the social

game example presented in Section VI-C, we will see that

these constraints are physical bounds. Thus, given x−i, agent

i faces the following optimization problem:

max{fi(xi, x−i)| xi ∈ Ci}.

(2)

B. Game Formulation
The game (f1, . . . , fp) is a continuous game on a convex strategy space C = C1 × · · · × Cp. To model the outcome of the strategic interactions of agents, we use the Nash equilibrium concept.

Deﬁnition 1. A point x ∈ C is a Nash equilibrium for the game (f1, . . . , fp) on C if, for each i ∈ I,

fi(xi, x−i) ≥ fi(xi, x−i) ∀ xi ∈ Ci.

(3)

We say x ∈ C is an ε–Nash equilibrium for ε > 0 if the above inequality is relaxed:

fi(xi, x−i) + ε ≥ fi(xi, x−i) ∀ xi ∈ Ci.

(4)

1http://crest.berkeley.edu/ 2We refer to the decision-makers as agents and use the term interchangeably with players. 3Note that while for notational simplicity we assume that xi ∈ R, the work easily extends to a higher dimensional choice vector for each agent.

We say a point is a local Nash equilibrium (respectively, a ε–local Nash equilibrium) if there exists Wi ⊂ Ci such that xi ∈ Wi and the above inequalities hold for all xi ∈ Wi.
If each fi is concave in xi and C is convex, then the game is a p–person concave game. In the seminal work by Rosen [9], it was shown that a (pure) Nash equilibrium exists for every concave game.
The Lagrangian of agent i’s optimization problem is given by
Li(xi, x−i, µi) = fi(xi, x−i) + j∈Ai(xi) µi,j hi,j (xi) (5)
where Ai(xi) is the active constraint set at xi and µ = (µ1, . . . , µp) with µi = (µi,j)ji=1 are the Lagrange multipliers. Assuming appropriate smoothness conditions on each fi and hi,j, the differential game form [3], [10]—which characterizes the ﬁrst–order conditions of the game—is given by
ω(x, µ) = [D1L1(x, µ1) · · · DpLp(x, µp) ] (6)
where DiLi denotes the derivative of Li with respect to xi. Consider agent i’s optimization problem (2) with x−i ﬁxed
and where each fi and hi,j for j ∈ {1, . . . , i}, i ∈ I are concave, twice continuously differentiable functions. Then, assuming an appropriate constraint qualiﬁcation condition [11], the necessary and sufﬁcient conditions for optimality of a point xi are as follows: there exists µi ∈ R+i such that (i) DiLi(x, µi) = 0; (ii) µihi,j(xi) = 0 for each j ∈ {1, . . . , i}; (iii) hi,j(xi) ≥ 0 for each j ∈ {1, . . . , i}. Regardless of the concavity assumption, the point xi is a local maximizer if µi,j > 0 and z Di2iLi(x, µi)z < 0 for all z = 0 such that Dihi,j(xi) z = 0 for j ∈ Ai(xi). Such conditions motivate the following deﬁnition.
Deﬁnition 2 (Differential Nash Equilibrium). Consider a game (f1, . . . , fp) on C where fi and hi,j for each j ∈ {1, . . . , i} and i ∈ I are twice continuously differentiable. A point x ∈ C ⊂ Rp is a differential Nash equilibrium if
p
there is a µ ∈ R i=1 i such that the pair (x, µ) satisﬁes (i) ω(x, µ) = 0; (ii) for each i ∈ I, z DiiLi(x, µi)z < 0 for all z = 0 such that Dihi,j(xi) z = 0, and µi,j > 0 for j ∈ Ai(xi). If, for a given ε > 0, (i’) ω(x, µ) = ε with all the other conditions being satisﬁed, then x is a ε–differential Nash equilibrium.
The above deﬁnition extends the deﬁnition of a differential Nash (if we restrict to Euclidean spaces), ﬁrst appearing in [10], to constrained games on Euclidean spaces. Using this deﬁnition, we can also extend Proposition 1 of [10], again where strategy spaces are restricted to be subsets Euclidean.
Proposition 1. A differential Nash equilibrium of the p–person concave game (f1, . . . , fp) on C is a local Nash equilibrium.
The proof is straightforward and we leave it to Appendix A. The proposition says that the conditions of Deﬁnition 2 are sufﬁcient for a local Nash. In contrast to single-agent optimization problems, for games, the second order conditions do not imply the equilibrium is isolated [10]. A sufﬁcient condition guaranteeing that a Nash equilibrium x is isolated is that the Jacobian of ω(x, µ), denoted Dω(x, µ), is invertible [3].

We use (necessary and sufﬁcient) optimality conditions on individual player optimization problems holding other players’ strategies ﬁxed to formulate the utility learning framework.

III. ROBUST UTILITY LEARNING

In previous work, we have explored utility learning and incentive design as a coupled problem both in theory [3] and in practice [8], [12], [13]. In the present work, we re-examine the utility learning problem using statistical methods that serve to improve estimation and prediction accuracy.
Looking forward, our aim is to fold the new estimation scheme into the overall incentive design framework. This goal motivates why we are interested in learning more than a simple predictive model for agents, but rather a utility-based forecasting framework that accounts for individual preferences.
We parameterize fi by θi = (θi1, . . . , θimi ) ∈ Rmi and a ﬁnite set of basis functions {φij(xi, x−i)}m j=i1 such that

fi(x; θi) = φi(xi, x−i), θi + f¯i(x)

(7)

where φi = [φi,1 · · · φi,mi ] and f¯i(x) is a function that captures a priori knowledge of the agent’s utility function
(e.g., the incentive component designed by the planner).

A. Base Utility Estimation Framework

We start by describing the basic utility estimation framework using equilibrium conditions for the game played between the players. The utility learning framework we propose is quite broad in that it encompasses a wide class of continuous games. In previous works [3], [12], [13] we have shown that the utility learning problem can be formulated as a convex optimization problem by using ﬁrst– and second– order conditions for Nash equilibria. Let us brieﬂy review this formulation as it serves as the basis for the robust utility learning method.
Each observation x(k) is assumed to be an ε–approximate differential Nash equilibrium where the superscript notation (·)(k) indicates the k–th observation. For each observation x(k), it may be the case that only a subset of the players, say Sk ⊂ I at observation k, participate in the game. Then notationally each observation is such that

x(k) = x(jk)

.

(8)

j ∈S k

If player i participates in ni instances of the game, then there

are ni observations for that player. Let n =

p i=1

ni

be

the

total number of observations.

We can consider ﬁrst–order optimality conditions for each

player’s optimization problem and deﬁne a residual function capturing the degree of suboptimality of x(ik) [8], [14]. Indeed,
for player i’s optimization problem, let the residual of the

stationarity condition be given by

rs(,ki)(θi, µi) = Difi(x(ik), x(−ki)) +

i
j=1

µji

Dihi,j

(x(ik))

(9)

and the residual of the complementary conditions be given by

rcj,,i(k)(µ) = µji hi,j (x(ik)), j ∈ {1, . . . , i}.

(10)

Deﬁne

rc(,ki)(µi) = [rc1,,i(k)(µi) · · · rc,ii,(k)(µi)].

(11)

Using data from the players’ decisions (e.g., lighting votes from the social game experiment which we describe in Section VI-A), the base utility learning framework consists of solving the optimization problem given by

min

p i=1

ni k=1

χi(rs(,ki)(θ,

µ),

rc(,ki)(µ))

µ,θ

(P)

s.t. θi ∈ Θi, µi ≥ 0 ∀ i ∈ {1, . . . , p}

where Θi is a constraint set on the parameters θi that captures

prior information about the objective, χ : Rp × R

→ p
i=1 i

R+ is a non-negative, convex penalty function satisfying

χ(z1, z2) = 0 if and only if z1 = 0 and z2 = 0, i.e. any norm

on Rp × R

p
i=1 i , and the inequality µi ≥ 0 is element-wise.

The goal of this optimization problem—which is a ﬁnite

dimensional optimization problem in the θi’s—is to ﬁnd θi for each player such that (fˆi)i∈I is consistent (or approximately

consistent) with the data. As is noted in [14], we also remark

that it is important that the sets Θi contain enough prior

information about the objectives fi in order to prevent trivial solutions. For example, if it is the case that f¯i(x(k)) = 0 for each k and each Θi = Rmi then the trivial solution θi = 0mi

is feasible. For many applications some a priori knowledge

on part of the utility functions of players may be encoded in

each Θi (e.g., choosing Θi such that θ1i = 1 or similarly

selecting the incentive component of the utility, a design

possibility for the planner [3]) or through other normalization

techniques to prevent such trivial solutions. In the context of

the social game application (in Section VI-C), we explicitly

discuss how to construct this constraint set in such a way that

we ensure the estimated utility functions are concave which

in turn guarantees that there exists a Nash equilibrium to the

estimated game.

B. Robust Utility Learning

Let us now formulate a robust version of the utility learning framework that allows us to reduce our forecasting error and learn the noise structure which can be leveraged in extracting pseudo–coalitions between players which we describe in the sequel.
Deﬁne

X(k) = Dihi(x(ik)) Diφi(x(k))) ,

(12)

i

hˆ i (x(ik) )

0 i×mi

where hˆi(xi) = diag(hi,1(xi), . . . , hi, i (xi)), (13)

Dihi(xi) = [Dihi,1(xi) · · · Dihi, i (xi)],

(14)

and nd = ( i + 1)n is the total number of data points. The regressor matrix is then deﬁned as X = diag(X1, · · · , Xp) ∈ Rnd×( i+1)p where Xi = [(Xi(1)) · · · (Xi(ni)) ] . Deﬁne the regression coefﬁcient
β = [µ11 . . . µ11 θ1 · · · µ1p . . . µpp θp] ∈ R( i+1)p (15)

and the observation matrix Y = [Y1 · · · Yp] ∈ R( i+1)p
where Yi = [f¯i(x(1)) 0 i · · · f¯i(x(ni)) 0 i ] . (16)

Using the Euclidean norm for χ in (P) leads to an OLS

problem with inequality constraints—i.e. a constrained OLS

(cOLS):

min Y − Xβ 2 β ∈ B

(P1)

β

where B = {β| θi ∈ Θi, µi ≥ 0, ∀i ∈ I}. Enforcing that each of the constraint sets Θi is encoded by inequalities on θi, the above stated problem can be viewed as a classical multiple linear regression model with inequality constraints described by the data generation process

Y = Xβ + , β ∈ B

(17)

where = ( 1, . . . , p) is the error term satisfying: (i) E( |X) = 0nd×1; (ii) cov( |X) = σ2Ind×nd ; (iii) { i}pi=1 independent and identically distributed (i.i.d) with a zero mean and σ2 variance. In addition, we assume is nonspherical [15]. With this general statistical model we are able to describe a data generation processes in which the error terms are correlated or lack constant variance. This fact will be leveraged in creating coalitions between players as we describe in Section V.
Mathematically the nonspherical errors are modelled by

cov( |X) = G 0, G ∈ Rnd×nd .

(18)

One drawback of this technique is that, given nonspherical

standard errors, the cOLS estimator is biased—that is, it does

not satisfy the Best Linear Unbiased Estimator (BLUE) prop-

erty, a result of the Gauss–Markov theorem [15, Theorem 1,

Chapter 5]. However, we can derive an unbiased estimator by

multiplying

(17)

on

the

left

with

G−

1 2

.

This

leads

to

the

cGLS

statistical model given by

(G−

1 2

Y

)

=

(G−

1 2

X )β

+

(G−

1 2

),

β∈B

(19)

which now satisﬁes the BLUE property. In general, the explicit
form of cov( |X) = G is unknown. We use the residuals (17)
to infer the noise by imposing structural constraints on G.
We remark that there are many types of noise structures
that can be used for imposing structure on G. We provide
two example noise structures that could be used. The ﬁrst
is block diagonal structure [15, Chapter 5]; in particular, we impose that G = blkdiag(K1, · · · , Kp) ∈ Rnd×nd where Ki = blkdiag(Bi,1, . . . , Bi,ni ) ∈ R( i+1)ni×( i+1)ni with each Bi,k ∈ R( i+1)×( i+1). Estimating β with cOLS, we get βˆcOLS with residual vector e = Y − XβˆcOLS ∈ R( i+1)n. The residual vector e can be decomposed into residuals for each
player by writing e = [e1 · · · ep ] . We use ei to compute an estimate Kˆi of Ki which is, in turn, used to compute Gˆ. The residuals come in triplets since for each k, Yi(k) ∈ R i+1. For ease of presentation and comprehension, we will use a
paired index for the residuals instead of a single index. For
example, for player i, there are ni instances at which we have i observations. Let (ei)k,j = (ei)( i+1)(k−1)+j where k ∈ {1, . . . , ni} and j ∈ {1, . . . , ( i + 1)}. With the residuals,

we can then form estimates Bˆi,k ∈ R( i+1)×( i+1) of Bi,k where Bˆi,k takes the form

Bˆi,k = [(Bˆi,k)l,j )]l,ij+=11

(20)

with n−i 1

(Bˆi,k )j,j

=

n−i 1

ni t=1

e2t,j

and

(Bˆi,k )l,j

=

ni t=1

et,j

et,l

for

j

=

l.

We

provide

this

noise

structure

as an example because in our formulation we allow for

constraints on the players’ optimization problems so that

for each iteration k, we in fact have multidimensional

observations as can be seen in (12).

The second noise structure we consider is adapted from the

HC4 estimator [16] and is given by

Gˆ = diag

, , · · · , e21
(1−b1 )δ1

e22 (1−b2 )δ2

e2nd (1−bnd )δnd

(21)

where δi = min {4, ndbi/(

nd i=1

bi)}

and

the

bi’s

are

the

di-

agonal elements of B = X(X X)−1X . With this structure,

the penalty for each residual increases with bi/

nd j=1

bj

.

As

with the previous noise structure, we use the ﬁtted cOLS

estimator βˆcOLS and residuals to get an initial Gˆ. We selected

to present this noise structure because it is computationally

efﬁcient compared to many other noise structures. In both cases, we substitute the inferred noise, Gˆ, into the

cGLS statistical model (19) to get the one–step constrained

Feasible GLS (cFGLS) estimators. We iterate between the estimation of Gˆ and βcFGLS either until convergence or for

a ﬁxed number of iterations to prevent overﬁtting. To resolve

this trade-off and ﬁnd the optimal iteration size we adopt a

simple cross validation method.

C. Boosting with Ensemble Methods

In this subsection, we describe several ensemble methods.

Combined with a bootstrapping process, ensemble methods

not only boost the size of what can often be a small data

set in practice but also allow us to improve the estimator

performance and explore the bias–variance tradeoff.

1) Bootstrapping and Bagging: Bootstrapping is a tech-

nique for asymptotic approximation of the bias and standard

error of an estimator in a complex and noisy statistical

model [15], [17]. We employ wild bootstrapping to generate

a pseudo-data set from which we generate several weak

estimators that we then combine using bagging. While we

assume that E(Y |X) = Xβ, we also allow for heteroskedas-

ticity by conditioning on the residual transformations that

we imposed in the noise structure. Wild bootstrapping is a

technique of parametric bootstrapping that is consistent with

heteroskedastic inference and cFGLS data generation.

The bootstrapping process can be described in two steps: First, we ﬁt our cFGLS model which gives us βˆcFGLS. Then,

generate N replicates of pseudo–data using the data generation

process

Y˜ = XβˆcFGLS + Φ(e)ε,

(22)

where Y˜ ∈ Rnd is the new observation vector (pseudoobservations), βˆcFGLS ∈ Rnd is the cFGLS estimator, ε ∼
N (0, Ind×nd ), e ∈ Rnd is the residual vector given by e = Y˜ − XβˆcFGLS and Φ : Rnd → Rnd is a nonlinear

transformation such that Φ(e)

=

Gˆ

1 2

∈

Rnd×nd . Since

E(Φ(e)ε|X) = Φ(e)E(ε|X) = Φ(e)E(ε) = 0nd×nd , using

the data generation process in (24), we resample from i.i.d

variables.

Bagging in regression models and trees is a technique for

reducing the overall variance [17]. Using the N replicates

of pseudo–data generated by wild bootstrapping, we train

N different models. We combine the resulting bootstrapped

estimators by averaging:

βˆbag

=

1 N

N j=1

βˆcFGLS,j

(23)

where βˆcFGLS,j is the estimator using the j–th pseudo–data sample. Bagging works efﬁciently with high variance models and does not hurt the overall performance of the statistical model. We refer to the bagged estimates as bagged megalearners since they combine several weak learners/estimators. Using wild bootstrapping, the empirical covariance matrix of βˆ is an asymptotic approximation of the covariance matrix and is given by

Cˆβ

=

1 N

N j=1

βˆcFGLS,j − βˆbag

βˆcFGLS,j − βˆbag

. (24)

Asymptotic estimation of the empirical covariance matrix reveals hidden structures between players and is what we leverage in the correlation utility learning procedures.
2) Bootstrapping and Bumping: In a similar fashion as the bagging ensemble method, we combine bumping—a method for ﬁtting cFGLS estimators by using a random search over the model space [18]—with the wild bootstrapping generated pseudo-data. In particular, we apply a stochastic search over several different statistical models coming from a similar data process—i.e. the data process in (24).
We add the original training data sample to the N replicates of pseudo-data generated by the wild bootstrapping process and we use this data to estimate N + 1 cFGLS estimators. We evaluate these estimators on the training set and select the one with the least training error. The cFGLS bumping estimator is given by

βˆbump = arg min

Y˜ − XβˆcFGLS,j

2 2

βˆcFGLS,j

(25)

where βˆcFGLS,j’s are the cFGLS estimators from derived from the bootstrapped data.
3) Gradient Boosting: We combine L2–gradient boosting—which is a repeated least squares ﬁtting of residuals [19]—with cFGLS. Gradient boosting is a boosting technique that uses an L2 loss function combined with a gradient descent update method for combining weak learners at each iteration. Boosting estimators are trained in sequence using a weighted version of the original data set. In general, boosting methods are extremely useful for combining models by incrementally training each new model by emphasizing the errors of the previous training instances. They are used extensively in classiﬁcation methods such as logistic regression and support vector machines.
Repeated residual ﬁtting is applied until we reach iteration mstop, a stopping criteria selected using Akaike Information Criterion (AIC) to avoid overﬁtting [20]. . The procedure is detailed in Algorithm 1.

Algorithm 1 L2–gradient boosting with cFGLS

1: function CFGLSGRADBOOST(X,Y ,ν)

2: Hˆ ← X(X X)−1X

compute Hˆ matrix

3: ν ← s ∈ (0, 1] set shrinkage (updating) parameter

4: mstop ← 1

iteration number

5: choose Mmax

upper iterations bound

6: AIClist ←[ ]

create empty list

7: Compute stopping iteration time mstop:

8: while mstop < Mmax do

9: Rmstop ← (Ind×nd − νHˆ )mstop

10:

Bmstop ← (Ind×nd − Rmstop )

11: σm2 stop ← n−d 1 ni=d1(Yi − (Bmstop Y )i)2

12: AI Cmstop ← log σm2 stop + 1−1+(T(Tr(rB(Bmm stospto)p+))2/)n/dnd

13:

AIClist.append(AICmstop )

14:

mstop ← mstop + 1

15: end while

16: Mˆ ← arg min AICtotal

ﬁnd minimum point

17: βˆcFGLS ← estimate of βcFGLS using cFLGS

18: eFGLS ← Y − XβˆcFGLS

residuals estimation

19: e ← ecFGLS

initialize residuals

20: k ← 1

21:

βboost ← βˆcFGLS

iteration index initialize cFGLS boosted learner

22: Compute boosted learner βboost:

23: while k < Mˆ do

24:

βi ← (X X)−1X e

25:

βˆboost ← βˆboost + νβi

26:

e ← Y − Xβˆboost

residuals ﬁtting update formula residuals update

27:

k ← k+1

28: end while

29: end function

IV. APPLICATION TO BERTRAND-NASH COMPETITION
Let us illustrate the framework and its performance of the robust utility learning framework before moving on by applying it to estimate market demand functions under BertrandNash equilibrium (see, e.g., [21]–[23]). The toy model can be thought of as an abstraction of Bertrand-price setting for commodities such as oil, gas, and coal [24], [25].
Consider two ﬁrms competing to sell their product by setting the price p1 and p2 for ﬁrm 1 and 2, respectively. The ﬁrms utility functions are their revenue, i.e. fi(p1, p2) = piDi(p1, p2, ξ) where Di is the demand function for ﬁrm i and ξ ∼ N (1.5, 0.5) is a random variable that captures the fact that demand is dependent on economic indicators in addition to the prices set by the ﬁrms. In this stylized example, we consider linear demand functions given by

Di(p1, p2, ξ) = θi,1 + θi,2p1 + θi,3p2 + νξ

(26)

where θi = (θi,j)3j=1 are unknown parameters to be estimated and ν = 1.5 is a known parameter. The prices are constrained
to be in the interval [0, p¯] where p¯ ∈ R+ is the upper bound. We let θ1 = (−1.0, 0.5, −1) and θ2 = (0.3, −1, 0.3) be the
ground truth values for the parameters we wish to estimate. Thus, f¯i(p1, p2) = νξ and examining the marginal revenue functions Difi(p1, p2) we have that φ1(p1, p2) = [1 2p1 p2] , and φ2 = [1 p1 2p2] .

TABLE I MEAN SQUARE ERROR (MSE) OF FORECASTING USING THE PROPOSED ROBUST
UTILITY LEARNING METHODS VS COLS ESTIMATORS FOR BERTRAND-NASH COMPETITION. THE BEST PERFORMING METHOD IS INDICATED IN BOLD TEXT FOR
EACH OF THE FIRMS.

Firm 1 MSE
Firm 2 MSE

bagging 0.05
bagging 1.58

boosting 0.51
boosting 0.71

bumping 0.65
bumping 0.89

cOLS 1.62
cOLS 2.54

In order to generate the data set we add a noise term ε ∼ N (0, 0.5) to the marginal revenue functions, i.e. Difi(p1, p2) + ε, and solve for the Bertrand-Nash equilibrium. We simulate the game between the ﬁrms 600 times. In the robust utility learning framework, for this example, we employ the HC4 noise structure and compute the cOLS, cFGLS, bagging, boosting and bumping estimators. We use a 10–fold cross validation proceedure to prevent over-ﬁtting. Table I contains error using two metrics for both ﬁrms. Figure 1 shows the forecast for part of the testing set using cOLS and each of the ensemble methods as compared to the ground truth. While bagging performed best for ﬁrm 1 and boosting for ﬁrm 2 in the particular instantiation of this toy example, the performance more generally is dependent on the noise structure in the demand and marginal revenue functions, the sample size, and the dynamics between the two ﬁrms. However, it is interesting to point out that as we increase the variance on ξ, each of the ensemble methods performance stay relatively the same yet the cOLS error increases signiﬁcantly.

Price [$], Firm 1

14

Forecasting for Bertrand-Nash

12

10

8

6

4

2

0

18 16 14 12 10 8 6 4 2 00 1 2 3 4 5 6 7 8 9 10111213141516171819

ground truth cOLS bumping boosting bagging cOLS

Price [$], Firm 2

Fig. 1. Forcast for Firms 1 & 2 using cOLS and each of the ensemble methods. The ground truth prices are depicted by the blue dots; the cOLS forecasts are depicted in black, the bagging forecasts are depicted in gray, the bumping forecasts are depicted in green, and the boosting forecasts are depicted in gold.

V. CORRELATED UTILITY LEARNING
In this section, we describe how learned correlations between players can be leveraged to boost estimator performance. We add a second step to the estimation procedure in

which we craft a new game where players’ utilities are composed of their original estimated utility plus some combination of other players’ utilities weighted by the estimated correlation between players.
When the correlations between players are positive, we are creating what we refer to as pseudo-coalitions since players are not explicitly agreeing to collude in the game but rather are doing so implicitly. The degree of coalition is discovered by the robust utility learning process through estimating the empirical covariance Cˆβ, i.e. asymptotic approximation of the covariance matrix—of βˆest where we use the notation βˆest to abstractly denote the estimator derived from whichever of the methods described in the previous section is employed. On the other hand, when the correlations between players are negative, by combining their utilities we aim to take advantage of active players’ richer data sets in predicting the behavior of players with less variation and frequency in their observed actions.
We refer to the learned utility—fˆi for player i—from the robust utility learning framework as the base utility and it is given by

fˆi(xi, x−i; θˆi) = f¯i(xi, x−i) + φi(xi, x−i), θˆi

(27)

where θˆi is extracted from βˆest,i. Using the correlations we learn when we estimate fˆi, we
construct a new utility gˆi by combining scaled versions of a subset (potentially all) of the other agents’ utilities that are
correlated with agent i. We formulate an optimization problem
to deterimine the scaling coefﬁcients. The correlated utility gˆi for player i is given by

gˆi(xi, x−i) = j∈Ki zi,j σi,j f¯i(xi, x−i)

+ σi,j φi(xi, x−i), θˆj

(28)

where Ki ⊂ Ii a subset of the players correlated with player i, σi,i is the estimated variance of player i determined by the empirical covariance matrix, σi,j is the covariance between the parameter estimates for player i and j also determined by the empirical covariance matrix, and zi,j are scaling constants to be optimized. We refer to the resulting game as an approximated correlation game4.
Given the form of gˆi, our goal is to select the scaling constants zi,j in order to reduce the forecasting error. Analogous to the base utility learning framework presented in
Section III-A, using our training data, we formulate a convex
optimization problem using optimality conditions on each
player’s individual optimization problem where we assume that player i is optimizing gˆi with respect to its own choice variable xi. In particular, we solve a convex optimization problem formulated as follows. Deﬁne the vector zi ∈ R|Ki| by zi = (zi,j)j∈Ki and let z = (zi)i∈I . For player i’s

4We remark that there exists an equilibrium concept called correlated equilibrium [26] which generalizes a Nash equilibrium by characterizing correlations between randomized strategies; we mention this only to alleviate any potential confusion. The equilibrium concept we utilize for the approximated correlation game is still a pure Nash equilibrium and there is no coordinating mechanism.

optimization problem max{gˆi(xi, x−i)| xi ∈ Ci}, let the residual of the stationarity condition be given by

rs(,ki)(zi, µi; θˆ) = Digˆi(x(ik), x(−ki)) +

i
j=1

µji

Dihi,j

(x(ik))

(29)

and the residual of the complementary conditions be given by

rcj,,i(k)(µi) = µji hi,j (x(ik)), j ∈ {1, . . . , i}.

(30)

As before, let rc(,ki)(µi) = [rc1,,i(k)(µi) · · · rc,ii,(k)(µi)]. Deﬁne Qi ∈ Rni×|Ki| by

Qi = σi,j Di2,if¯i(x(k)) nk=i 1,j∈Ki .

(31)

and qi ∈ Rni by

qi =

j∈Ki σi,j Di2,iφi(x(k)), θˆj

ni
.
k=1

(32)

Then, we have the following convex optimization problem to determine the scaling factors zi,j:

min

p i=1

ni k=1

χi(rs(,ki)(zi,

µi;

θˆ),

rc(,ki)(µi))

z,µ

(P’)

s.t. Qizi + qi ≤ 0, µi ≥ 0 ∀ i ∈ I

Solving P’ gives us estimated correlated utilities gˆi for each i ∈ I that we then use to forecast the players’ decisions.

VI. APPLICATION TO SMART BUILDING SOCIAL GAME
We now specialize the robust and correlated utility learning frameworks to the smart building social game.

A. Social Game Experimental Set-Up
Our experimental setup is in a collaboratory space—an open, shared work space with cubicles—within the CREST center on the UC Berkeley campus. We crafted a social game such that occupants in this collaboratory freely vote according to their usage preferences of shared resources and are rewarded with points based on how energy efﬁcient their strategy is in comparison with the other occupants. We employ a lottery mechanism consisting of three Amazon gift cards executed bi-weekly to reward occupants; occupants with more points are more likely to win the lottery.
The ofﬁce is divided into ﬁve lighting zones and two heating, ventilating, and air conditioning (HVAC) zones. In this space, there is a total 20 occupants who are eligible to participate in the social game. If the occupants are not present in the ofﬁce, they are excluded from the game at that time instant. When they arrive at the ofﬁce, they can rejoin the game. To enforce the rule that those who are not present in the space cannot vote remotely, we executed a simple presence detection algorithm based on their power usage [27], [28].
We have installed a Lutron5 system for precise control of the lighting setting (dim level of the lights) in the ofﬁce as well as desk–level energy monitoring devices (i.e. ACME wireless sensors [29]) to meter the energy usage of each occupant. In addition, we have modiﬁed the HVAC system so that it can be precisely controlled. We have veriﬁed prior to our experiment
5http://www.lutron.com/en-US/Pages/default.aspx

(a)

(b)

Fig. 2. Graphical user interface (GUI) for energy based social game: (a) Display, in table form, of points and votes for energy consumption, HVAC, and lights. (b) Display of the GUI for logging lighting setting preferences.

(a)
(b) Fig. 3. Occupants can access a variety of information when they log into the social game portal, including various displays of energy consumption by other participants in the game: (a) Display of current light level and temperature in the collaboratory space; energy efﬁciency of the lights is coded by color where light green indicates higher energy efﬁciency. (b) Display of collaboratory ﬂoor plan with dots indicating where present and participating players sit. Players not in the ofﬁce are excluded from the game. The color of the dot indicates the level of energy efﬁciency of the player as compare to the other participants; green indicates higher efﬁciency while red indicates lower efﬁciency.
that implemented control of these systems results in expected performance.
We have developed a platform to interface with the occupants as well as manage and process collected data. The platform includes a web portal and mobile app that the

occupants may use to participate in the game. It also allows for occupants to visualize different aspects of the social game— e.g., the lighting setting and the energy efﬁciency level of different occupants or the entire building—as well as view the point level and historical voting record of other occupants among many other statistics. Figure 2 shows the user interface for viewing points and logging votes. Figure 3a shows a visualization of the current light level using a green–to–red color scale with green being more energy efﬁcient. The current temperature is also displayed. Figure 3b shows a visualization of each present and participating occupant’s energy efﬁciency level.
In this paper, we report on a social game experiment conducted based only the lighting shared resource6. Prior to the start of the social game experiment, the lighting setting was 90% of the maximum possible lighting setting. At the start of the social game experiment, we set a default lighting setting which acts as the suggested lighting setting and is the dim level setting in the ofﬁce if, e.g., no occupants are participating in the game. Throughout the game, we adjust the default lighting setting as well as the points. The lottery mechanism coupled with the points we distribute compose the incentive component of the feedback to the participants while the default lighting level is the physical control component of the feedback. These two mechanisms act as our control inputs and our feedback mechanism to the participants. We seek to design them by taking into consideration the preferences of the participants. In this way, these mechanisms close the loop around the participant and with our proposed utility learning scheme, these mechanisms can be modiﬁed to encourage more energy efﬁcient resource consumption.
The game is designed to leverage interactions amongst occupants, who win points based on how energy efﬁcient their lighting vote is compared to others. An occupant’s vote is for the lighting setting in their zone as well as for neighboring zones. The occupants select their desired lighting setting in
6We remark that while our experimental platform is capable of conducting a social game that includes lights, HVAC, and personal energy consumption, we only report on an experiment that focuses on lighting in order to isolate combined effects from these different resources. In on-going experiments, we are examining all aspects jointly.

the continuous interval [0, 100] where each value represents the percentage of the maximum lighting setting possible in the space. The occupants can vote as frequently as they like and the average of all the occupants’ current votes sets the implemented lighting setting in the collaboratory. An occupant can leave the lighting setting as the default level after logging in or they can change it depending on their preferences and other environmental factors that may affect their choice.
The experimental trials reported on in this paper were conducted over the period of 285 days7. Experiments with 4 different default levels, {10%, 20%, 60%, 90%}, were conducted, covering a spectrum of lighting conditions. Since occupants were allowed to vote whenever they chose, their response rate per day varies. The data set we collected consists of occupant votes (meaning the lighting level they select) over the period of investigation as well as the points that were distributed to each occupant. We collected 6,885 votes over the period of the experiment.
B. Brief Background
In order to place the work pertaining to building energy efﬁciency in the context of the state of the art, we brieﬂy overview existing approaches.
Recognizing that HVAC systems are responsible for a large portion of building energy consumption, many control theoretic approaches such as [30], [31] derive model predictive and distributed control polices for HVAC systems. While these control theoretic approaches make efforts to account for the presence of occupants, they tend to ignore occupant behaviors and, more importantly, their heterogeneous preferences.
There are other works that make strides towards incorporating behavioral models of occupants; e.g., the authors of [32] employ a multi-agent systems approach to develop a framework for incorporating occupant comfort preferences and the authors of [33] develop behavioral models for lighting usage. In a more active approach, the authors of [34] develop a collaborative setting deﬁnition paradigm in which occupants and facilities managers submit preferences and requirements and a rule engine tries to resolve them in order to create a universal control policy. While occupants’ preferences are taken as inputs to the building control design, it is not clear that it is possible to satisfy all the occupants’ comfort preferences simultaneously with those of the facilities manager; hence, the misalignment between preferences and incentives remains.
In our approach, on the other hand, we leverage a social game that creates (friendly) competition between users and employs incentives to resolve conﬂicting preferences by compensating users. Within the energy application domain, gamiﬁcation has been largely used for education or awareness (see, e.g., [35], [36]). There are works that are closely related to ours in the sense that they also recognize that occupants are selfinterested participants in smart buildings and try to account for their strategic behavior. For example, in [37], the authors develop an interesting scheme for engaging occupants directly in DR. Analogous to our approach, occupants are modeled as utility maximizers in a game theoretic context where they
7The period of the experiment was 2014/3/3–2014/12/14.

are incentivized to curtail their consumption in response to an event. Our approach differs in that we focus on shared resources such as lighting and HVAC instead of personal devices (e.g., desk appliances). Furthermore, it is assumed in [37] that the type space (i.e. their preferences) of the users is a known ﬁnite set of two possible values. We do not assume the facility manager knows the utility function or the type of the users and we propose an algorithm for learning this utility function from observations of decisions.
While incorporating occupant preferences into building automation is not novel in and of itself, we propose an innovative algorithm for learning occupant preferences in competitive environments and, moreover, learn how their actions are correlated. Such correlations can be leveraged in improving incentive mechanisms to shape users’ preferences thereby providing more ﬂexibility. Our method is applied to real-world data from experimental trials we conducted as opposed to simulations as is the case with many existing works. Furthermore, it is agnostic to the application and could be applied in general to other scenarios in which users are competing for constrained but shared resources. For example, the utility learning method can be easily adapted to learning preferences of individual buildings interacting with an aggregator or learning preferences of drivers seeking on-street parking [7]. In each of these cases, there exists a planner—the aggregator or department of transportation—tasked with managing a resource being consumed by self-interested users.

C. Occupant Decision-Making Model

Each agent’s vote xi is constrained to be in the interval

[0, 100] ⊂ R. Let x¯ denote the average of the lighting votes

and the setting that is implement—e.g., at observation instance

indexed by k, x¯(k)

=

1 |S k |

j∈Sk x(jk). We model each

agent’s utility as being composed of two basis functions that

capture the tradeoff between desired lighting (satisfaction) and

desire to win. The lighting satisfaction an occupant feels may

be a function of several factors including their productivity

(ability to perform their job) as well as physical comfort. We

abstractly model their desired lighting level using a Taguchi loss function, ψi(xi, x−i) = − (x¯ − xi)2, which is interpreted

as modeling occupant dissatisfaction in such a way that it is

increasing as variation increases from their reported desired

lighting setting (their vote) [38].

We acknowledge that an agent may have some internal

desired lighting level that is different than its vote; e.g., the

agent may realize that voting an extreme value pushes the

average toward a more desirable setting. This type of gaming

results in moral hazard type issues which can be addressed

in the incentive design step [1], [2]. Thus, we set this type

of gaming aside for the time being, and focus instead on

the unknown preferences—a different kind of asymmetric

information that leads to adverse selection—between lighting

and winning.

Points are distributed by the planner using the relationship ρ(xb − xi)(p(xb − x¯))−1 where xb is the baseline setting for

the lights. For the experiment xb = 90%, i.e. the lighting

setting used before the implementation of the social game.

However, we model each occupant as having a winning basis function given by φi(xi, x−i) = −ρc (xi)2 where ρ is the total number of points distributed by the planner and c is a
scaling factor that is used primarily to scale the two terms of
the utility function given that we artiﬁcially inﬂate the points
offered in order to increase their appeal to players and thus induce greater participation8. The form of the winning function
can be interpreted as capturing the perception that by voting
zero, the occupant is selecting the action that will provide the
greatest return of points given that points are awarded based on how energy efﬁcient their vote is compared to others9.
Hence, the utility functions for the social game are modeled as fi(xi, x−i; θi) = θiφi(xi, x−i)+ψi(xi, x−i). The constraint sets Ci for each player are determined by the box constraints on the lighting vote for that player, i.e. Ci = {xi ∈ R| hi,j(xi) ≥ 0, j ∈ {1, 2}} where hi,1(xi) = 100 − xi and hi,2(xi) = xi.
In order to formulate (P) for the social game application, we need to determine the admissible parameter sets Θi, i ∈ I in such a way that we ensure the estimated utility functions
are concave and such that equilibria of the estimated game are isolated. We derive a lower bound θLB such that all θi ∈ Θi = {θi ∈ R| θi > θLB}, i ∈ I induce games with these characteristics. To this end, we utilize the second derivative condition on players’ utility functions; that is, if for each i ∈ I, Di2,ifi(x) < 0, then the game is concave. Computing Di2,ifi and using some algebra, we have that θi > −(cρ)−1(1−p−1)2 where the right-hand side is a negative non-increasing function of p. Thus, concavity is ensured regardless of the number of players by setting p = 2, the minimum number of players in a non-cooperative game. Then, given ﬁxed ρ and 0 < ζ << 1, the lower bound θ¯LB = −(4cρ)−1 + ζ will guarantee the estimated game is concave.
If Dω(x, µ) is invertible, we know that differential Nash
equilibria are isolated [10]. Hence, we can augment the constraint sets Θi to encode this condition. Given the structure of the utility functions, Dω(x, µ) is simply the game Hessian H = [Hi,j ]pj,i=1 with Hi,i = Di2,ifi and Hi,j = Di2,j fi. Hence, if H is invertible, then the differential Nash are isolated; this is guaranteed for p ≥ 4 provided the constraint deﬁned by θ¯LB = −(4cρ)−1 + ζ using ζ = 10−2. Indeed, let H(p) denote the game Hessian as a function of the number of players and note that for a particular p, with some simple algebra, it is easy to write H(p) as a off-diagonal matrix constant matrix such that Hii = di + α and Hi,j = α where di = −2(1 − 1/p) − 2cρθi and α = 2(p − 1)/p2. It is straightforward to verify by determining the eigenvalues of H as p varies via the method described in [40] that for p ≥ 4, H will
8Inﬂating the points is a process of framing [39]—that is, dependent on how the reward system is presented to agents greatly impacts their participation. Framing is routinely used in rewards programs for credit cards among many other point-based programs. The scaling factor c in the winning function removes the framing effect from the estimation procedure. It is selected to ensure the scale of the two basis functions are similar.
9We explored other forms of the winning function including the log function, a quasi-concave function that is typically used to represent how individuals value money since it represents the diminishing returns property well [12]. However, the quadratic form of the function we report on here signiﬁcantly outperformed other choices so that, for the purpose of a prescriptive model, it captures the agents’ perceptions about the point distribution mechanism and their value more accurately.

be invertible . For the social game data, at each observation indexed by k, the number of participating players is at least 4. Thus, to ensure concavity and isolated equilibria of the estimated social game, we deﬁne Θi = {θi ∈ R| θi > θ¯LB} with θ¯LB = −(cρ4)−1 + ζ with ζ = 10−2.
VII. UTILITY LEARNING RESULTS
We now present the results of the proposed robust utility learning method applied to data collected from the social game experiment.
As we previously described, our data set consists of the votes logged by the players which vote throughout the day. We present estimation results for the complete data set of all the votes—which we refer to as the dynamic data set— and estimation results for an aggregated data set constructed by taking the average of a players’ votes over the course of each day in the experiment—this is referred to as the average data set. While this aggregation signiﬁcantly reduces the size of our data set, it smooths the players’ voting proﬁles and increases the size of active players in each game—occupants may arrive or leave the ofﬁce when they so choose. This average data set also reduces the computational load, which may be beneﬁcial to a facilities manager in the incentive design process, especially if the incentive scheme is quasi-static and uses historical data to generate the next incentive. The dynamic data set is much richer, being composed of every vote (a total of 6, 885 votes) the occupants made throughout the duration of the experiment (285 days). The time from one vote to the next may be several minutes to hours depending on the activity of the occupants. This data set is much larger and thus, increases the computational load. However, it allows us to extract more distinct player proﬁles and can support realtime incentive design schemes.
We present results for both data sets using data from the period of the experiment in which the default lighting setting was 20%—the results for the other default lighting settings are similar. The period of the experiment where the default lighting setting was 20% consisted of 42 days and thus the size of the averaged data set is 42. Over this period there were 220 votes by occupants, which is the size of the dynamic data set. We divide each of the data sets into training (80% of the data) and testing (20% of the data) sets and apply each of the methods discussed in Section III. We apply a 10–fold cross validation [17] procedure to limit overﬁtting.
A. Forecasting via Robust Utility Learning
We estimate the parameters using cFGLS and the ensemble methods bagging, bumping, and boosting for both the average and dynamic data sets. For gradient boosting, we use the HC4 noise structure (see (21)) since the leverage values bii of B are larger [16]; in each of the other methods, we used the block diagonal noise structure (see (20)).
Using the estimated utility functions, we simulate the game using a projected gradient descent algorithm which is known to converge for concave games [42]. In Figure 4a and 4b, we compare the ground truth voting data to the predictions for each of the learning schemes using the dynamic and averaged

average of lighting votes, [%] 0 1 2 3 4 5 6 7 8 109 1121 1143 1165 1187 2109 2221 2243 2265 2287 3209 3321
average of lighting votes, [%]

55

Forecasting with Dynamic Data

50

45

40

35

30

25

20

15

10

vote index

cOLS

30

bagging

bumping

25

boosting

ground truth 20

Forecasting with Averaged Data

cOLS bagging bumping boosting ground truth

15

10 4-2 4-3 4-4 4-5 4-6 4-7 4-8 4-9 4-10 4-11 4-12 date, year=2014

(a)

(b)

Fig. 4. Forecasting results for (a) dynamic data and (b) averaged data for the default lighting setting 20: For the dynamic data, the x–axis values indicate the index of when a choice was made by one or more of the occupants (i.e. when the implemented lighting setting is changed); the time from one index to the next may be several minutes to hours depending on the activity of the occupants. For the averaged data, the x–axis values are dates (month and day). The ground truth average of the lighting votes is depicted by the blue dots; the forecast for cOLS is depicted in black; the forecast for bagging is depicted in gray; the forecast for bumping is depicted in green; the forecast for boosting is depicted in gold. The forecast for the robust utility learning methods is approximately near the ground truth for both data sets while the cOLS estimates produce Nash equilibria with a large error.

TABLE II ROOT MEAN SQUARE ERROR (RMSE), MEAN ABSOLUTE ERROR (MAE) AND MEAN ABSOLUTE SCALED ERROR (MASE) [41] OF FORECASTING USING THE PROPOSED ROBUST UTILITY LEARNING METHODS VS COLS ESTIMATORS FOR BOTH DATA SETS IN DEFAULT LIGHTING SETTING 20. THE BEST PERFORMING METHOD IS INDICATED IN BOLD TEXT FOR EACH OF THE DATA SETS, DYNAMIC AND AVERAGE.

Dynamic, fˆi RMSE MAE MASE
Averaged, fˆi RMSE MAE MASE

bagging 8.31 5.20 2.08
bagging 2.05 1.58 0.71

boosting 10.11 6.55 6.38
boosting 1.68 1.31 0.59

bumping 12.56 6.38 2.55
bumping 1.96 1.48 0.67

cOLS 22.53 18.35 7.34
cOLS 9.36 6.01 2.69

data sets, respectively. Our proposed robust models—i.e. using the estimated parameters obtained via bagging, bumping, and boosting—capture most of the variation in the true votes (in both data sets) and signiﬁcantly outperform cOLS. In Table II, using three metrics—Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Scaled Error (MASE)—we report the forecasting error for each of the methods.
The estimated models using our robust utility learning methods signiﬁcantly reduce the forecasting error as compared to cOLS. The cOLS method has particularly poor forecasting performance on the dynamic data set since it does not capture the correlated error terms describing the interactions between users. Moreover, our robust methods perform better than cOLS with the averaged data set even though the sample size is small.
As for the ensemble methods, bagging outperforms the other three methods when using the dynamic data set. On the other hand, for the averaged data set, gradient boosting gives the least forecasting error. This is in large part due to the fact that we use the HC4 noise structure. Since the average data set has been smoothed, we expect less correlation between players and the HC4 noise structure captures this.

B. Estimated Utility Functions
Figure 5 shows the estimated utility functions and their contour plots for occupants 2 and 8—passive and aggressive occupants respectively—using the parameters obtained via the bagging ensemble method with the dynamic data set. We remark that we do not observe the actual value of agents’ utilities; we instead observe only the agents’ decisions. The purpose of the ﬁgures is to show the estimated utility shapes for players with signiﬁcantly different voting proﬁles (the observable we have). The particular occupants we selected represent players that prefer winning to lighting satisfaction (occupant 8) and players that prefer lighting satisfaction to winning (occupant 2). In particular, occupant 2’s estimated utility function appears to be higher at greater lighting settings. Exactly the opposite occurs for occupant 8 whose estimated utility function indicates that despite changes in the average lighting vote of other players, occupant 8 aggressively votes for a zero lighting setting which returns the most points.
For comparison—and to highlight the improvement that the robust utility learning framework offers—in Figure 6 we show the estimated utility function for occupant 8 using cOLS. What we see is a very different utility function that indicates occupant 8 cares more about lighting satisfaction than winning— indicated by the fact that its utility is not maximized at zero. This is misleading since occupant 8 predominately votes for zero. This is signiﬁcant since incentive/control design based on such an erroneous utility function may lead to very poor performance and occupant dissatisfaction.

C. Bias Approximation and Bias–Variance Tradeoff

Forecasting accuracy can be enhanced by allowing for a small amount of bias if it results in a large reduction in variance. For a process Y = Xθ + , the Mean Square Error (MSE) characterizes the bias–variance tradeoff :

MSE(x) = E[(Y − θestx)2]

(33)

= (E[θestx] − Y )2 + E[(θestx − E[θestx])2] (34)

bias

variance

−0e2 −5e2
fˆ−10e2 2 −15e2 −20e2 x −25e2 90 80 70 60 50 40 −2 30 20 10 0 0 (a)

x 80 100

60

40 20

2

−0e6

−2e6
fˆ8 −4e6

−6e6

x −8e6

908070

60 50 40

−8 30 20 10

20

00

(b)

40 x860

80 100

Fig. 5. Bagging estimated utility functions—using the dynamic data set—of (a) agent 2 and (b) agent 8. The functions are plotted as a function of each agent’s own vote x2 (resp. x8) and other players’ votes x−2 (resp. x−8). Notice that agent 8, a very aggressive player, is indifferent to the choices of the other agents as indicated by the fact that its utility is maximized in the same location given any value of x−8. On the other hand, occupant 2 responds to changes in the other agents’ votes and appears to prefer a greater lighting settings (more illumination). This indicates that there are different types of players and thus, incentives may need to be designed individually for these player types in order to elicit the desired response.

10e3
5e3
fˆ8 0e3
−5e3
−10e3
x 90 80 70 60 50 40 −8 30 20 10 0 0

x 80 100

60

40 20

8

Fig. 6. Agent 8’s cOLS estimated utility function—using the dynamic data set—plotted as a function of (x8, x−8). This ﬁgure demonstrates that using cOLS (the worst performing estimator) results in learning a utility function that is not representative of this type of player’s behavior (as can be seen by comparing to Figure 5b). Incentives or control designed using this function may result in performance.

Introducing bias in exchange for reduced variance is widely used in ridge regression and in lasso techniques in the form of a priori knowledge [17]. In our robust utility learning framework, we introduce noise structures that approximate the true data process so that we can ﬁt cFGLS estimators that are nearly unbiased for those players whose historical voting record has a large amount of variation.
We approximate the bias for each of the estimators. In Table III, we present cFGLS estimates obtained using the dynamic data during the time window in which the default lighting setting was 20%10 for selected occupants—the most active players—as well as the approximated bias for the estimates generated by bagging, bumping, and boosting.
10The results for the other default lighting settings are similar.

TABLE III THE CFGLS ESTIMATOR VALUE AND THE BAGGING, GRADIENT
BOOSTING AND BUMPING ENSEMBLE METHODS BIAS APPROXIMATION
FOR THE MOST ACTIVE USERS. WE UTILIZED THE DYNAMIC DATA SET
FROM THE PERIOD IN WHICH THE DEFAULT LIGHTING SETTING WAS SET
TO 20. IN BOLD, WE DENOTE THE OCCUPANTS WITH NEARLY UNBIASED ESTIMATORS.

Id cFGLS Bagging Bias Boosting Bias Bumping Bias

2

-0.7

6

0.5

8 298.1

14 337.5

20 -0.8

0.11 1.12 -176.9 -186.3 0.07

0.17 1.77 -370.3 -400.2 0.21

0.02 0.93 120.5 149.7 -0.53

Figures 8 and 7 contain histograms of the cFGLS estimators obtained using the bootstrapped average and dynamic data, respectively. In each of these histograms, we also indicate the original cFGLS11 (indicated in red), bagging (indicated in blue), bumping (indicated in green), and boosting (indicated in orange) estimators with dashed vertical lines.
The histogram in Figure 8 contains the cFGLS estimators for occupant 2. This histogram is representative of the other occupants for the average data set. We see that the original cFGLS, bagging, bumping, and boosting estimators each show some amount of bias. This is largely due to the fact that the average data set has a small sample size.
On the other hand, in Figure 7a we show the histogram of cFGLS estimators for occupant 2 produced via bootstrapped dynamic data and we can see that the original cFGLS estimator (vertical red line) is nearly unbiased, indicated by the approximate Gaussian distribution around the cFGLS estimate. This is generally true for the occupants with the most variation and frequency in their voting record. However, bagging, bumping, and boosting produce estimates that are slightly biased in
11This is the cFGLS estimator produced using the original average and dynamic data sets and not the bootstrapped data sets.

# of estimators

300 Histogram of cFGLS for Player 2 (dynamic) 250 200 150 100 50
0 0.8 0.6 0.4 0.2 0.0
θ2
(a)

cFGLS bumping bagging boosting

# of estimators

800 Histogram of cFGLS for Player 8 (dynamic)

700

600

500

400

300

200

100

0500 0

500 1000 1500 2000 2500 3000
θ8

(b)

cFGLS bumping bagging boosting

Fig. 7. The histograms depict the estimates generated with the wild bootstrapping technique using the dynamic data set for (a) player 2 and (b) player 8. The vertical lines mark the value of the cFGLS (red), bumping (green), bagging (blue), and boosting (orange) estimators. The histogram for player 2 is approximately normally distributed around the initial cFGLS estimator, indicating that it is unbiased. On the other hand, this is not the case for player 8. Thus its cFGLS estimator is biased. Overall, the majority of the proposed ensemble methods result in a signiﬁcant reduction in variance in exchange for an small increase in bias and greater forecasting accuracy. In our other work [43], we develop a hierarchical mixture model that considers both bias and variance.

# of estimators

450 Histogram of cFGLS for Player 2 (average) 400 350 300 250 200 150 100 50
0 0.8 0.6 0.4 0.2 0.0
θ2

cFGLS bumping bagging boosting

Fig. 8. The histogram depicts estimator values for player 2 using the wild bootstrapping technique using the average data set. The vertical lines mark the value of the cFGLS (red), bumping (green), bagging (blue), and boosting (orange) estimators. We remark that the estimators are all biased. This is expected due to limited sample size of the average data set. Thus, the average data set cannot be used for optimizing the bias-variance tradeoff.

exchange for a reduction in estimator variance—see (33). Occupant 2 is representative of players which prefer to
focus on lighting satisfaction as opposed to winning whereas occupant 8 is representative of players which prefer winning to lighting satisfaction. While a very active voter, frequently participating in the game, occupant 8’s voting record has little variation (the majority of the time x8 = 0). Figure 7b contains the cFGLS estimators for occupant 8 and we see that each of the estimators are slightly biased. Again, these estimators introduce bias in exchange for a reduction in variance.

D. Forecasting via Approximated Correlated Game
We now show the results for the correlated utility learning method. Let us use the notation
gˆi(xi, x−i; {θˆj }j∈Ki ) = j∈Ki zi,j σi,j ψi(xi, x−i) + σi,j θˆj φi(xi, x−i) (35)

where recall that Ki ⊂ I is the index set for the players whose parameters are used to modify player i’s utility function in generating the correlated game and θˆj is the estimated parameter from the utility learning methods including cOLS,
cFGLS, bagging, bumping, and boosting. We use the notation gˆi(·; {θˆj}j∈Ki }) as short-hand.
In Table IV, we show a subset of the estimated covariance
matrices obtained using the dynamic and average data sets.
Using these values, we construct the following correlated
game. Player 2’s utility function is modiﬁed by player 20’s:

gˆ2(x2, x−2; K2) = (z2,2σ2,2 + z2,20σ2,20) ψ2(x2, x−2) + (σ2,2θˆ2 + σ2,20θˆ20)φ2(x2, x−2) (36)

where K2 = {2, 20}. Player 2 and 20 are passive players in that their votes tend to be strongly related to their lighting satisfaction as opposed to increasing their chances of winning. They are also very active players, having a lot of variation in their voting record. These two players are positively correlated with one another (see the red cells in Table IV).
On the other hand, player 8 and 14 are aggressive players in that their votes tend to be much lower indicating a greater desire to win points. These players are also positively correlated (see the green cell’s in Table IV). With this in mind, we modify player 8’s utility function by player 14’s:

gˆ8(x8, x−8; K8) = (z8,8σ8,8 + z8,14σ8,14) ψ8(x8, x−8) + (σ8,8θˆ8 + σ8,14θˆ20)φ8(x8, x−8) (37)

where K8 = {8, 14}. Player 14 is also negatively correlated with player 2. Hence,
player 14’s utility function is modiﬁed by player 2’s and 8’s utilities. That is, with K14 = {2, 8, 14}, we have

gˆ14(x14, x−14; K14) = i∈K14 (z14,iσ14,iψ14(x14, x−14)

+ σ14,iθˆiφ14(x14, x−14))

(38)

All the other players’ utilities in the correlated game remain unchanged; that is, they are taken to be gˆi = fˆi,
i ∈ I/{2, 8, 14}.

TABLE IV ESTIMATED COVARIANCE MATRIX FOR THE MOST ACTIVE PLAYERS USING THE (a) DYNAMIC DATA SET AND (b) AVERAGE DATA SET. THE COLORED
COLUMN-ROW PAIRS INDICATE THE AGENTS WHOSE UTILITIES WE MODIFY TO CREATE THE CORRELATED GAME; THE COLUMN INDICATES THE AGENT(S) WHOSE ESTIMATED PARAMETER IS USED TO MODIFY THE ROW AGENT’S UTILITY. IN PARTICULAR, AGENT 2’S UTILITY FUNCTION IS MODIFIED BY BY AGENT 20’S ESTIMATED PARAMETER (RED), AGENT 8’S UTILITY FUNCTION IS MODIFIED BY AGENT 14’S ESTIMATED PARAMETER (GREEN), AND AGENT 14’S UTILITY FUNCTION IS MODIFIED BY AGENT 2’S AND AGENT 8’S ESTIMATED PARAMETER (BLUE). NOTE THAT AGENTS 2 AND 14 ARE ANTI-CORRELATED, WHERE AGENTS 8 AND 14 (RESP. AGENTS 2 AND 20) ARE POSITIVELY CORRELATED. AGENTS 2 AND 20 ARE PASSIVE
PLAYERS, VOTING MORE FOR COMFORT THAN WINNING, WHERE AGENTS 8 AND 14 VOTE MORE AGGRESSIVELY.

Id

2

6

8

14

20

2 0.086 0.080 -0.190 -0.248 0.059

6

0.080 7.56 8.64

9.02 0.028

8 -0.190 8.64 170.98 44.29 -0.337

14 -0.248 9.02 44.29 87.34 -0.312

20 0.059 0.028 -0.337 -0.312 0.063

(a) Average Data

Id

2

6

8

14

20

2 0.044 0.059 -2.805

-5.191

0.031

6

0.059 7.836

-16.82

0.844

-0.016

8 -2.805 -16.82 6.43×104 4.28×104 -7.60

14 -5.191 0.844 4.28×104 8.84×104 -12.59

20 0.031 -0.016

-7.60

-12.59

0.073

(b) Dynamic Data

average of lighting votes, [%] 0 1 2 3 4 5 6 7 8 109 1121 1143 1165 1187 2109 2221 2243 2265 2287 3209 3321
average of lighting votes, [%]

55 Forecasting with Dynamic Data, Correlated Game 50 45 40 35 30 25 20 15 10
vote index

cOLS corr. cOLS corr. bagging corr. bumping corr. boosting ground truth

30 Forecasting with Averaged Data, Correlated Game 25 20 15

cOLS corr. cOLS corr. bagging corr. bumping corr. boosting ground truth

10 4-2 4-3 4-4 4-5 4-6 4-7 4-8 4-9 4-10 4-11 4-12 date, year=2014

(a)

(b)

Fig. 9. Forecasting results for the correlated game using (a) dynamic data and (b) averaged data for the default lighting setting 20: For the dynamic data, the x–axis values indicate the index of when a choice was made by one or more of the occupants (i.e. when the implemented lighting setting is changed); the time from one index to the next may be several minutes to hours depending on the activity of the occupants. For the averaged data, the x–axis values are dates (month and day). The ground truth of the average of the lighting votes is depicted by the blue dots; the forecast for cOLS is depicted in black; the forecast for correlated cOLS is depicted in purple; the forecast for correlated bagging is depicted in gray; the forecast for correlated bumping is depicted in green; the forecast for correlated boosting is depicted in gold. The forecast for the robust utility learning methods is approximately near the ground truth for both data sets while the cOLS estimates produce Nash equilibria with a large error. However, the correlated cOLS forecast signiﬁcantly improves on the cOLS forecast.

TABLE V ROOT MEAN SQUARE ERROR (RMSE), MEAN ABSOLUTE ERROR (MAE) AND
MEAN ABSOLUTE SCALED ERROR (MASE) OF FORECASTING USING THE ESTIMATED CORRELATED UTILITY FUNCTIONS. WE ESTIMATED CORRELATED UTILITY FUNCTIONS gˆi(·; {θj }j∈Ki ) USING PARAMETERS FROM THE BAGGING, BUMPING, BOOSTING, AND COLS METHODS FOR BOTH DATA SETS IN DEFAULT
LIGHTING SETTING 20.

Dynamic, gˆi RMSE MAE MASE
Averaged, gˆi RMSE MAE MASE

bagging 6.38 4.59 1.84
bagging 2.18 1.75 0.78

boosting 9.58 6.81 2.72
boosting 1.63 1.27 0.56

bumping 8.82 5.52 2.21
bumping 2.36 1.92 0.86

cOLS 8.44 5.58 2.23
cOLS 2.83 2.30 1.03

These player combinations were selected since, through the correlated game, we aim to improve our estimators by leveraging correlations between players. In particular, the goal is to utilize information learned from players with the most variation in their votes in improving the estimates of players who consistently vote the same value or have a limited participation record.
In Table V, we present the RMSE, MAE, and MASE for

the estimated correlated game {gˆi(·; {θˆj}j∈Ki )}i∈I where the θˆj’s are taken to be the cOLS, bagging, boosting, and bumping estimators. Comparing these results to those in Table II, we see that correlated estimation schemes applied to the dynamic data set reduce the estimation error for almost every method. Moreover, correlated bagging outperforms bagging, the best performing ensemble method, by all three metrics. For the average data set, correlated boosting outperforms the best performing ensemble method, boosting, again by all three metrics.
In Figure 9, we show the forecast produced by the correlated utility learning method using the cOLS, bagging, bumping, and boosting estimators and the ground truth test data. Figure 9a and 9b are the forecasts for the dynamic and average data sets, respectively.
What is perhaps most interesting is that, for both data sets, the correlated cOLS results improve the forecasting error as compared to cOLS and the results are not signiﬁcantly different than the other ensemble methods. This can be seen in Table V and Figure 9. The importance of this ﬁnding is that correlated cOLS has the potential to be integrated into an online algorithm. The classical cOLS can be performed

online and is, thus, amenable to an online incentive design framework [3], [8]. However, as we have seen, the ensemble methods signiﬁcantly outperform cOLS. Determining the estimated covariance matrix requires solving a generalized least squares (GLS) and noise covariance estimation problem [44]. Given that the estimated correlated game using cOLS parameters provides nearly the same estimation error as the ensemble methods, these methods can be adapted to estimate the correlated game parameters and then introduced into an adaptive incentive design framework. We are currently exploring this extension as the ultimate objective is to utilize the learned utilities in an incentive design framework, preferably one that can be executed in an adaptive/online manner. This will support a more robust online utility learning and incentive design algorithm.
VIII. DISCUSSION
We presented a general framework for robust utility learning using a heteroskedastic inference adaptation to cGLS and we leveraged learned correlations between players in constructing a correlated utility learning framework that matches the robust utility learning errors while also being amenable to online implementation. The latter is important for integrating the proposed utility learning techniques with adaptive control or online incentive design. For example, it has been shown that static programs for encouraging energy efﬁciency are subject to the rebound effect in which participants often return to less efﬁcient behavior after some time [45], [46]. By integrating our utility learning framework with incentive design, we will be able to create an adaptive model that learns how users’ preferences change over time and thus, generate the appropriate incentives to ensure active participation.
To demonstrate the utility learning methods, we applied them to data collected from a smart building social game we conducted where occupants vote for shared resources and participate in a lottery. We were able to estimate nearly unbiased estimators for several agent proﬁles and signiﬁcantly reduce the forecasting error as compared to cOLS. The robust utility learning framework enables us to effectively close the loop around smart building occupants by providing the foundation for learning a decision-making model that can be integrated into the incentive or control design process. While we apply the method to smart building social game data, it can be applied more generally to scenarios with the task of inverse modeling of competitive agents and provides a useful tool for many smart infrastructure applications where learning decision–making behavior is crucial.
ACKNOWLEDGMENT
We thank Mr. Christopher Hsu, Applications Programmer at CREST laboratory, who developed and deployed the web portal application of the social game at UC Berkeley.
APPENDIX
Proof of Proposition 1. Suppose the assumptions hold. The constraints for each player do not depend on other players’

choice variables. We can hold x∗−i ﬁxed and apply Proposition 3.3.2 [11] to the i-th player’s optimization problem max fi(xi, x∗−i) | xi ∈ Ci . Since each fi is concave and each Ci is a convex set, x∗i is a global optimum of the i-th player’s optimization problem under the assumptions. Since this is true for each of the i ∈ {1, . . . , n} players, x∗ is a
Nash equilibrium.
REFERENCES
[1] P. Bolton and M. Dewatripont, Contract theory. MIT press, 2005. [2] J.-J. Laffont and D. Martimort, The Theory of Incentives: The Principal–
Agent Model. Princeton University Press, 2002. [3] L. J. Ratliff, “Incentivizing efﬁciency in societal-scale cyber-physical
systems,” Ph.D. dissertation, University of California, Berkeley, 2015. [4] O. E. P. Ofﬁce, “Operational excellence program ofﬁce progress report—
toward a sustainable future,” University of California Berkeley, Tech. Rep., April 2015. [5] A. Aswani and C. Tomlin, “Incentive design for efﬁcient building quality of service,” in Proc. 50th Annu. Allerton Conf. Communication, Control, and Computing, 2012, pp. 90–97. [6] M. Jin, N. Bekiaris-Liberis, K. Weekly, C. J. Spanos, and A. M. Bayen, “Occupancy detection via environmental sensing,” IEEE Transactions on Automation Science and Engineering, pp. 1–13, 2016. [7] M. Jin, W. Feng, P. Liu, C. Marnay, and C. Spanos, “Mod-dr: Microgrid optimal dispatch with demand response,” Applied Energy, vol. 187, pp. 758–776, 2017. [8] L. J. Ratliff, R. Dong, H. Ohlsson, and S. S. Sastry, “Incentive design and utility learning via energy disaggregation,” in Proc. 19th World Congress of the Int. Federation of Automatic Control, 2014. [9] J. B. Rosen, “Existence and uniqueness of equilibrium points for concave n-person games,” Econometrica, vol. 33, no. 3, p. 520, 1965. [10] L. J. Ratliff, S. A. Burden, and S. S. Sastry, “On the Characterization of Local Nash Equilibria in Continuous Games,” IEEE Trans. on Autom. Control, vol. 61, no. 8, pp. 2301–2307, 2016. [11] D. P. Bertsekas, Nonlinear programming. Athena Scientiﬁc, 1999. [12] L. J. Ratliff, M. Jin, I. C. Konstantakopoulos, C. Spanos, and S. S. Sastry, “Social Game for Building Energy Efﬁciency: Incentive Design,” in Proc. 52nd Allerton Conf. Communication, Control, and Computing, 2014. [13] M. Jin, L. J. Ratliff, I. Konstantakopoulos, C. Spanos, and S. Sastry, “Rest: a reliable estimation of stopping time algorithm for social game experiments,” in Proceedings of the ACM/IEEE Sixth International Conference on Cyber-Physical Systems. ACM, 2015, pp. 90–99. [14] A. Keshavarz, Y. Wang, and S. Boyd, “Imputing a convex objective function,” in IEEE Int. Symp. Intelligent Control. IEEE, 2011, pp. 613–619. [15] D. A. Freedman, Statistical Models: Theory and Practice. Cambridge University Press, 2009. [16] F. Cribari-Neto, “Asymptotic inference under heteroskedasticity of unknown form,” Computational Statistics & Data Analysis, vol. 45, no. 2, pp. 215–233, 2004. [17] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning, Data Mining, Interference, and Prediction. Springer, 2009. [18] R. Tibshirani and K. Knight, “Model search and inference by bootstrap ”bumping”,” J. Comp. and Graph. Statistics, vol. 8, no. 4, pp. 671–686, 1999. [19] J. H. Friedman, “Greedy function approximation: a gradient boosting machine,” Ann. Statistics, pp. 1189–1232, 2001. [20] C. M. Hurvich, J. S. Simonoff, and C.-L. Tsai, “Smoothing parameter selection in nonparametric regression using an improved akaike information criterion,” J. Royal Statistical Society: Series B (Statistical Methodology), vol. 60, no. 2, pp. 271–293, 1998. [21] H. Prakash, R. Narayanam, D. Garg, and Y. Narahari, Game Theoretic Problems in Network Economics and Mechanism Design Solutions. Springer London, 2009. [22] S. Berry, J. Levinsohn, and A. Pakes, “Automobile prices in market equilibrium,” Econometrica, vol. 63, no. 4, pp. 841–890, 1995. [23] D. Bertsimas, V. Gupta, and I. C. Paschalidis, “Data-driven estimation in equilibrium using inverse optimization,” Mathematical Programming, vol. 153, no. 2, pp. 595–633, 2015. [24] S. Gabriel, A. Conejo, J. Fuller, B. Hobbs, and C. Ruiz, Complementarity Modeling in Energy Markets, ser. Int. Series Operations Research & Management Science. Springer-Verlag, 2013.

[25] A. Ledvina and R. Sircar, “Dynamic bertrand oligopoly,” Applied Mathematics & Optimization, vol. 63, no. 1, pp. 11–44, 2011.
[26] R. J. Aumann et al., “Subjectivity and correlation in randomized strategies,” J. Mathematical Economics, vol. 1, no. 1, pp. 67–96, 1974.
[27] M. Jin, R. Jia, Z. Kang, I. C. Konstantakopoulos, and C. Spanos, “Presencesense: Zero-training algorithm for individual presence detection based on power monitoring,” in Proc. 1st ACM Conf. Embedded Systems for Energy–Efﬁcient Buildings, 2014, pp. 1–10.
[28] M. Jin, R. Jia, and C. Spanos, “Virtual occupancy sensing: Using smart meters to indicate your presence,” IEEE Transactions on Mobile Computing, vol. PP, no. 99, p. 1, 2017.
[29] X. Jiang, S. Dawson-Haggerty, P. Dutta, and D. Culler, “Design and implementation of a high-ﬁdelity ac metering network,” in Proc. Inter. Conf. on Information Processing in Sensor Networks. IEEE, 2009, pp. 253–264.
[30] A. Aswani, N. Master, J. Taneja, V. Smith, A. Krioukov, D. Culler, and C. Tomlin, “Identifying models of HVAC systems using semi-parametric regression,” in Proc. of the American Control Conf., 2012, pp. 3675– 3680.
[31] M. Jin, N. Bekiaris-Liberis, K. Weekly, C. Spanos, and A. Bayen, “Sensing by proxy: Occupancy detection based on indoor co2 concentration,” UBICOMM 2015, pp. 1–14, 2015.
[32] M. Boman, P. Davidsson, N. Skarmeas, K. Clark, and R. Gustavsson, “Energy saving and added customer value in intelligent buildings,” in Third Int. Conf. Practical Application of Intelligent Agents and MultiAgent Technology, 1998, pp. 505–517.
[33] D. Bourgeois, C. Reinhart, and I. Macdonald, “Adding advanced behavioural models in whole building energy simulation: A study on the total energy impact of manual and automated lighting control,” Energy and Buildings, vol. 38, no. 7, pp. 814–823, 2006.
[34] Z. Song, “Collaborative building control to optimize energy saving and improve occupants’ experience,” ASHRAE Trans., vol. 119, no. AA1, 2013.
[35] M. Bang, C. Torstensson, and C. Katzeff, The PowerHouse: A Persuasive Computer Game Designed to Raise Awareness of Domestic Energy Consumption. Springer Berlin Heidelberg, 2006.
[36] J. Simon, M. Jahn, and A. Al-Akkad, “Saving energy at work: The design of a pervasive game for ofﬁce spaces,” in Proc. 11th Int. Conf. Mobile and Ubiquitous Multimedia, 2012.
[37] S. Li, K. Deng, and M. Zhou, “Social incentive policies to engage commercial building occupants in demand response,” in IEEE Inter. Conf. Automation Science and Engineering, Aug 2014, pp. 407–412.
[38] G. Taguchi, E. A. Elsayed, and T. C. Hsiang, Quality engineering in production systems. McGraw-Hill College, 1989.
[39] A. Tversky and D. Kahneman, “The framing of decisions and the psychology of choice,” Science, vol. 211, no. 4481, pp. 453–458, 1981.
[40] M. Gendreau, “On the location of eigenvalues of off-diagonal constant matrices,” Linear Algebra and its Applications, vol. 79, pp. 99 – 102, 1986.
[41] R. J. Hyndman and A. B. Koehler, “Another look at measures of forecast accuracy,” Inter. J. forecasting, vol. 22, no. 4, pp. 679–688, 2006.
[42] S. D. Fla˚m, “Solving non-cooperative games by continuous subgradient projection methods,” in System Modelling and Optimization. Springer, 1990, pp. 115–123.
[43] I. C. Konstantakopoulos, L. Ratliff, M. Jin, C. Spanos, and S. S. Sastry., “Inverse Modeling of Non-Cooperative Agents via Mixture of Utilities,” in IEEE Conf. Decision and Control, 2016.
[44] R. Isermann and M. Mu¨nchhof, Identiﬁcation of Dynamic Systems: An Introduction with Applications. Springer, 2011.
[45] J. A. Laitner, “Energy efﬁciency: rebounding to a sound analytical perspective,” Energy Policy, vol. 28, no. 6–7, pp. 471—475, 2000.
[46] L. Schipper and M. Grubb, “On the rebound? feedback between energy intensities and energy uses in IEA countries,” Energy Policy, vol. 28, no. 6–7, pp. 367–388, 2000.

