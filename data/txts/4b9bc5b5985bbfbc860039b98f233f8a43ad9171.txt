arXiv:1707.03351v3 [math.NA] 22 May 2018

Solving PDE problems with uncertainty using neural-networks
Yuehaw Khoo∗ Jianfeng Lu † Lexing Ying‡
May 24, 2018
Abstract The curse of dimensionality is commonly encountered in numerical partial diﬀerential equations (PDE), especially when uncertainties have to be modeled into the equations as random coeﬃcients. However, very often the variability of physical quantities derived from a PDE can be captured by a few features on the space of the coeﬃcient ﬁelds. Based on such an observation, we propose using a neural-network (NN) based method to parameterize the physical quantity of interest as a function of input coeﬃcients. The representability of such quantity using a neuralnetwork can be justiﬁed by viewing the neural-network as performing time evolution to ﬁnd the solutions to the PDE. We further demonstrate the simplicity and accuracy of the approach through notable examples of PDEs in engineering and physics.
1 Introduction
Uncertainty quantiﬁcations in physical and engineering applications often involve the study of partial diﬀerential equations (PDE) with random coeﬃcient ﬁeld. To understand the behavior of a system in the presence of uncertainties, one can extract PDE-derived physical quantities as functionals of the coeﬃcient ﬁelds. This can potentially require solving the PDE an exponential number of times numerically even with a suitable discretization of the PDE domain, and of the range of random variables. Fortunately in most PDE applications, often these functionals depend only on a few characteristic “features” of the coeﬃcient ﬁelds, allowing them to be determined from solving the PDE a limited number of times.
A commonly used approach for uncertainty quantiﬁcations is Monte-Carlo sampling. An ensemble of solutions is built by repeatedly solving the PDE with
∗Department of Mathematics, Stanford University, Stanford, CA 94305, USA (ykhoo@stanford.edu).
†Department of Mathematics, Department of Chemistry and Department of Physics, Duke University, Durham, NC 27708, USA (jianfeng@math.duke.edu).
‡Department of Mathematics and ICME, Stanford University, Stanford, CA 94305, USA (lexing@stanford.edu).
1

diﬀerent realizations of the coeﬃcient ﬁeld. Then physical quantities of interest,

for example, the mean of the solution at a given location, can be computed from

the ensemble of solutions. Although being applicable in many situations, the

computed quantity is inherently noisy. Moreover, this method lacks the ability to

obtain new solutions if they are not sampled previously. Other approaches exploit

the low underlying dimensionality assumption in a more direct manner. For

example the stochastic Galerkin method [17, 20] expands the random solution

using certain preﬁxed basis functions (i.e. polynomial chaos [22, 23]) on the

space of random variables, thereby reducing the high dimensional problem to

a few deterministic PDEs. Such type of methods requires careful treatment of

the uncertainty distributions, and since the basis used is problem independent,

the method could be expensive when the dimensionality of the random variables

is high. There are data-driven approaches for basis learning such as applying

Karhunen-Lo`eve expansion to PDE solutions from diﬀerent realizations of the

PDE [4]. Similarly to the related principal component analysis, such linear

dimension-reduction techniques may not fully exploit the nonlinear interplay

between the random variables. At the end of day, the problem of uncertainty

quantiﬁcation is one of characterizing the low-dimensional structure of the

coeﬃcient ﬁeld that gives the observed quantities.

On the other hand, the problem of dimensionality reduction has been central

to the ﬁelds of statistics and machine learning. The fundamental task of regression

seeks to ﬁnd a function hθ parameterized by a parameter vector θ ∈ Rp such

that

f (a) ≈ hθ(a), a ∈ Rq.

(1)

However, choosing a suﬃciently large class of approximation functions without the issue of over-ﬁtting remains a delicate business, for example when choosing the set of basis {φk(a)} such that f (a) = k βkφk(a) in linear regression. In the last decade, deep neural-networks have demonstrated immense success in solving a variety of diﬃcult regression problems related to pattern recognitions [10, 15, 19]. A key advantage of using neural-network is that it bypasses the traditional need to handcraft basis for spanning f (a) as in linear regression but instead, directly learns an approximation that satisﬁes (1) in a data-driven way. The performance of neural-network in machine learning applications, and more recently in physical applications such as representing quantum many-body states (e.g. [3, 21]), encourages us to study its use in the context of solving PDE with random coeﬃcients. More precisely, we want to learn f (a) that maps the random coeﬃcient vector a in a PDE to some physical quantities described by the PDE.
Our approach to solving quantities arise from PDE with randomness is conceptually simple, consisting of the following steps:

• Sample the random coeﬃcients (a in (1)) of the PDE from a user-speciﬁed distribution. For each set of coeﬃcients, solve the deterministic PDE to obtain the physical quantity of interest (f (a) in (1)).

• Use a neural-network as the surrogate model hθ(a) in (1) and train it using the previously obtained samples.

2

• Validate the surrogate forward model with more samples. The neural network is now ready for applications.
Though being a simple method, to the best of our knowledge, dimension reduction based on neural-network representation has not been adapted to solving PDE with uncertainties. We demonstrate the success of neural-network in two important PDEs that have wide applications in physics and engineering. In particular, we consider solving for the eﬀective conductance in inhomogeneous media and the ground state energy of a nonlinear Schr¨odinger equation (NLSE) having inhomogeneous potential. These quantities are f in (1) that we want to learn as a function of a, where a is the random conductivity coeﬃcient or the random potential. The main contributions of our work are
• We provide theoretical guarantees on neural-network representation of f (a) through explicit construction for the parametric PDE problems under study;
• We show that even a rather simple neural-network architecture can learn a good representation of f (a) through training.
We note that our work is diﬀerent from [7, 12, 13, 16, 18], which solve deterministic PDE numerically using a neural-network. The goal of these works is to parameterize the solution of a deterministic PDE using neural-network and use optimization methods to solve for the PDE solution. It is also diﬀerent from [8] where a deterministic PDE is solved as a stochastic control problem using neural-network. In this paper, the function that we want to parameterize is over the coeﬃcient ﬁeld of the PDE.
The advantages of having an explicitly parameterized approximation to f (·) are numerous, which we will only list a couple here. First, the neural-network parameterized function can serve as a surrogate forward model for generating samples cheaply for statistical analysis. Second, the task of optimizing some function of the physical quantity with respect to the PDE coeﬃcients in engineering design problems can be done with the help of a gradient calculated from the neural-network. To summarize, obtaining a neural-network parametrization could limit the use of expensive PDE solvers in applications.
The paper is organized as followed. In Section 2, we provide background on the two PDEs of interest. In Section 3, the theoretical justiﬁcation of using NN to represent the physical quantities derived from the PDEs introduced in Section 2, is provided. In Section 4, we describe the neural-network architecture for handling these PDE problems and report the numerical results. We ﬁnally conclude in Section 5.
2 Two examples of parametric PDE problems
This section introduces the two PDE models – the linear elliptic equation [14] and the nonlinear Schro¨dinger equation [11] – we want to solve for. Elliptic equations are commonly used to study steady heat conduction in a given material. When
3

the material has inhomogeneities (modeled as random conductivity coeﬃcients in the elliptic equation), one typically wants to understand the eﬀective conductivity of the material. NLSE is used to understand light propagation in waveguide and also the quantum mechanical phenomena where bosonic particles highly concentrate in the lowest-energy state (Bose-Einstein condensation). We want to study how the energy of such ground state behaves when the NLSE is subjected to random potential ﬁeld. Therefore, we focus on the map from the coeﬃcient ﬁeld of these PDEs to their relevant physical quantities. In both of the PDEs, the boundary condition is taken to be periodic for simplicity.

2.1 Eﬀective coeﬃcients for inhomogeneous elliptic equation

Our ﬁrst example will be ﬁnding the eﬀective conductance/coeﬃcient in a non-homogeneous media. For this, we consider a class of coeﬃcient functions

A = {a ∈ L∞([0, 1]d) | λ1 ≥ a(x) ≥ λ0 > 0},

(2)

for some ﬁxed constants λ0 and λ1. Fix a direction ξ ∈ Rd with ξ 2 = 1 ( · 2 is the Euclidean norm). We want to obtain the eﬀective conductance functional Aeﬀ : A → R deﬁned by

Aeﬀ (a) = min

a(x)

∇u(x) + ξ

2 2

dx.

(3)

u(x) [0,1]d

The minimizer ua(x) of this variational problem (here the subscript “a” in ua is used to denote its dependence on the coeﬃcient ﬁeld a) satisﬁes the following elliptic partial diﬀerent equation

− ∇ · (a(x)(∇u(x) + ξ)) = 0, x ∈ [0, 1]d

(4)

with periodic boundary condition. With ua available, one obtains

Aeﬀ (a) =

a(x)

∇ua(x) + ξ

2 2

dx.

[0,1]d

In practice, to parameterize Aeﬀ as a functional of the coeﬃcient ﬁeld a(·), we

discretize the domain using a uniform grid with step size h = 1/n and grid points

denoted by xi = ih, where the multi-index i ∈ {(i1, . . . , id)}, 1 ≤ i1, . . . , id ≤ n.

In this way, we can think about the coeﬃcient ﬁeld a(x) and the solution u(x)

represented on the grid points both as vectors with length nd. More precisely,

we redeﬁne

A

=

{a

∈

nd
R

|

ai

∈

[λ0, λ1],

∀i}

and discretize the term −∇ · (a(x)∇u(x)) using central diﬀerence

− d ai+ek/2(ui+ek − ui) − ai−ek/2(ui − ui−ek ) , (5) h2
k=1

4

for each i, where {ek}dk=1 denotes the canonical basis in Rd and each value of a at a half grid point is obtained by averaging the values at its two nearest grid
points. Then the discrete version of (4) is the linear system Lau = ba with

d −ai+e /2ui+e + (ai−e /2 + ai+e /2)ui − ai−e /2ui−e

(Lau)i :=

k

k

k

k

k

k (6)

h2

k=1

d ξk(ai+e /2 − ai−e /2)

(ba)i :=

k

k.

(7)

h

k=1

From (3), one can see that the discrete version of the eﬀective conductivity, also denoted by Aeﬀ (a) for a ∈ Rnd , can be obtained from solving the discrete variational problem

hd Aeﬀ (a) = 2 um∈Rinnd E(u; a), E(u; a) := 2 (u Lau − 2u ba + a 1), (8)

or equivalently, solving ua from Lau = ba and setting

Aeﬀ (a) = hd(ua Laua − 2ua ba + a 1).

We stress that in order to simplify the notations, we use u and a as vectors in Rnd , although they were previously used as functions in (3). The interpretation
of u and a as functions or vectors should be clear from the context.

2.2 NLSE with inhomogeneous background potential
For the second PDE example, we want to ﬁnd the ground state energy E0 of a nonlinear Schr¨odinger equation with potential a(x):

− ∆u(x) + a(x)u(x) + σu(x)3 = E0u(x), x ∈ [0, 1]d, s.t.

u(x)2 dx = 1.

[0,1]d

(9)

We take σ = 2 in this work and thus consider a defocusing cubic Schr¨odinger

equation, which can be understood as a model for soliton in nonlinear photonics

or Bose-Einstein condensate with inhomogeneous media. Similar to (6), we solve

the discretized version of the NLSE

(Lu)i+aiui+σu3i = E0ui,

nd
u2i hd = 1,
i=1

d −ui+e + 2ui − ui−e

(Lu)i :=

k h2 k .

k=1

(10)

Due to the nonlinear cubic term, it is more diﬃcult to solve for the NLSE

numerically compare to (4). Therefore in this case, the value of having a surrogate

model of E0 as a function of a is more signiﬁcant. We note that the solution u to

(10) (and thus E0) can also be obtained from the following variational problem

min

σ u Lu + u diag(a)u +

u4,

u∈Rnd : u 22=nd

2

i

i

(11)

where the diag(·) operator forms a diagonal matrix given a vector.

5

3 Theoretical justiﬁcation of deep neural-network representation

The physical quantities introduced in Section 2 are determined through the solution of the PDEs given the coeﬃcient ﬁeld. Rather than solving the PDE, we will prove that the map from coeﬃcient ﬁeld to such quantities can be represented using convolutional NNs. The main idea is to view the solution u of the PDE as being obtained via time evolution, where each layer of the NN corresponds to the solution at discrete time step. In other words, mapping the input a from the ﬁrst layer to last layer in the NN resembles the time-evolution of a PDE with discrete time-steps. We focus here on the case of solving elliptic equations with inhomogeneous coeﬃcients. Similar line of reasoning can be used to demonstrate the representability of the ground state-energy E0 as a function of a using an NN.

Theorem 1. Fix an error tolerance > 0, there exists a neural-network hθ(·)
with O(nd) hidden nodes per-layer and O(( λλ10 + 1) n2 ) layers such that for any a ∈ A = {a ∈ Rnd | ai ∈ [λ0, λ1], ∀i}, we have

|hθ(a) − Aeﬀ (a)| ≤ λ1.

(12)

Note that due to the ellipticity assumption a ∈ A, the eﬀective conductivity is bounded from below by Aeﬀ (a) ≥ λ0 > 0. Therefore the theorem immediately implies a relative error bound

|hθ(a) − Aeﬀ (a)| ≤ λ1 . (13)

Aeﬀ (a)

λ0

We illustrate the main idea of the proof in the rest of the section, the technical details of the proof are deferred to the supplementary materials.
The ﬁrst observation is that, due to the variational characterization (8), in order to get Aeﬀ (a) we may minimize E(u; a) over the solution space u, using e.g., steepest descent:

um+1 = um − ∆t ∂E(um; a)

∂u

(14)

= um − ∆t Laum − ba ,

where ∆t is a step size chosen suﬃciently small to ensure descent of the energy. Note that the optimization problem is convex due to the ellipticity assumption (2) of the coeﬃcient ﬁeld a (which ensures u Lau > 0 except for u = 1) with Lipshitz continuous gradient, therefore the iterative scheme converges to the minimizer with proper choice of step size for any initial condition. Thus we can choose u0 = 0.
Now we identify the iteration scheme in (14) with a convolutional NN architecture (Fig. 1) by viewing m as an index of the NN layers. The input of the NN is the vector a ∈ Rnd , and the hidden layers are used to map between the

6

Figure 1 Construction of the NN in the proof of Theorem 1. The NN takes the
coeﬃcient ﬁeld a as an input and the convolutional and local nonlinearity layers are used to map from um, a to um+1, a, m = 0, M − 1. At uM , local convolutions and nonlinearity are used to obtained E(uM ; a).

consecutive pairs of (d + 1)-tensors Uim0i1...id and Uim0i+1.1..id . The zeroth dimension for each tensor U m is the channel dimension and the last d dimensions are the spatial dimensions. If we let the channels in each U m be consisted of a copy of a and a copy of um, e.g., let

U0mi1...id = a(i1,...,id), U1mi1...id = um (i1,...,id),

(15)

in light of (14) and (6), one simply needs to perform local convolution (to
aggregate a locally) and nonlinearity (to approximate quadratic form of a and um) to get from U1mi1...id = um (i1,...,id) to U1mi1+..1.id = um (i1+,.1..,id); while the 0-channel is simply copied to carry along the information of a. Stopping at m = M layer and letting uM be the approximate minimizer of E(u; a), based on (8), we let E(uM ; a)
be an approximation to Aeﬀ (a). This architecture of NN to approximate the eﬀective conductance is illustrated in Fig. 1. Note that the architecture of NN
used in the proof resembles a deep ResNet [9], as the coeﬃcient ﬁeld a is passed
from the ﬁrst to the last layer. The detailed estimates of the approximation error
and the number of parameters will be deferred to the supplementary materials.
Let us point out that if we take the continuum time limit of the steepest
descent dynamics, we obtain a system of ODE

∂tu = −(Lau − ba),

(16)

which can be viewed as a spatially discretized PDE. Thus our construction of the neural network in the proof is also related to the work [16] where multiple layers of convolutional NN is used to learn and solve evolutionary PDEs. However, the goal of the neural network here is to approximate the physical quantity of interest as a functional of the (high-dimensional) coeﬃcient ﬁeld, which is quite diﬀerent from the view point of [16].
We also remark that the number of layers of the NN required by Theorem 1 is rather large. This is due to the choice of the (unconditioned) steepest descent algorithm as the engine of optimization to generate the neural network architecture used in the proof. Nevertheless, the number of parameters required in the NN representation is still much fewer than the exponential scaling in terms of the input dimension of a shallow network [2] from universal approximation theorem. Moreover, with a better preconditioner such as the algebraic multigrid [24] for

7

the gradient, we can eﬀectively reduce the number of layers to O(1) and thus achieves an optimal count of parameters involved in the NN; the details will be left for future works. In practice, as shown in the next section by actual training of parametric PDEs, the neural network architecture can be much simpliﬁed while maintaining a good approximation to the quantity of interest.

4 Proposed network architecture and numerical results
In this section, based on the discussion in Section 3, we propose using convolutional NN to approximate the physical quantities given by the PDE with a periodic boundary condition. We ﬁrst describe the architecture of the neural network in Section 4.1, then the implementation details and numerical results are provided in Section 4.2 and 4.3 respectively.

4.1 Architecture

In Fig. 2, we show the architecture for the 2D case with domain being a unit
square, though it can be generalized to solving PDEs in any dimensions. The input to the NN is an n × n matrix representing the coeﬃcient ﬁeld a ∈ Rn2 on grid points, and the output of the network gives physical quantity of interest
from the PDE. The main part of the network are convolutional layers with ReLU
being the nonlinearity. This extracts the relevant features of the coeﬃcient
ﬁeld around each grid point that contribute to the ﬁnal output. The use of a
sum-pooling followed by a linear map to obtain the ﬁnal output is based on the
translational symmetry of the function f (·) to be represented. More precisely, let aτij1τ2 := a(i+τ1)(j+τ2) where the additions are done on Zn. The output of the convolutional layer gives basis functions that satisfy

φ˜kij (aτ1τ2 ) = φ˜k(i−τ1)(j−τ2)(a), k = 1, . . . , α, i, j = 1, . . . , n, ∀τ1, τ2 = 1, . . . , n. (17)
When using the architecture in Fig. 2, for any τ1, τ2,

f (aτ1τ2 ) =

α

nn

βk

α

nn

φ˜kij (aτ1τ2 ) = βk

φ˜k(i−τ1)(j−τ2)(a)

k=1 i=1 j=1

k=1

α

nn

=

βkφk(a), φk :=

φ˜kij ,

i=1 j=1

(18)

k=1

i=1 j=1

where βk’s are the weights of the last densely connected layer. The summation over i, j comes from the sum-pooling operation. Therefore, (18) shows that the translational symmetry of f is preserved.
We note that all operations in Fig. 2 are standard except the padding operation. Typically, zero-padding is used to enlarge the size of the input in image classiﬁcation task, whereas we extend the input periodically due to the assumed periodic boundary condition.

8

Inputs 1@nxn

1@ 2n-1x2n-1

α@nxn

α@1x1

Output 1@1x1

Padding Periodic

Conv. ReLU nxn kernel

Pooling Sum

Dense Linear

Figure 2 Single convolutional layer neural network for representing translational invariant function.

4.2 Implementation
The neural-network is implemented using Keras [5], an application programming interface running on top of TensorFlow [1] (a library of toolboxes for training neural-network). We use a mean-squared-error loss function. The optimization is done using the NAdam optimizer [6]. The hyper-parameter we tune is the learning rate, which we lower if the training error ﬂuctuates too much. The weights are initialized randomly from the normal distribution. The input to the neural-network is whitened to have unit variance and zero-mean on each dimension. The mini-batch size is always set to between 50 and 200.

4.3 Numerical examples

4.3.1 Eﬀective conductance

For the case of eﬀective conductance, we assume the entries ai’s of a ∈ Rnd are independently and identically distributed according to U [0.3, 3] where U [λ0, λ1] denotes the uniform distribution on the interval [λ0, λ1]. The results of learning the eﬀective conductance function are presented in Table 1. To get the training samples, we solve the linear system in (6). We use the same number of samples for training and validation. Both the training and validation error are measured by

k(hθ(ak) − Aeﬀ (ak))2 , k Aeﬀ (ak)2

(19)

where ak’s can either be the training or validation samples sampled from the same distribution and hθ is the neural-network-parameterized approximation function. We remark that although incorporating domain knowledge in PDE to build a sophisticated neural-network architecture would likely boost the approximation quality, such as what we do in the constructive proof for Theorem 1, our results in Table 1 show that even a simple network as in Fig. 2 can already give decent results with near 10−3 accuracy. The simplicity of the NN is particularly

9

important when using it as a surrogate model of the PDE to generate samples cheaply.
Table 1 Error in approximating the eﬀective conductance function Aeﬀ (a) in 2D. The mean and standard deviation of the eﬀective conductance are computed from the samples in order to show the variability. The sample sizes for training and validation are the same.

nα

Training error

Validation

error

Average Aeﬀ

No. of samples

No. of parameters

8 16 2.4 × 10−3 3.0 × 10−3 1.86 ± 0.10 1.2 × 104 16 16 2.1 × 10−3 2.2 × 10−3 1.87 ± 0.052 2.4 × 104

1057 4129

Before concluding this subsection, we use the exercise of determining the eﬀective conductance in 1D to provide another motivation for the usage of a neural-network. Unlike the 2D case, in 1D the eﬀective conductance can be expressed analytically as the harmonic mean of ai’s:

Aeﬀ (a) =

1 n 1 −1 .
n i=1 ai

(20)

This function indeed approximately corresponds to the deep neural-network shown in Fig. 3. The neural-network is separated into three stages. In the ﬁrst stage, the approximation to function 1/ai is constructed for each ai by applying a few convolution layers with size 1 kernel window. In this stage, the channel size for these convolution layers is chosen to be 16 except the last layer since the output of the ﬁrst stage should be a vector of size n. In the second stage, a layer of sum-pooling with size n window is used to perform the summation in (20), giving a scalar output. The third and ﬁrst stages have the exact same architecture except the input to the third stage is a scalar. 2560 samples are used for training and another 2560 samples are used for validation. We let ai ∼ U[0.3, 1.5], giving an eﬀective conductance of 0.77 ± 0.13 for n = 8. We obtain 4.9 × 10−4 validation error with the neural-network in Fig. 3 while with the network in Fig. 2, we get 5.5 × 10−3 accuracy with α = 16. As a check, in Fig. 4 we show that the output from the ﬁrst stage is well-ﬁtted by the reciprocal function.

4.3.2 Ground state energy of NLSE
We next focus on the 2D NLSE example (10) with σ = 2. The goal here is to obtain a neural-network parametrization for E0(a), with input now being a ∈ Rn2 with i.i.d. entries distributed according to U [1, 16]. As mentioned before, solving for the ground state energy in NLSE is more expensive than solving for the eﬀective conductance as in this case, we need to solve a system of nonlinear equations. In order to generate training samples, for each realization of a, the nonlinear eigenvalue problem (10) is solved via a homotopy method. In such

10

Stage 1: Reciprocal

Stage 2: Summation

Stage 3: Reciprocal

Inputs 1@1xn

16@1xn 16@1xn 16@1xn 1@1xn

1@1x1

16@1x1

16@1x1

16@1x1

Conv. Linear 1@1x1

Conv. Conv. Conv. Conv. ReLU ReLU ReLU Linear Pooling 1x1 kernel 1x1 kernel 1x1 kernel 1x1 kernel Sum

Conv. Conv. Conv. Conv. ReLU ReLU ReLU linear 1x1 kernel 1x1 kernel 1x1 kernel 1x1 kernel

Figure 3 Neural-network architecture for approximating Aeﬀ (a) in the 1D case. Although the layers in third stage are essentially densely-connected layers, we still identify them as convolution layers to reﬂect the symmetry between the ﬁrst and third stages.

y

12.5 10.0
7.5 5.0 2.5 0.0 −2.5 −5.0

First-stage's output β1/x + β2
0.4 0.6 0.8 1.0 1.2 1.4 1.6 x

Figure 4 The ﬁrst stage’s output of the neural-network in Fig. 3 ﬁtted by β1/x + β2. The training result agrees well with the analytical structure of the solution to the 1D eﬀective conductance.

11

method, a sequence of NLSE Lu + aiui + su3i = E0ui ∀i with the normalization constraint on u is solved with s = s1, . . . , sK where 0 = s1 < s2 < . . . < sK = σ. First, the case s = 0 is solved as a standard eigenvalue problem. Then for each si with i > 1, Newton’s method is used to solved the NLSE and u, E0 obtained with s = si will be used to warm start the Newton’s iteration for si+1. In our example, we change s from 0 to 2 with a step size equals to 0.4. The results are presented in Table 2.
Table 2 Error in approximating the lowest energy level E0(a) for n = 8, 16 discretization.

nα

Training error

Validation

error

Average E0

No. of samples

No. of parameters

8 5 4.9 × 10−4 5.0 × 10−4 10.48 ± 0.51

4800

16 5 1.5 × 10−4 1.5 × 10−4 10.46 ± 0.27 1.05 × 104

331 1291

5 Conclusion
In this note, we present method based on deep neural-network to solve PDE with inhomogeneous coeﬃcient ﬁelds. Physical quantities of interest are learned as a function of the coeﬃcient ﬁeld. Based on the time-evolution technique for solving PDE, we provide theoretical motivation to represent these quantities using an NN. The numerical experiments on elliptic equation and NLSE show the eﬀectiveness of simple convolutional neural network in parameterizing such function to 10−3 accuracy. We remark that while many questions should be asked, such as what is the best network architecture and what situations can this approach handle, the goal of this short note is simply to suggest neural-network as a promising tool for model reduction when solving PDEs with uncertainties.
References
[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoﬀrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Josh Kudlur, Manjunath Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
12

[2] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39:930–945, 1993.
[3] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artiﬁcial neural networks. Science, 355(6325):602–606, 2017.
[4] Mulin Cheng, Thomas Y Hou, Mike Yan, and Zhiwen Zhang. A data-driven stochastic method for elliptic PDEs with random coeﬃcients. SIAM/ASA Journal on Uncertainty Quantiﬁcation, 1(1):452–493, 2013.
[5] Franc¸ois Chollet. Keras (2015). URL http://keras. io, 2017.
[6] Timothy Dozat. Incorporating Nesterov momentum into ADAM. In Proc. ICLR Workshop, 2016.
[7] Weinan E and Bing Yu. The deep Ritz method: A deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6:1–12, 2018.
[8] Jiequn Han, Arnulf Jentzen, and Weinan E. Overcoming the curse of dimensionality: Solving high-dimensional partial diﬀerential equations using deep learning. arXiv preprint arXiv:1707.02568, 2017.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[10] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
[11] Tosio Kato. Nonlinear schr¨odinger equations. In Schr¨odinger operators, pages 218–263. Springer, 1989.
[12] Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving for high dimensional committor functions using artiﬁcial neural networks. arXiv preprint arXiv:1802.10275, 2018.
[13] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artiﬁcial neural networks for solving ordinary and partial diﬀerential equations. IEEE Transactions on Neural Networks, 9(5):987–1000, 1998.
[14] Stig Larsson and Vidar Thom´ee. Partial diﬀerential equations with numerical methods, volume 45. Springer Science & Business Media, 2008.
[15] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
[16] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-net: Learning PDEs from data. arXiv preprint arXiv:1710.09668, 2017.
13

[17] Hermann G Matthies and Andreas Keese. Galerkin methods for linear and nonlinear elliptic stochastic partial diﬀerential equations. Computer methods in applied mechanics and engineering, 194(12):1295–1331, 2005.
[18] Keith Rudd and Silvia Ferrari. A constrained integration (CINT) approach to solving partial diﬀerential equations using artiﬁcial neural networks. Neurocomputing, 155:277–285, 2015.
[19] Ju¨rgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117, 2015.
[20] George Stefanou. The stochastic ﬁnite element method: past, present and future. Computer Methods in Applied Mechanics and Engineering, 198(9):1031–1051, 2009.
[21] Giacomo Torlai and Roger G Melko. Learning thermodynamics with Boltzmann machines. Physical Review B, 94(16):165134, 2016.
[22] Norbert Wiener. The homogeneous chaos. American Journal of Mathematics, 60(4):897–936, 1938.
[23] Dongbin Xiu and George Em Karniadakis. The Wiener–Askey polynomial chaos for stochastic diﬀerential equations. SIAM journal on scientiﬁc computing, 24(2):619–644, 2002.
[24] Jinchao Xu and Ludmil Zikatanov. Algebraic multigrid methods. Acta Numerica, 26:591–721, 2017.
14

Supplementary material: Proof of representability of eﬀective conductance by NN

As mentioned previously in Section 3, the ﬁrst step of constructing an NN to represent the eﬀective conductance is to perform time-evolution iterations in the form of (14). However, since at each step we need to approximate the map from um to um+1 in (14) using NN, the process of time-evolution is similar to applying noisy gradient descent on E(u; a). More precisely, after performing a step of gradient descent update, the NN approximation incur noise to the update, i.e.

v0 = u0 = 0, um+1 = vm − ∆t∇E(vm), vm+1 = um+1 + ∆tεm+1. (21)

Here E(u; a) is abbreviated as E(u), and εm+1 is the error for each layer of the NN in approximating each exact time-evolution iteration um+1. To be sure, instead of um, the object that is evolving in the NN as m changes is vm.

Assumption 1. We assume a ∈ A = {a ∈ Rnd | ai ∈ [λ0, λ1], ∀i} with λ0 > 0.

Under this assumption λa := La 2 and µa :=

1
†

satisfy

La 2

λa = O(λ1hd−2), µa = Ω(λ0hd).

(22)

Here for matrices · 2 denotes the spectral norm, h = 1/n.
Assumption 2. We assume the NN results an approximation error term εm+1 with properties

εm+1 2 ≤ c ∇E(vm) 2, 1 εm+1 = 0, m = 0, . . . M − 1,

(23)

when approximating each step of time-evolution.

Lemma 1. The iterations in (21) satisﬁes

E(vm+1) − E(vm) ≤ − ∆2t ∇E(vm) 22, (24)

if ∆t ≤ δ, δ =

1 − 2(11−c)

2 λ

with λa = (1 + 1c−2c )λa. Furthermore,

a

∆t M−1

∇E (vm+1)

2 2

≤ E(v0) − E(vM ) ≤ E(v0) − E(u∗).

(25)

2

m=0

Proof. From Lipshitz property of ∇E(u) (22),

E(vm+1) − E(vm) ≤ =

∇E (vm), vm+1 − vm + λ2a vm+1 − vm 22 ∇E(vm), vm − ∆t(∇E(vm) + εm+1) − vm

+ λ2a vm − ∆t(∇E(vm) + εm+1) − vm 22

15

= −∆t(1 − ∆t2λa ) ∇E(vm) 22

∆tλa m+1

m

λa∆t2 m 2

+∆t(1 −

)ε

2

, ∇E(v ) + 2

ε2

≤ −∆t(1 − ∆t2λa ) ∇E(vm) 22

+c∆t(1 − ∆t2λa + c∆2tλa ) ∇E(vm) 22

= −∆t (1 − c) − (1 − c + c2) ∆t2λa ∇E(vm) 22

1 − c + c2 ∆tλa

m2

= −∆t(1 − c) 1 − 1−c

2 ∇E(v ) 2

= −∆t(1 − c) 1 − ∆t2λa ∇E(vm) 22. (26)

Letting ∆t ≤ 1 − 1 2 , we get
2(1−c) λa

E(vm+1) − E(vm) ≤ − ∆2t ∇E(vm) 22. (27)

Summing the LHS and RHS gives (25). This concludes the lemma.

Theorem 2. If ∆t satisﬁes the condition in Lemma 1, given any E (v)| ≤ for M = O(( λλ210 + λ1) n2 ).
Proof. Since by convexity

> 0, |E(vM )−

E(u∗) − E(vm) ≥ ∇E(vm), u∗ − vm ,

(28)

along with Lemma 1,

E(vm+1) ≤ E(u∗) + ∇E(vm), vm − u∗ − ∆2t ∇E(vm) 22

= E(u∗) + 2∆1 t 2∆t ∇E(vm), vm − u∗ − ∆t2 ∇E(vm) 22

+

vm − u∗

2 2

−

vm − u∗

2 2

= E(u∗) + 2∆1 t ( vm − u∗ 22 − vm − ∆t∇E(vm) − u∗ 22) = E (u∗) + 2∆1 t ( vm − u∗ 22 − vm+1 − ∆tεm+1 − u∗ 22) = E (u∗) + 2∆1 t ( vm − u∗ 22 − vm+1 − u∗ 22
+2∆t εm+1, vm+1 − u∗ − ∆t2 εm+1 22) = E (u∗) + 2∆1 t vm − u∗ 22 − vm+1 − u∗ 22 + ∆t2 εm+1 22
+2∆t εm+1, vm − u∗ − 2∆t εm+1, ∇E(vm)

≤ E (u∗) + 2∆1 t vm − u∗ 22 − vm+1 − u∗ 22 + ∆t2 εm+1 22 +2∆t εm+1 2( vm − u∗ 2 + ∇E (vm) 2)

≤ E (u∗) + 2∆1 t vm − u∗ 22 − vm+1 − u∗ 22 + ∆t2 εm+1 22

16

+2∆t(1 + 2/µa) εm+1 2 ∇E(vm) 2

≤ E (u∗) + 2∆1 t vm − u∗ 22 − vm+1 − u∗ 22 + c2∆t2 ∇E (vm) 22

+2c(1 + 2/µa)∆t

∇E (vm )

2 2

.

(29)

The last second inequality follows from (22), which implies Lau 2 ≥ µa u 2 if

u 1 = 0. More precisely, the fact that v0 = 0, ∇E(u) 1 = 0 (follows from the

form of La and ba deﬁned in (6)), and εm 1 = 0 ∀m (due to the assumption

in (23)) implies vm

1 = 0, hence

µa 2

vm − u∗

2≤

∇E(vm) − ∇E(u∗) 2 =

∇E(vm) 2. Reorganizing (29) we get

E(vm+1) − E(u∗)

1 ≤
2∆t

vm − u∗

2−

vm+1 − u∗

2 + c∆t c∆t + 2(1 +

2 )

2

2

µa

∇E (vm )

2 2

.

(30)

Summing both left and right hand sides results in

E(vM ) − E(u∗) ≤ 1 M−1 E(vm+1) − E(u∗) M
m=0

1 ≤
M

v0 − u∗ 22 + 2c c∆t + 2(1 + 2 ) (E(v0) − E(u∗)) (31)

2∆t

∆t

µa

where the second inequality follows from (25). In order to derive a bound for v0 − u∗ 22, we appeal to strong convexity property of E(u): E(v0) − E(u∗) ≥ ∇E(u∗), v0 − u∗ + µ2a v0 − u∗ 22 = µ2a v0 − u∗ 22 (32)
for 1, v0 − u∗ = 0. The last equality follows from the optimality of u∗. Then

E(vM ) − E(u∗) ≤ 1 M

1 µa∆t

2c

2

+ c∆t + 2(1 + )

∆t

µa

(E(v0) − E(u∗)). (33)

Since E(v0) = hd a 21 = O(λ1), along with λa = O(λ1hd−2) and µa = Ω(λ0hd) (Assumption 1), we establish the claim.

17

