Stochastic Spectral and Conjugate Descent Methods

arXiv:1802.03703v1 [math.OC] 11 Feb 2018

Dmitry Kovalev 1 Eduard Gorbunov 1 Elnur Gasanov 1 Peter Richta´rik 2 3 1

Abstract
The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.
1. Introduction
An increasing array of learning and training tasks reduce to optimization problem in very large dimensions. The stateof-the-art algorithms in this regime are based on randomized coordinate descent (RCD). Various acceleration strategies were proposed for RCD in the literature in recent years, based on techniques such as Nesterov’s momentum (Nesterov, 1983; Lee & Sidford, 2013; Fercoq & Richta´rik, 2015; Allen-Zhu et al., 2016; Nesterov & Stich, 2017), heavy ball momentum (Polyak, 1964; Loizou & Richta´rik, 2017), importance sampling (Nesterov, 2012; Richta´rik & Taka´cˇ, 2016a), adaptive sampling (Csiba et al., 2015), random permutations (Lee & Wright, 2016), greedy rules (Nutini et al., 2015), mini-batching (Richta´rik & Taka´cˇ, 2016b), and locality breaking (Tu et al., 2017). These techniques enable faster rates in theory and practice.
1Moscow Institute of Physics and Technology, Dolgoprudny, Russia 2King Abdullah University of Science and Technology, Thuwal, Saudi Arabia 3University of Edinburgh, Edinburgh, United Kingdom. Correspondence to: Dmitry Kovalev <dakovalev1@mail.ru>, Peter Richta´rik <peter.richtarik@kaust.edu.sa, peter.richtarik@ed.ac.uk>.
February 10, 2018

In this paper we introduce a fundamentally new type of acceleration strategy for RCD which relies on the idea of enriching the set of (unit) coordinate directions {e1, e2, . . . , en} in Rn, which are used in RCD as directions of descent, via the addition of a few spectral or conjugate directions. The algorithms we develop and analyze in this paper randomize over this enriched larger set of directions.

1.1. The problem

For simplicity1, we focus on quadratic minimization

1

min f (x) = x Ax − b x,

(1)

x∈Rn

2

where A is an n × n symmetric and positive deﬁnite matrix. The optimal solution is unique, and equal to x∗ = A−1b.

1.2. Randomized coordinate descent

Applied to (1), RCD performs the iteration

xt+1 = xt − A:i xt − bi ei,

(2)

Aii

where at each iteration, i is chosen with probability pi > 0. It was shown by Leventhal & Lewis (2010) that if the probabilities are proportional to the diagonal elements of A (i.e., pi ∼ Aii), then the random iterates of RCD satisfy

E[ xt − x∗ 2A] ≤ (1 − ρ)t x0 − x∗ 2A,

where

ρ

=

λmin (A) Tr(A)

and

λmin(A)

is

the

minimal

eigenvalue

of A. That is, as long as the number of iterations t is at least

O Tr(A) log 1 ,

(3)

λmin(A)

we have E[ xt − x∗ 2A] ≤ . Note that Tr(A)/λmin(A) ≥ n, and that this can be arbitrarily larger that n.

1.3. Stochastic descent
Recently, Gower & Richta´rik (2015a) developed an iterative “sketch and project” framework for solving linear systems
1Many of our results can be extended to convex functions of the form f (x) = φ(Ax) − b x, where φ is a smooth and strongly convex function. However, due to space limitations, and the fact that we already have a lot to say in the special case φ(y) = 12 y 2, we leave these more general developments to a follow-up paper.

Stochastic Spectral and Conjugate Descent Methods

Method Name stochastic descent (SD) stochastic spectral descent (SSD) stochastic conjugate descent (SconD) randomized coordinate descent (RCD) stochastic spectral coordinate descent (SSCD) mini-batch SD (mSD) mini-batch SSCD (mSSCD) inexact SconD (iSconD) inexact SSD (iSSD)

Algorithm (4), Algorithm 1
Algorithm 2 read Section 2.2 (2), Algorithm 3
Algorithm 4 Algorithm 5 Algorithm 6 Algorithm 7 Algorithm 8

Rate (5), Lemma 1 (6), Theorem 2
Theorem 2 (3), (11)
(7), Theorem 8 Lemma 9
Theorem 10 Theorem 15 see Section 10.2

Table 1: Algorithms described in this paper.

Reference Gower & Richta´rik (2015a)
NEW NEW Gower & Richta´rik (2015a) NEW Richta´rik & Taka´cˇ (2017) NEW NEW NEW

and quadratic optimization problems; see also (Gower & Richta´rik, 2015b) for extensions. In the context of problem (1), and specialized to sketching matrices with a single column, their method takes the form

xt+1

=

xt

−

st

(Axt

−

b) st,

(4)

st Ast

where st ∈ Rn is a random vector sampled from some ﬁxed distribution D. In this paper we will refer to this method by the name stochastic descent (SD).

Note that xt+1 is obtained from xt by minimizing f (xt + hst) for h ∈ R and setting xt+1 = xt + hst. Further, note that RCD arises as a special case with D being a discrete probability distribution over the set {e1, . . . , en}. However, SD converges for virtually any distribution D, including
discrete and continuous distributions. In particular, Gower & Richta´rik (2015a) show that as long as Es∼D[H] is invertible, where H := sssAs , then SD converges as

O

1 log 1 ,

(5)

λmin(W)

where W := Es∼D[A1/2HA1/2] (see Lemma 1 for a more reﬁned result due to Richta´rik & Taka´cˇ (2017)). Rate of RCD in (3) can be obtained as a special case of (5).

1.4. Stochastic spectral descent

The starting point of this paper is the new observation that stochastic descent obtains the rate

1

O n log

(6)

in the special case when D is chosen to be the uniform distribution over the eigenvectors of A (see Theorem 2). For obvious reasons, we refer to this new method as stochastic spectral descent (SSD).
To the best of our knowledge, SSD was not explicitly considered in the literature before. We should note that SSD is fundamentally different from spectral gradient descent (Birgin et al., 2014; Barzilai & M., 1988), which refers to a

family of gradient descent methods with a special choice of stepsize depending on the spectrum of the Hessian of f .
The rate (6) does not merely provide an improvement on the rate of RCD given in (3); what is remarkable is that this rate is completely independent of the properties (such as conditioning) of A. Moreover, we show that this method is optimal among the class of stochastic descent methods (4) parameterized by the choice of the distribution D (see Theorem 8). Despite the attractiveness of its rate, SSD is not a practical method. This is because once we have the eigenvectors of A available, the optimal solution x∗ can be assembled directly without the need for an iterative method.
1.5. Stochastic conjugate descent
We extend all results discussed above for SSD, including the rate (6), to the more general class of methods we call stochastic conjugate descent (SconD), for which D is the uniform distribution over vectors v1, . . . , vn which are mutually A conjugate: vi Avj = 0 for i = j and vi Avi = 1.
1.6. Optimizing probabilities in RCD
The idea of speeding up RCD via the use of non-uniform probabilities was pioneered by Nesterov (2012) in the context of smooth convex minimization, and later built on by many authors (Richta´rik & Taka´cˇ, 2016a; Qu & Richta´rik, 2016; Allen-Zhu et al., 2016). In the case of non-accelerated RCD, and in the context of smooth convex optimization, the most popular choice of probabilities is to set pi ∼ Li, where Li is the Lipschitz constant of the gradient of the objective corresponding to coordinate i (Nesterov, 2012; Richta´rik & Taka´cˇ, 2016a). For problem (1), we have Li = Aii. Gower & Richta´rik (2015a) showed that the optimal probabilities for (1) can in principle be computed through semideﬁnite programming (SDP); however, no theoretical properties of the optimal solution of the SDP were given.
As a warm-up, we ﬁrst ask the following question: how important is importance sampling? More precisely, we investigate RCD with probabilities pi ∼ Aii, and RCD with probabilities pi ∼ Ai: 2, considered as RCD with

Stochastic Spectral and Conjugate Descent Methods

Result Uniform probabilities are optimal for n = 2 Uniform probabilities are optimal for any n ≥ 2 as long as A is diagonal “Importance sampling” pi ∼ Aii can lead to an arbitrarily worse rate than uniform probabilities “Importance sampling” pi ∼ Ai: 2 can lead to an arbitrarily worse rate than uniform probabilities For every n ≥ 2 and T > 0, there is A such that the rate of RCD with optimal probabilities is O(T log 1 ) For every n ≥ 2 and T > 0, there is A such that the rate of RCD with optimal probabilities is Ω(T log 1 )
Table 2: Summary of results on importance and optimal sampling in RCD.

Theorem 3 4 5 5 6 7

“importance sampling”, and compare these with the baseline RCD with uniform probabilities. Our result (see Theorem 5) contradicts conventional “wisdom”. In particular, we show that for every n there is a matrix A such that diagonal probabilities lead to the best rate. Moreover, the rate of RCD with “importance” can be arbitrarily worse than the rate of RCD with uniform probabilities. The same result applies to probabilities proportional to the square of the norm of the ith row of A.
We then switch gears, and motivated by the nature of SSD, we ask the following question: in order to obtain a conditionnumber-independent rate such as (6), do we have to consider new (and hard to compute) descent directions, such as eigenvectors of A, or can a similar effect be obtained using RCD with a better selection of probabilities? We give two negative results to this question (see Theorems 6 and 7). First, we show that for any n ≥ 2 and any T > 0, there is a matrix A such that the rate of RCD with any probabilities (including the optimal probabilities) is O(T log 1 ). Second, we give a similar but much stronger statement where we reach the same conclusion, but for the lower bound as opposed to the upper bound. That is, O is replaced by Ω.
As a by-product of our investigations into importance sampling, we establish that for n = 2, uniform probabilities are optimal for all matrices A (see Theorem 3). For a summary of all these results, see Table 2.
1.7. Interpolating between RCD and SSD
RCD and SSD lie on opposite ends of a continuum of stochastic descent methods for solving (1). RCD “minimizes” the work per iteration without any regard for the number of iterations, while SSD minimizes the number of iterations without any regard for the cost per iteration (or preprocessing cost). Indeed, one step of RCD costs O( Ai: 0) (the number of nonzero entries in the ith row of A), and hence RCD can be implemented very efﬁciently for sparse A. If uniform probabilities are used, no pre-processing (for computing probabilities) is needed. These advantages are paid for by the rate (3), which can be arbitrarily high. On the other hand, the rate of SSD does not depend on A. This advantage is paid for by a high pre-processing cost: the computation of the eigenvectors. This pre-processing cost makes the method utterly impractical.

One of the main contributions of this paper is the development of a new parametric family of algorithms that in some sense interpolate between RCD and SSD.
In particular, we consider the stochastic descent algorithm (4) with D being a discrete distribution over the search directions {e1, . . . , en} ∪ {u1, . . . , uk}, where ui is the eigenvectors of A corresponding to the ith smallest eigenvalue of A. We refer to this new method by the name stochastic spectral coordinate descent (SSCD).
We compute the optimal probabilities of this distribution, which turn out to be unique, and show that for k ≥ 1 they depend on the k + 1 smallest eigenvalues of A: 0 < λ1 ≤ λ2 ≤ · · · ≤ λk+1. In particular, we prove (see Theorem 8) that the rate of SSCD with optimal probabilities is
O (k + 1)λk+1 + ni=k+2 λi log 1 . (7) λk+1
For k = 0, SSCD reduces to RCD with pi ∼ Aii, and the rate (7) reduces to (3). For k = n − 1, SSCD does not reduce to SSD. However, the rates match. Indeed, in this case the rate (7) reduces to (6). Moreover, the rate improves monotonically as k increases, from O( λTmrin(A(A) ) log 1 ) (for k = 0) to O(n log 1 ) (for k = n − 1).
SSCD removes the effect of the k smallest eigenvalues. Note that the rate (7) does not depend on the k smallest eigenvalues of A. That is, by adding the eigenvectors u1, . . . , uk corresponding to the k smallest eigenvalues to the set of descent directions, we have removed the effect of these eigenvalues.
Clustered eigenvalues. Assume that the n − k largest eigenvalues are clustered: c ≤ λi ≤ γc for some c > 0 and γ > 1, for all k + 1 ≤ i ≤ n. In this case, the rate (7) can be estimated as a function of the clustering “tightness” parameter γ: O γn log 1 . See Table 3.
This can be arbitrarily better than the rate of RCD, even for k = 1. In other words, there are situations where by enriching the set of directions used by RCD by a single eigenvector only, the resulting method accelerates dramatically. To give a concrete and simpliﬁed example to illustrate this, assume that λ1 = δ > 0, while λ2 = · · · = λn = 1. In this case, RCD has the rate O((1 + n−δ 1 ) log 1 ), while

Stochastic Spectral and Conjugate Descent Methods

RCD (pi ∼ Aii) SSCD SSD

general spectrum

˜ i λi

O λ1
(k+1)λk+1 +

n i=k+2

λi

O˜ λk+1

O˜(n)

n − k largest eigvls are γ-clustered c ≤ λi ≤ γc for k + 1 ≤ i ≤ n O˜ γλn1c O˜ (γn)
O˜(n)

α-exp decaying eigvls
O˜ αn1−1 O˜ 1
αn−k−1
O˜(n)

Table 3: Comparison of complexities of RCD, SSCD (with parameter 0 ≤ k ≤ n − 1) and SSD under various regimes on the spectrum of A. The O˜ notation supresses a log 1 term.

SSCD with k = 1 has the rate O(n log 1 ). So, SSCD is 1δ times better than RCD, and the difference grows to inﬁnity
as δ approaches zero even for ﬁxed dimension n.

Exponentially decaying eigenvalues. If the eigenvalues

of A follow an exponential decay with factor 0 < α < 1,

then

the

rate

of

RCD

is

O

(

α

1
n−

1

log

1 ),

while

the

rate

of

SSCD

is

O

(

αn

1
−k

−1

log

1 ).

This

is

an

improvement

by

the

factor

1 αk

,

which

can

be

very

large

even

for

small

k

if

α

is

small. See Table 3. For an experimental conﬁrmation of this

prediction, see Figure 5.

Adding a few “largest” eigenvectors does not help. We show that in contrast with the situation above, adding a few of the “largest” eigenvectors to the coordinate directions of RCD does not help. This is captured formally in the appendix as Theorem 12.

Mini-batching. We extend SSCD to a mini-batch setting; we call the new method mSSCD. We show that the rate of mSSCD interpolates between the rate of mini-batch RCD and rate of SSD. Moreover, we show that mSSCD is optimal among a certain parametric family of methods, and that its rate improves as k increases. See Theorem 10.

1.8. Inexact Directions
Finally, we relax the need to compute exact eigenvectors or A- conjugate vectors, and analyze the behavior of our methods for inexact directions. Moreover, we propose and analyze an inexact variant of SSD which does not arise as a special case of SD. See Sections 9 and 10.

2. Stochastic Descent
The stochastic descent method was described in (4). We now formalize it as Algorithm 1, and equip it with a stepsize, which will be useful in Section 3.2, where we study minibatch version of SD.
In order to guarantee convergence of SD, we restrict our attention to the class of proper distributions, deﬁned next.
Assumption 1. Distribution D is proper with respect to A. That is, Es∼D[H] is invertible, where

Algorithm 1 Stochastic Descent (SD)
Parameters: Distribution D; Stepsize parameter ω > 0 Initialize: Choose x0 ∈ Rn for t = 0, 1, 2, . . . do
Sample search direction st ∼ D Set xt+1 = xt − ω st s(tAAxts−t b) st end for

Next we present the main convergence result for SD.
Lemma 1 (Convergence of stochastic descent (Gower & Richta´rik, 2015a; Richta´rik & Taka´cˇ, 2017)). Let D be proper with respect to A, and let 0 < ω < 2. Stochastic descent (Algorithm 1) converges linearly in expectation. In particular, we have

(1 − ω(2 − ω)λmax(W))t

x0 − x∗

2 A

≤

E[

xt − x∗

2A]

and

E[ xt − x∗ 2A] ≤ (1 − ω(2 − ω)λmin(W))t x0 − x∗ 2A,

where

W := Es∼D[A1/2HA1/2].

(9)

Finally, the statement remains true if we replace

xt − x∗

2 A

by f (xt) − f (x∗) for all t.

It is easy to observe that the stepsize choice ω = 1 is optimal. This is why we have decided to present the SD method (4) with this choice of stepsize. Moreover, notice that due to linearity of expectation,

Tr(W) (=9) E[Tr(A1/2HA1/2)]

(8)

zz

= E Tr

zz

zz = E Tr z z = 1,

where z = A1/2s. Therefore,

ss

H :=

.

s As

1 (8) 0 < λmin(W) ≤ n ≤ λmax(W) ≤ 1.

Stochastic Spectral and Conjugate Descent Methods

2.1. Stochastic Spectral Descent

Let A =

n i=1

λiuiui

be the eigenvalue decomposition of

A. That is, 0 < λ1 ≤ λ2 ≤ . . . ≤ λn are the eigenvalues

of A and u1, . . . , un are the corresponding orthonormal

eigenvectors. Consider now the SD method with D being

the uniform distribution over the set {u1, . . . , un}, and ω =

1. This gives rise to a new variant of SD which we call

stochastic spectral descent (SSD).

Algorithm 2 Stochastic Spectral Descent (SSD)
Initialize: x0 ∈ Rn; (u1, λ1), . . . (un, λn): eigenvectors and eigenvalues of A for t = 0, 1, 2, . . . do
Choose i ∈ [n] uniformly at random Set xt+1 = xt − ui xt − uλiib ui end for

For SSD we can establish an unusually strong convergence result, both in terms of speed and tightness.
Theorem 2 (Convergence of stochastic spectral descent). Let {xk} be the sequence of random iterates produced by stochastic spectral descent (Algorithm 2). Then

2

1t

2

E[ xt − x∗ A] =

1− n

x0 − x∗ A.

(10)

The above theorem implies the rate (6) mentioned in the introduction. It means that up to a logarithmic factor, SSD only needs n iterations to converge. Notice that (10) is an identity, and hence the rate is not improvable.

2.2. Stochastic Conjugate Descent
The same rate as in Theorem 2 holds for the stochastic conjugate descent (SconD) method, which arises as a special case of stochastic descent for ω = 1 and D being a uniform distribution over a set of A-orthogonal (i.e., conjugate) vectors. The proof follows by combining Lemmas 1 and 13.

2.3. Randomized Coordinate Descent
RCD (Algorithm 3) arises as a special case of SD with unit stepsize (ω = 1) and distribution D given by st = ei with probability pi > 0.
Algorithm 3 Randomized Coordinate Descent (RCD)
Parameters: probabilities p1, . . . , pn > 0 Initialize: x0 ∈ Rn for t = 0, 1, 2, . . . do
Choose i ∈ [n] with probability pi > 0 Set xt+1 = xt − Ai:Axtii−bi ei end for

The rate of RCD (Algorithm 3) can therefore be deduced

from Lemma 1. Notice that in view of (8), we have

E[H] = n pi eiei = Diag p1 , . . . , pn .

i=1 Aii

A11

Ann

So, as long as all probabilities are positive, Assumption 1 is

satisﬁed. Therefore, Lemma 1 applies and RCD enjoys the

rate  

1

1

O

log  .

(11)

λmin ADiag Apiii

Uniform probabilities can be optimal. We ﬁrst prove that uniform probabilities are optimal in 2D.

Theorem 3. Let n = 2 and consider RCD (Algorithm 3)

with probabilities p1 > 0 and p2 > 0, p1 + p2 = 1. Then

the

choice

p1

=

p2

=

1 2

optimizes

the

rate

of

RCD

in

(11).

Next we claim that uniform probabilities are optimal in any dimension n as long as the matrix A is diagonal.

Theorem 4. Let n ≥ 2 and let A be diagonal. Then uni-

form probabilities (pi

=

1 n

for all i) optimize the rate of

RCD in (11).

“Importance” sampling can be unimportant. In our next result we contradict conventional wisdom about typical choices of “importance sampling” probabilities. In particular, we claim that diagonal and row-squared-norm probabilities can lead to an arbitrarily worse performance than uniform probabilities.
Theorem 5. For every n ≥ 2 and T > 0, there exists A such that: (i) The rate of RCD with pi ∼ Aii is T times worse than the rate of RCD with uniform probabilities. (ii) The rate of RCD with pi ∼ Ai: 2 is T times worse than the rate of RCD with uniform probabilities.

Optimal probabilities can be bad. Finally, we show that there is no hope for adjustment of probabilities in RCD to lead to a rate independent of the data A, as is the case for SSD. Our ﬁrst result states that such a result can’t be obtained from the generic rate (11).
Theorem 6. For every n ≥ 2 and T > 0, there exists A such that the number of iterations (as expressed by formula (11)) of RCD with any choice of probabilities p1, . . . , pn > 0 is O(T log(1/ )).
However, that does not mean, by itself, that such a result can’t be possibly obtained via a different analysis. Our next result shatters these hopes as we establish a lower bound which can be arbitrarily larger than the dimension n.
Theorem 7. For every n ≥ 2 and T > 0, there exists an n × n positive deﬁnite matrix A and starting point x0, such that the number of iterations of RCD with any choice probabilities p1, . . . , pn > 0 is Ω(T log(1/ )).

Stochastic Spectral and Conjugate Descent Methods

3. Interpolating Between RCD and SSD
Assume now that we have some partial spectral information available. In particular, ﬁx k ∈ {0, 1, . . . , n − 1} and assume we know eigenvectors ui and eigenvalues λi for i = 1, . . . , k. We now deﬁne a parametric distribution D(α, β1, . . . , βk) with parameters α > 0 and β1, . . . , βk ≥ 0 as follows. Sample s ∼ D(α, β1, . . . , βk) arises through the process

s = ei with probability pi = αCAkii , i ∈ [n], ui with probability pn+i = Cβki , i ∈ [k],

where Ck := αTr(A) +

k i=1

βi

is

a

normalizing

factor

ensuring that the probabilities sum up to 1.

3.1. SSCD
Applying the SD method with the distribution D = D(α, β1, . . . , βk) gives rise to a new speciﬁc method which we call stochastic spectral coordinate descent (SSCD).

Algorithm 4 Stochastic Spectral Coordinate Descent
(SSCD)
Parameters: Distribution D(α, β1, . . . , βk) Initialize: x0 ∈ Rn for t = 0, 1, 2, . . . do
Sample st ∼ D(α, β1, . . . , βk) Set xt+1 = xt − st s(tAAxts−t b) st end for

Theorem 8. Consider Stochastic Spectral Coordinate Descent (Algorithm 4) for ﬁxed k ∈ {0, 1, . . . , n − 1}. The method converges linearly for all positive α > 0 and nonnegative βi. The best rate is obtained for parameters α = 1 and βi = λk+1 − λi; and this is the unique choice of parameters leading to the best rate. In this case,

E[ xt − x∗ 2A] ≤

1 − λk+1 Ck

t
x0 − x∗ 2A,

where

n

Ck = (k + 1)λk+1 +

λi.

i=k+2

Moreover, the rate improves as k grows, and we have

λ1 = λ1 ≤ · · · ≤ λk+1 ≤ · · · ≤ λn = 1 .

Tr(A) C0

Ck

Cn−1 n

If k = 0, SSCD reduces to RCD (with diagonal probabilities). Since Cλ10 = Trλ(1A) , we recover the rate of RCD of Leventhal & Lewis (2010). With the choice k = n − 1 our
method does not reduce to SSD. However, the rates match. Indeed, Cλnn−1 = nλλnn = n1 (compare with Theorem 2).

“Largest” eigenvectors do not help. It is natural to ask whether there is any beneﬁt in considering a few “largest” eigenvectors instead. Unfortunately, for the same parametric family as in Theorem 8, the answer is negative. The optimal parameters suggest that RCD has better rate without these directions. See Theorem 12 in the appendix.

3.2. Mini-batch SD

A mini-batch version of SD was developed by Richta´rik & Taka´cˇ (2017). Here we restate the method as Algorithm 5.

Algorithm 5 Mini-batch Stochastic Descent (mSD)

Parameters: Distribution D; stepsize parameter ω > 0;

mini-batch size τ ≥ 1

Initialize: x0 ∈ Rn

for t = 0, 1, 2, . . . do

for i = 1, 2, . . . , τ do

Sample sti ∼ D

Set xt+1,i = xt − ω stis(tAiAxstt−i b) sti

end for τ

Set

xt+1

=

1 τ

xt+1,i

i=1

end for

Lemma 9 (Convergence of mSD (Richta´rik & Taka´cˇ, 2017)). Let D be proper with respect to A, and let 0 < ω < ξ(2τ) , where ξ(τ ) := τ1 + 1 − τ1 λmax(W). Then
E[ xt − x∗ 2A] ≤ (ρ(ω, τ ))t x0 − x∗ 2A,

where

ρ(ω, τ ) = 1 − ω[2 − ωξ(τ )]λmin(W).

For any ﬁxed τ ≥ 1, the optimal stepsize choice is ω(τ ) = ξ(1τ) and the associated optimal rate is

λmin(W)

ρ(ω(τ ), τ ) = 1 − 1 + 1 − 1 λ

. (W)

τ

τ max

3.3. Mini-batch SSCD

Specializing mSD to the distribution D = D(α, β1, . . . , βk) gives rise to a new speciﬁc method which we call minibatch stochastic spectral coordinate descent (mSSCD), and formalize as Algorithm 6.

The rate of mSSCD is governed by the following result.

Theorem 10. Consider mSSCD (Algorithm 6) for ﬁxed k ∈
{0, 1, . . . , n − 1} and optimal stepsize parameter ω(τ ) = ξ(1τ) . The method converges linearly for all positive α > 0 and nonnegative βi. The best rate is obtained for parameters α = 1 and βi = λk+1 − λi; and this is the unique choice of parameters leading to the best rate. In this case,

E[ xt − x∗ 2A] ≤

1 − λk+1 Fk

t
x0 − x∗ 2A,

Stochastic Spectral and Conjugate Descent Methods

Algorithm 6 Mini-batch Stochastic Spectral Coordinate Descent (mSSCD)

Parameters: Distribution D(α, β1, . . . , βk); relaxation

parameter ω ∈ R; mini-batch size τ ≥ 1 Initialize: x0 ∈ Rn

for t = 0, 1, 2, . . . do

for i = 1, 2, . . . , τ do

Sample sti ∼ D(α, β1, . . . , βk)

Set xt+1,i = xt − ω stis(tAiAxstt−i b) sti

end for τ

Set

xt+1

=

1 τ

xt+1,i

i=1

end for

where

1

n

1

Fk := τ (k + 1)λk+1 +

λi + 1 − τ λn.

i=k+2

Moreover, the rate improves as k grows, and we have

λ1

λ1

λk+1

1 Tr(A) + 1 − 1

= ≤ ··· ≤ λn F0

Fk

τ

τ

and

λk+1

λn

1

Fk

≤ ··· ≤

Fn−1

=

n−1

. +1

τ

If k = 0, mSSCD reduces to mini-batch RCD (with di-

agonal probabilities). Since λ1 =

λ1

, we

F0

1 τ

Tr(A)+(1−

1 τ

)λn

recover the rate of mini-batch RCD (Richta´rik & Taka´cˇ,

2017). With the choice k = n − 1 our method does not

reduce to mSSD. However, the rates match.

4. Experiments
4.1. Stochastic spectral coordinate descent (SSCD)
In our ﬁrst experiment we study how the practical behavior of SSCD (Algorithm 4) depends on the choice of k. What we study here does not depend on the dimensionality of the problem (n), and hence it sufﬁces to perform the experiments on small dimensional problems (n = 30).
In this experiment we consider the regime of clustered eigenvalues described in Section 1.7 and summarized in Table 3. In particular, we construct a synthetic matrix A ∈ R30×30 with the smallest 15 eigenvalues clustered in the interval (5, 5 + ∆) and the largest 15 eigenvalues clustered in the interval (θ, θ + ∆). We vary the tightness parameter ∆ and the separation parameter θ, and study the performance of SSCD for various choices of k. See Figure 3.
Our ﬁrst ﬁnding is a conﬁrmation of the phase transition phenomenon predicted by our theory. Recall that the rate of

SSCD (see Theorem 8) is

O˜ (k + 1)λk+1 +

n i=k+2

λi

.

λk+1

If k < 15, we know λi ∈ (5, 5 + ∆) for i = 1, 2, . . . , k + 1, and λi ∈ (θ, θ + ∆) for i = k + 2, . . . , n. Therefore, the rate can be estimated as

rsmall := O˜

(n − k − 1)(θ + ∆) k+1+

.

5

On the other hand, if k ≥ 15, we know that λi ∈ (θ, θ + ∆) for i = k + 1, . . . , n, and hence the rate can be estimated as

rlarge := O˜

(n − k − 1)(θ + ∆) k+1+

.

θ

Note that if the separation θ between the two clusters is large, the rate rlarge is much better than the rate rsmall. Indeed, in this regime, the rate rlarge becomes O˜(n), while rsmall can be arbitrarily large.
Going back to Figure 3, notice that this can be observed in the experiments. There is a clear phase transition at k = 15, as predicted be the above analysis. Methods using k ∈ {0, 6, 12} are relatively slow (although still enjoying a linear rate), and tend to have similar behaviour, especially when ∆ is small. On the other hand, methods using k ∈ {18, 24, 29} are much faster, with a behaviour nearly independent of θ and ∆. Moreover, as θ increases, the difference in the rates between the slow methods using k ∈ {0, 6, 12} and the fast methods using k ∈ {18, 24, 29} grows.
We have performed additional experiments with three clusters; see Figure 4 in the appendix.

4.2. Mini-batch SSCD
In Figure 2 we report on the behavior of mSSCD, the minibatch version of SSCD, for four choices of the mini-batch parameter τ , and several choices of k. Mini-batch of size τ is processed in parallel on τ processors, and the cost of a single iteration of mSSCD is (roughly) the same for all τ .
For τ = 1, the method reduces to SSCD, considered in previous experiment (but on a different dataset). Since the number of iterations is small, there are no noticeable differences across using different values of k. As τ grows, however, all methods become faster. Mini-batching seems to be more useful as k is larger. Moreover, we can observe that acceleration through mini-batching starts more aggressively for small values op k, and its added beneﬁt for increasing values of k is getting smaller and smaller. This means that even for relatively small values of k, mini-batching can be expected to lead to substantial speed-ups.

Stochastic Spectral and Conjugate Descent Methods

Expected precision

Expected precision

Expected precision

100 = 50, = 1

100 = 50, = 10

100 = 50, = 25

100 = 50, = 50

Expected precision

Expected precision

Expected precision

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

100 = 100, = 1

100 = 100, = 10

100 = 100, = 25

100 = 100, = 50

Expected precision

Expected precision

Expected precision

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

100 = 200, = 1

100 = 200, = 10

100 = 200, = 25

100 = 200, = 50

Expected precision

Expected precision

Expected precision

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

100 = 400, = 1

100 = 400, = 10

100 = 400, = 25

100 = 400, = 50

Expected precision

Expected precision

Expected precision

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

10 1

k = 0

10 2

k = 6 k = 12

k = 18

10 3

k = 24

k = 29

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

0 N50umbe10r0of it1e5r0atio2n0s0 250

Figure 1: Expected precision E

xt − x∗

2A/ x0 − x∗

2 A

versus # iterations of SSCD for symmetric positive deﬁnite matrices A of

size 30 × 30 with different structures of spectra. The spectrum of A consists of 2 equally sized clusters of eigenvalues; one in the interval

(5, 5 + ∆), and the other in the interval (θ, θ + ∆).

Expected precision

Expected precision

100

=1

100

=5

100

=20

100

=100

Expected precision

Expected precision

Expected precision

10 1

10 1

10 1

10 1

10 2

10 2

10 2

10 2

10 3 k = 0

k = 6

10 4

k = 12 k = 18

k = 24

10 5

0

k = 29
N10umbe2r0 of ite30ration4s0

10 3 k = 0

k = 6

10 4

k = 12 k = 18

k = 24

50

10 5

0

k = 29
N10umbe2r0 of ite30ration4s0

10 3 k = 0

k = 6

10 4

k = 12 k = 18

k = 24

50

10 5

0

k = 29
N10umbe2r0 of ite30ration4s0

10 3 10 4 50 10 5 0

k = 0 k = 6 k = 12 k = 18 k = 24 k = 29
N10umbe2r0 of ite30ration4s0 50

Figure 2: Expected precision E

xt − x∗

2A/ x0 − x∗

2 A

versus # iterations of mini-batch SSCD for A ∈ R30×30 and several choices

of mini-batch size τ . The spectrum of A was chosen as a uniform discretization of the interval [1, 60].

4.3. Matrix with 10 billion entries
In Figure 3 we report on an experiment using a synthetic problem with data matrix A of dimension n = 105 (i.e., potentially with 1010 entries). As all experiments were done on a laptop, we worked with sparse matrices with 106 nonzeros only.

In the ﬁrst row of Figure 3 we consider matrix A with all eigenvalues distributed uniformly on the interval [1, 100]. We observe that SSCD with k = 104 (just 10% of n) requires about an order of magnitude less iterations than SSCD with k = 0 (=RCD).
In the second row we consider a scenario where l eigenvalues are small, contained in [1, 2], with the rest of the

Stochastic Spectral and Conjugate Descent Methods

eigenvalues contained in [100, 200]. We consider l = 10 and l = 1000 and study the behaviour of SSCD with k = l. We see that for l = 10, SSCD performs dramatically better than RCD: it is able to achieve machine precision while RCD struggles to reduce the initial error by a factor larger than 106. For l = 1000, SSCD achieves error 10−9 while RCD struggles to push the error below 10−4. These tests show that in terms of # iterations, SSCD has the capacity to accelerate on RCD by many orders of magnitude.

Expected precision Expected precision
Expected precision

100

k = 0

10 1

k = 10000

10 2

10 3

100 10 2 10 4 10 6 10 8 10 10 10 12
0.0

10 4 0.0

N0u.5mber1.0of iter1a.5tions2.0 1e6

k = 0

100

k = 10 10 2

k = 0 k = 1000

10 4

10 6

10 8

0N.5 um1b.0er o1f.5iter2a.0tion2s.5 31.e06

0.0 N0u.5mber 1o.0f itera1t.i5ons 2.01e6

Figure 3: Expected precision E

xt − x∗

2A/ x0 − x∗

2 A

ver-

sus # iterations of SSCD for a matrix A ∈ R105×105 . Top row:

spectrum of A is uniformly distributed on [1, 100]. Bottom row:

spectrum contained in two clusters: [1, 2] and [100, 200].

Fercoq, Olivier and Richta´rik, Peter. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):1997–2023, 2015.
Gower, Robert M and Richta´rik, Peter. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690, 2015a.
Gower, Robert Mansel and Richta´rik, Peter. Stochastic dual ascent for solving linear systems. arXiv preprint arXiv:1512.06890, 2015b.
Lee, Ching-Pei and Wright, Stephen J. Random permutations ﬁx a worst case for cyclic coordinate descent. arXiv:1607.08320, 2016.
Lee, Yin Tat and Sidford, Aaron. Efﬁcient accelerated coordinate descent methods and faster algorithms for solving linear systems. In FOCS, 2013.
Leventhal, Dennis and Lewis, Adrian. Randomized methods for linear constraints: convergence rates and conditioning. Mathematics of Operations Research, 35:641–654, 2010.
Loizou, Nicolas and Richta´rik, Peter. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. arXiv preprint arXiv:1712.09677, 2017.

5. Extensions
Our algorithms and convergence results can be extended to eigenvectors and conjugate directions which are only computed approximately. Some of this development can be found in the appendix (see Section 9). Finally, as mentioned in the introduction, our results can be extended to the more general problem of minimizing f (x) = φ(Ax), where φ is smooth and strongly convex.

Nesterov, Yurii. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372–376, 1983.
Nesterov, Yurii. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012. doi: 10.1137/100802001. URL https://doi.org/10.1137/100802001. First appeared in 2010 as CORE discussion paper 2010/2.

References
Allen-Zhu, Zeyuan, Qu, Zheng, Richta´rik, Peter, and Yuan, Yang. Even faster accelerated coordinate descent using non-uniform sampling. In ICML, pp. 1110–1119, 2016.
Barzilai, Jonathan and M., Borwein Jonathan. Two point step size gradient methods. IMA Journal of Numerical Analysis, 8:141–148, 1988.
Birgin, Ernesto G., Mart´ınez, Jose´ Mario, and Raydan, Marcos. Spectral projected gradient methods: Review and perspectives. Journal of Statistical Software, 60(3):1–21, 2014.
Csiba, Dominik, Qu, Zheng, and Richta´rik, Peter. Stochastic dual coordinate ascent with adaptive probabilities. In ICML, pp. 674–683, 2015.

Nesterov, Yurii and Stich, Sebastian. Efﬁciency of accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1): 110–123, 2017.
Nutini, Julie, Schmidt, Mark, Laradji, Issam H., Friedlander, Michael, and Koepke, Hoyt. Coordinate descent converges faster with the Gauss-Southwell rule than random selection. In ICML, 2015.
Polyak, Boris. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1 – 17, 1964.
Qu, Zheng and Richta´rik, Peter. Coordinate descent with arbitrary sampling I: algorithms and complexity. Optimization Methods and Software, 31(5):829–857, 2016.

Stochastic Spectral and Conjugate Descent Methods
Richta´rik, Peter and Taka´cˇ, Martin. Stochastic reformulations of linear systems: Algorithms and convergence theory. arXiv preprint arXiv:1706.01108, 2017.
Richta´rik, Peter and Taka´cˇ, Martin. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, 10(6):1233–1243, 2016a.
Richta´rik, Peter and Taka´cˇ, Peter. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1):433–484, 2016b.
Tu, Stephen, Venkataraman, Shivaram, Wilson, Ashia C., Gittens, Alex, Jordan, Michael I., and Recht, Benjamin. Breaking locality accelerates block Gauss-Seidel. In ICML, 2017.

Appendix

Expected precision

Expected precision

Expected precision

6. Extra Experiments
In this section we report on some additional experiments which shed more light on the behaviour of our methods. 6.1. Performance on SSCD on A with three clusters eigenvalues

100 = 100, = 1

100 = 100, = 10

100 = 100, = 25

100 = 100.0, = 100

Expected precision

Expected precision

Expected precision

k = 0

k = 6

k = 12

10 1

k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

100 = 250, = 1

100 = 250, = 10

100 = 250, = 25

100 = 250, = 100

Expected precision

Expected precision

Expected precision

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

100 = 500, = 1

100 = 500, = 10

100 = 500, = 25

100 = 500, = 100

Expected precision

Expected precision

Expected precision

k = 0

k = 6

k = 12

10 1

k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

100 = 1000, = 1

100 = 1000, = 10

100 = 1000, = 25

100 = 1000, = 100

Expected precision

Expected precision

Expected precision

k = 0

k = 6

k = 12

10 1

k = 18

k = 24

k = 29

k = 0

k = 6

k = 12

10 1

k = 18

k = 24

k = 29

k = 0

k = 6

k = 12

10 1

k = 18

k = 24

k = 29

k = 0

k = 6

10 1

k = 12 k = 18

k = 24

k = 29

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

0 N20umbe4r0 of ite60ration8s0 100

Figure 4: Expected precision E ||xt−x∗||2A versus the number of iterations of SSCD for symmetric positive deﬁnite matrices A of size ||x0 −x∗ ||2A
30 × 30 with different structures of spectrum. The spectrum of A consists of 3 equally sized clusters of eigenvalues; one in the interval (10, 10 + ∆), the second in the interval (θ, θ + ∆) and the third in the interval (2θ, 2θ + ∆). We show results for 16 combinations of θ and ∆: ∆ ∈ {1, 10, 25, 100} and θ ∈ {100, 250, 500, 1000}.
In Figure 4 we report on experiments similar to those performed in Section 4.1, but on data matrix A ∈ R30×30 whose eigenvalues belong to three clusters, with 10 eigenvalues in each. We can observe that the SSCD methods can be grouped into three categories: slow, fast, and very fast, depending on whether k corresponds to the smallest 10 eigenvalues, the next

Expected precision

Stochastic Spectral and Conjugate Descent Methods
cluster of 10 eigenvalues, or the 10 largest eigenvalues. That is, there are two phase transitions.
6.2. Exponentially decaying eigenvalues We now consider matrix A ∈ R10×10 with eigenvalues 20, 21, . . . , 29. We apply SSCD with increasing values of k (see Figure 5).
100

Expected precision

Figure 5: Expected precision E 10 × 10.

10 1 10 2
||xt −x∗ ||2A ||x0 −x∗ ||2A

k = 0 k = 2 k = 4 k = 6 k = 8 k = 9
0 N10umbe2r0 of ite30ration4s0 50
versus the number of iterations of SSCD for symmetric positive deﬁnite matrix A of size

We can see that the performance boost accelerates as k increases. So, while one may not expect much speed-up for very
small k, there will be substantial speed-up for moderate values of k. This is predicted by our theory. Indeed, consulting Table 3 (last column), we have α = 1/2, and hence for k = 0 the theoretical rate is O˜( α19 ). For general k we have O˜( α91−k ). So, the speedup for value k > 0 compared to the baseline case of k = 0 (=RCD) is 2k, i.e., exponential.

7. Proofs
In this section we provide proofs of the statements from the main body of the paper. Table 4 provides a guide on where the proof of the various results can be found.

Result
Lemma 1 Theorem 2 Theorem 3 Theorem 4 Theorem 5 Theorem 6 Theorem 7 Theorem 8 Lemma 9 Theorem 10

Section
7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 7.10

Table 4: Proof of lemmas and theorems stated in the main paper.

7.1. Proof of Lemma 1
The result follows from Theorem 4.8(i) in (Richta´rik & Taka´cˇ, 2017) with the choice B = A. Note that since x∗ = A−1b is the unique solution of Ax = b, it is equal to the projection of x0 onto the solution space of Ax = b, as required by the assumption in Theorem 4.8(i). It only remains to check that Assumption 3.5 (exactness) in (Richta´rik & Taka´cˇ, 2017) holds. In view of Theorem 3.6(iv) in (Richta´rik & Taka´cˇ, 2017), it sufﬁces to check that the nullspace of E[H] is trivial. However, this is equivalent to the assumption in Lemma 1 that E[H] be invertible.

Stochastic Spectral and Conjugate Descent Methods

Finally, observe that

1 2

x − x∗

2 A

=

1 2

(x

−

x∗

)

A(x − x∗)

=

12 x Ax + 21 x∗ Ax∗ − x Ax∗

= 21 x Ax + 12 x∗ Ax∗ − x AA−1b (=1) f (x) + 12 x∗ Ax∗

= f (x) − f (x∗).

7.2. Proof of Theorem 2 We will break down the proof into three steps.

1. First, let us show that Algorithm 2 is indeed SSD, as described in (4), i.e., xt+1 = xt − st s(tAAxts−t b) st. We known that st = ui with probability 1/n. Since Aui = λiui, and assuming that at iteration t we have st = ui, we get

xt+1

=

xt

−

ui

(Axt

−

b) ui

=

xt

−

ui

(Axt

−

b) ui

ui Aui

λi

=

xt

−

λiui

xt

−

ui

b ui

=

xt − u xt − ui b ui.

λi

i

λi

2. We now need to argue that the assumption that E[H] is invertible is satisﬁed.

(8) n 1 uiui

n 1 uiui

E[H] =

=

.

(12)

i=1 n ui Aui i=1 n λi

Since E[H] has positive eigenvalues 1/(nλi), it is invertible.

3. Applying Lemma 1, we get

(1 − λmax(W))tE[ x0 − x∗ 2A] ≤ E[ xt − x∗ 2A] ≤ (1 − λmin(A))tE[ x0 − x∗ 2A]. √
It remains to show that λmin(W) = λmax(W) = n1 . In view of (12), and since A1/2ui = λiui, we get

(9) 1/2

1/2 (12) 1/2 n 1 uiui 1/2 n 1 A1/2uiui A1/2 1

W = A E[H]A = A

A=

= I.

i=1 n λi i=1 n λi n

7.3. Proof of Theorem 3

Let A be a 2 × 2 symmetric positive deﬁnite matrix:

A= a c . cb

We know that a, b > 0, and ab − c2 > 0. Assume that st = e1 = (1, 0) with probability p > 0 and st = e2 = (0, 1) with probability q > 0, where p + q = 1. Then

E[H] (=8) p e1e1

+ q e2e2

=

p a

0,

e1 Ae1 e2 Ae2

0 qb

and therefore,

p pc E[H]A = q c qa .
b

Note that E[H]A has the same eigenvalues as W = A1/2E[H]A1/2. We now ﬁnd the eigenvalues of E[H]A by ﬁnding the zeros of the characteristic polynomial:

det(E[H]A − λI) = det p − λ p ac = λ2 − λ + pq 1 − c2 = 0

q cb q − λ

ab

It can be seen that

Stochastic Spectral and Conjugate Descent Methods

11

c2

11

c2

λmin(E[H]A) = 2 − 2 1 − 4pq 1 − ab = 2 − 2 1 − 4p(1 − p) 1 − ab .

The expression λmin(E[H]A) is maximized for p = 12 , independently of the values of a, b and c.

7.4. Proof of Theorem 4

Fix n ≥ 2, and let ∆+n := {p ∈ Rn : p > 0, i pi = 1} be the (interior of the) probability simplex. Further, let A = Diag(A11, A22, . . . , Ann) be a diagonal matrix with positive diagonal entries.

The rate of RCD with any probabilities arises as a special case of Lemma 1. We therefore need to study the smallest eigenvalue of W (deﬁned in (9)) as a function of p = (p1, . . . , pn). We have

(8)

pi

H(p) := Es∼D[H] = Aii eiei = Diag(p1/A11, p2/A22, . . . , pn/Ann),

i

and hence

 p1 0 . . . 

(9)

n

1/2

1/2

 0 p2 . . . 

W = W(p) := A H(p)A = i=1 pieiei = . . . . . . . . .  . (13)

0 0 . . . pn

Note that λmin(W(p)) (=13) λmin(Diag(p1, p2, . . . , pn)) = mini pi, and thus

1

max λmin(W(p)) = .

p∈∆+ n

n

Clearly,

the

optimal

probabilities

are

uniform:

p∗i

=

1 n

for

all

i.

7.5. Proof of Theorem 5 We continue from the proof of Theorem 4.

1. Consider probabilities proportional to the diagonal elements: pi = Aii/Tr(A) for all i. Choose A11 := t, and A22 = · · · = Ann = 1. Then

λmin(W(p)) ≤ p2 = A22 = 1 −→ 0 as t −→ ∞. Tr(A) t + n − 1

2. Consider probabilities proportional to the squared row norms: pi = Ai: 2/Tr(A A) for all i. Choose A11 := t, and A22 = · · · = Ann = 1. Then

A22

1

λmin(W(p)) ≤ p2 = Tr(A A) = t2 + n − 1 −→ 0 as t −→ ∞.

In both cases, λλmminin((W W((pp∗)))) can be made arbitrarily small by a suitable choice of t.
7.6. Proof of Theorem 6
The rate of RCD with any probabilities arises as a special case of Lemma 1. We therefore need to study the smallest eigenvalue of W (deﬁned in (9)). Since we wish to show that the rate can be bad, we will ﬁrst prove a lemma bounding λmin(W) from above.

Stochastic Spectral and Conjugate Descent Methods

Lemma 11. Let 0 < λ1 ≤ λ2 ≤ · · · ≤ λn be the eigenvalues of A. Then

n

1/n

1

λk

λmin(W) ≤ n Akk . (14)

k=1

Proof. We have

W

(=9)

A

1 2

E

[H]A

1 2

(=8)

A1 2

n pkekek k=1 Akk

1

1

A 2 = A 2 Diag

pk Akk

1
A2.

From the above we see that the determinant of W is given by

n pk

det(W) = det(A)

.

(15)

k=1 Akk

On the other hand, we have the trivial bound

n

det(W) = λk(W) ≥ (λmin(W))n.

(16)

k=1

Putting these together, we get an upper bound on λmin(W) in terms of the eigenvalues and diagonal elements of A:

(16)
λmin(W) ≤ n det(W)

(=15)

n det(A) · n n pk k=1 Akk

n1

n

= n det(A) · n

·n

pk

Akk

k=1

k=1

(≤∗) n det(A) · n n 1 · nk=1 pk k=1 Akk n

n det(A)

n1

=

·n

n k=1 Akk

(16) 1 n λk

=

n

,

n k=1 Akk

where (*) follows from the arithmetic-geometric mean inequality.

The Proof: Let λ1, . . . , λn are any positive real numbers. We now construct matrix A = MΛM , where Λ :=

Diag(λ1, . . . , λn) and

√

√

 1/ √2 1/√2 0 · · · 0

−1/ 2 1/ 2 0 · · · 0

 M :=  0
 ...

 0 1 · · · 0 ∈ Rn×n. ... ... . . . 0

0

0 0 ··· 1

Clearly, A is symmetric. Since M is orthonormal, λ1, . . . , λn are, by construction, the eigenvalues of A. Hence, A is symmetric and positive deﬁnite. Further, note that the diagonal entries of A are related to its eigenvalues as follows:

λ1+λ2 , k = 1, 2;

Akk =

2

(17)

λk ,

otherwise.

Stochastic Spectral and Conjugate Descent Methods

Applying Lemma 11, we get the bound

(14) 1

n

1/n

λk

λmin(W) ≤ n Akk

k=1

2

n

1/n

1

λk

λk

=

·

n k=1 Akk k=3 Akk

2

1/n

(17) 1

λk

=

n k=1 Akk

(17) 1

4λ1λ2 1/n

= n (λ1 + λ2)2 .

Let c > 0 be such that λ1 = cλ2. Then (λ41λ+1λλ22)2 = (1+4cc)2 . If choose c small enough so that (1+4cc)2 ≤

λmin(W)

≤

1 T

.

The

statement

of

the

theorem

follows.

Tn n, then

7.7. Proof of Theorem 7
Let W = UΛU be the eigenvalue decomposition of W, where U = [u1, . . . , un] are the eigenvectors, λ1(W) ≤ . . . ≤ λn(W) are the eigenvalues and Λ = Diag (λ1(W), . . . , λn(W)). From Theorem 4.3 of (Richta´rik & Taka´cˇ, 2017) we get

E U A1/2(xt − x∗) = (I − Λ)tU A1/2(x0 − x∗).

(18)

Now we use Jensen’s inequality and get

E

xt − x∗

2 A

=E

2
U A1/2(xt − x∗) ≥ E U A1/2(xt − x∗)
2

2 (18)
=

(I − Λ)tU

2
A1/2(x0 − x∗)

2

2

n

2

2

=

(1 − λi(W))2t ui A1/2(x0 − x∗) ≥ (1 − λ1(W))2t u1 A1/2(x0 − x∗) .

i=1

Now we take an example of matrix A, for which we set λmin(W)

≤

1 T

for arbitrary T

>

0, like we did in Section 7.6.

We

also choose x0 = x∗ + A−1/2u1. For this choice of A and x0 we get x0 − x∗ 2A = u1 22 and

2

2t

2

1 2t

2

1 2t

2

E xt − x∗ A ≥ (1 − λ1(W))

u1 2 ≥

1− T

u1 2 =

1− T

x0 − x∗ A .

7.8. Proof of Theorem 8 We divide the proof into several steps.

1. Let us ﬁrst show that SSCD converges with a linear rate for any choice of α > 0 and nonnegative {βi}. Since SSCD arises as a special case of SD, it sufﬁces to apply Lemma 1. In order to apply this lemma, we need to argue that D = D(α, β1, . . . , βn) is a proper distribution. Indeed,

n (8)

eiei

k

uiui

Es∼D[H] = pi

+ pn+i

i=1 ei Aei i=1

ui Aui

1

k

βi

=

αI +

Ck

uiui λi

(21)

i=1

α

I

0.

Ck

Stochastic Spectral and Conjugate Descent Methods

2. For the speciﬁc choice of parameters α = 1 and βi = λk+1 − λi we have

1

k

λk+1 − λi

Es∼D[H] = Ck I + uiui λi

,

i=1

m

and Ck = (k + 1)λk+1 +

λi. Therefore,

i=k+2

1k

n

Es∼D[AH] = Ck

λk+1uiui +

λiuiui .

i=1

i=k+1

The minimal eigenvalue of this matrix, which has the same spectrum as W, is

λk+1

λk+1

λmin(Es∼D[AH]) = Ck =

n

.

(k + 1)λk+1 +

λi

i=k+2

The main statement follows by applying Lemma 1.

3. We now show that the rate improves as k increases. Indeed,

1m

1m

1m

k + λk+1 λi = k + 1 + λk+1 λi ≥ k + 1 + λk+2 λi.

i=k+1

i=k+2

i=k+2

By taking reciprocals, we get

λk+2 m ≥

λk+1m .

(k + 1)λk+2 +

λi kλk+1 +

λi

i=k+2

i=k+1

4. It remains to establish optimality of the speciﬁc parameter choice α = 1 and βi = λk+1 − λi. Continuing from (21), we get

(21) 1

n

k

1k

n

Es∼D[AH] = Ck

uiui αλi +

uiui βi

= Ck

(αλi + βi)uiui +

αλiuiui .

i=1

i=1

i=1

i=k+1

The eigenvalues of Es∼D[AH] are { αλCi+k βi }ki=1 ∪ { αCλki }ni=k+1. Let γ be the smallest eigenvalue, i.e., γ :=
λmin(Es∼D[AH]) = Cθk , and Ω be the largest eigenvalue, i.e., Ω := λmax(Es∼D[AH]) = C∆k , where θ and ∆ are appropriate constants. There are now two options.

(a) γ = αλCkk+1 . Then αλi + βi ≥ αλk+1 for i ∈ {1, . . . , k}. In this case we obtain:

k

k

n

Ck = αTr (A) + βi = (αλi + βi) + α

λi ≥ α

i=1

i=1

i=k+1

n

kλk+1 +

λi

i=k+1

and therefore

γ ≤ λk+1 n .

kλk+1 +

λi

i=k+1

(b) γ = αλCj+k βj = Cθk for some j ∈ {1, . . . , k}. Then

k

k

n

n

Ck = αTr (A) + βi = (αλi + βi) + α

λi ≥ kθ + α

λi

i=1

i=1

i=k+1

i=k+1

Stochastic Spectral and Conjugate Descent Methods

whence

θ

γ≤

n

.

kθ + α

λi

i=k+1

Note that the function f (θ) =

θ
n

increases monotonically:

kθ+α

λi

i=k+1

n

f (θ) =

1

n

−

α

λi

kθ

i=k+1

n

=

n

> 0.

kθ + α

λi (kθ + α

λi)2 (kθ + α

λi)2

i=k+1

i=k+1

i=k+1

From this and inequality αλk+1 ≥ θ we get

γ ≤ αλk+1 n = λk+1 n .

α(kλk+1 +

λi) kλk+1 +

λi

i=k+1

i=k+1

In both possible cases we have shown that

λmin(Es∼D[AH]) ≤

λk+1 n .

kλk+1 +

λi

i=k+1

So, it is the optimal rate in this family of methods. Optimal distribution is unique and it is:

s ∼ D ⇔ s = ei with probability pi = ACiki i = 1, 2, . . . , n ui with probability pn+i = λk+C1k−λi i = 1, 2, . . . , k,

n

where Ck = kλk+1 +

λi.

i=k+1

7.9. Proof of Lemma 9 The steps are analogous to the proof of Lemma 1.

7.10. Proof of Theorem 10

n

Let Ck = (k + 1)λk+1 +

λi γ = Cθk — the minimal eigenvalue of the matrix W and Ω = C∆k — the maximal

i=k+2

eigenvalue of the matrix W. The optimal rate of the method (Richta´rik & Taka´cˇ, 2017) is

γ

θ

r(τ ) = τ1 + 1 − τ1 Ω = τ1 Ck + 1 − τ1 ∆ .

From the Section 7.8 we have

1k

n

Es∼D[AH] = Ck

λk+1uiui +

λiuiui .

i=1

i=k+1

There are two options.

1. γ = αλCkk+1 . Then αλi + βi ≥ αλk+1 for i ∈ {1, . . . , k} and ∆ αλn. In this case we obtain:

k

k

n

Ck = αTr (A) + βi = (αλi + βi) + α

λi ≥ α

i=1

i=1

i=k+1

n

kλk+1 +

λi

i=k+1

Stochastic Spectral and Conjugate Descent Methods

and therefore
r(τ ) ≤
α τ

αλk+1

n

kλk+1 +

λi +

i=k+1

1 − τ1

=

αλn

1 τ

λk+1

n

kλk+1 +

λi

i=k+1

. + 1 − τ1 λn

2. γ = αλCj+k βj = Cθk for some j ∈ {1, . . . , k}. Then

k

k

n

n

Ck = αTr (A) + βi = (αλi + βi) + α

λi ≥ kθ + α

λi,

i=1

i=1

i=k+1

i=k+1

∆ ≥ αλn

whence

r(τ )

θ .

n

τ1 kθ + α

λi + 1 − τ1 αλn

i=k+1

Note that the function f (θ) =

θ

increases monotonically:

n

τ1 kθ+α

λi

+(1−

1 τ

)αλn

i=k+1

f (θ) =
1 τ

1

n

kθ+α

λi

i=k+1

−

+(1−

1 τ

)αλn

τk θ

( ) n 2

τ1 kθ+α

λi + 1− τ1 αλn

i=k+1

n

α
( ) τ

λi+ 1− τ1 αλn

i=k+1

= > 0. n 2

( ) τ1 kθ+α

λi + 1− τ1 αλn

i=k+1

From this and inequality αλk+1 ≥ θ we get

r(τ ) ≤
1 τ

αλk+1

n

αkλk+1 + α

λi

i=k+1

=

+ 1 − τ1 αλn

1 τ

λk+1

n

kλk+1 +

λi

i=k+1

. + 1 − τ1 λn

For both possible cases we shown that r(τ ) ≤

λk+1

. So, it is the optimal rate in this family of

n

1 τ

kλk+1 +

λi

+(1−

1 τ

)λn

i=k+1

methods. Note that α could be any positive number. Optimal distribution is unique and it is:

s ∼ D ⇔ s = ei with probability pi = ACiki i = 1, 2, . . . , n ui with probability pn+i = λk+C1k−λi i = 1, 2, . . . , k,

n

where Ck = kλk+1 +

λi. For k = 0 we obtain mRCD, for k = n − 1 we get the optimal rate

i=k+1

increases when k increases.

1
n
τ1 +(1− τ1 ) n1

and rate

8. Results mentioned informally in the paper
8.1. Adding “largest” eigenvectors does not help
In Section 3.1 describing the SSCD method we have argued, without supplying any detail, that it does not make sense to consider replacing the k “smallest” eigenvectors with a few “largest” eigenvectors. Here we make this statement precise, and prove it. Fix k ∈ {0, 1, . . . , n − 1} and consider running stochastic descent with the distribution D deﬁned via
s ∼ D ⇔ s = ei with probability pi = αCAkii i = 1, 2, . . . , n ui with probability pn−k+i = Cβki i = k + 1, k + 2, . . . , n,

Stochastic Spectral and Conjugate Descent Methods

n

where Ck = αTr (A) +

βi and for βi ≥ 0 for i ∈ {1, 2, . . . , k}.

i=k+1

That is, we consider “enriching” RCD with a collection of a n−k eigenvectors corresponding to the n−k largest eigenvectors of A. We have the following negative result, which loosely speaking says that it is not worth enriching RCD with such vectors.

Theorem 12. The optimal parameters of the above method are k = n or βi = 0 for all i = k + 1, . . . , n.

Proof. We follow similar steps as in the proof of Theorem 8. In this setting we have

1

n βi

Es∼D[H] = Ck αI +

λi uiui ,

i=k+1

whence and

1 AEs∼D[H] = Ck

n

αA +

βiuiui

i=k+1

1 =
Ck

k

n

αλiuiui +

(βi + αλi)uiui

i=1

i=k+1

λmin (AEs∼D[H]) = αλ1 ≤ αλ1 = λ1 . Ck αTr (A) Tr (A)

It means that the best rate in this family of methods is obtained when k = n or βi = 0 for all i = k + 1, . . . , n.

So, to use spectral information about n − k last eigenvectors we should use more complicated distributions (for instance, one may need to replace α by αi).

8.2. Stochastic Conjugate Descent
The lemma below was referred to in Section 2.2. As explained in that section, this lemma can be used to argue that stochastic conjugate descent achieves the same rate as SSD: O(n log 1 ). Lemma 13. Let {v1 . . . vn} be an A-orthonormal system:

1 i=j vi Avj = 0 i = j .

If

distribution

D

consists

of

vectors

vi

chosen

with

uniform

probabilities,

then

λmin(W)

=

1 n

Proof. That is,

1/2

1/2 1 n A1/2vivi A1/2 1 n 1/2

1/2

W = A E[H]A

= n i=1

vi Avi

= n

A vivi A .

i=1

Making a substitution ui = A1/2si, we get

1n

1

W= n

uiui

= I, n

i=1

because {u1 . . . un} is orthonormal system.

9. Inexact Stochastic Conjuagate Descent
In Section 2.2 we stated, that we can achieve an optimal rate of stochastic descent by using uniform distribution over a set of n A-conjugate directions. In this section we consider the case when A-conjugate directions are computed approximately.
More formally, we consider a system of vectors v1, . . . , vn, which satisﬁes vi Avj ≤ ε for i = j and vi Avi = 1 for some parameter ε > 0. Further we’ll call such vectors ε-approximate A-conjugate vectors.

Stochastic Spectral and Conjugate Descent Methods
Now we formalize the idea of using approximate A-conjugate directions in Stochastic Conjugate Descent, which leads to Algorithm 7.
Algorithm 7 Inexact Stochastic Conjugate Descent (iSconD) Initialize: x0 ∈ Rn; v1, . . . , vn: ε-approximate A-conjugate directions for t = 0, 1, 2, . . . do Choose i ∈ [n] uniformly at random Set xt+1 = xt − vi (Axt − b) vi end for

For this algorithm we are going to obtain rate O(n log 1 ), the optimal rate for stochastic descent.

9.1. Lemma

Lemma 14. Let S = [v1, . . . , vn], where v1, . . . , vn are ε-approximate A-conjugate vectors.

If ε satisﬁes

1

ε<

(22)

n−1

then ˜I := S AS is positive deﬁnite matrix and

λmin(A1/2SS A1/2) ≥ 1 − ε(n − 1) 1 + ε(n − 1)

(23)

1 − ε(n − 1)

λmax(A1/2SS A1/2) ≤ 1 + ε(n − 1) 1 + ε(n − 1) 1 − ε(n − 1)

Proof. For unit vector x we can write

x ˜Ix = xixl˜Iil = 1 +

xixl˜Iil ≥ 1 − ε

21 (x2i + x2l ) = 1 − ε(n − 1).

i,l

i,l:i=l

i,l:i=l

Under condition (22) we get x ˜Ix > 0 for any x, which proves the ﬁrst part of lemma.
Since S AS is positive deﬁnite, vectors A1/2v1, . . . , A1/2vn are linearly independent. represented as x = A1/2Sα with normalization condition:

Any unit vector x may be

1 = x x = α ˜Iα = α α +

˜Iilαiαl,

i,l:i=l

or

α α=1−

˜Iilαiαl.

(25)

i,l:i=l

Now we can analyse spectrum of matrix A1/2SS A1/2.

2

n

x A1/2SS A1/2x = α S ASS ASα = α ˜I2α = ˜Iα =

2 i=1

n

2

˜Iilαl =

l=1


n
= αi +

2



n

˜Iilαl = αi2 + 2αi

 ˜Iilαl + 

2 ˜Iilαl  .


i=1

l:l=i

i=1

l:l=i

l:l=i

Stochastic Spectral and Conjugate Descent Methods

Using (25) we get

x A1/2SS A1/2x = 1 +


n
˜Iilαiαl + 

2 ˜Iilαl = 1 + R1 + R2

i,l:l=i

i=1 l:l=i

R1

R2

(26)

To estimate |R1| and |R2| we need to estimate α α using (25):

α α≤1+ε

αi2 + αl2 = 1 + ε(n − 1)α α,

i,l:i=l 2

which under condition (22) implies that α α ≤ 1−ε(1n−1) . Now we can estimate |R1| and |R2|.

R1 ≤ ε αi2 +2 αl2 = ε(n − 1)α α ≤ 1 −ε(nε(−n −1)1) (27)
i,l:i=l

n

ε2(n − 1)2

R2 ≤ (n − 1) αl2ε2 = ε2(n − 1)2α α ≤

(28)

1 − ε(n − 1)

i=1

l:l=i

Finally from (26), (27) and (28) we get

λmin(A1/2SS A1/2) ≥ 1 − ε(n − 1) + ε2(n − 1)2 = 1 − ε(n − 1) 1 + ε(n − 1)

1 − ε(n − 1)

1 − ε(n − 1)

λmax(A1/2SS A1/2) ≤ 1 + ε(n − 1) + ε2(n − 1)2 = 1 + ε(n − 1) 1 + ε(n − 1)

1 − ε(n − 1)

1 − ε(n − 1)

Corollary 14.1. bound:

√

If

ε

<

2−1 (n−1)

then

λmin(A1/2SS

A1/2) > 0 and condition number of A1/2SS

λmax(A1/2SS A1/2) <

1 + ε2(n − 1)2

λmin(A1/2SS A1/2) 1 − 2ε(n − 1) − ε2(n − 1)2

A1/2 has the following

9.2. Rate of convergence

The following theorem gives the rate of convergence of iSconD.

Theorem 15.

Let S

=

[v1 . . . vn], where {v1 . . . vn} is ε-approximate A-conjugate system.

If ε

≤

1 3(n−1)

then λmin(W)

>

31n , which means that the rate of iSconD is O(n log 1 ).

Proof. As in Lemma 13, we can show that W = n1 A1/2SS A, where S = [v1 . . . vn]. Using bound (23) and Corol-

lary 14.1, we get

1

1 + ε(n − 1)

λmin(W) > n 1 − ε(n − 1) 1 − ε(n − 1)

for

small

enough

ε

(see

Corollary

14.1).

For

ε

=

1 3(n−1)

we

get

λmin(W)

>

31n .

9.3. Experiment
Figure 6 illustrates the theoretical results about iSonD. For this experiment we generate random orthogonal matrix V and random symmetric positive deﬁnite matrix ˜I, which satisﬁes ˜Iii = 1, ˜Iij ≤ ε for i = j. Columns of matrix A−1/2V˜I1/2 were taken as approximate A-conjugate vectors.

Stochastic Spectral and Conjugate Descent Methods
100

Expected precision

10 1
10 2 0
Figure 6: Expected precision E ||xt−x∗||2A ||x0 −x∗ ||2A

= 3(n50 1)

= 3(n30 1)

= 3(n1 1)

=

3(1n0

3
1)

= 0

Nu50mber of100 iteratio150ns 200

vs. the number of iterations of iSconD with different choices of parameter ε.

9.4. Approximate solution without iterative methods

Note that the problem (1) is equivalent to the following problem of ﬁnding x such that

Ax = b.

(29)

Let S = [v1 . . . vn] be a set of A-conjugate vectors, i.e., S AS = I. We can now ﬁnd the solution to the linear system (29). Since S b = S Ax = S ASS−1x = S−1x, we conclude that

x = SS b.

(30)

We will now show that unlike in the exact case, using formula (30) with ε-approximate A-conjugate vectors does not lead to

a precise solution of our problem.

Lemma 16. Let S = [v1 . . . vn] be an ε-A-orthonormal system. Let x∗ = A−1b be the solution of the linear system (29). Let xˆ be an estimate of the solution, calculated with formula (30) using ε-approximate A-conjugate vectors: xˆ = SS b. If

ε < 1/(n − 1), then

1 + ε(n − 1)

xˆ − x∗

A

≤

ε(n

−

1) 1

−

ε(n

−

1)

x∗ A

Proof. Note that A1/2xˆ = A1/2SS that
and hence
Therefore,

A1/2A1/2x∗ = ˆIA1/2x∗, where ˆI = A1/2SS

λi(ˆI − I)

1 + ε(n − 1)

≤ ε(n − 1)

,

1 − ε(n − 1)

ˆI − I

1 + ε(n − 1)

≤ ε(n − 1)

.

2

1 − ε(n − 1)

A1/2. From Lemma 14 we now get

xˆ − x∗ A = A1/2(xˆ − x∗) 2 = (ˆI − I)A1/2x∗ 2 ≤ ˆI − I 2 A1/2x∗ 2 ≤ ε(n − 1) 11 +− εε((nn −− 11)) x∗ A .

If we choose ε = 3(n1−1) , like we did in Theorem 15, we get the following precision:
2 xˆ − x∗ A ≤ 3 x∗ A , which is rather poor. However if we use Algorithm 7, we can get approximate solution with any precision (after enough iterations).

Stochastic Spectral and Conjugate Descent Methods
10. Inexact SSD: a method that is not a special case of stochastic descent
In Section 2.1 we deﬁned Stochastic Spectral Descent (Algorithm 2). We now design a new method which will “try” to use the same iterations, but with inexact eigenvectors of A. We call w an inexact eigenvector of A if

Aw = λw + ε

(31)

for some ε and λ > 0 (inexact eigenvalue). Clearly, any vector can be written in the form (31). This idea leads to Algorithm 8.
Algorithm 8 Inexact Stochastic Spectral Descent (iSSD) Initialize: x0 ∈ Rn; (w1, λ1), . . . (wn, λn): inexact eigenvectors and eigenvalues of A for t = 0, 1, 2, . . . do Choose i ∈ [n] uniformly at random Set xt+1 = xt − wi xt − wλiib wi end for

Note that the above method is not equivalent to applying stochastic descent D being the uniform distribution over the inexact eigenvectors. This is because in arriving at SSD, we have used some properties of the eigenvectors and eigenvalues to simplify the calculation of the stepsize. The same simpliﬁcations do not apply for inexact eigenvectors. Nevertheless, we can formally run SSD, as presented in Algorithm 2, and replace the exact eigenvectors and eigenvalues by inexact versions thereof, thus capitalizing on the fast computation of stepsize which positively affects the cost of one iteration of the method. This leads to Algorithm 8.
Hence, in order to analyze the above method, we need to develop a completely new approach. We will show that Algorithm 8 converges only to a neighbourhood of the optimal solution.

10.1. Lemmas
Further we will use the following notation: S = [w1 . . . wn] – inexact eigenvectors matrix, Λ = Diag (λ1 . . . λn) – inexact eigenvalues matrix, E = [ε1 . . . εn] – error matrix, A˜ = SΛS – estimation of matrix A. We also assume, that inexact eigenvectors are ε-approximate orthonormal for ε < n−1 1 , i.e. wi wi = 1, wi wj ≤ ε for i = j. The following lemma gives an answer to the question: how precise is A˜ as an estimate of matrix A? Lemma 17. A˜ = ˆIA − SE , where matrix ˆI = SS satisﬁes

ˆI − I

1 + ε(n − 1)

≤ ε(n − 1)

.

(32)

2

1 − ε(n − 1)

Proof. Indeed, the deﬁnition of inexact eigenvectors can be written in matrix form as AS = SΛ + E, from which follows that ˆIA = SS A = SΛS + SE . Equality (32) follows immediately from Lemma 14.

The next lemma gives a general recursion capturing one step of iSSD, shedding light on the convergence of the method. Lemma 18. Sequence of {xt} generated by inexact SSD satisﬁes equality

E

xt+1 − x∗

2 A

=

1

21

1− n

E xt − x∗ A + n E (xt − x∗) Γ(xt − x∗)

+ n1 E xt 2EΛ−1E + x∗ EΛ−2CE x∗ − n2 E (xt − x∗) SCΛ−1E x∗ ,

where Γ = (I − ˆI)A − SE − EΛ−1E + SCS and

C = Diag w1 ε1 . . . wn εn .

(33)

Stochastic Spectral and Conjugate Descent Methods

Proof.

xt+1 − x∗

2 A

=

=

= =

=

εt x∗ 2 xt − x∗ − ωwtwt (xt − x∗) + ω λt wt A

2

2

εt x∗ 2

xt − x∗ A + ω wt Awt wt (xt − x∗) − λt

+2ω(xt − x∗) Awt

εt x∗ − w (xt − x∗)

λt

t

2

2

εt x∗ 2

xt − x∗ A + ω (λt + wt εt) wt (xt − x∗) − λt

+2ω(xt − x∗) (λtwt + εt) εt x∗ − w (xt − x∗)

λt

t

xt − x∗ 2 − ω(2 − ω)(xt − x∗) λtwtw (xt − x∗) + ω2 x∗ εtεt x∗

A

t

λt

(xt − x∗) εtεt x∗ 2

εt x∗ 2

+2ω λt

+ ω wt εt wt (xt − x∗) − λt

+2(ω − ω2)(xt − x∗) wtεt x∗ − 2ω(xt − x∗) wtεt (xt − x∗)

xt − x∗ 2A − ω(2 − ω)(xt − x∗) λtwtwt (xt − x∗) + x∗(ω − 1) + xt εtεt
λt

2

εt x∗ 2

− xt − x∗ εtλεtt + 2ω(xt − x∗) wtεt (x∗(2 − ω) − xt) + ω wt εt wt (xt − x∗) − λt .

Now we can take conditional expectation E[ · | xt].

2

2 ω(2 − ω)

21

21

2

E[ xt+1 − x∗ A | xt] = xt − x∗ A − n

xt − x∗ A˜ + n x∗(ω − 1) + xt Σ − n xt − x∗ Σ −

2ω

ω2 n

εi x∗ 2

− n (xt − x∗) SE (xt − (2 − ω)x∗) + n wi εi wi (xt − x∗) − λi ,

i=1

where Σ = EΛ−1E .

Now we set ω = 1 and use Lemma 17.

2

21

21

21

2

E[ xt+1 − x∗ A | xt] = xt − x∗ A − n xt − x∗ A˜ + n xt Σ − n xt − x∗ Σ −

2

1n

εi x∗ 2

− n (xt − x∗) SE (xt − x∗) + n wi εi wi (xt − x∗) − λi =

i=1

2

11

= xt − x∗ A

1− n

+ n (xt − x∗)

(I − ˆI)A + SE − 2SE

1

21

2

(xt − x∗) + n xt Σ − n xt − x∗ Σ +

1n

εi x∗ 2

+ n

wi εi wi (xt − x∗) − λi

=

i=1

1 = 1−
n

21 xt − x∗ A + n (xt − x∗)

(I − ˆI)A − SE − Σ (xt − x∗) + n1 xt 2Σ +

1n

εi x∗ 2

+ n

wi εi wi (xt − x∗) − λi

=

i=1

1 = 1−
n

21 xt − x∗ A + n (xt − x∗)

(I − ˆI)A − SE − Σ (xt − x∗) + n1 xt 2Σ +

+ n1 xt − x∗ 2SCS + n1 x∗ EΛ−2CE

2 x∗ − (xt − x∗)

SCΛ−1E

x∗,

n

Stochastic Spectral and Conjugate Descent Methods

where C = Diag w1 ε1 . . . wn εn . We get

E[

xt+1 − x∗

2 A

|

xt]

=

1 1−
n 1 + n

21 xt − x∗ A + n (xt − x∗) Γ(xt − x∗)

xt

2 EΛ−1 E

+ x∗ EΛ−2CE x∗ − 2(xt − x∗) SCΛ−1E x∗

,

where Γ = (I − ˆI)A − SE − EΛ−1E + SCS .

The following lemma describes which inexact eigenvalues are optimal for a ﬁxed set of inexact eigenvectors.

Lemma 19. Let wi be ﬁxed. Then the choice

λi = wi Awi

(34)

minimizes εi 2 in λ, where i := Awi − λwi 2. Moreover, for this choice of λi we get wi εi = 0.

Proof. Minimizing

Awi − λwi

2 2

in

λ

gives

(34).

For

this

choice

of

λi

we

get

wi

εi = wi

Awi − λiwi

wi = wi

Awi −

wi Awi = 0.

10.2. Convergence Choosing eigenvalues as deﬁned in (34), and in view of (33), we see that C = 0. From this and Lemma 18 we get

2

1

21

1

2

E xt+1 − x∗ A =

1− n

E xt − x∗ A + n E (xt − x∗) Γ(xt − x∗) + n E xt EΛ−1E ,

where Γ = (I − ˆI)A − SE − EΛ−1E . From the Cauchy–Schwarz inequality we get

1

2

1

2

2

2

2

2

n E xt EΛ−1E = n E xt − x∗ + x∗ EΛ−1E ≤ n E xt − x∗ EΛ−1E + n E x∗ EΛ−1E ,

which leads to

2

1

21

2

2

E xt+1 − x∗ A ≤

1− n

E

xt − x∗ A + n E (xt − x∗)

Q(xt − x∗)

+ n

x∗ EΛ−1E

,

(36)

where Q = (I − ˆI)A − SE + EΛ−1E . Inequality (36) implies that

2

2 q−1

2 r0

E xt+1 − x∗ A ≤ E xt − x∗ A +

n

E

xt − x∗

A+

, n

(37)

where q = max zz

QAzz , r0 = 2

x∗

2 EΛ−1 E

.

If the errors ε1, . . . , εn and ε are small enough, we can make q and r0 arbitrarily small for ﬁxed x∗. From (37) we can see

that E

xt+1 − x∗

2 A

is

going

to

decrease

as

long

as

2

r0

E

xt − x∗

A

≥

1

−

. q

(38)

Hence, for small enough ε1, . . . , εn and parameter ε, iSSD will converge to a neighborhood of the optimal solution, with limited precision (38).

