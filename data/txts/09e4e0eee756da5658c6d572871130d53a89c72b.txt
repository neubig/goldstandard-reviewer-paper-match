arXiv:2112.06283v2 [cs.GT] 21 Feb 2022

Bayesian Persuasion for Algorithmic Recourse
Keegan Harris Valerie Chen Joon Sik Kim Ameet Talwalkar Hoda Heidari
Zhiwei Steven Wu
School of Computer Science, Carnegie Mellon University∗
Abstract
When subjected to automated decision-making, decision subjects may strategically modify their observable features in ways they believe will maximize their chances of receiving a favorable decision. In many practical situations, the underlying assessment rule is deliberately kept secret to avoid gaming and maintain competitive advantage. The resulting opacity forces the decision subjects to rely on incomplete information when making strategic feature modiﬁcations. We capture such settings as a game of Bayesian persuasion, in which the decision maker oﬀers a form of recourse to the decision subject by providing them with an action recommendation (or signal ) to incentivize them to modify their features in desirable ways. We show that when using persuasion, both the decision maker and decision subject are never worse oﬀ in expectation, while the decision maker can be signiﬁcantly better oﬀ. While the decision maker’s problem of ﬁnding the optimal Bayesian incentive-compatible (BIC) signaling policy takes the form of optimization over inﬁnitely-many variables, we show that this optimization can be cast as a linear program over ﬁnitely-many regions of the space of possible assessment rules. While this reformulation simpliﬁes the problem dramatically, solving the linear program requires reasoning about exponentially-many variables, even under relatively simple settings. Motivated by this observation, we provide a polynomial-time approximation scheme that recovers a near-optimal signaling policy. Finally, our numerical simulations on semi-synthetic data empirically illustrate the beneﬁts of using persuasion in the algorithmic recourse setting.
1 Introduction
High-stakes decision-making systems increasingly utilize data-driven algorithms to assess individuals in such domains as education [33], employment [7, 39], and lending [27]. Individuals subjected to these assessments (henceforth, decision subjects) may strategically modify their observable features in ways they believe maximize their chances of receiving favorable decisions [11, 24]. The decision subject often has a set of actions/interventions available to them. Each of these actions leads to some measurable eﬀect on their observable features, and subsequently, their decision. From the decision maker’s perspective, some of these actions may be more desirable than others. Consider credit scoring as an example.1 Credit scores predict how likely an individual applicant is to pay back a loan on time. Financial institutions regularly utilize credit scores to decide whether to oﬀer applicants their ﬁnancial products and determine the terms and conditions of their oﬀers (e.g., by setting the interest rate or credit limit). Given their (partial) knowledge of
∗{keeganh, valeriechen, joonkim, talwalkar, hheidari, zstevenwu}@cmu.edu 1Other examples of strategic settings which arise as a result of decision-making include college admissions, in which a college/university (decision maker) decides whether or not to admit a prospective student (decision subject), hiring, in which a company decides whether or not to hire a job applicant, and lending, in which a banking institution decides to accept or reject someone applying for a loan. Oftentimes, the decision maker is aided by automated decision-making tools in these situations (e.g., [27, 33, 41]).
1

credit scoring instruments, applicants regularly attempt to improve their scores. For instance, a business applying for a loan may improve its score by paying oﬀ existing debt or cleverly manipulating its ﬁnancial records to appear more proﬁtable. While both of these interventions may improve credit score, the former is more desirable than the latter from the perspective of the ﬁnancial institution oﬀering the loan. The question we are interested in answering in this work is: how can the decision maker incentivize decision subjects to take such beneﬁcial actions while discouraging manipulations?
The strategic interactions between decision-making algorithms and decision subjects has motivated a growing literature known as strategic learning (see e.g., [14, 21, 22, 32, 44]). While much of the prior work in strategic learning operates under the assumption of full transparency (i.e., the assessment rule is public knowledge), we consider settings where the full disclosure of the assessment rule is not a viable alternative. In many real-world situations, revealing the exact logic of the decision rule is either infeasible or irresponsible. For instance, credit scoring formulae are closely guarded trade secrets, in part to prevent the risk of default rates surging if applicants learn how to manipulate them. In such settings, the decision maker may still have a vested interest in providing some information about the decision rule to decision subjects to provide a certain level of transparency and recourse. In particular, the decision maker may be legally obliged, or economically motivated, to guide decision subjects to take actions that improve their underlying qualiﬁcations. To do so, the decision maker can recommend actions for decision subjects to take. Of course, such recommendations need to be chosen carefully and credibly; otherwise, self-interested decision subjects may not follow them or, even worse, they may utilize the recommendations to ﬁnd pathways for manipulation.
We study a model of strategic learning in which the underlying assessment rule is not revealed to decision subjects. Our model captures several key aspects of the setting described above: First, even though the assessment rule is not revealed to the decision subjects, they often have prior knowledge about what the rule may be. Secondly, when the decision maker provides recommendations to decision subjects on which action to take, the recommendations should be compatible with the subjects’ incentives to ensure they will follow the recommendation. Finally, our model assumes the decision maker discloses how they generate recommendations for recourse—an increasingly relevant requirement under recent regulations (e.g., [12]).
Utilizing our model, we aim to design a mechanism for a decision maker to provide recourse to a decision subject with incomplete information about the underlying assessment rule. We assume the assessment rule makes predictions about some future outcome of the decision subject (e.g., whether they pay back the loan in time if granted). Before the assessment rule is trained (i.e., before the model parameters are ﬁt), the decision maker and decision subject have some prior belief about the realization of the assessment rule. This prior represents the “common knowledge” about the importance of various observable features for making accurate predictions. After training, the assessment rule is revealed to the decision maker, who then recommends an action for the decision subject to take, based on their pre-determined signaling policy. Upon receiving this action recommendation, the decision subject updates their belief about the underlying assessment rule. They then take the action which they believe (according to the update belief) will maximize their expected utility (i.e., the beneﬁt from the decision they receive, minus the cost of taking their selected action). Finally, the decision maker uses the assessment rule to make a prediction about the decision subject.
The interaction described above is an instance of Bayesian persuasion, a game-theoretic model of information revelation originally due to Kamenica and Gentzkow. For background on the general Bayesian persuasion model, see Section 1.1. The speciﬁc instance of Bayesian persuasion we consider in this work is summarized below.
2

Interaction protocol for our setting
1. Before training, the decision maker and decision subject have some prior/belief about the true assessment rule.
2. After training, the assessment rule is revealed to the decision maker.
3. The decision maker then uses their signaling policy and knowledge of the assessment rule to recommend an action for the decision subject to take.
4. The decision subject updates their belief given the recommendation. They then take a (possibly diﬀerent) action, and receive a prediction through the assessment rule.
Our contributions Our central conceptual contribution is to cast the problem of oﬀering recourse under partial transparency as a game of Bayesian persuasion. Our key technical contributions consist of comparing optimal action-recommendation policies in this new setup with two natural alternatives: (1) fully revealing the assessment rule to the decision subjects, or (2) revealing no information at all about the assessment rule. We provide new insights about the potentially signiﬁcant advantages of action recommendation over these baselines, and oﬀer eﬃcient formulations to derive the optimal recommendations. More speciﬁcally, our analysis oﬀers the following takeaways:
1. Using tools from Bayesian persuasion, we show that it is possible for the decision maker to provide incentive-compatible action recommendations that encourage rational decision subjects to modify their features through beneﬁcial interventions (Section 2.1).
2. Perhaps most importantly, we show that the optimal signaling policy is more eﬀective than the above two baselines in encouraging positive interventions on the part of the decision subjects (Section 3).
3. While the decision maker and decision subjects are never worse oﬀ in expectation from using optimal incentive-compatible recommendations, we show that situations exist in which the decision maker is signiﬁcantly better oﬀ in expectation utilizing the optimal signaling policy (as opposed to the two baselines) (Section 3.1).
4. We derive the optimal signaling policy for the decision maker. While the decision maker’s optimal signaling policy initially appears challenging (as it involves optimizing over continuously-many variables), we show that the problem can naturally be cast as a linear program (Section 4).
5. We show that even for relatively simple examples, solving this linear program requires reasoning about exponentially-many variables. Motivated by this observation, we provide a polynomial-time algorithm to approximate the optimal signaling policy up to additive terms (Section 5).
6. Finally, we empirically evaluate our persuasion mechanism on semi-synthetic data based on the Home Equity Line of Credit (HELOC) dataset, and ﬁnd that the optimal signaling policy performs signiﬁcantly better than the two natural alternatives in practice (Section 6).
1.1 Related Work
Bayesian Persuasion. In its most basic form, Bayesian persuasion [30] is modeled as a game between a sender (with private information) and a receiver. At the beginning of the game, the
3

sender and receiver share a prior over some unknown state of nature, which will eventually be revealed to the sender. Before the state of nature is revealed, the sender commits to a signaling policy, a (probabilistic) mapping from states of nature to action recommendations.2 After the sender commits to a signaling policy, the state of nature is revealed to the sender, who then sends a signal (according to their policy) to the receiver. The receiver uses this signal to form a posterior over the possible states of nature, and then takes an action which aﬀects the payoﬀs of both players. Several extensions to the original Bayesian persuasion model have been proposed, including persuasion with multiple receivers [3], persuasion with multiple senders [35], and persuasion with heterogeneous priors [2]. There has been growing interest in persuasion in the computer science and machine learning communities in recent years. Dughmi and Xu [15, 16] characterize the computational complexity of computing the optimal signaling policy for several popular models of persuasion. Castiglioni et al. [6] study the problem of learning the receiver’s utilities through repeated interactions. Work in the multi-arm bandit literature [8, 25, 36, 37, 43] leverages techniques from Bayesian persuasion to incentivize agents to perform bandit exploration. Strategic responses to unknown predictive models. To the best of our knowledge, our work is the ﬁrst to use tools from persuasion to model the strategic interaction between a decision maker and strategic decision subjects when the underlying predictive model is not public knowledge. Several prior articles have addressed similar problems through diﬀerent models and techniques. For example, Akyol et al. [1] quantify the “price of transparency”, a quantity which compares the decision maker’s utility when the predictive model is fully known with their utility when the model is not revealed to the decision subjects. Ghalme et al. [20] compare the prediction error of a classiﬁer when it is public knowledge with the error when decision subjects must learn a version of it, and label this diﬀerence the “price of opacity”. They show that small errors in decision subjects’ estimates of the true underlying model may lead to large errors in the performance of the model. The authors argue that their work provides formal incentives for decision makers to adopt full transparency as a policy. Our work, in contrast, is based on the observation that even if decision makers are willing to reveal their models, legal requirements, privacy concerns, and intellectual property restrictions may prohibit full transparency. So we instead study the consequences of partial transparency—a commonplace condition in real-world domains.
Bechavod et al. [4] study the eﬀects of information discrepancy across diﬀerent sub-populations of decision subjects on their ability to improve their observable features in strategic learning settings. Like us, they do not assume the predictive model is fully known to the decision subjects. Instead, the authors model decision subjects as trying to infer the underlying predictive model by learning from their social circle of family and friends, which naturally causes diﬀerent groups to form within the population. In contrast to this line of work, we study a setting in which the decision maker provides customized feedback to each decision subject individually. Additionally, while the models proposed by [4, 20] circumvent the assumption of full information about the deployed model, they restrict the decision subjects’ knowledge to be obtained only through past data. Algorithmic recourse. Our work is closely related to recent work on algorithmic recourse [31]. Algorithmic recourse is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. A line of algorithmic recourse methods including [28, 46, 47] focus on ﬁnding recourses that are actionable, or realistic, for decision subjects to take to improve their decision. In contrast, our action recommendations are “actionable” in the sense that they are interventions which promote longterm desirable behaviors while ensuring that the decision subject is not worse oﬀ in expectation. Finally, more recent work [45] shows that existing recourse methods based on counterfactual
2Such commitment is especially possible when the sender is a software agent (as is the case in our setting), since the agent is committed to playing the policy prescribed by its code once it is deployed.
4

approaches are not robust to manipulations. Our approach to recourse is not counterfactual-based and instead uses a Bayesian persuasion mechanism to ensure decision subject compliance. Transparency. Recent legal and regulatory frameworks, such as the General Data Protection Regulation (GDPR) [12], motivate the development of forms of algorithmic transparency suitable for real-world deployment. While this work can be thought of as providing additional transparency into the decision-making process, it does not naturally fall into the existing organizations of explanation methods (e.g., as outlined in [9]), as our policy does not simply recommend actions based on the decision rule. Rather, our goal is to incentivize actionable interventions on the decision subjects’ observable features which are desirable to the decision maker, and we leverage persuasion techniques to ensure compliance. One of the most prevalent use cases of automated decision-making is credit scoring models (a widely used one is the FICO scoring model). These models evaluate an individual’s credit worthiness based on that individual’s payment history, credit utilization, credit history, and other factors, all of which are then weighted based on proprietary formulas. Two statutes, the Fair Credit Reporting Act (FCRA) and the Equal Credit Opportunity Act (ECOA), govern these models and enforce a requirement to provide individuals who are adversely impacted by such automated decision-making with a statement of reasons, or an outcome-based explanation [42]. Other strategic learning settings. The strategic learning literature [4, 10, 19–23, 26, 32, 34] broadly studies machine learning questions in the presence of strategic decision subjects. There has been a long line of work in strategic learning that focuses on how strategic decision subjects adapt their input to a machine learning algorithm in order to receive a more desirable prediction, although most prior work in this literature assumes that the underlying assessment rule is fully revealed to the decision subjects, which is typically not true in reality.
2 Setting and Background
Consider a setting in which a decision maker assigns a predicted label yˆ ∈ {−1, +1} (e.g., whether or not someone will repay a loan if granted one) to a decision subject with observable features x0 = (x0,1, · · · , x0,d−1, 1) ∈ Rd (e.g., amount of current debt, bank account balance, etc.).3 We assume the decision maker uses a linear decision rule to make predictions, i.e., yˆ = sign{x0 θ}, where the assessment rule θ ∈ Θ ⊆ Rd is chosen by the decision maker. The goal of the decision subject is to receive a positive classiﬁcation (e.g., get approved for a loan). Given this goal, the decision subject may choose to take some action a from some set of possible actions A to modify their observable features (for example, they may decide to pay oﬀ a certain amount of existing debt, or redistribute their debt to game the credit score). We assume that the decision subject has m actions {a1, a2, . . . am} ∈ A at their disposal in order to improve their outcomes. For convenience, we add a∅ to A to denote taking "no action". By taking action a, the decision subject incurs some cost c(a) ∈ R. This could be an actual monetary cost, but it can also represent non-monetary notions of cost such as opportunity cost or the time/eﬀort cost the decision subject may have to exert to take the action. We assume taking an action a changes a decision subject’s observable feature values from x0 to x0 + ∆x(a), where ∆x(a) ∈ Rd, and ∆xj(a) speciﬁes the change in the jth observable feature as the result of taking action a. For the special case of a∅, we have ∆x(a∅) = 0, c(a∅) = 0. As a result of taking action a, a decision subject, ds, receives utility uds(a, θ) = sign{(x0 + ∆x(a)) θ} − c(a). In other words, the decision subject receives some positive (negative) utility for a positive (negative) classiﬁcation, subject to some cost for taking said action.
If the decision subject had exact knowledge of the assessment rule θ used by the decision maker, they could solve an optimization problem to determine the best action to take in order to maximize their utility. However, in many settings it is not realistic for a decision subject to
3We append a 1 to the decision subject’s feature vector for notational convenience.
5

have perfect knowledge of θ. Instead, we model the decision subject’s information through a prior Π over θ, which can be thought of as “common knowledge” about the relative importance of each observable feature to the classiﬁer. We will use π(·) to denote the probability density function of Π (so that π(θ) denotes the probability of the deployed assessment rule being θ). We assume the decision subject is rational and risk-neutral. So at any point during the interaction, if they hold a belief Π about the underlying assessment rule, they would pick an action a∗ that maximize their expected utility with respect to that belief. More precisely, they solve:
a∗ ∈ arg max Eθ∼Π [uds(a, θ)].
a∈A
From the decision maker’s perspective, some actions may be more desirable than others. For example, a bank may prefer that an applicant pay oﬀ more existing debt than less when applying for a loan. To formalize this notion of action preference, we say that the decision maker receives some utility udm(a) ∈ R when the decision subject takes action a. In the loan example, udm(pay oﬀ more debt) > udm(pay oﬀ less debt).

2.1 Bayesian Persuasion in the Algorithmic Recourse Setting

The decision maker has an information advantage over the decision subject, due to the fact that they know the true assessment rule θ, whereas the decision subject does not. The decision maker may be able to leverage this information advantage to incentivize the decision subject to take a more favorable action (compared to the one they would have taken according to their prior) by recommending an action to the decision subject according to a commonly known signaling policy.

Deﬁnition 2.1 (Signaling Policy). A signaling policy S : Θ → A is a (possibly stochastic) mapping from assessment rules to actions.4

We use σ ∼ S(θ) to denote the action recommendation sampled from signaling policy S, where σ is a realization from A.
The decision maker’s signaling policy is assumed to be ﬁxed and common knowledge. This is because in order for the decision subject to perform a Bayesian update based on the observed recommendation, they must know the signaling policy. Additionally, the decision maker must have the power of commitment, i.e., the decision subject must believe that the decision maker will select actions according to their signaling policy. In our setting, this means that the decision maker must commit to their signaling policy before training their assessment rule. This can be seen as a form of transparency, as the decision maker is publicly committing to how they will use their assessment rule to provide action recommendations/recourse before they even train it. For simplicity, we assume that the decision maker shares the same prior beliefs Π as the decision subject over the observable features before the model is trained. These assumptions are standard in the Bayesian persuasion literature (see, e.g., [30, 36, 37]).
In order for the decision subject to be incentivized to follow the actions recommended by the decision maker, the signaling policy S needs to be Bayesian incentive-compatible.

Deﬁnition 2.2 (Bayesian incentive-compatibility). Consider a decision subject ds with initial

observable features x0 and prior Π. A signaling policy S is Bayesian incentive-compatible (BIC)

for ds if

Eθ∼Π[uds(a, θ)|σ = a] ≥ Eθ∼Π[uds(a , θ)|σ = a],

(1)

for all actions a, a ∈ A such that S(θ) had positive support on σ = a.

4Note that since our model is focused on the decision maker’s interactions with a single decision subject, we drop the dependence of σ on the decision subject’s characteristics.

6

In other words, a signaling policy S is BIC if, given that the decision maker recommends action a, the decision subject’s expected utility is at least as high as the expected utility of taking any other action a under the posterior.
We remark that while for the ease of exposition our model focuses the interactions between the decision maker and a single decision subject, our results can be extended to a heterogeneous population of decision subjects. Under such a heterogeneous setting, the decision maker would publicly commit to a method of computing the signaling policy, given a decision subject’s initial observable features as input. Once a decision subject arrives, their feature values are observed and the signaling policy is computed.
3 The Motivation Behind Persuasion
As is the case in the Bayesian persuasion literature [16, 29, 30], the decision maker can in general achieve a higher expected utility with an optimized signaling policy than the utilities had they provided no recommendation or fully disclosed the model. To characterize how much leveraging the decision maker’s information advantage (by recommending actions according to a BIC signaling policy) may improve their expected utility, we study the following example.
Consider a simple setting under which a single decision subject has one observable feature x0 (e.g., credit score) and two possible actions: a∅ = “do nothing” (i.e., ∆x(a∅) = 0, c(a∅) = 0, udm(a∅) = 0) and a1 = “pay oﬀ existing debt” (i.e., ∆x(a1) > 0, c(a1) > 0, udm(a1) = 1), which in turn raises their credit score. For the sake of our illustration, we assume credit-worthiness to be a mutually desirable trait, and credit scores to be a good measure of credit-worthiness. We assume the decision maker would like to design a signaling policy to maximize the chance of the decision subject taking action a1, regardless of whether or not the applicant will receive the loan. In this simple setting, the decision maker’s decision rule can be characterized by a single threshold parameter θ, i.e., the decision subject receives a positive classiﬁcation if x + θ ≥ 0 and a negative classiﬁcation otherwise. Note that while the decision subject does not know the exact value of θ, they instead have some prior over it, denoted by Π.
Given the true value of θ, the decision maker recommends an action σ ∈ {a∅, a1} for the decision subject to take. The decision subject then takes a possibly diﬀerent action a ∈ {a∅, a1}, which changes their observable feature from x0 to x = x0 + ∆x(a). Recall that the decision subject’s utility takes the form uds(a, θ) = sign{(x0 + ∆x(a)) + θ} − c(a). Note that if c(a1) > 2, then uds(a∅, θ) > uds(a1, θ) holds for any value of b, meaning that it is impossible to incentivize any rational decision subject to play action a1. Therefore, in order to give the decision maker a “ﬁghting chance” at incentivizing action a1, we assume the cost of action a1 is such that c(a1) < 2.
We observe that in this simple setting, we can bin values of θ into three diﬀerent “regions”, based on the outcome the decision subject would receive if θ were actually in that region. First, if x0 + ∆x(a1) + θ < 0, the decision subject will not receive a positive classiﬁcation, even if they take action a1. In this region, the decision subject’s initial feature value x0 is “too low” for taking the desired action to make a diﬀerence in their classiﬁcation. We refer to this region as region L. Second, if x0 + θ ≥ 0, the decision subject will receive a positive classiﬁcation no matter what action they take. In this region, x0 is “too high” for the action they take to make any diﬀerence on their classiﬁcation. We refer to this region as region H. Third, if x0 + θ < 0 and x0 + ∆x(a1) + θ ≥ 0, the decision subject will receive a positive classiﬁcation if they take action a1 and a negative classiﬁcation if they take action a∅. We refer to this region as region M . Consider the following signaling policy.
7

Signaling policy S(θ)
Case 1: θ ∈ L. Recommend action a1 with probability q and action a∅ with probability 1 − q Case 2: θ ∈ M . Recommend action a1 with probability 1 Case 3: θ ∈ H. Recommend action a1 with probability q and action a∅ with probability 1 − q

In Case 2, S recommends the action (a1) that the decision subject would have taken had they known the true θ, with probability 1. However, in Case 1 and Case 3, the decision maker recommends, with probability q, an action (a1) that the decision subject would not have taken knowing θ, leveraging the fact that the decision subject does not know exactly which case they are currently in. If the decision subject follows the decision maker’s recommendation from S, then the decision maker expected utility will increase from 0 to q if the realized θ ∈ L or θ ∈ H, and will remain the same otherwise. Intuitively, if q is “small enough” (where the precise deﬁnition of “small” depends on the prior over θ and the cost of taking action a1), then it will be in the decision subject’s best interest to follow the decision maker’s recommendation, even though they know that the decision maker may sometimes recommend taking action a1 when it is not in their best interest to take that action! That is, the decision maker may recommend that a decision subject pay oﬀ existing debt with probability q when it is unnecessary for them to do so in order to secure a loan. We now give a criteria on q which ensures the signaling policy S is BIC.
Proposition 3.1. Signaling policy S is Bayesian incentive-compatible if q = min{ πc((aM1))((12−−πc((Ma1)))) , 1}, where π(M ) = π(x0 + θ < 0 and x0 + ∆x(a1) + θ ≥ 0).

Proof Sketch. We show that Eθ∼Π[uds(a∅, θ)|σ = a∅] ≥ Eθ∼Π[uds(a1, θ)|σ = a∅] and Eθ∼Π[uds(a1, θ)|σ = a1] ≥ Eθ∼Π[uds(a∅, θ)|σ = a1]. Since these conditions are satisﬁed, S is BIC.

Proof. Based on the decision subject’s prior over θ, they can calculate

(1) π(L) = π(x0 + ∆x(a1) + θ < 0), i.e., the probability the decision subject is in region L according to the prior

(2) π(M ) = π(x0 + θ < 0 and x0 + ∆x(a1) + θ ≥ 0), i.e., the probability the decision subject is in region M according to the prior

(3) π(H) = π(x0 + θ ≥ 0), i.e., the probability the decision subject is in region H according to the prior

Case 1: σ = a0. Given the signal σ = a∅, the decision subject’s posterior probability density function π(·|σ = a∅) over L, M , and H will take the form

π(L|σ

=

a∅)

=

p(σ=a∅|L)π(L) p(σ=a )

=

π(L) π(L)+π(H )

∅

π(M |σ

=

a∅)

=

p(σ=a∅|M )π(M ) p(σ=a )

=

0

∅

π(H |σ

=

a∅)

=

p(σ=a∅ |H )π(H ) p(σ=a )

=

π(H ) π(L)+π(H )

∅

If the decision subject receives signal σ = a0, they know with probability 1 that they are not in region M with probability 1. Therefore, they know that taking action a1 will not change their classiﬁcation, so they will follow the decision maker’s recommendation and take action a∅.
Case 2: σ = a1. Given the signal σ = a1, the decision subject’s posterior density over L, M , and H will take the form

8

π(L|σ = a1) = p(σ=p(aσ1=|La)1π)(L) = π(M )+qq(ππ((LL))+π(H)) = π(M )+qqπ((1L−)π(M )) π(M |σ = a1) = p(σ=pa(σ1|=Ma)1π)(M ) = π(M )+qπ(π(M(L))+π(H)) = π(M )+πq((M1−)π(M )) π(H |σ = a1) = p(σ=p(aσ1=|Ha)1π)(H) = π(M )+qq(ππ((HL))+π(H)) = π(M )+qπq((1H−)π(M )) The decision subject’s expected utility of taking actions a∅ and a1 under the posterior induced by σ = a1 are Eθ∼Π[uds(a∅, θ)|σ = a1] = π(H|σ = a1) · (1 − 0) + π(M |σ = a1) · (−1 − 0) + π(L|σ = a1) · (−1 − 0)
= π(H|σ = a1) − π(M |σ = a1) − π(L|σ = a1) and
Eθ∼Π[uds(a1, θ)|σ = a1] = π(H|σ = a1) · (1 − c(a1)) + π(M |σ = a1) · (1 − c(a1)) + π(L|σ = a1) · (−1 − c(a1))
In order for S to be BIC,
Eθ∼Π[uds(a1, θ)|σ = a1] ≥ Eθ∼Π[uds(a∅, θ)|σ = a1]. Plugging in our expressions for Eθ∼Π[uds(a1, θ)|σ = a1] and Eθ∼Π[uds(a∅, θ)|σ = a1], we see that
π(H|σ = a1) · (1 − c(a1))+π(M |σ = a1) · (1 − c(a1)) + π(L|σ = a1) · (−1 − c(a1)) ≥ π(H|σ = a1) − π(M |σ = a1) − π(L|σ = a1)
After canceling terms and simplifying, we see that
−(π(L|σ = a1) + π(H|σ = a1))c(a1) + π(M |σ = a1)(2 − c(a1)) ≥ 0
Next, we plug in for π(L|σ = a1), π(M |σ = a1), and π(H|σ = a1). Note that the denominators of π(L|σ = a1), π(M |σ = a1), and π(H|σ = a1) cancel out.

−q(π(L) + π(H))c(a1) + π(M )(2 − c(a1)) = −q(1 − π(M ))c(a1) + π(M )(2 − c(a1)) ≥ 0

Solving for q, we see that

q ≤ π(M )(2 − c(a1)) . c(a1)(1 − π(M ))

Note that q ≥ 0 always. Finally, in order for q to be a valid probability, we restrict q such that

This completes the proof.

q = min{ π(M )(2 − c(a1)) , 1}. c(a1)(1 − π(M ))

Under this setting, the decision maker will achieve expected utility u = π(M ) + q(1 − π(M )). See Figure 1 for an illustration of how q and u vary with π(M ) and c(a1).
But how much better can the decision maker do by recommending actions via a BIC signaling policy, compared to natural alternatives? We answer this question concretely in the following section.

9

q utility

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0

0.0

0.5

1.0

0

1

2

0.0

0.5

1.0

0

1

2

π(M )

c(a1)

π(M )

c(a1)

(a) q as a function of π(M ) (c(a1) = 1, left) and (b) u as a function of π(M ) (c(a1) = 1, left) and

c(a1) (π(M ) = 12 , right).

c(a1) (π(M ) = 12 , right).

Figure 1: Illustration of how q (the probability of recommending action a1 when θ ∈ M , left) and u (the expected decision maker utility, right) change as a function of π(M ) and c(a1). As π(M ) increases, q and expected utility increase until π(M ), at which point they remain constant. As
c(a1) increases, q and u remain constant until taking action a1 becomes prohibitively expensive, at which point both start to decay.

3.1 Unbounded Utility Improvements Using Persuasion
As we will see in Section 4, the expected utility of the decision maker when recommending actions via the optimal (BIC) signaling policy is trivially no worse than their expected utility if they had revealed full information about the assessment rule to the decision subject, or if they had revealed no information and let the decision subject act according to the prior. In this section, we show that the decision maker’s expected utility when recommending actions according to the optimal signaling policy can be arbitrarily higher than their expected utility from revealing full information or no information. In particular, we prove the following theorem.
Theorem 3.2. For any > 0, there exists a problem instance such that the expected decision maker utility from recommending actions according to the optimal signaling policy is 1 − and the expected decision maker utility for revealing full information or revealing no information is at most .
Proof. Consider the example in Section 3. Expected utility from revealing no information. If the decision subject acts exclusively according to the prior, they will select action a1 with probability 1 if Eθ∼Π[uds(a1, θ)] ≥ Eθ∼Π[uds(a∅, θ)] and with probability 0 otherwise. Plugging in our expressions for Eθ∼Π[uds(a1, θ)] and Eθ∼Π[uds(a∅, θ)], we see that the decision subject will select action a1 only if
π(L)(−1−c(a1))+π(M )(1−c(a1))+π(H)(1−c(a1)) ≥ π(L)(−1−0)+π(M )(−1−0)+π(H)(1−0)
Canceling terms and simplifying, we see that
−c(a1)(π(L) + π(H)) + π(M )(2 − c(a1)) ≥ 0
must hold for the decision subject to select action a1. Finally, substituting π(L)+π(H) = 1−π(M ) gives us the condition 2π(M ) − c(a1) ≥ 0. Alternatively, if πc((aM1)) < 12 , the decision subject will select action a∅ with probability 1. Intuitively, this means that a rational decision subject would take action a1 if the ratio of π(M ) (the probability according to the prior that taking action a1 is in the decision subject’s best interest) to c(a1) (the cost of taking action a1) is high, and would take action a∅ otherwise. Expected utility from revealing full information. If the decision maker reveals the assessment rule to the decision subject, they will select action a1 when θ ∈ M and action a∅ otherwise. Therefore since udm(a1) = 1 and udm(a∅) = 0, the decision maker’s expected utility if they reveal full information is π(M ).
10

Expected utility from S. Recall that the decision maker’s signaling policy S from Section 3 sets q = min{ πc((aM1))((12−−πc((Ma1)))) , 1}. Under this setting, the decision maker’s expected utility is min{1 · π(M ) + q · (1 − π(M )), 1}. Substituting in our expression for q and simplifying, we see that the decision maker’s expected utility for recommending actions via S is min{ 2cπ((aM1)) , 1}.
Suppose that 2π(M ) = c(a1)(1 − ) and c(a1) = 2 , for some small > 0. The decision maker’s expected utility will always be 0 from revealing no information because 2cπ((aM1)) = 1 − < 1. The decision maker’s expected utility from recommending actions via S will be 2cπ((aM1)) = 1 − . Since π(M ) = (1 − ) < , the decision maker’s expected utility from revealing full information
will be less than . Therefore, as approaches 0, the decision maker’s expected utility from
revealing full information approaches 0 (the smallest value possible), and the decision maker’s
expected utility from S approaches 1 (the highest value possible). This completes the proof.

The decision maker’s expected utility as a function of their possible strategies is summarized in
Table 1. Note that when 1{π(M ) ≥ c(a21) } = 1, q = 1. Therefore, the decision maker’s expected
utility is always as least as good as the two natural alternatives of revealing no information about
the assessment rule, or revealing full information about the rule.

No information Signaling with S Full information

Decision maker utility 1{π(M ) ≥ c(a21) } π(M ) + q(1 − π(M ))

π(M )

Table 1: Decision maker’s expected utility when (1) revealing no information about the model, (2) recommending actions according to S, and (3) revealing full information about the model. See Section 3.1 for the full derivations.

4 Optimal Signaling Policy

In Section 3, we show a one-dimensional setting, where a signaling policy can obtain unbounded better utilities compared to revealing full information and revealing no information. We now derive the decision maker’s optimal signaling policy for the general setting with arbitrary numbers of observable features and actions described in Section 2. Under the general setting, the decision maker’s optimal signaling policy can be described by the following optimization:

max

Eσ∼S,θ∼Π[udm(σ)]

p(σ=a|θ),∀a∈A

(2)

s.t. Eθ∼Π[uds(a, θ) − uds(a , θ)|σ = a] ≥ 0, ∀a, a ∈ A,

where we omit the valid probability constraints over p(σ = a|θ), a ∈ A for brevity. In words, the decision maker wants to design a signaling policy S in order to maximize their expected utility, subject to the constraint that the signaling policy is BIC. At ﬁrst glance, the optimization may initially seem hopeless as there are inﬁnitely many values of p(σ = a|θ) (one for every possible θ ∈ Θ) that the decision maker’s optimal policy must optimize over. However, we will show that the decision maker’s optimal policy can actually be recovered by optimizing over ﬁnitely many variables.
By rewriting the BIC constraints as integrals over Θ and applying Bayes’ rule, our optimization over p(σ = a|θ), a ∈ A takes the following form

max

Eσ∼S,θ∼Π[udm(σ)]

p(σ=a|θ),∀a∈A

s.t.

p(σ = a|θ)π(θ)(uds(a, θ) − uds(a , θ))dθ ≥ 0, ∀a, a ∈ A.

Θ

11

θ2 1
R2
{a2}
1 2
R0
{∅}

0

1

2

R0 {a1, a2}
R1 {a1}

θ1 2

Figure 2: An illustration of the equivalence regions for a two action (a1, a2) and two observable feature (x1, x2) setting, where Θ = [0, 2] × [0, 1] × { 21 }. Consider an individual with x0 = [0, 0, 1] , ∆x(a1) = [1, 0, 0] , and ∆x(a2) = [0, 1, 0] . The equivalence regions of Θ are quadrants described
the set of actions the decision subject could take in order to receive a positive classiﬁcation. Region
R0 contains the bottom-left and top-right quadrants of Θ, region R1 contains the bottom-right
quadrant of Θ, and region R2 contains the top-left quadrant of Θ.

Note that if uds(a, θ) − uds(a , θ) is the same for some “equivalence region” R ⊆ Θ (which we formally deﬁne below), we can pull uds(a, θ) − uds(a , θ) out of the integral and instead sum over the diﬀerent equivalence regions. Intuitively, an equivalence region can be thought of as the set of all θ ∈ Θ pairs that are indistinguishable from a decision subject’s perspective because they lead to the exact same utility for any possible action the decision subject could take. Based on this idea, we formally deﬁne a region of Θ as follows.
Deﬁnition 4.1 (Equivalence Region). Two assignments θ, θ are equivalent (w.r.t. uds) if uds(a, θ) − uds(a , θ) = uds(a, θ ) − uds(a , θ ), ∀a, a ∈ A. An equivalence region R is a subset of Θ such that for any θ ∈ R, all θ equivalent to θ are also in R. We denote the set of all equivalence regions by R.
In Figure 2, we show an example of how diﬀerent equivalence regions might partition the space of possible assessment rules Θ. In this example, there are two actions and two observable features, and the space of Θ is partitioned into three diﬀerent equivalence regions. Note that as long as the set of actions A is ﬁnite, |R| < ∞. After pulling the decision subject utility function out of the integral, our optimization takes the following form:

max

Eσ∼S,θ∼Π[udm(σ)]

p(σ=a|θ),∀a∈A

s.t.

(uds(a, R) − uds(a , R))

p(σ = a|θ)π(θ)dθ ≥ 0, ∀a, a ∈ A.

R∈R

θ∈R

Now that the decision subject’s utility uds(·) no longer depends on θ, we can integrate p(σ = a|θ)π(θ) over each equivalence region R. We denote p(R) as the probability that the true θ ∈ R according to the prior.

max

Eσ∼S,θ∼Π[udm(σ)]

p(σ=a|R),∀a∈A,R∈R

s.t.

p(σ = a|R)π(R)(uds(a, R) − uds(a, R)) ≥ 0, ∀a, a ∈ A.

R∈R

Since it is possible to write the constraints in terms of p(σ = a|R), ∀a ∈ A, R ∈ R, it suﬃces to optimize directly over these quantities. The ﬁnal step is to rewrite the objective. For completeness, we include the constraints which make each {p(σ = a1|R), p(σ = a2|R), . . . , p(σ = am|R)}, ∀R a valid probability distribution.

12

ad,1

...

ad,md

...

a∅

ai,1

...

ai,mi

...

a1,1

...

a1,m1

Figure 3: Graphical representation of special ordering over the actions available to each decision subject. Each branch corresponds to an observable feature and each node corresponds to a possible action the decision subject may take. The root corresponds to taking no action (denoted by a∅). Nodes further away from the root on branch i correspond to higher ∆xi, i.e., ∆xi(a∅) ≺ ∆xi(ai,1) ≺ . . . ≺ xi(ai,mi ).

Theorem 4.2 (Optimal signaling policy). The decision maker’s optimal signaling policy can be characterized by the following linear program OPT-LP:

max
p(σ=a|R),∀a∈A,R∈R
s.t.

p(R)p(σ = a|R)udm(a)
a∈A R∈R
p(σ = a|R)p(R)(uds(a, R) − uds(a , R)) ≥ 0, ∀a, a ∈ A
R∈R
p(σ = a|R) = 1, ∀R, p(σ = a|R) ≥ 0, ∀R ∈ R, a ∈ A,
a∈A

(OPT-LP)

where p(σ = a|R) denotes the probability of sending recommendation σ = a if θ ∈ R. Note that the linear program OPT-LP is always feasible, as the decision maker can always recommend the action the decision subject would play according to the prior, which is BIC.

5 Computing the Optimal Signaling Policy
In Section 4, we show that the problem of determining the decision maker’s optimal signaling policy can be transformed from an optimization over inﬁnitely many variables into an optimization over the set of ﬁnitely many equivalence regions R (Theorem 4.2). However, as we will show in Section 5.1, computing the decision maker’s optimal signaling policy by solving (OPT-LP) requires reasoning over exponentially-many variables, even in relatively simple settings. This motivates the need for a computationally eﬃcient algorithm to approximate (OPT-LP), which we present in Section 5.2.
5.1 Computational Barriers
In this section, we show that even in the setting where each action only aﬀects one observable feature (e.g., as shown in Figure 3), the number of equivalence regions in (OPT-LP) is still exponential in the size of the input. While somewhat simplistic, we believe this action scheme reasonably reﬂects real-world settings in which the decision subjects are under time or resource constraints when deciding which action to take. For example, the decision subject may need to choose between paying oﬀ some amount of debt and opening a new credit card when strategically modifying their observable features before applying for a loan.
Under this setting, (OPT-LP) optimizes over Θ(m|R|) variables, where m is the number of actions available to each agent and |R| is the number of equivalence regions. In order to determine the size of R, we note that an equivalence region can be alternatively characterized by
13

observing that assessment rules θ and θ belong to the same equivalence region if the diﬀerence in their predictions for any two actions a and a is the same. (This follows from straightforward algebraic manipulation of Deﬁnition 4.1.) As such, an equivalence region R can essentially be characterized by the set of actions AR ⊆ A which receive a positive classiﬁcation when θ ∈ R.5
Armed with this new characterization of an equivalence region, we are now ready to show the scale of |R| for the setting described in Figure 3.
Proposition 5.1. For the setting described in Figure 3, there are |R| = Πdi=1mi − 1 equivalence regions, where d is the number of observable features of the decision subject and mi (∀i ∈ [d]) is the number of actions the decision subject has at their disposal to improve observable feature i.
Proof. In order to characterize the number of equivalence regions |R|, we deﬁne the notion of a dominated action a, where an action a is dominated by some other action a if ∆x(a) ∆x(a ), with strict inequality holding for at least one index. Using this notion of dominated actions and our reﬁned characterization of an equivalence region, it is straightforward to see that if action a is dominated by action a , then a ∈ AR for any equivalence region R where a ∈ AR. Proposition 5.1 then follows directly from the fact that each action only aﬀects one observable feature.
Proposition 5.1 shows that the computation of (OPT-LP) quickly becomes intractable as the number of observable features grows large, even in this relatively simple setting. This motivates the need for an approximation algorithm for (OPT-LP), which we present in Section 5.2.
5.2 An Eﬃcient Approximation Algorithm
Motivated by the results in Section 5.1, we aim to design a computationally eﬃcient approximation scheme to compute an approximately optimal signaling policy for the decision maker. In particular, we adapt the sampling-based approximation algorithm of Dughmi and Xu to our setting in order to compute an -optimal and -approximate signaling policy in polynomial time, as shown in Algorithm 1. At a high level, Algorithm 1 samples polynomially-many times from the prior distribution over the space of assessment rules, and solves an empirical analogue of (OPT-LP). We show that the resulting signaling policy is -BIC, and is -optimal with high probability, for any > 0.
Theorem 5.2. Algorithm 1 runs in poly(m, 1 ) time (where m = |A|), and implements an -BIC signaling policy that is -optimal with probability at least 1 − δ.
Proof. Our proof is similar to the approximation algorithm proof in Dughmi and Xu [16], and follows directly from the following lemmas, whose proofs are in Appendix A. First, since the approximation algorithm solves an approximation LP (APPROX-LP) of polynomial size, it runs in polynomial time.
Lemma 5.3. Algorithm 1 runs in poly(m, 1 ) time.
By bounding the approximation error in the BIC constraints of (APPROX-LP), we show that the resulting policy satisﬁes approximate BIC.
Lemma 5.4. Algorithm 1 implements an -BIC signaling policy.
Next, we show that a feasible solution to (APPROX-LP) exists which achieves expected decision maker utility at least OPT - with probability at least 1 − δ. In order to do so, we ﬁrst show that there exists an approximately optimal solution S to (OPT-LP) such that each signal is either (i) large (i.e., output with probability above a certain threshold), or (ii) honest
5Speciﬁcally, if taking action a results in a positive classiﬁcation for some θ ∈ Θ and a negative classiﬁcation for θ ∈ Θ, the only way for θ and θ to be in the same equivalence region is if taking any action in A results in a positive classiﬁcation for θ and a negative classiﬁcation for θ . Besides this special case, if θ and θ result in diﬀerent classiﬁcations for the same action, they are in diﬀerent equivalence regions.
14

ALGORITHM 1: Approximation Algorithm for (OPT-LP)
Input: θ ∈ Θ, > 0, δ > 0 Output: Signaling policy S := {p(σ = a|Rθ)}∀a∈A (where region Rθ contains θ) Set K = 22 log 2(mδ2+1) Pick ∈ {1, . . . , K} uniformly at random. Set θ = θ.
Sample Θ = {θ1, . . . , θ −1, θ +1, . . . , θK } ∼ π(θ). Let R denote the set of observed regions. Compute p˜(R), ∀R ∈ R, where p˜(R) is the empirical
probability of θ ∈ R. Solve

max
p(σ=a|R),∀a∈A,R∈R

p˜(R)p(σ = a|R)udm(a)
a∈A R∈R

s.t.

p(σ = a|R)p˜(R)(uds(a, R) − uds(a , R) + ) ≥ 0, ∀a, a ∈ A

R∈R

p(σ = a|R) = 1, ∀R ∈ R,
a∈A
Return signaling policy S := {p(σ = a|Rθ)}∀a∈A.

p(σ = a|R) ≥ 0, ∀R ∈ R, a ∈ A. (APPROX-LP)

(i.e., the signal recommends the action the decision subject would take, had they known the true assessment rule θ). Next, we show that S is a feasible solution to (APPROX-LP) with high probability by applying McDiarmid’s inequality [38] and a union bound.
Lemma 5.5. There exists an 2 -optimal signaling policy S that is large or honest.
Lemma 5.6. With probability at least 1 − δ, S is a feasible solution to (APPROX-LP) and the expected decision maker utility from playing S is at least OPT - .
By Lemmas 5.5 and 5.6, the decision maker’s expected utility will be at least OPT - with probability at least 1 − δ.
Bi-criteria approximation. It is important to note that the signaling policy from Algorithm 1 is both -optimal and -incentive compatible. While one may wonder whether (i) an -optimal and exactly incentive compatible signaling policy exists, or (ii) an exactly optimal and -incentive compatible signaling policy exists, Dughmi and Xu show that this is generally not possible for sampling-based approximation algorithms for Bayesian persuasion (see Theorem 27 in Dughmi and Xu [16]). Note that unlike the other results in Dughmi and Xu [16], these results directly apply to the setting we consider. Computational complexity. Recall that the algorithm for computing the optimal policy runs in time polynomial in the number of equivalence regions |R|, which can scale exponentially in the number of actions m. However, without any structural assumptions, the input prior over the space of assessment rules Θ can scale exponentially in the number of features d. When m and d are comparable, our algorithm runs in time polynomial in the input size. We leave open the question of whether there are classes of succinctly represented prior distributions that permit eﬃcient algorithms for computing the optimal policy in time polynomial in d and m. It is also plausible to design eﬃcient algorithms that only require some form of query access to the prior distribution. However, information-theoretic lower bounds of [16] rule out the query access through sampling, as they show that no sampling-based algorithm can compute the optimal signaling policy with ﬁnite samples across all problem instances.

15

Pair
(x1, a1) (x2, a2) (x3, a3) (x4, a4)

Feature (xi) # payments with high-utilization ratio
# satisfactory payments % payments that were not delinquent revolving balance to credit limit ratio

Action (ai) decrease this value increase this value increase this value decrease this value

Table 2: Decision subject’s observable features from the HELOC dataset and corresponding actions to improve each feature. For simplicity, we assume that each action only aﬀects one observable feature, although our model generally allows for more intricate relationships between actions and changes in observable features.

6 Experiments
In this section, we provide experimental results that validate our ﬁndings using a semi-synthetic setting where decision subjects are based on individuals in the Home Equity Line of Credit (HELOC) dataset [18]. We compare the decision maker utility for diﬀerent models of information revelation: our optimal signaling, revealing full information, revealing no information. To do so, we ﬁrst estimate agent costs using the Bradley-Terry model [5] and compute the decision maker’s expected utility for each information revelation scheme we consider. We ﬁnd that the expected decision maker utility when recommending actions according to the optimal signaling policy either matches or exceeds the expected utility from revealing full information or no information about the assessment rule across all problem instances. Moreover, the expected decision maker utility from signaling is signiﬁcantly higher on average. Next, we explore how the decision maker’s expected utility changes when action costs and changes in observable features are varied jointly. Our results are summarized in Figures 4, 5, and 6.
The HELOC dataset contains information about 9,282 customers who received a Home Equity Line of Credit. Each individual in the dataset has 23 observable features related to an applicant’s ﬁnancial history (e.g., percentage of previous payments that were delinquent) and a label which characterizes their loan repayment status (repaid/defaulted). In order to adapt the HELOC dataset to our strategic setting, we select four features from the original 23 and deﬁne ﬁve hypothetical actions A = {a∅, a1, a2, a3, a4} that decision subjects may take in order to improve their observable features. Actions {a1, a2, a3, a4} result in changes to each of the decision subject’s four observable features, whereas action a∅ does not. For simplicity, we view actions {a1, a2, a3, a4} as equally desirable to the decision maker, and assume they are all more desirable than a∅. See Table 2 for details about the observable features and actions we consider. Using these four features, we train a logistic regression model that predicts whether an individual is likely to pay back a loan if given one, which will serve as the decision maker’s realized assessment rule. Common prior. We assume the common prior over the realized assessment rule θ takes the form of a multivariate Gaussian N (θ, σ2I4×4) before training. This captures the setting in which both the decision maker and decision subjects have a good estimate of what the true model will be, but are somewhat uncertain about their estimate. We note that our methods extend to more complicated priors beyond the isotropic Gaussian prior we consider in this setting. Changes in observable features. In order to examine the eﬀects that diﬀerent ∆x(ai)(i ∈ {1, 2, 3, 4}) have on the decision maker’s expected utility, we consider settings in which each ∆x(ai) takes a value in {0, 0.25, 0.5, 0.75, 1}. Utilities and costs of actions. As the decision maker views actions {a1, a2, a3, a4} as equally desirable, we deﬁne udm(ai) = 1, i ∈ {1, 2, 3, 4} and udm(a∅) = 0.6 Since there are 1,320 individuals in our test dataset, the maximum utility the decision maker can obtain is 1,320.
6We set udm(a1) = udm(a2) = udm(a3) = udm(a4) for ease of exposition — in general, actions can have diﬀerent utility values based on their relative importance.

16

Utility

1200

AverAagcreoTssotDailffDeeresinction-amndakCeorsUttility

1000

800

600

400 BIC

200

Full None

0 2 = 0.1

2 = 0.4

2 = 1.0

Figure 4: Total decision maker utility averaged across all cost and ∆x(a) conﬁgurations for three diﬀerent prior variances (σ2 = 0.1, 0.4, 1.0). See Figure 8 to view individual plots of the settings which were averaged in order to generate this plot. The optimal signaling policy (red) consistently yields higher utility compared to the two baselines: revealing full information (blue) and no information (green). This gap increases when the decision subjects are less certain about the model parameters being used (higher σ2).

Utility

1300 1200 1100 1000 900
0 x(a01.)5

1.0 0.5 0.c2(5a1)0

1300 1200 1100 1000 900
0 x(a02.)5

1.0 0.5 0.c2(5a2)0

1300 1200 1100 1000 900 800
0 x(a03.)5

1.0 0.5 0.c2(5a3)0

1300 1200 1100 1000 900 800
0 x(a04.)5

1.0 0.5 0.c2(5a4)0

Figure 5: Utility surface across diﬀerent c(a) and ∆x(a) pairs for σ2 = 0.4. Optimal signaling policy (red) eﬀectively upper-bounds the two baselines, revealing everything (blue) and revealing nothing (green) in all settings.

As proposed in [40], we use the Bradley-Terry model [5] to generate the decision subject’s cost c(ai) of taking action ai, for i = 1, 2, 3, 4. See Appendix C.2 for details on our exact generation methods. Results. Given a {(c(ai), ∆x(ai))}4i=1 instance and information revelation scheme, we calculate the decision maker’s total expected utility by summing their expected utility for each applicant. Figure 4 shows the average total expected decision maker utility across diﬀerent ∆x(a) and cost conﬁgurations for priors with varying amounts of uncertainty. See Figure 8 in Appendix C.3 for plots of all instances which were used to generate Figure 4. Across all instances, the optimal signaling policy (red) achieves higher average total utility compared to the other information revelation schemes (blue and green). The diﬀerence is further ampliﬁed whenever the decision subjects are less certain about the true assessment rule (i.e., when σ is large). Intuitively, this is because the decision maker leverages the decision subjects’ uncertainty about the true assessment rule in order to incentivize them to take desirable actions, and as the uncertainty increases, so does their ability of persuasion.

6.1 Patterns under diﬀerent ∆x(a) and c(a)

To better understand how the decision maker’s expected utility changes as a function of c(a)

and

∆x(a),

we

sweep

through

multiple

{(

c(a

i

)

,

∆

x

(a

i

))}

4 i=1

tuples

on

a

grid

of

(c(ai), ∆x(ai))

∈

{0, 0.25, 0.5}×{0, 0.5, 1.0} for i ∈ {1, 2, 3, 4} and measure the eﬀectiveness of the three information

revelation schemes. Figure 5 shows the surface of the decision maker utility as a function of

(c(ai), ∆x(ai)) for the optimal signaling policy (red), revealing full information (blue), and

17

x(a1) = 0.00
1250 1000
750 0 0.25 0.5 c(a1)
x(a1) = 0.50
1250 1000
750 0 0.25 0.5 c(a1)
x(a1) = 1.00
1250 1000
750 0 0.25 0.5 c(a1)

x(a2) = 0.00
0 c0(.a225) 0.5 x(a2) = 0.50
0 c0(.a225) 0.5 x(a2) = 1.00
0 c0(.a225) 0.5

x(a3) = 0.00
0 c0(.a235) 0.5 x(a3) = 0.50
0 c0(.a235) 0.5 x(a3) = 1.00
0 c0(.a235) 0.5

x(a4) = 0.00
0 c0(.a245) 0.5 x(a4) = 0.50
0 c0(.a245) 0.5 x(a4) = 1.00
0 c0(.a245) 0.5

c(a1) = 0.00
1250 1000
750 0 0.5 1.0 x(a1)
c(a1) = 0.25
1250 1000
750 0 0.5 1.0 x(a1)
c(a1) = 0.50
1250 1000
750 0 0.5 1.0 x(a1)

c(a2) = 0.00
0 x0(.a52) 1.0 c(a2) = 0.25
0 x0(.a52) 1.0 c(a2) = 0.50
0 x0(.a52) 1.0

c(a3) = 0.00
0 x0(.a53) 1.0 c(a3) = 0.25
0 x0(.a53) 1.0 c(a3) = 0.50
0 x0(.a53) 1.0

c(a4) = 0.00
0 x0(.a54) 1.0 c(a4) = 0.25
0 x0(.a54) 1.0 c(a4) = 0.50
0 x0(.a54) 1.0

Figure 6: 2-D slices of Figure 5 across c(a) (left) and ∆x(a) (right). Across these two axes, the optimal signaling policy (red) dominates the revealing full information (blue) and revealing no information (green), though it may be possible for (blue) and (green) to vary in terms of which provides higher decision maker utility.

revealing no information (green). When c(ai) is high and ∆x(ai) is low, the total expected decision maker utility is low as there is less incentive for the decision subject to take actions (although even under this setting, the optimal signaling policy outperforms the other two baselines). As c(ai) decreases and ∆x(ai) increases, the total expected decision maker utility increases.
In Figure 6, we show 2-D slices of Figure 5 along the c(a) axis (left) and ∆x(a) axis (right). As is expected, with small cost and suﬃciently large ∆x(a) (top row, right), the two baselines become as eﬀective as the optimal signaling policy. Interestingly, we note that changes in diﬀerent (c(ai), ∆x(ai)) result in signiﬁcantly diﬀerent rates of change in decision maker utility. For example, the optimal signaling policy (red) and revealing full information (blue) are more resistant to the increase in c(a4) in range [0, 0.25] than they are for the increase in other c(ai), i = 4, showing a concave drop in utility rather than a convex one (bottom row, left). Such behavior can be attributed to the relative weight of each feature on the learned assessment rule, where |θ4| > |θ3| > |θ1| > |θ2|. Because the fourth feature has the largest weight, taking action a4 will have the largest eﬀect on an individual’s prediction. As a result, the decision maker utility is the least sensitive to increases in the cost of taking that action. Similarly, we observe that the degree to which changes in ∆x(a) aﬀect the expected utility is more drastic for a4 compared to other actions (middle row, right).
7 Conclusion
In this work, we investigated the problem of oﬀering algorithmic recourse without requiring full transparency (i.e., revealing the assessment rule). We cast this problem as a game of Bayesian persuasion, and oﬀered several new insights regarding how a decision maker can leverage their information advantage over decision subjects to incentivize mutually beneﬁcial actions. Our stylized model relies on several simplifying assumptions, which suggest important directions for future work: Public persuasion. Throughout this work, we assumed that the recommendations received by each decision subject were private. However, if a decision subject is given access to recommendations for multiple individuals, it may be possible for them to reconstruct the underlying model. While out of the scope of this work, it would be interesting to study models of public persuasion

18

in the algorithmic recourse setting. Alternative models of information design. Cheap talk [13] and veriﬁable disclosure [17] are two alternative models of information disclosure which may be applicable whenever the sender does not have the power to commit to a signaling policy before the state of nature is revealed. As a consequence, the resulting equilibria are often diﬃcult to characterize, and the players may face an equilibrium selection problem. Nevertheless, it may be worthwhile to analyze these alternative games in the algorithmic recourse setting to capture situations in which the decision maker cannot commit to a signaling policy. Beyond linear decision rules. Finally, we focus on settings with linear decision rules and assume all decision subject parameters (e.g., cost function, initial observable features, etc.) are known to the decision maker. We leave it for future work to extend our ﬁndings to non-linear decision rules, or settings in which some of the decision subjects’ parameters are unknown to the decision maker.
8 Acknowledgements
ZSW and KH were supported in part by the NSF FAI Award #1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Facebook Research Award, and a Mozilla Research Grant. AT was supported in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471, an Amazon Web Services Award, a Facebook Faculty Research Award, funding from Booz Allen Hamilton Inc., and a Block Center Grant. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of any of these funding agencies. The authors would like to thank Haifeng Xu for insightful conversations about Dughmi and Xu [16], and James Best, Yatong Chen, Jeremy Cohen, Daniel Ngo, Chara Podimata, and Logan Stapleton for helpful discussions and suggestions.
19

References

[1] E. Akyol, C. Langbort, and T. Basar. Price of transparency in strategic machine learning. arXiv preprint arXiv:1610.08210, 2016.

[2] R. Alonso and O. Câmara. Bayesian persuasion with heterogeneous priors. Journal of Economic Theory, 165:672–706, 2016.

[3] I. Arieli and Y. Babichenko. Private bayesian persuasion. Journal of Economic Theory, 182: 185–217, 2019.

[4] Y. Bechavod, C. Podimata, Z. S. Wu, and J. Ziani. Information discrepancy in strategic learning. arXiv preprint arXiv:2103.01028, 2021.

[5] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. ISSN 00063444. URL http: //www.jstor.org/stable/2334029.

[6] M. Castiglioni, A. Celli, A. Marchesi, and N. Gatti. Online bayesian persuasion. Advances in Neural Information Processing Systems, 33, 2020.

[7] A. Chalﬁn, O. Danieli, A. Hillis, Z. Jelveh, M. Luca, J. Ludwig, and S. Mullainathan. Productivity and selection of human capital with machine learning. American Economic Review, 106(5):124–27, 2016.

[8] B. Chen, P. Frazier, and D. Kempe. Incentivizing exploration by heterogeneous users. In Conference On Learning Theory, pages 798–818. PMLR, 2018.

[9] V. Chen, J. Li, J. S. Kim, G. Plumb, and A. Talwalkar. Interpretable machine learning: Moving from mythos to diagnostics. arXiv preprint arXiv:2103.06254, 2021.

[10] Y. Chen, J. Wang, and Y. Liu. Strategic classiﬁcation with a light touch: Learning classiﬁers that incentivize constructive adaptation, 2021.

[11] D. K. Citron and F. Pasquale. The scored society: Due process for automated predictions. Wash. L. Rev., 89:1, 2014.

[12] Council of European Union. Council regulation (EU) no 679/2016, 2016. https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679.

[13] V. P. Crawford and J. Sobel. Strategic information transmission. Econometrica: Journal of the Econometric Society, pages 1431–1451, 1982.

[14] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, and Z. S. Wu. Strategic classiﬁcation from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 55–70, 2018.

[15] S. Dughmi and H. Xu. Algorithmic persuasion with no externalities. In Proceedings of the 2017 ACM Conference on Economics and Computation, pages 351–368, 2017.

[16] S. Dughmi and H. Xu. Algorithmic bayesian persuasion. SIAM Journal on Computing, (0): STOC16–68, 2019.

[17] R. A. Dye. Disclosure of nonproprietary information. Journal of accounting research, pages 123–145, 1985.

[18] FICO. Explainable machine learning challenge. explainable-machine-learning-challenge, 2018.

https://community.fico.com/s/

20

[19] A. Frankel and N. Kartik. Improving information from manipulable data. Journal of the European Economic Association, 2019.
[20] G. Ghalme, V. Nair, I. Eilat, I. Talgam-Cohen, and N. Rosenfeld. Strategic classiﬁcation in the dark. arXiv preprint arXiv:2102.11592, 2021.
[21] M. Hardt, N. Megiddo, C. Papadimitriou, and M. Wootters. Strategic classiﬁcation. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111–122, 2016.
[22] K. Harris, H. Heidari, and Z. S. Wu. Stateful strategic regression. arXiv preprint arXiv:2106.03827, 2021.
[23] K. Harris, D. Ngo, L. Stapleton, H. Heidari, and Z. S. Wu. Strategic instrumental variable regression: Recovering causal relationships from strategic responses. arXiv preprint arXiv:2107.05762, 2021.
[24] T. Homonoﬀ, R. O’Brien, and A. B. Sussman. Does knowing your ﬁco score change ﬁnancial behavior? evidence from a ﬁeld experiment with student loan borrowers. Review of Economics and Statistics, 103(2):236–250, 2021.
[25] N. Immorlica, J. Mao, A. Slivkins, and Z. S. Wu. Bayesian exploration with heterogeneous agents. In The World Wide Web Conference, pages 751–761, 2019.
[26] M. Jagadeesan, C. Mendler-Dünner, and M. Hardt. Alternative microfoundations for strategic classiﬁcation. In International Conference on Machine Learning, pages 4687–4697. PMLR, 2021.
[27] J. Jagtiani and C. Lemieux. The roles of alternative data and machine learning in ﬁntech lending: evidence from the lendingclub consumer platform. Financial Management, 48(4): 1009–1029, 2019.
[28] S. Joshi, O. Koyejo, W. Vijitbenjaronk, B. Kim, and J. Ghosh. Towards realistic individual recourse and actionable explanations in black-box decision making systems. arXiv preprint arXiv:1907.09615, 2019.
[29] E. Kamenica. Bayesian persuasion and information design. Annual Review of Economics, 11:249–272, 2019.
[30] E. Kamenica and M. Gentzkow. Bayesian persuasion. American Economic Review, 101(6): 2590–2615, 2011.
[31] A.-H. Karimi, G. Barthe, B. Schölkopf, and I. Valera. A survey of algorithmic recourse: deﬁnitions, formulations, solutions, and prospects, 2021.
[32] J. Kleinberg and M. Raghavan. How do classiﬁers induce agents to invest eﬀort strategically? ACM Transactions on Economics and Computation (TEAC), 8(4):1–23, 2020.
[33] D. Kučak, V. Juričić, and G. Ðambić. Machine learning in education-a survey of current research trends. Annals of DAAAM & Proceedings, 29, 2018.
[34] S. Levanon and N. Rosenfeld. Strategic classiﬁcation made practical. arXiv preprint arXiv:2103.01826, 2021.
[35] F. Li and P. Norman. On bayesian persuasion with multiple senders. Economics Letters, 170:66–70, 2018.
21

[36] Y. Mansour, A. Slivkins, and V. Syrgkanis. Bayesian incentive-compatible bandit exploration. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, pages 565–582, 2015.
[37] Y. Mansour, A. Slivkins, V. Syrgkanis, and Z. S. Wu. Bayesian exploration: Incentivizing exploration in bayesian games. In V. Conitzer, D. Bergemann, and Y. Chen, editors, Proceedings of the 2016 ACM Conference on Economics and Computation, EC ’16, Maastricht, The Netherlands, July 24-28, 2016, page 661. ACM, 2016. doi: 10.1145/2940716.2940755. URL https://doi.org/10.1145/2940716.2940755.
[38] C. McDiarmid et al. On the method of bounded diﬀerences. Surveys in combinatorics, 141 (1):148–188, 1989.
[39] M. Raghavan, S. Barocas, J. Kleinberg, and K. Levy. Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 469–481, 2020.
[40] K. Rawal and H. Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. Advances in Neural Information Processing Systems, 33, 2020.
[41] J. Sánchez-Monedero, L. Dencik, and L. Edwards. What does it mean to’solve’the problem of discrimination in hiring? social, technical and legal perspectives from the uk on automated hiring systems. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 458–468, 2020.
[42] A. D. Selbst and S. Barocas. The intuitive appeal of explainable machines. Fordham L. Rev., 87:1085, 2018.
[43] M. Sellke and A. Slivkins. The price of incentivizing exploration: A characterization via thompson sampling and sample complexity. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 795–796, 2021.
[44] Y. Shavit, B. Edelman, and B. Axelrod. Causal strategic linear regression. In International Conference on Machine Learning, pages 8676–8686. PMLR, 2020.
[45] D. Slack, S. Hilgard, H. Lakkaraju, and S. Singh. Counterfactual explanations can be manipulated. arXiv preprint arXiv:2106.02666, 2021.
[46] B. Ustun, A. Spangher, and Y. Liu. Actionable recourse in linear classiﬁcation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 10–19, 2019.
[47] S. Wachter, B. Mittelstadt, and C. Russell. Counterfactual explanations without opening the black box: Automated decisions and the gdpr. Harv. JL & Tech., 31:841, 2017.
22

A Proof of Theorem 5.2

Proof of Lemma 5.3.
Proof. Lines 1-3 trivially run in poly(m, 1 ) time. p˜(R), ∀R ∈ R can be computed in poly(m, 1 ) time in an online manner as follows: for each k ∈ {1, . . . , K}, check if θk belongs to an existing region. (Note that this can be done in O(m) time for each region.) If θk belongs to an existing region, update the existing empirical probabilities. Otherwise, create a new region. Finally, note that LP (APPROX-LP) has poly(m, 1 ) variables and constraints, and can therefore be solved in poly(m, 1 ) time using, e.g., the Ellipsoid Algorithm.

Proof of Lemma 5.4.

Proof. By the principle of deferred decisions, θ ∼ Π , where Π is the uniform distribution over Θ. (APPROX-LP) implements an -BIC signaling policy for Π by deﬁnition, so

Eθ∼Π [uds(a, R) − uds(a , R)|σ = a] ≥ − , ∀a, a ∈ A.

Finally, apply the law of iterated expectation with respect to Θ to obtain the desired result.

Proof of Lemma 5.5. In order to prove Lemma 5.5, we make use of the following deﬁnitions.

Deﬁnition A.1 (Large signal). A signal σ = a is large if p(σ = a) = 2m .

R∈R p(σ = a|R)p(R) >

Deﬁnition A.2 (Honest signal). A signal σ = a is honest if a ∈ arg maxa ∈A uds(a , R).

Proof. We proceed via proof by construction. Let S∗ be the optimal BIC signaling policy. Deﬁne the signaling policy S as follows: for a given θ, it ﬁrst samples a signal a ∼ S∗(θ). If the
signal is large, output signal σ = a. Otherwise, output signal σ = a ∈ arg maxa ∈A uds(a , Rθ). Every signal of S is trivially large or honest. S is BIC since S∗ is BIC and S only replaces recommendations of S∗ with honest recommendations. Finally, since the total probability of
signals that are not large is at most 2 , and the decision maker’s utilities are in [0, 1], their expected utility is no worse than 2 smaller than their expected utility from S∗.

Proof of Lemma 5.6. The following claim will be useful when proving Lemma 5.6.

Claim A.3. The expected decision maker utility from playing S is a|R)udm(a).

a∈A R∈R p(R)p(σ =

Proof. The expected decision maker utility from playing S is Eθ∼Π[ a∈A R∈R p˜(R)p(σ = a|R)udm(a)] = Eθ∼Π[ a∈A R∈R p˜(R)p(σ = a|R)udm(a)], by the principle of deferred decisions.
Apply the law of iterated expectation with respect to Θ to obtain the desired result.

Additionally, we will make use of McDiarmid’s inequality [38], stated for completeness below.

Lemma A.4 (McDiarmid’s Inequality [38]). Let X1, . . . , Xn be independent random variables, with Xk taking values in a set Ak for each k. Suppose that the (measurable) function f : ΠAk → R satisﬁes
|f (x) − f (x )| ≤ ck
whenever the vectors x and x diﬀer only in the kth coordinate. Let Y be the random variable f (X1, . . . , Xn). Then for any t > 0.

P(|Y − E[Y ]| ≥ t) ≤ 2 exp −2t2/ c2k .
k
23

We are now ready to prove Lemma 5.6.

Proof. First, note that the -BIC constraints can be rewritten using the observed decision rules

as 1K K p(σ = a|Rk)(uds(a, Rk) − uds(a , Rk)) ≥ − , ∀a, a ∈ A,
k=1

where

θk

∈

Rk .

Note

that

this

is

a

bounded

function

of

θ1, . . . , θK.

Let

Y (a, a

)

=

1 K

K k=1

p(σ

=

a|Rk)(uds(a, Rk) − uds(a , Rk)). Note that E[Y (a, a )] = R∈R p(σ = a|R)p(R)(uds(a, R) −

uds(a , R)).

Applying Lemma A.4, we see that ∀a, a ∈ A,

P(|Y (a, a ) − E[Y (a, a )]| ≥ ) ≤ 2 exp −K 2/2 .

Similarly, let Z =

a∈A

R∈R

p˜(R)p(σ

=

a|R)udm(a)

=

1 K

K k=1

a∈A p(σ = a|Rk)udm(a)

(where Rk contains θk). By Claim A.3, E[Z] = a∈A R∈R p(R)p(σ = a|R)udm(a). Applying

Lemma A.4,

P(|Z − E[Z]| ≥ /2) ≤ 2 exp −K 2/2 .
Applying the union bound, we see that the probability that all m2 + 1 above inequalities hold is at least 2(m2 +1) exp −K 2/2 . By inverting the tail bound and picking K = 22 log 2(mδ2+1) , we get that |Z − E[Z]| ≤ /2 and |Y (a, a ) − E[Y (a, a )]| ≤ , ∀a, a ∈ A, with probability at least 1 − δ. Therefore, with probability at least 1 − δ, S is a feasible solution for LP (APPROX-LP) and the objective value is at most OPT − 2 − 2 = OPT − .

B Instantiating 1-Dimensional Scenario
In this section we instantiate the example introduced in Section 3 and demonstrate the decision maker’s gain in utility from the optimal signaling policy over other baselines. To contextualize this simple synthetic setup, consider a banking institution deciding whether approve a loan application from an applicant based on credit score x0 ∈ [300, 850] with a simple threshold classiﬁer. The bank approves the application (yˆ = 1) if x0 + θ > 0 and rejects (yˆ = −1) otherwise. Here, we assume the ground-truth threshold value used by the decision maker to be 670 (i.e. θ = −670), which is typically considered as a decent credit score. Recall that a∅ = “do nothing” and a1 = “pay oﬀ existing debt” and set the utility of the decision maker to be udm(a1) = 1, udm(a∅) = 0, as, for the sake of our illustration, we assume credit score to be a good measure of credit-worthiness. Finally, we assume the prior to be π(θ) ∼ N (µθ, σθ2).
In Figure 7, we verify that our optimal signaling policy (BIC, red) yields higher decision maker utility compared to the two baselines: revealing full information (Full, blue) and revealing no information (None, green)7. To measure the total amount of decision maker’s expected utility yielded by each policy, we assume a uniform distribution of the decision subjects’ credit scores x0 in the population and take the sum of expected decision maker utility values across diﬀerent scores. We plot these total utility values in Figure 7a, and as expected, the larger the σθ is, the more comparative advantage our method has over the baselines. As the decision subjects’ uncertainty about the true θ increases (i.e., the standard deviation of the prior distribution increases from 10 to 50), the decision maker beneﬁts from our optimal signaling policy even more.
When action a1 becomes more cost-prohibitive (or less eﬀective), as there is less incentive for the decision subjects to take the action, we expect the decision maker’s utility to decrease8.
7We set the decision subject cost of taking action a1 to c(a1) = 0.5, and ∆x(a1) = 40 (i.e., action a1 improves an applicant’s credit score by 40 points).
8In this setting, we set µθ = −650 and σθ = 50 so that the decision subjects are considered to have a reasonable estimate of the true threshold θ = −670, to make the situation more favorable to the baselines.

24

TotDaleciEsxipoen-ctmeadkeUtri'lsity TotDaelciEsixopne-ctmeadkeUrt'ilsity TotDaelciEsixopne-ctmeadkeUrt'ilsity

15

BIC Full

None

10

5

0 = 10

= 50

(a) Expected decision maker utility summed up across different x0 values under diﬀerent standard deviation σθ of the prior.

= 670, x(a1) = 50

40

BIC Full

20

None

0 0 0.5 1.0 1.5 2 c(a1)

= 670, c(a1) = 0.5

10

0 0 10 20 30 40 50 x(a1)

(b) Total expected decision maker utility under diﬀerent c(a1) (top) and ∆x (bottom)

Figure 7: (a) Total expected decision maker utility summed across diﬀerence x0 for our optimal signaling policy (BIC, red), against the two baselines: revealing full information about the assessment rule (Full, blue), and revealing no information (None, green). As the decision subject’s uncertainty about the true threshold θ (measured by σθ) increases, the advantage of the optimal signaling policy becomes more visible. (c) When taking action a1 becomes costprohibitive (high c(a1)) or less eﬀective (small ∆x(a1)), the decision maker’s utility decreases as there is less incentive for the decision subject to take the action. Nevertheless, the optimal signaling policy yields consistently higher decision maker utility compared to the baselines.

As shown in Figure 7b, we indeed observe such a trend as c(a1) increases (top) and ∆x(a1) decreases (bottom). Nevertheless, our optimal signaling policy yields consistently higher total decision maker utility compared to the baselines across all conditions.
C Experiment Details and Additional Results
C.1 Remark on the decision maker’s assessment rule for HELOC dataset
To simulate a setting in which the decision maker employs a machine learning model for making decisions about the decision subjects, we train a simple logistic regression model on the subset of HELOC dataset. We speciﬁcally work on four features selected in Table 2, and split the dataset into train/test set (7425, 1857 data points respectively). The test accuracy of the model was 71.08 percent, and the corresponding model coeﬃcients were θ = [−0.22974527, 0.15633134, 0.52023116, −0.61600619] with the bias term −0.08242841. Note that each coeﬃcient term has the sign that is aligned with how the desired action was deﬁned in Table 2 (i.e., for features where increasing the value is desirable, the sign is positive and vice-versa). To further make sure that the deﬁned actions correctly align with the model, we select the test samples that the trained model made no mistakes on. This resulted in a total of 1,320 samples from the test set on which each policy was optimized.
C.2 Computing diﬀerent costs for HELOC dataset using Bradley-Terry model
While exact action costs may be unknown, it is often reasonable for the decision maker to know an ordering over possible actions in terms of their cost for decision subjects. For example, it may be common knowledge that opening a new credit card is easier than paying oﬀ some existing amount of debt, but exactly how much easier may be unclear. The Bradley-Terry model uses exponential score functions to model the probability that feature xi is more costly for a decision subject to take compared to feature xj. Speciﬁcally, it assumes

25

Feature A
x1 x1 x1 x2 x2 x3

Feature B
x2 x3 x4 x3 x4 x4

# (A > B)
8 9 7 2 0 1

# (A < B)Feature A

2

x1

1

x1

3

x1

8

x2

10

x2

9

x3

Feature B
x2 x3 x4 x3 x4 x4

# (A > B)
2 3 4 6 7 6

# (A < B)
8 7 6 4 3 4

(a) c(a1) > c(a4) > c(a3) > c(a2) Feature A Feature B # (A > B)

x1

x2

2

x1

x3

1

x1

x4

4

x2

x3

3

x2

x4

7

x3

x4

7

(b) c(a2) > c(a3) > c(a4) > c(a1) # (A < B)Feature A Feature B # (A > B)

8

x1

x2

8

9

x1

x3

9

6

x1

x4

2

7

x2

x3

7

3

x2

x4

0

3

x3

x4

1

# (A < B)
2 1 3 8 10 9

(c) c(a3) > c(a2) > c(a4) > c(a1)

Conﬁguration c(a1)

(i)

0.5151

(ii)

0.1159

(iii)

0.07640764

(iv)

0.2987

(d) c(a4) > c(a1) > c(a3) > c(a2)

c(a2)

c(a3)

c(a4)

0.0282

0.0723

0.3844

0.428

0.2758

0.1803

0.27692769 0.50635064 0.14031403

0.0428

0.0476

0.6109

(e) Cost values learned by the Bradley-Terry model from the pair-wise comparison inputs above.

Table 3: Comparison inputs used by the Bradley-Terry model to generate diﬀerent cost conﬁgurations.

P(ai

ec(ai) aj ) = ec(ai) + ec(aj) .

Given pairwise cost comparisons (generated from common knowledge or gathered from experts) we can estimate P(ai aj) and solve for the parameters {c(ai)}4i=1 using maximum likelihood estimation. In order to gain more insight into how diﬀerent action cost orderings aﬀect the decision maker utility, we consider several diﬀerent ground-truth cost orderings over actions and simulate expert advice in order to estimate P(ai aj), ∀ai, aj ∈ A. While the expert advice is purely synthetic in our setting, this method provides a principled way to estimate action costs whenever input from domain experts (e.g., ﬁnancial advisors) is available to the decision maker.
We use the following set of comparison inputs (manually generated) in Table 3a-3d to generate cost values with the relative ordering presented in Section 6. While these comparison inputs are generated arbitrarily for the simulations, these can be obtained by querying several domain experts and aggregating their answers regarding which feature is more diﬃcult to change. The resulting cost values are shown in Table 3e.

C.3 Additional results for diﬀerent cost and ∆x(a) conﬁgurations
Figure 8 shows more exhaustive results on diﬀerent cost conﬁgurations (i)-(iv) as deﬁned in Table 3e and ∆x(ai) ∈ 0, 0.25, 0.5, 0.75, 1.0 for i = 1, 2, 3, 4 on HELOC datset. For all conﬁgurations considered, our optimal signaling policy (red) consistently yields utility no less than both baselines: revealing full information about the assessment rule (blue), and revealing

26

DecTiostiaoln-Utimliatkyer's

other
0.004 0.002 0.000
other
500
0
other
1300
1200
other
1300
1200
other
1300 1250
0.0
other
1000 500
0
other
1000 500
0
other
1000 500
0
other
1300 1200 1100
other
1300
1200
0.0

fixed at 0.00 other
1000 500
0
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1200
1000
fixed at 0.75 other
1300 1200 1100
fixed at 1.00 other
1300
1200
0.5 1.0 1100 0.0
x(a1) fixed at 0.00 100other
50
0
fixed at 0.25 other
100
0
fixed at 0.50 other
1000 500
0
fixed at 0.75 other
1300
1200
fixed at 1.00 other
1300 1250 1200
x0(.a51) 1.0 0.0

fixed at 0.00 other
1000 500
0
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1000 500
0
fixed at 0.75 other
1000 500
0
fixed at 1.00 other
1250 1000 750
x0(.a52) 1.0 0.0 fixed at 0.00 other
0.05
0.00
0.05
fixed at 0.25 other
50
0
fixed at 0.50 other
1000 500
0
fixed at 0.75 other
1300
1200
fixed at 1.00 other
1300 1250 0.5 1.0 1200 0.0
x(a2)

fixed at 0.00 fixed at 0.25

other
100
50
0
other

500

0
fixed at 0.50 other
1300
1200

fixed at 0.75 other
1300

1200
fixed at 1.00 other
1300 1250 1200
x0(.a53) 1.0 0.0
fixed at 0.00 other
1000 500
0
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1000 500
0
fixed at 0.75 other
1000 500
0
fixed at 1.00 other
1000 500
0
x0(.a53) 1.0 0.0

fixed at 0.00 fixed at 0.25 fixed at 0.50 fixed at 0.75 fixed at 1.00
BIC
x0(.a54) 1.0FNuolnl e fixed at 0.00 fixed at 0.25 fixed at 0.50 fixed at 0.75 fixed at 1.00
BIC
x0(.a54) 1.0FNuolnl e

DecTiostiaoln-Utimliatkyer's

DecTiostiaoln-Utimliatkyer's

other
500
0
other
1000
0
other
1000
0
other
1200
1000
other
1300 1200
0.0
other
50
0
other
1000 500
0
other
1300
1200
other
1300 1250 1200
other
1300 1250 1200
0.0

fixed at 0.00 other
0.05
0.00
0.05
fixed at 0.25 other
10
0
fixed at 0.50 other
250
0
fixed at 0.75 other
1300 1200 1100
fixed at 1.00 other
1300
1200
x0(.a51) 1.0 0.0 fixed at 0.00 other
1000 500
0
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1300 1200 1100
fixed at 0.75 other
1300 1200 1100
fixed at 1.00 other
1300 1200
x0(.a51) 1.0 0.0

fixed at 0.00 other
200
0
fixed at 0.25 other
200
0
fixed at 0.50 other
500
0
fixed at 0.75 other
1300 1200 1100
fixed at 1.00 other
1300
1200
x0(.a52) 1.0 0.0 fixed at 0.00 other
1000 500
0
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1000 500
0
fixed at 0.75 other
1000 500
0
fixed at 1.00 other
1000 500
0
x0(.a52) 1.0 0.0

fixed at 0.00 other
1000
0
fixed at 0.25 other
1000
0
fixed at 0.50 other
1000
0
fixed at 0.75 other
1000
0
fixed at 1.00 other
1000
0
x0(.a53) 1.0 0.0 fixed at 0.00 other
0.05 0.00 0.05
fixed at 0.25 other
1000 500
0
fixed at 0.50 other
1300
1200
fixed at 0.75 other
1300 1250 1200
fixed at 1.00 other
1300 1250
x0(.a53) 1.0 0.0

fixed at 0.00 fixed at 0.25 fixed at 0.50 fixed at 0.75 fixed at 1.00
BIC
x0(.a54) 1.0FNuolnl e fixed at 0.00 fixed at 0.25 fixed at 0.50 fixed at 0.75 fixed at 1.00
BIC
x0(.a54) 1.0FNuolnl e

DecTiostiaoln-Utimliatkyer's

Figure 8: More detailed view on decision maker utility for diﬀerent ∆x(a) values and cost conﬁgurations (i)-(iv) (from upper right to bottom right quadrants). Our optimal signaling policy (red) consistently achieves utility no less than revealing full information (blue) and revealing no information (green) in all settings.

no information (green).

27

