Learning Causal State Representations of Partially Observable Environments

arXiv:1906.10437v2 [cs.LG] 8 Feb 2021

Learning Causal State Representations of Partially Observable Environments

Amy Zhang McGill University Facebook AI Research
Zachary C. Lipton Carnegie Mellon University
Luis Pineda Facebook AI Research
Kamyar Azizzadenesheli Purdue University
Anima Anandkumar California Institute of Technology
Laurent Itti University of Southern California
Joelle Pineau McGill University Facebook AI Research
Tommaso Furlanello HK3 Lab

amy.x.zhang@mail.mcgill.ca tf@hk3lab.ai

Editor:

Abstract
Intelligent agents can cope with sensory-rich environments by learning task-agnostic state abstractions. In this paper, we propose an algorithm to approximate causal states, which are the coarsest partition of the joint history of actions and observations in partiallyobservable Markov decision processes (POMDP). Our method learns approximate causal state representations from RNNs trained to predict subsequent observations given the history. We demonstrate that these learned state representations are useful for learning policies eﬃciently in reinforcement learning problems with rich observation spaces. We connect causal states with causal feature sets from the causal inference literature, and also provide theoretical guarantees on the optimality of the continuous version of this causal state representation under Lipschitz assumptions by proving equivalence to bisimulation, a relation between behaviorally equivalent systems. This allows for lower bounds on the optimal value function of the learned representation, which is tight given certain assumptions. Finally, we empirically evaluate causal state representations using multiple partially observable tasks and compare with prior methods.
Keywords: Causal States, POMDPs, predictive state representation, bisimulation, reinforcement learning
1

Zhang et al.
1. Introduction
Decision-making and control often require that an agent interact with a partially-observed environment to learn an approximation of the true states and the underlying dynamics. To enable eﬃcient planning, we must construct latent representations of sequences of (action, observation) trajectories. At present, there are three general approaches for learning these latent representations. All three methods infer structure in the sequence of actions and observations to learn an optimal policy. The ﬁrst is based on the class POMDP formalism, where dynamics of a domain are captured by state-to-state transition probability distributions and state-conditional observation density functions (Aastrom, 1965; Cassandra et al., 1994; Veness et al., 2011; Roy et al., 2005; Azizzadenesheli et al., 2016). The second is framed in terms of predictive state representations (PSRs), where a systems dynamic matrix is used to model the trajectories of observations (Littman and Sutton, 2001; Singh et al., 2004), and statistical or spectral methods are used to ﬁnd a low-dimensional approximation. The third uses Recurrent Neural Networks (RNNs) (Schmidhuber, 1990a,b), to learn policies for partially observable environments via gradient methods (Hausknecht and Stone, 2015). Though several approaches have been explored, we still lack methods that can handle highdimensional observations spaces, while providing theoretical guarantees for the representation they ﬁnd. We also are short of understanding the relation between several of these methods and the causal learning literature, which has also begun to address the problem of recovering representations for planning and decision-making. In sum, we propose a fourth approach for learning latent representations in partial observability settings with causal states, and provide theoretical guarantees from both computational mechanics and causal inference literature.
Summary of Contributions
The three main contributions of our work are (1) a new connection between PSRs and bisimulation via causal states; (2) a novel gradient-based algorithm for training a causal state representation; and (3) a bound on the optimal value function of this causal state representation.
We propose a principled approach for learning state representations that generalizes PSRs to RNN-based methods. We exploit the idea of causal states, which were introduced in the computational mechanics literature and are deﬁned as the coarsest partition of histories into classes that are maximally predictive of the future (Crutchﬁeld and Young, 1989). Causal states constitute a discrete causal graph, and can be described by a hidden Markov model with uniﬁlar dynamics, i.e. there is no conditional entropy on the next state once the next observation is known.
Causal states also have a clear connection to bisimulation, a binary relation between behaviorally equivalent systems (Ferns et al., 2004). Bisimulation relations are a type of state abstraction which oﬀer a mathematically precise deﬁnition of what it means for two environments to ‘share the same structure’ (Larsen and Skou, 1989; Givan et al., 2003). We say that two states are bisimilar if they share the same expected reward and equivalent distributions over next bisimilar states. We show that causal states are equivalent to the coarsest bisimilar partition of the environment. Further, one can deﬁne bisimulation metrics, which induce an analogous notion of behavioural distance between states in an MDP. This
2

Learning Causal State Representations of Partially Observable Environments
connection allows us to obtain a lower bound on the optimal value function of the learned causal state representation which relies on this metric. This bound is derived from the distance between the learned states and the original POMDP. In summary, causal states form an MDP that is behaviorally equivalent to the observed POMDP and guaranteed to be the coarsest partitioned MDP in the family of behaviorally equivalent MDPs.
We provide a gradient-based algorithm for learning causal state representations, and evaluate our algorithm for approximate causal states reconstruction on multiple GridWorld navigation tasks with partially observable states. We further evaluate approximate causal states on environments with continuous latent states, a modiﬁcation of the original VizDoom environment used in (Koul et al., 2018), and ﬂickering Atari tasks (Bellemare et al., 2013) to show that our algorithm is also robust to this setting.
2. Background
In this section we present notation and a general overview of concepts used throughout this paper, starting with state representations for decision processes, causal states as deﬁned in computational mechanics, causal inference via invariant prediction, and state abstractions and bisimulation.
2.1 State Representations for Decision Processes
Consider the nonlinear stochastic process emerging from the interaction between an agent’s policy that chooses discrete actions At, taking values at from the alphabet A, and an environmental response Ot+1, taking values from the alphabet O. Let Yt := (Ot, At−1) be the joint observation-action variable with realizations yt = (ot, at−1), where yt ∈ O × A,1. The future dynamics P(Y>t) = P(O>t, A≥t) depend jointly on the stationary policy P(A≥t|Y≤t) that maps the joint histories Y≤t into future actions A≥t and the environment channel, P(O>t|A≥t, Y≤t) that maps the joint histories Y≤t and future actions A≥t into future observations O>t. An agent’s preferences over the future dynamics Y>t are deﬁned via its reward function R : O × A → R. The optimal policy of the agent π∗(o≤t, a<t) maximizes the expected reward E[R(Y>t)|Y≤t); π∗))]. We restrict our attention to environment channels and agent policies that generate a stochastic process P(Y>t|Y≤t) that is ergodic stationary, i.e., processes for which the probability of every bi-sequence (at, ot+1, .., at+L, ot+1+L) of ﬁnite length L ∈ Z+, for some L and all t is time-invariant, which can be reliably estimated from empirical data.
Formally, POMDPs suppose a hidden Markov process P(S>t|St, At), with realizations st ∈ S, and observations emitted through the action-conditional probability P(Ot+1|St, At) (Kaelbling et al., 1998). The causal relationship between the observed process and the hidden states implies that the mutual information I[Ot+1; St, At] between the generator state St and current action At (jointly) and the next observation Ot+1 is at least as great as that achieved by any competing representation of the history.
1. For a ﬁxed random variable Yt, we indicate its inclusive past with Y≤t = ...Yt−3, Yt−2, Yt−1Yt and its non-inclusive future with Y>t = Yt+1, Yt+2..., dropping the subscript t from the notation for convenience when the context is clear.
3

Zhang et al.
This next-step suﬃciency is extended to the inﬁnite due to the recursive nature of generator.
I[O>t; St, At] ≥ I[O>t; Y≤t, At].
2.1.1 Belief and Predictive State Representations A typical approach to planning in POMDPs assumes the agent has access to P(S>t|St, At) and P(Ot+1|St, At) and uses it to construct the belief states bt = P(St|Y≤t) from the ﬁnite realizations Y≤t (Kaelbling et al., 1998). Belief states are computed recursively using Bayes’ formula from an initial belief b0 = P(S0) and give rise to the belief process P(B>t|B≤t, Y>t). The belief process is a suﬃcient statistic of the generator state when
I[O>t; st, at] = I[O>t; bt, at],
and is said to be asymptotically synchronized when
lim H[St|Y≤t,>t−L] = H[St|bt] = 0,
L→∞
where H is the conditional-entropy function. When I[Ot+1; St, At] > I[Ot+1; Y≤t, At], the generator states contain more information about the future observable than the complete history of observations Y≤t, implying absence of asymptotical synchronization (Crutchﬁeld et al., 2010) and that belief states are only suﬃcient statistics of the history Y≤t such that
I[O>t; St, At] > I[O>t; bt, At] = I[O>t; Y≤t, At].
The PSR approach relaxes the requirement that we know the underlying generative model, instead constructing representation using the outputs of the predictive model,
ML = {P(O>t,≤t+L|Y≤t, A≥t,<t+L = q1), .., P(O>t,≤t+L|Y≤t, A≥t,<t+L = qn)},
of the next L observations, conditioned on the next L actions A≥t,<t+L (the test) sampled from the set of feasible L-length action sequences QL = {q1, .., qn} (Littman and Sutton, 2001). By M, we indicate the collections of predictive models for all L ∈ Z+. Each model ML is a suﬃcient statistic of the L-length future observations O>t,≤t+L, and the complete collection M is a suﬃcient statistic of the future observations O>t, i.e., I[O>t; M, At] = I[O>t; Y≤t, At]. Traditionally, PSRs are constructed using a linear model that enables approximate solutions by assuming that the inﬁnite-dimensional system dynamics matrix 2 has ﬁnite rank (Singh et al., 2004).
2.1.2 Causal States Representations We now show how the causal states representation connects to PSRs by allowing the deﬁnition of a formal equivalence between the latent generator states and the causal states reconstructed from history. As in the PSR framework, causal states depend on a predictive model of the observation process.
2. A matrix of probabilities of all combinations of histories and tests
4

Learning Causal State Representations of Partially Observable Environments

Deﬁnition 1 (Crutchﬁeld and Young, 1989; Shalizi and Crutchﬁeld, 2001) The causal states of a stochastic process are partitions σ ∈ S of the space of feasible pasts Y≤t induced by the causal equivalence ∼ :

y≤t ∼ y≤t ⇐⇒ P(Y>t|Y≤t = y≤t) = P(Y>t|Y≤t = y≤t).

(1)

Which implies:

P(Y>t|St = σi) = P(Y>t|Y≤t = y≤t) ∀y≤t ∈ σi,

(2)

where St is the variable denoting the causal state at time t, overwriting the deﬁnition in Section 2.1 of the unknown ground truth state. Since all histories belonging to the same equivalence class predict the same (conditional) future, the corresponding causal state can be used to compress the information content of those histories.
In the computational mechanics literature, causal state models are usually called machines and are formally deﬁned as:
←→ Deﬁnition 2 The -machine of a stochastic process Y is given by the tuple = S, Y, T where S is the discrete alphabet of causal states, Y the discrete alphabet of observation and T is a set of observation conditional state-to-state transition matrices (Crutchﬁeld and Young, 1989; Shalizi and Crutchﬁeld, 2001).
To summarize, the -machine of a stochastic process is the minimal uniﬁlar hidden Markov model able to generate its empirical realizations. The hidden states of an -machine are called the causal states of the process, and correspond to partitions of the process history. Furthermore, when the -machine has ﬁnite causal states, it is possible to represent it as a labeled directed graph G = (S, Ti(jo|a)), with causal states as nodes and action-observation conditional transitions as edges. Because of the uniﬁlar property, once an agent has perfect knowledge of the current causal states, the information in the future action-observation process are suﬃcient to uniquely determine the future causal states. This property can be exploited by multi-step planning algorithms which need not keeping track of potential stochastic transitions between the underlying states. This enables a variety of methods that are otherwise amenable only to MDPs like Dijkstra’s algorithm. Empirical support for this approach can be found in Section 5.3.
It can be demonstrated (Shalizi and Crutchﬁeld, 2001) that the partition induced by ∼ is the coarsest possible and generates the minimal suﬃcient representation across the model class. In other words, this partition has the lowest cardinality while being capable of modeling all possible futures of the given system. Sampling of new symbols in the sequence induces the creation of new histories and consequently new causal states. Because of this mapping from histories to states, the resulting hidden Markov model is uniﬁlar, which we deﬁne below.

Deﬁnition 3 (Shalizi and Crutchﬁeld, 2001) A uniﬁlar hidden Markov model is a HMM whose state transition probability P(St+1|St) is deterministic if conditioned on the output symbol, i.e., H[St+1|Yt+1, St] = 0.

With explicit reference to the joint input-output history, the state transition dynamics are governed by input-conditional transition matrices To|a ∈ T with elements:

Tioj|a = P(St+1 = σj, Ot+1 = o|St = σi, At = a).

(3)

5

Zhang et al.
All POMDPs can be captured by this model, but the causal states are not necessarily ﬁnite in continuous state-action settings, and in environments with non uniﬁlar generators. We further explore the inﬁnite causal state setting in the empirical results below in Section 5.4 and Section 5.5. Since the causal states are deﬁned over histories of joint symbols, the causal state model is uniﬁlar with respect to the joint variable At, Ot+1, i.e., the transitions between states are deterministic once the next action and observable have been sampled or H[St+1|At, Ot+1, St] = 0. Uniﬁlarity implies that the recurrent dynamics of the causal states are fully speciﬁed by the state-action-conditional symbol emission probability P(Yt+1|St, At) and the action-symbol-conditional causal state emission probability P(St+1|Yt+1, At, St). As a consequence, knowledge of the current causal states St and of the future actionobservation sequence O>t, A≥t induces a deterministic sequence of future causal states S>t, H(S>t|O>t, A≥t, St) = 0.
2.1.3 Stochastic Processes with Finite Causal States
When the joint process admits a ﬁnite causal state representation, it is called a ﬁnitary stochastic process which has multiple theoretical implications. In discrete stochastic processes with ﬁnite actions, ﬁnite-symbol alphabets, and ﬁnite memory of length k, the causal states are always ﬁnite, with a worst case scenario in which each sub-sequence of length k belongs to a distinct causal state forming a k-length Markov model (Shalizi and Crutchﬁeld, 2001). When the causal states are ﬁnite, they are also unique up to isomorphisms (Shalizi and Crutchﬁeld, 2001) and always generate a stationary stochastic process. If the underlying generator is non-uniﬁlar, the causal states can be inﬁnite and have the same information content as the potentially non-synchronizing belief states of the generator, and the belief states deﬁned over the causal states always asymptotically synchronize to the actual causal states (Crutchﬁeld et al., 2010).
We ﬁrst focus on partially-observable environments with discrete causal states, and either continuous or discrete observations. For continuous observations it is not possible to derive generic conditions that imply discrete or ﬁnite causal states. Therefore, the existence of discrete latent states has to be directly assumed or derived from alternative assumptions, like the existence of ﬁnite latent discrete variables underlying each continuous observation. When a memory-less map from continuous observation to latent discrete variables exists, the causal states of the revealed continuous variable process coincide with those deﬁned over the underlying discrete variables up to isomorphisms.
2.2 State Abstractions and Bisimulation
State abstractions have been studied as a way to distinguish relevant from irrelevant information (Li et al., 2006) in order to create a more compact representation for easier decision making and planning. Bertsekas and Castanon (1989); Roy (2006) provide bounds for approximation errors for various aggregation methods, and Li et al. (2006) discuss the merits of abstraction discovery as a way to solve related MDPs.
Bisimulation relations are a type of state abstraction that oﬀers a mathematically precise deﬁnition of what it means for two environments to ‘share the same structure’ (Larsen and Skou, 1989; Givan et al., 2003). We say that two states are bisimilar if they share the same expected reward and equivalent distributions over the next bisimilar states. For example, if
6

Learning Causal State Representations of Partially Observable Environments

a robot is given the task of washing the dishes in a kitchen, changing the wallpaper in the kitchen doesn’t change anything relevant to the task. One then could deﬁne a bisimulation relation that equates observations based on the locations and soil levels of dishes in the room and ignores the wallpaper. These relations can be used to simplify the state space for tasks like policy transfer (Castro and Precup, 2010), and are intimately tied to state abstraction. For example, the model-irrelevance abstraction described by Li et al. (2006) is precisely characterized as a bisimulation relation.
Deﬁnition 4 (Givan et al. 2003) Given an MDP S, A, P, R , an equivalence relation E : S × S → {0, 1} is a bisimulation relation if for all pairs of states s1, s2 ∈ S where the states are bisimilar, s1Es2, the following properties hold:
1. R(s1, a) = R(s2, a)
2. P (s |s1, a) = P (s |s2, a), ∀s ∈ S/E
where S/E denotes the partition of S into E-equivalence classes.

2.3 Causal Inference Using Invariant Prediction
We deﬁne the assumptions in the causal inference setting from Peters et al. 2016 and relate it to POMDPs in Section 3. Causal inference considers the setting where there are diﬀerent experimental conditions e ∈ E and diﬀerent i.i.d. samples of (Xe, Y e) from each environment. Xe ∈ Rp is the observed data and Y e the target variable, and only a subset S∗ ⊂ {1, ..., p} of the variables in Xe have a causal eﬀect on Y e, leading to the following relationship:

∀e ∈ E, Y e = g(XSe∗, e),
where e is environment speciﬁc noise independent of XSe∗, XSe∗ are the variables from Xe with indices in S∗, and g a real-valued function. From this formulation we can see that we want to ﬁnd an invariant model g that persists across all experiments e ∈ E.
Peters et al. (2016) ﬁrst introduced an algorithm, Invariant Causal Prediction (ICP), to ﬁnd the causal feature set S∗, the minimal set of features which are causal predictors of a target variable, by exploiting the fact that causal models have an invariance property (Pearl, 2009; Scho¨lkopf et al., 2012). Arjovsky et al. (2019) extend this work by proposing invariant risk minimization (IRM, see Equation (4)), augmenting empirical risk minimization to learn a data representation free of spurious correlations. They assume there exists some partition of the training data X into experiments e ∈ E, and that the model’s predictions take the form Y e = w φ(Xe). IRM aims to learn a representation φ for which the optimal linear classiﬁer, w, is invariant across e, where optimality is deﬁned as minimizing the empirical risk Re. We can then expect this representation and classiﬁer to have low risk in new experiments e, which have the same causal structure as the training set.

min

Re(w φ(Xe))

φw:∈XR→dRd e∈E (4)

s.t. w ∈ arg min Re(w¯ φ(Xe)) ∀e ∈ E.

w¯ ∈Rd

7

Zhang et al.
The IRM objective in Equation (4) can be thought of as a constrained optimization problem, where the objective is to learn a set of features φ for which the optimal classiﬁer in each environment is the same. Conditioned on the environments corresponding to diﬀerent interventions on the data-generating process, this is hypothesized to yield features that only depend on variables that bear a causal relationship to the predicted value. Because the constrained optimization problem is not generally feasible to optimize, Arjovsky et al. (2019) propose a penalized optimization problem with a schedule on the penalty term as a tractable alternative.
In order for invariant prediction to ﬁnd a causal feature set, certain assumptions about the training data are needed. We require access to do-interventions (Pearl, 2009) or noiseinterventions (Eberhardt and Scheines, 2007). Do-interventions are direct interventions that change the value of speciﬁc variables in X. Noise interventions are a form of “soft” intervention that changes the distribution of a variable. A third form of intervention are simultaneous noise interventions, which are useful in settings where it is not possible to only aﬀect one variable at a time. In an MDP, we can intervene on variables in the state space through actions (Sch¨olkopf, 2019), but cannot guarantee that an action only aﬀects one variable. In the linear setting, using only simultaneous noise interventions can be suﬃcient for identiﬁability of the causal feature set S∗ (Peters et al., 2016). We will use this result to explain under what assumptions causal states are causal feature sets.
3. Connections to Bisimulation & Invariant Prediction
To simplify the notation when showing connections to bisimulation (Ferns et al., 2004), we fold the reward into the observation seen by the agent, i.e., O := (O, R) in the taskspeciﬁc setting. We ﬁrst connect this task-speciﬁc version of causal states to trajectory equivalence, an equivalence between states based on equivalent trajectories conditioned on action sequences (formally deﬁned in Theorem 6), and use Theorem 2.6 in Castro et al. (2009) to show that it is also a bisimulation. For notational convenience we introduce the following deﬁnition.
Deﬁnition 5 (MDP of Histories of Observations (HOMDP)) Given a POMDP S, A, P, R, O, γ with state space S, action space A, reward function R, observation space O, transition distribution P , and discount factor γ, we can form an MDP {O}t0, P , A, R, γ with histories of observations O≤t as states and transition distribution P . We denote this the corresponding HOMDP of our POMDP.
We prove that causal states are a reduction (coarser bisimulation) of the HOMDP in Theorem 13. Any optimal policy on the causal states MDP generalizes to an optimal policy on the HOMDP.
We also show that we can relax our assumptions to the continuous state space setting, where causal states are inﬁnite by removing the discretization step. In this setting, the continuous representation can be connected to bisimulation metrics (Ferns et al., 2011), as shown in Theorem 3. Bisimulation metrics upper bound the optimal value function diﬀerence between two states (Theorem 2). By using the trick of joining the HOMDP and causal state
3. This is similar to the belief state MDP except the states are now learned causal states and the beliefs are Dirac functions.
8

Learning Causal State Representations of Partially Observable Environments

MDP into a single super-MDP (where we give up the irreducibility assumption common to the RL setting, i.e., we can no longer reach every state si from any other state sj), we can bound the distance between the optimal value function of the causal state MDP to the HOMDP, showing that the optimal value function has bounded Lipschitz constant with respect to bisimulation metrics, shown in Theorem 4. With this result, we can lower bound the value function learned using causal states with respect to the optimal value function of the MDP deﬁned by the history of observations (Theorem 5).
Finally, we draw a connection between the causal states objective and invariant prediction for causal inference. Given certain requirements on the diversity of data seen at training time, namely an intervention on every underlying variable in our MDP (as discussed in Section 2.3, causal states will learn the causal feature set.

3.1 Causal States are a Bisimulation We start with showing the equivalence between causal states and bisimulation.
Deﬁnition 6 (Castro et al. 2009) Given an MDP and a partitioning of the state space into disjoint subsets via Ψ : S → Ψ(S), states s1, s2 ∈ S are Ψ-trajectory equivalent if and only if Ψ(s1) = Ψ(s2) and for any ﬁnite action sequence a≥t ∈ A≥t and reward-state trajectory α ∈ (R × Ψ(S))∗ conditioned on that action sequence,
1. For any a ∈ A, R(s1, a) = R(s2, a).
2. P(α|Ψ(s1), a≥t) = P(α|Ψ(s2), a≥t).
Here Ψ∗ is the ﬁxed point of the iterates Ψ(n), which is equivalent to bisimulation. A more thorough discussion of this relation can be found in Castro et al. (2009).

Theorem 1 (Causal States are Bisimilar) If two observation sequences belong to the same causal state as per Theorem 1, they are also bisimilar as per Theorem 4.

Proof in Appendix A. By showing the state aggregation deﬁned by causal states is also a bisimulation, we prove that optimal value functions on the causal state MDP are also optimal for the HOMDP.

3.2 Lower Bounds on the Optimal Value Function via Bisimulation Metrics
Next, we bound the optimal value function diﬀerence between any two states under an approximate causal state representation and taking into account model error. We deﬁne a bisimulation metric:

Deﬁnition 7 (Ferns et al. 2004) Given an MDP S, A, P, R, γ , we deﬁne

F (d)(s1, s2) = max((1 − γ)|rsa − rsa | + γWd(Psa , Psa )),

a∈A

1

2

1

2

where Wd is the Wasserstein distance between probability distributions and d a pseudometric in a space of pseudometrics D. F has a least ﬁxed point d˜, which is our bisimulation metric.

9

Zhang et al.

This bisimulation metric d˜ bounds the optimal value function diﬀerence between two states s, s .

Theorem 2 (Theorem 3.20 in Ferns et al. 2011) Let V ∗ be the optimal value function for MDP M with discount factor γ ∈ [0, 1). Then V ∗ is Lipschitz continuous with respect to d˜ with Lipschitz constant 1−1 c where γ ≤ c,
|V ∗(s) − V ∗(s )| ≤ 1 d˜(s, s ). 1−γ

This result is achieved using the Banach ﬁxed point theorem and proves Lipschitz continuity of the optimal value function with respect to bisimulation metrics.
Our representation can be upper bounded by bisimulation metrics with an approach similar to Gelada et al. (2019). We deﬁne Lipschitz constants for the transition and reward functions of an MDP with metric ds for the state space, Wasserstein distance for probability distributions over transitions, and L1 distance for reward:

LD := sup Wd(P (s, a), P (s , a)) ,

s,s ∈S

ds(s, s )

a∈A

|R(s) − R(s )|

LR := sup
s,s ∈S

. ds(s, s )

Theorem 3 Let M be the HOMDP representing a POMDP whose states are the history of observations O≤t, and M¯ be the LD-LR-Lipschitz MDP with Ld and Lr representing the
losses. Let φ denote the encoder that takes in the history of observations and outputs the approximate causal state. The bisimulation distance d˜ : O≤t × O≤t → R in M can be upper bounded by L2 distance in M¯ as

d˜(o1≤t, o2≤t) ≤

(1 − γ)LR Wd

φ

(

o

1 ≤t

)

,

φ(

o2≤

t

)

1 − γLD

+2 L∞ + γL∞ LR ,

r

d 1 − γLD

where L∞ x := sups∈S,a∈A Lx. Proof found in Appendix B. Theorem 2 and Theorem 3 can be combined to give an overall bound (Theorem 4) on the smoothness of the optimal value function that depends on characteristics of the learned causal state representation.

Theorem 4 Given a HOMDP M and corresponding learned causal state MDP M¯ , the
Lipschitz constant of the optimal value function of a HOMDP M is upper bounded by L2 distance in M¯ as follows,

|V ∗(o1≤t) − V ∗(o2≤t)| ≤

LR

Wd

φ

(o

1 ≤

t

),

φ(o

2 ≤t

)

1 − γLD

+2 L∞ + γL∞ LR .

r

d 1 − γLD

10

Learning Causal State Representations of Partially Observable Environments

Using this fact that the optimal value func tion is Lipschitz, we can write a lower bound for the optimal state-action value function in our causal state MDP. Given the above bound on the optimal value function, we denote its Lipschitz constant LV ∗.
Theorem 5 Given a HOMDP M with states O≤t, where 0 ≤ t ≤ H and causal state MDP Mˆ with states φ : O≤t → Sˆ. The suboptimality of the state-action value function corresponding to the optimal policy in the causal state MDP πˆ is bounded as follows:
Q∗(o≤t, a) − Qπˆ∗ (φ(o≤t), a) ≤ 2 L∞ r +1 γ−LγV ∗ L∞ d . (5)
Proof in Appendix C. This connection to bisimulation shows that there exists an optimal solution to the causal state objective and that we approach it by minimizing losses L∞ d and L∞ r , giving a lower bound on the optimal state-action value function of the causal state MDP. This maps how well we can perform on a downstream control task directly to the quality of our learned causal states representation. This result now lends itself to a gradient-based learning method, detailed in Section 4.

3.3 Causal States are a Causal Feature Set
We now show how learning causal states connects to invariant prediction, and learns the causal feature set under certain requirements. Invariant causal prediction aims to identify a set S∗ of causal variables such that a linear predictor with support on S∗ will attain consistent performance in next reward and observation prediction under all policies. In other words, ICP removes irrelevant variables from the input, just as state abstractions remove irrelevant information from the environment’s observations.
We consider whether such a state abstraction can be obtained by ICP. Intuitively, one would then expect that the causal variables should have nice properties as a state abstraction. The following result conﬁrms this to be the case; a state abstraction that selects the set of causal variables from the observation space of a HOMDP will be a bisimulation.

Assumption 1 (Environment Interventions) For a HOMDP

{O

}

t 0

,

P

, A, R, γ

,

let

S = S1 × · · · × Sn be the underlying variables of the observations O. Each action a ∈ A

corresponds to a simultaneous noise-intervention on some subset of variables Si.

Theorem 6 (Bisimulation is a Causal Feature Set) Consider a HOMDP M = {O}t0, P , A, R, γ . Let M satisfy Assumption 1. Let SRO ⊆ {1, . . . , k} be the set of variables such that the reward R(s, a) and future observations O are a function only of [s]SRO (s restricted to the indices in SRO). Then let S = AN(RO) denote the ancestors of SRO in the (fully observable) causal graph corresponding to the transition dynamics of M. Then the state encoder φS that produces abstraction φS({O}t0) = [s]S is a bisimulation, and therefore also produces causal states.
An important detail in the previous result, established in Zhang et al. (2020a) is the bisimulation relation incorporates not just the parents of the reward and next observation, but also their ancestors. This is because in RL, we seek to model return rather than solely
11

Zhang et al.
Figure 1: Graphical causal models with temporal dependence – note that while s2 (circled in blue) is the only causal parent of the reward and observation, because its next-timestep distribution depends on s1, a bisimilar abstraction must include both variables. Shaded in blue: the graphical causal model of a POMDP with states s = (s1, s2) when ignoring timesteps.
rewards, which requires a state abstraction that can capture multi-timestep interactions. We provide an illustration of this in Figure 1. As a concrete example, we note that in the popular benchmark CartPole, only position x and angle θ are necessary to predict the reward. However, predicting the return requires θ˙ and x˙ , their respective velocities.
If interventions by actions seen at training time cover all variables that can be intervened on, then the causal feature set that is robust to any interventions on those variables at test time is identiﬁable. We can now use the result from Section 3.1 to connect causal states to learning causal feature sets, as we’ve shown that causal states and causal feature sets both correspond to a bisimilar abstraction of the original HOMDP.
4. Methods for Reconstructing Causal States from Empirical Data
In the previous sections, we introduced a class of stochastic processes with discrete or continuous outputs that are optimally compressed by a ﬁnite-state hidden-Markov representation, called the causal state model of the process. Existing methods either directly partition past sequences of length L into a ﬁnite number of causal states via conditional hypothesis tests (Shalizi and Shalizi, 2004) or use Bayesian inference over a set of existing candidate states (Strelioﬀ and Crutchﬁeld, 2014). Either method can be adapted to model a joint-process and consequently obtain the next-step conditional output by marginalizing out the action At, but do not extend to the real-valued measurement case described without strong assumptions on the shape of the conditional density function.
Methods for learning a causal feature set generally require access to the underlying state variables, and are super-exponential in the number of variables (Peters et al., 2016), requiring hypothesis testing of every possible subset of variables to ﬁnd the causal set. Zhang et al. (2020a) exploits invariance across tasks to learn an invariant representation that corresponds to a bisimulation, but relies on the multi-task (and fully observable) setting. We now propose a new approach to approximately reconstruct these causal states from empirical data.
12

Learning Causal State Representations of Partially Observable Environments

Figure 2: Graphical model of the joint action-measurement stochastic process. Black-arrows indicate causal relationships between random variables and red-arrows indicate the predictive relationship between the action at, internal state s¯ (sˆ) and the next measurement ot+1. Circular boxes indicate continuous variables.

4.1 Learning Suﬃcient Statistics of History with Recurrent Networks
Recurrent neural networks (RNNs) are uniﬁlar hidden Markov models with continuous states, where the transitions and state output probabilities are parameterized by diﬀerentiable functions (Hinton, 2013). We use them to obtain recursively-computable high-dimensional suﬃcient statistics of the action-measurement joint process. This representation is learned via a recurrent encoder φ : Y≤t → Sˆ, a next step prediction network f : Sˆ × A → O, and next step reward prediction network r : Sˆ × A → R.
We note that when f (sˆt, at) is maximally predictive of the subsequent observations (the future O≥t), sˆt constitutes a suﬃcient statistic of the latent states S>t. In practice, we estimate the continuous representations using the empirical realizations4 (o>t, a≥t) to learn a neural network that approximates end-to-end the maps φ and f by minimizing the temporal loss through the following objectives:

T

LD = W P(Ot+1|o>t, a≥t, at), f (φ(o>t, a≥t), at) ,

t

(6)

T

LR = R(Ot+1|o>t, a≥t, at) − r(φ(o>t, a≥t), at) .

t

After solving Equation (6) we use the neural networks parameterized by the optimal parameters φ∗, f ∗ to derive suﬃcient continuous representations to create discrete states that are reﬁnements of the causal states.
4. With a small abuse of notation, we use the same convention of Y>t, where the joint process {O, A} has elements (O>t, A≥t).

13

Zhang et al.

4.2 Discretization of the RNN Hidden States

Together with the uniﬁlar and Markovian nature of transitions in RNNs, the suﬃciency of sˆt implies that there exists a function Ds : Rk → S that allows us to describe the causal states
s as a partition of the learned latent state sˆ (Shalizi and Crutchﬁeld, 2001; Crutchﬁeld et al.,
2010).
We set up a second optimization problem using the trained neural network and the empirical realizations of the process o to estimate the discretizer φ¯ : Sˆ → S¯ with |S¯| = |S| and the new dynamics and reward prediction networks f¯ : S¯×A → O, r¯ : S¯×A → R that map the
estimated discrete states into the next observable. We match the predictive behavior between the old network φ and the new networks Λ(o≤t, at) = (φ ◦ φ¯ ◦ f¯)(o≤t, at), (φ ◦ φ¯ ◦ r¯)(o≤t, at) that use discretized states s¯ and corresponding prediction functions f¯, r¯ by minimizing the
knowledge distillation (Hinton et al., 2015) loss:

T

min LKD f (φ(o≤t, a<t), at), r(φ(o≤t, a<t), at), Λ(o≤t, at) .

(7)

f¯,r¯,φ¯ t

When Equation (7) goes to 0 we have a suﬃcient discrete representation. Any discretiza-
tion method can be used here, either learning-based such as VQ-VAE (van den Oord et al.,
2017) and ternary tangent neurons (Koul et al., 2018) or clustering-based, such as k-means. Figure 2 shows the stochastic process representing the environment and our learned states Sˆ and S¯ and their interactions. In Algorithm 1 we present our method both with and without
discretization, where LQ is the Bellman update. We initialize the continuous causal state encoder φ, optional discrete causal state encoder φ¯, Q function, dynamics model f , reward
model r, and replay buﬀer D. At each timestep, we pass the causal state, generated by φ or φ¯, to the Q function to obtain the action that maximizes the estimated value, with
-greedy exploration. The agent steps in the environment, and saves that transition to the
replay buﬀer. Then we sample a batch of data, and use it to update the encoder through the
transition and reward losses LD and LR, respectively. The optional discrete φ is similarly updated, and ﬁnally the Q function is updated with the Bellman update.

4.3 Equivalence of 1-Step Supervision to Inﬁnite Future
Causal states are, by deﬁnition, checking equivalence over all possible future trajectories. In practice, with reinforcement learning, we do not have access to these counterfactual trajectories, but only a single trajectory taken for a given history, usually. In this setting, supervision of the next step observation is suﬃcient to learn a causal state representation. To see why, note that full episode trajectories are stored in the replay buﬀer but sampled as independent transitions. If we perform 1-step supervision (with observation-action histories) over all batches in the replay buﬀer, then each step in the episode is checked independently for equivalence.

5. Experiments
We consider three settings to learn approximate causal states through self-supervised learning and use these representations as input for reinforcement learning tasks deﬁned over the domains. The ﬁrst setting consists of grid worlds with discrete state-action spaces, which are

14

Learning Causal State Representations of Partially Observable Environments
Algorithm 1 Learning Causal States with DQN Result: φ, a causal state encoder Qθ, φ, f, r ← Q0, φ0, f0, r0
h0 ← 0 D←∅ for episode=1,M do
for t = 1, T do at, ht ← arg maxa∈A Q(φ(ot, ht−1)) # Replace with φ¯ if using discretization ot+1, rt ← step(ot, a) store(ot, at, rt, ot+1) in D Sample batch B from D f, r, φ ← ∇f,φ[LD(B)] + αR∇r,φ[LR(B)] f¯, r¯, φ¯ ← ∇f¯,r¯,φ¯[LKD(B)] # Optional discretization step Qθ ← ∇θLQ(B)
end end
simple enough that we know exactly the natural causal states. We use this set of experiments to validate that our method successfully uncovers the correct set of causal states. We further show the beneﬁt of discrete causal states – we can construct a graph of experienced transitions and run Dijkstra’s algorithm to obtain a policy.
We then expand to the rich observation setting: 1) a similar maze environment in VizDoom (Kempka et al., 2016), and 2) Atari in OpenAI Gym (Brockman et al., 2016) with ﬂickering to make it partially observable, to show our method works in settings with inﬁnite causal states. In the diﬀerent environments we compare with diﬀerent baselines – in gridworlds we show how the causal state representations, both discrete and continuous, fare against value iteration representations using RNNs and explicit memory. We validate what our theoretical results show – that causal states are a principled method for uncovering true latent states and therefore achieve optimal performance, whereas value iteration RNN methods do not. In the rich observation settings, we compare with state-of-the-art deep RL methods for learning belief states – Deep Variational Reinforcement Learning (DVRL, Igl et al. (2018)) and RNN-based method Deep Recursive Q-Networks (DRQN, Hausknecht and Stone (2015)).
5.1 Implementation Details
For the Gridworld experiments, the base forward model architecture is composed of a threelayer perceptron (MLP) encoding the observation ot and a single layer linear embedding for the action at−1. The outputs of the respective layers are concatenated and fed to a Gated Recurrent Unit (GRU) (Chung et al., 2014), and the output of the recurrent network sˆt is concatenated with the embedding of at and fed to a second MLP that outputs predictions for ot+1. All the embeddings have 64 neurons except in layout 4 where we use 256 dimensional embeddings.
For the discretization phase we use two approaches that lead to equivalent performance. In the ﬁrst, gradient based method we use a discretization network composed of a Quantized-
15

Zhang et al.
Bottleneck-Network (Koul et al., 2018) with ternary tangent neurons that auto-encodes the continuous representation sˆt generating the discrete variables s¯t, and a MLP decoder that uses the estimated discrete states for predicting the next observation ot+1. Both networks are trained with the RMSprop algorithm using cross-entropy loss for discrete observations and reconstruction loss for the continuous setting. The world-model is trained through supervised learning of the temporal loss, while the discretization network is trained via knowledge distillation using the soft outputs of the GRU decoder as targets. We experimented with both online and oﬄine discretization and found the oﬄine procedure to lead to more stable training. Alternatively, we simply run k-means clustering on the continuous representation sˆt and then use it as input to predict the next observation ot+1. We choose the minimum k leading to satisfactory performances for the next step prediction network. For all the tasks we ran downstream evaluation of our learned representation with value iteration and compare with baselines. To approximate the value function we use a 2-layer fully-connected DQN architecture separated by ReLU with a 64-dimensional hidden layer. The DRQN (Hausknecht and Stone, 2015) baselines use the same architectures but they are trained end-to-end via reward maximization.
In practice, we found using ternary neurons (Koul et al., 2018) for discretization to work best among the competing gradient based methods like straight-through estimator or VQ-VAE. To summarize, we ﬁrst minimize Lr to obtain a neural model able to generate continuous suﬃcient statistics of the future observables of the process and subsequently minimize Ld to obtain a suﬃcient representation of the dynamical system that is a reﬁnement of the original causal states.
For the VizDoom and ALE experiments we build upon Rainbow DQN (Hessel et al., 2017) and drop the discretization step to account for continuous causal states.
5.2 Gridworlds
Figure 3: Visual representation of the layouts 1 and 2 used in the gridworld experiments. In Blue the starting location, in Red the key location, in Yellow the ﬁnal goal.
We create partially observable gridworld environments where the task is for the agent to ﬁrst obtain the key, then pass through the door to obtain the ﬁnal reward. The ﬁnal state is unseen (i.e., the agent cannot pass through the door to reach it) if the agent does
16

Learning Causal State Representations of Partially Observable Environments

not have the key. The agent only knows it has the key if it remembers entering the state with the key, so without inﬁnite memory this task is partially observable. At each time step the agent receives -0.1 reward, 0.5 reward for picking up the key, and a ﬁnal reward of 1 for passing through the door. We conduct experiments with two layouts shown in Figure 3. The minimal memory requirement for solving the environment is given by the shortest path from the key to the ﬁnal destination.
DQN on Sˆ is able to achieve optimal policy across all 10 random seeds with very low or zero standard deviation as seen in Figure 4, showing the stability of our learned Sˆ. We expect Sˆ to perform as well as or better than S¯, as S¯ is distilled from Sˆ and therefore contains the same information or less. Using only current observation learns using a recurrent DQN (DRQN) (Hausknecht and Stone, 2015). We also compare against several baselines, including observation without memory (obs), both with explicit memory (obs, mem) and implicit memory in the form of an RNN (obs, rnn), and the ground truth state (s gt, which includes key information).

Layout 1

Layout 2

Figure 4: DQN training curves for Gridworlds across 10 seeds for Layouts 1 and 2 - Causal States Sˆ estimated using K-mean clustering. Y-axis is reward. Two levels of shading represent 1 and 2 standard deviations from the mean.
A more extensive comparison can be found in Table 1 with the inclusion of tabular methods. Surprisingly, DQN outperforms the tabular methods, perhaps because of insuﬃcient exploration in the tabular regime. These results are using K-means for discretization, equivalent results using ternary neurons and gradient descent can be found in Appendix E.
5.3 Planning with Causal States
With a minimal suﬃcient uniﬁlar model we can perform eﬃcient planning by representing it as a labeled directed graph G = (S, Ti(jo|a)), with causal states as nodes and action-observation conditional transitions as edges. Because of the uniﬁlar property, once an agent has perfect knowledge of the current causal states, the information in the future action-observation process are suﬃcient to uniquely determine the future causal states. This property can
17

Zhang et al.

Table 1: Results for Gridworlds. Reward obtained with tabular Q-learning, DQN, and DRQN with γ = 0.99. Models trained on 1000 episodes and evaluated on 100. Numbers are mean, standard error across 10 random seeds. First section is our method. Second is baselines on current observation, history of observations, and S. Final section is using ground truth states. Note that tabular methods are deterministic and so std. is always 0.

Method
Tabular, S¯ DQN, S¯ DQN, Sˆ Dijkstra, S¯
DQN, Y DQN, Y≤t DRQN, Y Tabular, Y
Tabular, Sgt DQN, Sgt Dijkstra, Sgt

Layout 1
0.43 ± 0. 0.50 ± 0.005 0.5 ± 0. 0.5, 0.
−9.46 ± 0.06 −0.91 ± 0.95 −9.75 ± 0.07 −9.40 ± 0.
0.45 ± 0. 0.44 ± 0.01 0.5, 0.

Layout 2
0.01 ± 0. −0.17 ± 0.24 0.30 ± 0. 0.3, 0.
−9.48 ± 0.04 0.23 ± 0.05 −5.63 ± 1.18 −9.11 ± 0.
0.23 ± 0. 0.30 ± 0.003 0.3, 0.

be exploited by multi-step planning algorithms which need not keeping track of potential stochastic transitions between the underlying states, enabling a variety of methods that are otherwise amenable only to MDPs like Dijkstra’s algorithm. We plan over our learned discrete representation by building a graph G := (V, E) where V := {S¯}, E := {(s¯i, s¯j); si, sj ∈ S¯}, and p(sj|si, a) > 0 for a ∈ A. We obtain the optimal policy for Layout 1 and 2 obtaining respectively 0.5 and 0.3 reward (Table 1). We derive G and save high reward states seen during the rollouts as goal states. Then one can map the initial and ﬁnal observations to a node in the graph and run Dijkstra’s algorithm to ﬁnd the shortest path as proposed in Zhang et al. (2018). Unlike value iteration, this requires no learning and no re-sampling of the environment by making use of the graph edges, where Q-learning only uses the nodes.
5.4 Doom
We now extend our results to continuous state space environments, where causal states are potentially inﬁnite. We modify the T-maze VizDoom (Kempka et al., 2016) environment of Corneil et al. (2018) to make it partially observable. We randomize the goal location between the two ending corners and signal its location with a stochastic signal in the observation space. The agent must remember where the goal is in order to navigate to it. We convey the signal to the agent through a fourth channel (after RGB) that intermittently contains information about where the goal is. The frequency at which the information is displayed is a tunable factor f = 2 for these experiments. Example trajectories to diﬀerent goals that the learned agent takes are shown in Figure 5. We compare with Rainbow DQN (Hessel
18

Learning Causal State Representations of Partially Observable Environments
Figure 5: Example trajectories in VizDoom to two goals.
Figure 6: Doom T-Maze POMDP: Averaged over 100 runs with diﬀerent random seeds with one standard error shaded. Y-axis is mean reward per step.
et al., 2017) modiﬁed with an LSTM to become Rainbow DRQN (Hausknecht and Stone, 2015), and Deep Variational Reinforcement Learning (DVRL) (Igl et al., 2018), another gradient-based method for learning belief states. Figure 6 shows the speedup in learning from explicitly learning to cluster sequences of observations into causal states. In Igl et al. (2018) DVRL was originally evaluated on Atari, and we adapted the implementation to VizDoom but found the method to transfer poorly. Rainbow DRQN performs better, but does not handle the required memory perfectly. Our method, causal states, quickly learns a latent representation maximally predictive of future states and successfully learns a good policy. 5.5 Flickering Atari We also evaluate our model on a suite of tasks from Atari in OpenAI Gym (Brockman et al., 2016), another setting where causal states are less intuitive. Example pixel observations from the tasks evaluated can be found in Figure 7. We use the same preprocessing of observations in Mnih et al. (2013), except each frame has a p = 0.5 chance of being blank, introduced as
19

Zhang et al.
a POMDP version of the environment by Hausknecht and Stone (2015). The environment is now partially observable as the agent must learn to retain information when the current observation is blank from the hidden state of the RNN. Our algorithm learns an approximate causal state representation suitable for learning successful policies, achieving a higher ﬁnal score in most environments evaluated (Table 2). We again see a decrease in performance with Rainbow DRQN and DVRL, due to instability in training, with better performance exhibited by Rainbow DRQN.

Figure 7: Pixel observations from the 10 Atari games evaluated, in alphabetical order from left to right and top to bottom: Air Raid, Asteroids, Bowling, Boxing, Centipede, Gopher, Ice Hockey, Ms. Pacman, Pong, and Space Invaders.

Game
Air Raid Asteroids Bowling Boxing Centipede Gopher Ice Hockey Ms. Pacman Pong Space Invaders

Causal States
950 ± 271 1129 ± 345 34 ± 8 4±4 4586 ± 763 783 ± 151 −3 ± 1 671 ± 36 −2 ± 6 354 ± 67

DRQN
518 ± 231 929 ± 285 29 ± 0 0±2 3127 ± 71 620 ± 129 −5 ± 1 849 ± 60 −7 ± 7 381 ± 14

DVRL
748 ± 156 349 ± 54 23 ± 1 16 ± 3 1157 ± 130 255 ± 129 −11 ± 0 181 ± 45 −20 ± 0 68 ± 9

Table 2: Mean results and 95% conﬁdence intervals across 10 seeds for Atari games evaluated at 2.5M steps.

20

Learning Causal State Representations of Partially Observable Environments
6. Related Literature
In this section we explore other works that have drawn similar parallels across causal states and partial observability methods, as well as related works broaching bisimulation and causal inference, and their corresponding algorithms. We end with an exploration of similar works using knowledge distillation methods.
The relationship between PSRs and causal states has been previously suggested in the computational mechanics literature (Shalizi and Crutchﬁeld, 2001; Shalizi and Shalizi, 2004; Barnett and Crutchﬁeld, 2015) but not fully explored as an objective for partial observability settings in reinforcement learning. Hundt et al. (2007) also derive parallels between PSRs, POMDPs, and automata through the construction of equivalence classes that groups states with common action conditional future observations, but do not extend this to the causal literature. Causal states, and the related information-theoretic notion of complexity, minimality, and suﬃciency are used to derive task-agnostic policies with an intrinsic exploration-exploitation trade oﬀ in Still (2009); Still and Precup (2012). More similar to our latent discrete states with continuous observables, Goerg and Shalizi (2013) model spatio-temporal processes as being generated by a ﬁnite set of discrete causal states in the form of light-cones. Our additional contribution beyond these works is proving the optimality of the causal state representation for POMDPs, providing additional lower bounds on the optimal state-action value function, and providing a gradient-based algorithm for learning causal state representations.
Now focusing on methods that approach the partial observability problem in reinforcement learning, Boots et al. (2013) learn PSRs in Reproducing Kernel Hilbert Space, extending the approach to continuous, potentially inﬁnite, action-observation processes. However, their method requires maintaining a state vector with length proportional to the size of the training dataset, which would be untenable in deep RL settings with millions of data points. AIXI (Veness et al., 2011) is another Bayesian optimality approach for general reinforcement learning in unknown environments, but also relies on maximizing rewards for a ﬁnite m steps into the future, as opposed to causal states which (theoretically) optimize for predictive performance into the inﬁnite future. Other methods for learning deep representations for reinforcement learning POMDPs have been proposed, starting with adding recurrency to DQN (Hausknecht and Stone, 2015) to integrate the history in the estimation of the Q-value as opposed to using only the current observation. However, this method stops short of ensuring suﬃciency for next step prediction as it learns a task speciﬁc representation. Igl et al. (2018); Tschiatschek et al. (2018) use deep variational methods to learn a probability distribution over states, i.e., belief states, and use the belief states for policy optimization with actor-critic (Igl et al., 2018) and planning (Tschiatschek et al., 2018). Guo et al. (2018) also use neural methods to learn belief states with next-step prediction, Hefny et al. (2015, 2018) learn PSRs with RNNs and spectral methods and use policy gradient with an alternating optimization method for the policy and the state representation to handle continuous action spaces. Finally, Jiang et al. (2016) propose a gradient-based method for learning a spectral decomposition of the systems dynamic matrix to extract PSRs. None of these methods explore the connection to causal states, bisimulation, and compression via a discrete representation, and therefore give no guarantee that the method converges to an optimal solution, or bounds on closeness to that solution. Corneil et al. (2018) do learn
21

Zhang et al.
discrete representations but not in partially observable environments and with no link to PSRs. Instead, they propose the discrete representation solely for using tabular Q-learning with prioritized sweeping.
Various forms of state abstractions have been deﬁned in Markov decision processes (MDPs) to group states into clusters whilst preserving some property (e.g. the optimal value, or all values, or all action values from each state) (Larsen and Skou, 1989; Givan et al., 2003; Li et al., 2006). The strictest form, which generally preserves the most properties, is bisimulation (Larsen and Skou, 1989). Bisimulation only groups states that are indistinguishable w.r.t. reward sequences output given any action sequence tested. A related concept is bisimulation metrics (Ferns and Precup, 2014), which measure how “behaviorally similar” states are. Ferns et al. (2011) deﬁnes the bisimulation metric with respect to continuous MDPs, and propose a Monte Carlo algorithm for learning it using an exact computation of the Wasserstein distance between empirically measured transition distributions. However, this method does not scale well to large state spaces. Taylor et al. (2009) relate MDP homomorphisms to lax probabilistic bisimulation, and deﬁne a lax bisimulation metric. They then compute a value bound based on this metric for MDP homomorphisms, where approximately equivalent state-action pairs are aggregated. Castro (2020) propose an algorithm for computing on-policy bisimulation metrics, but with a focus only on deterministic settings and the policy evaluation problem. Zhang et al. (2020b) propose a method for learning bisimulation metrics to learn a coarsest bisimulation partition in the control setting, while Castro et al. (2009) showed the link between trajectory equivalence in the partial observability setting and bisimulation. We believe our work is the ﬁrst to apply bisimulation-based bounds to learned representations in the partially observable setting and prove a link to causal states. Similarly, Zhang et al. (2020a) show the connection between bisimulation and causal feature sets, and utilize the requirements of interventions over all variables in the multi-task reinforcement learning setting with full observability.
The idea of extracting the implicit knowledge of a neural network (Towell and Shavlik, 1993; Fu, 1994; Lu et al., 1996; Hailesilassie, 2016) is not novel and is rooted in early attempts to merge traditional rule-based methods with machine learning. The most recent examples (Weiss et al., 2017; Wang et al., 2017, 2018) are focused on the ability of character level RNNs to implicitly learn grammars. The only application of these ideas to POMDPs that we are aware of is in Koul et al. (2018), where deep recurrent policies (Hausknecht and Stone, 2015) are quantized into Moore machines. The main diﬀerence between our approaches is that we reduce models of the environment, not of the policy, to -machines.
7. Discussion
We conclude with a recap of our contributions, a nuanced analysis of our approach and results, and a discussion of future work.
In this paper, we proposed a self-supervised method for learning a minimal suﬃcient statistic for next step prediction in POMDPs and articulated its connection to the causal states of a controlled process, as well as the equivalence of causal states and bisimulation. This connection leads us to a new bound on the optimal value function in the MDP generated by the learned causal states representation. We further show connections between causal
22

Learning Causal State Representations of Partially Observable Environments
states and causal feature sets from the causal inference literature, allowing us to borrow explicit requirements about the diversity of data through the language of interventions.
Our experiments demonstrate the practical utility of this representation, both with value iteration for control and exhaustive planning, in solving inﬁnite-memory POMDP environments (Section 5.2), as well as k-order MDP environments with high dimensional observations (Section 5.4, Section 5.5), matching the performance achieved with ground truth states. Further, we show that the method extends to settings with continuous stateaction spaces with an inﬁnite number of causal states, where we must relax the discrete assumption and only learn a continuous representation, also supported by empirical results in Section 5.4, Section 5.5. However, we stress the importance of the discrete causal state results as empirical conﬁrmation that our method is able to reconstruct the correct latent states, and that they are informationally equivalent to the causal states.
Causal states have been mostly studied from the discrete perspective, but in continuous state settings there are potentially an inﬁnite number – hence the relaxation to a continuous representation in rich observation settings. We also ﬁnd that discrete optimization is often diﬃcult to tune empirically, and found it to be unstable in rich observation settings. Recent progress in discrete world models (Hafner et al., 2020) may oﬀer a promising approach to a successful empirical implementation for more complex problems. The beneﬁt of the discrete representation is its interpretability and ability to use graph-based planning methods (Zhang et al., 2018; Yang et al., 2020). Nevertheless, our results in the continuous setting are still promising, in that they empirically outperform other POMDP approaches, while maintaining theoretical guarantees.
However, we are aware that the empirical choices made to implement the gradient-based method for learning causal states induce some limitations. In this work we chose a two-step method of learning a continuous representation and using knowledge distillation to discretize the continuous causal states. We found this method to be more stable than end-to-end discretization methods like VQ-VAE or straight-through estimators.
The trade-oﬀ is that with a two-step procedure, the performance of the discretized causal states uniquely depends on the performance of the continuous representation. Hence, even in situations where the environment has a ﬁnite number of states, we are currently not using this information to improve the robustness or learning speed of the continuous representation. While learning an end-to-end model, we could use the discrete outputs as input for state counting policies or other exploration strategies, potentially reducing the number of samples necessary to learn a model covering all the possible state transitions. Analogously, while learning a reward maximizing policy, discrete causal states could be used in combination with path ﬁnding algorithms to eﬃciently plan more eﬀective exploration.
To conclude, we think causal states can further be exploited to solve problems other than control by oﬀering a practically and theoretically sound bridge between deep learning, hidden Markov and causal inference representations of dynamical systems.
Acknowledgments
T. Furlanello and L. Itti were supported by the National Science Foundation (grant number CCF-1317433), C-BRIC (one of six centers in JUMP, a Semiconductor Research Corporation
23

Zhang et al. (SRC) program sponsored by DARPA), and the Intel Corporation. A. Anandkumar is supported in part by Bren endowed chair, Darpa PAI, Raytheon, and Microsoft, Google and Adobe faculty fellowships. K. Azizzadenesheli is supported in part by NSF Career Award CCF-1254106 and AFOSR YIP FA9550-15-1-0221, work done while he was visiting Caltech. The authors aﬃrm that the views expressed herein are solely their own, and do not represent the views of the United States government or any agency thereof. A. Zhang would like to thank Pablo Samuel Castro, Marc Bellemare, and Doina Precup for helpful discussions and feedback.
24

Learning Causal State Representations of Partially Observable Environments

Appendix A. Proof of Theorem 1

Theorem 1 (Causal States are Bisimilar) If two observation sequences belong to the same causal state as per Theorem 1, they are also bisimilar as per Theorem 4.

Proof

First,

we

show

that

if

two

observation

sequences

(o

1 ≤t

,

a1<t

)

,

(

o2≤

t

,

a2<t

)

are

in

the

same

causal state σi, they are trajectory equivalent. This is evident from Theorem 1, where we

have

P(O>t, A≥t|o1≤t, a1<t) = P(O>t, A≥t|o2≤t, a2<t),

therefore conditioned on any action sequence a≥t ∈ A≥t we have

P(O>t, A≥t|o1≤t, a1<t, a≥t) = P(O>t, A≥t|o2≤t, a2<t, a≥t).

Denoting the causal state mapping as Ψ and remembering that reward is now part of the observation, gives us
P(α|Ψ(s1), a≥t) = P(α|Ψ(s2), a≥t),

where

s1

:=

o1≤t, a1<t

and

s2

:=

o

2 ≤t

,

a2<t

.

Remember

that

α

is

a

ﬁnite

reward-state

trajectory

and therefore from this equivalence we can pull out,

R(Ψ(s1), a) = R(Ψ(s2), a) P(s |Ψ(s1), a≥t) = P(s |Ψ(s2), a≥t).

where s ∈ Ψ(S).

Appendix B. Proof of Theorem 3

Theorem 3 Let M be the HOMDP representing a POMDP whose states are the history of observations O≤t, and M¯ be the LD-LR-Lipschitz MDP with Ld and Lr representing the
losses. Let φ denote the encoder that takes in the history of observations and outputs the approximate causal state. The bisimulation distance d˜ : O≤t × O≤t → R in M can be upper bounded by L2 distance in M¯ as

d˜(o1≤t, o2≤t) ≤

(1 − γ)LR Wd

φ

(

o

1 ≤t

)

,

φ(

o2≤

t

)

1 − γLD

+2 L∞ + γL∞ LR ,

r

d 1 − γLD

Proof We prove this with the triangle inequality in three steps. First, we need to show that in the LD-LR-Lipschitz causal state MDP with metric d on the state space the bisimulation metric is bounded,

d˜(φ(o1 ), φ(o2 )) ≤ (1 − γ)LR d φ(o1 ), φ(o2 ) .

≤t

≤t

1 − γLD

≤t

≤t

We do so with a proof by induction for a sequence of psuedometrics dn that converge to d˜, starting with d0(φ(o1≤t), φ(o2≤t)) = 0.

25

Zhang et al.

dn+1(φ(o1≤t), φ(o2≤t)) = max (1 − γ)|rφa(o1 ) − rφa(o2 )| + γWdn (Pφa(o1 ), Pφa(o2 ))

a∈A

≤t

≤t

≤t

≤t

from Theorem 7

= max
a∈A

|rφa(o1 ) − rφa(o2 )|

Wdn (Pφa(o1 ), Pφa(o2 ))

(1 − γ) ≤t

≤t + γ

≤t

≤t

d(

φ

(o

1 ≤

t

),

φ(o

2 ≤t

))

d(

φ

(o

1 ≤

t

),

φ(o

2 ≤t

)

d

φ

(o

1 ≤t

),

φ(

o

2 ≤t

))

|rφa(o1 ) − rφa(o2 )|

Wdn (Pφa(o1 ), Pφa(o2 ))

= (1 − γ) max ≤t

≤t + γ max

≤t

≤t

a∈A d(φ(o1 ), φ(o2 )) a∈A d(φ(o1 ), φ(o2 ))

d

φ

(o

1 ≤

t

),

φ(o

2 ≤

t

)

≤t

≤t

≤t

≤t

Wdn (Pφa(o1 ), Pφa(o2 ))

≤ (1 − γ)LR + γ max
a∈A

≤t

≤t

d(φ(o1 ), φ(o2 ))

d

φ

(o

1 ≤

t

),

φ(o

2 ≤t

)

≤t

≤t

dn

(

φ

(o

1 ≤

t

),

φ(o

2 ≤t

))

Wd˜(Pφa(o1 ), Pφa(o2 ))

= (1 − γ)LR + γ d φ(o1 ), φ(o2 )

max
a∈A

≤t

≤t

d(φ(o1 ), φ(o2 ))

d(

φ

(o

1 ≤

t

),

φ(o

2 ≤t

))

≤t

≤t

≤t

≤t

property of Wasserstein

≤ (1 − γ)LR + γdn(φ(o1≤t), φ(o2≤t))d(φ(o1≤t), φ(o2≤t))LD

Solving for n → ∞ gives us,

∞
d˜(φ(o1≤t), φ(o2≤t)) ≤ (1 − γ)LRd(φ(o1≤t), φ(o2≤t)) (γLD)i

i=0

= (1 − γ)LR d φ(o1 ), φ(o2 ) .

1 − γLD

≤t

≤t

Next, we need to show that if we join the HOMDP and causal state MDP into a single MDP, we can bound the bisimulation metric from a state in one, o≤t, to its corresponding state in the other, φ(o≤t),

d˜(o≤t, φ(o≤t)) ≤ L∞ + γL∞ LR .

r

d 1 − γLD

d˜(o≤t, φ(o≤t)) = max (1 − γ)|roa ) − r¯φa(o )| + γWd˜(P (·|o≤t, a), P¯(·|φ(o≤t), a)

a∈A

≤t

≤t

≤ (1 − γ)L∞ r + γ max Wd˜ P (·|o≤t, a), P¯(·|φ(o≤t), a)
a∈A

≤ (1 − γ)L∞ r + γ max Wd˜ P (·|o≤t, a), φP (·|o≤t, a) + Wd˜ φP (·|o≤t, a), P¯(·|φ(o≤t), a)
a∈A

≤

(1

−

γ)L∞

+

(1 γ

−

γ)LR L∞

+

γ

max W ˜

φP (·|o≤t, a), P¯(·|φ(o≤t), a)

r

1 − γLD d

a∈A d

≤ (1 − γ)L∞ + γ (1 − γ)LR L∞ + γd˜(o≤t+1, φ(o≤t+1))

r

1 − γLD d

= L∞ + γL∞ LR

r

d 1 − γLD

By recursion.

26

Learning Causal State Representations of Partially Observable Environments Now, we can plug these into the triangle inequality,

d˜(o1≤t, o2≤t) ≤ d˜(φ(o1≤t), φ(o2≤t)) + d˜(φ(o1≤t), o2≤t) + d˜(o1≤t, φ(o2≤t))

≤ d˜(φ(o1 ), φ(o2 )) + 2 L∞ + γL∞ LR

≤t

≤t

r

d 1 − γLD

≤ (1 − γ)LR d(φ(o1 ), φ(o2 )) + 2 L∞ + γL∞ LR .

1 − γLD

≤t

≤t

r

d 1 − γLD

Appendix C. Proof of Theorem 5
Theorem 5 Given a HOMDP M with states O≤t, where 0 ≤ t ≤ H and causal state MDP Mˆ with states φ : O≤t → Sˆ. The suboptimality of the state-action value function corresponding to the optimal policy in the causal state MDP πˆ is bounded as follows:
Q∗(o≤t, a) − Qπˆ∗ (φ(o≤t), a) ≤ 2 L∞ r +1 γ−LγV ∗ L∞ d . (5) 27

Zhang et al.

Proof

sup |Qπ(o≤t, at) − Q¯π(φ(o≤t), at)|
o≤t∈O≤t,at∈A

≤ sup |R(φ(o≤t), a, φ(o≤t+1)) − r|
o≤t∈O≤t,at∈A

+γ

sup

|Eo≤t+1∼P (·|o≤t,at)V π(o≤t+1) − Esˆt+1∼f (·|φ(o≤t),at)V¯ π(sˆt+1)|

o≤t∈O≤t,at∈A

= L∞ r + γ sup

Eo≤t+1∼P (·|o≤t,at)[V π(o≤t+1) − V¯ π(φ(o≤t+1))]

o≤t∈O≤t,at∈A

+ E o≤t+1∼P (·|o≤t,at) [V¯ π(φ(o≤t+1)) − V¯ π(sˆt+1)]
sˆt+1∼f (·|φ(o≤t),at)

≤ L∞ r + γ sup

Eo≤t+1∼P (·|o≤t,at)[V π(o≤t+1) − V¯ π(φ(o≤t+1))]

o≤t∈O≤t,at∈A

+ γ sup

E o≤t+1∼P (·|o≤t,at) [V¯ π(φ(o≤t+1)) − V¯ π(sˆt+1)]

o≤t∈O≤t,at∈A sˆt+1∼f (·|φ(o≤t),at)

≤ L∞ r + γ sup

Eo≤t+1∼P (·|o≤t,at)[V π(o≤t+1) − V¯ π(φ(o≤t+1))]

o≤t∈O≤t,at∈A

+ γLV ∗ sup Wd(φ(P (·|o≤t, at)), f (·|φ(o≤t), at))
o≤t∈O≤t,at∈A

= L∞ r + γ sup

Eo≤t+1∼P (·|o≤t,at)[V π(o≤t+1) − V¯ π(φ(o≤t+1))] + γLV ∗ L∞ d

o≤t∈O≤t,at∈A

≤ L∞ r + γ sup Eo≤t+1∼P (·|o≤t,at) [V π(o≤t+1) − V¯ π(φ(o≤t+1))] + γLV ∗ L∞ d
o≤t∈O≤t,at∈A

≤ L∞ r + γ sup

[V π(o≤t) − V¯ π(φ(o≤t))] + γLV ∗ L∞ d

o≤t∈O≤t,at∈A

≤ L∞ r + γ sup

[Qπ(xt−1, at−1) − Q¯π(φ(xt−1), at−1)] + γLV ∗ L∞ d

o≤t∈O≤t,at∈A

= L∞ r + γLV ∗ L∞ d 1−γ

Appendix D. Proof of Theorem 6
Theorem 6 (Bisimulation is a Causal Feature Set) Consider a HOMDP M = {O}t0, P , A, R, γ . Let M satisfy Assumption 1. Let SRO ⊆ {1, . . . , k} be the set of variables such that the reward R(s, a) and future observations O are a function only of [s]SRO (s restricted to the indices in SRO). Then let S = AN(RO) denote the ancestors of SRO in the (fully observable) causal graph corresponding to the transition dynamics of M. Then the state encoder φS that produces abstraction φS({O}t0) = [s]S is a bisimulation, and therefore also produces causal states.
Proof To prove that φS is a bisimulation, we must ﬁrst show that r(x) = r(x ) for any x, x : φS(x) = φS(x ). For this, we note that E[R(x)] = r∈R rdp(r|x) = r∈R rdp(r|[x]S, [x]SC )
28

Learning Causal State Representations of Partially Observable Environments

and, because by deﬁnition SC ⊂ PA(R)C, we have that R ⊥ [x]SC . Therefore,

E[R(x)] = rdp(r|[x]S) = rdp(r|[x ]S) = E[R(x )].

(8)

r∈R

r∈R

To show that [x]S is a bisimulation, we must also show that for any x1, x2 such that

φ(x1) = φ(x2), and for any e ∈ E, the distribution over next state equivalence classes will be

equal for x1 and x2:

Pxe1x =

Pxe2x .

x ∈φ−1(X¯ )

x ∈φ−1(X¯ )

For this, it suﬃces to observe that S is closed under taking parents in the causal graph, and that by construction environments only contain interventions on variables outside of the causal set. Speciﬁcally, we observe that the probability of seeing any particular equivalence class [x ]S after state x is only a function of [x]S: P ([x ]S|x) = f ([x]S, [x ]S). This allows us to deﬁne a natural decomposition of the transition function as follows:

P (x |x) = P [x]S ⊕ [x]SC [x ]S ⊕ [x ]SC , which by the independent noise assumption gives

P (x |x) = f ([x ]S, [x]S)P ([x ]Sc|x). We further observe that since the components of x are independent, [x ]SC P ([x ]SC |x) = 1. We now return to the property we want to show:

Pxe1x =

f ([x1]S, [x ]S)P (x |x1)

x ∈φ−1(x¯)

x ∈φ−1(x¯)

= f (φ(x1), x¯)

P

[x ]SC

= f (φ(x1), x¯)

[x ]SC x1

and because φ(x1) = φ(x2), we have

= f (φ(x2), x¯)

for which we can apply the previous chain of equalities backward to obtain

= Pxe2x
x ∈φ−1(x¯)

Appendix E. Additional Gridworld Results
In Figure 8 we have additional results using ternary neurons as a gradient-based approach to discretization. We compare the continuous representation, which performs best, to the discretized causal states in blue, and DRQN in red. We ﬁnd a drop in performance when performing discretization, showing that we are not able to empirically retain the required information with this discretization method.
29

Zhang et al.
Figure 8: Training curves for DQN policies—discretization with gradient descent and bottleneck networks (left) Layout 1 using discrete inputs.(right) Layout 2 using discrete inputs. Averages over 10 runs with diﬀerent random seeds with two standard deviations shaded. Y-axis is mean reward per step. Green is continuous causal states, Blue is discrete causal states, red is DRQN.
References
Karl J Aastrom. Optimal control of markov processes with incomplete state information. Journal of Mathematical Analysis and Applications, 10(1):174–205, 1965.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization. arXiv e-prints, July 2019.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning in rich-observation MDPs using spectral methods. arXiv preprint arXiv:1611.03907, 2016.
Nix Barnett and James P Crutchﬁeld. Computational mechanics of input–output processes: Structured transformations and the eps-transducer. Journal of Statistical Physics, 161(2): 404–451, 2015.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Int. Res., 47(1):253–279, May 2013. ISSN 1076-9757.
Dimitri Bertsekas and David Castanon. Adaptive aggregation for inﬁnite horizon dynamic programming. Automatic Control, IEEE Transactions on, 34:589 – 598, 07 1989. doi: 10.1109/9.24227.
Byron Boots, Geoﬀrey Gordon, and Arthur Gretton. Hilbert space embeddings of predictive state representations. arXiv preprint arXiv:1309.6819, 2013. 30

Learning Causal State Representations of Partially Observable Environments
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. URL http://arxiv.org/abs/1606. 01540. cite arxiv:1606.01540.
Anthony R Cassandra, Leslie Pack Kaelbling, and Michael L Littman. Acting optimally in partially observable stochastic domains. In AAAI, volume 94, pages 1023–1028, 1994.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov decision processes. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2020.
Pablo Samuel Castro and Doina Precup. Using bisimulation for policy transfer in MDPs. In Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2010.
Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. Equivalence relations in fully and partially observable markov decision processes. In Proceedings of the 21st International Jont Conference on Artiﬁcal Intelligence, IJCAI’09, pages 1653–1658, San Francisco, CA, USA, 2009. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/ citation.cfm?id=1661445.1661711.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Dane S. Corneil, Wulfram Gerstner, and Johanni Brea. Eﬃcient model-based deep reinforcement learning with variational state tabulation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, pages 1057–1066, 2018.
James P Crutchﬁeld and Karl Young. Inferring statistical complexity. Physical Review Letters, 63(2):105, 1989.
James P Crutchﬁeld, Christopher J Ellison, Ryan G James, and John R Mahoney. Synchronization and control in intrinsic and designed computation: An information-theoretic analysis of competing models of stochastic computation. Chaos: An Interdisciplinary Journal of Nonlinear Science, 20(3):037105, 2010.
Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of Science, 74(5):981–995, 2007. doi: 10.1086/525638. URL https://doi.org/10.1086/ 525638.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for ﬁnite markov decision processes. In Proceedings of the 20th Conference on Uncertainty in Artiﬁcial Intelligence, UAI ’04, pages 162–169, Arlington, Virginia, United States, 2004. AUAI Press. ISBN 0-9749039-0-6. URL http://dl.acm.org/citation.cfm?id=1036843.1036863.
Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov decision processes. SIAM J. Comput., 40(6):1662–1714, December 2011. ISSN 0097-5397. doi: 10.1137/10080484X. URL http://dx.doi.org/10.1137/10080484X.
31

Zhang et al.
Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In Uncertainty in Artiﬁcial Intelligence (UAI), pages 210–219, 2014.
LiMin Fu. Rule generation from neural networks. IEEE Transactions on Systems, Man, and Cybernetics, 24(8):1114–1124, 1994.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G. Bellemare. DeepMDP: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2170–2179, Long Beach, California, USA, 09–15 Jun 2019. PMLR.
Robert Givan, Thomas L. Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. Artif. Intell., 147:163–223, 2003.
Georg Goerg and Cosma Shalizi. Mixed licors: A nonparametric algorithm for predictive state reconstruction. In Artiﬁcial Intelligence and Statistics, pages 289–297, 2013.
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A. Pires, Toby Pohlen, and R´emi Munos. Neural predictive belief representations. CoRR, abs/1811.06407, 2018.
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models, 2020.
Tameru Hailesilassie. Rule extraction algorithm for deep neural networks: A review. arXiv preprint arXiv:1610.05267, 2016.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable MDPs, 2015. URL https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/ 11673.
Ahmed Hefny, Carlton Downey, and Geoﬀrey J Gordon. Supervised learning for dynamical system learning. In Advances in neural information processing systems, pages 1963–1971, 2015.
Ahmed Hefny, Zita Marinho, Wen Sun, Siddhartha Srinivasa, and Geoﬀrey Gordon. Recurrent predictive state policy networks. arXiv preprint arXiv:1803.01489, 2018.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, 2017.
Geoﬀrey Hinton. Recurrent neural networks, lecture 10 of CSC2535. 2013.
Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
32

Learning Causal State Representations of Partially Observable Environments
Christopher Hundt, Prakash Panangaden, Joelle Pineau, and Doina Precup. Representing systems with hidden state. In Clayton T. Morrison and Tim Oates, editors, Computational Approaches to Representation Change during Learning and Development, Papers from the 2007 AAAI Fall Symposium, Arlington, Virginia, USA, November 9-11, 2007, volume FS-07-03 of AAAI Technical Report, pages 17–23. AAAI Press, 2007. URL https: //www.aaai.org/Library/Symposia/Fall/2007/fs07-03-003.php.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational reinforcement learning for POMDPs. arXiv preprint arXiv:1806.02426, 2018.
Nan Jiang, Alex Kulesza, and Satinder Singh. Improving predictive state representations via gradient descent. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, AAAI’16, pages 1709–1715. AAAI Press, 2016. URL http://dl.acm.org/ citation.cfm?id=3016100.3016138.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial intelligence, 101(1-2):99–134, 1998.
Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, pages 341–348, Santorini, Greece, Sep 2016. IEEE. URL http://arxiv.org/abs/1605.02097. The best paper award.
Anurag Koul, Sam Greydanus, and Alan Fern. Learning ﬁnite state representations of recurrent policy networks. arXiv preprint arXiv:1811.12530, 2018.
Kim G. Larsen and Arne Skou. Bisimulation through probabilistic testing (preliminary report). In Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’89, page 344–352, New York, NY, USA, 1989. Association for Computing Machinery. ISBN 0897912942. doi: 10.1145/75277.75307. URL https: //doi.org/10.1145/75277.75307.
Lihong Li, Thomas Walsh, and Michael Littman. Towards a uniﬁed theory of state abstraction for MDPs. Proceedings of the Ninth International Symposium on Artiﬁcial Intelligence and Mathematics, 01 2006.
Michael L Littman and Richard S Sutton. Predictive representations of state. In Advances in neural information processing systems, pages 1555–1561, 2001.
Hongjun Lu, Rudy Setiono, and Huan Liu. Eﬀective data mining using neural networks. IEEE transactions on knowledge and data engineering, 8(6):957–961, 1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.
Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.
33

Zhang et al.
Jonas Peters, Peter Bu¨hlmann, and Nicolai Meinshausen. Causal inference using invariant prediction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society, Series B (with discussion), 78(5):947–1012, 2016.
Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation. Math. Oper. Res., 31(2):234–244, 2006. doi: 10.1287/moor.1060.0188. URL https://doi.org/10.1287/moor.1060.0188.
Nicholas Roy, Geoﬀrey Gordon, and Sebastian Thrun. Finding approximate POMDP solutions through belief compression. J. Artif. Int. Res., 23(1):1–40, January 2005. ISSN 1076-9757.
Ju¨rgen Schmidhuber. Learning algorithms for networks with internal and external feedback. Proc. of the 1990 Connectionist Models Summer School, pages 52–61, 1990a.
Ju¨rgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In In Proc. IEEE/INNS International Joint Conference on Neural Networks, pages 253–258. IEEE Press, 1990b.
Bernhard Sch¨olkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML’12, page 459–466, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851.
Bernhard Sch¨olkopf. Causality for machine learning. CoRR, abs/1911.10500, 2019.
Cosma Rohilla Shalizi and James P Crutchﬁeld. Computational mechanics: Pattern and prediction, structure and simplicity. Journal of statistical physics, 104(3-4):817–879, 2001.
Cosma Rohilla Shalizi and Kristina Lisa Shalizi. Blind construction of optimal nonlinear recursive predictors for discrete sequences. In Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence, pages 504–511. AUAI Press, 2004.
Satinder Singh, Michael R James, and Matthew R Rudary. Predictive state representations: A new theory for modeling dynamical systems. In Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence, pages 512–519. AUAI Press, 2004.
Susanne Still. Information-theoretic approach to interactive learning. EPL (Europhysics Letters), 85(2):28005, 2009.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139–148, 2012.
Christopher C Strelioﬀ and James P Crutchﬁeld. Bayesian structural inference for hidden processes. Physical Review E, 89(4):042119, 2014.
Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bounding performance loss in approximate MDP homomorphisms. In Neural Information Processing (NeurIPS), pages 1649–1656, 2009. URL http://papers.nips.cc/paper/ 3423-bounding-performance-loss-in-approximate-mdp-homomorphisms.pdf.
34

Learning Causal State Representations of Partially Observable Environments
Geoﬀrey G Towell and Jude W Shavlik. Extracting reﬁned rules from knowledge-based neural networks. Machine learning, 13(1):71–101, 1993.
Sebastian Tschiatschek, Kai Arulkumaran, Jan Stu¨hmer, and Katja Hofmann. Variational inference for data-eﬃcient model learning in POMDPs. CoRR, abs/1805.09281, 2018.
Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6306–6315. Curran Associates, Inc., 2017.
Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver. A monte-carlo aixi approximation. J. Artif. Int. Res., 40(1):95–142, January 2011. ISSN 1076-9757.
Qinglong Wang, Kaixuan Zhang, II Ororbia, G Alexander, Xinyu Xing, Xue Liu, and C Lee Giles. An empirical evaluation of recurrent neural network rule extraction. arXiv preprint arXiv:1709.10380, 2017.
Qinglong Wang, Kaixuan Zhang, II Ororbia, G Alexander, Xinyu Xing, Xue Liu, and C Lee Giles. A comparison of rule extraction for diﬀerent recurrent neural network models and grammatical complexity. arXiv preprint arXiv:1801.05420, 2018.
Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. arXiv preprint arXiv:1711.09576, 2017.
Ge Yang, Amy Zhang, Ari S. Morcos, Joelle Pineau, Pieter Abbeel, and Roberto Calandra. Plan2vec: Unsupervised representation learning by latent plans. In Proceedings of The 2nd Annual Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 1–12, 2020. arXiv:2005.03648.
Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, and Arthur Szlam. Composable planning with attributes. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, volume 80, pages 5837–5846. JMLR.org, 2018.
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block MDPs. In International Conference on Machine Learning (ICML), 2020a.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020b.
35

