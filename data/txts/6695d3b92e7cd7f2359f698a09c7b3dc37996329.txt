LAMP: Label Augmented Multimodal Pretraining

Jia Guo Zhejiang University
jiuzhou@zju.edu.cn

Chen Zhu Alibaba Group
none.zc@alibaba-inc.com

Yilun Zhao Zhejiang University
zhaoyilun@zju.edu.cn

Heda Wang Alibaba Group
heda.whd@alibaba-inc.com

Yao Hu Alibaba Group
yaoohu@alibaba-inc.com

Xiaofei He Zhejiang University
xiaofei h@qq.com

Deng Cai Zhejiang University
dengcai78@qq.com

arXiv:2012.04446v1 [cs.MM] 8 Dec 2020

Abstract
Multi-modal representation learning by pretraining has become an increasing interest due to its easy-to-use and potential beneﬁt for various Visual-and-Language (V-L) tasks. However its requirement of large volume and high-quality vision-language pairs highly hinders its values in practice. In this paper, we proposed a novel label-augmented V-L pretraining model, named LAMP, to address this problem. Speciﬁcally, we leveraged auto-generated labels of visual objects to enrich vision-language pairs with ﬁne-grained alignment and correspondingly designed a novel pretraining task. Besides, we also found such label augmentation in second-stage pretraining would further universally beneﬁt various downstream tasks. To evaluate LAMP, we compared it with some state-of-the-art models on four downstream tasks. The quantitative results and analysis have well proven the value of labels in V-L pretraining and the effectiveness of LAMP.

1. introduction
Recently, due to the easy-to-use and strong representation ability of pretrained models, pretrain-then-ﬁnetune learning has been widely adapted in computer vision (CV) and natural language processing (NLP). This naturally draws the attention of a lot of researchers in Vision-andLanguage (V-L) ﬁeld, where most of tasks, such as captioning [3] and visual question answering [2], also highly rely on effective cross-modal representation.
Some pioneering works, such as LXMERT [27] and ViLBERT [19], have shown the potential of pretrain-thenﬁnetune learning for V-L tasks. Most of them followed the intuition of BERT [6] and designed some cross-modal pretraining tasks, including aligning images with the corresponding sentences (i.e. instance level alignment) and aligning objects in images and entities in texts (i.e. objects-

Figure 1: A illustration of miss-alignment problem. Solid boxes are mentioned in MSCOCO, but the dashed ones are not. Many boxes are required in VQA task while not mentioned in captions.
entities level alignment), to bridge the gaps between visual and language.
However, two problems arise during the adaption from BERT to multi-modal representation learning. i. Most of current V-L pretraining datasets (e.g. MSCOCO [3] and Visual Genome (VG) [12]) compose of image-caption pairs. In pretraining, captions are used to predict the objects in images. But captions often cannot provide comprehensive description for images. Figure 1 is an example of imagecaption pair. We can ﬁnd the caption only cover two ob-

1

jects (i.e. “couch”, “chair”) in the image. The lack of textual description of other objects (i.e. pillow) probably distorts objects-entities level alignment in pretraining. ii. The semantic discrepancy of textual input between pretraining tasks and downstream tasks is another obvious problem in the V-L ﬁeld. Also take Figure 1 as an example, “what” and “how” are high-frequent words in the Visual Question Answering (VQA) task, but both of them rarely appear in captions of images. The gaps probably weaken the representation learned by pretrained models.
To address the above problems, we proposed a LabelAugmented Multimodal Pretraining model, named LAMP, where we leverage auto-generated labels of visual objects for better cross-modal alignment. Speciﬁcally, our pretrained model follows the basic architecture of BERT. But different from the previous works, we extract objects-labels pairs by Faster-RCNN to propose a novel objects-entities level alignment pretraining task. So that when pretraining, all of visual objects have the corresponding ﬁne-aligned tokens in lingual side. Besides, we also propose a secondstage pretraining, which serves as a bridge to reduce the discrepancy between pretraining and downstream dataset. The objects-labels pairs are also used to construct the ﬁnegrained alignment.
In general, We proposed a two-stage pretraining method augmented by label textual information. Our contributions can be summarized as:
1. We presented a new V-L pretraining model, named LAMP, where a new object-label alignment task was designed to reduce the distortion, which is caused by the insufﬁcient coverage of textual description for the corresponding image.
2. We proposed a label-augmented second-stage pretraining to reduce the discrepancy between pretraining and downstream tasks.
3. Our proposed model were evaluated in various experiment settings and achieved outstanding performances. Besides, we also conducted a series of ablation experiments to prove our motivations.
2. Related Work
In recent years, pretraining models have made great progress in both computer vision (CV) and natural language processing (NLP) communities. Generally, the design of pretraining paradigm is consisted of two parts: the design of pretraining tasks, and the design of model architecture.
In vision community, most of pretraining models are based on CNN structure and use ImageNet Classiﬁcation [5] as the pretraining task. AlexNet [13] is the pioneer in this ﬁeld. After that, a series of architectures such as VGGNet [23] and ResNet [9] were proposed and achieved impressive results. These models are often used for feature extraction and have proven their values in various down-

(a) Example of objects, captions and labels used in LAMP.
(b) LAMP Model for objects-labels pairs.
(c) LAMP Model for objects-caption pairs.
Figure 2: Overview of the proposed LAMP model. Both image-caption and image-labels are used in pretraining, to provide both ﬁne-grained and coarse description to images. stream tasks, such as object detection [7], instance segmentation [8], semantic segmentation [18].
As for NLP community, most of the pretraining models are based on multi-layer Transformer, such as BERT [6], GPT [21], XLNet [28] and RoBERTa [17]. Among them,

2

BERT is perhaps the most classic and popular one due to its simplicity and outstanding performance. As a pioneering work, BERT proved that self-supervised learning, such as Masked Language Model task, is an effective way for universal representation learning in NLP.
More recently, there has been a surging interest in multimodal representation learning, by pretraining on large-scale image/video and text pairs. VideoBERT [26] is the ﬁrst work to perform BERT-based visual-linguistic pretraining. After VideoBERT, some researchers followed the approach of BERT and proposed some creative multi-modal pretaining model architectures. For example, ViLBert [19] and LXMERT [27] proposed a speciﬁcally designed two-stream architecture. While VisualBERT [15], UNICODER [14], VL-BERT [24] and ImageBERT [20] used a single stream architecture in contrary.
Meanwhile, many powerful tasks for V-L pretraining, including instance level alignment and objects-entities level alignment, were also proposed to bridge the semantic gaps between image and text with image-caption dataset. For example, Visual-linguistic Matching (VLM) [14, 20], an instance level alignment task, is to predict whether an image and a text are matched. Masked Language Modeling (MLM) [26, 15], adapted from BERT, is to predict the category of a masked text token, given the information of visual side. Similarly, Masked Vision Modeling (MVM) [19, 27] is to predict visual objects by the clues of textual side. The common MVM tasks include Masked Object Classiﬁcation (MOC), Masked Object Classiﬁcation with KL-Divergence (MOC-kl), and Masked Objects Regression (MOR). Although their objective functions are different, the intuitions behind them are similar to MLM, which is to predict the masked elements by the clues from other modalities. However, such tasks ignore the fact that the amount of information contained in the two modalities is different. Generally, a caption cannot provide comprehensive description for an image. And we believe this makes MVM pre-training tasks more difﬁcult than MLM. To solve this problem, we propose to leverage the auto-generated labels of visual objects to improve the comprehensiveness of the linguistic information.
3. Approach
In this section, we will ﬁrst explain two motivations of our LAMP model (i.e. label-augmented MVM task and label-augmented second-stage pretraining). Then we will introduce its details and the pretraining process.
We will use objects-caption to denote the input data used in pretraining tasks. Similarly, objects-sentence and objects-labels refer to the data of downstream, and our autogenereted objects-labels dataset respectively in the same way.

3.1. MVM Task Augmented by Label
Multi-modal pretraining is dedicated to construct alignment between two modalities. Such alignment exists in two levels, instance level and objects-entities (ﬁne-grained) level. The latter alignment is achieved by a masked technique, ﬁrst proposed by BERT, that is applied on vision and text respectively. But when coming into multi-modal ﬁeld, such alignment expects that every masked object or entity has its counterpart in the other modality. And this is overlooked in the previous practice.
Depending on the modality of the forecast target, objects-entities level alignment in V-L ﬁeld can be divided into two categories, namely masked language modeling (MLM), predicting textual elements by visual side, and masked visual modeling (MVM), predicting visual elements by the textual side.
A key assumption to guarantee the effectiveness of these approaches is that the information in one modality is enough to predict information in the other modality. But such assumption is not true. As shown in Figure 1, even by human beings, it is almost impossible to predict some visual objects (i.e. pillow, plate) just by the caption. Thus generally, MVM is a harder task due to the insufﬁciency of textual information.
At ﬁrst glance, we can solve this problem by collecting more captions for each image, which depict every visual objects in it. But this hardly works in practice. One reason is due to the labour cost, and the other is related to the model’s representative power. For example, Visual Genome have nearly 50 captions with the above mentioned picture. But current pretraining paradigm won’t allow us to encode such numbers of sentences in pretraining models, especially when these sentences barely have context relationships among each other.
We notice that labels contains rich semantic information of a given image, and shares the same position information with its corresponding objects, which will be beneﬁcial to object-entity alignment.
Thus, We propose to automatically extract labels of visual objects to augment textual information in MVM task. And this process add no extra computation burden, since the previous MVM practice already use labels as supervised target. Another advantage of this one-to-one correspondence between labels and objects is that label texts can use the same position embedding as objects.
Implement Details. Given an image, we use autogenerated labels to construct objects-labels pairs. Labels and objects are generated with a Faster-RCNN pre-trained on Visual Genome dataset[12], which has a total of 1600 category labels and 400 attribute labels. We keep a constant number N objects given one image. For each visual object, we can get two labels, a category label and an attribute label. So we get 2N labels with each image. We tokenize

3

these labels, Then these labels and objects are input into LAMP together to apply the MOC and MOR tasks. In terms of embedding, all these labels use the same word embedding mechanism as captions, and the same spatial embedding of the corresponding visual objects. Please note that during pretraining, we use objects-labels pairs to do MOC and MOR tasks. While objects-caption pairs are used to do MOR, VLM and MLM tasks.
3.2. Label-augmented Second-stage Pretraining
The discrepancy between pretraining and ﬁnetuning is always a problem for pretrain-then-ﬁnetune learning. In VL ﬁeld, the problem becomes even more serious. For example, in VQA dataset “what” and “how” are very common words but both of them are rare in image-caption pairs, which are widely used for pretraining.
Fortunately, second stage pretraining has been proved to be helpful to alleviate this problem in NLP. By a second stage pretraining, the representation knowledge pretrained earlier can adapt to the new distribution of downstream dataset. [4] But when adapted in V-L representation learning, this approach need to be modiﬁed. Because imagecaption pairs are required for MVM training, and questions are not designed on describing the images precisely.
To tackle this problem, we can still rely on the autogenerated objects-labels pairs to enrich the information in the textual side again. We use both objects-labels and objects-sentence pairs in second stage pretraining.
Implement Details. The second stage of pretraining is similar to the ﬁrst stage, that involves training with both objects-sentence pairs and objects-labels paris. On some tasks such as VQA, training directly on objects-sentence pairs can be better than directly ﬁnetuning, while on other tasks such as NLVR2, it will fail. And jointly training on objects-sentence pairs and objects-labels paris will always bring better performance. Due to the unclear correspondence between the text and image of some tasks, we remove VLM pretraining for these tasks. For example, we use VLM in visual question answering task, but not in NLVR2.
3.3. LAMP
In this section, we will ﬁrst review the architecture of our LAMP. Then we introduce its input and pretraining tasks, respectively.
In LAMP, we use BERT-base as our base architecture. BERT is a multi-layer transformer based model proposed for NLP pretraining. With tokenized sub-words as input, BERT is pretrained with MLM(Masked Language Modeling and NSP(Next Sentence Prediction) tasks. For multimodal learning, LAMP modiﬁed BERT from the aspect of input and pretraining tasks.
The input of LAMP consists of visual part and text part. Among them, the text part is also tokenized by the bert to-

kenizer, while the visual part uses a Faster-RCNN to extract features as input. The extracted features contains three parts, visual features, position information and labels. For a given image, we only keep 36 regions due to the limited computation resources.
3.3.1 Input Embedding
Both visual and textual information are represented as vectors, and then be fed into the model.
Text Token Embedding. Following the practice of BERT, we ﬁrst tokenized sentences as text input and embed these text tokens by vectors. And we follow the idea in BERT to use position embedding to embed the position information. Besides we use segment embedding to denote the modality of input, where zero represents text and one represents visual. The ﬁnal representation for each token is the summation of word embedding, position embedding and segment embedding.
Visual Embedding. For each input image, we use a Faster-RCNN to extract a constant number of region features, and their corresponding positions. The position are represented by a 7-d vector, including the bottom-left and top-right corner and height, width of image, and the fraction of image area covered respectively. Then the position vector and the region feature are projected to the same embedding space respectively. We also set segment embedding as mentioned above. Finally, the overall embedding are the summation of feature embedding, position embedding, and segment embedding.
Label Embedding. For label embedding, we use the same word embedding and segment embedding as them in Text Token Embedding. Since these labels are not natural language, it is not appropriate to embed the positions with the setting of Text Token Embedding. Thus we use the spatial embeddings of the corresponding regions.
3.3.2 Pretraining Tasks
In order to learn a good multi-modal representation, we use four pretraining tasks, namely VLM, MLM, MOC and MOR, to align image and text, which can be classiﬁed into two categories (i.e. instance level alignment and objectentity level alignment). Among them, MOC and MOR are enhanced by the objects-labels pairs.
Instance Level Alignment. We selected VLM task to align images and captions.
VLM. We added an additional token [CLS] in our samples to represent the overall representation of objectscaption pairs. Then we fed the output of [CLS] into a fullyconnected layer to predict whether an image and a caption are matched. During pretraining, we sampled 15% of objects-caption pairs as positive samples and generate the

4

same number of negative samples by replacing the caption with another random-selected one.
Object-entity Level Alignment. Here we selected MLM and two MVM tasks (i.e. MOC and MOR) to align visual objects and textual entities.
MLM. In MLM, we followed the practice in standard BERT, masking approximately 15% of sub-words, and asking the model to predict their category given the remaining inputs. Masked sub-words are replaced by a special token [MASK] 80% of the time, or replaced by a random word 10% of the time, the rest of them stay unchanged.
MVM Similar to MLM, we randomly sample 15% of the objects to be masked. 80% of the masked object features is set to zero, 10% are replaced by a random feature, and the remaining 10% remains unchanged. When a object is masked, the model is required to predict its category or its input feature, depending on whether the task is MOC or MOR. We use cross entropy for MOC and MSE for MOR respectively. Please note again, we discard MOC task for objects-caption pairs during pretraining, because the insufﬁcient textual information problem we mentioned before.

3.3.3 Two-stage Pretraining
The pretraining of LAMP consists of two stages. The ﬁrst stage is to pretrain on both objects-caption pairs and objects-labels pairs. We apply MLM, VLM, and MOR for objects-caption pairs, while apply MOR and MOC for objects-labels pairs.
The second stage is to pretrain on task-speciﬁc data. Given a downstream task, we generally follow the idea of the ﬁrst stage to pretrain on the dataset and auto-generated objects-labels pairs of the given downstream task. According to the speciﬁc downstream task, VLM may not be used in second-stage pretraining.
In a word, we use label-augmentation in both the ﬁrst and second stages to improve performance.

4. Experiment

In this section, we will ﬁrst introduce the pretraining data and procedure for pretraining our LAMP model. And we will show our evaluation results on four downstream tasks and compare them with seven state-of-the-art models. A total of two models were obtained which are trained with MSCOCO and MSCOCO+VG respectively. We will share our data and models 1.

4.1. Pretrianing Data

We use two datasets, MSCOCO [16] and Visual Genome (VG) datasets [12], for pretraining our LAMP

1https : / / docs . google . com / document / d /

1FzZIXNO8e2Qj2xB2JOm0uXiwPC3DxWyUjzMQKARghWo

/

edit?usp=sharing

model. We select train and dev sets of MSCOCO and VG for
pretraining to avoid data leakage in downstream tasks. 5K data in MSCOCO dev sets is kept for validation. And for each image, we selected 36 detected regions by their conﬁdence values and used a public Faster-RCNN [22, 1] to generate their labels. Removing the overlapping part of MSCOCO and VG, The ﬁnal pretraining dataset totally contains 175K images with 63M labels distributed in 2000 categories, and 5.9M image-caption pairs.
4.2. Pretraining Procedure
As for the model architecture, we used a Transformer with 12 layers, where each layer had 768 hidden units and 12 self-attention heads. The maximum sequence length of text was set to 20. Parameters were initialized with BERTbase. We trained the objects-caption pairs and objectslabels pairs all with batch size of 256 respectively.
The ﬁrst stage pretraining took 20 epochs while the second one took 10 epochs. We used Adam [11] as the optimizer with initial learning rates of 1e-4. And a linear decay learning rate schedule with warm up was applied. Experiments were conducted on Titan RTXs and GTX 1080Tis, and all experiments can be replicated on at most 2 Titan RTXs each with 24GBs of GPU memory. We also used apex to accelerate training and save GPU memory.
Pretraining on MSCOCO generally takes less than 10 hours on 2 Titan RTXs while task-speciﬁc pretraining and ﬁnetuning will take much less.
4.3. Fine-tuning on Downstream Tasks
We ﬁnetuned our LAMP model on three downstream tasks by transferring the pre-trained model to each target task. To directly test the model and avoid the impact of different ﬁnetune strategies, we additionally conducted a zeroshot task. We will describe the problem, dataset, model modiﬁcations, and training objective for each task below.
Visual Question Answering (VQA). The VQA task is, given a natural image, to select the correct answer for a perceptual-level question. We trained and evaluated on the VQA 2.0 dataset [2], consisting 1.1M questions about MSCOCO images, each with 10 ground truth answers.
We treat the task of VQA as a classiﬁcation problem. On VQA, we built a classiﬁer upon the output representation of [CLS]. And before ﬁnetuning, we did the second-stage pretraining on VGQA, GQA and VQA datasets with MLM, MOC, MOR, VLM and visual question task for 10 epochs with 1-e4 learning rate. Label information is used of the same way as in ﬁrst stage pretraining. Then the pretrained model was ﬁnetuned by 4 epochs with 5e-5 learning rate.
GQA. The task of GQA [10] is the same as VQA, but GQA requires stronger reasoning ability (e.g., spatial understanding and multi-step inference). GQA generated 22M

5

Task
1 Pre-train
2 VQA 3 GQA 4 NLVR 5 ZS IR

Datasets
MS-COCO Visual Genome
VQA 2.0 GQA NLVR
Flickr30K

Image Src.
– – COCO COCO Web Crawled Web Crawled

#Images
123K 108K 204K 85K 214K 1K

#Text
617K 5.39M 1.1M 22M 107K
5K

Metric
– – VQA-score GQA-score Accuracy Recall@1, 5, 10

Table 1: Statistics on the datasets used for pretraining and downstream tasks

Method

VQA test-dev test-std

GQA test

NLVR test-P

ZS Image Retrieval R1 R5 R10

ImageBERT(with LAIT)

–

–

–

–

54.3 79.6 87.5

ImageBERT(w/o LAIT)

–

–

–

–

48.4 76.0 85.2

VisualBERT

70.80 71.01

–

67.0

–

–

–

SOTA VL-BERTLARGE

71.79 72.22

–

–

–

–

–

LXMERTLARGE

72.42 72.54

60.30

74.5

–

–

–

Unicoder-VL

–

–

–

–

48.4 76.0 85.2

ViLBERT

70.55 70.92

–

–

31.9 61.1 72.8

LAMP (MSCOCO)

70.85 71.0

–

74.34

42.5 70.9 80.8

LAMP (MSCOCO+VG)

72.48 72.62

61.05

75.43

51.8 77.4 85.3

Table 2: Results of downstream tasks. UNITER and ImageBERT are compared in the fair settings.

questions about MSCOCO images from ground truth image scene graph for explicitly controlling question quality.
We used the same training procedure as that in VQA. After pretraining with MSCOCO and VG, we continued to use VGQA (Visual Genome Question Answering), GQA, and VQA to do second-stage pretraining augmented by label information, and then ﬁnetuned on GQA. To ﬁnetune LAMP on GQA, we also built a classiﬁer upon the output representation of [CLS].
NLVR2. Each datum in NLVR2 [25] contains two related natural images and one natural language statement. The dataset consists of over 100K examples of English sentences paired with web images. The task of NLVR2 is to predict whether the statement correctly describes these two images or not. Unlike VQA and GQA dataset, all the sentences and images of NLVR2 are not covered in pre-training data.
We used both label information and NLVR2 data to do the second-stage pretraining. We found that in this setting, the second stage failed if we did not use label. To ﬁnetune LAMP on NLVR2, we built a classiﬁer upon the output representation of [CLS] of both sentences corresponding to a image.
Flickr30K. This dataset contains 31,783 images collected from the Flickr website, each with ﬁve captions. We used this dataset to do the zero-shot image retrieval(ZS IR), in which we directly applied the pretrained VLM prediction mechanism to caption-based image retrieval without ﬁne-

tuning. Through this task, our model demonstrates excellent multi-modal representation capabilities without ﬁnetuning. Following ViL-BERT[19], we chose 1000 images and 5000 corresponding captions to perform zero short tasks. We used three evaluation metrics, i.e., R@K (K=1,5,10), where R@K was the percentage of ground-truth matchings occurred in the top K-ranked results.
4.4. Results on Downstream Tasks
In this section, we compare our method with some of the strong performance models proposed recently, LXMERT, VisualBERT, ViLBERT, VLBERT, UNITER, ImageBERT, and UNICODER-VL. These models are almost the state-ofthe-art models in terms of model design, training task combination, and training dataset. And although most of them leverage labels of visual objects as supervised signals, the alignment between the textual information of labels and the visual objects is overlooked. ImageBERT is trained on Conceptual Caption, SBU caption, and an additional caption dataset containing 10M image-caption pairs, called LAIT. We report two results of zero shot image retrieval of ImageBERT trained with and without the LAIT. UNITER has better results with additional pretraining dataset, and we used the results under comparable pretraining and ﬁnetuning settings for fairness.
The overall performances of our LAMP and baselines are shown in Table 2. These downstream tasks can be divided into ﬁnetuning and zero shot settings. Next we will

6

discuss them, respectively. Fine-tuning Tasks. We compare our model with other
state-of-the-art BERT-based multi-modal pretrained models. The results in Table 2 demonstrate the strong performances of LAMP. In VQA and GQA, LAMP with MSCOCO+VG achieved the best performances. In NLVR2, our model got a comparable performance with UNITER, which has the best results right now. AND LAMP trained with MSCOCO only achieve remarkable result on NLVR2 compared with LXMERT and VisualBERT.
Zero Shot Image Retrieval Task. UNITER and ImageBERT achieves better results in this task. However, they use a much larger corpus in pretraining procedure. In speciﬁc, UNITER combines four datasets (Conceptual Captions, SBU Captions, VG and MSCOCO) together to generate a 9.6M training corpus. And ImageBERT aggregates a dataset consisted of more than 10M images. Both UnicoderVL and ImageBERT use over features of 100 objects per image. Considering LAMP just use 36 objects, they demonstrate that more objects can lead to better result. Even so, we still outperform UNICODER. LAMP(MSCOCO) also establishes a great baseline on this task, exceeding ViLBERT over 8% with a relative small pretraining dataset.
5. Analysis
In this section, we will ﬁrst test the effectiveness of label augmentation in pretraining tasks. We will also evaluate the performance of second-stage pretraining on various experimental settings.
5.1. Evaluation of Different Pretrained Models on MSCOCO
The difﬁculty in comparing different pretraining models is that pretraining data and model size are different among them. This prevents us from having a insight of the different pretraining tasks and architecture. We use MSCOCO as the benchmark dataset to compare our model with two representative models, LXMERT and VisualBERT, which are a single-stream model and a two-stream model respectively. In addition, they were all pretrained on MSCOCO and ﬁnetune on two tasks, VQA and NLVR2 respectively. The results are shown in Table 3. The released VisualBERT was just pretrained on MSCOCO so we directly used its published results. But the released LXMERT is also pretrained on much larger data (i.e. VG, VQA, GQA, and VGQA). So we retrained LXMERT on MSCOCO with their own hyperparameters.
We can see with this same setting, LAMP achieved impressive result. VisualBERT also used a second-stage pretraining and got good results on VQA, but it was not as good as us on NLVR2. There is a big gap between the full training version of LXMERT and the MSCOCO version. We speculate that it is related to the size of the dataset and to

Method
LAMP LAMP with 2nd stage pretraining (VQA) VisualBERT LXMERT (MSCOCO) LXMERT (Full Version)

VQA test-dev
70.05 70.85 70.8 68.62 72.42

NLVR test-P
74.34 –
67.0 66.79 74.5

Table 3: Comparison of pretraining models pretrained on MSCOCO dataset.

the speciﬁc visual answering task of LXMERT. It is worth noting that on the NLVR2 task, LAMP with MSCOCO has achieved similar result compared with the full training version of LXMERT.
5.2. Evaluation of Label Augmentation on Pretraining Tasks
To validate the effectiveness of label augmentation in pretraining, We conducted extensive experiments with different combinations of losses.
The results are shown in Table 4. We can ﬁnd that the label augmentation has a stable improvement in all of situations, while MOC generally harms the performance in downstream tasks.

Method
LM + VLM LM + VLM + MOR LM + VLM + MOR + MOC LM + VLM + Label LM + VLM + MOR + Label (ours) LM + VLM + MOR + MOC + Label

VQA dev
66.80 66.91 65.57 67.23 67.50 67.49

NLVR dev
73.35 73.89 71.40 73.70 74.42 73.35

Table 4: Evaluation of pretrained model with different pretraining tasks.

5.3. Evaluation of Label-Augmented Second-stage Pretraining
In this section, we test the effectiveness of labelaugmented second-stage pretraining on VQA and NLVR2. The results are shown in Table 5.
VQA. In VQA task, three different strategies are applied on the ﬁrst stage LAMP pretrained on MSCOCO+VG. We ﬁrst directly ﬁnetune on VQA dataset. The result is already better than several models (VLBERT, VisualBERT, and VilBERT) with this setting. Then we apply the second stage pretraining with and without label augmentation, achieving 71.77 and 71.9 on VQA test-std, without any other data augmentation. The second stage pretraining with label augmentation effectively improves the performance of VQA.
NLVR2. Unlike VQA dataset, a sentence in NLVR2 describes the relationship of two pictures, which make the la-

7

bel augmentation much more important. We only use MLM and MOR with NLVR2 dataset, because the task of VLM conﬂicts with NLVR2 task. Speciﬁcally, we conducted three experiments. First we directly ﬁnetuned on NLVR2 task, achieving 75.74 accuracy. Then we conducted second stage pretraining with and without label augmentation. It turns out that only with label dataset can make second stage pretraining useful.

Method
Without 2nd stage pretraining 2nd stage MLM pretraining 2nd stage pretraining with VQA task

VQA test dev test std

71.35 71.74 71.93

71.53 71.77 71.9

NVLR2 dev
75.74 75.12 76.41

Table 5: Evaluation of second stage pretraining on VQA and NLVR tasks (MSCOCO + VG).
6. Conclusion
In conclusion, to solve the insufﬁciency of textual information and the discrepancy between pretraining tasks and downstream tasks, we leveraged auto-generated labels of visual objects and proposed a label-augmented pretraining model, named LAMP. The extensive experiments well proved the value of labels in V-L pretraining and the effectiveness of LAMP.
References
[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077–6086, 2018. 5
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425– 2433, 2015. 1, 5
[3] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 1
[4] Alexandra Chronopoulou, Christos Baziotis, and Alexandros Potamianos. An embarrassingly simple approach for transfer learning from pretrained language models. arXiv preprint arXiv:1902.10547, 2019. 4
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 2
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1, 2

[7] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014. 2
[8] Bharath Hariharan, Pablo Arbela´ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision, pages 297–312. Springer, 2014. 2
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2
[10] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. arXiv preprint arXiv:1902.09506, 2019. 5
[11] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5
[12] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32–73, 2017. 1, 3, 5
[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. 2
[14] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019. 3
[15] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 3
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. 5
[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 2
[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015. 2
[19] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13–23, 2019. 1, 3, 6
[20] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with

8

large-scale weak-supervised image-text data. arXiv preprint

arXiv:2001.07966, 2020. 3

[21] Alec Radford, Karthik Narasimhan, Tim Sali-

mans, and Ilya Sutskever.

Improving language

understanding by generative pre-training.

URL

https://s3-us-west-2.

amazonaws.

com/openai-

assets/researchcovers/languageunsupervised/language

understanding paper. pdf, 2018. 2

[22] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

Faster r-cnn: Towards real-time object detection with region

proposal networks. In Advances in neural information pro-

cessing systems, pages 91–99, 2015. 5

[23] Karen Simonyan and Andrew Zisserman. Very deep convo-

lutional networks for large-scale image recognition. arXiv

preprint arXiv:1409.1556, 2014. 2

[24] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu

Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-

linguistic representations. arXiv preprint arXiv:1908.08530,

2019. 3

[25] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-

jun Bai, and Yoav Artzi. A corpus for reasoning about

natural language grounded in photographs. arXiv preprint

arXiv:1811.00491, 2018. 6

[26] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and

Cordelia Schmid. Videobert: A joint model for video and

language representation learning. In Proceedings of the IEEE

International Conference on Computer Vision, pages 7464–

7473, 2019. 3

[27] Hao Tan and Mohit Bansal. Lxmert: Learning cross-

modality encoder representations from transformers. arXiv

preprint arXiv:1908.07490, 2019. 1, 3

[28] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,

Russ R Salakhutdinov, and Quoc V Le. Xlnet: General-

ized autoregressive pretraining for language understanding.

In Advances in neural information processing systems, pages

5754–5764, 2019. 2

9

