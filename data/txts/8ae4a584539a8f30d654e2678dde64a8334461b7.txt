Generative Concatenative Nets Jointly Learn to Write and Classify Reviews

arXiv:1511.03683v5 [cs.CL] 7 Apr 2016

Zachary C. Lipton∗, Sharad Vikram †, Julian McAuley ‡ Computer Science and Engineering University of California, San Diego La Jolla, CA 92093, USA
{zlipton,svikram,jmcauley}@cs.ucsd.edu

Abstract
A recommender system’s basic task is to estimate how users will respond to unseen items. This is typically modeled in terms of how a user might rate a product, but here we aim to extend such approaches to model how a user would write about the product. To do so, we design a characterlevel Recurrent Neural Network (RNN) that generates personalized product reviews. The network convincingly learns styles and opinions of nearly 1000 distinct authors, using a large corpus of reviews from BeerAdvocate.com. It also tailors reviews to describe speciﬁc items, categories, and star ratings. Using a simple input replication strategy, the Generative Concatenative Network (GCN) preserves the signal of static auxiliary inputs across wide sequence intervals. Without any additional training, the generative model can classify reviews, identifying the author of the review, the product category, and the sentiment (rating), with remarkable accuracy. Our evaluation shows the GCN captures complex dynamics in text, such as the effect of negation, misspellings, slang, and large vocabularies gracefully absent any machinery explicitly dedicated to the purpose.
1 Introduction
Recommender systems assist users in navigating an unprecedented selection of items, personalizing services to a diverse set of users with distinct individual tastes. Typical approaches surface items that a customer is likely to purchase or rate highly, providing a basic set of primitives for building functioning internet applications. Our goal, however, is to create richer user experiences, not only recom-
∗Author website: http://zacklipton.com †Author website: http://www.sharadvikram.com ‡Author website: http://cseweb.ucsd.edu/∼jmcauley/

mending products but generating personalized descriptive text. Engaged users may wish to know what precisely their impression of an item is expected to be, not simply whether it warrants a thumbs up or thumbs down. Customer reviews help this issue to some extent, but large volumes of reviews are difﬁcult to sift through, especially for users interested in some niche aspect. In this work, we address this problem by building systems to generate personalized reviews. In other words, we aim to build systems that given a user/item combination, generate the review that the user would write, if they reviewed the product. We show that such systems can generate plausible reviews that match the style and opinion of a chosen author. We also show that reviews can be generated speciﬁcally to reﬂect a sentiment (star rating) or broad category (style of beer).
Our work focuses on reviews scraped from BeerAdvocate.com (McAuley and Leskovec, 2013). Among product review datasets, BeerAdvocate is notable for its density and for its structure. The dataset contains thousands of reviewers who have written hundreds of reviews each and thousands of items that have received hundreds of reviews each. Notably, reviews exhibit consistent large-scale structure, discussing ﬁve attributes of each beer (appearance, smell, taste, mouthfeel, and drinkability) in sequence. BeerAdvocate users exhibit idiosyncratic writing styles, adhering to consistent protocols (hyphenation, colons, line-breaks) for delineating the various aspects of reviews. Naturally, the bulk of the variance between users’ reviews of a particular beer owes to their own subjective opinions and tastes, something which a successful model must take into account.
Review data also poses subtler challenges. For example, proper nouns are prominent, contributing to an enormous vocabulary, and punctuation is essential to the style of reviews. These are aspects which might be ignored or pose computational challenges for word-level models.
Character-level Recurrent Neural Networks (RNNs) have a remarkable ability to generate coherent text (Sutskever et al., 2011), appearing to hallucinate passages that plausibly resemble a training corpus. In contrast to word-level

Poured from 12oz bottle into half-liter Pilsner Urquell branded pilsner glass. Appearance: Pours a cloudy goldenorange color with a small, quickly dissipating white head that leaves a bit of lace behind. Smell: Smells HEAVILY of citrus. By heavily, I mean that this smells like kitchen cleaner with added wheat. Taste: Tastes heavily of citrus- lemon, lime, and orange with a hint of wheat at the end. Mouthfeel: Thin, with a bit too much carbonation. Refreshing. Drinkability: If I wanted lemonade, then I would have bought that.
(a) Real Review

Poured from a 12oz bottle into a 16oz Samuel Adams Perfect Pint glass. Appearance: Very pale golden color with a thin, white head that leaves little lacing. Smell: Very mild and inoffensive aromas of citrus. Taste: Starts with the same tastes of the citrus and fruit ﬂavors of orange and lemon and the orange taste is all there. There is a little bit of wheat that is pretty weak, but it is sort of harsh (in a good way) and ends with a slightly bitter aftertaste. Mouthfeel: Light body with a little alcohol burn. Finish is slightly dry with some lingering spice. Drinkability: A decent beer, but not great. I don’t think I would rate this anytime soon as it says that there are other Belgian beers out there, but this is a good choice for a warm day when it’s always available in the North Coast Brewing Company party.
(b) Synthetic review

Figure 1: Real and synthetic reviews by user Mosstrooper describing the beer Shock Top Belgian White. Note that this user and item combination never occurs in the training set. The GCN captures Mosstrooper’s idiosyncracies, including his/her tendency to mark each aspect of the beer with fully spelled-out headings (Appearance, Smell, etc., in a particular order). The GCN also accurately predicts that Mosstrooper will comment on the beer’s golden color, citrus ﬂavor, and hints of wheat (highlighted in color).

models, they do not suffer from computational costs that scale with the size of the input or output vocabularies. Character-level LSTMs powerfully demonstrate the ability of RNNs to model sequences on multiple scales simultaneously. They learn to form words, to form sentences, and to generate paragraphs of appropriate length. To our knowledge, all previous character-level generative models are unsupervised.1 However, our goal is to generate character-level text in a supervised fashion, conditioning upon auxiliary input such as an item’s rating or category.2 Such conditioning of sequential output has been performed successfully with word-level models, for tasks including machine translation (Sutskever et al., 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al., 2014). However, despite the aforementioned virtues of character-level models, no prior work, to our knowledge, has successfully trained them in such a supervised fashion.
Most supervised approaches to word-level generative text models follow the encoder-decoder approach popularized by Sutskever et al. (2014). Some auxiliary input, which might be a sentence or an image, is encoded by an encoder model as a ﬁxed-length vector. This vector becomes the initial input to a decoder model, which then outputs at
1Perhaps excepting a manuscript by Ling et al. (2015), which describes a machine translation model with a character-level decoder.
2We use auxiliary input to differentiate the “context” input from the character representation passed in at each sequence step. By supervised, we mean the output sequence depends upon some auxiliary input.

each sequence step a probability distribution predicting the next word. During training, weights are updated to give high likelihood to the sequences encountered in the training data. When generating output, words are sampled from each predicted distribution and passed as input at the subsequent sequence step. This approach successfully produces coherent and relevant sentences, but quality deteriorates as target sequence length exceeds 35 (Sutskever et al., 2014).
To model longer passages of text (such as reviews), and to do so at the character level, we must produce sequences with hundreds or thousands of elements, longer than seems practically trainable with an encoder-decoder approach. Attention mechanisms are popular methods for overcoming the limitations of encoder-decoder architectures (Bahdanau et al., 2014). These approaches have two primary aspects. First, the input is revisited at each decoding step. Second, a mechanism is applied to determine which part of the input to focus on at each step. This method has been used both to perform machine translation (Bahdanau et al., 2014) and to caption images (Xu et al., 2015). To overcome the challenges that we face in this work, we need only the persistence conferred by input replication. Attention is not required, as our auxiliary information consists of one-hot representations. Thus we present a stripped down model, which we term the Generative Concatenative Network (GCN). At each sequence step t, we concatenate the auxiliary input vector xaux with the character representation x(cth)ar , using the resulting vector x (t) to train an otherwise standard generative RNN model. It might seem redundant to replicate xaux at each sequence step, but by

providing it, we eliminate pressure on the model to memorize it. Instead, all computation can focus on modeling the text and its interaction with the auxiliary input.

1.1 Contributions
In this paper we offer the following contributions:
• A simple character-level RNN architecture which effectively generates personalized reviews, conditioned on the author, item, category, or sentiment.
• A demonstration that the generative model can be run in reverse as a classiﬁer, accurately retrieving a review’s author or item. We also show that the same system can classify the sentiment (star rating) of the rating and the category of the item.
• An empirical evaluation of the generative model showing that the GCN achieves signiﬁcantly lower perplexity than a standard RNN language model.

Beyond the methodology we contribute, this work shall help to build novel recommender systems that offer richer personalization than existing approaches. Understanding and summarizing opinion text is a broad topic (see e.g. Hu and Liu (2004)), where the goal is often to design a system that extracts personalized summaries of reviews (or snippets from reviews) that an individual may agree with (Lerman et al., 2009). Our approach attacks this problem more directly, generating text that a speciﬁc user would be likely to write about a speciﬁc product. Potentially, our approach coul lead to more intricately personalized experiences.

Figure 2: Our generative model infers ratings and categories given reviews without any a priori notion of words.
We implement a GCN using an LSTM RNN (Hochreiter and Schmidhuber, 1997), demonstrating its efﬁcacy at both review generation and traditional supervised learning tasks. In generative mode, the GCN produces convincing reviews, tailored to a star rating and category. This GCN can also run in reverse, performing classiﬁcation with surprising accuracy (Figure 2). While the chief purpose of our model is to generate text, we ﬁnd that classiﬁcation accuracy of the reverse model provides an objective way to assess what the model has learned. Further, for the task of author identiﬁcation, it rivals the performance of a state-of-the-art tf-idf ngram logistic regression classiﬁer. An empirical evaluation shows that GCNs can accurately classify previously unseen reviews as positive or negative and determine which of 5 beer categories is being described, despite operating at the character level and not being optimized directly to minimize classiﬁcation error. Our exploratory analysis also reveals that the GCN implicitly learns a large vocabulary and can effectively model nonlinear dynamics, like the effect of negation. Plotting the inferred rating as each character is encountered for many sentences (Figure 2) shows qualitatively that the model infers ratings quickly and anticipates words after ‘reading’ particularly informative characters.

2 Data
We focus on data scraped from BeerAdvocate as originally collected and described by McAuley and Leskovec (2013). BeerAdvocate is a large online review community boasting 1,586,614 reviews of 66,051 distinct items composed by 33,387 users. The reviews employ a large vocabulary, with 241,962 words occurring two times or more and 73,394 words occurring 10 times or more.3 Each review is accompanied by a number of numerical ratings, corresponding to “appearance”, “aroma”, “palate”, “taste”, and also the user’s “overall” impression. The reviews are also annotated with the item’s category.
In addition to the explicitly structured meta-data (ratings, category, item ID and user ID, etc), the reviews themselves conform to unenforced protocols. Typically, each review discusses the appearance, smell, taste, mouthfeel and drinkability of the beer, in precisely this sequence.4 Reviewers frequently demarcate the various sections of the
3 This vocabulary exceeds that of Sutskever et al. (2014), who required eight GPUs to train word level models, four of which were used to perform softmax across the output layer. In contrast, we had access to only two GPUs, and yet our model can train to convergence in several days.
4Mouthfeel refers to the texture of the beer. Drinkability refers to the ease with which the beer can be consumed. Heavier beers

reviews. One popular style is to abbreviate each section, e.g. in the passage: “A: Pours a deep amber color with two ﬁngers of foam, leaving little or no lacing on the glass”, “A:” introduces prose describing the beer’s appearance. While some users follow initials with colons (A:, S:, T:, M:, D:), others use hyphens, e.g., “T- The black cherry is almost completely covered by the alcohol ﬂavor”. Reviewers nearly always adhere to a consistent protocol within a single review. We have also observed that proliﬁc reviewers typically adhere to consistent patterns across the vast majority of their reviews. Thus it is possible, qualitatively, to determine whether generated reviews, conditioned on a speciﬁc author, capture that author’s style of writing.
For our experiments generating reviews conditioned on rating, user ID, and item ID, we select 242k reviews for training and 27k for testing, focusing on the most active users and popular items. We use a standard procedure for selecting dense subgraphs called the k-core. We simply prune the graph iteratively, removing unpopular items and inactive users from the dataset until each remaining user has reviewed at least 190 remaining items and each remaining item has received at least 190 reviews from remaining users. For our experiments focusing on generating reviews conditioned on item category, we select a balanced subset consisting of 150k reviews, 30,000 each from 5 among the top categories, namely “American IPA”, “Russian Imperial Stout”, “American Porter”, “Fruit/Vegetable Beer”, and “American Adjunct Lager”. From both datasets, we hold out 10% of reviews for testing.
3 Recurrent Neural Network Methodology
Recurrent neural networks extend the capabilities of feedforward networks to handle sequential data. Inputs x(1), ..., x(T ) are passed to the network one by one. At each step t, the network updates its hidden state as a function of both the current input and the previous step’s hidden state, outputting a prediction yˆ(t). In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al. (2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994). In short, each memory cell has an input node g, which commonly has a tanh or sigmoid activation function. The activation from the input node ﬂows into each cell’s internal state s, a structure in which activation is preserved along a self-connected recurrent edge. Each cell also contains three sigmoidal gating units for input (i), output (o), and to forget (f ) that respectively determine when to let activation (from g) into the
might be tasty but less drinkable while an otherwise despised beer (typically American Lagers such as Bud Light) could score high points for drinkability, even in a review that likens it to urine.

internal state s, when to pass activation (from s) through to
the rest of the network, and when to ﬂush the cell’s hidden
state. The output of each LSTM layer is another sequence,
allowing us to stack several layers of LSTMs as in Graves (2013). At step t, each LSTM layer h(lt) receives input from the previous layer h(l−t)1 at the same sequence step and the same layer at the previous time step h(lt−1). The recursion ends with h(0t) = x(t) and h(l0) = 0. Formally, for a layer hl the equations to calculate the forward pass through
an LSTM layer are:

g(lt) = φ(Wlgxh(l−t)1 + Wlghhl(t−1) + bgl )

i(t) = σ(W ixh(t) + W ihh(t−1) + bi)

l

l l−1

ll

l

f (t) = σ(W fxh(t) + W fhh(t−1) + bf)

l

l l−1

ll

l

o(t) = σ(W oxh(t) + W ohh(t−1) + bo)

l

l l−1

ll

l

s(lt) = g(lt) il(i) + s(lt−1) f (lt))

h(lt) = φ(s(lt)) o(lt).

Here, σ denotes an element-wise sigmoid function, φ an element-wise tanh, and is an element-wise product. While a thorough treatment of the LSTM is beyond the scope of this paper, we refer to our review of the literature (Lipton et al., 2015) for a gentler unpacking of the material.

3.1 Generative RNNs
Before introducing our contributions, we review the generative RNN model of Sutskever et al. (2011, 2014) on which we build. A generative RNN is trained to predict the next token in a sequence, i.e. yˆt = x(t+1), given all inputs to that point (x1, ..., xt). Thus input and output strings are equivalent but for a one token shift (Figure 3a). The output layer is fully connected with softmax activation, ensuring that outputs specify a distribution. Cross entropy is the loss function during training.
Once trained, the model is run in generative mode by sampling stochastically from the distribution output at each sequence step, given some starting token and state. Passing the sampled output as the subsequent input, we generate another output conditioned on the ﬁrst prediction, and can continue in this manner to produce arbitrarily long sequences. Sampling can be done directly according to softmax outputs, but it is also common to sharpen the distribution by setting a temperature ≤ 1, analogous to the sonamed parameter in a Boltzmann distribution. Applied to text, generative models trained in this fashion produce surprisingly coherent passages that appear to reﬂect the characteristics of the training corpus. They can also be used to continue passages given some starting tokens.

cisely as with the standard generative RNN (Figure 3c). At

training time, xaux is a feature of the training set. At pre-

diction time, we ﬁx some xaux, concatenating it with each character sampled from yˆ(t). One might reasonably note

that this replicated input information is redundant. How-

ever, since it is ﬁxed over the course of the review, we see

no reason to require the model to transmit this signal across

(a)

hundreds of time steps. By replicating xaux at each input,

we free the model to focus on learning the complex interac-

tion between the auxiliary input and language, rather than

memorizing the input.

(b)
(c)
Figure 3: (a) Standard generative RNN; (b) encoderdecoder RNN; (c) concatenated input RNN.
3.2 Generative Concatenative RNNs
Our goal is to generate text in a supervised fashion, conditioned on an auxiliary input xaux. This has been done at the word-level with encoder-decoder models (Figure 3b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014). Such models have successfully produced (short) image captions, but seem impractical for generating full reviews at the character level because signal from xaux must survive for hundreds of sequence steps. We take inspiration from an analogy to human text generation. Consider that given a topic and told to speak at length, a human might be apt to meander and ramble. But given a subject to stare at, it is far easier to remain focused. The value of re-iterating high-level material is borne out in one study, Surber and Schroeder (2007), which showed that repetitive subject headings in textbooks resulted in faster learning, less rereading and more accurate answers to highlevel questions. Thus we propose the generative concatenative network, a simple architecture in which input xaux is concatenated with the character representation x(cth)ar. Given this new input x (t) = [x(cth)ar; xaux] we can train the model pre-

3.3 Weight Transplantation
Models with even modestly sized auxiliary input representations are considerably harder to train than a typical unsupervised character model. To overcome this problem, we ﬁrst train a character model to convergence. Then we transplant these weights into a concatenated input model, initializing the extra weights (between the input layer and the ﬁrst hidden layer) to zero. Zero initialization is not problematic here because symmetry in the hidden layers is already broken. Thus we guarantee that the model will achieve a strictly lower loss than a character model, saving (days of) repeated training. This scheme bears some resemblance to the pre-training common in the computer vision community (Yosinski et al., 2014). Here, instead of new output weights, we train new input weights.

3.4 Running the Model in Reverse

Many common document classiﬁcation models, like tf-idf logistic regression, maximize the likelihood of the training labels given the text. Given our generative model, we can produce a predictor by reversing the order of inference, that is, by maximizing the likelihood of the text, given a classiﬁcation. The relationship between these two tasks (P (xaux|Review) and P (Review|xaux)) follows from Bayes’ rule. That is, our model predicts the conditional probability P (Review|xaux) of an entire review given some xaux (such as a star rating). The normalizing term can be disregarded in determining the most probable rating and when the classes are balanced, as they are in our test cases, the prior also vanishes from the decision rule leaving P (xaux|Review) ∝ P (Review|xaux).
In the speciﬁc case of author identiﬁcation, we are solving the problem

argmax P (Review|u) · P (u)
u∈users

T

= argmax

P (yt|u, y1, ..., yt−1) · P (u)

u

t=1

T

= argmax

log(P (yt|u, y1, ..., yt−1)) + log (P (u))

u

t=1

where yt is the tth character in the review, and T is the last character’s index. In practice, we must do these calculations in log space to avoid losing precision. The joint probability (over all characters) of any review is extremely small, but the relative values of these probabilities is nevertheless informative. This method allows us to perform classiﬁcation without any additional training, and provides an intuitive way to ask, “what does the generative model know?” Our experiments show, surprisingly, that for the task of author identiﬁcation, this approach rivals traditional state-of-the-art classiﬁers. It has the beneﬁt that it can make a prediction given a fragment of a document, however short. For example, the GCN can be used identify users given the ﬁrst 100 characters of a review without any modiﬁcations to the model. In contrast, an n-gram logistic regression model would require retraining. However, one drawback to the model might be the inefﬁciency of separately running each user through the network, to determine which makes the review most likely.
4 Experiments
All experiments are executed with a custom recurrent neural network library written in Python, using Theano (Bergstra et al.) for GPU acceleration. Our networks use 2 hidden layers with 1024 LSTM cells per layer. During training, examples are processed in mini-batches and we update weights with RMSprop (Tieleman and Hinton, 2012). To assemble batches, we concatenate all reviews in the training set together, delimiting them with (<STR>) and (<EOS>) tokens. We split this string into minibatches of size 256 and again split each mini-batch into segments with sequence length 200. Furthermore, LSTM state is preserved across batches during training. To combat exploding gradients, we clip the elements of each gradient at ± 5. We found that we could speed up training by ﬁrst training an unsupervised character-level generative RNN to convergence. We then transplant weights from the unsupervised net to initialize the GCN. We implement two GCNs in this fashion, one using the star rating scaled to [-1, 1] as xaux, and a second using a one-hot encoding of 5 beer categories as xaux.
4.1 Generating Text
First, we evaluate the reviews generated by the GCN. Conditioning on authors (Figure 1) and items, we produce reviews that capture the author’s style. This is best appreciated by evaluating side by side real reviews corresponding to a user-item pair, and synthesized reviews conditioned on the same user-item pair. As demonstrated in Figure 1, the GCN learns the peculiarities of user Mosstrooper’s style, demarcating each section in similar fashion. The GCN also accurately predicts several sentiments that Mosstrooper would express regarding Shock Top Belgian White, such as

its golden color, citrusy ﬂavor, and hints of wheat. Notably, both our qualitative and quantitative analyses show that user information is far more salient than item information for predicting review text.
We similarly evaluate the GCN’s ability to generate reviews conditioned on star ratings and categories. Running the GCN in generative mode and conditioning upon a 5 star rating, we produce a decidedly positive review:
Poured from a 12oz bottle into a pint glass. A: Pours a deep brown color with a thin tan head. The aroma is of coffee, chocolate, and coffee. The taste is of roasted malts, coffee, chocolate, and coffee. The ﬁnish is slightly sweet and smooth with a light bitterness and a light bitterness that lingers on the palate. The ﬁnish is slightly bitter and dry. Mouthfeel is medium bodied with a good amount of carbonation. The alcohol is well hidden. Drinkability is good. I could drink this all day long. I would love to try this one again and again.
Conditioning on the “Fruit / Vegetable Beer” category, the model generates a commensurately botanical review; interestingly the user “Mikeygrootia” does not exist in the dataset.
Thanks to Mikeygrootia for the opportunity to try this one. A: Poured a nice deep copper with a one ﬁnger head that disappears quickly. Some lacing. S: A very strong smelling beer. Some corn and grain, some apple and lemon peel. Taste: A very sweet berry ﬂavor with a little bit of a spice to it. I am not sure what to expect from this beer. This stuff is a good summer beer. I could drink this all day long. Not a bad one for me to recommend this beer.
4.2 Generative Model Quantitative Results
To prove that our generative model makes use of the auxiliary information to produce more contextually likely reviews, we report the test set perplexities for all models (Table 1. As a baseline, we report the perplexity achieved by an unsupervised LSTM language model. We then report the perplexities of GCNs trained with rating, item ID, and user ID, as auxiliary information.
Because perplexity is a brittle measure, with outliers capable of dominating the performance across an entire dataset with unbounded loss, we report both the average perplexity over all test set reviews and the median perplexity over all test set reviews.
Among all models, the GCN using user and item information performed best. User information proved most valu-

Test Set Perplexity

Unsupervised Language Model Rating Item User User-Item

Mean 4.23 2.94 4.48 2.26 2.25

Median 2.22 2.07 2.17 2.03 1.98

Table 1: Perplexity on test set data for unsupervised RNN language model as well as GCNs with rating, category, user, item, user and item info.

able, and item information conferred little additional power to predict the review. In fact, item information ID proved less useful than rating information for explaining the content of reviews. This struck us as surprising, because intuitively, the item gives a decent indication of the rating and fully speciﬁes the beer category. It also suggests which proper nouns are likely to occur in the review. In contrast, we suspected that user information would be more difﬁcult to use but were surprised that our model could capture user’s writing patterns extremely accurately. For nearly 1000 users, the model can generate reviews that clearly capture each author’s writing style despite forging unique reviews (Figure 1).

Figure 4: Probability of each category as each character in the review is encountered. The GCN learns Budweiser is a lager and that stouts and porters are heavy.

4.3 Predicting Sentiment and Category One Character at a Time
In addition to running the GCN to generate output, we take example sentences from unseen reviews and plot the rating which gives the sentence maximum likelihood as each character is encountered (Figure 5). We can also plot the network’s perception of item category, using each category’s prior and the review’s likelihood to infer posterior probabilities after reading each character. These visualizations demonstrate that by the “d” in “Budweiser”, our model recognizes a “lager”. Similarly, reading the “f” in “awful”, the network seems to comprehend that the beer is “awful” and not “awesome” (Figure 5).
To verify that the argmax over many settings of the rating is reasonable, we plot the log likelihood after the ﬁnal character is processed, given by a range of ﬁne-grained values for the rating (1.0, 1.1, etc.). These plots show that the log likelihood tends to be smooth, peaking at an extreme for sentences with unambiguous sentiment, e.g., “Mindblowing experience”, and peaking in the middle when sentiment is ambiguous , e.g., “not the best, not the worst.” We also ﬁnd that the model understands nonlinear dynamics of negation and can handle simple spelling mistakes.

4.4 Classiﬁcation Results
While our motivation is to produce a character-level generative model, running in reverse-fashion as a classiﬁer proved an effective way to objectively gauge what the model knows. To investigate this capability more thoroughly, we compared it to a word-level tf-idf n-gram multinomial logistic regression (LR) model, using the top 10,000 n-grams. For the task of author identiﬁcation, the GCN equals the performance of the ngram tf-idf model (Table 2). However, classifying items proved more difﬁcult. On the task of category prediction, the GCN achieves a classiﬁcation accuracy of 89.9% while LR achieves 93.4% (Table 3). Both models make the majority of their mistakes confusing Russian Imperial Stouts for American Porters, which is not surprising because stouts are a sub-type of porter. If we collapse these two into one category, the RNN achieves 94.7% accuracy while LR achieves 96.5%. While the reverse model does not in this case eclipse a state of the art classiﬁer, it was trained at the character level and was not optimized to minimize classiﬁcation error or with attention to generalization error. In this light, the results appear to warrant a deeper exploration of this capability. We also ran the model in reverse to classify results as positive (≥ 4.0 stars) or negative (≤ 2.0 stars), achieving AUC of .88 on a balanced test set with 1000 examples (Table 4).

Figure 5: Most likely star rating as each letter is encountered. The GCN learns to tilt positive by the ‘c’ in ‘excellent’ and that the ‘f’ in ‘awful’ reveals negative sentiment.
4.5 Learning Nonlinear Dynamics of Negation
By qualitatively evaluating the beliefs of the network about the sentiment (rating) corresponding to various phrases, we can easily show many clear cases where the GCN is able to model the nonlinear dynamics in text. To demonstrate this capacity, we plot the likelihoods of the review conditioned on each setting of the rating for the phrases “this beer is great”, “this beer is not great”, “this beer is bad”, and “this beer is not bad” Figure 6.
5 Related Work
The prospect of capturing meaning in character-level text has long captivated neural network researchers. In the seminal work, “Finding Structure in Time”, Elman (1990) speculated, “one can ask whether the notion ‘word’ (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).” In this work, an ‘Elman RNN’ was trained with 5 input nodes, 5 output nodes, and a single hidden layer of 20 nodes, each of which had a corresponding context unit to predict the next character in a sequence. At each step, the network received a binary encoding (not one-hot) of a character and tried to predict the next character’s binary encoding. Elman plots the error

Predicting User from Review Accuracy AUC Recall@10%

GCN-Character TF-IDF n-gram

.9190 .9133

.9979 .9979

.9700 .9756

Predicting Item from Review Accuracy AUC Recall@10%

GCN-Character TF-IDF n-gram

.1280 .2427

.9620 .9672

.4370 .5974

Table 2: Information retrieval performance for predicting the author of a review and item described.

Predicted F/V Lager Stout Porter IPA

F/V 910 28

7

14 41

Lager 50 927 3

3 17

True Stout 16 1 801 180 2

Porter 22 3 111 856 8

IPA 19 12

4

12 953

(a) GCN

Predicted F/V Lager Stout Porter IPA

F/V 923 36

9

10 22

Lager 16 976 0

1

7

True Stout 9

4 920 65 2

Porter 11 6

90 887 6

IPA 18 13

1

2 966

(b) ngram tf-idf.

Table 3: Beer category classiﬁcation confusion matrices.

of the net character by character, showing that it is typically high at the onset of words, but decreasing as it becomes clear what each word is. While these nets do not possess the size or capabilities of large modern LSTM networks trained on GPUs, this work lays the foundation for much of our research. Subsequently, in 2011, Sutskever et al. (2011) introduced the model of text generation on which we build. In that paper, the authors generate text resembling Wikipedia articles and New York Times articles. They sanity check the model by showing that it can perform a debagging task in which it unscrambles bag-ofwords representations of sentences by determining which unscrambling has the highest likelihood. Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989, 1998). Demonstrating success on both English and Chinese language datasets, their models achieve high accuracy on a number of classiﬁcation tasks. Dai and Le (2015) train a character level LSTM to perform document classiﬁcation, using LSTM RNNs pretrained as either language models

Method GCN
ngram tf-idf

True Label
Negative Positive Negative Positive

Predicted Label Negative Positive

294

206

7

493

459

41

42

458

Table 4: Sentiment (rating) classiﬁcation confusion matrices for GCN (top) and ngram tf-idf (bottom).

(a) “This beer is great.”

(b) “This beer is not great”

5.1 Key Differences and Contributions
RNNs have been used previously to generate text at the character level. And they have been used to generate text in a supervised fashion at the word-level. However, to our knowledge, this is the ﬁrst work to demonstrate that an RNN can generate relevant text at the character level. Further, while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism for the toy problem of unscrambling strings, to our knowledge, this is the ﬁrst paper to use generative model likelihood scores to infer labels, simultaneously learning to generate text and to perform classiﬁcation with high accuracy. Our work is not the ﬁrst to demonstrate a character-level classiﬁer, as Zhang and LeCun (2015) offered such an approach. However, while their model is strictly discriminative, our model’s main purpose is to generate text, a capability missing from their approach. Further, while we present a preliminary exploration of ways that our generative model can be used as a classiﬁer, we do not train it directly to minimize classiﬁcation error or generalization error, rather using the classiﬁer interpretation to validate that the generative model is in fact modeling the auxiliary information meaningfully.

(c) “This beer is bad.”

(d) “This beer is not bad.”

Figure 6: We plot the likelihood given to a review by each rating. The network learns nonlinear dynamics of negation. “Not” reduces the rating when applied to “great” but increases the rating when applied to “bad”.

or sequence-to-sequence auto-encoders.
Related work generating sequences in a supervised fashion generally follow the pattern of Sutskever et al. (2014), which uses a word-level encoder-decoder RNN to map sequences onto sequences. Their system for machine translation demonstrated that a recurrent neural network can compete with state of the art machine translation systems absent any hard-coded notion of language (beyond that of words). Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014). Most similar to our generative model, Bahdanau et al. (2014) introduced the idea of an attention mechanism for the task of machine translation. In this model, the entire input is revisited at each decoding step. The attention mechanism determines which part of the input to focus on at each step. This idea was subsequently revisited in the context of machine translation by Xu et al. (2015), who apply attention to the task of image captioning. While attention has been increasingly well-studied, to our knowledge no papers have simpliﬁed the model, studying generation with replicated inputs independent of the complex machinery of attention.

6 Conclusion
In this work, we demonstrate a character-level recurrent neural network to generate relevant text conditioned on auxiliary input. It is the ﬁrst attempt to generate personalized product reviews, conditioned on speciﬁc users and items with deep learning. It is also the ﬁrst work to use deep learning for generating coherent product reviews conditioned upon rating and category data. The conditionally generated reviews achieve signiﬁcantly lower mean perplexity than those achieved with standard RNN language models.
Our quantitative and qualitative analysis show that the GCN can accurately identify authors. Additionally, the model does a good job of predicting items ratings and categories, nearly matching the performance of purely discriminative models. The classiﬁcation accuracy of the generative model provides a straightforward way to determine what the model knows. Unlike mean perplexity, the classiﬁcation results are less susceptible to outliers. The GCN learns nonlinear dynamics of negation, and appears to respond intelligently to a large vocabulary despite lacking any a priori notion of words.
While the capability of the generative model to perform classiﬁcation is intriguing, more work can be done to tune this approach. One problem with the current approach is that inference becomes slow as the number of classes becomes large. In the case of author identiﬁcation, we must run each review through the network roughly 1k times to obtain a likelihood of the review separately conditioned

upon each of the roughly 1k authors. At scale, this might not be unacceptable. How best to perform this form of inference with a deep net (recovering most probable inputs given outputs) remains an interesting and open research question.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2): 157–166, 1994.
James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A cpu and gpu math compiler in python.
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3061–3069, 2015.
Jeffrey L. Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.
Felix A. Gers, Ju¨rgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. Neural computation, 12(10):2451–2471, 2000.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In KDD, 2004.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306, 2014.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4): 541–551, 1989.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278– 2324, 1998.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. Sentiment summarization: Evaluating and learning user preferences. In ACL, 2009.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W Black. Character-based neural machine translation. arXiv preprint arXiv:1511.04586, 2015.
Zachary C. Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019, 2015.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-RNN). arXiv preprint arXiv:1412.6632, 2014.
Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In Proceedings of the 22nd international conference on World Wide Web, pages 897– 908. International World Wide Web Conferences Steering Committee, 2013.
John R. Surber and Mark Schroeder. Effect of Prior Domain Knowledge and Headings on Processing of Informative Text. Contemporary Educational Psychology, 32(3):485–498, jul 2007. ISSN 0361476X. doi: 10.1016/j.cedpsych.2006.08.002. URL http://www.sciencedirect.com/science/ article/pii/S0361476X06000348.
Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104– 3112, 2014.
Tijmen Tieleman and Geoffrey E. Hinton. Lecture 6.5RMSprop: Divide the gradient by a running average of its recent magnitude. https://www.youtube. com/watch?v=LGA-gRkLEsI, 2012.
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156– 3164, 2015.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pages 3320–3328, 2014.
Xiang Zhang and Yann LeCun. Text understanding from scratch. arXiv preprint arXiv:1502.01710, 2015.

