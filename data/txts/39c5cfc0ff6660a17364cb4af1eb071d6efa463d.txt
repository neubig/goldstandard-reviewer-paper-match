When is it Better to Compare than to Score?

arXiv:1406.6618v1 [stat.ML] 25 Jun 2014

Nihar B. Shah Abhay Parekh

Sivaraman Balakrishnan

Joseph Bradley

Kannan Ramchandran Martin Wainwright

University of California, Berkeley

Abstract

When eliciting judgements from humans for an unknown quantity, one often has the choice

of making direct-scoring (cardinal) or comparative (ordinal) measurements. In this paper

we study the relative merits of either choice, providing empirical and theoretical guidelines

for the selection of a measurement scheme. We provide empirical evidence based on ex-

periments on Amazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)

ordinal measurements have lower per sample noise and are typically faster to elicit than

cardinal ones. Ordinal measurements however typically provide less information. We then

consider the popular Thurstone and Bradley-Terry-Luce (BTL) models for ordinal mea-

surements and characterize the minimax error rates for estimating the unknown quantity.

We compare these minimax error rates to those under cardinal measurement models and

quantify for what noise levels ordinal measurements are better. Finally, we revisit the data

collected from our experiments and show that ﬁtting these models conﬁrms this prediction:

for tasks where the noise in ordinal measurements is sufﬁciently low, the ordinal approach

results in smaller errors in the estimation. What%is%the%distance%between%% How%many%words%are%misspelled% Who%do%you%think%is%OLDER?%

1 Introduction

the%following%pairs%of%ci4es?%
%
San$Francisco$and$Aus.n%%%

in%this%paragraph?%%
But that is the beginning of a new story - the story of the gradual reneual of a man, the

Eliciting judgements or knowledge about unknmoiwlens% quantitiesstofrryoomf hisngroandu-ael xrepgeenretrathioun,mof ahins spasiisngcommonplace in from one world into another, of his intiation into
many domains of society today. This has been facilitated by tahneewemunkenrogwnenlifec.eThoatfmsigehvt beertahle snuebjwect ‘crowdsourcing’ platforms such as Amazon Mechanical Turk, that have becomeofpaonwewesrtofruy,lb,ult oouwr p-rcesoensttsttooryoislsenfdoedr. collecting!h%uman !%

knowledge and judgements. However, this low cost comes at the pricewoofrndos%iasree,%mduisesptoeltlheed%unreliability in the

crowd response. This paper addresses this issue of noise at the source by studying how responses should be

elicited.

Which%tone%corresponds%to%a%%

We consider a setting in which %hWuhmicahn%sciprcelerf%iosr%BmIGeGvEaRlu?a%%tions that have numeric answeHrIsG. HEExRa%nmupmlebseri%nocnl%uad%pehaone%keypad?%

crowdsourcing task that involves counting the number of malaria parasites in an image of a blood smear [13],

or a peer-grading task that involves students assigning grades to homeworks submitted by other students [18]. A standard design of such a task takes a cardinal approach where the evaluators directly enter n!um% eric scores !% as the answers. This is illustrated by !th%e example!i%n Figure 1a where the subject is asked to rate the relevance

of an image for the search query ‘Internet’ as a numeric entry between 0 and 100.

How relevant is this image for the search query 'INRaTtEe%RthNisE%tTa'g?line%for%a%
healthcare%plaMorm%
“Simple, fast but sure cure”
/%10%

%Which%image%is%more%relevant% How%relevant%is%this%image%for%% for%the%search%query%‘INTERNET’?% the%search%query%'INTERNET'?%

/ 100 (a) Cardinal interface

!%

!%

(b) Ordinal interface

/%100%

Figure 1: Examples of cardinal and ordinal interfaces for a task on relevance rating.

1

Alternatively, one could take an ordinal approach, asking the evaluator to compare (or rank order) multiple items. Such an ordinal method is illustrated in Figure 1b where the evaluator is shown a pair of images, and is asked to select the one that is more relevant for the search query ‘Internet’. In this paper, we restrict our attention to comparisons of only pairs of items in the ordinal setting.
Cardinal measurements allow for more precise measurements; in Figure 1, one cardinal measurement can take 100 values, whereas one ordinal measurement provides a single bit. One may be tempted to go even further and argue that ordinal measurements necessarily give less information, for one can always convert a set of cardinal measurements into ordinal, simply by ordering the measurements by value. The data processing inequality [5, Section 2.8] then suggests that an estimation procedure on any manipulation of the data cannot perform better than estimating from the original data. This may lead one to conclude that the ordinal data cannot yield superior results.
In contrast, ordinal measurements avoid calibration issues that are frequently encountered in cardinal measurements [23], such as the evaluators’ inherent (and possibly time-varying) biases, or tendencies to give inﬂated or conservative evaluations. Ordinal measurements are also recognized to be easier or faster for humans to make [2, 20], allowing for more evaluations for the same level of time, effort and perhaps cost as well.
The lack of clarity regarding when to use a cardinal versus an ordinal approach forms the motivation for this paper. We ﬁrst address the fundamental question of how much information we gain from each type of measurement. In extensive experiments on a variety of tasks, we ﬁnd that the average per-sample noise is often signiﬁcantly higher in cardinal measurements than in ordinal ones. In other words, the data processing inequality does not apply when comparing cardinal and ordinal work from humans.
While revealing, this still leaves two questions: Can we still make reliable estimates from paired comparisons? How much lower does the noise have to be for comparative measurements to be preferred over cardinal measurements? To address this, we invoke theoretical models for pairwise and cardinal measurements. We study the Thurstone (Case V) model [22], one of the most widely used models in both theory [4, 9] and practice [7,19,21]. We will show that it is indeed possible to perform estimation using pairwise comparisons, and via minimax theory we will quantify the settings in which pairwise comparisons are preferable to cardinal measurements. Minimax theory is a cornerstone of statistical decision theory and is a standard tool used in the comparison of estimators in a given model. In this paper, we will investigate the utility of this statistical perspective in comparing estimators across cardinal and ordinal models.
We also provide topology-aware bounds that incorporate the choice of pairs to be compared for the Thurstone and other popular pairwise-comparison models. Of particular importance is the popular Bradley-Terry-Luce (BTL) model [3, 12]. These bounds highlight the inﬂuence of the comparison graph on the estimation error.
Finally, we return to the data obtained from our experiments and ﬁt our ordinal and cardinal models. We observe that the estimates produced from the ordinal data are more accurate than those from cardinal data when the ordinal noise is low enough. This suggests the following practical guideline in choosing between the cardinal and ordinal methods of data collection, of ﬁrst estimating the noise in the two approaches by eliciting a few samples where the ground truth is known. The ordinal approach is then preferred if the ordinal noise is “low enough”. For tasks in which the ordinal approach is preferred, our topology-aware results provide guidelines for the selection of items to compare when given a ﬁxed budget.
2 Experiments Comparing Per-sample Noise in Cardinal and Ordinal
It is tempting to argue that a cardinal sample always gives more information than an ordinal sample: given cardinal samples, one can always order them thereby obtaining ordinal values. This argument suggests that an ordinal approach leads to a loss of information, and due to the data-processing inequality, cannot lead to better results. In this section, by means of seven different experiments conducted on Amazon Mechanical Turk (mturk.com), we show that such an argument is ﬂawed. The experiments also provide insights into the per-sample noise in the ordinal and cardinal methods of data collection, which is a metric that the subsequent theory in this paper will also focus on.
Each experiment involved a certain task that was given to 100 human subjects. Each of these subjects was randomly given either the ordinal or the cardinal version of the task. Both versions had the same set of questions, and each question had a numeric answer. In the cardinal version of the task, the subject was
2

Rate%this%tagline%for%a%

%Which%circle%is%BIGGER?%%

How%many%words%are%misspelled% inH%othwis%m%paanrya%gwroarpdhs%?a%r%e%misspelled%

healthcare%pRlaaMteo%rtmhis%%tagline%for%a% Rateh%Rtehaatilset%h%ttcahagisrl%eitna%pgell%iafnoMer%of%aorm%r%a%%

Whi%cW%Wh%Whhciihcicrhihcc%%lhcceii%rrciccsilrleceB%li%esiI%sG%Bi%sBIGG%BIGGEIGEGRRGE??ER%%RB??u%%t%%mthHaistosiwsHHp%toeohmwelwlaebnmd%emygai%inwnaninnytoihn%yrwtgidsh%wosoip%frsaoda%arpsrrenaad%eagmrswrar%eiaagssprstopeharye%?pmlh-leti?hds%es%% pelled%

“Simple, fast bh“ueStaimlstuhphrelceeaa,lrtcfheaucs%rpaterlba”eu%MptolsaruMmroer%mcu%re”

story Bouft tthhaetBisugtitrhnateh%datbuhteagiiislnisn%ntr%phietneagnherboiauesf agg%pilrnaneaowipfnrghsaato?gor%fymr%a-aatnphee,hwt?hse%t%ory - the

“Sim/%p1“lS0ei%m, fpalset,bfaustt/bs%uu1tr0es%ucruerceu”re” !%

!%

story ostBfourhytistohfgastrtohaiesrdyutgharoealfdbrutehaggleienrenngnienreagrudaaoutlifaooalnf ,nraeeonwmfehausnitaso,lrptyhoae-fsthianegman, the from osntsoetroyrwyoBfooshruftilsotdtrghtyihrneaaotdotfgurhaiaaslidsnrteuohggaterelhanerdbeeruren,aagetoilouifnrnaenh,lgiionsoefgfnhinieasotrifpaamatatisiaooinnnnn,g,eoitwnhfethositsorpyas-inthge
frsotmoroynsoeftfroowhrmoiysrldgoornifanedtotuhwaaeonl orelgtdhgreeainrn,dteoourfaahtlinisoornient,htnioaeeftriu,ohnoaisflinphtoaoisf ininagtiamtiaonn,intthoe

/%10/%%10%

!!% %

!%!%

a newafurnonemwknsouoatnnowkenrnynweowowolrinfflduehlin.fiienskT.tnoTghoharawaanttdnommtuhliaieifggelrh,h.rtoetTbfgehbheaeitshnteetimnhrstaeiiuagtbthisijooetuncbbt,ienjoettfochhteissupbajseicntg of a neofwa snetowryst,obryu, tbuotuorurpprreeseennttsstotroyriys eisndeend.ded.

¢ !% ¢ !%

a newfrooufmnaknonoenwwenwslitfooer.rlydT,hbianutttomoaiugnrhotptbrheeestreh,neotfssuhtboisjreyicntistiaetniodnedin.to of a naewnestworyu,nbkunt oowurnprleifsee.nTt hstaotrymisigehntdbeed. the subject

(a)

(b)

owf aownerwowdrossdrt%sodarwsyar,%earoberu%remtdmo%msuii%srsaipsssrrsppeepsee%eemllnlllelteeisdsddtso%p%ryeislleendde%d.

Who%do%you%tWhiWhnokh%odis%od%O%oy%LoyDouu%Et%htRhi?nin%kk%%iiss%OLWDDEEhRRa??%t%%Wish%taht%eWis%%dthhiaset%%adisins%tcahene%c%de%ibsteatwnceee%n% %%

(wc)ords%are%misspelled% WWhhicihc%hto%Wsnoehu%cincohrdr%%eahsupadosi%noad%%cs%l%itpo%ha%a% s%%

Who%do%you%think%is%OLDbEeRt?w% Weehtnhae%tt%%hfbioseel%ltstohwewe%eic%nedig.ni%spe%%ttasahi?rnes% cs%oeef%%%cbciie.4tews?e%% eHnI%G% HHEIRGW%nHuhEmiRcabh%feH%rtrIe%oGoqnnHue%aEe%%cRpno%hfcrroyrene?qes%u%pkeeonynpcdyas?d%%?t%o%a%%

%

%

San$FranthceiSsS%acfanoon$lF$l$aorFanwrnadcinn$iAscgciu%sopcs$aao.ni$nrads%n$%A%oduf$sA%c.uin4s%.%e% ns?%%%% HIGHER%number%on%a%phone%keypad?%

!%

!!!% %%

!!%%

San$Framnicleissc%o% $amnimlde$siAl%euss% .n%%%

!!%% !% !% !% !%

!%

(d) !%

(e)miles%

(f)

!%

!%

Figure 2: Screenshots of the tasks presented to the subjects. For each task, only one version (cardinal or

ordinal) is shown here.

fo%Wr%%Wthhihec%ihsc%ehima%irmacghae%qg%iues%e%mirsyo%m%‘rIeNo%rTreeElR%ervNeaElenTtv’%?a%nHtth%oew%sH%ereoalrwecvh%ra%eqnlute%eivsra%ytnh%'IitNs%%iTism%EtRahgNiseE%%iTfmo'?ra%%%ge%for%%

raenqdufiorerdeatochdipraeicrt,ltyhpersouvbijdeectthhiasdfnot%uWorm%tshhbeieelcer%hcsa%teismta%htWrfheaoceghro%e%tainqch%neihuess%w%w%eismmreheyaoirac%.r‘grhcIeTeNh%s%hr%TihqseeEe%ulmoReebrvNoredyarlEi%ienn‘eTI%atNvr’%l?eeT%vdlEeeRHhtvrhNsaaoiednEow%tTnas%%’er?lpea%rlrHreegtcshoveheewarn%%qnnst%reueted%aemilsrertc%bhyvthehe%a'%rIqniqNsautu%s%TieimesEtrs%hyRttaei%h'gNoIiaNensEn%s%TiTfsmEoi'wn?Rra%e%Ngp%rE.eaiT%rf'os?,r% %%

for%the%search%query%‘INTERNET’?% the%search%query%'INTERNET'?%

We now describe the tasks presented to the subjects in the seven experiments. The tasks were selected to have

broad coverage of several important subjective judgment paradigms such as preference elicitation, knowledge

elicitation, audio and visual perception, and skill !ut%ilization. !%

/%100%

(a) Rating taglines for a product: A product wa!s d% escribed and ten!t%aglines for this product/%w10e0re% shown

(Figure 2a). this product.

The

subject

had

to

rate

eac!h o% f

these taglines !%

in!te% rms

of its !%

originality,

clar/i%t1y0a0n%d relevance /%100%

to

(b) Estimating areas of circles: The task comprised 25 questions. In each question, the subject was shown

a circle in a bounding box (Figure 2b), and the subject was required to identify the fraction of the box’s area

that the circle occupied.

(c) Finding spelling mistakes in text: Eight paragraphs of text were shown, and the subject had to identify

the number of words that were misspelled in each paragraph (Figure 2c).

(d) Estimating age of people from photographs: The subject was shown photographs of ten people (Fig-

ure 2d) and was asked to estimate the ages of the ten people.

(e) Estimating distances between pairs of cities: The subject was shown sixteen pairs of cities (Figure 2e)

and for each pair, the subject had to estimate the distance between them.

(f) Identifying sounds: The subject was presented with ten audio clips, each of which was the sound of a

single key on a piano (which corresponds to a single frequency). The subject had to estimate the frequency

of the sound in each audio clip (Figure 2f).

(g) Rating relevance of the results of a search query: Twenty results for the query ‘Internet’ for an image

search were shown (Figure 1) and the subject had to rate the relevance of these results with respect to the

given query.

Upon obtaining the data from the experiments, we ﬁrst reduced the cardinal data into ordinal form by comparing answers given by the subjects to consecutive questions. For ﬁve of the seven experiments ((b) through (f)), we had access to the “ground truth” solutions, using which we computed the fraction of answers that

Task

Tagline Circle Spelling Age Distance Audio Relevance

Error in Ordinal 29% 6%

40% 13% 17% 20%

22%

Error in Cardinal 31% 18% 46% 17% 46% 31%

27%

Time in Ordinal 251s 98s

144s 31s

84s

66s

105s

Time in Cardinal 342s 181s 525s 70s 305s 134s

185s

Table 1: Comparison of the average amount of error when ordinal data was collected directly vs. when cardinal data was collected and converted to ordinal. Also tabulated is the median time (in seconds) taken to complete a task by a subject in either type of task.

3

were incorrect in the ordinal and the cardinal-converted-to-ordinal data (any tie in the latter case was counted as half an error). For the two remaining experiments ((a) and (g)) for which there is no ground truth, we computed the ‘error’ as the fraction of (ordinal or cardinal-converted-to-ordinal) answers provided by the subjects that disagreed with each other.
The results are tabulated in Table 1 (boldface indicates a better performance). If the data-processing inequality were true, then it would be unlikely for the amount of error in the ordinal setting to be lower than that in the cardinal setting. On the contrary, one can see from Table 1 that converting cardinal data to an ordinal form results in a typically higher (and sometimes signiﬁcantly higher) per-sample error than directly asking for ordinal evaluations. This absence of data-processing inequality may be explained by the argument that the inherent evaluation process in the human subjects is not the same in the cardinal and ordinal cases – humans do not perform an ordinal evaluation by ﬁrst performing cardinal evaluations and then comparing them (this is why it is often found to be easier to compare than score [2, 20]). One can also see from Table 1 that the amount of time required for cardinal evaluations was typically (much) higher than for ordinal evaluations.

3 Theoretical Comparison of Cardinal and Ordinal Measurement Schemes

The experiments in the previous section established that the ‘per-sample noise’ in the cardinal setup is typically larger than that in the ordinal setting. However, each ordinal sample, unlike a cardinal value, can provide just one bit of information. This discrepancy is further complicated by the fact that the multitude of samples collected from multiple workers need to be aggregated in order to produce ﬁnal estimates of the answers. It is thus not clear for a given a problem setting, whether an ordinal or a cardinal method of data collection would yield a superior performance. This section aims at addressing this issue: given that ordinal and cardinal samples have a different nature and amount of noise, which method of data collection will produce a smaller aggregate error?

In this section we focus our attention on the Thurstone (Case V) generative model [22], which is one of the most popular models considered in both theory [4, 9, 16] and practice [7, 19, 21]. This model assumes that every item has a certain numeric quality score, and a comparison of two items is generated via a comparison of the two qualities in the presence of an additive Gaussian noise.

We deﬁne a vector w∗ ∈ Rd of qualities, so item j ∈ [d] has quality wj∗. Under the Thurstone model we
compare pairs of items items. For i ∈ [n] the outcome of the ith comparison is yi(o) ∈ {−1, 1}, where yi(o) is given by

yi(o) = sign(w∗T xi + (io)),

(THURSTONE)

(o) i

is

independent

Gaussian

noise

with

variance

σo2,

and

xi

∈

Rd

is

a

differencing

vector

with

one

entry

+1, one entry −1 and the rest 0. Observe that the ordinal model is identiﬁable only upto a shift in w∗ so we

always assume 1T w∗ = 0.

The cardinal analogue of this model involves a cardinal evaluation of individual items, where for i ∈ [n] the

outcome yi(c) is given by

yi(c) = w∗T ui +

(c) i

(CARDINAL)

where ui in this case is a coordinate vector with one of its entries equal to 1 and remaining entries 0, and

(c) i

is independent Gaussian noise, with a different variance σc2.

In order to build intuition on how to compare these models, in this section we focus on a simple scenario. Subsequently, in Section 4 we consider general settings. Analogous to the ﬁxed design regression setup, we choose the vectors xi a priori. Suppose that n is large enough, and that in the ordinal case we compare each pair n/ d2 times. In the cardinal case suppose that we evaluate the quality of each item n/d times.
To facilitate a comparison between the CARDINAL and THURSTONE models we consider the minimax risk. In each case a vector w induces a distribution Pw from which the observed samples {y1, . . . , yn} are drawn (recall that the vectors xi are ﬁxed). Let P denote the family of induced distributions and W denote the set of allowed vectors w. An estimator wˆ is a (measurable) map from the observed samples to W. For a semi-norm ρ the minimax risk is
Mρn := inf sup E[ρ(wˆ , w)]
wˆ Pw∈P

4

Ordinal noise:

Figure 3: Characterizing the regions of (σc, σo) where cardinal or ordinal methods lead to a lower minimax error under the CARDINAL and THURSTONE models respectively. B is ﬁxed at 1. The bounds for the THURSTONE model are loose when the signal to noise ratio (SNR) is high but relatively tighter at low SNR; the log-log scale of the axes attempts to focus on this low-SNR regime.

Cardinal noise:

where the expectation is taken over the samples {y1, . . . , yn}. The minimax risk characterizes the per-

formance of the best estimator in the metric induced by ρ. In this section we focus on the case when

ρ(wˆ , w) =

wˆ − w

2 2

and

we

denote

the

minimax

risk

as

M2n.

With these preliminaries in place we can attempt to ask a basic question for the simple case of evenly budgeted measurements: Given n samples with noise standard-deviation σc in the cardinal case and σo in the ordinal case, is the expected minimax error in the estimation of d items lower in the cardinal case or the ordinal case? The following theorem provides an answer for many regimes of (σc, σo).

Theorem 1 Suppose that n is large enough and that in the CARDINAL model we observe each coordinate

n/d times. The minimax risk is

M2n(CARDINAL) = dσc2 .

d

n

Suppose that n is large enough and that in the THURSTONE model we observe each pair n/ d2 times. Suppose w∗ ∞ ≤ B, and that B and σo are known. Let Φ denote the standard Gaussian c.d.f., and let

κ := Φ(2B/σo)(1 − Φ(2B/σo)). Then the minimax risk is bounded as

0.0008κ dσo2 ≤ M2n(THURSTONE) ≤ 5 dσo2 .

n

d

κ2 n

In the cardinal case when each coordinate is measured the same number of times, the CARDINAL model reduces to the well-studied normal location model, for which the MLE is known to be the minimax estimator and its risk is straightforward to characterize (see [10] for instance). In the ordinal case the result follows from the general treatment in Section 4. Observe that the THURSTONE minimax bounds depend on w∗ ∞. This is related to the strong convexity parameter of the likelihood in the THURSTONE model which degrades for increasing w∗ ∞. Informally, this is related to the difﬁculty of estimating very small (or very large) probabilities that can arise in the THURSTONE model for large w∗ ∞.
Observe from Theorem 1 that the minimax risks in the cardinal and ordinal settings have the same dependency on d and n. An ordinal approach of collecting data is thus better overall whenever its per-sample error is “low enough”. Figure 3 summarizes the result of Theorem 1.

4 General Bounds and Topology Considerations

In the previous section, we analyzed one paired comparison model, the THURSTONE model. We now provide a more general treatment by considering three models while allowing arbitrary comparisons. In addition to the THURSTONE model we also provide results for its linear and logistic analogues:

where

yi = w∗T xi + i for i ∈ [n],

i are i.i.d. N (0, σ2), and

P(yi = 1|xi, w∗) =

1

1 + exp −wσ∗T xi

for i ∈ [n].

(PAIRED LINEAR) (BTL)

As before, the xi’s are difference vectors, and we assume 1T w∗ = 0. The second model is the popular Bradley-Terry-Luce (BTL) model [3, 12]. The BTL model is also a popular choice for modeling pairwise

5

comparisons [1, 6, 8, 11, 16], especially since it allows for a computationally simple maximum likelihood inference. The parameter σ plays the role of a noise parameter, with a higher value of σ leading to more uncertainty in the comparisons. We will assume, under all the models, that the value of σ is known. We note in passing that each of these models is a special case of generalized linear models (GLMs) [14], and that many of the insights here carry over to this general class. We defer a detailed treatment of GLMs to an extended version.

In this section we will not assume that items are chosen uniformly at random, rather we provide bounds in the

general case when the measurements are ﬁxed a priori. This will highlight the central role of the Laplacian of

the weighted graph of chosen comparisons. The minimax rate for estimating the underlying quality in

2 2

will

depend on the spectral properties of the Laplacian which in turn depends on the topology of the underlying

comparison graph.

In the ordinal models, each measurement is related to a difference of two quality assessments. Observe that the covariance matrix of the measurements is
Σˆ := n1 n xixTi := Ln
i=1

where L is the combinatorial graph Laplacian of the undirected graph with each edge having a weight equal to the number of times its end points are compared. We refer to Σˆ as the standardized Laplacian. The
standardized Laplacian is positive semi-deﬁnite and has at least one zero-eigenvalue corresponding to the all
ones vector. We assume that the graph induced by the comparisons is connected, since it is easy to verify
that without this the model is not identiﬁable. The covariance matrix induces a semi-norm on vectors in Rd, deﬁned as v Σˆ = vT Σˆ v. We denote the Moore-Penrose pseudo inverse of Σˆ by Σˆ †. We ﬁrst focus on the minimax risk of estimating w∗ in the squared semi-norm induced by Σˆ . We denote this as MΣnˆ . Theorem 2 below bounds this minimax risk in each of the three models. To cleanly state our results we make the simplifying assumption that d > 9.

Theorem 2.A (PAIRED LINEAR) The minimax rate is bounded as 0.00013 dnσ2 ≤ MΣnˆ (PAIRED LINEAR) ≤ 0.68 dnσ2 .
Theorem 2.B (THURSTONE) Assume that w∗ ∞ ≤ B (known). Let κ := Φ(2B/σ)(1 − Φ(2B/σ)), and let n ≥ σ02.κ0t3r5(BΣˆ2†) . The minimax rate is bounded as
0.0008κ dnσ2 ≤ MΣnˆ (THURSTONE) ≤ κ52 dnσ2 .

Theorem 2.C (BTL) bounded as

Assume that w∗ ∞ ≤ B (known) and n ≥ 0.04467Bσ22tr(Σˆ †) . 0.001 dnσ2 ≤ MΣnˆ (BTL) ≤ 1.37 e Bσ + e −σB 4 dnσ2 .

The minimax rate is

The upper bound in each case is from an analysis of the maximum likelihood (ML) estimator. The ML

estimator, in all three settings, is the solution to a convex-optimization problem (while this is clear for the

PAIRED LINEAR and BTL models, see for instance [23] for a proof in the THURSTONE case).

Proof Sketch: Lower bound: The lower bounds are based on a combination of information-theoretic

techniques and carefully constructed packings of the parameter set W. Such techniques are standard in

minimax analysis [24]. The main technical difﬁculty is in constructing a packing in the semi-norm induced

by Σˆ . A consequence of Fano’s inequality (see for instance Theorem 2.5 in [24]) is that if we can construct

a packing of vectors {w1, . . . , wM } of vectors in W such that (a) the KL divergence between the induced

distributions is small, i.e. maxij DKL(Pwi Pwj ) ≤ β log M for a sufﬁcient small (universal) constant β,

and (b) minij

wi − wj

2 Σˆ

≥

δ

for

some

parameter

δ,

then

for

a

small

constant

c,

the

minimax

risk

above

is at least cδ. The main effort is in constructing an exponentially large (in d) packing in the Σˆ norm with

sufﬁciently large δ, and bounding the model speciﬁc constants β and c above. The condition on n is used to

ensure that the constituents of the packing satisfy w ∞ ≤ B. We relegate the details to the Appendix.

6

Upper bound: In each case we analyze the maximum likelihood estimator wˆ = arg min1T w=0 (w) where : W → R is the negative log-likelihood under the corresponding model. In the case of the BTL and
THURSTONE models we impose the additional constraint that w ∞ ≤ B.

The optimization problem in each case is convex. The analysis follows along the lines of standard statistical analyses of M-estimators [25]. We proceed by upper and lower bounding the quantity
f (wˆ ) = (wˆ ) − (w∗) − ∇ (w∗), wˆ − w∗
where ∇ (w∗) ∈ Rd is the gradient of the negative log-likelihood.

In particular, an analysis of the strong convexity parameter of the negative log-likelihood provides a lower

bound of the form f (wˆ ) ≥ γ

wˆ − w∗

2 Σˆ

for

an

appropriate

γ.

Since

wˆ

is

the

maximum-likelihood

estimator,

we get (wˆ ) ≤ (w∗). This implies f (wˆ ) ≤ − ∇ (w∗), wˆ − w∗ ≤ wˆ − w∗ Σˆ ∇ (w∗) Σˆ† via Cauchy-

Schwarz under appropriate conditions (recall that Σ only induces a semi-norm). Putting these together we

arrive at the bound, γ wˆ − w∗ Σˆ ≤ ∇ (w∗) Σˆ† . The main model-speciﬁc effort is in analyzing the strong convexity parameter and bounding the Σ†-norm of ∇ (w∗). We defer the details to the Appendix.

A minimax analysis of the BTL model is also provided in Negahban et al. [15]. Although their main focus is the analysis of a random walk based algorithm, they also provide an analysis for the MLE for the case of uniformly randomly chosen xi. Their information theoretic lower bound studies a related but different problem. Their analysis applies only to the speciﬁc sampling schemes considered and show a considerable gap between the MLE and the lower bound. Our analysis however eliminates this discrepancy and shows that MLE is in fact minimax (rate) optimal for MΣnˆ .

To conclude this section, let us develop some consequences of this theorem. Let us focus on upper bounds in the ordinal setting, and consider estimation error in 22. As in the theorem, we assume that the graph induced by the comparisons is connected. Now ignoring model speciﬁc constants we can see that
M2 ≤ dσ2 n nλ2(Σˆ )
where λ2(Σˆ ) is the second smallest eigenvalue of Σˆ . Recall that Σˆ is simply the standardized Laplacian of the comparison graph, and its second eigenvalue is determined by the topology of the chosen comparisons. To understand this we consider three canonical examples, and in each case we assume that the comparison graph is ﬁxed, n is large enough and that the samples are distributed evenly along the ﬁxed graph. It is straightforward to extend this to the case of randomly chosen comparisons from a ﬁxed graph using matrix concentration inequalities (see for instance [17]).

1. Dumbell graph: This is the graph on d vertices, which consists of two cliques of d/2 disjoint sets of

vertices with a single edge between them. Suppose n ≥

d/2 2

+1. Since the unweighted graph has λ2

= O(1)

we get λ2(Σˆ ) = O(1) and the 2 error scales as M2 ≤ ( ) d2/2 dσ2 .

(d2/2)+1

2

n

n

2. Complete graph: Suppose n ≥ d2 . It is easy to verify that since the unweighted complete graph has

λ2 = d, we get λ2(Σˆ ) = d and the 2 error scales as M2 ≤ (d2)σ2 .

(d2)

2

n

n

3. Degree-k expander: The unweighted degree-k expander has λ2 = O(k) and a similar argument as before shows that if n ≥ kd then we get the error scales as M2n ≤ d2nσ2 .

To summarize we see the

2 2

error

scaling

of

d2 σ2 n

for

the

complete

graph

and

the

degree-k

expander.

We

conjecture that this is in fact the best possible scaling. Observe that the degree-k expander requires n ≥ kd

while the complete graph requires n ≥ d2 , so in practical applications at least for small sample sizes, we should prefer a low-degree expander. On the other hand, for the dumbell graph, the error scales as d3σ2/n

indicating that is a bad topology.

5 Inference in the Experimental Data

In this section we return to our experimental data from Section 2. We consider data from the three experiments of identifying number of spelling errors, estimating the distances between cities, and recognizing the

7

frequencies of audio, for which we know the ground truth. For each of the three experiments, we execute 100 iterations of the following procedure. Select ﬁve workers from the cardinal and ﬁve from the ordinal pool of workers who did this experiment, uniformly at random without replacement. (The number ﬁve is inspired by practical systems [18, 26].) Run the maximum-likelihood estimator of the CARDINAL model on the data from the ﬁve workers selected from the cardinal pool, and the maximum-likelihood estimator of the THURSTONE model on the data from the ﬁve workers of the ordinal pool. In particular, the estimator for the ordinal case ﬁrst estimates σ via 3-fold cross-validation, choosing the value that maximizes held-out data log likelihood, and then uses this best ﬁt for the rest of the estimation procedure. Note that unlike Section 2, the cardinal data here is not converted to ordinal.
We evaluated the performance of these two estimators as follows. The true and inferred vectors were ﬁrst scaled to have their maximum elements equal to 1 and minimum elements equal to −1; this mimics the effect of knowing the scaling B via ‘domain knowledge’. The (scaled) inferred vectors in either case were then compared with the (scaled) true vector in terms of two metrics: (i) d1 times the squared 2 distance, and (ii) the Kendall’s tau rank correlation coefﬁcient.
The results of this evaluation are enumerated in Table 2 (boldface indicates a better performance). To put the results in perspective of the rest of the paper, let us also recall the per-sample errors in these experiments from Table 1. Observe that in the experiment of estimating distances, the per-sample error in the cardinal data was signiﬁcantly higher than the ordinal data. This is reﬂected in the results of Table 2 where the estimator on the ordinal data performs much better (in terms of the 2 error) than the estimator on the cardinal data. On the other hand, the task of identifying the number of spelling mistakes involved a per-sample noise that was comparable across the two settings, and hence the estimator on the cardinal data scores over the ordinal one. As one would expect, the ordinal approach outperforms cardinal in terms of the (ordinal) Kendall’s tau coefﬁcient.

Task
Squared 2-distance in Ordinal Squared 2-distance in Cardinal Kendall’s tau coefﬁcient in Ordinal Kendall’s tau coefﬁcient in Cardinal

Spelling 0.358 0.350 0.277 0.129

Distance 0.168 0.330 0.547 0.085

Audio 0.444 0.508 0.513 0.304

Table 2: Evaluation of the inferred solution from the data received from multiple workers.

6 Conclusion
This paper compares cardinal and ordinal approaches to evaluation performed by humans. With an increasing number of systems relying on non-expert human evaluators (e.g., using crowdsourcing), the choice of the evaluation mechanism forms a critical component of these systems. We argue by means of experiments and fundamental theoretical bounds that ordinal data provides a better estimate of the true solution when the per-sample noise is low enough relative to cardinal data, and the threshold for this choice is independent of the number of observations and the number of questions. This suggests a guideline for deciding whether to deploy a cardinal or an ordinal method of data collection: estimate the noise in the data by obtaining a few samples from either method, and then use the bounds on the overall error to determine the better of the two options.
We suggest further research to understand the tradeoffs in cardinal and ordinal measurements. Our theoretical results were based on simple models, but more complex models, such as ones incorporating the abilities of the different human workers, could be more accurate. Other model classes might have different noise thresholds determining when cardinal or ordinal performs best. Also, it would be useful to make in-depth studies of noise in speciﬁc crowdsourcing settings, such as user experience testing and peer grading in classes.
Future research could also improve data collection. For both cardinal and ordinal data, it would be useful to derive methods for adaptively choosing which measurements to take. Our results on topology-aware bounds could potentially be used to improve ordinal evaluation by analyzing the best topologies for choosing pairs of items to compare.
8

References
[1] D. R. Atkinson, B. E. Wampold, S. M. Lowe, L. Matthews, and H.-N. Ahn. Asian American preferences for counselor characteristics: Application of the Bradley-Terry-Luce model to paired comparison data. The Counseling Psychologist, 26(1):101–123, 1998.
[2] W. Barnett. The modern theory of consumer behavior: Ordinal or cardinal? The Quarterly Journal of Austrian Economics, 6(1):41–65, 2003.
[3] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, pages 324–345, 1952.
[4] T. Bramley et al. A rank-ordering method for equating tests by expert judgment. Journal of Applied Measurement, 6(2):202–223, 2005.
[5] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012. [6] S. Heldsinger and S. Humphry. Using the method of pairwise comparison to obtain reliable teacher
assessments. The Australian Educational Researcher, 37(2):1–19, 2010. [7] R. Herbrich, T. Minka, and T. Graepel. Trueskill: A bayesian skill rating system. Advances in Neural
Information Processing Systems, 19:569, 2007. [8] K. J. Koehler and H. Ridpath. An application of a biased version of the Bradley-Terry-Luce model to
professional basketball results. Journal of Mathematical Psychology, 25(3), 1982. [9] P. F. Krabbe. Thurstone scaling as a measurement method to quantify subjective health outcomes.
Medical care, 46(4):357–365, 2008. [10] E. Lehmann and G. Casella. Theory of Point Estimation. Springer Texts in Statistics. 1998. [11] P. J. Loewen, D. Rubenson, and A. Spirling. Testing the power of arguments in referendums: A Bradley–
Terry approach. Electoral Studies, 31(1):212–221, 2012. [12] R. D. Luce. Individual choice behavior, a theoretical analysis. Bull. Amer. Math. Soc. 66 (1960), 259-
260, pages 0002–9904, 1960. [13] M. A. Luengo-Oroz, A. Arranz, and J. Frean. Crowdsourcing malaria parasite quantiﬁcation: an online
game for analyzing images of infected thick blood smears. Journal of medical Internet research, 14(6), 2012. [14] P. McCullagh and J. Nelder. Generalized Linear Models. Monographs on Statistics and Applied Probability. Routledge, Chapman & Hall, Incorporated, 1983. [15] S. Negahban, S. Oh, and D. Shah. Rank centrality: Ranking from pair-wise comparisons. arXiv preprint arXiv:1209.1688, 2014. [16] R. M. Nosofsky. Luce’s choice model and Thurstone’s categorical judgment model compared: Kornbrot’s data revisited. Attention, Perception, & Psychophysics, 37(1):89–91, 1985. [17] R. I. Oliveira. Concentration of the adjacency matrix and of the laplacian in random graphs with independent edges, 2009. [18] C. Piech, J. Huang, Z. Chen, C. Do, A. Ng, and D. Koller. Tuned models of peer assessment in MOOCs. In International Conference on Educational Data Mining, 2013. [19] D. Ross. Arpad Elo and the Elo rating system, 2007. [20] N. Stewart, G. D. Brown, and N. Chater. Absolute identiﬁcation by relative judgment. Psychological review, 112(4):881, 2005. [21] J. Swets. The relative operating characteristic in psychology. Science, 182(4116), 1973. [22] L. L. Thurstone. A law of comparative judgment. Psychological Review, 34(4):273, 1927. [23] K. Tsukida and M. R. Gupta. How to analyze paired comparison data. Technical report, DTIC Document, 2011. [24] A. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. 2008. [25] S. van de Geer. Empirical Processes in M-Estimation. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2009. [26] J. Wang, P. G. Ipeirotis, and F. Provost. Managing crowdsourcing workers. In The 2011 Winter Conference on Business Intelligence, pages 10–12, 2011.
9

Appendix A provides some additional details on the experiments. Appendix B reviews some technical results that are used in our theoretical proofs. Appendix C presents proofs of the theoretical results.

A Additional Details on Experiments

This section presents additional details on the experiments presented in Section 2 and Section 5. We ﬁrst discuss the experiments of Section 2. The data was collected by putting up tasks on Amazon Mechanical Turk (mturk.com). Amazon Mechanical Turk is an online platform for putting up tasks, where any individual or institution can put up tasks and offer certain payments, and anyone can log in and complete the tasks in exchange for some payment that was speciﬁed along with the task. The following are some additional speciﬁcs about the experiments described in Section 2.

• Each experiment comprised of 100 tasks, all comprising the same set of questions but organized in either a cardinal or ordinal format at random.
• A worker was offered 20 cents for any task she completed.
• A worker was allowed to do no more than one task in an experiment.
• Workers were required to answer all the questions in a task.
• Only those workers who had 100 or more approved works prior to this and also had at least 95% approval rate were allowed.
• Workers from any country were allowed to participate, except for the task of estimating distances between cities where only workers from the USA were allowed since all the questions were about American cities.

We now move on to discuss the inference algorithms of Section 5. The inference algorithms in this section operated on the data from three of the experiments. The four remaining experiments were unsuitable for this purpose: the experiments on rating the relevance of search results and rating taglines for a product had no ground truth; the task of identifying the area of circles had each question drawn independently at random from a Beta distribution, and hence no two workers answered the same questions; the comparison graph for the experiment on identifying age from pictures was not a connected graph.

Table 2 in Section 5 presented the average errors across 100 runs of the inference procedure; Table 3 here

tabulates the associated standard deviation of the errors across the 100 runs.

Task

Spelling Distance Audio

Squared 2-distance in Ordinal

0.122 0.070 0.302

Squared 2-distance in Cardinal

0.207 0.076 0.279

Kendall’s tau coefﬁcient in Ordinal 0.244 0.113 0.217

Kendall’s tau coefﬁcient in Cardinal 0.214 0.148 0.239

Table 3: The standard deviation of the errors incurred in the 100 runs of the inference procedure of Section 5. The average of the errors is listed in Table 2.
B Review of some Technical Results
In this section we present some well known information-theoretic results that we use in our proofs. See for instance [3] for proofs of these claims.
B.1 Fano’s inequality: Multiple hypothesis version
Lemma 3 Let X be a random variable with distribution equal to one of r + 1 possible distributions P1, . . . , Pr+1. Furthermore, the Kullback-Leibler divergence between any pair of densities cannot be too large,
DKL(Pi Pj) ≤ β ∀ i = j. Let ψ(X) ∈ {1, . . . , r + 1} be an estimate of the index. Then
β + log 2 suip Pi(ψ(X) = i) ≥ 1 − log r .

10

B.2 Fano’s inequality: Two hypothesis version

Lemma 4 Let X be a random variable with distribution either P0 or P1, and suppose that the KL divergence between f0 and f1 be bounded as
DKL(P0 P1) ≤ α < ∞.
Then for any ψ(X) ∈ {0, 1} we have

1 − α/2

sup Pi(ψ(X) = i) ≥
i∈{0,1}

. 2

B.3 Estimation error

Let P be a family of distributions. Consider a map θ : P → Ω, and let ρ be a semi-norm on Ω. An estimator θˆ is a measurable function θˆ : P → Ω. The following Lemma gives a lower bound on the minimax error in
estimating θ in the metric induced by ρ.

Lemma 5 Let X be a random variable with distribution equal to one of r + 1 possible distributions P1, . . . , Pr+1 such that
DKL(Pi Pj) ≤ β ∀ i = j.

Suppose that

min ρ(θ(Pi), θ(Pj)) ≥ δ
ij

then the minimax estimation error of any estimator is lower bounded as

inf

sup

E[ρ(θˆ, θ(Pi))] ≥ δ

β + log 2 1−

.

θˆ i∈{1,...,r+1}

2

log r

C Proofs

We ﬁrst introduce some notation which will be employed subsequently in the proofs. Observe that L is a positive semi-deﬁnite matrix (recall from Section 4). Let
L = U ΛU T .

Let λ1 > . . . > λd be the eigenvalues of L and assume without loss of generality that ∀i ∈ [d], λi is the (i, i)th entry of Λ. Since the graph topologies are assumed to be connected, we have λi = 0 ∀ i ∈ [d − 1] and λd = 0. The Moore-Penrose pseudoinverse of L is the (d × d) matrix K, and this satisﬁes

K := U Λ˜ U T where λ˜i = λ−i 11{λi = 0}.

Note that K is also positive semideﬁnite, has a rank equal to (d − 1), and T r(LK) = d − 1. Furthermore,

ΛΛ˜

=

Λ˜

1 2

ΛΛ˜

1 2

=

d−1 i=1

eieTi

.

The standardized versions of L and K are Σˆ := n1 L and Σˆ † := nK. Note that Σˆ † is the Moore-Penrose pseudoinverse of Σˆ .

We ﬁrst state two lemmas that we will use to prove our results. Lemma 6 is used to prove lower bounds and Lemma 7 is used to prove upper bounds. The proofs of the two Lemmas are provided at the end of this section.

Lemma 6 For any δ > 0, α ∈ (0, 1), β ∈ (0, 1) with β = log 2+α2log α−α , there exist a set of eβd vectors {w1, . . . , weβd }, each of length d, such that every pair of vectors satisﬁes
αδ2 ≤ (wi − wj)T M (wi − wj) ≤ 4δ2 and every vector in this set also satisﬁes
1T wi = 0 .

11

Lemma 7 Consider any (d × d) positive semideﬁnite matrix L, and any vectors x, y ∈ Rd such that x ⊥ nullspace(L). If K is the Moore-Penrose pseudoinverse of L, then
√ xT y ≤ xT Lx yT Ky .

Proof of Theorem 1 In the cardinal case when each coordinate is measured the same number of times, the CARDINAL model reduces to the well-studied normal location model, for which the MLE is known to be the minimax estimator and its risk is straightforward to characterize (see [2] for instance).

In the ordinal case the result follows from Theorem 2.B, with Σˆ = d(d2−1) dI − 11T , i.e., an appropri-

ately scaled Laplacian of the complete graph. We know that 1T w∗ = 0, and further observe in the proof

of Theorem 2.B that it sufﬁces to consider wˆ such that 1T wˆ = 0. It follows that MΣnˆ (THURSTONE) =

2d M2n(THURSTONE) . The quantity MΣˆ (THURSTONE) is bounded in Theorem 2.B.

d−1

d

n

Proof of Theorem 2.A (PAIRED LINEAR): Lower Bounds: For any w1 and w2, the KL divergence between the distributions of y under w1 and w2 as the true values is

DKL(Pw (y) Pw (y)) = 1 (w1 − w2)T L(w1 − w2) .

1

2

σ2

For any δ > 0, Lemma 6 constructs a packing {w1, . . . , weβd } such that every pair of distinct vectors wi and wj in this packing satisﬁes (with α = 0.15 and β = 0.13)
0.15δ2 ≤ (wi − wj)T M (wi − wj) ≤ 4δ2
and furthermore every vector in this set also satisﬁes
1T wi = 0 .

Given this packing, we have

4δ2

max DKL(Pwi (y)
i,j

Pwj (y)) ≤

σ2

and min(wi − wj)T L(wi − wj) ≥ 0.15δ2 .
i,j

Using Fano’s inequality, we get

(wˆ − w∗)T L(wˆ − w∗) ≥ 0.15 δ2 1 − 4σδ22 + log 2

2

0.13d

Choosing

δ2 = 0.0076σ2d ,

bounding lodg 2 < 0.07 whenever d > 9, and noting that MΣnˆ (THURSTONE) = n1 (wˆ − w∗)T L(wˆ − w∗) ,
we get the desired result.

Upper Bounds: Deﬁne function as

n
(w) = (yi − xTi w)2 .
i=1

Consider the maximum likelihood estimator

wˆ ∈ arg min (w) .
w

12

The solution is not unique (since the objective is invariant to shifting of w), and hence we impose an additional constraint
wˆ ∈ arg min (w) .
w:wT 1=0
This is a loss function for the maximum likelihood estimator (and needs to be minimized). Now, the gradient and Hessian of this loss function is
n
∇ (w) = −2 (yi − xTi w)xi = −2XT
i=1

∇2 (w) = 2L .

The third and higher order derivatives of are zero. Deﬁning ∆ := wˆ − w∗, we have

(w∗ + ∆) − (w∗) − ∇ (w∗), ∆ = 2∆T L∆ .

Also, since wˆ minimizes this loss function, we have (w∗ + ∆) − (w∗) − ∇ (w∗), ∆

≤ − ∇ (w∗), ∆

≤ ∇ (w∗)T K∇ (w∗)

∆T L∆

where the last equation follows from Lemma 7 proved below.

We shall now upper bound the quantity ∇ (w∗)T K∇ (w∗). We have

∇ (w∗)T K∇ (w∗) = =

T XKXT

Λ˜

1 2

U

T

X

T

2 2

.

Now,

Λ˜

1 2

U

T

X

T

∼

N

(0,

Λ˜

1 2

U

LU

T

Λ˜

1 2

)

and

hence

E[

Λ˜

1 2

U

T

X

T

22]

=

tr(Λ˜

1 2

U

T

LU

Λ˜

1 2

)

= d−1.

We will use [1, Proposition 1] which says that for ∼ N (0, σ2I) and any matrix A,

P ( A 22/σ2 > tr(AT A) + 2 tr((AT A)2)t + 2 AT A 2t) ≤ e−t ∀ t ≥ 0

(1)

In

our

setting,

we

have

A

=

Λ˜

1 2

U

T

X

T

and

hence

tr(AT A) = tr(XKXT ) = tr(LK) = d − 1 ,

tr((AT A)2) = tr(XKXT XKXT ) = d − 1 ,

A

2 op

= arg max vT XKXT v = arg max vT V ΣT U T U Λ˜ U T U ΣV v = arg max vT ΣT Λ˜ Σv = 1

v 2=1

v 2=1

v 2=1

where the last equation follows from setting the singular value decomposition of X as U ΣV T and noting that

by deﬁnition of K, we have ΣT Λ˜Σ =

d−1 i=1

eieTi

.

Substituting

these

values,

we

have

P(

Λ˜

1 2

U

T

X

T

22/σ2 > (d − 1) + 2

(d − 1)t + 2t) ≤ e−t

∀t≥0

⇒ P(

Λ˜

1 2

U

T

X

T

2 2

>

2tσ2d)

≤

e−t

∀t≥1

(2)

Putting everything together, we get

√ ∆T L∆ ≤

dσ2t w.p. ≥ 1 − e−t 2

∀t≥1.

13

Squaring both sides and substituting Σˆ = n1 L, we get In terms of the standardized Laplacian, we have

∆T Σˆ ∆ ≤ dσ2t 2n

w.p. ≥ 1 − e−t

∀t≥1.

Finally,

2n E

∆T Σˆ ∆

=

∞
P

dσ2

t=0

2n ∆T Σˆ ∆ > t dσ2

≤ 1 + ∞ e−t = 1 + 1 .

t=1

e

Proof of Theorem 2.B (THURSTONE): Lower Bounds: Let Φ denote the c.d.f. of the standard Gaussian distribution and let φ denote its p.d.f. For any w1 and w2, the KL divergence between the distributions of y under w1 and w2 as the true values is

n T

Φ(w

T 1

x

i

/σ

)

T

1 − Φ(w1T xi/σ)

DKL(Pw1 (y) Pw2 (y)) =

Φ(w1 xi/σ) log Φ(wT x /σ) + (1 − Φ(w1 xi/σ)) log 1 − Φ(wT x /σ)

i=1

2i

2i

n T

T

Φ(w1T xi/σ)

≤

(Φ(w1 xi/σ) − Φ(w2 xi/σ)) Φ(wT x /σ)

i=1

2i

T

T

1 − Φ(w1T xi/σ)

+((1 − Φ(w1 xi/σ)) − (1 − Φ(w2 xi/σ))) 1 − Φ(wT x /σ)

2i

n (Φ(w1T xi/σ) − Φ(w2T xi/σ))2

=

Φ(wT x /σ)(1 − Φ(wT x /σ))

i=1

2i

2i

≤ n (Φ(w1T xi/σ) − Φ(w2T xi/σ))2 Φ(2B/σ)(1 − Φ(2B/σ))
i=1

n
≤

f (0)2

(w1T xi/σ − w2T xi/σ)2 .

Φ(2B/σ)(1 − Φ(2B/σ))

i=1

=

1

(w1 − w2)T L(w1 − w2)

2πσ2Φ(2B/σ)(1 − Φ(2B/σ))

For any δ > 0, Lemma 6 constructs a packing {w1, . . . , weβd } such that every pair of distinct vectors wi and wj in this packing satisﬁes (with α = 0.15 and β = 0.13)
0.15δ2 ≤ (wi − wj)T M (wi − wj) ≤ 4δ2
and furthermore every vector wi in this set also satisﬁes 1T wi = 0 .

Given this packing, we have

4δ2

max DKL(Pwi (y)
i,j

Pwj (y)) ≤

2πΦ(2B/σ)(1 − Φ(2B/σ))σ2

and min(wi − wj)T L(wi − wj) ≥ 0.15δ2 .
i,j

Using Fano’s inequality, we get

 0.15

2πΦ(2B/σ)(41δ−2Φ(2B/σ))σ2 + log 2 

(wˆ − w∗)T L(wˆ − w∗) ≥

δ2 1 −



2

0.13d

Choosing

δ2 = 0.0076σ2d × 2πΦ(2B/σ)(1 − Φ(2B/σ))

14

bounding lodg 2 < 0.07 whenever d > 9, and noting that

MΣnˆ (THURSTONE) = n1 (wˆ − w∗)T L(wˆ − w∗) ,

we get the desired result. The only issue remaining to consider is the bounded assumption of w, and this is veriﬁed below.

w∞

=

δ √

U

Λ˜

1 2

w

(2)

∞

d

δ ≤√

sup

uT

Λ˜

1 2

w

(2)

d u: u 2=1

δ

(Λ˜

1 2

w

(2)

)T

Λ˜

1 2

w

(2)

=√ d

Λ˜

1 2

w(2)

2

δ ≤√

tr(Λ˜ )

(3)

d

δ = √ tr(K)
d

= 0.00555σ2 × 2πΦ(2B/σ)(1 − Φ(2B/σ)) tr(Σˆ †) n
≤ B,

where (3) follows from the fact that w(2) ∈ {−1, 0, 1}d and the ﬁnal equation follows from our assumption relating n and tr(Σˆ †).
Upper Bounds: Deﬁne function as

n
(w) = − 1{yi = 1} log Φ(wT xi/σ) + 1{yi = −1} log(1 − Φ(wT xi/σ)) .
i=1

Consider the maximum likelihood estimator

wˆ ∈ arg min

(w) .

w:1T w=0, w ∞≤B

The gradient and Hessian of this loss function are

−1 n

φ(wT xi/σ)

φ(wT xi/σ)

∇ (w) = σ

1{yi

=

1} Φ(wT

xi/σ)

−

1{yi

=

−1} 1

−

Φ(wT

xi/σ)

xi ,

i=1

and

2 1n

φ(wT xi/σ)2 − Φ(wT xi/σ)φ (wT xi/σ)

∇ (w) = σ2

1{yi = 1}

Φ(wT xi/σ)2

i=1

φ(wT xi/σ)2 + (1 − Φ(wT xi/σ))φ (wT xi/σ)

T

+ 1{yi = −1}

(1 − Φ(wT xi/σ))2

xixi

(4)

respectively. The scalar in the summation is always non-negative (since Φ is log-concave), and hence maximum likelihood inference is a convex optimization problem.

15

Deﬁne

φ(wT xi/σ)2 − Φ(wT xi/σ)φ (wT xi/σ)

c1(σ, B) :=

inf

min

w:1T w=0, w ∞≤B,i∈[n]

Φ(wT xi/σ)2 ,

φ(wT xi/σ)2 + (1 − Φ(wT xi/σ))φ (wT xi/σ)

(1 − Φ(wT xi/σ))2

= inf φ(wT xi/σ)2 + (1 − Φ(wT xi/σ))φ (wT xi/σ)

w:1T w=0, w ∞≤B,i∈[n]

(1 − Φ(wT xi/σ))2

φ(t)2 + (1 − Φ(t))φ (t)

=

inf

t∈[−2B/σ,2B/σ]

(1 − Φ(t))2

φ(t) 2

φ(t)

=

inf

−t

t∈[−2B/σ,2B/σ] 1 − Φ(t)

1 − Φ(t)

 t+

2 t2 + π8

√ t + t2 + 4

≥

 −t

2

2

2t =−
π2

t2 + 4 −

t2 + 8 π

2 t (t2 + 4) − (t2 + π8 )

= π − 2√ 2

28

t +4+ t + π

2

4

t

=

− 2−

π

π

√
2

28

t +4+ t + π

4 ≥ −1.
π

Then for all w in the allowed set and any vector v ∈ Rd, we have

T2

c1(σ, B) n T T

v ∇ (w)v ≥ σ2

v xixi v.

i=1

Deﬁning ∆ := wˆ − w∗, we have (w∗ + ∆) − (w∗) − ∇ (w∗), ∆

≥ ∆T

c1(σ, B) n T

σ2

xixi . ∆

i=1

= c1(σ, B) ∆T L∆ . σ2

Also, since wˆ minimizes this loss function, we have (w∗ + ∆) − (w∗) − ∇ (w∗), ∆

≤ − ∇ (w∗), ∆

≤ ∇ (w∗)T K∇ (w∗) ∆T L∆

where the last equation follows from Lemma 7.

We will now upper bound the quantity ∇ (w∗)T K∇ (w∗). Deﬁne independent random variables {θi}ni=1 as

θi =

φ(wT xi/σ) Φ(wT xi/σ) −φ(wT xi/σ) 1−Φ(wT xi/σ)

w.p. w.p.

Φ(wT xi/σ) 1 − Φ(wT xi/σ)

and let θT = [θ1, · · · , θn]. Then

∇ (w) = −1 XT θ , σ

16

and
∇ (w∗)T K∇ (w∗) = 1 θT XKXT θ σ2
= σ12 Λ˜ 12 U T XT θ 22 .

We will now apply [1, Theorem 2.1] which says that any random vector that is zero-mean and sub-gaussian

with parameter σ, and any matrix A, must satisfy (1). We will now set θ as

and

Λ˜

1 2

U

T

X

T

as

A

in

(1).

To

this end, we see that

E [θ] = 0

and by virtue of each coordinate being being bounded, θ is sub-gaussian with parameter at most c2(B, σ) where c2(B, σ) is deﬁned as

φ(wT xi/σ)

c2(B, σ)

=

sup
w:1T w=0, w

max ≤B i∈[n] Φ(wT xi/σ)(1 − Φ(wT xi/σ))

∞

≤√ 1 . 2πΦ(2B/σ)(1 − Φ(2B/σ))

Substituting these in (1) and following the simpliﬁcations of (2), we get

P

Λ˜

1 2

U

T

X

T

2 2

>

2tc2(B,

σ)2d

≤ e−t

∀t≥1.

Putting everything together, we have

∆T L∆ ≤ c2(B, σ)2 2dσ2t w.p. ≥ 1 − e−t ∀t ≥ 1 . c1(B, σ)2

Substituting the bounds on c1 and c2, and substituting Σˆ = n1 L, we get

∆T Σˆ ∆ ≤

3.66

dσ2t

(Φ(2B/σ)(1 − Φ(2B/σ)))2 n

w.p. ≥ 1 − e−t

∀t ≥ 1 .

Converting this to a bound on E ∆T Σˆ ∆ as done in the ﬁnal step of the proof of Theorem 2.A gives the desired result.

Proof of Theorem 2.C (BTL): For any differencing vector x, let a(x) be the index of the ‘1’ in x and let b(x) be the index of the ‘-1’ in x. Now deﬁne a function Ψ : Rd × {−1, 0, 1}d → R, where the second
argument is always a differencing vector, as

Ψ(w, x) = log exp wa(x) + exp wb(x)

σ

σ

− wa(x) + wb(x) . 2σ

First, consider a single sample with observation y and differencing vector x. We can rewrite the likelihood function of the BTL model as
P (y|w) = exp y (w)T x − Ψ(w, x) . 2σ
Using this form, one can compute that

(w1)T x

1 1−e σ DKL(Pw (y)||Pw (y)) =

(w1 − w2)T x

1

2

2σ

1

+

e (w1)T σ

x

− (w1 − w2)T ∇Ψ(w1, x) + (w1 − w2)T ∇2Ψ(w3, x)(w1 − w2) , (5)

for some w3. One can evaluate that

(w1)T x
1 1−e σ ∇Ψ(w1, x) = 2σ 1 + e (w1σ)T x x

17

and that

∇2Ψ(w3, x) = 1

1

xxT

2σ2

e (w3)T x σ

+ e −(w3)T x σ

+2

≤ 1 xxT . 8σ2

It follows that

DKL(Pw (y)||Pw (y)) ≤ 1 (w1 − w2)T xxT (w1 − w2) .

1

2

8σ2

Aggregating this over all samples, and observing that the distribution of the observation is independent across

samples, we get

DKL(Pw (y)||Pw (y)) ≤ 1 (w1 − w2)T L(w1 − w2) .

1

2

8σ2

For any δ > 0, Lemma 6 constructs a packing {w1, . . . , weβd } such that every pair of distinct vectors wi and wj in this packing satisﬁes (with α = 0.15 and β = 0.13)
0.15δ2 ≤ (wi − wj)T M (wi − wj) ≤ 4δ2

and furthermore every vector in this set also satisﬁes 1T wi = 0 .

Given this packing, we have

δ2

max DKL(Pw1 (y)||Pw2 (y))
i,j

≤

2σ2

and

min(w1 − w2)T L(w1 − w2) ≥ 0.15δ2 .

(6)

i,j

Using Fano’s inequality, we get

(wˆ − w∗)T L(wˆ − w∗) ≥ 0.15 δ2 1 − 2δσ22 + log 2 (7)

2

0.13d

Choosing

δ2 = 0.06σ2d ,

(8)

bounding lodg 2 < 0.07 whenever d > 9, and noting that

MΣnˆ (THURSTONE) = n1 (wˆ − w∗)T L(wˆ − w∗) ,

we get the desired result. The only issue remaining to consider is the boundedness of w, and this is veriﬁed below.

w∞

=

δ √

U

Λ˜

1 2

w

(2)

∞

d

δ ≤√

sup

uT

Λ˜

1 2

w

(2)

d u: u 2=1

δ

(Λ˜

1 2

w

(2)

)T

Λ˜

1 2

w

(2)

=√ d

Λ˜

1 2

w(2)

2

δ ≤√

tr(Λ˜ )

(9)

d

δ = √ tr(K)
d

= 0.04467σ2 tr(Σˆ †) n
≤ B,

18

where (9) follows from the fact that w(2) ∈ {−1, 0, 1}d and the ﬁnal equation follows from our assumption relating n and tr(Σˆ †).

Upper Bounds: Deﬁne function as

n

−yiwT xi

(w) = log 1 + exp

.

i=1 σ

Consider the maximum likelihood estimator

wˆ ∈ arg min

(w) .

w:1T w=0, w ∞≤B

The gradient and Hessian of this loss function are

n

−yiwT xi

1 −yie σ

∇ (w) =

T xi

σ

i=1

1

+

e

−yi w σ

xi

n

−yiwT xi

∇2 (w) = 1 σ2

eσ
−y wT x

2 xixTi .

i

i

i=1 1 + e σ

One can see that the Hessian is positive semi-deﬁnite, making function a convex function.

Then for all w in the allowed set, any observation yi ∈ {−1, 1} and any differencing vector xi, it must be

that

e −yiwT xi σ
1 + e −yiwT xi σ

2≥

1

2.

e

B σ

−B
+e σ

Deﬁning ∆ := wˆ − w∗, we have (w∗ + ∆) − (w∗) − ∇ (w∗), ∆

≥ ∆T ∇2 (w)∆





T

1 nT

≥ ∆ 

2 xixi . ∆

σ2

e

B σ

−B
+e σ

i=1

=

1

∆T L∆ .

σ2 e Bσ + e −σB 2

Also, since wˆ minimizes this loss function, we have (w∗ + ∆) − (w∗) − ∇ (w∗), ∆

≤ − ∇ (w∗), ∆ ≤ ∇ (w∗)T K∇ (w∗) ∆T L∆

where the last equation follows from Lemma 7.

We will now upper bound the quantity ∇ (w∗)T K∇ (w∗). Deﬁne independent random variables {θi}ni=1

as

 wT xi

 −e σ



T

w.p.

1
T

 

w xi

w xi

θi = 1+e σ

1+e σ

−wT xi

 

e

σ



T

w.p.

1
T



−w xi

−w xi

1+e σ

1+e σ

and let θT = [θ1, · · · , θn]. Then

∇ (w) = −1 XT θ , σ

19

and

∇ (w∗)T K∇ (w∗) = 1 θT XKXT θ

(10)

σ2

= σ12 Λ˜ 12 U T XT θ 22 .

We will now apply [1, Theorem 2.1] which says that any random vector that is zero-mean and sub-gaussian

with parameter σ, and any matrix A, must satisfy (1). We will now set θ as

and

Λ˜

1 2

U

T

X

T

as

A

in

(1).

To

this end, we see that

E [θ] = 0

and by virtue of each coordinate being being bounded, θ is sub-gaussian. The sub-gaussianity parameter is

upper bounded by

e −wT xi σ
1 + e −wT xi σ

wT xi

−e σ

−

T

1

+

e

w

xi σ

=1.

Substituting these in (1) and following the simpliﬁcations of (2), we get

P(

Λ˜

1 2

U

T

X

T

2 2

>

2td)

≤

e−t

∀t≥1.

Putting everything together, we have ∆T L∆ ≤ e Bσ + e −σB 2 √2dσ2t

w.p. ≥ 1 − e−t

∀t ≥ 1 .

Squaring and substituting Σˆ = n1 L, we get

∆T Σˆ ∆ ≤

B

−B

eσ +e σ

4 dσ2t n

w.p. ≥ 1 − e−t

∀t ≥ 1 .

Converting this to a bound on E ∆T Σˆ ∆ as done in the ﬁnal step of the proof of Theorem 2.A gives the desired result. Proof of Lemma 6: First construct a set of eβd vectors {w1(1), . . . , we(1β)d }, each belonging to {−1, +1}d−1 such that
min Hamming-distance(wi(1) − wj(1)) ≥ αd .
i=j

The existence of such a set is guaranteed by the Gilbert-Varshamov bound, which guarantees existence of a (binary) code of length (d − 1), minimum Hamming distance αd, and the number of code words at least

2d−1
αd−1 d−1 =0

≥ 2d−1 αd − 1 αd−1 (d − 1)e

=

e(d−1)

log

2+(αd−1)

log(

αd−1 ed−e

)

≥

e

d 2

(log

2+

α

log

(

α e

))

= eβd .

It follows from the construction that for every pair of distinct vectors in this set, 4αd ≤ (wi(1) − wj(1))T (wi(1) − wj(1)) ≤ 4d .

Now construct a second set of eβd vectors {w1(2), . . . , we(2β)d }, each of length d, as

T

T

T

wi(2) = wi(1) 0

∀i .

20

It is easy to see that every pair of distinct vectors in this set satisﬁes

4αd ≤ (wi(2) − wj(2))T (wi(2) − wj(2)) ≤ 4d .

Finally, construct a third set of eβd vectors {w1, . . . , weβd }, each of length d, as wi = √δd U Λ˜ 12 wi(2) ∀i .
For any vector in this set

1T wi = √δd 1T U Λ˜ 12 wi(2) = √δd edeTd Λ˜ 12 wi(2) = 0.

For any pair of vectors in this set,

(wi − wj )T L(wi − wj ) = δd2 (wi(2) − wj(2))T Λ˜ 21 U T LU Λ˜ 12 (wi(2) − wj(2))

=

δ2

(w(2)

−

w(2))T

Λ˜

1 2

ΛΛ˜

1 2

(w(2)

−

w(2))

di

j

i

j

δ2 (2)

(2) T (2)

(2)

= d (wi − wj ) (wi − wj )

where the last step makes use of the fact that the last coordinate of each vector in the set {w1(2), . . . , we(2β)d } is zero. It follows that

4αδ2 ≤ (wi − wj)T L(wi − wj) ≤ 4δ2 .

Proof of Lemma 7: Consider the singular value decompositions L = U ΛU T , K = U Λ˜U T . Let x˜ :=

Λ

1 2

UT

x

and

y˜

:=

Λ˜

1 2

UT

y.

Then

√

√

xT Lx yT Ky = xT U ΛU T x yT U Λ˜ U T y

= x˜ 2 y˜ 2

≥ x˜T y˜

=

xT

U

Λ

1 2

Λ˜

1 2

U

T

y

= xT U U T y (since x ⊥ nullspace(L))

= xT y .

References
[1] D. Hsu, S. M. Kakade, and T. Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electron. Commun. Probab, 17(52):6, 2012.
[2] E. Lehmann and G. Casella. Theory of Point Estimation. Springer Texts in Statistics. 1998. [3] A. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. 2008.

21

