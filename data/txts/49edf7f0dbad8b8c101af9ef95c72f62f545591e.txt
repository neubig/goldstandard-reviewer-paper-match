E icient Correlated Topic Modeling with Topic Embedding

Junxian He∗1,3, Zhiting Hu∗1,2, Taylor Berg-Kirkpatrick1, Ying Huang3, Eric P. Xing1,2
Carnegie Mellon University1 Petuum Inc.2 Shanghai Jiao Tong University3 ∗ Equal contribution {junxianh,zhitingh,tberg,epxing}@cs.cmu.edu , hy941001@sjtu.edu.cn

arXiv:1707.00206v1 [cs.LG] 1 Jul 2017

ABSTRACT
Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables e cient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacri cing modeling quality by providing competitive or superior performance in document classi cation and retrieval.
CCS CONCEPTS
•Computing methodologies → Latent variable models;
KEYWORDS
Correlated topic models; topic embedding; scalability
1 INTRODUCTION
Large ever-growing document collections provide great opportunities, and pose compelling challenges, to infer rich semantic structures underlying the data for data management and utilization. Topic models, particularly the Latent Dirichlet Allocation (LDA) model [6], have been one of the most popular statistical frameworks to identify latent semantics from text corpora. One drawback of LDA derives from the conjugate Dirichlet prior, as it models topic occurrence (almost) independently and fails to capture rich topical correlations (e.g., a document about virus may be likely to also be about disease while unlikely to also be about nance). E ective modeling of the pervasive correlation pa erns is essential for structural topic navigation, improved document representation, and accurate prediction [5, 9, 37]. Correlated Topic Model (CTM) [5] extends LDA using a logistic-normal prior which explicitly models correlation pa erns with a Gaussian covariance matrix.
Despite the enhanced expressiveness and resulting richer representations, practical applications of correlated topic modeling have unfortunately been limited due to high model complexity and
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. KDD’17, August 13–17, 2017, Halifax, NS, Canada. © 2017 ACM. 978-1-4503-4887-4/17/08. . . $15.00 DOI: 10.1145/3097983.3098074

poor scaling on large data. For instance, in CTM, direct modeling of pairwise correlations and the non-conjugacy of logistic-normal prior impose inference complexity of O(K3), where K is the number of latent topics, signi cantly more demanding compared to LDA which scales only linearly. While there has been recent work on improved modeling and inference [2, 9, 35, 36], the model scale has still limited to less than 1000s of latent topics. is stands in stark contrast to recent industrial-scale LDA models which handle millions of topics on billions of documents [8, 42] for capturing longtail semantics and supporting industrial applications [40], yet, such rich extraction task is expected to be be er addressed with more expressive correlation models. It is therefore highly desirable to develop e cient correlated topic models with great representational power and highly scalable inference, for practical deployment.
In this paper, we develop a new model that extracts correlation structures of latent topics, sharing comparable expressiveness with the costly CTM model, while keeping as e cient as the simple LDA. We propose to learn a distributed representation for each latent topic, and characterize correlatedness of two topics through the closeness of respective topic vectors in the embedding space. Compared to previous pairwise correlation modeling, our topic embedding scheme is parsimonious with less parameters to estimate, yet exible to enable richer analysis and visualization. Figure 1 illustrates the correlation pa erns of 10K topics inferred by our model from two million NYTimes news articles, in which we can see clear dependency structures among the large collection of topics and grasp the semantics of the massive text corpus.
We further derive an e cient variational inference procedure combined with a fast sparsity-aware sampler for stochastic tackling of non-conjugacies. Our embedding based correlation modeling enables inference in the low-dimensional vector space, resulting in linear complexity w.r.t topic size as with the lightweight LDA.
is allows us to discover 100s of 1000s of latent topics with their correlations on near 10 million articles, which is several orders of magnitude larger than prior work [5, 9].
Our work di ers from recent research which combines topic models with word embeddings [3, 10, 22, 29] for capturing word dependencies, as we instead focus on modeling dependencies in the latent topic space which exhibit uncertainty and are inferentially more challenging. To the best of our knowledge, this is the rst work to incorporate distributed representation learning with topic correlation modeling, o ering both intuitive geometric interpretation and theoretical Bayesian modeling advantages.
We demonstrate the e cacy of our method through extensive experiments on various large text corpora. Our approach shows greatly improved e ciency over previous correlated topic models, and scales well as with the much simpler LDA. is is achieved without sacri cing the modeling power—the proposed model extracts high-quality topics and correlations, obtaining competitive

casualty equipment
drug Kuwait effort
Iraq report role

deploy country
help expect
military
armor deployment

humanitarian diplomacy
diplomat
outline power far line

plan destruct warn
artillery ally
spill strengthen
infantry

banana
whipped toast
sour cream
cucumber squash
aroma world frozen
culinary cherry
fruit cure pour

scallion
eggplant fillet
garlic clove
pickle mustard
grate remain
onion gentle
dipped flavor nut crunchy

prostate mutate
suspect cancer
diagnose
surgical month
diagnose
doctor asthma

deadly infect
outbreak cough
bacteria
transmission

contaminate syndrome
virus
transmit flu
expect pediatric

Diabetes symptom
disease epidemic

headache research
medicine

antibiotics

cardiac prescribe

fluid patient

chronic kidney

week three point decline
commercial park
demand interest
official soon
higher yield people
market percent

largest justified get billion import Compaq leave
grocery
long quarterly
offset old profit
strong plummet neutral confederal

proxy official unit
bank Citicorp
buckle country organ
private example
follow sent invest
risky skeptic expert

Figure 1: Visualization of 10K correlated topics on the NYTimes news corpus. e point cloud shows the 10K topic embeddings where each point represents a latent topic. Smaller distance indicates stronger correlation. We show four sets of topics which are nearby each other in the embedding space, respectively. Each topic is characterized by the top words according to the word distribution. Edge indicates correlation between topics with strength above some threshold.

or be er performance than CTM in document classi cation and retrieval tasks.
e rest of the paper is organized as follows: section 2 brie y reviews related work; section 3 presents the proposed topic embedding model; section 4 shows extensive experimental ; and section 5 concludes the paper.
2 RELATED WORK
2.1 Correlated Topic Modeling
Topic models represent a document as a mixture of latent topics. Among the most popular topic models is the LDA model [6] which assumes conjugate Dirichlet prior over topic mixing proportions for easier inference. Due to its simplicity and scalability, LDA has extracted broad interest for industrial applications [40, 42]. e Dirichlet prior is however incapable of capturing dependencies between topics. e classic CTM model provides an elegant extension of LDA by replacing the Dirichlet prior with a logistic-normal prior which models pairwise topic correlations with the Gaussian covariance matrix. However, the enriched extraction comes with computational cost. e number of parameters in the covariance matrix grows as square of the number of topics, and parameter estimation for the full-rank matrix can be inaccurate in high-dimensional space. More importantly, frequent matrix inversion operations during inference lead to O(K3) time complexity, which has signi cantly restricted the model and data scales. To address this, Chen et al. [9] derives a scalable Gibbs sampling algorithm based on data augmentation. ough bringing down the inference cost to O(K2) per document, the computation is still too expensive to be practical in real-world massive tasks. Pu hividhya et al. [36] reformulates

the correlation prior with independent factor models for faster inference. However, similar to many other approaches, the problem scale has still limited to thousands of documents and hundreds of topics. In contrast, we aim to scale correlated topic modeling to industrial level deployment by reducing the complexity to the LDA level which is linear to the topic size, while providing as rich extraction as the costly CTM model. We note that recent scalable extensions of LDA such as alias methods [28, 42] are orthogonal to our approach and can be applied in our inference for further speedup. We consider this as our future work.
Another line of topic models organizes latent topics in a hierarchy which also captures topic dependencies. However, the hierarchy structure is either pre-de ned [7, 19, 30] or inferred from data using Bayesian nonparametric methods [4, 11] which are known to be computationally demanding [12, 17]. Our proposed model is
exible without sacri cing scalability.
2.2 Distributed Representation Learning
ere has been a growing interest in distributed representation that learns compact vectors (a.k.a embeddings) for words [27, 33], entities [18, 31] , network nodes [14, 38], and others. e induced vectors are expected to capture semantic relatedness of the target items, and are successfully used in various applications. Compared to most work that induces embeddings for observed units, we learn distributed representations of latent topics which poses unique challenge for inference. Some previous work [25, 26] also induces compact topic manifold for visualizing large document collections. Our work is distinct in that we leverage the learned topic vectors

embedding space
u3

u1

u4

u2 ad

topic weights

⇢ ad ⌘d zdn

⌧
uk
K
↵

u1 u2 u3 u4

wdn

k

Nd

K

D

Figure 2: Graphical model representation. e le part schematically shows our correlation modeling mechanism, where nearby topics tend to have similar (either large or small) weights in a document.

for e cient correlation modeling and account for the uncertainty of correlations.
An emerging line of approaches [3, 10, 22, 29] incorporates word embeddings (either pre-trained or jointly inferred) with conventional topic models for capturing word dependencies and improving topic coherence. Our work di ers since we are interested in the topic level, aiming at capturing topic dependencies with learned topic embeddings.
3 TOPIC EMBEDDING MODEL
is section proposes our topic embedding model for correlated topic modeling. We rst give an overview of our approach, and present the model structure in detail. We then derive an e cient variational algorithm for inference.
3.1 Overview
We aim to develop an expressive topic model that discovers latent topics and underlying correlation structures. Despite this added representational power, we want to keep the model parsimonious and e cient in order to scale to large text data. As discussed above (section 2), CTM captures correlations between topic pairs with a Gaussian covariance matrix, imposing O(K2) parameter size and O(K3) inference cost. In contrast, we adopt a new modeling scheme drawing inspiration from recent work on distributed representations, such as word embeddings [33] which learn low-dimensional word vectors and have shown to be e ective in encoding word semantic relatedness.
We induce continuous distributed representations for latent topics, and, as in word embeddings, expect topics with relevant semantics to be close to each other in the embedding space. e contiguity of the embedding space enables us to capture topical co-occurrence pa erns conveniently—we further embed documents into the same vector space, and characterize document’s topic proportions with its distances to the topics. Smaller distance indicates larger topic weight. By the triangle inequality of distance metric, intuitively, a

Symbol
D, K, V Nd M uk ad ηd
wdn zdn ϕk Ks
Vs

Description
number of documents, latent topics, and vocabulary words number of words in document d embedding dimension of topic and document embedding vector of topic k embedding vector of document d (unnormalized) topic weight vector of document d the nth word in document d the topic assignment of word wdn word distribution of topic k number of non-zero entries of document’s topic proportion number of non-zero entries of topic word distribution
Table 1: Notations used in this paper.

document vector will have similar (either large or small) distances to the vectors of two semantically correlated topics which are themselves nearby each other in the space, and thus tend to assign similar probability mass to the two topics. Figure 2, le part, schematically illustrates the embedding based correlation modeling.
We thus avoid expensive modeling of pairwise topic correlation matrix, and are enabled to perform inference in the low-dimensional embedding space, leading to signi cant reduction in model and inference complexity. We further exploit the intrinsic sparsity of topic occurrence, and develop stochastic variational inference with fast sparsity-aware sampling to enable high scalability. We derive the inference algorithm in section 3.3.
In contrast to word representation learning where word tokens are observed and embeddings can be induced directly from word collocation pa erns, topics are hidden from the text, posing additional inferential challenge. We resort to generative framework as in conventional topic models by associating a word distribution with each topic. We also take into account uncertainty of topic correlations for exibility. us, in addition to the intuitive geometric interpretation of our embedding based correlation scheme, the full Bayesian treatment also endows connection to the classic CTM model, o ering theoretical insights into our approach. We present the model structure in the next section. (Table 1 lists key notations; Figure 2 shows the graphical model representation of our model.)

3.2 Model Structure

We

rst establish the notations.

Let W

=

{wd

}D
d =1

be

a

collection

of

documents.

Each

document d

contains

Nd

words wd

=

{w

d

n

}Nd
n=1

from a vocabulary of size V .

We assume K topics underlying the corpus. As discussed above,

for each topic k, we want to learn a compact distributed representation uk ∈ RM with low dimensionality (M K). Let U ∈ RK×M denote the topic vector collection with the kth row Uk · = uTk . As a common choice in word embedding methods, we use the vector

inner product for measuring the closeness between embedding vec-

tors. In addition to topic embeddings, we also induce document vectors in the same vector space. Let ad ∈ RM denote the embed-
ding of document d. We now can conveniently compute the a nity of a document d to a topic k through uTk ad . A topic k nearby, and thus semantically correlated to topic k, will naturally have similar distance to the document, as |uTk ad − uTk ad | ≤ uk − uk ad and uk − uk is small.

We express uncertainty of the a nity by modeling the actual topic weights ηd ∈ RK as a Gaussian variable centered at the a nity vector, following ηd ∼ N (U ad , τ −1I ). Here τ characterizes the uncertainty degree and is pre-speci ed for simplicity. As in logisticnormal models, we project the topic weights into the probability simplex to obtain topic distribution θd = so max(ηd ), from which we sample a topic zdn ∈ {1, . . . , K } for each word wdn in the document. As in conventional topic models, each topic k is associated with a multinomial distribution ϕk over the word vocabulary, and each observed word is drawn from respective word distribution indicated by its topic assignment.
Pu ing everything together, the generative process of the proposed model is summarized in Algorithm 1. A theoretically appealing property of our method is its intrinsic connection to conventional logistic-normal models such as the CTM model. If we marginalize out the document embedding variable ad , we obtain ηd ∼ N (0, U U T + τ −1I ), recovering the pairwise topic correlation matrix with low rank constraint, where each element is just the closeness of respective topic embeddings, coherent to the above geometric intuitions. Such covariance decomposition has been used in other context, such as sparse Gaussian processes [39] for e cient approximation and Gaussian reparameterization [23, 41] for di erentiation and reduced variance. Here we relate low-dimensional embedding learning with low-rank covariance decomposition and estimation.
e low-dimensional representations of latent topics enable parsimonious correlation modeling with parameter complexity of O(MK) (i.e., topic embedding parameters), which is e cient in terms of topic number K. Moreover, we are allowed to perform e cient inference in the embedding space, with inference cost linear in K, a huge advance compared to previous cubic complexity of vanilla CTM [5] and quadratic of recent improved version [9]. We derive our inference algorithm in the next section.
3.3 Inference
Posterior inference and parameter estimation is not analytically tractable due to the coupling between latent variables and the nonconjugate logistic-normal prior. is makes the learning di cult especially in our context of scaling to unprecedentedly large data and model sizes. We develop a stochastic variational method that (1) involves only compact topic vectors which are cheap to infer, and (2) includes a fast sampling strategy which tackles non-conjugacy and exploits intrinsic sparsity of both the document topic occurrence and the topical words.
We rst assume a mean- eld family of variational distributions:
q(u, ϕ, a, η, z) = (1)
k q(uk )q(ϕk ) d q(ad )q(ηd ) n q(zdn ).
where the factors have the parametric forms:
q(uk ) = N (uk |µk , Σk(u)), q(ad ) = N (ad |γd , Σd(a)), q(ϕk ) = Dir(ϕk |λk ), q(ηd ) = N (ηd |ξd , Σd(η)), (2)
q(zdn ) = Multi(zdn |κdn )
Variational algorithms aim to minimize KL divergence from q to the true posterior, which is equivalent to tightening the evidence

Algorithm 1 Generative Process
1. For each topic k = 1, 2, · · · , K,
• Draw the topic word distribution ϕk ∼ Dir(β) • Draw the topic embedding uk ∼ N (0, α −1I ) 2. For each document d = 1, 2, · · · , D, • Draw the document embedding ad ∼ N (0, ρ−1I ) • Draw the document topic weight ηd ∼ N (U ad , τ −1I ) • Derive the distribution over topics θd = so max(ηd ) • For each word n = 1, 2, · · · , Nd ,
(a) Draw the topic assignment zdn ∼ Multi(θd ) (b) Draw the word wdn ∼ Multi(ϕzdn )

lower bound (ELBO):

L(q) =

Eq log p(uk )p(ϕk ) +

k

q(uk )q(ϕk )

Eq log p(ad )p(ηd |ad , U )p(zdn |ηd )p(wdn |zdn, ϕ)

d,n

q(ad )q(ηd )q(zdn )

(3)

We optimize L(q) via coordinate ascent, interleaving the update of the variational parameters at each iteration. We employ stochastic variational inference which optimizes the parameters with stochastic gradients estimated on data minibatchs. Due to the space limitations, here we only describe key computation rules of the gradients (or closed-form solutions). ese stochastically estimated quantities are then used to update the variational parameters after scaled by a learning rate. Please refer to the supplementary material [1] for detailed derivations.
Updating topic and document embeddings. For each topic k, we isolate only the terms that contain q(uk |µk , Σk(u)),

L(q(uk )) = Eq [log p(uk )] + d Eq [log p(ηd |ad , U )] (4) − Eq [log q(uk )] .

e optimal solution for q(uk ) is then obtained by se ing the gradient to zero, with the variational parameters computed as:

µk = τ Σ(u) ·

ξdkγd ,

d

(5)

Σ(u) = αI + τ

d Σd(a) + γdγdT

−1
,

where we have omi ed the subscript k of the variational covariance matrix Σ(u) as it is independent with k. Intuitively, the optimal variational topic embeddings are the centers of variational document embeddings scaled by respective document topic weights and transformed by the variational covariance matrix.
By symmetry, the variational parameters of document embedding ad is similarly updated as:

γd = τ Σ(a) ·

ξdk µk ,

k

−1

(6)

Σ(a) = γ I + τ k Σ(u) + µk µTk ,

where, again, Σ(a) is independent with d and thus the subscript d is omi ed.
Learning low-dimensional topic and document embeddings is computationally cheap. Speci cally, by Eq.(5), updating the set

of variational topic vector means {µk }kK=1 imposes complexity O(KM2), and updating the covariance Σ(u) requires only O(M3). Similarly, by Eq.(6), the cost of optimizing γd and Σ(a) is O(KM) and O(KM2), respectively. Note that Σ(a) is shared across all documents and does not need updates per document. We see that all the updates cost only linearly w.r.t to the topic size K which is critical to scale to large-scale practical applications.
Sparsity-aware topic sampling. We next consider the optimization of the variational topic assignment q(zdn ) for each word wdn . Le ing wdn = , the optimal solution is:

q(zdn = k) ∝ exp {ξdk } exp Ψ(λk ) − Ψ

λk , (7)

where Ψ(·) is the digamma function; and ξd and λk are the variational means of the document’s topic weights and the variational
word weights (Eq.(2)), respectively. Direct computation of q(zdn ) with Eq.(7) has complexity of O(K), which becomes prohibitive in
the presence of many latent topics. To address this, we exploit two
aspects of intrinsic sparsity in the modeling: (1) ough a whole
corpus can cover a large diverse set of topics, a single document
in the corpus is usually about only a small number of them. We
thus only maintain the top Ks entries in each ξd , where Ks K, making the complexity due to the rst term in the right-hand side
of Eq.(7) only O(Ks ) for all K topics in total; (2) A topic is typically characterized by only a few words in the large vocabulary, we thus
cut o the variational word weight vector λk for each k by maintaining only its top Vs entries (Vs V ). Such sparse treatment helps enhance the interpretability of learned topics, and allows
cheap computation with on average O(KVs /V ) cost for the second term1. With the above sparsity-aware updates, the resulting com-
plexity for Eq.(7) with K topics is brought down to O(Ks + KVs /V ), a great speedup over the original O(K) cost. e top Ks entries of ξd are selected using a Min-heap data structure, whose computational cost is amortized across all words in the document, imposing
O(K/Nd log Ks ) computation per word. e cost for nding the top Vs entries of λk is similarly amortized across documents and words, and becomes insigni cant.
Updating the remaining variational parameters will frequently
involve computation of variational expectations under q(zdn ). It is thus crucial to speedup this operation. To this end, we employ
sparse approximation by sampling from q(zdn ) a single indicator z˜dn , and use the “hard” sparse distribution q˜(zdn = k) := 1(z˜dn = k) to estimate the expectations. Note that the sampling operation is
cheap, having the same complexity with computing q(zdn ) as above. As shown shortly, such sparse computation will signi cantly reduce
our running cost. ough stochastic expectation approximation is
commonly used for tackling intractability [24, 34], here we instead
apply the technique for fast estimation of tractable expectations. We next optimize the variational topic weights q(ηd |ξd , Σd(η)).
Extracting only the terms in L(q) involving q(ηd ), we get:
L(q(ηd )) = Eq [log p(ηd |ad , U )] + Eq [log p(zd |ηd )] (8) − Eq [log q(ηd )] ,

1In practice we also set a threshold s such that each word needs to have at least

s non-zero entries in {λk }kK=1. O(max{KVs /V , s }).

us the exact complexity of the second term is

where the second term

Eq [log p(zd |ηd )] = k,n q(zdn = k)Eq [log(so maxk (ηd ))]

involves variational expectations of the logistic transformation

which does not have an analytic form. We construct a fast Monto

Carlo estimator for approximation. Particularly, we employ repa-

rameterization trick by rst assuming a diagonal covariance matrix

Σ(η) = diag(σ 2) as is commonly used in previous work [5, 23],

d

d

where σd denotes the vector of standard deviations, resulting in

the following sampling procedure:

ηd(t ) = ξd + σd ϵ(t ); ϵ(t ) ∼ N (0, I ),

(9)

where is the element-wise multiplication. With T samples of ηd , we can estimate the variational lower bound and the derivatives
∇L w.r.t the variational parameters {ξd , σd }. For instance,

∇ξd Eq [log p(zd |ηd )] ≈ k,n q(zdn = k)ek − (Nd /T ) Tt =1 so max ηd(t ) (10) ≈ k,n 1(z˜dn = k)ek − (Nd /T ) Tt =1 so max ηd(t )

where ek is an indicator vector with the kth element being 1 and the rest 0. In practice T = 1 is usually su cient for e ective inference.
e second equation applies the hard topic sample mentioned above, which reduces the time complexity O(KNd ) of the original standard computation (the rst equation) to O(Nd + K) (i.e., O(Nd ) for the
rst term and O(K) for the second). e rst term in Eq.(8) depends on the topic and document em-
beddings to encode topic correlations in document’s topic weights. e derivative w.r.t to the variational parameter ξd is computed as:
∇ξd Eq [log p(ηd |U , ad )] = τ (U˜ γd − ξd ). (11)

Here U˜ is the collection of variational means of topic embeddings where the kth row U˜k · = µTk . We see that, with low-dimensional topic and document vector representations, inferring topic corre-
lations is of low cost O(KM) which grows only linearly w.r.t to
the topic size. e complexity of the remaining terms in Eq.(8),
as well as respective derivatives w.r.t the variational parameters,
has complexity of O(KM) (Please see the supplements [1] for more
details). In summary, the cost of updating q(ηd ) for each document d is O(KM + K + Nd ).
Finally, the optimal solution of the variational topic word distri-
bution q(ϕk |λk ) is given by:

λk = β + d,n 1(wdn = )1(z˜dn = k).

(12)

Algorithm summarization. We summarize our variational
inference in Algorithm 2. As analyzed above, the time complexity of our variational method is O(KM2 + M3) for inferring topic
embeddings q(ud ). e cost per document is O(KM) for computing q(ad ), O(KM) for updating q(ηd ), and O((Ks + KVs /V )Nd ) for maintaining q(zd ). e overall complexity for each document is thus O(KM + (Ks + KVs /V )Nd ), which is linear to model size (K), comparable to the LDA model while greatly improving over
previous correlation methods with cubic or quadratic complexity.
e variational inference algorithm endows rich independence
structures between the variational parameters, allowing straight-
forward parallel computing. In our implementation, updates of

Algorithm 2 Stochastic variational inference

1: Initialize variational parameters randomly

2: repeat 3: Compute learning rate ιiter = 1/(1 + iter)0.9
4: Sample a minibatch of documents B

5: for all d ∈ B do

6:

repeat

7:

Update q(zd ) with Eq.(7) and sample z˜d

8:

Update γd with Eq.(6)

9:

Update q(ηd ) using respective gradients computed with

Eqs.(10),(11),and more in the supplements [1].

10:

until convergence

11:

Compute stochastic optimal values µ∗, Σ(u)∗ with Eq.(5)

12:

Compute stochastic optimal values λ∗ with Eq.(12)

13:

Update x = (1 − ιiter)x + ιiterx∗ with x ∈ {µ, Σ(u), λ}

14:

Update Σ(a) with Eq.(6)

15: end for

16: until convergence

variational topic embeddings {µk } (Eq.(5)), topic word distributions {λk } (Eq.(12)), and document embeddings {γd } (Eq.(6)) for a data minibatch, are all computed in parallel across multiple CPU cores.
4 EXPERIMENTS
We demonstrate the e cacy of our approach with extensive experiments. (1) We evaluate the extraction quality in the tasks of document classi cation and retrieval, in which our model achieves similar or be er performance than existing correlated topic models, signi cantly improving over simple LDA. (2) For scalability, our approach scales comparably with LDA, and handles massive problem sizes orders-of-magnitude larger than previously reported correlation results. (3) alitatively, our model reveals very meaningful topic correlation structures.
4.1 Setup
Datasets. We use three public corpora provided in the UCI repository2 for the evaluation: 20Newsgroups is a collection of news documents partitioned (nearly) evenly across 20 di erent newsgroups. Each article is associated with a category label, serving as ground truth in the tasks of document classi cation and retrieval; NYTimes is a widely-used large corpus of New York Times news articles; and PubMed is a large set of PubMed abstracts. e detailed statistics of the datasets are listed in Table 2. We removed a standard list of 174 stop words and performed stemming. For NYTimes and Pubmed, we kept the top 10K frequent words in vocabulary, and selected 10% documents uniformly at random as test sets, respectively. For 20Newsgroups, we followed the standard training/test spli ing, and performed the widely-used pre-processing3 by removing indicative meta text such as headers and footers so that document classi cation is forced to be based on the semantics of plain text.
2h p://archive.ics.uci.edu/ml 3h p://scikit-learn.org/stable/datasets/twenty newsgroups.html

Dataset
20Newsgroups NYTimes PubMed

#doc (D)
18K 1.8M 8.2M

vocab size (V )
30K 10K 10K

doc length
130 284 77

Table 2: Statistics of the three datasets, including the number of documents (D), vocabulary size (V ), and average number of words in each document.

0.7

0.6

Accuracy

0.5

0.4 0.3 20
Figure 3: Classi

LDA CTM Ours
40 60 80 100 K
cation accuracy on 20newsgroup.

Baselines. We compare the proposed model with a set of carefully selected competitors:
• Latent Dirichlet Allocation (LDA) [6] uses conjugate Dirichlet priors and thus scales linearly w.r.t the topic size but fails to capture topic correlations. Inference is based on the stochastic variational algorithm [16]. When evaluating scalability, we leverage the same sparsity assumptions as in our model for speeding up.
• Correlated Topic Model (CTM) [5] employs standard logistic-normal prior which captures pairwise topic correlations. e model uses stochastic variational inference with O(K3) time complexity.
• Scalable CTM (S-CTM) [9] developed a scalable sparse Gibbs sampler for CTM inference with time complexity of O(K2). Using distributed inference on 40 machines, the method discovers 1K topics from millions of documents, which to our knowledge is the largest automatically learned topic correlation structures so far.
Parameter Setting. roughout the experiments, we set the embedding dimension to M = 50, and sparseness parameters to Ks = 50 and Vs = 100. We found our modeling quality is robust to these parameters. Following common practice, the hyperparameters are xed to β = 1/K, α = 0.1, ρ = 0.1, and τ = 1. e baselines are using similar hyper-parameter se ings.
All experiments were performed on Linux with 24 4.0GHz CPU cores and 128GB RAM. All models are implemented using C/C++, and parallelized whenever possible using the OpenMP library.
4.2 Document Classi cation
We rst evaluate the performance of document classi cation based on the learned document representations. We evaluate on the 20Newsgroups dataset where ground truth class labels are available. We compare our proposed model with LDA and CTM. For LDA

Precision(%) Precision(%) Precision(%)

60 50 40 30 20 10 0 0.1

LDA CTM Ours
0.4 1.6 6.4 Recall(%)

25.6 100

60 50 40 30 20 10 0 0.1

LDA CTM Ours
0.4 1.6 6.4 Recall(%)

25.6 100

60 50 40 30 20 10 0 0.1

LDA CTM Ours
0.4 1.6 6.4 Recall(%)

25.6 100

Figure 4: Precision-Recall curves on 20Newsgroups. Le : #topic K = 20. Middle: K = 60. Right: K = 100.

and CTM, a multi-class SVM classi er is trained for each of them based on the topic distributions of the training documents, while for the proposed model, the SVM classi er takes the document embedding vectors as input. Generally, more accurate modeling of topic correlations enables be er document modeling and representations, resulting in improved document classi cation accuracy.
Figure 3 shows the classi cation accuracy as the number of topics varies. We see that the proposed model performs best in most of the cases, indicating that our method can discover high-quality latent topics and correlations. Both CTM and our model signi cantly outperforms LDA which treats latent topics independently, validating the importance of topic correlation for accurate text semantic modeling. Compared to CTM, our method achieves be er or competitive accuracy as K varies, which indicates that our model, though orders-of-magnitude faster (as shown in the next), does not sacri ce modeling power compared to the complicated and computationally demanding CTM model.
4.3 Document Retrieval
We further evaluate the topic modeling quality by measuring the performance of document retrieval [15]. We use the 20Newsgroups dataset. A retrieved document is relevant to the query document when they have the same class label. For LDA and CTM, document similarity is measured as the inner product of topic distributions, and for our model we use the inner product of document embedding vectors.
Figure 4 shows the retrieval results with varying number of topics, where we use the test set as query documents to retrieve similar documents from the training set, and the results are averaged over all possible queries. We observe similar pa erns as in the document classi cation task. Our model obtains competitive performance with CTM, both of which capture topic correlations and greatly improve over LDA. is again validates our goal that the proposed method has lower modeling complexity while at the same time is as accurate and powerful as previous complicated correlation models. In addition to e cient model inference and learning, our approach based on compact document embedding vectors also enables faster document retrieval compared to conventional topic models which are based on topic distribution vectors (i.e., M K).
4.4 Scalability
We now investigate the e ciency and scalability of the proposed model. Compared to topic extraction quality in which our model

Dataset 20Newsgroups NYTimes PubMed

K
100 100 1K 10K 100K

LDA
11 min
2.5 hr 5.6 hr 8.4 hr
16.7 hr

Running Time

CTM S-CTM

60 min 22 min

– 6.4 hr

–

–

–

–

–

–

Ours
20 min
3.5 hr 5.7 hr 9.2 hr
19.9 hr

Table 3: Total training time on various datasets with di erent number of topics K. Entries marked with “–” indicates model training is too slow to be nished in 2 days.

achieves similar or be er level of performance as the conventional complicated correlated topic model, here we want our approach to tackle large problem sizes which are impossible for existing correlation methods, and to scale as e ciently as the lightweight LDA, for practical deployment.
Table 3 compares the total running time of model training with di erent sized datasets and models. As a common practice [16], we determine convergence of training when the di erence between the test set per-word log-likelihoods of two consecutive iterations is smaller than some threshold. On small dataset like 20Newsgroups (thousands of documents) and small model (hundreds of topics), all approaches nish training in a reasonable time. However, with increasing number of documents and latent topics, we see that the vanilla CTM model (with O(K3) inference complexity) and its scalable version S-CTM (with O(K2) inference complexity) quickly becomes impractical, limiting their deployment in real-world scale tasks. Our proposed topic embedding method, by contrast, scales linearly with the topic size, and is capable of handling 100K topics on over 8M documents (PubMed)—a problem size several orders of magnitude larger than previously reported largest results [9] (1K topics on millions of documents). Notably, even with added model power and increased extraction performance compared to LDA (as has been shown in sections 4.2-4.3), our model only imposes negligible additional training time, showing strong potential of our method for practical deployment of real-world large-scale applications as LDA does.
Figure 5, le panel, shows the convergence curves on NYTimes as training goes. Using similar time, our model converges to a be er point (higher test likelihood) than LDA does, while S-CTM is much slower, failing to arrive convergence within the time frame.

Log Predictive Probability Time(min)
Time(s) per batch

7.6 8.0 8.4 8.8 9.2
0

48 Time(h)

LDA S-CTM Ours
12 16 20

60

LDA

50

CTM

S-CTM

40

Ours

30

20

10

0
20 40 60 80 100 K

LDA

CTM

S-CTM

3000

Ours

400

20 5 0 100

1k

10k 100k

K

Figure 5: Le : Convergence on NYTimes with 1K topics. Middle: Total training time on 20Newsgroups. Right: Runtime of one inference iteration on a minibatch of 500 NYTimes articles, where the result points of CTM and S-CTM on large K are omitted as they fail to nish one iteration within 2 hours.

nation war
holocaust aggression Europe
Turkish
Armenian
Armenia
Soviet Istanbul
conference
Islamic jaeger Ghetto jihad
privacy ISDN
citizen escrow decode

exist first proof express benefit
Karabakh belief
Azerbajian
Nagorno religious
escrow
key algorithm
NSA descryptor

general
press science
take useful

camera
ray study
SSTO analysis

rainer frequency
shuttle
select launch

planet clock
orbit socket evil
package system information
file large

visual
program LARC
NASA
propulsion

encryption chips
act cryptography
penalty

exploration
Luna launch
Washington
operational
water fossil
steam MWRA
EPA
conductor
GFCIs grounding
wire insulation

Figure 6: A portion of topic correlation graph learned from 20Newsgroups. Each node denotes a latent topic whose semantic meaning is characterized by the top words according to the topic’s word distribution. e font size of each word is proportional to the word weight. Topics with correlation strength over some threshold are connected with edges. e thickness of the edges is proportional to the correlation strengths.

Figure 5, middle panel, measures the total training time with varying number of topics. We use the small 20Newsgroups dataset since on larger data (e.g., NYTimes and PubMed) the CTM and SCTM models are usually too slow to converge in a reasonable time. We see that the training time of CTM increases quickly as more topics are used. S-CTM works well in this small data and model scale, but, as have been shown above, it is incapable of tackling larger problems. In contrast, our approach scales as e ciently as the simpler LDA model. Figure 5, right panel, evaluates the runtime of one inference iteration on a minibatch of 500 documents. when the topic size grows to a large number, CTM and S-CTM fail to
nish one iteration in 2 hours. Our model, by contrast, keeps as scalable as LDA and considerably speeds up over CTM and S-CTM.
4.5 Visualization and Analysis
We qualitatively evaluate our approach by visualizing and exploring the extracted latent topics and correlation pa erns.

Figure 6 visualizes the topic correlation graph inferred from the 20Newsgroups dataset. We can see many topics are strongly correlated to each other and exhibit clear correlation structure. For instance, the set of topics in the right upper region are mainly about astronomy and are interrelated closely, while their connections to information security topics shown in the lower part are weak. Figure 7 shows 100K topic embeddings and their correlations on the PubMed dataset. Related topics are close to each other in the embedding space, revealing diverse substructures of themes in the collection. Our model discovers very meaningful structures, providing insights into the semantics underlying the large text corpora and facilitating understanding of the large collection of topics.
5 CONCLUSIONS
We have developed a new correlated topic model which induces distributed vector representations of latent topics, and characterizes correlations with the closeness of topic vectors in the embedding

tissue
peak time
diastolic
treatment

differ heart time
ventriculoarterial

cardiac
unfold DNA
resistant

vasodilator
coronary
endothelial
epricardial

RNA
thermoresponsive
integrety aspheric

rRNA invade
acidophilum
substract reconstruct

prothesis psym
sequence enterohepatic
venereology

factor detect
bacteriophage
nonsurvive
incriminate

Figure 7: Visualization of 100K correlated topics on PubMed. See the captions of Figure 1 for more depictions.
space. Such modeling scheme, along with the sparsity-aware sampling in inference, enables highly e cient model training with linear time complexity in terms of the model size. Our approach scales to unprecedentedly large data and models, while achieving strong performance in document classi cation and retrieval. e proposed correlation method is generally applicable to other context, such as modeling word dependencies for improved topical coherence. It is interesting to further speedup of the model inference through variational neural Bayes techniques [13, 23] for amortized variational updates across data examples. Note that our model is particularly suitable to incorporate neural inference networks that, replacing the per-document variational embedding distributions, map documents into compact document embeddings directly. We are also interested in combining generative topic models with advanced deep text generative approaches [20, 21, 32] for improved text modeling.
ACKNOWLEDGMENTS
is research is supported by NSF IIS1447676, ONR N000141410684, and ONR N000141712463.
REFERENCES
[1] 2017. Supplementary material. (2017). www.cs.cmu.edu/∼zhitingh/kddsupp [2] Amr Ahmed and Eric Xing. 2007. On tight approximate inference of the logistic-
normal topic admixture model. In AISTATS. [3] Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, and Sam Gersh-
man. 2016. Nonparametric Spherical Topic Modeling with Word Embeddings. In ACL. [4] David M Blei, omas L Gri ths, and Michael I Jordan. 2010. e nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies. J. ACM 57, 2 (2010), 7. [5] David M Blei and John D La erty. 2007. A correlated topic model of science. e Annals of Applied Statistics (2007), 17–35. [6] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet allocation. JMLR 3, Jan (2003), 993–1022. [7] Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu. 2007. A Topic Model for Word Sense Disambiguation.. In EMNLP-CoNLL. 1024–1033.

[8] Jianfei Chen, Kaiwei Li, Jun Zhu, and Wenguang Chen. 2016. WarpLDA: a Simple and E cient O(1) Algorithm for Latent Dirichlet Allocation. In VLDB.
[9] Jianfei Chen, Jun Zhu, Zi Wang, Xun Zheng, and Bo Zhang. 2013. Scalable inference for logistic-normal topic models. In NIPS. 2445–2453.
[10] Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015. Gaussian LDA for topic models with word embeddings. In ACL.
[11] Kumar Dubey, Qirong Ho, Sinead A Williamson, and Eric P Xing. 2014. Dependent nonparametric trees for dynamic hierarchical clustering. In NIPS. 1152– 1160.
[12] Yarin Gal and Zoubin Ghahramani. 2014. Pitfalls in the use of Parallel Inference for the Dirichlet Process.. In ICML. 208–216.
[13] Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. 2017. Nonparametric Variational Auto-encoders for Hierarchical Representation Learning. arXiv preprint arXiv:1703.07027 (2017).
[14] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD. ACM, 855–864.
[15] Geo rey E Hinton and Ruslan R Salakhutdinov. 2009. Replicated so max: an undirected topic model. In NIPS. 1607–1614.
[16] Ma hew D Ho man, David M Blei, Chong Wang, and John William Paisley. 2013. Stochastic variational inference. JMLR 14, 1 (2013), 1303–1347.
[17] Zhiting Hu, Qirong Ho, Avinava Dubey, and Eric P Xing. 2015. Large-scale Distributed Dependent Nonparametric Trees.. In ICML. 1651–1659.
[18] Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao, and Eric P Xing. 2015. Entity Hierarchy Embedding.. In ACL. 1292–1300.
[19] Zhiting Hu, Gang Luo, Mrinmaya Sachan, Eric Xing, and Zaiqing Nie. 2016. Grounding topic models with knowledge bases. In IJCAI.
[20] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Controllable Text Generation. ICML (2017).
[21] Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P Xing. 2017. On Unifying Deep Generative Models. arXiv preprint arXiv:1706.00550 (2017).
[22] Di Jiang, Rongzhong Lian, Lei Shi, and Hua Wu. 2016. Latent Topic Embedding. In COLING.
[23] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 (2013).
[24] Miguel La´zaro-Gredilla. 2014. Doubly stochastic variational Bayes for nonconjugate inference. ICML.
[25] Tuan Le and Hady W Lauw. 2014. Semantic visualization for spherical representation. In KDD. ACM, 1007–1016.
[26] Tuan Minh Van LE and Hady W Lauw. 2014. Manifold learning for jointly modeling topic and visualization. (2014).
[27] Tao Lei, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. ACL.
[28] Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexander J Smola. 2014. Reducing the sampling complexity of topic models. In KDD. ACM, 891–900.
[29] Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. 2016. Generative topic embedding: a continuous representation of documents. In ACL.
[30] Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In ICML. ACM, 577–584.
[31] Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, and Katia Sycara. 2016. Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classi cation. In COLING.
[32] Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and Eric P Xing. 2017. Recurrent Topic-Transition GAN for Visual Paragraph Generation. arXiv preprint arXiv:1703.07022 (2017).
[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Je rey Dean. 2013. Distributed Representations of Words and Phrases and eir Compositionality. In NIPS.
[34] David Mimno, Ma Ho man, and David Blei. 2012. Sparse stochastic inference for latent Dirichlet allocation. arXiv preprint arXiv:1206.6425 (2012).
[35] John Paisley, Chong Wang, David M Blei, et al. 2012. e discrete in nite logistic normal distribution. Bayesian Analysis 7, 4 (2012), 997–1034.
[36] Duangmanee Pew Pu hividhya, Hagai T A ias, and Srikantan Nagarajan. 2009. Independent factor topic models. In ICML. ACM, 833–840.
[37] Rajesh Ranganath and David M Blei. 2016. Correlated random measures. JASA (2016).
[38] Jian Tang, Meng , and Qiaozhu Mei. 2015. PTE: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 1165–1174.
[39] Michalis K Titsias. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In AISTATS, Vol. 5. 567–574.
[40] Yi Wang, Xuemin Zhao, Zhenlong Sun, Hao Yan, Lifeng Wang, Zhihui Jin, Liubin Wang, Yang Gao, Ching Law, and Jia Zeng. 2015. Peacock: Learning long-tail topic features for industrial applications. TIST 6, 4 (2015), 47.
[41] Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. 2016. Stochastic Variational Deep Kernel Learning. In NIPS. 2586–2594.
[42] Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and Wei-Ying Ma. 2015. LightLDA: Big topic models on modest computer clusters. In WWW. ACM, 1351–1361.

A INFERENCE A.1 Stochastic Mean-Field Variational Inference
We rst assume a mean- eld family of variational distributions:

q(u, ϕ, a, η, z) = k q(uk )q(ϕk ) d q(ad )q(ηd ) n q(zdn ),

(A.13)

where the factors have the parametric forms:

q(uk ) = N (uk |µk , Σk(u)), q(ϕk ) = Dir(ϕk |λk ),
q(zdn ) = Multi(zdn |κdn ).

q(ad ) = N (ad |γd , Σd(a)), q(ηd ) = N (ηd |ξd , Σd(η)),

(A.14)

Variational algorithms aim to minimize KL divergence from q to the true posterior, which is equivalent to tightening the evidence lower bound (ELBO):

L(q) = Eq [log p(u, a, η, z, w, ϕ |α, β, ρ, τ )] − Eq [log q(u, a, η, z, ϕ)] = Eq [log p(u |α )] + Eq [log p(ϕ |β)] + Eq [log p(a|ρ)] + Eq [log p(η|u, a, τ )] + Eq [log p(z|η)] + Eq [log p(w |ϕ, z)] − Eq [log q(u, a, η, z, ϕ)] .

(A.15)

A.2 Optimize q(z)

q(zdn = k) ∝ exp E−zdn log p(zdn = k |ηd ) + E−zdn [log p(wdn |ϕk , zdn = k)]

∝ exp E−zdn [log(so maxk (ηd ))] + E−zdn
V

1(wdn = ) log ϕk

(A.16)

∝ exp ξdk +

1(wdn = )(Ψ(λk ) − Ψ( λk )) .
=1

Sparsity-aware topic sampling. Direct computation of q(zdn ) with Eq.(A.16) has complexity of O(K), which becomes prohibitive in the presence of many latent topics. To address this, we exploit two aspects of intrinsic sparsity in the modeling: (1) ough a whole corpus

can cover a large diverse set of topics, a single document in the corpus is usually about only a small number of them. We thus only maintain

the top Ks entries in each ξd , where Ks K, making the complexity due to the rst term in the right-hand side of Eq.(A.16) only O(Ks ) for all K topics in total; (2) A topic is typically characterized by only a few words in the large vocabulary, we thus cut o the variational word

weight vector λk for each k by maintaining only its top Vs entries (Vs V ). Such sparse treatment helps enhance the interpretability of learned topics, and allows cheap computation with average O(KVs /V ) cost for the second term4. With the above sparsity-aware updates,
the resulting complexity for Eq.(A.16) with K topics is brought down to O(Ks + KVs /V ), a great speedup over the original O(K) cost. e
top Ks entries of ξd are selected using a Min-heap data structure, whose computational cost is amortized across all words in the document, imposing O(K/Nd log Ks ) computation per word. e cost for nding the top Vs entries of λk is similarly amortized across documents and words, and becomes insigni cant.

Besides, updating the remaining variational parameters will frequently involve computation of variational expectations under q(zdn ). It is thus crucial to speedup this operation. To this end, we employ sparse approximation by sampling from q(zdn ) a single indicator z˜dn , and use the “hard” sparse distribution q˜(zdn = k) := 1(z˜dn = k) to estimate the expectations. Note that the sampling operation is cheap, having the same complexity with computing q(zdn ) as above. As shown shortly, such sparse computation will signi cantly reduce our running cost.

A.3 Optimize q(ϕ)
For each topic k, we isolate only the terms that contain q(ϕk ), q(ϕk ) ∝ exp E−ϕk (log ϕkβ −1) + E−ϕk (log ∝ ϕkβ −1+ d,n 1(wdn = )·1(z˜dn =k ).

d,n, ϕk1(wdn = )·1(z˜dn =k ))

(A.17)

erefore,

q(ϕk ) ∼ Dir(λk ),

(A.18)

λk = β + d,n 1(wdn = ) · 1(z˜dn = k).

(A.19)

e cost for updating q(ϕ) is globally amortized across documents and words, and thus insigni cant compared with other local parameter

update.

4In practice we also set a threshold s such that each word needs to have at least s non-zero entries in {λk }kK=1. us the exact complexity of the second term is O(max{KVs /V , s }).

A.4 Optimize q(u) and q(a)

q(uk ) ∝ exp E−uk [log p(uk |α )] + d E−uk [log p(ηd |ad , u, τ )] ,

E−uk [log p(uk |α )] = E−uk log ∝ − α2 uTk uk ,

(2π ) M21α − M2 exp(− α2 uTk uk )

(A.20) (A.21)

E−u [log p(ηd |ad , u, τ )] = E−u log

1

exp(−τ (ηd − U ad )T (ηd − U ad ))

k

k

(2π

)

M 2

τ

−

M 2

2

= −τ uT 2k

d (Σd(a) + γdγdT ) uk + τ d ξdkγdT uk + C.

(A.22)

erefore,

q(uk ) ∝ exp − 21uTk αI + d (τ Σd(a) + τγdγdT ) uk + τ

where Σd(a) is the covariance matrix of ad . From Eq.(A.23), we know q(uk ) ∼ N (µk , Σk(u)).

d ξdkγdT uk ,

(A.23)

Σk(u) = αI +

d (τ Σd(a)

+

τ

γ

d

γ

T d

)

−1
.

(A.24)

Notice that Σk(u) is unrelated to k, which means all topic embeddings share the same covariance matrix, we denote it as Σ(u).

µk = τ Σ(u) · ( d ξdkγd ).

(A.25)

Analogously,

γd = τ Σ(a) · ( k ξdk µk ),

(A.26)

Σ(a) = γ I + τ K Σ(u) + Since Σ(a) is unrelated to d, we can rewrite Eq.(A.24) as

k τ µk µTk −1 .

(A.27)

Σ(u) = αI + τ DΣd(a) + d τγdγdT −1 .

(A.28)

e cost for optimizing γd is O(KDM). Updating the set of variational topic vector means {µk }kK=1 and Σ(a) both imposes complexity O(KM2), and update of Σ(u) costs O(M3). Since µ, Σ(a), and Σ(u) are all global parameters, we update them in a distributed manner.

A.5 Optimize q(η)
Assume q(ηd ) is Gaussian Distribution and its covariance matrix is diagonal, i.e., ηdk ∼ N (ξdk , Σd(η)), Σd(η) = diag(σd ). We can isolate the terms in ELBO including ηd ,
L(ηd ) = Eq [log p(ηd |U , ad )] + Eq [log p(zd |ηd )] − Eq [log q(ηd )] ,

(A.29)

τ Eq [log p(ηd |U , ad )] = − 2

k (ξd2k + σd2k ) + τ ξdT µγd + C,

(A.30)

Eq [log p(zd |ηd )] = k,n 1(zdn = k)Eq [log(so maxk (ηd ))] ,

(A.31)

Eq [log q(ηd )] = − k log σdk + C.

(A.32)

For Eq.(A.31), the expectation is intractable due to normalization term in so max. As a result, we use reparameterization trick and Monto

Carlo estimator to approximate the expectation:

ηd(t ) = ξd + σd ϵ(t ); ϵ(t ) ∼ N (0, I ),

(A.33)

where is the element-wise multiplication. With T samples of ηd , we can estimate the variational lower bound and the derivatives ∇L w.r.t. the variational parameters {ξd , σd }.

∇ξd Eq [log p(ηd |U , ad )] = τ (U˜ γd − ξd ).

(A.34)

Here U˜ is the collection of variational means of topic embeddings where the kth row U˜k · = µk .
∇ξd Eq [log p(zd |ηd )] = Eq ∇ξd log p(zd |ηd ) ≈ T1 Tt =1 k,n 1(z˜dn = k) ek − so max(ξd + σd ≈ k,n 1(z˜dn = k)ek − (Nd /T ) Tt =1 so max ηd(t )
where ek is an one-hot vector, which evaluates to 1 in its kth entry. T is the sample number.

ϵd(l )) ,

(A.35)

∇σd Eq [log p(ηd |U , ad )] = −τ σd ,

(A.36)

where σ1d is element-wise computation.

∇σd Eq [log p(zd |ηd )] = Eq ∇σd log p(zd |ηd ) = 0, ∇σd Eq [log q(ηd )] = − σ1d ,
erefore,

(A.37) (A.38)

∇ξd L = τ (U˜ γd − ξd ) + k,n 1(z˜dn = k)ek − (Nd /T ) Tt =1 so max ηd(t ) ,

(A.39)

∇σd L = −τ σd + σ1d .

(A.40)

We can conclude that σdk = τ and thus there is no update for σ in our algorithm. In the experiment, we set T = 1 and use Adagrad to

update ξd . From Eq.(A.39), the time complexity for updating variational mean topic weight vector ξd is O(KM + Nd + K).

