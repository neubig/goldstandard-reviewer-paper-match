1
Adjusting the Ground Truth Annotations for Connectivity-Based Learning to Delineate
Doruk Oner, Leonardo Citraro, Mateusz Kozin´ ski, Pascal Fua

arXiv:2112.02781v1 [cs.CV] 6 Dec 2021

Abstract— Deep learning-based approaches to delineating 3D structure depend on accurate annotations to train the networks. Yet, in practice, people, no matter how conscientious, have trouble precisely delineating in 3D and on a large scale, in part because the data is often hard to interpret visually and in part because the 3D interfaces are awkward to use.
In this paper, we introduce a method that explicitly accounts for annotation inaccuracies. To this end, we treat the annotations as active contour models that can deform themselves while preserving their topology. This enables us to jointly train the network and correct potential errors in the original annotations. The result is an approach that boosts performance of deep networks trained with potentially inaccurate annotations.

I. INTRODUCTION
As in many areas of computer vision, deep networks now deliver state-of-the-art results for delineation tasks, such as ﬁnding axons and dendrites in 3D light microscopy images. However, their performance depends critically on the accuracy of the ground-truth data used to train them. This is especially true when the delineation task is treated as a segmentation one and the network is trained by minimizing the crossentropy between the centerline predictions and ground-truth annotations, which is one of the most popular paradigms.
In practice, these so-called ground-truth annotations are usually supplied manually by an annotator who may not draw with the utmost accuracy and can therefore easily be a few voxels off the true centerline. This is not a matter of carelessness but a consequence of 3D delineation being a truly difﬁcult to do well on a large scale. As a result, inaccurate annotations are more the rule than the exception and this adversely affects how well the networks ultimately perform. One solution would be to have several annotators delineate the same data and combine their delineations. However, this would turn an already tedious, slow, and expensive process into an even slower and more expensive one that almost no one can afford.
In this paper, we introduce a method that explicitly accounts for annotation inaccuracies and delivers the same performance as if they were perfectly accurate. Our main insight is that the annotations are usually imprecise more in terms of the

Doruk Oner, Leonardo Citraro, Mateusz Kozin´ ski, and Pascal

Fua are with Computer Vision Laboratory, School of Computer

and Communication Sciences, EPFL (e-mail: doruk.oner@epﬂ.ch;

leonardo.citraro@epﬂ.ch;

matesz.kozinski@epﬂ.ch;

pas-

cal.fua@epﬂ.ch).

Fig. 1. Our approach. To account for annotation inaccuracies during training, we jointly train the network and adjust the annotations while preserving their topology.
3D location of the centerlines than of the topology of the graph they deﬁne. We can therefore treat them as deformable contours forming a graph that can be reﬁned by moving its nodes while preserving its structure. We cast this approach to training a deep network as a joint optimization over the network parameters and node positions. We then show that we can eliminate the node variables from the optimization problem, which can then be solved by minimizing a loss function. This loss function accounts for the annotation’s lack of spatial precision. It can be minimized in the traditional manner and the output of the re-trained network used to reﬁne the annotation.
Fig. 1 depicts our approach and Fig. 2 showcases its behavior. We will demonstrate that it brings substantial improvements when training networks to delineate neurons in two-photon and confocal microscopy image stacks. Hence, our contribution is an automated approach to better leveraging inaccurate training data, which, in our experience, represents the vast majority of data available to practitioners.
II. RELATED WORK
A. Automated Delineation
Automatic delineation of curvilinear structures has been an active research topic for decades. It has evolved from manually designing ﬁlters that respond strongly to tubular structures [10], [19], [34] to feeding hand-designed features into boosted trees [40], [3], support vector machines [15],

2



(a)

(b)

(c)

(d)

Fig. 2. Correcting an inaccurate annotation. (a) A microscopy scan of a neurite with an inaccurate annotation overlaid in white. (b) Distance map

predicted by the deep net. Ideally, it should equal zero along the neurite centerline. In practice, there is are non-zero values in the area indicated by

the red arrow, presumably because because the neurite is hardly visible there. (c) Nevertheless, the distance map is sufﬁciently good to adjust the

annotation. (d) This network retrained with adjusted annotations can now generate a better distance map even where the neurite is barely visible.

or GradientBoost [32], and ﬁnally to fully relying on neural networks [24], [13], [20], [28], [25].
The latter now routinely deliver the best performance when properly trained. However, obtaining accurately annotated data, especially in 3D, is a challenge. In practice it is rarely available in sufﬁcient quantities. And what annotated data there is, is rarely accurate because manually delineating 3D structures is challenging. Introducing a degree of selfsupervision is a way to address this difﬁculty [2], [8] but this does not detract from the fact that the training would work even better if the available annotated data were accurate. This can be partially ascribed to the fact that most current networks are trained by minimizing the standard cross entropy or differentiable intersection-over-union loss [22]. As pixelwise measures, both are sensitive to even small misplacements of the linear structures’ centerlines. In [26], this is partially addressed by introducing a loss component that accounts for global statistics of the network output, but the cross entropy remains a key component of the overall loss. Similarly, the method of [9] relies on introducing a topology-preserving term but still depends on the annotation being accurate.
Accuracy can be improved by having several people annotate and combining their results using robust statistics. This is effective but even more expensive than obtaining one set of annotations and therefore out of reach for most practitioners. The problem can be partially alleviated by annotating only in 2D projections of the 3D data volumes [27], [42], [18], which is easier, but may result in even less precise annotations than those performed in 3D.
A similar problem to the one we address here also arises in the context of two-dimensional semantic boundary detection. The outlines one ﬁnds in annotated training sets are often rather imprecise and training the networks to nevertheless discover contours that overlap with them is an issue. In [41], training is reformulated as simultaneously optimizing the parameters of a deep net and correcting the annotations by solving a mixed binary-continuous optimization problem. However, unlike in our approach, preservation of annotation topology is not warranted and the corrections may break the continuity of annotations. This is a major problem when tracing neurons or blood vessels, because topology changes inﬂuence the biological interpretation of the results. The same

problem is addressed in [1] by proposing a neural layer and a loss function that can be added on top of an edge detector and make it possible to ﬁnd more accurate contours than those in the annotations. However, because the regions are represented in an implicit fashion, there is no more guarantee than in [41] that the annotations’ connectivity will be preserved. Connectivity being at the heart of our applications, we therefore chose to use explicit deformable models, such as those described below.
B. Deformable Contour Models
Deformable contours [16], [33], [12] were initially introduced as a means to semi-automatically delineate simple contours while imposing smoothness constraints on the resulting outlines. They were later generalized to model network structures [11], [5] that can deform while preserving their topology. They are therefore well suited for reﬁning our inaccurate annotations under the assumption they are topologically correct but that their locations are imprecise.
More recent deformable contours rely on minimizing energy functions generated by deep networks [21], [6], [37], [14], which enables end-to-end learning. Unlike in these methods, which rely on evolving the contour for segmenting the image at test time, our use of deformable contours is limited to adjusting the annotations during training.
Active appearance models [7] enable modelling the appearance of imaged objects, in addition to their shape. They can be learnt from coarse annotations, which are adjusted when ﬁtting the model to the data [29]. The level of detail of the active appearance model can then be increased and, before the more detailed model is ﬁtted to the data, it can be initialized with the parameters of its less detailed version. In this work, we also adjust the annotation during learning, but represent them as network snakes, and train a deep convolutional network, instead of ﬁtting an active appearance model.
III. METHOD
Given a set of microscopy stacks along with the corresponding and possibly imprecise centerline annotations, we want to train a deep net to produce precise delineation. To this end, when training the deep network, we adjust not only its weights

3

but also the annotations themselves. We ﬁrst present the vanilla training procedure without annotation adjustment and explain why it is sub-optimal when the annotations lack precision. We then formalize our training procedure with adjustment.

A. Standard Training Procedure

Let us consider a set of N microscopy scans {Xi}1≤i≤N and corresponding centerline annotations {yˆi}1≤i≤N , in the form of distance maps of the same size as the scans. Voxel p of annotation yˆ, denoted yˆ[p], contains the distance from the center of p to the closest centerline. Let F (·; Θ) be a deep network, with weights Θ. We want to train it to take as input a scan Xi and return a volume yi = F (Xi; Θ), containing a delineation of centerlines visible in Xi. To keep the notation concise, we omit the dependencies on yi on Θ. The traditional approach to learning the network weights is to make yi as close as possible to yˆi by solving

N

Θ∗ = arg min L yˆi, yi ,

(1)

Θ i=1

where the loss term L(yˆ, y) measures the voxel-wise difference between the annotation and the prediction. In our experiments, we take L to be the Mean Square Error. This assumes that the deviations of the annotations from actual centerline trajectories are small and unbiased. In reality, they rarely are. Hence, the network learns to accommodate this uncertainty in the annotations by blurring the predictions. At test time, this leads to breaking the continuity of predictions wherever the image quality is compromised by high level of noise or low contrast between the foreground and the background, as illustrated by Fig. 2.

B. Overview of our Approach

The formulation of Eq. 1 assumes that the deviations of the annotations from reality are small and unbiased. This work is predicated on the fact that they rarely are and that we must allow for substantial non-Gaussian deviations from the original annotations. Thus, instead of encoding the annotations in terms of volumes yˆi, we represent the annotated centerline Ci of each Xi as a graph, with the set of vertices Vi and the set of edges Ei. Each vertex v ∈ Vi has a 3D coordinate cv, and each edge (u, v) ∈ Ei represents a short line segment. This is shown in Fig. 2 where the circles along the annotations denote the vertices. Let ci be the vector formed by concatenating coordinates of all the vertices of Vi. To accommodate the possible lack of precision of the annotations, we let ci change its initial value. Doing so changes the shape of Ci but preserves its topology and can be used to explicitly model the deviation of the annotated centerlines from their true position. In other words, the minimization problem can be reformulated as ﬁnding

N

Θ∗, C∗ = arg min L(ci, yi) + R(ci),

(2)

Θ,C i=1

where L = L D(ci), yi ;

C is the vector obtained by concatenating all the ci; R is a regularization term that forces the deformed centerlines to be smooth, and that we deﬁne in Sec. III-C; L is the same MSE as in Eq. 1; and D is a distance transform that creates a volume in which a voxel with coordinates q is assigned its truncated distance to the closest edge of C. Formally, we write

where

D(c)[q] = min{δ(c, q), d},

(3)

δ(c, q) = min min |φcu + (1 − φ)cv − q|2, (4)
(u,v)∈E 0≤φ≤1

d is the threshold used to truncate the distance map, and the minimization over φ serves to ﬁnd the point on edge (u, v), that is closest to q.
Solving the problem of Eq. 2 means training the network to ﬁnd centerlines that are smooth and with the same topology as the annotations. This is what we want but, unfortunately, this optimization problem involves two kinds of variables, the components of C and Θ respectively, which are not commensurate in any way. In practice, this makes optimization difﬁcult. We address this problem by eliminating the C variables by rewriting Eq. 2 as

c∗i (yi) = arg min L(ci, yi) + R(ci),

(5)

c

N

Θ∗ = arg min

L

c

∗ i

(

yi

),

y

i

+ R c∗i (yi) ,

(6)

Θ i=1

In the following section, we describe our choice of R and the formulation of c∗i (yi) that results from it. Eq. 6 is a standard continuous optimization problem that we can solve using the
usual tools of the trade.

C. Annotations as Network Snakes
We propose to represent each Ci as a network snake, and to take R to be a classical sum of spring and elasticity terms [11], [5]. This regularization term takes the form

R(c) = α

cu − cv 2 + β

cu − 2cv + cw 2, (7)

(u,v)∈E

(u,v,w)∈T

where α and β are hyper-parameters that balance the strength of the two terms, E is the set of edges of C and T is the set of node triples (u, v, w) such that (u, v) ∈ E, (v, w) ∈ E, and v is a node of order two, that is, not a junction of multiple snake branches. As shown in [11], [5], R can be written as
R(ci) = 21 cTi Aci, (8)
where A is a sparse symmetric matrix. Given this quadratic formulation of R, we can use the well-known semi-implicit scheme introduced to deform snakes, also known as active contour models [16], to minimize Eq. 5. It involves initializing each snake c0i to the manually produced annotation and reﬁning it by iteratively solving

(A + γI)cti+1 = γcti − ∂∂Lc (cti, yi) (9)
for cti+1, where γ is a hyper-parameter known as the viscosity and is inversely proportional to the step size in each iteration. We refer the reader to [16] for the complete derivation. Here

4

we only note, that when the iteration stabilizes, we have

∀i,

c

t i

≈

cti+1.

We

can

therefore

denote

the

stable

vector

of

node locations by c∗i , substitute cti+1 ≈ cti ≈ c∗i in Eq. 9, and

use the derivative of Eq. 8, to write

∀i, ∂∂Rc (c∗i ) + ∂∂Lc (c∗i , yi) ≈ 0, (10) which means that c∗ minimizes R + L and is a solution of Eq. 5.
In practice, we solve Eq. 9 by inverting the matrix (A+γI) at the start of the training procedure and then multiplying the right-hand-side of the equation by the inverse at each iteration. Hence, we write

cti+1 = (A + γI)−1 γcti − ∂∂Lc (cti, yi) . (11)

We perform the update or Eq. (11) for 0 ≤ t < T . We take T = 10 in our implementation, which is sufﬁcient for the

process to stabilize, and denote the result of the last iteration

by

c∗i (yi)

=

c

T i

.

D. Computing the Gradients of the Loss Function

Performing the minimization in Eq. 6 requires computing
at each iteration the gradient of the loss with respect to the
network output yi. To avoid cluttering the notation, we denote c∗(yi) by c∗. The gradient can then be expressed as

∂∂y L(c∗i , yi) + R(c∗i )

∂L ∗

∂L ∗

∂R ∗ ∂c∗i

= ∂y (ci , yi) + ∂c (ci , yi) + ∂c (ci ) ∂y (12)

≈ ∂∂Ly (c∗i , yi),

where we used Eq. 10 to eliminate the second term. In other words, even though c∗ is a function of yi, we do not need to compute its derivatives with respect to yi to train the neural network. We only need those of L, and can treat c∗ as a constant when evaluating them. Therefore, the only
difference between using our approach and the standard one
of Section III-A is that instead of evaluating the loss using the original annotation c, we use its optimized version c∗. We
call this approach SnakeFull and it is depicted at the top of
Fig. 3.

E. Speeding Things Up

We will show in Section IV that SnakeFull performs well but is slow to train. The culprit is the term ∂∂Lc in the update Eq. 11, which involves a time-consuming computation of the

gradient of a distance map. To speed things up, we introduce

a faster approach that we call SnakeFast. In it, we replace the

term L in Eq. 5 by a simpler objective function S directly

inspired by the classical external snake energy [16]. We take

it to be

S(c, y) = y ∗ G [cv],

(13)

v∈V

where ∗G denotes a convolution with a Gaussian kernel and y[cv] denotes the network output at vertex v. S is very

c inaccurate annotation c
network output y

snake adjustment minc L + R c∗

y
∂L ∂y
SnakeFull

∂∂Lc + ∂∂Rc ≈ 0
loss computation L+R

c inaccurate annotation c
network output y

snake adjustment mincS + R
∂c† ∂L ∂y ∂c
y
∂L ∂y

c†
∂L ∂c
loss computation L

SnakeFast

c inaccurate annotation c

snake adjustment mincS + R c†

network output y

y
∂L ∂y

loss computation L

SnakeSimple

Fig. 3. The three approaches to training described in Sec. III-D and IIIE. In SnakeFull, the training objective is also used as the objective of the snake. This makes some gradient components vanish, simplifying gradient computation, but results in snake updates that are costly to compute. SnakeFast can accommodate an arbitrary snake objective, which makes it faster than SnakeFull, even though it requires backpropagation through a sequence of snake updates. In SnakeSimple, the backpropagation over the snake updates is simply omitted. This approach is the fastest. We analyze the accuracy vs. speed tradeoff induced by these three methods in section IV.

similar to the energies used in traditional network snake formulations [11], [5]. Importantly, S and its gradients are easy and fast to compute because doing so only requires convolving y with a Gaussian kernel and sampling the result at the locations of the snake nodes. Deforming the annotations then involves ﬁnding

c†i (yi) = arg min S(c, yi) + R(c),

(14)

c

which means that the sum of distance values along the

snake should be as low as possible while preserving snake

smoothness. As in Section III-C, the snake update takes the

form cti+1 = (A + γI)−1 γcti − ∂∂Sc (cti, yi) . (15)

In practice, we take c†i (yi)

=

c

T i

,

where

T

=

10, as in

Section III-C. Finally, we take the network training objective

Initial distance map and inaccurate annotation

5

Snake at convergence and distance map updated after 100 GD iterations

SnakeFull

SnakeFast

SnakeSimple

dsd Ground-truth distance map and Difference between the ground-truth distance map and updated one after 100 GD iterations
accurate annotation

dsd

computation time [s]:

72

12

5.3

Fig. 4. Compared behavior of SnakeSimple, SnakeFast, and SnakeFull on a synthetic 2D example. (Left column) At the bottom, distance map and corresponding annotation. At the top, we simulated an unwarranted break in the distance map (horizontal yellow line) and shifted the annotation by several pixels. (Other Columns) In three separate runs, we performed 100 Gradient Descent using either SnakeFull, SnakeFast, or SnakeSimple. In the top row, we show the corrected annotation and the updated distance maps. The bottom row depicts the differences between the updated maps and the ground-truth one. We also indicate the computation times. SnakeFull removes the interruption in the distance map but the computation is slow. SnakeFast is much faster and ﬁlls the gap in the distance map almost as well. SnakeSimple is even faster but yields a corrected annotation that is too short, as highlighted by the red arrow.

to be

Θ∗ = arg min

L(c

† i

(y

i

)

,

yi

)

,

(16)

Θ

i

where we still use the original L of Eq. 2. We do this because S only depends on a small subset of voxels of y. Hence, it only provides a sparse supervisory signal and is not well suited as the training objective for the network that produces a dense distance map. The gradient of the objective of Eq. 16 is

∂ L(c†, y) = ∂L (c†, y) + ∂L (c†, y) ∂c† . (17)

∂y

∂y

∂c

∂y

Because we minimized S instead of L in Eq. 14, we can no longer assume that the second term is zero as we did in Section III-D. Hence, to compute it during the minimization, we backpropagate through the snake update procedure of Eq. 15, as depicted by the middle row of Fig. 3. In practice, we use the autograd functionality of Pytorch to this end.
The non-zero second term of Eq. 17 helps guide the snake to a position where the data loss L is low and ultimately inﬂuences the distance map that our deep network F outputs. It could be argued that ignoring this term so that the networks focuses exclusively on ﬁtting the annotations would be preferable. To test this assertion, we implemented

SnakeSimple, a third variant or our approach in which we take the second term of Eq. 17 to be zero. SnakeSimple is even faster than SnakeFast. In essence, it is a simpliﬁed version of SnakeFull and SnakeFast in which we successively optimize the network weights and then the snake position without any direct interaction between these two optimization steps.
Fig. 4 uses a synthetic example to illustrates the differences between our three variants. SnakeFast and SnakeFull yield similar results with the former being much faster whereas SnakeSimple is even faster but prone to generating artifacts. We now turn to our experimental results on real data that conﬁrm this.
IV. EXPERIMENTS
A. Datasets
We tested our approach on two datasets of three-dimensional neuron scans.
• The Brain dataset comprises fourteen 3D stacks of twophoton microscopy images of fragments of a mouse brain, with manually traced axons and dendrites. We use four volumes for testing and ten for training, each of size 200 × 250 × 250 voxels and spatial resolution

6

0.3 × 0.3 × 1.0 µm. Fig. 5 features maximum-intensity projections of two of these volumes. • The Neurons dataset contains two 3D images of neurons in a mouse brain. The axons and dendrites have been outlined manually while viewing the sample under a microscope and the image has been captured later. The sample deformed in the meantime, resulting in a misalignment between the annotation and the image. We use one stack of size 151 × 714 × 865 voxels and a spatial resolution of 1 µm for training and one of size 228 × 764 × 1360 pixels for testing. Fig. 6 shows a part of the test stack. • The MRA dataset is a publicly available set of Magnetic Resonance Angiography brain scans [4], two of which is shown in Fig. 7. It consists of 42 annotated stacks, which we cropped to a size of 416 × 320 × 128 voxels by removing their empty margins. Their resolution is 0.5 × 0.5 × 0.6 mm. We randomly partitioned the data into 31 training and 11 test volumes.
B. Metrics
We used the following metrics to evaluate the performance of our algorithms:
• CCQ. Since standard segmentation metrics such as the F1 score [31] and precision-recall break-even point [23] are very sensitive to misalignment of thin structures, we use the correctness-completeness-quality, which is specifically designed for linear structures [39]. Correctness corresponds to precision, completeness to recall, and quality to the intersection-over-union. However, the notion of a true positive is relaxed from perfect coincidence of the ground truth and the prediction to their co-occurrence within a distance of d pixels. For all our experiments, we use d = 3. Even though it accounts for the possible misalignment of the ground truth, CCQ is still a voxelwise metric, insensitive to a number of topological errors, such as short interruptions of neurites.
• APLS. The Average Path Length Similarity is deﬁned as the aggregation of relative length difference of shortest paths between pairs of randomly sampled corresponding points in the reconstructed and predicted graphs. It has been introduced to evaluate road map reconstructions from aerial images [35] and aims to evaluate the connectivity of the reconstructions, as opposed to their pixelwise accuracy, which makes it a perfect performance measure for our task.
• TLTS. The Too-Long-Too-Short is another performance criterion based on statistics of relative lengths of shortest paths between corresponding pairs of end points in the prediction and the ground truth [38]. The predicted paths are classiﬁed with respect to their length, relative to the corresponding ground truth paths, as too long, too short, correct, or infeasible, if a ground truth path is not reﬂected in the prediction. The single-number value we use to evaluate our results is the fraction of correct paths, with relative length difference not larger than 15%.

TABLE I PERFORMANCE OF DEEP NETS TRAINED WITH DIFFERENT LOSS FUNCTIONS ON THE Brain DATA SET AND THE TIME NEEDED FOR
SINGLE TRAINING ITERATION.

Pixel-wise

Arch. Method

Corr. Compl. Qual.

OrigAnnotations 98.9 91.3 90.4

U-Net SnakeSimple SnakeFast

98.4 92.5 91.2 98.7 95.0 93.8

SnakeFull

99.0 94.4 93.5

OrigAnnotations 97.2 94.0 91.5

DRU SnakeSimple SnakeFast

97.4 95.2 92.9 97.0 97.1 94.2

SnakeFull

96.9 96.9 94.1

Topology-aware
APLS TLTS
80.3 80.9 84.2 83.4 91.1 85.9 89.3 85.9
84.3 83.9 90.8 85.9 91.7 88.1 91.8 89.3

iter. t.
s
2.8 3.8 5.2 18.9
2.7 3.8 5.3 19.1

C. Architectures and Training Details
Our contribution lies in the updating of the annotations and the loss function we use to achieve it, which should improve performance independently of any speciﬁc network architecture. To demonstrate this, we used two different architectures in our experiments.
• U-Net. A 3D U-Net [30] with three max-pooling layers and two convolutional blocks. The ﬁrst layer has 64 ﬁlters. Each convolution layer is followed by a batchnormalization and dropout with a probability of 0.15. During training, we randomly crop sub-volumes of size 96 × 96 × 96 and ﬂip them along each dimension with probability 0.5. We combine them into batches of 8.
• DRU. A recurrent architecture iteratively reﬁning segmentation output 3 times [36]. The ﬁrst layer has 64 ﬁlters. Each convolution layer is followed by a groupnormalization and dropout with a probability of 0.15. During training, outputs of all reﬁnement steps are used to compute losses and the average of them used to train the network. During testing, the output of the ﬁnal reﬁnement step is used to evaluate the performance. During training, we randomly crop sub-volumes of size 96 × 96 × 96 and ﬂip them along each dimension with probability 0.5. We combine them into batches of 4.
We trained both architectures in four different ways. The ﬁrst is the standard one described in Section III-A in which we use the original annotations and simply minimize the Mean Squared Error, which we will refer to as OrigAnnotations. The other three are the SnakeSimple, SnakeFull, and SnakeFast variants of our approach, as described in Sections III-D and III-E and depicted by Fig. 3. In all cases, we used Adam [17] with the learning rate set to 1e−4, and a weight decay of 1e−4. At test time, the predicted distance map were thresholded at 2 and skeletonized to obtain centerlines. To compute the TLTS and APLS scores, we converted these centerlines into graphs.
D. Comparative Evaluation.
As shown in Tables I, II and III, SnakeFull and SnakeFast outperform OrigAnnotations in CCQ terms by a small margin, and in APLS and TLTS terms by a signiﬁcantly larger one,

7

TABLE II PERFORMANCE OF DEEP NETS TRAINED WITH DIFFERENT LOSS FUNCTIONS ON THE Neurons DATA SET AND THE TIME NEEDED FOR
SINGLE TRAINING ITERATION.

TABLE VI PERFORMANCE OF U-Net TRAINED USING SnakeFast AND OrigAnnotations ON THE Brain DATA SET WITH VERY COARSE ANNOTATIONS. PERFORMANCE OF U-Net TRAINED USING THE PRECISE
ANNOTATIONS SHOWN FOR REFERENCE.

Pixel-wise

Arch. Method

Corr. Compl. Qual.

OrigAnnotations 81.8 83.5 70.4

U-Net SnakeSimple SnakeFast

83.0 83.9 71.6 83.1 85.5 72.9

SnakeFull

83.5 85.4 73.1

OrigAnnotations 82.1 86.5 72.8

DRU SnakeSimple SnakeFast

83.2 87.7 74.5 84.2 88.9 76.2

SnakeFull

84.4 88.5 76.1

Topology-aware
APLS TLTS
65.8 63.6 70.4 68.8 73.9 70.2 74.2 69.9
68.9 69.5 73.8 74.6 75.1 77.7 74.8 78.1

iter. t.
s
2.8 3.4 4.9 17.8
2.7 3.5 5.1 18.3

TABLE III PERFORMANCE OF DEEP NETS TRAINED WITH OUR LOSS FUNCTIONS ON THE MRA DATA SET AND THE TIME NEEDED FOR SINGLE TRAINING
ITERATION.

Pixel-wise

Arch. Method

Corr. Compl. Qual.

OrigAnnotations 90.1 72.2 66.9

U-Net SnakeSimple SnakeFast

89.9 73.1 67.5 90.3 73.5 68.1

SnakeFull

90.2 73.5 68.0

OrigAnnotations 80.2 79.3 66.3

DRU SnakeSimple SnakeFast

80.7 79.9 67.1 81.0 80.5 67.7

SnakeFull

80.9 80.5 67.6

Topology-aware
APLS TLTS
49.8 50.4 53.5 53.1 55.4 55.2 55.6 55.0
48.7 49.9 53.3 53.0 55.3 55.4 55.6 55.2

iter. t.
s
2.8 3.7 5.1 18.5
2.7 3.7 5.2 18.8

TABLE IV PERFORMANCE OF U-Net TRAINED USING SnakeFast ON THE Brain DATA SET WHEN VARYING THE ELASTICITY AND SPRING TERM COEFFICIENTS.

Pixel-wise

Method

Annot. Corr. Compl. Qual.

OrigAnnotations coarse 85.2 67.6 60.4

SnakeFast

coarse 97.6 87.0 85.3

OrigAnnotations precise 98.9 91.3 90.4

SnakeFast

precise 98.7 95.0 93.8

Topology-aware
APLS TLTS
46.5 50.9 66.8 73.5
80.3 80.9 91.1 85.9

which conﬁrms that the main beneﬁt of our loss is the improved connectivity of the predictions. As can be seen in Figs 5, 6, and 7, our approach to training yields delineations with fewer unwarranted breaks and longer uninterrupted curvilinear segments.
On average SnakeFull and SnakeFast performs best. However, SnakeFast requires three times less time per training iteration. SnakeSimple delivers a further 20-30% speedup but incurs a clear performance drop. Crucially, these conclusions apply to both the U-Net and DRU architectures. In fact, the performance gain resulting from switching from OrigAnnotations to SnakeFast is larger than the one resulting from changing from the simpler U-Net to the more sophisticated DRU while retaining the standard OrigAnnotations approach to training.
In short, SnakeFast represents an excellent compromise between training time and performance. This being said, at test time, the run-time is the same no matter how the network was trained and there is no alignment of annotations anymore. Hence, given sufﬁcient computational resources, SnakeFull is also a valid option.

Pixel-wise

Corr. Compl. Qual.

β = 1e − 4 99.0 94.5 93.5 α = 1e − 2 β = 1e − 3 98.7 95.0 93.8
β = 1e − 2 98.4 94.0 92.7 β = 1e − 1 98.9 93.8 92.8

α = 1e − 4

98.4 92.9 91.5

α = 1e − 3 β = 1e − 3 99.0 94.2 93.4

α = 1e − 2

98.7 95.0 93.8

α = 1e − 1

98.7 94.3 93.1

Topology-aware
APLS TLTS
88.1 84.8 91.1 85.9 85.1 84.3 83.8 84.1
86.6 83.0 85.3 84.4 91.1 85.9 79.8 82.5

iter. t.
s
5.2 5.2 5.2 5.2
5.2 5.2 5.2 5.2

TABLE V PERFORMANCE OF U-Net TRAINED USING SnakeFast ON THE Brain DATA
SET WHEN VARYING THE INVERSE STEPSIZE, TOGETHER WITH THE
NUMBER OF SNAKE UPDATES USED IN EVERY TRAINING ITERATION AND
THE RESULTING ITERATION TIME.

Pixel-wise

Topology-aware

Corr. Compl. Qual. APLS TLTS

γ = 100 98.8 γ = 10 98.7 γ=1

94.5 93.4 90.9 85.8 95.0 93.8 91.1 85.9 — the snake diverged —

no steps
80 10 10

iter. t.
s
6.3 5.2 5.2

E. Ablation Studies.
a) Regularization terms: The regularization term R of Eq. 7 is the sum of a spring term, weighted by a coefﬁcient α, and an elasticity term, weighted by a coefﬁcient β. To investigate their inﬂuence on performance, we varied α and β and trained our U-Net on the Brain data set. The results are presented in Tab. IV. The best results are attained with relatively low values of both terms. Higher values of the spring term, originally proposed for closed contours, effectively reguralize loopy topologies, but when used on tree-shaped structures, representing blood vessels and neuronal processes, tend to shorten the reconstructed neurites and vessels. Higher values of the elasticity term make it more difﬁcult to ﬁt irregular trajectories of neurites, like the ones shown in Figs 5 and 6.
b) Step size for snake update: As explained in section III-C, the snake update iteration has a parameter γ, called viscosity, that acts as an inverse step size. We report the results of changing γ in Tab. V. Low viscosity results in large step size and can make the snake update procedure diverge, which we observed for γ = 1. On the other hand, high viscosity corresponds to small step size and increases the risk that the snake does not converge within the preset number of iterations. With γ = 100, we needed to increase the number

8

of snake updates from 10 to 80 to ensure convergence. This also increased the iteration time by one second. γ = 10 made the snake converge within 10 updates, while also resulting in marginally higher performance than γ = 100.
c) Resilience to lack of precision in the annotations: To quantize the effect of the lack of precision in the annotations, we trained the U-Net with SnakeFast and OrigAnnotations on the Brain data set with very coarse annotations, like the ones in Fig. 8. We obtained these coarse annotations by connecting neurite branching- and end-points with straight lines. The results are presented in Tab. VI, and example segmentations are shown in Fg. 9. As expected, training on the coarse annotations without adjusting them results in a signiﬁcant performance drop as compared to training on precise annotations. Switching from precise to coarse annotations incurs a much smaller performance drop when using SnakeFast, which shows that it is indeed robust to lack of precision in the annotations. The gap between performance of SnakeFast on the coarse and precise annotations shows the main limitation of our algorithm. The snake converges to a local optimum, and therefore it may fail to correct very large annotation displacements.
V. CONCLUSION AND FUTURE WORK
We have proposed a method that accounts for the inevitable inaccuracies in manual annotations of curvilinear 3D structures, such as neurites and blood vessels, in 3D image stacks. It leverages on the network snake formalism to deﬁne a loss function that simultaneously trains the deep network to produce the delineation and adjusts the initially imprecise annotations.
Our approach does not depend on the speciﬁc network architecture we use. Hence, its effectiveness suggests that handling such imprecisions may be even more important than reﬁning the network architecture, which is something that has been largely neglected in the literature.
In future work, we will investigate the extension our approach to segmenting surfaces, like cell membranes in electron microscopy scans.
REFERENCES
[1] D. Acuna, A. Kar, and S. Fidler. Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations. In Conference on Computer Vision and Pattern Recognition, 2019.
[2] Y. Bengio, A. Courville, and P. Vincent. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
[3] D. Breitenreicher, M. Sofka, S. Britzen, and S.K. Zhou. Hierarchical Discriminative Framework for Detecting Tubular Structures in 3D Images. In Conference on Medical Image Computing and Computer Assisted Intervention, pages 328–340, 2013.
[4] E. Bullitt, D. Zeng, G. Gerig, S. Aylward, S. Joshi, J. Smith, W. Lin, and M. Ewend. Vessel Tortuosity and Brain Tumor Malignancy: A Blinded Study. Acad Radiol, 12(10):1232–1240, October 2005.
[5] M. Butenuth and C. Heipke. Network Snakes: Graph-Based Object Delineation with Active Contour Models. Machine Vision and Applications, 23(1):91–109, 2012.
[6] D. Cheng, R. Liao, S. Fidler, and R. Urtasun. DARNet: Deep Active Ray Network for Building Segmentation. In Conference on Computer Vision and Pattern Recognition, 2019.
[7] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active Appearance Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6), June 2001.

[8] C. Doersch and A. Zisserman. Multi-Task Self-Supervised Visual Learning. In International Conference on Computer Vision, October 2017.
[9] O. Doruk, K. Mateusz, C. Leonardo, N. Dadap, A. Konings, and P. Fua. Promoting Connectivity of Network-Like Structures by Enforcing Region Separation. In arXiv Preprint, 2020. Submitted for publication to PAMI.
[10] A.F. Frangi, W.J. Niessen, K.L. Vincken, and M.A. Viergever. Multiscale Vessel Enhancement Filtering. Lecture Notes in Computer Science, 1496:130–137, 1998.
[11] P. Fua. Model-Based Optimization: Accurate and Consistent Site Modeling. In International Society for Photogrammetry and Remote Sensing, July 1996.
[12] P. Fua and Y. G. Leclerc. Model Driven Edge Detection. Machine Vision and Applications, 3:45–56, 1990.
[13] Y. Ganin and V. Lempitsky. N4-Fields: Neural Network Nearest Neighbor Fields. In Asian Conference on Computer Vision, pages 536– 551, 2014.
[14] A. Hatamizadeh, D. Sengupta, and D. Terzopoulos. End-To-End Trainable Deep Active Contour Models for Automated Image Segmentation: Delineating Buildings in Aerial Imagery. In arXiv Preprint, 2020.
[15] X. Huang and L. Zhang. Road Centreline Extraction from HighResolution Imagery Based on Multiscale Structural Features and Support Vector Machines. International Journal of Remote Sensing, 30:1977– 1987, 2009.
[16] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active Contour Models. International Journal of Computer Vision, 1(4):321–331, 1988.
[17] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimisation. In International Conference on Learning Representations, 2015.
[18] M. Kozin´ski, A. Mosinska, M. Salzmann, and P. Fua. Tracing in 2D to Reduce the Annotation Effort for 3D Deep Delineation of Linear Structures. Medical Image Analysis, 60, 2020.
[19] M. Law and A. Chung. Three Dimensional Curvilinear Structure Detection Using Optimally Oriented Flux. In European Conference on Computer Vision, pages 368–382, 2008.
[20] K.K. Maninis, J. Pont-Tuset, P. Arbela´ez, and L. Van Gool. Deep Retinal Image Understanding. In Conference on Medical Image Computing and Computer Assisted Intervention, pages 140–148, 2016.
[21] D. Marcos, D. Tuia, B. Kellenbergerg, and R. Urtasun. Learning Deep Structured Active Contours End-To-End. In Conference on Computer Vision and Pattern Recognition, 2018.
[22] G. Ma´ttyus, W. Luo, and R. Urtasun. Deeproadmapper: Extracting Road Topology from Aerial Images. In International Conference on Computer Vision, pages 3458–3466, 2017.
[23] V. Mnih. Machine Learning for Aerial Image Labeling. PhD thesis, University of Toronto, 2013.
[24] V. Mnih and G.E. Hinton. Learning to Detect Roads in High-Resolution Aerial Images. In European Conference on Computer Vision, pages 210– 223, 2010.
[25] A. Mosin´ska, M. Kozinski, and P. Fua. Joint Segmentation and Path Classiﬁcation of Curvilinear Structures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(6):1515–1521, 2020.
[26] A. Mosin´ska, P. Marquez-Neila, M. Kozinski, and P. Fua. Beyond the Pixel-Wise Loss for Topology-Aware Delineation. In Conference on Computer Vision and Pattern Recognition, pages 3136–3145, 2018.
[27] H. Peng, J. Tang, H. Xiao, A. Bria, J. Zhou, V. Butler, Z. Zhou, P.T. Gonzalez-Bellido, S.W. Oh, and C. A. others. Virtual Finger Boosts Three-Dimensional Imaging and Microsurgery as Well as Terabyte Volume Image Visualization and Analysis. Nature Communications, 5:4342–4355, 2014.
[28] H. Peng, Z. Zhou, E.Meijering, T.Zhao, G.A. Ascoli, and M.Hawrylycz. Automatic Tracing of Ultra-Volumes of Neuronal Images. Nature Methods, 14:332–333, 2017.
[29] K. Ramnath, S. Baker, I. Matthews, and D. Ramanan. Increasing the Density of Active Appearance Models. In Conference on Computer Vision and Pattern Recognition, 2008.
[30] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Conference on Medical Image Computing and Computer Assisted Intervention, pages 234–241, 2015.
[31] M. Seyedhosseini, M. Sajjadi, and T. Tasdizen. Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks. In International Conference on Computer Vision, 2013.
[32] A. Sironi, E. Turetken, V. Lepetit, and P. Fua. Multiscale Centerline Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(7):1327–1341, 2016.

input

OrigAnnotations

SnakeSimple

SnakeFast

9
SnakeFull

U-Net

DRU

Fig. 5. Qualitative comparison of the test results on the Brain data set. The green ellipses denote areas where training with the original annotations results in unwarranted breaks in the delineations at test time whereas our approach does not.

[33] D. Terzopoulos, A. Witkin, and M. Kass. Constraints on Deformable Models: Recovering 3D Shape and Nonrigid Motion. Artiﬁcial Intelligence, 36(1):91–123, 1988.
[34] E. Turetken, C. Becker, P. Glowacki, F. Benmansour, and P. Fua. Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-Directional Oriented Flux. In International Conference on Computer Vision, pages 1553–1560, December 2013.
[35] A. Van Etten. Spacenet Road Detection and Routing Challenge Part II — APLS Implementation.
[36] W. Wang, K. Yu, J. Hugonot, P. Fua, and M. Salzmann. Recurrent U-Net for Resource-Constrained Segmentation. In International Conference on Computer Vision, 2019.
[37] Z. Wang, D. Acuna, H. Ling, A. Kar, and S. Fidler. Object Instance Annotation with Deep Extreme Level Set Evolution. In European Conference on Computer Vision, 2020.
[38] J.D. Wegner, J.A. Montoya-Zegarra, and K. Schindler. A Higher-Order CRF Model for Road Network Extraction. In Conference on Computer

Vision and Pattern Recognition, pages 1698–1705, 2013. [39] C. Wiedemann, C. Heipke, H. Mayer, and O. Jamet. Empirical Evalu-
ation of Automatically Extracted Road Axes. In Empirical Evaluation Techniques in Computer Vision, pages 172–187, 1998. [40] D. Wu, D. Liu, Z. Puskas, C. Lu, A. Wimmer, C. Tietjen, G. Soza, and S. K. Zhou. A Learning Based Deformable Template Matching Method for Automatic Rib Centerline Extraction and Labeling in CT Images. In Conference on Computer Vision and Pattern Recognition, 2012. [41] Z. Yu, W. Liu, Y. Zou, C. Feng, S. Ramalingam, K. Vijaya, and J. Kautz. Simultaneous Edge Alignment and Learning. In European Conference on Computer Vision, 2018. [42] Z. Zhou, X. Liu, B. Long, and H. Peng. TReMAP: Automatic 3D Neuron Reconstruction Based on Tracing, Reverse Mapping and Assembling of 2D Projections. Neuroinformatics, 14(1):41–50, January 2016.

10

input

OrigAnnotations

SnakeSimple

SnakeFast

SnakeFull

U-Net

DRU

Fig. 6. Qualitative comparison of the test results on the Neurons data set. The green ellipses denote areas where training with the original annotations results in unwarranted breaks in the delineations at test time whereas our approach does not.

11

input

OrigAnnotations

SnakeSimple

SnakeFast

SnakeFull

U-Net

DRU

Fig. 7. Qualitative comparison of the test results on the MRA data set. The green ellipses denote areas where training with the original annotations results in unwarranted breaks in the delineations at test time whereas our approach does not.

12

(a)

(b)

(c)

Fig. 8. Easy Annotations (a) Training image of a neurite (b) Distance map obtained from original annotation overlaid in red (c) Distance map obtained from easy

annotation overlaid in red. Easy annotation is obtained by annotating just the end points and bifurcation points of the neurite.

input

OrigAnnotations

SnakeFast

Fig. 9. Qualitative comparison of the test results on the Brain data set with easy annotations. The green ellipses denote areas where training with the original annotations results in unwarranted breaks in the delineations at test time whereas our approach does not.

