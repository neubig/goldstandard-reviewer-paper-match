Findings of the Second Workshop on Neural Machine Translation and Generation
Alexandra Birch♠, Andrew Finch♥, Minh-Thang Luong♣, Graham Neubig♦, Yusuke Oda◦ ♠University of Edinburgh, ♥Apple, ♣Google Brain, ♦Carnegie Mellon University ◦Google Translate

arXiv:1806.02940v3 [cs.CL] 18 Jun 2018

Abstract
This document describes the ﬁndings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop’s shared task on efﬁcient neural machine translation (NMT), where participants were tasked with creating NMT systems that are both accurate and efﬁcient.
1 Introduction
Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simpliﬁcation. The 2nd Workshop on Neural Machine Translation and Generation (WNMT 2018) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals:
First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: This year we will continue to encourage submissions that not only advance the state of

the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this goal, we received a number of high-quality research contributions on the topics of linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models, which are summarized in Section 2.
Second, it aimed to expand the research horizons in NMT: Based on panel discussions from the ﬁrst workshop, we organized a shared task. Speciﬁcally, the shared task was on “Efﬁcient NMT”. The aim of this task was to focus on not only accuracy, but also memory and computational efﬁciency, which are paramount concerns in practical deployment settings. The workshop provided a set of baselines for the task, and elicited contributions to help push forward the Pareto frontier of both efﬁciency and accuracy. The results of the shared task are summarized in Section 3
2 Summary of Research Contributions
We published a call for long papers, extended abstracts for preliminary work, and crosssubmissions of papers submitted to other venues. The goal was to encourage discussion and interaction with researchers from related areas. We received a total of 25 submissions, out of which 16 submissions were accepted. The acceptance rate was 64%. Three extended abstracts, two crosssubmissions and eleven long papers were accepted after a process of double blind reviewing.
Most of the papers looked at the application of machine translation, but there is one paper on abstractive summarization (Fan et al., 2018) and one paper on automatic post-editing of translations (Unanue et al., 2018).
The workshop proceedings cover a wide range of phenomena relevant to sequence to sequence

model research, with the contributions being concentrated on the following topics:
Linguistic structure: How can we incorporate linguistic structure in neural MT or generation models? Contributions examined the effect of considering semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018).
Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018)
Data augmentation: A number of the contributed papers examined ways to augment data for more efﬁcient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018)
Inadequate resources: Several contributions involved settings in which resources were insufﬁcient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018).
Model analysis: There were also many methods that analyzed modeling and design decisions, including investigations of individual neuron contributions (Bau et al., 2018), parameter sharing (Jean et al., 2018), controlling output characteristics (Fan et al., 2018), and shared attention (Unanue et al., 2018)
3 Shared Task
Many shared tasks, such as the ones run by the Conference on Machine Translation (Bojar et al., 2017), aim to improve the state of the art for MT with respect to accuracy: ﬁnding the most accurate MT system regardless of computational cost. However, in production settings, the efﬁciency of the implementation is also extremely important. The shared task for WNMT (inspired by the “small NMT” task at the Workshop on Asian

Translation (Nakazawa et al., 2017)) was focused on creating systems for NMT that are not only accurate, but also efﬁcient. Efﬁciency can include a number of concepts, including memory efﬁciency and computational efﬁciency. This task concerns itself with both, and we cover the detail of the evaluation below.
3.1 Evaluation Measures
The ﬁrst step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU.
Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores.
Computational Efﬁciency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty ﬁle, then subtracting this from the total time to translate the test set ﬁle.
Memory Efﬁciency Measures: We measured: (1) the size on disk of the model, (2) the number of parameters in the model, and (3) the peak consumption of the host memory and GPU memory.
These metrics were measured by having participants submit a container for the virtualization environment Docker1, then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2, speciﬁcally of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU).
3.2 Data
The data used was from the WMT 2014 EnglishGerman task (Bojar et al., 2014), using the preprocessed corpus provided by the Stanford NLP Group3. Use of other data was prohibited.
1https://www.docker.com/ 2https://aws.amazon.com/ 3https://nlp.stanford.edu/projects/ nmt/

BLEU (%)

30

Marian-Trans-Big Marian-Trans-Big-int8

Marian-Trans-Base-AAN

28

Marian-Trans-Small-AAN OpenNMT-Small

26 OpenNMT-Tiny 24

22

20

Baseline

18

102

103

Time (s, log scale)

(a) CPU Time vs. Accuracy

BLEU (%)

Marian-Trans-Big

30

Marian-Trans-Base-AAN

NICT

Marian-Trans-Small-AAN

28 26 Marian-TinyRNN Amun-MLSTM

24

22

20 Amun-FastGRU 18100

101 Time (s, log scale)

Baseline

(b) GPU Time vs. Accuracy

30

Marian-Trans-Big Marian-Trans-Base-AAN Marian-Trans-Big-int8

Marian-Trans-Small-AAN

28

OpenNMT-Small

26 OpenNMT-Tiny

24

22

20
Baseline
18 250 500 750 1000 1250 1500 1750 2000 Memory (MB)

(c) CPU Memory vs. Accuracy

BLEU (%)

Marian-Trans-Big

30

Marian-Trans-Base-NAIACNT

Marian-Trans-Small-AAN

28

26

Marian-TinyRNN

Amun-MLSTM

24

22

20

Amun-FastGRU

Baseline

18

2 × 103

3 × 103 4 × 103

6 × 103

Memory (MB)

(d) GPU Memory vs. Accuracy

Figure 1: Time and memory vs. accuracy measured by BLEU, calculated on both CPU and GPU

BLEU (%)

3.3 Baseline Systems
Two baseline systems were prepared:
Echo: Just send the input back to the output.
Base: A baseline system using attentional LSTMbased encoder-decoders with attention (Bahdanau et al., 2015).
3.4 Submitted Systems
Four teams, Team Amun, Team Marian, Team OpenNMT, and Team NICT submitted to the shared task, and we will summarize each below. Before stepping in to the details of each system, we ﬁrst note general trends that all or many systems attempted. The ﬁrst general trend was a fast C++ decoder, with Teams Amun, Marian, and NICT using the Amun or Marian decoders included in the Marian toolkit,4 and team OpenNMT
4https://marian-nmt.github.io

using the C++-decoder decoder for OpenNMT.5. The second trend was the use of data augmentation techniques allowing the systems to train on data other than the true references. Teams Amun, Marian, and OpenNMT all performed model distillation (Kim and Rush, 2016), where a larger teacher model is used to train a smaller student model, while team NICT used back translation, training the model on sampled translations from the target to the source (Imamura et al., 2018). Finally, a common optimization was the use of lower-precision arithmetic, where Teams Amun, Marian, and OpenNMT all used some variety of 16/8-bit or integer calculation, along with the corresponding optimized CPU or GPU operations. These three improvements seem to be best practices for efﬁcient NMT implementation.
5http://opennmt.net

3.4.1 Team Amun
Team Amun’s contribution (Hoang et al., 2018a) was based on the “Amun” decoder and consisted of a number of optimizations to improve translation speed on GPU. The ﬁrst major unique contribution was a strategy of batching together computations from multiple hypotheses within beam search to exploit parallelism of hardware. Another contribution was a methodology to create a fused GPU kernel for the softmax calculation, that calculates all of the operations within the softmax (e.g. max, exponentiation, and sum) in a single kernel. In the end they submitted two systems, Amun-FastGRU and Amun-MLSTM, which use GRU (Cho et al., 2014) and multiplicative LSTM (Krause et al., 2016) units respectively.
3.4.2 Team Marian
Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efﬁcient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models.
3.4.3 Team OpenNMT
Team OpenNMT (Senellart et al., 2018) built a system based on the OpenNMT toolkit. The model was based on a large self-attentional teacher model distilled into a smaller, fast RNN-based model. The system also used a version of vocabulary selection (Shi and Knight, 2017), and a method to increase the size of the encoder but decrease the size of the decoder to improve the efﬁciency of beam search. They submitted two systems, OpenNMT-Small and OpenNMT-Tiny, which were two variously-sized implementations of this model.

3.4.4 Team NICT
Team NICT’s contribution (Imamura and Sumita, 2018) to the shared task was centered around using self-training as a way to improve NMT accuracy without changing the architecture. Speciﬁcally, they used a method of randomly sampling pseudo-source sentences from a back-translation model (Imamura et al., 2018) and used this to augment the data set to increase coverage. They tested two basic architectures for the actual translation model, a recurrent neural network-based model trained using OpenNMT, and a self-attentional model trained using Marian, ﬁnally submitting the self-attentional model using Marian as their sole contribution to the shared task NICT.
3.5 Shared Task Results
A brief summary of the results of the shared task (for newstest2015) can be found in Figure 1, while full results tables for all of the systems can be found in Appendix A. From this ﬁgure we can glean a number of observations.
First, encouragingly all the submitted systems handily beat the baseline system in speed and accuracy.
Secondly, observing the speed/accuracy curves, we can see that Team Marian’s submissions tended to carve out the Pareto frontier, indicating that the large number of optimizations that went into creating the system paid off in aggregate. Interestingly, on GPU, RNN-based systems carved out the faster but less accurate part of the Pareto curve, while on CPU self-attentional models were largely found to be more effective. None of the submissions consisted of a Transformer-style model so small that it under-performed the RNN models, but a further examination of where the curves cross (if they do) would be an interesting examination for future shared tasks.
Next, considering memory usage, we can see again that the submissions from the Marian team tend to be the most efﬁcient. One exception is the extremely small memory system OpenNMT-Tiny, which achieves signiﬁcantly lower translation accuracies, but ﬁts in a mere 220MB of memory on the CPU.
In this ﬁrst iteration of the task, we attempted to establish best practices and strong baselines upon which to build efﬁcient test-time methods for NMT. One characteristic of the ﬁrst iteration of the task was that the basic model architectures

used relatively standard, with the valuable contributions lying in solid engineering work and best practices in neural network optimization such as low-precision calculation and model distillation. With these contributions, we now believe we have very strong baselines upon which future iterations of the task can build, examining novel architectures or methods for further optimizing the training speed. We also will examine other considerations, such as efﬁcient adaptation to new training data, or latency from receiving a sentence to translating it.
4 Conclusion
This paper summarized the results of the Second Workshop on Neural Machine Translation and Generation, where we saw a number of research advances, particularly in the area of efﬁciency in neural MT through submissions to the shared task. The workshop series will continue next year, and continue to push forward the state of the art on these topics for faster, more accurate, more ﬂexible, and more widely applicable neural MT and generation systems.
Acknowledgments
We would like to warmly thank Amazon for its support of the shared task, both through its donation of computation time, and through its provision of a baseline system for participants to build upon. We also thank Google and Apple for their monetary support of the workshop.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. ICLR.
Joost Bastings, Wilker Aziz, Ivan Titov, and Khalil Simaan. 2018. Modeling latent sentence structure in neural machine translation. In Proc. WNMT.
D. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2018. On individual neurons in neural machine translation. In Proc. WNMT.
Yonatan Bisk and Ke Tran. 2018. Inducing grammars with and for neural machine translation. In Proc. WNMT.
Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara

Logacheva, et al. 2017. Findings of the 2017 conference on machine translation (wmt17). In Proc. WMT, pages 169–214.
Ondrej Bojar et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proc. WMT, pages 12–58.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proc. EMNLP, pages 1724–1734.
George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. HLT, pages 138–145.
Angela Fan, David Grangier, and Michael Auli. 2018. Controllable abstractive summarization. In Proc. WNMT.
Hieu Hoang, Tomasz Dwojak, Rihards Krislauks, Daniel Torregrosa, and Kenneth Heaﬁeld. 2018a. Fast neural machine translation implementation. In Proc. WNMT.
Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. 2018b. Iterative backtranslation for neural machine translation. In Proc. WNMT.
Kenji Imamura, Atsushi Fujita, and Eiichiro Sumita. 2018. Enhancement of encoder and attention using target monolingual corpora in neural machine translation. In Proc. WNMT.
Kenji Imamura and Eiichiro Sumita. 2018. Nict selftraining approach to neural machine translation at nmt-2018. In Proc. WNMT.
Se`bastien Jean, Stanislas Lauly, and Kyunghyun Cho. 2018. Parameter sharing strategies in neural machine translation. In Proc. WNMT.
Marcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu Hoang, Roman Grundkiewicz, and Anthony Aue. 2018. Marian: Cost-effective high-quality neural machine translation in c++. In Proc. WNMT.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. EMNLP, pages 1700–1709.
Huda Khayrallah and Philipp Koehn. 2018. On the impact of various types of noise on neural machine translation. In Proc. WNMT.
Huda Khayrallah, Brian Thompson, Kevin Duh, and Philipp Koehn. 2018. Regularized training objective for continued training for domain adaption in neural machine translation. In Proc. WNMT.
Yoon Kim and Alexander M. Rush. 2016. Sequencelevel knowledge distillation. In Proc. EMNLP, pages 1317–1327.

Sachith Sri Ram Kothur, Rebecca Knowles, and Philipp Koehn. 2018. Document-level adaptation for neural machine translation. In Proc. WNMT.
Ben Krause, Liang Lu, Iain Murray, and Steve Renals. 2016. Multiplicative lstm for sequence modelling. arXiv preprint arXiv:1609.07959.
Diego Marcheggiani, Joost Bastings, and Ivan Titov. 2018. Exploiting semantics in neural machine translation with graph convolutional networks. In Proc. WNMT.
Paul Michel and Graham Neubig. 2018. Extreme adaptation for personalized neural machine translation. In Proc. WNMT.
Toshiaki Nakazawa, Shohei Higashiyama, Chenchen Ding, Hideya Mino, Isao Goto, Hideto Kazawa, Yusuke Oda, Graham Neubig, and Sadao Kurohashi. 2017. Overview of the 4th workshop on asian translation. In Proc. WAT, pages 1–54, Taipei, Taiwan. Asian Federation of Natural Language Processing.
Yuta Nishimura, Katsuhito Sudoh, Graham Neubig, and Satoshi Nakamura. 2018. Multi-source neural machine translation with missing data. In Proc. WNMT.
Xing Niu, Michael Denkowski, and Marine Carpuat. 2018. Bi-directional neural machine translation with synthetic parallel data. In Proc. WNMT.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.
Ngoc-Quan Pham, Jan Niehues, and Alexander Waibel. 2018. Towards one-shot learning for rare-word translation with external experts. In Proc. WNMT.
Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proc. EMNLP, pages 379– 389.
Jean Senellart, Dakun Zhang, Bo Wang, Guillaume Klein, Jean-Pierre Ramatchandirin, Josep Crego, and Alexander Rush. 2018. OpenNMT system description for WNMT 2018: 800 words/sec on a single-core CPU. In Proc. WNMT.
Xing Shi and Kevin Knight. 2017. Speeding up neural machine translation decoding by shrinking run-time vocabulary. In Proc. ACL, pages 574–579.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Proc. NIPS, pages 3104–3112.
Inigo Jauregi Unanue, Ehsan Zare Borzeshi, and Massimo Piccardi. 2018. A shared attention mechanism for interpretation of neural automatic post-editing systems. In Proc. WNMT.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NIPS.
Oriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869.
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksˇic´, PeiHao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In Proc. EMNLP, pages 1711–1721.
Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accelerating neural transformer via an average attention network. In Proc. ACL.
A Full Shared Task Results
For completeness, in this section we add tables of the full shared task results. These include the full size of the image ﬁle for the translation system (Table 1), the comparison between compute time and evaluation scores on CPU (Table 2) and GPU (Table 3), and the comparison between memory and evaluation scores on CPU (Table 4) and GPU (Table 5).

Table 1: Image ﬁle sizes of submitted systems.

Team edin-amun Marian
NICT OpenNMT Organizer

System fastgru mlstm.1280 cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st cpu1 cpu2 echo nmt-1cpu nmt-1gpu

Size [MiB] 4823.43 5220.72 493.20 1085.93 1085.92 367.92 399.08 686.59 1279.32 564.32 2987.57 339.02 203.89 110.42 1668.25 3729.40

Table 2: Time consumption and MT evaluation metrics (CPU systems).

Dataset Empty
newstest2014
newstest2015

Team Marian
OpenNMT Organizer Marian
OpenNMT Organizer Marian
OpenNMT Organizer

System
cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu

Time Consumption [s]

CPU

Real

Diff

6.48

6.55

—

7.01

9.02

—

7.31

7.51

—

6.32

6.33

—

0.64

0.65

—

0.56

0.56

—

0.05

0.06

—

1.50

1.50

—

281.72 281.80 275.25

1539.34 1541.00 1531.98

1173.32 1173.41 1165.90

100.36 100.42 94.08

471.41 471.43 470.78

77.41 77.42 76.86

0.05

0.06

0.00

4436.08 4436.27 4434.77

223.86 223.96 217.41

1189.04 1190.97 1181.95

907.95 908.43 900.92

80.21 80.25 73.92

368.93 368.95 368.30

59.02 59.02 58.46

0.05

0.06

0.00

3401.99 3402.14 3400.64

BLEU %
— — — — — — — — 27.44 28.12 27.50 25.99 25.77 23.11 2.79 16.79 29.59 30.56 30.15 28.61 28.60 25.75 3.24 18.66

NIST
— — — — — — — — 7.362 7.436 7.355 7.169 7.140 6.760 1.479 5.545 7.452 7.577 7.514 7.312 7.346 6.947 1.599 5.758

Table 3: Time consumption and MT evaluation metrics (GPU systems).

Dataset Empty
newstest2014
newstest2015

Team edin-amun Marian
NICT Organizer edin-amun Marian
NICT Organizer edin-amun Marian
NICT Organizer

System
fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu

Time Consumption [s] CPU Real Diff 4.18 4.24 — 4.44 4.50 — 4.27 4.33 — 5.62 5.68 — 6.00 6.05 — 5.48 5.54 — 5.78 5.84 — 3.73 3.80 — 5.68 5.74 1.50 8.64 8.70 4.20 5.90 5.96 1.63 14.58 14.64 8.95 36.74 36.80 30.74 12.46 12.52 6.97 82.07 82.14 76.30 51.24 82.14 47.50 5.41 5.47 1.23 8.15 8.22 3.71 5.62 5.68 1.35 12.67 12.73 7.04 30.81 30.90 24.84 11.04 11.10 5.56 72.84 72.90 67.06 39.95 40.01 36.21

BLEU %
— — — — — — — — 17.74 23.85 24.06 27.80 28.34 26.34 27.59 16.79 19.26 26.51 26.86 30.10 30.87 28.87 30.19 18.66

NIST
— — — — — — — — 5.783 6.833 6.879 7.415 7.486 7.219 7.375 5.545 5.905 7.015 7.065 7.526 7.630 7.379 7.541 5.758

Table 4: Peak memory consumption (CPU systems).

Dataset Empty
newstest2014
newstest2015

Team Marian
OpenNMT Organizer Marian
OpenNMT Organizer Marian
OpenNMT Organizer

System
cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu cpu-transformer-base-aan cpu-transformer-big cpu-transformer-big-int8 cpu-transformer-small-aan cpu1 cpu2 echo nmt-1cpu

Memory [MiB]

Host GPU Both

531.39 — 531.39

1768.56 — 1768.56

1193.13 — 1193.13

367.22 — 367.22

403.86 — 403.86

194.61 — 194.61

1.15 —

1.15

1699.71 — 1699.71

761.07 — 531.39

1681.81 — 1768.56

2084.66 — 1193.13

476.21 — 367.22

458.08 — 403.86

219.79 — 194.61

1.20 —

1.15

1770.69 — 1699.71

749.29 — 531.39

1712.08 — 1768.56

2086.02 — 1193.13

461.27 — 367.22

455.21 — 403.86

217.64 — 194.61

1.11 —

1.15

1771.35 — 1699.71

Table 5: Peak memory consumption (GPU systems).

Dataset Empty
newstest2014
newstest2015

Team edin-amun Marian
NICT Organizer edin-amun Marian
NICT Organizer edin-amun Marian
NICT Organizer

System
fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu fastgru mlstm.1280 gpu-amun-tinyrnn gpu-transformer-base-aan gpu-transformer-big gpu-transformer-small-aan marian-st nmt-1gpu

Memory [MiB] Host GPU Both 442.55 668 1110.55 664.88 540 1204.88 346.80 522 868.80 487.18 484 971.18 1085.29 484 1569.29 366.26 484 850.26 510.43 484 994.43 378.81 640 1018.81 456.29 1232 1688.29 686.29 5144 5830.29 346.93 1526 1872.93 492.91 1350 1842.91 1081.69 2070 3151.69 366.55 1228 1594.55 922.53 1780 2702.53 377.18 2178 2555.18 473.72 1680 2153.72 684.84 6090 6774.84 350.19 1982 2332.19 489.62 1350 1839.62 1082.52 2198 3280.52 372.56 1228 1600.56 929.70 1778 2707.70 383.02 2178 2561.02

