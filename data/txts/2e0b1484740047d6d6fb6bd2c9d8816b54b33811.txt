Design and Analysis of the NIPS 2016 Review Process

arXiv:1708.09794v2 [cs.DL] 23 Apr 2018

Nihar B. Shah∗ Machine Learning Department, and
Computer Science Department Carnegie Mellon University nihars@cs.cmu.edu

Behzad Tabibian∗ Max Planck Institute for Intelligent Systems, and
Max Planck Institute for Software Systems me@btabibian.com

Krikamol Muandet Max Planck Institute for Intelligent Systems
Tübingen, Germany krikamol@tuebingen.mpg.de

Isabelle Guyon Universite Paris-Saclay, France, and
ChaLearn, California guyon@chalearn.org

Ulrike von Luxburg Department of Computer Science
University of Tübingen luxburg@informatik.uni-tuebingen.de

Abstract
Neural Information Processing Systems (NIPS) is a top-tier annual conference in machine learning. The 2016 edition of the conference comprised more than 2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and over 100% in terms of attendees as compared to the previous year. The massive scale as well as rapid growth of the conference calls for a thorough quality assessment of the peer-review process and novel means of improvement. In this paper, we analyze several aspects of the data collected during the review process, including an experiment investigating the efﬁcacy of collecting ordinal rankings from reviewers. Our goal is to check the soundness of the review process, and provide insights that may be useful in the design of the review process of subsequent conferences.
1 Introduction
The review process for NIPS 2016 involved 2,425 papers submitted by 5,756 authors, 100 area chairs, and 3,242 active reviewers submitting 13,674 reviews in total. Designing a review process as fair as possible at this scale was a challenge. In order to scale, all parts of the process have to be as decentralized as possible. Just to get a feeling, if the two program chairs were supposed to take ﬁnal decisions just for the 5% most challenging submissions, which means that they would have to read and decide on 150 papers — this is the scale of a whole conference such as COLT. Furthermore, the complexity of the logistics and software to manage the review process is rather high already. A controlled experiment (Lawrence and Cortes, 2014) from NIPS 2014 has shown that there is a high disagreement in the reviews. Hence the primary goal must be to keep bias and variance of the decisions as small as possible.
∗authors contributed equally Nihar Shah, Behzad Tabibian and Krikamol Muandet performed most of the data analysis reported in this paper. Behzad Tabibian and Krikamol Muandet were also the workﬂow team of NIPS 2016 and were responsible for all the programs, scripts and CMT-related issues during the review process. Isabelle Guyon and Ulrike von Luxburg were the program chairs of NIPS 2016.

In this paper, we present an analysis of many aspects of the data collected throughout the review phase of the NIPS 2016 conference, performed subsequent to the completion of the review process. Our goal in this analysis is to examine various aspects of the data collected from the peer review process to check for any systematic issues. Before delving into the details, the reader should importantly note the following limitations of this analysis:
• There is no ground truth ranking of the papers or knowledge of the set of papers which should ideally have been accepted.
• The analysis is post hoc, unlike the controlled experiment from NIPS 2014 (Lawrence and Cortes, 2014).
• The analysis primarily evaluates the ratings and rankings provided by reviewers, and does not study the textual comments provided by the reviewers.
The analysis is used to obtain insights into the peer-review process, usable suggestions for subsequent conferences, and important open problems towards improving peer-review in academia.
Here is a summary of our ﬁndings:
(i) there are very few positive bids by reviewers and area chairs (Section 3.1), (ii) graph-theoretic techniques can be used to ensure a good reviewer assignment (Section 3.2), (iii) there is signiﬁcant miscalibration with respect to the rating scale (Section 3.3), (iv) review scores provided by invited and volunteer reviewers have comparable biases and variance;
junior reviewers report a lower conﬁdence (Section 3.4), (v) there is little change in reviewer scores after rebuttals (Section 3.5), (vi) there is no observable bias towards any research area in accepted papers (Section 3.6), (vii) there is lower disagreement among reviewers in NIPS 2016 as compared to NIPS 2015 (Sec-
tion 3.7), (viii) signiﬁcant fraction of scores provided by the reviewers are tied and ordinal rankings can
ameliorate this issue (Section 3.8), (ix) there are some inconsistencies in the reviews and these can be identiﬁed in an automated manner
using ordinal rankings (Section 3.9).
We describe the review procedure followed at NIPS 2016 in Section 2. We present an elaborate description of the analysis and the results in Section 3. Alongside each analysis, we present a set of key observations, action items for future conferences, and some open problems that arise out of the analysis. We conclude the paper with a discussion in Section 4.
2 Review procedure
In this section, we present an overview of the design of the review process at NIPS 2016.
2.1 Selecting area chairs and reviewers
Area Chairs (ACs) are the backbone of the NIPS reviewing process. Their role is similar to that of an associate editor for a journal. Each AC typically handles 20-30 submissions, so with an estimated number of submissions between 2000 and 3000, we needed to recruit about 100 area chairs. As it is impossible to intimately know all the diverse research areas covered by NIPS, we came up with the following procedure. We asked the NIPS Board and all the ACs of NIPS from the past two years to nominate potential ACs for this year. In this manner, we covered the entire variety of NIPS topics and obtained qualiﬁed suggestions. We obtained around 350 suggestions. We asked the NIPS Board to go through the list of suggested ACs and vote in favor of suggested ACs. We also accounted for the distribution of subject areas of submitted papers of the previous year’s NIPS conference. Combining all these inputs, we compiled a ﬁnal list of ACs: by the end of January we had recruited exactly 100 ACs. In a subsequent step, we formed “buddy pairs” among the ACs. Based on the ACs preferences, each AC got assigned a buddy AC. We revisit the role of buddy pairs in more detail later.
2

The process of recruiting reviewers is time consuming, it essentially went on from January until the submission deadline at end of May. A signiﬁcant departure from the review processes of NIPS from earlier years, this time we had two kinds of reviewers, “invited senior reviewers” (Pool 1) and “volunteer reviewers” (Pool 2):

• Pool 1, invited senior reviewers: We asked all ACs to suggest at least 30 reviewers who have completed their PhDs (however, this requirement was not strictly observed by all ACs). We obtained 2500 suggested experienced reviewers. We invited all of them, and 1100 accepted. We then asked all conﬁrmed reviewers to “clone themselves” by inviting at least one researcher with a similar research background and with at least as good a qualiﬁcation as themselves. This resulted in an additional 500 experienced reviewers.
• Pool 2, volunteer author-reviewers: The rapid growth in the number of submissions at NIPS poses the formidable challenge of accordingly scaling the number of reviewers. An obvious mean to achieve this objective is to ask authors to become reviewers as well. This idea has been used in the past, for example, to evaluate NSF grant proposals (Mervis, 2014) or to allocate telescope time (Merriﬁeld and Saari, 2009). In order to implement this idea, without constraining unwilling authors, we requested authors to volunteer during the submission process by naming at least one author per paper as volunteer reviewers. We invited all of them and about 2000 of the volunteers accepted the invitation.

The area chairs were aware of the respective pools to which each of their reviewers belonged. The number of reviewers that we eventually ended up with are as follows:

Pool 1: Invited reviewers Pool 2: Volunteer reviewers

Senior researchers / faculty 1236 143

Junior researchers / postdocs 566 206

PhD students
255 827

2.2 Assignment of papers to reviewers and area chairs
The assignment of papers to area chairs was made in the following manner. Prior to the review process, the ACs (and reviewers) were allowed to see the list of submitted papers and “bid” whether they were interested or disinterested in handling (or reviewing) any paper. For any paper, an AC (or reviewer) could either indicate “Not Willing” or “In-a-pinch” – which we count as negative bids, or indicate “Willing” or “Eager” – which we count as positive bids, or choose to not bid for that paper. The Toronto paper matching system or TPMS was then employed to compute an afﬁnity score for every AC (and reviewer) with every submitted paper based on the content of the paper and the academic proﬁle of the AC or reviewer. In addition, every AC (and reviewer) as well as the submitter of every paper was asked to select a set of most relevant subject areas, and these subject areas were also employed to compute a similarity between each AC (and reviewer) and paper.
Based on the similarity scores and bids, an overall similarity score is computed for every {paper, AC} and every {paper, reviewer} pair: score = b(safﬁnity + ssubject), where safﬁnity ∈ [0, 1] is the afﬁnity score obtained from TPMS, ssubject ∈ [0, 1] is the score obtained by comparing the subject areas of the paper and the subject areas selected by the AC or reviewer, and b ∈ [0.25, 1] is the bidding score provided by the AC or reviewer. Based on these overall similarity scores, a preliminary paper assignment to ACs was then produced in an automated manner using the TPMS assignment algorithm (Charlin and Zemel, 2013). The ACs were given a provision to decline handling certain papers for various reasons such as conﬂicts of interest. These papers were re-assigned manually by the program chairs.
The AC of each paper was responsible to ﬁrst assign one senior, highly qualiﬁed reviewer manually. Two more invited reviewers from pool 1 and three volunteer reviewers from pool 2 were then assigned automatically to each paper using the same procedure as described above. The ACs were asked to verify whether each of their assigned papers had at least 3 highly competent reviewers; the ACs could manually change reviewer assignments to ensure that this is the case. During the decision process, additional emergency reviewers were invited to provide complementary reviews if some of the reviewers had defected or if no consensus was reached among the selected reviewers.

3

2.3 Review criteria and scores
We completely changed the scoring method this year. In previous years, NIPS papers were rated using a single score between 1 and 10. A single score alone did not allow reviewers to give a differentiated quantitative appreciation on various aspect of paper quality. Furthermore, the role of the ACs was implicitly to combine the decisions of the reviewers (late integration) rather than combining the reviews to make the ﬁnal decision (early integration). Introducing multiple scores allowed us to better separate the roles: the reviewers were in charge of evaluating the papers; the ACs were in charge of making decisions based on all the evaluations. Furthermore the multiple specialized scores allowed the ACs to guide reviewers to focus discussions on “facts” rather than “opinion” in the discussion phase. We asked reviewers to provide a separate score for each of the following four features:
• Technical quality, • Novelty/originality, • Potential impact or usefulness, • Clarity and presentation.
The scores were on a scale of 1 to 5, with the following rubric provided to the reviewers:
5 = Award level (1/1000 submissions), 4 = Oral level (top 3% submissions), 3 = Poster level (top 30% submissions), 2 = Sub-standard for NIPS, 1 = Low or very low.
The scoring guidelines also reﬂect the hierarchy of the papers: the conference selects the top few papers for awards, the next best accepted papers are presented as oral presentations, and the remaining accepted papers are presented as posters at the conference. The scores provided by reviewers had to be complemented by justiﬁcations in designated text boxes. We also asked the reviewers to ﬂag “fatal ﬂaws” in the papers they reviewed. For each paper, we also asked the reviewers to declare their overall “level of conﬁdence”:
3 = Expert (read the paper in detail, know the area, quite certain of opinion), 2 = Conﬁdent (read it all, understood it all reasonably well), 1 = Less conﬁdent (might not have understood signiﬁcant parts).
2.4 Discussions and rebuttals
Once most reviews were in, authors had the opportunity to look at the reviews and write a rebuttal. One section of the rebuttal was revealed to all the reviewers of the paper, and a second section was private and visible only to the ACs. Some reviews were still missing at this point, but it would not have helped to delay the rebuttal deadline as the missing reviews trickled in only slowly. Subsequently, ACs and reviewers engaged in discussions about the pros and cons of the submitted papers. To support the ACs, we sent individual reports to all area chairs to ﬂag papers whose reviews were of too low conﬁdence, too high variance or where reviews were still missing. In many cases, area chairs recruited additional emergency reviewers to increase the overall quality of the decisions.
2.5 Decision procedure
The decision procedure involved making an acceptance or rejection decision for each paper, and furthermore, to select a subset of (the best) accepted papers for oral presentation.
We introduced a decentralized decision process based on pairs of ACs (“buddy pairs”). Each AC got assigned one buddy AC. Each pair of buddy ACs was responsible for all papers in their joint bag and made the accept/reject decisions jointly, following guidelines given by the program chairs. Difﬁcult cases were taken to the program chairs, which included cases involving conﬂicts of interest and plagiarism. In order to harmonize decisions across buddy pairs, all area chairs had access to various statistics and histograms over the set of their papers and the set of all submitted papers. To decide which accepted paper would get an oral presentation, each buddy pair was asked to champion one or two papers from their joint bag as a candidate for an oral presentation. The ﬁnal selection was then made by the program chairs, with the goals of exhibiting the diversity of NIPS papers and
4

exposing the community with novel and thought provoking ideas. In the end, 568 papers got accepted to the conference, and 45 of these papers were selected for oral presentations.
Like previous years, we adopted a “double blind” review policy. That is, the author(s) of each paper did not get to know the identity of the reviewers and vice versa throughout the review process. ACs got to know the identity of the reviewers and the author(s) for the papers under their responsibility. During the discussion phase, reviewers who reviewed the same papers got to know each others’ identity. Lastly, PCs and program managers had access to all information about the submissions, the ACs, the reviewers, and the authors.
2.6 Experimental ordinal reviews
In the main NIPS 2016 review process, we elicited only cardinal scores from the reviewers – one score in 1 to 5 for each of four features. Subsequent to the review process, we then requested each reviewer to also provide a total ranking of the papers that they reviewed. We received rankings from a total of 2189 reviewers. Note that the collection of ordinal data was performed subsequent to the normal review submission but before release of the ﬁnal decisions. The ordinal data was not used as a part of the decision procedure in the conference.
3 Detailed analysis
In this section, we present details of our analyses of the review data and the associated results. Each subsection contains one analysis and concludes with a summary that highlights the key observations, concrete action items for future conferences, and open problems that arise from the analysis.
The results are computed for a snapshot of reviews at the end of the review process when the acceptance decisions were made. This choice does not affect the results since there was very little change in the scores provided by reviewers across different time instants. All t-tests conducted correspond to two-sample t-tests with unequal variances. All mentions of p-values correspond to two-sided tail probabilities. All mentions of statistical signiﬁcance correspond to a p-value threshold of 0.01 (we also provide the exact p-values alongside). Multiple testing is accounted for using the Bonferroni correction. The effect sizes refer to Cohen’s d. Wherever applicable, the error bars in the ﬁgures represent 95% conﬁdence intervals.
Wherever applicable, we also perform our analyses on a subset of the submitted papers which we term as the top 2k papers. The top 2k papers comprise all of the 568 accepted papers, and an equal number (568) of the rejected papers. The 568 rejected papers are chosen as those with maximum scores averaged across all reviewers and all features.
3.1 Reviewer bids
A large number of conferences in computer science ask area chairs and/or reviewers to bid which papers they would like or not like to review, in order to obtain a better understanding of the expertise and the preferences of reviewers. Such an improved understanding is desirable as it leads to a more informed assignment of reviewers to papers, thereby improving the overall quality of the review process.
Figure 1 depicts the distribution of number of bids on papers submitted by area chairs and reviewers in NIPS 2016. Panels (a) and (b) of the ﬁgure depict the distribution of counts per paper for reviewers and area chairs respectively; panels (c) and (d) depict the distribution per area chairs and reviewers. From the data, we observe that there are very few positive bids, but a considerably higher number of negative bids.
The distribution of number of bids by reviewers is skewed by few reviewers who bid (positive and negative) on too many papers: 27% of reviewers make 90% of all bids, and 50% of reviewers make 90% of all positive bids. Moreover, there are 148 reviewers with no (positive or negative) bids and 1201 reviewers with at most 2 positive bids. In comparison, NIPS 2016 assigned at least 3 papers to most reviewers and many conferences do likewise. We thus observe that a large number of reviewers do not even provide positive bids amounting to the number of papers they would review. As a consequence of the low number of bids by reviewers, we are left with 278 papers with at most 2 positive bids and 816 papers with at most 5 positive bids. In contrast, NIPS 2016 assigned 6 reviewers
5

# papers

1500 Negative 1000 Positive 500
00 3 #1r5eview63er-b2id5s5 1023
(a) total number of bids by reviewers per paper

# papers

1000

Negative Positive

500

00 3 15# AC6-3bids255 1023
(b) total number of bids by ACs per paper

# reviewers

750

Negative Positive

500

250

00 3 #15pap6e3r-bid2s55 1023

(c) number of bids per reviewer

# ACs

75

Negative

50

Positive

25

00 3 #15pap6e3r-bid2s55 1023

(d) number of bids per AC

Figure 1: Histogram of number of positive and negative bids (x-axis; on a logarithmic scale) per entity (counts on y-axis) for various entities. The “not willing” and “in-a-pinch” bids were considered negative bids, whereas “willing” and “eager” bids were considered positive bids. The ﬁrst column in each histogram represents number of entities with 0 bids. For example, the ﬁrst column of panel (c) depicts that 756 reviewers made zero positive bids and 425 reviewers made zero negative bids.

to most papers. There is thus a signiﬁcant fraction of papers with fewer positive bids than the number of requisite reviewers. Finally there are 1090 papers with no positive bids by any AC.
Summary 1: Reviewer bids
Key observations:
• There are very few positive bids, with 278 papers receiving at most 2 positive bids and 816 papers receiving at most 5 positive bids.
• From the reviewers’ side, the bids are highly skewed: 50% of reviewers make 90% of all positive bids, 148 reviewers make no (positive or negative) bids, and 1201 reviewers make at most 2 positive bids.
Action items:
• When a reviewer logs into the system, the interface can show the unbid papers on top.
• Inform reviewers of the procedure employed to use their bids for assigning papers. Make reviewers aware of the beneﬁts of bidding, such as receiving more relevant papers to read and serving the community by improving the review process.
Open problems:
• How to incentivize more (positive) bids so that the organizers understand preferences better for accurate reviewer assignment?
• Design a principled means of combining bids, paper content-reviewer proﬁle similarity, and subject similarity.

3.2 Reviewer assignment
Figure 2 depicts the histograms of the number of reviewers assigned per paper, and the number of papers handled by each reviewer. In order to ensure that the information about each paper “spreads” across the entire system, it is important that there is no set of reviewers or papers that has only a small overlap with the remaining reviewers and papers (Olfati-Saber et al., 2007, Shah et al., 2016a). To analyze whether this was the
6

1500 1000 500
0 23456789
(a) number of reviewers per paper

1000 750 500 250
0 123456789
(b) number of papers per reviewer

Figure 2: Histogram of number of reviews.

case, we considered two graphs. We built a reviewer graph that has reviewers as vertices, and an edge between any two reviewers if there exists at least one paper that has been reviewed by both of them. Analogously we built a paper graph, where vertices represent papers, and we connect two papers by an edge if there exists a reviewer who has reviewed both papers. Note that the graph structure is in part dictated by a constraint on the maximum number of papers per reviewer, as well as the speciﬁed number of reviewers per paper.
Our objective is to examine the structure of the graphs and determine if there were any separated communities of nodes. In order to do so, we employ a method based on spectral clustering. Formally, denote any graph as G = (V, E) where V is set of nodes, and E is the set of (undirected) edges between nodes, and |V | is number of nodes in the graph. We can denote graph connectivity by its associated adjacency matrix A which is a (|V | × |V |) matrix; we have Aij = 1 if there is an edge between nodes i and j and Aij = 0 otherwise. With this notation, a quantity known as the “conductance” Φ of any set of nodes S ⊂ V is then deﬁned as:
Aij Φ(S) = i∈S,j∈S ,
max{|S|, |V \S|}
where V \S is the complement of set S. A lower value of the conductance indicates that the nodes in the cut are less connected to the remaining graph. Next, with a minor abuse of notation, the conductance of a graph as function of cluster sizes is deﬁned as:
Φ(k) = min Φ(S),
S∈V,|S|=k
for every k ∈ {1, . . . , |V | − 1}. The plot of k versus Φ(k) is called a Network Community Proﬁle or NCP plot (Leskovec et al., 2008). The NCP plot measures the quality of the least connected community (lowest conductance) in a large network, as a function of the size of the community. Although computing the function Φ(k) exactly may be computationally hard, an approximate value can be computed using a simple “second left eigenvector” procedure (Section 2.3 of Benson et al., 2015). A well connected graph would have a smooth plot of Φ(k) with a minima at around k = |V |/2.
Figure 3 shows the NCP plot for an increasing number of papers (respectively reviewers) in the paper graph (respectively reviewer graph). For reference we also plot the same curve for graphs associated with NIPS 2015 conference. Both plots for NIPS 2015 have local minima at around k = 0.96|V |,

conductance conductance

1.00

2016

0.75

2015

0.50

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0
fraction of papers in cluster

(a) paper graph

1.00

2016

0.75

2015

0.50

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0
fraction of reviewers in cluster

(b) reviewer graph

Figure 3: Conductance value as function of varying cluster size. The x-axes in these plots is the normalized cluster size k/|V |.

7

Reviewers

Papers

count

NIPS 2015

NIPS 2016

NIPS 2015

NIPS 2016

Figure 4: Graphs depicting connectivity of reviewers and that of papers for NIPS 2015 and NIPS 2016. The nodes in black (dark) show set of nodes identiﬁed by the local minima in the conductance plots (Figure 3) for NIPS 2015, and the remaining nodes are plotted in blue (light).

50

40

30

20

10

0

RL

robotics

deep

bandit

theoretical

control

other

algorithms

learning suablgjoercitthamresa neuroscience

theory

Figure 5: Histogram of subject areas in the identiﬁed cluster (from Figure 4) of reviewers in NIPS

2015 which is not well connected with the set of remaining reviewers.

indicating that there is a densely connected community of reviewers and papers that are not well connected with the rest of the graph. In contrast, the plot associated with NIPS 2016 decreases smoothly and reaches its global minimum when half of the nodes are in one cluster and the other half in another cluster, indicating an absence of such a fragmentation.
In Figure 4, we plot the graph of reviewers and papers using the algorithm of Fruchterman and Reingold (1991). In these ﬁgures we identify the set of nodes that are identiﬁed using the aforementioned NCP method; these nodes are colored black (dark) in the ﬁgure in contrast to the blue (light) color of the remaining nodes. We can see from the Figure 4 that these nodes are on the periphery of the network with lower connectivity compared to the rest of the graph.
We further examine the cluster of reviewers in NIPS 2015 which is not well connected with the rest. In Figure 5, we plot the decomposition of this set in terms of the primary subject areas indicated by the reviewers. Our analysis reveals that a bulk of this cluster comprises a single subject area— reinforcement learning. Conversely, 50 out of 78 reviewers who identiﬁed their primary subject area as reinforcement learning lie in this cluster. All in all, graph connectivity issues of this form can lead to increased noise or bias in the overall decisions. Our main message for future conferences is to employ such methods of graph analysis in order to catch issues of this form at a global level (not just local to individual ACs) before the reviews are assigned.
Summary 2: Reviewer assignment
Key observations:
• A cluster of papers and reviewers primarily in the reinforcement learning area are not well connected to the remaining papers and reviewers in the NIPS 2015 reviewer assignments. We did not ﬁnd any such separated cluster in NIPS 2016.
Action items:
• Use graph-theoretic techniques to check global structure of graph for reviewer assignment.
Open problems:
• Design principled graph-theoretic techniques, tailored speciﬁcally to the nuances of peer-review graphs, to verify soundness of reviewer assignments.

8

reject 1.5

accept as poster

1.0

0.5

0.01.0 1.5 2.0 2.m5 ean3.0scor3e.5

reject 1.5

(a) clarity
accept as poster

1.0

0.5

0.01.0 1.5 2.0 2.m5 ean3.0scor3e.5

(c) novelty

accept as oral
4.0 4.5 5.0 accept as oral
4.0 4.5 5.0

reject 1.5

accept as poster

1.0

0.5

0.01.0 1.5 2.0 2.m5 ean3.0scor3e.5

reject 1.5

(b) impact
accept as poster

1.0

0.5

0.01.0 1.5 2.0 2.m5 ean3.0scor3e.5

(d) quality

accept as oral
4.0 4.5 5.0 accept as oral
4.0 4.5 5.0

Figure 6: Distribution of the mean value (across reviewers) of the score per paper for different features, separated according to the ﬁnal decisions.

3.3 Review-score distribution and mismatches in calibration
Recall from Section 2.3 that in the review process, for each feature, the reviewers were asked to provide a score on a scale of 1 to 5. Speciﬁcally, they were asked to provide a score of 5 for submissions they considered as being in the top 0.1%, a score of 4 for submissions that they deemed to be in the top 3%, and a score of 3 for submissions they deemed to be in the top 30%. In this section, we compare the actual empirical distribution of reviewer scores with the distribution prescribed in the guidelines to reviewers.
We begin by computing the distribution of the mean value (across reviewers) of the score per paper for different features, separated according to the ﬁnal decisions. We plot these distributions in Figure 6 for each of the four features of clarity, impact, novelty, and quality separately.
At ﬁrst glance, these histograms and numbers look quite reasonable. However, what was surprising to us was the percentage of papers that received any particular score – see Table 1. Even though the reviewers were asked to give a paper a score of 3 (poster level) or higher only if they think the paper lies in the top 30% of all papers, nearly 60% of the scores were 3 or higher. Similar effects occurred for scores 4 and 5.
One possible explanation for this phenomenon is that there were a large number of high-quality submissions to NIPS 2016. Such an improvement in quality has obvious upsides such as uplifting the overall experience of the conference. The downside is that the burden on selecting the accepted papers among all those good submissions is with the area chairs, who now still had to reduce the 60% good papers to 23% accepted papers. A second possible explanation is that the reviewers were not calibrated that well with respect to the paper quality. In either case, we understand that this obviously led to the frustration of many authors, whose papers received good scores but were rejected.

1 (low or very low)

2 (sub-standard)

3 (poster level:
top 30%)

4 (oral level:
top 3%)

5 (award level:
top 0.1%)

Impact

6.6 %

36.4 %

45.9 %

10.7 %

0.4 %

Quality

6.7 %

38.3 %

45.0 %

9.6 %

0.4 %

Novelty

6.4 %

35.0 %

48.4 %

9.8 %

0.4 %

Clarity

7.1 %

28.1 %

48.9 %

14.7 %

1.2 %

Table 1: Distribution of the reviews according to the provided scores for each of the four features.

The column headings indicate the guidelines that were provided to the reviewers. Observe that the

percentage of reviews providing scores of 3, 4 or 5 is considerably higher than the requested values.

9

In addition to scores for the four features, the reviewer could also indicate whether the paper had a “fatal ﬂaw”. We observe that 32% of all papers were ﬂagged to have a “fatal ﬂaw” by at least one reviewer.
Summary 3: Review-score distribution and mismatches in calibration
Key observations:
• The fraction of reviews with high ratings is signiﬁcantly higher than what was asked from the reviewers. For instance, nearly 60% of scores are 3 or higher even though reviewers were asked of scores of 3 or higher only when they thought the paper was in the top 30% of submissions.
Action items:
• If eliciting ratings, do not use numbered scales (that is, do not use “1”, “2”, . . . ). Alternatively, one may employ other means of elicitation such as rankings.
• When making reviews visible to authors, show the percentile with respect to the data instead of absolute scores, e.g., provide feedback of the form “your paper is in the top 40% of all submitted papers in terms of novelty...”
• Include an expert in elicitation, survey methodology or user interface design to help to design what and how to ask (O’Hagan et al., 2006).
Open problems:
• Since each reviewer reviews only a small subset of the submitted papers, how to calibrate the reviews?
• What is the best interface for eliciting reviewer responses?
• What is the best way to present the review results to authors in order to provide most useful feedback and minimizing distress?
3.4 Behavior of different pools of reviewers
In this section, we compare the reviews provided by the volunteer (pool 2) reviewers to those provided by the invited (pool 1) reviewers. The inclusion of volunteer reviewers has two important beneﬁts: (a) It increases the transparency of the review process. (b) Volunteer reviewers may be new today but in 2 years down the line they will gain experience and become useful to accommodate the massive growth of the conference. Given these beneﬁts of including volunteer reviewers, this analysis looks for any systematic differences between the review scores provided by the two pools of reviewers.
Mean scores. Junior reviewers are often perceived to be more critical than senior reviewers (Tomiyama, 2007, Toor, 2009). As Tomiyama (2007) notes, “You submit your manuscript and then just pray it doesn’t get sent to a junior faculty member – young faculty are merciless!” In this section, we examine this hypothesis in the NIPS 2016 reviews. In Figure 7, we plot the mean score provided by each group of reviewers for each individual feature. We apply a t-test on observed scores and compute the effect size to examine if there is a statistically signiﬁcant difference in the underlying means of the scores provided by different categories of reviewers. For Pool 1 vs Pool 2, this analysis shows only clarity to have a statistically signiﬁcant difference between the two pools after accounting for multiple testing. Speciﬁcally, the p-values (before accounting for multiple testing) and effect sizes for the four features are: novelty p=0.2143, d= 0.0264, quality p=0.0061, d= 0.0581, impact p=0.0961, d= 0.0353, and clarity p=1.91 × 10−04, d= 0.0788. Sample sizes for Pool 1 and Pool 2 reviews are 9244 and 4430 respectively.
A similar analysis between senior researchers (e.g., faculty), junior researchers (e.g., postdocs), and PhD students reveals no signiﬁcant difference between these categories. Speciﬁcally, the pvalues (before accounting for multiple testing) and effect sizes for senior researcher vs. junior researchers for the four features are: quality p=0.0071, d= −0.0662, novelty p=0.0037, d= −0.0704, impact p=0.0199, d= −0.0569, and clarity p=0.3064, d= −0.0253; for junior researcher vs. students: quality p=0.4662, d= 0.0164, novelty p=0.8247, d= 0.0049, impact p=0.8733,d= −0.0036, and clarity p=0.3529, d= 0.0209; for senior researcher vs. students: quality p=0.0440, d= −0.0454, novelty p=0.0499, d= −0.0629, impact p=0.0076, d= −0.0601 and clarity p=0.9968, d= 0.00009.
10

The sample sizes for senior, junior and student reviews are: 6335, 3938, and 3354 respectively. This analysis excludes 47 reviews by reviewers who did not identify themselves as any of the above categories.
Self-reported conﬁdence. We next study the difference in the self-reported conﬁdence among different groups of reviewers. The mean value of reported conﬁdence is plotted in Figure 8. In this case, we see a statistically signiﬁcant correlation between seniority and self-reported conﬁdence. Following are p-values (before accounting for multiple testing) and corresponding effect sizes: senior vs. junior researcher: p=4.1683 × 10−11, d= 0.1604, senior researcher vs. PhD student: p=3.308 × 10−57, d= 0.3577 and junior researcher vs. PhD student: p=8.074 × 10−15, d= 0.1758. We observe a similar difference in conﬁdence score and effect size between pool 1 and pool 2 reviewers: p=3.9679 × 10−44, d= 0.2943.
Consistency. We now study the consistency within reviewers of pool 1 (invited), and within reviewers of pool 2 (volunteer). The consistency captures the amount of variance or disagreements in the reviews provided by that pool. As noted by Ragone et al. (2013), “the disagreement among reviewers is a useful metric to check and monitor during the review process. Having a high disagreement means, in some way, that the judgment of the involved peers is not sufﬁcient to state the value of the contribution itself. This metric can be useful to improve the quality of the review process...”
Concretely, consider any pair of reviewers within a given pool, any pair of papers that is reviewed by both the reviewers, and any feature. We say that this pair of reviewers agrees on this pair of papers (for this feature) if both reviewers rate the same paper higher than the other; we say that this pair

mean score

3.0 pool 1 pool 2 2.5 2.0 1.5 1.0 quality novelty impact clarity
(a) invited and volunteer reviewers

Mean Score

3.0 senior junior student 2.5 2.0 1.5 1.0 quality novelty impact clarity
(b) senior, junior, and student reviewers

Figure 7: Mean of scores provided for different features grouped by different reviewer types.

disfraagrcteieonmeofnts

conmfiedaennce

confmiedaennce

2.0

2.0

1.5

1.5

1.0 pool 1 pool 2
(a) invited and volunteer reviewers

1.0 senior junior student
(b) senior, junior, and student reviewers

Figure 8: Self-reported conﬁdence grouped by different reviewer types.

0.5

0.5

0.5

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.0 overall pool 1 pool 2 0.0 overall pool 1 pool 2 0.0 overall pool 1 pool 2 0.0 overall pool 1 pool 2

(a) novelty

(b) quality

(c) impact

(d) clarity

Figure 9: Proportions of inter-reviewer disagreements on each feature.

11

disagrees if the paper rated higher by one reviewer is rated lower by the other. Ties are discarded. We count the total number of such agreements and disagreements within each of the two pools.
Figure 9 plots the fraction of disagreements within each of the two pools for the cardinal scores. At this aggregate level, we do not see enough difference to conclusively rate any one pool’s intra-pool agreement above the other, and this conclusion is also conﬁrmed by an absence of a statistically signiﬁcant difference in the proportion of agreements within pool 1 from the proportion of agreements within pool 2. Speciﬁcally, for the Pearson’s chi-squared test and effect sizes of pool 1 vs. pool 2, the results for the four features (before accounting for multiple testing) are: novelty p=0.9269 d= -0.0426, quality p=0.8648, d= 0.0039, impact p=0.7296, d= -0.0936, and clarity p=0.8029, d= -0.0709. The total sample sizes for the three categories of overall, pool 1 and pool 2 respectively across the four features are: novelty 554, 282 and 49; quality 523, 285 and 41; impact 513, 276 and 37; and clarity 572, 286 and 42. Section 3.8 presents similar consistency results for the two pools in the ordinal rankings. (We also attempted to run this analysis restricted to the top 2k papers, but this restriction results in a very low sample complexity and hence underpowered tests.)
Participation in discussions. One fact that caught our attention was the amount of participation in the discussion by the different reviewer groups: senior reviewers take much more active roles in the discussions than junior researchers. Please see Section 3.5.1 for details, where we provide a more detailed study of the discussion phase.
Summary 4: Behavior of different pools of reviewers
Key observations: • We ﬁnd no evidence of a critical bias of junior reviewers (except for the “clarity” feature). • Self-reported conﬁdence correlates with seniority. • Volunteer reviewers yield beneﬁts of scalability and transparency, with no observable biases and a similar inter-reviewer agreement as the invited pool. These reviewers can soon be an asset in dealing with the rapid growth of conferences such as NIPS.
Action items: • Continue to include volunteer reviewers.
Open problems: • How do we make most effective use of volunteer reviewers in a manner that authors can trust, reduces randomness in the peer-review process, and trains junior reviewers effectively?
3.5 Rebuttals and discussions
This section is devoted to the analysis of the rebuttal stage and the participation of reviewers in discussions. We begin with some summary statistics. The authors of 2188 papers submitted a rebuttal.
There were a total of 12154 reviews that came in before the rebuttals started, and with some more reviews received after the rebuttal round, the total number of {reviewer, paper} pairs eventually ended up being 13674. Among the 12154 reviews that were submitted before the rebuttal, the scores of only 1193 of them changed subsequent to the rebuttal round. These changed review scores were distributed among 886 papers.
There were 842 papers for which no reviewer participated in the discussions, 339 papers for which exactly one reviewer participated, and 436, 376, 218, 135 and 49 papers for which 2, 3, 4, 5 and 6 reviewers participated respectively. There were a total of 5255 discussion posts, and 4180 of the 13674 {reviewer, paper} pairs participated in the discussions.
3.5.1 Who participates in discussions?
We compare the amount of participation of various groups of reviewers in the discussion phase of the review process.
Pool 1 (invited) versus pool 2 (volunteer) reviewers. We compare the participation of the reviewers in two pools in the discussions as follows, and plot the results in Figure 10(a). In order to set a
12

baseline, we ﬁrst compute the total number of {pool 1 reviewer, paper} pairs and the total number of {pool 2 reviewer, paper} pairs – these counts are computed irrespective of whether the reviewer participated in the discussions or not. We plot the proportions of these counts as the “count” bar in the ﬁgure. Next we compute the total number of posts that were made by pool 1 reviewers and the total number of posts that were made by pool 2 reviewers – the resulting proportions are plotted as the “posts” bar in the ﬁgure. Finally, we compute the total number of {pool 1 reviewer, paper} pairs in which that reviewer put at least one post in the discussion for that paper, and the total number of {pool 2 reviewer, paper} pairs in which that reviewer put at least one post in the discussion for that paper. We plot the two proportions in the “papers” bar. The total sample sizes for the categories of counts, posts and papers are 13674, 5255 and 4180 respectively.
We tested whether the mean number of posts per {reviewer, paper} pair is identical for the two pools of reviewers. For the null hypothesis that the means are identical for the two pools of reviewers, the t-test yielded a p-value of p = 1.36 × 10−4. We also conducted this analysis for the restriction of papers to the top 2k, and for this subset, the t-test yielded a p-value of p = 9.458 × 10−4. We see a statistically signiﬁcantly higher participation by the pool 1 reviewers as compared to the pool 2 reviewers in the discussions. However, the absolute amount of participation by either group is moderate at best, and the effect sizes are small with d= 0.0704 and d= 0.0894 for analysis of all papers and top 2k papers respectively.
Student versus non-student reviewers. We calculated the above three sets of quantities for student and non-student reviewers. Figure 10(b) depicts the results. We tested whether the mean number of posts per {reviewer, paper} pair for the student reviewers is identical to the non-student reviewers. For the null hypothesis that the means are identical, the t-test yielded a p-value of p = 3.016 × 10−4. We also conducted this analysis for the restriction of papers to the top 2k, and for this subset, the t-test yielded a p-value of p = 8.932 × 10−4. We see a statistically signiﬁcantly higher participation by the non-student reviewers as compared to the student reviewers in the discussions. However, the total amount of participation by either group is not too large, and the effect sizes are small with d= 0.0695 and d= 0.0929 respectively.
3.5.2 How do discussions change the scores?
A total of 1193 out of 12154 reviews that were submitted before rebuttals changed subsequently. These changed reviews were distributed among 886 papers. As a result, the amount of change in review scores is quite small. Figure 11 depicts the score change – in absolute value – averaged across all reviewers and all papers. While the allowed range of the scores is 1 to 5, the change in mean score is less than 0.1.
From the point of view of reviewers, we see a signiﬁcant correlation between participation in the discussions and the ﬁnal decisions. Speciﬁcally, for each paper we computed the average of scores given by all reviewers who participated in the discussions and the average of scores given by all reviewers who did not participate (when there was at least one reviewer of each type). We discarded this paper if both types of reviewers provided an identical average score. Now, if the participating reviewers gave a higher score than the non-participating reviewers and if the paper was accepted, we counted it as an agreement of the ﬁnal decision with the participating reviewers. If the participating reviewers gave a lower score than the non-participating reviewers and if the paper was not accepted, then also we count it as an agreement of the ﬁnal decision with the participating reviewers. Otherwise,

1.00

pool 1

pool 2

0.75

0.50

0.25

0.00 papers

posts

count

(a) invited and volunteer reviewers

1.00

non-student

0.75

0.50

0.25

0.00 papers

posts

student count

(b) student and non-student reviewers

Figure 10: Proportions of contributions from different types of reviewers in discussions (“posts” and “papers”) and the total number of such reviewers (“count”).

13

chmaenagneaibnssolcuotree

reject

accept as poster

accept as oral

0.1

0.0 quality

clarity

novelty

impact fatal flaws confidence

Figure 11: Mean absolute value of the change in the scores from before the rebuttal round to the end of the discussion phase.

we counted the paper as having a disagreement between the ﬁnal decisions and the participating reviewers. From the data, we observe a statistically signiﬁcant agreement of the ﬁnal decisions and the participating reviewers with p = 1.6 × 10−6 with d= 0.13. We continue to observe a statistically signiﬁcant correlation when this analysis is performed restricted to pool 1 reviewers (p = 7.7 × 10−4) or to pool 2 reviewers (p = 1.3 × 10−4) alone. Of course, we cannot tell the causality from this correlation, as to whether the discussions actually inﬂuenced the decisions or not.
All in all, we observe that only a small fraction of the reviews change scores following the rebuttals. Moreover the magnitude of this change in scores is very small. This observation suggests that this rebuttal process may not be very useful. That said, there are various qualitative aspects that are not accommodated in this quantitative aggregate statistic. First, it may be possible that more reviews changed with respect to the text comments but the reviewers just did not bother to change the scores – we are unable to check this property as there is no snapshot of the text comments before the rebuttal. Second, there are a reasonable number of discussion posts, however, we do not know what fraction of these posts where reviewers shifted from their earlier opinion. Third, the ﬁnal decisions are correlated positively with the reviewers who participated in discussions. Taking these factors into account, we think that the present rebuttal system should be put under the microscope regarding its value for the time and effort of such a large number of people. It may also be worth trying alternative systems of recourse for authors, such as a formal appeals process, that help to put more focus on the actual borderline cases.
Summary 5: Rebuttals and discussions
Key observations:
• There is little change in scores post-rebuttal and a moderate amount of discussion.
• Invited and non-student reviewers participate more in the discussions.
• Final decisions correlated with scores given by reviewers who participated in discussions, even when stratiﬁed by individual pools.
Action items:
• Force every reviewer to change or conﬁrm their scores after the end of the discussion session.
Open problems:
• How to incentivize reviewer participation in rebuttals/discussions?
• How to de-bias reviewers from their initial opinion?
• Compare the amount of discussion and changes in scores with that in open review processes (particularly, when open reviews are used for conferences of this scale).
• Compare the efﬁciency of the rebuttal process with a post-decision appeal procedure to catch only cases that deserve discussion (i.e., possible mistakes).

3.6 Distribution across subject areas
Figure 12 plots the distribution per subject area (primary subject area), for the submitted papers and for the accepted papers. Of course the proportions are not identical, but the plots do not show any systematic bias either towards or against any particular areas. The heavy tail in the distributions below also corroborates the signiﬁcant diversity of topics in the NIPS community. A chi-square test
14

count

103

Submitted Papers

Accepted Papers

102

101

100 1 2 3 4 5 6 7 8 9 10 11 12 13 14su1b5je1c6t a1r7ea18 19 20 21 22 23 24 25 26 27 28 29 30 31

103 102
101
100 32 33 34 35 36 37 38 39 40 41 42 43 44 45su4b6je4c7t a4r8ea49 50 51 52 53 54 55 56 57 58 59 60 61 62

count

Figure 12: The number of submitted and accepted papers per (primary) subject area. The names of the subject areas corresponding to each of the numbers on the x-axis are provided in Appendix A.

of homogeneity of the two distributions failed to detect any signiﬁcant difference between the two distributions: p=0.6029, χ2(dof = 62, #samples = 2425) = 57.51.
Summary 6: Distribution across subject areas
Key observations: • No observable bias across subject areas in terms of ﬁnal acceptances
Action items: • Test for systematic biases for/against any subject area before announcing ﬁnal decisions
Open problems: • How to assimilate different, subjective opinions of reviewers across different subject areas

3.7 Quantifying the randomness
Quantifying the extent to which the outcome of a peer-review process is different from a random selection of papers is one of the most pressing questions for the scientiﬁc community (Somerville, 2016). In this section we conduct two analyses to quantify the randomness in the review scores in NIPS 2016.
3.7.1 Messy middle model
The NIPS 2014 experiment (Lawrence and Cortes, 2014) led to the proposal of an interesting “messy middle” model (Price, 2014). The messy middle model postulates that the best and the worst papers are clear accepts and clear rejects, respectively, whereas the papers in the middle suffer from random decisions that are independent of the content of the papers. The messy middle model is obviously a stylized model, but it nevertheless suggests an interesting investigation into the randomness in the reviews and decisions of the papers that lie in the middle. In this section, we describe such an investigation using the NIPS 2016 data.
The messy middle model assumes random judgments for the middle papers. If the messy middle model were correct then for any pair of papers in the middle, and any pair of common reviewers, the probability of an agreement on the relative ranking of the two papers must be identical to the probability of disagreement. With this model in mind, we restrict attention to the papers in the middle, and then measure how far the agreements of the reviewers are from equiprobable agreements and disagreements. An analysis of this quantity for various notions of the “middle” papers yields insight into the messiness in the reviews for papers in the middle.
Procedure: We now describe the procedure employed for the analysis. Here we let n denote the total number of papers submitted to the conference and β denote the fraction of papers accepted to
15

the conference (we have n = 2425 and β = 0.237 in NIPS 2016). The procedure is associated to two parameters: µ is the minimum number of samples required and α is a threshold of messiness. We choose µ = 100 and α = 0.01 in our subsequent analysis, noting that importantly, our overall conclusions are robust to these choices.
1. Rank order all papers with respect to their mean scores. Call this ordering as O.
2. For every t ∈ [0, 1] and b ∈ [0, 1] (up to some granularity), do the following.
2.1 Initialize variables nagree[t, b] = ndisagree[t, b] = 0. 2.2 Consider the set of papers obtained by removing the top t fraction of papers and bottom b
fraction of papers from O. Call this (unordered) set of “middle papers” as M . 2.3 If (β − t)n < µ or ((1 − β) − b)n < µ then continue to the next values of (t, b) in Step 2. 2.4 Consider any pair of reviewers and any pair of papers in M that is reviewed by both the
reviewers. We say that this pair of reviewers agrees on this pair of papers if both reviewers provide a higher mean score (taken across the features) to the same paper as compared to the other paper. We say that this pair disagrees if the paper rated higher by one reviewer (in terms of the mean score across the features) is rated lower by the other reviewer. Ties are discarded. We count the total number of such agreements (denoted as nagree[t, b]) and disagreements (denoted as ndisagree[t, b]) within each of the two pools.
3. Find the largest value of (1 − t − b) such that (nagree[t, b] + ndisagree[t, b]) ≥ µ and nagree[tn,ba]g+rene[dti,sba]gree[t,b] < 0.5 + α. This largest value of (1 − t − b) is deﬁned as the size of the messy middle.
Let us spend a moment interpreting some steps of the procedure. Step 2.3 as well as the µ-condition in Step 3 ensures that there are a sufﬁcient number of samples for any computation on the messy middle region. Speciﬁcally, the conditions (β − t)n < µ and ((1 − β) − b)n < µ ensure existence of a sufﬁcient number of papers above and below the acceptance threshold. Under this constraint, Step 3 then ﬁnds the largest window of papers in the middle such that the fraction of reviewer-agreements is at most (0.5 + α). Thus a smaller size of the window is a desirable property.
We can now use this analysis to compare messy middle window sizes for two or more conferences. When making such a comparison, we make one adjustment. In the last step (Step 3), we consider only those values of (t, b) such that nagree[t, b] + ndisagree[t, b] ≥ µ for both datasets. Then compare the sizes of the messy middle.
Results: We used this procedure to compute the size of the messy middle in NIPS 2016 and also in NIPS 2015. The granularity we used is 1/20, that is, t, b ∈ {0, 1/20, 2/20, . . . , 1}. NIPS 2015 had a marginally higher average number of reviews per paper as compared to NIPS 2016. We set µ = 100 and α = 0.01 (note that the conclusions drawn below are robust to these choices). The results of the analysis are tabulated in Figure 13a.
In the NIPS 2016 data, we observe that the size of the messy middle is 30%. Speciﬁcally, if we remove the bottom 70% of papers (and none of the top papers) then we see that the inter-reviewer agreements are near-random, but farther from random otherwise. On the other hand, we observe that the size of the messy middle is 45% in the NIPS 2015 data, which occurs when removing 15% of the top papers and 40% of the bottom papers. We thus see that in terms of this metric of the size of the messy middle, the NIPS 2016 review data is an improvement over the previous edition of the conference.
Such an analysis is useful in comparing the noise in the review data across conferences. It can particularly be useful to evaluate the effects of any changes made in the peer-review process. The ease of doing this post hoc analysis, without necessitating any controlled experiment, is a signiﬁcant beneﬁt to this approach of analysis. In order to enable comparisons of the size of the messy middle of NIPS 2016 with other conferences, we provide the values of nagree[tn,ba]g+rene[dti,sba]gree[t,b] and (nagree[t, b] + ndisagree[t, b]) for the NIPS 2016 data for all values of (t, b) in Appendix B.
It is important to note that this post hoc analysis is not strictly comparable to the NIPS 2014 controlled experiment because we do not have access to a true ranking or a counterfactual. That said, since such an analysis can easily be performed post hoc using the data from reviews and does not require any special arrangement in the review process, it would be useful to see how these results compare to the data from other conferences.
16

Conference NIPS 2015 NIPS 2016

Size of messy middle 45% 30%

(a) Size of the messy middle windows.

0.6

reject

0.4

poster oral

0.2

0.00.00

0.05

0.10

0.15

0.20

0.25

variance of bootstrapped samples

(b) Histogram of the variance of acceptance decisions (according to mean scores) of the papers in a bootstrapped analysis.

Figure 13: Amount of randomness in the reviews.

3.7.2 A bootstrapped analysis
In this section we conduct an analysis to measure the randomness in the reviews in the NIPS 2016 data compared to that of random selection. In our analysis, we ﬁrst conduct 1000 iterations of the following procedure. For each paper, we consider the set of reviewers who reviewed this paper. We then choose the same number of reviewers uniformly at random with replacement from the set of original reviewers for this paper. We then take the mean of the scores across all features and across all the sampled reviewers for that paper. Next we rank order all papers in terms of these mean scores and choose the top 23.7% of the papers as “accepted” in this iteration and the others as rejected.
Our analysis focuses on the variance of the acceptance decisions for each paper. At the end of all iterations, for each paper, we compute the fraction of iterations in which the paper was accepted. Letting βi ∈ [0, 1] denote this fraction for any paper i, the variance in the acceptance decisions for this paper equals βi(1 − βi). We plot a histogram of the computed variances (for every paper) in Figure 13b. For comparison, note that in an ideal world, the variance of the decisions for each paper would be zero. Observe that a large fraction of rejected papers as well as a large fraction of papers that were accepted as oral presentations have a near-zero variance. On the other hand, a notable fraction of papers accepted as posters as well as those rejected have a variance close to its largest possible value of 14 .
Summary 7: Quantifying the randomness
Key observations: • A notable subset of papers incurs “messy middle” randomness. • The messy middle region is smaller in NIPS 2016 as compared to NIPS 2015. • A bootstrapped analysis shows a signiﬁcant variance in reviewer scores for a notable fraction of papers that are accepted as posters. A large fraction of papers accepted for oral presentations or rejected have near-zero variance.
Action items: • Measure and compare post hoc goodness (using the analysis of this paper or otherwise) of various review processes in order to choose a good review process in a data-dependent manner.
Open problems: • Principled design of statistical tests for post hoc comparison of goodness of different review processes.
17

3.8 Ordinal data collection
The data collected from the reviewers in the NIPS 2016 review process comprises cardinal ratings (in addition to the free-form text-based reviews) where reviewers score each paper on four features on a scale of 1 to 5. A second form of data collection that is popular in many applications, although not as much in conference reviews, is ordinal or comparative ranked data. The ordinal data collection procedure that we consider asks each reviewer to provide a total ordering of all papers that the reviewer reviewed.
There are various tradeoffs between collecting cardinal ratings and ordinal rankings. In the context of paper reviews, cardinal ratings make reviewers read each individual paper more carefully (and not make snap judgments), and can elicit more than a just one bit of information. On the other hand, ordinal rankings allow for nuanced comparative feedback, help avoid ties, and are free of various biases and calibration issues that otherwise arise in cardinal scores (Harzing et al., 2009, Krosnick and Alwin, 1988, Russell and Gray, 1994, Rankin and Grube, 1980, Cambre et al., 2018). We refer the reader to the papers by Barnett (2003), Stewart et al. (2005), Shah et al. (2016a,b) and references therein for more details on ordinal data collection and processing. In the present paper, we present three sets of analyses with the ordinal rankings collected from reviewers.
3.8.1 Tie breaks
An ordinal ranking of the papers provided by a reviewer ensures that there are no ties in the reviewer’s evaluations. On the other hand, asking cardinal scores can result in scores that are tied, thereby preventing an opportunity for the AC to discern a difference between the two papers from the provided scores.
In order to evaluate the prevalence of ties under cardinal scores, we performed the following computation. For every {paper, paper, reviewer} triplet such that the reviewer reviewed both papers, and for any chosen feature (i.e., quality, novelty, impact, and clarity), we computed whether the reviewer provided the same score to both papers or not. We totaled such ties and non-ties across all such triplets.
Figure 14 depicts the proportion of ties computed across all submitted papers. The total sample size is 26106. Observe that a signiﬁcant fraction – exceeding 30% for each of the four features – of pairs of reviewer scores are tied. When only the top 2k papers were used in the calculation, the fraction of ties in each feature is even higher, by approximately 10% − 15% of the respective value in the setting of all papers. In conclusion, these results reveal a signiﬁcant proportion of ties in the cardinal scoring scheme and also conﬁrm that, by design, ties are inevitable in this scoring scheme. The use of ordinal rankings, on the other hand, does not suffer from such a drawback.
3.8.2 Consistency of ordinal ranking data
While there is substantial literature on beneﬁts of collecting data in an ordinal ranking form, several past works also recommend verifying if the application setting under consideration is appropriate for ordinal rankings. For instance, Russell and Gray (1994) state the beneﬁts of ranking for settings

1.00

non-ties

ties

0.75

0.50

0.25

0.00

quality novelty impact clarity
mean median

Figure 14: Proportion of ties in reviewer scores. The bars titled “mean” and “median” represent the mean and median scores across all four features.
18

“where the items are highly discriminable”; Peng et al. (1997) ask respondents to rank 18 values in order of importance but observe unstable and inconsistent results; Harzing et al. (2009) argue that ranking generally requires a higher level of attention than rating and that asking respondents to rank more than a handful of statements puts a very high demand on their cognitive abilities. Accordingly, this section is devoted to performing sanity checks on the ordinal ranking data obtained subsequent to the NIPS 2016 review process. We do so by comparing certain measures of consistency of the ordinal data with the cardinal data obtained in the main review process.
Agreements within ordinal rankings. For every pair of papers that have two reviewers in common, we compute whether these two reviewers agree on the relative ordinal ranking of the two papers or if they disagree. In more detail, we say that this pair of reviewers agrees on this pair of papers if both reviewers rank the same paper higher than the other in their respective ordinal rankings; we say that this pair disagrees if the paper ranked higher by one reviewer is rated lower by the other. Figure 15a depicts the proportion of disagreements for the ordinal rankings in the entire set of papers, as well as broken down by the type of reviewer. First, observe that the ordinal rankings have a similar level of consistency as that observed in the cardinal scores in Figure 9. Second, we observe no statistically signiﬁcant difference between the two pools: p=0.9849 for Pearson’s chi-squared test and effect size d= 0.0018. The sample sizes are 696, 348 and 56 for all reviewers, pool 1 and pool 2 respectively.
Agreement of ordinal rankings with cardinal ratings. Let us now evaluate how well the overall ordinal rankings associate with the cardinal scores given for the individual features. For every pair of papers that have a common reviewer, we compare whether the relative ordering of the cardinal scores for a given feature agree with the ordinal ranking given by the reviewer for the pair of papers. We report the proportion of disagreements in Figure 15b. We observe the high amount of agreement of the ordinal rankings with the cardinal scores – for instance, the median cardinal score agrees in about 90% of cases with the overall ordinal rankings provided by the reviewers.
Agreement of ordinal rankings with ﬁnal decisions. We ﬁnally compute the amount of agreement between the ordinal rankings provided by the reviewers and the ﬁnal decisions of acceptance. We consider all {paper, paper, reviewer} triplets where the reviewer reviewed both papers, and one of these papers was eventually accepted and the other was rejected. For every such triplet, we evaluate whether the reviewer had ranked the accepted paper higher than the rejected paper (“agreement”) or vice versa (“disagreement”). We report the proportion of agreements and disagreements in Figure 15c. We see that the agreement of the overall rankings with the eventual decisions is quite high – there are roughly ﬁve agreements for every disagreement.
When restricted to the top 2k papers, we observe that the disagreements of ordinal rankings with ﬁnal decisions increase to 27-28% in all three categories (overall, pool 1 and pool 2) from 16-17% in the case of all papers. Note that the experiments on inter-reviewer agreements do not permit an effective analysis when restricted to top 2k papers as the sample size reduces quadratically (that is, reduces to a fraction .472 ≈ .2 of the sample size with all papers).

fraction of disagreement

0.5 0.4 0.3 0.2 0.1 0.0 overall pool 1 pool 2
(a)

quality novelty impact clarity mean median
(b)

overall pool 1 pool 2
(c)

Figure 15: Fraction of disagreements (a) within ordinal rankings between different pairs of reviewer types; (b) between ordinal rankings and cardinal ratings (“mean” and “median” refer to the mean and median of the cardinal scores for the four features); and (c) between ordinal rankings and ﬁnal acceptance decisions.

19

3.8.3 Detecting anomalies
Ordinal rankings can be used to detect anomalies in reviews. We discuss this aspect in the Section 3.9.
Summary 8: Ordinal data collection
Key observations: • Ordinal rankings are a viable option for collecting reviewer opinions. • There are a large number of ties in ratings provided by reviewers: there are more than 30% ties in each feature and even greater fraction of ties in the top 2k papers. • Ordinal rankings can be used to check inconsistencies in the reviews.
Action items: • Use a hybrid collection method with cardinal ratings for individual features and an overall ordinal ranking to avail beneﬁts of both data-collection methods.
Open problems: • Perform controlled experiments in order to quantify the beneﬁts and possible issues with ordinal rankings. • Design algorithms to efﬁciently combine cardinal ratings for features and ordinal overall rankings to provide useful guidelines to area chairs for their decisions.
3.9 Checking inconsistencies
In this section, we propose an automated technique to help reduce some human errors and inconsistencies in the review process. In particular, we propose to automatically check for inconsistencies in the review ratings provided by the reviewers. On ﬁnding any such inconsistency, we propose to then have the area chairs either manually investigate this inconsistency or to manually or automatically contact the reviewer requesting an explanation. In what follows, we propose two notions of inconsistencies in regards to the NIPS 2016 review process and quantify their presence in the NIPS 2016 review data.
Anomalies in feature ratings. We investigate whether any reviewer indicated that paper “A” is strictly better than paper “B” in all four features, but rank paper “A” lower than paper “B” in the ordinal ranking. We ﬁnd that there are 55 such pairs of reviews provided by 44 distinct reviewers. If we restrict attention to the top 2k papers, we ﬁnd that that there are 10 such pairs of reviews provided by 10 distinct reviewers.2
Anomalies in fatal ﬂaws. We now investigate if there are cases when a reviewer indicated a fatal ﬂaw in a paper, but that reviewer ranked it above another paper that did not have a fatal ﬂaw according to the reviewer. We found 349 such cases across 176 such reviewers. The proportion of such cases is similar among volunteer and invited reviewers. Among the top 2k papers, there are 55 such pairs across 33 reviewers.
One may think that the number of such cases is large because ordinal survey was done after the review process, so people may not have remembered the papers well or may not have done a thorough job as they knew it would not count towards the reviews. However, the ordinal data actually is quite consistent with the cardinal data (Section 3.8.2). Hence we do not think such a large discrepancy with fatal ﬂaws can be explained solely due to such a delay-related noise.
Two possible explanations for such anomalies are as follows. Either the reviewer may not have done an adequate job of the review, or the set of provided features are grossly inadequate to express reviewers’ opinions. In either case, we suggest automatically checking for such glaring inconsistencies (irrespective of whether ordinal or cardinal ﬁnal ratings are used) during the review process, and contacting the respective reviewers to understand their reasoning.3 We hope that such a checkpoint will be useful in improving the overall quality of the review process.
2Note that the total number of pairs of papers reduces more than 4-fold when moving from the set of all papers to the top 2k set.
3This analysis was performed after completion of the review process, and hence reviewers were not contacted for these inconsistencies.
20

Summary 9: Checking inconsistencies
Key observations:
• 55 instances (across 44 reviewers) of a reviewer rating a paper higher than another for all features but inverting the relative ranking of the two papers in the overall ordering.
• 349 cases where a reviewer indicated a fatal ﬂaw in a paper but ranked it higher than another paper without any indicated fatal ﬂaw.
Action items:
• Check for inconsistencies in the reviews and contact respective reviewers.
Open problems:
• What are other inconsistencies that can be checked in an automated manner?
4 Discussion and conclusions
NIPS has historically been the terrain of much experimentation to improve the review process and this paper is our contribution to advance the state of the art in review process design. In this paper, we reported a post hoc analysis of the NIPS 2016 review process. Our analysis yielded useful insights into the peer-review process, suggested action items for future conferences, and resulted in several open problems towards improving the academic peer-review process, as enumerated throughout this paper.
Our tools include several means of detecting potential artifacts or biases, and statistical tests to validate hypotheses made: Comparing the distribution of topics in submitted papers and accepted papers; creating a graph of proximity of reviewers (according to commonly reviewed papers) and papers (according to common reviewers) to detect potential disconnected communities; test to compare two pools of reviewers; quantifying the noise in the review scores. We also observed that the histogram of scores obtained included a signiﬁcantly larger fraction of papers than the guidelines suggested. This observation suggests a more careful design of the elicitation interface and the type of feedback provided to authors.
Selection biases that arise when recruiting reviewers and ACs in a review process of this scale are difﬁcult to deal with. Some designs in the selection of reviewers lend themselves more to bias than others. In NIPS2016, we made some design choices of the review process with the intention of reducing these biases. For instance, the recruitment of volunteer author-reviewers helped increase the diversity of the reviewer pool. They were less prone to selection bias compared to selecting reviewers by invitation only, primarily based on AC recommendations. With respect to reducing bias across AC decisions, we introduced the “AC buddy system” in which pairs of ACs had to make decisions jointly about all their papers. This method scales well with the increase in number of papers, but is sub-optimal to calibrate well decisions since buddy pairs form disjoint decision units (no paper overlap between buddy pairs). However, decision processes based on a conference between several or all ACs, as done in earlier editions of the conference, are also not perfect because decisions are sometimes dominated by self-conﬁdent and/or opinionated ACs. Although the evidence we gathered from our analyses did not reveal any “obvious” bias, it does not mean that there is none. We hope that some designs of our review process will shed some lights on ways of improving bias-immune or bias-avoidance procedures for future conferences.
The reviews themselves were of mixed quality, but recruiting more reviewers (between 4 and 6 per paper) ensured that each paper had a better chance to get a few competent reviews. We gave a strong role to the ACs who arbitrated between good and bad reviews and made the ﬁnal decision, which was not just based on an average score. To recruit more reviewers (and possibly a more diverse and less biased set of reviewers) we introduced the new idea to invite volunteer author reviewers, which we think is a good contribution. In particular, next to many PhD students, this brought a considerable amount of senior reviewers in the system as well. Some of the ACs systematically disregarded volunteer reviews, judging that they could not be trusted. But, our analysis did not reveal that reviewers from that pool made decisions signiﬁcantly different from the pool of reviewers invited by recommendation. However, more senior reviewers seem to put more effort into providing detailed reviews, and participating to rebuttals and discussions. Hence we need to ﬁnd means of
21

encouraging and possibly educating more junior reviewers to participate in these aspects. As a means of self-assessment and encouragement, reviewers could receive statistics about review length, amount of agreement between reviewers, and participation to rebuttals and discussions, as well as ﬁgures concerning their own participation. Naturally, the participation of junior reviewers in the review process is a form of education. It would be nice to track from year to year whether individual reviewers ramp up their review length, level of agreement with other reviewers, and participation in discussions and rebuttals. Note that we believe that such statistics should not be used as a means of selecting reviewers because this could bias the selection.
It is an on-going debate to which extent the decision process should be automated and what means could be used to automate it. We provide some elements to fuel this discussion. We evaluated how rebuttals and discussions change the scores. Although this concerns only a minority of papers, we believe that ACs have a key role in arbitrating decisions when there is a controversy and that this is not easy to monitor merely with scores. Since scores do not seem to be consistently updated by reviewers after rebuttal/discussions, maybe the review process should include a score conﬁrmation to make sure that absence of change in score is not due to negligence. Mixing ordinal and cardinal scores may reduce the problems of reviewer calibration, tie breaking, and identifying anomalies possibly due to human error.
All in all, it is important to realize that in a review process of this scale, there is not a single person who really controls what is going on at all levels. Program chairs spend a lot of time on quality control, but deﬁnitely cannot control the decisions on all individual papers or the quality of individual reviewers. In the end, we have to trust the area chairs and reviewers: the better reviews all of us provide, the better the outcome of the review process. We as a community must also continue to strive improving the peer-review process itself, via experiments, analysis, and open discussions. This topic in itself is a fertile ground for future research with many useful open problems including those enumerated throughout the paper.
Acknowledgments
This work would not have been possible without the support of the NIPS foundation and the entire committee of the NIPS 2016 conference. We thank the program chairs and program managers of the NIPS 2015 conference for their help during the organization. We thank Baiyu Chen for preliminary experiments conducted on ordinal data. Isabelle Guyon acknowledges funding from the Paris-Saclay scientiﬁc foundation. Ulrike von Luxburg has been supported by the Deutsche Forschungsgemeinschaft (Institutional Strategy of the University of Tübingen, ZUK 63). Krikamol Muandet acknowledges fundings from the Faculty of Science, Mahidol University and the Thailand Research Fund (TRF).
References
W. Barnett. The modern theory of consumer behavior: Ordinal or cardinal? Quarterly Journal of Austrian Economics, 6(1):41–65, 2003.
A. R. Benson, D. F. Gleich, and J. Leskovec. Tensor spectral clustering for partitioning higher-order network structures. In SIAM International Conference on Data Mining, pages 118–126. SIAM, 2015.
Julia Cambre, Scott Klemmer, and Chinmay Kulkarni. Juxtapeer: Comparative peer review yields higher quality feedback and promotes deeper reﬂection. In CHI, 2018.
Laurent Charlin and Richard Zemel. The toronto paper matching system: an automated paper-reviewer assignment system. In ICML, 2013.
T. Fruchterman and E. Reingold. Graph drawing by force-directed placement. Software: Practice and experience, 21(11):1129–1164, 1991.
Anne-Wil Harzing, Joyce Baldueza, Wilhelm Barner-Rasmussen, Cordula Barzantny, Anne Canabal, Anabella Davila, Alvaro Espejo, Rita Ferreira, Axele Giroud, Kathrin Koester, et al. Rating versus ranking: What is the best way to reduce response and language bias in cross-national research? International Business Review, 18(4):417–432, 2009.
22

Jon A Krosnick and Duane F Alwin. A test of the form-resistant correlation hypothesis: Ratings, rankings, and the measurement of values. Public Opinion Quarterly, 52(4):526–538, 1988.

N. Lawrence and C. Cortes. The NIPS Experiment. http://inverseprobability.com/2014/ 12/16/the-nips-experiment, 2014. [Online; accessed 3-June-2017].

J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney. Statistical properties of community structure in large social and information networks. In Proceedings of the 17th international conference on World Wide Web, pages 695–704. ACM, 2008.

M. Merriﬁeld and D. Saari. Telescope time without tears: a distributed approach to peer review. Astronomy & Geophysics, 50(4):4.16–4.20, 2009.

J. Mervis.

Want a grant?

First review someone else’s proposal.

Sci-

encemag News, July 2014. URL http://www.sciencemag.org/news/2014/07/

want-grant-first-review-someone-elses-proposal.

A O’Hagan, C Buck, A Daneshkhah, J Eiser, P Garthwaite, D Jenkinson, J Oakley, and T Rakow. Uncertain judgements: eliciting experts’ probabilities. John Wiley & Sons, 2006.

R. Olfati-Saber, J. A. Fax, and R. Murray. Consensus and cooperation in networked multi-agent systems. Proceedings of the IEEE, 95(1):215–233, 2007.

Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values across cultures and possible solutions. Psychological methods, 2(4):329, 1997.

E. Price. The NIPS experiment. http://blog.mrtz.org/2014/12/15/the-nips-experiment. html, 2014. [Online; accessed 3-June-2017].

Azzurra Ragone, Katsiaryna Mirylenka, Fabio Casati, and Maurizio Marchese. On peer review in computer science: Analysis of its effectiveness and suggestions for improvement. Scientometrics, 97(2):317–356, 2013.

William L Rankin and Joel W Grube. A comparison of ranking and rating procedures for value system measurement. European Journal of Social Psychology, 10(3):233–246, 1980.

Philip A Russell and Colin D Gray. Ranking or rating? some data and their implications for the measurement of evaluative response. British journal of Psychology, 85(1):79–92, 1994.

N. B Shah, S. Balakrishnan, J. Bradley, A. Parekh, K. Ramchandran, and M. J Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. Journal of Machine Learning Research, 17:1–47, 2016a.

N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. IEEE Transactions on Information Theory, 2016b.

A. Somerville. A Bayesian analysis of peer reviewing. Signiﬁcance, 13(1):32–37, 2016.

N. Stewart, G. Brown, and N. Chater. Absolute identiﬁcation by relative judgment. Psychological review, 112(4):881, 2005.

A. Janet Tomiyama. Getting Involved in the Peer Review Process. Psychological Science Agenda, American Psychological Association. http://www.apa.org/science/about/psa/2007/06/ student-council.aspx, 2007. [Online; accessed 27-January-2018].

Rachel Toor. Reading Like a Graduate Student. The Chronicle of Higher Education. https://www. chronicle.com/article/Reading-Like-a-Graduate/47922, 2009. [Online; accessed 27January-2018].

23

APPENDIX

In the appendix we present some additional details about the experiments.

A Subject areas

Here are the subject areas associated to the subject area numbers in Figure 12.

1. Deep learning/Neural networks

32 Causality

2. (Application) Computer Vision

33 Bayesian nonparametrics

3. Learning theory

34 Variational inference

4. Convex opt. and big data

35 Similarity and Distance Learning

5. Sparsity and feature selection

36 (Other) Statistics

6. Clustering

37 Spectral methods

7. Reinforcement learning

38 Active Learning

8. Large scale learning

39 Graph-based Learning

9. Graphical models

40 (Other) Bayesian Inference

10. Bandit algorithms

41 (Application) Collab. Filtering / Recommender Systems

11. Matrix factorization

42 Information Theory

12. Online learning

43 (Application) Signal and Speech Processing

13. (Other) Optimization

44 (Application) Social Networks

14. (Other) Neuroscience

45 (Other) Robotics and Control

15. Kernel methods

46 Nonlin. dim. reduction

16. Gaussian process

47 Model selection and structure learning

17. Multitask/Transfer learning

48 Ensemble methods and Boosting

18. Component Analysis (ICA, PCA, . . . )

49 Stochastic methods

19. Combinatorial optimization

50 (Other) Cognitive Science

20. Time series analysis

51 Structured prediction

21. (Other) Probabilistic Models and Methods 52 Ranking and Preference Learning

22. (Other) Applications

53 Game Theory and Econometrics

23. (Other) Machine Learning Topics

54 (Application) Privacy, Anonymity, Security

24. (Cognitive/Neuro) Theoretical Neuroscience 55 (Cognitive/Neuro) Perception

25. (Other) Unsupervised Learning Methods 56 (Application) Bioinfo. and Systems Bio.

26. MCMC

57 Regularization and Large Margin Methods

27. Semi-supervised

58 (Other) Regression

28. (Other) Classiﬁcation

59 (Application) Information Retrieval

29. (Application) Natural Language and Text 60 (Application) Web App. and Internet

30. (Application) Object and Pattern Recognition61 (Cognitive/Neuro) Reinforcement Learning

31. (Cognitive/Neuro) Neural Coding

62 (Cognitive/Neuro) Language

B Messy middle details
In Figure 16 we provide the values of the fraction of agreements r := nagree[tn,ba]g+rene[dti,sba]gree[t,b] at the top of the corresponding cell and number of pairs m := (nagree[t, b] + ndisagree[t, b]) for every value of (t, b) at the bottom of the corresponding cell. Note that the values are computed for all values of (t, b) ignoring the sample size restriction imposed by Step 2.3 of the procedure outlined in Section 3.7.1. Each cell in the table is color-coded by the size of the 95% conﬁdence interval (on a log-scale) computed as (2 × 1.96) r(1m−r) .

24

top % papers removed (t)

95 - - - - - - - - - - - - - - - - - - - -

90 02.532 - - - - - - - - - - - - - - - - - - -

0.7

85 03.672 01.800 - - - - - - - - - - - - - - - - - -

80 06.611 02.673 01.579 - - - - - - - - - - - - - - - - -

75 09.534 04.582 03.418 01.400 - - - - - - - - - - - - - - - -

0.5

70 01.1594 06.499 04.468 02.454 01.450 - - - - - - - - - - - - - - -

65 01.4574 08.541 05.477 03.432 02.400 0.838 - - - - - - - - - - - - - -

60 01.8565 01.1583 08.479 05.474 03.491 02.462 0.743 - - - - - - - - - - - - -

0.3

55 02.1586 01.4583 01.1540 08.425 05.485 04.425 02.400 0.743 - - - - - - - - - - - -

50 02.6516 01.8573 01.4489 01.1435 08.445 06.442 03.339 01.323 0.520 - - - - - - - - - - -

45 03.1517 02.3515 01.8520 01.4406 01.0464 08.412 05.400 02.343 01.400 0.560 - - - - - - - - - -

40 03.5558 02.7526 02.2512 01.7449 01.3489 01.0477 07.457 04.457 02.592 01.477 0.862 - - - - - - - - -

35 04.3559 03.4546 02.8521 02.2499 01.9408 01.5456 01.2416 08.446 05.439 03.487 02.446 01.443 - - - - - - - -

30 05.1519 04.1567 03.4563 02.8591 02.4429 01.9476 01.5495 01.1456 07.498 06.405 04.423 02.433 0.933 - - - - - - -

25 05.8631 04.8559 04.0525 03.3582 02.8581 02.3478 01.9448 01.4458 01.0570 08.409 05.497 03.563 02.512 0.560 - - - - - -

20 06.5672 05.5661 04.6597 03.9585 03.4523 02.8541 02.3591 01.8541 01.4524 01.0581 08.418 05.439 03.447 01.540 0.944 - - - - -

0.1

15 07.4604 06.3642 05.3569 04.6527 04.0516 03.3564 02.8585 02.2585 01.7597 01.4516 01.1535 07.589 05.650 03.603 01.338 0.540 - - - -

10 08.6635 07.5614 06.4632 05.5680 04.8588 04.1557 03.6537 02.9578 02.4630 02.0509 01.6588 01.2559 09.579 06.612 03.444 01.477 0.757 - - -

5 09.6666 08.5615 07.3693 06.4641 05.6599 04.9508 04.3578 03.6589 03.0680 02.5589 02.1588 01.7610 01.3547 09.518 05.485 03.444 02.446 01.500 - -

0 100.6887 09.6667 08.4675 07.4683 06.6642 05.7680 05.2611 04.4682 03.8662 03.2661 02.8640 02.3621 01.8598 01.3588 01.0409 06.499 05.522 02.474 02.418 -

0

5

10

15

20

25

30

35 40 bottom %

p4a5pers50rem5o5ved6(0b)

65

70

75

80

85

90

95

Figure 16: The inter-reviewer agreement ratios in the messy middle. For each value of t and b,
we report two numbers: The agreement ratio r := nagree/(nagree + ndisagree) and the number of overlapping paper-reviewer pairs m := nagree + ndisagree. Each cell is color-coded by the size of the 95% conﬁdence interval (on a log scale).

25

