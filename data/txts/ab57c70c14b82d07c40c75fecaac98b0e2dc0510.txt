A Hierarchical Graphical Model for Record Linkage

Pradeep Ravikumar
Center for Automated Learning and Discovery, School of Computer Science, Carnegie Mellon University pradeepr@cs.cmu.edu

William W. Cohen
Center for Automated Learning and Discovery, School of Computer Science, Carnegie Mellon University
wcohen@cs.cmu.edu

Abstract
The task of matching co-referent records is known among other names as record linkage. For large record-linkage problems, often there is little or no labeled data available, but unlabeled data shows a reasonably clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods. In this paper, we describe a hierarchical graphical model framework for the record-linkage problem in an unsupervised setting. In addition to proposing new methods, we also cast existing unsupervised probabilistic record-linkage methods in this framework. Some of the techniques we propose to minimize overﬁtting in the above model are of interest in the general graphical model setting. We describe a method for incorporating monotonicity constraints in a graphical model. We also outline a bootstrapping approach of using “singleﬁeld” classiﬁers to noisily label latent variables in a hierarchical model. Experimental results show that our proposed unsupervised methods perform quite competitively even with fully supervised record-linkage methods.
1 Introduction
Databases frequently contain multiple records that refer to the same entity, but are not identical. The task of matching such co-referent records has been explored by a number of communities, including statistics, databases, and artiﬁcial intelligence. Each community has formulated the problem diﬀerently, and different techniques have been proposed.
In the database community, some work on record matching has been based on knowledge-intensive approaches [7, 6, 13]. More recently, the use of string-

edit distances as a general-purpose record matching scheme was proposed by Monge and Elkan [10, 9], and in previous work [2, 3], we developed a toolkit of various string-distance based methods for matching entitynames. The AI community has focused on applying supervised learning to the record-linkage task — for learning the parameters of string-edit distance metrics [14, 1] and combining the results of diﬀerent distance functions [15, 4, 1]. More recently, probabilistic object identiﬁcation methods have been adapted to matching tasks [12]. In statistics, a long line of research has been conducted in probabilistic record linkage, largely based on the seminal paper by Fellegi and Sunter [5].
In this paper, we follow the Fellegi-Sunter approach of treating the record-linkage problem as a classiﬁcation task, where the basic goal is to classify record-pairs as matching or non-matching. Many record-linkage problems are quite large, such as the matching of individuals and/or families between samples and censuses, e.g., in the evaluation of the coverage of the U.S. decennial census. Often for such large problems, there is little or no labeled data available, but unlabeled data shows reasonably clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods.
In this paper, we describe a hierarchical graphical model framework for approaching this problem. In addition to proposing new methods, we also cast existing unsupervised probabilistic record-linkage methods in the framework. The proposed graphical model has (k + 1) latent variables for records with k ﬁelds, and hence ﬁtting it to the data with minimal overﬁtting is a non-trivial task. We outline approaches to deal with this estimation problem in Section 4, some of which could also be utilized in more general graphical model applications. We address the problem of incorporating monotonicity constraints into a graphical model, which should be helpful in reducing overﬁtting in complex generative models where such constraints exist. We also outline a bootstrapping approach of

using “single-ﬁeld” classiﬁers to assign noisy labels to the latent variables in a hierarchical model. Results show that this enables us to capture constraints in the multi-ﬁeld-record data more eﬀectively. We also note that the proposed hierarchical model could be used to address the general problem of ﬁtting a graphical model to continuous data (Section 7).
Experimental results show that our proposed unsupervised methods are competitive with fully supervised record-linkage methods.
2 Preliminaries
Given two lists of records, A and B, we look at the the task of detecting the matching record-pairs (a, b) ∈ A× B. A record is basically a vector of ﬁelds, e.g., Figure 1. Thus, a record-pair is essentially a vector of ﬁeld-pairs. More generally, we can represent a record-pair (a, b) as a vector of features, often called a comparison vector: f (a, b) = f1(a, b), ..., fk(a, b), where f1, . . . , fk are the features.
Unless speciﬁed otherwise, we will consider the recordpair feature vector to be a vector of distances, one for each ﬁeld-pair. If there are k ﬁelds, we denote the feature vector by f , where fi is the distance feature for the ith ﬁeld.
The record-linkage problem is the classiﬁcation task of assigning the record-pair feature vectors to a label “matching” or “non-matching”. Denote the matchclass by a binary variable M , where M = 0 indicates a non-match and M = 1 indicates a match. The goal of probabilistic record-linkage is to formulate a probabilistic model for the match-class M and the feature vector f , and use the same to estimate the probability of the match class given the record-pair feature vector, P (M |f ). In an unsupervised setting, this amounts to estimating a generative model for (f , M ).
3 Graphical Models for existing Record-Linkage Methods
Existing unsupervised methods for probabilistic record-linkage use a generative model for the recordpair feature vector with a single latent match-class variable, as shown in Figure 2. Note that the generative model is for the record-pair feature vector rather than the record-pair itself. Some other generative models for classiﬁcation such as Naive-Bayes and TreeAugmented Naive Bayes form special cases of Figure 2.
The predominant problem with the graphical model in Figure 2 is that the fi feature values are continuous, which precludes the normal multinomial probability model for a Bayesian network. There are two

M: MATCH CLASS LATENT VARIABLE M

F1

F2

F3

F4

FEATURE VECTOR

Figure 2: Model with a single latent match-class
basic approaches to deal with continuous variables in a graphical model: we can restrict to speciﬁc families of parametric distributions e.g., Gaussian mixture models, or we can discretize the variables and learn the model over the discrete domain. Discretization: The f feature vector is discretized to a discrete-valued vector w. For example, one way of discretizing into binary values is:
wi = 01 iiff ffii ≤> θθii (1)
One could then ﬁt a multinomial probability model to the graphical model in Figure 2, as the observation values for the bottom layer are now discrete. This approach of binarization of the distance-features has been adopted by Winkler et al [16, 17] and is one of the baseline methods we compare our model to.
While the above approach enables using the eﬃcient estimation and inference machinery of discrete graphical models, it has the problem that discretization into a small number of values leads to a poor approximation of the continuous distribution. However, if we increase the number of discrete values d, there is a potential explosion in the number of parameters of the model. In the graphical model of Figure 2, if the average number of parents for a node is q, and each node has d values, then the number of multinomial parameters to estimate for k such nodes is O(kdq). This might cause standard estimation methods to overﬁt the data.
Using speciﬁc parametric families: Instead of using the discrete-valued multinomial distribution for the variables, we could use other parametric families which allow continuous values, and for which there exists an eﬃcient machinery for estimation and inference, such as the Gaussian distribution.
Gaussian distributions have the added advantage that a mixture of m Gaussians can model any probability distribution to arbitrary accuracy, provided m is large enough [8]. This suggests the following semi-supervised approach: we cluster the unlabeled feature-vectors using a Gaussian mixture model. We

NAME1 ADDRESS1 CITY1 OCCUP1 SEX1

RECORD 1

NAME2 ADDRESS2 CITY2 OCCUP2 SEX2

RECORD 2

dist(name1,name2) = 0.8

Address Match

City Match

Occup.

Sex

Non−match Match

RECORD−PAIR FEATURE VECTOR

Figure 1: Records and Record Pairs

MATCH CLASS LATENT VARIABLE M

X1

X2

X3

X4

LATENT MATCH−CLASS VARIABLES

F1

F2

F3

F4

FEATURE VECTOR

Figure 3: Hierarchical generative model for Record Linkage

then use the limited labeled training data to label the clusters. The most frequently occurring label in the labeled feature vectors of a cluster is the label assigned to that cluster. This approach is another of the baselines we compare our model to.
Supervised Learning: One could also train the graphical model in Figure 2 in a supervised learning setting, if adequate labeled training data is available. As in [15, 4, 1], we could train, for example, a binary SVM classiﬁer to predict the match-class given the continuous-valued feature-vector. Since this is a fully supervised method, it is not applicable when little or no labeled data is available.
4 A hierarchical graphical model for record-linkage
The graphical model for record linkage in Figure 2 can be generalized to a hierarchical three-layer model as shown in Figure 3. The bottom layer f in this model is a feature-vector layer as before, where each node in this layer corresponds to a distance-feature fi. The diﬀerence in this model, when compared to Figure 2, is the set of binary latent variables xi for each distance feature fi. The match-class latent variable M in turn depends on these intermediate latent variables. Thus, we have a hierarchical mixture model with (k + 1) latent variables and k dimensional observed data, given

k ﬁelds. Rationale for the hierarchical model: While the intermediate latent variables as described above are operationally free to take any value, superimposing a certain semantic interpretation upon the latent variables shall give an intuition for the hierarchical model, as well as allow us to constrain the model in order to make the estimation of the structure and parameters easier.
Speciﬁcally, one could interpret the binary-valued middle layer x nodes in Figure 3 as latent match variables for each ﬁeld. Thus, each node xi in the middle layer corresponds to the match-class of a single ﬁeldpair distance feature fi. The top node in Figure 3 is the record-match class latent variable, which gives the match class of the entire record-pair, and which depends on the latent match class variables xi of the individual ﬁelds.
Thus, P (xi|fi) gives the error model for ﬁeld i. Assuming an independence of error models, there can be dependencies among nodes only in the middle layer, and all the bottom layer nodes are independent conditional on their latent-classes in the middle layer. This captures a natural intuition, since when we talk about dependencies between ﬁeld-pairs, we imply a dependency between the match-classes of the ﬁeld-pairs rather than the particular values themselves. Thus, when we say that the address ﬁeld-pair is dependent on the name ﬁeld-pair, we intuitively imply only that an address-match is dependent on a name-match. This is what is captured by the above graphical model.
As we model dependencies only between nodes in the middle layer, which are binary valued, we do not have to estimate as many parameters as a discretization of the model in Figure 2. Thus, assuming the average number of parents of a node in the middle layer is q, the number of multinomial parameters for any node in the middle layer is O(2q) and for all k nodes is O(k2q). This is as opposed to O(kdq) if we directly modeled dependencies in Figure 2 after discretization as in the previous section.
However, the number of multinomial parameters in the graphical model of Figure 3 is still quite large, which is

not surprising given that it is a hierarchical latent variable model with (k + 1) latent variables and k observation variables. Hence, normal estimation techniques like EM would overﬁt the data. The remainder of the paper describes assumptions and constraints that reduce overﬁtting substantially: in fact, the ﬁnal model does overﬁt, but performance is still very good and is competitive with supervised approaches.
5 Probability Estimation in the general model
Estimating the probabilities in the graphical model of Figure 3 via structural EM without any constraints is expensive with respect to both computation and generalization-error. Hence, we impose three types of constraints on the model before performing structural EM on the same. Our experiments thus follow the procedure below:
• We discretize each node in the bottom layer fi to have d values. This does not cause a blowup of parameters since each bottom-layer node fi has only one parent xi, and thus the number of multinomial parameters for each node fi is just O(d).
• We then impose further constraints on the model as described in this section.
• Given the constraints, we then use structural EM to estimate the structure of the dependencies in the middle layer, and the parameters of the entire model.
5.1 Semantic Constraints
As described in Section 4, we can impose a semantic interpretation on the latent variables of the hierarchical graphical model: the middle layer x nodes as latent match-classes for the individual ﬁelds, and the top node M as the match-class for the entire record-pair. This gives the constraint:
P (M = 1|x) = 10 iofthxeerwq i1se (2)
The constraint in Equation 2 follows from the intuitive expectation that for a matching-record pair, the individual ﬁeld-pairs would also be matches. Note that this does not assume that the ﬁeld-pairs are error-free. In fact, given that the xi nodes represent the true match class for each individual ﬁeld-pair i, P (xi|fi) captures the error model for ﬁeld i.
The match-class probability conditional on the obser-

vations can be estimated from Figure 3 as:

P (M = 1|f ) = P (M = 1, x|f )

(3)

x

As the overall match-class latent variable M at the root is independent of the feature vector f of the bottom layer, given the latent match-classes x, we have

P (M = 1, x|f ) = P (M = 1|x)P (x|f ) (4)

x

x

From Equations 3 and 4, and the constraint in Equation 2 it follows:

P (M = 1|f ) =

P (M |x)P (x|f )

x

= P (M = 1|x = 1)P (x = 1|f ) + 0

= P (x = 1|f )

X1

X2

X3

X4

LATENT MATCH−CLASS VARIABLES

F1

F2

F3

F4

FEATURE VECTOR

Figure 4: Two Layer Model
Thus, it follows that the dependencies between the latent variable M and the latent variable layer x need not be modeled, and the graphical model in Figure 3 reduces to a non-hierarchical latent variable model with only the middle and bottom layers as shown in Figure 4. The record-linkage task thus reduces to a more manageable task of estimating P (x = 1|f ) after learning the structure and parameters of the model in Figure 4.
5.2 Monotonicity Constraints
Given that the parent’s label is “match”, the probability of a node also having the label “match” is greater than having the label “non-match”. The multinomial model of Figure 4 is still prone to overﬁtting as it does not capture such monotonicity constraints.
Speciﬁcally, if we discretize fi ∈ [0, 1], with a higher discrete value indicating a higher degree of “match”, we would like to enforce the monotonicity constraint:
j1 ≥ j2 ⇒ P (fi = j1|xi = 1) ≥ P (fi = j2|xi = 1)
We can capture these constraints by a simple modiﬁcation to the multinomial model. Consider the standard multinomial model with P r(x) = i pni i where pi are

the multinomial parameters and ni are the counts. We want to enforce additional constraints:

pi ≥ pj if i > j

(5)

We can do this by requiring pi = pi−1 + ∆i and constraining ∆i to be positive. This leads to the reparametrization:

k
p1 = ∆1, p2 = ∆1 + ∆2, . . . , pk = ∆j
j=1
∆i ≥ 0 for i = 1, . . . , k

The constraint in Equation 5 follows.

Estimation of the ∆ parameters The parameters pi of the standard multinomial model P r(x) = i pni i , when estimated via maximum likelihood, are given by pi = niini .

We want to estimate the ∆i parameters by maximiz-

ing the likelihood P r(x) =

i(

i j=1

∆j

)ni

under

the

constraints ∆i ≥ 0,

i(

i j=1

∆j

)

=

1.

We cast this

as an unconstrained optimization problem by using a

Lagrange multiplier for the equality constraint, and

using a barrier function for the inequality constraints.

The barrier function attains value 0 if any ∆i < 0 and

attains value 1 otherwise. In other words, we want

the barrier function to be a step function at zero, but

would also like it to be continuous and smooth to fa-

cilitate an easy optimization of the objective function.

We

use

the

sigmoid

function

σ(x) =

1 1+e−ax

for

such

a

barrier function. In our experiments, we used a = 20.

The optimization function then becomes:

i
( ∆j)ni σ(∆i) − λ[

i
( ∆j) − 1]

i j=1

i j=1

where λ is a Lagrange multiplier.

On setting the derivative of the above function to zero, we get the following ﬁxed point equations:

pi = ni − gipi

(6)

N − j gj pj

where N =

i ni, pi =

i j=1

∆j

,

gi

=

a(σ(∆i) −

σ(∆i+1)) for i < k and gk = a(σ(∆k) − 1).

The optimal estimates are thus given by the following iterative updates:

∆ˆ (it) = pˆi(t) − pˆ(i−t)1

gi(t) = a(σ(∆ˆ (it)) − σ(∆ˆ (i+t)1))

pˆ (t+1) = ni − gi(t)pˆi(t)

i

N − g(t)pˆ(t)

jj j

The convergence rate is quite fast empirically, averaging around 20 iterations for our datasets.
5.3 Bootstrapping with noisy labels
In this approach, we take a “single-ﬁeld” classiﬁer, which predicts the class-label xi given a single ﬁeldpair fi, and use its output to noisily label the latent match-class variable xi given the observed ﬁeld-pair feature fi. Thus, in Figure 4, we have noisy labels for the latent match-class variables in the middle layer, and observed values for the distance feature variables in the bottom layer. We can thus learn the structure and parameters of the model as if in a completely supervised setting. In other words, we are able to bootstrap the model by allowing it to combine the outputs of k single-ﬁeld classiﬁers for each of the k ﬁelds.
Our experiments show that while training the entire model in this way, using noisy labels, performs better than using plain structural EM, there seems to be a problem of overﬁtting the noise in the labels. Thus, by labeling only a part of the unlabeled data using the noisy-label approach above, and performing EM on both the unlabeled and the noisily labeled data, as in [11], we are able to get even better results. In our experiments, we use a classiﬁer based on the SoftTFIDF distance-metric[2].
6 Results
We have used two datasets to evaluate and compare the above methods, both of which have labeled records consisting of many correlated ﬁelds. The “census” dataset is a synthetic, census-like dataset containing 841 records, from which only textual ﬁelds were used (last name, ﬁrst name, middle initial, house number, and street). The “restaurant” dataset contains 864 restaurant names and addresses with 112 duplicates, the ﬁelds being restaurant name, street address, city and cuisine.
Since it is not computationally practical to consider all pairs of records, we use a “blocking” method that outputs a smaller set of candidate pairs. For the moderate-size test sets considered here, we consider all pairs that share some character 4-gram. This 4gram blocker ﬁnds an average of 99% of the correct pairs.
To evaluate a method on a dataset, we ranked all candidate pairs from the appropriate grouping algorithm by the posterior probability of the match-class given the observations. Following our earlier work [3], we computed the non-interpolated average precision of this ranking, the maximum F1 score of the ranking, and also interpolated precision at the eleven recall lev-

els 0.0, 0.1, . . . , 0.9, 1.0. Precision of a ranking con-

taining N pairs for a task with m correct matches at a

position i is the fraction of pairs ranked before position

i

that

are

correct,

i.e.,

c(i) i

where

c(i)

is

the

number

of correct pairs ranked before position i. Recall at a

position i is the fraction of correct pairs ranked before

position i, i.e, cm(i) . F1 score at a position i is the har-

monic mean of recall and precision at that position,

2pr p+r

.

The non-interpolated average precision is

1 m

N r=1

c(i)iδ(i) ,

where δ(i)

=

1 if the pair at

rank i is correct and 0 otherwise. Interpolated pre-

cision at recall r is the maxi c(ii) , where the max is

taken

over

all

ranks

i

such

that

c(i) m

≥

r.

6.1 Baseline Methods
We compare our hierarchical model and its modiﬁcations to various baseline methods in Table 1. The baseline methods are described in detail in Section 2. The Winkler unsup method refers to the unsupervised EMbased estimation of parameters in the 2 layer model of Figure 2 as in [16, 17]. Winkler sup refers to a supervised Maximum Likelihood Estimation of the above 2 layer model given fully labeled data. Winkler semisup refers to a semi-supervised EM-based estimation of parameters given partially labeled data (one-third of a dataset) as in [11]. The Gaussian Mixture Model, described in Section 2, is also a semi-supervised method. Our experiments used a mixture model of 6 Gaussians. We used the SoftTFIDF distance metric [2] for all ﬁeld distances. For the binarization in the Winkler methods above, a threshold of 0.8 was used. In the case of supervised and semi-supervised methods, three-fold cross-validation was used to evaluate performance.
Figure 5 compares our proposed hierarchical graphical model (HGM) to the above methods. Note that the HGM is a completely unsupervised method requiring no labeled data, and hence the comparison against semi-supervised and supervised methods stacks the odds against it. As Figure 5 shows, the HGM clearly outperforms baseline unsupervised methods, and is competitive with even fully supervised and semi-supervised methods.

6.2 Comparisons of modiﬁcations to the HGM
6.2.1 Semantic Constraints
From Table 1, we see that the hierarchical model, in the absence of semantic constraints on the latent variables, is not able to ﬁt the data at all. However, the with the addition of semantic constraints, i.e. the

model of Figure 4, the performance of the model rises well above other baseline unsupervised methods.
6.2.2 Monotonicity Constraints
As in Section 5.2, we modify the likelihood function by appending a barrier function, in order to satisfy certain monotonicity constraints implicit in the graphical model. From the table 1, we see that incorporating monotonicity constraints raise the performance markedly for the restaurant dataset, and slightly for the census dataset.
6.2.3 Bootstrapping
As the Table 1 shows, training the hierarchical model (Figure 4) by noisily labeling the latent match-class nodes using a “single-ﬁeld classiﬁer” as described in Section 5.3, leads to a great improvement in performance. We also observe that adding the monotonicity constraints in addition to the bootstrapping, does not lead to a further increase in performance. This indicates that the bootstrapping approach is also able to constrain the model monotonically for this dataset. But for other datasets, adding both methods could conceivably improve performance to a greater extent.
7 Conclusions
For large record-linkage problems, often there is little or no labeled data available, but unlabeled data has reasonably clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods. We have described a hierarchical latent variable graphical model (Figure 3) to perform record-linkage in such an unsupervised setting. Existing generative record-linkage methods can also be cast as special cases of the above general model. The hierarchical model has k + 1 latent variables for k observations, and hence ﬁtting it to the data with minimal overﬁtting is a non-trivial task. We outline approaches to the estimation problem which are applicable even in a general graphical model setting. We address the problem of incorporating monotonicity constraints in a graphical model, which should be helpful in reducing overﬁtting in complex generative models where such constraints exist. We also outline a bootstrapping approach of using a single-ﬁeld classiﬁer to assign noisy labels to the latent variables in the hierarchical model, which as results show, enable us to capture constraints in the data more eﬀectively.
We also note that the above hierarchical model, and the estimation methods therein, can be used to address the general problem of ﬁtting a graphical model to continuous data. For this continuous variable prob-

Method
Winkler semi-supervised supervised

Restaurant AvgPrec MaxF1

0.900 0.902

0.900 0.904

Census AvgPrec MaxF1

0.667 0.679

0.785 0.784

Winkler unsupervised Gaussian Mixture Model

0.617 0.702

0.568 0.704

0.495 0.242

0.612 0.388

Hierarchical Graphical Model

0.102

0.106

0.101

0.116

Semantically Constrained HGM + Monotonic Constraints + Bootstrap + both

0.786 0.795 0.820 0.820

0.820 0.823 0.844 0.844

0.727 0.728 0.728 0.728

0.758 0.759 0.759 0.759

Table 1: Average precision and MaxF1 values for the record-linkage methods

Precision Precision

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4 Hierarchical Graph. Model

Winkler Supervised

0.3

Winkler Unsupervised

0.4 Hierarchical Graph. Model

Winkler Supervised

0.3

Winkler Unsupervised

0.2

0.2

0.1

0.1

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Recall

Recall

Figure 5: Comparison of the graphical models: Restaurant dataset to the left, Census dataset to the right

lem, ﬁtting a Gaussian mixture model performs poorly on our datasets, and discretization does not allow full freedom in modeling dependencies between variables as it suﬀers from a combinatorial explosion in the number of parameters. The method described in our paper, of introducing a latent variable for each node, and modeling dependencies only between the latent classes reduces the dimensionality of the parameter space considerably. Also sometimes, as with the record-linkage case, it makes more intuitive sense to model dependencies only in a latent-match-class layer.
As Figure 5 shows, the unsupervised methods we propose are competitive with even fully supervised methods.
References
[1] M. Bilenko and R. Mooney. Learning to combine trained distance metrics for duplicate detection in databases. Technical Report Technical Report AI 02-296, Artiﬁcial Intelligence Lab, University of Texas at Austin, 2002.

[2] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A comparison of string distance metrics for namematching tasks. In Proceedings of the IJCAI2003 Workshop on Information Integration on the Web, 2003.
[3] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A comparison of string metrics for matching names and records. In Proceedings of the KDD2003 Workshop on Data Cleaning, Record Linkage, and Object Consolidation, 2003.
[4] W. W. Cohen and J. Richman. Learning to match and cluster large high-dimensional data sets for data integration. In Proceedings of KDD-2002, 2002.
[5] I. P. Fellegi and A. B. Sunter. A theory for record linkage. Journal of the American Statistical Society, 64:1183–1210, 1969.
[6] H. Galhardas, D. Florescu, D. Shasha, and E. Simon. An extensible framework for data cleaning. In ICDE, page 312, 2000.

[7] M. Hernandez and S. Stolfo. The merge/purge problem for large databases. In Proceedings of the 1995 ACM SIGMOD, May 1995.
[8] G. J. McLachlan and K. E. Basford. Mixture Models: Inference and Applications to Clustering. Marcel Dekker, New York, 1988.
[9] A. Monge and C. Elkan. The ﬁeld-matching problem: algorithm and applications. In Proceedings of KDD-96, August 1996.
[10] A. Monge and C. Elkan. An eﬃcient domainindependent algorithm for detecting approximately duplicate database records. In The proceedings of the SIGMOD 1997 workshop on data mining and knowledge discovery, May 1997.
[11] K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell. Text classiﬁcation from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134, 2000.
[12] H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. Identity uncertainty and citation matching. In Advances in Neural Processing Systems 15, Vancouver, British Columbia, 2002. MIT Press.
[13] V. Raman and J. Hellerstein. Potter’s wheel: An interactive data cleaning system. In The VLDB Journal, pages 381–390, 2001.
[14] E. S. Ristad and P. N. Yianilos. Learning string edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522– 532, 1998.
[15] S. Tejada, C. A. Knoblock, and S. Minton. Learning object identiﬁcation rules for information integration. Information Systems, 26(8):607–633, 2001.
[16] W. E. Winkler. Matching and record linkage. In Business Survey methods. Wiley, 1995.
[17] W. E. Winkler. The state of record linkage and current research problems. Statistics of Income Division, Internal Revenue Service Publication R99/04., 1999.

