ONLINE END-TO-END NEURAL DIARIZATION WITH SPEAKER-TRACING BUFFER
Yawen Xue1, Shota Horiguchi1, Yusuke Fujita1, Shinji Watanabe2, Paola Garc´ıa2, Kenji Nagamatsu1
1Hitachi, Ltd. Research & Development Group 2Center for Language and Speech Processing, Johns Hopkins University

arXiv:2006.02616v2 [eess.AS] 7 Mar 2021

ABSTRACT
This paper proposes a novel online speaker diarization algorithm based on a fully supervised self-attention mechanism (SA-EEND). Online diarization inherently presents a speaker’s permutation problem due to the possibility to assign speaker regions incorrectly across the recording. To circumvent this inconsistency, we proposed a speaker-tracing buffer mechanism that selects several input frames representing the speaker permutation information from previous chunks and stores them in a buffer. These buffered frames are stacked with the input frames in the current chunk and fed into a selfattention network. Our method ensures consistent diarization outputs across the buffer and the current chunk by checking the correlation between their corresponding outputs. Additionally, we trained SA-EEND with variable chunk-sizes to mitigate the mismatch between training and inference introduced by the speaker-tracing buffer mechanism. Experimental results, including online SA-EEND and variable chunk-size, achieved DERs of 12.54 % for CALLHOME and 20.77 % for CSJ with 1.4 s actual latency.
Index Terms— Online speaker diarization, speakertracing buffer, end-to-end, self-attention.
1. INTRODUCTION
With the recent advances in technology, audio-based human interaction systems are becoming quite popular. For them to work correctly, it is crucial to provide relevant information about the speakers and the speech transcription. Speaker diarization — which answers the question “who speaks when”— is a crucial stage in the pipeline, since it can locate speaker turns and assign speech segments to speakers. Nowadays, speaker diarization has been widely studied in different scenarios; for example, meetings [1, 2], call-center telephone conversations [3, 4], and the home environment (CHiME-5, CHiME-6) [5–7].
Currently, few speaker diarization systems can be applied in practical scenarios because most of them work well only under speciﬁc conditions such as long latency, no overlap, or low noise level [8, 9]. An online speaker diarization system with low latency is still an open technical problem. Online speaker diarization outputs the diarization result as soon as

the audio segment arrives, which means no future information is available when analyzing the current segment. In contrast, in an ofﬂine mode, the whole recording is processed so that all segments can be compared and clustered at the same time [10].
State-of-the-art speaker diarization systems mostly concentrate on integrating several components: voice activity detection, speaker change detection, feature representation, and clustering [11, 12]. Current research focuses primarily on the speaker model or speaker embeddings, such as Gaussian mixture models (GMM) [10, 13], i-vector [14–16], d-vector [17, 18], and x-vector [19, 20], and on a better clustering method such as agglomerative hierarchical clustering or spectral clustering [19, 21–23]. The issue with these methods is that they cannot directly minimize the diarization error because they are based on an unsupervised algorithm. A supervised online speaker diarization method UIS-RNN [12, 24] was proposed while the method still assumes only one speaker in one segment (no overlapping).
To solve these issues, Fujita, et al. [25–27] proposed an end-to-end speaker diarization system (EEND). Instead of applying several separate independent modules, EEND directly minimizes the diarization error by training a neural network using Permutation Invariant Training (PIT) with multi-speaker recordings. The experimental results show that the self-attention based end-to-end speaker diarization (SA-EEND) system [26,27] outperformed the state-of-the-art i-vector and x-vector clustering and long short-term memory (LSTM) [25] based end-to-end method. Although SA-EEND has achieved signiﬁcant improvement, it is only working in the ofﬂine condition which outputs speaker labels only after the whole recording is provided.
This paper aims to extend ofﬂine SA-EEND to online speaker diarization. First of all, we investigate a straightforward online extension of SA-EEND by performing diarization independently for each chunked recording. However, this straightforward online extension degrades the diarization error rate (DER) due to the speaker permutation inconsistency across the chunk, especially for short-length chunks. To circumvent this inconsistency, our proposed method, called speaker-tracing buffer, selects several input frames representing the speaker permutation information from previous chunks and stores them in a buffer. These buffered frames are

stacked with the input frames in the current chunk and fed into the self-attention network together so that our method obtains consistent diarization outputs across the buffer and the current chunk by checking the correlation between the corresponding outputs. Additionally, we also propose to train SA-EEND with variable chunk-sizes, which can mitigate the chunk size mismatch between training and inference due to the additional frames introduced by the above speaker-tracing buffer mechanism. Lastly, we focus on the performance of the proposed method with respect to the original SA-EEND in an online situation by testing on the simulated dataset which was created using two speaker recordings with controlled overlap ratio, and two real datasets, CALLHOME and Corpus of Spontaneous Japanese (CSJ) datasets. In order to make our results reproducible, the code will be published online.
2. RELATED WORK
Several online speaker diarization systems have already been developed along the last decade [8, 10, 12, 13, 16, 24]. Early online speaker diarization systems [10, 13] usually trained a Gaussian Mixture Model (GMM) from a huge amount of speech from different speakers to produce a universal background model. When a speech region was assigned to a new speaker, maximum a posteriori adaptation was applied to adjust the GMM to these new speakers. Later on, the GMM approach was replaced by adapted i-vector or d-vector [12, 16, 24] which are referred to as speaker embeddings that represent individual information from each speaker. The speaker embeddings are then compared and grouped using unsupervised or supervised clustering [12, 24]. In [8], an all-neural online approach that performed source separation, speaker counting, and diarization all together was proposed making it possible to optimize the entire online process. However, these systems are based on several separate modules and/or they can not directly minimize the diarization error.
Recently, end-to-end neural networks have been successfully applied to various speech processing ﬁelds such as speech recognition, speech synthesis, and voice conversion. Following this trend, Fujita et al., [25, 26] ﬁrst proposed an end-to-end neural diarization (EEND) with several extensions, e.g., to deal with variable numbers of speakers in [28–30]. While these extensions only consider an ofﬂine scenario, this paper focuses on extending the ofﬂine EEND method to an online scenario.
3. ANALYSIS OF ONLINE SA-EEND
3.1. SA-EEND
In SA-EEND [26], the speaker diarization task is formulated as a probabilistic multi-label classiﬁcation problem. Given the T length acoustic feature X = xt ∈ RD | t = 1, · · · , T ,

with a D-dimensional observation feature vector at time in-

dex t, SA-EEND predicts the corresponding speaker label sequence Yˆ = (yˆt | t = 1, · · · , T ). Here, speaker label yˆt =

[yˆt,s ∈ {0, 1} | s = 1, · · · , S] represents a joint activity for S

speakers at time t. For example, yˆt,s = yˆt,s = 1 (s = s ) means both s and s spoke at time t. Thus, determining Yˆ is

the key to determine the speaker diarization information as

follows:

Yˆ = SA (X) ∈ (0, 1)S×T ,

(1)

where SA(·) is a multi-head self-attention based neural network.
Note that the vanilla self-attention layers have to wait for all speech features in the entire recording to be processed in order to compute the output speaker labels. Thus, this method causes very high latency determined by the length of the recording, and cannot be adequate for online/real-time speech interface.

3.2. Chunk-wise SA-EEND for online inference

This paper ﬁrst investigates the use of SA-EEND as shown in Eq. (1) for chunked recordings with chunk size ∆, as follows:
Yˆti+1:ti+∆ = SA(Xti+1:ti+∆) ∈ (0, 1)S×∆ . (2)

Yˆi

Xi

i denotes a chunk index, and ti=1 0. Xi and Yˆi denote subsequences of X and Yˆ at chunk i, respectively. The latency can be suppressed by chunk size ∆ instead of the entire recording length T . We ﬁrst investigate the inﬂuence of chunk size ∆ in terms of the diarization performance.

3.2.1. Model conﬁguration and dataset
The SA-EEND system was trained using simulated training/test sets for two speakers following the procedure in [27]. Here, four encoder blocks with 256 attention units containing four heads without residual connections were trained. The input features were 23-dimensional log-Mel-ﬁlterbanks concatenated with the previous seven frames and subsequent seven frames with a 25-ms frame length and 10-ms frame shift. A subsampling factor of ten was applied afterwards. As a summary, a (23 × 15)-dimensional feature was inputted into the neural network every 100 ms.
Two datasets were used for this analysis. The ﬁrst one, a subset of CALLHOME [3], consists of actual two-speaker telephone conversations. Following the steps in [27], we split CALLHOME into two parts: 155 recordings for adaptation and 148 recordings for evaluation. The overall overlap ratio (including the test set) is of 13.0 %. The average duration is 72.1 s. The second dataset is the Corpus of Spontaneous Japanese (CSJ) [31] which consists of interviews, natural conversations, etc. We selected 54 recordings from this data with an overlap ratio of 20.1 %. There are consistently two speakers in each recording with an average duration is 767.0 s.

DER (%) DER (%)

36

Recording-wise

33

Chunk-wise

30

27

24

21

18

15

12

9 0 250 500 750 1000

Chunk size

(a) CALLHOME

38

36

Recording-wise

34

Chunk-wise

32

30

28

26

24

22

20

18 0 250 500 750 1000

Chunk size

(b) CSJ

Fig. 1: Recording-wise and oracle chunk-wise DER (%).

3.2.2. Analysis results
In this section, we analyzed the relationship between chunk size ∆ in Eq. (2) and the DER. The recordings to be analyzed were ﬁrst divided equally according to predeﬁned chunk size and then fed into a SA-EEND system. These chunk-wise diarization results were then combined with the original order as the ﬁnal diarization result of the whole recording. We call it as recording-wise DER which was calculated on the entire recording. When computing the DER in both overlapping and non-speech regions, a 0.25 s collar tolerance was used at the start and the end of each segment.
Note that this chunk-wise SA-EEND method does not guarantee that the permutation of speaker labels obtained across the chunk is the same due to the speaker permutation ambiguity underlying in the general speaker diarization problem. Thus, the recording-wise DER would be degraded due to this across-chunk speaker inconsistency. To measure this degradation, we also computed the oracle DER in each chunk separately (chunk-wise DER), which does not include the across-chunk speaker inconsistency error.
The analytical results are shown in Figure 1 for the CALLHOME and CSJ datasets. In these ﬁgures, the xaxis represents a chunk size ∆ during inference. Here, one chunk unit corresponds to 0.1 s, which means the latency of the system (without considering the excution time) is 1 s when the chunk size is 10 (i.e., 0.1 s × 10 = 1 s). The y-axis represents the ﬁnal DER of the whole dataset. As shown in Figure 1, the recording-wise DER decreased as the chunk size increased for both datasets. When the chunk size was larger than 800, the recording-wise DER tended to converge for CALLHOME. On the other hand, the oracle chunk-wise DER was much smaller and more stable than the recordingwise DER even when the chunk size was small, for both datasets. This indicates that the main degradation of online chunk-wise SA-EEND comes from the across-chunk speaker permutation inconsistency.

Algorithm 1: Online diarization using speakertracing buffer.

Input: {Xi}i S
Lmax SA(·) Output: Yˆ

// Chunked acoustic subsequences // #speakers
// Buffer size // SA-EEND system // Diarization results

1 Xbuf ← ∅, Y buf ← ∅ 2 for i = 1, . . . do

// Initialize buffer

3

Yˆ buf ; Yˆi ← SA Xbuf ; Xi

4 if Y buf = ∅ then

5

ψ ← arg maxφ∈perm(S) CC Y buf , Yˆφbuf

6

Yˆi ← Yˆi,ψ

7 Yˆ ← Yˆ ; Yˆi

8 Update Xbuf and Y buf according to selection

rules

// Sec. 4.2

4. SPEAKER-TRACING BUFFER
In this section, we propose a method called speaker-tracing buffer (STB), that utilizes previous information as a clue to solve the across-chunk permutation issue.
4.1. Speaker-tracing with buffer
Let Lmax be the size of STB, and Xbuf ∈ RD×L and Y buf ∈ (0, 1)S×L (0 ≤ L ≤ Lmax) be the L-length acoustic feature and the corresponding SA-EEND outputs stored in STB, respectively, which contain the speaker-tracing information. At the initial stage, Xbuf and Y buf are empty. Our online diarization is performed by referring and updating this STB, as shown in Algorithm 1. The input of the SA-EEND system is the concatenation of acoustic feature subsequence Xi ∈ RD×∆ at current chunk i and the acoustic features in buffer Xbuf , i.e., Xbuf ; Xi ∈ RD×(L+∆). The corresponding output of SA-EEND is Yˆ buf ; Yˆi ∈ (0, 1)S×(L+∆). If Y buf is not empty, the correlation coefﬁcient CC (·, ·) between Y buf and the current buffer output Yˆφbuf at speaker permutation output φ is calculated as

CC Y buf , Yˆφbuf =

S s=1

L l=1

ysb,ulf − ybuf

yˆφbusf,l − yˆφbuf

,

S

L

2
ybuf − ybuf

S

L

2
yˆbuf − yˆbuf

s=1 l=1 s,l

s=1 l=1 φs,l

φ

where ybuf =

S s=1

L l=1

ysb,ulf

,

SL

yˆφbuf =

Ss=1 Ll=1 yˆφbusf,l . (3) SL

1st chunk SA-EEND

Selection

Buffer

Output for 1st chunk

2nd chunk SA-EEND

Selection

Buffer

Solve permutation using and

Output for 2nd chunk

Fig. 2: Applying speaker-tracing buffer (STB) for SA-EEND.

Permutation ψ with the largest correlation coefﬁcient is chosen as follows:

ψ = arg max CC Y buf , Yˆφbuf ,

(4)

φ∈perm(S)

where perm(S) generates all permutations according to the number of speakers S. The corresponding buffer output Yˆib,ψuf is chosen as the ﬁnal output Yˆi of chunk i, which can maintain a consistent speaker permutation across the chunk. The obtained output Yˆi is stacked with the previously estimated output to form the whole recording’s output Yˆ in the end.
An example of applying the STB to SA-EEND in the ﬁrst two chunks is shown in Figure 2, where ∆ is equal to 10, the buffer size Lmax is 5, and the speaker number S is 2.
Speaker-tracing buffer Xbuf ; Y buf for the next chunk i+
1 is selected from Y buf ; Yˆi and Xbuf ; Xi in the current
chunk. We consider four selection strategies, as explained in
the next section.

4.2. Selection strategy for speaker-tracing buffer
If chunk size ∆ is not larger than the predeﬁned buffer size Lmax, we can simply store all the features in the buffer until the number of stored features reaches the buffer size. Once the number of accumulated features becomes larger than the buffer size Lmax, we have to select and store informative features that contain the speaker permutation information from Xbuf ; Xi and Y buf ; Yˆi . In this section, four selection rules for updating the buffer are listed. Here, we assume that the number of speakers S is 2.
• First-in-ﬁrst-out. The buffer is managed in a ﬁrst-inﬁrst-out manner to store the most recent Lmax features.
• Uniform sampling. Lmax acoustic features from Xbuf ; Xi and the corresponding diarization results from Y buf ; Yˆi are randomly extracted based on the uniform distribution.

• Deterministic selection using the absolute difference of probabilities of speakers, as

δm = |ym,1 − ym,2| ,

(5)

where y1,m, y2,m are the probabilities of the ﬁrst and second speakers at time index m. The maximum value of δm (= 1) is realized in either case of ym,1 = 1, ym,2 = 0 or ym,1 = 0, ym,2 = 1. This means that we try to ﬁnd dominant active-speaker frames. Top Lmax samples with the highest δm are selected from Xbuf ; Xi and Y buf ; Yˆi

• Weighted sampling: This is a combination of the uniform sampling and deterministic selection. We randomly select Lmax features but the probability of selecting m-th feature is proportional to δmin Eq. (5).

4.3. Efﬁcient training scheme for SA-EEND
The additional frames introduced by the speaker-tracing buffer mechanism cause the length of the input chunks to get larger overtime at the inference stage. However, the model trained in [26] used a ﬁxed chunk size. This will originate a mismatch between training and evaluation, which would degrade the performance of this system [32, 33].
Since self-attention modeling does not depend on the input length, we propose a variable chunk size training (VCT) scheme to mitigate the chunk size mismatch issue. First, we split each recording into chunks with size γ, randomly sampled from {50, 51, . . . , 500}. When we created a minibatch, it contained variable-length sequences due to the variable chunk size, and we used a padding technique to compensate for the different lengths. VCT scheme is applied to both training and adaptation stages. Due to the padding technique, the training efﬁciency was marginally degraded compared with ﬁxed chunk-size training.

Table 1: DERs (%) of online diarization using STB with four buffer selection strategies. We varied buffer size Lmax, but ﬁxed chunk size ∆ = 10. Note that online diarization without STB with ∆ = 10 showed 38.29 % and 44.57 % DERs on CALLHOME and CSJ, respectively.

System
First-in-ﬁrst-out Uniform sampling Deterministic selection Weighted sampling

(a) CALLHOME

10
48.71 45.03 37.56 42.23

50
29.49 22.38 23.23 20.10

Lmax

100 200

18.05 16.28 17.11 15.47

14.09 13.95 14.47 13.26

500
12.80 13.05 12.80 12.84

1000
12.66 12.65 12.66 12.66

System
First-in-ﬁrst-out Uniform sampling Deterministic selection Weighted sampling

(b) CSJ

10
51.32 39.24 45.70 49.87

50
44.08 31.06 29.89 30.11

Lmax

100 200

37.22 26.38 27.06 25.44

26.21 24.99 25.32 22.69

500
22.02 24.51 24.70 21.64

1000
20.45 20.59 24.13 21.62

5. EXPERIMENTAL RESULTS
5.1. Effect of selection strategy
We analyzed the effect of the speaker tracing buffer (STB) and the selection strategy in the section. For the effect of selection strategy, we used the same chunk size ∆ = 10 and several buffer sizes Lmax varied from 10 to 1000 in Table 1. The model used here is the ﬁxed-length training model with four encoder blocks and four heads.
Note that online diarization without STB with ∆ = 10 showed 38.29 % and 44.57 % DERs on CALLHOME and CSJ, respectively. Comparing online diarization without STB and with STB, applying the STB improved the performance of online SA-EEND regardless of which selection strategy was used. As for the strategies, weighted sampling performed best for both datasets in most cases when Lmax was large. Therefore, we considered weighted sampling as a selection strategy for future analysis.
5.2. Effect of buffer and chunk size
Next, we analyzed the effect of the buffer and the chunk size. The DER results for the CALLHOME and CSJ when applying the weighted sampling selection strategy are shown in Figure 3. Chunk sizes ∆ were 10 and 20 with the latency time of 1 s and 2 s respectively. Regarding the chunk size in Figure 3, all DERs from the large chunk size ∆ = 20 are better than those from the small chunk size ∆ = 10 even if the buffer size is the same. As for the buffer size, DER decreased as buffer size increased. These results were in line with our assumption that a large input size would lead to a better result.

DER (%) DER (%)

33

Chunk size: 10

30

Chunk size: 20

27

24

21

18

15

12

0 100 200 300 Buffer size Lmax

(a) CALLHOME

36

34 32

Chunk size: 10 Chunk size: 20

30

28

26

24

22

20 0 100 200 300 400 500

Buffer size Lmax

(b) CSJ

Fig. 3: Relationship among DER, chunk size and buffer size.

The model used here was the ﬁxed-length training model with four encoder blocks and four heads.
5.3. Real-time factor
Real-time factor (RTF) was calculated as the ratio between the summation of the execution time of every chunk to a recording duration It measures the speech decoding speed and expresses the time performance of the proposed system. To avoid the unequal size of buffers in the ﬁrst several chunks, we ﬁrst ﬁlled the buffer with dummy values and then calculated the RTF. Our experiment was conducted on an Intel® Xeon® CPU E52697A v2 @ 2.60GHz using one thread. RTFs are equal to 0.40, 1.07 when the chunk size ∆ = 10 and the buffer size Lmax = 500, 1000. This indicates that the proposed method is acceptable for online applications when buffer size is smaller than 1000 (100 s). So, the actual latency time for online SA-EEND is 1.4 s when ∆ = 10 and Lmax = 500.
5.4. Comparison with other methods
For a comparison with other methods, we evaluated our proposed methods using two real datasets (CALLHOME and CSJ), and three simulated datasets which are shown in Table 2. The simulated datasets were created by using two speaker segments. The background noise and room impulse response come from MUSAN corpus [34] and Simulated Room Response corpus [35] following the procedure in [26]. Three kinds of simulated datasets were created with overlap ratios equal to ρ = 34.4 %, 27.2 %, and 19.5 %, respectively.
Note that we included errors in overlapping speech segments and speech-activity-detection-related errors in contrast with most works, e.g., the Kaldi CALLHOME diarization recipe, that did not evaluate such errors.
For the ofﬂine i-vector and x-vector method, we used version 1 and 2 (v1 and v2) from Kaldi CALLHOME diarization recipe [19, 36, 37]. These are ofﬂine methods that employ probabilistic linear discriminant analysis [38] along with

Table 2: DERs (%) on simulated mixtures and real datasets. ρ denotes the overlap ratio of each simulated dataset. Note that all results include the overlapping regions without oracle speech activity detection.

System
Ofﬂine i-vector Ofﬂine x-vector Ofﬂine SA-EEND (γ = 500)
Online x-vector (∆ = 15) Online SA-EEND (∆ = 10) Online SA-EEND w/ STB (∆ = 10, Lmax = 500) Online SA-EEND w/ STB (∆ = 5, Lmax = 500) Online SA-EEND w/ STB and VCT (∆ = 10, Lmax = 500) Online SA-EEND w/ STB and VCT (∆ = 5, Lmax = 500)

Simulated

ρ = 34.4 % 27.2 %

33.73 28.77
4.56

30.93 24.46
4.50

36.94 33.18 7.91 7.87 7.41 7.79

34.94 37.31 7.31 7.48 6.98 7.53

19.5 %
25.96 17.78
3.85
33.19 41.41 6.91 7.15 6.27 6.88

Real
CALLHOME
12.10 11.53 9.54
26.90 38.29 12.84 13.08 12.54 12.66

CSJ
27.99 22.96 20.48
25.45 44.57 21.64 22.54 20.77 21.62

Table 3: DERs (%) on real datasets with 30 s of calibration period.

System
Ofﬂine SA-EEND Online SA-EEND w/ STB Online SA-EEND w/ STB and VCT

CALLHOME

Within 30 s After 30 s

9.38 15.91 14.89

9.53 10.05 10.58

All
9.53 12.84 12.54

Within 30 s
23.43 25.37 23.52

CSJ
After 30 s
20.23 21.30 20.49

All
20.48 21.64 20.77

agglomerative-cluster, a TDNN-based speech activity detection [39] and oracle number of speakers. Ofﬂine SA-EEND refers to the method that uses the entire recording as one chunk. The system in [27] which achieved the best performance is applied here, not only for the ofﬂine SA-EEND but also for the online SA-EEND w/ STB. For the online SAEEND, the chunk size is ∆ = 10 without applying STB. The proposed method applied the weighted sampling based STB as shown in Section 4.2 to the SA-EEND.
For the online x-vector, the speech segments were divided into subsequent 1.5 s chunks (∆ = 15). Then, the system decided whether the entire chunk was speech or silence based on the output of the energy VAD for real datasets and oracle VAD for simulated datasets. If the percentage of voiced frames of the entire chunk was fewer than 20 %, it was considered as silence, and the process was skipped. If it was a voiced chunk, we extracted an x-vector and assigned it to the ﬁrst cluster until a dissimilar x-vector arrives according to the probabilistic linear discriminant analysis (PLDA) score. Here, we applied a suitable threshold of 0.1 as the dissimilar criterion after scanning thresholds from 0.2 to -0.2 with a step of 0.1. Once two clusters exist we computed the PLDA score between the new segment and the two clusters. Finally, we assigned an x-vector to the nearest cluster.
As shown in Table 2, among these online systems including the system based on x-vector, online SA-EEND with STB and VCT achieved the best result. The proposed online SAEEND performed even better than the ofﬂine i-vector and xvector based methods on the CSJ dataset.
The online SA-EEND w/ STB and VCT increased the

DER by about 3 % when compared with the ofﬂine SA-EEND system for CALLHOME and Simulated dataset when buffer size is 500 and the chunk size is 10. It is also shown that the online SA-EEND with STB and VCT almost achieved the ofﬂine performance by 20.77 % DER as the DER of the ofﬂine SA-EEND is 20.48 %. As the average duration of the recordings in CSJ (767.0 s) is much longer than CALLHOME (74 s) and Simulated (87.6 s). It indicates that online SA-EEND with STB can achieve better results for long recordings.
In order to explore the increase of DER, we broke down the DER with a calibration period of 30 s as described in Table 3. We can observe that within the 30s, the model trained with VCT performs better than with a ﬁxed length. But after the 30s, the model trained with ﬁxed length can achieve better results. Both methods show comparable results with ofﬂine SA-EEND after the 30s which also explains the reason why STB is much more suitable for long recordings.
6. CONCLUSION
In this paper, we proposed a speaker-tracing buffer to memorize the permutation information of the previous chunk which enables the pre-trained ofﬂine SA-EEND system directly work online. In addition, the variable chunk size training scheme was proposed to handle the variable input length using speaker tracing buffer. The latency time can be reduced to 500 ms with comparable diarization performance. Future work will be focused on a variable number of speakers as the current method is limited to the two-speaker case. In addition, the combination of ﬁxed-length training and the variable

training scheme will be considered.

7. REFERENCES
[1] Xavier Anguera, Chuck Wooters, and Javier Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2011–2022, 2007.
[2] Wonjune Kang, Brandon C Roy, and Wesley Chow, “Multimodal speaker diarization of real-world meetings using d-vectors with spatial features,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6509–6513.
[3] “2000 NIST Speaker Recognition Evaluation,” https: //catalog.ldc.upenn.edu/LDC2001S97.
[4] Alvin Martin and Mark Przybocki, “The NIST 1999 speaker recognition evaluation—an overview,” Digital signal processing, vol. 10, no. 1-3, pp. 1–18, 2000.
[5] Jon Barker, Shinji Watanabe, Emmanuel Vincent, and Jan Trmal, “The ﬁfth “CHiME” speech separation and recognition challenge: Dataset, task and baselines,” in INTERSPEECH, 2018.
[6] Naoyuki Kanda, Rintaro Ikeshita, Shota Horiguchi, Yusuke Fujita, Kenji Nagamatsu, Xiaofei Wang, Vimal Manohar, Nelson Enrique Yalta Soplin, Matthew Maciejewski, Szu-Jui Chen, et al., “The Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays,” in CHiME-5, 2018.
[7] Shinji Watanabe, Michael Mandel, Jon Barker, and Emmanuel Vincent, “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in CHiME-6, 2020.
[8] Thilo von Neumann, Keisuke Kinoshita, Marc Delcroix, Shoko Araki, Tomohiro Nakatani, and Reinhold HaebUmbach, “All-neural online source separation, counting, and diarization for meeting analysis,” in ICASSP, 2019, pp. 91–95.
[9] Matthew Maciejewski, David Snyder, Vimal Manohar, Najim Dehak, and Sanjeev Khudanpur, “Characterizing performance of speaker diarization systems on far-ﬁeld speech using standard methods,” in ICASSP, 2018, pp. 5244–5248.
[10] Ju¨rgen Geiger, Frank Wallhoff, and Gerhard Rigoll, “GMM-UBM based open-set online speaker diarization,” in INTERSPEECH, 2010.
[11] Stephen H Shum, Najim Dehak, Re´da Dehak, and James R Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” IEEE

Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2015–2028, 2013.
[12] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and Chong Wang, “Fully supervised speaker diarization,” in ICASSP, 2019, pp. 6301–6305.
[13] Konstantin Markov and Satoshi Nakamura, “Improved novelty detection for online gmm based speaker diarization,” in INTERSPEECH, 2008.
[14] Srikanth Madikeri, Ivan Himawan, Petr Motlicek, and Marc Ferras, “Integrating online i-vector extractor with information bottleneck based speaker diarization system,” in INTERSPEECH, 2015, pp. 3105–3109.
[15] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan McCree, “Speaker diarization using deep neural network embeddings,” in ICASSP, 2017, pp. 4930–4934.
[16] Weizhong Zhu and Jason Pelecanos, “Online speaker diarization using adapted i-vector transforms,” in ICASSP, 2016, pp. 5045–5049.
[17] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansﬁeld, and Ignacio Lopz Moreno, “Speaker diarization with LSTM,” in ICASSP, 2018, pp. 5239–5243.
[18] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, “Generalized end-to-end loss for speaker veriﬁcation,” in ICASSP, 2018, pp. 4879–4883.
[19] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in ICASSP, 2018, pp. 5329–5333.
[20] Gregory Sell and Daniel Garcia-Romero, “Speaker diarization with plda i-vector scoring and unsupervised calibration,” in SLT, 2014, pp. 413–417.
[21] Huazhong Ning, Ming Liu, Hao Tang, and Thomas S Huang, “A spectral clustering approach to speaker diarization,” in Ninth International Conference on Spoken Language Processing, 2006.
[22] Dimitrios Dimitriadis and Petr Fousek, “Developing online speaker diarization system.,” in INTERSPEECH, 2017, pp. 2739–2743.
[23] Jose Patino, Ruiqing Yin, He´ctor Delgado, Herve´ Bredin, Alain Komaty, Guillaume Wisniewski, Claude Barras, Nicholas WD Evans, and Se´bastien Marcel, “Low-latency speaker spotting with online diarization and detection.,” in Odyssey, 2018, pp. 140–146.
[24] Enrico Fini and Alessio Brutti, “Supervised online diarization with sample mean loss for multi-domain data,” in ICASSP, 2020, pp. 7134–7138.

[25] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in INTERSPEECH, 2019, pp. 4300–4304.
[26] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Yawen Xue, Kenji Nagamatsu, and Shinji Watanabe, “End-toend neural speaker diarization with self-attention,” in ASRU, 2019, pp. 296–303.
[27] Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Yawen Xue, and Kenji Nagamatsu, “End-to-end neural diarization: Reformulating speaker diarization as simple multi-label classiﬁcation,” arXiv preprint arXiv:2003.02966, 2020.
[28] Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Yawen Xue, Jing Shi, and Kenji Nagamatsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv preprint arXiv:2006.01796, 2020.
[29] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Kenji Nagamatsu, “End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” in INTERSPEECH, 2020, pp. 269–273.
[30] Qingjian Lin, Tingle Li, Lin Yang, Junjie Wang, and Ming Li, “Optimal mapping loss: A faster loss for endto-end speaker diarization,” in Proc. Odyssey 2020 The Speaker and Language Recognition Workshop, 2020, pp. 125–131.
[31] Kikuo Maekawa, “Corpus of Spontaneous Japanese: Its design and evaluation,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[32] Jee-weon Jung, Hee-Soo Heo, Hye-jin Shim, and HaJin Yu, “Short utterance compensation in speaker veriﬁcation via cosine-based teacher-student learning of speaker embeddings,” in ASRU, 2019, pp. 335–341.
[33] Ahilan Kanagasundaram, Sridha Sridharan, Sriram Ganapathy, Prachi Singh, and Clinton B Fookes, “A study of x-vector based speaker recognition on short utterances,” in INTERSPEECH, 2019, pp. 2943–2947.
[34] David Snyder, Guoguo Chen, and Daniel Povey, “MUSAN: A music, speech, and noise corpus,” arXiv preprint arXiv:1510.08484, 2015.
[35] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in ICASSP, 2017, pp. 5220–5224.

[36] David Snyder, Daniel Garcia-Romero, Daniel Povey, and Sanjeev Khudanpur, “Deep neural network embeddings for text-independent speaker veriﬁcation.,” in INTERSPEECH, 2017, pp. 999–1003.
[37] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in ASRU, 2011.
[38] Sergey Ioffe, “Probabilistic linear discriminant analysis,” in ECCV, 2006, pp. 531–542.
[39] Vijayaditya Peddinti, Guoguo Chen, Vimal Manohar, Tom Ko, Daniel Povey, and Sanjeev Khudanpur, “JHU ASPIRE system: Robust LVCSR with TDNNs, iVector adaptation and RNN-LMS,” in ASRU, 2015, pp. 539– 546.

