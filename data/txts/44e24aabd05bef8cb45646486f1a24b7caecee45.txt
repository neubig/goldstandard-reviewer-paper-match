MULTILINGUAL SEQUENCE-TO-SEQUENCE SPEECH RECOGNITION: ARCHITECTURE, TRANSFER LEARNING, AND LANGUAGE MODELING

Jaejin Cho1,‡ , Murali Karthick Baskar2,‡, Ruizhi Li1,‡, Matthew Wiesner1, Sri Harish Mallidi3, Nelson Yalta4, Martin Karaﬁa´t2, Shinji Watanabe1, Takaaki Hori5
1Johns Hopkins University, 2Brno University of Technology, 3Amazon, 4Waseda University, 5Mitsubishi Electric Research Laboratories (MERL)
{ruizhili,jcho52,shinjiw}@jhu.edu,{baskar,karafiat}@fit.vutbr.cz,thori@merl.com

arXiv:1810.03459v1 [cs.CL] 4 Oct 2018

ABSTRACT
Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach beneﬁts by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings signiﬁcant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data. Index Terms: Automatic speech recognition (ASR), sequence to sequence, multilingual setup, transfer learning, language modeling
1. INTRODUCTION
The sequence-to-sequence (seq2seq) model proposed in [1, 2, 3] is a neural architecture for performing sequence classiﬁcation and later adopted to perform speech recognition in [4, 5, 6]. The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classiﬁcation (CTC) [6, 5] and attention [4, 7] based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models requires a huge amount of data [8]. Intuitively, this is due to the wide-range role of the model in performing alignment and language modeling along with acoustic to character label mapping at each iteration.
In this paper, we explore the multilingual training approaches [9, 10, 11] used in hybrid DNN/RNN-HMMs to incorporate them into the seq2seq models. In a context of applications of multilingual approaches towards seq2seq model, CTC is mainly used instead of the attention models. A multilingual CTC is proposed in [12], which uses a universal phoneset, FST decoder and language model. The
‡ All three authors share equal contribution

authors also use linear hidden unit contribution (LHUC) [13] technique to rescale the hidden unit outputs for each language as a way to adapt to a particular language. Another work [14] on multilingual CTC shows the importance of language adaptive vectors as auxiliary input to the encoder in multilingual CTC model. The decoder used here is a simple argmax decoder. An extensive analysis on multilingual CTC mainly focusing on improving under limited data condition is performed in [15]. Here, the authors use a word level FST decoder integrated with CTC during decoding.
On a similar front, attention models are explored within a multilingual setup in [16, 17] based on attention-based seq2seq to build a model from multiple languages. The data is just combined together assuming the target languages are seen during the training. And, hence no special transfer learning techniques were used here to address the unseen languages during training. The main motivation and contribution behind this work is as follows:
• To incorporate the existing multilingual approaches in a joint CTC-attention [18] (seq2seq) framework, which uses a simple beam-search decoder as described in sections 2 and 4
• Investigate the effectiveness of transferring a multilingual model to a target language under various data sizes. This is explained in section 4.3.
• Tackle the low-resource data condition with both transfer learning and including a character-based RNNLM trained with multiple languages. Section 4.4 explains this in detail.
2. SEQUENCE-TO-SEQUENCE MODEL
In this work, we use the attention based approach [2] as it provides an effective methodology to perform sequence-to-sequence (seq2seq) training. Considering the limitations of attention in performing monotonic alignment [19, 20], we choose to use CTC loss function to aid the attention mechanism in both training and decoding. The basic network architecture is shown in Fig. 1.
Let X = (xt|t = 1, . . . , T ) be a T -length speech feature sequence and C = (cl|l = 1, . . . , L) be a L-length grapheme sequence. A multi-objective learning framework Lmol proposed in [18] is used in this work to unify attention loss patt(C|X) and CTC loss pctc(C|X) with a linear interpolation weight λ, as follows:
Lmod = λ log pctc(C|X) + (1 − λ) log p∗att(C|X) (1)
The uniﬁed model allows to obtain both monotonicity and effective sequence level training.

…… cl-1

cl ……

Joint Decoder

CTC

Attention Decoder RNN-LM

Shared Encoder

BLSTM Deep CNN (VGG net)

x1 …… xt

…… xT

Fig. 1: Hybrid attention/CTC network with LM extension: the shared encoder is trained by both CTC and attention model objectives simultaneously. The joint decoder predicts an output label sequence by the CTC, attention decoder and RNN-LM.

patt (C|X) represents the posterior probability of character label sequence C w.r.t input sequence X based on the attention approach, which is decomposed with the probabilistic chain rule, as follows:

L

p∗att (C|X) ≈ p (cl|c∗1, ...., c∗l−1, X) ,

(2)

l=1

where c∗l denotes the ground truth history. Detailed explanations about the attention mechanism is described later.
Similarly, pctc (C|X) represents the posterior probability based on the CTC approach.

pctc (C|X) ≈

p(Z |X ),

(3)

Z ∈Z (C )

where Z = (zt|t = 1, . . . , T ) is a CTC state sequence composed of the original grapheme set and the additional blank symbol. Z(C) is a set of all possible sequences given the character sequence C.
The following paragraphs explain the encoder, attention decoder, CTC, and joint decoding used in our approach.

Encoder

In our approach, both CTC and attention use the same encoder func-

tion, as follows:

ht = Encoder(X),

(4)

where ht is an encoder output state at t. As an encoder function Encoder(·), we use bidirectional LSTM (BLSTM) or deep CNN followed by BLSTMs. Convolutional neural networks (CNN) has achieved great success in image recognition [21]. Previous studies applying CNN in seq2seq speech recognition [22] also showed that incorporating a deep CNNs in the encoder could further boost the performance.
In this work, we investigate the effect of convolutional layers in joint CTC-attention framework for multilingual setting. We use the initial 6 layers of VGG net architecture [21] in table 2. For each speech feature image, one feature map is formed initially. VGG net then extracts 128 feature maps, where each feature map is downsampled to (1/4 × 1/4) images along time-frequency axis via the two maxpooling layers with stride = 2.

Attention Decoder:
Location aware attention mechanism [23] is used in this work. Equation (5) denotes the output of location aware attention, where alt acts as an attention weight.

alt = LocationAttention {al−1}Tt=1 , ql−1, ht .

(5)

Here, ql−1 denotes the decoder hidden state, ht is the encoder output state as shown in equation (4). The location attention function represents a convolution function * as in equation (6). It maps the attention weight of the previous label al−1 to a multi channel view ft for better representation.

ft = K ∗ al−1,

(6)

elt = gT tanh(Lin(ql−1) + Lin(ht) + LinB(ft)), (7)

alt = Softmax({elt}Tt=1)

(8)

Equation (7) provides the unnormalized attention vectors computed

with the learnable vector g, linear transformation Lin(·), and afﬁne

transformation LinB(·). Equation (8) computes a normalized atten-

tion weight based on the softmax operation Softmax(·). Finally, the

context vector rl is obtained by the weighted summation of the encoder output state ht over entire frames with the attention weight as

follows:

T

rl = altht.

(9)

t=1

The decoder function is an LSTM layer which decodes the next character output label cl from their previous label cl−1, hidden state of the decoder ql−1 and attention output rl, as follows:

p (cl|c1, ...., cl−1, X) = Decoder(rl, ql−1, cl−1) (10) This equation is incrementally applied to form p∗att in equation (2).

Connectionist temporal classiﬁcation (CTC):
Unlike the attention approach, CTC do not use any speciﬁc decoder. Instead it invokes two important components to perform character level training and decoding. First component, is an RNN based encoding module p(Z|X). The second component contains a language model and state transition module. The CTC formalism is a special case [6, 24] of hybrid DNN-HMM framework with an inclusion of Bayes rule to obtain p(C|X).

Joint decoding:
Once we have both CTC and attention-based seq2seq models trained, both are jointly used for decoding as below:

log phyp(cl|c1, ...., cl−1, X) =

α log pctc(cl|c1, ...., cl−1, X)

(11)

+(1 − α) log patt(cl|c1, ...., cl−1, X)

Here log phyp is a ﬁnal score used during beam search. α controls the weight between attention and CTC models. α and multi-task learning weight λ in equation (1) are set differently in our experiments.

Table 1: Details of the BABEL data used for performing the multilingual experiments

Usage Train Target

Language
Cantonese Bengali Pashto Turkish
Vietnamese Haitian Tamil Kurdish Tokpisin Georgian
Assamese Tagalog Swahili
Lao

Train # spkrs. # hours

952

126.73

720

55.18

959

70.26

963

68.98

954

78.62

724

60.11

724

62.11

502

37.69

482

35.32

490

45.35

720

54.35

966

44.0

491

40.0

733

58.79

Eval # spkrs. # hours

120

17.71

121

9.79

121

9.95

121

9.76

120

10.9

120

10.63

121

11.61

120

10.21

120

9.88

120

12.30

120

9.58

120

10.60

120

10.58

119

10.50

# of characters
3302 66 49 66 131 60 49 64 55 35 66 56 56 54

3. DATA DETAILS AND EXPERIMENTAL SETUP
In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far ﬁeld recordings are presented as well. Table 1 presents the details of the languages used in this work for training and evaluation.
80 dimensional Mel-ﬁlterbank (fbank) features are then extracted from the speech samples using a sliding window of size 25 ms with 10ms stride. KALDI toolkit [25] is used to perform the feature processing. The fbank features are then fed to a seq2seq model with the following conﬁguration:
The Bi-RNN [26] models mentioned above uses a LSTM [27] cell followed by a projection layer (BLSTMP). In our experiments below, we use only a character-level seq2seq model trained by CTC and attention decoder. Thus in the following experiments we intend to use character error rate (% CER) as a suitable measure to analyze the model performance. However, in section 4.4 we integrate a character-level RNNLM [28] with seq2seq model externally and showcase the performance in terms of word error rate (% WER). In this case the words are obtained by concatenating the characters and the space together for scoring with reference words. All experiments are implemented in ESPnet, end-to-end speech processing toolkit [29].
4. MULTILINGUAL EXPERIMENTS
Multilingual approaches used in hybrid RNN/DNN-HMM systems [11] have been used for for tackling the problem of lowresource data condition. Some of these approaches include language adaptive training and shared layer retraining [30]. Among them, the most beneﬁted method is the parameter sharing technique [11]. To incorporate the former approach into encoder, CTC and attention decoder model, we performed the following experiments:
• Stage 0 - Naive training combining all languages
• Stage 1 - Retraining the decoder (both CTC and attention) after initializing with the multilingual model from stage-0

Table 2: Experiment details

Model Conﬁguration
Encoder # encoder layers # encoder units # projection units
Decoder # decoder layers # decoder units # projection units
Attention # feature maps # window size
Training Conﬁguration
MOL Optimizer Initial learning rate AdaDelta AdaDelta decay Batch size Optimizer
Decoding Conﬁguration
Beam size ctc-weight

Bi-RNN 5
320 320 Bi-RNN
1 300 300 Location-aware 10 100
5e−1 AdaDelta
1.0 1e−8 1e−2
30 AdaDelta
20 3e−1

(a) Convolutional layers in joint CTC-attention

CNN Model Conﬁguration -2 components

Component 1 Convolution 2D Convolution 2D
Maxpool 2D Component 2 Convolution 2D Convolution 2D Maxpool 2D

2 convolution layers in = 1, out = 64, ﬁlter = 3× 3 in = 64, out = 64, ﬁlter = 3× 3 patch = 2×2, stride = 2×2
2 convolution layers in = 64, out = 128, ﬁlter = 3× 3 in = 128, out = 128, ﬁlter = 3× 3
patch = 2×2, stride = 2×2

• Stage 2 - The resulting model obtained from stage-1 is further retrained across both encoder and decoder
Table 4: Comparison of naive approach and training only the last layer performed using the Assamese language

Model type
Monolingual Multi. (after 4th epoch) Multi. (after 4th epoch) Multi. (after 15th epoch)

Retraining
Stage 1 Stage 2 Stage 2

% CER
45.6 61.3 44.0 41.3

% Absolute gain
-15.7 1.6 4.3

4.1. Stage 0 - Naive approach
In this approach, the model is ﬁrst trained with 10 multiple languages as denoted in table 1 approximating to 600 hours of training data. data from all languages available during training is used to build a single seq2seq model. The model is trained with a character label set composed of characters from all languages including both train and target set as mentioned in table 1. The model provides better generalization across languages. Languages with limited data when

Table 3: Recognition performance of naive multilingual approach for eval set of 10 BABEL training languages trained with the train set of same languages

%CER on Eval set for
Monolingual - BLSTMP Multilingual - BLSTMP
+ VGG

Bengali 43.4 42.9 39.6

Cantonese 37.4 36.3 34.3

Georgian 35.4 38.9 36.0

Haitian 39.7 38.5 34.5

Target languages

Kurmanji Pashto

55.0

37.3

52.1

39.0

49.9

34.7

Tamil 55.3 48.5 45.5

Turkish 50.3 36.4 28.7

Tokpisin 32.7 31.7 33.7

Vietnamese 54.3 41.0 37.4

trained with other languages allows them to be robust and helps in improving the recognition performance. In spite of being simple, the model has limitations in keeping the target language data unseen during training.
Comparison of VGG-BLSTM and BLSTMP
Table 3 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model. Even though the VGG-BLSTM gave improvements, we were not able to perform stage-1 and stage-2 retraining with it due to time constraints. Thus, we proceed further with multilingual BLSTMP model for retraining experiments tabulated below.
4.2. Stage 1 - Retraining decoder only
To alleviate the limitation in the previous approach, the ﬁnal layer of the seq2seq model which is mainly responsible for classiﬁcation is retrained to the target language.

found using SGD optimizer with initial learning rate of 1e−4 works better for retraining compared to AdaDelta.
The learning rate is decayed in this training at a factor of 1e−1 if there is a drop in validation accuracy. Table 4 shows the performance of simply retraining the last layer using a single target language Assamese.

4.3. Stage 2 - Finetuning both encoder and decoder
Based on the observations from stage-1 model in section 4.2, we found that simply retraining the decoder towards a target language resulted in degrading %CER the performance from 45.6 to 61.3. This is mainly due to the difference in distribution across encoder and decoder. So, to alleviate this difference the encoder and decoder is once again retrained or ﬁne-tuned using the model from stage-1. The optimizer used here is SGD as in stage-1, but the initial learning rate is kept to 1e−2 and decayed based on validation performance. The resulting model gave an absolute gain of 1.6% when ﬁnetuned a multilingual model after 4th epoch. Also, ﬁnetuning a model after 15th epoch gave an absolute gain of 4.3%.

Table 5: Stage-2 retraining across all languages with full set of target language data

% CER on eval set

Target Languages Assamese Tagalog Swahili Lao

Monolingual

45.6

Stage-2 retraining

41.3

43.1

33.1 42.1

37.9

29.1 38.7

Fig. 2: Difference in performance for 5 hours, 10 hours, 20 hours and full set of target language data used to retrain a multilingual model from stage-1
In previous works [11, 30] related to hybrid DNN/RNN models and CTC based models [12, 15] the softmax layer is only adapted. However in our case, the attention decoder and CTC decoder both have to be retrained to the target language. This means the CTC and attention layers are only updated for gradients during this stage. We

To further investigate the performance of this approach across different target data sizes, we split the train set into ∼5 hours, ∼10 hours, ∼20 hours and ∼full set. Since, in this approach the model is only ﬁnetuned by initializing from stage-1 model, the model architecture is ﬁxed for all data sizes. Figure 2 shows the effectiveness of ﬁnetuning both encoder and decoder. The gains from 5 to 10 hours was more compared to 20 hours to full set.
Table 5 tabulates the % CER obtained by retraining the stage-1 model with ∼full set of target language data. An absolute gain is observed using stage-2 retraining across all languages compared to monolingual model.
4.4. Multilingual RNNLM
In an ASR system, a language model (LM) takes an important role by incorporating external knowledge into the system. Conventional ASR systems combine an LM with an acoustic model by FST giving a huge performance gain. This trend is also shown in general including hybrid ASR systems and neural network-based sequenceto-sequence ASR systems.
The following experiments show a beneﬁt of using a language model in decoding with the previous stage-2 transferred models. Al-

though the performance gains in %CER are also generally observed over all target languages, the improvement in %WER was more distinctive. The results shown in the following Fig. 3 are in %WER. “whole” in each ﬁgure means we used all the available data for the target language as full set explained before.

WER (%)

Assamese

90

83.2

79.7

80

76.4

73.1

75.8

71.9

70

69.9 65.3

60

50 5

10

20

whole

target language data (hrs)

stage 2 retraining RNNLM added

WER (%)

Tagalog

90

84.1

80

76.5

78.8

75.5

71.4

70

71.1 67.7 64.3

60

50 5

10

20

whole

target language data (hrs)

stage 2 retraining RNNLM added

WER (%)

Swahili

90

82.5

7800 72.1 7655..71 69.9 66.2

60

60.1

56.2

50 5

10

20

whole

target language data (hrs)

stage 2 retraining RNNLM added

WER (%)

Lao

90

80

74.7

73.7

70

70.8

70.5

65.1

62.4

60 61.3 57.9

50

5

10

20

whole

target language data (hrs)

stage 2 retraining RNNLM added

Fig. 3: Recognition performance after integrating RNNLM during decoding in %WER for different amounts of target data
We used a character-level RNNLM, which was trained with 2layer LSTM on character sequences. We use all available paired text in the corresponding target language to train the LM for the language. No external text data were used. All language models are trained separately from the seq2seq models. When building dictionary, we combined all the characters over all 15 languages mentioned in table 1 to make them work with transferred models. Regardless of the amount of data used for transfer learning, the

RNNLM provides consistent gains across all languages over different data sizes.
Table 6: Recognition performance in %WER using stage-2 retraining and multilingual RNNLM

Model type
Stage-2 retraining + Multi. RNNLM

%WER on target languages

Assamese Tagalog Swahili Lao

71.9

71.4

66.2 62.4

65.3

64.3

56.2 57.9

As explained already, language models were trained separately and used to decode jointly with seq2seq models. The intuition behind it is to use the separately trained language model as a complementary component that works with a implicit language model within a seq2seq decoder. The way of RNNLM assisting decoding follows the equation below:

log p(cl|c1:l−1, X) = log phyp(cl|c1:l−1, X) (12)
+ β log plm(cl|c1:l−1, X)
β is a scaling factor that combines the scores from a joint decoding eq.(11) with RNN-LM, denoted as plm. This approach is called shallow fusion.
Our experiments for target languages show that the gains from adding RNNLM are consistent regardless of the amount of data used for transfer learning. In other words, in Figure 3, the gap between two lines are almost consistent over all languages.
Also, we observe the gain we get by adding RNN-LM in decoding is large. For example, in the case of assamese, the gain by RNN-LM in decoding with a model retrained on 5 hours of the target language data is almost comparable with the model stage-2 retrained with 20 hours of target language data. On average, absolute gain ∼6% is obtained across all target languages as noted in table 6.
5. CONCLUSION
In this work, we have shown the importance of transfer learning approach such as stage-2 multilingual retraining in a seq2seq model setting. Also, careful selection of train and target languages from BABEL provide a wide variety in recognition performance (%CER) and helps in understanding the efﬁcacy of seq2seq model. The experiments using character-based RNNLM showed the importance of language model in boosting recognition performance (%WER) over all different hours of target data available for transfer learning.
Table 5 and 6 summarizes, the effect of these techniques in terms of %CER and %WER. These methods also show their ﬂexibility in incorporating it in attention and CTC based seq2seq model without compromising loss in performance.
6. FUTURE WORK
We could use better architectures such as VGG-BLSTM as a multilingual prior model before transferring them to a new target language by performing stage-2 retraining. The naive multilingual approach can be improved by including language vectors as input or target during training to reduce the confusions. Also, investigation of multilingual bottleneck features [31] for seq2seq model can provide better performance. Apart from using the character level language model as in this work, a word level RNNLM can be connected during decoding to further improve %WER. The attention based decoder can

be aided with the help of RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.
7. REFERENCES
[1] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems, 2014, pp. 3104–3112.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.
[3] Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning phrase representations using RNN encoder-decoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.
[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in neural information processing systems, 2015, pp. 577–585.
[5] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks.,” in ICML, 2014, vol. 14, pp. 1764–1772.
[6] Alex Graves, “Supervised sequence labelling,” in Supervised sequence labelling with recurrent neural networks, pp. 5–13. Springer, 2012.
[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964.
[8] Andrew Rosenberg, Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran, and Michael Picheny, “End-to-end speech recognition and keyword search on low-resource languages,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 5280–5284.
[9] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz, “Automatic speech recognition for under-resourced languages: A survey,” Speech Communication, vol. 56, pp. 85–100, 2014.
[10] Zoltan Tuske, David Nolden, Ralf Schluter, and Hermann Ney, “Multilingual mrasta features for low-resource keyword search and speech recognition systems,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 7854–7858.
[11] Martin Karaﬁa´t, Murali Karthick Baskar, Pavel Mateˇjka, Karel Vesely`, Frantisˇek Gre´zl, and Jan Cˇ ernocky, “Multilingual blstm and speaker-speciﬁc vector adaptation in 2016 but babel system,” in Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 637–643.
[12] Sibo Tong, Philip N Garner, and Herve´ Bourlard, “Multilingual training and cross-lingual adaptation on CTC-based acoustic model,” arXiv preprint arXiv:1711.10025, 2017.
[13] Pawel Swietojanski and Steve Renals, “Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models,” in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 171–176.

[14] Markus Mu¨ller, Sebastian Stu¨ker, and Alex Waibel, “Language adaptive multilingual CTC speech recognition,” in International Conference on Speech and Computer. Springer, 2017, pp. 473–482.
[15] Siddharth Dalmia, Ramon Sanabria, Florian Metze, and Alan W Black, “Sequence-based multi-lingual low resource speech recognition,” arXiv preprint arXiv:1802.07420, 2018.
[16] Shinji Watanabe, Takaaki Hori, and John R Hershey, “Language independent end-to-end architecture for joint language identiﬁcation and speech recognition,” in Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE. IEEE, 2017, pp. 265–271.
[17] Shubham Toshniwal, Tara N Sainath, Ron J Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, and Kanishka Rao, “Multilingual speech recognition with a single end-to-end model,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.
[18] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[19] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian StA˜ Œker, and Alex Waibel, “Self-attentional acoustic models,” in 19th Annual Conference of the International Speech Communication Association (InterSpeech 2018), 2018.
[20] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwise attention,” CoRR, vol. abs/1712.05382, 2017.
[21] Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for large-scale image recognition,” 09 2014.
[22] Ying Zhang, Mohammad Pezeshki, Phile´mon Brakel, Saizheng Zhang, Ce´sar Laurent, Y Bengio, and Aaron Courville, “Towards end-to-end speech recognition with deep convolutional neural networks,” September 2016.
[23] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems. 2015, vol. 2015-January, pp. 577– 585, Neural information processing systems foundation.
[24] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan, “Advances in joint CTC-attention based end-to-end speech recognition with a deep cnn encoder and RNN-LM,” arXiv preprint arXiv:1706.02737, 2017.
[25] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The kaldi speech recognition toolkit,” in Automatic Speech Recognition and Understanding, 2011 IEEE Workshop on. IEEE, 2011, pp. 1–4.
[26] Mike Schuster and Kuldip K Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, 1997.
[27] Sepp Hochreiter and Ju¨rgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[28] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and Jan Cernocky, “RNNLM-recurrent neural network language modeling toolkit,” in Proc. of the 2011 ASRU Workshop, 2011, pp. 196–201.

[29] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., “Espnet: End-to-end speech processing toolkit,” arXiv preprint arXiv:1804.00015, 2018.
[30] Sibo Tong, Philip N Garner, and Herve´ Bourlard, “An investigation of deep neural networks for multilingual speech recognition training and adaptation,” Tech. Rep., 2017.
[31] Frantisek Gre´zl, Martin Karaﬁa´t, and Karel Vesely`, “Adaptation of multilingual stacked bottle-neck neural network structure for new language,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.

