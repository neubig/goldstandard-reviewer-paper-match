arXiv:1702.08489v1 [cs.LG] 27 Feb 2017

Depth Separation for Neural Networks
Amit Daniely∗
March 1, 2017
Abstract
Let f : Sd−1 × Sd−1 → R be a function of the form f (x, x ) = g( x, x ) for g : [−1, 1] → R. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate f whenever g cannot be approximated by a low degree polynomial. Moreover, for many g’s, such as g(x) = sin(πd3x), the number of neurons must be 2Ω(d log(d)). Furthermore, the result holds w.r.t. the uniform distribution on Sd−1 × Sd−1. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t. the uniform distribution on Sd−1 × Sd−1.
1 Introduction and main result
Many aspects of the expressive power of neural networks has been studied over the years. In particular, separation for deep networks [11, 10], expressive power of depth two networks [4, 8, 7, 2], and more [5, 3]. We focus on the basic setting of depth 2 versus depth 3 networks. We ask what functions are expressible (or well approximated) by poly-sized depth-3 networks, but cannot be approximated by an exponential size depth-2 network.
Two recent papers [9, 6] addressed this issue. Both papers presented a speciﬁc function f : Rd → R and a distribution D on Rd such that f can be approximated w.r.t. D by a poly(d)-size depth 3 network, but not by a poly(d)-size depth 2 network. In Martens et al. [9] this was shown for f being the inner product mod 2 and D being the uniform distribution on {0, 1}d ×{0, 1}d. In Eldan and Shamir [6] it was shown for a diﬀerent (radial) function and some (unbounded) distribution.
We extend the above results and prove a similar result for an explicit and rich family of functions, and w.r.t. the uniform distribution on Sd−1 × Sd−1. In addition, our lower bound on the number of required neurons is stronger: while previous papers showed that the number of neurons has to be exponential in d, we show exponential dependency on d log(d). Last, our proof is short, direct and is based only on basic Harmonic analysis over the sphere. In contrast, Eldan and Shamir [6]’s proof is rather lengthy and requires advanced technical tools such as tempered distributions, while Martens et al. [9] relied on the discrepancy of the inner product function mod 2. On the other hand, Eldan and Shamir [6] do not put any restriction on the magnitude of the weights, while we and Martens et al. [9] do require a mild (exponential) bound.
∗Google Brain
1

Let us ﬁx an activation function σ : R → R. For x ∈ Rn we denote σ(x) = (σ(x1), . . . , σ(xn)). We say that F : Sd−1 × Sd−1 → R can be implemented by a depth-2 σ-network of width r and
weights bounded by B if

F (x, x ) = w2T σ(W1x + W1x + b1) + b2 ,

where W1, W1 ∈ [−B, B]r×d, w2 ∈ [−B, B]r, b1 ∈ [−B, B]r and b2 ∈ [−B, B]. Similarly, F : Sd−1 × Sd−1 → R can be implemented by a depth-3 σ-network of width r and weights bounded by
B if F (x, x ) = w3T σ(W2σ(W1x + W1x + b1) + b2) + b3

for W1, W1 ∈ [−B, B]r×d, W2 ∈ [−B, B]r×r, w3 ∈ [−B, B]r, b1, b2 ∈ [−B, B]r and b3 ∈ [−B, B].

Denote

d+n−1

d + n − 3 (2n + d − 2)(n + d − 3)!

Nd,n = d − 1 − d − 1 = n!(d − 2)! .

Let µd be the probability measure on [−1, 1] given by dµd(x) = √ Γ( d2 ) (1 − x2) d−2 3 dx and deﬁne

π

Γ

(

d−1 2

)

An,d(f ) =

min

f − p L2(µd)

p is degree n−1 polynomial

Our main theorem shows that if An,d(f ) is large then (x, x ) → f ( x, x ) cannot be approximated by a small depth-2 network.
Theorem 1 (main). Let N : Sd−1 × Sd−1 → R be any function implemented by a depth-2 σ-network of width r, with weights bounded by B. Let f : [−1, 1] → R and deﬁne F : Sd−1 × Sd−1 → R by F (x, x ) = f ( x, x ). Then, for all n,

N − F L2(Sd−1×Sd−1) ≥ An,d(f )

2rB max|x|≤√4dB+B |σ(x)| + 2B An,d(f ) −
Nd,n

Example 2. Let us consider the case that σ(x) = max(0, x) is the ReLU function, f (x) =

sin(πd3x), n = d2 and B = 2d. In this case, lemma 5 implies that An,d(f ) ≥ 5e1π . Hence, to

have

1 50e2

π

2

-approximation

of

F,

the

number

of

hidden

neuorons

has

to

be

at

least,

Nd,d2 √

= 2Ω(d log(d))

20eπ22d(1 + 4d) + 2d+1

On the other hand, corollary 7 implies that F can be -approximated by a ReLU network of depth 3, width 16πd5 and weights bounded by 2πd3

2 Proofs
Throughout, we ﬁx a dimension d. All functions f : Sd−1 → R and f : Sd−1 × Sd−1 → R will be assumed to be square integrable w.r.t. the uniform measure. Likewise, functions f : [−1, 1] → R and f : [−1, 1] × [−1, 1] → R will be assumed to be square integrable w.r.t. µd or µd × µd. Norms and inner products of such functions are of the corresponding L2 spaces. We will use the fact that µd is the probability measure on [−1, 1] that is obtained by pushing forward the uniform measure on Sd−1 via the function x → x1. We denote by Pn : L2(µd) → L2(µd) the projection on the complement of the space of degree ≤ n − 1 polynomials. Note that An,d(f ) = Pn,df L2(µd).
2

2.1 Some Harmonic Analysis on the Sphere
The d dimensional Legendre polynomials are the sequence of polynomials over [−1, 1] deﬁned by the recursion formula
Pn(x) = 2nn++dd−−34 xPn−1(x) − n+n−d−1 3 Pn−2(x) P0 ≡ 1, P1(x) = x
We also deﬁne hn : Sd−1 × Sd−1 → R by hn(x, x ) = Nd,nPn( x, x ), and for x ∈ Sd−1 we denote Lxn(x ) = hn(x, x ). We will make use of the following properties of the Legendre polynomials. Proposition 3 (e.g. [1] chapters 1 and 2).
1. For every d ≥ 2, the sequence { Nd,nPn} is orthonormal basis of the Hilbert space L2 (µd).
2. For every n, ||Pn||∞ = 1 and Pn(1) = 1. 3. Lxi , Lxj = Pi( x, x )δij.

2.2 Main Result

We say that f : Sd−1 × Sd−1 → R is an inner product function if it has the form f (x, x ) = φ( x, x )

for some function φ : [−1, 1] → R. Let Hd ⊂ L2(Sd−1 × Sd−1) be the space of inner product

functions. We note that

f 2 = E E φ2( x, x ) = E φ 2 = φ 2

xx

x

Hence, the correspondence φ ↔ f deﬁnes an isomorphism of Hilbert spaces between L2(µd) and Hd. In particular, the orthonormal basis { Nd,nPn}∞ n=0 is mapped to {hn}∞ n=0. In particular,

∞

∞

Pn

αihi = αihi

i=0

i=n

Let v, v ∈ Sd−1. We say that f : Sd−1 × Sd−1 → R is (v, v )-separable if it has the form f (x, x ) = ψ( v, x , v , x ) for some ψ : [−1, 1]2 → R. We note that each neuron implements a separable function. Let Hv,v ⊂ L2(Sd−1 × Sd−1) be the space of (v, v )-separable functions. We note that

f 2 = E ψ2( v, x , v , x ) = ψ 2
x,x

Hence, the correspondence ψ ↔ f deﬁnes an isomorphism of Hilbert spaces between L2(µd × µd) and Hv,v . In particular, the orthonormal basis { Nd,nPn ⊗ Nd,mPm}∞ n,m=0 is mapped to {Lvn ⊗ Lvn }∞ n,m=0.
The following theorem implies theorem 1, as under the conditions of theorem 1, any hidden
neuron implement a separable function with norm at most B max|x|≤√4dB+B |σ(x)|, and the bias term is a separable function with norm at most B.

Theorem 4. Let f : Sd−1×Sd−1 → R be an inner product function and let g1, . . . , gr : Sd−1×Sd−1 → R be separable functions. Then

r

2

2 r gi

f − gi ≥ Pnf Pnf − i=1

(1)

i=1

Nd,n

3

Proof. We note that

E hn(x, x )Lvi (x)Lvj (x ) = E Lvi (x) E hn(x, x )Lvj (x )

x,x

x

x

= E Lvi (x) E Lxn(x )Lvj (x )

x

x

= δnj E Lvi (x)Pn( x, v )

(2)

x

= δnj E Lv(x)Lv (x)

Nd,n x i

n

= δnjδniPn( v, v ) Nd,n

Suppose now that f =

∞ i=n

αihi

and

suppose

that

g

=

r j=1

gj

where

each

gj

depends

only

on

vj, x , vj, x for some vj, vj ∈ Sd−1. Write gj(x, x ) = ∞ k,l=0 βkj,lLvkj (x)Lkvj (x ). By equa-

tion (2), Lkvj (x)Lvl j (x ) is orthogonal to f whever k = l. Hence, if we replace each gj with

∞ k=0 βkj,kLkvj (x)Lvkj (x ), the l.h.s. of (1) does not increase. Likewise, the r.h.s. does not decrease.

Hence, we can assume w.l.o.g. that each gj is of the form gj(x, x ) = ∞ i=0 βijLvi j (x)Lvi j (x ). Now,

using (2) again, we have that

f −g 2 =

2

∞

r

αihi − βij Lvi j ⊗ Livj

i=0

j=1

2

∞

r

≥

αihi − βij Lvi j ⊗ Livj

i=n

j=1

∞

∞r

≥

αi2 − 2

αihi, βij Lvi j ⊗ Livj

i=n

i=n j=1

= Pnf 2 − 2 ∞ r βijαiPi( vj, vj )

i=n j=1

Nd,k

≥ Pnf 2 − 2 r ∞ |βij||αi| j=1 i=n Nd,n

r
≥ Pnf 2 − 2

1 Nd,n

∞
|βij |2

∞
|αi|2

j=1

i=n

i=n

≥ Pnf 2 − 2 Pnf

r j=1

gj

Nd,n

4

2.3 Approximating the cosine function
√ Lemma 5. Deﬁne gd,m(x) = sin π dmx . Then, for any d ≥ d0, for a universal constant d0 > 0, and for any degree k polynomial p we have

1 (gd,m(x) − p(x))2dµd(x) ≥ m − k

−1

4eπm

Proof. We have that (e.g. [1]) dµd(x) = √ Γ( d2 )

(1

−

x2)

d−3 2

dx.

Likewise,

for

large

enough

d

and

πΓ( d−2 1 )

|x| < √1 we have 1 − x2 ≥ e−2x2 ≥ e− d2 and hence (1 − x2) d−2 3 ≥ e− d−d 3 ≥ e−1. Likewise, since

d

ΓΓ((d−d2 1)) ∼

√

d2 ,

we

have

that

for

large

enough

d

and

|x| ≤

√1 ,
d

dµd(x) ≥

2edπ .

Hence,

for

f

≥0

we

2

have

−1

√

−1

1

d2

d d2

11

t

−1 f (x)dµd(x) ≥ −d− 21 f (x)dµd(x) ≥ 2eπ −d− 21 f (x)dx = 2eπ −1 f √d dt

Applying this equation for f = gd,m − p we get that

1 (gd,m(x) − p(x))2dµd(x) ≥ 1

1
(sin(πmx) − q(x))2 dx

−1

2eπ −1

Where q(x) := p

√x d

. Now, in the 2m segments Ii =

−1 + i−m1 , −1 + mi

,

i ∈ [2m] we have at

least m − k segments on which x → sin(πmx) and q do not change signs and have opposite signs.

1

On each of these intervals we have

I (sin(πmx) − q(x))2 dx ≥

m
0

sin2(πmx)dx

=

21m .

Lemma 6 (e.g. [6]). Let σ(x) = max(x, 0) be the ReLU activation, f : [−R, R] → R an L-Lipschitz function, and > 0. There is a function

m
g(x) = f (0) + αiσ(γix − βi)
i=1

for which g − f ∞ ≤ . Furthermore, m ≤ 2RL , |βi| ≤ R, |αi| ≤ 2L, γi ∈ {−1, 1}, and g is L-Lipschitz on all R.
Corollary 7. Let f : [−1, 1] → [−1, 1] be an L-Lipschitz function and let > 0. Deﬁne F : Sd−1 × Sd−1 → [−1, 1] by F (x, x ) = f ( x, x ). There is a function G : Sd−1 × Sd−1 → [−1, 1] that satisﬁes F − G ∞ ≤ and furthermore G can be implemented by a depth-3 ReLU network of width 16d2L and weights bounded by max(4, 2L)

Proof.

By

Lemma

6

there

is

a

depth-2

network

Nsquare

that

calculates

x2 2

in

[−2, 2],

with

an

error

of 2dL and has width at most 16dL and hidden layer weights bounded by 2, and prediction layer

weights bounded by 4. For each i ∈ [d] we can compose the linear function (x, x ) → xi + xi with Nsquare to get a depth-2 network Ni that calculates (xi+2xi)2 with an error of 2dL and has the same

width and weight bound as Nsquare. Summing the networks Ni and subtracting 1 results with a

depth-2 network Ninner that calculates x, x with an error of 2L and has width 16d2L and hidden

layer weights bounded by 2, and prediction layer weights bounded by 4.

5

Now, again by lemma 6 there is a depth-2 network Nf that calculates f in [−1, 1], with an error of 2 , has width at most 2L , hidden layer weights bounded by 1 and prediction layer weights bounded by 2L, and is L-Lipschitz. Finally, consider the depth-3 network NF that is the composition of Ninner and Nf . NF has width at most 16d2L weight bound of max(4, 2L), and it satisﬁes
|NF (x, x ) − F (x, x )| = |Nf (Ninner(x, x )) − f ( x, x )| ≤ |Nf (Ninner(x, x )) − Nf ( x, x )| + |Nf ( x, x ) − f ( x, x )| ≤ L|Ninner(x, x ) − x, x | + 2 ≤L + = 2L 2
References
[1] K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An Introduction, volume 2044. Springer, 2012.
[2] Andrew R Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine Learning, 14(1):115–133, 1994.
[3] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In 29th Annual Conference on Learning Theory, pages 698–728, 2016.
[4] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 1989.
[5] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems, pages 666–674, 2011.
[6] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th Annual Conference on Learning Theory, pages 907–940, 2016.
[7] Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183–192, 1989.
[8] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.
[9] James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational eﬃciency of restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages 2877–2885, 2013.
[10] Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-linear functions. arXiv preprint arXiv:1610.09887, 2016.
[11] Matus Telgarsky. Representation beneﬁts of deep feedforward networks. In COLT, 2016.
6

