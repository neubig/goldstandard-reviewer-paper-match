Think about it! Improving defeasible reasoning by ﬁrst modeling the question scenario
Aman Madaan, Niket Tandon†, Dheeraj Rajagopal, Peter Clark†, Yiming Yang, Eduard Hovy
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA † Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA {dheeraj,amadaan,yiming,hovy}@cs.cmu.edu {nikett, peterc}@allenai.org

arXiv:2110.12349v1 [cs.AI] 24 Oct 2021

Abstract

Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly beneﬁt from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model ﬁrst create a graph of relevant inﬂuences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is signiﬁcant as it illustrates that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reﬂexively.1
1 Introduction
Defeasible inference is a mode of reasoning where additional information can modify conclusions (Koons, 2017). Here we consider the speciﬁc formulation and challenge in Rudinger et al. (2020): Given that some premise P plausibly implies a hypothesis H, does new information that the situation is S weaken or strengthen the conclusion H? For example, consider the premise “The drinking glass fell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow” here weakens the implication.
We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans with an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al., 2021), which we use in this paper, draws connections between the P, H, and S through mediating
1Code and data located at https://github.com/ madaan/thinkaboutit

Figure 1: CURIOUS improves defeasible reasoning by modeling the question scenario with an inference graph G adapted from cognitive science literature. The graph is encoded judiciously using our graph encoder h(.), resulting in end task performance improvement.
events. This can be seen as a mental model of the question scenario before answering the question (Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario with inference graphs help machines in defeasible reasoning?
Our approach is as follows. First, given a question, generate an inference graph describing important inﬂuences between question elements. Then, use that graph as an additional input when answering the defeasible reasoning query. Our proposed system, CURIOUS, comprises a graph generation module and a graph encoding module to use the generated graph for the query (Figure 2).
To generate inference graphs, we build upon past work that uses a sequence to sequence approach (Madaan et al., 2021). However, our analysis re-

Figure 2: An overview of CURIOUS
vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream QA task.
The generated inference graph is then used for the QA task on three existing defeasible inference datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015), δ-SOCIAL (reasoning about social norms) (Forbes et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph-augmented question that accounts for interactions between the graph nodes. To achieve this, we use the mixture of experts approach (Jacobs et al., 1991) to include a mixture of experts layers during encoding, enabling the ability to attend to speciﬁc nodes while capturing their interactions selectively.
In summary, our contribution is in drawing on the idea of an inference graph from cognitive science to show beneﬁts in a defeasible inference QA task. Using an error correction module in the graph generation process, and a judicious encoding of the graph augmented question, CURIOUS achieves a new state-of-the-art over three defeasible datasets. This result is signiﬁcant also because our work illustrates that guiding a system to “think about" a question before answering can improve performance.
2 Task
We use the defeasible inference task and datasets deﬁned in (Rudinger et al., 2020), namely given an input x = (P,H,S), predict the output y ∈ {strengthens, weakens}, where P, H, and S are

sentences describing a premise, hypothesis, and scenario respectively, and y denotes whether S strengthens/weakens the plausible conclusion that H follows from P, as described in Section 1.
3 Approach
Inspired by past results (Madaan et al., 2021) that humans found inference graphs useful for defeasible inference, we investigate whether neural models can beneﬁt from envisioning the question scenario using an inference graph before answering a defeasible inference query.
Inference graphs As inference graphs are central to our work, we give a brief description of their structure next. Inference graphs were introduced in philosophy by Pollock (2009) to aid defeasible reasoning for humans, and in NLP by Tandon et al. (2019) for a counterfactual reasoning task. We interpret the inference graphs as having four kinds of nodes (Pollock, 2009; Madaan et al., 2021):
i. Contextualizers (C-, C+): these nodes set the context around a situation and connect to the P.
ii. Situations (S, S-): these nodes are new situations that emerge which might overturn an inference.
iii. Hypothesis (H-, H+): Hypothesis nodes describe the outcome/conclusion of the situation.
iv. Mediators (M-, M+): Mediators are nodes that help bridge the knowledge gap between a situation and a hypothesis node by explaining their connection explicitly. These node can either act as a weakener or strengthener.
Each node in an inﬂuence graph is labeled with an event (a sentence or a phrase). The signs - and + capture the nature of the inﬂuence event node. Concrete examples are present in Figures 1, 4, and in Appendix §D.
3.1 Overview of CURIOUS
Our system, CURIOUS, comprises three components, (i) a graph generator GENinit, (ii) a graph corrector GENcorr, (iii) a graph encoder (Figure 1). GENinit generates an inference graph from the input x. We borrow the sequence to sequence approach of GENinit from Madaan et al. (2021) without any architectural changes. However, we found that the resulting graphs can often be erroneous (which hurts task performance), so CURIOUS includes an error correction module GENcorr to generate higher

Figure 3: An overview of our method to perform graph-augmented defeasible reasoning using a hierarchical mixture of experts. First, MOE-V selectively pools the node representations to generate a representation hG of the inference graph. Then, MOE-GX pools the query representation hx and the graph representation generated by MOE-V to pass to the upstream classiﬁer.

quality inference graphs that are then judiciously encoded using the graph encoder. This encoded representation is then passed through a classiﬁer to generate an end task label. The overall architecture is shown in Figure 2. 3.2 Graph generator
Figure 4: The graphs generated by GENinit.The input graph has repetitions for nodes {C−, C+} and {S, S−}. The corrected graph generated by GENcorr replaces the repetitions with meaningful labels.
As the initial graph generator, we use the method described in Madaan et al. (2021) (GENinit) to generate inference graphs for defeasible reasoning.2 Their approach involves ﬁrst training a graph-generation module, and then performing zero-shot inference on a defeasible query to obtain an inference graph. They obtain training
2We use their publicly available code and data

data for the graph-generation module from WIQA dataset (Tandon et al., 2019). WIQA is a dataset of 2107 (Ti, Gi) tuples, where Ti is the passage text that describes a process (e.g., waves hitting a beach), and Gi is the corresponding inﬂuence graph. The graph generator GENinit is trained as a seq2seq model, by setting input = [Premise] Ti | [Situation] Si | [Hypothesis] Hi, and output = Gi. Note that Si and Hi are nodes in the inﬂuence graph Gi, allowing grounded generation. [Premise], [Situation], [Hypothesis] are special tokens used to demarcate the input.
3.3 Graph corrector
We found that 70% of the randomly sampled 100 graphs produced by GENinit (undesirably) had repeated nodes (an example of repeated nodes is in Figure 4). Repeated nodes introduce noise because they violate the semantic structure of a graph, e.g., in Figure 4, nodes C+ and C- are repeated, although they are expected to have opposite semantics. Higher graph quality yields better end task performance when using inference graphs (as we will show in §4.3.1)
To repair such problems, we train a graph corrector, GENcorr, that takes as input G , and as output it gives a graph G∗, with repetitions ﬁxed. To train the model, we require (G , G∗) examples, which we generate using a data augmentation technique described in the Appendix §A. Because the nodes in the graph are from an open vocabulary, we then train a T5 sequence-to-sequence model (Raffel et al., 2020) with input = G and output =

G∗. In summary, given a defeasible query PHS, we generate a potentially incorrect initial graph G using GENinit. We then feed G to GENcorr to obtain an improved graph G.

3.4 Graph Encoder
For each defeasible query (P, H, S), we add the inference graph G from CURIOUS (the corrected graph from §3.3), to provide additional context for the query, as we now describe.
We concatenate the components (P, H, S) of the defeasible query into a single sequence of tokens x = (P H S), where denotes concatenation. Thus, each sample of our graphaugmented binary-classiﬁcation task takes the form ((x, G), y), where y ∈ {strengthener, weakener}. Following (Madaan et al., 2021), we do not use edge labels and treat all the graphs as undirected graphs.

Overview: We ﬁrst use a language model L to obtain a dense representation hx for the defeasible query x, and a dense representation hv for each node v ∈ G. The node representations hv are then pooled using a hierarchical mixture of experts (MoE) to obtain a graph representation hG. The query representation hx and the graph representation hG are combined to solve the defeasible task. We now provide details on obtaining hx, hv, hG.
3.4.1 Encoding the query and nodes
Let L be a pre-trained language model (in our case RoBERTa (Liu et al., 2019)). We use hS = L(S) ∈ Rd to denote the dense representation of sequence of tokens S returned by the language model L. Speciﬁcally, we use the pooled representation of the beginning-of-sequence token <s> as the sequence representation.
We encode the defeasible query x and the nodes of the graph using L. Query representation is computed as hx = L(x), and we similarly obtain a matrix of node representations hV by encoding each node v in G with L as follows:

hV = [hv1 ; hv2 ; . . . ; h|V|]

(1)

where hvi ∈ Rd refers to the dense representation for the ith node of G derived from L (i.e., hvi = L(vi)), and hV ∈ R|V|×d to refer to the
matrix of node representations.

3.4.2 Graph representations using MoE Recently, mixture-of-experts (Jacobs et al., 1991; Shazeer et al., 2017; Fedus et al., 2021) has

emerged as a promising method of combining multiple feature types. Mixture-of-experts (MoE) is especially useful when the input consists of multiple facets, where each facet has a speciﬁc semantic meaning. Previously, Gu et al. (2018); Chen et al. (2019) have used the ability of MoE to pool disparate features on low-resource and cross-lingual language tasks. Since each node in the inference graphs used by us plays a speciﬁc role in defeasible reasoning (contextualizer, situation node, or mediator), we take inspiration from these works to design a hierarchical MoE model (Jordan and Xu, 1995) to pool node representations hV into a graph representation hG.
An MoE consists of n expert networks E1, E2, . . . , En and a gating network M. Given an input x ∈ Rd, each expert network Ei : Rd → Rk learns a transform over the input. The gating network M : Rd → ∆d gives the weights p = {p1, p2, . . . , pn} to combine the expert outputs for input x. Finally, the output y is returned as a convex combination of the expert outputs:

p = M(x)

n

y = piEi(x)

(2)

i=1

The output y can either be the logits for an end task (Shazeer et al., 2017; Fedus et al., 2021) or pooled features that are passed to a downstream learner (Chen et al., 2019; Gu et al., 2018). The gating network M and the expert networks E1, E2, . . . , En are trained end-to-end. During learning, the gradients to M train it to generate a distribution over the experts that favors the best expert for a given input. Appendix §B presents a further discussion on our MoE formulation and an analysis of the gradients.

Hierarchical MoE for defeasible reasoning Different parts of the inference graphs might help answer a query to a different degree. Further, for certain queries, graphs might not be helpful (and could even be distracting), and the model could rely primarily on the input query alone. This motivates a two-level architecture that can: (i) select a subset of the nodes in the graph and ii) selectively reason across the query and the graph to varying degrees.
Given these requirements, a hierarchical MoE (Jordan and Jacobs, 1994) model presents itself as a natural choice to model this task. The ﬁrst MoE (MOE-V) creates a graph representation

by taking a convex combination of the node representations. The second MoE (MOE-GX) then takes a convex-combination of the graph representation returned by MOE-V and query representation and passes it to an MLP for the downstream task.
• MOE-V consists of ﬁve node-experts and gating network to selectively pool node representations hv to graph representation hG:

p = M(hV)

hG = pvEv(v)

(3)

v∈V

• MOE-GX contains two experts (graph expert EG and question expert EQ) and a gating network to combine the graph representation hG returned by MOE-GX and the query representation hx:

p = M([hG; hQ])

hy = EG(hG) + EQ(hQ)

(4)

hy is then passed to a 1-layer MLP to perform classiﬁcation. The gates and the experts in our MoE model are single-layer MLPs, with equal input and output dimensions for the experts.
4 Experiments

Datasets Our end task performance is measured on the three existing datasets for defeasible inference provided by Rudinger et al. (2020):3 δATOMIC, δ-SNLI, δ-SOCIAL (Table 1). These datasets exhibit substantial diversity because of their domains: δ-SNLI (natural language inference), δ-SOCIAL (reasoning about social norms), and δATOMIC (commonsense reasoning). Thus, it would require a general model to perform well across these diverse datasets.
Baselines and setup The previous state-of-theart (SOTA) is the RoBERTa (Liu et al., 2019) model presented in Rudinger et al. (2020), and we report the published numbers for this baseline. For an additional baseline, we directly use the initial inference graph G generated by GENinit, and provide it to the model simply as a string (i.e., sequence of tokens; a simple, often-used approach). This baseline is called E2E-STR. We use the same hyperparameters as Rudinger et al. (2020), and add a detailed description of the hyperparameters in Appendix §C. For all the QA experiments, we report the accuracy on the test set using the checkpoint with the highest accuracy on the development set. We use the McNemar’s test (McNemar, 1947; Dror et al., 2018) and use p < 0.05 as a threshold for statistical signiﬁcance. All the p-values are reported in Appendix §G.

In this section, we empirically investigate if CURIOUS can improve defeasible inference by ﬁrst modeling the question scenario using inference graphs. We also study the reasons for any improvements.
4.1 Experimental setup

Dataset δ-ATOMIC δ-SOCIAL δ-SNLI

Split # Samples Total

train

35,001

test

4137 42,977

dev

3839

train

88,675

test

1836 92,295

dev

1784

train

77,015

test

9438 95,795

dev

9342

4.2 Results
Table 2 compares QA accuracy on these datasets without and with modeling the question scenario. The results suggest that we get consistent gains across all datasets, with δ-SNLI gaining about 4 points. CURIOUS achieves a new state-of-the-art across three datasets, as well as now producing justiﬁcations for its answers with inference graphs.

Prev-SOTA E2E-STR CURIOUS

δ-ATOMIC
78.3 78.8 80.2*

δ-SNLI
81.6 82.2 85.6*

δ-SOCIAL
86.2 86.7 88.6*

Table 2: CURIOUS is better across all the datasets. This demonstrates that understanding the question scenario through generating an inference graph helps. * indicates statistical signiﬁcance.

Table 1: Number of samples in each dataset by split.

3github.com/rudinger/defeasible-nli

4.3 Understanding CURIOUS gains
In this section, we study the contribution of the main components of the CURIOUS pipeline.
4.3.1 Impact of graph corrector
We ablate the graph corrector module GENcorr in CURIOUS by directly supplying the output from GENinit to the graph encoder. Table 3 shows that this ablation consistently hurts across all the datasets. GENcorr provides 2 points gain across datasets. This indicates that better graphs lead to better task performance, assuming that GENcorr actually reduces the noise. Next, we investigate if GENcorr can produce more informative graphs.

δ-ATOMIC δ-SNLI δ-SOCIAL

G

78.5 83.8

88.2

G

80.2* 85.6*

88.6

Table 3: Performance w.r.t. the graph used. G is the initial graph from GENinit, G is the corrected graph from GENcorr. Better graphs lead to better task performance. * indicates statistical signiﬁcance.

Do graphs corrected by GENcorr show fewer repetitions? We evaluate the repetitions in the graphs produced by GENinit and GENcorr using two metrics: (i) repetitions per graph: average number of repeated nodes in a graph. (ii) % with repetitions: % of graphs with at least one repeated node.

δ-ATOMIC δ-SNLI δ-SOCIAL
OVERALL

Repetitions per graph % graphs per graph % graphs per graph % graphs per graph % graphs

GENinit
2.05 72 2.09 73 2.2 75

GENcorr
1.26 48 1.18 46 1.32 49 ∆ -40% ∆ -25.7%

Table 4: GENcorr reduces the inconsistencies in graphs. The number of repetitions per graph and percentage of graphs with some repetition, both improve.

Table 4 shows GENcorr does reduce repetitions by approximately 40% (2.11 to 1.25) per graph across all datasets, and also reduces the fraction of graphs with at least one repetition by 25.7% across.
4.3.2 Impact of graph encoder We experiment with two alternative approaches to graph encoding to compare our MoE approach by

using the graphs generated by GENcorr: 1. Graph convolutional networks: We follow the approach of Lv et al. (2020) who use GCN (Kipf and Welling, 2017) to learn rich node representations from graphs. Broadly, node representations are initialized by L and then reﬁned using a GCN. Finally, multi-headed attention (Vaswani et al., 2017) between question representation hx and the node representations is used to yield hG. We add a detailed description of this method in Appendix §H.
2. String based representation: Another popular approach (Sakaguchi et al., 2021) is to concatenate the string representation of the nodes, and then using L to obtain the graph representation hG = L(v1 v2 ..) where denotes string concatenation.
Table 5 shows that MoE graph encoder improves end task performance signiﬁcantly compared to the baseline.4 In the following analysis, we study the reasons for these gains in-depth.
We hypothesize that GCN is less resistant to noise than MoE in our setting, thus causing a lower performance. The graphs augmented with each query are not human-curated and are instead generated by a language model in a zero-shot inference setting. Thus, the GCN style message passing might amplify the noise in graph representations. On the other hand, MOE-V ﬁrst selects the most useful nodes to answer the query to form the graph representation hG. Further, MOE-GX can also decide to completely discard the graph representations, as it does in many cases where the true answer for the defeasible query is weakens.
To further establish the possibility of message passing hampering the downstream task performance, we experiment with a GCN-MoE hybrid, wherein we ﬁrst reﬁne the node representations using a 2-layer GCN as used by (Lv et al., 2020), and then pool the node representations using an MoE. We found the results to be about the same as ones we obtained with GCN (3rd-row Table 5), indicating that bad node representations are indeed the root cause for the bad performance of GCN. This is also supported by Shi et al. (2019) who found that noise propagation directly deteriorates network embedding and GCN is sensitive to noise.
Interestingly, graphs help the end-task even when encoded using a relatively simple STR based encoding scheme, further establishing their utility.
4Appendix §E provides an analysis on time and memory requirements.

STR GCN
GCN + MoE MoE

δ-ATOMIC
79.5 78.9 78.7 80.2

δ-SNLI
83.1 82.4 84.3 85.6

δ-SOCIAL
87.2 88.1 87.8 88.6

Table 5: Contribution of MoE-based graph encoding compared with alternative graph encoding methods. The gains of MoE over GCN are statistically signiﬁcant for all the datasets, and the gains over STR are signiﬁcant for δ-SNLI and δ-SOCIAL.

4.3.3 Detailed MoE analysis
We now analyze the two MoEs used in CURIOUS: (i) the MoE over the nodes (MOE-V), and (ii) the MoE over G and input x (MOE-GX).

more background context to the question, and Snode is typically an inverse situation (i.e., inverse S), while M- and M+ are the mediator nodes leading to the hypothesis. Figure 6 shows that the situation node S- was the most important, followed by the contextualizer and the mediator. Notably, our analysis shows that mediators are less important for machines than they were for humans in the experiments conducted by Madaan et al. (2021). This is probably because humans and machines use different pieces of information. As our error analysis shows in §5, the mediators can be redundant given the query x. Humans might have used the redundancy to reinforce their beliefs, whereas machines leverage the unique signals present in S- and the contextualizers.

Figure 6: MOE-V gate values for the three datasets.

Figure 5: MOE-GX gate values for the classes strengthens and weakens, averaged over the three datasets.
MOE-GX performs better for y = strengthens: Figure 5 shows that the graph makes a stronger contribution than the input, when the label is strengthens. In instances where the label is weakens, the gate of MOE-GX gives a higher weight to the question. This trend was present across all the datasets. We conjecture that this happens because language models are tuned to generate events that happen rather than events that do not. In the case of a weakener, the nodes must be of the type event1 leads to less of event2, whereas language models are naturally trained for event1 leads to event2. Understanding this in-depth requires further investigation in the future.
MOE-V relies more on speciﬁc nodes: We study the distribution over the types of nodes and their contribution to MOE-V. Recall from Figure 3 that C- and C+ nodes are contextualizers that provide

MOE-V, MOE-GX have a peaky distribution: A peaky distribution over the gate values implies that the network is judiciously selecting the right expert for a given input. We compute the average entropy of MOE-V and MOE-GX and found the entropy values to be 0.52 (max 1.61) for MOE-V, and 0.08 (max 0.69) for MOE-GX. The distribution of the gate values of MOE-V is relatively ﬂat, indicating that specialization of the node experts might have some room for improvement (additional discussion in Appendix §B). Analogous to scene graphs-based explanations in visual QA (Ghosh et al., 2019), peaky distributions over nodes can be considered as an explanation through supporting nodes.
MOE-V learns the node semantics: The network learned the semantic grouping of the nodes (contextualizers, situation, mediators), which became evident when plotting the correlation between the gate weights. As Figure 7 shows, there is a strong negative correlation between the situation nodes and the context nodes, indicating that only

one of them is activated at a time.

Figure 7: Correlation between probability assigned to each semantic type of the node by MOE-V
5 Error analysis

now fail

prev. fail

δ-ATOMIC 615 δ-SNLI 197 δ-SOCIAL 772

now succ
δ-ATOMIC 294 δ-SNLI 124 δ-SOCIAL 398

prev. succ

δ-ATOMIC 207 δ-SNLI 68 δ-SOCIAL 302

δ-ATOMIC 3022 δ-SNLI 1448 δ-SOCIAL 7967

Table 6: Confusion matrix: can CURIOUS ﬁx previously failing or successful examples?
Table 6 shows that CURIOUS is able to correct several previously wrong examples. When CURIOUS corrected previously failing cases, the MOE-V relied more on mediators, as the average mediator probabilities go up from 0.09 to 0.13 averaged over the datasets. CURIOUS still fails, and more concerning are the cases when previously successful examples now fail. To study this, we annotate 50 random dev samples over the three datasets (26/24 examples for weakens/strengthens label). For each sample, a human-annotated if the graph had errors. We observe the following error categories:5
• All nodes off-topic (4%): The graph nodes were not on topic. This (rarely) happens when CURIOUS cannot distinguish the sense of a word in the input question. For instance, S = there is a water fountain in the center – CURIOUS generated based on an incorrect word sense of natural water spring.
5Concrete examples in Appendix §F

• Repeated nodes (20%): These may be exact or near-exact matches. Node pairs with similar effects tend to be repeated in some samples. E.g., the S- node is often repeated with contextualizer C- perhaps because these nodes indirectly affect graph nodes in a similar way.
• Mediators are uninformative (34%): The mediating nodes are not correct or informative. One source of these errors is when the H and S are nearly connected by a single hop, e.g., H = personX pees, and S = personX drank a lot of water previously.
• Good graphs are ineffective (42%): These graphs contained the information required to answer the question, but the gating MOE mostly ignored this graph. This could be attributed in part to the observation in the histogram in Figure 5, that samples with weakens label disproportionately ignore the graph.
In accordance with the ﬁndings of Rudinger et al. (2020), the maximum percentage of errors was in δ-ATOMIC, in part due to low question quality.
6 Explainability
In this section, we analyze the explainability of CURIOUS model. Jacovi and Goldberg (2020) note that an explanation should aim towards two complementary goals: i) Plausibility: provide an interpretation of system outputs that is convincing for humans, and ii) Faithfulness: capture the actual reasoning process of a model. We discuss how our approach takes a step towards addressing these goals.
Plausibility In a prior work, Madaan et al. (2021) show that human annotators selectively picked and chose parts of the graph that explained a model decision and enabled them in improving on the task of defeasible reasoning. We show in §4.3.3, the MoE gate values gives insights into the part of the graph (contextualizer, mediator, situation node) that the model leveraged to answer a query. Our model thus produces a reasoning chain that is similar to the explanation that humans understand, providing a step towards building inherently plausible models, while also achieving better performance.
Measuring faithfulness w.r.t. graphs Since faithfulness is a widely debated term, we restrict its deﬁnition to measure faithfulness w.r.t to the reasoning graph. This can be measured by the correlation between the model performance and graph

correctness. A high correlation implies that the model uses both the graph and query to generate an answer and thus is faithful to the stated reasoning mechanism (i.e., graphs used to answer a question). Our analysis reveals this to be the case: in cases where the model answers incorrectly, 42% of the graphs were entirely correct (§5). In contrast, when the model answers correctly, 82% of the graphs are correct. In summary, we hope that CURIOUS serves as a step towards building reasoning models that are both plausible and faithful.
7 Related work
Mental Models Cognitive science has long promoted mental models - coherent, constructed representations of the world - as central to understanding, communication, and problem-solving (JohnsonLaird, 1983; Gentner and Stevens, 1983; Hilton, 1996). Our work draws on these ideas, using inference graphs to represent the machine’s “mental model” of the problem at hand. Building the inference graph can be viewed as ﬁrst asking clariﬁcation questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of inﬂuence.
Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question.
Encoding Graph Representations Several existing methods use graphs as an additional input for commonsense reasoning (Sun et al., 2018; Lin et al., 2019; Lv et al., 2020; Feng et al., 2020; Bosselut et al., 2021; Ma et al., 2021; Kapanipathi et al., 2020). These methods ﬁrst retrieve a graph relevant to a question using information retrieval techniques and then encode the graph using graph representation techniques like GCN (Kipf and Welling, 2017) and graph attention (Velickovic et al., 2018). Different from these works, we use a graph generated from the query for answering the commonsense question. The graphs con-

sumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best model uses a mixture-of-experts (MoE) (Jacobs et al., 1991) model to pool multifaceted input. Prior work has shown the effectiveness of using MoE for graph classiﬁcation (Zhou and Luo, 2019; Hu et al., 2021), cross-lingual language learning (Chen et al., 2019; Gu et al., 2018), and model ensemble learning (Fedus et al., 2021; Shazeer et al., 2017). To the best of our knowledge, we are the ﬁrst to use MoE for learning and pooling graph representations for QA task.
8 Summary and Conclusion
Cognitive science suggests that people form “mental models” of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from GCN-based models popular in graph learning, we use mixtureof-experts to pool graph representations. Our experiments show that MoE-based pooling can be a strong (both in terms of performance and explainability) alternative to GCN for graph-based learning for reasoning tasks. Our method establishes a new state-of-the-art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reﬂexively.
Acknowledgments
We thank the anonymous reviewers for their feedback. Special thanks to reviewer 2 for their insightful comments on our MoE formulation. This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.

References
Antoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021. Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. In Proceedings of the 35th AAAI Conference on Artiﬁcial Intelligence (AAAI).
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762–4779, Florence, Italy. Association for Computational Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Association for Computational Linguistics.
Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, and Claire Cardie. 2019. Multisource cross-lingual model transfer: Learning what to share. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3098–3112, Florence, Italy. Association for Computational Linguistics.
P. Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, B. D. Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, Dirk Groeneveld, Michal Guerquin, and Michael Schmitz. 2020. From ’f’ to ’a’ on the n.y. regents science exams: An overview of the aristo project. AI Mag., 41:39–53.
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhiker’s guide to testing statistical signiﬁcance in natural language processing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1383–1392, Melbourne, Australia. Association for Computational Linguistics.
et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorchlightning, 3.
William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961.
Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020. Scalable multi-hop relational reasoning for knowledge-aware question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1295–1309, Online. Association for Computational Linguistics.

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653–670, Online. Association for Computational Linguistics.
Dedre Gentner and Albert L. Stevens. 1983. Mental Models. Lawrence Erlbaum Associates.
S. Ghosh, Giedrius Burachas, Arijit Ray, and Avi Ziskind. 2019. Generating natural language explanations for visual question answering using scene graphs and visual attention. ArXiv, abs/1902.05715.
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018. Universal neural machine translation for extremely low resource languages. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344–354, New Orleans, Louisiana. Association for Computational Linguistics.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International Conference on Machine Learning, pages 3929–3938. PMLR.
D. Hilton. 1996. Mental models and causal explanation: Judgements of probable cause and explanatory relevance. Thinking & Reasoning, 2:273–308.
Fenyu Hu, Liping Wang, Shu Wu, Liang Wang, and Tieniu Tan. 2021. Graph classiﬁcation by mixture of diverse experts. arXiv preprint arXiv:2103.15622.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):79–87.
Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we deﬁne and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198–4205, Online. Association for Computational Linguistics.
P. Johnson-Laird. 1983. Mental Models : Towards a Cognitive Science of Language. Harvard University Press.
Michael I Jordan and Robert A Jacobs. 1994. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181–214.
Michael I Jordan and Lei Xu. 1995. Convergence results for the em approach to mixtures of experts architectures. Neural networks, 8(9):1409–1431.
Pavan Kapanipathi, Veronika Thost, Siva Sankalp Patel, Spencer Whitehead, Ibrahim Abdelaziz, Avinash Balakrishnan, Maria Chang, Kshitij P. Fadnis, R. Chulaka Gunasekara, Bassem Makni, Nicholas

Mattei, Kartik Talamadupula, and Achille Fokoue. 2020. Infusing knowledge into the textual entailment task using graph convolutional networks. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8074–8081. AAAI Press.
Thomas N. Kipf and Max Welling. 2017. Semisupervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
Robert Koons. 2017. Defeasible Reasoning. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Winter 2017 edition. Metaphysics Research Lab, Stanford University.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2829–2839, Hong Kong, China. Association for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu. 2020. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8449– 8456. AAAI Press.
Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, and Alessandro Oltramari. 2019. Towards generalizable neuro-symbolic systems for commonsense question answering. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 22–32, Hong Kong, China. Association for Computational Linguistics.
Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, and Alessandro Oltramari. 2021. Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 13507–13515.

Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, and Eduard Hovy. 2021. Could you give me a hint ? generating inference graphs for defeasible reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5138–5147, Online. Association for Computational Linguistics.
Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153–157.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. NIPS 2017 Workshop Autodiff Submission.
J. Pollock. 1987. Defeasible reasoning. Cogn. Sci., 11:481–518.
J. Pollock. 2009. A recursive semantics for defeasible reasoning. In Argumentation in Artiﬁcial Intelligence.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67.
Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, and Yejin Choi. 2020. Thinking like a skeptic: Defeasible inference in natural language. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4661–4675, Online. Association for Computational Linguistics.
Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. 2021. proscript: Partially ordered scripts generation via pre-trained language models. arxiv.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: an atlas of machine commonsense for if-then reasoning. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 3027–3035. AAAI Press.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 2426, 2017, Conference Track Proceedings. OpenReview.net.

M. Shi, Yufei Tang, Xingquan Zhu, and J. Liu. 2019. Feature-attention graph convolutional networks for noise resilient learning. ArXiv, abs/1912.11755.
Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615–4629, Online. Association for Computational Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 4444–4451. AAAI Press.
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4231–4242, Brussels, Belgium. Association for Computational Linguistics.
Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reasoning about actions and state changes by injecting commonsense knowledge. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 57–66, Brussels, Belgium. Association for Computational Linguistics.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, and Antoine Bosselut. 2019. WIQA: A dataset for “what if...” reasoning over procedural text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6076–6085, Hong Kong, China. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998–6008.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
Thomas Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, et al. 2019. Huggingface’s transformers:

State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72–77, Minneapolis, Minnesota. Association for Computational Linguistics.
Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 42–49.
Xuanyu Zhou and Yuanhang Luo. 2019. Explore mixture of experts in graph neural networks. Stanford CS224W.

A Training graph corrector
As mentioned in Section §3.2, the graph generator GENinit is trained as a seq2seq model from WIQA with input = [Premise] Ti | [Situation] Si | [Hypothesis] Hi, and output = Gi. Graphs in WIQA additionally capture the inﬂuence that the situation has on the hypothesis. Denoting this inﬂuence label by yi can be either helps or hurts
From our experiments, we observe that appending yi to the training data (from input = [Premise] Ti | [Situation] Si | [Hypothesis] Hi to input = [Premise] Ti | [Situation] Si | [Hypothesis] Hi | yi) reduces repetitions by 13%.
We refer to this data generator as GEN∗init, and the graphs produced by it as G∗. However, we do not have access to y during test time, and thus GEN∗init cannot be used directly to produce G∗ for defeasible queries. We circumvent this by using GEN∗init to train a graph-to-graph generation model, that takes as input G and generates G∗ as output (G → G∗). We call this system GENcorr. We give an overview of the process in Figure 8. In Figure 9, we give examples of an intial graph produced by GENinit, the corresponding graph produced by GEN∗init, and the graph produced by GENcorr.

yˆc = =

exp(oc)

k j=1

exp(oj

)

exp(

n i=1

piEic)

k j=1

exp(

n i=1

piEij

)

Let L be the cross-entropy loss:

k

L = − log yˆc = −oc + log exp(oj)

j=1

n

k

n

= − piEic + log exp( piEij)

i=1

j=1

i=1

Evaluating ∂∂pLm The derivatives w.r.t. the mth expert gate probability pm is given by:

∂L ∂pm = −Emc +

k j=1

Emj

exp(

n i=1

piEij

)

k j=1

exp(

n i=1

piEij

)

k

= −Emc + yˆjEmj

j=1

k

= −Emc(1 − yˆc) +

yˆjEmj (5)

j=1,j=c

Evaluating ∂E∂Lmc the derivatives w.r.t. the logits Emc (logit for the correct class by mth expert) is given by:

Figure 8: Training data generation to train GENcorr.

B MoE gradient analysis
We restate Equation 2 for quick reference:

p = M(x)
n
o = piEi(x)
i=1

where we have changed the notation slightly to

use o as the MoE output instead of y. We also

refer to Ei(x) as Ei. Further, oj =

n i=1

piEij

.

We present the analysis for a generic multi-class

classiﬁcation setting with k classes, with training

done using a cross-entropy loss L (Figure 10) Let

yˆc be the normalized probability of the correct class

c calculated using softmax:

∂L

exp(oc)pm

∂Emc = −pm +

k j=1

exp

oj

= −pm(1 − yˆc)

(6)

Equations 5 and 6 have natural interpretations: the gradient on both the mixture probability pm and the logits Emc will be 0 (note that for Equation 5, yc = 1 =⇒ yj = 0 for j = c) when the network makes perfect predictions (yˆc = 1). As noted by Jacobs et al. (1991) (Section 1), this might cause the network to specialize slower, as the gradient will be small for experts that are helping in making the correct prediction. They suggest a different loss function that promotes faster specialization by redeﬁning the error function in terms of a mixture distribution, with the mixture weights supplied by the pi terms. Analyzing the effect of loss function for applications where the MoE is used to pool representations remains an interesting future work.

Figure 9: The graphs generated by GENinit (left), GEN∗init (middle), and GENcorr (right).The input graph has repetitions for nodes {C−, S−}, {C+, H+}, and {M −, M +}. The corrected graph replaces the repetitions with
meaningful labels.

Figure 10: MoE gradient analysis setup: we consider a simple setting where the weighted output of the experts (using the expert weights p) is directly fed to a softmax and is used for generating class probabilities yˆ.

C Hyperparameters

Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Falcon, 2019) and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP. For the experts, we set the input size set to be the same as output size. Table 7 shows the parameters shared by all the methods, and 8 shows the hyperparameters applicable to GCN encoder.
D Schema of an inﬂuence graph

Hyperparameter
Pre-trained model Learning rate Gradient accumulation batches Num epochs Optimizer Dropout Learning rate scheduling Warmup Batch size Weight decay Gradient clipping

Value
RoBERTa-base 2e-5 2 30
AdamW 0.1
linear 3 epochs
16 0.01
1.0

Table 7: General hyperparameters used by all the models.

Figure 11 shows the skeleton of an inﬂuence graph.

Hyperparameter
# Layers Layer dropout Number of attention heads Attention dimension

Value
2 0.1
1 256

Table 8: Hyperparameters speciﬁc to GCN.

and these are for the cases where CURIOUS introduced a wrong answer, while baseline answered this correctly without the graph.
• Figure 12 shows a failure case when a good graph is unused. Example from δ-ATOMIC dev set.
• Figure 13 shows a failure case when an off topic graph is produced due to confusion in the sense of water fountain. Example from δ-SNLI dev set.
• Figure 14 shows a failure case when the mediator is wrong. Example from δ-SOCIAL dev set.

Figure 11: Schema of an inference graph.

G Signiﬁcance Tests
We perform two statistical tests for verifying our results: i) The micro-sign test (s-test) (Yang and Liu, 1999), and ii) McNermar’s test (Dror et al., 2018).

E Runtime Analysis
Finally, we discuss the cost-performance tradeoffs for various encoding mechanisms (Table 9). As Table 9 shows, both GCN and MoE take about 7% more number of parameters than the STR encoding scheme and have about 2x the runtime. Further, as we use one expert per node, the number of parameters scales linearly with the number of nodes. While this is not prohibitive in our setting (each graph has a small number of nodes), our analysis shows that the behavior of the nodes that have similar semantics is correlated, indicating that the experts for those nodes can share parameters. Alternatively, MoE with more than two layers (Jordan and Xu, 1995) can also help in scaling the number of parameters only logarithmically with the number of nodes.

Method
#Params Runtime

STR
124M 0.17

GCN
131M 0.47

MoE
133M 0.40

Table 9: Number of parameters in the different encoding methods. Runtime reports the number of seconds to process one training example.

Dataset
δ-ATOMIC δ-SNLI δ-SOCIAL

s-test
5.07e-05 2.65e-05
1.4e-04

McNemar’s test
1.1e-04 6.5e-05 3.2e-04

Table 10: p-values for the three datasets and two different statistical tests while comparing the results with and without graphs (Table 2). As the p-values show, the results in Table 2 are highly signiﬁcant

Dataset
δ-ATOMIC δ-SNLI δ-SOCIAL

s-test
0.001 0.01 0.06

McNemar’s test
0.003676 0.026556 0.146536

Table 11: p-values for the three datasets and two different statistical tests while comparing the results with noisy vs. cleaned graphs (Table 3).

δ-ATOMIC δ-SNLI δ-SOCIAL

STR

0.13 1.8e-06 8.7e-06

GCN

0.006 1.31e-05

0.03

Table 12: p-values for the s-test for Table 5.

F Error Analysis Examples
We show three examples with different types of errors. These examples are taken from Dev set,

Figure 12: Example of a failure case: A good graph is unused. Example from δ-ATOMIC dev set.
Figure 13: Example of a failure case: The generated graph is off topic (wrong sense of water fountain is used). Example from δ-SNLI dev set.

Figure 14: Example of a failure case: The mediator nodes (second last level in the graph) are unhelpful. Example from δ-SOCIAL dev set.

δ-ATOMIC δ-SNLI δ-SOCIAL

STR

0.28 4e-06

GCN 0.015127 3.2e-05

2e-05 0.06

Table 13: p-values for the McNemar’s for Table 5.

H Description of GCN encoder
We now describe our adaptation of the method by Lv et al. (2020) to pool hV into hG using GCN. Figure 15 captures the overall design.
Reﬁning node representations The representation for each node v ∈ V is ﬁrst initialized using:
h0v = W0hv
Where hv ∈ Rd is the node representation returned by the L, and W0 ∈ Rd×k. This initial representation is then reﬁned by running L-layers of a GCN (Kipf and Welling, 2017), where each layer l + 1 is updated by using representations from the lth layer as follows:





h(vl+1) = σ  |A1(v)|

Wlhlw + Wlhlv

w∈A(v)

HL = [hL0 ; hL1 ; . . . ; hL|V|−1]

(7)

(8)

Where σ is a non-linear activation function, Wl ∈ Rk×k is the GCN weight matrix for the lth layer, A(v) is the list of neighbors of a vertex v, and HL ∈ R|V|×k is a matrix of the Lth layer representations the |V| nodes such that HLi = hLi .

Learning graph representation We use multi-
headed attention (Vaswani et al., 2017) to combine the query representation hQ and the nodes representations HL to learn a graph representation hG. The multiheaded attention operation is deﬁned as
follows:

ai = softmax

(WiqhQ)(WikHL)T √ d

headi = ai(WivHL) hG = Concat(head1, . . . , headh)WO

= MultiHead(hQ, HL)

(9)

Where h is the number of attention heads,

Wiq

,

W

k i

,

W

v i

∈

Rk×d

and

WO

∈

Rhd×d.

Finally, the graph representation generated by

the the MultiHead attention hG ∈ Rn is concate-

nated with with the question representation hQ to

get the prediction:

I All results
Our experiments span two types of graphs (G , G), three datasets (δ-SNLI, δ-SOCIAL, δ-ATOMIC), and three graph encoding schemes (STR, GCN, MoE). Table 14 above shows the results on all 18 combinations of {graph types} × {datasets} × {graph encoding schemes}

Dataset δ-ATOMIC δ-SNLI δ-SOCIAL

Encoder
STR STR GCN GCN
MoE MoE
STR STR GCN GCN
MoE MoE
STR STR GCN GCN
MoE MoE

Graph Type
n/a G G G G G G
n/a G G G G G G
n/a G G G G G G

Accuracy
78.78 79.48 78.25 78.85 78.83 80.15
82.16 83.11 82.63 83.09 83.83 85.59
87.6 86.75 87.24 87.92 88.12 88.45 88.62

Table 14: Results for different combinations of graph encoder, graph type.

J Graph-augmented defeasible reasoning algorithm
In Algorithm 1, we outline our graph-augmented defeasible learning process.

yˆ = softmax([hG, hQ]Wout) where Wout ∈ Rd×2 is a single linear layer MLP.

Figure 15: Overview of the GCN encoder.
Algorithm 1: Graph-augmented defeasible reasoning using MoE. Given: A language model L, defeasible
query with graph (x, G). Result: Result for the query. // Encode query hQ ← L(x); // encode nodes of G hV ← L(v ∈ G) ; // MOE1: Combine nodes hG ← Equation 3; // MOE2: Combine Q, G hy ← Equation 4; return softmax(MLP(hy))

