When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection
Aditi Chaudhary†, Kayo Yin†, Antonios Anastasopoulos‡, Graham Neubig† †Carnegie Mellon University, ‡George Mason University
{aschaudh,kayoy,gneubig}@cs.cmu.edu antonis@gmu.edu

arXiv:2109.06014v1 [cs.CL] 13 Sep 2021

Abstract
Learning ﬁne-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun “wall” has different lexical manifestations in Spanish – “pared” refers to an indoor wall while “muro” refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying ﬁne-grained lexical distinctions, and extracting concise descriptions explaining these distinctions in a human- and machine-readable format. We conﬁrm the quality of these extracted descriptions in a language learning setup for two languages, Spanish and Greek, where we use them to teach non-native speakers when to translate a given ambiguous word into its different possible translations. Code and data are publicly released here.1
1 Introduction
With increasing globalization there is a widespread prevalence and need for good materials and tools to help people learn languages. Curating such content manually requires a large time and cost investment which poses a challenge particularly for languages where protection and revival efforts are ongoing (Moline, 2020). One of the most important and challenging processes in learning a new language (L2) is vocabulary acquisition (Ellis, 1996; Moore, 1996), which is generally made easy by associating L2 words with words from the ﬁrst language (L1) (Hulstijn et al., 1996; Watanabe, 1997). In many cases, L1 words or word senses can be unambiguously associated with L2 words. For example “linguistics” and “lingüística” essentially form a one-to-one mapping between English and Spanish. However, different languages carve up the semantic space of the world in different ways leading
1https://github.com/Aditi138/ LexSelection

En: wall Es: muro
Es: pared
Figure 1: Semantic subdivision for the concept ‘wall’ results in different lexical manifestations in Spanish: ‘muro’ for outside wall and ‘pared’ for inside wall whereas in English both are referred as ‘wall’.
to semantic subdivisions, distinctions made in one language not made in another. For example, “wall” in English is manifested differently in Spanish as “pared” or “muro”, as shown in Figure 1, and for an L1 English speaker it may not be immediately obvious when one should be used over the other. A skilled teacher or comprehensive language learning resource may be able to provide explanations that resolve this ambiguity. For example, Robertson (2020) present word deﬁnitions in-context for Finnish learners, while CAVOCA (Groot, 2000) takes a learner through various stages of the word acquisition process including word usage, syntax.
In this work, we propose a method to automatically discover rules regarding ﬁne-grained lexical distinctions and present L2 learners with concise descriptions derived from them in an interactive framework. Research in L2 vocabulary acquisition (Groot, 2000) has shown that it is effective to combine strategies using explicit deﬁnitions and examples in context. However, our work contrasts to most prior work in this ﬁeld such as CAVOCA (Groot, 2000) or Duolingo,2 which use learning
2https://www.duolingo.com/

content manually created by subject matter experts. This necessity for curation makes it difﬁcult to comprehensively scale this approach to many languages.
Speciﬁcally, our framework consists of two steps: i) use a parallel corpus to identify words in L1 which have different lexical manifestations owing to a semantic subdivision in L2, and ii) create human- and machine-readable concise descriptions that allow for easier interpretation of each lexical distinction. First, we extract source (L1) and target (L2) parallel sentences for each shortlisted L1 word. We then extract lexical and semantic features, as well as a label encoding the lexical choice in the target language from these parallel sentences for each L1 word. Finally, we train a prediction model that distinguishes between the lexical choices, and extract human-understandable descriptions from this model. These descriptions could either be used as-is, or could be used as a starting point for further curation by educators.
To conﬁrm the quality of the extracted descriptions, we conduct a study where we use them to teach English native speakers lexical distinctions arising from semantic subdivisions in Spanish and Greek. We make our study interactive by presenting the learning content in the form of cloze tests (Taylor, 1953) where the English word to be taught is presented to the learner in context along with extracted concise description. The learner is then required to select the most appropriate lexical choice from the given set. The main methodological contributions therefore are automated methods to:
• Identify ﬁne-grained lexical distinctions arising due to semantic subdivisions. To evaluate this and future work, we also create a lexical selection dataset for two language pairs, English-Spanish and English-Greek.
• Extract rules to help humans understand the usage of lexical distinctions in context. Studies with 7 Spanish and 9 Greek learners show that they learn faster when given access to our extracted descriptions; for example they achieve an (avg.) accuracy of 81% within roughly 20 questions, as opposed to more than 40 questions required otherwise.
2 Problem Formulation
For the purpose of this paper, we deﬁne the task of lexical selection as choosing contextually correct translations from a set of target translations for an

ambiguous word in the source language (Lefever and Hoste, 2010). We ﬁrst deﬁne some variables: x = x1, x2, . . . , x|x| denotes a sentence in the source language (L1), y = y1, y2, . . . , y|y| is its translation in the target language (L2) and Vx and Vy are the source and target vocabulary respectively. Given a source sentence x containing an ambiguous word xi, then trans(xi) ⊆ Vy denotes the set of its “possible” target translations i.e. words in the target language to which the ambiguous word xi might be translated (concrete methods to deﬁne this set are explained later). The task of lexical selection involves choosing the most appropriate translation yi ∈ trans(xi), and can be performed either by machines or humans.3 In this work, we particularly focus on machine-learned methods to help humans learn lexical selection, extracting lexical selection models that are not only usable by machines, but also interpretable by humans in order to aid the process of learning a new language. We thus plan to extract the rule set Rvx which governs this lexical selection process in a human- and machine-readable format.
3 Identifying Semantic Subdivisions
In this section, we describe in detail the procedure for identifying L1 words that have different lexical manifestations in L2 owing to semantic subdivisions. For the purpose of this work, we refer to these different lexical manifestations in L2 as lexical choices and the corresponding L1 words as focus words. Our work is “loosely inspired” by ContraWSD (Rios et al., 2018) and SemEval2013 (Lefever and Hoste, 2013) which construct a dataset for cross-lingual word sense disambiguation, using a semi-automatic approach combining frequency-based heuristics with human supervision. These datasets are restricted to a subset of manually selected nouns (20 for SemEval-2013 and 70-80 for ContraWSD). In contrast, our approach is fully automated going beyond using just frequency-based ﬁlters. Furthermore, we do not restrict to any one word class leading to words being identiﬁed across different word classes (nouns, verbs, adjectives, adverbs) for both Spanish and Greek.4
We start with a parallel corpus D = {(x1, y1), · · · , (x|D|, y|D|)} where (xm, ym) denote the source and target sentence pair. Next, we
3The notation here refers to single-word translations which are the focus of this work.
4More details in Section §5.

extract word alignments automatically using a word aligner that ﬁnds sets of pairs of source and target words Am = { xi, yj : xi ∈ xm, yj ∈ ym}, where for each word pair xi, yj , xi and yj are semantically similar to each other within this context.
To focus on translations of the underlying content, as opposed to morphological variations, we then lemmatize all words in both the source and target sentence pairs. Thus, Vx and Vy refer to the lemmatized vocabulary of the source and target language. Going forward, all words refer to their respective lemmatized forms. We perform automatic part-of-speech (POS) tagging, dependency parsing and word sense disambiguation (WSD) on the source side data, resulting in a POS tag and word sense associated with each source word, tag(xi) ∈ Tx and sense(xi) ∈ Sx where Tx is the set of POS tags and Sx is the word sense vocabulary in the source language.
In order to identify the focus words, we extract a list of lemmatized L1 word types vx ﬁltered by their part-of-speech (POS) tags tx giving us tuples of the form vx, tx . This ensures that we don’t conﬂate meanings across POS tags, because in many languages the semantics of a word can vary widely across its different POS tags.5 We refer to the extracted tuples vx, tx as focus words for simplicity. We then extract the focus words with their respective lexical choices as follows:
1. Extract translations : For each aligned word pair xi, yj compute the number of times c(vx, tx, vy) the lemmatized source word type (vx = lemma(xi)) along with its POS tag (tx = tag(xi)) is aligned to the lemmatized target word type (vy = lemma(yj)) across the whole corpus. Also, store the number of times the word sense of xi (sx = sense(xi)) appears with the source word type, source POS tag and the translation word type in g(vx, tx, sx, vy).
2. Filter on frequency : Extract tuples of source types and POS tags vx, tx that have been aligned to at least two target words at least 50 times ({vy : |c(vx, tx, vy) ≥ 50}| ≥ 2), to account for alignment errors. To avoid ambiguity on the target side, translations aligned to words other than the word vx in question (at least 3 times) are excluded.
3. Filter on entropy : Remove source tuples that have an entropy H(vx, tx) less than a pre-selected
5“Brown” as a verb (as in “brown the meat”) is treated differently from the adjective sense (as in “brown hair”).

threshold. The entropy is computed using the conditional probability of a target translation given the source type and POS tag:
p := p(vy|vx, tx) = c(vx, tx, vy) c(vx, tx)

H(vx, tx) =

−p loge p

vy ∈trans(vx ,tx )

where trans(vx, tx) is the set of target translations for the source tuple vx, tx and p(vy|vx, tx) is the conditional probability of the target translation for this source type vx and its POS tag tx. High entropy suggests that a word is ambiguous, with ﬁnegrained distinctions that likely require context to be resolved, and thus is a word we should focus on.

4. Filter on word sense : Remove source tuples whose target translations have distinct sourceword senses. For some words, the differences between target translations can be straightforwardly explained by the different source word senses. For example, banco in Spanish refers to the ﬁnancial institution, given by the WordNet (Miller, 1995) sense ‘bank.n.02’ while orilla refers to the edge of a river, outright matched to ‘bank.n.01’. For such words, the word sense deﬁnitions would be an easy-to-provide rule for learners, but we want to go beyond that. We are interested in ﬁnding those words where the word sense information alone is insufﬁcient to distinguish between the lexical choices and are hence likely to be hard for human learners. For a source tuple, use the highest occurring word sense for a given target translation vy computed as:

Q(vy) = arg max g(vx, tx, sx, vy)
sx∈Sx

Finally, retain the source tuples whose target translations all have the same sense, giving us L lexical choices trans(vx, tx) = {vy0, . . . , vy|L|} for a source tuple (vx, tx).

4 Lexical Selection Model
After identifying a set of focus words in the source language, we train a lexical selection model parameterized by θ vx,tx for each focus word vx, tx . We extract the parallel sentences from D that include the focus word and its corresponding lexical choices, denoting them with D vx,tx . The model takes as input the source sentences x vx,tx ∈ D vx,tx and predicts the contextually correct target

translation vy from a set of possible translations trans(vx, tx) = vy1 , vy2 , · · · , vyk
Since we aim to induce concise, humanunderstandable explanations of semantic distinctions that can be presented to learners to help them better understand the lexical selection process, we train a prediction model which allows us to easily extract such descriptions for each lexical choice vy ∈ trans(vx, tx). In this paper, we use humanreadable descriptions of the features learned by a linear model, where these features are deﬁned over a set of lexical and semantic features extracted from the source sentences in D vx,tx . For designing features, we take inspiration from prior work which uses extracted contextual information to improve cross-lingual sense disambiguation in machine translation systems (Garcia-Varea et al., 2001; Carpuat and Wu, 2007b,a).
4.1 Model Features
For training a lexical selection model θ vx,tx for the focus word vx, tx , we construct training data from the source-target sentence pairs D vx,tx . We focus on features extracted only from the current source sentence, although the framework can be easily extended to include features from the target sentence as well. We represent each source sentence x vx,tx ∈ D vx,tx with a set of features extracted from the neighborhood of the focus word context relevant to the lexical selection process. This neighborhood includes (1) words from the source sentence that occur within a ﬁxed window of the given ambiguous word, and (2) the head and dependents of the focus word as given by the dependency parse of the sentence. For each word in this relevant context, we extract the following lexical features: • Lemma Lemma of the token. • WSD Word sense of the token as extracted
from a state-of-the-art word sense disambiguation (WSD) model. • Bigram Bigrams constructed from lemmas of the words present within a ﬁxed window around the focus word. We exclude punctuation and stop words within the window.6
4.2 Model Training
To enable extraction of human-understandable descriptions, we use a model that is conducive to interpretation: the linear SVM (LinearSVM; Cortes
6Stop words as provided by NLTK(Bird and Klein, 2009)

and Vapnik, 1995), which gives us feature weights θ vx,tx that can be easily interpreted as the importance of each feature in making the decision. Since there can be n-ary lexical choices for a given focus word, we train using the one-vs-rest (OvR) method which trains one model per each lexical choice vyk , where data from vyk are treated as positive examples and data from all other choices as negative, allowing us to extract feature weights for each decision.
4.3 Rule Extraction
As mentioned above, we use human-readable descriptions of the features learned by a linear model to be presented to the human learners. More broadly, we refer to these descriptions as “rules”, however these rules could take other forms as well, and we hope that future work by us or others could ﬁnd other creative ways to induce or deﬁne these rules.
For each focus word vx, tx , we extract the rule set R vx,tx,vyk , which is the set of rules for selecting a given lexical choice vyk from the set of possible choices trans(vx, tx). For this, we extract salient features from the trained model θ vx,tx for each lexical choice. As mentioned above, using the OvR classiﬁcation method we get one model per choice vyk , from which we can then extract the top-N features having the highest weight coefﬁcients for each choice. In order to present this rules in a human-readable form, we create concise rule templates as shown in Appendix B.1.
5 Automated Validation
Since our main research goal is to aid human learners in their learning, we focus on two approaches of evaluation: (a) automated validation, a preliminary evaluation where we validate to what extent our interpretable model can perform cross-lingual lexical selection, and (b) human evaluation (§6) which answers our main question of whether it can teach human learners the usage of L2 words.
For the automated evaluation in particular, we verify several things. First, we check whether our interpretable lexical selection model is able to learn cross-lingual lexical selection at all by measuring its performance compared to selecting the most frequently occurring translation in the corpus for a given focus word (“Frequency”). We also compare with another alternative interpretable model, decision trees (DTree) trained using the same features

Figure 2: Learning Interface. Rules for the correct answer are displayed to the learner after each question. Individual rules that apply to the given example are highlighted for the convenience of the learner. “wall” here refers to an outside wall and the adjective stone serves as a hint in arriving at the correct answer.

as LinearSVM, to validate the choice of SVMs as an interpretable model over other alternatives. Further, we check how our interpretable linear SVM model compares with a “performance skyline”; a less interpretable BERT-based neural model (Devlin et al., 2019) that extracts representations of the source sentence from BERT and trains a classiﬁer to predict the correct lexical choice.
5.1 Setup
Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, we use 31 million English-Greek parallel sentences extracted from OpenSubtitles. For word alignment we use the AWESOME aligner (Dou and Neubig, 2021), for lemmatization we use spaCy (Honnibal et al., 2020), for POS tagging and dependency parsing we use Stanza (Qi et al., 2020), and for English WSD we use EWISER (Bevilacqua and Navigli, 2020).8
7We use only 1 million sentences from Europarl because we found sentences from Europarl to contain fewer semantic subdivisions owing to the very speciﬁc domain of the dataset.
8POS tagging, dependency parsing and WSD is required only for the source language, here English.

Using our automatic pipeline (§3), we identify 157 English words which have ﬁne-grained distinctions in Spanish and 707 English words for Greek. Among these, for Spanish there are 127 nouns, 15 verbs, 10 adjectives, 5 adverbs and, for Greek there are 452 nouns, 123 verbs, 126 adjectives and 6 adverbs. Along with nouns which do account for much of the data, we do ﬁnd signiﬁcant number of verbs and adjectives also exhibiting ≥ 2 lexical choices.9 A manual inspection by a Greek-English bilingual speaker revealed that most automatically created lexical choices were correct. In just a couple of cases, lemmatizer errors lead to two choices corresponding to the same actual lemma (which were manually corrected for the user studies).
Model: We train a linear SVM lexical selection model with sklearn (Pedregosa et al., 2011) for each L1 focus word and divide the extracted parallel sentences into a train/test split with a 80-20 ratio per lexical choice. We perform 5-fold crossvalidation to select the best model hyperparameters (detailed in Appendix A.2) from which we then extract the top-20 features for each lexical choice to form our rule set. Details on the setup of DTree and BERT are also in Appendix A.2.
5.2 Results
Table 1 shows the test accuracy averaged across all focus words for both Spanish and Greek. Regarding our underlying questions, we ﬁrst ﬁnd that Lin-
9More details in Appendix A

Lang. Spanish Greek

Model
Frequency (Baseline) DTree LinearSVM BERT
Baseline DTree LinearSVM BERT

All
59.43 62.40 66.87 70.72
58.56 63.79 66.46 71.74

Test Accuracy nouns verbs adj.

59.36 62.45 67.41 71.75

60.17 61.57 65.34 69.04

60.67 65.22 66.91 67.31

59.48 64.49 67.09 70.91

53.04 59.74 63.30 78.14

60.48 65.39 67.51 68.86

adv.
53.03 54.82 56.29 54.07
61.82 61.13 64.98 62.76

Table 1: The interpretable LinearSVM lexical selection model is almost on par with the BERT skyline.

earSVM signiﬁcantly outperforms both Frequency and DTree by a signiﬁcant margin, indicating that it is both learning to perform lexical selection to a signiﬁcant degree, and outperforming other reasonable alternatives for interpretable models.10 This gives us conﬁdence to proceed to use it in our following human learning experiments. Interestingly, our interpretable LinearSVM model is within 97% relative accuracy of the skyline BERT model (just 2.09 percentage points behind). The fact that the more complicated but less inherently interpretable BERT model is better overall paves the way for future work in applying model interpretation techniques (Abnar and Zuidema, 2020, inter alia) to extract human-interpretable rules for lexical selection, although this is beyond the scope of the current paper.11 We ﬁnd that lexical selection accuracy varies by part of speech; all models perform poorly on adverbs with (avg.) gain of only +0.97 points over the baseline (c.f. with gains of +8.04 for nouns, +5.16 for verbs, +6.24 for adjectives).
6 Evaluation with Human Learners
We move to our main evaluation where we examine how effective our extracted rules are in aiding human learners in understanding the distinctions in L2 words.
6.1 Evaluation Methodology
We take inspiration from existing research on second language acquisition (SLA) to design our evaluation method. For instance, Groot (2000) highlights the different learning strategies which are based on generally accepted language acquisition theories (Nation, 2005; Richards et al., 1999), which suggest that a learner is required to go through different levels of language processing for
10Individual scores per focus word listed in Appendix A.2 11Overall accuracy is low, with even BERT getting 70%, possibly due to lack of sufﬁcient source-side context. OpenSubtitles comprises of movie dialogues where the sufﬁcient context could span more than a single sentence.

effectively learning vocabulary. In particular, Groot (2000) empirically show that some of these levels can be accelerated with appropriate design of the language tasks by combining learning strategies which use both examples in context and deﬁnitions for effective learning. Our cloze-style tasks are essentially examples in context showing the word usage in a given context and the extracted rules are a proxy for human-provided deﬁnitions.
Speciﬁcally, we set up an interactive exercise where a human learner is presented with the English focus word in context, along with a set of possible L2 (Spanish or Greek) lexical choices. The learner is then required to select one of the possible lexical choices, based on which they think correctly translates the focus word in the given source context. They must also mark how conﬁdent they are in their answer (“Not at all”, “Slightly”, “Somewhat”, “Quite” or “Very”). After they select the answer to each question, they are told the correct answer immediately. For each focus word, we ask the learner to answer up to N multiple-choice questions in sequence, which contain roughly equal number of questions for each lexical choice.
In order to evaluate how effective the extracted rules are in aiding the learning process, we perform this study in two setups, a baseline one without rules, and one using our proposed system with rules.
Baseline Setup: In this setup, the human learner does not have access to any rules and immediately starts answering questions. If the learners do not know the target language, they are likely to start out with approximately chance accuracy (e.g. 50% if there are two choices), but as they are given feedback they may be able to grasp the patterns under which one particular translation or another is used, and gradually rise above chance accuracy.
Proposed Setup: In the proposed setup, before starting the task, the learner is shown brief rules regarding when you would use each possible lexical choice vyk ∈ trans(vx, tx), constructed from the rule set R vx,tx,vyk . They take as much time as they want to review these rules, and then move to answering questions. The interface for answering questions is the same as the baseline, but below the task screen they can review the rules of different translation choices (ﬁgures in Appendix B.2). On selecting a choice, the learner is shown the correct answer accompanied with its corresponding

human-readable rules of only the correct answer. Further, we highlight those individual rules that helped decide the correct answer (Figure 2) for the convenience of the learner. By highlighting it in the two bottom panes, we hope to draw the learner’s attention to these hints and thus strengthen the understanding of the underlying concept.
In this setting, the annotator may achieve nonchance accuracy even at the very beginning of answering questions, as they have been given an explanation regarding the underlying rules that they can leverage in answering questions. The accuracy will likely further increase as they practice and become familiar with actual examples and how the extracted features apply to them.
6.2 Experimental Details
We select native English speakers, 7 for the Spanish study and 9 for the Greek study.12 Each annotator is presented with the same set of English words or tasks. For each study, half of the words will be annotated using the baseline setup and remaining half with the proposed setup. To ensure an unbiased setup, we randomize whether each focus word uses rules or not, while ensuring that at least half the annotators see the proposed setup and the other half perform the same task in the baseline setup for each word. We further shufﬂe the order in which the words are presented. For each English word, we select up to 40 examples each for the respective lexical choices. However, as an incentive, we end a task early if the annotator correctly answers 10 questions straight in a row for each lexical choice. We explain below the selection procedure for the English words used in the experiments.
Word Selection: In an ideal situation, we would like to conduct these experiments for all identiﬁed English focus words, but this would involve annotating thousands of sentences, requiring a large time commitment from the annotators. Instead, we shortlist a handful of words using the following automated procedure: First, for a given L2 study, we sort all focus words by the number of available data points (D vx,tx ). Next, from the trained lexical selection model θ vx,tx we compute an F1-score for each lexical choice and ﬁlter focus words where the model gets an F1 > 0.5 for each lexical choice. Finally, we select upto 10 focus words with the most data points that ﬁt the above condition. For each
12We allow participants who know other languages but none that are familiar with the L2 or its related languages.

word ( vx, tx ), we then select 40 representative examples for each lexical choice (see paragraph below). Details on the shortlisted words can be found in Appendix B.3.
Representative Example Selection: To facilitate an effective learning process, we present examples to the learner that have sufﬁcient sourceside context required for correctly identifying the target-side lexical choice. This is important because there are examples in the corpus where the sufﬁcient context requires context spanning over multiple sentences. To make our learning content both concise and effective, we focus only on context self-contained in a single sentence. Further to efﬁciently conduct a high-quality study, we enlist help from native speakers of the L2 language to ﬁlter the required sentences. We note, though, that the relevant sentences could also be potentially ﬁltered automatically (left for future work).
To get such meaningful examples, we present bilingual English-Spanish and English-Greek speakers with the English sentence containing the focus word and the set of possible lexical choices in Spanish and Greek respectively. They then select the word which best suits the given context and mark their conﬁdence in the selection. The interface for the example selection is the same as Figure 2 (but without rules). We collect these annotations from multiple native speakers and only keep those sentences on which all native speakers agree (see Appendix B.3 for details).
6.3 Results and Discussion
To conﬁrm whether the extracted rules are effective to the learning process, we examine the following questions:
Do the extracted rules result in increased learner accuracy? We compute the learner accuracy across all learners for each L2 study. If a learner attains higher accuracy with fewer attempted examples for the experiment with rules than without, then the extracted rules could be considered effective in the learning process. However, we cannot directly use the learner accuracy as-is because of the possibility of other sources of variability such as (a) underlying learner ability, as some learners may be more proﬁcient than others, (b) underlying task difﬁculty, as some words may be harder to disambiguate than others, or (c) word ordering, as learners may become proﬁcient as they do more tasks. Therefore, we use a mixed

Accuracy

Spanish

(Learner Accuracy)

1

0.8

0.6

0.4

0.2

0

510 20 30 40 50

80

Spanish (Learner Conﬁdence) 5

4

3

2

1

510 20 30 40 50

80

Accuracy

Greek (Learner Accuracy) 1 0.8 0.6 0.4 0.2 0 510 20 30 40 50

with rules

without rules

Greek

(Learner Conﬁdence)

5

4

3

2

1

120 510 20 30 40 50

120

Figure 3: Learner accuracy and conﬁdence in correct answers with and without access to rules against the number of attempted examples (x-axis ). Learners achieve higher accuracy with increasing conﬁdence with fewer examples when they have access to rules.

effects model (McLean et al., 1991), which models random effects and ﬁxed effects to account for such random variability. Random effects are variables responsible for random variation such as task-identity, task-order and the learner, while ﬁxed effects such as the presence of rules are the variables of interest for determining the response variable i.e. learner accuracy. A linear mixed-effect model (LME) is deﬁned as: y = Xβ + Zu + where y is the learner accuracy, β and u are the ﬁxed-effect and random-effect regression coefﬁcients, X and Z are the respective design matrices and the noise.
We ﬁt LME models on our data by varying the number of ﬁrst n attempted examples n = [5, 10, 20, 30, 40, 50, all]. Each ﬁtted LME model gives us an intercept which informs us of the learner accuracy in absence of rules, and the ﬁxed-effect coefﬁcient β which informs us about the gain with rules. As shown in Figure 3, it is clear that learners having access to our automatically extracted rules achieve higher accuracy with fewer examples as compared to without. As expected, with an increasing number of attempted examples the gap in accuracy between the two settings reduces. Interestingly, we ﬁnd that the rules still have a signiﬁcant effect on the learner’s conﬁdence even later in the learning process. This suggests that with our rules learners require fewer examples to infer the patterns governing each lexical choice and further get more conﬁdent in their understanding. This is encouraging as in true settings the learning exercise would be conducted for every focus word that the learner is attempting to learn, and because this process will have to be repeated many times, making it more efﬁcient is of signiﬁcant value. In

Rule Effect (β)

Rule Effect (β)

oil Spanish

0.3
0.2 farmer wall
0.1

vote
language pitlilcket

0

0.6

0.8

Greek

0.3

tour

break turn

0.2 wheel

bone

0.1

rooolfd effect

bill tie

0

0.6

0.8

Figure 4: Rules help more for words where learners do worse. x-axis is the (avg.) learner accuracy (without rules) for ﬁrst 20 examples.

0.1 0 pill
−0.1
0.6

Spanish

oil

vote

wafvaermer ticket 0.1

ﬁgure

turn

language

0 roof

wall

−0.1

0.7

0.8

0.9

0.4

Greek tour

tie

bone brwehakeel

bill effect

old

0.6

0.8

Figure 5: Rules help more for words where the model performs well. x-axis is model accuracy per word.

Appendix B.4 we report the p-value for the ﬁtted LME models which shows that the positive gains from the presence of rules are most signiﬁcant for ≤20 examples for Spanish and for all examples for Greek.
Overall, we ﬁnd our extracted rules help both Spanish and Greek learners in their learning process. We note that the results on Greek are promising as it does not enjoy the same luxuries as Spanish in having a high-quality lemmatizer or word aligner. This is encouraging especially for researchers involved in the revival efforts of endangered languages.

Do the extracted rules result in increased learner conﬁdence? While answering the questions we ask the learner to mark how conﬁdent they are in their answer. As before, we ﬁt LME models for each n using annotator conﬁdence as the response variable Y and presence of rules as the ﬁxed-effect. We ﬁnd that the learners’ conﬁdence in the correct answer increases more when they are provided rules (Figure 3) for both the languages.

Do the extracted rules help some words more over others? Since the focus words may vary in the difﬁculty level, we check if our extracted rules are more effective for some words over others. So, we ﬁt a LME model on each focus word and compute the β coefﬁcient to measure the effect of rules on learner accuracy after 20 attempted examples.13 We plot the β coefﬁcient with the accuracy (averaged across all learners) for each focus word when they didn’t have rules in Figure 4 and ﬁnd that words on which the learners performed the worst
13Because analysis revealed that rules are more effective earlier in the learning process.

such as wall, oil, farmer, and vote for Spanish, beneﬁt most by our rules. Similar observations can be seen for Greek where learners are beneﬁted more for words (break, wheel, tour, old, roof ) on which they performed the worst. Some of these words, in fact, indeed have ﬁner semantic subdivisions than the rest. For instance, the choices for farmer: agricultor refers to exclusively the one who works the land, harvests, sows, etc., whereas granjero is less formal referring to the one who manages a farm, or works or lives on it.14 This analysis shows that, encouragingly, our rules are especially helping learners with more difﬁcult words.
We also plot the β coefﬁcient with the lexical model accuracy (Figure 5) and ﬁnd a positive correlation, meaning that rules help more for words where the model performs well. This suggests that if we can develop more accurate models with an equal level of interpretability, the learning effect might become even stronger.
7 Related Work
Computer-assisted language learning CALL systems have been increasingly using NLP for creating learning content. Both SMILLE (Zilio et al., 2017) and WERTi (Meurers et al., 2010) aim to help the text understanding process by highlighting linguistic structures using hand-written rules and automatically acquired syntactic analysis. Apertium (Tyers et al., 2012), a rule-based MT system, while not aimed at language learning, does use human- and machine-readable rules, whose formalism can account for only ﬁxed-length ordered contexts restricting their application. Further, these rules use a combination of only lemma and POS tags while our framework uses more features.
Cross-lingual word sense disambiguation CLWSD disambiguates a word in-context by providing appropriate translation across languages. Lefever and Hoste (2010) construct a dataset (25 ambiguous English nouns across six languages) semi-automatically from parallel corpora which are then veriﬁed by expert translators. Such lexical choice tasks have been created also for evaluating MT systems (Rios Gonzales et al., 2017; Rios et al., 2018). However, these methods cover a limited set of words (mostly nouns) and require some manual intervention during the data creation process. To the best of our knowledge, our proposed pipeline is
14This is based on explanations collected from native Spanish speakers, which can also be found in Appendix B.4

the only fully automated one that extracts several ambiguous words across multiple POS tags.
8 Future Work
While we have demonstrated the efﬁcacy of our extracted rules in teaching new words for two languages, we plan to apply our framework on much less-resourced languages which have fewer available learning resources where learners would beneﬁt more from an automated system. We also plan to use automated methods such as selection using model conﬁdence to select ‘representative’ examples for the learning setup instead of using the native speakers. Furthermore, multimodal features have proven their utility in automatic methods for lexical acquisition (Hewitt et al., 2018), and we plan to examine their effectiveness for L2 learning.
Acknowledgements
The authors are grateful to the anonymous reviewers who took the time to provide many interesting comments that made the paper signiﬁcantly better. We would also like to thank Nikolai Vogler for the original interface for data annotation, and all the learners for their participation in our study, and without whom this study would not have been possible or meaningful. This work is sponsored by the Waibel Presidential Fellowship and by the National Science Foundation under grants 1761548 and 2125466.
References
Samira Abnar and Willem Zuidema. 2020. Quantifying attention ﬂow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics.
Michele Bevilacqua and Roberto Navigli. 2020. Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2854–2864, Online. Association for Computational Linguistics.
Edward Loper Bird, Steven and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.
Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. Classiﬁcation and regression trees. CRC press.

Marine Carpuat and Dekai Wu. 2007a. How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation. Proceedings of TMI, pages 43–52.
Marine Carpuat and Dekai Wu. 2007b. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72.
Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT.
Zi-Yi Dou and Graham Neubig. 2021. Word alignment by ﬁne-tuning embeddings on parallel corpora. arXiv preprint arXiv:2101.08231.
Nick C Ellis. 1996. Sequencing in sla: Phonological memory, chunking, and points of order. Studies in second language acquisition, pages 91–126.
Ismael Garcia-Varea, Franz Josef Och, Hermann Ney, and Francisco Casacuberta. 2001. Reﬁned lexicon models for statistical machine translation using a maximum entropy approach. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 204–211.
Peter JM Groot. 2000. Computer assisted second language vocabulary acquisition. Language learning & technology, 4(1):56–76.
John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz, Derry Tanti Wijaya, and Chris Callison-Burch. 2018. Learning translations via images with a massively multilingual image dataset. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2566–2576, Melbourne, Australia. Association for Computational Linguistics.
Matthew Honnibal, Ines Montani, Soﬁe Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python. Zenodo.
Jan H Hulstijn, Merel Hollander, and Tine Greidanus. 1996. Incidental vocabulary learning by advanced foreign language students: The inﬂuence of marginal glosses, dictionary use, and reoccurrence of unknown words. The modern language journal, 80(3):327–339.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.

Els Lefever and Veronique Hoste. 2010. SemEval2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 15–20, Uppsala, Sweden. Association for Computational Linguistics.
Els Lefever and Véronique Hoste. 2013. SemEval2013 task 10: Cross-lingual word sense disambiguation. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 158– 166, Atlanta, Georgia, USA. Association for Computational Linguistics.
Pierre Lison and Jörg Tiedemann. 2016. OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 923–929, Portorož, Slovenia. European Language Resources Association (ELRA).
Robert A McLean, William L Sanders, and Walter W Stroup. 1991. A uniﬁed approach to mixed linear models. The American Statistician, 45(1):54–64.
Detmar Meurers, Ramon Ziai, Luiz Amaral, Adriane Boyd, Aleksandar Dimitrov, Vanessa Metcalf, and Niels Ott. 2010. Enhancing authentic web pages for language learners. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10–18, Los Angeles, California. Association for Computational Linguistics.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.
Emily Ariel Moline. 2020. Indigenous language teaching policy in california/the us: What’s left unsaid in discourse/funding. Issues in Applied Linguistics, 21(1).
Zena Moore. 1996. Foreign language teacher education: Multiple perspectives. University Press of America.
ISP Nation. 2005. Teaching and learning vocabulary. In Handbook of research in second language teaching and learning, pages 605–620. Routledge.
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830.
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational

Linguistics: System Demonstrations, pages 101– 108, Online. Association for Computational Linguistics.
J. Ross Quinlan. 1986. Induction of decision trees. Machine learning, 1(1):81–106.
Jack C Richards, David Singleton, and Michael H Long. 1999. Exploring the second language mental lexicon. Cambridge University Press.
Annette Rios, Mathias Müller, and Rico Sennrich. 2018. The word sense disambiguation test suite at WMT18. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 588–596, Belgium, Brussels. Association for Computational Linguistics.
Annette Rios Gonzales, Laura Mascarell, and Rico Sennrich. 2017. Improving word sense disambiguation in neural machine translation with sense embeddings. In Proceedings of the Second Conference on Machine Translation, pages 11–19, Copenhagen, Denmark. Association for Computational Linguistics.
Frankie Robertson. 2020. Show, don’t tell: Visualising Finnish word formation in a browser-based reading assistant. In Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning, pages 37–45, Gothenburg, Sweden. LiU Electronic Press.
Wilson L Taylor. 1953. “cloze procedure”: A new tool for measuring readability. Journalism quarterly, 30(4):415–433.
Jörg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).
Francis M. Tyers, Felipe Sánchez-Martínez, and Mikel L. Forcada. 2012. Flexible ﬁnite-state lexical selection for rule-based machine translation. In Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 213–220, Trento, Italy. European Association for Machine Translation.
Yuichi Watanabe. 1997. Input, intake, and retention: Effects of increased processing on incidental learning of foreign language vocabulary. Studies in second language acquisition, pages 287–307.
Leonardo Zilio, Rodrigo Wilkens, and Cédrick Fairon. 2017. Using NLP for enhancing second language acquisition. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 839–846, Varna, Bulgaria. INCOMA Ltd.

A Automated Evaluation
A.1 Identifying Semantic Subdivisions
In Section 3, we describe the procedure for identifying focus words in L1. For the step of ﬁlter on entropy within that procedure, we use a threshold of 0.69 so focus words having an entorpy H > 0.69 are selected in that step. For binary lexical choices, an ambiguous word would be aligned to each choice with uniform distribution and the entropy in that case would be 0.69. Hence we are interested in words that exceed this minimum threshold. In Figure 6 we show the distribution of number of lexical choices for all extracted focus words, ﬁltered by each POS tag for Spanish and Greek. We check the CEFR levels15 which measure the reading proﬁciency in a language. We use the automated tool provided by Duolingo16 (currently available only for Spanish and English) to get these levels and ﬁnd that 60% of the extracted Spanish lexical choices belong to the B level which is the intermediate level and 20% belong to the advanced level. This suggests that the identiﬁed words are indeed more challenging.
A.2 Model Hyperparameters and Results
For the LinearSVM and DTree models, we clean the data to remove punctuation and extract features within a 3-word window of the focus word. As mentioned before, we train a lexical selection model for each focus word and in Table 5, 6, 7, 8 we report the individual accuracy for the test accuracy for LinearSVM, DTree, BERT and the baseline method across both Spanish and Greek. We also provide the train accuracy for our main model, LinearSVM.
LinearSVM We perform a grid search over the following hyperparameters: C = [0.001, 0.01], class weight =[’balanced’, None].
DTree We also experimented with other interpretable models such as decision trees (Quinlan, 1986) using the CART algorithm (Breiman et al., 1984), however we found them to be performing worse than the SVM model. We used the following hyperparameters: criterion = [gini, entropy], max depth = [6,15], min impurity decrease = 1e−3.
15https://en.wikipedia.org/wiki/Common_ European_Framework_of_Reference_for_ Languages
16https://cefr.duolingo.com/

BERT We compare the interpretable models LinearSVM and DTree with more complex neural model based on the popular BERT (Devlin et al., 2019). We retain the same hyperparameters as the original paper using 768 dimentions for the encoder representations. We train the model for 20 epochs, using the AdamW optimzer with a learning rate of 5e − 5.
B Human Evaluation
B.1 Rule Templates
The human-understandable “rules” are essentially those features from the training set which the model thought were important for determining the correct label (i.e. features that were given higher weights for a given label). In particular, for each label (i.e. muro or pared), we choose the top-20 features. We then group these individual features together by their feature types, for instance, all the bigram features are grouped under the category called “Short Phrase”, lemma features are grouped under the category “Words”, and the WSD features are ﬁrst expanded into their natural language form using the WordNet (Miller, 1995) knowledge base and then grouped under the category “Concepts”, as shown in Table 2.
B.2 Language Learning Interface
In our proposed language learning setup, the learner is ﬁrst presented with a screen showing concise explanations for each lexical choice (Figure 7(a)). They can take as much time as they require for reviewing the rules and then proceed to the tasks. Within each task, the learner is then shown an English sentence with the focus word highlighted and a set of possible lexical choices. The page also displays the concise explanations for the learner to refer if they wish to (Figure 7(b)). The learner is required to select one of the lexical choices and mark how conﬁdent they are in their answer. Once submitted, the learner is immediately shown the correct answer along with individual rules that applied to that example highlighted (Figure 2 in main text). Learners took (avg.) 3-4 hours in total to complete all tasks. Table 3 presents the tasks performed by the respective Spanish and Greek learners. Since English speakers might not be familiar with the Greek alphabet, we display the English transliteration of the respective Greek words. Some of the lexical choices (e.g. muralla/muro/muros) contain multiple inﬂec-

Number of Words Number of Words

120 100 80 60 40 20
0
NOUN

>3 choices

3 choices 2 choices

400

300

>3 choices 3 choices 2 choices

200

100

VERB

ADJ

ADV

(a) Spanish

0
NOUN

VERB

ADJ

ADV

(b) Greek

Figure 6: Distribution of number of lexical choices for each POS tag.

Lexical Choice muro
pared

feature −→ rule name feature value
Bigram −→ Short phrases: (’climb’, ’wall’), (’city’, ’wall’), (’brick’, ’wall’) Lemma −→ Words: break, climb WSD −→ Concepts: ‘city’ as in a large and densely populated urban area (city.n.01)
Bigram −→ Short phrases: (’face’, ’wall’), (’hang’, ’wall’), (’picture’, ’wall’) Lemma −→ Words: ear, hang, room

Table 2: Human-readable rules extracted for the ambiguous word wall (top-6 rules per lexical choice).

tions of the same lemma (muro). This is due to errors in the automatic Spanish lemmatizer which failed to correctly map the inﬂections to a single lemma. We therefore run an edit-distance based post-processing to combine lexical choices having the same preﬁx. We note that this simple heuristic might not be ideal for languages such as Indonesian that use afﬁxes and/or reduplication with far-from-perfect lemmatizers; nevertheless such a post-processing method, when applied carefully, helps ﬁx many of the erroneous lemmatization issues.
B.3 Representative Example Selection
The shortlisted words for both the Spanish and Greek study can be found in Table 3. We use native speakers to ﬁlter sentences that have sufﬁcient context for correctly identifying a lexical choice. We enlist 3 Spanish native speakers who each annotate roughly 200 examples each for 10 English focus words. The inter-annotator agreement for Spanish, computed using Fleiss’ kappa is 0.77. For Greek, we use 2 native speakers to annotate 10 English words. For 7 out of 10 words we did not always

have access to 2 native speakers so we relied on a single expert annotator. The (avg.) inter-annotator agreement for the remaining 3 words (tour, tie, bill) between the two annotators is 0.83. Of the 10 selected words, we discard words/lexical choices which have < 10 examples on which all native speakers agree (Table 3) giving us 9 English words for the Spanish study and 10 English for the Greek study.
B.4 Results
In Table 4 we report the p-values for the linear mixed effect (LME) models ﬁtted on predicting learner accuracy with rules as ﬁxed-effect. The results show that the positive effect of rules on accuracy is statistical signiﬁcant up to ﬁrst 20 attempted examples for Spanish and up to all examples for Greek.

(a) Rules for “pared” vs “muro”.
(b) Rules for the correct answer are displayed to the learner after each question. Individual rules which apply to the given example are highlighted for the convenience of the learner.
Figure 7: User interface for human language learning experiment.

(en) word wall.N farmer.N ﬁgure.N vote.N oil.N wave.N pill.N
language.N ticket.N servant.N

Spanish (es) lexical choices
muralla/muro/muros: 33, pared/paredón: 60 agricultor: 29, granjero: 48 cifra/cifras: 87, ﬁgura: 85
votemos/voto: 77, votación: 75 aceite: 81, óleo/petróleo/petrolera/petrolero: 74
onda: 55, ola: 40, oleado: 0 pastilla: 41, somnífero: 27, píldora: 3
idioma: 52, lenguaje: 68 multa: 24, boleto: 23, pasaje: 0 sirvienta/sirviente: 39, servidor/servidora: 8, siervo/siervos: 10

(en) word bill.N tour.N
break.JJ turn.JJ roof.N wheel.N old.JJ turn.JJ effect.N bone.N

Greek (el) lexical choices
χαρτονόμισμα: 40, λογαριασμός: 40, νόμος/νομοσχέδιο: 40 (chartonómisma, logariasmós, nómos/nomoschédio)
θητεία:23, περιοδεία: 29, ξενάγηση: 33 (thitía, periodeía, xenágisi)
σπάω: 40, ράγομαι: 40, ξεσπάω: 40, διαρρηγνύω: 40 (spáo, rágomai, ksespáo, diarrignío)
στρίβω: 40, χαμηλώνω: 40, απορρίπτω: 40, καταδίδω: 40, σβήνω: 34 (strívo,chamilòno, aporrípto,katadído, svíno)
ταράτσα: 40,οροφή: 40, στέγη: 39 (tarátsa, orofí, stégi)
τροχός: 40, ρόδα: 40, τιμόν: 40 (trohós, róda, timóni)
αρχαίος: 40, κλασικ: 21, έτος: 40, ηλικιωμένος: 40, παραδοσιακός: 36 (archaios, klasikos, etos, elikiomenos,paradosiakos)
στρίβω: 40, χαμηλώνω: 40, απορρίπτω: 40, καταδίδω: 40, σβήνω: 34 (strívo, chamilóno, aporrípto, katadído,svíno)
παρενέργεια: 40, επίδραση: 40, εφέ: 40 (parenírgeia, epídrasi, efé)
μυελός: 40, οστό: 40, Μπόουν: 40 (myelós, ostó, bone)

Table 3: Example tasks with their lexical choices selected for Spanish and Greek learning setup. Words/choices marked in red are discarded from the language learning setup as they have ≤ 10 ﬁltered examples from the represenative example selection step.

Number
5 10 20 30 40 50
All

Fixed-effect coefﬁcient (β)
0.118 0.112 0.056 0.039 0.017 0.007
0.006

Spanish p-value
0.013** 0.009***
0.070* 0.131 0.462 0.718
0.739

Greek p-value
4.50e−09*** 1.64e−07*** 1.32e−06*** 4.23e−05*** 7.22e−05*** 0.00015***
0.00173**

Table 4: p-value tests show that the ﬁxed-effect of presence of rules for predicting learner accuracy is statistical signiﬁcant up to ﬁrst 20 attempted examples for Spanish and up to all examples for Greek. Signiﬁcance codes: ‘***’: 0.01, ‘**’: 0.05, ‘*’: 0.1.

Focus word
speciﬁcally.RB transfer.N pen.N fry.V cord.N ﬁgure.N poker.N plumber.N pee.V puppet.N bowl.N appeal.N fan.N mob.N properly.RB hobby.N stew.N bait.N trunk.N port.N shell.N cap.N bra.N park.V drunk.JJ vegetable.N opening.N rule.V riﬂe.N herd.N language.N parking.N approach.N bracelet.N horn.N razor.N computer.N ﬂock.N cliff.N prayer.N promotion.N vote.N record.N hunch.N skull.N essentially.RB requirement.N pneumonia.N greed.N dagger.N editor.N maid.N temper.N

FreqBaseline - DTree - LinearSVM Train/Test - BERT Focus word

0.67 - 0.7 - 0.67 / 0.67 - 0.72 0.64 - 0.71 - 0.92 / 0.69 - 0.72 0.73 - 0.74 - 0.73 / 0.73 - 0.73 0.57 - 0.71 - 0.84 / 0.69 - 0.88
0.52 - 1.0 - 0.98 / 1.0 - 1.0 0.53 - 0.55 - 0.93 / 0.76 - 0.93 0.62 - 0.62 - 0.62 / 0.62 - 0.54 0.57 - 0.57 - 0.57 / 0.57 - 0.65 0.61 - 0.61 - 0.62 / 0.61 - 0.58 0.53 - 0.53 - 0.9 / 0.51 - 0.46 0.6 - 0.6 - 0.96 / 0.72 - 0.72 0.75 - 0.8 - 0.95 / 0.82 - 0.93 0.31 - 0.3 - 0.89 / 0.43 - 0.59 0.7 - 0.74 - 0.98 / 0.74 - 0.93 0.42 - 0.42 - 0.42 / 0.42 - 0.41 0.58 - 0.58 - 0.68 / 0.58 - 0.63 0.59 - 0.41 - 0.6 / 0.59 - 0.62 0.54 - 0.62 - 0.58 / 0.59 - 0.5 0.74 - 0.76 - 0.77 / 0.74 - 0.81 0.41 - 0.61 - 0.98 / 0.79 - 0.96 0.41 - 0.41 - 0.95 / 0.59 - 0.55 0.89 - 0.91 - 1.0 / 0.91 - 0.99 0.52 - 0.5 - 0.9 / 0.48 - 0.45 0.53 - 0.54 - 0.86 / 0.54 - 0.58 0.56 - 0.69 - 0.77 / 0.68 - 0.76 0.59 - 0.61 - 0.74 / 0.61 - 0.71 0.55 - 0.6 - 0.85 / 0.59 - 0.7 0.68 - 0.7 - 0.87 / 0.72 - 0.88 0.7 - 0.71 - 0.96 / 0.78 - 0.85 0.51 - 0.51 - 0.93 / 0.67 - 0.62 0.59 - 0.65 - 0.89 / 0.75 - 0.81 0.68 - 0.65 - 0.67 / 0.68 - 0.65 0.63 - 0.64 - 0.9 / 0.62 - 0.62 0.54 - 0.53 - 0.88 / 0.55 - 0.65 0.57 - 0.49 - 0.89 / 0.64 - 0.74 0.69 - 0.71 - 0.89 / 0.76 - 0.68 0.65 - 0.65 - 0.65 / 0.65 - 0.61 0.61 - 0.7 - 0.95 / 0.8 - 0.91 0.69 - 0.31 - 0.7 / 0.69 - 0.74 0.62 - 0.7 - 0.75 / 0.69 - 0.63 0.56 - 0.57 - 0.89 / 0.6 - 0.68 0.53 - 0.76 - 0.87 / 0.83 - 0.92 0.34 - 0.41 - 0.92 / 0.59 - 0.79 0.65 - 0.65 - 0.67 / 0.65 - 0.6 0.81 - 0.83 - 0.98 / 0.82 - 0.88 0.74 - 0.74 - 0.75 / 0.74 - 0.67
0.7 - 0.7 - 0.72 / 0.7 - 0.62 0.64 - 0.36 - 0.65 / 0.64 - 0.61 0.57 - 0.57 - 0.98 / 0.51 - 0.49 0.67 - 0.67 - 0.95 / 0.7 - 0.68 0.54 - 0.63 - 0.9 / 0.69 - 0.86 0.36 - 0.54 - 0.67 / 0.56 - 0.56 0.29 - 0.5 - 0.55 / 0.48 - 0.49

block.N slipper.N foundation.N bug.N waste.N hood.N heel.N replacement.N greedy.JJ basket.N dump.N promote.V disappoint.V romance.N jungle.N pupil.N farmer.N eve.N oil.N lock.N servant.N teddy.N lump.N comfort.N barn.N peanut.N cabbage.N sandwich.N link.N supply.N privacy.N alien.JJ pit.N gossip.N dick.N cleaner.N survivor.N dutch.JJ bite.N match.N retire.V honesty.N twenty.N praise.N wall.N mud.N driver.N relevant.JJ pill.N riddle.N rude.JJ calf.N ticket.N

Overall Average:

FreqBaseline - DTree - LinearSVM Train/Test - BERT
0.58 - 0.69 - 0.92 / 0.79 - 0.86 0.7 - 0.7 - 0.7 / 0.7 - 0.63
0.43 - 0.5 - 0.96 / 0.64 - 0.69 0.57 - 0.6 - 0.82 / 0.6 - 0.48 0.48 - 0.53 - 0.87 / 0.74 - 0.8 0.55 - 0.61 - 0.94 / 0.74 - 0.92 0.53 - 0.72 - 0.9 / 0.71 - 0.85 0.6 - 0.6 - 0.61 / 0.6 - 0.67 0.68 - 0.68 - 0.68 / 0.68 - 0.75 0.66 - 0.66 - 0.69 / 0.66 - 0.66 0.61 - 0.64 - 0.9 / 0.64 - 0.67 0.74 - 0.73 - 0.74 / 0.74 - 0.64 0.68 - 0.68 - 0.68 / 0.68 - 0.74 0.58 - 0.64 - 0.98 / 0.73 - 0.79 0.55 - 0.55 - 0.77 / 0.56 - 0.53 0.53 - 0.56 - 0.96 / 0.67 - 0.79 0.66 - 0.7 - 0.93 / 0.77 - 0.84 0.96 - 0.96 - 0.99 / 0.96 - 0.96 0.55 - 0.63 - 0.95 / 0.85 - 0.89 0.8 - 0.85 - 0.91 / 0.85 - 0.89 0.64 - 0.7 - 0.78 / 0.71 - 0.73 0.52 - 0.45 - 0.91 / 0.48 - 0.64 0.52 - 0.87 - 0.97 / 0.91 - 0.87 0.78 - 0.78 - 0.78 / 0.78 - 0.77 0.75 - 0.75 - 0.78 / 0.78 - 0.79 0.51 - 0.57 - 0.8 / 0.6 - 0.59 0.63 - 0.63 - 0.7 / 0.65 - 0.74 0.62 - 0.62 - 0.61 / 0.62 - 0.6 0.5 - 0.78 - 0.96 / 0.83 - 0.94 0.59 - 0.59 - 0.85 / 0.61 - 0.65 0.56 - 0.58 - 0.9 / 0.6 - 0.66 0.64 - 0.64 - 0.96 / 0.64 - 0.6 0.63 - 0.63 - 0.64 / 0.63 - 0.63 0.55 - 0.55 - 0.81 / 0.57 - 0.65 0.56 - 0.56 - 0.54 / 0.56 - 0.56 0.56 - 0.98 - 1.0 / 0.98 - 0.98 0.55 - 0.56 - 0.81 / 0.59 - 0.47 0.69 - 0.81 - 0.98 / 0.83 - 0.86 0.57 - 0.66 - 0.85 / 0.64 - 0.77 0.52 - 0.52 - 0.52 / 0.52 - 0.56 0.52 - 0.52 - 0.88 / 0.64 - 0.69 0.66 - 0.66 - 0.66 / 0.66 - 0.59 0.74 - 0.76 - 0.77 / 0.74 - 0.93 0.71 - 0.68 - 0.97 / 0.76 - 0.83 0.66 - 0.69 - 0.86 / 0.69 - 0.75 0.61 - 0.61 - 0.6 / 0.61 - 0.5 0.63 - 0.68 - 0.68 / 0.68 - 0.69 0.66 - 0.66 - 0.98 / 0.68 - 0.61 0.46 - 0.49 - 0.72 / 0.52 - 0.59 0.71 - 0.29 - 0.71 / 0.71 - 0.7 0.76 - 0.76 - 0.75 / 0.76 - 0.7 0.64 - 0.66 - 0.96 / 0.67 - 0.6 0.58 - 0.64 - 0.77 / 0.67 - 0.66
59.43 - 62.40 - 66.87 - 70.72

Focus word
desert.V mushroom.N mercy.N toast.N opponent.N steak.N plot.N thick.JJ barber.N marble.N lipstick.N brush.N persuade.V raincoat.N nail.N regularly.RB pipe.N transitional.JJ ancestor.N cripple.N bean.N scale.N shuttle.N fuse.N radioactive.JJ pretend.V custom.N log.V riot.N sweater.N intrusion.N ﬁghter.N cover.N transfer.V reﬂection.N speaker.N condolence.N ounce.N spread.V twenty.JJ tribute.N jar.N advance.N jean.N restore.V alien.N chin.N wave.N unfortunately.RB encourage.V belly.N

FreqBaseline - DTree - LinearSVM Train/Test - BERT
0.53 - 0.55 - 0.96 / 0.73 - 0.95 0.42 - 0.39 - 0.91 / 0.44 - 0.49 0.58 - 0.67 - 0.83 / 0.72 - 0.77 0.64 - 0.72 - 0.96 / 0.87 - 0.94 0.66 - 0.66 - 0.67 / 0.66 - 0.59 0.66 - 0.34 - 0.66 / 0.66 - 0.61 0.6 - 0.6 - 0.99 / 0.76 - 0.88 0.59 - 0.73 - 0.98 / 0.73 - 0.69 0.75 - 0.69 - 0.76 / 0.75 - 0.74 0.51 - 0.71 - 0.98 / 0.8 - 0.93 0.51 - 0.49 - 0.93 / 0.4 - 0.53 0.55 - 0.58 - 0.96 / 0.66 - 0.87
0.7 - 0.7 - 0.74 / 0.7 - 0.58 0.65 - 0.65 - 0.64 / 0.65 - 0.56 0.6 - 0.67 - 0.93 / 0.81 - 0.9 0.64 - 0.66 - 0.96 / 0.7 - 0.5 0.5 - 0.52 - 0.93 / 0.62 - 0.83 0.56 - 0.56 - 0.9 / 0.56 - 0.52 0.56 - 0.56 - 0.95 / 0.55 - 0.58 0.63 - 0.63 - 0.63 / 0.63 - 0.63 0.59 - 0.7 - 0.94 / 0.79 - 0.68 0.53 - 0.59 - 0.95 / 0.59 - 0.82 0.85 - 0.85 - 0.84 / 0.85 - 0.7 0.58 - 0.72 - 0.96 / 0.83 - 0.78 0.54 - 0.54 - 0.98 / 0.69 - 0.63 0.56 - 0.54 - 0.81 / 0.57 - 0.69 0.52 - 0.65 - 0.99 / 0.81 - 0.98 0.81 - 0.81 - 0.82 / 0.81 - 0.76 0.57 - 0.57 - 0.85 / 0.57 - 0.56 0.56 - 0.54 - 0.56 / 0.56 - 0.47 0.65 - 0.61 - 0.64 / 0.65 - 0.65 0.56 - 0.57 - 0.85 / 0.62 - 0.68 0.5 - 0.63 - 0.92 / 0.68 - 0.87 0.63 - 0.64 - 0.63 / 0.63 - 0.64 0.6 - 0.67 - 0.98 / 0.79 - 0.86 0.72 - 0.83 - 0.93 / 0.84 - 0.95 0.57 - 0.59 - 0.56 / 0.54 - 0.54 0.57 - 0.57 - 0.79 / 0.62 - 0.52 0.37 - 0.44 - 0.95 / 0.52 - 0.51 0.4 - 0.46 - 0.84 / 0.46 - 0.79 0.62 - 0.71 - 0.79 / 0.71 - 0.68 0.59 - 0.59 - 0.6 / 0.59 - 0.62 0.36 - 0.41 - 0.87 / 0.5 - 0.7 0.58 - 0.58 - 0.8 / 0.62 - 0.54 0.67 - 0.63 - 0.93 / 0.7 - 0.73 0.55 - 0.56 - 0.87 / 0.52 - 0.59 0.64 - 0.64 - 0.93 / 0.56 - 0.59 0.66 - 0.73 - 0.89 / 0.78 - 0.89 0.36 - 0.36 - 0.59 / 0.34 - 0.42 0.42 - 0.43 - 0.98 / 0.51 - 0.51 0.41 - 0.42 - 0.89 / 0.52 - 0.5

Table 5: Lexical selection model test accuracies for all 157 English-Spanish words.

Focus word
free.JJ roof.N sword.N storm.N break.V bitch.N set.V wheel.N bone.N tunnel.N cool.JJ bedroom.N mountain.N cake.N coat.N bunch.N turn.V effect.N farm.N tie.N tour.N band.N self.N bill.N cookie.N dozen.N fruit.N pen.N trigger.N ring.N drag.V old.JJ bug.N campaign.N match.N beat.V bottom.N solve.V sell.V butter.N culture.N season.N tank.N wash.V ball.N painful.JJ general.JJ evil.JJ degree.N captain.N serial.JJ infection.N ﬁght.N gut.N spring.N spread.V hot.JJ cup.N clown.N lieutenant.N original.JJ grass.N radiation.N sad.JJ necklace.N grade.N beast.N blade.N rise.V autopsy.JJ jacket.N oil.N compliment.N pulse.N nephew.N step.N suffer.V run.V ﬁght.V store.N ﬁre.V bright.JJ inch.N barn.N gas.N cover.N pot.N alley.N liver.N escape.V beard.N moon.N crown.N arrest.V male.JJ gum.N approve.V candy.N ﬁfth.JJ egg.N

FreqBaseline - DTree - LinearSVM Train/Test - BERT
0.7 - 0.73 - 0.73 / 0.73 - 0.68 0.37 - 0.39 - 0.63 / 0.39 - 0.46 0.76 - 0.77 - 0.76 / 0.76 - 0.7 0.85 - 0.84 - 0.87 / 0.84 - 0.83 0.16 - 0.42 - 0.73 / 0.55 - 0.75 0.32 - 0.45 - 0.46 / 0.45 - 0.42 0.3 - 0.48 - 0.59 / 0.51 - 0.71 0.38 - 0.59 - 0.84 / 0.68 - 0.74 0.33 - 0.46 - 0.85 / 0.62 - 0.82 0.67 - 0.67 - 0.68 / 0.67 - 0.61 0.48 - 0.49 - 0.51 / 0.48 - 0.5 0.63 - 0.63 - 0.67 / 0.64 - 0.6 0.94 - 0.95 - 0.94 / 0.94 - 0.95 0.93 - 0.99 - 0.98 / 0.99 - 0.99 0.87 - 0.88 - 0.88 / 0.88 - 0.87 0.78 - 0.79 - 0.84 / 0.79 - 0.73 0.12 - 0.19 - 0.66 / 0.34 - 0.83 0.38 - 0.8 - 0.77 / 0.8 - 0.82 0.95 - 0.95 - 0.95 / 0.95 - 0.94 0.67 - 0.76 - 0.89 / 0.81 - 0.93 0.48 - 0.57 - 0.81 / 0.65 - 0.66 0.79 - 0.81 - 0.85 / 0.81 - 0.79 0.33 - 0.38 - 0.87 / 0.46 - 0.85 0.57 - 0.64 - 0.91 / 0.74 - 0.88 0.79 - 0.77 - 0.8 / 0.79 - 0.73 0.58 - 0.66 - 0.83 / 0.71 - 0.81 0.79 - 0.86 - 0.95 / 0.88 - 0.93 0.78 - 0.8 - 0.79 / 0.79 - 0.79 0.87 - 0.88 - 0.97 / 0.88 - 0.93 0.42 - 0.63 - 0.79 / 0.72 - 0.81 0.27 - 0.33 - 0.65 / 0.43 - 0.64 0.4 - 0.66 - 0.86 / 0.73 - 0.91 0.35 - 0.42 - 0.54 / 0.46 - 0.52 0.53 - 0.53 - 0.83 / 0.53 - 0.58 0.47 - 0.54 - 0.79 / 0.58 - 0.79 0.22 - 0.29 - 0.53 / 0.34 - 0.54 0.69 - 0.78 - 0.85 / 0.78 - 0.8 0.41 - 0.51 - 0.67 / 0.53 - 0.66 0.44 - 0.49 - 0.75 / 0.57 - 0.75 0.59 - 0.82 - 0.86 / 0.82 - 0.93 0.48 - 0.53 - 0.89 / 0.58 - 0.64 0.62 - 0.67 - 0.81 / 0.68 - 0.66 0.5 - 0.66 - 0.8 / 0.68 - 0.63 0.5 - 0.59 - 0.77 / 0.62 - 0.85 0.51 - 0.54 - 0.58 / 0.54 - 0.63 0.48 - 0.49 - 0.5 / 0.49 - 0.46 0.68 - 0.69 - 0.95 / 0.78 - 0.99 0.5 - 0.5 - 0.66 / 0.52 - 0.48 0.5 - 0.62 - 0.93 / 0.86 - 0.93 0.52 - 0.54 - 0.52 / 0.52 - 0.67 0.63 - 0.84 - 0.85 / 0.85 - 0.82
0.7 - 0.7 - 0.7 / 0.7 - 0.66 0.48 - 0.52 - 0.76 / 0.51 - 0.62 0.56 - 0.74 - 0.87 / 0.79 - 0.84 0.92 - 0.92 - 0.92 / 0.92 - 0.97 0.38 - 0.41 - 0.66 / 0.44 - 0.65 0.5 - 0.71 - 0.88 / 0.75 - 0.83 0.7 - 0.68 - 0.72 / 0.7 - 0.67 0.95 - 0.95 - 0.95 / 0.95 - 0.95 0.41 - 0.48 - 0.71 / 0.52 - 0.63 0.46 - 0.59 - 0.82 / 0.65 - 0.77 0.51 - 0.52 - 0.83 / 0.58 - 0.64 0.63 - 0.66 - 0.88 / 0.66 - 0.64 0.5 - 0.64 - 0.85 / 0.73 - 0.81 0.79 - 0.79 - 0.79 / 0.79 - 0.76 0.42 - 0.68 - 0.88 / 0.89 - 0.9 0.66 - 0.67 - 0.8 / 0.67 - 0.66 0.77 - 0.84 - 0.83 / 0.84 - 0.83 0.23 - 0.43 - 0.88 / 0.6 - 0.82 0.54 - 0.53 - 0.79 / 0.51 - 0.48 0.65 - 0.65 - 0.66 / 0.66 - 0.55 0.83 - 0.9 - 0.95 / 0.89 - 0.88 0.57 - 0.26 - 0.58 / 0.57 - 0.51 0.7 - 0.76 - 0.78 / 0.77 - 0.82 0.76 - 0.76 - 0.77 / 0.76 - 0.72 0.35 - 0.61 - 0.74 / 0.68 - 0.73 0.72 - 0.81 - 0.89 / 0.8 - 0.92 0.19 - 0.32 - 0.84 / 0.59 - 0.84 0.21 - 0.29 - 0.61 / 0.34 - 0.65 0.24 - 0.66 - 0.81 / 0.69 - 0.7 0.76 - 0.81 - 0.91 / 0.83 - 0.95 0.71 - 0.86 - 0.89 / 0.86 - 0.87 0.5 - 0.56 - 0.69 / 0.54 - 0.5 0.79 - 0.79 - 0.79 / 0.79 - 0.75 0.45 - 0.81 - 0.89 / 0.85 - 0.9 0.65 - 0.66 - 0.89 / 0.71 - 0.91 0.38 - 0.49 - 0.68 / 0.6 - 0.56 0.44 - 0.44 - 0.81 / 0.49 - 0.51 0.65 - 0.73 - 0.88 / 0.75 - 0.74 0.43 - 0.46 - 0.86 / 0.5 - 0.72
0.6 - 0.6 - 0.6 / 0.6 - 0.6 0.6 - 0.95 - 0.97 / 0.96 - 0.96 0.81 - 0.85 - 0.96 / 0.86 - 0.89 0.49 - 0.54 - 0.59 / 0.54 - 0.82 0.6 - 0.61 - 0.86 / 0.63 - 0.69 0.5 - 0.5 - 0.77 / 0.52 - 0.69 0.43 - 0.48 - 0.85 / 0.54 - 0.83 0.51 - 0.61 - 0.82 / 0.62 - 0.65 0.61 - 0.66 - 0.73 / 0.67 - 0.72 0.63 - 0.78 - 0.86 / 0.81 - 0.89

Focus word
peaceful.JJ sharp.JJ tie.V puzzle.N fan.N shake.V capital.N sixth.JJ illusion.N wet.JJ costume.N plague.N vault.N deadly.JJ collect.V cliff.N scale.N horn.N stick.N porn.N range.N host.N grow.V expose.V upset.JJ lightning.N laptop.N response.N obvious.JJ distant.JJ niece.N string.N promote.V straight.JJ worthy.JJ rocket.N tub.N heir.N circumstance.N trunk.N engagement.N remain.N crash.N cellar.N opening.N seal.N leak.V harmless.JJ demand.N hip.N pride.N ﬁle.V burn.V charm.N partner.N youth.N raise.V depressed.JJ toast.N burger.N bury.V fatal.JJ abuse.N soft.JJ invade.V crack.N maid.N spare.V inappropriate.JJ trouble.N daily.JJ recording.N sink.N custom.N brandy.N souvenir.N pepper.N distraction.N remarkable.JJ mob.N casualty.N increase.V melt.V thick.JJ mole.N football.N cattle.N special.JJ high.JJ clear.JJ paperwork.N dry.V beneﬁt.N scratch.N peanut.N immunity.N cancel.V wing.N camp.N chip.N

FreqBaseline - DTree - LinearSVM Train/Test- BERT
0.68 - 0.7 - 0.82 / 0.72 - 0.76 0.53 - 0.62 - 0.71 / 0.63 - 0.65 0.34 - 0.5 - 0.82 / 0.63 - 0.89 0.61 - 0.69 - 0.81 / 0.71 - 0.7 0.61 - 0.65 - 0.87 / 0.69 - 0.8 0.32 - 0.66 - 0.83 / 0.68 - 0.85 0.71 - 0.78 - 0.9 / 0.77 - 0.98 0.78 - 0.78 - 0.78 / 0.78 - 0.71 0.67 - 0.68 - 0.78 / 0.68 - 0.68 0.41 - 0.68 - 0.77 / 0.68 - 0.71 0.65 - 0.65 - 0.65 / 0.65 - 0.57 0.69 - 0.68 - 0.84 / 0.69 - 0.67 0.48 - 0.48 - 0.81 / 0.48 - 0.53 0.72 - 0.78 - 0.75 / 0.73 - 0.73 0.53 - 0.61 - 0.86 / 0.65 - 0.83 0.58 - 0.59 - 0.85 / 0.59 - 0.63 0.73 - 0.75 - 0.9 / 0.77 - 0.82 0.62 - 0.71 - 0.82 / 0.73 - 0.79 0.46 - 0.52 - 0.52 / 0.51 - 0.47 0.79 - 0.79 - 0.79 / 0.79 - 0.77 0.58 - 0.66 - 0.86 / 0.67 - 0.69 0.56 - 0.64 - 0.88 / 0.74 - 0.87 0.46 - 0.62 - 0.92 / 0.69 - 0.92 0.62 - 0.62 - 0.62 / 0.62 - 0.68 0.47 - 0.53 - 0.54 / 0.53 - 0.46 0.51 - 0.66 - 0.8 / 0.68 - 0.65 0.79 - 0.79 - 0.82 / 0.79 - 0.77 0.57 - 0.59 - 0.61 / 0.58 - 0.64 0.52 - 0.54 - 0.6 / 0.55 - 0.58 0.67 - 0.77 - 0.96 / 0.89 - 0.92 0.76 - 0.76 - 0.76 / 0.76 - 0.72 0.38 - 0.66 - 0.85 / 0.7 - 0.77 0.59 - 0.62 - 0.92 / 0.75 - 0.83 0.43 - 0.53 - 0.86 / 0.61 - 0.76 0.75 - 0.75 - 0.75 / 0.75 - 0.81 0.74 - 0.74 - 0.76 / 0.74 - 0.74 0.7 - 0.89 - 0.88 / 0.89 - 0.87 0.66 - 0.68 - 0.87 / 0.68 - 0.67 0.94 - 0.94 - 0.94 / 0.94 - 0.94
0.4 - 0.52 - 0.7 / 0.57 - 0.66 0.64 - 0.84 - 0.91 / 0.84 - 0.85 0.37 - 0.38 - 0.91 / 0.35 - 0.38 0.48 - 0.61 - 0.72 / 0.6 - 0.64 0.84 - 0.84 - 0.84 / 0.84 - 0.84 0.37 - 0.62 - 0.82 / 0.64 - 0.76
0.6 - 0.66 - 0.95 / 0.77 - 0.9 0.56 - 0.58 - 0.85 / 0.6 - 0.76 0.51 - 0.56 - 0.56 / 0.55 - 0.49 0.66 - 0.76 - 0.95 / 0.8 - 0.94 0.65 - 0.66 - 0.86 / 0.68 - 0.85 0.61 - 0.64 - 0.8 / 0.65 - 0.85 0.4 - 0.43 - 0.57 / 0.46 - 0.58 0.5 - 0.58 - 0.88 / 0.6 - 0.78 0.75 - 0.85 - 0.88 / 0.85 - 0.88 0.53 - 0.63 - 0.93 / 0.72 - 0.88 0.39 - 0.45 - 0.85 / 0.47 - 0.6 0.25 - 0.41 - 0.87 / 0.55 - 0.77 0.83 - 0.83 - 0.82 / 0.83 - 0.81 0.73 - 0.73 - 0.74 / 0.73 - 0.6 0.52 - 0.5 - 0.52 / 0.52 - 0.44 0.74 - 0.76 - 0.76 / 0.75 - 0.88 0.52 - 0.57 - 0.89 / 0.55 - 0.52 0.61 - 0.72 - 0.79 / 0.72 - 0.77 0.75 - 0.82 - 0.88 / 0.82 - 0.87 0.74 - 0.74 - 0.74 / 0.74 - 0.86 0.46 - 0.59 - 0.91 / 0.64 - 0.83 0.59 - 0.77 - 0.8 / 0.78 - 0.74 0.32 - 0.46 - 0.81 / 0.52 - 0.63 0.65 - 0.65 - 0.81 / 0.66 - 0.6 0.32 - 0.49 - 0.81 / 0.59 - 0.86 0.7 - 0.83 - 0.96 / 0.83 - 0.83 0.6 - 0.56 - 0.82 / 0.59 - 0.59
0.7 - 0.7 - 0.7 / 0.7 - 0.68 0.58 - 0.66 - 0.89 / 0.71 - 0.97 0.76 - 0.76 - 0.76 / 0.76 - 0.74 0.48 - 0.49 - 0.57 / 0.5 - 0.46 0.67 - 0.84 - 0.9 / 0.84 - 0.97 0.53 - 0.64 - 0.75 / 0.62 - 0.73 0.44 - 0.5 - 0.78 / 0.52 - 0.52 0.58 - 0.62 - 0.91 / 0.69 - 0.86 0.78 - 0.78 - 0.78 / 0.78 - 0.7 0.63 - 0.66 - 0.96 / 0.68 - 0.9 0.63 - 0.64 - 0.85 / 0.65 - 0.83 0.47 - 0.55 - 0.88 / 0.57 - 0.63 0.34 - 0.36 - 0.84 / 0.42 - 0.54 0.39 - 0.41 - 0.75 / 0.49 - 0.52 0.27 - 0.27 - 0.35 / 0.28 - 0.34 0.85 - 0.89 - 0.93 / 0.89 - 0.88 0.6 - 0.66 - 0.69 / 0.66 - 0.63 0.31 - 0.38 - 0.86 / 0.49 - 0.81 0.55 - 0.55 - 0.64 / 0.55 - 0.56
0.74 - 0.8 - 0.86 / 0.8 - 0.89 0.39 - 0.45 - 0.87 / 0.53 - 0.62 0.56 - 0.59 - 0.57 / 0.56 - 0.56 0.45 - 0.82 - 0.82 / 0.82 - 0.78 0.8 - 0.85 - 0.91 / 0.85 - 0.88 0.83 - 0.83 - 0.83 / 0.83 - 0.85 0.85 - 0.96 - 0.99 / 0.96 - 0.99 0.81 - 0.85 - 0.87 / 0.85 - 0.89
0.87 - 1.0 - 0.99 / 1.0 - 1.0

Focus word
ﬁghter.N crew.N convinced.JJ point.V broke.JJ sail.V civil.JJ beef.N deadline.N makeup.N text.N feed.V bubble.N drum.N drill.N musical.JJ burn.N lamb.N frame.N column.N brilliant.JJ explode.V fort.N impact.N scarf.N fail.V skill.N mail.N issue.V involve.V label.N psychic.JJ stamp.N dump.N shell.N disease.N rule.V appeal.N can.N glorious.JJ focused.JJ delicious.JJ addict.N player.N lethal.JJ server.N welcome.JJ penny.N immune.JJ vet.N deﬁne.V desperate.JJ move.V acre.N star.N claim.N leadership.N collar.N donate.V suspend.V shade.N sketch.N hood.N build.V tear.V determine.V mourn.V abandon.V lottery.N pole.N exhausted.JJ rubber.N nipple.N sew.V mental.JJ janitor.N efﬁcient.JJ speaker.N lawn.N therapist.N administration.N reckless.JJ ham.N settle.V headline.N estate.N smooth.JJ worker.N gallon.N scan.V lure.V sophisticated.JJ offensive.JJ contribute.V management.N straw.N donkey.N delicate.JJ brutal.JJ sunny.JJ

FreqBaseline - DTree - LinearSVM - BERT
0.29 - 0.31 - 0.78 / 0.31 - 0.47 0.83 - 0.88 - 0.91 / 0.85 - 0.94 0.66 - 0.68 - 0.73 / 0.66 - 0.65 0.38 - 0.42 - 0.55 / 0.43 - 0.61 0.37 - 0.4 - 0.72 / 0.35 - 0.4 0.55 - 0.55 - 0.8 / 0.52 - 0.62 0.53 - 0.84 - 0.96 / 0.86 - 0.91 0.37 - 0.37 - 0.77 / 0.39 - 0.43
0.8 - 0.8 - 0.8 / 0.8 - 0.76 0.61 - 0.6 - 0.7 / 0.62 - 0.49 0.84 - 0.84 - 0.92 / 0.84 - 0.86 0.32 - 0.46 - 0.88 / 0.48 - 0.76 0.54 - 0.57 - 0.86 / 0.55 - 0.56 0.52 - 0.68 - 0.83 / 0.75 - 0.76 0.52 - 0.59 - 0.91 / 0.74 - 0.83 0.55 - 0.7 - 0.93 / 0.76 - 0.91 0.68 - 0.73 - 0.88 / 0.74 - 0.83 0.77 - 0.76 - 0.86 / 0.77 - 0.77 0.37 - 0.35 - 0.97 / 0.57 - 0.57 0.77 - 0.78 - 0.9 / 0.8 - 0.88 0.43 - 0.43 - 0.56 / 0.44 - 0.51 0.51 - 0.59 - 0.85 / 0.58 - 0.9 0.61 - 0.62 - 0.7 / 0.62 - 0.55 0.47 - 0.56 - 0.92 / 0.63 - 0.8 0.45 - 0.45 - 0.63 / 0.45 - 0.45 0.71 - 0.71 - 0.74 / 0.7 - 0.99 0.54 - 0.57 - 0.82 / 0.6 - 0.71 0.9 - 0.95 - 0.97 / 0.96 - 0.95 0.5 - 0.54 - 0.86 / 0.58 - 0.79 0.28 - 0.31 - 0.69 / 0.32 - 0.38 0.55 - 0.66 - 0.8 / 0.68 - 0.79 0.61 - 0.74 - 0.93 / 0.79 - 0.84 0.61 - 0.7 - 0.91 / 0.72 - 0.83 0.42 - 0.62 - 0.73 / 0.64 - 0.7 0.36 - 0.53 - 0.9 / 0.56 - 0.7 0.61 - 0.63 - 0.89 / 0.65 - 0.75 0.33 - 0.4 - 0.9 / 0.59 - 0.76 0.76 - 0.78 - 0.85 / 0.8 - 0.84 0.55 - 0.68 - 0.87 / 0.7 - 0.71 0.74 - 0.74 - 0.75 / 0.74 - 0.7 0.75 - 0.78 - 0.75 / 0.75 - 0.73 0.54 - 0.54 - 0.63 / 0.54 - 0.58 0.52 - 0.52 - 0.52 / 0.52 - 0.54 0.39 - 0.97 - 0.99 / 0.98 - 0.98 0.75 - 0.79 - 0.85 / 0.79 - 0.78 0.67 - 0.67 - 0.83 / 0.68 - 0.62 0.46 - 0.48 - 0.81 / 0.6 - 0.77 0.93 - 0.92 - 0.93 / 0.92 - 0.98 0.57 - 0.93 - 0.94 / 0.93 - 0.97 0.73 - 0.8 - 0.94 / 0.82 - 0.91 0.46 - 0.49 - 0.87 / 0.51 - 0.59 0.73 - 0.72 - 0.74 / 0.73 - 0.78 0.31 - 0.47 - 0.9 / 0.62 - 0.89 0.72 - 0.72 - 0.72 / 0.72 - 0.72 0.28 - 0.67 - 0.91 / 0.77 - 0.87 0.47 - 0.49 - 0.89 / 0.58 - 0.69 0.77 - 0.81 - 0.87 / 0.8 - 0.81 0.69 - 0.69 - 0.85 / 0.7 - 0.68 0.55 - 0.55 - 0.87 / 0.55 - 0.68 0.53 - 0.52 - 0.78 / 0.55 - 0.7 0.58 - 0.76 - 0.94 / 0.88 - 0.92 0.79 - 0.81 - 0.8 / 0.79 - 0.93 0.55 - 0.7 - 0.94 / 0.82 - 0.88 0.36 - 0.4 - 0.94 / 0.45 - 0.76 0.39 - 0.49 - 0.86 / 0.51 - 0.75 0.62 - 0.63 - 0.93 / 0.7 - 0.94 0.56 - 0.55 - 0.84 / 0.6 - 0.62 0.48 - 0.55 - 0.86 / 0.59 - 0.75 0.51 - 0.63 - 0.78 / 0.61 - 0.59 0.44 - 0.58 - 0.91 / 0.67 - 0.65 0.62 - 0.6 - 0.61 / 0.62 - 0.49 0.42 - 0.42 - 0.88 / 0.44 - 0.51 0.76 - 0.76 - 0.76 / 0.76 - 0.73 0.48 - 0.52 - 0.87 / 0.56 - 0.72 0.44 - 0.38 - 0.84 / 0.45 - 0.47
0.8 - 0.8 - 0.8 / 0.8 - 0.77 0.7 - 0.7 - 0.7 / 0.7 - 0.66 0.45 - 0.51 - 0.84 / 0.58 - 0.8 0.57 - 0.57 - 0.82 / 0.53 - 0.54 0.69 - 0.69 - 0.69 / 0.69 - 0.65 0.52 - 0.52 - 0.9 / 0.56 - 0.67 0.63 - 0.6 - 0.77 / 0.62 - 0.56 0.73 - 0.12 - 0.73 / 0.73 - 0.69 0.39 - 0.51 - 0.83 / 0.64 - 0.91 0.52 - 0.52 - 0.86 / 0.55 - 0.49 0.43 - 0.71 - 0.82 / 0.71 - 0.76 0.38 - 0.45 - 0.88 / 0.45 - 0.6 0.75 - 0.99 - 0.97 / 0.99 - 0.99 0.69 - 0.68 - 0.72 / 0.69 - 0.71 0.48 - 0.52 - 0.59 / 0.52 - 0.58 0.43 - 0.42 - 0.82 / 0.42 - 0.69 0.35 - 0.35 - 0.66 / 0.37 - 0.45 0.72 - 0.82 - 0.92 / 0.81 - 0.89 0.59 - 0.63 - 0.84 / 0.63 - 0.72 0.55 - 0.6 - 0.89 / 0.61 - 0.7 0.51 - 0.62 - 0.91 / 0.66 - 0.86 0.61 - 0.6 - 0.7 / 0.62 - 0.63 0.7 - 0.7 - 0.7 / 0.7 - 0.68 0.39 - 0.42 - 0.43 / 0.4 - 0.28 0.6 - 0.6 - 0.94 / 0.71 - 0.81

Table 6: Part-1: Lexical selection model test accuracies for a English-Greek words.

Focus word
light.JJ current.JJ humiliating.JJ drone.N bald.JJ wipe.V institution.N retirement.N bunny.N despair.N cult.N trainer.N genius.N competition.N thread.N coach.N trance.N respectable.JJ notebook.N recommendation.N wire.N humiliation.N dock.N recover.V fortress.N furious.JJ light.V sign.V crop.N spill.V congressman.N sale.N advanced.JJ publish.V popcorn.N thunder.N wreck.N shine.V bite.N contaminate.V intern.N willing.JJ fountain.N compete.V mentally.RB swim.N birth.N sequence.N dirty.JJ attempt.V link.N request.N cautious.JJ accessory.N ounce.N spinal.JJ editor.N shocking.JJ miserable.JJ scan.N rider.N choose.V push.V pajama.N thorough.JJ stubborn.JJ penetrate.V guard.N decorate.V cord.N eighth.JJ poll.N pathetic.JJ prey.N settlement.N bow.N dorm.N serve.V contribution.N grocery.N sunshine.N sponsor.N broken.JJ convoy.N unbearable.JJ vague.JJ torch.N boxer.N chin.N cube.N lean.V carrier.N release.V dot.N pantie.N roadblock.N blackout.N superstition.N silver.N complex.JJ

FreqBaseline - DTree - LinearSVM Train/Test - BERT
0.84 - 0.89 - 0.99 / 0.92 - 0.99 0.9 - 0.92 - 0.9 / 0.9 - 0.91
0.51 - 0.47 - 0.78 / 0.51 - 0.47 0.38 - 0.39 - 0.89 / 0.41 - 0.52 0.69 - 0.69 - 0.69 / 0.69 - 0.64 0.38 - 0.4 - 0.85 / 0.46 - 0.55 0.54 - 0.67 - 0.89 / 0.68 - 0.76 0.54 - 0.58 - 0.87 / 0.63 - 0.57 0.5 - 0.47 - 0.83 / 0.5 - 0.53 0.53 - 0.53 - 0.95 / 0.55 - 0.56 0.75 - 0.75 - 0.75 / 0.75 - 0.71 0.44 - 0.54 - 0.83 / 0.51 - 0.57 0.66 - 0.66 - 0.81 / 0.66 - 0.67 0.87 - 0.86 - 0.87 / 0.87 - 0.87 0.63 - 0.65 - 0.81 / 0.64 - 0.67 0.83 - 0.82 - 0.84 / 0.83 - 0.94 0.82 - 0.84 - 0.82 / 0.82 - 0.99 0.52 - 0.51 - 0.53 / 0.52 - 0.34 0.78 - 0.78 - 0.78 / 0.78 - 0.78 0.54 - 0.73 - 0.83 / 0.74 - 0.7 0.61 - 0.71 - 0.93 / 0.73 - 0.77 0.6 - 0.62 - 0.69 / 0.63 - 0.56 0.48 - 0.61 - 0.8 / 0.6 - 0.74 0.38 - 0.48 - 0.87 / 0.52 - 0.79 0.71 - 0.69 - 0.71 / 0.71 - 0.61 0.67 - 0.67 - 0.68 / 0.67 - 0.62 0.62 - 0.73 - 0.94 / 0.74 - 0.83 0.82 - 0.82 - 0.83 / 0.82 - 0.98 0.63 - 0.69 - 0.86 / 0.69 - 0.63 0.56 - 0.65 - 0.85 / 0.7 - 0.81 0.59 - 0.62 - 0.74 / 0.61 - 0.52 0.53 - 0.71 - 0.92 / 0.76 - 0.99 0.67 - 0.73 - 0.84 / 0.73 - 0.78 0.52 - 0.53 - 0.62 / 0.55 - 0.74 0.77 - 0.23 - 0.77 / 0.77 - 0.71 0.62 - 0.61 - 0.68 / 0.62 - 0.68 0.35 - 0.39 - 0.51 / 0.38 - 0.58 0.33 - 0.65 - 0.88 / 0.71 - 0.84 0.76 - 0.87 - 0.97 / 0.83 - 0.92 0.44 - 0.43 - 0.57 / 0.43 - 0.8 0.56 - 0.56 - 0.57 / 0.56 - 0.55 0.69 - 0.83 - 0.82 / 0.83 - 0.77 0.63 - 0.63 - 0.69 / 0.63 - 0.59 0.56 - 0.58 - 0.86 / 0.62 - 0.71 0.59 - 0.71 - 0.82 / 0.73 - 0.73 0.5 - 0.71 - 0.84 / 0.68 - 0.76 0.37 - 0.82 - 0.84 / 0.83 - 0.89 0.72 - 0.77 - 0.85 / 0.78 - 0.74 0.4 - 0.84 - 0.97 / 0.87 - 0.93
0.8 - 0.8 - 0.8 / 0.8 - 0.85 0.57 - 0.74 - 0.96 / 0.78 - 0.94 0.55 - 0.6 - 0.8 / 0.61 - 0.68 0.79 - 0.79 - 0.78 / 0.79 - 0.73 0.61 - 0.71 - 0.96 / 0.7 - 0.88 0.58 - 0.53 - 0.8 / 0.53 - 0.58 0.52 - 0.68 - 0.81 / 0.71 - 0.62 0.59 - 0.64 - 0.92 / 0.59 - 0.65 0.75 - 0.75 - 0.74 / 0.75 - 0.69 0.74 - 0.75 - 0.75 / 0.74 - 0.73 0.66 - 0.77 - 0.86 / 0.78 - 0.73 0.6 - 0.62 - 0.85 / 0.64 - 0.7 0.59 - 0.68 - 0.84 / 0.7 - 0.87 0.54 - 0.58 - 0.61 / 0.58 - 0.7
0.6 - 0.6 - 0.6 / 0.6 - 0.58 0.39 - 0.43 - 0.6 / 0.4 - 0.34 0.86 - 0.86 - 0.86 / 0.86 - 0.85 0.57 - 0.57 - 0.93 / 0.65 - 0.69 0.23 - 0.69 - 0.95 / 0.77 - 0.93 0.59 - 0.77 - 0.89 / 0.78 - 0.83 0.44 - 0.77 - 0.9 / 0.8 - 0.8 0.76 - 0.76 - 0.76 / 0.76 - 0.7 0.6 - 0.79 - 0.89 / 0.77 - 0.78 0.49 - 0.27 - 0.53 / 0.49 - 0.44 0.55 - 0.61 - 0.91 / 0.64 - 0.6 0.63 - 0.63 - 0.89 / 0.64 - 0.79 0.33 - 0.64 - 0.9 / 0.72 - 0.88 0.69 - 0.69 - 0.68 / 0.69 - 0.64 0.66 - 0.66 - 0.9 / 0.74 - 0.89
0.7 - 0.7 - 0.7 / 0.7 - 0.67 0.36 - 0.4 - 0.73 / 0.41 - 0.38 0.48 - 0.57 - 0.8 / 0.63 - 0.65 0.56 - 0.56 - 0.78 / 0.59 - 0.65 0.43 - 0.86 - 0.94 / 0.86 - 0.9 0.28 - 0.28 - 0.79 / 0.33 - 0.39 0.44 - 0.46 - 0.66 / 0.49 - 0.39
0.6 - 0.6 - 0.6 / 0.6 - 0.56 0.44 - 0.44 - 0.44 / 0.44 - 0.44 0.76 - 0.76 - 0.77 / 0.76 - 0.74 0.61 - 0.23 - 0.61 / 0.61 - 0.59 0.61 - 0.95 - 0.97 / 0.95 - 0.95 0.3 - 0.34 - 0.85 / 0.39 - 0.65 0.59 - 0.65 - 0.94 / 0.73 - 0.95 0.56 - 0.68 - 0.91 / 0.74 - 0.93 0.58 - 0.65 - 0.89 / 0.78 - 0.81 0.46 - 0.46 - 0.66 / 0.46 - 0.44 0.66 - 0.66 - 0.66 / 0.66 - 0.53 0.62 - 0.66 - 0.82 / 0.66 - 0.53 0.37 - 0.37 - 0.5 / 0.37 - 0.33 0.85 - 0.91 - 0.98 / 0.92 - 0.97 0.53 - 0.54 - 0.96 / 0.54 - 0.66

Focus word
camping.N cheat.V act.V rose.N preacher.N doorman.N unite.V ﬂock.N remark.N expel.V dizzy.JJ vulture.N simulation.N contempt.N dean.N beg.V grieve.V pea.N extension.N intercept.V homemade.JJ domestic.JJ spear.N printer.N carnival.N intervene.V concrete.N argument.N extensive.JJ kiss.V harvest.N foreman.N pier.N ignorant.JJ fart.V sabotage.N stripe.N rooftop.N destroyer.N whine.V mufﬁn.N delusion.N tornado.N rock.N courier.N rat.N gather.V countryside.N train.N guest.N tonic.N yen.N equal.V isolated.JJ arrogance.N native.N redemption.N rookie.N robber.N decency.N mold.N brothel.N breathe.V deliberately.RB adjustment.N component.N lust.N classy.JJ unsolved.JJ bravery.N radius.N register.V paint.V powerless.JJ exhaust.V bend.V porter.N guinea.N serpent.N purity.N college.N guarantee.V camp.V promising.JJ detector.N elect.V paramedic.N shiny.JJ racial.JJ vagina.N poor.JJ neural.JJ substitute.N bolt.N oral.JJ forensic.JJ stench.N carpenter.N mri.N deport.V

FreqBaseline - DTree - LinearSVM Train/Test- BERT
0.69 - 0.69 - 0.68 / 0.69 - 0.63 0.34 - 0.33 - 0.64 / 0.3 - 0.67 0.51 - 0.53 - 0.87 / 0.51 - 0.65 0.47 - 0.51 - 0.9 / 0.57 - 0.76 0.64 - 0.64 - 0.65 / 0.64 - 0.67 0.72 - 0.69 - 0.72 / 0.72 - 0.67 0.53 - 0.53 - 0.91 / 0.52 - 0.7 0.51 - 0.54 - 0.88 / 0.56 - 0.59 0.59 - 0.58 - 0.6 / 0.59 - 0.51 0.5 - 0.62 - 0.68 / 0.62 - 0.74 0.45 - 0.51 - 0.66 / 0.53 - 0.56 0.53 - 0.19 - 0.83 / 0.62 - 0.62 0.78 - 0.78 - 0.79 / 0.78 - 0.79 0.74 - 0.85 - 0.88 / 0.86 - 0.81
0.8 - 0.81 - 0.8 / 0.8 - 0.76 0.79 - 0.82 - 0.94 / 0.8 - 0.91 0.67 - 0.67 - 0.69 / 0.67 - 0.63 0.59 - 0.56 - 0.78 / 0.59 - 0.62 0.38 - 0.51 - 0.94 / 0.56 - 0.68 0.55 - 0.65 - 0.87 / 0.72 - 0.87 0.77 - 0.79 - 0.77 / 0.77 - 0.85 0.48 - 0.7 - 0.93 / 0.62 - 0.75 0.43 - 0.42 - 0.8 / 0.38 - 0.58 0.78 - 0.77 - 0.78 / 0.78 - 0.86 0.72 - 0.75 - 0.72 / 0.72 - 0.64 0.73 - 0.7 - 0.73 / 0.73 - 0.68 0.63 - 0.65 - 0.82 / 0.67 - 0.7 0.58 - 0.58 - 0.57 / 0.58 - 0.55 0.72 - 0.72 - 0.72 / 0.72 - 0.71 0.36 - 0.68 - 0.92 / 0.77 - 0.89 0.61 - 0.64 - 0.74 / 0.64 - 0.71 0.43 - 0.51 - 0.81 / 0.58 - 0.77 0.82 - 0.82 - 0.82 / 0.82 - 0.78
0.6 - 0.56 - 0.6 / 0.6 - 0.5 0.68 - 0.73 - 0.68 / 0.68 - 0.9 0.78 - 0.78 - 0.78 / 0.78 - 0.68 0.54 - 0.63 - 0.89 / 0.65 - 0.59 0.66 - 0.66 - 0.66 / 0.66 - 0.66 0.59 - 0.64 - 0.95 / 0.79 - 0.86 0.41 - 0.41 - 0.53 / 0.39 - 0.32 0.59 - 0.59 - 0.76 / 0.6 - 0.55 0.59 - 0.59 - 0.59 / 0.59 - 0.55 0.76 - 0.76 - 0.76 / 0.76 - 0.77 0.81 - 0.96 - 1.0 / 0.96 - 1.0 0.52 - 0.52 - 0.89 / 0.58 - 0.48 0.41 - 0.67 - 0.86 / 0.68 - 0.86 0.62 - 0.83 - 0.95 / 0.9 - 0.9 0.65 - 0.71 - 0.65 / 0.65 - 0.79 0.49 - 0.74 - 0.94 / 0.86 - 0.94 0.52 - 0.74 - 0.89 / 0.76 - 0.87 0.78 - 0.92 - 0.83 / 0.9 - 0.99 0.67 - 0.65 - 0.65 / 0.67 - 0.67 0.56 - 0.61 - 0.6 / 0.57 - 0.62 0.7 - 0.94 - 0.91 / 0.94 - 0.97 0.78 - 0.78 - 0.78 / 0.78 - 0.73 0.51 - 0.39 - 0.79 / 0.39 - 0.54 0.81 - 0.19 - 0.8 / 0.81 - 0.74 0.36 - 0.36 - 0.65 / 0.35 - 0.27 0.38 - 0.64 - 0.76 / 0.67 - 0.67 0.59 - 0.56 - 0.86 / 0.54 - 0.54 0.63 - 0.76 - 0.93 / 0.77 - 0.97 0.52 - 0.52 - 0.84 / 0.48 - 0.52 0.64 - 0.72 - 0.9 / 0.7 - 0.86 0.69 - 0.68 - 0.7 / 0.69 - 0.64 0.63 - 0.65 - 0.81 / 0.73 - 0.72 0.51 - 0.61 - 0.82 / 0.54 - 0.87 0.58 - 0.58 - 0.81 / 0.6 - 0.57 0.38 - 0.38 - 0.47 / 0.38 - 0.27 0.63 - 0.63 - 0.63 / 0.63 - 0.67 0.66 - 0.7 - 0.95 / 0.68 - 0.59 0.29 - 0.46 - 0.88 / 0.66 - 0.8 0.29 - 0.38 - 0.93 / 0.47 - 0.73 0.61 - 0.65 - 0.93 / 0.61 - 0.89 0.53 - 0.47 - 0.81 / 0.51 - 0.65 0.78 - 0.78 - 0.78 / 0.78 - 0.65 0.3 - 0.53 - 0.88 / 0.65 - 0.76 0.41 - 0.41 - 0.79 / 0.43 - 0.65
0.52 - 1.0 - 0.98 / 1.0 - 1.0 0.58 - 0.42 - 0.56 / 0.58 - 0.52 0.77 - 0.77 - 0.78 / 0.77 - 0.79 0.44 - 0.56 - 0.91 / 0.71 - 0.78 0.68 - 0.68 - 0.67 / 0.68 - 0.86 0.68 - 0.71 - 0.86 / 0.72 - 0.84 0.77 - 0.77 - 0.77 / 0.77 - 0.61 0.54 - 0.8 - 0.87 / 0.83 - 0.79 0.6 - 0.61 - 0.82 / 0.63 - 0.73 0.41 - 0.42 - 0.75 / 0.44 - 0.58 0.79 - 0.79 - 0.79 / 0.79 - 0.76 0.56 - 0.69 - 0.85 / 0.69 - 0.56 0.55 - 0.55 - 0.55 / 0.55 - 0.48 0.74 - 0.74 - 0.73 / 0.74 - 0.71 0.54 - 0.58 - 0.93 / 0.68 - 0.65 0.57 - 0.64 - 0.79 / 0.65 - 0.74 0.29 - 0.56 - 0.69 / 0.57 - 0.54 0.74 - 0.88 - 0.91 / 0.75 - 0.96 0.51 - 0.58 - 0.94 / 0.65 - 0.5 0.55 - 0.55 - 0.81 / 0.55 - 0.51 0.72 - 0.72 - 0.73 / 0.72 - 0.63 0.78 - 0.78 - 0.77 / 0.78 - 0.78 0.63 - 0.63 - 0.68 / 0.65 - 0.66

Focus word
suspension.N notorious.JJ relative.JJ honor.V duct.N scooter.N temporal.JJ sis.N terrace.N tenth.JJ banner.N mixture.N swelling.N clip.N mix.V eliminate.V bouquet.N dig.V abs.N desperation.N slogan.N raven.N waste.V honorable.JJ clarity.N minority.N frustrating.JJ resident.N spaghetti.N relic.N shaft.N breathe.RB contraction.N outbreak.N record.V ranger.N cathedral.N boredom.N manual.JJ interfere.V quality.N obstruction.N modiﬁcation.N marrow.N loser.N branch.N bankruptcy.N proﬁtable.JJ broker.N ofﬁcial.N distract.V remote.N ignition.N weed.N vigilante.N proportion.N pedophile.N scenery.N ballot.N nightfall.N lookout.N violet.N caleb.JJ static.JJ baptize.V mark.V modest.JJ insight.N chart.N shrink.N whorehouse.N memorial.N mineral.N tracker.N rebuild.V consumption.N viper.N milligram.N hanging.N expansion.N reluctant.JJ declaration.N regional.JJ sterile.JJ skinny.JJ amendment.N binocular.N crutch.N grill.N cone.N militia.N count.V voluntarily.RB gallow.N hangover.N operating.N pasta.N overrate.V charge.V rift.N

FreqBaseline - DTree - LinearSVM - BERT
0.42 - 0.48 - 0.96 / 0.58 - 0.73 0.7 - 0.72 - 0.7 / 0.7 - 0.67
0.59 - 0.64 - 0.98 / 0.76 - 0.95 0.82 - 0.88 - 0.99 / 0.93 - 0.93 0.46 - 0.65 - 0.88 / 0.68 - 0.7 0.78 - 0.78 - 0.79 / 0.78 - 0.72 0.51 - 0.93 - 0.94 / 0.89 - 0.89 0.55 - 0.55 - 0.85 / 0.58 - 0.42 0.63 - 0.63 - 0.63 / 0.63 - 0.7 0.77 - 0.77 - 0.76 / 0.77 - 0.73 0.5 - 0.52 - 0.91 / 0.55 - 0.67 0.53 - 0.54 - 0.95 / 0.5 - 0.41 0.73 - 0.71 - 0.79 / 0.7 - 0.73 0.37 - 0.68 - 0.93 / 0.83 - 0.83 0.57 - 0.6 - 0.86 / 0.64 - 0.84 0.46 - 0.51 - 0.52 / 0.46 - 0.63 0.63 - 0.63 - 0.73 / 0.64 - 0.57 0.53 - 0.63 - 0.65 / 0.63 - 0.85 0.55 - 0.58 - 0.93 / 0.64 - 0.97 0.59 - 0.59 - 0.9 / 0.52 - 0.48 0.77 - 0.77 - 0.77 / 0.77 - 0.79 0.65 - 0.68 - 0.88 / 0.65 - 0.89 0.58 - 0.86 - 0.96 / 0.83 - 0.94 0.75 - 0.79 - 0.97 / 0.77 - 0.75 0.69 - 0.69 - 0.7 / 0.69 - 0.69 0.66 - 0.66 - 0.91 / 0.67 - 0.72 0.73 - 0.73 - 0.76 / 0.73 - 0.61 0.7 - 0.73 - 0.69 / 0.7 - 0.85 0.56 - 0.56 - 0.55 / 0.56 - 0.48 0.6 - 0.6 - 0.78 / 0.58 - 0.75 0.5 - 0.65 - 0.83 / 0.62 - 0.75 0.67 - 0.67 - 0.6 / 0.67 - 0.61 0.69 - 0.69 - 0.69 / 0.69 - 0.67
0.6 - 0.6 - 0.6 / 0.6 - 0.53 0.83 - 0.85 - 0.85 / 0.83 - 0.97 0.55 - 0.7 - 0.86 / 0.73 - 0.78 0.55 - 0.52 - 0.86 / 0.57 - 0.57 0.48 - 0.48 - 0.52 / 0.48 - 0.5
0.73 - 1.0 - 1.0 / 1.0 - 1.0 0.76 - 0.78 - 0.75 / 0.76 - 0.85 0.57 - 0.55 - 0.84 / 0.62 - 0.58 0.65 - 0.56 - 0.66 / 0.65 - 0.56 0.65 - 0.67 - 0.69 / 0.65 - 0.72 0.71 - 0.72 - 0.89 / 0.72 - 0.8 0.66 - 0.66 - 0.68 / 0.66 - 0.75 0.53 - 0.54 - 0.9 / 0.59 - 0.9 0.5 - 0.59 - 0.89 / 0.61 - 0.56 0.54 - 0.48 - 0.91 / 0.58 - 0.5 0.62 - 0.67 - 0.74 / 0.67 - 0.65 0.69 - 0.7 - 0.96 / 0.69 - 0.8 0.58 - 0.58 - 0.84 / 0.65 - 0.87 0.53 - 0.49 - 0.86 / 0.53 - 0.53 0.77 - 0.82 - 0.95 / 0.79 - 0.89 0.47 - 0.57 - 0.92 / 0.59 - 0.71
0.7 - 0.7 - 0.7 / 0.7 - 0.68 0.62 - 0.62 - 0.98 / 0.7 - 0.81 0.63 - 0.63 - 0.72 / 0.63 - 0.58 0.68 - 0.88 - 0.91 / 0.8 - 0.88 0.55 - 0.68 - 0.98 / 0.7 - 0.64 0.52 - 0.5 - 0.9 / 0.55 - 0.78 0.53 - 0.53 - 0.83 / 0.56 - 0.78 0.62 - 0.66 - 0.98 / 0.75 - 0.94 0.74 - 0.74 - 0.74 / 0.74 - 0.65 0.65 - 0.72 - 0.9 / 0.69 - 0.89 0.55 - 0.59 - 0.86 / 0.55 - 0.62 0.52 - 0.65 - 0.92 / 0.72 - 0.83 0.58 - 0.51 - 0.83 / 0.55 - 0.58 0.42 - 0.42 - 0.81 / 0.46 - 0.5 0.86 - 0.86 - 1.0 / 0.88 - 0.94 0.6 - 0.6 - 0.58 / 0.6 - 0.53 0.51 - 0.46 - 0.94 / 0.41 - 0.56 0.53 - 0.58 - 0.98 / 0.77 - 0.72 0.52 - 0.85 - 0.9 / 0.85 - 0.89 0.45 - 0.5 - 0.67 / 0.52 - 0.69 0.6 - 0.6 - 0.97 / 0.75 - 0.64 0.81 - 0.83 - 0.97 / 0.83 - 0.87 0.57 - 0.57 - 0.97 / 0.61 - 0.78 0.73 - 0.76 - 0.76 / 0.73 - 0.64 0.72 - 0.76 - 0.96 / 0.85 - 0.65 0.83 - 0.88 - 0.95 / 0.94 - 0.88 0.64 - 0.62 - 0.68 / 0.64 - 0.6 0.56 - 0.82 - 0.94 / 0.82 - 0.71 0.6 - 0.64 - 0.89 / 0.6 - 0.55 0.6 - 0.6 - 0.92 / 0.69 - 0.9 0.65 - 0.62 - 0.66 / 0.65 - 0.56 0.57 - 0.57 - 0.8 / 0.57 - 0.61 0.65 - 0.65 - 0.64 / 0.65 - 0.53 0.61 - 0.59 - 0.7 / 0.61 - 0.64 0.47 - 0.47 - 0.64 / 0.47 - 0.49 0.53 - 0.65 - 0.89 / 0.69 - 0.78 0.64 - 0.64 - 0.65 / 0.64 - 0.68 0.42 - 0.42 - 0.86 / 0.49 - 0.91 0.63 - 0.37 - 0.63 / 0.63 - 0.54 0.62 - 0.66 - 0.73 / 0.66 - 0.5 0.6 - 0.6 - 0.77 / 0.6 - 0.53
0.74 - 1.0 - 1.0 / 1.0 - 1.0 0.56 - 0.56 - 0.9 / 0.6 - 0.55 0.53 - 0.53 - 0.82 / 0.53 - 0.61 0.6 - 0.88 - 0.97 / 0.9 - 0.93 0.68 - 0.78 - 0.78 / 0.73 - 0.84

Table 7: Part-2: Lexical selection model test accuracies for a English-Greek words.

Focus word
descent.N coaster.N teacher.N blow.V fairy.N diversity.N postmortem.N clamp.N reconsider.V knit.V medium.JJ bastard.N vacant.JJ parliament.N pretzel.N abrasion.N walkie.N tango.N expand.V resourceful.JJ seeker.N algae.N fatigue.N gator.N riddance.N hide.V cunning.JJ particle.N commit.V scatter.V yuan.N absolution.N ﬁrecracker.N kneecap.N railing.N carnage.N

FreqBaseline - DTree - LinearSVM Train/Test - BERT
0.64 - 0.64 - 0.94 / 0.67 - 0.86 0.68 - 0.85 - 0.93 / 0.85 - 0.94 0.4 - 0.91 - 0.96 / 0.91 - 0.96 0.84 - 0.81 - 0.91 / 0.89 - 0.96
0.69 - 1.0 - 1.0 / 1.0 - 1.0 0.5 - 0.41 - 0.88 / 0.45 - 0.41 0.48 - 0.48 - 0.96 / 0.57 - 0.64 0.62 - 0.62 - 0.92 / 0.56 - 0.53 0.61 - 0.61 - 0.6 / 0.61 - 0.58 0.63 - 0.65 - 0.78 / 0.63 - 0.74 0.52 - 0.6 - 0.94 / 0.78 - 0.68 0.38 - 0.68 - 0.95 / 0.76 - 0.89 0.69 - 0.77 - 0.99 / 0.82 - 0.95 0.59 - 0.59 - 0.61 / 0.59 - 0.57
0.7 - 0.7 - 0.69 / 0.7 - 0.62 0.61 - 0.39 - 0.6 / 0.61 - 0.71 0.68 - 0.68 - 0.68 / 0.68 - 0.71 0.52 - 0.52 - 0.94 / 0.6 - 0.55 0.54 - 0.76 - 0.97 / 0.84 - 0.95
0.6 - 0.6 - 0.6 / 0.6 - 0.51 0.55 - 0.55 - 0.58 / 0.55 - 0.59 weep.V
0.55 - 0.67 - 0.85 / 0.58 - 0.58 0.52 - 0.45 - 0.88 / 0.5 - 0.59 0.73 - 0.76 - 0.73 / 0.73 - 0.84 0.52 - 0.52 - 0.7 / 0.38 - 0.76 0.75 - 0.75 - 0.98 / 0.86 - 0.82 0.56 - 0.56 - 0.87 / 0.62 - 0.66 0.55 - 0.92 - 0.97 / 0.92 - 1.0 0.92 - 0.93 - 0.93 / 0.92 - 0.99 0.59 - 0.63 - 0.61 / 0.59 - 0.67 0.59 - 0.59 - 0.88 / 0.73 - 0.73 0.59 - 0.59 - 0.9 / 0.48 - 0.69 0.59 - 0.64 - 0.86 / 0.5 - 0.73 0.54 - 0.54 - 0.95 / 0.62 - 0.69 0.56 - 0.56 - 0.61 / 0.56 - 0.72 0.55 - 0.52 - 0.56 / 0.55 - 0.59

Focus word
suit.N vamp.N tangible.JJ jap.N jelly.N proud.JJ extend.V remote.JJ rethink.V conductor.N elite.JJ helm.N piss.V hideout.N deek.N countess.N tight.N pedal.N envy.N heinous.JJ 0.62 - 0.67 - 0.61 / 0.62 - 0.95 newlywed.N farmhouse.N outskirt.N infectious.JJ conquer.V plague.V theo.N rivalry.N donor.N autopsy.N modesty.N honorary.JJ insubordination.N tremor.N memento.N
Overall Average:

FreqBaseline - DTree - LinearSVM Train/Test- BERT
0.63 - 0.63 - 0.95 / 0.7 - 1.0 0.53 - 0.53 - 0.96 / 0.57 - 0.57 0.52 - 0.57 - 0.74 / 0.57 - 0.57 0.61 - 0.61 - 0.63 / 0.61 - 0.59 0.52 - 0.59 - 0.72 / 0.59 - 0.59 0.64 - 0.64 - 0.65 / 0.64 - 0.7 0.62 - 0.76 - 0.9 / 0.76 - 0.98 0.49 - 0.49 - 0.88 / 0.59 - 0.65 0.79 - 0.79 - 0.79 / 0.79 - 0.79 0.47 - 0.5 - 0.91 / 0.61 - 0.82 0.65 - 0.65 - 0.95 / 0.67 - 0.69 0.81 - 0.83 - 0.79 / 0.81 - 0.78 0.69 - 0.69 - 0.69 / 0.69 - 0.7 0.56 - 0.56 - 0.56 / 0.56 - 0.46 0.61 - 0.61 - 0.62 / 0.61 - 0.48 0.58 - 0.58 - 0.58 / 0.58 - 0.48 0.62 - 0.62 - 0.73 / 0.62 - 0.64 0.53 - 0.49 - 0.92 / 0.53 - 0.63 0.56 - 0.56 - 0.92 / 0.64 - 0.71 0.72 - 0.28 - 0.84 / 0.88 - 0.84
jewel.N 0.62 - 0.62 - 0.6 / 0.62 - 0.79 0.54 - 0.54 - 0.64 / 0.54 - 0.46 0.52 - 0.52 - 0.82 / 0.52 - 0.76 0.54 - 0.57 - 0.79 / 0.57 - 0.54
0.56 - 1.0 - 1.0 / 1.0 - 1.0 0.62 - 0.62 - 0.98 / 0.94 - 1.0 0.64 - 0.64 - 0.96 / 0.62 - 0.5 0.58 - 0.5 - 0.84 / 0.54 - 0.54 0.53 - 0.88 - 0.94 / 0.91 - 0.94 0.56 - 0.56 - 0.92 / 0.6 - 0.52 0.58 - 0.75 - 0.83 / 0.71 - 0.71 0.58 - 0.67 - 0.91 / 0.67 - 0.92 0.56 - 0.56 - 0.89 / 0.32 - 0.64 0.59 - 0.55 - 0.82 / 0.64 - 0.86 0.59 - 0.38 - 0.84 / 0.38 - 0.59
58.56 - 63.79 - 66.46 - 71.74

Focus word
shrine.N wisely.RB boxing.N podium.N ruler.N goat.N bakery.N dusty.JJ agne.N memorable.JJ holodeck.N cradle.N motto.N bond.N abnormal.JJ dioxide.N invaluable.JJ petal.N partially.RB contusion.N 0.7 - 0.3 - 0.7 / 0.7 - 0.7 barge.N imply.V gel.N chord.N compression.N morbid.N femur.N pyjama.N smither.N gullible.JJ handgun.N rod.N poacher.N coalition.N

FreqBaseline - DTree - LinearSVM - BERT
0.51 - 0.51 - 0.85 / 0.56 - 0.61 0.56 - 0.58 - 0.84 / 0.63 - 0.58 0.59 - 0.6 - 0.71 / 0.6 - 0.53 0.62 - 0.62 - 0.94 / 0.75 - 0.75 0.48 - 0.48 - 0.91 / 0.42 - 0.7 0.62 - 0.62 - 0.7 / 0.63 - 0.77 0.45 - 0.45 - 0.73 / 0.38 - 0.5 0.79 - 0.85 - 0.97 / 0.82 - 0.88 0.62 - 0.62 - 0.96 / 0.5 - 0.69 0.62 - 0.62 - 0.72 / 0.65 - 0.6 0.52 - 0.52 - 0.97 / 0.7 - 0.67 0.66 - 0.76 - 0.97 / 0.84 - 0.95 0.48 - 0.46 - 0.46 / 0.48 - 0.45 0.63 - 0.66 - 0.9 / 0.61 - 0.98 0.66 - 0.61 - 0.67 / 0.66 - 0.59 0.66 - 0.66 - 0.68 / 0.66 - 0.72 0.65 - 0.65 - 0.64 / 0.65 - 0.65 0.56 - 0.83 - 0.92 / 0.78 - 0.81 0.57 - 0.66 - 0.87 / 0.55 - 0.66 0.55 - 0.53 - 0.9 / 0.61 - 0.58
0.56 - 0.56 - 0.91 / 0.59 - 0.75 0.72 - 0.72 - 0.68 / 0.72 - 0.9 0.5 - 0.73 - 0.81 / 0.75 - 0.68 0.62 - 0.72 - 0.8 / 0.88 - 0.66 0.76 - 0.91 - 0.91 / 0.88 - 0.85 0.6 - 0.52 - 0.63 / 0.6 - 0.56 0.61 - 0.61 - 0.61 / 0.61 - 0.61 0.62 - 0.62 - 0.62 / 0.62 - 0.66 0.6 - 0.6 - 0.97 / 0.47 - 0.43 0.52 - 0.41 - 0.89 / 0.37 - 0.63 0.58 - 0.58 - 0.59 / 0.58 - 0.42 0.66 - 0.66 - 0.98 / 0.74 - 0.83 0.52 - 0.52 - 0.98 / 0.43 - 0.43 0.52 - 0.43 - 0.83 / 0.61 - 0.52

Table 8: Part-3: Lexical selection model test accuracies for a English-Greek words.

