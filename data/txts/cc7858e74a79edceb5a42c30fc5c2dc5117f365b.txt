arXiv:1806.05780v4 [cs.LG] 5 Sep 2019

Surprising Negative Results for Generative Adversarial Tree Search
Kamyar Azizzadenesheli1, Brandon Yang2, Weitang Liu3, Zachary C. Lipton4, Animashree Anandkumar1 Caltech1, Stanford University2, UC Davis3, Carnegie Mellon University4
September 6, 2019
Abstract
While many recent advances in deep reinforcement learning (RL) rely on model-free methods, modelbased approaches remain an alluring prospect for their potential to exploit unsupervised data to learn environment model. In this work, we provide an extensive study on the design of deep generative models for RL environments and propose a sample efﬁcient and robust method to learn the model of Atari environments. We deploy this model and propose generative adversarial tree search (GATS) a deep RL algorithm that learns the environment model and implements Monte Carlo tree search (MCTS) on the learned model for planning. While MCTS on the learned model is computationally expensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs deep Q network (DQN) and learns a Q-function to assign values to the leaves of the tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance trade-off and show GATS is able to mitigate the worst-case error in the Q-estimate. While we were expecting GATS to enjoy a better sample complexity and faster converges to better policies, surprisingly, GATS fails to outperform DQN. We provide a study on which we show why depth limited MCTS fails to perform desirably.
1 Introduction
The earliest and best-publicized applications of deep RL involve Atari games [36] and the board game of Go [44] which are simulated environments and experiences are inexpensive. In such scenarios, model-free deep RL methods have been used to design suitable policies. But these approaches mainly suffer from high sample complexity and are known to be biased [46, 3, 47], making them less appropriate when the experiences are expensive. To mitigate the effect of the bias, one can deploy MCTS methods [25, 27] to enhance the quality of policies by rolling out on the simulated environment. But this approach, for problems with a long horizon, e.g., Go, becomes computationally expensive. As a remedy, Alpha Go [44] propose to combine model free deep RL methods with model-based MCTS. They employ a depth-limited MCTS on the Go emulator and learn a Q-function to assign values to the leaf nodes. Despite advances in the Go game [44], this approach still does not fully address the sample complexity issue. Moreover, in real-world applications, such as robotics [29] and dialogue systems [30], not only collecting experiences takes considerable effort, but also there is no such simulator for these environments.
Recently, generative adversarial networks (GANs) [18] have emerged as a prominent tool for synthesizing realistic-seeming data, especially for high-dimensional domains, e.g., images. Unlike previous approaches
1

to image generation, which typically produced blurry images due to optimizing on L1 or L2 loss, GANs produces crisp images. GANs have been extended to conditional generation, e.g., generating images conditioned on labels [34, 37] or next frames of a video given a context window [33]. Recently, the PIX2PIX approach propose to use U-Net architecture and has demonstrated impressive results on a range of image-toimage translation tasks [22].
In this study, we use Atari games in Arcade Learning Environment (ALE) [13] as our testbed to design sample efﬁcient deep RL algorithms. We propose generative adversarial tree search (GATS), a Deep RL algorithm that learns the model of the environment and performs depth-limited MCTS on the learned model for planning. GATS consists of three main components: 1) generative dynamics model (GDM), a deep generative model that leverages PIX2PIX GANs and efﬁciently learns the dynamics of the environments. GDM uses Wasserstein distance [4] as the learning metric and deploys spectral normalization technique [35] to stabilize the training. Condition on a state and a sequence of actions, GDM produces visually crisp successor frames that agree closely with the real frames of games. 2) reward predictor (RP), a classiﬁers that predict future rewards, clipped rewards of [−1, 0, 1] in ALE, 3) a value based deep RL component to assign value to the leaf nodes of the depth limited trees in MCTS, Fig. 9. For this purpose, we use DQN [36] and DDQN [47], but any other value-based deep RL approach is also suitable.
GDM: Designing an efﬁcient and robust GDM model for RL task, in particular for Atari games is significantly challenging. The recent studies on GANs are manly dedicated to generating samples from a ﬁxed distribution, while neither fast convergence, fast adaption, nor continual learning is considered. Even in the case of ﬁxed distributions, GANs are known to be unstable and hard to train. RL problems are entirely on the opposite side of the hardness spectra. For RL problems, we need a GDM with a proper model capacity that statistically adapts quickly to the distribution changes due to policy updates and exploration. Followed by the online nature of RL, we require a GDM that computationally converges fast in the presence of changes in the data distribution, e.g. learning a new skill. More importantly, we need a GDM that continually learns the environment dynamics without diverging or becoming unstable, even once, therefore being robust. These are the critical requirements for a useful GDM. Despite the high computation cost of such design study, we thoroughly and extensively study various image translation methods, various GAN-based losses, architectures, in both feed-forward and recurrent neural networks, to design a GDM, suitable for RL tasks. As a result of this study, we propose a GDM architecture and learning procedure that reasonably satisfy all the mentioned requirements. We test the performance of GDM visually, with L1 loss, L2 loss, and also by testing a Q function on generated and real samples.
Theoretical analysis: We analyze the components of error in the estimation of the expected return in GATS, including the bias and variance in the Q-function. We empirical study the error in the Q estimate of DQN/DDQN. Since GATS deploys the Q function in the leaf nodes of the MCTS, we show that the errors in the Q-estimation disappear exponentially fast as the depth of MCTS grows.
Domain change results: In order to thoroughly test the GDM and RP, we developed a new OpenAI [15] gym-like interface for the latest Arcade Learning Environment (ALE) [32] that supports different modes and difﬁculties of Atari games. We documented and open-sourced this package along with the codes for GDM, RP, and GATS. We study the sample complexity of GDM and RP in adapting and transfer from one domain (mode and difﬁculty in ALE) to another domain. We show that the GDM and RP surprisingly are even robust to mode changes, including some that destroy the DQN policy. They adapt to the new domain with order of thousands samples, while the Q-network requires signiﬁcantly more, order of millions.
Our initial empirical studies are conducted on Pong. The comparably low computation cost of Pong allows an extensive investigation of GATS. We study MCTS with depths at most 5, even for Pong, each run 5M time
2

steps requires at least ﬁve weeks of GPU time. This is evidence of the massive computational complexity of GATS. We extend our study to four more popular Atari games.
Surprising negative results: Despite learning near-perfect environment model (GDM and RP) that achieves accuracy exceeding our expectations, GATS is unable to outperform its base model-free model DQN/DDQN on any Atari game besides Pong for which we deployed an extra substantial parameter tuning. To boost up GATS, we make an extensive and costly study with many different hyper-parameters and learning strategies, including several that uses the generated frames to train the Q-model, e.g., DynaQ Sutton [45]. We also develop various exploration strategies, including optimism-based strategy for which We use the errors in the GDM to navigate the exploration. The negative result persisted across all these innovations, and none of our developments helped to provide a modest improvement in GATS performance. Our initial hypothesis was that GATS helps to improve the performance of deep RL methods, but after the extensive empirical study and rigorous test of this hypothesis, we observe that GATS does not pass this test. We put forth a new hypothesis for why GATS, despite the outstanding performance of its components along with the theoretical advantages, might fail in short rollouts. In fact, we show that GATS locally keeps the agent away from adverse events without letting the agent learn from them, resulting in faulty Q estimation. Holland et al. [21], in fact, observe that it might require to make the depth up to the effective horizon in order to draw an improvement.
To make our study concrete, we create a new environment, Goldﬁsh and gold bucket. To exclude the effect of model estimation error, we provide GATS with the true model for the MCTS. We show that as long as the depth of the MCTS is not close to the full effective horizon of the game, GATS does not outperform DQN. This study results in a conclusion that combining MCTS with model-free Deep RL, as long as the depth of MCTS is not deep enough, might degrade the performance of RL agents. It is important to note that the theoretical justiﬁcation does not contradict our conclusion. The theoretical analysis guarantees an improvement in the worse case error in Q estimation, but not in the average performance. It also does not state that following GATS would result in a better Q estimation; in fact, the above argument states that GATS might worsen the Q estimation.
Consider that all the known successes of MCTS involve tree depth in the hundreds, e.g., 300 on Atari emulator[20]. Such deep MCTS on GDM requires massive amounts of computation beyond the scale of academic research and the scope of this paper. Considering the broader enthusiasm for both model-based RL and GANs, we believe that this study, despite its failure to advance the leaderboard of deep RL, illuminates several important considerations for future work in combining model-based and model-free reinforcement learning.
Structure: This paper consists of a long study on GATS and concludes with a set of negative results. We dedicate the main body of this paper to present GATS and its components. We leave the detail of these our developments to the supplementary section and instead devote the rest to explain the negative results. We hope that the readers ﬁnd this structure succinct, useful, and beneﬁcial.
2 Related Work
Sample efﬁcient exploration-exploitation trade-off is extensively studied in RL literature from Bandits [6], MDPs [26, 14, 5, 10, 23], and Monte Carlo sampling [25, 27] to partially observable and partial monitoring [9, 11] in low dimensional environment, leaves it open in high dimensional RL [36, 2, 8]. To extend
3

exploration-exploitation efﬁcient methods to high dimensional RL problems, Lipton et al. [31] suggests variational approximations, Osband et al. [40] suggests a bootstrapped-ensemble, Bellemare et al. [12] propose a surrogate to optimism, [17] suggest noisy exploration. Among these methods, Abbasi-Yadkori et al. [1], Azizzadenesheli et al. [7] provide insight in high dimensional linear RL problems. While most of the deep RL methods are based on model-free approaches, the model-based deep RL has been less studied.
Model based deep RL approaches requires building a notion of model for the reasoning. Recently, conditional video predictions based on L1 and L2 losses [38] have been deployed by Weber et al. [49] who train a neural network to encodes the generated trajectories into an abstract representation, which then is used as an additional input to the policy model. They validate their methods on Sokoban, a small puzzle world, and their miniPacman environment. Unlike GATS, Weber et al. [49] does not have explicit planning and roll-out strategies. A similar approach to GATS is concurrently developed using variational methods and empirically studied on Car Racing and VizDoom [16]. Other works propose to roll out on a transition model learned on a encoded state representation [39], and demonstrate modest gains on Atari games. A similar approach also has been studied in robotics [48]. In contrast, we learn the model dynamics in the original state/pixel space.
Compared to the previous works, GATS propose a ﬂexible, modular, and general framework for the study of model-based and model-free RL with four building blocks: (i) value learning (ii) planning (iii) reward predictor, and (iv) dynamics model. This modularity provides a base for further study and adaptation in future works. For instance, for value learning (i): one can use Count-based methods [12]. For planning (ii): one can use upper conﬁdence bound tree search (UCT) [27] or policy gradient methods [24, 42]. For the reward model (iii): if the reward is continuous, one can deploy regression models. Lastly, for model dynamics (iv), one can extend GDM or choose any other generative model. Furthermore, this work can also be extended to the λ-return setting, where a mixture of n steps MCTS and Q is desired. Although GATS provides an appealing and ﬂexible RL paradigm, it suffers from massive computation cost by modeling the environment. Clearly, such costs are acceptable when experiences in the real world are expensive. Potentially, we could mitigate this bottleneck with parallelization or distilled policy methods [20].

3 Generative Adversarial Tree Search

Bias-Variance Trade-Off: Consider an MDP M = X , A, T, R, γ , with state space X , action space A, transition kernel T , reward distribution R with [0, 1]-bounded mean, and discount factor 0 ≤ γ < 1. A policy π is a mapping from state to action and Qπ(x, a) denotes the expected return of action a at state x, then following policy π. Following the Bellman equation, the agent can learn the Q function by minimizing the following loss over samples to improve its behavior:

(Q(x, a) − Eπ [r + γQ(x′, a′)| x, a])2

(1)

Since in RL we do not have access internal expectation, we instead minimize the Bellman residual [43][28, 3] which has been extensively used in DRL literature [36, 47]:

Eπ (Q(x, a)−(r + γQ(x′, a′)))2 x, a = Q(x, a)−Eπ r+γQ(x′,a′) x, a

2
+Varπ r+γQ(x′, a′) x, a

4

This is the sum of Eq. 1 and an additional variance term, rustling in a biased estimation of Q function. DQN deploys a target network to slightly mitigate this bias [47] and minimizes:

L(Q, Qtarget) = Eπ Q(x, a)− r−γQtarget(x′, a′) 2

(2)

In addition to this bias, there are statistical biases due to limited network capacity, optimization, model mismatch, and max operator or the choice of a′ [46]. Let · denote an estimate of a given quantity. Deﬁne
the estimation error in Q function (bias+variance), the reward estimation r by RP, and transition estimation T by GDM as follows: ∀x, x′, a ∈ X , A

|Q(x, a) − Q(x, a)| ≤ eQ,
a

r(x, a) − r(x, a) ≤ eR,
x′

T (x′|x, a) − T (x′|x, a) ≤ eT

For a rollout policy πr, we compute the expected return using GDM, RP and Q as follows:

ξp(πr, x) := Eπr,GDM,RP

H−1
γ h rh
h=0

+ γH max Q(xH , a) x
a

The agent can compute ξp(πr, x) with no interaction with the environment.

Deﬁne ξ(πr, x) := Eπr

H−1 h=0

γ h rh

+ γH maxa Q(xH , a) x .

Proposition 1 [Model-based Model-free trade-off] Building a depth limited MCTS on the learned model, GDM,RP, and estimated Q function, Q, results in expected return of ∀x, πr:

|ξp(πr, x) − ξ(πr, x)| ≤ 1 − γH + HγH (1 − γ) eT + 1 − γH eR + γH eQ, ∀x ∈ X , ∀πr (3)

(1 − γ)2

1−γ

Proof in the Appendix A

Therefore, as the depth of the rollout increases, the effect of error in Q function approximation disappears exponentially at the cost of magnifying the error in GDM and RP. In the next sections, we show that in fact, the error in GDM and RP are surprisingly low (refer to Appendix B).
Model: Algorithm 1 present GATS, Fig. 9. We parameterize GDM with θGDM, and RP with θRP. We adopt the standard DQN architecture and game settings [36]. We train a DQN model as well as GDM and RP on samples from replay buffer. To train GDM, we deploy Wasserstein distance along with L1 and L2 loss, in the presence of spectrally normalized discriminator. For exploration and exploitation trade-off, we deploy ǫgreedy, the strategy used in DQN. We also propose a new optimism-based explore/exploit method using the discriminator loss. We empirically observed that computed Wasserstein distance by the discriminator is high for barely visited state-action pairs and low otherwise. Inspired by pseudo-count construction in Ostrovski et al. [41], we deploy Wasserstein distance as a notion of count N˜ (x, a). Following the construction of optimism in the face of uncertainty [23], we utilize N˜ (x, a) to guide the agent to explore unknown parts of the state space. We deﬁne the optimistic (in Wasserstein sense) Q˜ as Q˜π(x, a) = Qπ(x, a) + Cπ(x, a) where Cπ(x, a) is

c 1/N˜ (x, a) + γ T (x′|x, a)Cπ(x′, π(x′))

(4)

x′

with a scaling factor c. We train a separate DQN to learn the C model, the same way we learn Q. We use C for planning: maxπ{ξ(π, x) + C(π, x)} instead of ǫ-greedy to guide exploration/exploitation.

5

Algorithm 1 GATS (H)

1: Initialize parameter sets θ, θtarget, θGDM, θRP, m, replay buffer and set counter = 0

2: for episode = 1 to inf do

3: for t = 1 to the end of episode do

4:

Compute at and sample {(xi, ai, ri, xi+1)}m 0 from M CT S(xt, H, θ, θGDM, θRP)

5:

Set the real (xt, at, rt, xt+1) and generated {(xi, ai, ri, xi+1)}m 0 to the replay buffer

6:

Sample a minibatch of experiences (xτ , aτ , rτ , xτ+1)

7: yτ← rrττ + maxa′ Q(xτ+1, a′; θtarget) tneornm-itneraml inal

8:

θ ← θ − η · ∇θ(yτ − Q(xτ , aτ ; θ))2

9:

Update GDM, and RP

4 Experiments
We study the performance of GATS with depth of at most ﬁve on ﬁve Atari games; Pong, Asterix, Breakout, Crazy Climber and Freeway. For the GDM architecture, Fig. 11, we build upon the U-Net model of the imageto-image generator originally used in PIX2PIX [22]. The GDM receives a state, sequence of actions, and a Gaussian noise vector and generates the next states.1 The RP is a simple model with 3 outputs, it receives the current state, action, and the successor state as an input and outputs a label, one for each possible clipped reward {−1, 0, 1}. We train GDM and RP using prioritized weighted mini-batches of size 128 (more weight on recent samples), and update the two networks every 16 decision steps (4 times less frequently than the Q update).2
Our experiments show that with less than 100k samples the GDM learns the environment’s dynamics and generalizes well to a test set (DQN requires around 5M . We also observe that it adapts quickly even if we change the policy or the difﬁculty or the mode of the domain. Designing GDM model is critical and hard since we need a model which learns fast, does not diverge, is robust, and adapts fast. GAN models are known to diverge even in iid sample setting. It become more challenging, when the GDM is condition on past frames as well as future actions, and needs to predict future given its out generated samples. In order to develop our GDM, we experimented many different model architectures for the generator-discriminator, as well as different loss functions.
Since the L1 and L2 losses are not good metrics for image generating models, we evaluated the performance of GDM visually also by applying Q function on generated test sample, Appendix. E.1, Fig. 12. We studied PatchGAN discriminator (patch sizes 1, 16, and 70) and L1 loss used in PIX2PIX [22], ﬁnding that this architecture takes approximately 10× more training iterations to learn game dynamics for Pong than the current GDM. This is likely since learning the game dynamics such as ball position requires the entire frame for the discriminator, more than the patch-based texture loss given by PatchGAN. Previous works propose ACVP and train large models with L2 loss to predict long future trajectories of frames given actions [38]. We empirically studied these methods. While GDM is much smaller, we observe that it requires signiﬁcantly fewer iterations to converge to perceptually unidentiﬁable frames. We also observed signiﬁcantly lower error for GDM when a Q function is applied to generated frames from both models. ACVP also struggles to produce meaningful frames in stochastic environments(Appendix E.2).
1See the Appendix for the detailed explanation on the architecture and optimization procedure. 2For the short lookhead in deterministic environment, we expand the whole tree.

6

For the choice of the GAN loss, we ﬁrst tried the original GAN-loss [18], which is based on Jensen–Shannon divergence. With this criterion, not only it is difﬁcult to ﬁnd the right parameters but also not stable enough for non-stationary domains. We did the experiments using this loss and trained for Pong while the resulting model was not stable enough for RL tasks. The training loss is sometimes unstable even for a given ﬁxed data set. Since, Wasserstein metric provides Wasserstein distance criterion and propose a more general loss in GANs, we deployed W-GAN [4] for our GDM. Because W-GAN requires the discriminator to be a bounded Lipschitz function, the authors adopt gradient clipping. Using W-GAN results in an improvement but still not sufﬁcient for RL where fast and stable convergence is required.
In order to improve the learning stability, parameter robustness, and quality of frames, we also tried a followup work on improved-W-GAN [19], which adds a gradient penalty into the loss of discriminator in order to satisfy the bounded Lipschitzness. Even though it made the GDM more stable than before, it was still not sufﬁcient due to the huge instability in the loss curve. Finally, we tried spectral normalization [35], a recent technique that not only provides high-quality frames but also converges quickly while the loss function stays smooth. We observed that this model, is stability, robustness to hyperparameter choices, and fast learning thank to spectral normalization combined with W-GAN in the presence of L1 and L2 losses. We deploy this approach in GDM. For the training, we trained GDM to predict the next frame and generate future frames given its own generated frames. We also trained it to predict multiple future frames. Moreover, we studied both feed-forward and recurrent version of it. Furthermore, we used both one step loss and multi-step loss. After these study, we observed that the feed-forward model which predicts the next state and trained on the loss of the next three frames generalizes to longer rollout and unseen data. More detailed study is left to Appendix. It is worth noting that the code to all these studies are publicly available (Appendix E).
Fig. 3 shows the effectiveness of GDM and how accurate it can generate next 9 frames just conditioning on the previous 4 frames and a sequnce of actions. We train GDM using 100,000 frames and a 3-step loss, and evaluate its performance on 8-step roll-outs on unseen 10,000 frames. Moreover, we illustrate the tree constructed by MCTS in Figs. 13, 14. We also test a learned Q function on both generated and real frames and observed that the relative deviation is signiﬁcantly small (O(10−2)). Furthermore, we constructed the tree in MCTS As deep RL methods are data hungry, we can re-use the data generated by GDM to train the Q-function even more. We also study ways we can incorporate the generated samples by GDM-RP to train the Q-function, similar to Dyna-Q Fig. 4.
Shift in Domain We extend our study to the case where we change the game mode. In this case, change Pong game mode, the opponent paddle gets halved in size, resulting in much easier game. While we expect a trained DDQN agent to perform well on this easier game, surprisingly we observe that not only the agent breaks, but also it provides a score of -21, the most negative score possible. While mastering Pong takes 5M step from DDQN, we expected a short ﬁne tuning would be enough for DDQN to adapt to this new domain. Again surprisingly, we observe that it take 5M times to step for this agent to adapt to this new domain. It means that DDQN clearly overﬁt to the original domain. While DDQN appears unacceptably brittle in this scenario, GDM and RP adapt to the new model dynamics in less than 3k samples, which is signiﬁcantly smaller (Appendix E.1). As stated before, for this study, we wrote a Gym-style wrapper for the new version of ALE which supports different modes of difﬁculty levels. This package is publicly available.
7

5 Discussion

Our initial hypothesis was that GATS enhances model-free approaches. With that regard, we extensively

studied GATS to provide an improvement. But, after more than a year of unsuccessful efforts by multiple

researchers in pushing for improvement, we reevaluated GATS from the ﬁrst principles and realized that this

approach, despite near-perfect modeling, surprisingly might not even be capable of offering any improve-

ment. We believe this negative result would help to shape the future study of model-based and model-free

RL. In the following, we describe our efforts and conclude with our ﬁnal hypothesis on the negative result,

Table 1.

Table 1: Set of approaches explored to improve GATS performance

Replay Buffer

Optimizer

(i) Plain DQN (i) Learning rate (ii) Dyna-Q (ii) Mini-batch size

Sampling strategies
(i) Leaf nodes (ii) Random samples from the tree (iii) Samples by following greedy Q (iv) Samples by following ε-greedy Q
(v) Geometric distribution

Optimism
(i)W-loss (ii) exp(W-loss) (iii) L1+L2+W-distance (iv) exp(L1+L2+W-distance)

Replay Buffer: The agent’s decision under GATS sometimes differs from the decision of the model-free component. To learn a reasonable Q-function, the Q-leaner needs to observe the outcome of its own decision. To address this problem, we tried storing the samples generated in the tree search and use them to further train the Q-model. We studied two scenarios: (i) using plain DQN with no generated samples and (ii) using Dyna-Q approach and train the Q function also on the generated samples in MCTS. However, these techniques did not improve the performance of GATS. Optimizer: Since GATS, specially in the presence of Dyna-Q is different approach from DQN, we deploy a further hyper parameter tuning on learning rate and mini-batch size.
Sampling strategy: We considered a variety ways to exploit the samples generated in tree search for further learning the Q. (i) Since we use the Q-function at the leaf nodes, we further train the Q-function on generated experience at the leaf nodes. (ii) We randomly sampled generated experience in the tree to further learn the Q. (iii) We choose the generated experience following the greedy action of the Q-learner on the tree, the trajectory that we would have received by following the greedy decision of Q (counterfactual branch). We hypothesized that training the Q-function on its own decisions results in an improvement. (iv) We also considered training on the result of ε-greedy policy instead of the greedy. (v) Finally, since deeper in the tree has experiences with bigger shift the policy, we tried a variety of different geometric distributions to give more weight of sampling to the later part of the tree to see which one was most helpful.
Optimism: We observed that parts of the state-action space that are novel to the GDM are often explored less and result in higher discriminator errors. We added (i) the W -loss and also (ii) its exponent as a notion of external reward to encourage exploration/exploitation. In (iii) and (iv) We did the same with the summation of different losses, e.g., L1, L2.
Despite this extensive and costly study on GATS, we were not able to show that GATS, Fig. 1 beneﬁts besides a limited improvement on Pong (Appendix C,D).

Hypothesis on negative results: In the following we reason why GATS with short depth might not boost the performance even with perfect modeling with GDM and RP, despite reducing local bias. Consider a

8

Figure 1: GATS and GATS +Dyna with 1 and 4 step look-ahead.

(a)

(b)

(c)

Figure 2: The Goldﬁsh looks for a Gold bucket. The Q function is initialized such that the yellow arrows represent the greedy action of each state. The red arrows are the actions suggested by MCTS depth two. GATS is given the true model(a) GATS locally prevents the goldﬁsh from hitting the sharks but also prevents learning from such events, therefore slows down the learning.(b) Even if the goldﬁsh uses the prediction of the future event for further learning, as in Dyna-Q,the slow down issue still persists.(c) For a grid world of 10 × 10, and randomly initialized Q function, GATS with depth of 10 (GATS-10) results in the highest return. Moreover, GATS with nonzero depth locally saves the agent from hitting the sharks, but in the long run it degrades the performance.

toy example described in Fig. 2(a) where a ﬁsh starts with an initialization of the Q function such that the greedy action is represented in yellow arrows. We give the true model to GATS (no modeling error). If the ﬁsh follows DQN, it reaches the sharks quickly and receives a negative reward, update the Q function and learns the down action is not a good action. Now consider the depth 2 GATS with the same Q-function initialization. When the agent reaches the step above the sharks, the MCTS roll-out informs the agent that there is a negative reward of going down and the agent chooses action right, following the GATS action (red arrows). The agent keeps following the red arrows until it dies following ε-greedy. GATS locally avoid bad states, but does this information globally get propagated? Consider that this negative reward happens much later in the trajectory than for a DQN agent. Therefore, many more updates are required for the agent to learn that action down is not a good action while this negative signal may even vanish due to the long update lengths. This shows how GATS roll-outs can locally help to avoid (or reach) catastrophic (or good) events, but slow down global understanding.
As we suggested in our negative results, this problem can be solved by also putting the generated experiences in the replay buffer following Dyna-Q. Fig. 2(b) illustrates the situation where the GATS action in the third row before the sharks is “right”. Therefore, similar to the previous setting, the agent keeps avoiding the sharks, choosing action right over and over and do not experience the shark negative signal. In this case, two-step roll-outs do not see the sharks, and thus Dyna-Q does not speed up the learning. In practice, especially with limited memory of the replay buffer and limited capacity of the function classes, it can be also difﬁcult to tune Dyna-Q to work well.

9

To empirically test this hypothesis, we implemented the 10x10 version of Goldﬁsh and gold bucket environment where the GATS agent has access to the true model of environment. We tested GATS with depths of 0 (i.e., the plain DQN), 1, 2, 4 and GATS +Dyna-Q with the depth of 1 and 2 (even for this simple environment GATS is computationally unpleasantly massive (two weeks of cpu machine for GATS − 4)). We use a randomly initialized Q network to learn the Q function. Figure 2(c) represents the per episode return of different algorithms. Each episode has a maximum length of 100 steps unless the agent either reaches the gold bucket (the reward of +1) or hit any of the sharks (the reward of −1). Here the discount factor is 0.99 and cost of living is 0.05. As expected, GATS with the depth of 10 (the dimension of the grid) receives the highest return. We observe that GATS with nonzero depth locally saves the agent and initially result in higher returns than DQN. However, in the long run, GATS with short roll-outs (e.g. GATS-1 and GATS-2) degrade the performance, as seen in the later parts of the runs. Furthermore, we observe that the Dyna-Q approach also fails in improving performance. We train GATS+Dyna-Q with both executed experiences and predicted ones. We observe that GATS+Dyna-Q does not provide much beneﬁt over GATS. Consider that generated samples in the tree and real samples in GATS+Dyna-Q are similar, resulting in repeated experiences in the replay buffer. Concolusion: For many complex applications, like hard Atari games, GATS may require signiﬁcantly longer roll-out depths with sophisticated Dyna-Q in order to perform well, which is computationally in-feasible for this study. However, the insights in designing near-perfect modeling algorithms and the extensive study of GATS, highlight several key considerations and provide an important framework to effectively design algorithms for combining model-based and model-free reinforcement learning.
Acknowledgments
K. Azizzadenesheli is supported in part by NSF Career Award CCF-1254106 and AFOSR YIP FA9550-15-10221. This research has been conducted when the ﬁrst author was a visiting researcher at Stanford University and Caltech. A. Anandkumar is supported in part by Bren endowed chair, Darpa PAI, and Microsoft, Google, Adobe faculty fellowships, NSF Career Award CCF-1254106, and AFOSR YIP FA9550-15-1-0221. All the experimental study have been done using Caltech AWS credits grant.
10

References
[1] Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. (2011). Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pages 2312–2320.
[2] Abel, D., Agarwal, A., Diaz, F., Krishnamurthy, A., and Schapire, R. E. (2016). Exploratory gradient boosting for reinforcement learning in complex domains. arXiv.
[3] Antos, A., Szepesvári, C., and Munos, R. (2008). Learning near-optimal policies with bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning.
[4] Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein gan. arXiv preprint arXiv:1701.07875.
[5] Asmuth, J., Li, L., Littman, M. L., Nouri, A., and Wingate, D. (2009). A bayesian sampling approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence.
[6] Auer, P. (2003). Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397–422.
[7] Azizzadenesheli, K., and Anandkumar, A. (2018). Efﬁcient exploration through bayesian deep qnetworks. arXiv preprint arXiv:1802.04412.
[8] Azizzadenesheli, K., Lazaric, A., and Anandkumar, A. (2016a). Reinforcement learning in richobservation mdps using spectral methods. arXiv preprint arXiv:1611.03907.
[9] Azizzadenesheli, K., Lazaric, A., and Anandkumar, A. (2016b). Reinforcement learning of pomdps using spectral methods. In Proceedings of the 29th Annual Conference on Learning Theory (COLT).
[10] Bartlett, P. L. and Tewari, A. (2009). REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In Proceedings of the 25th Annual Conference on Uncertainty in Artiﬁcial Intelligence.
[11] Bartók, G., Foster, D. P., Pál, D., Rakhlin, A., and Szepesvári, C. (2014). Partial monitoring—classiﬁcation, regret bounds, and algorithms. Mathematics of Operations Research.
[12] Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471–1479.
[13] Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR).
[14] Brafman, R. I. and Tennenholtz, M. (2003). R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. The Journal of Machine Learning Research, 3:213–231.
[15] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym.
[16] David Ha, J. S. (2018). World models. arXiv preprint arXiv:1803.10122.
[17] Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.
11

[18] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680.
[19] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769–5779.
[20] Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. (2014). Deep learning for real-time atari game play using ofﬂine monte-carlo tree search planning. In Advances in neural information processing systems, pages 3338–3346.
[21] Holland, G. Z., Talvitie, E. J., and Bowling, M. (2018). The effect of planning shape on dyna-style planning in high-dimensional state spaces. arXiv preprint arXiv:1806.01825.
[22] Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.
[23] Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research.
[24] Kakade, S. M. (2002). A natural policy gradient. In Advances in neural information processing systems.
[25] Kearns, M., Mansour, Y., and Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large markov decision processes. Machine Learning, 49(2-3):193–208.
[26] Kearns, M. and Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209–232.
[27] Kocsis, L. and Szepesvári, C. (2006). Bandit based monte-carlo planning. In Machine Learning: ECML 2006, pages 282–293. Springer.
[28] Lagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. Journal of machine learning research, 4(Dec):1107–1149.
[29] Levine et al., S. (2016). End-to-end training of deep visuomotor policies. JMLR.
[30] Lipton, Z. C., Azizzadenesheli, K., Gao, J., Li, L., Chen, J., and Deng, L. (2016). Combating reinforcement learning’s sisyphean curse with intrinsic fear. arXiv preprint arXiv:1611.01211.
[31] Lipton, Z. C., Gao, J., Li, L., Li, X., Ahmed, F., and Deng, L. (2018). Efﬁcient exploration for dialogue policy learning with bbq networks & replay buffer spiking. AAAI.
[32] Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. (2017). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. arXiv preprint arXiv:1709.06009.
[33] Mathieu, M., Couprie, C., and LeCun, Y. (2015). Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440.
[34] Mirza, M. and Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.
[35] Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. (2018). Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957.
12

[36] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature.
[37] Odena, A., Olah, C., and Shlens, J. (2016). Conditional image synthesis with auxiliary classiﬁer gans. arXiv preprint arXiv:1610.09585.
[38] Oh, J., Guo, X., Lee, H., Lewis, R. L., and Singh, S. (2015). Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems.
[39] Oh, J., Singh, S., and Lee, H. (2017). Value prediction network. In Advances in Neural Information Processing Systems, pages 6120–6130.
[40] Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. (2016). Deep exploration via bootstrapped dqn. In Advances in Neural Information Processing Systems.
[41] Ostrovski, G., Bellemare, M. G., Oord, A. v. d., and Munos, R. (2017). Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310.
[42] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15).
[43] Schweitzer, P. J. and Seidmann, A. (1985). Generalized polynomial approximations in markovian decision processes. Journal of mathematical analysis and applications, 110(2):568–582.
[44] Silver et al., D. (2016). Mastering the game of go with deep neural networks and tree search. Nature. [45] Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximat-
ing dynamic programming. In Machine Learning Proceedings 1990, pages 216–224. Elsevier. [46] Thrun, S. and Schwartz, A. (1993). Issues in using function approximation for reinforcement learning.
In Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum. [47] Van Hasselt, H., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double q-learning.
In AAAI. [48] Wahlström, N., Schön, T. B., and Deisenroth, M. P. (2015). From pixels to torques: Policy learning
with deep dynamical models. arXiv preprint arXiv:1502.02251. [49] Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., Badia, A. P., Vinyals,
O., Heess, N., Li, Y., et al. (2017). Imagination-augmented agents for deep reinforcement learning. arXiv.
13

Appendix A Proof of Proposition 1
Let’s restate the estimated returns with the learned model as follows;

Eπr ,G D M,R P

H−1
γhrh + γH max Q(xH , a) x
a h=0

H

:=

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1)

xi,ai,∀i∈[1,.,H]

j=2





H

r(x, a1) + γj−1r(xj−1, aj) + γH max Q(xH , a)
a j=2

Now consider the following lemma;
Lemma 1 (Deviation in Q-function) Deﬁne eQ as the uniform bound on error in the estimation of the Q function, such that |Q(x, a) − Q(x, a)| ≤ eQ , ∀x, a. Then;

max Q(x, a) − max Q(x, a) ≤ eQ

a

a

Proof 1 For a given state x, deﬁne a1(x) := arg maxa Q(x, a) and a2(x) := arg maxa Q(x, a). Then;

Q(x, a1(x)) − Q(x, a2(x)) = Q(x, a1(x)) − Q(x, a1(x)) + Q(x, a1(x)) − Q(x, a2(x)) ≤ Q(x, a1(x)) − Q(x, a1(x)) ≤ eQ

since Q(x, a1(x)) − Q(x, a2(x)) ≤ 0. With similar argument, we have;

Q(x, a1(x)) − Q(x, a2(x)) = Q(x, a1(x)) − Q(x, a2(x)) + Q(xH , a2(x)) − Q(x, a2(x)) ≥ Q(x, a2(x)) − Q(x, a2(x)) ≥ −eQ

since Q(x, a1(x)) − Q(x, a2(x)) ≥ 0. Therefore;
−eQ ≤ Q(x, a1(x)) − Q(x, a2(x)) ≤ eQ
resulting in Lemma 1. In the remaining proof, we repeatedly apply the addition and subtraction technique to upper bound the error. To show how we apply this technique, we illustrate how we derive the error terms T (x1|x, a1) − T (x1|x, a1) and r(x, a1) − r(x, a1) for the ﬁrst time step in detail. Let us restate the objective that we desire to upper bound:

14

H−1

H

Eπr

γhrh + γH max Q(xH , a) x − Eπr,GDM,RP

γhrh + γH max Q(xH , a) x

a

a

h=0

h=0 



H

H

=

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj)πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

(5)

Then, we add and subtract the following term.





H

H

T (x1|x, a1)πr(a1|x) T (xj|xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

Notice this term differs from the ﬁrst term of Eq. 5 just in the transition kernel of the ﬁrst time step, i.e., T (x1|x, a1) −→ T (x1|x, a1). Thus, we have;

H−1

H

Eπr

γhrh + γH max Q(xH , a) x − Eπr,GDM,RP

γhrh + γH max Q(xH , a) x

a

a

h=0

h=0 



H

H

=

T (x1|x, a1)πr(a1|x) T (xj|xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

+

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

(6)

We derive the error term T (x1|x, a1) − T (x1|x, a1) from the ﬁrst two terms of Eq. 6. Notice that all the parameters of the ﬁrst two terms are the same except the transition kernel for the ﬁrst state. We can thus refactor the ﬁrst two terms of Eq. 6 as:

15

T (x1|x, a1) − T (x1|x, a1)
xi,ai,∀i∈[1,.,H]

H

πr(a1|x) T (xj|xj−1, aj )πr(aj |xj−1)

j=2





H

r(x, a1) + γj−1r(xj−1, aj) + γH max Q(xH , a)
a j=2

We then expand the third and fourth terms of Eq. 6 to derive the error term r(x, a1)−r(x, a1) and a remainder. To do this, we add and subtract the following term which is the same as the third term in Eq. 6 except it differs in the reward of the ﬁrst time step:





H

H

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj)πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

We can thus express the third and fourth terms of Eq. 6 along with the addition and subtraction terms

as:





H

H

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj)πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

+

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

(7)

Notice that the ﬁrst two terms in Eq. 7 are the same except in in the ﬁrst reward term, from which we derive the error term r(x, a1) − r(x, a1). We refactor the ﬁrst two terms in Eq. 7 as:

H

(r(x, a1) − r(x, a1))T (x1|x, a1)πr(a1|x) T (xj|xj−1, aj)πr(aj |xj−1)

xi,ai,∀i∈[1,.,H]

j=2

16

Finally, we have the remaining last two terms of Eq. 7.





H

H

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj)πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2





H

H

−

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj )πr(aj |xj−1) r(x, a1) + γj−1r(xj−1, aj ) + γH max Q(xH , a)

a

xi,ai,∀i∈[1,.,H]

j=2

j=2

We repeatedly expand this remainder for the following time steps using the same steps as described above to derive the full bound. Following this procedure, we have:

H−1

H

Eπr

γhrh + γH max Q(xH , a) x − Eπr,GDM,RP

γhrh + γH max Q(xH , a) x

a

a

h=0

h=0

≤

T (x1|x, a1) − T (x1|x, a1) πr(a1|x)

xi,ai,∀i∈[H]





H

H

r(x, a1) + γj−1r(xj−1, aj) + γH max Q(xH , a) T (xj|xj−1, aj )πr(aj |xj−1)

a

j=2

j=2

H

+

T (x1|x, a1)πr(a1|x) (|r(x, a1) − r(x, a1)|) T (xj|xj−1, aj)πr(aj|xj−1)

xi,ai,∀i∈[H]

j=2

H

+

T (x1|x, a1)πr(a1|x) T (xj |xj−1, aj ) −T (xj |xj−1, aj )

j=2 xh,ah,∀i∈[H]





H



γh−1r(xh−1, ah) + γH max Q(xH , a)

a

h=j+1

j−1

H

T (xh|xh−1, ah)πr(ah|xh−1)

T (xh|xh−1, ah)πr(ah|xh−1)

h=2

h=j+1

H
+ γj−1

T (x1|x, a1)πr(a1|x)

j=2

xh,ah,∀i∈[H]

r(xj−1, aj) − r(xj−1, aj)

j

H

T (xh|xh−1, ah)πr(ah|xh−1)

T (xh|xh−1, ah)πr(ah|xh−1)

h=2

h=j+1

H

+

T (x1|x, a1)πr(a1|x) T (xh|xh−1, ah)πr(ah|xh−1)γH max Q(xH , a) − max Q(xH , a))

a

a

xh ,ah ,∀i∈[H ]

h=2

17

As a result,

H−1

H−1

Eπr

γhrh + γH max Q(xH , a) x − Eπr,GDM,RP

γhrh + γH max Q(xH , a) x

a

a

h=0

h=0

≤ H γi−1 1 −1γ−H+γ1−i eT + H γi−1eR + γH eQ

i=1

i=1

≤ 1 − γH + HγH (1 − γ) eT + 1 − γH eR + γH eQ

(1 − γ)2

1−γ

Therefore, the proposition follows.

B Bias-Variance in Q function
To observe the existing bias and variance in Qθ, we run solely DQN on the game Pong, for 20M frame steps. Fig. 4 shows 4 consecutive frames where the agent receives a negative score and Table. 4 shows the estimated Q values by DQN for these steps. As we observe in Fig. 4 and Table. 4, at the time step t, the estimated Q value of all the actions are almost the same. The agent takes the down action and the environment goes to the next state t + 1. The second row of Table. 4 expresses the Q value of the actions at this new state. Since this transition does not carry any reward and the discount factor is close to 1, (γ = 0.99) we expect the max Q values at time step t + 1 to be close the Q values of action down, but it is very different. Moreover, in Fig. 5 and Table. 5 we investigate the case that the agent catches the ball. The ball is going to the right and agent needs to catch it. At time step t, the paddle is not on the direction of ball velocity, and as shown in Table. 5, the optimal action is down. But a closer look at the estimated Q value of action up reveals that the Q value for both action up is unreasonably close, when it could lead to losing the point. Lastly, we studied the existing errors in the estimation of the Q function using DQN. In Table.4, if the agent could roll-out even one step before making a decision, it could observe negative consequence of action down. The positive effect of the roll-out is more signiﬁcant in earlier stages of Q learning, where the Q estimation is more off.

C GATS on Pong
We run GATS with 1, 2, 3, and 4 steps lookahead (GAT S1, GAT S2, GAT S3, GAT S4) and after extensive hyper parameter tuning for the DQN model we show the GATS performance improvement over DQN in Fig. 6 (left). Fig. 6 (right) shows the RP prediction accuracy. We observe that when the transition phase occurs at decision step 1M, the RP model mis-classiﬁes the positive rewards. But the RP rapidly adapts to this shift and reduces the classiﬁcation error to less than 2 errors per episode.
As DRL methods are data hungry, we can re-use the data to efﬁciently learn the model dynamics. Fig. 3 shows how accurate the GDM can generate next 9 frames just conditioning on the ﬁrst frame and the trajectory of actions. This trajectory is generated at decision step 100k. Moreover, we extend our study to the case where we change the model dynamics by changing the game mode. In this case, by going from default mode to alternate mode in pong, the opponent paddle gets halved in size. We expected that in this case, where the game became easier, the DDQN agent would preserve its performance but surprisingly it gave us the most negative score possible, i.e -21 and broke. Therefore, we start ﬁne tuning DDQN and took 3M
18

time step (12M frame) to master the game again. It is worth noting that it takes DDQN 5M time step (20M frame) to master from scratch. While DDQN shows a vulnerable and undesirably breakable behaviour to this scenario, GDM and RP thanks to their detailed design, adapt to the new model dynamics in 3k samples, which is amazingly smaller (see more details in E.1)
In addition to GATS on DQN, we also study two other set of experiments on DDQN. Since Fig. 6 shows that the deeper roll-outs beyond one step do not provide much additional beneﬁt for Pong, we focus on one-step roll-outs for the next two experiments. In the ﬁrst experiment, we equip GATS + DDQN with the mentioned Wasserstein-optimism approach, and compare it with DDQN and plain GATS + DDQN, which both use εgreedy based approaches for exploration. In Fig. 7left, we observe that this optimism heuristic is helpful for better exploration.
In the second experiment, we investigate the effect of prioritizing training samples for the GDM, fresher samples are more probable to be chosen, which we do in all experiments reported in Fig. 7left. We study the case where the input samples to GDM are instead chosen uniformly at random from the replay buffer in Fig. 7right. In this case the GATS learns a better policy faster at the beginning of the game, but the performance stays behind DDQN, due to the shift in the state distribution. It is worth mentioning that for optimism based exploration, there is no ε-greedy, which is why it gets close to the maximum score of 21. We tested DDQN and GATS-DDQN with ε = 0, and they also perform close to 21. We further extend the study of GDM to more games 3 and observed same robust behaviour as Pong. We also tried to apply GATS to more games, but were not able to extend it due to mainly its high computation cost. We tried different strategies of storing samples generated by MCTS, e.g. random generated experience, trajectory followed by Q on the tree, storing just leaf nodes, max leaf, also variety of different distributions, e.g. geometric distributing, but again due the height cost of hyper parameter tuning we were not successful to come up with a setting that GATS works for other games

D Asterix and Breakout Negative Results
We include the results for GATS with 1 step look-ahead (GATS-1) and compare its performance to DDQN as an example for the negative results we obtained with short roll-outs with the GATS algorithm. While apply the same hyper parameters we tuned for pong, for Asterix results in performance slightly above random policy, we re-do the hyper-parameter tuning speciﬁcally for this game again and Fig. 8 is the best performance we achieved.
This illustrates the challenges of learning strong global policies with short roll-outs even with near-perfect modeling.

E GDM Architecture and parameters

For the generative dynamic model, we propose a generic GDM which consists of a generator G and a discriminator D, trained adversarially w.r.t. the extended conditional Wasserstein metric between two probability measures P̟, PG under a third probability measure P;

W (P̟, PG; P) := supD∈ · L E̟∼P̟|̺,̺∼P[D(̟|̺)] − E̟:G(̺∼P,z∼N (0,I))[D(̟|̺)]

(8)

19

Here, z is a mean-zero unit-variance Gaussian random vector and · L indicates the space of 1-Lipschitz functions. In GDM, D solves the interior sup, while G’s objective is to minimize this distance and learn the P̟|̺ for all ̺. In GATS, P is the distribution over pairs of ̺ : (x, a) in the replay buffer, and P̟|̺ is the distribution over the successor states ̟ : x′, as the transition kernel T (x′|x, a).
The GDM model consists of seven convolution and also seven deconvolution layers. Each convolution layer is followed by Batch Normalization layers and the leaky ReLU activation function with negative slope of −0.2. Also each deconvolution layer is followed by a Batch Normalization layer and the ReLU activation instead of leaky RELU. The encoder part of the network uses channel dimensions of 32, 32, 64, 128, 256, 512, 512 and kernel sizes of 4, 4, 4, 4, 2, 2, 2. The reverse is true for the decoder part. We concatenate the bottleneck and next 5 deconvolution layers with a random Gaussian noise of dimension 100, the action sequence, and also the corresponding layer in the encoder. The last layer of decoder is not concatenated. Fig. 11. For the discriminator, instead of convolution, we use SN-convolution [35] which ensures the Lipschitz constant of the discriminator is below 1. The discriminator consists of four SN-convolution layers followed by Batch Normalization layers and a leaky RELU activation with negative slope of −0.2. The number of channels increase as 64, 128, 256, 16 with kernel size of 8, 4, 4, 3, which is followed by two fully connected layers of size 400 and 18 where their inputs are concatenated with the action sequence. The output is a single number without any non-linearity. The action sequence uses one hot encoding representation.
We train the generator using Adam optimizer with weight decay of 0.001, learning rate of 0.0001 and also beta1, beta2 = 0.5, 0.999. For the discriminator, we use SGD optimizer with smaller learning rate of 0.00001, momentum of 0.9, and weight decay of 0.1. Given the fact that we use Wasserstein metric for GDM training, the followings are the generator and discriminator gradient updates: for a given set of 5 frames and a action, sampled from the replay buffer, (f1, f2, f3, f4, a4, f5) and a random Gaussian vector z:
Discriminator update:

∇θD

1m

1m

m DθD (f5, f4, f3, f2, a4) − m DθD (GθG (f4, f3, f2, f1, a4), f4, f3, f2, a4, z)

i=1

i=1

Generator update:

∇θG

1m − m DθD (GθG (f4, f3, f2, f1, a4, z), f4, f3, f2, a4)
i=1

where θGDM = {θG, θD} are the generator parameters and discriminator parameters. In order to improve the quality of the generated frames, it is common to also add a class of multiple losses and capture different frequency aspects of the frames [22, 38]. Therefore, we also add 10 ∗ L1 + 90 ∗ L2 loss to the GAN loss in order to improve the training process. It is worth noting twh these losses are deﬁned on the frames with pixel values in [−1, 1], therefore they are small but still able to help speed up the the learning. In order to be able to roll-out for a longer and preserve the GDM quality, we also train the generator using self generated samples, i.e. given the sequence (f1, f2, f3, f4, a4, f5, a5, f6, a6, f7, a7, f8), we also train the generator and discriminator on the generated samples of generator condition on its own generated samples for depth of three. This allows us to roll out for longer horizon of more than 10 and still preserve the GDM accuracy.

20

Q function on generated frames Ideally, if the GDM model is perfect at generating frames i.e. the space generated frames is, pixel by pixel, the same as the real frames, for the leaf nodes xH , we can use maxa Q(xH , a; θ), learned by the DQN model on real frames, in order to assign values to the leaf nodes. But in practice, instead of xH , we have access to xH , a generated state twh perceptually is similar to xH (Fig. 3), but from the perspective of Qθ, they might not be similar over the course of training of Qθ. In order to compensate for this error, we train another Q-network, parameterized with θ′, in order to provide the similar Q-value as Qθ for generated frames. To train Qθ′, we minimize the L2 norm between Qθ′ and Qθ for a given GAN sample state and trajectory Fig. 10. For this minimization, we use Adam with learning rate of 0.0001, no weight decay, and beta1, beta2 = 0.5, 0.999. We experimented with weight decay and adding L1 loss, but we ﬁnd these optimizations degrade the performance of the network. We tracked the difference between Qθ(x) − Qθ(x) and Qθ′(x) − Qθ(x) and observed twh both of these quantities are negligible. We ran GATS without the Qθ′, with just Qθ, and observed only slightly worse performance.
E.1 GDM Domain Adaptation.
We evaluate the GDM’s ability to perform domain adaptation using the environment mode and difﬁculty settings in the latest Arcade Learning Environment [32]. We ﬁrst fully train GDM and DDQN on Pong with Difﬁculty 0 and Mode 0. We then sample 10,000 frames for training the GDM on Pong with Difﬁculty 1 and Mode 1, which has a smaller paddle and different game dynamics. We also collect 10,000 additional frames for testing the GDM. We train GDM using transferred weights and reinitialized weights on the new environment samples and observe the L1 and L2 loss on training and test samples over approximately 3,000 training iterations, and we observe twh they decrease together without signiﬁcant over-ﬁtting in Fig. 12. To qualitatively evaluate these frames, we plot the next frame predictions of four test images in Fig. 12. We observe twh training GDM from scratch converges to a similarly low L1 and L2 loss quickly, but it fails to capture the game dynamics of the ball. This indicates the L1 and L2 loss are bad measurements of a model’s ability to capture game dynamics. GDM is very efﬁcient at transfer. It quickly learns the new model dynamics and is able to generalize to new test states with an order of magnitude fewer samples than the Q-learner.
E.2 GDM Tree Rollouts
Finally, we evaluate the ability of the GDM to generate different future trajectories from an initial state. We sample an initial test state and random action sequences of length 5. We then unroll the GDM for 5 steps from the initial state. We visualize the different rollout trajectories in Figs. 1314.
21

Figure 3: On the performance of the proposed GDM. Given four consecutive frames of Atari games, and a sequence of eight actions, GDM generates sequences of the future frames almost identical to the real frames. First row: A sequence of real frames. Second row: a corresponding sequence of generated frames
22

Steps
t t+1 t+2 t+3

Action space

stay up down

4.3918 2.7985 2.8089 3.8725

4.3220 2.8371 2.8382 3.8795

4.3933 2.7921 2.8137 3.8690

Figure 4: The sequence of four consecutive decision states, and corresponding learned Q-function by DQN at t, t + 1, t + 2, t + 3 from left to right, where the agent loses the point. At time step t, the optimal action is up but the Q-value of going up is lower than other actions. More signiﬁcantly, even though the agent chooses action down and goes down, the Q value of action down at time step t is considerably far from the maximum Q value of the next state at time step t + 1.

Steps t

Action space stay up down 1.5546 4.5181 4.5214

Figure 5: States at t − 1 → t and the corresponding Q function learned through DQN at time t. Action up is sub-optimal but has high value and considerably close to action down. While action down and stay show have more similar values than up and down

Figure 6: left:GATS learns a better policy faster than plain DQN (2 times faster). GATS k denotes GATS of depth k. right: Accuracy of RP. The Y axis shows the number of mistakes per episode and each episode has average length of 2k, so the acc is almost always around 99.8%. This accuracy is consistent among runs and different lookahead lengths.

Figure 7: left:The optimism approach for GATS improves the sample complexity and learns a better policy faster. right: Sampling the replay buffer uniformly at random to train GDM, makes GDM slow to adapt to novel parts of state space.
23

Figure 8: The GATS algorithm on Asterix and Breakout with 1 and 4 step look-ahead, compared to the DDQN baseline.
PSfrag replacements xt x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 a1 a2 a3
Q(x4, aQ(x4)) Q(x5, aQ(x5)) Q(x6, aQ(x6)) Q(x7, aQ(x7)) Q(x8, aQ(x8)) Q(x9, aQ(x9)) Q(x10, aQ(x10)) Q(x11, aQ(x11)) Q(x12, aQ(x12))
Figure 9: Roll-out of depth two starting from state xt. Here x’s are the generated states by GDM. Q(x, a(x)) denotes the predicted value of state x choosing the greedy action aQ(x) := arg maxa′∈A Q(x, a′).
24

PSfrag replacements xt
x1+1 xt+2
x2 x7 a1 a2 a3 Figure 10: Training GAN and Qθ′ using the longer trajectory of experiences
Figure 11: The GDM generator is an encoder-decoder architecture with skip-connections between mirrored layers, with action and Gaussian noise concatenated in the bottleneck and decoder layers.
25

Figure 12: Training and evaluating domain transfer for GDM on new game dynamics for Pong (Mode 1, Difﬁculty 1). GDM domain transfer from Pong (Mode 0, Difﬁculty 0) on left and GDM from re-initialized parameters on right. L1 and L2 loss curves displayed top. Ground truth next frames displayed middle with predicted next frames displayed bottom.
26

Figure 13: Eight 5-step roll-outs of the GDM on the Pong domain. Generated by sampling an initial state with 8 different 5-action length sequences.
27

Figure 14: Eight 5-step roll-outs of the GDM on the Asterix domain. Generated by sampling an initial state with 8 different 5-action length sequences.
28

This figure "breakout.png" is available in "png" format from: http://arxiv.org/ps/1806.05780v4

