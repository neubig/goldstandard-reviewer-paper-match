A Uniﬁed Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent

arXiv:1905.11261v1 [math.OC] 27 May 2019

Eduard Gorbunov MIPT, Russia
eduard.gorbunov@phystech.edu

Filip Hanzely KAUST, Saudi Arabia filip.hanzely@kaust.edu.sa

Peter Richtárik KAUST, Saudi Arabia and MIPT, Russia
peter.richtarik@kaust.edu.sa

Abstract
In this paper we introduce a uniﬁed analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling. As a by-product, we obtain the ﬁrst uniﬁed theory of SGD and randomized coordinate descent (RCD) methods, the ﬁrst uniﬁed theory of variance reduced and non-variance-reduced SGD methods, and the ﬁrst uniﬁed theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop ﬁve new variants of SGD, and through numerical experiments demonstrate some of their properties.

1 Introduction

In this paper we are interested in the optimization problem

min f (x) + R(x),

(1)

x∈Rd

where f is convex, differentiable with Lipschitz gradient, and R : Rd → R ∪ {+∞} is a proximable (proper closed convex) regularizer. In particular, we focus on situations when it is prohibitively expensive to compute the gradient of f , while an unbiased estimator of the gradient can be computed efﬁciently. This is typically the case for stochastic optimization problems, i.e., when

f (x) = Eξ∼D [fξ(x)] ,

(2)

where ξ is a random variable, and fξ : Rd → R is smooth for all ξ. Stochastic optimization problems are of key importance in statistical supervised learning theory. In this setup, x represents a machine learning model described by d parameters (e.g., logistic regression or a deep neural network), D is an unknown distribution of labelled examples, fξ(x) represents the loss of model x on datapoint ξ, and f

Preprint. Under review.

is the generalization error. Problem (1) seeks to ﬁnd the model x minimizing the generalization error. In statistical learning theory one assumes that while D is not known, samples ξ ∼ D are available. In such a case, ∇f (x) is not computable, while ∇fξ(x), which is an unbiased estimator of the gradient of f at x, is easily computable.
Another prominent example, one of special interest in this paper, are functions f which arise as averages of a very large number of smooth functions:
1n f (x) = n fi(x). (3)
i=1
This problem often arises by approximation of the stochastic optimization loss function (2) via Monte Carlo integration, and is in this context known as the empirical risk minimization (ERM) problem. ERM is currently the dominant paradigm for solving supervised learning problems [36]. If index i is chosen uniformly at random from [n] := {1, 2, . . . , n}, ∇fi(x) is an unbiased estimator of ∇f (x). Typically, ∇f (x) is about n times more expensive to compute than ∇fi(x).
Lastly, in some applications, especially in distributed training of supervised models, one considers problem (3), with n being the number of machines, and each fi also having a ﬁnite sum structure, i.e.,
1m fi(x) = m fij(x), (4)
j=1
where m corresponds to the number of training examples stored on machine i.

2 The Many Faces of Stochastic Gradient Descent

Stochastic gradient descent (SGD) [32, 23, 40] is a state-of-the-art algorithmic paradigm for solving optimization problems (1) in situations when f is either of structure (2) or (3). In its generic form, (proximal) SGD deﬁnes the new iterate by subtracting a multiple of a stochastic gradient from the current iterate, and subsequently applying the proximal operator of R:

xk+1 = proxγR(xk − γgk).

(5)

Here, gk is an unbiased estimator of the gradient (i.e., a stochastic gradient),

E gk | xk = ∇f (xk),

(6)

and

proxγR(x)

:=

argminu{γR(x)

+

1 2

u − x 2}. However, and this is the starting point of our

journey in this paper, there are inﬁnitely many ways of obtaining a random vector gk satisfying

(6). On the one hand, this gives algorithm designers the ﬂexibility to construct stochastic gradients

in various ways in order to target desirable properties such as convergence speed, iteration cost,

parallelizability and generalization. On the other hand, this poses considerable challenges in terms

of convergence analysis. Indeed, if one aims to, as one should, obtain the sharpest bounds possible,

dedicated analyses are needed to handle each of the particular variants of SGD.

Vanilla1 SGD. The ﬂexibility in the design of efﬁcient strategies for constructing gk has led to a

creative renaissance in the optimization and machine learning communities, yielding a large number

of immensely powerful new variants of SGD, such as those employing importance sampling [44, 22],

and mini-batching [16]. These efforts are subsumed by the recently developed and remarkably sharp

analysis of SGD under arbitrary sampling paradigm [6], ﬁrst introduced in the study of randomized

coordinate descent methods by [30]. The arbitrary sampling paradigm covers virtually all stationary

mini-batch and importance sampling strategies in a uniﬁed way, thus making headway towards

theoretical uniﬁcation of two separate strategies for constructing stochastic gradients. For strongly

convex f , the SGD methods analyzed in [6] converge linearly to a neighbourhood of the solution

x∗ = arg minx f (x) for a ﬁxed stepsize γk = γ. The size of the neighbourhood is proportional to

the second moment of the stochastic gradient at the optimum (σ2 := n1

n i=1

∇fi(x∗) 2), to the

stepsize (γ), and inversely proportional to the modulus of strong convexity. The effect of various

1In this paper, by vanilla SGD we refer to SGD variants with or without importance sampling and mini-batching, but excluding variance-reduced variants, such as SAGA [5] and SVRG [15].

2

sampling strategies, such as importance sampling and mini-batching, is twofold: i) improvement of the linear convergence rate by enabling larger stepsizes, and ii) modiﬁcation of σ2. However, none of these strategies2 is able to completely eliminate the adverse effect of σ2. That is, SGD with a ﬁxed stepsize does not reach the optimum, unless one happens to be in the overparameterized case characterized by the identity σ2 = 0.
Variance reduced SGD. While sampling strategies such as importance sampling and mini-batching reduce the variance of the stochastic gradient, in the ﬁnite-sum case (3) a new type of variance reduction strategies has been developed over the last few years [33, 5, 15, 37, 29, 27, 18]. These variance-reduced SGD methods differ from the sampling strategies discussed before in a signiﬁcant way: they can iteratively learn the stochastic gradients at the optimum, and in so doing are able to eliminate the adverse effect of the gradient noise σ2 > 0 which, as mentioned above, prevents the iterates of vanilla SGD from converging to the optimum. As a result, for strongly convex f , these new variance-reduced SGD methods converge linearly to x∗, with a ﬁxed stepsize. At the moment, these variance-reduced variants require a markedly different convergence theory from the vanilla variants of SGD. An exception to this is the situation when σ2 = 0 as then variance reduction is not needed; indeed, vanilla SGD already converges to the optimum, and with a ﬁxed stepsize. We end the discussion here by remarking that this hints at a possible existence of a more uniﬁed theory, one that would include both vanilla and variance-reduced SGD.
Distributed SGD, quantization and variance reduction. When SGD is implemented in a distributed fashion, the problem is often expressed in the form (3), where n is the number of workers/nodes, and fi corresponds to the loss based on data stored on node i. Depending on the number of data points stored on each node, it may or may not be efﬁcient to compute the gradient of fi in each iteration. In general, SGD is implemented in this way: each node i ﬁrst computes a stochastic gradient gik of fi at the current point xk (maintained individually by each node). These gradients are then aggregated by a master node [38, 17], in-network by a switch [34], or a different technique best suited to the architecture used. To alleviate the communication bottleneck, various lossy update compression strategies such as quantization [35, 10, 43], sparsiﬁcation [17, 2, 41] and dithering [1] were proposed. The basic idea is for each worker to apply a randomized transformation Q : Rd → Rd to gik, resulting in a vector which is still an unbiased estimator of the gradient, but one that can be communicated with fewer bits. Mathematically, this amounts to injecting additional noise into the already noisy stochastic gradient gik. The ﬁeld of quantized SGD is still young, and even some basic questions remained open until recently. For instance, there was no distributed quantized SGD capable of provably solving (1) until the DIANA algorithm [20] was introduced. DIANA applies quantization to gradient differences, and in so doing is able to learn the gradients at the optimum, which makes is able to work for any regularizer R. DIANA has some structural similarities with SEGA [11]—the ﬁrst coordinate descent type method which works for non-separable regularizers—but a more precise relationship remains elusive. When the functions of fi are of a ﬁnite-sum structure as in (4), one can apply variance reduction to reduce the variance of the stochastic gradients gik together with quantization, resulting in the VR-DIANA method [14]. This is the ﬁrst distributed quantized SGD method which provably converges to the solution of (1)+(4) with a ﬁxed stepsize.
Randomized coordinate descent (RCD). Lastly, in a distinctly separate strain, there are SGD methods for the coordinate/subspace descent variety [24]. While it is possible to see some RCD methods as special cases of (5)+(6), most of them do not follow this algorithmic template. First, standard RCD methods use different stepsizes for updating different coordinates [28], and this seems to be crucial to their success. Second, until the recent discovery of the SEGA method, RCD methods were not able to converge with non-separable regularizers. Third, RCD methods are naturally variance-reduced in the R = 0 case as partial derivatives at the optimum are all zero. As a consequence, attempts at creating variance-reduced RCD methods seem to be futile. Lastly, RCD methods are typically analyzed using different techniques. While there are deep links between standard SGD and RCD methods, these are often indirect and rely on duality [37, 4, 8].
3 Contributions
As outlined in the previous section, the world of SGD is vast and beautiful. It is formed by many largely disconnected islands populated by elegant and efﬁcient methods, with their own applications,
2Except for the full batch strategy, which is prohibitively expensive.
3

intuitions, and convergence analysis techniques. While some links already exist (e.g., the uniﬁcation of importance sampling and mini-batching variants under the arbitrary sampling umbrella), there is no comprehensive general theory. It is becoming increasingly difﬁcult for the community to understand the relationships between these variants, both in theory and practice. New variants are yet to be discovered, but it is not clear what tangible principles one should adopt beyond intuition to aid the discovery. This situation is exacerbated by the fact that a number of different assumptions on the stochastic gradient, of various levels of strength, is being used in the literature.
The main contributions of this work include:
• Uniﬁed analysis. In this work we propose a unifying theoretical framework which covers all of the variants of SGD outlined in Section 2. As a by-product, we obtain the ﬁrst uniﬁed analysis of vanilla and variance-reduced SGD methods. For instance, our analysis covers as special cases vanilla SGD methods from [26] and [6], variance-reduced SGD methods such as SAGA [5], L-SVRG [13, 18] and JacSketch [9]. Another by-product is the ﬁrst uniﬁed analysis of SGD methods which include RCD. For instance, our theory covers the subspace descent method SEGA [11] as a special case. Lastly, our framework is general enough to capture the phenomenon of quantization. For instance, we obtain the DIANA and VR-DIANA methods in special cases.
• Generalization of existing methods. An important yet relatively minor contribution of our work is that it enables generalization of knowns methods. For instance, some particular methods we consider, such as L-SVRG (Alg 10) [18], were not analyzed in the proximal (R = 0) case before. To illustrate how this can be done within our framework, we do it here for L-SVRG. Further, all methods we analyze can be extended to the arbitrary sampling paradigm.
• Sharp rates. In all known special cases, the rates obtained from our general theorem (Theorem 4.1) are the best known rates for these methods.
• New methods. Our general analysis provides estimates for a possibly inﬁnite array of new and yet-to-be-developed variants of SGD. One only needs to verify that Assumption 4.1 holds, and a complexity estimate is readily furnished by Theorem 4.1. Selected existing and new methods that ﬁt our framework are summarized in Table 1. This list is for illustration only, we believe that future work by us and others will lead to its rapid expansion.
• Experiments. We show through extensive experimentation that some of the new and generalized methods proposed here and analyzed via our framework have some intriguing practical properties when compared against appropriately selected existing methods.

4 Main Result
We ﬁrst introduce the key assumption on the stochastic gradients gk enabling our general analysis (Assumption 4.1), then state our assumptions on f (Assumption 4.2), and ﬁnally state and comment on our uniﬁed convergence result (Theorem 4.1).
Notation. We use the following notation. x, y := i xiyi is the standard Euclidean inner product, and x := x, x 1/2 is the induced 2 norm. For simplicity we assume that (1) has a unique minimizer, which we denote x∗. Let Df (x, y) denote the Bregman divergence associated with f : Df (x, y) := f (x) − f (y) − ∇f (y), x − y . We often write [n] := {1, 2, . . . , n}.

4.1 Key assumption

Our ﬁrst assumption is of key importance. It is mainly an assumption on the sequence of stochastic gradients {gk} generated by an arbitrary randomized algorithm. Besides unbiasedness (see (7)), we require two recursions to hold for the iterates xk and the stochastic gradients gk of a randomized method. We allow for ﬂexibility by casting these inequalities in a parametric manner.
Assumption 4.1. Let {xk} be the random iterates produced by proximal SGD (Algorithm in Eq (5)). We ﬁrst assume that the stochastic gradients gk are unbiased

E gk | xk = ∇f (xk),

(7)

4

for all k ≥ 0. Further, we assume that there exist non-negative constants A, B, C, D1, D2, ρ and a (possibly) random sequence {σk2}k≥0 such that the following two relations hold3

E gk − ∇f (x∗) 2 | xk ≤ 2ADf (xk, x∗) + Bσk2 + D1,

(8)

E σk2+1 | σk2 ≤ (1 − ρ)σk2 + 2CDf (xk, x∗) + D2,

(9)

The expectation above is with respect to the randomness of the algorithm.

The unbiasedness assumption (7) is standard. The key innovation we bring is inequality (8) coupled
with (9). We argue, and justify this statement by furnishing many examples in Section 5, that these
inequalities capture the essence of a wide array of existing and some new SGD methods, including vanilla, variance reduced, arbitrary sampling, quantized and coordinate descent variants. Note that in the case when ∇f (x∗) = 0 (e.g., when R = 0), the inequalities in Assumption 4.1 reduce to

E gk 2 | xk ≤ 2A(f (xk) − f (x∗)) + Bσk2 + D1,

(10)

E σk2+1 | σk2 ≤ (1 − ρ)σk2 + 2C(f (xk) − f (x∗)) + D2.

(11)

Similar inequalities can be found in the analysis of stochastic ﬁrst-order methods. However, this is the ﬁrst time that such inequalities are generalized, equipped with parameters, and elevated to the status of an assumption that can be used on its own, independently from any other details deﬁning the underlying method that generated them.

4.2 Main theorem

For simplicity, we shall assume throughout that f is µ-strongly quasi-convex, which is a generalization of µ-strong convexity. We leave an analysis under different assumptions on f to future work.
Assumption 4.2 (µ-strong quasi-convexity). There exists µ > 0 such that f : Rd → R is µ-strongly quasi-convex. That is, the following inequality holds:

f (x∗) ≥ f (x) +

∇f (x), x∗ − x

µ +

x∗ − x 2 ,

∀x ∈ Rd .

(12)

2

We are now ready to present our main convergence result.

Theorem 4.1. Let Assumptions 4.1 and 4.2 be satisﬁed. Choose constant M such that M > Bρ . Choose a stepsize satisfying

11

0 < γ ≤ min ,

.

(13)

µ A + CM

Then the iterates {xk}k≥0 of proximal SGD (Algorithm (5)) satisfy

k

k

B

k 0 (D1 + M D2)γ2

E V ≤ max (1 − γµ) , 1 + − ρ M

V

+ min

γµ, ρ − B

,

(14)

M

where the Lyapunov function V k is deﬁned by V k := xk − x∗ 2 + M γ2σk2.

This theorem establishes a linear rate for a wide range of proximal SGD methods up to a certain oscillation radius, controlled by the additive term in (14), and namely, by parameters D1 and D2. As we shall see in Section A (refer to Table 2), the main difference between the vanilla and variancereduced SGD methods is that while the former satisfy inequality (9) with D1 > 0 or D2 > 0, which in view of (14) prevents them from reaching the optimum x∗ (using a ﬁxed stepsize), the latter methods satisfy inequality (9) with D1 = D2 = 0, which in view of (14) enables them to reach the optimum.

3For convex and L-smooth f , one can show that ∇f (x) − ∇f (y) 2 ≤ 2LDf (x, y). Hence, Df can be used as a measure of proximity for the gradients.

5

Problem

Method Alg # Citation VR? AS? Quant? RCD? Section Result

(1)+(2)

SGD

Alg 1 [26]









A.1 Cor A.1

(1)+(3)

SGD-SR Alg 2

[6]







A.2 Cor A.2

(1)+(3)

SGD-MB

Alg 3 NEW









A.3 Cor A.3

(1)+(3)

SGD-star Alg 4 NEW









A.4 Cor A.4

(1)+(3)

SAGA

Alg 5

[5]







A.5 Cor A.5

(1)+(3)

N-SAGA

Alg 6 NEW









A.6 Cor A.6

(1)

SEGA

Alg 7 [11]







A.7 Cor A.7

(1)

N-SEGA

Alg 8 NEW







(1)+(3)

SVRGa

Alg 9 [15]







A.8 Cor A.8



A.9 Cor A.9

(1)+(3)

L-SVRG Alg 10 [13, 18]  





A.10 Cor A.10

(1)+(3)

DIANA

Alg 11 [20, 14] 







A.11 Cor A.11

(1)+(3)

DIANAb Alg 12 [20, 14] 







A.11 Cor A.12

(1)+(3)

Q-SGD-SR Alg 13 NEW









A.12 Cor A.13

(1)+(3)+(4) VR-DIANA Alg 14 [14]







A.13 Cor A.15

(1)+(3) JacSketch Alg 15 [9]

 





A.14 Cor A.16

Table 1: List of speciﬁc existing (in some cases generalized) and new methods which ﬁt our general
analysis framework. VR = variance reduced method, AS = arbitrary sampling, Quant = supports gradient quantization, RCD = randomized coordinate descent type method. a Special case of SVRG with 1 outer loop only; b Special case of DIANA with 1 node and quantization of exact gradient.

5 The Classic, The Recent and The Brand New
In this section we deliver on the promise from the introduction and show how many existing and some new variants of SGD ﬁt our general framework (see Table 1).
An overview. As claimed, our framework is powerful enough to include vanilla methods ( in the “VR” column) as well as variance-reduced methods ( in the “VR” column), methods which generalize to arbitrary sampling ( in the “AS” column), methods supporting gradient quantization ( in the “Quant” column) and ﬁnally, also RCD type methods ( in the “RCD” column).
For existing methods we provide a citation; new methods developed in this paper are marked accordingly. Due to space restrictions, all algorithms are described (in detail) in the Appendix; we provide a link to the appropriate section for easy navigation. While these details are important, the main message of this paper, i.e., the generality of our approach, is captured by Table 1. The “Result” column of Table 1 points to a corollary of Theorem 4.1; these corollaries state in detail the convergence statements for the various methods. In all cases where known methods are recovered, these corollaries of Theorem 4.1 recover the best known rates.
Parameters. From the point of view of Assumption 4.1, the methods listed in Table 1 exhibit certain patterns. To shed some light on this, in Table 2 we summarize the values of these parameters.
Note, for example, that for all methods the parameter A is non-zero. Typically, this a multiple of an appropriately deﬁned smoothness parameter (e.g., L is the Lipschitz constant of the gradient of f , L and L1 in SGD-SR4, SGD-star and JacSketch are expected smoothness parameters). In the three variants of the DIANA method, ω captures the variance of the quantization operator Q. That is, one assumes that EQ(x) = x and E Q(x) − x 2 ≤ ω x 2 for all x ∈ Rd. In view of (13), large A means a smaller stepsize, which slows down the rate. Likewise, the variance ω also affects the parameter B, which in view of (14) also has an adverse effect on the rate. Further, as predicted by Theorem 4.1, whenever either D1 > 0 or D2 > 0, the corresponding method converges to an oscillation region only. These methods are not variance-reduced. All symbols used in Table 2 are deﬁned in the appendix, in the same place where the methods are described and analyzed.
Five new methods. To illustrate the usefulness of our general framework, we develop 5 new variants of SGD never explicitly considered in the literature before (see Table 1). Here we brieﬂy motivate them; details can be found in the Appendix.
4SGD-SR is ﬁrst SGD method analyzed in the arbitrary sampling paradigm. It was developed using the stochastic reformulation approach (whence the “SR”) pioneered in [31] in a numerical linear algebra setting, and later extended to develop the JacSketch variance-reduction technique for ﬁnite-sum optimization [9].
6

Method SGD
SGD-SR
SGD-MB SGD-star
SAGA
N-SAGA SEGA
N-SEGA SVRGa L-SVRG
DIANA DIANAb Q-SGD-SR
VR-DIANA JacSketch

A 2L 2L
A +L(τ −1) τ
2L 2L
2L 2dL
2dL 2L 2L
1 + 2nω L (1 + 2ω) L 2(1 + ω)L 1 + 4ωn+2 L
2L1

B 0 0
0 0 2
2 2d
2d 2 2
2ω n
2ω 0
2(ω+1) n
2λmax n

ρ 1 1
1 1 1/n
1/n 1/d
1/d 0 p
α α 1 α λmin

C

D1

D2

0

2σ2

0

0

2σ2

0

0 Dτ 0

0

0

0

L/n
L/n L/d
L/d 0

0

0

2σ2

σ2

n

0

0

2dσ2

σ2

d

0

0

Lp

0

0

Lα

(1+ω)σ2

ασ2

n

Lα

0

0

0

2(1 + ω)σ2 0

m1 + 4α L 0 0 Ln2 0 0

Table 2: The parameters for which the methods from Table 1 satisfy Assumption 4.1. The meaning of the expressions appearing in the table is deﬁned in detail in the Appendix.

• SGD-MB (Algorithm 3). This method is speciﬁcally designed for functions of the ﬁnite-sum structure (4). As we show through experiments, this is a powerful mini-batch SGD method, with mini-batches formed with replacement as follows: in each iteration, we repeatedly (τ times) and independently pick i ∈ [n] with probability pi > 0. Stochastic gradient gk is then formed by averaging the stochastic gradients ∇fi(xk) for all selected indices i (including each i as many times as this index was selected).
• SGD-star (Algorithm 4). This new method forms a bridge between vanilla and variance-reduced SGD methods. While not practical, it sheds light on the role of variance reduction. Again, we consider functions of the ﬁnite-sum form (4). This methods answers the following question: assuming that the gradients ∇fi(x∗), i ∈ [n] are known, can they be used to design a more powerful SGD variant? The answer is yes, and SGD-star is the method. In its most basic form, SGD-star constructs the stochastic gradient via gk = ∇fi(xk) − ∇fi(x∗), where i ∈ [n] is chosen uniformly at random. That is, the standard stochastic gradient ∇fi(xk) is perturbed by the stochastic gradient at the same index i evaluated at the optimal point x∗. Inferring from Table 2, where D1 = D2 = 0, this method converges to x∗, and not merely to some oscillation region. Variance-reduced methods essentially work by iteratively constructing increasingly more accurate estimates of ∇fi(x∗). Typically, the term σk2 in the Lyapunov function of variance reduced methods will contain a term of the form i hki − ∇fi(xk) 2, with hki being the estimators maintained by the method. Remarkably, SGD-star was never explicitly considered in the literature before.
• N-SAGA (Algorithm 6). This is a novel variant of SAGA [5], one in which one does not have access to the gradients of fi, but instead only has access to noisy stochastic estimators thereof (with noise σ2). Like SAGA, N-SAGA is able to reduce the variance inherent in the ﬁnite sum structure (4) of the problem. However, it necessarily pays the price of noisy estimates of ∇fi, and hence, just like vanilla SGD methods, is ultimately unable to converge to x∗. The oscillation region is governed by the noise level σ2 (refer to D1 and D2 in Table 2). This method will be of practical importance for problems where each fi is of the form (2), i.e., for problems of the “average of expectations” structure. Batch versions of N-SAGA would be well suited for distributed optimization, where each fi is owned by a different worker, as in such a case one wants the workers to work in parallel.
• N-SEGA (Algorithm 8). This is a noisy extension of the RCD-type method SEGA, in complete analogy with the relationship between SAGA and N-SAGA. Here we assume that we only have noisy estimates of partial derivatives (with noise σ2). This situation is common in derivative-free optimization, where such a noisy estimate can be obtained by taking (a random) ﬁnite difference approximation [25]. Unlike SEGA, N-SEGA only converges to an oscillation region the size of which is governed by σ2.
7

• Q-SGD-SR (Algorithm 13). This is a quantized version of SGD-SR, which is the ﬁrst SGD method analyzed in the arbitrary sampling paradigm. As such, Q-SGD-SR is a vast generalization of the celebrated QSGD method [1].

6 Experiments

In this section we numerically verify the claims from the paper. We present only a fraction of experiments here, the rest is contained in Appendix B.
In Section A.3, we describe in detail the SGD-MB method already outlined before. The main advantage of SGD-MB is that the sampling procedure it employs can be implemented in just O(τ log n) time. In contrast, even the simplest without-replacement sampling which selects each function into the minibatch with a prescribed probability independently (we will refer to it as independent SGD) requires n calls of a uniform random generator. We demonstrate numerically that SGD-MB has essentially identical iteration complexity to independent SGD in practice. We consider logistic regression with Tikhonov regularization. For a ﬁxed expected sampling size τ , consider two options for the probability of sampling the i-th function:

(i) nτ , or (ii) δ+aiai2+2+λλ , where δ is such that5

ni=1 δ+aiai2+2+λλ = 1.

The results can be found in Figure 1, where we also report the choice of stepsize γ and the choice of τ in the legend and title of the plot, respectively.

Relative suboptimality

100 10 1 10 2 10 3 10 4 10 5
0
100 10 1 10 2 10 3 10 4 10 5
0

Dataset: w1a, tau: 10, unif
r: True, gamma: 0.96201 r: True, gamma: 0.09620 r: False, gamma: 0.96201 r: False, gamma: 0.09620 10000 20000 It3e0r0a0t0ion 40000 50000 60000
Dataset: gisette_scale, tau: 10, unif
r: True, gamma: 0.48750 r: True, gamma: 0.04875 r: False, gamma: 0.48750 r: False, gamma: 0.04875 20000 40000 6000It0era8t0i0o0n0 100000 120000 140000

Relative suboptimality

Relative suboptimality

Dataset: w1a, tau: 10, imp

100

r: True, gamma: 2.26520

r: True, gamma: 0.22652

r: False, gamma: 2.26520

10 1

r: False, gamma: 0.22652

10 2

10 3 0
100
10 1

10000 20000 It3e0r0a0t0ion 40000 50000 60000
Dataset: gisette_scale, tau: 10, imp r: True, gamma: 0.60646 r: True, gamma: 0.06065 r: False, gamma: 0.60646 r: False, gamma: 0.06065

10 2

10 3 0

20000 40000 6000It0era8t0i0o0n0 100000 120000 140000

Relative suboptimality

Relative suboptimality

100 10 1 10 2 10 3 10 4 10 5 10 6
0
100 10 1 10 2

Dataset: w1a, tau: 50, unif r: True, gamma: 2.59545 r: True, gamma: 0.25955 r: False, gamma: 2.59545 r: False, gamma: 0.25955
10000 20000 It3e0r0a0t0ion 40000 50000 60000 Dataset: gisette_scale, tau: 50, unif
r: True, gamma: 0.64045 r: True, gamma: 0.06404 r: False, gamma: 0.64045 r: False, gamma: 0.06404

10 3

10 4

10 5 0

20000 40000 6000It0era8t0i0o0n0 100000 120000 140000

Relative suboptimality

Relative suboptimality

Dataset: w1a, tau: 50, imp

100

r: True, gamma: 3.38902

r: True, gamma: 0.33890

r: False, gamma: 3.38902

10 1

r: False, gamma: 0.33890

10 2

10 3

10 4 0
100 10 1

10000 20000 It3e0r0a0t0ion 40000 50000 60000
Dataset: gisette_scale, tau: 50, imp r: True, gamma: 0.67163 r: True, gamma: 0.06716 r: False, gamma: 0.67163 r: False, gamma: 0.06716

10 2

10 3

0 20000 40000 6000It0era8t0i0o0n0 100000 120000 140000

Relative suboptimality

Figure 1: SGD-MB and independent SGD applied on LIBSVM [3]. Title label “unif” corresponds to probabilities chosen by (i) while label “imp” corresponds to probabilities chosen by (ii). Lastly, legend label “r” corresponds to “replacement” with value “True” for SGD-MB and value “False” for independent SGD.

Indeed, iteration complexity of SGD-MB and independent SGD is almost identical. Since the cost of each iteration of SGD-MB is cheaper6, we conclude superiority of SGD-MB to independent SGD.

7 Limitations and Extensions
Although our approach is rather general, we still see several possible directions for future extensions, including:
• We believe our results can be extended to weakly convex functions. However, producing a comparable result in the nonconvex case remains a major open problem.
5An RCD version of this sampling was proposed in [12]; it was shown to be superior to uniform sampling both in theory and practice.
6The relative difference between iteration costs of SGD-MB and independent SGD can be arbitrary, especially for the case when cost of evaluating ∇fi(x) is cheap, n is huge and n τ . In such case, cost of one iteration of SGD-MB is τ Cost(∇fi) + τ log(n) while the cost of one iteration of independent SGD is τ Cost(∇fi) + n.

8

• It would be further interesting to unify our theory with biased gradient estimators. If this was possible, one could recover methods as SAG [33] in special cases, or obtain rates for the zero-order optimization. We have some preliminary results in this direction already.
• Although our theory allows for non-uniform stochasticity, it does not recover the best known rates for RCD type methods with importance sampling. It would be thus interesting to provide a more reﬁned analysis capable of capturing importance sampling phenomena more accurately.
• An extension of Assumption 4.1 to iteration dependent parameters A, B, C, D1, D2, ρ would enable an array of new methods, such as SGD with decreasing stepsizes.
• It would be interesting to provide a uniﬁed analysis of stochastic methods with acceleration and momentum. In fact, [19] provide (separately) a uniﬁcation of some methods with and without variance reduction. Hence, an attempt to combine our insights with their approach seems to be a promising starting point in these efforts.
References
[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709–1720, 2017.
[2] Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information Processing Systems, pages 5977–5987, 2018.
[3] Chih-Chung Chang and Chih-Jen Lin. LibSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.
[4] Dominik Csiba and Peter Richtárik. Coordinate descent face-off: primal or dual? In JMLR Workshop and Conference Proceedings, The 29th International Conference on Algorithmic Learning Theory, 2018.
[5] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646–1654, 2014.
[6] Robert M Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. SGD: General analysis and improved rates. arXiv preprint arXiv:1901.09401, 2019.
[7] Robert M Gower and Peter Richtárik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690, 2015.
[8] Robert M Gower and Peter Richtárik. Stochastic dual ascent for solving linear systems. arXiv:1512.06890, 2015.
[9] Robert M Gower, Peter Richtárik, and Francis Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018.
[10] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, pages 1737–1746. JMLR.org, 2015.
[11] Filip Hanzely, Konstantin Mishchenko, and Peter Richtárik. SEGA: Variance reduction via gradient sketching. In Advances in Neural Information Processing Systems 31, pages 2082–2093, 2018.
[12] Filip Hanzely and Peter Richtárik. Accelerated coordinate descent with arbitrary sampling and best rates for minibatches. In Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages 304–312. PMLR, 16–18 Apr 2019.
[13] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.
[14] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
9

[15] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315–323, 2013.
[16] Jakub Konecˇný, Jie Lu, Peter Richtárik, and Martin Takácˇ. Mini-batch semi-stochastic gradient descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):242– 255, 2016.
[17] Jakub Konecˇný and Peter Richtárik. Randomized distributed mean estimation: accuracy vs communication. Frontiers in Applied Mathematics and Statistics, 4(62):1–11, 2018.
[18] Dmitry Kovalev, Samuel Horváth, and Peter Richtárik. Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop. arXiv preprint arXiv:1901.08689, 2019.
[19] Andrei Kulunchakov and Julien Mairal. Estimate sequences for variance-reduced stochastic composite optimization. arXiv preprint arXiv:1905.02374, 2019.
[20] Konstantin Mishchenko, Eduard Gorbunov, Martin Takácˇ, and Peter Richtárik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
[21] Konstantin Mishchenko, Filip Hanzely, and Peter Richtárik. 99% of parallel optimization is inevitably a waste of time. arXiv preprint arXiv:1901.09437, 2019.
[22] Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. Mathematical Programming, 155(1– 2):549–573, 2015.
[23] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574– 1609, 2009.
[24] Yurii Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.
[25] Yurii Nesterov. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–566, 2017.
[26] Lam Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtárik, Katya Scheinberg, and Martin Takácˇ. SGD and Hogwild! Convergence without the bounded gradients assumption. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3750–3758, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.
[27] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takácˇ. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2613–2621. PMLR, 2017.
[28] Zheng Qu and Peter Richtárik. Coordinate descent with arbitrary sampling I: Algorithms and complexity. Optimization Methods and Software, 31(5):829–857, 2016.
[29] Zheng Qu, Peter Richtárik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with arbitrary sampling. In Advances in Neural Information Processing Systems 28, pages 865–873, 2015.
[30] Peter Richtárik and Martin Takácˇ. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, 10(6):1233–1243, 2016.
[31] Peter Richtárik and Martin Takácˇ. Stochastic reformulations of linear systems: algorithms and convergence theory. arXiv:1706.01108, 2017.
[32] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400–407, 1951.
[33] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems, pages 2663–2671, 2012.
[34] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, and Peter Richtárik. Scaling distributed machine learning with in-network aggregation. arXiv preprint ArXiv:1903.06701, 2019.
10

[35] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. pages 1058–1062. ISCA, 2014.
[36] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algorithms. Cambridge University Press, 2014.
[37] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. Journal of Machine Learning Research, 14(1):567–599, 2013.
[38] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efﬁcient distributed optimization using an approximate Newton-type method. In Proceedings of the 31st International Conference on Machine Learning, PMLR, volume 32, pages 1000–1008, 2014.
[39] Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920, 2019.
[40] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron. In 22nd International Conference on Artiﬁcial Intelligence and Statistics, volume 89 of PMLR, pages 1195–1204, 2019.
[41] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiﬁcation for communication-efﬁcient distributed optimization. In Advances in Neural Information Processing Systems, pages 1306–1316, 2018.
[42] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1509–1519, 2017.
[43] Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 4035–4043, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
[44] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning, PMLR, volume 37, pages 1–9, 2015.
11

Appendix
A Uniﬁed Theory of SGD: Variance Reduction, Sampling,
Quantization and Coordinate Descent

Contents

1 Introduction

1

2 The Many Faces of Stochastic Gradient Descent

2

3 Contributions

3

4 Main Result

4

4.1 Key assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

4.2 Main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

5 The Classic, The Recent and The Brand New

6

6 Experiments

8

7 Limitations and Extensions

8

A Special Cases

14

A.1 Proximal SGD for stochastic optimization . . . . . . . . . . . . . . . . . . . . . . . 14

A.2 SGD-SR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.3 SGD-MB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

A.4 SGD-star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.5 SAGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.6 N-SAGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

A.7 SEGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

A.8 N-SEGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

A.9 SVRG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

A.10 L-SVRG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

A.11 DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

A.12 Q-SGD-SR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

A.13 VR-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

A.14 JacSketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

A.15 Interpolation between methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

B Extra Experiments

34

B.1 SGD-MB: remaining experiments and exact problem setup. . . . . . . . . . . . . . . 34

B.2 Experiments on SGD-star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

B.3 Experiments on N-SEGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

12

C Proofs for Section 4

37

C.1 Basic Facts and Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

C.2 A Key Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

C.3 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

13

A Special Cases
A.1 Proximal SGD for stochastic optimization

Algorithm 1 SGD
Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ for k = 0, 1, 2, . . . do
Sample ξ ∼ D gk = ∇fξ(xk) xk+1 = proxγR(xk − γgk) end for

We start with stating the problem, the assumptions on the objective and on the stochastic gradients for SGD [26]. Consider the expectation minimization problem

min f (x) + R(x), f (x) := ED [fξ(x)]

(15)

x∈Rd

where ξ ∼ D, fξ(x) is differentiable and L-smooth almost surely in ξ.
Lemma A.1 shows that the stochastic gradient gk = ∇fξ(xk) satisﬁes Assumption 4.1. The corresponding choice of parameters can be found in Table 2.
Lemma A.1 (Generalization of Lemmas 1,2 from [26]). Assume that fξ(x) is convex in x for every ξ. Then for every x ∈ Rd

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4L(Df (x, x∗)) + 2σ2,

(16)

where σ2 := Eξ ∇fξ(x∗) 2 . If further f (x) is µ-strongly convex with possibly non-convex fξ, then for every x ∈ Rd

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4Lκ(Df (x, x∗)) + 2σ2,

(17)

where κ = Lµ . Corollary A.1. Assume that fξ(x) is convex in x for every ξ and f is µ-strongly quasi-convex. Then SGD with γ ≤ 21L satisﬁes
E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γσ2 . (18) µ
If we further assume that f (x) is µ-strongly convex with possibly non-convex fξ(x), SGD with γ ≤ 2L1κ satisﬁes (18) as well.

Proof. It sufﬁces to plug parameters from Table 2 into Theorem 4.1.

Proof of Lemma A.1

The proof is a direct generalization to the one from [26]. Note that

1 2 ED

∇fξ(x) − ∇f (x∗) 2 − ED ∇fξ(x∗) − ∇f (x∗) 2

1 = 2 ED
(77)
≤ ED

∇fξ(x) − ∇f (x∗) 2 − ∇fξ(x) − ∇fξ(x∗) 2

∇fξ(x∗) − ∇f (x∗) 2

≤ 2LDf (x, x∗).

14

It remains to rearrange the above to get (16). To obtain (17), we shall proceed similarly:

1 2 ED

∇fξ(x) − ∇f (x∗) 2 − ED ∇fξ(x∗) − ∇f (x∗) 2

1 = 2 ED

∇fξ(x) − ∇f (x∗) 2 − ∇fξ(x∗) − ∇f (x∗) 2

(77)
≤ ED

∇fξ(x) − ∇fξ(x∗) 2

≤ L2 x − x∗ 2 ≤ 2 L2 Df (x, x∗).
µ Again, it remains to rearrange the terms.

A.2 SGD-SR

In this section, we recover convergence result of SGD under expected smoothness property from [6].
This setup allows obtaining tight convergence rates of SGD under arbitrary stochastic reformulation of ﬁnite sum minimization7.

The stochastic reformulation is a special instance of (15):

1n

min f (x) + R(x), f (x) = ED [fξ(x)] , fξ(x) :=

ξifi(x)

(19)

x∈Rd

n

i=1

where ξ is a random vector from distribution D such that for all i: ED [ξi] = 1 and fi (for all i) is smooth, possibly non-convex function. We next state the expextes smoothness assumption. A speciﬁc instances of this assumption allows to get tight convergence rates of SGD, which we recover in this section.

Algorithm 2 SGD-SR
Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ] is vector of ones for k = 0, 1, 2, . . . do Sample ξ ∼ D gk = ∇fξ(xk) xk+1 = proxγR(xk − γgk) end for

Assumption A.1 (Expected smoothness). We say that f is L-smooth in expectation with respect to distribution D if there exists L = L(f, D) > 0 such that

ED ∇fξ(x) − ∇fξ(x∗) 2 ≤ 2LDf (x, x∗),

(20)

for all x ∈ Rd. For simplicity, we will write (f, D) ∼ ES(L) to say that (20) holds.

Next, we present Lemma A.2 which shows that choice of constants for Assumption 4.1 from Table 2 is valid.

Lemma A.2 (Generalization of Lemma 2.4, [6]). If (f, D) ∼ ES(L), then

ED ∇fξ(x) − ∇f (x∗) 2 ≤ 4LDf (x, x∗) + 2σ2.

(21)

where σ2 := ED ∇fξ(x∗) 2 .

A direct consequence of Theorem 4.1 in this setup is Corollary A.2.
Corollary A.2. Assume that f (x) is µ-strongly quasi-convex and (f, D) ∼ ES(L). Then SGD-SR with γk ≡ γ ≤ 21L satisﬁes
E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γσ2 . (22) µ
7For technical details on how to exploit expected smoothness for speciﬁc reformulations, see [6]

15

Proof of Lemma A.2
Here we present the generalization of the proof of Lemma 2.4 from [6] for the case when ∇f (x∗) = 0. In this proof all expectations are conditioned on xk.

E ∇fξ(x) − ∇f (x∗) 2

= E ∇fξ(x) − ∇fξ(x∗) + ∇fξ(x∗) − ∇f (x∗) 2

(76)
≤

2E

∇fξ(x) − ∇fξ(x∗) 2 + 2E

∇fξ(x∗) − ∇f (x∗) 2

(20)
≤ 4LDf (x, x∗) + 2σ2.

A.3 SGD-MB

In this section, we present a speciﬁc practical formulation of (19) which was not considered in [6]. The resulting algorithm (Algorithm 3) is novel; it was not considered in [6] as a speciﬁc instance of SGD-SR. The key idea behind SGD-MB is constructing unbiased gradient estimate via with-replacement sampling.

Consider random variable ν ∼ D such that

n

P(ν = i) = pi;

pi = 1.

(23)

i=1

Notice that if we deﬁne

1

ψi(x) := fi(x), i = 1, 2, . . . , n,

(24)

npi

then

1n

n (24)

(23)

f (x) = n fi(x) = piψi(x) = ED [ψν (x)] . (25)

i=1

i=1

So, we have rewritten the ﬁnite sum problem (3) into the equivalent stochastic optimization problem

min ED [ψν (x)] .

(26)

x∈Rd

We are now ready to describe our method. At each iteration k we sample νik, . . . , ντk ∼ D indepen-

dently (1 ≤ τ ≤ n), and deﬁne gk := τ1

τ i=1

∇ψνk (xk).

Further,

we

use

gk

as

a

stochastic

gradient,

i

resulting in Algorithm 3.

Algorithm 3 SGD-MB

Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ν such that (23) holds.

for k = 0, 1, 2, . . . do

Sample νik, . . . , ντk ∼ D independently

gk = τ1

τ i=1

∇ψνk

(xk

)

i

xk+1 = xk − γgk

end for

To remain in full generality, consider the following Assumption. Assumption A.2. There exists constants A > 0 and D ≥ 0 such that

ED ∇ψν (x) 2 ≤ 2A (f (x) − f (x∗)) + D

(27)

for all x ∈ Rd.

Note that it is sufﬁcient to have convex and smooth fi in order to satisfy Assumption A.2, as Lemma A.3 states.

16

Lemma A.3. Let σ2 := ED ∇ψν(x∗) 2 . If fi are convex and Li-smooth, then Assumption A.2 holds for A = 2L and D = 2σ2, where
L ≤ max Li . (28) i npi
If moreover ∇fi(x∗) = 0 for all i, then Assumption A.2 holds for A = L and D = 0.

Next, Lemma A.4 states that Algorithm 3 indeed satisﬁes Assumption 4.1.
Lemma A.4. Suppose that Assumption A.2 holds. Then gk is unbiased; i.e. ED gk = ∇f (xk). Further,

ED gk 2 ≤ 2A + 2L(τ − 1) (f (xk) − f (x∗)) + D .

τ

τ

Thus, parameters from Table 2 are validated. As a direct consequence of Theorem 4.1 we get Corollary A.3. Corollary A.3. As long as 0 < γ ≤ A +Lτ(τ−1) , we have

E

xk − x∗ 2 ≤ (1 − γµ)k

x0 − x∗

2 γD +

.

(29)

µτ

Remark A.1. For τ = 1, SGD-MB is a special of the method from [6], Section 3.2. However, for τ > 1, this is a different method; the difference lies in the with-replacement sampling. Note that with-replacement trick allows for efﬁcient and implementation of independent importance sampling 8 with complexity O(τ log(n)). In contrast, implementation of without-replacement importance sampling has complexity O(n), which can be signiﬁcantly more expensive to the cost of evaluating
i∈S ∇fi(x).

Proof of Lemma A.4 Notice ﬁrst that

ED gk

(24) 1 τ

1

k

= τ i=1 ED npνik ∇fνik (x )

= ED 1 ∇fν (xk) npν

n (23)

1

k

=

pi ∇fi(x )

i=1 npi

= ∇f (xk).

So, gk is an unbiased estimator of the gradient ∇f (xk). Next,

ED gk 2


τ

2

1 = ED  τ

∇ψνk (xk)  i

i=1





1

τ

2

= τ 2 ED 

∇ψνk (xk) + 2 i

∇ψνk (xk), ∇ψνk (xk) 

i

j

i=1

i<j

=

1 ED

∇ψν (xk) 2 + 2

τ

τ2

ED ∇ψνk (xk) , ED ∇ψνk (xk)

i

j

i<j

=

1 ED

∇ψν (xk) 2

τ −1 +

∇f (xk) 2

τ

τ

(27) 2A (f (xk) − f (x∗)) + D + 2L(τ − 1)(f (xk) − f (x∗))

≤

.

τ

8Distribution of random sets S for which random variables i ∈ S and j ∈ S are independent for j = i.

17

Proof of Lemma A.3

Let L = L(f, D) > 0 be any constant for which

Eξ∼D ∇φξ(x) − ∇φξ(x∗) 2 ≤ 2L(f (x) − f (x∗))

(30)

holds for all x ∈ Rd. This is the expected smoothness property (for a single item sampling) from [6]. It was shown in [6, Proposition 3.7] that (30) holds, and that L satisﬁes (28). The claim now follows

by applying [6, Lemma 2.4].

A.4 SGD-star
Consider problem (19). Suppose that ∇fi(x∗) is known for all i. In this section we present a novel algorithm — SGD-star — which is SGD-SR shifted by the stochastic gradient in the optimum. The method is presented under Expected Smoothness Assumption (20), obtaining general rates under arbitrary sampling. The algorithm is presented as Algorithm 4.

Algorithm 4 SGD-star
Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ] is vector of ones for k = 0, 1, 2, . . . do Sample ξ ∼ D gk = ∇fξ(xk) − ∇fξ(x∗) + ∇f (x∗) xk+1 = proxγR(xk − γgk) end for

Suppose that (f, D) ∼ ES(L). Note next that SGD-star is just SGD-SR applied on objective Df (x, x∗) instead of f (x) when ∇f (x∗) = 0. This careful design of the objective yields
(Df (·, x∗), D) ∼ ES(L) and ED ∇xDfξ (x, x∗) 2 | x = x∗ = 0, and thus Lemma (A.2) be-
comes

Lemma A.5 (Lemma 2.4, [6]). If (f, D) ∼ ES(L), then

ED gk − ∇f (x∗) 2 ≤ 4LDf (xk, x∗).

(31)

A direct consequence of Corollary (thus also a direct consequence of Theorem 4.1) in this setup is

Corollary A.4.

Corollary A.4. Suppose that (f, D) ∼ ES(L). Then SGD-star with γ = 21L satisﬁes

E

xk − x∗ 2 ≤

µ 1−

k x0 − x∗ 2 .

(32)

2L

Remark A.2. Note that results from this section are obtained by applying results from A.2. Since

Section A.3 presets a speciﬁc sampling algorithm for SGD-SR, the results can be thus extended to

SGD-star as well.

Proof of Lemma A.5

In this proof all expectations are conditioned on xk. ED gk − ∇f (x∗) 2 = ED

∇fξ(xk) − ∇fξ(x∗) 2

(20)
≤ 4LDf (xk, x∗).

A.5 SAGA
In this section we show that our approach is suitable for SAGA [5] (see Algorithm 5). Consider the ﬁnite-sum minimization problem
1n f (x) = n fi(x) + R(x), (33)
i=1
where fi is convex, L-smooth for each i and f is µ-strongly convex.

18

Algorithm 5 SAGA [5]

Input: learning rate γ > 0, starting point x0 ∈ Rd Set ψj0 = x0 for each j ∈ [n]
for k = 0, 1, 2, . . . do

Sample j ∈ [n] uniformly at random

Set φkj +1 = xk and φki +1 = φki for i = j

n

gk

=

∇fj (φkj +1)

−

∇fj (φkj )

+

1 n

∇fi(φki )

i=1

xk+1 = proxγR xk − γgk

end for

Lemma A.6. We have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2

(34)

and E σk2+1 | xk ≤ 1 − n1 σk2 + 2nL Df (xk, x∗), (35)
where σk2 = n1 n ∇fi(φki ) − ∇fi(x∗) 2.
i=1

Clearly, Lemma A.6 shows that Algorithm 5 satisﬁes Assumption 4.1; the corresponding parameter choice can be found in Table 2. Thus, as a direct consequence of Theorem 4.1 with M = 4n we obtain the next corollary. Corollary A.5. SAGA with γ = 61L satisﬁes
EV k ≤ 1 − min µ , 1 k V 0. (36) 6L 2n

Proof of Lemma A.6

Note that Lemma A.6 is a special case of Lemmas 3,4 from [21] without prox term. We reprove it with prox for completeness.
Let all expectations be conditioned on xk in this proof. Note that L-smoothness and convexity of fi implies
1 ∇fi(x) − ∇fi(y) 2 ≤ fi(x) − fi(y) − ∇fi(y), x − y , ∀x, y ∈ Rd, i ∈ [n]. (37) 2L

By deﬁnition of gk we have

E gk − ∇f (x∗) 2

=

=



n

2

E  ∇fj(φkj +1) − ∇fj(φkj ) + n1 ∇fi(φki ) − ∇f (x∗) 

i=1



n

2

E  ∇fj(xk) − ∇fj(x∗) + ∇fj(x∗) − ∇fj(φkj ) + n1 ∇fi(φki ) − ∇f (x∗) 

i=1

(76)
≤

2E ∇fj(xk) − ∇fj(x∗) 2 | xk

+2E ∇fj(x∗) − ∇fj(φkj ) − E ∇fj(x∗) − ∇fj(φkj ) 2

(78)+(37)
≤
=

4L n Df (xk, x∗) + 2E

n

i

i=1

∇fj (x∗) − ∇fj (φkj ) 2 | xk

1n 4LDf (xk, x∗) + 2
n
i=1

∇fi(φki ) − ∇fi(x∗) 2 .

σk2

19

To proceed with (35), we have

E σk2+1

1n

k+1

∗2

= n

E ∇fi(φi ) − ∇fi(x )

i=1

1 n n−1

21

2

=

∇fi(φki ) − ∇fi(x∗) + ∇fi(xk) − ∇fi(x∗)

n

n

n

i=1

(37)

1 1n

2

≤ 1−

∇fi(φki ) − ∇fi(x∗)

nn

i=1

2L n

+

Df (xk, x∗)

n2

i

i=1

= 1 − n1 σk2 + 2nL Df (xk, x∗).

A.6 N-SAGA

Algorithm 6 Noisy SAGA (N-SAGA)

Input: learning rate γ > 0, starting point x0 ∈ Rd Set ψj0 = x0 for each j ∈ [0]
for k = 0, 1, 2, . . . do

Sample j ∈ [n] uniformly at random and ζ

Set gjk+1 = gj (xk, ξ) and gik+1 = gik for i = j

n

gk

=

gj(xk, ξ)

−

gjk

+

1 n

gik

i=1

xk+1 = proxγR(xk − γgk)

end for

Note that it can in practice happen that instead of ∇fi(x) one can query gi(x, ζ) such that Eξgi(·, ξ) = ∇fi(·) and Eξ gi(·, ξ) 2 ≤ σ2. This leads to a variant of SAGA which only uses noisy estimates of the stochastic gradients ∇i(·). We call this variant N-SAGA (see Algorithm 6).
Lemma A.7. We have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2 + 2σ2,

(38)

and

2

k

1 2 2L

k ∗ σ2

E σk+1 | x

≤

1− n

σk +

n Df (x

,x

)+

, n

(39)

where σk2 := n1 n gik − ∇fi(x∗) 2.
i=1
Corollary A.6. Let γ = 61L . Then, iterates of Algorithm 6 satisfy

EV k ≤ 1 − min

µ1 ,

6L 2n

k 0

σ2

V + L min(µ, 3L ) .

n

Analogous results can be obtained for L-SVRG.

20

Proof of Lemma A.7

Let all expectations be conditioned on xk. By deﬁnition of gk we have

E gk − ∇f (x∗) 2



n

2

≤ E  gj(xk, ζ) − gjk + n1 gik − ∇f (x∗) 

i=1



n

2

= E  gj(xk, ζ) − ∇fj(x∗) + ∇fj(x∗) − gjk + n1 gik − ∇f (x∗) 

i=1

(76)
≤ 2E

gj(xk, ζ) − ∇fj(x∗) 2

+2E ∇fj (x∗) − gjk − E ∇fj (x∗) − gjk 2

(78)
≤ 2E
= 2E

gj(xk, ζ) − ∇fj(x∗) 2 gj(xk, ζ) − ∇fj(x∗) 2

+ 2E ∇fj (x∗) − gjk 2

1n +2
n i=1

gik − ∇fi(x∗) 2

(78)
≤ 2E

σk2
∇fj (xk) − ∇fj (x∗) 2 + 2σ2 + 2σk2

(37)
≤ 4LDf (xk, x∗) + 2σk2 + 2σ2

For the second inequality, we have

E σk2+1

1n

k+1

∗2

= n

E gi − ∇fi(x )

i=1

1 n n−1

21

2

=

gik − ∇fi(x∗) + E gi(xk, ζ) − ∇fi(x∗)

n

n

n

i=1

1 n n−1

21

2 σ2

≤

gik − ∇fi(x∗) + ∇fi(xk) − ∇fi(x∗) +

n

n

n

n

i=1

(37)

1 2 2L

k ∗ σ2

≤

1− n

σk +

n Df (x

,x

)+

. n

A.7 SEGA

Algorithm 7 SEGA [11]
Input: learning rate γ > 0, starting point x0 ∈ Rd Set h0 = 0 for k = 0, 1, 2, . . . do Sample j ∈ [d] uniformly at random Set hk+1 = hk + ei(∇if (xk) − hki ) gk = dei(∇if (xk) − hki ) + hk xk+1 = proxγR(xk − γgk) end for

We show that the framework recovers the simplest version of SEGA (i.e., setup from Theorem D1 from [11]) in the proximal setting9.
9General version for arbitrary gradient sketches instead of partial derivatives can be recovered as well, however, we omit it for simplicity

21

Lemma A.8. (Consequence of Lemmas A.3., A.4. from [11]) We have E gk − ∇f (x∗) | xk 2 ≤ 2d ∇f xk − ∇f (x∗) 2 + 2dσk2

and E σk2+1 | xk =
where σk2 := hk − ∇f (x∗) 2.

1 1−
d

σk2 + d1

∇f xk − ∇f (x∗) 2 ,

Given that we have from convexity and smoothness ∇f (xk) − ∇f (x∗) 2 ≤ 2LDf (xk, x∗), Assumption 4.1 holds the parameter choice as per Table 2. Setting further M = 4d2, we get the next corollary.
Corollary A.7. SEGA with γ = 6d1L satisﬁes

EV k ≤ 1 − µ

k
V 0.

6dL

A.8 N-SEGA

Algorithm 8 Noisy SEGA (N-SEGA)
Input: learning rate γ > 0, starting point x0 ∈ Rd Set h0 = 0 for k = 0, 1, 2, . . . do Sample i ∈ [d] uniformly at random and sample ξ Set hk+1 = hk + ei(gi(x, ξ) − hki ) gk = dei(gi(x, ξ) − hki ) + hk xk+1 = xk − γgk end for

Here we assume that gi(x, ζ) is a noisy estimate of the partial derivative ∇if (x) such that Eζ gi(x, ζ) = ∇if (x) and Eζ |gi(x, ζ) − ∇if (x)|2 ≤ σd2 . Lemma A.9. The following inequalities hold:
E gk − ∇f (x∗) 2 ≤ 4dLDf (xk, x∗) + 2dσk2 + 2dσ2,

2

1 2 2L

k ∗ σ2

E σk+1 ≤

1− d

σk +

d Df (x

,x )+

, d

where σk2 = hk − ∇f (x∗) 2.

Corollary A.8. Let γ = 6L1d . Applying Theorem 4.1 with M = 4d2, iterates of Algorithm 8 satisfy

EV k ≤ 1 − µ k V 0 + σ2 .

6dL

Lµ

Proof of Lemma A.9

Let all expectations be conditioned on xk. For the ﬁrst bound, we write

gk − ∇f (x∗) = hk − ∇f (x∗) − dhki ei + d∇if (x∗)ei + dgi(xk, ξ)ei − d∇if (x∗)ei .

a

b

Let us bound the expectation of each term individually. The ﬁrst term can be bounded as

E a 2 = E I − deiei (hk − ∇f (x∗)) 22 = (d − 1) hk − ∇f (x∗) 2
≤ d hk − ∇f (x∗) 2 .

22

The second term can be bounded as

E b 2 = EiEξ dgi(x, ξ)ei − d∇fi(x∗)ei 2

=

EiEξ

dgi(xk, ξ)ei − d∇if (xk)ei

2
+ Ei

d∇if (xk)ei − d∇fi(x∗)ei 2

≤ dσ2 + d ∇f (xk) − ∇f (x∗) 2

≤ dσ2 + 2LdDf (xk, x∗),

where in the last step we used L–smoothness of f . It remains to combine the two bounds.

For the second bound, we have

E hk+1 − ∇f (x∗) 2 = E hk + gi(xk, ξ)ei − hki − ∇f (x∗) 2 = E I − eiei hk + gi(xk, ξ)ei − ∇f (x∗) 2 = E I − eiei (hk − ∇f (x∗)) 2 + E gi(xk, ξ)ei − ∇if (x∗)ei 2

=

1 1−

hk − ∇f (x∗) 2 + E gi(xk, ξ)ei − ∇if (xk)ei 2

d

+E ∇if (xk)ei − ∇if (x∗)ei 2

= 1 − 1 hk − ∇f (x∗) 2 + σ2 + 1 ∇f (xk) − ∇f (x∗) 2

d

dd

≤ 1 − 1 hk − ∇f (x∗) 2 + σ2 + 2L Df (xk, x∗).

d

dd

A.9 SVRG

Algorithm 9 SVRG [15]

Input: learning rate γ > 0, epoch length m, starting point x0 ∈ Rd φ = x0

for s = 0, 1, 2, . . . do

for k = 0, 1, 2, . . . , m − 1 do

Sample i ∈ {1, . . . , n} uniformly at random

gk = ∇fi(xk) − ∇fi(φ) + ∇f (φ)

xk+1 = proxγR(xk − γgk)

end for φ = x0 = m1 end for

m k=1 xk

n

Let

σk2

:=

1 n

∇fi(φ) − ∇fi(x∗) 2. We will show that Lemma C.1 recovers per-epoch analysis

i=1

of SVRG in a special case.

Lemma A.10. For k mod m = 0 we have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2

(40)

and

E σk2+1 | xk = σk2+1 = σk2.

(41)

Proof. The proof of (40) is identical to the proof of (34). Next, (41) holds since σk does not depend on k.

Thus, Assumption 4.1 holds with parameter choice as per Table 2 and Lemma C.1 implies the next corollary. Corollary A.9.
E xk+1 − x∗ 2 + γ(1 − 2γL)EDf (xk, x∗) ≤ (1 − γµ)E xk − x∗ 2 + 2γ2Eσk2. (42)

23

Recovering SVRG rate
Summing (42) for k = 0, . . . , m − 1 using σk = σ0 we arrive at
m
E xm − x∗ 2 + γ(1 − 2γL)EDf (xk, x∗) ≤ (1 − γµ)E x0 − x∗ 2 + 2mγ2Eσ02
k=1
≤ 2 µ−1 + 2mγ2L Df (x0, x∗) .

Since Df is convex in the ﬁrst argument, we have

mγ(1 − 2γL)Df

1 m xk, x∗ m
k=1

m
≤ xm − x∗ 2 + γ(1 − 2γL)Df (xk, x∗)
k=1

and thus

1m

2 µ−1 + 2mγ2L

Df

xk, x∗ ≤

Df (x0, x∗),

m

mγ(1 − 2γL)

k=1

which recovers rate from Theorem 1 in [15].

A.10 L-SVRG
In this section we show that our approach also covers L-SVRG analysis from [13, 18] (see Algorithm 10). Consider the ﬁnite-sum minimization problem
1n f (x) = n fi(x) + R(x), (43)
i=1
where fi is L-smooth for each i and f is µ-strongly convex.

Algorithm 10 L-SVRG [13, 18]
Input: learning rate γ > 0, probability p ∈ (0, 1], starting point x0 ∈ Rd w0 = x0 for k = 0, 1, 2, . . . do Sample i ∈ {1, . . . , n} uniformly at random gk = ∇fi(xk) − ∇fi(wk) + ∇f (wk) xk+1 = xk − γgk wk+1 = xk with probability p wk with probability 1 − p
end for

Note that the gradient estimator is again unbiased, i.e. E gk | xk = ∇f (xk). Next, Lemma A.11 provides with the remaining constants for Assumption 4.1. The corresponding choice is stated in Table 2.

Lemma A.11 (Lemma 4.2 and Lemma 4.3 from [18] extended to prox setup). We have

E gk − ∇f (x∗) 2 | xk ≤ 4LDf (xk, x∗) + 2σk2

(44)

and

E σk2+1 | xk ≤ (1 − p)σk2 + 2LpDf (xk, x∗),

(45)

where σk2 := n1 n ∇fi(wk) − ∇fi(x∗) 2.
i=1

Next, applying Theorem 4.1 on Algorithm 10 with M = p4 we get Corollary A.10.

Corollary A.10. L-SVRG with γ = 61L satisﬁes

EV k ≤ 1 − min

µp ,

k
V 0.

(46)

6L 2

24

Proof of Lemma A.11

Let all expectations be conditioned on xk. Using deﬁnition of gk

E gk − ∇f (x∗) 2

Alg. 10
=

E

∇fi(xk) − ∇fi(x∗) + ∇fi(x∗) − ∇fi(wk) + ∇f (wk) − ∇f (x∗) 2

(76)
≤

2E ∇fi(xk) − ∇fi(x∗) 2

+2E ∇fi(x∗) − ∇fi(wk) − E ∇fi(x∗) − ∇fi(wk) | xk 2

(37),(78)
≤ 4LDf (xk, x∗) + 2E

∇fi(wk) − ∇fi(x∗) 2

= 4LDf (xk, x∗) + 2σk2.

For the second bound, we shall have

E σk2+1

Alg. 10
=

pn

2

(1 − p)σk2 +

∇f (xk) − ∇f (x∗)

n

i=1

(37)
≤ (1 − p)σk2 + 2LpDf (xk, x∗).

A.11 DIANA

In this section we consider a distributed setup where each function fi from (3) is owned by i-th machine (thus, we have all together n machines).

We show that our approach covers the analysis of DIANA from [20, 14]. DIANA is a speciﬁc algorithm
for distributed optimization with quantization – lossy compression of gradient updates, which reduces the communication between the server and workers10.

In particular, DIANA quantizes gradient differences instead of the actual gradients. This trick allows for the linear convergence to the optimum once the full gradients are evaluated on each machine, unlike other popular quantization methods such as QSGD [1] or TernGrad [42]. In this case, DIANA behaves as variance reduced method – it reduces a variance that was injected due to the quantization. However, DIANA also allows for evaluation of stochastic gradients on each machine, as we shall further see.

First of all, we introduce the notion of quantization operator.

Deﬁnition A.1 (Quantization). We say that ∆ˆ is a quantization of vector ∆ ∈ Rd and write ∆ˆ ∼

Q(∆) if

E∆ˆ = ∆,

E

∆ˆ − ∆

2
≤ω

∆2

(47)

for some ω > 0.

The aforementioned method is applied to solve problem (1)+(3) where each fi is convex and L-smooth and f is µ-strongly convex.
Lemma A.12 (Lemma 1 and consequence of Lemma 2 from [14]). Suppose that α ≤ 1+1ω . For all iterations k ≥ 0 of Algorithm 11 it holds

E gk | xk E gk − ∇f (x∗) 2 | xk
E σk2+1 | xk

= ∇f (xk),

(48)

2ω 1 n

2

≤ 1+

∇fi(xk) − ∇fi(x∗)

nn

i=1

+ 2ωσk2

+

(1

+

ω)σ2 ,

(49)

n

n

αn

2

≤ (1 − α)σk2 +

∇fi(xk) − ∇fi(x∗) + ασ2. (50)

n

i=1

10It is a well-known problem in distributed optimization that the communication between machines often takes more time than actual computation.

25

Algorithm 11 DIANA [20, 14]

Input:

learning rates α > 0 and γ

> 0, initial vectors x0, h01, . . . , h0n

∈ Rd and h0

=

1 n

1: for k = 0, 1, . . . do

2: Broadcast xk to all workers

3: for i = 1, . . . , n in parallel do

4:

Sample gik such that E[gik | xk] = ∇fi(xk)

5:

∆ki = gik − hki

6:

Sample ∆ˆ ki ∼ Q(∆ki )

7:

hki +1 = hki + α∆ˆ ki

8:

gˆik = hki + ∆ˆ ki

9: end for

10:

∆ˆ k = n1

n i=1

∆ˆ ki

11:

gk = n1

n i=1

gˆik

=

hk

+

∆ˆ k

12: xk+1 = proxγR xk − γgk

13: hk+1 = n1 14: end for

n i=1

hki +1

=

hk

+

α∆ˆ k

n i=1

h0i

where σk2 = n1 n hki − ∇fi(x∗) 2 and σ2 is such that n1 n E

i=1

i=1

gik − ∇fi(xk) 2 | xk ≤ σ2.

Bounding further n1

n i=1

∇fi(xk) − ∇fi(x∗) 2 ≤ 2LDf (xk, x∗) in the above Lemma, we see

that Assumption 4.1 as per Table 2 is valid. Thus, as a special case of Theorem 4.1, we obtain the

following corollary.

Corollary A.11. Assume that fi is convex and L-smooth for all i ∈ [n] and f is µ strongly convex,

α≤ 1 ,γ≤

1

where M > 2ω . Then the iterates of DIANA satisfy

ω+1

(

1+

2ω n

)

L+M

Lα

nα

k

k

2ω

k 0

1+nω + M α σ2γ2

E V ≤ max (1 − γµ) , 1 + − α nM

V

+ min

γµ, α − 2ω

,

(51)

nM

where the Lyapunov function V k is deﬁned by V k := xk − x∗ 2 + M γ2σk2. For the particular

choice α = 1 , M = 4ω(ω+1) , γ = 1 , then DIANA converges to a solution neighborhood

ω+1

n

(

1+

6ω n

)L

and the leading iteration complexity term is

11

6ω

max

, γµ α −

2ω

= max κ + κ , 2(ω + 1) , n

(52)

nM

where κ = Lµ .

As mentioned, once the full (deterministic) gradients are evaluated on each machine, DIANA converges linearly to the exact optimum. In particular, in such case we have σ2 = 0. Corollary A.12 states the result in the case when n = 1, i.e. there is only a single node 11. For completeness, we present the mentioned simple case of DIANA as Algorithm 12.
Corollary A.12. Assume that fi is µ-strongly convex and L-smooth for all i ∈ [n], α ≤ ω+1 1 , γ ≤ (1+2ω)L1 +MLα where M > 2αω . Then the stochastic gradient gˆk and the objective function f satisfy Assumption 4.1 with A = (1 + 2ω) L, B = 2ω, σk2 = hk − h∗ 2 , ρ = α, C = Lα, D1 = 0, D2 = 0 and
E V k ≤ max (1 − γµ)k, 1 + 2ω − α k V 0, (53) M
where the Lyapunov function V k is deﬁned by V k := xk − x∗ 2 + M γ2σk2. For the particular choice α = ω+1 1 , M = 4ω(ω + 1), γ = (1+61ω)L the leading term in the iteration complexity bound

11node = machine

26

Algorithm 12 DIANA: 1 node & exact gradients [20, 14]
Input: learning rates α > 0 and γ > 0, initial vectors x0, h0 ∈ Rd 1: for k = 0, 1, . . . do 2: ∆k = ∇f (xk) − hk 3: Sample ∆ˆ k ∼ Q(∆k) 4: hk+1 = hk + α∆ˆ k 5: gk = hk + ∆ˆ k 6: xk+1 = proxγR xk − γgk 7: end for

is

11

max

, γµ

α

−

2ω

= max {κ + 6κω, 2(ω + 1)} ,

(54)

M

where κ = Lµ .

A.12 Q-SGD-SR In this section, we consider a quantized version of SGD-SR.

Algorithm 13 Q-SGD-SR
Input: learning rate γ > 0, starting point x0 ∈ Rd, distribution D over ξ ∈ Rn such that ED [ξ] is vector of ones for k = 0, 1, 2, . . . do Sample ξ ∼ D gk ∼ Q(∇fξ(xk)) xk+1 = proxγR(xk − γgk) end for

Lemma A.13 (Generalization of Lemma 2.4, [6]). If (f, D) ∼ ES(L), then

ED gk − ∇f (x∗) 2 ≤ 4L(1 + ω)Df (xk, x∗) + 2σ2(1 + ω).

(55)

where σ2 := ED ∇fξ(x∗) 2 .

A direct consequence of Theorem 4.1 in this setup is Corollary A.13. Corollary A.13. Assume that f (x) is µ-strongly quasi-convex and (f, D) ∼ ES(L). Q-SGD-SR with γk ≡ γ ≤ 2(1+1ω)L satisﬁes
E xk − x∗ 2 ≤ (1 − γµ)k x0 − x∗ 2 + 2γ(1 + ω)σ2 . µ

Then (56)

Proof of Lemma A.13 In this proof all expectations are conditioned on xk. First of all, from Lemma A.2 we have
ED ∇fξ(xk) − ∇f (x∗) 2 ≤ 4LDf (xk, x∗) + 2σ2.

The remaining step is to understand how quantization of ∇fξ(xk) changes the above inequality if we put gk ∼ Q(∇fξ(xk)) instead of ∇fξ(xk). Let us denote mathematical expectation with respect randomness coming from quantization by EQ [·]. Using tower property of mathematical expectation we get
E gk − ∇f (x∗) 2 = ED EQ gk − ∇f (x∗) 2
(=78) E gk − ∇fξ(xk) 2 + E ∇fξ(xk) − ∇f (x∗) 2
(55)
≤ E gk − ∇fξ(xk) 2 + 4LDf (xk, x∗) + 2σ2.

27

Algorithm 14 VR-DIANA based on L-SVRG (Variant 1), SAGA (Variant 2), [14]

Input:

learning rates α > 0 and γ

> 0, initial vectors x0, h01, . . . , h0n, h0

=

1 n

1: for k = 0, 1, . . . do

n i=1

h0i

2: Sample random uk = 1, with probability m1 0, with probability 1 − m1
3: Broadcast xk, uk to all workers

only for Variant 1

4: for i = 1, . . . , n in parallel do

5:

Pick random jik ∼u.a.r. [m]

m

6:

µki

=

1 m

∇fij (wikj )

j=1

Worker side

7: gik = ∇fijik (xk) − ∇fijik (wikjik ) + µki

8:

∆ˆ ki = Q(gik − hki )

9:

hki +1 = hki + α∆ˆ ki

10:

for j = 1, . . . , m do

11:

wk+1 = xk, if uk = 1 Variant 1 (L-SVRG): update epoch gradient if uk = 1

ij

wikj , if uk = 0

12:

wk+1 = xk, j = jik

ij

wikj , j = jik

Variant 2 (SAGA): update gradient table

13:

end for

14: end for n

15: hk+1 = hk + αn ∆ˆ ki

Gather quantized updates

i=1 n
16: gk = n1 (∆ˆ ki + hki )
i=1
17: xk+1 = xk − γgk

18: end for

Next, we estimate the ﬁrst term in the last row of the previous inequality

E gk − ∇fξ(xk) 2

(47)
≤ ωE ∇fξ(xk) 2
(76)
≤ 2ωE ∇fξ(xk) − ∇fξ(x∗) 2 + 2ωE ∇fξ(x∗) 2 ≤ 4ωLDf (xk, x∗) + 2ωσ2.

Putting all together we get the result.

A.13 VR-DIANA
Corollary A.11 shows that once each machine evaluates a stochastic gradient instead of the full gradient, DIANA converges linearly only to a certain neighborhood. In contrast, VR-DIANA [14] uses a variance reduction trick within each machine, which enables linear convergence to the exact solution. In this section, we show that our approach recovers VR-DIANA as well. The aforementioned method is applied to solve problem (1)+(3) where each fi is also of a ﬁnite sum structure, as in (4), with each fij(x) being convex and L-smooth, and fi(x) being µ-strongly convex. Note that ∇f (x∗) = 0 and, in particular, Df (x, x∗) = f (x) − f (x∗) since the problem is considered without regularization.
28

Lemma A.14 (Lemmas 3, 5, 6 and 7 from [14]). Let α ≤ ω+1 1 . Then for all iterates k ≥ 0 of Algorithm 14 the following inequalities hold:

E gk | xk E Hk+1 | xk E Dk+1 | xk E gk 2 | xk

= ∇f (xk),

(57)

≤ (1 − α) Hk + 2α Dk + 8αLn f (xk) − f (x∗) ,

(58)

m

≤

1 1−

Dk + 2Ln f (xk) − f (x∗) ,

(59)

m

4ω + 2 ≤ 2L 1 +
n

k

∗ 2ω Dk 2(ω + 1) k

f (x ) − f (x ) + n2 m + n2 H , (60)

where Hk = n hki − ∇fi(x∗) 2 and Dk = n m ∇fij (wikj ) − ∇fij (x∗) 2.

i=1

i=1 j=1

Corollary A.14. Let α ≤ min 31m , ω+1 1 . Then stochastic gradient gˆk (Algorithm 14) and the objective function f satisfy Assumption 4.1 with A = 1 + 4ωn+2 L, B = 2(ωn+1) , ρ = α, C = L m1 + 4α , D1 = 0, D2 = 0 and

Hk Dk 1 n σk2 = + =
n nm n
i=1

hki − ∇fi(x∗) 2 + 1 n m nm
i=1 j=1

∇fij (wikj ) − ∇fij (x∗) 2 .

Proof. Indeed, (7) holds due to (57). Inequality (8) follows from (60) with A = 1 + 4ωn+2 L, B =

2(ω+1) n

,

D1

=

0, σk2

=

Hnk + nDmk if we take into account that 2nω2 Dmk + 2(ωn+2 1) Hk

≤

2(ωn+1) nDmk + Hnk . Finally, summing inequalities (58) and (59) and using α ≤ 31m

E σk2 | xk

=
(58)+(59)
≤
≤

1 E

Hk+1 | xk

+

1 E Dk+1 | xk

n

nm

Hk

1 Dk

(1 − α) + 1 + 2α −

+ 2L

n

m nm

1 + 4α
m

(1 − α) σk2 + 2L m1 + 4α f (xk) − f (x∗)

f (xk) − f (x∗)

we get (9) with ρ = α, C = L m1 + 4α , D2 = 0.

Corollary A.15. Assume that fi is µ-strongly convex and fij is convex and L-smooth for all

i ∈ [n], j ∈ [m], α ≤ min 1 , 1 , γ ≤

1

where M > 2(ω+1) . Then the

3m ω+1

(1+

4ω+2 n

)

L+M

L

(

1 m

+4α)

nα

iterates of VR-DIANA satisfy

E V k ≤ max (1 − γµ)k, 1 + 2(ω + 1) − α k V 0, (61) nM

where the Lyapunov function V k is deﬁned by V k :=

xk − x∗ 2 +

M γ2σk2.

Further, if we set α = min 31m , ω+1 1 , M = 4(ωn+α1) , γ =

(1+

20ω

1
+18

+

4ω

+4

)

L

,

then

to

achieve

precision

E

n

nαm

xk − x∗ 2

≤ εV 0 VR-DIANA needs

O max κ + κ ωn+1 + κ (ω+1) mnaxm{m,ω+1} , m, ω + 1 log 1ε iterations, where κ = Lµ .

Proof. Using Corollary A.14 we apply Theorem 4.1 and get the result. Remark A.3. VR-DIANA can be easily extended to the proximal setup in our framework.

29

A.14 JacSketch

In this section, we show that our approach covers the analysis of JacSketch from [9]. JacSketch is a generalization of SAGA in the following manner. SAGA observes every iteration ∇fi(x) for random index i and uses it to build both stochastic gradient as well as the control variates on the stochastic gradient in order to progressively decrease variance. In contrast, JacSketch observes every iteration the random sketch of the Jacobian, which is again used to build both stochastic gradient as well as the control variates on the stochastic gradient.

For simplicity, we do not consider proximal setup, since [9] does not either.

We ﬁrst introduce the necessary notation (same as in [9]). Denote ﬁrst the Jacobian the objective

∇F(x) := [∇f1(x), . . . , ∇fn(x)] ∈ Rd×n.

(62)

Every iteration of the method, a random sketch of Jacobian ∇F (xk)S (where S ∼ D) is observed. Then, the method builds a variable Jk, which is the current Jacobian estimate, updated using so-called sketch and project iteration [7]:

Jk+1 = Jk(I − ΠSk ) + ∇F(xk)ΠSk ,

where ΠS is a projection under W norm12 (W ∈ Rn×n is some positive deﬁnite weight matrix) deﬁned as ΠS := S(S WS)†S W13.

Further, in order to construct unbiased stochastic gradient, an access to the random scalar θS such that

ED [θSΠS] e = e,

(63)

where e is the vector of all ones.

Next, the simplest option for the choice of the stochastic gradient is ∇fS(x) – an unbiased estimate of ∇f directly constructed using S, θS:

∇fS(x) = θS ∇F(x)ΠSe. (64) n

However, one can build a smarter estimate ∇fS,J(x) via control variates constructed from J:

∇fS,J(x) = θS (∇F(x) − J)ΠSe + 1 Je. (65)

n

n

The resulting algorithm is stated as Algorithm 15.

Algorithm 15 JacSketch [9]
Input: (D, W, θS), x0 ∈ Rd, Jacobian estimate J0 ∈ Rd×n, stepsize γ > 0 1: for k = 0, 1, 2, . . . do 2: Sample a fresh copy Sk ∼ D 3: Jk+1 = Jk(I − ΠSk ) + ∇F(xk)ΠSk 4: gk = ∇fSk,Jk (xk) 5: xk+1 = xk − γgk
6: end for

Next we present Lemma A.15 which directly justiﬁes the parameter choice from Table 1.
Lemma A.15 (Lemmas 2.5, 3.9 and 3.10 from [9]). Suppose that there are constants L1, L2 > 0 such that

ED

∇fS(x) − ∇fS(x∗)

2 2

≤ 2L1(f (x) − f (x∗)),

∀x ∈ Rd

ED

(∇F(x) − ∇F(x∗))ΠS

2 W−1

≤ 2L2(f (x) − f (x∗)),

∀x ∈ Rd,

12Weighted Frobenius norm of matrix X ∈ Rn×n with a positive deﬁnite weight matrix W ∈ Rn×n is deﬁned as X W−1 := Tr (XW−1X ).
13Symbol † stands for Moore-Penrose pseudoinverse.

30

Then ED

Jk+1 − ∇F(x∗) 2W−1 ≤ (1 − λmin) Jk − ∇F(x∗) 2W−1 + 2L2(f (xk) − f (x∗)), (66)

ED gk 2 ≤ 4L1(f (xk) − f (x∗)) + 2 λmax

2

n2

where λmin = λmin (ED [ΠS]) and λmax = λmax W1/2

Further, ED [∇fS,J(x)] = ∇f (x).

Jk − ∇F(x∗) 2W−1 , ED θS2 ΠSee ΠS − ee

(67) W1/2 .

Thus, as a direct consequence of Theorem 4.1, we obtain the next corollary. Corollary A.16. Consider the setup from Lemma A.15. Suppose that f is µ-strongly convex and choose γ ≤ min µ1 , 2L1+1M Ln2 where M > 2nλλmmainx . Then the iterates of JacSketch satisfy
E V k ≤ max (1 − γµ)k, 1 + 2λmax − λmin k V 0. (68) nM

A.15 Interpolation between methods

Given that a set of stochastic gradients satisfy Assumption 4.1, we show that an any convex combina-

tion of the mentioned stochastic gradients satisfy Assumption 4.1 as well.

Lemma A.16. Assume that sequences of stochastic gradients {g1k}k≥0, . . . , {gmk }k≥0

at the common iterates {xk}k≥0 satisfy the Assumption 4.1 with parameters

A

(j

)

,

B

(j

)

,

{σ

2 k

(j

)}

k

≥

0

,

C

(

j

),

ρ(j

),

D1

(j

),

D2

(j

),

j

∈

[m] respectively.

Then for any vec-

m

m

tor τ = (τ1, . . . , τm) such as τj = 1 and τj ≥ 0, j ∈ [m] stochastic gradient gτk = τjgjk

j=1

j=1

satisﬁes the Assumption 4.1 with parameters:

m
Aτ = τjA(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τj σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τjC(j)B(j), Dτ,1 = τjD1(j), Dτ,2 = τjD2(j)B(j).

(69)

j=1

j=1

j=1

Furthermore, if stochastic gradients g1k, . . . , gmk are independent for all k, Assumption 4.1 is satisﬁed with parameters

m
Aτ = L + τj2A(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τj2σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τj2C(j)B(j), Dτ,1 = τj2D1(j), Dτ,2 = τj2D2(j)B(j).

(70)

j=1

j=1

j=1

What is more, instead of taking convex combination one can choose stochastic gradient at random. Lemma A.17 provides the result.

Lemma A.17. Assume that sequences of stochastic gradients {g1k}k≥0, . . . , {gmk }k≥0

at the common iterates {xk}k≥0 satisfy the Assumption 4.1 with parameters

A

(j

)

,

B

(j

)

,

{σ

2 k

(j

)}

k

≥

0

,

C

(

j

),

ρ(j

),

D1

(j

),

D2

(j

),

j

∈

[m] respectively.

Then for any vec-

m

tor τ = (τ1, . . . , τm) such as τj = 1 and τj ≥ 0, j ∈ [m] stochastic gradient gτk which equals

j=1

gjk with probability τj satisﬁes the Assumption 4.1 with parameters:

m
Aτ = τjA(j),
j=1

Bτ = 1,

m
στ2,k = τj B(j)σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τjB(j)C(j), Dτ,1 = τjD1(j), Dτ,2 = B(j)τjD2(j).

(71)

j=1

j=1

j=1

31

Furthermore, if stochastic gradients g1k, . . . , gmk are independent for all k, Assumption 4.1 is satisﬁed with parameters

m
Aτ = L + τj2A(j),
j=1

Bτ = 1,

m
στ2,k = B(j)τj2σk2(j),
j=1

ρτ = min ρ(j),
j∈[m]

m

m

m

Cτ = τj2C(j)B(j), Dτ,1 = τj2D1(j), Dτ,2 = τj2D2(j)B(j).

(72)

j=1

j=1

j=1

Example A.1 (τ -L-SVRG). Consider the following method — τ -L-SVRG — which interpolates between vanilla SGD and L-SVRG. When τ = 0 the Algorithm 16 becomes L-SVRG and when τ = 1

Algorithm 16 τ -L-SVRG
Input: learning rate γ > 0, probability p ∈ (0, 1], starting point x0 ∈ Rd, convex combination parameter τ ∈ [0, 1] w0 = x0 for k = 0, 1, 2, . . . do Sample i ∈ {1, . . . , n} uniformly at random gLk−SV RG = ∇fi(xk) − ∇fi(wk) + ∇f (wk) Sample j ∈ {1, . . . , n} uniformly at random gSkGD = ∇fj (xk) gk = τ gSkGD + (1 − τ )gLk −SV RG xk+1 = xk − γgk wk+1 = xk with probability p wk with probability 1 − p
end for

it is just SGD with uniform sampling. Notice that Lemmas A.11 and A.2 still hold as they does not depend on the update rule for xk+1.

Thus, sequences {gSkGD}k≥0 and {gLk−SV RG}k≥0 satisfy the conditions of Lemma A.16 and, as a consequence, stochastic gradient gk from τ -L-SVRG meets the Assumption 4.1 with the following

parameters:

Aτ = L + 2τ 2L + 2(1 − τ )2L,

Bτ = 1,

(1 − τ )2 n στ2,k = 2
n
i=1

∇fi(wk) − ∇fi(x∗) 2 ,

ρτ = p, Cτ = 2(1 − τ )2Lp, Dτ,1 = 2τ 2σ2, Dτ,2 = 0.

Remark A.4. Similar interpolation with the analogous analysis can be considered between SGD and SAGA, or SGD and SVRG.

Proof of Lemma A.16

Indeed, (7) holds due to linearity of mathematical expectation. Next, summing inequalities (8) for

g1k, . . . , gmk and using convexity of · 2 we get

m

E gτk − ∇f (x∗) 2 | xk ≤

τj E gjk − ∇f (x∗) 2 | xk

j=1

(8)

m

m

m

≤ 2 τjA(j)Df (xk, x∗) + B(j)τjσk2(j) + τjD1(j),

j=1

j=1

j=1

m

m

which implies (8) for gτk with Aτ =

τj A(j), Bτ = 1, στ2,k =

τ

j

B

(j

)

σ

2 k

(

j

),

Dτ

,

1

=

j=1

j=1

m
τjD1(j). Finally, summing (9) for g1k, . . . , gmk gives us

j=1

(9)
E στ2,k+1 | στ2,k ≤

1 − min ρ(j)
j∈[m]

m

m

στ2,k + 2 τjB(j)C(j)Df (xk, x∗) + τjB(j)D2(j),

j=1

j=1

32

m

m

which is exactly (9) for στ2,k with ρ = min ρ(j), Cτ = τjC(j), Dτ,2 = τjD2(j).

j∈[m]

j=1

j=1

Next, for independent gradients we have

E gτk − ∇f (x∗) 2 | xk

m

=

τj2E

j=1

m

=

τj2E

j=1

m

≤

τj2E

j=1

m

=

τj2E

j=1

m

≤

τj2E

j=1

gjk − ∇f (x∗) 2 | xk + 2 τiτjE gjk − ∇f (x∗), gik − ∇f (x∗)
i<j

gjk − ∇f (x∗) 2 | xk gjk − ∇f (x∗) 2 | xk

+ 2 τiτj ∇f (xk) − ∇f (x∗) 2

i<j



2

m

+  τj ∇f (xk) − ∇f (x∗) 2

j=1

gjk − ∇f (x∗) 2 | xk + ∇f (xk) − ∇f (x∗) 2

gjk − ∇f (x∗) 2 | xk + 2LDf (xk, x∗).

(73)

and further the bounds follow.

Proof of Lemma A.17

Indeed, (7) holds due to linearity and tower property of mathematical expectation. Next, using tower property of mathematical expectation and inequalities (8) for g1k, . . . , gmk we get

E gτk − ∇f (x∗) 2 | xk

= E Eτ

m
gτk − ∇f (x∗) 2 | xk = τj E
j=1

gjk − ∇f (x∗) 2 | xk

(8)

m

m

m

≤ 2 τjA(j)Df (xk, x∗) + B(j)τjσk2(j) + τjD1(j),

j=1

j=1

j=1

m

m

which implies (8) for gτk with Aτ =

τj A(j), Bτ = 1, στ2,k =

τ

j

B

(j

)

σ

2 k

(

j

),

Dτ

,

1

=

j=1

j=1

m
τjD1(j). Finally, summing (9) for g1k, . . . , gmk gives us

j=1

(9)
E στ2,k+1 | στ2,k ≤

1 − min ρ(j)
j∈[m]

m

m

στ2,k + 2 τjB(j)C(j)Df (xk, x∗) + τjB(j)D2(j),

j=1

j=1

m

m

which is exactly (9) for στ2,k with ρ = min ρ(j), Cτ = τjB(j)C(j), Dτ,2 = τjB(j)D2(j).

j∈[m]

j=1

j=1

To show (72), it sufﬁces to combine above bounds with the trick (73).

Remark A.5. Recently, [39] demonstrated in that the convex combination of SGD and SARAH [27] performs very well on non-convex problems.

33

B Extra Experiments

B.1 SGD-MB: remaining experiments and exact problem setup.

As already described in Section 6, we demonstrate that SGD-MB have indistinguishable iteration

complexity to independent SGD. The considered problem is logistic regression with Tikhonov regular-

ization of order λ:

1n

λ2

n

log 1 + exp ai x · bi

+x 2

,

(74)

i=1

where ai ∈ Rn, bi ∈ {−1, 1} is i-th data-label pair is a vector of labels and λ ≥ 0 is the regularization
parameter. The data and labels were obtained from LibSVM datasets a1a, a9a, w1a, w8a, gisette, madelon, phishing and mushrooms. Further, the data were rescaled by a random variable cu2i where ui is random integer from 1, 2, . . . , 1000 and c is such that the mean norm of ai is 1.

Note that we have now an inﬁnite array of possibilities on how to write (74) as (3). For simplicity, distribute l2 term evenly among the ﬁnite sum.

The full results can be found in Figure 2.

Relative suboptimality

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 10, unif

100

r: True, gamma: 0.65601

r: True, gamma: 0.06560

r: False, gamma: 0.65601

10 1

r: False, gamma: 0.06560

10 2

10 3

0
100 10 1 10 2 10 3 10 4 10 5
0

5000 10000 15000Ite2r0a0t0i0on25000 30000 35000 40000 Dataset: w1a, tau: 10, unif
r: True, gamma: 0.96201 r: True, gamma: 0.09620 r: False, gamma: 0.96201 r: False, gamma: 0.09620 10000 20000 It3e0r0a0t0ion 40000 50000 60000
Dataset: madelon, tau: 10, unif

100

10 1

r: True, gamma: 0.38553

r: True, gamma: 0.03855

10 2

r: False, gamma: 0.38553

r: False, gamma: 0.03855

0

10000 2000It0eratio3n0000 40000 50000

Dataset: gisette_scale, tau: 10, unif
100

10 1

10 2

10 3

10 4

r: True, gamma: 0.48750 r: True, gamma: 0.04875

r: False, gamma: 0.48750

10 5

r: False, gamma: 0.04875

0 20000 40000 6000It0era8t0i0o0n0 100000 120000 140000

Dataset: mushrooms, tau: 10, unif
100

10 1

10 2

10 3

10 4

10 5

10 6

r: True, gamma: 0.61851 r: True, gamma: 0.06185

10 7

r: False, gamma: 0.61851

10 8

r: False, gamma: 0.06185

0 25000 50000 75000It1e0r0a0t0i0on125000 150000 175000 200000

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 10, imp

100

r: True, gamma: 0.90368

r: True, gamma: 0.09037

r: False, gamma: 0.90368

r: False, gamma: 0.09037

10 1

10 2

0 100 10 1

5000 10000 15000Ite2r0a0t0i0on25000 30000 35000 40000
Dataset: w1a, tau: 10, imp r: True, gamma: 2.26520 r: True, gamma: 0.22652 r: False, gamma: 2.26520 r: False, gamma: 0.22652

10 2

10 3 0

10000 20000 It3e0r0a0t0ion 40000 50000 60000 Dataset: madelon, tau: 10, imp

100

10 1 10 2
0 100 10 1

r: True, gamma: 0.45669 r: True, gamma: 0.04567 r: False, gamma: 0.45669 r: False, gamma: 0.04567
10000 2000It0eratio3n0000 40000 50000
Dataset: gisette_scale, tau: 10, imp r: True, gamma: 0.60646 r: True, gamma: 0.06065 r: False, gamma: 0.60646 r: False, gamma: 0.06065

10 2

10 3 0
100 10 1

20000 40000 6000It0era8t0i0o0n0 100000 120000 140000
Dataset: mushrooms, tau: 10, imp r: True, gamma: 0.84208 r: True, gamma: 0.08421 r: False, gamma: 0.84208 r: False, gamma: 0.08421

10 2

10 3

0 25000 50000 75000It1e0r0a0t0i0on125000 150000 175000 200000

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 50, unif

100

r: True, gamma: 0.98071

r: True, gamma: 0.09807

10 1

r: False, gamma: 0.98071 r: False, gamma: 0.09807

10 2

10 3

10 4 0
100 10 1 10 2 10 3 10 4 10 5 10 6
0
100

5000 10000 15000Ite2r0a0t0i0on25000 30000 35000 40000 Dataset: w1a, tau: 50, unif r: True, gamma: 2.59545 r: True, gamma: 0.25955 r: False, gamma: 2.59545 r: False, gamma: 0.25955
10000 20000 It3e0r0a0t0ion 40000 50000 60000 Dataset: madelon, tau: 50, unif

10 1

10 2
0
100 10 1 10 2

r: True, gamma: 0.47525 r: True, gamma: 0.04752 r: False, gamma: 0.47525 r: False, gamma: 0.04752
10000 2000It0eratio3n0000 40000 50000
Dataset: gisette_scale, tau: 50, unif r: True, gamma: 0.64045 r: True, gamma: 0.06404 r: False, gamma: 0.64045 r: False, gamma: 0.06404

10 3

10 4

10 5 0
100 10 1 10 2 10 3 10 4 10 5 10 6

20000 40000 6000It0era8t0i0o0n0 100000 120000 140000
Dataset: mushrooms, tau: 50, unif r: True, gamma: 0.90749 r: True, gamma: 0.09075 r: False, gamma: 0.90749 r: False, gamma: 0.09075

0 25000 50000 75000It1e0r0a0t0i0on125000 150000 175000 200000

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Relative suboptimality

Dataset: a1a, tau: 50, imp

100

r: True, gamma: 1.05639

r: True, gamma: 0.10564

r: False, gamma: 1.05639

10 1

r: False, gamma: 0.10564

10 2

10 3

0 100 10 1

5000 10000 15000Ite2r0a0t0i0on25000 30000 35000 40000
Dataset: w1a, tau: 50, imp r: True, gamma: 3.38902 r: True, gamma: 0.33890 r: False, gamma: 3.38902 r: False, gamma: 0.33890

10 2

10 3

10 4 0
100

10000 20000 It3e0r0a0t0ion 40000 50000 60000 Dataset: madelon, tau: 50, imp

10 1

10 2 0
100 10 1

r: True, gamma: 0.49269 r: True, gamma: 0.04927 r: False, gamma: 0.49269 r: False, gamma: 0.04927
10000 2000It0eratio3n0000 40000 50000
Dataset: gisette_scale, tau: 50, imp r: True, gamma: 0.67163 r: True, gamma: 0.06716 r: False, gamma: 0.67163 r: False, gamma: 0.06716

10 2

10 3

0
100 10 1 10 2

20000 40000 6000It0era8t0i0o0n0 100000 120000 140000
Dataset: mushrooms, tau: 50, imp r: True, gamma: 0.97322 r: True, gamma: 0.09732 r: False, gamma: 0.97322 r: False, gamma: 0.09732

10 3

10 4
0 25000 50000 75000It1e0r0a0t0i0on125000 150000 175000 200000

Relative suboptimality

Relative suboptimality

Figure 2: SGD-MB and independent SGD applied on LIBSVM [3] datasets with regularization parameter λ = 10−5. Axis y stands for relative suboptimality, i.e. ff((xxkk))−−ff((xx∗0)) . Title label “unif” corresponds to probabilities chosen by (i) while label “imp” corresponds to probabilities chosen by (ii). Lastly,
legend label “r” corresponds to “replacement” with value “True” for SGD-MB and value “False” for independent SGD.

34

Note that plots which are not included in the main body (due to space limitations) only support claims from Section 6.

B.2 Experiments on SGD-star

In this section, we study SGD-star and numerically verify claims from Section A.4. In particu-

lar, Corollary A.4 shows that SGD-star enjoys linear convergence rate which is constant times

better to the rate of SAGA (given that problem condition number is high enough). We compare

3 methods – SGD-star, SGD and SAGA. We consider simple and well-understood least squares

problem

minx

1 2

Ax − b

2 where elements of A, b were generated (independently) from standard

normal distribution. Further, rows of A were normalized so that Ai: = 1. Thus, denoting

fi(x)

=

1 2

(A

i

:

x

−

bi)2,

fi

is

1-smooth.

For

simplicity,

we

consider

SGD-star

with

uniform

serial

sampling, i.e. L = 1.

Next, for both SGD-star and SGD we use stepsize γ = 12 (theory supported stepsize for SGD-star), while for SAGA we set γ = 51 (almost theory supported stepsize). Figure 3 shows the results.

Figure 3: Comparison of SGD-star, SGD and SAGA on least squares problem.
Note that, as theory predicts, SGD-star is always faster to SAGA, although only constant times. Further, in the cases where d ≥ n, performance of SGD seems identical to the performance of SGD-shift. This is due to a simple reason: if d ≥ n, we must have ∇fi(x∗) = 0 for all i, and thus SGD and SGD-shift are in fact identical algorithms.
B.3 Experiments on N-SEGA
In this experiment we study the effect of noise on N-SEGA. We consider unit ball constrained least squares problem: min x ≤1 f (x) where f (x) = Ax − b 2. and we suppose that there is an oracle providing us with noised partial derivative gi(x, ζ) = ∇if (x) + ζ, where ζ ∼ N (0, σ2). For each problem instance (i.e. pair A, b), we compare performance of N-SEGA under various noise magnitudes σ2. The speciﬁc problem instances are presented in Table 3. Figure 4 shows the results.
35

Type
1 2 3 4

A
Aij ∼ N (0, 1) (independently) Same as 1, but scaled so that λmax(A A) = 1 Aij = ij j ∀i, j : ij, j ∼ N (0, 1) (independently) Same as 3, but scaled so that λmax(A A) = 1
Table 3: Four types of least squares.

b
vector of ones vector of ones vector of ones vector of ones

We shall mention that this experiment serves to support and give a better intuition about the results from Section A.8 and is by no means practical. The results show, as predicted by theory, linear convergence to a speciﬁc neighborhood of the objective. The effect of the noise varies, however, as a general rule, the larger strong convexity µ is (i.e. problems 1,3 where scaling was not applied), the smaller the effect of noise is.

Figure 4: N-SEGA applied on constrained least squares problem with noised partial derivative oracle. Legend labels stand for the magnitude σ2 of the oracle noise.

36

C Proofs for Section 4

C.1 Basic Facts and Inequalities

For all a, b ∈ Rd and ξ > 0 the following inequalities holds:

a2 ξ b2

a, b ≤

+

,

(75)

2ξ

2

a+b 2 ≤2 a 2+2 b 2,

(76)

and 1 a 2 − b 2 ≤ a + b 2 . (77) 2

For a random vector ξ ∈ Rd and any x ∈ Rd the variance can be decomposed as

E ξ − Eξ 2 = E ξ − x 2 − E Eξ − x 2 .

(78)

C.2 A Key Lemma
The following lemma will be used in the proof of our main theorem. Lemma C.1 (Key single iteration recurrence). Let Assumptions 4.1 and 4.2 be satisﬁed. Then the following inequality holds for all k ≥ 0:
E xk+1 − x∗ 2 + M γ2E σk2+1 + 2γ (1 − γ(A + CM )) E Df (xk, x∗) ≤ (1 − γµ)E xk − x∗ 2 + (1 − ρ) M γ2E σk2 + Bγ2E σk2 + (D1 + M D2)γ2.

Proof. We start with estimating the ﬁrst term of the Lyapunov function. Let rk = xk − x∗. Then

rk+1 2 = ≤ =

proxγR(xk − γgk) − proxγR(x∗ − γ∇f (x∗)) 2 xk − x∗ − γ(gk − ∇f (x∗)) 2 rk 2 − 2γ rk, gk − ∇f (x∗) + γ2 gk − ∇f (x∗) 2 .

Taking expectation conditioned on xk we get

E rk+1 2 | xk

=

rk 2 − 2γ rk, ∇f (xk) − ∇f (x∗) + γ2E gk − ∇f (x∗) 2 | xk

(12)
≤

(1 − γµ) rk 2 − 2γDf (xk, x∗) + γ2E gk − ∇f (x∗) 2 | xk

(7)≤+(8) (1 − γµ) rk 2 + 2γ (Aγ − 1) Df (xk, x∗) + Bγ2σk2 + γ2D1.

Using this we estimate the full expectation of V k+1 in the following way:

E xk+1 − x∗ 2 + M γ2Eσk2+1 (≤9) (1 − γµ)E xk − x∗ 2 + 2γ (Aγ − 1) Df (xk, x∗) + Bγ2Eσk2
+(1 − ρ)M γ2Eσk2 + 2CM γ2E Df (xk, x∗) + (D1 + M D2)γ2 = (1 − γµ)E xk − x∗ 2 + 1 + MB − ρ M γ2Eσk2
+2γ (γ(A + CM ) − 1) E Df (xk, x∗) + (D1 + M D2)γ2 .

It remains to rearrange the terms.

37

C.3 Proof of Theorem 4.1

Note ﬁrst that due to (13) we have 2γ (1 − γ(A + CM )) EDf (xk, x∗) > 0, thus we can omit the term.

Unrolling the recurrence from Lemma C.1 and using the Lyapunov function notation gives us

EV k ≤ max (1 − γµ)k, 1 + B − ρ k V 0 M

k−1
+(D1 + M D2)γ2 max
l=0

(1 − γµ)l,

B

l

1+ −ρ

M

≤ max (1 − γµ)k, 1 + B − ρ k V 0 M

∞
+(D1 + M D2)γ2 max
l=0

(1 − γµ)l,

B

l

1+ −ρ

M

k

B

k 0 (D1 + M D2)γ2

≤ max (1 − γµ) , 1 + − ρ M

V

+ min

γµ, ρ − B

.

M

38

