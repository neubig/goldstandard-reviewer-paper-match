Stochastic optimization under distributional drift

Joshua Cutler∗

Dmitriy Drusvyatskiy†

Zaid Harchaoui‡

arXiv:2108.07356v2 [math.OC] 25 Nov 2021

Abstract
We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The eﬃciency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we show that the tracking eﬃciency of the proximal stochastic gradient method depends only logarithmically on the initialization quality, when equipped with a stepdecay schedule. Numerical experiments illustrate our results.

1 Introduction

Stochastic optimization underpins much of machine learning theory and practice. Signiﬁcant progress has been made over the last two decades in the ﬁnite-time analysis of stochastic approximation algorithms [1, 2, 8, 9, 10, 33, 38, 45, 46]. The predominant assumption in much of the work on stochastic optimization for machine learning is that the distribution generating the data is ﬁxed throughout the run of the process. There is no shortage of problems, however, where this assumption is grossly violated. There are two main sources of such distributional shifts. The ﬁrst is temporal, where the distribution varies slowly in time due to reasons that are independent of the learning process. This setting is often called dynamic stochastic approximation [16] and is the basis for adaptive algorithms for stochastic tracking [6]. The second common source is due to a feedback mechanism, wherein the distribution generating the data may depend on, or react to, the decisions made by the learner. This setting has been a subject of increased interest recently in the context of strategic classiﬁcation and performative prediction [15, 37].
In this work, we present ﬁnite-time eﬃciency estimates in expectation and with high probability for the tracking error of the proximal stochastic gradient method under time drift. The results are presented in a single framework that encompasses both the purely temporal and the decisiondependent time drift. Our results concisely explain the interplay between the learning rate, the noise variance in the gradient oracle, and the strength of the time drift. While conventional wisdom and previous work recommend the use of constant step sizes under time drift, we show that in important regimes a signiﬁcantly better step size schedule is one that is geometrically decaying to a “critical step size”.

∗Department of Mathematics, University of Washington, Seattle, WA 98195. jocutler@uw.edu.

†Department of Mathematics,

University of Washington,

Seattle,

WA 98195.

sites.math.washington.edu/∼ddrusv/. Research was funded by NSF DMS-1651851 and CCF-2023166 awards.

‡Department of Statistics, University of Washington, Seattle, WA 98195. faculty.washington.edu/zaid/. Re-

search was funded by NSF CCF-2019844, CIFAR-LMB, and faculty research awards. Part of this work was done

while Z. Harchaoui was visiting the Simons Institute for the Theory of Computing.

1

Setting the stage, consider the sequence of stochastic optimization problems

min ϕt(x) := ft(x) + rt(x)

(1)

x

indexed by time t ∈ N. In typical machine learning and signal processing settings, the function ft corresponds to an average loss that varies in time, while the regularizer rt models constraints or

promotes structure (e.g. sparsity) in the variable x. Two examples are worth highlighting. The

ﬁrst is a classical problem in signal processing related to stochastic tracking [31, 43], wherein the

learning algorithm aims to track over time a moving target driven by an unknown stochastic process.

The second example is the concept drift phenomenon in online learning [24, 44], wherein the true

hypothesis may be changing over time.

The main goal of a learning algorithm for problem (1) is to generate a sequence of points {xt} that

minimize some natural performance metric. To make progress, we impose the standard assumption

that each function ft is µ-strongly convex with L-Lipschitz continuous gradient, while each regularizer

rt is proper, closed, and convex. The online proximal stochastic gradient method (PSG) naturally

applies to the sequence of problems (1). At each time t, the method simply takes a step

xt+1 = proxηtrt xt − ηt∇ft(xt) ,

where the vector ∇ft(xt) is an unbiased estimator of the true gradient of ft at xt, the constants
ηt > 0 are user-speciﬁed, and proxηtrt(·) is the proximal map of the scaled regularizer ηtrt. In this work, we analyze two types of tracking error for PSG: the squared distance xt − xt 2 and the suboptimality gap ϕt(xˆt) − ϕt(xt ). Here, xt denotes the minimizer of the function ϕt and may evolve stochastically in time, while xˆt denotes a weighted average of iterates up to time t. We next outline
the main results of the paper.

1.1 Tracking the distance

We begin with a simple bound on distance tracking of the constant-step PSG:

E xt − xt 2 (1 − µη)t x0 − x0 2 + ηµσ2 + µ∆η 2 . (2)

optimization

noise

drift

Here η ∈ (0, 1/2L] is the constant step size used by PSG, σ2 upper-bounds the variance of the stochastic gradient, and ∆2 upper-bounds the minimizer variations E xt − xt+1 2. Inequality (2) asserts that the tracking error E xt − xt 2 decays linearly in time t, until it reaches the “noise+drift” error ησ2/µ + (∆/µη)2. Notice that the “noise+drift” error cannot be made arbitrarily small by tuning η. This is perfectly in line with intuition: a step size η that is too small prevents the algorithm from catching up with the minimizers xt . We note that the individual error terms due to the optimization and noise are classically known to be tight for PSG; tightness of the drift term is proved in Madden et al. [35, Theorem 3.2]. Though the estimate (2) is likely known, we were unable to ﬁnd a precise reference in this generality.
Letting t tend to inﬁnity in (2), the optimization error tends to zero, leaving only the noise+drift term. Optimizing this remaining term over η, it is natural to deﬁne the asymptotic distance tracking error of PSG and the corresponding optimal learning rate as

E := min
η∈(0,1/2L]

ησ2 + µ

∆2 µη

1 2∆2 1/3

and η := min 2L , µσ2

.

Two regimes of variation are brought to light: the high drift-to-noise regime ∆/σ ≥ µ/16L3, and the low drift-to-noise regime ∆/σ < µ/16L3. The high drift-to-noise regime is uninteresting from

2

the viewpoint of stochastic optimization because the optimal learning rate is as large as in the deterministic setting, η = 1/2L. In contrast, the low drift-to-noise regime is interesting because it necessitates using a smaller learning rate that exhibits a nontrivial scaling with the problem parameters. Consequently, for the rest of the introduction we focus on the low drift-to-noise regime.
A central question is to ﬁnd a learning rate schedule that achieves a tracking error E xt − xt 2 that is within a constant factor of E in the shortest possible time. The simplest strategy is to execute PSG with the constant learning rate η . Then a direct application of (2) yields the eﬃciency estimate E xt − xt 2 E in time t (σ2/µ2E) log( x0 − x0 2/E). This eﬃciency estimate can be signiﬁcantly improved by gradually decaying the learning rate using a “step-decay schedule”, wherein the algorithm is implemented in epochs with the new learning rate chosen to be the midpoint between the current learning rate and η . Such schedules are well known to improve eﬃciency in the static setting, as was discovered in Ghadimi and Lan [19, 20], and can be used here. The end result is an algorithm that produces a point xt satisfying
E xt − xt 2 E after time t Lµ log x0 −E x0 2 + µσ22E . (3)
Remarkably, this eﬃciency estimate looks identical to that in the classical static setting [32], with E playing the role of the target accuracy ε. In particular, note that (3) is independent of the dimension of the ambient space.
The estimate (3) is a baseline guarantee for PSG. Since the result is stated in terms of the expected tracking error E xt − xt 2, it is only meaningful if the entire algorithm can be repeated from scratch multiple times on the same problem. However, there is no shortage of situations in which a learning algorithm is operating in real time and the time drift is irreversible; in such settings, the algorithm may only be executed once. Such settings call for eﬃciency estimates that hold with high probability, rather than only in expectation. With this in mind, we show that under mild light tail assumptions, restarted PSG with probability at least 1 − δ produces a point xt satisfying
xt − xt 2 E log (1/δ) after the same order of iterations as in (3). The proof follows closely the probabilistic techniques developed in Harvey et al. [23] for bounding moment generating functions.

1.2 Tracking the optimal value

The results outlined so far have focused on tracking the minimizer xt ; stronger guarantees may be
obtained for tracking the minimal value ϕt . To this end, we require stronger assumptions on the variation of the functions ft beyond control on the minimizer drift xt − xt+1 2. Namely, following the online learning literature [24], we will be concerned with the gradient drift

Gi,t := sup ∇fi(x) − ∇ft(x)
x

and assume the bound E G2i,t/µ2 ≤ ∆2|i − t|2. Thus the second moment of the gradient drift should grow at most quadratically in the time horizon. Assuming henceforth that the regularizers rt ≡ r are identical for all times t, this condition on the gradient drift implies E xt − xt+1 2 ≤ ∆2, a weaker assumption used in Section 1.1.
Analogously to (2), we show that PSG generates a point xˆt (an average iterate) satisfying

E ϕt(xˆt) − ϕt

1

−

µη 2

t

ϕ0(x0) − ϕ0

optimization

+ ησ2 + ∆2 . µη2
noise drift

Again, the eﬃciency estimate decouples nicely into three terms, signifying the error due to optimization, gradient noise, and time drift. Taking the limit as t tends to inﬁnity, the asymptotically

3

optimal function gap tracking error is precisely the corresponding distance tracking error E scaled by µ. Similarly to (3), we show that restarted PSG produces a point xˆt satisfying
E ϕt(xˆt) − ϕt G after time t Lµ log ϕ0(x0G) − ϕ0 + µσG2 , (4)
where G := µE is the optimal asymptotic tracking error of the function gap. Again, the similarity to the static setting [32], with G playing the role of a target accuracy, is striking. We then provide a high probability extension of this estimate: under mild light tail assumptions, restarted PSG with probability at least 1 − δ produces a point xˆt satisfying ϕt(xˆt) − ϕt G log (1/δ) after the same order of iterations as in (4) up to a factor of log log(1/δ). The proofs are based on the generalized Freedman inequality of Harvey et al. [23]—a remarkably ﬂexible tool for analyzing stochastic gradient-type algorithms.
1.3 Extensions to decision-dependent problems with time drift
We have so far focused on stochastic optimization problems that undergo a temporal shift. A primary reason for this phenomenon in machine learning, and data science more broadly, is that data distributions often evolve in time independently of the learning process. Recent literature, on the other hand, highlights a diﬀerent source of distributional shift due to decision-dependent or performative eﬀects. Namely, the distribution generating the data in iteration t may depend on, or react to, the current “decision” xt. For example, deployment of a classiﬁer by a learning system, when made public, often causes the population to adapt their attributes in order to increase the likelihood of being positively labeled—a process called “gaming”. Even when the population is agnostic to the classiﬁer, the decisions made by the learning system (e.g. loan approval) may inadvertently alter the proﬁle of the population (e.g. credit score). The goal of the learning system therefore is to ﬁnd a classiﬁer that generalizes well under the response distribution. Recent research in strategic classiﬁcation [5, 11, 14, 22] and performative prediction [36, 39] has highlighted the prevalence of this phenomenon.
Combining time-dependence and decision-dependence yields the class of problems (1) where the loss function ft(x) takes the special form ft(x) = Ew∼D(t,x) (x, w). Here D(t, x) is a distribution that depends on both time t and the decision variable x. Thus for any ﬁxed time t, the problem (1) becomes the performative risk problem considered in Perdomo et al. [39] and Mendler-Dünner et al. [36]. Following this line of work, instead of tracking the true minimizer of ϕt—typically a challenging task—we will settle for tracking the equilibrium points x¯t. These are the points satisfying
x¯t ∈ arg min E (x, w) + r(x)
x w∼D(t,x¯t)
Equilibrium points are sure to exist and are unique under mild Lipschitzness and strong convexity assumptions. We refer the reader to Perdomo et al. [39] for a compelling motivation for considering such equilibrium points. The problem of tracking equilibrium points is yet again an instance of (1), but now with the diﬀerent function ft(x) = Ew∼D(t,x¯t) (x, w), induced by the equilibrium distributions. The PSG algorithm is not directly applicable since one cannot sample from D(t, x¯t) directly. Instead, a natural algorithm for this problem class in each iteration samples wt from the current distribution D(t, xt) and declares xt+1 = proxηtr(xt − ηt∇ (xt, wt)). Notice that the vector ∇ (xt, wt) is a biased estimator of ∇ft(xt), because wt is sampled from the wrong distribution. Nonetheless, as pointed out in Drusvyatskiy and Xiao [15], the gradient bias is small for any ﬁxed time, decaying linearly with the distance to x¯t. Using this perspective, we show that all guarantees for PSG in the time-dependent setting naturally extend to this biased PSG algorithm for tracking equilibrium points, with essentially no loss in eﬃciency.
4

1.4 Related work
Our current work ﬁts within the broader literature on stochastic tracking, online optimization with controlled increments, high probability guarantees in stochastic optimization, and performative prediction. We now survey the most relevant literature in these areas.
Stochastic tracking. Stochastic optimization with time drift was considered soon after the Robbins-Monro approach for stochastic optimization was introduced; see Kushner and Yin [31] for a survey. Early results can be traced back to Dupač [16] in sequential estimation and Gaivoronskii [18] in stochastic optimization; see also Fujita and Fukao [17], Uosaki [49], Ruppert [42], Tsypkin and Nikolic [48], and Tsypkin and Polyak [47]. Stochastic algorithms have also been extensively studied as adaptive algorithms for stochastic tracking [6, 31, 48], for their ability to indeed track parameters under time drift. Most works have focused on the so-called least mean-squares (LMS) algorithm and its variants, which can be viewed as a stochastic gradient method on a least-squares loss-based objective. Other stochastic algorithms that have been studied in these settings with a larger cost per iteration include recursive least-squares and related Kalman ﬁltering algorithms [21]. Recent works have revisited these methods from a more modern viewpoint [7, 35, 51]. In particular, the paper Madden et al. [35] focuses on (accelerated) gradient methods for deterministic tracking problems, while Wilson et al. [51] analyzes a stochastic gradient method for online problems that is adaptive to unknown parameters. The paper Besbes et al. [7] analyzes the dynamic regret of stochastic algorithms for time-varying problems, focusing both on lower and upper complexity bounds. Though the proof techniques in our paper share many aspects with those available in the literature, the results we obtain are distinct. In particular, the guarantees (3) and (4) for the restarted PSG, along with their high probability variants, are new to the best of our knowledge.
Online optimization with controlled increments. Online learning under concept drift has been extensively studied [3, 27, 34]. One line of research works under the assumption that the data distribution is ﬁxed over time and the rate of variation is stated in terms of the probability of disagreement of consecutive target functions. The latter is assumed to be upper bounded and algorithms are compared on the basis of the dependence of their guarantee on that upper bound ∆. The algorithm of Long [34] scales in O(∆1/2) in the realizable setting. Another line of research works under the assumption of a time partitioning with an expert in each time interval. Then the goal is to compete with the expert in each segment. Closer to this work is Hazan [24] and Hazan and Seshadhri [26], where in the framework of online convex optimization the bounds are stated in terms of maximum regret over any contiguous time interval; see also Chiang et al. [12], Rakhlin and Sridharan [40], and Besbes et al. [7]. In contrast to these works, in our framework we state our bounds in the same spirit as in classical stochastic approximation, that is, in terms of distance to optimum and objective function gap, and we present results holding both in expectation and with high probability. Similar in spirit to these works, our bounds depend on a characteristic quantity of the problem diﬃculty encapsulating the variation rate and the noise level and hence delineate two regimes depending on the drift-to-noise ratio.
High probability guarantees in stochastic optimization. A large part of our work revolves around high probability guarantees for stochastic optimization. Classical references on the subject in static settings and for minimizing regret in online optimization include Lan [32], Rakhlin et al. [41], Bartlett et al. [4], and Hazan and Kale [25]. There exists a variety of techniques for establishing high probability guarantees based on Freedman’s inequality and doubling tricks; see, e.g., Bartlett et al. [4] and Hazan and Kale [25]. A more recent line of work Harvey et al. [23] establishes a generalized
5

Freedman inequality that is custom-tailored for analyzing stochastic gradient-type methods and results in best known high probability guarantees. Our arguments closely follow the paradigm of Harvey et al. [23] based on the generalized Freedman inequality.
Performative prediction and decision-dependent learning. Recent works on strategic classiﬁcation [5, 11, 14, 22] and performative prediction [36, 39] have highlighted the importance of strategic behavior in machine learning. That is, common learning systems exhibit a feedback mechanism, wherein the distribution generating the data in iteration t may depend on, or react to, the current “decision” of an algorithm xt. The recent paper Perdomo et al. [39] put forth an elegant framework for thinking about such problems, while Mendler-Dünner et al. [36] develops stochastic algorithms for this setting. The subsequent work Drusvyatskiy and Xiao [15] shows that a variety of stochastic algorithms for performative prediction can be understood as biased variants of the same algorithms on a certain static problem in equilibrium. Building on the techniques in Drusvyatskiy and Xiao [15], we show how all our results for time-dependent problems extend to problems that simultaneously depend on time and on the decision variable. We note that during the ﬁnal stage of completing this paper, the closely related and complementary work Wood et al. [52] was posted on arXiv.1 The paper Wood et al. [52] considers decision-dependent projected gradient descent under time drift in the distributional framework proposed in Perdomo et al. [39], establishing distance tracking bounds in expectation and with high probability under sub-Weibull gradient noise. In particular, the light tail assumptions used in Wood et al. [52] for obtaining high probability guarantees are signiﬁcantly weaker than those in our paper. On the other hand, we analyze both distance tracking and function gap tracking, allow presence of general convex regularizers, and propose a step-decay schedule for improved eﬃciency.
1.5 Outline
The outline of the paper is as follows. Section 2 formalizes the problem setting of time-dependent stochastic optimization and records the relevant assumptions. Sections 3–5 summarize the main results of the paper. Speciﬁcally, Section 3 focuses on eﬃciency estimates for tracking the minimizer, Section 4 focuses on eﬃciency estimates for tracking the minimal value, and Section 5 develops an extension to the decision-dependent setting via tracking equilibria. Section 6 presents the proofs of the main results in a uniﬁed framework. Illustrative numerical results appear in Section 7, and additional proofs appear in Appendix A.

2 Framework and assumptions

Throughout Sections 2–4, we consider the sequence of stochastic optimization problems

min ϕt(x) := ft(x) + rt(x)

(5)

x

indexed by time t ∈ N. We make the standard standing assumption that (i) each function ft : Rd → R is µ-strongly convex and C1-smooth with L-Lipschitz continuous gradient for some common parameters µ, L > 0, and (ii) each regularizer rt : Rd → R ∪ {∞} is proper, closed, and convex. The minimizer and minimal value of (5) will be denoted by xt and ϕt , respectively. Throughout, · denotes the 2-norm on Rd induced by the dot product ·, · .

1More precisely, a short version of our paper [13] was submitted to NeurIPS in May ’21, the paper Wood et al. [52] appeared on arXiv in July ’21, and our full paper was posted on arXiv in August ’21.

6

Algorithm 1 Online Proximal Stochastic Gradient Input: initial x0 and step sizes {ηt}Tt=0 ⊂ (0, ∞) Step t = 0, . . . , T − 1:
Set gt = ∇ft(xt)
Set xt+1 = proxηtrt (xt − ηtgt)
Return xT

PSG(x0, {ηt}, T )

As motivation, we describe two classical examples of (5) that are worth keeping in mind and that guide our framework: stochastic tracking of a drifting target and online learning under a distributional drift.
Example 2.1 (Stochastic tracking of a drifting target). The problem of stochastic tracking, related to the ﬁltering problem in signal processing, is to track a moving target xt from observations
bt = ct(xt ) + t,
where ct(·) is a known measurement map and t is a mean-zero noise vector. A typical time-dependent problem formulation takes the form
min E t(bt − ct(x)) + rt(x),
xt
where t(·) derives from the distribution of t and rt(·) encodes available side information about the target xt . Common choices for rt are the 1-norm and the squared 2-norm. The motion of the target xt is typically driven by a random walk or a diﬀusion [21, 43].
Example 2.2 (Online learning under distributional drift). The problem of online learning under a distributional drift is to learn while the data distribution may change over time. More formally, one problem formulation takes the form
min E (x, w) + r(x).
x w∼D(ut)
where D(ut) is a data distribution that depends on an unknown parameter sequence {ut}, which itself may evolve stochastically. The evolution of ut is often assumed to be piecewise constant in t in online learning [24, 44].
The main goal of a learning algorithm for problem (5) is to generate a sequence of points {xt} that minimize some natural performance metric. The most prevalent performance metrics in the literature are the tracking error and the dynamic regret. We will focus on two types of tracking error,
xt − xt 2 and ϕt(xt) − ϕt(xt ). We make the standing assumption that at every time t, and at every query point x, the learner
may obtain an unbiased estimator ∇ft(x) of the true gradient ∇ft(x) in order to proceed with a stochastic gradient-like optimization algorithm. With this oracle access, the online proximal stochastic gradient method—recorded as Algorithm 1 above—in each iteration t simply takes a stochastic gradient step on ft at xt followed by a proximal operation on rt:
xt+1 := proxηtrt (xt − ηtgt) = arug∈mRdin rt(u) + 21ηt u − (xt − ηtgt) 2 . The goal of our work is to obtain eﬃciency estimates for this procedure that hold both in expectation and with high probability.
The guarantees we obtain allow both the iterates xt and the minimizers xt to evolve stochastically. This is convenient for example when tracking a moving target xt whose motion may be governed by
7

a stochastic process such as a random walk or a diﬀusion (see Example 2.1). Throughout, we deﬁne the minimizer drift at time t to be the random variable
∆t := xt − xt+1 . Clearly, an eﬃciency estimate for Algorithm 1 must take into account the variation of the problems (5) in time t. Two of the most popular metrics for measuring such variations are the minimizer drift ∆t and the gradient variation supx ∇ft(x) − ∇ft+1(x) . The following elementary lemma shows that a bound on the gradient variation implies a bound on the minimizer drift, whenever the regularizers do not vary in time.
Lemma 2.3 (Gradient variation vs. minimizer drift). Suppose i, t ≥ 0 are such that the regularizers ri and rt are identical. Then we have
µ xi − xt ≤ ∇fi(xt ) − ∇ft(xt ) .
Proof. Let r denote the common regularizer: r = ri = rt. Then the ﬁrst-order optimality condition 0 ∈ ∂ϕt(xt ) = ∇ft(xt ) + ∂r(xt )
implies −∇ft(xt ) ∈ ∂r(xt ), so the vector v := ∇fi(xt ) − ∇ft(xt ) lies in ∂ϕi(xt ). Hence the µ-strong convexity of ϕi and the inclusion 0 ∈ ∂ϕi(xi ) imply µ xi − xt ≤ 0 − v .
Given {xt} and {gt} as in Algorithm 1, we let zt := ∇ft(xt) − gt
denote the gradient noise at time t and we impose the following assumption modeling stochasticity in the online problem throughout Sections 3 and 4.
Assumption 2.4 (Stochastic framework). There exists a ﬁltered probability space (Ω, F, F, P) with ﬁltration F = (Ft)t≥0 such that F0 = {∅, Ω} and the following hold for all t ≥ 0:
(i) xt, xt : Ω → Rd are Ft-measurable. (ii) zt : Ω → Rd is Ft+1-measurable with E[zt | Ft] = 0.
The ﬁrst item of Assumption 2.4 simply says that xt and xt are fully determined by information up to time t. The second item of Assumption 2.4 asserts that the gradient noise zt is fully determined by information up to time t + 1 and has zero mean conditioned on the information up to time t; for example, this holds naturally in Example 2.2 if we take gt = ∇ (xt, wt) with wt ∼ D(ut) provided the loss (·, wt) is C1-smooth.
3 Tracking the minimizer with the last iterate
This section presents bounds on the tracking error xt − xt 2 that are valid both in expectation and with high probability under light tail assumptions. Further, we show that a geometrically decaying learning rate schedule may be superior to a constant learning rate in terms of eﬃciency.
3.1 Bounds in expectation
We begin with bounding the expected value E xt − xt 2. Proofs appear in Section 6.1. The starting point for our analysis is the following standard one-step improvement guarantee.
8

Lemma 3.1 (One-step improvement). For all x ∈ Rd, the iterates {xt} produced by Algorithm 1 with ηt < 1/L satisfy the bound:
2ηt(ϕt(xt+1) − ϕt(x)) ≤ (1 − µηt) xt − x 2 − xt+1 − x 2 + 2ηt zt, xt − x + 1−ηLt2ηt zt 2.
For simplicity, we state the main results under the assumption that the second moments E ∆2t and E zt 2 are uniformly bounded; more general guarantees that take into account weighted averages of the moments and allow for time-dependent learning rates follow from Lemma 3.1 as well.

Assumption 3.2 (Bounded second moments). There exist constants ∆, σ > 0 such that the following hold for all t ≥ 0:

(i) (Drift) The minimizer drift ∆t satisﬁes E ∆2t ≤ ∆2. (ii) (Noise) The gradient noise zt satisﬁes E zt 2 ≤ σ2.

The following theorem establishes an expected improvement guarantee for Algorithm 1, and serves as the basis for much of what follows.

Theorem 3.3 (Expected distance). Suppose that Assumption 3.2 holds. Then the iterates produced by Algorithm 1 with constant learning rate η ≤ 1/2L satisfy the bound:

E xt − xt 2

(1 − µη)t x0 − x0
optimization

2 + ησ2 + µ
noise

∆ 2. µη
drift

Interplay of optimization, noise, and drift. Theorem 3.3 states that when using a constant learning rate, the error E xt − xt 2 decays linearly in time t, until it reaches the “noise+drift” error ησ2/µ + (∆/µη)2. Notice that the “noise+drift” error cannot be made arbitrarily small. This is

perfectly in line with intuition: a learning rate that is too small prevents the algorithm from catching

up with xt . We note that the individual error terms due to the optimization and noise are classically known to be tight for PSG; tightness of the drift term is proved in Madden et al. [35, Theorem 3.2].

With Theorem 3.3 in hand, we deﬁne the asymptotic tracking error of Algorithm 1 corresponding to E xt − xt 2, together with the corresponding optimal step size:

E := min
η∈(0,1/2L]

ησ2 + µ

∆2 µη

1 2∆2 1/3

and η := min 2L , µσ2

.

Plugging η into the deﬁnition of E, we see that Algorithm 1 exhibits qualitatively diﬀerent behaviors in settings with high or low drift-to-noise ratio ∆/σ, explicitly given by

  σ2 +

L∆ 2

if ∆ ≥

µ

E  µL

µ 2/3

σ

16L3

 ∆σ2  µ2

otherwise.

Two regimes of variation are brought to light by the above computation: the high drift-to-noise regime ∆/σ ≥ µ/16L3 and the low drift-to-noise regime ∆/σ < µ/16L3. The high drift-to-noise
regime is uninteresting from the viewpoint of stochastic optimization because the optimal learning
rate is as large as in the deterministic setting, η = 1/2L. In contrast, the low drift-to-noise regime is interesting because the optimal learning rate η = (2∆2/µσ2)1/3 is smaller than 1/2L and exhibits
a nontrivial scaling with the problem parameters.

9

Learning rate vs. rate of variation. A central question is to ﬁnd a learning rate schedule that achieves a tracking error E xt − xt 2 that is within a constant factor of E in the shortest possible time. The answer is clear in the high drift-to-noise regime ∆/σ ≥ µ/16L3. Indeed, in this case,
Theorem 3.3 directly implies that Algorithm 1 with the constant learning rate η = 1/2L will ﬁnd a point xt satisfying E xt − xt 2 E in time t (L/µ) log( x0 − x0 2/E). Notice that the eﬃciency estimate is logarithmic in 1/E; intuitively, the reason for the absence of a sublinear component is that the error due to the drift ∆ dominates the error due to the variance σ2 in the stochastic gradient.
The low drift-to-noise regime ∆/σ < µ/16L3 is more subtle. Namely, the simplest strategy is to execute Algorithm 1 with the constant learning rate η = (2∆2/µσ2)1/3. Then a direct application of Theorem 3.3 yields the estimate E xt − xt 2 E in time t (σ2/µ2E) log( x0 − x0 2/E). This eﬃciency estimate can be signiﬁcantly improved by gradually decaying the learning rate using a
“step-decay schedule”, wherein the algorithm is implemented in epochs with the new learning rate
chosen to be the midpoint between the current learning rate and η . Such schedules are well known
to improve eﬃciency in the static setting, as was discovered in Ghadimi and Lan [19, 20], and can
be used here. The end result is the following theorem; see Theorem 6.6 for the formal statement.

Theorem 3.4 (Time to track in expectation, informal). Suppose that Assumption 3.2 holds. Then there is a learning rate schedule {ηt} such that Algorithm 1 produces a point xt satisfying

E xt − xt 2

E after time t

L log µ

x0 − x0 2 E

σ2 + µ2E .

Remarkably, the eﬃciency estimate in Theorem 3.4 looks identical to the eﬃciency estimate in the classical static setting [32], with E playing the role of the target accuracy ε. Theorems 3.3
and 3.4 provide useful baseline guarantees for the performance of Algorithm 1. Nonetheless, these guarantees are all stated in terms of the expected tracking error E xt − xt 2, and are therefore only meaningful if the entire algorithm can be repeated from scratch multiple times. There is no shortage
of situations in which a learning algorithm is operating in real time and the time drift is irreversible;
in such settings, the algorithm may only be executed once. Such settings call for eﬃciency estimates
that hold with high probability, rather than only in expectation.

3.2 High probability guarantees
We next present high probability guarantees on the tracking error xt − xt 2. Proofs appear in Section 6.2. We make the following standard light tail assumptions on the minimizer drift and gradient noise [23, 32, 38].

Assumption 3.5 (Sub-Gaussian drift and noise). There exist constants ∆, σ > 0 such that the following hold for all t ≥ 0.

(i) (Drift) The drift ∆2t is sub-exponential conditioned on Ft with parameter ∆2:

E

exp(λ

∆

2 t

)

|

Ft

≤ exp(λ∆2)

for all

0 ≤ λ ≤ ∆−2.

(ii) (Noise) The noise zt is norm sub-Gaussian conditioned on Ft with parameter σ/2: P zt ≥ τ | Ft ≤ 2 exp(−2τ 2/σ2) for all τ > 0.

Note that the ﬁrst item of Assumption 3.5 is equivalent to asserting that the minimizer drift ∆t is sub-Gaussian conditioned on Ft. Clearly Assumption 3.5 implies Assumption 3.2 with the same constants ∆, σ. It is worthwhile to note some common settings in which Assumption 3.5 holds; the claims in Remark 3.6 follow from standard results on sub-Gaussian random variables [28, 50].

10

Remark 3.6 (Common settings for Assumption 3.5). Fix constants ∆, σ > 0. If ∆t is bounded by ∆, then clearly ∆2t is sub-exponential (conditioned on Ft) with parameter ∆2. Similarly, if zt is bounded by σ, then zt is norm sub-Gaussian (conditioned on Ft) with parameter σ/2 (by Markov’s inequality). Alternativ√ely, if the increment xt − xt+1 is mean-zero sub-Gaussian conditioned on Ft with para√meter ∆/ d, then xt − xt+1 is mean-zero norm sub-Gaussian conditioned on Ft with parameter 2 2 · ∆ and hence ∆2t is sub-exponential conditioned on Ft with parameter c · ∆2 for som√e absolute constant c > 0. Similarly, if zt is sub-Gaussian conditioned on Ft with parameter σ/4 2d, then zt is norm sub-Gaussian conditioned on Ft with parameter σ/2.
The following theorem shows that if Assumption 3.5 holds, then the expected bound on xt −xt 2 derived in Theorem 3.3 holds with high probability.

Theorem 3.7 (High probability distance tracking). Suppose that Assumption 3.5 holds and let {xt} be the iterates produced by Algorithm 1 with constant learning rate η ≤ 1/2L. Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the estimate
xt − xt 2 ≤ 1 − µ2η t x0 − x0 2 + c ηµσ2 + µ∆η 2 log δe
holds with probability at least 1 − δ.

The proof of Theorem 3.7 employs a technique used in Harvey et al. [23]. The idea is to build a careful recursion for the moment generating function of xt − xt 2, leading to a one-sided subexponential tail bound. As a consequence of Theorem 3.7, we can again implement a step-decay
schedule to obtain the following eﬃciency estimate with high probability; see Theorem 6.9 for the
formal statement.

Theorem 3.8 (Time to track with high probability, informal). Suppose that Assumption 3.5 holds

and that we are in the low drift-to-noise regime ∆/σ < µ/16L3. Then there is a learning rate

schedule {ηt} such that for any speciﬁed δ ∈ (0, 1), Algorithm 1 produces a point xt satisfying

xt − xt 2

E log e δ

with probability at least 1 − Kδ after time

t Lµ log x0 −E x0 2 + µσ22E , where K log2 L1 · σ∆22µ 1/3 .

4 Tracking the minimal value
The results outlined so far have focused on tracking the minimizer xt . In this section, we present results for tracking the minimal value ϕt . These two goals are fundamentally diﬀerent. Generally speaking, good bounds on the function gap along with strong convexity imply good bounds on the distance to the minimizer; the reverse implication is false. To this end, we require a stronger assumption on the variation of the functions ft in time t: rather than merely controlling the minimizer drift ∆t, we will assume control on the gradient drift
Gi,t := sup ∇fi(x) − ∇ft(x) .
x
Our strategy is to track the minimal value along the running average xˆt of the iterates xt produced by Algorithm 1, as deﬁned in Algorithm 2 below. The reason behind using this particular running average is brought to light in Section 6.3, wherein we apply a standard averaging technique

11

Algorithm 2 Averaged Online Proximal Stochastic Gradient

PSG(x0, µ, {ηt}, T )

Input: initial x0 =: xˆ0, strong convexity parameter µ, and step sizes {ηt}Tt=0 ⊂ (0, 2µ−1) Step t = 0, . . . , T − 1:

Set gt = ∇ft(xt)

Set xt+1 = proxηtrt (xt − ηtgt)

Set xˆt+1 = 1 − 2−µηµtηt xˆt + 2−µηµtηt xt+1

Return xˆT

(Lemma A.1) to a one-step improvement along xt (Lemma 6.10) to obtain the desired progress along xˆt (Proposition 6.11).

4.1 Bounds in expectation

We begin with bounding the expected value E[ϕt(xˆt) − ϕt ]. Proofs appear in Section 6.3. Analogous to Assumption 3.2, we make the following assumption regarding drift and noise.

Assumption 4.1 (Bounded second moments). The regularizers rt ≡ r are identical for all times t and there exist constants ∆, σ > 0 such that the following hold for all 0 ≤ i < t:

(i) (Drift) The gradient drift Gi,t satisﬁes E G2i,t ≤ (µ∆|i − t|)2.

(ii) (Noise) The gradient noise zi satisﬁes E zi 2 ≤ σ2 and E zi, xt = 0.

These two assumptions are natural indeed. Taking into account Lemma 2.3, it is clear that
Assumption 4.1 implies the earlier Assumption 3.2 with the same constants ∆, σ. The assumption
on the drift intuitively asserts that gradient drift Gi,t can grow only linearly in time |i − t| (in expectation). In particular, returning to Example 2.2, suppose that the distribution map D(·) is ε-Lipschitz continuous in the Wasserstein-1 distance, the loss (·, w) is C1 smooth for all w, and
the gradient ∇ (x, ·) is β-Lipschitz continuous for all x. Then the Kantorovich-Rubinste˘ın duality theorem [29] directly implies E G2i,t ≤ (εβ)2E ui − ut 2. Therefore, as long as the second moment E ui − ut 2 scales quadratically in |i − t|, the desired drift assumption holds. The assumption on the noise requires a uniform bound on the second moment E zi 2 and for the condition E zi, xt = 0 to hold. The latter property confers a weak form of uncorrelatedness between the gradient noise
zi and the future minimizer xt , and holds automatically if the gradient noise and the minimizers evolve independently of each other, as would typically be the case for instance in Example 2.2.
The following theorem establishes an expected improvement guarantee for Algorithm 2.

Theorem 4.2 (Expected function gap). Suppose that Assumption 4.1 holds, and let {xˆt} be the iterates produced by Algorithm 2 with constant learning rate η ≤ 1/2L. Then the following bound holds for all t ≥ 0:

E ϕt(xˆt) − ϕt

1

−

µη 2

t

ϕ0(x0) − ϕ0

optimization

+ ησ2 + ∆2 . µη2
noise drift

The “noise+drift” error term in Theorem 4.2 coincides with µ times the error term in Theorem 3.3,

as expected. With Theorem 4.2 in hand, we are led to deﬁne the following asymptotic tracking error of Algorithm 2 corresponding to E[ϕt(xˆt) − ϕt ]:

G := µE = min ησ2 + ∆2 .

η∈(0,1/2L]

µη2

12

The corresponding asymptotically optimal choice of η is again given by

1 2∆2 1/3

η = min 2L , µσ2

,

and the dichotomy governed by the drift-to-noise ratio ∆/σ remains:

 σ2 + (L∆)2 if ∆ ≥

µ
3

G L

µ

σ

2/3

16L

µ ∆µσ22

otherwise.

In the high drift-to-noise regime ∆/σ ≥ µ/16L3, Theorem 4.2 directly implies that Algorithm 2
with the constant learning rate η = 1/2L ﬁnds a point xˆt satisfying E[ϕt(xˆt) − ϕt ] G in time t (L/µ) log((ϕ0(x0) − ϕ0)/G). In the low drift-to-noise regime ∆/σ < µ/16L3, another direct application of Theorem 4.2 shows that Algorithm 2 with the constant learning rate η = (2∆2/µσ2)1/3 ﬁnds a point xˆt satisfying E[ϕt(xˆt) − ϕt ] G in time t (σ2/µG) log((ϕ0(x0) − ϕ0)/G). As before, this eﬃciency estimate can be signiﬁcantly improved by implementing a step-decay schedule. The end result is the following theorem; see Theorem 6.13
for the formal statement.

Theorem 4.3 (Time to track in expectation, informal). Suppose that Assumption 4.1 holds. Then there is a learning rate schedule {ηt} such that Algorithm 2 produces a point xˆt satisfying

E ϕt(xˆt) − ϕt

G after time t

L log µ

ϕ0(x0) − ϕ0 G

σ2 + µG .

4.2 High probability guarantees
Next, we obtain high probability analogues of Theorems 4.2 and 4.3. Proofs appear in Section 6.4. Naturally, such results should rely on light tail assumptions on the gradient drift Gi,t and the norm of the gradient noise zi . We state the guarantee under the assumption that Gi,t and zi are conditionally sub-Gaussian (Assumption 4.4). In particular, we require for the ﬁrst time that the gradient noise zi is mean-zero conditioned on the σ-algebra
Fi,t := σ(Fi, xt ) for all 0 ≤ i < t; the property E[zi | Fi,t] = 0 would follow from independence of the gradient noise zi and the future minimizer xt and is very reasonable in light of Examples 2.1 and 2.2.
Assumption 4.4 (Sub-Gaussian drift and noise). The regularizers rt ≡ r are identical for all times t and there exist constants ∆, σ > 0 such that the following hold for all 0 ≤ i < t.
(i) (Drift) The square gradient drift G2i,t is sub-exponential with parameter (µ∆|i − t|)2: E exp λG2i,t ≤ exp λ(µ∆|i − t|)2 for all 0 ≤ λ ≤ (µ∆|i − t|)−2.
(ii) (Noise) The gradient noise zi is mean-zero norm sub-Gaussian conditioned on Fi,t with parameter σ/2, i.e., E[zi | Fi,t] = 0 and P zi ≥ τ | Fi,t ≤ 2 exp(−2τ 2/σ2) for all τ ≥ 0.
Clearly the chain of implications holds:
Assumption 4.4 =⇒ Assumption 4.1 =⇒ Assumption 3.2.
The following theorem shows that if Assumption 4.4 holds, then the expected bound on ϕt(xˆt)−ϕt derived in Theorem 4.2 holds with high probability.

13

Theorem 4.5 (Function gap with high probability). Suppose that Assumption 4.4 holds, and let {xˆt} be the iterates produced by Algorithm 2 with constant learning rate η ≤ 1/2L. Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the estimate
ϕt(xˆt) − ϕt ≤ c 1 − µ2η t ϕ0(x0) − ϕ0 + ησ2 + µ∆η22 log δe
holds with probability at least 1 − δ.

The proof of Theorem 4.5 is based on combining the generalized Freedman inequality of Harvey

et al. [23] with careful control on the drift and noise in improvement guarantees for the proximal

stochastic gradient method. The key observation is that although we do not have simple recursive

control on the moment generating function of ϕt(xˆt) − ϕt (as we do with xt − xt 2), we can control

the tracking error ϕt(xˆt) − ϕt by leveraging control on the martingale

t−1 i=0

zi, xi − xt

ζt−1−i, where

ζ = 1 − µη/(2 − µη). This martingale is self-regulating in the sense that its total conditional variance

is bounded by the history of the process; the generalized Freedman inequality is precisely suited to

bound such martingales with high probability.

With Theorem 4.5 in hand, we may implement a step-decay schedule as before to obtain the

following eﬃciency estimate; see Theorem 6.19 for the formal statement.

Theorem 4.6 (Time to track with high probability, informal). Suppose that Assumption 4.4 holds

and that we are in the low drift-to-noise regime ∆/σ < µ/16L3. Fix δ ∈ (0, 1). Then there is a

learning rate schedule {ηt} such that Algorithm 2 produces a point xˆt satisfying

ϕt(xˆt) − ϕt

G log e δ

with probability at least 1 − Kδ after time

t Lµ log ϕ0(x0G) − ϕ0 + µσG2 log log δe , where K log2 L1 · σ∆22µ 1/3 .

5 Extension to the decision-dependent setting

In this section, we extend the framework and results of the previous sections to a much wider class of tracking problems. In particular, the material in this section is a strict generalization of all the results in the previous sections and more generally can model the performative prediction framework of Perdomo et al. [39] in a temporal setting.
Setting the stage, suppose we have a family of functions {ft,x(·)} indexed by time t ∈ N and points x ∈ Rd. Under reasonable assumptions, this family yields a sequence of equilibrium points x¯t satisfying
x¯t ∈ arg min ft,x¯t(u) + rt(u)
u
(see Assumption 5.2 and Lemma 5.3). When the functions ft,x are independent of x, the equilibrium points are simply the minimizers of ft + rt—the content of the previous sections. In this more general setting, our goal is to track the equilibrium points x¯t, or equivalently to track the minimizers of the time-varying optimization problem

min ft,x¯t (u) + rt(u).

(6)

u

Formally, (6) is an example of (5), but this viewpoint is not directly useful since x¯t is unknown. This more general framework allows us to model more dynamic settings. The main example stems from the setting of performative prediction introduced in Perdomo et al. [39]. This will be a running example throughout the section, and we will revisit it often.

14

Example 5.1 (Performative prediction). Within the framework of performative prediction, the functions take the form ft,x(u) = Ew∼D(t,x) (u, w) for some family of distributions D(t, x) indexed by both time t and the decision x. The motivation for the dependence of the distribution on x is that often deployment of a learning rule parametrized by x causes the population to change their proﬁle to increase the likelihood of a better personal outcome—a process called “gaming”. In other words, the population data is a function of the decision taken by the learner. Moreover, the dependence of the population data on time appears naturally when the population evolves due to exogenous temporal eﬀects (e.g. seasonal, economic). The equilibrium points x¯t have a clear meaning in this context. Namely, x¯t is an equilibrium point if the learner has no reason deviate from the learning rule x¯t based on the response distribution D(t, x¯t) alone.
Whenever we refer back to this example, we will impose the following assumptions that are direct extensions of Perdomo et al. [39] to the temporal setting. Namely, ﬁx a nonempty metric space M equipped with its Borel σ-algebra and let P1(M ) denote the space of Radon probability measures on M with ﬁnite ﬁrst moment, equipped with the Wasserstein-1 distance W1. Naturally, we assume that there are constants θ, ε ≥ 0 such that the distribution map D(·, ·) satisﬁes the Lipschitz condition:
W1 D(i, x), D(t, y) ≤ θ|i − t| + ε x − y for all (i, x), (t, y) ∈ N × Rd. Moreover, we suppose that the loss function : Rd × M → R has the following three properties: (u, ·) ∈ L1(π) for all u ∈ Rd and π ∈ P1(M ); (·, w) is C1-smooth for all w ∈ M ; and there is a constant β ≥ 0 such that the map w → ∇ (u, w) is β-Lipschitz continuous for all u ∈ Rd, where ∇ (u, w) denotes the gradient of (·, w) evaluated at u. These assumptions directly imply the following stability property of the gradients with respect to distributional perturbations [15, Lemma 2.1]:
sup ∇fi,x(u) − ∇ft,y(u) ≤ θβ|i − t| + εβ x − y for all (i, x), (t, y) ∈ N × Rd. (7)
u∈Rd
As we will see, upon supposing that there exists a strong convexity parameter µ > εβ such that each expected loss ft,x(·) is µ-strongly convex, the control on the gradient drift furnished by (7) will place us in the setting to apply the forthcoming results.
5.1 Decision-dependent framework
We begin by recording the assumptions of our framework. Similar to the previous sections, we make the standing assumption that (i) each function ft,x : Rd → R is µ-strongly convex and C1-smooth with L-Lipschitz continuous gradient for some common parameters µ, L > 0, and (ii) each regularizer rt : Rd → R ∪ {∞} is proper, closed, and convex. For each t ∈ N and x, u ∈ Rd, we let ∇ft,x(u) denote the gradient of the function ft,x(·) evaluated at u. In order to control the variation of the family {ft,x(·)} in the decision variable x, we impose the following assumption on the gradient drift in x throughout Section 5.
Assumption 5.2 (Gradient drift in the decision variable). There is a parameter γ ∈ [0, µ) such that the following bound holds for all t ∈ N and x, y ∈ Rd:
sup ∇ft,x(u) − ∇ft,y(u) ≤ γ x − y .
u∈Rd
Returning to Example 5.1, it follows from (7) that Assumption 5.2 holds with γ = εβ. As the following lemma shows, the requirement γ < µ guarantees that for each t ∈ N, the equilibrium point x¯t is well deﬁned and unique.
15

Algorithm 3 Decision-Dependent PSG Input: initial x0 and step sizes {ηt}Tt=0 ⊂ (0, ∞) Step t = 0, . . . , T − 1:
Set gt = ∇ft,xt (xt) Set xt+1 = proxηtrt (xt − ηtgt)
Return xT

D-PSG(x0, {ηt}, T )

Lemma 5.3 (Existence of equilibrium point). For each t ∈ N, the map St : x → arg min{ft,x(u) + rt(u)}
u
is (γ/µ)-contractive. Thus, there exists a unique equilibrium point x¯t for each time t.
Proof. Fix t ∈ N, and note ﬁrst that St is well deﬁned by the strong convexity of each function ϕt,x := ft,x + rt. Next, given x, y ∈ Rd, observe that we have the ﬁrst order optimality conditions 0 ∈ ∂ϕt,x(St(x)) and 0 ∈ ∂ϕt,y(St(y)); this last inclusion implies −∇ft,y(St(y)) ∈ ∂rt(St(y)) and hence ∇ft,x(St(y)) − ∇ft,y(St(y)) ∈ ∂ϕt,x(St(y)). On the other hand, the µ-strong convexity of ϕt,x implies that for all u, u ∈ dom ϕt,x, we have
µ u − u ≤ w − w for all w ∈ ∂ϕt,x(u) and w ∈ ∂ϕt,x(u ).
Thus, taking u = St(x), w = 0, u = St(y), and w = ∇ft,x(St(y)) − ∇ft,y(St(y)) yields
µ St(x) − St(y) ≤ ∇ft,x(St(y)) − ∇ft,y(St(y)) ≤ γ x − y ,
where the last inequality holds by Assumption 5.2. Hence St(x) − St(y) ≤ (γ/µ) x − y , that is, St is (γ/µ)-contractive. Therefore St has a unique ﬁxed point x¯t by the Banach ﬁxed-point theorem.
We will track the equilibria x¯t using a decision-dependent proximal stochastic gradient method. Speciﬁcally, we make the standing assumption that at every time t, and at every query point x, the learner may obtain an unbiased estimator ∇ft,x(x) of the true gradient ∇ft,x(x). With this oracle access, the decision-dependent proximal stochastic gradient method—recorded as Algorithm 3 above—in each iteration t simply takes a stochastic gradient step on ft,xt at xt followed by a proximal operation on rt:
xt+1 := proxηtrt (xt − ηtgt) = arug∈mRdin rt(u) + 21ηt u − (xt − ηtgt) 2 . As before, our goal is to obtain eﬃciency estimates for this procedure that hold both in expectation and with high probability.
The guarantees we obtain allow both the iterates xt and the equilibria x¯t to evolve stochastically. Setting the stage, given {xt} and {gt} as in Algorithm 3 we let
zt := ∇ft,xt (xt) − gt
denote the gradient noise at time t and we impose the following assumption modeling stochasticity throughout Section 5.
Assumption 5.4 (Stochastic framework). There exists a ﬁltered probability space (Ω, F, F, P) with ﬁltration F = (Ft)t≥0 such that F0 = {∅, Ω} and the following hold for all t ≥ 0:
(i) xt, x¯t : Ω → Rd are Ft-measurable.
(ii) zt : Ω → Rd is Ft+1-measurable with E[zt | Ft] = 0.
16

The ﬁrst item of Assumption 5.4 simply says that xt and x¯t are fully determined by information up to time t. The second item of Assumption 5.4 asserts that the gradient noise zt is fully determined by information up to time t + 1 and has zero mean conditioned on the information up to time t; for example, this holds naturally in Example 5.1 if we take gt = ∇ (xt, wt) with wt ∼ D(t, xt).
Finally, we ﬁx some notation to be used henceforth. We deﬁne the positive parameter

µ¯ := µ − γ,

and we deﬁne the equilibrium drift ∆¯ t and the temporal gradient drift G¯i,t to be the random variables

∆¯ t := x¯t − x¯t+1

and G¯i,t := sup ∇fi,x(u) − ∇ft,x(u) .
u,x∈Rd

Note that in the setting of Example 5.1, the estimate (7) implies G¯i,t ≤ θβ|i−t| and hence ∆¯ t ≤ θβ/µ¯ by Lemma 2.3, provided that rt is independent of time t. We also set

ϕt := ft,xt + rt, xt := arg min ϕt, ϕt := min ϕt

and
ψt := ft,x¯t + rt and ψt := min ψt. In particular, the equilibrium point x¯t is the minimizer of the equilibrium function ψt, and ψt denotes its minimal value. Observe that when γ = 0, we have ϕt = ψt + c for some constant of integration c, hence we recover the setting of Section 2 with xt = x¯t.

5.2 Tracking the equilibrium point

In a nutshell, the results of Section 3 extend directly to tracking the equilibrium points x¯t, with µ replaced by µ¯ and ∆ replaced by ∆¯ (deﬁned below). We begin with bounding the expected value E xt − x¯t 2. Due to the fact that Algorithm 3 takes steps on the current functions ϕt but the minimizers we aim to track are those of the equilibrium functions ψt, we will rely at the outset on
controlling the function gaps

fi,x(u) − fi,x(v) − ft,y(u) − ft,y(v)

(at ﬁrst for i = t, and later for general i and t). We achieve this control in terms of the gradient drift.

Lemma 5.5 (Function gap variation). For all (i, x), (t, y) ∈ N × Rd and u, v ∈ Rd, we have fi,x(u) − fi,x(v) − ft,y(u) − ft,y(v) ≤ G¯i,t + γ x − y u − v .

Proof. Fix (i, x), (t, y) ∈ N × Rd and u, v ∈ Rd, and set uτ := v + τ (u − v) for all τ ∈ [0, 1]. By the fundamental theorem of calculus and Cauchy-Schwarz, we have

1

fi,x(u) − fi,x(v) − ft,y(u) − ft,y(v) =

∇fi,x(uτ ) − ∇ft,y(uτ ), u − v dτ

0

≤ G¯i,t + γ x − y u − v .

Switching (i, x) and (t, y) completes the proof.

Using Lemmas 3.1 and 5.5, we obtain the following equilibrium one-step improvement.

Lemma 5.6 (Equilibrium one-step improvement). The iterates {xt} produced by Algorithm 3 with ηt < 1/L satisfy the bound:
2ηt(ψt(xt+1) − ψt ) ≤ (1 − µ¯ηt) xt − x¯t 2 − (1 − γηt) xt+1 − x¯t 2 + 2ηt zt, xt − x¯t + 1−ηLt2ηt zt 2.

17

Proof. By Lemma 5.5, we have

ψt(xt+1) − ψt(x¯t) − ϕt(xt+1) − ϕt(x¯t)

= ft,x¯t (xt+1) − ft,x¯t (x¯t) − ft,xt (xt+1) − ft,xt (x¯t)

≤ γ xt − x¯t xt+1 − x¯t .

Hence

ψt(xt+1) − ψt ≤ ϕt(xt+1) − ϕt(x¯t) + γ xt − x¯t xt+1 − x¯t . Moreover, Young’s inequality implies

γ xt − x¯t

xt+1 − x¯t

≤

γ 2

xt − x¯t

2

+

γ 2

xt+1 − x¯t

2.

Multiplying through by 2ηt and applying Lemma 3.1 completes the proof.

For simplicity, we state the main results under the assumption that the second moments E ∆¯ 2t and E zt 2 are uniformly bounded; more general guarantees that take into account weighted averages of the moments and allow for time-dependent learning rates follow from Lemma 5.6 as well.
Assumption 5.7 (Bounded second moments). There exist constants ∆¯ , σ > 0 such that the following hold for all t ≥ 0:
(i) (Drift) The equilibrium drift ∆¯ t satisﬁes E ∆¯ 2t ≤ ∆¯ 2.

(ii) (Noise) The gradient noise zt satisﬁes E zt 2 ≤ σ2.

The following theorem establishes an expected improvement guarantee for Algorithm 3, thereby extending Theorem 3.3; see Section 6.1 for the precise statement (Corollary 6.5) and proof.

Theorem 5.8 (Expected distance). Suppose that Assumption 5.7 holds. Then the iterates produced by Algorithm 3 with constant learning rate η ≤ 1/2L satisfy the bound:

E xt − x¯t 2

(1 − µ¯η)t x0 − x¯0 2 + ησ2 + µ¯

optimization

noise

∆¯ 2 . µ¯η
drift

With Theorem 5.8 in hand, we are led to deﬁne the following asymptotic tracking error of Algorithm 3 corresponding to E xt − x¯t 2, together with the corresponding optimal step size:

E¯ := min
η∈(0,1/2L]

ησ2 + µ¯

∆¯ 2 µ¯η

1 2∆¯ 2 1/3

and η¯ := min 2L , µ¯σ2

.

Plugging η¯ into the deﬁnition of E¯, we see that Algorithm 3 exhibits qualitatively diﬀerent behaviors in settings corresponding to high or low drift-to-noise ratio ∆¯ /σ:

  σ2 +

L∆¯ 2

if ∆¯ ≥

µ¯

E¯  µ¯L

µ¯ 2/3

σ

16L3

 ∆¯ σ2  µ¯2

otherwise.

As before, the high drift-to-noise regime ∆¯ /σ ≥ µ¯/16L3 is uninteresting from the viewpoint of stochastic optimization and we focus on the low drift-to-noise regime ∆¯ /σ < µ¯/16L3. The following theorem extends Theorem 3.4; see Theorem 6.6 for the formal statement and proof.

Theorem 5.9 (Time to track in expectation, informal). Suppose that Assumption 5.7 holds. Then there is a learning rate schedule {ηt} such that Algorithm 3 produces a point xt satisfying

E xt − x¯t 2

E¯ after time t

L log µ¯

x0 − x¯0 2 E¯

σ2 + µ¯2E¯.

18

Next, we present high probability guarantees on the tracking error xt − x¯t 2 under the following standard light tail assumption on the equilibrium drift and gradient noise.

Assumption 5.10 (Sub-Gaussian drift and noise). There exist constants ∆¯ , σ > 0 such that the following hold for all t ≥ 0.

(i) (Drift) The drift ∆¯ 2t is sub-exponential conditioned on Ft with parameter ∆¯ 2:

E

exp(λ

∆¯

2 t

)

|

Ft

≤ exp(λ∆¯ 2)

for all

0 ≤ λ ≤ ∆¯ −2.

(ii) (Noise) The noise zt is norm sub-Gaussian conditioned on Ft with parameter σ/2: P zt ≥ τ | Ft ≤ 2 exp(−2τ 2/σ2) for all τ > 0.

Note that the ﬁrst item of Assumption 5.10 is equivalent to asserting that the equilibrium drift ∆¯ t is sub-Gaussian conditioned on Ft, and that this condition holds trivially in the setting of Example 5.1 with ∆¯ = θβ/µ¯ provided the regularizers rt ≡ r are identical for all times t. Clearly Assumption 5.10 implies Assumption 5.7 with the same constants ∆¯ , σ. The theorem below shows that if Assumption 5.10 holds, then the expected bound on xt − x¯t 2 derived in Theorem 5.8 holds
with high probability.

Theorem 5.11 (High probability distance tracking). Suppose that Assumption 5.10 holds and let {xt} be the iterates produced by Algorithm 3 with constant learning rate η ≤ 1/2L. Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the estimate
xt − x¯t 2 ≤ 1 − µ¯2η t x0 − x¯0 2 + c ηµσ¯2 + µ¯∆¯η 2 log δe (8)
holds with probability at least 1 − δ.

Theorem 5.11 is an extension of Theorem 3.7. As a consequence of Theorem 5.11, we can again implement a step-decay schedule in the low drift-to-noise regime to obtain the following eﬃciency estimate with high probability, thereby extending Theorem 3.8; see Section 6.2 for the precise statements (Theorems 6.8 and 6.9) and proofs.

Theorem 5.12 (Time to track with high probability, informal). Suppose that Assumption 5.10 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µ¯/16L3. Then there is a learning

rate schedule {ηt} such that for any speciﬁed δ ∈ (0, 1), Algorithm 3 produces a point xt satisfying

xt − x¯t 2

E¯ log e δ

with probability at least 1 − Kδ after time

L

x0 − x¯0 2

σ2

1 σ2µ¯ 1/3

t µ¯ log

E¯

+ µ¯2E¯, where K log2 L · ∆¯ 2

.

5.3 Tracking the equilibrium value

The results outlined so far have focused on tracking the equilibrium point x¯t, i.e., the minimizer of ψt. In this section, we present results for tracking the equilibrium value ψt in the parameter regime

µ > 2γ.

(9)

Imposing the regime (9), we deﬁne the positive parameter

µˆ := µ − 2γ.

19

Algorithm 4 Averaged Decision-Dependent PSG

D-PSG(x0, µ, γ, {ηt}, T )

Input: initial x0 =: xˆ0, strong convexity parameter µ, gradient drift parameter γ ∈ (0, µ/2), and step sizes {ηt}Tt=0 ⊂ (0, 2µ−1); set µˆ := µ − 2γ Step t = 0, . . . , T − 1:
Set gt = ∇ft,xt (xt)

Set xt+1 = proxηtrt (xt − ηtgt)

Set xˆt+1 = 1 − 2−µˆηµtηt xˆt + 2−µˆηµtηt xt+1

Return xˆT

We will track ψt along the running average xˆt of the iterates xt produced by Algorithm 3, as deﬁned in Algorithm 4 above. In a nutshell, the results of Sections 4 extend directly to tracking the equilibrium values ψt , with µ replaced by µˆ and ∆ replaced by ∆¯ (as deﬁned below).
We begin with bounding the expected value E[ψt(xˆt) − ψt ]. This requires a weak form of uncorrelatedness between the gradient noise zi and the future equilibrium point x¯t, which we
stipulate in the following analogue of Assumption 4.1.

Assumption 5.13 (Bounded second moments). The regularizers rt ≡ r are identical for all times t and there exist constants ∆¯ , σ > 0 such that the following hold for all 0 ≤ i < t:
(i) (Drift) The temporal gradient drift G¯i,t satisﬁes E G¯2i,t ≤ (µˆ∆¯ |i − t|)2.

(ii) (Noise) The gradient noise zi satisﬁes E zi 2 ≤ σ2 and E zi, x¯t = 0.

Taking into account Lemma 2.3, it is clear that Assumption 5.13 implies the earlier Assumption 5.7 with the same constants ∆¯ , σ. Further, the condition on the drift holds trivially in the setting of Example 5.1 with ∆¯ = θβ/µˆ provided µ > 2εβ. The following theorem presents an expected improvement guarantee for Algorithm 4, thereby extending Theorem 4.2; see Corollary 6.12 for the precise statement and proof.

Theorem 5.14 (Expected function gap). Suppose that Assumption 5.13 holds, and let {xˆt} be the

iterates produced by Algorithm 4 with constant learning rate η ≤ 1/2L. Then the following bound

holds for all t ≥ 0:

E ψt(xˆt) − ψt

1

−

µˆη 2

t

ψ0(x0) − ψ0

optimization

+ ησ2 + ∆¯ 2 . µˆη2
noise drift

With Theorem 5.14 in hand, we are led to deﬁne the following asymptotic tracking error of Algorithm 4 corresponding to E[ψt(xˆt) − ψt ], together with the corresponding optimal step size:

G := min
η∈(0,1/2L]

ησ2 + ∆¯ 2 µˆη2

1 2∆¯ 2 1/3

and ηˆ := min 2L , µˆσ2

.

A familiar dichotomy governed by the drift-to-noise ratio ∆¯ /σ arises. We again focus on the low driftto-noise regime ∆¯ /σ < µˆ/16L3. The following theorem extends Theorem 4.3; see Theorem 6.13 for the formal statement.

Theorem 5.15 (Time to track in expectation, informal). Suppose that Assumption 5.13 holds. Then there is a learning rate schedule {ηt} such that Algorithm 4 produces a point xˆt satisfying

E[ψt(xˆt) − ψt ]

G after time t

L log µˆ

ψ0(x0) − ψ0 G

+ σ2 . µˆG

20

Next, we obtain high probability analogues of Theorems 5.14 and 5.15. Naturally, such results should rely on light tail assumptions on the temporal gradient drift G¯i,t and the norm of the gradient noise zi . We state the guarantee under the assumption that G¯i,t and zi are conditionally sub-Gaussian (Assumption 5.16). In particular, we require for the ﬁrst time that the gradient noise zi is mean-zero conditioned on the σ-algebra
Fi,t := σ(Fi, x¯t)
for all 0 ≤ i < t; the property E[zi | Fi,t] = 0 would follow from independence of the gradient noise zi and the future equilibrium point x¯t.

Assumption 5.16 (Sub-Gaussian drift and noise). The regularizers rt ≡ r are identical for all times t and there exist constants ∆¯ , σ > 0 such that the following hold for all 0 ≤ i < t.
(i) (Drift) The drift G¯2i,t is sub-exponential with parameter (µˆ∆¯ |i − t|)2: E exp λG¯2i,t ≤ exp λ(µˆ∆¯ |i − t|)2 for all 0 ≤ λ ≤ (µˆ∆¯ |i − t|)−2.

(ii) (Noise) The noise zi is mean-zero norm sub-Gaussian conditioned on Fi,t with parameter σ/2, i.e., E[zi | Fi,t] = 0 and
P zi ≥ τ | Fi,t ≤ 2 exp(−2τ 2/σ2) for all τ ≥ 0.

Clearly the chain of implications

Assumption 5.16 =⇒ Assumption 5.13 =⇒ Assumption 5.7

holds, and the condition on the drift in Assumption 5.16 holds trivially in the setting of Example 5.1 with ∆¯ = θβ/µˆ provided µ > 2εβ. The following theorem shows that if Assumption 5.16 holds, then
the expected bound on ψt(xˆt) − ψt derived in Theorem 5.14 holds with high probability, thereby extending Theorem 4.5.

Theorem 5.17 (Function gap with high probability). Suppose that Assumption 5.16 holds, and let {xˆt} be the iterates produced by Algorithm 4 with constant learning rate η ≤ 1/2L. Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the estimate
ψt(xˆt) − ψt ≤ c 1 − µˆ2η t ψ0(x0) − ψ0 + ησ2 + µˆ∆¯η22 log δe (10)
holds with probability at least 1 − δ.

With Theorem 5.17 in hand, we may implement a step-decay schedule as before to obtain the following eﬃciency estimate, thereby extending Theorem 4.6; see Section 6.4 for the precise statements (Theorems 6.17 and 6.19) and proofs.

Theorem 5.18 (Time to track with high probability, informal). Suppose that Assumption 5.16 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µˆ/16L3. Fix δ ∈ (0, 1). Then there

is a learning rate schedule {ηt} such that Algorithm 4 produces a point xˆt satisfying

ψt(xˆt) − ψt

G log e δ

with probability at least 1 − Kδ after time

t Lµˆ log ψ0(x0G) − ψ0 + µˆσG2 log log δe , where K log2 L1 · σ∆¯22µˆ 1/3 .

21

6 Proofs of main results
Roadmap. In this section, we derive the results of the preceding sections under the uniﬁed framework presented in Section 5.1; we impose the assumptions and notation of Section 5.1 henceforth. Sections 6.1 and 6.2 handle distance tracking in expectation and with high probability, respectively; this corresponds to the results presented in Section 5.2 (entailing those of Sections 3.1 and 3.2). Then Sections 6.3 and 6.4 handle function gap tracking in expectation and with high probability, respectively; this corresponds to the results presented in Section 5.3 (entailing those of Sections 4.1 and 4.2).

6.1 Tracking the equilibrium point: bounds in expectation

The proof of Theorem 5.8 follows a familiar pattern in stochastic optimization. We begin by recalling Lemma 3.1, which gives a standard one-step improvement guarantee [32] for the proximal stochastic gradient method on the ﬁxed problem min ϕt.

Lemma 6.1 (One-step improvement). For all x ∈ Rd, the iterates {xt} produced by Algorithm 3 with ηt < 1/L satisfy the bound:
2ηt(ϕt(xt+1) − ϕt(x)) ≤ (1 − µηt) xt − x 2 − xt+1 − x 2 + 2ηt zt, xt − x + 1−ηLt2ηt zt 2.

Proof. Since ft := ft,xt is L-smooth, we have

ϕt(xt+1) = ft(xt+1) + rt(xt+1)

≤ ft(xt) +

∇ft(xt), xt+1 − xt

+

L 2

xt+1 − xt

2 + rt(xt+1)

= ft(xt) + rt(xt+1) +

gt, xt+1 − xt

+

L 2

xt+1 − xt

2+

zt, xt+1 − xt .

Next, given any δt > 0, Young’s inequality yields

zt, xt+1 − xt ≤ δ2t zt 2 + 21δt xt+1 − xt 2.

Therefore, given any x ∈ Rd, we have

ϕt(xt+1) ≤ ft(xt) + rt(xt+1) +

gt, xt+1 − xt

+ δt−12+L

xt+1 − xt

2

+

δt 2

zt

2

= ft(xt) + rt(xt+1) + gt, xt+1 − xt + 21ηt xt+1 − xt 2

+ δt−1+L2−ηt−1

xt+1 − xt

2

+

δt 2

zt

2

≤ ft(xt) + rt(x) + gt, x − xt + 21ηt x − xt 2 − 21ηt x − xt+1 2

+ δt−1+L2−ηt−1

xt+1 − xt

2

+

δt 2

zt

2,

where the last inequality holds because xt+1 = proxηtrt(xt − ηtgt) is the minimizer of the ηt−1-strongly convex function rt + gt, · − xt + 21ηt · −xt 2. Now we estimate

ft(xt) + rt(x) + gt, x − xt = ft(xt) + ∇ft(xt), x − xt + rt(x) + zt, xt − x

≤

ft(x)

−

µ 2

x − xt

2 + rt(x) +

zt, xt − x

=

ϕt(x)

−

µ 2

x − xt

2+

zt, xt − x

using the µ-strong convexity of ft. Thus,

ϕt(xt+1) ≤ ϕt(x) − µ2 x − xt 2 + zt, xt − x + 21ηt x − xt 2 − 21ηt x − xt+1 2

+ δt−1+L2−ηt−1

xt+1 − xt

2

+

δt 2

zt

2.

22

Finally, taking δt = ηt/(1 − Lηt) and rearranging (note that ϕt(xt+1) is ﬁnite) yields 2ηt(ϕt(xt+1) − ϕt(x)) ≤ (1 − µηt) xt − x 2 − xt+1 − x 2 + 2ηt zt, xt − x + 1−ηLt2ηt zt 2,
as claimed.

It is critically important that the one-step improvement estimate in Lemma 6.1 holds with respect to any reference point x. In particular, as we already showed in Section 5.2, taking x = x¯t and applying Lemma 5.5 yields Lemma 5.6:

Lemma 6.2 (Equilibrium one-step improvement). The iterates {xt} produced by Algorithm 3 with ηt < 1/L satisfy the bound:
2ηt(ψt(xt+1) − ψt ) ≤ (1 − µ¯ηt) xt − x¯t 2 − (1 − γηt) xt+1 − x¯t 2 + 2ηt zt, xt − x¯t + 1−ηLt2ηt zt 2.
With Lemma 6.2 in hand, we obtain the following recursion on xt − x¯t 2.

Lemma 6.3 (Distance recursion). The iterates {xt} produced by Algorithm 3 with ηt < 1/L satisfy the bound:
xt+1 − x¯t+1 2 ≤ (1 − µ¯ηt) xt − x¯t 2 + 2ηt zt, xt − x¯t + 1−ηLt2ηt zt 2 + 1 + µ¯1ηt ∆¯ 2t .

Proof.

Note

ﬁrst

that

the

µ-strong

convexity

of

ψt

implies

µ 2

xt+1 − x¯t

2 ≤ ψt(xt+1)−ψt .

Combining

this estimate with Lemma 6.2 yields

(1 + µ¯ηt) xt+1 − x¯t 2 ≤ (1 − µ¯ηt) xt − x¯t 2 + 2ηt zt, xt − x¯t + 1−ηLt2ηt zt 2. Next, an application of Young’s inequality reveals

xt+1 − x¯t+1 2 ≤ (1 + µ¯ηt) xt+1 − x¯t 2 + 1 + (µ¯ηt)−1 x¯t − x¯t+1 2,

thereby completing the proof.

Applying Lemma 6.3 recursively furnishes a bound on xt − x¯t 2. When the step size is constant, the next proposition follows immediately.

Proposition 6.4 (Last-iterate progress). The iterates {xt} produced by Algorithm 3 with constant step size η < 1/L satisfy the bound:

xt − x¯t

t−1
2 ≤ (1 − µ¯η)t x0 − x¯0 2 + 2η zi, xi − x¯i (1 − µ¯η)t−1−i

i=0

t−1

+

η2 1−Lη

i=0

zi 2(1 − µ¯η)t−1−i +

1

+

1 µ¯η

t−1
∆¯ 2i (1 − µ¯η)t−1−i.
i=0

By taking expectations in Proposition 6.4, we obtain the following precise version of Theorem 5.8.

Corollary 6.5 (Expected distance). Suppose that Assumption 5.7 holds. Then the iterates {xt} generated by Algorithm 3 with constant learning rate η ≤ 1/2L satisfy the bound:

E xt − x¯t 2 ≤ (1 − µ¯η)t x0 − x¯0 2 + 2 ησ2 +

∆¯ 2 .

µ¯

µ¯η

With Corollary 6.5 in hand, we can now prove an expected eﬃciency estimate for the online proximal stochastic gradient method using a step-decay schedule, wherein the algorithm is implemented

23

in epochs with the new learning rate chosen to be the midpoint between the current learning rate and the asymptotically optimal learning rate η¯ . The following is the formal statement of Theorem 5.9 (note that in the high drift-to-noise regime ∆¯ /σ ≥ µ¯/16L3, Theorem 5.9 holds trivially with the constant learning rate η¯ = 1/2L). The argument is close in spirit to the justiﬁcations of the restart schemes in Ghadimi and Lan [19, 20].

Theorem 6.6 (Time to track in expectation). Suppose that Assumption 5.7 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µ¯/16L3. Set η¯ = (2∆¯ 2/µ¯σ2)1/3 and E¯ = (∆¯ σ2/µ¯2)2/3. Suppose moreover that we have available a positive upper bound on the initial square distance D ≥ x0 − x¯0 2. Consider running Algorithm 3 in k = 0, . . . , K − 1 epochs, namely, set X0 = x0 and iterate the process
Xk+1 = D-PSG(Xk, ηk, Tk) for k = 0, . . . , K − 1,
where the number of epochs is
1 σ2µ¯ 1/3 K = 1 + log2 L · ∆¯ 2

and we set 1
η0 = 2L ,

T0 =

2L log µ¯

µ¯LD + σ2

and ηk = ηk−1 + η¯ , Tk = log(4)

2

µ¯ηk

∀k ≥ 1.

Then the time horizon T = T0 + · · · + TK−1 satisﬁes

L

µ¯LD + σ2 L

T µ¯ log σ2 + µ¯2E¯ ≤ µ¯ log

while the corresponding tracking error satisﬁes E XK − X¯K 2 of ψT .

D + σ2 E¯ + µ¯2E¯, E¯, where X¯K denotes the minimizer

Proof. For each index k, let tk := T0 + · · · + Tk−1 (with t0 := 0), X¯k be the minimizer of the corresponding equilibrium function ψtk , and
E¯k := µ2¯ ηkσ2 + µ¯∆¯η¯22 .

Then taking into account ηk ≥ η¯ , Corollary 6.5 directly implies

E Xk+1 − X¯k+1 2 ≤ (1 − µ¯ηk)Tk E Xk − X¯k 2 + µ2¯

ηkσ2 + µ¯∆¯η22
k

≤ e−µ¯ηkTk E Xk − X¯k 2 + E¯k.

We will verify by induction that the estimate E Xk+1 − X¯k+1 2 ≤ 2E¯k holds for all indices k. To see the base case, observe

E X1 − X¯1 2 ≤ e−µ¯η0T0 X0 − X¯0 2 + E0 ≤ 2E0.

Now assume that the claim holds for index k − 1. We then conclude

E Xk+1 − X¯k+1 2 ≤ e−µ¯ηkTk E Xk − X¯k 2 + E¯k

≤ 41 E Xk − X¯k 2 + E¯k

≤

E¯k ¯

E Xk − X¯k 2 + E¯k ≤ 2E¯k,

2Ek−1

thereby completing the induction. Hence E XK − X¯K 2 ≤ 2E¯K−1.

24

Next, observe

E¯K−1

−

√ 3 54

so

Finally, note

and

∆¯ σ2 2/3 2σ2

2σ2 η0 − η¯

µ¯2

= µ¯ (ηK−1 − η¯ ) = µ¯ · 2K−1 ≤

E XK − X¯K 2 ≤ 2(1 + √3 54) ∆¯µ¯σ22 2/3 E¯.

L

µ¯LD + 1 K−1 1

T µ¯ log σ2 + µ¯ ηk

k=1

K −1

1

K −1
≤ 2L 2k ≤ 2L · 2K = 8L · 2K−2 ≤ 8

σ2µ¯

1/3 = 8σ2 ·

ηk

∆¯ 2

µ¯

k=1

k=1

This completes the proof.

∆¯ σ2 µ¯2
∆¯ σ2 µ¯2

2/3
= E¯,
−2/3 σ2 µ¯E¯.

6.2 Tracking the equilibrium point: high probability guarantees

The proof strategy of Theorem 5.11 follows a similar argument as in Harvey et al. [23, Claim D.1], which recursively controls the moment generating function of xt − x¯t 2. Namely, Lemma 6.3 in the regime ηt ≤ 1/2L directly yields
xt+1 − x¯t+1 2 ≤ (1 − µ¯ηt) xt − x¯t 2 + 2ηt zt, ut xt − x¯t + 2ηt2 zt 2 + µ¯2ηt ∆¯ 2t , (11)
where we set ut := xxtt−−xx¯¯tt if xt is distinct from x¯t and set it to zero otherwise. The right-hand side of (11) has the form of a contraction factor, gradient noise, and drift. The goal is now to control the moment generating function E eλ xt−x¯t 2 through this recursion. The basic probabilistic tool for similar settings under bounded noise assumptions was developed in Harvey et al. [23]. The following proposition is a slight generalization of Harvey et al. [23, Claim D.1] to a light tail setting.

Proposition 6.7 (Recursive control on MGF). Consider scalar stochastic processes (Vt), (Dt), and (Xt) on a probability space with ﬁltration (Ht), which are linked by the inequality
Vt+1 ≤ αtVt + Dt Vt + Xt + κt for some deterministic constants αt ∈ (−∞, 1] and κt ∈ R. Assume that the following properties hold.

• Vt is nonnegative and Ht-measurable.

• Dt is mean-zero sub-Gaussian conditioned on Ht with deterministic parameter σt: E[exp(λDt) | Ht] ≤ exp(λ2σt2/2) for all λ ∈ R.

• Xt is nonnegative and sub-exponential conditioned on Ht with deterministic parameter νt: E[exp(λXt) | Ht] ≤ exp(λνt) for all 0 ≤ λ ≤ 1/νt.

Then the estimate E[exp(λVt+1)] ≤ exp λ(νt + κt) E exp λ 1 +2 αt Vt

holds for any λ satisfying 0 ≤ λ ≤ min

1−αt 2σ2

,

1 2νt

.

t

25

Proof. For any index t and any scalar λ ≥ 0, the tower rule implies E[exp(λVt+1)] ≤ E exp λ αtVt + Dt Vt + Xt + κt = exp(λκt)E exp(λαtVt)E exp λDt Vt exp(λXt) | Ht .
Hölder’s inequality in turn yields

E exp λDt Vt exp(λXt) | Ht] ≤ E exp 2λ VtDt | Ht] · E[exp(2λXt) | Ht]

≤

exp(2λ2

Vt

σ

2 t

)

exp(2λνt

)

= exp(λ2σt2Vt) exp(λνt)

provided 0 ≤ λ ≤ 21νt . Thus, if 0 ≤ λ ≤ min

1−αt 2σ2

,

1 2νt

, then the following estimate holds:

t

E[exp(λVt+1)] ≤ exp(λκt)E exp(λαtVt) exp(λ2σt2Vt) exp(λνt) = exp λ(νt + κt) E exp(λ(αt + λσt2)Vt)

≤ exp λ(νt + κt) E exp λ 1 +2 αt Vt .

The proof is complete.

We may now use Proposition 6.7 to derive the following precise version of Theorem 5.11.

Theorem 6.8 (High probability distance tracking). Suppose that Assumption 5.10 holds and let
{xt} be the iterates produced by Algorithm 3 with constant learning rate η ≤ 1/2L. Then there exists an absolute constant2 c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the estimate

xt − x¯t 2 ≤

1

−

µ¯η 2

t x0 − x¯0 2 +

8η(cσ)2 + 4 ∆¯ 2

µ¯

µ¯η

log e δ

holds with probability at least 1 − δ.

Proof. Note ﬁrst that under Assumption 5.10, there exists an absolute constant c ≥ 1 such that zt 2 is sub-exponential conditioned on Ft with parameter cσ2 and zt is mean-zero sub-Gaussian conditioned

on Ft with parameter cσ for all t. Therefore zt, ut is mean-zero sub-Gaussian conditioned on Ft with parameter cσ, while ∆¯ 2t is sub-exponential conditioned on Ft with parameter ∆¯ 2 by

assumption. Thus, in light of inequality (11), we may apply Proposition 6.7 with Ht = Ft, Vt = xt − x¯t 2, Dt = 2ηt zt, ut , Xt = 2ηt2 zt 2 + 2∆¯ 2t /µ¯ηt, αt = 1 − µ¯ηt, κt = 0, σt = 2ηtcσ, and νt = 2ηt2cσ2 + 2∆¯ 2/µ¯ηt, yielding the estimate

E exp λ xt+1 − x¯t+1 2 ≤ exp λ 2η2cσ2 + 2∆¯ 2 E exp λ 1 − µ¯ηt xt − x¯t 2

(12)

t

µ¯ηt

2

for all

0 ≤ λ ≤ min

µ¯ ,

1

.

8ηt(cσ)2 4ηt2cσ2 + 4∆¯ 2/µ¯ηt

2Explicitly, one can take any c ≥ 1 such that zt 2 is sub-exponential conditioned on Ft with parameter cσ2 and zt is mean-zero sub-Gaussian conditioned on Ft with parameter cσ for all t.

26

Taking into account ηt ≡ η and iterating the recursion (12), we deduce

E exp λ xt − x¯t 2

≤ exp λ 1 − µ¯2η t x0 − x¯0 2 + λ 2η2cσ2 + 2µ¯∆¯η2

t−1

1

−

µ¯η 2

i

i=0

≤ exp λ 1 − µ¯2η t x0 − x¯0 2 + 4ηµ¯cσ2 + 4 µ¯∆¯η 2

for all

0 ≤ λ ≤ min 8η(µ¯cσ)2 , 4η2cσ2 +1 4∆¯ 2/µ¯η .

Moreover, setting

ν := 8η(cσ)2 + 4 ∆¯ 2

µ¯

µ¯η

and taking into account c ≥ 1 and µ¯η ≤ 1, we have

4ηcσ2 + 4 ∆¯ 2 ≤ ν

µ¯

µ¯η

and Hence

ν1 = 8η(cσ)2 +µ¯4∆¯ 2/µ¯η2 ≤ min 8η(µ¯cσ)2 , 4η2cσ2 +1 4∆¯ 2/µ¯η .

E exp λ

xt − x¯t

2−

1

−

µ¯η 2

t

x0 − x¯0

2

≤ exp(λν) for all 0 ≤ λ ≤ 1/ν.

Taking λ = 1/ν and applying Markov’s inequality completes the proof.

With Theorem 6.8 in hand, we can now prove a high probability eﬃciency estimate for Algorithm 3 using a step-decay schedule. The following theorem is the precise form of Theorem 5.12. The argument follows the same reasoning as in the proof of Theorem 6.6, with Theorem 6.8 playing the role of Corollary 6.5 while using a union bound over the epochs. The proof appears in the appendix (see Section A.1).

Theorem 6.9 (Time to track with high probability). Suppose that Assumption 5.10 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µ¯/16L3. Set η¯ = (2∆¯ 2/µ¯σ2)1/3 and E¯ = (∆¯ σ2/µ¯2)2/3. Suppose moreover that we have available an upper bound on the initial square distance D ≥ x0 −x¯0 2. Consider running Algorithm 3 in k = 0, . . . , K − 1 epochs, namely, set X0 = x0 and iterate the process
Xk+1 = D-PSG(Xk, ηk, Tk) for k = 0, . . . , K − 1,
where the number of epochs is
1 σ2µ¯ 1/3 K = 1 + log2 L · ∆¯ 2

and we set 1
η0 = 2L ,

T0 =

4L log µ¯

µ¯LD + σ2

and ηk = ηk−1 + η¯ , Tk = 2 log(4)

2

µ¯ηk

∀k ≥ 1.

Then the time horizon T = T0 + · · · + TK−1 satisﬁes

L

µ¯LD + σ2 L

D + σ2

T µ¯ log σ2 + µ¯2E¯ ≤ µ¯ log E¯ + µ¯2E¯,

27

and for any speciﬁed δ ∈ (0, 1), the corresponding tracking error satisﬁes

XK − X¯K 2

E¯ log e δ

with probability at least 1 − Kδ, where X¯K denotes the minimizer of ψT .

6.3 Tracking the equilibrium value: bounds in expectation

We turn now to tracking the equilibrium value. To begin, we require a more ﬂexible version of Lemma 6.2 which holds in the static regularizer setting rt ≡ r.

Lemma 6.10 (Equilibrium one-step improvement). The iterates {xt} produced by Algorithm 3 with rt ≡ r and ηt < 1/L satisfy the following bound for all indices i, t ∈ N and arbitrary α > 0:
2ηi ψt(xi+1) − ψt ≤ (1 − µ¯ηi) xi − x¯t 2 − 1 − (γ + α)ηi xi+1 − x¯t 2
+ 2ηi zi, xi − x¯t + 1−ηLi2ηi zi 2 + ηαi G¯2i,t.

Proof. Taking into account rt ≡ r and applying Lemma 5.5, we have

ψt(xi+1) − ψt(x¯t) − ϕi(xi+1) − ϕi(x¯t)

= ft,x¯t (xi+1) − ft,x¯t (x¯t) − fi,xi (xt+1) − fi,xi (x¯t) ≤ G¯i,t + γ xi − x¯t xi+1 − x¯t .

Hence ψt(xi+1) − ψt ≤ ϕi(xi+1) − ϕi(x¯t) + G¯i,t + γ xi − x¯t
Moreover, Young’s inequality implies

xi+1 − x¯t .

G¯i,t + γ xi − x¯t

xi+1 − x¯t

≤

γ 2

xi − x¯t

2

+

γ+α 2

xi+1 − x¯t

2 + 21α G¯2i,t.

Multiplying through by 2ηi and applying Lemma 6.1 completes the proof.

Turning the estimate in Lemma 6.10 into an eﬃciency guarantee on the average iterate is essentially standard and follows for example from the averaging techniques in Drusvyatskiy and Xiao [15], Ghadimi and Lan [19, 20], and Kulunchakov and Mairal [30]. The resulting progress along the average iterate is summarized in the following proposition, while the description of the key averaging lemma is placed in the appendix (see Section A.2); henceforth, we impose the regime (9): µ > 2γ.

Proposition 6.11 (Progress along the average iterate). The iterates {xˆt} produced by Algorithm 4 with rt ≡ r and constant step size η ≤ 1/2L satisfy the bound

ψt(xˆt) − ψt

t−1

≤ (1 − ρˆ)t

ψt(x0) − ψt

+

µˆ 4

x0 − x¯t

2

+ ρˆ

zi, xi − x¯t

i=0

t−1

t−1

+ ρˆη

zi

2(1

−

ρˆ)t−1−i

+

ρˆ µˆ

G¯2i,t(1 − ρˆ)t−1−i,

i=0

i=0

(1 − ρˆ)t−1−i

where ρˆ := µˆη/(2 − µη).

Proof. Setting α = µˆ/2 in Lemma 6.10, we obtain the following recursion for all indices k ≥ 0 and t ≥ 1:
ρ ψk(xt) − ψk ≤ (1 − c1ρ)Vt−1 − (1 + c2ρ)Vt + ωt, where ρ = 2η, c1 = µ¯/2, c2 = −µ/4, Vi = xi − x¯k 2, and ωt = 2η zt−1, xt−1 − x¯k + 2η2 zt−1 2 + (2η/µˆ)G¯2t−1,k. The result follows by applying the averaging Lemma A.1 with h = ψt − ψt .

28

Taking expectations in Proposition 6.11, we obtain the following precise version of Theorem 5.14.

Corollary 6.12 (Expected function gap). Suppose that Assumption 5.13 holds, let {xˆt} be the iterates produced by Algorithm 4 with constant step size η ≤ 1/2L, and set ρˆ := µˆη/(2 − µη). Then the following bound holds for all t ≥ 0:
E ψt(xˆt) − ψt ≤ (1 − ρˆ)t · E ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 + ησ2 + 8µˆ∆¯η22 . (13)

Consequently, we have E ψt(xˆt) − ψt

(1 − ρˆ)t ψ0(x0) − ψ0 + ησ2 + µˆ∆¯η22

for all t ≥ 0, and the following asymptotic error bound holds:

lim sup E ψt(xˆt) − ψ ≤ ησ2 + 8∆¯ 2 .

t→∞

t

µˆη2

Proof. The bound (13) follows by taking expectations in Proposition 6.11 and noting

t−1
E

zi

2(1 − ρˆ)t−1−i ≤

σ2

and

t−1
E G¯2

(1 − ρˆ)t−1−i ≤ (µˆ∆¯ )2(2 − ρˆ)

ρˆ

i,t

ρˆ3

i=0

i=0

by Assumption 5.13. Next, applying Lemmas 5.5 and 2.3 and applying Young’s inequality together

with the µ-strong convexity of ψ0 and yields

ψt(x0) − ψt

+

µˆ 4

x0 − x¯t 2 ≤ 3 ψ0(x0) − ψ0

+ 5G¯20,t/µ¯,

(14)

and then taking expectations and invoking Assumption 5.13 gives

E

ψt(x0) − ψt

+

µˆ 4

x0 − x¯t 2

≤ 3 ψ0(x0) − ψ0

+ 5µˆ∆¯ 2t2.

(15)

Further, the inequality

e−µˆηt/2µˆt2 ≤ 16/µˆη2 ∀µˆ, η, t > 0

(16)

combines with inequality (15) to yield

(1 − ρˆ)t

·E

ψt(x0) − ψt

+

µˆ 4

x0 − x¯t 2

≤ 3(1 − ρˆ)t ψ0(x0) − ψ0

80∆¯ 2 + µˆη2

and the remaining assertions of the corollary follow.

We may now apply Corollary 6.12 to obtain the formal version of Theorem 5.15; the proof closely follows that of Theorem 6.6 and is included in the appendix (see Section A.3).
Theorem 6.13 (Time to track in expectation). Suppose that Assumption 5.13 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µˆ/16L3. Set ηˆ = (2∆¯ 2/µˆσ2)1/3 and G = µˆ(∆¯ σ2/µˆ2)2/3. Suppose moreover that we have available a positive upper bound on the initial gap D ≥ ψ0(x0) − ψ0. Consider running Algorithm 4 in k = 0, . . . , K − 1 epochs, namely, set X0 = x0 and iterate the process
Xk+1 = D-PSG(Xk, µ, γ, ηk, Tk) for k = 0, . . . , K − 1, where the number of epochs is
1 σ2µˆ 1/3 K = 1 + log2 L · ∆¯ 2

and we set 1
η0 = 2L ,

T0 =

4L log µˆ

LD + σ2

and ηk = ηk−1 + ηˆ , Tk = 2 log(12)

2

µˆηk

∀k ≥ 1.

29

Then the time horizon T = T0 + · · · + TK−1 satisﬁes

L

LD + σ2 L

D

T µˆ log σ2 + µˆG ≤ µˆ log G

and the corresponding tracking error satisﬁes E ψT (XK) − ψT

+ + σ2 µˆG
G.

6.4 Tracking the equilibrium value: high probability guarantees

In this section, we derive the high probability analogues of the results in Section 6.3. In light of Proposition 6.11, we seek upper bounds on the sums

t−1
zi, xi − x¯t (1 − ρˆ)t−1−i,
i=0

t−1
zi 2(1 − ρˆ)t−1−i,
i=0

t−1
G¯2i,t(1 − ρˆ)t−1−i
i=0

that hold with high probability. The last two sums can easily be estimated under boundedness or light tail assumptions on zi and G¯i,t. Controlling the ﬁrst sum is more challenging because the error xi − x¯t may in principle grow large. In order to control this term, we will use a remarkable
generalization of Freedman’s inequality, recently proved in Harvey et al. [23] for the purpose of
analyzing the stochastic gradient method on static nonsmooth problems (without a regularizer). The main idea is as follows. Fix a horizon t, assume E[zi | Fi,t] = 0 for all 0 ≤ i < t (recall that
Fi,t := σ(Fi, x¯t)), and deﬁne the martingale diﬀerence sequence

di := zi, xi − x¯t (1 − ρˆ)t−1−i

adapted to the ﬁltration (Fi+1,t)ti=−01. Roughly speaking, under mild light tail assumptions, the

total conditional variance of the corresponding martingale

t−1 i=0

di

can

be

bounded

above

with

high probability by an aﬃne transformation of itself, i.e., by an aﬃne combination of the sequence

{di}ti=−01. In this way, the martingale is self-regulating. This is the content of the following proposition.

The proof follows from Lemma 6.10 and algebraic manipulation and is placed in the appendix (see

Section A.4).

Proposition 6.14 (Self-regulation). The iterates {xt} produced by Algorithm 3 with rt ≡ r and constant step size η ≤ 1/2L satisfy the following bound for all λ ∈ (0, µ¯η]:





t−1

t−2

t−1

xi − x¯t 2(1 − λ)2(t−1−i) ≤ 2η

(1 − λ)t−2−i zj, xj − x¯t (1 − λ)t−1−j

i=0

j=0

i=j+1

t−2

+ λ1 (1 − λ)t−1 x0 − x¯t 2 + 2λη2

zj 2(1 − λ)t−2−j

j=0

t−2

+

η µ¯λ

G¯2j,t(1 − λ)t−2−j .

j=0

In order to bound the self-regulating martingale

t−1 i=0

di,

we

will

use

the

generalized

Freedman

inequality developed in Harvey et al. [23], or rather a direct consequence thereof.

Theorem 6.15 (Consequence of generalized Freedman). Let (Di)ni=0 and (Vi)ni=0 be scalar stochastic processes on a probability space with ﬁltration (Hi)ni=+01 satisfying
E[exp(λDi) | Hi] ≤ exp(λ2Vi/2) for all λ ≥ 0.
Suppose that Di is Hi+1-measurable with E|Di| < ∞ and E[Di | Hi] = 0, and that Vi is nonnegative and Hi-measurable. Suppose moreover that there are constants α0, . . . , αn ≥ 0, δ ∈ [0, 1], and

30

β(δ) ≥ 0 satisfying

n

n

P

Vi ≤ αiDi + β(δ) ≥ 1 − δ.

i=0

i=0

Set α := max{α0, . . . , αn}. Then for all τ > 0, the following bound holds:

n

P

Di ≥ τ ≤ δ + exp −

τ

.

i=0 4α + 8β(δ)/τ

Combining Proposition 6.14 and Theorem 6.15 yields the following tail bound for

t−1 i=0

di.

Proposition 6.16 (Noise martingale tail bound). Suppose that Assumption 5.16 holds, let {xt} be
the iterates produced by Algorithm 3 with constant step size η ≤ 1/2L, and set ρˆ := µˆη/(2 − µη). Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N, δ ∈ (0, 1), and τ > 0, the following bound holds:

t−1

P

zi, xi − x¯t (1 − ρˆ)t−1−i ≥ τ ≤ δ + exp −

τ

,

i=0 4α + 8βt log(3e/δ)/τ

where α := 3η(cσ)2/ρˆ and

βt := (1 − ρˆ)t−1

x0 − x¯0 2 + ∆¯ 2t2 2(cσ)2 + 2η2(cσ)4 + 3µˆ∆¯ 2η(cσ)2 .

ρˆ

ρˆ2

ρˆ4

Proof. By Assumption 5.16, there exists an absolute constant c ≥ 1 such that zi 2 is sub-exponential conditioned on Fi,t with parameter cσ2 and zi is mean-zero sub-Gaussian conditioned on Fi,t with
parameter cσ for all indices 0 ≤ i < t. Then for each 0 ≤ i < t, the Fi+1,t-measurable random variable zi, xi − x¯t is mean-zero sub-Gaussian conditioned on Fi,t with parameter cσ xi − x¯t , so

E exp λ zi, xi − x¯t (1 − ρˆ)t−1−i | Fi,t ≤ exp λ2(cσ)2 xi − x¯t 2(1 − ρˆ)2(t−1−i)/2 ∀λ ∈ R;
note also that E| zi, xi − x¯t | < ∞ by Hölder’s inequality, Assumption 5.7, and Corollary 6.5. Now ﬁx t ≥ 1 and observe that Proposition 6.14 yields the total conditional variance bound

t−1

t−2

(cσ)2 xi − x¯t 2(1 − ρˆ)2(t−1−i) ≤ αj zj, xj − x¯t (1 − ρˆ)t−1−j + Rt,

i=0

j=0

where 0 ≤ αj ≤ α for all 0 ≤ j ≤ t − 2 and

t−2

t−2

Rt := (cσρˆ)2 (1 − ρˆ)t−1 x0 − x¯t 2 + 2η2(ρˆcσ)2

zj 2(1 − ρˆ)t−2−j + η(µc¯σρˆ)2 G¯2j,t(1 − ρˆ)t−2−j .

j=0

j=0

We claim that

P Rt ≤ βt log 3δe ≥ 1 − δ ∀δ ∈ (0, 1). (17)

To verify (17), observe ﬁrst that for all n ≥ 0, the sum

n i=0

zi

2(1 − ρˆ)n−i is sub-exponential with

parameter

n i=0

cσ2(1

−

ρˆ)n−i

≤

(cσ)2/ρˆ,

so

Markov’s

inequality

implies

P n zi 2(1 − ρˆ)n−i ≤ (cσ)2 log e ≥ 1 − δ ∀δ ∈ (0, 1).

(18)

i=0 ρˆ δ

Further, for all 0 ≤ n < t, it follows from Assumption 5.16 and Lemma 2.3 that x0 − x¯t 2 is

sub-exponential with parameter 2 x0 − x¯0 2 + ∆¯ 2t2 and

n i=0

G¯2i,t(1

−

ρˆ)n−i

is

sub-exponential

31

with parameter

n

n

(µˆ∆¯ )2(t − i)2(1 − ρˆ)n−i = (µˆ∆¯ )2(1 − ρˆ)n+1−t (t − i)2(1 − ρˆ)t−i−1 ≤

2(µˆ∆¯ )2 ,

i=0 i=0 ρˆ3(1 − ρˆ)t−1−n

so Markov’s inequality implies

P x0 − x¯t 2 ≤ 2 x0 − x¯0 2 + ∆¯ 2t2 log δe ≥ 1 − δ ∀δ ∈ (0, 1) (19)

and

n

P

G¯2 (1 − ρˆ)n−i ≤

2(µˆ∆¯ )2

log e ≥ 1 − δ

∀δ ∈ (0, 1).

(20)

i,t

ρˆ3(1 − ρˆ)t−1−n

δ

i=0

Thus, (18)–(20) and a union bound yield (17). Consequently, Theorem 6.15 implies that the following

bound holds for all δ ∈ (0, 1) and τ > 0:

t−1

P

zi, xi − x¯t (1 − ρˆ)t−1−i ≥ τ ≤ δ + exp −

τ

,

i=0 4α + 8βt log(3e/δ)/τ

as claimed.

We may now deduce the following precise version of Theorem 5.17 using the tail bound furnished by Proposition 6.16.

Theorem 6.17 (Function gap with high probability). Suppose that Assumption 5.16 holds, let {xˆt} be the iterates produced by Algorithm 4 with constant step size η ≤ 1/2L, and set ρˆ := µˆη/(2 − µη). Then there is an absolute constant c > 0 such that for any speciﬁed t ∈ N and δ ∈ (0, 1), the following estimate holds with probability at least 1 − δ:
ψt(xˆt) − ψt ≤ (1 − ρˆ)t ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 + η(cσ)2 + 8µˆ∆¯η22 + 9ρˆ 8βt log 4δe ,

where

βt := (1 − ρˆ)t−1

x0 − x¯0 2 + ∆¯ 2t2 2(cσ)2 + 2η2(cσ)4 + 3µˆ∆¯ 2η(cσ)2 .

ρˆ

ρˆ2

ρˆ4

Proof. A quick computation shows that given any δ ∈ (0, 1), we may take

√

3e

τ = (2 + 5) 8βt log δ

in Proposition 6.16 to obtain

P

t−1

√

zi, xi − x¯t (1 − ρˆ)t−1−i < (2 + 5)

8βt log

3e

≥ 1 − 2δ.

(21)

i=0 δ

We may now combine (18), (20), and (21) together with Proposition 6.11 and a union bound to conclude that for all δ ∈ (0, 1), the estimate

ψt(xˆt) − ψt ≤ (1 − ρˆ)t ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 + η(cσ)2 + 2µˆρˆ∆2¯ 2 log δe

√

3e

+ (2 + 5)ρˆ 8βt log δ

holds with probability at least 1 − 4δ; hence

ψt(xˆt) − ψt ≤ (1 − ρˆ)t ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 + η(cσ)2 + 8µˆ∆¯η22 + 9ρˆ 8βt log δe .

32

with probability at least 1 − 4δ.

Remark 6.18. To see that Theorem 6.17 entails Theorem 5.17, observe that upon setting C :=

max{c, 1} and selecting any t ∈ N, we have

ρˆ 8βt ≤ 4C2

(1 − ρˆ)t

x0 − x¯0

2 + ∆¯ 2t2

µˆησ2

+

ησ2

+

√ 6

∆¯ σ √

,

µˆη

while the AM-GM inequality implies

2 (1 − ρˆ)t x0 − x¯0 2 + ∆¯ 2t2 µˆησ2 ≤ (1 − ρˆ)t µˆ x0 − x¯0 2 + µˆ∆¯ 2t2 + ησ2,

inequality (16) implies

(1 − ρˆ)t µˆ x0 − x¯0 2 + µˆ∆¯ 2t2 ≤ 2(1 − ρˆ)t ψ0(x0) − ψ0 + 1µˆ6η∆¯22 ,

and Young’s inequality implies

2∆¯ σ √

≤ ησ2 +

∆¯ 2

.

µˆη

µˆη2

Hence

ρˆ 8βt (1 − ρˆ)t ψ0(x0) − ψ0 + ησ2 + µˆ∆¯η22 .

Further, inequalities (14) and (16) together with Assumption 5.16 imply that for all t ∈ N and

δ ∈ (0, 1), the estimate

(1 − ρˆ)t ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 ≤ 3(1 − ρˆ)t ψ0(x0) − ψ0 + 8µˆ0η∆¯22 log δe

holds with probability at least 1 − δ. On the other hand, given the assumptions of Theorem 6.17, for all t ∈ N and δ ∈ (0, 1) the estimate
ψt(xˆt) − ψt ≤ (1 − ρˆ)t ψt(x0) − ψt + µ4ˆ x0 − x¯t 2 + η(cσ)2 + 8µˆ∆¯η22 + 9ρˆ 8βt log 4δe

holds with probability at least 1 − δ. Thus, for all t ∈ N and δ ∈ (0, 1), a union bound reveals that

the estimate

ψt(xˆt) − ψt (1 − ρˆ)t ψ0(x0) − ψ0 + ησ2 + µˆ∆¯η22 log δe

holds with probability at least 1 − δ.

We may now apply Theorem 5.17 to obtain the formal version of Theorem 5.18; the proof is analogous to that of Theorem 6.9 and appears in the appendix (see Section A.5).

Theorem 6.19 (Time to track with high probability). Suppose that Assumption 5.16 holds and that we are in the low drift-to-noise regime ∆¯ /σ < µˆ/16L3. Set ηˆ = (2∆¯ 2/µˆσ2)1/3 and G = µˆ(∆¯ σ2/µˆ2)2/3. Suppose moreover that we have available a positive upper bound on the initial gap D ≥ ψ0(x0) − ψ0. Fix δ ∈ (0, 1) and consider running Algorithm 4 in k = 0, . . . , K − 1 epochs, namely, set X0 = x0 and iterate the process
Xk+1 = D-PSG(Xk, µ, γ, ηk, Tk) for k = 0, . . . , K − 1,
where the number of epochs is
1 σ2µˆ 1/3 K = 1 + log2 L · ∆¯ 2

33

and we set 1
η0 = 2L ,

T0 =

4L log µˆ

LD + σ2

ηk−1 + ηˆ

2 log 4c log(e/δ) +

and ηk = 2 , Tk =

µˆηk

for all k ≥ 1, where c > 0 is the absolute constant furnished by the bound (10). Then the time horizon T = T0 + · · · + TK−1 satisﬁes

L

LD + σ2

eL

D + σ2

e

T µˆ log σ2 + µˆG 1 ∨ log log δ ≤ µˆ log G + µˆG 1 ∨ log log δ

and the corresponding tracking error satisﬁes ψT (XK ) − ψT
with probability at least 1 − Kδ.

G log e δ

7 Numerical illustrations
We investigate the empirical behavior of our ﬁnite-time bounds on numerical examples with synthetic data. We consider examples of a) least-squares recovery; b) sparse least-squares recovery; c) 22-regularized logistic regression; and investigate the behavior of xt − xt 2 and ϕt(xˆt) − ϕt in each case. The main ﬁndings are that our bounds exhibit: 1) the correct dependence on η, σ, and ∆; 2) excellent coverage in Monte-Carlo simulations. Code is available online at https://github.com/joshuacutler/TimeDriftExperiments.

102 102

E xt − xt 2 E[ϕt(ˆxt) − ϕt ]

101 101

0

20

40

60

t

Guaranteed bound

80

100

0

20

40

60

t

Asymptotic bound

Empirical average

80

100

95% CI

Figure 1: Semilog plots of guaranteed bounds and empirical tracking errors with respect to iteration t for least-squares recovery. Shaded regions indicate the 95% conﬁdence intervals for xt − xt 2 and ϕt(xˆt) − ϕt ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter values: µ = 1,
L = 1, σ = 10, ∆ = 1, and η = η .

34

Least-squares recovery. Fix x0, x0 ∈ Rd with standard Gaussian entries, and consider a Gaussian random walk (xt ) given by xt+1 = xt + vt, where vt is drawn uniformly from the sphere of radius ∆ in Rd. Given a ﬁxed matrix A ∈ Rn×d, we aim to recover the vectors (xt ) via least-squares:

min

E

1 2

Ax − w 2,

x∈Rd w∼Pt

where Pt = N (Axt , C) with C = (σ2/n A 2op)In. This amounts to the target problem (5) under the

identiﬁcations

ft(x)

=

Ew∼Pt

1 2

Ax − w

2 and rt = 0; clearly

xt − xt+1

= ∆ and supx

∇ft(x) −

∇ft+1(x) ≤ A 2op∆. We implement Algorithms 1 and 2 initialized at x0 using the sample gradient

gt = AT (Axt − w) at step t, where w ∼ Pt; hence E ∇ft(xt) − gt 2 ≤ σ2.

In our simulations, we take d = 50, n = 100, and randomly generate A via its singular value

d√ecomposition (using Haar-distributed√orthogonal matrices) so that its minimal singular value is µ and its maximal singular value is L. In Figures 1 and 2, we use default parameter values

µ = 1, L = 1, σ = 10, ∆ = 1, and the corresponding asymptotically optimal step size η = η .

Since ft is µ-strongly convex and L-smooth, this puts us in the low drift/noise regime in Figure 1: ∆/σ < µ/16L3 = 1/4. To estimate the empirical averages and conﬁdence intervals of xt − xt 2 and ϕt(xˆt) − ϕt , we run 100 trials with horizon T = 100. The results conﬁrm our bounds and show
that they capture the correct dependence on η, σ, and ∆.

E xT − xT 2

102 103
103

102

101

102

101

0.0

0.1

0.2

0.3

0.4

0.5

101

0

5

10

15

20

0

5

10

15

20

102
103 103

E[ϕT (ˆxT ) − ϕT ]

102

102

101

101 101

0.0

0.1

0.2

0.3

0.4

0.5

η

100

0

5

10

15

20

σ

0

5

10

15

20

∆

Guaranteed bound

Empirical average

95% CI

Figure 2: Semilog plots of guaranteed bounds and empirical tracking errors at horizon T = 100 with respect
to η, σ, and ∆ for least-squares recovery. Shaded regions indicate the 95% conﬁdence intervals for xT − xT 2 and ϕT (xˆT ) − ϕT ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter values: µ = 1, L = 1, σ = 10, ∆ = 1, and η = η .

35

10−1

10−1

E xt − xt 2 E[ϕt(ˆxt) − ϕt ]

10−2

10−3

0

20

40

60

t

Guaranteed bound

10−2

80

100

0

20

40

60

t

Asymptotic bound

Empirical average

80

100

95% CI

Figure 3: Semilog plots of guaranteed bounds and empirical tracking errors with respect to iteration t
for sparse least-squares recovery. Shaded regions indicate the 95% conﬁdence intervals for xt − xt 2 and ϕt(xˆt) − ϕt ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter values: µ = 1, L = 1, σ = 1/2, ∆ = 1/20, and η = η .

Sparse least-squares recovery. Next, we consider least-squares recovery constrained to the

closed 1-ball in Rd, which we denote by B1. We aim to recover a sparse sequence of vectors in B1 deﬁned as follows. Set s = l√og d , draw a vector u uniformly from the 1-ball in Rs, ﬁx x0 = (u, 0) ∈ Rd, and select ∆ ∈ (0, 2]. At step t, with probability p = (4 − 2∆2)/(4 − ∆√2), we set xt+1 = xt + v, where v is selected to have the same support as xt and satisfy v = ∆/ 2 and xt + v ∈ B1; otherwise, with probability 1 − p, we obtain xt+1 from xt by swapping precisely one
nonzero coordinate with a zero coordinate. Then the resulting sparse sequence (xt ) in B1 satisﬁes E xt − xt+1 2 ≤ ∆2. Given a ﬁxed matrix A ∈ Rn×d, we aim to recover (xt ) via constrained
least-squares:

min

E

1 2

Ax − w 2,

x∈B1 w∼Pt

where Pt = N (Axt , C) with C = (σ2/n A 2op)In. This amounts to the target problem (5) under

the

identiﬁcations

ft(x)

=

Ew∼Pt

1 2

Ax − w

2

and rt = δB1

(the convex indicator of B1); clearly

E[supx ∇ft(x) − ∇ft+1(x) 2] ≤ ( A 2op∆)2. Fixing x0 drawn uniformly from B1, we implement

Algorithms 1 and 2 initialized at x0 using the sample gradient gt = AT (Axt − w) at step t, where w ∼ Pt; hence E ∇ft(xt) − gt 2 ≤ σ2.

In our simulations, we take d = 50, n = 100, and randomly generate A via its singular value

d√ecomposition (using Haar-distributed√orthogonal matrices) so that its minimal singular value is µ and its maximal singular value is L. In Figures 3 and 4, we use default parameter values

µ = 1, L = 1, σ = 1/2, ∆ = 1/20, and the corresponding asymptotically optimal step size η = η .

Since ft is µ-strongly convex and L-smooth, this puts us in the low drift/noise regime in Figure 3: ∆/σ < µ/16L3 = 1/4. To estimate the empirical averages and conﬁdence intervals of xt − xt 2

and ϕt(xˆt) − ϕt , we run 100 trials with horizon T = 100. The results conﬁrm our bounds and show

that they capture the correct dependence on η, σ, and ∆.

36

E xT − xT 2

101

101

100 10−1 10−2

100 10−1

10−3

10−2

10−4

10−3

0.0

0.1

0.2

0.3

0.4

0.5

0

1

101

101

100

100

10−1

10−1

10−2

10−2

10−3

10−3

0.0

0.1

0.2

0.3

0.4

0.5

η

0

1

Guaranteed bound

100

10−1

10−2

10−3

2

3

4

5

0.0

0.1

0.2

0.3

0.4

0.5

100

10−1

10−2

10−3

10−4

2

3

4

5

σ

0.0

0.1

0.2

0.3

0.4

0.5

∆

Empirical average

95% CI

E[ϕT (ˆxT ) − ϕT ]

Figure 4: Semilog plots of guaranteed bounds and empirical tracking errors at horizon T = 100 with respect
to η, σ, and ∆ for sparse least-squares recovery. Shaded regions indicate the 95% conﬁdence intervals for xT − xT 2 and ϕT (xˆT ) − ϕT ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter values: µ = 1, L = 1, σ = 1/2, ∆ = 1/20, and η = η .

22-regularized logistic regression. Finally, we consider the time-varying 22-regularized logistic regression problem

min 1 x∈Rd n

n
log(1 + exp ai, x ) − Ax, bt
i=1

+ µ x 2, 2

where the ﬁxed matrix A ∈ Rn×d has standard Gaussian rows a1, ..., an ∈ Rd, (bt) is a random sequence of label vectors in {0, 1}n such that bt and bt+1 diﬀer in precisely one coordinate for

each t, and µ > 0. This amounts to the target problem (5) under the identiﬁcations ft(x) =

1 n

(

n i=1

log(1

+

exp

ai, x

)

−

Ax, bt

)

+

µ 2

x

2

and

rt

=

0;

setting

L

=

1 4n

A

2 op

+

µ,

it

follows

that

ft is µ-strongly convex and L-smooth. Letting (xt ) denote the corresponding sequence of minimizers

and

setting

∆

=

1 µn

maxi=1,...,n

ai

, it follows that supx

∇ft(x) − ∇ft+1(x)

≤ µ∆ and hence

xt − xt+1 ≤ ∆. Fixing the initial label b0 (drawn uniformly from {0, 1}n) and a standard Gaussian

vector x0 ∈ Rd, we implement Algorithms 1 and 2 initialized at x0 using the following random

summand sample gradient at each step t:

gt =

exp ak, xt − bk 1 + exp ak, xt t

ak + µxt,

where k ∼ U {1, . . . , n} and bkt denotes the kth coordinate of bt. Then E ∇ft(xt) − gt 2 ≤ σ2, where



σ2 = n12 (n − 2) n ai 2 + n ai

i=1

i,j=1

 aj  ≤ max 2 ai 2.
i=1,...,n

37

101 101

E xt − xt 2

100 10−1

E[ϕt(ˆxt) − ϕt ]

100 10−1 10−2

0

100 200 300 400 500 600

t

0

100 200 300 400 500 600

t

Guaranteed bound

Asymptotic bound

Empirical average

95% CI

Figure 5: Semilog plots of guaranteed bounds and empirical tracking errors with respect to iteration t for
22-regularized logistic regression. Shaded regions indicate the 95% conﬁdence intervals for xt − xt 2 and ϕt(xˆt) − ϕt ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter values: µ = 1 and η = η .

E xT − xT 2

103 101 10−1 10−3

E[ϕT (ˆxT ) − ϕT ]

102 101 100 10−1 10−2 10−3

0

5

10

15

20

µ

0

5

10

15

20

µ

Guaranteed bound

Empirical average

95% CI

Figure 6: Semilog plots of guaranteed bounds and empirical tracking errors at horizon T = 600 with respect to the strong convexity parameter µ for 22-regularized logistic regression. Shaded regions indicate the 95% conﬁdence intervals for xT − xT 2 and ϕT (xˆT ) − ϕT ; empirical averages and conﬁdence intervals are computed over 100 trials, using the asymptotically optimal step size η (which itself depends on µ).
In our simulations, we take d = 20 and n = 200, and we generate bt+1 from bt by ﬂipping a single coordinate selected uniformly at random. In Figure 5, we use default parameter values µ = 1 and the corresponding asymptotically optimal step size η = η . In Figure 6, we illustrate the dependence of tracking error on the regularization parameter µ; here, the asymptotically optimal step size η is used (which itself depends on µ). In Figure 7, we use the default parameter value µ = 1. To estimate the empirical averages and conﬁdence intervals of xt − xt 2 and ϕt(xˆt) − ϕt , we run 100 trials with horizon T = 600. The results conﬁrm our bounds and show that they capture the correct dependence on µ and η. In particular, Figure 7 illustrates that η is close to empirically optimal.

38

E xT − xT 2 E[ϕT (ˆxT ) − ϕT ]

103

102

101

100

10−1

10−2

0.0

0.1

Guaranteed bound

103

102

101

100

10−1

10−2

0.2

0.3

0.0

η

Asymptotically optimal step size

0.1

0.2

0.3

η

Empirical average

95% CI

Figure 7: Semilog plots of guaranteed bounds and empirical tracking errors at horizon T = 600 with respect
to the step size η for 22-regularized logistic regression. Shaded regions indicate the 95% conﬁdence intervals for xT − xT 2 and ϕT (xˆT ) − ϕT ; empirical averages and conﬁdence intervals are computed over 100 trials. Default parameter value: µ = 1. Observe that η is close to empirically optimal.

A Additional proofs

A.1 Proof of Theorem 6.9

For each index k, let tk := T0 + · · · + Tk−1 (with t0 := 0), X¯k be the minimizer of the corresponding function ψtk , and
E¯k := c ηkµ¯σ2 + µ¯∆¯η¯ 2 ,

where c ≥ 1 is an absolute constant satisfying the bound (8) in Theorem 5.11. Taking into account ηk ≥ η¯ and our selection of c, Theorem 5.11 implies that for any speciﬁed k ≥ 0 and δ ∈ (0, 1) the following estimate holds with probability at least 1 − δ:

X − X¯

2 ≤ 1 − µ¯ηk Tk X − X¯ 2 + c ηkσ2 +

∆¯ 2

e

log

k+1

k+1

2

k

k

µ¯

µ¯ηk

δ

≤ e−µ¯ηkTk/2 Xk − X¯k 2 + E¯k log e . δ
We will verify by induction that for all indices k ≥ 1, the estimate Xk − X¯k 2 ≤ 3E¯k−1 log(e/δ) holds with probability at least 1 − kδ for all δ ∈ (0, 1). To see the base case, observe that the estimate

X1 − X¯1 2 ≤ e−µ¯η0T0/2 X0 − X¯0 2 + E¯0 log e ≤ 3E¯0 log e

δ

δ

holds with probability at least 1 − δ for all δ ∈ (0, 1). Now assume that the claim holds for some index k ≥ 1, and let δ ∈ (0, 1); then Xk − X¯k 2 ≤ 3E¯k−1 log(e/δ) with probability at least 1 − kδ.

39

Thus, since we also have

Xk+1 − X¯k+1 2 ≤ e−µ¯ηkTk/2 Xk − X¯k 2 + E¯k log e δ

≤ 1 Xk − X¯k 2 + E¯k log e

4

δ

≤ E¯k X − X¯ 2 + E¯ log e

2E¯k−1 k

k

k

δ

with probability at least 1−δ, a union bound reveals Xk+1 −X¯k+1 2 ≤ 3E¯k log(e/δ) with probability

at least 1 − (k + 1)δ, thereby completing the induction. Hence, upon ﬁxing δ ∈ (0, 1), we have XK − X¯K 2 ≤ 3E¯K−1 log(e/δ) with probability at least 1 − Kδ.

Next, observe

2

E¯K−1

−

√ 3 54

c

∆¯ σ2 2/3 2σ2

2σ2 η0 − η¯

µ¯2

= µ¯ (ηK−1 − η¯ ) = µ¯ · 2K−1 ≤

∆¯ σ2

2/3
= E¯,

µ¯2

so XK − X¯K 2 ≤ 32c 1 + √3 54 E¯ log δe
with probability at least 1 − Kδ. Finally, note

E¯ log e δ

L

µ¯LD + 1 K−1 1

T µ¯ log σ2 + µ¯ ηk

k=1

and

K −1

1

K −1
≤ 2L 2k ≤ 2L · 2K = 8L · 2K−2 ≤ 8

σ2µ¯ 1/3 = 8σ2 ·

∆¯ σ2 −2/3

σ2 .

ηk

∆¯ 2

µ¯

µ¯2

µ¯E¯

k=1

k=1

This completes the proof.

A.2 The averaging lemma

We will use a small variation of the averaging lemma in [19]. To this end, consider a convex function h : Rd → R ∪ {∞} and let {xt}t≥0 be a sequence of vectors in dom h. Suppose that there are constants c1, c2 ∈ R, a nonnegative sequence of weights {ρt}t≥1, and scalar sequences {Vt}t≥0 and {ωt}t≥1 satisfying the recursion

ρth(xt) ≤ (1 − c1ρt)Vt−1 − (1 + c2ρt)Vt + ωt

(22)

for all t ≥ 1. The goal is to bound the function value h(xˆt) evaluated along an “average iterate” xˆt. Suppose that the relations c1 + c2 > 0, 1 − c1ρt > 0, and 1 + c2ρt > 0 hold for all t ≥ 1. Deﬁne
the augmented weights and products

ρˆ = ρt(c1 + c2)

and

t
Γˆ = (1 − ρˆ )

t 1 + c2ρt

t

i

i=1

for each t ≥ 1, while setting Γˆ0 = 1. A straightforward induction yields the relation

t ρˆi 1 1 + Γˆ = Γˆ .

i=1 i

t

Now set xˆ0 = x0 and recursively deﬁne the average iterates

xˆt = (1 − ρˆt)xˆt−1 + ρˆtxt

40

for all t ≥ 1. Unrolling this recursion, we may equivalently write

xˆ = Γˆ x + t ρˆi x .

(23)

t

t0

Γˆi i

i=1

The following is the key estimate we will need.

Lemma A.1 (Averaging). The following estimate holds for all t ≥ 0:

h(xˆt) + V ≤ Γˆ h(x0) + V + t

ωi

.

c1 + c2 t t c1 + c2 0 i=1 Γˆi(1 + c2ρi)

Proof. Observe that (23) expresses xˆt as a convex combination of x0, . . . , xt. Therefore, by the convexity of h we may apply Jensen’s inequality to obtain

h(xˆ ) ≤ Γˆ h(x ) + t Γˆtρˆi h(x ).

t

t0

Γˆi i

i=1

On the other hand, for each i ≥ 1, we may divide the recursion (22) by Γˆi(1 + c2ρi) to obtain

ρˆi h(xi) ≤ Vi−1 − Vi +

ωi

,

Γˆi(c1 + c2)

Γˆi−1 Γˆi Γˆi(1 + c2ρi)

which telescopes to yield

1 t ρˆi

Vt t

ωi

c1 + c2 i=1 Γˆi h(xi) ≤ V0 − Γˆt + i=1 Γˆi(1 + c2ρi) .

Hence as claimed.

h(xˆt) ≤ Γˆ h(x0) + V − Vt + t

ωi

,

c1 + c2 t c1 + c2 0 Γˆt i=1 Γˆi(1 + c2ρi)

A.3 Proof of Theorem 6.13

For each index k, let tk := T0 + · · · + Tk−1 (with t0 := 0) and Gk := ηkσ2 + 8∆¯ 2/µˆηˆ2. Then taking into account ηk ≥ ηˆ , Corollary 6.12 and inequality (14) directly imply

E ψt (Xk+1) − ψ

≤ 1 − µˆηk Tk E 3 ψt (Xk) − ψ + 5µˆ∆¯ 2T 2 + ηkσ2 + 8∆¯ 2

k+1

tk+1

2

k

tk

k

µˆηk2

≤ 3e−µˆηkTk/2E ψtk (Xk) − ψtk + 5e−µˆηkTk/2µˆ∆¯ 2Tk2 + Gk.

We will verify by induction that the estimate E ψtk+1(Xk+1) − ψtk+1 ≤ 11Gk holds for all indices k. To see the base case, observe that inequality (16) facilitates the estimation

E ψt1 (X1) − ψt1 ≤ 3e−µˆη0T0/2 ψ0(x0) − ψ0 + 5e−µˆη0T0/2µˆ∆¯ 2T02 + G0 ≤ 11G0.

Now assume that the claim holds for index k − 1. We then conclude

E ψtk+1 (Xk+1) − ψtk+1

≤ 3e−µˆηkTk/2E ψtk (Xk) − ψtk + 5e−µˆηkTk/2µˆ∆¯ 2Tk2 + Gk

≤ 1 E ψt (Xk) − ψ

16∆¯ 2

+

+ Gk

4

k

tk

µˆηk2

≤ Gk E ψt (Xk) − ψ

16∆¯ 2

+

+ Gk < 11Gk,

2Gk−1

k

tk

µˆηk2

41

completing the induction. Hence E ψT (XK ) − ψT ≤ 11GK−1. Next, observe

√ GK−1 − 3 250 · µˆ

∆¯ σ2

2/3 = σ2(ηK−1 − ηˆ ) = σ2 · η0 − ηˆ

µˆ2

2K−1

so E ψT (XK ) − ψT ≤ 11 12 + √3 250 · µˆ ∆¯µˆσ22

Finally, note

L

LD + 1 K−1 1

T µˆ log σ2 + µˆ ηk

k=1

and

≤ µˆ 2
2/3

∆¯ σ2 µˆ2
G.

2/3 = 1 G, 2

K −1

1

K −1
≤ 2L 2k ≤ 2L · 2K = 8L · 2K−2 ≤ 8

σ2µˆ

1/3
= 8σ2 · µˆ−1

∆¯ σ2

−2/3

σ2 .

ηk

∆¯ 2

µˆ2

G

k=1

k=1

This completes the proof.

A.4 Proof of Proposition 6.14

Fix t ≥ 1. Given i ≥ 1 and α > 0, the µ-strong convexity of ψt and Lemma 6.10 imply µη xi − x¯t 2 ≤ 2η ψt(xi) − ψt ≤ (1 − µ¯η) xi−1 − x¯t 2 − 1 − (γ + α)η xi − x¯t 2 + 2η zi−1, xi−1 − x¯t + 2η2 zi−1 2 + αη G¯2i−1,t,
hence 1 + (µ¯ − α)η xi − x¯t 2 ≤ (1 − µ¯η) xi−1 − x¯t 2 + 2η zi−1, xi−1 − x¯t + 2η2 zi−1 2 + αη G¯2i−1,t.
Taking α = µ¯, we obtain

xi − x¯t 2 ≤ (1 − µ¯η) xi−1 − x¯t 2 + 2η zi−1, xi−1 − x¯t + 2η2 zi−1 2 + µη¯ G¯2i−1,t.

Thus, given any λ ∈ (0, µ¯η] and proceeding by induction, we conclude

i−1
xi − x¯t 2 ≤ (1 − λ)i x0 − x¯t 2 + 2η zj, xj − x¯t (1 − λ)i−1−j
j=0

i−1

i−1

+ 2η2

zj

2(1

−

λ)i−1−j

+

η µ¯

G¯2j,t(1 − λ)i−1−j

j=0

j=0

for all i ≥ 1. Therefore

t−1
xi − x¯t 2(1 − λ)2(t−1−i)

i=0

t−1

t−1 i−1

≤ x0 − x¯t 2 (1 − λ)2(t−1)−i + 2η

zj , xj − x¯t (1 − λ)2t−3−j−i

i=0

i=1 j=0

t−1 i−1

t−1 i−1

+ 2η2

zj

2(1

−

λ)2t−3−j−i

+

η µ¯

G¯2j,t(1 − λ)2t−3−j−i.

i=1 j=0

i=1 j=0

42

Next, we compute

t−1

t−1

(1 − λ)2(t−1)−i = (1 − λ)t−1 (1 − λ)t−1−i < λ1 (1 − λ)t−1

i=0

i=0

and observe that for any scalar sequence (Xj)tj−=20, we have

t−1 i−1





t−2 t−1

Xj (1 − λ)2t−3−j−i = 

(1 − λ)t−2−i Xj (1 − λ)t−1−j .

i=1 j=0

j=0 i=j+1

Further, if Xj ≥ 0 for all j = 0, . . . , t − 2, then we have

t−1 i−1





t−2 t−1

Xj (1 − λ)2t−3−j−i = 

(1 − λ)t−1−i Xj (1 − λ)t−2−j

i=1 j=0

j=0 i=j+1

t−2

≤

1 λ

Xj (1 − λ)t−2−j .

j=0

Hence the following estimation holds:





t−1

t−2

t−1

xi − x¯t 2(1 − λ)2(t−1−i) ≤ 2η

(1 − λ)t−2−i zj, xj − x¯t (1 − λ)t−1−j

i=0

j=0

i=j+1

t−2

+ λ1 (1 − λ)t−1 x0 − x¯t 2 + 2λη2

zj 2(1 − λ)t−2−j

j=0

t−2

+

η µ¯λ

G¯2j,t(1 − λ)t−2−j .

j=0

This completes the proof.

A.5 Proof of Theorem 6.19

For each index k, let tk := T0 + · · · + Tk−1 (with t0 := 0) and Gk := ηkσ2 + ∆¯ 2/µˆηˆ2. Then taking

into account ηk ≥ ηˆ and our selection of the absolute constant c > 0 via (10), it follows that for all

indices k the estimate

ψtk+1 (Xk+1) − ψtk+1 ≤ c ≤c

1 − µˆ2ηk Tk ψtk (Xk) − ψtk + ηkσ2 + µˆ∆¯η22
k
e−µˆηkTk/2 ψtk (Xk) − ψtk + Gk log δe

log e δ

holds with probability at least 1 − δ.

We will verify by induction that for all indices k ≥ 1, the estimate

ψtk (Xk) − ψtk ≤ 3c · Gk−1 log δe

holds with probability at least 1 − kδ. To see the base case, observe that the estimate

ψt1 (X1) − ψt1 ≤ c e−µˆη0T0/2 ψ0(x0) − ψ0 + G0 log δe ≤ 3c · G0 log δe

holds with probability at least 1 − δ. Now assume that the claim holds for some index k ≥ 1. Then

43

because we also have ψtk+1 (Xk+1) − ψtk+1 ≤ c e−µˆηkTk/2 ψtk (Xk) − ψtk + Gk log δe
≤ c 4c log1(e/δ) ψtk (Xk) − ψtk + Gk log δe

≤c

Gk

ψt (Xk) − ψ + Gk log e

2c · Gk−1 log(e/δ) k

tk

δ

with probability at least 1 − δ, a union bound reveals that the estimate ψtk+1 (Xk+1) − ψtk+1 ≤ 3c · Gk log δe
holds with probability at least 1 − (k + 1)δ, thereby completing the ψT (XK ) − ψT ≤ 3c · GK−1 log(e/δ) with probability at least 1 − Kδ.
Next, observe

induction.

In particular,

GK−1 −

3

27 4

·

µˆ

∆¯µˆσ22 2/3 = σ2(ηK−1 − ηˆ ) = σ2 · η20K−−ηˆ1 ≤ µ2ˆ

∆¯ σ2 2/3 1

µˆ2

= 2G,

so

ψT (XK ) − ψT ≤ 3c 12 + 3 247 · µˆ ∆¯µˆσ22 2/3 log δe

G log e δ

with probability at least 1 − Kδ. Finally, note

L

LD +

e 1 K−1 1

T µˆ log σ2 + 1 ∨ log log δ µˆ ηk

k=1

and

K −1

1

K −1
≤ 2L 2k ≤ 2L · 2K = 8L · 2K−2 ≤ 8

σ2µˆ

1/3
= 8σ2 · µˆ−1

∆¯ σ2

−2/3

σ2 .

ηk

∆¯ 2

µˆ2

G

k=1

k=1

This completes the proof.

44

References
[1] A. Agarwal, O. Chapelle, M. Dudík, and J. Langford. A reliable eﬀective terascale linear learning system. Journal of Machine Learning Research, 15(1):1111–1133, 2014.
[2] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, volume 24, pages 451–459. Curran Associates, Inc., 2011.
[3] P. L. Bartlett, S. Ben-David, and S. R. Kulkarni. Learning changing concepts by exploiting the structure of change. Machine Learning, 41(2):153–174, 2000.
[4] P. L. Bartlett, V. Dani, T. P. Hayes, S. M. Kakade, A. Rakhlin, and A. Tewari. High-probability regret bounds for bandit online linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory. Omnipress, 2008.
[5] Y. Bechavod, K. Ligett, Z. S. Wu, and J. Ziani. Causal feature discovery through strategic modiﬁcation. arXiv:2002.07024, 2020.
[6] A. Benveniste, M. Métivier, and P. Priouret. Adaptive algorithms and stochastic approximations, volume 22. Springer Science & Business Media, 2012.
[7] O. Besbes, Y. Gur, and A. Zeevi. Non-stationary stochastic optimization. Operations Research, 63(5):1227–1244, 2015.
[8] L. Bottou. Stochastic learning. In Advanced Lectures on Machine Learning, ML Summer Schools 2003, volume 3176 of Lecture Notes in Computer Science, pages 146–168. Springer, 2003.
[9] L. Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade - Second Edition, volume 7700 of Lecture Notes in Computer Science, pages 421–436. Springer, 2012.
[10] L. Bottou and O. Bousquet. The tradeoﬀs of large scale learning. In Advances in Neural Information Processing Systems, volume 20, pages 161–168. Curran Associates, Inc., 2007.
[11] M. Brückner, C. Kanzow, and T. Scheﬀer. Static prediction games for adversarial learning problems. Journal of Machine Learning Research, 13(1):2617–2654, 2012.
[12] C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory, volume 23 of Proceedings of Machine Learning Research, pages 6.1–6.20. PMLR, 2012.
[13] J. Cutler, D. Drusvyatskiy, and Z. Harchaoui. Stochastic optimization under time drift: iterate averaging, step decay, and high probability guarantees. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021.
[14] N. N. Dalvi, P. M. Domingos, Mausam, S. K. Sanghai, and D. Verma. Adversarial classiﬁcation. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 99–108. ACM, 2004.
[15] D. Drusvyatskiy and L. Xiao. Stochastic optimization with decision-dependent distributions. arXiv:2011.11173, 2020.
[16] V. Dupač. A dynamic stochastic approximation method. The Annals of Mathematical Statistics, 36(6):1695–1702, 1965.
45

[17] S. Fujita and T. Fukao. Convergence conditions of dynamic stochastic approximation method for nonlinear stochastic discrete-time dynamic systems. IEEE Transactions on Automatic Control, 17(5):715–717, 1972.
[18] A. Gaivoronskii. Nonstationary stochastic programming problems. Cybernetics, 14(4):575–579, 1978.
[19] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469–1492, 2012.
[20] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 23(4):2061–2089, 2013.
[21] L. Guo and L. Ljung. Exponential stability of general tracking algorithms. IEEE Transactions on Automatic Control, 40(8):1376–1387, 1995.
[22] M. Hardt, N. Megiddo, C. H. Papadimitriou, and M. Wootters. Strategic classiﬁcation. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, pages 111–122. ACM, 2016.
[23] N. J. A. Harvey, C. Liaw, Y. Plan, and S. Randhawa. Tight analyses for non-smooth stochastic gradient descent. In Proceedings of the 32nd Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 1579–1613. PMLR, 2019.
[24] E. Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157–325, 2016.
[25] E. Hazan and S. Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15(1):2489–2512, 2014.
[26] E. Hazan and C. Seshadhri. Eﬃcient learning algorithms for changing environments. In Proceedings of the 26th International Conference on Machine Learning. Omnipress, 2009.
[27] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32(2):151–178, 1998.
[28] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. arXiv:1902.03736, 2019.
[29] L. V. Kantorovich and G. S. Rubinshte˘ın. On a space of completely additive functions. Vestnik Leningradskogo Universiteta. Matematika, Mekhanika, Astronomiya, 13(2):52–59, 1958. ISSN 0024-0850.
[30] A. Kulunchakov and J. Mairal. Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise. Journal of Machine Learning Research, 21(155):1–52, 2020.
[31] H. J. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Applications, volume 35 of Applications of Mathematics. Springer, 1997. ISBN 978-1-4899-2698-2.
[32] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365–397, 2012.
46

[33] H. Lang, L. Xiao, and P. Zhang. Using statistics to automate stochastic optimization. In Advances in Neural Information Processing Systems, volume 32, pages 9536–9546. Curran Associates, Inc., 2019.
[34] P. M. Long. The complexity of learning according to two models of a drifting environment. Machine Learning, 37(3):337–354, 1999.
[35] L. Madden, S. Becker, and E. Dall’Anese. Bounds for the tracking error of ﬁrst-order online optimization methods. Journal of Optimization Theory and Applications, 189(2):437–457, 2021.
[36] C. Mendler-Dünner, J. Perdomo, T. Zrnic, and M. Hardt. Stochastic optimization for performative prediction. In Advances in Neural Information Processing Systems, volume 33, pages 4929–4939. Curran Associates, Inc., 2020.
[37] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, and M. Hardt. Stochastic optimization for performative prediction. arXiv preprint arXiv:2006.06887, 2020.
[38] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[39] J. Perdomo, T. Zrnic, C. Mendler-Dünner, and M. Hardt. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7599–7609. PMLR, 2020.
[40] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pages 993–1019. PMLR, 2013.
[41] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning. Omnipress, 2012.
[42] D. Ruppert. A new dynamic stochastic approximation procedure. The Annals of Statistics, 7 (6):1179–1195, 1979.
[43] A. H. Sayed. Fundamentals of Adaptive Filtering. John Wiley & Sons, 2003.
[44] R. E. Schapire and Y. Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.
[45] S. Sra, S. Nowozin, and S. J. Wright. Optimization for Machine Learning. The MIT Press, 2011.
[46] N. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In Advances in Neural Information Processing Systems, volume 24, pages 2645–2653. Curran Associates, Inc., 2011.
[47] Y. Tsypkin and B. Polyak. Optimal recurrent algorithms for identiﬁcation of nonstationary plants. Computers and Electrical Engineering, 18(5):365–371, 1992.
[48] Y. Z. Tsypkin and Z. J. Nikolic. Adaptation and Learning in Automatic Systems. Elsevier Science, 1971.
[49] K. Uosaki. Some generalizations of dynamic stochastic approximation processes. The Annals of Statistics, 2(5):1042–1048, 1974.
47

[50] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science, volume 47 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
[51] C. Wilson, V. V. Veeravalli, and A. Nedić. Adaptive sequential stochastic optimization. IEEE Transactions on Automatic Control, 64(2):496–509, 2018.
[52] K. Wood, G. Bianchin, and E. Dall’Anese. Online projected gradient descent for stochastic optimization with decision-dependent distributions. arXiv:2107.09721, 2021.
48

