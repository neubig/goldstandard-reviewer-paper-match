Automatic Extraction of Rules Governing Morphological Agreement
Aditi Chaudhary1, Antonios Anastasopoulos2,†, Adithya Pratapa1, David R. Mortensen1, Zaid Sheikh1, Yulia Tsvetkov1, Graham Neubig1
1Language Technologies Institute, Carnegie Mellon University 2Department of Computer Science, George Mason University
{aschaudh,vpratapa,dmortens,zsheikh,ytsvetko,gneubig}@cs.cmu.edu antonis@gmu.edu

arXiv:2010.01160v2 [cs.CL] 6 Oct 2020

Abstract
Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a ﬁrst-pass grammatical speciﬁcation from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical speciﬁcation which is nearly equivalent to those created with large amounts of gold-standard annotated data. We conﬁrm this ﬁnding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https: //neulab.github.io/lase/. The code is publicly available here.1
1 Introduction
While the languages of the world are amazingly diverse, one thing they share in common is their adherence to grammars — sets of morpho-syntactic rules specifying how to create sentences in the language. Hence, an important step in the understanding and documentation of languages is the creation of a grammar sketch, a concise and human-readable description of the unique characteristics of that particular language (e.g. Huddleston (2002) for En-
1https://github.com/Aditi138/ LASE-Agreement
†: Work done at Carnegie Mellon University.

glish, or Brown and Ogilvie (2010) for the world’s languages).
One aspect of morphosyntax that is widely described in such grammatical speciﬁcations is agreement, the process wherein a word or morpheme selects morphemes in correspondence with another word or phrase in the sentence (Corbett, 2009). Languages have varying degrees of agreement ranging from none (e.g. Japanese, Malay) to a large amount (e.g. Hindi, Russian, Chichewa). Patterns of agreement also vary across syntactic subcategories. For instance, regular verbs in English agree with their subject in number and person but modal verbs such as “will” show no agreement.
Having a concise description of these rules is of obvious use not only to linguists but also language teachers and learners. Furthermore, having such descriptions in machine-readable format will further enable applications in natural language processing (NLP) such as identifying and mitigating gender stereotypes in morphologically rich languages (Zmigrod et al., 2019).
The notion of describing a language “in its own terms” based solely on raw data has an established tradition in descriptive linguistics (e.g. Harris (1951)). In this work we present a framework (outlined in Figure 1) that automatically creates a ﬁrst-pass speciﬁcation of morphological agreement rules for various morphological features (Gender, Number, Person, etc.) from a raw text corpus for the language in question. First, we perform syntactic analysis, predicting part-of-speech (POS) tags, morphological features, and dependency trees. Using this analyzed data, we then learn an agreement prediction model that contains the desired rules. Speciﬁcally, we devise a binary classiﬁcation problem of identifying whether agreement will be observed between a head and its dependent token on a given morphological property. We use decision trees as our classiﬁcation model because

Raw Text

λιµάνι

της

Ηγουµενίτσας

συνδέεται µε πολλά

λιµάνια

της

port.SG

DET

Igoumenítsa.GEN connect.SG with many

port.PL

DET

Ιταλίας Italy.GEN

και

της

and

DET

Αλβανίας Albania.GEN

Dependency Parsed Data

mod

comp:obj

mod

det

udep

mod

det

λιµάνι

της

Ηγουµενίτσας συνδέεται µε πολλά

λιµάνια

της

Ιταλίας

NOUN;NEUT DET;FEM PROPN;FEM VERB

ADP ADJ;NEUT NOUN;NEUT DET;FEM PROPN;FEM

Training Data Extraction

Decision Tree Learning

Training Sample NOUN det DET PROPN det DET NOUN mod ADJ PROPN mod NOUN PROPN mod PROPN

Agree? Yes Yes Yes No Yes

Leaf -1: relation = det, head-POS = NOUN, PROPN, child-POS = *
Leaf -2: relation = mod, head-POS = NOUN, PROPN, child-POS = ADJ,PROPN

mod

conj

det

και

της

Αλβανίας

CCONJ DET;FEM PROPN;FEM

Labeling

Use/Evaluation

Leaf-1:

Required-Agreement

Leaf-2: Chance-Agreement

Linguist

Figure 1: An overview of our method’s workﬂow for gender agreement in Greek. The example sentence translates to “The port of Igoumenitsa is connected to many ports in Italy and Albania.” First, we dependency parse and morphologically analyze raw text to create training data for our binary agreement classiﬁcation task. Next, we learn a decision tree to extract the rule set governing gender agreement, and label the extracted leaves as either representing required or chance agreement. Finally these rules are presented to a linguist for perusal.

they are easy to interpret and we can easily extract the classiﬁcation rules from the tree leaves to get an initial set of potential agreement rules. Finally, we perform rule labeling of the extracted rules, identifying which tree leaves correspond to probable agreement. This is required because not all agreeing head/dependent token pairs are necessarily due to some underlying rule. For instance, in Figure 1’s example of Greek gender agreement, both the head and its dependent token Ιταλίας→Αλβανίας have feminine gender, but this agreement is purely bychance, as correctly identiﬁed by our framework.
The quality of the learnt rules depends crucially on the quality and quantity of dependency parsed data, which is often not readily available for lowresource languages. Therefore, we experiment with not only gold-standard treebanks, but also trees generated automatically using models trained using cross-lingual transfer learning. This assesses the applicability of the proposed method in a situation where a linguist may want to explore the characteristics of agreement in a language that does not have a large annotated dependency treebank.
We evaluate the correctness of the extracted rules conducting human evaluation with linguists for Greek, Russian, and Catalan. In addition to the manual veriﬁcation, we also devise a new metric for automatic evaluation of the rules over unseen test data. Our contributions can be summarized to: 1. We propose a framework to automatically ex-
tract agreement rules from raw text, and release these rules for 55 languages as part of an interface2 which visualizes the rules in detail along
2https://neulab.github.io/lase/

with examples and counter-examples. 2. We design a human evaluation interface to allow
linguists to easily verify the extracted rules. Our framework produces a decent ﬁrst-pass grammatical speciﬁcation with the extracted rules having an average accuracy of 78%. We also devise an automated metric to evaluate our framework when human evaluation is infeasible. 3. We evaluate the quality of extracted rules under real zero-shot conditions (on Breton, Buryat, Faroese, Tagalog, and Welsh) as well as lowresource conditions (with simulation experiments on Spanish, Greek, Belarusian and Lithuanian) varying the amount of training data. Using cross-lingual transfer, rules extracted with as few as 50 sentences with gold-standard syntactic analysis are nearly equivalent to the rules extracted when we have hundreds/thousands of gold-standard data available.
2 Problem Formulation
For a head h and a dependent d that are in a dependency relation r, we will say that they agree on a morphological property f if they share the same value for that particular property i.e. fh = fd. Some agreements that we observe in parsed data can be attributed to an underlying grammatical rule. For example, in Figure 2 the Spanish A.1 shows an example of where subject (enigmas) and verb (son) need to agree on number. We will refer to such rules as required-agreement. Such a required agreement rule dictates that an example like A.2 is ungrammatical and would not appear in wellformed Spanish sentences, since the subject and

A.1 Los

enigmas son fáciles

DET.PL

riddle.PL be.PL easy.PL _req__

‘The riddles are easy.’

A.2 *Los DET.PL

enigmas es riddle.PL be.SG
wrong

fácil easy.SG

B.1 Mi hermano tiene un

perro

My brother.SG has.SG ART.SG dog.SG _req__ _____chance_____

‘My brother has a dog.’

B.2 Mi hermano tiene muchos perros My brother.SG has.SG many.PL dog.PL _req__ _____correct_____ ‘My brother has many dogs.’

Figure 2: Subject-verb number agreement is required in Spanish, as in example A.1, which renders example A.2 ungrammatical. Object-verb agreement is not required, so both B.1 and B.2 are grammatical. The object and the verb in B.1 only agree by chance.

the verb do not have the same number marking. However, not all word pairs that agree do so because of some underlying rule, and we will refer to such cases as chance-agreement. For example, in Figure 2 the object (perro) and verb (tiene) in B.1 only agree in number by chance, and example B.2 (where the object of a singular verb is plural) is perfectly acceptable.
Our goal is to extract, from textual examples, the set of rules Rfl that concisely describe the agreement process for language l. Concretely, this will indicate for which head-dependent pairs the language displays required-agreement and for which we will observe at most chance-agreement. Canonically, agreement rules are deﬁned over syntactic features of a language as seen in Figure 2 where we have the following rule for Spanish: “subjects agree with their verbs on number”.3 To formalize this notion, we deﬁne a rule to be a set of features which are deﬁned over the dependency relation, head and dependent token types. In this paper, we make the simplifying assumption that head and dependent tokens are represented by only part-ofspeech features, as we would like our extracted rules to be concise and easily interpretable downstream, although this assumption could be relaxed in future work.
The rule discovery process consists of two major steps: a rule extraction step followed by a rule labeling and merging step (also see Figure 1).
3Sometimes semantic features are used for agreement for eg. United Nations is, despite United Nations being plural, it is treated as singular for purposes of agreement.

2.1 Rule Extraction
To create our training data for rule extraction, we ﬁrst annotate raw text with part-of-speech (POS) tags, morphological analyses, and dependency trees. We then base our training data on these annotations by converting each dependency relation into a triple h, d, r , indicating the head token, dependent/child token, and dependency relation between h and d respectively. From the whole treebank, we now have input features Xf = { h1, d1, r1 , . . . , hn, dn, rn } and binary output labels Y =y1, . . . , yn, where if the head and the dependent token agree on feature f (such that fh=fd) we set y = 1, otherwise y = 0. We ﬁlter out the tuples where either of the linked tokens does not display the morphological feature f .
We train a model for p(Y |X) using decision trees (Quinlan, 1986) using the CART algorithm (Breiman et al., 1984). A major advantage of decision trees is that they are easy to interpret and we can visualize the exact features used by the decision tree to split nodes. The decision tree induces a distribution of agreement over training samples in each leaf, e.g. 99% agree, 1% not agree in Leaf-3 for gender agreement in Spanish (Figure 3(a)).
2.2 Rule Labeling
Now that we have constructed a decision tree where each tree leaf corresponds to a salient partition of the possible syntactic structures in the language, we then label these tree leaves as required-agreement or chance-agreement. For this we apply a threshold on the ratio of agreeing training samples within a leaf – if the ratio exceeds a certain number the leaf will be judged as required-agreement. We experiment with two types of thresholds:
Hard Threshold: We set a hard threshold on the ratio that is identical for all leaves. In all experiments, we set this threshold to 90% based on manually inspecting some resulting trees to ﬁnd a threshold that limited the number of non-agreeing syntactic structures being labeled as required-agreement.
Statistical Threshold: Leaves with very few examples may exceed the hard threshold purely by chance. In order to better determine whether the agreements are indeed due to a true pattern of required agreement, we devise a thresholding strategy based on signiﬁcance testing. For all agreementmajority leaves, we apply a chi-squared goodness of ﬁt test to compare the observed output distri-

node 1 child-pos= aux,adj,verb,pron,
propn,det,num

node 1 child-pos= aux,adj,verb,pron,
propn,det,num

node 1 child-pos= aux,adj,verb,pron,
propn,det,num

child-pos= noun
node 2
relation= det Leaf 1: not agree: 1462, agree: 2433
relation = conj, det head-pos = any child-pos = noun

Leaf 3: not agree: 778, agree: 58076
relation = any head-pos = any child-pos = aux,adj,verb,pron,
propn,det,num
relation= comp:obj
Leaf 2: not agree: 268, agree: 373
relation = comp:obj head-pos = any child-pos = noun

child-pos= noun
node 2

Leaf 3: required-agreement
relation = any head-pos = any child-pos = aux,adj,verb,pron,
propn,det,num

relation= det

relation= comp:obj

Leaf 1: chance-agreement Leaf 2: chance-agreement

relation = conj, det head-pos = any child-pos = noun

relation = comp:obj head-pos = any child-pos = noun

Leaf 3: required-agreement

child-pos= noun

relation = any head-pos = any child-pos = aux,adj,verb,pron,
propn,det,num

Leaf 1: chance-agreement
relation = conj, det, comp:obj head-pos = any child-pos = noun

(a) Rule Extraction

(b) Rule Labeling

(c) Rule Merging

Figure 3: Extracting gender agreement rules in Spanish. (a) A decision tree is learned over dependency link triples,

inducing a distribution of agreement over examples in each leaf. However, simple majority voting leads to false

positives: Leaf-1 includes more agreeing data points, but in reality this agreement is purely by chance. (b) With

a statistically-inspired threshold to label the leaves, Leaf-1 gets correctly labeled as chance-agreement. (c) We

merge leaves with the same label to get a concise representation. Every dependency link triple receives the label

of the unique leaf it falls under.

bution with an expected probability distribution speciﬁed by a null hypothesis. Our null hypothesis H0 will be that any agreement we observe is due to chance. If we reject the null hypothesis, we will conclude from the alternate hypothesis H1 that there exists a grammatical rule requiring agreement for this leaf’s cases:

H0 : The leaf has chance-agreement. H1 : The leaf has required-agreement.

If there is no rule requiring agreement, we assume that the morphological properties of the head and the dependent token are independent and identically distributed discrete random variables following a categorical distribution. We compute the probability of chance agreement based on the number of values that the speciﬁc morphological property f can take. Since morphological feature values are not equally probable, we use a probability proportional to the observed value counts. For a binary number property where 90% of all observed occurrences are singular and 10% are plural, the probability of chance agreement is equal to 0.82=0.9×0.9+0.1×0.1, which gives the observed output distribution p=[0.18, 0.82]. Using p we compute the expected frequency count Ei = npi where n is the total number of samples in the given leaf, i=[0, 1] is the output class of the leaf, and pi is the hypothesized proportion of observations for class i. The chi-squared test calculates the test statistic χ2 as follows:

χ2 =

(Oi − Ei)2

Ei

i∈[0,1]

where Oi is the observed frequency count in the given leaf. The test outputs a p-value, which is the

probability of observing a sample statistic as extreme as the test statistic. If the p-value is smaller than a chosen signiﬁcance level (we use 0.01) we reject the null hypothesis and label the leaf as required-agreement.
The chi-squared test especially helps in being cautious with leaves with very few examples. However, for leaves with larger number of examples statistical signiﬁcance alone is insufﬁcient, because there are a large number of cases where there are small but signiﬁcant differences from the ratio of agreement expected by chance.4 Therefore, in addition to comparing the p-value we also compute the effect size which provides a quantitative measure on the magnitude of an effect (Sullivan and Feinn, 2012). Cramér’s phi φc (Cramér, 1946) is a commonly used method to measure the effect size:
χ2 φc = N (k − 1)
where χ2 is the test statistic computed from the chi-squared test, N is the total number of samples within a leaf, and k is the degree of freedom (which in this case is 2 since we have two output classes). Cohen (1988) provides rules of thumb for interpreting these effect size. For instance, φc > 0.5 is considered to be a large effect size and a large effect size suggests that the difference between the two hypotheses is important. Therefore, a leaf is labeled as required-agreement when the p-value is less than the signiﬁcance value and the effect size is greater than 0.5. Now Leaf-1 in Figure 3(b) is correctly identiﬁed as chance-agreement.
4One limitation of this is that rules that show agreement sometimes get incorrectly labeled as either chance-agreement or required-agreement. We consider this in evaluation, but predicting sometimes agreement is relegated to future work.

Rule Merging: Because we are aiming to have a concise, human-readable representation of agreement rules of a language, after labeling the tree leaves we merge sibling leaves with the same label as shown in Figure 3(c). Further, we collapse tree nodes having all leaves with the same label thereby reducing the apparent depth of the tree.
3 Experimental Settings and Evaluation
Our experiments aim to answer the following research questions: (1) can our framework extract linguistically plausible agreement rules across diverse languages? and (2) can it do so even if gold-standard syntactic analyses are not available? To answer the ﬁrst question we evaluate rules extracted from gold-standard syntactic analysis (Sec. §4). For the second question we experiment in low-resource and zero-shot scenarios using crosslingual transfer to obtain parsers on the languages of interest, and evaluate the effect of noisy parsing results on the quality of rules (Sec. §5).
3.1 Settings
Data We use the Surface-Syntactic Universal Dependencies (SUD) treebanks (Gerdes et al., 2018, 2019) as the gold-standard source of complete syntactic analysis. The SUD treebanks are derived from Universal Dependencies (UD) (Nivre et al., 2016, 2018), but unlike the UD treebanks which favor content words as heads, the SUD ones express dependency labels and links using purely syntactic criteria, which is more conducive to our goal of learning syntactic rules. We use the tool of Gerdes et al. (2019) to convert UD v.2.5 (Nivre et al., 2020) into SUD. We only use the training portion of the treebanks for learning our rules.
Rule Learning We use sklearn’s (Pedregosa et al., 2011) implementation of decision trees and train a separate model for each morphological feature f for a given language. We experiment with six morphological features (Gender, Person, Number, Mood, Case, Tense) which are most frequently present across several languages. We perform a grid search over the decision tree parameters (detailed in Appendix A.1) and select the model performing best on the validation set. We report results with the Statistical Threshold because on manual inspection we ﬁnd the trees to be more reliable than the ones learnt from the Hard Threshold (see Appendix A.5 for an example).

3.2 Evaluation
We explore two approaches to evaluate the extracted rules, one based on expert annotations, and an automated proxy evaluation.
Expert Evaluation Ideally, we would collect annotations for all head-relation-dependent triples in a treebank, but this would involve annotating hundreds of triples, requiring a large time commitment from linguists in each language we wish to evaluate. Instead, for each language/treebank we extract and evaluate the top 20 most frequent “head POS, dependency relation, dependent POS” triples for the six morphological features amounting to 120 sets of triples to be annotated.5 We then present these triples with 10 randomly selected illustrative examples and ask a linguist to annotate whether there is a rule in this language governing agreement between the head-dependent pair for this relation. The allowed labels are: Almost always agree if the construction must almost always exhibit agreement on the given feature; Sometimes agree if the linked arguments sometimes must agree, but sometimes do not have to; Need not agree if any agreement on the feature is random. An example of the annotation interface is shown in the Appendix A.2.
For each of the human annotated triples in feature f , we extract the label assigned to it by the learnt decision tree T . We ﬁnd the leaf to which the given triple t belongs and assign that leaf’s label to the triple, referred by ltree,f,t. The human evaluation score (HS) for each triple marking feature f is given by:

HSf,t = 1

1 lhuman,f,t = ltree,f,t 0 otherwise

where lhuman,f,t is the label assigned to the triple t by the human annotator. These scores are then averaged across all annotated triples Tf to get the human evaluation metric (HRM) for feature f

HRMf =

t∈Tf HSf,t .
|Tf |

Automated Evaluation As an alternative to the infeasible manual evaluation of all rules in every language, we propose an automated rule metric (ARM) that evaluates how well the rules extracted from decision tree T ﬁt to unseen gold-annotated test data. For each triple t marking feature f , we
5The top 20 most frequent triples covered approximately 95% of the triples where this feature was active on average.

∆ARM

∆ARM

Indo-Aryan Uralic Slavic

Germanic Baltic Semitic
0.3

0.2

0.1

0

−0.1

0.3 Gender

Case

Person

0.2

0.1

0

−0.1

Number

Mood

Tense

Figure 4: Difference in the ARM scores of decision

trees over gold-standard syntactic analysis with base-

line trees where all leaves predict chance-agreement.

ﬁrst retrieve all examples from the test data corresponding to that triple. Next, we calculate the empirical agreement by counting the fraction of test samples that exhibit agreement, referred by qf,t. For a required-agreement leaf, we expect most test samples satisfying that rule to show agreement.6 To account for any exceptions to the rule and/or parsing-related errors, we use a threshold that acts as proxy for evaluating whether the given triple denotes required agreement. We use a threshold of 0.95, and if qf,t > 0.95 then we assign the test label ltest,f,t for that triple as required-agreement, and otherwise choose chance-agreement.7 Similar to the human evaluation, we compute a score for each triple t marking feature f

ASt = 1

1 ltest,f,t = ltree,f,t 0 otherwise

then average scores across all annotated triples in Tf to get the ARM score for each feature f :

ARMf =

t∈Tf ASt |Tf |

4 Experiments with Gold-Standard Data
In this section, we evaluate the quality of the rules induced by our framework, using gold-standard syntactic analyses and learning the decision trees over triples obtained from the training portion of all SUD treebanks. As baseline, we compare with trees predicting all leaves as chance-agreement.
6There are exceptions: e.g. when the head of dependent is a multiword expression (MWE), in which case dependency parsers might miss or pick only one of its constituents as head/dependent, or if the MWE is syntactically idiosyncratic.
7We keep a 5% margin to account for any exceptions or parsing errors based on the feedback given by the annotators.

ARM

1

Hindi Russian North Sami Tamil Arabic

0.8

0.6

0.4

0.2

0

×

×

×

Gender

Case

Person

Tense

Figure 5: Our approach (shaded bars) outperforms the chance-agreement baseline (solid bars) in all cases where there exist agreement rules. Features not present in the language are marked with ×.

The extracted rules have an 0.574 ARM score (averaged across all treebanks and features), outperforming the baseline scores by 0.074 ARM points.8 Of all the 451 decision trees across all treebanks and features, we ﬁnd 78% trees outperforming the baseline trees. In Figure 4, we show the improvements over the baseline averaged across language families/genera. In families with extensive agreement systems such as Slavic and Baltic our models clearly outperform the baseline discovering correct rules, as they do for the other Indo-European genera, Indo-Aryan and Germanic. For mood and tense, the chance-agreement baseline performs on par with our method. This is not surprising because there is little agreement observed for these features given that only verbs and auxiliary verbs mark these features. We ﬁnd that for both tense and mood in the Indo-Aryan family, our model identiﬁes required-agreement primarily for conjoined verbs, which mostly need to agree only if they share the same subject. However, subsequent analysis revealed that in the treebanks nearly 50% of the agreeing verbs do not share the same subject but do agree by chance.
Agreement for Indo-European languages like Hindi and Russian is well documented (Comrie, 1984; Crockett, 1976) and is reﬂected in our large improvements over the baseline (Figure 5). Similarly, Arabic exhibits extensive agreement on noun phrases including determiners and adjectives (Aoun et al., 1994). We ﬁnd that for Arabic gender the lower ARM scores of our method are an artifact of the small test data.
North Sami is an interesting test bed: as a Uralic language, case agreement would be somewhat unexpected and indeed our model’s predictions are not better than the baseline. Nevertheless, with our interface we ﬁnd patterns of rare positive paratactic constructions with required agree-
8Individual scores for each treebank are in Appendix A.5.

Morphological richness

8.5

8.0

7.5

tar r dsle

7.0 hu edsnaoitetanfsrv nul r

6.5
6.0 lzh

af wo gdgl

et cs fi

laphl ruk grrcu orv

sr

rosklt hy

eglot shmi e

cu

ga

5.5 kk cop hsb mr 5.0 kmr 4.5 bolxor

123456789 Avg. number of leaves

Figure 6: Correlation between size of the decision trees constructed by our framework and morphological complexity of languages.
ment where demonstrative pronouns overwhelmingly agree with their heads.9 The case decision tree also uncovers interesting patterns of 100% agreement on Tamil constructions with nominalized verbs (Gerunds) where the markings propagate to the whole phrase.
Conciseness of Extracted Rules We further analyze the decision trees learnt by our framework for conciseness and ﬁnd that the trees grow more complex with increasing morphological complexity of languages as seen in Figure 6. To compute the morphological complexity of a language, we use the word entropy measure proposed by Bentz et al. (2016) which measures the average information content of words and is computed as follows:

H(D) = − p(wi) log p(wi)
i∈V

where V is the vocabulary, D is the monolingual text extracted from the training portion of the respective treebank, p(wi) is the word type frequency normalized by the total tokens. Since this entropy doesn’t account for unseen word types, Bentz et al. (2016) use the James-Stein shrinkage estimator (Hausser and Strimmer, 2009) to calculate p(wi):

p(wi) = λptarget(wi) + (1 − λ)pML(wi)

where λ∈[0, 1], ptarget denotes the maximum en-

tropy case given by the uniform distribution

1 V

and

pML is the maximum likelihood estimator which

is given by the normalized word type frequency.

Languages with a larger word entropy are consid-

ered to be morphologically rich as they pack more

information into the words. In Figure 6 we plot the

9Leaf 3 here: https://bit.ly/34mHTeG

morphological richness with the average number of leaves across all features and ﬁnd these to be highly correlated.
Manual Evaluation Results We conduct an expert evaluation for Greek (el), Russian (ru) and Catalan (ca) as described in Section §3.2. For a strict setting, we consider both Sometimes agree and Need not agree as chance-agreement and report the human evaluation metric (HRM) in Figure 7. Overall, our method extracts ﬁrst-pass grammar rules achieving 89% accuracy for Greek, 78% for Russian and 66% for Catalan.
In most error cases, like person in Russian, our model produces required-agreement labels, which we can attribute to skewed data statistics in the treebanks. In Russian and Greek, for instance, conjoined verbs only need to agree in person and number if they share the same subject (in which case they implicitly agree because they both must agree with the same subject phrase). In the treebanks, though, only 15% of the agreeing verbs do indeed share the same subject – the rest agree by chance. In a reverse example from Catalan, the overwhelming majority (92%) of 8650 tokens are in the third-person, causing our model to label all leaves as chance agreement despite the fact that person/number agreement is required in such cases. Similarly for tense in Catalan, our framework predicts chance-agreement for auxiliary verbs with verbs as their dependent because of overwhelming majority of disagreeing examples. We believe this is because of both annotation artifact and the way past tense is realized.
To demonstrate how well the automated evaluation correlates with the human evaluation protocol, we compute the Pearson’s correlation (r) between the ARM and HRM for each language under four model settings: simulate-50, simulate100, baseline and gold. simulate-x is a simulated low-resource setting where the model is trained using x gold-standard syntactically analysed data.10 The baseline setting is the one where all leaves predict chance-agrement and under the gold setting we train using the entire gold-standard data. We compute the ARM and HRM scores for the rules learnt under each of the four settings and report the Pearson’s correlation, averaged across all features. Overall, we observe a moderate correlation for all three languages, with r = 0.59 for Greek, r=0.41 for Russian and r=0.38 for Catalan. The correla-
10More details on the experimental setup in § 5.1.

ARM

Gender Number
1 0.8 0.6 0.4 0.2
0
Greek

Person Tense
×
Russian

Case Mood
×
Catalan

Figure 7: Annotation accuracy for Greek, Russian and Catalan per each morphological feature.

tions are very strong for some features such as Gender (rel=0.97, rru=0.82, rca=0.98) and Number (rel=0.97, rru=0.69, rca=0.96) where we expect to see extensive agreement.

5 Low-Resource Experiments
5.1 Simulated Zero-/Few-Shot Experiments
It is not always possible to have access to goldstandard syntactic analyses. Therefore, in order to investigate how the quality of rules are affected by the quality of syntactic analysis, we conduct simulation experiments by varying the amount of goldstandard syntactically analysed training data. For each language, we sample x fully parsed sentences from the its treebank out of L training sentences available. For the remaining L − x sentences, we use silver syntactic analysis i.e., we train a syntactic analysis model on x sentences and use the model predictions for the L − x sentences.
Data and Setup: We experiment with Spanish, Greek, Belarusian and Lithuanian. For transfer learning, we use Portuguese, Ancient Greek, Ukrainian and Latvian treebanks respectively. The data statistics and details are in Appendix A.2.
We train Udify (Kondratyuk and Straka, 2019), a parser that jointly predict POS tags, morphological features, and dependency trees, using the x goldstandard sentences as our training data. We generate model predictions on the remaining L − x sentences. Finally, we concatenate the x gold data with the L − x automatically parsed data from which we extract the training data for learning the decision tree. We experiment with x = [50, 100, 500] gold-standard sentences. To account of sampling randomness, we repeat the process 5 times and report averages across runs.
To further improve the quality of the automatically obtained syntactic analysis, we use crosslingual transfer learning where we train the Udify model by concatenating x sentences of the target language with the entire treebank of the related

ARM

Greek (Ancient Greek)
1

0.8 0.630.680.64 0.6
0.53 0.63
0.4 0 50 100

0.69 0.68

0.63 0.62

500 1662

Lithuanian
(Latvian)
0.7 0.663 0.65 0.6180.632 0.62

0.6 0.55

0.610.6 0.61

0.5

0 50 100

500

0.625 0.63
2341

ARM

Spanish (Portuguese)
0.7 0.65 0.65
0.63 0.62

0.6 0.61 0.6

0.61

0.5 0 50 100 500

without Belarusian with transfer (Ukrainian)

0.7

0.64

0.63

0.64

0.64

0.59 0.6 0.59

0.60 0.64

0.56

0.5

14187

0 50 100 319

Figure 8: Comparing the (avg.) ARM score for Number agreement with and without cross-lingual transfer learning (transfer language in parenthesis). xaxis in log space. The higher the ARM the better.

[Relation, Head, Dependent] correct label

det, NOUN, DET.

almost always

mod, NOUN, ADJ

almost always

ﬂat, PROPN, PROPN

almost always

mod, PROPN, PROPN

almost always

appos, PROPN, PROPN

sometimes

comp:aux@pass, AUX, VERB need not

conj, PROPN, PROPN

need not

ARM score over the test set:

gold
required required required required required chance required
0.644

zero-shot
required required chance chance chance required chance
0.632

Table 1: The Spanish gender rules extracted in a zeroshot setting are generally similar to the ones extracted from the gold data (93%). We highlight the few mistakes that the zero-shot tree makes.

language. We also conduct zero-shot experiments under this setting where we directly use the Udify model trained only on the related language and get the model predictions on L sentences. As before, we train ﬁve decision trees for each x setting and report the average ARM over the test data.
Results We report the results for Number agreement in Figure 8. Similar plots for other languages and features can be found in the Appendix A.5. We observe that using cross-lingual transfer learning (CLTL) already leads to high scores across all languages even in zero-shot settings where we do not use any data from the gold-standard treebank. Taking Spanish gender as an example, 93% of the ruletriples extracted from the gold-standard tree (which are overwhelmingly correct) are also extracted by the zero-shot tree. The zero-shot tree only makes a few mistakes (shown in Table 1 and reﬂected in its overall ARM score) on certain proper noun and auxiliary verb constructions. Interestingly, using CLTL, training with just 50 gold-standard target language sentences is almost equivalent to

training with 100 or 500 gold-standard sentences. This opens new avenues for language documentation: with as few as 50 expertly-annotated syntactic analysis of a new language and CLTL our framework can produce decent ﬁrst-pass agreement rules. Needless to say, in most cases the extracted rules improve as we increase the number of goldstandard sentences and CLTL further helps bridge the data availability gap for low-resource settings.

ARM

1 Gender Person Number Mood, Tense

0.8

0.6

0.4

0.2

0

× ××

×

Breton Buryat Faroese Tagalog Welsh

Figure 9: In most cases our framework (shaded bars) extracts a good ﬁrst-pass speciﬁcation for true zeroshot settings. Solid bars indicate the baseline.

5.2 Real Zero-Shot Experiments
Some languages like Breton, Buryat, Faroese, Tagalog and Welsh have test data only; there is no goldstandard training data available, which presents a true zero-shot setting. In such cases, we can still extract grammar rules with our framework using zero-shot dependency parsing.
Data and Setup: We collect raw text for the above languages from the Leipzig corpora (Goldhahn et al., 2012). Data statistics are listed in Appendix A.2. We parse these sentences using the “universal" Udify model that has been pre-trained on all of the UD treebanks, as released by (Kondratyuk and Straka, 2019). As before, we use these automatically parsed syntactic analyses to extract the rules which we evaluate with ARM over the gold standard test data of the corresponding SUD treebanks.
Results: We report the ARM scores in Figure 9. Averaged over all rules, our approach obtains a ARM of 0.566, while the naive all-chance baseline only achieves 0.506. The difference appears to be small, but we still consider it signiﬁcant, because these languages do not actually require agreement for many grammatical features. Tagalog and Buryat are the most distant languages that we test on (no Philippine and Mongolic language is present in our training data) and yet we observe our method being at par with the baseline and even outperforming in case of Tagalog. Breton and Welsh, on the other hand, are an interesting test bed: Celtic languages are to some degree outliers among Indo-European languages (Borsley and Roberts, 2005), and we suspect that as a result the parser performs generally worse. Despite that, our approach has an ARM of 0.730 for Welsh gender agreement, as opposed to the mere 0.615 that the baseline achieves.

6 Related Work
Bender et al. (2014) use interlinear glossed text (IGT) to extract lexical entities and morphological rules for an endangered language. They experiment with different systems which individually extract lemmas, lexical rules, word order and the case system, some of which use hand-speciﬁed rules. Howell et al. (2017) extend this to work to predict case system on additional languages. Zamaraeva (2016) also infer morphotactics from IGT using k-means clustering. To the best of our knowledge, our work is the ﬁrst to propose a framework to extract ﬁrstpass grammatical agreement rules directly from raw text in a statistically-informed objective way. A parallel line of work (Hellan, 2010) extracts a construction proﬁle of a language by having templates that deﬁne how sentences are constructed.
7 Future Work
While we have demonstrated that our approach is effective in extracting a ﬁrst-pass set of agreement rules directly from raw text, it focuses only on agreement between a pair of words and hence might fail to capture more complex phenomena that require broader context or operate at the phrase level. Consider this simple English example: “John and Mary love their dog”. Under both UD and SUD formalisms, the coordinating conjunction “and" is a dependent, hence the verb will not agree with either of the (singular) nouns (“John" or “Mary"). Also, deciding agreement based on only POS tags is insufﬁcient to capture all phenomena that may inﬂuence agreement for e.g. mass nouns such as ‘rice’ do not follow the standard number agreement rules in English. We leave a more expressive model and evaluation on more languages as future work. We also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena.

Acknowledgments
The authors are grateful to the anonymous reviewers who took the time to provide many interesting comments that made the paper signiﬁcantly better, and to Josep Quer, Ekaterina Vylomova and Maria Ryskina, for participating in the human annotation experiments. This work is sponsored by the DARPA grant FA8750-18-2-0018 and by the National Science Foundation under grant 1761548.
References
Joseph Aoun, Elabbas Benmamoun, and Dominique Sportiche. 1994. Agreement, word order, and conjunction in some varieties of arabic. Linguistic inquiry, pages 195–220.
Emily M. Bender, Joshua Crowgey, Michael Wayne Goodman, and Fei Xia. 2014. Learning grammar speciﬁcations from IGT: A case study of chintang. In Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43–53, Baltimore, Maryland, USA. Association for Computational Linguistics.
Christian Bentz, Tatyana Ruzsics, Alexander Koplenig, and Tanja Samardžic´. 2016. A comparison between morphological complexity measures: Typological data vs. language corpora. In Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC), pages 142–153, Osaka, Japan. The COLING 2016 Organizing Committee.
Robert D Borsley and Ian Roberts. 2005. The syntax of the Celtic languages: a comparative perspective. Cambridge University Press.
Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. Classiﬁcation and regression trees. CRC press.
Keith Brown and Sarah Ogilvie. 2010. Concise encyclopedia of languages of the world. Elsevier.
Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108–122.
Robin Cohen. 1988. Book reviews: Reasoning and discourse processes. Computational Linguistics, 14(4).
Bernard Comrie. 1984. Reﬂections on verb agreement in hindi and related languages.

Greville G Corbett. 2009. Agreement. In Die slavischen Sprachen/The Slavic Languages.
Harald Cramér. 1946. Mathematical methods of statistics. Princeton U. Press, Princeton, page 500.
Dina B Crockett. 1976. Agreement in contemporary standard Russian. Slavica Publishers Inc.
Kim Gerdes, Bruno Guillaume, Sylvain Kahane, and Guy Perrier. 2018. SUD or surface-syntactic universal dependencies: An annotation scheme nearisomorphic to UD. In Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 66–74, Brussels, Belgium. Association for Computational Linguistics.
Kim Gerdes, Bruno Guillaume, Sylvain Kahane, and Guy Perrier. 2019. Improving surface-syntactic universal dependencies (SUD): MWEs and deep syntactic features. In Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019), pages 126–132, Paris, France. Association for Computational Linguistics.
Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff. 2012. Building large monolingual dictionaries at the leipzig corpora collection: From 100 to 200 languages. In LREC, volume 29, pages 31–43.
Zellig S Harris. 1951. Methods in structural linguistics.
Jean Hausser and Korbinian Strimmer. 2009. Entropy inference and the james-stein estimator, with application to nonlinear gene association networks. Journal of Machine Learning Research, 10(7).
Lars Hellan. 2010. From descriptive annotation to grammar speciﬁcation. In Proceedings of the Fourth Linguistic Annotation Workshop, pages 172–176, Uppsala, Sweden. Association for Computational Linguistics.
Kristen Howell, Emily M Bender, Michel Lockwood, Fei Xia, and Olga Zamaraeva. 2017. Inferring case systems from igt: Enriching the enrichment. In Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 67–75.
Rodney D Huddleston. 2002. The Cambridge grammar of the English language. Cambridge, UK; New York: Cambridge University Press.
Dan Kondratyuk and Milan Straka. 2019. 75 languages, 1 model: Parsing universal dependencies universally. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2779–2795, Hong Kong, China. Association for Computational Linguistics.
Joakim Nivre, Rogier Blokland, Niko Partanen, Michael Rießler, and Jack Rueter. 2018. Universal Dependencies 2.3.

Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. 2016. Universal dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 1659–1666.
Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4034–4043, Marseille, France. European Language Resources Association.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.
J. Ross Quinlan. 1986. Induction of decision trees. Machine learning, 1(1):81–106.
Gail M Sullivan and Richard Feinn. 2012. Using effect size—or why the p value is not enough. Journal of graduate medical education, 4(3):279–282.
Olga Zamaraeva. 2016. Inferring morphotactics from interlinear glossed text: Combining clustering and precision grammars. In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 141–150, Berlin, Germany. Association for Computational Linguistics.
Ran Zmigrod, Sabrina J. Mielke, Hanna Wallach, and Ryan Cotterell. 2019. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1651–1661, Florence, Italy. Association for Computational Linguistics.

A Appendix
A.1 Decision Tree Hyperparameters We perform a grid search over the following hyperparameters of the decision tree:

• criterion = [gini, entropy]

• max depth = [6,15] • min impurity decrease = 1e−3

The best parameters are selected based on the validation set performance. For some treebanks which have no validation set we use the default crossvalidation provided by sklearn (Buitinck et al., 2013). Average model runtime for a treebanks is 5-10mins depending on the size of the treebank.

A.2 Dataset Statistics
For the true low-resource experiments, the dataset details are in Table 2.

LANGUAGE
Breton-KEB Buryat-BXR Faroese-OFT Tagalog-TRG Welsh-CCG

TRAIN / TEST
30000 / 888 10000 / 908 50000/ 1208 30000 / 55 30000 / 956

Table 2: Dataset statistics. Training data is obtained by parsing the Liepzig corpora (Goldhahn et al., 2012) and test data is obtained from the respective treebank. Each cell denotes the number of sentences in train/test.

A.3 Evaluation
A.4 Annotation Interface for Expert Evaluation
In Figure 10, we show the annotation interface used for verifying Gender agreement rules in Catalan. For each triple, we display 10 randomly selected examples from the training portion of the treebank.
A.5 Low-resource Experiment Results
For the simulation experiments, the dataset details are in Table 3.
A.5.1 Udify (Kondratyuk and Straka, 2019) Model Details
We used the Udify model for automatically annotating the raw text with part-of-speech (POS), dependency links and morphological features. For each of the simulation experiment we report the udify parsing performance on the test data in

Table 4. We used the same hyperparameters for training with a related languages as speciﬁed by the authors.11. In the conﬁguration ﬁle, we only change the parameters warmup steps= 100 and start-step= 100, as recommended by the authors for low-resource languages.
A.5.2 Results and Discussion
For each language and feature, we plot the ARM score with and without transfer learning in Figure 12-14. Similar to our ﬁndings for Gender in Figure 5, we ﬁnd that cross-lingual transfer leads to a better score across all languages in the zeroshot setting. As we increase the number of goldstandard sentences, the quality of extracted rules improve. Although, for Belarusian we observe the opposite trend for Person agreement. On closer inspection we ﬁnd that it is because person applies only to non-past ﬁnite verb forms (VERB and AUX) as an inﬂectional feature and to pronouns (PRON) as a lexical feature which means that in many cases person is not explicitly marked, even though it implicitly exists 12.
A.6 Experiments with Gold-Standard Data
We present the ARM scores for all treebanks and features in Tables 5-11. We also report the validation results in the same tables for our best setting which uses the Statistical Threshold. In Section 2.2, we proposed using two types of thresholds for retaining the high probability agreement rules. In order to compare which threshold is the best for all treebanks, we manually inspect some of the learnt decision trees. We ﬁnd that for the trees learnt from the hard threshold often over-ﬁt on the training data causing to produce leaves with very few examples. In Figure 15 we compare the trees constructed for number agreement with the two thresholds for Marathi. One reason why Statistical-Threshold performs better for low-resource languages is because there are more leaves with fewer samples overall causing the Hard Threshold to have more false positives. Whereas the Statistical Threshold uses effect size with the signiﬁcance test which takes into account the sample size within a leaf leading to better leaves. Therefore, we choose to use StatisticalThreshold for all our simulation experiments.
In Figure 11, we report that (avg.) number of leaves in the decision trees grouped by language
11https://github.com/Hyperparticle/ udify
12https://universaldependencies.org/be/

Figure 10: Annotation interface for evaluating Gender agreement in Catalan.

LANGUAGE
Spanish-GSD Greek-GDT Belarusian-HSE Lithuanian-ALKSNIS

TRAIN/DEV/TEST
14187 / 1400/ 426 1662 / 403 / 456 319 / 65/ 253 2341 / 617 / 684

TRANSFER LANGUAGE
Portuguese-Bosque Ancient Greek-PROIEL Ukrainian-IU Latvian-LVTB

Table 3: Dataset statistics. Train/Dev/Test denote the number of sentences in the respective treebank used for the target language.

family. Overall, Gender and Case tend to have more complex trees. For Case, it is probably because languages have more number of cases making it harder for the decision tree to model them.
A.7 SUD treebanks
Figure 16 presents a comparison of UD and SUDstyle trees for the German sentence, “Ich werde lange Bücher lesen.". The SUD tree has the function word ‘werde’ as the syntactic head to the content word ‘lesen’.

LANGUAGE Greek
Spanish
Belarusian Lithuanian

#TRAINING
0 50 100 500
0 50 100 500
0 50 100
0 50 100 500

W/O TRANSFER

SETTING
+TRANSFER

upos:0.507, ufeats:0.330, uas:0.309, las:0.203 upos:0.915, ufeats:0.664, uas: 0.755, las: 0.691 upos: 0.970, ufeats: 0.891, uas: 0.891, las: 0.866

upos:0.661, ufeats:0.392, uas:0.632, las :0.465 upos:0.877, ufeats:0.631, uas:0.724, las:0.653 upos: 0.906, ufeats: 0.719, uas: 0.758, las: 0.703 upos: 0.954, ufeats: 0.860, uas: 0.849, las: 0.817

upos: 0.529, ufeats: 0.463, uas: 0.289, las: 0.152 upos: 0.920, ufeats: 0.832, uas: 0.755, las: 0.690 upos: 0.952, ufeats: 0.919, uas: 0.860, las: 0.820

upos: 0.922, ufeats: 0.764, uas: 0.855, las: 0.776 upos: 0.913, ufeats: 0.792, , uas: 0.844, las: 0.767 upos: 0.916, ufeats: 0.840, uas: 0.849, las: 0.784 upos: 0.949, ufeats: 0.889, uas: 0.859, las: 0.822

upos: 0.570, ufeats: 0.323, uas: 0.217, las: 0.141 upos: 0.919, ufeats: 0.446, uas: 0.521, las: 0.482

upos: 0.941, ufeats: 0.520, uas: 0.863, las: 0.797 upos: 0.952, ufeats: 0.726, uas: 0.763, las: 0.727 upos: 0.961, ufeats: 0.777, uas: 0.854, las: 0.800

upos: 0.566, ufeats: 0.371, uas: 0.346, las: 0.211 upos: 0.813, ufeats: 0.453, uas: 0.551, las: 0.421 upos: 0.925, ufeats: 0.744, uas: 0.757, las: 0.697

upos: 0.869, ufeats: 0.528, uas: 0.752, las: 0.610 upos: 0.874, ufeats: 0.5841, uas: 0.757, las: 0.623 upos: 0.883, ufeats: 0.637, uas: 0.761, las: 0.659 upos: 0.912, ufeats: 0.747, uas: 0.779, las: 0.714

Table 4: udify model performance on the test data for each low-resource setting. The scores are averaged across ﬁve runs of each setting.

Avg. number of unique leaves

gender person

14

12

10

8

6

4

2

0

aryan ance slavic

indo

rom

number tense

anic germ

westsla…

case baltic

mood celtic

Figure 11: (Avg.) number of leaves for each feature grouped by language family.

ARM

0.9

0.8

0.7

0.591

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

Greek 0.644

Greek (Ancient Greek )

0.78 0.674

0.702 0.639

0.434

50

100

500

0.638 0.638
A LL

ARM

0.8 0.672
0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

Spanish 0.7

Spanish (Portuguese)

0.663 0.699

0.685 0.67

0.492

50

100

500

0.718 0.718
A LL

(a)

Bela rusia n

0.7

0.639

0.634

0.6

Belarusian (Ukrainian)

0.625 0.648

0.595 0.595

0.5

0.577

0.4

0.3

0.2

0.1

0

0

0

50

100

500

0.595 0.595
A LL

ARM

(b)

0.8 0.663
0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

Lithua nia n 0.687 0.73

Lithuanian (Latvian)

0.627 0.65

0.661 0.634

50

100

500

0.711 0.711
A LL

ARM

(c)

(d)

Figure 12: Comparing the (avg.) ARM score for Gender agreement with and without cross-lingual transfer learning (transfer language in parenthesis). Note: the higher the ARM the better.

ARM

Greek

Greek (Ancient Greek)

0.8

0.7 0.579 0.6
0.5 0.4

0.535 0.51

0.479 0.519

0.592 0.56

0.3

0.2

0.1

0

0

0

50

100

500

0.667 0.667
ALL

ARM

Spanish

Spanish (Portuguese)

1

0.9

0.8

0.7

0.6

0.5

0.591

0.4

0.3

0.2

0.1

0

0

0

0.732 0.542

0.5490.542

0.552 0.533

50

100

500

0.591 0.591
ALL

(a)

(b)

ARM

Belarusian 0.6

0.5
0.4 0.4
0.3

0.433

0.2

0.1

0

0

0

0

50

Belarusian (Ukrainian)

0.5

0.56

0.5 0.5

0.458

0.5

100

500

ALL

ARM

Lithuanian

Lithuanian (Latvian)

0.9

0.8

0.7

0.6

0.526

0.5

0.819 0.615

0.579 0.562

0.509.6799

0.667 0.667

0.4

0.3

0.2

0.1

0

0

0

50

100

500

ALL

(c)

(d)

Figure 13: Comparing the (avg.) ARM score for Person agreement with and without cross-lingual transfer learning (transfer language in parenthesis). Note: the higher the ARM the better.

ARM ARM
ARM

0.841

Greek

0.9

0.791

0.8

Greek (Ancient Greek)

0.847

0.849

0.848

0.7

0.806

0.725

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

50

100

500

0.808 0.808
ALL

Belarusian

0.8

0.68

0.7

0.625

0.6

0.625

0.5

0.4

0.3

0.2

0.1

0

0

0

50

Belarusian (Ukrainian) 0.725 0.725
0.555 0.558

0.725 0.725

100

500

ALL

(a)

(b)

Lithuanian

Lithuanian (Latvian)

0.9

0.761 0.8

0.795 0.799

0.7

0.57

0.6

0.674

0.754 0.568

0.5

0.4

0.3

0.2

0.1

0

0

0

50

100

500

0.826 0.826
ALL

(c)

Figure 14: Comparing the (avg.) ARM score for Case agreement with and without cross-lingual transfer learning (transfer language in parenthesis). Note: the higher the ARM the better. For Spanish, there was < 10 data points with Case annotated hence we do not report results for it.

node - 0 samples = 1142 value = [143, 999]

relation in udep,comp:pred

relation in mod@poss,discourse,subj@pass,compound@redup,ﬂat,conj,mod compound@lvc,compound,mod@relcl,comp:obl,vocative,comp:aux,orphan
arpeplaotsio,undienps@ubpjoss,dislocated,det,parataxis,comp:obj,comp:aux@pass

node - 13 samples = 366 value = [30, 336]

node - 3 samples = 650 value = [113, 537]

Leaf- 3 relation = udep,comp:pred
head-pos = *
child-pos = *

samples=126 value = [0,126] class = agreement

child-pos in aux,adj,pron,det,propn,verb,noun
Leaf- 4 relation = subj
head-pos = *
child-pos = det,propn,verb,aux,adj noun,pron

child-pos in adp
node - 15 samples = 2 value = [1, 1]

child-pos in noun,pron

child-pos in aux,adj,det,propn,verb,adp

Leaf- 2 relation = mod@poss,discourse,subj@pass,ﬂat,compound@redup
conj,mod,compound@lvc,compound,mod@relcl comp:obl,vocative,comp:aux,orphan,appos
udep@poss,dislocated,det,parataxis,comp:obj comp:aux@pass

node - 5 samples = 287 value = [31, 256]

head-pos = * child-pos = noun,pron

samples=364 value = [29,335] class = agreement

samples=363 value = [82,281] class = chance-agreement

head-pos in aux,adj,pron,det,propn,adp,noun
Leaf- 5 relation = subj
head-pos = det,propn,aux,adj,adp noun,pron
child-pos = adp
samples=1 value = [0,1] class = agreement

relation in mod@relcl,mod@poss,subj@pass,comp:obl,ﬂat,compound@redup,vocative appos,conj,comp:aux,compound@lvc,udep@poss,dislocated,mod orphan,det,discourse,comp:obj,compound,comp:aux@pass

head-pos in verb
Leaf- 6 relation = subj

Leaf- 0 relation = mod@relcl,mod@poss,comp:obl,vocative,comp:aux
orphan,discourse,subj@pass,ﬂat,compound@redup appos,conj,udep@poss,dislocated,mod
det,compound@lvc,comp:obj,compound,comp:aux@pass

head-pos = verb

head-pos = *

child-pos = adp

child-pos = det,propn,verb,aux,adj adp

samples=1 value = [1,0] class = chance-agreement

samples=235 value = [19,216] class = agreement

relation in parataxis
Leaf- 1 relation = parataxis
head-pos = *
child-pos = det,propn,verb,aux,adj adp
samples=52 value = [12,40] class = chance-agreement

(a)

relation in comp:pred,udep
Leaf- 1 relation = comp:pred,udep
head-pos = *
child-pos = *
samples=126 value = [0,126] class = agreement

node - 0

samples = 1142

relation in compound@redup,vocative,comp:obl,parataxis comp:aux,comp:aux@pass

value = [143, 999]

orphan,appos,dislocated

mod@relcl,ﬂat

,subj

subj@pass,discourse,comp:obj,compound,mod

compound@lvc,mod@poss,udep@poss,det,conj

Leaf- 0 relation = compound@redup,orphan,dislocated,comp:obj,discourse
subj@pass,mod@relcl,mod,compound@lvc,mod@poss udep@poss,vocative,comp:obl,comp:aux@pass,parataxis
appos,subj,comp:aux,compound,ﬂat det,conj

head-pos = ,det,adj,verb,noun propn,aux,pron,adp

child-pos = det,adj,verb,noun,propn aux,pron,adp

samples=1016 value = [143,873] class = chance-agreement

(b)
Figure 15: Comparing the learnt trees for Number agreement extracted using (a) Hard Threshold and (b) Statistical Threshold. Hard Threshold overﬁts on the training data resulting in leaves with very few samples.

(a)

(b)

Figure 16: Comparing the UD (a) tree with the SUD (b) tree for the German sentence “Ich werde lange Bücher lesen.".

TREEBANK
ru-gsd ru-gsd ru-gsd ru-gsd ru-gsd ru-gsd id-gsd it-isdt it-isdt it-isdt it-isdt it-isdt la-proiel la-proiel la-proiel la-proiel la-proiel la-proiel ro-nonstandard ro-nonstandard ro-nonstandard ro-nonstandard ro-nonstandard ro-nonstandard he-htb he-htb he-htb he-htb he-htb no-bokmaal no-bokmaal no-bokmaal no-bokmaal no-bokmaal no-bokmaal no-nynorsk no-nynorsk no-nynorsk no-nynorsk no-nynorsk ﬁ-tdt ﬁ-tdt ﬁ-tdt ﬁ-tdt ﬁ-tdt pl-lfg pl-lfg pl-lfg pl-lfg pl-lfg pl-lfg grc-perseus grc-perseus grc-perseus grc-perseus grc-perseus grc-perseus ﬁ-ftb ﬁ-ftb ﬁ-ftb ﬁ-ftb ﬁ-ftb wo-wtb wo-wtb wo-wtb wo-wtb wo-wtb en-partut en-partut en-partut en-partut

FEATURE
Gender Person Number Tense Mood Case Number Gender Person Number Tense Mood Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Person Number Tense Mood Case Gender Person Number Tense Mood Person Number Tense Mood

STATISTICAL
0.678 0.125 0.628 0.667 0.0 0.649 0.047 0.816 0.304 0.615 0.765 0.25 0.538 0.56 0.648 0.818 0.6 0.759 0.64 0.636 0.626 0.452 0.676 0.694 0.747 0.737 0.585 0.3 0.5 0.477 1.0 0.655 0.55 0.0 0.0 0.464 0.0 0.702 0.368 0.0 0.387 0.502 0.474 0.75 0.786 0.646 0.688 0.691 0.556 0.333 0.744 0.62 0.8 0.531 0.889 0.833 0.708 0.56 0.524 0.846 0.429 0.724 0.5 0.55 0.486 0.5 0.143 0.5 0.559 0.667 0.091

HARD
0.875 0.512 0.667 1.0 0.614 0.961 0.816 0.87 0.603 0.765 0.75 0.568 0.6 0.574 0.879 0.52 0.782 0.57 0.606 0.626 0.839 0.765 0.702 0.747 0.789 0.585 0.3 0.5 0.568 1.0 0.673 0.55 1.0 0.333 0.536 0.0 0.702 0.368 1.0 0.677 0.493 0.368 0.75 0.828 0.646 0.688 0.68 0.667 0.667 0.667 0.718 0.8 0.63 1.0 0.833 0.792 0.76 0.441 0.769 0.5 0.848 0.5 0.45 0.6 0.625 0.143 0.5 0.559 0.733 0.818

BASELINE
0.51 0.125 0.384 0.667 0.0 0.395 0.047 0.289 0.304 0.41 0.647 0.25 0.636 0.54 0.452 0.879 0.44 0.466 0.407 0.606 0.586 0.645 0.676 0.636 0.663 0.789 0.415 0.1 0.0 0.545 0.5 0.364 0.55 0.0 0.0 0.536 0.0 0.511 0.684 0.0 0.677 0.511 0.474 0.75 0.821 0.463 0.562 0.412 0.667 0.333 0.41 0.563 0.7 0.605 1.0 0.667 0.556 0.6 0.524 0.308 0.429 0.781 0.5 0.4 0.6 0.375 0.143 0.417 0.441 0.667 0.091

DEV
0.623 0.286 0.62 0.571 0.1 0.537 0.045 0.738 0.619 0.588 0.611 0.273 0.496 0.653 0.553 0.824 0.667 0.691 0.75 0.683 0.693 0.467 0.4 0.704 0.629 0.769 0.505 0.545 0.5 0.675 1.0 0.733 0.55 0.1 0.0 0.514 0.667 0.596 0.429 0.048 0.607 0.559 0.5 0.471 0.781 0.641 0.714 0.624 0.6 0.4 0.617 0.699 0.636 0.537 0.778 0.429 0.712 0.63 0.54 0.538 0.529 0.748 0.0 0.609 0.632 0.625 0.364 0.857 0.676 0.583 0.1

Table 5: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
fr-ftb fr-ftb fr-ftb fr-ftb fr-ftb lv-lvtb lv-lvtb lv-lvtb lv-lvtb lv-lvtb lv-lvtb ro-rrt ro-rrt ro-rrt ro-rrt ro-rrt ro-rrt it-vit it-vit it-vit it-vit it-vit fr-partut fr-partut fr-partut fr-partut fr-partut en-ewt en-ewt en-ewt en-ewt ru-syntagrus ru-syntagrus ru-syntagrus ru-syntagrus ru-syntagrus ru-syntagrus sv-talbanken sv-talbanken sv-talbanken sv-talbanken sv-talbanken olo-kkpp olo-kkpp olo-kkpp olo-kkpp olo-kkpp cs-cac cs-cac cs-cac cs-cac cs-cac cs-cac ur-udtb ur-udtb ur-udtb ur-udtb ur-udtb ur-udtb et-ewt et-ewt et-ewt et-ewt et-ewt fro-srcmf es-gsd es-gsd es-gsd es-gsd es-gsd es-gsd

FEATURE
Gender Person Number Tense Mood Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Gender Person Number Tense Mood Person Number Tense Mood Gender Person Number Tense Mood Case Gender Number Tense Mood Case Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Person Number Tense Mood Case Tense Gender Person Number Tense Mood Case

STATISTICAL
0.631 0.14 0.635 0.714 0.409 0.727 0.5 0.688 0.667 0.476 0.719 0.583 0.327 0.535 0.421 0.931 0.862 0.672 0.625 0.712 0.773 0.4 0.579 0.818 0.771 0.857 0.333 0.812 0.357 0.591 0.4 0.697 0.625 0.591 0.727 0.4 0.649 0.719 0.659 0.559 0.048 0.189 0.286 0.667 0.75 0.0 0.7 0.663 0.562 0.636 0.467 0.2 0.81 0.567 0.152 0.485 0.333 0.714 0.685 0.609 0.551 0.409 0.533 0.7 0.5 0.718 0.591 0.644 0.529 0.533 0.0

HARD
0.631 0.86 0.635 0.857 0.591 0.734 0.632 0.688 0.815 0.619 0.734 0.583 0.755 0.585 0.684 1.0 0.788 0.672 0.625 0.728 0.955 0.6 0.632 0.727 0.542 0.857 0.667 0.812 0.643 0.773 0.733 0.747 0.667 0.661 0.818 0.8 0.707 0.719 0.634 0.588 0.952 0.623 0.571 0.692 0.75 0.75 0.7 0.673 0.562 0.531 0.667 0.4 0.84 0.567 0.946 0.583 0.5 0.714 0.696 0.696 0.551 0.682 0.4 0.754 1.0 0.718 0.545 0.644 0.824 0.467 1.0

BASELINE
0.477 0.14 0.502 0.857 0.409 0.461 0.579 0.429 0.889 0.476 0.489 0.51 0.347 0.528 0.789 0.448 0.588 0.375 0.792 0.528 0.955 0.4 0.421 0.273 0.292 0.714 0.333 0.25 0.357 0.773 0.4 0.624 0.667 0.562 0.818 0.44 0.575 0.438 0.463 0.5 0.048 0.189 0.286 0.667 0.75 0.0 0.7 0.602 0.5 0.469 0.6 0.2 0.46 0.536 0.065 0.485 0.5 0.143 0.696 0.609 0.48 0.636 0.533 0.657 0.5 0.366 0.591 0.424 0.824 0.533 0.0

DEV
0.621 0.171 0.634 0.833 0.6 0.677 0.583 0.706 0.741 0.333 0.772 0.591 0.304 0.56 0.526 0.867 0.854 0.678 0.667 0.61 0.75 0.231 0.615 0.75 0.542 0.6 0.167 0.85 0.304 0.593 0.333 0.673 0.72 0.576 0.667 0.407 0.681 0.643 0.571 0.607 0.056 0.143 0.678 0.583 0.575 0.333 0.111 0.833 0.576 0.195 0.496 0.667 0.714 0.7 1.0 0.736 0.355 0.567 0.409 0.474 0.0

Table 6: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
sl-ssj sl-ssj sl-ssj sl-ssj sl-ssj sl-ssj cs-pdt cs-pdt cs-pdt cs-pdt cs-pdt cs-pdt hsb-ufal hsb-ufal hsb-ufal hsb-ufal ga-idt ga-idt ga-idt ga-idt ga-idt ga-idt gl-treegal gl-treegal gl-treegal gl-treegal gl-treegal fa-seraji fa-seraji fa-seraji fa-seraji et-edt et-edt et-edt et-edt et-edt la-perseus la-perseus la-perseus la-perseus la-perseus la-perseus ug-udt ug-udt ug-udt ug-udt ug-udt es-ancora es-ancora es-ancora es-ancora es-ancora de-hdt de-hdt de-hdt de-hdt de-hdt de-hdt kk-ktb kk-ktb kk-ktb de-gsd de-gsd de-gsd de-gsd de-gsd de-gsd nl-alpino nl-alpino nl-alpino af-afribooms af-afribooms af-afribooms

FEATURE
Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Number Tense Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Person Number Tense Mood Person Number Tense Mood Case Gender Person Number Tense Mood Case Person Number Tense Mood Case Gender Person Number Tense Mood Gender Person Number Tense Mood Case Person Number Mood Gender Person Number Tense Mood Case Gender Number Tense Number Tense Case

STATISTICAL
0.818 0.667 0.683 0.333 0.5 0.607 0.564 0.591 0.477 0.667 0.538 0.646 0.857 0.692 0.667 1.0 0.64 0.625 0.468 0.714 0.833 0.69 0.722 0.522 0.68 0.462 0.462 0.667 0.514 0.455 0.333 0.613 0.648 0.579 0.524 0.565 0.692 0.5 0.544 0.75 0.667 0.717 0.526 0.767 0.625 0.692 0.683 0.754 0.429 0.664 0.625 0.652 0.541 0.071 0.561 0.8 0.0 0.738 0.636 0.538 1.0 0.699 0.567 0.638 0.455 0.5 0.55 0.667 0.548 0.562 0.6 0.842 0.0

HARD
0.8 0.722 0.683 0.583 0.75 0.721 0.788 0.705 0.64 0.786 0.538 0.675 0.714 0.538 0.667 0.846 0.76 0.875 0.571 0.571 0.833 0.724 0.685 0.565 0.546 0.538 0.692 0.667 0.514 0.545 0.667 0.613 0.644 0.632 0.571 0.756 0.585 0.667 0.662 1.0 0.667 0.66 0.526 0.6 0.75 0.923 0.683 0.754 0.429 0.664 0.833 0.348 0.607 0.929 0.595 0.88 1.0 0.836 0.545 0.615 1.0 0.781 0.433 0.638 0.636 0.455 0.588 0.8 0.548 0.5 0.667 0.842 1.0

BASELINE
0.527 0.722 0.564 0.333 0.5 0.557 0.75 0.614 0.629 0.786 0.538 0.545 0.786 0.692 0.667 0.462 0.78 0.5 0.468 0.429 0.667 0.724 0.333 0.522 0.361 0.692 0.462 0.381 0.514 0.636 0.333 0.71 0.539 0.763 0.571 0.786 0.538 0.833 0.603 1.0 0.833 0.528 0.579 0.533 0.5 0.769 0.683 0.431 0.429 0.539 0.833 0.652 0.607 0.071 0.59 0.88 0.0 0.574 0.636 0.538 0.6 0.397 0.567 0.35 0.591 0.455 0.362 0.8 0.565 0.375 0.533 0.842 0.0

DEV
0.772 0.706 0.712 0.364 0.667 0.61 0.58 0.541 0.481 0.658 0.48 0.633 0.647 1.0 0.446 0.5 0.714 0.667 0.842 0.556 0.545 0.0 0.714 0.676 0.537 0.667 0.614 0.611 0.697 0.778 0.833 0.671 0.759 0.526 0.651 0.63 0.5 0.603 0.085 0.533 0.692 0.077 0.7 0.641 0.667 0.619 0.526 0.421 0.603 0.562 0.625 0.529 0.667 0.588 0.0

Table 7: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
uk-iu uk-iu uk-iu uk-iu uk-iu uk-iu cs-cltt cs-cltt cs-cltt cs-cltt cs-cltt cop-scriptorium cop-scriptorium ru-taiga ru-taiga ru-taiga ru-taiga ru-taiga ru-taiga hu-szeged hu-szeged hu-szeged hu-szeged sr-set sr-set sr-set sr-set sr-set sr-set en-lines en-lines en-lines en-lines en-lines sk-snk sk-snk sk-snk sk-snk sk-snk sk-snk pl-pdb pl-pdb pl-pdb pl-pdb pl-pdb pl-pdb la-ittb la-ittb la-ittb la-ittb la-ittb la-ittb

FEATURE
Gender Person Number Tense Mood Case Gender Number Tense Mood Case Gender Number Gender Person Number Tense Mood Case Person Number Tense Mood Gender Person Number Tense Mood Case Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case

STATISTICAL
0.701 0.7 0.647 0.476 0.318 0.741 0.857 0.646 0.167 0.0 0.697 0.714 0.4 0.648 0.667 0.662 0.538 0.611 0.557 0.444 0.396 0.6 0.714 0.803 0.35 0.64 0.474 0.286 0.704 0.625 0.319 0.704 0.211 0.778 0.692 0.778 0.558 0.667 1.0 0.731 0.645 0.556 0.637 0.5 0.25 0.72 0.735 0.19 0.579 0.5 0.476 0.757

HARD
0.693 0.5 0.659 0.476 0.409 0.741 0.929 0.688 0.5 1.0 0.758 0.857 0.6 0.724 0.75 0.601 0.615 0.667 0.696 0.556 0.64 0.8 0.714 0.817 0.75 0.64 0.684 0.714 0.765 0.688 0.783 0.778 0.789 0.778 0.776 0.333 0.558 0.556 1.0 0.756 0.779 0.778 0.613 0.6 0.75 0.748 0.725 0.81 0.579 0.6 0.476 0.796

BASELINE
0.559 0.35 0.479 0.571 0.318 0.504 0.75 0.479 0.167 0.0 0.636 0.143 0.2 0.638 0.583 0.459 0.615 0.611 0.633 0.444 0.396 0.8 0.714 0.479 0.35 0.509 0.684 0.286 0.531 0.562 0.319 0.704 0.211 0.444 0.533 0.222 0.5 0.444 0.25 0.526 0.529 0.704 0.481 0.7 0.25 0.514 0.48 0.19 0.386 0.6 0.571 0.495

DEV
0.771 0.9 0.656 0.615 0.357 0.732 0.806 0.576 0.143 0.0 0.658 0.8 0.714 0.667 0.786 0.646 0.583 0.5 0.593 0.138 0.434 0.769 0.5 0.622 0.4 0.615 0.444 0.2 0.651 0.789 0.325 0.636 0.207 0.833 0.638 0.625 0.571 0.8 0.857 0.833 0.661 0.72 0.644 0.6 0.05 0.679 0.805 0.273 0.562 0.414 0.591 0.792

Table 8: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
da-ddt da-ddt da-ddt da-ddt it-postwita it-postwita it-postwita it-postwita it-postwita eu-bdt eu-bdt eu-bdt sl-sst sl-sst sl-sst sl-sst sl-sst sl-sst be-hse be-hse be-hse be-hse be-hse be-hse fr-sequoia fr-sequoia fr-sequoia fr-sequoia fr-sequoia sme-giella sme-giella sme-giella sme-giella el-gdt el-gdt el-gdt el-gdt el-gdt el-gdt orv-torot orv-torot orv-torot orv-torot orv-torot orv-torot sv-lines sv-lines sv-lines sv-lines sv-lines ta-ttb ta-ttb ta-ttb ta-ttb ta-ttb ta-ttb it-partut it-partut it-partut it-partut it-partut ar-padt ar-padt ar-padt ar-padt ar-padt bg-btb bg-btb bg-btb bg-btb bg-btb

FEATURE
Gender Number Tense Mood Gender Person Number Tense Mood Number Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Gender Person Number Mood Case Gender Person Number Tense Mood

STATISTICAL
0.818 0.667 0.737 0.2 0.702 0.595 0.744 0.481 0.792 0.508 0.421 0.795 0.724 0.688 0.678 0.48 0.56 0.615 0.596 0.5 0.646 0.429 0.286 0.725 0.8 0.667 0.56 0.529 0.286 0.653 0.455 0.214 0.741 0.638 0.667 0.627 0.6 0.0 0.809 0.655 0.6 0.621 0.731 0.316 0.709 0.538 0.643 0.429 0.231 0.583 0.682 0.091 0.523 0.625 0.5 0.846 0.786 0.833 0.714 0.9 0.2 0.592 0.0 0.512 0.571 0.871 0.638 0.625 0.639 0.6 0.056

HARD
0.818 0.667 0.737 0.8 0.702 0.676 0.744 0.704 0.792 0.6 0.737 0.803 0.618 0.812 0.672 0.76 0.6 0.637 0.553 0.5 0.646 0.429 0.286 0.55 0.771 0.667 0.62 0.765 0.714 0.653 0.545 0.571 0.704 0.745 0.667 0.7 1.0 1.0 0.809 0.669 0.6 0.621 0.731 0.789 0.775 0.538 0.643 0.476 0.769 0.583 0.682 0.955 0.591 0.5 1.0 0.846 0.786 0.917 0.508 0.9 0.4 0.592 0.833 0.643 0.571 0.871 0.66 0.625 0.631 0.6 0.944

BASELINE
0.364 0.286 0.842 0.2 0.362 0.73 0.558 0.704 0.792 0.415 0.421 0.726 0.513 0.719 0.483 0.48 0.56 0.549 0.404 0.0 0.431 0.571 0.286 0.45 0.371 0.4 0.45 0.765 0.286 0.561 0.455 0.214 0.667 0.447 0.458 0.427 1.0 0.0 0.319 0.547 0.6 0.581 0.769 0.421 0.609 0.308 0.452 0.429 0.231 0.25 0.659 0.091 0.545 0.625 0.5 0.692 0.25 0.25 0.286 0.6 0.2 0.549 0.0 0.512 0.571 0.753 0.404 0.625 0.533 0.6 0.056

DEV
0.889 0.725 0.562 0.077 0.674 0.595 0.642 0.607 0.556 0.473 0.529 0.776 0.692 0.75 0.596 0.333 0.2 0.733 0.647 0.857 0.68 0.684 0.077 0.744 0.667 0.615 0.462 0.0 0.814 0.679 0.594 0.618 0.72 0.176 0.691 0.64 0.529 0.655 0.161 0.51 0.5 0.167 0.533 0.667 0.5 1.0 0.846 0.615 0.576 0.583 0.167 0.712 0.263 0.593 0.6 0.824 0.585 0.625 0.679 0.579 0.176

Table 9: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
pt-bosque pt-bosque pt-bosque pt-bosque pt-bosque lt-alksnis lt-alksnis lt-alksnis lt-alksnis lt-alksnis lt-alksnis ar-nyuad ar-nyuad ar-nyuad ar-nyuad ar-nyuad ca-ancora ca-ancora ca-ancora ca-ancora ca-ancora grc-proiel grc-proiel grc-proiel grc-proiel grc-proiel grc-proiel it-twittiro it-twittiro it-twittiro it-twittiro it-twittiro mr-ufal mr-ufal mr-ufal mr-ufal tr-imst tr-imst tr-imst tr-imst tr-imst bxr-bdt hi-hdtb hi-hdtb hi-hdtb hi-hdtb hi-hdtb hi-hdtb hr-set hr-set hr-set hr-set hr-set hr-set kmr-mg kmr-mg kmr-mg nl-lassysmall nl-lassysmall nl-lassysmall fr-gsd fr-gsd fr-gsd fr-gsd fr-gsd

FEATURE
Gender Person Number Tense Mood Gender Person Number Tense Mood Case Gender Person Number Mood Case Gender Person Number Tense Mood Gender Person Number Tense Mood Case Gender Person Number Tense Mood Gender Person Number Case Person Number Tense Mood Case Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Number Case Gender Number Tense Gender Person Number Tense Mood

STATISTICAL
0.656 0.25 0.669 0.5 0.375 0.711 0.667 0.625 0.667 0.667 0.826 0.606 0.469 0.502 0.438 0.627 0.804 0.389 0.652 0.5 0.32 0.605 0.543 0.533 0.643 0.529 0.809 0.808 0.591 0.568 0.25 0.5 0.609 0.727 0.394 0.583 0.359 0.47 0.762 0.714 0.717 0.818 0.586 0.045 0.416 0.333 1.0 0.654 0.725 0.769 0.675 0.429 0.412 0.669 1.0 0.783 0.909 0.85 0.646 0.6 0.727 0.375 0.624 0.706 0.273

HARD
0.721 0.75 0.669 0.438 0.5 0.711 0.8 0.625 0.667 0.333 0.826 0.718 0.562 0.554 0.562 0.747 0.786 0.611 0.652 0.731 0.68 0.516 0.543 0.61 0.786 0.529 0.854 0.808 0.318 0.568 0.75 0.5 0.652 0.727 0.794 0.583 0.818 0.536 0.762 0.714 0.804 0.545 0.617 0.955 0.615 0.333 1.0 0.709 0.717 0.769 0.675 0.714 0.588 0.725 0.818 0.739 0.727 0.85 0.646 0.6 0.727 0.719 0.624 0.706 0.727

BASELINE
0.41 0.25 0.378 0.438 0.375 0.667 0.531 0.667 0.667 0.496 0.718 0.469 0.502 0.438 0.747 0.464 0.389 0.511 0.692 0.32 0.535 0.6 0.538 0.786 0.529 0.51 0.385 0.682 0.419 0.75 0.5 0.565 0.364 0.242 0.417 0.359 0.47 0.81 0.714 0.804 0.818 0.5 0.045 0.416 0.333 0.333 0.63 0.525 0.577 0.51 0.714 0.412 0.577 1.0 0.783 0.909 0.9 0.523 0.4 0.485 0.375 0.441 0.765 0.273

DEV
0.792 0.455 0.698 0.692 0.429 0.671 0.571 0.595 0.6 0.375 0.798 0.536 0.343 0.468 0.5 0.551 0.77 0.219 0.616 0.56 0.348 0.588 0.737 0.585 0.774 0.65 0.813 0.65 0.579 0.634 0.462 0.364 0.52 0.889 0.514 0.857 0.342 0.485 0.68 0.68 0.678 0.631 0.052 0.455 0.2 0.667 0.62 0.643 0.692 0.658 0.542 0.158 0.659 0.81 0.646 0.364 0.807 0.312 0.593 0.81 0.25

Table 10: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

TREEBANK
got-proiel got-proiel got-proiel got-proiel got-proiel got-proiel en-gum en-gum en-gum en-gum lzh-kyoto lzh-kyoto cs-ﬁctree cs-ﬁctree cs-ﬁctree cs-ﬁctree cs-ﬁctree cs-ﬁctree hy-armtdp hy-armtdp hy-armtdp hy-armtdp hy-armtdp gd-arcosg gd-arcosg gd-arcosg gd-arcosg gd-arcosg gd-arcosg lt-hse lt-hse lt-hse lt-hse lt-hse lt-hse no-nynorsklia no-nynorsklia no-nynorsklia no-nynorsklia no-nynorsklia no-nynorsklia cu-proiel cu-proiel cu-proiel cu-proiel cu-proiel cu-proiel

FEATURE
Gender Person Number Tense Mood Case Person Number Tense Mood Mood Case Gender Person Number Tense Mood Case Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case Gender Person Number Tense Mood Case

STATISTICAL 0.559 0.571 0.64 0.714 0.722 0.82 0.167 0.397 0.579 0.176 0.0 0.0 0.717 0.667 0.649 0.833 0.455 0.697 0.444 0.592 0.824 0.789 0.857 0.615 0.6 0.562 0.833 0.667 0.85 0.658 0.778 0.642 0.714 0.2 0.564 0.727 1.0 0.743 0.435 0.0 0.5 0.61 0.667 0.672 0.567 0.348 0.818

HARD 0.595 0.771 0.68 0.714 0.722 0.784 0.917 0.767 0.684 0.824 1.0 1.0 0.683 0.905 0.649 0.889 0.455 0.652 0.593 0.612 0.765 0.789 0.857 0.615 0.8 0.562 0.333 0.667 0.85 0.553 0.444 0.597 0.857 0.6 0.615 0.697 1.0 0.743 0.826 1.0 1.0 0.66 0.667 0.579 0.533 0.652 0.818

BASELINE 0.559 0.657 0.503 0.714 0.611 0.505 0.167 0.397 0.579 0.176 0.0 0.0 0.4 0.81 0.364 0.778 0.545 0.461 0.593 0.561 0.529 0.737 0.821 0.615 0.6 0.562 0.5 0.333 0.5 0.474 0.444 0.478 0.857 0.2 0.641 0.455 0.0 0.343 0.783 0.0 0.5 0.54 0.528 0.503 0.6 0.348 0.473

DEV 0.658 0.614 0.591 0.586 0.682 0.803 0.176 0.259 0.625 0.05 0.0 0.125 0.691 0.625 0.673 0.565 0.643 0.738 0.692 0.676 0.733 0.8 0.772 0.609 0.75 0.588 0.8 0.714 0.833 0.6 0.8 0.667 0.889 0.429 0.816 0.667 1.0 0.649 0.435 0.043 0.0 0.706 0.579 0.641 0.655 0.364 0.793

Table 11: Comparing the ARM scores for SUD treebanks across both Statistical and Hard thresholding.

