SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy
Shuai Guo1∗, Jiatong Shi2∗, Tao Qian1, Shinji Watanabe2, Qin Jin1
1School of Information, Renmin University of China, P.R.China 2Language Technologies Institute, Carnegie Mellon University, U.S.A.
{shuaiguo, qiantao, qjin}@ruc.edu.cn, jiatongs@cs.cmu.edu, shinjiw@ieee.org

arXiv:2203.17001v1 [eess.AS] 31 Mar 2022

Abstract
Deep learning based singing voice synthesis (SVS) systems have been demonstrated to ﬂexibly generate singing with better qualities, compared to conventional statistical parametric based methods. However, neural systems are generally datahungry and have difﬁculty to reach reasonable singing quality with limited public available training data. In this work, we explore different data augmentation methods to boost the training of SVS systems, including several strategies customized to SVS based on pitch augmentation and mix-up augmentation. To further stabilize the training, we introduce the cycle-consistent training strategy. Extensive experiments on two public singing databases demonstrate that our proposed augmentation methods and the stabilizing training strategy can signiﬁcantly improve the performance on both objective and subjective evaluations. Index Terms: singing voice synthesis, data augmentation, cycle-consistent training strategy
1. Introduction
In recent years, singing voice synthesis (SVS) has attracted much attention in both academic and industrial ﬁelds. The task takes music score and lyrics as input and generates natural singing voices. Over the last decade, deep neural network (DNN) based systems have achieved great performance in the synthesis ﬁeld including SVS, and have shown their superiority over hidden Markov model (HMM) based models in both objective and subjective scores. In the beginning, the DNN model was proposed to predict the spectral information (e.g., Mel spectrogram, linear spectrogram, vocoder parameters) and started to outperform HMMs in mean opinion scores signiﬁcantly [1–3]. Later, variations of the neural networks, including recurrent neural networks (RNN) and convolutional neural networks (CNN), also demonstrated their power on acoustic modeling for singing voice [4–7]. Architectures like generative adversarial network, were also shown to get reasonable performance by introducing various discriminators [8–15].
As sequence-to-sequence models have become the dominant architectures in neural-based TTS, recent SVS systems have also adopted the encoder-decoder methods and achieved improved performance over the simple network structure (e.g., DNN, CNN, RNN) [12, 16–22]. In these methods, the encoders and decoders include Long-Short-Term Memory units (LSTM) with attention mechanism, multi-head self-attention (MHSA), and conformer blocks. However, in most cases, SVS has much less training data than TTS due to its high data annotation costs and more strict copyright requirements in the music domain.
We have observed three types of methods to mitigate the data scarcity problem, including: incorporating low-quality
∗Equal Contribution.

data, transfer learning, and regularization. Ren et al. [21] apply the data crawled from the Internet and multi-step preprocessing to train the end-to-end synthesis system. As singing has several similarities with speech, previous researches also investigate to apply transfer learning from speech [23, 24]. However, the method requires parallel speech & singing corpora, which is difﬁcult to collect. Since limited data would more likely lead to over-ﬁtting, a speciﬁc regularization loss based on perceptual entropy is proposed in [22], which improves the quality of synthesized singing. Since the perceptual entropy loss only acts as a regularization term, it has no decisive impact on the model training process, and there is still much room for improvements.
Besides the methods mentioned above, data augmentation has been a simple and effective approach in other speech or singing related tasks [25–30]. Following their insights, in this paper, we propose two simple data augmentation techniques, pitch augmentation and mix-up method, to improve the singing quality with limited training data. Moreover, a cycle-consistent predictor is introduced as an additional module to stabilize the training process with the augmentation techniques. Both our proposed data augmentation policies and the predictor module have demonstrated their superiority in both objective and subjective tests on two public singing datasets. Our model boosts the synthetic quality signiﬁcantly, with the MOS performance improvement of 22.3% and 12.8% on Ofuton [31] and Opencpop [32], respectively. 1

2. Method

We denote the acoustic feature of a singing phrase as Y ∈ RT ×Da , where T is the number of frames in the sequence and Da is the acoustic feature dimension, and the music score X := (Xph, Xpi, Xbeats) including phoneme, pitch, and duration se-
quences are denoted as Xph ∈ NT , Xpi ∈ NT , Xbeats ∈ NT , where T is the number of tokens. Training the SVS model is
to minimize the following loss:

Losvris = L1 (f (X; θxy), Y )

(1)

where f (X; θxy) denotes the acoustic model, which generates the prediction of acoustic features Yˆ based on the music score (i.e., (Xph, Xpi, Xbeats)). θxy refers to the trainable parameters of the acoustic model. L1(·) denotes the L1 loss function.
2.1. Non-Autoregressive SVS Framework
The base framework of our SVS system (similar to [20, 32]) is illustrated in Fig. 1 (a). The phoneme sequence Xph is converted into expanded phoneme hidden states Hph ∈ RT ×Dh after an embedding layer, an encoder, and a length regulator,

1The source code is publicly available at https://github.com/ SJTMusicTeam/Muskits

Phoneme

Embedding

Pitch

Encoder

Embedding

Length Regulator

G.T. Duration

Decoder

i< l a t e x i t s h a 1 _ b a s e 6 4 = " E h d U w y v e c h O A l H O i o z / 1 o j S 9 m + I = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 U O L 9 c s W t u n O Q V e L l p A I 5 G v 3 y V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 1 E s 1 J p S N 6 R C 7 l k o a o f a z + a V T c m a V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 2 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 6 9 V 6 j d 5 H E U 4 g V M 4 B w + u o A 5 3 0 I A m M A j h G V 7 h z R k 7 L 8 6 7 8 7 F o L T j 5 z D H 8 g f P 5 A w a Q j Q o = < / l a t e x i t > -th

Phoneme

Hidden States

< l a t e x i t s h a 1 _ b a s e 6 4 = " C w N W y e p z B 2 m e h X 6 3 T f 5 4 e S J 7 x O U = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c q 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 L z 3 2 y h W 3 6 s 5 A l o m X k w r k q P f K X 9 1 + z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 G T U j f V m F A 2 o g P s W C p p h N r P Z p d O y I l V + i S M l S 1 p y E z 9 P Z H R S O t x F N j O i J q h X v S m 4 n 9 e J z X h l Z 9 x m a Q G J Z s v C l N B T E y m b 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D g l G 4 K 3 + P I y a Z 5 V v Y v q + d 1 5 p X a d x 1 G E I z i G U / D g E m p w C 3 V o A I M Q n u E V 3 p y R 8 + K 8 O x / z 1 o K T z x z C H z i f P w g U j Q s = < / l a t e x i t >

j-th

Phoneme

Hidden States

i< l a t e x i t s h a 1 _ b a s e 6 4 = " E h d U w y v e c h O A l H O i o z / 1 o j S 9 m + I = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q d e P C j i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 U O L 9 c s W t u n O Q V e L l p A I 5 G v 3 y V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 1 E s 1 J p S N 6 R C 7 l k o a o f a z + a V T c m a V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 2 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 0 7 J h u A t v 7 x K W h d V 7 7 J a u 6 9 V 6 j d 5 H E U 4 g V M 4 B w + u o A 5 3 0 I A m M A j h G V 7 h z R k 7 L 8 6 7 8 7 F o L T j 5 z D H 8 g f P 5 A w a Q j Q o = < / l a t e x i t >

-th

Pitch

Hidden States

< l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 m + G O h Y u W i N j a E I D A g J 0 3 0 x M m o = " > A A A B 7 3 i c b V D L S g M x F L 2 p r z q + q i 7 d B I v g q s x I U Z d F N y 4 r 2 A e 0 Q 8 l k M m 1 o J j M m G a E M / Q k 3 L h R x 6 + + 4 8 2 9 M 2 1 l o 6 4 H A 4 Z x z y b 0 n S A X X x n W / U W l t f W N z q 7 z t 7 O z u 7 R 9 U D o / a O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 D D e C d V P F S B w I 1 g n G t z O / 8 8 S U 5 o l 8 M J O U + T E Z S h 5 x S o y V u k 5 f 2 G x I B p W q W 3 P n w K v E K 0 g V C j Q H l a 9 + m N A s Z t J Q Q b T u e W 5 q / J w o w 6 l g U 6 e f a Z Y S O i Z D 1 r N U k p h p P 5 / v O 8 V n V g l x l C j 7 p M F z 9 f d E T m K t J 3 F g k z E x I 7 3 s z c T / v F 5 m o m s / 5 z L N D J N 0 8 V G U C W w S P D s e h 1 w x a s T E E k I V t 7 t i O i K K U G M r c m w J 3 v L J q 6 R 9 U f M u a / X 7 e r V x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E F A c / w C m / o E b 2 g d / S x i J Z Q M X M M f 4 A + f w B 0 E Y + a < / l a t e x i t >

1< l a t e x i t s h a 1 _ b a s e 6 4 = " z D 9 + G N r s B 5 4 n / 1 Z 3 2 b 2 q a R Q w g j 0 = " > A A A B 8 X i c b V D L S g M x F L 3 j s 4 6 v q k s 3 w S K 4 s c x I U Z d F N y 4 r 2 A e 2 Q 8 l k M m 1 o J h m S j F C G / o U b F 4 q 4 9 W / c + T e m 7 S y 0 9 U D g c M 6 5 5 N 4 T p p x p 4 3 n f z s r q 2 v r G Z m n L 3 d 7 Z 3 d s v H x y 2 t M w U o U 0 i u V S d E G v K m a B N w w y n n V R R n I S c t s P R 7 d R v P 1 G l m R Q P Z p z S I M E D w W J G s L H S o + u f 9 7 h N R 7 h f r n h V b w a 0 T P y C V K B A o 1 / + 6 k W S Z A k V h n C s d d f 3 U h P k W B l G O J 2 4 v U z T F J M R H t C u p Q I n V A f 5 b O M J O r V K h G K p 7 B M G z d T f E z l O t B 4 n o U 0 m 2 A z 1 o j c V / / O 6 m Y m v g 5 y J N D N U k P l H c c a R k W h 6 P o q Y o s T w s S W Y K G Z 3 R W S I F S b G l u T a E v z F k 5 d J 6 6 L q X 1 Z r 9 7 V K / a a o o w T H c A J n 4 M M V 1 O E O G t A E A g K e 4 R X e H O 2 8 O O / O x z y 6 4 h Q z R / A H z u c P T 1 W Q D A = = < / l a t e x i t >

Decoder

< l a t e x i t s h a 1 _ b a s e 6 4 = " C w N W y e p z B 2 m e h X 6 3 T f 5 4 e S J 7 x O U = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c q 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 L z 3 2 y h W 3 6 s 5 A l o m X k w r k q P f K X 9 1 + z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 G T U j f V m F A 2 o g P s W C p p h N r P Z p d O y I l V + i S M l S 1 p y E z 9 P Z H R S O t x F N j O i J q h X v S m 4 n 9 e J z X h l Z 9 x m a Q G J Z s v C l N B T E y m b 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D g l G 4 K 3 + P I y a Z 5 V v Y v q + d 1 5 p X a d x 1 G E I z i G U / D g E m p w C 3 V o A I M Q n u E V 3 p y R 8 + K 8 O x / z 1 o K T z x z C H z i f P w g U j Q s = < / l a t e x i t >

j

-th

Pitch

Hidden States

Acoustic Feature

L< l a t e x i t s h a 1 _ b a s e 6 4 = " 6 a e H A c H T c P X 5 A D 6 r d i R I q m 8 W n p 8 = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 m k q M e i F w 8 e K t g P a E P Z b D f t 0 t 1 N 2 N 0 I J f Q v e P G g i F f / k D f / j Z s 2 B 2 1 9 M P B 4 b 4 a Z e W H C m T a e 9 + 2 U 1 t Y 3 N r f K 2 + 7 O 7 t 7 + Q e X w q K 3 j V B H a I j G P V T f E m n I m a c s w w 2 k 3 U R S L k N N O O L n N / c 4 T V Z r F 8 t F M E x o I P J I s Y g S b X H L v B / 6 g U v V q 3 h x o l f g F q U K B 5 q D y 1 R / G J B V U G s K x 1 j 3 f S 0 y Q Y W U Y 4 X T m 9 l N N E 0 w m e E R 7 l k o s q A 6 y + a 0 z d G a V I Y p i Z U s a N F d / T 2 R Y a D 0 V o e 0 U 2 I z 1 s p e L / 3 m 9 1 E T X Q c Z k k h o q y W J R l H J k Y p Q / j o Z M U W L 4 1 B J M F L O 3 I j L G C h N j 4 3 F t C P 7 y y 6 u k f V H z L 2 v 1 h 3 q 1 c V P E U Y Y T O I V z 8 O E K G n A H T W g B g T E 8 w y u 8 O c J 5 c d 6 d j 0 V r y S l m j u E P n M 8 f / 7 W N k Q = = < / l a t e x i t > 1

-

loss

Music Score Pitch Augment Music Score

Acoustic Feature

Acoustic Feature

Pitch Augment

Phoneme
Acoustic Model

(c)
Cycle-Consistent
Acoustic Feature

Phoneme Predictor

Acoustic Feature

< l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 m + G O h Y u W i N j a E I D A g J 0 3 0 x M m o = " > A A A B 7 3 i c b V D L S g M x F L 2 p r z q + q i 7 d B I v g q s x I U Z d F N y 4 r 2 A e 0 Q 8 l k M m 1 o J j M m G a E M / Q k 3 L h R x 6 + + 4 8 2 9 M 2 1 l o 6 4 H A 4 Z x z y b 0 n S A X X x n W / U W l t f W N z q 7 z t 7 O z u 7 R 9 U D o / a O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 D D e C d V P F S B w I 1 g n G t z O / 8 8 S U 5 o l 8 M J O U + T E Z S h 5 x S o y V u k 5 f 2 G x I B p W q W 3 P n w K v E K 0 g V C j Q H l a 9 + m N A s Z t J Q Q b T u e W 5 q / J w o w 6 l g U 6 e f a Z Y S O i Z D 1 r N U k p h p P 5 / v O 8 V n V g l x l C j 7 p M F z 9 f d E T m K t J 3 F g k z E x I 7 3 s z c T / v F 5 m o m s / 5 z L N D J N 0 8 V G U C W w S P D s e h 1 w x a s T E E k I V t 7 t i O i K K U G M r c m w J 3 v L J q 6 R 9 U f M u a / X 7 e r V x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E F A c / w C m / o E b 2 g d / S x i J Z Q M X M M f 4 A + f w B 0 E Y + a < / l a t e x i t >

1< l a t e x i t s h a 1 _ b a s e 6 4 = " z D 9 + G N r s B 5 4 n / 1 Z 3 2 b 2 q a R Q w g j 0 = " > A A A B 8 X i c b V D L S g M x F L 3 j s 4 6 v q k s 3 w S K 4 s c x I U Z d F N y 4 r 2 A e 2 Q 8 l k M m 1 o J h m S j F C G / o U b F 4 q 4 9 W / c + T e m 7 S y 0 9 U D g c M 6 5 5 N 4 T p p x p 4 3 n f z s r q 2 v r G Z m n L 3 d 7 Z 3 d s v H x y 2 t M w U o U 0 i u V S d E G v K m a B N w w y n n V R R n I S c t s P R 7 d R v P 1 G l m R Q P Z p z S I M E D w W J G s L H S o + u f 9 7 h N R 7 h f r n h V b w a 0 T P y C V K B A o 1 / + 6 k W S Z A k V h n C s d d f 3 U h P k W B l G O J 2 4 v U z T F J M R H t C u p Q I n V A f 5 b O M J O r V K h G K p 7 B M G z d T f E z l O t B 4 n o U 0 m 2 A z 1 o j c V / / O 6 m Y m v g 5 y J N D N U k P l H c c a R k W h 6 P o q Y o s T w s S W Y K G Z 3 R W S I F S b G l u T a E v z F k 5 d J 6 6 L q X 1 Z r 9 7 V K / a a o o w T H c A J n 4 M M V 1 O E O G t A E A g K e 4 R X e H O 2 8 O O / O x z y 6 4 h Q z R / A H z u c P T 1 W Q D A = = < / l a t e x i t >

Pitch

Pitch

Cycle-Consistent

(a)

(b)

(d)

Figure 1: An overview of data augmentation and cycle-consistent training strategy for Singing Voice Synthesis. The blue blocks are the

inputs, yellow and orange blocks are the main components of SVS model, green blocks are the outputs. (a) is the conventional pipeline

of singing voice synthesis. (b) is the detailed process for mix-up augmentation. λ is the combination weight in Eq. (2) and (3). (c) is

the diagram for pitch augmentation. The input music score and target acoustic feature need to be augmented correspondingly at the

same time. (d) is the architecture for the cycle-consistent training strategy with predictor module.

where Dh is the dimension of the hidden states Hph. Meanwhile, Xpi is passed to the pitch embedding layer and expanded by the same length regulator, resulting in expanded pitch hidden states Hpi ∈ RT ×Dh . In this work, similar to [13, 22, 33], we directly use the ground truth (G.T.) duration in our length regulator to avoid the duration modeling impact on our experiments. As the G.T. duration already contains the beats information, there is no need to use Xbeats in our SVS model. Based on the input of summing Hph, Hpi, and the positional embedding, the decoder generates the prediction of acoustic feature Yˆ , from which the loss is calculated as in Eq. (1).
Based on this framework, in the following subsections, we propose two singing augmentation methods, pitch and mix-up augmentation, to compensate the data scarcity issue for SVS. We further introduce a new cycle-consistent training strategy to improve the synthetic quality.
2.2. Singing Augmentation
Mix-up Augmentation (MA) Mix-up based data augmentation methods have been applied in supervised classiﬁcation tasks [29, 34, 35]. However, there are no related attempts in the regression tasks like TTS or SVS yet.
The training pipeline of SVS with MA is shown in Fig. 1 (b). Inspired by the MA in machine translation tasks [35], we also adopt the MA in the embedding spaces to combine the music score information X. However, our MA methods are different from the MA in machine translation, as we perform the MA on the expanded hidden states Hph and Hpi instead of on the input Xph and Xpi. The primary reason is that the samples selected for MA might have different duration for each note and leads to different expansions during length regulation. Specifically, on the basis of original training process, when applying MA, two samples i and j from a minibatch are selected randomly to form a mixture loss Lmix. We denote H ∈ RT ×Dh as the sum of Hph, Hpi, and positional information. Then, the hidden states from the two samples are Hi ∈ RTi×Dh and Hj ∈ RTj×Dh . We ﬁrstly pad hidden states on the right to make them in the same length Tmax = max(Ti, Tj) and then interpolate them into Hmix ∈ RTmax×Dh on the left as:

Hmix = λ ∗ Hi + (1 − λ) ∗ Hj ,

(2)

where the weight λ is sampled from a Beta distribution with hyper-parameter α (i.e., λ ∼ Beta(α, α)).

Given Hmix, the decoder predicts the acoustic feature as Yˆmix ∈ RTmax×Da . Yˆmix is used to compute the loss against the ground truth features Yi ∈ RTi×Da and Yj ∈ RTj×Da . With the same weight λ used in Eq. (2), we deﬁne the mixture loss Lmix as:
Lmix = λ ∗ L1(Yˆmix, Yi) + (1 − λ) ∗ L1(Yˆmix, Yj ), (3)

Finally, we utilize wmix to combine the original SVS loss Losvris and the additional mix-up loss Lmix:

Lsvs = (1 − wmix) ∗ Losvris + wmix ∗ Lmix.

(4)

Pitch Augmentation (PA) According to the chromatic scale [36], the musical pitches can be scaled to 12 unique semitones. They are designed to be equally-spaced in twelve-tone equal temperament, which is one√of the dominant temperaments [37]. Each semitone differs by 12 2 in frequency domain (Hz), equals to 1.059 approximately.
For pitch augmentation, we employ WORLD vocoder [38] to get the corresponding singing voice after semitone adjustments as shown in Fig. 1(c). The input information of the WORLD vocoder consists of the fundamental frequency F0, the spectral envelope SP (harmonic spectral envelope) and the aperiodic signal A√P (aperiodic spectral envelope). By multiplying or dividing 12 2 to F0 sequence and keeping the SP and AP unchanged, we can easily obtain the time domain singing waveform signal corresponding to its semitone adjustments. Different from pitch augmentation or pitch shifting discussed in [4, 30, 39], in our strategy, the entire singing phrase is processed with a ﬁxed amount of semitone over raw audios, and the music score is processed accordingly. Based on the chromatic scale, the melody can be kept as the same over another tonality.
2.3. Cycle-consistent training strategy (CC)
As discussed in Section 2.2, our proposed augmentation policies would signiﬁcantly increase the diversity of training samples. However, both MA and PA apply modiﬁcation directly to acoustic features, which may introduce artifacts and noises to the original signal. For PA, the noise may come from the WORLD vocoder when modifying target singing signals, as its quality still has a gap with the real singing voice. For MA, i-th and j-th samples can be regarded as the noise of each other. These artifacts and noises may disrupt the training of

Table 1: Objective and subjective evaluation of SVS models with different settings on Ofuton and Opencpop databases. Augmentation policies include pitch augmentation (PA) and mix-up augmentation (MA). CC denotes as the acoustic model that is jointly trained with cycle-consistent predictor module. Both objective and subjective metrics are introduced in Sec. 3.2.

Dataset

Method

MCD↓ LF0-RMSE↓ F0-CORR↑ ST ACC↑ VUV ERR↓ MOS↑

Ofuton

Baseline

6.88

+ PA

6.55

+ MA

6.65

+ PA&MA

6.55

+ PA&MA&CC 6.44

0.103 0.106 0.101 0.107 0.100

0.75

69.74

2.24

2.65 ± 0.08

0.86

71.09

2.38

2.87 ± 0.09

0.75

70.29

2.07

2.89 ± 0.09

0.87

71.74

2.32

2.98 ± 0.09

0.91

71.88

2.45

3.24 ± 0.09

G.T.

-

-

-

-

-

4.60 ± 0.07

Baseline

8.15

+ PA

7.85

Opencpop + MA

8.11

+ PA&MA

8.23

+ PA&MA&CC 7.76

0.226 0.219 0.241 0.265 0.214

0.87

74.46

6.04

2.89 ± 0.11

0.87

74.37

5.68

3.06 ± 0.11

0.88

74.41

6.43

2.92 ± 0.10

0.89

73.40

6.59

2.43 ± 0.09

0.86

74.30

5.71

3.26 ± 0.11

G.T.

-

-

-

-

-

4.60 ± 0.09

the SVS acoustic model. Therefore, we propose our new cycleconsistent training strategy which is shown in Fig. 1 (d). It applies an additional predictor to stabilize the network and makes it resistant to the potential noise from augmentation.
Under the cycle-consistent training strategy with predictor module, the score information loss Lsi helps the predictor module train steadily from the G.T. acoustic feature and music score:

Lsi = CrossEntropy g(Y ; θyx), X ,

(5)

where g(Y ; θyx) denotes the predictor module, which takes the G.T. acoustic feature as input, produces the prediction of expanded phoneme and pitch sequences. θyx represents trainable parameters of the predictor module. Given any paired (x, y), we use the Cross-Entropy loss to train the predictor module.
Similar to models for lyric recognition and music transcription, the predictor takes the predicted acoustic feature as input and predicts their phoneme and pitch sequences. The related formulation is deﬁned as follows, which is the composition function of Eq. (1) and Eq. (5):

Lpd = CrossEntropy g(f (X; θxy); θyx), X , (6)

when calculating the loss Lpd of the predictor module, it takes the predicted acoustic feature as input, produces the prediction of phoneme and pitch sequences. It aims to help the acoustic model keep the cycle consistency of music score information.
At last, we combine all the losses in Eq. (1), (5) and (6) using different weights, including wsvs, wsi and wpd. The combined loss L is used for our cycle-consistent SVS training:

L = wsvs ∗ Lsvs + wsi ∗ Lsi + wpd ∗ Lpd.

(7)

3. Experiments
3.1. Datasets
To evaluate the effectiveness of our methods, we conduct related experiments on both Ofuton [31] and Opencpop [32] databases. Ofuton dataset is a public Japanese male singing voice corpora, which has 56 Japanese songs (61 minutes) in total. Since Ofuton corpora has no ofﬁcial segmentation, we split each song into several singing phrases, resulting in 547 phrases for training, 58 for validation, and 70 for testing. The splitting is based on the silence between lyrics. Opencpop dataset is a public Mandarin female singing voice corpus. It has 100 popular Mandarin

songs, which are performed by a female singer. As for segmentation of Opencpop dataset, we follow the ofﬁcial split. For preprocessing, we down-sample the songs to a sampling rate of 24k Hz and extract the 80-dim Mel spectrogram as the target acoustic features in our SVS system. The Mel spectrogram is extracted with 12.5 ms frame-shift and 50 ms frame-length.
3.2. Experiment Setups
For the SVS acoustic model in our paper, we follow the architecture of Fastspeech [40], which is a encoder-decoder based sequence-to-sequence (Seq2Seq) model widely used in both TTS and SVS domain. The encoder has 6 blocks of encoder layers. Each encoder layer consists of a 384-dimension, four-heads self-attention layer and 1D-convolution feed-forward module. The convolutional layer in the feed-forward module consists of 1536 ﬁlters with shape 1 × 1. The embedding size of phoneme and pitch are 384. The decoder has 6 blocks of decoder layers, which keep the same model structure as encoder layer. LayerNorm and dropout are adopted in both encoder and decoder layer. The dropout rate is set as 0.1. The post-net module is utilized after decoder to reﬁne the output Mel spectrogram by predicting the residual. The detailed model structure of post-net module follows the Tacotron 2 [41].
For the predictor module, 4 blocks of predictor layers are included in both phoneme predictor module and pitch predictor module. Each predictor layer consists of a 80-dimension, fourheads self-attention layer and 1D-convolution feed-forward module. The convolutional layer in the feed-forward module consists of 512 ﬁlters with shape 1 × 1. The dropout rate is set as 0.1. After the predictor layer, a linear mapping is adopted to produce the prediction of phoneme or pitch sequence.
For the vocoder, we utilize the HiFi-GAN vocoder [42] to generate the singing waveform from predicted Mel spectrogram. The vocoder is pre-trained with the G.T. audio and Mel spectrogram pairs for 300k steps in advance.
In the training stage, the Adam optimizer with 0.001 learning rate and noam warm-up policy [43] are utilized. Global mean normalization is applied for acoustic features. We train the acoustic models for 500 epochs in Ofuton dataset and 250 epochs in Opencpop dataset. The models with the lowest validation loss Lsvs are chosen for testing. During inference, we use the G.T. duration information to expand the encoder outputs. For PA, we randomly shift semitones in a phrase by {-1, 0, 1}. For MA, the proportion of mix-up samples within a batch is set as 0.15. The weight of mix-up loss wmix is 0.1 in Ofuton

Table 2: The comparison of different ways to choose the PA shifting factor on Opencpop dataset.

Method MCD↓ ST ACC↑ VUV ERR↓

P-adaptive 8.26

73.29

6.77

P1

7.85

74.37

5.68

P2

8.08

73.92

6.06

Table 3: The comparison of different weights in MA on Opencpop dataset.

Method MCD↓ ST ACC↑ VUV ERR↓

wmix = 0.1 8.11

74.41

6.43

wmix = 0.2 8.15

73.60

6.43

wmix = 0.3 8.20

74.25

6.36

database and Opencpoop database. The α of Beta distribution which controls the combination weight λ in Eq. (2) is set as 0.5. In Ofuton database, the CC’s weights to combine the loss terms mentioned in Equation (2) are set as wsvs = 0.7, wsi = 0.2, wpd = 0.1. While in the Opencpop database, the weights are set as 0.85, 0.1, 0.05 respectively.
For the objective evaluation, we utilize ﬁve metrics, including Mel-cepstrum distortion (MCD), log-F0 root mean square error (LF0 RMSE), Pearson correlation coefﬁcients of F0 measures (F0 CORR), semitone accuracy (ST ACC) and voice/unvoiced error rate (VUV ERR). We use ST ACC to allow some tolerance over F0 differences, based on music theory and human hearing perception [36].
For the subjective evaluation, we conduct the Mean opinion score (MOS) test to verify the effectiveness of our methods. We invite 25 listeners ranging from musicians and nonprofessionals. Listeners are asked to give their opinion score from one (non-intelligible) to ﬁve (excellent naturalness). For each setting, 15 samples are chosen randomly from the test set.
3.3. Comparison with the Baseline
Table 1 presents the results on Ofuton and Opencpoop databases using our proposed augmentation methods and cycle-consistent training strategy. The detailed structure of the baseline model is described in Section 3.2.
On the Ofuton database, we can observe a general trend that PA, MA, PA&MA&CC improve the performance on both subjective and objective scores, expect on the VUV ERR metric. This may be due to the artifacts introduced from the semitone shift on the training data when PA is applied.
On the Opencpop database, although PA, MA, or PA&MA&CC achieves the best scores in subjective and objective evaluations, we do observe more results ﬂuctuations in F0based measures. It may indicate that the augmentation methods could sacriﬁce limited performance in F0 accuracy of the music score to reach a signiﬁcant gain on naturalness of the signal.
Compared to results with single augmentation method (PA or MA), combining both augmentations (PA&MA) can enhance the performance on Ofuton, but not on Opencpop. However, after applying CC, the system reaches the best MOS, which aligns with our assumption that the introduction of CC can improve the stability over potential noises from augmentation policies.
3.4. Ablation Studies
We conduct experiments to investigate the impact of different hyper-parameters on the model. All of the ablation studies are conducted on the Opencpop dataset. A. Pitch shifting factor in PA Table 2 presents the results with different PA shifting. P-adaptive means that we choose the fac-

Table 4: The comparison of different weights in cycle consistent training strategy on Opencpop dataset. CC1 denotes the setting of wsvs=0.7, wsi=0.2, wpd=0.1. CC2 denotes the setting of wsvs=0.85, wsi=0.1, wpd=0.05. CC3 denotes the setting of wsvs=1, wsi=1, wpd=1.

Method

MCD↓ ST ACC↑ VUV ERR↓

CC1 + PA + MA 8.25

73.89

6.62

CC2 + PA + MA 7.76

74.30

5.71

CC3 + PA + MA 8.71

73.83

6.45

Table 5: The ablation study of cycle consistent SVS framework with pitch or mix-up augmentation on Opencpop dataset.

Method

MCD↓ ST ACC↑ VUV ERR↓

CC2

8.06

73.67

6.63

CC2 + PA

8.11

73.46

6.44

CC2 + MA

8.04

74.28

6.20

CC2 + PA + MA 7.76

74.30

5.71

tor to make the average pitch of samples closer to the mean of Opencpop dataset. P1 is the default setting we used in Sec. 3.2. The approach of P2 is similar to P1, while the shifting ranges in {-2, -1, 0, 1, 2}. Among these three methods of pitch augmentation, P1 achieves the best performance on all objective metrics. As shown in Table 2, the variation range of PA is not proportional to the synthetic quality. Too much variation in PA may reduce the effectiveness of the acoustic model. B. Mixing weights in MA Table 3 presents the results with different weights applied in MA. We change the weights of mix-up augmentation wmix in Eq. (4). When wmix = 0.1, the model reaches the optimal value on MCD and ST ACC. The model with wmix = 0.3 achieves the best VUV ERR performance. Note that the trend in MA is similar to that in PA, large weights of wmix may damage the synthetic performance. C. Different weights in CC Table 4 presents the results with different weights in CC with predictor. We verify the models with the combination of PA and MA (both with default settings). Among different weights in Table 4, CC2 ( wsvs = 0.85, w si = 0.1, wpe = 0.05) achieves the best performance on all objective metrics. Compared to the methods using PA and MA in Table 1 on Opencpop dataset, combining the CC predictor module does not necessarily enhance the synthetic quality of the acoustic model. It indicates that the weights of CC predictor module might need some tuning for usage. D. Combination of CC with PA or MA Table 5 presents the results of the combination of CC with PA or MA, among which CC+PA+MA achieves the best performance on all objective metrics. Compared to the baseline model in Table 1, the methods with CC improve the MCD metric regardless of whether PA or MA is used. Moreover, combing the CC with PA and MA could bring further improvements on synthetic quality, which demonstrates the effectiveness of our methods.
4. Conclusions
In this paper, we propose two data augmentation methods to mitigate the data scarcity issue for SVS. We further introduce a new training strategy that jointly trains the SVS network with a cycle-consistent predictor module for music and lyrics information. Our proposed methods bring improvements on both subjective and objective metrics on two different datasets, e.g. gaining 22.3% and 12.8% MOS improvement on Ofuton [31] and Opencpop [32], respectively. We will explore extending the proposed framework into unsupervised fashion in the future work, e.g., using dual learning or self-supervised learning.

5. References
[1] M. Nishimura, K. Hashimoto, K. Oura et al., “Singing Voice Synthesis Based on Deep Neural Networks,” in Interspeech, 2016, pp. 2478–2482.
[2] Y. Hono, S. Murata, K. Nakamura et al., “Recent development of the DNN-based singing voice synthesis system—sinsy,” in APSIPA, 2018, pp. 1003–1009.
[3] J. Tae, H. Kim, and Y. Lee, “MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis,” in MLPS, 2021, pp. 1–6.
[4] M. Blaauw and J. Bonada, “A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs,” Applied Sciences, vol. 7, no. 12, p. 1313, 2017.
[5] J. Kim, H. Choi, J. Park, S. Kim, J. Kim, and M. Hahn, “Korean Singing Voice Synthesis System based on an LSTM Recurrent Neural Network,” in Interspeech, 2018, pp. 1551–1555.
[6] K. Nakamura, K. Hashimoto, K. Oura et al., “Singing voice synthesis based on convolutional neural networks,” arXiv preprint arXiv:1904.06868, 2019.
[7] K. Nakamura, S. Takaki, K. Hashimoto et al., “Fast and HighQuality Singing Voice Synthesis System based on Convolutional Neural Networks,” in ICASSP, 2020, pp. 7239–7243.
[8] Y. Hono, K. Hashimoto, K. Oura et al., “Singing Voice Synthesis based on Generative Adversarial Networks,” in ICASSP, 2019, pp. 6955–6959.
[9] P. Chandna, M. Blaauw, J. Bonada, and E. Go´mez, “WGANSing: A Multi-Voice Singing Voice Synthesizer Based on the Wasserstein-GAN,” in EUSIPCO, 2019, pp. 1–5.
[10] J. Liu, Y. Chen, Y. Yeh, and Y. Yang, “Score and Lyrics-Free Singing Voice Generation,” arXiv preprint arXiv:1912.11747, 2019.
[11] S. Choi, W. Kim, S. Park, S. Yong, and J. Nam, “Korean Singing Voice Synthesis Based on Auto-Regressive Boundary Equilibrium Gan,” in ICASSP, 2020, pp. 7234–7238.
[12] J. Chen, X. Tan, J. Luan, T. Qin, and T.-Y. Liu, “HiFiSinger: Towards High-Fidelity Neural Singing Voice Synthesis,” arXiv preprint arXiv:2009.01776, 2020.
[13] J. Wu and J. Luan, “Adversarially Trained Multi-Singer Sequenceto-Sequence Singing Synthesizer,” Interspeech, pp. 1296–1300, 2020.
[14] J. Lee, H.-S. Choi, C.-B. Jeon, J. Koo, and K. Lee, “Adversarially Trained End-to-End Korean Singing Voice Synthesis System,” Interspeech, pp. 2588–2592, 2019.
[15] Y. Zhang, J. Cong, H. Xue, L. Xie, P. Zhu, and M. Bi, “VISinger: Variational Inference with Adversarial Learning for End-toEnd Singing Voice Synthesis,” arXiv preprint arXiv:2110.08813, 2021.
[16] M. Blaauw and J. Bonada, “Sequence-to-sequence singing synthesis using the feed-forward transformer,” in ICASSP, 2020, pp. 7229–7233.
[17] L. Zhang, C. Yu, H. Lu et al., “DurIAN-SC: Duration Informed Attention Network based Singing Voice Conversion System,” arXiv preprint arXiv:2008.03009, 2020.
[18] Y. Wu, S. Li, C. Yu et al., “Peking Opera Synthesis via Duration Informed Attention Network,” arXiv preprint arXiv:2008.03029, 2020.
[19] Y. Gu, X. Yin, Y. Rao, Y. Wan, B. Tang, Y. Zhang, J. Chen, Y. Wang, and Z. Ma, “Bytesing: A Chinese singing voice synthesis system using duration allocated encoder-decoder acoustic models and WaveRNN vocoders,” in ISCSLP, 2021, pp. 1–5.
[20] P. Lu, J. Wu, J. Luan, X. Tan, and L. Zhou, “XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System,” Interspeech, pp. 1306–1310, 2020.
[21] Y. Ren, X. Tan, T. Qin et al., “DeepSinger: Singing Voice Synthesis with Data Mined From the Web,” in ACM SIGKDD, 2020, pp. 1979–1989.
[22] J. Shi, S. Guo, N. Huo, Y. Zhang, and Q. Jin, “Sequence-tosequence singing voice synthesis with perceptual entropy loss,” in ICASSP, 2021, pp. 76–80.

[23] R. Valle, J. Li, R. Prenger, and B. Catanzaro, “Mellotron: Multispeaker Expressive Voice Synthesis by Conditioning on Rhythm, Pitch and Global Style Tokens,” in ICASSP, 2020, pp. 6189–6193.

[24] L. Zhang, C. Yu, H. Lu, C. Weng, Y. Wu, X. Xie, Z. Li, and D. Yu, “Learning Singing From Speech,” arXiv preprint arXiv:1912.10128, 2019.

[25] W.-N. Hsu, Y. Zhang, R. J. Weiss, Y.-A. Chung, Y. Wang, Y. Wu, and J. Glass, “Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization,” in ICASSP, 2019, pp. 5901–5905.

[26] M.-J. Hwang, R. Yamamoto, E. Song, and J.-M. Kim, “TTS-byTTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis,” arXiv preprint arXiv:2010.13421, 2020.

[27] G. Huybrechts, T. Merritt, G. Comini, B. Perz, R. Shah, and J. Lorenzo-Trueba, “Low-resource expressive text-to-speech using data augmentation,” arXiv preprint arXiv:2011.05707, 2020.

[28] Y. Hwang, H. Cho, H. Yang, D.-O. Won, I. Oh, and S.-W. Lee, “Mel-spectrogram augmentation for sequence to sequence voice conversion,” arXiv preprint arXiv:2001.01401, 2020.

[29] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, “MixSpeech: Data augmentation for low-resource automatic speech recognition,” in ICASSP, 2021, pp. 7008–7012.

[30] C. Zhang, J. Yu, L. Chang, X. Tan, J. Chen, T. Qin, and K. Zhang, “PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription,” arXiv preprint arXiv:2109.07940, 2021.

[31] P.

Futon,

“DB

Production:

Fu-

ton

P,”

https://sites.google.com/view/oftn-

utagoedb/%E3%83%9B%E3%83%BC%E3%83%A0, accessed:

2022.03.10.

[32] Y. Wang, X. Wang, P. Zhu, J. Wu, H. Li, H. Xue, Y. Zhang, L. Xie, and M. Bi, “Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,” arXiv preprint arXiv:2201.07429, 2022.

[33] Y.-H. Yi, Y. Ai, Z.-H. Ling, and L.-R. Dai, “Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling,” Interspeech, pp. 2593–2597, 2019.

[34] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond Empirical Risk Minimization,” in ICLR, 2018.

[35] Y. Cheng, L. Jiang, W. Macherey, and J. Eisenstein, “Advaug: Robust adversarial augmentation for neural machine translation,” in ACL, 2020, pp. 5961–5970.

[36] I. Quinn, “Tonal harmony,” in The Oxford Handbook of Critical Concepts in Music Theory. Oxford University Press, 2019.

[37] B. Benward, Music in Theory and Practice Volume 1. McGrawHill Higher Education, 2014.

[38] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A VocoderBased High-Quality Speech Synthesis System for Real-Time Applications,” IEICE TRANSACTIONS on Information and Systems, vol. 99, no. 7, pp. 1877–1884, 2016.

[39] Z. Zhang, Y. Zheng, X. Li, and L. Lu, “Wesinger: Dataaugmented singing voice synthesis with auxiliary losses,” arXiv preprint arXiv:2203.10750, 2022.

[40] Y. Ren, Y. Ruan, X. Tan et al., “FastSpeech: Fast, Robust and Controllable Text to Speech,” in NIPS, 2019, pp. 3171–3180.

[41] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions,” in ICASSP, 2018, pp. 4779–4783.

[42] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative Adversarial Networks for Efﬁcient and High Fidelity Speech Synthesis,” NIPS, vol. 33, pp. 17 022–17 033, 2020.

[43] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention Is All You Need,” in NIPS, 2017, pp. 5998–6008.

