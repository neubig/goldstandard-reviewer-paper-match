Improving Diversity of Neural Text Generation via Inverse Probability Weighting
Xinran Zhang1, Maosong Sun12*, Jiafeng Liu1, Xiaobing Li1 1Department of Music Artiﬁcial Intelligence and Music Information Technology
Central Conservatory of Music, Beijing, China 2Department of Computer Science and Technology, Tsinghua University, Beijing, China
Institute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China
zhangxr.wspn@gmail.com, sms@tsinghua.edu.cn

arXiv:2103.07649v3 [cs.CL] 26 Aug 2021

Abstract
The neural text generation suffers from the text degeneration issue such as repetition. Traditional stochastic sampling methods only focus on truncating the unreliable “tail” of the distribution, and do not address the “head” part, which we show might contain tedious or even repetitive candidates with high probability that lead to repetition loops. They also do not consider the issue that human text does not always favor high-probability words. Inspired by these, in this work we propose a heuristic sampling method. We propose to use interquartile range of the predicted distribution to determine the “head” part, then permutate and rescale the “head” with inverse probability. This aims at decreasing the probability for the tedious and possibly repetitive candidates with higher probability, and increasing the probability for the rational but more surprising candidates with lower probability. The proposed algorithm provides a reasonable permutation on the predicted distribution which enhances diversity without compromising rationality of the distribution. We use pre-trained language model to compare our algorithm with traditional methods. Results show that our algorithm can effectively increase the diversity of generated samples while achieving close resemblance to human text.
1 Introduction
Neural text generation is an important natural language processing (NLP) task, and have beneﬁted a lot from Transformer (Vaswani et al., 2017) architecture. However, it suffers from the well-known text degeneration issue (Holtzman et al., 2020), that is, the decoded texts exhibit a strong tendency to be repetitive with low diversity. To address this, many works have focused on stochastic sampling by truncating the “tail” of the distribution, e.g., the top-k sampling (Fan et al., 2018; Holtzman et al., 2018) or nucleus sampling (top-p sampling, Holtzman et al., 2020), which directly truncates the

predicted distribution during sampling, excluding unreliable “tail” with low probability. Recent work by Basu et al. (2021) adaptively truncates the “tail” to achieve controllable quality.
Regrettably, none of these methods have directly addressed the discrepancy that human text does not always favor high-probability candidates (Holtzman et al., 2020), i.e., the “head” of the distribution remains unprocessed. We show in our analysis that repetitive samples with low diversity are actually caused by the “head” part with high probability. Inspired by this, we propose the interquartile range inverse probability (IQR-IP) sampling algorithm. It brings a controllable permutation on the “head” part of the predicted distribution on the ﬁltered vocabulary to enhance diversity without compromising the rationality of the distribution as well as the ﬂuency of the generated text. Experiment results show that our algorithm can increase diversity while achieving close resemblance to human text compared with traditional methods.
2 Observation on the “Tail” and “Head”
2.1 Traditional Methods: Truncating the “Tail” to Balance between Quality and Diversity
As is widely acknowledged for text generation, directly sampling on the predicted distribution will produce unsatisfactory samples due to the lowprobability “tail” of the distribution (Holtzman et al., 2020). This is self-explanatory since the “tail” contains unreasonable words that lead to less repetition as well as lower quality. Consequently, traditional methods always start by truncating the “tail”. For example, the top-k sampling (Fan et al., 2018; Holtzman et al., 2018) ﬁlters the top k probable candidates from the vocabulary (denoted by V ) as follows.
V k = x | rank p(x) ≤ k, x ∈ V , (1)

where p(x) denotes the predicted distribution of the language model, and rank refers to the ranking order of p(x). The auto-regressive dependency of p(x) on the context of word x on each sampling step is omitted for simplicity throughout this work.
According to Holtzman et al. (2020), top-k sampling cannot address the discrepancy between peaked distribution and ﬂat distribution. They propose nucleus sampling (top-p sampling) which ﬁlters the vocabulary with top p mass of cumulative probability as follows.
V p = x | cdf (x) ≤ p, x ∈ V , (2)
where the cumulative density function cdf (x) is calculated on the sorted distribution of p(x). This produces better results than top-k sampling, because it can dynamically drop more “tails” on peaked distribution, while top-k sampling can’t.
Clearly, these methods balance between quality and diversity by truncating the “tail”. Dropping more “tails” dynamically like nucleus sampling will improve quality but result in more repetition and lower diversity (see Table 1, Holtzman et al., 2020), while keeping more “tails” like top-k sampling will achieve less repetition but lower quality. Recent methods such as MIROSTAT by Basu et al. (2021) adaptively truncate the “tail” with pre-deﬁned quality target (perplexity) for better balancing effect.
2.2 Repetition Loops Caused by the “Head”
However, traditional methods do not address the “head” part, which we show may lead to the annoying repetition loops.
To explore the behavior of repetition loops, we use GPT-2 Small (Radford et al., 2019) with nucleus sampling (p = 0.95) to generate 5,000 samples with the same input context and set maximum generation length to be 1,024. The sharing input context is “She walks in beauty” (from Lord Byron’s most famous poetry).
To detect repetition as well as measuring the concentration tendency of vocabulary, we use a very straightforward metric by calculating the entropy of word distribution in a ﬁxed-length window as follows.
Hrep = − p(w) × log p(w), (3)
w

p(w) = f (w)/ f (w),

(4)

w

where f (w) denotes the frequency of word w. Samples with repetition loops will have concentrated distribution of p(w) hence having lower Hrep, while samples with diverse usage of vocabulary will have ﬂat distribution of p(w) hence having higher Hrep. Empirically, we use Hrep < 2 for all 200-length token windows to detect repetitive passages for observation.
We present a very representative sample that contains inﬁnite loops of “She walks in beauty.” (with generated period). The trajectory of ﬁrst 3 generated loops is presented in Figure 1. We found several phenomena that cause this repetition.
• Repetitive candidates always have high probability and high rank in the predicted distribution (see “*” labeled candidates in each heatmap box in Figure 1).
• Repetition tendency grows stronger when more loops occur (due to a few sampling steps that happen to pick repetitive token in nonextreme distribution, e.g, in Loop #2), as the ﬂat distribution in Loop #1 (e.g., “She” and “walks”) gradually becomes peaked distribution in Loop #3, and peaked distribution in Loop #1 (e.g., “in” and “beauty”) becomes extreme distribution in Loop #3, which reciprocally contributes to stronger repetition pattern in the context.
• The predicted distribution got stuck in extreme distribution that assigns almost all probability mass for repetitive candidates (e.g., “in” and “beauty” in Loop #3).
To further verify these phenomena, we extract and align the trajectories of each repetitive words to observe the overall trajectory for repetitive words (e.g., aligning all appearances of “She” sequentially on the x axis). Figure 2 presents the trajectories of predicted probability, rank in predicted distribution and entropy of predicted distribution, where x axis is the number of appearance of repetitive candidates. It shows that after a few appearances of repetitive candidates, the predicted distribution will quickly get stuck in extreme distribution where predicted probability approaches 1, rank approaches 1, and entropy approaches 0, which will surely render repetition loops.
From these results, it is clear that the model tends to predict high probability for repetitive candidates that exist in the context. This is in accordance with

Flat distribution

Peaked distribution

Extreme distribution Peaked distribution

Loop #1 repetitive candidates have higher rank

Loop #2 repetitive pattern grows stronger

Loop #3 get stuck in extreme distribution

Figure 1: Trajectory of predicted probability (“o” marker) and predicted distribution (heatmap box besides each marker in “word-probability” format, with the sampled word marked by “*”) for the ﬁrst 3 repetition loops. This speciﬁc sample contains inﬁnite repetitive loops of “She walks in beauty.” (with generated period). The trajectory of repetitive word “She” is highlighted in shadow which shows the increase of predicted probability and the gradually peaked predicted distribution.

Figure 2: Trajectories of repetitive candidates extracted from samples that contain repetition loops. Repetition loops are detected using Hrep < 2 on 200-length token windows. Repetitive candidates that appears more than 30 times in the window are extracted and aligned to form their trajectories. It shows that a few appearances of repetitive candidates quickly lead the model to extreme distribution that causes repetition loops.

analysis by Kang and Hashimoto (2020), which shows that words directly entailed in the context tend to have lower loss, i.e., higher predicted probability.
Clearly, these undesirable behaviors of the “head” with high probability will lead the model to generate samples that might contain repetition loops with low diversity. Regrettably, this issue is unable to address by tradition stochastic sampling algorithms, since they still encourage to sample on high-probability candidates.
2.3 Improving Diversity by Permutating the “Head” on Flat Distributions
Recall the results by Holtzman et al. (2020) which show that human text does not always choose highprobability candidates, as the beam-search-based decoding method that generates samples with low perplexity actually deviates from human text behavior (see Figure 2, Holtzman et al., 2020). Our results in Section 2.2 also show that it will be harm-

ful to sample according to likelihood of candidates due to the behavior of the “head”.
To ﬁx this, we present a detailed observation of the “head” in Figure 3. It shows that lowerprobability candidates on a ﬂat distribution are actually reasonable but more surprising with higher diversity. Consequently, it is possible to increase diversity by emphasizing on less probable candidates on ﬂat distributions without compromising the rationality of the distribution as well as the ﬂuency of the generated text.
Intuitively, this can be achieved similarly to the inverse probability weighting technique that is commonly seen in causal inference (see Chapter 2, Hernán MA, Robins JM, 2020). Inspired by this, as long as we can identify a small subset of candidates (i.e., the “head”) of the distribution that contains all reasonable candidates (such as in Figure 3), we may use inverse probability weighting to rescale the distribution for these candidates to suppress repetition and increase diversity without

compromising ﬂuency.
Repetitive candidate often has higher rank. Flat distribution

Lower probability candidates on flat distributions are actually reasonable but more diverse and surprising.
Figure 3: Illustration of the “Head” on the ﬂat distribution of the ﬁrst sampling step of Loop #1 from Figure 1. Besides “She” that has highest predicted probability, lower probability candidates (“\n”, “He”, “I”, “The”, ...) are also reasonable but more surprising with higher diversity. If using inverse probability weighting to emphasize on these candidates, the ﬂuency of samples will not be compromised, while repetition will be suppressed and diversity will be improved.

3 Interquartile Range Inverse Probability Sampling Algorithm

3.1 Use Interquartile Range to Identify the “Head”

Clearly, the major difﬁculty in identifying the “head” is the variation of the shape of the distribution, i.e., the discrepancy between ﬂat distribution and peaked distribution. Intuitively, the interquartile range (IQR) can adapt to such variation since it is based on quantile calculation and does not have strict requirements for the shape of the distribution.
As a result, we propose to adopt IQR to identify the “head” for permutation. First, we need to ensure that only the most reliable candidates are kept in order not to interfere with the identiﬁcation of the “head”. Following the common ﬁltering method of stochastic sampling, we propose to jointly ﬁlter an initial subset V K0 of candidates with p and k as follows.

V K0 = V k ∩ V p.

(5)

Let pfil(x) denote the regularized distribution on V K0. We propose to calculate IQR of pfil(x), that
is, calculate 75% percentile as Q3, 25% percentile as Q1, IQR = Q3 − Q1, and divide V K0 into
subsets as follows.

IQR Subset Division of V K0:
V V eryHigh : pfil(x) ≥ Q3 + ρ × IQR V High : Q3 + ρ × IQR > pfil(x) ≥ Q3 , (6)
V Medium : Q3 > pfil(x) ≥ Q1 V Low : Q1 > pfil(x) ≥ Q1 − ρ × IQR
where ρ is the hyper parameter for the coefﬁcient of IQR with typical value being 1.5. Considering the outlier-identiﬁcation nature of IQR, V V eryHigh can be regarded as the “head” part that we need to permutate, which we expect that the least probable candidate in V V eryHigh is still likely to be “high enough” to be reasonable choices.
Since IQR is based on quantile, V V eryHigh is empirically to be non-singleton on ﬂat distribution only, hence permutation on V V eryHigh will not interfere with peaked distribution which may compromise rationality of the distribution. See Appendix C for more discussions.
3.2 “Leakage” of the “Tail” on Peaked Distribution Interferes with Identiﬁcation of the “Head”

0.49 0.45

Peaked distribution with more than one peak values

0.0054
0.0024 0.0016 0.0016 0.0016 0.0012 0.0011 0.0008

“Tail” that is too “far” from peak values. Nucleus sampling did not consider the relative “shape” or “distance” constraints.
Figure 4: The incurring of “leakage” of the “tail” on peaked distribution that has more than one peak value. On such distribution, small value of p for nucleus sampling will miss the second peak, while large value of p will easily let in low-probability candidates close to the peak (i.e., “leakage”), which will interfere with the identiﬁcation of the “head”. This distribution is also selected from one of the generated samples using GPT-2 Small model.
Before proceeding, we take a deeper look for “tail” part on peaked distribution. As is studied by Holtzman et al. (2020), nucleus sampling can adaptively truncate low-probability “tails” on peaked distribution, while top-k sampling can’t (see Figure 5, Holtzman et al., 2020).
We consider a special case which is not considered by Holtzman et al. (2020) . Figure 4 presents an actual example of peaked distribution with more

than one peak value. In this case, small value of p for nucleus sampling will miss the second peak, while large value of p will easily let in lowprobability candidates, i.e., resulting in “leakage”. Although such leakage might affect very little on sampling (since the “leakage” part has low probability), but clearly it will affect the identiﬁcation of “head” (since IQR calculation is based on quantile), hence cannot be ignored.
We argue that the incurring of leakage is because neither top-k sampling nor nucleus sampling considers the relative “shape” or “distance” constraints during ﬁltering. To ﬁx this, we propose a new ﬁltering metric to further exclude low-probability candidates that is too “far” from the peaked ones. We deﬁne a threshold that is the fraction of the maximum probability on a predicted distribution, and exclude candidates with probability below that threshold, which we name as the “top-1 controlled” (top1ctrl) ﬁltering metric with parameter n as follows.

V n = x | p(x) ≥ max p(x)/n, x ∈ V . (7)

We propose to use this metric to prune V K0 (on the basis on joint vocabulary ﬁltering in Equation 5) in a dynamic way. Our method is described in the following equations, in which we denote the pruned set to be V K1.

V V eryHigh ∪ V High,

if V n ⊆



 V K1 =

V V eryHigh ∪ V High . (8)

 V K0 ∩ V n,

otherwise

The ﬁrst sub-equation ensures that V n does not truncate any candidates categorized as “Very High” or “High”, since they are identiﬁed by IQR and likely to contain rational candidates. In this case we drop all candidates in V Medium and V Low, because they are considered too “far” from maximum value in the distribution. And the second sub-equation describes other cases where V n works jointly with V k and V p in a straight-forward way. Practically n is set to a fairly loose value of 100 in our experiment in order to function correctly with top-k ﬁltering and nucleus ﬁltering and not to over-prune V K0 .

3.3 Inverse Probability Permutation on the “Head”
With V K1 acquired, we propose to re-assign probability mass for each candidate in V V eryHigh (i.e.,

the “head”) proportionally to its inverse probabil-
ity, while keeping the sum of probability mass in V V eryHigh constant. In this way, distribution of
the “head” is rescaled and has inverse monotonicity, while distribution on V K1 still maintains the
probability distribution feature. For simplicity, now let pfil(x) denote the regularized distribution on V K1. The permutation on V V eryHigh is described
as follows.

pinv(x) =

pfil(x) ×
x∈V V eryHigh

pfil(x)−1 , x∈V V eryHigh pf il(x)−1
(9)

where pinv(x) denotes the permutated distribution, and pinv(x) outside V V eryHigh remains the same

as pfil(x). Finally the stochastic sampling is per-

formed according to pinv(x). We refer to the above

algorithm as the interquartile range inverse proba-

bility (IQR-IP) sampling algorithm. We summarize

the main differences of our algorithm as follows.

• We use dynamic vocabulary ﬁltering with 3 parameters (p, k, and n). This aims at guaranteeing the correct identiﬁcation of the “head” of the distribution.

• Distribution of the “head” identiﬁed by IQR is permutated using Equation 9. This aims at improving diversity by decreasing the probability of tedious and possibly repetitive candidates with high probability and increasing the probability of reasonable but more surprising candidates with low probability.

3.4 Total Variance Analysis
We provide total variance analysis to explain the behavior of our algorithm. Following proposition by Kang and Hashimoto (2020), we can evaluate the permutation by analyzing the upper bound of total variance between pinv(x) and reference distribution pref (x) with the following corollary.

Corollary 1. Upper bound of total variance between pinv and pref satisﬁes

|pinv − pref |2 ≤ 1 KL(pref ||pfil) + 2m + m2, (10) 2

where

m = max |pfil − Zp |,

x∈V V eryHigh

pf il

(11)

Zp =

p x∈V V eryHigh f il p . x∈V V eryHigh f il−1

(12)

See Appendix A for proof.

Equation 10 reveals an additional term

controlled by m besides the original bound

1 2

K L(pref

||pf il )

(achieved

by

pf il

without

inverse

probability permutation). Since m contains

an value of inverse probability, the new upper

bound will change dramatically. This provides a

controllable diversity enhancement measure. See

Appendix B for more analysis.

4 Evaluation
4.1 Experiment Setup
The primary goal of the evaluation is to test whether our methods generate ﬂuent samples with higher diversity. We consider the following principles when choosing baselines.

• Ablation of permutating the “head”. This means the baseline method should be without permutation, i.e., choosing plain stochastic sampling that only truncates the “tail” for comparison.

• Fair comparison on human-level PPL. This means the baseline method as well as our method should already achieve close PPL to human text like Figure 6 by Holtzman et al. (2020), i.e., choosing hyper parameters near the intersection points with human PPL for fair comparison.

• Impact of model size. This answers the question that does model size affect the conclusion from our experiments. We choose the smallest and largest plain auto-regressive Transformer language models from GPT-2 family for interpolative conclusions.

We use pre-trained GPT-2 Small (117M parameters) and GPT-2 XL (1,542M parameters) released by Wolf et al. (2019). Following identical settings by Holtzman et al. (2020), we set maximum length of generation to be 200 and generate 5,000 samples for each sampling method with the same context in Section 2.2. We set ﬁxed value of n = 100 for top1ctrl ﬁltering and ρ = 1.5 for IQR.

4.2 Statistical Evaluation
We ﬁrst follow the statistical evaluation procedure by Holtzman et al. (2020), which evaluates the following metrics (closer score to the human metric is better).

• Perplexity. This metric is calculated on the generated texts with the per-trained model to reﬂect its general quality and ﬂuency. Lower score indicates higher quality.
• Self-BLEU (4 and 5) (Holtzman et al., 2020; Zhu et al., 2018). One sample is calculated against all other samples to reﬂect diversity among all samples. Lower score indicates higher diversity.
• Zipf coefﬁcient (Zipf, 1949; Newman, 2005). This metric represents linguistic feature of word frequency distribution. Lower score indicates more ﬂat distribution of words and higher diversity.
• Repetition. We directly take Hrep from Equation 3 to evaluate repetition tendency, which reﬂects diversity within the sample. Higher score indicates less repetition and higher diversity.
As is shown in Figure 5(a) and 5(f), the PPL of generated samples using our algorithm can also achieve human level perplexity but with more strictly ﬁltered vocabulary, which means our algorithm truncates more low-probability “tails” and still achieves equal PPL to human text, which is a desirable feature, since low-probability “tails” that contain unreasonable candidates will lower the quality of the generated text. This indicates that our algorithm relieves text degeneration not only by letting in the “tails” but also by permutating the “head”, unlike traditional methods that solely rely on the “tails”.
Note that our algorithm is highly sensitive to ﬁltering metric, which is caused by the fast increase of additional term m from Corollary 1 when loosening the ﬁltering. Such diversity gain will be destructive (e.g., for p > 0.9), because the inverse value in term m will grow too big and “blow up” the algorithm. Thus the intersection points with human PPL is the reasonable choices for our algorithm.
As is clearly shown in Figure 5(b) and 5(h), the Self-BLEU scores achieved by our algorithm decrease signiﬁcantly faster than nucleus sampling, which indicates great diversity gain. Note that it can achieve almost the same score with “pure sampling” near p = 0.999 that represents highest diversity in traditional methods. This means that our

Model

Method

GPT-2 Small GPT-2 XL

Human
Nucleus, p = 0.9 Top-k, k = 200
IQR-IP (ours) p = 0.8, k = 640
Human
Nucleus, p = 0.9 Top-k, k = 200
IQR-IP (ours) p = 0.8, k = 640

PPL
29.41 30.64 25.14 32.88
18.34
17.09 17.86 16.77

Statistical Evaluation

Self-BLEU 4 Self-BLEU 5

Zipf Coef.

0.31

0.17

0.93

0.42

0.26

1.27

0.46

0.29

1.24

0.43

0.27

1.05

0.31

0.17

0.93

0.44

0.28

1.49

0.44

0.27

1.41

0.47

0.29

1.17

Hrep 4.50
4.55 4.52 4.61
4.50
4.42 4.45 4.45

Human Evaluation

Fluency ↑

Diversity ↑

Overall ↑

-

-

-

3.78

4.44

4.11

3.74

4.56

4.15

3.87

4.64

4.25

-

-

-

4.61

4.53

4.57

4.56

4.64

4.60

4.64

4.70

4.67

Table 1: Statistical evaluation (closer metric to human text is better) and human evaluation (higher score is better) for selected decoding parameters. Note that our algorithm can achieve human level PPL with less repetition (with high Hrep). Also note the Zipf coefﬁcient of our algorithm is much closer to human metric and unable to achieve by traditional methods. Human evaluation shows that our algorithm can achieve similar ﬂuency but higher diversity.

Human, 29.41

Human, 0.31

Human, 0.17

Human, 0.93

012345ÿ789

(a) Perplexity for GPT-2 (b) Self-BLEU 4 for

Small. Horizontal line

GPT-2 Small.

(29.41, Radford et al., Horizontal line (0.31,

2019) refers to human Holtzman et al., 2020)

text.

refers to human text.

(c) Self-BLEU 5 for GPT-2 Small.
Horizontal line (0.17, Holtzman et al., 2020) refers to human text.

(d) Zipf coefﬁcient for GPT-2 Small.
Horizontal line (0.93, Holtzman et al., 2020) refers to human text.

(e) Hrep for GPT-2 Small. Horizontal line (4.50) refers to human
text on test set of WikiText-2

Human, 18.34

Human, 0.31

Human, 0.17

Human, 0.93

Human, 4.50

(f) Perplexity for GPT-2 (g) Self-BLEU 4 for

XL. Horizontal line GPT-2 XL. Horizontal

(18.34, Radford et al., line (0.31, Holtzman

2019) refers to human et al., 2020) refers to

text.

human text.

(h) Self-BLEU 5 for GPT-2 XL. Horizontal line (0.17, Holtzman et al., 2020) refers to
human text.

(i) Zipf coefﬁcient for GPT-2 XL. Horizontal line (0.93, Holtzman et al., 2020) refers to
human text.

(j) Hrep for GPT-2 XL. Horizontal line (4.50) refers to human text on test set of WikiText-2

Figure 5: Statistical results and metric behavior comparison with nucleus sampling. They show that our algorithm achieves human level metrics with more strict ﬁltering parameters (i.e., with less “tail”), which is contributed by the diversity gain from inverse probability permutation on the “head”. Also note that the behavior of Zipf coefﬁcient of our algorithm (with intersection to human metric) is signiﬁcantly different from nucleus sampling (without intersection), because our algorithm encourages to sample on less probable tokens of the “head” which renders more ﬂat distribution of the vocabulary and achieves closer resemblance to human text.

algorithm achieves signiﬁcantly higher diversity but with less “tails”.
As is shown in Figure 5(d) and 5(i), our algorithm can ﬁt identical Zipf coefﬁcient to human text, while nucleus sampling can’t. This indicates that the permutation of our algorithm renders more ﬂat vocabulary distribution (by encouraging sampling on less probable tokens) which is less concentrated, closer to human text and is unable to achieve by plain stochastic sampling (which always picks highprobability candidates and results in peaked and

less diverse usage of vocabulary). Results for repetition are shown in Figure 5(e)
and 5(j). Similar to results for Self-BLEU scores, they also show that Hrep of our algorithm grows faster and stays higher than nucleus sampling, which represents less repetition and higher diversity.
4.3 Human Evaluation
We collect 117 copies of human annotations per each sampling algorithm on ﬂuency (focusing on

Human
GPT-2 Small, nucleus p=0.9
GPT-2 Small, Top-k k=200
GPT-2 Small, IQR-IP
p=0.8, k=640 (ours)
GPT-2 XL, nucleus p=0.9
GPT-2 XL, Top-k k=200
GPT-2 XL, IQR-IP
p=0.8, k=640 (ours)

She walks in beauty, like the night \n Of cloudless climes and starry skies; \n And all that’s best of dark and bright \n Meet in her aspect and her eyes; \n Thus mellowed to that tender light \n Which heaven to gaudy day denies. \n One shade the more, one ray the less, \n Had half impaired the nameless grace \n Which waves in every raven tress, \n Or softly lightens o’er her face; \n Where thoughts serenely sweet express, \n How pure, how dear their dwellingplace. \n And on that cheek, and o’er that brow, \n So soft, so calm, yet eloquent, \n The smiles that win, the tints that glow, \n But tell of days in goodness spent, \n A mind at peace with all below, \n A heart whose love is innocent!
She walks in beauty wheel chair (The Art Of Sleeping). Despite her excessive muscularity, who knows how she'll dress. Whispered last night by Georgina... See full summary »\n\n 07:29 She walks in beauty wheel chair (The Art Of Sleeping). Despite her excessive muscularity, who knows how she'll dress. Whispered last night by Georgina... See full summary »\n\n 07:17 She walks in beauty wheel chair (The Art Of Sleeping). Despite her excessive muscularity, who knows how she'll dress. Whispered last night by Georgina... See full summary »(…)
She walks in beauty salon, she shakes hands, but what she was getting me was I wasn't even in shock... and that was the most helpful thing I could've done because she wasn't so "judgmental to me" ... It's unfortunate that my heart hurts when I see some bad interactions such as this but that's the sweet start of my career, is it not? \n\n -- Kelly Knapp \n\n ALSO ON HOLLYWOOD! \n\n -- Andrew Ryan \n\n ALSO ON HOLLYWOOD! \n\n -- Chris \n\n ALSO ON HOLLYWOOD! \n\n -- Richard Rumsfeld \n\n ALSO ON HOLLYWOOD! (…)
She walks in beauty at once. A simple life could not exist without me. My words could not possibly exist without you. She watches as my thoughts, even if their sound and pain and grief echo across the ocean and sky, build on the last gasp of hope that gave life to this island. Her life could not take a stand against us. It is something we will always hold on to. She will be right back where she left off. It will be the best thing for the family to do for her. Her arms hug my chest. Her mother. My father. We walk in the bright sunshine, laughing and praying. I love the sunshine. It's what I always do. As she leaves, she makes her way back into the ship. My life can never change. I am lost.
She walks in beauty and elegance, and she is a natural leader." Weiner's future with the department is still undecided. \n\n The Post's full story continues here. \n\n NOW WATCH: 'Hiroshima' -- A huge robot briefly took over Tokyo's tallest building on Thursday morning \n\n More From Business Insider \n\n More From Business Insider \n\n More From Business Insider \n\n More From Business Insider \n\n More From Business Insider \n\n(…)
She walks in beauty and grace that makes me want to do good things at my home. \n\n My god isn't this a movie without the female lead \n\n They break the glass ceiling yet again, and with great power! \n\n My god isn't this a movie without the female lead? \n\n My god isn't this a movie without the female lead? \n\n My god isn't this a movie without the female lead? \n\n My god isn't this a movie without the female lead? (…)
She walks in beauty; a light has fallen upon her beauty, and I cannot for the life of me think of saying how or why; the place was illuminated by the sunshine which it gave out, and that made me love it. It was at the centre of the road, at the gate; when I went to look at it, I was startled to see my heart swelling, and rising up to the skies, as if it were alive and striving to free itself from its imprisonment in my own feelings. All its bright gleams made me look on it with wonder and interest, and not a soul on earth could do justice to it; and my affection for it became the object of my reverie, for I longed to be in the same state of joy that it excited. This love and reverence for it I conveyed to her by writing letters, and when she read them, she did not laugh, but gave me that proud smile of hers which made me love her more.

Table 2: Generated examples using different sampling methods that have average perplexity near human text. Repetition is marked in red with underline. They show that traditional methods might still generate repetitive sentences, because they only focus on truncating the “tail” and ignore the “head”, while our algorithm generates more diverse, more surprising texts without hurting ﬂuency by permutating the “head”.

grammar error, linguistic clarity and consistency) and diversity (focusing on boredom, wordiness and repetition) on a 1-5 scale (larger better). Results are shown in Table 1. It shows that our algorithm achieves similar ﬂuency score to traditional methods (because they all achieve human-level PPL), which suggests the correct manipulation for the distribution that does not compromise the rationality of the distribution. On the other hand, our algorithm can achieve higher diversity score, which is contributed by the inverse probability permutation that emphasizes on less probable tokens from the “head”. Note that the corresponding Zipf coefﬁcient of our algorithm indicates more ﬂat and diverse distribution of the vocabulary and closer resemblance to human text. And higher Hrep of our algorithm indicates less repetition. These results clearly suggest diversity gain from our algorithm.
The diversity gain will also reﬂect on the style of the generated text. We present samples from our experiment in Table 2. It can be seen that our algorithm favors creating diverse and surprising sentences without sacriﬁcing ﬂuency, while traditional method favors creating comparatively plain and

ordinary sentences. Such difference of language style is also contributed by the inverse probability permutation, which suppresses the sampling for unsurprising high-probability tokens on ﬂat distributions.
5 Conclusion and Future Work
In this work we propose the interquartile range inverse probability sampling algorithm. It brings reasonable permutation on the “head” of the predicted distribution to enhance diversity without sacriﬁcing ﬂuency. We evaluate our algorithm with pre-trained language models and compare it with traditional stochastic sampling methods. Results show that our algorithm can generate ﬂuent samples with higher diversity and less repetition compared with traditional methods.
Our results reveal a possible direction of discouraging sampling according to likelihood on ﬂat distributions to increase diversity without hurting ﬂuency. This might lead to interesting results for other decoding algorithm (such as MIROSTAT, Basu et al., 2021) or generation tasks (such as summarization).

References
Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R. Varshney. 2021. MIROSTAT: A neural text decoding algorithm that directly controls perplexity. In International Conference on Learning Representations.
Imre Csiszár and János Körner. 2011. Information Theory: Coding Theorems for Discrete Memoryless Systems, 2 edition. Cambridge University Press.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics.
Hernán MA, Robins JM. 2020. Causal Inference: What If. Chapman & Hall/CRC, Boca Raton.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.
Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1638–1649, Melbourne, Australia. Association for Computational Linguistics.
Daniel Kang and Tatsunori Hashimoto. 2020. Improved natural language generation via loss truncation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 718–731, Online. Association for Computational Linguistics.
Mark EJ Newman. 2005. Power laws, pareto distributions and zipf’s law. Contemporary physics, 46(5):323–351.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019.

Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’18, pages 1097–1100, New York, NY, USA. Association for Computing Machinery.
George K. Zipf. 1949. Human Behaviour and the Principle of Least Effort. Addison-Wesley.

A Proof of Corollary

First, with Pinsker’s inequality (Csiszár and Körner, 2011), the total variance between the original ﬁltered distribution pfil and the reference distribution pref satisﬁes

|pf il

−

pref |2

≤

1 KL(pref ||pfil).

(13)

2

Then we may use similar methods by Kang and Hashimoto (2020) to derive the new bound as follows.

Proof.

|pinv − pref |2 ≤ (|pinv − pfil| + |pfil − pref |)2 (14)
By deﬁnition of pinv in Equation 9, we have

|pinv − pfil|2 ≤ max |pfil − Zp |. (15)

x∈V V eryHigh

pf il

Then expand Equation 14, and use m deﬁned in Equation 11 and 15 to bound |pinv − pfil|, and use Equation 13 to bound |pfil − pref |, the inequality is proved.

This corollary has the same form as Kang and Hashimoto (2020), although with different constant m, which corresponds to the truncation ratio c of their proposition. In our work, m is controlled by inverse probability permutation and can be fairly large, while the truncation ratio c satisﬁes 0 ≤ c ≤ 1. In this way, it can be regarded as an extension from proposition by Kang and Hashimoto (2020) in a different scenario.
Note that since 0 < Zp ≤ 1, max |pfil − pZfpil | can only be achieved on the largest or smallest value of pfil in V V eryHigh, i.e., on the ﬁrst or last candidate of V V eryHigh. As a result, m is controlled by ρ in Equation 6 and ﬁltering parameters in Equation 8. For example, with a loosely ﬁltered V K1, V V eryHigh might contain a last candidate with too small value of probability and render too large value of m, hence the total variance will become too high and corrupt the algorithm. However, with carefully chosen parameters, m may provide reasonable variation that enhances diversity and reduces repetition, as is shown in the evaluation results.

B Ablation Study

We present ablation study of IQR coefﬁcient and top1ctrl ﬁltering in Table 3. Clearly, when ρ in

Method
GPT-2 XL IQR-IP p = 0.8
k = 640 n = 100 ρ = 1.5
ρ = 3.0 ρ = 5.0 ρ = 10.0 ρ = 50.0
n = 10 n = 50 n = 200 n = 1000

PPL Self-B4 LEU Self-B5LEU Zipf Coef. Hrep

16.77

0.47

0.29

1.17

4.45

14.90

0.50

0.32

1.22

4.39

12.76

0.52

0.34

1.26

4.34

11.57

0.53

0.36

1.39

4.30

9.62

0.55

0.39

1.54

4.19

13.39

0.53

0.35

1.22

4.35

16.50

0.48

0.30

1.17

4.43

19.48

0.45

0.28

1.15

4.47

20.52

0.44

0.27

1.15

4.48

Table 3: Ablation study of IQR coefﬁcient and top1ctrl ﬁltering for GPT-2 XL.

Equation 6 increases, it shortens the identiﬁcation range of V V eryHigh hence decreasing the intensity of inverse probability weighting, which leads to more repetition (with higher Self-Bleu score and lower Hrep), more concentrated distribution of vocabulary (with higher Zipf coefﬁcient), more plain and unsurprising sentences (with lower PPL). As a result, ρ can be used to control the diversity gain that results in style difference. For example, one may need to tune ρ to higher values, if the generated texts seem to lose ﬂuency and have too many obscure sentences (this may be more suitable for artistic generation that requires high diversity and creativity such as poetry or music generation). If ρ is set to inﬁnity, there will be no V V eryHigh and our algorithm will degrade to plain stochastic sampling ﬁltered by Equation 5 and 8 (this may be more suitable for tasks that require high ﬂuency such as summarization or translation).
For the ablation of top1ctrl ﬁltering, Table 3 clearly shows that loosening n will be harmful, since generated samples will lose quality (with higher PPL). Although this results in less repetition and higher diversity (with lower Self-Bleu score and higher Hrep), but clearly due to the “leakage” of tail described in Section 3.2, the diversity gain will be destructive which is introduced by candidates with too low probability that interfere with the identiﬁcation of V V eryHigh, which is also reﬂected by the decrease of Zipf coefﬁcient that represents more ﬂat distribution of vocabulary. On the other hand, small value of n will over-prune the vocabulary, which indirectly decreases the range of V V eryHigh hence decreasing the intensity of inverse probability weighting, resulting in lower

diversity and more repetition.
C Further Explanations on IQR
A possible concern of IQR is whether it will interfere with peaked distribution that has only a few reasonable candidates (e.g., 1 or 2) with high probability in V K0. Note that by deﬁnition of IQR, it will only put “outliers” in V V eryHigh. Clearly, for V K0 with less than 4 candidates, they will be partitioned among the “middle part” of subsets, i.e., symmetrically distributed on V High, V Medium and V Low. As a result, on highly peaked distribution with only a few “unquestionably correct” candidates with high probability in V K0, there will be no V V eryHigh as we have observed, which means that the inverse probability permutation won’t work and the algorithm will degrade into plain stochastic sampling. This indicates that IQR can adaptively work on ﬂat distribution and peaked distribution without compromising ﬂuency.
Another issue to clarify is that by the deﬁnition of IQR, there should be a V V eryLow that locates symmetrically to V V eryHigh on the identiﬁcation range. In our experiment we found that this boundary is always below 0, i.e., V V eryLow is always empty set during IQR calculation. As a result, we omit the narration for V V eryLow.
Note that one may even design different and more “mild” permutation strategies besides Equation 9, e.g., evenly redistributing V V eryHigh, or simply adding some noise on V V eryHigh, to achieve a less severe permutation bounded by Equation 15. In that case, our algorithm is actually an extreme case that we completely re-order V V eryHigh with inverse probability which brings signiﬁcant permutation on the predicted distribution.

