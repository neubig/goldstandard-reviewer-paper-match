Approval Voting and Incentives in Crowdsourcing

Nihar B. Shah
UC Berkeley nihar@eecs.berkeley.edu

Dengyong Zhou
Microsoft Research dengyong.zhou@microsoft.com

Yuval Peres
Microsoft Research peres@microsoft.com

arXiv:1502.05696v3 [cs.GT] 7 Sep 2015

Abstract
The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (“strictly proper”) incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.
1 Introduction
In the big data era, with the ever increasing complexity of machine learning models such as deep learning, the demand for large amounts of labeled data is growing at an unprecedented scale. A primary means of label collection is crowdsourcing, through commercial web services like Amazon Mechanical Turk where crowdsourcing workers or annotators perform tasks in exchange for monetary payments. Unfortunately, the data obtained via crowdsourcing is typically highly erroneous (Kazai et al., 2011; Vuurens et al., 2011; Wais et al., 2010) due to the lack of expertise of workers, lack of appropriate incentives, and often the lack of an appropriate interface for the workers to express their knowledge. Several statistical aggregation methods (Dawid and Skene, 1979; Whitehill et al., 2009; Raykar et al., 2010; Karger et al., 2011; Liu et al.,

What is the language in this image? Latin Thai Tamil Japanese Hebrew Chinese Russian Hindi
(a)

Select ALL options that could be the language in this image
Latin Thai Tamil Japanese Hebrew Chinese Russian Hindi
(b)

Figure 1: Illustration of a task with (a) the standard single selection interface, and (b) an approval-voting interface.

1

2012; Zhou et al., 2012; Shah et al., 2015) have been proposed in the literature for improving the quality of the data. Our approach complements these techniques in that we endeavor to obtain higher-quality labels directly via novel interface and incentive mechanisms while not increasing the labeling cost.
The typical crowdsourcing labeling task consists of a set of questions such as images to be labeled, and each question is associated with a set of options. Each option is the name of a category and the true label for any question is one of these options. In principle, for each question, the worker is required to select the option that she believes is most likely to be correct. More formally, it involves eliciting the mode of the worker’s belief. Such a “single-selection” crowdsourcing setting has been studied extensively, both empirically and theoretically.
In this paper, we consider an alternative “approval-voting” means of eliciting labels from the workers, wherein the worker is allowed to select multiple options for every question.1 See Figure 1 for an example. Approval voting is known to have many advantages over single-selection systems in psychology and social choice theory (Horst, 1932; Coombs, 1953; Coombs et al., 1956; Collet, 1971; Brams and Fishburn, 1978; Gibbons et al., 1979): it provides workers more ﬂexibility to express their beliefs, and utilizes the expertise of workers with partial knowledge more effectively. For instance, Coombs (1953) posits that “It seems to be a common experience of individuals taking objective tests to feel conﬁdent about eliminating some of the wrong alternatives and then guess from among the remaining ones” and that “Individuals taking the test should be instructed to cross out all the alternatives which they consider wrong.” Under this approval-voting interface, we will require a worker to select every option which she believes could possibly be correct. Mathematically, we formulate this problem as eliciting the support of the beliefs of workers for each question. In the setting of crowdsourcing, as compared to single-selection, selecting multiple options would allow for obtaining more information about the partial knowledge of these non-expert workers. This additional information is particularly valuable for difﬁcult labeling questions, allowing for the identiﬁcation of the sources of difﬁculty. Indeed, Coombs et al. (1956) conclude that under such a questionnaire, “clear evidence for the existence of partial information mediating responses to multiple choice items was obtained.”
Let us illustrate the utility of approval voting using an example in Figure 1. Assume that there are two workers. The ﬁrst worker believes the true label to be either “cheetah” or “leopard”, but certainly not any other option; the second worker is confused about some other aspect of the image, and believes the true label to be either “cheetah” or “jaguar”, but certainly none of the others. If each worker is allowed to select only a single answer, it may turn out that the ﬁrst worker selects “leopard” and the second worker selects “jaguar”. Their responses will thus not provide any deﬁnitive answer about the true label. In contrast, if we fully elicit their knowledge by letting them select multiple options, that is, (“cheetah”, “leopard”) from the ﬁrst worker and (“cheetah”, “jaguar”) from the other worker, then “cheetah” becomes a clear winner.
Albeit its great ﬂexibility in eliciting partial knowledge, approval voting alone is not sufﬁcient for high quality crowdsourcing. A worker may have no incentive to truthfully disclose her partial knowledge on the crowdsourcing question. For instance, the worker may simply choose all provided options as her answer and get paid. To address this problem, we need to couple approval voting with an appropriate “incentivecompatible” payment mechanism such that a worker receives her maximum expected payment if and only if she truthfully discloses her partial knowledge (that is, the support of her belief) on the crowdsourcing question. In other words, the payment mechanism has to be a “strictly proper scoring rule”. Moreover, we want the mechanism to be “frugal”, paying as less as possible to a worker who simply selects all provided options as her answer. The problem setting for incentive mechanism design is formally described in Section 2.
Our ﬁrst result is negative, proving that unfortunately no mechanism can be incentive compatible for this setting (Section 3). This impossibility result leads us to introduce a “coarse belief” assumption that relies on a certain granularity in people’s beliefs.
Our next result is the design of a payment mechanism and associated proofs showing that our mechanism
1The literature on psychology often refers to approval voting as “subset selection”.
2

is incentive compatible and frugal (Section 4). Furthermore, we show that it is the only mechanism which satisﬁes these two requirements.
We then generalize the analysis of our mechanism to settings where the coarse belief assumption may not be satisﬁed, and show that our mechanism simply incentivizes workers to select options for which their belief is relatively high enough (Section 5). This perspective also leads to a simple axiomatic characterization of our mechanism.
We then report results from preliminary experiments verifying certain basic hypotheses underlying our approach (Section 6). The paper then diversiﬁes to investigate two closely related settings, that of general utility functions (Section 7) and that of a problem of reporting only high enough beliefs (Section 8). The paper concludes with a discussion in Section 9.
Related literature
Approval voting (Ottewell, 1977; Kellett and Mott, 1977; Weber, 1977; Brams and Fishburn, 1978) is a form of voting in which each voter can “approve of” (that is, select) multiple candidates. No further preferences among these candidates is speciﬁed by the voter. Our proposed interface for crowdsourcing elicits approvals on the candidate options for each question. Closer to our setting of crowdsourcing, approval voting has been studied in the context of question and answer forums (Jain et al., 2009) and Doodle polls (Zou et al., 2014). The focus of the present paper is on the design of incentive mechanisms with properties that fundamentally hold irrespective of the nature of the setting.
The framework of scoring rules (Brier, 1950; Savage, 1971; Gneiting and Raftery, 2007; Lambert and Shoham, 2009) considers the design of payment mechanisms to elicit predictions about an event whose actual outcome will be observed in the future. The payment is a function of the agent’s response and the outcome of the event. The payment is called “strictly proper” if its expectation, with respect to the belief of the agent about the event, is strictly maximized when the agent reports her true belief. Proper scoring rules however provide a very broad class of mechanisms, and do not specify any particular mechanism for use. The mechanism proposed in the present paper may alternatively be viewed as the “optimal” proper scoring rules for eliciting supports of workers’ beliefs across multiple questions.
Shah and Zhou (2015) consider a crowdsourcing setup with the traditional single-selection setting, also eliciting the workers’ conﬁdences for each response. They propose a mechanism to suitably incentivize workers and show that their proposed mechanism is shown to be the only one satisfying a proposed “no-freelunch” axiom. While the setting of our work is different from that of Shah and Zhou (2015), interestingly, our mechanism that was derived for a different interface and under a different set of assumptions, turns out to be the only mechanism that can satisfy the no-free-lunch axiom (adapted to our setting).
The mechanisms presented subsequently in the present paper assume the presence of some “gold standard” questions whose answers are known apriori to the system designer. There is a parallel line of literature (Prelec, 2004; Miller et al., 2005; Faltings et al., 2014; Miller et al., 2005; Dasgupta and Ghosh, 2013) that explores the design of mechanisms that operate in the absence of any gold standard questions. These works typically elicit additional information from the workers, such as asking them to predict the responses of other workers. The mechanisms designed therein can generally provide only weaker guarantees due to the absence of a gold standard answer to compare with.
2 Problem setup
Consider N ≥ 1 questions, each of which has B ≥ 2 options to choose from. For each option, exactly one of the B options is correct. We assume that these N questions contain G (1 ≤ G ≤ N ) “gold standard” questions, that is, questions to which the mechanism designer knows the answers apriori. These
3

gold standard questions are assumed to be mixed uniformly at random among the N questions, and the worker is evaluated based on her performance on these G questions. For every individual question, we assume that the worker has, in her mind, a distribution over the B options representing her beliefs of the probabilities of the respective options being correct. We assume that these belief-distributions of a worker are independent across questions (Gibbons et al., 1979). For any integer K, we will use the standard notation of [K] as a shorthand for the set {1, . . . , K}.
Our goal is to elicit, for every question, the support of the worker’s distribution over the B options. In other words, we wish to incentivize the worker such that for each question, the worker should select the smallest subset of the set of options such that the correct answer according to her belief lies in the selected subset. Formally, suppose that for any question i ∈ [N ], the worker believes that the probability of option b ∈ [B] being correct is pib, for some non-negative values pi1, . . . , piB that sum to one. Then the goal is to incentivize the worker to, for each question i ∈ [N ], select precisely the set of options

{b ∈ [B] | pib = 0}.

(1)

Payment function. As mentioned earlier, the worker’s performance is evaluated based on her responses to the gold standard questions. For any question in the gold standard, we denote the evaluation of the worker’s performance on this question by a value in the set {−(B − 1), . . . , −1, 1, . . . , B}: the magnitude of this value represents the number of options she had selected and the sign is positive if the correct answer was in that subset and negative otherwise. For instance, if the worker selected four options for a certain gold standard question but none of them was correct, then the evaluation of this response is denoted as “−4”; if the worker selects two options for a gold standard question and one of them turns out to be the correct option then the evaluation of this response is denoted as “+2”.
We will assume that the payments are bounded, that is, any payment must lie in the interval [αmin, αmax], for some values αmin and αmax > αmin. The choice of the two parameters αmin and αmax may be made keeping various factors in mind, such as guidelines of the crowdsourcing platform used, the budget constraints, and the minimum wage. We will assume that the values of the two parameters are given to us.
Let
f : {−(B − 1), . . . , −1, 1, . . . , B}G → [αmin, αmax]

denote the payment function. It is this function f which must be designed in order to incentivize the worker. We will let that a worker who answers everything perfectly should be paid an amount αmax, that is,

f (1, . . . , 1) = αmax.

(2)

Expected payment. A quantity central to our analysis is the expected payment, where the expectation is from the point of view of the worker, and is taken over the randomness in the choice of the G gold standard questions among the N questions, and over the N probability distributions representing her beliefs for the N questions. Let us formalize this notion. Suppose that for question i ∈ [N ], the worker has selected some yi ∈ [B] of the B options. Further, let si ∈ [0, 1] denote the probability, under the worker’s beliefs, that the correct answer to question i lies in this set of yi selected options. In other words, si denotes the sum of the beliefs for the yi options selected by the worker (consequently, the sum of the beliefs for the options not selected is (1 − si)). Then from the worker’s point of view, her expected payment for this selection is

1

G 1{ =−1} 1{ i=1}

N

(1 − sji ) i

sji

f ( 1yj1 , . . . , GyjG ) .

(3)

G (j1,...,jG)⊆[N ] ( 1,..., G)∈{−1,1}G i=1

The outer summation in (3) corresponds to the expectation with respect to the random distribution of the G gold standard questions in the N total questions, and the inner summation corresponds to the expectation with respect to the worker’s beliefs of her choices being correct. In this paper, we assume that the

4

workers aim to maximize their expected rewards; extending our theory to more general utility functions is straightforward.
Given the presence of gold standard questions, the performance of any worker is based only on her responses to questions to which answers are already known by the mechanism designer, the payments made to different workers do not depend on each other and hence we consider only one worker without loss of generality.
Goal. The goal is to design mechanisms that are incentive compatible:
Deﬁnition 1 (Incentive compatibility). A mechanism is incentive compatible if the expected payment (Equation (3)), from the worker’s point of view, is strictly maximized when she selects precisely the support (Equation (1)) of her belief for each question.
Note that the deﬁnition of incentive compatibility used here considers a “strict” maximization. Observe that a worker who selects all the options for all the questions doesn’t give any useful information. In order to deter such “freeloading” behavior, one would like to ensure that in addition to paying a (large enough) amount α to a good worker, the mechanism should expend as small an amount as possible on such a worker. This leads to a notion of “frugality”.
Deﬁnition 2 (Frugality). An incentive-compatible mechanism f is frugal if
f (B, . . . , B) ≤ f (B, . . . , B)
for every incentive-compatible mechanism f that has f (1, . . . , 1) = f (1, . . . , 1).
Our goal is to design mechanisms that are incentive-compatible, and whenever they exist, ﬁnd the mechanism(s) that is (are) most frugal.
3 An impossibility result and a coarse-beliefs assumption
It turns out that, unfortunately, we must face a roadblock in the ﬁrst step: We can show that there exists no mechanism that is incentive compatible.
Theorem 3.1. For any N , G and B ≥ 2, there is no mechanism that can guarantee that the worker will be incentivized to select precisely the support of her distribution for each question.
The proof of this result and other theoretical results (except Theorem 4.1) are provided in the appendix. In order to circumvent this impossibility result, we appeal to a certain well-understood property of human belief.
Coarse beliefs assumption
There is an extensive literature in psychology establishing the coarseness of processing and perception in humans. For instance, Miller’s celebrated paper (Miller, 1956) establishes the information and storage capacity of humans, that an average human being can typically distinguish at most about seven states. This granualrity of human computation is veriﬁed in many subsequent experiments (Shiffrin and Nosofsky, 1994; Saaty and Ozdemir, 2003). Jones and Loe (2013) establish the ineffectiveness of ﬁner-granularity response elicitation. Mullainathan et al. (2008) hypothesize that humans often group things into categories; this hypothesis is experimentally veriﬁed by Siddiqi (2011) in a speciﬁc setting. We incorporate this established notion of coarseness of human processing in our model in terms of a simple assumption.
Consider some (ﬁxed and known) value ρ > 0, and assume that the probability of any option for any question, according to the worker’s belief, is either zero or greater than ρ. The impossibility shown in
5

Theorem 3.1 pertains to ρ = 0. Also, one must necessarily take into account situations when a worker is totally clueless about a question, that is, when her belief is distributed uniformly over all options. Hence we restrict ρ < B1 . To summarize, we make the following “coarse belief” assumption. Deﬁnition 3 (Coarse belief assumption). The worker’s belief for any option for any question lies in the set {0} ∪ (ρ, 1] for some (ﬁxed and known) ρ ∈ 0, B1 .
We wish to elicit the full support of the workers’ beliefs, given a coarseness of belief that assigns a value of zero to very low probability categories. The goal is to design mechanisms that are incentive-compatible and frugal, assuming the coarse belief assumption holds true.
4 Incentive mechanism
Mechanism 1 presents our proposed mechanism for the problem at hand, under the coarse belief assumption.

Mechanism 1 Incentive mechanism for approval voting
• Input: Evaluations of the worker’s answers to the G gold standard questions (x1, . . . , xG) ∈ {−(B − 1), . . . , −1, 1, . . . , B}G

• Output: The worker’s payment

f (x1, . . . , xG) = (αmax − αmin)(1 − ρ)

G
Gi=1(xi−1) 1{xi ≥ 1} + αmin
i=1

The payment is based only on the evaluation of the worker’s responses to the gold standard questions. It is easy to describe the mechanism in words: The payment is αmin plus
• 0 if the correct answer is not selected for any of the questions, otherwise
• (αmax − αmin) reduced by (100ρ)% for each incorrect option selected.
The following pair of theorems present our main results, proving that this mechanism is the one and only mechanism that satisﬁes our requirements.
Theorem 4.1. Under the coarse-beliefs assumption, Mechanism 1 is incentive-compatible and frugal.
The following theorem shows that our mechanism is strictly better than any other mechanism.
Theorem 4.2. Under the coarse-beliefs assumption, there is no other incentive-compatible mechanism that expends as small an amount as Mechanism 1 on a worker who does not attempt any question.
To show the optimality and uniqueness properties claimed in Theorem 4.1 and Theorem 4.2 respectively, we prove the absence of other good mechanisms via contradiction-based arguments. Speciﬁcally, for any candidate mechanism, we identify a set of beliefs for which the worker will not be incentivized to act as required. In line with our earlier argument of beliefs being “coarse”, the beliefs considered in these proofs are simple enough: the worker has some belief about one of the options, knows for sure that certain other options are incorrect, and is indifferent among the rest of the options.

6

To put things in perspective, observe that ρ = 0 eliminates the dependence of the payment in Mechanism 1 on i xi and makes the mechanism incentive incompatible. The impossibility result of Theorem 3.1 proves that every possible mechanism must necessarily suffer this fate.
The remainder of this section is devoted to the proof of Theorem 4.1. The reader may feel free to jump to Section 5 without any loss in continuity.

Proof of Theorem 4.1
Without loss of generality, assume that αmin = 0 since in our setting, the property of incentive compatibility is invariant to any constant shift and positive scale of the payment. We adopt the succinct notation of α := αmax − αmin.
Incentive compatibility. First consider the case N = G = 1. In this case, Mechanism 1 reduces to
f (x) = α(1 − ρ)(x1−1)1{x1 ≥ 1}.
Suppose without loss of generality that the worker’s beliefs for the B options are p1 ≥ · · · ≥ pm > ρ > pm+1 = · · · = pB = 0 for some m ∈ [B]. An incentive-compatible mechanism must strictly maximize the worker’s expected payment when she selects the support of her belief, that is, the options {1, . . . , m}. The expected payment, $sup, under this selection is
m
$sup = α pi(1 − ρ)m−1
i=1
= (1 − ρ)m−1.
Suppose the worker selects some other set of options {o1, . . . , o } ⊆ [B], {o1, . . . , o } = [m]. Then her expected payment $oth under the proposed mechanism for this selection is

$oth = α poi(1 − ρ) −1
i=1

≤ α pi(1 − ρ) −1,

(4)

i=1

since p1 ≥ · · · ≥ pB. If = m then the inequality in (4) is strict since pj < pi for all (j > m, i ≤ m). Thus
the expected payment under the choice = m but with a selection different from the support is strictly lower than $sup. Also observe that the expected payment on selecting > m is upper bounded by (1 − ρ) −1, which is strictly smaller than $sup. Let us now consider the remaining, interesting case of < m. Since pi > ρ for all i ∈ [m], we have

m

$oth < α

pi − (m − )ρ (1 − ρ) −1

i=1

= α (1 − (m − )ρ) (1 − ρ) −1

≤ α (1 − (m − ( ... ≤ α(1 − ρ)m−1

+ 1))ρ) (1 − ρ)

= $sup.

7

This completes the proof for the case N = G = 1. Let us now consider the case of N = G ≥ 1. By our assumption of the independence of the beliefs of
the worker across the questions, the expected payment equals

G
E α(1 − ρ)(xi−1)1{xi ≥ 1} .
i=1

Since the payments are non-negative, if each individual component in the product is maximized then the product is also necessarily maximized. Each individual component simply corresponds to the setting of N = G = 1 discussed earlier. Thus calling upon our earlier result, we get that the expected payment for the case N = G > 1 is maximized when the worker acts as desired for every question.
Let us ﬁnally consider the case of N > G ≥ 1. Recall from (3) that the expected payment for the general case is a cascade of two expectations: the outer expectation is with respect to the uniformly random distribution of the G gold standard questions among the N total questions, while the inner expectation is taken over the worker’s beliefs of the different questions conditioned on the choice of the gold standard questions. The arguments above for the case N = G prove that every individual term in the inner expectation is maximized when the worker acts as desired. The expected payment is thus maximized when the worker acts as desired.
Frugality. We ﬁrst present a lemma that forms the workhorse of this and other subsequent proofs.

Lemma 4.3. Consider some y, y ∈ [B]N and some I ⊆ [N ] such that yi = yi + 1 for all i ∈ I, and yi = yi for all i ∈/ I. Then any incentive compatible mechanism f must necessarily satisfy

N1 f (yj1 , . . . , yjG ) ≥ N1 (1 − ρ)|I∩{j1,...,jG}|f (yj1 , . . . , yjG ).

G (j1,...,jG)⊆[N ]

G (j1,...,jG)⊆[N ]

Furthermore, a necessary condition for the above equation to be satisﬁed with equality is

f ( 1yj1 , . . . , GyjG ) = 0
for all (j1, . . . , jG) ⊆ [N ], and all {( 1, . . . , G) ∈ {−1, 1}G\{1}G | i = 1 whenever ji ∈/ I}.
The proof of the lemma is provided in the appendix. We now prove the frugality of our proposed mechanism using this lemma. Consider any incentive compatible mechanism f such that f (1, . . . , 1) = α. Consider any x0 ∈ [B − 1]. Applying Lemma 4.3 with y = (x0 + 1, . . . , x0 + 1), y = (x0, . . . , x0) and I = [G] gives
f (x0 + 1, . . . , x0 + 1) ≥ (1 − ρ)Gf (x0, . . . , x0).

A repeated application of this inequality for all x0 ∈ [B − 1] gives f (B, . . . , B) ≥ (1 − ρ)(B−1)Gf (1, . . . , 1) = (1 − ρ)(B−1)Gα.

Mechanism 1 achieves this lower bound on f (B, . . . , B) with equality, thereby completing the proof.

5 Robustness to the coarse beliefs assumption
We earlier made the “coarse belief” assumption that the worker’s belief for any option, when non-zero, is atleast ρ. We then designed the Mechanism 1 that is incentive compatible with respect to eliciting the
8

supports of the beliefs of the worker. A natural question then arises is: How does the mechanism perform if the coarse beliefs assumption is violated? Does the mechanism break down?
In this section, we generalize the results presented earlier in the paper to the setting where workers may have arbitrary beliefs. It turns out that our proposed mechanism continues to incentivize workers to act in a certain desirable way.

5.1 Incentivizing workers with ﬁner beliefs

Suppose that Mechanism 1 (for a certain value of ρ) is encountered by a worker who may have arbitrary beliefs. Interestingly, it turns out that the mechanism doesn’t break down, but instead does something desirable: it incentivizes the worker to select all options for which the relative belief of the worker is high enough.

Theorem 5.1. Under Mechanism 1, for any question, a worker with beliefs 1 ≥ p1 ≥ . . . ≥ pB ≥ 0 will be incentivized to select options {1, . . . , m} where

m = arg max
z∈[m]

pz zi=1 pi > ρ .

It is not hard to interpret this incentivized action. The worker selects options one by one in decreasing order of her beliefs as long as the selected option contributes a fraction more than ρ to the total belief of the selected options.
Let us now verify that the earlier result of Theorem 4.1 for “coarse beliefs” is indeed a special case of Theorem 5.1. To this end, suppose the beliefs of the worker for any particular question are p1 ≥ · · · pk > ρ > pk+1 = · · · = pB = 0 for some k ∈ [B]. Then we have

pz zi=1 pi =

0 zi=1 pi = 0 < ρ

for all z ≥ k + 1,

and

pz

pz

zi=1 pi ≥ 1 > ρ

for all z ≤ k.

It follows that under the result of Theorem 5.1, a worker with “coarse beliefs” will be incentivized to select precisely the support of her beliefs.

5.2 An axiomatic derivation
We now present an alternative axiomatic derivation of our mechanism when accommodating workers with arbitrary beliefs. The derivation involves a “no-free-lunch axiom” of Shah and Zhou (2015), which when adapted to our approval-voting based setting is deﬁned as follows. We say that a worker has ‘attempted’ a question if for that question, she doesn’t select all the B options. We say that the answer to a question is wrong if the correct option does not lie in the set of selected options.
Deﬁnition 4 (No-free-lunch; adapted from Shah and Zhou (2015)). If the answer to every attempted question in the gold standard turns out to be wrong, then the worker gets a payment of zero, namely,

f (x1, . . . , xG) = 0 ∀ (x1, . . . , xG) ∈ {−(B − 1), . . . , −1, B}G\{B}G. 9

Select ALL options that could be the animal in this image.
Cheetah Lion Tiger Leopard Jaguar Puma
(a)

Select ALL options that could be the texture in this image.
Sand Brick Grass Wood Cloth Gravel
(b)

Figure 2: Illustration of two of the three experiments we conducted on Amazon Mechanical Turk.

The no-free-lunch axiom is quantitatively different from the criterion of frugality proposed in this paper. However, both these notions have the same qualitative goal, namely to minimize the expenditure when no useful data is obtained, while providing higher payments to workers providing better data. Interestingly, as we show below, both these notions lead to the same (unique) mechanism under our setting of approval voting.

Theorem 5.2. Consider no assumptions on the minimum value of the belief, and suppose the workers must

be incentivized to select options {1, . . . , m} where m = arg maxz

pz
z

> ρ . Then, Mechanism 1 is

i=1 pi

the one and only mechanism that is incentive compatible and satisﬁes no-free-lunch.

6 Preliminary experiments
This section presents results from an evaluation of our proposed mechanism, Mechanism 1, on the popular Amazon Mechanical Turk (mturk.com) commercial crowdsourcing platform. The goal of this preliminary experimental exercise is to perform a basic check on whether our mechanism has the potential to work in practice. Speciﬁcally, our goal is to evaluate the primary hypotheses underlying the theory: (i) whether workers are able to make a judicious use of the approval voting setup, (ii) whether the existence of the mechanism make any difference, and (iii) if there is a opposition from the workers to the interface or the mechanism for any reason.
It is important to keep in mind that conclusive experiments for mechanism design are in general quite expensive with respect to time (workers may need months to understand a new mechanism) and budget. They are unlike typical machine-learning experiments that require only existing benchmark datasets. Moreover, the wordings or the interface may exert a signiﬁcant inﬂuence on the workers’ behavior. Like most mechanism design papers, we position our work primarily as a theoretical study. We expect that more detailed experiments will follow the publication of our work; indeed, it is best if experiments on such incentive schemes are conducted by multiple groups.
6.1 Methods
We conducted three separate sets of experiments, with over 200 workers in each experiment:
• Identifying languages from displayed text (Figure 1)
• Identifying animals in displayed images (Figure 2(a))
• Identifying textures in displayed images (Figure 2(b)).
10

0.8

Single without skip, additive

0.6

Single with skip, multiplicative Approval, fixed

0.4

Approval, Algorithm 1

0.2

0.0 -7 -6 -5 -4 -3 -2 -1 1 2 3 4 5 6 7 8 Evaluation

(a) Fraction of responses that evaluate to different values. The magnitude of the evaluation represents the number of options selected and its sign denotes whether the correct option was selected (positive) or not (negative).

0.3 0.2 0.1 0.0
(b) Fraction wrong among attempted questions

0.3 0.2 0.1 0.0
(c) Fraction wrong when only one option was selected

40 30 20 10 0
(d) Average bonus per worker (cents)

Figure 3: Raw data from the three experiments conducted on Amazon Mechanical Turk.

In each experiment, every worker was assigned one of four mechanisms uniformly at random. The variable component of each mechanism was executed as a “bonus payment” based on the evaluation of the worker’s performance on the gold standard questions, on top of a guaranteed payment of 10 cents (this was αmin). The four mechanisms tested were:
• Single-selection interface with additive payments: The worker must select a single option for every question. The bonus starts at zero and is increased additively by a ﬁxed amount for every correct answer.
• Skip-based single-selection interface with multiplicative payments (Shah and Zhou, 2015): For every question, the worker can either select one option or skip the question. The bonus starts at a certain positive value, is reduced by a certain fraction for each skipped question, and becomes zero in case of an incorrect answer.
• Approval-voting interface with a ﬁxed payment: The bonus is ﬁxed.
• Approval-voting interface with Mechanism 1.
Given the caveats associated to experiments on mechanism design as mentioned earlier, we provided detailed instructions about the task and the mechanism to each worker, and also made them work through multiple examples. The entire data related to the experiments, including the interfaces used, speciﬁcs about the payment mechanisms, and the responses of the workers, is available on the website of the ﬁrst author.
6.2 Results
Let us ﬁrst eyeball the raw data. Figure 3 presents combined results from the three experiments. Figure 3(a) shows the breakup of the evaluations of all the collected responses. The magnitude of the evaluation represents the number of options selected and its sign denotes whether the correct option was selected (positive) or not (negative). Figure 3(b) depicts the fraction of responses to attempted questions that turned out to

11

Experiment T 2 F

p

Languages 15.7 7.8 0.0004

Textures 21.3 10.7 0.000025

Animals 10.2 5.1 0.0062

Table 1: Hotelling’s T-squared test comparing the data from the ﬁxed payment mechanism and the data from Mechanism 1 proposed in this paper.

be wrong. Figure 3(c) depicts the fraction of responses that were correct when only one option was selected. Figure 3(d) depicts the average payment per worker. Using this data, let us now investigate the three questions posed at the beginning of the experiments:
(i) Are the workers making a judicious use of the approval voting setup? One can observe from Figure 3(a) more than 40% responses comprised a selection of two or three options, suggesting that the workers did understand the concept of approval voting.
(ii) Does the presence of a mechanism makes a difference? We compared the data from the approval voting setup under the ﬁxed mechanism with the data from the approval voting setup under Mechanism 1. In particular, we applied Hotelling’s T-squared test, where we treated the response by any worker to any question as a two-dimensional data point, with the number options selected and the correctness of the answer as the two dimensions. The results of this test are listed in Table 1. We could reject the null hypothesis (of the two sets of data being drawn from distributions with identical means) with p < 0.01 for each of the three experiments.
(iii) Is there is an opposition from the workers to the interface or the mechanism for any reason? We also elicited feedback about the task from every worker, informing them that the feedback will not affect their payment. We received mostly neutral feedback, some positive feedback, and no negative feedback about either the approval voting interface or our mechanism.
All in all, these preliminary experiments indicate that our mechanism is practical and can potentially be useful for many applications in machine learning, paying higher amounts to good workers and lower amounts to freeloaders or spammers.
A concluding remark. A standard means of denoising data from crowdsourcing is to ask every question to multiple workers, and employ a statistical aggregation algorithm to aggregate the data so obtained. In the future, we wish to evaluate the performance of our proposed interface and mechanism on such aggregated data. To this end, our goal for the future is to design algorithms designed towards statistical aggregation of data collected through the interface and mechanism proposed in this paper.

7 General utility functions
In this section, we consider a setting where the worker, instead of maximizing her expected payment, aims to maximize the expected value of some utility function of her payment.
Consider any function U : R → R. Suppose that instead of aiming to maximize the expected payment, the worker has some utility U for any payment made to her, and that she aims to maximize the expected utility. In other words, for any payment f made to the worker (based on the evaluation of her answers to the gold standard questions), her utility for this payment is U (f ). The worker aims to maximize the expected value of U (f ).
We will require the function U to be strictly increasing and invertible. The results presented so far in the paper implicitly assumed that the utility is simply the identity function, namely U (x) = x. The function U is assumed to be public knowledge.
Given the evaluations x1, . . . , xG of the worker’s responses to the G gold standard questions, consider

12

the following payment mechanism:
G
f (x1, . . . , xG) = U −1 (U (αmax) − U (αmin))(1 − ρ) Gi=1(xi−1) 1{xi ≥ 1} + U (αmin) . (5)
i=1
It is easy to see that the properties of Mechanism 1 carry over to this mechanism in the case of a general utility function U . This feature is formalized in the following proposition.
Proposition 7.1. For a worker who aims to maximize function U of the payment, the mechanism in Equation (5) is incentive compatible, frugal, and is the one and only incentive compatible mechanism to satisfy the no-free-lunch axiom.

8 An alternative problem statement

In

earlier

sections,

we

made

the

coarse

belief

assumption

of

the

existence

of

some

ρ

∈

(0,

1 B

)

such

that

the

belief of a worker for any option is assumed to either equal 0 or more than ρ. We then designed a mechanism

to elicit the support of the worker’s belief under this assumption. A natural question that arises is that instead

of making a coarse belief assumption, can we ﬁx a parameter, say, σ ∈ (0, 1), and incentivize the worker to

select all options for which her belief is strictly greater σ? Although not the primary focus of this paper, we

devote the present section to investigating this complimentary setting out of intellectual curiosity as well as

practical relevance. As we show below, the answer to this question is both yes and no.

8.1 Problem setting

For a given value of σ ∈ (0, 1), we will call a mechanism as incentive compatible if the expected payment of

any worker is strictly maximized when the worker selects all options for which her belief is strictly greater

than σ.

We retain most notation form Section 2, with a few exceptions as follows. We continue to let f denote the payment function; f : {−(B − 1), . . . , B}G → [αmin, αmax]. Observe that unlike the setting considered

earlier in Section 2, here we have included 0 in the domain of the payment function. This is because under

the present setting, when σ ≥ B1 , there is a possibility that the worker has a belief no more than σ for each

option, for instance, if the worker is totally clueless.

Let us deﬁne two integers smin and smax as smin

=

1{σ

<

B1 } and smax

=

min{

1 σ

− 1, B}.2

Observe that if if σ

<

1 B

then it is meaningless to let the worker select zero options since the belief for

at

least

one

option

must

be

1 B

or

higher.

Also

observe

that

for

any

value

of

σ

∈

(0, 1),

it

is

meaningless

to allow the worker to select

1 σ

or more options, since it is mathematically impossible for those many

options to have beliefs more than σ. As a result, we will require the worker to select at least smin and at

most smax options for any question. The goal remains to design the payment function f (x1, . . . , xG) when

|xi| ∈ {smin, . . . , smax} for every i ∈ [G]. If the worker’s responses do not satisfy this condition, then we

assume the convention of setting the payment to a small enough value (say, αmin or some further penalty).

We do not assume the restriction of coarseness of the beliefs. We stick to the identity utility function,

while noting that extension to other utility functions is straightforward following Section 7.

Finally, we note some special cases which we exclude from the subsequent analysis. The case of σ = 0

degenerates to the impossibility result of Theorem 3.1 proved earlier.

The cases of B

=

2 or σ

≥

1 2

degenerate to the “skip-based” single-selection setting studied in Shah and Zhou (2015). Hence we focus

on the case of B ≥ 3 and σ ∈ (0, 21 ) in the rest of this section.

2The function 1 : {T rue, F alse} → {0, 1} is the indicator function, with 1{x} = 1 if x is true, and 0 otherwise.

13

8.2 Mechanism
Mechanism 2 Incentive mechanism for the alternative problem formulation • Input: Evaluations of the worker’s answers to the G gold standard questions (x1, . . . , xG)
• Output: Deﬁne a function g : R → R as
g(y) = (B − |y|)σ + 1{y ≥ 1}.
The worker’s payment is
G
f (x1, . . . , xG) = a + b g(xi),
i=1
where a = αmin and b = Gα((mBax−−1α)σm+in1) .

Given the conventions described in the previous subsection for the payment function, it remains to construct the payment function under “normal” conditions, that is, when smin ≤ xi ≤ smax for every i ∈ [G]. Mechanism 2 now presents our proposed mechanism for this setting.
Theorem 8.1. Consider any σ ∈ (0, 12 ), N ≥ G ≥ 1 and B ≥ 3. Consider the goal of designing a mechanism such that for each question, the worker is incentivized to select every option for which her belief is more than σ. Assume that no belief equals exactly σ. Then Mechanism 2 is incentive compatible.

The function g, in words, penalizes the selection of an incorrect option by σ and rewards the selection of

the correct option by 1. Under beliefs {p1, . . . , pB} for a gold standard question, when the worker answers

as per our requirements, the expected value of g equals

B i=1

max{pi

,

σ

}.

The setting also permits a “multiplicative” mechanism, consistent with the earlier results in this paper.

Corollary 8.2. Under the assumption that no belief equals exactly σ, the mechanism

G
f (x1, . . . , xG) = a + b (g(xi) − c),
i=1

for some constants a, b > 0 and c ≤ g(−smax), is also incentive compatible.

8.3 Uniqueness and an impossibility result
In this section, we show that the core structure of Mechanism 2, namely the function g, is essential for any mechanism. We also show that the (mild) assumption of no belief equalling exactly σ is unavoidable.
Theorem 8.3. Consider any σ ∈ (0, 12 ) and any B ≥ 3. Consider the goal of designing a mechanism such that for each question, the worker is incentivized to select every option for which her belief is more than σ. Then: (A) Under the assumption that no belief equals exactly σ, when G = 1, the function g is the one and only incentive-compatible mechanism upto a constant shift and positive scaling. (B) For any N ≥ G ≥ 1, no mechanism is incentive compatible in the absence of this assumption.
While we do not have a complete answer as to what the “best” or “unique” mechanism is for general values of N and G, but going by results proved earlier in the paper, we conjecture that the multiplicative version of the mechanism (Corollary 8.2) may possess attractive properties. Further exploration of this setting is beyond the scope of this paper.

14

9 Discussion and open problems
Our goal is to deliver high quality labels for machine learning applications, at low costs, by means of incentive mechanisms or aggregation algorithms or both. In this paper, we pursue the former approach. We take an approval-voting based means of gathering labeled data from crowdsourcing. We design an incentive mechanism via a principled theoretical approach, and prove appealing properties of optimality and uniqueness of our proposed mechanism. Preliminary experiments conducted on Amazon Mechanical Turk corroborate the usefulness of this mechanism for practical scenarios. Our mechanism may also draw more experts to the crowdsourcing platform since their compensation will be signiﬁcantly higher than that of mediocre workers, unlike most compensation mechanisms in current use.
We conclude with a discussion on closely related topics that merit investigation in the future. Aggregation of labels. For the traditional single-selection setting, there is a long, existing line of work on statistical methods to aggregate redundant noisy data from multiple workers (Dawid and Skene, 1979; Whitehill et al., 2009; Raykar et al., 2010; Karger et al., 2011; Liu et al., 2012; Zhou et al., 2012). An open problem is the design of aggregation algorithms for approval-voting-based data: algorithms that can exploit the speciﬁc structure of the responses that arise as a result of the approval voting interface and the proposed mechanism. There is indeed work on aggregation algorithms (Masso´ and Vorsatz, 2008; Caragiannis et al., 2010; Brams and Kilgour, 2014; Procaccia and Shah, 2015) and probabilistic models (Marley, 1993; Falmagne and Regenwetter, 1996; Doignon et al., 2004; Regenwetter and Tsetlin, 2004) for approval-voting in the context of social choice theory; their objective, however, is primarily of fairness and stretgyprooﬁng of the voting procedure, as opposed to our goal of denoising data obtained from multiple heterogeneous workers as required for labeling tasks in crowdsourcing. Choosing the right interface. There are tradeoffs between various interfaces for crowdsourcing. For instance, the approval voting interface elicits the support of the belief whereas the single selection interface elicits the mode. Choosing among these two interfaces would depend on the application under consideration, and moreover, one may adaptively switch between the two depending on the data obtained. A natural question that one may further ask is, why not elicit the entire belief distribution itself? While the entire belief distribution seems to supercede the support and the mode, stating the distribution will also require much more time and effort from the workers, and often also suffer from a higher noise. These tradeoffs must be taken into account when choosing the interface for the application at hand. The coarse beliefs parameter. One may wish to evaluate the value of ρ by explicitly asking workers on the crowdsourcing platform for this value. However, it is noted in the literature (e.g., see Shah et al. (2015) for experiments on Amazon Mechanical Turk) that the cardinal representations that humans provide are not always consistent with their respective mental beliefs, and are far noisier. This phenomenon suggests the requirement of developing alternative methods of evaluating this parameter. Indeed, measurement is considered one of the most difﬁcult parts of behavioral research. We look forward to future work exploring these topics in depth.
Acknowledgements
This paper was presented in part at the International Conference on Machine Learning (ICML) 2015. The work of the ﬁrst author was supported in part by a Microsoft Research PhD fellowship.
References
Steven J Brams and Peter C Fishburn. Approval voting. American Political Science Review, 72(03):831– 847, 1978.
15

Steven J Brams and D Marc Kilgour. Satisfaction approval voting. In Voting Power and Procedures, pages 323–346. Springer, 2014.
Glenn W Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly weather review, 78(1): 1–3, 1950.
Ioannis Caragiannis, Dimitris Kalaitzis, and Evangelos Markakis. Approximation algorithms and mechanism design for minimax approval voting. In AAAI, 2010.
Leverne S Collet. Elimination scoring: An empirical evaluation. Journal of Educational Measurement, 8 (3):209–214, 1971.
Clyde H Coombs. On the use of objective examinations. Educational and Psychological Measurement, 13 (2):308–310, 1953.
Clyde H Coombs, John Edgar Milholland, and Frank Burton Womer. The assessment of partial knowledge. Educational and Psychological Measurement, 16(1):13–37, 1956.
Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous proﬁciency. In Proceedings of the 22nd international conference on World Wide Web, pages 319–330. International World Wide Web Conferences Steering Committee, 2013.
Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics, pages 20–28, 1979.
Jean-Paul Doignon, Aleksandar Pekecˇ, and Michel Regenwetter. The repeated insertion model for rankings: Missing link between two subset choice models. Psychometrika, 69(1):33–54, 2004.
J-Cl Falmagne and Michael Regenwetter. A random utility model for approval voting. Journal of Mathematical Psychology, 40(2):152–159, 1996.
Boi Faltings, Radu Jurca, Pearl Pu, and Bao Duy Tran. Incentives to counter bias in human computation. In Second AAAI Conference on Human Computation and Crowdsourcing, 2014.
Jean D Gibbons, Ingram Olkin, and Milton Sobel. A subset selection technique for scoring items on a multiple choice test. Psychometrika, 44(3):259–270, 1979.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359–378, 2007.
Paul Horst. The chance element in the multiple choice test item. The Journal of General Psychology, 6(1): 209–211, 1932.
Shaili Jain, Yiling Chen, and David C Parkes. Designing incentives for online question and answer forums. In Proceedings of the 10th ACM conference on Electronic commerce, pages 129–138, 2009.
W Paul Jones and Scott A Loe. Optimal number of questionnaire response categories more may not be better. SAGE Open, 3(2):2158244013489691, 2013.
David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In Advances in neural information processing systems, pages 1953–1961, 2011.
Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of HIT design on comparative system ranking. In ACM SIGIR conference on Research and development in Information Retrieval, pages 205–214, 2011.
16

John Kellett and Kenneth Mott. Presidential primaries: Measuring popular choice. Polity, pages 528–537, 1977.
Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions. In ACM conference on Electronic commerce, pages 109–118, 2009.
Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In NIPS, pages 701–709, 2012.
AAJ Marley. Aggregation theorems and the combination of probabilistic rank orders. In Probability models and statistical analyses for ranking data, pages 216–240. Springer, 1993.
Jordi Masso´ and Marc Vorsatz. Weighted approval voting. Economic Theory, 36(1):129–146, 2008.
George A Miller. The magical number seven, plus or minus two: some limits on our capacity for processing information. Psychological review, 63(2):81, 1956.
Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-prediction method. Management Science, 51(9):1359–1373, 2005.
Sendhil Mullainathan, Joshua Schwartzstein, and Andrei Shleifer. Coarse thinking and persuasion. The Quarterly journal of economics, 123(2):577–619, 2008.
Guy Ottewell. The arthmetic of voting. In defence of variety, 1977.
Drazˇen Prelec. A Bayesian truth serum for subjective data. Science, 306(5695):462–466, 2004.
Ariel D Procaccia and Nisarg Shah. Is approval voting optimal given approval votes? In NIPS, 2015.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. The Journal of Machine Learning Research, 11:1297–1322, 2010.
Michel Regenwetter and Ilia Tsetlin. Approval voting and positional voting methods: Inference, relationship, examples. Social Choice and Welfare, 22(3):539–566, 2004.
Thomas L Saaty and Mujgan S Ozdemir. Why the magic number seven plus or minus two. Mathematical and Computer Modelling, 38(3):233–244, 2003.
Leonard J Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical Association, 66(336):783–801, 1971.
Nihar B. Shah and Dengyong Zhou. Double or nothing: Multiplicative incentive mechanisms for crowdsourcing. In NIPS, December 2015.
Nihar B Shah, Sivaraman Balakrishnan, Joseph K Bradley, Abhay Parekh, Kannan Ramchandran, and Martin Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. In AIStats, 2015.
RM Shiffrin and RM Nosofsky. Seven plus or minus two: a commentary on capacity limitations. Psychological review, 101(2):357, 1994.
Hammad Siddiqi. Does coarse thinking matter for option pricing? Evidence from an experiment. IUP Journal of Behavioral Finance, 8(2), 2011.
17

Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. How much spam can you take? An analysis of crowdsourcing results to increase accuracy. In ACM SIGIR Workshop on Crowdsourcing for Information Retrieval, pages 21–26, 2011.
Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a high-quality workforce with Mechanical Turk. NIPS workshop on computational social science and the wisdom of crowds, 2010.
Robert J Weber. Comparison of voting systems. New Haven: Cowles Foundation Discussion paper A, 498, 1977.
Jacob Whitehill, Paul Ruvolo, Ting-fan Wu, Jacob Bergsma, and Javier Movellan. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing systems, pages 2035–2043, 2009.
Dengyong Zhou, John Platt, Sumit Basu, and Yi Mao. Learning from the wisdom of crowds by minimax entropy. In Advances in Neural Information Processing Systems 25, pages 2204–2212, 2012.
James Zou, Reshef Meir, and David Parkes. Approval voting behavior in Doodle polls. In The 5th Workshop on Computational Social Choice, June 2014.

APPENDIX

A Proofs
In this section, we present proofs of the various theoretical results presented in the paper.

A.1 Proof of Theorem 3.1: Impossibility
We assume that there indeed exists some incentive-compatible payment function f , and prove a contradiction.
Let us ﬁrst consider the special case of N = G = 1 and B = 2. Since N = G = 1, there is only one question. Let p1 > 0.5 be the probability, according to the belief of the worker, that option 1 is correct; the worker then believes that option 2 is correct with probability (1 − p1).
When p1 = 1, we need the worker to select option 1 alone. Thus we need

f (1) > f (2).

When p1 ∈ (0.5, 1), we require the worker to select options 1 and 2, as opposed to selecting option 1 alone. For this we need

p1f (1) + (1 − p1)f (−1) < f (2)

It follows that we need

(1 − p1)(f (1) − f (−1)) > f (1) − f (2).

(6)

However, the inequality (6) is satisﬁed only when f (1) > f (−1) and (1 − p1) > ff((11))−−ff((−21)) . Thus for any

given

payment

function

f

,

a

worker

with

belief

(1

−

p1)

∈

(0,

f (1)−f (2) f (1)−f (−1)

)

will

not

be

incentivized

to

select

the support of her belief. This yields a contradiction.

18

We now move on to the general case of N ≥ G ≥ 1 and B ≥ 2. Consider a worker who is clueless about questions 2 through N (i.e., her belief is uniform across all options for these questions). Suppose this worker selects all B options for these questions as desired. For the ﬁrst question, suppose that the worker is sure that options 3, . . . , B are incorrect. We are now left with the ﬁrst question and the ﬁrst two options for this question. Letting X denote a random variable representing the evaluation of the worker’s response to the ﬁrst question, the expected payment then is

G

G

N E[f (X, B, . . . , B)] + (1 − N )f (B, . . . , B).

The expectation in the ﬁrst term is taken with respect to the randomness in X. Deﬁning

f˜(X )

:=

G f (X, B, . . . , B) + (1 −

G )f (B, . . . , B),

N

N

and applying the same arguments to f˜ as those for f for the case of N = G = 1, B = 2 above gives the desired contradiction. This thus completes the proof of impossibility.

A.2 Proof of Lemma 4.3: The workhorse lemma

Consider some ρ0 ∈ (ρ, B1 ). Consider a worker such that for every question i ∈ I, her belief is ρ0 for the ﬁrst option and 1y−i−ρ10 for each of the last (yi − 1) options. For every question i ∈/ I, her belief is uniformly distributed among the ﬁrst yi options. Now, if the worker selects precisely the support of her beliefs for every question then her expected payment $1 is

1

$1 = N

f (yj1 , . . . , yjG ).

(7)

G (j1,...,jG)⊆[N ]

We will compare the aforementioned action to another action, where for each question i ∈ I, the worker selects only the last (yi − 1) options but not the ﬁrst option; for each question i ∈/ I, the worker selects the support of her belief. Under this action, the expected payment $2 is

1 $2 = N

1{{ji | i = −1} ⊆ I}(1 − ρ0)|I∩{ji| i=1}|ρ0|I∩{ji| i=−1}|f ( 1yj1 , . . . , GyjG ).

G (j1,...,jG) ( 1,..., G)

⊆[N ] ∈{−1,1}G

(8)

In the expression (8), the outer summation represents the expectation over the random choice of the G gold standard questions among the N questions. The inner summation represents the expectation with respect to the correctness or incorrectness of the answers to the G gold standard questions: for any question i, i = 1 captures the event where the ith question in the gold standard is answered correctly and i = −1 represents the event of this question being answered incorrectly. The term 1{{ji | i = −1} ⊆ I} ensures that only the questions in I can be wrong, since it is only these questions for which the worker has selected a subset of her belief’s support.
Since f (x) ≥ 0 for all x, we can lower bound $2 as
$2 ≥ N1 (1 − ρ0)|I∩{j1,...,jG}|f (yj1 , . . . , yjG ). (9)
G (j1,...,jG)⊆[N ]

19

An incentive compatible mechanism must incentivize the worker to perform the ﬁrst action (over the second), i.e, must have $1 > $2. Thus from (7) and (9), we get

N1 f (yj1 , . . . , yjG ) > N1 (1 − ρ0)|I∩{j1,...,jG}|f (yj1 , . . . , yjG ).

G (j1,...,jG)⊆[N ]

G (j1,...,jG)⊆[N ]

(10)

Note that (10) must hold for all ρ0 > ρ. The left hand side of (10) does not involve ρ0 whereas the right hand side is continuous in ρ0. It follows that

N1 f (yj1 , . . . , yjG ) ≥ N1 (1 − ρ)|I∩{j1,...,jG}|f (yj1 , . . . , yjG ).

G (j1,...,jG)⊆[N ]

G (j1,...,jG)⊆[N ]

(11)

This proves the ﬁrst part of the lemma.
We now move on to the second part of the lemma, concerning equality in (11). Suppose f ( 1yj1, . . . , GyjG) is strictly positive for any (j1, . . . , jG) ⊆ [N ], {( 1, . . . , G) ∈ {−1, 1}G\{1}G | i = 1 whenever ji ∈/ I}. Then (11) will necessarily be a strict inequality. The claimed necessary condition for equality is thus estab-
lished.

A.3 Proof of Theorem 4.2: Frugality
Without loss of generality, assume that αmin = 0 since in our setting, the property of incentive compatibility is invariant to any constant shift and positive scale of the payment. We adopt the succinct notation of α := αmax − αmin.
Consider any incentive compatible mechanism f such that f (1, . . . , 1) = α and f (B, . . . , B) = (1 − ρ)G(B−1)α. We will show that this payment mechanism must be identical to Mechanism 1.
We consider the set of evaluations x whose elements are non-decreasing, i.e., x1 ≥ x2 ≥ · · · ≥ xG; The proof for any other ordering follows in an identical manner.
First consider any x such that xG > 0.
• Let γ(x) denote the number of distinct entries in x:
G−1
γ(x) := 1 + 1{xi = xi+1}
i=1

• Let σ(x) denote the size of the last jump in x:

σ(x) := xj − xj+1

where j = arg max xi = xi+1
i∈[G−1]

• Let β(x) denote the numeric value of x in a B-ary number system:
G
β(x) := BG−i(xi − 1).
i=1
For example, if B = 5, G = 5 and x = (5, 5, 4, 1, 1) then γ(x) = |{5, 4, 1}| = 3, σ(x) = 4 − 1 = 3 (where j = 3), and β(x) = 4 · 54 + 4 · 53 + 3 · 52 + 0 · 51 + 0 · 50 = 3075. The proof involves three nested levels of induction: on γ, on σ and then on β.

20

We ﬁrst induct on γ. The base case is the set {x|γ(x) = 1}, i.e., the set of vectors which have the same value for all its components. Consider any x0 ∈ [B −1]. Applying Lemma 4.3 with y = (x0 +1, . . . , x0 +1) and y = (x0, . . . , x0) gives
f (x0 + 1, . . . , x0 + 1) ≥ (1 − ρ)Gf (x0, . . . , x0).

Since this inequality is true for every x0 ∈ [B − 1], we have f (B, . . . , B) ≥ (1 − ρ)(B−x0)Gf (x0, . . . , x0) ≥ (1 − ρ)(B−1)Gf (1, . . . , 1).

Setting f (B, . . . , B) = (1 − ρ)(B−1)α and f (1, . . . , 1) = α proves the base case. Now suppose our hypothesis is true for all {x|γ(x) ≤ γ0 − 1} for some γ0 ∈ {2, . . . , B}. We will
now prove that the hypothesis is also true for all {x|γ(x) ≤ γ0}. Towards this goal, we will now induct on σ. The set of all {x|γ(x) = γ0 − 1} can be treated as a base case for our induction, with this base case corresponding to σ = 0. Due to the induction hypothesis on γ, the base case of σ = 0 is already proven.
Now suppose that the hypothesis is true for all {x|γ(x) = γ0, σ(x) ≤ σ0 − 1} for some σ0 ∈ [B − 1]. We will prove that the hypothesis remains true for all {x|γ(x) = γ0, σ(x) = σ0}. To this end, we will induct on β.
Recall that we have restricted our attention to those x which have their elements in a descending order. Observe that the element with the minimum value of β in the set {x|γ(x) = γ0, σ(x) = σ0} is (γ0 + σ0 − 1, . . . , σ0 + 1, 1, . . . , 1). We will prove the hypothesis for this element as the base case for our induction on β. Applying Lemma 4.3 with y = (γ0 + σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1) and y = (γ0 + σ0 − 1, . . . , σ0 + 2, σ0, 1, . . . , 1) gives the inequality

c1f (γ0 + σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1) + c1f (γ0 + σ0 − 1, . . . , σ0 + 2, 1, 1, . . . , 1)

+

csf (s, 1, 1, . . . , 1) + csf (s, σ0 + 1, 1, . . . , 1)

s {γ0+σ0−1,...,σ0+2}

≥ c1(1 − ρ)f (γ0 + σ0 − 1, . . . , σ0 + 2, σ0, 1, . . . , 1) + c1f (γ0 + σ0 − 1, . . . , σ0 + 2, 1, 1, . . . , 1)

+

csf (s, 1, 1, . . . , 1) + cs(1 − ρ)f (s, σ0, 1, . . . , 1) ,

(12)

s {γ0+σ0−1,...,σ0+2}

for some positive constants c1, c1, cs, cs (which represent the probabilities of the respective set of G questions being chosen as the G gold standard questions). Now, for any s {γ0 + σ0 − 1, . . . , σ0 + 2}, observe that γ(s, σ0 + 1, 1, . . . , 1) ≤ γ0 − 1 and γ(s, σ0, 1, . . . , 1) ≤ σ0 − 1. Thus from our induction hypothesis,
we have

f (s, σ0 + 1, 1, . . . , 1) = (1 − ρ)f (s, σ0, 1, . . . , 1).

(13)

Also, γ(γ0 + σ0 − 1, . . . , σ0 + 2, σ0, 1, . . . , 1) = γ0 and σ(γ0 + σ0 − 1, . . . , σ0 + 2, σ0, 1, . . . , 1) = σ0 − 1. Consequently from our induction hypothesis, we have

f (γ0 + σ0 − 1, . . . , σ0 + 2, σ0, 1, . . . , 1) = (1 − ρ)γ0+σ0−2+···+σ0+1+σ0−1α.

(14)

Substituting (13) and (14) in (12) and canceling out common terms gives f (γ0+σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1) ≥ (1 − ρ)γ0+σ0−2+···+σ0 α.

21

We will now derive a matching upper bound on f (γ0 + σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1). Applying Lemma 4.3 with y = (γ0 + σ0 − 1, . . . , σ0 + 1, 2, . . . , 2) and y = (γ0 + σ0 − 1, . . . , σ0 + 1, 1, . . . , 1) gives

c1f (γ0 + σ0 − 1, . . . , σ0 + 1, 2, . . . , 2) +

csf (s, 2, . . . , 2)

s {γ0+σ0−1,...,σ0+1}

≥ c1(1 − ρ)G−γ+1f (γ0 + σ0 − 1, . . . , σ0 + 1, 1, . . . , 1) +

cs(1 − ρ)G−|s|f (s, 1, . . . , 1),

s {γ0+σ0−1,...,σ0+1}

(15)

for some positive constants c1, cs. Now, for any s {γ0 +σ0 −1, . . . , σ0 +2}, observe that γ(s, 2, . . . , 2) ≤ γ0 − 1 and γ(s, 1, . . . , 1) ≤ σ0 − 1. Thus from our induction hypothesis, we have

f (s, 2, . . . , 2) = (1 − ρ)G−|s|f (s, 1, . . . , 1).

(16)

Also, γ(γ0 + σ0 − 1, . . . , σ0 + 1, 2, . . . , 2) ≤ γ0 and σ(γ0 + σ0 − 1, . . . , σ0 + 1, 2, . . . , 2) = σ0 − 1. Consequently from our induction hypothesis,

f (γ0 + σ0 − 1, . . . , σ0 + 1, 2, . . . , 2) = (1 − ρ)γ0+σ0−2+...+σ0+G−γ+1α.

(17)

Substituting these values in (15) and canceling out common terms gives f (γ0 + σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1) ≤ (1 − ρ)γ0+σ0−2+···+σ0 α.

We have thus proved that the hypothesis is true for x = (γ0 + σ0 − 1, . . . , σ0 + 2, σ0 + 1, 1, . . . , 1), the base
case for our induction on β. Now consider some x∗ such that γ(x∗) = γ0, σ(x∗) = σ0 and β(x∗) = β0, for some β0. Let us denote
the components of x∗ as x∗ = (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G, . . . , x∗G) with x∗1 ≥ x∗2 ≥ · · · ≥

m1

x∗m > σ0 + x∗G for some m ≥ 0, m1 ≥ 1, m + m1 < G. Suppose the hypothesis is true for all {x|γ(x) =

γ0,

σ(x)

=

σ0,

β(x)

≤

β0−1}.

Applying

Lemma

4.3

with

y

=

(x

∗ 1

,

.

.

.

,

x

∗ m

,

σ0

+

x

∗ G

,

.

.

.

,

σ

0

+

x

∗ G

,

x∗G

,

.

.

.

,

x

∗ G

)

m1
and y = (x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G) gives the inequality

m1

c1f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G, . . . , x∗G)

m1

+

cs

f

(s,

x∗G

,

.

.

.

,

x

∗ G

)

s {x∗1,...,x∗m,σ0+x∗G,...,σ0+x∗G}

m1
≥ c1(1 − ρ)m1f (x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G)

m1

+

cs(1 − ρ)

s {x∗1,...,x∗m,σ0+x∗G−1,...,σ0+x∗G−1}

i 1{si=σ0+x∗G−1}f (s, x∗G, . . . , x∗G),

m1

(18)

for some positive constants c1, cs. Observe that

γ(x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G) =
m1

γ0 − 1 γ0

if σ0 = 1 otherwise,

22

and the induction hypothesis is satisﬁed in the ﬁrst case. In the second case,

σ(x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G) = σ0 − 1,
m1

and hence the induction hypothesis is satisﬁed in the second case as well. Thus

f (x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G)

= (1 − ρ)

m1
m i=1(x∗i −1)+m1(σ0+x∗G−2)+(G−m1−m)(x∗G−1)α.

(19)

For any for any s {x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1}, deﬁne m1(s) := i 1{si = σ0 + x∗G −

m1

1}.

Observe

that

if

m1(s)

>

0

then

either

γ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

γ0

−

1

or

σ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

σ0

−

1;

if m1(s) = 0 then γ((s, x∗G, . . . , x∗G)) ≤ γ0 − 1. For any s {x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G}, deﬁne

m1

m˜ 1(s) :=

i 1{si

=

σ0

+ x∗G}.

Observe

that

if

m˜ 1(s)

>

0

then

either

γ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

γ0

−1

or

β

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

β0 − 1;

if m˜ 1(s)

=

0 then γ((s, x∗G, . . . , x∗G))

≤

γ0 − 1.

Consequently from our

induction hypothesis we have

cs

f

(

s,

x∗G

,

.

.

.

,

x

∗ G

)

s {x∗1,...,x∗m,σ0+x∗G,...,σ0+x∗G}

m1

=

cs(1 − ρ)

s {x∗1,...,x∗m,σ0+x∗G−1,...,σ0+x∗G−1}

m1

i 1{si=σ0+x∗G−1}f (s, x∗G, . . . , x∗G). (20)

Substituting (19) and (20) in (18) and canceling out common terms gives

f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G, . . . , x∗G)

m1
≥ (1 − ρ)m1f (x∗1, . . . , x∗m, σ0 + x∗G − 1, . . . , σ0 + x∗G − 1, x∗G, . . . , x∗G)

= (1 − ρ)

m1
m i=1(x∗i −1)+m1(σ0+x∗G−1)+(G−m1−m)(x∗G−1)α.

We will now employ Lemma 4.3 again to derive a matching lower bound.

Setting y

=

(

x

∗ 1

,

.

.

.

,

x

∗ m

,

σ0 + x∗G, . . . , σ0 + x∗G, x∗G + 1, . . . , x∗G + 1) and y

=

(

x

∗ 1

,

.

.

.

,

x

∗ m

,

σ0

+ x∗G, . . . , σ0

+ x∗G, x∗G, . . . , x∗G)

m1

m1

23

in Lemma 4.3 yields the inequality

c1f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G + 1, . . . , x∗G + 1)

m1

+

csf (s, x∗G + 1, . . . , x∗G + 1)

s {x∗1,...,x∗m,σ0+x∗G,...,σ0+x∗G}

m1
≥ c1(1 − ρ)m1 f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G, . . . , x∗G)

m1

+

cs(1 − ρ)G−|s|f (s, x∗G, . . . , x∗G),

s {x∗1,...,x∗m,σ0+x∗G,...,σ0+x∗G}

(21)

m1

for some positive constants c1, cs. Observe that

γ(x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G + 1, . . . , x∗G + 1) =
m1

γ0 − 1 γ0

if σ0 = 1 otherwise,

and that the induction hypothesis is satisﬁed in the ﬁrst case. In the second case,

σ(x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G + 1, . . . , x∗G + 1) = σ0 − 1,
m1

and hence the induction hypothesis is satisﬁed in the second case as well. Thus

f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G + 1, . . . , x∗G + 1)
m1
= (1 − ρ) m i=1(x∗i −1)+m1(σ0+x∗G−1)+(G−m1−m)(x∗G−2)α.

(22)

Now consider any s {x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G}, and recall our notation of m˜ 1(s) := i 1{si =

m1

σ0 + x∗G}. If σ0 = 1 or if m˜ 1(s) = 0 then γ((s, x∗G + 1, . . . , x∗G + 1)) ≤ γ0 − 1; if σ > 1 and

m˜ 1(s)

>

0

then

γ((s, x∗G

+

1,

.

.

.

,

x

∗ G

+

1))

≤

γ0

and

σ(s, x∗G

+

1,

.

.

.

,

x

∗ G

+

1)

≤

σ0

−

1.

If

m˜ 1(s)

=

0

then

γ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

γ0

− 1,

otherwise

γ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

≤

γ0,

σ

((s,

x∗G

,

.

.

.

,

x

∗ G

))

=

σ0

and

β((s, x∗G, . . . , x∗G)) ≤ β0 − 1. These terms thus satisfy our induction hypothesis and hence

f (s, x∗G + 1, . . . , x∗G + 1) = (1 − ρ)G−|s|f (s, x∗G, . . . , x∗G).

(23)

Substituting (22) and (23) in (21) gives us our desired matching lower bound
f (x∗1, . . . , x∗m, σ0 + x∗G, . . . , σ0 + x∗G, x∗G, . . . , x∗G) ≤ (1 − ρ) m i=1(x∗i −1)+m1(σ0+x∗G−1)+(G−m1−m)(x∗G−1)α.
m1
This completes the proof for {x|xi ≥ 0 ∀ i ∈ [G]}. We will now show that f (x) = 0 for all {x | mini∈[G] xi < 0}. The arguments above for the case
{x | mini∈[G] xi > 0} imply that for any incentive-compatible function f , the ﬁrst part of Lemma 4.3 must be satisﬁed with equality. This allows us to employ the second part of Lemma 4.3. For i ∈ [G], let yi = yi = xi if xi > 0, and yi − 1 = yi = |xi| otherwise; set yi = yi = B for all i ∈ {G + 1, . . . , N }. Then the second part of Lemma 4.3 necessitates f (x1, . . . , xG) = 0, thus completing the proof.

24

A.4 Proof of Theorem 5.1: Mechanism in absence of coarse belief assumption

Without loss of generality, assume that αmin = 0 since in our setting, the property of incentive compatibility

is invariant to any constant shift and positive scale of the payment. We adopt the succinct notation of

α := αmax − αmin. First consider the case of N = G = 1. Mechanism 1 reduces to f (x) = α(1 − ρ)(x1−1)1{x1 ≥ 0}.

Suppose without loss of generality that the worker’s beliefs for the B options are p1 ≥ · · · ≥ pB and

suppose m = arg maxz

p(z)
z

> ρ . A mechanism that is incentive compatible will strictly maximize

i=1 p(i)

the worker’s expected payment when she selects the options {1, . . . , m}.

Suppose a worker decides to select some of the B options, say options {o1, . . . , o } ⊆ [B]. Then it is

easy to see that her expected payment,

α poi (1 − ρ) −1,
i=1
is maximized when she selects options {1, . . . , }, i.e., the options that are most likely to be correct. It remains to show that among all choices of ∈ [B], the expected payment is maximized when the worker selects = m. Let $ denote the expected payment when the worker selects options:

$ = α pi(1 − ρ) −1.
i=1

Hence for any ∈ {2, . . . , B}, we have

$ −1 = α i=−11 pi(1 − ρ) −2 = 1 1 −

$

α i=1 pi(1 − ρ) −1 1 − ρ

p .
i=1 pi

We know that p < ρ whenever > m, and p > ρ when = m. Furthermore, since p decreases

i=1 pi

i=1 pi

with and i=1 pi increases with , it must also be that ip=1 pi > ρ for all < m. Thus we have $$−1 > 1

for all ≤ m and $$−1 < 1 for all > m, or in other words,

· · · < $m−2 < $m−1 < $m > $m+1 > $m+2 > · · · .

It follows that the worker will be incentivized to choose = m. Let us now consider the case of N = G ≥ 1. By our assumption of the independence of the beliefs of
the worker across the questions, the expected payment equals

G
E α(1 − ρ)(xi−1)1{xi ≥ 0} .
i=1

Since the payments are non-negative, if each individual component in the product is maximized then the product is also necessarily maximized. Each individual component simply corresponds to the setting of N = G = 1 discussed earlier. Thus calling upon our earlier result, we get that the expected payment for the case N = G ≥ 1 is maximized when the worker acts as desired for every question.
Let us ﬁnally consider the general case of N ≥ G ≥ 1. Recall from (3) that the expected payment for the general case is a cascade of two expectations: the outer expectation is with respect to the uniformly random distribution of the G gold standard questions among the N total questions, while the inner expectation is

25

taken over the worker’s beliefs of the different questions conditioned on the choice of the gold standard questions and restricts attention to only these G questions. The arguments above for the case N = G prove that every individual term in the inner expectation is maximized when the worker acts as desired. The outer expectation does not affect this argument. The expected payment is thus maximized when the worker acts as desired.
A.5 Proof of Theorem 5.2: Uniqueness
Without loss of generality, assume that αmin = 0 since in our setting, the property of incentive compatibility is invariant to any constant shift and positive scale of the payment. We adopt the succinct notation of α := αmax−αmin. The proof of this theorem employs some of the tools developed in Shah and Zhou (2015). We begin with a lemma deriving a condition that must necessarily be satisﬁed by any incentive-compatible mechanism. Note that we are not making the coarse belief assumption and supposing that workers can have arbitrary beliefs.
Lemma A.1. Any incentive-compatible mechanism must satisfy
f (x1, . . . , xi−1, xi + 1, xi+1, . . . , xG) = (1 − ρ)f (x1, . . . , xi−1, xi, xi+1, . . . , xG) + ρf (x1, . . . , xi−1, −xi, xi+1, . . . , xG),
for every i ∈ [G] and (x1, . . . , xi−1, xi+1, . . . , xG) ∈ {−(B − 1), . . . , −1, 1, . . . , B}G−1, xi ∈ [B − 1].
Note that the lemma does not use the no-free-lunch condition. The proof of the lemma is provided at the end of this section. Using this lemma, we now complete the proof of the theorem.
Consider any incentive-compatible mechanism f that satisﬁes the no-free-lunch condition. We ﬁrst show that the mechanism must necessarily make a zero payment when one more more questions in the gold standard are attempted incorrectly. To this end, observe that since f ≥ 0 and ρ ∈ (0, 1), the statement of Lemma A.1 necessitates that for every i ∈ [G] and (x1, . . . , xi−1, xi+1, . . . , xG) ∈ {−(B − 1), . . . , B}G−1, xi ∈ [B − 1]:
If f (x1, . . . , xi−1, xi + 1, xi+1, . . . , xG) = 0 then f (x1, . . . , xi−1, xi, xi+1, . . . , xG) = f (x1, . . . , xi−1, −xi, xi+1, . . . , xG) = 0.
A repeated application of this argument implies:
If f (x1, . . . , xi−1, B, xi+1, . . . , xG) = 0 then f (x1, . . . , xi−1, xi, xi+1, . . . , xG) = 0,
for all xi ∈ {−(B − 1), . . . , −1, 1, . . . , B − 1}. Now consider any evaluation (x1, . . . , xG) which has at least one incorrect answer. Suppose with-
out loss of generality that the ﬁrst question is the one answered incorrectly, i.e., x1 ≤ −1. The nofree-lunch condition then makes f (x1, B, . . . , B) = 0. Applying our arguments from above we get that f (x1, x2, . . . , xG) = 0 for every value of (x2, . . . , xG) ∈ {−(B − 1), . . . , −1, 1, . . . , B}.
Substituting this necessary condition in Lemma A.1, we get that for every question i ∈ {1, . . . , G} and every (x1, . . . , xi−1, xi+1, . . . , xG) ∈ [B]G−1, xi ∈ [B − 1],
f (x1, . . . , xi−1, xi + 1, xi+1, . . . , xG) = (1 − ρ)f (x1, . . . , xi−1, xi, xi+1, . . . , xG).
Substituting f (1, . . . , 1) = α, we get the desired answer. We now return to complete the proof of Lemma A.1.
26

Proof of Lemma A.1. First consider the case of G = N . Consider some η, γ ∈ {0, . . . , G−1} with η +γ <

G. Suppose i = η + γ + 1, x1, . . . , xη ∈ [B − 1], xη+1, . . . , xη+γ ∈ −[B − 1] and xη+γ+2, . . . , xN = B.

For every question j ∈ [η + γ], suppose the worker’s belief is δj ∈ (0, ρ) for the last option and 1|−xjδ|j

each for the ﬁrst |xj| options.

One can verify that since δj

<

ρ

<

1 B

and |xj|

≤

B

− 1,

it must be that

1|−xjδ|j > δj, and that incentive-compatibility requires incentivizing the worker to select the ﬁrst |xj| options.

Suppose the worker does so. Now for every question j ∈ {η + γ + 2, . . . , N }, suppose the belief of the

worker is uniform across all B options. The worker should be incentivized to select all B options in this case; suppose the worker does so. Finally, for question i, suppose the worker’s belief is δ ∈ ( ρ2 , 32ρ ) for the last option and 1|x−iδ| each for the ﬁrst |xi| options. Then the worker must be incentivized to select the ﬁrst |xi| options alone if δ < ρ, and select the last option along with the ﬁrst |xi| options if δ > ρ.

Deﬁne {rj}j∈[η+γ] as rj = δj for j ∈ [η], and rj = 1 − δj for j ∈ {η + 1, η + γ}. Let := { 1, . . . , η+γ} ∈ {−1, 1}η+γ. Incentive-compatibility for question i necessitates





(1 − δ)

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, xi, B, . . . , B)

1− j

1+ j

rj 2 (1 − rj ) 2 

∈{−1,1}η+γ

j∈[η+γ]





1− j

1+ j

+δ

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, −xi, B, . . . , B)

rj 2 (1 − rj ) 2 

∈{−1,1}η+γ

j∈[η+γ]





δ>ρ

1− j

1+ j

≶

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, xi + 1, B, . . . , B)

rj 2 (1 − rj) 2  .

δ<ρ ∈{−1,1}η+γ

j∈[η+γ]

The left hand side of this expression is the expected payment if the worker chooses the ﬁrst |xi| options for question (η + γ + 1), while the right hand side is the expected payment if she chooses the ﬁrst |xi| options as well as the last option. For any real-valued variable q, and for any real-valued constants a, b and c,

q<c
aq ≶ b ⇒
q>c

ac = b .

With q = 1 − δ in this argument, we get





(1 − ρ)

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, xi, B, . . . , B)

1− j

1+ j

rj 2 (1 − rj ) 2 

∈{−1,1}η+γ

j∈[η+γ]





1− j

1+ j

+ρ

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, −xi, B, . . . , B)

rj 2 (1 − rj ) 2 

∈{−1,1}η+γ

j∈[η+γ]





1− j

1+ j

−

f ( 1x1, . . . , ηxη, η+1xη+1, . . . , η+γxη+γ, xi + 1, B, . . . , B)

rj 2 (1 − rj) 2  = 0.

∈{−1,1}η+γ

j∈[η+γ]

(24)

The left hand side of (24) represents a polynomial in (η + γ) variables {rj}ηj=+1γ which evaluates to zero for all values of the variables within an (η + γ)-dimensional solid ball. Thus, the coefﬁcients of the monomials
in this polynomial must be zero. In particular, the constant term must be zero. The constant term appears when j = 1 ∀ j in the summations in (24). Setting the constant term to zero gives

(1 − ρ)f (x1, . . . , xη+γ, xη+γ+1, B, . . . , B) + ρf (x1, . . . , xη+γ, −xη+γ+1, B, . . . , B) − f (x1, . . . , xη+γ, xη+γ+1 + 1, B, . . . , B) = 0

27

as desired. Since the arguments above hold for any permutation of the N questions, this completes the proof
for the case of G = N . Now consider the case G < N . Let g : {−(B − 1), . . . , −1, 1, · · · , B}N → R+ represent the expected
payment given an evaluation of all the N answers, when the identities of the gold standard questions are
unknown. Here, the expectation is with respect to the (uniformly random) choice of the G gold standard questions. If (x1, . . . , xN ) ∈ {−(B − 1), . . . , −1, 1, · · · , B}N are the evaluations of the worker’s answers to the N questions then the expected payment is

1

g(x1, . . . , xN ) = N

f (xi1 , . . . , xiG ).

(25)

G (i1,...,iG)⊆{1,...,N }

Applying the same arguments to g as done to f above, gives

(1 − ρ)g(x1, . . . , xη+γ, xη+γ+1, B, . . . , B) + ρg(x1, . . . , xη+γ, −xη+γ+1, B, . . . , B) − g(x1, . . . , xη+γ, xη+γ+1 + 1, B, . . . , B) = 0. (26)

The proof now proceeds via an induction on the quantity (G − η − γ − 1). We begin with the case of (G − η − γ − 1) = G − 1 which implies η = γ = 0. In this case (24) simpliﬁes to

(1 − ρ)g(x1, B, . . . , B) + ρg(−x1, B, . . . , B) = g(x1 + 1, B, . . . , B).

Applying the expansion of function g in terms of function f from (25) for some x1 ∈ [B − 1] gives

(1 − ρ) (c1f (x1, B, . . . , B) + c2f (B, B, . . . , B)) + ρ (c1f (−x1, B, . . . , B) + c2f (B, B, . . . , B)) = c1f (x1 + 1, B, . . . , B) + c2f (B, B, . . . , B)

for constants c1 > 0 and c2 > 0 that respectively represent the probabilities that the ﬁrst question is picked and not picked in the set of G gold standard questions. Cancelling out the common terms on both sides of the equation, we get the desired result

(1 − ρ)f (x1, B, . . . , B) + ρf (−x1, B, . . . , B) = f (x1 + 1, B, . . . , B).

Next, we consider the case when (G − η − γ − 1) questions are skipped in the gold standard, and assume that the result is true when more than (G − η − γ − 1) questions are skipped in the gold standard. In (26), the functions g decompose into a sum of the constituent f functions. These constituent functions f are of two types: the ﬁrst where all of the ﬁrst (η + γ + 1) questions are included in the gold standard, and the second where one or more of the ﬁrst (η + γ + 1) questions are not included in the gold standard. The second case corresponds to situations where there are more than (G − η − γ − 1) questions skipped in the gold standard and hence satisﬁes our induction hypothesis. The terms corresponding to these functions thus cancel out in the expansion of (26). The remainder comprises only evaluations of function f for arguments in which the ﬁrst (η + γ + 1) questions are included in the gold standard. Since the last (N − η − γ − 1) questions are skipped by the worker, the remainder evaluates to

(1 − ρ)c3f (x1, . . . , xη+γ, xi, B, . . . , B) + ρc3f (x1, . . . , xη+γ, −xi, B, . . . , B) = c3f (x1, . . . , xη+γ, xi + 1, B, . . . , B) (27)

for some constant c3 > 0. Dividing throughout by c3 gives the desired result. Finally, the arguments above hold for any permutation of the ﬁrst G questions, thus completing the
proof.

28

A.6 Proof of Theorem 8.1: Mechanism under alternative formulation
Without loss of generality, assume that a = 0 and b = 1 since in our setting, the property of incentive compatibility is invariant to any constant shift and positive scale of the payment.
First consider the case of N = G = 1. Suppose without loss of generality that the worker’s beliefs for the B options are p1, . . . , pB. It is easy to verify that the expected payment $sup when the worker selects the options {o1, . . . , om}, for some m, equals
B
Bσ + (poi − σ).
i=1
It follows that the payment is strictly maximized when the worker selects all options whose beliefs are greater than B, given the assumption that none of the beliefs exactly equals σ.
The arguments above complete the proof for the case N = G = 1. The extension to N ≥ G ≥ 1 follow in a manner identical to the analogous extension in the proof of Theorem 4.1.
The proof of Corollary 8.2 follows in an identical fashion.

A.7 Proof of Theorem 8.3: Negative results under alternative formulation
We present the results of uniqueness and impossibility respectively. We will let f denote any incentive compatible mechanism.

A.7.1 Part A: Uniqueness

Consider

any

m

∈

{1, . . . , smax

−

1}.

Consider

the

set

of

beliefs

p1

=

σ

+

δ,

p2

=

···

=

pm+1

=

1−σ−δ m

and pm+2 = · · · = pB = 0, for some value of δ in the neighborhood of 0. For the values of m under

consideration,

one

can

verify

that

σ

<

1−σ m

<

1.

Consequently,

there

exists

some

value

δmax

>

0

such

that

for every δ

∈

[−δmax, δmax] we have 0

≤

σ+δ

≤

1 and σ

<

1−σ−δ m

≤

1.

In order to achieve the stated

goal, we would thus require to incentivize the worker to select options 1 through (m + 1) if δ > 0, and

select options 2 through (m + 1) if δ < 0. The mechanism f therefore must satisfy the pair of inequalities

δ<0
f (m + 1) ≶ (1 − σ − δ)f (m) + (σ + δ)f (−m).
δ>0
Since the right hand side of the expression above is linear in δ but the left hand side is a constant, we must have

f (m + 1) = (1 − σ)f (m) + σf (−m) for all m ∈ {1, . . . , smax − 1}.

(28)

We will return to this equation later.

Next consider any m ∈ {1, . . . , smax − 2}. Consider the set of beliefs p1 = σ + δ, p2 = σ + δ,

p3

=

···

=

pm+2

=

1−2σ−2δ m

and

pm+3

=

···

=

pB

=

0,

for

some

value

of

δ

in

the

neighborhood

of

0.

For

the values of m under consideration,

one can verify that σ

<

1−2σ m

<

1.

Consequently,

there exists some

value δmax

>

0 such that for every δ

∈

[−δmax, δmax] we have 0

≤

σ+δ

≤

1 and σ

<

1−2σ−2δ m

≤

1.

In

order to achieve the stated goal, we would thus require to incentivize the worker to select options 1 through

(m + 2) if δ > 0, and select options 3 through (m + 2) if δ < 0. The mechanism f thus must satisfy

δ<0
f (m + 2) ≶ (1 − 2σ − 2δ)f (m) + (2σ + 2δ)f (−m).
δ>0

29

Since the right hand side of the expression above is linear in δ but the left hand side is a constant, we must have

f (m + 2) = (1 − 2σ)f (m) + 2σf (−m) for all m ∈ {1, . . . , smax − 2}.

(29)

It follows from (28) and (29) that the values of f (m) for every m ∈ {−(smax−1), . . . , −1, 1, . . . , smax− 2} can be expressed in terms of a linear combination of f (smax) and f (smax − 1). We will now prove that the same holds true for f (−smax) and f (0) as well, whenever these quantities are deﬁned.
The quantity f (−smax) is deﬁned only when smax < B. The reason is that when smax = B, f (−smax) = f (−B) corresponds to a scenario where all the options are selected and the correct option is not, which is impossible. Now consider the set of beliefs p1 = σ + δ, p2 = · · · = psmax = 1s−mσa−x−δ−1 , psmax+1 = , and psmax+2 = · · · = pB = 0, for some values of ≥ 0 and δ in the neighborhood of 0. From the deﬁnition of smax, one can easily verify that σ < s1m−aσx−−1 < 1 whenever smax > 1. Consequently, there exist some values δmax > 0 and max ∈ (0, σ) such that for every δ ∈ [−δmax, δmax] and for every ∈ [0, max], we have 0 ≤ σ + δ ≤ 1 and when smax > 1, we also have σ < 1s−mσa−x−δ−1 ≤ 1. In order to achieve the stated goal, we would thus require to incentivize the worker to select options 1 through smax if δ > 0, and select options 2 through smax if δ < 0. The mechanism f therefore must satisfy
δ<0
(1 − )f (smax) + f (−smax) ≶ (1 − σ − δ − )f (smax − 1) + (σ + δ + )f (−(smax − 1)).
δ>0
Since the right hand side of the expression above is linear in δ but the left hand side does not depend onδ,
we must have

(1 − )f (smax) + f (−smax) = (1 − σ − )f (smax − 1) + (σ + )f (−(smax − 1)).

Since this equation must be true for every ∈ [0, max], we must have

−f (smax) + f (−smax) = −f (smax − 1) + f (−(smax − 1)).

Thus the term f (−smax), whenever applicable, can also be written as a linear combination of f (smax) and

f (smax − 1).

The quantity f (0) is deﬁned only when σ > B1 . The reason is that when σ ≤ B1 , it is mathematically

impossible for the beliefs for all the B options to be less than or equal to σ (recall our assumption that

no belief equals exactly σ). Now consider the set of beliefs p1 = σ + δ, p2 = · · · = pB = 1−Bσ−−1δ , for

some value of δ in the neighborhood of 0. One can verify that in this case of σ > B1 , it must be that

0

<

1−σ B−1

<

σ.

Consequently,

there

exists

some

value

δmax

>

0

such

that

for

every

δ

∈

[−δmax, δmax],

we

have 0

≤

σ+δ

≤

1 and 0

≤

1−σ−δ B−1

<

σ.

In order to achieve the stated goal, we would thus require to

incentivize the worker to select option 1 if δ > 0, and select no options if δ < 0. The mechanism f therefore

must satisfy

δ<0
(σ + δ)f (1) + (1 − σ − δ)f (−1) ≶ f (0).
δ>0
Since the left hand side of the expression above is linear in δ but the right hand side is a constant, we must have

σf (1) + (1 − σ)f (−1) = f (0).

Thus the term f (0), whenever applicable, can also be written as a linear combination of f (smax) and f (smax − 1).
From the arguments above, we get that the design of f has only two degrees of freedom. Given that our claim is only up to some shift and scale, the claim is proved.

30

A.7.2 Part B: Impossibility

Let us ﬁrst prove the result for the case of N = G = 1. The result of part A of Theorem 8.3 implies that if there exists an incentive compatible mechanism for this setting, then the mechanism must be that of Mechanism 2 up to a constant shift and positive scale. Consider a worker with the belief p1 = 1 − σ, p2 = σ and p3 = · · · pB = 0. Since σ < 21 , under an incentive compatible mechanism, the expected payment must be strictly larger if the worker selects only option 1 as compared to the expected payment when the worker selects options 1 and 2. However, one can compute that under Mechanism 2, the expected payment in the two cases is identical. It follows that under any possible incentive-compatible mechanism, the expected payment must be identical in the two following two actions of the worker (a) selecting only option 1, and (b) selecting options 1 and 2. It follows that no mechanism is incentive compatible.
We now move on to the general case of N ≥ G ≥ 1. Consider a worker who knows the answers to questions 2 through N with a belief of 1 in each case. Suppose that for each of these (N − 1) questions, this worker selects the respective options that she thinks are correct. We are now left with the ﬁrst question. Letting X denote a random variable representing the evaluation of the worker’s response to the ﬁrst question, the expected payment from the worker’s point of view is

G

G

N E[f (X, 1, . . . , 1)] + (1 − N )f (1, . . . , 1).

The expectation in the ﬁrst term is taken with respect to the randomness in X. Deﬁning

f˜(X )

:=

G f (X, 1, . . . , 1) + (1 −

G )f (1, . . . , 1),

N

N

and applying the same arguments to f˜ as those for f for the case of N = G = 1 above gives the desired contradiction. This completes the proof.

31

