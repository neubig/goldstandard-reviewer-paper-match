arXiv:1402.1892v2 [stat.ML] 14 May 2014

Thresholding Classiﬁers to Maximize F1 Score
Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy
University of California, San Diego, La Jolla, California, 92093-0404, USA {zlipton,celkan,muralib}@cs.ucsd.edu
Abstract. This paper provides new insight into maximizing F1 scores in the context of binary classiﬁcation and also in the context of multilabel classiﬁcation. The harmonic mean of precision and recall, F1 score is widely used to measure the success of a binary classiﬁer when one class is rare. Micro average, macro average, and per instance average F1 scores are used in multilabel classiﬁcation. For any classiﬁer that produces a real-valued output, we derive the relationship between the best achievable F1 score and the decision-making threshold that achieves this optimum. As a special case, if the classiﬁer outputs are well-calibrated conditional probabilities, then the optimal threshold is half the optimal F1 score. As another special case, if the classiﬁer is completely uninformative, then the optimal behavior is to classify all examples as positive. Since the actual prevalence of positive examples typically is low, this behavior can be considered undesirable. As a case study, we discuss the results, which can be surprising, of applying this procedure when predicting 26,853 labels for Medline documents.
Keywords: machine learning, evaluation methodology, F1-score, multilabel classiﬁcation, binary classiﬁcation
1 Introduction
Performance metrics are useful for comparing the quality of predictions across systems. Some commonly used metrics for binary classiﬁcation are accuracy, precision, recall, F1 score, and Jaccard index [15]. Multilabel classiﬁcation is an extension of binary classiﬁcation that is currently an area of active research in supervised machine learning [18]. Micro averaging, macro averaging, and per instance averaging are three commonly used variants of F1 score used in the multilabel setting. In general, macro averaging increases the impact on ﬁnal score of performance on rare labels, while per instance averaging increases the importance of performing well on each example [17]. In this paper, we present theoretical and experimental results on the properties of the F1 metric.1
1 For concreteness, the results of this paper are given speciﬁcally for the F1 metric and its multilabel variants. However, the results can be generalized to Fβ metrics for β = 1.

2

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

Two approaches exist for optimizing performance on F1. Structured loss minimization incorporates the performance metric into the loss function and then optimizes during training. In contrast, plug-in rules convert the numerical outputs of a classiﬁer into optimal predictions [5]. In this paper, we highlight the latter scenario to diﬀerentiate between the beliefs of a system and the predictions selected to optimize alternative metrics. In the multilabel case, we show that the same beliefs can produce markedly dissimilar optimally thresholded predictions depending upon the choice of averaging method.
That F1 is asymmetric in the positive and negative class is well-known. Given complemented predictions and actual labels, F1 may award a diﬀerent score. It also generally known that micro F1 is aﬀected less by performance on rare labels, while Macro-F1 weighs the F1 of on each label equally [11]. In this paper, we show how these properties are manifest in the optimal decision-making thresholds and introduce a theorem to describe that threshold. Additionally, we demonstrate that given an uninformative classiﬁer, optimal thresholding to maximize F1 predicts all instances positive regardless of the base rate.
While F1 is widely used, some of its properties are not widely recognized. In particular, when choosing predictions to maximize the expectation of F1 for a batch of examples, each prediction depends not only on the probability that the label applies to that example, but also on the distribution of probabilities for all other examples in the batch. We quantify this dependence in Theorem 1, where we derive an expression for optimal thresholds. The dependence makes it diﬃcult to relate predictions that are optimally thresholded for F1 to a system’s predicted probabilities.
We show that the diﬀerence in F1 score between perfect predictions and optimally thresholded random guesses depends strongly on the base rate. As a result, assuming optimal thresholding and a classiﬁer outputting calibrated probabilities, predictions on rare labels typically gets a score between close to zero and one, while scores on common labels will always be high. In this sense, macro average F1 can be argued not to weigh labels equally, but actually to give greater weight to performance on rare labels.
As a case study, we consider tagging articles in the biomedical literature with MeSH terms, a controlled vocabulary of 26,853 labels. These labels have heterogeneously distributed base rates. We show that if the predictive features for rare labels are lost (because of feature selection or another cause) then the optimal threshold to maximize macro F1 leads to predicting these rare labels frequently. For the case study application, and likely for similar ones, this behavior is far from desirable.

2 Deﬁnitions of Performance Metrics
Consider binary classiﬁcation in the single or multilabel setting. Given training data of the form { x1, y1 , . . . , xn, yn } where each xi is a feature vector of dimension d and each yi is a binary vector of true labels of dimension m, a probabilistic classiﬁer outputs a model which speciﬁes the conditional probability

Thresholding Classiﬁers to Maximize F1 Score

3

Actual Positive Actual Negative

Predicted Positive

tp

fp

Predicted Negative

fn

tn

Fig. 1: Confusion Matrix

of each label applying to each instance given the feature vector. For a batch of
data of dimension n × d, the model outputs an n × m matrix C of probabilities.
In the single-label setting, m = 1 and C is an n × 1 matrix, i.e. a column vector. A decision rule D(C) : Rn×m → {0, 1}n×m converts a matrix of probabilities
C to binary predictions P . The gold standard G ∈ Rn×m represents the true values of all labels for all instances in a given batch. A performance metric M
assigns a score to a prediction given a gold standard:

M (P |G) : {0, 1}n×m × {0, 1}n×m → R ∈ [0, 1].

The counts of true positives tp, false positives f p, false negatives f n, and true negatives tn are represented via a confusion matrix (Figure 1).
Precision p = tp/(tp + f p) is the fraction of all positive predictions that are true positives, while recall r = tp/(tp + f n) is the fraction of all actual positives that are predicted positive. By deﬁnition the F1 score is the harmonic mean of precision and recall: F 1 = 2/(1/r + 1/p). By substitution, F1 can be expressed as a function of counts of true positives, false positives and false negatives:

2tp

F1 =

.

(1)

2tp + f p + f n

The harmonic mean expression for F1 is undeﬁned when tp = 0, but the translated expression is deﬁned. This diﬀerence does not impact the results below.

2.1 Basic Properties of F1
Before explaining optimal thresholding to maximize F1, we ﬁrst discuss some properties of F1. For any ﬁxed number of actual positives in the gold standard, only two of the four entries in the confusion matrix (Figure 1) vary independently. This is because the number of actual positives is equal to the sum tp + f n while the number of actual negatives is equal to the sum tn + f p. A second basic property of F1 is that it is non-linear in its inputs. Speciﬁcally, ﬁxing the number f p, F1 is concave as a function of tp (Figure 2). By contrast, accuracy is a linear function of tp and tn (Figure 3).
As mentioned in the introduction, F1 is asymmetric. By this, we mean that the score assigned to a prediction P given gold standard G can be arbitrarily diﬀerent from the score assigned to a complementary prediction P c given complementary gold standard Gc. This can be seen by comparing Figure 2 with Figure 5. This asymmetry is problematic when both false positives and false negatives are costly. For example, F1 has been used to evaluate the classiﬁcation of tumors as benign or malignant [1], a domain where both false positives and false negatives have considerable costs.

4

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

F1 score Accuracy

Base Rate of 0.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
True Positive

Base Rate of 0.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
True Positive

Fig. 2: Holding base rate and f p constant, F1 is concave in tp. Each line is a diﬀerent value of f p.

Fig. 3: Unlike F1, accuracy oﬀers linearly increasing returns. Each line is a ﬁxed value of f p.

2.2 Multilabel Performance Measures

While F1 was developed for single-label information retrieval, as mentioned there are variants of F1 for the multilabel setting. Micro F1 treats all predictions on all labels as one vector and then calculates the F1 score. In particular,

nm

tp = 2

1(Pij = 1)1(Gij = 1).

i=1 j=1

We deﬁne f p and f n analogously and calculate the ﬁnal score using (1). Macro F1, which can also be called per label F1, calculates the F1 for each of the m labels and averages them:
1m F 1Macro(P |G) = m F 1(P:j , G:j ).
j=1

Per instance F1 is similar but averages F1 over all n examples: 1n
F 1Instance(P |G) = n F 1(Pi:, Gi:).
i=1

Accuracy is the fraction of all instances that are predicted correctly:

tp + tn

Acc =

.

tp + tn + f p + f n

Accuracy is adapted to the multilabel setting by summing tp and tn for all labels and then dividing by the total number of predictions:

1n Acc(P |G) =
nm

m
1(Pij = Gij).

i=1 j=1

Thresholding Classiﬁers to Maximize F1 Score

5

Base Rate 0.1

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0.1
0.08 0.06 0.04 0.02
True Positive

00

1 0.8 0.6 0.4 0.2
False Positive

Fig. 4: For ﬁxed base rate, F1 is a non-linear function with only two degrees of freedom.

Jaccard Index, a monotonically increasing function of F1, is the ratio of the intersection of predictions and gold standard to their union:

tp

Jaccard =

.

tp + f n + f p

3 Prior Work
Motivated by the widespread use of F1 in information retrieval and in single and multilabel binary classiﬁcation, researchers have published extensively on its optimization. [8] propose an outer-inner maximization technique for F1 maximization, and [4] study extensions to the multilabel setting, showing that simple threshold search strategies are suﬃcient when individual probabilistic classiﬁers are independent. Finally, [6] describe how the method of [8] can be extended to eﬃciently label data points even when classiﬁer outputs are dependent. More recent work in this direction can be found in [19]. However, none of this work directly identiﬁes the relationship of optimal thresholds to the maximum achievable F1 score over all thresholds, as we do here.
While there has been work on applying general constrained optimization techniques to related metrics [13], research often focuses on speciﬁc classiﬁcation methods. In particular, [16] study F1 optimization for conditional random ﬁelds and [14] perform the same optimization for SVMs. In our work, we study the consequences of such optimization for probabilistic classiﬁers, particularly in the multilabel setting.
A result similar to our special case (Corollary 1) was recently derived in [20]. However, their derivation is complex and does not prove our more general

6

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

F1 score
Expected F1 Score

Base Rate of 0.1 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

True Negative

Fig. 5: F1 score for ﬁxed base rate and number f n of false negatives. F1 oﬀers increasing marginal returns as a function of tn. Each line is a ﬁxed value of f n.

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Percent Predicted Positive
Fig. 6: The expected F1 score of an optimally thresholded random guess is highly dependent on the base rate.

Theorem 1 which describes the optimal decision-making threshold even when the scores output by a classiﬁer are not probabilities. Their paper also does not contain the empirical version we derive for the multilabel setting in Theorem 2.
The batch observation is related to the observation in [9] that given some classiﬁer, a speciﬁc example may or may not cross the decision threshold, depending on the other examples present in the test data. However, they do not identify this threshold as F21 or make use of this fact to explain the diﬀerences between predictions made to optimize micro and macro average F1.
4 Optimal Decision Regions for F1 Maximization
In this section, we provide a characterization of the optimal decision regions that maximize F1 and, for a special case, we present a relationship between the optimal threshold and the maximum achievable F1 score.
We assume that the classiﬁer outputs real-valued scores s and that there exist two distributions p(s|t = 1) and p(s|t = 0) that are the conditional probability of seeing the score s when the true label t is 1 or 0, respectively. We assume that these distributions are known in this section; the next section discusses an empirical version of the result. Note also that in this section tp etc. are fractions that sum to one, not counts.
Given p(s|t = 1) and p(s|t = 0), we seek a decision rule D : s → {0, 1} mapping scores to class labels such that the resultant classiﬁer maximizes F1. We start with a lemma that is valid for any D.
Lemma 1. The true positive rate tp = b s:D(s)=1 p(s|t = 1)ds where b = p(t = 1) is the base rate.

Thresholding Classiﬁers to Maximize F1 Score

7

Proof. Clearly tp = s:D(s)=1 p(t = 1|s)p(s)ds. Bayes rule says that p(t = 1|s) = p(s|t = 1)p(t = 1)/p(s). Hence tp = b s:D(s)=1 p(s|t = 1)ds.
Using three similar lemmas, the entries of the confusion matrix are

tp = b

p(s|t = 1)ds

s:D(s)=1

fn = b

p(s|t = 1)ds

s:D(s)=0

f p = (1 − b)

p(s|t = 0)ds

s:D(s)=1

tn = (1 − b)

p(s|t = 0)ds.

s:D(s)=0

The following theorem describes the optimal decision rule that maximizes F1.

Theorem 1. A score s is assigned to the positive class, that is D(s) = 1, by a classiﬁer that maximizes F1 if and only if

b · p(s|t = 1)

≥J

(2)

(1 − b) · p(s|t = 0)

where J = fn+ttpp+fp is the Jaccard index of the optimal classiﬁer, with ambiguity given equality in (2).

Before we provide the proof of this theorem, we note the diﬀerence between the rule in (2) and conventional cost-sensitive decision making [7] or NeymanPearson detection. In the latter, the right hand side J is replaced by a constant λ that depends only on the costs of 0 − 1 and 1 − 0 classiﬁcation errors, and not on the performance of the classiﬁer on the entire batch. We will later elaborate on this point, and describe how this relationship leads to potentially undesirable thresholding behavior for many applications in the multilabel setting.

Proof. Divide the domain of s into regions of size ∆. Suppose that the decision
rule D(·) has been ﬁxed for all regions except a particular region denoted ∆
around a point (with some abuse of notation) s. Write P1(∆) = ∆ p(s|t = 1) and deﬁne P0(∆) similarly.
Suppose that the F1 achieved with decision rule D for all scores besides D(∆) is F 1 = 2tp+2ftnp+fp . Now, if we add ∆ to the positive part of the decision rule, D(∆) = 1, then the new F1 score will be

F1 =

2tp + 2bP1(∆) .

2tp + 2bP1(∆) + f n + f p + (1 − b)P0(∆)

On the other hand, if we add ∆ to the negative part of the decision rule, D(∆) = 0, then the new F1 score will be

2tp

F1 =

.

2tp + f n + bP1(∆) + f p

8

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

We add ∆ to the positive class only if F 1 ≥ F 1 . With some algebraic simpliﬁcation, this condition becomes

bP1(∆) ≥

tp .

(1 − b)P0(∆) tp + f n + f p

Taking the limit |∆| → 0 gives the claimed result.

If, as a special case, the model outputs calibrated probabilities, that is p(t = 1|s) = s and p(t = 0|s) = 1 − s, then we have the following corollary.

Corollary 1. An instance with predicted probability s is assigned to the positive class by the optimal decision rule that maximizes F1 if and only if s ≥ F/2 where F = 2tp+2ftnp+fp is the F1 score achieved by this optimal decision rule.
Proof. Using the deﬁnition of calibration and then Bayes rule, for the optimal decision surface for assigning a score s to the positive class

p(t = 1|s) s

p(s|t = 1)b

=

=

.

(3)

p(t = 0|s) 1 − s p(s|t = 0)(1 − b)

Incorporating (3) in Theorem 1 gives

s

tp

≥

.

1 − s f n + tp + f p

Simplifying results in

tp

F

s≥

=.

2tp + f n + f p 2

Thus, the optimal threshold in the calibrated case is half the maximum F 1. Above, we assume that scores have a distribution conditioned on the true
class. Using the intuition in the proof of Theorem 1, we can also derive an empirical version of the result. To save space, we provide a more general version of the empirical result in the next section for multilabel problems, noting that a similar non-probabilistic statement holds for the single label setting as well.

4.1 Maximizing Expected F1 Using a Probabilistic Classiﬁer
The above result can be extended to the multilabel setting with dependence. We give a diﬀerent proof that conﬁrms the optimal threshold for empirical maximization of F1.
We ﬁrst present an algorithm from [6]. Let s be the output vector of length n scores from a model, to predict n labels in the multilabel setting. Let t ∈ {0, 1}n be the gold standard and h ∈ {0, 1}n be the thresholded output for a given set of n labels. In addition, deﬁne a = tp + f n, the total count of positive labels in the gold standard and c = tp + f p the total count of predicted positive labels. Note that a and c are functions of t and h, though we suppress this dependence

Thresholding Classiﬁers to Maximize F1 Score

9

in notation. Deﬁne za = is

t:tp+fn=a tp(t). The maximum achievable macro F1

F 1 = max max Ep(t|s) c h:tp+f p=c = max max 2hT c h:tp+f p=c a

2tp 2tp + f p + f n
za .
a+c

Algorithm: Loop over the number of predicted positives c. Sort the vector a az+ac of length n. Proceed along its entries one by one. Adding an entry to the positive class increases the numerator by za, which is always positive. Stop after entry number c. Pick the c value and corresponding threshold which give the largest F1. Some algebra gives the following interpretation:

max E(F 1) = max E(tp|c) p(a).
c c a a+c

Theorem

2.

The

stopping

threshold

will

be

max

E

p

(y|s

)

[

F1 2

].

4.2 Consequences of F1 Optimal Classiﬁer Design
We demonstrate two consequences of designing classiﬁers that maximize F1. These are the “batch observation” and the “uninformative classiﬁer observation.” We will later demonstrate with a case study that these can combine to produce surprising and potentially undesirable optimal predictions when macro F1 is optimized in practice.
The batch observation is that a label may or may not be predicted for an instance depending on the distribution of other probabilities in the batch. Earlier, we observed a relationship between the optimal threshold and the maximum
E(F 1) and demonstrated that the maximum E(F 1) is related to the distribution
of probabilities for all predictions. Therefore, depending upon the distribution in which an instance is placed, it may or may not exceed the optimal threshold. Note that because F1 can never exceed 1, the optimal threshold can never exceed .5.
Consider for example an instance with probability 0.1. It will be predicted positive if it has the highest probability of all instances in a batch. However, in a diﬀerent batch, where the probabilities assigned to all other elements are 0.5
and n is large, the maximum E(F 1) would be close to 2/3. According to the
theorem, we will predict positive on this last instance only if it has a probability greater than 1/3.
An uninformative classiﬁer is one that assigns the same score to all examples. If these scores are calibrated probabilities, the base rate is assigned to every example.

10

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

Theorem 3. Given an uninformative classiﬁer for a label, optimal thresholding to maximize F1 results in predicting all examples positive.

Proof. Given an uninformative classiﬁer, we seek the optimal threshold that maximizes E(F 1). The only choice is how many labels to predict. By symmetry between the instances, it doesn’t matter which instances are labeled positive.
Let a = tp + f n be the number of actual positives and let c = tp + f p be the number of positive predictions. The denominator of the expression for F1 in Equation (1), that is 2tp + f p + f n = a + c, is constant. The number of true positives, however, is a random variable. Its expected value is equal to the sum of the probabilities that each example predicted positive actually is positive:

(F 1) = 2 ci=1 b = 2c · b

E

a+c a+c

where b = a/n is the base rate. To maximize this expectation as a function of c, we calculate the partial derivative with respect to c, applying the product rule:

∂

∂ 2c · b 2b

2c · b

∂c E(F 1) = ∂c a + c = a + c − (a + c)2 .

Both terms in the diﬀerence are always positive, so we can show that this derivative is always positive by showing that

2b

2c · b

a + c > (a + c)2 .

Simpliﬁcation gives the condition 1 > a+c c . As this condition always holds, the derivative is always positive. Therefore, whenever the frequency of actual posi-
tives in the test set is nonzero, and the classiﬁer is uninformative, expected F1
is maximized by predicting that all examples are positive.

For low base rates an optimally thresholded uninformative classiﬁer achieves
E(F 1) close to 0, while for high base rates E(F 1) is close to 1 (Figure 6). We
revisit this point in the context of macro F1.

5 Multilabel Setting
Diﬀerent metrics are used to measure diﬀerent aspects of a system’s performance. However, by changing the loss function, this can change the optimal predictions. We relate the batch observation to discrepancies between predictions optimal for micro and macro F1. We show that while micro F1 is dominated by performance on common labels, macro F1 disproportionately weights rare labels. Additionally, we show that macro averaging over F1 can conceal uninformative classiﬁer thresholding.
Consider the equation for F1, and imagine tp, f p, and f n to be known for m − 1 labels with some distribution of base rates. Now consider the mth label

Thresholding Classiﬁers to Maximize F1 Score

11

to be rare with respect to the distribution. A perfect classiﬁer increases tp by a small amount ε equal to the number b · n of actual positives for that rare label, while contributing nothing to the counts f p or f n:

2(tp + b · n)

F1 =

.

2(tp + b · n) + f p + f n

On the other hand, a trivial prediction of all negative only increases f n by a

small amount:

2tp

F1 =

.

2tp + f p + (f n + b · n)

By contrast, predicting all positive for a rare label will increase f p by a large amount β = n − ε. We have

F1

1 + bt·pn

= F1 1+

nb

.

a+c+b·n

where a and c are the number of positives in the gold standard and the number

of positive predictions for the ﬁrst m − 1 labels. We have a + c ≤ n i bi and

so if bm

i bi this ratio is small. Thus, performance on rare labels is washed

out.

In the single-label setting, the small range between the F1 value achieved by a

trivial classiﬁer and a perfect one may not be problematic. If a trivial system gets

a score of 0.9, we can adjust the scale for what constitutes a good score. However,

when averaging separately calculated F1 over all labels, this variability can skew

scores to disproportionately weight performance on rare labels. Consider the two

label case when one label has a base rate of 0.5 and the other has a base rate

of 0.1. The corresponding expected F1 for trivial classiﬁers are 0.67 and 0.18

respectively. Thus the expected F1 for optimally thresholded trivial classiﬁers is

0.42. However, an improvement to perfect predictions on the rare label elevates

the macro F1 to 0.84 while such an improvement on the common label would

only correspond to a macro F1 of 0.59. Thus the increased variability of F1

results in high weight for rare labels in macro F1.

For a rare label with an uninformative classiﬁer, micro F1 is optimized by

predicting all negative while macro is optimized by predicting all positive. Ear-

lier, we proved that the optimal threshold for predictions based on a calibrated

probabilistic classiﬁer is half of the maximum F1 attainable given any thresh-

old setting. In other words, which batch an example is submitted with aﬀects

whether a positive prediction will be made. In practice, a system may be tasked

with predicting labels with widely varying base rates. Additionally a classiﬁer’s

ability to make conﬁdent predictions may vary widely from label to label.

Optimizing micro F1 as compared to macro F1 can be thought of as choosing

optimal thresholds given very diﬀerent batches. If the base rate and distribution

of probabilities assigned to instances vary from label to label, so will the predic-

tions. Generally, labels with low base rates and less informative classiﬁers will

be over-predicted to maximize macro F1 as compared to micro F1. We present

empirical evidence of this phenomenon in the following case study.

12

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

MeSH Term

Count Max F1 Threshold

Humans

2346

0.9160

0.458

Male

1472

0.8055

0.403

Female

1439

0.8131

0.407

Phosphinic Acids 1401 1.544 · 10−4 7.71 · 10−5

Penicillanic Acid 1064 8.534 · 10−4 4.27 · 10−4

Adult

1063

0.7004

0.350

Middle Aged Platypus

1028

0.7513

0.376

980 4.676 · 10−4 2.34 · 10−4

Fig. 7: Frequently predicted MeSH Terms. When macro F1 is optimized, low thresholds are set for rare labels (bold) with uninformative classiﬁers.

6 Case Study
This section discusses a case study that demonstrates how in practice, thresholding to maximize macro-F1 can produce undesirable predictions. To our knowledge, a similar real-world case of pathological behavior has not been previously described in the literature, even though macro averaging F1 is a common approach.
We consider the task of assigning tags from a controlled vocabulary of 26,853 MeSH terms to articles in the biomedical literature using only titles and abstracts. We represent each abstract as a sparse bag-of-words vector over a vocabulary of 188,923 words. The training data consists of a matrix A with n rows and d columns, where n is the number of abstracts and d is the number of features in the bag of words representation. We apply a tf-idf text preprocessing step to the bag of words representation to account for word burstiness [10] and to elevate the impact of rare words.
Because linear regression models can be trained for multiple labels eﬃciently, we choose linear regression as a model. Note that square loss is a proper loss function and does yield calibrated probabilistic predictions [12]. Further, to increase the speed of training and prevent overﬁtting, we approximate the training matrix A by a rank restricted Ak using singular value decomposition. One potential consequence of this rank restriction is that the signal of extremely rare words can be lost. This can be problematic when rare terms are the only features of predictive value for a label.
Given the probabilistic output of the classiﬁer and the theory relating optimal thresholds to maximum attainable F1, we designed three diﬀerent plug-in rules to maximize micro, macro and per instance F1. Inspection of the predictions to maximize micro F1 revealed no irregularities. However, inspecting the predictions thresholded to maximize performance on macro F1 showed that several terms with very low base rates were predicted for more than a third of all test documents. Among these terms were “Platypus”, “Penicillanic Acids” and “Phosphinic Acids” (Figure 7).
In multilabel classiﬁcation, a label can have low base rate and an uninformative classiﬁer. In this case, optimal thresholding requires the system to predict

Thresholding Classiﬁers to Maximize F1 Score

13

all examples positive for this label. In the single-label case, such a system would achieve a low F1 and not be used. But in the macro averaging multilabel case, the extreme thresholding behavior can take place on a subset of labels, while the system manages to perform well overall.

7 A Winner’s Curse

In practice, decision regions that maximize F1 are often set experimentally, rather than analytically. That is, given a set of training examples, their scores and ground truth decision regions for scores that map to diﬀerent labels are set that maximize F1 on the training batch.
In such situations, the optimal threshold can be subject to a winner’s curse [2] where a sub-optimal threshold is chosen because of sampling eﬀects or limited training data. As a result, the future performance of a classiﬁer using this threshold is less than the empirical performance. We show that threshold optimization for F1 is particularly susceptible to this phenomenon (which is a type of overﬁtting).
In particular, diﬀerent thresholds have diﬀerent rates of convergence of estimated F1 with number of samples n. As a result, for a given n, comparing the empirical performance of low and high thresholds can result in suboptimal performance. This is because, for a ﬁxed number of samples, some thresholds converge to their true error rates while others have higher variance and may be set erroneously. We demonstrate these ideas for a scenario with an uninformative model, though they hold more generally.
Consider an uninformative model, for a label with base rate b. The model is uninformative in the sense that output scores are si = b + ni ∀ i, where ni = N (0, σ2). Thus, scores are uncorrelated with and independent of the true labels. The empirical accuracy for a threshold t is

Atexp = n1 1[Si ≥ t] + n1 1[Si ≤ t] (4)

i∈+

i∈−

where + and − index the positive and negative class respectively. Each term in Equation (4) is the sum of O(n) i.i.d random variables and has exponential (in n) rate of convergence to the mean irrespective of the base rate b and the threshold t. Thus, for a ﬁxed number T of threshold choices, the probability of choosing the wrong threshold Perr ≤ T 2− n where depends on the distance between the optimal and next nearest threshold. Even if errors occur the most likely errors are thresholds close to the true optimal threshold (a consequence of Sanov’s Theorem [3]).
Consider how F1-maximizing thresholds would be set experimentally, given a training batch of independent ground truth and scores from an uninformative classiﬁer. The scores si can be sorted in decreasing order (w.l.o.g.) since they are independent of the true labels for an uninformative classiﬁer. Based on these, we empirically select the threshold that maximizes F1 on the training batch. The

14

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

optimal empirical threshold will lie between two scores that include the value F21 , when the scores are calibrated, in accordance with Theorem 1.
The threshold smin that classiﬁes all examples positive (and maximizes F 1
analytically by Theorem 3) has an empirical F1 close to its expectation of 12+bb = 1+21/b since tp, f p and f n are all estimated from the entire data. Consider the threshold smax that classiﬁes only the ﬁrst example positive and all others
negative. With probability b, this has F1 score 2/(2 + b · n), which is lower than
that of the optimal threshold only when

1 + n8 − 1

b≥

.

2

Despite the threshold smax being far from optimal, it has a constant probability of having a higher F1 on training data, a probability that does not decrease with n, for n < (1 − b)/b2. Therefore, optimizing F1 will have a sharp threshold behavior, where for n < (1 − b)/b2 the algorithm will identify large thresholds with constant probability, whereas for larger n it will correctly identify small thresholds. Note that identifying optimal thresholds for F 1 is still problematic since it then leads to issue identiﬁed in the previous section. While these issues are distinct, they both arise from the nonlinearity of F1 score and its asymmetric treatment of positive and negative labels.
We simulate this behavior, executing 10,000 runs for each setting of the base rate, with n = 106 samples for each run to set the threshold (Figure 8). Scores are chosen using variance σ2 = 1. True labels are assigned at the base rate, independent of the scores. The threshold that maximizes F1 on the training set is selected. We plot a histogram of the fraction predicted positive as a function of the empirically chosen threshold. There is a shift from predicting almost all positives to almost all negatives as base rate is decreased. In particular, for low base rate b, even with a large number of samples, a small fraction of examples are predicted positive. The analytically derived optimal decision in all cases is to predict all positive, i.e. to use a threshold of 0.

8 Discussion
In this paper, we present theoretical and empirical results describing the properties of the F1 performance metric for multilabel classiﬁcation. We relate the best achievable F1 score to the optimal decision-making threshold and show that when a classiﬁer is uninformative, predicting all instances positive maximizes the expectation of F1. Further, we show that in the multilabel setting, this behavior can be problematic when the metric to maximize is macro F1 and for a subset of rare labels the classiﬁer is uninformative. In contrast, we demonstrate that given the same scenario, expected micro F1 is maximized by predicting all examples to be negative. This knowledge can be useful as such scenarios are likely to occur in settings with a large number of labels. We also demonstrate that micro F1 has the potentially undesirable property of washing out performance on rare labels.

Thresholding Classiﬁers to Maximize F1 Score

15

1

Base rate 0.5

0.1

0.05

0.8

0.01

0.001

0.0001

0.6 0.00001 0.000001

0.4

Fraction of runs

0.2

0

0

0.2

0.4

0.6

0.8

1

Percentage declared positive

Fig. 8: The distribution of experimentally chosen thresholds changes with varying b. For small b, a small fraction of examples are predicted positive even though the optimal thresholding is to predict all positive.

No single performance metric can capture every desirable property. For example, separately reporting precision and recall is more informative than reporting F1 alone. Sometimes, however, it is practically necessary to deﬁne a single performance metric to optimize. Evaluating competing systems and objectively choosing a winner presents such a scenario. In these cases, a change of performance metric can have the consequence of altering optimal thresholding behavior.
References
1. Akay, M.F.: Support vector machines combined with feature selection for breast cancer diagnosis. Expert Systems with Applications 36(2), 3240–3247 (2009)
2. Capen, E.C., Clapp, R.V., Campbell, W.M.: Competitive bidding in high-risk situations. Journal of Petroleum Technology 23(6), 641–653 (1971)
3. Cover, T.M., Thomas, J.A.: Elements of information theory. John Wiley & Sons (2012)
4. del Coz, J.J., Diez, J., Bahamonde, A.: Learning nondeterministic classiﬁers. Journal of Machine Learning Research 10, 2273–2293 (2009)
5. Dembczynski, K., Kotlowski, W., Jachnik, A., Waegeman, W., Hu¨llermeier, E.: Optimizing the F-measure in multi-label classiﬁcation: Plug-in rule approach versus structured loss minimization. In: ICML (2013)

16

Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy

6. Dembczyn´ski, K., Waegeman, W., Cheng, W., Hu¨llermeier, E.: An exact algorithm for F-measure maximization. In: Neural Information Processing Systems (2011)
7. Elkan, C.: The foundations of cost-sensitive learning. In: International joint conference on artiﬁcial intelligence. pp. 973–978 (2001)
8. Jansche, M.: A maximum expected utility framework for binary sequence labeling. In: Annual Meeting of the Association For Computational Linguistics. p. 736 (2007)
9. Lewis, D.D.: Evaluating and optimizing autonomous text classiﬁcation systems. In: Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval. pp. 246–254. ACM (1995)
10. Madsen, R., Kauchak, D., Elkan, C.: Modeling word burstiness using the Dirichlet distribution. In: Proceedings of the International Conference on Machine Learning (ICML). pp. 545–552 (Aug 2005)
11. Manning, C., Raghavan, P., Schu¨tze, H.: Introduction to information retrieval, vol. 1. Cambridge University Press (2008)
12. Menon, A., Jiang, X., Vembu, S., Elkan, C., Ohno-Machado, L.: Predicting accurate probabilities with a ranking loss. In: Proceedings of the International Conference on Machine Learning (ICML) (Jun 2012)
13. Mozer, M.C., Dodier, R.H., Colagrosso, M.D., Guerra-Salcedo, C., Wolniewicz, R.H.: Prodding the ROC curve: Constrained optimization of classiﬁer performance. In: NIPS. pp. 1409–1415 (2001)
14. Musicant, D.R., Kumar, V., Ozgur, A., et al.: Optimizing F-measure with support vector machines. In: FLAIRS Conference. pp. 356–360 (2003)
15. Sokolova, M., Lapalme, G.: A systematic analysis of performance measures for classiﬁcation tasks. Information Processing and Management 45, 427–437 (2009)
16. Suzuki, J., McDermott, E., Isozaki, H.: Training conditional random ﬁelds with multivariate evaluation measures. In: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. pp. 217–224. Association for Computational Linguistics (2006)
17. Tan, S.: Neighbor-weighted k-nearest neighbor for unbalanced text corpus. Expert Systems with Applications 28, 667–671 (2005)
18. Tsoumakas, Grigorios & Katakis, I.: Multi-label classiﬁcation: An overview. International Journal of Data Warehousing and Mining 3(3), 1–13 (2007)
19. Ye, N., Chai, K.M., Lee, W.S., Chieu, H.L.: Optimizing F-measures: A tale of two approaches. In: Proceedings of the International Conference on Machine Learning (2012)
20. Zhao, M.J., Edakunni, N., Pocock, A., Brown, G.: Beyond Fano’s inequality: Bounds on the optimal F-score, BER, and cost-sensitive risk and their implications. Journal of Machine Learning Research 14(1), 1033–1090 (2013)

