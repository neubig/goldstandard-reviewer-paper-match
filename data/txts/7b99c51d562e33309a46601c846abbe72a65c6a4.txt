What to Pre-Train on? Efﬁcient Intermediate Task Selection
Clifton Poth, Jonas Pfeiffer, Andreas Rücklé∗, and Iryna Gurevych Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt
www.ukp.tu-darmstadt.de

arXiv:2104.08247v2 [cs.CL] 10 Sep 2021

Abstract
Intermediate task ﬁne-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to ﬁnd the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efﬁciently identifying beneﬁcial tasks for intermediate transfer learning. We focus on parameter and computationally efﬁcient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method. We experiment with a diverse set of 42 intermediate and 11 target English classiﬁcation, multiple choice, question answering, and sequence tagging tasks. Our results demonstrate that efﬁcient embedding based methods, which rely solely on the respective datasets, outperform computational expensive few-shot ﬁne-tuning approaches. Our best methods achieve an average Regret@3 of 1% across all target tasks, demonstrating that we are able to efﬁciently identify the best datasets for intermediate training. 1
1 Introduction
Large pre-trained language models (LMs) are continuously pushing the state of the art across various NLP tasks. The established procedure performs self-supervised pre-training on a large text corpus and subsequently ﬁne-tunes the model on a speciﬁc target task (Devlin et al., 2019; Liu et al., 2019b). The same procedure has also been applied to adapter-based training strategies, which achieve on-par task performance to full model ﬁne-tuning while being considerably more parameter efﬁcient (Houlsby et al., 2019) and faster to
∗Contributions made prior to joining Amazon. 1Code released at https://github.com/ Adapter-Hub/efficient-task-transfer.

train (Rücklé et al., 2021).2 Besides being more efﬁcient, adapters are also highly modular, enabling a wider range of transfer learning techniques (Pfeiffer et al., 2020b, 2021a,b; Üstün et al., 2020; Vidoni et al., 2020; Rust et al., 2021; Ansell et al., 2021).
Extending upon the established two-step learning procedure, incorporating intermediate stages of knowledge transfer can yield further gains for fully ﬁne-tuned models. For instance, Phang et al. (2018) sequentially ﬁne-tune a pre-trained language model on a compatible intermediate task before target task ﬁne-tuning. It has been shown that this is most effective for low-resource target tasks, however, not all task combinations are beneﬁcial and many yield decreased performances (Phang et al., 2018; Wang et al., 2019a; Pruksachatkun et al., 2020). The abundance of diverse labeled datasets as well as the continuous development of new pre-trained LMs calls for methods that efﬁciently identify intermediate dataset that beneﬁt the target task.
So far, it is unclear how adapter-based approaches behave with intermediate ﬁne-tuning. In the ﬁrst part of this work, we thus establish that this setup results in similar gains for adapters, as has been shown for full model ﬁne-tuning (Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020). Focusing on a low-resource target task setup, we ﬁnd that only a subset of intermediate adapters yield positive gains, while others hurt the performance considerably (see Table 1 and Figure 2). Our results demonstrate that it is necessary to obtain methods that efﬁciently identify beneﬁcial intermediately trained adapters.
In the second part, we leverage the transfer results from part one to automatically rank and identify beneﬁcial intermediate tasks. With the rise of large publicly accessible repositories for NLP
2Adapters are new weights at every layer of a pre-trained transformer model. To ﬁne-tune a model on a downstream task, all pre-trained transformer weights are frozen and only the newly introduced adapter weights are trained.

models (Wolf et al., 2020; Pfeiffer et al., 2020a), the chances of ﬁnding pre-trained models that yield positive transfer gains are high. However, it is infeasible to brute-force the identiﬁcation of the best intermediate task. Existing approaches have focused on beneﬁcial task selection for multi-task learning (Bingel and Søgaard, 2017), full ﬁne-tuning of intermediate and target transformer-based LMs for NLP tasks (Vu et al., 2020), adapter-based models for vision tasks (Puigcerver et al., 2021) and unsupervised approaches for zero-shot transfer for community question answering (Rücklé et al., 2020). Each of these works require different types of data, such as intermediate task data and/or intermediate model weights, which, depending on the scenario, are potentially not accessible.3
In this work we thus aim to address the efﬁciency aspect of transfer learning in NLP from multiple different angles, resulting in the following contributions: 1) We focus on adapter-based transfer learning which is considerably more parameter (Houlsby et al., 2019) and computationally efﬁcient than full model ﬁne-tuning (Rücklé et al., 2021), while achieving on-par performance; 2) We evaluate sequential ﬁne-tuning of adapter-based approaches on a diverse set of 42 intermediate and 11 target tasks (i.e. classiﬁcation, multiple choice, question answering, and sequence tagging); 3) We identify the best intermediate task for transfer learning, without the necessity of computational expensive, explicit training on all potential candidates. We compare different selection techniques, consolidating previously proposed and new methods; 4) We provide a thorough analysis of the different techniques, available data scenarios, and task-, and model types, thus presenting deeper insights into the best approach for each respective setting; 5) We provide computational cost estimates, enabling informed decision making for trade-offs between expense and downstream task performance.
2 Related Work
2.1 Transfer between tasks
Phang et al. (2018) show that training on intermediate tasks results in performance gains for many target tasks. Subsequent work further explores the effects on more diverse sets of tasks (Wang et al.,
3Bingel and Søgaard (2017) and Vu et al. (2020) require access to both intermediate task data and models, Puigcerver et al. (2021) require access to only the intermediate model, and Rücklé et al. (2020) only to the intermediate task data.

2019a; Talmor and Berant, 2019; Liu et al., 2019a; Sap et al., 2019; Pruksachatkun et al., 2020; Vu et al., 2020). Wang et al. (2019a), Yogatama et al. (2019), and Pruksachatkun et al. (2020) emphasizes the risks of catastrophic forgetting and negative transfer results, ﬁnding that the success of sequential transfer varies largely when considering different intermediate tasks.
While previous work has shown that intermediate task training improves the performance on the target task in full ﬁne-tuning setups, we establish that the same holds true for adapter-based training.
2.2 Predicting Beneﬁcial Transfer Sources
Automatically selecting intermediate tasks that yield transfer gains is critical when considering the increasing availability of tasks and models.
Proxy estimators have been proposed to evaluate the transferability of pre-trained models towards a target task. Nguyen et al. (2020), Li et al. (2021) and Deshpande et al. (2021) estimate the transferability between classiﬁcation tasks by building an empirical classiﬁer from the source and target task label distribution. Puigcerver et al. (2021) experiment with multiple model selection methods, including kNN proxy models to estimate the target task performance. In a similar direction, Renggli et al. (2020) study proxy models based on kNN and linear classiﬁers, ﬁnding that a hybrid approach combination of task-aware and task-agnostic strategies yields the best results.
Bingel and Søgaard (2017) ﬁnd that gradients of the learning curves correlate with multi-task learning success. Zamir et al. (2018) build a taxonomy of vision tasks, giving insights into non-trivial transfer relations between tasks. Multiple works propose using embeddings that capture statistics, features, or the domain of a dataset. Edwards and Storkey (2017) leverage variational autoencoders (Kingma and Welling, 2014) to encode all samples of a dataset. Jomaa et al. (2019) train a dataset meta-feature extractor that can successfully capture the domain of a dataset. Vu et al. (2020) encode each training example of a dataset by averaging over BERT’s representations of the last layer. Rücklé et al. (2020) capture domain similarity by embedding dataset examples using a sentence embedding model. Achille et al. (2019) and Vu et al. (2020) compute task embeddings based on the Fisher Information Matrix of a probe network.
While many different methods have been pro-

posed, there lacks a direct comparison among them. Additionally, previous work has only focus on BERT, which we ﬁnd to behave considerably different to other model types such as RoBERTa for some methods. In this work we aim to consolidate all methods and experiment with newer model types to provide a more thorough perspective.
3 Adapter-Based Sequential Transfer
We present a large-scale study on adapter-based sequential ﬁne-tuning, ﬁnding that around half of the task combinations yield no positive gains. This demonstrates the importance of ﬁnding approaches that efﬁciently identify suitable intermediate tasks.
3.1 Tasks
We select QA tasks from the MultiQA repository (Talmor and Berant, 2019) and sequence tagging tasks from Liu et al. (2019a). Most of our classiﬁcation tasks are available in the (Super)GLUE (Wang et al., 2018, 2019b) benchmarks. We experiment with multiple choice commonsense reasoning tasks to cover a broader range of different types, and domains. In total, we experiment with 53 tasks, divided into 42 intermediate and 11 target tasks.4
3.2 Experimental Setup
We experiment with BERT-base (Devlin et al., 2019) and RoBERTa-base (Liu et al., 2019b), training adapters with the conﬁguration proposed by Pfeiffer et al. (2021a). We adopt the two-stage sequential ﬁne-tuning setup of Phang et al. (2018), splitting the tasks in two disjoint subsets S and T , denoted as intermediate and target tasks, respectively. For each pair (s, t) with s ∈ S and t ∈ T , we ﬁrst train a randomly initialized adapter on s (keeping the base model’s parameters ﬁxed). We then ﬁne-tune the trained adapter on t.5
For target task ﬁne-tuning, we simulate a lowresource setup by limiting the maximum number of training examples on t to 1000. This choice is motivated by the observation that smaller target tasks beneﬁt the most from sequential ﬁne-tuning while at the same time revealing the largest performance variances (Phang et al., 2018; Vu et al., 2020). Low-resource setups, thus, reﬂect the most beneﬁcial application setting for our transfer learn-
4The choice for our intermediate and target task split was motivated by previous work (Sap et al., 2019; Vu et al., 2020, inter alia). For more details see Appendix A.
5For more details please refer to Appendix B.

Performance with BERT

90

80

70

60

50

RTE

COPA

40

R. Tomatoes CQ

CoNLL 2003

30

CS QA

STS-B

20

DROP Quail

BoolQ

10

DepRel-EWT

20 30 40 50 60 70 80 90 Performance with RoBERTa

Figure 1: Comparison of transfer performance between BERT and RoBERTa for the respective target tasks, pretrained on the 42 intermediate tasks.

ing strategy and also allow us to more thoroughly study different transfer relations.

3.3 Results
Figure 2 shows the relative transfer gains and Table 1 lists the absolute scores of all intermediate and target task combinations for RoBERTa.6 We observe large variations in transfer gains (and losses) across the different combinations. Even though larger variances may be explained by a higher task difﬁculty (see ‘No Transfer’ in Table 1), they also illustrate the heterogeneity and potential of sequential ﬁne-tuning in our adapter-based setting. At the same time, we ﬁnd several cases of transfer losses— with up to 60% lower performances (see Figure 2)— potentially occurring due to catastrophic forgetting.
Overall, for RoBERTa, 243 (53%) transfer combinations yield positive transfer gains whereas 203 (44%) yield losses. The mean of all transfer gains is 2.3%. However, from our eleven target tasks only ﬁve beneﬁt on average (see ‘Avg. Transfer’ in Table 1). This illustrates the high risk of choosing the wrong intermediate tasks. Avoiding such hurtful combinations and efﬁciently identifying the best ones is necessary; evaluating all combinations is inefﬁcient and often not feasible.
We further ﬁnd that the best performing intermediate tasks for BERT and RoBERTa overlap considerably as illustrated in Figure 1, with transfer performances correlating with a Spearman correlation of 0.94 when averaged over all settings, and 0.68 when averaged per target task.
6We list the corresponding transfer results for BERT in Table 10 of the Appendix.

Relative transfer gain (%)

100

75

50

25

0

25

50

75

100

BoolQ

COPA

CQ

CS QA

CoNLL 2003

DROP

DepRel-EWT

Quail

R. Tomatoes

RTE

STS-B

Target task

Figure 2: Transfer gains/losses between all intermediate and target tasks with RoBERTa as base model. Each violin represents one target task where each dot represents the relative transfer gain (y-axis) from one intermediate task.

Task

BoolQ COPA CQ

No Transfer
Avg. Transfer
ANLI ART CoLA CoNLL’00 Cosmos QA DuoRC-p DuoRC-s EmoContext Emotion GED-FCE Hellaswag HotpotQA IMDb MIT Movie MNLI MRPC MultiRC NewsQA POS-Co.’03 POS-EWT QNLI QQP QuaRTz Quoref RACE ReCoRD SICK SNLI SQuAD SQuAD 2.0 SST-2 ST-PMB SWAG SciCite SciTail Social IQA TREC WNUT17 WiC WikiHop WinoGrande Yelp Polarity

62.17
66.93
76.75 70.87 63.01 62.17 74.28 62.17 62.17 68.64 65.54 62.17 69.44 62.17 68.09 62.17 75.86 64.19 74.05 62.17 62.17 62.17 73.61 63.03 69.44 62.17 76.72 62.17 72.53 69.61 62.17 68.34 67.14 62.17 69.30 65.98 72.13 73.36 67.41 62.17 62.17 62.18 69.93 66.97

56.68
66.49
73.64 79.00 63.60 57.40 78.48 64.76 67.68 64.12 60.36 55.68 77.24 59.12 62.76 53.12 76.20 68.56 71.88 68.24 51.08 53.96 72.00 72.16 68.60 60.72 72.04 63.28 73.48 72.36 68.36 73.24 65.84 52.16 74.64 64.44 72.00 79.92 59.92 53.08 72.88 55.64 73.04 65.80

28.24
30.60
32.67 27.12 31.27 30.32 29.83 39.11 42.45 23.52 22.58 30.05 30.67 42.58 28.04 31.32 31.52 29.06 30.59 43.52 20.09 31.21 36.01 24.27 27.30 40.34 23.91 28.25 29.66 19.08 39.71 44.40 28.43 15.53 28.96 28.75 29.91 30.88 31.47 28.13 29.55 41.35 29.58 22.41

CS QA CoNLL DROP 2003

DepRel- Quail EWT

49.12
47.88
51.35 53.02 51.11 44.95 53.81 49.63 51.29 46.96 45.54 46.26 56.05 39.72 46.18 44.01 48.80 48.44 51.35 49.09 23.11 46.22 51.94 48.85 50.48 46.83 53.15 46.50 53.89 42.21 51.09 50.43 47.81 26.36 55.00 47.71 53.89 56.41 48.04 45.36 52.09 39.02 54.58 42.42

85.74
83.80
85.82 84.48 85.53 87.30 84.17 85.42 85.66 86.37 84.41 86.19 85.87 73.59 85.90 87.43 85.59 86.18 81.86 81.41 87.14 87.76 85.96 81.77 86.01 85.86 81.05 84.21 85.45 32.29 86.48 86.14 85.66 87.47 85.61 86.07 84.74 86.00 86.09 87.96 85.73 84.40 86.16 80.42

43.42
43.96
44.66 41.01 43.34 44.33 43.66 51.48 52.29 42.34 42.22 42.52 42.86 53.68 43.30 44.59 43.95 43.43 43.89 53.16 40.74 44.31 46.47 41.79 42.69 52.95 42.46 41.85 44.11 14.66 57.40 59.78 42.73 28.00 43.30 42.18 43.74 43.34 42.86 44.03 42.74 46.45 43.54 37.40

79.96
74.59
76.77 74.39 79.24 81.58 74.37 76.33 75.46 77.69 77.03 79.61 75.34 54.60 77.55 78.58 71.41 77.05 73.38 65.57 81.84 82.25 76.91 69.14 78.56 76.98 65.74 71.87 79.23 28.62 76.33 76.99 77.28 81.94 76.97 78.95 77.72 78.38 78.02 77.51 79.08 70.47 76.75 69.25

61.94
60.50
66.34 64.00 63.46 54.17 65.56 60.20 61.94 62.01 59.99 59.92 62.06 55.36 61.60 58.42 61.27 62.95 64.70 60.44 41.31 55.71 64.67 61.57 62.47 62.75 67.89 63.63 63.63 52.53 61.99 63.79 60.82 38.31 63.71 62.40 63.77 65.91 62.38 55.59 63.16 56.87 63.93 57.74

R. Tomatoes
88.35
86.95
87.64 87.73 88.14 87.19 88.12 86.98 86.92 88.03 88.01 87.64 87.64 83.30 88.97 87.56 88.31 87.35 87.17 87.22 71.24 86.81 87.64 87.58 88.16 87.04 87.19 86.96 87.94 82.78 86.21 87.90 92.03 78.71 88.35 87.77 87.82 88.27 88.14 87.54 87.92 86.59 87.94 89.31

RTE
65.56
70.38
84.40 73.43 66.93 61.52 79.21 71.55 72.78 71.48 63.97 60.51 76.82 62.74 68.45 67.94 83.47 72.71 77.98 72.06 53.14 56.17 70.76 72.71 68.30 71.48 83.32 71.41 75.02 76.39 72.27 75.60 69.82 53.43 74.44 68.59 73.94 78.34 70.32 61.44 68.23 62.74 75.16 64.98

STS-B
87.35
86.31
88.24 88.45 87.45 88.11 88.00 88.54 88.51 86.91 87.59 86.49 88.67 86.26 86.38 87.05 89.25 88.97 88.06 87.66 78.53 87.55 89.07 89.51 88.27 88.19 88.13 86.98 88.26 85.90 87.82 89.06 86.75 39.12 89.32 86.63 89.93 88.57 86.94 86.00 88.00 85.56 87.88 82.45

Table 1: Target task performances for transferring between intermediate tasks (rows) and target tasks (columns) with RoBERTa as base model. The ﬁrst row ‘No Transfer’ shows the baseline performance when training only on the target task without transfer. All scores are mean values over ﬁve random restarts.

4 Methods for the Efﬁcient Selection of Intermediate Tasks
We now present different model selection methods, and later in §5, study their effectiveness in our setting outlined above. We group the different methods based on the assumptions they make with regard to the availability of intermediate task data DS and intermediate models MS. Access to both can be expensive when considering large pretrained model repositories with hundreds of tasks.
4.1 Metadata: Educated Guess
A setting in which there exist neither access to the intermediate task data DS nor models trained on the data MS, can be regarded as an educated guess scenario. The selection criterion can only rely on metadata available for an intermediate task dataset.
Dataset Size. Under the assumption that more data implies better transfer performance, the selection criterion denoted as Size ranks all intermediate tasks in descending order by the training data size.
Task Type. Under the assumption that similar objective functions transfer well, we pre-select the subset of tasks of the same type. This approach may be combined with a random selection of the remaining tasks, or with ranking them by size.
4.2 Intermediate Task Data
With an abundance of available datasets,7 and the continuous development of new LMs, ﬁne-tuned versions for every task-model combination are not (immediately) available. The following methods, thus, leverage the intermediate task data DS without requiring the respective ﬁne-tuned models MS.
Text Embeddings (TextEmb). Vu et al. (2020) pass each example through a LM and average over the output representations of the ﬁnal layer (across all examples and all input tokens). Assuming that similar embeddings imply positive transfer gains, they rank the intermediate tasks according to their embeddings’ cosine similarity to the target task.
SBERT Embeddings (SEmb). Sentence embedding models such as Sentence-BERT (SBERT; Reimers and Gurevych, 2019) may be better suited to represent the dataset examples. Similar to TextEmb, we rank the intermediate tasks according to their embedding cosine similarity.
7e.g. via https://huggingface.co/datasets.

4.3 Intermediate Model

Scenarios in which we only have access to the trained intermediate models (MS) occur when the training data is proprietary or if implementing all dataset is too tedious. With the availability of model repositories (Wolf et al., 2020; Pfeiffer et al., 2020a) such approaches can be implemented without requiring additional data during model upload (i.e. in contrast to TaskEmbs, where the training dataset information needs to be made available). The following describes methods only requiring access to the intermediate models MS.

Few-Shot Fine-Tuning (FSFT). Fine-tuning of all available intermediate task models on the entire target task is infeasible. As an alternative, we can train models for a few steps on the target task to approximate the ﬁnal performance. After N steps on the target task, we rank the intermediate models based on their respective transfer performance.

Proxy Models. Following Puigcerver et al. (2021),

we leverage simple proxy models to obtain a perfor-

mance estimation of each trained model MS on on

the target dataset DT . Speciﬁcally, we experiment

with k-Nearest Neighbors (kNN), with k = 1 and

Euclidian distance, and logistic/ linear regression

(linear) as proxy models. For both, we ﬁrst com-

pute hM xi , the token-wise averaged output represen-

tations of MS, for each training input xi ∈ DT .

Using these, we deﬁne DTM

=

{(

h

M xi

,

yi

)}

N i=1

as

the target dataset embedded by MS. In the next

step, we apply the proxy model on DTM and obtain

its performance using cross-validation. By repeat-

ing this process for each intermediate task model,

we obtain a list of performance scores which we

leverage to rank the intermediate tasks.

4.4 Intermediate Model and Task Data
Access to both intermediate dataset DS and intermediates model MS provides a wholesome depiction of the intermediate task, as all previously mentioned methods are applicable in this scenario. Further methods which require access to both are:
Task Embeddings (TaskEmb). Achille et al. (2019) and Vu et al. (2020) obtain task embeddings via the Fisher Information Matrix (FIM). The FIM captures how sensitive the loss function is towards small perturbations in the weights of the model and thus gives an indication on the importance of certain weights towards solving a task.
Given the model weights θ and the joint distribu-

tion of task features and labels Pθ(X, Y ), we can deﬁne the FIM as the expected covariance of the gradients of the log-likelihood w.r.t. θ:
Fθ = Ex,y∼Pθ(X,Y ) [∇θ log Pθ(x, y) · ∇θ log Pθ(x, y) ]
We follow the implementation details given in Vu et al. (2020). For a dataset D and a model M ﬁnetuned on D, we compute the empirical FIM based on D’s examples. The task embeddings are the diagonal entries of the FIM.
Few-Shot Task Embeddings (FS-TaskEmb). We also leverage task embeddings in our few-shot scenario outlined above (see FSFT), where we ﬁnetune intermediate models for a few steps on the target dataset. With very few training instances, the accuracy scores of FSFT (alone) may not be reliable indicators of the ﬁnal transfer performances. As an alternative, we compute the TaskEmb similarity of each intermediate model before and after training N steps on the target task. We then rank all intermediate models in decreasing order of this similarity.
5 Experimental Setup
We evaluate the approaches of §4, each having the objective to rank the intermediate adapters s ∈ |S| with respect to their performance on t ∈ T when applied in a sequential adapter training setup. We leverage the transfer performance results of our 462 experiments obtained in §3 for our ranking task.
5.1 Hyperparameters
If not otherwise mentioned, we follow the experimental setup as described in §3. We describe method speciﬁc hyperparameters in the following.
SEmb. We use a Sentence-(Ro)BERT(a)-base models, ﬁne-tuned on NLI and STS tasks, in concordance with the respective target model type.
FSFT. We ﬁne-tune each intermediate adapter on the target task for one full epoch and rank them based on their target task performances.8
Proxy Models. For both kNN and linear, we obtain performance scores with 5-fold cross-validation on each target task. The architectures slightly vary across task types. For classiﬁcation, regression, and multiple-choice target tasks, proxy models predict the label or answer choice. For sequence tagging
8As this represents a rather optimistic estimate of the fewshot transfer performance, in Appendix D we also investigate settings in which we train for only 5, 10, or 25 update steps.

tasks, each token in a sequence represents a training instance of DTM , with the tag being the class label. Since this would increase the total number of training examples, we randomly select 1000 embedded examples from DT , to maintain equal sizes of DTM across all target tasks. We do not study proxy models on extractive QA tasks as they cannot directly be transformed into classiﬁcation tasks.
TaskEmb. We perform standard ﬁne-tuning of randomly initialized adapter modules within the pretrained LM to obtain task embeddings.
FS-TaskEmb. We follow the setup of FSFT by training for one epoch (50 update steps).

5.2 Metrics
We compute the NDCG (Järvelin and Kekäläinen, 2002), a widely used information retrieval metric that evaluates a ranking with attached relevances (which correspond to our transfer results of §3).
Furthermore, we calculate Regret@k (Renggli et al., 2020), which measures the relative performance difference between the top k selected intermediate tasks and the optimal intermediate task:

O(S ,t)

Mk (S ,t)

max E[T (s, t)] − max E[T (sˆ, t)]

Regretk = s∈S

sˆ∈Sk
O(S, t)

where T (s, t) is the performance on target task t when transferring from intermediate task s. O(S, t) denotes the expected target task performance of an optimal selection. Mk(S, t) is the highest performance on t among the k top-ranked intermediate tasks of the tested selection method. We take the difference between both measures and normalize it by the optimal target task performance to obtain our ﬁnal relative notion of regret.9

6 Experimental Results
Table 2 shows the results when selecting among all available intermediate tasks for BERT and RoBERTa.10 As expected the Random and Size baselines do not yield good rankings when selecting among all intermediate tasks.
Access to only DS or MS. These methods typically perform better than our baselines.
9We provide more details about our selection of metrics in Appendix C.
10Table 5 in the appendix shows results when preferring tasks of the same type for BERT and RoBERTa.

DS MS DS, MS

Random Size
SEmb TextEmb
FSFT kNN linear
FS-TaskEmb TaskEmb

Classiﬁcation
NDCG R@1 R@3
0.49 11.09 5.55 0.53 10.80 6.26
0.82 0.27 0.27 0.72 2.54 1.20
0.89 0.28 0.00 0.83 2.49 0.12 0.79 2.51 1.00
0.87 0.19 0.19 0.71 14.04 3.08

M. Choice
NDCG R@1 R@3
0.43 13.80 6.30 0.39 14.89 11.10
0.79 4.47 0.00 0.74 2.94 0.00
0.89 0.00 0.00 0.76 1.91 1.57 0.89 0.00 0.00
0.73 3.03 0.83 0.67 6.70 1.92

NDCG
0.33 0.36
0.49 0.48
0.28 -
0.28 0.24

QA
R@1
26.95 33.18
17.04 17.04
21.21 -
12.90 30.02

R@3
18.98 33.18
7.59 15.34
18.20 -
10.38 22.40

Tagging
NDCG R@1
0.60 7.65 0.48 8.44
0.80 0.47 0.88 0.47
0.97 0.00 0.88 1.44 0.92 0.28
0.88 0.19 0.78 31.84

R@3
2.82 8.44
0.47 0.11
0.00 0.11 0.28
0.19 0.19

NDCG
0.47 0.45
0.75 0.71
0.79 -
0.73 0.63

All
R@1
14.09 15.55
4.50 4.91
3.96 -
3.28 18.18

R@3
7.70 12.87
1.56 3.25
3.31 -
2.22 5.75

DS MS DS, MS

Random Size
SEmb TextEmb
FSFT kNN linear
FS-TaskEmb TaskEmb

Classiﬁcation
NDCG R@1 R@3
0.45 8.61 6.04 0.51 11.34 5.87
0.78 0.75 0.53 0.81 1.26 0.75
0.93 0.33 0.33 0.90 1.10 0.00 0.82 3.66 1.86
0.92 0.62 0.00 0.83 3.89 2.02

(a) RoBERTa

M. Choice
NDCG R@1 R@3
0.50 8.40 5.03 0.50 11.85 7.51
0.59 7.93 1.25 0.60 6.77 1.46
0.72 4.16 1.64 0.68 2.82 1.85 0.76 1.35 0.86
0.72 5.38 0.93 0.72 4.19 1.17

NDCG
0.44 0.42
0.88 0.86
0.39 -
0.66 0.67

QA
R@1
29.35 33.80
2.98 2.98
17.07 -
11.17 3.61

R@3
18.65 33.80
0.00 2.42
17.07 -
2.07 3.61

Tagging
NDCG R@1
0.56 7.26 0.48 9.37
0.79 0.56 0.79 0.56
0.89 0.65 0.94 0.00 0.96 0.00
0.82 1.37 0.73 1.36

R@3
2.95 9.37
0.56 0.51
0.50 0.00 0.00
0.50 0.50

NDCG
0.48 0.48
0.75 0.76
0.77 -
0.80 0.75

All
R@1
12.08 15.20
3.08 2.95
4.48 -
3.97 3.46

R@3
7.50 12.03
0.63 1.20
3.76 -
0.72 1.80

(b) BERT
Table 2: Evaluation of intermediate task rankings produced by different methods for RoBERTa (a) and BERT (b). The table shows the mean NDCG and Regret scores by target task type. The best score in each group is highlighted in bold, best overall score is underlined. For NDCG, higher is better; for Regret, lower is better.

TextEmb and SEmb perform on par in most cases.11 While FSFT outperforms the other approaches in most cases, it comes at the high cost of requiring downloading and ﬁne-tuning all intermediate models for a few steps. This can be prohibitive if we consider many intermediate tasks. If we have access to TextEmb or SEmb information of the intermediate task (i.e., individual vectors distributed as part of a model repository), these techniques yield similar performances at a much lower cost.
Access to both DS and MS. Assuming the availability of both intermediate models and intermediate data is the most prohibitive setting. Surprisingly, we ﬁnd BERT and RoBERTa to behave considerably differently, especially evident for QA tasks. As shown by Vu et al. (2020), TaskEmb performs very well for BERT, however we ﬁnd that the results of this gradient based approach do not translate to RoBERTa. While these approaches perform best or competitively for all task types using BERT, they considerably underperform all methods when leveraging pre-trained RoBERTa weights. Here, the two much simpler domain embedding methods outperform the TaskEmb method based on the FIM.
Summary. We ﬁnd that simple indicators such
11The used SBERT model is trained on NLI and STS-B tasks, which are included in our set of intermediate and target tasks, respectively. A direct comparison between TextEmb and SEmb for the respective classiﬁcation tasks is thus difﬁcult.

as domain similarity are suitable for selecting intermediate pre-training tasks for both BERT and RoBERTa based models. Our evaluated methods are able to efﬁciently select the best performing intermediate tasks with a Regret@3 of 0.0 in many cases. Our results, thus, show that the selection methods are able to effectively rank the top tasks with relative certainty, thus considerably reducing the number of necessary experiments.12
7 Analysis
Computational Costs. Table 3 estimates the computational costs of each transfer source selection method. Complexity shows the required data passes through the model.13 For the embedding-based approaches, we assume pre-computed embeddings for all intermediate tasks. For TaskEmb, we only train an adapter on the target task for e epochs.
In addition to the complexity, we calculate the required Multiply-Accumulate computations (MAC) for 42 intermediate tasks and one target task with 1000 training examples, each with an average sequence length of 128.14 Following our experi-
12We also ﬁnd that combining domain and task type match indicators often yield the best overall results, outperforming computationally more expensive methods. See Appendix ?? for more experiments with task type pre-selection.
13We neglect computations related to embedding similarities and proxy models as they are cheap compared to model forward/ backward passes.
14We recorded MAC with the pytorch-OpCounter package.

Method
Metadata TextEmb/ SEmb TaskEmb kNN/ linear FSFT/ FS-TaskEmb

Complexity
1 f (e + 1)f + eb nf 2nef + neb

MACs
0 1.10E+13 3.30E+14 4.61E+14 1.38E+15

Table 3: Computational cost of transfer source selection. f denotes a forward pass through all target task examples once, b is the corresponding backward pass, n is the number of source models, and e is the number of full training epochs (for FS approaches e ≤ 1).

SEmb-BERTD SEmb-BERTB SEmb-BERTL SEmb-RoBERTaD SEmb-RoBERTaB SEmb-RoBERTaL

NDCG
0.72 0.72 0.70 0.77 0.75 0.74

R@1
5.50 4.99 6.30 4.60 4.50 3.96

R@3
2.07 1.16 2.12 0.82 1.56 0.47

R@5
1.69 0.07 1.01 0.44 0.48 0.07

Table 4: Intermediate SEmb rankings for RoBERTa tasks produced by different model-type variants. D, B, and L stand for Distill, Base, and Large, respectively. The table shows the mean NDCG and Regret scores. For NDCG, higher is better; for Regret, lower is better.

mental setup in §5, we set e = 15 for TaskEmb and e = 1 for FSFT/ FS-TaskEmb. We ﬁnd that embedding-based methods require two orders of magnitude fewer computations compared to ﬁnetuning approaches. The difference may be even larger when we consider more intermediate tasks. Since ﬁne-tuning approaches do not yield gains that would warrant the high computational expense (see §6), we conclude that SEmb has the most favorable trade-off between efﬁciency and effectiveness.
SEmb Model Dependency. We compare different pre-trained sentence-embedding model variants to identify the extent to which SEmb is invariant to such changes. We experiment with BERT and RoBERTa variants of sizes Distill, Base, and Large, and present results for RoBERTa tasks in Table 4.15 We ﬁnd that all variants perform comparably, demonstrating that SEmb is a computationally efﬁcient, model-type invariant method for selecting beneﬁcial intermediate tasks.
BERT vs RoBERTa TaskEmb Space. To better understand the TaskEmb performance differences between BERT and RoBERTa models, we visualize the respective embedding spaces using T-SNE in Figure 3. We ﬁnd that BERT embeddings are clustered much more closely in the vector space than
15The full results can be found in Table 7 of the appendix.

model = BERT 400

200

0

200

400

model = RoBERTa 400

200

0

200

400

400

200

0

200

400

Figure 3: Clustering of BERT and RoBERTa TaskEmbs, respectively, using T-SNE. Colors indicate task types. We compared different random seeds, all of which resulted in similar visualizations.

Relative transfer gain (%)

30

Same Task Type False

20

True

10

0

Classification

M. Choice

QA

Target task

Tagging

Figure 4: Relative transfer gains for transfer within and across types, split by target task type. Results shown for RoBERTa.

RoBERTa embeddings. While TaskEmbs of BERT also seem to be located in the proximity of related tasks, TaskEmbs of RoBERTa are distributed further apart. This can result in worse performance due to the curse of dimensionality.
Overall, our results and analysis suggest that TaskEmb, unlike SentEmb, considerably depend on the chosen base model.
Within- and Across-Type Transfer. Our experimental setup includes tasks of four different types, i.e. Transformer prediction head structures: sequence classiﬁcation/ regression, multiple choice, extractive question answering and sequence tagging. Figure 4 compares the relative transfer gains within and across these task types for RoBERTa. We see that within-type transfer is consistently stronger across all target tasks. We ﬁnd the largest differences between within-type and across-type

transfer for the extractive QA target tasks. These observations may be partly explained by the homogeneity of the included QA intermediate tasks; They overwhelmingly focus on general reading comprehension across multiple domains with paragraphs from Wikipedia or the web as contexts. Tasks of other types more distinctly focus on individual domains and scenarios.
Overall, we ﬁnd a negative across-type transfer gain (i.e., loss) for 8 out of 11 tested target tasks (on average). This suggests that task type match between intermediate and target task is a strong indicator for transfer success. Thus, in the next section, we evaluate variants of all methods presented in §4 that prefer intermediate tasks of the same type as the target task.
Pre-Ranking by Task Types. We implement a simple mechanism to ensure that tasks with the same type as the target task are always ranked before tasks of other types during intermediate task selection. Given a task selection method, we ﬁrst rank all tasks of the same type at the top before ranking tasks of all other types below. Results for applying this mechanism to all presented task selection methods are given for BERT and RoBERTa in Table 5 of the Appendix.
We ﬁnd that even though the random and Size baselines do not yield good rankings when selecting among all intermediate tasks (cf. Table 2), the scores considerably improve when preferring tasks of the same type. In general, we see almost consistent improvements across all task selection methods for both BERT and RoBERTa when implementing pre-ranking by task types. Considering all target tasks and all methods, preferring intermediate tasks of the same type yields improved NDCG scores in 77 of 99 cases.
Further Analysis. We further ﬁnd that embedding based approaches are sample efﬁcient, while FSFT appproaches are not (§D). We also report results for combining ranking approaches with Rank Fusion, which does not yield consistent improvements over the individual approaches presented before (§E).
8 Conclusion
In this work we have established that intermediate pre-training can yield gains in adapter-based setups, however, around 44% of all transfer combinations result in decreased performances. We have consolidated several existing and new methods for efﬁciently identifying beneﬁcial intermediate tasks.

Experimenting with different model types, we ﬁnd that the previously proposed best performing approaches for BERT do not translate to RoBERTa.
Overall, efﬁcient embedding based methods, such as those relying on pre-computable sentence representations, perform better or often on-par with more expensive approaches. The best methods achieve a Regret@3 of less than 1% on average, demonstrating that they are effective at efﬁciently identifying the best intermediate tasks. The approaches evaluated and proposed in this work, thus, enable the automatic identiﬁcation of beneﬁcial intermediate tasks, deeming exhaustive experimentation on many task-combinations unnecessary. When applied on a broad scale, these methods can contribute to more sustainable (Strubell et al., 2019; Moosavi et al., 2020) and more inclusive (Joshi et al., 2020) natural language processing.
Acknowledgements
Clifton and Jonas are supported by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. Andreas was supported by the German Research Foundation (DFG) as part of the UKPSQuARE project (grant GU 798/29-1).
We thank Leonardo Ribeiro and the anonymous reviewers for insightful feedback and suggestions on a draft of this paper.
References
Lasha Abzianidze and Johan Bos. 2017. Towards universal semantic tagging. In IWCS 2017 - 12th International Conference on Computational Semantics Short Papers, Montpellier, France, September 19 22, 2017. The Association for Computer Linguistics.
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. 2019. Task2Vec: Task embedding for meta-learning. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6429–6438. IEEE.
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš, Ivan Vulic´, and Anna Korhonen. 2021. MAD-G: Multilingual Adapter Generation for Efﬁcient Cross-Lingual Transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, Online.
Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-based question answering with knowledge graph. In COLING 2016, 26th International Conference on Computational

Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan, pages 2503–2514. Association for Computational Linguistics.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
Joachim Bingel and Anders Søgaard. 2017. Identifying beneﬁcial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, pages 164–169. Association for Computational Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632–642. The Association for Computational Linguistics.
Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017. SemEval2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pages 1–14. Association for Computational Linguistics.
Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. SemEval-2019 task 3: EmoContext contextual emotion detection in text. In Proceedings of the 13th International Workshop on Semantic Evaluation, SemEval@NAACLHLT 2019, Minneapolis, MN, USA, June 6-7, 2019, pages 39–48. Association for Computational Linguistics.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difﬁculty of natural Yes/No questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2924– 2936. Association for Computational Linguistics.
Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for citation intent classiﬁcation in scientiﬁc publications. In Proceedings of the 2019 Conference of the North

American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 27, 2019, Volume 1 (Long and Short Papers), pages 3586–3596. Association for Computational Linguistics.
Gordon V. Cormack, Charles L. A. Clarke, and Stefan Büttcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009, pages 758–759. ACM.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classiﬁcation and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 1113, 2005, Revised Selected Papers, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.
Pradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 5924–5931. Association for Computational Linguistics.
Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-Generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pages 140– 147. Association for Computational Linguistics.
Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless C. Fowlkes, Rahul Bhotika, Stefano Soatto, and Pietro Perona. 2021. A linearized framework and a new benchmark for model selection for ﬁne-tuning. arXiv preprint.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Association for Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases.

In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368– 2378. Association for Computational Linguistics.
Harrison Edwards and Amos J. Storkey. 2017. Towards a neural statistician. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
Andrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Proceedings of the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012, Montréal, Canada, June 7-8, 2012, pages 394–398. The Association for Computer Linguistics.
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8342–8360.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR.
Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2391–2401. Association for Computational Linguistics.
Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First Quora Dataset Release: Question Pairs.
Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM

Transactions on Information Systems, 20(4):422– 446.
Hadi S. Jomaa, Josif Grabocka, and Lars SchmidtThieme. 2019. Dataset2Vec: Learning dataset metafeatures. arXiv preprint.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282–6293, Online. Association for Computational Linguistics.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 252–262. Association for Computational Linguistics.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL: A textual entailment dataset from science question answering. In Proceedings of the ThirtySecond AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th Innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5189–5197. AAAI Press.
Diederik P. Kingma and Max Welling. 2014. Autoencoding variational bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 911, 2017, pages 785–794. Association for Computational Linguistics.
Xin Li and Dan Roth. 2002. Learning question classiﬁers. In 19th International Conference on Computational Linguistics, COLING 2002, Howard International House and Academia Sinica, Taipei, Taiwan, August 24 - September 1, 2002.
Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, and Boqing Gong. 2021. Ranking neural checkpoints. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 2663– 2673. Computer Vision Foundation / IEEE.

Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019a. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 1073–1094. Association for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142–150. The Association for Computer Linguistics.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-31, 2014, pages 216–223. European Language Resources Association (ELRA).
Naﬁse Sadat Moosavi, Angela Fan, Vered Shwartz, Goran Glavaš, Shaﬁq Joty, Alex Wang, and Thomas Wolf, editors. 2020. Proceedings of SustaiNLP: Workshop on Simple and Efﬁcient Natural Language Processing. Association for Computational Linguistics, Online.
Cuong Nguyen, Tal Hassner, Matthias W. Seeger, and Cédric Archambeau. 2020. LEEP: A new measure to evaluate transferability of learned representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7294–7305. PMLR.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4885–4901. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA, pages 115– 124. The Association for Computer Linguistics.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021a. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Online. Association for Computational Linguistics.
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 46–54. Association for Computational Linguistics.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Sebastian Ruder. 2020b. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Sebastian Ruder. 2021b. UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Online, November , 2021.
Jason Phang, Thibault Févry, and Samuel R. Bowman. 2018. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint.
Mohammad Taher Pilehvar and José CamachoCollados. 2019. WiC: The word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 1267–1273. Association for Computational Linguistics.
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5231–5247. Association for Computational Linguistics.
Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Cédric Renggli, André Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. 2021. Scalable transfer learning with expert models. In 9th International Conference on Learning Representations,

ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 784– 789. Association for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2383–2392. The Association for Computational Linguistics.
Marek Rei and Helen Yannakoudakis. 2016. Compositional sequence labeling models for error detection in learner writing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.
Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3980–3990. Association for Computational Linguistics.
Cédric Renggli, André Susano Pinto, Luka Rimanic, Joan Puigcerver, Carlos Riquelme, Ce Zhang, and Mario Lucic. 2020. Which model to transfer? Finding the needle in the growing haystack. arXiv preprint.
Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. 2020. Getting closer to AI complete question answering: A set of prerequisite real tasks. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, the ThirtySecond Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, the Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8722–8731. AAAI Press.
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. 2021. AdapterDrop: On the Efﬁciency of Adapters in Transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Online, November , 2021.
Andreas Rücklé, Jonas Pfeiffer, and Iryna Gurevych. 2020. MultiCQA: Zero-shot transfer of selfsupervised text matching models on a massive scale.

In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2471– 2486. Association for Computational Linguistics.
Phillip Rust, Jonas Pfeiffer, Ivan Vulic´, Sebastian Ruder, and Iryna Gurevych. 2021. How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, ACL 2021, Online, August 1-6, 2021. Association for Computational Linguistics.
Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 1683–1693. Association for Computational Linguistics.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. WinoGrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, the Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, the Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8732– 8740. AAAI Press.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task chunking. In Fourth Conference on Computational Natural Language Learning, CoNLL 2000, and the Second Learning Language in Logic Workshop, LLL 2000, Held in Cooperation with ICGI-2000, Lisbon, Portugal, September 13-14, 2000, pages 127–132. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in Cooperation with HLT-NAACL 2003, Edmonton, Canada, May 31 - June 1, 2003, pages 142–147. Association for Computational Linguistics.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4462– 4472. Association for Computational Linguistics.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-

textualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 3687–3697. Association for Computational Linguistics.
Natalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel R. Bowman, Miriam Connor, John Bauer, and Christopher D. Manning. 2014. A gold standard dependency corpus for english. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-31, 2014, pages 2897–2904. European Language Resources Association (ELRA).
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A Meeting of SIGDAT, a Special Interest Group of the ACL, pages 1631–1642. Association for Computational Linguistics.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy. Association for Computational Linguistics.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019. QuaRTz: An open-domain dataset of qualitative relationship questions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 5940–5945. Association for Computational Linguistics.
Alon Talmor and Jonathan Berant. 2019. MultiQA: An empirical investigation of generalization and transfer in reading comprehension. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4911–4921. Association for Computational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149–4158. Association for Computational Linguistics.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017, Vancouver, Canada, August 3, 2017, pages 191–200. Association for Computational Linguistics.
Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020. UDapter: Language Adaptation for Truly Universal Dependency Parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020).
Marko Vidoni, Ivan Vulic´, and Goran Glavaš. 2020. Orthogonal language and task adapters in zero-shot cross-lingual transfer. In arXiv preprint.
Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew MattarellaMicke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 7882–7926. Association for Computational Linguistics.
Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. 2019a. Can you tell me how to get past sesame street? Sentence-level pretraining beyond language modeling. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4465–4476. Association for Computational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pages 353–355. Association for Computational Linguistics.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments.

Transactions of the Association for Computational Linguistics 2019, 7:625–641.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics 2018, 6:287–302.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1112–1122. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 38–45. Association for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2369–2380. Association for Computational Linguistics.
Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomás Kociský, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelligence. arXiv preprint.
Amir Roshan Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. 2018. Taskonomy: Disentangling task transfer learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 3712–3722. IEEE Computer Society.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 93– 104. Association for Computational Linguistics.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really ﬁnish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational Linguistics.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 712, 2015, Montreal, Quebec, Canada, pages 649– 657.
A Tasks
Our experiments cover a diverse set of 53 different tasks, broadly divided into the four task types sequence classiﬁcation/ regression, multiple choice, extractive question answering and sequence tagging. Motivated by previous work, we ﬁrst select tasks that are either part of widely used benchmarks (Wang et al., 2018, 2019b; Talmor and Berant, 2019) or have been successfully applied to sequential transfer setups previously (Sap et al., 2019; Liu et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020). Additionally, we include other recent challenging tasks that fall under the four deﬁned task types (e.g. Bhagavatula et al. (2020); Rogers et al. (2020)) and tasks that extend the range of included dataset sizes and task domains. In general, we focus on tasks with publicly available datasets, e.g. via HuggingFace Datasets16. Our full set of tasks is split into 42 intermediate tasks, presented in Table 8, and 11 target tasks, presented in Table 9.
B Transfer training details
For all our experiments, we use the PyTorch implementations of BERT and RoBERTa in the HuggingFace Transformers library (Wolf et al., 2020) as the basis. The adapter implementation is provided by the AdapterHub framework (Pfeiffer et al., 2020a) and integrated into the Transformers library 17.
In the light of the number and variety of different tasks used, we don’t perform any extensive
16https://huggingface.co/datasets 17https://github.com/Adapter-Hub/ adapter-transformers

hyperparameter tuning on each training task. We mostly adhere to the hyperparameter recommendations of the Transformers library and Pfeiffer et al. (2021a) for adapter training. Speciﬁcally, we train all adapters for a maximum of 15 epochs, with early stopping after 3 epochs without improvements on the validation set. We use a learning rate of 10−4 and batch sizes between 4 and 32, depending on the size of the dataset. These settings apply to the adapter training on each intermediate task as well as the subsequent ﬁne-tuning on the target dataset. Additionally, since performances on the low-resource target tasks can be unstable, we perform multiple random restarts (ﬁve restarts for RoBERTa and three restarts for BERT) for all training runs on the target tasks, reporting the mean of all restarts. The ﬁnal scores on each task are computed on the respective tests set if publicly available, otherwise on the validation sets.
Results for RoBERTa are shown in Table 1 and results for BERT are shown in Table 10.
C Metrics for transfer source selection
C.1 NDCG
Following Vu et al. (2020), we compute the Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2002), a widely used information retrieval metric that evaluates a ranking with attached relevances. The NDCG is deﬁned via the Discounted Cumulative Gain (DCG), which represents a relevance score for a set of items, each discounted by its position in the ranking. The DCG of a ranking R, accumulated at a particular rank position p, can be computed as:
p 2reli − 1 DCGp(R) =
i=1 log2(i + 1)
In our setting, R refers to a ranking of intermediate tasks where the relevance reli of the intermediate task with rank i is set to the mean target performance when transferring the adapter trained on this intermediate task, i.e. reli ∈ [0, 100]. We always evaluate the full ranking of intermediate tasks, thus we set p = |S|.
The NDCG ﬁnally normalizes the DCG of the ranking predicted by the task selection method (Rpred) by the perfect ranking produced by the empirical transfer results (Rtrue). An NDCG of 100% indicates a perfect ranking.

NDCG

0.75 0.70 0.65 0.60 0.55 0.50 101 102 103
Dataset examples

TextEmb SEmb TaskEmb

Figure 5: Intermediate task selection performances for feature embedding methods with different data sizes on the target task. Results shown for RoBERTa and averaged over all targets.

NDCGp(R) = DCGp(Rpred) DCGp(Rtrue)
C.2 Choice of metrics
Our selection of evaluation metrics combines two measures that both evaluate the quality of the full ranking (NDCG) and the top selections of each methods (Regret). We prefer this combination of metrics over various other common possible evaluation metrics. We experimented with classical correlation measures such as Spearman rank correlation, ﬁnding they give poor indication on the overall quality of a selection method. The Spearman correlation is agnostic to the location within the ranking, thus penalizing mismatches at the bottom of the ranking with the same weight as mismatches at the top. In our setting, the top ranks are more important, making the NDCG which is biased towards correct rankings at the top a better ﬁt. Renggli et al. (2020) further discuss the limitations of correlation as an evaluation metric for task selection.
Vu et al. (2020) use the average predicted rank ρ of the source task with the best target performance as an additional metric. However, this metric does not account for the real target performance difference between the top ranked source tasks across different methods. In a simple example, assume two selection methods A and B assign the top performing source task smax to the same average rank. Further, A ranks a different source task on top which nearly performs on par with smax while B predicts a much weaker source task on top. In this case, we clearly would want to prefer method A over method B. Unlike ρ, our choice of regret as evaluation metric considers these differences.

NDCG

0.80

FS-TaskEmb

0.75

FSFT

0.70

0.65

0.60

5 10

N (tra2i5ning steps)

50

Figure 6: Intermediate task selection performances for ﬁne-tuning methods at different checkpoints. Results shown for RoBERTa and averaged over all targets.

D Sample Efﬁciency
Embedding-based approaches. Intermediate pretraining can have a larger impact on small target tasks. We therefore analyze and compare the effectiveness of embedding-based approaches with only 10, 100, and 1000 target examples.
Figure 5 plots the results for all feature embedding methods when applied to intermediate task selection for RoBERTa. We ﬁnd that the quality of the rankings can decrease substantially in the smallest setting with only 10 target examples. SEmb is a notable exception, achieving results close to that of the full 1000 examples (73% vs. 74.9% NDCG). With that, SEmb consistently performs above all other methods in all settings.
Few-Shot approaches. We experiment with N ∈ {5, 10, 25, 50} update steps for the ﬁne-tuning methods FSFT and FS-TaskEmb. Results for RoBERTa are shown in Figure 6. While unsurprisingly, the performance for both methods improves consistently with the number of ﬁne-tuning steps, FS-TaskEmb produces superior rankings at earlier checkpoints, however is outperformed by FSFT on the long run. The results indicate that updating for < 25 update steps does not provide sufﬁcient evidence to reliably predict the best intermediate tasks.
E Rank Fusion
Vu et al. (2020) use the Reciprocal Rank Fusion algorithm (Cormack et al., 2009) to aggregate the rankings of TextEmb and TaskEmb. further experiment with various combinations of ranks produced by methods of different categories, e.g. Size + SEmb. Table 6 shows the results for a selection of all possible method combinations when applied to intermediate task selection for RoBERTa.
In a few cases, fusing improves performance

over the single-method performances of all included methods (e.g. TaskEmb+TextEmb). However, for most cases, rank fusion performance is either roughly on-par with the performance of the best included single method (e.g. SEmb+TaskEmb) or even hurts task selection performance sometimes signiﬁcantly (e.g. Size+SEmb). Thus, while adding additional computational overhead to the task selection process, fusing does not yield better performance in general.
F SEmb Model Dependency
The full results of our experiments with sentenceembedding model variants can be found in Table 7. Experiments were conducted on RoBERTa transfer results.

DS MS DS, MS

Random-T Size-T
SEmb-T TextEmb-T
FSFT-T kNN-T linear-T
FS-TaskEmb-T TaskEmb-T

Classiﬁcation
NDCG R@1 R@3
0.54 8.81 5.09 0.54 10.80 6.26
0.83 0.27 0.27 0.75 2.13 0.19
0.86 0.28 0.00 0.82 2.49 0.12 0.78 1.84 1.49
0.88 0.19 0.00 0.76 4.82 0.12

M. Choice
NDCG R@1 R@3
0.66 5.50 1.81 0.66 4.30 1.22
0.92 0.00 0.00 0.89 0.38 0.00
0.93 0.00 0.00 0.81 1.91 1.91 0.96 0.00 0.00
0.75 3.03 0.83 0.76 3.74 0.60

NDCG
0.57 0.96
0.54 0.62
0.49 -
0.46 0.45

QA
R@1
9.46 0.00
7.76 4.04
10.99 -
12.90 12.90

R@3
3.78 0.00
4.04 2.05
10.39 -
4.19 5.42

Tagging
NDCG R@1
0.84 1.54 0.87 0.47
0.94 0.47 0.95 0.47
0.97 0.00 0.95 0.11 0.95 0.00
0.93 0.19 0.92 0.19

R@3
0.38 0.47
0.00 0.00
0.00 0.11 0.00
0.19 0.19

NDCG
0.63 0.71
0.82 0.80
0.83 -
0.78 0.73

All
R@1
6.71 5.18
1.59 1.70
2.10 -
3.28 5.15

R@3
3.10 2.69
0.83 0.44
1.89 -
1.02 1.23

DS MS DS, MS

Random-T Size-T
SEmb-T TextEmb-T
FSFT-T kNN-T linear-T
FS-TaskEmb-T TaskEmb-T

Classiﬁcation
NDCG R@1 R@3
0.50 8.88 5.58 0.53 11.34 5.87
0.82 0.75 0.31 0.82 1.26 0.75
0.92 0.33 0.00 0.91 1.10 0.00 0.79 3.00 1.70
0.95 0.00 0.00 0.87 2.13 0.33

(a) RoBERTa

M. Choice
NDCG R@1 R@3
0.58 6.14 3.31 0.60 5.97 1.78
0.74 0.93 0.93 0.72 1.95 0.93
0.73 5.38 1.95 0.70 2.82 1.46 0.73 2.94 0.93
0.71 5.38 0.93 0.72 4.19 0.93

NDCG
0.72 0.84
0.89 0.87
0.78 -
0.67 0.77

QA
R@1
8.49 3.61
2.98 2.98
0.00 -
11.17 3.61

R@3
2.95 0.00
0.00 2.42
0.00 -
2.07 0.00

Tagging
NDCG R@1
0.74 2.02 0.74 1.36
0.83 0.56 0.82 0.56
0.88 0.65 0.88 0.56 0.85 0.91
0.80 1.37 0.80 1.36

R@3
0.91 0.85
0.51 0.51
0.50 0.51 0.51
0.50 0.50

NDCG
0.61 0.64
0.81 0.80
0.84 -
0.81 0.80

All
R@1
6.82 6.65
1.17 1.63
1.70 -
3.75 2.82

R@3
3.63 2.78
0.46 1.06
0.62 -
0.72 0.46

(b) BERT
Table 5: Evaluation of intermediate task rankings produced by different methods for RoBERTa (a) and BERT (b) when preferring tasks of the same type. The table shows the mean NDCG and Regret scores by target task type. The best score in each group is highlighted in bold, best overall score is underlined. For NDCG, higher is better; for Regret, lower is better.

DS MS
DS, MS

Size+SEmb Size+TextEmb
FSFT+kNN+linear Size+FSFT Size+kNN Size+linear
FSFT+FS-TaskEmb SEmb+TaskEmb Size+FS-TaskEmb Size+SEmb+FSFT+FS-TaskEmb Size+SEmb+linear+TaskEmb Size+TaskEmb Size+TaskEmb+TextEmb TaskEmb+FS-TaskEmb TaskEmb+TextEmb All

Classiﬁcation
NDCG R@1 R@3
0.72 5.68 0.47 0.77 2.13 1.13
0.91 0.19 0.12 0.80 1.21 0.19 0.73 6.42 0.12 0.70 3.53 2.44
0.90 0.46 0.00 0.92 0.27 0.27 0.83 0.93 0.17 0.91 0.19 0.19 0.85 3.45 0.19 0.66 5.98 1.15 0.81 2.13 0.12 0.79 5.53 0.19 0.86 1.12 0.12 0.90 0.19 0.19

M. Choice
NDCG R@1 R@3
0.64 6.95 2.39 0.59 6.98 4.86
0.83 1.91 0.00 0.55 9.56 2.39 0.50 7.07 4.30 0.61 4.30 2.39
0.88 0.83 0.00 0.78 4.88 0.00 0.61 9.87 1.22 0.80 4.88 0.00 0.79 4.88 0.00 0.50 9.56 3.37 0.67 4.88 2.39 0.71 3.74 1.22 0.76 4.88 0.00 0.87 0.38 0.00

NDCG
0.61 0.52
0.28
-
0.25 0.30 0.31 0.33
0.28 0.32 0.25 0.30
-

QA
R@1
33.18 33.18
57.20
-
34.25 28.20 16.39 21.21
43.50 38.07 22.40 36.36
-

R@3
0.00 2.05
18.52
-
18.20 20.15 12.90 10.38
32.70 32.68 22.40 28.75
-

Tagging
NDCG R@1
0.60 7.81 0.62 7.94
0.95 0.11 0.66 5.33 0.62 8.44 0.66 4.37
0.93 0.19 0.81 0.65 0.69 1.54 0.86 0.65 0.78 1.54 0.55 64.25 0.68 31.84 0.85 0.19 0.83 0.19 0.91 0.65

R@3
1.33 1.33
0.11 0.47 1.03 0.47
0.19 0.19 0.47 0.47 0.47 1.53 0.47 0.19 0.19 0.00

NDCG
0.66 0.65
0.62
-
0.78 0.75 0.65 0.76
0.53 0.66 0.68 0.73
-

All
R@1
11.41 10.15
14.42
-
6.66 6.68 6.29 5.38
24.38 14.82 7.14 8.39
-

R@3
1.06 2.35
4.17
-
3.34 3.80 2.83 2.04
7.56 6.72 4.51 5.30
-

Table 6: Evaluation of intermediate task rankings produced by method combinations for RoBERTa. The table shows the mean NDCG and Regret scores by target task type. The best score in each group is highlighted in bold, best overall score is underlined. For NDCG, higher is better; for Regret, lower is better.

SEmb-BERTD SEmb-BERTB SEmb-BERTL SEmb-RoBERTaD SEmb-RoBERTaB SEmb-RoBERTaL

Classiﬁcation
NDCG R@1 R@3
0.83 0.27 0.19 0.82 0.27 0.24 0.82 0.27 0.27 0.84 0.27 0.19 0.82 0.27 0.27 0.76 3.73 0.27

M. Choice
NDCG R@1 R@3
0.67 8.13 2.56 0.58 12.85 2.56 0.58 11.07 2.56 0.76 4.86 0.00 0.79 4.47 0.00 0.75 4.47 0.00

NDCG
0.48 0.61 0.49 0.52 0.49 0.59

QA
R@1
17.04 7.16 17.04 17.04 17.04 7.16

R@3
7.16 2.05 7.16 4.04 7.59 2.05

Tagging
NDCG R@1
0.84 0.47 0.84 0.47 0.84 0.47 0.88 0.47 0.80 0.47 0.82 0.47

R@3
0.00 0.00 0.11 0.11 0.47 0.00

NDCG
0.72 0.72 0.70 0.77 0.75 0.74

All
R@1
5.50 4.99 6.30 4.60 4.50 3.96

R@3
2.07 1.16 2.12 0.82 1.56 0.47

Table 7: Evaluation of intermediate task rankings produced by SEmb variations for RoBERTa tasks. D, B, and L stand for Distill, Base, and Large, respectively. The table shows the mean NDCG and Regret scores by target task type. The best overall scores are highlighted in bold. For NDCG, higher is better; for Regret, lower is better.

Name
Sequence classiﬁcation/ regression
MRPC (Dolan and Brockett, 2005) SICK (Marelli et al., 2014) WiC (Pilehvar and Camacho-Collados, 2019) TREC (Li and Roth, 2002) SciCite (Cohan et al., 2019) CoLA (Warstadt et al., 2019) Emotion (Saravia et al., 2018) IMDb (Maas et al., 2011) MultiRC (Khashabi et al., 2018) SciTail (Khot et al., 2018) EmoContext (Chatterjee et al., 2019) SST-2 (Socher et al., 2013) ReCoRD (Zhang et al., 2018) QNLI (Wang et al., 2018) ANLI (Nie et al., 2020) QQP (Iyer et al., 2017)
MNLI (Williams et al., 2018) SNLI (Bowman et al., 2015) Yelp Polarity (Zhang et al., 2015)
Multiple-choice
QuaRTz (Tafjord et al., 2019) Cosmos QA (Huang et al., 2019) Social IQA (Sap et al., 2019) HellaSwag (Zellers et al., 2019) WinoGrande (Sakaguchi et al., 2020) SWAG (Zellers et al., 2018) RACE (Lai et al., 2017)
ART (Bhagavatula et al., 2020)
Extractive question answering
Quoref (Dasigi et al., 2019) WikiHop (Welbl et al., 2018)18

|Train|
3.7K 4.4K 5.4K
5.5K 8.2K 8.5K 16K 25K 27K 27K 30K 67K 101K 105K 163K 364K 393K 550K 560K
2.7K 25K 33K 40K 41K 74K 88K 170K
20K 51K

Task

semantic similarity NLI

textual

word sense disambiguation

question classiﬁcation citation intents

linguistic acceptability emotion classiﬁcation sentiment classiﬁcation reading comprehension NLI

emotion classiﬁcation sentiment classiﬁcation commonsense reasoning question-answer NLI NLI semantic textual similarity NLI

NLI

sentiment classiﬁcation

qualitative reasoning commonsense reasoning commonsense reasoning commonsense reasoning coreference resolution commonsense reasoning reading comprehension NLI

coreference QA multi-hop QA

Domain/ Source
news image/ video captions misc.
misc. scientiﬁc papers books, journals Twitter movie reviews misc. science exams crowdsourced movie reviews news articles Wikipedia misc. Quora misc. misc. Yelp reviews
crowdsourced crowdsourced knowledge base misc. crowdsourced video captions English exams stories
Wikipedia Wikipedia

Metric(s)
acc./ F1 acc. acc.
acc. acc. Matthews acc. acc. EM/ F1 acc. acc. acc. EM/ F1 acc. acc. acc./ F1 acc. (matched) acc. acc.
acc. acc. acc. acc. acc. acc. acc. acc.
EM/ F1 EM/ F1

RoBERTa BERT

88.48/ 91.53 89.29
65.52
96.4
84.72
59.18
94.1
94.19
28.96/ 67.01 95.25
89
94.95
80.55/ 81.25 92.75
41.5 90.80/ 87.68 87.5
91.13
96.61

84.80/ 89.53 84.24
65.99
95.60
85.26
62.18
93.5
91.76
18.57/ 66.35 93.79
89.74
92.20
64.58/ 65.24 91.14
45.42 90.31/ 87.04 84.20
90.62
95.71

79.69 70.49 72.21 62.04 63.54 83.29 73.46 73.43

52.86 60.47 62.49 38.20 54.38 80.06 65.97 64.36

68.73/ 73.22 56.48/ 61.71

64.06/ 68.15 55.72/ 60.79

DuoRC-s (Saha et al., 2018)18 HotpotQA (Yang et al., 2018)18 DuoRC-p (Saha et al., 2018)18 SQuAD 1.0 (Rajpurkar et al., 2016)18 NewsQA (Trischler et al., 2017)18 SQuAD 2.0 (Rajpurkar et al., 2018)18
Sequence tagging
NER-WNUT17 (Derczynski et al., 2017) NER-MITMovie Chunk-CoNLL2000 (Sang and Buchholz, 2000) POS-EWT (Silveira et al., 2014) POS-CoNLL2003 (Sang and Meulder, 2003) GED-FCE (Rei and Yannakoudakis, 2016) ST-PMB (Abzianidze and Bos, 2017)

86K 90K 100K 108K 120K 162K
3.4K 7.8K 8.9K
12.5K 14K
29K 63K

QA multi-hop QA QA QA QA QA
NER NER chunking
POS POS
GED semantic tagging

Wikipedia Wikipedia IMDb Wikipedia news articles Wikipedia
Twitter, forums movie reviews Penn Treebank
web treebank news
misc. meaning bank

EM/ F1 EM/ F1 EM/ F1 EM/ F1 EM/ F1 EM/ F1
F1 F1 F1
F1 F1
F0.5 acc./ F1

59.36/ 67.10 57.60/ 71.05 49.76/ 53.38 84.02/ 91.06 48.70/ 63.93 78.39/ 81.47
55.24
69.29 96.35
97.30
95.05
89.79/ 68.12 89.50/ 89.38

53.19/ 60.73 54.81/ 68.49 47.76/ 51.31 80.26/ 88.08 48.68/ 64.86 67.99/ 71.22
45.27
68.63 95.92
96.79
93.96
64.94
90.26/ 90.26

Table 8: Overview of intermediate tasks used in our experiments, grouped by task type and ordered by training set size.

Name
Sequence classiﬁcation/ regression
BoolQ (Clark et al., 2019) RTE (Dagan et al., 2005) Rotten Tomatoes (Pang and Lee, 2005) STS-B (Cer et al., 2017)
Multiple-choice
COPA (Gordon et al., 2012) CS QA (Talmor et al., 2019) QuAIL (Rogers et al., 2020)
Extractive question answering
CQ (Bao et al., 2016)18 DROP (Dua et al., 2019)18
Sequence labeling
DepRel-EWT (Silveira et al., 2014) NER-CoNLL2003 (Sang and Meulder, 2003)

Task
binary QA NLI sentiment classiﬁcation semantic textual similarity
commonsense reasoning commonsense reasoning multiple-choice QA
QA QA
relation classiﬁcation19 NER

Domain/ Source
Wikipedia, web queries news, Wikipedia movie reviews misc.
blogs, encyclopedia knowledge base misc.
web snippets Wikipedia
web treebank news

Metric(s)
acc. acc. acc. Spearman
acc. acc. acc.
EM/ F1 EM/ F1
F1 F1

Table 9: Overview of target tasks used in our experiments, grouped by task type.

18We use the version provided in MultiQA (Talmor and Berant, 2019). 19Instead of performing full dependency parsing, we only label each token in a sentence with a label corresponding to the
dependency relation to its head as this task can be modeled directly as a sequence tagging task.

Task

BoolQ COPA CQ

No Transfer
Avg. Transfer
ANLI ART CoLA CoNLL’00 Cosmos QA DuoRC-p DuoRC-s EmoContext Emotion GED-FCE Hellaswag HotpotQA IMDb MIT Movie MNLI MRPC MultiRC NewsQA POS-Co.’03 POS-EWT QNLI QQP QuaRTz Quoref RACE ReCoRD SICK SNLI SQuAD SQuAD 2.0 SST-2 ST-PMB SWAG SciCite SciTail Social IQA TREC WNUT17 WiC WikiHop WinoGrande Yelp Polarity

63.60
67.29
75.84 68.55 64.96 64.00 71.36 68.17 68.43 66.79 65.79 64.11 65.81 65.14 67.55 63.64 76.26 67.88 70.14 68.71 63.52 64.48 69.09 68.88 64.14 66.92 73.00 62.17 67.32 71.07 69.54 70.18 66.99 64.51 65.26 65.74 70.23 70.60 64.48 64.20 64.04 62.95 67.92 66.04

67.04
67.99
71.87 71.27 68.93 57.13 73.73 66.53 70.80 68.93 66.13 68.47 68.93 65.40 69.73 66.20 69.00 72.73 71.73 72.80 56.93 62.87 71.73 73.80 63.40 75.40 72.80 61.13 71.93 68.80 68.47 68.13 69.67 54.47 66.33 67.93 72.00 74.07 67.60 64.73 72.13 62.07 65.47 63.40

23.98
25.76
26.55 23.43 24.61 22.49 25.00 36.97 40.97 22.84 20.42 22.37 20.74 42.35 23.56 23.69 24.32 23.35 22.20 40.60 19.64 22.93 35.15 22.94 22.84 34.47 21.23 27.86 21.18 15.26 37.45 39.30 21.46 19.21 22.33 22.08 21.84 21.74 22.83 22.45 22.11 39.02 20.31 19.79

CS QA CoNLL DROP 2003

DepRel- Quail EWT

51.06
50.28
51.73 51.05 51.57 43.57 52.03 51.79 52.28 50.42 50.15 49.36 51.41 48.02 50.56 48.65 52.36 52.36 53.18 50.34 44.88 48.18 52.72 48.89 51.87 50.83 49.93 50.26 52.63 46.82 52.42 52.63 50.31 41.47 52.63 51.11 52.58 52.61 52.03 49.85 50.31 48.24 49.09 48.57

83.66
82.33
83.80 81.21 82.25 82.23 82.29 85.47 85.69 82.23 82.38 82.47 81.65 80.99 82.49 84.60 82.23 82.90 83.71 83.89 84.72 84.86 83.47 81.34 83.45 85.45 72.29 81.59 82.99 67.23 85.73 85.51 80.08 84.84 83.55 83.69 83.11 78.50 82.60 84.49 83.46 84.16 81.93 76.51

14.36
13.96
13.99 12.85 13.74 10.62 13.92 17.22 17.13 13.85 13.65 12.37 13.56 17.91 14.06 12.10 14.45 14.23 14.05 18.13 11.46 10.74 15.79 11.42 14.16 17.47 12.90 12.69 14.37 10.22 18.80 19.05 12.37 10.79 13.93 14.00 14.23 13.77 14.09 12.17 14.11 15.47 13.98 10.40

76.20
75.06
74.66 73.36 77.02 78.97 75.21 75.03 76.46 76.88 73.49 77.84 75.78 68.55 75.56 76.58 73.39 75.59 75.25 74.18 80.27 80.84 76.76 72.63 76.13 77.94 69.47 70.01 75.93 61.90 77.24 77.39 74.37 79.46 76.50 76.63 76.39 76.61 77.30 75.78 74.76 71.11 73.65 69.65

54.31
54.83
59.23 55.82 54.81 51.88 56.84 52.76 53.80 54.42 54.71 54.36 55.75 50.94 54.74 53.14 59.69 54.54 56.82 52.73 51.85 53.16 56.45 53.77 54.37 54.50 61.09 56.16 55.36 53.68 53.70 54.87 55.61 50.52 56.41 55.07 55.99 57.46 55.45 54.00 55.25 51.96 55.14 54.16

R. Tomatoes
85.35
85.16
84.08 84.99 85.62 84.99 85.30 85.05 85.21 85.93 84.74 85.55 84.93 83.74 86.62 85.99 85.99 85.90 85.58 84.83 84.49 84.08 84.40 85.24 85.77 84.62 85.87 84.83 85.24 82.65 84.96 85.40 91.78 83.08 85.21 85.46 85.65 84.96 85.58 84.83 84.99 84.30 83.02 85.18

RTE
60.94
67.21
77.38 69.43 64.26 64.74 75.21 67.63 66.06 65.82 64.02 63.42 67.99 66.67 60.65 64.86 78.10 66.91 73.65 66.79 61.01 64.26 69.07 70.16 60.89 67.03 73.41 65.70 67.63 73.04 68.95 68.71 63.54 62.09 71.36 58.72 72.20 71.72 64.62 63.30 62.58 65.82 69.55 63.90

STS-B
86.52
86.72
87.77 87.13 86.24 84.98 86.92 87.88 87.20 86.44 86.22 86.14 86.69 85.84 86.21 85.66 88.90 87.19 87.11 87.80 85.25 86.23 88.08 88.02 86.40 87.46 88.29 85.58 87.22 85.36 87.13 87.34 86.36 85.03 86.86 86.72 87.70 87.02 86.53 86.54 86.53 85.47 86.91 85.76

Table 10: Target task performances for transferring between intermediate tasks (rows) and target tasks (columns) with BERT as base model. The ﬁrst row ‘No Transfer’ shows the baseline performance when training only on the target task without transfer. All scores are mean values over three random restarts.

