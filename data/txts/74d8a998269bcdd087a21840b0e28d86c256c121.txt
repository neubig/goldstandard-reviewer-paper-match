Learning Time Dependent Choice

Zachary Chase∗

Siddharth Prasad†

September 11, 2018

arXiv:1809.03154v1 [cs.GT] 10 Sep 2018

Abstract
We explore questions dealing with the learnability of models of choice over time. We present a large class of preference models deﬁned by a structural criterion for which we are able to obtain an exponential improvement over previously known learning bounds for more general preference models. This in particular implies that the three most important discounted utility models of intertemporal choice – exponential, hyperbolic, and quasihyperbolic discounting – are learnable in the PAC setting with VC dimension that grows logarithmically in the number of time periods. We also examine these models in the framework of active learning. We ﬁnd that the commonly studied stream-based setting is in general diﬃcult to analyze for preference models, but we provide a redeeming situation in which the learner can indeed improve upon the guarantees provided by PAC learning. In contrast to the stream-based setting, we show that if the learner is given full power over the data he learns from – in the form of learning via membership queries – even very naive algorithms signiﬁcantly outperform the guarantees provided by higher level active learning algorithms.
1 Introduction
We study the learnability of economic models of choice over time. Our setting is that of an analyst who ﬁrst observes an agent’s choices between plans that specify payoﬀs over time, and then attempts to learn the preference parameters guiding the choices. While such parameters are stylized – in reality subjects are not likely to perform standardized computations according to private parameters before making decisions – experiments have shown that they often provide accurate descriptions of how an agent behaves. By observing enough choice data, one can hope to learn the economic parameters that most closely describe the agent’s preferences. Thus, learning theory provides an especially meaningful lens with which to view the theory of choice – it allows us to answer questions regarding the volume of data required to faithfully predict future decisions made by an observed agent. The overarching goal of this paper is to identify structural criteria that yield strong learnability results for preferences over time under diﬀerent restrictions placed on the learner/analyst. The criteria we present captures a large class of preference models that give the agent signiﬁcant freedom in weighting decisions against time delays. In particular, it encompasses the most popular models of time dependent choice used by economists.
∗California Institute of Technology, zchase@caltech.edu †California Institute of Technology, sprasad@caltech.edu
1

The main economic application of our results is in understanding the learnability of models of intertemporal choice. Intertemporal choice is what governs an agent’s decisions over several time periods. The most important models of intertemporal choice are discounted utility models, in which agents evaluate plans by discounting actions as they are delayed – in analogy to how markets value the loss or gain of money over time. The ﬁrst axiomatic treatment of discounting was by Koopmans in 1960 [18], in which he demonstrates that simple postulates for preferences over an inﬁnite time horizon yield “impatience.” The three most commonly studied discounting models are exponential, hyperbolic, and quasi-hyperbolic, and all have been studied by both economists and computer scientists (though less so by the latter) as well as researchers from various other ﬁelds. The importance of discounted utility in economics cannot be overstated – it is the canonical framework used by economists to study choice over time.
Problems of learning economic parameters have received recent attention from computer scientists; see, e.g., [1, 2, 3, 16, 22]. Inspired by a general theme of demanding computational robustness from economic models (Echenique, Golovin, and Wierman provide a nice discussion of this topic in [11]), the tools of learning theory provide relevant and exciting perspectives from which to view economic models that have been around for several decades. In contrast to the usual goal of truthfully extracting the agent’s parameters adopted by classical mechanism design, the learning problem aims to eﬃciently extract a truthful agent’s parameters in the restricted message space of binary classiﬁcation. Our paper contributes to the line of work that speciﬁcally studies models of choice using the perspectives of learning theory. This conﬂuence of decision theory and learning theory was initiated by Basu and Echenique [2], who consider the learning problem for models of choice under uncertainty. Our investigation in this paper is motivated by models of how agents make choices over time. We provide learnability results that are ﬁne tuned to structural requirements on such models.
We now summarize our main contributions at a high level. Section 3 contains a more detailed exposition of our results.
Summary of results and techniques
Our situation is one of an analyst trying to learn the parameters governing an agent’s preferences over time. The two main learning themes we consider are (1) when the analyst has no control over the data he sees and (2) when the analyst has some control over the data he sees. The ﬁrst theme is aptly captured by probably approximately correct (PAC) learning. To analyze the second theme, we investigate two models of active learning: stream-based selective sampling and membership queries.
In the ﬁrst part, we study the PAC model, where the analyst is presented with pairs of alternatives and a label for each pair indicating the agent’s preference between the alternatives. The data points are drawn according to some unknown distribution, and the analyst has no control over the data he is presented with. Our main result here is a structural criterion on preference models that allows for a drastic improvement over the PAC learning complexity bounds achieved in [2]. We stipulate that the agent weights time-delayed payoﬀs according to polynomials, which allows for considerable freedom in how payoﬀs are weighted. Under this requirement, we show that such classes of preference models admit an exponential improvement in sample complexity bounds over the more general preference models considered in [2]. This is achieved via a computation of the VC dimension (which quantiﬁes the complexity of
2

PAC learning). A simple application of our result shows that each of the discounted utility models are learnable, with sample complexity that grows logarithmically in the number of time periods T over which decisions are being made. The computation of the VC dimension is due to a natural connection between pairs of choices and the signs of polynomials that arise from the choices.
In the second part, we consider active learning models, where the analyst is given a certain amount of control over the data that he uses to learn. The two active learning models we study are stream-based selective sampling and learning via membership queries. In the former, the analyst is given some control over what data he learns from: as in the PAC setting he is presented with points drawn from an unknown distribution, but now the analyst chooses whether or not to see the label representing the agent’s choice for each point. In the latter, the analyst has complete control over the data he learns from: the analyst can at any time request the label for any point. The former model seems to have been commonly adopted in order to study the very general problem of concept learning, when there is no extra information about the structure of the concepts. We ﬁnd that the disagreement methods used to study the stream-based setting are in general diﬃcult to analyze in the context of preference models – requiring quantitative information about the underlying distribution from which points are drawn. However, we provide a redeeming situation (by examining a particular distribution) where we obtain an improvement over the PAC guarantees. Membership queries, on the other hand, allow us to heavily exploit the structure of the preference models we consider. We present a naive membership query-based algorithm that signiﬁcantly outperforms the guarantees provided in the stream-based setting. Learning via membership queries, we conclude, seems to be the appropriate model to actively learn economic parameters. It allows the analyst to make use of the preference relations’ structure, and also precisely captures the situation in which the analyst and agent are participating in a real time experiment.
Related work
Discounted utility models of intertemporal choice have been studied extensively not only by economists, but also by researchers from various other ﬁelds. We ﬁrst brieﬂy survey some of the relevant work pertaining to the exponential, quasi-hyperbolic, and hyperbolic discounting models and then survey existing work in the more general topic of learning economic parameters.
In the exponential discounting model, the agent evaluates his utilities based on a discount factor δ ∈ (0, 1), where a delay of t time periods incurs an exponential discount in utility by δt. Climate change policies are traditionally evaluated according to an exponential discounting model – for example, the Stern review on the economics of climate change deals with issues of how to choose an appropriate discount rate in evaluating such policies [21]. Chambers and Echenique [8] present results related to the problem of aggregating discount rates proposed by a group of experts facing disagreement. While it is the most commonly used discounting model due to its simplicity, the exponential discounting model has been criticized due to its inability to match empirical data recording actual human behavior. Quasi-hyperbolic and hyperbolic discounting aim to mend such issues. The quasi-hyperbolic discounting model is parametrized by β, δ ∈ (0, 1), where a delay of t time periods incurs a discount in utility by βδt, and was ﬁrst introduced by Phelps and Pollack [20] to study preferences over generations. They proposed that the constant β discount factor represents how much a given generation t
3

is aﬀected by the utilities of other people relative to their own – and remark that β = 1 represents “perfect altruism,” while β < 1 represents “imperfect altruism.” Kleinberg and Oren [17] study agents with quasi-hyperbolic discounting and propose a graph-theoretic model to investigate phenomena such as procrastination and abandonment of long-range tasks. Hyperbolic discounting aims to capture the notion that people are more impatient in making short term decisions (today vs. tomorrow) than long term decisions (365 days from today vs. 366 days from today)1, and is modeled via a discount of (1 + tα)−1 at time t. Researchers in ﬁelds such as psychology and neuroscience [4, 15] have adopted the hyperbolic discounting model to study, for example, issues of self control and anticipation in humans and animals, and have compared the predictions by the diﬀerent discounted utility models to neurobiological data obtained via MRI scans. Chabris et al. [7] give an exposition of the discounted utility models of intertemporal choice and survey sociological research that examines empirical data pertaining to how discount rates are aﬀected by factors like age, drug use, gambling, etc.
The study of economic models has witnessed a recent inﬂux of work from computer scientists dealing with questions of robustness under various notions of complexity (learning complexity, computational complexity, communication complexity, etc.). Kalai [16] in 2001 studied the learnability of choice functions, where the observed choices are in the form of a given set of alternatives along with the most preferred alternative from the set. Beigman and Vohra [3], Zadimoghaddam and Roth [22], and Balcan et al. [1] investigate the problem of learning utility functions in the context of an expected utility maximizing agent in a demand environment. Most recently (and most related to our work), Basu and Echenique [2] study the learnability of preference models of choice under uncertainty, in which an agent is uncertain about states of a lottery and is made to choose between acts that encode utilities over each state. Here, the diﬀerent models of choice under uncertainty arise from diﬀerent ways of representing the subjective probability held by an agent. They are also the ﬁrst to study learnability in the decision-theoretic setting where choice is modeled by preference relations rather than by expected utility maximizing behavior in a demand setting. However, it does not appear that the learnability of models of intertemporal choice has been previously studied.
2 Model and Preliminaries
We now formally set up the discounted utility models of intertemporal choice and state the standard deﬁnitions from learning theory in the context of preference relations. Much of the following material regarding learning and preference relations is taken from [2] since we require a similar list of deﬁnitions and setup. First, we sketch our high level model.
Let X be a Euclidean space equipped with a Borel σ-algebra. A preference relation on X is a binary relation ⊆ X × X such that is measurable with respect to the product σ-algebra on X × X. A model P of preference relations is a collection of preference relations.
An agent makes choices from pairs of alternatives (xi, yi)ni=1 that are drawn according to some unknown distribution on X ×X. The choices are presented as labels (ai)ni=1 where ai = 1 if the agent chooses xi and ai = 0 if the agent chooses yi. A dataset is any ﬁnite sequence of pairs of plans and their labels (((x1, y1), a1), . . . , ((xn, yn), an)). An analyst observes a dataset, and attempts to guess the preference relation governing the agent’s choices. A learning rule
1In particular note that exponential discounting does not capture this issue, i.e. it is dynamically consistent, in that preferences do not change according to shifts in time.
4

is any map σ from datasets to preference relations. The output of the learning rule is the analyst’s hypothesis as to what the agent’s true preference relation is, having seen some ﬁnite dataset.

2.1 Learnability

The two notions of learnability we consider are the PAC model and the active model. We

now state the standard deﬁnitions of PAC and active learning in the context of preference

relations. Most of the following deﬁnitions for the PAC setting are taken from [2] since the

setup involving preference relations is identical. These deﬁnitions of course apply to the more

general setting of concept learning (for example, see [6]).

A collection P of preference relations is (PAC) learnable if there is a learning rule σ such

that for every 0 < ε, δ < 1, there is s(ε, δ) ∈ N such that for every n ≥ s(ε, δ), ∈ P, and

µ ∈ ∆(X × X),

µn({((x1, y1), . . . , (xn, yn)) : µ( ∗ △ ) > ε}) < δ,

where

∗= σ({((x1, y1), Ix1 y1 ), . . . , ((xn, yn), Ixn yn )})

is the hypothesis preference relation produced by the learning rule2. The quantity s(ε, δ) is
called the sample complexity of the learning rule σ.
The complexity of learning is commonly quantiﬁed by the Vapnik-Chervonenkis (VC) dimension, which we now deﬁne. A set of points {(x1, y1), . . . , (xn, yn)} from X × X is shattered by a model of preferences P if for every vector of labels (a1, . . . , an) ∈ {0, 1}n, there is a preference relation ∈ P that realizes the labelling, i.e. for i = 1, . . . , n we have that xi yi if and only if ai = 1. In this case, P is said to rationalize the dataset {((x1, y1), a1), . . . , ((xn, yn), an)}. The VC dimension of P, denoted by V C(P), is the largest integer n such that there exist n points that are shattered by P.
Blumer et al. [6] in 1989 proved that learnability is equivalent to having a ﬁnite VC dimension3.

Theorem 2.1. A model of preferences P is learnable if and only if V C(P) < ∞.

The VC dimension (denoted by d for the remainder of this subsection) also plays a role in the sample complexity of learning a model of preferences. In the same paper, Blumer et al. [6] show that any algorithm that outputs a hypothesis consistent with the data seen is a valid learning rule requiring sample complexity

s(ε, δ) = O 1 d log 1 + log 1 .

ε

ε

δ

In 2016, Hanneke [14] showed that these bounds (after a small improvement) are tight: the optimal sample complexity of PAC learning is

s(ε, δ) = Θ 1 d + log 1 .

ε

δ

2µn denotes the product measure induced by µ on (X × X)n. 3This result requires P to satisfy a certain measurability requirement. We note in Section 4 that the models
of choice we consider all satisfy said requirement.

5

The other learning model we consider is the active learning framework, where the analyst

has some control over the data from which he learns. In stream-based selective sampling,

points drawn according to an unknown distribution are presented to the analyst as before,

but without the labels. The analyst can choose whether or not to query the label of a

given point, and the complexity of the learning rule is measured by label complexity, i.e. the

number of labels requested by the analyst. Disagreement based active learning refers to the

paradigm in which the learner only requests labels on points that signiﬁcantly reduce the

hypothesis space. The disagreement of a preference model with respect to the underlying

distribution is quantiﬁed through the disagreement coeﬃcient θ, which is deﬁned in Section

5. A ﬁnite disagreement coeﬃcient implies (for the underlying distribution) an exponential

improvement in label complexity over the sample complexity of PAC learning. For example,

the CAL algorithm [9, 10, 13], a simple disagreement based learning algorithm, yields a label

complexity of

1

log(1/ε)

ℓCAL(ε, δ) = O θ log ε d log θ + log δ

.

In the membership queries model, the analyst is allowed to request the label for any point at any time. There appears to be a dearth of literature/results pertaining to the complexity of membership query algorithms for learning when the hypothesis space is inﬁnite. One explanation for this is that improvements to the “passive” disagreement based methods used in the stream-based setting would need speciﬁc information about the problem domain: disagreement based methods are designed to work on a very general class of concept learning problems without assuming anything about the learning space. In our case, we have speciﬁc details about how the preference relations take shape. Thus, the membership query model turns out to be an interesting and useful perspective to use in the study of learning preference models.
For a more detailed survey of active learning, see [10].

2.2 Discounted utility
We now present the deﬁnitions for the discounted utility models of intertemporal choice. An agent chooses between plans or vectors in X = RT that encode payoﬀs over T time periods. A preference relation over plans is a binary relation ⊆ RT × RT .
The most important model of intertemporal choice is the discounted utility model, in which the agent’s payoﬀs xt for having chosen a plan x ∈ RT are reduced, or discounted, as t increases from 1 to T . In its most general form, we can characterize the preference relations that follow time discounting as follows:

Deﬁnition 2.1 (Discounted utility model). The class of preference relations PD that satisfy the discounted utility model are those such that there exists a decreasing map D : {1, . . . , T } → (0, 1) where

T

T

x y if and only if

D(t)xt ≥ D(t)yt.

t=1

t=1

We use the following notation for the preference models arising from the three most commonly studied discounting functions D:

6

• PD denotes the set of preferences that satisfy the discounted utility model.

• PED denotes the set of preferences that satisfy the discounted utility model with exponential discounting: D(t) = δt for δ ∈ (0, 1).

• PHD denotes the set of preferences that satisfy the discounted utility model with hyper-

bolic

discounting :

D(t)

=

1 1+tα

for

α

>

0.

• PQHD denotes the set of preferences that satisfy the discounted utility model with quasihyperbolic discounting: D(t) = 1 if t = 1, D(t) = β · δt−1 if t > 1 for β, δ ∈ (0, 1).

For a more thorough exposition on the various discounted utility models of intertemporal choice, see [7].

3 Main Results
In this section we provide a formal discussion and interpretation of our results, which is split into two themes: the ﬁrst dealing with an analyst who has no control over the learning data, the second dealing with an analyst who has some control over the learning data.

A powerless analyst

The ﬁrst part of our paper investigates the situation of an analyst trying to learn the preference

relation by which an agent makes choices, but has no control over what choices he gets to

observe, and is agnostic to the process by which they are drawn. We thus adopt the PAC

learning model.

The agent chooses between plans that encode payoﬀs over T periods of time and evaluates

the total payoﬀ of a plan vector x ∈ RT according to private weights w1, . . . , wT that he mul-

tiplicatively applies to each state: payoﬀ(x) =

T t=1

wtxt.

This

deﬁnes

a

model

of

preference

relations, which we denote by PW , where for any ∈ PW , there exists a vector of weights w = (w1, . . . , wT ) ∈ RT such that

x y if and only if w.x ≥ w.y.

In [2], it is shown that T − 1 ≤ V C(PW ) ≤ T + 1. In the context of choice over time, however, this model is extremely general and does not capture any of the intuitive notions of how an agent values payoﬀs when they are delayed4. For example, the discounted utility models of intertemporal choice require the weights to be of a particular functional form. Moreover, when there is no structure to the discount function we cannot improve the bounds on PW :
Proposition 3.1. T − 1 ≤ V C(PD) ≤ T + 1.
This leads us to the motivating question of the ﬁrst part of the paper: what structural conditions can we impose on the weights w1, . . . , wT such that this bound can be improved?
We investigate the situation where the agent computes his weights by evaluating polynomials at a private parameter δ. Speciﬁcally, let Q1, . . . , QT be polynomials of degree at
4In [2] the complete control over weights is used to model choice under uncertainty, which calls for such generality since the agent’s beliefs/weights are given by an element of the probability simplex on RT .

7

most d, and suppose the agent evaluates total payoﬀ of a plan vector x ∈ RT by payoﬀ(x) =

T t=1

Qt(δ)xt

.

Consequently,

let

PP W

be the

model

of

preference

relations

parametrized

by

δ such that

T

T

x y if and only if Qt(δ)xt ≥ Qt(δ)yt.

t=1

t=1

This class of preference models allows us to approximate preference relations where the

weights are given by any real valued functions – we choose Q1, . . . , QT to be the appropriate

Taylor polynomials. Moreover, existing models of intertemporal choice ﬁt this characterization

– for example PED and PHD.

We additionally consider a slightly larger class of preference models where the agent has a

private parameter β (in addition to δ) that in evaluating total payoﬀ of a plan vector x ∈ RT

allows the agent to modify the constant term

T t=1

Qt(0)xt

of

the

polynomial

T t=1

Qt

(δ)xt

.

This model aims to more generally capture the eﬀects of the β parameter in quasi-hyperbolic

discounting. For polynomials Q1, . . . , QT of degree at most d, let PBPW be the model of

preference relations parametrized by β and δ such that x y if and only if

1 −1 β

T

T

Qt(0)xt + Qt(δ)xt ≥

t=1

t=1

1 −1 β

T

T

Qt(0)yt + Qt(δ)yt.5

t=1

t=1

Our main results show that with this additional structure on the preference model, we can
achieve an exponential improvement in the bounds for the VC dimension of PW obtained in [2]6.

Theorem 3.2. For every ε > 0, there exists a dε such that for every d ≥ dε we have V C(PPW ), V C(PBPW ) ≤ (1 + ε) log d for any T and any T polynomials Q1, . . . , QT of degree at most d.

Note that when Q1, . . . , QT have degree at most polynomial in T , we obtain an exponential improvement over the linear growth of V C(PW ). We show that in this case, we get a tight (asymptotic) bound of log T :

Theorem 3.3. Let Q1, . . . , QT be polynomials in δ of degree at most T − 1 that span the space of polynomials in δ of degree at most T − 1. Then V C(PPW ), V C(PBPW ) ≥ log(T − 1).

An interesting feature of Theorems 3.2 and 3.3 is that for ﬁxed Q1, . . . , QT with degrees

at most T − 1, V C(PPW ) and V C(PBPW ) satisfy the same asymptotic bounds, so giving

the agent an extra parameter that allows control over the constant term of the polynomial

T t=1

Qt(δ)xt

does

not

introduce

a

signiﬁcant

amount

of

richness

to

the

model.

Applying Theorems 3.2 and 3.3 to the discounted utility models, we have:

Corollary 3.4. V C(PED), V C(PHD), V C(PQHD) ∼ log(T − 1)
5We use a factor of (1/β − 1) since it yields a clean description of quasi-hyperbolic discounting. 6It is important to note that the classes PPW and PBPW are deﬁned for a given Q1, . . . , QT . That is, the analyst knows Q1, . . . , QT , and is trying to learn the parameters β and δ. If the Q1, . . . , QT are private information only available to the agent, we are in no better shape than in the case of PW.

8

Thus, PD, PED, PHD, and PQHD are all learnable. PD requires a minimum sample size that grows linearly with T , while PED, PHD, and PQHD require a minimum sample size that grows logarithmically in T .
The main technique in proving Theorems 3.2 and 3.3 is interpreting the shattering criteria as a statement about the sign combinations achieved by a collection of polynomials. The upper bound on the VC dimension follows from an upper bound on the number of sign combinations a collection of polynomials can achieve. In demonstrating the lower bound on the VC dimension, we construct a set of points that is shattered by ﬁnding polynomials achieving all possible sign combinations – chosen according to a Hamiltonian path in the log(T − 1)-dimensional hypercube.
A powerful analyst
Our other results concern the active learning framework, which broadly deals with situations in which the analyst has some control over the choices he observes and learns from. The two models we consider are stream-based selective sampling and learning via membership queries. A large body of active learning research is devoted to the stream-based model, speciﬁcally focusing on disagreement based algorithms – a class of learning algorithms that instructs the analyst only to request labels on points he sees that reduce the hypothesis space signiﬁcantly. In the most general setting of concept learning, this is a useful framework since the error guarantees can be described using the same setup as the PAC model. Moreover without additional information about the problem domain, it is unclear how to devise eﬃcient algorithms that are more speciﬁc in instructing the analyst on what questions to ask.
We ﬁnd that the stream-based model is in general diﬃcult to analyze for the preference relations we work with. This diﬃculty seems to arise from the apparent need to quantify disagreement in order to explicitly write down learning guarantees. Though in most general situations it is unclear how to quantify disagreement for our preference relations, we present a redeeming situation for which we are able to provide a precise analysis of the learning guarantees for PED. Here, the analyst can learn PED with an exponential improvement in label complexity over the guarantees provided by the PAC model. This is achieved via a computation of the disagreement coeﬃcient (deﬁned in Section 5) of PED for a speciﬁc distribution.
Theorem 3.5. There exists a distribution µ on RT ×RT for which the disagreement coeﬃcient of PED is θ = 2. Thus, for this distribution,
1 ℓCAL(ε) = O log T log ε ,
where the O notation suppresses terms that are logarithmic in log T and log 1/ε.
The measure µ we construct is induced by the product Lebesgue measure on (0, 1)T −1, and allows us to precisely translate statements about disagreement into statements about the roots of polynomials arising from a given choice. Once we have deﬁned µ, the calculation of θ follows from basic probability arguments.
Now, in our case the analyst has structural information regarding the preference relation of the agent he is questioning. We ﬁnd that allowing the analyst full control over the membership queries he makes yields a learning algorithm that, despite its simplicity, takes advantage of
9

this extra structure and yields a signiﬁcant improvement in complexity over the stream-based

setting. Additionally, the membership queries model naturally describes an experimental

environment in which the analyst is able to ask the agent questions in real time.

We show that when the preference model satisﬁes some relatively benign structural re-

quirements, even very naive algorithms outperform the guarantees provided by CAL in the

stream-based setting. The example algorithm we give, relying on a simple binary search, has

a query complexity of O(log 1/ε), which gets rid of the log T dependence in Theorem 3.5.

The class of preference models is deﬁned as follows: let g1, . . . , gT : R → R be a collection

of functions satisfying the properties listed in Section 5.3 and consider the model of preference

relations P parametrized by δ where x

y if and only if

T t=1

gt

(δ)xt

≥

T t=1

gt

(δ)yt

7.

We

have

Proposition 3.6. There exists an algorithm that takes as input ε > 0 and using O(log 1/ε) membership queries outputs δh such that |δ−δh| ≤ ε, where δ parametrizes the target preference
relation in P.

The remainder of the paper is devoted to proving the results discussed in this section.

4 PAC Learning

In this section we prove Theorems 3.2 and 3.3. We ﬁrst note a preliminary upper bound due to Basu and Echenique [2]. Let PI be the set of preference relations that satisfy the following axioms:

Order: For all x, y either x y or y x (completeness). For all x, y, z, if x y and y z, then x z (transitivity).

Independence: For all x, y, z and for any λ ∈ (0, 1), x λy + (1 − λ)z.

y if and only if λx + (1 − λ)z

The class PI satisﬁes the property that for any ∈ PI , there are ﬁnitely many vectors q1, . . . , qK , with K ≤ T , such that x y if and only if (qk.x)Kk=1 ≥L (qk.y)Kk=1, where ≥L denotes the lexicographic order [5]. Then, PPW , PBPW ⊂ PW ⊂ PI, since the aforementioned characterization is satisﬁed with K = 1 and q1 = (w1, . . . , wT ).
This has two main consequences. First, PPW and PBPW (and thus all the discounted utility models) satisfy the measurability requirement discussed in Lemma 4 of [2] for the equivalence result of Theorem 2.1 to hold. Second, the VC dimensions of PPW and PBPW are all bounded above by T + 1 (and in particular T − 1 ≤ V C(PW ) ≤ T + 1). This follows due to Theorem 3.1 of [2], in which an argument similar to that required to compute the VC dimension of the class of half-spaces is used to show that V C(PI) = T + 1. In all cases excluding the most general model of discounted utility, we are able to bring this down to log(T − 1) (which we then show is tight by demonstrating the corresponding lower bound).
We begin by demonstrating that even in the discounted utility setting, without any structure we cannot do better than the learning bounds obtained for PI .
Proposition 3.1. T − 1 ≤ V C(PD) ≤ T + 1.
7As before, the g1, . . . , gT are known to the analyst.

10

Proof. That V C(PD) ≤ T + 1 follows from Theorem 3.1 of [2], since PD ⊂ PI . Here is a simple construction that shows V C(PD) ≥ T − 1. Fix an ε > 0. Let e1, . . . , eT be
the standard unit vectors in RT , and consider the set of points {(x1, y1), . . . , (xT −1, yT −1)}, where xi = (1 − ε)ei and yi = ei+1.
This set is shattered by PD: for any (ai)Ti=−11, choose D(1) arbitrarily from (0, 1), and if D(i) has been deﬁned, inductively deﬁne D(i + 1) such that D(i + 1) ≤ D(i)(1 − ε) if ai = 1 and D(i) > D(i + 1) > (1 − ε)D(i) if ai = 0.
We now prove Theorems 3.2 and 3.3, which are restated below for convenience.

Theorem 3.2. For every ε > 0, there exists a dε such that for every d ≥ dε we have V C(PPW ), V C(PBPW ) ≤ (1 + ε) log d for any T and any T polynomials Q1, . . . , QT of degree at most d.
Proof. It suﬃces to establish the bound for PPBW . Let (z1, . . . , zn) be a set of points in RT × RT , zi = (xi, yi). For each zi = (xi, yi), deﬁne
the plan f i := xi − yi. Then, note that (z1, . . . , zn) is shattered by PPBW if and only if ((f 1, 0), . . . , (f n, 0)) is shattered by PPBW . Hence, we may (and do) restrict attention to datasets of the form ((f 1, 0), . . . , (f n, 0)).
We have that ((f 1, 0), . . . , (f n, 0)) is shattered by PPBW if and only if for all vectors (a1, . . . , an) ∈ {0, 1}n, there exists a δ and β (which determines the preference relation) such that
(QT (δ)fTi + · · · + Q1(δ)f1i) + β1 − 1 (QT (0)fTi + · · · + Q1(0)f1i) ≥ 0 whenever ai = 1,
and
(QT (δ)fTi + · · · + Q1(δ)f1i) + β1 − 1 (QT (0)fTi + · · · + Q1(0)f1i) < 0 whenever ai = 0.

We ﬁrst show that for all ε > 0, for suﬃciently large d we have V C(PPBW ) ≤ (1 + ε) log d. Note that if the n points ((f 1, 0), . . . , (f n, 0)) can be shattered, there are polynomials

P1, . . .

, Pn

in

δ

(where

Pi

is

the

polynomial

Q1(δ)f1i

+

···

+

Q

T

(δ

)f

i T

),

each

of

degree

at

most

d, such that for every labeling (a1, . . . , an) ∈ {0, 1}n, there exists a δ and β such that

(sgn(P1(δ) + (1/β − 1)P1(0)), . . . , sgn(Pn(δ) + (1/β − 1)Pn(0))) = (a1, . . . , an)8.

First, for any n polynomials P1, . . . , Pn of degree at most d, we give an upper bound on the number of possible values (sgn(P1(δ)), . . . , sgn(Pn(δ))) can realize. Each polynomial has at most d real roots, so together P1, . . . , Pn have at most nd distinct real roots. Since sign changes can only occur at the roots, there are at most nd + 1 possible values of {0, 1}n that
(sgn(P1(δ)), . . . , sgn(Pn(δ))) can realize. Now, for a ﬁxed δ, varying β shifts the collection of polynomials

P1(δ) + (1/β − 1)P1(0), . . . , Pn(δ) + (1/β − 1)Pn(0)
8For notational convenience, let sgn(x) be 1 if x ≥ 0 and 0 otherwise.

11

vertically, which in the worst case induces sign changes in all entries. We thus get at most an additional n new sign combinations for every sign combination realized by (sgn(P1(δ)), . . . , sgn(Pn(δ))). Hence, there are at most nd + 1 + n(nd + 1) = (n2 + n)d + n + 1 possible values of {0, 1}n that

(sgn(P1(δ) + (1/β − 1)P1(0)), . . . , sgn(Pn(δ) + (1/β − 1)Pn(0)))

can realize. In order for all 2n elements of {0, 1}n to be realized, it must be that
(n2 + n)d + n + 1 ≥ 2n.

If n > (1 + ε) log d, then for large enough d this inequality does not hold, and so any set of n points cannot be shattered. Thus, for all ε > 0, n ≤ (1 + ε) log d for large enough d, i.e. V C(P) ≤ (1 + ε) log d.

We now establish the corresponding lower bound when the polynomials Q1, . . . , QT span the space of polynomials of degree at most T − 1.

Theorem 3.3. Let Q1, . . . , QT be polynomials in δ of degree at most T − 1 that span the space of polynomials in δ of degree at most T − 1. Then V C(PPW ), V C(PBPW ) ≥ log(T − 1).

Proof. It suﬃces to establish the bound for PPW . Consider the graph on {0, 1}n where two vertices are connected by an edge if they diﬀer

in exactly one location. Fix a Hamiltonian path v1, v2, . . . , v2n in this graph (the existence of

which is well known). Let b1,2, . . . , b2n−1,2n be the sequence where bi,i+1 is the index of the

location at which vi and vi+1 diﬀer. Note that if n = log(T − 1), the graph has T − 1 vertices,

so each index in {1, . . . , n} can appear in the sequence (bi,i+1) at most T − 1 times.

Now, let r1 < r2 < · · · < r2n be any points in (0, 1). Deﬁne n polynomials P1, . . . , Pn by

Pk(δ) = bi,i+1=k(δ − ri), so the roots of Pk are precisely the ri’s that correspond to a ﬂip

in the entry at the kth position of a vertex in the path. Then, (sgn(P1(δ)), . . . , sgn(Pn(δ))) realizes every element of {0, 1}n.

Since Q1, . . . , QT span the space of polynomials of degree at most T − 1, for each Pi we

can

ﬁnd

f

i 1

,

.

.

.

,

f

i T

such

that

Pi(δ) = Q1(δ)f1i + · · · + QT (δ)fTi ,

which gives us a collection of log(T − 1) points that is shattered. Hence log(T − 1) ≤ V C(P).

It is readily seen that PED and PQHD satisfy the conditions of Theorems 3.2 and 3.39.

We give a quick argument verifying that PHD does as well: For 1 ≤ t ≤ T , let Qt(α) =

ℓ∈{1,...,T }\{t}(1 + ℓα) (these are the polynomials obtained by clearing denominators of the

hyperbolic discount factors). We argue that {Qt(α)}Tt=1 are linearly independent over the

vector space of polynomials in α of degree at most T −1. Indeed, if Q1(α)f1 +· · ·+QT (α)fT =

0, then we must have that f1

= · · · = fT

= 0 since at α =

−1 t

we get Qt(α)ft

= 0.

Hence,

PHD is simply PPW with Qt(α) = ℓ∈{1,...,T }\{t}(1 + ℓα).

9PED is given by PPW with Qt(δ) = δt−1, and PQHD is given by PBPW with Qt(δ) = δt−1.

12

Remark. These bounds also hold in the scenario where the agent can report indiﬀerence in the data. More precisely, the condition for x y is now a strict inequality, and we have three possible labels for the pair (x, y): +1 indicates that x y, −1 indicates that y x, and 0 indicates that x ∼ y. Then, using a Hamiltonian path on {−1, 0, 1}n, we can construct polynomials P1, . . . , Pn such that (sgn(P1(δ)), . . . , sgn(Pn(δ))) realizes all elements of {−1, 0, 1}n as δ ranges from 0 to 1 (where sgn is the true sign function).
4.1 A Remark on Eﬃcient Learnability
While PAC learnability is a positive result, it does not take into account the computational complexity of computing a hypothesis. Blumer et. al. [6] show that any learning rule that outputs a hypothesis consistent with the data seen yields with high probability a hypothesis that has very low error. However, if the problem of outputting a consistent hypothesis is computationally intractable, PAC learnability on its own is perhaps unsatisfying. In this section we note that the discounted utility models of intertemporal choice are eﬃciently learnable. This is due to an algorithm of Grigor’ev and Vorobjov [12] for solving a system of polynomial inequalities.
For notational convenience, it will be useful to write P = {PT }T ≥1, for each of the models above, where PT is the collection of preference relations for a given T . Moreover, suppose acts are chosen from [−1, 1]T instead of RT . It is clear that this does not change any of the analysis above.
Polynomial learnability, as deﬁned by Blumer et. al. [6], stipulates that the learning rule be computable in poly(1/ε, 1/δ, T )-time (where ε and δ denote the error threshold and conﬁdence threshold respectively). Polynomial learnability is equivalent to the task of outputting a hypothesis consistent with the given data set in polynomial time [6].
Deﬁnition 4.1. A randomized polynomial hypothesis ﬁnder (r-poly hy-ﬁ) for P is a randomized polynomial time algorithm that takes as input a sample of a preference relation in P, and for some γ > 0, with probability at least γ produces a hypothesis that is consistent with the sample.
Theorem 4.1. P is properly polynomially learnable if and only if there is an r-poly hy-ﬁ for P and V C(PT ) grows only polynomially in T .
We have just shown that V C(PT ) ∼ log(T − 1). Using techniques involving algebraic geometry, Grigor’ev and Vorobjov [12] present an algorithm to solve a system of N polynomial inequalities with a poly(N, T ) runtime. This serves as our r-poly hy-ﬁ, and thus we obtain:
Theorem 4.2. PED, PHD, and PQHD are all properly polynomially learnable.
5 Active Learning
In this section we study two models of active learning: stream-based selective sampling and learning via membership queries. We ﬁrst deﬁne a distribution for which the disagreement coeﬃcient of PED is 2, showing that disagreement methods (speciﬁcally the CAL algorithm [9, 10, 13]) in the stream-based model can yield an exponential improvement over the sample complexity of PAC learning (thus proving Theorem 3.5).
13

We then consider learning via membership queries and show that in this setting even very naive algorithms outperform the disagreement methods in the stream-based model (that is, the analyst needs to ask fewer questions to the agent in order to learn his preference than the number of label requests he would need to make using disagreement methods).

5.1 Preliminaries

For notational convenience, δ will to refer to the preference relation in PED with discounting factor δ.
Let µ be a distribution on RT ×RT . µ induces a metric on PED by d( δ, γ) = µ( δ △ γ), and thus we can deﬁne the closed ball of radius R centered at δ by

B( δ, R) = { γ: d( δ, γ ) ≤ R}.

For V ⊆ PEU , the disagreement region of V , Dis(V ) is deﬁned by

Dis(V ) = {(x, y) ∈ RT × RT : ∃ δ, γ∈ V s.t. (x, y) ∈ δ △ γ} =

( δ △ γ)

δ, γ ∈V

Intuitively, Dis(V ) is the collection of points (x, y) such that we can ﬁnd

relations in the current version space that rank x and y diﬀerently.

If δ is the target preference relation, the disagreement coeﬃcient of δ

µ is the quantity

θ = sup µ(Dis(B( δ, R))

R>0

R

two hypothesis with respect to

5.2 Disagreement based active learning
In this subsection, we deﬁne a distribution µ on RT × RT and show that the disagreement coeﬃcient of PED with respect to µ is 2.

Choosing a measure
The main challenge here is that θ depends on the underlying distribution over RT × RT . Since
preferences are polynomial inequalities, the disagreement coeﬃcient seems to lend itself to a
characterization involving polynomials and their roots, which is the motivation for our choice of distribution. For a general distribution µ over RT × RT , it is not clear how to compute the
disagreement coeﬃcient. We show that θ = 2 for a suitably chosen distribution on RT × RT , which is induced by
the Lebesgue measure on (0, 1)T −1. This allows us to work with a measure on sets of roots of
polynomials that arise from the deﬁnition of the preference relations. Let µ∗∗ be a measure on (0, 1)T −1. We interchangeably represent elements of RT as
polynomials P of degree at most T − 1 or as T − 1-tuples of coeﬃcients. Let ∼ be the equivalence relation on RT deﬁned by P ∼ Q ⇐⇒ P = cQ for some constant c, and let RT / ∼ be the resulting quotient space. Let g : (0, 1)T −1 → RT / ∼ be the map taking a tuple of roots to the equivalence class of the polynomials with those roots, and let h : RT × RT → RT / ∼
be the map h(x, y) = [x − y]. We deﬁne the following measures µ∗ and µ on g((0, 1)T −1) ⊂ RT / ∼ and h−1(g((0, 1)T −1)) ⊂
RT × RT respectively.

14

• Deﬁne µ∗ on all sets S ⊂ g((0, 1)T −1) such that {(r1(P ), . . . , rT −1(P )) ∈ (0, 1)T −1 : P ∈ S}
(where r1(P ), . . . rT −1(P ) denote the roots of P ) is µ∗∗-measurable, for which we set µ∗(S) = µ∗∗({(r1(P ), . . . , rT −1(P )) ∈ (0, 1)T −1 : P ∈ S}).

• Deﬁne µ on all sets S ⊂ h−1(g((0, 1)T −1)) such that {[z] ∈ g((0, 1)T −1) : ∃(x, y) ∈ S s.t. z ∼ x − y}
is µ∗-measurable, for which we set µ(S) = µ∗({[z] ∈ g((0, 1)T −1) : ∃(x, y) ∈ S s.t. z ∼ x − y}).

Intuitively, µ∗ is deﬁned only on those polynomials that have all their roots in (0, 1). When
T = 2, this is a desirable property since the analyst is only presented with polynomials that
have some disagreement in (0, 1). He is not presented with meaningless polynomials that are,
for example, always positive on (0, 1) (the analyst has nothing to learn from such polynomials
since such a polynomial will be preferred to 0 for all δ ∈ (0, 1)). For T ≥ 3, this is a more
restrictive property since the analyst is only presented with polynomials that have all T − 1
roots in (0, 1). Let µ∗∗ be the product Lebesgue measure on (0, 1)T −1. Choosing µ∗∗ in this fashion allows
us to neatly characterize B( δ, R). Let X1, . . . , XT −1 be uniform i.i.d. random variables on (0, 1), and let Yδ,γ be the random
variable Yδ,γ = |{i : Xi is between δ and γ}|. Let Eδo,dγd denote the event that Yδ,γ is odd, let Eδk,γ denote the event Yδ,γ = k, and let Eδ≥,γk denote the event Yδ,γ ≥ k.

Lemma 5.1. γ∈ B( δ, R) if and only if P[Eδo,dγd] ≤ R.

Proof. Given (x, y) ∈ RT × RT , let Px−y(X) =

T t=1

X

t−1

· (xt − yt).

Then,

γ∈ B( δ, R) ⇐⇒ µ({(x, y) ∈ h−1(g((0, 1)T −1)) : sgn(Px−y(δ)) = sgn(Px−y(γ)))}) ≤ R ⇐⇒ µ∗({[P ] ∈ g((0, 1)T −1) : sgn(P (δ)) = sgn(P (γ))}) ≤ R

⇐⇒ µ∗∗({(r1, . . . , rT −1) ∈ (0, 1)T −1 : sgn( (δ − ri)) = sgn( (γ − ri))}) ≤ R

But sgn( (δ − ri)) = sgn( (γ − ri)) occurs exactly when an odd number of roots lie between γ and δ (modulo a set of measure 0 since the probability that we have a root of
multiplicity greater than 1 is 0).

While it seems diﬃcult to write down a general characterization of µ, Propositions 5.2 and 5.3 give some basic observations regarding the σ-algebras on which µ∗ and µ are deﬁned. We defer their proofs (along with a description of µ∗ in the case T = 2) to the appendix:
Proposition 5.2. The σ-algebra on g((0, 1)T −1) is the Borel σ-algebra.
The σ-algebra induced on h−1(g((0, 1)T −1)) does not appear to yield a clean characterization, but we can show the weaker statement that µ is a Borel measure, i.e. it is deﬁned on all open sets of h−1(g((0, 1)T −1)).
Proposition 5.3. µ is a Borel measure on h−1(g((0, 1)T −1)).

15

Computing θ

We now show θ = 2 for the distribution µ as chosen above. We use the notation d = dδ,γ :=

|δ − γ| to denote the distance between δ and γ. Let δ be the target preference relation. First, note that since Yδ,γ is distributed according to Bin(T −1, d), P[Eδo,dγd] = 1−(1−22d)T−1 10.
We us this fact to derive an explicit description of the preference relations γ contained in

the ball B( δ, R) in terms of d. We break the analysis up into a few cases.

First,

when

R>

21 ,

we

have

supR> 1

µ(Dis(B( R

δ ,R)))

= 2.

Let

R≤

21 .

By

Lemma

5.1,

2

γ∈ B( δ, R) ⇐⇒ P[Eδo,dγd] = 1 − (1 − 2d)T −1 ≤ R

(1)

2

Suppose d ≤ 12 . Then 1 − 2d and 1 − 2R are both non-negative, so rearranging Equation (1) yields
d ≤ 1 − (1 − 2R)1/(T −1) . (2) 2

Suppose d > 12 , so 1 − 2d < 0. Rearranging Equation (1), we get 1 − 2R ≤ (1 − 2d)T −1. If T − 1 is odd, (1 − 2d)T −1 is negative, so 1 − 2R ≤ (1 − 2d)T −1 does not hold. Thus, when T − 1 is odd the ball consists of γ such that d satisﬁes condition (2). If T − 1 is even, (1 − 2d)T −1
is positive, so we get d ≥ 1 + (1 − 2R)1/(T −1) . (3) 2

Thus, when T − 1 is even the ball consists of γ such that d satisﬁes conditions (2) or (3).

Now, the disagreement region of B( δ, R) consists of all points (x, y) such that the polyno-

mial Px−y (as deﬁned in Lemma 5.1) has a root γ such that γ∈ B( δ, R) (since we can ﬁnd

two hypotheses that disagree on (x, y) by taking a point slightly below γ and a point slightly

above γ

such that Px−y

has no sign changes in between).

Hence,

with R1 =

1−(1−2R)1/(T −1) 2

and R2 = 1+(1−2R2)1/(T −1) , we have that

µ(Dis(B( δ, R))) =

P[Eδ≥−1R1,δ+R1 ] P[Eδ≥−1R1,δ+R1 ∪ E0≥,δ1−R2 ∪ Eδ≥+1R2,1]

if T − 1 is odd if T − 1 is even

We have and

P[Eδ≥−1R1,δ+R1 ] = 1 − (1 − 2R1)T −1 = 2R,

P[Eδ≥−1R1,δ+R1 ∪ E0≥,δ1−R2 ∪ Eδ≥+1R2,1] = 1 − (2(R2 − R1))T −1 = 1 − 2T −1(1 − 2R).

Therefore,

when

T

−1

is

odd

sup0<R≤1/2

µ(Dis(B( R

δ ,R)))

=2

and

when

T

−1

is

even

sup µ(Dis(B( δ, R))) = sup 1 − 2T −1(1 − 2R) = 2,

0<R≤1/2

R

0<R≤1/2

R

10This is due to the general fact that if X is a random variable distributed according to Bin(n, p), the probability that X is odd is 1−(1−2 2p)n .

16

which is achieved at R = 1/2 since 1−2T−R1(1−2R) is increasing on 0 < R ≤ 12 .

Finally,

θ = supR>0

µ(Dis(B( R

δ ,R)))

= 2.

We have thus established Theorem 3.5:

Theorem 3.5. There exists a distribution µ on RT ×RT for which the disagreement coeﬃcient of PED is θ = 2. Thus, for this distribution,

1 ℓCAL(ε) = O log T log ε ,

where the O notation suppresses terms that are logarithmic in log T and log 1/ε.

5.3 Learning via membership queries

In this subsection, we present a simple membership queries algorithm that outperforms the guarantees provided by the disagreement based CAL algorithm. For simplicity, we restrict attention to preference models that are parametrized by a single parameter (e.g. PED and PHD ).
Let g1, . . . , gT : R → R be a collection of functions such that there exist 1 ≤ t1, t2 ≤ T satisfying
1. M := supδ ggtt12((δδ)) is ﬁnite, and
2. The map δ → ggtt12((δδ)) satisﬁes an inverse Lipschitz condition with constant C :
|δ − δ′| ≤ C gt1 (δ) − gt1 (δ′) . gt2 (δ) gt2 (δ′)

Consider the model of preference relations P parametrized by δ such that

T

T

x y if and only if gt(δ)xt ≥ gt(δ)yt.

t=1

t=1

Proposition 3.6. There exists an algorithm that takes as input ε > 0 and using O(log 1/ε) membership queries outputs δh such that |δ−δh| ≤ ε, where δ parametrizes the target preference
relation in P.

Proof. Fix a ρ > 0 and an η-cover of [0, M ρ], where 0 < η ≤ ρCε . Let bρ be the quantity such that the agent is indiﬀerent between receiving a payoﬀ of ρ at time t1 or receiving a payoﬀ of bρ at time t2, i.e. bρ solves
gt2 (δ)bρ = gt1 (δ)ρ.

By running a binary search over the η-cover of [ρ, M ρ], the analyst can ﬁnd an approximation

bhρ to the indiﬀerence point for which |bρ − bhρ| ≤ η (the binary search is performed on the

parameter

bhρ

by

requesting

labels

for

pairs

of

the

form

(ρet1

,

b

h ρ

et2

)).

The analyst

then

outputs

the δh that solves gt2 (δh)bhρ = gt1 (δh)ρ.

We have

|δ − δh| ≤ C gt1 (δ) − gt1 (δh) = C bρ − bhρ ≤ Cη ≤ ε,

gt2 (δ) gt2 (δh)

ρρ ρ

17

as desired. Since M := supδ ggtt21((δδ)) is ﬁnite, bρ ≤ M ρ, so the binary search over the η-cover of [0, M ρ]
terminates.
Remark. Outputting a hypothesis parameter δh that is ε-close to δ is a reasonable measurement for the error of learning via membership queries since there is no underlying distribution providing points to the analyst. However, note that for a distribution on RT × RT , a hypothesis close to the target parameter implies the set of misclassiﬁed points is assigned a small measure, due to continuity of measure.
The main feature of this algorithm is that its query complexity has no dependence on the number of time periods T . Both PED and PHD ﬁt the conditions of Proposition 3.6, and thus we obtain a large improvement over the guarantees provided by disagreement methods in the stream-based model. Such methods assume no extra knowledge about the problem domain and are written to ﬁt a wide class of learning problems. When we are learning economic parameters, membership queries allow us to take advantage of the extra structure present in preference models.
Acknowledgements
We would like to thank Federico Echenique and Adam Wierman for several helpful comments and suggestions.
References
[1] Balcan, M. F., Daniely, A., Mehta, R., Urner, R., & Vazirani, V. V. (2014, December). Learning economic parameters from revealed preferences. In International Conference on Web and Internet Economics (pp. 338-353). Springer, Cham.
[2] Basu, P., & Echenique, F. (2018, February). Learnability and Models of Decision Making under Uncertainty. In Proceedings of the 2018 ACM Conference on Economics and Computation (pp. 53-53). ACM.
[3] Beigman, E., & Vohra, R. (2006, June). Learning from revealed preference. In Proceedings of the 7th ACM Conference on Electronic Commerce (pp. 36-42). ACM.
[4] Berns, G. S., Laibson, D., & Loewenstein, G. (2007). Intertemporal choice – toward an integrative framework. Trends in cognitive sciences, 11(11), 482-488.
[5] Blume, L., Brandenburger, A., & Dekel, E. (1991). Lexicographic probabilities and choice under uncertainty. Econometrica: Journal of the Econometric Society, 61-79.
[6] Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1989). Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4), 929-965.
[7] Chabris, C. F., Laibson, D. I., & Schuldt, J. P. (2010). Intertemporal choice. In Behavioural and Experimental Economics (pp. 168-177). Palgrave Macmillan, London.
[8] Chambers, C. P., & Echenique, F. (2018). On multiple discount rates. Econometrica, 86(4), 1325-1346.
18

[9] Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning. Machine learning, 15(2), 201-221.
[10] Dasgupta, S. (2011). Two faces of active learning. Theoretical computer science, 412(19), 1767-1781.
[11] Echenique, F., Golovin, D., & Wierman, A. (2011, June). A revealed preference approach to computational complexity in economics. In Proceedings of the 12th ACM conference on Electronic commerce (pp. 101-110). ACM.
[12] Grigoriev, D., & Vorobjov, N. (1988). Solving systems of polynomial inequalities in subexponential time. J. Symb. Comput., 5(1/2), 37-64.
[13] Hanneke, S. (2009). Theoretical foundations of active learning (No. CMU-ML-09-106). CARNEGIE-MELLON UNIV PITTSBURGH PA MACHINE LEARNING DEPT.
[14] Hanneke, S. (2016). The optimal sample complexity of PAC learning. The Journal of Machine Learning Research, 17(1), 1319-1333.
[15] Kable, J. W., & Glimcher, P. W. (2007). The neural correlates of subjective value during intertemporal choice. Nature neuroscience, 10(12), 1625.
[16] Kalai, G. (2003). Learnability and rationality of choice. Journal of Economic theory, 113(1), 104-117.
[17] Kleinberg, J., & Oren, S. (2014, June). Time-inconsistent planning: a computational problem in behavioral economics. In Proceedings of the ﬁfteenth ACM conference on Economics and computation (pp. 547-564). ACM.
[18] Koopmans, T. C. (1960). Stationary ordinal utility and impatience. Econometrica: Journal of the Econometric Society, 287-309.
[19] Kreps, D. M. (1988). Notes on the theory of choice (Underground Classics in Economics). Westview Press Incorporated.
[20] Phelps, E. S., & Pollak, R. A. (1968). On second-best national saving and gameequilibrium growth. The Review of Economic Studies, 35(2), 185-199.
[21] Stern, N., Peters, S., Bakhshi, V., Bowen, A., Cameron, C., Catovsky, S., ... & Edmonson, N. (2006). Stern Review: The economics of climate change (Vol. 30, p. 2006). London: HM treasury.
[22] Zadimoghaddam, M., & Roth, A. (2012, December). Eﬃciently learning from revealed preference. In International Workshop on Internet and Network Economics (pp. 114-127). Springer, Berlin, Heidelberg.
19

A Properties of µ and µ∗

Proof of Proposition 5.2. Let B denote the Borel σ-algebra on (0, 1)T −1. Let X = RT / ∼ endowed with the quotient topology, and g∗ : [0, 1]T −1 → X be the
map g∗(r1, . . . , rT −1) = [ (x − ri)]. Explicitly, the terms of g∗(r1, . . . , rT −1) are given by
symmetric sums:





g∗(r1, . . . , rT −1) = c, −c ri, c rirj, . . . , (−1)T −1cr1 · · · rT −1 ,

i

i,j

where c is the appropriate constant for the representative of the equivalence class. Each symmetric sum is a continuous function of T −1 variables, so g∗ is continuous. Moreover, note that g∗ is injective. Then, with Y = g∗([0, 1]T −1), we have that g∗ : [0, 1]T −1 → Y is a continuous bijection from a compact set into a Hausdorﬀ space. Hence, g∗ is a homeomorphism. Then g, which is the restriction of g∗ to (0, 1)T −1 is a homeomorphism onto Z := g((0, 1)T −1). Thus, the σ-algebra g(B) that we obtain on Z is the Borel σ-algebra.
When T = 2, we can give an explicit description of µ∗. Identify R2/ ∼ with the unit circle. Then, a degree 1 polynomial P is identiﬁed with the point (cos θ, sin θ), where P (x) = (cos θ)x + sin θ. Z := g((0, 1)T −1) consists of the boundary of the unit circle for which the argument θ satisﬁes − tan θ ∈ (0, 1). This is satisﬁed precisely for θ ∈ (3π/4, π) ∪ (7π/4, 2π). Hence, if U is a basic open subset of {(cos θ, sin θ) : θ ∈ (3π/4, π) ∪ (7π/4, 2π)}, we can write U = {(cos θ, sin θ) : θ1 < θ < θ2} with θ1, θ2 both in the same segment of the unit circle and
µ∗(U ) = µ∗∗({− tan θ : θ1 < θ < θ2}) = | tan θ1 − tan θ2|.
Proof of Proposition 5.3. Let h : RT × RT → RT / ∼ be the map h(x, y) = [x − y]. Let V ⊆ h−1(g((0, 1)T −1)) be open. We show that

h(V ) = {[z] : ∃(x, y) ∈ V (z = x − y)}

is open. Indeed, let z = x − y for (x, y) ∈ V and choose ε small enough such that the square with vertices {(x + ε, y + ε), (x + ε, y − ε), (x − ε, y + ε), (x − ε, y − ε)} is contained in V . Then, for any λ ≤ ε, [z + λ] = [(x + λ) − y] with (x + λ, y) ∈ V and [z − λ] = [x − (y + λ)] with (x, y + λ) ∈ V , so in particular the open ball with radius λ centered at [z] is contained in h(V ).

20

