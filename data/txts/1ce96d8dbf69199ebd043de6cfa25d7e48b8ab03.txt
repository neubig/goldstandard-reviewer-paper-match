Causal Effects of Linguistic Properties
Reid Pryzant1 Dallas Card1 Dan Jurafsky1 Victor Veitch2 Dhanya Sridhar3 1Stanford Unversity
2University of Chicago 3Columbia University 1{rpryzant,dcard,jurafsky}@stanford.edu 2victorveitch@gmail.com 3ds3778@columbia.edu

arXiv:2010.12919v5 [cs.CL] 14 Jun 2021

Abstract
We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer’s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest—e.g., predictions from classiﬁers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TEXTCAUSE, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales ﬁgures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.
1 Introduction
Social scientists have long been interested in the causal effects of language, studying questions like:
• How should political candidates describe their personal history to appeal to voters (Fong and Grimmer, 2016)?
• How can business owners write product descriptions to increase sales on e-commerce platforms (Pryzant et al., 2017, 2018a)?
• How can consumers word their complaints to receive faster responses (Egami et al., 2018)?

• What conversational strategies can mental health counselors use to have more successful counseling sessions (Zhang et al., 2020)?
To study the causal effects of linguistic properties, we must reason about interventions: what would the response time for a complaint be if we could make that complaint polite while keeping all other properties (topic, sentiment, etc.) ﬁxed? Although it is sometimes feasible to run such experiments where text is manipulated and outcomes are recorded (Grimmer and Fong, 2020), analysts typically have observational data consisting of texts and outcomes obtained without intervention. This paper formalizes the estimation of causal effects of linguistic properties in observational settings.
Estimating causal effects from observational data requires addressing two challenges. First, we need to formalize the causal effect of interest by specifying the hypothetical intervention to which it corresponds. The ﬁrst contribution of this paper is articulating the causal effects of linguistic properties; we imagine intervening on the writer of a text document and telling them to use different linguistic properties.
The second challenge of causal inference is identiﬁcation: we need to express causal quantities in terms of variables we can observe. Often, instead of the true linguistic property of interest we have access to a noisy measurement called the proxy label. Analysts typically infer these values from text with classiﬁers, lexicons, or topic models (Grimmer and Stewart, 2013; Lucas et al., 2015; Prabhakaran et al., 2016; Voigt et al., 2017; Luo et al., 2019; Lucy et al., 2020). The second contribution of this paper is establishing the assumptions we need to recover the true effects of a latent linguistic property from these noisy proxy labels. In particular, we propose an adjustment for the confounding information in a text document and prove that this bounds the bias of the resulting estimates.
The third contribution of this paper is practical:

an algorithm for estimating the causal effects of linguistic properties. The algorithm uses distantly supervised label propagation to improve the proxy label (Zhur and Ghahramani, 2002; Mintz et al., 2009; Hamilton et al., 2016), then BERT to adjust for the bias due to text (Devlin et al., 2018; Veitch et al., 2020). We demonstrate the method’s accuracy with partially-simulated Amazon reviews and sales data, perform a sensitivity analysis in situations where assumptions are violated, and show an application to consumer ﬁnance complaints. Data and a package for performing textbased causal inferences is available at https:// github.com/rpryzant/causal-text.
2 Causal Inference Background
Causal inference from observational data is wellstudied (Pearl, 2009; Rosenbaum and Rubin, 1983, 1984; Shalizi, 2013). In this setting, analysts are interested in the effect of a treatment T (e.g., a drug) on an outcome Y (e.g., disease progression). For ease, we consider binary treatments. The average treatment effect (ATE) on the outcome Y is,
ψ = E [Y ; do(T = 1)] − E [Y ; do(T = 0)] , (1)
where the operation do(T = t) means that we hypothetically intervene and set the treatment T to some value (Pearl, 2009).
Typically, the ATE ψ is not the simple difference in average conditional outcomes, E [Y | T = 1] − E [Y | T = 0]. This is because confounding variables C are associated with both the treatment and outcome, inducing non-causal associations between them, referred to as open backdoor paths (Pearl, 2009). When all the confounding variables are observed, we can write the ATE in terms of observed variables using the backdoor-adjustment formula (Pearl, 2009),
ψ = EC E [Y | T = 1, C] − E [Y | T = 0, C] .
(2)
For example, if the confounding variable C is discrete, we group the data into values of C, calculate the average difference in outcomes between the treated and untreated samples of each group, and take the average over groups.
3 Causal Effects of Linguistic Properties
We are interested in the causal effects of linguistic properties. To formalize this as a treatment, we

Figure 1: The proposed causal model of text and outcomes. A writer uses linguistic property T and other properties Z, which may be correlated (denoted by bidirected arrow), to write the text W . From the text, the reader perceives the property of interest, captured by T˜, and together with other perceived information Z˜, produces the outcome Y . The proxy label of the property obtained via a classiﬁer or lexicon is captured by Tˆ.
imagine intervening on the writer of a text, e.g., telling people to write with a property (or not). We show that to estimate the effect of using a linguistic property, we must consider how a reader of the text perceives the property. These dual perspectives of the reader and writer are well studied in linguistics and NLP;1 we adapt the idea for causal inference.
Figure 1 illustrates a causal model of the setting. Let W be a text document and let T (binary) be whether or not a writer uses a particular linguistic property of interest.2 For example, in consumer complaints, the variable T can indicate whether the writer intends to be polite or not. The outcome is a variable Y , e.g., how long it took for this complaint to be serviced. Let Z be other linguistic properties that the writer communicated (consciously or unconsciously) via the text W , e.g. topic, brevity or sentiment. The linguistic properties T and Z are typically correlated, and both variables affect the outcome Y .
1Literary theory argues that language is subject to two perspectives: the “artistic” pole – the text as intended by the author – and the “aesthetic” pole – the text as interpreted by the reader (Iser, 1974, 1979). The noisy channel model (Yuret and Yatbaz, 2010; Gibson et al., 2013) connects these poles by supposing that the reader perceives a noisy version of the author’s intent. This duality has also been modeled in linguistic pragmatics as the difference between speaker meaning and literal or utterance meaning (Potts, 2009; Levinson, 1995, 2000). Gricean pragmatic models like RSA (Goodman and Frank, 2016) similarly formalize this as the reader using the literal meaning to help make inferences about the speaker’s intent.
2We leave higher-dimensional extensions to future work.

We are interested in the average treatment effect,
ψwri. = E [Y ; do(T = 1)] − E [Y ; do(T = 0)] , (3)
where we imagine intervening on writers and telling them to use the linguistic property of interest (setting T = 1, “write politely”) or not (T = 0). This causal effect is appealing because the hypothetical intervention is well-deﬁned – it corresponds to an intervention we could perform in theory. However, without further assumptions, ψwri. is not identiﬁed from the observational data. The reason is that we would need to adjust for the unobserved linguistic properties Z, which create open backdoor paths because they are correlated with both the treatment T and outcome Y (Figure 1).
To solve this problem, we observe that the reader is the one who produces outcomes. Readers use the text W to perceive a value for the property of interest (captured by the variable T˜) as well as other properties (captured by Z˜) then produce the outcome Y based on these perceived values. For example, a customer service representative reads a consumer complaint, judges whether (among other things) the complaint is polite or not, and chooses how quickly to respond based on this.
Consider the average treatment effect,
ψrea. = E Y ; do(T˜ = 1) − E Y ; do(T˜ = 0) ,
(4)
where we imagine intervening on the reader’s perception of a linguistic property T˜. The following result shows that we can identify the causal effect of interest, ψwri., by exploiting this ATE ψrea.. Theorem 1. Let Z˜ = f (W ) be a function of the words W such that E [Y | W ] = E Y | T˜, Z˜ . Suppose that the following assumptions hold:
1. (no unobserved confounding) W blocks backdoor paths between T˜ and Y ,
2. (agreement of intent and perception) T = T˜.
3. (overlap) For some constant > 0,
< P (T˜ = 1 | Z˜) < 1 −
with probability 1.3
3Informally, it must be possible to perceive a property (T˜=1) for all settings of Z˜, and Z˜ cannot perfectly predict T˜.

Then the ATE ψrea. is identiﬁed as,
ψrea. =EW E Y | T˜ = 1, Z˜ = f (W ) − (5)
E Y | T˜ = 0, Z˜ = f (W ) . (6)
Moreover, the ATE ψrea. is equal to ψwri..
The proof is in Appendix A. Intuitively, the result says that the information in the text W that the reader uses to determine the outcome Y splits into two parts: the information the reader uses to perceive the linguistic property of interest (T˜), and the information used to perceive other properties (Z˜ = f (W )). The information captured by the variable Z˜ is confounding; it affects the outcome and is also correlated with the treatment T˜. Under certain assumptions, adjusting for the function of text Z˜ that captures confounding sufﬁces to identify the ψrea.; in Figure 1, the backdoor path T˜ → W → Z˜ → Y is blocked.4 Moreover, if we assume that readers correctly perceive the writer’s intent, the effect ψrea., which can be expressed in terms of observed variables, is equivalent to the effect that we want, ψwri..
4 Substituting Proxy Labels
If we observed T˜, the reader’s perception of the linguistic property of interest, then we could proceed by estimating the effect ψrea. (equivalently, ψwri.). However, in most settings, one does not observe the linguistic properties that a writer intends to use (T and Z) or that a reader perceives (T˜ and the information in Z˜). Instead, one uses a classiﬁer or lexicon to predict values for this property from the text, producing a proxy label Tˆ (e.g. predicted politeness).
For this setting, where we only have access to proxy labels, we introduce the estimand ψproxy which substitutes the proxy Tˆ for the unobserved treatment T˜ in the effect ψrea.:
ψproxy =EW E Y | Tˆ = 1, Z˜ = f (W ) (7)
− E Y | Tˆ = 0, Z˜ = f (W ) .
(8)
4Grimmer and Fong (2020) studied a closely related setting where text documents are randomly assigned to readers who produce outcomes. From this experiment, they discover text properties that cause the outcome. Their causal identiﬁcation result requires an exclusion restriction assumption, which is related to the no unobserved confounding assumption that we make.

This estimand only requires an adjustment for the confounding information Z˜. We show how to extract this information using pretrained language models in Section 5. Prior work on causal inference with proxy treatments (Wood-Doughty et al., 2018) requires an adjustment using the measurement model P (T˜ | Tˆ), i.e. the true relationship between the proxy label Tˆ and its target T˜, which is typically unobserved. In contrast, the estimand ψproxy does not require the measurement model.
The following result shows that the estimand ψproxy only attenuates the ATE that we want, ψrea.. That is, the bias due to proxy treatments is benign; it can only decrease the magnitude of the effect but it does not change the sign.
Theorem 2. Let 0 = Pr(T˜ = 0 | Tˆ = 1, Z˜) and let 1 = Pr(T˜ = 1 | Tˆ = 0, Z˜). Then,
ψproxy =ψrea. − EW E[Y | T˜ = 1, Z˜]
− E[Y | T˜ = 0, Z˜] 0 + 1
The proof is in Appendix E. This result shows that the proposed estimand ψproxy, which we can estimate, is equal to the ATE ψrea. that we want, minus a bias term related to measurement error. In particular, if the classiﬁer is better than chance and the treatment effect sign is homogeneous across possible texts — i.e., it always helps or always hurts, an assumption the analyst must carefully assess — then the bias term is positive with the degree of attenuation dependent on the error rate of the proxy label Tˆ. The result tells us to construct the most accurate proxy treatment Tˆ possible, so long as we adjust for the confounding part of the text.5 This is a novel result for causal inference with proxy treatments and sidesteps the need for the measurement model.
5 TEXTCAUSE , A Causal Estimation Procedure
We introduce a practical algorithm for estimating the causal effects of linguistic properties. Motivated by Theorem 2, we ﬁrst describe an approach for improving the accuracy of proxy labels. We then use the improved proxy labels, text and outcomes to ﬁt a model that extracts and adjusts for the confounding information in the text Z˜. In practice,
5We prove in Appendix F that without the adjustment for confounding information Z˜, estimates of the ATE ψrea. will be arbitrarily biased.

one may observe additional covariates C that capture confounding properties, e.g., the product that a review is about or complaint type. We will include these covariates in the estimation algorithm.

5.1 Improved Proxy Labels

The ﬁrst stage of TEXTCAUSE is motivated by Theorem 2, which said that a more accurate proxy can yield lower estimation bias. Accordingly, this stage uses distant supervision to improve the ﬁdelity of lexicon-based proxy labels Tˆ. In particular, we exploit an inductive bias of frequently used lexiconbased proxy treatments: the words in a lexicon correctly capture the linguistic property of interest (i.e., high precision, Tausczik and Pennebaker, 2010), but can omit words and discourse-level elements that also map to the desired property (i.e., low recall, Kim and Hovy, 2006; Rao and Ravichandran, 2009).
Motivated by work on lexicon induction and label propagation (Hamilton et al., 2016; An et al., 2018), we improve the recall of proxy labels, training a classiﬁer Pθ to predict the proxy label Tˆ, then using that classiﬁer to relabel examples which were labeled Tˆ = 0 but look like T˜ = 1. Formally, given a dataset of tuples {(Yi, Wi, Ci, Tˆi)}ni=1 the algorithm is:

1. Train a classiﬁer to predict Pθ(Tˆ | W ), e.g.,
logistic regression trained with bag-of-words features and Tˆ labels.

2. Relabel some Tˆ = 0 examples (we experi-

ment with ablating this in Appendix C):

Tˆi∗ =

1
1[Pθ(Tˆi = 1|Wi) > 0.5]

if Tˆi = 1 otherwise

3. Use Tˆ∗ as the new proxy treatment variable.

5.2 Adjusting for Text
The second stage of TEXTCAUSE estimates the effect ψproxy using the text W , improved proxy labels Tˆ∗, and outcomes Y . This stage is motivated by Theorem 1, which described how to adjust for the confounding parts of the text. We approximate this confounding information in the text, Z˜ = f (W ), with a learned representation b(W ) that predicts the expected outcomes E[Y | Tˆ∗ = t, b(W ), C] for t = 0, 1 (Eq. 7).
We use DistilBERT (Sanh et al., 2019) to produce a representation of the text b(W ) by embedding the text then selecting the vector corresponding to a prepended [CLS] token. We proceed

Figure 2: The second stage of TEXTCAUSE adapts word embeddings to predict both of Y ’s potential outcomes.

to optimize the model so that the representation
b(W ) directly approximates the confounding information Z˜ = f (w). In particular, we train an
estimator for the expected conditional outcome Q(t, b(W ), C) = E[Y | Tˆ∗ = t, b(W ), C]:

Qˆ(t, b(W ), C) = σ(Mbt b(W ) + Mct c + b)),

where the vector c is a one-hot encoding of the covariates C, the vectors Mbt ∈ R768 and Mct ∈ R|C| are learned, one for each value t of the treatment, and the scalar b is a bias term.
Letting θ be all parameters of the model, our
training objective is to minimize,

n
min L(Yi, Qˆθ(Tˆi∗, b(Wi), Ci)) + α · R(Wi),
θ i=1

where L(·) is the cross-entropy loss and R(·) is
the original BERT masked language modeling ob-
jective, which we include following Veitch et al. (2020). The hyperparameter α is a penalty for the
masked language modeling objective. The parameters Mt are updated on examples where Tˆi∗ = t.
Once Qˆ(·) is ﬁtted, an estimator ψˆproxy for the effect ψproxy (Eq. 7) is,

ψˆproxy = 1 ni

Qˆ(1, b(Wi), Ci)

− Qˆ(0, b(Wi), Ci) , (9)

where we approximate the outer expectation over
the text W with a sample average. Intuitively, this
procedure works because the representation b(W ) extracts the confounding information Z˜ = f (W );
it explains the outcome Y as well as possible given the proxy label Tˆ∗.

6 Experiments
We evaluate the proposed algorithm’s ability to recover causal effects of linguistic properties. Since ground-truth causal effects are unavailable without randomized controlled trials, we produce a semisynthetic dataset based on Amazon reviews where only the outcomes are simulated. We also conduct an applied study using real-world complaints and bureaucratic response times. Our key ﬁndings are
• More accurate proxies combined with text adjustment leads to more accurate ATE estimates.
• Naive proxy-based procedures signiﬁcantly underestimate true causal effects.
• ATE estimates can lose ﬁdelity when the proxy is less than 80% accurate.
6.1 Amazon Reviews
6.1.1 Experimental Setup
Dataset. Here we use real world and publicly available Amazon review data to answer the question, “how much does a positive product review affect sales?” We create a scenario where positive reviews increase sales, but this effect is confounded by the type of product. Speciﬁcally:
• The text W is a publicly available corpus of Amazon reviews for digital music products (Ni et al., 2019). For simplicity, we only include reviews for mp3, CD, or Vinyl. We also exclude reviews for products worth more than $100 or fewer than 5 words.
• The observed covariate C is a binary indicator for whether the associated review is a CD or not, and we use this to simulate a confounded outcome.
• The treatment T = T˜ is whether that review is positive (5 stars) or not (1 or 2 stars). Hence,

we omit reviews with 3 or 4 stars. Note that here it is reasonable to assume writer’s intention (T ) equals the reader’s perception (T˜), as the author is deliberately communicating their sentiment (or a very close proxy) with the stars. We use this variable to (1) simulate outcomes and (2) calculate ground truth causal effects for evaluation. • The proxy treatment Tˆ is computed via two strategies: (1) a randomly noised version of T ﬁxed to 93% accuracy (to resemble a reasonable classiﬁer’s output, later called “proxy-noised”), and (2) a binary indicator for whether any words in W overlap with a positive sentiment lexicon (Liu et al., 2010). • The outcome Y ∼ Bernoulli(σ(βc(π(C) − βo) + βtT˜ + N(0, γ))) represents whether a product received a click or not. The parameter βc controls confound strength, βt controls treatment strength, βo is an offset and the propensity π(C) = P (T = 1|C) is estimated from data.
The ﬁnal data set consists of 17,000 examples. Protocol. All nonlinear models were implemented using PyTorch (Paszke et al., 2019). We use the transformers6 implementation of DistillBERT and the distilbert-base-uncased model, which has 66M parameters. To this we added 3,080 parameters for text adjustment (the Mbt and Mct vectors). Models were trained in a cross-validated fashion, with the data being split into 12,000, 2,000, and 4,000-example train, validation, and test sets.7 BERT was optimized for 3 epochs on each fold using Adam (Kingma and Ba, 2014), a learning rate of 2e−5, and a batch size of 32. The weighting on the potential outcome and masked language modeling heads was 0.1 and 1.0, respectively. Linear models were implemented with sklearn. For T-boosting, we used a vocab size of 2,000 and L2 regularization with a strength of c = 1e−4. Each experiment was replicated using 100 different random seeds for robustness. Each trial took an average of 32 minutes with three 1.2 GHz CPU cores and one TITAN X GPU. Baselines. The “unadjusted” baseline is ψˆnaive = Eˆ[Y |Tˆ = 1] − Eˆ[Y |Tˆ = 0], the expected difference in outcomes conditioned on Tˆ.8 The
6https://huggingface.co/transformers 7See Egami et al. (2018) for an investigation into train/test splits for text-based causal inference. 8See Appendix F for an investigation into this estimator.

proxy-* baselines perform backdoor adjustment for the observed covariate C and are based on Sridhar and Getoor (2019): ψˆnaive+C = |C1 | c(Eˆ[Y |Tˆ = 1, C = c] − Eˆ[Y |Tˆ = 0, C = c]), using randomly drawn and lexicon-based Tˆ proxies. We also compare against “semi-oracle”, ψˆmatrix, an estimator which assumes additional access to the ground truth measurement model P (Tˆ | T ) (WoodDoughty et al., 2018); see Appendix G for derivation.
Note that for clarity, we henceforth refer to the treatment-boosting and text-adjusting stages of TEXTCAUSE as T-boost and W-Adjust .
6.1.2 Results
Our primary results are summarized in Table 1. Individually, T-boost and W-Adjust perform well, generating estimates which are closer to the oracle than the naive “unadjusted” and “proxy-lex’ baselines. However, these components fail to outperform the highly accurate “proxy-noised” baseline unless they are combined (i.e., the TEXTCAUSE algorithm). Only the full T extCause algorithm consistently outperformed (i.e. produced higher quality ATE estimates) than the baselines. This result is robust to varying levels of noise and treatment/confound strength. Indeed TEXTCAUSE ’s estimates were on average within 2% of the semioracle. Furthermore, these results support Theorem 2: methods which adjusted for the text always attenuated the true ATE.
Our results suggest that adjusting for the confounding parts of text can be crucial: estimators that adjust for the covariates C but not the text perform poorly, sometimes even worse than the unadjusted estimator ψˆnaive.
Does it always help to adjust for the text? We consider the case where confounding information in the text causes a naive estimator which does not adjust for this information (ψnaive) to have the opposite sign of the true effect ψ. Does our proposed text adjustment help in this situation? Theorem 2 says it should, because ψproxy estimates are bounded in [0, ψ]. This ensures that the most important of bits, the bit of directional information, is preserved.
Table 2 shows results from such a scenario. We see that the true ATE of T , ψ, has a strong negative effect, while the naive estimator ψnaive+C produces a positive effect. Adding an adjustment for the confounding parts of the text with TEXTCAUSE

Noise:
Treatment:
Confounding:
oracle (ψ) semi-oracle (ψˆmatrix) unadjusted (ψˆnaive) proxy-lex (ψˆnaive+C) proxy-noised (ψˆnaive+C) +T-boost (ψˆnaive+C) +W-Adjust (ψˆproxy)
+T-boost +W-Adjust (TEXTCAUSE , ψˆproxy)

Low

Low

High

Low High Low High

9.92 10.03 18.98 19.30

9.73 9.82 18.77 19.08

6.84 7.66 13.53 14.50

6.67 6.73 12.88 13.09

8.25 8.27 15.90 16.12

8.11 8.16 15.53 15.73

7.82 8.57 14.96 16.13 9.42 10.27 18.20 19.32

High

Low

High

Low High Low High

8.28 8.28 16.04 16.19

8.25 8.28 16.02 16.21

5.79 6.42 11.51 12.26

5.65 5.67 10.98 11.12

6.69 6.72 13.22 13.33

6.78 6.80 13.19 13.32

6.62 7.22 12.95 13.76 7.85 8.53 15.45 16.30

Mean delta from oracle
0.0 0.13 3.58 4.43 2.35 2.51 2.39 0.37

Table 1: ATE estimates: expected change in click probabilities if one were to manipulate the sentiment of a review from negative to positive. TEXTCAUSE performs best in most settings. The true ATE is given in the top row (“oracle”). Estimates closer to the oracle are better. The last column gives the average difference between the estimated and true ATEs; lower is better. Rows 3-6 are baselines. Rows 7-9 are proposed. The second row and the bottom three rows use lexicon-based proxy treatments (we observed similar results using other proxy treatments). All columns have βo = 0.9. Low and high noise corresponds to γ = 0 and 1. Low and high treatment corresponds to βt = 0.4, 0.8. Low and high confounding corresponds to βc = -0.4, 4.0. All standard errors are less than 0.5.

Estimator
oracle (ψ) proxy-lex (ψˆnaive+C) +T-boost (ψˆnaive+C) +T-boost +W-Adjust (TEXTCAUSE , ψˆproxy)

ATE
-14.99 6.29 4.18 0.50

SE
± 0.1 ± 0.3 ± 0.5 ± 1.3

Table 2: Estimator performance in a worst-case scenario where the estimated ATE of Tˆ and Tˆ∗ indicates
the opposite sign of the true ATE of T (βc = 0.8, βt = −1, π(C) = 0.8, βo = 0.6).

successfully brings the proxy-based estimate to 0, which is indicative of the bounded behavior that Theorem 2 suggests.
Sensitivity analysis. In Figure 3 we synthetically vary the accuracy of a proxy Tˆ by dropping random subsets of the data. This is to evaluate the robustness of various estimation procedures. We would expect (1) methods that do not adjust for the text to behave unpredictably, and (2) methods that do adjust for the text to be more robust.
These results support our ﬁrst hypothesis: boosting treatment labels without text adjustment can behave unpredictably, as proxy-lex and T-boost both overestimate the true ATE. In other words, the predictions of both estimators grow further from the oracle as Tˆ’s accuracy increases.
The results are mixed with respect to our second hypothesis. Both methods which adjust for

Figure 3: ATE estimates as the accuracy of Tˆ is varied. Without text adjustment, T-boost ’s errors can increase with the error rate of Tˆ. The dotted black lines correspond to the true ATE (left) and 0 error (right).
the text (W-Adjust and TEXTCAUSE ) consistently attenuate the true ATE, which is in line with Theorem 2. However, we ﬁnd that TEXTCAUSE , which makes use of T-boost and W-Adjust , may not always provide the highest quality ATE estimates in ﬁnite data regimes. Notably, when Tˆ is less than 90% accurate, both proxy-lex and T-boost can produce higher-quality estimates than the proposed TEXTCAUSE algorithm.
Note that all estimates quickly lose ﬁdelity as the proxy Tˆ becomes noisier. It rapidly becomes difﬁcult for any method to recover the true ATE

when the proxy Tˆ is less than 80% accurate.
6.2 Application: Complaints to the Financial Protection Bureau
We proceed to offer an applied pilot study which seeks to answer, “how does the perceived politeness of a complaint affect the time it takes for that complaint to be addressed?” We consider complaints ﬁled with the Consumer Financial Protection Bureau (CFPB).9 This is a government agency which solicits and handles complaints about ﬁnancial products. When they receive a complaint it is forwarded to the relevant company. The time it takes for that company to process the complaint is recorded. Some submissions are handled quickly (< 15 days) while others languish. This 15-day threshold is our outcome Y . We additionally adjust for an observed covariate C that captures what product and company the complaint is about (mortgage or bank account). To reduce other potentially confounding effects, we pair each Y = 1 complaint with the most similar Y = 0 complaint according to cosine similarity of TF-IDF vectors (Mozer et al., 2020). From this we select the 4,000 most similar pairs for a total of 8,000 complaints.
For our treatment (politeness), we use a state-ofthe-art politeness detection package geared towards social scientists (Yeomans et al., 2018). This package reports a score from a trained classiﬁer using expert features of politeness and a hand-labeled dataset. We take examples in the top and bottom 25% of the scoring distribution to be our Tˆ = 1 and Tˆ = 0 examples and throw out all others. The ﬁnal dataset consists of 4,000 complaints, topics, and outcomes.
We use the same training procedure and hyperparameters as Section 6.1, except now W-Adjust is trained for 9 epochs and each cross validation fold is of size 2,000.
Results are given in Figure 3 and suggest that perceived politeness may have an effect on reducing response time. We ﬁnd that the effect size increases as we adjust for increasing amounts of information. The “unadjusted” approach which does not perform any adjustment produces the smallest ATE. “proxy-lex”, which only adjusts for covariates, indicated the second-smallest ATE. The W-Adjust and TEXTCAUSE methods, which adjust for covariates and text, produced the largest ATE
9https://www.consumer-action.org/ downloads/english/cfpb_full_dbase_report. pdf/

Estimator
unadjusted (ψˆnaive) proxy-lex (ψˆnaive+C) +T-boost (ψˆnaive+C) +W-Adjust (ψˆproxy)
+T-boost +W-Adjust TEXTCAUSE , (ψˆproxy )

ATE
3.01 4.03 9.64 6.30 10.30

SE
± 0.3 ± 0.4 ± 0.5 ± 1.6 ± 2.1

Table 3: Effect size can vary across estimation methods, with methods that adjust for more information producing larger ATEs. Each number represents the expected percent change in the likelihood of getting a timely response when the politeness of a complaint is hypothetically increased.

estimates. This suggests that there is a signiﬁcant amount of confounding in real world studies, and the choice of estimator can yield highly varying conclusions.
7 Related Work
Our focus ﬁts into a body of work on text-based causal inference that includes text as treatments (Egami et al., 2018; Fong and Grimmer, 2016; Grimmer and Fong, 2020; Wood-Doughty et al., 2018), text as outcomes (Egami et al., 2018), and text as confounders (Roberts et al. (2020); Veitch et al. (2020); see Keith et al. (2020) for a review of that space). We build on Veitch et al. (2020), which proposed a BERT-based text adjustment method similar to our W-Adjust algorithm. This paper is related to work by Grimmer and Fong (2020), which discusses assumptions needed to estimate causal effects of text-based treatments in randomized controlled trials. There is also work on discovering causal structure in text, as topics with latent variable models (Fong and Grimmer, 2016) and as words and n-grams with adversarial learning (Pryzant et al., 2018b) and residualization (Pryzant et al., 2018a). There is also a growing body of applications in the social sciences (Hall, 2017; Olteanu et al., 2017; Saha et al., 2019; Mozer et al., 2020; Karell and Freedman, 2019; Sobolev, 2019; Zhang et al., 2020).
This paper also ﬁts into a long-standing body of work on measurement error and causal inference (Pearl, 2012; Kuroki and Pearl, 2014; Buonaccorsi, 2010; Carroll et al., 2006; Shu and Yi, 2019; Oktay et al., 2019; Wood-Doughty et al., 2018). Most of this work deals with proxies for confounding variables. The present paper is most closely related to Wood-Doughty et al. (2018), which also

deals with proxy treatments, but instead proposes an adjustment using the measurement model.
8 Conclusion
This paper addressed a setting of interest to NLP and social science researchers: estimating the causal effects of latent linguistic properties from observational data. We clariﬁed critical ambiguities in the problem, showed how causal effects can be interpreted, presented a method, and demonstrated how it offers practical and theoretical advantages over the existing practice. We also release a package for performing text-based causal inferences.10 This work opens new avenues for further conceptual, methodological, and theoretical reﬁnement. This includes improving non-lexicon based treatments, heterogeneous effects, overlap violations, counterfactual inference, ethical considerations, extensions to higher-dimensional outcomes and covariates, and benchmark datasets based on paired randomized controlled trials and observational studies.
9 Acknowledgements
This project recieved partial funding from the Stanford Data Science Institute, NSF Award IIS1514268 and a Google Faculty Research Award. We thank Justin Grimmer, Stefan Wager, Percy Liang, Tatsunori Hashimoto, Zach Wood-Doughty, Katherine Keith, the Stanford NLP Group, and our anonymous reviewers for their thoughtful comments and suggestions.
References
Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. 2018. Semaxis: A lightweight framework to characterize domain-speciﬁc word semantics beyond sentiment. In Proceedings of ACL.
John P Buonaccorsi. 2010. Measurement error: models, methods, and applications. CRC press.
Raymond J Carroll, David Ruppert, Leonard A Stefanski, and Ciprian M Crainiceanu. 2006. Measurement error in nonlinear models: a modern perspective. CRC press.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
10https://github.com/rpryzant/ causal-text

Naoki Egami, Christian J Fong, Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. 2018. How to make causal inferences using texts. arXiv preprint arXiv:1802.02163.
Christian Fong and Justin Grimmer. 2016. Discovery of treatments from text corpora. In Proceedings of ACL, pages 1600–1609.
Edward Gibson, Leon Bergen, and Steven T Piantadosi. 2013. Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. Proceedings of the National Academy of Sciences, 110(20):8051–8056.
Gene Glass and Kenneth Hopkins. 1996. Statistical methods in education and psychology. Psyccritiques, 41(12).
Noah D Goodman and Michael C Frank. 2016. Pragmatic language interpretation as probabilistic inference. Trends in cognitive sciences, 20(11):818–829.
Justin Grimmer and Christian Fong. 2020. Causal inference with latent treatments. In Unpublished.
Justin Grimmer and Brandon M Stewart. 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political analysis, 21(3):267–297.
Allie Hall. 2017. How hiring language reinforces pink collar jobs.
William L Hamilton, Kevin Clark, Jure Leskovec, and Dan Jurafsky. 2016. Inducing domain-speciﬁc sentiment lexicons from unlabeled corpora. In Proceedings of EMNLP.
Wolfgang Iser. 1974. The implied reader: Patterns of communication in prose ﬁction from Bunyan to Beckett. Johns Hopkins University Press.
Wolfgang Iser. 1979. The act of reading: A theory of aesthetic response. Johns Hopkins University Press.
Daniel Karell and Michael Freedman. 2019. Rhetorics of radicalism. American Sociological Review, 84(4):726–753.
Katherine Keith, David Jenson, and Brendan O’Connor. 2020. Text and causal inference: A review of using text to remove confounding from causal estimates. In Proceedings of ACL.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and analyzing judgment opinions. In Proceedings of NAACL.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceedings of ICLR.
Manabu Kuroki and Judea Pearl. 2014. Measurement bias and effect restoration in causal inference. Biometrika, 101(2):423–437.

Stephen C. Levinson. 1995. Three levels of meaning. In Frank R. Palmer, editor, Grammar and Meaning: Essays in Honor of Sir John Lyons, pages 90–115. Cambridge University Press.
Stephen C. Levinson. 2000. Presumptive Meanings: The Theory of Generalized Conversational Implicature. MIT Press, Cambridge, MA.
Bing Liu et al. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing, 2(2010):627–666.
Christopher Lucas, Richard A Nielsen, Margaret E Roberts, Brandon M Stewart, Alex Storer, and Dustin Tingley. 2015. Computer-assisted text analysis for comparative politics. Political Analysis, 23(2):254–277.
Li Lucy, Dorottya Demszky, Patricia Bromley, and Dan Jurafsky. 2020. Content analysis of textbooks via natural language processing: Findings on gender, race, and ethnicity in Texas U.S. history textbooks. AERA Open, 6(3).
Yiwei Luo, Dan Jurafsky, and Beth Levin. 2019. From insanely jealous to insanely delicious: Computational models for the semantic bleaching of english intensiﬁers. In Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of ACL, pages 1003–1011.
Reagan Mozer, Luke Miratrix, Aaron Russell Kaufman, and L Jason Anastasopoulos. 2020. Matching with text data: An experimental evaluation of methods for matching documents and of measuring match quality. Political Analysis, 28(4):445–468.
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and ﬁne-grained aspects. In Proceedings of EMNLP, pages 188–197.
Hüseyin Oktay, Akanksha Atrey, and David Jensen. 2019. Identifying when effect restoration will improve estimates of causal effect. In Proceedings of the 2019 SIAM International Conference on Data Mining, pages 190–198. SIAM.
Alexandra Olteanu, Onur Varol, and Emre Kiciman. 2017. Distilling the outcomes of personal experiences: A propensity-scored analysis of social media. In Proceedings of CSCW.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch:

An imperative style, high-performance deep learning library. In Proceedings of NeurIPS.
Judea Pearl. 2009. Causality. Cambridge university press.
Judea Pearl. 2012. On measurement bias in causal inference. Proceedings of UAI.
Christopher Potts. 2009. Formal pragmatics. The Routledge Encyclopedia of Pragmatics.
Vinodkumar Prabhakaran, William L Hamilton, Dan McFarland, and Dan Jurafsky. 2016. Predicting the rise and fall of scientiﬁc topics from trends in their rhetorical framing. In Proceedings of ACL, pages 1170–1180.
Reid Pryzant, Sugato Basu, and Kazoo Sone. 2018a. Interpretable neural architectures for attributing an ad’s performance to its writing style. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.
Reid Pryzant, Youngjoo Chung, and Dan Jurafsky. 2017. Predicting sales from the language of product descriptions. In Proceedings of the SIGIR Workshop on eCommerce.
Reid Pryzant, Kelly Shen, Dan Jurafsky, and Stefan Wagner. 2018b. Deconfounded lexicon induction for interpretable social science. In Proceedings of NAACL, pages 1615–1625.
Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of EACL.
Margaret E Roberts, Brandon M Stewart, and Richard A Nielsen. 2020. Adjusting for confounding with text matching. American Journal of Political Science, 64(4):887–903.
Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41–55.
Paul R Rosenbaum and Donald B Rubin. 1984. Reducing bias in observational studies using subclassiﬁcation on the propensity score. Journal of the American statistical Association, 79(387):516–524.
Koustuv Saha, Benjamin Sugar, John Torous, Bruno Abrahao, Emre Kıcıman, and Munmun De Choudhury. 2019. A social media study on the effects of psychiatric medication use. In Proceedings of the International AAAI Conference on Web and Social Media.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.
Cosma Shalizi. 2013. Advanced data analysis from an elementary point of view.

Di Shu and Grace Y Yi. 2019. Weighted causal inference methods with mismeasured covariates and misclassiﬁed outcomes. Statistics in medicine, 38(10):1835–1854.
Anton Sobolev. 2019. How pro-government “trolls” inﬂuence online conversations in russia. Unpublished.
Dhanya Sridhar and Lise Getoor. 2019. Estimating causal effects of tone in online debates. Proceedings of IJCAI.
Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: LIWC and computerized text analysis methods. Journal of language and social psychology, 29(1):24–54.
Victor Veitch, Dhanya Sridhar, and David Blei. 2020. Adapting text embeddings for causal inference. In Proceedings of UAI.
Rob Voigt, Nicholas P Camp, Vinodkumar Prabhakaran, William L Hamilton, Rebecca C Hetey, Camilla M Grifﬁths, David Jurgens, Dan Jurafsky, and Jennifer L Eberhardt. 2017. Language from police body camera footage shows racial disparities in ofﬁcer respect. Proceedings of the National Academy of Sciences, 114(25):6521–6526.
Zach Wood-Doughty, Ilya Shpitser, and Mark Dredze. 2018. Challenges of using text classiﬁers for causal inference. In Proceedings of EMNLP.
Michael Yeomans, Alejandro Kantor, and Dustin Tingley. 2018. The politeness package: Detecting politeness in natural language. R Journal, 10(2).
Deniz Yuret and Mehmet Ali Yatbaz. 2010. The noisy channel model for unsupervised word sense disambiguation. Computational Linguistics, 36(1):111– 127.
Justine Zhang, Sendhil Mullainathan, and Cristian Danescu-Niculescu-Mizil. 2020. Quantifying the causal effects of conversational tendencies. Proceedings of CSCW.
Xiaojin Zhur and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. CMU CALD tech report.

A Proof of Theorem 1
Consider the expected outcome µ(t) = E Y ; do(T˜ = t) .

µ(t) = EW E Y | W ; do(T˜ = t)

(10)

(by iterated expectation)

= EW E Y | T˜, Z˜ ; do(T˜ = t)

(11)

(by deﬁnition)

= EW E Y | T˜ = t, Z˜

(12)

(by overlap and no unobserved confounding)

The proof is complete because the estimand ψrea. is simply µ(1) − µ(0).

B C-restriction
Section 4 said that adjusting for confounding information Z˜ = f (W ) is sufﬁcient for blocking the confounding backdoor path created by non-treatment properties of the text that readers might perceive. In Section 5.2 we proposed performing this adjustment with BERT. We could alternatively try to block this path by restricting a Tˆ-boosting model to only use information related to the confounding covariates C (for which we can adjust). This could capture the same desired effect as conditioning on Z˜ = f (W ) by accounting for whatever extra information was leaked from W into Tˆ. We accordingly experiment with restricting the model to features that are highly correlated with C: we compute point-biserial correlation coefﬁcients (Glass and Hopkins, 1996) between each word and C, then select the top 2000 words as features for the bootstrapping model. Results are given in Table 4 and suggest that while it still gives an improvement over the raw Tˆ’s, C-restriction yields more conservative estimates than adjusting for W .

oracle (ψ) proxy-lex (ψˆnaive+C)
T-boost (C only, ψˆnaive+C)
T-boost (ψˆnaive+C) W-Adjust (ψproxy)

Lexicon 15.54 11.21 13.30
14.68 15.01

BERT 15.54 7.83 10.92
12.01 13.88

Random 15.54 12.34 12.70
13.59 13.30

Table 4: ATE estimates from a C-restricted classiﬁer (row 2) are preferable to Tˆ but conservative compared to less restrictive methods (T-boost and W-Adjust ).

C Ablating T-boost
T-boost only uses a classiﬁer to change the treatment status of an example on Tˆ = 0 examples. Intuitively, this is because Tˆ is assumed to be a reasonable estimate of T˜ and therefore has a low false positive rate. We investigate this by ablating this part of the algorithm and directly setting Tˆ∗ to the classiﬁer’s prediction. Our results on lexicon-based Tˆ’s (Table 5, we observed similar outcomes with other Tˆ’s) suggest this can reduce performance because a large number of correctly labeled Tˆ = 1 examples are ﬂipped.

Estimator
oracle (ψ) proxy-lex (ψˆnaive+C) T-boost (Tˆ = 0 only, ψˆnaive+C) T-boost (all examples, ψˆnaive+C)

Estimate
15.54 11.21 14.60 10.00

Table 5: It is advantageous to only relabel Tˆ = 0 examples.

D Lemma 1 (used by Theorems 2 and 3)

E[Y | W, Tˆ = 1] = E[Y | W, T = 1] Pr(T = 1 | W, Tˆ = 1) + E[Y | W, T = 0] Pr(T = 0 | W, Tˆ = 1)
E[Y | W, Tˆ = 0] = E[Y | W, T = 0] Pr(T = 0 | W, Tˆ = 0) + E[Y | W, T = 1] Pr(T = 1 | W, Tˆ = 0)

Proof. Apply the law of total probability and deﬁnition of conditional independence to the causal graph given in Figure 3.
E Proof of Theorem 2
Let
0 = P (T = 0 | Tˆ = 1, Z˜) 1 = P (T = 1 | Tˆ = 0, Z˜, C) p1 = P (T = 1 | Tˆ = 1, Z˜) p0 = P (T = 0 | Tˆ = 0, Z˜) E1 = E[Y | Z˜, T = 1] E0 = E[Y | Z˜, T = 0]

Now recall

ψˆproxy = EW [E[Y | Tˆ = 1, Z˜] − E[Y | Tˆ = 0, Z˜]]

Now we write the inner part using Lemma D, collect terms, and use the law of total probability to write everything in terms of misclassiﬁcation probabilities:

= (E1p1 + E0 0) − (E0p0 − E1 1) = E1(p1 + 1) + E0( 0 − p0) = E1((1 − 0) + 1) + E0( 0 − (1 − 1)) = (E1 − E0)(1 − ( 0 + 1))

which completes the proof

F Theorem about the bias due to noisy proxies

Here we show that the naive estimand which does not adjust for the text,

ψnaive = E Y | Tˆ = 1 − E Y | Tˆ = 0 ,

(13)

can be arbitrarily biased away from the effect of interest, ψrea..

Theorem 3. where

ψnaive = EW E[Y | T˜ = 1, W ]α(W ) − E[Y | T˜ = 0, W ]β(W )

P (T˜ = 1, Tˆ = 1 | W ) P (T˜ = 1, Tˆ = 0 | W )

α(W ) =

P (Tˆ = 1)

−

P (Tˆ = 0)

P (T˜ = 0, Tˆ = 0 | W ) P (T˜ = 0, Tˆ = 1 | W )

β(W ) =

P (Tˆ = 0)

−

P (Tˆ = 1)

The α and β terms are related to the error of the proxy label. This theorem says that correlations between the outcome and errors in the proxy can induce bias. Intuitively, this is similar to bias from confounding, though it is mathematically distinct. This means that even a highly accurate proxy label can result in highly misleading estimates. Proof:

E[Y | Tˆ = 1] = E =E

E[Y | Tˆ = 1, W ] | Tˆ = 1

E[Y

| Tˆ

Pr(W = 1, W ]

| Tˆ

= 1)

Pr(W )

=E

E[Y

| Tˆ

Pr(Tˆ = 1, W ]

= 1 | W)

Pr(Tˆ = 1)

Where the ﬁrst equality is by the tower property, the second by inverse probability weighting, and the third Bayes’ rule. We continue by invoking Lemma 1:

E[Y

| Tˆ

Pr(Tˆ = 1, W ]

= 1 | W)

= E[Y

| W, T

= 1] Pr(T

= 1 | W, Tˆ

Pr(Tˆ = 1)

= 1 | W)

Pr(Tˆ = 1)

Pr(Tˆ = 1)

+ E[Y

| W, T

= 0] Pr(T

= 0 | W, Tˆ

Pr(Tˆ = 1)

= 1 | W)

Pr(Tˆ = 1)

Pr(T = 1, Tˆ = 1 | W )

= E[Y | W, T = 1]

Pr(Tˆ = 1)

Pr(T = 0, Tˆ = 1 | W )

+ E[Y | W, T = 0]

Pr(Tˆ = 1)

.

The analogous expression for E[Y | Tˆ = 0]:

E[Y | Tˆ = 0] = E

Pr(T = 0, Tˆ = 0 | W )

E[Y | W, T = 0]

Pr(Tˆ = 0)

Pr(T = 1, Tˆ = 0 | W )

+ E[Y | W, T = 1]

Pr(Tˆ = 0)

And now plugging into the ATE formula:

ψˆrea. = E[E[Y ; do(Tˆ = 1)] − E[Y ; do(Tˆ = 0)]] = E[E[Y |Tˆ = 1] − E[Y |Tˆ = 0]]

=E

Pr(T = 1, Tˆ = 1 | W ) Pr(T = 1, Tˆ = 0 | W )

E[Y | W, T = 1]

Pr(Tˆ = 1)

−

Pr(Tˆ = 0)

Pr(T = 0, Tˆ = 0 | W ) Pr(T = 0, Tˆ = 1 | W )

− E[Y | W, T = 0]

Pr(Tˆ = 0)

−

Pr(Tˆ = 1)

The result follows immediately.
G Deriving semi-oracle, a Causal Estimator for when P (T |Tˆ) is known
This is an ATE estimator which assumes access to P (Tˆ|T ) instead of T but is still unbiased. We derive this estimator using the “matrix adjustment” technique of Wood-Doughty et al. (2018); Pearl(2012). We start by decomposing the joint distribution
P (Y, T, Tˆ, C) = P (Tˆ|Y, C, T )P (Y, C, T ) P (Y, C, Tˆ) = P (Tˆ|Y, C, T )P (Y, C, T )
T
We can write this as a product between a matrix Mc,y(Tˆ, T ) = P (Tˆ|Y, C, T ) and vector Vc,y(T ) = P (Y, C, T ):

Vc,y(Tˆ) = Mc,y(Tˆ, T )Vc,y(T )
T
= Mc,yVc,y

For our binary setting Mc,y is:

Mc,y = 1 − δc,y δc,y

c,y
1 − c,y

M−1 =

1

1 − c,y

c,y 1 − c,y − δc,y −δc,y

c,y = P (Tˆ = 0|T = 1, C, Y )

δc,y = P (Tˆ = 1|T = 0, C, Y )

− c,y 1 − δc,y

Under fairly broad conditions, M has an inverse, which allows us to reconstruct the joint distribution:

P (Y, T, C) =

M

−1 c,y

(

T

,

Tˆ

)

V

c,y

(Tˆ

)

Tˆ

From which we can recover the ATE

ψmatrix =
c

P (Y, T = 1, C) −
Y P (Y, T = 1, C)

P (Y, T = 0, C) P (C)
Y P (Y, T = 0, C)

Note also that this expression is similar to τME in Wood-Doughty et al. (2018) except their error terms are of the form P (T | Tˆ).

