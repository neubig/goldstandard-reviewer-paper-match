BUILDING CORPORA FOR SINGLE-CHANNEL SPEECH SEPARATION ACROSS MULTIPLE DOMAINS
Matthew Maciejewski, Gregory Sell, Leibny Paola Garcia-Perera, Shinji Watanabe, Sanjeev Khudanpur
Center for Language and Speech Processing Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA

arXiv:1811.02641v1 [cs.CL] 6 Nov 2018

ABSTRACT
To date, the bulk of research on single-channel speech separation has been conducted using clean, near-ﬁeld, read speech, which is not representative of many modern applications. In this work, we develop a procedure for constructing high-quality synthetic overlap datasets, necessary for most deep learning-based separation frameworks. We produced datasets that are more representative of realistic applications using the CHiME-5 and Mixer 6 corpora and evaluate standard methods on this data to demonstrate the shortcomings of current source-separation performance. We also demonstrate the value of a wide variety of data in training robust models that generalize well to multiple conditions.
Index Terms— speech separation, far-ﬁeld speech
1. INTRODUCTION
A common situation that arises in audio featuring multiple speakers is that two speakers will inevitably speak at the same time. This can lead to a breakdown of performance in speech technologies such as automatic speech recognition (ASR) and speaker identiﬁcation, as the models are unable to tease apart the speech from the different sources. Speech separation seeks to solve this problem by producing multiple clean, non-overlapping waveforms from a recording in which multiple speakers are talking at the same time.
The bulk of recent work conducted on speech separation [1, 2, 3, 4, 5, 6, 7] has been done using the Wall Street Journal (WSJ0) [8] corpus, which is not representative of many speech separation tasks. This dataset consists of read speech of utterances from Wall Street Journal news, recorded in a clean environment on close-talking microphones, which is then synthetically combined to form overlapped speech. In the advent of home assistant devices, a common application situation is one in which conversational speech has been recorded in the home from a far-ﬁeld microphone. Our goal is to establish high-quality datasets that expand evaluation conditions for speech separation and to demonstrate the shortcomings of current standard speech separation techniques on them.
Since evaluation metrics and model training typically require ground truth, supervised methods are typically limited to using synthetic overlap data, where non-overlapping speech utterances are summed together. As such, creating new speech separation datasets involves selecting a source corpus and combining speech from that corpus in a way that creates high-quality overlapping speech.
This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.

For our datasets we used the CHiME-5 [9] and Mixer 6 [10] corpora, which each include multiple microphones, allowing near/farﬁeld comparisons. We created a system to isolate high-quality single-speaker speech regions and developed a principled method for pairing utterances. Finally, we used the utterance-level Permutation Invariant Training (uPIT) method [4] to investigate speech separation performance on these conditions.
2. CORPUS SELECTION AND DATA PREPARATION
When working with synthetically overlapped speech, careful considerations must be made in data selection. Not only does the selection of the source corpora play a role, but care also must be taken in how the speech is overlapped so that the resulting dataset meets the needs of the application and does so with efﬁcient use of the source speech.
To ensure this high standard of quality would be met, we devised a process for dataset creation with three main components: corpus selection, data cleanup for single-speaker segmentation, and algorithmic generation of mixture lists. We also are releasing the code and resulting mixture lists used in our experiments1,2.
2.1. Corpus Selection
For our experiments, we used the WSJ0, CHiME-5, and Mixer 6 speech corpora.
The WSJ0 corpus was selected due to the pre-existence of a synthetic overlap dataset [1], a standard of speech separation evaluation. We did not generate our own WSJ0 dataset, instead using the existing dataset. This dataset has been effectively used in a number of speech separation research experiments, and so its composition was the model for our dataset generation pipeline. For our experiments, the WSJ0 overlap dataset also represents the clean, near-ﬁeld, read speech condition.
The CHiME-5 corpus was chosen to serve as the most challenging, “realistic” condition. The corpus consists of dinner parties recorded with microphone arrays placed around the apartment as well as binaural microphones. This condition resulted in a number of unique challenges in the audio, such as naturally occurring nonspeech noises, multiple simultaneous speakers, and time-varying locations. In support of the CHiME-5 challenge, the dataset was fully transcribed, which proved to be essential for the creation of a dataset for speech separation involving realistic condition speech.
1https://github.com/mmaciej2/kaldi/tree/chime5-single-speakergeneration/egs/chime5/single speaker generation
2https://github.com/mmaciej2/kaldi/tree/mixer6-single-speakergeneration/egs/mixer6/single speaker generation

In addition, we were able to generate parallel near-ﬁeld and far-ﬁeld datasets with identical utterances from this corpus.
The Mixer 6 corpus was chosen to serve as a middle ground between the WSJ0 and CHiME-5 corpora. Consisting of interviews and read speech recorded with 14 microphones in a constructed recording room, the Mixer 6 corpus allows a similar near- and far-ﬁeld comparison, but in a more controlled environment with stationary speakers, consistent channel, and relatively minimal noise.
Mixer 6 sessions consist of three components: an interview, read speech, and a phone call. For our mixtures we used only speech from the interview session, as we preferred the spontaneity of the format, while the existence of separate close-microphone recordings for both the interviewer and interviewee simpliﬁed the process of extracting single-speaker segments, as described below.
2.2. Cleanup Methods
The quality of a machine learning system is highly dependent on the quality of its training data, a characteristic that is especially true for deep learning. As such, it is critical that the original segments used for constructing synthetic overlap contain speech from only one person—no more, no less.
To accomplish this, we used a pipeline implemented with the Kaldi Speech Recognition Toolkit [11] to clean the single-speaker segmentation. This pipeline consists of three parts: initial segmentation, speech activity detection, and segment veriﬁcation.
2.2.1. Initial Single-Speaker Segmentation
The initial segmentation is the only part of the corpus cleanup to differ between the CHiME-5 and Mixer 6 corpora. The purpose of the initial segmentation is to use the basic corpus properties and annotations to generate regions where we have some base conﬁdence level that there is speech present and that only one person is talking.
For the CHiME-5 corpus, we have full speech transcriptions, so the initial segmentation was done by identifying the regions where only one speaker was marked to have been speaking.
For the Mixer 6 corpus, the initial segmentation was done through simple energy analysis between the lapel microphones of the interviewer and the participant. The energy level in the participant’s microphone had to exceed a tuned threshold to be considered speech, and the ratio of energy between the microphones had to exceed a tuned threshold to be considered speech by the participant and only the participant.
2.2.2. Speech Activity Detection
The second stage in the corpus cleanup pipeline is to run a speech activity detection (SAD) system designed for ASR on top of the initial segmentation, which serves multiple purposes.
The ﬁrst purpose is to remove non-speech segments, as human annotators can make mistakes or be imprecise about annotations, and the energy-based system can be fooled by things such as rustling of clothes. The second purpose is to break the segmentation up into reasonable ASR-style utterances using natural pauses to split longer regions which could prove undesirable for synthetic overlap.
The SAD system is a Time-Delay Neural Network-based system with statistics pooling [12] for long-context. The model was trained on data from the LibriSpeech corpus [13], perturbed with room impulse responses and additive noise/music from the MUSAN corpus [14].
For optimal performance, for a given utterance, the SAD system was run on the corresponding speaker’s close-talking microphone,

Table 1. Dataset Statistics
which was then mapped back across all microphones using timealignment drift annotations in the case of CHiME-5 and relying on the synchronous recording property of Mixer 6.
2.2.3. Segment Veriﬁcation
The last stage in the corpus cleanup pipeline is to ensure the ﬁnal segments are sufﬁciently high-quality. Part of this includes removing utterances deemed “too short for use” (shorter than 1.3 seconds, the minimum length of the WSJ0-based mixture corpus), but this stage primarily is done using speaker veriﬁcation techniques.
We used x-vectors [15], a deep-neural-network-based embedding for speaker identiﬁcation, along with a Probabilistic Linear Discriminant Analysis (PLDA) scoring backend [16, 17], state-of-theart in the speaker identiﬁcation task. The models were trained using the VoxCeleb corpus [18] augmented with music and noise [14].
Since speaker embeddings contain information about the voice quality of the enrolled speaker, the system can be used to reject segments with unexpected acoustic properties for that speaker, which might arise from undesirable issues like incorrect speaker labels, previously-missed overlapping speech, or non-speech vocalizations. The procedure was to extract and enroll x-vectors over all speech for a given speaker, extract x-vectors for each individual segment, score the segment x-vectors against their speaker’s enrolled x-vector using PLDA, and ﬁnally reject segments whose scores were below a given threshold. The threshold was tuned based on qualitative assessment of randomly-selected segments.
2.3. Mixture List Generation
For consistency, we generated the mixture datasets by creating lists compatible with the MERL scripts for generating overlap3 that also roughly matched the properties of the WSJ0 mixtures. Despite this, there was a lot of freedom in how to pair utterances from the base corpus to generate mixtures. As a result, we created the mixture lists algorithmically according to a set of desirable criteria selected to maximize the quality of the resulting mixtures given a set of utterances from a corpus.
Mixture lists were constructed based on four criteria. First, we wanted to avoid mixtures of two utterances by the same speaker. Then, we wanted maximal coverage of utterances—for example not using an utterance for a third time if another utterance had only be used once (equivalent to minimizing the maximum amount of data repetition). Next, we wanted to maximize speaker diversity within pairs, i.e. not combining an utterance with two utterances by the
3http://www.merl.com/demos/deep-clustering/create-speakermixtures.zip

Algorithm 1 Mixture list generation
while num mixes < target mixes do S1 = {utt : usage count(utt) = min usage count(·)} u1 = arg maxu∈S1 length(u) i←0 while u1 not yet matched do S2 = {utt : usage count(utt) = min usage count(·)+i} S3 = {utt : spk(utt) ∈/ {spk : u1 previously paired}} if S2 ∩ S3 = ∅ then u2 = arg minu2∈S2∩S3 | len(u1) − len(u2) | pair u1 and u2, update data structures else if S2 = ∅ then {spk : u1 previously paired} ← ∅ i←0 else i←i+1 end if end if end while
end while
same speaker (similar to the previous constraint, except over speaker pair repetition instead of utterance repetition). The ﬁnal constraint is to pair utterances with utterances of similar length. Since the longer utterance in a pair must be truncated (or the shorter padded), a mismatch in length leads to ‘wasted’ audio, and we thus want to pair long utterances with long utterances and short utterances with short utterances.
Since it is computationally infeasible to compute an optimal solution, we used a greedy algorithm, elaborated in Algorithm 1. The basic structure is to proceed through a priority queue of utterances, constructing sets of eligible utterances to be paired with it and selecting the eligible utterance that is closest in length. Every time an eligible set is empty, we relax the least important constraint. The process is repeated until a target number of mixtures has been generated.
2.4. Description of Resulting Datasets
2.4.1. Single-Speaker Data
The CHiME-5 dataset cleanup resulted in roughly 8 hours of speech across 40 speakers. Speakers on average contributed 327 utterances with a mean length of 2.23 seconds (standard deviation of 0.94), for an average of 12 minutes of speech per speaker.
The Mixer 6 dataset cleanup produced 163 hours of speech from 548 speakers. The average number of utterances per speaker was comparable to CHiME-5 at 341 utterances, though with roughly 50% more speech on average, with a mean utterance length of 3.13 seconds and standard deviation of 1.44, with an average of 18 minutes of speech per speaker.
The above statistics refer to unique speech, and do not account for the fact that both corpora were recorded on a number of microphones. CHiME-5 was recorded with six 4-microphone arrays in addition to binaural microphones on each of the four participants. Mixer 6 was recorded on 14 microphones of various type and positioning. All of our processing allows for mapping utterances’ time marks between all microphones.
We selected two microphone conditions from each corpus for use in our experiments. For the far-ﬁeld CHiME-5 condition, we

selected the ﬁrst channel of the ﬁrst microphone array, which is consistent within recordings but not across sessions. For the nearﬁeld CHiME-5 condition, we used the binaural microphone for the speaker corresponding to each utterance. For the far-ﬁeld Mixer 6 condition, we chose channel 9, which is the microphone placed farthest from the speaker. For the near-ﬁeld condition, we chose channel 2, which is the lapel microphone for the subject.
2.4.2. Overlapped Speech Data
In constructing the CHiME-5 and Mixer 6 mixture data, an attempt was made to match the WSJ0 mixture data as closely as possible.
The most natural correspondence was to construct train, crossvalidation, and test sets of equivalent size (20,000, 5,000, and 4,000 utterances respectively). We chose mixture SNR levels following the same distribution as well. However, because the size of each source corpus varied, the usage of each speaker and utterance varied as well. Comparison of usage statistics are in Table 1. For all conditions, there is no overlap of speakers between data subsets.
For the CHiME-5 data, to increase compatibility with the CHiME-5 ASR evaluation, the data split corresponds with the ASR split. At time of writing, the evaluation transcripts had not yet been released, so cleanup and mixture generation for a test set could not be done. As a result, we evaluated using the cross-validation set generated from the development data and omitted cross-validation.
For the Mixer 6 data, we constructed subsets of 45 and 50 highquality speakers, roughly gender balanced, to generate testing and cross-validation sets, leaving 451 for the training set.
In both the CHiME-5 and Mixer 6 mixture datasets, we constructed both near-ﬁeld and far-ﬁeld conditions. When doing so, we used identical utterance pairs and mixture weights, as opposed to generating new mixture sets, to reduce the number of confounding factors when comparing performance between near-ﬁeld and farﬁeld conditions.
In addition, due to the extensive size of the Mixer 6 corpus, the usage statistics of the resulting mixture datasets generated by our algorithm are dissimilar to the WSJ0 mixture dataset. As a result we constructed two additional sets of Mixer 6 mixtures. In the ﬁrst, training data was restricted to 100 speakers in order to be ‘speakerbalanced’ to the WSJ0 training data (mx6 * spk-bal). In the second, the total size of the training dataset was increased ﬁve-fold to 100,000, resulting in more similar speaker usage. This allowed us to do an analysis of the trade-off between number of speakers and amount of speech per speaker.
Finally, we constructed a training set of equivalent size by combining of each of the ﬁve base datasets (WSJ0, CHiME-5 near and far, Mixer 6 near and far). Two iterations were created. In the ﬁrst, the combinations were sub-sampled to maintain the size of 20,000 training examples. In the second, they were fully combined, resulting in 100,000 examples. These sets allowed us to analyze the potential for producing a robust system based on training on a wide variety of properly manicured data.
3. SPEECH SEPARATION EXPERIMENTS
3.1. Separation Technique
For evaluation of the data, we used a deep learning mask-based technique called utterance-level Permutation Invariant Training (uPIT) [4]. The method consists of using a recurrent neural network to produce spectral masks from an input mixture magnitude spectrum, then applying the masks to the mixture spectrogram and inverting to re-

Table 2. SDR improvement across varying train and test conditions

construct the estimated source waveforms. For computational reasons, we used down-sampled 8 kHz au-
dio. The spectrograms were generated using a short-time Fourier transform (STFT) with a window length of 512 and step of 128. The magnitude spectrum was used for input to the neural network.
For the purposes of our experiments, it was assumed that the input speech was a mixture of two speakers, and so the network was speciﬁcally designed to output two masks. This is not a limitation of the method or uPIT training strategy, but rather a choice for simplifying the experiments.
The network used consists of two 600-node bidirectional LSTM layers followed by a linear layer, with sigmoid output. It is trained with mean squared error loss between the estimated (masked) and ground truth source magnitude spectra:

Loss(Mˆ , π) = 1 S B

Mˆ π(s) ◦ Amix − As

2 F

s=1

where Mˆ are the estimated masks, A are the STFT magnitudes, B is the total number of STFT coefﬁcients across sources, and πˆ is the permutation of output masks that produces the lowest loss:

S
πˆ = arg min Loss(Mˆ , π) π∈P s=1

We used the Adam optimizer with a learning rate of 0.001. The models were trained for 200 training epochs, with the ﬁnal model being chosen based on a minimum loss on the cross-validation set.

3.2. Evaluation Metric
For evaluation, we used a standard evaluation metric for source separation, signal to distortion ratio (SDR) [19]. It measures the overall estimation error of the target sources.

4. RESULTS
The results of our experiments are shown in Table 2. The two biggest conclusions are that models do not appear to generalize well and that performance degrades in more realistic far-ﬁeld conditions.
We see that the best overall separation performance appears in the baseline WSJ0 test condition with the model trained on WSJ0 data. However, there is a clear degradation in performance moving from WSJ0 to more realistic conversational, near-ﬁeld conditions, and even further degradation in far-ﬁeld.

We also see that matched conditions between train and test sets typically achieve the greatest performance, but the models do not seem to generalize well to other sets. One exception is that the nearﬁeld Mixer 6 model outperformed the near-ﬁeld CHiME-5 model on the near-ﬁeld CHiME-5 test condition. It is possible that the higher-quality audio of Mixer 6 produced a better model, though the performance between the two models is close enough that strong conclusions should not be made. In general, near-ﬁeld trained systems perform reasonably well on other near-ﬁeld conditions while degrading in far-ﬁeld, and vice versa.
Something else worth noting is that the Mixer 6 training set where the number of speakers was balanced to match the WSJ0 dataset performed slightly worse that regular Mixer 6 training set. The small degradation in performance might be due to the overall decrease in total speech present in the dataset, though performance is again close enough that strong conclusions should not be made. However, the absence of noticeable improvements suggests that having depth in speech per speaker does not have much impact over greater diversity in speakers.
The expanded 100,000 Mixer 6 mixture training sets showed some improvement, but the gains were modest and tended to be restricted to matched conditions where the model was already performing well (especially in the far-ﬁeld case), doing little to improve performance otherwise.
Finally, an interesting result is that the models trained on a combination of training conditions achieved near-optimal performance. Though they performed on average slightly worse than the best system in each test condition, they are the only systems to have success across conditions. Given that the other models performed quite poorly outside of their own domain, this is an important result that demonstrates that diversity of training data is important for producing robust speech separation models. Furthermore, as with the 100,000 Mixer 6 data, the expanded training set appears to be more valuable in far-ﬁeld conditions than near-ﬁeld.
5. CONCLUSIONS
In this work we have presented and shared a framework for producing new, high-quality synthetic overlap datasets for training and evaluation of source separation technologies, and we have used it to produce additional datasets that expand the domain of single-channel speech separation evaluation from clean, near-ﬁeld conditions to farﬁeld, realistic conversational speech.
We have demonstrated the value of this work through an analysis of standard speech separation technologies that reveals a large degradation in performance in challenging conditions. We have also shown that including multiple conditions in training data, made possible through this work, greatly improves the robustness of the resulting model, a shortcoming in the technology otherwise.
6. ACKNOWLEDGEMENTS
We would like to thank Xuankai Chang for providing comments useful to producing the ﬁnal version of this manuscript.
7. REFERENCES
[1] John R. Hershey, Jonathan Le Roux, Zhuo Chen, and Shinji Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proceedings of the IEEE In-

ternational Conference on Acoustics, Speech and Signal Processing, 2016.
[2] Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, and John R. Hershey, “Single-channel multi-speaker separation using deep clustering,” CoRR, vol. abs/1607.02173, 2016.
[3] D. Yu, M. Kolbk, Z. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2017, pp. 241–245.
[4] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks,” ArXiv e-prints, Mar. 2017.
[5] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network for single-microphone speaker separation,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2017, pp. 246–250.
[6] Zhuo Chen, Yi Luo, and Nima Mesgarani, “Speakerindependent speech separation with deep attractor network,” CoRR, vol. abs/1707.03634, 2017.
[7] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Listening to each speaker one by one with recurrent selective hearing networks,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2018, pp. 5064–5068.
[8] J. Garofolo, D. Graff, D. Paul, and D. Pallett, CSR-I (WSJ0) Complete LDC93S6A, Linguistic Data Consortium, Philadelphia, 1993.
[9] Jon Barker, Shinji Watanabe, Emmanuel Vincent, and Jan Trmal, “The Fifth CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines,” in Proceedings of Interspeech, 2018.
[10] L. Brandschain, D. Graff, and K. Walker, Mixer-6 Speech LDC2013S03, Linguistic Data Consortium, Philadelphia, 2013.
[11] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka´s˘ Burget, Ond˘rej Glembek, Nagendra Goel, Mirko Hannermann, Petr Motl´ıc˘ek, Yanmin Qian, Petr Schwarz, Jan Silovsky´, Georg Stemmer, and Karel Vesely´, “The Kaldi Speech Recognition Toolkit,” in Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, 2011.
[12] Pegah Ghahremani, Vimal Manohar, Daniel Povey, and Sanjeev Khudanpur, “Acoustic modelling from the signal domain using cnns.,” in INTERSPEECH, 2016, pp. 3434–3438.
[13] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2015, pp. 5206–5210.
[14] “MUSAN: A Music, Speech, and Noise Corpus,” 2015, arXiv:1510.08484.
[15] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, “X-Vectors: Robust DNN Embeddings for Speaker Recognition Using Data Augmentation,” in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2018.

[16] Simon J. D. Prince and James H. Elder, “Probabilistic Linear Discriminant Analysis for Inferences About Identity,” in IEEE International Conference on Computer Vision, 2007, pp. 1–8.
[17] Sergey Ioffe, “Probabilistic Linear Discriminant Analysis,” in ECCV, A. Leonardis, H. Bischof, and A. Pinz, Eds., pp. 531– 42. Springer-Verlag, 2006.
[18] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, “VoxCeleb: a large-scale speaker identiﬁcation dataset,” in Proceedings of Interspeech, 2017.
[19] Emmanuel Vincent, Re´mi Gribonval, and Ce´dric Fe´votte, “Performance Measurement in Blind Audio Source Separation,” IEEE Transactions on Acoustics, Speech, and Language Processing, vol. 14, no. 4, pp. 1462–9, July 2006.

