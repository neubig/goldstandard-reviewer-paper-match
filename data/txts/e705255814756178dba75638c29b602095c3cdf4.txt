arXiv:2002.10021v1 [cs.LG] 24 Feb 2020

HOW TRANSFERABLE ARE THE REPRESENTATIONS LEARNED BY DEEP Q AGENTS?
Jacob Tyo & Zachary Lipton Machine Learning Department Carnegie Mellon University Pittsburgh, PA 15213, USA jtyo@cs.cmu.edu, zlipton@cmu.edu
ABSTRACT
In this paper, we consider the source of Deep Reinforcement Learning (DRL)’s sample complexity, asking how much derives from the requirement of learning useful representations of environment states and how much is due to the sample complexity of learning a policy. While for DRL agents, the distinction between representation and policy may not be clear, we seek new insight through a set of transfer learning experiments. In each experiment, we retain some fraction of layers trained on either the same game or a related game, comparing the beneﬁts of transfer learning to learning a policy from scratch. Interestingly, we ﬁnd that beneﬁts due to transfer are highly variable in general and non-symmetric across pairs of tasks. Our experiments suggest that perhaps transfer from simpler environments can boost performance on more complex downstream tasks and that the requirements of learning a useful representation can range from negligible to the majority of the sample complexity, based on the environment. Furthermore, we ﬁnd that ﬁne-tuning generally outperforms training with the transferred layers frozen, conﬁrming an insight ﬁrst noted in the classiﬁcation setting.
1 INTRODUCTION
Deep Reinforcement Learning (DRL) agents learn policies by selecting actions directly from raw perceptual data. Despite numerous breakthroughs (Mnih et al., 2015; Vinyals et al., 2019; Silver et al., 2016), DRL’s prohibitive sample complexity limits its real world application. The sample complexity of DRL may derive from many sources, including the requirement of learning useful state representations and the requirement of learning good policies given a suitable representation. This paper provides several simple experiments to provide new insights into this breakdown.
For a DRL agent, where precisely the “representation” ends and the “policy” begins is not clear. In our experiments, we consider multiple interpretations of this divide by partitioning the network at various layers. To evaluate the extent to which representation learning contributes to the sample complexity of DRL, we execute a series of transfer learning experiments aimed to determine how quickly an agent can learn given pre-learned representations (from either the same or a different game). Our experiments proceed in the following manner: 1) Train a parent network until best reported performance is achieved. 2) Transplant the ﬁrst k layers into a child network, re-initializing the remaining l − k layers randomly. 3) Train the child network, either ﬁne-tuning the transplanted layers or keeping them frozen, following the methodology of Yosinski et al. (2014).
In particular, our experiments address the transferability of DQN representations among the three Atari games Berzerk, Krull, and River Raid. We ﬁnd the beneﬁts of transferring representations from pretrained networks to be remarkably variable in general and non-symmetric across pairs of tasks. While we have not evaluated enough tasks to draw deﬁnitive conclusions, our preliminary results suggest that transfer from simple environments may improve performance on more complex tasks more than the reverse transfer. Furthermore, our results suggest that the contribution of learning a useful representation to the overall sample complexity of the problem can range from negligible to the majority, based on the destination environment. Lastly, ﬁne-tuning the transferred layers outperforms training with those layers frozen in general.
1

2 BACKGROUND AND EXPERIMENTAL SETUP

Reinforcement Learning (RL) methods address the problem where an agent must learn to act in
an unknown environment to maximize a reward signal. Initially, the environment provides a state s0 to the agent, and then the agent selects an action a0 based on the provided state. Thereafter, at each time step the environment provides an agent with a state st and a reward rt each inﬂuenced by the agents previous action. The agent must then select subsequent actions at, and so on, until the episode terminates. Formally, this interaction is described by a Markov Decision Process (MDP) M = S, A, T , r, γ , where S is the set of states, A is the set of actions, T (s, a, s ) = P(st+1 = s |st = s, at = a) is the transition function, r(s, a) = E[rt+1|st = s, at = a] is the reward function, and γ ∈ [0, 1] is a discount factor.

We focus on Q-learning, an off-policy value-based RL algorithm. The value of each state is rep-

resented by vπ(s) = Eπ[Gt(st)|st = s], and the value of each state-action pair is represented by

qπ(s, a) = Eπ[Gt(st)|st = s, at = a] where Gt(s) =

T i=t

γ

i−trt(s).

In

Q-learning,

the

agent

estimates the state-action value function by predicting the expected discounted return.

Many interesting problems, including Atari games, have a large state and action space, making tabular estimates of the Q-function intractable. In these cases, the Q-function can be approximated. DRL denotes methods that approximate either the value function (or, in other algorithms, the policy directly) by deep neural networks.

In this paper, we build on a DRL Q-learning implementation called Rainbow (Hessel et al., 2018): a 5-layer convolutional neural network based on Mnih et al. (2015) that incorporates double Qlearning (Van Hasselt et al., 2016), prioritized replay (Schaul et al., 2015), dueling networks (Wang et al., 2015), multi-step learning (Sutton, 1988), distributional RL (Bellemare et al., 2017), and Noisy Nets (Fortunato et al., 2017). Furthermore, Rainbow maintains two separate Q-networks: one with parameters θ, and a second with parameters θtarget that are updated from θ every ﬁxed number of iterations. In order to capture the game dynamics, a state is represented by a sequence (four in our case) of history frames.

We tested the transferability of features learned by Rainbow agents on Atari games (i.e. environments). To make transfer learning experiments feasible within our resources, we selected environments according to the speed that a Rainbow agents can reach high performance, requiring that the cardinality of the state and action spaces of each environment be equivalent and that two among the three environments were qualitatively similar (same genre of game). We are interested in game similarity to test a hypothesis that more similar games have more similar representations, and therefore agent transfer should be more effective between them. Environments with the same state and action space cardinality is required as it made agent transfer possible without modiﬁcation. Pulling from the results of Hessel et al. (2018), we selected Berzerk, Krull, and River Raid from the Arcade Learning Environment (Bellemare et al., 2013).

Drawing inspiration from Yosinski et al. (2014), our experiments proceeded as follows:

1. For all environments, train a parent network (a Rainbow agent) until best-reported performance is reached.
2. For every permutation of environment pairs, transplant the ﬁrst k layers of the parent network into a child network (also a Rainbow agent), then reinitialize the remaining l − k layers randomly (where l is the total number of layers in the network and k ∈ {2, 4}).
3. Train the child network, either ﬁne-tuning or freezing the transplanted portion of the network (we explore both settings).

With 3 runs per pair of environments, setting of k, and each choice among {freezing, ﬁne-tuning}, we ran a total of 111 trials taking over 35 days on the available resources.

3 EXPERIMENTAL RESULTS AND DISCUSSION
In this section, we analyze the performance and learned representations of the child agents, where one child exists for every environment pair, k-value, and choice among freezing/ﬁne-tuning. For brevity, we will denote child agents with 4 layers transferred from a parent trained on environment1

2

Figure 1: Child agent (blue, red, and green) performance over iterations of training when the transplanted layers are frozen. Compare against the parent network trained from scratch (black).
then frozen as child4-frozen-environment1. We will refer to the parents as baselines with respect to their performance on the environment that they were trained.
We evaluate each network separated at two places, transferring 2, and transferring 4 (out of 5) layers and compare freezing vs ﬁne-tuning over the transplanted layers. For each respective experiment, we refer to the output of the last layer of the transplanted portion of the network as the “representation.”
Figures 1a, 1b, and 1c demonstrate the performance of all environment pairs when 4 layers are transplanted and then frozen during subsequent training. Because only the output layer is not frozen, these plots show the performance of a linear policy. trained on the transplanted representation. Levine et al. (2017) deem representations that can be used with a linear policy as a “good” representation, thus our analysis starts with these corresponding experiments.
Figure 1a presents the performance of the agents on Berzerk for runs with 4 transplanted layers, all of which are frozen during the subsequent training. The child4-frozen-Berzerk agent (blue) does not out perform the baseline (either by ﬁnal performance or training speed), which we ﬁnd surprising, since four of ﬁve layers of this agent were transferred from a high-performing parent trained on the same environment, leaving only a linear policy to be relearned. We might deduce that for this game, the entire difﬁculty of DRL can be attributed simply to the “RL” since starting off with the representations upon which a strong linear policy can be learnt seems to confer no beneﬁt. Notably, the agents transferred from foreign games (child4-frozen-Krull and child4-frozen-RiverRaid) have worse ﬁnal performance than the baseline. As a linear policy could not be learnt from a known good representation, we would not expect any other representation to perform better. Figure 1d shows these trends continuing when 2 layers are transplanted. The difference in performance between the children and the baseline is negligible, and therefore indicating that the difﬁculty does lie within the RL problem. Even with more model complexity allowed to the policy and different representation, no beneﬁts are seen.
3

Figure 2: Child agent (blue, red, and green) performance over iterations of training when the transplanted layers are ﬁne-tuned. Compare against the parent network trained from scratch (black).
However, as shown in Figure 1b, the transfer from Krull-to-Krull results in faster training and higher ﬁnal performance than the baseline, perhaps suggesting that for this game, representation learning is a more signiﬁcant part of the challenge. Note that with only two layers transferred, the representations pretrained on both Krull and Berzerk, outperform the baseline as shown in Figure 1e. This reﬂects our intuition that these games are similar, while the representations pretrained on River Raid confer no beneﬁt.
Transferring and freezing 4 of 5 layers from the parent to child agent on Berzerk and Krull showed negligible and positive changes in performance, respectively. However, River Raid paints a different picture. As shown in Figure 1c, no agent is able to learn a useful policy. This is especially surprising because we know that if nothing else, the transfer from the parent trained on River Raid itself can replicate the baseline results by learning a linear policy. Similarly, we see in Figure 1f that the agent with transplanted layers trained on the same environment performs the worst. Again this is surprising as, theoretically, the baseline could be replicated. Interestingly, this may indicate that a representation learned from a more simple environment may be helpful in solving more complex environments. The converse does not appear to hold from our experiments, but a deeper analysis on more tasks would be required to conﬁrm these intuitions.
In general, we see that when transplanted layers are frozen, there is a trade-off between negative transfer and training speed. As the number of unfrozen layers increases, the effect of negative transfer decreases but the training speed slows slightly. Furthermore, Figure 1 shows that transfer between environments is not symmetric. In other words, if a representation x is learned on environment X and is shown to perform well on environment Y , then this does not imply that a representation y learned on environment Y will be effective on X. This relation continues to hold, even when ﬁnetuning over the transplanted layers. However, when ﬁne-tuning over the transplanted layers we do not see the same level of negative transfer.
4

Figures 2a, 2b, and 2c show the performance of all environment pairs when four layers are transplanted from the parents to the children and then frozen, whereas Figures 2d, 2e, and 2f show the performance when only two layers are transplanted and frozen. Intuitively, these experiments which allow the transplanted representations to change during training, examine a more ﬂexible notion of their utility (as initializations only). For these experiments, in all environments except for River Raid, negative transfer is no longer an issue. In River Raid, negative transfer is seen in 2 of 6 children, vs in 6 of 6 children when the transferred layers are frozen.
The performance of all agents on Berzerk with 4 and 2 layers transplanted and ﬁne-tuned respectively, shown in Figures 2a and 2d, are similar to the performance seen for the same game under frozen transplanted layers. There is little difference between each of the agents, regardless of the environment on which the parent was trained. This further supports the earlier conclusions that the difﬁculty of this game is due entirely to learning the policy as no transferred representation improves performance.
Transplanting and ﬁne-tuning layers results in a large decrease in training time, and a large increase in ﬁnal performance on the Krull environment. This holds for both transplanting and ﬁne-tuning over 4 layers as shown in Figure 2b, and over 2 layers as shown in Figure 2e. As expected, the transplanted layers from the parent trained on Krull performed the best when 4 layers were transplanted. But interestingly when only 2 layers were transplanted, the transplanted layers from the parent trained on Berzerk were more effective. Overall, this supports our earlier conclusion of the importance of representations for Krull—in all cases, transfer is preferred to random initialization.
In Figures 1 and 2, we see the general trend that the transfer of pretrained layers has negligible effect on Berzerk, positive effects on Krull, and negative effects on River Raid. Figures 2c and 2f show the performance of all agents with 4 and 2 layers transplanted and ﬁne-tune respectively. Interestingly, the trend observed in Krull, where the best performing child was the one with features transferred from a parent trained on the same environment is the best performer when 4 layers are transferred, yet it is not the best performer when only 2 layers are transferred, is also observed with River Raid. However, the baseline trains faster and reaches better ﬁnal performance than all of the children on River Raid.
4 CONCLUSIONS
This paper presents an empirical evaluation of 111 transfer learning experiments on agents trained on the Atari 2600 games Berzerk, Krull, and River Raid. We compare the effect of transplanting initial layers of a pretrained network into a child network while either freezing or ﬁne-tuning over the transplanted layers. Surprisingly, the beneﬁts of transferring portions of pretrained networks are highly variable and non-symmetric across tasks. Furthermore, the requirements of learning a useful representation can range from nothing to the majority of the sample complexity based on the destination environment. We present analyses for why each task transfer occurs as shown, and give intuition for understanding representations and policies in DQNs. We show that, in general, ﬁne tuning is better than freezing portions of networks, as performance gains can still be expected with less likelihood of negative transfer.
Zahavy et al. (2016) have shown that DQNs ﬁnd hierarchical abstractions automatically. Our work suggests that the similarity of the high level task abstraction may be a good metric to determine the transferabitliy of DQN agents on. Future work includes heavier analysis on these experiments to determine how agents pretrained on different environments “focus” differently, how their representations differ, and how to numerically quantify the contribution of representation learning to the overall sample complexity. Furthermore, this methodology can pass insight to questions about the beneﬁt of unsupervised reinforcement learning in pre-training, e.g. techniques based on intrinsic motivation (Chentanez et al., 2005). To the extent that intrinsic motivation serves to learn representations suitable for ﬁne-tuning to a given reward signal, quantifying just how much representation learning is the bottleneck to learning in the ﬁrst place can provide insight in assessing its potential.
5

REFERENCES
Mehran Asadi and Manfred Huber. Effective control knowledge transfer through learning skill and representation hierarchies. In IJCAI, volume 7, pp. 2054–2059, 2007.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47: 253–279, 2013.
Marc G Bellemare, Will Dabney, and Re´mi Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 449–458. JMLR. org, 2017.
Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement learning. In Advances in Neural Information Processing Systems, pp. 819–827, 2014.
Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 1281–1288, 2005.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.
David Foster and Peter Dayan. Structure in the space of value functions. Machine Learning, 49 (2-3):325–346, 2002.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
Alessandro Lazaric. Knowledge transfer in reinforcement learning. PhD thesis, PhD thesis, Politecnico di Milano, 2008.
Nir Levine, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3135–3145, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323–339, 1992.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3 (1):9–44, 1988.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.
6

Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M. Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/ alphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.
Thomas J Walsh, Lihong Li, and Michael L Littman. Transferring state abstractions between mdps. In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320–3328, 2014.
Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In International Conference on Machine Learning, pp. 1899–1908, 2016.
7

A RELATED WORK
Hessel et al. (2018) present Rainbow, the DQN used in this paper, incorporating Double Qlearning (Van Hasselt et al., 2016), prioritized replay (Schaul et al., 2015), dueling networks (Wang et al., 2015), multi-step learning (Sutton, 1988), distributional reinforcement learning (Bellemare et al., 2017), and noisy networks Fortunato et al. (2017). Zahavy et al. (2016) analyze Deep Q Networks (DQNs) by observing the activation’s of the model’s last layer and saliency maps. They show that DQNs learn temporal abstractions, such as hierarchical state aggregation and options, automatically. Levine et al. (2017) show that the last layer in a deep architecture can be seen as a linear representation, and thus can be learned using standard shallow reinforcement learning algorithms. They then show that this hybrid approach improves performance on the Atari benchmark. Yosinski et al. (2014) present a large scale study of feature transferabitlity in deep neural networks. They show that transferability is negatively affected by the specialization of higher layer neurons to their original task. Furthermore, optimization difﬁculties can arise when co-adapted neurons are split during transfer, and freezing vs ﬁne-tuning over transferred layers are compared. The authors show that transferring features, even if from a very different task, can improve generalization performance even after substantial ﬁne-tuning on a new task. Lastly, a relation between the effectiveness of transfer and the distance between tasks is presented, but even in the worst case is shown to be better than random. Taylor & Stone (2009) provides a survey of transfer learning techniques in reinforcement learning. Here we will focus on the tasks that allow variation in the reward function, as we assume the state spaces are of the same cardinality, and the action spaces are equivalent in this paper. Singh (1992) and Foster & Dayan (2002) learn multiple tasks by assuming that each goal (or composite) task is composed of several elemental tasks, and then learning a set of elemental tasks that can be composed to solve each task of interest. Solving multiple MDPs has also been approached from the representation perspective, speciﬁcally with the goal of developing a shared representation that can then be used to solve all tasks. The approach proposed by Asadi & Huber (2007) focuses on learning a more efﬁcient state-space representation of the problem that will transfer between multiple tasks, and then learning options on the new representation. Walsh et al. (2006) use a similar approach, but rely on the learned state abstraction techniques to transfer between tasks. Another approach similar to state abstractions is to compare observations ( s, a, r, s ) tuples from previous tasks to new tasks, then select the best action from the most similar previously experienced observation. Lazaric (2008) uses this approach in an attempt to generalize experiences from learned to novel tasks, and then Calandriello et al. (2014) extend this approach to include sparse representations. Rusu et al. (2016) introduce progressive neural networks, which is a novel model architecture which retains a pool of pretrained models throughout training, and learns lateral connections from these to extract useful features for new tasks. This architecture leverages transfer learning while avoiding catastrophic forgetting and allowing for better incorporation of prior knowledge vs the traditional method of initialization.
8

