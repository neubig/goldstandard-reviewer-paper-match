Differentiable Allophone Graphs for Language-Universal Speech Recognition
Brian Yan, Siddharth Dalmia, David R. Mortensen, Florian Metze, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA
{byan,sdalmia}@cs.cmu.edu

arXiv:2107.11628v1 [cs.CL] 24 Jul 2021

Abstract
Building language-universal speech recognition systems entails producing phonological units of spoken sound that can be shared across languages. While speech annotations at the language-speciﬁc phoneme or surface levels are readily available, annotations at a universal phone level are relatively rare and difﬁcult to produce. In this work, we present a general framework to derive phone-level supervision from only phonemic transcriptions and phone-to-phoneme mappings with learnable weights represented using weighted ﬁnite-state transducers, which we call differentiable allophone graphs. By training multilingually, we build a universal phone-based speech recognition model with interpretable probabilistic phone-to-phoneme mappings for each language. These phone-based systems with learned allophone graphs can be used by linguists to document new languages, build phone-based lexicons that capture rich pronunciation variations, and re-evaluate the allophone mappings of seen language. We demonstrate the aforementioned beneﬁts of our proposed framework with a system trained on 7 diverse languages. Index Terms: universal phone recognition, differentiable WFST, multilingual ASR, phonetic pronunciation, allophones
1. Introduction
The objective of language-universal speech recognition is to indiscriminately process utterances from anywhere in the world and produce intelligible transcriptions of what was said [1, 2]. In order to be truly universal, recognition systems need to encompass not only speech from many languages, but also intrasentential code-switched speech [3, 4], speech with accents or otherwise non-standard pronunciations [5, 6], and speech from languages without known written forms [7, 8].
Language-universal speech recognition requires phonological units that are agnostic to any particular language such as articulatory features [9–11] or global phones [12, 13], which can be annotated through examination of audio data. While recent advancements in the related ﬁeld of multilingual speech recognition have signiﬁcantly improved the language coverage of a single system [14, 15], these works differ in that they operate on language-speciﬁc levels of surface vocabulary units [16] or phonemic units that are deﬁned with reference to the unique phonological rules of each language [17]. Prior works have avoided universal phone level annotation by implicitly incorporating this knowledge in shared latent representations that map to language-speciﬁc phonemes with neural nets [17–19].
Another approach is to learn explicit universal phone representations by relating language-speciﬁc units to their universal phonetic distinctions. Instead of relying on phone annotations, these prior works approximate universal phonological units through statistical acoustic-phonetic methods [1] or phone-to-phoneme realization rules [13,20]. Unlike the implicit latent approach, this method allows for language-universal prediction. However, performance is dependent on the clarity of

MANY-TO-ONE

[a]

/a/

ONE-TO-MANY

[k]

/k/

MANY-TO-MANY

[s]

/s/

[a:]

/q/

[S]

/S/

Figure 1: Examples showing three types of manifold mappings of phones (in brackets) to phonemes (in slashes). Many-to-one describes allophones of a phoneme. One-to-many describes a duplicitous phone that maps to multiple phonemes. Many-tomany consists of both allophones and duplicitous phones.

phone-phoneme dynamics in the selected training languages [13, 21].
We are interested in systems that can incorporate the strengths of both the implicit and explicit approaches to representing universal phones. In particular, we are interested in language-universal automatic speech recognition (ASR) systems that can 1) explicitly represent universal phones and language-speciﬁc phonemes, 2) be built using only automatically generated grapheme-to-phoneme annotations and phoneto-phoneme rules, 3) resolve naturally ambiguous phone-tophoneme mappings using information from other languages, and 4) learn interpretable probabilistic weights of each mapping.
In this work, we seek to incorporate these desiderata in a phone-based speech recognition system. We ﬁrst propose a general framework to represent phone-to-phoneme rules as differentiable allophone graphs using weighted ﬁnite-state transducers [22–27] to probabilistically map phone realizations to their underlying language-speciﬁc phonemes (§3.1). We then incorporate these differentiable allophone graphs in a multilingual model with a universal phone recognizing layer trained in an end-to-end manner, which we call the AlloGraph model (§3.2). We show the efﬁcacy of the AlloGraph model in predicting phonemes for 7 seen languages and predicting phones for 2 unseen languages with comparison to prior works (§5). More importantly, we show that our model resolves the ambiguity of manifold phone-to-phoneme mappings with an analysis of substitution errors and an examination of the interpretable allophone graph weights (§5.2). Finally we demonstrate our phone-based approach in two linguistic applications: pronunciation variation and allophone discovery (§5.3).
2. Background and Motivation
In this section, we ﬁrst introduce phone-to-phoneme mappings for manufacturing phone supervision from phoneme annotations (§2.1). Then we discuss short-comings of a baseline method representing mappings as a pass-through matrix (§2.2) to motivate our graph-based framework in the subsequent section (§3).

2.1. Phonological Units
2.1.1. Language-Speciﬁc Phonemes vs. Universal Phones
A phone n is a unit of spoken sound within a universal set N which is invariant across all languages, where N = {n1, ..., n|N|} consists of |N | total phones [12]. In contrast, a phoneme m(l) is a unit of linguistically contrastive sound for a given language l within a language speciﬁc set, where M(l) = {m(1l), ..., m(|M l) (l)|} consists of |M(l)| total phonemes [28]. Phonemes deﬁned for different languages describe different underlying sounds. Multilingual systems that conﬂate phonemes across languages have been shown to perform worse than those that treat phonemes as language-speciﬁc [13, 21].
2.1.2. Phone-to-Phoneme Mappings
For each language, the phone-to-phoneme mappings are deﬁned as a series of tuples, (ni, m(jl)), where m(jl) ∈ M(l) and ni ∈ N ⊆ N for some subset N of phones that occur as realizations in the language. Each phoneme has one or more phone realization and not all universal phones are necessarily mapped to a phoneme grounding in a particular language. Note that mappings may be imperfect in our resources [20].
Phone-to-phonemes can be one-to-one mappings, but often the relationships are manifold. As shown in Figure 1, manyto-one mappings are found in scenarios where multiple phones are allophones, or different realizations, of the same phoneme. This is the prototypical mapping type. One-to-many mappings also occur for duplicitous phones that are mapped to multiple phonemes.1 Furthermore, many-to-one and one-to-many mappings can occur together in various many-to-many forms.
2.1.3. Manufacturing Phone-Level Supervision
Since phones are ﬁne-grained distinctions of spoken sounds in the universal space, phonemes are only fuzzy approximations. Multilingual sharing between diverse languages is required to properly learn phonetic distinctions. Consider the following: One-to-One: If a phone is mapped one-to-one with a phoneme, then the learned phone representation will directly correspond to one supervising phoneme. In the multilingual setting, these direct mappings help other languages disambiguate this phone. One-to-Many: If a phone is mapped to many phonemes, then each phoneme provides supervision in proportion to their prior distributions. If the learned phonemes representations are mapped from the learned phone, phoneme confusions occur if the one-to-many mappings are not disambiguated. This ambiguity persists despite information sharing from other languages. Many-to-One: If many phones are mapped to a phoneme, each phone receives the same supervision. A second language with complementary mappings is required to learn distinct phones. Many-to-Many: When one-to-many and many-to-one mappings occur together, they can take various forms. Generally, the many-to-one portions can be resolved through multilingual sharing but the one-to-many portions would still be problematic.
1These occur in resources like [20] when the source conﬂates allophonic and morphophonemic alternations, in instances of archiphonemic underspeciﬁcation and neutralization (e.g. treating Japanese [m] as a realization of both /m/ and /N/ or English [R] as a realization of both /t/ and /d/ as in writer [ôajRô] and rider [ôa:jRô]), or—spuriously—when the grapheme-phoneme mapping is complex.

2.2. Encoding Phone-to-Phoneme as Pass-through Matrix

Prior works have shown that phone-to-phoneme mappings can
be encoded as pass-through layers that convert a phone distribu-
tion into a phoneme distribution [13]. This phone-to-phoneme encoding, which we call AlloMatrix, is a sparse matrix A(l) = {0, 1}|N |×|M(l)| where each (ni, m(jl)) tuple in the mappings desribed in §2.1.2 is represented by a(i,lj) = 1. The AlloMatrix transforms a logit vector of phones, pN = [pN i , ..., pN |N |], to a logit vector of phonemes, pM(l) = [pjM(l) , ..., p|M M((ll))|] by the dot product of the jth column of A(l) with each phone logit pN i :

|N |

pM(l) j

=

(

a(i,lj)

)(p

N i

)

(1)

i

In the many-to-one approach, this amounts to summing the phone contributions which is in accordance with our desired mapping of allophones in §2.1.2. However, in one-to-many mappings a phone logit broadcast equally to each of the phonemes. This disagrees with the deﬁnition of phone realization. Rather we state that a realized phone in an utterance is grounded to each of the mapped phonemes with probability.

3. Proposed Framework

3.1. Encoding Phone-to-Phoneme as WFST

We deﬁne the allophone graph for language l, denoted by G(l), to be a single state weighted ﬁnite-state transducer

(WFST) with a transition function π(ni, m(jl)) giving each

phone-to-phoneme mapping and a corresponding weight func-

tion w(ni, m(jl)) giving the likelihood that ni is the phonetic re-

alization of m(jl) for each transition. The allophone graph G(l)

accepts phone emission probabilities EN and transduces them

into phonemes EM(l) through WFST composition [22], which

is denoted as ◦.

EM(l) = EN ◦ G(l)

(2)

This WFST is an analogous data structure to the aforementioned matrix in §2.2, but this graphical representation of phone-to-phoneme mappings as arcs in a probabilistic transduction allows us to make two key intuitive determinations. First, many-to-one mappings are transductions of several phones into the same phoneme and therefore the phoneme posterior is given by summing over the input phone posteriors, as is also done in §2.2. Second, one-to-many mappings are transductions splitting the posterior of a single phone to several phoneme posteriors, depending on how likely those phonemes are to be groundings of the phone. In §2.2, the broadcasting method fails to do this probabilistic splitting in one-to-many scenarios, creating ambiguity.

3.2. Phone Recognition with Allophone Graphs
In this section, we apply the allophone graphs as differentiable WFST [22–27] layers in phone-based ASR systems optimized with only multilingual phoneme supervision.
In this work, we use the connectionist temporal classiﬁcation network (CTC) [29, 30] where a language-universal ENCODER maps input sequence x = [xt, ..., xT ] to a sequence of hidden representations h = [ht, ..., hT ], where ht ∈ Rd. The phone emission probabilities EN ∪∅ are given by the afﬁne projection of h followed by the softmax function, denoted as

Table 1: Results presenting the performances of our proposed AlloGraph models with our implementations of Phoneme-Only and AlloMatrix baselines, as measured by language-speciﬁc phoneme error-rate (%) for seen languages and universal phone error-rate (%) for unseen languages. Performances on unseen languages were evaluated using phone-level annotations for the Tusom and Inuktitut corpora. Note that while our proposed AlloGraph and our baseline AlloMatrix models produce both phone and phonemelevel predictions, the Phoneme-Only approach only recognizes language-speciﬁc phonemes. The averaged totals across unseen/seen are shown in bold and the best performing models in each category are shown in bold.

Uses

Seen (Phoneme Error Rate %)

Unseen (Phone Error Rate %)

Model Type

Model Name

Phones Eng Tur Tgl Vie Kaz Amh Jav Total Tusom Inuktitut Total

Phoneme-Only Multilingual-CTC [17]

 25.3 27.7 28.5 31.9 31.5 28.6 35.2 29.8

No Phone Predictions

AlloMatrix

Allosaurus [13]

 26.5 27.6 33.1 32.0 31.9 28.2 39.0 31.2 91.2

96.7

94.0

AlloGraph

Our Proposed Model

 26.0 28.6 28.2 31.9 32.5 29.1 36.2 30.5 81.2

85.8

84.1

AlloGraph

+ Universal Constraint (UC)  27.3 28.7 29.9 32.5 35.1 30.9 36.6 31.6 80.5

79.9

80.2

SOFTMAXOUT.2 To handle the blank token ∅ used in CTC to represent the null emission [29], we add the ∅ → ∅ transition
as an additional arc in the language-speciﬁc allophone graphs G(l). Phone and phoneme emissions are thus given by:

h = ENCODER(x)

(3)

EN ∪∅ = SOFTMAXOUT(h)

(4)

EM(l)∪∅ = EN ∪∅ ◦ G(l)

(5)

Equation 5 shows the CTC speciﬁc form of the general phoneto-phoneme emission transduction shown in Equation 2. During training, we maximize the likelihood of the ground-truth phonemes y = [y1, ..., yS], where ys ∈ M(l) and S is the length of the ground-truth which is at most the length of the input T , by marginalizing over all possible CTC alignments using the forward-backward computation [29, 30].
We refer to this multilingual CTC architecture with allophone graphs as our proposed AlloGraph model. In the vanilla AlloGraph, we allow the weights of G(l) to freely take on any values. This is a loose-coupling of phone and phoneme emissions where each G(l) may amplify or reduce the phone posteriors; for instance, this allows G(l) to learn cases where a phone is universally rare but is a prominent realization in language l.
While loose-coupling of phone and phoneme emissions is beneﬁcial to language-speciﬁc phoneme recognition, it dilutes supervision to the universal phone layer. We address this by enforcing a tight-coupling of phone and phoneme emissions such that the phone posterior is only isometrically transformed:
m(l)∈M (l) w(ni, m) = 1, where M (l) is the subset of phonemes M(l) that ni is mapped to in language l. Now, Equation (5) exactly sums phone posteriors for many-to-one and splits phone posteriors for one-to-many in the manner that we desire, as stated in §3.1. We call this tightly-coupled variant the AlloGraph + Universal Constraint (UC) model.

4. Data and Experimental Setup
Data: We use the English LDC Switchboard Dataset [32–34] and 6 languages from the IARPA BABEL Program: Turkish, Tagalog, Vietnamese, Kazakh, Amharic and Javanese [35]. These datasets contain 8kHz recordings of conversational speech each containing around 50 to 80 hours of training data, with an exception of around 300 hours for English. We also consider two indigenous languages with phone level annotations, Tusom [36] and Inukitut, during evaluation only. We ob-
2In training, logits corresponding to unmapped phones in a particular language are masked prior to being softmax normalized similar to [31].

Table 2: Results showing the performance of the AlloMatrix and AlloGraph models on two unseen language, as measured by Phone Error Rate (PER), Substitution Error Rate (SER), and Articulatory Feature Distance (AFD). AFD measures the severity of substitution errors, computed via the distance between vectors of 22 articulatory features corresponding to each phone.

Model
AlloMatrix
AlloGraph + UC

Tusom PER SER AFD 91.2 65.6 12.3 81.2 56.8 8.7 80.5 54.9 7.8

Inuktitut PER SER AFD 96.7 75.3 12.4 85.8 65.8 8.4 79.9 59.9 7.8

tain phonemic annotations using Epitran for auto grapheme-tophoneme [28] and phone-to-phoneme rules from Allovera [20]. Experimental Setup: All our models were trained using the ESPnet toolkit [37] with differentiable WFSTs implemented using the GTN toolkit [26]. To prepare our speech input features we ﬁrst upsample the audio to 16kHz, augment it by applying a speed perturbation of 0.9 and 1.1, and then extract global meanvariance normalized 83 log-mel ﬁlterbank and pitch features. Input frames are processed by an audio encoder with convolutional blocks to subsample by 4 [37] before feeding to 12 transformer-encoder blocks with a feed-forward dim of 2048, attention dim of 256, and 4 attention heads. We augment our data with the Switchboard Strong (SS) augmentation policy of SpecAugment [38] and apply a dropout of 0.1 for the entire network. We use the Adam optimizer to train 100 epochs with an inverse square root decay schedule, a transformer-lr scale [37] of 5, 25k warmup steps, and an effective batchsize of 768.

5. Results
In Table 1, we show the results of our AlloGraph and AlloGraph + UC models. As mentioned in §4, we use Tusom and Inuktitut as two unseen languages with phone level annotations to evaluate our language-universal predictions; since these languages are unseen our model does not know their phoneme sets or which phones appear as realizations, allowing us to assess how universal our phone-based predictions are. On these two unseen languages our AlloGraph model outperforms our AlloMatrix baseline based on [13] by an average of 9.9 phone errorrate (%). When using the Universal Constraint described in §3.2, our approach gains an additional 3.9 phone error-rate improvement. The AlloGraph models make fewer substitution errors than the AlloMatrix baseline, and the substitutions are also less severe; we examine these improvements further in §5.1.
Table 1 also shows the language-speciﬁc phoneme level performance of the AlloGraph model on 7 seen languages. Note

Table 3: Results showing the top 3 phone confusion pairs of the AlloMatrix and AlloGraph + UC models on two unseen languages. Confusion pairs are denoted as [correct] → [incorrect]. Articulatory Feature Distance (AFD) measures the severity of each confusion, computed via the distance between vectors of 22 articulatory features corresponding to each phone.

Model AlloMatrix AlloGraph AlloGraph + UC

Tusom
Confusion AFD
[1] → [Bﬂ] 15 [@] → [Bﬂ] 13 [@] → [s’] 17
[i] → [i:] 2 >
[k] → [kp] 4 [a] → [a:] 2
[a] → [5] 4 [@] → [5] 2 [a] → [A] 2

Inuktitut

Confusion AFD

[a] → [Bﬂ] 13 [i] → [Bﬂ] 13 [u] → [s’] 23

[a] → [A] 3

[u]

→

¯ [o]

4

[a] → [a:] 2

[q] → [k] 2 [a] → [5] 4 [i] → [I] 2

that these languages are annotated with phonemes as described in §4 but not with phones. Here our AlloGraph model slightly outperforms the AlloMatrix baseline, but both show degradation compared to our Phoneme-Only3 baseline based on [17]. We observe that models placing emphasis on learning universal phones do so with some cost to the language-speciﬁc level.
The AlloGraph is advantageous in jointly modeling phones and phonemes compared to the AlloMatrix baseline due to learned disambiguations of phone-to-phoneme mappings; we examine this beneﬁt further in §5.2.

5.1. Universal Phone Recognition for Unseen Languages
As shown in Table 2, the improvements of the AlloGraph models over the AlloMatrix baseline come from reduced phone substitution errors. In addition to making fewer substitution errors, the AlloGraph models also make less severe substitutions than the AlloMatrix baseline. We quantify this severity by computing the averaged distance between articulatory feature vectors [39] between the ground truth and incorrectly predicted phones for all substitution errors. Compared to the AlloMatrix, the substitutions made by the AlloGraph and AlloGraph + UC models are 31% and 37% closer in articulatory feature distance (AFD).
The high AFD of the AlloMatrix baseline results from degenerate behavior in which vowels are frequently confused for plosives, as shown by the top confusion pairs in Table 3. On the other, the top confusion pairs of the AlloGraph models are between related vowels which are proximate in the articulatory feature space. Thus the AlloGraph models produce intelligible phone transcriptions, while the AlloMatrix model fails. For qualitative examples of phone recognition, please see §A.1.

5.2. Probabilistic Phone-to-Phoneme Disambiguation
An added beneﬁt of our model is the ability to interpret the weights of learned AlloGraphs, which show disambiguations of ambiguous phone-to-phoneme mappings. As shown in Figure 2, our AlloGraph + UC model distributes phone emissions to multiple phonemes in the one-to-many and many-to-many scenarios. These probabilities can be interpreted as prior distri-
3Phoneme-Only [17] directly maps the shared ENCODER hidden states to language-speciﬁc phoneme level SOFTMAXOUT, replacing the shared phone level in Equation (4). Thus there are no phone predictions.

ONE-TO-MANY [k] 0.0 /q/
1.0 /k/
(JAVANESE)

MANY-TO-MANY

1.0

[s]

/s/

0.75 0.0

[S]

/S/

0.25

(TAGALOG)

Figure 2: Examples of disambiguated phone-to-phoneme mappings using the interpretable weights of our AlloGraph + UC model, where each [phone] is probabilistically mapped to a /phoneme/. In the one-to-many example from Javanese, [k] is predominantly a realization of /k/. In the many-to-many example from Tagalog, [s] is predominantly a realization of /s/ while [S] is a realization of /s/ 75% of the time and /S/ otherwise.

% Phoneme Substitution Rate (↓)

20

15

12.51 1.79

10

10.72

18.02 7.17

MAPPING TYPE Any-to-One Any-to-Many

10.85

12.3 1.57 10.73

Phoneme-Only AlloMatrix AlloGraph
Figure 3: Results comparing the performances of our baseline Phoneme-Only, baseline AlloMatrix, and proposed AlloGraph models on a high phone-to-phoneme complexity language, Tagalog, as measured by phoneme substitution errorrate (%). The any-to-one category includes phonemes in oneto-one and many-to-one mappings, and any-to-many includes phonemes in one-to-many and many-to-many mappings.

butions of each mapping captured by the allophone graph and can be used to determine the relative dominance of each arc in manifold mappings that can be otherwise difﬁcult to explain.
The performance of AlloGraph + UC on languages with complex phone-phoneme mappings, such as Tagalog and Javanese, is greatly improved over the AlloMatrix baseline. In these languages, phones are frequently deﬁned as realizations of multiple ostensive phonemes and there are many allophones of each phoneme. As shown in Figure 3 these ambiguous mappings are especially detrimental to the AlloMatrix model, which produces a high number of phoneme substitution errors compared to our AlloGraph model and Phoneme-Only baseline.

5.3. Linguistic Applications
In this section, we demonstrate the efﬁcacy of phone-based predictions from our AlloGraph + UC model in two applications.
As shown in Table 4, our AlloGraph + UC model produces different phonetic realizations of a single phonemic pronunciation. By collecting all of the phonetic realizations for correct phonemic transcriptions of the word ‘hello’ uttered by numerous speakers across test sets in our conversational corpora, we automatically identiﬁed the most frequent phonetic pronunciations. These qualitative examples suggest that dynamic methods for building lexicons using universal phone recognition systems can capture diverse pronunciations that can bolster knowledge sets [5]. This may beneﬁt pronunciation-sensitive tasks like code-switched [4] or accented speech recognition [40].

Table 4: Results showing the pronunciations of the word ‘hello’ across the 7 languages discovered by our AlloGraph + UC model, as shown in phonemic and phonetic forms. Pronunciation variations between different speakers in our conversational test set are captured at the phonetic level. We present the 3 most frequent phone-based pronunciations and their percentages.

Pronunciations

Lang. Word Phonemic

Phonetic

Eng Tur Tgl Vie Kaz Amh Jav

hello alo hello a lô
halo

/h@low/ /alo/ /hello/ /Pa lo/ /Allo/ /helo/ /halo/

[halo] 54% [h@low] 8% [hElow] 8%

[a:ëo] 100%

-

-

[hello] 99% [hellu] 1%

-

[Pa lo] 100%

-

-

[A”l”lo] [H¯elo]

75% [A6”l”l o] 20% [6”l”l o] 5%

99% [h¯elo]

1%

-

[halo] 88% [hOlo] 11% [helo] 1%

Table 5: Results showing the most frequent triphone contexts and realization rates of various phones mapped to the phonemes /b/ and /@/ in Amharic, as discovered by our AlloGraph + UC model on our test corpus. Phones that are not mapped to any phoneme, such as [5] in Amharic, can still appear as hypothesized realizations suggesting new phone-to-phoneme mappings.

Phone-toPhoneme
[b] → /b/ [Bﬂ] → /b/
[@] → /@/ [5] → /@/ [E] → /@/ [O] → /@/

Realization Rate (%)
64.5 29.7
32.7 29.2 16.4 13.8

Predeﬁned Mapping
 
   

Frequent Triphone Contexts

[#b5] [OBﬂe]
[n@w] [P5l] [gEr] [POw]

[#b@] [@BﬂH]
[d@H] [s5l] [bEr] [POj]

[#bI] [#BﬂI]
[d@t] [s5m] [lEt] [POn]

Since the AlloGraph + UC model produces joint alignments of phones and phonemes for seen languages, it can also discover the allophone realization rates and triphone contexts in test corpora (Table 5). Our method can also hypothesize new allophones such as the the phone [5] which is not mapped to any of the phonemes in Amharic [20]. One important step in language documentation is discovering and deﬁning the relationship between phones and phonemes [7], ensuring that mappings are exhaustive but devoid of spurious pairs. Automatic, data-driven methods to generate phone-phoneme mappings allow linguists to discover these relationships more effectively.
6. Conclusion and Future Work
We present differentiable allophone graphs for building universal phone-based ASR using only language-speciﬁc phonemic annotations and phone-to-phoneme rules. We show improvements in phone and phoneme prediction over prior works. More importantly, our framework enables model interpretability and unique linguistic applications, such as phone-based lexicons and allophone discovery. In future work, we will seek to incorporate contexually dynamic phone-to-phoneme mappings using convolutional or attention-based WFST weights. We hope that the insights of this work stimulate research on learnable representations of other linguistic rules, such as articulatory features [11], phonotactics [41], and cross-lingual mappings [42] in multilingual speech processing.
7. Acknowledgements
We thank Xinjian Li, Awni Hannun, Alex Shypula, and Xinyi Zhang for helpful discussions. This work was supported in part

by grants from National Science Foundation for Bridges PSC (ACI-1548562, ACI-1445606) and DARPA KAIROS program from the Air Force Research Laboratory (FA8750-19-2-0200). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.
8. References
[1] J. Köhler, “Multilingual phone models for vocabularyindependent speech recognition tasks,” Speech Communication, 2001.
[2] T. Schultz and A. Waibel, “Language-independent and languageadaptive acoustic modeling for speech recognition,” Speech Communication, 2001.
[3] B. E. Bullock and A. J. E. Toribio, The Cambridge handbook of linguistic code-switching. Cambridge University Press, 2009.
[4] K. Li, J. Li, G. Ye, R. Zhao, and Y. Gong, “Towards codeswitching asr for end-to-end ctc models,” in Proc. ICASSP, 2019.
[5] N. Coupland, Style: Language variation and identity. Cambridge University Press, 2007.
[6] S. Sun, C.-F. Yeh, M.-Y. Hwang, M. Ostendorf, and L. Xie, “Domain adversarial training for accented speech recognition,” in Proc. ICASSP, 2018.
[7] N. P. Himmelmann et al., “Language documentation: What is it and what is it good for,” Essentials of language documentation, 2006.
[8] S. Hillis, A. P. Kumar, and A. W. Black, “Unsupervised phonetic and word level discovery for speech to speech translation for unwritten languages.” in Proc. Interspeech, 2019.
[9] S. Stuker, F. Metze, T. Schultz, and A. Waibel, “Integrating multilingual articulatory features into speech recognition,” in Eighth European Conference on Speech Communication and Technology, 2003.
[10] K. Livescu, P. Jyothi, and E. Fosler-Lussier, “Articulatory featurebased pronunciation modeling,” Computer Speech & Language, 2016.
[11] X. Li, S. Dalmia, D. Mortensen, J. Li, A. Black, and F. Metze, “Towards zero-shot learning for automatic phonemic transcription,” in Proc. AAAI, 2020.
[12] T. Schultz, “Globalphone: a multilingual speech and text database developed at karlsruhe university,” in Seventh International Conference on Spoken Language Processing, 2002.
[13] X. Li, S. Dalmia, J. Li, M. Lee, P. Littell, J. Yao, A. Anastasopoulos, D. R. Mortensen, G. Neubig, A. W. Black et al., “Universal phone recognition with a multilingual allophone system,” in Proc. ICASSP, 2020.
[14] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, “Massively multilingual adversarial speech recognition,” in Proceedings of NAACL-HLT, 2019.
[15] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Synnaeve, and R. Collobert, “Massively multilingual asr: 50 languages, 1 model, 1 billion parameters,” Proc. Interspeech, 2020.
[16] B. Li, Y. Zhang, T. Sainath, Y. Wu, and W. Chan, “Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes,” in Proc. ICASSP, 2019.
[17] S. Dalmia, R. Sanabria, F. Metze, and A. W. Black, “Sequencebased multi-lingual low resource speech recognition,” in Proc. ICASSP, 2018.
[18] A. Stolcke, F. Grezl, M.-Y. Hwang, X. Lei, N. Morgan, and D. Vergyri, “Cross-domain and cross-language portability of acoustic features estimated by multilayer perceptrons,” in Proc. ICASSP, 2006.
[19] K. Vesely`, M. Karaﬁát, F. Grézl, M. Janda, and E. Egorova, “The language-independent bottleneck features,” in Proc. SLT, 2012.

[20] D. R. Mortensen, X. Li, P. Littell, A. Michaud, S. Rijhwani, A. Anastasopoulos, A. W. Black, F. Metze, and G. Neubig, “Allovera: a multilingual allophone database,” in Proc. LREC, 2020.
[21] J. Kohler, “Multi-lingual phoneme recognition exploiting acoustic-phonetic similarities of sounds,” in Proc. ICSLP, 1996.
[22] M. Mohri, F. Pereira, and M. Riley, “Weighted ﬁnite-state transducers in speech recognition,” Computer Speech & Language, 2002.
[23] N. Moritz, T. Hori, and J. L. Roux, “Semi-supervised speech recognition via graph-based temporal classiﬁcation,” Proc. ICASSP, 2021.
[24] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schlüter, and H. Ney, “Returnn: The rwth extensible training framework for universal recurrent neural networks,” in Proc. ICASSP, 2017.
[25] Y. Shao, Y. Wang, D. Povey, and S. Khudanpur, “ PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for Endto-End ASR,” in Proc. Interspeech 2020, 2020.
[26] A. Hannun, V. Pratap, J. Kahn, and W.-N. Hsu, “Differentiable weighted ﬁnite-state transducers,” arXiv preprint arXiv:2010.01003, 2020.
[27] D. Povey, F. Kuang, H. Qiu et al., “k2 fsa and fst autograd integration,” https://github.com/k2-fsa/k2, 2021.
[28] D. R. Mortensen, S. Dalmia, and P. Littell, “Epitran: Precision G2P for many languages.” in LREC, 2018.
[29] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006.
[30] Y. Miao, M. Gowayyed, and F. Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in Proc. ASRU, 2015.
[31] S. Dalmia, X. Li, A. W. Black, and F. Metze, “Phoneme level language models for sequence based low resource asr,” in Proc. ICASSP, 2019.
[32] J. Godfrey and E. Holliman, “Switchboard-1 Release 2 LDC97S62,” Web Download. Philadelphia: Linguistic Data Consortium, 1993.
[33] L. D. Consortium, “2000 HUB5 English Evaluation Transcripts LDC2002T43,” Philadelphia: Linguistic Data Consortium, 2002.
[34] ——, “2000 HUB5 English Evaluation Speech LDC2002S09,” Philadelphia: Linguistic Data Consortium, 2002.
[35] “Full Language Packs (FLP) released by the IARPA Babel Research Program (IARPA-BAA-11-02): IARPAbabel105b-v0.4, IARPA-babel106-v0.2g, IARPA-babel107bv0.7, IARPAbabel302b-v1.0a, IARPAbabel307b-v1.0b, IARPAbabel402b-v1.0b.”
[36] D. R. Mortensen, J. Picone, X. Li, and K. Siminyu, “Tusom2021: A phonetically transcribed speech dataset from an endangered language for universal phone recognition experiments,” arXiv preprint arXiv:2104.00824, 2021.
[37] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018.
[38] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech, 2019.
[39] D. R. Mortensen, P. Littell, A. Bharadwaj, K. Goyal, C. Dyer, and L. Levin, “PanPhon: A resource for mapping IPA segments to articulatory feature vectors,” in Proc. COLING, 2016.
[40] T. Viglino, P. Motlicek, and M. Cernak, “End-to-end accented speech recognition.” in Proc. Interspeech, 2019.

[41] S. Feng, P. Z˙ elasko, L. Moro-Velázquez, A. Abavisani, M. Hasegawa-Johnson, O. Scharenborg, and N. Dehak, “How Phonotactics Affect Multilingual and Zero-shot ASR Performance,” in Proc. ICASSP, 2021.
[42] K. Hu, A. Bruguier, T. N. Sainath, R. Prabhavalkar, and G. Pundak, “Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models,” in Proc. Interspeech, 2019.

A. Appendix
A.1. Qualitative Examples of Universal Phone Recognition
In Table 6, we show qualitative examples of phone transcriptions on two unseen languages along with the phone error rate (PER), substitution error rate (SER), and articulatory feature distance (AFD). As discussed in §5.1, the AlloGraph models produce intelligible results while the AlloMatrix baseline frequently substitutes vowels for plosives, resulting in high AFD and phone transcriptions that are mostly uninterpretable.

Table 6: Qualitative examples of universal phone transcriptions of the AlloMatrix baseline and AlloGraph models on two unseen languages, Tusom and Inuktitut. The errors of each phone output sequence are highlighted in red. The phone error rate (PER), substitution error rate (SER), and articulatory feature distance (AFD) of each sequence are also shown.

UNSEEN LANGUAGE: Tusom

Model / Source Phone Output PER SER

AlloMatrix
AlloGraph + UC Ground-Truth

[s’s’Bﬂ] [@k1ôu]
[P1kru] [P1khru]

100.0 60.0

80.0 60.0 20.0 20.0

-

-

AlloMatrix AlloGraph + UC Ground-Truth

[bs’Bﬂgs’ô] [b5Ng¨s’ô] [b5Ng¨Yr] [baNg¨or]

83.3 83.3

66.6 66.6

50.0 50.0

-

-

AlloMatrix
AlloGraph + UC Ground-Truth

[Bﬂks’bs’Bﬂ] [Poku:bu:Se:] [Pokubu:Se:] [Pukxuk@Sue]

90.0 50.0

70.0 50.0

60.0 40.0

-

-

UNSEEN LANGUAGE: Inuktitut

Model / Source Phone Output PER SER

AlloMatrix AlloGraph + UC Ground-Truth

[ks’Bs’k ks’Bs’k] 60.0 60.0

[kimuckh kimu] 50.0 30.0

[kINok kINuk]

30.0 30.0

[kiNuk kiNuk]

-

-

AlloMatrix AlloGraph + UC Ground-Truth

[SBs’k SBks’] [s1ka:k su:ka:k] [”suk2k suk2k] [sukaq sukaq]

80.0 70.0

60.0 60.0

50.0 50.0

-

-

AlloMatrix AlloGraph + UC Ground-Truth

[s’ks’tP s’ks’t] [i:ki:kh i:ki:kh] [ikIp ikIpq] [ikiq ikiq]

87.5 75.0

75.0 75.0

62.5 50.0

-

-

AFD
13.3 4.7 2.0 -
12.2 8.3 4.0 -
15.4 5.6 6.5 -
AFD
18.3 6.0 2.7 -
9.7 2.3 2.8 -
13.8 2.7 6.5 -

