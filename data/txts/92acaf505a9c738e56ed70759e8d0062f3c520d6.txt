Toward Streaming ASR with Non-Autoregressive Insertion-based Model
Yuya Fujita1, Tianzi Wang2, Shinji Watanabe3, Motoi Omachi1
1Yahoo Japan Corporation, Tokyo, JAPAN 2Johns Hopkins University, MD, USA, 3Carnegie Mellon University, PA, USA
{yuyfujit, momachi}@yahoo-corp.jp, wtianzi1@jhu.edu, shinjiw@iee.org

arXiv:2012.10128v2 [eess.AS] 16 Jul 2021

Abstract
Neural end-to-end (E2E) models have become a promising technique to realize practical automatic speech recognition (ASR) systems. When realizing such a system, one important issue is the segmentation of audio to deal with streaming input or long recording. After audio segmentation, the ASR model with a small real-time factor (RTF) is preferable because the latency of the system can be faster. Recently, E2E ASR based on non-autoregressive models becomes a promising approach since it can decode an N -length token sequence with less than N iterations. We propose a system to concatenate audio segmentation and non-autoregressive ASR to realize high accuracy and low RTF ASR. As a non-autoregressive ASR, the insertion-based model is used. In addition, instead of concatenating separated models for segmentation and ASR, we introduce a new architecture that realizes audio segmentation and non-autoregressive ASR by a single neural network. Experimental results on Japanese and English dataset show that the method achieved a reasonable trade-off between accuracy and RTF compared with baseline autoregressive Transformer and connectionist temporal classiﬁcation. Index Terms: ASR, end-to-end, non-autoregressive, audio segmentation
1. Introduction
End-to-end (E2E) models have become a promising option to realize practical automatic speech recognition (ASR) systems based on the signiﬁcant improvement of the ASR performance [1–8]. In particular, the E2E model is suitable for ASR systems run on a device like a smartphone which only has limited computing capability. Most of the E2E models are composed of a single neural network and its computational optimization is simple. Therefore, many techniques to make the model footprint lower like quantization and compression, are easily applied. This leads to several works that aim to run E2E ASR models on a device like smartphones [9–11].
One important issue to be solved when realizing a practical ASR system is the segmentation of the input audio. If the input audio is long or is fed in a streaming way, the audio should be segmented into an appropriate length of utterances to avoid large memory consumption. In particular, estimating the end of the speech segment is important because it also affects latency which directly relates to the user’s subjective impression of the performance of the ASR system. However, realizing low latency and high accuracy is a trade-off. One way to realize such an ASR system is to separately employ models for segmentation and ASR.
In such a scenario, it is obvious that making the decoding process faster leads to lower latency. Therefore, decoding with a small real-time factor (RTF) is important. Recently, nonautoregressive Transformer (NAT) is intensively investigated in

the research ﬁeld of neural machine translation [12–15]. It aims to realize faster decoding than an autoregressive Transformer at the expense of a small loss of accuracy. The main contribution to the RTF improvement of NAT is the ability to decode N length token sequences with less than N iterations, which can not be achieved by autoregressive Transformer. Mask-predict, one of the NAT models, was applied to ASR and realized signiﬁcant improvement of RTF with a small degradation in accuracy [16].
In this paper, we aim to realize ASR of streaming input or long recording with non-autoregressive ASR. Among several works of non-autoregressive ASR [16–19], we propose to use the insertion-based model recently proposed in [19] because of its high accuracy and small numbers of iterations required for decoding. The model jointly trains connectionist temporal classiﬁcation (CTC) [20] and insertion-based models. It achieves better accuracy with approximately log2(N ) iterations during inference than autoregressive Transformer with greedy decoding, while the other non-autoregressive models like mask CTC [17] does not reach this performance. Our ﬁrst attempt in this proposed framework is to concatenate audio segmentation and this insertion-based model toward streaming ASR.
In addition, we propose to integrate audio segmentation and non-autoregressive ASR in a single neural network by exploiting the CTC part of the insertion-based model with causal self-attention. Causal self-attention is realized by block selfattention which is similar to Transformer XL [21]. It computes attention weights inside a limited context (block). In general, introducing causal self-attention degrades accuracy. However, once the audio segment is ﬁxed, the insertion-based model can reﬁne the hypothesis by looking at the whole acoustic feature in the segment with a smaller number of iterations than the autoregressive Transformer. Therefore, the accuracy can be recovered while achieving faster inference than the autoregressive Transformer.
Experimental results showed the proposed method achieved a good balance between accuracy and RTF compared with baseline autoregressive Transformer and CTC. To the best of our knowledge, this is the ﬁrst work to apply non-autoregressive ASR to streaming or long recording.
2. Related Work
Several works of non-autoregressive Transformer for ASR have been proposed [16–18]. However, all of them do not aim for recognizing streaming input or long recording. There are some works of realizing audio segmentation and ASR in a single model. In [22], it is proposed to use the CTC part of a jointly trained CTC and attention-based model. Integrating an endpointer into a single RNN-T model and second-stage rescoring with an attention-based model is proposed in [23]. It assumes a voice search application which only detects a single endpoint of

streaming audio. Both of them uses autoregressive model while ours are non-autoregressive one.

3. Insertion-based model with CTC
In this section, the insertion-based model used in this work, KERMIT (Kontextuell Encoder Representations Made by Insertion Transformations) [24], is ﬁrst introduced. Then, joint modeling with CTC proposed in [19] is explained.

3.1. Insertion-based model: KERMIT

First, the general formulation of E2E ASR models and insertion-based models is described. Suppose X = (xt ∈ Rd|t = 1, · · · , T ) is an acoustic feature sequence whose dimension is d. The output token sequence is deﬁned as C = (cn ∈ V|n = 1, · · · , N ). T is the length of the acoustic feature, N is the length of output token sequence, and V is a set of
distinct tokens. The E2E ASR model is a probability distribution over the output token sequence C given the acoustic feature sequence X, i.e. pe2e(C|X). It is modeled by a single neural
network.
In the case of insertion-based models, the probability distribution pe2e(C|X) is assumed to be marginalized over insertion order Z:

pe2e(C|X) = p(C, Z|X) = p(CZ |X)p(Z). (1)

Z

Z

Insertion order Z represents the permutation of the token se-
quence C, e.g. if C = (A, B, C, D) and Z = (3, 1, 2, 4), CZ = (C, A, B, D). For the KERMIT case, let cZi be a token to be inserted and liZ be a position where the token is inserted at the i-th generation step under an insertion order Z, p(CZ |X)
in Eq. (1) is deﬁned as:

I
p(CZ |X) =: p
i=1 I
=p
i=1

cZi , liZ | cZ0 , l0Z , · · · , cZi−1, liZ−1 , X

cZi , liZ |cZ 0:i−1, X ,

(2)

where i = 1, · · · , I is the index of generation step and cZ 0:i−1

is the sorted token sequence at the (i − 1)-th generation step.

The posterior in Eq. (2) is modeled by only the encoder block of

Transformer. The output at the ﬁnal layer of the encoder block

is

sliced

as

Htok

∈

dsa ×i
R

then

used

to

calculate

the

posterior:

p cZi , liZ |cZ 0:i−1, X =: p(cZi |liZ , Htok) p(liZ |Htok) ,

Token prediction

Position prediction
(3)

where dsa is the dimension of self-attention. The token and po-
sition prediction term are calculated by a linear transformation of Htok and softmax. Non-autoregressive parallel decoding is
possible using only the token prediction term in Eq. (3):

cˆ = arg max p c|l, cZ 0:i−1, X .

(4)

c

When p(Z) is the balanced binary tree (BBT) insertion order proposed in [15], decoding ﬁnishes empirically with log2(N ) iterations. The BBT order is to insert the centermost tokens of the current hypothesis. For example, given a sequence C = (c1, · · · , c9), the hypothesis grows based on the tree structure like (c5) → (c3, c5, c7) → (c2, c3, c4, c5, c6, c7, c8) → (c1, c2, c3, c4, c5, c6, c7, c8, c9).

Figure 1: Schematic diagram of KERMIT and joint modeling with CTC.

Figure 2: Decoding example of KERMIT and CTC.

3.2. Joint modeling with CTC
Next, joint modeling of KERMIT and CTC is introduced [19]. In general, joint modeling is to model a joint distribution over different sequences. Suppose Y is another output token sequence for joint modeling. Then, the posterior in Eq. (2) is modiﬁed as:
p(CZ |X) =: p(CZ , Y |X) = p(Y |X, CZ )p(CZ |X). (5)

The term p(Y |X, CZ ) in Eq. (5) can be any kind of probability distribution. We set Y = C and choose CTC for the term because joint modeling with CTC results in faster convergence and performance improvement [4]. Suppose A is a sequence of tokens including a blank symbol b , i.e. A = (at ∈ V ∪ { b }|t = 1, · · · , T ). F(·) is a function which deletes repetitions and the blank symbol from the sequence A hence F(A) = Y . The CTC probability is formulated as:

p(Y |X, CZ ) =:

p(A|X, CZ ).

(6)

A∈F -1(Y )

Because all the output of KERMIT is dependent on both the

acoustic feature sequence and the token sequence, the ﬁnal

output of the encoder block of KERMIT is sliced as Hfeat ∈

dsa ×T
R

and

used:

p(A|X, CZ ) =: p(A|Hfeat).

(7)

This process is depicted in Figure 1. It can be seen as a multitask training of the two terms in Eq. (5), i.e. p(Y |X, CZ ) in Eq. (6) and p(CZ |X) in Eq. (2). The formulation means CTC is
reinforced by insertion-based token sequence generation. During decoding, either of the CTC part p(Y |X, CZ ) or the insertion part p(CZ |X) in Eq. (5) can be used as a ﬁnal result.
Figure 2 shows an example of the decoding process. The error
in the original CTC result (Y in the ﬁrst iteration) was reﬁned
after the iterations.

Figure 3: Example of the block self-attention (Tq = 4, L = 2). In the case of block self-attention, dotted lines are not computed.
4. Audio segmentation and non-autoregressive decoding
This section proposes to extend the model described in Section 3.2 to enable joint audio segmentation and non-autoregressive decoding.

4.1. Preliminary: Block self-attention

In the model described in Section 3.2, self-attention is the component that prevents the model being used for audio segmentation of long recordings. This is because self-attention computes attention weights over the whole input sequence. In order to avoid this, block self-attention, which computes attention weights inside a limited context (block), is introduced. This is almost the same structure as Transformer XL [21] except this formulation uses future context inside a block and passes gradients to the previous block.
First, we deﬁne multi-headed attention [25] as follows:
Uh = Attention(QWQh, KWKh, VWVh), (8) MultiHead(Q, K, V) = Concat(U1, · · · , UH )WO, (9)

where Q ∈ RT q×datt , K ∈ RT k×datt , and V ∈ RT v×datt denote query, key, and value matrices, respectively. T q, T k, and T v, are the length of each elements and datt is the dimension of the
input to MultiHead(·). h = 1, · · · , H is the index of the head. WhQ, WhK, WhV ∈ Rdatt×datt/H and WO ∈ Rdatt×datt .
Self-attention is the multi-headed attention whose inputs
are the same, i.e. Q = K = V:

SA(Q) = MultiHead(Q, Q, Q).

(10)

Block self-attention introduces block length B and its index b, and deﬁne query matrix of b-th block Qb ∈ RB×datt as:

Qb = qb×B , qb×B+1, · · · , q(b+1)×B−1 .

(11)

Then, block self-attention at b-th block is deﬁned by setting Q = Qb, K = V = [Qb−1, Qb], i.e.:

BlockSA(Qb) = MultiHead(Qb, [Qb−1, Qb], [Qb−1, Qb]). (12)

Note that when computing the b-th block, only the B frames of
future context are required. This avoids computing attention
weights over the whole input sequence, hence realizing seg-
mentation of long or streaming input audio. Another beneﬁt is computational cost. The self-attention of length T q requires O (T q)2 computations. In the block self-attention with block length B requires O B2 × T q/B . This computation is de-
picted in Figure 3.

4.2. Proposed joint modeling with block self-attention
First, in order to apply joint modeling of KERMIT and CTC in Section 3.2, we further extend block self-attention in Eq. (12) to consider extra input M ∈ RT m×datt . This extension is realized by setting K = V = [Qb−1, Qb, M] of multi-headed attention, i.e.:

ExtBlockSA(Qb, M) = MultiHead(Qb, [Qb−1, Qb, M], [Qb−1, Qb, M]), (13)

where T m is the length of extra input.
Now, we consider the acoustic feature sequence X. For
ease of explanation, the acoustic feature sequence is assumed
to be segmented, e.g. training phase where segment infor-
mation is given or inference phase where the segment is es-
timated. The segmented acoustic feature sequence is passed
to an audio embedding layer to obtain an embedding matrix XE ∈ RT SS×datt , where T SS is the segmented sequence length after subsampling. In order to apply block self-attention, same as Eq. (11), b-th block of XE is deﬁned as XEb = xb×B , xb×B+1, · · · , x(b+1)×B−1 where b = 0, . . . , TBSS − 1.
In our joint modeling, we use the partial hypothesis Chyp = (c1:i) at i-th generation step as an extra input, which is artiﬁcially generated according to p(Z) in the case of the training
phase. It is passed to a character embedding layer to obtain an embedding matrix Chyp ∈ R , Nhyp×datt where N hyp is the length of the hypothesis.
Thus, the proposed forward pass of KERMIT using block
self-attention deﬁned by Eq. (13) is


 Zb(j)  
Y(j)
    

=ExtBlockSA(Z(bj−1), Y(j−1)) for b = 0,

=MultiHead(Y(j−1), K, V)

where K = V = [Z(j−1)]

T ss

b

b=0,··· , B

T SS

...,

− 1,

B

−1, Y(j−1) , (14)

where j = 1, · · · , J is the index of the encoder layer and Z(b0) = XEb and Y(0) = Chyp. Note that ExtBlockSA(·) and MultiHead(·) in Eq. (14) share the parameters. In the ﬁnal layer J, Z(J) corresponds to Hfeat in the CTC part of Figure 1 (also introduced in Eq. (7)), while Y(J) corresponds to Htok in the
insertion part of Figure 1 (also introduced in Eq. (3)).

4.3. Audio segmentation and decoding
Once the model is trained, audio segmentation and decoding are processed as follows. Different to the segmented case explained in Section 4.2, when the input audio is long or is fed in a streaming way, b in Eq. (14) is very long or unbounded. Therefore, we proposed to segment audio by using only the ﬁrst line of Eq. (14) because this operation is possible at every B frames of input are obtained by using the embedding Csos of C = { s } as Y(0). Then, output of CTC probability p(a(b−1)×B, · · · , ab×B|Hfeat) in Eq. (7) is computed by using Z(bJ) as Hfeat. If the blank symbol has the highest probability of more than τ consecutive frames, i.e.:
arg max p(at|Hfeat) = b for t = b · B − τ, · · · , b · B,
at
(15)

Table 1: CER and RTF of CSJ when oracle segmentation and full-attention model is used.

Eval1 Eval2 Eval3 RTF

ART

7.5

CTC

8.6

KERMIT 7.5

5.0 12.2 0.98 5.9 13.9 0.27 5.0 12.2 0.21

Table 2: CER and RTF of CSJ when separated TDNN-based audio segmentation and full-attention model is used.

Eval1 Eval2 Eval3 RTF

ART

9.8 6.7 14.3 0.92

CTC

10.5 6.8 15.3 0.31

KERMIT 10.1 6.7 14.4 0.23

the end of the current audio segment is detected as b × B. This
is the same strategy proposed in [22]. Suppose the index of current audio segment is r and its end as Trend, r-th segment is decoded with b = Tre−nd1, · · · , Trend by both line of Eq. (14) and iteratively updating Y(0) by new hypothesis.

5. Experiments
5.1. Setup
The proposed framework is evaluated on the Corpus of Spontaneous Japanese (CSJ) [26] and TEDLIUM2 [27]. Note that for CSJ, only the Academic lecture data, whose amount is around 270 hours, is used for training. The unsegmented evaluation set is used to evaluate the performance of audio segmentation and ASR. The segmentation is performed in two ways. The ﬁrst one is to employ a separated model for segmentation. We used a model provided as a baseline of CHiME6 challenge [28]. The model is based on time-delayed neural network (TDNN) architecture and trained on a target label generated by a highly tuned hybrid ASR model trained on CHiME6 training data. The second one is using the integrated CTC part as described in Section 4.3. Baseline models are autoregressive Transformer (ART) and CTC. Both of the baseline models can use integrated CTC part for segmentation in the same way as described in Section 4.3 by using block-SA. Note that there can be a mismatch of the length of the segmented audio compared with oracle segmentation used in training in both ways. Hence it is not apparent that the combination of audio segmentation and non-autoregressive ASR works or not.
Implementation is based on the recipe of ESPnet [29]. Most of the parameters are almost the same as used in [19] except the following two points. The learning rate and warmup steps were adjusted to stabilize training, and the number of epochs is set to 200 for the KERMIT with block-SA. The segmentation threshold τ , as discussed in Section 4.3, is set to 8, 16, or 24 depending on the models according to a preliminary experiment. The RTF is measured with Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz using two threads for the forward propagation of a neural network. The beam size of ART and CTC and the number of iteration of KERMIT is set to 5.

5.2. Results and Discussion
The character error rate (CER) and real time factor (RTF) of CSJ of each methods are shown in Table 1 to 3. The oracle segmentation results in Table 1 showed that KERMIT achieved the best CER, which is the same as ART, but the RTF is smaller than ART. This is the advantage of non-autoregressive ASR.
As shown in Table 2, when using the separated TDNN

Table 3: CER and RTF of CSJ when integrated CTC-based audio segmentation with block-SA is used.

Future context [sec]

Eval1 Eval2 Eval3 RTF

0.19

ART

15.5 11.1 21.9 1.54

CTC

14.8 11.2 21.8 0.45

KERMIT 11.7 8.6 17.3 0.54

0.67

ART

14.1 9.9 19.6 1.62

CTC

12.8 9.3 18.4 0.39

KERMIT 10.8 7.7 16.2 0.45

Table 4: WER and RTF of TEDLIUM2. For the integrated CTCbased segmentation, future context is set to 0.19 sec.

Segmentation

dev test RTF

Oracle

ART 10.2 9.1 1.11

KERMIT 11.1 9.9 0.19

Separated TDNN ART 10.4 13.4 0.85

KERMIT 11.8 14.2 0.20

Integrated CTC

ART 17.9 19.5 1.15

KERMIT 16.5 19.9 0.38

model for segmentation, ART achieved the best CER, but KERMIT also achieved competitive CER with around 1/4 RTF compared to ART. In the case of using integrated CTC as segmentation, as shown in Table 3, KERMIT is in a good balance between CER and RTF. The RTF of CTC is a bit smaller, but its CER is worse than KERMIT. ART was the worst CER and RTF. The integrated approach does not use any alignment nor segmentation criterion while training. This can lead to a signiﬁcant mismatch between estimated segment length and oracle segment, and the autoregressive model would not be robust on such a mismatch. The word error rate (WER) and RTF of TEDLIUM2 are shown in Table 4. Same as the CSJ case, KERMIT is in a good balance between WER and RTF.
In summary, the proposed combination of TDNN segmentation and KERMIT-based NAT achieved a reasonable performance trade-off in terms of RTF and CER. The performance degradation from the oracle segmentation is acceptable (less than 3%) while keeping around 0.2 RTF, which is suitable for a streaming scenario. Another proposal of the integrated CTC approach also shows promising results since it restricts the future context with 0.67 seconds while still keeping 0.45 RTF and within 4% CER degradation from the oracle segmentation result. Also, the integrated approach is based on a single neural network, and further optimization can mitigate the issue.
6. Conclusion
This paper proposed combining audio segmentation and nonautoregressive ASR toward streaming or long recording audio recognition. Also, we introduced a new architecture that realizes audio segmentation and non-autoregressive ASR by a single neural network. The insertion-based model, which is jointly trained with CTC, is used as non-autoregressive ASR. By employing causal self-attention, the CTC part is used for audio segmentation. Experimental results showed that a combination of audio segmentation and non-autoregressive ASR worked well and achieved a good balance between CER and RTF compared with baseline AR Transformer and CTC. The single model approach also shows promising results, and the improvement of this approach is left as future work.

7. References
[1] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Proc. Advances in Neural Information Processing Systems (NIPS) 28, 2015, pp. 577–585. [Online]. Available: http://papers.nips.cc/ paper/5847-attention-based-models-for-speech-recognition.pdf
[2] D. Amodei et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” in Proc. of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016, pp. 173–182.
[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 4960–4964.
[4] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid ctc/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, Dec 2017.
[5] C. Chiu et al., “State-of-the-art speech recognition with sequenceto-sequence models,” in Proc. ICASSP 2018, April 2018, pp. 4774–4778.
[6] C. Lu¨scher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlu¨ter, and H. Ney, “RWTH ASR Systems for LibriSpeech: Hybrid vs Attention,” in Proc. Interspeech 2019, 2019, pp. 231– 235. [Online]. Available: http://dx.doi.org/10.21437/Interspeech. 2019-1780
[7] S. Karita et al., “A comparative study on transformer vs RNN in speech applications,” arXiv preprint arXiv:1909.06317, 2019.
[8] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li, M. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu, “Two-Pass End-to-End Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 2773–2777. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2019-1341
[9] Y. He et al., “Streaming end-to-end speech recognition for mobile devices,” in Proc. ICASSP, 2019, pp. 6381–6385.
[10] K. Kim et al., “Attention based on-device streaming speech recognition with large speech corpus,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 956–963.
[11] T. N. Sainath et al., “A streaming on-device end-to-end model surpassing server-side conventional model quality and latency,” in Proc. ICASSP 2020, 2020, pp. 6059–6063.
[12] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher, “Non-autoregressive neural machine translation,” arXiv preprint arXiv:1711.02281, 2017.
[13] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Mask-predict: Parallel decoding of conditional masked language models,” in Proceedings of the EMNLP-IJCNLP. Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 6112–6121. [Online]. Available: https://www.aclweb.org/ anthology/D19-1633
[14] J. Gu, C. Wang, and J. Zhao, “Levenshtein transformer,” in Proc. Advances in Neural Information Processing Systems (NIPS) 32, 2019, pp. 11 181–11 191. [Online]. Available: http://papers.nips.cc/paper/9297-levenshtein-transformer.pdf
[15] M. Stern, W. Chan, J. Kiros, and J. Uszkoreit, “Insertion transformer: Flexible sequence generation via insertion operations,” in Proc. of International Conference on Machine Learning (ICML), 2019, pp. 5976–5985.
[16] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Listen and ﬁll in the missing letters: Non-autoregressive transformer for speech recognition.” arXiv preprint arXiv:1911.04908, 2020.
[17] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask ctc: Non-autoregressive end-to-end asr with ctc and mask predict,” arXiv preprint arXiv:2005.08700, 2020.

[18] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, “Imputer: Sequence modelling via imputation and dynamic programming,” arXiv preprint arXiv:2002.08926, 2020.
[19] Y. Fujita, S. Watanabe, M. Omachi, and X. Chan, “Insertion-based modeling for end-to-end automatic speech recognition,” arXiv preprint arXiv:2005.13211, 2020.
[20] A. Graves, S. Ferna´ndez, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369–376.
[21] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov, “Transformer-XL: Attentive language models beyond a ﬁxed-length context,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 2978–2988. [Online]. Available: https://www.aclweb. org/anthology/P19-1285
[22] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, “End-toend automatic speech recognition integrated with ctc-based voice activity detection,” in Proc. ICASSP, 2020, pp. 6999–7003.
[23] B. Li et al., “Towards fast and accurate streaming end-to-end asr,” in Proc. ICASSP, 2020, pp. 6069–6073.
[24] W. Chan, N. Kitaev, K. Guu, M. Stern, and J. Uszkoreit, “Kermit: Generative insertion-based modeling for sequences,” arXiv preprint arXiv:1906.01604, 2019.
[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. Advances in Neural Information Processing Systems (NIPS) 30, 2017, pp. 5998–6008. [Online]. Available: http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
[26] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous speech corpus of Japanese,” in Proc. the Second International Conference on Language Resources and Evaluation (LREC’00), 2000.
[27] A. Rousseau, P. Dele´glise, and Y. Este`ve, “Enhancing the TEDLIUM corpus with selected data for language modeling and more TED talks,” in Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), May 2014, pp. 3935–3939.
[28] S. Watanabe et al., “CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings,” in Proc. The 6th International Workshop on Speech Processing in Everyday Environments (CHiME 2020), 2020, pp. 1–7. [Online]. Available: http://dx.doi.org/10.21437/CHiME.2020-1
[29] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, “Espnet: End-to-end speech processing toolkit,” in Proc. Interspeech 2018, 2018, pp. 2207–2211. [Online]. Available: http://dx.doi.org/10.21437/ Interspeech.2018-1456

