GAUSSIAN KERNELIZED SELF-ATTENTION FOR LONG SEQUENCE DATA AND ITS APPLICATION TO CTC-BASED SPEECH RECOGNITION
Yosuke Kashiwagi1, Emiru Tsunoo1, Shinji Watanabe2
1Sony Corporation, Japan, 2Johns Hopkins University, USA

arXiv:2102.09168v1 [eess.AS] 18 Feb 2021

ABSTRACT
Self-attention (SA) based models have recently achieved signiﬁcant performance improvements in hybrid and end-to-end automatic speech recognition (ASR) systems owing to their ﬂexible context modeling capability. However, it is also known that the accuracy degrades when applying SA to long sequence data. This is mainly due to the length mismatch between the inference and training data because the training data are usually divided into short segments for efﬁcient training. To mitigate this mismatch, we propose a new architecture, which is a variant of the Gaussian kernel, which itself is a shift-invariant kernel. First, we mathematically demonstrate that self-attention with shared weight parameters for queries and keys is equivalent to a normalized kernel function. By replacing this kernel function with the proposed Gaussian kernel, the architecture becomes completely shift-invariant with the relative position information embedded using a frame indexing technique. The proposed Gaussian kernelized SA was applied to connectionist temporal classiﬁcation (CTC) based ASR. An experimental evaluation with the Corpus of Spontaneous Japanese (CSJ) and TEDLIUM 3 benchmarks shows that the proposed SA achieves a signiﬁcant improvement in accuracy (e.g., from 24.0% WER to 6.0% in CSJ) in long sequence data without any windowing techniques. Index Terms: speech recognition, end-to-end, self-attention, long sequence data
1. INTRODUCTION
In recent years, automatic speech recognition (ASR) using selfattention (SA) [1] has attracted considerable attention. Both transformer-based speech recognition [2–6] and hybrid [7, 8] and connectionist temporal classiﬁcation (CTC) [9, 10] models have shown a high recognition performance with SA. The SA network has a mathematically simple structure by fully using matrix-vector based operations designed for an efﬁcient parallel computation. Thus, the recurrent neural network (RNN) based architecture has been replaced with the SA network because of the efﬁcient computation property and its high performance.
However, SA is unsuitable for decoding long sequence data because it has a high computational complexity on the order of the square of the sequence length. In addition, the recognition accuracy degrades in long utterances owing to its excessive ﬂexibility in context modeling. In this paper, we focus on the problems of the accuracy degradation in long sequence data. In general, self-attention requires dividing original long recordings into short segments during training for efﬁcient GPU computing. This leads to a mismatch between the sequence lengths of the training and test data, resulting in a performance degradation.
To solve this problem, several studies have been proposed. Masking [11] limits the range of self-attention by using a Gaussian window, whereas relative positional encoding [12, 13] uses relative embedding in a self-attention architecture to eliminate the effect of

the length mismatch. However, masking does not take into account the correlation between input features and relative distance. In addition, the relative positional encoding does not limit the attention to the neighborhood in a mathematical.
Inspired by the mathematical expression based on the sharedQK attention used in Reformer [14], in this paper, yet another self-attention reformulation based on a Gaussian kernel is proposed. First, we mathematically demonstrate that the linear layers and softmax functions in the shared-QK attention can be represented as normalized kernel functions, similarly to [15] and [16], interpreting bilinear pooling as a kernel function. These kernel functions are replaced with a Gaussian kernel and thus we call our model Gaussian kernelized self-attention. Gaussian kernel, known as a radial basis function kernel, has several useful features and has been widely used with a support vector machine (SVM) [17–21]. The Gaussian kernelization applied in our new formulation also provides a shiftinvariance into the self-attention architecture. This shift-invariant property is a highly desirable property for controlling the relative position. To take advantage of this property, we propose concatenating the bare frame index to an input feature, which is called a frame indexing technique.
To compare the differences in SA structure, this paper applies the proposed Gaussian kernelized SA to CTC-based ASR because the decoder network of a CTC is rather simple compared with other end-to-end architectures, and we can purely evaluate the effectiveness between the proposed and conventional SA methods. An experimental evaluation shows that our proposed SA with frame indexing achieved a signiﬁcant improvement in the long sequence data.

2. SELF-ATTENTION FOR LONG SEQUENCE DATA

2.1. Self-attention

Let Xi and Xj be D-dimensional input features of the self-attention

network with time indexes i and j in a sequence, respectively. The

scaled dot product attention [1] calculates the attention weight as

follows:

(W (Q)Xi) (W (K)Xj )

Attn(i, j) = softmax

√

(1)

dk

where W (Q) and W (K) represent dk × D trainable matrices in the linear operation for Xi and Xj, respectively. Note that the bias term is included in each matrix. Multi head attention, which individually calculates the above attention in multiple heads, is effectively used in every layer. For simplicity, we omit the head and layer indexes in our formulation.

2.2. Masking
Self-attention itself attends to the target frames without any positional limitations. This ﬂexibility is an advantage over conventional neural networks. However, in typical speech recognition encoders,

local information is more important than global characteristics

for representing phonetic features, particularly in long sequences.

Therefore, several masking approaches are studied to control the

attention and allow it to be more local.

Sperber et al. used a self-attention architecture with a weighting

technique applying a hard or soft mask in acoustic modeling [11].

They limited the target frames to be calculated by adding a mask

that has values within a range of − inf to zero to the attention before

the softmax function. In [11], the authors reported that the soft mask

is more effective than the hard mask with proper initialization. The soft mask M soft is equal to the Gaussian window, which is deﬁned

as

soft −(i − j)2

Mi,j = 2σ2 ,

(2)

where σ is a trainable parameter and a standard deviation of the Gaussian, which controls the window size.

2.3. Positional encoding

Relative positional encoding [12, 13] is an extension of an absolute positional encoding technique that allows self-attention to handle relative positional information. The absolute positional encoding is deﬁned as follows:



 

sin



Ui,d =

 

cos



i d
10000 D
i (d−1) 10000 D

if d = 2k , (3)
if d = 2k + 1

where i is the frame index, d is the index of the feature dimension, and k is an arbitrary integer. Typically, the positional encoding is added to the input speech feature, i.e., Xi → Xi + Ui. However, this coding depends on absolute positional information. When the testing data are longer than training data, the indexes near the end of the utterance become unseen. This mismatch degrades the speech recognition performance in long sequence data. Relative positional encoding can remove the effects of this mismatch.
Relative positional encoding modiﬁes the attention value before the softmax function as follows:

Ariejl = Xi W (Q) W (K,X)Xj + Xi W (Q) W (K,R)Ri−j

+ u W (K,X)Xj + v W (K,R)Ri−j .

(4)

Here, Ri−j is a sinusoid encoding matrix based on Eq. (3), and u and v are trainable parameters.

3. GAUSSIAN KERNELIZED SELF-ATTENTION
3.1. Motivation
Although relative position encoding can reduce the mismatch, which is a problem with absolute position encoding, the relative encoding itself does not limit the attention to the neighborhood of the frame. This still allows the self-attention to attend to the distant frames that are not important in the encoder of the speech recognition model. By contrast, masking is a reasonable approach for the encoder of the speech recognition models because it structurally limits the range of attention. However, the effective window length σ in Eq. (2) is ﬁxed during the testing time as a trained parameter. Thus, the window length is constant for any input features. To eliminate mismatches and improve the recognition accuracy in long sequence data, our aim is to extend the masking technique to be more adaptive, allowing the range of attention to be trained depending on both the input features and their positions.

3.2. Shared-QK attention

Before describing our proposed Gaussian kernelized self-attention,

we describe an important related approach, i.e., shared-QK atten-

tion [14]. Shared-QK attention is one variant of self-attention and

computes Qi and Kj in Eq. (1) using the shared linear transforma-

tion parameters, W (Q) = W (K) W (S). The shared-QK attention

is then calculated from Eq. (1) as follows:

(S)

(W (S)Xi) (W (S)Xj )

Attn (i, j) = softmax

√

. (5)

dk

It has been reported to achieve a performance comparable to that of non-shared self-attention with a smaller parameter size [14].

3.3. Relationship between shared-QK attention and kernel

The set of shared-QK attentions is a non-negative symmetric ma-

trix, because Q and K are identical and exponential calculations exist

in the softmax function. We interpret the attention as a normalized

Gram matrix. By introducing the normalization term Z(S), we can

rewrite the shared-QK attention in Eq. (5) as follows:

Attn(S)(i, j) = 1 exp Xi Σ−1Xj ,

(6)

Z (S)

where Σ and Z(S) are deﬁned in the following manner.

Σ−1 Wˆ (S) Wˆ (S), Wˆ (S) W (S)

(7)

1

(dk) 4

Z (S)

exp Xi Wˆ (S) Wˆ (S)Xj

(8)

j

Here, Σ−1 in Eq. (7) is a positive semideﬁnite matrix, which can be

regarded as the inverse of the full-covariance matrix.

Attn(S)(i, j) is further rewritten by completing the square of

Eq. (6) as follows.

Attn(S)(i, j) =

1

exp

1 − (Xi − Xj )

Σ−1(Xi − Xj )

Z (S)

2

× exp

1 Xi

Σ−1Xi

exp

1 Xj

Σ−1Xj

,

(9)

2

2

where we have three matrix square forms based on Xi −Xj, Xi, and Xj. We call the matrix square forms of Xi and Xj energy terms.

3.4. Proposed Gaussian kernelization

We propose replacing the kernel function of the self-attention in

Eq. (9) with a Gaussian kernel having a full-covariance trainable

matrix. Gaussian kernelized attention Attn(G) is deﬁned as

Attn(G)(i, j) =

1

exp

1 − (Xi − Xj )

Σ−1(Xi − Xj )

.

Z (G)

2

(10)

The normalization term Z(G) is deﬁned as follows.

Z (G)

1 − (Xi − Xj )

Σ−1(Xi − Xj )

(11)

j2

Eq. (10) can be interpreted as the removal of the energy terms from the conventional shared-QK attention in Eq. (9). The Gaussian kernel depends only on the difference between the input features (Xi − Xj), which is shift-invariant. In addition, because it is an exponential function, the attention value approaches zero as the difference increases.
The proposed self-attention architecture requires the query and key to be of the same matrix. Therefore, this approach cannot be used in source-target attention and can only be used in self-attention.

 


VHOIDWWHQWLRQ UHODWLYH SRVLWRQDO HQFRGLQJ *DXVVLDQ NHUQHO  IUDPH LQGH[LQJ

&(5  





          DYHUDJH VHTXHQFH OHQJWK VHF
Fig. 1: Comparison of self-attention, relative positional encoding, and Gaussian kernelized self-attention with frame indexing for each sequence length (character error rate.)

3.5. Relative positional information with frame indexing

The Gaussian kernel is a function that depends only on the Xi − Xj term as in Eq. (10). However, the Gaussian kernel itself does not have the ability to obtain relative positional information. Therefore, we include the absolute positional information by simply appending the frame index i to Xi. Owing to the shift-invariant nature of the Gaussian kernelized self-attention, the Xi − Xj term is rewritten as follows:

Xˆi − Xˆj = [(Xi − Xj) , (i − j)/α] ,

(12)

where α is a scaling factor used to control the scales of the relative position and the input features, and is normalized through a layer normalization function. We set α to 100 in this paper.
By assigning Eq. (12) to Eq. (10), the frame indexing element becomes similar to Eq. (2). However, because Σ−1 in Eq. (7) is trained by considering both Xi and frame indexing, the standard deviation of the Gaussian window, which is proportional to Σ, is statistically adaptive to the input features. Thus, the proposed method properly embeds relative positional information to the model by concatenating the frame indexing to the input features. Note that the original self-attention has energy terms as in Eq. (9). When the frame indexes are concatenated to the input feature in the same way as the Gaussian kernel, these energy terms become dependent on the absolute indexes. Therefore, this indexing is ineffective unless the attention architecture is shift-invariant, as shown in the experiments below.

4. EXPERIMENTAL EVALUATION
4.1. Experimental setup
The Gaussian kernelized self-attention was evaluated using the CSJ dataset [22] and TED-LIUM 3 dataset [23]. We compared the Gaussian kernelized self-attention with an RNN, self-attention (Sec. 2.1), masking (Sec. 2.2), relative encoding (Sec. 2.3) and shared-QK attention (Sec. 3.2). The CTC [24] model based on self-attention [9] was used as our baseline architecture to purely compare the difference between the proposed and other self attention methods because CTC has a simple decoder architecture compared with other endto-end models. The methods other than an RNN were implemented under the same conditions except for the structure corresponding to the self-attention. The baseline model consisted of convolutional layers and a subsequent 12-layer self-attention blocks. In each selfattention block, the number of dimensions dk in Eq. (1) was 256 and

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 2: Examples of the attention heat map of (a) self-attention in short data, (b) Gaussian kernelized self-attention w/o frame indexing in short data, and (c) Gaussian kernelized self-attention w/ frame indexing in short data. (d), (e), and (f) describe the attentions in long data corresponding to (a), (b), and (c), respectively. The vertical axis represents the source frame index and the horizontal axis represents the target frame index.

the number of heads was 4. A middle linear layer followed each self-attention network and a position-wise feedforward network expanded the dimension of the middle layer to 2,048. The input features were 80-dimensional Mel ﬁlter banks and pitch features. The SpecAugment [25] technique was applied to the data. In addition, the features were subsampled to reduce their number by a factor of 4 in the convolutional layers. A positional encoding was added to the input feature just before the ﬁrst self-attention block. The RNN based encoder consisted of 4 RNN layers with 1,204 units. All methods were evaluated using greedy decoding without any external language model to purely evaluate the performance of the proposed self-attention network.
For CSJ data, the training and development set consisted of 413,408 and 4,000 utterances, respectively. The tokens consisted of 3,262 Japanese characters, including a blank label. For the evaluation data, we prepared standard evaluation sets, eval1, eval2, and eval3, which were split into short segment units (short eval (1, 2, 3)). To investigate the recognition performance in long sequence data, we also used the original long data without splitting into segments as an additional evaluation set (long eval (1, 2, 3)). The average sequence length was approximately 4.7 sec. for a short evaluation, and 772.6 sec. for long evaluation.
For TED-LIUM 3 data, the training and development sets consisted of 268,262 and 507 utterances respectively. The tokens consisted of 654 English tokens which were encoded using the unigram language model [26], including a blank label. For the evaluation data, a longer dataset was prepared in addition to the standard set as in CSJ. The average sequence length was 8.2 sec. and 1,004.1 sec., respectively. Note that the longest single talk (1,772 sec.) was extremely long, and was evaluated by splitting it in half to avoid a memory shortage.
4.2. Results
Table 1 shows the performance of different model architectures in the CSJ data. In our experiments, the self-attention and shared-QK

Table 1: Comparison of the recognition performance (character error rate) for short and long data in CSJ data. The relative encoding in long sequence data was skipped due to the huge memory requirements over 700GB (-).

short data

long data

eval1 eval2 eval3 avg. eval1 eval2 eval3 avg.

average length (sec.) 5.2 5.4 3.4 4.7 829.5 871.7 616.6 772.6

RNN

7.9 5.8 6.3 6.7 9.7 6.5 7.4 7.9

self-attention

6.5 4.7 5.3 5.5 25.4 23.6 23.1 24.0

+ soft mask

7.8 5.5 6.4 6.6 9.1 6.2 7.0 7.4

+ frame indexing

20.4 26.4 18.9 21.9 80.1 78.5 75.2 77.9

+ relative encoding

9.5 7.9 10.1 9.2

-

-

-

-

+ shared-QK

6.3 4.9 5.7 5.6 82.1 81.4 77.4 80.3

+ Gaussian kernel

6.7 4.7 5.6 5.7 79.9 79.1 79.0 79.3

+ frame indexing 6.5 4.5 5.4 5.5 7.5 5.0 5.6 6.0

Table 2: Comparison of the recognition performance (token error rate) for short and long data in TED-LIUM 3 data.

average length (sec.) RNN self-attention + soft mask + Gaussian kernel
+ frame indexing

short data dev test 11.35 8.16 21.7 25.6 15.2 17.3 14.7 17.4 21.9 20.1 15.0 17.2

long data

dev

test

771.50 1004.14

22.4

30.8

82.8

84.6

15.0

22.0

99.0

98.5

14.9

21.0

attention achieved similarly lower error rates in the short dataset. However, in the long dataset, the accuracy of these methods decreased. As reason for this, the structure of the self-attention itself could not limit the attention to its neighborhood. By contrast, masking was effective and the performance difference between short and long sequence data was small. However, the recognition performance for short utterances was worse than that of a simple selfattention because the ﬂexibility of the attention was suppressed by the ﬁxed-length window. As with self-attention, Gaussian kernelization achieved low error rates in short sequence data, but its performance degraded signiﬁcantly in long sequence data. By using the frame indexing to take into account the relative positional information along with the input features, the Gaussian kernelized attention signiﬁcantly improved the recognition performance in long sequence data. However, when the frame indexing was used for selfattention, the recognition accuracy signiﬁcantly degraded in both short and long utterances. This was because the energy term of the self-attention became dependent on the absolute positional information, which greatly reduced its generalization ability.
Unfortunately, we could not decode the long data using the relative positional encoding. Because the second term of Eq. (4) required the same amount of memory as the self-attention, relative positional encoding required more than twice as much memory, at over 700 GB.
Therefore, we further investigated the speech recognition performance per utterance length including the relative positional encoding within a decodable range. We sampled 100 segments each from the CSJ eval1 set to create subsets such that the average utterance length of each subset became 10 seconds, 20 seconds, and so on. Figure 1 shows the speech recognition performances of the self-attention, the relative positional encoding, and Gaussian kernelized attention with frame indexing for each sequence length of the evaluated data. The relative positional encoding was found to be more robust to the length mismatches than absolute positional encoding. Although self-attention achieved a better performance than Gaussian kernelized attention in short data, the performance of selfattention degraded as the sequence length increased. By contrast, the

Gaussian kernel with frame indexing did not degrade much as the sequence length increased. Therefore, we can conﬁrm that Gaussian was more robust to a length mismatch than self-attention or that with relative positional encoding.
Figure 2 visualizes the attention weights obtained by a standard self-attention network based on Attn(i, j) in Eq. (1) (Figure 2 (a) and (d)), the Gaussian kernelized self-attention Attn(G)(i, j) in Eq. (10) (Figure 2 (b) and (e)), and the Gaussian kernelized selfattention with the frame indexing in Eq. (12) (Figure 2 (c) and (f)). Self-attention was ﬂexible in short utterances as indicated in Figure 2 (a). However, when there was a length mismatch between the training and testing data, attention was dispersed and the attention weights became smaller, as shown in Figure 2 (d). In the case of the Gaussian kernel, the diagonal components mathematically became peaky as in Figure 2 (b). However, the attention was dispersed in long sequence data as in the case of self-attention shown in Figure 2 (e). By contrast, using frame indexing, the components around the diagonal location were correctly attended even in the long speech, as indicated in Figure 2 (f).
Table 2 shows the performance for the TED-LIUM 3 data. In this case, masking maintained the recognition performance even for short utterances. The performance of the self-attention with frame indexing was signiﬁcantly worse than that of CSJ data. This may be because the average length of the evaluation data was longer than that of CSJ. By contrast, the Gaussian kernelized self-attention with frame indexing can achieve a low token error rate similar to masking for both short and long data.
5. CONCLUSION
In this paper, we proposed a new SA architecture called Gaussian kernelized SA. This structure was a natural combination of conventional masking with the kernel structure of SA. With frame indexing, the attention can statistically adapt depending on both the input features and their relative positions. We applied this novel structure to the encoder of the CTC-based ASR model to improve the recognition performance in long sequence data, which showed length mismatches between the training and testing data. In the experiments using CSJ and TED-LIUM 3 data, the Gaussian kernelized SA with frame indexing achieved a performance close to that of conventional SA in short sequence data. In addition, our model achieved a signiﬁcant accuracy improvement (e.g., from 24.0% WER to 6.0% in the Corpus of Spontaneous Japanese (CSJ) benchmark) in long sequence data. In the future, we will attempt to apply the Gaussian kernelized self-attention to RNN-T. In addition, we will expand the Gaussian kernel to include asymmetric attention and source-target attention and use our architecture in a transformer-based ASR.

6. REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in neural information processing systems, 2017, pp. 5998–6008.
[2] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a norecurrence sequence-to-sequence model for speech recognition,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884–5888.
[3] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A Comparative Study on Transformer vs RNN in Speech Applications,” arXiv preprint arXiv:1909.06317, 2019.
[4] A. Mohamed, D. Okhonko, and L. Zettlemoyer, “Transformers with convolutional context for ASR,” arXiv preprint arXiv:1904.11660, 2019.
[5] A. Zeyer, P. Bahar, K. Irie, R. Schlu¨ter, and H. Ney, “A comparison of transformer and LSTM encoder decoder models for ASR,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 8–15.
[6] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, “End-to-end multi-speaker speech recognition with transformer,” in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6134–6138.
[7] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, “A time-restricted self-attention layer for asr,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5874–5878.
[8] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformerbased acoustic modeling for hybrid speech recognition,” in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6874–6878.
[9] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Mu¨ller, and A. Waibel, “Very deep self-attention networks for end-to-end speech recognition,” Proc. Interspeech 2019, pp. 66–70, 2019.
[10] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks for connectionist temporal classiﬁcation in speech recognition,” in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 7115–7119.
[11] M. Sperber, J. Niehues, G. Neubig, S. Stu¨ker, and A. Waibel, “Self-attentional acoustic models,” in Proc. Interspeech 2018, 2018, pp. 3723–3727. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2018-1910
[12] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 464–468. [Online]. Available: https://www.aclweb. org/anthology/N18-2074
[13] N.-Q. Pham, T.-L. Ha, T.-N. Nguyen, T.-S. Nguyen, E. Salesky, S. Stueker, J. Niehues, and A. Waibel, “Relative positional encoding for speech recognition and direct translation,” arXiv preprint arXiv:2005.09940, 2020.

[14] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The Efﬁcient Transformer,” arXiv preprint arXiv:2001.04451, 2020.
[15] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pooling,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 317–326.
[16] M. Raginsky and S. Lazebnik, “Locality-sensitive binary codes from shift-invariant kernels,” in Advances in neural information processing systems, 2009, pp. 1509–1517.
[17] B.-C. Kuo, H.-H. Ho, C.-H. Li, C.-C. Hung, and J.-S. Taur, “A kernel-based feature selection method for SVM with RBF kernel for hyperspectral image classiﬁcation,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 7, no. 1, pp. 317–326, 2013.
[18] P. P. Dahake, K. Shaw, and P. Malathi, “Speaker dependent speech emotion recognition using MFCC and support vector machine,” in 2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT). IEEE, 2016, pp. 1080–1084.
[19] Y. Shao and C.-H. Chang, “Wavelet transform to hybrid support vector machine and hidden Markov model for speech recognition,” in 2005 IEEE International Symposium on Circuits and Systems. IEEE, 2005, pp. 3833–3836.
[20] J. Stadermann and G. Rigoll, “A hybrid SVM/HMM acoustic modeling approach to automatic speech recognition,” in Proc. Int. Conf. on Spoken Language Processing ICSLP# 2004, Jeju Island, South Korea, 2004.
[21] A. Ganapathiraju, J. E. Hamaker, and J. Picone, “Applications of support vector machines to speech recognition,” IEEE transactions on signal processing, vol. 52, no. 8, pp. 2348–2355, 2004.
[22] K. Maekawa, “Corpus of spontaneous japanese: Its design and evaluation,” Proceedings of SSPR, 01 2003.
[23] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Este`ve, “Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation,” in International Conference on Speech and Computer. Springer, 2018, pp. 198–208.
[24] A. Graves, S. Ferna´ndez, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.
[25] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple augmentation method for automatic speech recognition,” in INTERSPEECH, 2019.
[26] T. Kudo, “Subword regularization: Improving neural network translation models with multiple subword candidates,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 66–75. [Online]. Available: https://www.aclweb.org/anthology/P18-1007

