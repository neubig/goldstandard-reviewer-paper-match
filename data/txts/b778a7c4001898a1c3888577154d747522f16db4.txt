Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting
Hao-Wen Dong · Yi-Hsuan Yang

arXiv:1901.08753v2 [cs.LG] 18 May 2020

Abstract Recent work has proposed various adversarial loss functions for training either generative or discriminative models. Yet, it remains unclear what certain types of functions are valid adversarial losses, and how these loss functions perform against one another. In this paper, we aim to gain a deeper understanding of adversarial losses by decoupling the effects of their component functions and regularization terms. We ﬁrst derive in theory some necessary and sufﬁcient conditions of the component functions such that the adversarial loss is a divergence-like measure between the data and the model distributions. In order to systematically compare different adversarial losses, we then propose a new, simple comparative framework, dubbed DANTest, based on discriminative adversarial networks (DANs). With this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. Our theoretical and empirical results can together serve as a reference for choosing or designing adversarial training objectives in future research.
Keywords Adversarial loss · Generative adversarial network · Discriminative adversarial network
1 Introduction
Generative adversarial networks (GANs) (Goodfellow et al 2014) are a class of unsupervised machine learning algorithms. In essence, a GAN learns a generative model with
Hao-Wen Dong Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan E-mail: salu133445@citi.sinica.edu.tw
Yi-Hsuan Yang Taiwan AI Labs, Taipei, Taiwan Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan E-mail: yang@citi.sinica.edu.tw

the guidance of another discriminative model which is trained jointly, and such variable loss function provided by the discriminator is referred to as an adversarial loss. However, the idea of adversarial losses is not limited to unsupervised learning. Adversarial losses can also be applied to supervised scenarios and have advanced the state of the art in many ﬁelds over the past years (Goodfellow 2016), including image-to-image translation (Isola et al 2017; Zhu et al 2017), image super-resolution (Ledig et al 2017) and domain adaptation (Tzeng et al 2017).
Despite the success, there are several open questions that need to be addressed. On one hand, although plenty adversarial losses have been proposed, we have little theoretical understanding of what makes a loss function a valid one. In this paper, we follow the variational divergence minimization perspective and consider an adversarial loss valid if minimizing such a loss function is equivalent to minimize a lower bound of some divergence. From this view, Nowozin et al (2016) has shown a certain class of component functions (the activation functions used in the main loss function that sets up the two-player adversarial game; see Section 2.1 for deﬁnitions) can result in valid adversarial losses. However, since they considered only the f -divergences, the two component functions considered in their formulations are not independent of each other. In this work, we would like to consider all classes of divergences and ﬁgure out the necessary and sufﬁcient conditions on these component functions for a theoretically valid adversarial loss.
On the other hand, we note that any two adversarial losses can differ in terms of not only the component functions (e.g., classic or hinge; see Section 2.1), but also the regularization approaches (e.g., gradient penalties (Gulrajani et al 2017) and spectral normalization (Miyato et al 2018); see Sections 2.2 and 2.3) used to regularize the models. However, it remains unclear how they respectively contribute to the performance of an adversarial loss. In other words, when empirically compare two adversarial losses, we need to decouple

2

Hao-Wen Dong, Yi-Hsuan Yang

the effects of the component functions and the regularization terms, otherwise we cannot tell which one of them makes an adversarial loss better than the other.
Among existing comparative analysis of adversarial loss functions, to the best of our knowledge, only Lucic et al (2018) and Kurach et al (2018) attempted to decouple the effects of the component functions and regularization approaches. However, only few combinations of component functions and regularization approaches were tested in these two prior works, only seven and nine respectively. We attribute this to the high computational cost that may involve to conduct the experiments, and, more importantly, the lack of a framework to systematically evaluate adversarial losses.
In sum, the two research problems that we aim to tackle in this paper can be summarized as follows:
Problem 1 What certain types of component functions are theoretically valid adversarial loss functions?
Problem 2 How different combinations of the component functions and the regularization approaches perform empirically against one another?
Our contribution to Problem 1 is based on the intuition that a favorable adversarial loss should be a divergence-like measure between the distribution of the real data and the distribution of the model output, since in this way we can use the adversarial loss as the training criterion to learn the model parameters. Following this line of thought, we derive necessary and sufﬁcient conditions such that an adversarial loss has such a favorable property (Section 3.3). Interestingly, our theoretical analysis leads to a new perspective to understand the underlying game dynamics of adversarial losses (Section 3.5).
For Problem 2, we need an efﬁcient way to compare different adversarial losses. Hence, we adopt the discriminative adversarial networks (DANs) (dos Santos et al 2017), which are essentially conditional GANs with both the generator and the discriminator being discriminative models. Based on DANs, we propose DANTest—a new, simple framework for comparing adversarial losses (Section 4). The main idea is to ﬁrst train a number of DANs for a supervised learning task (e.g., classiﬁcation) using different adversarial losses, and then compare their performance using standard evaluation metrics for supervised learning (e.g., classiﬁcation accuracy). With the DANTest, we systematically evaluate 168 adversarial losses featuring the combination of 12 component functions (ten existing and two originally proposed in this paper in light of our theoretical analysis) and 14 existing regularization approaches (Section 5). Moreover, we use the DANTest to empirically study the effects of the Lipschitz constant (Arjovsky et al 2017), penalty weights (Mescheder et al 2018), momentum terms (Kingma and Ba 2014), and others. For simplicity and computation cost reasons, we consider the MNIST dataset (LeCun et al 1998). We discuss the

new insights that are gained, and their implications to the design of adversarial losses in future research. We should note that it is unclear if the empirical results for DANs can be generalized to conditional and unconditional GANs. However, the idea of adversarial losses is rather generic and can be used to train either generative or discriminative models. Moreover, even though the generator is no longer a generative model in this framework, the discriminator in a DAN is still trained by the exact same loss function as that in a conditional GAN (see Section 2.4). This way, we can still gain insight into the performance and stability for different adversarial losses. For reproducibility, all source code can be found at https://github.com/salu133445/dan.

2 Background
2.1 Generative Adversarial Networks (GANs)
A generative adversarial network (Goodfellow et al 2014) is a generative latent variable model that aims to learn a mapping from a latent space Z to the data space X , i.e., a generative model G, which we will refer to as the generator. A discriminative model D (i.e., the discriminator) deﬁned on X is trained alongside G to provide guidance for it. Let pd denote the data distribution and pg be the model distribution implicitly deﬁned by G(z) when z ∼ pz. In general, most GAN loss functions proposed in the literature can be formulated as:

max Ex∼pd [f (D(x))] + Ex˜∼pg [g(D(x˜))] ,

(1)

D

min Ex˜∼pg [h(D(x˜))] ,

(2)

G

where f , g and h are real functions deﬁned on the data space (i.e., X → R) and we will refer to them as the compoenent functions. We summarize in Table 1 the component functions f , g and h used in some existing adversarial losses.
Some prior work has also investigated the so-called IPMbased GANs, where the discriminator is trained to estimate an integral probability metric (IPM) between pd and pg:

d(pd, pg) = − sup Ex∼pd [D(x)] + Ex˜∼pg [D(x˜)] , (3)
D∈D
where D is a set of functions from X to R. For example, the Wasserstein GANs (Arjovsky et al 2017) consider D to be the set of all 1-Lipschitz functions. Other examples include McGAN (Mroueh et al 2017), MMD GAN (Li et al 2017) and Fisher GAN (Mroueh and Sercu 2017). Please note that the main difference between (1) and (3) is that in the latter we constrain D to be in some set of functions D.

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

3

Table 1 Component functions for a few adversarial losses (see (1) and (2)). y∗ denotes the root of f (y) = g(y) and f (y) = −g (y).

f

g

h

y∗

classic minimax (Goodfellow et al 2014)

− log(1 + e−y) −y − log(1 + e−y) −y − log(1 + e−y) 0

classic nonsaturating (Goodfellow et al 2014) − log(1 + e−y) −y − log(1 + e−y)

log(1 + e−y)

0

Wasserstein (Arjovsky et al 2017)

y

−y

−y

0

least squares (Mao et al 2017) hinge (Lim and Ye 2017; Tran et al 2017)

−(y − 1)2 min(0, y − 1)

−y2 min(0, −y − 1)

(y − 1)2 21

−y

0

Table 2 Distribution pxˆ and function R in (4) for common gradient penalties, where c, k ∈ R are considered hyperparameters (k is the Lipschitz constant). We will refer to the (x − k)2 and the max(x − k) versions as the two-side and the one-side penalties, respectively.

coupled gradient penalties (Gulrajani et al 2017) local gradient penalties (Kodali et al 2017) R1 gradient penalties (Mescheder et al 2018) R2 gradient penalties (Mescheder et al 2018)

pxˆ
pd + U [0, 1] (pg − pd) pd + c N (0, I) pd pg

R(x)
(x − k)2 or max(x, k) (x − k)2 or max(x, k)
x x

(a) coupled gradient penalties (b) local gradient penalties

(c) R1 gradient penalties

(d) R2 gradient penalties

Fig. 1 Illustrations of the regions in the data space where gradient penalties are imposed (i.e., the support of px˜ ) for common gradient penalties, shown as the red shaded area in (a) and (b) and the red curves in (c) and (d). The blue and black curves denote the model and the data manifolds, respectively. The right ﬁgure in (a) shows the case when the generator perfectly fabricates the data distribution (i.e., pg = pd). For (c) and (d), the gradient penalties are enforced directly on the model and the data manifolds, respectively.

2.2 Gradient Penalties (GP)

As the discriminator is often found to be too strong to provide reliable gradients to the generator, one regularization approach is to use some gradient penalties to constrain the modeling capability of the discriminator. Most gradient penalties proposed in the literature take the following form:

λ Exˆ∼pxˆ [R(||∇xˆ D(xˆ)||)] ,

(4)

where the penality weight λ ∈ R is a pre-deﬁned constant, and R(·) is a real function. The distribution pxˆ deﬁnes where the gradient penalties are enforced. Table 2 shows the distribution pxˆ and function R used in some common gradient penalties. And, Fig. 1 illustrates pxˆ. Note that when gradient penalties are enforced, the loss function for training the discriminator contains not only the component functions f and g in (1) but also the regularization term (4).

Fig. 2 An example of a DAN for MNIST digit classiﬁcation.
2.3 Spectral Normalization (SN)
The other regularization approach we consider is the spectral normalization proposed by Miyato et al (2018). It enforces a Lipschitz constraint on a neural network by normalizing the spectral norms of the weight matrices of each layer. While the gradient penalties (see Section 2.2) impose local regularizations, the spectral normalization imposes a global regularization on the discriminator. Therefore, it is possible to combine the two regularization approaches.
2.4 Discriminative Adversarial Networks (DANs)
A discriminative adversarial network (dos Santos et al 2017) is essentially a conditional GAN (Mirza and Osindero 2014) where both the generator G and the discriminator D are discriminative models (see Fig. 2). In a DAN, the generator aims to predict the label of a real data sample, whereas the discriminator takes as input either a real pair—“(real data, real label)” or a fake pair—“(real data, fake label)”, and aims to examine its authenticity. Mathematically, the generator in a DAN learns the mapping from the data space to the label

4
space (i.e., X → Y, where Y denotes the label space), rather than from some latent space to the data space (i.e., Z → X ) in a GAN. In contrast, the discriminator in either a DAN or a GAN learns the same mapping (X , Y) → R.

3 Theoretical Results
In this section, we present our theoretical results regarding Problem 1. We follow the notations used in (1) and (2).

3.1 Favorable Properties for Adversarial Losses

Now, let us ﬁrst consider the minimax formulation:

min max Ex∼pd [f (D(x))] + Ex˜∼pg [g(D(x˜))] .

(5)

GD

We can see that if the discriminator is able to reach optimality, the training criterion for the generator is

LG = max Ex∼pd [f (D(x))] + Ex˜∼pg [g(D(x˜))] .

(6)

D

In general, for a valid adversarial loss, the discriminator is responsible for providing a measure of the discrepancy between the data distribution pd and the model distribution pg. In principle, this will then serve as the training criterion for the generator to push pg towards pd. Hence, we would like such an adversarial loss to be a divergence-like measure between pg and pd. From this view, we can now deﬁne the following two favorable properties of adversarial losses.

Property 1 (Weak favorable property) For any ﬁxed pd, LG has a global minimum at pg = pd.
Property 2 (Strong favorable property) For any ﬁxed pd, LG has a unique global minimum at pg = pd.
We can see that Property 2 makes LG − L∗G a divergence of pd and pg for any ﬁxed pd, where L∗G = LG pg=pd is a constant term irrelevant to optimization. Moreover, Property 1 provides a weaker version when the identity of indiscernibles is not necessary.

3.2 Ψ and ψ Functions
In order to derive the necessary and sufﬁcient conditions for Properties 1 and 2, we ﬁrst observe from (6) that
LG = max pd(x) f (D(x)) + pg(x) g(D(x)) dx (7)
Dx
= max pd(x) f (D(x)) + pg(x) g(D(x)) dx (8)
xD

Hao-Wen Dong, Yi-Hsuan Yang

= (pd(x) + pg(x))

x

(9)

max pd(x) f (D(x)) + pg(x) g(D(x)) dx .

D pd(x) + pg(x) pd(x) + pg(x)

Note that since D can be any function, we can exchange the
order of the integral and the maximum in (7), and obtain (8).1 Now, if we let γ˜ = pd(xp)d+(xp)g(x) and y˜ = D(x), we get

LG = (pd(x) + pg(x)) max γ˜ f (y˜) + (1 − γ˜) g(y˜) dx .

x

y˜

(10)

Please

note

that

γ˜(x)

=

1 2

if

and

only

if

pd(x)

=

pg (x).

Let us now consider the latter term inside the integral

and deﬁne the following two functions:

Ψ (γ, y) = γ f (y) + (1 − γ) g(y) ,

(11)

ψ(γ) = max Ψ (γ, y) ,

(12)

y

where γ ∈ [0, 1] and y ∈ R are two variables independent of x. We visualize in Figs. 3(a)–(d) the Ψ functions and Fig. 4 the ψ functions for different common adversarial losses.2
These two functions actually reﬂect some important characteristics of the adversarial losses (see Section 3.5) and will be used intensively in our theoretical analysis.

3.3 Necessary and Sufﬁcient Conditions for the Favorable Properties of Adversarial Losses
For the necessary conditions of Properties 1 and 2, we have two theorems as follows (see Appendix A for the proofs).
Theorem 1 If Property 1 holds, then for any γ ∈ [0, 1], ψ(γ) + ψ(1 − γ) ≥ 2 ψ( 21 ).
Theorem 2 If Property 2 holds, then for any γ ∈ [0, 1] \ { 12 }, ψ(γ) + ψ(1 − γ) > 2 ψ( 12 ).
For the sufﬁcient conditions, we have the following two theorems (see Appendix A for the proofs).
Theorem 3 If ψ(γ) has a global minimum at γ = 12 , then Property 1 holds.
Theorem 4 If ψ(γ) has a unique global minimum at γ = 12 , then Property 2 holds.
With Theorems 1–4, we can easily check if a pair of component functions f and g form a valid adversarial loss.
Moreover, we also have the following theorem for a more speciﬁc guideline for choosing or designing the component functions f and g.
1 We note that some regularization approaches might make this statement false, but we do not consider the regularization approaches here in order to isolate the effects of the component functions.
2 For the Wasserstein loss, ψ is only deﬁned at γ = 0.5, where it takes the value of zero. Hence, we do not include it in Fig. 4.

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

5

1.00

(a) classic

3.63 1.00

(b) least squares

3.50 1.00

(c) Wasserstein

5.00

0.75

1.47 0.75

1.62 0.75

2.50

0.50

-0.69 0.50

-0.25 0.50

0.00

0.25

-2.85 0.25

-2.12 0.25

-2.50

0.00-5.0 1.00

-2.5 0y.0 2.5 (d) hinge

5.0 -5.01 4.00

0.00-1.0 1.00

-0.2 0y.5 1.2 (e) absolute

2.0 -4.00 1.00

0.00-5.0 1.00

-2.5 0y.0 2.5 (f) asymmetric

5.0 -5.00 5.00

0.75

1.50 0.75

0.25 0.75

2.50

0.50

-1.00 0.50

-0.50 0.50

0.00

0.25

-3.50 0.25

-1.25 0.25

-2.50

0.00-5.0 -2.5 0.0 2.5 5.0 -6.00 0.00-1.0 -0.2 0.5 1.2 2.0 -2.00 0.00-5.0 -2.5 0.0 2.5 5.0 -5.00

y

y

y

Fig. 3 Graphs of the Ψ functions of different adversarial losses. The green lines show the domains of the ψ functions (i.e., the value(s) that y can take for different γ in (12)). The star marks, and any points on the yellow dashed lines, are the minimum points of ψ. The midpoints of the color maps are intentionally set to the minima of ψ (i.e., the values taken at the star marks or the yellow segments). Note that γ ∈ [0, 1] and y ∈ R, so we plot different portions of y where the characteristics of Ψ can be clearly seen. (Best viewed in color.)

0.0

0.2 (0.50, -0.25)
0.4

0.6

(0.50, -0.50)

0.8

classic least squares

(0.50, -0.69)

hinge absolute

(0.50, -1.00)

1.00.00

0.25

0.50

0.75

1.00

Fig. 4 Graphs of the ψ functions of different adversarial losses. The star marks indicate their minima.

Theorem 5 If f + g ≤ 0 and there exists some y∗ such that f (y∗) = g(y∗) and f (y∗) = −g (y∗) = 0, then ψ(γ) has a unique global minimum at γ = 12 .
By Theorems 4 and 5, we now see that any component function pair f and g that satisﬁes the prerequisites in Theorem 5 makes LG − L∗G a divergence between pd and pg for any ﬁxed pd. However, we should note that in practice, the discriminator often cannot reach optimality at each iteration. Therefore, as discussed by Nowozin et al (2016) and Fedus et al (2018), the objective of the generator can be considered as variational divergence minimization (i.e., to minimize a lower bound of some divergence between pd and pg), where the divergence is estimated by the discriminator.
Interestingly, while such a theoretical analysis has not been done before, it happens that all the adversarial loss

functions listed in Table 1 have such favorable properties (see Table 1 for the y∗ in Theorem 5 for each loss). We intend to examine in Section 5.3 empirically the cases when the prerequisites of Theorem 5 do not hold.
3.4 Loss Functions for the Generator
Intuitively, the generator should minimize the divergencelike measure estimated by the discriminator, and we have accordingly h = g. However, some prior works have also investigated setting h different from g. For example, the classic nonsaturating loss changes the concavity of h in the classic minimax loss (Goodfellow et al (2014); see Table 1). In general, most alternative generator losses do not change the solutions of the game and are proposed base on some heuristics. While our theoretical analysis concerns with only f and g, we intend to empirically examine the effects of the component function h in the generator loss in Section 5.5.
3.5 Analyzing the Adversarial Game by the Ψ Functions
In fact, Fig. 3 gives us some new insights regarding the adversarial behaviors of the discriminator and the generator. On one hand, if we follow (10) and consider y˜ = D(x) and γ˜(x) = pd(xp)d+(xp)g(x) , then the discriminator can be viewed as maximizing Ψ along the y˜-axis. On the other hand, since the generator is trained to push pg towards pd, it can be viewed as minimizing Ψ along the γ˜-axis. From this view, we can now see why all these Ψ functions are saddle-shaped,

6

Hao-Wen Dong, Yi-Hsuan Yang

which reﬂects the minimax nature of the game, and have saddle points at γ = 21 , which is when pd(x) = pg(x). Ideally, if the discriminator can be trained till optimality at each iteration, then we will always stay on the green line (i.e., the domain of ψ). In this case, the generator can be viewed as minimizing Ψ along the green line (i.e., minizing ψ). Please note that since LG is an integral over all possible x, such adversarial game is actually being played in a (usually) high dimensional space.
Moreover, we can see from Fig. 3 that the neighborhoods around the saddle points of Ψ possess distinct characteristics for different adversarial losses, and such difference can affect the game dynamics and the optimization. Further investigation on the properties of Ψ and ψ might lead insights into why and how different component functions perform differently in practice, and we leave it as a future work.
3.6 Designing Adversarial Losses by the Landscape of Ψ
By observing and designing the landscape of Ψ , we propose here two new adversarial losses:
– The absolute loss, with f (y) = −h(y) = −|1 − y|, g(y) = −|y|. Its Ψ -landscape is similar to those of the least squares and the hinge losses (see Fig. 3(e)). Note that we can obtain the absolute loss by simply replacing the square functions in the least squares loss with the absolute functions.
– The asymmetric loss, with f (y) = −|y|, g(y) = h(y) = −y. Note that the asymmetric loss is only different from the Wasserstein loss in f . Its Ψ -landscape is similar to that of the Wasserstein loss, but the positive part of y is blocked (see Fig. 3(f)).
We intend to examine these two new losses in our empirical study in Section 5.4.
4 Proposed Comparative Framework for Adversarial Losses
4.1 DANTest

In this paper, we adopt the discriminative adversarial networks (DANs; see Section 2.4) and propose a new, simple comparative framework for adversarial losses, which is dubbed DANTest and goes as follows:
1. Build several DANs. For each of them, the generator G takes as input a real sample and outputs a fake label. The discriminator D takes as input a real sample with either its true label, or a fake label predicted by G, and outputs a scalar indicating if such a “sample–label” pair is real.
2. Train these DANs with different component loss functions, regularization approaches or hyperparameters.
3. Predict the labels of test data by the trained models. 4. Compare the performance of different models with stan-
dard evaluation metrics used in supervised learning (e.g., classiﬁcation accuracy).
Although we take a classiﬁcation task as an example here, we note that the proposed framework is generic and can also be applied to other supervised learning tasks, as long as the evaluation metrics for that task are well deﬁned.
Note that even though the generator is no longer a generative model in this framework, the discriminator in a DAN is still trained by the exact same loss function as that in a conditional GAN (see Section 2.4). Therefore, we can still gain insight into the performance and stability for different adversarial losses with the proposed framework. Moreover, recent work has also applied adversarial losses to various supervised scenarios (Isola et al 2017; Zhu et al 2017; Ledig et al 2017; Tzeng et al 2017). Hence, it is worth investigating the behaviors of adversarial losses in different scenarios.
4.2 Imbalanced Dataset Test
An extension of the proposed framework is the imbalanced dataset test, where we can examine the ability of different adversarial losses on datasets that feature class imbalance. This can serve as a measure of the mode collapse phenomenon, which is a commonly-encountered failure case in GAN training. By testing on datasets with different levels of imbalance, we can examine how different adversarial losses suffer from the mode collapse problem.

In the previous section, we derive the necessary and sufﬁcient conditions such that an adversarial loss is a divergencelike measure between the model and the data distributions. However, we would also like to know how different adversarial losses perform empirically against one another. In particular, we aim to decouple the effects of the component functions and the regularization approaches. In order to avoid the high computational cost to conduct such a comparative experiment, we need an efﬁcient way to systematically compare different adversarial losses.

5 Experiments and Empirical Results
5.1 Datasets
All the experiments reported here are done based on the DANTest. If not otherwise speciﬁed, we use the MNIST handwritten digits database (LeCun et al 1998), which we refer to as the standard dataset. As it is class-balanced, we create two imbalanced versions of it. The ﬁrst one, referred to as the imbalanced dataset, is created by augmenting the

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

7

Table 3 Network architectures for the generator and the discriminator used for all the experiments. For the convolutional layer (conv), the values represent (from left to right): the number of ﬁlters, the kernel sizes and the strides. For the max pooling (maxpool) layer, the values represent (from left to right): the pool sizes and the strides. For the dense (dense) layer, the value indicates the number of nodes. The activation functions are ReLUs except for the last layer of the generator, which uses the softmax functions, and the last layer of the discriminator, which has no activation function.

Generator (G)

conv

32 3×3 3×3

conv

64 3×3 3×3

maxpool - 2×2 2×2

dense 128

dense 10

Discriminator (D)

conv

32 3×3 3×3

conv

64 3×3 3×3

maxpool - 2×2 2×2

dense 128

dense 1

training samples for digit ‘0’ by shifting them each by one pixel to the top, bottom, left and right, so that it contains ﬁve times more training samples of ‘0’ than the standard dataset. Moreover, we create the very imbalanced dataset, where we have seven times more training samples for digit ‘0’ than the standard dataset. For other digits, we randomly sample from the standard dataset and intentionally make the sizes of the resulting datasets identical to that of the standard dataset. Note that we use the same test set for all the experiments.

5.2 Implementation Details
We implement both the generator G and the discriminator D as convolutional neural networks (CNNs; see Appendix 3 for the network architectures). We use the batch normalization (Ioffe and Szegedy 2015) in G. If the spectral normalization is used, we only apply it to D, otherwise we use the layer normalization (Ba et al 2016) in D. We concatenate the label vector to each layer of D. If not otherwise speciﬁed, we use Euclidean norms for the gradient penalties and set λ to 10.0 (see (4)), k to 1.0 and c to 0.01 (see Table 2). We use the Adam optimizers (Kingma and Ba 2014) with α = 0.001, β1 = 0.0 and β2 = 0.9. We alternatively update G and D once in each iteration and train the model for 100,000 generator steps. The batch size is 64. We implement the model in Python and TensorFlow (Abadi et al 2016). We run each experiment for ten runs and report the mean and the standard deviation of the error rates.

5.3 Examining the Necessary Conditions for Favorable Adversarial Loss Functions

As discussed in Section 3.3, we examine here the cases when the prerequisites in Theorem 5 do not hold. Speciﬁcally, We change the training objective for the discriminator into the following -weighted losses:

max Ex∼pd [f (D(x))] + Ex˜∼pg [g(D(x˜))] ,

(13)

D

Table 4 Error rates (%) for the -weighted versions of the nonsaturating, the Wasserstein and the hinge losses (see (13)) on the standard dataset. Note that = 1.0 corresponds to the original losses.

= 0.5 = 0.9 = 1.0 = 1.1 = 2.0

nonsaturating
8.47±0.36 8.96±0.63 8.25±0.35 8.62±0.45 9.18±0.94

Wasserstein
73.16±6.36 57.66±5.13 5.89±0.26 60.30±7.61 69.54±5.37

hinge
15.20±2.46 8.94±0.87 6.59±0.31 8.02±0.35 11.87±0.85

where ∈ R is a constant. We can see that the prerequisites in Theorem 5 do not hold when = 1. We consider the classic nonsaturating, the Wasserstein and the hinge losses, using the spectral normalization for regularization. Table 4 shows the results for = 0.5, 0.9, 1.0, 1.1, 2.0 (see Appendix B for the graphs of the corresponding Ψ and ψ functions). We can see that all the original losses (i.e., = 1) result in the lowest error rates. In general, the error rates increase as goes away from 1.0. Notably, the Wasserstein loss turn out failing with error rates over 50% when = 1.

5.4 On Different Discriminator Loss Functions
In this experiment, we aim to compare different discriminator loss functions and decouple the effects of the component functions and the regularization terms. Hence, we evaluate a comprehensive set (in total 168) of different combinations of 12 component functions and 14 regularization approaches.
For the component functions, we consider the classic minimax and the classic nonsaturating losses (Goodfellow et al 2014), the Wasserstein loss (Arjovsky et al 2017), the least squares loss (Mao et al 2017), the hinge loss (Lim and Ye 2017; Tran et al 2017), the relativistic average and the relativistic average hinge losses (Jolicoeur-Martineau 2018), as well as the absolute and the asymmetric losses we propose and describe in Section 3.5.
For the regularization approaches, we consider the coupled, the local, the R1 and the R2 gradient penalties (GP; see Section 2.2) and the spectral normalization (SN; see Section 2.3). For the coupled and the local gradient penalties, we examine both the two-side and the one-side versions (see Table 2). We will use in the captions OCGP and TCGP as the shorthands for the one-side and the two-side coupled gradient penalties, respectively, and OLCP and TLCP for the one-side and the two-side local gradient penalties, respectively. We also consider the combinations of the spectral normalization with different gradient penalties.
We report in Table 5 the results for all the combinations and present in Fig. 5 the training progress for the nonsaturating and the hinge losses. We can see that there is no single winning component functions and regularization approach across all different settings. Some observations are:

8

Hao-Wen Dong, Yi-Hsuan Yang

Table 5 Error rates (%) for different adversarial losses and regularization approaches, on the standard dataset. See Section 5.4 and 5.5 for the abbreviations. Underlined and bold fonts indicate respectively entries with the lowest and lowest-three mean error rates in each column.

unregularized TCGP

TLGP

R1 GP

R2 GP

SN SN + TCGP SN + TLGP SN + R1 GP SN + R2 GP

classic (M) (2014) classic (N) (2014) classic (L)

9.11±0.63 26.83±7.17 17.38±5.16

5.65±0.27 5.64±0.23 5.66±0.36

5.42±0.17 5.56±0.31 5.55±0.16

19.01±3.73 14.67±4.86 18.49±5.51

12.91±1.13 13.80±3.20 14.92±5.20

7.37±0.52 8.25±0.35 7.98±0.36

5.55±0.37 5.52±0.16 5.70±0.36

5.57±0.28 5.61±0.50 5.48±0.29

11.16±2.66 12.98±2.71 15.45±6.54

14.00±2.49 13.50±3.78 17.61±7.60

hinge (M) hinge (N) hinge (L) (2017; 2017)

5.57±0.26 37.55±20.22 11.50±5.32

4.83±0.34 5.00±0.24 5.01±0.26

4.88±0.25 4.97±0.24 4.89±0.18

7.31±1.49 7.34±1.83 8.96±3.55

9.49±5.30 7.54±1.31 7.71±1.82

6.22±0.23 6.90±0.33 6.59±0.31

4.93±0.20 5.05±0.22 4.97±0.19

5.06±0.33 5.06±0.39 5.18±0.27

10.62±2.10 11.91±4.02 13.63±4.13

12.91±4.29 12.10±4.74 11.35±3.40

Wasserstein (2017) least squares (2017) relativistic (2018) relativistic hinge (2018)

7.69±0.33 7.15±0.47 90.20±0.00 52.01±9.38

5.04±0.19 7.27±0.44 5.25±0.25 8.28±10.26

4.92±0.23 6.70±0.44 5.01±0.31 4.71±0.12

13.89±20.64 30.12±28.43 8.00±1.63 8.39±1.92

7.25±1.19 32.44±21.05 8.75±5.83
7.67±1.82

5.89±0.26 7.88±0.45 7.14±0.39 6.44±0.16

5.50±0.18 6.69±0.25 5.35±0.29 5.02±0.31

5.76±0.70 7.11±0.37 5.25±0.26 5.03±0.21

13.74±5.47 9.91±1.55 9.31±2.01 12.56±4.42

13.82±4.93 11.56±4.09 8.62±0.59 12.40±4.55

absolute asymmetric

6.69±0.24 7.81±0.27

5.23±0.29 5.20±0.26 8.01±1.96 4.77±0.34 4.94±0.14 8.79±3.18

6.64±0.51 6.79±0.45 5.23±0.13 7.33±1.01 5.98±0.40 5.60±0.29

5.18±0.35 10.42±3.07 9.93±2.28 5.82±0.44 8.46±0.43 8.80±1.18

With respect to the component functions—
– The classic minimax and nonsaturating losses never get the lowest three error rates for all different settings.
– The hinge, the asymmetric and the two relativistic losses are robust to different regularization approaches and tend to achieve lower error rates. We note that they also feature lower computation costs as all their components functions are piecewise linear (see Table 1 and Section 3.6).
– The relativistic average loss outperforms both the classic minimax and nonsaturating losses across all regularization approaches. But, the relativistic average hinge loss does not always outperform the standard hinge loss.
With respect to the regularization approaches—
– The coupled and the local gradient penalties outperform the R1 and the R2 gradient penalties across nearly all different component functions, no matter whether the spectral normalization is used or not.
– The coupled and the local gradient penalties stabilize the training (see Fig. 5) and tend to have lower error rates.
– The R2 gradient penalties achieve lower error rates than the R1 gradient penalties. In some cases, they can be too strong and even stop the training early (see Fig. 5 (a)). This is possibly because they encourage D to have small gradients, and thus the gradients for both D and G might vanish when pg and pd are close enough.
– Combining either the coupled or the local gradient penalty with the spectral normalization usually leads to higher error rates than using the gradient penalty only.
– Similarly, combining either the R1 or the R2 gradient penalty with the spectral normalization degrades the result and leads to unstable training (see Figs. 5(b) and (d)). This implies that the R1 and the R2 gradient penalties do not work well with the spectral normalization.
– Using the one-side gradient penalties instead of their two-side counterparts increase the error rates by 0.1– 9.5% (see Appendix C for the results).
We note that some combinations result in remarkably high error rates, e.g., “classic minimax loss + R1 gradient

Table 6 Error rates (%) for different gradient penalties on datasets with different levels of imbalance, using the classic nonsaturating loss.

TCGP OCGP TLGP OLGP R1 GP R2 GP

standard
5.64±0.23 7.20±0.39 5.51±0.27 6.92±0.21 14.67±4.86 13.80±3.20

imbalanced
7.09±0.64 8.86±0.65 6.94±0.28 8.63±0.75 18.66±5.60 15.70±2.07

very imbalanced
8.12±0.31 10.23±0.75
8.10±0.55 10.21±0.52 27.90±9.59 29.97±12.4

penalty,” “least squares loss + R1 gradient penalty,” and “least squares loss + R2 gradient penalty.” Moreover, We also conduct the imbalanced dataset test (see Section 4.1) on the two imbalanced datasets described in Section 5.1 to compare the regularization approaches, using the classic nonsaturating loss. As shown in Table 6, the error rates increase as the level of imbalance increases. The two-side local gradient penalty achieve the lowest error rates across all three datasets. The error rates for the R1 and the R2 gradient penalties increase signiﬁcantly when the dataset goes imbalanced.
5.4.1 Effects of the Lipschitz Constants
In this experiment, we examine the effects of the Lipschitz constant (k) used in the coupled and the local gradient penalties (see Table 2). We use the classic nonsaturating loss and report in Fig. 6 the results for k = 0.01, 0.1, 1, 10, 100. We can see that the error rate increases as k goes away from 1.0, which suggests that k = 1 is a good default value. Moreover, the two-side gradient penalties achieve lower error rates and are more sensitive to k than their one-side counterparts.
We note that Petzka et al (2018) suggested that the oneside coupled gradient penalty are preferable to the two-side version and showed empirically that the former has more stable behaviors. However, we observe in our experiments that the two-side penalties usually lead to faster convergence to lower error rates as compared to the one-side penalties (see Fig. 5). This is possibly because the gradients for G become smaller as pg move towards pd (and eventually zero

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

9

Fig. 5 Error rates along 40 tthhee ntroaninsaintugraptirnoggraensds tfhoer 35 hinge losses with different 30 regularization approaches. 25 The models are evaluated 20 every 100 stpdf and the results are smoothed by 15 a 5-point median ﬁlter. 10 The shaded regions rep- 5 resent the standard deviations over ten runs. (Best 00 viewed in color.)
40 35 30 25 20 15 10 5 00

Error rate (%)

Error rate (%)

(a) nonsaturating
20 Gener4a0tor step (6×01000) 80
(c) hinge
20 Gener4a0tor step (6×01000) 80

40

35

30

Error rate (%)

25

20

15

10

5

100

00

40

35

30

Error rate (%)

25

20

15

10

5

100

00

(b) nonsaturating + SN

20 Gener4a0tor step (6×01000) 80

100

(d) hinge + SN

20 Gener4a0tor step (6×01000) 80

100

unregularized TCGP OCGP TLGP OLGP R1GP R2GP

100.0 80.0 60.0

k = 0.01 k = 0.1 k=1 k = 10 k = 100

Error rate (%)

40.0

20.0

0.0 TCGP OCGP TCLP OCLP
Fig. 6 Error rates for different Lipschitz constants (k) in the coupled and the local gradient penalties, using the classic nonsaturating loss. (Best viewed in color.)

when pd = pg), which can slow down the training. In contrast, the two-side penalties can alleviate this issue by encouraging the norm of the gradients to be a ﬁxed value.
5.4.2 Effects of the Penalty Weights
We examine in this experiment the effects of the penalty weights (λ) for the R1 and the R2 gradient penalties (see (4)). We consider the classic nonsaturating, the Wasserstein and the hinge losses. Fig. 7 shows the results for λ = 0.01, 0.1, 1, 10, 100. We can see that the R2 gradient penalty tends to outperform the R1 gradient penalty. However, they are both sensitive to the value of λ, and therefore future research should run hyperparmeter search for λ to ﬁnd out its optimal

value. We also note that when the spectral normalization is not used, the hinge loss is less sensitive to λ than the other two losses. Moreover, when spectral normalization is used, the error rate increases as λ increases, which again implies that the R1 and the R2 gradient penalties and the spectral normalization do not work well together.
5.5 On Different Generator Loss Functions
As discussed in Section 3.4, we also aim to examine the effects of the generator loss function h(·). We consider the classic and the hinge losses for the discriminator and the following three generator loss functions: minimax (M)— h(x) = g(x), nonsaturating (N)—h(x) = log(1+e−x), and linear (L)—h(x) = −x. We report the results in the ﬁrst six rows of Table 5. For the classic discriminator loss, we see no single winner among the three generator loss functions across all the regularization approaches, which implies that the heuristics behind these alternative losses might not be true. For the hinge discriminator loss, the minimax generator loss is robust to different regularization approaches and achieves three lowest and four lowest-three error rates.
5.6 Effects of the Momentum Terms of the Optimizers
We observe a trend in the literature towards using smaller momentum (Radford et al 2016) or even no momentum (Ar-

10
Fig. 7 Error rates for different penalty weights (λ) in the R1 and the R2 gradient penalties. (Best viewed in color.)
Fig. 8 Error rates for different momentum terms (β1) in the Adam optimizers. The parenthesized superscripts ‘(G)’ and ‘(D)’ on β1 indicate respectively the generator and the discriminator. (Best viewed in color.)

Error rate (%)

Error rate (%)

(a) without the spectral normalization

70.0

= 0.01

= 0.1

=1

= 10

= 100

60.0

R1 R2

50.0

40.0

30.0

20.0

10.0

0.0 nonsaturating wasserstein hinge

10.0

(a) classic loss

1(D) = 0.5

1(D) = 0.0

1(D) = 0.5

1(D) = 0.9

8.0

SN + TCGP

SN + OCGP

6.0

4.0

2.0

0.0 -0.5

0.0

0.5

0.9

1(G)

Error rate (%)

Error rate (%)

Hao-Wen Dong, Yi-Hsuan Yang

50.0

(b) with the spectral normalization

= 0.01

= 0.1

=1

= 10

= 100

40.0

SN + R1 SN + R2

30.0

20.0

10.0

0.0 nonsaturating wasserstein hinge

(b) hinge loss

8.0

1(D) = 0.5

1(D) = 0.0

1(D) = 0.5

1(D) = 0.9

7.0

SN + TCGP

SN + OCGP

6.0

5.0

4.0

3.0

2.0

1.0

0.0 -0.5

0.0

0.5

0.9

1(G)

jovsky et al 2017; Gulrajani et al 2017; Miyato et al 2018; Brock et al 2018) for GAN optimization. Hence, we would also like to examine the effects of momentum terms in the optimizers with the proposed framework. Since the generator and the discriminator are trained by distinct objectives, we also investigate setting different momentum values for them. Moreover, as suggested by Gidel et al (2018), we also include a negative momentum value of −0.5. We present in Fig. 8 the results for all combinations of β1 = −0.5, 0.0, 0.5, 0.9 for the generator and the discriminator, using the classic nonsaturating loss along with the spectral normalization and the coupled gradient penalties for regularization. We can see that for the two-side coupled gradient penalty, using larger momenta in both the generator and the discriminator leads to lower error rates. However, there is no speciﬁc trend for the one-side coupled gradient penalty.
6 Discussions and Conclusions
In this paper, we have shown in theory what certain types of component functions form a valid adversarial loss based on the intuition that an adversarial loss should provide a divergence-like measure between the model and the data distributions. We have also introduced a new comparative framework called DANTest for adversarial losses. With the proposed framework, we systematically compared the combinations of different component functions and regulariza-

tion approaches to decouple their effects. Our empirical results show that there is no single winning component functions or regularization approach across all different settings.
We should note that while the discriminator in a DAN is trained to optimize the same objective as in a conditional GAN, the generators in the two models actually work in distinct ways (i.e., X → Y in a DAN versus Z → X in a GAN). Hence, it is unclear whether the empirical results can be generalized to conditional and unconditional GANs. However, as discussed in Section 4.1, the idea of adversarial losses is rather generic and not limited to unsupervised learning. In principle, the adversarial loss estimates a divergence-like measure between the model and the data distributions, which then serves as the training criterion for the model, no matter whether the model is generative or discriminative, as discussed in Section 3. Moreover, as compared to the commonly used metrics for evaluating generative models, such as the Inception Score (Salimans et al 2016) and the Fre´chet Inception Distance (Heusel et al 2017) adopted in Lucic et al (2018) and Kurach et al (2018), the proposed framework is simpler and is easier to control and extend. This allows us to evaluate new adversarial losses even with limited computation resource.
On the other hand, our theoretical analysis not only verify again the validdity of some common adversarial losses, but also reveals a large class of component functions valid for adversarial losses (see Theorem 5). Moreover, it also pro-

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

11

vides us a new perspective on adversarial losses as discussed in Section 3.5. However, we should note that in practice, since the discriminator usually cannot reach optimality at each iteration, the theoretical minimum of the adversarial loss is not necessarily achievable. Hence, a future direction is to investigate the necessary and sufﬁcient conditions for the existence and the uniqueness of a Nash equilibrium.
References
Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M, Kudlur M, Levenberg J, Monga R, Moore S, Murray DG, Steiner B, Tucker P, Vasudevan V, Warden P, Wicke M, Yu Y, Zheng X (2016) Tensorﬂow: A system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp 265–283
Arjovsky M, Chintala S, Bottou L (2017) Wasserstein generative adversarial networks. In: Proceedings of the 34th International Conference on Machine Learning (ICML), pp 214–223
Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv:160706450
Brock A, Donahue J, Simonyan K (2018) Large scale GAN training for high ﬁdelity natural image synthesis. arXiv:180911096
Fedus W, Rosca M, Lakshminarayanan B, Dai AM, Mohamed S, Goodfellow I (2018) Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In: International Conference on Learning Representations (ICLR)
Gidel G, Hemmat RA, Pezeshki M, Lepriol R, Huang G, LacosteJulien S, Mitliagkas I (2018) Negative momentum for improved game dynamics. In: NeurIPS Workshop on Smooth Games Optimization and Machine Learning
Goodfellow I (2016) NIPS 2016 tutorial: Generative adversarial networks. In: Thirtieth Conference on Neural Information Processing Systems (NeurIPS)
Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: Advances in Neural Information Processing Systems 27, pp 2672–2680
Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville A (2017) Improved training of Wasserstein GANs. In: Advances in Neural Information Processing Systems 30 (NeurIPS), pp 5767–5777
Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S (2017) GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In: Advances in Neural Information Processing Systems 30 (NeurIPS), pp 6626–6637
Ioffe S, Szegedy C (2015) Batch Normalization: Accelerating deep network training by reducing internal covariate shift. In: Proceedings of the 34th International Conference on Machine Learning (ICML), pp 448–456
Isola P, Zhu JY, Zhou T, Efros AA (2017) Image-to-image translation with conditional adversarial nets. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
Jolicoeur-Martineau A (2018) The relativistic discriminator: a key element missing from standard GAN. arXiv preprint arXiv:180700734
Kingma DP, Ba J (2014) Adam: A method for stochastic optimization. In: International Conference on Learning Representations (ICLR)
Kodali N, Abernethy J, Hays J, Kira Z (2017) On convergence and stability of GANs. arXiv preprint arXiv:170507215
Kurach K, Lucic M, Zhai X, Michalski M, Gelly S (2018) The GAN landscape: Losses, architectures, regularization, and normaliza-

tion. In: International Conference on Learning Representations (ICLR) LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278–2324 Ledig C, Theis L, Huszar F, Caballero J, Cunningham A, Acosta A, Aitken AP, Tejani A, Totz J, Wang Z, Shi W (2017) Photorealistic single image super-resolution using a generative adversarial network. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 105–114 Li CL, Chang WC, Cheng Y, Yang Y, Po´czos B (2017) MMD GAN: Towards deeper understanding of moment matching network. In: Advances in Neural Information Processing Systems 30 (NeurIPS), pp 2203–2213 Lim JH, Ye JC (2017) Geometric GAN. arXiv preprint arXiv:170502894 Lucic M, Kurach K, Michalski M, Gelly S, Bousquet O (2018) Are GANs created equal? a large-scale study. In: Advances in Neural Information Processing Systems 31 (NeurIPS), pp 698–707 Mao X, Li Q, Xie H, Lau RY, Wang Z, Smolley SP (2017) Least squares generative adversarial networks. In: IEEE International Conference on Computer Vision (ICCV) Mescheder L, Geiger A, Nowozin S (2018) Which training methods for GANs do actually converge? In: Proceedings of the 35th International Conference on Machine Learning (ICML), pp 3481–3490 Mirza M, Osindero S (2014) Conditional generative adversarial nets. arXiv:14111784 Miyato T, Kataoka T, Koyama M, Yoshida Y (2018) Spectral normalization for generative adversarial networks. In: International Conference on Learning Representations (ICLR) Mroueh Y, Sercu T (2017) Fisher GAN. In: Advances in Neural Information Processing Systems 30 (NeurIPS), pp 2513–2523 Mroueh Y, Sercu T, Goel V (2017) McGan: Mean and covariance feature matching GAN. In: Proceedings of the 34th International Conference on Machine Learning (ICML), pp 2527–2535 Nowozin S, Cseke B, Tomioka R (2016) f-GAN: Training generative neural samplers using variational divergence minimization. In: Advances in Neural Information Processing Systems 29 (NeurIPS), pp 271–279 Petzka H, Fischer A, Lukovnicov D (2018) On the regularization of wasserstein gans. In: International Conference on Learning Representations (ICLR) Radford A, Metz L, Chintala S (2016) Unsupervised representation learning with deep convolutional generative adversarial networks. In: International Conference on Learning Representations (ICLR) Salimans T, Goodfellow I, Zaremba W, Cheung V, Radford A, Chen X (2016) Improved techniques for training GANs. In: Advances in Neural Information Processing Systems 29 (NeurIPS), pp 2234– 2242 dos Santos CN, Wadhawan K, Zhou B (2017) Learning loss functions for semi-supervised learning via discriminative adversarial networks. NeurIPS Workshop on Learning with Limited Labeled Data Tran D, Ranganath R, Blei DM (2017) Hierarchical implicit models and likelihood-free variational inference. In: Advances in Neural Information Processing Systems 30 (NeurIPS), pp 5523–5533 Tzeng E, Hoffman J, Saenko K, Darrell T (2017) Adversarial discriminative domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 7167–7176 Zhu JY, Park T, Isola P, Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (ICCV)

12

Appendix A Proofs of Theorems

Theorem 1 If Property 1 holds, then for any γ ∈ [0, 1], ψ(γ) + ψ(1 − γ) ≥ 2 ψ( 21 ).
Proof Since Property 1 holds, we have for any ﬁxed pd,

LG ≥ LG pg=pd . (14) Let us consider

pd(x) = γ δ(x − s) + (1 − γ) δ(x − t) ,

(15)

pg(x) = (1 − γ) δ(x − s) + γ δ(x − t) .

(16)

for some γ ∈ [0, 1] and s, t ∈ X , s = t. Then, we have

LG pg=pd

(17)

= max pd(x) f (D(x)) + pd(x) g(D(x)) dx

(18)

Dx

= max pd(x) (f (D(x)) + g(D(x))) dx

(19)

Dx

= max (γ δ(x − s) + (1 − γ) δ(x − t))

Dx

(20)

(f (D(x)) + g(D(x))) dx

= max γ (f (D(s)) + g(D(s)))

D

(21)

+ (1 − γ)(f (D(t)) + g(D(t)))

= max γ (f (y1) + g(y1)) + (1 − γ)(f (y2) + g(y2))
y1 ,y2
(22)

= max γ (f (y1) + g(y1)) + max (1 − γ)(f (y2) + g(y2))

y1

y2

(23)

= max f (y) + g(y)

(24)

y

= 2 ψ( 12 ) . (25)

Moreover, we have

LG

= max pd(x) f (D(x)) + pg(x) g(D(x)) dx

(26)

Dx

= max
D

(γ δ(x − s) + (1 − γ) δ(x − t)) f (D(x))
x
+ ((1 − γ) δ(x − s) + γ δ(x − t)) g(D(x)) dx
(27)

= max γ f (D(s)) + (1 − γ) f (D(t))

D

(28)

+ (1 − γ) g(D(s)) + γ g(D(t))

= max γ f (y1) + (1 − γ) g(y1))

y1 ,y2

(29)

+ (1 − γ) f (y2) + γ g(y2)

= max γ f (y1) + (1 − γ) g(y1))
y1
(30) + max(1 − γ) f (y2) + γ g(y2)
y2

Hao-Wen Dong, Yi-Hsuan Yang

= ψ(γ) + ψ(1 − γ) .

(31)

(Note that we can obtain (22) from (21) and (29) from (28) because D can be any function and thus D(s) is independent of D(t).)
As (14) holds for any ﬁxed pd, by substituting (25) and (31) into (14), we get
ψ(γ) + ψ(1 − γ) ≥ 2 ψ( 12 ) (32)
for any γ ∈ [0, 1], which concludes the proof.

Theorem 2 If Property 2 holds, then for any γ ∈ [0, 1] \ { 12 }, ψ(γ) + ψ(1 − γ) > 2 ψ( 12 ).
Proof Since Property 2 holds, we have for any ﬁxed pd,

LG pg=pd > LG pg=pd .

(33)

Following the proof of Theorem 1, consider

pd(x) = γ δ(x − s) + (1 − γ) δ(x − t) ,

(34)

pg(x) = (1 − γ) δ(x − s) + γ δ(x − t) ,

(35)

for some γ ∈ [0, 1] and some s, t ∈ X , s = t. It can be easily shown that pg = pd if and only if γ = 12 .
As (33) holds for any ﬁxed pd, by substituting (34) and (35) into (33), we get
ψ(γ) + ψ(1 − γ) > 2 ψ( 12 ) , (36)
for any γ ∈ [0, 1] \ { 21 }, concluding the proof.
Theorem 3 If ψ(γ) has a global minimum at γ = 12 , then Property 1 holds.

Proof First, we see that

LG pg=pd

(37)

= max pd(x) f (D(x)) + pd(x) g(D(x)) dx

(38)

Dx

= max pd(x) f (y) + pd(x) g(y) dx

(39)

yx

= max pd(x) (f (y) + g(y)) dx

(40)

yx

= max (f (y) + g(y)) pd(x) dx

(41)

y

x

= max f (y) + g(y)

(42)

y

= 2 ψ( 21 ) . (43)

On the other had, we have

LG

= max pd(x) f (D(x)) + pg(x) g(D(x)) dx

(44)

Dx

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

13

= max pd(x) f (y) + pg(x) g(y) dx

(45)

yx

= max (pd(x) + pg(x)) y x (46) pd(x) f (y) + pg(x) g(y) dx pd(x) + pg(x) pd(x) + pg(x)

= (pd(x) + pg(x))
x
max pd(x) f (y) + pg(x) g(y) y pd(x) + pg(x) pd(x) + pg(x)

dx . (47)

Since pd(xp)d+(xp)g(x) ∈ [0, 1], we have

LG = (pd(x) + pg(x)) ψ

pd(x)

dx . (48)

x

pd(x) + pg(x)

As ψ(γ) has a global minimum at γ = 21 , now we have

LG ≥

(pd

(x)

+

pg

(x))

ψ

(

1 2

)

dx

(49)

x

= ψ( 12 ) (pd(x) + pg(x)) dx

(50)

x

= 2 ψ( 12 ) . (51)

Finally, combining (43) and (51) yields

LG ≥ LG pg=pd , (52)
which holds for any pd, thus concluding the proof.
Theorem 4 If ψ(γ) has a unique global minimum at γ = 12 , then Property 2 holds.
Proof Since ψ(γ) has a unique global minimum at γ = 12 , we have for any γ ∈ [0, 1] \ 12 , ψ(γ) > ψ( 21 ) . (53)
When pg = pd, there must be some x0 ∈ X such that pg(x0) = pd(x0). Thus, pd(xp0d)(+xp0g)(x0) = 12 , and thereby ψ pd(xp0d)(+xp0g)(x0) > ψ( 21 ). Now, by (48) we have

LG pg=pd

(54)

= (pd(x) + pg(x)) ψ pd(x) dx (55)

x

pd(x) + pg(x)

>

(pd

(x)

+

pg

(x))

ψ

(

1 2

)

dx

(56)

x

= ψ( 21 ) (pd(x) + pg(x)) dx

(57)

x

= 2 ψ( 12 ) . (58)

Finally, combining (43) and (58) yields

LG pg=pd > LG pg=pd ,

(59)

which holds for any pd, thus concluding the proof.

Theorem 5 If f + g ≤ 0 and there exists some y∗ such that f (y∗) = g(y∗) and f (y∗) = −g (y∗) = 0, then ψ(γ) has a unique global minimum at γ = 21 .
Proof First, we have by deﬁnition

Ψ (γ, y) = γ f (y) + (1 − γ) g(y) .

(60)

By taking the partial derivatives, we get

∂Ψ

= f (y) − g(y) ,

(61)

∂γ

∂Ψ

= γ f (y) + (1 − γ) g (y) ,

(62)

∂y

∂2Ψ

∂y2 = γ f (y) + (1 − γ) g (y) .

(63)

We know that there exists some y∗ such that

f (y∗) = g(y∗) ,

(64)

f (y∗) = −g (y∗) = 0 .

(65)

(i) By (61) and (62), we see that

∂Ψ

= 0,

(66)

∂γ y=y∗

∂Ψ = 0.
∂y (γ,y)=( 21 ,y∗)

(67)

Now, by (66) we know that Ψ is constant when y = y∗. That is, for any γ ∈ [0, 1],

Ψ (γ, y∗) = Ψ ( 21 , y∗) . (68)

(ii) Because f + g ≤ 0, by (63) we have

∂2Ψ

= 1 f (y) + 1 g (y)

(69)

∂y2 γ= 21 2 2

≤ 0.

(70)

By (67) and (69), we see that y∗ is a global minimum point of Ψ 1 . Thus, we now have
γ= 2

Ψ ( 1 , y∗) = max Ψ ( 1 , y)

(71)

2

y

2

= ψ( 12 ) . (72)

(iii) By (62), we see that

∂Ψ

= γ f (y∗) + (1 − γ) g (y∗)

(73)

∂y y=y∗

= γ f (y∗) + (1 − γ) (−f (y∗))

(74)

= (2γ − 1) f (y∗) .

(75)

Since f (y∗) = 0, we have

∂Ψ

= 0 ∀ γ ∈ [0, 1] \ 1 .

(76)

∂y y=y∗

2

14

This shows that for any γ ∈ [0, 1] \ 21 , there must exists some y◦ such that

Ψ (γ, y◦) > Ψ (γ, y∗) .

(77)

And by deﬁnition we have

Ψ (γ, y◦) < max Ψ (γ, y)

(78)

y

= ψ(γ) .

(79)

Hence, by (77) and (78) we get

ψ(γ) > Ψ (γ, y∗) .

(80)

Finally, combining (67), (72) and (80) yields ψ(γ) > ψ( 12 ) ∀ γ ∈ [0, 1] \ 12 , (81) which concludes the proof.

Appendix B Graphs of Ψ and ψ for -weighted Losses
Figs. 9 and 10 show the graphs of Ψ and ψ, respectively, for the -weighted versions of the classic, the Wasserstein and the hinge losses. Note that for the Wasserstein loss, ψ is only deﬁned at γ = 23 , 12 , 31 when = 0.5, 1.0, 2.0, respectively, where it takes the value of zero, and thus we do not include the Wasserstein loss in Fig. 10.

Appendix C Empirical Results for One-side Penalties
Table 7 shows the results for the one-side coupled and local gradient penalties.

Hao-Wen Dong, Yi-Hsuan Yang

Towards a Deeper Understanding of Adversarial Losses under a Discriminative Adversarial Network Setting

15

1.00 -weighted classic ( = 0.5) 4.04

0.75

1.78

0.50

-0.48

0.25

-2.74

0.00-5.0 -2.5 0.0 2.5 5.0 -5.01
y
1.00-weighted Wasserstein ( = 0.5) 5.00

0.75

2.50

0.50

0.00

0.25

-2.50

0.00-5.0 -2.5 0.0 2.5 5.0 -5.00
y
1.00 -weighted hinge ( = 0.5) 4.67

0.75

2.00

0.50

-0.67

0.25

-3.33

0.00-5.0 -2.5 0.0 2.5 5.0 -6.00
y

1.00 0.75 0.50 0.25 0.00-5.0
1.00 0.75 0.50 0.25 0.00-5.0
1.00 0.75 0.50 0.25 0.00-5.0

classic
-2.5 0y.0 2.5
Wasserstein
-2.5 0y.0 2.5
hinge
-2.5 0y.0 2.5

3.63 1.47 -0.69 -2.85 5.0 -5.01
5.00 2.50 0.00 -2.50 5.0 -5.00
4.00 1.50 -1.00 -3.50 5.0 -6.00

1.00 -weighted classic ( = 2.0) 8.09

0.75

3.56

0.50

-0.96

0.25

-5.49

0.00-5.0 -2.5 0.0 2.5 5.0 -10.01
y
1.00-weighted Wasserstein ( = 2.0) 10.00

0.75

5.00

0.50

0.00

0.25

-5.00

0.00-5.0 -2.5 0.0 2.5 5.0 -10.00
y
1.00 -weighted hinge ( = 2.0) 9.34

0.75

4.00

0.50

-1.33

0.25

-6.67

0.00-5.0 -2.5 0.0 2.5 5.0 -12.00
y

Fig. 9 Graphs of the Ψ functions of different adversarial losses. The green lines show the domains of the ψ functions (i.e., the value(s) that y can take for different γ in the ψ function). The star marks, and any points on the yellow dashed lines, are the minimum points of ψ. The midpoints of the color maps are intentionally set to the minima of ψ (i.e., the values taken at the star marks or the yellow segments). Note that γ ∈ [0, 1] and y ∈ R, so we plot different portions of y where the characteristics of Ψ can be clearly seen. (Best viewed in color.)

0.0

-weighted classic ( = 0.5)

classic

0.2

-weighted classic ( = 2.0)

0.4

(0.55, -0.48)

0.6

(0.50, -0.69)

0.8

(0.45, -0.96)

1.00.00

0.25

0.50

0.75

1.00

0.0

0.2

0.4

0.6

0.8

1.0

1.2 (0.33, -1.33)

1.40.00

0.25

(0.67, -0.67)

(0.50, -1.00)

-weighted hinge ( = 0.5) hinge -weighted hinge ( = 2.0)

0.50

0.75

1.00

Fig. 10 Graphs of the ψ functions for the -weighted versions of (left) the classic and (right) the hinge losses. The star marks indicate their minima.

Table 7 Error rates (%) for different adversarial losses and regularization approaches, on the standard dataset. See Section 5.4 and 5.5 for the abbreviations. Underlined and bold fonts indicate respectively entries with the lowest and lowest-three mean error rates in each column.

classic (M) (Goodfellow et al 2014) classic (N) (Goodfellow et al 2014) classic (L)
hinge (M) hinge (N) hinge (L) (Lim and Ye 2017; Tran et al 2017)
Wasserstein (Arjovsky et al 2017) least squares (Mao et al 2017) relativistic (Jolicoeur-Martineau 2018) relativistic hinge (Jolicoeur-Martineau 2018)
absolute asymmetric

OCGP
7.15±0.77 7.20±0.39 7.12±0.61
5.82±0.31 5.69±0.30 5.77±0.29
7.60±3.02 7.99±0.35 8.03±3.32 10.70±2.51
5.95±0.19 5.85±0.35

OLGP
6.95±0.51 6.98±0.22 7.00±1.00
7.33±1.35 7.88±1.33 6.22±1.04
13.34±1.49 8.06±0.49 9.41±2.90 14.17±1.79
5.88±0.41 7.57±0.98

SN + OCGP
7.16±0.31 7.47±0.62 7.29±0.35
5.80±0.24 5.92±0.36 5.77±0.30
6.35±0.43 8.43±0.50 6.18±0.29 5.42±0.33
6.22±0.25 6.21±0.34

SN + OLGP
6.86±0.29 7.15±0.36 7.18±0.54
5.83±0.20 5.74±0.27 5.82±0.20
6.06±0.45 8.31±0.52 6.03±0.24 5.42±0.33
6.08±0.32 5.92±0.37

