Hierarchical Coding for Distributed Computing
Hyegyeong Park, Kangwook Lee, Jy-yong Sohn, Changho Suh and Jaekyun Moon School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST)
Email: {parkh, kw1jjang, jysohn1108, chsuh}@kaist.ac.kr, jmoon@kaist.edu

arXiv:1801.04686v1 [cs.DC] 15 Jan 2018

Abstract—Coding for distributed computing supports lowlatency computation by relieving the burden of straggling workers. While most existing works assume a simple master-worker model, we consider a hierarchical computational structure consisting of groups of workers, motivated by the need to reﬂect the architectures of real-world distributed computing systems. In this work, we propose a hierarchical coding scheme for this model, as well as analyze its decoding cost and expected computation time. Speciﬁcally, we ﬁrst provide upper and lower bounds on the expected computing time of the proposed scheme. We also show that our scheme enables efﬁcient parallel decoding, thus reducing decoding costs by orders of magnitude over non-hierarchical schemes. When considering both decoding cost and computing time, the proposed hierarchical coding is shown to outperform existing schemes in many practical scenarios.
I. INTRODUCTION
Enabling large-scale computations for big data analytics, distributed computing systems have received signiﬁcant attention in recent years [1]. The distributed computing system divides a computational task to a number of subtasks, each of which is allocated to a different worker. This helps reduce computing time by exploiting parallel computing options and thus enables handling of large-scale computing tasks.
In a distributed computing system, the “stragglers”, which refers to the computing nodes that slow down in some random fashion due to a variety of factors, may increase the total runtime of the computing system. To address this problem, the notion of coded computation is introduced in [2] where an (n, k) maximum distance separable (MDS) code is employed to speed up distributed matrix multiplications. The authors show that for linear computing tasks, one can design n distributed computing tasks such that any k out of n tasks sufﬁce to complete the assigned task. Since then, coded computation has been applied to a wide variety of task scenarios such as matrix-matrix multiplication [3], [4], distributed gradient computation [5]–[8], convolution [9], Fourier transform [10] and matrix sparsiﬁcation [11], [12].
While the idea of coded computation has been studied in various settings, existing works have not taken into account the underlying hierarchical nature of practical distributed systems [13]–[15]. In modern distributed computing systems, each group of workers is collocated in the same rack, which contains a Top of Rack (ToR) switch, and cross-rack communication is available only via these ToR switches. Surveys on real cloud computing systems show that cross-rack communication through the ToR switches is highly unstable due to the limited bandwidth, whereas intra-rack communication is faster and more reliable [14], [15]. A natural question is whether one

Group-master M
communication

SM

SM

SM

Completion time

μ1

of the worker

...

...

WWW

WWW

WWW

Group

Group

Group

Fig. 1. Illustration of the hierarchical computing system

can devise a coded computation scheme that exploits such hierarchical structure.

A. Contribution
In this work, we ﬁrst model a distributed computing system with a tree-like hierarchical structure illustrated in Fig. 1, which is inspired by the practical computing systems in [13]– [15]. The workers (denoted by “W”) are divided into groups, each of which has a submaster (denoted by “SM”). Each submaster sends the computational result of its group to the master (denoted by “M”). The suggested model can be viewed as a generalization of the existing non-hierarchical coded computation.
In this framework, we propose a hierarchical coding scheme which employs an (n1(i), k1(i)) MDS code within group i and another (n2, k2) outer MDS code across the groups as depicted in Fig. 2. We also develop a parallel decoding algorithm which exploits the concatenated code structure and allows low complexity.
Moreover, we analyze the latency performance of our proposed solution. It turns out that the latency performance of our scheme cannot be analyzed via simple order statistics as in other existing schemes. Here we resort to ﬁnd lower and upper bounds on the average latency performance: Our upper bound relies on concentration inequalities, and our lower bound is obtained via constructing and analyzing an auxiliary Markov chain (to be detailed later).

B. Related Work
Previous works on coded computation have rarely considered the inherent hierarchical structure of most real-world systems. Whereas a very recent work [16] deals with the multirack computing system reﬂecting imbalance between intraand cross-rack communications, it is based on the settings of the coded MapReduce architecture which do not include general linear computation tasks that we focus on in this work. Another distinction is that the analysis of [16] includes only

(n1(i), k1(i)) MDS coding within group i

(n2, k2) MDS coding
across groups

…

Group 1 w(1, 1) w(1, 2)

… w(1, n1(1))

Group 2

…
w(2, 1) w(2, 2) w(1, n1(2))

Group n2

…

w(n2, 1) w(n2, 2)

w(1, n1(n2))

Master Submaster

Fig. 2. Illustration of the proposed coding scheme applied to the hierarchical computing system. An (n1(i), k1(i)) MDS code is employed within group i, and an (n2, k2) MDS code is applied across the groups. w(i, j) denotes
worker j in group i.

the cross-rack redundancy whereas our analysis considers both intra- and cross-group coding.

C. Notations

We use boldface uppercase letters for matrices and boldface

lowercase letters for vectors. The transpose of a matrix A is

denoted

by

AT .

For

a

matrix

A

satisfying

AT

=

[AT1

A

T 2

]

,

we write A = [A1; A2]. For a positive integer n, the set {1, 2, . . . , n} is denoted by [n]. The jth worker in group i is represented by w(i, j) for i ∈ [n2] and j ∈ [n(1i)]. The symbol

r indicates the largest integer less than or equal to a real

number r.

II. HIERARCHICAL CODED COMPUTATION

A. Proposed Coding Scheme

Consider a matrix-vector multiplication task, i.e., computing

Ax for a matrix A ∈ Rm×d and a vector x ∈ Rd×1.

The input matrix A is split into k2 submatrices as A = [A1; A2; . . . ; Ak2 ], where Ai ∈ R km2 ×d for i ∈ [k2]. Here

we assume that m is divisible by k2 for simplicity. Then, we

apply an (n2, k2) MDS code to set {Ai}i∈[k2] in obtaining

{Ai}i∈[n2]. Then, each coded matrix Ai is further divided

into k1(i) submatrices as Ai = [Ai,1; Ai,2; . . . ; Ai,k(i) ] where

m ×d

1

A ∈ R (i)

i,j

k1 k2

for j ∈ [k1(i)] and m divisible by k1(i)k2.

Afterwards, for each i ∈ [n2], we apply an (n(1i), k1(i)) MDS

code to set {Ai,j }j∈[k(i)] to obtain {Ai,j }j∈[n(i)]. Then, for

1

1

each i ∈ [n2] and j ∈ [n(1i)], worker w(i, j) computes

Ai,jx. Fig. 2 illustrates the proposed coding scheme for

the hierarchical computing system with a different number of workers in each group. In the case of n(1i) = n1 and k1(i) = k1 for all i ∈ [n2], we will refer this coding scheme as

(n1, k1) × (n2, k2) coded computation.

We present our code in Fig. 3 via a toy example. In this

example, (n1, k1) = (n2, k2) = (3, 2). That is, the input

matrix A = [A1; A2] is encoded via (n2, k2) = (3, 2) MDS

code, yielding [A1; A2; A1 + A2]. Afterwards, the matrix

Ai = [Ai,1; Ai,2] is encoded via an (n1, k1) = (3, 2)

MDS code, producing [Ai,1; Ai,2; Ai,1 + Ai,2]. For notational

simplicity, we deﬁne A3 = A1 + A2 and Ai,3 = Ai,1 + Ai,2.

Group 1

𝑨𝑨 = 𝑨𝑨1; 𝑨𝑨2 𝑨�𝑨𝑖𝑖 = 𝑨�𝑨𝑖𝑖,1; 𝑨�𝑨𝑖𝑖,2 𝑨�𝑨1,1 x

(3,2) MDS (3,2) MDS

𝑨�𝑨1; 𝑨�𝑨2; 𝑨�𝑨3 ≔ 𝑨�𝑨1 + 𝑨�𝑨2 𝑨�𝑨𝑖𝑖,1; 𝑨�𝑨𝑖𝑖,2; 𝑨�𝑨𝑖𝑖,3 ≔ 𝑨�𝑨𝑖𝑖,1 + 𝑨�𝑨𝑖𝑖,2

𝑨�𝑨1,2 x

𝑨�𝑨1,3 x = (𝑨�𝑨1,1 +𝑨�𝑨1,2) x

Group 2

𝑨�𝑨2,1 x

𝑨�𝑨2,2 x

𝑨�𝑨2,3 x = (𝑨�𝑨2,1 +𝑨�𝑨2,2) x

Group 3 𝑨�𝑨3,1 x = (𝑨�𝑨1,1 +𝑨�𝑨2,1) x

𝑨�𝑨3,2 x = (𝑨�𝑨1,2 +𝑨�𝑨2,2) x 𝑨�𝑨3,3 x = (∑𝑖2𝑖=1 ∑𝑗2𝑗=1 𝑨�𝑨i,j) x

Fig. 3. Allocation of the computational task to workers in a (3, 2) × (3, 2) coded computation

For i, j ∈ {1, 2, 3}, worker w(i, j) computes Ai,jx. Note that group i is assigned a subtask with respect to Ai.
We now describe the decoding algorithm for our proposed coding scheme. When a worker completes its task, it sends the result to its submaster. With the aid of the (n1, k1) MDS code, submaster i (in group i) can compute Aix as soon as the task results from any k1 workers within group i are collected. Once Aix is computed, it is sent to the master. The master can obtain Ax by retrieving Aix from any k2 submasters. For each worker, we deﬁne completion time as the sum of the runtime of the worker and the time required for delivering its computation result to the submaster. For each group, we further deﬁne intragroup latency as the time for completing its assigned subtask. The total computation time is deﬁned as the time from when the workers start to run until the master completes computing Ax. The proposed computation framework can be applied to practical multi-rack systems where the input data A is coded and distributed into n2 racks; the ith rack contains Ai. For instance, in the Facebook’s warehouse cluster, data is encoded with a (14, 10) MDS code, and then the 14 encoded chunks are stored across different racks [17]. Once x is given from the master, the ith rack can compute Aix using the coded data Ai that it contains.
B. Application: Matrix-Matrix Multiplications
Our scheme can be also applied to matrix-matrix multiplications. More speciﬁcally, consider computing AT B for given matrices A and B = [b1 b2 · · · bk2 ]. After applying an (n2, k2) MDS code to B, we have Bˇ = [bˇ1 bˇ2 · · · bˇn2 ]. Moreover, group i divides A into k1(i) equal-sized submatrices as A = [Ai,1 Ai,2 · · · Ai,k(i) ], and we apply an (n(1i), k1(i)) MDS code, resulting in Aˇ 1i = [Aˇ i,1 Aˇ i,2 · · · Aˇ (i) ]. The
i,n1
computation Aˇ Ti,jbˇi is assigned to worker w(i, j). Using an (n(1i), k1(i)) MDS code, submaster i can compute AT bˇi when any k1(i) workers within its group delivered their computation results. The master can calculate AT B by gathering AT bˇi results from any k2 submasters, using the (n2, k2) MDS code. Under the homogeneous setting of n(1i) = n1 and k1(i) = k1 for all i ∈ [n2], the encoding algorithm of the proposed scheme reduces to that of the product coded scheme [3]. However, the suggested scheme with the homogeneous setting is shown to reduce the decoding cost compared to the product coded

Group 1 Group 2 Group 3

S1 S2 S3 T(2) T(4) T(6)

Time

Fig. 4. Illustration of obtaining L in a (3, 2) × (3, 2) coded computation

4, 2

5, 2

6, 2 v = k2

µ2

µ2

2µ2

7µ1

6µ1

5µ1

4µ1

2, 1

3, 1

4, 1

5, 1

6, 1

µ2

µ2

2µ2

2µ2

3µ2

9µ1

8µ1

7µ1

6µ1

5µ1

4µ1

0, 0

1, 0

2, 0

3, 0

4, 0

5, 0

6, 0

v=0

scheme under the hierarchical computing structure: a detailed analysis is in Sec. IV.

III. LATENCY ANALYSIS

We start by providing some preliminaries for the order

statistics. For n random variables, the kth order statistic is

deﬁned by the kth smallest one of n. From the known results

from the order statistics [18], the expected value of the kth

order statistics out of n (n > k) exponential random variables

with rate µ is (Hn − Hn−k)/µ, where Hk =

k1 l=1 l

log k + γ as k grows for a ﬁxed constant γ. This leads to

(Hn − Hn−k)/µ µ1 log n−nk . For n = k, the expected

latency is given by Hn/µ (log n)/µ. Further, deﬁne

H0 := 0 for ease of exposition. Consider the hierarchical computing system1 of Fig. 2.

Assume that for i ∈ [n2] and j ∈ [n1], the completion time

Ti,j of worker w(i, j) is exponentially distributed with rate µ1 (i.e., Pr[Ti,j ≤ t] = 1 − e−µ1t). Further, the communication time Ti(c) from the ith group to the master is also exponentially distributed with rate µ2 (i.e., Pr[Ti(c) ≤ t] = 1 − e−µ2t).
Here, we assume that all latencies are independent with one

another. Given the assumptions, the total computation time of

the (n1, k1) × (n2, k2) coded computation is written as

T = k2th min Ti(c) + Si

(1)

i∈[n2 ]

where

Si = k1th min Ti,j

(2)

j ∈[n1 ]

denotes the time to wait for the k1 fastest workers in group
i. The group index i is relabeled such that S1 ≤ S2 ≤ · · · ≤
Sn2 . In other words, the fastest group that ﬁnishes its assigned subtask is relabeled as the 1st group, while the slowest group is relabeled as the n2th group. Here we provide upper and lower
bounds on E[T ].

A. Lower Bound
Let T(m) be the mth smallest element of {Ti,j }i∈[n2],j∈[n1]. Then, T(1) ≤ T(2) ≤ · · · ≤ T(n1n2) holds. Using this notation, we derive a lower bound on E[T ], formally stated below.
Theorem 1: The expected total computation time of the (n1, k1) × (n2, k2) coded computation is lower bounded as

E[T ] ≥ E k2th min Ti(c) + T(ik1) := L .

(3)

i∈[n2 ]

1For simplicity of analysis, we only consider the homogeneous setting of n(1i) = n1 and k1(i) = k1 for all i ∈ [n2].

u=0

u = k1

u = k2k1

u = n2k1

Fig. 5. State transition diagram producing the lower bound L on the expected latency in a (3, 2) × (3, 2) coded computation. Each state is labeled with (u, v), where u is the number of completed workers and v is the number of groups that have sent their computation results.

Proof: Consider a realization of {Ti,j }i∈[n2],j∈[n1] and {Ti(c)}i∈[n2]. Recall that a group ﬁnishes its assigned subtask if k1 workers within the group complete their tasks. Hence, it is impossible for the ith group to ﬁnish its work if the total
number of completed workers in the system is less than ik1.
In other words, it must hold that

T(ik1) ≤ Si for all i ∈ [n2] .

(4)

Thus, the total computation time in (1) should be:

T = k2th min(Ti(c) + Si) ≥ k2th min(Ti(c) + T(ik1)) .

i∈[n2 ]

i∈[n2 ]

Averaging over all possible realizations, we complete the proof.
To further illustrate the proof, we provide a schematic example in Fig. 4. Consider a (3, 2) × (3, 2) coded computation. The yellow circles denote the completion times of the workers. After k1 = 2 workers in a group ﬁnish their computations, the group-master communication, shown as the red arrows, starts from each group. As can be seen, T(2) ≤ S1, T(4) ≤ S2 and T(6) ≤ S3, which concur with (4). The following lemma shows that L can be computed by analyzing the hitting time of an auxiliary Markov chain.
Lemma 1: Let C be the continuous-time Markov chain deﬁned over the state space (u, v) ∈ {0, 1, . . . , n2k1} × {0, 1, . . . , k2}. The state transition rates of C are deﬁned as follows:
• From state (u, v) to state (u + 1, v) at rate (n1n2 − i)µ1, if vk1 ≤ u < n2k1,
• From state (u, v) to state (u, v + 1) at rate ( u/k1 − v) µ2, if 0 ≤ v < min { u/k1 , k2}.
Then, the expected hitting time of C from state (0, 0) to the set of states {(u, k2)}nu=2kk12k1 is equal to L.
Proof: See Appendix A for the proof. Markov chain C deﬁned in Lemma 1 consists of the states (u, v), where u represents the number of completed workers and v indicates the number of groups which have delivered their computation results to the master. For an illustrative example, the state transition diagram for a (3, 2) × (3, 2) coded computation yielding a lower bound is shown in Fig. 5. The overall computation is terminated when

Expected total computation time Expected total computation time

4

Upper bound (Lemma 2)

Upper bound (Theorem 2)

3

Simulation

Lower bound

2

4

Upper bound (Lemma 2)

Upper bound (Theorem 2)

3

Simulation

Lower bound

2

1

1

0 1 2 3 4 5 6 7 8 9 10 k
2
(a) n1 = 10, k1 = 5, n2 = 10

0 1 2 3 4 5 6 7 8 9 10 k
2
(b) n1 = 600, k1 = 300, n2 = 10

Fig. 6. The expected total computation time of the (n1, k1)×(n2, k2) coded computing with its bounds for varying k2

the k2 = 2 groups ﬁnish conveying their computational results to the master, i.e., when the Markov chain visits the states with v = 2 for the ﬁrst time. We see that u increases by one when a worker completes its computation, and v increases by one when master receives the computation result from a group. The rightward transition (to increase u) rate is determined by the product of µ1 and the number of remaining workers. The upward transition (to increase v) rate is the product of µ2 and the number of groups that have not delivered their computation results to the master. The proposed lower bound L can be easily computed from the ﬁrst-step analysis [19] of the Markov chain produced by Lemma 1.
B. Upper Bound
We here provide two upper bounds on the expected total computation time. The ﬁrst bound in the following lemma is applicable for all values of n1 and k1.
Lemma 2: The expected total computation time of the (n1, k1) × (n2, k2) coded computation is upper bounded as E[T ] ≤ Hn1n2 /µ1 + (Hn2 − Hn2−k2 )/µ2 .
Proof: See Appendix B for the proof. We now establish another upper bound using the following two steps. First we ﬁnd an upper bound on the maximum intragroup latency among n2 groups. Afterwards, adding this value to the expected latency of the group-master communication yields an upper bound on the expected total computation time. For given n1 and k1, we use δ1 > 0 which satisﬁes n1 = (1 + δ1)k1. We now present the asymptotic upper bound as follows. Theorem 2: For a ﬁxed constant δ1 > 0, the expected total computation time of the (n1, k1) × (n2, k2) coded computing system is upper bounded as E[T ] ≤ [log(1 + δ1)/δ1]/µ1 + (Hn2 − Hn2−k2 )/µ2 + o(1) in the limit of k1.
Proof: See Appendix C for the proof.
C. Evaluation of Bounds
Fig. 6 shows the behavior of the expected total computation time and its upper/lower bounds with varying k2. Here we consider two upper bounds proposed in Lemma 2 and Theorem 2. To see the impact of k1, the values of k1 are ﬁxed to 5 and 300 in Figs. 6a and 6b, respectively. The other code parameters are set to n1 = (1 + δ1)k1, n2 = 10 for both ﬁgures, where δ1 is ﬁxed to 1. The rates of the completion

time of the worker and group-master communication are set to µ1 = 10 and µ2 = 1. For a relatively small values of k1, the upper bound in Lemma 2 is a tighter upper bound than the upper bound in Theorem 2. As can be seen in Fig. 6b, the asymptotic upper bound in Theorem 2 becomes tighter as k1 grows, which concurs with Theorem 2. We also have numerically conﬁrmed that the proposed lower bound is tight.
IV. DECODING COMPLEXITY
In this section, we compare decoding complexity of our hierarchical coding with the replication and non-hierarchical coding schemes including the (n1, k1)×(n2, k2) product code [3] and the (n, k) polynomial code [4]. For fair comparison, we set n = n1n2 and k = k1k2. We further assume that the decoding complexity of the (n, k) MDS code is O(kβ) for some β > 1.2 In our framework, the n2 intra-group codes can be decoded in parallel, followed by decoding of the crossgroup code using the k2 fastest results. Thus, the overall decoding procedure consists of 1) parallel decoding of (n1, k1) intra-group MDS codes and 2) decoding of the (n2, k2) crossgroup MDS code, resulting in the total decoding cost of O(k1β + k1k2β). Similarly, one can show that the decoding cost of polynomial codes is O(kβ), and that of product code is O(k1k2β + k2k1β). We note that the hierarchical code can have a substantial improvement, sometimes by an order of magnitude, in decoding complexity, compared to the product code. For instance, if β = 2 and k1 = k22, the decoding cost of hierarchical code becomes O(k24) while that of the product code is O(k25); if k1 = k21.5, the decoding costs are O(k23.5) and O(k24), respectively. In general, if k1 = k2p, one can show that the relative gain of the hierarchical codes in decoding cost monotonically increases as p increases, providing a guideline for efﬁcient code designs. Table I summarizes the computing times and decoding costs of various coding schemes.
We now compare the expected total execution time deﬁned as Texec := Tcomp + αTdec, where Tcomp is the computing time, Tdec is the decoding cost, and α ≥ 0 is the relative weight of the decoding cost. We note that α is a systemspeciﬁc parameter that depends on 1) the relative CPU speed of the master compared to the workers and 2) dimension of the input data. Shown in Fig. 7 are the expected total execution times for parameters of (n1, k1) = (800, 400), (n2, k2) = (40, 20), (µ1, µ2) = (10, 1) and β = 2.
We ﬁrst observe that with all tested practical values of µ1 and µ2, the hierarchical code strictly outperforms the product code for all values of α. Further, we observe that the optimal choice of coding scheme depends on the value of α as follows:
• (moderate α) when both Tcomp and Tdec have to be minimized, the hierarchical code achieves the lowest Texec by striking a balance between them;
• (low α) when Tdec is negligible, the polynomial code achieves the lowest Texec; and
• (high α) when Tdec dominates Texec, the replication code is the best.
2Note that this is the case for most practical decoding algorithms [20], [21]. Decoding with β = 1 requires a large ﬁeld size [4].

TABLE I COMPARISONS OF VARIOUS CODING SCHEMES

Coding scheme Replication
Hierarchical code
Product code [3]
Polynomial code [4]

Computing time (Tcomp)

kHk /(nµ2 )

µ12 log

√E[T ] √ n√/k+ 4 n/k
n/k−1

(Hn − Hn−k)/µ2

Decoding cost (Tdec) 0
O(k1k2β + k1β ) O(k1k2β + k2k1β )
O(k1β k2β )

𝑻𝑻𝟏(𝟏𝒄𝒄) 𝑻𝑻𝟐(𝟐𝒄𝒄)

The lower bound 𝓛𝓛 : the time when the 𝒌𝒌𝟐𝐭𝐭𝟐𝐭𝐭 fastest
red dashed arrow finishes

⋯

𝑻𝑻𝒏(𝒏𝒄𝟐𝒄𝟐)
0 𝑇𝑇(1) 𝑇𝑇(2) ⋯ 𝑇𝑇(𝑘𝑘1) ⋯ 𝑇𝑇(2𝑘𝑘1) ⋯ 𝑇𝑇(𝑛𝑛2𝑘𝑘1)

Time

Fig. 8. Illustration of the lower bound L

variables {T(lk1)}l=u/1k1 , only v random variables satisfy t ≥ Tc(l) + T(lk1). Therefore, the transition from state (u, v) to state (u, v + 1) occurs with rate ( u/k1 − v)µ2, for v ∈ {0, 1, · · · , min{ u/k1 , k2}−1}. Fig. 9 shows the consequent state transition diagram. This Markov chain is identical to C,
which completes the proof.

Fig. 7. E[Texec] of various coding schemes for parameters of (n1, k1) = (800, 400), (n2, k2) = (40, 20), (µ1, µ2) = (10, 1) and β = 2

Note that the shaded area in Fig. 7 represents the additional achievable (α, E[Texec]) region thanks to introducing the hierarchical code.

APPENDIX A
PROOF OF LEMMA 1
Note that the lower bound L in Theorem 1 can be illustrated
as in Fig. 8. The lower bound depends on two types of variables: {T(m)}m n2=k11, the set of n2k1 smallest realizations of n2n1 exponentially distributed random variables with rate µ1 and {Tc(l)}nl=21, the set of n2 exponentially distributed random variables with rate µ2. Consider arbitrary realizations of {T(m)} and {Tc(l)}. For a given time t, deﬁne

u := max {t ≥ T(m)},

(5)

m∈[n2 k1 ]

v := {l ∈ [n2] : t ≥ Tc(l) + T(lk1)} .

(6)

Thus, each time slot t can be assigned to a state (u, v) for
u ∈ {0, 1, · · · , n2k1} and v ∈ {0, 1, · · · , n2}. From the deﬁnition of L in (3), the lower bound corresponds to the
expected time t to achieve v = k2. Thus, we consider the state space of (u, v) ∈ {0, 1, · · · , n2k1} × {0, 1, · · · , k2}, and ﬁnd the expected time to arrive at states (u, v) with v = k2 from state (0, 0).
We now examine the state transition rates. From the def-
initions of T(m) and (5), the transition from state (u, v) to state (u + 1, v) occurs with rate (n1n2 − u)µ1, since there are n1n2 − u remaining {T(m)}nm1=nu2+1 such that t < T(m) holds. Moreover, for a given time t and the corresponding state (u, v), we have {T(lk1)}l=u/1k1 which satisﬁes t ≥ T(lk1), and v in (6) is expressed as

v = {l ∈ {1, 2, · · · , u/k1 } : t ≥ Tc(l) + T(lk1)} (7)

since Tc(l) is a random variable with nonnegative values. Thus, out of u/k1 activated (i.e., t ≥ T(lk1)) random

APPENDIX B PROOF OF LEMMA 2
Hn1n2 /µ1 is the maximum intra-group latency, which comes from waiting for all n1n2 workers. Assuming that every group starts the group-master communication at time Hn1n2 /µ1, the expected total computation time can be obtained by summing up the group-master communication time to Hn1n2 /µ1. The group-master communication time is calculated from the time that the k2th fastest group ﬁnishes communication to the master, which is given by (Hn2 − Hn2−k2 )/µ2. This completes the proof.

APPENDIX C PROOF OF THEOREM 2

A part of the proof generalizes the idea of [3], which

analyzes the latency of the product code. First, we focus on the

intra-group latency of each group. The expected latency of the

k1th fastest worker out of n1 is given by (Hn1 − Hn1−k1 )/µ1,

where the latency of a worker assumes an exponential distribu-

tion with rate µ1. Noticing that the expected completion time of a worker (Hn1 − Hn1−k1 )/µ1 is rewritten as µ11 log 1+δ1δ1 for a ﬁxed constant δ1 > 0 and a sufﬁciently large n1, we

deﬁne

t0 := 1 log 1 + δ1 + α

µ1

δ1

log k1 k1

for some constant α > 0.
Consider group i0 with n1 workers. Then, for worker w(i0, j), assume a Bernoulli random variable Xi0,j which takes 0 when worker w(i0, j) has completed its computation by time t0, and takes 1 otherwise. Then, probability p0 that Xi0,j takes 1 is:

p0 := Pr[Ti ,j > t0] = e−µ1t0 = δ1 e−µ1α

0

1 + δ1

log k1 k1

δ1 1 + δ1

1 − µ1α

log k1 k1

, (8)

(n2 − k2)µ2 (n2 − k2)µ2

v = k2 (n2 − k2 + 1)µ2

(n1n2 − 2k1)µ1

... ... ...

(n2 − 3)µ2 (n2 − 3)µ2 (n1n2 − 2k1 − 1)µ1
...

(n2 − 2)µ2 (n2k1 − 1)µ1 ...

v=2

µ2

µ2

(n2 − 2)µ2 (n2 − 2)µ2

(n1n2 − k1)µ1

(n1n2 − k1 − 1)µ1

(n1n2 − 2k1 + 1)µ1

...

(n1n2 − 2k1)µ1

(n1n2 − 2k1 − 1)µ1 ...

(n2 − 1)µ2 (n2k1 − 1)µ1 ...

v=1

µ2

µ2

2µ2

2µ2

(n2 − 1)µ2 (n2 − 1)µ2

n2 µ2

n1 n2 µ1

(n1n2 − 1)µ1

(n1n2 − k1 + 1)µ1

...

(n1n2 − k1)µ1

(n1n2 − k1 − 1)µ1

(n1n2 − 2k1 + 1)µ1

...

(n1n2 − 2k1)µ1

(n1n2 − 2k1 − 1)µ1 ...

(n2k1 − 1)µ1 ...

v=0

u=0

u=1

u = k1

u = k1 + 1

u = 2k1

u = 2k1 + 1

u = (n2 − 1)k1

u = (n2 − 1)k1 + 1

u = n2k1

u, v

Fig. 9. State transition diagram for the (n1, k1) × (n2, k2) coded computation producing a lower bound. Any state is denoted by (u, v), where u describes the number of completed workers and v is the number of groups that sent their computation results.

where (8) follows because µ1α lokg1k1 is quite small with a sufﬁciently large k1. Out of n1 workers in group i0, the set
of workers not completed by time t0 is represented as

St0 = {j ∈ [n1] : Ti0,j > t0} = {j ∈ [n1] : Xi0,j = 1}

with |St0 | representing the number of workers not completed by time t0, where |·| denotes the cardinality of a set.
Since Xi0,j is a Bernoulli random variable with parameter p0, the expected number of workers no√t completed in group i0 is calculated as n1p0 = δ1(k1 − µ1α k1 log k1) for a given n1 = (1 + δ1)k1. Recall that a group ﬁnishes its assigned subtask when k1 out of n1 workers in the group completed their works. At time t0, we thus denote a case where the number of stragglers in group i is greater than δ1k1 = n1 − k1 by an error event Ei for i ∈ [n2]. For group i0, we wish to ﬁnd an upper bound on the probability that Ei0 occurs, which is equivalent to the probability that group i0 has not ﬁnished its assigned subtask by time t0. We establish such a bound using Hoeffding’s inequality [22] to bound the deviation of
|St0 | from the mean:

− 2t2
Pr[|St0 | − δ1(k1 − µ1α k1 log k1) ≥ t] ≤ e (1+δ1)k1 . √
By setting t = δ1µ1α k1 log k1, we obtain

− 2δ12µ21α2 log k

− 2δ12µ21α2
1+δ

Pr [|St0 | ≥ δ1k1] ≤ e 1+δ1

1 = k1

1.

Combining all n2 groups, the upper bound on the probability that n2 groups not ﬁnished their assigned subtasks by time t0 is obtained by the union bound. Let TI be the time when all n2 groups ﬁnish their assigned subtasks. Hence, we have

Pr[TI > t0] = Pr[E1 ∪ E2 ∪ · · · ∪ En2 ]

(9)

n2

− 2δ12µ21α2

≤ Pr[Ei] = n2k1 1+δ1 = o(k1−1) ,

(10)

i=1

where the last equality holds since α can be made arbitrarily large. Then the expected intra-group latency E[TI ] satisﬁes

E[TI ] ≤ Pr[TI ≤ t0]t0 + Pr[TI > t0] Hn1n2 + t0 (11) µ1

= 1 − o(k−1) t0 + o(k−1) Hn1n2 + t0

(12)

1

1

µ1

= 1 log 1 + δ1 + o(1) , (13)

µ1

δ1

where (11) is due to the fact that t0 is the worst case latency for all events satisfying TI ≤ t0, and Hn1n2 /µ1 + t0 is an upper bound on E[TI |TI > t0].
From (13), we conclude that all the n2 groups embark on the group-master communication before time µ11 log 1+δ1δ1 , as k1 grows large. Hence, adding the latency of the group-master
communication (Hn2 − Hn2−k2 )/µ2 to (13) gives an upper bound on the expected total computation time. This completes
the proof of the case where n2 > k2. When n2 = k2, the group-master communication time is represented by Hn2 /µ2. Thus, adding this value to (13) completes the proof, using
H0 = 0.

REFERENCES
[1] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le et al., “Large scale distributed deep networks,” in Proc. NIPS, 2012, pp. 1223–1231.
[2] K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, “Speeding up distributed machine learning using codes,” IEEE Trans. Inf. Theory, vol. PP, no. 99, pp. 1–1, 2017.
[3] K. Lee, C. Suh, and K. Ramchandran, “High-dimensional coded matrix multiplication,” in Proc. IEEE ISIT, June 2017, pp. 2418–2422.
[4] Q. Yu, M. Maddah-Ali, and S. Avestimehr, “Polynomial codes: An optimal design for high-dimensional coded matrix multiplication,” in Proc. NIPS, 2017, pp. 4406–4416.
[5] R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, “Gradient coding: Avoiding stragglers in distributed learning,” in Proc. ICML, 2017, pp. 3368–3376.
[6] W. Halbawi, N. Azizan-Ruhi, F. Salehi, and B. Hassibi, “Improving distributed gradient descent using Reed-Solomon codes,” arXiv:1706.05436, 2017.

[7] N. Raviv, I. Tamo, R. Tandon, and A. G. Dimakis, “Gradient coding from cyclic MDS codes and expander graphs,” arXiv:1707.03858, 2017.
[8] Z. Charles, D. Papailiopoulos, and J. Ellenberg, “Approximate gradient coding via sparse random graphs,” arXiv:1711.06771, 2017.
[9] S. Dutta, V. Cadambe, and P. Grover, “Coded convolution for parallel and distributed computing within a deadline,” in Proc. IEEE ISIT, June 2017, pp. 2403–2407.
[10] Q. Yu, M. A. Maddah-Ali, and A. S. Avestimehr, “Coded Fourier transform,” Proc. Allerton Conf., 2017.
[11] S. Dutta, V. Cadambe, and P. Grover, “Short-Dot: Computing large linear transforms distributedly using coded short dot products,” in Proc. NIPS, 2016, pp. 2100–2108.
[12] G. Suh, K. Lee, and C. Suh, “Matrix sparsiﬁcation for coded matrix multiplication,” in Proc. Allerton Conf., 2017.
[13] J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed data processing on large clusters,” Commun. ACM, vol. 51, no. 1, pp. 107–113, 2008.
[14] F. Ahmad, S. T. Chakradhar, A. Raghunathan, and T. Vijaykumar, “ShufﬂeWatcher: Shufﬂe-aware scheduling in multi-tenant MapReduce clusters.” in Proc. USENIX ATC, 2014, pp. 1–12.
[15] A. Vahdat, M. Al-Fares, N. Farrington, R. N. Mysore, G. Porter, and S. Radhakrishnan, “Scale-out networking in the data center,” IEEE Micro, vol. 30, no. 4, pp. 29–41, 2010.
[16] S. Gupta and V. Lalitha, “Locality-aware hybrid coded MapReduce for server-rack architecture,” arXiv:1709.01440, 2017.
[17] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, “A solution to the network challenges of data recovery in erasure-coded distributed storage systems: A study on the Facebook warehouse cluster.” in Proc. USENIX HotStorage, 2013.
[18] H. A. David and H. N. Nagaraja, Order Statistics. Wiley, New York, 2003.
[19] P. Brémaud, Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues. Springer Science & Business Media, 2013, vol. 31.
[20] W. Halbawi, Z. Liu, and B. Hassibi, “Balanced Reed-Solomon codes,” in Proc. IEEE ISIT, July 2016, pp. 935–939.
[21] ——, “Balanced Reed-Solomon codes for all parameters,” in Proc. IEEE ITW, Sept. 2016, pp. 409–413.
[22] W. Hoeffding, “Probability inequalities for sums of bounded random variables,” J. Am. Stat. Assoc., vol. 58, no. 301, pp. 13–30, 1963.

