arXiv:1905.11361v1 [cs.LG] 27 May 2019

Eﬃcient candidate screening under multiple tests and implications for fairness
Lee Cohen∗ Zachary C. Lipton† Yishay Mansour‡
May 28, 2019
Abstract When recruiting job candidates, employers rarely observe their underlying skill level directly. Instead, they must administer a series of interviews and/or collate other noisy signals in order to estimate the worker’s skill. Traditional economics papers address screening models where employers access worker skill via a single noisy signal. In this paper, we extend this theoretical analysis to a multi-test setting, considering both Bernoulli and Gaussian models. We analyze the optimal employer policy both when the employer sets a ﬁxed number of tests per candidate and when the employer can set a dynamic policy, assigning further tests adaptively based on results from the previous tests. To start, we characterize the optimal policy when employees constitute a single group, demonstrating some interesting trade-oﬀs. Subsequently, we address the multi-group setting, demonstrating that when the noise levels vary across groups, a fundamental impossibility emerges whereby we cannot administer the same number of tests, subject candidates to the same decision rule, and yet realize the same outcomes in both groups.
1 Introduction
Consider an employer seeking to hire new employees. Clearly, the employer would like to hire the best employees for the task, but how will she know which are best ﬁt? Typically, the employee will gather information on each candidate, including their education, work history, reference letters, and for many jobs, they will actively conduct interviews. Altogether, this information can be viewed as the signal available to the employer.
Suppose that candidates can be either skilled or unskilled. If the ﬁrm hires an “unskilled” candidate, it will incur a signiﬁcant cost on account of lost productivity. For this reason, the employer would like to minimize the number of False Positive mistakes, instances where unskilled candidates are hired. On the other hand, the employer desires not to overspend on the hiring process, limiting the number of interviews per hired candidate (either on average, or absolutely). However, fewer
∗Tel Aviv University. This work was supported in part by The Yandex Initiative for Machine Learning. Email: leecohencs@gmail.com.
†Carnegie Mellon University and Amazon AI. This work was supported by the AI Ethics and Governance Fund. Email: zlipton@cmu.edu.
‡Tel Aviv University and Google Research. This work was supported in part by a grant from ISF. Email: mansour.yishay@gmail.com.
1

interviews weakens the signal, causing the employer to make more mistakes. At the heart of our model is this inherent trade-oﬀ between the quality of the signal and the cost of obtaining the signal. This marks a departure from the classical economics literature, in which the signal is commonly regarded as a given.
Complicating matters, hiring eﬃciency is not the only desiderata at play. In society, employees belong to various demographic groups, and we may strive to design policies that are in some sense fair vis-a-vis group membership. While fairness can be an elusive notion, regulators must translate it to concrete rules and laws. In the United States, a body of anti-discrimination law dating to the Civil Rights act of 1964, subjects decisions that result in disparate outcomes (as delineated by race, age, gender, religion, etc) to extra scrutiny: employers must not only show that preference for some groups over others did not drive the decision (disparate treatment doctrine) but also justify that any observed disparities arise from a business necessity (disparate impact doctrine), whether or not those disparities were intentional.
In this paper, we seek to understand how a complex hiring process would interact with the requirements of fairness. We extend the theory on candidate screening and statistical discrimination, addressing the setting in which employers can subject employees to multiple tests, which we assume to be conditionally independent given the worker’s skill level and group membership. To build intuition, most of our analysis focuses on a Bernoulli model of both worker skill and screening. Additionally, we begin to extend the traditionally-studied Gaussian skill and screening models to the multi-test setting (Section 5).
Unlike the classical papers, in which an employer’s hiring policy is given by a simple thresholding rule, our setting requires greater care to derive the optimal employer policy. In our setting, we imagine that the employer wishes to minimize the number of tests performed subject to a constraint upper-bounding the false positive rate. We characterize the optimal policy in this case as a randomized threshold policy.
We also consider the setting in which employers can allocate tests dynamically, deciding after each result whether to (i) hire the candidate; (ii) reject the candidate and move on to the next one; or (iii) administer a subsequent test. In the Bernoulli case, the optimal policy consists of administering tests until each candidate’s posterior likelihood of being a high-skilled worker either dips below the prior or rises above a threshold determined by the tolerable false positive rate. We demonstrate that the analysis of this process can be reduced to a random walk over the log posterior odds and derive the solution via the corresponding Gambler’s ruin problem.
Finally, we consider the ramiﬁcations for fairness within our model when employees, despite possessing similarly-distributed skills, are evaluated with diﬀering noise levels.
1.1 Related work
The classical economics literature on discrimination in employment can broadly be divided into two focuses. The taste-based discrimination model due to [3] models the market outcomes in a setting where employers express an explicit preference for hiring members of one group, acting as if an employee’s demographic membership provides utility. This preference for certain groups induces a sorting of employees from the disadvantaged group towards those employers who discriminate the least with wages ultimately determined by the marginal discriminator. Subsequently, [16] suggested
2

a statistical mechanism by which similarly-skilled employees from diﬀerent groups might experience diﬀerential outcomes: the comparative diﬃculty of screening from one group vs. another. Many subsequent works extend this analysis, typically focusing on Gaussian models of worker quality and conditionally-Gaussian test scores [2, 1]. These papers consider the setting where workers are assessed via a single test characterized by a group-dependent noise level. Our work is diﬀerentiated from these by considering richer mechanisms for acquiring signal.
In the more recent literature on fairness in machine learning, researchers often focus on binary classiﬁcation, with employees characterized by a protected characteristic (group membership), and other (non-protected) covariates [15, 11, 12]. There, the predictor is presumably used to guide a consequential decision, such as allocating some economic good (loans, jobs, etc.) [6] or assessing some penalty (e.g. risk scores to guide bail decisions) [5]. Papers then focus on various interventions for ensuring accurate prediction subject to various constraints such as demographic parity (outcomes independent of group membership), blindness (model cannot observe group membership), and equalized false negative and/or false positive rates [9]. Several simple impossibility results preclude simultaneously satisfying several combinations of these parities [4, 5, 13]. More recently, a number of papers have drawn inspiration from economic modeling, extending the literature on fairness in classiﬁcation to consider longer-term dynamics, equilibria, and the emergence of feedback loops [10, 9, 7]. Finally, [17] provide a survey of deﬁnitions from the algorithmic fairness literature.

2 The Bernoulli Model

We formalize our problem as follows. An employer accesses an inﬁnite pool of candidates (indexed

by i ∈ N+), each of which has some (hidden) skill level yi ∈ {0, +1}, which denote unskilled and

skilled, respectively. Underlying worker skill levels yi are sampled independently from a Bernoulli

distribution with parameter p. An employer can access information about the i-th candidate through

a sequence of τ tests, which are conditionally independent given yi. Each test result, yˆi,j ∈ {0, +1}

disagrees with the ground truth skill with probability Pr[yˆi,j = yi] = 1−2σ , where σ ∈ (0, 1), i.e.,

yˆi,j

=

yi

⊕

Br(

1−σ 2

)1.

For

convenience,

we

denote

the

noise

level

as

η

=

1−σ 2

∈

(0,

21 ).

We

say

that

a test result yˆi,j is ﬂipped if yˆi,j = yi, and the number of ﬂipped results for a given candidate is

denoted by Zτη is Zτη =

τ j=1

I(yˆi,j

=

yi),

where

I(·)

is

the

indicator

function.

A selection criterion is a mapping between test results to actions: Select(yˆi,1, . . . , yˆi,τi) ∈ {0, 1},

where 0 means reject and 1 means accept (hire). A policy π sets the selection criteria based

on σ, p and other possible constraints such as probability to hire, error probability, etc. A ran-

domized threshold policy is a policy π with parameters (τ, θ, r) such that π(yˆi,1, . . . , yˆi,τi) = 1

for Sτ :=

τ i=1

yˆi,j

>

θ,

π(yˆi,1, . . . , yˆi,τi )

=

0

for

Sτ

<

θ,

and

for

Sτ

=

θ

the

probability

that

π(yˆi,1, . . . , yˆi,τi) = 1 is r. We call a policy π a threshold policy if r = 1. In a dynamic policy, rather

than setting a ﬁxed number of tests per candidate, the employer may decide after each test whether

to accept, reject, or to perform an additional test, i.e., π(yˆi,1, . . . , yˆi,τi) ∈ {0, 1, more}. Note that

for a dynamic policy, the number of tests τ is a random variable determined based on the tests’

outcomes. When designing a policy, one must carefully consider the balance between the following

desiderata:

1⊕ is the XOR operation between two binary random variables, and therefore yˆi,j is also a random variable.

3

1. Minimize False Discovery Rate (FDR)—the fraction of unskilled workers among the accepted candidates, i.e., FDR := Pr[yi = 0|π(yˆi,1, . . . , yˆi,τ ) = +1].
2. Minimize False Omission Rate (FOR)—the fraction of skilled workers among the rejected candidates, i.e., FOR := Pr[yi = +1|π(yˆi,1, . . . , yˆi,τ ) = 0].
3. Minimize False Negatives (FN)—the amount of skilled workers that are classiﬁed as unskilled.

4. Minimize False Positives (FP)—the amount of unskilled workers that are classiﬁed as skilled.

5. Ratio of accept probability and the number of tests—the number of tests performed

per

candidate

hired,

using

a

parameter

B

>

1,

we

have

τ B

≤

Pr[π(yˆi,1, . . . , yˆi,τ )

=

+1].

For any ﬁxed number of tests τ , increasing the threshold θ of a threshold policy decreases FDR while increasing FOR.

Loss: To handle the trade-oﬀ between the false positives, (i.e., when an unskilled candidate is accepted) and false negatives (i.e., when a skilled candidate is rejected), we introduce an α-loss, paramaterized by α ∈ [0, 1] and deﬁned as follows:

ℓα(b1, b2) = αI[b1 = 0, b2 = 1] + (1 − α)I[b1 = 1, b2 = 0]

where I[·] is the indicator function and b1, b2 ∈ {0, 1}. The expected loss of a policy π is,

lα(π) = E[ℓα(yi, π(yˆi,1, . . . , yˆi,τ ))]

(1)

where the expectation is over the type of the candidates yi, the test results yˆi,j, and the decisions of π.

3 Analysis of the Bernoulli Model with One Group
To begin, we analyze this hiring model for a single group of candidates. The employer’s goal is to minimize the expected loss, lα(π), while maintaining a given acceptance probability. For brevity, we relegate all proofs to the appendix.

3.1 The Simple Threshold Policy (Equal Number of Tests)

Consider the setting where the employer must subject all candidates to an equal number of tests τ and threshold θ (these parameters are chosen by the employer but thereafter constant across candidates). For a given threshold, we can relate the ﬂip probability (error rate) of the test to the probability that a candidate is accepted as follows:

Recall that yˆi,j = yi ⊕ Br(η), Sτ =

τ j=1

yˆi,j

,

that

Zτη

=

τ t=1

I(yˆi,j

=

yi),

and

that

τ

and

θ

are

the only parameters of the threshold policy, π. Informally, Sτ is the number of passed tests and

Zτη is the number of ﬂips (tests in error). The probability of hiring an unskilled candidate is given

by:

Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|yi = 0] = Pr[Sτ ≥ θ|yi = 0] = Pr[Zτη ≥ θ].

4

Since Zτη is a binomial random variable with parameters τ and η, we can calculate this probability

precisely as: Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|yi = 0] = Pr[Zτη ≥ θ] =

τ k=θ

τ k

ηk(1 − η)τ −k,

and

the

probability of rejecting a skilled candidate is the probability that they encounter more than τ − θ

ﬂips, thus: Pr[π(yˆi,1, . . . , yˆi,τ ) = 0|yi = +1] = Pr[Sτ < θ|y = +1] = Pr[Zτη > τ − θ] =

τ k=τ −θ+1

τ k

ηk(1−η)τ−k. Similarly, given a candidate’s skill level, we can calculate the probability

that they obtain exactly k positive tests out of τ , i.e,

Pr[Sτ = k|yi = 0] = Pr[Zτη = k] = τk ηk(1 − η)τ−k.

Pr[Sτ = k|yi = +1] = Pr[Zτη = τ − k] = τk ητ−k(1 − η)k. Given these observations, we can now analyze the employer’s choices.

Optimal solution for any ratio α ∈ (0, 1)

The next theorem shows that for threshold policies, the expected loss lα(π) = lα(θ) is minimized

at θp∗,α such that |θp∗,α − τ /2| ≤ lo2glo(gp1()1++lo1g2−σ(σα1)) .

Theorem 1. The loss function lα(θ) is quasi-convex and a threshold of θp∗,α =

1 2

τ − log( p1lo−g(11)++lo2gσ( α1) −1)

1−σ

minimizes loss for any values of α, p, σ ∈ (0, 1). Namely,

θ∗ = arg min l (θ) = τ − log( p1 − 1) + log( α1 − 1) . p,α θ α 2 2 log(1 + 12−σσ )

Next, we bound the number of tests required to guarantee that the probability of classiﬁcation error

by the majority decision rule (i.e., θ = ⌈ τ2 ⌉) does not exceed a speciﬁed quantity δ.

Theorem

2.

For

every

δ, p, α

∈

(0, 1),

performing

τ

=

Ω

(

α+

p− σ2

2pα

ln( δ1 ))

tests

per

candidate

and

using majority as a decision rule (i.e., θ = τ /2) guarantees lα(π) ≤ δ.

Equal cost for false positives and false negatives (α = 12 )

Consider the simple loss consisting of the classiﬁcation error rate (false positives and false negatives

count equally), expressed via our loss function by setting α = 12 . When skilled and unskilled candidates occur with equal frequency, i.e., p = 1/2, we can derive that the majority decision rule

minimizes the classiﬁcation error for any number of tests.

Corollary 3. Assume p = 1/2 and α = 1/2. For any number of tests τ , the majority decision rule

minimizes loss lα. Namely, arg minθ l 21 (θ) = ⌈ 21 τ ⌉. In addition, for every δ ∈ (0, 1), performing

τ

=

Ω

(

1 σ2

ln( δ1 ))

tests

per

candidate

and

using

majority

as

a

decision

rule

guarantees

classiﬁcation

error with probability of at most δ.

5

FDR minimization with limited number of tests per hire for balanced groups Again, assuming balanced groups (i.e., p = 1/2), suppose that an employer would like to minimize the false discovery rate, subject to the constraint of lower bounding the hiring probability. We can model this optimization problem by introducing a budget parameter B > 1 to bound any predetermined (ﬁxed) number of tests per hired candidate as follows:
arg min FDRπ = Pr[yi = 0| Pr[π(yˆi,1, . . . , yˆi,τ ) = 1]
π
subject to τπ ≤ B (2) Pr[π(yˆi,1, . . . , yˆi,τ ) = 1]
where τπ is the number of tests π performs. The following theorem shows that the optimal policy is a randomized threshold policy. Theorem 4. There exists a randomized threshold policy π which is an optimal solution for (2).

3.2 The Dynamic Policy (Adaptively-Allocated Tests)
Recall that under a dynamic policy, the employer can decide after each test whether to accept, reject, or perform another test. In general, dynamic policies are more eﬃcient than those that must set a ﬁxed number of tests. To build intuition, consider a candidate that has passed 2 out of 3 tests. As seen above, under an optimally-constructed ﬁxed-test policy, any candidate that fails a single test might be rejected. 2 However, the posterior probability that this candidate is in fact skilled may still be greater than that of a fresh candidate sampled from the pool. Thus we can improve on the ﬁxed-test policy by dynamically allocating more tests to candidates until their posterior odds either dip below the prior odds or rise above the threshold for hiring. The following theorem formalizes this notion that it is better to administer more tests to a candidate that passed the majority of previous tests than to start afresh with a new candidate: Theorem 5. For any p, σ, τ , a candidate i that passed θ > τ2 out of τ tests is more likely to be a skilled than a freshly-sampled candidate i′ for whom no test results are yet available, i.e., Pr[yi′ = +1] = p < Pr[yi = +1|Sτ = θ]. Remark 6. If θ < τ2 , the inequality would have been reversed.

The Greedy Policy We now present a greedy algorithm that continues to test a candidate so long as the posterior probability that yi = +1 is greater than ǫ′ and smaller than 1 − ǫ, rejects a candidate whenever the posterior falls below ǫ′ (absent fairness concerns, employers will set ǫ′ = p for all groups), and accepts whenever the posterior rises above 1 − ǫ. Given parameters ǫ, ǫ′ > 0,
we show that the greedy policy solves the optimization problem of minimizing the mean number of
tests under these constraints, i.e.,

minimize
τ
subject to

E[τ ]
∀iπ(yˆi,1, . . . , yˆi,τ ) = 1 iﬀ Pr[yi = +1|yˆi,1, . . . , yˆi,τ ] ≥ 1 − ǫ ∀iπ(yˆi,1, . . . , yˆi,τ ) = 0 iﬀ Pr[yi = +1|yˆi,1, . . . , yˆi,τ ] < ǫ′

Our analysis of this policy builds upon the observation that conditioned on a worker’s skill, the posterior log-odds after each test perform a one-dimensional random walk, starting with the prior

2For example, if B = 18 and η = 31 , the lowest false discovery rate is achieved by τ = θ = 3.

6

log-odds log( 1−p p ) and moving, after each test result, either left (upon a failed test) or right (upon a passed test). When (as in our model) the probability of a ﬂip are equal for skilled and unskilled candidates, our random walk has a ﬁxed step size. Moreover, our random walk has absorbing barriers corresponding to (when ǫ′ = p) falling below the prior log odds (on the left) and exceeding the hiring threshold (on the right). Owing to the ﬁxed step size and absorbing barriers, our policy
resembles the classic problem of Gambler’s ruin, in which a gambler wins or loses a unit of currency
at each step, and loses when crossing a threshold on the left (going bankrupt) or on the right (bankrupting the opponent). We formalize the random walk as follows where Xj is the position on the walk at time j:

1.

X0

is

the

prior

log-odds

of

the

candidate,

i.e.,

X0

=

log

p 1−p

.

2. After each test result, yˆi,j is observed, Xj = Xj−1 + (2yˆi,j − 1) · log PPr[ryˆ[yˆi,ij,j==++11|y|yii==+01] ] .

Let πGreedy be the policy that accepts a candidate if Pr[yi = +1|yˆi,1, . . . , yˆi,j] ≥ 1 − ǫ, rejects if Pr[yi = +1|yˆi,1, . . . , yˆi,j] < ǫ′, and otherwise conducts an additional test, i.e.,

 0  πGreedy(yˆi,1, . . . , yˆi,j ) = 1
retest

if Pr[yi = +1|yˆi,1, . . . , yˆi,j] < ǫ′ if Pr[yi = +1|yˆi,1, . . . , yˆi,j] ≥ 1 − ǫ . else

An employer will generally set the lower absorbing barrier to reject all candidates with posterior

log odds less than p since a fresh candidate from the pool is expected to be better. However, when

noise levels diﬀer across groups, we may prefer in the interest of fairness to set ǫ′ lower than p for

members of the noisier group, allowing us to equalize the frequency of false negatives across groups

(see Section 4).

Lemma

7.

Let

β, β′

∈

R

be

the

parameters

that

satisfy

β β+1

=

1−ǫ

and

β′ β′+1

=

ǫ′

(i.e.,

β

=

1−ǫ ǫ and

β′

=

ǫ′ 1−ǫ′

).

Then

Xτ

≥

log β

iﬀ

Pr[yi

=

+1|yˆi,1, . . . , yˆi,τ ]

≥

1−ǫ

(iﬀ

the

candidate

is

accepted)

and

Xτ < log β′ iﬀ Pr[yi = +1|yˆi,1, . . . , yˆi,τ ] < ǫ′ (iﬀ the candidate is rejected).

Corollary 8. The policy πGreedy can be described as follows.

 0  πGreedy(yˆi,1, . . . , yˆi,τ ) = 1
retest

if

Xτ

<

log

ǫ′ 1−ǫ′

if Xτ ≥ log( 1−ǫ ǫ )

else

We use the following parameters in the next theorems:

 log( (1−ǫǫ)ǫ(1′(−1−ǫ′σ)()1+σ) )  1

a= 


log( 11+−σσ )

 ≫ σ and 

 log( p(1−ǫ′)(1+σ) ) 

z=

ǫ′(1−p)(1−σ)




lo

g

(

1+σ 1−σ

)







Theorem 9 (Expected number of tests per type). The expected number of tests until a decision

(namely accept or reject) for skilled candidates is E[τs] = 1

a·

1−(

1−σ 1+σ

)z

−z

σ

1−(

1−σ 1+σ

)a

≈

2a 1+σ

−

z σ

and

E[τu]

=

1 σ

z − a · 1−(

1+σ 1−σ

)z

1−(

1+σ 1−σ

)a

≈

z σ

for

unskilled

candidates.

7

Table 1: Confusion matrix for πgreedy assuming ǫ ≤ 1/4 and ǫ′ ≤ p ≤ 1/2.

accept reject

General ǫ′

Skilled (yi = +1)

Unskilled (yi = 0)

TPR = Θ FNR = Θ

1 − ǫp′ (1 − σ) ǫp′ (1 − σ)

FPR = Θ (ǫ(p − ǫ′ + ǫ′σ)) TNR = Θ (1 − ǫ(p − ǫ′ + ǫ′σ))

When ǫ′ = p Skilled Unskilled

Θ(σ)

Θ(ǫpσ)

Θ(1 − σ) Θ(1 − ǫpσ)

For the probabilities of the candidates to be accepted or rejected, conditioned on their true skill

level, we present the results in a form of confusion matrix in Table 1.

Theorem 10. The expected number of tests until deciding whether to accept or reject a candidate

is

E[τ |π(yi,τ )

∈

{0, 1}]

≈

aσp ,

where

a

≫

1 σ

.

4 Fairness Considerations in the Two-Group Setting

Two Groups—Threshold Policies We now discuss the eﬀects of a threshold policy when candidates belong to two groups, G1 and G2 whose skill level is distributed identically, but whose tests are characterized by diﬀerent noise levels. Without loss of generality, we assume that η1 < η2, where ηi is the probability that a test result of a candidate from Gi is diﬀerent from his skill level. To begin, we note the fundamental irreconcilability of equalizing either the false positive or the false negative rates across groups with subjecting candidates to the same policy. Theorem 11 (Impossibility result). When noise levels diﬀer between two groups with identical skill level distribution, a single Threshold Policy π (with the same number of tests τ and the same threshold θ for both groups) cannot have equality in either the false negative rates or in the false positive rates across the groups. Particularly, there is a higher false positive rate in the noisier group, as an unskilled candidate from G2 is more likely to be accepted by the threshold policy than an unskilled candidate from G1:

FPRηθ,1τ

=

Pr[π(yˆi,1, . . . , yˆi,τ )

=

1|yi

=

0]

<

Pr[π(yˆi,1, . . . , yˆi,τ )

=

1|yi

=

0]

=

FPR

η2 θ,τ

,

η1

η2

and also a higher false negative rate, as a skilled candidate from G2 is more likely to be rejected than a skilled candidate from G1:

FNRηθ,1τ = Pr[π(yˆi,1, . . . , yˆi,τ ) = 0|yi = +1] < Pr[π(yˆi,1, . . . , yˆi,τ ) = 0|yi = +1] = FNRηθ2,τ .

η1

η2

Connection to Economics Literature Aigner and Cain [1] discuss a similar case under a Gaussian screening model where the variance (noise level) of the single test diﬀers across the two groups. Similarly, they note that qualiﬁed candidates fare worse in the noisy group but that unqualiﬁed candidates fare better in the noisier group. Our work diﬀers from theirs in that we consider the eﬀect of multiple tests and the ability to optimize over the number of tests.

Two Groups–Dynamic policy We now consider the (dynamic) hiring policy in the setting when employees belong to two groups, G1 and G2 with identically-distributed skills but diﬀerent
8

noise levels η1 < η2. We note that there are two ingredients that explain the diﬀerences among the groups: (i) The step size, log PPr[ryˆ[yˆi,ij,j==++11|y|yi=i=+01]] = log 1−η η of G2 (the noisier group) is smaller than the step size of G1. Thus these candidates must typically pass more tests before they are accepted; and (ii) Skilled candidates in group G2 exhibit less drift to the right (they have a higher probability of failing a test). Consequently, when an employer (rationally) sets ǫ′ = p for all groups, a skilled candidate from G2 is more likely to be fail a test in step 1, at which point the dynamic policy summarily rejects them. These two facts explain both the higher false negative rates for G2 and the longer expected duration until acceptance. By setting ǫ′ < p for members of the noisier group, we can equalize false negative rates. Precisely, setting ǫ′ = ηη21 p achieves the desired parity. The cost of this intervention is that it requires more tests for candidates from the noisier group. Here, our random walk analysis can be leveraged to determine exactly how many more. Once again, we cannot provide equality across the groups in all desired ways—the same acceptance criterion, the same expected number of tests, and the same false negative rates between groups—with the noise diﬀers across groups.
5 Gaussian Worker Screening Model
In this section, we work out the analytic solutions for the conditional expectation of worker qualities given a series of conditionally independent tests Y1, ...Yn s.t. ∀i, j, Yi ⊥ Yj|Q. We assume that the worker quality Q normally distributed with mean µQ and variance σQ2 , so instead of binary skill level we have continuous quality of candidates. Conditioned on Q = q, each test is generated according to the structural equation yi = q + η, where η is a normally distributed noise term with mean 0 and variance ση2. Equivalently, we can say that the conditional distribution for each test P (Y |Q = q) is Gaussian with mean q and variance ση2. We refer the reader to Appendix B for further details.
We show that we can equalize conditional variance between the two groups by giving more interviews to to noisier group, and that it yields the same conditional expectations. Theorem 12. For two groups, G1, G2 with the same worker quality Q, that diﬀer only in the variance of their noise ση21 < ση22 , the variance can be equalized by using n2 = σσηη2212 n1 interviews (or tests) for G2, where n1 is the number of interviews for each candidate from G1. Theorem 13. When equalizing conditional variances between G1, G2 by using n2 = ση22 n1, we get
ση21
the same conditional expectations, η1 [Q|Y1, ..., Yn1 ] = η2[Q|Y1, ..., Yn2].
6 Unsupervised Parameter Estimation
Now, under the assumption of realizable case, we explain how one can estimate the parameters p and σ given tests results from a homogeneous population. Surprisingly, we discover that parameter recovery in this model does not require any ground truth labels indicating whether an employee is skilled or unskilled. We use Hoeﬀding’s inequality to bound the absolute diﬀerence between the estimated parameters and the true parameters by choosing δ as the wanted upper bound and solving for the number of samples or ǫ.
9

Lemma 14 (Hoeﬀding’s for any ǫ > 0,

inequality). Let y1, . . . , ym be 1m
Pr m yi − E[yi] ≥ ǫ
i=1

σ2−sub–gaussian ≤ 2e−mǫ2/2σ2 .

random

variables.

Then,

If y1, . . . , ym are Bernoulli random variables with parameter p,

1m

−2mǫ2

Pr m yi − p ≥ ǫ ≤ 2e

.

i=1

We start by estimating σ and then use it to derive an estimate for p. The estimated parameters are denoted by σˆ and pˆ. Notice that in order to have any information regarding the true value of σ, we need to have candidates with at least two tests. Hence, from now on we assume exactly that, i.e., ∀iπGreedy(yˆi,1) = more for dynamic policies and τ ≥ 2 for ﬁxed number of tests policies.

Now, in both policies we have showed that the optimal rule is to reject candidates that fail their ﬁrst test. Therefore inconsistencies between the ﬁrst two tests are seen only in cases where yˆi,1 = 1, yˆi,2 = 0.

Let c be the number of inconsistencies in the ﬁrst two tests, i.e., c = |{(yˆi,1, yˆi,2) : yi,1 = yi,2}|,

and let m be the number of candidates with at least two tests. Since c is generated by sampling m

times, the distribution Br(( 1+2σ )( 1−2σ )) = Br( 1−4σ2 ) and we can estimate σ as stated in the next theorem:

Theorem

15.

If

we

have

results

from

m≥

1 2ǫ2

ln

2 δ

candidates,

by

using

σˆ

=

probability 1 − δ we have that |σˆ − σ| ≤ ǫ.

1 − 4 mc , then with

Having an estimation of the parameter σˆ, we can calculate the estimated p as follows: Let pyˆ∗,1=1 :=

i I(yˆi,1=1) be the percentage of positive ﬁrst tests. Since this number is generated by the distribu-

tion

m
Br( 1 (p(1

+

σ)

+

(1

−

p)(1

−

σ)))

=

Br( 1

+

(2p

−

1) σ ),

we

can

estimate

pˆ

using

the

estimated

2

2

2

value of σˆ.

Theorem 16. If we have results from m ≥ 21ǫ2 ln δ2 candidates, by using pˆ = 2(py∗,1=σˆ1−1)+σˆ , we get

that with probability 1 − δ we have that |pˆ − p| ≤ 2ǫ.

Under the Gaussian screening model, the parameter estimation is also straightforward (assuming

realizability) without access to the true skill level of the employees. We start by looking at a single

candidate, i. Each of his test results, yˆi,j is generated from a conditional distribution P (Yi|Qi = qi)

which is a Gaussian with mean qi and variance ση2. Since this variance is common among all the

candidates, we can simply average the estimated variance of every candidate to get an approximation

for

ση2.

Suppose

yˆi,1, . . . , yˆi,n

is

a

sequence

of

n

i.i.d

tests

of

candidate

i,

and

let

yi

=

1 n

n j=1

yi,j

be the empirical mean of candidate i’s tests.

The following theorem is a result from Hoeﬀding’s Inequality, in which we use to bound the error

of our estimated parameters.

Theorem

17.

By

using

the

following

as

estimators

for

Gaussian

parameters

µˆQ

=

1 m

m i=1

yi,

σˆη2

=

1 m

m1 i=1 n

nj=1(yi,j − yi)2

and

σˆQ2

=

1 m

m i=1

(µˆQ

−

yi

)2

(notice

that

E[σˆη2]

=

ση2

and

E[σˆQ2 ]

=

σQ2 ), the diﬀerence between each parameter and it’s estimator is bounded by O(

1 m

ln(

1 δ

)).

10

7 Discussion and Future Work
Consider two groups with identically-distributed skills and characterized by diﬀerent noise levels in screening. Our results demonstrate that if a regulatory body (e.g., policymakers or a regulator) insists on the same number of tests and the same decision rule for both groups, this would yield higher false positive rates in any threshold policy. As a result, hired candidates from the noisier group would suﬀer higher rates of ﬁring. In turn, this might lead employers to erroneously conclude that this group’s skill level is lower than it actually is. This paper presents a policy that handles this problem by minimizing the false positive rates of both groups, in the form of a greedy policy. Moreover the greedy policy is eﬃcient, minimizing the expected number of tests per hire among all policies that achieve a speciﬁed false positive rate and continue testing every candidates that appear better than the a new one. However, the dynamic policy will still suﬀer (as does the simple threshold policy) from higher false negative rates for the noisier group, violating a notion of fairness dubbed equality of opportunity in the recent literature on fairness in machine learning [9]. We addressed this problem by modifying the greedy policy to reject candidate iﬀ Pr[yi = +1|yˆi,1 . . . yˆi,τ ] < ǫ′ by setting ǫ′ < p. Our greedy policy can be made forgiving and equalize false negative rates across groups. In future work, we plan to explore extensions to the Gaussian model.
11

References
[1] Aigner, D. J., & Cain, G. G. (1977). Statistical theories of discrimination in labor markets. ILR Review , 30 (2), 175–187.
[2] Arrow, K., et al. (1973). The theory of discrimination. Discrimination in labor markets, 3 (10), 3–33.
[3] Becker, G. S. (1957). The economics of discrimination chicago. University of Chicago. [4] Berk, R., Heidari, H., Jabbari, S., Kearns, M., & Roth, A. (2018). Fairness in criminal justice
risk assessments: The state of the art. Sociological Methods & Research. [5] Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big data, 5 (2), 153–163. [6] Corbett-Davies, S., & Goel, S. (2018). The measure and mismeasure of fairness: A critical
review of fair machine learning. arXiv preprint arXiv:1808.00023 . [7] Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C., & Venkatasubramanian, S. (2017).
Runaway feedback loops in predictive policing. arXiv preprint arXiv:1706.09847 . [8] Feller, W. (1968). An Introduction to Probability Theory and Its Applications, vol. 1. Wiley. [9] Hardt, M., Price, E., Srebro, N., et al. (2016). Equality of opportunity in supervised learning.
In Advances in neural information processing systems, (pp. 3315–3323). [10] Hu, L., & Chen, Y. (2018). A short-term intervention for long-term fairness in the labor market.
In Proceedings of the 2018 World Wide Web Conference on World Wide Web, (pp. 1389–1398). International World Wide Web Conferences Steering Committee. [11] Kamiran, F., Calders, T., & Pechenizkiy, M. (2010). Discrimination aware decision tree learning. In ICDM . [12] Kamishima, T., Akaho, S., & Sakuma, J. (2011). Fairness-aware learning through regularization approach. In ICDM Workshops. [13] Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-oﬀs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807 . [14] Miller, K. S. (1981). On the inverse of the sum of matrices. Mathematics magazine, 54 (2), 67–72. [15] Pedreshi, D., Ruggieri, S., & Turini, F. (2008). Discrimination-aware data mining. In KDD. [16] Phelps, E. S. (1972). The statistical theory of racism and sexism. The american economic review , (pp. 659–661). [17] Verma, S., & Rubin, J. (2018). Fairness deﬁnitions explained. In Proceedings of the International Workshop on Software Fairness, FairWare ’18, (pp. 1–7). New York, NY, USA: ACM. [18] Whitt, W. (1980). Uniform conditional stochastic order. Journal of Applied Probability, 17 (1), 112–123.
12

A Proofs

A.1 Proofs from Section 3

Proof of Theorem 1. To prove the theorem, we show that the loss function lα(τ, θ), as a function

of θ is quasi-convex and achieves its minimum value at

1 (τ

−

l

og

(

1 p

−

1)

+

l

og

(

1 α

−

1)

)

.

2

log(1+

2σ 1−σ

)

Namely, we show that the loss is monotone increasing for

1 (τ

−

l

og

(

1 p

−

1)

+

l

og

(

1 α

−

1)

)

≤ θ ≤ τ − 1,

2

log

(1+

2σ 1−σ

)

i.e., increasing θ increases the loss: lα(θ) < lα(θ + 1).

Similarly, we show that for 1 ≤ θ ≤ Indeed,

1 (τ − log( p1 −1)+lo2gσ( α1 −1) ) , we have lα(θ) < lα(θ − 1).

2

log(1+ 1−σ )

lα(θ + 1, τ ) − lα(θ, τ ) = −α Pr[y = 0, Sτ = θ] + (1 − α) Pr[y = +1, Sτ = θ]

= −α Pr[Sτ = θ|y = 0] Pr[y = 0] + (1 − α) Pr[Sτ = θ|y = +1] Pr[y = +1]

Since Pr[y = 0] = 1 − p and Pr[y = +1] = p, we have

l 1 (θ + 1, τ ) − l 1 (θ, τ ) = −(1 − p)α Pr[Sτ = θ|y = 0] + p(1 − α) Pr[Sτ = θ|y = +1].

2

2

The above expression is positive iﬀ

(1 − p)α Pr[Sτ = θ|y = 0] < p(1 − α) Pr[Sτ = θ|y = +1]

(3)

Since Pr[Sτ = θ|y = 0] is the probability of exactly θ ﬂips, and Pr[Sτ = θ|y = +1] is the probability of exactly τ − θ ﬂips, we can calculate those probabilities as follows:

Pr[Sτ = θ|y = 0] = τ ( 1 − σ )θ( 1 + σ )τ−θ

θ2

2

Pr[Sτ = θ|y = +1] = Substituting expression in (3), we get

τ ( 1 − σ )τ−θ( 1 + σ )θ

τ −θ 2

2

(1 − p)α τ ( 1 − σ )θ( 1 + σ )τ−θ < p(1 − α) τ ( 1 − σ )τ−θ( 1 + σ )θ.

θ2

2

τ −θ 2

2

Rearranging, we get

( 1 − σ )2θ < ( 1 − σ )τ ( p )( 1 − α ).

1+σ

1+σ 1−p α

Applying log on both sides gets us

2θ log( 1 − σ ) < τ log( 1 − σ ) + log( p ) + log( 1 − α ).

1+σ

1+σ

1−p

α

Solving for θ, we ﬁnd that the inequality holds if

τ

log(

1−σ 1+σ

)

+

log(

p 1−p

)

+

log(

1−α α

)

θ > 2 log( 11+−σσ ) =

1

log( p1 − 1) + log( α1 − 1)

2 (τ −

log(1 + 12−σσ ) )

13

For θ ≥

1 (τ

−

l

og

(

1 p

−

1)

+

l

o

g

(

1 α

−

1)

)

, we have

2

log

(1+

2σ 1−σ

)

(1 − p)α Pr[Sτ = θ|y = 0] < p(1 − α) Pr[Sτ = θ|y = +1],

and for θ ≤

1 (τ

−

l

og

(

1 p

−

1)

+

l

og

(

1 α

−

1)

)

, we have

2

log

(1+

2σ 1−σ

)

α(1 − p) Pr[Sτ = θ|y = 0] > (1 − α)p Pr[Sτ = θ|y = +1].

This implies that the maximum is θp∗,α = 12 (τ − log( p1lo−g(11)++lo2gσ( α1) −1) ) . 1−σ

Proof of Theorem 2. We start with a skilled candidate. The expected number of tests that a skilled

candidate

passes

is

E[Sτ |y

=

+1]

=

τ

(

1+ 2

σ

)

>

τ 2

.

By using Hoeﬀding’s inequality for Bernoulli distributions, for every ǫ > 0,

Pr[E[Sτ ] − Sτ ≥ ǫ|y = +1] = Pr[τ ( 1 + σ ) − Sτ ≥ ǫ|y = +1] ≤ e−2ǫ2τ < δ. 2

Choosing

ǫ

=

σ 2

yields

Sτ

≤

τ 2

<

⌈ τ2 ⌉

(as

τ

is

odd),

which

holds

iﬀ

a

majority

threshold

policy

would

predict

that

this

is

an

unskilled

candidate

(false

negative).

Solving

for

τ,

we

get

τ

>

1 σ2

ln( 1δ ).

We now repeat the process for an unskilled candidate. The expected number of tests that an

unskilled

candidate

passes

is

E[Sτ |y

=

0]

=

τ

(

1− 2

σ

)

<

τ 2

.

By using Hoeﬀding’s inequality again, we have

Pr[Sτ − E[Sτ ] ≥ ǫ|y = 0] = Pr[Sτ − τ ( 1 − σ ) ≥ ǫ|y = 0] ≤ e−2ǫ2τ < δ 2

Choosing

ǫ

=

σ 2

yields

Sτ

>

τ2 ,

which

holds

iﬀ

a

majority

threshold

falsely

predicts

that

this

is

a

skilled

candidate

(false

positive).

Solving

for

τ

again,

we

get

τ

>

1 σ2

ln( δ1 ).

Overall,

τ

>

α(1−p) σ2

ln(

1 δ

)

+

p(1−α) σ2

ln( 1δ )

=

Ω

(

α+

p− σ2

2pα

ln( 1δ ))

Proof of Theorem 4. Let π′ be any optimal policy for (2) (not necessarily threshold) with a ﬁxed
number of tests, τ . We will show, in two steps, how to transform it into an optimal randomized threshold policy. The ﬁrst step is to symmetrize π′. Let rk = Pr[π(yˆ) = 1|Sτ = k]. Deﬁne a policy π′′, which performs τ tests, and accepts with probability rk where k = Sτ . Clearly, both π′ and π′′ have the same accept probability. In addition, since condition on Sτ = k, any sequence of outcomes is equally likely. Furthermore, and the probability that y = 1 given any sequence of outcomes with
Sτ = k, is identical. (Technically, Sτ is a suﬃcient statistics.) This implies that the false discovery rate is also unchanged.

This yields that π with the randomization vector r is also optimal.
The second step is to suppose—for sake of contradiction—that π′′ is not a randomized threshold policy. We will show that we can improve the FDR of π′′ while keeping the probability of acceptance unchanged. This will contradict the hypothesis that π′ is optimal.

14

If π′′ is not a randomized threshold policy, then there is no θ and k, such that

rk = Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|Sτ = k = θ] = 0, 1

if k < θ . if k > θ

Now, let k be the minimal value such that rk > 0 and let 0 < i < τ − k be the minimal value for which 0 < rk+i < 1. Clearly, the FDR is lower at Sτ = k + i than at Sτ = k. Intuitively, we can shift some probability mass, ǫk > 0 from rk to rk+i in a way that maintains the acceptance probability of π and decreases the false positive rates.

Let ǫk+i > 0 be such that ǫk · rk = ǫk+i · rk+i. Let r′ be a modiﬁed randomization vector for

π such that rk′ = rk(1 − ǫk), rk′ +i = rk+i(1 + ǫk+i) and for every l ∈/ {k, k + i} rl′ = rl. Since

Pr[π(yˆi,1, . . . , yˆi,τ ) = 1] =

τ l=1

rl

=

l∈/{k,k+i} rl + rk′ + rk′ +i, the acceptance probability remains

the same. As for the false discovery rate, since Pr[yi = 0|Sτ = k + i] < Pr[yi = 0|Sτ = k], Pr[Sτ = k + i] is higher with r′ than with r, Pr[Sτ = k] is lower with r′ than with r and for any l ∈/ {k, k + i}, Pr[Sτ = l] with r′ is the same as with r, the false discovery rate with r′ is lower,

which contradicts the optimality of π with r as the randomization vector.

Proof of Theorem 5. Using Bayes’ theorem, the conditional probability can be decomposed as

Pr[yi = +1|Sτ = θ] = Pr[yi = +1] Pr[Sτ = θ|yi = +1] = Pr[Sτ = θ]

Since τ − θ < θ and

p

τ θ

τ θ

=

p

τ θ

(

1− 2

σ

)τ

−

θ

(

1+ 2

σ

)θ

.

( 1−2σ )τ −θ( 1+2σ )θ + (1 − p)

τ τ −θ

(

1+ 2

σ

)τ

−

θ

(

1− 2

σ

)θ

τ τ −θ

,

we

get

p(1 + σ)2θ−τ

p

(

1+σ 1−σ

)2θ

−

τ

p(1 + σ)2θ−τ + (1 − p)(1 − σ)2θ−τ = p( 11+−σσ )2θ−τ + 1 − p .

Since ( 11+−σσ ) > 1 it holds that ( 11+−σσ )2θ−τ > 1,

So, And ﬁnally,

( 1 + σ )2θ−τ (1 − p) > 1 − p. 1−σ

( 1 + σ )2θ−τ > p( 1 + σ )2θ−τ + 1 − p,

1−σ

1−σ

p

(

1+σ 1−σ

)2θ

−

τ

Pr[yi′ = +1] = p < p( 11+−σσ )2θ−τ + 1 − p = Pr[yi = +1|Sτ = θ].

Proof of Lemma 7. Let Sτ′ = of Sτ′ . Note that

τ j=1

(2

yˆi,

j

−

1

),

and

let

sτ

∈

{−τ, . . . , τ}

be

any

of

the

possible

values

Pr[yˆi,j = 1|yi = 1] = 1 + σ . Pr[yˆi,j = 1|yi = 0] 1 − σ

15

Since the yˆi,j are i.i.d., we have

τ

Pr[yˆi,j = +1|yi = +1]

Xτ =X0 + (2yˆi,j − 1) · log( Pr[yˆi,j = +1|yi = 0] )

j=1

p

1+σ

= log( 1 − p ) + Sτ log( 1 − σ )

= log(( p )( 1 + σ )Sτ ). 1−p 1−σ

Since

Pr[Sτ = sτ |yi = 1] = ( 1 + σ )sτ , Pr[Sτ = sτ |yi = 0] 1 − σ

we have

Xτ = log(( p )( Pr[Sτ = sτ |yi = 1] )).

(4)

1 − p Pr[Sτ = sτ |yi = 0]

Since

Pr[Sτ = sτ |yi = 1] = Pr[Sτ = sτ ] · Pr[yi = 1|Sτ = sτ ] Pr[yi = 1]

and Pr[Sτ = sτ |yi = 0] = Pr[Sτ = sτ ] · Pr[yi = 0|Sτ = sτ ] , Pr[yi = 0]

assigning Pr[yi = 0] = 1 − p and Pr[yi = 1] = p, we get

Pr[Sτ = sτ |yi = 1] = (1 − p) · Pr[yi = 1|Sτ = sτ ] .

(5)

Pr[Sτ = sτ |yi = 0]

p · Pr[yi = 0|Sτ = sτ ]

Applying (5) in (4) and adding Xτ ≥ log β gives us

Xτ = log

Pr[yi = 1|Sτ = sτ ] Pr[yi = 0|Sτ = sτ ]

= log

Pr[yi = 1|Sτ = sτ ] 1 − Pr[yi = 1|Sτ = sτ ]

≥ log β

Pr[yi = 1|Sτ = sτ ] ≥ β 1 − Pr[yi = 1|Sτ = sτ ] Pr[yi = 1|Sτ = sτ ] ≥ β(1 − Pr[yi = 1|Sτ = sτ ])
β Pr[yi = 1|Sτ = sτ ] ≥ 1 + β

Applying (5) in (4) and adding Xτ < log β′ gives us

Pr[yi = 1|Sτ = sτ ] < β′ 1 − Pr[yi = 1|Sτ = sτ ]

Hence

β′ Pr[yi = 1|Sτ = sτ ] < 1 + β′

16

Proof of Theorem 9. First recall that given a skilled candidate, for every test j,

1+σ Pr[yˆi,j = +1|yi = +1] = 2

Hence

1−σ Pr[yˆi,j = 0|yi = +1] = 2 Pr[yˆi,j = 0|yi = 1] − Pr[yˆi,j = +1|yi = 1] = −σ.

The lower absorbing barrier is reached when a candidate’s posterior skill level is lower than the

prior of the skill level, i.e.,

ǫ′

1+σ

log 1 − ǫ′ − log 1 − σ

and the starting point is just one step away from the lower absorbing barrier:

p X0 = log 1 − p .

According to Corollary 8, the upper absorbing barrier is in

log( 1 − ǫ ). ǫ

To derive the results for the expected duration of the random walk for skilled and unskilled candidates, we shift the locations of the absorbing points so that the lower barrier would be in 0 and also divide them by a step size (so now we have that every step is of size 1). The new upper absorbing barrier is at

log( 1−ǫ ǫ )

−

(log

ǫ′ 1−ǫ′

−

log( 11−+σσ ))

 log( (1−ǫǫ)ǫ(1′(−1−ǫ′)σ()1+σ) ) 

a=

log( 1+σ )

= 

log( 1+σ )

. 

1−σ



1−σ



And we also shift the starting point:

log

p 1−p

− (log

ǫ′ 1−ǫ′

−

log(

1+σ 1−σ

))

 log( pǫ′((11−−ǫp′))((11−+σσ)) ) 

z = log( 11+−σσ ) =  log( 11+−σσ ) 

As stated in [8], the expected duration of a random walk with absorbing barriers of 0 and a from z = 1 is (equation 3.4, chapter XIV [page 348]):

1

1

−

(

q p

)z

1

1

−

(

1−σ 1+σ

)z

E[τs] = E[Dz=1] = q − p z − a · 1 − ( pq )a = −σ z − a · 1 − ( 11+−σσ )a .

Hence,

1

1

−

(

1−σ 1+σ

)z

E[τs] = σ a · 1 − ( 1−σ )a − z .

1+σ

17

As for unskilled candidates, the absorbing points and the starting point are the same, the only

diﬀerence is that

1−σ Pr[yˆi,j = +1|yi] = 2

and 1 + σ Pr[yˆi,j = 0|yi = +1] = 2 .

Therefore,

Pr[yˆi,j = 0|yi = 0] − Pr[yˆi,j = +1|yi = 0] = σ

and we deduce

1

1

−

(

1+σ 1−σ

)z

E[τu] = σ z − a · 1 − ( 11+−σσ )a .

Deviations for the confusion matrix (Table 1). We split the claim in the confusion matrix (Table 1) into two parts. First, using equation (2.4) from chapter XIV [page 345] in [8], we get

( 11−+σσ )a − ( 11−+σσ )z

FNR = Pr[πGreedy(yˆi,1, . . . , yˆi,τ ) = 0|yi = +1] =

(

1−σ 1+σ

)a

−

1

and

(

1+σ 1−σ

)a

−

(

1+σ 1−σ

)z

TNR = Pr[πGreedy(yˆi,1, . . . , yˆi,τ ) = 0|yi = 0] = ( 1+σ )a − 1 .

1−σ

The second part follows from the fact the gambler’s ruin must end in case of absorbing barriers.

( 11−+σσ )a − ( 11+−σσ )z TPR = Pr[πGreedy(yˆi,1, . . . , yˆi,τ ) = 1|yi = +1] = 1 − ( 11+−σσ )a − 1 =

( 11−+σσ )z − 1

pǫ′((11−−ǫp′))((11+−σσ)) − 1

µ(1−p) p

−

1

(1 − ǫ)(µ(1 − p) − p)

( 11−+σσ )a − 1 = (1−ǫǫ′)′ǫ(1(1−−ǫσ)()1+σ) − 1 = (1ǫ−µǫ) − 1 = p(ǫµ − (1 − ǫ)) ,

Where µ := therefore

(

ǫ′(1−σ) 1−ǫ′)(1+

σ

)

.

For ǫ ≤ 1/4 and p < 1/2 we get 0 ≤ µ ≤ 1/3 and µ = Θ(ǫ′(1 − σ)),

TPR = Θ p − µ = Θ 1 − ǫ′ (1 − σ) .

p

p

Hence

FNR

=

Θ

(

ǫ′ p

(1

−

σ)).

( 11+−σσ )z − 1

p(1(−1−pǫ)ǫ′)′((11−+σσ)) − 1

FPR = Pr[πGreedy(yˆi,1, . . . , yˆi,τ ) = 1|yi = 0] = ( 11+−σσ )a − 1 = (1−ǫǫ′)′ǫ(1(1−−ǫσ)()1+σ) − 1 =

p (1−p)µ

−

1

ǫ(p − (1 − p)µ)

′′

= (1−ǫ) − 1 (1 − p)(1 − ǫ − ǫµ) = Θ (ǫ(p − µ)) = Θ (ǫ(p − ǫ + ǫ σ))

ǫµ

Hence TNR = Θ (1 − ǫ(p − ǫ′ + ǫ′σ)).

18

Proof of Theorem 10.

E[τ ] = E[τs]p + E[τu](1 − p) =

=1 σ

1

−

(

1−σ 1+σ

)z

a · 1 − ( 11−+σσ )a − z

≈ 1 a · (1 − ǫ′ (1 − σ)) − z

σ

p

p+ 1 σ

1

−

(

1+σ 1−σ

)z

z − a · 1 − ( 11+−σσ )a

(1 − p) =

p + 1 (z − a(ǫ(p − ǫ′ + ǫ′σ)))(1 − p) ≈ ap

σ

σ

A.2 Proofs from Section 4

The next lemma aids in the proof of Theorem (11).

Lemma 18. Let Znη be a Binomial random variable with parameters n ∈ N and η ∈ (0, 1). Given

a number of successes, k ∈ {0, . . . , n}, we know that the probability mass function of Znη is fk(η) :=

Pr[Znη = k] =

n k

ηk(1 − η)n−k.

Let

L(η|k)

be

the

likelihood

function

of

the

event

Znη

=

k.

Then

the

maximum likelihood of fk(η) is η = nk . I.e.,

L(η|k) = argmaxηfk(η) = argmaxη nk ηk(1 − η)n−k = nk .

Proof of Lemma 18. We notice that

n k

does not depend on η, thus

argmaxηfk(η) = argmaxη nk ηk(1 − η)n−k = argmaxηηk(1 − η)n−k

The log-likelihood is particularly convenient for maximum likelihood estimation. Logarithms are strictly increasing functions, as a result, maximizing the likelihood is equivalent to maximizing the log-likelihood, i.e.,

argmaxηηk(1 − η)n−k = argmaxη ln(ηk(1 − η)n−k) = argmaxηk ln(η) + (n − k) ln(1 − η)

Diﬀerentiating (with respect to η) and comparing to zero we get

d ln(fk(η)) = k − n − k = 0.

dη

η 1−η

And after refactoring,

k(1 − η) = (n − k)η

The function ln(fk(η)) is a strictly concave as its second derivative is negative,

d2 ln(fk(η)) = − k − n − k < 0,

dη2

η2 (1 − η)2

And

since

the

derivative

of

a

strictly

concave

function

is

zero

at

nk ,

then

ηˆ

=

k n

is

a

global

maximum.

Therefore,

ηˆ =

k n

obtains

absolute

maximum

in

fk(η).

19

Proof of Theorem 11. Let Zτηi be a random variable that represents the number of ﬂips out of a τ -tests sequence with a noise level of ηi, i.e., Zτηi is the number of times when yj = y for 1 ≤ j ≤ τ . We use Zτηi to express Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|yi = 0, η = ηi] as the probability that at least θ ﬂips,
Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|yi = 0, η = ηi] = Pr[Zτηi ≥ θ]

and the probability of Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|q = +1, η = ηi] as at most τ − θ ﬂips, thus

Pr[π(yˆi,1, . . . , yˆi,τ ) = 1|yi = +1, η = ηi] = Pr[Zτηi ≤ τ − θ].

From Lemma (18) and since probability density function (pdf) are is monotone increasing, we derive

that the pdf of

Znη2

satisﬁes monotone

likelihood

ratio

property

over the

pdf

of

Z

η1 n

.

This implies

that the pdf of Znη2 also has ﬁrst-order stochastic dominance over Znη1 by Theorem 1.1 in [18]. From

stochastic dominance, we can derive the desired inequalities

F Pθη,1τ

=

Pr[θ

≤

Z

η1 n

]

<

Pr[θ

≤

Z

η2 n

]

=

F Pθη,2τ

and

F

N

η1 θ,τ

=

Pr[Znη1

≤

τ

−

θ]

<

Pr[Znη2

≤

τ

−

θ]

=

F

N

η2 θ,τ

.

B Gaussian Worker Screening Model Extension

In this extension, we characterize the conditional expectation, [Q|Y1, ..., Yn] and the conditional variance of Q given the testsYi, i.e., Var[Q|Y1, ..., Yn].
First, note that because P (Q) is Gaussian, and the conditionals P (Yi|Q) are all Gaussian, the joint probability P (Q, Y1, ..., Yn) is a multivariate Gaussian. We work out the precise analytic forms for the mean and variance of the conditional P (Q|Y1, ..., Yn) in terms of the quality and noise variances (σQ2 and ση2) and the number of tests n.
To begin, we note the properties of the joint distribution over P (Q, Y1, ..., Yn). Owing to the generative process for our Yi, all have mean µQ, and thus the joint over the means is an n + 1dimensional vector (µQ, ..., µQ). The full n + 1 × n + 1 covariance matrix Σ has the form

σQ2

σQ2

...

σQ2 

σQ2 σQ2 + ση2 . . .

σQ2 

Σ =  ... ... . . . ...  (6)

σQ2

σQ2

. . . σQ2 + ση2

where all oﬀ-diagonal entries Σij for i = j have value σQ2 and all diagonal entries i = j ≥ 2 corresponding to the variance of tests Yi take value σQ2 + ση2. The top-left entry corresponds to the variance of the test Q and thus has variance σQ2 .
We can now derive the equations for the conditional mean and conditional variance of Q|Y1, ..., Yn. To begin, note the following basic facts about deriving conditionals from multivariate Gaussians:

20

to estimate the conditional of one set of variables, given another set P (x1|x2) we can segment our data matrix into those rows corresponding to the variables we don’t condition upon (here, just

Q) and those we do (here, Y1, .., Yn), expressing our covariance matrix in terms of the following

submatrices:

Σ = Σ11 Σ12 Σ21 Σ22

Here, Σ11 ∈ Ê1×1, Σ21 ∈ Ên×1, Σ21 ∈ Ê1×n, and Σ22 ∈ Ên×n.

The conditional expectation [Q|Y1, ..., Yn] is then expressed as

[Q|Y1, ..., Yn] = µQ + Σ12Σ−221(y − µy)

(7)

and the conditional variance V ar[Q|Y1, ..., Yn] can be expressed as

V ar[Q|Y1, ..., Yn] = Σ11 − Σ12Σ−221Σ21

(8)

which should be familiar as the Schur complement of the n × n matrix Σ22. Intuitively, this corresponds to inverting the full matrix Σ, deleting those rows and columns corresponding to observed variables, and then inverting the resulting (1 × 1) matrix back.
What remains is to show that for the particular covariance matrix that interests us (Equation 6), these expressions have simple analytically computable forms. Speciﬁcally we state the following simple theorems. Theorem 19. For jointly Gaussian variables Q, Y1, ..., Yn characterized by the covariance matrix given in (Equation 6), the conditional expectation takes form





1

[Q|Y1, ..., Yn] = µQ +  σ2 , . . .  · (y − µy)

(9)

η
σ2

+n

Q

Theorem 20. For jointly Gaussian variables Q, Y1, ..., Yn characterized by the covariance matrix given in (Equation 6), the conditional variance of Q given the tests Yi takes form

1

Var[Q|Y1, ..., Yn] = 1 + n

σQ 2

ση2

(10)

B.1 Proofs from Appendix B

Proof of Theorem 19. The crucial step to apply Equation 7 to this data is to work out a simple expression for the inverse of the n × n submatrix Σ22. We recall that this matrix is symmetrical with all diagonal entries equal to σQ2 + ση2 and all oﬀ diagonals equal to σQ2 .
We call upon a lemma due to [14]. Which states that when A is invertible and B is a rank-1 matrix, the inverse of their sum takes the following form:

(A + B)−1 = A−1 −

1

A−1BA−1

(11)

1 + Trace(BA−1)

21

We can decompose Σ22 into such an A and B by deﬁning B to be the matrix that takes value σQ2 everywhere and A to be a diagonal matrix that takes values ση2 (along the main diagonal). Thus Σ−221 = (A + B)−1 and we can proceed by applying the lemma.

First

we

note

that

A−1

is

a

diagonal

matrix

with

all

entries

on

the

main

diagonal

set

to

1 σ2

.

Then

η

we note that BA−1 is an n × n matrix with all entries set to σσQ 2η2 . Thus, Trace(BA−1) = nσση2Q 2 .

The matrix A−1BA−1 has all entries equal to σσQ 2η4 , and thus our desired inverse Σ2−21 can be expressed

as follows:

1
σ2

0

η

 σσQ 2η4 . . . . . .

Σ−221

=

(A

+

B)−1

=

 

...

 

−

1

2

 

...

...

...

 

 0


1
2

1+

nσQ ση2

 

..

 .. 

ση

. ... .

Thus each oﬀ-diagonal entry takes values

σQ 2

σQ 2

ση2

− ση4

= − ση4

· σQ 2

1 + nσση2Q 2

1 + nσQ 2 ση2

ση2

σQ 2

1
= − ση2 σσQ 2η2 + n

1

= − σ4

η
σ2

+ nση2

Q

(12) (13) (14)

and each on-diagonal entry has an

additional

1 σ2

term

that

comes from

A−1.

η

1

1

ση2 − ση4 + nσ2

σQ 2

η

(15)

Now that we know the precise expression for all entries of Σ−221, we can calculate the vector-matrix product Σ12Σ2−21. Because every entry of Σ12 takes value σQ2 , and because every column of Σ−221 has the same n values (just in diﬀerent order), the product Σ12Σ−221 is an n dimensional vector, where

22

all n values are equal:

Σ12Σ−221 =

σQ2

nσQ2

ση2

−

ση4

,... + nσ2

σ2

η

 σQ 2 =  ση2


ση4 σ2

+ nση2

Q

ση4 σ2

+ nση2

Q

 nσQ2
,... 





ση2

=  σ4

,...

η
σ2

+ nση2

Q





1

=  σ2 , . . . 

η
σ2

+n

Q

(16)

This expression for Σ12Σ−221, together with the deﬁnition of the conditional expectation of a multivariate Gaussian (Equation 7) concludes the proof.

Proof of Theorem 20. We can now produce the expression for Σ12Σ2−21Σ21. Because every entry of the 1 × n matrix Σ12Σ−221 takes value
1 σσQ 2η2 + n and because every entry in the n × 1 matrix Σ21 takes value σQ2 ,

Σ12Σ−221Σ21 =

nσQ2 ση2 + n
σQ 2

(17)

The expression for the conditional variance Var[Q|Y1, ..., Yn] follows:

Var[Q|Y1, ..., Yn] = Σ11 − Σ12Σ−221Σ21

= σ2 − nσQ2 Q σσQ 2η2 + n

σQ2 =

σσ2η2 + n − nσQ2 Q ση2 + n σQ 2

ση2

= σ2

η
σ2

+n

Q

1

= 1 +n

σQ 2

ση2

(18)

23

B.2 Proofs

Proof of Theorem 12. First, recall that

1

σQ2 ση2

Var[Q|Y1, ..., Yn] = σ1Q 2 + σnη2 = ση2 + nσQ2 .

Solving for n2 in the equation Var1[Q|Y1, ..., Yn1 ] = Var2[Q|Y1, ..., Yn2 ],

σQ2 ση21

σQ2 ση22

ση2 + n1σQ2 = ση2 + n2σQ2

1

2

we get

ση21 (ση22 + n2σQ2 ) = ση22 (ση21 + n1σQ2 )

and hence

ση21 n2 = ση22 n1.

Extracting n2, we ﬁnd that n2 = ση22 n1.
ση21

Proof of Theorem 13. First, recall that





1

[Q|Y1, ..., Yn] = µQ +  σ2 , . . .  · (y − µy) = µQ +

η
σ2

+n

Q

σQ2

σ2 + nσ2 , . . .

η

Q

· (y − µy)

Now,

1[Q|Y1, ..., Yn1 ] − 2[Q|Y1, ..., Yn2 ] =

σQ2

ση2

+ n1σ2 , . . .
Q

1

· (y1 − µy) −

σQ2

ση2

+ n2σ2 , . . .
Q

2

· (y2 − µy)

σQ2

σQ2

= σ2
η

+ n1σQ2 n1(y¯1) − ση2

+ n2σ2 n2(y¯2)
Q

1

2

σQ2 n1

σQ2 n2

= σ2
η

+

n1σ2 (y¯1)
Q

−

ση2

+ n2σ2 (y¯2)
Q

1

2

σ2 n

σQ2

ση22 σ2

n1

=

Q 1 (y¯1) −

η1

(y¯2)

ση21 + n1σQ2

ση22 + σσηη2212 n1σQ2

σQ2 n1

σQ2 n1

= σ2
η

+

n1σ2 (y¯1)
Q

−

ση2

+ n1σ2 (y¯2)
Q

1

1

24

