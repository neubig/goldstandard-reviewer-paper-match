Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation

Xinyi Wang Language Technologies Institute
Carnegie Mellon University xinyiw1@cs.cmu.edu

Graham Neubig Language Technologies Institute
Carnegie Mellon University gneubig@cs.cmu.edu

arXiv:1905.08212v1 [cs.CL] 20 May 2019

Abstract
To improve low-resource Neural Machine Translation (NMT) with multilingual corpora, training on the most related high-resource language only is often more effective than using all data available (Neubig and Hu, 2018). However, it is possible that an intelligent data selection strategy can further improve lowresource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose an efﬁcient algorithm, Target Conditioned Sampling (TCS), which ﬁrst samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings signiﬁcant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead.
1 Introduction
Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, using a single language is also substantially faster to train, speeding experimental cycles (Neubig and Hu, 2018). In this paper, we go a

step further and ask the question: can we design an intelligent data selection strategy that allows us to choose the most relevant multilingual data to further boost NMT performance and training speed for LRLs?
Prior work has examined data selection from the view of domain adaptation, selecting good training data from out-of-domain text to improve in-domain performance. In general, these methods select data that score above a preset threshold according to some metric, such as the difference between in-domain and out-ofdomain language models (Axelrod et al., 2011; Moore and Lewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encountered for LRLs (Duh et al., 2013).
In this paper, we create a mathematical framework for data selection in multilingual MT that selects data from all languages, such that minimizing the training objective over the sampled data approximately minimizes the loss of the LRL MT model. The formulation leads to an simple, efﬁcient, and effective algorithm that ﬁrst samples a target sentence and then conditionally samples which of several source sentences to use for training. We name the method Target Conditioned Sampling (TCS). We also propose and experiment with several design choices for TCS, which are especially effective for LRLs. On the TED multilin-

gual corpus (Qi et al., 2018), TCS leads to large improvements of up to 2 BLEU on three of the four languages we test, and no degradation on the fourth, with only slightly increased training time. To our knowledge, this is the ﬁrst successful application of data selection to multilingual NMT.

2 Method

2.1 Multilingual Training Objective

First, in this section we introduce our problem for-

mally, where we use the upper case letters X, Y to

denote the random variables, and the correspond-

ing lower case letters x, y to denote their actual

values. Suppose our objective is to learn param-

eters θ of a translation model from a source lan-

guage s into target language t. Let x be a source

sentence from s, and y be the equivalent target sen-

tence from t, given loss function L(x, y; θ) our objective is to ﬁnd optimal parameters θ∗ that mini-

mize:

Ex,y∼PS(X,Y )[L(x, y; θ)]

(1)

where Ps(X, Y ) is the data distribution of s-t parallel sentences.
Unfortunately, we do not have enough data to accurately estimate θ∗, but instead we have a mul-
tilingual corpus of parallel data from languages {s1, S2, ..., Sn} all into t. Therefore, we resort to multilingual training to facilitate the learning of θ. Formally, we want to construct a distribution Q(X, Y ) with support over s1, s2, ..., sn-T to augment the s-t data with samples from Q during training. Intuitively, a good Q(X, Y ) will have an
expected loss

Ex,y∼Q(X,Y )[L(x, y; θ)]

(2)

that is correlated with Eqn 1 over the space of all θ, so that training over data sampled from Q(X, Y ) can facilitate the learning of θ. Next, we explain a version of Q(X, Y ) designed to promote efﬁcient multilingual training.

2.2 Target Conditioned Sampling
We argue that the optimal Q(X, Y ) should satisfy the following two properties.
First, Q(X, Y ) and Ps(X, Y ) should be target invariant; the marginalized distributions Q(Y ) and Ps(Y ) should match as closely as possible:

Q(Y ) ≈ Ps(Y )

(3)

This property ensures that Eqn 1 and Eqn 2 are optimizing towards the same target Y distribution.

Second, to have Eqn 2 correlated with Eqn 1 over the space of all θ, we need Q(X, Y ) to be correlated with Ps(X, Y ), which can be loosely written as

Q(X, Y ) ≈ Ps(X, Y ).

(4)

Because we also make the target invariance assumption in Eqn 3,

Q(X, Y ) ≈ Ps(X, Y )

(5)

Q(Y )

Ps(Y )

Q(X|Y ) ≈ Ps(X|Y ).

(6)

We call this approximation of Ps(X|Y ) by Q(X|Y ) conditional source invariance. Based on these two assumptions, we deﬁne Target Conditioned Sampling (TCS), a training framework that ﬁrst samples y ∼ Q(Y ), and then conditionally samples x ∼ Q(X|y) during training. Note Ps(X|Y = y) is the optimal back-translation distribution, which implies that back-translation (?) is a particular instance of TCS.
Of course, we do not have enough s-t parallel data to obtain a good estimate of the true backtranslation distribution Ps(X|y) (otherwise, we can simply use that data to learn θ). However, we posit that even a small amount of data is sufﬁcient to construct an adequate data selection policy Q(X|y) to sample the sentences x from multilingual data for training. Thus, the training objective that we optimize is

Ey∼Q(Y )Ex∼Q(X|y) [L(x, y; θ)]

(7)

Next, in Section 2.3, we discuss the choices of Q(Y ) and Q(X|y).

2.3 Choosing the Sampling Distributions
Choosing Q(Y ). Target invariance requires that we need Q(Y ) to match Ps(Y ), which is the distribution over the target of s-t. We have parallel data from multiple languages s1, s2, ..., sn, all into t. Assuming no systematic inter-language distribution differences, a uniform sample of a target sentence y from the multilingual data can approximate Ps(Y ). We thus only need to sample y uniformly from the union of all extra data.
Choosing Q(X|y). Choosing Q(X|y) to approximate Ps(X|y) is more difﬁcult, and there are a number of methods could be used to do so. To do so, we note that conditioning on the same target y and restricting the support of Ps(X|y) to the

sentences that translate into y in at least one of sit, Ps(X = x|y) simply measures how likely x is in s. We thus deﬁne a heuristic function sim(x, s) that approximates the probability that x is a sentence in s, and follow the data augmentation ob-
jective in Wang et al. (2018) in deﬁning this prob-
ability according to

Q∗(x|y) = exp (sim(x, s)/τ )

(8)

x′ exp (sim(x′, s)/τ )

where is a temperature parameter that adjusts the peakiness of the distribution.

2.4 Algorithms
The formulation of Q(X, Y ) allows one to sample multilingual data with the following algorithm:
1. Select the target y based on Q(y). In our case we can simply use the uniform distribution.
2. Given the target y, gather all data (xi, y) ∈ s1, s2, ...sn-t and calculate sim(xi, s)
3. Sample (xi, y) based on Q(X|y) The algorithm requires calculating Q(X|y) repeatedly during training. To reduce this overhead, we propose two strategies for implementation: 1) Stochastic: compute Q(X|y) before training starts, and dynamically sample each minibatch using the precomputed Q(X|y); 2) Deterministic: compute Q(X|y) before training starts and select x′ = argmaxx Q(x|y) for training. The deterministic method is equivalent to setting τ , the degree of diversity in Q(X|y), to be 0.

2.5 Similarity Measure
In this section, we deﬁne two formulations of the similarity measure sim(s, x), which is essential for constructing Q(X|y). Each of the similarity measures can be calculated at two granularities: 1) language level, which means we calculate one similarity score for each language based on all of its training data; 2) sentence level, which means we calculate a similarity score for each sentence in the training data.

Vocab Overlap provides a crude measure of surface form similarity between two languages. It is efﬁcient to calculate, and is often quite effective, especially for low-resource languages. Here we use the number of character n-grams that two languages share to measure the similarity between the two languages.

LRL Train Dev Test HRL Train
aze 5.94k 671 903 tur 182k bel 4.51k 248 664 rus 208k glg 10.0k 682 1007 por 185k slk 61.5k 2271 2445 ces 103k
Table 1: Statistics of our datasets.

We can calculate the language-level similarity between Si and S

simvocab-lang(si, s) = |vocabk(s) ∩ vocabk(si)| k

vocabk(·) represents the top k most frequent character n-grams in the training data of a language.
Then we can assign the same language-level similarity to all the sentences in si.
This can be easily extended to the sentence level by replacing vocabk(si) to the set of character ngrams of all the words in the sentence x.

Language Model trained on s can be used to

calculate the probability that a data sequence be-

longs to s. Although it might not perform well if

s does not have enough training data, it may still

be sufﬁcient for use in the TCS algorithm. The

language-level metric is deﬁned as

simLM-lang(si, s) = exp

ci∈si NLLs(ci) |ci ∈ si|

where NLLs(·) is negative log likelihood of a

character-level LM trained on data from s. Sim-

ilarly, the corresponding sentence level metric is

the LM probability over each sentence x.

3 Experiment

3.1 Dataset and Baselines
We use the 58-language-to-English TED dataset (Qi et al., 2018). Following the setup in prior work (Qi et al., 2018; Neubig and Hu, 2018), we use three low-resource languages Azerbaijani (aze), Belarusian (bel), Galician (glg) to English, and a slightly higher-resource dataset, Slovak (slk) to English.
We use multiple settings for baselines: 1) Bi: each LRL is paired with its related HRL, following Neubig and Hu (2018). The statistics of the LRL and their corresponding HRL are listed in Table 1; 2) All: we train a model on all 58 languages; 3) Copied: following Currey et al. (2017), we use the union of all English sentences as monolingual data by copying them to the source side.

Sim
-
Back-Translate
LM-sent LM-sent
LM-lang LM-lang
Vocab-sent Vocab-sent
Vocab-lang Vocab-lang

Method
Bi All copied
TCS
TCS-D TCS-S
TCS-D TCS-S
TCS-D TCS-S
TCS-D TCS-S

aze
10.35 10.21 9.54
7.79
10.34 10.95
10.76 11.47∗
10.68 11.09
10.58 11.46∗

bel
15.82 17.46 13.88
11.50
14.68 17.15
14.97 17.61
16.13 16.30
16.32 17.79

glg
27.63 26.01 26.24
27.45
27.90 27.91
27.92 28.53
27.29 28.36
28.17 29.57∗

slk
26.38 26.64 26.77
28.44
27.29 27.24
28.40 28.56∗
27.03 27.01
28.27 28.45

Table 2: BLEU scores on four languages. Statistical significance (Clark et al., 2011) is indicated with ∗ (p < 0.001), compared with the best baseline.

3.2 Experiment Settings

A

standard

sequence-to-

sequence (Sutskever et al., 2014) NMT model

with attention is used for all experiments. Byte

Pair Encoding (BPE) (Sennrich et al., 2016;

Kudo and Richardson, 2018) with vocabulary size

of 8000 is applied for each language individually.

Details of other hyperparameters can be found in

Appendix A.1.

3.3 Results
We test both the Deterministic (TCS-D) and Stochastic (TCS-S) algorithms described in Section 2.4. For each algorithm, we experiment with the similarity measures introduced in Section 2.5. The results are listed in Table 2.
Of all the baselines, Bi in general has the best performance, while All, which uses all the data and takes much longer to train, generally hurts the performance. This is consistent with ﬁndings in prior work (Neubig and Hu, 2018). Copied is only competitive for slk, which indicates the gain of TCS is not simply due to extra English data.
TCS-S combined with the language-level similarity achieves the best performance for all four languages, improving around 1 BLEU over the best baseline for aze, and around 2 BLEU for glg and slk. For bel, TCS leads to no degradation while taking much less training time than the best baseline All.

TCS-D vs. TCS-S. Both algorithms, when using document-level similarity, improve over the baseline for all languages. TCS-D is quite effective without any extra sampling overhead. TCS-S

Sim
-
LM-lang LM-lang
Vocab-lang Vocab-lang

Model
Bi All copied
TCS-D TCS-S
TCS-D TCS-S

aze
11.87 10.87 10.74
11.97 12.55†
12.30 12.37

bel
18.03 17.77 17.19
17.17 17.23
18.96 19.83†

glg
28.70 25.49 29.75
30.10 30.69 31.10∗ 30.94

slk
26.77 26.28 27.81
28.78 28.95 29.35∗ 29.00

Table 3: BLEU scores using SDE as word encoding. Statistical signiﬁcance is indicated with ∗ (p < 0.001) and † (p < 0.05), compared with the best baseline.

outperforms TCS-D for all experiments, indicating the importance of diversity in the training data.
Sent. vs. Lang. For all experiments, languagelevel outperforms the sentence-level similarity. This is probably because language-level metric provides a less noisy estimation, making Q(x|y) closer to Ps(x|y).
LM vs. Vocab. In general, the best performing methods using LM and Vocab are comparable, except for glg, where Vocab-lang outperforms LMlang by 1 BLEU. Slk is the only language where LM outperformed Vocab in all settings, probably because it has the largest amount of data to obtain a good language model. These results show that easy-to-compute language similarity features are quite effective for data selection in low-resource languages.
Back-Translation TCS constructs Q(X|y) to sample augmented multilingual data, when the LRL data cannot estimate a good back-translation model. Here we conﬁrm this intuition by replacing the Q(X|y) in TCS with the back-translations generated by the model trained on the LRLs. To make it comparable to Bi, we use the sentence from the LRL and its most related HRL if there is one for the sampled y, but use the backtranslated sentence otherwise. Table 2 shows that for slk, back-translate achieves comparable results with the best similarity measure, mainly because slk has enough data to get a reasonable backtranslation model. However, it performs much worse for aze and bel, which have the smallest amount of data.
3.4 Effect on SDE
To ensure that our results also generalize to other models, speciﬁcally ones that are tailored for better sharing of information across languages, we also test TCS on a slightly different multilingual

Dev ppl

20 16
18 14
16 12
14 10
12
8 20000 40000 60000 80000
11
10 10

8

9

8 6
7

20000

40000 Step

60000

80000

Bi Det Sam
20000 40000 60000

25000

50000 75000 100000 Step

Dev ppl

Figure 1: Development set perplexity vs. training steps. Top left: aze. Top right: bel. Bottom left: glg. Bottom right: slk.

NMT model using soft decoupled encoding (SDE; Wang et al. (2019)), a word encoding method that assists lexical transfer for multilingual training. The results are shown in Table 3. Overall the results are stronger, but the best TCS model outperforms the baseline by 0.5 BLEU for aze, and around 2 BLEU for the rest of the three languages, suggesting the orthogonality of data selection and better multilingual training methods.
3.5 Effect on Training Curves
In Figure 1, we plot the development perplexity of all four languages during training. Compared to Bi, TCS always achieves lower development perplexity, with only slightly more training steps. Although using all languages, TCS is able to decrease the development perplexity at similar rate as Bi. This indicates that TCS is effective at sampling helpful multilingual data for training NMT models for LRLs.
4 Conclusion
We propose Target Conditioned Sampling (TCS), an efﬁcient data selection framework for multilingual data by constructing a data sampling distribution that facilitates the NMT training of LRLs. TCS brings up to 2 BLEU improvements over strong baselines with only slight increase in training time.
Acknowledgements
The authors thank Hieu Pham and Zihang Dai for helpful discussions and comments on the paper. We also thank Paul Michel, Zi-Yi Dou, and Calvin McCarter for proofreading the paper. This material is based upon work supported in part by the

Defense Advanced Research Projects Agency Information Innovation Ofﬁce (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In EMNLP.
Boxing Chen, Colin Cherry, George Foster, and Samuel Larkin. 2017. Cost weighting for neural machine translation domain adaptation. In WMT.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In ACL.
Anna Currey, Antonio Valerio Miceli Barone, and Kenneth Heaﬁeld. 2017. Copied monolingual data improves low-resource neural machine translation. In WMT.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. 2013. Adaptation data selection using neural language models: Experiments in machine translation. In ACL.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. NAACL.
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal neural machine translation for extremely low resource languages. NAACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation.
Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP.
Robert C. Moore and William D. Lewis. 2010. Intelligent selection of language model training data. In ACL.
Graham Neubig and Junjie Hu. 2018. Rapid adaptation of neural machine translation to new languages. EMNLP.

Toan Q. Nguyen and David Chiang. 2018. Transfer learning across low-resource, related languages for neural machine translation. In NAACL.
Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? NAACL.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.
Jo¨rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In LREC.
Rui Wang, Andrew Finch, Masao Utiyama, and Eiichiro Sumita. 2017. Sentence embedding for neural machine translation domain adaptation. In ACL.
Xinyi Wang, Hieu Pham, Philip Arthur, and Graham Neubig. 2019. Multilingual neural machine translation with soft decoupled encoding. In ICLR.
Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. Switchout: an efﬁcient data augmentation algorithm for neural machine translation. In EMNLP.
Marlies van der Wees, Arianna Bisazza, and Christof Monz. 2017. Dynamic data selection for neural machine translation. In EMNLP.
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low resource neural machine translation. EMNLP.

A Appendix
A.1 Model Details and Hyperparameters
• The LM similarity is calculated using a character-level LM1
• We use character n-grams with n = {1, 2, 3, 4} for Vocab similarity and SDE.
• During training, we ﬁx the language order of multilingual parallel data for each LRL, and only randomly shufﬂe the parallel sentences for each language. Therefore, we control the effect of the order of training data for all experiments.
• For TCS-S, we search over τ = {0.01, 0.02, 0.1} and pick the best model based on its performance on the development set.

1We sligtly modify the LM code from https://github.com/zihangdai/mos for our experiments.

