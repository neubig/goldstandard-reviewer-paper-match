Recent theoretical advances in decentralized distributed convex optimization
Eduard Gorbunov1,2,3, Alexander Rogozin1,2,3, Aleksandr Beznosikov1,2, Darina Dvinskikh4,1,5, Alexander Gasnikov1,5,6

arXiv:2011.13259v3 [math.OC] 29 Nov 2021

Abstract In the last few years, the theory of decentralized distributed convex optimization has made signiﬁcant progress. The lower bounds on communications rounds and oracle calls have appeared, as well as methods that reach both of these bounds. In this paper, we focus on how these results can be explained based on optimal algorithms for the non-distributed setup. In particular, we provide our recent results that have not been published yet and that could be found in details only in arXiv preprints.

1 Introduction

In this work, we focus on the following convex optimization problem

∑ 1 m

min f (x) :=

fi(x),

(1)

x∈Q⊆Rn

m i=1

where

the

functions

{

f

i

}

m i=

1

are

convex

and

Q

is

a

convex

set.

Such

kind

of

problems

arise

in

many

machine

learning

applications [138] (e.g., empirical risk minimization) and statistical applications [145] (e.g., maximum likelihood estima-

tion). To solve these problems, decentralized distributed methods are widely used (see [110, 33] and reference therein).

This direction has gained popularity with the release of the book [13]. Many researchers (among which we especially note

Angelia Nedich) have productively promoted distributed algorithms in the last 30 years. Due to the emergence of big data

and the rapid growth of problem sizes, decentralized distributed methods have gained increased interest in the last decade.

In this paper, we mainly focus on the last ﬁve years of theoretical advances, starting with the remarkable paper [8]. The
authors of [8] introduce the lower complexity bounds for communication rounds required to achieve ε-accuracy solution xN of (1) in the function value, i.e., f (xN ) − minx∈Q f (x) ≤ ε.
Let us formulate the result of [8] (see also [142, 134, 136, 87, 159]) formally. Assume that we have some connected

undirected graph (network) with m nodes. For each node i of this graph, we privately assign function fi and suppose
that the node i can calculate ∇ fi at some point x. At each communication round the nodes can communicate with their neighbors, i.e., send and receive a message with no more than O(n) numbers. In the O(R) neighborhood of a solution x∗ of (1) (where R = x0 − x∗ 2 is the Euclidean distance between starting point x0 and the solution x∗ that corresponds to the minimum of this norm), we suppose that functions fi’s are M-Lipschitz continuous (i.e., ∇ fi(x) 2 ≤ M) and L-Lipschitz
smooth (i.e., ∇ fi(y) − ∇ fi(x) 2 ≤ L y − x 2). The optimal bounds on the number of communications and the number of ora√cle calls per node are summarized in Table 1. Here and below O˜( ) means the same as O( ) up to a log(1/ε) factor, and O˜ ( χ) corresponds to the consensus time, that is the num√ber of communication rounds required to reach the consensus in the considered network (more accurate deﬁnition of O˜ ( χ) is given in Sections 2, 3).

1Moscow Institute of Physics and Technology, Russia 2Sirius University of Science and Technology, Russia 3Russian Presidential Academy of National Economy and Public Administration, Moscow, Russia 4Weierstrass Institute for Applied Analysis and Stochastics, Germany 5Institute for Information Transmission Problems RAS, Moscow, Russia 6Caucasus Mathematical Center, Adyghe State University, Russia
1

2

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

In the last few years, algorithms have been developed that reach the lower bounds from Table 1. In Section 2, we consider one of such algorithms [131, 133] for the case when the functions fi’s are smooth. This algorithm has the simplest nature among all known alternatives: this is a direct consensus-projection generalization of Nesterov’s fast gradient method.
When√communication networks vary from time to time (time-varying communication networks, see Section 2), we replace χ by χ and we suppose that different fi’s may have different constants of smoothness Li.
The non-smooth case (when functions fi’s are Lipschitz continuous) is studied in Section 3, where the results from [32, 51] are summarized. The approach is based on reformulation of the distributed decentralized problem as non-distributed convex optimization problem with afﬁne constraints, which are further brought into the target function as a composite quadratic penalty. To solve this problem, Lan’s sliding algorithm [86] can be used.

Table 1 Optimal bounds for communication rounds and deterministic oracle calls of ∇ fi per node

fi is µ-strongly convex fi is L-smooth fi is µ-strongly convex and L-smooth

# communication rounds

O

L µ

χ

O

LR2 ε

χ

O

M2 µε

χ

O

# orapcleer cnaoldlseoif ∇ fi O µL O LRε 2 O Mµε2 O

M 2 R2 ε2

χ

M 2 R2 ε2

The same construction and estimates hold (see Table 2) in the non-smooth stochastic case, when instead of subgradients

∇ fi(x)’s

we

have an

access

only

to

their unbiased estimates

∇ fi(x, ξi)’s.

We

assume here that

E

∇ f (x, ξ )

2 2

≤

M2

on

a

O(R) neighborhood of x∗.

Table 2 Optimal bounds for communication rounds and stochastic oracle calls of ∇ fi(x, ξi) per node

fi is µ-strongly convex and L-smooth

fi is L-smooth

fi is µ-strongly convex

# communication rounds

O

L µ

χ

O

LR2 ε

χ

O

M2 µε

χ

O

M 2 R2 ε2

χ

# oracle calls of ∇ fi(x, ξi) O max σ2 , L

per node i

mµε µ

O

max

σ 2R2 mε 2

,

LR2 ε

O

M2 µε

O Mε22R2

The smooth part of Table 2 describes the known lower bounds. There exist methods that are optimal only in one of the

two mentioned criteria [32]: either in communication rounds or in oracle calls per node. The technique from [133] (also

described in Section 2) combined with proper batch-size policy [36] allows to reach these lower bounds up to a logarithmic

factor [130].

Section 3 also contains analogues of the results mentioned in Tables 1 and 2 for dual (stochastic) gradient type oracle. That is, instead of an access at each node to ∇ fi we have an access to the gradient of conjugated function ∇ fi∗ [134, 159]. Such oracle appears in different applications, in particular, in Wasserstein barycenter problem [157, 37, 79, 34, 31].

In Section 4, we transfer the results mentioned above to gradient-free oracle assuming that we have an access only to fi instead of ∇ fi. In this case, a trivial solution comes to mind: to restore the gradient from ﬁnite differences. Based on

optimal gradient-type methods in smooth case, it is possible to build optimal gradient-free methods. But what is about

non-smooth case? To the best of our knowledge, until recently, it was an open question. Based on [15], we provide an

answer for this question (Section 4). To say more precisely, we transfer optimal gradient-free algorithms for non-smooth

(stochastic two-points) convex optimization problems [140, 12] from non-distributed set up to decentralized distributed

one. Here, as in Section 3, we also mainly use the penalty trick and the Lan’s sliding.

It is worth to add several results to the list of recent advances collected in Tables 1, 2. The ﬁrst result describes the

case

when

fi(x)

=

1 r

∑rj=1

fij (x)

in

(1),

Q

=

Rn.

All

fij

are

L-smooth

and

µ -strongly

convex.

In

this

case,

the

lower

bounds

were obtained in [59]. Optimal algorithms were proposed in [59, 96]. These algorithms require O˜

L µ

χ

communication

rounds and O˜ r +

r

L µ

oracle calls (∇ fij calculations) per node. This is valuable result since in real machine learning

applications the sum-type representation of fi is typical.

Another way to use this representation is statistical similarity of fi. The lower bound for communication rounds in

deterministic smooth case was also obtained [8]. Roughly speaking, if the Hessians of the fi’s are β -close in the 2-norm,

then the lower bound for communication rounds will be Ω˜

β µ

χ

. Here Ω˜ (·) is the notation for lower bounds on the

Recent theoretical advances in decentralized distributed convex optimization

3

growth

rate

hiding

logarithms.

For

example,

if

fi(x)

=

1 r

∑rj=1

fij (x)

and

all

fij

are

µ -strongly

convex

we

have

β

≃

µ

+

co√nrst .

In the decentralized distributed setup there is a gap between this lower bound and the optimal (non-accelerated) bound

O˜

β µ

χ

that can be achieved at the moment [152]. But for centralized distributed architectures with additional assumptions

on fi’s, partial acceleration is possible [60]. In the recent paper [154] the gap for decentralized optimization was closed by

using distributed Catalyst.

Other group of results relate to very speciﬁc (but rather popular) centralized federated learning architectures [68].

According to mentioned above estimates, heterogeneous federated learning can be considered as a partial case (with

χ = 1) [70, 164, 75, 53]. In the paper [75], this was explained based on the analysis of uniﬁed decentralized SGD. Paper

[75] also summarizes a lot of different distributed setups in one general approach. We partially try to use the generality

from [75] in Section 2. To the best of our knowledge, it is an open question to accelerate all the results of [75]. Section 2

contains such an acceleration only in deterministic case.

Along with minimization, distributed saddle-point problems are an interesting venue of research [106, 162, 129, 103].

The basis for distributed solution of min-max problems are extragradient and Mirror-Prox met√hods [113]. Unlike minimization, where the optimal dependence of iteration complexity on function condition number is κ, the lower complexity

obdosu.nIdnfdoercseandtdraleli-zpeodinctaaselg, othriethlomwseirnbcoluudnedsfκo,r annudmNbeerstoefrocvomacmceulneircaattiioonndsoisesOn(oκt√imχplorogv(e1/cεla)s)s[ic1a2l9n].oTnh-aecrceefolerrea,tNedesmteertohv-

acceleration technique is not needed for obtaining optimal methods for min-max problems both in classical and distributed

optimization. But in particular cases (i.e. different constants of strong convexity and strong concavity) acceleration is

possible due to distributed Catalyst [154] and [99, 45, 168, 155]. Note, also that lower bound and optimal decentralized

algorithm for saddle-point problems with variance reduction was proposed in [17] (this paper develops the results of

[59, 96]). Lower bound optimal decentralized algorithm for saddle-point problems with similarity was proposed in [20].

2 Decentralized Optimization of Smooth Convex Functions

Consider problem (1) and rewrite it in the following form:

m

∑ min F(X ) = fi(xi) s.t. x1 = . . . = xm

(2)

X ∈Rm×n

i=1

where X = (x1 . . . xm)⊤. Decentralized optimization problem is now reformulated as an optimization problem with linear constraints. The constraint set writes as C = {x1 = . . . = xm}.
Functions fi are stored on the nodes across the network, which is represented as an undirected graph G = (V, E). Every node has an access to the function and its ﬁrst-order characteristics. Decentralized ﬁrst-order methods use two types of steps – computational steps, i.e. performing local computations and communication steps, which is exchanging the information with neighbors. Alternating these two types of steps results in minimizing the objective while maintaining agents’ vectors approximately equal.
We begin with an overview of how communication procedures are developed and analyzed. The iterative information exchange is referred to as consensus or gossip algorithms in the literature [22, 156, 166, 109].

2.1 Consensus algorithms
Let each agent in the network initially hold a vector x0i and let the communication network be represented by a connected graph G = (V, E). The agents seek to ﬁnd the average vector across the network, but their communication is restricted to sending and receiving information from their direct neighbors. In one communication round, every two nodes linked by an edge exchange their vector values. After that, agent i sums the received values with predeﬁned coefﬁcients mi j, where j is the number of the corresponding neighbor. In other words, every node runs an update
xki +1 = [M]iixki + ∑ [M]i jxkj,
(i, j)∈E
where [M]i j are elements of the mixing matrix M. The update at one communication step takes the form

4

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

X k+1 = MX k.

(3)

Under additional assumptions this iterative scheme converges to the average of initial vectors over network, i.e. to

∑ X 0 = 1 11⊤X 0 = 1

m
x0.

m

m i=1 i

Assumption 1 Mixing matrix M satisﬁes the following properties.

• (Decentralized property) If (i, j) ∈/ E and i = j, then [M]i j = 0. Otherwise [M]i j > 0. • (Symmetry and double stochasticity) M1 = 1 and M = M⊤. • (Spectrum property) Denote λ2(M) the absolute value of second largest (in absolute value) eigenvalue of M. Then
λ2(M) < 1.

The choice of weights for mixing matrix is an interesting problem we do not address here (see [22] for details). A mixing matrix with Metropolis weights satisﬁes Assumption 1:

 1/(1 + max{di, d j}) [M]i j = 0
1 − ∑ [M]im
m:(i,m)∈E

if (i, j) ∈ E, if (i, j) ∈/ E, if i = j,

where di denotes the degree of node i. Several variations of Assumption 1 can be found in literature. In particular, in [101] the mixing matrix is not needed to
be symmetric. Instead, it is assumed to be doubly stochastic and have a real spectrum. Moreover, the spectrum property in Assumption 1 implies that 1 is the only (up to a scaling factor) eigenvector corresponding to eigenvalue 1, i.e. ker (I − M) = span(1).

Lemma 1. For iterative consensus procedure (3) it holds

X k − X0 ≤ (λ2(M))k X 0 − X0 .

2

2

Proof.

Let

x ∈ Rn

and

x=

m1 11⊤x.

First,

note

that

Mx

=

M

·

1 m

11⊤x

=

1 m

11⊤x

=

x.

It

can

be

easily

seen

that

x−x ∈

(span(1))⊥ and Mx − x ∈ (span(1))⊤:

x − x, 1 = Mx − x, 1 =

I − 1 11⊤ x, 1 = x, I − 1 11⊤ 1

m

m

M − 1 11⊤ x, 1 = x, M − 1 11⊤

m

m

= 0, 1 = 0.

On subspace (span(1))⊤ the largest eigenvalue of M is λ2(M). We have

Mx − x 2 = M(x − x) 2 ≤ λ2(M) x − x 2 .

Applying the derived fact to every column of X k we get Xk = X0 and X k+1 − X0 ≤ λ2(M) X k − X0 for every k ≥ 0,

2

2

which concludes the proof.

By Lemma 1, consensus scheme (3) requires O 1−λ12(M) log( ε1 ) iterations to achieve accuracy ε, i.e. to ﬁnd arithmetic

mean of vectors over the network with precision ε: X k − X0 ≤ ε.

2
Remark. If the graph changes with time, we associate a sequence of mixing matrices

Mk ∞

with it. In the time-

k=0

varying case, the consensus algorithm convergence rate is ruled by worst-case second largest eigenvalue, i.e. max λ2(Mk).

k≥0

Provided that each Mk is symmetric, doubly stochastic and satisﬁes the decentralized property in Assumption 1, the number

of communication rounds to reach consensus accuracy ε is O

1 1−max λ

(Mk )

log

1 ε

.

k≥0 2

Recent theoretical advances in decentralized distributed convex optimization

5

2.1.1 Quadratic optimization point of view

For a given undirected graph G = (V, E) introduce its Laplacian matrix

 −1, [W ]i j = deg(i),
0

if (i, j) ∈ E, if i = j, otherwise.

Laplacian matrix is positive semi-deﬁnite and for X = (x1 . . . xm)⊤ it holds W X = 0 ⇔ x1 = . . . = xm. A more detailed discussion of Laplacian matrix and its applications is provided in Section 3.3. The consensus problem can be reformulated as

min g(X) := 1 X,W X . (4)

X ∈Rm×n

2

Any matrix X∗ with equal rows is a solution of Problem (4), and therefore the set of minimizers of Problem (4) is a linear subspace of form X ∗ = 1x⊤ : x ∈ Rn . Denote λmax(W ) and λm+in(W ) the largest and the smallest non-zero eigenvalues of W , respectively. Then g(X ) is has Lipschitz gradients with constant λmax(W ) and is strongly convex on (kerW )⊥ with modulus λm+in(W ). Let non-accelerated gradient descent be run over function g(X )

X k+1 = X k − 1 W X k.

(5)

λmax(W )

First, note that trajectory of method (5) stays in X 0 + (kerW )⊥. To verify this, consider Z ∈ kerW :

W X k, Z = X k,W Z = 0 ⇒ W X k ∈ (kerW )⊥,

∑ X N+1 − X 0 = − 1

N
W X k ∈ (kerW )⊥.

λmax(W ) k=0

Method (5) converges to some point in X ∗. Since its trajectory lies in X 0 + (kerW )⊥, the limit point of X k ∞k=0 is the

projection of X 0 onto kerW , i.e. to X 0. The algorithm requires O

λmax(W ) λ + (W )

log(

1 ε

)

iterations to reach accuracy ε.

min

In order to establish the connection between gradient descent for problem (4) and consensus algorithm (3), introduce

M = I − λmaWx(W) . Matrix M satisﬁes Assumption 1, and update rule (5) then rewrites as

X k+1 = X k − W X k = I − W

X k = MX k.

λmax(W )

λmax(W )

Therefore, gradient descent on g(X ) with constant step-size λmax1(W) is equivalent to non-accelerated consensus algo-

rithm (3). Moreover, the iteration complexities coincide, since λ2(M) = 1 − λm+in(W) and therefore O

λmax(W
+

)

log(

1

)

=

λmax(W )

λmin(W )

ε

O 1−λ12(M) log( ε1 ) .

We note that the same gradient descent analogy holds for time-varying networks. Given a sequence of connected

undirected graphs G k ∞ , consider a sequence of corresponding Laplacians W k ∞ . The consensus algorithm may

k=0

k=0

k(X ) ∞ , where gk(X ) is deﬁned as

be interpreted as a gradient descent on a time-varying quadratic function g k=0

gk(X ) = 1 X ,W kX .

(6)

2

All gk(X ) have a common set of minimizers 1x⊤ : x ∈ Rd . The worst-case Lipschitz constant over time is max λmax(W k),
k≥0
and consensus iteration writes similar to (5) with λmax(W ) replaced by max λmax(W k). The convergence guarantees of gra-
k≥0
dient descent over a time varying function gk(X ) ∞k=0 do not break since the Lyapunov function for non-accelerated gradient dynamics is the squared distance to the solution set, i.e. it does not depend on the minimization objective. Therefore,

6

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

non-accelerated gradient descent decreases the Lyapunov function at each step and is robust to changes in the objective

max λmax(W k)
function. The number of communication rounds to reach consensus accuracy ε is O mk≥a0xλm+in(Wk) log ε1 . k≥0

In

order

to

obtain

a

better

dependence

on

λ

max +

(W

)

,

Nesterov

acceleration

[115]

may

be

employed.

Consider

Nesterov

λmin(W )

accelerated method for strongly convex objectives

λmax(W ) − λm+in(W )

β=

,

(7a)

λmax(W ) + λm+in(W )

Y k = X k + β (X k − X k−1),

(7b)

X k+1 = Y k − W Y k.

(7c)

λmax(W )

Analogously to non-accelerated scheme (5), the trajectory of accelerated Nesterov method lies in X 0 + (kerW )⊥. This can be easily seen by induction:

Y k − X 0 = (X k − X 0) + β ((X k − X 0) − (X k−1 − X 0)) ∈ (kerW )⊥,

X k+1 − X 0 = (Y k − Y 0) − 1

WY k ∈ (kerW )⊥.

λmax(W )

∈(kerW )⊥

∈ImW =(kerW )⊥

Therefore, accelerated scheme converges to the projection of X 0 onto kerW , which is X0, i.e. a matrix which rows are arithmetic averages of X 0.
Note that acceleration is not attainable over time-varying graphs. Imagine we run Nesterov gradient method over a time-varying objective gk(X ) ∞k=0 deﬁned in (6). The potential function for accelerated gradient dynamics [10] includes the objective function. Since the objective function changes, the potential function is also time-dependent and this fact breaks the prove of convergence result. A formal proof of why acceleration is impossible over time-varying networks that stay connected and undirected at each iteration is provided in [76].

2.1.2 Chebyshev acceleration

As

shown

in

Section

2.1.1,

the

convergence

of

consensus

algorithm

depends

on

the

condition

number

χ

=

λ

max +

(W

)

.

The

λmin(W )

factor χ also appears in upper complexity bounds for decentralized algorithms [134] and represents the measure of the

communication graph connectivity. There exists a technique called Chebyshev acceleration that enhances the dependence

on χ by replacing the communication matrix W with a Chebyshev polynomial P√K(W ). The structure of the polynomial ensures that the c√ondition number of PK(W ) is O(1) whence its power is K = ⌊ χ⌋. In other words, multiplication by PK(W ) requires ⌊ χ⌋ communication rounds, but the condition number is reduced from χ to O(1). A multiple PK(W )X is computed via an iterative consensus-based process. Introduce c2 = χχ+−11 , a0 = 1, a1 = c2, c3 = λmax(W)+2 λm+in(W ) , X 0 = X , X 1 = c2(I − c3W )X , for t = 1, . . . , K − 1 do

at+1 = 2c2at − at−1, Xt+1 = 2c2(I − c3W )Xt − Xt−1

and return X 0 − X K/aK.

√

Summing up, a consensus algorithm of form X k+1 = (I − PK(W )/λmax(PK(W )))X k requires a total of O( χ log 1/ε)

communications to achieve accuracy ε. This complexity is better than O(χ log 1/ε) that corresponds to standard consensus

algorithm X k+1 = (I −W /λmax(W ))X k. Chebyshev acceleration was used to obtain ﬁrst optimal decentralized methods in

[134].

2.1.3 Summary In this section, we covered consensus algorithms over time-static and time-varying undirected graphs.

Recent theoretical advances in decentralized distributed convex optimization

7

Firstly, let us cover the time-static networks. Different types of matrices may correspond to the graph: mixing matrix M or Laplacian matrix W . The key difference is that mixing matrix is doubly stochastic, i.e. M1 = 1 and 1⊤M = 1⊤, while the

Laplacian has a null-space property W 1 = 0. Consensus step based on mixing matrix is just multiplication by M; therefore,

it does not require additional knowledge of mixing matrix spectrum. Concerning Laplacian W , we can either build a mixing

matrix I −W /λmax(W ) or use a quadratic minimization approach (see Section 2.1.1). In both cases, knowledge of λmax(W )

is required.

Secondly, acceleration techniques are also applicable to consensus iteration schemes, but only in the time-static case.

We can use Chebyshev acceleration covered in Section 2.1.2 or employ an accelerated Nesterov problem (Section 2.1.1). In both cases, we improve the iteration complexity from O χ log 1 to

Ome√thoχdlotgo

a quadratic 1 . How-

ε

ε

ever, acceleration is not attainable in the time-varying case. When the graph changes, we can only run non-accelerated

consensus.

2.2 Main assumptions on objective functions

In this section, we introduce basic assumptions on the functions locally held by computational entities in the network. Assumption 2 For every i = 1, . . . , m, function fi is differentiable, convex and Li-smooth (Li > 0).

Assumption 3 For every i = 1, . . . , m, function fi is µi-strongly convex (µi > 0). Under Assumptions 2 and 3 for any xi, yi ∈ Rm for i = 1, . . . , m it holds
µ2i yi − xi 22 ≤ fi(yi) − fi(xi) − ∇ f (xi), yi − xi ≤ L2i yi − xi 22 . Summing the above inequality on i = 1, . . . , m we obtain

∑ ∑ mini µi Y − X 2 ≤ m µi yi − xi 2 ≤ F(Y ) − F(X ) − ∇F(X ),Y − X ≤ m Li yi − xi 2 ≤ maxi Li Y − X 2 .

2

2 i=1 2

2

i=1 2

2

2

2

On the other hand, given that X ,Y ∈ C, i.e. x1 = . . . = xm, y1 = . . . = ym, we have

∑ ∑ 1

m
µi

Y −X

2 ≤ F(Y ) − F(X ) −

∇F(X),Y − X

≤

1

m
Li Y − X 2 .

2m i=1

2

2m i=1

2

Therefore, F(X ) has different strong convexity and smoothness constants on Rm×n and C. Following the deﬁnitions in [134], we introduce

• (local constants) F(X ) is µl-strongly convex and Ll-smooth on Rm×d, where µl = min µi, Ll = max Li.

i

i

•

(global

constants)

F (X )

is

µg-strongly

convex

and

Lg-smooth

on

C,

where

µg

=

1 m

∑mi=1

µi,

Lg

=

1 m

∑mi=1 Li.

Note that local smoothness and convexity constants may be signiﬁcantly worse then global, i.e. Ll ≫ Lg, µl ≪ µg (see [134] for details). We denote

κl = Ll , κg = Lg .

(8)

µl

µg

the local and global condition numbers, respectively. A trick proposed in [134] allows to improve the local condition

number by slightly changing functions fi. Namely, introduce fi(x) = fi(x) − µi−2µg

x

2 2

instead

of

fi.

Then

the

local

condition number writes as

κl = maxk(Lk − µk) + 1. µg

8
2.3 Distributed gradient descent

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

Distributed gradient methods alternate taking optimization updates and information exchange steps. One (synchronized) communication round can be represented as a multiplication by a mixing matrix compatible with the graph topology. One of the ﬁrst distributed gradient dynamics studied in the literature [112, 172] uses a time-static mixing matrix and writes as

∑m
xki +1 = [M]i jxkj − αk∇ fi(xki ).
j=1

Using the notion of X = (x1 . . . xm)⊤ the above update rule takes the form

X k+1 = MX k − α∇F(X k),

(9)

which is a combination of two step types: gradient step with constant step-size α and communication round with mixing matrix Mk. In [172] the authors showed that function residual f (X k) − f (x∗) in iterative scheme (9) decreases at O(1/k) rate until reaching O(α)-neighborhood of solution.
Method (9) does not ﬁnd an exact solution in the general case. We follow the arguments in [142] to illustrate this fact. First, note that X˜ is a solution of (2) if and only if two following conditions hold.
1. (Consensus) X˜ = MX˜ . 2. (Optimality) 1⊤∇F(X˜ ) = 0.
Let X ∞ be a limit point of (9). Then

X ∞ = MX ∞ − α∇F(X ∞).

Consensus condition yields X ∞ = MX ∞, i.e. X ∞ has identical rows [X ]∞i = x∞. Therefore, ∇F(X ∞) = 0, which means ∇ f1(x∞) = . . . = ∇ fm(x∞) = 0. Consequently, x∞ is a common minimizer of every fi, which is not a realistic case.
Method (9) is a basic distributed ﬁrst-order method. Its different variations include feasible point algorithms [92] and sub-gradient methods [112] (actually, the latter work initially proposed scheme (9)). Extensions to stochastic objectives and stochastic mixing matrices have been addressed in [75, 2, 100].

2.4 EXTRA

Distributed gradient descent (9) is unable to converge to the exact minimum of (2), which is the major drawback of the method. An exact decentralized ﬁrst-order algorithm EXTRA was proposed in [142]. The approach of [142] is based on using two different mixing matrices. Namely, consider two consequent updates of type (9).

X k+2 = MX k+1 − α∇F(X k+1),

(10)

X k+1 = M˜ X k − α∇F(X k)

(11)

where M˜ is mixing matrix, i.e. M˜ = (M + I)/2 as proposed in [142]. Subtracting (11) from (10) yields

X k+2 − X k+1 = MX k+1 − M˜ X k − α ∇F(X k+1) − ∇F(X k)

(12)

thus leading to an algorithm

Algorithm 1 EXTRA
Input: Step-size α > 0. X1 = MX0 − α∇F(X0) for k = 0, 1, . . . do Xk+2 = (I + M)Xk+1 − M˜ Xk − α
end for

∇F (Xk+1) − ∇F(Xk)

Let X ∞ be a limit point of iterate sequence X k ∞k=0 generated by (10), (11). Then

Recent theoretical advances in decentralized distributed convex optimization

9

X ∞ − X ∞ = MX ∞ − M˜ X ∞ − α [∇F(X ∞) − ∇F(X ∞)] , (M − M˜ )X ∞ = 1 (MX ∞ − X ∞) = 0.
2
The last equality means that X∞ is consensual, i.e. its rows are equal. On the other hand, rearranging the terms in (12) and taking into account that X 1 = MX 0 − α∇F(X 0) gives

∑k+1
X k+2 = M˜ X k+1 − α∇F(X k+1) + (M − M˜ )Xt .
t=0
Multiplying by 1⊤ from the left yields

∑k−1
1⊤X k+2 = 1⊤M˜ X k+1 − α1⊤∇F(X k+1) + 1⊤(M − M˜ )Xt
t=0
= 1⊤M˜ X k+1 − α1⊤∇F(X k+1)

and taking the limit over k → ∞ we obtain
1⊤∇F(X ∞) = 0
which is the optimality condition for point X ∞. Therefore, a limit point of X k ∞k=0 generated by Algorithm 1 is both consensual and optimal, i.e. is a solution of (2).
In the original paper [142] Algorithm 1 was proved to converge at a O(1/k) rate for L-smooth objectives and achieve a geometric rate O(C−k) (where C < 1 is some constant) for strongly convex smooth objectives. In [94] explicit dependencies on graph topology were established. Namely, EXTRA requires

O

Ll + χ

(LR2 + M˜ 2/L)χ log

µl

ε

O Ll + χ log((LR2 + M˜ 2/L)χ) ε

iterations for strongly convex smooth objectives, iterations for (non-strongly) convex smooth objectives,

where

χ= 1 ,

(13)

1 − λ2(M)

λ2(M) denotes the second largest eigenvalue of mixing matrix M,

X 0 − X ∗ 22 ≤ mR2,

X∗

2 2

≤

mR2

,

∇ f (X ∗)

2 2

≤

mM˜ 2.

The term χ characterizes graph connectivity. A similar term, also referred to as graph condition number, is used in Section

3.3 for graph Laplacian matrix. Graph condition numbers based on mixing matrix and Laplacian have the same meaning,

as discussed in Section 2.1.

2.5 Accelerated decentralized algorithms
Performance of decentralized gradient methods typically depends on function (local or global) condition number κ and graph condition number χ d√eﬁned in√(13). For non-accelerated dynamics [172, 142] complexity bounds include κ and χ. Improving dependencies to κ and χ is an important direction of research in distributed optimization. This can be done by applying direct Nesterov acceleration [115] or by employing meta-acceleration techniques such as Catalyst [98]. The two major approaches studied in the literature are primal and dual algorithms.
Dual methods are based on a reformulation of problem (1) using a Laplacian matrix induced by the communication network. This reformulation is discussed in Section 3.3 in more details. The basic idea behind dual approach is to run ﬁrstorder methods on a dual problem to (2). Every gradient step on the dual is equivalent to one communication round and one√local gradient step taken by every node in the network. In [134], algorithms using Chebyshev acceleration that achieve mOethoκdl χs olovge(r1s/trεo)ngcloymcomnuvnexicsamtioonotchomobpjleecxtiivtyesaries pΩrop√osκegdχ. lOong(t1h/eεo)th,earshsahnodw, lnowiner[1c3o4m].plexity bound for deterministic

10

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

bouInnddsu, ai.lea.p√prκoχac. hF,oornperimmaayl-rounnlynomne-tdhiostdrisbtuhtiesdisacncoetlethraetecdassec,haenmdepsroimn adluaallgporroitbhlmems hanavdeotbotaailntearcncaeteleoraptteimd cizoamtipolnexaintdy consensus steps in a proper way and employ speciﬁc techniques such as gradient tracking. A direct distributed scheme for Nesterov accelerated method was proposed in [125].

Algorithm 2 Accelerated Distributed Nesterov Method
Input: Starting points X0 = Y 0 = V 0, S0 = ∇F(X0), step-size η > 0, momentum term α = √µlη

for k = 0, 1, . . . do

Xk+1 = MY k − η Sk

V

k+1

=

(1

−

α )MV

k

+

α MY

k

−

η α

Sk

Y k+1

=

X k+1+αV k+1 1+α

Sk+1 = MSk + ∇F(Y k+1) − ∇F(Y k)

end for

In Algorithm 2 quantity Sk+1 stands for a gradient estimator. The information about the gradients held by different agents is diffused through the network via consensus steps, i.e. MSk multiplication. Every node stores one row of Sk which
approximates the average gradient over the nodes in network:

∑ sk ≈ 1

m
∇ fi(yk).

i m k=1

i

This technique is referred to as gradient tracking and is employed in several primal decentralized methods [125, 111, 169,
74, 3, 124, 123]. Algorithm 2 requires O(χ3/2κl5/7 log(1/ε)) computation and communication steps √to achieve accuracy ε, which does
not match optimal bounds. EXTRA acceleration via Catalyst envelope [94] requires O( κl χ log χ log(1/ε)) iterations for smooth strongly convex objectives. Recently a new method Mudag which uniﬁes gradient tracking, Nesterov acceleration
and multi-step consensus procedures was proposed in [169]. It has

√

1

O κglog ε

O √κgχ log Ll κg log 1

Lg

ε

computation complexity and communication complexity.

Mudag reaches optimal computation complexity and optimal communication complexity up to log LLgl κg term. A valuable feature of the method is that it has dependencies on global condition number κl instead of local κg. In the general case, global condition number may be signiﬁcantly better. A proxim√al version of Mudag method for composite optimization is studied in [170]. The method in [170] requires an optimal O( κgχ log(1/ε)) number of computations and matches
the lower communication complexity bound up to a logarithmic factor. Global condition number is also utilized in paper

[133] where an inexact oracle framework [28, 29] for decentralized optimization is studied. The latter work is discussed

in more details in Section 2.7. Finally, in [77] authors proposed a primal-only method OPAPC which reaches both optimal computation and communication complexities (up to replacing κg with κl).
Chebyshev acceleration is widely used to obtain optimal decentralized algorithms. For example, in [77] the authors

propose method APAPC, which has O

L µ

χ

+

χ

log

1 ε

communication and computational complexities. After that,

the authors replace Laplacia√n W with a Chebyshev polynomial PK(W ), which results in χ(PK(W )) = O(1), but every communication round costs O( χ) communication rounds. Therefore, APAPC is modiﬁed to a new method OPAPC, which

has O

L µ

log

1 ε

oracle per node complexity and O

L µ

χ

log

1 ε

communication complexity. In this particular case,

Chebyshev acceleration not only allows to achieve optimal complexity bounds, but also separates oracle and communica-

tion complexities of the algorithm.

Paper [143] proposed OGT, a method based on loopless Chebyshev acceleration scheme. On the contrary to classical

Chebyshev acceleration (used i.e. in OPAPC [77]), the loopless technique does not require multiple communication steps

at each iteration. OGT requires O

L µ

log

1 ε

oracle calls at each node and O

L µ

χ

log

1 ε

communication steps, which

meets the lower bounds.

Moreover, a recent work [144] showed that Nesterov acceleration can also√be applied for distributed optimization over directed graphs. Their algorithm APD has communication complexity ∼ 1/ ε for non-strongly convex objectives and

Recent theoretical advances in decentralized distributed convex optimization

11

APD-SC has ∼ L/µ log(1/ε) complexity for strongly convex tasks. As stated in [143], in the case of undirected graphs

the explicit dependence on network characteristics is attained: the complexity of APD-SC writes as O

L µ

χ

3/2

log

1 ε

.

2.6 Time-varying networks
In the time-varying case, the communication network changes from time to time. In practice this changes are typically caused by malfunctions such as loss of connection between the agents. The network is represented as a sequence of undirected communication graphs G k = (V, Ek) ∞k=0, and every graph G k is associated with a mixing matrix Mk. The algorithms capable of working over time-varying graphs must be robust to sudden network changes. A linearly convergent method DIGing was proposed in [111].
Algorithm 3 DIGing
Input: Step-size α > 0, starting iterate X0, Y 0 = ∇F(X0) for k = 0, 1, . . . do Xk+1 = MkXk − αY k Y k+1 = MkY k + ∇F(Xk+1) − ∇F (Xk) end for

DIGing incorporates a gradient-tracking scheme and achieves linear convergence under realistic assumptions such as

B-connectivity (i.e. a union of any B consequent graphs is connected). In [151] authors propose an algorithm which utilizes

speciﬁc convex surrogates of local functions and local functions similarity in order to enhance convergence speed. In [95]

a gradient-tracking technique combined with Nesterov acceleration was employed to construct an accelerated method

AccGT over time-varying B-connected networks.

Another class of time-varying networks are graphs that stay connected at each iteration. For this type of problems,

denote Laplacian at k-th iteration W k and deﬁne condition number χtw = maxk λmax(Wk) . The lower bounds for this class of

√

mink λm+√in(W k)

problems are O( κl log(1/ε)) for the number of (local) computations and O(χ κl log(1/ε)) for the number of commu-

nications [76]. AccGT [95] and ADOM+ [76] are optimal algorithms using primal oracle, and ADOM [78] is an optimal

dual method.

2.7 Inexact oracle point of view

In [133] the authors study an algorithm which alternates making gradient updates and running multi-step communication procedures. Introduce

X

=

1 11⊤X

= ΠC(X ) = (x . . . x)⊤,

where x =

1

m
∑ xi

and 1 = (1 . . . 1)⊤.

m

m i=1

Also deﬁne an average gradient over nodes ∇F(X ) = 1/m ∑mi=1 ∇ fi(xi). Consider a projection gradient method with trajectory lying in C

Xk+1 = Xk − β ∇F(X k).

In a centralized scenario, the computational network is endowed with a master agent, which communicates with all agents in the network. The master node is able to collect vectors xi from every node in the network and compute a precise average x. In decentralized case the master agent is not available, and therefore nodes are only able to compute an approximate average using consensus procedures. The network is allowed to change with time and the sequence of corresponding mixing matrices is restricted to the following assumption.
Assumption 4 Mixing matrix sequence Mk ∞k=0 satisﬁes the following properties.
• (Decentralized property) (i, j) ∈/ Ek ⇒ [Mk]i j = 0.

12

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

• (Double stochasticity) Mk1 = 1, 1⊤Mk = 1⊤. • (Contraction property) There exist τ ∈ Z++ and λ ∈ (0, 1) such that for every k ≥ τ − 1 it holds

where Mτk = Mk . . . Mk−τ+1.

Mτk X − X ≤ (1 − λ ) X − X 2 ,
2

Algorithm 4 Consensus
Input: Initial X0 ∈ C, number of iterations T . for t = 1, . . ., T do Xt+1 = Mt Xt end for

Algorithm 5 Decentralized AGD with consensus subroutine

Input: Initial guess X0 ∈ C, constants L, µ > 0, U0 = X0, α0 = M0 = 0

1: for k = 0, 1, 2, . . . do

2: Find αk+1 as the greater root of (Ak + αk+1)(1 + Ak µ) = L(αk+1)2

3: Ak+1 = Ak + α k+1

4: Y k+1 = α k+1Uk + AkXk

Ak+1

5: V k+1 = µY k+1 + (1 + Ak µ )Uk − α k+1 ∇F(Y k+1)

1 + Akµ + µ

1 + Akµ + µ

6: Uk+1 = Consensus(V k+1, T k)

7: Xk+1 = α k+1Uk+1 + AkXk Ak+1
8: end for

2.7.1 Inexact oracle construction

Trajectory of Algorithm 5 lies in the neighborhood of constraint set C. It is analyzed in [133] (based on the technique
developed in [131]) using the notation of inexact oracle [29, 28]. Algorithms of this type have been analyzed in timestatic case [63] using inexact oracle notation, as well. Let h(x) be a convex function deﬁned on a convex set Q ⊆ Rm. For δ > 0, L > µ > 0, a pair (hδ,L,µ (x), sδ,L,µ (x)) is called a (δ , L, µ)-model of h(x) at point x ∈ Q if for all y ∈ Q it holds

µ

y−x

2 2

≤

h(y)

−

hδ ,L,µ (x) +

sδ ,L,µ (x), y − x

≤ L y − x 22 + δ .

(14)

2

2

The inexactness originates from computation of gradient at a point in neighborhood of C. The next lemma identiﬁes the size of neighborhood and describes the inexact oracle construction.

Lemma 2. Deﬁne

δ = 1 L2l + 2L2l + L − µ δ ′,

(15)

2n Lg µg l l

fδ,L,µ (x, X ) = 1n F(X ) + ∇F(X ), X − X + 12 µl − 2µLg2l X − X 22 ,

∑ 1 n
gδ ,L,µ (x, X ) = n ∇ fi(xi).
i=1

Then ( fδ,L,µ (x, X ), gδ,L,µ (x, X )) is a (δ , 2Lg, µg/2)-model of f at point x, i.e.

µ4g y − x 22 ≤ f (y) − fδ ,L,µ (x, X ) − gδ ,L,µ (x, X ), y − x ≤ Lg y − x 22 + δ .

Recent theoretical advances in decentralized distributed convex optimization

13

The inexact oracle deﬁned in Lemma 2 represents a (δ , 2Lg, µg/2)-model of F. Note that it uses global strong convexity
constants instead of local ones. Global constants may be signiﬁcantly better for method performance, as pointed out in [134]. The lemma relates the projection accuracy δ ′ to inexact oracle parameter δ .

2.7.2 Convergence result for Algorithm 5

First, Lemma 2 states that (δ , 2Lg, µg/2)-model of F is obtained if the gradient is computed in δ ′-neighborhood of C. In order to achieve this δ ′-neighborhood, one needs to make a sufﬁcient number of consensus (Algorithm 4) iterations. Lemma 3. Let consensus accuracy be maintained at level δ ′, i.e. U j − U j 2 ≤ δ ′ for j = 1, . . . , k and let Assumption 4
2
hold. Deﬁne

√ D :=

√2Ll

+1

√ δ

′

+

Ll

√ n

Lµ

µ

u0 − x∗

2 8δ ′ +√

1/2 2 +

∇F(X ∗) √

2.

2 Lµ

Lµ

Then

it

is

sufﬁcient to

make

Tk

=T

=

τ 2λ

log

D δ′

consensus iterations (where

τ

and

λ

are

deﬁned in

Assumption

4)

in

order to obtain consensus with δ ′-accuracy on step k + 1, i.e. U k+1 − Uk+1 2 ≤ δ ′.

2

A basis for the proof of Lemma 3 is a contraction property of mixing matrix sequence Mk ∞ (see Assumption 4).

Second,

provided

that

projection

accuracy

on

every

step

of

Algorithm

5

is

sustained

at

k=0
level δ

′

,

the

algorithm

turns

into

an accelerated scheme with inexactness. Its convergence rate is given by the following

Lemma 4. Provided that consensus accuracy is δ ′, i.e.

U j −U j

2
≤ δ ′ for j = 1, . . . , k, we have

2

f (xk) − f (x∗) ≤

uk − x∗

2
≤

2

u0 − x∗ 22 2 ∑kj=1 A jδ 2Ak + Ak
u0 − x∗ 22 4 ∑kj=1 A jδ 1 + Akµ + 1 + Akµ

where δ is given in (15). Finally, putting Lemmas 3 and 4 together yields a convergence result for Algorithm 5.
Theorem 5. Recall the deﬁnitions of τ and λ from Assumption 2, choose some ε > 0 and set

T = T = τ log D , δ ′ = nε µg3/2 . k 2λ δ ′ 32 L1g/2L2l

Also deﬁne

D1 = Ll L1g/2 µg
D2 = Ll L1g/2 µg

√

Lg 3/4 4√2 ∇F(X ∗)

8 2Ll u0 − x∗ 2 µg +

√

2

n

√

√ Lg 1/4

3 µg + 4 2n µg

.

Lg 1/4

µg

,

Then Algorithm 5 requires

Lg

u0 − x∗ 22

N = 2 µg log 2Lgε

(17)

gradient computations at each node and

N = N · T = 2 Lg τ · log 2Lg u0 − x∗ 22 log √D1 + D

(18)

tot

µg λ

ε

ε2

14

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

communication steps to yield X N such that

f (xN) − f (x∗) ≤ ε, X N − XN 2 ≤ δ ′.
2
In the time-static case, contraction term τ/λ turns into χ(M), and an accelerated consensus procedure of type (7) may be employed. This results in a better dependence on graph connectivity and leads to a complexity bound O Lµgg χ(M) log2( ε1 ) which is optimal up to a logarithmic term. Similar results are attained in works which use penalty-based methods [93, 132, 51] (see Appendix B in [51]) for details.
Remark. The analysis of Algorithm 5 presented in [130] results in constants Lg, µg in the complexity bound. These constants are better than local constants Ll, µl, but still can be improved. The inexact oracle concept allows to reduce decentralized optimization problem to minimization of f (x) over Rd with inexact oracle. Therefore, the complexity will depend on constants L f , µ f which characterize f itself, not its ﬂattened variant F. An accurate analysis on this issue is presented in Section 2.8.

2.7.3 Stochastic decentralized optimization

The technique used in Algorithm 5 can be extended to stochastic objectives. Following the deﬁnitions in [130], let
fi(x) := Eξ fi(x, ξi), where ξi’s are random variables. Variables ξi represent the source of stochasticity in fi(x, ξi) which may be caused by random sampling or stochastic noise. For each i = 1, . . . , n we assume that ∇ fi(x, ξi) is Li(ξ ) continuous

and there exists a constant Li ≥ 0 such that Eξi Li(ξi)2 ≤ Li < +∞. Under these assumptions fi is Li-smooth. We also

bound the variance of ∇ fi(x, ξi):

Eξi [ ∇ fi(x, ξi) − ∇ fi(x) 22] σi2.

m

Let

us

deﬁne

σg2

=

1 m

∑

σi2.

The

algorithm

in

[130]

combines

a

consensus

subroutine

technique

similar

to

Algorithm

5

i=1

and also uses a speciﬁc batch-size policy. In order to analyze the method, inexact oracle framework similar to that of

Section 2.7 is used. The inexactness of gradient has two sources: inexact projection onto the constraint set via consensus

subroutine and stochastic noise. On the one hand, tuning the batch size allows to reduce the variance of the batched

gradient at the cost of additional stochastic oracle calls. Therefore, proper batch size guarantees a balance between the

stochastic gradient noise and the number of gradient calculations. On the other hand, the accuracy of the consensus is

tuned by the choice of number of consensus iterations. Choosing a proper batch size and number of consensus iterations

allows to obtain optimal complexities both in the number of computations and communications up to a logarithmic factor.

Namely, the method in [130] requires O max σg2 , Lg log 1

nµgε

µg

ε

oracle calls per node. In the time-varying case, it

requires O λτ Lµgg communication rounds (where τ and λ are deﬁned in Assumption 4), and in time-static case its communication complexity takes the form O Lµgg χ and is achieved by using Chebyshev acceleration.

2.8 Decentralized Saddle-Point Problems

Along with minimization problems, sum-type min-max problems of type

∑ 1 m

min max f (x, y) :=

fi(x, y)

x∈X y∈Y

m i=1

where X and Y are convex compacts, can be solved in a decentralized manner, as well. The same way as in Assumptions 2, 3 for minimization tasks, we introduce assumptions for min-max problems.

Assumption 6 For every i = 1, . . . , m, function fi is differentiable, convex in x, concave in y and Li-smooth.

Assumption 7 Function f is µ-strongly-convex in x, µ f -strongly-concave in y (µ f > 0) and L f -smooth.

Saddle-point problems have many practical applications: classical and well-studied in economy and in game theory [161, 40], and modern in imaging denoising [24], in adversarial training [9, 50], and in statistical learning [1]. But dis-

Recent theoretical advances in decentralized distributed convex optimization

15

tributed saddle-point problems is not as widely studied in the literature as the minimization problems. Let us highlighted the main works devoted to decentralized min-max problems. Most of the works are devoted to decentralized algorithms on ﬁxed graph topology. In paper [20], the authors present lower bounds for deterministic decentralized saddle point problems under Assumptions 6 and 7. These estimates are as follows

Ω L f log (1/ε) µf

computation complexity and

Ω √χ L f log (1/ε)

communication complexity.

(19)

µf

Additionally, the paper provides an optimal algorithm (up to logarithmic factors), which achieves the lower bounds. Among the disadvantages of the Algorithm presented in [20], one can single out multiple gossip steps, this approach is unstable and not a popular in practice. A similar Algorithm with multiple gossip steps is proposed in [102], but they consider convergence in the non-convex case (under the minty condition [107, 66]). Also, this work shows the effectiveness of decentralized training of GANs. [104] is also devoted to minty non-convex saddle-point problems. It is also interesting to note the work [20] on saddle-point problems in terms of data-similarity. The work gives lower bounds for communication complexity, as well as optimal algorithms for such setting of the problem. In particular, the lower and upper bounds state that

√χ 1 + δ log (1/ε) µf

communication rounds

are enough to achiev√e ε-precision. Interesting to note, that for uniformly distributed data, with high probability Li ≈ L f and δ ∼ O˜(maxi Li/ n), where n – the number of local samples on each node. This means that the data-simularity bounds on communication rounds is signiﬁcantly better than the general one (19).
It is also important to mention the works devoted to saddle-point problems on time-varying networks. In particular, paper [18] is devoted to lower bounds and optimal algorithms for connected topology. Work [14] is devoted to the broader case of time-varying networks, for example, methods can do local steps (iterations without communication).
An interesting variant of saddle-point problems are extensions to local and global variables, i.e. problems of the form

∑ 1 m

min max

fi(xi, p, yi, r).

p,{xi}mi=1 r,{yi}mi=1 m i=1

(20)

Applications of problems of this type minimization tasks with separable and semi-deﬁnite constraints [106], decentralized
reinforcement learning [162] and distributed computation of Wassers√tein barycenters [129, 35]. A subgradient method for problems of type (20) was proposed in [106]. The method has a O(1/ N) convergence rate. A recent work [129] proposed a method based on Mirror-Prox, capable of working in general proximal setup, reaching a O(1/N) convergence rate and an accelerated rate on χ. The method achieves optimal oracle and communication complexities in Euclidean convex-concave case over time-static graphs.

3 Convex Problems with Afﬁne Constraints

In this section1, we consider convex optimization problem with afﬁne constraints

where A

min f (x),

(21)

Ax=0,x∈Q

0, KerA = {0} and Q is a closed convex subset of Rn. Up to a sign, the dual problem is deﬁned as follows:

min ψ(y), where

(22)

y

ϕ(y) = max { y, x − f (x)} ,

(23)

x∈Q

ψ(y) = ϕ(A⊤y) = max { y, Ax − f (x)} = A⊤y, x(A⊤y) − f (x(A⊤y)),

(24)

x∈Q

1 The narrative in this section follows [51].

16

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

where x(y) d=ef argmaxx∈Q { y, x − f (x)}. Since KerA = {0} the solution of the dual problem (22) is not unique. We use y∗ to denote the solution of (22) with the smallest ℓ2-norm Ry d=ef y∗ 2.

3.1 Primal Approach

In this section, we focus on primal approaches to solve (21) and, in particular, the main goal of this section is to present ﬁrst-order methods that are optimal both in terms of ∇ f (x) and A⊤Ax calculations. One can apply the following trick
[32, 44, 51] to solve problem (21): instead of (21) one can solve penalized problem

min F(x) = f (x) + R2y Ax 2 ,

(25)

x∈Q

ε

2

where ε > 0 is the desired accuracy of the solution in terms of f (x) that we want to achieve (see the details in [51]). Next, we assume that f is µ-strongly convex, but possibly non-smooth function with bounded (sub) gradients:
∇ f (x) 2 ≤ M for all x ∈ Q. In this setting, one can apply Sliding algorithm from [85, 84] to get optimal rates of convergence. The method is presented as Algorithm 6 and it is aimed to solve the following problem:

min {Ψ (x) = h(x) + f (x)} ,

(26)

x∈Q

where h(x) is convex and L-smooth, f (x) is convex, but can be non-smooth, and x∗ is an arbitrary solution of the problem.
In this case, it is additionally assumed that f (x) has uniformly bounded subgradients: there exists non-negative constant M such that2 ∇ f (x) 2 ≤ M for all x ∈ Q and all subgradients at this point ∇ f (x) ∈ ∂ f (x).

Algorithm 6 Sliding Algorithm [85, 84]
Input: Initial point x0 ∈ Q and iteration limit N. Let βk ∈ R++, γk ∈ R+, and Tk ∈ N, k = 1, 2, . . ., be given and set x0 = x0. for k = 1, 2, . . ., N do
1. Set xk = (1 − γk)xk−1 + γkxk−1, and let hk(·) ≡ lh(xk, ·), where lh(x, y) = h(x) + ∇h(x), y − x . 2. Set
(xk, x˜k) = PS(hk, xk−1, βk, Tk).
3. Set xk = (1 − γk)xk−1 + γkx˜k. end for
Output: xN .

The PS (prox-sliding) procedure. procedure: (x+, x˜+) = PS(g, x, β , T )
Let the parameters pt ∈ R++ and θt ∈ [0, 1], t = 1, . . ., be given. Set u0 = u˜0 = x. for t = 1, 2, . . ., T do

ut = argmin

g(u) + l f (ut−1, u) + β

u − x 2 + β pt
2

u − ut−1

2 2

,

u∈Q

2

2

u˜t = (1 − θt )u˜t−1 + θt ut ,

where l f (x, y) = f (x) + ∇ f (x), y − x .
end for Set x+ = uT and x˜+ = u˜T .
end procedure:

The key property of Algorithm 6 is its ability to separate oracle complexities for smooth and non-smooth parts of the objective. That is, to ﬁnd such xˆ that Ψ (xˆ) −Ψ(x∗) ≤ ε Sliding requires

LR2

O

ε calculations of ∇h(x)

2 For the sake of simplicity, we slightly abuse the notation and denote gradients and subgradients similarly.

Recent theoretical advances in decentralized distributed convex optimization

17

and

M2R2

LR2

O ε2 + ε calculations of ∇ f (x),

where R = x0 − x∗ 2. Now, we go back to the problem (25) and consider the case when µ = 0. In these settings, to ﬁnd xˆ such that

F(xˆ) − F(x∗) ≤ ε

(27)

one can run Algorithm 6 considering f (x) as the non-smooth term and R2y/ε

Ax

2 2

as

the

smooth

one.

In

this

case,

Sliding

requires





O

λ

m

ax

(A

⊤

A

)R

2 y

R

2



calculations

of

A⊤Ax,

(28)

ε2

M2R2

O ε2 calculations of ∇ f (x).

(29)

Next, we consider the situation when Q is a compact set, ∇ f (x) is not available, and unbiased stochastic gradient ∇ f (x, ξ ) is used instead:

Eξ [∇ f (x, ξ )] − ∇ f (x) 2 ≤ δ ,

(30)

∇ f (x, ξ ) − Eξ [∇ f (x, ξ )] 2

Eξ exp

σ 2 2 ≤ exp(1),

(31)

where δ ≥ 0 and σ ≥ 0. When δ = 0, i.e., stochastic gradients are unbiased, one can show [85, 84] that Stochastic
Sliding (S-Sliding) method can achieve (27) with probability at least 1 − β , β ∈ (0, 1) requiring the same number of calculations of A⊤Ax as in (28) up to logarithmic factors and

(M2 + σ 2)R2

O

ε2

calculations of ∇ f (x, ξ ).

(32)

When µ > 0 one can apply restarts technique for S-Sliding and get the method (RS-Sliding) [32, 158] that guarantees (27) with probability at least 1 − β , β ∈ (0, 1) using





O

λ

m

ax

(A

⊤

A

)R

2 y



calculations

of

A⊤Ax,

(33)

µε

M2 + σ2

O µε calculations of ∇ f (x, ξ ).

(34)

We notice that bounds presented above for the non-smooth case are proved when Q is bounded. For the case of unbounded Q the convergence results with such rates were established only in expectation. Moreover, it would be interesting to study S-Sliding and RS-Sliding in the case when δ > 0, i.e., stochastic gradient is biased.

3.2 Dual Approach

In this section, we assume that one can construct a dual problem for (21). If f is µ-strongly convex in ℓ2-norm, then
ψ and ϕ have Lψ –Lipschitz continuous and Lϕ –Lipschitz continuous in ℓ2-norm gradients respectively [69, 128], where Lψ = λmax(A⊤A)/µ and Lϕ = 1/µ. In our proofs, we often use Demyanov–Danskin theorem [128] which states that

∇ψ(y) = Ax(A⊤y), ∇ϕ(y) = x(y).

(35)

Moreover, we do not assume that A is symmetric or positive semideﬁnite.

18

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

Below we propose a primal-dual method for the case when f is additionally Lipschitz continuous on some ball and
two methods for the problems when the primal function is also L-smooth and Lipschitz continuous on some ball. In the subsections below, we assume that Q = Rn. The formal proofs of the presented results are given in [51].

3.2.1 Convex Dual Function

In this section, we assume that the dual function ϕ(y) could be rewritten as an expectation, i.e., ϕ(y) = Eξ [ϕ(y, ξ )], where stochastic realizations ϕ(y, ξ ) are differentiable in y functions almost surely in ξ . Then, we can also represent ψ(y)
as an expectation: ψ(y) = Eξ [ψ(y, ξ )]. Consider the stochastic function f (x, ξ ) which is deﬁned implicitly as follows:

ϕ(y, ξ ) = max { y, x − f (x, ξ )} .

(36)

x∈Rn

Similarly to the deterministic case, we introduce x(y, ξ ) d=ef argmaxx∈Rn { y, x − f (x, ξ )} which satisﬁes ∇ϕ(y, ξ ) = x(y, ξ ) due to Demyanov–Danskin theorem, where the gradient is taken w.r.t. y. As a simple corollary, we get ∇ψ(y, ξ ) = Ax(A⊤y).
Finally, introduced notations and obtained relations imply that x(y) = Eξ [x(y, ξ )] and ∇ψ(y) = Eξ [∇ψ(y, ξ )]. Consider the situation when x(y, ξ ) is known only through the noisy observations x˜(y, ξ ) = x(y, ξ ) + δ (y, ξ ) and assume
that the noise is bounded in expectation, i.e., there exists non-negative deterministic constant δy ≥ 0, such that

Eξ [δ (y, ξ )] 2 ≤ δy, ∀y ∈ Rn.

(37)

Assume additionally that x˜(y, ξ ) satisﬁes so-called “light-tails” inequality:

x˜(y, ξ ) − Eξ [x˜(y, ξ )] 22

n

Eξ exp

σ2

≤ exp(1), ∀y ∈ R ,

(38)

x

where σx is some positive constant. It implies that we have an access to the biased stochastic gradient ∇˜ ψ(y, ξ ) d=ef Ax˜(y, ξ ) which satisﬁes following relations:

Eξ ∇˜ ψ(y, ξ ) − ∇ψ(y) 2 ≤ δ , ∀y ∈ Rn,

(39)





E exp  ∇˜ ψ(y, ξ ) − Eξ ∇˜ ψ(y, ξ )

2
2  ≤ exp(1),

∀y ∈ Rd,

(40)

ξ

σψ2

where δ d=ef λmax(A⊤A)δy and σψ d=ef λmax(A⊤A)σx. We will use ∇˜ Ψ (y, ξ k) to denote batched stochastic gradient:

∑ ∑ ∇˜ Ψ (y, ξ k) =

1

rk
∇˜ ψ(y, ξ l),

x˜(y, ξ k) =

1

rk
x˜(y, ξ l)

(41)

rk l=1

rk l=1

The size of the batch rk could always be restored from the context, so, we do not specify it here. Note that the batch version satisﬁes (see the details in [51])

E ∇˜ Ψ (x, ξ k) − ∇ψ(x) ≤ δ , ∀x ∈ Rn,

(42)



2

∇˜ Ψ (x, ξ k) − E ∇˜ Ψ (x, ξ k) 2

exp 

2  ≤ exp(1), ∀x ∈ Rn,

(43)

E O(σψ2/rk2)

In these settings, we consider a method called SPDSTM (Stochastic Primal-Dual Similar Triangles Method, see Algorithm 7). Note that Algorithm 4 from [34] is a special case of SPDSTM when δ = 0, i.e., stochastic gradient is unbiased, up to a factor 2 in the choice of L˜ .
Below we present the main convergence result of this section.
Theorem 8 (Theorem 5.1 from [51]). Assume that f is µ-strongly convex and ∇ f (x∗) 2 = Mf . Let ε > 0 be a desired accuracy. Next, assume that f is L f -Lipschitz continuous on the ball BRf (0) with

Recent theoretical advances in decentralized distributed convex optimization

19

Algorithm 7 SPDSTM

Input: y˜0 = z0 = y0 = 0, number of iterations N, α0 = A0 = 0

1: for k = 0, . . ., N do

2: Set L˜ = 2Lψ

3: Set Ak+1 = Ak + αk+1, where 2L˜ αk2+1 = Ak + αk+1

4:

y˜k+1 = (Ak yk +αk+1zk )/Ak+1

5: zk+1 = zk − αk+1∇˜ Ψ (y˜k+1, ξ k)

6:

yk+1 = (Ak yk +αk+1zk+1)/Ak+1

7: end for Output: yN , x˜N = A1N ∑Nk=0 αkx˜(A⊤y˜k, ξ k).

R f = Ω˜ max

Ry

λmax(A⊤A)Ry

,

, Rx ,

AN λmax(A⊤A)

µ

where Ry is such that y∗ 2 ≤ Ry, y∗ is the solution of the dual problem (22), and Rx = x(A⊤y∗) 2. Assume that at iteration

k of Algorithm 7 batch size is chosen according to the formula rk ≥ max

1, σψ2 αk ˆln(N/β)

,

where

αk

=

k+˜1 ,

0

<

ε

≤

HL˜ R20
2

,

Cε

2L

N

0 ≤ δ ≤ (NG+L˜R10)2 and N ≥ 1 for some numeric constant H > 0, G > 0 and Cˆ > 0. Then with probability ≥ 1 − 4β , where

β ∈ (0, 1/8), after N = O

Mf µε

χ (A⊤A)

satisfy the following condition

iterations where χ(A⊤A) = λλmm+ainx((AA⊤⊤AA)) , the outputs x˜N and yN of Algorithm 7

f (x˜N ) − f (x∗) ≤ f (x˜N ) + ψ(yN) ≤ ε, Ax˜N 2 ≤ ε

(44)

Ry

with probability at least 1 − 4β . What is more, to guarantee (44) with probability at least 1 − 4β Algorithm 7 requires

O max

σx2

M

2 f

χ

(A⊤

A)

ln

1

Mf χ(A⊤A) ,

Mf χ(A⊤A)

(45)

ε2

β µε

µε

calls of the biased stochastic oracle ∇˜ ψ(y, ξ ), i.e. x˜(y, ξ ).

3.2.2 Strongly Convex Dual Functions and Restarts Technique

In this section, we assume that primal functional f is additionally L-smooth. It implies that the dual function ψ in (22) is additionally µψ -strongly convex in y0 + (KerA⊤)⊥ where µψ = λm+in(A⊤A)/L [69, 128] and λm+in(A⊤A) is the minimal positive eigenvalue of A⊤A.
From weak duality − f (x∗) ≤ ψ(y∗) and (24) we get the key relation of this section (see also [6, 7, 116])

f (x(A⊤y)) − f (x∗) ≤ ∇ψ(y), y = Ax(A⊤y), y .

(46)

This inequality implies the following theorem.

Theorem 9 (Theorem 5.2 from [51]). Consider function f and its dual function ψ deﬁned in (24) such that problems (21) and (22) have solutions. Assume that yN is such that ∇ψ(yN) 2 ≤ ε/Ry and yN ≤ 2Ry, where ε > 0 is some positive number and Ry = y∗ 2 where y∗ is any minimizer of ψ. Then for xN = x(A⊤yN) following relations hold:

f (xN ) − f (x∗) ≤ 2ε, AxN 2 ≤ ε ,

(47)

Ry

where x∗ is any minimizer of f .

That is why, in this section we mainly focus on the methods that provide optimal convergence rates for the gradient norm. In particular, we consider Recursive Regularization Meta-Algorithm from (see Algorithm 8) [43] with AC-SA2 (see Algorithm 10) as a subroutine (i.e. RRMA-AC-SA2) which is based on AC-SA algorithm (see Algorithm 9) from [48]. We notice that RRMA-AC-SA2 is applied for a regularized dual function

20

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

ψ˜ (y) = ψ(y) + λ2 y − y0 22, (48)

where λ > 0 is some positive number which will be deﬁned further. Function ψ˜ is λ -strongly convex and L˜ ψ -smooth in Rn where L˜ ψ = Lψ + λ . For now, we just assume w.l.o.g. that ψ˜ is (µψ + λ )-strongly convex in Rn, but we will go back to this question further.
In this section we consider the same oracle as in Section 3.2.1, but we additionally assume that δ = 0, i.e., stochastic
ﬁrst-order oracle is unbiased. To deﬁne batched version of the stochastic gradient we will use the following notation:

∑ ∑ ∇Ψ (y, ξ t , r ) = 1 rt ∇ψ(y, ξ l), x(y, ξ t , r ) = 1 rt x(y, ξ l).

(49)

t rt l=1

t rt l=1

As before, in the cases when the batch-size rt can be restored from the context, we will use simpliﬁed notation ∇Ψ (y, ξ t ) and x(y, ξ t ).

Algorithm 8 RRMA-AC-SA2 [43]

Input: y0 — starting point, m — total number of iterations

1: ψ0 ← ψ˜ , yˆ0 ← y0, T ← log2 L˜λψ

2: for k = 1, . . ., T do

3: Run AC-SA2 for m/T iterations to optimize ψk−1 with yˆk−1 as a starting point and get the output yˆk

4:

ψk(y) ← ψ˜ (y) + λ ∑kl=1 2l−1

y − yˆl

2 2

5: end for

Output: yˆT .

In the AC-SA algorithm we use batched stochastic gradients of functions ψk which are deﬁned as follows:

∑ ∇Ψ (y, ξ t ) = 1 rt ∇ψ (y, ξ l),

(50)

k

rt l=1 k

k
∇ψk(y, ξ ) = ∇ψ(y, ξ ) + λ (y − y0) + λ ∑ 2l(y − yˆl). l=1

Algorithm 9 AC-SA [48]

Input: z0 — starting point, m — number of iterations, ψk — objective function 1: y0ag ← z0, y0md ← z0 2: for t = 1, . . ., m do

3:

αt

←

t+21 ,

γt

←

4L˜ ψ t (t +1)

4:

ytmd

←

(1−αt )(λ +γt ) γ +(1−α2)λ

yta−g 1

+

αtγ((+1−(1α−t α)λ2+)λγt ) zt−1

t

t

t

t

5: zt ← λα+t λγt ytmd + (1−λα+t )γλt+γt zt−1 − λα+tγt ∇Ψk(ytmd , ξ t )

6: ytag ← αt zt + (1 − αt )xta−g 1

7: end for

Output: ymag.

Algorithm 10 AC-SA2 [43]
Input: z0 — starting point, m — number of iterations, ψk — objective function 1: Run AC-SA for m/2 iterations to optimize ψk with z0 as a starting point and get the output y1 2: Run AC-SA for m/2 iterations to optimize ψk with y1 as a starting point and get the output y2 Output: y2.
The following theorem states the main result for RRMA-AC-SA2 that we need in the section.

Recent theoretical advances in decentralized distributed convex optimization

21

Theorem 10 (Corollary 1 from [43]). Let ψ be Lψ -smooth and µψ -strongly convex function and λ = Θ (Lψ ln2 N)/N2 for some N > 1. If the Algorithm 8 performs N iterations in total3 with batch size r for all iterations, then it will provide such a point yˆ that
E ∇ψ(yˆ) 22 | y0, r ≤ C L2ψ y0 −Ny4∗ 22 ln4 N + σψ2rlNn6 N , (51)
where C > 0 is some positive constant and y∗ is a solution of the dual problem (22).
The following result shows that w.l.o.g. we can assume that function ψ deﬁned in (24) is µψ -strongly convex everywhere with µψ = λm+in(A⊤A)/L. In fact, from L-smoothness of f we have only that ψ is µψ -strongly convex in y0 + Ker(A⊤) ⊥ (see [69, 128] for the details). However, the structure of the considered here methods is such that all points generated by the RRMA-AC-SA2 and, in particular, AC-SA lie in y0 + Ker(A⊤) ⊥.

Theorem 11 (Theorem 5.4 from [51]). Assume that Algorithm 9 is run for the objective ψk(y) = ψ˜ (y) + λ ∑kl=1 2l−1 y − yˆl 2 with z0 as a starting point, where z0, yˆ1, . . . , yˆk are some points from y0 + Ker(A⊤) ⊥ and y0 ∈ Rn. Then for all t ≥ 0
2
we have ytmd, zt , ytag ∈ y0 + Ker(A⊤) ⊥.

Corollary 1 (Corollary 5.5 from [51]). Assume that Algorithm 8 is run for the objective ψk(y) = ψ˜ (y) + λ ∑kl=1 2l−1 y − yˆl 2 with y0 as a starting point. Then for all k ≥ 0 we have yˆk ∈ y0 + Ker(A⊤) ⊥.
2

Now we are ready to present our approach4 of constructing an accelerated method for the strongly convex dual problem using restarts of RRMA-AC-SA2. To explain the main idea we start with the simplest case: σψ2 = 0, r = 0. It means that there is no stochasticity in the method and the bound (51) can be rewritten in the following form:

√ CLψ

y0 − y∗

2 ln2 N

√ CLψ ∇ψ(y0) 2 ln2 N

∇ψ(yˆ) 2 ≤

N2

≤

µψ N2

,

(52)

where we used inequality ∇ψ(y0) ≥ µψ y0 − y∗ which follows from the µψ -strong convexity of ψ. It implies that

after N¯ = O˜ (

Lψ/µψ ) iterations of RRMA-AC-SA2 the method returns such y¯1 = yˆ that

∇ψ (y¯1)

2≤

1 2

∇ψ(y0) 2. Next,

applying RRMA-AC-SA2 with y¯1 as a starting point for the same number of iterations we will get new point y¯2 such that

∇ψ (y¯2)

2

≤

1 2

∇ψ (y¯1 )

2

≤

1 4

∇ψ (y0)

2. Then, after l = O(ln(Ry ∇ψ(y0) 2/ε)) of such restarts we can get the point y¯l

such that ∇ψ(y¯l) 2 ≤ ε/Ry with total number of gradients computations N¯ l = O˜ Lψ/µψ ln(Ry ∇ψ(y0) 2/ε) .

When σψ2 = 0 we need to modify this approach. The ﬁrst ingredient to handle the stochasticity is large enough batch
size for the l-th restart: rl should be Ω σψ2/(N¯ ∇ψ(y¯l−1) 22) . However, in the stochastic case we do not have an access to the ∇ψ(y¯l−1), so, such batch size is impractical. One possible way to ﬁx this issue is to independently sample large enough
number rˆl ∼ R2y/ε2 of stochastic gradients additionally, which is the second ingredient of our approach, in order to get good enough approximation ∇Ψ (y¯l−1, ξ l−1, rˆl) of ∇ψ(y¯l−1) and use the norm of such an approximation which is close to the norm of the true gradient with big enough probability in order to estimate needed batch size rl for the optimization

procedure. Using this, we can get the bound of the following form:

E

∇ψ(y¯l) 2 | y¯l−1, r , rˆ

≤ A d=ef

∇ψ (y¯l−1)

2
2+

∇Ψ (y¯l−1, ξ l−1, rˆl ) − ∇ψ(y¯l−1)

2
2.

2

ll

l

8

32

The third ingredient is the ampliﬁcation trick: we run pl = Ω (ln(1/β)) independent trajectories of RRMA-AC-SA2, get

points y¯l,1, . . . , y¯l,pl and choose such y¯l,p(l) among of them that ∇ψ(y¯l,p(l)) 2 is close enough to minp=1,...,pl ∇ψ(y¯l,p) 2

with high probability, i.e.,

∇ψ (y¯l, p(l))

2 2

≤

2 minp=1,...,pl

∇ψ (y¯l, p )

2 2

+

ε 2/8R2y

with

probability

at

least

1−β

for

ﬁxed

∇Ψ (y¯l−1, ξ l−1, rˆl). We achieve it due to additional sampling of r¯l ∼ R2y/ε2 stochastic gradients at y¯l,p for each trajectory and

choosing such p(l) corresponding to the smallest norm of the obtained batched stochastic gradient. By Markov’s inequality

for all p = 1, . . . , pl

P

∇ψ (y¯l, p)

2 2

≥

2Al

|

y¯l−1, rl, r¯l

≤ 1,

2

hence

3 The overall number of performed iterations during the calls of AC-SA2 equals N. 4 This approach was described in [32] and formally proved in [51].

22

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

P

min

∇ψ (y¯l, p)

2 2

≥

2Al

|

y¯l−1, rl , r¯l

p=1,..., pl

That is, for pl = log2(1/β) we have that with probability at least 1 − 2β

1 ≤ 2pl .

∇ψ(y¯l,p(l)) 2 ≤

∇ψ (y¯l−1 )

2
2+

∇Ψ (y¯l−1, ξ l−1, rˆl ) − ∇ψ(y¯l−1)

2
2+

ε2

2

2

8

8R2y

for ﬁxed ∇Ψ (y¯l−1, ξ l−1, rˆl ) which means that

∇ψ(y¯l,p(l)) 2 ≤

∇ψ (y¯l−1 )

2
2+

ε2

2

2

4R2y

with probability at least 1 − 3β . Therefore, after l = log2(2R2y ∇ψ(y0) 22/ε2) of such restarts our method provides the point y¯l,p(l) such that with probability at least 1 − 3lβ

∇ψ (y¯l, p(l))

2 2

≤

∑ ∇ψ (y0)

2
2+

ε2

l−1
2−k ≤

ε2

+

ε2

·2 = ε2 .

2l

4R2y k=0

2R2y 4R2y

R2y

The approach informally described above is stated as Algorithm 11.

Algorithm 11 Restarted-RRMA-AC-SA2

Input: y0 — starting point, l — number of restarts, {rˆk}lk=1, {r¯k}lk=1 — batch-sizes, {pk}lk=1 — ampliﬁcation parameters 1: Choose the smallest integer N¯ > 1 such that CLµ2ψψ2 lNn¯ 44 N¯ ≤ 312

2: y¯0,p(0) ← y0

3: for k = 1, . . ., l do

4: Compute ∇Ψ (y¯k−1,p(k−1), ξ k−1,p(k−1), rˆk)

5: rk ← max 1,

64Cσψ2 ln6 N¯

N¯

∇Ψ (y¯k−1,p(k−1),ξ k−1,p(k−1),rˆk )

2 2

6: Run pk independent trajectories of RRMA-AC-SA2 for N¯ iterations with batch-size rk with y¯k−1,p(k−1) as a starting point and get outputs y¯k,1, . . . , y¯k,pk

7: Compute ∇Ψ (y¯k,1, ξ k,1, r¯k), . . ., ∇Ψ (y¯k,pk , ξ k,pk , r¯k) 8: p(k) ← argminp=1,...,pk ∇Ψ (y¯k,p, ξ k,p, r¯k) 2 9: end for

Output: y¯l,p(l).

Theorem 12 (Theorem 5.6 from [51]). Assume that ψ is µψ -strongly convex and Lψ -smooth. If Algorithm 11 is run with

2R2y

∇ψ (y0 )

2 2

l = max 1, log2

ε2



2



4σψ2 1 +

3

ln

l β

R

2 y



64Cσψ2 ln6 N¯

rˆk = max 1,

ε2

 ,

rk = max

1, N¯

∇Ψ (y¯k−1,p(k−1), ξ k−1,p(k−1), rˆ )

2

,

k2



2

l

 128σψ2 1 +

3

ln

l pk β

R

2 y



pk = max 1, log2 β , r¯k = max 1,

ε2



(53)

for all k = 1, . . . , l where N¯ > 1 is such that CLµ2ψψ2lNn¯ 44 N¯ ≤ 312 , β ∈ (0, 1/3) and ε > 0, then with probability at least 1 − 3β

∇ψ(y¯l,p(l)) 2 ≤ ε

(54)

Ry

and the total number of the oracle calls equals

Recent theoretical advances in decentralized distributed convex optimization

23

∑l
(rˆ + N¯ p r + p r¯ ) = O max

Lψ , σψ2 R2y

.

(55)

k k=1

kk kk

µψ ε2

Corollary 2 (Corollary 5.7 from [51]). Under assumptions of Theorem 12 we get that with probability at least 1 − 3β

y¯l,p(l) − y∗ 2 ≤ ε ,

(56)

µψ Ry

where β ∈ (0, 1/3) the total number of the oracle calls is deﬁned in (55).

Now we are ready to present convergence guarantees for the primal function and variables.

Corollary 3 (Corollary 5.8 from [51]). Let the assumptions of Theorem 12 hold. Assume that f is L f -Lipschitz continuous on BRf (0) where

Rf =

µψ + 8 λmax(A⊤A)

λmax(A⊤A) + Rx Ry

µ

Ry

and Rx = x(A⊤y∗) 2. Then, with probability at least 1 − 4β

f (xl) − f (x∗) ≤ 2 +

Lf

ε, Axl 2 ≤ 9ε ,

(57)

8Ry λmax(A⊤A)

8Ry

where β ∈ (0, 1/4), ε ∈ (0, µψ R2y) xl d=ef x(A⊤y¯l,p(l), ξ l,p(l), r¯l ) and to achieve it we need the following number of oracle

calls:

∑l
(rˆ + N¯ p r + p r¯ ) = O max

L χ(A⊤A), σx2M2 χ(A⊤A)

(58)

k k=1

kk kk

µ

ε2

where M = ∇ f (x∗) 2.

3.2.3 Direct Acceleration for Strongly Convex Dual Function

First of all, we consider the following minimization problem:

min ψ(y),

(59)

y∈Rn

where ψ(y) is µψ -strongly convex and Lψ -smooth. We use the same notation to deﬁne the objective in (59) as for the dual

function from (22) because later in the section we apply the algorithm introduced below to the (22), but for now it is not

important that ψ is a dual function for (21) and we prefer to consider more general situation. As in Section 3.2.1, we do not

assume that we have an access to the exact gradient of ψ(y) and consider instead of it biased stochastic gradient ∇˜ ψ(y, ξ )

satisfying inequalities (39) and (40) with δ ≥ 0 and σψ ≥ 0. In the main method of this section batched version of the

stochastic gradient is used:

∑ ∇˜ Ψ (y, ξ k) =

1

rk
∇˜ ψ(y, ξ l),

(60)

rk l=1

where rk is the batch-size that we leave unspeciﬁed for now. Note that ∇˜ Ψ (y, ξ k) satisﬁes inequalities (42) and (43).
We use Stochastic Similar Triangles Method which is stated in this section as Algorithm 12 to solve problem (59). To deﬁne the iterate zk+1 we use the following sequence of functions:

g˜0(z) d=ef 21 z − z0 22 + α0 ψ(y0) + ∇˜ Ψ (y0, ξ 0), z − y0 + µ2ψ z − y0 22 , g˜k+1(z) d=ef g˜k(z) + αk+1 ψ(y˜k+1) + ∇˜ Ψ (y˜k+1, ξ k+1), z − y˜k+1 + µ2ψ z − y˜k+1 22

∑ =

1

z − z0

k+1
2+ α

ψ(y˜l ) + ∇˜ Ψ (y˜l, ξ l), z − y˜l + µψ z − y˜l 2

(61)

2

2

l

l=0

2

2

We notice that g˜k(z) is (1 + Akµψ )-strongly convex.

24

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

Algorithm 12 Stochastic Similar Triangles Methods for strongly convex problems (SSTM sc)

Input: y˜0 = z0 = y0 — starting point, N — number of iterations

1: Set α0 = A0 = 1/Lψ

2: Get ∇˜ Ψ (y0, ξ 0) to deﬁne g˜0(z)

3: for k = 0, 1, . . ., N − 1 do

4: Choose αk+1 such that Ak+1 = Ak + αk+1, Ak+1(1 + Ak µψ ) =

5:

y˜k+1 = (Akyk +αk+1zk )/Ak+1

6: zk+1 = argminz∈Rn g˜k+1(z), where g˜k+1(z) is deﬁned in (61)

7:

yk+1 = (Akyk +αk+1zk+1)/Ak+1

8: end for

Output: xN

αk2+1Lψ

For this algorithm we have the following convergence result.

Theorem 13 (Theorem 5.11 from [51]). Assume that the function ψ is µψ -strongly convex and Lψ -smooth,

µψ

3/2

N2σψ2

ln

N β

rk = Θ max 1, Lψ

ε

,

1

µψ

3/2 N2σψ2 1+

3 ln N 2
β

i.e. rk ≥ C max 1, Lψ

ε

with positive constants C > 0, ε > 0 and N ≥ 1. If additionally δ ≤ NG√RA0N

and ε ≤ HARN20 where R0 = y∗ − y0 2 and Algorithm 12 is run for N iterations, then with probability at least 1 − 3β

yN − y∗ 2 ≤ Jˆ2R20 ,

(62)

2 AN

where β ∈ (0, 1/3),

ln

N β

gˆ(N) =

1+

+ ln ln Bbˆ 2,

3 ln

N β

b = 2σ12α12R20 , r1

D = 1 + µψ + Lψ

1 + µψ , Lψ

Bˆ = 8HC

Lψ

3/2
DR4

N

3

N
+1

µψ

0

2

Aˆ + 2Dh2G2 + 2C

Lψ

3/2
c + 2Du2 H

,

µψ

2 h = u = µψ ,

2 c = µ2 ,
ψ

Aˆ = 1 +

2G

2G2

Lψ 3/4

√ 2 2CH

Lψ 3/2 4CH

√+

+

√+

,

µψ Lψ µψ N AN µψ2 N2 µψ Lψ µψ N AN µψ Lψ µψ2 N2AN





 ˆ= max

3Bˆ1D + 9Bˆ2D2 + 4Aˆ + 8cHC Lψ 3/2 

3/2

1, 1

µψ

, Bˆ1 = hG + uC1 2HC Lψ gˆ(N)

J

 Lψ

2



µψ

and C1 is some positive constant. In other words, to achieve

needs N = O

Lψ µψ

iterations and O max

Lµψψ , σεψ2

pending on Lψ , µψ , R0, ε and β .

yN − y∗

2 2

≤

ε

with

probability

at

least

1

−

3β

Algorithm 12

oracle calls where O(·) hides polylogarithmic factors de-

Next, we apply the SSTM sc to the problem (22) when the objective of the primal problem (21) is L-smooth, µstrongly convex and L f -Lipschitz continuous on some ball which will be speciﬁed next, i.e., we consider the same setup as in Section 3.2.1 but we additionally assume that the primal functional f has L-Lipschitz continuous gradient. As in

Recent theoretical advances in decentralized distributed convex optimization

25

Section 3.2.1 we also consider the case when the gradient of the dual functional is known only through biased stochastic
estimators, see (36)–(43) and the paragraphs containing these formulas. In Section 3.2.1 and 3.2.2 we mentioned that in the considered case dual function ψ is Lψ -smooth on Rn and µψ -
strongly convex on y0 + (KerA⊤)⊥ where Lψ = λmax(A⊤A)/µ and µψ = λm+in(A⊤A)/L. Using the same technique as in the proof of Theorem 11 we show next that w.l.o.g. one can assume that ψ is µψ -strongly convex on Rn since ∇˜ Ψ (y, ξ k) lies in ImA = (KerA⊤)⊥ by deﬁnition of ∇˜ Ψ (y, ξ k). For this purposes we need the explicit formula for zk+1 which follows from the equation ∇g˜k+1(zk+1) = 0:

∑ ∑ zk+1 =

z0

k+1
+

αl µψ

y˜l −

1

k+1
α ∇˜ Ψ (y˜l, ξ l).

(63)

1 + Ak+1µψ l=0 1 + Ak+1µψ

1 + Ak+1µψ l=0 l

Theorem 14 (Theorem 5.12 from [51]). For all k ≥ 0 we have that the iterates of Algorithm 12 y˜k, zk, yk lie in y0 + Ker(A⊤) ⊥.

This theorem makes it possible to apply the result from Theorem 13 for SSTM sc which is run on the problem (22).

Corollary 4 (Corollary 5.13 from [51]). Under assumptions of Theorem 13 we get that after N = O of Algorithm 12 which is run on the problem (22) with probability at least 1 − 3β
∇ψ(yN ) 2 ≤ ε , Ry

Lψ µψ

ln

1 ε

iterations

(64)

where β ∈ (0, 1/3) and the total number of oracles calls equals

Lψ σψ2 R2y

O max µψ , ε2

.

(65)

If additionally ε ≤ µψ R2y, then with probability at least 1 − 3β

yN − y∗ 2 ≤ ε ,

(66)

µψ Ry

yN 2 ≤ 2Ry

(67)

Corollary 5 (Corollary 5.14 from [51]). Let the assumptions of Theorem 13 hold. Assume that f is L f -Lipschitz continuous on BRf (0) where

Rf =

2C λmax(A⊤A) + G1 +

λmax(A⊤A) µ

ε + Rx, Ry

Rx = x(A⊤y∗) 2, ε ≤ µψ R2y and δy ≤ NGR1εy for some positive constant G1. Assume additionally that the last batch-size rN is slightly bigger than other batch-sizes, i.e.



2

2

1



µψ 3/2 N2σψ2 1 +

3

ln

N β

R2y σψ2 1 +

3

ln

N β

R

2 y



rN ≥ C max 1, Lψ

ε2

,

ε2

 .

(68)

Then, with probability at least 1 − 4β

f (x˜N ) − f (x∗) ≤ 2 +

2C + G1 L f ε,

(69)

λmax(A⊤A)

Ry

Ax˜N

√ ≤ 1 + 2C + G

λ

(A⊤A) ε ,

(70)

2

1 max

Ry

where β ∈ (0, 1/4), x˜N d=ef x˜(A⊤yN, ξ N, rN) and to achieve it we need the total number of oracle calls including the cost of computing x˜N equals

26
where M = ∇ f (x∗) 2.

O max

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

L χ(A⊤A), σx2M2 χ(A⊤A)

(71)

µ

ε2

3.3 Applications to Decentralized Distributed Optimization

In this section, we apply our results to the decentralized optimization problems. First of all, we want to add additional

motivation to the problem we are focusing on. As it was stated in the introductory part of this work, we are interested in

the convex optimization problem

min f (x),

(72)

x∈Q⊆Rn

where f is a convex function and Q is closed and convex subset of Rn. More precisely, we study particular case of (72) when the objective function f could be represented as a mathematical expectation

f (x) = Eξ [ f (x, ξ )] ,

(73)

where ξ is a random variable. Typically x represents the feature vector deﬁning the model, only samples of ξ are available and the distribution of ξ is unknown. One possible way to minimize generalization error (73) is to solve empirical risk minimization or ﬁnite-sum minimization problem instead, i.e., solve (72) with the objective

fˆ(x) =

1

m
∑

f (x, ξi),

(74)

m i=1

where m should be sufﬁciently large to approximate the initial problem. Indeed, if f (x, ξ ) is convex and M-Lipschitz continuous for all ξ , Q has ﬁnite diameter D and xˆ = argminx∈Q fˆ(x), then (see [23, 139]) with probability at least 1 − β

f (xˆ) − min f (x) = O M2D2n ln(m) ln (n/β) , (75)

x∈Q

m

and if additionally f (x, ξ ) is µ-strongly convex for all ξ , then (see [42]) with probability at least 1 − β

M2D2 ln(m) ln (m/β ) M2D2 ln (1/β)

f (xˆ) − min f (x) = O
x∈Q

µm

+

m.

(76)

In other words, to solve (72)+(73) with ε functional accuracy via minimization of empirical risk (74) it is needed to have m = Ω M2D2n/ε2 in the convex case and m = Ω max M2D2/µε, M2D2/ε2 in the µ-strongly convex case where Ω (·) hides a constant factor, a logarithmic factor of 1/β and a polylogarithmic factor of 1/ε.
Stochastic ﬁrst-order methods such as Stochastic Gradient Descent (SGD) [57, 114, 119, 127, 160] or its accelerated variants like AC-SA [83] or Similar Triangles Method (STM) [39, 47, 117] are very popular choice to solve either (72)+(73) or (72)+(74). In contrast with their cheap iterations in terms of computational cost, these methods converge only to the neighbourhood of the solution, i.e., to the ball centered at the optimality and radius proportional to the standard deviation of the stochastic estimator. For the particular case of ﬁnite-sum minimization problem one can solve this issue via variancereduction trick [26, 54, 64, 137] and its accelerated variants [5, 173, 174]. Unfortunately, this technique is not applicable in general for the problems of type (72)+(73). Another possible way to reduce the variance is mini-batching. When the objective function is L-smooth one can accelerate the computations of batches using parallelization [27, 38, 47, 49], and it is one of the examples where centralized distributed optimization appears naturally [13].
In other words, in some situations, e.g., when the number of samples m is too big, it is preferable in practice to split the data into q blocks, assign each block to the separate worker, e.g., processor, and organize computation of the gradient or stochastic gradient in the parallel or distributed manner. Moreover, in view of (75)-(76) sometimes to solve an expectation minimization problem it is needed to have such a big number of samples that corresponding information (e.g. some objects like images, videos and etc.) cannot be stored on 1 machine because of the memory limitations (see Section 3.5 for the detailed example of such a situation). Then, we can rewrite the objective function in the following form

Recent theoretical advances in decentralized distributed convex optimization

27

∑ ∑ 1 q

1 si

f (x) = q i=1 fi(x), fi(x) = Eξi [ f (x, ξi)] or fi(x) = si j=1 f (x, ξi j). (77)

Here fi corresponds to the loss on the i-th data block and could be also represented as an expectation or a ﬁnite sum. So, the general idea for parallel optimization is to compute gradients or stochastic gradients by each worker, then aggregate the results by the master node and broadcast new iterate or needed information to obtain the new iterate back to the workers.
The visual simplicity of the parallel scheme hides synchronization drawback and high requirement to master node [134]. The big line of works is aimed to solve this issue via periodical synchronization [72, 147, 171, 165, 164, 75, 53], error-compensation [71, 148, 16, 55], quantization [4, 61, 62, 108, 163] or combination of these techniques [11, 105].
However, in this work we mainly focus on another approach to deal with aforementioned drawbacks — decentralized distributed optimization [13, 73]. It is based on two basic principles: every node communicates only with its neighbours and communications are performed simultaneously. Moreover, this architecture is more robust, e.g., it can be applied to time-varying (wireless) communication networks [133].
But let us consider ﬁrst the centralized or parallel architecture. As we mentioned in the introduction, when the objective function is L-smooth one can compute batches in parallel [27, 38, 47, 49] in order to accelerate the work of the method and get the method (see Section 3 from [51] for the details) using

σ 2R2/ε2 O
LR2/ε

or O

σ 2/µε L/µ ln µR2/ε

(78)

workers and having the working time proportional to the number of iterations of an accelerated ﬁrst-order method. However, the number of workers deﬁned in (78) could be too big in order to use such an approach in practice. But still computing the batches in parallel even with much smaller number of workers could reduce the working time of the method if the communication is fast enough.
Besides the computation of batches in parallel for the general type of problem (72)+(73), parallel optimization is often applied to the ﬁnite-sum minimization problems (72)+(74) or (72)+(77) that we rewrite here in the following form:

∑ 1 m

min f (x) =

fk (x).

(79)

x∈Q⊆Rn

m k=1

We notice that in this section m is a number of workers and fk(x) is known only for the k-th worker. Consider the situation when workers are connected in a network and one can construct a spanning tree for this network. Assume that the diameter of the obtained graph equals d, i.e., the height of the tree — maximal distance (in terms of connections) between the root and a leaf [134]. If we run Similar Triangles Methods (STM, [47]) on such a spanning tree then we will get that the number of communication rounds will be

σ 2R2

LR2/ε σ 2 LR2

L/µ

O dN + d min ε2 ln β , µε ln ε ln β

,

where

LR2 L LR2

N = O min

ε , µ ln ε

.

Now let us consider the decentralized case when workers can communicate only with their neighbours. Next, we describe the method of how to reﬂect this restriction in the problem (79). Consider the Laplacian matrix W ∈ Rm×m of the

network with vertices V and edges E which is deﬁned as follows:

 −1,

if (i, j) ∈ E,

W i j = deg(i), if i = j,

(80)

0

otherwise,

where deg(i) is degree of i-th node, i.e. number of neighbours of the i-th worker. Since we consider only connected networks the matrix W has unique eigenvector 1m d=ef (1, . . . , 1)⊤ ∈ Rm corresponding to the eigenvalue 0. It implies that for all vectors a = (a1, . . . , am)⊤ ∈ Rm the following equivalence holds:

a1 = . . . = am ⇐⇒ W a = 0.

(81)

28

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

Now let us think about ai as a number that i-th node stores. Then, using (81) we can use Laplacian matrix to express in the short matrix form the fact that all nodes of the network store the same number. In order to generalize it for the case when ai are vectors from Rn we should consider the matrix W d=ef W ⊗ In where ⊗ represents the Kronecker product. Indeed, if we consider vectors x1, . . . , xm ∈ Rn and x = x⊤1 , . . . , x⊤m ∈ Rnm, then (81) implies

x1 = . . . = xm ⇐⇒ W x = 0.

(82)

For simplicity, we also call W as a Laplacian matrix and it does not lead to misunderstanding since everywhere below we

use W instead of W . The key observation here that computation of W x requires one round of communications when the

k-th worker sends xk to all its neighbours and receives x j for all j such that (k, j) ∈ E, i.e. k-th wor√ker gets vectors from all its neighbours. Not√e, that W is symmetric and positive semideﬁnite [134] and, as a consequence, W exists. Moreover, we can replace W by W in (82) and get the equivalent statement:

√

x1 = . . . = xm ⇐⇒ W x = 0.

(83)

Using this we can rewrite the problem (79) in the following way:

∑ 1 m

√min
W x=0,

f (x) = m

fk (xk ).

x1,...,xm∈Q⊆Rn

k=1

(84)

We are interested in the general case when fk(xk) = Eξk [ fk(xk, ξk)] where {ξk}mk=1 are independent. This type of objective can be considered as a special case of (77). Then, as it was mentioned in the introduction it is natural to use stochastic
gradients ∇ fk(xk, ξk) that satisfy

Eξk [∇ fk(xk, ξk)] − ∇ fk(xk) 2 ≤ δ ,

(85)

∇ fk(xk, ξk) − Eξ [∇ fk(xk, ξk)] 2

Eξk exp

σ 2k 2 ≤ exp(1).

(86)

Then, the stochastic gradient

∑ ∇ f (x, ξ ) d=ef ∇ f (x, {ξ }m

) d=ef 1

m
∇ f (x , ξ )

k k=1

m k=1 k k k

satisﬁes (see also (43))

Eξ exp

∇ f (x, ξ ) − Eξ [∇ f (x, ξ )] 22 σ 2f

≤ exp(1)

with

σ

2 f

=

O

σ 2/m

.

As always, we start with the smooth case with Q = Rn and assume that each fk is L-smooth, µ-strongly convex and

satisﬁes ∇k fk(xk) 2 ≤ M on some ball BRM (x∗) where we use ∇k f (xk) to emphasize that fk depends only on the k-

th n-dimensional block of x. Since the functional f (x) in (84) has separable structure, it implies that f is L/m-smooth, µ/m-strongly convex and satisﬁes ∇ f (x) 2 ≤ M/√m on B√mRM (x∗). Indeed, for all x, y ∈ Rn

∑m

x−y

2 2

=

xk − yk 22,

k=1

∇ f (x) − ∇ f (y) 2 =

∑ 1 m ∇ f (x ) − ∇ f (y ) 2 ≤

m2 k=1 k k k

kk k 2

∑ L2 m x − y 2 = L x − y ,

m2 k=1 k k 2 m

2

∑ ∑ f (x) = 1 m f (x ) ≥ 1 m f (y ) + ∇ f (y ), x − y + µ xk − yk 2

m k=1 k k m k=1 k

kk k k k 2

2

= f (y) + ∇ f (y), x − y + 2µm x − y 22,

∑ ∇ f (x) 22 = m12 m ∇k fk(xk) 22. k=1

Recent theoretical advances in decentralized distributed convex optimization

29

√ Therefore, one can consider the problem (84) as (21) with A = W and Q = Rnm. Next, if the starting point x0 is such that x0 = ((x0)⊤, . . . , (x0)⊤)⊤ then

R2 d=ef

x0 − x∗

2 2

=

m

x0 − x∗

2 2

=

mR2,

R2y d=ef

y∗

2 2

≤

∇ f (x∗)

2 2

≤

M2

.

λm+in(W ) mλm+in(W )

Now it should become clear why in Section 3.1 we paid most of our attention on number of A⊤Ax calculations. In this

particular

scenario

A⊤Ax

=

√ ⊤√ W Wx

=

Wx

which

can

be

computed

via

one

round

of

communications

of

each

node

with its neighbours as it was mentioned earlier in this section. That is, for the primal approach we can simply use the

results discussed in Section 3.1. For convenience, we summarize them in Tables 3 and 4 which are obtained via plugging

the parameters that we obtained above in the bounds from Section 3.1. Note that the results presented in this match the

lower bounds obtained in [8] in terms of the number of communication rounds up to logarithmic factors and and there is

a conjecture [32] that these bounds are also optimal in terms of number of oracle calls per node for the class of methods

that require optimal number of communication rounds. Recently, the very similar result about the optimal balance between

number of oracle calls per node and number of communication round was proved for the case when the primal functional

is convex and L-smooth and deterministic ﬁrst-order oracle is available [167].

Assumptions on fk µ-strongly convex,
L-smooth
L-smooth
µ-strongly convex, ∇ fk(x) 2 ≤ M ∇ fk(x) 2 ≤ M

Method
D-MASG, Q = Rn,
[41] STP IPS with STP as a subroutine,
Q = Rn, [51]
R-Sliding, [32, 85, 84, 87]
Sliding, [85, 84, 87]

# of communication rounds

O

L µ

χ

O

LR2 ε

χ

O

M2 µε

χ

O

M2R2 ε2

χ

# of ∇ fk(x) oracle calls per node

O

L µ

O

LR2 ε

O

M2 µε

O Mε22R2

Table 3 Summary of the covered results in this paper for solving (84) using primal deterministic approach from Section 3.1. First column contains assumptions on fk, k = 1, . . ., m in addition to the convexity, χ = χ(W ) = λmax(W )/λm+in(W ), where λmax(W ) and λm+in(W ) are maximal and minimal positive eigenvalues of matrix W . All methods except D-MASG should be applied to solve (25).

Finally, consider the situation when Q = Rn and each fk from (84) is dual-friendly, i.e. one can construct dual problem for (84)

min Ψ (y), where y = (y⊤1 , . . . , y⊤m)⊤ ∈ Rnm, y1, . . . , ym ∈ Rn,

(87)

y∈Rnm

ϕk(yk) = max { yk, xk − fk(xk)} ,

(88)

xk ∈Rn

Φ(y) =

1

m

√

∑ ϕk(myk), Ψ (y) = Φ( W y) =

1

m

√

∑ ϕk(m[ W x]k),

(89)

m k=1

m k=1

√

√

where [ W x]k is the k-th n-dimensional block of W x. Note that

max { y, x − f (x)} = max

x∈Rnm

x∈Rnm

∑ ∑ m

1m

yk, xk −

fk (xk )

k=1

m k=1

∑ ∑ 1 m

1m

=

max { myk, xk − fk(xk)} =

ϕk(myk) = Φ(y),

m k=1 xk∈Rn

m k=1

so, Φ(y) is a dual function for f (x). As for the primal approach, we are interested in the general case when ϕk(yk) = Eξk [ϕk(yk, ξk)] where {ξk}mk=1 are independent and stochastic gradients ∇ϕk(xk, ξk) satisfy

30

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

Assumptions on fk

Method

# of communication rounds

# of ∇ fk(x, ξ ) oracle calls per node

µ-strongly convex, L-smooth

D-MASG,
in expectation, Q = Rn, [41]

O

L µ

χ

O max

L µ

,

σ2 µε

SSTP IPS with

L-smooth

STP as a subroutine,

Q = Rn,

O

conjecture,

LR2 ε

χ

O max

LR2 ε

,

σ 2R2 ε2

[32, 51]

µ-strongly convex, ∇ fk(x) 2 ≤ M

RS-Sliding Q is bounded, [32, 85, 84, 87]

O

M2 µε

χ

O

M2+σ 2 µε

∇ fk(x) 2 ≤ M

S-Sliding Q is bounded, [85, 84, 87]

O

M 2 R2 ε2

χ

O (M2+εσ2 2)R2

Table 4 Summary of the covered results in this paper for solving (84) using primal stochastic approach from Section 3.1 with the stochas-
tic oracle satisfying (85)-(86) with δ = 0. First column contains assumptions on fk, k = 1, . . ., m in addition to the convexity, χ = χ(W ) = λmax(W )/λm+in(W ), where λmax(W ) and λm+in(W ) are maximal and minimal positive eigenvalues of matrix W . All methods except D-MASG should be applied to solve (25). The bounds from the last two rows hold even in the case when Q is unbounded, but in the expectation (see [90]).

Eξk [∇ϕk(yk, ξk)] − ∇ϕk(yk) 2 ≤ δϕ ,

(90)

∇ϕk(yk, ξk) − Eξ [∇ϕk(yk, ξk)] 2

Eξk exp

σ 2k 2 ≤ exp(1).

(91)

Consider the stochastic function fk(xk, ξk) which is deﬁned implicitly as follows:

ϕk(yk, ξk) = max { yk, xk − f (xk, ξk)} .

(92)

xk ∈Rn

Since

∑ ∑ m

m

∇Φ(y) = ∇ϕk(myk) (=35) xk(myk) d=ef x(y),

k=1

k=1

xk(yk) d=ef argmax { yk, xk − fk(xk)}
xk ∈Rn

it is natural to deﬁne the stochastic gradient ∇Φ(y, ξ ) as follows:

∑ ∑ m

m

∇Φ(y, ξ ) d=ef ∇Φ(y, {ξk}mk=1) d=ef ∇ϕk(myk, ξk) (=35) xk(myk, ξk) d=ef x(y, ξ ),

k=1

k=1

xk(yk, ξk) d=ef argmax { yk, xk − fk(xk, ξk)} .
xk ∈Rn

It satisﬁes (see also (43))

Eξ exp

Eξ [∇Φ(y, ξ )] − ∇Φ(y) 2 ≤ δΦ ,

∇Φ(y, ξ ) − Eξ [∇Φ(y, ξ )] 22 σΦ2

≤ exp(1)

with δΦ = mδϕ and σ 2 = O

mσ 2

def √

√

. Using this, we deﬁne the stochastic gradient of Ψ (y) as ∇Ψ (y, ξ ) = W ∇Φ( W y, ξ ) =

√√

Φ

W x( W y, ξ ) and, as a consequence, we get

Eξ exp

Eξ [∇Ψ (y, ξ )] − ∇Ψ (y) 2 ≤ δΨ ,

∇Ψ (y, ξ ) − Eξ [∇Ψ (y, ξ )] 22 σΨ2

≤ exp(1)

Recent theoretical advances in decentralized distributed convex optimization

31

with δΨ = λmax(W )δΦ and σΨ = λmax(W )σΦ .

√

Taking all of this into account we conclude that problem (87) is a special case of (22) with A = W . To make√the

algorithms from Section 3.2 distributed we should change the variables in those methods via multiplying them by W

from the left [32, 34, 158], e.g. for the iterates of SPDSTM we will get

y˜k+1

:=

√ W

y˜k+1,

zk+1

:=

√ W

zk+1,

yk+1

:=

√ W

yk+1,

√ which means that it is needed to multiply lines 4-6 of Algorithm 7 by W from the left. After such a change of variables all

methods from Section 3.2 become suitable to run them in the distributed fashion. Besides that, it does not spoil the ability

of√recovering √the primal variables since before the change of variables all of the methods mentioned in Section 3.2 used

x( W y) or x( W y, ξ ) where points y were some dual iterates of those me√thods, so, after the change of variables we should

use x(y) or x(y, ξ ) respectively. Moreover, it is also possible to compute

Wx

2 2

=

x,W x

in the distributed fashion using

consensus type algorithms: one communication step is needed to compute W x, then each worker computes xk, [W x]k

locally and after that it is needed to run consensus algorithm. We summarize the results for this case in Table 5. Note that

the proposed bounds are optimal in terms of the number of communication rounds up to polylogarithmic factors [8, 134,

135, 136]. Note that the lower bounds from [134, 135, 136] are presented for the convolution of two criteria: number of

oracle calls per node and communication rounds. One can obtain lower bounds for the number of communication rounds

itself using additional assumption that time needed for one communication is big enough and the term which corresponds

to the number of oracle calls can be neglected. Regarding the number of oracle calls there is a conjecture [32] that the

bounds that we present in this paper are also optimal up to polylogarithmic factors for the class of methods that require

optimal number of communication rounds.

Assumptions on fk

Method

# of communication rounds

R-RRMA-AC-SA2

# of ∇ϕk(y, ξ ) oracle calls per node

µ-strongly convex, L-smooth,
∇ fk(x) 2 ≤ M

(Algorithm 11), Corollary 3, SSTM sc
(Algorithm 12),

O

L µ

χ

O max µL χ, σΦ2εM2 2 χ

Corollary 5

µ-strongly convex, ∇ fk(x) 2 ≤ M

SPDSTM (Algorithm 7),
Theorem 8

O

M2 µε

χ

O max Mµε2 χ , σΦ2εM2 2 χ

Table 5 Summary of the covered results in this paper for solving (87) using dual stochastic approach from Section 3.2 with the stochastic oracle satisfying (85)-(86) with δ = 0 for R-RRMA-AC-SA2 and δϕ = O (ε/(M√mχ)) for SSTM sc and SPDSTM. First column contains assumptions on fk, k = 1, . . ., m in addition to the convexity, χ = χ(W ).

3.4 Discussion

In this section, we want to discuss some aspects of the proposed results that were not covered in the main part of

this paper. First of all, we should say that in the smooth case for the primal approach our bounds for the number of

communication steps coincides with the optimal bounds for the number of communication steps for parallel optimization

if we substitute the diameter d of the spanning tree in the bounds for parallel optimization by O( χ(W )).

However, we want to discuss another interesting difference between parallel and decentralized optimization in terms

of the complexity results which was noticed in [32]. From the line of works [80, 81, 82, 89] it is known that for the

problem (72)+(77) (here we use m instead of q and iterator k instead of i for consistency) with L-smooth and µ-strongly

convex fk for all k = 1, . . . , m the optimal number of oracle calls, i.e. calculations of of the stochastic gradients of fk with σ 2-subgaussian variance is

L σ2

O m+ mµ + µε .

(93)

The bad news is that (93) does not work with full parallelization trick and the best possible way to parallelize it is described in [89]. However, standard accelerated scheme using mini-batched versions of the stochastic gradients without variancereduction technique and incremental oracles which gives the bound

32

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

L σ2

O m µ + µε

(94)

for the number of oracle calls and it admits full parallelization. It means that in the parallel optimization setup when we

have computational network with m nodes and the spanning tree for it with diameter d the number of oracle calls per node

is

L σ2

L σ2

O µ + mµε = O max µ , mµε

(95)

and the number of communication steps is

L

Od µ .

(96)

However, for the decentralized setup the second row of Table 4 states that the number of communication rounds is the same as in (96) up to substitution of d by χ(W ) and the number of oracle calls per node is

L σ2

O max µ , µε

(97)

which has m times bigger statistical term under the maximum than in (95). What is more, recently it was shown that there exists such a decentralized distributed method that requires
σ2 O mµε
stochastic gradient oracle calls per node [120, 121], but it is not optimal in terms of the number of communications. Recently a stochastic optimization method with consensus subroutine for time-varying graphs requiring O˜ σ 2/(nµε) oracle calls and O˜ L/µχ communications was proposed in [130]. The results of [130] can be easily extended to O˜( L/µ√χ) communication complexity in the time-static case via employing accelerated consensus with Chebyshev acceleration. Moreover, there is a hypothesis [32] that in the smooth case the bounds from Tables 3 and 4 (rows 2 and 3) are not optimal in terms of the number of oracle calls per node and optimal ones can be found in Table 2.

3.5 Application for Population Wasserstein Barycenter Calculation
In this section we consider the problem of calculation of population Wasserstein barycenter since this example hides different interesting details connected with the theory discussed in this paper. In our presentation of this example we rely mostly on the recent works [30, 31].

3.5.1 Deﬁnitions and Properties

We deﬁne the probability simplex in Rn as Sn(1) = x ∈ Rn+ | ∑ni=1 xi = 1 . One can interpret the elements of Sn(1)

as discrete probability measures with n shared atoms. For an arbitrary pair of measures p, q ∈ Sn(1) we introduce the set

Π (p, q) =

π

∈

Rn×n
+

|

π1

=

p,

π⊤1

=

q

called transportation polytope. Optimal transportation (OT) problem between

measures p, q ∈ Sn(1) is deﬁned as follows

n

∑ W (p, q) = min C, π = min

Ci j πi j

(98)

π∈Π (p,q)

π∈Π (p,q) i, j=1

where C is a transportation cost matrix. That is, (i, j)-th component Ci j of C is a cost of transportation of the unit mass from point xi to the point x j where points are atoms of measures from Sn(1).
Next, we consider the entropic OT problem (see [122, 126])

Recent theoretical advances in decentralized distributed convex optimization

33

n

∑ Wµ (p, q) = min

(Ci jπi j + µπi j ln πi j) .

(99)

π∈Π (p,q) i, j=1

Consider some probability measure P on Sn(1). Then one can deﬁne population barycenter of measures from Sn(1) as

p∗µ = argmin

Wµ (p, q)dP(q) = argmin Eq Wµ (p, q) .

p∈Sn(1) q∈Sn(1)

p∈Sn(1)

Wµ (p)

(100)

For a given set of samples q1, . . . , qm we introduce empirical barycenter as

∑ pˆ∗µ = argmin 1

m
Wµ (p, qi) .

p∈Sn(1) m i=1

Wˆ (p)

(101)

We consider the problem (100) of ﬁnding population barycenter with some accuracy and discuss possible approaches to
solve this problem in the following subsections. However, before that, we need to mention some useful properties of Wµ (p, q). First of all, one can write explicitly the
dual function of Wµ (p, q) for a ﬁxed q ∈ Sn(1) (see [25, 30]):

Wµ (p, q) = max

λ,p

−

W

∗ q,µ

(λ

)

λ ∈Rn

∑ ∑ n
Wq∗,µ (λ ) = µ q j ln

1n

−Ci j + λi

exp

.

j=1

q j i=1

µ

(102) (103)

Using this representation one can deduce the following theorem.

Theorem 15 ([30]). For an arbitrary q ∈ Sn(1) the entropic Wasserstein d√istance Wµ (·, q) : Sn(1) → R is µ-strongly convex w.r.t. ℓ2-norm and M-Lipschitz continuous w.r.t. ℓ2-norm. Moreover, M ≤ nM∞ where M∞ is Lipschitz constant of Wµ (·, q) w.r.t. ℓ∞-norm and5 M∞ = O( C ∞).

We also want to notice that function Wq∗,µ(λ ) is only strictly convex and the minimal eigenvalue of its hessian γ d=ef

λmin(∇2Wq,µ (λ ∗)) evaluated in the solution λ ∗ d=ef argmaxλ ∈Rn bounds that are exponentially small in n.

λ,p

−

W

∗ q,µ

(λ

)

is very small and there exist only such

We will also use another useful relation (see [30]):

∇Wµ (p, q) = λ ∗, λ ∗, 1 = 0

(104)

where the gradient ∇Wµ (p, q) is taken w.r.t. the ﬁrst argument.

3.5.2 SA Approach

Assume that one can obtain and use fresh samples q1, q2, . . . in online regime. This approach is called Stochastic Approximation (SA). It implies that at each iteration one can draw a fresh sample qk and√compute the gradient w.r.t. p of function Wµ (p, qk) which is µ-strongly convex and M-Lipschitz continuous with M = O( n C ∞). Optimal methods
for this case are based on iterations of the following form

pk+1 = projSn(1) pk − ηk∇Wµ (pk, qk)

where projSn(1)(x) is a projection of x ∈ Rn on Sn(1) and the gradient ∇Wµ (pk, qk) is taken w.r.t. the ﬁrst argument. One can show that restarted-SGD (R-SGD) from [67] that using biased stochastic gradients (see also [65, 46, 30]) ∇˜ Wµ (p, q)

such that

∇˜ Wµ (p, q) − ∇Wµ(p, q) 2 ≤ δ

(105)

5 Under assumption that measures are separated from zero, see the details in [21] and the proof of Proposition 2.5 from [30].

34

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

for some δ ≥ 0 and for all p, q ∈ Sn(1) after N calls of this oracle produces such a point pN that with probability at least 1 − β the following inequalities hold:

Wµ (pN) − Wµ(p∗µ ) = O n C 2∞µlNn(N/α) + δ

(106)

and, as a consequence of µ-strong convexity of Wµ (p, q) for all q,

pN − p∗µ 2 = O

n

C

2 ∞

ln(N/α)

+

δ

.

µ2N

µ

(107)

That is, to guarantee with probability at least 1 − β , R-SGD requires

pN − p∗µ 2 ≤ ε

(108)

n

C

2 ∞

O µ2ε2

∇˜ Wµ (p, q) oracle calls

(109)

under additional assumption that δ = O(µε2). However, it is computationally hard problem to ﬁnd ∇Wµ (p, q) with high-accuracy, i.e. ﬁnd ∇˜ Wµ (p, q) satisfying
(105) with δ = O(µε2). Taking into account the relation (104) we get that it is needed to solve the problem (102) with accuracy δ = O(µε2) in terms of the distance to the optimum. i.e. it is needed to ﬁnd such λ˜ that λ˜ − λ ∗ 2 ≤ δ and set ∇˜ Wµ (p, q) = λ˜ . Using variants of Sinkhorn algorithm [79, 149, 58] one can show [30] that R-SGD ﬁnds point pN such
that (108) holds with probability at least 1 − β and it requires

n3

C

2 ∞

C∞

O µ2ε2 min exp µ

C ∞ + ln C ∞

µ

γ µ 2ε 4

n , γµ3ε4

(110)

arithmetical operations.

3.5.3 SAA Approach

Now let us assume that large enough collection of samples q1, . . . , qm is available. Our goal is to ﬁnd such p ∈ Sn(1) that pˆ − p∗µ 2 ≤ ε with high probability, i.e. ε-approximation of the population barycenter, via solving empirical barycenter problem (101). This approach is ca√lled Stochastic Average Approximation (SAA). Since Wµ (p, qi) is µ-strongly convex and M-Lipschitz in p with M = O( n C ∞) for all i = 1, . . . , m we can conclude that with probability ≥ 1 − β

Wµ (pˆ∗µ ) − Wµ (p∗µ ) (=76) O n C 2∞ lnµ(mm) ln (m/β ) +

n C 2∞ ln (1/β) m

(111)

where we use that the diameter of Sn(1) is O(1). Moreover, in [139] it was shown that one can guarantee that with

probability ≥ 1 − β

Wµ (pˆ∗µ ) − Wµ(p∗µ ) (=76) O nβCµm2∞ .

(112)

Taking advantages of both inequalities we get that if

m=Ω

min

max

n

C

2 ∞

,

n

C

2 ∞

,n

C

2 ∞

µ2ε2 µ2ε4 β µ2ε2

= Ω n min

C

2
∞,

C

2 ∞

µ2ε4 β µ2ε2

(113)

then with probability at least 1 − β2

pˆ∗µ − p∗µ 2 ≤ µ2 Wµ (pˆ∗µ ) − Wµ (p∗µ ) (111),(1≤12),(113) ε2 .

(114)

Recent theoretical advances in decentralized distributed convex optimization

35

Assuming that

we

have

such

pˆ ∈ Sn(1)

that

with

probability at

least

1−

β 2

the

inequality

pˆ − pˆ∗µ 2 ≤ ε 2

(115)

holds, we apply the union bound and get that with probability ≥ 1 − β

pˆ − p∗µ 2 ≤ pˆ − pˆ∗µ 2 + pˆ∗µ − p∗µ 2 ≤ ε.

(116)

It remains to describe the approach that ﬁnds such pˆ ∈ Sn(1) that satisﬁes (116) with probability at least 1 − β . Recall that in this subsection we consider the following problem

∑ Wˆµ (p) = 1

m
Wµ (p, qi) →

min

.

m i=1

p∈Sn(1)

(117)

For each summand Wµ (p, qi) in the sum above we have the explicit formula (103) for the dual function Wq∗i,µ (λ ). Note

that

one

can

compute

the

gradient

of

W∗
qi,µ

(λ

)

via

O(n2)

arithmetical

operations.

What

is

more,

W∗
qi,µ

(λ

)

has

a

ﬁnite-sum

structure, so, one can sample j-th component of qi with probability qij and get stochastic gradient

∑ ∇W ∗ (λ , j) = µ∇ ln

1n

−Ci j + λi

exp

qi,µ

qij i=1

µ

(118)

which requires O(n) arithmetical operations to be computed. We start with the simple situation. Assume that each measures qi are stored on m separate machines that form some
network with Laplacian matrix W ∈ Rm×m. For this scenario we can apply the dual approach described in Section 3.3 and
apply bounds from Table 5. If for all i = 1, . . . , m the i-th node computes the full gradient of dual functions Wqi,µ at each iteration then in order to ﬁnd such a point pˆ that with probability at least 1 − β2

Wˆµ (pˆ) − Wˆµ(pˆ∗µ ) ≤ εˆ,

(119)

where W = W ⊗ In, this approach requires O

n

C µ εˆ

2∞

χ

(W

)

communication rounds and O n2.5

C µ

2∞
εˆ

χ

(W

)

arith-

metical

operations

per

node

to

ﬁnd

gradients

∇W

∗ qi,

µ

(λ

).

If

instead

of

full

gradients

workers

use

stochastic

gradients

∇W

∗ qi,

µ

(λ

,

j)

deﬁned

in

(118)

and

these

stochastic

gradients

have

light-tailed

distribution,

i.e.

satisfy

the

condition

(91)

with parameter σ > 0, then to guarantee (119) with probability ≥ 1 − β2 the aforementioned approach needs the same

number of communications rounds and O n max

n

C µ εˆ

2∞

χ

(W

),

mσ 2n C εˆ 2

2
∞ χ(W )

arithmetical operations per node to

ﬁnd

gradients ∇Wq∗i,µ (λ ,

j).

Using

µ -strong

convexity of

Wµ (p, qi)

for

all

i = 1, . . . , m

and

taking

εˆ

=

µε2 8

we

get

that

our

approach ﬁnds such a point pˆ that satisﬁes (115) with probability at least 1 − β2 using

√ nC∞
O µε

χ(W )

communication rounds

(120)

and O n2.5 C ∞ χ(W ) µε

arithmetical operations per node to ﬁnd gradients in the deterministic case and

√ nC ∞
O n max µε

χ(W ), mσ 2n

C

2
∞ χ(W )

µ2ε4

(121)

arithmetical operations per node to ﬁnd stochastic gradients in the stochastic case. However, the state-of-the-art theory of

learning states (see (113)) that m should so large that in the stochastic case the second term in the bound for arithmetical

operations typically dominates the ﬁrst term and the dimensional dependence reduction from n2.5 in the deterministic

case

to

n1.5

in

the

stochastic

case

is

typically

negligible

in

comparison

with

how

much

mσ 2√n

C

2
∞ χ(W )

is

larger

than

µ2ε4

36

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

C∞ µε

χ(W ). That is, our theory says that it is better to use full gradients in the particular example considered in this

section (see also Section 3.4). Therefore, further in the section we will assume that σ 2 = 0, i.e. workers use full gradients

of

dual

functions

W∗
qi,µ

(λ

).

However, bounds (120)-(121) were obtained under very restrictive at the ﬁrst sight assumption that we have m workers

and each worker stores only one measure which is unrealistic. One can relax this assumption in the following way. Assume that we have lˆ < m machines connected in a network with Laplacian matrix Wˆ and j-th machine stores mˆ j ≥ 1 measures for j = 1, . . . , lˆ and ∑lˆj=1 mˆ j = m. Next, for j-th machine we introduce mˆ j virtual workers also connected in some network that j-th machine can emulate along with communication between virtual workers and for every virtual worker we arrange

one measure, e.g. it can be implemented as an array-like data structure with some formal rules for exchanging the data

between cells that emulates communications. We also assume that inside the machine we can set the preferable network

for the virtual nodes in such a way that each machine emulates communication between virtual nodes and computations

inside them fast enough. Let us denote the Laplacian matrix of the obtained network of m virtual nodes as W . Then, our approach ﬁnds such a point pˆ that satisﬁes (115) with probability at least 1 − β2 using





 O 


max Tcm, j
j=1,...,lˆ

√ nC∞ µε

 χ(W )


(122)

Tcm,max

time to perform communications and

  O  

max Tcp, j
j=1,...,lˆ Tcp,max

n2.5 C ∞ µε

  χ(W ) 

(123)

time for arithmetical operations per machine to ﬁnd gradients where Tcm, j is time needed for j-th machine to emulate communication between corresponding virtual nodes at each iteration and Tcp, j is time required by j-th machine to perform 1 arithmetical operation for all corresponding virtual nodes in the gradients computation process at each iteration. For example, if we have only one machine and network of virtual nodes forms a complete graph than χ(W ) = 1, but Tcm,max and Tcp,max can be large and to reduce the running time one should use more powerful machine. In contrast, if we have m machines connected in a star-graph than Tcm,max and Tcp,max will be much smaller, but χ(W ) will be of order m which is large. Therefore, it is very important to choose balanced architecture of the network at least for virtual nodes per machine
if it is possible. This question requires a separate thorough study and lies out of scope of this paper.

3.5.4 SA vs SAA comparison
Recall that in SA approach we assume that it is possible to sample new measures in online regime which means that the computational process is performed on one machine, whereas in SAA approach we assume that large enough collection of measures is distributed among the network of machines that form some computational network. In practice measures from Sn(1) correspond to some images. As one can see from the complexity bounds, both SA and SAA approaches require large number of samples to learn the population barycenter deﬁned in (100). If these samples are images, then they typically cannot be stored in RAM of one computer. Therefore, it is natural to use distributed systems to store the data.
Now let us compare complexity bounds for SA and SAA. We summarize them in Table 6. When the communication is fast enough and µ is small we typically have that SAA approach signiﬁcantly outperforms SA approach in terms of the complexity as well even for communication architectures with big χ(W ). Therefore, for balanced architecture one can expect that SAA approach will outperform SA even more.
To conclude, we state that population barycenter computation is a natural example when it is typically much more preferable to use distributed algorithms with dual oracle instead of SA approach in terms of memory and complexity bounds.

Recent theoretical advances in decentralized distributed convex optimization

37

Approach
SA
SA, the 2-d term
is smaller

SAA

O

SAA, χ(W ) = Ω (m), Tcm,max = O(1), Tcp,max = O(1),
β ≥ε

Complexity

O n3µ2Cε22∞ min exp Cµ ∞

C∞ µ

+ ln

C∞ γµ2ε4

arithmetical operations

,

n γµ3ε4

O

n3.5 √

C

2 ∞

arithmetical operations

γ µ3.5 ε4

√

O

Tcm,max

nC ∞ µε

χ(W ) time to perform communications,

Tcp,maxn2.5

C∞ µε

χ(W ) time for arithmetical operations per machine,

where m = Ω n min

C

2
∞,

C

2 ∞

µ2ε4 βµ2ε2

O

√n

C

2 ∞

communication rounds,

βµ2ε2

O

√n3

C

2 ∞

arithmetical operations per machine

βµ2ε2

Table 6 Complexity bounds for SA and SAA approaches for computation of population barycenter deﬁned in (100) with accuracy ε. The third

row states the complexity bound for SA approach when the second term under the minimum in (110) is dominated by the ﬁrst one, e.g. when

µ is small enough. The last row corresponds to the case when Tcm,max = O(1), Tcp,max = O(1), communication network is star-like, which implies χ(W ) = Ω (m)

β ≥ ε, e.g. β = 0.01 and ε ≤ 0.1, and the

4 Derivative-Free Distributed Optimization

As mentioned above in Section 3, the decentralized optimization problem can be rewritten as a problem with afﬁne

constraints:

∑ 1 m
√Wmxi=n0, f (x) = m i=1 fi(xi),
x1 ,...,xm ∈Q

(124)

where we use matrix W d=ef W ⊗ In for Laplacian matrix W =

Wij

m,m i, j=1,1

∈

Rm×m

of

the

connection

graph.

In

turn,

the

problem with afﬁne constraints:

min f (x),
Ax=0,x∈Q

is rewritten in a penalized form as follows:

min F(x) = f (x) + R2y Ax 2,

x∈Q

ε

2

(125)

with some positive constants ε and Ry (for details see Section 3). As a result, we have a classical composite optimization problem, therefore this section will focus on this problem. In what follows, we will rely on work [15]. Note that the work [146] with a similar results has recently appeared (unlike work [15], it considers a more practical one-point feedback – for a more detailed explanation of the difference, see [146]). Note also, that results of [15, 146] can be generalized for saddle-point problems by using proper version of Sliding technique [88]. We will ﬁnd out a method based on the Sliding Algorithm (see [84] and Section 3) for the convex composite optimization problem with smooth and non-smooth terms. One can ﬁnd gradient-free methods for distributed optimization in the literature (see [97, 153]), but the method that will be discussed further is the ﬁrst, which combines zeroth-order and ﬁrst-order oracles. Its uses the ﬁrst-order oracle for the smooth part and the zeroth-order oracle for the non-smooth part.

4.1 Theoretical part
4.1.1 Convex Case We consider6 the composite optimization problem

6 The narrative in this section follows [15].

38

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

minΨ0(x) = f (x) + g(x).
x∈Q

(126)

In this part of paper, we will work not in the Euclidean norm · 2, but in a certain norm · (and the dual norm · ∗ for the norm · ). Also deﬁne the Bregman divergence associated with some function ν(x), which is 1-strongly convex
w.r.t. · -norm and differentiable on Q, as follows

V (x, y) = ν(y) − ν(x) − ∇ν(x), y − x , ∀x, y ∈ Q.

The use of Bregman divergence and special norms allows taking into account the geometric setup of the problem. For
example, when we work with the problem in a probability simplex, it seems natural to use the · 1-norm and the Kullback– Leibler divergence.
Next, we introduce some assumptions for problem (126): Q ⊆ Rn is a compact and convex set with diameter DQ in · -norm, function g is convex and L-smooth on Q w.r.t. norm · , i.e.

∇g(x) − ∇g(y) ∗ ≤ L x − y , ∀x, y ∈ Q,

f is convex differentiable function on Q.
Assume that we have an access to the ﬁrst-order oracle for g, i.e. gradient ∇g(x) is available, and to the biased stochastic zeroth-order oracle for f (see also [52, 19]) that for a given point x returns noisy value f˜(x, ξ ) such that

f˜(x, ξ ) = f (x, ξ ) + ∆ (x),

(127)

where ∆ (x) is a bounded noise of unknown nature |∆ (x)| ≤ ∆

and random variable ξ is such that

E[ f (x, ξ )] = f (x).

Additionally, we assume that for all x ∈ Qs (s ≤ DQ)

∇ f (x, ξ ) 2 ≤ M(ξ ), E[M2(ξ )] = M2.

It is important to note that for the function f (x) these assumptions are made only for theoretical estimates; we have no real access to ∇ f (x). The question is how to replace the gradient of the function f (x). The easiest way is to collect gradient completely using ﬁnite differences:

∑ f ′ (x, ξ ) = 1 n f˜(x + rhi, ξ ) − f˜(x − rhi, ξ ) hi,

full

r i=1

(128)

here we consider a standard orthogonal normalized basis {h1, . . . , hn}. This way we really get a vector close to the gradient. The obvious disadvantage of this method is that one need to call the oracle for f˜(x, ξ ) 2n times. Another way is to use random direction e uniformly distributed on the Euclidean sphere (see [118, 141]):

f˜′(x, ξ , e) = n ( f˜(x + re, ξ ) − f˜(x − re, ξ ))e.

r

2r

(129)

In particular, the authors of [15] use this approximation. Now another problem arises – we need to combine the zeroth-order and ﬁrst-order oracles for different parts of the
composite problem. It seems natural that the gradient-free oracle should be called more often than the gradient one. The
authors of paper [15] solve this problem and propose to apply the algorithm based on Lan’s Sliding [84]. The basic idea is that we ﬁx ∇g and iterate through the inner loop (PS procedure), changing only the point x in f˜r′(x, ξ , e).

Recent theoretical advances in decentralized distributed convex optimization

39

Algorithm 13 Zeroth-Order Sliding Algorithm (zoSA)
Input: Initial point x0 ∈ Q and iteration limit N. Let βk ∈ R++, γk ∈ R+, and Tk ∈ N, k = 1, 2, . . ., be given and set x0 = x0. for k = 1, 2, . . ., N do
1. Set xk = (1 − γk)xk−1 + γkxk−1, and let hk(·) ≡ lg(xk, ·) be deﬁned in (130). 2. Set
(xk, x˜k) = PS(hk, xk−1, βk, Tk);
3. Set xk = (1 − γk)xk−1 + γkx˜k. end for
Output: xN .

The PS (prox-sliding) procedure. procedure: (x+, x˜+) = PS(h, x, β , T ) Let the parameters pt ∈ R++ and θt ∈ [0, 1], t = 1, . . ., be given. Set u0 = u˜0 = x. for t = 1, 2, . . ., T do
ut = argmin h(u) + f˜r′(ut−1, ξt−1, et−1), u + βV (x, u) + β ptV (ut−1, u) ,
u∈Q
u˜t = (1 − θt )u˜t−1 + θt ut .
end for Set x+ = uT and x˜+ = u˜T . end procedure:

In the Algorithm 13 we need the following function

lg(x, y) = g(x) + ∇g(x), y − x .

(130)

It is important that the random variables ξt are independent, and also et is sampled independently from previous iterations. We also note that zoSA (in contrast to the basic version – Algorithm 6) takes into account the geometric setting of the
problem and uses Bregman divergence V (x, y) instead of the standard Euclidean distance in prox-sliding procedure. Next, we will brieﬂy talk about the convergence of this method (see the full version of the analysis in [15]). First
of all, we note the universal technical lemmas that forms a general approach to working with gradient-free methods for non-smooth functions. But before that we introduce a new notation:

F(x) = Ee[ f (x + re)].

(131)

F(x) is called the smoothed function of f (x). It is important to note that the function F(x) is not calculated by the algorithm, this object is needed only for theoretical analysis. The ﬁrst lemma states some properties of F(x):

Lemma 5. Assume that differentiable function f deﬁned on Qs satisfy ∇ f (x) 2 ≤ M with some constant M > 0. Then F(x) deﬁned in (131) is convex, differentiable and F(x) satisﬁes

sup |F(x) − f (x)| ≤ rM, ∇F(x) = Ee n f (x + re)e ,

x∈Q

r

√ ∇F(x) ∗ ≤ c˜p∗ nM,

where c˜ is some positive constant independent of n and p∗ is determined by the following relation: 4 E[ e 4∗] ≤ p∗. In other words, F(x) provides a good approximation of f (x) for small enough r. Lemma 6. For f˜r′(x, ξ , e) deﬁned in (129) the following inequalities hold:
E[ f˜r′(x, ξ , e)] − ∇F(x) ∗ ≤ n∆rp∗ , E[ f˜r′(x, ξ , e) 2∗] ≤ 2p2∗ cnM2 + n2r∆2 2 ,

where c is some positive constant independent of n.
In other words, one can consider f˜r′(x, ξ , e) as a biased stochastic gradient of F(x) with bounded second moment. Therefore, instead of solving (126) directly one can focus on the problem

minΨ (x) = F(x) + g(x)
x∈Q

(132)

40

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

with small enough r. As mentioned earlier, this approach is universal. In particular, the analysis of gradient-free methods for non-smooth saddle-point problems can be carried out in a similar way [19].
Now we will give the main facts from [15] for zoSA algorithm itself. The following theorem states convergence guarantees:
Theorem 16. Suppose that {pt }t≥1, {θt }t≥1 are

pt = t , θt = 2(t + 1) , for all t ≥ 1,

2

t(t + 3)

(133)

N is given, {βk}, {γk}, {Tk} are

2L

2

CN p2∗ nM2 + n2r∆2 2 k2

βk = k , γk = k + 1 , Tk =

D˜ L2

(134)

with D˜ = 3D2Q,V/4, DQ,V = max{ for all N ≥ 1

2V (x, y) | x, y ∈ Q}, DQ = max{ x − y | x, y ∈ Q}, with some positive constant C. Then E[Ψ (xN ) −Ψ (x∗)] ≤ N12(NLD+2Q1,V) + n∆ DrQ p∗ .

Finally, need to connect the result above to the initial problem (126).

Corollary 6. Under the assumptions of Theorem 16 we have that the following inequality holds for all N ≥ 1:

E[Ψ (x ) −Ψ (x∗)] ≤ 2rM + 12LD2Q,V + n∆ DQ p∗ .

0N

0

N(N + 1)

r

(135)

From (135) it follows that if

r =Θ

ε, M

∆ =O

ε2 nMDQ min{p∗, 1}

and ε = O (√nMDQ), then the number of evaluations for ∇g and f˜′, respectively, required by Algorithm 13 to ﬁnd an
r
ε-solution of (126), i.e. such xN that E[Ψ0(xN)] −Ψ0(x∗) ≤ ε, can be bounded by









O  LD2Q,V  and O  LD2Q,V + D2Q,V p2∗nM2  .

ε

ε

ε2

(136)

It is interesting to analyze the obtained results depending on p∗, and these constants are determined depending on what

geometry we have deﬁned for our problem. For example, if we consider Euclidean proximal setup, i.e. · = · 2,

V (x, y)

=

1 2

x−y

22, DQ,V = DQ. In this case we have p∗ and bound (136) for the number of (127) oracle calls reduces to





O  LD2Q + D2QnM2 

ε

ε2

and the number of ∇g(x) computations remains the same. It means that our result gives the same number of ﬁrst-order

oracle calls as in the original Gradient Sliding algorithm, while the number of the biased stochastic zeroth-order oracle

calls is n times larger in the leading term than in the analogous bound from the original ﬁrst-order method. In the Euclidean

case our bounds reﬂect the classical dimension dependence for the derivative-free optimization (see [91]).

But if we work on the probability simplex in Rn and the proximal set√up is entropic: V (x, y) is the Kullback–Leibler

divergence,

i.e.

V (x, y)

=

∑ni=1

xi

ln

xi y

.

In

this

situation

we

have

DQ,V

=

2 log n, DQ = 2, p∗ = O (log(n)/n) [56]. Then

i

number of ∇g(x) calculations is bounded by O (Llog2 n)/ε . As for the number of f˜r′(x, ξ , e) computations, we get the

following bound:

L log n M2 log2 n

O

ε + ε2

.

(137)

Recent theoretical advances in decentralized distributed convex optimization

41

4.1.2 Strongly Convex Case

In this section we additionally assume that g is µ-strongly convex w.r.t. Bregman divergence V (x, y) [150], i.e. for all x, y ∈ Q
g(x) ≥ g(y) + ∇g(y), x − y + µV(x, y).
The authors of [15] use restarts technique and get Algorithm 14.

Algorithm 14 The Multi-phase Zeroth-Order Sliding Algorithm (M-zoSA)
Input: Initial point y0 ∈ Q and iteration limit N0, initial estimate ρ0 (s.t. Ψ (y0) −Ψ (y∗) ≤ ρ0) for i = 1, 2, . . ., I do
Run zoSA with x0 = yi−1, N = N0, {pt } and {θt } in (133), {βk} and {γk}, {Tk} in (134) with D˜ = ρ0/µ2i, and yi is output. end for
Output: yI .

The following theorem states the main complexity results for M-zoSA.

Theorem 17. For M-zoSA with N0 = 2⌈ 5L/µ⌉ we have

E[Ψ (yi) −Ψ (y∗)] ≤ ρ0 + 2n∆ DQ p∗ .

2i

r

Using this we derive the complexity bounds for M-zoSA.

Corollary 7. For all N ≥ 1 the iterates of M-zoSA satisfy

E[Ψ0(yi) −Ψ0(y∗)] ≤ 2rM + ρ0 + 2n∆ DQ p∗ .

2i

r

(138)

From (138) it follows that if

r=Θ

ε, M

∆ =O

ε2 nMDQ min{p∗, 1}

and ε = O (√nMDQ), then the number of evaluations for ∇g and f˜′, respectively, required by Algorithm 14 to ﬁnd a
r
ε-solution of (126) can be bounded by

O µL log2 max [1, ρ0/ε] , O µL log2 max [1, ρ0/ε] + p2∗µnεM2 .

4.1.3 From Composite Optimization to Decentralized Distributed Optimization

Finally, we get an estimate for solving the decentralized optimization problem. With the help of (124) and (125),

we reduce the original decentralized problem to the penalized problem. Next, we need to deﬁne parameters of f using
parameters of local functions fi. Assume that for each fi we have ∇ fi(xi) 2 ≤ M for all xi ∈ Q, all fi are convex functions, the starting point is x⊤0 = (x⊤0 , . . . , x⊤0 )⊤ and x⊤∗ = (x⊤∗ , . . . , x⊤∗ )⊤ is the optimality point for (124). Then, one can show that ∇ f (x) 2 ≤ M/√m on the set of such x that x1, . . . , xm ∈ Q, D2Qm = mD2Q, D2Qm,V = mD2Q,V and R2y from (125) is R2y ≤ M2/mλm+in(W). And we have estimates in the Euclidean case:









O

χ

(W

)M

2

D

2 Q



communication

rounds

and

O

χ(W )M2D2Q

+

n

D

2 Q

M

2



calculations

of

f˜(x, ξ )

per

node.

ε2

ε2

ε2

At the same time, when we work on a simplex and use the Kullback-Leibler divergence, we get estimates similar to (137):

O

χ(W )M2 log n communication rounds and O

χ(W )M2 log n + M2 log2 n calculations of f˜(x, ξ ) per node.

ε2

ε2

ε2

42

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

The bound for the communication rounds matches the lower bound from [136, 135] and one can note that under above assumptions the obtained bound for zeroth-order oracle calculations per node is optimal up to polylogarithmic factors in the class of methods with optimal number of communication rounds (see also [32, 51]). In particular, in the Euclidean case, we lose n times (which corresponds to the case if we were to restore the gradient in the way (128)), and in the case of a simplex, only in the log n times.

Acknowledgements
Authors are express gratitude to A. Nazin, A. Nedich, G. Scutari, C. Uribe and P. Dvurechensky for fruitful discussions. The research of A. Gasnikov, A. Beznosikov and. A. Rogozin was partially supported by RFBR, project number 1931-51001. The research of E. Gorbunov and D. Dvinskikh was partially supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) № 075-00337-20-03, project No. 0714-2020-0005.

References
1. S. Abadeh, P. Esfahani, and D. Kuhn. Distributionally robust logistic regression. In Advances in Neural Information Processing Systems (NeurIPS)), pages 1576–1584, 2015.
2. A. Aghajan and B. Touri. Distributed optimization over dependent random networks. arXiv preprint arXiv:2010.01956, 2020. 3. S. A. Alghunaim, E. K. Ryu, K. Yuan, and A. H. Sayed. Decentralized proximal gradient algorithms with linear convergence rates. IEEE
Transactions on Automatic Control, 66(6):2787–2794, 2020. 4. D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding.
In Advances in Neural Information Processing Systems, pages 1709–1720, 2017. 5. Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2017, pages 1200–1205, New York, NY, USA, 2017. ACM. arXiv:1603.05953. 6. Z. Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information
Processing Systems, pages 1157–1167, 2018. 7. A. S. Anikin, A. V. Gasnikov, P. E. Dvurechensky, A. I. Tyurin, and A. V. Chernov. Dual approaches to the minimization of strongly convex
functionals with a simple structure under afﬁne constraints. Computational Mathematics and Mathematical Physics, 57(8):1262–1276, Aug 2017. 8. Y. Arjevani and O. Shamir. Communication complexity of distributed convex learning and optimization. In Advances in neural information processing systems, pages 1756–1764, 2015. 9. M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. Proceedings of the 34th International Conference on Machine Learning (ICML), 70(1):214–223, 2017. 10. N. Bansal and A. Gupta. Potential-function proofs for gradient methods. Theory of Computing, 15(1):1–32, 2019. 11. D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsiﬁcation, and local computations. arXiv preprint arXiv:1906.02367, 2019. 12. A. Bayandina, P. Dvurechensky, A. Gasnikov, F. Stonyakin, and A. Titov. Mirror descent and convex optimization problems with nonsmooth inequality constraints. In Large-Scale and Distributed Optimization, pages 181–213. Springer, 2018. 13. D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliffs, NJ, 1989. 14. A. Beznosikov, P. Dvurechensky, A. Koloskova, V. Samokhin, S. U. Stich, and A. Gasnikov. Decentralized local stochastic extra-gradient for variational inequalities. arXiv preprint arXiv:2106.08315, 2021. 15. A. Beznosikov, E. Gorbunov, and A. Gasnikov. Derivative-free method for composite optimization with applications to decentralized distributed optimization. IFAC-PapersOnLine, 53(2):4038–4043, 2020. 16. A. Beznosikov, S. Horva´th, P. Richta´rik, and M. Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020. 17. A. Beznosikov, D. Kovalev, A. Sadiev, P. Richtarik, and A. Gasnikov. Optimal distributed algorithms for stochastic variational inequalities. arXiv preprint, 2021. 18. A. Beznosikov, A. Rogozin, D. Kovalev, and A. Gasnikov. Near-optimal decentralized algorithms for saddle point problems over timevarying networks. In International Conference on Optimization and Applications, pages 246–257. Springer, 2021. 19. A. Beznosikov, A. Sadiev, and A. Gasnikov. Gradient-free methods with inexact oracle for convex-concave stochastic saddle-point problem. In International Conference on Mathematical Optimization Theory and Operations Research, pages 105–119. Springer, 2020. 20. A. Beznosikov, G. Scutari, A. Rogozin, and A. Gasnikov. Distributed saddle-point problems under data similarity. Advances in Neural Information Processing Systems, 34, 2021. 21. J. Blanchet, A. Jambulapati, C. Kent, and A. Sidford. Towards optimal running times for optimal transport. arXiv preprint arXiv:1810.07717, 2018. 22. S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms. IEEE transactions on information theory, 52(6):2508– 2530, 2006. 23. N. Cesa-bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 359–366. MIT Press, 2002.

Recent theoretical advances in decentralized distributed convex optimization

43

24. A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40(1):120–145, 2011.
25. M. Cuturi and G. Peyre´. A smoothed dual approach for variational wasserstein problems. SIAM Journal on Imaging Sciences, 9(1):320– 343, 2016.
26. A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS’14, pages 1646–1654, Cambridge, MA, USA, 2014. MIT Press.
27. O. Devolder. Exactness, inexactness and stochasticity in ﬁrst-order methods for large-scale convex optimization. PhD thesis, PhD thesis, ICTEAM and CORE, Universite´ Catholique de Louvain, 2013.
28. O. Devolder, F. Glineur, and Y. Nesterov. First-order methods with inexact oracle: the strongly convex case. CORE Discussion Papers, 2013016:47, 2013.
29. O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2014.
30. D. Dvinskikh. Stochastic approximation versus sample average approximation for population wasserstein barycenters. arXiv preprint arXiv:2001.07697, 2020.
31. D. Dvinskikh. Decentralized algorithms for wasserstein barycenters. arXiv preprint arXiv:2105.01587, 2021. 32. D. Dvinskikh and A. Gasnikov. Decentralized and parallel primal and dual accelerated methods for stochastic convex programming
problems. Journal of Inverse and Ill-posed Problems, 29(3):385–405, 2021. 33. D. Dvinskikh, A. Gasnikov, A. Rogozin, and A. Beznosikov. Parallel and distributed algorithms for ml problems. arXiv preprint
arXiv:2010.09585, 2020. 34. D. Dvinskikh, E. Gorbunov, A. Gasnikov, P. Dvurechensky, and C. A. Uribe. On primal and dual approaches for distributed stochastic
convex optimization over networks. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7435–7440. IEEE, 2019. 35. D. Dvinskikh and D. Tiapkin. Improved complexity bounds in wasserstein barycenter problem. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 1738–1746. PMLR, 2021. 36. D. M. Dvinskikh, A. I. Turin, A. V. Gasnikov, and S. S. Omelchenko. Accelerated and non accelerated stochastic gradient descent in
model generality. Matematicheskie Zametki, 108(4):515–528, 2020. 37. P. Dvurechenskii, D. Dvinskikh, A. Gasnikov, C. Uribe, and A. Nedich. Decentralize and randomize: Faster algorithm for wasserstein
barycenters. In Advances in Neural Information Processing Systems, pages 10760–10770, 2018. 38. P. Dvurechensky and A. Gasnikov. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. Journal
of Optimization Theory and Applications, 171(1):121–145, 2016. 39. P. Dvurechensky, A. Gasnikov, and A. Tiurin. Randomized similar triangles method: A unifying framework for accelerated randomized
optimization methods (coordinate descent, directional search, derivative-free method). arXiv:1707.08486, 2017. 40. F. Facchinei and J. Pang. Finite-Dimensional Variational Inequalities and Complementarity Problems. Springer Series in Operations
Research and Financial Engineering. Springer New York, 2007. 41. A. Fallah, M. Gurbuzbalaban, A. Ozdaglar, U. Simsekli, and L. Zhu. Robust distributed accelerated stochastic gradient methods for
multi-agent networks. arXiv preprint arXiv:1910.08701, 2019. 42. V. Feldman and J. Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. arXiv
preprint arXiv:1902.10710, 2019. 43. D. Foster, A. Sekhari, O. Shamir, N. Srebro, K. Sridharan, and B. Woodworth. The complexity of making the gradient small in stochastic
convex optimization. arXiv preprint arXiv:1902.04686, 2019. 44. A. Gasnikov. Universal gradient descent. arXiv preprint arXiv:1711.00394, 2017. 45. A. Gasnikov, D. Dvinskikh, P. Dvurechensky, D. Kamzolov, V. Matyukhin, D. Pasechnyuk, N. Tupitsa, and A. Chernov. Accelerated
meta-algorithm for convex optimization problems. Computational Mathematics and Mathematical Physics, 61(1):17–28, 2021. 46. A. V. Gasnikov, A. A. Lagunovskaya, I. N. Usmanova, and F. A. Fedorenko. Gradient-free proximal methods with inexact oracle for
convex stochastic nonsmooth optimization problems on the simplex. Automation and Remote Control, 77(11):2018–2034, Nov 2016. arXiv:1412.3890. 47. A. V. Gasnikov and Y. E. Nesterov. Universal method for stochastic composite optimization problems. Computational Mathematics and Mathematical Physics, 58(1):48–64, 2018. 48. S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469–1492, 2012. 49. S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. arXiv:1309.5549. 50. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS)), pages 2672–2680, 2014. 51. E. Gorbunov, D. Dvinskikh, and A. Gasnikov. Optimal decentralized distributed algorithms for stochastic convex optimization. arXiv preprint arXiv:1911.07363, 2019. 52. E. Gorbunov, P. Dvurechensky, and A. Gasnikov. An accelerated method for derivative-free smooth stochastic convex optimization. SIOPT (in print), 2022. 53. E. Gorbunov, F. Hanzely, and P. Richta´rik. Local sgd: Uniﬁed theory and new efﬁcient methods. arXiv preprint arXiv:2011.02828, 2020. 54. E. Gorbunov, F. Hanzely, and P. Richta´rik. A uniﬁed theory of sgd: Variance reduction, sampling, quantization and coordinate descent. In International Conference on Artiﬁcial Intelligence and Statistics, pages 680–690. PMLR, 2020. 55. E. Gorbunov, D. Kovalev, D. Makarenko, and P. Richta´rik. Linearly converging error compensated sgd. Advances in Neural Information Processing Systems, 33, 2020. 56. E. Gorbunov, E. A. Vorontsova, and A. V. Gasnikov. On the upper bound for the expectation of the norm of a vector uniformly distributed on the sphere and the phenomenon of concentration of uniform measure on the sphere. Mathematical Notes, 106, 2019. 57. R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richtarik. Sgd: General analysis and improved rates. arXiv preprint arXiv:1901.09401, 2019. 58. S. Guminov, P. Dvurechensky, N. Tupitsa, and A. Gasnikov. On a combination of alternating minimization and nesterov’s momentum. In International Conference on Machine Learning, pages 3886–3898. PMLR, 2021.

44

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

59. H. Hendrikx, F. Bach, and L. Massoulie. An optimal algorithm for decentralized ﬁnite sum optimization. arXiv preprint arXiv:2005.10675, 2020.
60. H. Hendrikx, L. Xiao, S. Bubeck, F. Bach, and L. Massoulie. Statistically preconditioned accelerated gradient method for distributed optimization. arXiv preprint arXiv:2002.10726, 2020.
61. S. Horvath, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtarik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019.
62. S. Horva´th, D. Kovalev, K. Mishchenko, S. Stich, and P. Richta´rik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
63. D. Jakovetic´, J. Xavier, and J. M. Moura. Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131–1146, 2014.
64. R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315–323, 2013.
65. A. Juditsky and A. Nemirovski. First order methods for non-smooth convex large-scale optimization, i: General purpose methods. In S. W. Suvrit Sra, Sebastian Nowozin, editor, Optimization for Machine Learning, pages 121–184. Cambridge, MA: MIT Press, 2012.
66. A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.
67. A. Juditsky and Y. Nesterov. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems, 4(1):44–80, 2014.
68. P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
69. S. Kakade, S. Shalev-Shwartz, and A. Tewari. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09.pdf, 2(1), 2009.
70. S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. arXiv preprint arXiv:1910.06378, 2019.
71. S. P. Karimireddy, Q. Rebjock, S. U. Stich, and M. Jaggi. Error feedback ﬁxes signsgd and other gradient compression schemes. arXiv preprint arXiv:1901.09847, 2019.
72. A. Khaled, K. Mishchenko, and P. Richta´rik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4519–4529, 2020.
73. V. Kibardin. Decomposition into functions in the minimization problem. Avtomatika i Telemekhanika, (9):66–79, 1979. 74. A. Koloskova, T. Lin, and S. U. Stich. An improved analysis of gradient tracking for decentralized machine learning. Advances in Neural
Information Processing Systems, 34, 2021. 75. A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. U. Stich. A uniﬁed theory of decentralized sgd with changing topology and local
updates. ICML 2020, arXiv preprint arXiv:2003.10422, 2020. 76. D. Kovalev, E. Gasanov, A. Gasnikov, and P. Richtarik. Lower bounds and optimal algorithms for smooth and strongly convex decentral-
ized optimization over time-varying networks. Advances in Neural Information Processing Systems, 34, 2021. 77. D. Kovalev, A. Salim, and P. Richta´rik. Optimal and practical algorithms for smooth and strongly convex decentralized optimization.
Advances in Neural Information Processing Systems, 33, 2020. 78. D. Kovalev, E. Shulgin, P. Richta´rik, A. Rogozin, and A. Gasnikov. Adom: Accelerated decentralized optimization method for time-
varying networks. arXiv preprint arXiv:2102.09234, 2021. 79. A. Kroshnin, N. Tupitsa, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and C. Uribe. On the complexity of approximating wasserstein
barycenters. In International conference on machine learning, pages 3530–3540. PMLR, 2019. 80. A. Kulunchakov and J. Mairal. Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness
to noise. arXiv preprint arXiv:1901.08788, 2019. 81. A. Kulunchakov and J. Mairal. Estimate sequences for variance-reduced stochastic composite optimization. arXiv preprint
arXiv:1905.02374, 2019. 82. A. Kulunchakov and J. Mairal. A generic acceleration framework for stochastic composite optimization. arXiv preprint arXiv:1906.01164,
2019. 83. G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365–397, Jun 2012. Firs appeared
in June 2008. 84. G. Lan. Gradient sliding for composite optimization. Mathematical Programming, 159(1):201–235, Sep 2016. 85. G. Lan. Lectures on optimization methods for machine learning. e-print, 2019. 86. G. Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer, 2020. 87. G. Lan, S. Lee, and Y. Zhou. Communication-efﬁcient algorithms for decentralized and stochastic optimization. Mathematical Program-
ming, pages 1–48, 2017. 88. G. Lan and Y. Ouyang. Mirror-prox sliding methods for solving a class of monotone variational inequalities. arXiv preprint
arXiv:2111.00996, 2021. 89. G. Lan and Y. Zhou. Random gradient extrapolation for distributed and stochastic optimization. SIAM Journal on Optimization,
28(4):2753–2782, 2018. 90. G. Lan and Z. Zhou. Algorithms for stochastic optimization with expectation constraints. arXiv:1604.03887, 2016. 91. J. Larson, M. Menickelly, and S. M. Wild. Derivative-free optimization methods. Acta Numerica, 28:287–404, 2019. 92. S. Lee and A. Nedic. Distributed random projection algorithm for convex optimization. IEEE Journal of Selected Topics in Signal
Processing, 7(2):221–229, 2013. 93. H. Li, C. Fang, W. Yin, and Z. Lin. Decentralized accelerated gradient methods with increasing penalty parameters. IEEE Transactions
on Signal Processing, 68:4855–4870, 2020. 94. H. Li and Z. Lin. Revisiting extra for smooth distributed optimization. arXiv preprint arXiv:2002.10110, 2020. 95. H. Li and Z. Lin. Accelerated gradient tracking over time-varying graphs for decentralized optimization. arXiv preprint arXiv:2104.02596,
2021. 96. H. Li, Z. Lin, and Y. Fang. Optimal accelerated variance reduced extra and diging for strongly convex and smooth decentralized optimiza-
tion. arXiv preprint arXiv:2009.04373, 2020.

Recent theoretical advances in decentralized distributed convex optimization

45

97. J. Li, C. Wu, Z. Wu, and Q. Long. Gradient-free method for nonsmooth distributed optimization. Journal of Global Optimization, 61, 02 2014.
98. H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS’15, pages 3384–3392, Cambridge, MA, USA, 2015. MIT Press.
99. T. Lin, C. Jin, and M. I. Jordan. Near-optimal algorithms for minimax optimization. In Conference on Learning Theory, pages 2738–2779. PMLR, 2020.
100. T. Lin, S. P. Karimireddy, S. U. Stich, and M. Jaggi. Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. arXiv preprint arXiv:2102.04761, 2021.
101. J. Liu and A. S. Morse. Accelerated linear iterations for distributed averaging. Annual Reviews in Control, 35(2):160–165, 2011. 102. M. Liu, W. Zhang, Y. Mroueh, X. Cui, J. Ross, T. Yang, and P. Das. A decentralized parallel algorithm for training generative adversarial
nets. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 103. W. Liu, A. Mokhtari, A. Ozdaglar, S. Pattathil, Z. Shen, and N. Zheng. A decentralized proximal point-type method for non-convex
non-concave saddle point problems. 104. W. Liu, A. Mokhtari, A. Ozdaglar, S. Pattathil, Z. Shen, and N. Zheng. A decentralized proximal point-type method for saddle point
problems. arXiv preprint arXiv:1910.14380, 2019. 105. X. Liu, Y. Li, J. Tang, and M. Yan. A double residual compression algorithm for efﬁcient distributed learning. arXiv preprint
arXiv:1910.07561, 2019. 106. D. Mateos-Nu´nez and J. Corte´s. Distributed subgradient methods for saddle-point problems. In 2015 54th IEEE Conference on Decision
and Control (CDC), pages 5462–5467. IEEE, 2015. 107. G. J. Minty. Monotone (nonlinear) operators in Hilbert space. Duke Mathematical Journal, 29(3):341 – 346, 1962. 108. K. Mishchenko, E. Gorbunov, M. Taka´cˇ, and P. Richta´rik. Distributed learning with compressed gradient differences. arXiv preprint
arXiv:1901.09269, 2019. 109. S. Muthukrishnan, B. Ghosh, and M. H. Schultz. First-and second-order diffusive methods for rapid, coarse, distributed load balancing.
Theory of computing systems, 31(4):331–354, 1998. 110. A. Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed optimization. IEEE Signal
Processing Magazine, 37(3):92–101, 2020. 111. A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM
Journal on Optimization, 27(4):2597–2633, 2017. 112. A. Nedic´ and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control,
54(1):48–61, 2009. 113. A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators
and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229–251, 2004. 114. A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal
on Optimization, 19(4):1574–1609, 2009. 115. Y. Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004. 116. Y. Nesterov. How to make the gradients small. Optima, 88:10–11, 2012. 117. Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018. 118. Y. Nesterov and V. G. Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics,
17(2):527–566, 2017. 119. L. M. Nguyen, P. H. Nguyen, M. van Dijk, P. Richta´rik, K. Scheinberg, and M. Taka´cˇ. Sgd and hogwild! convergence without the bounded
gradients assumption. arXiv preprint arXiv:1802.03801, 2018. 120. A. Olshevsky, I. C. Paschalidis, and S. Pu. Asymptotic network independence in distributed optimization for machine learning. arXiv
preprint arXiv:1906.12345, 2019. 121. A. Olshevsky, I. C. Paschalidis, and S. Pu. A non-asymptotic analysis of network independence for distributed stochastic gradient descent.
arXiv preprint arXiv:1906.02702, 2019. 122. G. Peyre´, M. Cuturi, et al. Computational optimal transport. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019. 123. S. Pu and A. Nedic´. Distributed stochastic gradient tracking methods. Mathematical Programming, 187(1):409–457, 2021. 124. G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems,
5(3):1245–1260, 2017. 125. G. Qu and N. Li. Accelerated distributed nesterov gradient descent. IEEE Transactions on Automatic Control, 2019. 126. P. Rigollet and J. Weed. Entropic optimal transport is maximum-likelihood deconvolution. Comptes Rendus Mathematique, 356(11-
12):1228–1235, 2018. 127. H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400–407, 1951. 128. R. T. Rockafellar. Convex analysis. Princeton university press, 2015. 129. A. Rogozin, A. Beznosikov, D. Dvinskikh, D. Kovalev, P. Dvurechensky, and A. Gasnikov. Decentralized distributed optimization for
saddle point problems. arXiv preprint arXiv:2102.07758, 2021. 130. A. Rogozin, M. Bochko, P. Dvurechensky, A. Gasnikov, and V. Lukoshkin. An accelerated method for decentralized distributed stochastic
optimization over time-varying graphs. Conference on decision and control, 2021. 131. A. Rogozin and A. Gasnikov. Projected gradient method for decentralized optimization over time-varying networks. arXiv preprint
arXiv:1911.08527, 2019. 132. A. Rogozin and A. Gasnikov. Penalty-based method for decentralized optimization over time-varying graphs. In International Conference
on Optimization and Applications, pages 239–256. Springer, 2020. 133. A. Rogozin, V. Lukoshkin, A. Gasnikov, D. Kovalev, and E. Shulgin. Towards accelerated rates for distributed optimization over time-
varying networks. In International Conference on Optimization and Applications, pages 258–272. Springer, 2021. 134. K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulie´. Optimal algorithms for smooth and strongly convex distributed optimization
in networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3027–3036. JMLR. org, 2017. 135. K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulie´. Optimal convergence rates for convex distributed optimization in networks.
Journal of Machine Learning Research, 20(159):1–31, 2019.

46

E. Gorbunov, A. Rogozin, A. Beznosikov, D. Dvinskikh, A. Gasnikov

136. K. Scaman, F. Bach, S. Bubeck, L. Massoulie´, and Y. T. Lee. Optimal algorithms for non-smooth distributed optimization in networks. In Advances in Neural Information Processing Systems, pages 2740–2749, 2018.
137. M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming, 162(1-
2):83–112, 2017. 138. S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. 139. S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT, 2009. 140. O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning
Research, 18:52:1–52:11, 2017. First appeared in arXiv:1507.08752. 141. O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning
Research, 18(52):1–11, 2017. 142. W. Shi, Q. Ling, G. Wu, and W. Yin. Extra: An exact ﬁrst-order algorithm for decentralized consensus optimization. SIAM Journal on
Optimization, 25(2):944–966, 2015. 143. Z. Song, L. Shi, S. Pu, and M. Yan. Optimal gradient tracking for decentralized optimization. arXiv preprint arXiv:2110.05282, 2021. 144. Z. Song, L. Shi, S. Pu, and M. Yan. Provably accelerated decentralized gradient method over unbalanced directed graphs. arXiv preprint
arXiv:2107.12065, 2021. 145. V. Spokoiny et al. Parametric estimation. ﬁnite sample theory. The Annals of Statistics, 40(6):2877–2909, 2012. 146. I. Stepanov, A. Voronov, A. Beznosikov, and A. Gasnikov. One-point gradient-free methods for composite optimization with applications
to distributed optimization. arXiv preprint arXiv:2107.05951, 2021. 147. S. U. Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018. 148. S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsiﬁed sgd with memory. In Advances in Neural Information Processing Systems, pages
4447–4458, 2018. 149. F. Stonyakin, D. Dvinskikh, P. Dvurechensky, A. Kroshnin, O. Kuznetsova, A. Agafonov, A. Gasnikov, A. Tyurin, C. A. Uribe, D. Pasech-
nyuk, et al. Gradient methods for problems with inexact model of the objective. arXiv preprint arXiv:1902.09001, 2019. 150. F. Stonyakin, A. Tyurin, A. Gasnikov, P. Dvurechensky, A. Agafonov, D. Dvinskikh, M. Alkousa, D. Pasechnyuk, S. Artamonov, and
V. Piskunova. Inexact model: A framework for optimization and variational inequalities. Optimization Methods and Software, pages
1–47, 2021. 151. Y. Sun, A. Daneshmand, and G. Scutari. Convergence rate of distributed optimization algorithms based on gradient tracking. arXiv
preprint arXiv:1905.02637, 2019. 152. Y. Sun, A. Daneshmand, and G. Scutari. Distributed optimization based on gradient-tracking revisited: Enhancing convergence rate via
surrogation. arXiv preprint arXiv:1905.02637, 2020. 153. Y. Tang, J. Zhang, and N. Li. Distributed zero-order algorithms for nonconvex multi-agent optimization. IEEE Transactions on Control
of Network Systems, 2020. 154. Y. Tian, G. Scutari, T. Cao, and A. Gasnikov. Acceleration in distributed optimization under similarity. arXiv preprint arXiv:2110.12347,
2021. 155. V. Tominin, Y. Tominin, E. Borodich, D. Kovalev, A. Gasnikov, and P. Dvurechensky. On accelerated methods for saddle-point problems
with composite structure. arXiv preprint arXiv:2103.09344, 2021. 156. J. N. Tsitsiklis. Problems in decentralized decision making and computation. Technical report, Massachusetts Inst of Tech Cambridge
Lab for Information and Decision Systems, 1984. 157. C. A. Uribe, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and A. Nedic´. Distributed computation of Wasserstein barycenters over
networks. In 2018 IEEE 57th Annual Conference on Decision and Control (CDC), 2018. Accepted, arXiv:1803.02933. 158. C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic´. Optimal algorithms for distributed optimization. arXiv preprint arXiv:1712.00232, 2017. 159. C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic´. A dual approach for optimal algorithms in distributed optimization over networks.
Optimization Methods and Software, pages 1–40, 2020. 160. S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron.
In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1195–1204, 2019. 161. J. von Neumann, O. Morgenstern, and H. Kuhn. Theory of Games and Economic Behavior (commemorative edition). Princeton University
Press, 2007. 162. H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. arXiv
preprint arXiv:1806.00877, 2018. 163. W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: Ternary gradients to reduce communication in distributed deep
learning. In Advances in Neural Information Processing Systems, pages 1509–1519, 2017. 164. B. Woodworth, K. K. Patel, and N. Srebro. Minibatch vs local sgd for heterogeneous distributed learning. arXiv preprint
arXiv:2006.04735, 2020. 165. B. Woodworth, K. K. Patel, S. U. Stich, Z. Dai, B. Bullins, H. B. McMahan, O. Shamir, and N. Srebro. Is local sgd better than minibatch
sgd? arXiv preprint arXiv:2002.07839, 2020. 166. L. Xiao and S. Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):65–78, 2004. 167. J. Xu, Y. Tian, Y. Sun, and G. Scutari. Accelerated primal-dual algorithms for distributed smooth convex optimization over networks.
arXiv preprint arXiv:1910.10666, 2019. 168. J. Yang, S. Zhang, N. Kiyavash, and N. He. A catalyst framework for minimax optimization. Advances in Neural Information Processing
Systems, 2020. 169. H. Ye, L. Luo, Z. Zhou, and T. Zhang. Multi-consensus decentralized accelerated gradient descent. arXiv preprint arXiv:2005.00797,
2020. 170. H. Ye, Z. Zhou, L. Luo, and T. Zhang. Decentralized accelerated proximal gradient descent. Advances in Neural Information Processing
Systems, 33, 2020. 171. H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication efﬁcient momentum sgd for distributed non-convex opti-
mization. arXiv preprint arXiv:1905.03817, 2019. 172. K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835–1854,
2016. 173. K. Zhou. Direct acceleration of saga using sampled negative momentum. arXiv preprint arXiv:1806.11048, 2018. 174. K. Zhou, F. Shang, and J. Cheng. A simple stochastic variance reduced algorithm with fast convergence rates. arXiv preprint
arXiv:1806.11027, 2018.

