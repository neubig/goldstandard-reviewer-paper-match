arXiv:1806.06266v3 [cs.GT] 1 Feb 2020

On Strategyproof Conference Peer Review

Yichong Xu˚ Machine Learning Department Carnegie Mellon University, Pittsburgh, PA, USA
Han Zhao˚ Machine Learning Department Carnegie Mellon University, Pittsburgh, PA, USA
Xiaofei Shi Department of Mathematical Sciences Carnegie Mellon University, Pittsburgh, PA, USA
Jeremy Zhang Computer Science Department Carnegie Mellon University, Pittsburgh, PA, USA
Nihar B. Shah Machine Learning Department and Computer Science Department Carnegie Mellon University, Pittsburgh, PA, USA

yichongx@cs.cmu.edu han.zhao@cs.cmu.edu xiaofeis@andrew.cmu.edu jbz@andrew.cmu.edu
nihars@cs.cmu.edu

Abstract
We consider peer review in a conference setting where there is typically an overlap between the set of reviewers and the set of authors. This overlap can incentivize strategic reviews to inﬂuence the ﬁnal ranking of one’s own papers. In this work, we address this problem through the lens of social choice, and present a theoretical framework for strategyproof and eﬃcient peer review. We ﬁrst present and analyze an algorithm for reviewer-assignment and aggregation that guarantees strategyproofness and a natural eﬃciency property called unanimity, when the authorship graph satisﬁes a simple property. Our algorithm is based on the so-called partitioning method, and can be thought as a generalization of this method to conference peer review settings. We then empirically show that the requisite property on the authorship graph is indeed satisﬁed in the submission data from the ICLR conference, and further demonstrate a simple trick to make the partitioning method more practically appealing for conference peer review. Finally, we complement our positive results with negative theoretical results where we prove that under various ways of strengthening the requirements, it is impossible for any algorithm to be strategyproof and eﬃcient.
1. Introduction
Peer review serves as an eﬀective solution for quality evaluation in reviewing processes, especially in academic paper review (Dörﬂer et al., 2017; Shah et al., 2017) and massive open online courses (MOOCs) (Díez Peláez et al., 2013; Piech et al., 2013; Shah et al., 2013). However, despite its scalability, competitive peer review faces the serious challenge of being vulnerable to strategic manipulations (Alon et al., 2011; Anderson et al., 2007; Kahng et al., 2017; Kurokawa et al., 2015; Thurner and Hanel, 2011). By giving lower scores to competitive
∗. Equal contribution.

Xu, Zhao, Shi, Zhang & Shah
submissions, reviewers may be able to increase the chance that their own submissions get accepted. For instance, a recent experimental study (Balietti et al., 2016) on peer review of art, published in the Proceedings of the National Academy of Sciences (USA), concludes
“...competition incentivizes reviewers to behave strategically, which reduces the fairness of evaluations and the consensus among referees.”
As noted by Thurner and Hanel (2011), even a small number of selﬁsh, strategic reviewers can drastically reduce the quality of scientiﬁc standard. In the context of conference peer review, Langford (2008) calls academia inherently adversarial:
“It explains why your paper was rejected based on poor logic. The reviewer wasn’t concerned with research quality, but rather with rejecting a competitor.”
Langford states that a number of people agree with this viewpoint. Thus the importance of peer review in academia and its considerable inﬂuence over the careers of researchers signiﬁcantly underscores the need to design peer review systems that are insulated from strategic manipulations.
In this work, we present a higher-level framework to address the problem of strategic behavior in conference peer review. We present an informal description of the framework here and formalize it later in the paper. The problem setting comprises a number of submitted papers and a number of reviewers. We are given a graph which we term as the “conﬂict graph”. The conﬂict graph is a bipartite graph with the reviewers and papers as the two partitions of vertices, and an edge between any reviewer and paper if that reviewer has a conﬂict with that paper. Conﬂicts may arise due to authorship (the reviewer is an author of the paper) or other reasons such as being associated to the same institution. Given this conﬂict graph, there are two design steps in the peer review procedure: (i) assigning each paper to a subset of reviewers, and (ii) aggregating the reviews provided by the reviewers to give a ﬁnal evaluation of each paper. Under our framework, the goal is to design these two steps of the peer-review procedure that satisﬁes two properties – strategyproofness and eﬃciency.
Our goal is to design peer-review procedures that are strategyproof with respect to the given conﬂict graph. A peer-review procedure is said to be strategyproof if no reviewer can change the outcome for any paper(s) with which she/he has a conﬂict. This deﬁnition is formalized later in the paper. Strategyproofness not only reassures the authors that the review process is fair, but also ensures that the authors receive proper feedback for their work. We note that a strategyproof peer-review procedure alone is inadequate with respect to any practical requirements – simply giving out a ﬁxed, arbitrary evaluation makes the peer-review procedure strategyproof.
Consequently, in addition to requiring strategyproofness, our framework measures the peer-review procedure with another yardstick – that of eﬃciency. Informally, the eﬃciency of a peer-review procedure is a measurement of how well the ﬁnal outcome reﬂects reviewers’ assessments of the quality of the submissions, or a measurement of the accuracy in terms of the ﬁnal acceptance decisions. There are several ways to deﬁne eﬃciency – from a social choice perspective or a statistical perspective. In this paper, we consider eﬃciency in terms of the notion of unanimity in social choice theory: an agreement among all reviewers must be reﬂected in the ﬁnal aggregation.
2

On Strategyproof Conference Peer Review
In addition to the conceptual contribution based on this framework, we make several technical contributions towards this important problem. We ﬁrst design a peer review algorithm which theoretically guarantees strategyproofness along with a notion of eﬃciency that we term “group unanimity”. Our result requires only a mild assumption on the conﬂict graph of the peer-review design task. We show this assumption indeed holds true in practice via an empirical analysis of the submissions made to the International Conference on Learning Representations (ICLR) conference1. Our algorithm is based on the popular partitioning method, and our positive results can be regarded as generalizing it to the setting of conference peer review. We further demonstrate a simple trick to make the partitioning method more practically appealing for conference peer review and validate it on the ICLR data.
We then complement our positive results with negative results showing that one cannot expect to meet requirements that are much stronger than that provided by our algorithm. In particular, we show that under mild assumptions on the authorships, there is no algorithm that can be both strategyproof and “pairwise unanimous”. Pairwise unanimity is a stronger notion of eﬃciency than group unanimity, and is also known as Pareto eﬃciency in the literature of social choice (Brandt et al., 2016). We show that our negative result continues to hold even when the notion of strategyproofness is made extremely weak. We then provide a conjecture and insightful results on the impossibility when the assignment satisﬁes a simple “connectivity” condition. Finally, we connect back to the traditional settings in social choice theory, and show an impossibility when every reviewer reviews every paper. These negative results highlight the intrinsic hardness in designing strategyproof conference review systems.
2. Related Work
As early as in the 1970s, Gibbard and Satterthwaite had already been aware of the importance of a healthy voting rule that is strategyproof in the setting of social choice (Gibbard, 1973; Satterthwaite, 1975). Nowadays, the fact that prominent peer review mechanisms such as the one used by the National Science Foundation (Hazelrigg, 2013) and the one for time allocation on telescope (Merriﬁeld and Saari, 2009) are manipulable has further called for strategyproof peer review mechanisms.
Our work is most closely related to a series of works on strategyproof peer selection (Alon et al., 2011; Aziz et al., 2016, 2019; De Clippel et al., 2008; Fischer and Klimm, 2015; Holzman and Moulin, 2013; Kahng et al., 2017; Kurokawa et al., 2015), where agents cannot beneﬁt from misreporting their preferences over other agents.2 De Clippel et al. (2008) consider strategyproof decision making under the setting where a divisible resource is shared among a set of agents. Later, Alon et al. (2011); Holzman and Moulin (2013) consider strategyproof peer approval voting where each agent nominates a subset of agents and the goal is to select one agent with large approvals. Alon et al. (2011) propose a randomized strategyproof mechanism using partitioning that achieves provable approximate guarantee to the deterministic but non-strategyproof mechanism that simply selects the agent with
1. https://openreview.net/group?id=ICLR.cc/2017/conference 2. Some past literature refers to this requirement as ensuring that agents are “impartial”. However, the term
“impartial” also has connotations on (possibly implicit) biases due to extraneous factors such as some features about the agents (Hojat et al., 2003). In this paper, we deliberately use the term “strategyproof” in order to make the scope of our contribution clear in that we do not address implicit biases.
3

Xu, Zhao, Shi, Zhang & Shah
maximum approvals. Bousquet et al. (2014) and Fischer and Klimm (2015) further extended and analyzed this mechanism to provide an optimal approximate ratio in expectation. Although the ﬁrst partitioning-based mechanism partitions all the voters into two disjoint subsets, this has been recently extended to k-partition by Kahng et al. (2017). In all these works, each agent is essentially required to evaluate all the other agents except herself. This is impractical for conference peer review, where each reviewer only has limited time and energy to review a small subset of submissions. In light of such constraints, Kurokawa et al. (2015) propose an impartial mechanism (Credible Subset) and provide associated approximation guarantees for a setting in which each agent is only required to review a few other agents. Credible Subset is a randomized mechanism that outputs a subset of k agents, but it has non-zero probability returns an empty set. Based on the work of De Clippel et al. (2008), Aziz et al. (2016) propose a mechanism for peer selection, termed as Dollar Partition, which is strategyproof and satisﬁes a natural monotonicity property. Empirically the authors showed that Dollar Partition outperforms Credible Subset consistently and in the worst case is better than partition-based approach. However, even if the target output size is k, Dollar Partition may return a subset of size strictly larger than k. This problem has recently been ﬁxed by the Exact Dollar Partition mechanism (Aziz et al., 2019), which empirically selects more high-quality agents more often and consistently than Credible Subset. Our positive results, speciﬁcally our Divide-and-Rank algorithm presented subsequently, borrows heavily from this line of literature. That said, our work addresses the application of conference peer review which is more general and challenging as compared to the settings considered in past works.
Our setting of conference peer review is more challenging as compared to these past works as each reviewer may author multiple papers and moreover each paper may have multiple authors as reviewers. Speciﬁcally, the conﬂict graph under conference peer review is a general bipartite graph, where conﬂicts between reviewers and papers can arise not only because of authorships, but also advisor-advisee relationships, institutional conﬂicts, etc. In contrast, past works focus on applications of peer-grading and grant proposal review, and hence consider only one-to-one conﬂict graphs (that is, where every reviewer is conﬂicted with exactly one paper).
Apart from the most important diﬀerence mentioned above, there are a couple of other diﬀerences of this work as compared to some past works. In this paper we focus on ordinal preferences where each reviewer is asked to give a total ranking of the assigned papers, as opposed to providing numeric ratings. We do so inspired by past literature (Barnett, 2003; Douceur, 2009; Shah et al., 2016, 2013; Stewart et al., 2005; Tsukida and Gupta, 2011) which highlights the beneﬁts of ordinal data in terms of avoiding biases as well as allowing for a more direct comparison between papers. Secondly, while most previous mechanisms either output a single paper or a subset of papers, we require our mechanism to output a total ranking over all papers. We consider this requirement since this automated output in practice will be used by the program chairs as a guideline to make their decisions, and this more nuanced data comprising the ranking of the papers can be more useful towards this goal.
A number of papers study various other aspects of conference peer review, and we mention the most relevant ones here. Several works (Charlin and Zemel, 2013; Garg et al., 2010; Hartvigsen et al., 1999; Stelmakh et al., 2019b) design algorithms for assigning reviewers to papers under various objectives, and these objectives and algorithms may in fact be used as alternative deﬁnitions of the objective of “eﬃciency” studied in the present paper. The
4

On Strategyproof Conference Peer Review
papers Ge et al. (2013); Roos et al. (2011); Wang and Shah (2019) consider review settings where reviewers provide scores to each paper, with the aim of addressing the problems of miscalibration in these scores. Stelmakh et al. (2019a); Tomkins et al. (2017) study biases in peer review, Noothigattu et al. (2018) address issues of subjectivity, Gao et al. (2019) investigate rebuttals, and Fiez et al. (2019) improve the eﬃciency of the bidding process. Experiments and empirical evaluations of conference peer reviews can be found in Connolly et al. (2014); Gao et al. (2019); Lawrence and Cortes (2014); Mathieus (2008); Noothigattu et al. (2018); Shah et al. (2017); Tomkins et al. (2017).
3. Problem setting
In this section, we ﬁrst give a brief introduction to the setting of our problem, and then introduce the notation used in the paper. We then formally deﬁne various concepts and properties to be discussed in the subsequent sections.
Modern review process is governed by four key steps: (i) a number of papers are submitted for review; (ii) each paper is assigned to a set of reviewers; (iii) reviewers provide their feedback on the papers they are reviewing; and (iv) the feedback from all reviewers is aggregated to make ﬁnal decisions on the papers. Let m be the number of reviewers and n be the number of submitted papers. Deﬁne R :“ tr1, . . . , rmu to be the set of m reviewers and P :“ tp1, . . . , pnu to be the set of n submitted papers.
The review process must deal with conﬂicts of interest. To characterize conﬂicts of interest, we use a bipartite graph C with vertices pR, Pq, where an edge is connected between a reviewer r and a paper p if there exists some conﬂict of interests between reviewer r and paper p. Reviewers who do not have conﬂicts of interest with any paper are nodes with no edges. Given the set of submitted papers and reviewers, this graph is ﬁxed and cannot be controlled. Note that the conﬂict graph C deﬁned above can be viewed as a generalization of the authorship graph in the previously-studied settings (Alon et al., 2011; Aziz et al., 2016; Fischer and Klimm, 2015; Holzman and Moulin, 2013; Kahng et al., 2017; Kurokawa et al., 2015; Merriﬁeld and Saari, 2009) of peer grading and grant proposal review, where each reviewer (paper) is connected to at most one paper (reviewer).
The review process is modeled by a second bipartite graph G, termed as review graph, that also has the reviewers and papers pR, Pq as its vertices. This review graph has an edge between a reviewer and a paper if that reviewer reviews that paper. For every reviewer ri pi P rmsq,3 we let Pi Ď P denote the set of papers assigned to this reviewer for review, or in other words, the neighborhood of node ri in the bipartite graph G. The program chairs of the conference are free to choose this graph, but subject to certain constraints and preferences. To ensure balanced workloads across reviewers, we require that every reviewer is assigned at most µ papers for some integers 1 ď µ ď n. In other words, every node in R has at most µ neighbors (in P) in graph G. Additionally, each paper must be reviewed by a certain minimum number of reviewers, and we denote this minimum number as λ. Thus every node in the set P must have at least λ neighbors (in R) in the graph G. For any (directed or undirected) graph H, we let the notation EH denote the set of (directed or undirected, respectively) edges in graph H.
3. We use the standard notation rκs to represent the set t1, . . . , κu for any positive integer κ.
5

Xu, Zhao, Shi, Zhang & Shah

At the end of the reviewing period, each reviewer provides a total ranking of the papers

that she/he reviewed. For any set of papers P1 Ď P, we let ΠpP1q denote the set of all

permutations of papers in P1. Furthermore, for any paper pj P P1 and any permutation
πpP1q P ΠpP1q, we let πjpP1q denote the position of paper pj in the permutation πpP1q. At the end of the reviewing period, each reviewer ri pi P rmsq submits a total ranking πpiqpPiq P ΠpPiq of the papers in Pi. We deﬁne a (partial) ranking proﬁle π :“ pπp1qpP1q, . . . , πpmqpPmqq as

the collection of rankings from all the reviewers. When the assignment P1, . . . , Pm of papers to reviewers is ﬁxed, we use the shorthand pπp1q, . . . , πpmqq for proﬁle π. For any subset of

papers P1 Ď P, we let πP1 denote the restriction of π to only the induced rankings on P1.

Finally, when the ranking under consideration is clear from context, we use the notation

p ą p1 to say that paper p is ranked higher than paper p1 in the ranking.

Under this framework, the goal is to jointly design: (a) a paper-reviewer assignment

scheme, that is, edges of the graph G, and (b) an associated review aggregation rule

f

:

śm
i“1

ΠpPiq

Ñ

ΠpP q

which

maps

from

the

ranking

proﬁle

to

an

aggregate

total

ranking

of all papers.4 For any aggregation function f , we let fjpπq be the position of paper pj when

the input to f is the proﬁle π.

We note that although we assume ordinal feedback from the reviewers, our results continue

to hold if we have review scores as our input instead of rankings; our framework is ﬂexible

enough to take the scores into account (cf. Section 4.1).

In what follows we deﬁne strategyproofness and eﬃciency that any conference review

mechanism f should satisfy under our paper-review setting. Inspired by the theory of social

choice, in this paper we deﬁne the notion of eﬃciency via two variants of “unanimity”, and

we also discuss two natural notions of strategyproofness.

3.1 Strategyproofness
Intuitively, strategyproofness means that a reviewer cannot beneﬁt from being dishonest. In the context of conference review, strategyproofness is deﬁned with respect to a given conﬂict graph C; we recall the notation EC as the set of edges of graph C. It means that a reviewer cannot change the position of her conﬂicting papers, by manipulating the ranking she provides.
Deﬁnition 3.1 (Strategyproofness, SP). A review process pG, f q is called strategyproof with respect to a conﬂict graph C if for every reviewer ri P R and paper pj P P such that pri, pjq P EC the following condition holds: for every pair of proﬁles (under assignment G) that diﬀer only in the ranking given by reviewer ri, the position of pj is unchanged.5 Formally, @π “ pπp1q, . . . , πpi´1q, πpiq, πpi`1q, . . . , πpmqq and π1 “ pπp1q, . . . , πpi´1q, πpiq1, πpi`1q, . . . , πpmqq, it must be that fjpπq “ fjpπ1q.
A strategyproof peer review procedure alone is inadequate with respect to any practical requirements – simply giving out a ﬁxed, arbitrary evaluation makes the peer review procedure
4. To be clear, the function f is tied to the assignment graph G. The graph G speciﬁes the sets pP1, . . . , Pmq, and then the function f takes permutations of these sets of papers as its inputs. We omit this from the notation for brevity.
5. A related (and weaker) deﬁnition of strategyproof is that the position of any pj cannot be improved. It is easy to show that any mechanism that satisﬁes the weaker notion can also satisfy our notion of strategyproofness.

6

On Strategyproof Conference Peer Review
strategyproof. We therefore consider eﬃciency of the procedure in the next section, to ensure that the authors receive meaningful and helpful feedback for their work.
3.2 Eﬃciency (unanimity)
Consequently, in addition to requiring strategyproofness, we measure the peer review procedure with another yardstick – eﬃciency. The peer review procedure needs to not only reassure the authors that the review process is fair, but also ensure that the authors receive proper feedback for their work in an eﬃcient way.
In this work, we consider eﬃciency of a peer-review process in terms of the notion of unanimity. Unanimity is one of the most prevalent and classic properties to measure the eﬃciency of a voting system in the theory of social choice (Fishburn, 2015). At a colloquial level, unanimity states that when there is a common agreement among all reviewers, then the aggregation of their opinions must also respect this agreement. In this paper we discuss two kinds of unanimity, termed group unanimity (GU) and pairwise unanimity (PU). Both kinds of unanimity impose requirements on the aggregation function for any given reviewer assignment.
We ﬁrst deﬁne group unanimity:
Deﬁnition 3.2 (Group Unanimity, GU). We deﬁne pG, f q to be group unanimous (GU) if the following condition holds for every possible proﬁle π. If there is a non-empty set of papers P1 Ă P such that every reviewer ranks the papers she reviewed from P1 higher than those she reviewed from PzP1, then f pπq must have px ą py for every pair of papers px P P1 and py P PzP1 such that at least one reviewer has reviewed both px and py.
Intuitively, group unanimity says that if papers can be partitioned into two sets such that every reviewer who has reviewed papers from both sets agrees that the papers she has reviewed from the ﬁrst set are better than what she reviewed from the second set, then the ﬁnal output ranking should respect this agreement.
Our second notion of unanimity, termed pairwise unanimity, is a local reﬁnement of group unanimity. This notion is identical to the classical notion of unanimity stated in Arrow’s impossibility theorem (Arrow, 1950) – the classical unanimity considers every reviewer to review all papers (that is, Pi “ P, @i P rms), whereas our notion is also deﬁned for settings where reviewers may review only subsets of papers.
Deﬁnition 3.3 (Pairwise Unanimity, PU). We deﬁne pG, f q to be pairwise unanimous (PU) if the following condition holds for every possible proﬁle π and every pair of papers pj1, pj2 P P: If at least one reviewer has reviewed both pj1 and pj2 and all the reviewers that have reviewed pj1 and pj2 agree on pj1 ą pj2, then fj1pπq ą fj2pπq.
An important property is that pairwise unanimity is stronger than group unanimity.
Proposition 3.4. If pG, f q is pairwise unanimous, then pG, f q is also group unanimous.
The proof of this proposition is provided in Section 7.1.
7

Xu, Zhao, Shi, Zhang & Shah
4. Positive Theoretical Results and Algorithm
In this section we consider the design of reviewer assignments and aggregation rules for strategyproofness and group unanimity (eﬃciency). It is not hard to see that strategyproofness and group unanimity cannot be simultaneously guaranteed for arbitrary conﬂict graphs C, for instance, when C is a fully-connected bipartite graph. Prior works on this topic consider a speciﬁc class of conﬂict graphs — those with one-to-one relations between papers and reviewers — which do not capture conference peer review settings. We consider a more general class of conﬂict graphs and present an algorithm based on the partitioning-based method (Alon et al., 2011), which we show can achieve group unanimous and strategyproofness.
We then empirically demonstrate, using submission data from the ICLR conference, that this class of conﬂict graphs is indeed representative of peer review settings. We observe that the quality of the reviewer assignment under our method (that guarantees strategyproofness) is only slightly lower as compared to the optimal quality in the absence of strategyprooﬁng requirements. Finally, we present a simple trick to signiﬁcantly improve the practical appeal of our algorithm (and more generally the partitioning method) to conference peer review.
4.1 The Divide-and-Rank Algorithm
We now present our “Divide-and-Rank” framework consisting of the reviewer assignment algorithm (Algorithm 1) and the rank aggregation algorithm (Algorithm 2). At a high level, our algorithm performs a partition of the reviewers and papers for assignment, and aggregates the reviews by computing a ranking which is consistent with any group agreements. The Divide-and-Rank algorithm works for a general conﬂict graph C as long as the conﬂict graph can be divided into two reasonably-sized disconnected components.
Importantly, the framework is simple yet ﬂexible in that the assignment within each partition and the aggregation among certain groups of papers can leverage any existing algorithm for assignment and aggregation respectively, which is useful as it allows to further optimize various other metrics in addition to strategyproofness and unanimity.
Below we describe our framework in more detail. We ﬁrst introduce the assignment procedure in Algorithm 1.
The Divide-and-Rank assignment algorithm begins by partitioning the conﬂict graph into two disconnected components such that (1) they meet the requirements speciﬁed by µ and λ; and (2) the two disconnected components have roughly equal size in terms of number of nodes. This is achieved using the subroutine Partition. In more detail, Partition ﬁrst runs a breadth-ﬁrst-search (BFS) algorithm to partition the original conﬂict graph into K connected components, where the kth connected component contains rk ě 0 reviewers and pk ě 0 papers. Next, the algorithm performs a dynamic programming to compute all the possible subset sums, i.e., sum of the number of reviewers and the number of papers in a given subset, achievable by the K connected components. Here T rk, r, ps “ 1 means that there exists a partition of the ﬁrst k components such that one side of the partition has r reviewers and p papers, and 0 otherwise. The last step is to check whether there exists a subset C satisfying the constraint given by λ and µ, and if so, runs a standard backtracking algorithm along the table to ﬁnd the actual subset C. Clearly the Partition runs in OpKnmq, and since K ď nm, it runs in polynomial time in the size of the input conﬂict graph C.
8

On Strategyproof Conference Peer Review

Algorithm 1 Divide-and-Rank assignment

Input: conﬂict graph C, parameters λ, µ, assignment algorithm A Output: an assignment of reviewers to papers
1: pRC , PC q, pRC , PC q Ð PartitionpC, λ, µq
ss
2: use algorithm A to assign papers PC to reviewers RC
s
3: use algorithm A to assign papers PC to reviewers RC
s
4: return the union of assignments from step 2 and 3

5:

6: procedure Partition(conﬂict graph C, parameters λ, µ)

7:

run

a

BFS

on

C

to

get

connected

K

components

tp

Rk

,

Pk

qu

K k“

1

8: let rk “ |Rk|, pk “ |Pk|, @k P rKs

9: initialize a table T r¨, ¨, ¨s P t0, 1uKˆpm`1qˆpn`1q so that T r1, r1, p1s “ T r1, 0, 0s “ 1,

otherwise 0

10: for k “ 2 to K do

11:

T rk, r, ps “ T rk ´ 1, r, ps _ T rk ´ 1, r ´ rk, p ´ pks, @0 ď r ď m, 0 ď p ď n

12: end for
13: for 0 ď r ď m, 0 ď p ď n, if there is no T rK, r, ps “ 1 such that maxt mp´r , n´r p u ď µλ , return error

14: use the standard backtracking in the table T r¨, ¨, ¨s to return pRC, PCq and pRC, PCq
ss

15: end procedure

In the next step, the algorithm assigns papers to reviewers in a fashion that guarantees

each paper is going to be reviewed by at least λ reviewers and each reviewer reviews at most

µ papers. The assignment of papers in any individual component (to reviewers in the other

component) can be done using any assignment algorithm (taken as an input A) as long as the

algorithm can satisfy the pµ, λq-requirements. Possible choices for the algorithm A include

the popular Toronto paper matching system (Charlin and Zemel, 2013) and others (Garg

et al., 2010; Hartvigsen et al., 1999; Stelmakh et al., 2019b). We can also use the typical

reviewer bidding system, while constraining the reviewers in RC to review PC and RC to

s

s

review PC .

We then introduce to the aggregation procedure in Algorithm 2. At a high level, the

papers in each component are aggregated separately using the subroutine Contract-and-

Sort. This aggregation in Contract-and-Sort is performed by a topological ordering of all

strongly connected components (SCCs) according to the reviews, and then ranking the

papers within each set using any arbitrary aggregation algorithm (taken as an input B).6

Possible choices for the algorithm B include the modiﬁed Borda count (Emerson, 2013),

Plackett-Luce aggregation (Hajek et al., 2014), or others (Caragiannis et al., 2017) Moving

back to Algorithm 2, the two rankings returned by Contract-and-Sort respectively for the

two components are simply interlaced to obtain a total ranking over all the papers: the slots

for C are reserved in set I, and rnszI contain the slots for the remaining papers. In our

extended version of the paper we also show that the interleaving only causes a small change

w.r.t an underlying optimal ranking.

The following theorem now shows that Divide-and-Rank satisﬁes group unanimity and is

also strategyproof, detailed proof is in Section 7.2.

6. In the case where there are multiple topological orderings, any one of them suﬃces.

9

Xu, Zhao, Shi, Zhang & Shah

Algorithm 2 Divide-and-Rank aggregation

Input: proﬁle π “ pπp1qpP1q, . . . , πpmqpPmqq, groups pRC , PC q, pRC , PC q with |PC | ě |PC |,

ss

s

aggregation algorithm B

Output: total ranking of all papers

1: compute πC as the restriction of proﬁle π to only papers in PC, and πC¯ as the restriction of proﬁle π to only papers in PC
s

2: πC Ð Contract-and-SortpπC , Bq

3: πC Ð Contract-and-SortpπC¯, Bq
s
´Y ] Y ] ¯ 4: deﬁne I “ |PnC| , |P2nC| , ..., n 5: return total ranking obtained by ﬁlling papers in PC into positions in I in order given

by πC, and papers in PC into positions in rnszI in order given by πC

s

s

6:

7: procedure Contract-and-Sort(proﬁle π, aggregation algorithm B) r

8:

build

a

directed

graph

Gπ

with

the

papers

in

π r

as

its

vertices

and

no

edges

r

9: for each i P rm1s do

10:

denoting πpiq “ ppi1 ą . . . ą pit ), add a directed edge from pij to pij`1 in Gπ,

i

r

@j P rti ´ 1s

11: end for

12: for every ordered pair ppj1, pj2q P EGπ , replace multiple edges from pj1 to pj2 with a r single edge

13: compute a topological ordering of the strongly connected components (SCCs) in Gπ
r
14: for every SCC in Gπ, compute a permutation of the papers in the component using
r
algorithm B

15: return the permutation of all papers that is consistent with the topological ordering

of the SCCs and the permutations within the SCCs

16: end procedure

Theorem 4.1. Suppose the vertices of C can be partitioned into two groups pRC, PCq and

pRC , PC q such that there are no edges in C across the groups and that max ||RPC|| , ||RPCs || ( ď µλ .

ss

Cs

C

Then Divide-and-Rank is group unanimous and strategyproof.

Remark. Our Divide-and-Rank framework aptly handles the various nuances of real-world conferences peer review, which render other algorithms inapplicable. This includes the aspects that each reviewer can write multiple papers and each paper can have multiple authors, and furthermore that each reviewer may review only a subset of papers. Even under this challenging setting, our algorithm guarantees that no reviewer can inﬂuence the ranking of her own paper via strategic behavior, and it is eﬃcient from a social choice perspective.
Further, we delve a little deeper into the interleaving step (Step 5) of the aggregation algorithm. At ﬁrst glance, this interleaving – performed independent of the reviewers’ reports – may be a cause of concern. Indeed, assuming there is some ground truth ranking of all papers and even under the assumption that the outputs of the Contract-and-Sort procedure are consistent with this ranking, the worst case scenario is where the interleaving causes papers to be placed at a positions that are Θpnq away from their respective positions in the true ranking. We show that, however, such a worst case scenario is unlikely to arise, when
10

On Strategyproof Conference Peer Review

the ground truth ranking is independent of the conﬂict graph. We summarize our ﬁndings in the following proposition, with the proof provided later in Section 7.3.

Proposition 4.2. Suppose C satisﬁes the conditions given in Theorem 4.1 and there exists a constant c ě 2 such that maxt |P|PC|| , |P|PC|| u ď c. Assume the ground-truth ranking π˚ is
s
chosen uniformly at random from all permutations in ΠpPq independent of C, and that the two partial outputs of Contract-and-Sort in Algorithm 2 respect π˚. Let the output ranking
of Divide-and-Rank be π. Then for every n ě 4c{ log 2, for any δ P p0, 1q, with probability at p
least 1 ´ δ, we have:

max

|πi˚

´

πi|

ď

a 2 nc

¨

logp2n{δq.

1ďiďn

p

Proposition 4.2 shows that the maximum deviation between the aggregated ranking and the a
ground truth ranking is Op n logpn{δqq with high probability. Hence when n is large enough, such deviation is negligible when program chairs of conferences need to make accept/reject decisions, where the number of accepted papers usually scales linearly with n.
Extension to review scores. Our framework extends to a score-based setting, wherein each reviewer ri provides their opinion as a score oij for every paper pj P Pi. The assignment algorithm remains the same in this setting; for aggregation, we can use the same procedure with the ranking induced by the review scores. The only diﬀerence is that in step 10 of Contract-and-Sort, we add an edge between every pair of papers pj1 Ñ pj2 such that oij1 ą oij2. This makes sure that the graph reﬂects the opinion of the reviewer and does not impose constraints on papers that are equally rated. In the score-based setting, the aggregation algorithm B is allowed to leverage the review scores for a more granularized ranking (e.g., mean scores).

5. Empirical evaluations
In this section, we perform certain empirical evaluations regarding the feasibility and performance of our Divide-and-Rank algorithm based on data from the ICLR conference7. Recall that the Divide-and-Rank algorithm restricts the assignment of reviewers to papers according to a partition of reviewers and papers into two disconnected groups. By means of these empirical evaluations, we investigate the following questions:
Q1. Is such a partition feasible?
Q2. How can one impart more ﬂexibility to the partition (which can allow for better assignments)?
Q3. How does the quality of the assignment compare with standard settings without strategyproofness?
Q4. One may envisage that reviewers that are more related to the topic of a paper would be more likely to be connected (in the conﬂict graph) to that paper. Under Divide-and-Rank,
7. The code and data is available at https://github.com/xycforgithub/StrategyProof_Conference_ Review.

11

Xu, Zhao, Shi, Zhang & Shah

such a reviewer will be barred from being assigned to such a related paper. How much does such a restriction of the assignment between connected reviewers-papers hurt the assignment quality as compared to assignment under a uniform random partition of reviewers and papers?
The most prominent type of conﬂicts is authorships, and throughout this section we restrict attention to the authorship conﬂict graph.

5.1 Analysis of the Conﬂict Graph on ICLR 2017 submissions (Q1 and Q2)
We address questions Q1 and Q2 using data from the ICLR 2017 conference. In a nutshell:
A1. Yes, partitioning is feasible.
A2. We show that removing only a small number of reviewers can result in a dramatic reduction in the size of the largest component in the conﬂict graph thereby providing great ﬂexibility towards partitioning the papers and reviewers. For instance, removing only 3.5% of all authors from the reviewer pool reduces the size of the largest component (in terms of number of papers) by 86%.
We analyze all papers submitted to the ICLR 2017 conference with the given authorship relationship as the conﬂict graph. ICLR 2017 received 489 submissions by 1417 authors; we believe this dataset is a good representative of a medium-sized modern conference. In the analysis of this dataset, we instantiate the conﬂict graph as the authorship graph. It is important to note that we consider only the set of authors as the entire reviewer pool (since we do not have access to the actual reviewer identities). Adding reviewers from outside the set of authors would only improve the results since these additional reviewers will have no edges in the authorship conﬂict graph.
Table 1: Statistics of ICLR 2017 submissions.

Description
Number of submitted papers Number of distinct authors Mean # papers written per author Maximum # papers written by an author
Number of connected components #authors; #papers in largest connected component #authors; #papers in second largest connected component

Number
489 1417 1.27
14
253 371; 133
65; 20

We ﬁrst investigate the existence of (moderately sized) components in the conﬂict graph. Our analysis shows that the authorship conﬂict graph is disconnected, and moreover, has more than 250 components. The largest connected component (CC) contains 133 (that is, about 27%) of all papers, and the second largest CC is much smaller. We tabulate the results from our analysis in Table 1. These statistics indeed verify our assumption in Theorem 4.1 that the conﬂict graph is disconnected and can be divided into two disconnected parts of similar size.

12

On Strategyproof Conference Peer Review

The partitioning method has previously been considered for the problem of peer grading (Kahng et al., 2017). The peer grading setting is homogeneous in that each reviewer (student) goes through the same course and hence any paper (homework) can be assigned to any reviewer. In peer review, however, diﬀerent reviewers typically have diﬀerent areas of expertise and hence their abilities to review any paper varies by the subject area of the paper. In order to accommodate this diversity in area of expertise in peer review, one must have a greater ﬂexibility in terms of assigning papers to reviewers. In our analysis in Table 1 we saw that the largest connected component comprises 372 authors and 133 papers. It is reasonable to expect that a large number of reviewers with expertise required to review these 133 papers may fall in the same connected component, meaning that a naïve application of Divide-and-Rank to this data would assign these 133 papers to reviewers who may have a lower expertise for these papers. This is indeed a concern, and in what follows, we discuss a simple yet eﬀective way to ameliorate this problem.
A simple yet (as we demonstrate below) eﬀective idea is to remove some authors from the reviewer pool. Empirically using the ICLR 2017 data, we show that by removing only a small number of authors from the reviewer pool, we can make the conﬂict graph considerably more sparse, thereby allowing for a signiﬁcantly more ﬂexible application of our algorithm Divide-and-Rank (or more generally, any partition-based algorithm). We use the simple heuristic of removing the authors with the maximum degree in the (authorship) conﬂict graph. We then study the resulting conﬂict graph (containing all submitted papers but only the remaining reviewers) in terms of the numbers and sizes of the connected components. We present the results in Table 2. We see that on removing only a small fraction of authors — 50 authors which is only about 3.5% of all authors — the number of papers in the largest connected component reduces by 86% to just 18. Likewise, the number of authors in the largest connected component reduces to 55 from 371 originally.
Table 2: Statistics of the conﬂict graph on removing a small number (ă 7%) of authors from the reviewer pool comprising the 1417 authors.

Number of Components Number of Authors in Largest CC Number of Papers in Largest CC

#Authors removed from reviewer pool 0 5 10 15 20 50 100
253 268 278 292 302 334 389 371 313 304 228 205 55 28 133 114 110 82 74 18 8

5.2 Analysis of the Partition Algorithm on ICLR 2018 submissions (Q3 and Q4)
In the previous section, we empirically veriﬁed that we can partition the reviewers and papers into two disconnected groups. A natural question that arises is how such a partition aﬀects to the overall reviewer-paper matching process, i.e., will the partition cause a great loss in quality of the assignment algorithm used in practice? We empirically investigate this question by performing experiments using data from the ICLR 2018 conference (where we also use ICLR 2017 as a reference point later). In a nutshell, using the popular “mean similarity score” as a measure of the quality of the assignment (detailed below), we see that:
13

Xu, Zhao, Shi, Zhang & Shah

A3. In comparison to when there is no strategyprooﬁng, the quality of the assignment reduces by 11% when it is made strategyproof using the Partition algorithm.

A4. The utility under Partition is only marginally lower than when the partition is done uniformly at random. Thus Partition is roughly equivalent to shrinking the conference size (randomly) to a half.

We now describe the experiment in more detail. We follow the assignment framework used popularly in practice, which comprises of two phases. The ﬁrst phase computes a similarity score for every (reviewer, paper) pair. A higher similarity score is interpreted as a higher envisaged quality of review. As in the case of ICLR 2017 above, we set the collection of all authors as the reviewer pool since the actual identities of the collection of reviewers are not available. We then compute a similarity between every reviewer-paper pair based on the text of the paper and the contents of the reviewer’s published papers. We refer the reader to Appendix A for details of this construction.
Here are some basic statistics about the computed similarity matrix. In Figure 1a we plot the histogram of the computed similarity scores between 911 papers and 2435 reviewers. The mean of the similarity scores across all reviewer-paper pairs is approximately 0.03. This skewed distribution of similarity scores is consistent with our intuition: for each paper, there is only a handful of reviewers who have the aligned background and expertise. In Figure 1b, we show the histogram of the top similarity score computed for each paper (excluding reviewers that are also authors of the corresponding paper). We see that the mean (across all papers) of these top scores is approximately 0.14, which is signiﬁcantly higher than that of 0.03 among all similarity scores.

Frequency Frequency

106 105 104 103 102 101 100
0.0

Mean Similarity Score: 0.03 0.2 0.4 0.6 0.8
Similarity Score

(a) Scores between reviewers and papers.

102

Mean Top Score: 0.14

101

100 0.0

0.2 0.4 0.6 0.8 Top Score of each Paper

(b) Top score of each paper.

Figure 1: The left histogram is the similarity scores between reviewers and papers of the ICLR 2018 dataset. The right histogram plots the top similarity score of each paper in the ICLR 2018 dataset. In both plots, the vertical black line shows the mean of the distribution.

The second phase of the assignment procedure uses the similarity scores to assign reviewers to papers. The most widely used assignment method used in practice is the Toronto Paper Matching System or TPMS (Charlin and Zemel, 2013). This assignment method maximizes the mean similarity score across all assigned reviewer-paper pairs.

14

On Strategyproof Conference Peer Review

In what follows, we evaluate three diﬀerent methods of assigning reviewers to papers in terms of the resulting mean similarity score across all assigned reviewer-paper pairs:

• Using TPMS assignment without any partitioning.

• The Divide-and-Rank (i.e., partitioning reviewers and papers into two sets that are disconnected in the authorship conﬂict graph) with TPMS as the assignment algorithm A.

• Partitioning the reviewers and papers into two equal groups uniformly at random, and using TPMS with the restriction of assigning each reviewers to papers from the other group.

We use the values µ “ 6, λ “ 3 which are typical of conferences today. We provide speciﬁc implementation details in Appendix A.
Basic statistics and experiment results are shown in Table 3. ICLR has grown dramatically from 2017 to 2018, with the number of papers rising from 489 to 911, and the corresponding numbers of authors and components also almost double. The largest component now has 757 authors and 274 components, twice the size of 2017. On the other hand, the second largest component is smaller than that of 2017, which we speculate is because the machine learning community has grown into more reﬁned subﬁelds, creating more smaller clusters. Nevertheless, we are still able to divide the authors and papers into two clusters of approximately equal size using Partition.

Table 3: Result of experiments on ICLR 2018 data.

Description
Number of submitted papers Number of distinct authors Number of connected components #authors, #papers in largest connected component #authors, #papers in second largest connected component
Mean similarity score without partitioning Mean similarity score with random partition (20 runs) Mean similarity score with Partition

Number
911 2428 465 757, 274 30, 11
0.0880 0.0779 ˘ 0.0001
0.0782

The mean of similarity scores using TPMS score assignment (see Appendix A.2) without partitioning is 0.0880 and that with partition is 0.0782, which represents a 11.4% decrease from partitioning. On the other hand, if we randomly partition the reviewers and papers to two sets of equal sizes, the mean (and standard deviation) of the similarity is 0.0779 (and 0.0001 respectively) from 20 runs. The result from Partition is similar to the result obtained from random partition. So in terms of author-paper similarities, Partition does no additional harm than shrinking the conference size (randomly) to a half. For ICLR 2018, this just corresponds to the size of ICLR 2017.
15

Xu, Zhao, Shi, Zhang & Shah

6. Negative Theoretical Results
The positive results in the previous section focus on group unanimity, which is weaker than the conventional notion of unanimity (the conventional notion is also known as pairwise unanimity). Moreover, the algorithm had a disconnected review graph whereas the review graphs of conferences today are typically connected (Shah et al., 2017). It is thus natural to wonder about the extent to which these results can be strengthened: Can a peer-review system with a connected reviewer graph satisfy these properties? Can a strategyproof peer-review system be pairwise unanimous? In this section we present some negative results toward these questions, thereby highlighting the critical impediments towards (much) stronger results.
Before stating our results, we introduce another notion of strategyproofness, which is signiﬁcantly weaker than the notion of strategyproofness (Deﬁnition 3.1), and is hence termed as weak strategyproofness. As compared to strategyproofness which is deﬁned with respect to a given conﬂict graph, weak strategyproofness only requires the existence of a conﬂict graph (with non-zero reviewer-degrees) for which the review process is strategyproof.
Deﬁnition 6.1 (Weak Strategyproofness, WSP). A review process pG, f q is called weakly strategyproof, if for every reviewer ri, there exists some paper pj P P such that for every pair of distinct proﬁles (under assignment G) π “ pπp1q, . . . , πpi´1q, πpiq, πpi`1q, . . . , πpmqq and π1 “ pπp1q, . . . , πpi´1q, πpiq1, πpi`1q, . . . , πpmqq, it is guaranteed that fjpπq “ fjpπ1q.
In other words, weak strategyproofness requires that for each reviewer there is at least one paper (not necessarily shares conﬂicts this reviewer) whose ranking cannot be inﬂuenced by the reviewer. As the name suggests, strategyproofness is strictly stronger than weak strategyproofness, when each reviewer has at least one paper of conﬂict.
We deﬁne the notion of weak strategyproofness mainly for theoretical purposes to establish negative results, since WSP is too weak to be practical. However, even this extremely weak requirement is impossible to satisfy in situations of practical interest.

Table 4: Summary of our negative results (ﬁrst three rows of the table), and a comparison to our positve result (fourth row).

Unanimity Pairwise Group Pairwise Group

Strategyproof None Weak Weak Yes

Requirement on G Mild (see Corollary 6.3)
Mild (Connected G) Complete G None

Possible? No
Conjecture: No No Yes

Reference Theorem 6.2 Proposition 6.4 Theorem 6.5 Theorem 4.1

We summarize our results in Table 4. Recall that we show the property of group unanimity and strategyproof for Divide-and-Rank; as the ﬁrst direction of possible extension, we show in Theorem 6.2 that the slightly stronger notion of pairwise unanimity is impossible to satisfy under mild assumptions, even without strategyproof constraints. Then in Section 6.2 we explore the second direction of extension, by requiring a connected G; we give conjectures and insights that group unanimity and weak strategyproofness is impossible under this setting. At last in Theorem 6.5 we revert to the traditional setting of social choice, where every reviewer gives a total ranking of the set of all papers P; we show that in this setting it is impossible for any review process to be pairwise unanimous and weakly strategyproof.

16

On Strategyproof Conference Peer Review
6.1 Impossibility of Pairwise Unanimity
We show in this section that pairwise unanimity is too strong to satisfy under mild assumptions. These assumptions are mild in the sense that a violation of the assumptions leads to severely limited and somewhat impractical choices of G.
In order to precisely state our result, we ﬁrst introduce the notion of a review-relation graph H. Given a paper-review assignment tPiumi“1, the review-relation graph H is an undirected graph with rns as its vertices and where any two papers pj1 and pj2 are connected iﬀ there exists at least one reviewer who reviews both the papers. With this preliminary in place, we are now ready to state the main result of this section:
Theorem 6.2. If H has a cycle of length 3 or more and there is no single reviewer reviews all the papers in the cycle, then there is no review process pG, f q that is pairwise unanimous.
The proof of Theorem 6.2 is similar to a Condorcet cycle proof, and the details are in Section 7.4. In the corollary below we give some direct implications of the condition in Theorem 6.2 when |P1| “ ¨ ¨ ¨ “ |Pm| “ µ, that is, when every reviewer ranks a same number of papers.
Corollary 6.3. Suppose |P1| “ ¨ ¨ ¨ “ |Pm| “ µ ě 2. If pG, f q is pairwise unanimous, the following conditions hold:
(i) H does not contain any cycles of length µ ` 1 or more.
(ii) The set of papers reviewed by any pair of reviewers ri1 and ri2 must satisfy the condition |Pi1 X Pi2| P t0, 1, µu. In words, if a pair of reviewers review more than one common papers, they must review exactly the same set.
(iii) The number of distinct sets in Pi, . . . , Pm is at most nµ´´11 .
Remarks. In modern conferences (Shah et al., 2017), each reviewer usually reviews around 3 to 6 papers. If we make the review process pairwise unanimous, by Corollary 6.3 (iii) the number of distinct review sets is much smaller than the number of reviewers; this severely limits the design of review sets, since many reviewers would be necessitated to review identical sets of papers. Corollary 6.3 (ii) is a related, strong requirement, since the specialization of reviewers might not allow for such limiting of the intersection of review sets. For instance, there are a large number of pairs of reviewers who review more than one common paper but none with exactly the same set of papers (Shah et al., 2017).
In summary, Theorem 6.2 and Corollary 6.3 show that it is diﬃcult to satisfy pairwise unanimity, even without considering strategyproofness. This justiﬁes our choice of group unanimity in the positive results.
6.2 Group Unanimity and Strategyproof for a Connected Review Graph
Having shown that pairwise unanimity is too strong a requirement to satisfy, we now consider another direction for extension – conditions on the review graph G. A natural question follows: Under what condition on the review graph G are both group unanimity and strategyproofness possible? Although we will leave the question of ﬁnding the exact condition open, we conjecture that if we require G to be connected, then group unanimity and strategyproofness
17

Xu, Zhao, Shi, Zhang & Shah
cannot be simultaneously satisﬁed. To show our insights, we analyze an extremely simpliﬁed review setting.
Proposition 6.4. Consider any n ě 4 and suppose P “ P1YP2YP3YP4, where P1, P2, P3, P4 are disjoint nonempty sets of papers. Consider a review graph G with m “ 3 reviewers, where reviewer r1 reviews tP1, P2u, r2 reviews tP2, P3u, and r3 reviews tP3, P4u. Then there is no aggregation function f that is both weakly strategyproof and group unanimous.
Proposition 6.4 thus shows that for the simple review graph considered in the statement, group unanimity and weak strategyproofness cannot hold at the same time. Proof details can be found in Section 7.6.
We conjecture that such a negative result may hold for more general connected review graphs, and such a negative result may be proved by identifying a component of the general review graph that meets the condition of Proposition 6.4. This shows that our design process of the review graph in Section 4 is quite essential for ensuring those important properties.
6.3 Pairwise Unanimity and Strategyproof under Total Ranking
Throughout the paper so far, motivated by the application of conference peer review, we considered a setting where every reviewer reviews a (small) subset of the papers. In contrast, a bulk of the classical literature in social choice theory considers a setting where each reviewer ranks all candidates or papers (Arrow, 1950; Satterthwaite, 1975). Given this long line of literature, intellectual curiosity drives us to study the case of all reviewers reviewing all papers for our conference peer-review setting.
We now consider our notion of pairwise unanimous and weakly strategyproof in this section under this total-ranking setting, where P1 “ ¨ ¨ ¨ “ Pm “ P. In this case, the review graph G is always a complete bipartite graph, and it only remains to design the aggregation function f . Although total rankings might not be practical for large-scale conferences, it is still helpful for smaller-sized conferences and workshops.
Under this total ranking setting, we prove a negative result showing that pairwise unanimity and strategyproofness cannot be satisﬁed together, and furthermore, even the notion of weak strategyproofness (together with PU) is impossible to achieve.
Theorem 6.5. Suppose n ě 2. If P1 “ ¨ ¨ ¨ “ Pm “ P, then there is no aggregation function f that is both no aggregation function f that is both and pairwise unanimous.
Note that the conditions required for Theorem 6.2 are not met in the total ranking case. To prove Theorem 6.5(details in Section 7.7.1), we use Cantor’s diagonalization argument to generate a contradiction by assuming there exists f that is both PU and WSP.
It is interesting to note that pairwise unanimity can be easily satisﬁed in this setting of total rankings, by using a simple aggregation scheme such as the Borda count. However, Theorem 6.5 shows that surprisingly, even under the extremely mild notion of strategyproofness given by WSP, it is impossible to achieve pairwise unanimity and strategyproofness simultaneously.
7. Proofs
In this section, we provide the detailed proofs of all the results from previous sections.
18

On Strategyproof Conference Peer Review

7.1 Proof of Proposition 3.4
Suppose pG, f q is PU, and P1 Ă P satisﬁes that every reviewer ranks the papers she reviewed from P1 higher than those she reviewed from PzP1. Now for every px P P1 and py P PzP1 and reviewer ri such that ri reviews both px and py, ri must rank px ą py since otherwise the assumption of P1 is violated. Since f is PU, we know that f pπq must respect px ą py as well. This argument holds for every px P P1 and py P PzP1 that have been reviewed by at least one reviewer, and hence pG, f q is also GU.

7.2 Proof of Theorem 4.1

We assume that the condition on the partitioning of the conﬂict graph, as stated in the statement of this theorem, is met. We begin with a lemma which shows that for any aggregation algorithm B, Contract-and-Sort is group unanimous.

Lemma 7.1. For any assignment and aggregation algorithms A and B, the aggregation procedure Contract-and-Sort is group unanimous.

We prove this lemma in Section 7.2.1. Under the assumptions on µ, λ and sizes of

RC, RC, PC, PC, it is easy to verify that there is a paper allocation satisﬁes |Pi| ď µ, @ i P rms

s

s

and each paper gets at least λ reviews. The strategyproofness of Divide-and-Rank follows from

the standard ideas in the past literature on partitioning-based methods (Alon et al., 2011):

Algorithm 1 guarantees that reviewers in RC do not review papers in PC, and reviewers
s

in RC do not review papers in PC. Hence the fact that Divide-and-Rank is strategyproof
s

trivially follows from the assignment procedure where each reviewer does not review the

papers that are in conﬂict with her, as speciﬁed by the conﬂict graph C. Given that all the

other reviews are ﬁxed, the ranking of the papers in conﬂict with her will only be determined

by the other group of reviewers and so ﬁxed no matter how she changes her own ranking. On

the other hand, from Lemma 7.1, since Contract-and-Sort is group unanimous, we know that

πC and πC respect group unanimity w.r.t. πC and πC¯, respectively. Since π “ pπC , πC¯q, it
s

follows that πC and πC also respect group unanimity w.r.t. π. Finally, note that there is no
s

reviewer who has reviewed both papers from PC and PC, the interlacing step preserves the
s

group unanimity, which completes our proof.

7.2.1 Proof of Lemma 7.1

Let f pπq :“ Contract-and-Sortpπ, Bq, where π is a preference proﬁle. Deﬁne π “ f pπq. Let

r

r

r

r

k denote the number of SCCs in Gπ. Construct a directed graph Grπ such that each of its

r

r

vertices represents a SCC in Gπ, and there is an edge from one vertex to another in Grπ iﬀ

r

r

there exists an edge going from one SCC to the other in the original graph Gπ. Let v˜1, . . . , v˜k
r

be a topological ordering of the vertices in Grπ. Since v˜1, . . . , v˜k is a topological ordering,
r

then edges can only go from v˜j1 to v˜j2 where j1 ă j2. Now consider any cut pPX , PY q in Gπ
r

that satisﬁes the requirement of group unanimity, i.e., all edges in the cut direct from PX to

PY . Then there is no pair of papers px P PX and py P PY such that px and py are in the

same connected component, otherwise there will be both paths from px to py and py to px,

contradicting that pPX , PY q forms a cut where all the edges go in one direction. This shows

that PX and PY also form a partition of all the vertices v˜1, . . . , v˜k. Now consider any edge

ppx, pyq from PX to PY . Suppose px is in component v˜jx and py in component v˜jy . We have

19

Xu, Zhao, Shi, Zhang & Shah

jx ‰ jy, since PX and PY forms a partition of all SCCs; also it cannot happen that jx ą jy, otherwise v˜1, . . . , v˜k is not a topological ordering returned by f . So it must be jx ă jy, and the edge ppx, pyq is respected in the ﬁnal ordering.

7.3 Proof of Proposition 4.2 We would ﬁrst need a lemma for the location of papers:

!Y ] Y ] )

!´ ¯ ´ ¯

)

Lemma 7.2. Let I1 “ |PnC| , |P2nC| , ..., n , I2 “ g |PnCs | , g |P2nCs | , ..., n ´ 1 , where

gpxq “ rxs ´ 1 is the largest integer that is strictly smaller than x. Then I1 X I2 “ H and

I1 Y I2 “ rns.

We prove this lemma in Section 7.3.1.

Consider any paper pi, and suppose its position in π˚ is . Deﬁne n1 “ |PC| and

n2 “ |PC¯|. Without loss of generality assume n1 ě n2 (the other case is symmetric) and let n ě 4c{ log 2. We discuss the following two cases depending on whether pi P PC or pi P PC¯.

Case I: If pi P PC. Let k be the number of papers in PC ranked strictly higher (better) than according to π˚. Since the permutation π˚ is uniformly random, conditioned on

this value of , the other papers’ positions in the true ranking are uniformly at random in

positions rnszt u. Now for any paper pj, j ‰ i, let Xj be an indicator random variable set

as 1 if position of pj is higher than

in

π˚,

and

0

otherwise.

So

k

“

ř
pj PPC ztpiu

Xj ,

and

PrpXj

“

1q

“

´1 n´1

when

j

‰

i.

Then

using

Hoeﬀding’s

inequality

without

replacement,

we

have

Pr ˆˇˇˇ k ´ ´ 1 ˇˇˇ ě ε˙ ď 2 expp´2pn1 ´ 1qε2q ď 2 expp´n1ε2q ˇ n1 ´ 1 n ´ 1 ˇ

for any ε ą 0. The last inequality is due to n1 ě 2, which holds because n{n1 ď c with a b
constant c. Now setting ε “ lognp21{δq we have the bound

˜

d

¸

ˇ ˇ

k

´

1

ˇ ˇ

logp2{δq

Pr ˇ

´

ˇď

ě 1 ´ δ.

ˇ n1 ´ 1 n ´ 1 ˇ

n1

Y

]

Now note that by Algorithm 2, the position of paper pi in the ranking πp is πpi “ pk ` 1q ¨ nn1 .

Use this relationship to substitute k in the above inequality, and notice that by assumption

maxtn{n1, n{n2u ď c, we have

nˆ

ˆ

´1˙ ˙ n

pk ` 1q ¨ ď pn1 ´ 1q ε `

`1

n1

n´1

n1

“ n1 ´ 1 ¨ n p ´ 1q ` n1 ´ 1 ¨ nε ` n

n1 n ´ 1

n1

n1

n

ď n1 ` nε ` ´ 1.

20

On Strategyproof Conference Peer Review

On the other hand,

nˆ

ˆ ´1 ˙ ˙ n

pk ` 1q ¨ ě pn1 ´ 1q

´ε `1

n1

n´1

n1

“ n1 ´ 1 ¨ n p ´ 1q ´ n1 ´ 1 ¨ nε ` n

n1 n ´ 1

n1

n1

ě n ´ nε ` n1 ´ 1 ¨ n p ´ 1q.

n1

n1 n ´ 1

So

pk ` 1q ¨ n ´ ě n ´ nε ` n1 ´ 1 ¨ n p ´ 1q ´ (1)

n1

n1

n1 n ´ 1

ě n ´ nε ` n1 ´ 1 ¨ n pn ´ 1q ´ n (2)

n1

n1 n ´ 1

ě ´nε.

Here (2) is because n1n´1 1 ¨ n´n 1 ă 1, and thus RHS of (1) is minimized (as a function of ) when “ n. Combining the two inequalities above we have

d

|πpi ´ | ď nε ` n “ n logp2{δq ` n ď 2anc ¨ logp2{δq,

n1

n1

n1

where the last inequality is by the assumption that n is large enough so that 2c ď a
nc ¨ logp2{δq.

Case II: If pi P PC¯. Again, let k be the number of papers in PC¯ ranked strictly higher (better) than according to π˚. As the analysis in Case I, similarly, we have

k

“

ř
pj

PP

¯

ztpiu

Xj

,

and

PrpXj

“

1q

“

n´´11 .

With the same analysis using Hoeﬀding’s

C

inequality without replacement, with probability at least 1 ´ δ we have

d

ˇ ˇ

k

´

1

ˇ ˇ

ˇ

´

ˇď

ˇ n2 ´ 1 n ´ 1 ˇ

logp2{δq . n2

´

¯

Now using Lemma 7.2, the position of paper pi in πp in this case is πpi “ g pk ` 1q ¨ nn2 .

Using exactly the same analysis as Case I we have

n

n

´nε ď pk ` 1q ¨ n2 ´ ď n2 ` nε ´ 1,

and thus

d

|πpi ´ | ď nε ` n “ n logp2{δq ` n ď 2anc ¨ logp2{δq.

n2

n2

n2

Combine both Case I and Case II, and notice that πi˚ is uniformly distributed in rns. Using a union bound over i “ 1, 2, ..., n, with probability 1 ´ δ we have:

max

|πi

´

πi˚|

ď

a 2 nc

¨

logp2n{δq.

1ďiďn p

21

Xu, Zhao, Shi, Zhang & Shah

7.3.1 Proof of Lemma 7.2

Y] We show that for every slot q that there is no p such that p ¨ nn1 “ q, there exists one slot

´

¯

p1 for PC such that g

p1

¨

n n

“ q, i.e., all slots that are left empty by PC are taken by slots

s

2

of PC. Since that the two kinds of slots have a total number of n, we show that there are no
s

overlap between the two kinds of slots, thus proving the lemma. Y]
Let t “ n{n1. Suppose if there is no p such that p ¨ nn1 “ q, then there must exist some pˆ such that

q ` 1 ď pˆt ă q ` t.

(3)

This is because there must be a multiple of t in the range rq, q ` tq, but our assumption makes that there is no such multiply in rq, q ` 1q.
Now let u “ n{n2. By n1 ` n2 “ n we have 1{u ` 1{t “ 1; substituting t “ u{pu ´ 1q in (3) we have
q ă pq ´ pˆ ` 1qu ď q ` 1.
Thus there exists p1 “ gppq ´ pˆ ` 1quq P PC. Thus we prove the lemma.
s

7.4 Proof of Theorem 6.2
The proof of Theorem 6.2 is a direct formulation of our intuition in Section 6.1. Without loss of generality let pp1, . . . , plq be the cycle not reviewed by a single reviewer, for l ě 3. Hence there exists a partial proﬁle π such that for all the reviewers who have reviewed both pj and pj`1, pj ą pj`1, @j P rls (deﬁne pl`1 “ p1). On the other hand, since for each reviewer, at least one pair ppj, pj`1q is not reviewed by her, the constructed partial proﬁle is valid. Now assume f is PU, then we must have p1 ą ¨ ¨ ¨ ą pl and pl ą p1, which contradicts the transitivity of the ranking.

7.5 Proof of Corollary 6.3
We prove each of the conditions in order.
Proof of part (i): If there is a cycle of size µ ` 1, then no reviewer can review all the papers in it since it exceeds the size of review sets. So there is no such cycle.
Proof of part (ii): The statement trivially holds for µ “ 2. For µ ě 3, Suppose there are two reviewers ri1 and ri2 such that 2 ď |Pi1 X Pi2| ď µ ´ 1. Since Pi1 ‰ Pi2, there exist papers pj1 and pj2 such that pj1 P Pi1 zPi2 and pj2 P Pi2 zPi1 . Also |Pi1 X Pi2 | ě 2, and let pj3, pj4 P Pi1 X Pi2. By deﬁnition it is easy to verify that ppj1, pj3, pj2, pj4q forms a cycle that satisﬁes the condition in Theorem 6.2, and hence pG, f q is not pairwise unanimous.
Proof of part (iii): Deﬁne a “paper-relation graph” Gp as follows: Given a paper-review assignment tPiumi“1, the paper-relation graph Gp is an undirected graph, whose nodes are the distinct sets in tPiumi“1; we connect two review sets iﬀ they have one paper in common. Note that by (ii), each pair of distinct sets has at most one paper in common.

22

On Strategyproof Conference Peer Review

We ﬁrst show that pG, f q is pairwise unanimous, then Gp must necessarily be a forest. If there is a cycle in Gp, then there is a corresponding cycle in the review relation graph H. To see this, not losing generality suppose the shortest cycle in Gp is pP1, ..., Plq. Also, suppose P1 X P2 “ tp1u, P2 X P3 “ tp2u, ..., Pl X P1 “ tplu not losing generality. Then pp1, ..., plq forms a cycle in Gp by its deﬁnition. Since each reviewer reviews exactly one set in Gp, there is no reviewer reviewing all papers in this cycle of papers in Gp. Thus the condition in Theorem 6.2 is satisﬁed, and pG, f q is not pairwise unanimous.
We now use this result to complete our proof. Consider the union of all sets of papers
that form vertices of Gp. We know that this union contains exactly n papers since each paper is reviewed at least once. Now let kp denote the number of distinct review sets (that is, number of vertices of Gp), and let Pii, ..., Pikp denote the vertices of Gp. The union of three or more sets in tPik ukkp“1 is empty, since otherwise there will be a cycle in Gp. Using this fact, we apply the inclusion-exclusion principle to obtain

kp

ÿ

ÿ

n “ |Pik | ´

|Pik1 X Pik2 | “ kpµ ´ |EGp |.

k“1

1ďk1ăk2ďkp

Now use the inequality |EGp| ď kp ´ 1 which arises since Gp is a forest, to obtain the claimed bound kp ď nµ´´11 .

7.6 Proof of Proposition 6.4
Fix some ranking of papers within each individual set P1, P2, P3 and P4 (e.g., according to the natural order of their indices). In the remainder of the proof, any ranking of all papers always considers these ﬁxed rankings within these individual sets. With this in place, in what follows, we refer to any ranking in terms of the rankings of the four sets of papers.
Suppose there is one such f that satisﬁes group unanimity and weak strategyproofness for G, and consider the following 4 proﬁles:
(1) r1 : P1 ą P2, r2 : P2 ą P3, r3 : P3 ą P4 (2) r1 : P2 ą P1, r2 : P3 ą P2, r3 : P4 ą P3
(3) r1 : P2 ą P1, r2 : P2 ą P3, r3 : P3 ą P4 (4) r1 : P2 ą P1, r2 : P3 ą P2, r3 : P3 ą P4
By the property of GU, proﬁle (1) leads to output P1 ą P2 ą P3 ą P4, whereas (2) leads to output P4 ą P3 ą P2 ą P1. Now compare (1) and (3): The output of (3) must have P2 at the top and satisfy P3 ą P4, by the property of GU. So the output of proﬁle (3) must be one of i) P2 ą P1 ą P3 ą P4, ii) P2 ą P3 ą P1 ą P4, or iii) P2 ą P3 ą P4 ą P1. Now note that only reviewer r1 changes ranking across proﬁles (1) and (3), and hence by WSP the position of at least one paper in the output of proﬁle (3) must be the same as in that of proﬁle (1). This makes iii) infeasible, so the output of (3) must be either i) or ii). Similarly, the output of (4) is either P3 ą P4 ą P2 ą P1 or P3 ą P2 ą P4 ą P1. Now comparing (3) and (4): only r2 changes ranking, but none of the four papers can be at the same position no matter how we choose the outputs of (3) and (4). This yields a contradiction.

7.7 Proof of Theorem 6.5
We begin with a deﬁnition of an “inﬂuence graph” Gf induced by any given aggregation rule f.

23

Xu, Zhao, Shi, Zhang & Shah

Deﬁnition 7.3 (Inﬂuence graph). For any review aggregation rule f , the inﬂuence graph
Gf induced by f is a bipartite graph with two groups of vertices R and P, and edges as follows. A vertex ri is connected to vertex pj iﬀ there exists a certain proﬁle π such that ri is able to change the output ranking of pj by changing her own preference. Formally, there exists an edge between any pair pri, pjq P EGf iﬀ there exist proﬁles π “ tπp1q, . . . , πpi´1q, πpiq, πpi`1q, . . . , πpmqu and π1 “ tπp1q, . . . , πpi´1q, π˜piq, πpi`1q, . . . , πpmqu and j P rns such that f pπqpjq ‰ f pπ1qpjq.

From this deﬁnition, it is thus not hard to see that f is WSP if and only if the degree of
every reviewer node in Gf is strictly smaller than n.
We prove the claim via a contradiction argument. Assume that f is both PU and
WSP. Let Gf be the corresponding inﬂuence graph. Firstly we show that degppq ą 0 for every paper p, where the degree is for the inﬂuence graph Gf . Suppose otherwise that degppjq “ 0 for some paper pj. This means no reviewer can aﬀect the ranking of pj; in other words, the position of paper pj is ﬁxed regardless of the proﬁle. This contradicts with our assumption of pairwise unanimity; to see this, pick another paper pj1 where j1 ‰ j (this is possible since n ě 2). Not losing generality suppose j ă j1. Consider a proﬁle π where every reviewer ranks pj ą pj1 ą P´pj,j1q, and another proﬁle π1 where everyone ranks pj‘ ą pj ą P´pj,j1q; here P´pj,j1q means the ordinal ranking of papers other than pj, pj1, i.e., p1 ą ¨ ¨ ¨ ą pj´1 ą pj`1 ą ¨ ¨ ¨ ą pj1´1 ą pj1`1 ą ¨ ¨ ¨ ą pn. By the property of PU, when everyone ranks the same the ﬁnal result must be the same as everyone; however this means
the position of pj is diﬀerent in the two proﬁles, and thus the position of pj is not ﬁxed. This makes contradiction and we prove that degppq ą 0 for every paper p.
Now for any reviewer ri P R, let epriq P P be the paper with the lowest index in P such that pri, epriqq R EGf . Since f is WSP, epriq must exist for all i P rms. Deﬁne the set of such papers as P1 :“ te1, . . . , em1u “ tepriq : ri P Ru. Note that we must have m1 ď m and in fact m1 can be strictly smaller than m because of the possible overlap between epriq, @i P rms. From the deﬁnition of m1 and property of WSP, it is clear that m1 ě 1. If m1 “ 1, we have
pri, e1q R EGf for every reviewer ri; this contradicts with the fact that degppq ě 0 for every paper p. So m1 ą 1.
In this proof, we slightly overload the notation of fek pπq to mean the position of paper ek in f pπq. Based on the inverse mapping from P1 to R, we partition all the reviewers R into m1 groups tR11, . . . , R1m1u such that all reviewers in any set R1k contributed paper ek when deﬁning set P1. In particular, we have that no reviewer in R1j is connected to paper ej in the inﬂuence graph Gf .
In the description that follows, we restrict attention to the papers in P1, and assume that in any ranking all remaining pn ´ n1q papers are positioned at the end of the preference list of any reviewer. Now consider the following two preferences over P1:

π “ e1 ą e2 ą ¨ ¨ ¨ ą em1, π1 “ e2 ą e3 ą ¨ ¨ ¨ ą em1 ą e1.

Using π and π1, deﬁne the following m1 ` 1 diﬀerent proﬁles:

π0 “ pπ, . . . , πq, πk “ pπ1, . . . , π1, π, . . . , πq @k P rm1s. loooomoooon looomooon

k

m1´k

24

On Strategyproof Conference Peer Review
where in πk the ﬁrst k preferences are π1 and the last (m1 ´ k) preferences are π. In what follows, we will use a diagonalization argument to generate a contradiction using the condition that f is WSP. We ﬁrst present a lemma, which we prove in Section 7.7.1.
Lemma 7.4. If f is pairwise unanimous and weakly strategyproof, fek pπkq “ k for every k P rm1s, that is, under proﬁle πk, the k-th position in the output ranking must be taken by paper ek.
Applying Lemma 7.4 with k “ m1, we obtain that fem1 pπm1q “ m1. However, on the other side, since πm1 “ pπ1, . . . , π1q and π1 “ e2 ą e3 ą ¨ ¨ ¨ ą em1 ą e1, again by the PU property we have fem1 pπm1q “ 1. This leads to a contradiction, hence f cannot be both WSP and PU.
7.7.1 Proof of Lemma 7.4 We prove by induction on k. Base case. Since f is PU, the output ranking f pπ0q must be:
f pπ0q “ e1 ą e2 ą ¨ ¨ ¨ ą em1
Consider k “ 1. Note that π and π1 diﬀer only at the position of e1, and in π1, only P11 changes their preference and all the other preferences are kept ﬁxed. Then by the WSP of f , the output ranking of e1 will not be changed because P11 are not connected to e1 in the inﬂuence graph, so we must have:
f pπ1q “ e1 ą e2 ą ¨ ¨ ¨ ą em1,
Induction step. Suppose the claim of this lemma holds for t1, . . . , ku. Consider the case of k ` 1.
Observe that f pπkqpekq “ k, and in both π and π1 we have:
ek ą ek`1 ą ¨ ¨ ¨ ą em1 .
Then since f is PU, we know that the last m1 ´ k ` 1 positions in the output ranking of f pπkq must be given by ek ą ek`1 ą ¨ ¨ ¨ ą em1, i.e., fek`1pπkq “ k ` 1. The proﬁles πk and πk`1 diﬀer only in the preference given by Pk1 `1, and no reviewer in set Pk1 `1 can inﬂuence the position of paper ek`1. It follows that fek`1pπk`1q “ k ` 1, which completes our proof.
8. Discussion
In this paper we address the problem of designing strategyproof and eﬃcient peer-review mechanism. The setting of peer review is challenging due to the various idiosyncrasies of the peer-review process: reviewers review only a subset of papers, each paper has multiple authors who may be reviewers, and each reviewer may author multiple submissions. We provide a framework and associated algorithms to impart strategyproofness to conference peer review. Our framework, besides guaranteeing strategyproofness, is importantly very ﬂexible in allowing the program chairs to use the decision-making criteria of their choice. We complement these positive results with negative results showing that it is impossible for
25

Xu, Zhao, Shi, Zhang & Shah
any algorithm to remain strategyproof and satisfy the stronger notion of pairwise unanimity. Future work includes considering eﬃciency from a statistical perspective and characterizing the precise set of conﬂict-of-interest graphs that permit (or not) strategyproofness.
The framework established here leads to a number of useful open problems:
• Can recruitment of a small number of reviewers with no conﬂicts (e.g., in case of authorship conﬂicts, reviewers who have not submitted any papers) lead to signiﬁcant improvements in eﬃciency? Can better ways to eliminate some authors from the reviewer pool increase applicability of partition-based algorithms?
• The results in this paper considered the social choice property of unanimity as a measure of eﬃciency. While this can be regarded as a ﬁrst-order notion of eﬃciency, it is of interest to consider complex notions of eﬃciency. One useful notion of eﬃciency is the statistical utility of estimation (Stelmakh et al., 2019b) of the (partial or full) ranking of papers under a statistical model for reviewer reports. An alternative notion of eﬃciency combines an assignment quality based on the similarities of assigned reviewers and papers with group unanimity (e.g., maximizing the similarity scores in the assignment while preserving group unanimity).
Acknowledgments
This work was supported in parts by NSF grants CRII: CIF: 1755656 and CIF: 1763734.
References
Alon, N., Fischer, F., Procaccia, A., and Tennenholtz, M. (2011). Sum of us: Strategyproof selection from the selectors. In Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge, pages 101–110. ACM.
Anderson, M. S., Ronning, E. A., De Vries, R., and Martinson, B. C. (2007). The perverse eﬀects of competition on scientists’ work and relationships. Science and engineering ethics, 13(4):437–461.
Arrow, K. J. (1950). A diﬃculty in the concept of social welfare. Journal of political economy, 58(4):328–346.
Aziz, H., Lev, O., Mattei, N., Rosenschein, J. S., and Walsh, T. (2016). Strategyproof peer selection: Mechanisms, analyses, and experiments. In AAAI, pages 397–403.
Aziz, H., Lev, O., Mattei, N., Rosenschein, J. S., and Walsh, T. (2019). Strategyproof peer selection using randomization, partitioning, and apportionment. Artiﬁcial Intelligence.
Balietti, S., Goldstone, R. L., and Helbing, D. (2016). Peer review and competition in the art exhibition game. Proceedings of the National Academy of Sciences, 113(30):8414–8419.
Barnett, W. (2003). The modern theory of consumer behavior: Ordinal or cardinal? The Quarterly Journal of Austrian Economics, 6(1):41–65.
26

On Strategyproof Conference Peer Review
Bird, S., Loper, E., and Klein, E. (2009). Natural language processing with python o’reilly media inc.
Bousquet, N., Norin, S., and Vetta, A. (2014). A near-optimal mechanism for impartial selection. In International Conference on Web and Internet Economics, pages 133–146. Springer.
Brandt, F., Conitzer, V., Endriss, U., Procaccia, A. D., and Lang, J. (2016). Handbook of computational social choice. Cambridge University Press.
Caragiannis, I., Chatzigeorgiou, X., Krimpas, G. A., and Voudouris, A. A. (2017). Optimizing positional scoring rules for rank aggregation. In AAAI, pages 430–436.
Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated paper-reviewer assignment system.
Connolly, R., Miller, J., and Friedman, R. (2014). A longitudinal examination of sigite conference submission data, 2007-2012. In Proceedings of the 15th Annual Conference on Information technology education, pages 167–172. ACM.
De Clippel, G., Moulin, H., and Tideman, N. (2008). Impartial division of a dollar. Journal of Economic Theory, 139(1):176–191.
Díez Peláez, J., Luaces Rodríguez, Ó., Alonso Betanzos, A., Troncoso, A., and Bahamonde Rionda, A. (2013). Peer assessment in moocs using preference learning via matrix factorization. In NIPS Workshop on Data Driven Education.
Dörﬂer, F., Xiao, Y., and van der Schaar, M. (2017). Incentive design in peer review: Rating and repeated endogenous matching. IEEE Transactions on Network Science and Engineering.
Douceur, J. R. (2009). Paper rating vs. paper ranking. ACM SIGOPS Operating Systems Review, 43(2):117–121.
Emerson, P. (2013). The original Borda count and partial voting. Social Choice and Welfare, pages 1–6.
Fiez, T., Shah, N., and Ratliﬀ, L. (2019). A SUPER* algorithm to optimize paper bidding in peer review. In ICML workshop on Real-world Sequential Decision Making: Reinforcement Learning And Beyond.
Fischer, F. and Klimm, M. (2015). Optimal impartial selection. SIAM Journal on Computing, 44(5):1263–1285.
Fishburn, P. C. (2015). The theory of social choice. Princeton University Press.
Fulkerson, D. and Gross, O. (1965). Incidence matrices and interval graphs. Paciﬁc journal of mathematics, 15(3):835–855.
Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal matter? insights from a major nlp conference. arXiv preprint arXiv:1903.11367.
27

Xu, Zhao, Shi, Zhang & Shah
Garg, N., Kavitha, T., Kumar, A., Mehlhorn, K., and Mestre, J. (2010). Assigning papers to referees. Algorithmica, 58(1):119–136.
Ge, H., Welling, M., and Ghahramani, Z. (2013). A Bayesian model for calibrating conference review scores.
Gibbard, A. (1973). Manipulation of voting schemes: a general result. Econometrica: journal of the Econometric Society, pages 587–601.
Hajek, B., Oh, S., and Xu, J. (2014). Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, pages 1475–1483.
Hartvigsen, D., Wei, J. C., and Czuchlewski, R. (1999). The conference paper-reviewer assignment problem. Decision Sciences, 30(3):865–876.
Hazelrigg, G. (2013). Dear colleague letter: Information to principal investigators (PIs) planning to submit proposals to the Sensors and Sensing Systems (SSS) program October 1, 2013, deadline. Deadline (NSF Website, http://www.nsf.gov/pubs/2013/nsf13096/nsf13096.jsp).
Hojat, M., Gonnella, J. S., and Caelleigh, A. S. (2003). Impartial judgment by the “gatekeepers” of science: fallibility and accountability in the peer review process. Advances in Health Sciences Education, 8(1):75–96.
Holzman, R. and Moulin, H. (2013). Impartial nominations for a prize. Econometrica, 81(1):173–196.
Kahng, A. B., Kotturi, Y., Kulkarni, C., Kurokawa, D., and Procaccia, A. D. (2017). Ranking wily people who rank each other. Technical Report.
Kurokawa, D., Lev, O., Morgenstern, J., and Procaccia, A. D. (2015). Impartial peer review. In IJCAI, pages 582–588.
Langford, J. (2008). Adversarial academia. http://hunch.net/?p=499.
Lawrence, N. and Cortes, C. (2014). The NIPS Experiment. http://inverseprobability. com/2014/12/16/the-nips-experiment. [Online; accessed 3-June-2017].
Makhorin, A. (2001). Gnu linear programming kit. Moscow Aviation Institute, Moscow, Russia, 38.
Mathieus, C. (2008). SODA PC meetings. http://cs.brown.edu/~claire/SODAnotes.pdf (last retrieved May 21, 2018).
Merriﬁeld, M. R. and Saari, D. G. (2009). Telescope time without tears: a distributed approach to peer review. Astronomy & Geophysics, 50(4):4–16.
Noothigattu, R., Shah, N., and Procaccia, A. (2018). Choosing how to choose papers. arXiv preprint arxiv:1808.09057.
Piech, C., Huang, J., Chen, Z., Do, C., Ng, A., and Koller, D. (2013). Tuned models of peer assessment in moocs. arXiv preprint arXiv:1307.2579.
28

On Strategyproof Conference Peer Review
Roos, M., Rothe, J., and Scheuermann, B. (2011). How to calibrate the scores of biased reviewers by quadratic programming. In AAAI.
Salton, G., Wong, A., and Yang, C.-S. (1975). A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.
Satterthwaite, M. A. (1975). Strategy-proofness and arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of economic theory, 10(2):187–217.
Schütze, H., Manning, C. D., and Raghavan, P. (2008). Introduction to information retrieval. In Proceedings of the international communication of association for computing machinery conference, volume 4.
Shah, N. B., Balakrishnan, S., Bradley, J., Parekh, A., Ramchandran, K., and Wainwright, M. J. (2016). Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. The Journal of Machine Learning Research, 17(1):2049–2095.
Shah, N. B., Bradley, J. K., Parekh, A., Wainwright, M., and Ramchandran, K. (2013). A case for ordinal peer-evaluation in moocs. In NIPS Workshop on Data Driven Education.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and von Luxburg, U. (2017). Design and Analysis of the NIPS 2016 Review Process. arXiv preprint arXiv:1708.09794.
Stelmakh, I., Shah, N., and Singh, A. (2019a). On testing for biases in peer review. In NeurIPS.
Stelmakh, I., Shah, N. B., and Singh, A. (2019b). PeerReview4All: Fair and accurate reviewer assignment in peer review. In Algorithmic Learning Theory.
Stewart, N., Brown, G. D., and Chater, N. (2005). Absolute identiﬁcation by relative judgment. Psychological review, 112(4):881.
Thurner, S. and Hanel, R. (2011). Peer-review in a world with rational scientists: Toward selection of the average. The European Physical Journal B, 84(4):707–711.
Tomkins, A., Zhang, M., and Heavlin, W. D. (2017). Reviewer bias in single-versus doubleblind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Tsukida, K. and Gupta, M. R. (2011). How to analyze paired comparison data. Technical report, DTIC Document.
Wang, J. and Shah, N. B. (2019). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS.
Appendix A. More Details on the ICLR 2018 Experiment
In this section we describe in more detail on the similar scoring model and the optimization formulation used in our ICLR 2018 experiment in Section 5.2.
29

Xu, Zhao, Shi, Zhang & Shah

A.1 Similarity Scoring Model
We ﬁrst use text representations to model the reviewers. In particular, for each reviewer in the pool, we scrape their (at most 10) most recent papers from arXiv8 as the corresponding text representation. To preprocess the text of papers and reviewers, we remove the stop words, tokenize the text, and then use the PorterStemmer (Bird et al., 2009) to obtain the word stem of each word. After the preprocessing step, the dictionary contains d “ 290158 unique words. Based on this dictionary, we use the vector space model (Salton et al., 1975) to represent each reviewer/paper as a vector in Rd. The ICLR 2018 data contains 911 submitted papers and 2435 reviewers, hence there are N “ 3346 documents in total.
To compute the similarity scores between reviewers and papers, for each document D (reviewer or paper), we compute the corresponding term frequency-inverse document frequency (tf-idf) (Schütze et al., 2008) score as the vector representation. Speciﬁcally, for a term w in the document, we use Nw to denote the number of times w appears in the corpus that contains N documents. Then the inverse document frequency of the term w is given by:

idfpwq :“ log N . Nw

To prevent a bias towards longer documents, e.g., raw frequency of w divided by the raw frequency of the most occurring term in the document D, we use the following augmented frequency as the term frequency of w in document D:

tfpw, Dq :“ 1 ` 1

fw,D

,

2 2 maxtfw1,D : w1 P Du

where we use fw,D to denote the number of times term w appearing in D. Let vD P Rd be the vector representation of document D. Then the value of the coordinate corresponding to term w is given by:

ˆ1 1

fw,D

˙

N

vDpwq “ tfpw, Dq ˆ idfpwq “ 2 ` 2 maxtfw1,D : w1 P Du ˆ log Nw .

We then construct the similarity matrix S P Rmˆn between reviewers and papers whose each entry sij P r0, 1s corresponds to the similarity score between reviewer ri and paper pj. sij is given by the cosine similarity of the corresponding tf-idf vectors:

sij “ vrTi vpj P r0, 1s. }vri }2 ¨ }vpj }2

A.2 The Reviewer-Paper Assignment Algorithm
Matching is the process of assigning papers to reviewers. Given the similarity score matrix S P Rmˆn, we solve the following optimization problem, as used in the current TPMS system, to compute the assignment. The optimization problem formulated in (4) is an integer
8. To ensure that the downloaded papers belong to the corresponding author in the general area of artiﬁcial intelligence, we only scrape papers under the following categories: cs.LG, cs.AI, stat.ML, cs.CV, cs.NE, cs.CL, cs.GT and cs.RO.

30

On Strategyproof Conference Peer Review

program, where the objective function corresponds to maximizing the sum of similarity scores in the matching. Here for any reviewer-paper pair pi, jq, we have aij “ 1 iﬀ paper pj is assigned to reviewer ri in the matching:

maximize
aij

ÿÿ sij aij
iPrms jPrns

subject to aij P t0, 1u, @i P rms, j P rns

ÿ aij ď µ, @i P rms

(4)

j

ÿ aij ě λ, @j P rns
i

The

constraint

ř
j

aij

ď

µ

means

that

we

restrict

the

maximum

number

of

papers

assigned

to

a

reviewer

to

be

µ.

Furthermore,

we

also

use

the

constraint

ř
i

aij

ě

λ

to

enforce

that

each paper should be reviewed by at least λ reviewers. In the ICLR 2018 data the number of

optimization variables aij is more than 2 million, which is intractable to solve using existing

integer program solvers. So instead, we can relax the above integer program to the following

linear program (LP):

maximize
aij

ÿÿ sij aij
iPrms jPrns

subject to 0 ď aij ď 1, @i P rms, j P rns

ÿ aij ď µ, @i P rms

(5)

j

ÿ aij ě λ, @j P rns
i

In the above LP we relax the integral constraint over aij in (4) to 0 ď aij ď 1, @i P rms, j P rns.

Due to the relaxation, it is clear that the optimal value of (5) is at least that of (4). On the

other

hand,

observe

that

if

we

reformulate

the

constraints

ř
j

aij

ď

µ

and

ř
i

aij

ě

λ

into

the matrix form, then the corresponding constraint matrix will be the node-edge incidence

matrix of a complete bipartite graph consisting of a set of reviewers and a set of papers. It

follows from a known suﬃcient condition (Fulkerson and Gross, 1965) that the incidence

matrix of a bipartite graph is totally unimodular, which implies that the solution of the LP

in (5) is guaranteed to be integral. Hence in order to obtain the optimal solution of (4), we

can use existing polynomial time solvers to solve the relaxed LP, and since the constraint

matrix in (4) is totally unimodular, this gives us a polynomial time algorithm to compute

the optimal solution of (4). In our implementation we use the GNU Linear Programming

Kit (GLPK) (Makhorin, 2001) that implements the simplex algorithm to solve the LP in (5).

31

