Measuring and Increasing Context Usage in Context-Aware Machine Translation
Patrick Fernandes1,2,3 Kayo Yin1 Graham Neubig1 Andre´ F. T. Martins2,3,4 1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 2Instituto Superior Te´cnico & LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal 3Instituto de Telecomunicac¸o˜es, Lisbon, Portugal 4Unbabel, Lisbon, Portugal
{pfernand, kayoy, gneubig}@cs.cmu.edu andre.t.martins@tecnico.ulisboa.pt

arXiv:2105.03482v2 [cs.CL] 2 Jun 2021

Abstract

Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context — context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We ﬁnd that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reﬂects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.1
1 Introduction
While neural machine translation (NMT) is reported to have achieved human parity in some domains and language pairs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; La¨ubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT
1https://github.com/neulab/contextual-mt

Figure 1: Illustration of how we can measure context usage by a model qMT as the amount of information gained when a model is given the context C and source X vs when the model is only given the X.
models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020).
We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing training paradigms. However, even quantifying model usage of context is an ongoing challenge; while contrastive evaluation has been proposed to measure performance on inter-sentential discourse phenomena (Mu¨ller et al., 2018; Bawden et al., 2018), this approach is conﬁned to a narrow set of phenomena, such as pronoun translation and lexical cohesion. A toolbox to measure the impact of context in broader settings is still missing.

Source:

The Church is merciful. . . It always welcomes the misguided lamb.

Target: Baseline
Context-Aware
Context-Aware w/ our method

Die Kirche ist barmherzig. . .
Es heisst die fehlgeleiteten Scha¨ﬂein immer willkommen. Es heisst die fehlgeleiteten Scha¨ﬂein immer willkommen. Sie heisst die fehlgeleiteten Scha¨ﬂein immer willkommen.

Table 1: Example where context (italic) is needed to correctly translate the pronoun “it”. Both the sentencelevel baseline and context-aware model fail to correctly translate it while the context-aware model trained with COWORD dropout correctly captures the context.

To address the limitations above, we take inspiration from the recent work of Bugliarello et al. (2020) and propose a new metric, conditional cross-mutual information (CXMI, §3), to measure quantitatively how much context-aware models actually use the provided context by comparing the model distributions over a dataset with and without context. Figure 1 illustrates how it measures context usage. This metric applies to any probabilistic context-aware machine translation model, not only the ones used in this paper. We release a software package to encourage the use of this metric in future context-aware machine translation research. We then perform a rigorous empirical analysis of the CXMI between the context and target for different context sizes, and between source and target context. We ﬁnd that: (1) context-aware models use some information from the context, but the amount of information used does not increase uniformly with the context size, and can even lead to a reduction in context usage; (2) target context seems to be used more by models than source context.
Given the ﬁndings, we next consider how to encourage models to use more context. Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed COWORD dropout (§4). Put simply, we randomly drop words from the current source sentence by replacing them with a placeholder token. Intuitively, this encourages the model to use extra-sentential information to compensate for the missing information in the current source sentence. We show that models trained with COWORD dropout not only increase context usage compared to models trained

without it but also improve the quality of translation, both according to standard evaluation metrics (BLEU and COMET) and according to contrastive evaluation based on inter-sentential discourse phenomena such as anaphoric pronoun resolution and lexical cohesion (§4.2, Table 1).
2 Context-Aware Neural Machine Translation
We are interested in learning a system that translates documents consisting of multiple sentences between two languages.2 More formally, given a corpus of parallel documents in two languages, D = {D1, ..., DN }, where each document is a sequence of source and target sentences, D = {(x(1), y(1)), ..., (x(K), y(K))}, we are interested in learning the mapping between the two languages.
We consider the typical (auto-regressive) neural machine translation system qθ parameterized by θ. The probability of translating x(i) into y(i) given the context of the sentence C(i) is
T
qθ(y(i)|x(i), C(i)) = qθ(yt(i)|x(i), y<(i)t, C(i))
t=1
where yt(i) represents the tth token of sentence y(i). This context can take various forms. On one end, we have the case where no context is passed, C(i) = ∅, and the problem is reduced to sentence-level translation. On the other end, we have the case where all the source sentences and all the previous generated target sentences are passed as context C(i) = {x(1), ..., x(K), y(1), ..., y(i−1)}.
As mentioned, there are many architectural approaches to leveraging context (see §5 for a more complete review), and the methods that we present in this paper are compatible with most architectures because they do not specify how the model qθ uses the context. In experiments, we focus mostly on the simpler approach of concatenating the context to the current sentences (Tiedemann and Scherrer, 2017). Recent work by Lopes et al. (2020) has shown that, given enough data (either through pretraining or larger contextual datasets), this simple approach tends to be competitive with or even outperform its more complex counterparts
2Here, a “document” could be an actual document but it could also represent other contextual collections of text, such as a sequence of dialogue utterances.

3 Measuring Context Usage
3.1 Conditional Cross-Mutual Information
While context-aware models allow use of context, they do not ensure contextual information is actually used: models could just be relying on the current source sentence and/or previously generated target words from the same sentence when generating the output.
Contrastive evaluation, where models are assessed based on the ability to distinguish correct translations from contrastive ones, is a common way to assess the ability of context-aware models to capture speciﬁc discourse phenomena that require inter-sentential context, such as anaphora resolution (Mu¨ller et al., 2018) and lexical cohesion (Bawden et al., 2018). However, these methods only provide an indirect measure of context usage with respect to a limited number of phenomena and can fail to capture other, unknown ways in which the model might be using context. Kim et al. (2019) showed that most improvements to translation quality are due to non-interpretable usages of context, such as the introduction of noise that acts as a regularizer to the encoder/decoder. This problem is further exacerbated by the fact that there is no clear deﬁnition of what entails “context usage”.
In a different context, Bugliarello et al. (2020) introduced cross-mutual information (XMI), to measure the “difﬁculty” of translating between different language pairs in sentence-level neural machine translation. Given a language model qLM for a target sentence Y and a translation model qMT for translating from X to Y , XMI is deﬁned as:
XMI(X → Y ) = HqLM (Y ) − HqMT (Y |X),
where HqLM denotes the cross-entropy of the target sentence Y under the language model qLM and HqMT the conditional cross-entropy of Y given X under the translation model qMT . This allows us to measure how much information the source sentence gives us about the target sentence (an analogue of mutual information for cross-entropy). In the case where qLM and qMT perfectly model the underlying probabilities we would have XMI(X → Y ) = MI(X, Y ), the true mutual information.
Taking inspiration from the above, we propose Conditional Cross-Mutual Information (CXMI), a new measure of the inﬂuence of context on a model’s predictions. This is done by considering an additional variable for the context C and

measuring how much information the context C provides about the target Y given the source X. This can then be formulated as

CXMI(C → Y |X) =
HqMTA (Y |X) − HqMTC (Y |X, C)
where HqMTA is the entropy of a context-agnostic machine translation model, and HqMTC refers to a context-aware machine translation model. This quantity can be estimated (see Appendix A for a more formal derivation) over an held-out test set with N sentence pairs and the respective context as:

CXMI(C → Y |X) ≈

1N

−

log

qMTA (y(i)|x(i))

N i=1 qMTC (y(i)|x(i), C(i))

While qMTA and qMTC can, in theory, be any models, we are interested in removing any confounding factors other than the context that might lead to instability in the estimates of the distributions. For example, if qMTA and qMTC use completely different models, it would not be clear if the difference in the probability estimates is due to the introduction of context or due to other extraneous factors such as differences in architectures, training regimens, or random seeds. To address this we consider a single model, qMT , that is able to translate with and without context (more on how this achieved in §3.2). We can then set the context-agnostic model and the contextual model to be the same model qMTA = qMTC = qMT . This way we attribute the information gain to the introduction of context. Throughout the rest of this work, when we reference “context usage” we will precisely mean this information gain (or loss).

3.2 Experiments
Data We experiment with a document-level translation task by training models on the IWSLT2017 (Cettolo et al., 2012) dataset for language pairs EN → DE and EN → FR (with approximately 200K sentences for both pairs). We use the test sets 2011-2014 as validation sets and the 2015 as test sets. To address the concerns pointed out by Lopes et al. (2020) that gains in performance are due to the use of small training corpora and weak baselines, we use Paracrawl (Espla` et al., 2019) and perform some data cleaning based on language identiﬁcation tools, creating a pretraining dataset of around

82M and 104M sentence pairs for EN → DE and EN → FR respectively.
All data is encoded/vectorized with byte-pair encoding (Sennrich et al., 2016b) using the SentencePiece framework (Kudo and Richardson, 2018). For the non-pretrained case, we use 20K vocabulary size shared across source/target, while for the pretrained case we use a 32K vocabulary size.
Besides translation quality, we also evaluate our models on two contrastive datasets for different discourse phenomena to better assess the ability of our models to capture context (more on this in §4.2):
• For the EN → DE language pair, we evaluate on the ContraPro dataset (Mu¨ller et al., 2018), targeting anaphoric pronoun resolution. Source-side sentences contain the English anaphoric pronoun it while target-side sentences contain the corresponding German translations er, sie or es. Contrastive erroneous translations are automatically created by replacing the correct pronoun with one of the other two. The test set contains 4,000 examples for each target pronoun type and context is needed to correctly disambiguate. Context includes the four previous sentences
• For the EN → FR language pair, we evaluate on the dataset by Bawden et al. (2018) targeting anaphoric pronoun resolution and lexical cohesion. It contains 200 manually curated examples for each phenomenon. Anaphora examples include singular and plural personal and possessive pronouns that require context to be correctly inferred and the dataset is balanced such that a model that does not use context can only achieve 50% accuracy. Context includes the previous sentence
Models and Optimization For all our experiments, we consider an encoder-decoder Transformer architecture (Vaswani et al., 2017). In particular, we train the transformer small (hidden size of 512, feedforward size of 1024, 6 layers, 8 attention heads). For the pretrained setup, we also pre-train a transformer large architecture (hidden size of 1024, feedforward size of 4096, 6 layers, 16 attention heads) and subsequently ﬁne-tune on the IWSL2017 datasets.
As in Vaswani et al. (2017), we train using the Adam optimizer with β1 = 0.9 and β2 = 0.98 and use an inverse square root learning rate scheduler,

with an initial value of 10−4 and 5 × 10−4 for pretrained and non-pretrained cases respectively, and with a linear warm-up in the ﬁrst 4000 steps. We train the models with early stopping on the validation perplexity.
We train all our models on top of the Fairseq framework (Ott et al., 2019).
What Context Matters? To assess the relative importance of different context sizes on both the source and target side, we start by considering two models, one for the source-side context and one for the target-side context, that receive context of size k, C(i) = {x(i−k), . . . , x(i−1)} or C(i) = {y(i−k), . . . , y(i−1)}. During training, k is selected randomly to be in {1, . . . , 4} for every example. This way the model is trained to translate the same source without and with different context sizes and is thus able to translate based on any context size in that interval.
Figure 2 shows the CXMI values computed over the test set as a function of the context size for both the source-side and target-side contextual models for both the non-pretrained and pretrained regimens for the EN → DE language pair. Results for the EN → FR language pair are similar and can be found in Appendix B.
For the non-pretrained case, for both the source and target context, the biggest jump in context usage is when we increase the context size from 0 to 1. After that, increasing the context size leads to diminishing increases in context usage and even reduced context usage for the source-side context. Interestingly, when the model is stronger, such as in the pretrained case, we can see that it can leverage target-side context even better than the nonpretrained case, with a similar trend of diminishing increases in context usage for both regimes. However, this is not the case for the source-side context, and it seems that the pretrained model is barely able to use the contextual information on this side.
Overall, for this regime, we can conclude that having a context size of one or two previous sentences on both sides is beneﬁcial to the model, and that target-side context is slightly more used than source-side context. This appears to corroborate the ﬁndings of Bawden et al. (2018) that target-side context is more effective than the source context.

·10−2 2

·10−2 2

CXMI

1

1

0 Source

0 Source

Target

Target

−1

−1

0

1

2

3

4

0

1

2

3

4

Context Size

Context Size

Figure 2: CXMI values for the EN → DE as a function of source and target context sizes for non-pretrained (left) and pretrained (right) models.

Context Size
1 2 3 4

(1)
0.365 0.366 0.367 0.366

rpb
(2)
0.315 -

(3)
0.206 -

Table 2: Point-Biserial correlation coefﬁcients on the contrastive datasets with pretrained models for different context sizes. Measured on ContraPro (1) and Bawden et al. (2018), both for pronoun resolution (2) and lexical cohesion (3). Bold values mean the correlation is statistically signiﬁcant with p < 0.01.

Does CXMI Really Measure Context Usage? To assert that CXMI correlates with interpretable measures of context usage, we perform a correlation analysis with the performance in the contrastive datasets mentioned. In these datasets, usage of context is evident where the model picks the right answer when it is passed the context and is not able to do so when no context is given. Thus Table 2 shows the point-biserial correlation coefﬁcient3 between the per-sample CXMI and binary random variable and a binary variable that takes the value 1 if the contextual model picks the correct translation and the non-contextual model picks the incorrect one, for different context sizes on the pretrained model. We can see that there is a statistically signiﬁcant correlation between both values, which strengthens the notion that CXMI captures previous measures of context usage to some extent.
3The Point-Biserial correlation coefﬁcient is a special case of the Pearson correlation coefﬁcient when one of the random variables is dichotomous.

4 Increasing Context Usage

4.1 Context-aware Word Dropout
Motivated by the above results demonstrating the limited context usage of models trained using the standard MLE training paradigm, particularly with respect to more distant context, we now ask the question: “Is it possible to modify the training methodology to increase context usage by the model?” As an answer, we extend a popular regularization technique used in sentence-level machine translation, word dropout (Sennrich et al., 2016a), to the context-aware setting. The idea behind context-aware word (COWORD) dropout is to model the translation probability between x(i) and y(i) as

T
pθ(y(i)|x(i)) = pθ(yt(i)|x˜(i), y<(i)t, C(i)),
t=1

where x˜(i) is a perturbed version of the current source sentence generated by randomly dropping tokens and replacing them with a mask token given a dropout probability p:

rt(i) ∼ Bernoulli(p)

x˜(ti) =

MASK
x(ti)

if rt(i) = 1 otherwise.

In the case where no context is passed C(i) = ∅, COWORD dropout reduces to word dropout. The intuition behind such a perturbation is that, by dropping information from the current source and not the context, we increase the relative reliability of context C(i), therefore providing the inductive bias

·10−2

2

CXMI

1
0 0

p = 0.0 p = 0.1 p = 0.2 p = 0.3

1

2

3

4

Context Size

Figure 3: CXMI values as a function target context size for different values of COWORD dropout

that context is important for the translation. We will see in §4.2 that this inductive bias is beneﬁcial and that COWORD dropout not only improves performance but also increases context usage.
4.2 Experiments
Setup As in §3.2, we consider transformer models trained on the IWSLT2017 for both EN → DE and EN → FR, both from scratch and pretrained using the procedure previously described. In particular, due to ﬁndings in the previous section, we consider models with either only target-side context or both source-side and target-side context.
Context Usage To assess if our proposed regularization technique, COWORD dropout, increases context usage by models, we train a model using the same dynamic context size setting used in §3.2.
Figure 3 plots the CXMI values on the test set as a function of the target context size as we increase the dropout value p. We see that increasing this value consistently increases context usage according to CXMI across different context sizes. Note that, at test time, COWORD dropout is disabled, which means that it provides inductive bias only during training and models learn to use more context by themselves.
Table 3 illustrates some examples where the COWORD dropout increased the per-sample CXMI signiﬁcantly. While the model only has access to target context, we present the source context for clarity. In the ﬁrst example, while the source is a complete sentence, the target is only a fragment of one so the context helps complete it. In the other two examples shown, we can see that context helps disambiguate the gender of the German

translation of the English pronoun it. Interestingly, the words that use context the most according to CXMI match very closely to the ones that native speakers annotated.
Translation Quality To evaluate if the increased usage of context correlates with better machine translation quality, based on the previous experiments on context usage and values for COWORD dropout, we consider three models trained with ﬁxed-size context:
• A baseline that has no context, reducing to sentence-level model ie: i.e., C(i) = ∅;
• a one-to-two model having as context the previous target sentence, i.e., C(i) = {y(i−1)};
• a two-to-two model having as context the previous source sentence and the previous target sentence, i.e., C(i) = {x(i−1), y(i−1)}.
In addition, to explore the beneﬁts of COWORD dropout in other architectures, we also train a one-to-two multi-encoder (Jean et al., 2017) transformer small model (more details in Appendix §C). For all models with target context, when decoding, we use the previous decoded sentences as target context.
Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of COWORD dropout p. We also run the baseline with COWORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020).
For the non-pretrained case, we can see that a COWORD dropout value p > 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with COWORD dropout still always outperform models trained without it. This is perhaps a reﬂection of the general trend that better models are harder to improve.

Source Context
More people watched games because it was faster. The ball comes off track.
I really think that this lie that we’ve been sold about disability is the greatest injustice

Source
It was more entertaining
You don’t know where it’s going to land It makes life hard for us

Target Context
Mehr Menschen sahen zu, die Spiele wurden schneller Der Ball ist außer Kontrolle
Meiner Meinung nach ist diese Luge u¨ber Behinderung eine schreiende Ungerechtigkeit

Target und unterhaltsamer.
Sie wissen nicht, wo er landet.
Sie macht uns das Leben schwer.

∆CXMI 0.53
0.33
0.25

Table 3: Examples where models with COWORD dropout use the target context more than models trained without it. Word highlighted blue in the context are used to disambiguate translations while highlighted green in the target use context according to native speakers. Words underlined in the target are the ones with the highest per-word CXMI i.e. the ones that use the most context according to the model

EN → DE

EN → FR

w/ pretraining

w/ pretraining

p BLEU COMET BLEU COMET BLEU COMET BLEU COMET

0.0 26.36 baseline 0.1 27.26
0.2 26.97

0.083 0.159 0.163

35.10 35.15 35.13

0.521 0.525 0.524

37.62 38.16 38.34

0.450 0.472 0.474

42.98 43.28 42.99

0.679 0.679 0.678

1-to-2

0.0 26.60 0.1 27.36 0.2 27.33

0.087 0.174 0.193

35.22 34.92 34.75

0.528 0.527 0.524

37.59 38.25 38.27

0.450 0.472 0.485

42.89 42.88 42.90

0.672 0.677 0.678

2-to-2

0.0 26.85 0.1 27.72 0.2 27.21

0.090 0.169 0.177

34.47 34.51 34.65

0.471 0.522 0.525

37.54 38.30 38.15

0.453 0.467 0.468

42.97 42.95 42.88

0.674 0.676 0.675

Table 4: Results on IWSLT2017 with different probabilities for COWORD dropout. Averaged across three runs for each method.

EN → DE

EN → FR

p BLEU COMET BLEU COMET

0.0 26.36 baseline 0.1 27.26
0.2 26.97

0.083 0.159 0.163

37.62 38.16 38.34

0.450 0.472 0.474

multi

0.0 26.64 0.1 27.45 0.2 27.31

0.104 0.190 0.190

37.85 37.98 38.30

0.466 0.460 0.484

Table 5: Results on IWSLT2017 for a multi-encoder 1-to-2 model with different probabilities for COWORD dropout. Averaged across three runs for each method.

Table 5 shows that COWORD dropout is also helpful for the multi-encoder model, with COWORD dropout helping signiﬁcantly. This shows that this method could be helpful for contextaware architectures other than concatenationbased.

Discourse Phenomena While automatic metrics such as BLEU and COMET allow us to measure translation quality, they mostly target sentencelevel quality and do not speciﬁcally focus on phenomena that require context-awareness. Contrastive datasets, as described in §3.2, allow us to measure the performance of context-aware models in speciﬁc discourse phenomena by comparing the probability of correct translation against the contrastive translations. Models that capture the targeted discourse phenomena well will consistently rank the correct translation higher than the contrastive ones. While there is a disconnect between the translation (done via decoding) and contrastive evaluation, it is currently the best way to measure a model’s performance on context-aware discourse phenomena.

EN → DE

EN → FR

w/ pretraining

w/ pretraining

p Pronouns Pronouns Pronouns Cohesion Pronouns Cohesion

baseline 0.0 42.96

46.57

50.00

50.00

50.00

50.00

0.0 1-to-2 0.1
0.2

57.36 58.70 60.72

76.79 76.28 77.52

68.16 72.33 72.99

49.99 51.49 52.16

86.83 86.49 85.66

56.83 56.66 56.49

0.0 2-to-2 0.1
0.2

61.06 66.00 65.47

80.33 80.35 79.97

72.16 73.99 73.99

50.99 52.49 52.49

85.66 87.16 88.49

64.33 65.99 63.99

Table 6: Results on anaphoric pronoun resolution and lexical cohesion contrastive datasets with different probabilities for COWORD dropout. Averaged across three runs for each method.

EN → DE

EN → FR

p Pronouns Pronouns Cohesion

baseline 0.0 42.96

50.00

50.00

0.0 multi 0.1
0.2

42.85 47.29 47.57

49.74 51.74 52.50

49.99 50.24 50.99

Table 7: Results on anaphoric pronoun resolution and lexical cohesion contrastive datasets for the multiencoder 1-to-2 model with different probabilities for COWORD dropout. Averaged across three runs for each method.

Table 6 shows the average performance over the contrastive datasets of the baseline and contextual models for both the (non-)pretrained settings, with increasing values of COWORD dropout p. We can see that in general, increasing COWORD dropout leads to improved performance, particularly for the non-pretrained case. This gain is particularly clear for pronoun resolution and the EN → DE language pair. We hypothesise that this is due to the small size of the contrastive sets for the EN → FR language pair, which leads to high variance.
Table 7 similarly shows that COWORD dropout improves the performance of the multi-encoder model across all phenomena, which again shows that our proposed regularization method has beneﬁts for multiple architectures for context-aware machine translation. Curiously, when these models are trained without COWORD dropout, they achieve performance similar to the sentence-level baseline, while when dropout is applied, they are able to effectively start using context.

5 Related Work
Context-aware Machine Translation There have been many works in the literature that try to incorporate context into NMT systems. Tiedemann and Scherrer (2017) ﬁrst proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspeciﬁc encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with COWORD dropout, Jean and Cho (2019) attempted to maximise sensitivity to context by introducing a margin-based regularization term to explicitly encourage context usage.
For a more detailed overview, Maruf et al. (2019b) extensively describe the different approaches and how they leverage context. While these models lead to improvements with small training sets, Lopes et al. (2020) showed that the improvements are negligible when compared with the concatenation baseline when using larger datasets.

However, importantly, both our metric CXMI for measuring context usage and the proposed regularization method of COWORD dropout, can theoretically be applied to any of the above-mentioned methods.
Evaluation In terms of evaluation, most previous work focuses on targeting a system’s performance on contrastive datasets for speciﬁc inter-sentential discourse phenomena. Mu¨ller et al. (2018) built a large-scale dataset for anaphoric pronoun resolution, Bawden et al. (2018) manually created a dataset for both pronoun resolution and lexical choice and Voita et al. (2019) created a dataset that targets deixis, ellipsis and lexical cohesion. Stojanovski et al. (2020) showed through adversarial attacks that models that do well on other contrastive datasets rely on surface heuristics and create a contrastive dataset to address this. In contrast, our CXMI metric is phenomenon-agnostic and can be measured with respect to all phenomena that require context in translation.
Information-Theoretic Analysis Bugliarello et al. (2020) ﬁrst proposed cross-mutual information (XMI) in the context of measuring the difﬁculty of translating between languages. Our work differs in that we propose a conditional version of XMI, where S is always observed, and we use it to assess the information gain of context rather than the difﬁculty of translating different languages.
6 Implications and Future Work
We introduce a new, architecture-agnostic, metric to measure how context-aware machine translation models are using context and propose a simple regularization technique to increase context usage by these models. Our results are theoretically applicable to almost all recently proposed context-aware models and future work should go about measuring exactly how much these models leverage context and if COWORD dropout also improves context usage and performance in these.
We also hope this work motivates exploring (C)XMI for other uses cases where measuring the relevance/usage of inputs to a particular model other than context-aware machine translation. It could, for example, be used in conditional language modelling to analyse how the inputs we are conditioning on are being used by the model.

7 Acknowledgements
We would like to thank all the members of DeepSPIN, NeuLab, and Unbabel who provided feedback on earlier versions of this work. This work was supported by the European Research Council (ERC StG DeepSPIN 758969), by the P2020 programs MAIA and Unbabel4EU (LISBOA-01-0247FEDER-045909 and LISBOA-01-0247-FEDER042671), and by the Fundac¸a˜o para a Cieˆncia e Tecnologia through contracts SFRH/BD/150706/2020 and UIDB/50008/2020.
References
Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1304–1313, New Orleans, Louisiana. Association for Computational Linguistics.
Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki Okazaki. 2020. It’s easier to translate out of English than into it: Measuring neural translation difﬁculty by cross-mutual information. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, Online. Association for Computational Linguistics.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 261–268, Trento, Italy. European Association for Machine Translation.
Junxuan Chen, Xiang Li, Jiarui Zhang, Chulun Zhou, Jianwei Cui, Bin Wang, and Jinsong Su. 2020. Modeling discourse structure for document-level neural machine translation. In Proceedings of the First Workshop on Automatic Simultaneous Translation, pages 30–36, Seattle, Washington. Association for Computational Linguistics.
Miquel Espla`, Mikel Forcada, Gema Ram´ırez-Sa´nchez, and Hieu Hoang. 2019. ParaCrawl: Web-scale parallel corpora for the languages of the EU. In Proceedings of Machine Translation Summit XVII Volume 2: Translator, Project and User Tracks, pages 118–119, Dublin, Ireland. European Association for Machine Translation.
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce

Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018. Achieving human parity on automatic chinese to english news translation. CoRR, abs/1803.05567.
Se´bastien Jean and Kyunghyun Cho. 2019. Contextaware learning for neural machine translation. CoRR, abs/1903.04715.
Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun Cho. 2017. Does neural machine translation beneﬁt from larger context?
Yunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019. When and why is document-level context useful in neural machine translation? In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 24–34, Hong Kong, China. Association for Computational Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.
Samuel La¨ubli, Rico Sennrich, and Martin Volk. 2018. Has machine translation achieved human parity? a case for document-level evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796, Brussels, Belgium. Association for Computational Linguistics.
Anto´nio Lopes, M. Amin Farajian, Rachel Bawden, Michael Zhang, and Andre´ F. T. Martins. 2020. Document-level neural MT: A systematic comparison. In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 225–234, Lisboa, Portugal. European Association for Machine Translation.
Sameen Maruf and Gholamreza Haffari. 2018. Document context neural machine translation with memory networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1275– 1284, Melbourne, Australia. Association for Computational Linguistics.
Sameen Maruf, Andre´ F. T. Martins, and Gholamreza Haffari. 2019a. Selective attention for contextaware neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3092–3102, Minneapolis, Minnesota. Association for Computational Linguistics.
Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. 2019b. A survey on document-level machine translation: Methods and evaluation. ArXiv, abs/1912.08494.

Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-level neural machine translation with hierarchical attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2947–2954, Brussels, Belgium. Association for Computational Linguistics.
Mathias Mu¨ller, Annette Rios, Elena Voita, and Rico Sennrich. 2018. A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 61–72, Brussels, Belgium. Association for Computational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.
Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh neural machine translation systems for WMT 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 371–376, Berlin, Germany. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715– 1725, Berlin, Germany. Association for Computational Linguistics.
Dario Stojanovski, Benno Krojer, Denis Peskov, and Alexander Fraser. 2020. ContraCAT: Contrastive coreference analytical templates for machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4732– 4749, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2020. Capturing longer context for document-level neural machine translation: A multi-resolutional approach. arXiv, abs/2010.08961.
Jo¨rg Tiedemann and Yves Scherrer. 2017. Neural machine translation with extended context. In Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, Copenhagen, Denmark. Association for Computational Linguistics.
Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018. Attaining the unattainable? reassessing claims of human parity in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 113–123, Brussels, Belgium. Association for Computational Linguistics.
Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. 2018a. Learning to remember translation history with a continuous cache. Transactions of the Association for Computational Linguistics, 6:407–420.
Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. 2018b. Learning to remember translation history with a continuous cache. Transactions of the Association for Computational Linguistics, 6:407–420.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998–6008.
Elena Voita, Rico Sennrich, and Ivan Titov. 2019. When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1198–1212, Florence, Italy. Association for Computational Linguistics.
Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017. Exploiting cross-sentence context for neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2826–2831, Copenhagen, Denmark. Association for Computational Linguistics.
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018. Improving the transformer translation model with document-level context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 533–542, Brussels, Belgium. Association for Computational Linguistics.

A Estimating CXMI

B CXMI for EN → FR

Let S denote a random variable over source sentences, T a random variable over target sentences and C a random variable over possible context. We assume these random variables are distributed according to some true, unknown distribution p(s, t, c). The cross-entropy between the true distribution p and a probabilistic context-aware neural translation model qMTC (t|s, c) is deﬁned as

HqMT (T |S, C) =

−

p(s, t, c) log qMT (s, t, c)

s∈VS∗ t∈VT∗ c∈VC∗

where

V

∗ S

,

VT∗

,

VC∗

represent

the

space

of

possi-

ble source sentences, target sentences and con-

texts respectively. Since we do not know the

true distribution p, we cannot compute this quan-

tity exactly. However, given a dataset of samples {(s(i), t(i), c(i))}Ni=0 assumed to be drawn from p, we can estimate this quantity using the Monte Carlo

estimator

HqMTC (T |S, C) ≈ 1
− N

N
log qMTC (s(i), t(i), c(i))
i=0

If we consider the marginal p(s, t) = c∈V ∗ p(s, t, c), we can by a similar argument ob-
C
tain an estimate for the cross-entropy for a contextagnostic neural translation model qMTA as:

1N

(i) (i)

HqMTA (T |S) ≈ − N log qMTA (s , t )

i=0

This leads trivially to the estimator for the crossmutual information:

CXMI(C → Y |X) ≈

1N

−

log

qMTA (y(i)|x(i))

N i=1 qMTC (y(i)|x(i), C(i))

·10−2 2

CXMI

1

0 Source

Target

−1

0

1

2

3

4

·10−2

2

CXMI

1

0 Source

Target

−1

0

1

2

3

4

Context Size

Figure 4: CXMI values as a function of source and target context sizes for non-pretrained (top) and pretrained (bottom) models for the EN → FR language pair.

C Multi-Encoder
For the multi-encoder model, we take the approach of initializing a separate transformer encoder for the context, with shared input-output embeddings with the original encoder (or decoder in the case of target context). The tokens in the current sentence attend to the context by the means of crossattention. There are several other ways of formulation a multi-encoder context-aware systems, and exploring them is left for future research.

