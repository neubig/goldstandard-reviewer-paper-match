arXiv:1503.07240v1 [cs.LG] 25 Mar 2015

Regularized Minimax Conditional Entropy for Crowdsourcing

Dengyong Zhou ∗

Qiang Liu † John C. Platt ‡ Nihar B. Shah¶

Christopher Meek §

Abstract
There is a rapidly increasing interest in crowdsourcing for data labeling. By crowdsourcing, a large number of labels can be often quickly gathered at low cost. However, the labels provided by the crowdsourcing workers are usually not of high quality. In this paper, we propose a minimax conditional entropy principle to infer ground truth from noisy crowdsourced labels. Under this principle, we derive a unique probabilistic labeling model jointly parameterized by worker ability and item diﬃculty. We also propose an objective measurement principle, and show that our method is the only method which satisﬁes this objective measurement principle. We validate our method through a variety of real crowdsourcing datasets with binary, multiclass or ordinal labels.
Keywords: crowdsourcing, human computation, minimax conditional entropy

∗Microsoft Research, Redmond, WA 98052. Email: dengyong.zhou@microsoft.com. †Department of Computer Science, University of California at Irvine, Irvine, CA 92637. Email: qliu1@uci.edu. ‡Microsoft Research, Redmond, WA 98052. Email: jplatt@microsoft.com. §Microsoft Research, Redmond, WA 98052. Email: meek@microsoft.com. ¶Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA 94720. Email: nihar@eecs.berkeley.edu.
1

1 Introduction
In many real-world applications, the quality of a machine learning system is governed by the number of labeled training examples, but the labor for data labeling is usually costly. There has been considerable machine learning research work on learning when there are only few labeled examples, such as semi-supervised learning and active learning. In recent years, with the emergence of crowdsourcing (or human computation) services like Amazon Mechanical Turk1, the costs associated with collecting labeled data in many domains have dropped dramatically enabling the collection of large amounts of labeled data at a low cost. However, the labels provided by the workers are often not of high quality, in part, due to misaligned incentives and a lack of domain expertise in the workers. To overcome this quality issue, in general, the items are redundantly labeled by several diﬀerent workers, and then the workers’ labels are aggregated in some manner, for example, majority voting.
The assumption underlying majority voting is that all workers are equally good so they have equal vote. Obviously, such an assumption does not reﬂect the truth. It is easy to imagine that one worker is more capable than another in some labeling task. More subtly, the skill level of a worker may signiﬁcantly vary from one labeling category to another. To address these issues, Dawid and Skene (1979) propose a model which assumes that each worker has a latent probabilistic confusion matrix for generating her labels. The oﬀ-diagonal elements of the matrix represent the probabilities that the worker mislabels an item from one class as another while the diagonal elements correspond to her accuracy in each class. The true labels of the items and the confusion matrices of the workers can be jointly estimated by maximizing the likelihood of the workers’ labels.
In the Dawid-Skene method, the performance of a worker characterized by her confusion matrix stays the same across all items in the same class. That is not true in many labeling tasks, where some items are more diﬃcult to label than others, and a worker is more likely to mislabel a diﬃcult item than an easy one. Moreover, an item may be easily mislabeled as some class rather than others by whoever labels it. To address these issues, we develop a minimax conditional entropy principle for crowdsourcing. Under this principle, we derive a unique probabilistic model which takes both worker ability and item diﬃculty into account. When item diﬃcult is ignored, our model seamlessly reduces to the classical Dawid-Skene model. We also propose a natural objective measurement principle, and show that our method is the only method which satisﬁes this objective measurement principle.
The work is an extension of the earlier results presented in (Zhou et al., 2012, 2014). We organize the paper as follows. In Section 2, we propose the minimax conditional entropy principle for aggregating multiclass labels collected from a crowd and derive its dual form. In Section 3, we develop regularized minimax conditional entropy for preventing overﬁtting and generating probabilistic labels. In Section 4, we propose the objective measurement principle which also leads to the probabilistic model derived from the minimax conditional entropy principle. In Section 5, we extend our minimax conditional entropy method to ordinal labels, where we need to introduce a new assumption called adjacency confusability. In Section 6, we present a simple yet eﬃcient coordinate ascent method to solve the minimax program through its dual form and also a method for model selection. Related work are discussed in Section 7. Empirical results on real crowdsourcing data with binary, multiclass or ordinal labels are reported in Section 8, and conclusion are presented in Section 9.
1https://www.mturk.com
2

item 1 item 2 · · · item n

worker 1 x11

x12 · · · x1n

worker 2 x21

x22 · · · x2n

···

···

··· ··· ···

worker m xm1 xm2 · · · xmn

item 1 item 2 · · · item n

worker 1 π11

π12 · · · π1n

worker 2 π21

π22 · · · π2n

···

···

··· ··· ···

worker m πm1 πm2 · · · πmn

Figure 1: Left table: observed labels xij provided by worker i for item j. Right table: underlying distributions πij of worker i for generating a label for item j. In our approach, the rows and columns of of the unobserved right table are constrained to match the rows and columns of the observed left table.

2 Minimax Conditional Entropy Principle
In this section, we present the minimax conditional entropy principle for aggregating crowdsourced multiclass labels in both its primal and dual forms. We also show that minimax conditional entropy is equivalent to minimizing Kullback-Leibler (KL) divergence.

2.1 Notation and Problem Setting
Assume that there are a group of workers indexed by i, a set of items indexed by j, and a number of classes indexed by k or c. Let xij be the observed label that worker i assigns to item j, and Xij be the corresponding random variable. Denote by Q(Yj = c) the unobserved true probability that item j belongs to class c. A special case is that Q(Yj = c) = 1 and Q(Yj = k) = 0 for any other class k = c. That is, the labels are deterministic. Denote by P (Xij = k|Yj = c) the probability that worker i labels item j as class k while the true label is c. Our goal is to estimate the unobserved true labels from the noisy workers’ labels.

2.2 Primal Form
Our approach is built upon two four-dimensional tensors with the four dimensions corresponding to workers i, items j, observed labels k, and true labels c. The ﬁrst tensor is referred to as the empirical confusion tensor of which each element is given by

φij(c, k) = Q(Yj = c)I(xij = k)

to represent an observed confusion from class c to class k by worker i on item j. The other tensor is referred to as the expected confusion tensor of which each element is given by

φij(c, k) = Q(Yj = c)P (Xij = k|Yj = c)

to represent an expected confusion from class c to class k by worker i on item j. We assume that the labels of the items are independent. Thus, the entropy of the observed
workers’ labels conditioned on the true labels can be written as

H(X|Y ) = − Q(Yj = c) P (Xij = k|Yj = c) log P (Xij = k|Yj = c).

j,c

i,k

3

worker 1 worker 2 worker 3

item 1 1 2 1

item 2 2 1 1

item 3 2 2 1

item 4 1 2 2

item 5 3 1 2

item 6 2 3 3





110

φ1 = 1 1 0 ,

011





110

φ2 = 0 2 0 ,

101





200

φ3 = 1 1 0

011

Figure 2: An illustration of the empirical confusion tensors. The table contains three workers’ labels over six items. These items are assumed to have deterministic true labels as follows: class 1 = {item 1, item 2}, class 2 = {item 3, item 4}, and class 3 = {item 5, item 6}. The (c, k)-th entry of matrix φi represents the number of the items labeled as class k by worker i given that their true labels are class c.

Both the distributions P and Q are unknown here. To attack this problem, we ﬁrst consider a

simpler problem: estimate P when Q is given. Then, we proceed to jointly estimating P and Q

when both are unknown.

Given the true label distribution Q, we propose to estimate P which generates the workers’

labels by

max H(X|Y ),

(1)

P

subject to the worker and item constraints (Figure 1)

φij(c, k) − φij(c, k) = 0, ∀i, k, c,

(2a)

j

φij(c, k) − φij(c, k) = 0, ∀j, k, c,

(2b)

i

plus the probability constraints

P (Xij = k|Yj = c) = 1, ∀i, j, c,

(3a)

k

Q(Yj = c) = 1, ∀j,

(3b)

c

Q(Yj = c) ≥ 0, ∀j, c.

(3c)

The constraints in Equation (2a) enforce the expected confusion counts in the worker dimension to match their empirical counterparts. Symmetrically, the constraints in Equation (2b) enforce the expected confusion counts in the item dimension to match their empirical counterparts. An illustration of empirical confusion tensors is shown in Figure 2.
When both the distributions P and Q are unknown, we propose to jointly estimate them by

min max H(X|Y ),

(4)

QP

subject to the constrains in Equation (2) and (3). Intuitively, entropy can be understood as a measure of uncertainty. Thus, minimizing the maximum conditional entropy means that, given

4

the true labels, the workers’s labels are the least random. Theoretically, minimizing the maximum conditional entropy can be connected to maximum likelihood. In what follows, we show how the connection is established.

2.3 Dual Form
The Lagrangian of the maximization problem in (4) can be written as

L = H(X|Y ) + Lσ + Lτ + Lλ

(5)

with

Lσ = σi(c, k)

i,c,k

j

Lτ = τj(c, k)

j,c,k

i

φij(c, k) − φij(c, k) , φij(c, k) − φij(c, k) ,

Lλ = λijc
i,j,c

P (Xij = k|Yj = c) − 1 ,
k

where σi(c, k), τj (c, k) and λijc are introduced as the Lagrange multipliers. By the Karush-KuhnTucker (KKT) conditions (Boyd and Vandenberghe, 2004),

∂L = 0, ∂P (Xij = k|Yj = c)

which implies

log P (Xij = k|Yj = c) = λijc − 1 + σi(c, k) + τj(c, k).

Combining the above equation and the probability constraints in (3a) eliminates λ and yields

1

P (Xij = k|Yj = c) = Zij exp[σi(c, k) + τj(c, k)],

(6)

where Zij is the normalization factor given by

Zij = exp[σi(c, k) + τj(c, k)].
k
Although the matrices [σi(c, k)] and [τj(c, k)] in Equation (6) come out as the mathematical consequence of minimax conditional entropy, they can be understood intuitively. We can consider the matrix [σi(c, k)] as the measure of the intrinsic ability of worker i. The (c, k)-th entry measures how likely worker i labels a randomly chosen item in class c as class k. Similarly, we can consider the matrix [τi(c, k)] as the measure of the intrinsic diﬃcult of item j. The (c, k)-th entry measures how likely item j in class c is labeled as class k by a randomly chosen worker. In the following, we refer to [σi(c, k)] as worker confusion matrices and [τi(c, k)] as item confusion matrices.
Substituting the labeling model in Equation (6) into the Lagrangian in Equation (5), we can obtain the dual form of the minimax problem (4) as (see Appendix A)

max

Q(Yj = c) log P (Xij = xij|Yj = c).

(7)

σ,τ,Q

j,c

i

5

It is obvious that, to be optimal, the true label distribution has to be deterministic. Thus, the dual Lagrangian can be equivalently expressed as the complete log-likelihood

log

Q(Yj = c) P (Xij = xij |Yj = c) .

jc

i

In Section 3, we show how to regularize the objective function in (4) to generate probabilistic labels.

2.4 Minimizing KL Divergence

Let us extend the two distributions P and Q to the product space X ×Y. We extend the distribution
Q by deﬁning Q(Xij = xij) = 1, and Q(Y ) stays the same. We extend the distribution P with P (X, Y ) = ij P (Xij|Yj)P (Yj), where P (Xij |Yj) is given by Equation (6), and P (Y ) is a uniform distribution over all possible classes. Then, we have

Theorem 2.1 When the true labels are deterministic, minimizing the KL divergence from Q to P,

that is,

Q(X, Y )

min DKL(Q P ) =
P,Q

Q(X, Y ) log P (X, Y ) ,

(8)

X,Y

is equivalent to the minimax problem in (4).

The proof is presented in Appendix B. A sketch of the proof is as follows. We show that,

DKL(Q

P ) = − Q(Yj = c) P (Xij = k|Yj = c) log P (Xij = k|Yj = c)

j,c

i,k

+ Q(Y ) log Q(Y ) − log P (Y ).
Y

By the deﬁnition of P (X, Y ), P (Y ) is a constant. Moreover, when the true labels are deterministic, we have
Q(Y ) log Q(Y ) = 0.
Y
This concludes the proof of this theorem.

3 Regularized Minimax Conditional Entropy
In this section, we regularize our minimax conditional entropy method to address two practical issues:
• Preventing overﬁtting. While crowdsourcing is cheap, collecting many redundant labels may be more expensive than hiring experts. Typically, the number of labels collected for each item is limited to a small number. In this case, the empirical counts in Equation (2) may not match their expected values. It is likely that they ﬂuctuate around their expected values although these ﬂuctuations are not large.

6

• Generating probabilistic labels. Our minimax conditional entropy method can only generate deterministic labels (see Section 2.3). In practice, probabilistic labels are usually more useful than deterministic labels. When the estimated label distribution for an item is close to uniform over several classes, we can either ask for more labels for the item from the crowd or forward the item to an external expert.
For addressing the issue of overﬁtting, we formulate our observation by replacing exact matching with approximate matching while penalizing large ﬂuctuations. For generating probabilistic labels, we consider an entropy regularization over the unknown true label distribution. This is motivated by the analysis in Section 2.4.
Formally, we regularize our minimax conditional entropy method as follows. Let us denote the entropy of the true label distribution by

H(Y ) = − Q(Yj = c) log Q(Yj = c).
j,c

To estimate the true labels, we consider

min max H(X|Y ) − H(Y ) − 1 Ω(ξ) − 1 Ψ(ζ) (9)

QP

α

β

subject to the relaxed worker and item constraints

φij(c, k) − φij(c, k) = ξi(c, k), ∀i, s,
j
φij(c, k) − φij(c, k) = ζj(c, k), ∀j, s,
i

(10a) (10b)

plus the probability constraints in Equation (3). The regularization functions Ω and Ψ are chosen as

1

2

Ω(ξ) = 2

[ξi(c, k)] ,

i c,k

1

2

Ψ(ζ) = 2

[ζj(c, k)] .

j c,k

(11a) (11b)

The new slack variables ξi(c, k), ζj (c, k) in Equation (10) model the possible ﬂuctuations. Note that these slack variables are not restricted to be positive. When there are a suﬃciently large number of observations, the ﬂuctuations should be approximately normally distributed, due to the central limit theorem. This observation motivates the choice of the regularization functions in (11) to penalize large ﬂuctuations. The entropy term H(Y ) in the objective function, which is introduced for generating probabilistic labels, can be regarded as penalizing a large deviation from the uniform distribution.
Substituting the labeling model from Equation (6) into the Lagrangian of (9), we obtain the dual form (see Appendix C)

max

Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y ) − αΩ∗(σ) − βΨ∗(τ ),

(12)

σ,τ,Q

j,c

i

7

where

Ω∗(σ) = 1

[σi(c, k)]2 ,

(13)

2 i c,k

Ψ∗(τ ) = 1

[τj(c, k)]2 .

(14)

2 j c,k

When α = 0 and β = 0, the objective function in (12) turns out to be a lower bound of the log marginal likelihood

log = log

P (Xij = xij |Yj = c)
jci
Q(Yj = c) P (Xij = xij|Yj = c) j c Q(Yj = c) i

≥ Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y ).

j,c

i

The last step is based on Jensen’s inequality. Maximizing the marginal likelihood is more appropriate than maximizing the complete likelihood since only the observed data matters in our inference.
Finally, we introduce a variant of our regularized minimax conditional entropy. It is obtained by restricting the feasible region of the slack variables through

ξi(c, c) = 0, ∀i.

(15)

c

This is equivalent to

φij(c, c) − φij(c, c) = 0, ∀i.
j,c

It says that, the empirical count of the correct answers from each worker is equal to its expectation. According to the law of large numbers, this assumption is approximately correct when a worker has a suﬃciently large number of correct answers. Note that this does not mean that the percentage of the correct answers from the worker has to be large. Let K denote the class size. Under the additional constraints in Equation (15), the dual problem can still be expressed by (12) except (see Appendix C)

Ω∗(σ) = 1

2
σi(c, c) − σi(c, c) +

2
σi(c, k) − σi(c, k) ,

(16)

2 i,c k=c

where

1 σi(c, c) = K σi(c, c),
c

1

σi(c, k) = K(K − 1)

σi(c, k).

c k=c

From our empirical evaluations, this variant is somewhat worse than its original version on most datasets. We include it here only for theoretic interest.

8

4 Objective Measurement Principle

In this section, we introduce a natural objective measurement principle, and show that the probabilistic labeling model in Equation (6) is a consequence of this principle.
Intuitively, the objective measurement principle can be described as follows:
1. A comparison of labeling diﬃculty between two items should be independent of which particular workers were involved in the comparison; and it should also be independent of which other items might also be compared.
2. Symmetrically, a comparison of labeling ability between two workers should be independent of which particular items were involved in the comparison; and it should also be independent of which other workers might also be compared.
Next we mathematically deﬁne the objective measurement principle. Assume that worker i has labeled items j and j′ in class c. Denote by E the event that one of
these two items is labeled as k, and the other is labeled as c. Formally,

E = I(Xij = k) + I(Xij′ = k) = 1, I(Xij = c) + I(Xij′ = c) = 1 . Denote by A the event that item j is labeled as k and item j′ is labeled as c. Formally,

A = Xij = k, Xij′ = c .

It is obvious that A ⊂ E. Now we formulate the requirement (1) in the objective measurement principle as follows: P (A|E) is independent of worker i. Note that

P (A|E) =

P (Xij = k|Yj = c)P (Xij′ = c|Yj′ = c)

.

P (Xij = k|Yj = c)P (Xij′ = c|Yj′ = c) + P (Xij = c|Yj = c)P (Xij′ = k|Yj′ = c)

Hence, P (A|E) is independent of worker i if and only if

P (Xij = k|Yj = c)P (Xij′ = c|Yj′ = c) P (Xij = c|Yj = c)P (Xij′ = k|Yj′ = c)
is independent of worker i. In other words, given another arbitrary worker i′, we should have

P (Xij = k|Yj = c)P (Xij′ = c|Yj′ = c) = P (Xi′j = k|Yj = c)P (Xi′j′ = c|Yj′ = c) . P (Xij = c|Yj = c)P (Xij′ = k|Yj′ = c) P (Xi′j = c|Yj = c)P (Xi′j′ = k|Yj′ = c) Without loss of generality, we choose i′ = 0, j′ = 0 as the ﬁxed references. Then,

P (Xij = k|Yj = c) ∝ P (Xi0 = k|Y0 = c) P (X0j = k|Yj = c) . P (Xij = c|Yj = c) P (Xi0 = c|Y0 = c) P (X0j = c|Yj = c)

By the fact that probabilities are nonnegative, we can write

P (Xi0 = k|Y0 = c) = exp[σi(c, k)], P (X0j = k|Yj = c) = exp[τj(c, k)].
The probabilistic labeling model in Equation (6) follows immediately. It is easy to verify that due to the symmetry between item diﬃculty and worker ability, we can instead start from formulating the requirement (2) in the objective measurement principle to achieve the same result. Hence, in this sense, the two requirements are actually redundant.

9

5 Extension to Ordinal Labels
In this section, we extend the minimax conditional entropy principle from multiclass to ordinal labels. Eliciting ordinal labels is important in tasks such as judging the relative quality of web search results or consumer products. Since ordinal labels are a special case of multiclass labels, the approach that we have developed in the previous sections can be used to aggregate ordinal labels. However, we observe that, in ordinal labeling, workers usually have an error pattern diﬀerent from what we observe in multiclass labeling. We summarize our observation as the adjacency confusability assumption, and formulate it by introducing a diﬀerent set of constraints for workers and items.

5.1 Adjacency Confusability
In ordinal labeling, workers usually have diﬃculty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We refer to this observation as adjacency confusability.
To illustrate this observation, let us consider the example of screening mammograms. A mammogram is an x-ray picture used to check for breast cancer in women. Radiologists often rate mammograms on a scale such as no cancer, benign cancer, possible malignancy, or malignancy. In screening mammograms, a radiologist may rate a mammogram which indicates possible malignancy as malignancy, but it is less likely that she rates a mammogram which indicates no cancer as malignancy.

5.2 Ordinal Minimax Conditional Entropy

In what follows, we construct a diﬀerent set of worker and item constraints to encode adjacency

confusability. The formulation leads to an ordinal labeling model parameterized with structured

confusion matrices for workers and items.

We introduce two symbols ∆ and ∇ which take on arbitrary binary relations in {≥, <}. Ordinal

labels are represented by consecutive integers, and the minima one is 0. To estimate the true ordinal

labels, we consider

min max H(X|Y )

(17)

QP

subject to the ordinal-based worker and item constraints

c∆s k∇s j φij (c, k) − φij(c, k) = 0, ∀i, s ≥ 1,

(18a)

c∆s k∇s i φij (c, k) − φij(c, k) = 0, ∀j, s ≥ 1,

(18b)

for all ∆, ∇ ∈ {≥, <}, and the probability constraints in (3). We exclude the case s = 0 in which the constraints trivially hold.
Let us explain the meaning of the constraints in Equation (18). To construct ordinal-based constraints, the ﬁrst issue that we have to address is how to compare the observed label xij and the true label Yj in an ordinal sense. For multiclass labels, as we have seen in Section 2, the label comparison problem is trivial: we only need to check whether they are equal or not. For ordinal labels, such a problem becomes tricky. Here, we propose an indirect comparison between two

10

Reference label

≥, <

≥, <

True label

Observed label

Figure 3: Indirect comparison between a true label and an observed label via comparing both to a reference label which varies through all possible values in a given ordinal label set.

ordinal labels by comparing both to a reference label s which varies through all possible values in a given ordinal label set (Figure 3). Consequently, for every chosen reference label s, we partition the Cartesian product of the label set into four disjoint regions

{(c, k)|c < s, k < s}, {(c, k)|c < s, k ≥ s}, {(c, k)|c ≥ s, k < s}, {(c, k)|c ≥ s, k ≥ s}.

A partition example is shown in Table 1 where the given label set is {0, 1, 2, 3}. Then, Equation (18a) deﬁnes a set of constraints for the workers by summing Equation (2a) over each region. Similarly, Equation (18b) deﬁnes a set of constraints for the items by summing Equation (2b) over each region.
From the discussion above, we can see that when there are more than two ordinal classes, the constraints in Equation (18) are less restrictive than those in Equation (2). Consequently, as we see below, the labeling model resulted from Equation (18) has fewer parameters. In the case in which there are only two ordinal classes, the sets of disjoint regions degenerate to pairs (c, k) and, thus, the sets of constraints in Equations (18) and (2) are identical.
Next we explain why we construct the ordinal-based constraints in such a way. Let us write

φij(c, k) =

Q(Yj = c)I(xij = k)

c∆s k∇s j

j c∆s k∇s

=

Q(Yj = c) I(xij = k)

j c∆s

k∇s

= Q(Yj∆s)I(xij∇s).
j

For example, when ∆ =< and ∇ =≥, the above equation becomes

φij(c, k) = Q(Yj < s)I(xij ≥ s).

j c<s k≥s

j

This counts the items of which each belongs to a class less than s but worker i assigned a label larger or equal to s.
In general, for a comparison between an observed label and a reference label, there are two possible outcomes: the observed label is larger or equal to the reference label; or the observed label is smaller than the reference label. These are also the two possible outcomes for a comparison

11

(a) Partitioning with s = 1

(0, 0) (1, 0) (2, 0) (3, 0)

(0, 1) (1, 1) (2, 1) (3, 1)

(0, 2) (1, 2) (2, 2) (3, 2)

(0, 3) (1, 3) (2, 3) (3, 3)

(b) Partitioning with s = 2

(0, 0) (1, 0) (2, 0) (3, 0)

(0, 1) (1, 1) (2, 1) (3, 1)

(0, 2) (1, 2) (2, 2) (3, 2)

(0, 3) (1, 3) (2, 3) (3, 3)

(c) Partitioning with s = 3

(0, 0) (1, 0) (2, 0) (3, 0)

(0, 1) (1, 1) (2, 1) (3, 1)

(0, 2) (1, 2) (2, 2) (3, 2)

(0, 3) (1, 3) (2, 3) (3, 3)

Table 1: Partitioning the Cartesian product of the ordinal label set {0, 1, 2, 3}. With respect to each possible reference label, each table is partitioned into four disjoint regions.

between a true label and a reference label. Putting these together, we have four possible outcomes in total. The constraints in Equation (18a) enforce expected counts of all the four kinds of outcomes in the worker dimension to match their empirical counterparts. Symmetrically, the constraints in Equation (18b) enforce expected counts of all the four kinds of outcomes in the item dimension to match their empirical counterparts.
The Lagrangian of the maximization problem in (17) can be written as

L = H(X|Y ) + Lσ + Lτ + Lλ,

with

Lσ =

σi∆s ,∇

i,s ∆,∇

c∆s k∇s j

Lτ =

τj∆s,∇

j,s ∆,∇

c∆s k∇s i

φij(c, k) − φij(c, k) , φij(c, k) − φij(c, k) ,

Lλ = λijc
i,j,c

P (Xij = k|Yj = c) − 1 ,
k

where σi∆s,∇, τj∆s,∇ and λijc are the introduced Lagrange multipliers. By a procedure similar to that in Section 2, we obtain a probabilistic ordinal labeling model

1

P (Xij = k|Yj = c) = Zij exp[σi(c, k) + τj(c, k)],

(19)

12

where

σi(c, k) =

σ

∆ is

,

∇

I(c∆

s

,

k

∇

s

),

s≥1 ∆,∇

τj(c, k) =

τ

∆ js

,

∇

I(c∆

s

,

k

∇

s

).

s≥1 ∆,∇

(20a) (20b)

The ordinal labeling model in Equation (19) is actually the same as the multiclass labeling model in Equation (6) except the worker and item confusion matrices in Equation (19) are now subtly structured through Equation (20). It is because of the structure that the ordinal labeling model has fewer parameters than the multiclass labeling model when there are more than two classes. In the case in which there are only two classes, the ordinal labeling model and the multiclass labeling model coincide as one would expect.
The regularized minimax conditional entropy for ordinal labels can be written as

min max H(X|Y ) − H(Y ) − 1 Ω(ξ) − 1 Ψ(ζ) (21)

QP

α

β

subject to the relaxed worker and item constraints

c∆s k∇s j c∆s k∇s i

φij (c, k) − φij (c, k) = ξi∆s,∇, ∀i, s, φij (c, k) − φij (c, k) = ζj∆s,∇, ∀j, s,

(22a) (22b)

for all ∆, ∇ ∈ {≥, <}, and the probability constraints in Equation (3). When we choose

Ω(ξ) = 1 2

2
ξi∆s,∇ ,

i,s ∆,∇

Ψ(ζ) = 1 2

2
ζj∆s,∇ ,

j,s ∆,∇

the dual problem becomes

max
σ,τ,Q

Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y ) − αΩ∗(σ) − βΨ∗(τ ),

j,c

i

where

Ω∗(σ) = 1 2 i,s ∆,∇
Ψ∗(τ ) = 1 2 j,s ∆,∇

2
σi∆s,∇ ,
2
τj∆s,∇ .

13

5.3 Ordinal Objective Measurement Principle
In this section, we adapt the objective measurement principle developed in Section 4 to ordinal labels.
Assume that worker i has labeled items j and j′ in class c. For any class k, we deﬁne two events. The ﬁrst event is

E = I(Xij = k) + I(Xij′ = k) = 1, I(Xij = k + 1) + I(Xij′ = k + 1) = 1 ,

and the other event is

A = Xij = k, Xij′ = k + 1 .

Note that A ⊂ E. Now we formulate the objective measurement principle as follows: P (A|E) is independent of worker i. Assume that the labels of the items are independent. Then, P (A|E) can be written as

P (Xij = k|Yj = c)P (Xij′ = k + 1|Yj′ = c)

.

P (Xij = k|Yj = c)P (Xij′ = k + 1|Yj′ = c) + P (Xij = k + 1|Yj = c)P (Xij′ = k|Yj′ = c)

Hence, P (A|E) is independent of worker i if and only if

P (Xij = k|Yj = c)P (Xij′ = k + 1|Yj′ = c) P (Xij = k + 1|Yj = c)P (Xij′ = k|Yj′ = c)
is independent of worker i. In other words, given another arbitrary worker i′, we should have
P (Xij = k|Yj = c)P (Xij′ = k + 1|Yj′ = c) = P (Xi′j = k|Yj = c)P (Xi′j′ = k + 1|Yj′ = c) . P (Xij = k + 1|Yj = c)P (Xij′ = k|Yj′ = c) P (Xi′j = k + 1|Yj = c)P (Xi′j′ = k|Yj′ = c)
To introduce adjacency confusability, we further assume that, for any two classes c, c′ ≥ k + 1 (or c, c′ < k + 1),
P (Xij = k|Yj = c)P (Xij′ = k + 1|Yj′ = c) P (Xij = k|Yj = c′)P (Xij′ = k + 1|Yj′ = c′) P (Xij = k + 1|Yj = c)P (Xij′ = k|Yj′ = c) = P (Xij = k + 1|Yj = c′)P (Xij′ = k|Yj′ = c′) .
Then, by a procedure similar to that in Section 4, we reach the probabilistic ordinal labeling model described by Equation (19) and (20).

6 Implementation
In this section, we present a simple while eﬃcient coordinate ascent method to solve the minimax program through its dual form and also a practical procedure for model selection.
6.1 Coordinate Ascent
The dual problem of regularized minimax conditional entropy for either multiclass or ordinal labels is nonconvex. A stationary point can be obtained via coordinate ascent (Algorithm 1), which is essentially Expectation-Maximization (EM) (Dempster et al., 1977; Neal and Hinton, 1998). We ﬁrst initialize the label estimate via aggregating votes in Equation (23). Then, in each iteration step, given the current estimate of the labels, update the estimate of the confusion matrices of the
14

Algorithm 1 Regularized Minimax Conditional Entropy for Crowdsourcing input: {xij}, α, β

initialize: repeat:

Q(Yj = c) ∝

I(xij = c)
i

{σ, τ } = arg max Q(Yj = c) log P (Xij = xij|Yj = c) − αΩ∗(σ) − βΨ∗(τ )
σ,τ i,j,c

Q(Yj = c) ∝ output: Q

P (Xij = xij |Yj = c)
i

(23) (24a) (24b)

workers and items by solving the optimization problem in (24a); and, given the current estimate of the confusion matrices of worker and item, update the estimate of the labels through the closedform formula in (24b), which is identical to applying the Bayes’ rule with a uniform prior. The optimization problem in (24a) is strongly convex and smooth. Many algorithms can be applied here (Nesterov, 2004). In our experiments, we simply use gradient ascent. Denote by F the objective function in (24a). For multiclass labels, the gradients are computed as

∂F ∂σi(c, k) = Q(Yj = c) [I(xij = k) − P (Xij = k|Yj = c)] − ασi(c, k),
j
∂F ∂τj(c, k) = Q(Yj = c) [I(xij = k) − P (Xij = k|Yj = c)] − βτj(c, k).
i

For ordinal labels, the gradients are computed as

∂F = I(c∆s, k∇s) Q(Yj = c) [I(xij = k) − P (Xij = k|Yj = c)] − ασ∆,∇,

∂σi∆s,∇ c,k

j

is

∂F = I(c∆s, k∇s) Q(Yj = c) [I(xij = k) − P (Xij = k|Yj = c)] − βτ ∆,∇.

∂τj∆s,∇ c,k

i

js

It is worth pointing out that it is unnecessary to obtain the exact optimum at this intermediate step. We have observed that in practice, several gradient ascent steps here suﬃce for reaching a ﬁnal good solution.

6.2 Model Selection
The regularization parameters α and β can be chosen as follows. If the true labels of a subset of items are known—such subsets are usually referred to as validation sets—we may choose the regularization parameters such that those known true labels can be best predicted. Otherwise, we suggest to choose the regularization parameters via k-fold likelihood-based cross-validation. Speciﬁcally, we ﬁrst randomly partition the crowd labels into k equal-size subsets, and deﬁne a ﬁnite set of possible choices for the regularization parameters. Then, for each possible choice of the regularization parameters,

15

1. Leave out one subset and use the remaining k − 1 subsets to estimate the confusion matrices of the workers and items;
2. Plug the estimate into the probabilistic labeling model to compute the likelihood of the leftout subset;
3. Repeat the above two steps till each subset is left out once and only once;
4. Average the likelihoods that we have computed.
After going through all the possible choices for the regularization parameters, we choose the one which results in the largest average likelihood to run our algorithm over the full dataset. The cross-validation parameter k is typically set to 5 or 10.
To simplify the model selection process, we suggest to choose
α = γ × (number of classes)2, β = number of labels per worker × α. (25)
number of labels per item
In our experiments, we select γ from {2−2, 2−1, 20, 21, 22}. In our limited empirical studies, larger candidate sets for γ did not give more gains. Two empirical observations motivate us to consider using the square of the number of classes in Equation (25). First, the square of the number of classes has the same magnitude as the number of parameters in a confusion matrix. Second, the label noise dramatically increases when the number of classes increases, requiring a super linearly scaled regularization.

7 Related Work

In this section, we review some existing work that are closely related to our work. Dawid-Skene Model. Let K denote the number of classes. Dawid and Skene (1979) propose
a generative model in which the ability of worker i is characterized by a K × K probabilistic confusion matrix [pi(c, k)] in which the diagonal element pi(c, c) represents the probability that worker i correctly labels an arbitrary item in class c, and the oﬀ-diagonal element pi(c, k) represents the probability that worker i mislabels an arbitrary item in class c as class k. Our probabilistic labeling model in Equation (6) is reduced to the Dawid-Skene model when the item diﬃcult terms τj(c, k) in our model disappear since we can then reparameterize

pi(c, k) =

exp[σi(c, k)] . k′ exp[σi(c, k′)]

In this sense, our model generalizes the Dawid-Skene model to incorporate item diﬃculty. To

jointly estimate the workers’ abilities and the true labels in the Dawid-Skene model, in general, the

marginal likelihood is maximized using the EM algorithm.

For binary labeling task, the probabilistic confusion matrix in the Dawid-Skene model can be

written as

pi 1 − pi , 1 − qi qi

16

where pi is the accuracy of worker i in the ﬁrst class, and qi the accuracy in the second class.

Usually, this special case of the Dawid-Skene model is also referred to as the two-coin model

(Raykar et al., 2010; Liu et al., 2012; Chen et al., 2013). One may simplify the two-coin model by

assuming pi = qi (Ghosh et al., 2011; Karger et al., 2014; Dalvi et al., 2013). This simpliﬁcation is

accordingly referred to as the one-coin model.

Karger et al. (2014) propose an inference algorithm under the one-coin model, and show that

their algorithm achieves the minimax rate when the accuracy of every worker is bounded away

from 0 and 1, that is, with some ﬁxed number ǫ > 0, ǫ < pi < 1 − ǫ. Liu et al. (2012) show that

the algorithm proposed by Karger et al. (2014) is essentially a belief propagation update with the

Haldane prior which assumes that each worker is either a hammer (pi = 1) or adversary (pi = 0)

with equal probability.

Gao and Zhou (2013) show that under the one-coin model, the global optimum of maximum

likelihood achieves the minimax rate. A projected EM algorithm is suggested and shown to achieve

nearly the same rate as that of global optimum. Zhang et al. (2014) show that the EM algorithm

for the general Dawid-Skene model can achieve the minimax rate up to a logarithmic factor when

it is initialized by spectral methods (Anandkumar et al., 2012) and the accuracy of every worker is

bounded away from 0 and 1.

Raykar et al. (2010) extend the Dawid-Skene model by imposing a beta prior over the worker

confusion matrices. Moreover, they jointly learn the classiﬁer and the true labels by assuming that

the true labels are generated by a logistic model. Liu et al. (2012) develop full Bayesian inference

via variational methods including belief propagation and mean ﬁeld.

Rasch model (Rasch, 1961, 1968). In educational tests, the Rasch model illustrates the

response of each examinee of a given ability to each item in a test. In the model, the probability of

a correct response is modeled as a logistic function of the diﬀerence between the person and item

parameter which are locations on a continuous latent trait. Person parameters represent the ability

of examinees while item parameters represent the diﬃculty of items.

Let Xij ∈ {0, 1} be a dichotomous random variable where Xij = 1 denotes a correct response

and Xij = 0 an incorrect response to a given assessment item. Mathematically, the Rasch model is

given by

P (Xij = 1) = exp(βi − δj) , 1 + exp(βi − δj)

where βi is the ability of examinee i and δi the diﬃculty of item j. The larger an examinee’s ability relative to the diﬃculty of an item, the larger the probability of a correct response on that item. When the examinee’s ability on the latent trait is equal to the diﬃculty of the item, the probability of a correct response is 1/2.
The Rasch model is a special item response theory (IRT) model (Lord and Novick, 1968). However, unlike other IRT models, the Rasch model satisﬁes the objective measurement principle pioneered by Rasch. Our work generalizes both the Rasch model and the objective measurement principle to multiclass labeling tasks. In addition, unlike the Rasch model, in our scenario, the true answers are unknown and have to be estimated.
Polytomous Rasch model. The Rasch model has been adapted to the applications in which responses to items are scored with successive integers such as rating scales. Let Xij = {0, 1, · · · , m}. Andrich (1978) suggests

P (Xij = k) =

exp ks=0[βi − (δj − τs)] ,

m k′=0

exp

ks=′ 0[βi − (δj − τs)]

17

where βi is the location of person i on a latent continuum, δj the diﬃculty of item j on the same continuum, and τs the s-th threshold location of the rating scale which is in common to all the items. This model is usually referred to as the Rasch rating scale model. Later, the Rasch partial credit model developed by Master (1982) generalizes the Rasch rating scale model into

P (Xij = k) =

exp ks=0(βi − τjs) ,

m k′=0

exp

ks=′ 0(βi − τjs)

where τjs is the s-th threshold location of item i on a latent continuum. When τjs can be decom-

posed as τjs = δj − τs, these two models coincide. Uebersax and Grove (1993) and Mineiro (2011b)

apply the polytomous form of the Rasch model with minor changes to aggregate ordinal labels from

a crowd.

Probabilistic matrix factorization. Let Xij be the label given by worker i to item j. Let Yj

be the true label of item j. Whitehill et al. (2009) model the labeling process by revising the Rasch

model into

1

P (Xij = Yj) =

βi ,

1 + exp − δj

and refer to their model as GLAD (Generative Model of Labels, Abilities, and Diﬃculties). It is easy to see that GLAD violates the principle of invariant comparison. By using the per-worker confusion matrix in the Dawid-Skene model, Mineiro (2011a) generalizes GLAD to multiclass labeling as

P (Xij = k|Yj = c) ∝ exp βi(c, k) . δj

(Welinder et al., 2010) parameterize workers and items with vectors and suggest

P (Xij = Yi) = Φ(wi⊤zj − bj ),

where Φ(·) is the cumulative standardized normal distribution, wi ∈ Rd the unobserved worker parameter, and zj ∈ Rd, bj ∈ R the unobserved item parameter. GLAD can be roughly thought of as a special case of this model with the dimension d = 1 and bj = 0.
Other related work. For other probabilistic modelling of crowdsourcing, we refer the readers to (Bachrach et al., 2012; Tian and Zhu, 2012; Dai et al., 2013; Venanzi et al., 2014). For online decision making in crowdsourcing, we refer the readers to (Sheng et al., 2008; Abraham et al., 2013; Chen et al., 2013; Singla and Krause, 2013; Ho et al., 2014; Anari et al., 2014). Regularized maximum entropy is studied in (Chen and Rosenfeld, 2000; Lebanon and Laﬀerty, 2001; Kazama and Tsujii, 2003; Altun and Smola, 2006; Dudik et al., 2007). Zhu et al. (1997) propose a minimax entropy method for feature binding and selection, and apply it to texture modeling and obtain a new class of Markov random ﬁeld models. Shah and Zhou (2014) propose a multiplicative payment mechanism to incentivize crowdsourcing workers to answer a question when they are sure and skip when they are not sure. They obtain extremely high quality crowdsourced data by using their mechanism.

8 Experiments
In this section, we report empirical results of our method and some existing methods discussed in Section 7. Two error metrics are considered. One is the classiﬁcation error rate for binary or multiclass data, and the other is the mean square error for ordinal data.

18

8.1 Datasets
All datasets that we use are from real crowdsourcing tasks and publicly available.2 The details are as follows:
• Bluebirds (Welinder et al., 2010). This dataset contains a set of 108 images which are labeled as indigo bunting or blue grosbeak by 39 crowdsourcing workers. Every worker labeled every image. The average error rate of the workers is 36.44%, compared to the error rate of random guessing at 50%.
• Price (Liu et al., 2013). This dataset consists of 80 household items collected from stores such as Amazon and Costco. The prices of the products are estimated by 155 undergraduate students from UC Irvine. Seven price bins are created in this data collection: $0−$50, $51−$100, $101−$250, $251−$500, $501−$1000, $1001−$2000, and $2001−$5000. For each product, a student has to to decide which bin its price falls in. The average error rate of the students is 69.47%, compared to the error rate of random guessing at 85.71%. It may not be surprising that this dataset is systematically biased: all the students tend to underestimate the prices of the products.
• RTE (Snow et al., 2008). For each crowdsourced question, the worker is presented with two sentences and asked to check if the second hypothesis sentence can be inferred from the ﬁrst. This dataset contains 800 sentence pairs and 164 workers. Each sentence pair has 10 annotations. The average error rate of the workers is 15.87%, compared to the error rate of random guessing at 50%.
• Temp (Snow et al., 2008). For each crowdsourced question, the worker is presented with a pair of verb events and asked to check if the event described by the ﬁrst verb occurs before or after the second. This dataset contains 462 event pairs and 76 workers. Each event pair has 10 annotations. The average error rate of the workers is 16.30%, compared to the error rate of random guessing at 50%.
• Age (Han et al., 2014). Amazon mechanical turkers are asked to estimate the age of a person in a face image. This dataset contains 1002 images and 165 workers. Each image has 10 age estimates. Those estimates are integers not more than 100. We put them into 7 bins: [1, 9], [10, 19], [20, 29], [30, 39], [40, 49], [50, 59], [60, 100]. With respect to this partition, the average error rate of the workers is 44.64%, compared to the error rate of random guessing at 85.71%.
• Web search (Zhou et al., 2012). This dataset contains 2665 query-URL pairs and 177 workers. Give a query-URL pair, a worker is required to provide a rating to measure how the URL is relevant to the query. The rating scale is 5-level: perfect, excellent, good, fair, or bad. On average, each pair was labeled by around 6 diﬀerent workers, and each worker labeled around 90 pairs. More than 10 workers labeled only one query-URL pair. The ground truth labels used for evaluation are obtained via a consensus among a group of 9 search experts. The average error rate of the workers is 62.95%, compared to the error rate of random guessing at 80%.
2Some of the datasets can be found at http://research.microsoft.com/en-us/projects/crowd/
19

Bluebirds Price RTE Temp Age
Web search Web spam

# classes 2 7 2 2 7 5 2

# items 108 80 800 462 1002 2665 149

# workers 39 155 164 76 165 177 18

# worker labels 4212 12400 8000 4620 10020 15567 1901

Table 2: Summary of the real crowdsourcing datasets used in our experiments.

Bluebirds Price RTE Temp Age
Web search Web spam

MV 24.07 67.50 10.31 6.39 34.88 26.93 19.80

DS-EM 10.19 65.00 7.25 5.84 39.62 16.92 13.42

DS-MF 10.19 67.50 6.63 5.84 36.33 18.24 12.75

GLAD 12.04 68.75 7.00 5.63 35.73 19.30 18.12

MMCE(M) 8.33 67.50 7.50 5.63 31.14 11.12 12.75

Table 3: Error rates (in %) of various methods on real datasets.

• Web spam. This dataset is provided by Microsoft web spam team. It contains 149 web pages and 18 workers. The workers are required to identify which web pages are spam. In average, each web page is labeled by around 13 workers. The ground truth labels used for evaluation are provided by web spam experts. The average error rate of the workers is 16.30%, compared to the error rate of random guessing at 50%.
Table 2 shows a summary of these datasets.
8.2 Methods
We evaluate the following methods in our experiments:
• Majority voting (MV). It is perhaps the simplest baseline.
• Dawid-Skene model + EM (DS-EM). Under the generative model by (Dawid and Skene, 1979), this method jointly estimates workers’ parameters and true labels by maximizing the likelihood of observed labels with the EM algorithm.
• Dawid-Skene model + mean ﬁeld (DS-MF). This method performs variational Bayesian inference using the mean ﬁeld (MF) algorithm (Liu et al., 2012). It assumes a Dirichlet prior parameterized by a vector αk on the k-th row of the worker confusion matrix in the Dawid-Skene model with αk,k = c1, and αk,l = c2 for all l = k. The hyperparameters {c1, c2} are selected by maximizing the marginal likelihood calculated by MF, and searched in a 10 × 10 grid deﬁned by c1 = c2 × {100, 100.1, 100.2, · · · , 101} and c2 = {10−1, 10−0.8, · · · , 100, · · · , 100.8, 101}.

20

Price Age Web search

MV 1.605 0.730 0.930

DS-EM 1.517 0.852 0.539

DS-MF 1.487 0.739 0.559

LTA 1.504 0.696 0.481

MMCE(M) 1.643 0.605 0.419

MMCE(O) 1.466 0.794 0.384

Table 4: Mean square errors of various methods on ordinal datasets.

Probability Bin # items Error rate
Mean square error

(0, 0.5) 173 0.416 0.832

(0.5, 0.6) 291 0.381 0.423

(0.6, 0.7) 292 0.199 0.250

(0.7, 0.8) 313 0.080 0.118

(0.8, 0.9) 406 0.020 0.035

(0.9, 1) 1178 0.001 0.001

Table 5: Positive correlation between probabilistic labels and errors. The results are from the regularized ordinal minimax conditional entropy method on the web dataset.

• GLAD. We use the multiclass version of GLAD proposed by (Mineiro, 2011a) and also his open source implementation.
• Latent trait analysis (LTA). It is a variant of the polytomous Rasch model proposed by (Mineiro, 2011b) with an open source implementation.
• Regularized minimax conditional entropy for multiclass labels (MMCE(M)). It is implemented with the Euclidian norm based regularization.
• Regularized minimax conditional entropy for ordinal labels (MMCE(O)). It is implemented with the Euclidian norm based regularization.
The regularization parameters in MMCE are chose through the cross-validation procedure described in Section 6.2.3
8.3 Results
Table 3 shows the error rates of various methods on real crowdsourcing datasets. Our multiclass minimax conditional entropy method outperforms compared methods on most datasets. Table 4 shows the mean square errors of various methods on three ordinal datasets. Our ordinal minimax conditional entropy method performs best on the price and web search datasets but performs poorly on the age dataset. Table 5 shows the correlation between probabilistic labels and errors for our ordinal minimax conditional entropy method on the web dataset. From the results, the labels estimated with larger probabilities are more likely to be correct. We observed similar behavior for our multiclass minimax conditional entropy method. We also evaluated our method with the regularization in Equation (16) and observed that this variant somewhat hurts performance on most datasets.

9 Conclusion
We have developed a minimax conditional entropy principle for aggregating noisy labels from crowdsourcing workers. Our formulation involves two probabilistic distributions. One is the distribution
3Our code are available at http://research.microsoft.com/en-us/projects/crowd/

21

of the true labels of the items, and the other is the distribution under which the workers generate their labels for the items. Both the distributions are unknown. We jointly infer them by ﬁrst maximizing the entropy of the observed labels of the workers conditioned on the true labels of the items over the distribution of generating workers’ labels, and then minimizing the maximum entropy over the distribution of the true labels of the items. Empirical results on real crowdsourcing datasets validate our approach.
We have considered aggregating multiclass and ordinal labels via minimax conditional entropy. The framework is general and should be extensible to many other labeling tasks in which the labels are structured in diﬀerent ways, such as protein folding (Khatib et al., 2011), machine translation (Zaidan and Callison-Burch, 2011), hierarchical classiﬁcation (Koller and Sahami, 1997), and speech captioning (Murphy et al., 2013). To achieve the extension, the constraints for workers and items need to be customized speciﬁc to each domain, and this probably results in diﬀerently structured confusion matrices.
Acknowledgements
We would like to thank Sumit Basu and Yi Mao for their early contribution to this work, Daniel Hsu, Xi Chen, Chris Burges for helpful discussions, and Gabriella Kazai for providing the web search dataset.
22

A Dual Form of Minimax Conditional Entropy

To to derive the dual of minimax conditional entropy, we substitute the probabilistic model in Equation (6) into the Lagrangian (5) and obtain

L = − Q(Yj = c) P (Xij = k|Yj = c) log

i,j,c

k

1 Zij exp[σi(c, k) + τj(c, k)]

+ σi(c, k) Q(Yj = c) P (Xij = k|Yj = c) − I(xij = k)

i,c,k

j

+ τj(c, k) Q(Yj = c) P (Xij = k|Yj = c) − I(xij = k)

j,c,k

i

+ λijc
i,j,c

P (Xij = k|Yj = c) − 1
k

= − Q(Yj = c) P (Xij = k|Yj = c)[σi(c, k) + τj(c, k)] + log Zij

i,j,c

k

i,j

+ σi(c, k) Q(Yj = c) P (Xij = k|Yj = c) − I(xij = k)

i,c,k

j

+ τj(c, k) Q(Yj = c) P (Xij = k|Yj = c) − I(xij = k)

j,c,k

i

= − Q(Yj = c)
i,j,c

I(xij = k)[σi(c, k) + τj(c, k)] − log Zij
k

= − Q(Yj = c) log P (Xij = xij|Yj = c).
i,j,c

B Proof of Theorem 2.1

Let us ﬁrst check X,Y Q(X, Y ) log Q(X, Y ). By deﬁnition,

Hence, we have

Q(X) log Q(X) = 0, Q(X|Y ) = Q(X).
X

Q(X, Y ) log Q(X, Y ) = [Q(X|Y )Q(Y )] log[Q(X|Y )Q(Y )]

X,Y

X,Y

= [Q(X)Q(Y )] log[Q(X)Q(Y )]
X,Y

= Q(X) log Q(X) + Q(Y ) log Q(Y )

X

Y

= Q(Y ) log Q(Y ).
Y

23

Next we check X,Y Q(X, Y ) log P (X, Y ). Write

Q(X, Y ) log P (X, Y ) = Q(X, Y ) log P (X|Y ) + Q(X, Y )P (Y ).

X,Y

X,Y

X,Y

Since P is a uniform distribution over Y, P (Y ) is a constant. Thus,

Q(X, Y ) log P (Y ) = log P (Y ) Q(X, Y ) = log P (Y ),

X,Y

X,Y

which is still a constant. By Equation (6), we have

Q(X, Y ) log P (X|Y ) =

Q(Xij = k, Yj = c) log P (Xij = k|Yj = c)

X,Y

i,j,c,k

= Q(Xij = k, Yj = c) log
i,j,c,k

1 Zij exp[σi(c, k) + τj(c, k)]

= Q(Xij = k, Yj = c) [σi(c, k) + τj(c, k) − log Zij] .
i,j,c,k

By Equation (2a), we have

Q(Xij = k, Yj = c)σi(c, k) = σi(c, k) I(Xij = k)Q(Yj = c)

i,j,c,k

i,c,k

j

= σi(c, k) P (Xij = k|Yj = c)Q(Yj = c).

i,c,k

j

Similarly, by Equation (2b),

Q(Xij = k, Yj = c)τj(c, k) = τj(c, k) P (Xij = k|Yj = c)Q(Yj = c).

i,j,c,k

j,c,k

i

In addition, since Zij does not depend on k,

Q(Xij = k, Yj = c) log Zij =

i,j,c,k

i,j

=
i,j

=
i,j

Putting all the pieces together, we have

log Zij Q(Xij = k, Yj = c)

c

k

Q(Yj = c) log Zij
c

Q(Yj = c) P (Xij = k|Yj = c) log Zij.

c

k

DKL(Q

P ) = − Q(Yj = c) P (Xij = k|Yj = c)[σi(k, c) + τj(c, k) − log Zij]

j,c

i,k

+ Q(Y ) log Q(Y ) − log P (Y )
Y

= − Q(Yj = c) P (Xij = k|Yj = c) log P (Xij = k|Yj = c)

j,c

i,k

+ Q(Y ) log Q(Y ) − log P (Y ).
Y

24

Note that, when the true labels are deterministic,

Q(Y ) log Q(Y ) = 0.
Y
So,

DKL(Q P ) = − Q(Yj = c) P (Xij = k|Yj = c) log P (Xij = k|Yj = c) − log P (Y ).

j,c

i,k

This concludes the proof.

C Dual Form of Regularized Minimax Conditional Entropy

We derive the dual problem of regularized maximum conditional entropy with the sum-to-zero constraints in Equation (15). The dual derivation without the additional constraints can be obtained in a similar procedure. Let us write the Lagrangian as

1

1

L = H(X|Y ) − H(Y ) − α Ω(ξ) − β Ψ(ζ) + Lσ + Lτ + Lλ + Lµ,

(26)

in which

Lσ = σi(c, k) ξi(c, k) −

φij(c, k) − φij(c, k) ,

i,c,k

j

Lτ = τj(c, k) ζj(c, k) −

φij(c, k) − φij(c, k) ,

j,c,k

i

Lλ = λijc

P (Xij = k|Yj = c) − 1 ,

i,j,c

k,c

Lµ = µi ξi(c, c).

i

c

By the KKT conditions, maximizing L with respect to P results in

∂L ∂P (Xij = k|Yj = c) = − log P (Xij = k|Yj = c) − 1 + λijc + σi(c, k) + τj(c, k) = 0.

As showed in Section 2, this leads to the probabilistic model in Equation (3a). Similarly, maximizing L with respect to ξ results in

∂L

1

∂ξi(c, k) = σi(c, k) − α ξi(c, k) = 0, ∀c, k = c,

∂L

1

∂ξi(c, k) = σi(c, c) − α ξi(c, c) + µi = 0, ∀c.

So we have

ξi(c, k) = ασi(c, k), ∀c, k = c,

(27)

ξi(c, c) = α[σi(c, c) + µi], ∀c.

(28)

25

Moreover, maximizing L with respect to ζ results in

∂L

1

∂ζi(c, k) = τi(c, k) − β ζi(c, k) = 0, ∀c, k.

Hence,

ζi(c, k) = βτi(c, k), ∀c, k.

(29)

Substituting (6), (27), (28), and (29) into the Lagrangian (26), we have

L = − Q(Yj = c) log
i,j,c

1 Zij exp[σi(c, xij ) + τj(c, xij )]

− H(Y )

−α 2 i,c

[σi(c, k)]2 + [σi(c, c) + µi]2
k=c

− β [τj(c, k)]2 2 i,c,k

+α
i,c

[σi(c, k)]2 + σi(c, c)[σi(c, c) + µi] + β [τj(c, k)]2

k=c

i,c,k

+ α µi(c, c)[σi(c, c) + µi]

i,c

= − Q(Yj = c) log
i,j,c

1 Zij exp[σi(c, k) + τj(c, k)]

− H(Y )

+α 2 i,c

[σi(c, k)]2 + [σi(c, c) + µi]2
k=c

+ β [τj(c, k)]2. 2 i,c,k

By minimizing the Lagrangian over µi, we obtain

µi = −σi(c, c).

So, the dual problem can be expressed as

1

min −
Q,σ,τ

Q(Yj = c) log Zij exp[σi(c, xij) + τj(c, xij)] − H(Y )

i,j,c

+α 2 i,c

2
[σi(c, k)]2 + σi(c, c) − σi(c, c)
k=c

+ β [τj(c, k)]2. 2 i,c,k

Let us replace σi(c, k) with σi(c, k)+νi. It is easy to verify that this dual problem can be equivalently written as

min
Q,σ,τ,ν

− Q(Yj = c) log
i,j,c

1 Zij exp[σi(c, xij ) + τj(c, xij )]

+α 2 i,c

2
[σi(c, k) + νi]2 + σi(c, c) − σi(c, c)
k=c

− H(Y ) + β [τj(c, k)]2.
2 i,c,k

26

Minimizing the objective function over ν leads to

1

min −
Q,σ,τ

Q(Yj = c) log Zij exp[σi(c, xij ) + τj(c, xij)] − H(Y )

i,j,c

+α 2 i,c

2

2

σi(c, k) − σi(c, k) + σi(c, c) − σi(c, c)

k=c

+ β [τj(c, k)]2. 2 i,c,k

D Coordinate Algorithm

To solve the dual problem

max
σ,τ,Q

Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y ) − αΩ∗(σ) − βΨ∗(τ )

j,c

i

subject to the probability constraints

Q(Yj = c) = 1, ∀j, Q(Yj = c) ≥ 0, ∀j, k,
c

we ﬁrst split the variables into two groups and then alternatively update them. One group contains the parameters of workers and items in P (Xij = xij|Yj = c), that is, {σi(c, k), τj (c, k), ∀i, j, c, k} and the other groups contains the unknown true labels {Q(Yj = k), ∀j, k}. When we update the variables in the ﬁrst group, the variables in the second group take their current values. Then, the optimization problem becomes

max
σ,τ

Q(Yj = c) log P (Xij = xij|Yj = c) − αΩ∗(σ) − βΨ∗(τ ).

j,c

i

Instead, when we update the variables in the second group, the variables in the ﬁrst group take their current values. We thus have the optimization problem

max
Q

Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y )

j,c

i

subject to the above probability constraints. This constrained optimization problem can be solved with the Lagrangian dual

L = Q(Yj = c) log P (Xij = xij|Yj = c) + H(Y ) −

j,c

i

λj

Q(Yj = c) − 1 ,

c

where λj’s are the Lagrangian multipliers. By the KKT conditions,

∂L ∂Q(Yj = c) = log P (Xij = xij|Yj = c) − log Q(Yj = c) + 1 − λj = 0.
i

This implies

Q(Yj = c) ∝ P (Xij = xij |Yj = c).
i

27

References
I. Abraham, O. Alonso, V. Kandylas, and A. Slivkins. Adaptive crowdsourcing algorithms for the bandit survey problem. arXiv:1302.3268, 2013.
Y. Altun and A. Smola. Unifying divergence minimization and statistical inference via convex duality. In Proceedings of the 19th Annual Conference on Learning Theory, 2006.
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. arXiv:1210.7559, 2012.
N. Anari, G. Goel, and A. Nikzad. Mechanism design for crowdsourcing: An optimal 1 − 1/e competitive budget-feasible mechanism for large markets. arXiv:1405.2452, 2014.
D. Andrich. A rating formulation for ordered response categories. Psychometrika, 43:561–73, 1978.
Y. Bachrach, T. Minka, J. Guiver, and T. Graepel. How to grade a test without knowing the answers — A Bayesian graphical model for adaptive crowdsourcing and aptitude testing. In Proceedings of the 29th International Conference on Machine Learning, pages 1183–1190, 2012.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
S. F. Chen and R. Rosenfeld. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8(1):27–50, 2000.
X. Chen, Q. Lin, and D. Zhou. Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing. In Proceedings of the 30th International Conference on Machine Learning, 2013.
P. Dai, C. H. Lin, Mausam, and D. S. Weld. POMDP-based control of workﬂows for crowdsourcing. Artiﬁcial Intelligence, 202:52–85, 2013.
N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of the 22nd International Conference on World Wide Web, pages 1220–1229, 2013.
A. P. Dawid and A. M. Skene. Maximum likeihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society, 28(1):20–28, 1979.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38, 1977.
M. Dudik, S. J. Phillips, and R. E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. Journal of Machine Learning Research, 8:1217–1260, 2007.
C. Gao and D. Zhou. Minimax optimal convergence rates for estimating ground truth from crowdsourced labels. arXiv:1310.5764, 2013.
A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators? Crowdsourcing abuse detection in user-generated content. In Proceedings of the 12th ACM conference on Electronic Commerce, pages 167–176, 2011.
28

H. Han, C. Otto, X. Liu, and A. K. Jain. Demographic estimation from face images: Human vs. machine performance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014. To appear.

C.-J. Ho, A. Slivkins, and J. Wortman. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In Proceedings of the 15th ACM conference on Economics and Computation, pages 359–376, 2014.

D. R. Karger, S. Oh, and D. Shah. Budget-optimal task allocation for reliable crowdsourcing systems. Operations Research, 62(1):1–24, 2014.

J. Kazama and J. Tsujii. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 137–144, 2003.

F. Khatib, S. Cooper, M. D. Tyka, K. Xu, I. Makedon, Z. Popovi´c, D. Baker, and F. Players. Algorithm discovery by protein folding game players. Proceedings of the National Academy of Sciences, 108(47):18949–18953, 2011.

D. Koller and M. Sahami. Hierarchically classifying docuemnts using very few words. In Proc. 14th Intl. Conf. Machine Learning, pages 171–178, 1997.

G. Lebanon and J. Laﬀerty. Boosting and maximum likelihood for exponential models. In Advances in Neural Information Processing Systems 14, pages 447–454, 2001.

Q. Liu, J. Peng, and A. Ihler. Variational inference for crowdsourcing. In Advances in Neural Information Processing Systems 25, pages 701–709, 2012.

Q. Liu, M. Steyvers, and A. Ihler. Scoring workers in crowdsourcing: How many control questions are enough? In Advances in Neural Information Processing Systems 26, pages 1914–1922, 2013.

F. M. Lord and M. R. Novick. Statistical theories of mental test scores. Reading, MA: AddisonWesley, 1968.

G. N. Master. A Rasch model for partial credit scoring. Psychometrika, 47:149–174, 1982.

P. Mineiro. 2011a.

http://www.machinedlearnings.com/2011/08/low-rank-confusion-modeling-of.html,

P. Mineiro. http://www.machinedlearnings.com/2011/02/ordered-values-and-mechanical-turkpart.html, 2011b.

M. Murphy, C. D. Miller, W. S. Lasecki, and J. P. Bigham. Adaptive time windows for realtime crowd captioning. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, pages 13–18, 2013.

R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer Academic, Dordrecht, MA, 1998.

29

Yu. Nesterov. Introductory lectures on convex optimization: A basic course. Kluwer Academic, 2004.
G. Rasch. On general laws and the meaning of measurement in psychology. In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability, volume 4, pages 321–333, Berkeley, CA, 1961.
G. Rasch. A mathematical theory of objectivity and its consequences for model construction. In European Meeting on Statistics, Econometrics and Management Science, volume 2, Amsterdam, 1968.
V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.
N. B. Shah and D. Zhou. Double or nothing: Multiplicative incentive mechanisms for crowdsourcing. arXiv:1408.1387, 2014.
V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? Improving data quality and data mining using multiple noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 614–622, 2008.
A. Singla and A. Krause. Truthful incentives in crowdsourcing tasks using regret minimization mechanisms. In Proceedings of the 22nd international conference on World Wide Web, pages 1167–1178, 2013.
R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263, 2008.
Y. Tian and J. Zhu. Learning from crowds in the presence of schools of thought. In Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining, pages 226–234, 2012.
J. S. Uebersax and W. M. Grove. A latent trait ﬁnite mixture model for the analysis of rating agreement. Biometrics, 49:823–835, 1993.
M. Venanzi, J. Guiver, G. Kazai, P. Kohli, and M. Shokouhi. Community-based Bayesian aggregation models for crowdsourcing. In Proceedings of the 23rd International World Wide Web Conference, pages 155–164, 2014.
P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems 23, pages 2424–2432, 2010.
J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more: optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems 22, pages 2035–2043, 2009.
O. F. Zaidan and C. Callison-Burch. Crowdsourcing translation: Professional quality from nonprofessionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229, 2011.
30

Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Advances in Neural Information Processing Systems 27, 2014.
D. Zhou, J. C. Platt, S. Basu, and Y. Mao. Learning from the wisdom of crowds by minimax entropy. In Advances in Neural Information Processing Systems 25, pages 2204–2212, 2012.
D. Zhou, Q. Liu, J. C. Platt, and C. Meek. Aggregating ordinal labels from crowds by minimax conditional entropy. In Proceedings of the 31st International Conference on Machine Learning, 2014.
S. C. Zhu, Y. N. Wu, and D. B. Mumford. Minimax entropy principle and its applications to texture modeling. Neural Computation, 9:1627–1660, 1997.
31

