Cavs: A Vertex-centric Programming Interface for Dynamic Neural Networks
Hao Zhang†, Shizhen Xu†, Graham Neubig, Wei Dai, Qirong Ho, Guangwen Yang, Eric P. Xing
Carnegie Mellon University, Tsinghua University, Petuum Inc. † indicates equal contributions

arXiv:1712.04048v1 [cs.LG] 11 Dec 2017

Abstract
Recent deep learning (DL) models have moved beyond static network architectures to dynamic ones, handling data where the network structure changes every example, such as sequences of variable lengths, trees, and graphs. Existing dataﬂow-based programming models for DL—both static and dynamic declaration—either cannot readily express these dynamic models, or are inefﬁcient due to repeated dataﬂow graph construction and processing, and difﬁculties in batched execution. We present Cavs, a vertexcentric programming interface and optimized system implementation for dynamic DL models. Cavs represents dynamic network structure as a static vertex function F and a dynamic instance-speciﬁc graph G, and performs backpropagation by scheduling the execution of F following the dependencies in G. Cavs bypasses expensive graph construction and preprocessing overhead, allows for the use of static graph optimization techniques on pre-deﬁned operations in F, and naturally exposes batched execution opportunities over different graphs. Experiments comparing Cavs to two state-of-the-art frameworks for dynamic NNs (TensorFlow Fold and DyNet) demonstrate the efﬁcacy of this approach: Cavs achieves a near one order of magnitude speedup on training of various dynamic NN architectures, and ablations demonstrate the contribution of our proposed batching and memory management strategies.
1. Introduction
Deep learning (DL), which refers to a class of neural networks (NNs) with deep architectures, is now a workhorse powering state-of-the-art results on a wide spectrum of tasks [36, 61, 63]. One reason for its widespread adoption is the variety and quality of software toolkits, such as Caffe [30], TensorFlow [1] and DyNet [38, 39], which ease programming of DL models, and speed computation by harnessing modern computing hardware (e.g. GPUs), software libraries (e.g. CUDA, cuDNN [9]), and compute clusters [11, 65, 66].
One dominant paradigm in the training of DL models, adopted by toolkits such as Caffe and TensorFlow, uses static

dataﬂow graphs [1, 37]. These graphs represent the ﬂow of data through computational functions, and are deﬁned using symbolic programming [1, 5], once before beginning training or testing of the model. The training of these models is performed through auto-differentiation, in which users are only required to assemble their model architectures by connecting operators using high-level language interface (e.g. Python), after which the framework will automatically derive the correct algorithm for training [4]. With proper optimization, the execution of these static dataﬂow graphs can be highly efﬁcient. Speciﬁcally, by separating model declaration and execution, it makes it possible for the graph to be further processed and optimized before runtime [1]. In addition, the evaluation of multiple data samples in a dataﬂow graph can be naturally batched to leverage the improved computational capability of modern hardware (e.g. GPUs), which is extremely advantageous for DL workloads [32].
While these static dataﬂow graph have major efﬁciency advantages, their applicability highly relies on a key assumption – the dataﬂow graph (i.e. NN architecture) ﬁxed throughout the runtime. With the increasing complexity of the problems to be addressed, DL has been extended and applied on data with more complicated structures, such as sequences [28, 49], trees [50] and graphs [33], over which the NN may conditionally choose its own computation order for speciﬁc modeling needs, i.e. the structure of its dataﬂow graph changes over training. To better support these dynamic models, some recent frameworks [38, 56] propose to declare a dataﬂow graph per sample (a.k.a. dynamic declaration). While dynamic declaration is convenient to developers as code can basically be written in the same way as it usually is in the native programming language (e.g. Python, C++), it exhibits a few limitations. First, programmers still have to write code to explicitly assemble the dataﬂow graph for each input sample, which might be nontrivial for graphs with sophisticated structures. Second, as the graph construction needs to be performed repeatedly, its overhead grows linearly with the number of training instances, preventing the application of complex static graph optimization tech-

1

niques (in fact, graph construction takes longer time than the computation in some frameworks [34], see §5.2). Finally, since each sample owns a dataﬂow graph specifying its unique computational pattern, batching together similarly shaped computations across instances is non-trivial. Without batching operations, the computation is inefﬁcient due to its lack of ability to exploit modern computational hardware, and while some progress has been made in recent research [34, 39], how to automatically batch the computational operations from different graphs remains a difﬁcult problem.
To address these challenges, we present Cavs, a new programming interface for dynamic NNs, and a system implementation with optimization strategies tailored to it. Cavs leverages the recurrent and recursive nature of dynamic NNs. Instead of declaring a dataﬂow graph per sample, it alternatively decomposes a dynamic dataﬂow graph as two components: one static vertex function F that is only declared (by the user) and optimized once, and an input graph G that is instance-speciﬁc and not used until runtime. Thereby, the workﬂow of training a dynamic NN can be represented as scheduling the execution of F following the structure of the input graph G. Cavs combines the best of symbolic construction of dataﬂow graphs for DL [1, 5] with the vertex-centric model [19] in graph computing: it only requires users to deﬁne F symbolically by “thinking locally like a vertex” [55]. Cavs will perform auto-differentiation, schedule the function execution following the dependency reﬂected by G, and guarantee efﬁciency and correctness. It also inherits the ﬂexibility of symbolic programming, i.e. users are allowed to declare multiple vertex functions to express more dynamics, or connect static dataﬂow graphs with dynamic ones to construct more complex NN architectures.
Cavs demonstrates a few advantages over other programming models. It simpliﬁes user programs and avoids the overhead of repeated dataﬂow graph construction. Moreover, this vertex-centric model naturally exposes opportunities for batched computation: we introduce a simple batching policy in Cavs’ scheduler to parallelize the execution of F on multiple vertices during the evaluation of a batch of samples with different input graphs (§3.2), and a novel memory management mechanism to guarantee the memory coalescing (§3.3). Together they yield signiﬁcant performance improvements. Compared to dynamic declaration, as the dataﬂow graph encoded by the vertex function is static throughout the runtime, it can beneﬁt from various static graph optimizations [1, 8, 18, 20], such as lazy batching, streaming, and kernel fusion (§3.5), which would otherwise be less effective on the scenario of dynamic declaration because of the repeated preprocessing/optimization cost (see §6).
We implement Cavs as an additional layer pluggable to most existing DL frameworks to enhance their support for dynamic NNs. To evaluate its performance, we compare Cavs to TensorFlow Fold [34] and DyNet [38, 39], two stateof-the-art systems supporting dynamic NNs and dynamic

batching. We focus our experiments on GPU training, and verify that both Fold and DyNet suffer from substantial overhead caused by repeated graph preprocessing or construction, which is bypassed by Cavs (§5.2). In terms of overall performance, on static NNs, Cavs demonstrates equivalent or slightly better performance than Fold and DyNet, while on several dynamic NNs with notably difﬁcult-to-batch workloads (e.g. Tree-LSTM [50] and Tree-FC [34]), Cavs demonstrates near one order of magnitude speedups across various dataset and hyper-parameter settings (§5.1). We further investigate the key contributing factors to the performance: Cavs beneﬁts from not only a better memory management strategy, but also graph execution optimizations which are originally designed for static dataﬂow graphs and perhaps less useful in dynamic declaration.
2. Background
DL is distinguished from other ML algorithms mainly by its use of deep neural networks, a family of ML models with many interconnected layers, each composed of various mathematical operations (e.g. +, −, sigmoid, matmul). Before a DL model can give predictions, it is usually trained by stochastic gradient descent, an iterative process in which gradients are calculated through backpropagation [44]. There is a natural connection between directed graphs and NNs: we can map the graph nodes to the computational operations or parameters in NNs, and let the edges indicate the direction of the data being passed between the nodes. In this case, we can represent the process of training NNs as batches of data ﬂowing through computational graphs, i.e. dataﬂow graphs [1, 5, 38].
Figure 1(a) summarizes the programming model derived from these dataﬂow graphs, which is named as static declaration and has been adopted in many DL frameworks [1, 5, 8]. Without ambiguity, we use D to denote both the dataﬂow graph itself and the computational function implied by D. On one hand, we note its execution is highly efﬁcient as the computation over multiple samples is batched – at each iteration p, a batched tensor of K samples {xpk}Kk=1 is fed to D, and the computation is executed in a single pass, allowing for efﬁcient use of memory caches or parallelized computation. On the other hand, this paradigm relies on a key assumption: the dataﬂow graph D is static for all samples and ﬁxed throughout the computation. Hence, D will only be declared once with a constant graph construction/optimization overhead; all samples share the same computational pattern speciﬁed in D, so the computation of different samples can be by nature batched by simply expanding the input with a batch dimension. Though static declaration is effective on a wide range of NN models, such as convolutional neural networks (CNNs) over ﬁxed-size images, it is much more difﬁcult to apply to graphs with dynamically changing structures, some examples of which are shown in the next section.

2

/* (a) static declaration */
// all samples must share one graph
declare a static dataﬂow graph D.
for p = 1 → P : read the pth data batch {xp}K . batched computation: D({xkpk}=K1 ).
k k=1

/* (b) dynamic declaration */

for p = 1 → P :

read the pth data batch {xpk}K k=1.

for k = 1 → K:

declare a dataﬂow graph Dp for xp.

single-instance

k
computation:

Dp (xkp

).

kk

/* (c) our proposed vertex-centric model */

declare a symbolic vertex function F .

for p = 1 → P :

read the pth data batch {xp}K .

read

their

associated

k
graphs

k{=G1p }K

.

compute F over {Gp}K with inkpuk=ts1{xp}K .

k k=1

k k=1

Figure 1: The workﬂows of (a) static declaration, (b) dynamic declaration, (c) Cavs’ vertex-centric programming model.

(a) Cell (b) Function

o

(c)

VW

h

U x

Sequence (d) Tree

Graph

Model

static declaration
dynamic declaration (instant evaluation) dynamic declaration
(lazy evaluation) Fold
Vertex-centric

Frameworks
Caffe, Theano, TensorFlow, MxNet PyTorch, Chainer
DyNet TensorFlow-Fold
Cavs

Expressiveness
× √
√ √ √

Batching
×
× √ √ √

Graph Cons. Overhead
low
N/A
high high low

Graph Exec. Optimization
beneﬁcial
unavailable
not beneﬁcial unknown beneﬁcial

Figure 2: Left (a)-(d): A cell function shown in (a) could be applied on different structures such as a (b) chain (c) tree, or (d) graph. Right table: the landscape of existing programming models for dynamic NNs, and their advantages and disadvantages (see §2.2 and §6).

2.1 Dynamically Structured Computational Graphs
Modern DL has been developed and applied extensively over data with more complicated structures, e.g. data structured as sequences, trees and graphs, which are required to tackle practical problems such as machine translation [49, 50], questionanswering [51], and semantic image segmentation [33, 62]. As a concrete example of dynamic NNs, we will use recurrent and recursive neural networks (RNNs) [16, 28, 46]. RNNs are a class of NNs generally applied on modeling structured inputs or outputs, e.g. sequences or graphs. At the core of an RNN is a cell function with trainable parameters. It will be dynamically applied at different places of the input structure, and optionally produce an output if needed. Figure 2(a) illustrates such a cell function: it takes an input element x, forwards it through a few mathematical transformations, and generates some intermediate state h and an output o. Depending on what transformations are applied, different variants of RNNs have been derived, such as long-short term memory units (LSTM) [28] and gated recurrent units (GRU) [10]. However, the internals of the cells themselves are secondary; the dynamics of the net as a whole are mainly reﬂected by the structures that the NN works on. Sequence RNNs. When the input to the RNN are sequences (e.g. sentences) as in Figure 2b, the cell function is applied across all elements of the sequence. At each step t, it takes the element xt (e.g. a word) from the input sequence, and the state variable ht−1 maintained by the model at step t − 1. It computes an output ot, and a new state ht that will be used by the next step t + 1. Hence, This sequence RNN encodes not only the data values, but also the dependencies present in the sequence. If represented as a dataﬂow graph, the graph exhibits a chain structure. As the input or output sequences usually have variable length (e.g. translating an arbitrarylength English sentence into Chinese), the dataﬂow graph needs to be dynamically changed, i.e. the steps of the chain must be adapted to ﬁt the length of the input or output. Tree-structured RNNs. Further, RNNs can be enhanced to model data with more complex structures suited for down-

stream tasks. For example, tree-structured RNNs (TreeRNNs, Figure 2c), have been used to classify the sentiment of sentences [41] given an associated binary tree representing the sentence structure [45, 50]. In this case, a leaf of the tree maps to a word of the sentence, an internal node corresponds to a multi-word phrase. To process this structure, the cell function scans the tree recursively, starting from leaf nodes until reaching the root. At the node t, it computes the state ht = f (htl , htr , xt), where xt is the input to the node, and htl , htr are the states of its left and right children, respectively. As the tree structure vary from example to example, the dataﬂow graph of a Tree-RNN is highly dynamic. Graph-structured RNNs. Similarly, RNNs can be extended to compute over more general graphs, such as N-ary trees or graphs (Figure 2d), as long as their parameters are learnable. In fact, various NNs have been developed toward having more dynamic workﬂows [33, 50], and proven quite effective because of their ability to encode structured information. While we take RNNs as examples for explanation, we note there are many other dynamic NNs in the literature or production with their dynamics reﬂected in various perspectives: variably sized inputs/outputs [3, 6, 14, 16, 28, 49], variably structured inputs/outputs [33, 45, 50], or with nontrivial inference algorithms [21, 23, 31, 67].
2.2 Programming Dynamic Dataﬂow Graphs
As the assumption in §2 no longer holds for dynamic structures, static dataﬂow graphs in their original form cannot be used. There are currently two remedies to this problem: expanding the graph programming language to allow it to explicitly include controls structure necessary to implement these applications, or forgo the efﬁciency gains afforded by static dataﬂow graphs and instead use a dynamic declaration framework that reconstructs the graph for every training example. We explain below and summarize them in Figure 2. Static declaration. Static unrolling [1] is a standard way to express sequence RNNs with ﬁxed steps. To handle variablelength data, it declares an RNN that has number of steps equal with the length of the longest sequence in the dataset.

3

It then appends zeros at the end of other sequences to have equal length, and feeds them in batches to the dataﬂow graph for computation. Static unrolling enables batched computation of multiple sequences, but obviously results in substantial unnecessary computation.1 Dynamic unrolling implements basic control ﬂow functionality within static graphs, allowing for the declaration of graph operators similar to while loops. At each iteration of the training, the cell function of the RNN will be executed a conditional number of times determined at runtime by the length of the longest sentence in the batch. It then pads the sequences in the batch and perform batched computation, it waste computational resources. Notably, both of these methods essentially cannot support more complex structures than sequences. Dynamic declaration. Dynamic declaration is able to express dynamically varying dataﬂow graphs, by creating a unique dataﬂow graph for each sample according to its associated structure. It however requires users to explicitly write (more) code to build a dataﬂow graph for each input graph, which is nontrivial for graphs with sophisticated structures. As the dataﬂow graphs are always changing, it can hardly beneﬁt from well-established dataﬂow graph optimization techniques (§3.5) – we will have to perform graph processing/optimization for each dataﬂow only for a single sample, but incorporating the optimization itself has an overhead. More importantly, as we are unable to naturally batch the computation of different sample, single-instance training would be very inefﬁcient in the absence of batched computation. At the backend, since a dataﬂow graph needs to be constructed per sample, the overhead is linearly increasing with the number of samples, and sometimes yields downgraded performance [34] (§5.2), even for frameworks with optimized graph construction implementations [38].
Tensorﬂow Fold [34] and DyNet [39] go one step further and perform dynamic batching for dynamic dataﬂow graphs. Fold turns dynamic dataﬂow graphs into a static control ﬂow graph to enable batched execution, but introduces a complicated functional programming-like languages and a large graph preprocessing overhead. DyNet proposes an autobatching strategy that searches for batching opportunities by proﬁling every ﬁne-grained operator, while this step itself has non-negligible overhead, and loses the opportunities of graph-level optimizations. There are also some “imperative” frameworks, such as PyTorch [17] and Chainer [57] that allow users to construct dynamic NNs but performs instant evaluation of each user expression. As model construction and execution are coupled, they are usually difﬁcult to perform dynamic batching or graph optimization. Overall, they are still far from efﬁcient when handling dynamic NNs. We next describe our proposed vertex-centric programming model to overcome the aforementioned limitations.
1 It is also possible to split sentences into several buckets of different lengths, which alleviates this problem somewhat but adds some degree of code complexity and is not a fundamental solution.

3. Cavs Design and Optimization
Our motivation comes from several key principles ML developers usually comply with to ensure the feasibility and learnability of the model during their design of dynamic NNs. We note most dynamic NNs are designed to exhibit a recursive structure (e.g. sequence RNN, Tree-RNN), or a combination of static and recursive structures (e.g. LRCN [2, 13], attention [60]), or even a combination of different recursive structures (e.g. encoder-decoder RNNs [49]). Within one such structure, a function is dynamically applied over instancespeciﬁc graphs, and every vertex of the graph usually interacts in a same way with it neighboring vertices following the function. The computational function itself, however, is usually static and parameterized by ﬁxed learnable parameters.
This observation motivates us to design a new programming model, called Cavs, that combines the best of dataﬂow graphs with the vertex-centric model in graph computing. As a comparison, we present the workﬂow of Cavs in Figure 1c. For clarity, we will use the following terminology and notation in the rest of the paper: we call the instance-speciﬁc structure associated with the input sample as an input graph, and notate it as G, and a node in that graph as a vertex, to be distinguished from a dataﬂow graph D and the nodes (which are usually operators or variables) therein. Figure 3 illustrates the concept of this vertex-centric programming model. To describe an aforementioned dynamic structure, different from dynamic declaration, which requires users to manually declare dataﬂow graphs for each sample according to its associated graph, Cavs instead directly takes it as an input argument. To be aware of what computation shall be performed, Cavs requires users to implement a simple vertex function F by “thinking like a vertex”, informing the framework how one vertex in a dynamic NN will interact with its connected vertices (if these is any). In F, users can utilize conventional DL operators to assemble a symbolic construct that will be evaluated dynamically following the structure of G, while Cavs will ensure the correctness and efﬁciency. Therefore, a vertex function F, together with an input graph G, implicitly encodes a recurrent dataﬂow graph, which maps to a subgraph of the implicit full dataﬂow graph of the model that may needs to be explicitly declared in traditional programming models. For convenience of notations, we will call any part of the structure that cannot be encoded by F and G as external to (F, G), and vice versa. Cavs allows users to connect any external static dataﬂow graph to a dynamic structure encoded by (F, G) to express various model architectures (e.g. connecting a CNN to an RNN), or declare multiple vertex functions for different structures, and connect them appropriately to express more complex models (e.g. an encoder-decoder LSTM network).
While it is still necessary to create an I/O function to read input graphs for each sample, this must be done in any models, and only once before training commences, which means that it can be shared across epochs or even training runs.

4

Chain

ht-1 ct-1 f

LSTM

xt u|i ct O ht

push

gather Tree

scatter

ht-1 ct-1 f

GRU

xt u f ct O ht

pull

Input Output
Internal External On Both Data Path Data Path Data Path

h1 c1 f1 ChLiSldTsMum x u|i c O h
f0 h0 c0

Figure 3: Cavs represents a dynamic structure as a dynamic input graph G (left) and a static vertex function F (right).

Cavs no longer requires users to construct the full dataﬂow graphs for each sample by themselves. As repeated graph construction is bypassed, the overhead will also be avoided. With this vertex-centric model, Cavs transforms the problem of evaluating multiple dataﬂow graphs with different structures [34, 39] into a simpler form – scheduling the execution of the vertex functions following the input graphs. For the later problem, we can easily batch the execution of F over multiple vertices at runtime (§3.2), leveraging the batching computational capability of modern hardware. Moreover, as the vertex function itself maps to a static symbolic dataﬂow graph, it is open and can beneﬁt from various graph optimization techniques originally developed for static declaration, such as kernel fusion, streaming, and our proposed lazy batching, which might not be effective in the scenario of dynamic declaration. We next describe Cavs’ APIs.

3.1 Programming Interface
Besides the generic math operators used to declare the computation, Cavs exposes four symbolic APIs for users to specify how the messages shall be passed between vertices in their vertex functions: gather, scatter, pull, push.
• gather(child idx): gather accepts an index of the child vertices, gets the child content from gather/scatter buffer and returns a list of symbols that represent the output of these vertices.
• scatter(op): scatter is a reverse API of gather, and has a symbol op as its input argument. Scatter will set the output of current vertex to gather/scatter buffer.
gather and scatter resemble the GAS model in graph computing [19] – both are vertex-centric APIs that help users express the overall computational patterns by thinking locally like a vertex: gather receives messages from dependent vertices, while scatter updates information to parent vertices. But note several key differences: (1) gather and scatter here are fully symbolic – gather allows backpropagation through it; (2) In graph computing, all nodes interact with their connected nodes in the same way following a user-speciﬁed apply function, while in dynamic NNs, a vertex usually interacts differently with its different child vertices, speciﬁed by the symbolic programs (between the call of gather and scatter) in the vertex function; (3) In

1 def F ():

2 for k in range(N):

3

S = gather(k) # gather states of child vertices

4

ck, hk = split(S, 2) # get hidden states c and h

5 x = pull() # pull the first external input x

6

7 # specify the computation 8 h = N k=−01hk 9 i = sigmoid(W(i)× x + U(i)× h + b(i))

10 for k in range(N):

11

fk = sigmoid(W(f)× x + U(f)× hk + b(f))

12 o = sigmoid(W(o)× x + U(o)× h + b(o))

13 u = tanh(W(u)× x + U(u)× h + b(u))

14 c = i ⊗ u + N k=−01fk ⊗ ck 15 h = o ⊗ tanh(c)

16

17 scatter(concat([c, h], 1)) # scatter c, h to parent vertices

18 push(h)

# push to external connectors

Figure 4: An N-ary child-sum TreeLSTM [50] in Cavs.
graph computing, a vertex of a graph always interacts with other vertices of this graph, while in DL, the vertex of a dynamic NN usually takes input from not only the internal of the structure expressed by F and G (internal data path in Figure 3), but also from the external of it, e.g. a step in an RNN can take inputs from a CNN feature extractor or some external I/O (external data path in Figure 3). In this case, gather and scatter are insufﬁcient to express such semantics. Cavs therefore provides another two APIs:
• pull(): pull grabs inputs from the external of the current dynamic structure, e.g. another NN, or some I/O.
• push(op): push is thus the reverse of pull that sets the output of the current vertex as op. If this vertex is pulled by others, the content of op will be returned.
With appropriate indexing, push and pull connect a vertex inside a dynamic structure expressed by (F, G) to other connectors external to (F, G), such as another dynamic structure, or another static dataﬂow graph. With these four APIs, we present in Figure 4 an example user program how the N ary child-sum Tree-LSTM [50] can be simply expressed by using them and other mathematical operators. Expressiveness. With these four APIs, Cavs can be seen as a middle ground between static and dynamic declaration: In the best case, the model can be easily represented by a single vertex function plus input graphs. While in the worse case scenario, that every sample has a unique input graph while every vertex in the graph has a unique way to interact with its neighboring vertices, Cavs reduces to dynamic declaration that one has to deﬁne a vertex function for each vertex of input graphs. However, dynamic NNs in this scenario are very rare and usually not preferred because of the difﬁculty of design, programming and learning.

3.2 Scheduling
Once users deﬁne the vertex function F and launch the execution, the Cavs scheduler arranges the evaluation of F over the input graphs. Given F, Cavs’s scheduler follows designed policies to efﬁciently perform backpropagation for all samples {xi}Ni=1 and their associated graphs {Gi}Ni=1. Backpropagation. Cavs performs backpropagation [27] as follows. For a sample xi with its input graph Gi, the sched-

5

uler starts the forward pass from the input vertices of Gi, and proceeds following the direction indicated by the edges in Gi: at each sub-step, the scheduler ﬁgures out the next activated vertex in Gi, and evaluates F at this vertex following the symbolic programs in F. It then marks this vertex as evaluated, and proceeds with the next activated vertex until reaching a terminal vertex (e.g. the loss function). A vertex of G is activated if and only if all its dependent vertices have been evaluated. The backward pass is continued right after the forward. The scheduler ﬁrst resets the status of all vertices as not evaluated, then scans the graph in a reverse direction, starting from the ending point of the forward pass. It similarly ﬁgures out the next activated vertex, but applies another function ∂F, which is the backward function of F and automatically derived by Cavs via auto-differentiation (§3.4), until all vertices have been evaluated in backward. To train a NN to convergence, the above process has to be
Algorithm 1 Backpropagation with the batching policy.
1: function FORWARD({xk}K k=1, {Gk}K k=1, F ) 2: set task ID t ← 0, task stack S ← ∅. 3: while NOT all vertices in {Gk}K k=1 are evaluated do 4: ﬁgure out all activated vertices in {Gk}K k=1 as a set Vt. 5: push Vt into S. 6: evaluate F on Vt: GraphExecute(Vt, F ) (see §3.5). 7: set the status of all vertices in Vt as evaluated. 8: set t ← t + 1. 9: end while 10: return S. 11: end function 12: function BACKWARD(S, {Gk}K k=1, ∂F ) 13: set t as the size of S. 14: while S is not empty do 15: pop the top element of S as Vt. 16: Evaluate ∂F on Vt: GraphExecute(Vt, ∂F ) (§3.5). 17: set t ← t − 1. 18: end while 19: end function
iterated by the scheduler over all samples {xi}Ni=1 and their associated graphs {Gi}Ni=1, for many epochs. Instead a sequential execution, Cavs designs a batching policy to perform batched computation, considering the fact that evaluating a set of same arithmetic operations together is signiﬁcantly faster than the sequential evaluation of each of them. Batching policy. Given a data batch {xk}Kk=1 ⊆ {xi}Ni=1 and associated graphs {Gk}Kk=1, this policy groups multiple vertices and then performs batched evaluation of F in order to reduce kernel launches and exploit parallelism. Algorithm 1 details this policy. Speciﬁcally, the scheduler divides the forward evaluation of (a batch of) K graphs into multiple steps. At each step t, it analyzes {Gk}Kk=1 at runtime and determines a set Vt contains all activated vertices in {Gk}Kk=1. It then evaluates F over these vertices by creating a batching execution task, with the task ID set to t2. The task is passed to a graph execution engine, which will further optimize the
2 Without ambiguity, we use Vt to denote both the set of vertices to be batched together, as well as the batching task itself.

execution and conduct the actual computation (§3.5). Meanwhile, the scheduler records the information of this task by pushing Vt into a stack S. At each step of the backward, the scheduler pops out an element Vt from S, creates a corresponded backward batching task – the execution engine will evaluate the derivative function ∂F over vertices in Vt, until all vertices of {Gk}Kk=1 are evaluated.
We note the batching policy plays a similar role as the dynamic batching in Fold [34] and DyNet [38] in the scenario of dynamic declaration. However, Cavs determines how to batch fully dynamically during runtime using simple breadth-ﬁrst search with negligible cost (instead of analyzing graphs before every iteration of the execution). We next describe an improved management management strategy based on this batching policy.
3.3 Memory Management
In static declaration [1, 38], a symbol in the user program usually corresponds to a tensor object, with its shape inferred from the program and batch size speciﬁed in advance. The framework usually preallocates a continuous storage on memory for each tensor and ﬁxes it throughout runtime. However, in Cavs, as each batching task is determined at runtime and not visible before execution, its memory management exhibits more complexities. For the batched computation to be efﬁcient, Cavs must guarantee the inputs and intermediate states during the evaluation of F over a group of runtime-determined vertices coalescing in memory. If we adopt the aforementioned strategy, for each operation in F, Cavs has to index each slice of its input tensor (which may be scattered in different places) and rearrange them as a continuous memory block, which might cause nontrivial overhead.
Cavs proposes a novel data structure dynamic tensor to address this challenge (Figure 6). A dynamic tensor is a wrapper of a multi-dimensional array [1, 59] that contains four main members: shape, bs, a pointer p to a chunk of memory, and offset. shape is an array of integers representing the speciﬁc shape of the tensor excluding the batch dimension. It can be inferred from the user program and set before execution. The batch size bs is implemented as a placeholder in Cavs, with its value dynamically set by the scheduler at runtime at the beginning of a batching task. For each dynamic tensor, Cavs will preallocate a chunk of continuous memory, and point p to its starting address. This memory block is often very large and not ﬁxed-sized – it can be further extended if needed. To access a dynamic tensor, the execution engine moves p forward with the value speciﬁed in offset, and reads/writes number of elements equal to bs · i shape[i]. Therefore, bs together with offset provide a view of the tensor, and the state of the tensor will vary based on their values. Given a vertex function F, Cavs creates dynamic tensors {αn}Nn=1 for each non-parameter symbol sn(n = 1, . . . , N ) in F , and also {∇αn}Nn=1 as their gradients, while it creates static tensors for model parameters.

6

def ():

pull

gather

gather/scatter buffer

0 1 2 3 4 5 6 7 8 9 10 11 12 13 pull buffer

0 = pull() 1 = gather(0) 2 = gather(1)
3 = matmul(0 , u)
 +matmul(1, v)
+matmul(2 , w) scatter(0,3 )

push

scatter

batching task 0 0 1 2 5 6 7 8 9

 push( 3)

13

1

4

11

12

2

0125 6789

3 10 11 085 196

0125 6789 3 1011

4 12 37 2 10

0125 6789 3 1011 4 12

13
11 0 1 2 5 6789 3 1011 4
12 12

3 2 5 6 7 10

3 0 1 2 5 6 7 8 9

3 10 11

4 12

13

01

89

0 1 2 3 4 5 6 7 8 9 10 11 12 13 push buffer

Figure 5: A color or a dashed lined box corresponds to a batching task. The rectangles are memory blocks. The numbers are vertex IDs. Memory blocks in one row belong to a dynamic tensor (e.g. α0 ) and are physically continuous, though we separate them in different boxes.

Figure 5 illustrates how the memory is assigned during the forward pass by simply manipulating dynamic tensors. In particular, in a training iteration, for a batching

struct DynamicTensor { vector<int> shape; int bs; int offset; void* p; };
Figure 6: Dynamic tensor.

task Vt, the scheduler sets bs of all {αn}Nn=1 to Mt = |Vt| (the number of vertices in Vt). The execution engine then

performs batched evaluation of each expression in F one

by one. For an expression sl = op(sr)3, the engine ﬁrst

reads αr (the dynamic tensor of the RHS symbol sr) by

offsetting αr.p by αr.offset then reading a block of

Mt i αr.shape[i] elements, and presents it as a tensor with batch size Mt and other dimensions speciﬁed in αr.shape.

It then applies batched computational kernels of the oper-

ator op over this continuous block, and writes the results

to αl (the dynamic tensor of the LHS symbol sl) on the

continuous block in between [αl.p + αl.offset, αl.p +

αl.offset + Mt i αl.shape[i]]. Upon the completion of Vt, the scheduler increases the offset of all {αn}Nn=1
by Mt i αn.shape[i], respectively. It then starts the next batching task Vt+1 until F has been evaluated at all vertices of {Gk}Kk=1. Hence, intermediate results generated in each batching task at forward pass are stored continuously in the

dynamic tensors, and their offsets are recorded.

The scheduler then starts the backward pass. It initial-

izes ∇αn.offset for each n as αn.offset. Since the back-

ward execution follows an exactly reverse order of the for-

ward pass (Algorithm 1), the intermediate results generated

during forward can be easily accessed by decreasing the

offset of {αn}Nn=1. Speciﬁcally, for each batching task Vt popped out from S, the execution engine sets bs of all

dynamic tensors to Mt, and for each αn and ∇αn, de-

creases their offset by Mt i αn.shape[i]. For an expression ∇sr = grad op(∇sl, sl, sr) in ∂F that corresponds to

sl = op(sr) in F (see §3.4), the engine reads the current

states of ∇sl, sl, sr (which are continuous on memory) and

performs batched execution of grad op. Different from the

forward pass, the gradient result will be added to the current

state of ∇sr instead of overwriting.

3 Without losing generality the user-deﬁned expressions can be arbitrary, e.g. with more than one argument or return values.

At the entrance of F , the vertices {vm}M m=t 1 in Vt need to interact with its dependent vertices in previous Vt−1 to gather their outputs as inputs (L3 of Figure 4), or pull inputs from the external (L5 of Figure 4). Cavs maintains memory buffers to enable this (Figure 5). The memory buffers are key-value stores where the key is the vertex ID m and the value is a tensor slice corresponding to the results of the scattered symbol at vertex vm (batch size is 1). Cavs provides a query function IndexBuffer(op, key) that returns the value in op’s corresponded buffer given a key. During the execution, a gather expression sl = gather(child idx) will trigger memory movements: for each vm ∈ Vt, the scheduler ﬁgures out the vertex ID of its child vertex in the input graph with index child idx; it then indexes the content in gatherBuffer with the vertex ID as key, and copies and writes it into αl as a slice. Similarly, at the exit of F, a scatter expression scatter(sr) will split the current state of αr into a few slices, and move them to the gatherBuffer for its parent nodes to gather. The push/pull work similarly with gather/scatter, but over the pushBuffer and pullBuffer, respectively, to communicate messages with the external.
Algorithm 2 summarizes the memory management process during forward pass. With this strategy, Cavs guarantees memory continuity for any batched computation in F. Compared to dynamic batching in DyNet, Cavs performs memory movement only at the entrance and exit of F, instead of for each expression (operator), it therefore signiﬁcantly reduces overhead by memory operations (§5.3).
3.4 Auto-differentiation
Cavs by nature supports auto-differentiation. Given a vertex function F it derives ∂F following the auto-differentiation rules: for each math expression such as sl = op(sr) in F, Cavs generates a corresponded backward expression in ∂F as ∇sr = grad op(∇sl, sl, sr). For the four proposed operators, with the memory management strategy described above, we note scatter is the backward operator of gather in the sense that if gather collects inputs from gatherBuffer previously written by scatter at the forward pass, a scatter needs to be performed to write the gradients to the gatherBuffer for its dependent vertices

7

Algorithm 2 Memory management at forward pass.

1: function FORWARD({Vt}Tt=1, {αn}Nn=1, F ) 2: for t = 1 → T do

3: for n = 1 → N do αn.bs ← Mt end for

4: for each expression like sl = op(sr) in F do

5: if op ∈ {gather, pull} then

6:

C ← i αl.shape[i], q ← αl.p + αl.offset.

7:

for vm ∈ Vt(m = 1 → Mt) do

8:

src ← IndexBuffer(op, m), dest ← q + (m − 1)C.

9:

memcpy(dest, src, C).

10:

end for

11: else if op ∈ {scatter, push} then

12:

C ← i αr.shape[i], q ← αr.p + αr.offset.

13:

for vm ∈ Vt(m = 1 → Mt) do

14:

dest ← IndexBuffer(op, m), src ← q + (m − 1)C.

15:

memcpy(dest, src, C).

16:

end for

17: else

18:

perform batched computation: αl = op kernel(αr).

19: end if

20: end for

21: for n = 1 → N do αn.offset+ = Mt i αn.shape[i] end for 22: end for

23: end function

pull emloboekdudping matmul

SYNC

gather

slice matmul add slice

slice

Fused Kernel

add sig

add tan

mul

mul

add sig

mul add sig

mul

linear transformation

push

concat

scatter

parameter eager operator lazy operator lazy batching

stream1

stream2

Figure 7: The dataﬂow graph encoded by F of Tree-LSTM.

to gather at the backward pass. Hence, for an expression like sl = gather(child idx) in F , Cavs will generate a backward expression scatter(∇sl) in ∂F. Similarly, the gradient operator of scatter is gather. The same autodifferentiation rule applies for push and pull as well.

3.5 Graph Execution Engine
Beneﬁting from the vertex-centric representation, the vertex function F essentially deﬁnes a (small) static dataﬂow graph that is open to various graph execution optimization techniques (which might not be the case in dynamic declaration). We next discuss three proposed optimizations on Cavs’ execution engine for improved performance. Lazy batching and streaming4. In addition to batched execution of F, the lazy batching and streaming explore potential parallelism for a certain group of ﬁner-grained operators in F or ∂F called lazy and eager operators. Deﬁnition 1. An operator in F (∂F) is a lazy operator if at the forward (backward) pass, for ∀v ∈ G, ∀G ∈ {Gk}Kk=1,

4 Streaming is a borrowed terminology from CUDA programming which means executing different commands concurrently or out of order with respect to each other on different GPU streams. As Cavs’ optimization strategies are agnostic to the low-level hardware, we use streaming interchangeably with multi-threading if the underlying computing hardware is CPU.

the evaluation of F (∂F) at any parent (dependent) vertex of v does not rely on the evaluation of F at v. It is an eager operator if the evaluation at v does not rely on the evaluation of F (∂F) at any dependent (parent) vertices of v.
In Cavs, ﬁguring out eager and lazy operators in F and ∂F is straightforward given the following proposition:
Proposition 2. Denote DF (D∂F ) as the dataﬂow graph encoded by F (∂F ), and g, s ∈ DF (D∂F ) as corresponded nodes of the gather and scatter operator, respectively. A node that has g as its dependent and is not on any path from g to s is a lazy operator. A node that has s as its ancestor and is not on any path from g to s is an eager operator.
Figure 7 illustrates a dataﬂow graph of the vertex function of Tree-LSTM, with eager and lazy operators colored. A property of them is that their evaluation is not fully subject to the dependency reﬂected by the input graph G. For instance, the pull operator in Figure 7 is eager and can be executed in prior – even before F has been evaluated at the vertices gather tries to interact with; the push operator is lazy, so we can defer its execution without impacting the evaluation of F at parent vertices. Similarly, in ∂F, the gradient derivation for model parameters are mostly lazy – their execution can be deferred as long as the gradients of hidden states are derived and propagated in time. Cavs leverages this property and proposes the lazy batching strategy. It defers the execution of all lazy operators in F and ∂F until all batching tasks {Vt}Tt=1 has ﬁnished. It then performs a batched execution of these lazy operators over all vertices of {Gk}Kk=1. These operators includes, but is not limited to, the push operator that are doing memory copy, and the math operators for computing gradients of the model parameters. Lazy batching helps exploit more parallelism for the execution of lazy operators and signiﬁcantly reduces kernel launches. Empirically lazy batching brings 20% overall improvement (see §5.3).
On the other hand, we are unable to “eagerly” batch eager operators, as their execution over some vertices relies on knowing the detailed memory location of all intermediate results in advance, a condition which is not satisﬁed in Cavs where memory is dynamically assigned. To leverage the exhibited parallelization opportunity between eager operators and the operators on the path from gather to scatter (Figure 7), Cavs proposes a streaming strategy that pipelines the execution of these two groups of operators. It allocates two streams, and puts the eager operators on one stream, and the rest (excluding lazy operators) on the other. Hence, independent operators in two streams run in parallel, while for those operators that depend on an eager operator, this dependency is respected by synchronization barriers (see Figure 7). It is also possible to parallelize independent paths from g to s on the graph, but we ﬁnd it does not yield further improvement. Automatic kernel fusion. Since Cavs abstracts out a static dataﬂow graph encoded by F that will be dynamically evaluated elsewhere, we can replace the original F with an op-

8

timized one that runs more efﬁciently, as long as it accepts the same input and produces the same output.
Particularly, given F, before execution, Cavs will run a fusion detector [26] to scan its corresponded dataﬂow graph and report all fuse-able subgraphs therein, i.e. all nodes in a fuse-able subgraph can be fused as a single operator that behaves equivalently but takes less execution time (e.g. with fewer kernel launches and I/O, or faster computation). Currently, we only detect groups of directly linked elementwise operators, such as +, −, ×, ÷, tanh, sigmoid, as shown in Figure 7, and we use a simple union-ﬁnd algorithm to detect the largest possible fuse-able subgraphs. Given a fuseable subgraph, Cavs adopts de facto automatic code generation techniques [12, 40, 42, 43] to generate lower-level kernel codes as an implementation for it. Replacing the original fuse-able subgraphs with fused operators during execution is beneﬁcial in many aspects: (1) it reduces the number of kernel launches; (2) on some devices such as GPUs, kernel fusion transform device memory access into faster device registers access. We empirically report another 20% improvement with automatic kernel fusion (§5.3).
4. Implementation
We implement Cavs as a pluggable C++ library that can be integrated with existing DL frameworks to provide or enhance their support for dynamic NNs. We next brieﬂy discuss implementation details. For clarity, we assume the host framework is composed of three major layers (which is the case for most popular frameworks [1, 5, 38]): (1) a frontend that provides device-agnostic symbolic programming interface; (2) an intermediate layer that implements the core execution logic; (3) a backend with device-speciﬁc kernels for all provided operators. Frontend. Cavs provides a base class GraphSupport in addition to conventional operators and the four proposed APIs (§3.1). Users are required to instantiate it by providing a symbolic vertex function F – therefore an instantiation of GraphSupport represents a single dynamic structure. To construct more complex structures (e.g. encoder-decoder LSTM [49], LRCN [13])), users employ push and pull to connect this dynamic structure to external structures. Intermediate Layer. At the intermediate layer, Cavs will create a unique scope [1], and generates a small dataﬂow graph for each instantiation of GraphSupport, connecting them appropriately with other parts of the model according to user programs. Cavs implements its core runtime logic at this layer, i.e. the scheduler, the memory management, and the graph execution engine, etc. During execution, the execution engine ﬁrst analyzes the received dataﬂow graphs and incorporates described optimization in §3.5. The scheduler then instructs the system to read training samples and their associates graphs (e.g. adjacency matrices). It starts training by submitting batching tasks to the execution engine and assigning memory accordingly.

Backend. Following common practice [1, 18, 38], Cavs puts device-speciﬁc kernel implementations for each supported operator at this layer. Each operator implementation is a function that takes as inputs static tensors and produces static tensors as outputs – therefore the higher-layer logic, i.e. how the computation is scheduled or how the memory is assigned are invincible to this layer. Cavs will reuse the native operator implementations from the host framework, while it provides optimized implementations for the four proposed primitives (gather, scatter, pull, push). Speciﬁcally, gather and pull index different slices of a tensor and puts them together continuously on memory; scatter and push by contrast splits a tensor along its batch dimension, and copy different slices to different places. Cavs implements a customized memcpy kernel for there four operators so that copying multiple slices from (or to) different places is performed within one kernel, to further reduce kernel launches.
5. Evaluation
In this section, we evaluate Cavs on training different NNs across multiple datasets, obtaining the following major ﬁndings: (1) Cavs has little overhead: when training static NNs that can be by nature batched, Cavs demonstrates equal performance with other DL systems. On several NNs with notably difﬁcult-to-batch structures, Cavs outperforms all existing frameworks by a large margin; (2) We conﬁrm the graph construction overhead is substantial in both Fold [34] and dynamic declaration [38], while Cavs bypasses it by loading input graphs through I/O; (3) We verify the effectiveness of our proposed design and optimization via ablation studies, and discuss Cavs’ advantages over other stateof-the-art DL systems for dynamic dataﬂow graphs. Environment. We perform all experiments in this paper on a single machine with an NVIDIA Titan X (GM200) GPU, a 16-core (32 threads) CPU, and CUDA toolkit 8.0 and cuDNN v6 installed. As modern DL models are mostly trained using GPUs, we focus our evaluation on GPUs, but note Cavs’ design and implementation do not reply on a speciﬁc type of device. We borrow the implementations of most mathematical operators from TensorFlow v1.2, while we implement the four proposed operators and other system modules by ourselves. We mainly compare Cavs to TensorFlow v1.2 [1] with XLA [20] and its variant Fold [34], as well as DyNet v2.0 [38] with autobatching [39], as they have reported better performance than other frameworks [17, 57] on dynamic NNs. We focus on metrics for system performance, e.g. the average time to scan one epoch of data. Cavs produces exactly the same numerical results with other frameworks, hence the same per-epoch convergence5. Models and dataset. We experiment on the following models with increasing difﬁculty to batch: (a) Fixed-LSTM language model (LM): a static sequence LSTM with ﬁxed steps
5 The code of Cavs will be released along with a the next major release of the DyNet project: http://dynet.io/.

9

0.02 0.04 0.06 0.08
0.55

0.02 0.04 0.06 0.08
0.3

0.05 0.10 0.15 0.20
10..3379 0.82

Time (x1e3 s) 0.03 0.06 0.09 0.12
00..2173 00..7555

(a) Fixed-LSTM (h = 512) CuDNN Cavs TF DyNet

(b) Var-LSTM (h = 512) Cavs TF DyNet

(c) Tree-FC (h = 512) Cavs Fold DyNet

(d) Tree-LSTM (h = 512) Cavs Fold DyNet

0.12
0.02 0.04 0.06 0.08

0.29
0.02 0.04 0.06 0.08

0.05 0.10 0.15 0.20.016

Time (x1e3 s) 0.03 0.06 0.09 0.12

1 16 32 64 128 256 (e) Fixed-LSTM (b = 64)
CuDNN Cavs TF DyNet

1 16 32 64 128 256 (f) Var-LSTM (b = 64)
Cavs TF DyNet

1 16 32 64 128 256 (g) Tree-FC (b = 64)
Cavs Fold DyNet

1 16 32 64 128 256 (h) Tree-LSTM (b = 64)
Cavs Fold DyNet

64 128 256 512 1024 2048 64 128 256 512 1024 2048 64 128 256 512 1024 2048 64 128 256 512 1024 2048
Figure 8: Comparing ﬁve systems in terms of the averaged time to ﬁnish one epoch of training (lower is better) on four models: Fixed-LSTM, Var-LSTM, Tree-FC and Tree-LSTM. In (a)-(d) we ﬁx the hidden size h and vary the batch size bs, while in (e)-(h) we ﬁx bs and vary h.

for language modeling [48, 49, 64]. We train it using the PTB dataset [54] that contains over 10K different words. We set the number of steps as 64, i.e. at each iteration of training, the model takes a 64-word sentence from the training corpus, and predicts the next word of each word therein. Obviously, the computation can be by nature batched easily, as each sentence has exactly the same size. (b) Var-LSTM LM: that accepts variable-length inputs. At each iteration the model takes a batch of natural sentences with different length from PTB, and predicts the next words; (c) Tree-FC: the benchmarking model used in [34] with a single fullyconnected layer as its cell function. Following the same setting in [34], we train it over synthetic samples generated by their code [53] – each sample is associated with a complete binary tree with 256 leaves (therefore 511 vertices per graph); (d) Tree-LSTM: a family of dynamic NNs widely adopted for text analysis [33, 58]. We implement the binary child-sum Tree-LSTM model in [50], and train it as a sentiment classiﬁer using Stanford sentiment treebank (SST) dataset [46], which contains 8544 training sentences in which the longest sentence has 54 words. Each sentence is associated with a human annotated grammar tree.
5.1 Overall Performance
We ﬁrst verify the viability of our design on the easiestto-batch case: Fixed-LSTM language model. We compare Cavs to the following three strong baselines: (1) CuDNN [9]: a CuDNN-based ﬁxed-step sequence LSTM, which is highly optimized by NVIDIA using handcrafted kernels and stands as the best performed implementation on NVIDIA GPUs; (2) TF: the ofﬁcial implementation of Fixed-LSTM LM in TensorFlow repository [52] based on static declaration; (3) DyNet: we implement a 64-step LSTM in DyNet based on dynamic declaration – we declare a dataﬂow graph per sample, and train with the autobatching [39] enabled; (4) Cavs with batching policy, and all input samples have a same input graph – a 64-node chain. We train the model to converge, and report the average time per epoch in Figure 8(a)(e), where

in (a) we ﬁx the hidden size h of the LSTM unit as 512 and vary the batch size bs, and in (e) we ﬁx bs = 64 and vary h. Empirically, CuDNN performs best in all cases, but note it is highly inﬂexible. Cavs performs slightly better than TF in various settings, verifying that our system has little overhead dealing with fully static graphs, though it is specialized for dynamic ones. We also conclude from Figure 8 that batching is essential for GPU-based DL: bs = 128 is nearly one order of magnitude faster than bs = 1 regardless of used frameworks. For Cavs, the batching policy is 1.7x, 3.8x, 7.0x, 12x, 15x, 25x, 36x faster than the serial policy at bs = 2, 4, 8, 16, 32, 64, 128, respectively.
Next, we experiment with Var-LSTM, the most commonly used RNN for variable-length sequences. We compare the following three implementations (CuDNN-based LSTM cannot handle variable-length inputs): (1) TF: an ofﬁcial TensorFlow implementation based on the dynamic unroll approach described in §2.2; (2) DyNet: an ofﬁcial implementation from DyNet benchmark repository based on dynamic declaration [15]; (3) Cavs: where each input sentence is associated with a chain graph that has number of vertices equal to the number of words. We vary h and bs, and report the results in Figure 8(b)(f), respectively. Although all three systems perform batched computation in different ways, Cavs is constantly 2-3 times faster than TF, and outperforms DyNet by a large margin. Compared to TF, Cavs saves computational resources. TF dynamically unrolls the LSTM unit according to the longest sentence in the current batch, but it cannot prevent unnecessary computation for those sentences that are shorter than the longest one.
We then turn to Tree-FC, a dynamic model for benchmarking. Since vanilla TensorFlow is unable to batch its computation, we compare Cavs to (1) DyNet and (2) Fold, a specialized library built upon TensorFlow for dynamic NNs, with a depth-based dynamic batching strategy. To enable the batching, it however needs to preprocess the input graphs, translate them into intermediate representations and pass them to lower-level TensorFlow control ﬂow engine for ex-

10

ecution. We report the results in Figure 8(c)(g) with varying bs and h, respectively. For all systems, we allocate a single CPU thread for graph preprocessing or construction. Cavs shows at least an order of magnitude speedups than Fold and DyNet at (h ≤ 512). Because the size of the synthetic trees is large, one major advantage of Cavs over them is the alleviation of substantial graph preprocessing/construction overhead. With a single CPU thread, Fold takes even more time on graph preprocessing than computation (§5.3).
Finally, we compare three frameworks on Tree-LSTM in Figure 8(d)(h): Cavs is 8-10x faster than Fold, and consistently outperforms DyNet. One difference in this experiment is that we allocate as many CPU threads as possible (32 on our machine) to accelerate graph preprocessing for Fold, otherwise it will take much longer time. Further, we note DyNet performs much better here than on Tree-FC, as the size of the input graphs in SST (maximally 52 leaves) is much smaller than the synthetic ones (256 leaves each) in Tree-FC experiments. We observe DyNet needs more time on graph construction for large input graphs, and DyNet’s dynamic batching is less effective on larger input graphs, as it has to perform frequent memory checks to support its dynamic batching, which we will discuss in §5.3.
5.2 Graph Construction and Computation
In this section, we investigate the graph construction overhead in Fold and DyNet. To batch computation of different graphs, Fold analyzes the input graphs to recognize batchable dynamic operations, then translates them into intermediate instructions, with which, TensorFlow generates appropriate control ﬂow graphs for evaluation – we will treat the overhead caused in both steps as Fold’s graph construction overhead. DyNet, as a typical dynamic declaration framework, has to construct as many dataﬂow graphs as the number of samples. Though DyNet has optimized its graph construction to make it lightweight, the overhead still grows with the training set and the size of input graphs. By contrast, Cavs takes constant time to construct a small dataﬂow graph encoded by F, then reads input graphs through I/O. To quantify the overhead, we separate the graph construction from computation, and visualize in Figure 9(a) how it changes with the average number of leaves (graph size) of input graphs on training Tree-FC, with ﬁxed bs = 64, h = 512. We compare (1) Cavs (2) Fold-1 which is Fold with one graph processing thread and (3) DyNet. We plot for one epoch, both the (averaged) absolute time for graph construction and it percentage of the overall time. Clearly we ﬁnd that all three systems take increasingly more time when the size of the input graphs grows, but Cavs, which loads graphs through I/O, causes the least overhead at all settings. In terms of the relative time, Fold unfortunately wastes 50% at 32 leaves, and 80% when the tree has 1024 leaves, while DyNet and Cavs take only 10% and 20%, respectively.
We also wonder how the overhead is related with batch size when there is ﬁxed computational workload. We report

Time (s) 4 8 12 16 20 40 60 80 0.6 1.2 1.8 2.4 20 40 60 80 Percentage(%)

(a) Tree-FC (bs = 64, h = 512)

(b) Tree-LSTM (h = 512)

Time Cavs Fold-32 DyNet

Percentage Cavs Fold-1 Fold-32 DyNet

32 64 128 256 512 1024 Num of Leaves

1 16 32 64 128 256 Batch Size (bs)

Figure 9: The averaged graph construction overhead per epoch when training (a) Tree-FC with different size of input graphs (b) Tree-LSTM with different batch size. The curves show absolute time in second (left y-axis), and the bar graphs show its percentage of the overall time (right y-axis).

in Figure 9(b) the same metrics when training Tree-LSTM with varying bs. We add another baseline Fold-32 with 32 threads for Fold’s graph preprocessing. As Fold-1 takes much longer time than others, we report its time at bs = 1, 16, 32, 64, 128, 256 here (instead of showing in Figure 9): 1.1, 7.14, 31.35, 40.1, 46.13, 48.77. Except bs = 1, all three systems (except Fold-1) take almost constant time for graph construction in one epoch, regardless of bs, while Fold-32 and DyNet take similar time, but Cavs takes 20x less. Nevertheless, at the percentage scale, increasing bs makes this overhead more prominent, because larger batch size yields improved computational efﬁciency, therefore less time to ﬁnish one epoch. This, from one perspective, reﬂects that the graph construction is a main obstacle that grows with the number of training samples and prevents the efﬁcient training of dynamic NNs in existing frameworks, while Cavs successfully overcomes this barrier through its design.
Apart from the graph construction we report in Table 1 the computation-only time – Cavs demonstrates maximally 5.4x/9.7x and 7.2x/2.4x speedups over Fold/DyNet on Tree-FC and Tree-LSTM, respectively. Besides less system overhead, the advantages stem from two main sources: an optimized graph execution engine, and a better-suited memory management strategy, which we investigate next.

# leaves 32 64 128 256 512 1024

Comp. time (s)
0.58 / 3.1 / 4.1 1.1 / 3.9 / 8.0 2.0 / 6.2 / 16.0 3.9 / 10.6 / 33.7 8.0 / 18.5 / 70.6 15.8 / 32.4 / 153

Speedup
5.4x / 7.1x 3.7x / 7.5x 3.0x / 7.9x 2.7x / 8.7x 2.3x / 8.9x 2.1x / 9.7x

bs Comp. time (s)
1 76.2 / 550 / 61.6 16 9.80 / 69 / 12 32 6.15 / 43 / 9.9 64 4.1 / 29 / 7.4 128 2.9 / 20.5 / 5.9 256 2.3 / 15.8 / 5.4

Speedup
7.2x / 0.8x 7.0x / 1.2x 7.0x / 1.6x 7.2x / 1.8x 7.1x / 2.0x 7.0x / 2.4x

Table 1: The computation time in second (Cavs/Fold/DyNet) and the speedup (Cavs vs Fold/DyNet) for training one epoch on Tree-FC with varying size of the input trees (left part), and on Tree-LSTM with varying batch size (right part).

5.3 Ablation Studies
Graph Execution Engine. To reveal how much each optimization in §3.5 contributes to the ﬁnal performance, we disable lazy batching, fusion and streaming in Cavs and set this conﬁguration as a baseline (speedup = 1). We then

11

Speedup (x)

(a) Fixed-LSTM (bs = 64)
1.5 Lazy-batching
Fusion
1.4 Streamming

(b) Tree-LSTM (bs = 64)
1.5 Lazy-batching
Fusion
1.4 Streamming

1.3

1.3

1.2

1.2

1.1

1.1

1.0 64 128 256 512 1024 2048 1.0 64 128 256 512 1024 2048

Hidden Size (h)

Hidden Size (h)

Figure 10: Improvement of each optimization strategy on execution engine over a baseline conﬁguration (speedup = 1).

turn on one optimization at a time and record how much speedup it brings. We train Fixed-LSTM and Tree-LSTM, and report the averaged speedups one computation-only time in one epoch over the baseline conﬁguration in Figure 10, with bs = 64 but varying h. Lazy batching and fusion consistently deliver nontrivial improvement – lazy batching is more beneﬁcial with a larger h while fusion is more effective at smaller h, which are expected: lazy batching mainly parallelizes matrix-wise operations (e.g. matmul) commonly with O(h2) our higher complexity, while fusion mostly works on elementwise operations with a linear complexity with h [25].
Streaming, compared to the other strategies, is less effective on Tree-LSTM than on Fixed-LSTM, as we have found the depth of the input trees in SST exhibit high variance, i.e. some trees are much deeper than others. In this case, many batching tasks only have one vertex to be evaluated. The computation is highly fragmented and the efﬁciency is bounded by kernel launching latency. Lazy batching and fusion still help as they both reduce kernel launches (§3.5). Streaming, which tries to pipeline multiple kernels, can hardly yield obvious improvement. Memory Management. Cavs’ performance advantage also credits to its better memory management that reduces memory movements while guarantees memory continuity.
Quantitatively, it is difﬁcult to compare Cavs to Fold, as Fold relies on TensorFlow where memory management is highly coupled with other system aspects. Qualitatively, we ﬁnd Cavs requires less memory movement (e.g. memcpy) during dynamic batching. Built upon the tf while operator, whenever Fold performs depth-based batching at d, it has to move all the contents of nodes in the dataﬂow graphs at depth d − 1 to a desired location, as the control ﬂow does not support cross-depth memory indexing. This results in redundant memcpy, especially when the graphs are highly skewed. By contrast, Cavs only copies contents that are necessary to the batching task. DyNet has a specialized memory management strategy for dynamic NNs. Compared to Cavs, it however suffers substantial overhead caused by repeated checks of the memory continuity – whenever DyNet wants to batch operators with same signatures, it checks whether their inputs are continuous on memory [39]. The checking overhead increases with bs and is more prominent on GPUs. Thanks to the simplicity of both systems, we are able to proﬁle

Memory operations

bs

(s) (Cavs / DyNet)

Train

Inference

16 1.14 / 1.33 32 0.67 / 0.87 64 0.39 / 0.6 128 0.25 / 0.44 256 0.17 / 0.44

0.6 / 1.33 0.35 / 0.87 0.21 / 0.6 0.13 / 0.44 0.09 / 0.44

Computation (s)

(Cavs / DyNet)

Train

Inference

9.8 / 12 6.1 / 9.8 4.0 / 7.4 2.9 / 5.9 2.3 / 5.4

2.9 / 8.53 1.9 / 5.35 1.3 / 3.48 0.97 / 2.52 0.77 / 2.58

Table 2: The breakdowns of the average time per epoch on memoryrelated operations and computation. We compare Cavs to DyNet on Tree-LSTM on training and inference, with varying bs.

the memory-related overhead during both training and inference, and separate it from computation. We compare them on TreeLSTM, and report the breakdown time per epoch in Table 2 under different bs. We observe the improvement is signiﬁcant (2x - 3x) at larger bs, especially during inference where DyNet has its continuity checks concentrated. Others. Despite system advantages, we also try to investigate whether Cavs, as an interface, simpliﬁes user programs (though we do not claim as a contribution). We compare Cavs to Fold and DyNet in terms of the lines of code (LoC) needed to create a few notable dynamic NNs, including Var-LSTM, Tree-LSTM, and multi-layer sequence LSTM, with Python as the host language. If only for model declaration, Fold in general has 3.5x more LoC than Cavs, while DyNet has slightly more LoC than Cavs because of the function to repeatedly declare graphs.
6. Related Work
In addition to §2, we discuss some other related works. Graph execution optimization. Optimizing the execution of DL dataﬂow graphs comes in mainly two ways: better operator implementations or optimizing the execution of (sub)graphs. As Cavs is implemented as a plug-in to enhance existing frameworks, it beneﬁts from any improved implementations of speciﬁc operators (e.g. cuDNN) [9, 22, 24, 29]. In addition, Cavs has optimized implementations for its proposed four primitives (gather/scatter/pull/push). At the graph level, a variety of well-developed techniques from other areas, such as kernel fusion, common subexpression elimination, and constant folding, have been adapted and applied on speeding the computation of DL dataﬂow graphs [1, 8, 18, 20]. They are usually incorporated after the graph declaration, but before the execution, so that the actual computation is conducted on an optimized graph other than the original one. However, these graph optimizations are less beneﬁcial in dynamic declaration, in which the graph changes with the sample, and needs to be re-processed and re-optimized every iteration, and may cause substantial overhead. On the contrary, Cavs separates the static vertex function from the dynamic-varying input graph, so it beneﬁts from most of the aforementioned optimizations, as we have shown in §5.3. We draw insights from these strategies and reﬂect them in Cavs’ execution engine. We further propose lazy batching and streaming to exploit more parallelism exposed by our programming model.

12

Vertex-centric models. The vertex-centric programming model has been extensively developed in the area of graph computing [7, 19, 35, 47]. Cavs draws insights from the GAS model [19], but faces totally different challenges in system and interface design, such as expressiveness, scheduling for batched execution of different graphs, guaranteeing the memory continuity, etc., as we have discussed in §3.1.
7. Conclusion
We present Cavs as a vertex-centric programming interface as well as an efﬁcient system for dynamic deep learning. Cavs represents a dynamic NN structure as static vertex functions and dynamic input graphs. It provides four novel APIs to allow users to easily program these types of NNs. With designed scheduling policy, memory management strategy, and graph execution optimizations, Cavs avoids substantial graph construction overhead suffered by dynamic declaration, and reports new state-of-the-art system performance for various notable dynamic NN architectures.
References
[1] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A., DEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD, M., ET AL. Tensorﬂow: A system for large-scale machine learning. arXiv preprint arXiv:1605.08695 (2016).
[2] ANDREAS, J., ROHRBACH, M., DARRELL, T., AND KLEIN, D. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 39–48.
[3] BAHDANAU, D., CHO, K., AND BENGIO, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).
[4] BARTHOLOMEW-BIGGS, M., BROWN, S., CHRISTIANSON, B., AND DIXON, L. Automatic differentiation of algorithms. Journal of Computational and Applied Mathematics 124, 1 (2000), 171–190.
[5] BERGSTRA, J., BASTIEN, F., BREULEUX, O., LAMBLIN, P., PASCANU, R., DELALLEAU, O., DESJARDINS, G., WARDEFARLEY, D., GOODFELLOW, I. J., BERGERON, A., AND BENGIO, Y. Theano: Deep Learning on GPUs with Python. In NIPSW (2011).
[6] BUCKMAN, J., BALLESTEROS, M., AND DYER, C. Transition-based dependency parsing with heuristic backtracking. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing; 2016 Nov 1-5; Austin, Texas, USA. Stroudsburg (USA): Association for Computational Linguistics (ACL); 2016. p. 2313-18. (2016), ACL (Association for Computational Linguistics).
[7] CHEN, R., SHI, J., CHEN, Y., AND CHEN, H. Powerlyra: Differentiated graph computation and partitioning on skewed graphs. In Proceedings of the Tenth European Conference on Computer Systems (2015), ACM, p. 1.
[8] CHEN, T., LI, M., LI, Y., LIN, M., WANG, N., WANG, M., XIAO, T., XU, B., ZHANG, C., AND ZHANG, Z. Mxnet: A ﬂexible and efﬁcient machine learning library for heteroge-

neous distributed systems. arXiv preprint arXiv:1512.01274 (2015).

[9] CHETLUR, S., WOOLLEY, C., VANDERMERSCH, P., COHEN, J., TRAN, J., CATANZARO, B., AND SHELHAMER, E. cudnn: Efﬁcient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014).

[10] CHUNG, J., GULCEHRE, C., CHO, K., AND BENGIO, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).

[11] CUI, H., ZHANG, H., GANGER, G. R., GIBBONS, P. B., AND XING, E. P. Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server. In Proceedings of the Eleventh European Conference on Computer Systems (2016), ACM, p. 4.

[12] DAVE, C., BAE, H., MIN, S.-J., LEE, S., EIGENMANN, R., AND MIDKIFF, S. Cetus: A source-to-source compiler infrastructure for multicores. Computer 42, 12 (2009).

[13] DONAHUE, J., ANNE HENDRICKS, L., GUADARRAMA, S., ROHRBACH, M., VENUGOPALAN, S., SAENKO, K., AND DARRELL, T. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition (2015), pp. 2625–2634.

[14] DYER, C., BALLESTEROS, M., LING, W., MATTHEWS, A., AND SMITH, N. A. Transition-based dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075 (2015).

[15] DYNET VARIABLE LENGTH LSTM. https://github. com/neulab/dynet-benchmark.

[16] ELMAN, J. L. Finding structure in time. Cognitive science 14, 2 (1990), 179–211.

[17] FACEBOOK. http://pytorch.org/.

[18] FACEBOOK OPEN SOURCE. Caffe2 is a lightweight, modular, and scalable deep learning framework. https://github. com/caffe2/caffe2, 2017.

[19] GONZALEZ, J. E., LOW, Y., GU, H., BICKSON, D., AND GUESTRIN, C. Powergraph: Distributed graph-parallel computation on natural graphs.

[20] GOOGLE TENSORFLOW XLA. tensorflow.org/performance/xla/.

https://www.

[21] GORMLEY, M. R., DREDZE, M., AND EISNER, J. Approximation-aware dependency parsing by belief propagation. arXiv preprint arXiv:1508.02375 (2015).
[22] GRAVE, E´ ., JOULIN, A., CISSE´ , M., GRANGIER, D., AND JE´ GOU, H. Efﬁcient softmax approximation for gpus. arXiv preprint arXiv:1609.04309 (2016).

[23] GRAVES, A., FERNA´ NDEZ, S., GOMEZ, F., AND SCHMIDHUBER, J. Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks, 2006.

[24] GUENNEBAUD, G., JACOB, B., ET AL. http://eigen.tuxfamily.org, 2010.

Eigen v3.

[25] GUSTAFSON, J. L. Reevaluating amdahl’s law. Communications of the ACM 31, 5 (1988), 532–533.

13

[26] GYSI, T., OSUNA, C., FUHRER, O., BIANCO, M., AND SCHULTHESS, T. C. Stella: A domain-speciﬁc tool for structured grid methods in weather and climate models. In High Performance Computing, Networking, Storage and Analysis, 2015 SC-International Conference for (2015), IEEE, pp. 1– 12.

[27] HINTON, G. E., AND SALAKHUTDINOV, R. R. Reducing the dimensionality of data with neural networks. science 313, 5786 (2006), 504–507.

[28] HOCHREITER, S., AND SCHMIDHUBER, J. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.

[29] INTEL OPEN SOURCE TECHNOLOGY CENTER. Intel(r) math kernel library for deep neural networks (intel(r) mkl-dnn). https://github.com/01org/mkl-dnn, 2017.

[30] JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG, J., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093 (2014).

[31] KONG, L., DYER, C., AND SMITH, N. A. Segmental recurrent neural networks. arXiv preprint arXiv:1511.06018 (2015).

[32] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. In NIPS (2012).

[33] LIANG, X., SHEN, X., FENG, J., LIN, L., AND YAN, S. Semantic object parsing with graph lstm. In European Conference on Computer Vision (2016), Springer, pp. 125–143.

[34] LOOKS, M., HERRESHOFF, M., HUTCHINS, D., AND NORVIG, P. Deep learning with dynamic computation graphs. arXiv preprint arXiv:1702.02181 (2017).

[35] MALEWICZ, G., AUSTERN, M. H., BIK, A. J., DEHNERT, J. C., HORN, I., LEISER, N., AND CZAJKOWSKI, G. Pregel: a system for large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data (2010), ACM, pp. 135–146.

[36] MIKOLOV, T., CHEN, K., CORRADO, G., AND DEAN, J. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).

[37] MURRAY, D. G., MCSHERRY, F., ISAACS, R., ISARD, M., BARHAM, P., AND ABADI, M. Naiad: a timely dataﬂow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (2013), ACM, pp. 439–455.

[38] NEUBIG, G., DYER, C., GOLDBERG, Y., MATTHEWS, A., AMMAR, W., ANASTASOPOULOS, A., BALLESTEROS, M., CHIANG, D., CLOTHIAUX, D., COHN, T., ET AL. Dynet: The dynamic neural network toolkit. arXiv preprint arXiv:1701.03980 (2017).

[39] NEUBIG, G., GOLDBERG, Y., AND DYER, C. On-the-ﬂy operation batching in dynamic computation graphs. arXiv preprint arXiv:1705.07860 (2017).

[40] NVIDIA.

http://docs.nvidia.com/cuda/nvrtc/

index.html.

[41] PANG, B., LEE, L., AND VAITHYANATHAN, S. Thumbs up?: sentiment classiﬁcation using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods

in natural language processing-Volume 10 (2002), Association for Computational Linguistics, pp. 79–86.
[42] QUINLAN, D. Rose: Compiler support for object-oriented frameworks. Parallel Processing Letters 10, 02n03, 215–226.
[43] RAGAN-KELLEY, J., BARNES, C., ADAMS, A., PARIS, S., DURAND, F., AND AMARASINGHE, S. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. ACM SIGPLAN Notices 48, 6 (2013), 519–530.
[44] RUMELHART, D. E., HINTON, G. E., WILLIAMS, R. J., ET AL. Learning representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[45] SOCHER, R., LIN, C. C., MANNING, C., AND NG, A. Y. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11) (2011), pp. 129– 136.
[46] SOCHER, R., PERELYGIN, A., WU, J., CHUANG, J., MANNING, C. D., NG, A., AND POTTS, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing (2013), pp. 1631–1642.
[47] SUNDARAM, N., SATISH, N., PATWARY, M. M. A., DULLOOR, S. R., ANDERSON, M. J., VADLAMUDI, S. G., DAS, D., AND DUBEY, P. Graphmat: High performance graph analytics made productive. Proceedings of the VLDB Endowment 8, 11 (2015), 1214–1225.
[48] SUNDERMEYER, M., SCHLU¨ TER, R., AND NEY, H. Lstm neural networks for language modeling. In Thirteenth Annual Conference of the International Speech Communication Association (2012).
[49] SUTSKEVER, I., VINYALS, O., AND LE, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems (2014), pp. 3104– 3112.
[50] TAI, K. S., SOCHER, R., AND MANNING, C. D. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075 (2015).
[51] TAN, M., SANTOS, C. D., XIANG, B., AND ZHOU, B. Lstmbased deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108 (2015).
[52] TENSORFLOW FIXED-SIZED LSTM LANGUAGE MODEL. https://github.com/tensorflow/models/blob/ master/tutorials/rnn/ptb/ptb_word_lm.py.
[53] TENSORFLOW FOLD BENCHMARK CODE. https: //github.com/tensorflow/fold/tree/master/ tensorflow_fold/loom/benchmarks.
[54] THE PENN TREE BANK (PTB) DATASET. http://www. fit.vutbr.cz/~imikolov/rnnlm/simple-examples. tgz.
[55] TIAN, Y., BALMIN, A., CORSTEN, S. A., TATIKONDA, S., AND MCPHERSON, J. From think like a vertex to think like a graph. Proceedings of the VLDB Endowment 7, 3 (2013), 193–204.

14

[56] TOKUI, S., OONO, K., HIDO, S., AND CLAYTON, J. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS) (2015), vol. 5.
[57] TOKUI, S., OONO, K., HIDO, S., AND CLAYTON, J. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) (2015).
[58] VINYALS, O., KAISER, Ł., KOO, T., PETROV, S., SUTSKEVER, I., AND HINTON, G. Grammar as a foreign language. In Advances in Neural Information Processing Systems (2015), pp. 2773–2781.
[59] WALT, S. V. D., COLBERT, S. C., AND VAROQUAUX, G. The numpy array: a structure for efﬁcient numerical computation. Computing in Science & Engineering 13, 2 (2011), 22–30.
[60] XU, K., BA, J., KIROS, R., CHO, K., COURVILLE, A., SALAKHUDINOV, R., ZEMEL, R., AND BENGIO, Y. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (2015), pp. 2048–2057.
[61] YAN, Z., ZHANG, H., JAGADEESH, V., DECOSTE, D., DI, W., AND PIRAMUTHU, R. Hd-cnn: Hierarchical deep convolutional neural network for image classiﬁcation. ICCV (2015).
[62] YAN, Z., ZHANG, H., JIA, Y., BREUEL, T., AND YU, Y. Combining the best of convolutional layers and recurrent layers: A hybrid network for semantic segmentation. arXiv preprint arXiv:1603.04871 (2016).
[63] YAN, Z., ZHANG, H., WANG, B., PARIS, S., AND YU, Y. Automatic photo adjustment using deep neural networks. ACM Transactions on Graphics (TOG) 35, 2 (2016), 11.
[64] ZAREMBA, W., SUTSKEVER, I., AND VINYALS, O. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329 (2014).
[65] ZHANG, H., HU, Z., WEI, J., XIE, P., KIM, G., HO, Q., AND XING, E. Poseidon: A system architecture for efﬁcient gpu-based deep learning on multiple machines. arXiv preprint arXiv:1512.06216 (2015).
[66] ZHANG, H., ZHENG, Z., XU, S., DAI, W., HO, Q., LIANG, X., HU, Z., WEI, J., XIE, P., AND XING, E. P. Poseidon: An efﬁcient communication architecture for distributed deep learning on GPU clusters. In 2017 USENIX Annual Technical Conference (USENIX ATC 17) (Santa Clara, CA, 2017), USENIX Association, pp. 181–193.
[67] ZHENG, S., JAYASUMANA, S., ROMERA-PAREDES, B., VINEET, V., SU, Z., DU, D., HUANG, C., AND TORR, P. H. Conditional random ﬁelds as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision (2015), pp. 1529–1537.
15

