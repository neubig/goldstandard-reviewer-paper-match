On Learning Text Style Transfer with Direct Rewards
Yixin Liu, Graham Neubig, John Wieting Carnegie Mellon University
{yixinl2,gneubig,jwieting}@cs.cmu.edu

arXiv:2010.12771v2 [cs.CL] 13 May 2021

Abstract
In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the styletransferred outputs. In particular, we leverage semantic similarity metrics originally used for ﬁne-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efﬁcient strategies of using these metrics for training. The experimental results show that our model provides signiﬁcant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies.1
1 Introduction
Text style transfer aims to convert an input text into another generated text with a different style but the same basic semantics as the input. One major challenge in this setting is that many style transfer tasks lack parallel corpora, since the absence of human references makes it impossible to train the text style transfer models using maximum likelihood estimation (MLE), which aims to maximize the predicted likelihood of the references. As a result, some of the earliest work (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018) on unsupervised text style transfer proposed training algorithms that are still based on MLE by formulating the style transfer models as auto-encoders optimized with reconstruction loss. Speciﬁcally, during training the model is tasked to generate a style-agnostic encoding and reconstruct the input text based on this encoding with style-speciﬁc embeddings or decoders. During inference, the model aims to transfer the source
1Code and data are available at: https://github. com/yixinL7/Direct-Style-Transfer

text style using the target style information. While these methods have seen empirical success, they face the inherent difﬁculty of coming up with a style-agnostic but content-preserving encoding – this is a non-trivial task and failure at this ﬁrst step will diminish style transfer accuracy and content preservation of the ﬁnal output.
Another line of work (Xu et al., 2018; Pang and Gimpel, 2019; Luo et al., 2019) proposes training algorithms based on rewards related to the automatic evaluation metrics, which can assess the model performance more directly during training. This approach is conceptually similar to training algorithms that optimize models using rewards related to the corresponding evaluation metrics for other NLP tasks, such as machine translation (Shen et al., 2016; Wieting et al., 2019a) or text summarization (Paulus et al., 2018; Li et al., 2019). As for unsupervised style transfer, the widely used automatic metrics mainly attend to three desiderata: (1) style transfer accuracy – the generated sentence must be in the target style, commonly measured by the accuracy of a style classiﬁer applied to the transferred text, (2) ﬂuency – the generated text must be grammatically correct and natural, commonly measured by the perplexity of a language model and (3) content preservation – the semantics need to be preserved between the source and target, commonly measured by the BLEU score between the system outputs and source texts. Since these automatic metrics only require the system outputs and source texts, they can be used as rewards for training. Moreover, the two lines of approaches can be used together, and previous work (Yang et al., 2018; John et al., 2019; Madaan et al., 2020) proposed methods which use the auto-encoders as the backbone augmented with task-speciﬁc rewards. In particular, the style transfer accuracy reward is used by most of the recent work.
However, reward-based training algorithms still have their limitations, and in this paper we aim

to identify and address the bottlenecks of these methods. Speciﬁcally, we focus on two problems: (1) the difﬁculty of designing an efﬁcient reward for content preservation, (2) the lack of robustness of the existing automatic evaluation metrics.
Content preservation is more difﬁcult to measure compared to style transfer accuracy and ﬂuency because it needs to consider the overlap in the semantics between the source text and system outputs. While using BLEU score between the source text and system output would be a direct solution (Xu et al., 2018), this approach has an inherent limitation in that n-gram based metrics such as BLEU are sensitive to lexical differences and will penalize modiﬁcations that are necessary for transferring text style. In fact, previous work has proposed various different proxy rewards for content preservation. One of the most popular methods is the cycle-consistency loss (Luo et al., 2019; Dai et al., 2019; Pang and Gimpel, 2019), which introduces a round-trip generation process, where the model generates an output in the target style, and the ability of a reconstruction model to re-generate the original text is used as a proxy for content preservation. While this method is more tolerant to lexical differences, the correlation between the reconstruction loss and content preservation can be weak.
Therefore, we aim to design a reward for content preservation which can directly assess the semantic similarity between the system outputs and input texts. Speciﬁcally, we note that models of semantic similarity are widely studied (Wieting et al., 2016; Sharma et al., 2017; Pagliardini et al., 2018; Zhang* et al., 2020), and we can leverage these methods to directly calculate the similarity between the system outputs and input texts. This renders our method applicable for even unsupervised settings where no human references are available.
Another key challenge for reward-based training algorithms is that the existing automatic evaluation metrics are not well-correlated with human evaluation (Li et al., 2018). It poses general risks to the work in this ﬁeld with respect to model training and evaluation since these metrics are widely used. An important observation we made from our experiments is that style transfer models can exploit the weaknesses of the automatic metrics. They do this by making minimal changes to the input texts which are enough to trick the classiﬁer used for style transfer accuracy while achieving high content preservation and ﬂuency scores due to the high

lexical similarity with the input texts. Upon identifying this risk, we re-visit and propose several strategies that serve as auxiliary regularization on the style transfer models, effectively mitigating the problem discussed above.
We empirically show that our proposed reward functions can provide signiﬁcant gains in both automatic and human evaluation over strong baselines from the literature. In addition, the problems we identify with existing automatic evaluation metrics suggest that the automatic metrics need to be used with caution either for model training or evaluation in order to make it truthfully reﬂect human evaluation.
2 Methods
2.1 Overview
Data for unsupervised text style transfer can be deﬁned as
D = {(x(1), s(1)), ..., (x(i), s(i)), ..., (x(n), s(n))},
where x(i) denotes the text and s(i) denotes the corresponding style label. The objective of the task is to generate (via a generator g) the output with the target style conditioned on s while preserving most of the semantics of the source x. In other words, xˆ = g(x, s) should have style s and the semantics of x. We deﬁne the style as a binary attribute such that s ∈ {0, 1}, however, it can be easily extended to a multi-class setting.
2.2 Generator
For our generator, we ﬁne-tune a large-scale language model GPT-2 (Radford et al., 2019). GPT-2 is pre-trained on large corpora and can be ﬁnetuned to generate ﬂuent and coherent outputs for a variety of language generation tasks (Wolf et al., 2019). Since GPT-2 is a unidirectional language model, we reformulate the conditional generation task as a sequence completion task. Namely, as input to the generator, we concatenate the original sentence with a special token which indicates the target style. The sequence following the style token is our output.
2.3 Reward Functions
We use four reward functions to control the quality of the system outputs. The quality of the outputs is assessed in three ways: style transfer accuracy, content preservation, and ﬂuency. We attend to each of these factors with their respective rewards.

Figure 1: SIM Loss v.s. Cycle-Consistency Loss

Here we denote the input text x having style s by xs, and denote the output by x˜s, i.e., x˜s = g(xs, 1 − s).
Rewards for Style Transfer Accuracy We use a style classiﬁer to provide the supervision signal to the generator with respect to the style transfer accuracy. The min-max game between the generator g and the classiﬁer fcls is:

min max Exs[log(1 − fcls(g(xs, 1 − s), 1 − s))]
θg θfcls
+ Exs[log fcls(xs, s) + log(1 − fcls(xs, 1 − s))]. (1)

The style transfer accuracy reward for the generator is the log-likelihood of the output being labeled as the target style:

rcls(x˜s) = log(fcls(x˜s, 1 − s)).

(2)

Following prior work, we use the CNN-based classiﬁer (Kim, 2014) fcls, which takes both the sentence and the style label as input and its objective is to predict the likelihood of the sentence being coherent to the given style.
Rewards for Content Preservation To ensure that the system outputs still preserve the basic semantics of the source sentences, we use the pretrained SIM model introduced in Wieting et al. (2019b,a) to measure the semantic similarity between the source sentences and system outputs. The SIM score for a sentence pair is the cosine similarity of its sentence representations. These representations are constructed by averaging sub-word embeddings. Compared to the cycle-consistency loss (Luo et al., 2019; Dai et al., 2019; Pang and

Gimpel, 2019), our method is more direct since it doesn’t require a second-pass generation. It also has advantages over n-gram based metrics like BLEU (Papineni et al., 2002) since it is more robust to lexical changes and can provide smoother rewards.
In Wieting et al. (2019a), SIM is augmented with a length penalty to help control the length of the generated text. We use their entire model, SIMILE, as the content preservation reward,
rsim(x˜s) = LP(xs, x˜s)αSIM(xs, x˜s), (3)

where

LP(r, h) = e1− m mainx((||rr||,,||hh||)) ,

(4)

and α is an exponential term to control the weight of the length penalty, which is set to 0.25.
We also use the cycle-consistency loss Lcyc to bootstrap the training:

Lcyc(θg) = Exs[− log(pg(xs|g(xs, 1 − s), s))]. (5)
Here, pg is the likelihood assigned by the generator g. This introduces two generation passes, i.e., x˜s = g(x, 1 − s) and x¯s = g(x˜s, s) while SIM reward only requires one generation pass, as illustrated in
Fig. 1.

Rewards for Fluency Style transfer accuracy rewards and content preservation rewards do not have a signiﬁcant effect on the ﬂuency of the outputs. Therefore, we again use the pre-trained GPT-2 model, but as a reward this time. To encourage the outputs to be as ﬂuent as the source sentences, we deﬁne the ﬂuency reward as the difference of the perplexity between the system outputs and source sentences:

rlang(x˜s) = ppl(xs) − ppl(x˜s).

(6)

Here, ppl denotes the length-normalized perplexity assigned by the language model ﬁne-tuned on the training set.
As will be further discussed in Section 3.3, we found that using the rewards mentioned above can still result in unnatural outputs. Therefore, we additionally use a LSTM-based (Hochreiter and Schmidhuber, 1997) discriminator fadv to provide a naturalness reward, whose job is to discriminate the system outputs and the real sentences, i.e., an adversarial discriminator. It constructs a min-max game with the generator:

min max Exs[log(1 − fadv(g(xs, 1 − s)))]

θg θfadv

(7)

+ Exs [log(fadv(xs))].

The naturalness reward is the log-likelihood of the outputs being classiﬁed as real sentences:

radv(x˜s) = log(fadv(x˜s)).

(8)

2.4 Learning The ﬁnal corresponding loss term is:

1N

(i)

L∗(θg) = − N r∗(x˜s ).

(9)

i=1

Here, N is the number of samples in the dataset. To train the model, we use the weighted average of the losses deﬁned in the previous section:

L(θg) = λclsLcls(θg) + λadvLadv(θg) + λsimLsim(θg) + λlangLlang(θg) (10) + λrecLrec(θg).

where λ denotes the weight of the corresponding term. The setting of λ is chosen to make the training stable and have balanced style transfer accuracy and content preservation performance on the development set. Lrec is the reconstruction loss, i.e.,

Lrec(θg) = Exs[− log(pg(xs|xs, s))]. (11)
We follow a two-stage training procedure. We ﬁrst use the cycle-consistency loss Lcyc to bootstrap the training and then ﬁne-tune the model with the rewards we introduced above to improve the output quality.
In the bootstrap stage, the objective function is
Lboot(θg) = λcycLcyc(θg) + λclsLcls(θg) (12)
+ λrecLrec(θg)
We select the checkpoint with the highest mean of the style transfer accuracy and BLEU on the development set as the starting point for the second training stage.
In the second stage, the generator is optimized with Eq. 10. The classiﬁer fcls for Lcls is pretrained and the language model for Llang is ﬁnetuned on the training set. During training, the discriminator fadv for Ladv is trained against the generator. fcls is ﬁxed when trained on some datasets, while it is trained against the generator on others. We select the checkpoint that has the style transfer accuracy and BLEU score similar to that from the ﬁrst stage and the lowest perplexity on the development set.

Lastly, since gradients can not be propagated through the discrete samples, we use two approaches to circumvent this problem. For the content preservation reward (Eq. 3) and ﬂuency reward (Eq. 6), we use the REINFORCE (Williams, 1992) algorithm to optimize the model,

∇θg Ex˜s∼pg(x˜s)[r(x˜s)] (13) = Ex˜s∼pg(x˜s)[∇θg log pg(x˜s)r(x˜s)]

We approximate the expectation by greedy decod-

ing and the log-likelihood is normalized by se-

quence

length,

i.e.,

1 L

L i=1

log

pg (w˜i ),

where

w˜i

denotes the i-th token of x˜s and L is sequence

length. For the style transfer accuracy reward

(Eq. 2) and naturalness reward (Eq. 8), we use

a different approach to generate a continuous ap-

proximation of the discrete tokens, which allows

gradients to be back-propagated to the generator.

Namely, taking the style classiﬁer fcls as an exam-

ple, we use the distribution pi of each token pro-

duced by the generator as the input of the classiﬁer.

This distribution is then multiplied by the classiﬁer’s word embedding matrix W embed to obtain a

weighted average of word embeddings:

wˆi = piW embed

(14)

Then, the classiﬁer takes the sequence of wˆi as its input. We chose this method because it provides a token-level supervision signal to the generator, while the REINFORCE algorithm provides sentence-level signals.

3 Experiments
3.1 Datasets
We evaluate our approach on three datasets for sentiment transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset provided by Li et al. (2018),2 and the IMDb movie review dataset provided by Dai et al. (2019).3
We also evaluate our methods on a formality style transfer dataset, Grammarly’s Yahoo Answers Formality Corpus (GYAFC),4 introduced in Rao and Tetreault (2018). Although it is a parallel corpus, we treat it as an unaligned corpus in our experiments. In order to compare to previous work,
2https://github.com/lijuncen/ Sentiment-and-Style-Transfer
3https://github.com/fastnlp/ nlp-dataset
4https://github.com/raosudha89/ GYAFC-corpus

Dataset Yelp Amazon IMDb GYAFC

Style
Positive Negative
Positive Negative
Positive Negative
Formal Informal

Train
266K 177K
277K 279K
178K 187K
52K 52K

Dev
2000 2000
985 1015
2000 2000
2247 2788

Test
500 500
500 500
1000 1000
500 500

Table 1: Number of samples in the Train, Dev, and Test splits for each dataset in our experiments.

Dataset Model

Acc PPL BLEU

Yelp

DIRR-CYCLE

91.7 392 18.7

DIRR-YELP-ADV

95.2 353 20.7

Amazon DIRR

62.2 205 30.1

DIRR-AMAZON-ADV 83.2 228 29.0

Table 3: Adversarial Results. DIRR-YELP-ADV and DIRR-AMAZON-ADV denote the models which generate adversarial examples. Acc denotes the style transfer accuracy, PPL denotes the perplexity, BLEU is computed between the human references and system outputs.

Dataset Eq. λcls λadv λsim λlang λrec λcyc

Yelp (10) 2 0.5 20 2

(12) 1 -

-

-

0.1 1 1.5

Amazon (10) 2 0.5 20 2

(12) 5 -

-

-

11 0.5

IMDb (10) 1 0.5 20 2

(12) 1 -

-

-

111

GYAFC (10) 2 0.5 20 2

(12) 1 -

-

-

111

Table 2: Hyperparameter setting of Eq. 10 and Eq. 12 on each dataset.

we chose the Family & Relationships category for our experiments. Datasets statistics are shown in Table 1.
3.2 Experimental Details
Following previous work, we measure the style transfer accuracy using a FastText5 (Joulin et al., 2017) style classiﬁer trained on the respective training set of each dataset. To measure content preservation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when available. To measure the ﬂuency we use a pre-trained GPT-2 model to compute the perplexity.6 Our generator, GPT-2, has 1.5 billion parameters, and we train on a GTX 1080 Ti GPU for about 12 hours.
The weights of the loss terms in Eq. 10 and Eq. 12 are detailed in Table 2. While during our experiments we found that there are other possible conﬁgurations which give higher scores with respect to the automatic evaluation metrics, as will be discussed in Section 3.3, we also found that
5https://fasttext.cc/ 6Note that we didn’t ﬁne-tune it on the training set

better performance in automatic evaluation doesn’t always entail better performance in human evaluation. Therefore, we also manually checked the quality of the transferred texts on development set when we chose the value of the hyperparameters.
We compare our model with several state-ofthe-art methods: DeleteAndRetrieve (D&R) (Li et al., 2018), B-GST (Sudhakar et al., 2019), CycleMulti (Dai et al., 2019), Deep-Latent (He et al., 2020), Tag&Gen (Madaan et al., 2020), and DualRL (Luo et al., 2019). We also compare our ﬁnal model, DIRR(Direct-Reward), with the model only trained with the ﬁrst stage (DIRR-CYCLE) as mentioned in Section 2.4.
3.3 Adversarial Examples
Yelp and Amazon are arguably the most frequently used datasets for the sentiment transfer task. In our experiments, we found that the automatic evaluation metrics can be tricked on these datasets. Table 3 shows the performance of the models which generate adversarial examples. Upon identifying these risks, we propose several design options that can effectively mitigate these problems.
Yelp Dataset For the Yelp dataset, when trained without the adversarial discriminator fadv and the ﬂuency reward, our model (DIRR-YELP-ADV) is able to discover a trivial solution which receives high automatic evaluation scores: injecting a word that carries strong sentiment at the beginning of the output, and making minimum changes (if any) to the source sentences, as illustrated in Table 8. This obviously does not meet the objective of content-preserving sentiment transfer and is easily detectable for humans. In fact, after we manually removed the ﬁrst word from each of the output sentences, the transfer accuracy dropped from 95.2 to 58.4. To address this problem, we introduced an

Model
Train Test Human B-GST Tag&Gen DIRR DIRR-AMAZON-ADV

"game"
Pos. Neg.
58 7548 0 10 1 10 55 0 69 0 26 0 291 0

"phone"

Pos. Neg.

8947 20 18 13 14 19 190

2742 6 6 44 5 45 4

Table 4: Frequencies of words in the Amazon Dataset that appear often enough in speciﬁc classes to erroneously cause the classiﬁer to make incorrect predictions. Pos. denotes the positive sentences, Neg. denotes the negative sentences.

Model Source Adv
Source
Adv
Source Adv

Text
don t waste your time or money on these jeans . don t need your time or money on these phones .
i made beef bolognese in the oven and it turned out wonderfully . i made beef bolognese in the game and it turned out wonderfully .
this one does the job i need it for ! this game does the job i need it for !

Table 5: Adversarial examples received high style transfer accuracy scores on Amazon Dataset. Adv denotes the adversarial examples generated by DIRRAMAZON-ADV.

auxiliary discriminator fadv as we discussed above to penalize the trivial outputs since they can be easily captured by the discriminator. On the other hand, the output perplexity is not sensitive enough to this local feature so using the ﬂuency reward alone is not sufﬁcient. Our ﬁnal model has much more stable performance when the ﬁrst word of its output sentences is removed, experiencing only a small drop of the style transfer accuracy from 94.2 to 88.2.
Amazon Dataset For the Amazon dataset, we found that the style classiﬁer fcls needs to be updated during the training to prevent the model exploiting the data imbalance problem of the dataset. Namely, in the Amazon dataset some categories of products appear mostly in negative or positive reviews. In Table 4, we show the word frequency of game and phone in both negative and positive reviews. In the original dataset, game mostly appears in negative reviews while phone mostly appears in positive reviews. Therefore, without any prior knowledge, it is very likely that these words will be used as informative features by the sentiment classiﬁer, which makes its predictions unreliable.7
When our second-stage model is trained with the ﬁxed style classiﬁer, it (DIRR-AMAZON-ADV) learns to exploit this dataset bias by changing the nouns in the original sentences to game or phone, which achieves better transfer accuracy. We list some examples in Table 5. DIRR-AMAZONADV generated 291 game in 500 positive reviews, which obviously changes the semantics of the source sentences. In order to show that this phenomenon is independent to the classiﬁer architec-
7Notice that the style classiﬁer only achieves 43 accuracy on the human references.

ture, we additionally ﬁne-tuned a BERT-based (Devlin et al., 2019) classiﬁer, which yielded 51.3, 57.6, 70.4 accuracy on human references, DIRR, DIRR-AMAZON-ADV respectively, showing the same pattern of the fastText classiﬁer. We notice that some two-stage models (Li et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020) and other methods (Yang et al., 2018; Luo et al., 2019) also use a ﬁxed classiﬁer or use words with unbalanced frequencies in different styles as important features, which means that their methods may face the same risk. While Li et al. (2018) has pointed out this data imbalance problem of the Amazon dataset, we further demonstrate that a strong generator can even use this discrepancy to trick the automatic metrics. We are able to mitigate this problem by updating the style classiﬁer during the training, and in Table 4, DIRR is more robust to the data imbalance problem compared to other methods.
3.4 Automatic Evaluation
The automatic evaluation results are shown in Table 6. We report the performance of the previous methods based on the outputs they provided for fair comparison and omit those whose results are not available.
We have the following observations of the results. First, compared to our base model (DIRR-CYCLE), the model trained with our proposed rewards has higher ﬂuency, while remains the same level of content preservation. It indicates that SIM score is as effective as cycle-consistency loss for content preservation and our ﬂuency reward can effectively improve the output ﬂuency. Secondly, there exists a trade-off among the style transfer accuracy, content preservation and language ﬂuency. While our model does not outperform the previous meth-

Model
D&R B-GST Cycle-Multi Deep-Latent Tag&Gen DIRR-CYCLE DIRR Copy Human
D&R B-GST Tag&Gen DIRR-CYCLE DIRR Copy Human
Cycle-Multi DIRR-CYCLE DIRR Copy
D&R DualRL DIRR-CYCLE DIRR Copy Human

Acc PPL
Yelp
89.0 362 86.0 269 87.6 439 86.0 346 88.7 355 91.7 392 94.2 292 4.1 204 70.7 236
Amazon
50.0 233 60.3 197 79.9 312 68.4 374 62.2 205 21.1 218 43.0 209
IMDb
77.1 290 80.5 253 83.2 210 5.3 147
GYAFC
51.2 226 62.0 404 76.2 162 71.8 145 15.8 147 84.5 137

r-BLEU
10.1 14.5 19.8 15.2 12.4 18.7 20.7 22.5 99.3
24.1 20.3 27.6 29.0 30.1 40.0 100.0
N/A N/A N/A N/A
14.4 33.0 44.1 46.3 41.5 97.8

s-BLEU
29.1 35.1 55.2 40.7 35.5 51.2 52.6 100.0 22.5
54.1 44.6 62.3 60.6 61.3 100.0 40.0
70.4 64.3 64.2 100.0
27.1 50.8 66.5 59.9 98.5 21.5

Table 6: Automatic Evaluation. Acc is the accuracy of the sentiment classiﬁer. PPL is the perplexity assigned by the GPT-2 language model. r-BLEU is the BLEU score between the human references and system outputs. s-BLEU is the BLEU score between the source sentences and system outputs. Copy is an oracle which copies the source sentences as outputs. Human denotes the human references.

ods on all of the metrics, it is able to ﬁnd a better balance of the different metrics.
3.5 Human Evaluation
We conducted human evaluation on Yelp, Amazon and GYAFC datasets evaluating the style transfer accuracy, content preservation, and ﬂuency separately. The ﬁrst two aspects are rated with range 1 - 3 while the ﬂuency is rated with range 0 - 1. We randomly select 100 candidates and compare the outputs of different systems. We use Amazon Turk8 for human evaluation. Each candidate is rated by three annotators and we report the average scores here. We did not evaluate the style
8https://www.mturk.com/

Dataset Model

Style Flu. Con. Mean

Cycle

2.24 0.62 1.97 2.02

Yelp

B-GST

2.42 0.64 2.02 2.12

DIRR

2.42 0.66 2.04 2.14

Tag&Gen 1.98 0.87 1.95 2.19

Amazon B-GST

2.04 0.89 1.77 2.16

DIRR * 2.09 0.87 2.10 2.26

GYAMC

D&R DualRL DIRR *

N/A 0.40 2.13 1.66 N/A 0.51 2.23 1.88 N/A 0.70 2.34 2.22

Table 7: Human Evaluation. Style denotes style transfer accuracy, Flu. denotes ﬂuency, Con. denotes content preservation. Mean denotes the average of the metrics where the ﬂuency scores are scaled up to be consistent with other scores. *: signiﬁcantly better than other systems (p < 0.01) according to the mean score.

transfer accuracy for the GYAMC dataset since it is difﬁcult for human annotators to accurately capture the difference between formal and informal sentences. The results of our human evaluations are shown in Table 7. We additionally report the sample-wise mean score of the metrics where the ﬂuency scores are scaled up to be consistent with other scores. Our model achieves better overall performance when considering all three evaluation metrics on each dataset.
Interestingly, we found that the automatic metrics for both the style transfer accuracy and content preservation do not accurately reﬂect performance as measured by human evaluation. For example, on the Amazon dataset, although Tag&Gen (Madaan et al., 2020) achieves signiﬁcantly higher style transfer accuracy based on the automatic metric, our model achieves better performance based on the human evaluation. This phenomenon suggests that the importance of our ﬁndings discussed in Section 3.3, that strong neural models can potentially exploit the weaknesses of the automatic metrics.
4 Analysis
We next show an ablation study, demonstrating the effectiveness of the content preservation and ﬂuency rewards in DIRR, and how SIM can be used to replace the cycle-consistency loss. We also compare using BLEU versus using SIM as a content-preservation reward, ﬁnding that using BLEU results in reduced performance, unstable training, and artifacts in the outputs, which makes the results less natural than the results of the model trained with SIM score.
To illustrate that training with SIM can replace

Model Source DIRR-BLEU DIRR source DIRR-BLEU DIRR
source
DIRR-BLEU DIRR

Text

self-BLEU self-SIM

this was my ﬁrst stop in looking for a wedding dress . great this was my ﬁrst stop in looking for a wedding dress . this was my best stop in looking for a wedding dress .

100.0 91.2 64.8

100.0 95.2 81.9

just a frozen patty cooked like a home one . great a frozen patty cooked like a home one . just a great patty cooked like a home one .

100.0 88.0 70.7

100.0 94.6 88.5

wendy ’s has been know to be cheap with their drink reﬁlls for years . great wendy ’s has been know to be cheap with their drink reﬁlls for years . wendy ’s has been great with their drink reﬁlls for years .

100.0 93.0 57.2

100.0 97.5 84.9

Table 8: Comparison of using SIM and BLEU as the content preservation reward. Samples are from the Yelp dataset. The metrics self-BLEU and self-SIM are calculated between the source sentences and system outputs.

Model

Acc PPL s-BLEU s-SIM

DIRR-CYCLE 91.7 392 51.2 76.2

DIRR w/o FLU 92.1 348 51.4 79.8

DIRR-BLEU 91.3 315 59.4 81.8

DIRR

94.2 292 52.6 81.6

Table 9: Ablation and Comparative Study on Yelp Dataset. Acc is the accuracy of the sentiment classiﬁer. PPL is the perplexity assigned by the GPT-2 language model. self-BLEU (s-BLEU) and self-SIM (s-SIM) are computed between the source sentences and outputs.

the cycle-consistency loss for content preservation, we ﬁne-tuned DIRR-CYCLE on SIM to produce a new model, DIRR w/o FLU. The difference between DIRR and DIRR w/o FLU is that the former is additionally trained with our ﬂuency rewards. The results are shown in Table 9, and show two main trends. First, we see that DIRR w/o FLU has better ﬂuency and content preservation performance than DIRR-CYCLE, which shows that the cycle-consistency loss can be replaced by SIM score for content preservation. Second, DIRR has better ﬂuency than DIRR w/o FLU, showing the effectiveness of our ﬂuency rewards.
We next investigate the effectiveness of using SIM as a reward instead of BLEU. To do this, we train a model, DIRR-BLEU, which uses BLEU as the content reward and report the results in Table 9. The results show that using BLEU has larger content preservation as measured by BLEU, but has similar performance when measured by SIM. However, performance on the style transfer accuracy and ﬂuency decreases. We hypothesize that this

is because using SIM as a reward gives the model more freedom, allowing the model to have more balanced performance since there is less pressure to copy n-grams. We also observe more adversarial examples in the outputs of DIRR-BLEU. As discussed in Section 3.3, these adversarial examples are generated by injecting a word carrying strong sentiment at the beginning of the output. The model trained with BLEU is more likely to generate these outputs as it will try to avoid breaking up the ngrams in the source sentences, allowing for a higher BLEU reward. Examples of this behavior is shown in Table 8. Notice that the DIRR-BLEU samples start with the word great, which is enough to often fool the classiﬁer, but are unnatural.
5 Related Work
A main line of work (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Xu et al., 2018; John et al., 2019) for text style transfer aims to model the conditional distribution of the data with the encoderdecoder architecture. Due to the lack of parallel corpora, inductive biases are designed to make the generation conditioned on both source sentences and speciﬁc styles such that the model can rewrite the source texts with the target style while still preserve the content information of the source texts.
Efforts are also made to design training objectives to improve performance. For example, Backtranslation (Zhang et al., 2018; Prabhumoye et al., 2018), denoising auto-encoding (Lample et al., 2019) and the cycle-consistency loss (Luo et al., 2019; Dai et al., 2019; Pang and Gimpel, 2019)

have been shown effective for improving the model performance. Li et al. (2018) proposes a retrievebased pipeline, which contains three stages, namely, delete, retrieve and generate. Sudhakar et al. (2019) extends this pipeline by using GPT (Radford et al., 2018) as the generator. Compared to these methods, we propose a more direct and effective approach to encourage semantic-preserving transfer by directly measuring the semantic similarity of the source texts and system outputs.
Recently, other works have been proposed for unsupervised text style transfer (Jin et al., 2019; Lai et al., 2019; Wu et al., 2019; Li et al., 2020). He et al. (2020) proposes a probabilistic view which models the non-parallel data from two domains as a partially observed parallel corpus. Madaan et al. (2020) proposes a tag-and-generate pipeline, which ﬁrstly identiﬁes style attribute markers from the source texts, then replaces them with a special token, and generates the outputs based on the tagged sentences. Zhou et al. (2020) focuses on exploring the word-level style relevance which is assigned by a pre-trained style classiﬁer. They propose a reward for content preservation which is based on the weighted combination of the word embeddings of the source texts and system outputs. Compared to this reward, our proposed content reward is speciﬁcally designed for semantic similarity and pre-trained on large corpora, which makes it more robust across different datasets.
6 Conclusion
In this paper, we propose a direct approach of improving content preservation for text style transfer by leveraging a semantic similarity metric as the content reward. Our proposed rewards that target different aspects of the output quality, enable our model to have strong performance on both automatic and human evaluation. Recently, several semantic similarity metrics (Zhao et al., 2019; Sellam et al., 2020; Gao et al., 2020) based on pre-trained language models, have shown promising results. Introducing these metrics in our proposed method as the content preservation reward may lead to even further improvements.
Moreover, we identify several problems in the commonly used automatic evaluation metrics and datasets. We propose several practical strategies to mitigate these problems, which makes these metrics more effective rewards for model training. Considering the weaknesses of the automatic metrics

presented in this work, we believe that more rigorous discussion and investigation of the criteria for successfully transferring style is essential. Since existing work mostly relies on model-based metrics to determine the success of style transfer models, adversarial methods could be introduced to make the model-based metrics more robust and faithful indicators of style transfer. These improved evaluation models would then be beneﬁcial for both learning and evaluating new approaches for style transfer.
References
Ning Dai, Jianze Liang, Xipeng Qiu, and Xuan-Jing Huang. 2019. Style transformer: Unpaired text style transfer without disentangled latent representation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5997–6007.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018. Style transfer in text: Exploration and evaluation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1347– 1354, Online. Association for Computational Linguistics.
Junxian He, Xinyi Wang, Graham Neubig, and Taylor Berg-Kirkpatrick. 2020. A probabilistic formulation of unsupervised text style transfer. In International Conference on Learning Representations.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward controlled generation of text. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1587–1596. JMLR. org.
Zhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews, and Enrico Santus. 2019. Imat: Unsupervised text

attribute transfer via iterative matching and translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3088–3100.
Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. 2019. Disentangled representation learning for non-parallel text style transfer. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Florence, Italy. Association for Computational Linguistics.
Armand Joulin, Édouard Grave, Piotr Bojanowski, and Tomáš Mikolov. 2017. Bag of tricks for efﬁcient text classiﬁcation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431.
Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Linguistics.
Chih-Te Lai, Yi-Te Hong, Hong-You Chen, Chi-Jen Lu, and Shou-De Lin. 2019. Multiple text style transfer by using word-level conditional generative adversarial network with two-phase training. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3570–3575.
Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc’Aurelio Ranzato, and YLan Boureau. 2019. Multiple-attribute text rewriting. In International Conference on Learning Representations.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865–1874, New Orleans, Louisiana. Association for Computational Linguistics.
Siyao Li, Deren Lei, Pengda Qin, and William Yang Wang. 2019. Deep reinforcement learning with distributional semantic rewards for abstractive summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6038–6044, Hong Kong, China. Association for Computational Linguistics.
Yuan Li, Chunyuan Li, Yizhe Zhang, Xiujun Li, Guoqing Zheng, Lawrence Carin, and Jianfeng

Gao. 2020. Complementary auxiliary classiﬁers for label-conditional text generation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(05):8303–8310.
Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, and Zhifang Sui. 2019. A dual reinforcement learning framework for unsupervised text style transfer. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-19, pages 5116–5122. International Joint Conferences on Artiﬁcial Intelligence Organization.
Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, and Shrimai Prabhumoye. 2020. Politeness transfer: A tag and generate approach. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1869–1881, Online. Association for Computational Linguistics.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2018. Unsupervised learning of sentence embeddings using compositional n-gram features. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 528–540, New Orleans, Louisiana. Association for Computational Linguistics.
Richard Yuanzhe Pang and Kevin Gimpel. 2019. Unsupervised evaluation metrics and learning criteria for non-parallel textual transfer. In Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019).
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.
Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866–876, Melbourne, Australia. Association for Computational Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.
Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129–140, New Orleans, Louisiana. Association for Computational Linguistics.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.
Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. 2017. Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. CoRR, abs/1706.09799.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683–1692, Berlin, Germany. Association for Computational Linguistics.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2017. Style transfer from non-parallel text by cross-alignment. In Advances in neural information processing systems, pages 6830–6841.
Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. “transforming” delete, retrieve, generate approach for controlled text style transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3260– 3270.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Towards universal paraphrastic sentence embeddings. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.
John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019a. Beyond BLEU:training neural machine translation with semantic similarity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4344–4355, Florence, Italy. Association for Computational Linguistics.

John Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-Kirkpatrick. 2019b. Simple and effective paraphrastic similarity from parallel translations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4602– 4608.
Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256.
Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149.
Chen Wu, Xuancheng Ren, Fuli Luo, and Xu Sun. 2019. A hierarchical reinforced sequence operation method for unsupervised text style transfer. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4873– 4883, Florence, Italy. Association for Computational Linguistics.
Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 979–988.
Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and Taylor Berg-Kirkpatrick. 2018. Unsupervised text style transfer using language models as discriminators. In Advances in Neural Information Processing Systems, pages 7287–7298.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.
Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming Zhou, and Enhong Chen. 2018. Style transfer as unsupervised machine translation. CoRR, abs/1808.07894.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China. Association for Computational Linguistics.
Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, and Hua Wu. 2020. Exploring contextual word-level style relevance for unsupervised style transfer. In Proceedings of the

58th Annual Meeting of the Association for Computational Linguistics, pages 7135–7144, Online. Association for Computational Linguistics.

