1
PhaseCode: Fast and Efﬁcient Compressive Phase Retrieval based on Sparse-Graph Codes
Ramtin Pedarsani, Dong Yin, Kangwook Lee, and Kannan Ramchandran

arXiv:1408.0034v2 [cs.IT] 31 Mar 2017

Abstract—We consider the problem of recovering a complex signal x ∈ Cn from m intensity measurements of the form |aHi x|, 1 ≤ i ≤ m, where aHi is the i-th row of measurement matrix A ∈ Cm×n. Our main focus is on the case where the measurement vectors are unconstrained, and where x is exactly K-sparse, or the so-called general compressive phase retrieval problem. We introduce PhaseCode, a novel family of fast and efﬁcient algorithms that are based on a sparsegraph coding framework. We show that in the noiseless case, the PhaseCode algorithm can recover an arbitrarily-close-toone fraction of the K non-zero signal components using only slightly more than 4K measurements when the support of the signal is uniformly random, with order-optimal time and memory complexity of Θ(K)1. It is known that the fundamental limit for the number of measurements in compressive phase retrieval problem is 4K − o(K) for the more difﬁcult problem of recovering the signal exactly and with no assumptions on its support distribution [1], [2]. This shows that under mild relaxation of the conditions, our algorithm is the ﬁrst constructive capacity-approaching compressive phase retrieval algorithm: in fact, our algorithm is also order-optimal in complexity and memory. Further, we show that for any signal x, PhaseCode can recover a random (1 − p)-fraction of the non-zero components of x with high probability, where p can be made arbitrarily close to zero, with sample complexity m = c(p)K, where c(p) is a small constant depending on p that can be precisely calculated, with optimal time and memory complexity. As a result, assuming that the non-zero components of x are lower bounded by Θ(1) and upper bounded by Θ(Kγ) for some positive constant γ < 1, we are able to provide a strong 1 guarantee for the estimated signal xˆ as follows: xˆ − x 1 ≤ p x 1(1 + o(1)), where p can be made arbitrarily close to zero. As one instance, the PhaseCode algorithm can provably recover, with high probability, a random 1 − 10−7 fraction of the signiﬁcant signal components, using at most m = 14K measurements.
Next, motivated by some important practical classes of optical systems, we consider a “Fourier-friendly” constrained measurement setting, and show that its performance matches that of the unconstrained setting, when the signal is sparse in the Fourier domain with uniform support. In the Fourier-friendly setting that we consider, the measurement matrix is constrained to be a cascade of Fourier matrices (corresponding to optical lenses) and diagonal matrices (corresponding to diffraction mask patterns).
Finally, we tackle the compressive phase retrieval problem in the presence of noise, where measurements are in the form
Ramtin Pedarsani is with the ECE Department at UC Santa Barbara. email: ramtin@ece.ucsb.edu.
Dong Yin and Kannan Ramchandran are with the EECS Department at UC Berkeley. email:{dongyin,kannanr}@eecs.berkeley.edu.
Kangwook Lee is with the EE Department at KASIT. email: kw1jjang@kaist.ac.kr.
This paper was presented in part in Allerton 2015 and IEEE ISIT 2016. 1 Here, we deﬁne the notation O(·), Θ(·), and Ω(·). We have f = O(g) if and only if there exists a constant C1 > 0 such that |f /g| < C1; f = Θ(g) if and only if there exist two constants C1, C2 > 0 such that C1 < |f /g| < C2; and f = Ω(g) if and only if there exists a constant C1 > 0 such that |f /g| > C1.

of yi = |aHi x|2 + wi, and wi is the additive noise to the ith measurement. We assume that the signal is quantized, and each non-zero component can take Lm possible magnitudes and Lp possible phases. We consider the regime where K = βnδ, δ ∈ (0, 1). We use the same architecture of PhaseCode for the noiseless case, and robustify it using two schemes: the almost-linear scheme and the sublinear scheme. We prove that with high probability, the almost-linear scheme recovers x with sample complexity Θ(K log(n)) and computational complexity Θ(LmLpn log(n)), and the sublinear scheme recovers x with sample complexity Θ(K log3(n)) and computational complexity Θ(LmLpK log3(n)).
Throughout, we provide extensive simulation results that validate the practical power of our proposed algorithms for the sparse unconstrained and Fourier-friendly measurement settings, for noiseless and noisy scenarios.
I. INTRODUCTION
A. Phase Retrieval Problem
Compressive sensing (CS) has recently emerged as a powerful framework for understanding the fundamental limits for signal acquisition and recovery [3], [4]. The basic premise of CS is that a high-dimensional signal that is sparse in some basis can be recovered from linear projections of the signal with respect to an appropriate lower-dimensional measurement system. A key attribute of CS is that the measurement system is linear and phase-preserving. That is, the acquired samples, complex-valued in general, contain both the magnitude and phase of the measurements.
In many applications of interest, e.g. related to optics [5], X-ray crystallography [6], [7], astronomy [8], ptychography [9], quantum optics [10], etc., the phase information in the measured samples is not available. For example, in optical systems, one can measure only the intensity of the measurements as they relate to the photon count on a detector. Thus, the phase of the measurements is lost. Indeed, the problem of recovering a signal from only the magnitude of its Fourier transform has been a well-studied problem in the signal processing literature for several decades under the umbrella of phase retrieval [11]. It has recently received renewed interest in the “postcompressed-sensing” era [12]–[14], allowing for the insights from compressive sensing to be incorporated into the phase retrieval problem when the signal of interest is sparse, and the measurement matrix is unconstrained.
Concretely, consider a signal x ∈ Cn and a measurement matrix A ∈ Cm×n. The phase retrieval problem is to recover x from the observations y = |Ax|, x ∈ Cn, where the magnitude is taken on each element of the vector Ax. The compressive phase retrieval problem targets the case where x is K-sparse.

2

In this paper, we study the phase retrieval problem under the following settings: (i) General compressive phase retrieval of sparse signals2;
and (ii) “Fourier-friendly” compressive phase retrieval of signals
having a sparse spectrum.
We now summarize these settings:
(i) General compressive phase retrieval of sparse signals: In this setting, we are free to design the measurement matrix A without any constraints, and this represents the primary contribution of this paper. We consider it for three reasons. (1) It is of broadest theoretical interest, being the most general compressive phase retrieval problem, for which we propose a sparse-graph coding framework that is a signiﬁcant departure from currently popular approaches based on convex optimization, Semi-Deﬁnite Programming (SDP), alternating minimization, gradient descent, etc. [14]–[20]. (2) It provides the intellectual insights and the foundational framework needed to address more constrained problems, such as those studied under the Fourier-friendly setting of category (ii). (3) It is of independent interest in applications related to certain quantum optical systems. For example, compressive sensing has been used in recent work involving quantum optics [10] to measure the transverse wavefunction of a photon, where the design of the measurement matrix has no constraints.
(ii) Fourier-friendly compressive phase retrieval of signals having a sparse spectrum: In this category, motivated by applications related to Fourier optical systems, the measurement matrix A is constrained to be Fourier-friendly (see Section VI for a detailed treatment). Concretely, A is constrained to be the cascade of (up to a couple of) stages of a diagonal matrix (corresponding to a socalled optical mask or coded diffraction pattern) and a Fourier transform (corresponding to an optical lens). This constraint is motivated by practical optical systems [21], array imaging [22], etc., as also addressed recently by [23].
B. Main Contributions
A key contribution of this work is in the introduction of modern coding theory techniques such as density evolution and sparse-graph codes [24] for the compressive phase retrieval problem. Exploiting these techniques and a similar measurement system to [18], [25] allows us to come up with the provably efﬁcient and fast PhaseCode algorithm that is orderoptimal in terms of number of measurements needed, timecomplexity, and memory-complexity, which are all O(K). Furthermore, we provide precise constants for the number of measurements needed to achieve a targeted reliability. To the best of our knowledge, this is the ﬁrst work that
2This is easily extended, as is well known, to the case where the signal x is sparse w.r.t. some other basis, such as a wavelet, but in the interests of conceptual clarity, we will not consider such extensions in this work.

provides precise constants for the number of measurements. More speciﬁcally, the main contribution of this paper are the following:
(i) For an arbitrary signal x, the PhaseCode algorithm can provably recover a random fraction of at least 1 − 10−7 of the active signal components with 14K measurements, with optimal time and memory complexity Θ(K). This is one instance of an entire family of trade-offs between the number of measurements needed and the fraction of non-zero signal components that can be recovered using PhaseCode. More precisely, we show that for any signal x, PhaseCode can recover a random (1 − p)fraction of the non-zero components of x with high probability, for arbitrarily-close-to-zero constant p with sample complexity m = c(p)K, where c(p) is a small constant depending on p that can be precisely calculated. As a result, assuming that the non-zero components of x are lower bounded by Θ(1) and upper bounded by Θ(Kγ) for some positive constant γ < 1, we are able to provide a strong 1 guarantee for the estimated signal xˆ as follows: xˆ − x 1 ≤ p x 1(1 + o(1)), where p can be made arbitrarily close to zero.
(ii) The PhaseCode algorithm can recover an arbitrarilyclose-to-one fraction of the non-zero components of x using 4K(1 + ) measurements for an arbitrarily small constant > 0, when the support of the non-zero components of x is uniformly random, with optimal time and memory complexity of Θ(K). It is well-known that 4K − o(K) measurements is the fundamental limit for unique recovery of K-sparse signals [1], [2] for the more difﬁcult problem of recovering the signal exactly with no assumptions on the support of the signal. This shows that under mild relaxation of the conditions, the PhaseCode algorithm is capacity-approaching.
(iii) Another key contribution of this work is to adapt the PhaseCode algorithm to a more constrained Fourierfriendly setting that is useful in certain optical systems, when x has a sparse spectrum. Speciﬁcally, we show how it is possible to elegantly integrate the ChineseRemainder-Theorem-centric framework of Pawar and Ramchandran [26] (that was used to ﬁnd a fast sparse Discrete-Fourier-Transform) into our PhaseCode framework without any loss of system performance in terms of measurement cost or computational complexity. See Section VI for details.
(iv) We demonstrate that PhaseCode can be robustiﬁed in the presence of noise. We use the same architecture of PhaseCode for the noiseless case, and robustify it using two schemes: the almost-linear scheme and the sublinear scheme. We assume that the signal is quantized, and each non-zero component can take Lm possible magnitudes and Lp possible phases. We prove that with high probability, the almost-linear scheme recovers x with sample complexity Θ(K log(n)) and computational complexity Θ(LmLpn log(n)), and the sublinear scheme recovers x with sample complexity Θ(K log3(n)) and computational complexity Θ(LmLpK log3(n)).

3

We provide pseudocode of our algorithms (in Appendix O) and an extensive set of simulation results for all of the above settings that validate our theoretical ﬁndings, and verify the close match between theory and practice.
C. Related Work
The phase retrieval problem has been studied extensively over several decades. We do not attempt to provide a comprehensive literature review here; instead, we highlight here only some of the pertinent and diverse approaches to this problem that we are aware of. A large body of literature is dedicated to the phase retrieval problem for the case where the signal to be recovered has no structure and is not sparse. “Phaselift” proposed by Candes et al. [15] and “PhaseCut” proposed by Waldspurger et al. [27] are examples of convex optimization methods to solve the problem using semi-deﬁnite programming with Θ(n log(n)) measurements. While algorithms based on SDP provide theoretical performance guarantees and are robust to noise, they suffer from a high computational complexity of O(n3) rendering them unsuited for many practical applications that require n to scale.3 In [16], the authors propose an algorithm based on alternating minimization that reconstructs the signal with Θ(n log(n)3) measurements. In [20], the authors propose a non-convex algorithm based on Wirtinger ﬂow that reconstructs the signal with measurement and computational complexity of Θ(n log n).
In [2], [28]–[30], several sets of authors investigate the fundamental limits of phase retrieval problem, with the goal of ﬁnding necessary or sufﬁcient conditions on the minimum number of measurements needed to guarantee that the solution is unique. In summary, 4n − 4 measurements are shown to be sufﬁcient [30], and 4n − o(n) measurements are necessary [2] to reconstruct any signal perfectly.
We now review some relevant literature on compressive phase retrieval. To the best of our knowledge, the ﬁrst algorithm for compressive phase retrieval was proposed by Moravec et al. in [12]. This approach requires knowledge of the 1 norm of the signal, making it impractical in most scenarios. The authors in [1] showed that 4K − 1 measurements are theoretically sufﬁcient to reconstruct the signal, but did not propose any low-complexity algorithm. This number was later improved to 4K − 2 in [31], [32]. The PhaseLift method is also proposed for the sparse case in [14] and [17], requiring Θ(K2 log(n)) intensity measurements, and having a computational complexity of O(n3), making the method less practical for large-scale applications. In [33], the authors propose an efﬁcient algorithm based on polarization method that is able to stably reconstruct any K-sparse vector from Θ(K log(n)) noisy intensity measurements with complexity polynomial in n. The alternating minimization method in [16] can also be adapted to the sparse case with Θ(K2 log(n)) measurements and a complexity of O(K3n log(n)). Compressive phase retrieval via generalized approximate message passing (PR-GAMP) is proposed in [13], with good performance in
3This limits the use of SDP-based methods to small to moderate values of n in practice. In contrast, we show simulations in the paper where n can be very large, even as large as 1010. See Figures 6 and 8.

both runtime and noise robustness shown via simulations without theoretical justiﬁcation.
A common attribute of all of the above-mentioned compressive phase retrieval references is that they assume that the measurement matrix can be designed freely. This renders them inapplicable to many application-constrained settings such as Fourier-optical systems. In [23], Candes et al. consider measurement matrices that are Fourier-friendly as described in the previous subsection, but only for the non-sparse case. They show that PhaseLift is able to recover the signal with Θ(n log(n)4) measurements by using Θ(log(n)4) masks or coded diffraction patterns. For the sparse case, Jaganathan et al. consider the phase retrieval problem from Fourier measurements only [18], [19]. They propose an SDP-based algorithm, and show that the signal can be provably recovered with Θ(K2 log(n)) Fourier measurements [18]. They also propose a combinatorial algorithm for the case where the measurement matrix can be designed without constraints, and show that the signal can be recovered with Θ(K log(n)) measurements and time complexity of O(Kn log(n)) [18].
In the prior literature that we are aware of, the works which overlap the most in spirit with ours are (i) the recently proposed SUPER algorithm for compressive phase retrieval by Cai et al. in [25]; and (ii) the FFAST algorithm of Pawar and Ramchandran [26] which also features the use of codingtheoretic tools for efﬁciently computing a sparse Discrete Fourier Transform. With regard to the FFAST algorithm [26], despite the common use of coding-theoretic tools, our problem formulation, analysis, and resulting algorithm are signiﬁcantly different, mainly because our problem involves the loss of measurement phase, unlike that of FFAST.
With regard to the SUPER algorithm of [25], again, while there are some similarities between the two approaches – mainly to do with the use of certain system subcomponents such as a similar (but not identical) trigonometric-modulation method to resolve phase ambiguities, and the common use of a giant-component-cluster in the initial phase of our proposed PhaseCode algorithm (see Section V-B for details), our works are signiﬁcantly distinct at many levels. First, the SUPER algorithm targets only the general unconstrained compressive phase retrieval setting, whereas, as described earlier, we also target Fourier-friendly constrained settings that are applicable in optical systems. Secondly, even in the unconstrained phase retrieval setting, there are signiﬁcant distinctions between the two works with respect to theory, algorithm, and performance guarantees. As a quick overview, the SUPER algorithm uses Θ(K) measurements and features Θ(K log(K)) complexity with a zero-error-ﬂoor asymptotically. In contrast, by trading off the zero-error-ﬂoor for an arbitrarily-small controllable error-ﬂoor, our solution features key advantages. Speciﬁcally, this allows us to design a capacity-approaching measurement system that is based on a new and novel sparse-graph coding framework. The use of a sparse-graph coding framework in PhaseCode allows for iterative message-passing operations between the left nodes (signal components) of the sparse-graph code and the right nodes or measurements (see Section IV). This contrasts the more inefﬁcient strictly “one-way” procedure in SUPER [25] wherein measurements of different stages

4

are processed sequentially rather than iteratively. Moreover, PhaseCode has an optimal Θ(K) decoding complexity with optimal Θ(K) memory requirements. We also demonstrate how PhaseCode can be robustiﬁed in the presence of noise, unlike the work of [25]. We note that SUPER can also achieve O(K) results with error ﬂoor. However, their approach is unable to characterize and optimize this error ﬂoor when the number of measurements is cK for a speciﬁc constant c. Finally, we note that peeling-based algorithms and expander graphs have been used for compressive sensing [34].
D. Paper Organization
The rest of the paper is organized as follows. In Section II, we deﬁne the general compressive phase retrieval problem. In Section III, we explain the main idea of PhaseCode algorithm. We present PhaseCode algorithm in detail in Section IV. The main theoretical results of the paper are provided in Section V. Via extensive simulations, we evaluate PhaseCode algorithms, validating the theorem. In Section VI, we demonstrate how our proposed measurements can be adapted to a Fourierfriendly setting. In Section VII, we show that PhaseCode can be robustiﬁed to noise. Finally, we conclude the paper in Section VIII.
II. PROBLEM FORMULATION AND OVERVIEW OF THE MAIN RESULT
Consider a complex signal x ∈ Cn of length n which is exactly K-sparse; that is, only K out of n components of vector x are non-zero. Let A ∈ Cm×n be the measurement matrix that needs to be designed. The phase retrieval problem is to recover the signal x from magnitude measurements yi = |aHi x|, where aHi is the i-th row of measurement matrix A ∈ Cm×n. Figure 1 illustrates the block diagram of our problem.

x input A
signal

y = |Ax|

|.|

Decoder

xˆ

estimated

magnitude

signal

measurements

Fig. 1: Block diagram of general compressive phase retrieval problem. The measurements are yi = |aHi x|, where aHi is the i-th row of measurement matrix A. The objectives are to design measurement
matrix A and the decoding algorithm to guarantee high reliability,
while having small sample complexity as well as small time and
memory complexity.

The main objectives of the general compressive phase retrieval problem is to design matrix A, and the decoding algorithm to recover x such that
• The number of measurements m is as small as possible. Ideally, one wants m to be close to the fundamental limit of 4K − o(K) [1], [2].
• The decoding algorithm is fast with low computational complexity and memory requirements. Ideally, one wants the time complexity and the memory complexity of the algorithm to be O(K), which is optimal.

• The reliability of the recovery algorithm should be maximized. Ideally, one wants the probability of failure to be vanishing as the problem parameters K and m get large.
Remark In this work, we are interested in the asymptotic K regime. However, even when K is small, with proper modiﬁcation of our algorithm, high reliability can be guaranteed when m gets large. It is worth mentioning that in this case, the number of measurements will be larger than the fundamental limit that is 4K(1 + o(1)). We do not discuss this any further in the interest of presentation clarity.
The main result of our paper is stated in the following (informal) theorem.
Theorem 1. Consider a K-sparse signal x ∈ Cn, and the measurement matrix A ∈ Cm×n chosen by the PhaseCode algorithm.
(i) PhaseCode can recover a random (1 − p)-fraction of the non-zero components of x with high probability, for arbitrarily-close-to-zero constant p. The measurement complexity of the algorithm is m = c(p)K, where c(p) is a small constant depending on p that can be precisely calculated. The time and memory complexity of PhaseCode are also Θ(K). Further, for the estimated signal xˆ, assuming that the non-zero components of x are lower bounded by Θ(1) and upper bounded by Θ(Kγ) for some positive constant γ < 1, we have
xˆ − x 1 ≤ p x 1 1 + Θ( 1 ) . log(K )
(ii) Assuming that the support of x is distributed uniformly at random, with high probability, PhaseCode can recover an arbitrarily-close-to-one fraction of the non-zero components with m = 4K(1 + ) measurements for arbitrarily small constant > 0.
These results are more precisely stated in Theorems 2 and 3 in Section V. See Table II for some selected values of p and m.
III. MAIN IDEA OF THE PHASECODE ALGORITHM
We now describe the main idea behind PhaseCode. As mentioned, the main novelty of our work is that we use sparsegraph codes, and the powerful tools of modern coding theory for design and analysis.
The design of an appropriate measurement matrix A for the compressive phase retrieval problem is equivalent to the design of an appropriate bipartite graph G, as for each measurement matrix, there exists a corresponding bipartite graph. Speciﬁcally, the rows of A (the measurements) are the right nodes in the bipartite graph G, while the columns of A (the signal components) are left nodes of G. We call the left nodes of G that correspond to an active (non-zero) signal component as active left nodes. Left node i is connected to right node j if aji is non-zero. The example shown in Figure 2 illustrates this connection.
As is well-known and also intuitive, in the phase-retrieval problem, the signal of interest can be recovered only to within

5

⎡

⎤1

a 0 0a 0

1

A = ⎢⎢⎣ a01311 a032 a023 a01344 a025 ⎥⎥⎦ 32 32

New primitive: 0Bal0l-Coa4l3orin0 g R0ule 4

4

5

(a) Measurement matrix A.

(b) Bipartite graph G.

Fig. 2: Sparse graph codes. The rows of A (the measurements) correspond to right nodes in the bipartite graph G, while the columns of A (the signal

components) correspond to the left nodes of G.

Fig. 3: Coloring operation. The ﬁgure illustrates when a right node is connected to exactly one uncolored active left node, and the other active left nodes connected to the right node are colored with the same color, then the uncolored active left node is colored with that color. In the graph, we have shown only the active left nodes.
an unknown global phase. The idea of our iterative reconstruction algorithm is to detect a non-zero signal component, give it global zero-phase, and align all other signal components with respect to it. This suggests the intuition of building up one or more clusters of non-zero components, where in our terminology, these clusters are identiﬁed by their colors; i.e. all the non-zero components belonging to a particular cluster have the same color. Two (or more) non-zero components (active left nodes) can be colored with the same color if their components are known in location, magnitude and phase relative to each other.
Our goal in designing the measurement matrix of the sparse graph is to create iteratively decodable right nodes (set of appropriately designed measurements). The key property of a right node that is conducive to our desired coloring operation is as follows. If a right node is connected to one or more known components (colored active left nodes with the same color) and exactly one uncolored active left node (unresolved active signal component), then that component can be resolved, i.e. the uncolored active left node will be colored with the same color. See Figure 3.
Our idea is to make this coloring “primitive operation” iteratively trigger more such coloring primitive operations in the system. Of course, the key is to design the graph efﬁciently to ensure that the domino-effect will continue till all the active left nodes are colored, while minimizing the number of right nodes needed to accomplish this (measurement cost).
This is the high-level connection between the compressive phase retrieval problem and sparse-graph code design. Our recovery process is conceptually similar to the “peeling” decoding of packets based on Low-Density-Parity-Check (LDPC) codes in packet-erasure communication systems, with the key distinction that we cannot measure phase. This makes our problem more challenging, therefore requiring a different analysis of the density evolution in the graph, as we will describe. But at a high level, our coloring primitive operation

plays the analogous role of peeling in LDPC decoding. Of course, a natural question is how our measurement
system detects if a right node is indeed connected to one or more colored active left node and exactly one uncolored active left node. We can do so with a set of 4 cleverly designed “trigonometric” measurements that are part of each right node. We will explain the trigonometric measurements in detail in Section IV-A.

IV. PHASECODE ALGORITHM

First we deﬁne A ∈ C4M×n to be a “row tensor product”4

of matrices T and H, where H ∈ {0, 1}M×n is a binary

“code” matrix, to be shortly explained, and T ∈ 4 × n

is the “trigonometric modulation” matrix that provides 4

measurements per each row of H. We deﬁne a row tensor

product of matrices T and H, T ⊗ H, as follows. Let

A

=

T

⊗

H

=

[A

H 1

,

A

H 2

,

.

.

.

,

A

H M

]H

and

Ai

∈

C4×n.

Then,

Ai(jk) = TjkHik, 1 ≤ j ≤ 4, 1 ≤ k ≤ n.

Example 1. Consider matrices

T = 00..41 00..52 00..63





010

and H =  1 1 0  .

001

Then, our measurement matrix A is designed from:





0 0.2 0

 0 0.5 0 

A=T

⊗H

= 

0.1 0.4

0.2 0.5

0 0

 .

 0 0 0.3 

0 0 0.6

Matrix H is constructed using a carefully chosen random bipartite graph model with n left nodes and m right nodes. Each left node refers to a component of x, and each right node refers to a set of 4 measurements. There are K active left nodes corresponding to the K non-zero components of x. The bipartite graph is constructed as follows. Hij = 1 if and only if left node j is connected to right node i, and Hij = 0 otherwise.
While we provide the details of how to design matrix T in
Section IV-A, for completeness of the description, we state it precisely here deferring explanation to Section IV-A. Let ω

4Here, we apologize for not following popular convention for the notation for tensor product of matrices; instead, we deﬁne our own notation that is convenient for our purpose, which should hopefully not cause any confusion.

Notation x K n m M A H T

Description complex signal of length n
sparsity of the signal length of the signal number of measurements number of the rows of the code matrix measurement matrix
code matrix modulation matrix

TABLE I: Table of Notation.

6
New primitive: Ball-Coloring Rule
Fig. 4: Singleton coloring operation. The ﬁgure illustrates when a right node is a singleton, the corresponding active left node gets colored with a new color. In the graph, we have not showed the left nodes corresponding to 0 signal components.

be a uniformly random phase between 0 and 2π. We design

T ∈ C4×n to be

 eiω

ei2ω . . . einω 

T

=

 

e−iω cos(ω)

e−i2ω cos(2ω)

... ...

e−inω cos(nω)

 .

(1)

eiω

ei2ω . . . einω

As in [26], in the bipartite graph model, we use the following terminology extensively throughout the paper:
• Singleton: A right node is a singleton if it is connected to exactly one active left node.
• Doubleton: A right node is a doubleton if it is connected to exactly two active left nodes.
• Multiton: A right node is a multiton if it is connected to more than one active left node.5
We now describe PhaseCode algorithm, and analyze it in Section V. With the aid of the carefully designed matrix T , our decoder is capable of performing the following functions:
• When an active left node is connected to a singleton right node, the active left node can be colored with a new color. That is, the non-zero component can be found in magnitude and location. However, the relative phase of the component with respect to other resolved components cannot be recovered. Figure 4 illustrates this operation. Note that in our terminology, each color refers to a local coordinate with a local phase, for example, the red coordinate, blue coordinate, etc. Then, the relative phase of two non-zero components that are colored as red is known. However, the relative phase of a blue component and a red component is not known.
• When a right node is connected to exactly one uncolored active left node, and the other non-empty set of active left nodes connted to the right node have all the same color (let’s say green), then the uncolored active left node is colored with that color (i.e. it becomes green). Figure 3 illustrates this operation.
• When all the active left nodes connected to a right node are colored, with exactly two colors, then those two colors can be combined into a single composite color. Figure 5 illustrates this operation.6

PhaseCode Algorithm In the ﬁrst iteration of the algorithm, all the left nodes connected to singletons are colored. In the second iteration, all the doubletons that are connected to two colored active left nodes from the ﬁrst iteration (strong

5In our terminology, a doubleton is also a multiton. 6We use this operation only in the second iteration of PhaseCode.

Fig. 5: Combining colors. The ﬁgure illustrates when a right node is connected to only colored active left nodes with two colors, then the colors can be combined.
doubletons), are detected, and their colors are combined. Then, the largest set of active left nodes having the same color7 is selected, and every other colored active left node gets uncolored. At this point, there is only one color and no new colors are added to the system. In the following iterations, if a right node is connected to exactly one uncolored active left node and at least one colored active left node, then that uncolored active left node gets colored. (See Figure 3.) The algorithm continues until no more active left nodes can be colored.
We provide the pseudocode of the algorithm in Appendix O.
Remark PhaseCode has Θ(K) time and memory complexity.
Example 2. Let K = 4, M = 5 and d = 2. Without loss of generality, label the active left nodes by 1 to 4. Suppose that the bipartite graph is such that the right nodes are connected to {1}, {1, 2}, {3}, {1, 3}, and {2, 3, 4}. In the ﬁrst iteration, 1 and 3 are colored, let us say by red and blue, respectively since these active left nodes are connected to singletons. In the second iteration, PhaseCode ﬁnds a strong doubleton, {1, 3}, that is connected to colored left nodes 1 and 3. Thus, their colors are combined to a composite color, let us say green, which will be the only color of the system after this iteration. In the third iteration, left node 2 is colored through the right node {1, 2}, since 2 is the only uncolored left node connected to this right node. Finally, in the forth iteration, left node 4 is colored through right node {2, 3, 4}. This completes the successful decoding of PhaseCode algorithm.
A. Measurement Design: “Trig-Modulation”
In this section, we will explain the choice of the measurement matrix T . Our design of T draws heavily from the proposed trigonometric subsystem in [25] with proper modiﬁcations to better match our sparse-graph code subsystem, H, that is distinct from [25]. We also show that one can decrease the number of these trig-based measurements from 5 per right node as proposed in [25] to 4 per right node as we describe that is crucial in designing a capacity-approaching scheme.
7Whenever two active left nodes having colors C1 and C2 are combined, they get the same composite color C12, and all other active left nodes with colors C1 and C2 are also recolored to C12.

7

Deﬁne the length-4 vector yi to be the measurement vec-

tor corresponding to the i-th row of matrix H for 1 ≤

i

≤

M . Then y

=

[y

T 1

,

y

T 2

,

.

.

.

,

y

T M

]

T

,

where

yi

=

[yi,1, yi,2, yi,3, yi,4]T . Let ω = 2πn . We design the measurement

matrix T = [tj ] as follows. For all , 1 ≤ ≤ n,

t1 = eiω ,

(2)

t2 = e−iω ,

(3)

t3 = 2 cos(ω ),

(4)

t4 = eiω ,

(5)

where as mentioned in Section IV, ω is a random phase uniformly distributed between 0 and 2π.
As mentioned in Section IV, the measurement matrix should enable us to do the following operations: (1) Detect whether we have a singleton right node, and if yes, what the location index and magnitude of the corresponding active left node are (See Figure 4); (2) detect if a multiton right node is connected to colored active left nodes having exactly two unique colors, and if yes, what the relative phase of the colored components is. We call these as mergeable multitons (See Figure 5); (3) detect if a multiton right node is connected to colored active left nodes with the same color and only one uncolored active left node, the measurement system should be able to ﬁnd the index, magnitude, and relative phase of the uncolored active left node. We call these right nodes resolvable multitons as in [25] (See Figure 3). In the following, we show how each of these detections can be accomplished using “guess and check” approach. We provide pseudocode of these detection procedures in Appendix O.
(i) Singletons: Suppose that we want to check the hypothesis that the i-th right node is a singleton. If the right node is a singleton, only one non-zero component of x, let’s say x , is present in vector yi, that is yi,1 = |x eiω |, yi,2 = |x e−iω |, and so on. Thus, the i-th right node is a singleton only if yi,1 = yi,2 = yi,4. The event that i is not a singleton, and all these measurements are equal has measure 0 since ω is a uniformly random phase.8 In order to ﬁnd the index , one uses yi,3 to get

= 1 cos−1 (cos(ω )) = 1 cos−1 yi,3 .

ω

ω

2yi,1

Note

that

cos(ω )

is

positive

if

0

≤

ω

≤

π 2n

for

all

, 1 ≤ ≤ n.

(ii) Mergeable multitons: Consider a right node i as in

Figure 5, which is already known to be connected to

some (say, red) active left nodes (non-empty set R) and

some (say, blue) active left nodes (non-empty set B). This

means that the red (or blue) signal components are known

in location, magnitude, and phase relative to each other.

However, the relative phase of blue and red components’

coordinate systems is not known. If there is no other

active left node connected to i, we show that the relative

8In practice, every measurement system has a ﬁnite precision level. Moreover, practical systems suffer from the presence of noise. The measurement system introduced here is clearly not robust to noise and ﬁnite precision of the measurement matrix, but we will show in Section VII that PhaseCode can be robustiﬁed to noise while maintaining its iterative decoding architecture.

phase can be found. Thus, the colors can be combined. (We again deploy a guess and check strategy.) First, we guess that right node i is connected to no other active left nodes. Then, we have access to measurement

yi,1 = |r + b|,

where r = ∈R xjeiω is the sum of complex numbers corresponding to the red components, and b =
∈B x eiω is the sum of complex numbers corresponding to the blue components. Since red components are
known up to a local phase, |r| is known. Similarly, |b| is
also known. Without loss of generality, pick some r ∈ R
and set the phase of x r to 0 to form the local coordinate for red components. Furthermore, pick some b ∈ B and
set the phase of x b to 0 to form the local coordinate for blue components. Given the local coordinates, r = |r|eiφr and b = |b|eiφb are known. By the cosine law, the true
relative phase between r and b can be found as

θ = cos−1 |r|2 + |b|2 − yi2,1 ,

(6)

2|r||b|

up to a plus-minus sign. Assuming that the plus sign is true, we can merge these components as follows. Without loss of generality, we set the phase of x r to 0. Thus, r = |r|eiφr and b = |b|ei(φr+θ). This shows that the local coordinate in B should be rotated by an angle θ +φr −φb to match with the new coordinate. Hence, we recover all the blue components with respect to the coordinate of red components, and the colors can be combined. A similar procedure can be done for the solution of θ with a minus sign. Now we again use the check equation to ﬁnd whether one of these relative phases passes the check equation. If none of them passes, our guess is wrong, and right node i is not a mergeable multiton. Thus, we need to check whether

|

x eiω | = yi,4

∈R∪B

is satisﬁed or not for the 2 values of θ derived in (6). If the guess is correct, the probability that the check fails is 0 since ω is random. Moreover, if the guess is not correct, the probability that the check passes is 0. (iii) Resolvable multitons: Consider a right node i, for which we know that it is connected to some known active left nodes that have the same color. We want to check if i is connected to exactly one other active left node; i.e. one unknown non-zero component of x, say x , as in Figure 3. We now describe our guess and check strategy to check if right node i is indeed a resolvable multiton, and if so, to ﬁnd and x . If our guess is correct, we have access to measurements of the form:

yi,1 = |a + eiω x | = |u|,

(7)

yi,2 = |b + e−iω x | = |v|,

(8)

yi,3 = |c + 2 cos(ω )x | = |w|,

(9)

yi,4 = |d + eiω x |,

(10)

8

where complex numbers a, b, c and d are known values that depend on the values and locations of the known colored active left nodes. For the purpose of readability, we show the calculations of how to solve the system of equations (7)-(10) in Appendix A.
V. MAIN RESULT
In this section, we analyze the performance of PhaseCode and provide the main theoretical results of this paper.
A. Bipartite Graph Construction
As mentioned earlier, we design our code matrix based on a random bipartite graph model. Given a bipartite graph with n left nodes and M right nodes, deﬁne the pruned bipartite graph corresponding to x to be a bipartite graph with K left nodes corresponding to the non-zero components of x and M right nodes, such that all the left nodes corresponding to the zero components and their connected edges are deleted. From now on, we consider the pruned graph for analysis. Moreover, from now on, by a left node (of the pruned graph), we refer to an active left node.
We ﬁrst deﬁne the left and right edge degree distribution of the random bipartite graph. Deﬁne ρi to be the probability that a randomly selected edge is connected to a right node of degree i, and λi to be the probability that a randomly selected edge is connected to a left node of degree i. Deﬁne the edge degree distributions or edge degree polynomials of right and left nodes as follows.
ρ(x) = ρixi−1;
i≥1
λ(x) = λixi−1.
i≥1
We construct two random bipartite graph models as follows:
(i) Regular left degree: In this construction, each left node is connected to d right nodes randomly, where d is a constant to be chosen. Thus, the degree of all left nodes are d. More formally, let CK(d, M ) be the ensemble of regular left degree bipartite graphs with K left nodes, M right nodes, and left degree d. We pick a bipartite graph uniformly at random from this ensemble. When M and K get large, the degree of a random right node is Poisson distributed with parameter η = KMd . Note that the degree of a right node in the pruned graph is the number of active left nodes connected to it. Since ρi is the fraction of edges that are connected to a right node of degree i, we have
iM ρi = Kd P(random right node has degree i)
= i ηie−η η i! ηi−1e−η
= (i − 1)! .

Then, the left edge and right edge degree distributions are

λ(x) = xd−1

(11)

ρ(x) = e−η(1−x).

(12)

(ii) Irregular left degree: In this construction, we design

the left degree distribution λ(x) based on a truncated

harmonic distribution as follows. Let h(x) =

x i=1

1/i.

Then,

λi = 1 × 1 , 2 ≤ i ≤ D, (13) i − 1 h(D − 1)

where D is a (large) constant to be determined. The harmonic distribution for irregular LDPC codes is wellknown to be capacity-achieving for BEC channels [35].

The main theoretical results of this paper for the noiseless case are as follows.

Theorem 2. Let A = T ⊗ H be the measurement matrix,
where H is chosen uniformly at random from the ensemble Cn(d, M ) and T is the modulation matrix deﬁned in (1). Using the m measurements y = |Ax|, for any p > 0, Regular PhaseCode can recover at least a 1 − p fraction of the non-zero components of x chosen uniformly at random, where m = c(p)K and tabulated in Table II for selected
values. As a particular operating point, Regular PhaseCode is able to recover a random fraction 1 − 10−7 of non-zero components of x with 14K measurements with probability 1 − O(1/m). Furthermore, the decoding complexity of the algorithm is Θ(K) which is order-optimal.

Theorem 3. Let A = T ⊗ H be the measurement matrix, where H is chosen according to the irregular construction in (13), and T is the modulation matrix deﬁned in (1). Under the assumption that the support of the sparse signal is uniformly random, using m = 4K(1 + ) measurements y = |Ax| for arbitrarily small > 0, Irregular PhaseCode is able to recover all but an arbitrarily small random fraction of the non-zero components of x with probability 1 − O(1/m). Furthermore, the decoding complexity of the algorithm is Θ(K) which is order-optimal.

We provide the proofs in Sections V-B and V-C.

Corollary 4. Suppose that for a particular choice of parameters, PhaseCode has error ﬂoor p. For any signal x ∈ Cn, assuming that the non-zero components of x are lower bounded by Θ(1) and upper bounded by Θ(Kγ) for some positive constant γ < 1, we have
xˆ − x 1 ≤ p x 1(1 + Θ( 1 )), log(K )

with

probability

1

−

O(K

1+γ 2

e−

2K(1−γ)/2 log2 (K )

)

over

the

random-

ized choice of A.

We provide the proof of Corollary 4 in Appendix B. Before presenting the proof of the main theorems, we illustrate the performance of regular and irregular PhaseCode via simulations in Figures 6 and 7. Theorem 2 guarantees that regular PhaseCode recovers a fraction p∗(m) of x with

9

d m(p)
p

5
12.44K 1.1 × 10−3

6
12.72K 8 × 10−5

7
13.28K 3.2 × 10−6

8
13.92K 1 × 10−7

9
14.64K 2.9 × 10−9

10
15.4K 7 × 10−11

TABLE II: Family of trade-offs between error ﬂoor and number of measurements for Phasecode. The table shows that to achieve higher reliability, i.e. smaller error ﬂoor, the number of measurements m should be increased.

Average fraction of non-recovered components

1

K=100, Regular PhaseCode

0.9

K=500, Regular PhaseCode

K=1000, Regular PhaseCode

0.8

K=10000, Regular PhaseCode

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
2 2.2 2.4 2.6 2.8 3 3.2 3.4 M/K, Number of right nodes to sparsity

Fig. 6: Performance of regular PhaseCode Algorithm. We evaluate
Regular PhaseCode algorithm via simulations. We chose the 3rd column of the table as an operating point, i.e., (d, m, p∗(m)) = (7, 13.28K, 3.2 × 10−6). PhaseCode algorithm successfully recovers
almost all active signal components with high probability when
m = 4 × 3.32K = 13.28K.

Average fraction of non-recovered components

1

K=100, Irregular PhaseCode

0.9

K=500, Irregular PhaseCode

K=1000, Irregular PhaseCode

0.8

K=10000, Irregular PhaseCode

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

1.2

1.4

1.6

1.8

2

M/K, Number of right nodes to sparsity

Fig. 7: Performance of PhaseCode Algorithm. We also evaluate Irregular PhaseCode, which demonstrates that it is capacityapproaching. We observe that for K = 10000 irregular PhaseCode can recover almost all the non-zero signal components with m = 4 × 1.3K measurements.

m measurements with high probability, where (d, m, p∗(m)) can be chosen from Table II. We choose the 3-rd column of the table as an operating point, i.e., (d, m, p∗(m)) = (7, 13.28K, 3.2 × 10−6) for regular PhaseCode. We deﬁne the
error probability to be the fraction of non-zero components
of x that are not recovered. We measure the error probability while m is varied between 8K and 14K by averaging over 1000 simulation runs. We repeat the same procedure for several values of K. As expected, the PhaseCode algorithm
successfully recovers essentially all the signal components when m = 13.28K. We also show simulation results for
irregular PhaseCode in Fig. 7 that support Theorem 3. For ex-

Average Runtime of PhaseCode (seconds)

40

PhaseCode

35

Linear Fit

30

25

20

15

10

5

0

0

2000

4000

6000

8000 10000

K, Sparsity

Fig. 8: Time Complexity of PhaseCode. We measure run-time of PhaseCode algorithm. We choose n = 1010 and vary K.

ample, when K = 10000, the coloring algorithm successfully recovers the signal with only 4×1.3K = 5.2K measurements. From the simulations, it is clear that to operate close to capacity, one needs large asymptotics for K.
Theorems 2 and 3 also state that the decoding complexity of PhaseCode is Θ(K), which is order-optimal. In addition to that, its memory complexity is Θ(K), which is also orderoptimal. In order to corroborate the claims, we measure the running time of the PhaseCode Algorithm. We choose the same operating point for regular PhaseCode as in the above simulations. We randomly generate signals of length n = 1010, and increase the sparsity K up to 104 to see how the average runtime scales. The results are plotted in Figure 8; as K increases, the measured decoding time linearly increases. Indeed, PhaseCode successfully recovers K = 104 non-zero components in less then 40 seconds. The exact runtime can be further improved considering that the simulator is written in Python and is not fully optimized, and that the simulation is done on a normal laptop.9
B. Proof of Theorem 2
We ﬁrst provide a brief outline of the proof elements, highlighting the main technical components needed to show that PhaseCode recovers an arbitrarily-close-to-one fraction of non-zero signal components with high probability.
• Density evolution: We analyze the performance of PhaseCode on a typical random bipartite graph (regular or irregular), for a ﬁxed number of iterations, . First, we assume that a local neighborhood of depth 2 of every edge in the graph is tree-like, i.e., cycle-free. Under this assumption, all the messages between right and left nodes, in the ﬁrst j iterations of the algorithm,
9For the measurements, we used a laptop with 2GHz Intel Core i7 and 8GB memory.

Notation pj η d D
λ(x) ρ(x)
Z

Description average fraction of non-recovered signiﬁcant components at iteration j
average degree of right nodes degree of left nodes in d-regular construction truncation level for the harmonic distribution
left edge degree polynomial right edge degree polynomial number of uncolored edges after iterations

TABLE III: Table of Notation for Sections V-B and V-C.

10
c v

are independent. Using this independence assumption, we derive a recursive equation that represents the evolution of the expected number of unresolved components at each iteration. • Convergence to the cycle-free case: : Using a Doob martingale argument as in [36], we show that the 2 neighborhood of most of the edges of a randomly chosen graph from the ensemble is cycle-free with high probability. This proves that PhaseCode decodes all but a small fraction of the left nodes with high probability in a constant number of iterations. The main difference of our convergence analysis compared to [36] is that the right edge degree distribution in our graphs is Poisson distributed, while the right degree is regular in [36].
At each iteration of PhaseCode, we call the giant component as the largest set of signal components (left nodes) that have been resolved relative to each other. The algorithm follows 3 major steps to recover the active left nodes by coloring them.
• Step 1: All the singleton right nodes and their corresponding left nodes are detected.
• Step 2: Strong doubletons are detected, and the color of the corresponding 2 left nodes get merged. We call the largest set of left nodes that chain hands together through these strong doubletons to be the giant component at this step.
• Step 3: After the initial giant component is formed, at each iteration of the algorithm, left nodes are colored one at a time through resolvable multitons, and become part of the giant component.
Now we analyze the message passing algorithm. A left node v passes a 0 message to neighbor right node c if it is not colored (i.e. it is not part of the giant component). Let pj be the probability that a random message sent from a left node to a right node is 0, at iteration j of the algorithm. The density evolution equation is an equation relating pj to pj+1. Similarly, a right node c passes a message 0 to neighbor left node v if it can not get colored (become part of the giant component). Let qj be the probability that a random message sent from a right node to a left node is 0, at iteration j of the algorithm. Under the tree-like assumption, and for j ≥ 2 one has

pj+1 = (1 + e−η − e−ηpj )d−1.

(14)

Here is a proof of Equation (14). A left node v passes a 0
message to right node c at step j + 1, if all of the other d − 1
neighbor right nodes of v pass message 0 to v at step j. That is pj+1 = qjd−1. Note that for j ≥ 2, if a right node is a singleton, it passes message 0 to neighbor left nodes, since

Fig. 9: Length-2 tree-like neighborhood of (v, c) for d = 4. The neighborhood is the subgraph of all the edges and nodes of paths having length less than or equal to 2, that start from v and the ﬁrst edge of the path is not (v, c).
in PhaseCode only resolvable multitons can color active left nodes after the second step of the algorithm.
We calculate qj as follows. A right node c sends a message to a left node v that it is part of the giant component if c is connected to a non-empty set of left nodes other than v, and those left nodes are all in the giant component. Thus,
∞
1 − qj = ρi(1 − pj)i−1 = ρ(1 − pj) − ρ1
i=2
= e−ηpj − e−η.
This proves (14). See Figure 9 for an illustration of the proof for the case d = 4.
Remark Note that if a right node is a singleton, it cannot recover the corresponding active left node in both phase and magnitude. This is a fundamental difference of our decoding process compared to that of conventional peeling-based decoders such as the LDPC decoder for erasure channel [24]. In LDPC decoding, since there is no phase ambiguity, as soon as a singleton is detected, the corresponding non-zero component is recovered and it is peeled from all other right nodes that are connected to that component. However, active left nodes in singletons cannot be peeled in our setting. Indeed, our problem has the peculiar attribute that singleton right nodes, while critical to initiating the growth of the giant component at the outset, are not useful once a giant component is formed, and too many singletons actually hurt the system performance by featuring useless isolated measurements. This is a signiﬁcant departure from “phase-aware” measurement systems like LDPC codes. This is also the key reason to why our density evolution equation in (14) differs from that of linear measurement systems [24], [26], which is pj+1 = (1 − e−ηpj )d−1.
An interesting but unfortunate fact is that p0 = 1 is a ﬁxed point of the density evolution equation. Thus, one cannot use (14) at the outset to follow the evolution of pj, and to argue that it goes close to 0, since pj can get stuck at 1. To use Equation (14), we need a more careful characterization of the ﬁrst two steps of the algorithm that form the giant component. At the ﬁrst iteration, all the (active) left nodes that are connected to at least one singleton right node are found. Since the relative phase of these signal components is not known, no giant component is formed yet; thus, p1 = 1. At the second iteration, the giant component is formed by merging

11

the colors of left nodes in strong doubletons. Recall that a
strong doubleton right node is a right node that is connected
to two colored left nodes. After the giant component is formed
in the second iteration, the probability that a randomly chosen left node is not part of the giant component is p2. If one can show that p2 is small enough such that after a ﬁxed number of iterations pj gets close to 0, then concentration bounds can be used to show that the number of left nodes not being in the
giant component is indeed highly concentrated around its mean after iterations, that is Kp . In Lemma 7, we show that if p2 = 1 − δ for some arbitrary constant 0 < δ < 1 independent of K, pj gets close to 0 after a constant number of iterations. Clearly p2 = 1 − δ if there exists a giant component of size linear in K after the second step. In Lemma 5, we ﬁnd the condition for left-regular bipartite graph under which a linear
size giant component will be formed after the second step of
the algorithm.

Lemma 5. There exist operating points (d, M = cK) for which with probability 1−O(1/M ), a giant component of size linear in K is formed after the second step of PhaseCode. In particular, (d = 5, 3.11 ≤ c ≤ 19.24) and (d = 8, 3.48 ≤ c ≤ 55.36) are two of these operation points.

See Appendix C for the proof.

Remark Lemma 5 shows that for large enough m, a positive fraction of the signal components can get recovered after the second iteration of the algorithm. Thus, PhaseCode gets a proper jump-start, which is essential for proving that the algorithm terminates after a constant number of iterations, and successfully recovers an arbitrarily-close-to-one fraction of the signal components.

Remark As one observes in Lemma 5, if M is larger than some threshold (which corresponds to more measurements), the giant component will not get formed. At a ﬁrst glance, this sounds counter-intuitive since having more right nodes seems to only help. However, one should keep in mind that in the statement of the lemma, the left degree d is kept ﬁxed. Intuitively, when M is too large, for a ﬁxed small d, the bipartite graph (with active left nodes) becomes so sparse that there are too few doubletons to form a giant component.

Corollary 6. There exists a constant 0 < δ < 1 independent of K, such that p2 = 1 − δ.

Due to the formation of a linear-size giant component in step 2 of the algorithm, we can revisit the density evolution equation (14):

pj+1 = (1 + e−η − e−ηpj )d−1,

with the aid of Corollary 6, which guarantees that p2 is strictly smaller than 1. Recall that p0 = 1 is a ﬁxed point of (14). But with the giant component formation, we can break away from the shackles of “being stuck” at p0 = 1. With p2 < 1, we hope to ﬁnd a better ﬁxed point of (14) to which our density
evolution will converge. Towards this end, ideally one wants Equation (6) to have
the property

pj+1 = (1 + e−η − e−ηpj )d−1 < pj ,

(15)

for all pj ∈ (0, 1). Let’s take a closer look at the ﬁxed point

equation

t = f (t) = (1 + e−η − e−ηt)d−1.

(16)

As mentioned, one solution is t∗1 = 1. As we can break away from t∗1, fortunately there exists another solution approximately at t∗2 e−η(d−1) which is close to 0. To see this, consider the equation y = (1 + e−η − e−ηx)d−1. Suppose

that 0 < x = e−η(d−1)

1. Then, e−ηx

1 and

1+e−η −e−ηx e−η. Thus, y = x which shows that e−η(d−1)

is approximately another ﬁxed point of (14).10 From now on,

we will refer to this ﬁxed point as the error ﬂoor p∗.

Lemma 7. Let d = 5. If 2.33K ≤ M ≤ 13.99K, then the
ﬁxed point equation (16) has exactly 2 solutions for t ∈ [0, 1]: t∗1 = 1 and t∗2 e−η(d−1) (See Figure 10). For d = 8, a similar result holds if 2.63K ≤ M ≤ 47.05K.

See Appendix D for the proof. The following corollary is a direct result of Lemma 7.
Corollary 8. For any > 0, there exists a constant ( ) such that p ≤ p∗ + .
Table IV illustrates how the error ﬂoor p∗ and the minimum ratio of right nodes to active left nodes c = M/K change for different values of d. If our reliability target allows the error ﬂoor to be set at 1.1 × 10−3, then d = 5 minimizes the number of required right nodes. Recall that the total number of measurements is m = 4M which matches the result of Table II. (See Section IV) If one wants to achieve smaller error ﬂoor, then d and c should be both increased.
In the density evolution analysis so far, we have shown that the average fraction of active signal components that cannot be recovered will be arbitrarily close to the error ﬂoor after a ﬁxed number of iterations, provided that the tree-like assumption is valid. It remains to show that the actual fraction of left nodes that are not in the giant component after iterations is highly concentrated around p . Towards this end, ﬁrst in Lemma 9 we show that a neighborhood of depth of a typical edge is a tree with high probability for a constant . Second, in Lemma 10, we use the standard Doob’s martingale argument [36], to show that the number of active signal components that are not recovered after iterations of the algorithm is highly concentrated around Kp .
Consider a directed edge e = (v, c) from a left-node v to a right-node c. Deﬁne the directed neighborhood of depth of (e) as Ne , that is the subgraph of all the edges and nodes on paths having length less than or equal to , that start from v and the ﬁrst edge of the path is not e. As an example, the directed neighborhood of depth 2 of (e) is shown in Figure 9.
Lemma 9. For a ﬁxed ∗, Ne2 ∗ is a tree-like neighborhood with probability at least 1 − O(log(K) ∗ /K).

The proof is provided in Appendix E.

Lemma 10. Over the probability space of the ensemble of

d-left-regular

graphs

C

K 1

(

d,

M

),

let

Z

be

the

number

of

un-

10Of course, one can easily ﬁnd the exact solution to (16), using numerical methods for given values of d and η.

12

d

4

5

6

7

8

9

10

p∗ 2.7 × 10−2 1.1 × 10−3 8 × 10−5 3.2 × 10−6 1 × 10−7 2.9 × 10−9 7 × 10−11

c

3.31

3.11

3.18

3.32

3.48

3.66

3.85

TABLE IV: The table shows how the error ﬂoor, p∗, and c = M/K (which indirectly determines the number of measurements) vary for different values of left degree, d. The minimum value of c is 3.11 that is achieved when d = 5. Moreover, one can see that p∗ decreases as
d increases.

1

0.8

pj

0.6

0.4

0.2

0 5

10

15

20

25

30

j

(a) The density evolution curve for parameters d = 5 and η = 2.

(b) The evolution of pj after each iteration for d = 5 and η = 2.

Fig. 10: Figure (a) illustrates the density evolution equation, pj+1 = f (pj), for Regular PhaseCode. In order to track the evolution of pj, pictorially, one draws a vertical line from (pj, pj) to (pj, f (pj)), and then a horizontal line between (pj, f (pj)) and (f (pj), f (pj)). Since the two curves meet at (1, 1) if p0 = 1, then pj gets stuck at 1. However, if p0 = 1 − δ, pj decreases after each iteration, and it gets very close to 0. Figure (b) illustrates the same phenomenon by showing the evolution of pj versus the iteration, j. Note that in this example, pj gets very close to 0 after only 20 iterations.

colored edges11 after iterations of the PhaseCode algorithm. Then, for any > 0, there exist a large enough K and constants β and γ such that

|E[Z] − Kdp | < Kd /2

(17)

P(|Z − Kdp | > Kd ) < 2e−β 2K1/(4 , +1)

(18)

where p is derived from the density evolution equation (14).
The proof is provided in Appendix F.
Now gathering the results of Corollary 8 and Lemmas 5 and 10 completes the proof of Theorem 2. Note that since the construction of the bipartite graph is random, the fraction p of the non-zero components that can be missed are distributed uniformly at random among the K non-zero components. Indeed, the missed components are only a function of the graph structure that has a distribution which is oblivious to the indices of the left nodes by construction. Further, note that the dominant probability of error is due to the event that the giant component is not formed in the second iteration which happens with probability O(1/K). It is worth mentioning that Lemma 9 is used only to prove Lemma 10. Thus, the event that an edge does not have a tree-like neighborhood, which
∗
log(K) ), is not an error event of happens with probability O( K the algorithm. Given that a giant component has been formed after the second step of the algorithm, the error event of the algorithm is the event that more than a fraction p of the nonzero signal components are missed, and the probability of such event is upper bounded in (18).

11An edge is colored if its corresponding left node is colored.

C. Proof of Theorem 3

Recall that we design the left degree distribution λ(x) = i≥1 λixi−1 of Irregular PhaseCode as follows: λi = 0 for i ≥ D + 1 and

λi = 1 × 1 , 2 ≤ i ≤ D,

(19)

i − 1 h(D − 1)

where D is a (large) constant and h(x) =

x i=1

1/i.

We design the number of right nodes to be M = K/(1 −

) K(1 + ). How to choose constants D and will be

shortly clariﬁed in Lemma 12. The average degree of left

nodes (of the pruned graph with K active left nodes) is d¯ = i1λi/i . To see this, let E be the number of edges of the graph. Then, the number of left nodes of degree i is Eλi/i

since λi is the fraction of edges with degree i on the left.

Thus, the number of left nodes is i Eλi/i. So the average

left degree is

d¯ = E = i Eλi/i
Thus, with our design,

1. i λi/i

D
d¯ = (

λi )−1 = h(D − 1)

D

.

i=2 i

D−1

Consequently, the Poisson density parameter of the right-node

degree distribution is:

K d¯

D

η = M = h(D − 1) D − 1 (1 − ).

Lemma 11. Let f (x) = λ(1 + e−η − e−ηx). The ﬁxed point equation x = f (x) has exactly two solutions, x∗1 = 1 and 0 <

13

x∗2 < 1, in the interval x ∈ [0, 1]. Furthermore, if f (1) > 1, then f (x) < x for x ∈ (x∗2, 1).
See Appendix G for the proof. As shown in Lemma 11, given that f (1) > 1, the density evolution has a ﬁxed point at 1, and the other ﬁxed point of the equation is approximately p∗ λ(e−η), which corresponds to the error ﬂoor of the algorithm. In the following lemma, we show that for any arbitrarily small numbers p∗ and , there exists a large enough constant D(p∗, ) such that f (1) > 1. This shows that with only 4M = 4K/(1 − ) 4K(1 + ) measurements, Irregular PhaseCode algorithm can recover an arbitrarily-close-to-one fraction of the non-zero signal components. So given that the coloring procedure starts (the density evolution equation can be started from 1−δ), Irregular PhaseCode is capacity-approaching. Now we show that a linear size giant component of colored left nodes can be formed similar to Lemma 5 using a second stage of only m = K extra measurements. By assumption of Theorem 3, the support of the non-zero components of the signal is uniformly random. Now ﬁx some arbitrarily small constant δ > 0. Let x˜ be the vector of the ﬁrst δ n components of the signal. By the law of large numbers, the number of nonzero elements of x˜ is δ K + o(K). Consider the sub-problem of forming a giant component of size linear in K in x˜. By Lemma 5, one can design m = 14δ K measurements to form the giant component. Thus, = 14δ . Since δ can be made arbitrarily small, can also be made arbitrarily small. The main lemma for establishing the proof of Theorem 3 is as follows.
Lemma 12. For any p∗ > 0 and any > 0, there exists a large enough constant D( , p∗) such that M = K(1 − )−1 K(1+ ) is the number of right nodes (bins), and pj converges to p∗ as j goes to inﬁnity.
See Appendix H for the proof.
Corollary 13. Given that p2 = 1 − δ, for any 1 > 0, there exists a constant ( 1) such that p ≤ p∗ + 1.
The rest of the proof is similar to Theorem 2. It remains to show that the actual fraction of active signal components that are not recovered after iterations is highly concentrated around p . Since the maximum degree of left nodes is again a constant D, the exact procedure in Section V-B (Lemmas 9 and 10) can be used to get a similar concentration bound as in Lemma 10. Now the total number of measurements is m = 4K(1 + ) + m = 4K(1 + + ). Since and can be made arbitrarily small, the proof of Theorem 3 is complete.
VI. FOURIER-FRIENDLY PHASECODE
In some applications such as optical imaging [9], [21], the design of the measurement matrix cannot be arbitrary. In optical imaging, the object of interest, signal x, can be passed through an optical diffraction pattern or a mask and an optical Fourier lens. A typical setup for optical imaging is shown in Figure 12. With a complex-valued mask, we can modulate each component of the signal xi by some complex number di, while the lens takes the Fourier transform of the signal.

For example, consider passing the signal through a mask and then Fourier lens which is common in optical imaging. The output of this transform is F Dx, where F is the DFT matrix of length n and D ∈ Cn×n is a diagonal mask matrix (Figure 13). In general, it is possible to have multiple stages of masks and lenses. While increasing the number of stages can make the system more complex, in many optical systems, having up to two stages is considered practical [37], [38]. In our proposed solution, we will have two masks for all measurements.
In this section, we show how one can have a Fourierfriendly implementation of the set of measurements described in previous sections. We ﬁrst provide an overview of the result of [26] on constructing a sparse-graph code using “Chinese Remainder Theorem”, in Subsection VI-A. In Subsection VI-B, we show how our proposed measurements can be obtained in a Fourier-friendly setup, with the aid of the result of [26].

A. Ensemble of Graphs Constructed by Chinese Remainder Theorem

In this subsection, we provide a brief overview of the result

in [26] that uses the “Chinese Remainder Theorem” (CRT)

to construct a deterministic and well-structured coding matrix

that is also of practical interest. We use this construction

to design a Fourier-friendly measurement matrix. For more

details about the theory of the ensemble of graphs constructed

by the CRT, we refer the readers to [26].

In Section V-B, we analyzed the performance of PhaseCode

for the ensemble of graphs C1K (d, M ). In this ensemble, each

left node is connected to exactly d right nodes randomly. Now

we

consider

another

ensemble

C

K 2

(F

,

m)

.

Deﬁne

the

set

F

as

F = {f1, f2, . . . , fd}. Partition the right nodes into d sets. Let

the number of right nodes in stage i be fi; thus,

d i=1

fi

=

m.

In this construction, each left node is connected to exactly

one right node per stage randomly. Therefore, we again end up with having a bipartite graph with left regular degree d. Assuming that fi = F + Θ(1) for all i and consequently F = Θ(K), the edge degree distribution of the right nodes does not change for large enough K and is given in (12).

Therefore, the tree analysis and the density evolution equation

stated in (14) remain the same, and one can essentially get all

the previous results using this ensemble.

Note that sampling a graph from C2K (F , m) has no practical advantage over sampling from the ensemble C1K (d, M ). However, we use the CRT to show that if the K non-zero

components of the signal is chosen uniformly at random with
replacement from the n components, and if K is in the sublinear regime (more speciﬁcally, K = nδ for some δ ∈ (0, 1)),

one can design a deterministic coding matrix which consists of d stages of sub-matrices with rows that are circularly-

shifted versions of a deterministic subsampling pattern. The subsampling rate at stage i is fi. In the following example, we demonstrate how the deterministic matrix is constructed.

Example 3. Suppose that the coding matrix has two stages with f1 = 2 and f2 = 3. Assume that n = 6. Then, the coding

14

f(p) pj

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

p

1

0.8

0.6

0.4

0.2

0

0

10 20 30 40 50 60 70 80 90

j

(a) The density evolution curve for parameters K = 105, = 0.1 and D = 103.

(b) The evolution of pj after each iteration for parameters K = 105, = 0.1 and D = 103.

Fig. 11: Figure (a) illustrates the density evolution equation for Irregular PhaseCode, which is similar to Figure 10a. Figure (b) illustrates the same phenomenon showing the evolution of pj versus the iteration, j. Note that in this example, since = 0.1 and we are operating very close to the capacity, pj gets very close to 0 after around 90 iterations, which is much larger than around 20 iterations needed by Regular PhaseCode so that pj gets very close to 0. The reason is that the gap between the two curves in (a) gets smaller once the number of measurements is close to the capacity.

light source

object

mask

lens

detector

Fig. 12: A typical setup for many optical system where the object of interest is passed through a coded diffraction pattern or a mask , and then through a Fourier lens.

x
input signal

Mask

Lens

y |.|
magnitude measurements

Fig. 13: The block diagram of an optical imaging system where signal x is passed through a mask (modulated by a diagonal matrix), and then passed through a lens (DFT matrix). The magnitude block, |.|, is showing that the phase information is not available in the measurements.

matrix is





101010

 0 1 0 1 0 1 

 1 0 0 1 0 0  .

0 1 0 0 1 0

001001

Now, we formally deﬁne the ensemble of graphs constructed

by the CRT. First, assume n =

d i=1

fi

(i.e.

K

=

Θ(n1/d)).

Partition the set of m =

d i=1

fi

right

nodes

to

d

stages

in

the trivial way. Suppose that the K non-zero components of

the signal are chosen uniformly at random with replacement

from the n components. Note that the “with replacement”

assumption might lead to having a signal with less than K

non-zero components, but this is only a technical assumption

that we need to make, and via simulations we will show

the good performance of the CRT-based code for exactly K-

sparse signal. Let I = (i1, i2, . . . , iK ) denote the non-zero components where 1 ≤ ik ≤ n, 1 ≤ k ≤ K. We associate

the integers from 0 to n − 1 to d numbers (r1, r2, . . . , rd) using the CRT, where 0 ≤ ri ≤ fi − 1; thus, ik uniquely

determines one right node per stage. The way this association

is done will be explained shortly. Then, each active left node

ik is connected to the associated set of right nodes that are determined by (r1, r2, . . . , rd). The ensemble C3K (F , m) is the
collection of all the graphs that are constructed as described.

Furthermore, the uniformly at random selection of I makes

sure that all these graphs occur with equal probability. See

[26] for details.

To show how we associate I to (r1, r2, . . . , rd), we need

to review the Chinese Remainder Theorem. Let n =

d i=1

fi

and fi’s are pairwise co-prime positive integers. The theorem

states that every integer n between 0 and n − 1 is uniquely

represented by the sequence (r1, r2, ..., rd) of its remainders modulo f1, f2, . . . , fd respectively and vice-versa. We use this unique CRT mapping to associate the active left nodes with d

right nodes.

Lemma 14.

[26]

The

ensembles

C

K 2

(

F

,

m)

and

C

K 3

(F

,

m)

are identical.

Proof:

Clearly,

C

K 3

(F

,

m)

⊂

C2K (F , m).

The

reverse

is

also true by CRT since there is a unique integer between 0 to

n − 1 with remainders ri modulo fi for all i.

Figure 14 demonstrates the performance of PhaseCode with

two

ensembles:

C

K 1

(

d,

M

)

and

C

K 3

(F

,

m)

.

We

choose

d

=

7

and F = {47, 49, 50, 53, 57, 59, 61}. Thus, M =

d i=1

fi

=

376. We varied the value of K (107 ≤ K ≤ 170) such that

M/K varies between 2.2 and 3.5. Each point is averaged over

10000 runs to determine the error probability. One can observe

negligible difference between the performance of the algorithm

for the two ensembles.

In the following we provide remarks of how one can extend

the above construction of CRT.

Remark In the above example of CRT construction, we implicitly assumed K = Θ(n1/d). The technique can be extended to cases where K = Θ(nα/d) for 0 ≤ α < d. Instead

15

Average fraction of non-recovered components

1

Random Bipartite Graph

0.9

CRT

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 2.2

2.4 2.6 2.8

3

3.2 3.4

M/K, Number of right nodes to sparsity

Fig. 14: Comparison of random left-regular bipartite graph

ensemble and CRT ensemble. We choose the left degree d =

7, and construct an appropriate CRT ensemble based on F =

{47, 49, 50, 53, 57, 59, 61}. The number of right nodes is determined

by F, i.e., M =

d i=1

fi

=

376.

Each

operating

point

is

averaged

over 10000 runs. We observe negligible difference in performance

between the two ensembles.

of using F as heights of the d stages of the bipartite graph, we use F = {f1, ..., fd}, where

α−1

fi =

f((i+j) mod d)+1.

j=0

For example, if α = 2 and d = 7, one can convert a set of coprimes
{f1, f2, f3, f4, f5, f6, f7}

to the set

F = {f1f2, f2f3, f3f4, f4f5, f5f6, f6f7, f7f1}.

Then, M =

d i=1

α−1 j=0

n((i+j)

mod

d)+1

=

Θ(n2/d),

which

is in the order of K. Because F can be chosen from a dense

set of coprimes, one can always choose it carefully to induce

a right number of measurements. For the most general case

where K = Θ(np/q) and 0 ≤ p/q < 1, one can use a similar

extension and construction by ﬁnding q coprimes and stacking

p of them in each stage. We omit details of the technique and

refer interested readers to [26].

B. Fourier-Friendly Compressive Phase Retrieval
Without loss of generality, we consider only a 1-D case for x here, though our arguments extend in a straight-forward way to 2-D images as well. Suppose that the signal of interest x is sparse in the Fourier domain, which is of interest in many optical imaging settings. Let X = F x be the Fourier transform of the signal. In Subsection VI-A, we showed that the coding matrix H can be realized using d stages of circulant matrices without changing the performance of sparse-graph codes. To have a Fourier-friendly implementation of the CRT code matrix, we expand each stage of the fi × n matrix to a circulant n × n matrix. Let C denote this circulant coding matrix for one stage. In the following, we show that how using

x Mi
input signal

F Camera zi ∈ Cn

 

zi(1)

 

yi

=

  

zi(..2)

  





zi(fi)

Fig. 15: The block diagram of Fourier-friendly compressive phase retrieval using the CRT matrix. The ﬁgure shows stage i of the CRT matrix (1 ≤ i ≤ d). The signal of interest, x, is passed through a binary mask corresponding to stage i, and then the Fourier lens. The output of this experiment is signal zi of length n. However, these n measurements are not unique; they are n/fi replicas of fi unique measurements. Thus, the camera only reads the ﬁrst fi components of zi.

our proposed CRT code matrix, one can have access to all
the necessary measurements using only diagonal masks and
lenses. Note that we are interested in measurements of the modulated signal by complex exponentials such as eiω or by magnitude modulators cos(ω ). First let us see how the plain
measurements without these modulations can be obtained if
the coding matrix is circulant. The plain measurements are | j CijXj|. Since C is circulant, the eigenvectors of C are the columns of a unitary Fourier matrix [39]. Thus, the eigenvalue decomposition of C is C = F DF −1 for some diagonal matrix D. Hence, we construct our measurements
by modulating the signal x with the diagonal mask D and
then taking a Fourier transform by using an optical lens:

|F Dx| = |F F −1CF x| = |CF x| = |CX|.

For each stage of the CRT code matrix (there are d stages
overall), we need one physical experiment. The physical experiment corresponding to the i-th stage, where 1 ≤ i ≤ d, gives us n/fi replicas of fi unique measurements in one
shot. As illustrated in Figure 15, for each experiment, the camera measures only one copy of the fi measurements. Let yi ∈ Cfi be the measurements corresponding to stage i. Then, the measurements of the different stages are gathered to form the measurement vector y ∈ Cm as follows:

y = [yT1 , yT2 , . . . , yTd ]T .

Thus, the actual sample complexity is still m = Θ(K ).

d i=1

fi

=

Now we explain how one can get access to all the necessary

measurement y1,i to y4,i. We explain the construction for y1,i. Other measurements can be similarly realized. We use 3 blocks of Fourier transforms (lenses) and 2 masks as follows. Let D˜ be a diagonal matrix such that d˜ = eiω . We are interested in constructing the measurements of the form |CD˜ X|. This can be done by using two masks, D˜ and D, with three Fourier

lenses as follows.

|F DF D˜ F x| = |F F CF −1F D˜ X|

(20)

= |F 2CD˜ X|.

(21)

16

Note that F 2 is just a permutation matrix so we can construct all the measurements y1,i using only two masks and Fourier lenses.
Remark Since each optical lens is equivalent to a Fourier transform, we can also implement a compressive Fourierfriendly phase retrieval algorithm, when x is sparse (and X is not sparse) by just adding an optical lens to the measurement system as follows. Suppose that A is a Fourier-friendly measurement system that is able to recover x, when X is sparse. That is, one is measuring the sparse signal X with measurement matrix AF −1. Then, AF is a Fourier-friendly measurement matrix that is able to recover x, when x is sparse since AF x = AF −1F 2x. Note that F 2 is just a permutation matrix; thus, F 2x is a sparse signal that is again measured by AF −1.

VII. ROBUST PHASECODE

In this section, we consider the noisy compressive phase retrieval problem. The noisy compressive phase retrieval problem is to recover a K-sparse complex signal x, from a set of quadratic measurements

yi = aHi x 2 + wi, i ∈ [m],

where aHi ∈ Cn are rows of the measurement matrix A ∈ Cm×n, wi’s are noise, and [m] denotes the set {1, 2, . . . , m}. We consider the regime where there exist two constants β and δ such that K = βnδ, δ ∈ (0, 1). We assume that wi’s are independent, zero-mean, sub-exponential [40] random variables. This model is considered in many phase retrieval
literatures [14], [15], [41]. We also assume that signal x is quantized, which means
that the components of x lie in a ﬁnite set of complex numbers. More speciﬁcally, let Lm and Lp be the number of possible magnitudes and phases of the non-zero components, respectively. Then, each component of x is in the set

S

=

{uεe

i

2π

(v− Lp

1)

|u

∈

[Lm],

v

∈

[Lp]}

∪

{0}

⊂

C,

where ε > 0. Quantized signals can be good approximations of the real world signals and are natural for signal processing with computers [42], [43].
We propose two schemes to robustify PhaseCode in the presence of noise: almost-linear scheme and sublinear scheme. The main results of this section are the following theorems.

Theorem 15. The almost-linear scheme can recover a random fraction 1 − p, for arbitrarily small p, of the non-zero elements of x with probability 1−O(1/K), with Θ(K log(n)) measurements. The computational complexity of the algorithm is Θ(LmLpn log(n)).

Theorem 16. The sublinear scheme can recover a random fraction 1 − p, for arbitrarily small p, of the non-zero elements of x with probability 1 − O(1/K), with Θ(K log3(n))
measurements. The computational complexity of the algorithm is Θ(LmLpK log3(n)).

See the proofs of Theorems 15 and 16 in Appendix I and L. Details of the measurement design and the decoding algorithm are shown in the following subsections.

A. Almost-linear Scheme
The idea of the almost-linear scheme is to encode the columns as different patterns. With the number of measurements in each right node being Θ(log(n)), the patterns are guaranteed to be different enough, so that we can successfully resolve singletons, mergeable multitons, and resolvable multitons.
1) Design of Measurements: Instead of using the 4-by-n trigonometric modulation matrix, we use a new random matrix A0 = {aij}P ×n whose entries are i.i.d. with the following distribution:

0, with probability 1/2

aij = eiθij , with probability 1/2,

(22)

where θij’s are i.i.d. and uniformly distributed in [0, 2π). We call A0 the test matrix, and we can show that we need P = Θ(log(n)) for each right node to achieve successful recovery.
For the almost-linear algorithm, the measurement matrix of the lth right node is Al = A0diag(hl). Without loss of generality, we omit index l, and simply use h to denote the
coding pattern (the left nodes connected to the right node) of
a right node. Then the measurements of this right node are

yi = aHi diag(h)x 2 + wi, i ∈ [P ],

(23)

where aHi is the ith row of A0, and the noise wi ∈ R, i ∈ [n] satisﬁes the properties mentioned earlier. To simplify notation, we deﬁne a linear map A from Cn×n to RP :

A : Z → {aHi Zai}i∈[P ].

(24)

Now according to (23), by deﬁning z = diag(h)x, we have y = A(zzH) + w, where y = {yi}i∈[P ] and w = {wi}i∈[P ]
are the measurement vector and noise vector, respectively. We

call z the true signal corresponding to this right node.

2) Decoding Algorithm: As mentioned earlier, the PhaseC-

ode algorithm requires the measurements in each right node to

enable three operations: detecting singletons, resolving strong

doubletons, and detecting resolvable multitons. Using our new

measurement system, these operations can be done reliably

by a simple guess-and-check method: we guess all possible

indices, magnitudes, and relative phases, and use an energy

test to decide whether our guess is correct. For any of the

three operations, we make a hypothesis on the unknown index,

magnitude, and phase of the true signal z and construct the corresponding hypothesis signal zˆ. For example, when we

do singleton detecting, if our hypothesis is that the right

node is a singleton, and that the location index of the active

component is 5 with the magnitude being 3ε, we construct zˆ = 3εe5, where ei denotes the ith vector of the canonical basis. Similarly, we can resolve strong doubletons. For in-

stance, suppose that we know that a right node is connected

to two active components which are located at positions 2 and

5, respectively, and we also know the magnitudes of the two

components are 2ε and 3ε, respectively. Then, if we can make

a

hypothesis

that

the

relative

phase

is

π 4

,

we

can

construct

zˆ

=

2εe2

+

3εei

π 4

e5

.

Then,

we

need

to

check

whether

our

17

hypothesis is correct. To do this, we perform an 1 norm energy test shown in (25):

zˆ ∼ z, if 1 y − A(zˆzˆH) < t0,

P

1

(25)

zˆ z, otherwise,

where zˆ ∼ z means zˆ and z are equal up to a global phase, and t0 is the threshold. The intuitive reason why we do this test is that when zˆ ∼ z, A(zˆzˆH) = A(zzH), then y − A(zˆzˆH) = w, whose energy should be small. Conversely, when zˆ z, the energy of y − A(zˆzˆH) should be large. Here, we give a
result on the error probability of the energy test.

Lemma 17. When P = Θ(log(n)) and ε is appropriately large, with proper threshold t0, the error probability of the energy test shown in (25) is O(1/n2).

The proof of this lemma follows the similar idea which
appears in Lemma 14 in [44]. We can also show that we need to perform Θ(n) energy tests before the algorithm stops. Then,
using Lemma 17 and some basic principles in probability
theory, we can show that the failure probability of the almostlinear scheme is O(1/K). As for the sample and computational complexity, since we have Θ(log(n)) measurements for each right node and Θ(K) right nodes, the sample complexity of the almost-linear scheme would be Θ(K log(n)); and since the computational cost of each test is Θ(LmLp log(n)) and there are Θ(n) tests, the computational complexity of the almost-linear scheme is Θ(LmLpn log(n)).

B. Sublinear Scheme
Although the O(n log(n)) computational complexity of almost-linear scheme is compelling, we can further improve the computational complexity. Recall that in the noiseless scenario, we get the location index of the active component in a singleton and the non-recovered active component in resolvable multitons by only decoding the measurements of a recoverable right node. Based on this idea, we propose the sublinear scheme for the noisy scenario, which can achieve much lower computational cost compared to the almost-linear scheme, at the cost of slightly larger sample complexity.
1) Design of Measurements: In the sublinear scheme, the measurement matrix for each right node is designed to be a concatenation of the test matrix A0 deﬁned in the almostlinear scheme and R index matrices F 1, . . . , F R. The test matrix A0 is still used to perform the energy tests and the index matrices are used to ﬁnd the location indices.
Now we show how to design the index matrices. The main idea is to encode each column as a binary code such that we can directly decode the column index of the component to get recovered from the measurements. A similar idea is also used in the Chaining Pursuit method [45]. First, we deﬁne a deterministic matrix B = {bij} ∈ {0, 1}R×n, where R = log n , and the ith column of B is the binary representation of the integer i − 1. For example, when n = 4, we have,
B = 00 01 10 11 .

We use bi and Bj to denote the ith row and jth column of

B, respectively. Let F 0 ∈ CQ×n be a random matrix whose

elements are i.i.d. and uniformly distributed on the unit circle,

and F = F 0 ⊗ B ∈ CRQ×n. This means we have F =

[

F

H 1

F

H 2

·

·

·

F

H R

]H

,

where

Fi

= F 0diag(bi) ∈ CQ×n. By

concatenating with the test matrix, the measurement matrix of the lth right node is Al = [AH0 F H]Hdiag(hl) ∈ C(P +QR)×n. Here, we give a simple example of Al. Let n = 4 and thus

R = 2. We have





A0,1 A0,2 A0,3 A0,4

Al =  0

0 F 0,3 F 0,4  diag(hl), (26)

0 F 0,2 0 F 0,4

where A0,i’s and F 0,i’s are the columns of A0 and F 0. We can show that we need Q = Θ(log2(n)) to reliably ﬁnd the correct location index and we also need P = Θ(log(n)) to perform energy tests.
Consequently, there are R + 1 sets of measurements. The ﬁrst set y0 = {y0,i}i∈[P ] is the same as the measurements in almost-linear scheme and is called the test measurements:
y0,i = aHi z 2 + w0,i, i ∈ [P ],

where z = diag(h)x and is still called the true signal. The other R sets yj = {yj,i}i∈[Q], j ∈ [R] correspond to the index matrices and are called the index measurements. Each set is composed of Q measurements:
yj,i = f Hj,iz 2 + wj,i, i ∈ [Q], j ∈ [R],

where f Hj,i is the ith row of F j. We also let wj’s be the noise vectors, j ∈ {0} ∪ [R].
2) Decoding Algorithm: The sublinear scheme can ﬁnd
the location index by only looking at the measurements. For
example, assume that a right node with measurement matrix in
(26) is a singleton whose non-zero component is at position 2.
Then, the decoder can see that the elements of the ﬁrst set of index measurements y1 have small absolute value since these measurements only contain noise. Now the decoder knows that
the non-zero element should be in the ﬁrst half of the signal. Then, the decoder observes that the elements in y2 have large energy. The decoder knows that if the right node is indeed a
singleton, the only possible index of the non-zero component
would be 2. Actually this procedure is a binary search on all the n indices of the signal. After this indexing process, the decoder can use the same procedure as the almost-linear scheme to construct a signal zˆ as the hypothesis of the true signal of this right node, and then use the testing measurements
to perform the same energy test.
Now we formally show the details of the fast index search. Assume that |supp(z)| = T , and there are Ts non-recovered active components connected to the right node. More speciﬁcally, z = zc + zs, |supp(zs)| = Ts, supp(zc) ∩ supp(zs) = ∅, and we know a vector zˆc ∼ zc. Note that when T = Ts = 1, we have zˆc = zc = 0. Our goal is to ﬁnd the index ls of the non-zero element in zs when Ts = 1 and supp(zs) = {ls}. When T = 1 and T > 1, we are looking for non-zero component in a singleton and non-recovered non-zero com-
ponent in a resolvable multiton, respectively. We subtract the

18

measurements contributed by the signal components which are already known as follows. Let yˆj,i = |f Hj,izˆc|2; then, y˜j,i = yj,i − yˆj,i. We perform the following index tests for
j ∈ [R] with threshold t1 > 0 to get ls:

˜b = 0, if

1Q y˜

<t ,

j

Q

j,i

1

(27)

i=1

˜bj = 1, otherwise.

The index tests output a binary string b˜ = {˜bj}j∈[R]. Note that if Ts > 1, we still get an output after the index tests, but the energy test with the test measurements prevents us from making mistakes. Lemma 18 states that with high probability ˜bj = bjls .
Lemma 18. When Q = Θ(log2(n)), with proper threshold t1, if supp(xs) = {ls}, then P{˜bj = bjls } = O(1/K3).

Similar to the almost-linear scheme, using Lemma 18,
we can prove that the failure probability of the sublinear scheme is O(1/K). Since the total number of measurements per each right node is P + RQ = Θ(log3(n)), the sample complexity of the sublinear scheme is Θ(K log3(n)). In terms of the computational complexity, since there are Θ(K) right
nodes and a constant number of iterations, the computational complexity of the sublinear algorithm is Θ(LmLpK log3(n)).

C. Simulation Results

In this subsection, we show simulation results for the noisy case that validate our theoretical results. The simulations are conducted in Python. Since the sublinear scheme has much lower computational complexity than the almost-linear scheme, we only conduct simulations on the sublinear scheme here. We deﬁne the signal-to-noise ratio (SNR):

SNR = 10 log10

Rj=0 yj − wj 22 ,

R j=0

wj

2 2

and use Gaussian noise. Since the fraction of non-recovered non-zero components p can be made arbitrarily small, in the simulations, we simply deﬁne a successful recovery as the cases when all the non-zero components are correctly recovered up to a global phase. In all the simulations, we set P = 5 log(n), d = 15, M = 8K, and ε = 1.

SNR(dB) Pr{success}

28

1

26

0.8

24

22

0.6

20

18

0.4

16

14

0.2

12

10

0

1.04 1.68 2.32 2.96 3.60 4.24 4.88 5.52 6.16 6.80

number of measurements/105

Fig. 16: Probability of successful recovery. We choose n = 220, K = 50, Lm = 3, and Lp = 6. Different values of SNR are tested, and for each set of parameters, 1000 experiments are conducted.

time cost (sec) time cost1/3 (sec1/3)

3 n=216 n=218
2 n=220

0.8 K=5
0.7 K=10 K=15
0.6

0.5 1
0.4

0

0.3

0

20

40

60

10

12

14

16

K

log(n)

Fig. 17: Decoding complexity. We choose Q = 2 log2(n), SNR = 20dB, Lm = 3, and Lp = 6. Different values of n and K are tested, and for each set of parameters, 100 experiments are conducted and the average time cost is shown.

0.07 Lm=2 Lm=4
0.06 Lm=3 Lm=5

Running time (sec)

0.05

0.04

0.03

0.02 2

4

6

8

10

Number of possible phases Lp

Fig. 18: Decoding complexity vs. number of possible magnitudes and phases. We choose n = 4096, K = 10, Q = 5 log2(n) and SNR = 24dB. Different values of Lm and Lp are tested, and for each
set of parameters, 100 experiments are conducted and the average
time cost is shown.

In Figure 16, we show the simulation results on the prob-
ability of successful recovery as a function of the number
of measurements and the SNR. In Figure 17, we show the
simulation results on the decoding complexity of the sublinear scheme.12 It can be seen that the time cost of sublinear scheme is indeed low and only linear in K and Θ(log3(n)). In Figure 18, we show empirical results on the decoding complexity of
the sublinear scheme as a function of the number of possible magnitudes and phases (Lm and Lp). One can observe that the time cost grows linearly in Lm and Lp.

VIII. CONCLUSION
We have considered the problem of recovering a K-sparse complex signal x ∈ Cn from m intensity measurements of the form |Ax|, where A ∈ Cm×n is the measurement matrix. Our main focus was on the case where the measurement vectors are unconstrained and noiseless. We proposed the PhaseCode algorithm that is based on a sparse-graph codes framework. We showed that for any signal x ∈ Cn, using order-optimal sample and decoding complexity of Θ(K), PhaseCode can provably recover all but an arbitrarily small random fraction of the non-zero signal components with high probability. We also showed that PhaseCode can recover almost all the K non-zero signal components using only slightly more than 4K measurements if the support of the non-zero components of
12The simulations are conducted on a laptop with 2.8 GHz Intel Core i7 CPU and 16 GB memory.

19

signal is uniformly random. To the best of our knowledge, our work is the ﬁrst capacity-approaching low-complexity compressive phase retrieval algorithm. We furthermore showed that PhaseCode can be used for practical systems such as optical systems with proper modiﬁcations. Finally, we demonstrated how PhaseCode can be robustiﬁed in the presence of noise. Via extensive simulation results, we validated the performance of PhaseCode for various settings.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers for many helpful comments.
REFERENCES
[1] M. Akcakaya and V. Tarokh, “New conditions for sparse phase retrieval,” arXiv preprint arXiv:1310.1351, 2013.
[2] T. Heinosaari, L. Mazzarella, and M. M. Wolf, “Quantum tomography under prior information,” Communication in Mathematical Physics, vol. 318, no. 2, pp. 355–374, 2013.
[3] E. J. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,” IEEE Trans. Inform. Theory, vol. 52, no. 2, pp. 489–509, 2006.
[4] D. Donoho, “Compressed sensing,” IEEE Trans. Inform. Theory, vol. 52, no. 4, 2006.
[5] A. Walther, “The question of phase retrieval in optics,” Optica Acta, vol. 10, no. 1, pp. 41–49, 1963.
[6] R. P. Milane, “Phase retrieval in crystallography and optics,” J. Opt. Soc. Am. A, vol. 7, pp. 394–411, 1990.
[7] R. W., “Harrison ”phase problem in crystallography,” JOSA A, vol. 10, pp. 1046–1055, 1993.
[8] J. C. Dainty and J. R. Fienup, “Phase retrieval and image reconstruction for astronomy,” in Image Recovery: Theory and Application, pp. 231– 275, Academic Press, 1987.
[9] J. M. Rodenburg, “Ptychography and related diffractive imaging methods,” Advances in Imaging and Electron Physics, vol. 150, pp. 87–184, 2008.
[10] M. Mirhosseini, O. S. Magana-Loaiza, S. M. H. Rafsanjani, and R. W. Boyd, “Compressive direct measurement of the quantum wavefunction,” arXiv preprint arXiv:1404.2680, 2014.
[11] M. H. Hayes, J. S. Lim, and A. V. Oppenheim, “Signal reconstruction from phase or magnitude,” IEEE Trans. Acoust., Speech, Signal Process., vol. 28, no. 6, pp. 672–680, 1980.
[12] M. L. Moravec, J. K. Romberg, and R. Baraniuk, “Compressive phase retrieval,” SPIE Conf. Series, vol. 6701, 2007.
[13] P. Schniter and S. Rangan, “Compressive phase retrieval via generalized approximate message passing,” in Proceedings of Allerton Conference on Communication, Control, and Computing, 2012.
[14] H. Ohlsson, A. Yang, R. Dong, and S. Sastry, “Compressive phase retrieval from squared output mea- surements via semideﬁnite programming,” arXiv preprint arXiv:1111.6323, 2011.
[15] E. J. Candes, T. Strohmer, and V. Voroninski, “Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming,” Communications on Pure and Applied Mathematics, vol. 66, no. 8, pp. 1241–1274, 2013.
[16] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” arXiv preprints arXiv:1306.0160, 2013.
[17] X. Li and V. Voroninski, “Sparse signal recovery from quadratic measurements via convex programming,” arXiv preprints arXiv:1209.4785, 2012.
[18] K. Jaganathan, S. Oymak, and B. Hassibi, “Sparse phase retrieval: Convex algorithms and limitations,” pp. 1022–1026, 2013.
[19] K. Jaganathan, S. Oymak, and B. Hassibi, “Phase retrieval for sparse signals using rank minimization,” in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3449–3452, 2012.
[20] E. Candes, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” arXiv preprint arXiv:1407.1065, 2014.
[21] E. G. Loewen and E. Popov, Diffraction gratings and applications. CRC Press, 1997.

[22] O. Bunk, A. Diaz, F. Pfeiffer, C. David, B. Schmitt, D. K. Satapathy, and J. F. Veen, “Diffractive imaging for periodic samples: retrieving onedimensional concentration proﬁles across microﬂuidic channels,” Acta Crystallographica Section A: Foundations of Crystallography, vol. 63, no. 4, pp. 306–314, 2007.
[23] E. J. Candes, X. Li, and M. Soltanolkotabi, “Phase retrieval from coded diffraction patterns,” arXiv preprint arXiv:1310.3240, 2013.
[24] T. Richardson and R. Urbanke, Modern Coding Theory. Cambridge University Press, 2008.
[25] S. Cai, M. Bakshi, S. Jaggi, and M. Chen, “Super: Sparse signals with unknown phases efﬁciently recovered,” arXiv preprint arXiv:1401.4451, 2014.
[26] S. Pawar and K. Ramchandran, “Computing a k-sparse n-length discrete fourier transform using at most 4k samples and O(k log k) complexity,” arXiv preprint arXiv:1305.0870, 2013.
[27] I. Waldspurger, A. d’Aspremont, and S. Mallat, “Phase recovery, maxcut and complex semidenite programming,” Mathematical Programming, pp., pp. 1–35, 2013.
[28] R. Balan, P. G. Casazza, and D. Edidin, “On signal reconstruction without phase,” Applied and Computational Harmonic Analysis, vol. 20, May 2009.
[29] A. S. Bandeira, J. Cahill, D. G. Mixon, and A. A. Nelson, “Fundamental limits of phase retrieval,” Proc. 10th Intern. Conf. on Sampling Theory and Applications (SampTA), July 2013.
[30] B. G. Bodmann and N. Hammen, “Stable phase retrieval with lowredundancy frames,” arXiv preprint arXiv:1302.5487, 2013.
[31] Y. Wang and Z. Xu, “Phase retrieval for sparse signals,” Applied and Computational Harmonic Analysis, vol. 37, no. 3, pp. 531–544, 2014.
[32] M. Akc¸akaya and V. Tarokh, “Sparse signal recovery from a mixture of linear and magnitude-only measurements,” IEEE Signal Processing Letters, vol. 22, no. 9, pp. 1220–1223, 2015.
[33] A. S. Bandeira and D. G. Mixon, “Near-optimal phase retrieval of sparse vectors,” in SPIE Optical Engineering+ Applications, pp. 88581O– 88581O, International Society for Optics and Photonics, 2013.
[34] W. Xu and B. Hassibi, “Efﬁcient compressive sensing with deterministic guarantees using expander graphs,” in Information Theory Workshop, 2007. ITW’07. IEEE, pp. 414–419, IEEE, 2007.
[35] M. Luby, M. Mitzenmacher, M. A. Shokrollahi, and D. Spielman, “Improved low-density parity check codes using irregular graphs,” IEEE Trans. Info. Theory, vol. 47, pp. 585–598, 2001.
[36] T. Richardson and R. Urbanke, “The capacity of low-density paritycheck codes under message-passing decoding,” IEEE Transactions on Information Theory, vol. 47, pp. 599–618, February 2001.
[37] Z. Wang, L. Millet, M. Mir, H. Ding, S. Unarunotai, J. Rogers, M. U. Gillette, and G. Popescu, “Spatial light interference microscopy (slim),” Opt. Express, vol. 19, no. 2, pp. 1016–1026, 2011.
[38] S. R. P. Pavani and R. Piestun, “Three dimensional tracking of ﬂuorescent microparticles using a photon-limited double-helix response system,” Opt. Express, vol. 16, pp. 22048–22057, 2008.
[39] A. V. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal Processing. Prentice Hall, 1989.
[40] R. Vershynin, “Introduction to the non-asymptotic analysis of random matrices,” arXiv preprint arXiv:1011.3027, 2010.
[41] B. Alexeev, A. S. Bandeira, M. Fickus, and D. G. Mixon, “Phase retrieval with polarization,” SIAM Journal on Imaging Sciences, vol. 7, no. 1, pp. 35–66, 2014.
[42] D. J. Love, R. W. Heath, W. Santipach, and M. L. Honig, “What is the value of limited feedback for mimo channels?,” IEEE Communications Magazine, vol. 42, no. 10, pp. 54–59, 2004.
[43] J. Candy, “A use of limit cycle oscillations to obtain robust analogto-digital converters,” IEEE Transactions on Communications, vol. 22, no. 3, pp. 298–305, 1974.
[44] Y. Chen, X. Yi, and C. Caramanis, “A convex formulation for mixed regression with two components: Minimax optimal rates.,” in COLT, pp. 560–604, 2014.
[45] A. C. Gilbert, M. J. Strauss, J. A. Tropp, and R. Vershynin, “Algorithmic linear dimension reduction in the l 1 norm for sparse vectors,” arXiv preprint cs/0608079, 2006.
[46] P. Erdos and A. Renyi, “On the evolution of random graphs,” Publications of the Mathematical Institute of the Hungarian Academy of Sciences, vol. 5, pp. 17–61, 1960.
[47] B. Bollobas, Random graphs. Cambridge University Press, 2001. [48] S. A. Pawar, Pulse: Peeling-based ultra-low complexity algorithms for
sparse signal estimation. PhD thesis, University of California, Berkeley, 2013.

20

[49] M. Rudelson, R. Vershynin, et al., “Hanson-wright inequality and subgaussian concentration,” Electron. Commun. Probab, vol. 18, no. 82, pp. 1–9, 2013.

APPENDIX A. Guess and Check Strategy for Resolvable Multitons
Recall the equations:

Ramtin Pedarsani Ramtin Pedarsani is an Assistant

Professor in ECE Department at the University of

California, Santa Barbara. He received the B.Sc.

PLACE PHOTO HERE

degree in electrical engineering from the University of Tehran, Tehran, Iran, in 2009, the M.Sc. degree in communication systems from the Swiss Federal Institute of Technology (EPFL), Lausanne, Switzer-

land, in 2011, and his Ph.D. from the University of

California, Berkeley, in 2015. His research interests

include networks, machine learning, information and

coding theory, and transportation systems. Ramtin is

a recipient of the IEEE international conference on communications (ICC) best

paper award in 2014.

PLACE PHOTO HERE

Dong Yin Dong Yin is a PhD student in Department of Electrical Engineering and Computer Sciences at UC Berkeley, working with Prof. Kannan Ramchandran. He is interested in information and coding theory, machine learning, and signal processing. Before coming to Berkeley, he obtained his B.S. from Tsinghua University in China in 2014.

PLACE PHOTO HERE

Kangwook Lee Kangwook Lee is a postdoctoral scholar at Information and Electronics Research Institute at KAIST. He obtained his PhD degree in May 2016 from the EECS department at UC Berkeley. He also obtained his MS degree in EECS from UC Berkeley in 2012, and before that he obtained his BS degree in EE from KAIST in 2010. He is a recipient of the KFAS Fellowship 2010-15. His research interests lie in information theory and machine learning.

Kannan Ramchandran (Ph.D.: Columbia Univer-

sity, 1993) is a Professor of Electrical Engineering

and Computer Sciences at UC Berkeley, where he

PLACE PHOTO HERE

has been since 1999. He was on the faculty at the University of Illinois at Urbana-Champaign from 1993 to 1999, and with AT&T Bell Labs from 1984 to 1990. He is an IEEE Fellow, and a recipient

of the 2017 IEEE Kobayashi Computers and Com-

munications Award, which recognizes outstanding

contributions to the integration of computers and

communications. His research awards include an

IEEE Information Theory Society and Communication Society Joint Best

Paper award for 2012, an IEEE Communication Society Data Storage Best

Paper award in 2010, two Best Paper awards from the IEEE Signal Processing

Society in 1993 and 1999, an Okawa Foundation Prize for outstanding

research at Berkeley in 2001, an Outstanding Teaching Award at Berkeley

in 2009, and a Hank Magnuski Scholar award at Illinois in 1998. His

research interests are at the intersection of signal processing, coding theory,

communications and networking with a focus on theory and algorithms for

large-scale distributed systems.

yi,1 = |a + eiω x | = |u|,

(28)

yi,2 = |b + e−iω x | = |v|,

(29)

yi,3 = |c + 2 cos(ω )x | = |w|,

(30)

yi,4 = |d + eiω x |,

(31)

where complex numbers a, b, c and d are known values that depend on the values and locations of the known colored active left nodes. We want to solve the ﬁrst 3 equations (28)-(30) to ﬁnd and x , and use (31) to check if our guess is correct. Since eiω + e−iω = 2 cos(ω ), we know that u + v = w. Let α be the angle between complex numbers u and v. Then,
|u + v|2 = |u|2 + |v|2 + 2|u||v| cos(α).

Thus, one can ﬁnd α up to a plus-minus sign as,

α = cos−1( |u + v|2 − |u|2 − |v|2 ) 2|u||v|
= cos−1( yi2,3 − yi2,1 − yi2,2 ). 2yi,1yi,2

We ﬁnd possible x ’s for two different signs of α. If our guess is true, the check measurement yi,4 will determine which solution is the right one. Deﬁne a known variable z as

z = u/v = |u| eiωα. |v|

Thus,

a + eiω x = z(b + e−iω x),

or

zb − a

x = eiω − ze−iω .

(32)

Replacing x from (30) in (32), we have

yi,3 = |c + 2 cos(ω ) zb − a | eiω − ze−iω
= |c cos(ω )(1 − z + 2zbc−2a ) + i sin(ω )(1 + z) |. (33) cos(ω )(1 − z) + i sin(ω )(1 + z)

Deﬁne the following known complex variables:

k1 = 1 − z + 2zb − 2a ; c
k2 = 1 + z;
k3 = 1 − z;
k4 = yi,3/|c|.

Also let k1 = k1r + ik1i and use similar notation for the real and imaginary parts of other variables. Then, one can square (33) to get
(k1r cos(ω ) − k2i sin(ω ))2 + (k1i cos(ω ) + k2r sin(ω ))2 = k42[(k3r cos(ω ) − k2i sin(ω ))2 + (k3i cos(ω ) + k2r sin(ω ))2].

21

Now deﬁning appropriate new known real variables k5, k6 and k7, we get an equation of the form
k5 cos2(ω ) + k6 sin2(ω ) = k7 sin(ω ) cos(ω ).
Squaring the above equation and using sin2(ω ) = 1 − cos2(ω ), we get a quadratic equation in cos2(ω ) that one can easily solve to ﬁnd at most 2 possible values for . Note that cos(ω ) is positive by construction. Now since there are two possible values of α, one can get at most 4 solutions for and x . Those solutions can be checked by (31). If the guess is true, the probability that the check fails is 0; thus, one can recover the resolvable multiton with probability 1.

B. Proof of Corollary 4
Let (|x(1)|, |x(2)|, . . . , |x(K)|) be the magnitudes of the nonzero components that are ordered increasingly. We partition the K components to g = K(1+γ)/2 subgroups as follows:

(|x(1)|, . . . , |x(K/g)|), (|x(K/g+1)|, . . . , |x(2K/g)|), . . . , (|x(K−K/g+1)|, . . . , |x(K)|).

Let bi be the largest number in subgroup i. By AzumaHoeffding’s inequality, the probability that more than (p + )K/g components are missed in a subgroup is upper bounded by 2e−2 2K/g. Taking = 1/ log(K) and using union bound, we have
g

xˆ − x 1 ≤ (p + 1/ log(K))( bi)K/g, (34)
i=1

with

probability

O(g

e−

g

2K log2 (K

)

).

Further,

g

g

( bi)K/g ≤ (|x(1)| + bi)K/g

(35)

i=1

i=1

≤ x 1 + bgK/g

(36)

Kγ

≤ x 1(1 + Θ( g ))

(37)

=

x

1(1

+

Θ(K

−

1−γ 2

)).

(38)

Gathering (34) and (38), we conclude that with probability

1

−

O(K

1+γ 2

e−

2K(1−γ)/2 log2 (K )

),

xˆ − x 1 ≤ p x 1(1 + Θ(

1

)

+

Θ(K

−

1−γ 2

))

(39)

log(K )

1

= p x 1(1 + Θ( log(K) )).

(40)

C. Proof of Lemma 5

Proof: We form a graph with nodes that are active left

nodes which are in singleton right nodes. We construct edges

between these nodes if the corresponding active left nodes are

connected to a strong doubleton, and we use an Erdos-Renyi

random graph model [46] to ﬁnd parameters d and M for

which there is a giant component of size linear in K after

the second step of the algorithm. The Erdos-Renyi random

graph model is characterized by 2 parameters: n, the number

of nodes in the graph and p which is the probability that each

of the

n 2

possible edges are connected. Note that each edge

is connected in the graph with probability p independently

from every other edge. There is another variant of Erdos-Renyi

random graph model which is parametrized by (n, M ), where

M is the total number of edges. Then, the graph is chosen

uniformly at random from the collection of all graphs with n

nodes and M edges. By the law of large numbers, the two

models are equivalent for M =

n 2

p

as

long

as

n2p

→

∞.

It

is well known that in an Erdos-Renyi model if np → c > 1,

as n → ∞, where c is some constant, then the graph will have

a unique giant component of size linear in n [46].

Deﬁne Ks to be the random variable representing the

number of active left nodes that are connected to singletons.

We form an Erdos-Renyi random graph model with parameters

(Ks, ps) or equivalently parameters (Ks, Ms) where ps is the

probability that an edge is connected, and Ms is the total

number of edges. Thus, as Ks gets large, Ms approaches

Ks 2

ps.

Now

we

compute

the

parameters

Ks

and

ps

as

follows. The probability of an active left node being connected

to a singleton right node is the probability that at least one of

its d neighbors is a singleton, that is:

qs = 1 − (1 − ρ1)d.

(41)

Thus, by the law of large numbers as K gets large, there are

Kqs + o(K) distinct active left nodes in singleton right nodes.

Let M = cK for some constant c. As K gets large, the number of doubleton right nodes approaches M η22e!−η + o(K) since the degree of right nodes (on the pruned graph with active left

nodes) is Poisson distributed with parameter η = Kd/M =

d/c. However, we want to count only distinct doubleton right

nodes. It is easy to see that as K gets large, essentially all but

a vanishing fraction of the doubleton right nodes are distinct.

To this end, ﬁx a doubleton right node with neighbors (v1, v2).

The probability that a randomly chosen doubleton right node is

connected to (v1, v2) is 1/

K 2

. Since the number of doubleton

right nodes is linear in K, only a vanishing Θ(1/K) fraction

of them are non-distinct.

Let Ms be the number of strong doubletons (for which both

left nodes are also in other singletons). Thus, Ms is the number

of edges in our constructed Erdos-Renyi graph. Consider a

random left node i. Let D be the event that i is connected to

a doubleton right node and S be the event that i is connected

to a singleton right node. We compute the following 2 relevant

conditional probabilities:

p1 P(D|S) = P(D ∩ S) P(S)

1 − P(S¯) − P(D¯ ) + P(S¯ ∩ D¯ )

=

1 − P(S¯)

= 1 − (1 − ρ1)d − (1 − ρ2)d + (1 − ρ1 − ρ2)d . 1 − (1 − ρ1)d

p2 P(D|S¯) = 1 − P(D¯ |S¯) P(S¯ ∩ D¯ )
= 1 − P(S¯)
= 1 − (1 − ρ1 − ρ2)d . (1 − ρ1)d

22

1.8

1.6
× × × × 1.4 t 1 t 1 ∗2 ∗2 1.2

Ks ps

1

0.8

3.11

0.6

19.24

f (t) − 1 = 0
(a) The good case.

f (t) − 1 =

0.4

0.2

× 0

5

10

15

20

25

c

∗

t Fig.

19:

The

diagram

shows

the

values

of

c

for

2
which

the

giant

f (t) − component is formed after step 2 of the algorithm. Note that c =
M/K. In the random graph model the giant component is formed if

Ksps > 1, where Ks is the number of nodes in the random graph,

and ps is the probability that an edge is connected. From the diagram,

one can see that if 3.11 < c < 19.24, the condition for having a giant

component is satisﬁed.

Now we use Bayes’ rule to ﬁnd that

× 1
1=0

×

×

t∗2

1

f (t) − 1 = 0

(b) The bad case.

Fig. 20: Figure (a) illustrates the good case that there are no ﬁxed

points other than 1 and t∗2. Figure (b) illustrates the bad case that there

is

another

ﬁxed

point

in

the

interval

(

t

∗ 2

,

1)

.

In

this

case,

f

(t)

=

1

has two solutions for t ∈ (t∗2, 1), as shown in Figure (b).

q P(S|D) = P(D|S)PP((SD)|S+)PP((SD)|S¯)P(S¯)

=

p1qs

.

p1qs + p2(1 − qs)

Thus, Ms = M η22e!−η q2. (42)

The random graph is constructed with Ks = K(1 − (1 − ρ1)d)

nodes and Ms edges chosen uniformly at random among

Ks 2

possible edges. The probability of a randomly chosen edge

being connected is thus:

M η2e−η q2

ps =

2! .

Ks

2

From the well-known Erdos-Renyi random graph result [46] (also see [47]), a linear size giant component exists if Ksps > 1 with probability 1 − O(1/Ks). More precisely, let Z be the size of the giant component. Then, one has

P | Z − ζ| < ε = 1 − O 1 ,

Ks

ε2Ks

where ζ ∈ (0, 1) is the unique solution of ζ + e−2ζMs/Ks = 1, if 2Ms/Ks > 1 or equivalently Ksps > 1 [25], [47]. Thus, a linear-size giant component exists if

KqsMs > 1.
K qs 2

We present two concrete examples to complete the proof of
the lemma. Let d = 5. Replacing Ms and qs by (42) and (41), one can check that the inequality holds if 3.11 ≤ c ≤ 19.24 (See Figure 19). Similarly, one can set d = 8 and see that the inequality holds if 3.48 ≤ c ≤ 55.36.

D. Proof of Lemma 7
First, let us consider a small neighborhood around t∗1 = 1. We want
f (t∗1 − h) < t∗1 − h = f (t∗1) − h,

for some small h > 0. Equivalently, we want
f (t∗1) − f (t∗1 − h) > 1. h
Letting h → 0, the condition becomes f (t)|t=1 > 1. This is a necessary and sufﬁcient condition for instability of point t = 1. In other words, this condition makes sure that (15) holds for pj close to 1. Thus, in picking parameters d and η, one makes sure that

f (t)|t=1 = (d − 1)ηe−η > 1.

For d = 5, this leads to 0.3574 < η < 2.1533 or 2.32K <

M < 13.99K. For d = 8, this leads to 0.17 < η < 3.06

or 2.62K < M < 47.06K. To complete the proof, we need

to show that f (t) − t < 0 for t ∈ (t∗2, 1). Note that f (t)

is continuous and continuously differentiable. Thus to show

that f (t) − t < 0 for t ∈ (t∗2, 1), it is enough to show that

f (t)−1 = 0 has only one solution in that interval (the “good”

case: See Figure 20a). To see this, suppose that f (t) − t = 0

for

some

t

in

the

interval

(t

∗ 2

,

1)

.

Since

1

and

t∗2

are

also

solutions of f (t) − t = 0, then f (t) − 1 must change sign at

least twice in the interval (t∗2, 1) (the “bad” case: See Figure

20b).

Therefore,

to

ensure

that

f (t)

<

t,

∀t

∈

(t

∗ 2

,

1)

it

is

sufﬁcient to show that

f (t) = ηe−ηt(d − 1)(1 + e−η − e−ηt)d−2 = 1,

has only one solution in the interval t ∈ (t∗2, 1). After some algebra, one can re-write the above equation as

(η(d

−

1))−

1 d−

2

eηt(1/(d−2)+1)

=

eηt(1

+

e−η )

−

1.

23

Replacing x = eηt, we get an equation of the form xa = bx−c for a > 1 and b, c > 0. This equation has clearly at most two solutions for x ≥ 0. On the other hand, f (1) > 1 and f (∞) = 0. Thus, f (1) = 1 has a solution for t > 1, which shows that f (t) = 1 has at most one solution in t ∈ [0, 1].

E. Probability of Tree-like Neighborhood
In this section, we give a short proof of Lemma 9. Let C be the number of right-nodes and V be the number of left-nodes in Ne2 . Since the ensemble of the graphs that we consider is only left-regular (and not right-regular), we cannot immediately use the result of [36]. Note that the degree distribution of right nodes is Poisson distribution with constant rate. The key idea is to show that the size of the tree is bounded by O(log(K) ) with high probability. This is intuitively clear since Poisson distribution has a tail decaying faster than exponential decay. To formally show this, we keep unfolding the tree up to level ∗, and at each level we upper bound the probability that the size of the tree grows larger than O(log(K) ). Fix some constant c1. We upper bound the probability of not having a tree as follows.
P(Ne2 ∗ is not a tree) ≤ P(V ∗ > c1 log(K) ∗ )+
∗
P(C ∗ > c1 log(K) )+ P(Ne2 ∗ is not a tree|V ∗ < c1 log(K) ∗ , C ∗ < c1 log(K) ∗ ).

Note that since the left degree is a constant, d, if V ∗ is O(log(K) ∗ ), C ∗ is also O(log(K) ∗ ). Let α = P(V > c1 log(K) ). Then,
α ≤ α −1 + P(V > c1 log(K) |V −1 < c1 log(K) −1) (43)
≤ α −1 + P(V > c1 log(K) |C < c2 log(K) −1), (44)

where (44) is due to the fact that every left node has exactly d edges connected to right nodes so if V −1 < c1 log(K) −1, there exists some constant c2 such that C < c2 log(K) −1. To

count the number of left nodes in depth , let n < C be the

number of right nodes exactly at depth after unfolding the

tree. Let Xi, 1 ≤ i ≤ n be the degree of these right nodes.

Given that V −1 < c1 log(K) −1, one has V > c1 log(K) ,

only if X =

n i=1

Xi

>

c3 log(K)

for some constant c3.

The distribution of X is Poisson distribution with parameter

n λ. We know that the tail probability of a Poisson random

variable Y with parameter λ can be upper bounded as follows:

y

P(Y ≥ y) ≤

eλ y

. Thus,

P(X > c3 log(K) ) ≤

c4 c3 log(K) ≤ O( 1 ).

log(K )

K

Thus,

α ≤ α −1 + c5 ,

(45)

K

for some constant c5. Now since ∗ is a constant, summing up the inequalities in (45), we show that

α ∗ = P(V ∗ > c1 log(K) ∗ ) ≤ O( 1 ). K

Similarly, one can show that

P(C ∗ > c1 log(K) ∗ ) ≤ O( 1 ). K

To complete the proof, we need to show that with high
probability, we have a tree-like neighborhood, given that the number of nodes is bounded by O(log(K) ∗ ). First, we ﬁnd a lower bound on the probability that Ne2 +1 is a tree-like neighborhood if Ne2 is a tree-like neighborhood, when < ∗. Assume that t additional edges have been revealed at this stage
without forming a cycle. The probability that the next edge
from a left node does not create a cycle is the probability
that it is connected to one of the right nodes that is not already in the subgraph which is lower bounded by 1 − Cm∗ . Thus, the probability that Ne2 +1 is a tree-like neighborhood if Ne2 is a tree-like neighborhood, is lower-bounded by (1 − CM∗ )C +1−C . Similarly, the probability that Ne2 +2 is a tree-like neighborhood if Ne2 +1 is a tree-like neighborhood, is lower-bounded by (1− V ∗ )V +1−V . Therefore, the probability that Ne2 ∗ is a tree-like Kneighborhood is lower-bounded by

(1 − V ∗ )V ∗ (1 − C ∗ )C ∗ .

K

M

For large M and K, the above expression is approximately

e−(V

2 ∗

/K

+C

2 ∗

/M

)

≥

1

−

(V

2
∗

/K

+

C

2
∗

/M

).

Now since V ∗ and C ∗ are upper-bounded by O(log(K) ∗ ), the probability of having a tree-like neighborhood is at least 1 − O(log(K) ∗ /K).

F. Convergence to Cycle-free Case

In this section, we give a short proof of Lemma 10. The
proof follows similar steps as in [36], with the difference that
the right degree is irregular and Poisson-distributed. First, we prove (17). Let Zi = 1{ei is colored}, 1 ≤ i ≤ Kd
be the indicator that ei is colored after iterations of the algorithm. Let B be the event that Ne21 is tree-like. Then,
E[Z1] = E[Z1|B]P(B) + E[Z1|B¯]P(B¯) ≤ E[Z1|B] + P(B¯)
≤ p + γ log(K) , K
for some constant γ, where the last inequality is by Lemma 9. Trivially, |E[Z1|B]| ≤ 1. Furthermore, E[Z] = KdE[Z1]. Hence,

Kd(1 − γ log(K) ) < E[Z] < Kd(p + γ log(K) ).

K

K

Then, (17) follows from choosing K large enough such that

K log(K )

> 2γ .

Second, we prove that

P(|Z − Kdp | > Kd /2) < 2e−β 2K1/(2 . +1) (46)

Then, (18) follows from (17) and (46). To prove (46), we
use the standard Martingale argument and Azuma’s inequality
provided in [36] with some modiﬁcations to account for the right irregular degree. Suppose that we expose the Kd edges of the graph one at a time. Let Yi = E[Z|ei1]. By deﬁnition,

24

Y0, Y1, . . . , YKd is a Doob’s martingale process, where Y0 =

E[Z] and YKd = Z. To use Azuma’s inequality, we ﬁnd the

appropriate upper bound: |Yi+1 − Yi| ≤ αi. If the right degree

is regular and equal to dc, it is shown in [36] that αi can

be chosen as 8(dvdc) . We show that when the right degree

has Poisson distribution with constant rate, the degree of all

of

the

right

nodes

can

be

upper

bounded

by

O(K

2

1 +0.5

)

with

1

probability at least c6K(e−β1K 2 +0.5 ) for some constants c6

and β1. To show this, let X be a Poisson random variable with

parameter λ and c7 be some constant. Then,

1

P(X

>

c7K 2

1 +0.5

)

≤

eλ

c7K

2

1 +0.5

c7K 2 +0.5

1

≤ c6(e−β1K 2 +0.5 ).

Now considering M = Θ(K) right nodes and using union

bound, one can see that the probability that all the right

nodes

have

degree

less

than

O(K

2

1 +0.5

)

is

at

least

1−

1

O(K(e−β1K 2 +0.5 )). Let E be the event that at least one right 1

node has degree larger than c6K(e−β1K 2 +0.5 ). Given that E

has

not

happened,

one

can

upper

bound

αi2

by

O(K 2

2 +0.5

).

Then,

P(|Z − Kdp | > Kd /2)

≤ P(|Z − Kdp | > Kd /2|E¯) + P(E)

≤ 2e + c K(e ) − K2d2 2/4

1

2

i

α2 i

−β1K 2 +0.5 6

≤ 2e−β 2K1/(4 . +1)

G. Proof of Lemma 11

First note that it is easy to prove the lemma for speciﬁc

parameters by plotting the function. See for example Fig-

ure 10a. To formally show it, note that f (1) = 1 is one

solution of the ﬁxed point equation, since λ(1) = 1. Also

f (0) = λ(e−η) > 0. Thus, by continuity of f (x) and using

the assumption that f (1) > 1, there is another ﬁxed point x∗2.

Now since f (1) > 1, f (x) < x for x close to 1. In order to

show

that

f (x)

<

x

for

all

x

∈

(x

∗ 2

,

1)

,

it

is

enough

to

show

that f (x) − 1 = 0 has only one solution in x ∈ (0, 1). To this

end, see that

f (x) = ηe−ηxλ (1 + e−η − e−ηx).

For ease of notation, let y = 1 + e−η − e−ηx and y ∈ (e−η, 1). Equivalently, we want to show that

C(1 + e−η − y)(1 + y + y2 + . . . + yD−2) = 1

has only one solution where C = η/h(D − 1). This is easy to see since D is large so y 1−C1−−CCe−η .

H. Proof of Lemma 12

We show that if

D = max{( e )2/ , (1 + 1 )1/(1− )},

(47)

1−

p∗

then,

f (1) = ηe−η λi(i − 1) > 1,

(48)

i≥1

and the error ﬂoor which is approximately λ(e−η) is at most p∗; that is,

λie−η(i−1) ≤ p∗.

(49)

i≥1

This shows that in the density evolution equation, pj converges to p∗ as j goes to inﬁnity. This is illustrated in Figure 11.
Recall that

D
d¯ = (

λi )−1 = h(D − 1)

D

.

i=2 i

D−1

Thus, since M = K/(1 − ),

K d¯

D

η = M = h(D − 1) D − 1 (1 − ).

First, we show (49) in the following.

D λie−η(i−1) = h(D1− 1) D i −1 1 e−η(i−1)

i=2

i=2

≤1

∞
e−ηi

h(D − 1) i=1

e−η = h(D − 1)(1 − e−η) .

It is enough to show that h(D − 1)(eη − 1) ≥ p1∗ . We have

h(D − 1)(eη − 1) ≥ eη − 1

≥

e

log

(

D

)

.

D D−1

(1

−

)

−

1

≥ D1− − 1

≥ 1, p∗

where the last inequality is due to (47). Second, we show that (48) is satisﬁed in the following.

ηe−η D µi(i − 1) = ηe−η h(DD−−11) (50)
i=2

= D(1 −

)e

−

h(D

−

1)

D D−

1

(1

−

)

(51)

≥ D(1 −

)e

−

(1+log

(

D

))

D D−

1

(1

−

)

(52)

= 1 − D DD−−11

(53)

e

≥ 1 − D /2

(54)

e

≥ 1,

(55)

where (54) is due to (47) since D ≥ ( 1−e )2/ ≥ 2 implies that

D−1 D−1

≥

2,

and

(55)

is

due

to

(47).

This

shows

that

pj ,

j

≥1

is a strictly decreasing sequence which is lower bounded by

p∗. Thus, pj → p∗ as j → ∞. This completes the proof.

25

I. Proof of Theorem 15

The sample and computational complexity are already ana-

We ﬁrst introduce some notation. Here, · F denotes the lyzed in Section VII-A. This completes the proof of Theorem Frobenius norm of a matrix, · denotes the operator norm of a 15.

matrix. For a sub-exponential random variable, · ψ1 denotes

the sub-exponential norm of it; for a sub-gaussian random

variable, · ψ2 denotes the sub-gaussian norm of it [40]. The J. Proof of Lemma 19

notations c, ci, C, and Ci represent absolute constants with positive value.
In our model, we also assume that the noise wi satisﬁes

To prove Equation (56), we use the Bernstein’s inequality in [40] as follows. For any t > 0,

E[|wi|] = µ, E[wi2] = σ2, and wi ψ1 = ν. Since the entries

1P

t2 t

iηn =A0 |aanidj| Fψ02 aarned bηo0un=ded |fa0n,dij|thψu2s, swubh-egraeuassiijana,ndwef0l,eijt P P i=1(|wi| − E [|wi|]) > t ≤ exp −C1P min ν2 , ν .

are entries of A0 and F 0. In order to prove Theorem 15, we need to prove Lemma 17 Therefore, by choosing t0 > E [|wi|] = µ and t = t0 − µ, we

ﬁrst. Here, we restate Lemma 17 with more details.

have

Lemma 19. There exists ζ > 0, determined by η, ν, and σ, such that when φ > µ/ζ, for any t0 ∈ (µ, ζφ),

P P1 w 1 ≥ t0 ≤ exp [−δ1P ] .

P P1 w 1 ≥ t0 = O(1/n2), (56)

and

P 1 y − A(zˆzˆH) < t0 = O(1/n2), (57)

P

1

when zˆ z.

See the proof of Lemma 17 in Appendix J. Now we can
analyze the failure probability of the almost-linear scheme. Recall that the bipartite graph is d-left-regular; thus, there are dn edges in the graph. In the ﬁrst iteration, we need to check

Since δ1 is a constant and P = Θ(log(n)), (56) is proved.
Now we prove Equation (57). Before getting into the details of the proof, we give the deﬁnition of a new notation φ. For two vectors p, q ∈ Sn, it is easy to see that p q ⇔ ppH − qqH = 0. Since the entries of p and q lie in the quantized set S, we know that there exists φ > 0, such that ppH−qqH F > φ, when p q, where φ depends on ε, Lm, and Lp.
Lemma 20. Given two vectors x1, x2 ∈ CN , let X = x1xH1 − x2xH2 = 0. A is the linear function deﬁned in (24), and w is the noise. Then, for any s > 0, we have,

every edge and detect the singletons. Therefore, we need to do Θ(n) tests in the ﬁrst iteration. Similarly, in the following iterations, we need to do at most Θ(n) tests. Since the number of iterations is a constant, we need to do Nt = Θ(n) tests. Lemma 17 tells us that, for any energy test, if no error has
been made in the previous tests, the error probability of the energy test is O(1/n2). More speciﬁcally, let Ei be the event that there is an error in the ith test, while the tests 1, . . . , i − 1 are all correct. The event Etest that there exists an error in at least one energy test can be decomposed as
Nt
Etest = Ei.

P P1 A(X) + w 1 < (ζ − sηd) X F − 2sν ≤ exp −C0P min {s2, s} ,
where ζ > 0 depends on η, σ, and ν, ηd > 0 only depends on η.
See the proof of Lemma 20 in Appendix K. Note that y − A(zˆzˆH) = A(zzH −zˆzˆH)+w, and that zzH −zˆzˆH F > φ. Now using Lemma 20, conditioning on h, we have for any s > 0,

i=1
By union bound, we have
Nt
P {Etest} ≤ Nt P {Ei} = Θ(n)O(1/n2) = O(1/n).

P 1 y − A(zˆzˆH) < ζφ − (ηdφ + 2ν)s | h

P

1

≤ exp −C0P min {s2, s} .

(58)

i=1
Another possibility of making an error lies in the coloring algorithm itself. When there is no error in energy tests, this probability is O(1/K) as analyzed in the noiseless case. Therefore the failure probability of the almost-linear scheme is

Since (58) holds for any h, we know that it also holds without
conditioning on h. If ζφ > t0, we can choose s = ηζdφφ−+t20ν . Then

P 1 y − A(zˆzˆH) < t0 ≤ exp [−δ2P ] .

P

1

P {Ea} = P {Ea|Etest} P {Etest} + P{Ea|Etest}P{Etest} ≤ P {Etest} + P{Ea|Etest} = P {Etest} + P {Ecoloring} = O(1/n) + O(1/K) = O(1/K)

Since δ2 is a constant and P = Θ(log(n)), Equation (57) is proved.
We conclude that there exists ζ, determined by the statistics of noise, such that when φ > µ/ζ, for any threshold t0 ∈ (µ, ζφ), the energy test fails with probability O(1/n2). This completes the proof of Lemma 19.

26

K. Proof of Lemma 20

The proof of Lemma 20 is based on similar ideas in [44]. Let ξ = A(X) + w, then ξi = aHi Xai + wi. By the deﬁnition of matrix A, we know that the Hanson-Wright inequality for
complex random variables (shown in Appendix N) holds for aHi Xai. That is, for every t > 0,

P aHi Xai − E aHi Xai > t

(59)

≤ 6 exp −c min

t2

t

η4 X 2 , η2 X

F

≤ 6 exp −c min

t − 1, t

η2 X F 4 η2 X F

≤ 6 exp c 1 − t

,

4 η2 X F

where the second inequality is due to the fact that (a−1/2)2 ≥ 0 and X ≤ X F . From [40], we know that aHi Xai − E aHi Xai is a sub-exponential random variable with sub-
exponential norm

aHi Xai − E aHi Xai ψ1 ≤ Cη2 X F . (60)

On the other hand, E aHi Xai = 12
Thus,

x1 22 − x2 22 ≤ 12 X F .

ξi ψ1 = aHi Xai − E aHi Xai + E aHi Xai + wi ψ1

≤ aHi Xai − E aHi Xai ψ1 + E aHi Xai + ν

≤(Cη2 + 1/2) X F + ν,

(61)

where the ﬁrst inequality is due to the fact that E aHi Xai is a constant, E aHi Xai ψ1 = |E aHi Xai |, and that
wi ψ1 = ν. Then,

|ξi| − E [|ξi|] ψ1 ≤ 2 ξi ψ1 ≤ ηd X F + 2ν, (62)
where ηd = 2Cη2 + 1. Now by Bernstein’s inequality in [40], for every t > 0,

1P

P

(|ξi| − E [|ξi|]) < −t

P i=1

t2

t

≤ exp −C0P min (ηd X F + 2ν)2 , ηd X F + 2ν .

Let t = s(ηd X F + 2ν). For any s > 0,

1P P P (|ξi| − E [|ξi|]) < −s(ηd X F + 2ν)
i=1

≤ exp −C0P min {s2, s} .

(63)

By Cauchy-Schwartz inequality, for any i ∈ [P ], we have

E ξi2 2 ≤ E [|ξi|] E |ξi|3 ≤ E [|ξi|]

E

[ξ

2 i

]

E

[ξ

4 i

],

which implies

E [|ξ |] ≥

(E

[ξ

2 i

])3

.

(64)

i

E [ξi4]

By the deﬁnition of sub-exponential norm and the fact that ηd > 1, we have

E ξi4 ≤ (4 ξi ψ1 )4 ≤ (2ηd X F + 4ν)4

≤ (8ηd2

X

2 F

+

32ν2)2.

(65)

On the other hand, we have

E ξi2 = E (aHi Xai)2 + E wi2

= E (aHi Xai)tr aiaHi X + σ2

= E tr (aHi Xai)aiaHi X + σ2

= tr E (aHi Xai)aiaHi X + σ2

= 1 tr ((X + tr (X) I)X) + σ2

(66)

4

≥ 41 X 2F + σ2. (67)

Here we give an explanation of (66). Let Y = (aHi Xai)aiaHi .

Then,





E [Yjk] = E 

a∗igXghaihaij a∗ik

1≤g,h≤n

n
= E a∗igXggaigaij a∗ik +

E a∗igXghaihaij a∗ik .

g=1

g=h

If j = k, we have

n

E[Yjj ] = XggE[|aig|2|aij |2] + XghE[a∗igaih|aij |2]

g=1

g=h

1 = 4 (tr (X) + Xjj).

If j = k, we have

n

E[Yjk] = XggE[|aig|2aij a∗ik] + XghE[a∗igaihaij a∗ik]

g=1

g=h

= XjkE[|aij |2|aik|2]

1 = 4 Xjk.

Therefore, E[Y ] = 14 (X + tr (X) I). By combining (64), (65), and (67), we have

E [|ξi|] ≥

1 4

X

2 F

+

σ2

2
1 X 2 + σ2

8ηd2

X

2 F

+

32ν2

4

F

≥ζ X F,

where

ζ

=

1 2

min

1 32η2

,

σ2 32ν 2

is a constant determined by

d

the distribution of aij and wi. Then, by (63), we have

1P P P |ξi| < ζ X F − s(ηd X F + 2ν)
i=1
≤ exp −C0P min {s2, s} ,

which completes the proof.

27

L. Proof of Theorem 16
To prove Theorem 16, we make essential use of Lemma 18. Here, we restate Lemma 18, providing more details.
Lemma 21. If Ts = 1, supp(zs) = {ls}, and threshold t1 ∈ (0, ε2/2), then for any j ∈ [R],
P ˜bj = bjls = O(1/K3).
See the proof of Lemma 21 in Appendix M. Then, by union bound, P{b˜ = Bls } = O(R/K3) ≤ O(1/K2), since K = βnδ. Thus, we can reliably ﬁnd ls from the measurements with probability 1−O(1/K2). For a right node with Ts = 1, the probability of error in the index tests and the probability of error in the energy test are O(1/K2) and O(1/n2), respectively. Therefore, the error probability of the tests for a right node is O(1/K2). For a bin with Ts > 1, only the energy test needs to be considered and its error probability is O(1/n2). Then, we know the probability of error in the index and energy tests is O(1/K2). Since there are Θ(K) right nodes and a constant number of iterations, using the same decomposition method as in the proof of Theorem 15, the error probability of all the tests is O(1/K). Similar to the almost-linear scheme, considering the O(1/K) probability of unsuccessful recovery in the coloring algorithm when there is no error in the index and energy tests, the failure probability of sublinear scheme is P{Es} = O(1/K). Since the sample and computational complexity of the algorithm are already analyzed in Section VII-B, the proof of Theorem 16 is now complete.

M. Proof of Lemma 21

First, we deﬁne an event Eh such that there are more
than C3 log K active left nodes connected to a right node. As shown in [48], we have P{Eh} = O(1/K3). Now we
condition on the coding pattern h such that Eh happens, and thus |supp(z)| = T ≤ C3 log K. Similar to the almost-linear algorithm, we deﬁne R + 1 linear mappings, A0, A1, . . . , AR,
where A0 : Z → {aHi Zai}i∈[P ],

Aj : Z → {f Hj,iZf j,i}i∈[Q], for j ∈ [R].

Then, yj = Aj(zzH) + wj, j ∈ {0} ∪ [R].

Deﬁne the matrix Z˜ = {Z˜ij}N×N := zzH − z˜cz˜Hc =

z

z

H

−

z

c

z

H c

.

Then,

y˜ j

=

Aj(Z˜ )+wj

and

y˜j,i

=

f

H j,i

Z˜

f

j,i

+

wj,i. Let fj,i,m be the mth element of f j,i. Since for a ﬁxed

j, fj,i,m’s are independent, using similar argument to the one

in Appendix K, we have

f Hj,iZ˜ f j,i − E f Hj,iZ˜ f j,i

≤ C2η02 Z˜ .

ψ1

F

Thus, y˜j,i − E[y˜j,i] ψ1 ≤ C2η02 Z˜ F + ν. Since there a√re 2T − 1 nonzero entries in Z˜ , we have Z˜ F ≤
2T − 1Lmε. Moreover, T ≤√C3 log K, whic√h implies that y˜j,i−E[y˜j,i] ψ1 ≤ C4η02Lmε log K +ν ≤ ζ0 log K, where ζ0 is determined by η0, Lm, ε, and ν. On the other hand, since Ts = 1 and supp(zs) = {ls}, Z˜ has only one non-zero element on the diagonal, i.e., Z˜lsls =

|zls |2. Note that E[y˜j,i] = E[|fj,i,ls |2]|zls |2 = bjls |zls |2. Thus, by Bernstein’s inequality, for every t ≥ 0,

1Q

2

P Q (y˜j,i − bjls |zls | ) > t | h

i=1

≤ 2 exp −C5Q min

t2 , √ t ζ02 log K ζ0 log K

≤ 2 exp − C5 Q min t2, t , ζ02

where the last inequality is due to the fact that Q = Θ(log2 N ). We choose t1 = t < ε2/2. When bjls = 0, we have

P

1Q y˜

> t | h ≤ 2 exp − C5 Q min t2, t

,

Q

j,i

1

i=1

ζ02

11

(68)

and when bjls = 1, we have

1Q P Q y˜j,i < t1 | h
i=1

1Q ≤ P Q y˜j,i < t1 | h
i=1

1Q

2

≤ P Q y˜j,i < |zls | − t1 | h

(69)

i=1

1Q

2

≤ P Q y˜j,i − |zls | > t1 | h

i=1

≤ 2 exp − C5

Q min

t

2 1

,

t1

,

(70)

ζ02

where the inequality (69) is due to the fact that t1 < ε2/2 and

|zls |2 ≥ ε2. Deﬁne the error events Eindex = {| Q1

Q i=1

y˜j,i|

>

t1}, when bjls = 0, and Eindex = {| Q1

Q i=1

y˜j,i|

<

t1},

when

bjls = 1. Then, since Q = Θ(log2(n)) and inequalities (68)

and (70) hold for any h ∈ Eh, we have,

P{Eindex|Eh} = O(1/K3).

Now we know that

P{Eindex} = P{Eindex|Eh}P{Eh} + P{Eindex|Eh}P{Eh}
≤ P{Eindex|Eh} + P{Eh} = O(1/K3) + O(1/K3) = O(1/K3),

which completes the proof.

N. Hanson-Wright Inequality for Complex Random Variables

Theorem 22. Let γ = {γi}i∈[n] ∈ Cn be a random vector with independent entries γi, satisfying E [γi] = 0, and |γi| is sub-gaussian with |γi| ψ2 ≤ η for all i ∈ [n]. Let U ∈ Cn×n be a Hermitian matrix. Then, for every t ≥ 0,

P γHUγ − E γHUγ > t

t2

t

≤ 6 exp −c0 min η4 U 2 , η2 U .

F

28

Proof: Let α = {αi}i∈[n] and β = {βi}i∈[n] be the real and imaginary parts of γ. Then, we know that αi’s and βi’s are sub-gaussian random variables with αi ψ2 ≤ η and
βi ψ2 ≤ η for all i ∈ [n]. Note that here, although γi’s are independent, the real and imaginary parts of γi are not
necessarily independent for a certain i. In other words, for
any i, αi and βi may not be independent.
Let V and W be the real and imaginary parts of U . Since U is a Hermitian matrix, we have V = V T and W = −W T. We also know that γHU γ is a real number. Then, we have

γHU γ = αTV α − 2αTW β + βTV β.

Therefore, P γHU γ − E γHU γ > t is upper bounded by three terms,

P γHUγ − E γHUγ > t

(71)

≤ P αTV α − E αTV α > t/4

+ P αTW β − E αTW β > t/4

+ P βTV β − E βTV β > t/4 .

(72)

Since αi’s are independent and E [αi] = 0, according to the Hanson-Wright inequality for real numbers [49], we have

P αTV α − E αTV α > t/4

t2

t

≤ 2 exp −c1 min η4 V 2 , η2 V .

F

Further, we have V F ≤ U F , V ≤ U . Therefore,

P αTV α − E αTV α > t/4

≤ 2 exp −c1 min

t2

t

η4 U 2 , η2 U

F

. (73)

And similarly,

P βTV β − E βTV β > t/4

≤ 2 exp −c2 min

t2

t

η4 U 2 , η2 U

F

. (74)

Now consider the cross term. Let Wij be the entries of W . Since W = −W T, Wii = 0 for all i ∈ [n]. Then, αTW β = i=j Wijαiβj, and E αTW β = 0. Then, we can bound P αTW β > t/4 in the same way as in [49]
so that

P αTW β > t/4

≤ 2 exp −c3 min

t2

t

η4 U 2 , η2 U

F

. (75)

By combining (73), (74), and (75), Theorem 22 is proved.

O. Pseudocode
In this subsection, we provide the pseudocode of the PhaseCode algorithm. Moreover, we provide the pseudocodes of the right node processors: singleton processor, mergeable multiton processor, and resolvable multiton processor.

29

Pseudocode 1 PhaseCode Algorithm

I ←∅ for each i in {1, 2, ..., M } do

No active component is found in the beginning Find all singletons

Singleton Processor

for each i in {1, 2, ..., M } do

Find all doubletons and merge

Mergeable Multiton Processor

Color0 ← Color of the largest colored component for each in I do
if Color = Color0 then x ← None Color ← None I ←I−{ }

Find the largest colored component∗ Uncolor all other left nodes and delete all values of them

while |I| < K and any changes are made in the previous loop do

Keep resolving multitons

Resolvable Multiton Processor

Pseudocode 2 Singleton Processor
if yi,1 = yi,2 = yi,4 then ← ω1 cos−1( 2yyii,,31 )
x ← yi,1 I0 ← I0 ∪ { } Color ← new color

Check whether this right node is a singleton or not Find the index of the active left node connected to this right node
Assign a value to the active left node Declare a new found active left node Color the new active left node with a new color

Pseudocode 3 Mergeable Multiton Processor

if Right node i is connected to no colored active left node or the number of colors connected to the right node is not exactly

2 then

Return

If this right node is not mergeable

Red, Blue ← Two colors of the active left nodes connected to the right node

R ← indices of the active left nodes that are colored with Red

B ← indices of the active left nodes that are colored with Blue

r ← ∈R xj eiω

b ← ∈B x eiω

for each z1 in {+1, −1} do

φ ← z1 cos−1

|r|2 +|b|2 −yi2,1 2|r||b|

+ ∠r − ∠b

Consider two candidate Find a candidate for phase offset

if ∈R x eiω + exp(iφ) × ∈B x eiω = yi,4 then

Check the candidate with yi,4

Color Red and Color Blue are combined to a new color

for each in B do

Adjust phase of the components that are colored with Color Blue ∗

x ← x × exp(iφ)

Return

30

Pseudocode 4 Resolvable Multiton Processor if Right node i is connected to no colored active left node or they are colored with more than 1 color then

Return

If this right node is not resolvable

Color ← Common color of the connected active left nodes

I ← I ∩ {j|Hi,j = 1, 1 ≤ j ≤ n}

Colored active left nodes connected to this right node

a← b← c←

i∈I xieiω i∈I xie−iω i∈I 2 cos(ω )xi

d ← i∈I xieiω for each z1 in {+1, −1} do
α ← z1 cos−1( yi2,32−yiy,i21,y1i−,2yi2,2 ) z ← yyii,,12 exp(αi)

Consider two signs of α

k1

←

1

−

z

+

2(zb−a) c

k2 ← 1 + z

k3 ← 1 − z

k4

←

yi,3 |c|

k5 ← |k1|2 − k42|k3|2

k6 ← |k2|2 − k42|k2|2

k7 ← 2 Re(k1) Im(k2) − 2 Im(k1) Re(k2) + k42(2 Re(k2) Im(k3) − 2 Re(k3) Im(k2))

k8 ← k62 + k72 − 2k6k7 + k82

k9 ← 2k6k7 − k82 − 2k72

k10 ← k72

for each z2 in {+1, −1} do if k92 − 4k8k10 < 0 then

Consider two solutions of a quadratic equation

Contin√ue if −k9+z2 2kk892−4k8k10 < 0 then

Continue ← cos−1

√ /ω −k9+z2 k92−4k8k10
2k8

Find a candidate of

x

←

zb−a eiω −ze−iω

if yi,4 = |d + eiω

x ←x

x | then

Find a candidate of x
Check the validity of the candidates with yi,4 Assign a value to the component

I0 ← I0 ∪ { }

Declare a new found component

Color ← Color Color the new component with the color of the other components connected to the right node

Return

