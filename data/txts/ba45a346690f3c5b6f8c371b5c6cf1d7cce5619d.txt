Blind Identiﬁcation via Lifting
Henrik Ohlsson ∗,∗∗ Lillian Ratliﬀ ∗ Roy Dong ∗ S. Shankar Sastry ∗
∗ Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA (e-mail: ohlsson@eecs.berkeley.edu).
∗∗ Division of Automatic Control, Department of Electrical Engineering, Link¨oping University, Sweden.

arXiv:1312.2060v1 [cs.SY] 7 Dec 2013

Abstract: Blind system identiﬁcation is known to be an ill-posed problem and without further assumptions, no unique solution is at hand. In this contribution, we are concerned with the task of identifying an ARX model from only output measurements. We phrase this as a constrained rank minimization problem and present a relaxed convex formulation to approximate its solution. To make the problem well posed we assume that the sought input lies in some known linear subspace.

Keywords: System identiﬁcation; Parameter estimation; Identiﬁcation algorithms.

1. INTRODUCTION

Consider an auto-regressive exogenous input (ARX) model

y(t)−a1y(t − 1) − · · · − ana y(t − na) =b1u(t − nk − 1) + · · · + bnb u(t − nk − nb) (1)

with input u ∈ R and output y ∈ R. Estimation of this type of model is one of the most common tasks in system identiﬁcation and a very well studied problem, see for instance Ljung [1999]. The common setting is that
{(y(t), u(t))}Nt=1 is given and the summed residuals

N

nb

na

2

y(t) −

bk1 u(t − k1 − nk) −

ak2 y(t − k2)

t=n

k1 =1

k2 =1

where n = max(na, nk + nb) + 1, is minimized to obtain an
estimate for a1, . . . , ana , b1, . . . , bnb . This estimate is often referred to as the least squares (LS) estimate.

In this paper we study the more complicated problem of estimating an ARX model from solely outputs {y(t)}Nt=1. This is an ill-posed problem and it is easy to see that under
no further assumptions, it would be impossible to uniquely
determine a1, . . . , ana , b1, . . . , bnb . We will here therefore study this problem under the assumption that the stacked
inputs belong to some known subspace. The input could
for example be:

• known to change only at a set of discrete times due to a discrete controller or

The work presented is supported by the NSF Graduate Research Fellowship under grant DGE 1106400, NSF CPS:Large:ActionWebs award number 0931843, TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422), and FORCES (Foundations Of Resilient CybErphysical Systems), the Swedish Research Council in the Linnaeus center CADICS, the European Research Council under the advanced grant LEARN, contract 267381, a postdoctoral grant from the Sweden-America Foundation, donated by ASEA’s Fellowship Fund, and by a postdoctoral grant from the Swedish Research Council.

• known to be band-limited and therefore well represented by the projection on the ﬁrst discrete Fourier transform basis vectors.
It should be noticed that this assumption is not enough to uniquely determine the input or the ARX model. Speciﬁcally, we will not be able to decide the input or the ARX coeﬃcients b1, . . . , bnb more than up to a multiplicative scalar. It should be stressed that this is not a limitation of the method that we propose but an inherent limitation of the system identiﬁcation problem since the sought quantities always appear as products. To uniquely determine the input and the ARX coeﬃcients b1, . . . , bnb , further knowledge is needed.
The main contribution of the paper is a novel method for ARX model identiﬁcation from only output measurements. The method takes the form of a convex optimization problem and gives a computationally ﬂexible framework for handling diﬀerent types of measurement noises, constraints, etc.
2. BACKGROUND
Blind system identiﬁcan (BSI) has a broad application area and has been applied in ﬁelds such as data communications, speech recognition and seismic signal processing, see for instance Abed-Meraim et al. [1997]. Common for the type of modeling problems that BSI has been applied to is that the input is diﬃcult, costly or impossible to measure. In for example exploration seismology, the physical properties of the earth are explored by studying the response of an excitation (often a charge of dynamite). The excitation is often diﬃcult to measure and the modeling problem therefore a BSI problem, see e.g., Zerva et al. [1999].
Many methods have been proposed to solve the BSI problem throughout the years. We give a short overview here but refer the interested reader to Abed-Meraim et al.

[1997], Hua [2002], for a more extensive and complete review.
The maximum likelihood (ML) approach to BSI aims at ﬁnding the ML estimate of the model and input. The resulting non-convex optimization problem is often treated by alternating between optimizing with respect to the input and the system model. See Abed-Meraim and Moulines [1994]. The channel subspace (CS) methods to BSI indirectly determine the sought ﬁnite impulse response (FIR) model by estimating the nullspace of the Sylvester matrix associated with the FIR model to be identiﬁed. This is done by an eigen decomposition of a matrix derived from the outputs. See for instance AbedMeraim et al. [2006]. The method proposed in Zerva et al. [2000] works under the assumption that two or more output series are available and that these were generated by the same input. See also Van Vaerenbergh et al. [2013]. The methods proposed in Sato [1975], Tong et al. [1991] assume that the input consists of independent and identically (iid) distributed random variables and considers the autocorrelation of the output to decide a FIR model and the unknown input.
A number of approaches consider the blind identiﬁcation problem of Hammerstein systems under the assumption that the input is piecewise constant. Sun et al. [1999], Bai et al. [2010], Bai and Fu [2002], Wang et al. [2007, 2009, 2010]. Our approach assumes that the input belongs to some known subspace. Piecewise constant signal can be represented using the subspace assumption used here. However, we note that we are not restricted to piecewise constant signals, and our approach is signiﬁcantly diﬀerent. Also, we consider the blind identiﬁcation of ARX models while the blind identiﬁcation problem of Hammerstein systems is considered in Sun et al. [1999], Bai et al. [2010], Bai and Fu [2002], Wang et al. [2007, 2009, 2010].
The related problem of blind deconvolution have been studied in a number of contributions. In particular, see the very interesting paper by Ahmed et al. [2012] for a solution where the signals to be recovered are assumed to be in some known subspaces. The development presented in Ahmed et al. [2012] has similarities to the approach presented in this paper and was done in parellel to our work. Note that only FIR models are discussed in Ahmed et al. [2012] and that the analysis does not apply.
3. PROBLEM FORMULATION
Given the sequence of outputs {y(t)}Nt=1 ∈ R, ﬁnd an estimate for a1, . . . , ana , b1, . . . , bnb ∈ R and u(t) ∈ R, t = 1, . . . , N, such that
y(t)−a1y(t − 1) − · · · − ana y(t − na) =b1u(t − nk − 1) + · · · + bnb u(t − nk − nb) + w(t),
for t = n, . . . , N , where n = max(na, nk + nb) + 1, and w(t), t = n, . . . , N , some unknown zero mean noise. We will for simplicity assume that na, nb, nk, are known. To make the problem well posed, we will seek an input in a given subspace of RN .
4. NOTATION AND ASSUMPTIONS
We will use y to denote the output and u the input. We will for simplicity only consider single input single output

(SISO) systems, however with some extra bookkeeping also MIMO systems could be treated. We will assume that N measurements of y are available and stack them in the vector y, i.e.,

y = [y(1) . . . y(N )]T .

(2)

We also introduce u, η, a and b as

u = [u(1) . . . u(N )]T ,

(3)

η = [η(1) . . . η(N )]T ,

(4)

a = [a1 . . . ana ]T , (5)

b = [b1 . . . bnb ]T . (6)

We will use y(i) to denote the ith element of y. To pick out a subvector of y consisting of the ith to the jth element we will use the notation y(i : j) and similarly for picking out a subvector of u, a and b. To pick out a submatrix consisting of the ith to the jth rows of X we use the notation X(i : j, :). We will use normal font to represent scalars and bold for vectors and matrices. · 0 is the zero (quasi) norm which returns the number of nonzero elements of its argument and · ∗ the nuclear norm returning the sum of the singular values.

We will assume that it is known that the sought input, u, lies in some known subspace. We can hence write

u = Dx

(7)

for some known N × m-matrix D and an unknown vector x ∈ Rm. It is assumed that m ≤ N .

5. BLIND IDENTIFICATION VIA LIFTING

Consider the noise free setting where w(t) = 0, t = n, . . . , N. We can formulate the problem of ﬁnding an input and the ARX coeﬃcients as the feasibility problem

ﬁnd u(t), t = 1, . . . , N, a1, . . . , ana , b1, . . . , bnb
na
subj. to y(t)− ak2 y(t − k2)
k2 =1 nb
= bk1 u(t − nk − k1),
k1 =1

(8a)
t = n, . . . , N. (8b)

Note that the {ak}nk=a 1, {bk}nk=b 1, and {u(t)}Nt=1 are unknown. The problem is therefore non-convex.

Introduce X = xbT ∈ Rm×nb and note that (7) gives that DX = DxbT = ubT. Since ubT contains all products
u(i)bj, i = 1, . . . , N, j = 1, . . . , nb, the sum

nb

bk1 u(t − nk − k1)

(9)

k1 =1

can be realized by summing appropriate entries of DX. Problem (8) can now be reformulated as

ﬁndX,a
na

(10a)

subj. to y(t) − ak2 y(t − k2)

k2 =1 nb

= (DX)(t − nk − k1, k1), t = n, . . . , N,

k1 =1

(10b)

rank(X) = 1.

(10c)

Note that we need to require that rank(X) = 1 to not lose the possibility to decompose X as X = xbT. The problem (10) is equivalent to (8) in the following sense. Assume that (10), has a unique solution X∗, then X∗ must satisfy X∗ = x∗(b∗)T, with x∗ and b∗ solving (8). Extracting the rank 1 component of X∗, using e.g., singular value decomposition, we can hence decide both x∗ (and u∗ = Dx∗) and b∗ up to a multiplicative scalar (note that we can never do better with the information at hand, not even if we would be able to solve (8)). The estimates of a will be identical for both problems (if the estimates are unique).
The technique of introducing the matrix X to avoid products between x and b is well known in optimization and referred to as lifting [Shor, 1987, Lov´asz and Schrijver, 1991, Nesterov, 1998, Goemans and Williamson, 1995].
Problem (10) is a non-convex optimization problem and not easier to solve than (8). To get an optimization problem we can solve, we remove the rank constraint and instead minimize the rank. Since the rank of a matrix is not a convex function, we replace the rank with a convex heuristic. Here we choose the nuclear norm, but other heuristics are also available (see for instance Fazel et al. [2001]). We then obtain the convex program

min
X,a
subj. to

X∗

(11a)

na

y(t)− ak2 y(t − k2)

k2 =1 nb

= (DX)(t − nk − k1, k1), t = n, . . . , N,

k1 =1

(11b)

which we refer to as blind identiﬁcation via lifting (BIL).
Last, in the noisy setting we have to tolerate some nonzero modeling error. If the noise e is known to be bounded, say that |e(t)| ≤ , we suggest to use

min
X,a,η

X∗

na

subj. to y(t)− ak2 y(t − k2)

k2 =1 nb

= (DX)(t − nk − k1, k1) + η(t),

k1 =1

|η(t)| ≤ , t = n, . . . , N,

(12a)
(12b) (12c)

and if the noise is Gaussian,

min
X,a,η

X

∗+λ

η

2 2

na

(13a)

subj. to y(t)− ak2 y(t − k2)

k2 =1 nb

= (DX)(t − nk − k1, k1) + η(t), (13b)

k1 =1

t =n, . . . , N.

(13c)

In the latter case, we see λ > 0 as a design parameter and seek the largest λ such that X becomes rank 1.

6. ANALYSIS

The number of optimization variables in (8) is essentially na + nb + m, under the assumption that u = Dx. We can hence not expect a reliable identiﬁcation result from fewer than na + nb + m measurements. One may wonder how many measurements that are needed. Using that the constraint (11b) of BIL is linear in X, we have the following result:
Theorem 1. (Guaranteed Recovery using BIL).

Consider the noise free blind ARX identiﬁcation prob-

lem (8) and assume that it has a unique solution (up to a multiplicative scalar). Let the row vector di ∈ Rm be the
i:th row of D. If A =

 dn−nk −1 dn−nk −2 ... dn−nk −nb

y(n−1) ...

y(n−na) 

dn−nk

dn−nk −1 ... dn−nk −nb +1

y(n)

... y(n−na +1)



.



.



.

..

. .

. .

.

.

.

.



.



.



 dn−nk−2+nb

...



.

.

.

dn−nk −1 . . .

y(n+nb−2) ... y(n−na+nb−1) 



 .

.

.

.

.

.

dN −nk −1 dN −nk −2 ... dN −nk −nb y(N −1) ...

y(N −na )

has full column rank, then the ARX model and input

solving (8) are recovered, up to a multiplicative scalar,

by BIL.

Proof. From assumption, (8) has a unique solution. Form

X∗ by multiplying the solutions, x∗ and b∗, of (8). That is,

X∗

=

x∗(b∗)T.

Let

a∗1

,

.

.

.

,

a

∗ n

denote the corresponding

a
e

(8)

and

deﬁne

θ∗

as

values for a1, . . . , ana that solv

θ∗ = X∗(:, 1)T X∗(:, 2)T . . . X∗(:, nb)T −a∗1 . . . −a∗n T . a

We must have that

[y(n) y(n + 1) . . . y(N )]T = Aθ∗.

(14)

Note that the pair X∗ and a∗ is a feasible solution to (11).

Since the solution to (14) must be unique since A has full column rank, we must have that BIL gives θ∗. 2

Note that if the linear constraints of (11) alone give the solution of BIL, no optimization is necessary. Seeking the matrix X that gives the minimum nuclear norm is only of interest if we have too few measurements for the constraints to uniquely deﬁne the solution but more measurements than na + nb + m.
The noisy case is harder to analyze and we leave the analysis as future work.

7. COMPUTING λmin

In the noisy version (13) of BIL, the design parameter λ has to be chosen. Since λ regulates the tradeoﬀ between

the nuclear norm and the squared norm of the estimated noise η, it is natural to seek the largest λ such that the estimate X is rank 1. In seeking this λ, the value for λmin may come handy. λmin is deﬁned as the largest λ such that X = 0 in BIL. Since the estimate for X will stay the same for all λ ≤ λmin, we should limit our search of λ to be within [λmin ∞]. One may for example start with λ = λmin and then successively increase λ as long as rank(X) = 1.
Theorem 2. (Computing λmin).
Consider the optimization problem given in (13). There exists a λ, denoted λmin, such that whenever λ ≤ λmin, solving (13) results in X = 0. λmin is given by:

1/λmin = arg min V
V∈Rm×nb

(15a)

N

subj. to 0 = V(i, j) − 2

y(t)

t=n na

− aˆk2 y(t − k2) D(t − nk − j, i),

k2 =1

(15b)

i = 1, . . . , m, j = 1, . . . , nb, (15c)

with

N
{aˆ1, . . . , aˆna } = arg min
a t=n

na

2

y(t) − ak2 y(t − k2) .

k2 =1

(16)

Proof. The noisy version of BIL can be rewritten to take the form

N

min X ∗ + λ
X,a t=n
na

y(t)
nb

− ak2 y(t − k2) − (DX)(t − nk − k1, k1)

k2 =1

k1 =1

2
.
(17)

The nuclear norm is not diﬀerentiable and it follows that for X = 0 to be a valid solution, zero needs to be in the subdiﬀerential of the objective with respect to X evaluated at X = 0 (see e.g., Bertsekas et al. [2003, Prop. 4.7.2]). The subdiﬀerential of the objective of (17) at X = 0 and a = aˆ with respect to the (i, j)th element of X can be shown equal to

N
V(i, j) − 2λ
t=n

na
y(t) − aˆk2 y(t − k2)
k2 =1

D(t − nk − j, i). (18)

We further have that V ≤ 1 from the subdiﬀerential of the nuclear norm (see for instance Watson [1992] or Recht et al. [2010]). · is here the operator norm (the largest singular value). To ﬁnd λmin we could now consider the optimization problem

max λ
V∈Rm×nb ,λ

N

subj. to 0 = V(i, j) − 2λ

y(t)

t=n na

− aˆk2 y(t − k2) D(t − nk − j, i),

k2 =1

i = 1, . . . , m, j = 1, . . . , nb,

V ≤ 1,

which can be shown equivalent to (15). 2

λmin was also numerically veriﬁed.

(19a)
(19b) (19c) (19d)

8. SOLUTION ALGORITHMS AND SOFTWARE
Many standard methods of convex optimization can be used to solve problem (11), (12) and (13). Systems such as CVX [Grant and Boyd, 2010, 2008] or YALMIP [L¨ofberg, 2004] can readily handle the nuclear norm. For large scale problems, the alternating direction method of multipliers (ADMM, see e.g., Bertsekas and Tsitsiklis [1997], Boyd et al. [2011]) is an attractive choice and we have previously shown that ADMM can be very eﬃcient on similar problems Ohlsson et al. [2013]. Code for solving (11), (12) and (13) will be made available on http://www.rt.isy. liu.se/~ohlsson/code.html
9. NUMERICAL ILLUSTRATION
Consider the system given in the diagram below.
e

x

u y(t)−a1y(t − 1) = b1u(t − 1)

y

ZOH

+b2u(t − 2) + b3u(t − 3) + e(t)

Here the values x were generated by independently sam-

pling from a unit Gaussian and the noise e by indepen-

dently sampling from a uniform distribution between − /2

and /2. The ZOH (zero-order hold) block holds the input

to the ARX system constant for 6 consecutive samples. We

can therefore express u in terms of x as

16×1 06×1 06×1 . . . 06×1 06×1 16×1 06×1 . . . 06×1

 u=

...



06×1 . . .

06×1 . . .

...

...

  x.



16×1 06×1

06×1 16×1

(20)

We identify the matrix in the relation between u and x as D. The ARX coeﬃcients used were

a1 = −0.3, b3 = 1, b2 = 2, b1 = 3.

(21)

Figure 1 shows the output y for = 5.

If the noisy version of BIL (12) is used to estimate u and an ARX model, we get the input-estimate given in Figure 2 and the ARX coeﬃcients:

a1 = −0.21, b3 = 0.91, b2 = 1.80, b1 = 2.7.

(22)

It is interesting to notice that if we instead would be given the true input u and only estimated the ARX coeﬃcients

15

10 0.5
5 0.4
0
0.3 −5

−10

0.2

y

−15

−20 0

50

100

150

200

t

Fig. 1. The noise free (solid line) and noisy outputs (circles).

3

2

1

u

0

−1

−2

−3

0

50

100

150

200

t

Fig. 2. The estimated (dashed line) and the true input u (solid line).
by minimizing the squared residuals between the output y and the predicted output, we would have got the estimates:
a1 = −0.30, b3 = 0.88, b2 = 2.39, b1 = 2.85. (23)
As seen, these estimates are not that much better than what BIL is providing (see (22)). Remember that BIL is only given the y-measurements and not the inputs u. It is therefore quite remarkable that the estimates of BIL is comparable to those given in (23).
To further study the robustness of BIL we carried out a Monte Carlo simulation. In the simulation, the noise level was varied between 0 and 5. For each noise level, 100 trials were carried out with diﬀerent noise and input realizations. The true ARX model was kept ﬁxed (the same as above). The results are summarized in Figure 3.
The setup of above example does not give that A has full column rank. Nevertheless, a perfect result was obtained in the noise free case. It can however be veriﬁed that if D is instead generated by independently sampling each

0.1

||∆ b|| / || b ||

||∆ a|| / || a ||

||∆ u|| / || u ||

0

0

1

2

3

4

5

ε

Fig. 3. The relative errors along with their 0.5 standard deviation error bounds for varying noise levels.
element from a unit Gaussian distribution (everything else unchanged), the resulting A has full column rank.
10. CONCLUSION
This paper presented a novel framework for blind system identiﬁcation of ARX model. The framework uses the fact that the problem can be rewritten as a rank minimization problem. A convex relaxation is presented to approximate the sought ARX parameters and the unknown inputs.
REFERENCES
K. Abed-Meraim and E. Moulines. A maximum likelihood solution to blind identiﬁcation of multichannel FIR ﬁlters. In EUSIPCO, pages 1011–1015, 1994.
K. Abed-Meraim, Wanzhi Qiu, and Y. Hua. Blind system identiﬁcation. Proceedings of the IEEE, 85(8):1310– 1322, 1997.
K. Abed-Meraim, P. Loubaton, and E. Moulines. A subspace algorithm for certain blind identiﬁcation problems. IEEE Transansactions on Information Theory, 43(2):499–511, September 2006.
A. Ahmed, B. Recht, and J. Romberg. Blind deconvolution using convex programming. CoRR, abs/1211.5608, 2012.
E.-W. Bai and Minyue Fu. A blind approach to Hammerstein model identiﬁcation. IEEE Transactions on Signal Processing, 50(7):1610–1619, 2002.
E.-W. Bai, Q. Li, and S. Dasgupta. Blind identiﬁability of IIR systems. Automatica, 38(1):181–184, 2010.
D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Athena Scientiﬁc, 1997.
D. P. Bertsekas, A. Nedic, and A. E. Ozdaglar. Convex Analysis and Optimization. Athena Scientiﬁc, 2003.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
M. Fazel, H. Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system

approximation. In Proceedings of the 2001 American Control Conference, pages 4734–4739, 2001. M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and satisﬁability problems using semideﬁnite programming. J. ACM, 42 (6):1115–1145, November 1995. M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In V. D. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95–110. Springer-Verlag, 2008. http:// stanford.edu/~boyd/graph_dcp.html. M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21. http:// cvxr.com/cvx, August 2010. Y. Hua. Blind methods of system identiﬁcation. Circuits, Systems and Signal Processing, 21(1):91–108, 2002. ISSN 0278-081X. L. Ljung. System Identiﬁcation — Theory for the User. Prentice-Hall, Upper Saddle River, N.J., 2nd edition, 1999. J. L¨ofberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In Proceedings of the CACSD Conference, Taipei, Taiwan, 2004. URL http:// control.ee.ethz.ch/~joloef/yalmip.php. L. Lov´asz and A. Schrijver. Cones of matrices and setfunctions and 0-1 optimization. SIAM Journal on Optimization, 1:166–190, 1991. Y. Nesterov. Semideﬁnite relaxation and nonconvex quadratic optimization. Optimization Methods & Software, 9:141–160, 1998. H. Ohlsson, A. Y. Yang, R. Dong, M. Verhaegen, and S. S. Sastry. Quadratic basis pursuit. CoRR, abs/1301.7002, 2013. B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010. Y. Sato. A method of self-recovering equalization for multilevel amplitude-modulation systems. IEEE Transactions on Communications, 23(6):679–682, 1975. N.Z. Shor. Quadratic optimization problems. Soviet Journal of Computer and Systems Sciences, 25:1–11, 1987. L. Sun, W. Liu, and A. Sano. Identiﬁcation of a dynamical system with input nonlinearity. IEE Proceedings – Control Theory and Applications, 146(1):41–51, 1999. L. Tong, G. Xu, and T. Kailath. A new approach to blind identiﬁcation and equalization of multipath channels. In 1991 Conference Record of the TwentyFifth Asilomar Conference on Signals, Systems and Computers, volume 2, pages 856–860, 1991. S. Van Vaerenbergh, J. V´ıa, and I. Santamar´ıa. Blind identiﬁcation of SIMO Wiener systems based on kernel canonical correlation analysis. IEEE Transactions on Signal Processing, 61(9):2219–2230, 2013. J. Wang, A. Sano, D. Shook, T. Chen, and B. Huang. A blind approach to closed-loop identiﬁcation of Hammerstein systems. International Journal of Control, 80(2): 302–313, 2007. J. Wang, A. Sano, T. Chen, and B. Huang. Identiﬁcation of Hammerstein systems without explicit parameterisation of non-linearity. International Journal of Control, 82(5): 937–952, 2009.

J. Wang, A. Sano, T. Chen, and B. Huang. A blind approach to identiﬁcation of Hammerstein systems. In F. Giri and E.-W. Bai, editors, Block-oriented Nonlinear System Identiﬁcation, volume 404 of Lecture Notes in Control and Information Sciences, pages 293–312. Springer London, 2010.
G.A. Watson. Characterization of the subdiﬀerential of some matrix norms. Linear Algebra and its Applications, 170(0):33–45, 1992. ISSN 0024-3795.
A. Zerva, A.P. Petropulu, and P.-Y. Bard. Blind deconvolution methodology for site-response evaluation exclusively from ground-surface seismic recordings. Soil Dynamics and Earthquake Engineering, 18(1):47–57, 1999.
A. Zerva, A.P. Petropulu, and P.-Y. Bard. Blind system identiﬁcation. In Proceedings of the 8th ASCE Joint Specialty Conference on Probabilistic Mechanics and Structural Reliability, PMC2000, University of Notre Dame, Indiana, USA, 2000.

