A COMPARATIVE STUDY ON NON-AUTOREGRESSIVE MODELINGS FOR SPEECH-TO-TEXT GENERATION
Yosuke Higuchi1, Nanxin Chen2, Yuya Fujita3, Hirofumi Inaguma4, Tatsuya Komatsu5, Jaesong Lee6, Jumon Nozaki4,5, Tianzi Wang2, Shinji Watanabe7
1Waseda University, 2Johns Hopkins University, 3Yahoo Japan Corporation, 4Kyoto University, 5LINE Corporation, 6Naver Corporation, 7Carnegie Mellon University

arXiv:2110.05249v1 [eess.AS] 11 Oct 2021

ABSTRACT
Non-autoregressive (NAR) models simultaneously generate multiple outputs in a sequence, which signiÔ¨Åcantly reduces the inference speed at the cost of accuracy drop compared to autoregressive baselines. Showing great potential for real-time applications, an increasing number of NAR models have been explored in different Ô¨Åelds to mitigate the performance gap against AR models. In this work, we conduct a comparative study of various NAR modeling methods for end-to-end automatic speech recognition (ASR). Experiments are performed in the state-of-the-art setting using ESPnet. The results on various tasks provide interesting Ô¨Åndings for developing an understanding of NAR ASR, such as the accuracy-speed trade-off and robustness against long-form utterances. We also show that the techniques can be combined for further improvement and applied to NAR end-to-end speech translation. All the implementations are publicly available to encourage further research in NAR speech processing.
Index Terms‚Äî Non-autoregressive sequence generation, endto-end speech recognition, end-to-end speech translation
1. INTRODUCTION
In the last decade, deep learning has brought remarkable success in automatic speech recognition (ASR) [1, 2], which has become a central user interface in various IoT applications. Much of the recent research progress is attributed to improvement in the endto-end system [3‚Äì5], where an ASR model is trained to directly optimize speech-to-text conversion. Owing to the well-established sequence-to-sequence modeling techniques [6‚Äì8] and more sophisticated neural network architectures [9‚Äì11], end-to-end ASR systems have achieved comparable results with those of the conventional hybrid systems [12‚Äì14].
Current state-of-the-art end-to-end ASR systems are based on autoregressive (AR) models [6, 8], where each token prediction is conditioned on the previously generated tokens (Figure 1 left). Such a generation process can lead to slow inference speed, requiring L-step incremental calculations to generate an L-length sequence. Non-autoregressive (NAR) models [15, 16], in contrast, permit generating multiple tokens in parallel, which signiÔ¨Åcantly speeds up the decoding process (Figure 1 right). However, such simultaneous predictions often prevent the NAR models from learning the conditional dependencies between output tokens, worsening the recognition accuracy compared to AR models.
Fast inference is one of the crucial factors for deploying deep learning models to real-world applications. ASR systems, in particular, are desired to be fast and light in numerous scenes, such as in

ùë¶! ùë¶" ùë¶# ùë¶$ „Äàeos„Äâ

ùë¶! ùë¶" ùë¶# „Äàùë¶e$os„Äâ

„Äàsos„Äâ
Autoregressive ASR

Non-autoregressive ASR

Fig. 1. Illustrations of autoregressive and non-autoregressive ASR.
spoken dialogue systems, where users prefer quick interactions with a conversational agent. Accordingly, various attempts have been actively made to develop and improve NAR end-to-end ASR models [17‚Äì23], inspired by the great success of NAR modeling techniques in neural machine translation [24‚Äì30]. However, while an increasing number of NAR models have been proposed and shown their effectiveness, the research community lacks a comprehensive study for comparing different NAR methods in a fair experimental setting. Hence, it remains unclear what the advantages and disadvantages each model has when compared to other models.
Our work aims to conduct a comparative study on NAR modeling methods for end-to-end ASR. We have made an effort to cover a wide variety of methods, including a standard connectionist temporal classiÔ¨Åcation (CTC)-based model [3]; Mask-CTC [19] and Improved Mask-CTC [31] based on masked language modeling [28, 32]; Align-Denoise [33] based on reÔ¨Ånement training [25]; Insertion Transformer and KERMIT [21] based on insertion-based modeling [26, 34]; intermediate CTC [35] and self-conditioned CTC [36] based on regularization techniques; and a continuous integrate-and-Ô¨Åre (CIF)-based NAR model (CIF-NA) [37]. All the models are fairly evaluated in the state-of-the-art setup using ESPnet [38], adopting Conformer [11] for the network architecture.
The contributions of this work are summarized as follows:
‚Ä¢ We conduct comparative experiments on a variety of NAR ASR models using various ASR tasks. In addition to comparing the accuracy and speed, we further analyze the results to help develop a deep understanding of NAR ASR.
‚Ä¢ We show that different NAR techniques can be combined to improve the performance and applied to other NAR speechto-text tasks, e.g., end-to-end speech translation.
‚Ä¢ We provide reproducible implementations and recipes used in our experiments, hoping to encourage further research in NAR speech processing.

Table 1. Comparison of various NAR end-to-end ASR models.

Model

#iter

Processing unit CTC

A-CMLM [17] Imputer [18] LASO [20] Spike-Triggered [22] Mask-CTC [19] Improved Mask-CTC [31] Align-ReÔ¨Åne [23] Align-Denoise [33] Insertion Transformer [21] KERMIT [21] Intermediate CTC [35] Self-conditioned CTC [36] CIF-NA [39]

3 8 1 1 10 5 5 1 log2(L) log2(L) 1 1 1

token frame token token token token frame frame token token frame frame token

2. RELATED WORKS
In Table 1, we list several NAR end-to-end ASR models to help compare and understand them at a glance. Here, we include some important aspects for the comparison. The number of decoding iterations (#iter) can be increased to improve an output sequence at the expense of extra computations, which enables a model to generate more valid tokens conditioned on previously generated tokens in a semi-autoregressive manner [25, 28]. Processing unit is a unique property in NAR ASR, which can be either frame-level or tokenlevel. Token-level processing reduces the speed and computational cost during inference, which is especially important when a model performs the iterative prediction. However, it needs additional efforts to estimate or adjust the length of an output sequence [16]. Frame-level processing, on the other hand, is prone to slow inference with the requirement of computational resources, but it does not require the length prediction. CTC indicates the usage of connectionist temporal classiÔ¨Åcation (CTC) [15], which is a promising NAR modeling method for ASR.
CTC is the very fundamental method for realizing NAR end-toend ASR. CTC makes a strong conditional independence assumption between token frame predictions, enabling the model to perform fast inference while limiting the recognition accuracy compared to other end-to-end ASR models [40]. Inspired by the conditional masked language model (CMLM) [28], Audio-CMLM (A-CMLM) [17] effectively learns the conditional distribution of output tokens over a partially observed sequence through the NAR mask prediction task [32]. Imputer [18] and Mask-CTC [19, 31] combine CTC with CMLM to improve frame-level or token-level CTC predictions, getting rid of the cumbersome length prediction required in the previous approach. While Imputer and Mask-CTC suffer from the mismatch between training and testing conditions, Align-ReÔ¨Åne [23] and Align-Denoise [33] introduce iterative reÔ¨Ånement [25] to optimize the reÔ¨Ånement process of CTC predictions directly.
Some of the recent efforts in NAR end-to-end ASR focus on improving the performance of the standard CTC-based model itself. Intermediate CTC [35] and self-conditioned CTC [36] apply auxiliary CTC losses to intermediate layers as in [41], which effectively enhances the intermediate representations and leads to improved CTC performance. Convolution-based neural network architectures have been shown to improve the CTC-based and the other end-to-end ASR models in general [31, 42, 43]. When a large amount of speech data is available for pre-training, powerful speech representations learned by wav2vec 2.0 [44] can signiÔ¨Åcantly boost the performance of CTC [42].
Another direction for NAR ASR is based on insertion-based

modeling, which permits the model for generating tokens in an arbitrary order without the left-to-right constraint in AR models. Showing promising results in neural machine translation, Insertion Transformer [26] and Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT) [34] are successfully adopted for end-to-end ASR [21].

3. NON-AUTOREGRESSIVE ASR
This section reviews NAR modeling methods for end-to-end ASR compared in our study, including CTC, Mask-CTC, Improved MaskCTC, Align-Denoise, Insertion Transformer, KERMIT, intermediate CTC, self-conditioned CTC, and CIF-NA. We have made an effort to cover a wide variety of methods, each of which has a unique capability as an NAR model, as described in Section 2. Notations: We formulate end-to-end ASR as a sequence mapping between a T -length input sequence X = (xt ‚àà RD|t = 1, . . . , T ) and an L-length output sequence Y = (yl ‚àà V|l = 1, . . . , L). Here, xt is a D-dimensional acoustic feature at frame t, yl an output token at position l, and V a vocabulary.

3.1. Connectionist temporal classiÔ¨Åcation (CTC)

CTC [15] predicts a frame-level alignment sequence Z = (zt ‚àà V ‚à™ { }|t = 1, . . . , T ), which is obtained by introducing a special blank token into the output sequence Y . Based on the conditional independence assumption per token frame prediction, CTC models the conditional probability P (Y |X) by marginalizing over all paths (frame alignments) as:

T

Pctc(Y |X) =

P (zt|X),

(1)

Z‚ààB‚àí1(Y ) t=1

where B‚àí1(Y ) denotes all possible paths compatible with Y . The CTC objective is deÔ¨Åned by the negative log-likelihood of Eq. (1):

Lctc = ‚àí log Pctc(Y |X).

(2)

During inference, we use the best path decoding [15] to generate an output sequence, where the most probable tokens argmaxZ P (Z|X) are selected at each frame, and an output sequence is obtained by
suppressing repeated tokens and removing blank symbols.

3.2. Mask-CTC

Mask-CTC [19] is built upon an encoder-decoder structure, where the CTC loss (Eq. (1)) is applied to the encoder output, and the decoder adopts the conditional masked language model (CMLM) [17, 28]. The CMLM decoder is trained to predict output tokens Ymask ‚àà Y , given a partially observed ground-truth sequence Yobs = Y \Ymask:

Pcmlm(Ymask|Yobs, X) =

P (y|Yobs, X),

(3)

y‚ààYmask

where Ymask are obtained by randomly replacing ground-truth tokens with a special mask token <MASK>. The objective of the CMLM decoder is deÔ¨Åned as:

Lcmlm = ‚àí log Pcmlm(Ymask|Yobs, X).

(4)

The Ô¨Ånal loss of Mask-CTC is deÔ¨Åned as a weighted sum of the standard CTC loss Lctc in Eq. (2) and Lcmlm in Eq. (4). During inference, an output sequence of CTC is Ô¨Årst obtained from the encoder, and then the decoder reÔ¨Ånes the CTC output through the mask prediction process based on Eq. (3). By masking low-conÔ¨Ådence tokens in the CTC output and predicting the masked tokens based on the

other high-conÔ¨Ådence unmasked tokens, errors from the conditional independence assumption are expected to be recovered.
3.3. Improved Mask-CTC
One limitation of Mask-CTC is that the length of an output sequence cannot be changed from that of the CTC output during inference, making it difÔ¨Åcult to recover deletion and insertion errors. To overcome this problem, Mask-CTC is enhanced by introducing a length prediction network in the CMLM decoder [31]. The length prediction network is trained to predict the length of a partial target sequence from a masked token. For example, given a ground-truth sequence Y = [y1, y2, y3, y4] and its masked sequence [y1, <MASK>, y4, <MASK>], symbols 2 and 0 are predicted from each masked position, indicating the length of the corresponding partial sequence in Y . With this length prediction network, during inference, the number of <MASK> at each masked position is Ô¨Årst modiÔ¨Åed based on the predicted length. Each mask is then predicted conventionally as in Eq. (3), allowing the model to change the sequence length dynamically by deleting and inserting mask tokens.

3.4. Align-Denoise
Align-Denoise [33] is built on the prior work of Align-ReÔ¨Åne [23] and Imputer [18] and adopts a similar encoder-decoder structure on frames. CTC is applied to both the encoder output and decoder output. Instead of taking multiple steps to get the intermediate results as Align-ReÔ¨Åne, Align-Denoise generates noisy CTC alignment ZÀú during the training and requires a single iteration for decoding. The training objective is based on the CTC loss in Eq. (2) and a groundtruth alignment Zgt = argmaxZ P (Z|X):
Laligndenoise = EZÀú‚àºq(ZÀú|X,Zgt) log Pctc(Y |X, ZÀú) , (5)
where q(¬∑) is the noisy function for generating the noisy alignment. The idea is similar to the denoising autoencoder [45] (DAE), where the input to the decoder is the ground truth alignment with a certain level of noise.

3.5. Intermediate CTC

Intermediate CTC [35] extends the CTC-based modeling with a regularization loss. During training, a sequence of intermediate representations Xinter is obtained from an intermediate layer of the encoder, and its intermediate CTC loss is computed as:

Linter = ‚àí log Pctc(Y |Xinter).

(6)

The Ô¨Ånal loss is a weighted sum of the original CTC loss in Eq. (2) and intermediate loss. During inference, the intermediate loss is unused, and the model is treated as an ordinary CTC model.

3.6. Self-conditioned CTC

Self-conditioned CTC [36] extends intermediate CTC by exploiting the intermediate representations Xinter for conditioning the subsequent encoder layers. During both training and inference, each token posterior distribution in the intermediate layers Ainter = softmax(Xinter) is fed back to the input of the next layer, making the subsequent encoder layers conditioned on the intermediate predictions. The self-conditioned CTC loss is deÔ¨Åned as:

Lselfcond = ‚àí log Pctc(Y |X, Ainter).

(7)

The Ô¨Ånal loss is a weighted sum of the intermediate CTC loss in Eq. (6) and the self-conditioned CTC loss.

3.7. Insertion Transformer

In the case of insertion-based NAR models, the conditional probability P (Y |X) is modeled by marginalizing over insertion order œÄ:

Pins(Y |X) = P (Y, œÄ|X) = P (Y œÄ|X)P (œÄ). (8)

œÄ

œÄ

Insertion order œÄ represents the permutation of tokens in a sequence Y , e.g., if L = 4 and œÄ = (3, 1, 2, 4), Y œÄ = (y3, y1, y2, y4).
During training, an upper bound of negative log-likelihood is minimized under a prior distribution over insertion order œÄ:

Lins = ‚àí P (œÄ) log P (Y œÄ|X) ‚â• ‚àí log Pins(Y |X). (9)
œÄ

Insertion Transformer [26] trains Transformer, with an encoder-
decoder structure, to predict a token and its position to be inserted, which aims to model P (Y œÄ|X) in Eq. (8). When P (œÄ)
is deÔ¨Åned by the balanced binary tree (BBT)-based insertion order [26], decoding Ô¨Ånishes empirically with log2(L) iterations. The BBT order is to insert the centermost tokens of the cur-
rent hypothesis. For example, given an output sequence with L = 7, the hypothesis grows based on the tree structure like (y4) ‚Üí (y2, y4, y6) ‚Üí (y1, y2, y3, y4, y5, y6, y7).

3.8. KERMIT

KERMIT [34] is a variant of Insertion Transformer. Its basic formulation is the same but only the Transformer encoder is used to predict a token and its position to be inserted.
KERMIT can be trained with the CTC loss in a multi-task learning manner [21], which makes the CTC alignment prediction conditioned on a partial hypothesis from the insertion-based decoding. Given the posterior probability of a CTC alignment at k-th decoding step as P (Z|Y œÄk , X), the objective of KERMIT is to minimize the following negative log-likelihood:

Lkermit = Lins ‚àí Œªkermit log

P (Z|Y œÄk , X),

Z‚ààB‚àí1(Y )

(10)

where Œªkermit is a tunable weight on the CTC loss. During inference, the CTC decoding is performed using P (Z|Y œÄk , X) in Eq. (10),
and it can be terminated at any number of iterations.

3.9. CIF-NA

Continuous integrate-and-Ô¨Åre (CIF) [37] provides a soft and monotonic alignment in the encoder-decoder framework. CIF Ô¨Årst learns information weights (Œ±1, ...Œ±T ) from the encoder output Xenc and accumulates the weights from left to right to locate the acoustic boundary. Then, the acoustic embedding C = (c1, .., cL) for each target token is obtained by integrating the encoder sates based on their estimated weights. In this paper, we investigated CIF with an NAR decoder, noted as CIF-NA. Following the prior work [39], both the encoder states and acoustic embeddings are fed into the decoder to predict the probability of output tokens in parallel:

L

Lcif = ‚àílog P (yl|C, Xenc).

(11)

l=1

CIF also adopts a quantity loss to supervise the model to predict the

quantity of the integrated embeddings closer to the length of a target

sequence, deÔ¨Åned as Lqua = |

T t=1

Œ±t

‚àí

L|.

The

Ô¨Ånal

loss

of

CIF-

NA is a weighted sum of the CTC loss in Eq. (2), Lcif and Lqua.

Table 2. Word error rate (WER) or character error rate (CER) on LibriSpeech-100h (LS-100), TEDLIUM2 (TED2), and CSJ-APS. #iter denotes the number of iterations required to generate each token in an output sequence. Real time factor (RTF) was used to measure the inference speed and was evaluated on the LS-100 ‚Äútest-other‚Äù set using CPU.

Model

Inference speed

LS-100 (WER)

TED2 (WER)

CSJ-APS (CER)

#iter

dev

test

RTF Speedup clean other clean other dev test eval1 eval2 eval3

CTC/attention + beam-search
AR Transducer + beam-search

L

0.341 1.00√ó

6.8 18.8 7.4 19.0 11.6 8.7 5.4 4.0

9.8

>L

3.419 0.10√ó

6.3 18.2 6.8 18.5 10.4 8.4 5.1 3.8

9.0

L

0.069 4.94√ó

7.3 19.9 7.6 19.9 9.6 9.2 6.3 4.5 10.6

>L

0.234 1.46√ó

6.4 18.8 6.8 18.9 8.6 8.2 5.2 4.1 10.0

CTC

1

0.059 5.78√ó

7.4 20.5 7.8 20.8 8.9 8.6 5.4 4.0

9.6

Mask-CTC

10

0.063 5.41√ó

7.2 20.3 7.5 20.6 8.9 8.5 5.6 4.0

9.6

Improved Mask-CTC

5

0.072 4.74√ó

7.0 19.8 7.3 20.2 8.8 8.3 5.5 4.0

9.5

Align-Denoise

1

0.073 4.67√ó

8.0 22.3 8.4 22.5 9.0 8.7 5.4 3.7

9.1

NAR Intermediate CTC

1

0.059 5.78√ó

6.9 19.7 7.1 20.2 8.5 8.3 5.6 4.1

9.8

Self-conditioned CTC

1

0.059 5.78√ó

6.6 19.4 6.9 19.7 8.7 8.0 5.3 3.7

9.1

KERMIT

log2(L) 0.361 1.06√ó

7.1 19.7 7.4 20.2 9.1 8.2 5.4 3.7

9.5

Insertion Transformer

log2(L) 0.083 4.11√ó 16.0 27.3 16.2 27.4

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

CIF-NA‚Ä†

1

0.073 4.67√ó 15.4 34.0 15.7 34.6 ‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Ä†According to discussions with the author of CIF [37], CIF-NA suffers from the degradation due to the difÔ¨Åculty of acoustic boundary decisions.

4. EXPERIMENTS
Aiming to compare the NAR models in Section 3, we conducted ASR experiments using ESPnet [38, 46]. In addition to the NAR models, we evaluated autoregressive (AR) models, including the attention-based sequence-to-sequence model with the CTC/attention objectives [47, 48] and Conformer-Transducer [49]. The recognition accuracy was evaluated based on word error rate (WER) or character error rate (CER), depending on a task, and the inference speed was measured using real time factor (RTF).
4.1. Experimental setup
Data: The main experiments were carried out using three datasets, including LibriSpeech (LS) [50], TEDLIUM2 (TED2) [51], and Corpus of Spontaneous Japanese (CSJ) [52]. LS consists of utterances from read English audiobooks, and we used the 100-hour subset (LS-100) for training. TED2 contains utterances from English Ted Talks, and we used the 210-hour training data. CSJ includes Japanese public speeches on different academic topics, and we used the 271-hour subset of academic presentation speech (CSJ-APS) for training. For LS-100 and TED2, we used the standard validation and test sets for tuning hyper-parameters and evaluating performance, respectively. SpeciÔ¨Åcally, for LS-100, the validation and test sets are divided into ‚Äúclean‚Äù and ‚Äúother‚Äù based on the quality of the recorded utterances. For CSJ-APS, we used a part of the training set as a validation set and the ofÔ¨Åcial evaluation sets (‚Äúeval1‚Äù, ‚Äúeval2‚Äù, and ‚Äúeval3‚Äù) for testing. For LS-100 and TED2, we used 300 to 500 subwords for tokenizing output texts, which were constructed from each training set using SentencePiece [53]. For CSJ-APS, we used Japanese syllable characters (Kana) and Chinese characters (Kanji), which resulted in 2753 distinct tokens.
We also evaluated several models under noisy conditions using CHiME-4 [54]. CHiME-4 contains English recordings in everyday noisy environments, including a cafe, a street junction, public transport, and a pedestrian area. We combined the CHiME-4 and Wall Street Journal (WSJ) [55] datasets to obtain a 190-hour training set. We used the validation and test sets provided by CHiME4 for tuning hyper-parameters and evaluating performance, respectively. Charac-

ters (Latin alphabets) were used for tokenizing target texts.
As input speech features, we extracted 80 mel-scale Ô¨Ålterbank coefÔ¨Åcients with three-dimensional pitch features using Kaldi [56]. To avoid overÔ¨Åtting, we applied speed perturbation [57] and SpecAugment [58] to the input speech from LS-100, TED2, and CSJ-APS, and only SpecAugment to the input speech from CHiME4.
Network architecture: All the end-to-end ASR models were constructed based on the Conformer-based architecture [11, 49]. For the self-attention module, the number of heads dh, the dimension of a self-attention layer dmodel, and the dimension of a feed-forward network dÔ¨Ä were set to 4, 256, and 1024, respectively. The kernel size of the convolution module was set to 15. For the models with the encoder-decoder structure (i.e., CTC/attention, MaskCTC, Improved Mask-CTC, Align-Denoise, Insertion Transformer, and CIF-NA), the encoder consisted of two convolutional neural network (CNN)-based downsampling layers followed by a stack of 12 Conformer encoder blocks. The decoder consisted of 6 Transformer decoder blocks, where the self-attention module had the same conÔ¨Åguration as the Conformer block, except dÔ¨Ä was increased to 2048 to match the number of parameters in the Conformer block. For the models without the decoder (i.e., CTC, KERMIT, Intermediate CTC, Self-conditioned CTC), we used two CNN-based downsampling layers followed by a stack of 18 Conformer encoder blocks. Conformer-Transducer consisted of the 18-layer encoder and a single long short-term memory (LSTM) layer for the decoder. Note that the number of parameters in all the ASR models was around 30M.
Training and decoding conÔ¨Ågurations: The ASR models were trained using the Adam optimizer [59] with Œ≤1 = 0.9, Œ≤2 = 0.98,
= 10‚àí9, and the Noam learning rate scheduling [60]. Warmup steps (e.g., 4k to 25k) and a learning rate factor (e.g., 1.0 to 10.0) were tuned for each model. We followed the same setup as in [14, 49] for regularization hyperparameters (e.g., dropout rate and labelsmoothing weight). The models were trained up to 300 epochs until convergence. For evaluation, a Ô¨Ånal model was obtained by averaging model parameters over 5 to 30 checkpoints with the best validation performance. For decoding with the CTC-based NAR models, we performed the best path decoding of CTC [15] to keep the inference process NAR. For evaluating the AR models, we applied beam search decoding with beam sizes of 1 (i.e., greedy decoding) and

10. All of the decodings were done without using external language models (LMs). Decoding hyper-parameters that are unique to each model were tuned following the previous works. To evaluate the inference speed, RTF was measured using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz on the same machine environment. All of the implementations are publicly available1.
4.2. Main results
Table 2 shows the results on LS-100, TED2, and CSJ-APS. LibriSpeech-100h (LS-100): By comparing the results obtained from the NAR models on LS-100, all of our NAR models, except Insertion Transformer and CIF-NA, outperformed the standard CTCbased model. Moreover, on the ‚Äúclean‚Äù sets, Improved Mask-CTC, intermediate CTC, self-conditioned CTC, and KERMIT achieved comparable performances with those of the AR models. On the ‚Äúother‚Äù sets, in contrast, the NAR models resulted in worse performances than the CTC/attention model. Since the ‚Äúother‚Äù sets include utterances with lower quality than the ‚Äúclean‚Äù sets, a model is required to capture dependencies between output tokens to compensate for information loss in the low-quality speech. With the language model mechanism included in the network structure, the CTC/attention model resulted in better performances on ‚Äúother‚Äù sets, effectively modeling dependencies between output tokens. TEDLIUM2 (TED2): On TED2, all the NAR models, including the standard CTC-based model, achieved results competitive to those obtained from the AR models. Especially on the development set, the NAR models outperformed the CTC/attention model by a large margin. We further discuss this interesting outcome in Section 4.5. CSJ-APS: All the NAR models achieved comparable performance with the AR models on CSJ-APS, indicating the effectiveness independent of the language. Especially, the performances of AlignDenoise and Self-conditioned CTC aligned with the searched results obtained from the CTC/attention model. Discussions on Insertion Transformer and CIF-NA: Insertion Transformer resulted in the performance drop because of its two unique characteristics. The Ô¨Årst is that it is difÔ¨Åcult to judge whether more tokens to be inserted or not. It is controlled by a special token that represents ‚Äúno more token to be inserted‚Äù. However, partial hypothesis including recognition errors leads to misjudging of token insertion. The second is that once a hypothesis encounters recognition error, it can not be recovered during inference. The error is propagated to later iterations and leads to more errors. CIF is a novel alignment mechanism that can be adopted for various ASR scenarios. However, different from the original AR version, CIFNA resulted in the performance degradation due to the difÔ¨Åculty of estimating acoustic boundaries in the English speech dataset. According to [37, 39], it could achieve competitive performance in monosyllable languages with clear acoustic boundaries (e.g., Chinese Mandarin).
From the above results, it can be concluded that CTC is the key technique to realizing effective NAR end-to-end ASR, as all the well-performing models are based on CTC. Overall, selfconditioned CTC resulted in the best performance among the NAR models, Ô¨Ålling the gap against the AR performance.
4.3. Recognition accuracy vs. inference speed
We study the trade-off between recognition accuracy and inference speed. According to RTF results in Table 2, which are measured on
1Mask-CTC, Improved Mask-CTC, Align-Denoise, Intermediate CTC, Self-conditioned CTC, Insertion Transformer, KERMIT, CIF-NA

Table 3. Error analysis on LS-100. WERs on Table 2 are split into

substition (sub), deletion (del), and insertion (ins) error rates.

Model

test-clean sub del ins

test-other sub del ins

AR

CTC/attention + beam-search

5.3 1.2 0.9 14.5 2.4 2.1 5.4 0.6 0.8 14.7 1.8 2.1

NAR

CTC

6.3 0.7 0.8 16.6 2.2 1.9

Improved Mask-CTC 5.9 0.6 0.8 16.2 2.0 2.0

Intermediate CTC

5.7 0.6 0.8 16.2 1.9 2.1

Self-conditioned CTC 5.6 0.6 0.7 15.8 2.0 1.9

KERMIT

6.0 0.6 0.8 16.2 2.0 1.9

the ‚Äútest-other‚Äù set of LS-100, all the NAR models except KERMIT achieved fast inference speed compared to the AR models. This is because of the non-autoregressive nature of NAR models, i.e., it can generate multiple tokens at a single iteration step. The RTF improvement is achieved at the expense of quality-drop in WER. Especially on the ‚Äúother‚Äù sets of LS-100, NAR models did not reach the performance obtained from the CTC/attention model. In contrast, on the ‚Äúclean‚Äù sets, several NAR models (e.g., self-conditioned CTC) achieved better WERs than the greedy results of the CTC/attention model. Improving the recognition of difÔ¨Åcult utterances is an important problem for NAR models to be solved.
4.4. Error analysis
In Table 3, we break down WERs on LS-100 reported in Table 2. By comparing the results obtained from CTC and the other NAR models, it appeared that the major improvements on our NAR models are attributed to reducing substitution errors. Deletion and insertion error rates, in contrast, were kept relatively low and stayed almost the same from the CTC results, which can often be reduced by using wordpieces. The AR model handled substitution errors more effectively than the NAR models. However, deletion and insertion errors were higher than those of the NAR models, indicating that the NAR models are more effective at generating a sequence with a correct length. From these results, it can be suggested that the key for further improvement of NAR models is to mitigate substitution errors.
4.5. Robustness against output sequence length
Figure 2 shows the correlation between output sequence length and error rate for the LS-100 test-clean set. Here, we observed that the performance of the AR model is prone to degradation when the length of an output sequence is long (Figure 2 top). On the other hand, the NAR models successfully recognized the long sequence without having such a severe quality drop. To further investigate the results, we compared the error components between the AR and selfconditioned CTC models (Figure 2 bottom). While the substitution and insertion errors were in the same range between the two models, the deletion error in the AR model got signiÔ¨Åcantly high as the output sequence length increased, where we observed consecutive tokens in the sequence are completely skipped as reported in [61]. This explains why the performance of the AR models on the TED2 development set fell behind those of the NAR models (Table 2), as the set includes long-form utterances up to 40 seconds.
4.6. Evaluation under noisy condition
Table 4 shows results under noisy conditions based on the CHiME4 task. Note that we only focused on the models achieving promis-

Error rate [%]

WER [%]

50 40 30 20 10 00

AR CTC
20

30 AR sub. err.
AR ins. err.
20 AR del. err.

Improved Mask-CTC Intermediate CTC

Self-conditioned CTC KERMIT

Outp4u0t sequence le6n0gth

80

100

Self-cond. CTC sub. err. Self-cond. CTC ins. err. Self-cond. CTC del. err.

10

0

0

20 Outp4u0t sequence le6n0gth

80

100

Fig. 2. Comparison of NAR and AR (CTC/attention) models evaluated on different output sequence length. WERs are compared among the models (top), and the error components are compared between AR and Self-conditioned CTC (bottom).

Model

Table 4. Word error rate on CHiME4.

dt05

et05

real simu real simu

NAR AR

CTC/attention + beam-search

14.0 16.0 22.2 24.1 13.9 15.7 22.4 23.7

CTC

16.5 18.1 25.9 27.0

Improved Mask-CTC 14.9 16.8 24.6 25.2

Intermediate CTC

15.2 16.8 24.2 25.2

Self-conditioned CTC 14.6 16.6 23.7 24.3

ing results on the main tasks. The evaluations were performed on the 1-channel track, where the development (dt05) and test (et05) sets included real and simulated (simu) utterances recorded from one of a single microphones on the tablet device [54]. Comparing the results obtained from the NAR models, all of our NAR models outperformed the standard CTC-based model. However, the results were not comparable with those obtained from the AR model. With the nonstationary noises included in the input speech, it is crucial for an ASR model to attend to dependency among output tokens for generating an accurate sequence. As observed in the LS-100 task in Section 4.2, the results suggest that the NAR models are likely to depend more on acoustic information, having difÔ¨Åculty capturing the token dependencies when the quality of an input speech is low.

4.7. Combination of different techniques
As intermediate CTC is a regularization method for CTC modeling and does not require any architectural changes, it is possible to augment other CTC-based NAR modeling with intermediate CTC. We combine Mask-CTC and intermediate CTC by adding an intermediate CTC loss to the encoder of Mask-CTC, as proposed in [35]. Table 5 shows the comparison of Mask-CTC and its intermediate CTC extension on LS-100 and CHiME-4. In all cases, intermediate CTC improved Mask-CTC, similar to experimental results reported by [35], while requiring no extra computation during inference.

Table 5. Word error rate (WER) on LS-100 and CHiME4 test sets for Mask-CTC and its extension with intermediate CTC.

Model

LS-100 clean other

CHiME4 real simu

Mask-CTC

7.5 20.6 24.9 25.8

+ Intermediate CTC 7.2 20.4 24.9 25.0

Table 6. BLEU (‚Üë) scores of speech translation models on FisherCallHome Spanish. #iter is the number of iterations.

Model

Fisher dev dev2 test

CallHome devtest evltest

CTC

51.0 51.6 50.8 18.0

18.7

Mask-CTC

51.1 51.7 50.6 17.9

18.3

Intermediate CTC

51.3 51.4 51.0 19.0

19.0

Self-conditioned CTC 50.7 51.2 50.5 19.1

19.2

Orthros (#iter = 4)

50.1 50.6 48.7 19.5

19.8

Orthros (#iter = 10) 51.6 52.4 50.5 20.5

20.7

4.8. Application to end-to-end speech translation (E2E-ST)
We applied Mask-CTC, intermediate CTC, self-conditioned CTC, and Orthros [62] to the NAR E2E-ST task on Fisher-CallHome Spanish corpus [63]. Orthros is based on CMLM but has an additional AR decoder on the same encoder to select the most probable translation among multiple length candidates. All models used the Conformer encoder and 16k wordpieces as output units. The encoder parameters were initialized with a pre-trained AR ASR encoder. We followed the setting in [62] and applied sequence-level knowledge distillation [64]. With the help of the NAR modeling methods, we observed some gains over the CTC-based results (Table 6). While self-conditioned CTC was the most effective method for ASR, it had smaller impact on the E2E-ST task, and Orthros resulted in the best performance. Since input-output alignments are not monotonic in this task, token-level iterative reÔ¨Ånement is required to make the sequence generation conditioned on a translated sequence, rather than only depending on acoustic information from input speech.
5. CONCLUSIONS
This paper presented a comprehensive study of NAR modeling methods for end-to-end ASR. Various NAR models were compared based on different ASR tasks. The results provided several interesting Ô¨Åndings, including the accuracy-speed trade-off and robustness against long-form speech utterances. We also showed that different NAR techniques could be combined to improve the performance and applied to end-to-end speech translation. We believe that the reproducible implementations and recipes used in this paper will accelerate further NAR research in speech processing.
6. ACKNOWLEDGEMENT
This work was partly supported by ASAPP. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) [65], which is supported by National Science Foundation grant number ACI-1548562. SpeciÔ¨Åcally, it used the Bridges system [66], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). The authors would like to thank Linhao Dong and Florian Boyer for helpful discussions.

7. REFERENCES
[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., ‚ÄúDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,‚Äù IEEE Signal Process. Mag., vol. 29, no. 6, 2012.
[2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, ‚ÄúSpeech recognition with deep recurrent neural networks,‚Äù in Proc. ICASSP, 2013, pp. 6645‚Äì6649.
[3] Alex Graves and Navdeep Jaitly, ‚ÄúTowards end-to-end speech recognition with recurrent neural networks,‚Äù in Proc. ICML, 2014, pp. 1764‚Äì1772.
[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, ‚ÄúAttention-based models for speech recognition,‚Äù in Proc. NeurIPS, 2015, pp. 577‚Äì 585.
[5] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, ‚ÄúListen, attend and spell: A neural network for large vocabulary conversational speech recognition,‚Äù in Proc. ICASSP, 2016, pp. 4960‚Äì4964.
[6] Alex Graves, ‚ÄúSequence transduction with recurrent neural networks,‚Äù arXiv preprint arXiv:1211.3711, 2012.
[7] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, ‚ÄúSequence to sequence learning with neural networks,‚Äù in Proc. NeurIPS, 2014, pp. 3104‚Äì3112.
[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, ‚ÄúNeural machine translation by jointly learning to align and translate,‚Äù in Proc. ICLR, 2014.
[9] Linhao Dong, Shuang Xu, and Bo Xu, ‚ÄúSpeech-Transformer: a no-recurrence sequence-to-sequence model for speech recognition,‚Äù in Proc. ICASSP, 2018, pp. 5884‚Äì5888.
[10] Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang, ‚ÄúQuartznet: Deep automatic speech recognition with 1d time-channel separable convolutions,‚Äù in Proc. ICASSP, 2020, pp. 6124‚Äì6128.
[11] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang, ‚ÄúConformer: Convolution-augmented Transformer for speech recognition,‚Äù in Proc. Interspeech, 2020, pp. 5036‚Äì5040.
[12] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al., ‚ÄúState-of-theart speech recognition with sequence-to-sequence models,‚Äù in Proc. ICASSP, 2018, pp. 4774‚Äì4778.
[13] Christoph Lu¬®scher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel, Albert Zeyer, Ralf Schlu¬®ter, and Hermann Ney, ‚ÄúRWTH ASR systems for LibriSpeech: Hybrid vs attention,‚Äù in Proc. Interspeech, 2019, pp. 231‚Äì235.
[14] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al., ‚ÄúA comparative study on Transformer vs RNN in speech applications,‚Äù in Proc. ASRU, 2019, pp. 449‚Äì456.
[15] Alex Graves, Santiago Ferna¬¥ndez, Faustino Gomez, and Ju¬®rgen Schmidhuber, ‚ÄúConnectionist temporal classiÔ¨Åcation: Labelling unsegmented sequence data with recurrent neural networks,‚Äù in Proc. ICML, 2006, pp. 369‚Äì376.
[16] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, ‚ÄúNon-autoregressive neural machine translation,‚Äù in Proc. ICLR, 2018.

[17] Nanxin Chen, Shinji Watanabe, Jesus Antonio Villalba, Piotr Zelasko, and Najim Dehak, ‚ÄúNon-autoregressive transformer for speech recognition,‚Äù IEEE Signal Process. Lett., 2020.
[18] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, ‚ÄúImputer: Sequence modelling via imputation and dynamic programming,‚Äù in Proc. ICML, 2020, pp. 1403‚Äì1413.
[19] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, ‚ÄúMask CTC: Nonautoregressive end-to-end ASR with CTC and mask predict,‚Äù in Proc. Interspeech, 2020, pp. 3655‚Äì3659.
[20] Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi Wen, and Shuai Zhang, ‚ÄúListen attentively, and spell once: Whole sentence generation via a non-autoregressive architecture for low-latency speech recognition,‚Äù in Proc. Interspeech, 2020, pp. 3381‚Äì3385.
[21] Yuya Fujita, Shinji Watanabe, Motoi Omachi, and Xuankai Chang, ‚ÄúInsertion-based modeling for end-to-end automatic speech recognition,‚Äù in Proc. Interspeech, 2020, pp. 3660‚Äì 3664.
[22] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, and Zhengqi Wen, ‚ÄúSpike-triggered non-autoregressive Transformer for end-to-end speech recognition,‚Äù in Proc. Interspeech, 2020, pp. 5026‚Äì5030.
[23] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, ‚ÄúAlignReÔ¨Åne: Non-autoregressive speech recognition via iterative realignment,‚Äù in Proc. NAACL-HLT, 2021, pp. 1920‚Äì1927.
[24] JindÀárich Libovicky¬¥ and JindÀárich Helcl, ‚ÄúEnd-to-end nonautoregressive neural machine translation with connectionist temporal classiÔ¨Åcation,‚Äù in Proc. EMNLP, 2018, pp. 3016‚Äì 3021.
[25] Jason Lee, Elman Mansimov, and Kyunghyun Cho, ‚ÄúDeterministic non-autoregressive neural sequence modeling by iterative reÔ¨Ånement,‚Äù in Proc. EMNLP, 2018, pp. 1173‚Äì1182.
[26] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit, ‚ÄúInsertion Transformer: Flexible sequence generation via insertion operations,‚Äù in Proc. ICML, 2019, pp. 5976‚Äì5985.
[27] Jiatao Gu, Changhan Wang, and Junbo Zhao, ‚ÄúLevenshtein Transformer,‚Äù in Proc. NeurIPS, 2019, pp. 11181‚Äì11191.
[28] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer, ‚ÄúMask-predict: Parallel decoding of conditional masked language models,‚Äù in Proc. EMNLP-IJCNLP, 2019, pp. 6114‚Äì6123.
[29] Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi, ‚ÄúNon-autoregressive machine translation with latent alignments,‚Äù in Proc. EMNLP, 2020, pp. 1098‚Äì1108.
[30] Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy, ‚ÄúFlowSeq: Non-autoregressive conditional sequence generation with generative Ô¨Çow,‚Äù in Proc. EMNLPIJCNLP, 2019, pp. 4273‚Äì4283.
[31] Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe, Tetsuji Ogawa, and Tetsunori Kobayashi, ‚ÄúImproved Mask-CTC for non-autoregressive end-to-end ASR,‚Äù in Proc. ICASSP, 2021, pp. 8363‚Äì8367.
[32] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, ‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding,‚Äù in Proc. NAACL-HLT, 2019, pp. 4171‚Äì4186.
[33] Nanxin Chen, Piotr ZÀô elasko, Laureano Moro-Vela¬¥zquez, Jesu¬¥s Villalba, and Najim Dehak, ‚ÄúAlign-Denoise: Single-pass nonautoregressive speech recognition,‚Äù in Proc. Interspeech, 2021.
[34] William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit, ‚ÄúKERMIT: Generative insertion-based modeling for sequences,‚Äù arXiv preprint arXiv:1906.01604, 2019.

[35] Jaesong Lee and Shinji Watanabe, ‚ÄúIntermediate loss regularization for CTC-based speech recognition,‚Äù in Proc. ICASSP, 2021, pp. 6224‚Äì6228.
[36] Jumon Nozaki and Tatsuya Komatsu, ‚ÄúRelaxing the conditional independence assumption of CTC-based ASR by conditioning on intermediate predictions,‚Äù in Proc. Interspeech, 2021.
[37] Linhao Dong and Bo Xu, ‚ÄúCIF: Continuous integrate-and-Ô¨Åre for end-to-end speech recognition,‚Äù in Proc. ICASSP, 2020, pp. 6079‚Äì6083.
[38] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai, ‚ÄúESPnet: End-to-end speech processing toolkit,‚Äù in Proc. Interspeech, 2018, pp. 2207‚Äì2211.
[39] Fan Yu, Haoneng Luo, Pengcheng Guo, Yuhao Liang, Zhuoyuan Yao, Lei Xie, Yingying Gao, Leijing Hou, and Shilei Zhang, ‚ÄúBoundary and context aware training for CIFbased non-autoregressive end-to-end ASR,‚Äù arXiv preprint arXiv:2104.04702, 2021.
[40] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates, Yashesh Gaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop Sriram, and Zhenyao Zhu, ‚ÄúExploring neural transducers for end-to-end speech recognition,‚Äù in Proc. ASRU, 2017, pp. 206‚Äì 213.
[41] Andros Tjandra, Chunxi Liu, Frank Zhang, Xiaohui Zhang, Yongqiang Wang, Gabriel Synnaeve, Satoshi Nakamura, and Geoffrey Zweig, ‚ÄúDeja-vu: Double feature presentation and iterated loss in deep transformer networks,‚Äù in Proc. ICASSP, 2020, pp. 6899‚Äì6903.
[42] Edwin G Ng, Chung-Cheng Chiu, Yu Zhang, and William Chan, ‚ÄúPushing the limits of non-autoregressive speech recognition,‚Äù in Proc. Interspeech, 2021.
[43] Somshubra Majumdar, Jagadeesh Balam, Oleksii Hrinchuk, Vitaly Lavrukhin, Vahid Noroozi, and Boris Ginsburg, ‚ÄúCitrinet: Closing the gap between non-autoregressive and autoregressive end-to-end models for automatic speech recognition,‚Äù arXiv preprint arXiv:2104.01721, 2021.
[44] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, ‚Äúwav2vec 2.0: A framework for self-supervised learning of speech representations,‚Äù in Proc. NeurIPS, 2020.
[45] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and PierreAntoine Manzagol, ‚ÄúExtracting and composing robust features with denoising autoencoders,‚Äù in Proc. ICML, 2008, pp. 1096‚Äì 1103.
[46] Shinji Watanabe, Florian Boyer, Xuankai Chang, Pengcheng Guo, Tomoki Hayashi, Yosuke Higuchi, Takaaki Hori, WenChin Huang, Hirofumi Inaguma, Naoyuki Kamo, et al., ‚ÄúThe 2020 ESPNet update: New features, broadened applications, performance improvements, and future plans,‚Äù in 2021 IEEE Data Science and Learning Workshop (DSLW), 2021, pp. 1‚Äì6.
[47] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, ‚ÄúHybrid CTC/attention architecture for end-to-end speech recognition,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240‚Äì1253, 2017.
[48] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, and Tomohiro Nakatani, ‚ÄúImproving Transformer-based end-to-end speech recognition with connectionist temporal classiÔ¨Åcation and language model integration,‚Äù in Proc. Interspeech, 2019, pp. 1408‚Äì1412.
[49] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, et al., ‚ÄúRecent developments on ESPnet toolkit boosted by Conformer,‚Äù in Proc. ICASSP, 2021, pp. 5874‚Äì5878.

[50] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, ‚ÄúLibrispeech: An ASR corpus based on public domain audio books,‚Äù in Proc. ICASSP, 2015, pp. 5206‚Äì5210.
[51] Anthony Rousseau et al., ‚ÄúEnhancing the TED-LIUM corpus with selected data for language modeling and more TED talks,‚Äù in Porc. LREC, 2014, pp. 3935‚Äì3939.
[52] Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara, ‚ÄúSpontaneous speech corpus of Japanese.,‚Äù in Proc. LREC, 2000, pp. 1‚Äì5.
[53] Taku Kudo, ‚ÄúSubword regularization: Improving neural network translation models with multiple subword candidates,‚Äù in Proc. ACL, 2018.
[54] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer, ‚ÄúAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,‚Äù Computer Speech & Language, vol. 46, pp. 535‚Äì 557, 2017.
[55] Douglas B Paul and Janet M Baker, ‚ÄúThe design for the wall street journal-based CSR corpus,‚Äù in Proc. Workshop on Speech and Natural Language, 1992, pp. 357‚Äì362.
[56] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., ‚ÄúThe Kaldi speech recognition toolkit,‚Äù in Proc. ASRU, 2011.
[57] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, ‚ÄúAudio augmentation for speech recognition,‚Äù in Proc. Interspeech, 2015, pp. 3586‚Äì3589.
[58] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le, ‚ÄúSpecAugment: A simple data augmentation method for automatic speech recognition,‚Äù in Proc. Interspeech, 2019, pp. 2613‚Äì2617.
[59] Diederik P Kingma and Jimmy Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù in Proc. ICLR, 2015.
[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, ‚ÄúAttention is all you need,‚Äù in Proc. NeurIPS, 2017, pp. 5998‚Äì6008.
[61] Jan Chorowski and Navdeep Jaitly, ‚ÄúTowards better decoding and language model integration in sequence to sequence models,‚Äù in Proc. Interspeech, 2017, pp. 523‚Äì527.
[62] Hirofumi Inaguma, Yosuke Higuchi, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe, ‚ÄúOrthros: Non-autoregressive end-to-end speech translation with dual-decoder,‚Äù in Proc. ICASSP, 2021, pp. 7503‚Äì7507.
[63] Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur, ‚ÄúImproved speech-to-text translation with the Fisher and Callhome Spanish‚ÄìEnglish speech translation corpus,‚Äù in Proc. IWSLT, 2013.
[64] Yoon Kim and Alexander M. Rush, ‚ÄúSequence-level knowledge distillation,‚Äù in Proc. EMNLP, 2016, pp. 1317‚Äì1327.
[65] John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D Peterson, et al., ‚ÄúXSEDE: Accelerating scientiÔ¨Åc discovery,‚Äù Computing in Science & Engineering, vol. 16, no. 5, pp. 62‚Äì74, 2014.
[66] Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, and J Ray Scott, ‚ÄúBridges: a uniquely Ô¨Çexible HPC resource for new communities and data analytics,‚Äù in Proc. the 2015 XSEDE Conference: ScientiÔ¨Åc Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1‚Äì8.

