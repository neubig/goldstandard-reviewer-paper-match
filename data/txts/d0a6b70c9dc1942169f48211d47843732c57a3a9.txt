arXiv:2003.00443v5 [cs.AI] 21 Jul 2020

Environment-agnostic Multitask Learning for Natural Language Grounded Navigation
Xin Eric Wang1,2 , Vihan Jain3 , Eugene Ie3, William Yang Wang2, Zornitsa Kozareva3, and Sujith Ravi3,4
1 University of California, Santa Cruz 2 University of California, Santa Barbara
3 Google Research 4 Amazon
Abstract. Recent research eﬀorts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overﬁt training data in seen environments and fail to generalize well in previously unseen environments. To close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which beneﬁts from richer natural language guidance and eﬀectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that environment-agnostic multitask learning signiﬁcantly reduces the performance gap between seen and unseen environments, and the navigation agent trained so outperforms baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH. Our submission to the CVDN leaderboard establishes a new state-of-the-art for the NDH task on the holdout test set. Code is available at https: //github.com/google-research/valan.
Keywords: Vision-and-Language Navigation, Natural Language Grounding, Multitask Learning, Agnostic Learning
1 Introduction
Navigation in visual environments by following natural language guidance [19] is a fundamental capability of intelligent robots that simulate human behaviors, because humans can easily reason about the language guidance and navigate eﬃciently by interacting with the visual environments. Recent eﬀorts [3,9,44,38]
Equal contribution. Work done at Google.

2

X. Wang et al.

empower large-scale learning of natural language grounded navigation that is situated in photo-realistic simulation environments.
Nevertheless, the generalization problem commonly exists for these tasks, especially indoor navigation: the agent usually performs poorly on unknown environments that have never been seen during training. One of the leading causes of such behavior is data scarcity, as it is expensive and time-consuming to extend either visual environments or natural language guidance. The number of scanned houses for indoor navigation is limited due to high expense and privacy concerns. Besides, unlike vision-only navigation tasks [33,34,52,32,24,26] where episodes can be exhaustively sampled in simulation, natural language grounded navigation is supported by human demonstrated interaction in natural language. It is impractical to fully collect all the samples for individual tasks.
Therefore, it is essential though challenging to eﬃciently learn a more generalized policy for natural language grounded navigation tasks from existing data [50,51]. In this paper, we study how to resolve the generalization and data scarcity issues from two diﬀerent angles. First, previous methods are trained for one task at the time, so each new task requires training a new agent instance from scratch that can only solve the one task on which it was trained. In this work, we propose a generalized multitask model for natural language grounded navigation tasks such as Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH), aiming to eﬃciently transfer knowledge across tasks and eﬀectively solve all the tasks simultaneously with one agent.
Furthermore, even though there are thousands of trajectories paired with language guidance, the underlying house scans are restricted. For instance, the popular Matterport3D environment [6] contains only 61 unique house scans in the training set. The current models perform much better in seen environments by taking advantage of the knowledge of speciﬁc houses they have acquired over multiple task completions during training, but fail to generalize to houses not seen during training. To overcome this shortcoming, we propose an environmentagnostic learning method to learn a visual representation that is invariant to speciﬁc environments but can still support navigation. Endowed with the learned environment-agnostic representations, the agent is further prevented from the overﬁtting issue and generalizes better on unseen environments.
To the best of our knowledge, we are the ﬁrst to introduce natural language grounded multitask and environment-agnostic training regimes and validate their eﬀectiveness on VLN and NDH tasks. Extensive experiments demonstrate that our environment-agnostic multitask navigation model can not only eﬃciently execute diﬀerent language guidance in indoor environments but also outperform the single-task baseline models by a large margin on both tasks. Besides, the performance gap between seen and unseen environments is signiﬁcantly reduced. Furthermore, our leaderboard submission for the NDH task establishes a new state-of-the-art outperforming the existing best agent by more than 66% on the primary metric of goal progress on the holdout test set.

Environment-agnostic Multitask Navigation

3

2 Background
Vision-and-Language Navigation. As depicted in Figure 1, Vision-andLanguage Navigation [3,7] task requires an embodied agent to navigate in photorealistic environments to carry out natural language instructions. For a given path, the associated natural language instructions describe the step-by-step guidance from the starting position to the target position. The agent is spawned at an initial pose p0 = (v0, φ0, θ0), which includes the spatial location, heading and elevation angles. Given a natural language instruction X = {x1, x2, ..., xn}, the agent is expected to perform a sequence of actions {a1, a2, ..., aT } and arrive at the target position vtar speciﬁed by the language instruction X. In this work, we consider the VLN task deﬁned for Room-to-Room (R2R) [3] dataset, which contains instruction-trajectory pairs across 90 diﬀerent indoor environments (houses). The instructions for a given trajectory in the dataset on an average contain 29 words. Previous VLN methods have studied various aspects to improve the navigation performance, such as planning [48], data augmentation [14,42,15], cross-modal alignment [47,21], progress estimation [30], error correction [31,23], interactive language assistance [36,35] etc. This work tackles VLN via multitask learning and environment-agnostic learning, which is orthogonal to all these prior arts. Navigation from Dialog History. Diﬀerent from Visual Dialog [10] that involves dialog grounded in a single image, the recently introduced Cooperative Vision-and-Dialog Navigation (CVDN) dataset [44] includes interactive language assistance for indoor navigation, which consists of over 2,000 embodied, human-human dialogs situated in photo-realistic home environments. The task of Navigation from Dialog History (NDH) demonstrated in Figure 1, is deﬁned as: given a target object t0 and a dialog history between humans cooperating to perform the task, the embodied agent must infer navigation actions towards the goal room that contains the target object. The dialog history is denoted as < t0, Q1, A1, Q2, A2, ..., Qi, Ai >, including the target object t0, the questions Q and answers A till the turn i (0 ≤ i ≤ k, where k is the total number of Q-A turns from the beginning to the goal room). The agent, located in p0, is trying to move closer to the goal room by inferring from the dialog history that happened before. The dialog for a given trajectory lasts 6 utterances (3 question-answer exchanges) and is 82 words long on an average. Multitask Learning. The basis of multitask learning is the notion that tasks can serve as mutual sources of inductive bias for each other [5]. When multiple tasks are trained jointly, multitask learning causes the learner to prefer the hypothesis that explains all the tasks simultaneously, leading to more generalized solutions. Multitask learning has been successful in natural language processing [8], speech recognition [11], computer vision [17], drug discovery [39], and Atari games [43]. The deep reinforcement learning methods that have become very popular for training models on natural language grounded navigation tasks [47,20,21,42] are known to be data ineﬃcient. In this work, we introduce multitask reinforcement learning for such tasks to improve data eﬃciency by positive transfer across related tasks.

4

X. Wang et al.

NDH
I can't find an exit from this library. N Should I turn left or right?

Turn right and start walking around

the library.

O

VLN
Walk towards the bookshelf to your right, and walk all the way around it...Taking two lefts. Stop in front of the chair thats next to
the window.

...

Fig. 1: While the NDH task (left) requires an agent to navigate using dialog history between two human players - a navigator (N) who is trying to ﬁnd the goal room with the help of an oracle (O), the VLN task (right) requires navigating using instructions written by human annotators.

Agnostic Learning. A few studies on agnostic learning have been proposed recently. For example, Model-Agnostic Meta-Learning (MAML) [13] aims to train a model on a variety of learning tasks and solve a new task using only a few training examples. Liu et al. [29] proposes a uniﬁed feature disentangler that learns domain-invariant representation across multiple domains for image translation. Other domain-agnostic techniques are also proposed for supervised [28] and unsupervised domain adaption [40,37]. In this work, we pair the environment classiﬁer with a gradient reversal layer [16] to learn an environment-agnostic representation that can be better generalized on unseen environments in a zero-shot fashion where no adaptation is involved. Distributed Actor-Learner Navigation Learning Framework. To train models for the various language grounded navigation tasks like VLN and NDH, we use the VALAN framework [25], a distributed actor-learner learning infrastructure. The framework is inspired by IMPALA [12] and uses its oﬀ-policy correction method called V-trace to scale reinforcement learning methods to thousands of machines eﬃciently. The framework additionally supports a variety of supervision strategies essential for navigation tasks such as teacher-forcing [3], student-forcing [3] and mixed supervision [44]. The framework is built using TensorFlow [1] and supports ML accelerators (GPU, TPU).
3 Environment-agnostic Multitask Learning
3.1 Overview
Our environment-agnostic multitask navigation model is illustrated in Figure 2. First, we adapt the reinforced cross-modal matching (RCM) model [47] and make it seamlessly transfer across tasks by sharing all the learnable parameters for both NDH and VLN, including joint word embedding layer, language encoder, trajectory encoder, cross-modal attention module (CM-ATT), and action

NDH Dialog or
VLN Instruction
Word Embedding
Language Encoder

Environment-agnostic Multitask Navigation

5

Multitask Data Sampling

Paired Demo Path

CM-ATT Action Predictor

Gradient Reversal

Trajectory Encoder Environment Classifier

Fig. 2: Overview of environment-agnostic multitask learning.
predictor. Furthermore, to learn the environment-agnostic representation zt, we equip the navigation model with an environment classiﬁer whose objective is to predict which house the agent is. However, note that between trajectory encoder and environment classiﬁer, a gradient reversal layer [16] is introduced to reverse the gradients back-propagated to the trajectory encoder, making it learn representations that are environment-agnostic and thus more generalizable in unseen environments. During training, the environment classiﬁer is minimizing the environment classiﬁcation loss Lenv, while the trajectory encoder is maximizing Lenv and minimizing the navigation loss Lnav. The other modules are optimized with the navigation loss Lnav simultaneously. Below we introduce multitask reinforcement learning and environment-agnostic representation learning. A more detailed model architecture is presented in Sec. 4.
3.2 Multitask Reinforcement Learning
In this section, we describe how we adapted the RCM agent model to learn the two tasks of VLN and NDH simultaneously. It is worth noting that even though both the VLN and NDH tasks use the same Matterport3D indoor environments [6], there are signiﬁcant diﬀerences in the motivations and the overall objectives of the two tasks. While the natural language descriptions associated with the paths in the VLN task are step-by-step instructions to follow the ground-truth paths, the descriptions of the paths in the NDH task are series of question-answer interactions (dialog) between two human players which need not necessarily align sequentially with the ground-truth paths. This diﬀerence in the style of the two tasks also manifests in their respective datasets — the average path description length and average path length in the NDH task’s dataset are roughly three times that of the VLN task’s dataset. Furthermore, while the objective in VLN is to ﬁnd the exact goal node in the environment (i.e., point navigation), the objective in NDH is to ﬁnd the goal room that contains the speciﬁed object (i.e., room navigation).

6

X. Wang et al.

Interleaved Multitask Data Sampling. To avoid overﬁtting individual tasks, we adopt an interleaved multitask data sampling strategy to train the model. Particularly, each data sample within a mini-batch can be from either task, so that the VLN instruction-trajectory pairs and NDH dialog-trajectory pairs are interleaved in a mini-batch though they may have diﬀerent learning objectives.

Reward Shaping. Following prior art [48,47], we ﬁrst implement a discounted cumulative reward function R for the VLN and NDH tasks:

T

R(st, at) = γt −tr(st , at )

(1)

t =t

where γ is the discounted factor. For the VLN task, we choose the immediate reward function such that the agent is rewarded at each step for getting closer to (or penalized for getting further from) the target location. At the end of the episode, the agent receives a reward only if it terminated successfully. Formally,

d(st , vtar) − d(st +1, vtar) if t < T

r(st , at ) = 1[d(sT , vtar) ≤ dth]

if t = T

(2)

where d(st, vtar) is the distance between state st and the target location vtar,
1[.] is the indicator function and dth is the maximum distance from vtar that
the agent is allowed to terminate for success.
Diﬀerent from VLN, the NDH task is essentially room navigation instead of
point navigation because the agent is expected to reach a room that contains the target object. Suppose the goal room is occupied by a set of nodes {vi}N1 , we replace the distance function d(st, vtar) in Equation 2 with the minimum distance to the goal room droom(st, {vi}N1 ) for NDH:

droom(st, {vi}N1 ) = min d(st, vi)

(3)

1≤i≤N

Navigation Loss. Since human demonstrations are available for both VLN and NDH tasks, we use behavior cloning to constrain the learning algorithm to model state-action spaces that are most relevant to each task. Following previous works [47], we also use reinforcement learning to aid the agent’s ability to recover from erroneous actions in unseen environments. During navigation model training, we adopt a mixed training strategy of reinforcement learning and behavior cloning, so the navigation loss function is:

Lnav = −Eat∼π[R(st, at) − b] − E[log π(a∗t |st)]

(4)

where we use REINFORCE policy gradients [49] and supervised learning gradi-
ents to update the policy π. b is the estimated baseline to reduce the variance and a∗t is the human demonstrated action.

Environment-agnostic Multitask Navigation

7

3.3 Environment-agnostic Representation Learning

To further improve the navigation policy’s generalizability, we propose to learn a latent environment-agnostic representation that is invariant among seen environments. The objective is to not learn the intricate environment-speciﬁc features that are irrelevant to general navigation (e.g. unique house appearances), preventing the model from overﬁtting to speciﬁc seen environments. We can reformulate the navigation policy as

π(at|st) = p(at|zt, st)p(zt|st)

(5)

where zt is a latent representation.

As shown in Figure 2, p(at|zt, st) is modeled by the policy module (includ-

ing CM-ATT and action predictor) and p(zt|st) is modeled by the trajectory

encoder. In order to learn the environment-agnostic representation, we employ

an environment classiﬁer and a gradient reversal layer [16]. The environment

classiﬁer is parameterized to predict the house identity, so its loss function Lenv

is deﬁned as

Lenv = −E[log p(y = y∗|zt)]

(6)

where y∗ is the ground-truth house label. The gradient reversal layer has no parameters. It acts as an identity transform during forward-propagation, but multiplies the gradient by −λ and passes it to the trajectory encoder during backpropagation. Therefore, in addition to minimizing the navigation loss Lnav, the trajectory encoder is also maximizing the environment classiﬁcation loss Lenv. While the environment classiﬁer is minimizing the classiﬁcation loss conditioned on the latent representation zt, the trajectory encoder is trying to increase the classiﬁer’s entropy, resulting in an adversarial learning objective.

4 Model Architecture

Language Encoder. The natural language guidance (instruction or dialog) is tokenized and embedded into n-dimensional space X = {x1, x2, ..., xn} where the word vectors xi are initialized randomly. The vocabulary is restricted to tokens that occur at least ﬁve times in the training instructions (the vocabulary used when jointly training VLN and NDH tasks is the union of the two tasks’ vocabularies.). All out-of-vocabulary tokens are mapped to a single outof-vocabulary identiﬁer. The token sequence is encoded using a bi-directional LSTM [41] to create HX following:

HX = [hX 1 ; hX 2 ; ...; hX n ], hX t = σ(→−h X t , ←h−X t ) (7) →−h X t = LST M (xt, →−h X t−1), ←h−X t = LST M (xt, ←h−X t+1) (8)

where →−h X and ←h−X are the hidden states of the forward and backward LSTM

t

t

→−

layers

at

time

step

t

respectively,

and

the

σ

function

is

used

to

combine

h

X t

and

←h−X into hX .

t

t

8

X. Wang et al.

Trajectory Encoder. Similar to benchmark models [14,47,21], at each time step t, the agent perceives a 360-degree panoramic view at its current location. The view is discretized into k view angles (k = 36 in our implementation, 3 elevations by 12 headings at 30-degree intervals). The image at view angle i, heading angle φ and elevation angle θ is represented by a concatenation of the pre-trained CNN image features with the 4-dimensional orientation feature [sin φ; cos φ; sin θ; cos θ] to form vt,i. The visual input sequence V = {v1, v2, ..., vm} is encoded using a LSTM to create HV following:

HV = [hV1 ; hV2 ; ...; hVm], where hVt = LST M (vt, hVt−1)

(9)

vt = Attention(hVt−1, vt,1..k) is the attention-pooled representation of all view angles using previous agent state ht−1 as the query. We use the dot-product attention [45] hereafter. Policy Module. The policy module comprises of cross-modal attention (CMATT) unit as well as an action predictor. The agent learns a policy πθ over parameters θ that maps the natural language instruction X and the initial visual scene v1 to a sequence of actions [a1, a2, ..., an]. The action space which is common to VLN and NDH tasks consists of navigable directions from the current location. The available actions at time t are denoted as ut,1..l, where ut,j is the representation of the navigable direction j from the current location obtained similarly to vt,i. The number of available actions, l, varies per location, since graph node connectivity varies. Following Wang et al. [47], the model predicts the probability pd of each navigable direction d using a bilinear dot product:

pd = softmax([hVt ; cttext; cvt isual]Wc(ut,dWu)T )

(10)

where

cttext

=

Attention(hVt , hX 1..n)

and

cvt isual

=

Attention(

c

text t

,

vt,1..k

).

Wc

and Wu are learnable parameters.

Environment Classiﬁer. The environment classiﬁer is a two-layer perceptron

with a SoftMax layer as the last layer. Given the latent representation zt (which is hVt in our setting), the classiﬁer generates a probability distribution over the

house labels.

5 Experiments
5.1 Experimental Setup
Implementation Details. We use a 2-layer bi-directional LSTM for the instruction encoder, where the size of LSTM cells is 256 in each direction. The inputs to the encoder are 300-dimensional embeddings initialized randomly. For the visual encoder, we use a 2-layer LSTM with a cell size of 512. The encoder inputs are image features derived as mentioned in Sec. 4. The cross-modal attention layer size is 128 units. The environment classiﬁer has one hidden layer of size 128 units, followed by an output layer of size equal to the number of classes. The negative gradient multiplier λ in the gradient reversal layer is empirically tuned

Environment-agnostic Multitask Navigation

9

and ﬁxed at a value of 1.3 for all experiments. During training, some episodes in the batch are identical to available human demonstrations in the training dataset, where the objective is to increase the agent’s likelihood of choosing human actions (behavioral cloning [4]). The rest of the episodes are constructed by sampling from the agent’s own policy. For the NDH task, we deploy mixed supervision similar to Thomason et al. [44], where the navigator’s or oracle’s path is selected as ground-truth depending on if the navigator was successful in reaching the correct end node following the question-answer exchange with the oracle or not. In the experiments, unless otherwise stated, we use the entire dialog history from the NDH task for model training. All the reported results in subsequent studies are averages of at least three independent runs. Evaluation Metrics. The agents are evaluated on two datasets, namely Validation Seen that contains new paths from the training environments and Validation Unseen that contains paths from previously unseen environments. The evaluation metrics for VLN task are as follows: Path Length (PL) measures the total length of the predicted path; Navigation Error (NE) measures the distance between the last nodes in the predicted and the reference paths; Success Rate (SR) measures how often the last node in the predicted path is within some threshold distance of the last node in the reference path; Success weighted by Path Length (SPL) [2] measures Success Rate weighted by the normalized Path Length; and Coverage weighted by Length Score (CLS) [22] measures predicted path’s conformity to the reference path weighted by length score. For the NDH task, the agent’s progress is deﬁned as a reduction (in meters) from the distance to the goal region at the agent’s ﬁrst position versus at its last position [44].

5.2 Environment-agnostic Multitask Learning
Table 1 shows the results of training the navigation model using environmentagnostic learning (EnvAg) as well as multitask learning (MT-RCM ). First, both learning methods independently help the agent learn more generalized navigation policy, as is evidenced by a signiﬁcant reduction in agent’s performance gap between seen and unseen environments (better visualized with Figure 3). For instance, the performance gap in goal progress on the NDH task drops from 3.85m to 0.92m using multitask learning, and the performance gap in success rate on the VLN task drops from 9.26% to 8.39% using environment-agnostic learning. Second, the two techniques are complementary—the agent’s performance when trained with both the techniques simultaneously improves on unseen environments compared to when trained separately. Finally, we note here that MT-RCM + EnvAg outperforms the baseline goal progress of 2.10m [44] on NDH validation unseen dataset by more than 120%. At the same time, it outperforms the equivalent RCM baseline [47] of 40.6% success rate by more than 16% (relative measure) on VLN validation unseen dataset.
To further validate our results on NDH task, we evaluated the MT-RCM + EnvAg agent on the test set of NDH dataset which is held out as the CVDN
5 The equivalent RCM model without intrinsic reward is used as the benchmark.

10

X. Wang et al.

Table 1: The agent’s performance under diﬀerent training strategies. The singletask RCM (ST-RCM) model is independently trained and tested on VLN or NDH tasks. The standard deviation across 3 independent runs is reported.

Fold Model

NDH Progress ↑ PL

NE ↓

VLN SR ↑

SPL ↑

CLS ↑

seq2seq [44]

5.92

RCM [47]5

12.08

3.25

67.60

-

-

Val Seen

Ours ST-RCM ST-RCM + EnvAg MT-RCM MT-RCM + EnvAg

6.49 ±0.95 6.07 ±0.56 5.28 ±0.56 5.07 ±0.45

10.75 11.31 10.63 11.60

±0.26 ±0.26 ±0.10 ±0.30

5.09 ±0.49 4.93 ±0.49 5.09 ±0.05 4.83 ±0.12

52.39 ±3.58 52.79 ±3.72 56.42 ±1.21 53.30 ±0.71

48.86 ±3.66 48.85 ±3.71 49.67 ±1.07 49.39 ±0.74

63.91 ±2.41 63.26 ±2.31 68.28 ±0.16 64.10 ±0.16

seq2seq [44]

2.10

RCM [47]

15.00

6.02

40.60

-

-

Val Unseen

Ours ST-RCM ST-RCM + EnvAg MT-RCM MT-RCM + EnvAg

2.64 ±0.06 3.15 ±0.29 4.36 ±0.17 4.65 ±0.20

10.60 11.36 10.23 12.05

±0.27 ±0.27 ±0.14 ±0.23

6.10 ±0.06 5.79 ±0.06 5.31 ±0.18 5.41 ±0.20

42.93 ±0.21 44.40 ±2.14 46.20 ±0.55 47.22 ±1.00

38.88 ±0.20 40.30 ±2.12 44.19 ±0.64 41.80 ±1.11

54.86 ±0.92 55.77 ±1.31 54.99 ±0.87 56.22 ±0.87

Fig. 3: Visualizing performance gap between seen and unseen environments for VLN (success rate) and NDH (progress) tasks.
challenge6. Table 2 shows that our submission to the leaderboard with MTRCM + EnvAg establishes a new state-of-the-art on this task outperforming the existing best agent by more than 66%.
5.3 Multitask Learning
We then conduct studies to examine cross-task transfer using multitask learning alone. First, we experiment multitasking learning with access to diﬀerent parts of the dialog—the target object to, the last oracle answer Ai, the prefacing navigator question Qi, and the full dialog history. Table 3 shows the results of 6 https://evalai.cloudcv.org/web/challenges/challenge-page/463/
leaderboard/1292

Environment-agnostic Multitask Navigation

11

Table 2: Comparison on CVDN Leaderboard Test Set. Note that the metric Progress is the same as dist to end reduction.

Agent

Progress ↑

Baselines

Random

0.83

Shortest Path Agent (upper bound )

9.76

Leaderboard Submissions

Seq2Seq [44] MT-RCM + EnvAg

2.35 3.91

jointly training MT-RCM model on VLN and NDH tasks. (1) Does VLN complement NDH? Yes, consistently. On NDH Val Unseen, MT-RCM consistently beneﬁts from following shorter paths with step-by-step instructions in VLN for all kinds of dialog inputs. It shows that VLN can serve as an essential task to boost learning of primitive action-and-instruction following and therefore support more complicated navigation tasks like NDH. (2) Does NDH complement VLN? Yes, under certain conditions. From the results on VLN Val Unseen, we can observe that MT-RCM with only target objects as the guidance performs equivalently or slightly worse than VLN-RCM, showing that extending visual paths alone (even with ﬁnal targets) is not helpful in VLN. But we can see a consistent and gradual increase in the success rate of MT-RCM on the VLN task as it is trained on paths with richer dialog history from the NDH task. This shows that the agent beneﬁts from more ﬁne-grained information about the path implying the importance given by the agent to the language instructions in the task. (3) Multitask learning improves the generalizability of navigation models: the seen-unseen performance gap is narrowed. (4) As a side eﬀect, results of diﬀerent dialog inputs on NDH Val Seen versus Unseen verify the essence of language guidance in generalizing navigation to unseen environments.

R2R

NDH + R2R

NDH

NDH + R2R

# occurrences in training dataset

t bigt htstand (furtherroomsthese stoolphoto if same book goal faucet towel our chestlook box find still pot let ) itsbegin looks sayspillow don too ni

ninignetslconwyaybinevt ision mehdandfoyeproracrhblentingpsossitiettinrgingpillrarirdowrateor wer climsbtonerciseevactoturrelookidinagchinluemnain tree estungsesirnygwasytoolsse statathewr inhetubharpttedveerlightelveost der sidewersbedsffee ird

dci ab ba a ca tele fra

m pai op ente co sh

exe el pi sl m co m clos lo paent baermo le bat s po

sh fo un be flo co th

th

Fig. 4: Selected tokens from the vocabulary for VLN (left) and NDH (right) tasks which gained more than 40 additional occurrences in the training dataset due to joint-training.

Besides, we show multitask learning results in better language grounding through more appearance of individual words in Figure 4 and shared semantic encoding of the whole sentences in Table 4. Figure 4 illustrates that underrepresented tokens in each of the individual tasks get a signiﬁcant boost in the

12

X. Wang et al.

Table 3: Comparison of agent performance when trained separately vs. jointly on VLN and NDH tasks.

NDH Evaluation

VLN Evaluation

Fold Model

Inputs for NDH Progress PL NE SR SPL CLS

to Ai Qi A1:i−1; Q1:i−1

↑

↓↑ ↑ ↑



NDH-RCM    

Val

 



Seen VLN-RCM

6.97 6.92 6.47 6.49

10.75 5.09 52.39 48.86 63.91



MT-RCM    

 



3.00 11.73 4.87 54.56 52.00 65.64 5.92 11.12 4.62 54.89 52.62 66.05 5.43 10.94 4.59 54.23 52.06 66.93 5.28 10.63 5.09 56.42 49.67 68.28



NDH-RCM    

Val

 



Unseen VLN-RCM

1.25 2.69 2.69 2.64
10.60 6.10 42.93 38.88 54.86



MT-RCM    

 



1.69 4.01 3.75 4.36

13.12 5.84 42.75 38.71 53.09 11.06 5.88 42.98 40.62 54.30 11.08 5.70 44.50 39.67 54.95 10.23 5.31 46.20 44.19 54.99

Table 4: Comparison of agent performance when language instructions are encoded by separate vs. shared encoder for VLN and NDH tasks.

Val Seen

Val Unseen

Language Encoder NDH

VLN

NDH

VLN

Progress ↑ PL NE ↓ SR ↑ SPL ↑ CLS ↑ Progress ↑ PL NE ↓ SR ↑ SPL ↑ CLS ↑

Shared Separate

5.28 5.17

10.63 5.09 56.42 49.67 68.28 11.26 5.02 52.38 48.80 64.19

4.36 4.07

10.23 5.31 46.20 44.19 54.99 11.72 6.04 43.64 39.49 54.57

number of training samples. Table 4 shows that the model with shared language encoder for NDH and VLN tasks outperforms the model that has separate language encoders for the two tasks, hence demonstrating the importance of parameter sharing during multitask learning.
Furthermore, we observed that the agent’s performance improves signiﬁcantly when trained on a mixture of VLN and NDH paths even when the size of the training dataset is ﬁxed, advancing the argument that multitask learning on NDH and VLN tasks complements the agent’s learning. More details of the ablation studies can be found in the Appendix.

Environment-agnostic Multitask Navigation

13

(a)

(b)

(a’)

(b’)

Fig. 5: t-SNE visualization of trajectory encoder’s output for VLN task across 11 diﬀerent color-coded seen (a,b) and unseen (a’,b’ ) environments. The depicted representations in (a) and (a’) are learned with environment-aware objective while those in (b) and (b’) are learned with environment-agnostic objective.

Table 5: Environment-agnostic versus environment-aware learning.

(a) Comparison on NDH.

(b) Comparison on VLN.

Model

Val Seen Val Unseen Progress ↑ Progress ↑

RCM EnvAware EnvAg

6.49 8.38 6.07

2.64 1.81 3.15

Model

Val Seen

Val Unseen

PL NE ↓ SR ↑ SPL ↑ CLS ↑ PL NE ↓ SR ↑ SPL ↑ CLS ↑

RCM 10.75 5.09 52.39 48.86 63.91 10.60 6.10 42.93 38.88 54.86 EnvAware 10.30 4.36 57.59 54.05 68.49 10.13 6.30 38.83 35.65 54.79 EnvAg 11.31 4.93 52.79 48.85 63.26 11.36 5.79 44.40 40.30 55.77

5.4 Environment-agnostic Learning
From Table 1, it can be seen that both VLN and NDH tasks beneﬁt from environment-agnostic learning independently. To further examine the generalization property of environment-agnostic learning, we train a model with the opposite objective—learn to correctly predict the navigation environments by removing the gradient reversal layer (environment-aware learning). The results in Table 5 demonstrate that environment-aware learning leads to overﬁtting on the training dataset as the performance on environments seen during training consistently increases for both the tasks. In contrast, environment-agnostic learning leads to a more generalized navigation policy that performs better on unseen environments. Figure 5 further shows that due to environment-aware learning, the model learns to represent visual inputs from the same environment closer to each other while the representations of diﬀerent environments are farther from each other resulting in a clustering learning eﬀect. On the other hand, environment-agnostic learning leads to more general representation across diﬀerent environments, which results in better performance on unseen environments.
5.5 Reward Shaping for NDH task
As discussed in Sec. 3.2, we conducted studies to shape the reward for the NDH task. Table 6 presents the results of training the agent with access to diﬀerent

14

X. Wang et al.

Table 6: Average agent progress towards goal room when trained using diﬀerent rewards and mixed supervision strategy.

Model

Inputs

Goal Progress (m)

t0 Ai Qi A1:i−1; Q1:i−1 Val Seen Val Unseen

Shortest-Path Agent Random Agent

9.52

9.58

0.42

1.09

Baselines Seq2Seq [44]





 

 



5.71

1.29

6.04

2.05

6.16

1.83

5.92

2.10



NDH-RCM



(distance to goal location)   

Ours

 





NDH-RCM



(distance to goal room)   

 



4.18 4.96 4.60 5.02
6.97 6.92 6.47 6.49

0.42 2.34 2.25 2.58
1.25 2.69 2.69 2.64

parts of the dialog history. The results demonstrate that the agents rewarded for getting closer to the goal room consistently outperform the agents rewarded for getting closer to the exact goal location. This proves that using a reward function better aligned with the NDH task’s objective yields better performance than other reward functions.
6 Conclusion
In this work, we presented an environment-agnostic multitask learning framework to learn generalized policies for agents tasked with natural language grounded navigation. We applied the framework to train agents that can simultaneously solve two popular and challenging tasks in the space: Vision-and-Language Navigation and Navigation from Dialog History. We showed that our approach effectively transfers knowledge across tasks and learns more generalized environment representations. As a result, the trained agents not only close down the performance gap between seen and unseen environments but also outperform the single-task baselines on both tasks by a signiﬁcant margin. Furthermore, the studies show the two approaches of multitask learning and environment-agnostic learning independently beneﬁt the agent learning and complement each other. There are possible future extensions to our work—MT-RCM can further be adapted to other language-grounded navigation datasets (e.g., Touchdown [7], TalkTheWalk [46], StreetLearn [33]); and complementary techniques like environmental dropout [42] can be combined with environment-agnostic learning to learn more general representations.

Environment-agnostic Multitask Navigation

15

7 Acknowledgements

We thank Larry Lansing for his enormous eﬀorts in scaling the VALAN framework which made it possible to run experiments at scale.

References
1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D.G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., Zheng, X.: Tensorﬂow: A system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). pp. 265–283 (2016), https://www.usenix.org/system/files/conference/ osdi16/osdi16-abadi.pdf
2. Anderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., Zamir, A.R.: On evaluation of embodied navigation agents. arXiv (2018), arXiv:1807.06757 [cs.AI]
3. Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Su¨nderhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3674–3683 (2018)
4. Bain, M., Sammut, C.: A framework for behavioural cloning. In: Machine Intelligence 15, Intelligent Agents [St. Catherine’s College, Oxford, July 1995]. pp. 103– 129. Oxford University, Oxford, UK, UK (1999), http://dl.acm.org/citation. cfm?id=647636.733043
5. Caruana, R.: Multitask learning: A knowledge-based source of inductive bias. In: Proceedings of the Tenth International Conference on Machine Learning. pp. 41– 48. Morgan Kaufmann (1993)
6. Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV) (2017)
7. Chen, H., Suhr, A., Misra, D., Snavely, N., Artzi, Y.: Touchdown: Natural language navigation and spatial reasoning in visual street environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 12538– 12547 (2019)
8. Collobert, R., Weston, J.: A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In: Proceedings of the 25th International Conference on Machine Learning. pp. 160–167. ICML ’08, ACM, New York, NY, USA (2008). https://doi.org/10.1145/1390156.1390177, http: //doi.acm.org/10.1145/1390156.1390177
9. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp. 2054–2063 (2018)
10. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D., Batra, D.: Visual Dialog. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)

16

X. Wang et al.

11. Deng, L., Hinton, G., Kingsbury, B.: New types of deep neural network learning for speech recognition and related applications: an overview. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 8599–8603 (May 2013). https://doi.org/10.1109/ICASSP.2013.6639344
12. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., Kavukcuoglu, K.: IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In: Dy, J., Krause, A. (eds.) Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 80, pp. 1407–1416. PMLR, Stockholmsmssan, Stockholm Sweden (10–15 Jul 2018), http://proceedings.mlr.press/v80/espeholt18a.html
13. Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep networks. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 1126–1135. JMLR. org (2017)
14. Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.P., BergKirkpatrick, T., Saenko, K., Klein, D., Darrell, T.: Speaker-follower models for vision-and-language navigation. In: Neural Information Processing Systems (NeurIPS) (2018)
15. Fu, T.J., Wang, X., Peterson, M., Grafton, S., Eckstein, M., Wang, W.Y.: Counterfactual vision-and-language navigation via adversarial path sampling. arXiv preprint arXiv:1911.07308 (2019)
16. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation. In: Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37. pp. 1180–1189. ICML’15, JMLR.org (2015), http://dl.acm.org/citation.cfm?id=3045118.3045244
17. Girshick, R.: Fast r-cnn. In: 2015 IEEE International Conference on Computer Vision (ICCV). pp. 1440–1448 (Dec 2015). https://doi.org/10.1109/ICCV.2015.169
18. Hao, W., Li, C., Li, X., Carin, L., Gao, J.: Towards learning a generic agent for vision-and-language navigation via pre-training. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)
19. Hemachandra, S., Duvallet, F., Howard, T.M., Roy, N., Stentz, A., Walter, M.R.: Learning models for following natural language directions in unknown environments. In: 2015 IEEE International Conference on Robotics and Automation (ICRA). pp. 5608–5615. IEEE (2015)
20. Huang, H., Jain, V., Mehta, H., Baldridge, J., Ie, E.: Multi-modal discriminative model for vision-and-language navigation. In: Proceedings of the Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication for Robotics (RoboNLP). pp. 40–49. Association for Computational Linguistics, Minneapolis, Minnesota (2019). https://doi.org/10.18653/v1/W19-1605, https://www.aclweb.org/anthology/W19-1605
21. Huang, H., Jain, V., Mehta, H., Ku, A., Magalha˜es, G., Baldridge, J., Ie, E.: Transferable representation learning in vision-and-language navigation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision178(ICCV) (2019)
22. Jain, V., Magalha˜es, G., Ku, A., Vaswani, A., Ie, E., Baldridge, J.: Stay on the path: Instruction ﬁdelity in vision-and-language navigation. In: ACL (2019)
23. Ke, L., Li, X., Bisk, Y., Holtzman, A., Gan, Z., Liu, J., Gao, J., Choi, Y., Srinivasa, S.: Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6741–6749 (2019)

Environment-agnostic Multitask Navigation

17

24. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv (2017)
25. Lansing, L., Jain, V., Mehta, H., Huang, H., Ie, E.: Valan: Vision and language agent navigation. ArXiv abs/1912.03241 (2019)
26. Li, J., Wang, X., Tang, S., Shi, H., Wu, F., Zhuang, Y., Wang, W.Y.: Unsupervised reinforcement learning of transferable meta-skills for embodied navigation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12123–12132 (2020)
27. Li, X., Li, C., Xia, Q., Bisk, Y., C¸ elikyilmaz, A., Gao, J., Smith, N.A., Choi, Y.: Robust navigation with language pretraining and stochastic sampling. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLPIJCNLP 2019, Hong Kong, China, November 3-7, 2019. pp. 1494–1499. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/D19-1159, https://doi.org/10.18653/v1/D19-1159
28. Li, Y., Baldwin, T., Cohn, T.: What’s in a Domain? Learning Domain-Robust Text Representations using Adversarial Training. In: NAACL-HLT (2018)
29. Liu, A.H., Liu, Y.C., Yeh, Y.Y., Wang, Y.C.F.: A uniﬁed feature disentangler for multi-domain image translation and manipulation. In: Advances in Neural Information Processing Systems. pp. 2590–2599 (2018)
30. Ma, C.Y., Lu, J., Wu, Z., AlRegib, G., Kira, Z., Socher, R., Xiong, C.: Selfmonitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035 (2019)
31. Ma, C.Y., Wu, Z., AlRegib, G., Xiong, C., Kira, Z.: The regretful agent: Heuristicaided navigation through progress estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6732–6740 (2019)
32. Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: A Platform for Embodied AI Research. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019)
33. Mirowski, P., Grimes, M., Malinowski, M., Hermann, K.M., Anderson, K., Teplyashin, D., Simonyan, K., Kavukcuoglu, K., Zisserman, A., Hadsell, R.: Learning to navigate in cities without a map. In: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Processing Systems 31, pp. 2419–2430. Curran Associates, Inc. (2018), http://papers.nips.cc/paper/ 7509-learning-to-navigate-in-cities-without-a-map.pdf
34. Mirowski, P.W., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., Hadsell, R.: Learning to navigate in complex environments. ArXiv abs/1611.03673 (2016)
35. Nguyen, K., Daum´e III, H.: Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. arXiv preprint arXiv:1909.01871 (2019)
36. Nguyen, K., Dey, D., Brockett, C., Dolan, B.: Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 12527–12537 (2019)
37. Peng, X., Huang, Z., Sun, X., Saenko, K.: Domain agnostic learning with disentangled representations. In: Proceedings of the 36th International Conference on

18

X. Wang et al.

Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA. pp. 5102–5112 (2019) 38. Qi, Y., Wu, Q., Anderson, P., Wang, X., Wang, W.Y., Shen, C., Hengel, A.v.d.: Reverie: Remote embodied visual referring expression in real indoor environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9982–9991 (2020) 39. Ramsundar, B., Kearnes, S.M., Riley, P., Webster, D., Konerding, D.E., Pande, V.S.: Massively multitask networks for drug discovery. ArXiv abs/1502.02072 (2015) 40. Romijnders, R., Meletis, P., Dubbelman, G.: A domain agnostic normalization layer for unsupervised adversarial domain adaptation. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 1866–1875. IEEE (2019) 41. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Trans. Signal Processing 45, 2673–2681 (1997) 42. Tan, H., Yu, L., Bansal, M.: Learning to navigate unseen environments: Back translation with environmental dropout. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 2610–2621. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1268 43. Teh, Y., Bapst, V., Czarnecki, W.M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., Pascanu, R.: Distral: Robust multitask reinforcement learning. In: Advances in Neural Information Processing Systems. pp. 4496–4506 (2017) 44. Thomason, J., Murray, M., Cakmak, M., Zettlemoyer, L.: Vision-and-dialog navigation. In: Conference on Robot Learning (CoRL) (2019) 45. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998–6008 (2017) 46. de Vries, H., Shuster, K., Batra, D., Parikh, D., Weston, J., Kiela, D.: Talk the walk: Navigating new york city through grounded dialogue. arXiv preprint arXiv:1807.03367 (2018) 47. Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L.: Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6629–6638 (2019) 48. Wang, X., Xiong, W., Wang, H., Yang Wang, W.: Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 37–53 (2018) 49. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8(3), 229–256 (May 1992). https://doi.org/10.1007/BF00992696, https://doi.org/10.1007/BF00992696 50. Wu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209 (2018) 51. Wu, Y., Wu, Y., Tamar, A., Russell, S., Gkioxari, G., Tian, Y.: Learning and planning with a semantic model. arXiv preprint arXiv:1809.10842 (2018) 52. Xia, F., R. Zamir, A., He, Z.Y., Sax, A., Malik, J., Savarese, S.: Gibson env: realworld perception for embodied agents. In: Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on. IEEE (2018)

Environment-agnostic Multitask Navigation

19

A Appendix
A.1 Multitask Learning with Total Training Paths Fixed
To verify whether multitask learning helps only due to implicit data augmentation which increases the number of training paths for both the tasks, we conducted an experiment by ﬁxing the total number of training paths. We ﬁx the size of training dataset to be exactly 4, 742 paths (which is the same as the number of paths in the NDH task’s training dataset) and replace a fraction of the paths by the paths from the VLN task’s training dataset. The results in Table 7 show that the agent’s performance on previously unseen environments in NDH task improves signiﬁcantly when trained jointly on NDH paths mixed with a small fraction of VLN paths. Since the total training paths are ﬁxed, there is no beneﬁt due to data augmentation which furthers the argument that multitask learning on NDH and VLN tasks complements the agent’s learning. As expected, the agent’s performance on NDH task degrades when trained on datasets containing smaller fractions of NDH paths but larger fractions of VLN paths.

Table 7: Comparison of agent’s performance on NDH task when trained on ﬁxed number of paths. The paths belong to either of the two tasks to support multitask learning.
Fraction of VLN paths (%)
0 10 20 30 40 60 80
Progress (Val Seen) 6.49 6.21 5.69 5.72 5.82 5.74 3.66 Progress (Val Unseen) 2.64 3.13 3.09 3.31 2.80 2.86 2.48

A.2 Detailed Ablation on Parameter Sharing of Language Encoder
Table 8 presents a more detailed analysis from Table 4 with access to diﬀerent parts of dialog history. The models with shared language encoder consistently outperform those with separate encoders.
A.3 VLN Leaderboard Submission
Table 9 shows the performance of our MT-RCM + EnvAg agent on the test set of the R2R dataset for the VLN task. Our MT-RCM + EnvAg agent outperforms the comparable baseline RCM on the primary navigation metrics of SR and SPL which proves the eﬀectiveness of multitask and environment-agnostic learning. It is worth noting that the baselines scoring high on the test set use additional techniques like data augmentation and pre-training which were not explored in this work but are complementary to our techniques of multitask learning and environment-agnostic learning.

20

X. Wang et al.

Table 8: Comparison of agent’s performance when language instructions are encoded by separate vs. shared encoder for VLN and NDH tasks.

NDH Evaluation

VLN Evaluation

Fold

Language Inputs for NDH Progress PL NE SR SPL CLS

Encoder

t0 Ai Qi A1:i−1; Q1:i−1

↑

↓↑ ↑ ↑



Shared

  

Val

 



Seen



Separate

  

 



3.00 5.92 5.43 5.28
2.85 4.90 5.07 5.17

11.73 4.87 54.56 52.00 65.64 11.12 4.62 54.89 52.62 66.05 10.94 4.59 54.23 52.06 66.93 10.63 5.09 56.42 49.67 68.28
11.43 4.81 54.66 51.11 65.37 11.92 4.92 53.64 49.79 61.49 11.34 4.76 55.34 51.59 65.52 11.26 5.02 52.38 48.80 64.19



Shared

  

Val

 



Unseen



Separate

  

 



1.69 4.01 3.75 4.36
1.79 3.66 3.51 4.07

13.12 5.84 42.75 38.71 53.09 11.06 5.88 42.98 40.62 54.30 11.08 5.70 44.50 39.67 54.95 10.23 5.31 46.20 44.19 54.99
11.85 6.01 42.43 38.19 54.01 12.59 5.97 43.45 38.62 53.49 12.23 5.89 44.40 39.54 54.55 11.72 6.04 43.64 39.49 54.57

Environment-agnostic Multitask Navigation

21

Table 9: VLN Leaderboard results for R2R test dataset.

Model

PL NE ↓ SR ↑ SPL ↑

Human Random

11.85 1.61 86

76

9.93 9.77 13

12

Seq2Seq [3]

8.13 7.85 20

18

Look Before You Leap [48]

9.15 7.53 25

23

Speaker-Follower [14]

14.82 6.62 35

28

Self-Monitoring [30]

18.04 5.67 48

35

RCM [47]

11.97 6.12 43

38

The Regretful Agent [31]

13.69 5.69 48

40

ALTR [21]

10.27 5.49 48

45

Press [27]

10.77 5.49 49

45

EnvDrop [42]

11.66 5.23 51

47

PREVALENT [18]

10.51 5.30 54

51

MT-RCM + EnvAg (Ours) 13.35 6.03 45

40

