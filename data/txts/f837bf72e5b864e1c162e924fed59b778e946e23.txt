arXiv:2011.02917v1 [cs.CL] 5 Nov 2020

Imagining Grounded Conceptual Representations from Perceptual Information in Situated Guessing Games
Alessandro Suglia1, Antonio Vergari2, Ioannis Konstas1, Yonatan Bisk3, Emanuele Bastianelli1, Andrea Vanzo1, and Oliver Lemon1 1Heriot-Watt University, Edinburgh, UK 2University of California, Los Angeles, USA 3Carnegie Mellon University, Pittsburgh, USA
1{as247,i.konstas,a.vanzo,e.bastianelli,o.lemon}@hw.ac.uk 2aver@cs.ucla.edu, 3ybisk@cs.cmu.edu
Abstract
In visual guessing games, a Guesser has to identify a target object in a scene by asking questions to an Oracle. An effective strategy for the players is to learn conceptual representations of objects that are both discriminative and expressive enough to ask questions and guess correctly. However, as shown by Suglia et al. (2020), existing models fail to learn truly multi-modal representations, relying instead on gold category labels for objects in the scene both at training and inference time. This provides an unnatural performance advantage when categories at inference time match those at training time, and it causes models to fail in more realistic “zeroshot” scenarios where out-of-domain object categories are involved. To overcome this issue, we introduce a novel “imagination” module based on Regularized Auto-Encoders, that learns context-aware and category-aware latent embeddings without relying on category labels at inference time. Our imagination module outperforms state-of-the-art competitors by 8.26% gameplay accuracy in the CompGuessWhat?! zero-shot scenario (Suglia et al., 2020), and it improves the Oracle and Guesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no gold categories are available at inference time. The imagination module also boosts reasoning about object properties and attributes.
1 Introduction
Humans do not learn conceptual representations from language alone, but from a wide range of situational information (Beinborn et al., 2018; Bisk et al., 2020) as highlighted also by property-listing experiments (McRae et al., 2005). When humans experience the concept of “boat”, they simulate a new representation by reactivating and aggregating multi-modal representations that reside in their memory and are associated with the concept of “boat” (e.g., what a boat looks like, the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or reﬁned with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities.
Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve speciﬁc tasks in speciﬁc environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning to ground pixels of the scene into object representations that are relevant for the object category they belong to (category-aware), but are also particularized for the speciﬁc scene (context-aware), is fundamental for the Guesser to effectively converse with the Oracle and vice-versa.
This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.

Training time

Inference time

???

c< l a t e x i t s h a 1 _ b a s e 6 4 = " b g b 8 B W 6 S k Y N w c q d D M d x I H 9 R N r p 0 = " > A A A C O n i c b V B N a x R B E O 1 J 1 M T x a x N v e m l c B E / L j C x E b 8 F c P E Z w k + D O s N T 0 1 G y a d P c M 3 T U m S z P / w m v 8 J f k j u X q T X P M D 7 P 0 4 6 K 4 P C h 7 v V V G P V z R K O k q S 2 2 h r + 8 H D R z u 7 j + M n T 5 8 9 f 9 H b 2 z 9 x d W s F j k S t a n t W g E M l D Y 5 I k s K z x i L o Q u F p c X E 0 9 0 + / o 3 W y N l 9 p 1 m C u Y W p k J Q V Q k L 5 l G u i 8 q L z o J r 1 + M k g W 4 J s k X Z E + W + F 4 s h e 9 y s p a t B o N C Q X O j d O k o d y D J S k U d n H W O m x A X M A U x 4 E a 0 O h y v 4 j c 8 b d B K X l V 2 z C G + E L 9 + 8 K D d m 6 m i 7 A 5 j + j W v b n 4 X 6 / Q a 5 + p + p B 7 a Z q W 0 I j l 4 6 p V n G o + 7 4 O X 0 q I g N Q s E h J U h O x f n Y E F Q a C 2 O M 4 O X o t Y a T O k z s F M N V 9 0 4 z X 3 W m j I s I P l + 2 v m M 8 I r 8 0 u Z d 1 8 W h z X S 9 u 0 1 y 8 n 6 Q D g c f v w z 7 h 5 9 W v e 6 y 1 + w N e 8 d S d s A O 2 W d 2 z E Z M M M N + s G v 2 M 7 q J f k W / o 7 v l 6 l a 0 u n n J / k F 0 / w c P D a 4 h < / l a t e x i t >

pasticciotto

z< l a t e x i t s h a 1 _ b a s e 6 4 = " x p N q h g 0 P G b + c b Y n + l 5 H C I C q J k C Q = " > A A A C O n i c b V D L b t R A E B w n Q I J 5 Z c M N L i N W S J x W N l o J c o v g k m O Q 2 C R i b U X t c X s z y s z Y m m n D L i P / R a 7 J l + R H c u W G u O Y D M v s 4 w I a S W i p V d a t L V T R K O k q S m 2 h j 8 8 H D R 1 v b j + M n T 5 8 9 f 7 H T 2 z 1 y d W s F j k S t a n t S g E M l D Y 5 I k s K T x i L o Q u F x c f 5 5 7 h 9 / R + t k b b 7 S r M F c w 8 T I S g q g I H 3 L N N B Z U f m f 3 e l O P x k k C / D 7 J F 2 R P l v h 8 L Q X v c r K W r Q a D Q k F z o 3 T p K H c g y U p F H Z x 1 j p s Q J z D B M e B G t D o c r + I 3 P G 3 Q S l 5 V d s w h v h C / f v C g 3 Z u p o u w O Y / o 1 r 2 5 + F + v 0 G u f q f q Y e 2 m a l t C I 5 e O q V Z x q P u + D l 9 K i I D U L B I S V I T s X Z 2 B B U G g t j j O D P 0 S t N Z j S Z 2 A n G q b d O M 1 9 1 p o y L C D 5 f t r 5 j H B K f m n z r u v i 0 G a 6 3 t 1 9 c v R + k A 4 H e 1 + G / f 1 P q 1 6 3 2 W v 2 h r 1 j K f v A 9 t k B O 2 Q j J p h h F + y S X U X X 0 a / o d / R n u b o R r W 5 e s n 8 Q 3 d 4 B O I + u O A = = < / l a t e x i t >

c< l a t e x i t s h a 1 _ b a s e 6 4 = " b g b 8 B W 6 S k Y N w c q d D M d x I H 9 R N r p 0 = " > A A A C O n i c b V B N a x R B E O 1 J 1 M T x a x N v e m l c B E / L j C x E b 8 F c P E Z w k + D O s N T 0 1 G y a d P c M 3 T U m S z P / w m v 8 J f k j u X q T X P M D 7 P 0 4 6 K 4 P C h 7 v V V G P V z R K O k q S 2 2 h r + 8 H D R z u 7 j + M n T 5 8 9 f 9 H b 2 z 9 x d W s F j k S t a n t W g E M l D Y 5 I k s K z x i L o Q u F p c X E 0 9 0 + / o 3 W y N l 9 p 1 m C u Y W p k J Q V Q k L 5 l G u i 8 q L z o J r 1 + M k g W 4 J s k X Z E + W + F 4 s h e 9 y s p a t B o N C Q X O j d O k o d y D J S k U d n H W O m x A X M A U x 4 E a 0 O h y v 4 j c 8 b d B K X l V 2 z C G + E L 9 + 8 K D d m 6 m i 7 A 5 j + j W v b n 4 X 6 / Q a 5 + p + p B 7 a Z q W 0 I j l 4 6 p V n G o + 7 4 O X 0 q I g N Q s E h J U h O x f n Y E F Q a C 2 O M 4 O X o t Y a T O k z s F M N V 9 0 4 z X 3 W m j I s I P l + 2 v m M 8 I r 8 0 u Z d 1 8 W h z X S 9 u 0 1 y 8 n 6 Q D g c f v w z 7 h 5 9 W v e 6 y 1 + w N e 8 d S d s A O 2 W d 2 z E Z M M M N + s G v 2 M 7 q J f k W / o 7 v l 6 l a 0 u n n J / k F 0 / w c P D a 4 h < / l a t e x i t >
person person
z< l a t e x i t s h a 1 _ b a s e 6 4 = " x p N q h g 0 P G b + c b Y n + l 5 H C I C q J k C Q = " > A A A C O n i c b V D L b t R A E B w n Q I J 5 Z c M N L i N W S J x W N l o J c o v g k m O Q 2 C R i b U X t c X s z y s z Y m m n D L i P / R a 7 J l + R H c u W G u O Y D M v s 4 w I a S W i p V d a t L V T R K O k q S m 2 h j 8 8 H D R 1 v b j + M n T 5 8 9 f 7 H T 2 z 1 y d W s F j k S t a n t S g E M l D Y 5 I k s K T x i L o Q u F x c f 5 5 7 h 9 / R + t k b b 7 S r M F c w 8 T I S g q g I H 3 L N N B Z U f m f 3 e l O P x k k C / D 7 J F 2 R P l v h 8 L Q X v c r K W r Q a D Q k F z o 3 T p K H c g y U p F H Z x 1 j p s Q J z D B M e B G t D o c r + I 3 P G 3 Q S l 5 V d s w h v h C / f v C g 3 Z u p o u w O Y / o 1 r 2 5 + F + v 0 G u f q f q Y e 2 m a l t C I 5 e O q V Z x q P u + D l 9 K i I D U L B I S V I T s X Z 2 B B U G g t j j O D P 0 S t N Z j S Z 2 A n G q b d O M 1 9 1 p o y L C D 5 f t r 5 j H B K f m n z r u v i 0 G a 6 3 t 1 9 c v R + k A 4 H e 1 + G / f 1 P q 1 6 3 2 W v 2 h r 1 j K f v A 9 t k B O 2 Q j J p h h F + y S X U X X 0 a / o d / R n u b o R r W 5 e s n 8 Q 3 d 4 B O I + u O A = = < / l a t e x i t >

donut

donut

donut

c< l a t e x i t s h a 1 _ b a s e 6 4 = " b g b 8 B W 6 S k Y N w c q d D M d x I H 9 R N r p 0 = " > A A A C O n i c b V B N a x R B E O 1 J 1 M T x a x N v e m l c B E / L j C x E b 8 F c P E Z w k + D O s N T 0 1 G y a d P c M 3 T U m S z P / w m v 8 J f k j u X q T X P M D 7 P 0 4 6 K 4 P C h 7 v V V G P V z R K O k q S 2 2 h r + 8 H D R z u 7 j + M n T 5 8 9 f 9 H b 2 z 9 x d W s F j k S t a n t W g E M l D Y 5 I k s K z x i L o Q u F p c X E 0 9 0 + / o 3 W y N l 9 p 1 m C u Y W p k J Q V Q k L 5 l G u i 8 q L z o J r 1 + M k g W 4 J s k X Z E + W + F 4 s h e 9 y s p a t B o N C Q X O j d O k o d y D J S k U d n H W O m x A X M A U x 4 E a 0 O h y v 4 j c 8 b d B K X l V 2 z C G + E L 9 + 8 K D d m 6 m i 7 A 5 j + j W v b n 4 X 6 / Q a 5 + p + p B 7 a Z q W 0 I j l 4 6 p V n G o + 7 4 O X 0 q I g N Q s E h J U h O x f n Y E F Q a C 2 O M 4 O X o t Y a T O k z s F M N V 9 0 4 z X 3 W m j I s I P l + 2 v m M 8 I r 8 0 u Z d 1 8 W h z X S 9 u 0 1 y 8 n 6 Q D g c f v w z 7 h 5 9 W v e 6 y 1 + w N e 8 d S d s A O 2 W d 2 z E Z M M M N + s G v 2 M 7 q J f k W / o 7 v l 6 l a 0 u n n J / k F 0 / w c P D a 4 h < / l a t e x i t >

donut

z< l a t e x i t s h a 1 _ b a s e 6 4 = " x p N q h g 0 P G b + c b Y n + l 5 H C I C q J k C Q = " > A A A C O n i c b V D L b t R A E B w n Q I J 5 Z c M N L i N W S J x W N l o J c o v g k m O Q 2 C R i b U X t c X s z y s z Y m m n D L i P / R a 7 J l + R H c u W G u O Y D M v s 4 w I a S W i p V d a t L V T R K O k q S m 2 h j 8 8 H D R 1 v b j + M n T 5 8 9 f 7 H T 2 z 1 y d W s F j k S t a n t S g E M l D Y 5 I k s K T x i L o Q u F x c f 5 5 7 h 9 / R + t k b b 7 S r M F c w 8 T I S g q g I H 3 L N N B Z U f m f 3 e l O P x k k C / D 7 J F 2 R P l v h 8 L Q X v c r K W r Q a D Q k F z o 3 T p K H c g y U p F H Z x 1 j p s Q J z D B M e B G t D o c r + I 3 P G 3 Q S l 5 V d s w h v h C / f v C g 3 Z u p o u w O Y / o 1 r 2 5 + F + v 0 G u f q f q Y e 2 m a l t C I 5 e O q V Z x q P u + D l 9 K i I D U L B I S V I T s X Z 2 B B U G g t j j O D P 0 S t N Z j S Z 2 A n G q b d O M 1 9 1 p o y L C D 5 f t r 5 j H B K f m n z r u v i 0 G a 6 3 t 1 9 c v R + k A 4 H e 1 + G / f 1 P q 1 6 3 2 W v 2 h r 1 j K f v A 9 t k B O 2 Q j J p h h F + y S X U X X 0 a / o d / R n u b o R r W 5 e s n 8 Q 3 d 4 B O I + u O A = = < / l a t e x i t >

Figure 1: Common approaches to visual grounding such as De Vries et al. (2017) and Zhuang et al. (2018) rely on gold category labels at test time, thereby failing to ground novel objects from categories not seen during training (e.g., a “pasticciotto”, top right) or to properly encode known categories but with unseen visual features (like a “frosted donut”, bottom right) since they employ category embeddings c from a predeﬁned set that are ﬁxed for each object. Instead, embeddings z learned by our imagination module can be ﬂexibly category-aware allowing them to generalize to unseen categories.

We consider a model truly multi-modal if it always uses all the modalities to make decisions. However, existing approaches (De Vries et al., 2017; Shekhar et al., 2019) rely instead on gold category labels that are assumed to be available also at inference time, thus making these models depend on this modality and discarding the others. This not only poses an unnatural performance advantage for players in controlled benchmark scenarios like the GuessWhat?! game when categories at inference time match those at training time, but causes them to fail in more realistic zero-shot scenarios (Suglia et al., 2020) where players are required to generalize to out-of-domain object categories. For example, consider an agent that during training has only seen glazed donuts, associated with the ﬁxed “donut” category embedding (cf. Figure 1). At inference time, the model cannot ground visual representations for objects belonging to the “pasticciotto” (an Italian pastry) category, since such a category was not in its repertoire. Similarly, it will likely represent frosted donuts with a generic “donut” embedding, despite the perceptual differences among different types of donut.
In this paper, we tackle the above limitations by introducing a novel imagination module based on Regularized Auto-encoders (Ghosh et al., 2019), which are able to derive imagination embeddings directly from perceptual information in the form of the object crop. Our formulation of the reconstruction loss allows the model to learn context-aware and category-aware imagination embeddings. Thus, removing the need for gold category labels at inference time and greatly improving zero-shot generalization. Section 4.2 integrates our imagination component into the Oracle model of De Vries et al. (2017) and the Guesser model of Shekhar et al. (2019). We show that the new imagination models are state-of-the-art in the recently introduced CompGuessWhat?! benchmark (Suglia et al., 2020) outperforming current models by 8.26%. It also improves the Oracle’s and Guesser’s accuracy (by 2.08% and 12.86%, respectively) in the standard GuessWhat?! when no gold category labels are available. Lastly, we show that imagining latent object representations greatly helps to reason about object visual properties (i.e., color, shape, etc.), qualifying our module as a generic perceptual simulation component ala` Barsalou (2008).
2 Background: Guessing Games and Concept Representations
GuessWhat?! is an instance of a multi-word guessing game (Steels, 2015). Every game involves two players: an Oracle and a Guesser conversing about a scene S (a natural image). A scene S can be

abstracted into a collection of objects O, each of which is associated with a category ci ∈ C, i = {1, . . . , K}. The aim of the Guesser is to identify a target object o∗ ∈ O by asking questions about S to the Oracle. The gameplay of GuessWhat?! thus comprises three tasks: i) question generation where the Guesser inquires about an object in the scene S given the dialogue generated so far; ii) answer prediction, where the Oracle answers a ∈ A = {Yes, No, N/A} given the scene S, question and the target object o∗; and iii) target prediction where the Guesser selects a candidate object with the highest relevance score r(oi).
Several architectural variants have been proposed to tackle GuessWhat?! (cf. Section 5 for some related works). In this work we adopt the recent GDSE model (Shekhar et al., 2019), which learns a visually grounded dialogue state used to learn both question generation and target object prediction. As shown below, GDSE does not deliver the desired multi-modality needed, therefore we extend it with our Imagination component to obtain more effective multi-modal object representations.
For successful gameplay, both the Guesser and Oracle must build representations of the scene that contain speciﬁc perceptual information of objects (object-aware), are relevant for the object category they belong to (category-aware), and are specialized to the scene in which the game is played (contextaware). As the scene S is an image, it is natural to associate each object oi ∈ O with a perceptual embedding, i.e., a vector vi ∈ RdO extracted from the penultimate layer of a pretrained vision model (e.g. ResNet-152 (Shekhar et al., 2019)) based on their bounding box.1
However, these representations are not sufﬁcient as they are neither context-aware nor category-aware, i.e., they ignore other objects in the scene and do not leverage their category information. GDSE and other recent approaches (De Vries et al., 2017; Shekhar et al., 2019; Zhuang et al., 2018; Shukla et al., 2019) coped with the second issue by introducing category embeddings as dC-dimensional continuous representations ck ∈ RdC for k = 1, . . . , K. Once learned, a category embedding c is then concatenated to an 8-dimensional feature vector si derived from the object bounding box (cf. De Vries et al. (2017)). While these embeddings partially solve category-awareness, they are not object-aware. For instance, the embedding for the object category “apple” will be the same regardless of a particular object to be a red or green apple, i.e., most likely a centroid representation of the objects seen only during training. Moreover, if during training we only see red apples, at inference time, we will likely fail to detect green apples as belonging to the same category (Figure 2(a)). These issues have gone unnoticed since category embeddings usually boost performances on the original GuessWhat?! task, given that gold category labels are also available at inference time. However, this boost is illusory: models relying on this symbolic information to be always available are not learning to exploit all modalities. In fact, a 20% drop in the Guesser accuracy if gold category labels are not provided has been reported in Zhuang et al. (2018) for GuessWhat?! and analogous poor results in more realistic benchmarks measuring zero-shot generalization such as CompGuessWhat?! (Suglia et al., 2020).
3 Imagination Module: Learning Context- and Category-aware Object Representations
To overcome the limitations of GDSE and competitors and realize a form of perceptual simulation in a learning system, we introduce a generic component—named the imagination module—which learns latent concept representations that are both context- and category-aware, without relying on category labels at inference time. Our imagination model can be understood in the context of representation learning via deep generative models (Bengio et al., 2013) which has been popularized by variational autoencoders (VAEs) (Kingma and Welling, 2013; Kingma et al., 2014), and GANs (Goodfellow et al., 2014). Speciﬁcally, we substantially extend the recently introduced regularized autoencoders (RAEs) framework (Ghosh et al., 2019). RAEs are simpliﬁed VAEs where stochasticity in the encoder and decoder is dropped in favor of more stable training and more informative embedding learning. In fact, RAEs do not suffer from several issues known to affect VAEs, such as poor convergence and the possibility of learning embeddings that are independent of the input images (cf. Ghosh et al. (2019) for a detailed discussion). More crucially for our purposes, RAEs do not have to compromise the informativeness of the learned embeddings with a ﬁxed a-priori structure in the latent space that enables simple
1Bounding boxes are assumed to be given, e.g. by using object recognition as a pre-processing step (Anderson et al., 2018).

ResNet features
v< l a t e x i t s h a 1 _ b a s e 6 4 = " S v 2 o I x 1 l g r D 1 b 4 5 O R I n S N 1 / / o U Q = " > A A A C P H i c b V B N S x x B E O 0 x 8 W u i R p N b v D R Z B E / L j A g m N z G X H A 1 k V d g Z l p q e m r W x u 2 f o r j E u z f w N r + a X 5 H / k n l v I N e f 0 f h x 0 z Y O C x 3 t V 1 O M V j Z K O k u R n t P L i 5 e r a + s Z m / G p r e + f 1 7 t 6 b C 1 e 3 V u B A 1 K q 2 V w U 4 V N L g g C Q p v G o s g i 4 U X h Y 3 n 6 b + 5 S 1 a J 2 v z l S Y N 5 h r G R l Z S A A U p y z T Q d V H 5 2 2 4 k R 7 u 9 p J / M w J + T d E F 6 b I H z 0 V 7 0 L i t r 0 W o 0 J B Q 4 N 0 y T h n I P l q R Q 2 M V Z 6 7 A B c Q N j H A Z q Q K P L / S x 0 x w + C U v K q t m E M 8 Z n 6 + M K D d m 6 i i 7 A 5 D e m W v a n 4 X 6 / Q S 5 + p + p B 7 a Z q W 0 I j 5 4 6 p V n G o + b Y S X 0 q I g N Q k E h J U h O x f X Y E F Q 6 C 2 O M 4 P f R K 0 1 m N J n Y M c a 7 r p h m v u s N W V Y Q P K 9 t P M Z 4 R 3 5 u c 2 7 r o t D m + l y d 8 / J x V E / P e 5 / / H L c O z 1 b 9 L r B 9 t l 7 d s h S d s J O 2 W d 2 z g Z M s I b d s w f 2 P f o R / Y p + R 3 / m q y v R 4 u Y t e 4 L o 7 z / s 5 a 8 Q < / l a t e x i t > i

Imagination embedding

Encoder

Decoder

E
< l a t e x i t s h a 1 _ b a s e 6 4 = " T U C 0 O H K T l U o F R T + y I P j u e s m o Y 4 0 = " > A A A C O H i c b V B N S x x B E O 0 x M d G J S T T e 4 q X J I n h a Z o J g c p N I I E e F r C 7 s D E t N T + 1 u Y 3 f P 0 F 2 T u D T z J 3 I 1 v 8 R / 4 i 0 3 8 e o v s P f j k K x 5 U P B 4 7 x V V v K J W 0 l G S 3 E Z r z 5 6 v v 3 i 5 s R m / 2 n r 9 5 u 3 2 z r t z V z V W Y E 9 U q r L 9 A h w q a b B H k h T 2 a 4 u g C 4 U X x e X J z L / 4 g d b J y n y n a Y 2 5 h r G R I y m A g t T / O v R Z P Z H t c L u T d J M 5 + F O S L k m H L X E 6 3 I n e Z 2 U l G o 2 G h A L n B m l S U + 7 B k h Q K 2 z h r H N Y g L m G M g 0 A N a H S 5 n z / c 8 v 2 g l H x U 2 T C G + F z 9 e 8 O D d m 6 q i 5 D U Q B O 3 6 s 3 E / 3 q F X r l M o 0 + 5 l 6 Z u C I 1 Y H B 4 1 i l P F Z 2 3 w U l o U p K a B g L A y / M 7 F B C w I C p 3 F c W b w p 6 i 0 B l P 6 D O x Y w 1 U 7 S H O f N a Y M A S T f S V u f E V 6 R X 9 i 8 b d s 4 t J m u d v e U n H / s p o f d z 2 e H n e M v y 1 4 3 2 B 7 7 w A 5 Y y o 7 Y M f v G T l m P C a b Y L 3 b N f k c 3 0 Z / o L r p f R N e i 5 c 4 u + w f R w y M M p q 0 d < / l a t e x i t >

zi < l a t e x i t s h a 1 _ b a s e 6 4 = " s R g U g d 6 N P s C w B R o I 0 k b c V F T 7 O 3 w = " > A A A C P H i c b V B N S x x B E O 0 x M d G J S f y 4 x U u T R f C 0 z A T B 5 C b J x a O C q 8 L O s N T 0 1 K y N 3 T 1 D d 0 3 i p p m / k W v y S / I / c s 9 N v H p O 7 8 d B 1 z w o e L x X R T 1 e 0 S j p K E n + R C v P n q + + e L m 2 H r / a e P 3 m 7 e b W 9 r m r W y t w I G p V 2 8 s C H C p p c E C S F F 4 2 F k E X C i + K 6 y 9 T / + I r W i d r c 0 a T B n M N Y y M r K Y C C l G U a 6 K q o / P d u J E e b v a S f z M C f k n R B e m y B k 9 F W 9 C 4 r a 9 F q N C Q U O D d M k 4 Z y D 5 a k U N j F W e u w A X E N Y x w G a k C j y / 0 s d M f 3 g l L y q r Z h D P G Z + v D C g 3 Z u o o u w O Q 3 p l r 2 p + F + v 0 E u f q f q Y e 2 m a l t C I + e O q V Z x q P m 2 E l 9 K i I D U J B I S V I T s X V 2 B B U O g t j j O D 3 0 S t N Z j S Z 2 D H G m 6 6 Y Z r 7 r D V l W E D y v b T z G e E N + b n N u 6 6 L Q 5 v p c n d P y f m H f n r Q / 3 R 6 0 D v 6 v O h 1 j e 2 y 9 2 y f p e y Q H b F j d s I G T L C G / W A / 2 a / o d / Q 3 u o 3 u 5 q s r 0 e J m h z 1 C d P 8 P 9 C W v F A = = < / l a t e x i t >

D< l a t e x i t s h a 1 _ b a s e 6 4 = " j G J W U t 0 J O 8 H x p E U 3 W c h X p Z F j a z U = " > A A A C O n i c b V B N a x R B E O 2 J m o + J m s T c 4 q V x C X h a Z m Q h e g v R g 8 c I b h L c G Z a a n t r d J t 0 9 Q 3 d N k q W Z f + H V / J L 8 E a + 5 i V d / g L 0 f B 9 3 k Q c H j v V d U 8 Y p a S U d J 8 j N a e / L 0 2 f r G 5 l a 8 / f z F y 5 3 d v V d n r m q s w L 6 o V G U v C n C o p M E + S V J 4 U V s E X S g 8 L y 4 / z v z z K 7 R O V u Y r T W v M N Y y N H E k B F K R v n 4 Y + o w k S t M P d T t J N 5 u A P S b o k H b b E 6 X A v O s j K S j Q a D Q k F z g 3 S p K b c g y U p F L Z x 1 j i s Q V z C G A e B G t D o c j 9 / u e W H Q S n 5 q L J h D P G 5 + u + G B + 3 c V B c h q Y E m b t W b i Y 9 6 h V 6 5 T K P 3 u Z e m b g i N W B w e N Y p T x W d 9 8 F J a F K S m g Y C w M v z O x Q Q s C A q t x X F m 8 F p U W o M p f Q Z 2 r O G m H a S 5 z x p T h g C S 7 6 R t 6 B B v y C 9 s 3 r Z t H N p M V 7 t 7 S M 7 e d d N e 9 8 O X X u f 4 Z N n r J n v N 3 r C 3 L G V H 7 J h 9 Z q e s z w Q z 7 D v 7 w W 6 j u + g + + h X 9 X k T X o u X O P v s P 0 Z + / 2 7 G u B Q = = < / l a t e x i t > ✓

Recon. Input
v˜< l a t e x i t s h a 1 _ b a s e 6 4 = " s B E e r w 2 Y I X q T 6 I r G 1 A Z X G 7 t 4 w c Y = " > A A A C R n i c b V D L a h R B F L 0 9 8 R H b 1 y S 6 0 k 3 h I L g a u i W g 7 k L c u I z g J I H p Z q i u v j 0 p U l X d V N 2 O G Y v 6 l 2 z 1 S / w F f 8 K d u L X m s d C J B y 4 c z r m X e z h V p 6 S j L P u R D H Z u 3 b 5 z d / d e e v / B w 0 e P h 3 v 7 J 6 7 t r c C J a F V r z y r u U E m D E 5 K k 8 K y z y H W l 8 L S 6 e L / 0 T y / R O t m a T 7 T o s N R 8 b m Q j B a c o z Y Z P C 8 3 p v G p 8 Q V L V 6 C 9 D m M n Z c J S N s x X Y T Z J v y A g 2 O J 7 t J c + K u h W 9 R k N C c e e m e d Z R 6 b k l K R S G t O g d d l x c 8 D l O I z V c o y v 9 K n 5 g L 6 N S s 6 a 1 c Q y x l f r 3 h e f a u Y W u 4 u Y y r N v 2 l u J / v U p v f a b m b e m l 6 X p C I 9 a P m 1 4 x a t m y G 1 Z L i 4 L U I h I u r I z Z m T j n l g u K D a Z p Y f C z a L X m p v Y F t 3 P N r 8 I 0 L 3 3 R m z o u I P l R H m K R e E V + b b M Q Q h r b z L e 7 u 0 l O X o / z g / G 7 j w e j w 6 N N r 7 v w H F 7 A K 8 j h D R z C B z i G C Q j 4 A t f w F b 4 l 3 5 O f y a / k 9 3 p 1 k G x u n s A / G M A f 6 c e x + A = = < / l a t e x i t > i

L IRMEGC < l a t e x i t s h a 1 _ b a s e 6 4 = " w O 8 j N K W w g K o R K P P N K / m 0 U r a d / E 0 = " > A A A C a H i c b V D b a h R B E O 0 d L 4 n j b a M P o r 4 M W Q S f l h k J U d 9 C g q i g E M V N A j v j U t N T s 2 n S 3 T N 0 1 5 g s T f + J X + O r / o C / 4 F f Y e 0 F 0 Y 0 E 1 p 8 6 p o q p P 2 U p h K U 1 / 9 q I r V 6 9 d 3 9 i 8 E d + 8 d f v O 3 f 7 W v S P b d I b j i D e y M S c l W J R C 4 4 g E S T x p D Y I q J R 6 X Z w d z / f g L G i s a / Y l m L R Y K p l r U g g M F a t L f z V H b z q A C O n X 5 / O U g 3 T s / W R a 2 d h 9 f H X j v P / + p 3 7 5 / 7 f 2 k P 0 i H 6 S K S y y B b g Q F b x e F k q / c o r x r e K d T E J V g 7 z t K W C g e G B J f o 4 7 y z 2 A I / g y m O A 9 S g 0 B Z u 8 U G f P A l M l d S N C a k p W b B / T z h Q 1 s 5 U G T o X V 6 5 r c / K / W q n W N l P 9 o n B C t x 2 h 5 s v F d S c T a p K 5 e 0 k l D H K S s w C A G x F u T / g p G O A U P I 7 j X O M 5 b 5 Q C X b k c z F T B h R 9 n h c s 7 X Y U G J D f I v M s J L 8 g t 5 S R 4 G w c 3 s 3 X v L o O j Z 8 N s Z / j y w 8 5 g b 3 / l 6 y Z 7 z L b Z U 5 a x 5 2 y P v W G H b M Q 4 + 8 q + s e / s R + 9 X 1 I 8 e R A + X r V F v N X O f / R P R 9 m 8 q E r 4 B < / l a t e x i t >

L IRMEGC < l a t e x i t s h a 1 _ b a s e 6 4 = " w O 8 j N K W w g K o R K P P N K / m 0 U r a d / E 0 = " > A A A C a H i c b V D b a h R B E O 0 d L 4 n j b a M P o r 4 M W Q S f l h k J U d 9 C g q i g E M V N A j v j U t N T s 2 n S 3 T N 0 1 5 g s T f + J X + O r / o C / 4 F f Y e 0 F 0 Y 0 E 1 p 8 6 p o q p P 2 U p h K U 1 / 9 q I r V 6 9 d 3 9 i 8 E d + 8 d f v O 3 f 7 W v S P b d I b j i D e y M S c l W J R C 4 4 g E S T x p D Y I q J R 6 X Z w d z / f g L G i s a / Y l m L R Y K p l r U g g M F a t L f z V H b z q A C O n X 5 / O U g 3 T s / W R a 2 d h 9 f H X j v P / + p 3 7 5 / 7 f 2 k P 0 i H 6 S K S y y B b g Q F b x e F k q / c o r x r e K d T E J V g 7 z t K W C g e G B J f o 4 7 y z 2 A I / g y m O A 9 S g 0 B Z u 8 U G f P A l M l d S N C a k p W b B / T z h Q 1 s 5 U G T o X V 6 5 r c / K / W q n W N l P 9 o n B C t x 2 h 5 s v F d S c T a p K 5 e 0 k l D H K S s w C A G x F u T / g p G O A U P I 7 j X O M 5 b 5 Q C X b k c z F T B h R 9 n h c s 7 X Y U G J D f I v M s J L 8 g t 5 S R 4 G w c 3 s 3 X v L o O j Z 8 N s Z / j y w 8 5 g b 3 / l 6 y Z 7 z L b Z U 5 a x 5 2 y P v W G H b M Q 4 + 8 q + s e / s R + 9 X 1 I 8 e R A + X r V F v N X O f / R P R 9 m 8 q E r 4 B < / l a t e x i t >

:< l a t e x i t s h a 1 _ b a s e 6 4 = " d a k x W V t u 4 S R U o W 7 J D / G N M P B C x G Q = " > A A A C M n i c b V D L S h x B F K 3 W P E w b E x + 7 Z F M 4 C K 6 G b h F M A g E x m y x N y K g w 3 c j t 6 t t j Y V V 1 U 3 U 7 c S j 6 D 9 z q l + R n 4 k 7 c 5 i N S 8 1 g k Y w 5 c O J x z L v d y i k Z J R 0 n y K 1 p a f v L 0 2 f O V F / H q y 7 V X r 9 c 3 N k 9 c 3 V q B A 1 G r 2 p 4 V 4 F B J g w O S p P C s s Q i 6 U H h a X H 6 a + K f f 0 T p Z m 2 8 0 b j D X M D K y k g I o S F 8 / f D x f 7 y X 9 Z A r + m K R z 0 m N z H J 9 v R G + y s h a t R k N C g X P D N G k o 9 2 B J C o V d n L U O G x C X M M J h o A Y 0 u t x P X + 3 4 T l B K X t U 2 j C E + V f / e 8 K C d G + s i J D X Q h V v 0 J u J / v U I v X K b q X e 6 l a V p C I 2 a H q 1 Z x q v m k B 1 5 K i 4 L U O B A Q V o b f u b g A C 4 J C W 3 G c G f w h a q 3 B l D 4 D O 9 J w 1 Q 3 T 3 G e t K U M A y f f S z m e E V + R n N u + 6 L g 5 t p o v d P S Y n e / 1 0 v / / + y 3 7 v 8 G j e 6 w p 7 y 7 b Z L k v Z A T t k n 9 k x G z D B K n b N b t h t 9 D O 6 i + 6 j h 1 l 0 K Z r v b L F / E P 3 + A y t 3 q h 8 = < / l a t e x i t > =

max(0,

⌘

< l a t e x i t s h a 1 _ b a s e 6 4 = " 5 m S t 3 e t A d f e u B V i Q T w L c R D q 9 C w o = " > A A A C T n i c b V B N a x R B E O 1 Z v 9 b x I x u 9 q Y f G R d i D L j M S U G 9 B E b w I E d 0 k s D M s N T 0 1 m y b d P U N 3 T Z K l m Y u / x q v + E q / + E W + i v R 8 H 3 f i g 4 d V 7 V V T 1 K x o l H S X J j 6 h 3 5 e q 1 6 z f 6 N + N b t + / c 3 R n s 3 j t 0 d W s F T k S t a n t c g E M l D U 5 I k s L j x i L o Q u F R c f p m 6 R + d o X W y N p 9 o 0 W C u Y W 5 k J Q V Q k G a D R 5 m G i 1 H y l G d I w J / x U N K J q / z 7 j 2 + 7 0 W w w T M b J C v w y S T d k y D Y 4 m O 1 G D 7 K y F q 1 G Q 0 K B c 9 M 0 a S j 3 Y E k K h V 2 c t Q 4 b E K c w x 2 m g B j S 6 3 K + + 0 f E n Q S l 5 V d v w D P G V + v e E B + 3 c Q h e h c 3 X l t r c U / + s V e m s z V S 9 z L 0 3 T E h q x X l y 1 i l P N l x n x U l o U p B a B g L A y 3 M 7 F C V g Q F J K M 4 8 z g u a i 1 B l P 6 D O w 8 J N h N 0 9 x n r S l D A 5 I f p p 3 P C C / I r 2 3 e d V 0 c 0 k y 3 s 7 t M D p + P 0 7 3 x q w 9 7 w / 3 X m 1 z 7 7 C F 7 z E Y s Z S / Y P n v H D t i E C f a Z f W F f 2 b f o e / Q z + h X 9 X r f 2 o s 3 M f f Y P e v 0 / n t S z C w = = < / l a t e x i t >

MSE(

D
< l a t e x i t s h a 1 _ b a s e 6 4 = " P n R l C 2 n v G C 7 z 8 a P C a Y a e k V k f P V Y = " > A A A C O 3 i c b V B N a x R B E O 2 J i Y l j 1 E R v e m m y C D k t M x L Q 3 I J 6 8 J i A m w R 2 h q W m p 3 a 3 S X 8 M 3 T W a p Z m f 4 V V / i T / E s z f x m r u 9 H w e z y Y O C x 3 u v q O J V j Z K e s u x X s v F g c + v h 9 s 6 j 9 P H u k 6 f P 9 v a f n 3 v b O o E D Y Z V 1 l x V 4 V N L g g C Q p v G w c g q 4 U X l R X H + b + x R d 0 X l r z m W Y N l h o m R o 6 l A I r S 8 O M o F D R F g u 5 w t N f L + t k C / C 7 J V 6 T H V j g d 7 S c v i 9 q K V q M h o c D 7 Y Z 4 1 V A Z w J I X C L i 1 a j w 2 I K 5 j g M F I D G n 0 Z F j 9 3 / H V U a j 6 2 L o 4 h v l D / 3 w i g v Z / p K i Y 1 0 N S v e 3 P x X q / S a 5 d p / K 4 M 0 j Q t o R H L w + N W c b J 8 X g i v p U N B a h Y J C C f j 7 1 x M w Y G g W F u a F g a / C q s 1 m D o U 4 C Y a r r t h X o a i N X U M I I V e 3 s U S 8 Z r C 0 u Z d 1 6 W x z X y 9 u 7 v k / E 0 / P + o f n x 3 1 T t 6 v e t 1 h r 9 g B O 2 Q 5 e 8 t O 2 C d 2 y g Z M M M u + s e / s R / I z + Z 3 8 S f 4 u o x v J a u c F u 4 X k 5 h 9 N F K 4 3 < / l a t e x i t >

✓

(

),< l a t e x i t s h a 1 _ b a s e 6 4 = " T 7 A F b h o S s 7 N v t e k D g s U g 4 i 2 w b 7 o = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C B 7 C M i M L 0 Z v o x a N C V o W d Q W p 6 a t f G 7 p 6 h u y a 6 N P M L v C a / J L / G m 3 j N n 0 j v x 8 G s e V D w e O 8 V V b y i V t J R k j x G S 2 / e L r 9 b W V 2 L 3 6 9 / + L i x u b V 9 4 a r G C u y L S l X 2 q g C H S h r s k y S F V 7 V F 0 I X C y + L 2 Z O J f / k D r Z G W + 0 7 j G X M P I y K E U Q E E 6 / 3 q 9 2 U m 6 y R T 8 N U n n p M P m O L v e i j 5 l Z S U a j Y a E A u c G a V J T 7 s G S F A r b O G s c 1 i B u Y Y S D Q A 1 o d L m f f t r y 3 a C U f F j Z M I b 4 V H 2 5 4 U E 7 N 9 Z F S G q g G 7 f o T c T / e o V e u E z D g 9 x L U z e E R s w O D x v F q e K T G n g p L Q p S 4 0 B A W B l + 5 + I G L A g K Z c V x Z v B O V F q D K X 0 G d q T h v h 2 k u c 8 a U 4 Y A k u + k r c 8 I 7 8 n P b N 6 2 b R z a T B e 7 e 0 0 u 9 r t p r 3 t 4 3 u s c H c 9 7 X W W f 2 R e 2 x 1 L 2 j R 2 x U 3 b G + k w w Z A / s J / s V / Y 4 e o 6 f o e R Z d i u Y 7 O + w f R H / + A n 7 y q c o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

ResNet(
< l a t e x i t s h a 1 _ b a s e 6 4 = " j b d Y a I n A 2 p j i v k H x x I i 2 A O e 2 1 a 0 = " > A A A C P n i c b V B N S x x B E O 3 R m O j E J B p v 5 t J k E T w t M 0 G I 3 k Q v O Q U j W R V 2 R q n p q V 0 b u 3 u G 7 p r o 0 s z / 8 G p + i X / D P + A t 5 J q j v R + H Z M 2 D g s d 7 r 6 j i F b W S j p L k I V p Y f L H 0 8 t X y S v x 6 9 c 3 b d 2 v r 7 0 9 c 1 V i B P V G p y p 4 V 4 F B J g z 2 S p P C s t g i 6 U H h a X B 2 O / d M f a J 2 s z H c a 1 Z h r G B o 5 k A I o S O c Z 4 Q 3 5 Y 3 R f k d r t i 7 V O 0 k 0 m 4 M 9 J O i M d N s P R x X q 0 m Z W V a D Q a E g q c 6 6 d J T b k H S 1 I o b O O s c V i D u I I h 9 g M 1 o N H l f v J 2 y 7 e C U v J B Z c M Y 4 h P 1 7 w 0 P 2 r m R L k J S A 1 2 6 e W 8 s / t c r 9 N x l G u z m X p q 6 I T R i e n j Q K E 4 V H 3 f C S 2 l R k B o F A s L K 8 D s X l 2 B B U G g u j j O D 1 6 L S G k z p M 7 B D D T d t P 8 1 9 1 p g y B J B 8 J 2 3 9 t M q p z d u 2 j U O b 6 X x 3 z 8 n J p 2 6 6 0 9 3 7 t t P Z P 5 j 1 u s w + s I 9 s m 6 X s M 9 t n X 9 g R 6 z H B L L t l d + x n d B 8 9 R r + i 3 9 P o Q j T b 2 W D / I P r z B E H v r 7 I = < / l a t e x i t >

)
< l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

)
< l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

+< l a t e x i t s h a 1 _ b a s e 6 4 = " O 5 y u Q Y w j v H S f 3 L u 0 + 1 6 y + s g Y l c k = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C E J g m Z G F 6 E 3 0 4 l E h q 8 L O I D U 9 t W t j d 8 / Q X R N d m v k F X p N f k l / j T b z m T 6 T 3 4 2 D W P C h 4 v P e K K l 5 R K + k o S R 6 j p T d v l 9 + t r K 7 F 7 9 c / f N z Y 3 N q + c F V j B f Z F p S p 7 V Y B D J Q 3 2 S Z L C q 9 o i 6 E L h Z X F 7 M v E v f 6 B 1 s j L f a V x j r m F k 5 F A K o C C d f 7 3 e 7 C T d Z A r + m q R z 0 m F z n F 1 v R Z + y s h K N R k N C g X O D N K k p 9 2 B J C o V t n D U O a x C 3 M M J B o A Y 0 u t x P P 2 3 5 b l B K P q x s G E N 8 q r 7 c 8 K C d G + s i J D X Q j V v 0 J u J / v U I v X K b h Q e 6 l q R t C I 2 a H h 4 3 i V P F J D b y U F g W p c S A g r A y / c 3 E D F g S F s u I 4 M 3 g n K q 3 B l D 4 D O 9 J w 3 w 7 S 3 G e N K U M A y X f S 1 m e E 9 + R n N m / b N g 5 t p o v d v S Y X + 9 2 0 1 z 0 8 7 3 W O j u e 9 r r L P 7 A v b Y y n 7 x o 7 Y K T t j f S Y Y s g f 2 k / 2 K f k e P 0 V P 0 P I s u R f O d H f Y P o j 9 / A X 0 l q c k = < / l a t e x i t >

MS E(D ✓ < l a t e x i t s h a 1 _ b a s e 6 4 = " 7 w V N g U C n g X D d D R O j w b a v e C u 6 A 0 g = " > A A A C P X i c b V B N S x x B E O 0 x J p r J h 5 r c 4 q X J E v C 0 z A T B 5 C Y R I Z e A I V k V d g a p 6 a l Z G 7 t 7 x u 4 a 4 9 L M 7 / B q f k l + R 3 5 A b i F X r / Z + H H T N g 4 L H e 1 X U 4 x W N k o 6 S 5 H e 0 9 G j 5 8 Z O V 1 a f x s + c v X q 6 t b 7 w 6 d H V r B Q 5 E r W p 7 X I B D J Q 0 O S J L C 4 8 Y i 6 E L h U X G 2 N / G P L t A 6 W Z v v N G 4 w 1 z A y s p I C K E h 5 p o F O X e W / f N v v t k 7 W e 0 k / m Y I / J O m c 9 N g c B y c b 0 Z u s r E W r 0 Z B Q 4 N w w T R r K P V i S Q m E X Z 6 3 D B s Q Z j H A Y q A G N L v f T 1 B 1 / F 5 S S V 7 U N Y 4 h P 1 b s X H r R z Y 1 2 E z W n K R W 8 i / t c r 9 M J n q j 7 k X p q m J T R i 9 r h q F a e a T y r h p b Q o S I 0 D A W F l y M 7 F K V g Q F I q L 4 8 z g D 1 F r D a b 0 G d i R h s t u m O Y + a 0 0 Z F p B 8 L + 1 8 R n h J f m b z r u v i 0 G a 6 2 N 1 D c v i + n 2 7 3 P 3 7 d 7 u 1 + m v e 6 y j b Z W 7 b F U r b D d t l n d s A G T L B z d s W u 2 c / o V / Q n + h v 9 m 6 0 u R f O b 1 + w e o p t b 3 M G u + g = = < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " P n R l C 2 n v G C 7 z 8 a P C a Y a e k V k f P V Y = " > A A A C O 3 i c b V B N a x R B E O 2 J i Y l j 1 E R v e m m y C D k t M x L Q 3 I J 6 8 J i A m w R 2 h q W m p 3 a 3 S X 8 M 3 T W a p Z m f 4 V V / i T / E s z f x m r u 9 H w e z y Y O C x 3 u v q O J V j Z K e s u x X s v F g c + v h 9 s 6 j 9 P H u k 6 f P 9 v a f n 3 v b O o E D Y Z V 1 l x V 4 V N L g g C Q p v G w c g q 4 U X l R X H + b + x R d 0 X l r z m W Y N l h o m R o 6 l A I r S 8 O M o F D R F g u 5 w t N f L + t k C / C 7 J V 6 T H V j g d 7 S c v i 9 q K V q M h o c D 7 Y Z 4 1 V A Z w J I X C L i 1 a j w 2 I K 5 j g M F I D G n 0 Z F j 9 3 / H V U a j 6 2 L o 4 h v l D / 3 w i g v Z / p K i Y 1 0 N S v e 3 P x X q / S a 5 d p / K 4 M 0 j Q t o R H L w + N W c b J 8 X g i v p U N B a h Y J C C f j 7 1 x M w Y G g W F u a F g a / C q s 1 m D o U 4 C Y a r r t h X o a i N X U M I I V e 3 s U S 8 Z r C 0 u Z d 1 6 W x z X y 9 u 7 v k / E 0 / P + o f n x 3 1 T t 6 v e t 1 h r 9 g B O 2 Q 5 e 8 t O 2 C d 2 y g Z M M M u + s e / s R / I z + Z 3 8 S f 4 u o x v J a u c F u 4 X k 5 h 9 N F K 4 3 < / l a t e x i t >

(

),< l a t e x i t s h a 1 _ b a s e 6 4 = " T 7 A F b h o S s 7 N v t e k D g s U g 4 i 2 w b 7 o = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C B 7 C M i M L 0 Z v o x a N C V o W d Q W p 6 a t f G 7 p 6 h u y a 6 N P M L v C a / J L / G m 3 j N n 0 j v x 8 G s e V D w e O 8 V V b y i V t J R k j x G S 2 / e L r 9 b W V 2 L 3 6 9 / + L i x u b V 9 4 a r G C u y L S l X 2 q g C H S h r s k y S F V 7 V F 0 I X C y + L 2 Z O J f / k D r Z G W + 0 7 j G X M P I y K E U Q E E 6 / 3 q 9 2 U m 6 y R T 8 N U n n p M P m O L v e i j 5 l Z S U a j Y a E A u c G a V J T 7 s G S F A r b O G s c 1 i B u Y Y S D Q A 1 o d L m f f t r y 3 a C U f F j Z M I b 4 V H 2 5 4 U E 7 N 9 Z F S G q g G 7 f o T c T / e o V e u E z D g 9 x L U z e E R s w O D x v F q e K T G n g p L Q p S 4 0 B A W B l + 5 + I G L A g K Z c V x Z v B O V F q D K X 0 G d q T h v h 2 k u c 8 a U 4 Y A k u + k r c 8 I 7 8 n P b N 6 2 b R z a T B e 7 e 0 0 u 9 r t p r 3 t 4 3 u s c H c 9 7 X W W f 2 R e 2 x 1 L 2 j R 2 x U 3 b G + k w w Z A / s J / s V / Y 4 e o 6 f o e R Z d i u Y 7 O + w f R H / + A n 7 y q c o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

ResNet(
< l a t e x i t s h a 1 _ b a s e 6 4 = " j b d Y a I n A 2 p j i v k H x x I i 2 A O e 2 1 a 0 = " > A A A C P n i c b V B N S x x B E O 3 R m O j E J B p v 5 t J k E T w t M 0 G I 3 k Q v O Q U j W R V 2 R q n p q V 0 b u 3 u G 7 p r o 0 s z / 8 G p + i X / D P + A t 5 J q j v R + H Z M 2 D g s d 7 r 6 j i F b W S j p L k I V p Y f L H 0 8 t X y S v x 6 9 c 3 b d 2 v r 7 0 9 c 1 V i B P V G p y p 4 V 4 F B J g z 2 S p P C s t g i 6 U H h a X B 2 O / d M f a J 2 s z H c a 1 Z h r G B o 5 k A I o S O c Z 4 Q 3 5 Y 3 R f k d r t i 7 V O 0 k 0 m 4 M 9 J O i M d N s P R x X q 0 m Z W V a D Q a E g q c 6 6 d J T b k H S 1 I o b O O s c V i D u I I h 9 g M 1 o N H l f v J 2 y 7 e C U v J B Z c M Y 4 h P 1 7 w 0 P 2 r m R L k J S A 1 2 6 e W 8 s / t c r 9 N x l G u z m X p q 6 I T R i e n j Q K E 4 V H 3 f C S 2 l R k B o F A s L K 8 D s X l 2 B B U G g u j j O D 1 6 L S G k z p M 7 B D D T d t P 8 1 9 1 p g y B J B 8 J 2 3 9 t M q p z d u 2 j U O b 6 X x 3 z 8 n J p 2 6 6 0 9 3 7 t t P Z P 5 j 1 u s w + s I 9 s m 6 X s M 9 t n X 9 g R 6 z H B L L t l d + x n d B 8 9 R r + i 3 9 P o Q j T b 2 W D / I P r z B E H v r 7 I = < / l a t e x i t >

(a) Imagination module

)))

< l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " G Z l F Y J V 2 U k s t 5 D / Q n V 9 T U X s Z X t Q = " > A A A C M X i c b V B N S x x B E O 0 x M e r E x K 9 b c m m y C O a y z M h C 9 C Z 6 8 a i Q V W F n k J q e 2 r W x u 2 f o r o k u z f w C r 8 k v y a / x J l 7 z J 9 L 7 c T B r H h Q 8 3 n t F F a + o l X S U J I / R 0 p u 3 y + 9 W V t f i 9 + s f P m 5 s b m 1 f u K q x A v u i U p W 9 K s C h k g b 7 J E n h V W 0 R d K H w s r g 9 m f i X P 9 A 6 W Z n v N K 4 x 1 z A y c i g F U J D O v 1 5 v d p J u M g V / T d I 5 6 b A 5 z q 6 3 o k 9 Z W Y l G o y G h w L l B m t S U e 7 A k h c I 2 z h q H N Y h b G O E g U A M a X e 6 n n 7 Z 8 N y g l H 1 Y 2 j C E + V V 9 u e N D O j X U R k h r o x i 1 6 E / G / X q E X L t P w I P f S 1 A 2 h E b P D w 0 Z x q v i k B l 5 K i 4 L U O B A Q V o b f u b g B C 4 J C W X G c G b w T l d Z g S p + B H W m 4 b w d p 7 r P G l C G A 5 D t p 6 z P C e / I z m 7 d t G 4 c 2 0 8 X u X p O L / W 7 a 6 x 6 e 9 z p H x / N e V 9 l n 9 o X t s Z R 9 Y 0 f s l J 2 x P h M M 2 Q P 7 y X 5 F v 6 P H 6 C l 6 n k W X o v n O D v s H 0 Z + / e Y u p x w = = < / l a t e x i t >

(b) Oracle model

(c) Guesser model

Figure 2: Imagination-based Representation Learning: Given the perceptual information vi of object
oi, we learn an imagination embedding zi generated by Encoder Eφ. The latent code is optimized to
reconstruct the original visual representation vi (the “donut” ResNet encoding) via the reconstruction loss LIRMEGC using the Decoder Dθ. Figures 2(b) and 2(c) show how the imagination embedding z replaces the category embedding c in the Oracle model from De Vries et al. (2017) and Guesser model from
Shekhar et al. (2019) respectively, and is concatenated to the spatial information si.

sampling (e.g., an isotropic Gaussian prior). VAEs which need to have such a ﬁxed prior, instead, are deemed to learn embeddings that are less informative w.r.t. objects, categories, and context information.

Module architecture. Figure 2(a) summarizes our imagination module. Its aim is to distill a context and category-aware embedding zi ∈ RdZ per object oi in scene S. To this end, we adopt an encoder Eφ
parameterized by φ that maps a perceptual embedding vi of object oi to its imagined counterpart zi, i.e., Eφ(vi) = zi. A decoder Dθ realizes the inverse mapping v˜i = Dθ(zi), with v˜i ∈ RdO being also called the reconstruction of the input vi. As in RAEs, our per-object loss LIMG comprises a reconstruction loss (LREC), weighting how good the reconstructions of Dθ are w.r.t. the encoded representations by Eφ, and a regularization term (LREG) enhancing generalization by smoothing the decoder Dθ. This leads to the

following composite loss:

LIMG = LREC + αLREG,

(1)

where α is an hyperparameter controlling regularization.2 As in L2-RAE (Ghosh et al., 2019), the regularization component is deﬁned as LREG := ||zi|| + ||θ||2: the ﬁrst term bounds the latent embedding space learned by Eφ easing optimization; the second enforces smoothing over Dθ improving generaliza-
tion over regions of the latent space that are unseen during training.
Differently from RAEs, we devise a speciﬁc reconstruction loss tailored to learn contextual and
category-aware representations. In conventional RAEs, in fact, the reconstruction loss is deﬁned as the Mean Squared Error (MSE) representing the distance between vi and its reconstruction v˜i, so that LRRAECE := MSE(vi, v˜i). This loss is purely unsupervised and as such agnostic to object categories or to the scene context. To our aims, we deﬁne a custom imagination reconstruction loss LIRMEGC as an instance of a max-margin triplet-loss (Wang et al., 2014; Schroff et al., 2015), as follows. Let ci be the category

2Ghosh et al. (2019) use two different hyperparameters for the two terms in LREG Optimizing them independently had no evident beneﬁt in our experiments, hence we simply treat them as a single regularizer together.

of object oi with perceptual embedding vi in scene S and let O¬ci = {oj | oj ∈ O ∧ cj = ci} be the set of all objects in S belonging to a different category than ci. Our per-object LIRMEGC term is deﬁned as:

LIRMEGC := max(0, η − MSE(vi, Dθ(zi)) + MSE(vj, Dθ(zi))),

(2)

where η is the minimum margin between two components: i) the distance between the perceptual embedding vi and its reconstruction Dθ(zi), and ii) the distance between the perceptual embedding vj of a randomly sampled object oj ∈ O¬ci and the reconstruction Dθ(zi). By doing so, we enforce each object representation to be representative of its category given a speciﬁc context by locally contrasting
it to another object of a different category in the same scene. Note that this is strikingly different from
previous approaches employing a max-margin loss (Elliott and Ka´da´r, 2017; Kiros et al., 2018) where
“negative” objects are arbitrarily sampled from other scenes in the same batch.

Imagining at inference time. Differently from the category embeddings c employed by all previous work, our imagination embeddings z do not depend on gold category labels at inference time, while still being context-aware and category-aware. In fact, once parameters φ have been learned, the encoder Eφ contains all the information needed to distill embeddings z independently of LIMG, which is necessary only at training time. We consider imagination the ability of the model of generating latent representa-
tions on-the-ﬂy. Therefore, for both Guesser and Oracle models we consider an object representation for object oi that replaces ci with zi and concatenates it with its spatial information si (see Figures 2(b) and 2(c) and Appendix A.1 for details). By doing so, we consider every gameplay situated in a reference
scene as an experience where our imagination module is able to derive a latent conceptual representation simply by “looking” at objects, realizing a perceptual simulator (Barsalou, 2008). We plan to investigate how to combine label-dependent category embeddings c with our imagination embeddings z, similarly to how some VAE variants tackle semi-supervised classiﬁcation scenarios (Kingma et al., 2014).

4 Experimental Investigation

To assess the impact of using the imagination embeddings against the category embeddings, we use two evaluation benchmarks: GuessWhat?! and CompGuessWhat?!. More information about the training procedure can be found in Appendix A.2.

4.1 GuessWhat?! Evaluation
In this experiment, we evaluate the accuracy of the Oracle in answering questions and the accuracy of the Guesser in selecting the target object. We consider as both training and evaluation data all the gold dialogues (and questions) that have been labeled as successful in the dataset (De Vries et al., 2017). We want to highlight that in this evaluation phase, the models using label-aware object encodings have gold information both at training and test time. This is true both for the Oracle and Guesser models. However, this does not hold for all other models using the imagination component.

4.1.1 Experimental Setup
Oracle task. We evaluate the imagination-based Oracle and compare it to several combinations of the following baselines with and without category embeddings from De Vries et al. (2017): 1) MAJORITY: majority classiﬁer; 2) QUESTION: uses only the question; 3) IMAGE: uses only the image representation; 4) CROP: uses only the crop representation of the target object.

Guesser task. Similarly, we compare the GDSE model using imagination embeddings (GDSE+IMAGINATION) with the following label-aware baselines: 1) text-only baselines using LSTM encoder (LSTM) and Hierarchical Recurrent Encoder-Decoder architecture (Serban et al., 2017) (HRED) as well as their corresponding multi-modal models LSTM+IMAGE and HRED+IMAGE; 2) PARALLELATTENTION (Zhuang et al., 2018) and GDSE (Shekhar et al., 2019). We also compare with variants of the above that do not use any category embeddings or gold category labels (*-NOCAT), as well as models with predicted category labels (*-PREDCAT).3
3We train an object classiﬁer using as input the ResNet-101 features generated for the object crop. It achieves 65% accuracy evaluated on all objects in the GuessWhat?! test set.

PERCEPTUAL INFORMATION MODEL (DV-QUES+SPATIAL) LOCATION SHAPE COLOR TEXTURE

SIZE

+ CROP + CATEGORY + CATEGORY + CROP + IMAGINATION

66.86% 67.48% 65.27% 68.62%

69.08% 68.42% 60.34% 69.08%

67.25% 61.83% 59.14% 67.64%

68.30% 70.08% 65.76% 69.86%

65.09% 60.14% 59.08% 62.65%

CATEGORICAL INFORMATION

SUPER CATEGORY

OBJECT

88.94% 97.09% 96.19% 90.05%

80.48% 88.82% 86.32% 82.32%

Table 2: Oracle accuracy grouped by question type for the best Oracle model with category information (DV-QUES+SPATIAL) and for multi-modal variants using either perceptual or categorical information.

4.1.2 Results

Oracle task. In Table 1, we divide conﬁg-

MODEL

VAL

TEST

urations into category-aware (De Vries et al., 2017) and multi-modal. The model reference for several other publications on GuessWhat?! is a category-aware model QUESTION+SPATIAL+CATEGORY. However, by relying on symbolic information in the form of category labels, it is inevitably not truly multimodal anymore because the heavy-lifting is

W/ CAT BASE

MAJORITY QUES IMG CROP

53.80% 58.30% 53.30% 57.30%

DV-QUES+CAT DV-QUES+CROP+CAT DV-QUES+SPATIAL+CAT DV-QUES+SPATIAL+CROP+CAT DV-QUES+SPATIAL+IMG+CAT

74.20% 75.60% 78.90% 78.30% 76.80%

49.10% 58.80% 53.30% 57.00%
74.30% 75.30% 78.50% 77.90% 76.50%

done by these embeddings. As shown in the results, other multi-modal models such as QUESTION+SPATIAL+CROP and QUESTION+CROP, are not able to learn effective representations to bridge the gap between category-aware and

MM

DV-QUES+CROP DV-QUES+IMG DV-QUES+SPATIAL DV-QUES+SPATIAL+CROP DV-QUES+SPATIAL+CROP+IMG IMAGINATION

70.90% 59.80% 68.80% 74.00% 72.30% 75.78%

70.80% 60.20% 68.70% 73.80% 72.10% 75.88%

category-free models. On the other hand, the proposed imagination model is able to reduce this gap without relying on gold information as input. Indeed, we are able to learn categoryaware and context-aware latent codes by using category information only in our loss function.
We investigate this argument further by using

Table 1: Oracle results on gold questions: we compare the IMAGINATION Oracle model to models from De Vries et al. (2017) (DV-*). We group them into models relying on gold category labels (W/ CAT) and models that only use multi-modal perceptual information (MM).

a rule-based question classiﬁer (Shekhar et al., 2019) to partition the test questions according to their

type. Table 2 summarizes this analysis; we include models considered truly multi-modal and the best

Oracle model QUESTION+SPATIAL+CATEGORY. The latter can answer with high accuracy questions

about speciﬁc object instances (e.g., “is it the dog?”) or super-categories (e.g., “is it an animal?”) since it

is using category embeddings as input. However, when it comes to answering questions about perceptual

properties of the target object, it loses some accuracy points because the perceptual information is missing

from the category embedding representing a centroid of typical instances seen at training time only. On

the other hand, the IMAGINATION model is able to bring improvements of 1.34%, 5.81%, and 2.52% for

location, color, and shape questions, respectively. On questions related to perceptual information, models

using crop information seem to be on par with the IMAGINATION model. However, our model is able

to obtain an improvement over +CROP of 1.84% in object questions and of 1.11% on super category

questions solely by relying on the imagination embeddings.

Guesser task. Table 3 compares several category-aware and multi-modal models; PARALLELATTENTION and GDSE-SL are the two best performing conﬁgurations. However, when PARALLELATTENTION does not have access to category information (PARALLELATTENTION-NOCAT) its performance drops by 3.7% (also noted by Zhuang et al. (2018)). We conﬁrmed the same behavior for GDSE-SL as well (GDSE-SL-NOCAT), noticing a more signiﬁcant drop in performance of 16.95% which is in line with the simpler LSTM+IMAGE model. On the other hand, GDSE-SL with our imagination component (GDSE-SL+IMAGINATION), performs comparably with the category-aware model and better then all

Gameplay ACCURACY

RANDOM

15.81%

DEVRIES-SL DEVRIES-RL

41.5% 53.5%

GDSE-SL GDSE-CL

49.1% 59.8%

GDSE-SL+IMAGINATION 43.82% GDSE-CL+IMAGINATION 51.98%

Attribute Prediction A-F1 S-F1 AS-F1 L-F1

15.1 0.1

7.8

2.8

46.8 39.1 48.5 42.7 45.2 38.9 47.2 42.5

59.9 47.6 60.1 48.3 59.5 47.6 59.8 48.1

56.23 47.37 57.59 47.6

57.2 58.31

51.73 50.42

Zero-shot Gameplay ND-ACC OD-ACC

16.9% 18.6%

31.3% 43.9%

28.4% 38.7%

29.8% 43.4%

22.3% 29.8%

39.19% 46.56%

39.90% 46.96%

GROLLA
13.3
38.5 46.2
43.0 50.1
45.50 50.74

Table 4: Results for the CompGuessWhat?! benchmark (Suglia et al., 2020). We assess model quality in terms of gameplay accuracy, attribute prediction quality, measured in terms of F1 for the abstract (A-F1), situated (S-F1), abstract+situated (AS-F1) and location (L-F1) prediction scenario, as well as zero-shot learning gameplay. GROLLA is a macro-average of the individual scores.

multi-modal models. Therefore we argue that it is possible to learn object representations that, given a representation for the current dialogue state, allow for discriminating the target object among other candidates without relying on symbolic information.

4.2 CompGuessWhat?! Evaluation
CompGuessWhat?! is a benchmark proposed to assess the quality of models’ representations and out-of-domain generalization. It includes the following tasks: a) in-domain gameplay accuracy, – selecting the target object with model generated dialogues as input, b) attribute prediction task – assessing the ability of the dialogue representation to recover target object attributes, and c) zero-shot gameplay accuracy – selecting the target object among objects belonging to categories never seen by the model during training. In contrast to GuessWhat?!, the attribute prediction and zero-shot tasks give us more insights about the quality of the learned representations and the model’s generalization ability.

CATEGORY

MODEL
HUMAN RANDOM
LSTM HRED LSTM+IMAGE HRED+IMAGE PARALLELATTENTION GDSE-SL GDSE-SL-PREDCAT
LSTM+IMAGE-NOCAT PARALLELATTENTION-NOCAT GDSE-SL-NOCAT GDSE-SL-IMAGINATION

VAL
90.80% 17.10%
62.10% 61.80% 61.50% 61.60% 63.80% 63.14% 52.08%
50.10% 55.70% 46.11% 59.54%

TEST
90.80% 17.10%
61.30% 61.00% 60.50% 60.40% 63.40% 62.96% 51.00%
48.60% 59.70% 46.01% 58.90%

MM

Table 3: Guesser accuracy on successful gold dialogues: we compare GDSE-SL-IMAGINATION with i) models that are truly multi-modal (MM) and ii) use category information (CATEGORY).

4.2.1 Experimental Setup
We compare imagination-based models with baselines used in Suglia et al. (2020): 1) RANDOM: randomly selects an object; 2) DEVRIES-SL: presented in De Vries et al. (2017) trained using Supervised Learning; 3) DEVRIES-RL: DEVRIES-SL with Questioner ﬁne-tuned using Reinforcement Learning (Strub et al., 2017); and where 4) GDSE-SL and 5) GDSE-CL are the same as used in Section 4.1.

4.2.2 Results
In-domain gameplay. Table 4 presents the results on the CompGuessWhat?! benchmark. Models are tasked to play the game by generating up to 10 questions and corresponding answers. Firstly, we note that the results for GDSE-CL+IMAGINATION—the collaborative version of the model with Imagination— is still in the same ballpark of more complex models, such as DEVRIES-RL that is using category embeddings as input. At the same time, we notice that overall both imagination models perform worse than the GDSE-* models. We impute this drop to the introduction of additional loss terms that probably have changed the training dynamic of a cumbersome modulo-n multi-task training (Shekhar et al., 2019). This downside calls for a more principled way of handling tasks of different complexity (i.e., question generation and target prediction) in a multi-task learning system; we leave this for future work.

Attribute prediction. Table 4 reports the attribute prediction task results. In this scenario, we underline the fact that the dialogue state representation generated by the Guesser model is used to recover several types of attributes associated with the target object. In this work, we use the same dialogue state representation as used by Shekhar et al. (2019) and only focus on improving the object representations using the imagination component. Indeed, the best imagination model GDSE-SL+IMAGINATION is in line with GDSE-SL, currently the best model in terms of attribute prediction. In particular, even though the dialogue state representation is only indirectly affected by the imagination embeddings (via a dotproduct operation to score the candidate objects), we can still see an improvement in terms of F1 for Location attributes (L-F1) and similar performance for Situated attribute prediction (S-F1). Both can be considered, to some extent, a result of better situated object representations.
Zero-shot gameplay. As underlined in Section 3, the imagination module’s main strength is to be able to distill imagination embeddings from perceptual information only, without relying on externally provided category labels. The zero-shot gameplay scenario from CompGuessWhat?! (Table 4) sheds some light on the ability of the model to generalize to out-of-distribution examples. In the out-of-domain gameplay scenario where candidate objects belonging to categories never seen before are present, both imagination-based models GDSE-SL+IMAGINATION and GDSE-CL+IMAGINATION outperform the previous best performing system DEVRIES-RL by 1.2% and 8.26%, respectively in terms of OD accuracy (OD-ACC). By analyzing their output, we notice that the best imagination model achieves higher accuracy by learning a better gameplay strategy involving half the amount of location questions generated by DEVRIES-RL (39.68% vs 75.84%; see Appendix A.3 for more details). A further improvement in the near-domain scenario (ND-ACC) conﬁrms the effectiveness of the imagination component to generate category embeddings for objects on-the-ﬂy using only perceptual information.
Out-of-domain error analysis. Lastly, we report an error analysis comprising 50 dialogues selected at random from out-of-domain games (for more details refer to Appendix A.3). First, we manually annotated the Oracle answers and partitioned them according to their type using the same question classiﬁer used for the Oracle Task (Section 4.1.2). 83% of super-category questions (from a total of 80) were correctly answered by the model and 63.36% color related questions (from a total of 88) were correctly answered. For instance, as shown in Figure 3, GDSE-CL is not able to answer correctly the question “is it a person?” because it does not have category information for the label “girl” but only for the label

GDSE-CL+imagination

is it a vase?

yes

is it the one on the left? yes

is the whole thing? yes

is it on the left?

yes

is the tallest bottle?

no

is it between the two yes bottles and the pot?
is it between the two no bottles and the pot in the
middle? is it next to the blue cup? yes

status: failed

GDSE-CL

is it a cup?

no

is it a bottle?

no

is it a knife?

no

is the object is in the

no

background?

is the handle holding the no item?

is the the glass thing near no the corner of the photo?

is the the back part of it

no

visible?

is it the furthest back?

no

is it the furthest right?

no

is it the left most corner? no

status: failed

GDSE-CL+imagination

is it a person?

yes

is it the kid on the bike? yes

is it the whole person? yes

is it wearing an helmet? yes

status: success

GDSE-CL

is it a person?

no

is it a bike?

no

is it the green bag?

no

is the the bike?

yes

is it the bike in the back? no is the the bike wheel to no
the right? is it the bike to the left of yes
the bike? is it the whole bike? yes

status: failed

GDSE-CL+imagination

is it an animal?

yes

is it the one in the

yes

middle?

is it the whole animal? yes

is it the one whose head yes is visible?

is it the whole animal? yes

is it the entire animal? yes is the head facing to the yes left side of the picture? is the head facing to the yes
left?
status: success

GDSE-CL

Is it a giraﬀe?

no

Is it a car?

no

Is it the wood plank in the no background

is the animal the animal yes sitting on?

is the object the animal on no the brown animal?

is it the black jacket with no the brown handle?

is the animal's head

no

visible?

is the object brown?

no

is the handle black?

no

status: failed

Figure 3: Qualitative examples in the zero-shot gameplay scenario: the categories ’girl’ and ’antelope’ are not present in MSCOCO and therefore cannot be encoded by the GDSE-CL model. On the other hand, the imagination model is able to distill imagination embeddings by using the crop features only (for the sake of presentation quality we remove consecutive repeated questions).

“person”. On the other hand, GDSE-CL+IMAGINATION is able to a) categorize the object as a member of the super-category “person”, and b) correctly ground the expression “kid on the bike” to the target object. The same behavior can be observed when the “antelope” is the target object. Antelopes are not part of the MSCOCO classes, and therefore have not been seen by the model during training. First, the model refers to it as “animal”, hence the Oracle is able to correctly answer the question even though “antelope” was never involved in the training. Secondly, we found that the number of N o answers for GDSE-CL is considerably higher (88.06%) than GDSE-CL+IMAGINATION (51.02%), validating our hypothesis that the Oracle does not know how to deal with unseen instances. Finally, in the imagination dialogue of the ﬁrst example, even though the generated question/answers were probably referring to the correct object, the Guesser model is eventually unable to guess correctly. More work is required to better fuse the language modality and the object representations to improve its performance.
5 Related Work
Concerning unsupervised learning of concept representations, Bruni et al. (2014) ﬁrst learn modalityspeciﬁc representations and then fuse them into a uniﬁed representation for each concept. However, they rely on hand-crafted bags of visual features, making the approach laborious to extend to new domains and games. Kiela et al. (2018) cope with this issue by relying on CNN models to extract latent features from images for instances of speciﬁc objects. Lazaridou et al. (2015) use a margin loss but in the context of maximizing the similarity between the visual representation of a noun phrase and its corresponding text representation. Similarly, Collell et al. (2017) learn a mapping between the ResNet features and the word embeddings of a concept. As discussed in Section 2, unlike our imagination embeddings, these purelyperceptual representations are neither category-aware nor context-aware. Silberer et al. (2016) present a multi-modal model that uses a denoising auto-encoder framework. Unlike us, they do not use perceptual information as input but rely on an attribute-based representation derived from an additional attribute predictor. However, they do use a reconstruction loss (cross-entropy loss for attribute prediction) and an auxiliary category loss during training. Their training scheme is more complex as they ﬁrst separately train the AE for each modality and then fuse them, which we avoid by adopting a single end-to-end architecture. Ebert and Pavlick (2019) used VAEs to learn grounded representations for lexical concepts. However, as discussed in Section 3, VAEs are not as well suited as RAEs to representation learning for our imagination module. In the context of guessing games, all the previous approaches rely on categories embeddings (De Vries et al., 2017; Shekhar et al., 2019; Strub et al., 2017; Zhuang et al., 2018; Shukla et al., 2019) (see Section 2). Our imagination component can be ﬂexibly integrated in any of them by replacing the category embeddings with imagination embeddings.
6 Conclusions
We argued that existing models for learning grounded conceptual representations fail to learn compositional and generalizable multi-modal representations, relying instead on the use of category labels for every object in the scene both at training and inference time (De Vries et al., 2017). To address this, we introduced a novel “imagination” module based on Regularized Auto-Encoders, that learns a context-aware and category-aware latent embedding for every object directly from its image crop, without using category labels. We showed state-of-the-art performance in the CompGuessWhat?! zeroshot scenario (Suglia et al., 2020), outperforming current models by 8.26% in gameplay accuracy while performing comparably on the other tasks to models which use category labels at training time. The imagination-based model also shows improvements of 2.08% and 12.86% in Oracle and Guesser accuracy. Finally, we conducted an extensive error analysis and showed that imagination embeddings help to reason about object visual properties and attributes. For future work, we plan to 1) integrate category labels at training time in a more principled way following advances in semi-supervised learning (Kingma et al., 2014); 2) improve the multi-task learning procedure presented in (Shekhar et al., 2019) to optimize at the same time multiple tasks of different complexities.

References
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077–6086.
Lawrence W Barsalou. 2008. Grounded cognition. Annu. Rev. Psychol., 59:617–645.
Lisa Beinborn, Teresa Botschen, and Iryna Gurevych. 2018. Multimodal grounding for language processing. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2325–2339.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828.
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. 2020. Experience grounds language. arXiv preprint arXiv:2004.10151.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of artiﬁcial intelligence research, 49:1–47.
Guillem Collell, Ted Zhang, and Marie-Francine Moens. 2017. Imagined visual representations as multimodal embeddings. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton. 2013. Improving deep neural networks for lvcsr using rectiﬁed linear units and dropout. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8609–8613. IEEE.
Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. 2017. Guesswhat?! visual object discovery through multi-modal dialogue. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5503–5512.
Dylan Ebert and Ellie Pavlick. 2019. Using grounded word representations to study theories of lexical concepts. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 160–169, Minneapolis, Minnesota, June. Association for Computational Linguistics.
Desmond Elliott and A´ kos Ka´da´r. 2017. Imagination improves multimodal translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 130– 141.
Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scho¨lkopf. 2019. From variational to deterministic autoencoders. arXiv preprint arXiv:1903.12436.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680.
Douwe Kiela and Stephen Clark. 2015. Multi-and cross-modal semantics beyond vision: Grounding in auditory perception. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2461–2470.
Douwe Kiela, Luana Bulat, and Stephen Clark. 2015. Grounding semantics in olfactory perception. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 231–236.
Douwe Kiela, Alexis Conneau, Allan Jabri, and Maximilian Nickel. 2018. Learning visually grounded sentence representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 408–418.
Gary King and Langche Zeng. 2001. Logistic regression in rare events data. Political analysis, 9(2):137–163.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. 2014. Semi-supervised learning with deep generative models. In Advances in neural information processing systems, pages 3581–3589.

Jamie Kiros, William Chan, and Geoffrey Hinton. 2018. Illustrative language understanding: Large-scale visual grounding with image search. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 922–933.
Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
Stephen Laurence and Eric Margolis. 1999. Concepts and cognitive science. Concepts: core readings, 3:81.
Angeliki Lazaridou, Marco Baroni, et al. 2015. Combining language and vision with a multimodal skip-gram model. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 153–163.
Ken McRae, George S Cree, Mark S Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior research methods, 37(4):547–559.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A uniﬁed embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815–823.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.
Ravi Shekhar, Aashish Venkatesh, Tim Baumga¨rtner, Elia Bruni, Barbara Plank, Raffaella Bernardi, and Raquel Ferna´ndez. 2019. Beyond task success: A closer look at jointly learning to see, ask, and guesswhat. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2578–2587.
Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, and William Yang Wang. 2019. What should i ask? using conversationally informative rewards for goal-oriented visual dialog. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6442–6451.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2016. Visually grounded meaning representations. IEEE transactions on pattern analysis and machine intelligence, 39(11):2284–2297.
Luc Steels. 2015. The Talking Heads experiment: Origins of words and meanings, volume 1. Language Science Press.
Florian Strub, Harm De Vries, Jeremie Mary, Bilal Piot, Aaron Courvile, and Olivier Pietquin. 2017. End-to-end optimization of goal-driven and visually grounded dialogue systems. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, pages 2765–2771.
Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, and Oliver Lemon. 2020. CompGuessWhat?!: A multi-task evaluation framework for grounded language learning. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7625–7641, Online, July. Association for Computational Linguistics.
Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. 2014. Learning ﬁne-grained image similarity with deep ranking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1386–1393.
Ludwig Wittgenstein, Gertrude Elizabeth Margaret Anscombe, and Rush Rhees. 1953. Philosophische Untersuchungen.(Philosophical investigations). Basil Blackwell.
Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton Van Den Hengel. 2018. Parallel attention: A uniﬁed framework for visual object discovery through dialogs and queries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4252–4261.

A Appendix
A.1 Model details
As described in Section 3 of the main paper, we extend both the Oracle and Guesser model with an imagination component. For both roles, we keep the same model structure for the imagination component. In this paper we implement Eφ as a 2-layer feed-forward neural network with ReLU (Dahl et al., 2013) activation function. We acknowledge that many other implementations are possible in this case and we leave more complex designs for future work. Given the latent code zi generated by the function Eφ, we use a decoder Dθ to generate the reconstructed perceptual input (imagined) of the object oi, Dθ(zi) = v˜i. As common practice, we deﬁne the decoder Dθ as symmetric to the architecture of the encoder Eφ. For the category embeddings size dc, as in (Shekhar et al., 2019), we use 256 and 512 for the Oracle and Guesser respectively. For the imagination component, we run a grid search involving several parameters for the latent code z such as (16, 32, 64, 128, 256, 512). For both roles, we choose 512 because it was the value that lead to the highest accuracy on the validation set. We also experimented with several values for the coefﬁcient α of the regularization term LREG: (1e-3, 1e-5, 1e-6, 1e-7). For the Oracle the best value resulted to be 1e − 7, while 1e − 5 for the Guesser. When training the imagination component with the object category loss, due to the class imbalance, we apply loss weighting. We compute the class weights using the method reported in (King and Zeng, 2001). For the margin value η we opted for 1.0 after experimenting with a less effective dynamic margin that would change depending on the distance between the concepts in the WordNet hierarchy.
A.2 Training details
For both roles, we train the models using the Adam optimizer (Kingma and Ba, 2014). For the Oracle and Guesser training we use 0.0001 as learning rate. In both cases, we use the original GuessWhat?! validation set to select the best model that is used in the evaluation on the test set. As described in (Shekhar et al., 2019), we use a modulo-n training procedure to jointly optimize both the Guesser and Questioner. In our experimental evaluation we run a grid search of several values of n such as 3, 5, 7. We selected 5 as the best performing value on the validation set. For a fair comparison with all the GDSE model variants trained with Supervised Learning and Collaborative Learning, we made the same architectural choices and hyperparameters values. Please refer to the original codebase implementation available on GitHub 4. Another point of difference is in the Collaborative Learning ﬁne-tuning phase for the Guesser model. During this phase, only the Questioner and Guesser models are ﬁne-tuned whereas the Oracle model is ﬁxed (Shekhar et al., 2019) therefore, we decided to use the best performing Oracle so that the Guesser model is not negatively affected by a less performing Oracle and also to be comparable with the original implementation.
A.3 Error analysis
In order to provide a more ﬁne-grained evaluation of the generated dialogues, we adapt the quality evaluation script presented by Suglia et al. (2020) and extend it with additional metrics. First of all, it relies on a rule-based question classiﬁer that classiﬁes a given question in one of seven classes: 1) supercategory (e.g., “person”, “utensil”, etc.), 2) inanimate object (e.g., “car”, “oven”, etc.), 3) animate object (e.g., “dog”, “cat”, etc.), 3) “color”, 4) “size”, 5) “texture”, 6) “shape” and “location”. The question classiﬁer is useful to evaluate the dialogue strategy learned by the models. In particular, we look at two types of turn transitions: 1) super-category → object/attr, it measures how many times a question with an afﬁrmative answer from the Oracle related to a super-category is followed by either an object or attribute question (where “attribute” represents the set {color, size, texture, shape and location}; 2) object → attr, it measures how many times a question with an afﬁrmative answer from the Oracle related to an object is followed by either an object or attribute question. We compute the lexical diversity as the type/token ratio among all games, question diversity and the percentage of games with repeated questions. We also evaluate the percentage of dialogue turns involving location questions. Table 5 and 6 show the results of these analysis for the models GDSE-CL and GDSE-CL+imagination analyzed in this paper.
4https://github.com/shekharRavi/Beyond-Task-Success-NAACL2019

Using the above-mentioned question classiﬁer, we completed an error analysis trying to understand the quality of the generated gameplay in a zero-shot scenario from the point of view of the answers prediction performance and the guesser accuracy. In particular, we randomly sampled a pool of 50 reference games from the out-of-domain zero-shot scenario and we manually annotated whether a given answer generated by the Oracle model was correct or not. Table 7 shows the results of the manual annotation step. The model conﬁrms high performance in answering questions about super-category information demonstrating that it is able to correctly categories objects in macro-categories even though is has not seen them before.
A.3.1 Zero-shot gameplay quality

Model
DeVries-RL GDSE-CL GDSE-CL + Imagination

Lexical diversity
0.13 0.17 0.10

Question diversity
1.77 13.74 8.56

% games repeated questions
99.48 66.75 91.80

Super-cat -> obj/attr
97.39 93.62 93.15

Object -> attribute
98.70 66.27 60.72

% turns location questions
78.07 31.23 39.90

Vocab. size
702.00 1260 808

Accuracy
43.92% 43.42% 46.70%

Table 5: Comparison between the quality of gameplay in the near-domain zero-shot scenario between GDSE-CL and GDSE-CL with imagination. Number of total turns 10.

Model
DeVries-RL GDSE-CL GDSE-CL + Imagination

Lexical diversity
0.24 0.14 0.10

Question diversity
2.96 7.86 8.57

% games repeated questions
98.49 66.32 89.19

Super-cat -> obj/attr
91.26 91.67 94.82

Object -> attribute
98.57 72.33 58.51

% turns location questions
75.84 26.03 39.68

Vocab. size
1275 1002 814

Accuracy
38.73% 29.83% 46.93%

Table 6: Comparison between the quality of gameplay in out-of-domain zero-shot scenario between GDSE-CL and GDSE-CL with imagination. Number of total turns 10.

Question type
Inanimate object Animate object Super category Location Size Color Parts

Accuracy
65.48% 53.33% 83.33% 78.86% 100.00% 58.33% 100.00%

Count
168 15 60 175 1 24 2

Question type
Inanimate object Animate object Super category Location Size Color Parts

Accuracy
81.71% 70.00% 67.61% 72.97% 100% 63.64% 71.43%

Count
164 10 71 148 1 88 7

Table 7: Error analysis results completed on the Out-of-domain zero-shot scenario for the model GDSE-CL+Imagination (on the left) and GDSE-CL (on the right).

