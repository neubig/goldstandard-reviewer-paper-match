Overcoming a Theoretical Limitation of Self-Attention
David Chiang and Peter Cholak University of Notre Dame
{dchiang,cholak}@nd.edu

arXiv:2202.12172v1 [cs.LG] 24 Feb 2022

Abstract
Although transformers are remarkably eﬀective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer’s classiﬁcation decisions become less and less conﬁdent (that is, with crossentropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn’s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we ﬁnd that they can fail to generalize to longer strings; we oﬀer a simple remedy to this problem that also improves length generalization in machine translation.
1 Introduction
Although transformers (Vaswani et al., 2017) are remarkably eﬀective for many tasks, there are some surprisingly easy-looking formal languages that they struggle with. Hahn (2020) tries to explain some of these by showing (his Lemma 5) that changing a single input symbol only changes the output of a transformer encoder by 𝑂 (1/𝑛), where 𝑛 is the input string length. Thus, for a language where acceptance depends on a single input symbol, a transformer might accept or reject strings with perfect accuracy, but for large 𝑛, it must do so with low conﬁdence, giving accepted strings a probability just above ½ and rejected strings a probability just below ½. More precisely, as 𝑛 increases, the cross-entropy approaches its worst possible value of 1 bit per string.

Here, we examine this limitation using two simple regular languages:
PARITY = {𝑤 ∈ Σ∗ | 𝑤 has an odd number of 1s} FIRST = {𝑤 ∈ Σ∗ | 𝑤1 = 1}
where (here and throughout the paper) Σ = {0, 1}. Hahn’s lemma applies to PARITY because the network must attend to all the symbols of the string, and a change in any one of them changes the correct answer. We have chosen FIRST as one of the simplest examples of a language that the lemma applies to. It only requires attention on the ﬁrst symbol, but the lemma still applies because a change in this symbol changes the correct answer.
Although the lemma might be interpreted as limiting the ability of transformers to recognize these languages, we show three ways that this limitation can be overcome.
First, we show by explicit constructions that transformers do in fact exist that can recognize both languages with perfect accuracy for arbitrary lengths. We have implemented these constructions and veriﬁed them experimentally (§3).
As predicted by Hahn’s lemma, our constructed transformers have cross-entropy that approaches 1 bit (that is, just barely better than random guessing) as input length increases. But we show that by adding layer normalization, the cross-entropy can be made arbitrarily close to zero, independent of string length (§4).
In practice, we ﬁnd, like Bhattamishra et al. (2020a), that transformers cannot learn PARITY. Perhaps more surprisingly, when learning FIRST, transformers can have diﬃculty generalizing from shorter strings to longer strings. Although this is not a logical consequence of Hahn’s lemma, it is a consequence of the behavior that Hahn’s lemma predicts. Fortunately, this problem can be ﬁxed with a simple modiﬁcation, multiplying attention logits by log 𝑛. This modiﬁcation also improves length generalization in machine translation (§5).

2 Background
2.1 Notation If 𝜙 is a true-or-false statement, we write

1 if 𝜙 is true I[𝜙] =
0 otherwise.

For any 𝑚, 𝑛 > 0, we write 0𝑚×𝑛 for the 𝑚 × 𝑛 zero matrix and I𝑛×𝑛 for the 𝑛 × 𝑛 identity matrix.
2.2 Transformers
Following Hahn (2020), we consider transformer encoders with a sigmoid output layer on a single position. Diﬀerently from Hahn (2020), but in line with common practice (Devlin et al., 2019), we prepend a token CLS (for “classiﬁcation”) and use the encoder output at this token’s position for classifying the string.
We use the original deﬁnition of transformers (Vaswani et al., 2017), except for positional encodings.
2.2.1 Input layer The input to the network is a string 𝑤 ∈ Σ∗. Let 𝑛 = |𝑤| + 1, let 𝑤0 = CLS, and let 𝑤𝑖 be the 𝑖-th symbol of 𝑤.
The input layer has a word embedding and positional encodings,

WE :

Σ

→

𝑑
R

PE :

N

→

𝑑
R

which are used to compute input vectors for 𝑖 = 0, . . . 𝑛:
a0,𝑖 = WE(𝑤𝑖) + PE(𝑖).

The word embeddings are typically learned, while the positional encodings vary somewhat. Originally (Vaswani et al., 2017), they were ﬁxed and deﬁned in terms of sine and cosine waves, but they can also be learned (Gehring et al., 2017), in which case they are deﬁned only up to some maximum position. Here, we allow ourselves to deﬁne PE as an arbitrary function on all positions. It would seem that to remain in the spirit of the original paper, PE should be easy to compute, independent of 𝑤, and parallelizable over positions.

2.2.2 Encoder layers
The body of the encoder is a stack of 𝐿 layers, each of which has a self-attention sublayer followed by a position-wise feedforward sublayer. For ℓ = 1, . . . , 𝐿, layer ℓ is deﬁned as follows, where ℎ = 1, . . . , 𝐻, and 𝑖 = 0, . . . , 𝑛:

qℓ,ℎ,𝑖 = WQ,ℓ,ℎaℓ−1,𝑖

Kℓ,ℎ = WK,ℓ,ℎaℓ−1,0 · · · WK,ℓ,ℎaℓ−1,𝑛

Vℓ,ℎ = WV,ℓ,ℎaℓ−1,0 · · · WV,ℓ,ℎaℓ−1,𝑛

𝐻

∑︁

cℓ,𝑖 = LN

Att(qℓ,ℎ,𝑖, Kℓ,ℎ, Vℓ,ℎ) + aℓ−1,𝑖

ℎ=1

hℓ,𝑖 = max 0, WF,ℓ,1cℓ,𝑖 + bF,ℓ,1

aℓ,𝑖 = LN WF,ℓ,2hℓ,𝑖 + bF,ℓ,2 + cℓ,𝑖

where boldface lowercase letters stand for vectors in R𝑑 and boldface uppercase letters stand for matrices in R𝑑×𝑑. The learned parameters of the model are the W’s and b’s. The function Att is
scaled dot-product attention, deﬁned as

Att :

𝑑
R

×

R ( 𝑛+1) ×𝑑

×

R ( 𝑛+1) ×𝑑

→

𝑑
R

Kq Att(q, K, V) = V softmax √
𝑑

where the result of the softmax, sometimes written as 𝛼, is a vector of attention weights. The function LN is layer normalization, whose deﬁnition we defer to §4.

2.2.3 Output layer Finally, the network linearly projects the encoding of CLS to a scalar and applies a sigmoid function:

𝑦 = 𝜎 (W𝐿+1a𝐿,0 + b𝐿+1)

where W𝐿+1 ∈ R1×𝑑 and b𝐿+1 ∈ R1×1. The network accepts 𝑤 iﬀ the output probability is greater than 12 .
3 Exact Solutions

The ﬁrst way to overcome the limitation suggested by Hahn’s lemma is to show by explicit construction that our two languages can in fact be recognized with perfect accuracy by transformers.

3.1 FFNN for PARITY
Rumelhart et al. (1986) showed that for any 𝑛, there is a feedforward neural network (FFNN) that computes PARITY for strings of length exactly 𝑛. They

also showed that a randomly initialized FFNN can learn to do this automatically.
Since our construction is partially based on theirs, it may be helpful to review their construction in detail. Let 𝑤 be the input string, |𝑤| = 𝑛, and 𝑘 be the number of 1s in 𝑤. The input is a vector x such that x𝑖 = I[𝑤𝑖 = 1]. The ﬁrst layer computes 𝑘 and compares it against 1, 2, . . . , 𝑛:

1 1 · · · 1





1 1 · · · 1

W1

=

 .

.

.

 .

 .. .. . . .. 





1 1 · · · 1





so that

 −0.5 





 −1.5 

b1 =  . 

.

.





−𝑛 + 0.5





I[𝑘 ≥ 1]





I[𝑘 ≥ 2]

h1 = 𝐻 (W1x + b1) =  . 

.

.





I[𝑘 ≥ 𝑛]





where 𝐻 is the step function (𝐻 (𝑥) = I[𝑥 > 0]),

applied elementwise.

The second layer adds up the odd elements and

subtracts the even elements:

W2 = 1 −1 · · · (−1)𝑛+1 b2 = −0.5

𝑦 = 𝐻 (W2h1 + b2)

which is 1 if 𝑘 is odd and 0 is 𝑘 is even.

3.2 Transformer for PARITY

Proposition 1. There is a transformer encoder with sigmoid output layer that recognizes (in the above sense) the language PARITY for strings of arbitrary length.

Initially, we will construct a transformer encoder without layer normalization (that is, LN(x) = x); then we will show how to add layer normalization (§4). Let 𝑘 be the number of occurrences of 1 in 𝑤. All vectors computed by the network have 𝑑 = 9 dimensions; if we show fewer dimensions, assume the remaining dimensions to be zero.
The word and position embeddings are:

1
0  WE(0) = 0  0  0 
0
0  WE(CLS) = 1  0  0 

0
1
 WE(1) = 0
 0  0 

0

 

0

 

 PE(𝑖) =  0  .


𝑖



𝑛

cos 𝑖𝜋 

Since we are numbering positions starting from 0, dimension 4 ranges from 0 to 𝑛−1 , and dimension 5
𝑛
is +1 for even positions and −1 for odd positions. We argue that dimension 5, being a cosine wave,
is a fairly standard choice, although its period (2) is shorter than the shortest period in standard sinusoidal encodings (2𝜋). Dimension 4 is admittedly not standard; however, we argue that it is a reasonable encoding, and extremely easy to compute.
Thus, the encoding of word 𝑤𝑖 is:

 I[𝑤𝑖 = 0] 

 

I[𝑤𝑖 = 1]

 

a0,𝑖 = I[𝑤𝑖 = CLS] .





𝑖





𝑛

 cos 𝑖𝜋 





The network has 𝐿 = 2 layers and 𝐻 = 2 heads.
The ﬁrst self-attention layer has one head which
ﬁnds 𝑘, the number of 1s. More precisely, be-
cause attention always averages, it must compute the “average” number of 1s, that is, 𝑘 , and stores
𝑛
it in dimension 6. It also stores 1 in dimension 7,
𝑛
which we will need later.

WQ,1,1 = 0

WK,1,1 = 0



05×5



WV,1,1

=

 0

1

0

0

 0





0 0 1 0 0





The second head doesn’t do anything (WV,1,2 = 0; the queries and keys can be anything). After the residual connection, we have:

 I[𝑤𝑖 = 0] 

 

I[𝑤𝑖 = 1]

 

I[𝑤𝑖 = CLS]

c1,𝑖

=

 

𝑖

 .

𝑛





 cos 𝑖𝜋 





𝑘





𝑛 1

𝑛

In the construction of Rumelhart et al. (1986), the next step is to compute I[𝑖 ≤ 𝑘] for each 𝑖, using step activation functions. There are two differences in our construction. First, we have ReLU activation functions, not step activation functions. Second, because attention must sum to one, if 𝑛 is odd then the even and odd positions will get different attention weights, so the trick of subtracting even positions from odd positions will not work. Instead, we want to compute I[𝑖 = 𝑘] (Fig. 1).

1

0 𝑘−2 𝑘−1 𝑘 𝑘+1 𝑘+2
Figure 1: Piecewise linear function equivalent on the integers to I[𝑖 = 𝑘].

The ﬁrst FFNN has two layers. The ﬁrst is:

0 0 0 −1 0 1 −1

WF,1,1

=

 0

0

0

−1

0

1

 0





0 0 0 −1 0 1 1 





0

bF,1,1

=

 0

.

 0



This gives:

1 max(0, 𝑘 − 𝑖 − 1)

h1,𝑖 =

 

max(0, 𝑘 − 𝑖)

 .

𝑛

 max(0,

𝑘

−

𝑖

+

 1) 





The second layer linearly combines these three values to get I[𝑖 = 𝑘] as desired.

WF,1,2 = 07×3 1 −2 1

bF,1,2 = 0.

After the residual connection, we have:

 I[𝑤𝑖 = 0] 





 I[𝑤𝑖 = 1] 





I[𝑤𝑖 = CLS]







𝑖



a1,𝑖 =  cos𝑛𝑖𝜋  .







𝑘



𝑛 1

𝑛

 I[𝑖=𝑘] 





𝑛





The second self-attention layer tests whether po-

sition 𝑘 is even or odd. It does this using two heads,

one which attends more strongly to the odd posi-

tions, and one which attends more strongly to the

even positions; both average dimension 8: √
WQ,2,1 = 0 0 𝑐 𝑑 0 0 0 0 0

WK,2,1 = 0 0 0 0 −1 0 0 0

WV,2,1 =

08×8

00000001

√

WQ,2,2 = 0 0 𝑐 𝑑 0 0 0 0 0

WK,2,2 = 0 0 0 0 1 0 0 0

WV,2,2 =

08×8

0 0 0 0 0 0 0 −1

where 𝑐 > 0 can be any constant. The second FFNN doesn’t do anything (WF,2,1 = bF,2,1 = WF,2,2 = bF,2,2 = 0). The vector at CLS (posi-
tion 0) is then

0  0  1  0  a2,0 =  1   𝑘 𝑛 1 𝑛  I[𝑘=0]  
𝑛
 𝑠 
where 𝑠 has a somewhat complicated value. If 𝑛 is
even, it turns out to be
𝑠 = (−1)𝑘+1 2 tanh 𝑐 𝑛2

which is positive if 𝑘 is odd and negative if 𝑘 is even. As predicted by Hahn, it is in 𝑂 (1/𝑛). If 𝑛 is odd, the expression for 𝑠 is more complicated (see Appendix A), but it is still positive iﬀ 𝑘 is odd, and it is still in 𝑂 (1/𝑛).
Finally, the output layer is a sigmoid layer that just looks at dimension 9:

W3 = 0 0 0 0 0 0 0 0 1 𝑦= 1 . 1 + exp(−𝑠)

b3 = 0

So the output is greater than 12 iﬀ 𝑘 is odd.
3.3 Transformer for FIRST
Next, we construct a transformer for FIRST. In line with the common practice of learning per-position word embeddings (Gehring et al., 2017), we use position embeddings that test whether a word is at position 1:

 I[𝑤𝑖 = 0] 

a0,𝑖 =  I[𝑤𝑖 = 1]  . I[𝑤𝑖 = CLS]





 I[𝑖 = 1] 





The ﬁrst self-attention layer does nothing (WV,1,1 = 0), so after the residual connection, c1,𝑖 = a0,𝑖.
The ﬁrst FFNN computes a new component (5)

that tests whether 𝑖 = 1 and 𝑤1 = 1:

WF,1,1 = −1 0 −1 1

0

0

WF,1,2

=

 0

 0



1 

 I[𝑤𝑖 = 0] 

 

I[𝑤𝑖 = 1]

 

a1,𝑖

=

 

I[𝑤𝑖 = CLS]

 .





 I[𝑖 = 1] 





I[𝑤𝑖 = 1 ∧ 𝑖 = 1]





bF,1,1 = 0 bF,1,2 = 0

(We have chosen WF,1,1 in a slightly unusual way to avoid using the bias term bF,1,1, in anticipation of §4 when we will add layer normalization.)
The second self-attention layer has a single head, which makes CLS focus on position 1.
√ WQ,2,1 = 0 0 𝑐 𝑑 0 0
WK,2,1 = 0 0 0 1 0
WV,2,1 = 0 0 005×5− 1 1
2

where 𝑐 > 0 is a constant. The second FFNN doesn’t do anything (WF,2,1 = bF,2,1 = WF,2,2 = bF,2,2 = 0). So at CLS (position 0),

0

0



a2,0

=

1 

0

 0



𝑠

 𝑠 = exp e𝑐x+p 𝑛𝑐 − 1 I[𝑤1 = 1] − 12 . (1)

The ﬁnal output layer just selects component 6:

W3 = 0 0 0 0 0 1

b3 = 0.

So the output probability, 𝑦 = 𝜎(𝑠), is greater than

1 2

iﬀ

𝑤1

=

1.

However,

it

will

get

closer

to

1 2

as

𝑛

increases.

3.4 Experiments
We implemented both of the above constructions using modiﬁed versions of PyTorch’s built-in implementation of transformers (Paszke et al., 2019).
The code for this and other experiments in this paper are available at https://github.com/ndnlp/parity.

cross-entropy (bits)

PARITY

1

no layer norm

0.5
0 0

layer norm 𝜖 = 10−5 layer norm 𝜖 = 0
20 40 60 80 100

FIRST

cross-entropy (bits)

1
0.5
0 0

no layer norm layer norm 𝜖 = 10−5
layer norm 𝜖 = 0 200 400 600 800 1,000
string length 𝑛

Figure 2: Cross-entropy of exact solutions for PARITY and FIRST computed over 1000 random strings of length 𝑛. Without layer norm, the cross-entropy quickly approaches its upper bound of one. With layer norm and 𝜖 > 0, the cross-entropy is better but still grows with 𝑛. With 𝜖 = 0, cross-entropy is independent of 𝑛.

These constructions achieve perfect accuracy for strings with lengths sampled from [1, 1000].
However, in Fig. 2, the red curves (“no layer norm”) show that, as strings grow longer, the crossentropy approaches its worst possible value of 1 bit per string. We discuss this problem next.
4 Layer Normalization
The second way to mitigate or eliminate the limitation of Hahn’s lemma is layer normalization (Ba et al., 2016), which is deﬁned, for any vector x, as
LN(x; 𝛾, 𝛽) = x − mean(x) ◦ 𝛾 + 𝛽 √︁ var(x) + 𝜖
where the functions mean and var compute the mean and variance, respectively, of the elements of x, and ◦ is the elementwise (Hadamard) product. We ﬁx 𝛽 = 0 and 𝛾 = 1, so that the result has approximately zero mean and unit variance. The constant 𝜖 was not present in the original deﬁnition (Ba et al., 2016) but is added in all implementations that we are aware of, for numerical stability.
The original transformer deﬁnition performs layer normalization immediately after every residual connection. In this section, we modify our
It is also common to place layer normalization before residual connections (Wang et al., 2019; Nguyen and Salazar, 2019), but we follow the original transformer deﬁnition here.

two constructions above to use layer normalization. This modiﬁcation has two steps.

4.1 Removing centering
The ﬁrst is to nullify the centering eﬀect of layer normalization by making the network compute each value 𝑥 as well as its negation −𝑥. The new word encodings are deﬁned in terms of those in the original construction:

a¯0,𝑖 = −aa00,𝑖,𝑖 .

Likewise for the self-attention parameters:

W¯ Q,ℓ,ℎ = WQ,ℓ,ℎ 0 W¯ K,ℓ,ℎ = WK,ℓ,ℎ 0 W¯ V,ℓ,ℎ = −WWVV,ℓ,ℓ,ℎ,ℎ 00 .

Likewise for the position-wise FFNN parameters:

W¯ F,ℓ,1 = W¯ F,ℓ,2 =

WF,ℓ,1 0
WF,ℓ ,2 −WF,ℓ ,2

b¯ F,ℓ,1 = bF,ℓ,1 b¯ F,ℓ,2 = −bbFF,ℓ,ℓ,2,2 .

Then each layer of activations is

c¯ℓ,𝑖 = LN

cℓ ,𝑖 −cℓ ,𝑖

ℓ,𝑖

aℓ ,𝑖

a¯ = LN −aℓ,𝑖 .

The argument to LN always has zero mean, so that layer normalization does not add or subtract anything. It does scale the activations, but in the case of the two transformers constructed above, any activation layer can be scaled by any positive number without changing the ﬁnal decisions (see Appendix B).

4.2 Reducing cross-entropy
Furthermore, in any transformer, we can use layer normalization to shrink the cross-entropy as small as we like, contrary to Hahn’s Lemma 5. In Hahn’s formulation, position-wise functions like layer normalization can be subsumed into his 𝑓 act, but the lemma assumes that 𝑓 act is Lipschitz-continuous, and layer normalization with 𝜖 = 0 is not.
Proposition 2. For any transformer 𝑇 with layer normalization (𝜖 = 0) that recognizes a language L, and for any 𝜂 > 0, there is a transformer with layer normalization that recognizes L with crossentropy at most 𝜂.

Proof. Let 𝑑 be the number of dimensions in the original vectors of activations, and let 𝐿 be the number of layers. Then we add a new layer whose self-attention doesn’t do anything (WV,𝐿+1,ℎ = 0) and whose FFNN is deﬁned in terms of the original output layer:

W¯ F,𝐿+1,1 = I𝑑 −I𝑑

b¯ F,𝐿+1,1 = 0𝑑 0𝑑

W¯ F,𝐿+1,2 = −I𝑑 I𝑑

 b𝐿+1 

b¯ F,𝐿+1,2 = −b𝐿+1 .

 

0𝑑−2

 





 W𝐿+1

+

 

−W𝐿+1

0 ( 𝑑 −2) ×𝑑



−W𝐿+1 

W𝐿+1

 

0 ( 𝑑−2) ×𝑑 



This causes the residual connection to zero out all dimensions except two, so that if 𝑠 was the original output logit, the output of this new layer (before layer normalization) is

𝑠

a¯ 𝐿+1,𝑖 = LN

 

−𝑠

 

.

0𝑑−2



Now, if 𝜖 = 0, layer normalization scales this vector to have unit variance exactly, so it becomes

√︁ ± 𝑑/2

a¯ 𝐿+1,𝑖

=

 √︁  ∓ 𝑑/2

.

 

0𝑑−2

 





The new output layer simply selects the ﬁrst dimension, scaling it by 𝑐:

W¯ 𝐿+2 = 𝑐 0 0𝑑−2

b¯ 𝐿+2 = 0.

Finally, set 𝑐 = − √1 log(exp 𝜂 − 1). If the
𝑑/2
input string is in L, then the cross-entropy is √︁
log 𝜎(𝑐 𝑑/2) = 𝜂. Similarly, if the input string
is not in L, then the cross-entropy is log(1 − √︁
𝜎(−𝑐 𝑑/2)) = 𝜂.

However, in practice, 𝜖 is always set to a nonzero value, which makes layer normalization Lipschitzcontinuous, so Hahn’s Lemma 5 still applies.

4.3 Experiments
We tested our exact solutions, modiﬁed as described above to use layer normalization. Figure 2 shows that layer normalization with 𝜖 > 0 improves the cross-entropy, but it still grows with 𝑛 and approaches 1. With 𝜖 = 0, the cross-entropy is independent of 𝑛 and, as argued above (Proposition 2), can be made as low as desired.

cross-entropy (bits)

4

2

0

0.6

0.8

1

1.2

1.4

1

accuracy

0.5

0

0.6

0.8

1

1.2

1.4

parameter value

Figure 3: The cross-entropy and accuracy of our solu-
tion to PARITY are both extremely sensitive to the parameter W¯ V6,,21,1, which is responsible for computing 𝑛𝑘 . The correct parameter value is 1.

5 Learnability
In this section, we turn to the question of learnability, which will lead to a third way of overcoming the limitation suggested by Hahn’s lemma.
5.1 Experiments: standard transformers
We tried training transformers on both PARITY and FIRST. Each transformer had the same number of layers and heads and the same ﬁxed positional encodings as the corresponding exact solution. We used 𝑑model = 16 for word encodings, self-attention, and FFNN outputs, and 𝑑FFNN = 64 for FFNN hidden layers. We used layer normalization (𝜖 = 10−5) after residual connections. We used PyTorch’s default initialization and trained using Adam (Kingma and Ba, 2015) with learning rate 3 × 10−4 (Karpathy, 2016). We did not use dropout, as it did not seem to help.
We found, like Bhattamishra et al. (2020a), that a transformer with the above settings was unable to learn PARITY. We tried many other settings as well, to no avail. To give an idea of why our constructed solution, in particular, is diﬃcult to ﬁnd, Fig. 3 shows the cross-entropy and accuracy of the model if we start with our solution (with layer normalization, 𝜖 = 0) and vary the parameter W¯ V6,,21,1, which is responsible for computing 𝑘 . At 1, it
𝑛
has a cross-entropy of 0 and accuracy of 1, which are both optimal, but the cross-entropy oscillates so rapidly that even a small perturbation of this parameter would make it diﬃcult to recover the solution by gradient descent.

FIRST is much easier to learn, but the bad news is that the learned transformers do not generalize well to longer sentences. Figure 4 (left column) shows that when a transformer is trained from scratch on shorter strings (𝑛 = 10, 30, 100, 300) and tested on longer strings (𝑛 = 1000), the accuracy is not perfect. Indeed, for training 𝑛 = 10, the accuracy is hardly better than random guessing.

5.2 Flawed transformer for FIRST

In our solution above (§3.3), the second selfattention layer attended mostly to the ﬁrst position, but not totally. It relied on the fact that in the second self-attention layer, the values of the non-ﬁrst positions (V2𝑖,,41 and V2𝑖,,51 for 𝑖 ≠ 1) are exactly zero and therefore do not contribute to the output.
In practice, because word embeddings are randomly initialized in all dimensions, and are added to every layer via residual connections, it’s unlikely for any activation to be exactly zero. This explains why our exact solution cannot be learned.
But, as a further thought experiment about what the model might be learning instead, consider the following transformer, which uses only a single layer (𝐿 = 1) and does not zero out the values of the non-ﬁrst positions. As we will see, it performs worse than the transformer of §3.3 for long strings.
√ WQ,1,1 = 0 0 𝑐 𝑑 0
WK,1,1 = 0 0 0 1
WV,1,1 = − 1 104×4− 1 0 .
22 2
The FFNN doesn’t do anything (WF,1,1 = bF,1,1 = WF,1,2 = bF,1,2 = 0), and the ﬁnal output layer just selects component 5. So if 𝑘 is the total number of 1s, the ﬁnal logit at CLS (position 0) would be

exp 𝑐 − 1

1

=

I[𝑤1 = 1] −

𝑠

exp 𝑐 + 𝑛 − 1

2

+1

𝑛 𝑘− .

exp 𝑐 + 𝑛 − 1 2

If 𝑐 > log(𝑛 − 1), then this is positive iﬀ 𝑤1 = 1. But if 𝑐 ≤ log(𝑛 − 1), the new second term can be big enough to make the model output an incorrect answer. This suggests that if we train a transformer on strings with length up to 𝑁, then the learned parameters will be large enough to classify strings of length up to 𝑁 correctly, but may misclassify strings longer than 𝑁.
This explanation is corroborated by the bottomleft graph in Fig. 4, which shows the attention

weight on the ﬁrst position of the test string (summed over layers, averaged over strings) as a function of training epoch (starting from random initial parameters). The training strings have varying length (𝑛) and the test strings have ﬁxed length (1000). We might hope that the attention weight would converge to the same value independent of 𝑛. But the lower 𝑛 is, the more the attention weight is diluted, making it easier for the value in position 1 to be outweighed by values in other positions.

5.3 Log-length scaled attention

Fortunately, this problem is easy to ﬁx by scaling the logits of each attention layer by log 𝑛, that is, redeﬁning attention as

log 𝑛 Att(q, K, V) = V softmax √ Kq. (2)
𝑑

Then taking the model in §5.2 with 𝑐 = 1 gives

𝑛−1

1

1

=

I[𝑤1 = 1] − +

𝑛 𝑘−

𝑠 2𝑛 − 1

2 2𝑛 − 1 2

which is positive iﬀ 𝑤1 = 1. Moreover, scaling is another way to make the cross-entropy low:

Proposition 3. For any 𝜂 > 0 there is a transformer with attention deﬁned as in Eq. (2), and with or without layer normalization, that recognizes FIRST with cross-entropy at most 𝜂.

Proof. Without layer normalization, we can take the model in §3.3, set 𝑐 = 1 and log-scale the attention weights, which changes 𝑠 from Eq. (1) to

𝑛 =

1 I[𝑤1 = 1] −

𝑠

2𝑛 − 1

2

1 < |𝑠| ≤ 1.

4

2

With layer normalization (and 𝜖 > 0), we can

apply the modiﬁcation of §4 to nullify the center-

ing eﬀect of layer normalization. Then since the
variance of a2,0 is 16 (1 + 𝑠2), the layer-normalized ﬁnal logit is

1

− 12

𝑠¯ = 𝑠 (1 + 𝑠2) + 𝜖

6

and since |𝑠| > 14 , |𝑠¯| > 1 4

5

− 12

+𝜖 .

24

In either case, since the ﬁnal logit has a lower bound not dependent on 𝑛, the output layer weights can be scaled as in the proof of Proposition 2 to make the cross-entropy at most 𝜂.

train tokens test tokens
baseline scaled

train all test all
3M+3M 32k+34k
32.6 32.5

train short test long
1M+1M 24k+25k
11.4 12.4

Table 1: When training and testing on data with the same length distribution, scaling attention logits has no signiﬁcant eﬀect on BLEU, but when training on short sentences (≤ 20 tokens) and testing on long sentences (> 20 tokens), scaling helps signiﬁcantly (𝑝 < 0.01).

5.4 Experiments: scaled attention
Figure 4 (right column) shows the training of transformers with scaling of attention logits by log 𝑛. For all training lengths 𝑛, the model is able to learn with perfect test cross-entropy and accuracy.
We see a similar eﬀect on low-resource Englishto-Vietnamese machine translation (Table 1), using Witwicky, an open-source implementation of transformers. We use all default settings; in particular, residual connections come after layer normalization (𝜖 = 10−5). We measure translation accuracy using BLEU (Papineni et al., 2002) and use bootstrap resampling with 1000 samples for signiﬁcance testing.
When train and test length distributions are the same, scaling attention logits has no signiﬁcant effect. But if we train only on sentences with median length or shorter (≤ 20 tokens) and test only on sentences longer than median length (> 20 tokens), scaling attention logits by log 𝑛 improves BLEU by +1, which is statistically signiﬁcant (𝑝 < 0.01).
6 Related Work
Using very diﬀerent assumptions on the form of transformers and inputs, a number of recent theoretical studies of transformers show that they can solve much more diﬃcult problems than the ones studied here. Transformer encoders can be shown to be universal approximators by ﬁxing the string length and using a number of layers exponential in the length (Yun et al., 2020). Transformer encoder–decoders, where the decoder can run for an unbounded number of steps, have been shown to be Turing-complete (Bhattamishra et al., 2020b; Pérez et al., 2021).
https://github.com/tnq177/witwicky

Baseline 1

Scaled attention logits 1

accuracy

0.5

0.5

total attention weight cross-entropy (bits)

0

0

1.5

1.5

1

1

0.5

0.5

0

0

1

1

0.5

0.5

0

0

1

10

100

1000

1

10

100

1000

epoch (log-scale)

epoch (log-scale)

𝑛 = 10

𝑛 = 30

𝑛 = 100

𝑛 = 300

Figure 4: Training a transformer on FIRST. Each epoch has 100 training strings of varying length (see legend)
and 100 test strings of length 1000. All curves are averaged over 20 runs. Left: Standard transformer with layer normalization (𝜖 = 10−5). Right: Same, with attention logits scaled by log 𝑛.

RASP (Weiss et al., 2021) is a simple programming language whose programs can be compiled into transformers. While PARITY can easily be written in RASP, this does not imply in itself the existence of transformers that can recognize PARITY, for two reasons. First, RASP’s aggregate operation (which corresponds to attention) always attends uniformly to a subset of positions, unlike softmax attention. Second, RASP’s elementwise operations are embedded directly in the output transformer; they are not translated into FFNNs.
Bhattamishra et al. (2020a) carry out theoretical and experimental studies of transformers for various formal languages. The theoretical results are for a diﬀerent variant of transformers than ours (transformer encoders with self-attention masked so that each position attends only to previous positions), and focus on such transformers’ ability to maintain counters that are constrained to be nonnegative. Their experimental results suggest that transformers have diﬃculty learning some regular languages, including PARITY.

7 Conclusion
We’ve seen that the questions of (a) whether a neural network can recognize a language, (b) whether it can achieve low cross-entropy on a language, and (c) whether it can learn to recognize a language are three separate questions, because we have given examples of (a) without (b) and (b) without (c).
Namely, our explicit construction for PARITY shows that a neural network can recognize a language with perfect accuracy (a) but poor crossentropy (b). Adding layer normalization (𝜖 = 0) enables it to achieve low cross-entropy (b), but still does not learn well (c). We observe that because the answer to (b) can hinge on small details of the model, (b) is not probably not very useful as a way of measuring expressivity.
However, we did ﬁnd that the limited inﬂuence of a single input symbol, implied by Hahn’s lemma, has a serious practical implication for learnability (c). Namely, transformers can fail to generalize from shorter training strings to longer testing strings. Our proposed ﬁx, scaling attention logits by log 𝑛, is easy to implement and very eﬀective on a real machine translation task.

Acknowledgements
We would like to thank Toan Nguyen for assistance with his machine translation code, and Gail Weiss for catching some mistakes.
This paper is based upon work supported in part by the Oﬃce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA865017-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the oﬃcial policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoﬀrey E. Hinton. 2016. Layer normalization. arXiv:1607.06450.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020a. On the ability and limitations of Transformers to recognize formal languages. In Proc. EMNLP, pages 7096–7116.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. 2020b. On the computational power of Transformers and its implications in sequence modeling. In Proc. CoNLL, pages 455–475.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL HLT, pages 4171–4186.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proc. ICML, pages 1243–1252.
Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Trans. ACL, 8:156–171.
Andrej Karpathy. 2016. 3e-4 is the best learning rate for Adam, hands down. Twitter.
Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic optimization. In Proc. ICLR.
William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021. Eﬀects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proc. EMNLP, pages 1766–1781.
Toan Q. Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of

self-attention. In Proc. International Workshop on Spoken Language Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library. In Proc. NeurIPS.
Jorge Pérez, Pablo Barceló, and Javier Marinkovic. 2021. Attention is Turing-complete. Journal of Machine Learning Research, 22(75):1–35.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning Internal Representations by Error Propagation, pages 318–362. MIT Press, Cambridge, MA, USA.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS, pages 5998–6008.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep Transformer models for machine translation. In Proc. ACL, pages 1810–1822.
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like Transformers. In Proc. ICML.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. Are Transformers universal approximators of sequence-to-sequence functions? In Proc. ICLR.

A Correctness of PARITY Construction

In §3.2, we constructed a transformer that recog-

nizes PARITY; here we ﬁll in details of calculating
𝑠 = a92,0. If 𝑛 is even, the ﬁrst head computes √
q2,1,0 = 𝑐 𝑑

K2𝑖,,11,0 = − cos 𝑖𝜋 = (−1)𝑖+1

2,1,0
𝛼

=

exp(−1)𝑖+1𝑐

𝑖

𝑛 2

(exp

𝑐

+

exp

−𝑐)

V2,1,0

=

I[𝑖

=

𝑘] .

𝑖,9

𝑛

Similarly, the second head computes √
q2,2,0 = 𝑐 𝑑

K2𝑖,,12,0 = cos 𝑖𝜋 = (−1)𝑖

2,2,0
𝛼

=

exp (−1) 𝑖 𝑐

𝑖

𝑛 2

(exp

𝑐

+

exp

−𝑐)

V2,2,0 = − I[𝑖 = 𝑘] .

𝑖,9

𝑛

Then

𝑠

=

a2,0

=

1 2,1,0 𝛼

−

1 2,2,0 𝛼

9 𝑛𝑘

𝑛𝑘

exp(−1)𝑘+1𝑐 − exp(−1)𝑘 𝑐 =
𝑛22 (exp 𝑐 + exp −𝑐) = (−1)𝑘+1 exp 𝑐 − exp −𝑐
𝑛22 (exp 𝑐 + exp −𝑐)

= (−1)𝑘+1 2 tanh 𝑐 𝑛2

is negative if 𝑘 is even and positive if 𝑘 is odd. If 𝑛 is odd, calculating 𝑠 is more complicated
because there are unequal numbers of more- and less-attended positions. The attention weights are

2,1,0
𝛼

=

exp(−1)𝑖+1𝑐

𝑖 𝑛−21 exp 𝑐 + 𝑛2+1 exp −𝑐

𝑍1

2,2,0
𝛼

=

exp (−1) 𝑖 𝑐

𝑖 𝑛2+1 exp 𝑐 + 𝑛−21 exp −𝑐

𝑍2
𝑠 = (exp(−1)𝑘+1𝑐)𝑍2 − (exp(−1)𝑘 𝑐)𝑍1 . 𝑛𝑍1𝑍2

If 𝑘 is even,

𝑛−1 exp −2𝑐 − 𝑛−1 exp 2𝑐

𝑠= 2

2

𝑛𝑍1𝑍2

= − (𝑛 − 1) sinh 2𝑐 < 0

𝑛𝑍1𝑍2

whereas if 𝑘 is odd,

𝑛+1 exp 2𝑐 − 𝑛+1 exp −2𝑐

𝑠= 2

2

𝑛𝑍1𝑍2

(𝑛 + 1) cosh 2𝑐

=

> 0.

𝑛𝑍1𝑍2

B Scale-Invariance of PARITY and FIRST

Constructions

In §4.1, we claimed that the scaling eﬀect of layer normalization has no eﬀect on the decisions of our constructions for PARITY and FIRST. This is related to the property of approximate homogeneity studied by Merrill et al. (2021).
In general, we rely on the fact that the FFNNs we use all have no bias terms (bF,ℓ,1 and bF,ℓ,2), so the FFNNs are 1-homogenous (scaling the input scales the output by the same amount). For the selfattentions, our WQ,ℓ,ℎ all have a constant factor 𝑐 built into them, so any scaling of the input can be absorted into this constant.
For PARITY, suppose that layer normalization scales cℓ by 𝐶ℓ and aℓ by 𝐴ℓ.
c¯1,𝑖 = 𝐶1 −cc11,𝑖,𝑖

Because the ﬁrst FFNN has no bias term, a¯1,𝑖 = 𝐴1𝐶1 −aa11,𝑖,𝑖

In the second self-attention layer, the attention logits and the values are scaled by 𝐴1𝐶1. We’re only interested in what happens to 𝑠 = c29,0. If 𝑛 is even, 𝑠 becomes:
𝑠¯ = (−1)𝑘+1 2𝐶2 𝐴1𝐶1 tanh 𝐴1𝐶1𝑐 . 𝑛2
Since the second FFNN is the identity function, its layer normalization has no eﬀect (𝐴2 = 1). So the ﬁnal logit is 𝑠¯, which is still negative if 𝑘 is even and positive if 𝑘 is odd. Similarly if 𝑛 is odd.
For FIRST, again suppose that layer normalization scales cℓ by 𝐶ℓ and aℓ by 𝐴ℓ. As before,
a¯1,𝑖 = 𝐴1𝐶1 −aa11,𝑖,𝑖

In the second self-attention layer, the attention log-

its and the values are scaled by 𝐴1𝐶1. We’re only interested in what happens to 𝑠 = c26,0:

𝑠¯ = exp 𝐴1𝐶1𝑐 𝐶2 𝐴1𝐶1 I[𝑤1 = 1] − 1

exp 𝐴1𝐶1𝑐 + 𝑛 − 1

2

Since the second FFNN is the identity function,

𝐴2 = 1. So the ﬁnal logit is 𝑠¯, which is still positive if 𝑤1 = 1 and negative otherwise.

