arXiv:1905.10660v2 [cs.LG] 14 Oct 2020

An Algorithmic Framework for Fairness Elicitation
Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, Logan Stapleton†, Zhiwei Steven Wu‡
October 15, 2020
Abstract We consider settings in which the right notion of fairness is not captured by simple mathematical deﬁnitions (such as equality of error rates across groups), but might be more complex and nuanced and thus require elicitation from individual or collective stakeholders. We introduce a framework in which pairs of individuals can be identiﬁed as requiring (approximately) equal treatment under a learned model, or requiring ordered treatment such as “applicant Alice should be at least as likely to receive a loan as applicant Bob”. We provide a provably convergent and oracle eﬃcient algorithm for learning the most accurate model subject to the elicited fairness constraints, and prove generalization bounds for both accuracy and fairness. This algorithm can also combine the elicited constraints with traditional statistical fairness notions, thus “correcting” or modifying the latter by the former. We report preliminary ﬁndings of a behavioral study of our framework using human-subject fairness constraints elicited on the COMPAS criminal recidivism dataset.
1 Introduction
The literature on algorithmic fairness has consisted largely of researchers proposing and showing how to impose technical deﬁnitions of fairness [8, 18, 39, 2, 3, 21, 36, 29, 16, 14, 3, 6]. Because these imposed notions of fairness are described analytically, they are typically simplistic, and often have the form of equalizing simple error statistics across groups. Our starting point is the observation that:
1. This process cannot result in notions of fairness that do not have any simple, analytic description, and
2. This process also overlooks a more precursory problem: namely, who gets to deﬁne what is fair?
It’s unlikely that researchers alone are best ﬁt for deﬁning algorithmic fairness. Recent work identiﬁes undue power imbalances [25] and biases [24] that arise when algorithm designers and researchers are the only voices in conversations around ethical design. Veale et al. [35] ﬁnd that
University of Pennsylvania †University Of Minnesota ‡Carnegie Mellon University
1

many machine learning practitioners are disconnected from the “organisational and institutional realities, constraints and needs” speciﬁc to the contexts in which their algorithms are applied. Researchers may not be able to propose a concise technical deﬁnition, e.g. statistical parity, to capture the nuances of fairness in any given context. Furthermore, many philosophers hold that stakeholders who are aﬀected by moral decisions and experts who understand the context in which moral decisions are made will have the best judgment about which decisions are fair in that context [37, 24].
To this end, we aim to allow stakeholders and experts to play a central role in the process of deﬁning algorithmic fairness. This is aligned with recent work on virtual democracy, which propose and enact participatory methods to automate moral decision-making [5, 31, 17, 28, 11].
The way we involve stakeholders is motivated by two concerns:
1. We want stakeholders to have free rein over how they may deﬁne fairness, e.g. we don’t want to simply have them vote on whether existing, simple constraints like statistical parity or equalized odds is best; and
2. We want non-technical stakeholders to be able to contribute, even if they may not understand the inner workings of a learning algorithm.
We hold that people often cannot elucidate their conceptions of fairness; yet, they can identify speciﬁc scenarios where fairness or unfairness occurs.1 Drawing from individual notions of fairness like Dwork et al. [7], Joseph et al. [16] that are deﬁned in terms of pairwise comparisons, we therefore aim to elicit stakeholders conceptions of fairness by asking them to compare pairs of individuals in speciﬁc scenarios. Speciﬁcally, we ask whether it’s fair that one particular individual should receive an outcome that is as desirable or better than the other.
When pointing out fairness or unfairness, this kind of pairwise ranking is natural. For example, after Serena Williams was penalized for a verbal interaction with an umpire in the 2018 U.S. Open Finals, tennis player James Blake tweeted, “I have said worse and not gotten penalized. And I’ve also been given a ‘soft warning’ by the ump where they tell you knock it oﬀ or I will have to give you a violation. [The umpire] should have at least given [Williams] that courtesy” [38]. Here, Blake thinks that: 1) Williams should have been judged as or less severely than he would have been in a similar situation; and 2) the umpire’s decision was unfair, because Williams was judged more severely.
Thus, we ask a set of stakeholders about a ﬁxed set of pairs of individuals subject to a classiﬁcation problem. For each pair of individuals (A, B), we ask the stakeholder to choose from amongst a set of four options:
1. Fair outcomes must classify A and B the same way (i.e. they must either both get a favorable classiﬁcation or both get an unfavorable classiﬁcation).
2. Fair outcomes must give A an outcome that is equal to or preferable to the outcome of B.
3. Fair outcomes must give B an outcome that is equal to or preferable to the outcome of A
4. Fair outcomes may treat A and B diﬀerently without any constraints.
1This is philosophically akin to a theory of moral epistemology called moral perception, which claims that we know moral facts (e.g. goodness or fairness) via perception, as opposed to knowing them via rules of morality (see [4]).
2

These constraints, a data distribution, and a hypothesis class deﬁne a learning problem: minimize classiﬁcation error subject to the constraint that the rate of violation of the elicited pairwise constraints is held below some ﬁxed threshold. Crucially and intentionally we elicit relative pairwise orderings of outcomes (e.g. A and B should be treated equally), but do not elicit preferences for absolute outcomes (e.g. A should receive a positive outcome). This is because fairness — in contrast to justice — is often conceptualized as a measure of equality of outcomes, rather than correctness of outcomes2. In particular, it remains the job of the learning algorithm to optimize for correctness subject to elicited fairness constraints.
We remark that the premise (and the foundation for the enormous success) of machine learning is that accurate decision making rules in complex scenarios cannot be deﬁned with simple analytic rules, and instead are best derived directly from data. Our work can be viewed similarly, as deriving fairness constraints from data elicited from experts and stakeholders. In this paper, we solve the computational, statistical, and conceptual issues necessary to do this, and demonstrate the eﬀectiveness of our approach via a small behavioral study.
1.1 Results
Our Model We model individuals as having features in X and binary labels, drawn from some distribution P . A committee of stakeholders3 u ∈ U has preferences about whether one individual should be judged better than another individual. We imagine presenting each stakeholder with a set of pairs of individuals and asking them to choose one of four options for each pair, e.g. given the features of Serena Williams and Jacob Blake:
1. No constraint;
2. Williams should be treated as well as Blake or better;
3. Blake should be treated as well as Williams or better; or
4. Williams and Blake should be treated similarly.
Here, when we refer to how an individual should be treated, we mean the probability that an individual is given a positive label by the classiﬁer. This may be a bit of a relaxation of these judgments, since they are not about actualized classiﬁcations, but rather the probabilities of positive classiﬁcation. For example, we may not consider it a violation of fairness preference (2) if Williams is judged worse than Blake in a speciﬁc scenario; yet, if an ump is more likely to judge Williams worse than Blake in general, then this would violate this fairness preference.
We represent these preferences abstractly as a set of ordered pairs Cu ⊆ X × X for each stakeholder u. If (x, x ) ∈ Cu, this means that stakeholder u believes that individual x must be treated as well as individual x or better, i.e. ideally the classiﬁer h classiﬁes such that h(x ) ≥ h(x). This captures all possible responses above. For example, for Serena Williams (s) and Jacob Blake (b), if stakeholder u responds:
2Sidney Morgenbesser, following the Columbia University campus protests in the 1960s, reportedly said that the police had treated him unjustly, but not unfairly. He said that he was treated unjustly because the police hit him without provocation — but not unfairly, because the police were doing the same to everyone else as well.
3Though we develop our formalism as a committee of stakeholders, note that it permits the special case of a single subjective stakeholder, which we make use of in our behavioral study.
3

1. No constraint ⇔ (s, b) Cu nor (b, s) Cu;
2. Williams as well as Blake ⇔ (b, s) ∈ Cu;
3. Blake as well as Williams ⇔ (s, b) ∈ Cu; or
4. Treated similarly ⇔ (s, b) ∈ Cu and (b, s) ∈ Cu (since if h(b) ≥ h(s) and h(s) ≥ h(b), then h(s) = h(b)).
We impose no structure on how stakeholders form their views nor on the relationship between the views of diﬀerent stakeholders — i.e. the sets {Cu}u∈U are allowed to be arbitrary (for example, they need not satisfy a triangle inequality), and need not be mutually consistent. We write C = ∪u Cu .
We then formulate an optimization problem constrained by these pairwise fairness constraints. Since it is intractable to require that all constraints in C be satisﬁed exactly, we formulate two diﬀerent “knobs” with which we can quantitatively relax our fairness constraints.
For γ > 0 (our ﬁrst knob), we say that the classiﬁcation of an ordered pair of individuals (x, x ) ∈ C satisﬁes γ-fairness if the probability of positive classiﬁcation for x plus γ is no smaller than the probability of positive classiﬁcation for x, i.e. E[h(x )] + γ ≥ E[h(x)]. In this expression, the expectation is taken only over the randomness of the classiﬁer h. Equivalently, a γ-fairness violation corresponds to the classiﬁcation of an ordered pair of individuals (x, x ) ∈ C if the diﬀerence between these probabilities of positive classiﬁcation is greater than γ, i.e. E[h(x) − h(x )] > γ. Thus, γ acts as a buﬀer on how likely it is that x be classiﬁed worse than x before a fairness violation occurs. For example, if Blake (b) receives a good label (i.e. no penalty) 80% of the time and Williams (s) 50% of the time, then for γ = 0.1 this constitutes a γ-fairness violation for the ordered pair (b, s) ∈ C, since E[h(b) − h(s)] = 0.3 ≥ 0.1 = γ.
We might ask that for no pair of individuals do we have a γ-fairness violation: max(x,x )∈C E[h(x)− h(x )] ≤ γ. On the other hand, we could ask for the weaker constraint that over a random draw of a pair of individuals, the expected fairness violation is at most η (our second knob): E(x,x )∼P 2[(h(x) − h(x )) · 1[(x, x ) ∈ C]] ≤ η. We can also combine both relaxations to ask that the in expectation over random pairs, the “excess” fairness violation, on top of an allowed budget of γ, is at most η. For example, as above, if Blake receives a good label 80% of the time and Williams 50%, for γ = 0.1, the umpire classiﬁer would pick up 0.2 excess fairness violation for (b, s) ∈ C. In Section 2, we weight these excess fairness violations by the proportion of stakeholders who agree with the corresponding fairness constraint and mandate their sum be less than η. Subject to these constraints, we would like to ﬁnd the distribution over classiﬁers that minimizes classiﬁcation error: given a setting of the parameters γ and η, this deﬁnes a benchmark with which we would like to compete.
Our Theoretical Results Even absent fairness constraints, learning to minimize 0/1 loss (even over linear classiﬁers) is computationally hard in the worst case (see e.g. 10, 9). Despite this, learning seems to be empirically tractable in the real world. To capture the additional hardness of learning subject to fairness constraints, we follow several recent papers [1, 19] in aiming to develop oracle eﬃcient learning algorithms. Oracle eﬃcient algorithms are assumed to have access to an oracle (realized in experiments using a heuristic — see the next section) that can solve weighted classiﬁcation problems. Given access to such an oracle, oracle eﬃcient algorithms must run in polynomial time. We show that our fairness constrained learning problem is computationally no harder than unconstrained learning by giving such an oracle eﬃcient algorithm (or reduction),
4

and show moreover that its guarantees generalize from in-sample to out-of-sample in the usual way — with respect to both accuracy and the frequency and magnitude of fairness violations. Our algorithm is simple and amenable to implementation, and we use it in our experimental results.
Our Experimental Results We implement our algorithm and run a set of experiments on the COMPAS recidivism prediction dataset, using fairness constraints elicited from 43 human subjects. We establish that our algorithm converges quickly (even when implemented with fast learning heuristics, rather than “oracles”). We also explore the Pareto curves trading oﬀ error and fairness violations for diﬀerent human subjects, and ﬁnd empirically that there is a great deal of variability across subjects in terms of their conception of fairness, and in terms of the degree to which their expressed preferences are in conﬂict with accurate prediction. We ﬁnd that most of the diﬃculty in balancing accuracy with the elicited fairness constraints can be attributed to a small fraction of the constraints.
1.2 Related Work
Our work is related to existing notions of individual fairness like [7, 16] that conceptualize fairness as a set of constraints binding on pairs of individuals. In particular the notion of metric fairness proposed in [7] is closely related, but distinct from the fairness notions we elicit in this work. In particular: 1) We allow for constraints that require that individual A be treated better than or equal to individual B, whereas metric fairness constraints are symmetric, and only allow constraints of the form that A and B be treated similarly. In this sense our notion is more general. 2) We elicit binary judgements between pairs of individuals, whereas metric fairness is deﬁned as a Lipschitz constraint on a real valued metric. In this sense our notion is more restrictive, although (we believe) easier to elicit.
The most technically related piece of work is Rothblum and Yona [33], who prove similar generalization guarantees to ours for a relaxation of metric fairness: our deﬁnition is slightly more general, and our generalization guarantee somewhat tighter, but technically the results are closely related. Our conceptual focus and main results are quite diﬀerent, however: for general learning problems, they prove worst-case hardness results, whereas we derive practical algorithms in the oracle-eﬃcient model, and empirically evaluate them on real user data. The concurrent work of Lahoti et al. [26] makes a similar observation about guaranteeing fairness with respect to an unknown metric, although their aim is the orthogonal goal of fair representation learning.
Dwork et al. [7] ﬁrst proposed the notion of individual metric-fairness that we take inspiration from, imagining fairness as a Lipschitz constraint on a randomized algorithm, with respect to some “task-speciﬁc metric”. Since the original proposal, the question of where the metric should come from has been one of the primary obstacles to its adoption, and the focus of subsequent work. Zemel et al. [40] attempt to automatically learn a representation for the data (and hence, implicitly, a similarity metric) that causes a classiﬁer to label an equal proportion of two protected groups as positive. Kim et al. [22] consider a group-fairness like relaxation of individual metricfairness, asking that on average, individuals in pre-speciﬁed groups are classiﬁed with probabilities proportional to the average distance between individuals in those groups. They show how to learn such classiﬁers given access to an oracle which can evaluate the distance between two individuals according to the metric. Compared to our work, they assume the existence of a fairness metric which can be accessed using a quantitative oracle, and they use this metric to deﬁne a statistical rather than individual notion of fairness. Gillen et al. [13] assumes access to an oracle
5

which simply identiﬁes fairness violations across pairs of individuals. Under the assumption that the oracle is exactly consistent with a metric in a simple linear class, Gillen et al. [13] gives a polynomial time algorithm to compete with the best fair policy in an online linear contextual bandits problem. In contrast to Gillen et al. [13], we make essentially no assumptions at all on the structure of the “fairness” constraints. Ilvento [15] studies the problem of metric learning with the goal of using only a small number of numeric valued queries, which are hard for human beings to answer, relying more on comparison queries. In contrast with Ilvento [15], we do not attempt to learn a metric, and instead directly learn a classiﬁer consistent with the elicited pairwise fairness constraints.

2 Problem Formulation

Let S denote a set of labeled examples {zi = (xi, yi)}ni=1, where xi ∈ X is a feature vector and yi ∈ Y is a label. We will also write SX = {xi}ni=1 and SY = {yi}ni=1. Throughout the paper, we will restrict attention to binary labels, so let Y = {0, 1}. Let P denote the unknown distribution over X × Y . Let

H denote a hypothesis class containing binary classiﬁers h : X → Y . We assume that H contains a

constant classiﬁer (which will imply that the “fairness constrained” ERM problem that we deﬁne

is always feasible). We’ll denote classiﬁcation error of hypothesis h by err(h, P ) := Pr(x,y)∼P (h(x) y)

and

its

empirical

classiﬁcation

error

by

err(h, S)

:=

1 n

n i=1

1(h(x

i

)

yi ).

We assume there is a set of one or more stakeholders U , such that each stakeholder u ∈ U is

identiﬁed with a set of ordered pairs (x, x ) of individuals Cu ⊆ X 2: for each (x, x ) ∈ Cu, stake-

holder u thinks that x should be treated as well as x or better, i.e. ideally that for the learned

classiﬁer h, the classiﬁcation h(x ) ≥ h(x) (we will ask that this hold in expectation if the clas-

siﬁer is randomized, and will relax it in various ways). For each ordered pair (x, x ), let wx,x

be the fraction of stakeholders who would like individual x to be treated as well as x : that is, wx,x = |{u|(x,|xU)|∈Cu}| . Note that if (x, x ) ∈ Cu and (x , x) ∈ Cu, then the stakeholder wants x and x to

be treated similarly in that ideally h(x) = h(x ).

In practice, we will not have direct access to the sets of ordered pairs Cu corresponding to the

stakeholders u, but we may ask them whether particular ordered pairs are in this set (see Section

5 for details about how we actually query human subjects). We model this by imagining that we present each stakeholder with a random set of pairs A ⊆ [n]2, and for each ordered pair (xi, xj),

ask if xj should not be treated worse than xi; we learn the set of ordered pairs in A ∩ Cu for each u. Deﬁne the empirical constraint set Cˆu = {(xi , xj ) ∈ Cu}∀(i,j)∈A and wˆ xixj = |{u|(x,|xU)|∈Cˆu}| , if (i, j) ∈ A and 0 otherwise. We write that Cˆ = ∪uCˆu. For brevity, we will sometimes write wij instead of wxi,xj .

Note that wˆ ij = wij for every (i, j) ∈ A.

Our goal will be to ﬁnd the distribution over classiﬁers from H that minimizes classiﬁcation

error, while satisfying the stakeholders’ fairness preferences, captured by the constraints C. To do

so, we’ll try to ﬁnd D, a probability distribution over H, that minimizes the training error and satisﬁes the stakeholders’ empirical fairness constraints, Cˆ . For convenience, we denote the expected

classiﬁcation error of D as err(D, P ) := Eh∼D[err(h, P )] and likewise its expected empirical classiﬁ-

cation error as err(D, S) := Eh∼D[err(h, S)]. We say that any distribution D over classiﬁers satisﬁes

(γ, η)-approximate subjective fairness if it is a feasible solution to the following constrained em-

6

pirical risk minimization problem:

min err(D, S)

(1)

D∈∆H,αij ≥0

such that ∀(i, j) ∈ [n]2 : E h(xi) − h(xj ) ≤ αij + γ

(2)

h∼D

wˆ ij αij ≤ η. (3) (i,j)∈[n]2 |A|

This “Fair ERM” problem, whose feasible region we denote by Ω(S, wˆ , γ, η), has decision vari-

ables D and {αij}, representing the distribution over classiﬁers and the “fairness violation” terms

for each pair of training points, respectively. The parameters γ and η are constants which rep-

resent the two diﬀerent “knobs” we have at our disposal to quantitatively relax the fairness con-

straint, in an ∞ and 1 sense, respectively. The parameter γ deﬁnes, for any ordered pair (xi, xj), the maximum diﬀerence between the

probabilities that xi and xj receive positive labels without constituting a fairness violation. The

parameter αij captures the “excess fairness violation” beyond γ for (xi, xj). The parameter η upper

bounds the sum of these allotted excess fairness violation terms αij, each weighted by the propor-

tion of judges who perceive they ought to be treated similarly wˆ ij and normalized with the total

number of pairs presented |A|. Thus, η bounds the expected degree of dissatisfaction of the panel

of stakeholders U , over the random choice of an ordered pair (xi, xj) ∈ A and the randomness of their classiﬁcation. We iterate over all (i, j) ∈ [n]2 (not just those in Cˆ ) because wˆ ij = 0 if no judge

prefers xi should be classiﬁed as well as xj.

To better understand γ and η, we consider them in isolation. First, suppose we set γ = 0. Then, any diﬀerence in probabilities of positive classiﬁcation between pairs is deemed a fairness violation. So, if we choose (D, {αij}) such that the sum of weighted diﬀerences in positive classiﬁcation

probabilities exceeds η, i.e.

wˆ ij Eh∼D [h(xi ) − h(xj )] |A| > η,
(i ,j )∈[n]2

then this is an infeasible solution. For example, 50% of stakeholders think that Serena Williams (s) should be treated as well as James Blake (b), 70% of stakeholders think Williams should be treated as well as John McEnroe (m), and no other constraints (|A| = 6); if Williams receives a good label 50% of the time, Blake 80%, McEnroe 90%, and η = 0.07, this is an η-fairness violation, since

(wˆ bs E[h(b) − h(s)] + wˆ ms E[h(m) − h(s)]) /|A| = (0.5(0.8 − 0.5) + 0.7(0.9 − 0.5)) /6 ≈ 0.071 > 0.07 = η.
Second, suppose that η = 0. Then, for any (xi, xj) ∈ C (for which wˆ ij > 0), if the expected diﬀerence in labels exceeds γ, i.e. Eh∼D [h(xi) − h(xj )] > γ, then this is an infeasible solution.
2.1 Fairness Loss
Our goal is to develop an algorithm that will minimize its empirical error err(D, S), while satisfying the empirical fairness constraints Cˆ . The standard VC dimension argument states that empirical classiﬁcation error will concentrate around the true classiﬁcation error: we hope to show the same kind of generalization for fairness as well. To do so, we ﬁrst deﬁne fairness loss with respect to our elicited fairness preferences here.

7

For some ﬁxed randomized hypothesis D ∈ ∆H and w, deﬁne γ-fairness loss between an ordered pair as

ΠD,w,γ ((x, x )) = wx,x max 0, E h(x) − h(x ) − γ
h∼D
For a set of pairs M ⊂ X × X , the γ-fairness loss of M is deﬁned to be:

1

ΠD,w,γ (M) = |M|

ΠD,w,γ ((x, x ))

(x,x )∈M

This is the expected degree to which the diﬀerence in classiﬁcation probability for a randomly

selected pair exceeds the allowable budget γ, weighted by the fraction of stakeholders who think

that x should be treated as well as x. By construction, the empirical fairness loss is bounded

by η (i.e. ΠD,w,γ (M) ≤

wˆ ij αij ij |A|

≤ η),

and

we

show

in

Section

4,

the

empirical

fairness

should

concentrate around the true fairness loss ΠD,w,γ (P ) := Ex,x ∼P 2 ΠD,w,γ (x, x ) .

2.2 Cost-sensitive Classiﬁcation

In our algorithm, we will make use of a cost-sensitive classiﬁcation (CSC) oracle. An instance of

CSC problem can be described by a set of costs {(xi, ci0, ci1)}ni=1 and a hypothesis class, H. Costs

ci0 and ci1 correspond to the cost of labeling xi as 0 and 1 respectively. Invoking a CSC oracle on

{(xi, ci0, ci1)}ni=1 returns a hypothesis h∗ such that h∗ ∈ argminh∈H

n i=1

h(xi)ci1 + (1 − h(xi)) ci0

. We say

that an algorithm is oracle-eﬃcient if it runs in polynomial time assuming access to a CSC oracle.

3 Empirical Risk Minimization

In this section, we give an oracle-eﬃcient algorithm 1 for approximately solving our (in-sample) constrained empirical risk minimization problem. Details are deferred to the supplement. We prove the following theorem:

Theorem 3.1. Fix parameters ν, Cτ , Cλ that serve to trade oﬀ running time with approximation error.

√

2

There is an eﬃcient algorithm that makes T = 2Cλ loνg(n)+Cτ CSC oracle calls and outputs a solution

(Dˆ , αˆ) with the following guarantee. The objective value is approximately optimal:

err(Dˆ , S) ≤ min err(D, S) + 2ν.
(D,α)∈Ω(S,wˆ ,γ,η)

And the constraints are approximately satisﬁed: Eh∼Dˆ [h(xi) − h(xj )] ≤ αˆij + γ + 1+C2λν , ∀(i, j) ∈ [n]2 and |A1| (i,j)∈[n]2 wˆ ij αˆij ≤ η + 1+C2τν .

3.1 Outline of the Solution
We frame the problem of solving our constrained ERM problem (equations (1) through (3)) as ﬁnding an approximate equilibrium of a zero-sum game between a primal player and a dual player, trying to minimize and maximize respectively the Lagrangian of the constrained optimization problem.

8

The Lagrangian for our optimization problem is

L(D, α, λ, τ) = err(D, S) +

λij E [h(xi) − h(xj )] − αij − γ

(i ,j )∈[n]2

h∼D





 1



+ τ  |A|

wij αij − η

 (i,j)∈[n]2



For the constraint in equation (2), corresponding to the γ-fairness violation for each ordered
pair of individuals (xi, xj), we introduce a dual variable λij. For the constraint (3), which corresponds to the η-fairness violation over all pairs of individuals, we introduce a dual variable
of τ. For brevity, we deﬁne vectors λ ∈ Λ and α which are made up of all the multipliers λij and the excess fairness violation allotments αij, respectively. The primary player’s action space is (D, α) ∈ (∆H, [0, 1]n2), and the dual player’s action space is (λ, τ) ∈ (Rn2, R).
Solving our constrained ERM problem is equivalent to ﬁnding a minmax equilibrium of L:

argmin err(D, S) = argmin max L(D, α, λ, τ)

(D,α)∈Ω(S,wˆ ,γ,η)

D∈∆H,α∈[0,1]n2 λ∈Rn2 ,τ∈R

Because L is linear in terms of its parameters, Sion’s minimax theorem [34] gives us

min

max L(D, α, λ, τ) = max

min L(D, α, λ, τ).

D∈∆H,α∈[0,1]n2 λ∈Rn2 ,τ∈R

λ∈Rn2 ,τ∈R D∈∆H,α∈[0,1]n2

By a classic result of Freund and Schapire [12], one can compute an approximate equilibrium by simulating “no-regret” dynamics between the primal and dual player. “No-regret” meaning that the average regret –or diﬀerence between our algorithm’s plays and the single best play in hindsight– is bounded above by a term that converges to zero with increasing rounds.
In our case, we deﬁne a zero-sum game wherein the primary player’s plays from action space (D, α) ∈ (∆H, [0, 1]n2), and the dual player’s plays from action space (λ, τ) ∈ (Rn≥20, R≥0). In any given round t, the dual player plays ﬁrst and the primal second. The primal player can simply best respond to the dual player (see Algorithm 1).
However, since the dual player plays ﬁrst, they cannot simply best respond to the primal player’s action. The dual player has to anticipate the primal player’s best response in order to ﬁgure out what to play. Ideally, the dual player would enumerate every possible primal play and calculate the best dual response. However, this is intractable. So, the dual player updates dual variables {λ, τ} according to no-regret learning algorithms (exponentiated gradient descent [23] and online gradient descent [41], respectively).
The time-averaged play of both players converges to an approximate equilibrium of the zerosum game, where the approximation is controlled by the regret of the dual player. This approximate equilibrium corresponds to an approximate saddle point for the Lagrangian L, which is equivalent to an approximate solution to the Fair ERM problem.
We organize the rest of this section as follows. First, for simplicity, we show how the primal player updates {D, α} (even though the dual player plays ﬁrst). Second, we show how the dual player updates {λ, τ}. Finally, we prove that these updates are no-regret and relate the regret of the dual player to the approximation of the solution to the Fair ERM problem.

9

3.2 The Primal Player’s Best Response

In each round t, given the actions chosen by the dual player (λt, τt), the primal player needs to best respond by choosing (Dt, αt) such that (Dt, αt) ∈ argminD∈∆H,α∈[0,1]n2 L(D, α, λt, τt). In Lemma 3.2, we separate the optimization problem into two: one optimization over hypothesis D and one over
violation factor α. In Lemma 3.4, the primal player updates the hypothesis D by leveraging a CSC oracle. Given λt, we can set the costs as follows

ci0 = n1 Eh∼D [1(yi 0)]

ci1 = n1 Eh∼D [1 (yi 1)] + (λtij − λtji).

Then, Dt = ht = CSC {(xi, ci0, ci1)}ni=1 (we note that the best response is always a deterministic classiﬁer ht).
As for αt, we show in Lemma 3.3 that the primal player sets αitj = 1 if τt w|Aij| − λtij ≤ 0 and 0 otherwise. We provide the pseudo-code in Algorithm 1.

Algorithm 1 Best Response, BESTρ(λ, τ), for the primal player

Input: training examples S = {xi, yi}ni=1, λ ∈ Λ, τ ∈ T , CSC oracle CSC for i = 1, . . . , n do

if yi = 0 then

Set ci0 = 0

Set

ci1

=

1 n

+

j i λij − λji

else

Set

ci0

=

1 n

Set ci1 = j i λij − λji

D = CSC(S, c)

for (i, j) ∈ [n]2 do

 1 :

τ w|Aij| − λij ≤ 0

αij = 0 : τ w|Aij| − λij > 0.

Output: D, α

Lemma 3.2. For ﬁxed λ, τ, the best response optimization for the primal player is separable, i.e.

argmin L(D, α, λ, τ) = argmin Lρλ1,τ (D) × argmin Lρλ2,τ (α),

D ,α

D

α

where

Lρλ1,τ (D) = err(h, D) +

λij E h(xi) − h(xj )

(i,j)∈[n]2 h∼D

and  

Lρλ2,τ (α) =

 1 λij −αij + τ  |A|

 wij αij 

(i ,j )∈[n]2

 (i,j)∈[n]2



Lemma 3.3.

For ﬁxed λ and τ, the output α

from

BES

Tρ(λ,

τ

)

minimizes

Lρ2
λ,τ

10

Proof. The optimization





argmin Lρλ2,τ = argmin

α

α

 1

λij −αij + τ  |A|

2





wij αij 

2



(i ,j )∈[n]

(i,j)∈[n]

wij

= argmin
α

−λij αij +

τ |A| αij

(i ,j )∈[n]2

(i ,j )∈[n]2

wij

= argmin
α

αij τ |A| − λij .

(i ,j )∈[n]2

Note

that

for

any

pair

(i, j)

∈

[n]2,

the

term

αij

∈

[0, 1].

Thus,

when

the

constant

τ

wij |A|

− λij

≤

0,

we

assign

αij

as

the

maximum

bound,

1,

in

order

to

minimize

Lρ2 .

Otherwise,

when

τ

wij |A|

− λij

>

0,

we assign αij as the minimum bound, 0.

Lemma 3.4.

For ﬁxed λ and τ, the output D

from

BES

Tρ(λ,

τ

)

minimizes

Lρ1
λ,τ

Proof.

argmin Lρλ1,τ
D

= argmin err(D, S) +

λij E h(xi) − h(xj )

D

(i,j)∈[n]2 h∼D

= argmin n1 n Eh∼D [1(h(xi)

D

i=1

yi)] +

λij E [h(xi) − h(xj )]

(i,j)∈[n]2 h∼D

n

 1

= argDmin i=1  n Eh∼D [1(h(xi)



yi)] + λij h(xi) − λjih(xi)

ji

ji



n

 1

= argDmin i=1  n Eh∼D [1(h(xi)



yi)] + h(xi) λij − λji  .

ji



For each i ∈ [n], we assign the cost

cih(xi) = n1 Eh∼D [1(h(xi ) yi )] + h(xi ) λij − λji .

Note that the cost depends on whether yi = 0 or 1. For example, take yi = 1 and h(xi) = 0. The cost

cih(xi) = ci0 = =

1 Eh∼D [1(h(xi)
n

yi)] + h(xi)
ji

1

1

n · 1 + 0 · λij − λji = n

ji

λij − λji

11

3.3 The Dual Player’s No-regret Updates

In order to reason about convergence we need to restrict the dual player’s action space to lie within
a bounded 1 ball, deﬁned by the parameters Cτ and Cλ that appear in our theorem — and serve to trade oﬀ running time with approximation quality:

Λ=

λ

∈

n2
R+

:

λ 1 ≤ Cλ

,T

= {τ ∈ R+ :

τ

1 ≤ Cτ } .

The dual player will use exponentiated gradient descent [23] to update λ and online gradient descent [41] to update τ, where the reward function will be deﬁned as:

rλ(λt) =

λtij E h(xi) − h(xj ) − αij − γ

(i ,j )∈[n]2

h∼D

and  

rλ(τt) = τt  |A1|

 wij αij − η .

 (i,j)∈[n]2



We provide the pseudo-code in Algorithm 2 but defer some of the proofs to the supplement.

Algorithm 2 No-Regret Dynamics

Input: training examples {xi, yi}ni=1, bounds Cλ and Cτ , time horizon T , step sizes µλ and {µtτ }tT=1, Set θ10 = 0 ∈ Rn2 Set τ0 = 0

for t = 1, 2, . . . , T do Set λtij = Cλ 1+ i ,ejx∈p[nθ]2itj−e1xp θit−j1 for all pairs (i, j) ∈ [n]2

Set τt = proj[0,Cτ ]

τt−1 + µtτ

1 |A|

Dt, αt ← BESTρ(λt, τt)

i,j wij αitj−1 − η

for (i, j) ∈ [n]2 do

θitj = θitj−1 + µtλ−1 Eh∼Dt h(xi ) − h(xj ) − αitj − γ

Output:

1 T

T t=1

D

t

Lemma 3.5. For ﬁxed D and α, the best response optimization for the dual player is separable, i.e.

argmax

L(D

,

α

,

λ,

τ

)

=

argmax

Lψ1
D,α

(λ)

×

argmax

Lψ2
D,α

(τ

),

λ∈Λ,τ ∈T

λ∈Λ

τ ∈T

where

Lψ1
D ,α

(λ)

=

λij E h(xi) − h(xj ) − αij − γ

(i ,j )∈[n]2

h∼D

and  

Lψ2

 (τ) = τ 

1

D ,α

 |A|

 wij αij − η .

 (i,j)∈[n]2



12

Lemma 3.6. Running online gradient descent for τt, i.e. τt = proj[0,Cτ] τt−1 + µt−1 · ∇LψD2t,αt τt−1 , with step size µt = √Cτ yields the following regret
T

T

T

√

max LψD2t,αt (τ) − LψD2t,αt τt ≤ Cτ T .

τ∈T t=1 t=1

Proof. First, note that ∇LψD2t,αt τt−1 = W1 ij wij αitj−1 − η and





τt

=

pr oj[0,C

]

 

τ

t

−

1

+ µtτ

 

1

τ 

 W



wi

j

αitj−1

−

η

 

.

ij



From [41], we ﬁnd that the regret of this online gradient descent (translated into the terms of our paper) is bounded as follows:

T

T

2

∇Lψ2

2 T

max Lψ2 (τ) − Lψ2 τt ≤ Cτ + D,α

µt ,

(4)

τ ∈T

Dt ,αt

t=1

Dt ,αt t=1

2µTτ

2

τ

t=1

where the bound on our target τ term is Cτ , the gradient of our cost function at round t is

∇LψD2t,αt τt−1 , and the bound

∇Lψ2
D ,α

= supτ∈T , t∈[T ]

∇LψD2t,αt τ t−1

. To prove the above lemma,

we ﬁrst need to show that this bound

∇Lψ2
D,α

≤ 1.

Since

wij , αij , η

∈

[0, 1]

for

all

pairs

(i, j),

the

Lagrangian

1 |A|

ij wij αij − η =

ij |wAi|j αij − η ≤ 1. For

all t, the gradient

∇LψD2t,αt τ t−1 =

ij wij αitj−1 |A| − η ≤ 1.

Thus,

∇Lψ2
D,α

≤ 1.

Note that if we deﬁne µtτ = √CTτ , then the summation of the step sizes is equal to

T

√

µtτ = Cτ T

t=1

Substituting these two results into inequality (4), we get that the regret

T

T

max
τ ∈T

LψD2t,αt (τ) −

LψD2t,αt τ t

t=1

t=1

≤ Cτ2√ 2 Cτ / T

1√

√

+ 2 Cτ T = Cτ T

Lemma 3.7. Running exponentiated gradient descent for λt yields the following regret:

T

T

max
λ∈Λ

LψD1t,αt (λ) −

LψD1t,αt λt

t=1

t=1

≤ 2Cλ

T log n.

13

Proof. In each round, the dual player gets to charge either some (i, j) constraint or no constraint at all. In other words, he is presented with n2 + 1 options. Therefore, to account for the option of not
charging any constraint, we deﬁne vector λ = (λ, 0), where the last coordinate, which will always
be 0, corresponds to the option of not charging any constraint. Next, we deﬁne the reward vector ζt for λ t as





ζt =  E h(xi) − h(xj ) − αitj − γ

, 0 .

 h∼Dt

i,j∈[n]2 

Hence, the reward function is r(λ t) = ζt · λ t = LψD1t,αt λt .

The gradient of the reward function is

∇r(λ t) = ∇r(λt) i,j∈[n2] , 0 = ζt, 0
Note that the L-∞ norm of the gradient is bounded by 1, i.e. ∇r(λ t) ∞ ≤ 1
because for any t, each respective component of the gradient, E h(xi) − h(xj ) −αitj −γ, is bounded
h∼D t
by 1.

Here, by the regret bound of [23], we obtain the following regret bound:

T

T

max
λ∈Λ

LψD1t,αt (λ) −

LψD1t,αt (λt)

t=1

t=1

log n

2

2

≤ µ + µ λ 1 ∇r(λ ) ∞ T

≤ loµg n + µCλ2T .

If we take µ = C1λ loTgn , the regret is bounded as follows:

T

T

max

LψD1t,αt (λ) −

Lψ1
Dt ,αt

(λt

)

≤

2Cλ

T log n.

(5)

λ∈Λ t=1 t=1

Remark 3.8. If the primal learner’s approximate best response satisﬁes

T
L Dt, αt, λt, τt
t=1

T

− min

L D, α, λt, τt

D∈∆(H),α∈[0,1]n2 t=1

≤ ξρT

along with dual player’s regret of ξρT , then D¯ , α¯, λ¯, τ¯ is an ξρ + ξψ -approximate solution

14

Theorem 3.9. Let Dˆ , αˆ, λˆ, τˆ be a v-approximate solution to the Lagrangian problem. More speciﬁcally,

L Dˆ , αˆ, λˆ, τˆ ≤ min L D, α, λˆ, τˆ + v,
D ∈∆(H),α ∈[0,1]n2
and L(Dˆ , αˆ, λˆ, τˆ) ≥ max L Dˆ , αˆ, λ, τ − v.
λ∈Λ,τ ∈T
Then, err Dˆ , S ≤ OP T + 2v. And as for the constraints, we have

E h(xi) − h(xj ) ≤ αˆij + γ + 1 + 2v , ∀(i, j) ∈ [n]2

h∼Dˆ

Cλ

1

1 + 2v

|A|

wˆ ij αˆij ≤ η + Cτ .

(i ,j )∈[n]2

Proof. Let (D∗, α∗) = argmin(D,α)∈Ω(S,wˆ,γ,η) err(D, S), the optimal solution to the Fair ERM. Also, deﬁne

penaltyS,w (D, α, λ, τ) := λij E h(xi) − h(xj )
(i,j) h∼D

− αij − γ





+ τ  1  |A|

wˆ ij αij − η . 

(i,j)

Note that for any D and α, maxλ∈Λ,τ∈T penaltyS,wˆ (D, α, λ, τ) ≥ 0 because one can always set λ = 0 and τ = 0.

max L Dˆ , αˆ, λ, τ
λ∈Λ,τ ∈T

≤ L Dˆ , αˆ, λˆ, τˆ + v ≤ min L D, αλˆ, τˆ
D ∈∆(H),α ∈[0,1]n2

+ 2v

≤ L D∗, α∗, λˆ, τˆ + 2v

= err (D∗, S) + penaltyS,wˆ D∗, α∗, λˆ, τˆ + 2v ≤ err (D∗, S) + 2v

The ﬁrst inequality and the third inequality are from the deﬁnition of v-approximate saddle point, and the second to last equality comes from the fact that (D∗, a∗) is a feasible solution.
Now, we consider two cases when(Dˆ , αˆ) is a feasible solution and when it’s not.

1. Dˆ , αˆ ∈ Ω (S, wˆ , γ, η)

In this case, maxλ∈Λ,τ∈T penaltyS,wˆ Dˆ , αˆ, λ, τ = 0 because by the deﬁnition of being a feasi-

ble solution, we have Eh∼D h(xi) − h(xj ) ≤ αij + γ, ∀(i, j) ∈ [n]2 and

1 |A|

(i,j)∈[n]2 wˆ ij αij ≤ η. Hence, maxλ∈Λ,τ∈T L Dˆ , αˆ, λ, τ = err Dˆ , S . Therefore, we have err Dˆ , S ≤

err (D∗, S) + 2v.

15

2. Dˆ , αˆ Ω (S, wˆ , γ, η)

max L Dˆ , αˆ, λ, τ = err Dˆ , S + max penaltyS,wˆ Dˆ , αˆ, λ, τ .

λ∈Λ,τ ∈T

λ∈Λ,τ ∈T

Therefore, err Dˆ , S ≤ err (D∗, S) + 2v because

max penaltyS,wˆ Dˆ , αˆ, λ, τ ≥ 0.
λ∈Λ,τ ∈T

Now, we show that even when (Dˆ , αˆ) is not a feasible solution, the constraints are violated only by so much. Note that
max L(Dˆ , αˆ, λ, τ)
λ∈Λ,τ ∈T
= err(Dˆ , S) + max penaltyS,wˆ (Dˆ , αˆ, λ, τ) ≤ err(D∗, S) + 2v
λ∈Λ,τ ∈T

Therefore,

max penaltyS,wˆ (Dˆ , αˆ, λˆ, τˆ) ≤ err(D∗, S) − err(Dˆ , S) + 2v
λ∈Λ,τ ∈T
max penaltyS,wˆ (Dˆ , αˆ, λˆ, τˆ) ≤ 1 + 2v
λ∈Λ,τ ∈T

Let λ∗, τ∗ = BESTψ Dˆ , αˆ , which minimizes the function as shown in Lemma A.3 and A.4. Now, consider





λ∗ij hE∼D h(xi) − h(xj ) − αij − γ + τ∗  |A1|

wˆ ij αij − η ≤ 1 + 2v 

(i,j)

(i,j)

Say (i∗, j∗) = argmax(i,j)∈[n2] E h(xi) − h(xj ) − αij − γ. Remember that if E h(xi∗) − h(xj∗) −

h∼D

h∼D

αi∗j∗ − γ > 0, then λ∗i∗j∗ = Cτ and 0 for the other coordinates and else, it’s just a zero vector.

Also, τ = Cτ if (i,j) wˆ ij αij − η > 0 and 0 otherwise. Thus,

Therefore, we have and

λ∗ij E h(xi) − h(xj ) − αij − γ ≥ 0
(i,j) h∼D





τ

∗

 

1

 |A|

wˆ ij αij − η ≥ 0 

(i ,j )

1 + 2v

max E h(xi) − h(xj ) − αij − γ ≤
i,j∈[n]2 h∼D

Cλ

,

1

1 + 2v

|A|

wˆ ij αˆij ≤ η + Cτ

(i ,j )∈[n]2

16

Now, the proof of Theorem 3.1 is simply plugging in the best response guarantee of the learner, Lemma 3.3 and 3.4, and the no-regret guarantee of the auditor, Lemma 3.6 and 3.7, into Theorem 3.9. We defer the actual proof to the supplement.

4 Generalization
In this section, we show that fairness loss generalizes out-of-sample. (Error generalization follows from the standard VC-dimension bound, which — because it is a uniform convergece statement is unaﬀected by the addition of fairness constraints. See the supplement for the standard statement.)
Proving that the fairness loss generalizes doesn’t follow immediately from a standard VCdimension argument for several reasons: it is not linearly separable, but deﬁned as an average over non-disjoint pairs of individuals in the sample. The diﬀerence between empirical fairness loss and true fairness loss of a randomized hypothesis D ∈ ∆H is also a non-convex function of the supporting hypotheses h, and so it is not suﬃcient to prove a uniform convergence bound merely for the base hypotheses in our hypothesis class H. We circumvent these diﬃculties by making use of an ε-net argument, together with an application of a concentration inequality, and an application of Sauer’s lemma. Brieﬂy, we show that with respect to fairness loss, the continuous set of distributions over classiﬁers have an ε-net of sparse distributions. Using the two-sample trick and Sauer’s lemma, we can bound the number of such sparse distributions. The end result is the following generalization theorem:
Theorem 4.1. Let S consists of n i.i.d points drawn from P and let M represent a set of m pairs randomly drawn from S × S. Then we have:

Pr sup ΠD,w,γ (M) − E ΠD,w,γ (x, x ) > 2ε

S∼P n m D∈∆H

(x,x )∼P 2

M ∼(S ×S )

≤

8·

e · 2n dk exp

−nε2

+

e · 2n dk exp −8mε2

,

d

32

d

where k = 2lnε(22m) + 1, k = ln8(2εn22) + 1, and d is the VC-dimension of H.
To interpret this theorem, note that the right hand side (the probability of a failure of generalization) begins decreasing exponentially fast in the data and fairness constraint sample parameters n and m as soon as n ≥ Ω(d log(n) log(n/d)) and m ≥ Ω(d log(m) log(n/d)).

5 A Behavioral Study
The framework and algorithm we have provided can be viewed as a tool to elicit and enforce a notion of fairness deﬁned by a collection of stakeholders. In this section, we describe preliminary results from a human-subject study we performed in which pairwise fairness preferences were elicited and enforced by our algorithm. We note that the subjects included in our empirical study were not stakeholders aﬀected by the algorithm we used (the COMPAS algorithm). Thus, our results should not be interpreted as cogent

17

for any policy modiﬁcations to the COMPAS algorithm. We instead report our empirical ﬁndings primarily to showcase the performance of our algorithm and to act as a template for what should be reported if our framework were applied with relevant stakeholders (for example, if fairness preferences about COMPAS data were elicited from inmates).4
5.1 Data
Our study used the COMPAS recidivism data gathered by ProPublica 5 in their celebrated analysis of Northepointe’s risk assessment algorithm [27]. This data consists of defendants from Broward County in Florida between 2013 to 2014. For each defendant the data consists of sex (male, female), age (18-96), race (African-American, Caucasian, Hispanic, Asian, Native American), juvenile felony count, juvenile misdemeanor count, number of other juvenile oﬀenses, number of prior adult criminal oﬀenses, the severity of the crime for which they were incarcerated (felony or misdemeanor), as well as the outcome of whether or not they did in fact recidivate. Recidivism is deﬁned as a new arrest within 2 years, not counting traﬃc violations and municipal ordinance violations.
5.2 Subjective Fairness Elicitation
Figure 1: Screenshot of sample subjective fairness elicitation question posed to human subjects.
We implemented our fairness framework via a web app that elicited subjective fairness notions from 43 undergraduates at a major research university. After reading a document describing the data and recidivism prediction task, each subject was presented with 50 randomly chosen pairs of records from the COMPAS data set and asked whether in their opinion the two individuals should treated (predicted) equally or not. Importantly, the subjects were shown only the features for the individuals, and not their actual recidivism outcomes, since we sought to elicit subjects’ fairness notions regarding the predictions of those outcomes. While absolutely no guidance was given to subjects regarding fairness, the elicitation framework allows for rich possibilities. For example, subjects could choose to ignore demographic factors or criminal histories entirely if they liked, or a subject who believes that minorities are more vulnerable to overpolicing could discount their criminal histories relative to Caucasians in their pairwise elicitations.
4We omit such an empirical study due to the diﬃculty of accessing such stakeholders and leave this for future work. 5The data can be accessed on ProPublica’s Github page here. We cleaned the data as in the ProPublica study, removing any records with missing data. This left 5829 records, where the base rate of two-year recidivism was 46%.
18

For each subject, the pairs they identiﬁed to be treated equally were taken as constraints on error minimization with respect to the actual recidivism outcomes over the entire COMPAS dataset, and our algorithm was applied to solve this constrained optimization problem, using a linear threshold heuristic as the underlying learning oracle [19]. We ran our algorithm with η = 0 and variable γ in Equations (1) through (3), which represents the strongest enforcement of subjective fairness — the diﬀerence in predicted values must be at most γ on every pair selected by a subject. Because the issues we are most interested in here (convergence, tradeoﬀs with accuracy, and heterogeneity of fairness preferences) are orthogonal to generalization — and because we prove VC-dimension based generalization theorems — for simplicity, the results we report are in-sample.
5.3 Results

(a)

(b)

(c)

(d)

(e)

Figure 2: (a) Sample algorithm trajectory for a particular subject at various γ. (b) Sample subjective fairness Pareto curves for a sample of subjects. (c) Scatterplot of number of constraints speciﬁed and number of opposing constraints vs. error at γ = 0.3. (d) Scatterplot of number of constraints where the true labels are diﬀerent vs. error at γ = 0.3. (e) Correlation between false positive rate diﬀerence and γ for racial groups.

Since our algorithm relies on a learning heuristic for which worst-case guarantees are not possible, the ﬁrst empirical question is whether the algorithm converges rapidly on the behavioral data. We found that it did so consistently; a typical example is Figure 2a, where we show the trajectories of model error vs. fairness violation for a particular subject’s data for variable values of the input γ (horizontal lines). After 1000 iterations, the algorithm has converged to the optimal errors subject to the allowed γ.
Perhaps the most basic behavioral questions we might ask involve the extent and nature of

19

subject variability. For example, do some subjects identify constraint pairs that are much harder to satisfy than other subjects? And if so, what factors seem to account for such variation?
Figure 2b shows that there is indeed considerable variation in subject diﬃculty. For each of the 43 subjects, we have plotted the error vs. fairness violation Pareto curves obtained by varying γ from 0 (pairs selected by subjects must have identical probabilistic predictions of recidivism) to 1.0 (no fairness enforced whatsoever). Since our model space is closed under probabilistic mixtures, the worst-case Pareto curve is linear, obtained by all mixtures of the error-optimal model and random predictions. Easier constaint sets are more convex. We see in the ﬁgure that both extremes are exhibited behaviorally — some subjects yield linear or near-linear curves, while others permit huge reductions in unfairness for only slight increases in error, and virtually all the possibilities in between are realized as well. 6
Since each subject was presented with 50 random pairs and was free to constrain as many or as few as they wished, it is natural to wonder if the variation in diﬃculty is explained simply by the number of constraints chosen. In Figure 2c we show a scatterplot of the the number of constraints selected by a subject (x axis) versus the error obtained (y axis) for γ = 0.3 (an intermediate value that exhibits considerable variation in subject error rates) for all 43 subjects. While we see there is indeed strong correlation (approximately 0.69), it is far from the case that the number of constraints explains all the variability. For example, amongst subjects who selected approximately 16 constraints, the resulting error varies over a range of nearly 8%, which is over 40% of the range from the optimal error (0.32) to the worst fairness-constrained error (0.5). More surprisingly, when we consider only the ‘opposing’ constraints, pairs of points with diﬀerent true labels, the correlation (0.489) seems to be weaker. Enforcing a classiﬁer to predict similarly on a pair of points with diﬀerent true labels should increase the error, and yet, it is less correlated with error than the raw number of constraints. This suggests that the variability in subject diﬃculty is due to the nature of the constraints themselves rather than their number or disagreement with the true labels.
It is also interesting to consider the collective force of the 1432 constraints selected by all 43 subjects together, which we can view as a “fairness panel” of sorts. Given that there are already individual subjects whose constraints yield the worst-case Pareto curve, it is unsurprising that the collective constraints do as well. But we can exploit the ﬂexibility of our optimization framework in Equations (1) through constraint (3), and let γ = 0.0 and vary only η, thus giving the learner discretion in which subjects’ constraints to discount or discard at a given budget η. In doing so we ﬁnd that the unconstrained optimal error can be obtained while having the average (exact) pairwise constraint be violated by only roughly 25%, meaning roughly that only 25% of the collective constraints account for all the diﬃculty.
Finally, we can investigate the extent to which behavioral subjective fairness notions align with more standard statistical fairness deﬁnitions, such as equality of false positive rates. For instance, for each subject and a pair of racial groups, we take the absolute diﬀerence in false positive rates of the classiﬁer at γ ∈ {0.0, 0.1, . . . , 1.0} and calculate the correlation coeﬃcient between realized values of γ (which measure violation of subjective unfairness) and the false positive rate diﬀerences. Figure 2e shows the average correlation coeﬃcient across subjects for each pair of racial groups. We note that subjective fairness correlates with a smaller gap between the false positive rates across Caucasians and African Americans: but correlates substantially less for other pairs of racial groups.
6The slight deviations from true convexity are due to approximate rather than exact convergence.
20

We leave a more complete investigation of our behavioral study for future work, including the detailed nature of subject variability and further comparison of behavioral subjective fairness to standard algorithmic fairness notions.
Acknowledgements AR is supported in part by NSF grants AF-1763307, CNS-1253345, and an Amazon Research Award. ZSW is supported in part by an NSF grant FAI-1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, and a Facebook Research Award. Part of this work was completed while ZSW was visiting the Simons Institute for the Theory of Computing at UC Berkeley.
References
[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classiﬁcation. In International Conference on Machine Learning, pages 60–69, 2018.
[2] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna M. Wallach. A reductions approach to fair classiﬁcation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pages 60–69, 2018.
[3] Alekh Agarwal, Miroslav Dud´ık, and Zhiwei Steven Wu. Fair regression: Quantitative deﬁnitions and reduction-based algorithms. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 120–129, 2019.
[4] Lawrence Blum. Moral Perception and Particularity. Cambridge University Press, 1994. ISBN 9780511624605.
[5] Vincent Conitzer, Walter Sinnott-Armstrong, Jana Schaich Borg, Yuan Deng, and Max Kramer. Moral decision making frameworks for artiﬁcial intelligence. In Proceedings of the International Symposium on Artiﬁcial Intelligence and Mathematics (ISAIM), 2018.
[6] Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. CoRR, abs/1808.00023, 2018.
[7] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214–226. ACM, 2012.
[8] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214–226, 2012.
[9] Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. On agnostic learning of parities, monomials, and halfspaces. SIAM Journal on Computing, 39(2): 606–645, 2009.
21

[10] Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of monomials by halfspaces is hard. SIAM Journal on Computing, 41(6):1558–1590, 2012.
[11] Rachel Freedman, Jana Schaich Borg, Walter Sinnott-Armstrong, John P. Dickerson, and Vincent Conitzer. Adapting a kidney exchange algorithm to align with human values. Artiﬁcial Intelligence, 283:103261, 2020.
[12] Yoav Freund and Robert E Schapire. Game theory, on-line prediction and boosting. In COLT, volume 96, pages 325–332. Citeseer, 1996.
[13] Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an unknown fairness metric. In Advances in Neural Information Processing Systems, pages 2600–2609, 2018.
[14] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3315–3323, 2016.
[15] C Ilvento. Metric learning for individual fairness. Manuscript submitted for publication, 2019.
[16] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325–333, 2016.
[17] Anson Kahng, Min Kyung Lee, Ritesh Noothigattu, Ariel D. Procaccia, and ChristosAlexandros Psomas. Statistical foundations of virtual democracy. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 3173–3182, 2019.
[18] Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without discrimination. Knowledge and Information Systems, 33(1):1–33, 2012.
[19] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In International Conference on Machine Learning, pages 2569–2577, 2018.
[20] Michael J Kearns and Umesh Virkumar Vazirani. An introduction to computational learning theory. MIT press, 1994.
[21] Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pages 2569–2577, 2018.
[22] Michael Kim, Omer Reingold, and Guy Rothblum. Fairness through computationallybounded awareness. In Advances in Neural Information Processing Systems, pages 4842–4852, 2018.
[23] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63, 1997.
22

[24] Felicitas Kraemer, Kees van Overveld, and Martin Peterson. Is there an ethics of algorithms? Ethics and Information Technology, 13:251—-260, 2011.
[25] Bogdan Kulynych, David Madras, Smitha Milli, Inioluwa Deborah Raji, Angela Zhou, and Richard Zemel. Participatory approaches to machine learning. International Conference on Machine Learning Workshop, 2020.
[26] Preethi Lahoti, Krishna P. Gummadi, and Gerhard Weikum. Operationalizing individual fairness with pairwise fair representations. CoRR, abs/1907.01439, 2019.
[27] Jeﬀ Larson, Julia Angwin, Lauren Kirchner, and Surya Mattu. How we analyzed the compas recidivism algorithm, Mar 2019.
[28] Min Kyung Lee, Daniel Kusbit, Anson Kahng, Ji Tae Kim, Xinran Yuan, Allissa Chan, Daniel See, Ritesh Noothigattu, Siheon Lee, Alexandros Psomas, and Ariel D. Procaccia. Webuildai: Participatory framework for algorithmic governance. Proc. ACM Hum. Comput. Interact., 3 (CSCW):181:1–181:35, 2019.
[29] Arvind Narayanan. Translation tutorial: 21 fairness deﬁnitions and their politics. In Proc. Conf. Fairness Accountability Transp., New York, USA, 2018.
[30] Seth Neel, Aaron Roth, and Zhiwei Steven Wu. How to use heuristics for diﬀerential privacy. arXiv preprint arXiv:1811.07765, 2018.
[31] Ritesh Noothigattu, Snehalkumar (Neil) S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel D. Procaccia. A voting-based system for ethical decision making. In Proceedings of the 32nd Conference on Artiﬁcial Intelligence, (AAAI), pages 1587–1594, 2018.
[32] Robin Pemantle and Yuval Peres. Concentration of lipschitz functionals of determinantal and other strong rayleigh measures. Combinatorics, Probability and Computing, 23(1):140– 160, 2014.
[33] Guy N Rothblum and Gal Yona. Probably approximately metric-fair learning. arXiv preprint arXiv:1803.03242, 2018.
[34] Maurice Sion et al. On general minimax theorems. Paciﬁc Journal of mathematics, 8(1):171– 176, 1958.
[35] Michael Veale, Max Van Kleek, and Reuben Binns. Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making. In Proceedings of the 2018 chi conference on human factors in computing systems, pages 1–14, 2018.
[36] Sahil Verma and Julia Rubin. Fairness deﬁnitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), pages 1–7. IEEE, 2018.
[37] Pak-Hang Wong. Democratizing algorithmic fairness. Philosophy & Technology, 33:225–244, 2020.
[38] Ariana Yaptangco. Male tennis pros conﬁrm serena’s penalty was sexist and admit to saying worse on the court. Elle, 2018.
23

[39] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, WWW, pages 1171–1180. ACM, 2017.
[40] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013.
[41] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning (ICML2003), 2003.

A Omitted details in Section 3

A.1 Primal player’s best response

Lemma A.1 (Restatement of Lemma 3.2). For ﬁxed λ, τ, the best response optimization for the primal player is separable, i.e.

argmin L(D, α, λ, τ) = argmin Lρλ1,τ (D) × argmin Lρλ2,τ (α),

D ,α

D

α

where and

Lρλ1,τ (D) = err(h, D) +

λij E h(xi) − h(xj )

(i,j)∈[n]2 h∼D

Lρλ2,τ (α) =

λij

(i ,j )∈[n]2

−αij





 1



+ τ  |A|

wij αij 

 (i,j)∈[n]2



Proof. First, note that α is not dependent on D and vice versa. Thus, we may separate the optimization argminD,α L as such:

argmin L(D, α, λ, τ)
D,α





 1



= argDm,αin err(D, S) + (i,j)∈[n]2 λij hE∼D h(xi ) − h(xj ) − αij − γ + τ  |A| (i,j)∈[n]2 wij αij − η

= argmin err(D, S) +

λij E

D

(i,j)∈[n]2 h∼D

h(xi) − h(xj )

×

λij

(i ,j )∈[n]2

−αij





 1



+ τ  |A|

wij αij 

 (i,j)∈[n]2



= argmin Lρλ1,τ (D) × argmin Lρλ2,τ (α)

D

α

24

A.2 Dual player’s best response

Lemma A.2 (Restatement of Lemma 3.5). For ﬁxed D and α, the best response optimization for the dual player is separable, i.e.

argmax

L(D

,

α

,

λ,

τ

)

=

argmax

Lψ1
D,α

(λ)

×

argmax

Lψ2
D,α

(τ

),

λ∈Λ,τ ∈T

λ∈Λ

τ ∈T

where and Proof.

Lψ1
D ,α

(λ)

=

λij E h(xi) − h(xj ) − αij − γ

(i ,j )∈[n]2

h∼D





Lψ2

 (τ) = τ 

1

D ,α

 |A|

 wij αij − η .

 (i,j)∈[n]2



argmax L(D, α, λ, τ)
λ∈Λ,τ ∈T





 1



= aλr∈gΛm,τ∈aTx hE∼D [err(h, S)] + (i,j)∈[n]2 λij hE∼D h(xi ) − h(xj ) − αij − γ + τ  |A| (i,j)∈[n]2 wij αij − η





 1



= argλ∈mΛax (i,j)∈[n]2 λij hE∼D h(xi ) − h(xj ) − αij − γ × argτ∈mT ax τ  |A| (i,j)∈[n]2 wij αij − η

=

argmax

Lψ1
D ,α

(λ)

×

argmax

Lψ2
D ,α

(τ

)

λ∈Λ

τ ∈T

Algorithm 3 Best Response, BESTψ(D, α), for the dual player

Input: training examples S = {xi, yi}ni=1, D ∈ ∆(H), α ∈ [0, 1]n2 λ = 0 ∈ Rn2 (i∗, j∗) = argmax(i,j)∈[n]2 Eh∼D h(xi ) − h(xj ) − αij − γ

if Eh∼D h(xi∗ ) − h(xj∗ ) − αi∗j∗ − γ ≤ 0 then

λi∗j∗ = Cλ

 set τ = 0
Cτ

1 |A|

(i,j)∈[n]2 wij αij − η ≤ 0

o.w.

Output: λ, τ

Lemma A.3.

For ﬁxed D

and

α,

the

output

λ

from

BES

Tψ

(D

,

α)

minimizes

Lψ1
D,α

Proof.

Because

Lψ1
D ,α

is

linear

in

terms

of

λ

and

the

feasible

region

is

the

non-negative

orthant

bounded by 1-norm, the optimal solution must include putting all the weight to the pair (i, j)

where Eh∼D [h(xi) − h(xj ) − αij ] is maximized.

25

Lemma A.4.

For ﬁxed D

and α, the output τ

from

BES

Tψ

(D

,

α)

minimizes

Lψ2
D,α

Proof. Because LψD2,α is linear in terms of τ, the optimal solution is trivially to set τ at either Cτ or 0 depending on the sign.

A.3 No-regret dynamics

Algorithm 4 No-Regret Dynamics

Input: training examples {xi, yi}ni=1, bounds Cλ and Cτ , time horizon T , step sizes µλ and {µtτ }tT=1, Set θ10 = 0 ∈ Rn2 Set τ0 = 0

for t = 1, 2, . . . , T do Set λtij = Cλ 1+ i ,ejx∈p[nθ]2itj−e1xp θit−j1 for all pairs (i, j) ∈ [n]2

Set τt = proj[0,Cτ ]

τt−1 + µtτ

1 |A|

Dt, αt ← BESTρ(λt, τt)

i,j wij αitj−1 − η

for (i, j) ∈ [n]2 do

θitj = θitj−1 + µtλ−1 Eh∼Dt h(xi ) − h(xj ) − αitj − γ

Output:

1 T

T t=1

D

t

Theorem A.5 ([12]). Let (D1, α1), . . . , (DT , αT ) be the primal player’s sequence of actions, and (λ1, τ1), . . . , (λT , τT )

be the dual player’s sequence of actions.

Let

D¯

=

1 T

T t=1

D

t

,

α¯

=

1 T

τ¯

=

1 T

T t=1

τt.

Then,

if

the

regret

of

the

dual

player

satisﬁes

T t=1

α

t

,

λ¯

=

1 T

T t=1

λ

t

,

and

T

T

max L Dt, αt, λt, τt − L Dt, αt, λt, τt ≤ ξψT ,
λ∈Λ,τ∈T t=1 t=1

and the primal player best responds in each round (Dt, αt = argmaxD∈∆(H),α∈[0,1]n2 L D, α, λt, τt ), then (D¯ , α¯, λ¯, τ¯) is an ξψ-approximate solution

A.3.1 Omitted proof of theorem 3.1

proof of theorem 3.1. Observe that

L(D

,

α

,

λ,

τ

)

=

er

r

(D

,

S

)

+

Lψ1
D,α

(λ)

+

Lψ2
D,α

(τ

)

By how we constructed LψD1,α and LψD2,α, combining Lemma 3.6 and 3.7 yields

26

T

T

max L Dt, αt, λt, τt − L Dt, αt, λt, τt
λ∈Λ,τ∈T t=1 t=1

T

T

T

T

= max
τ ∈T

LψD2t,αt (τ) −

Lψ2 t

t

τt

+ max

D ,α

λ∈Λ

LψD1t,αt (λ) −

LψD1t,αt λt

t=1

t=1

t=1

t=1

≤ ξψT ,

√

√

where ξψ = 2Cλ

T log n+Cτ T

T.

Then, theorem A.5 tells us that D¯ , α¯, λ¯, α¯ form a ξψ-approximate equilibrium, where D¯ =

√

2

1 T

T t=1

D

t

,

α¯

=

1 T

T t=1

α

t

,

λ¯

=

1 T

T t=1

λt

,

and

τ¯

=

1 T

T t=1

τ

t

.

And

ﬁnally,

with

T

=

2Cλ

log(n)+Cτ v

results in ξψ = ν, theorem 3.9 gives

err(Dˆ , S) ≤ min err(D, S) + 2ν.
(D,α)∈Ω(S,wˆ ,γ,η)

And as for the constraints,

E h(xi) − h(xj ) ≤ αˆij + γ + 1 + 2ν , ∀(i, j) ∈ [n]2

h∼Dˆ

Cλ

and

1

1 + 2v

|A|

wˆ ij αˆij ≤ η + Cτ .

(i ,j )∈[n]2

B Generalization

B.0.1 Error

Theorem B.1 ([20]). Fix some hypothesis class H and distribution P . Let S ∼ P n be a dataset consisting

of n examples {xi, yi}ni=1 sampled i.i.d. from P . Then, for any 0 < δ < 1, with probability 1 − δ, for every

h ∈ H, we have

  |err(h, P ) − err(h, S)| ≤ O  



V

C

D

I

M

(H)

+

l

o

g

(

1 δ

)



n

 



B.0.2 Fairness Loss
At a high level, our argument proceeds as follows: using McDiarmid’s inequality, for any ﬁxed hypothesis, its empirical fairness loss concentrates around its expectation. This argument extends to an inﬁnite family of hypotheses with bounded VC-dimension via the standard two-sample trick, together with Sauer’s lemma: the only catch is that we need to use a variant of McDiarmid’s inequality that applies to sampling without replacement. However, proving that the fairness loss

27

for each ﬁxed hypothesis h concentrates around its expectation is not suﬃcient to obtain the same result for arbitrary distributions over hypotheses, because the diﬀerence between a randomized classiﬁer’s fairness loss and its expectation is a non-convex function of the mixture weights. To circumvent this issue, we show that with respect to fairness loss, there is an ε-net consisting of sparse distributions over hypotheses. Once we apply Sauer’s lemma and the two-sample trick, there are only ﬁnitely many such distributions, and we can union bound over them.
We begin by stating the standard version of McDiarmid’s inequality:

Theorem B.2 (McDiarmid’s Inequality). Suppose X1, . . . , Xn are independent and f satisﬁes

sup |f (x1, . . . , xn) − f (x1, . . . , xi−1, xˆi, xi+1, . . . , xn)| ≤ ci.
x1,...,xn,xˆi

Then, for any ε > 0,

Pr
X1,...,Xn



f (X1, . . . , Xn) − E [f (X1, . . . , Xn)] ≥ ε ≤ 2 exp −

X1,...,Xn



2ε2

 

ni=1 ci2 

Lemma B.3. Fix a randomized hypothesis D ∈ ∆H. Over the randomness of S ∼ P n, we have

Pr ΠD,w,γ (S × S) − E ΠD,w,γ (S × S) ≥ ε ≤ 2 exp −2nε2

S∼P n

S

Proof. Deﬁne a slightly modiﬁed fairness loss function that depends on each instance instead of a

pair.

1

ΠD,w,γ (x1, x2, . . . , xn) = n2

ΠD,w,γ (xi , xj ) .

(i ,j )∈[n]2

Note that ΠD,w,γ (x1, . . . , xn) = ΠD,w,γ (S ×S). The sensitivity of ΠD,w,γ (x1, x2, . . . , xn) is n1 , so applying McDiarmid’s inequality yields the above concentration.

Theorem B.4. If n ≥ 2lεn2(2) ,

e · 2n dk

−nε2

PSr Ds∈u∆pH ΠD,w,γ (S × S) − xE,x ΠD,w,γ (x, x ) > ε ≤ 8 · d

exp 32

where d is the VC-dimension of H, and k = ln8(2εn22) + 1.

Proof. First, by linearity of expectation, we note that ES ΠD,w,γ (S × S) = Ex,x ΠD,w,γ (x, x ) . Given
S, let DS∗ be some randomized classiﬁer such that ΠDS∗ ,w,γ (S × S) − Ex,x ΠDS∗ ,w,γ (x, x ) > ε; if such hypothesis does not exist, let it be some ﬁxed hypothesis in H. We now use standard symmetrization argument, which allows us to bound the diﬀerence between the fairness loss of our sample S and that of another independent ‘ghost’ sample S = (x1, . . . , xn) instead of bounding the diﬀerence

28

between the empirical fairness loss and its expected fairness loss.

ε S∼P nP,Sr ∼P n Ds∈u∆pH ΠD,w,γ (S × S) − ΠD,w,γ (S × S ) > 2

ε

≥ Pr
S ,S

ΠDS∗ ,w,γ (S × S) − ΠDS∗ ,w,γ (S × S ) > 2

ε

≥ Pr
S ,S

ΠDS∗ ,w,γ (S × S) − xE,x ΠDS∗ ,w,γ (x, x )

> ε and ΠD∗,w,γ (S × S ) − E ΠD∗,w,γ (x, x )
x,x

≤ 2

= E 1 ΠD∗ ,w,γ (S × S) − E ΠD∗ ,w,γ (x, x ) > ε · 1 ΠD∗,w,γ (S × S ) − E ΠD∗,w,γ (x, x ) ≤ ε

S ,S

S

x,x

S

x,x

2

= E 1 ΠD∗ ,w,γ (S × S) − E ΠD∗ ,w,γ (x, x ) > ε · Pr ΠD∗,w,γ (S × S ) − E ΠD∗,w,γ (x, x ) ≤ ε

S

S

x,x

S

S |S

x,x

2

nε2

≥ Pr
S

ΠDS∗ ,w,γ (S × S) − xE,x

ΠDS∗ ,w,γ (x, x )

> ε) · 1 − exp(− 2 )

1 ≥ 2 PSr Ds∈u∆pH ΠD,w,γ (S × S) − xE,x ΠD,w,γ (x, x ) > ε

We used Lemma B.3 for the second to last inequality, and the last inequality follows from the theorem’s condition and the deﬁnition of DS∗ .
Now, imagine sampling S¯ = 2n points from P , and uniformly choosing n points without re-
placement to be S and the remaining n points to be S . This process is equivalent to sampling n
points from P to form S and another independent set of n points from P to form S .

Pr
S¯ ,S ,S
=
S¯

ε Ds∈u∆pH ΠD,w,γ (S × S) − ΠD,w,γ (S × S ) > 2
 Pr S¯ Pr  sup ΠD,w,γ (S × S) − ΠD,w,γ (S
S,S D∈∆H

×S )

> 2ε S¯

Now, instead of bounding the supremum over ∆H, we pay approximation error of ε in order to bound the supremum over H.

Lemma B.5. For some ﬁxed data sample S of size n, any D ∈ ∆H can be approximated by some uniform

mixture

over

k

:=

2 ln(2n2) ε2

+

1

hypotheses

Dˆ

=

1k {h1, . . . , hk}

such

that

for

every

(x, x

)

∈

S

×

S,

E h(x) − h(x ) − E h(x) − h(x ) ≤ ε .

h∼D

h∼Dˆ

Proof. Fix some (x, x ) ∈ S × S. Randomly sample k hypotheses from D: {hi}ki=1 ∼ Dk. Because for each randomly drawn hypothesis hi ∼ D, the diﬀerence in its prediction for x and x is exactly Eh∼D [h(x) − h(x )], Hoeﬀding’s inequality yields that

29

 

1k

 

2k2ε 2

kε 2

hi∼DP,ri∈[k]  hE∼D h(x) − h(x ) − k i=1 hi(x) − hi(x ) > ε  ≤ 2 exp − 4k = 2 exp − 2 .

However, there are n2 ﬁxed pairs in S × S, and if we distribute the failure property between n2 pairs and union bound over all of them, we get

 

1k





2

kε 2

hi∼DP,ri∈[k] (x,mx )a∈Sx×S hE∼D h(x) − h(x ) − k i=1 [hi (x) − hi (x )] > ε  ≤ 2n exp − 2 .

In order to achieve non-zero probability of having

1k

E h(x) − h(x ) −

h∼D

k

[hi(x) − hi(x )] ≤ ε , ∀(x, x ) ∈ S × S,

i=1

we need to make sure 2n2 exp − kε2 2 < 1 or k > 2lnε(22n2) .

Corollary B.6. For some ﬁxed data sample S, any D ∈ ∆H can be approximated by a uniform mixture

of

k

:=

2 ln(2n2) ε2

+

1

hypotheses

Dˆ

=

1k {h1, . . . , hk}

such

that

ΠD,w,γ (S × S) − ΠDˆ ,w,γ (S × S) ≤ ε

Proof. It simply follows from Lemma B.5 and the fact that max 0, Eh∼D h(xi) − h(xj ) − γ is 1Lipschitz in terms of Eh∼D [h(xi) − h(xj )].

Using Corollary B.6 and using Sauer’s lemma that bounds the total number of possible labelings by H over 2n points to be e·d2n d, we can show

Pr S¯ Pr sup ΠD,w,γ (S × S) − ΠD,w,γ (S × S ) > ε S¯

S¯

S,S D∈∆H

2

≤

 Pr S¯ Pr  sup Π ˆ

(S × S) − Π ˆ

(S × S ) > ε + ε

 S¯

¯

S ,S

 Dˆ ∈Hk

D ,w,γ

D,w,γ

2



S

≤

Pr

S¯

·

e · 2n

dk
sup Pr

Πˆ

(S × S) − Π ˆ

(S × S ) > ε + ε S¯

¯

d

Dˆ ∈Hk S,S

D,w,γ

D,w,γ

2

S

Now, for any Dˆ , we will try to bound the probability that the diﬀerence in fairness loss between

S and S is big. We do so by union bounding over cases where both of them deviate from its mean

by too much.

If ΠDˆ ,w,γ (S × S) − ES|S¯ ΠDˆ ,w,γ (S × S)

≤

ε 4

+

ε 2

and

ΠDˆ ,w,γ (S

× S ) − ES|S¯

ΠDˆ ,w,γ (S × S)

≤

ε 4

+

ε 2

,

then

ΠDˆ ,w,γ (S × S) − ΠDˆ ,w,γ (S

×S

)

≤

ε 2

+

ε

.

In

other

words,

30

Pr Π ˆ (S × S) − Π ˆ (S × S ) ≤ ε + ε S¯

S ,S

D,w,γ

D ,w,γ

2

≥ Pr

εε

εε

ΠDˆ ,w,γ (S × S) − ES|S¯ ΠDˆ ,w,γ (S × S) ≤ + and ΠDˆ ,w,γ (S × S ) − ES|S¯ ΠDˆ ,w,γ (S × S) ≤ +

S¯ .

S ,S

42

42

Therefore, by looking at the compliment probabilities, we have

Pr Π ˆ (S × S) − Π ˆ (S × S ) > ε + ε S¯

S ,S

D,w,γ

D ,w,γ

2

≤ Pr

εε

εε

ΠDˆ ,w,γ (S × S) − ES|S¯ ΠDˆ ,w,γ (S × S) > + or ΠDˆ ,w,γ (S × S ) − ES|S¯ ΠDˆ ,w,γ (S × S) > +

S¯

S ,S

42

42

εε ≤ 2 Pr ΠDˆ ,w,γ (S × S) − ES|S¯ ΠDˆ ,w,γ (S × S) > +

S¯ .

S

42

Here, we can’t appeal to McDiarmid’s because S is sampled without replacement from S¯. However, we can use the same technique that [30] leveraged – stochastic covering property can be used to show concentration for sampling without replacement [32].

Deﬁnition B.7 ([32]). Z1, . . . , Zn satisfy the stochastic covering property, if for any I ⊂ [n] and a ≥ a ∈ {0, 1}I coordinate-wise such that ||a − a||1 = 1, there is a coupling ν of the distributions µ, µ of (Zj : j ∈ [n] \ I) conditioned on ZI = a or ZI = a , respectively, such that ν(x, y) = 0 unless x ≤ y coordinate-wise and ||x − y||1 ≤ 1.

Theorem B.8 ([32]). Let (Z1, . . . , Zn) ∈ {0, 1} be random variables such that Pr(

n i=1

Zi

=

k)

=

1

and

the

stochastic covering property is satisﬁed. Let f : {0, 1}n → R be an c-Lipschitz function. Then, for any

ε > 0,

−ε2 Pr (|f (Z1, . . . , Zn) − E [f (Z1, . . . , Zn)]| ≥ ε) ≤ 2 exp 8c2k

Lemma B.9 ([30]). Given a set S of n points, sample k ≤ n elements without replacement. Let Zi = {0, 1} indicate whether ith element has been chosen. Then, (Z1, . . . , Zn) satisfy the stochastic covering property.
Let S¯ = {x1, . . . , x2n}. If we slightly change the deﬁnition of the fairness loss so that it depends on the indicator variables Z1, . . . , Z2n,

1

ΠDˆ ,w,γ,S¯ (Z1, . . . , Z2n) = n2

Zi Zj ΠDˆ ,w,γ (xi , xj ) = ΠDˆ ,w,γ (S × S).

i ,j ∈[2n]2

We see that ΠDˆ ,w,γ,S¯ is n1 -Lipschitz, so by theorem B.8 and lemma B.9, we get

εε

 − 4ε + ε2 2 

 −n 4ε + ε2 2 

Pr
S

ΠDˆ ,w,γ (S × S) − ES|S¯ [ΠDˆ ,w,γ (S × S)] > 4 + 2

S¯ ≤ 2 exp  

8 1 ·n

 = 2 exp 





8

 

 n2







31

Combining everything, we get

Pr sup ΠD,w,γ (S × S) − E [ΠD,w,γ (x, x )] > ε

S D∈∆H

x,x

≤2

Pr

S¯

·

e · 2n

dk
sup Pr

Πˆ

(S × S) − Π ˆ

(S × S ) > ε + ε S¯

¯

d

Dˆ ∈Hk S,S

D,w,γ

D,w,γ

2

S

≤4

Pr S¯

·

e · 2n dk sup Pr

Πˆ

(S × S) − E ¯ Π ˆ

(S × S) > ε + ε S¯

¯

d

Dˆ ∈Hk S

D,w,γ

S|S D,w,γ

42

S

e · 2n dk  −n 4ε + ε2 2 

≤8·

exp 



d  8 

For convenience, we set ε = 2ε .

However, in our case, instead of ﬁnding the average over all pairs in S, we calculate the fairness loss only over m pairs. Fixing S, if m is suﬃciently large, our empirical fairness loss should concentrate around the fairness loss over all the pairs for S.

Lemma B.10. For ﬁxed S, randomly chosen pairs M ⊂ S × S, and randomized hypothesis D,

Pr ΠD,w,γ (M) − ΠD,w,γ (S × S) ≥ ε ≤ exp −2mε2
M ∼(S ×S )m

Proof. Write a random variable La = ΠD,w,γ ((x2a−1, x2a)) for the fairness loss of the ath pair. Note

that

1

E[La] =

n2 ΠD,w,γ (xi, xj ) = ΠD,w,γ (S × S), ∀a ∈ [|M|].

(i ,j )∈[n]2

Therefore, by Hoeﬀding’s inequality, we have

Pr ΠD,w,γ (M) − ΠD,w,γ (S × S) ≥ ε ≤ exp −2mε2 .
M

Lemma B.11. For ﬁxed S and randomly chosen pairs M ⊂ S × S,

Pr sup Π (M) − Π (S × S) ≥ ε ≤ e · 2n dk exp −8mε2 ,

M∼(S×S)m D∈∆H D,w,γ

D,w,γ

d

where k = 2lnε(22m) + 1.

32

Proof.

Pr sup ΠD,w,γ (M) − ΠD,w,γ (S × S) ≥ ε
M∼(S×S)m D∈∆H





≤ Pr  sup ΠDˆ ,w,γ (M) − ΠDˆ ,w,γ (S × S) ≥ ε + 2ε 

M∼(S×S)m Dˆ ∈Hk



≤

Pr

Dˆ ∈Hk M∼(S×S)m

e · 2n dk ≤ d exp

ΠDˆ ,w,γ (M) − ΠDˆ ,w,γ (S × S) −2m (ε + 2ε )2 ,

≥ ε + 2ε

where k = 2l4nε(22m) + 1. The last inequality is from Corollary B.6 and Lemma B.10. For convenience, we just set ε = ε/2.

B.1 Omitted proof of theorem 4.1

Combining theorem B.4 and lemma B.11 yields the following theorem for fairness loss generalization.
proof of theorem 4.1. With probability 1 − 8 · ( e·d2n )dk exp −3n2ε2 + e·d2n dk exp −8mε2 , where k = 2lnε(22m) + 1 and k = ln8(2εn22) + 1, we have

sup ΠD,w,γ (M) − ΠD,w,γ (S × S) ≤ ε
D ∈∆H

and

sup ΠD,w,γ (S × S) − E [ΠD,w,γ (x, x ) ≤ ε.

D ∈∆H

x,x

Then, by triangle inequality,

sup ΠD,w,γ (M) − E [ΠD,w,γ (x, x ) ≤ 2ε.

D ∈∆H

x,x

In other words, with probability 8 · e·d2n dk exp −3n2ε2 + e·d2n dk exp −8mε2 , we have

sup ΠD,w,γ (M) − E ΠD,w,γ (x, x ) > 2ε.

D ∈∆H

x,x

33

