arXiv:2112.05130v1 [cs.CV] 9 Dec 2021

Multimodal Conditional Image Synthesis with Product-of-Experts GANs

Xun Huang

Arun Mallya Ting-Chun Wang NVIDIA

Ming-Yu Liu

Inputs
Snow mountains near a frozen lake with pink clouds
in the sky.
text

Results using a subset of inputs

(g) Result using all inputs (text + sketch + seg)

sketch

(a) text

(d) text + sketch

seg

text (a)

(d)

sketch (b)

(e) (g) (f)

seg (c)

input combinations

(b) sketch (c) seg

(e) text + seg (f) sketch + seg

Figure 1. Given conditional inputs in multiple modalities (the left column), our approach can synthesize images that satisfy all input conditions (the right column, (g)) or any arbitrary subset of the input conditions (the middle column, (a)–(f)) with a single model.

Abstract
Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, sketch, or style reference. They are often unable to leverage multimodal user inputs when available, which reduces their practicality. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. PoE-GAN consists of a productof-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art

in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at this link.
1. Introduction
Conditional image synthesis allows users to use their creative inputs to control the output of image synthesis methods. It has found applications in many content creation tools. Over the years, a variety of input modalities have been studied, mostly based on conditional GANs [12, 20, 32, 41]. To this end, we have various single modality-to-image models. When the input modality is text, we have the text-to-image

model [48, 49, 60, 71, 73, 76, 82]. When the input modality is a segmentation mask, we have the segmentation-to-image model [10, 20, 34, 43, 53, 65]. When the input modality is a sketch, we have the sketch-to-image model [6, 8, 11, 52].
However, different input modalities are best suited for conveying different types of conditioning information. For example, as seen in the ﬁrst column of Fig. 1, segmentation makes it easy to deﬁne the coarse layout of semantic classes in an image—the relative locations and sizes of sky, cloud, mountain, and water regions. Sketch allows us to specify the structure and details within the same semantic region, such as individual mountain ridges. On the other hand, text is well-suited for modifying and describing objects or regions in the image, which cannot be achieved by using segmentation or sketch, e.g. ‘frozen lake’ and ‘pink clouds’ in Fig. 1. Nevertheless, despite this synergy among modalities, prior work has considered image generation conditioned on each modality as a distinct task and studied it in isolation. Existing models thus fail to utilize complementary information available in different modalities. Clearly, a conditional generative model that can combine input information from all available modalities would be of immense value.
Even though the beneﬁts are enormous, the task of conditional image synthesis with multiple input modalities poses several challenges. First, it is unclear how to combine multiple modalities with different dimensions and structures in a single framework. Second, from a practical standpoint, the generator needs to handle missing modalities since it is cumbersome to ask users to provide every single modality all the time. This means that the generator should work well even when only a subset of modalities are provided. Lastly, conditional GANs are known to be susceptible to mode collapse [20, 41], wherein the generator produces identical images when conditioned on the same inputs. This makes it difﬁcult for the generator to produce diverse output images that capture the full conditional distribution when conditioned on an arbitrary set of modalities.
We present Product-of-Experts Generative Adversarial Networks (PoE-GAN), a framework that can generate images conditioned on any subset of the input modalities presented during training, as illustrated in Fig. 1 (a)-(g). This framework provides users unprecedented control, allowing them to specify exactly what they want using multiple complementary input modalities. When users provide no inputs, it falls back to an unconditional GAN model [2, 12, 21, 23–25, 39, 47]. One key ingredient of our framework is a novel product-of-experts generator that can effectively fuse multimodal user inputs and handle missing modalities (Sec. 3.1). A novel hierarchical and multiscale latent representation leads to better usage of the structure in spatial modalities, such as segmentation and sketch (Sec. 3.2). Our model is trained with a multimodal projection discriminator (Sec. 3.4) together with contrastive

losses for better input-output alignment. In addition, we adopt modality dropout for additional robustness to missing inputs (Sec. 3.5). Extensive experiment results show that PoE-GAN outperforms prior work in both multimodal and unimodal settings (Sec. 4), including state-of-the-art approaches speciﬁcally designed for a single modality. We also show that PoE-GAN can generate diverse images when conditioned on the same inputs.
2. Related work
Image Synthesis. Our network architecture design is inspired by previous work in unconditional image synthesis. Our decoder employs some techniques proposed in StyleGAN [24] such as global modulation. Our latent space is constructed in a way similar to hierarchical variational autoencoders (VAEs) [9, 36, 57, 62]. While hierarchical VAEs encode the image itself to the latent space, our network encodes conditional information from different modalities into a uniﬁed latent space. Our discriminator design is inspired by the projection discriminator [40] and multiscale discriminators [34, 65], which we extend to our multimodal setting.
Multimodal Image Synthesis. Prior work [28, 54, 58, 59, 63, 68] has used VAEs [27, 50] to learn the joint distribution of multiple modalities. Some of them [28, 63, 68] use a product-of-experts inference network to approximate the posterior distribution. This is conceptually similar to how our generator combines information from multiple modalities. While their goal is to estimate the complete joint distribution, we focus on learning the image distribution conditioned on other modalities. Besides, our framework is based on GANs rather than VAEs and we perform experiments on high-resolution and large-scale datasets, unlike the above work. Recently, Xia et al. [69] propose a GANbased multimodal image synthesis method named TediGAN. Their method relies on a pretrained unconditional generator. However, such a generator is difﬁcult to train on a complex dataset such as MS-COCO [31]. Concurrently, Zhang et al. [80] propose a multimodal image synthesis method based on VQGAN. The way they combine different modalities is similar to our baseline using concatenation and modality dropout (Sec. 4.2). We will show that our product-of-experts generator design signiﬁcantly improves upon this baseline.
3. Product-of-experts GANs
Given a dataset of images x paired with M different input modalities (y1, y2, ..., yM ), our goal is to train a single generative model that learns to capture the image distribution conditioned on an arbitrary subset of possible modalities p(x|Y), ∀Y ⊆ {y1, y2, ..., yM }. In this paper, we consider four different modalities including text, semantic segmentation, sketch, and style reference. Note that our framework is general and can easily incorporate additional modalities.

S1

S1 ∩ S2

S2

Gaussian 1 Gaussian 2 Product

(a) Intersection of sets

(b) Product of distributions

Figure 2. The product of distributions is analogous to the intersection of sets. The product distribution has a high density where both distributions have relatively high densities.

Learning image distributions conditioned on any subset of M modalities is challenging because it requires a single generator to simultaneously model 2M distributions. Of par-
ticular note, the generator needs to capture the unconditional image distribution p(x) when Y is an empty set, and the unimodal conditional distributions p(x|yi), ∀i ∈ {1, 2, ..., M }, such as the image distribution conditioned on text alone.
These settings have been popular and widely studied in isola-
tion, and we aim to bring them all under a uniﬁed framework.

3.1. Product-of-experts modeling

Intuitively, each input modality adds a constraint the synthesized image must satisfy. The set of images that satisﬁes all constraints is the intersection of the sets each of which satisﬁes one individual constraint. As shown in Fig. 2, we model this by making a strong assumption that the jointly conditioned probability distribution p(x|yi, yj) is proportional to the product of singly conditioned probability distributions p(x|yi) and p(x|yj). Under this setting, for the product distribution to have a high density in a region, each of the individual distributions needs to have a high density in that region, thereby satisfying each constraint. This idea of combining several distributions (“experts”) by multiplying them has been previously referred to as product-of-experts [17].
Our generator is trained to map a latent code z to an image x. Since the output image is uniquely determined by the latent code, the problem of estimating p(x|Y) is equivalent to that of estimating p(z|Y). We use product-of-experts to model the conditional latent distribution

p(z|Y) ∝ p (z) q(z|yi) ,

(1)

yi ∈Y

where p (z) is the prior distribution and each expert q(z|yi) is a distribution predicted by the encoder of that single modality. When no modalities are given, the latent distribution is simply the prior. As more modalities are provided, the number of constraints increases, the space of possible output images shrinks, and the latent distribution becomes narrower.

3.2. Multiscale and hierarchical latent space

Some of the modalities we consider (e.g., sketch, segmentation) are two-dimensional and naturally contain information at multiple scales. Therefore, we devise a hierarchical

Global PoE-Net

Text Encoder

Segmentation Encoder

Decoder

Sketch Encoder

Style Encoder

Snow mountains near a frozen lake with pink
clouds in the sky.

Figure 3. An overview of our generator. The detailed architectures of Global PoE-Net and the decoder are shown in Fig. 4 and Fig. 5 respectively. The architectures of all modality encoders are provided in Appendix B.

latent space with latent variables at different resolutions.

This allows us to directly pass information from each resolu-

tion of the spatial encoder to the corresponding resolution of

the latent space, so that the high-resolution control signals

can be better preserved. Mathematically, our latent code is

partitioned into groups z = (z0, z1, ..., zN ) where z0 ∈ Rc0 is a feature vector and zk ∈ Rck×rk×rk , 1 ≤ k ≤ N are

feature maps of increasing resolutions (rk+1 = 2rk, r1 = 4,

rN is the image resolution). We can therefore decompose

the prior p (z) into

N k=0

p

(z k |z <k )

and

the

experts

q(z|yi)

into

N k=0

q (z k |z <k ,

yi).

This

design

is

similar

to

the

prior

and posterior networks in hierarchical VAEs [9, 36, 57, 62].

The difference is that our encoder encodes the conditional

information in the input modalities rather than the image

itself. Following Eq. (1), we assume the conditional latent

distribution at each resolution is a product-of-experts

p(zk|z<k, Y) ∝ p (zk|z<k) q(zk|z<k, yi) , (2)
yi ∈Y

where p (zk|z<k) = N µk0 , σ0k and q(zk|z<k, yi) = N µki , σik are independent Gaussian distributions with mean and standard deviation parameterized by a neural network.1 It can be shown [67] that the product of Gaussian experts is also a Gaussian p(zk|z<k, Y) = N (µk, σk), with

µk = σk =

µk0

µki

(σk)2 + (σk)2

0

i

i

−1

1

1

(σk)2 + (σk)2 ,

0

i

i

−1

1

1

(σk)2 + (σk)2

.

(3)

0

i

i

3.3. Generator architecture

Figure 3 shows an overview of our generator architecture. We encode each modality into a feature vector which is then aggregated in Global PoE-Net. We use convolutional networks with input skip connections to encode segmentation
1Except for p (z0), which is simply a standard Gaussian distribution.

MLP

MLP

MLP

Product-of-Experts (Eq. (3))

MLP

Loss Add Linear

Inner Product

Loss Add Linear

Inner Product

Inner Product

Inner Product

MLP

Figure 4. Global PoE-Net. We sample a latent feature vector z0 using product-of-experts (Eq. (3) in Sec. 3.2), which is then
processed by an MLP to output a feature vector w.

previous segmentation sketch output features features

CNN

CNN

CNN

Upsample

Conv

Conv LG-AdaIN
Conv LG-AdaIN
Add

Local PoE-Net

Product-of-Experts (Eq. (3))

Local PoE-Net

Instance Norm Local Affine Global Affine

LG-AdaIN Conv 1x1
Linear

Figure 5. A residual block in our decoder. Local PoE-Net samples a latent feature map zk using product-of-experts (Eq. (3) in Sec. 3.2). Here ⊕ denotes concatenation. LG-AdaIN uses w and zk to modulate the feature activations in the residual branch.
and sketch maps, a residual network to encode style images, and CLIP [46] to encode text. Details of all modality encoders are given in Appendix B. The decoder generates the image using the output of Global PoE-Net and skip connections from the segmentation and sketch encoders.
In Global PoE-Net (Fig. 4), we predict a Gaussian q(z0|yi) = N µ0i , σi0 from the feature vector of each modality using an MLP. We then compute the product of Gaussians including the prior p (z0) = N (µ00, σ00) = N (0, I) and sample z0 from the product distribution. We use another MLP to convert z0 to another feature vector w.
The decoder mainly consists of a stack of residual blocks2 [15], each of which is shown in Fig. 5. Local PoENet samples the latent feature map zk at the current resolution from the product of p (zk|z<k) = N µk0, σ0k and q(zk|z<k, yi) = N µki , σik , ∀yi ∈ Y, where µk0 , σ0k is computed from the output of the last layer and µki , σik is computed by concatenating the output of the last layer and the skip connection from the corresponding modality. Note that only modalities that have skip connections (segmentation and sketch, i.e. i = 1, 4) contribute to the computa-
2Except for the ﬁrst layer that convolves a constant feature map and the last layer that convolves the previous output to synthesize the output.

(a) Projection discriminator

(b) Multimodal projection discriminator

Figure 6. Comparison between the standard projection discriminator and our multimodal projection discriminator (MPD).

Image Encoder

MPD MPD MPD MPD

Conditional Encoder

Figure 7. Our multiscale multimodal projection discriminator. The image and other modalities are encoded into feature maps of multiple resolutions and we compute the MPD loss at each resolution.

tion. Other modalities (text and style reference) only provide global information but not local details. The latent feature map zk produced by Local PoE-Net and the feature vector w produced by Global PoE-Net are fed to our local-global adaptive instance normalization (LG-AdaIN) layer,

LG-AdaIN(hk, zk, w) = γw γzk hkσ−(µh(kh)k) + βzk + βw , (4)

where hk is a feature map in the residual branch after convolution, µ(hk) and σ(hk) are channel-wise mean and standard deviation. βw, γw are feature vectors computed from w, while βzk , γzk are feature maps computed from zk. The LGAdaIN layer can be viewed as a combination of AdaIN [18] and SPADE [43] that takes both a global feature vector and a spatially-varying feature map to modulate the activations.
3.4. Multiscale multimodal projection discriminator
Our discriminator receives the image x and a set of conditions Y as inputs and produces a score D(x, Y) = sigmoid(f (x, Y)) indicating the realness of x given Y . Under the GAN objective [12], the optimal solution of f is

f ∗(x, Y) =

q(x) log

+

log q(yi|x) , (5)

p(x) yi∈Y p(yi|x)

unconditional term

conditional term

if we assume conditional independence of different modalities given x. The projection discriminator (PD) [40] proposes to use the inner product to estimate the conditional term. This implementation restricts the conditional term to

be relatively simple, which imposes a good inductive bias that leads to strong empirical results. We propose a multimodal projection discriminator (MPD) that generalizes PD to our multimodal setting. As shown in Fig. 6, the original PD ﬁrst encodes both the image and the conditional input into a shared latent space. It then uses a linear layer to estimate the unconditional term from the image embedding and uses the inner product between the image embedding and the conditional embedding to estimate the conditional term. The unconditional term and the conditional term are summed to obtain the ﬁnal discriminator logits. In our multimodal scenario, we simply encode each observed modality and add its inner product with the image embedding to the ﬁnal loss
f (x, Y) = Linear(Dx(x)) + DyTi (yi)Dx(x) . (6)
yi ∈Y
For spatial modalities such as segmentation and sketch, it is more effective to enforce their alignment with the image in multiple scales [34]. As shown in Fig. 7, we encode the image and spatial modalities into feature maps of different resolutions and compute the MPD loss at each resolution. We compute a loss value at each location and resolution, and obtain the ﬁnal loss by averaging ﬁrst across locations then across resolutions. We name the resulting discriminator as the multiscale multimodal projection discriminator (MMPD) and describe its details in Appendix B.
3.5. Losses and training procedure
Latent regularization. Under the product-of-experts assumption (Eq. (1)), the marginalized conditional latent distribution should match the unconditional prior distribution:

p(z|yi)p(yi)dyi = p(z|∅) = p (z) .

(7)

To this end, we minimize the Kullback-Leibler (KL) divergence from the prior distribution p (z) to the conditional latent distribution p(z|yi) at every resolution

LKL =

ωi ωkEp(z<k|yi)

yi ∈Y

k

DKL(p(zk|z<k, yi)||p (zk|z<k)) ,

(8)

where ωk is a resolution-dependent rebalancing weight and ωi is a modality-speciﬁc loss weight. We describe both weights in detail in Appendix B.
The KL loss also reduces conditional mode collapse since it encourages the conditional latent distribution to be close to the prior and therefore have high entropy. From the perspective of information bottleneck [1], the KL loss encourages each modality to only provide the minimum information necessary to specify the conditional image distribution.

Contrastive losses. The contrastive loss has been widely adopted in representation learning [7, 14] and more recently

in image synthesis [13, 33, 42, 75]. Given a batch of paired vectors (u, v) = {(ui, vi), i = 1, 2, ..., N }, the symmetric cross-entropy loss [46, 79] maximizes the similarity of the vectors in a pair while keeping non-paired vectors apart

Lce(u, v) = − 1 N log 2N
i=1

exp(cos(ui, vi)/τ )

N j=1

exp(cos(ui

,

vj

)/τ

)

1N

exp(cos(ui, vi)/τ )

− 2N i=1 log Nj=1 exp(cos(uj, vi)/τ ) , (9)

where τ is a temperature hyper-parameter. We use two kinds of pairs to construct two contrastive loss terms: the image contrastive loss and the conditional contrastive loss.
The image contrastive loss maximizes the similarity between a real image x and a random fake image x˜ synthesized based on the corresponding conditional inputs:

Lcx = Lce(Evgg(x), Evgg(x˜)) ,

(10)

where Evgg is a pretrained VGG [56] encoder. This loss is similar to the perceptual loss widely used in conditional synthesis but has been found to perform better [42, 75].
The conditional contrastive loss aims to better align images with the corresponding conditions. Speciﬁcally, the discriminator is trained to maximize the similarity between its embedding of a real image x and the conditional input yi.

LDcy = Lce(Dx(x), Dyi (yi)) ,

(11)

where Dx and Dyi are two modules in the discriminator that extract features from x and yi, respectively, as shown in Eq. (6) and Fig. 6b. The generator is trained with the same loss, but using the generated image x˜ instead of the
real image to compute the discriminator embedding,

LGcy = Lce(Dx(x˜), Dyi (yi)) .

(12)

In practice, we only use the conditional contrastive loss for the text modality since it consumes too much GPU memory to use the conditional contrastive loss for the other modalities, especially when the image resolution and batch size are large. A similar image-text contrastive loss is used in XMCGAN [75], where they use a non-symmetric cross-entropy loss that only includes the ﬁrst term in Eq. (9).
Full training objective. In summary, the generator loss LG and the discriminator loss LD can be written as

LG = LGGAN + LKL + λ1Lcx + λ2LGcy ,

(13)

LD = LDGAN + λ2LDcy + λ3LGP ,

(14)

where LGGAN and LDGAN are non-saturated GAN losses [12], LGP is the R1 gradient penalty loss [38], and λ1, λ2, λ3 are weights associated with the loss terms.

Segmentation

Ground truth

SPADE

OASIS

PoE-GAN (Ours)

Figure 8. Qualitative comparison of segmentation-to-image synthesis results on MS-COCO 2017.

Text

Ground truth

DF-GAN

DM-GAN + CL

PoE-GAN (Ours)

A man riding a snowboard down a snow covered slope.

A red stop sign sitting on the side of a road.

Figure 9. Qualitative comparison of text-to-image synthesis results on MS-COCO 2017.

Uncond Text

StyleGAN2 [25] 11.7 —

SPADE-Seg [43]

—

—

pSp-Seg [51]

—

—

SPADE-Sketch [43] —

—

pSp-Sketch [51]

—

—

TediGAN [69]

— 38.4

PoE-GAN (Ours) 10.5 10.1

Seg Sketch All

—

—

—

48.6 —

—

44.1 —

—

— 33.0 —

— 45.8 —

45.1 45.1 45.1 9.9 9.9 8.3

Table 1. Comparison on MM-CelebA-HQ (1024×1024) using FID. We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold.

Modality dropout. By design, our generator, discriminator, and loss terms are able to handle missing modalities. We also ﬁnd that randomly dropping out some input modalities before each training iteration further improves the robustness of the generator towards missing modalities at test time.

Uncond Text

StyleGAN2 [25]

43.6 —

DF-GAN [60]

— 45.2

DM-GAN + CL [73] — 29.9

SPADE-Seg [43]

—

—

VQGAN [10]

—

—

OASIS [53]

—

—

SPADE-Sketch [43] —

—

PoE-GAN (Ours) 43.4 20.5

Seg Sketch All

—

—

—

—

—

—

—

—

—

22.1 —

—

21.6 —

—

19.2 —

—

— 63.7 —

15.8 25.5 13.6

Table 2. Comparison on MS-COCO 2017 (256×256) using FID. We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold.

landscape images. Images are labeled with all input modalities obtained from either manual annotation or pseudolabeling methods. More details about the datasets and the pseudo-labeling procedure are in Appendix A.

4. Experiments
We evaluate the proposed approach on several datasets, including MM-CelebA-HQ [69], MS-COCO 2017 [31] with COCO-Stuff annotations [3], and a proprietary dataset of

4.1. Main results
We compare PoE-GAN with a recent multimodal image synthesis method named TediGAN [69] and also with stateof-the-art approaches speciﬁcally designed for each modality.

A lake in the desert with mountains at a distance.

A waterfall in sunset.

A mountain with pine trees in a starry winter night.

Figure 10. Examples of multimodal conditional image synthesis results produced by PoE-GAN trained on the 1024×1024 landscape dataset. We show the segmentation/sketch/style inputs on the bottom right of the generated images for the results in the ﬁrst row. The results in the second row additionally leverage text inputs, which are shown below the corresponding generated images. Please zoom in for details.

For text-to-image, we compare with DF-GAN [60] and DMGAN + CL [73] on MS-COCO. Since the original models are trained on the 2014 split, we retrain their models on the 2017 split using the ofﬁcial code. For segmentation-to-image synthesis, we compare with SPADE [43], VQGAN [10], OASIS [53], and pSp [51]. For sketch-to-image synthesis, we compare with SPADE [43] and pSp [51]. We additionally compare with StyleGAN2 [25] in the unconditional setting. We use Clean-FID [44] for benchmarking due to its reported beneﬁts over previous implementations of FID [16].3
Results on MM-CelebA-HQ and MS-COCO are summarized in Tab. 1 and Tab. 2, respectively. As shown in Tab. 1, PoE-GAN obtains a much lower FID than TediGAN in all settings. In Appendix C.3, we compare PoE-GAN with TediGAN in more detail and show that PoE-GAN is faster and more general than TediGAN. When conditioned on a single modality, PoE-GAN surprisingly outperforms the state-ofthe-art method designed speciﬁcally for that modality on both datasets, although PoE-GAN is trained for a more general purpose. We believe our proposed architecture and train-
3The baseline scores differ slightly from those in the original papers.

ing scheme are responsible for the improved performance. In Fig. 8 and Fig. 9, we qualitatively compare PoE-GAN with previous segmentation-to-image and text-to-image methods on MS-COCO. We ﬁnd that PoE-GAN produces images of much better quality and can synthesize realistic objects with complex structures, such as human faces and stop signs. More qualitative comparisons are included in Appendix C.6.
In Fig. 10, we show example images generated by our PoE-GAN using multiple input modalities on the landscape dataset. Our model is able to synthesize a wide range of landscapes in high resolution with photo-realistic details. More results are included in Appendix C.6, where we additionally show that PoE-GAN can generate diverse images when given the same conditional inputs.
4.2. Ablation studies
In Tabs. 3 and 4, we analyze the importance of different components of PoE-GAN. We use LPIPS [78] as an additional metric to evaluate the diversity of images conditioned on the same input. Speciﬁcally, we randomly sample two output images conditioned on the same input and report the average LPIPS distance between the two outputs. A higher

Uncond

Text

Segmentation

Sketch

All

Methods (a) Concatenation + modality dropout (b) Ours w/o KL loss (c) Ours w/o modality dropout (d) Ours w/o MMPD (e) Ours w/o image contrastive loss (f) Ours w/o text contrastive loss (g) Ours

FID ↓ 29.3 29.1 30.8 21.5 15.4 15.8 14.9

FID ↓ 26.4 26.6 31.6 20.8 14.5 15.0 13.7

LPIPS ↑ 0.33 0.35 0.50 0.48 0.55 0.56 0.58

FID ↓ 19.4 18.1 21.0 18.3 13.5 13.1 12.9

LPIPS ↑ 0.22 0.21 0.39 0.40 0.46 0.40 0.43

FID ↓ 9.2 9.1 28.0 16.4 10.2 10.0 9.9

LPIPS ↑ 0.11 0.10 0.34 0.36 0.44 0.39 0.37

FID ↓ 7.7 7.7 9.5 16.2 9.5 8.9 8.5

LPIPS ↑ 0.11 0.12 0.30 0.34 0.42 0.38 0.35

Table 3. Ablation study on MM-CelebA-HQ (256×256). The best scores are highlighted in bold and the second best ones are underlined.

Uncond

Text

Segmentation

Sketch

All

Methods (a) Concatenation + modality dropout (b) Ours w/o KL loss (c) Ours w/o modality dropout (d) Ours w/o MMPD (e) Ours w/o image contrastive loss (f) Ours w/o text contrastive loss (g) Ours

FID ↓ 59.1 59.3 86.2 43.1 25.7 27.6 26.6

FID ↓ 30.7 30.5 87.8 40.2 21.3 26.0 22.2

LPIPS ↑ 0.40 0.39 0.58 0.64 0.64 0.66 0.65

FID ↓ 20.4 21.5 19.9 21.7 18.0 17.4 17.1

LPIPS ↑ 0.16 0.16 0.44 0.46 0.50 0.46 0.47

FID ↓ 36.1 33.0 85.1 45.5 37.9 33.5 30.2

LPIPS ↑ 0.27 0.27 0.55 0.57 0.61 0.55 0.58

FID ↓ 16.6 16.9 18.7 21.1 18.5 17.9 17.1

LPIPS ↑ 0.12 0.12 0.47 0.42 0.54 0.43 0.44

Table 4. Ablation study on MS-COCO 2017 (64×64). The best scores are highlighted in bold and the second best ones are underlined.

LPIPS score would indicate more diverse outputs.
First, we compare our product-of-experts generator, row (g) in Tabs. 3 and 4, with a baseline that simply concatenates the embedding of all modalities, while performing modality dropout (missing modality embeddings are set to zero). As seen in row (a), this baseline only works well when all modalities are available. Its FID signiﬁcantly drops when some modalities are missing. Further, the generated images have low diversity as indicated by the low LPIPS score. This is, however, not surprising as previous work has shown that conditional GANs are prone to mode collapse [19, 72, 81].
Row (b) of Tabs. 3 and 4 shows that the KL loss is important for training our model. Without it, our model suffers from low sample diversity and lack of robustness towards missing modalities, similar to the concatenation baseline described above. The variances of individual experts become near zero without the KL loss. The latent code zk then becomes a deterministic weighted average of the mean of each expert, which is equivalent to concatenating all modality embeddings and projecting it with a linear layer. This explains why our model without the KL loss behaves similarly to the concatenation baseline. Row (c) shows that our modality dropout scheme is important for handling missing modalities. Without it, the model tends to overly rely on the most informative modality, such as segmentation in MS-COCO.
To evaluate the proposed multiscale multimodal discriminator architecture, we replace MMPD with a discriminator that receives concatenated images and all conditional inputs. Row (d) shows that MMPD is much more effective than such a concatenation-based discriminator in all settings.
Finally in rows (e) and (f), we show that contrastive losses

are useful but not essential. The image contrastive loss slightly improves FID in most settings, while the text contrastive loss improves FID for text-to-image synthesis.
5. Conclusion
We introduce a multimodal conditional image synthesis model based on product-of-experts and show its effectiveness for converting an arbitrary subset of input modalities to an image satisfying all conditions. While empirically superior than the prior multimodal synthesis work, it also outperforms state-of-the-art unimodal conditional image synthesis approaches when conditioned on a single modality.
Limitations. If different modalities provide contradictory information, PoE-GAN produces unsatisfactory outputs as shown in Appendix D. PoE-GAN trained in the unimodal setting performs better than the model trained in the multimodal setting, when tested in the unimodal setting. This indicates room for improvement in the fusing of multiple modalities, and we leave this for future work.
Potential negative societal impacts. Image synthesis networks can help people express themselves and artists create digital content, but they can undeniably also be misused for visual misinformation [66]. Enabling users to synthesize images using multiple modalities makes it even easier to create a desired fake image. We encourage research that helps detect or prevent these potential negative misuses. We will provide implementations and training data to help forensics researchers detect fake images [4, 64] and develop effective schemes for watermarking networks and training data [74].

Acknowledgements. We thank Jan Kautz, David Luebke, Tero Karras, Timo Aila, and Zinan Lin for their feedback on the manuscript. We thank Daniel Gifford and Andrea Gagliano on their help on data collection.
References
[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017. 5
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. In ICLR, 2019. 2
[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCOStuff: Thing and stuff classes in context. In CVPR, 2018. 6, 12
[4] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola. What makes fake images detectable? understanding properties that generalize. In ECCV, 2020. 8
[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 2017. 12
[6] Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. DeepFaceDrawing: Deep generation of face images from sketches. ACM Transactions on Graphics (TOG), 2020. 2, 12
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 5
[8] Wengling Chen and James Hays. SketchyGAN: Towards diverse and realistic sketch to image synthesis. In CVPR, 2018. 2
[9] Rewon Child. Very deep VAEs generalize autoregressive models and can outperform them on images. In ICLR, 2021. 2, 3
[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 2, 6, 7, 14, 16
[11] Arnab Ghosh, Richard Zhang, Puneet K Dokania, Oliver Wang, Alexei A Efros, Philip HS Torr, and Eli Shechtman. Interactive sketch & ﬁll: Multiclass sketch-to-image translation. In ICCV, 2019. 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 1, 2, 4, 5
[13] Junlin Han, Mehrdad Shoeiby, Lars Petersson, and Mohammad Ali Armin. Dual contrastive learning for unsupervised image-to-image translation. In CVPR, 2021. 5
[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 5
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 4

[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 7
[17] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 2002. 3
[18] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 4
[19] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018. 8
[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 1, 2
[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. 2, 12
[22] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In NeurIPS, 2020. 12
[23] Tero Karras, Miika Aittala, Samuli Laine, Erik Ha¨rko¨nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. 2
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. 2
[25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020. 2, 6, 7, 12, 14
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014. 12
[27] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2
[28] Svetlana Kutuzova, Oswin Krause, Douglas McCloskey, Mads Nielsen, and Christian Igel. Multimodal variational autoencoders for semi-supervised learning: In defense of product-of-experts. arXiv preprint arXiv:2101.07240, 2021. 2
[29] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In CVPR, 2020. 12
[30] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. Controllable text-to-image generation. In NeurIPS, 2019. 15
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 2, 6, 12
[32] Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, and Arun Mallya. Generative adversarial networks for image and video synthesis: Algorithms and applications. Proceedings of the IEEE, 2021. 1
[33] Rui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. DivCo: Diverse conditional image synthesis

via contrastive generative adversarial network. In CVPR, 2021. 5
[34] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, and Hongsheng Li. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. In NeurIPS, 2019. 2, 5
[35] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 12
[36] Lars Maaløe, Marco Fraccaro, Valentin Lie´vin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. NeurIPS, 2019. 2, 3
[37] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In ICML, 2013. 12
[38] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In ICML, 2018. 5, 12
[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018. 2
[40] Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018. 2, 4
[41] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classiﬁer GANs. In ICML, 2017. 1, 2
[42] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In ECCV. Springer, 2020. 5
[43] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In CVPR, 2019. 2, 4, 6, 7, 14, 16
[44] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in FID calculation. arXiv preprint arXiv:2104.11222, 2021. 7
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 12
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4, 5, 12, 14
[47] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In CVPR, 2016. 2
[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 2
[49] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016. 2
[50] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014. 2

[51] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a StyleGAN encoder for image-to-image translation. In CVPR, 2021. 6, 7, 12, 14
[52] Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Scribbler: Controlling deep image synthesis with sketch and color. In CVPR, 2017. 2
[53] Edgar Scho¨nfeld, Vadim Sushko, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. In ICLR, 2020. 2, 6, 7, 14, 16
[54] Yuge Shi, N Siddharth, Brooks Paige, and Philip Torr. Variational mixture-of-experts autoencoders for multi-modal deep generative models. In NeurIPS, 2019. 2
[55] Edgar Simo-Serra, Satoshi Iizuka, Kazuma Sasaki, and Hiroshi Ishikawa. Learning to simplify: fully convolutional networks for rough sketch cleanup. ACM Transactions on Graphics (TOG), 2016. 12
[56] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 5
[57] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder variational autoencoders. In NeurIPS, 2016. 2, 3
[58] Thomas M Sutter, Imant Daunhawer, and Julia E Vogt. Generalized multimodal ELBO. In ICLR, 2020. 2
[59] Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint multimodal learning with deep generative models. In ICLR workshop, 2017. 2
[60] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865, 2020. 2, 6, 7, 14, 15, 16
[61] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, 2017. 13
[62] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020. 2, 3, 12
[63] Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually grounded imagination. In ICLR, 2018. 2
[64] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. CNN-generated images are surprisingly easy to spot... for now. In CVPR, 2020. 8
[65] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. In CVPR, 2018. 2
[66] Mika Westerlund. The emergence of deepfake technology: A review. Technology Innovation Management Review, 2019. 8
[67] Christopher KI Williams, Felix V Agakov, and Stephen N Felderhof. Products of gaussians. In NeurIPS, 2001. 3
[68] Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised learning. In NeurIPS, 2018. 2
[69] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. TediGAN: Text-guided diverse face image generation and manipulation. In CVPR, 2021. 2, 6, 12, 14, 15

[70] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In ICCV, 2015. 12
[71] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Finegrained text to image generation with attentional generative adversarial networks. In CVPR, 2018. 2, 15
[72] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. In ICLR, 2019. 8
[73] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-to-image synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021. 2, 6, 7, 14, 16
[74] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artiﬁcial ﬁngerprinting for generative models: Rooting deepfake attribution in training data. In ICCV, 2021. 8
[75] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-toimage generation. In CVPR, 2021. 5
[76] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. 2
[77] Richard Zhang. Making convolutional networks shiftinvariant again. In ICML, 2019. 12
[78] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 7
[79] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv:2010.00747, 2020. 5
[80] Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, and Hongxia Yang. M6UFC: Unifying multi-modal controls for conditional image synthesis. In NeurIPS, 2021. 2, 14, 15
[81] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In NeurIPS, 2017. 8
[82] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DMGAN: Dynamic memory generative adversarial networks for text-to-image synthesis. In CVPR, 2019. 2, 15

A. Datasets
We evaluate the proposed PoE-GAN approach for multimodal conditional image synthesis on three datasets, including MM-CelebA-HQ [69], MS-COCO [31], and a proprietary dataset of landscape images using four modalities including text, segmentation, sketch, and style reference. The style is extracted from ground truth images and the other modalities are obtained from either human annotation or pseudo-labeling methods. We describe details about each dataset in the following.
MM-CelebA-HQ contains 30,000 images of celebrity faces with a resolution of 1024×1024, which are created from the original CelebA dataset [35] using a procedure that improves image quality and resolution by Karras et al. [21]. Each image is annotated with text, segmentation, and sketch. While the segmentation maps are annotated by humans [29], the text descriptions are automatically generated from the ground truth attribute labels by Xia et al. [69]. On the other hand, sketch maps are generated by Chen et al. [6] using the Photoshop edge extractor and sketch simpliﬁcation [55]. We use the pretrained CLIP [46] text encoder to encode the text before feeding them to the generator. We use the standard train-test split [51, 69] which yields 24,000 images in the training set and 6,000 images in the test set. We use images in the original resolution (1024×1024) in our main experiment and use images downsampled to 256×256 in ablation studies.
MS-COCO contains 123,287 images of complex indoor/outdoor scenes, containing various common objects. We use the segmentation maps provided in COCO-Stuff [3] as the ground truth segmentation maps for the images. In MS-COCO, each image has up to 5 text descriptions. We use the pretrained CLIP text encoder to extract a feature vector per description. We additionally annotate each image with a sketch map produced by running HED [70] edge detector followed by a sketch simpliﬁcation process [55]. We use the 2017 split, which leads to 118,287 training images and 5,000 test images. We use images resized to 256×256 in our main experiment and to 64×64 in ablation studies.
Landscape is a proprietary dataset containing around 10 million landscape images of resolution higher than 1024×1024. It does not come with any manual annotation and we use DeepLab-v2 [5] to produce pseudo segmentation annotation and HED [70] with sketch simpliﬁcation [55] to produce pseudo sketch annotation. For the text annotation, we use the CLIP image embedding as the pseudo text embedding to train our model. We randomly choose 50,000 images as the test set and use the rest of the images as the training set. Images are randomly cropped to 1024×1024 during training.

B. Implementation details

Our implementation is based on Pytorch [45]. We use the Adam optimizer [26] with β1 = 0 and β2 = 0.99 and the same constant learning rate for both the generator and the discriminator. The ﬁnal model weight is given by an exponential moving average of the generator’s weights during the course of training. To stabilize GAN training, we employ leaky ReLU [37] with slope 0.2, R1 gradient penalty [38] with lazy regularization [25] applied every 16 iterations, equalized learning rate [25], anti-aliased resampling [77], and clip the gradient norms at 10. We also limit the range of the Gaussian log variance in product-of-experts layers to (−θ, θ) by applying the activation function θ tanh( θ. ). We use θ = 1 for the prior expert and θ = 10 for experts predicted from input modalities. We use mixed-precision training in all of our experiments and clamp the output of every convolutional/fully-connected layer to ±256 [22]. We use a dropout rate of 0.5 when performing modality dropout. The contrastive loss temperature is initialized at 0.3 and learnable during training. We use the relu5 1 feature in VGG-19 to compute the image contrastive loss.
Inspired by NVAE [62], we rebalance the KL terms of different resolutions. The rebalancing weight ωk in Eq. (8) of the main paper is proportional to the unbalanced KL term and inversely proportional to the resolution:

ωk ∝ 1 Ep(y )[Ep(z<k|y )[

wk hk

i

i

DKL(p(zk|z<k, yi)||p (zk|z<k))]] , (15)

with the constraint that N1 Nk=1 ωk = 1. The rebalancing weights encourage having the amount of information en-
coded in each latent variable. We use a running average of
the rebalancing weights with a decay factor of 0.99.

B.1. Decoder

We introduced our decoder design in Sec. 3.1 of the main paper. Here, we provide additional details. In Global PoE-Net (Fig. 4 of the main paper), each MLP consists of 4 fully-connected layers with a hidden dimension 4 times smaller than the input dimension. Both z0 and w are 512dimensional. The output MLP has two layers with a hidden dimension of 512.
In Local PoE-Net (Fig. 5 of the main paper), each CNN similarly contains 4 convolutional layers with the number of ﬁlters 4 times smaller than the input channel size. The ﬁrst and last convolutions have 1×1 kernels, and the convolutions in the middle have a kernel size of 3. The dimension of w is 512. The dimensions of zk’s are described in Appendix B.4.

B.2. Encoders

Fig. 11 shows the architecture of encoders used in our generator to encode each modality. The text encoder (Fig. 11a)

Hyper-parameters

MM-CelebAHQ (1024 × 1024)

MM-CelebAHQ (256 × 256)

Learning rate Batch size Text KL weight Segmentation KL weight Sketch KL weight Style KL weight Image contrastive loss weight Text contrastive loss weight Base # channels for Dec/Dis Maximum # channels for Dec/Dis Base # channels for Latent Maximum # channels for Latent

0.003 64 0.01 0.01 0.1
0.0005 0.3 0.3 16 512 2 32

0.003 64 0.1 0.1 1
0.005 0.3 0.3 64 512 16 64

MS-COCO (256 × 256)
0.004 256 0.01 0.01 0.1 0.001
3 0.3 128 1024 16 64

Table 5. Hyper-parameters on different datasets.

MS-COCO (64 × 64)
0.004 128 0.1 0.1
1 0.01
3 0.3 256 512 64 64

Landscape (1024 × 1024)
0.004 768 0.05 0.1
1 0.01
3 0.3 32 1024 2 32

Number of GPUs Training time Inference time (per image)

MM-CelebAHQ (1024 × 1024)
16 71h 0.07s

MM-CelebAHQ (256 × 256)
8 35h 0.04s

MS-COCO (256 × 256)
32 85h 0.06s

MS-COCO (64 × 64)
8 76h 0.02s

Landscape (1024 × 1024)
256 101h 0.12s

Table 6. Hardware and training/inference speed on different datasets. We train all models using NVIDIA Tesla V100 GPUs, except for the Landscape model which is trained using NVIDIA Ampere A100 GPUs with 80 GB of memory. The inference time is evaluated on a workstation with a single NVIDIA TITAN RTX GPU.

is a 4-layer MLP with dimension 512 that processes the CLIP embedding of a caption. The segmentation and sketch encoders are CNNs with skip connections from the input, which are illustrated in Fig. 11b and Fig. 11c. The segmentation or sketch map is downsampled multiple times. The embeddings of the downsampled map are added to the intermediate outputs of the corresponding convolutional layers. The intermediate outputs of convolutional layers are provided to the decoder via skip connections. The style encoder for encoding the reference style image is a residual network with instance normalization [61]. As shown in Fig. 11d, we obtain the style code by concatenating the mean and standard deviation of the output of every residual block.
B.3. Discriminator
As shown in Fig. 7 of the main paper, our discriminator encodes the image and the other modalities into multiscale feature maps and computes the MPD loss at each scale. We

use a residual network to encode the image. For encoding other modalities, we use the same architecture as described in the previous section (Appendix B.2 and Fig. 11). However, the parameters are not shared with those encoders used in the generator. For encoders (the style encoder or the text encoder) that only outputs a single feature vector, we spatially replicate the feature vector to different resolutions.
B.4. Hyper-parameters
Tab. 5 shows the hyper-parameters used on all the benchmark datasets, including learning rate, batch size, loss weights, and channel size. The decoder and discriminator have the same channel size, which is always 4 times larger than the channel size of the modality encoders. We hence omit the channel size of modality encoders from the table. The layer operating at the highest resolution always has the smallest number of channels (denoted as “base # channels for DecDis”). The number of channels doubles while the

MLP CLIP (Pretrained)

Down Down

Embed Embed Embed

Conv Conv Conv

Down Down

Embed Embed Embed

Conv Conv Conv

ResBlock ResBlock ResBlock

Concat

Snow mountains near a frozen lake with pink
clouds in the sky.

(a) Text encoder

(b) Segmentation encoder

(c) Sketch encoder

(d) Style encoder

Figure 11. Architecture of encoders. We use a pretrained CLIP [46] and an MLP to encoder text, a convolutional network with input skip connections to encode segmentation/sketch maps, and a residual network to encoder style images.

StyleGAN2 [25] 30M

SPADE-Seg [43] 96M

pSp-Seg [51] 298M

SPADE-Sketch [43] 95M

pSp-Sketch [51] 298M

TediGAN [69] 565M

Table 7. Number of parameter of compared models on MM-CelebA-HQ (1024×1024).

PoE-GAN 33M

StyleGAN2 [25] 25M

DF-GAN [60] 12M

DM-GAN + CL [73] 32M

SPADE-Seg [51] 280M

VQGAN [10] 798M

OASIS [53] 94M

SPADE-Sketch [51] 95M

PoE-GAN 142M

Table 8. Number of parameters of compared models on MS-COCO 2017 (256×256).

resolution halves until it reaches a maximum number (denoted as “maximum # channels for DecDis”). The ”Base # of channels for Latent” refers to the channel size of the feature map ziN to the decoder extracted from the spatial modalities (segmentation and sketch) in the highest resolution (the largest value of k is N ). We also note that the channel number of µki or σik equals that of zik. As explained in Appendix B.2, we double the channel size as we halve the spatial resolution. The channel number for zik doubles until it reaches the ”Maximum # of channels for Latent,” the channel number for zi0.
B.5. Hardware and speed
We report the computation infrastructure used in our experiments Tab. 6. We also report the training and inference speed of our model for different datasets in the table.
C. Additional results
We provide additional experimental results in this section, including more comparison with baselines and more analysis on the proposed approach. Please check the accompanying video for additional results on the landscape dataset.

C.1. Model size comparison
In Tab. 7 and Tab. 8, we compare the number of parameters used in PoE-GAN and in our baselines. We show that PoE-GAN does not use signiﬁcantly more parameters — actually it uses fewer parameters than some of the single-modal baselines, although PoE-GAN is trained for a much more challenging task. This shows that our improvement does not come from using a larger model.
C.2. Comparison on MM-CelebA-HQ (256×256)
In Tab. 9, We compare PoE-GAN trained on the 256×256 MM-CelebA-HQ dataset with the text-to-image baselines reported in TediGAN [69] and M6-UFC [80]. Our model achieves a signiﬁcantly lower FID than previous methods.
C.3. Additional comparison with TediGAN
We provide a more detailed comparison between PoEGAN and TediGAN, the only existing multimodal image synthesis method that can produce high-resolution images. TediGAN mainly contains four steps: 1) encodes different modalities into the latent space of StyleGAN, 2) mixes the latent code according to a hand-designed rule, 3) generates the image from the latent code using StyleGAN, and 4) iteratively reﬁnes the image. It has several disadvantages

Method
FID ↓ LPIPS ↑

AttnGAN [71]
125.98 0.512

ControlGAN [30]
116.32 0.522

DF-GAN [60]
137.60 0.581

DM-GAN [82]
131.05 0.544

TediGAN [69]
106.37 0.456

M6-UFC [80]
66.72 0.448

Ours
13.71 0.583

Table 9. Comparison of text-to-image synthesis on MM-CelebA-HQ (256×256). ↑: the higher the better, ↓: the lower the better.

Uncond Text

Ours (Uncond) 13.0

—

Ours (Text)

13.8

13.4

Ours (Seg)

14.2

—

Ours (Sketch) 14.2

—

Ours (All)

14.9

13.7

Seg

Sketch

All

—

—

—

—

—

—

10.9

—

—

—

9.5

—

12.9

9.9

8.5

Table 10. Comparison on MM-CelebA-HQ (256×256) using FID. We compare our PoE-GAN trained using all modalities with PoEGAN trained using a single modality. Note that PoE-GAN trained using a single modality can also be used for unconditional synthesis and we also report the achieved FID in the table.

Uncond Text

Ours (Uncond) 23.3

—

Ours (Text)

24.0

21.1

Ours (Seg)

25.5

—

Ours (Sketch) 24.7

—

Ours (All)

26.6

22.2

Seg

Sketch

All

—

—

—

—

—

—

16.3

—

—

—

24.7

—

17.1

30.2

17.1

Table 11. Comparison on MS-COCO 2017 (64×64) using FID. We compare our PoE-GAN trained using all modalities with PoE-GAN trained using a single modality. Note that PoE-GAN trained using a single modality can also be used for unconditional synthesis and we also report the achieved FID in the table.

compared with our PoE-GAN:
1. TediGAN relies on a pretrained unconditional generator, and its image quality is upper bounded by that unconditional model. This is not ideal because unconditional models usually produce images of lower quality than conditional ones. On the other hand, PoE-GAN learns conditional and unconditional generation simultaneously, and its FID improves when more conditions are provided, as shown in Tab. 1 of the main paper.
2. TediGAN uses a handcrafted rule to combine the latent code from different modalities. Speciﬁcally, the StyleGAN latent space contains 14 layers of latent vectors. TediGAN uses the top-layer latent vectors from one modality and the bottom-layer latent vectors from another modality when combining two modalities. This combination rule cannot be generalized to other generator architectures and other modalities. In contrast, we use product-of-experts with learned parameters to combine different modalities which is more general.
3. Sampling from TediGAN is very slow due to its instance-level optimization. It takes 51.2 seconds to generate a 1024×1024 image with TediGAN, while PoE-GAN only needs 0.07 seconds.
C.4. Training PoE-GAN with a single modality
Although PoE-GAN is designed for the multimodal conditional image synthesis task, it can be trained in a single modality setting. That is, we can train a pure segmentationto-image model, a pure sketch-to-image model, or a pure text-to-image model using the same PoE-GAN model. Here,

we compare a PoE-GAN model trained using multiple modalities with the same PoE-GAN model but trained using one single modality. We compare their performance when applied to convert the user input in a single modality to the output image. This experiment helps understand the penalty the model pays for the multimodal synthesis capability.
As shown in Tab. 10 and Tab. 11, the model trained for a speciﬁc modality always slightly outperforms the joint model when conditioned on that modality. This indicates that the increased task complexity of an additional modality outweighs the beneﬁts of additional annotations from that modality. This result also shows that the improvement of PoE-GAN over state-of-the-art unimodal image synthesis methods comes from our architecture and training scheme rather than additional annotations.
C.5. User study
We conduct a user study to compare PoE-GAN with stateof-the-art text-to-image and segmentation-to-image synthesis methods on MS-COCO. We show users two images generated by different algorithms from the same conditional input and ask them which one is more realistic. As shown in Tab. 12 and Tab. 13, the majority of users prefer PoE-GAN over the baseline methods.
C.6. Additional qualitative examples
In Figs. 13 to 17, we show that PoE-GAN can generate diverse images when conditioned on two different input modalities. We show additional qualitative comparison of text-to-image synthesis and segmentation-to-image synthesis on MS-COCO in Figs. 18 and 19 respectively. Figs. 20 to 22 show uncurated samples generated unconditionally on MMCelebA-HQ, MS-COCO, and Landscape.

Ours vs.

DF-GAN [60] 82.1%

DM-GAN + CL [73] 72.9%

Table 12. User study on text-to-image synthesis. Each column shows the percentage of users that prefer the image generated by our model over that generated by the baseline method.

Ours vs.

SPADE [43] 69%

VQGAN [10] 66.7%

OASIS [53] 64.9%

Table 13. User study on segmentation-to-image synthesis. Each column shows the percentage of users that prefer the image generated by our model over that generated by the baseline method.

Condition #1
This person is wearing hat, lipstick. She has big lips, and wavy hair.

Condition #2

This smiling man has bushy eyebrows, and
bags under eyes.

Samples

Huge ocean waves clash into rocks.

A beach with black sand and palm trees.

Figure 12. Examples generated by PoE-GAN when conditioned on contradictory segmentation and text inputs. The text input is simply ignored.

D. Limitation
We ﬁnd that the PoE-GAN model does not work well when conditioned on contradictory multimodality inputs. For example, when the segmentation and text are contradictory to each other, the text input is usually ignored, as shown in Fig. 12. In the product-of-experts formulation, an expert with a larger variance will have a smaller inﬂuence on the product distribution, and we indeed ﬁnd the variance of the text expert is usually larger than that of the segmentation expert, which explains the behavior of our model.
In addition, as discussed in Appendix C.4, the PoE-GAN model still pays the penalty in terms of a higher FID score as achieving the multimodal conditional image synthesis capability. This indicates room for improvement in the fusing of multiple modalities, and we leave this for future work

The person has goatee, mustache, and sideburns.
Figure 13. Examples of multimodal conditional image synthesis on MM-CelebA-HQ. We show three random samples from PoE-GAN conditioned on two modalities.

Condition #1
Large mustard yellow commercial airplane parked in the airport.

Condition #2

A rain covered terrain after a night of rain.

Samples

Figure 14. Examples of multimodal conditional image synthesis on MS-COCO. We show three random samples from PoE-GAN conditioned on two modalities (from top to bottom: text + segmentation, text + sketch, and segmentation + sketch).

Condition #1 Condition #2 (Style)

Samples

A person on a surf board riding a wave.
A person on a surf board riding a wave.
Figure 15. Examples of multimodal conditional image synthesis on MS-COCO. We show three random samples from PoE-GAN conditioned on two modalities, one being text/segmentation/sketch and another being style reference.

Condition #1

Condition #2

Trees near a lake in an autumn rainy day.

Huge ocean waves crash into rocks in a day
with colorful clouds.

Waterfall and river between mountains.

A white sand beach near cyan ocean.

Samples

Figure 16. Examples of multimodal conditional image synthesis on Landscape. We show three random samples from PoE-GAN conditioned on two modalities (from top to bottom: text + segmentation, text + sketch, and segmentation + sketch).

Condition #1

Condition #2 (Style)

Samples

A lake surrounded by mountains.
A lake surrounded by mountains.
Figure 17. Examples of multimodal conditional image synthesis on Landscape. We show three random samples from PoE-GAN conditioned on two modalities, one being text/segmentation/sketch and another being style reference.

Text
A kitchen with a counter and a table with
chairs.

Ground truth

A view of mountains from the window of a
jet airplane.

DF-GAN

DM-GAN + CL

PoE-GAN (Ours)

A blueberry cake is on a plate and is topped
with butter.
Three men each holding something and
posing for a picture.
A plate holds a good size portion of a
cooked, mixed dish that includes broccoli and
pasta.
A red blue and yellow train and some people
on a platform.

A man blowing out candles on a birthday
cake.
A desk set up as a workstation with a
laptop.
Figure 18. Additional qualitative comparison of text-to-image synthesis on MS-COCO 2017.

Segmentation

Ground truth

SPADE

OASIS

PoE-GAN (Ours)

Figure 19. Additional qualitative comparison of segmentation-to-image synthesis on MS-COCO 2017.

Figure 20. Uncurated unconditional results on the 1024×1024 MM-CelebA-HQ dataset. Figure 21. Uncurated unconditional results on the 256×256 MS-COCO dataset.

Figure 22. Uncurated unconditional results on the 1024×1024 landscape dataset.

