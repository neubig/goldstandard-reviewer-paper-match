FASTEMIT: LOW-LATENCY STREAMING ASR WITH
SEQUENCE-LEVEL EMISSION REGULARIZATION

Jiahui Yu Chung-Cheng Chiu Bo Li Shuo-yiin Chang Tara N. Sainath Yanzhang He Arun Narayanan Wei Han Anmol Gulati Yonghui Wu Ruoming Pang
Google LLC, USA
{jiahuiyu,rpang}@google.com

arXiv:2010.11148v2 [eess.AS] 3 Feb 2021

ABSTRACT
Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible. However, emitting fast without degrading quality, as measured by word error rate (WER), is highly challenging. Existing approaches including Early and Late Penalties [1] and Constrained Alignments [2, 3] penalize emission delay by manipulating per-token or per-frame probability prediction in sequence transducer models [4]. While being successful in reducing delay, these approaches suffer from signiﬁcant accuracy regression and also require additional word alignment information from an existing model. In this work, we propose a sequence-level emission regularization method, named FastEmit, that applies latency regularization directly on per-sequence probability in training transducer models, and does not require any alignment. We demonstrate that FastEmit is more suitable to the sequence-level optimization of transducer models [4] for streaming ASR by applying it on various end-to-end streaming ASR networks including RNN-Transducer [5], Transformer-Transducer [6, 7], ConvNet-Transducer [8] and Conformer-Transducer [9]. We achieve 150 ∼ 300ms latency reduction with signiﬁcantly better accuracy over previous techniques on a Voice Search test set. FastEmit also improves streaming ASR accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile latency from 210ms to only 30ms on LibriSpeech.
1. INTRODUCTION
End-to-end (E2E) recurrent neural network transducer (RNN-T) [4] models have gained enormous popularity for streaming ASR applications, as they are naturally streamable [1, 5, 6, 7, 10, 11, 12, 13]. However, naive training with a sequence transduction objective [4] to maximize the log-probability of target sequence is unregularized and these streaming models learn to predict better by using more context, causing signiﬁcant emission delay (i.e., the delay between the user speaking and the text appearing). Recently there are some approaches trying to regularize or penalize the emission delay. For example, Li et al. [1] proposed Early and Late Penalties to enforce the prediction of </s> (end of sentence) within a reasonable time window given by a voice activity detector (VAD). Constrained Alignments [2, 3] were also proposed by extending the penalty terms to each word, based on speech-text alignment information [14] generated from an existing speech model.
While being successful in terms of reducing latency of streaming RNN-T models, these two regularization approaches suffer from accuracy regression [1, 3]. One important reason is because both regularization techniques penalize the per-token or per-frame prediction probability independently, which is inconsistent with the sequence-level transducer optimization of per-sequence probability

calculated by the transducer forward-backward algorithm [4]. Although some remedies like second-pass Listen, Attend and Spell (LAS) [15] rescorer [16, 17] and minimum word error rate (MWER) training technique [18] have been used to reduce the accuracy regression, these approaches come at a non-negligible compute cost in both training and serving.
In this work, we propose a novel sequence-level emission regularization method for streaming models based on transducers, which we call FastEmit. FastEmit is designed to be directly applied on the transducer forward-backward per-sequence probability, rather than individual per-token or per-frame prediction of probability independently. In breif, in RNN-T [4] it ﬁrst extends the output vocabulary space Y with a ‘blank token’ ∅, meaning ‘output nothing’. Then the transducer forward-backward algorithm calculates the probability of each lattice (speech-text alignment) in the T × U matrix, where T and U is the length of input and output sequence respectively. Finally the optimal lattice in this matrix can be automatically learned by maximizing log-probability of the target sequence. It is noteworthy that in this transducer optimization, emitting a vocabulary token y ∈ Y and the blank token ∅ are treated equally, as long as the logprobability of the target sequence can be maximized. However, in streaming ASR systems the blank token ∅ ‘output nothing’ should be discouraged as it leads to higher emission latency. We will show in detail that FastEmit, as a sequence-level regularization method, encourages emitting vocabulary tokens y ∈ Y and suppresses blank tokens ∅ across the entire sequence based on transducer forwardbackward probabilities, leading to signiﬁcantly lower emission latency while retaining recognition accuracy.
FastEmit has many advantages over other regularization methods to reduce emission latency in end-to-end streaming ASR models: (1) FastEmit is a sequence-level regularization based on transducer forward-backward probabilities, thus is more suitable when applied jointly with the sequence-level transducer objective. (2) FastEmit does not require any speech-word alignment information [3] either by labeling or generated from an existing speech model. Thus it is easy to ‘plug and play’ in any transducer model on any dataset without any extra effort. (3) FastEmit has minimal hyper-parameters to tune. It only introduces one hyper-parameter λ to balance the transducer loss and regularization loss. (4) There is no additional training or serving cost to apply FastEmit.
We apply FastEmit on various end-to-end streaming ASR networks including RNN-Transducer [5], Transformer-Transducer [6, 7], ConvNet-Transducer [8] and Conformer-Transducer [9]. We achieve 150 ∼ 300ms latency reduction with signiﬁcantly better accuracy over previous methods [2, 3, 10] on a Voice Search test set. FastEmit also improves streaming ASR accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile latency from 210ms to only 30ms on LibriSpeech.

Fig. 1. Examples of fast and slow transducer emission lattices (speech-text alignments). Transducer aims to maximize the logprobability of any lattice, regardless of its emission latency.

Fig. 2. Illustration of FastEmit regularization. Consider any node (e.g., blue node), FastEmit encourages predicting label y ∈ Y (green node) instead of predicting blank ∅ (red node).

2. TRANSDUCER WITH FASTEMIT
In this section, we ﬁrst delve into transducer [4] and show why naively optimizing the transducer objective is unregularized thus unsuitable for low-latency streaming ASR models. We then propose FastEmit as a sequence-level emission regularization method to regularize the emission latency.

2.1. Transducer
Transducer optimization [4] automatically learns probabilistic alignments between an input sequence x = (x1, x2, . . . , xT ) and an output sequence y = (y1, y2, . . . , yU ), where T and U denote the length of input and output sequences respectively. To learn the probabilistic alignments, it ﬁrst extends the output space Y with a ‘blank token’ ∅ (meaning ‘output nothing’, visually denoted as right arrows in Figure 1 and 2): Y¯ = Y ∪ ∅. The allocation of these blank tokens then determines an alignment between the input and output sequences. Given an input sequence x, the transducer aims to maximize the log-probability of a conditional distribution:

L = − log P (yˆ|x) = − log

P (a|x)

(1)

a∈B−1 (yˆ )

where B : Y¯ → Y is a function that removes the ∅ tokens from each alignment lattice a, and yˆ is the ground truth output sequence tokenized from text label.
As shown in Figure 1, we denote each node (t, u) as the probability of emitting the ﬁrst u elements of the output sequence by the ﬁrst t frames of the input sequence. We further denote the prediction from a neural network yˆ(t, u) and b(t, u) as the probability of label token (up arrows in ﬁgures) and blank token (right arrows in ﬁgures) at node (t, u). To optimize the transducer objective, an efﬁcient forward-backward algorithm [4] is used to calculate the probability of each alignment and aggregate all possible alignments before propagating gradients back to yˆ(t, u) and b(t, u). It is achieved by deﬁning forward variable α(t, u) as the probability of emitting yˆ[1:u] during x[1:t], and backward variable β(t, u) as the probability of emitting yˆ[u + 1:U ] during x[t:T ], using an efﬁcient forward-

backward propagation algorithm:

α(t, u) = yˆ(t, u−1)α(t, u−1) + b(t−1, u)α(t−1, u), (2)

β(t, u) = yˆ(t, u)β(t, u+1) + b(t, u)β(t+1, u),

(3)

where the initial conditions are α(1, 0) = 1, β(T, U ) = b(T, U ). It is noteworthy that α(t, u)β(t, u) deﬁnes the probability of all complete alignments in At,u : {complete alignment through node(t, u)}:

P (At,u|x) =

P (a|x) = α(t, u)β(t, u).

(4)

a∈At,u

By diffusion analysis of the probability of all alignments, we know that P (yˆ|x) is equal to the sum of P (At,u|x) over any topleft to bottom-right diagonal nodes (i.e., all complete alignments will pass through any diagonal cut in the T × U matrix in Figure 1) [4]:

P (yˆ|x) =

P (At,u|x), ∀n : 1 ≤ n ≤ U + T. (5)

(t,u):t+u=n

Finally, gradients of transducer loss function L = − log P (yˆ|x) w.r.t. neural network prediction of probability yˆ(t, u) and b(t, u) can be calculated according to Equations 1, 2, 3, 4 and 5.

2.2. FastEmit
Now let us consider any node in the T × U matrix, for example, the blue node at (t, u), as shown in Figure 2. First we know that the probability of emitting yˆ[1:u] during x[1:t] is α(t, u). At the next step, the alignment can either ‘go up’ by predicting label u+1 to the green node with probability yˆ(t, u), or ‘turn right’ by predicting blank ∅ to the red node with probability b(t, u). Finally together with backward probability β of the new node, the probability of all complete alignments At,u passing through node (t, u) in Equation 4 can be decomposed into two parts:

P (At,u|x) = α(t, u)β(t, u) =

(6)

α(t, u)b(t, u)β(t+1, u) + α(t, u)yˆ(t, u)β(t, u+1),

predict blank

predict label

which is equivalent as replacing β(t, u) in Equation 4 with Equation 3. From Equation 6 we know that gradients of transducer loss

L w.r.t. the probability prediction of any node (t, u) have following properties (closed-form gradients can be found in [4] Equation 20):

∂L ∝ α(t, u)β(t, u+1) (7) ∂yˆ(t, u)
∂L ∝ α(t, u)β(t+1, u). (8) ∂b(t, u)

However, this transducer loss L aims to maximize log-probability of all possible alignments, regardless of their emission latency. In other words, as shown in Figure 2, emitting a vocabulary token y ∈ Y and the blank token ∅ are treated equally, as long as the log-probability is maximized. It inevitably leads to emission delay because streaming ASR models learn to predict better by using more future context, causing signiﬁcant emission delay.
By the decomposition in Equation 6, we propose a simple and effective transducer regularization method, FastEmit, which encourages predicting label instead of blank by additionally maximizing the probability of ‘predict label’ based on Equation 1, 5 and 6:

P˜(At,u|x) = α(t, u)yˆ(t, u)β(t, u+1),

(9)

predict label

L˜ = − log

(P (At,u|x) + λP˜(At,u|x)), (10)

(t,u):t+u=n

∀n : 1 ≤ n ≤ U + T . L˜ is the new transducer loss with FastEmit regularization and λ is a hyper-parameter to balance the transducer
loss and regularization loss. FastEmit is easy to implement based on
an existing transducer implementation, because the gradients calculation of this new regularized transducer loss L˜ follows:

∂L˜

∂L

∂yˆ(t, u) = (1+λ) ∂yˆ(t, u) , (11)

∂L˜

∂L

∂b(t, u) = ∂b(t, u) , (12)

To interpret the gradients of FastEmit, intuitively it simply means that the gradients of emitting label tokens has a ‘higher learning rate’ back-propagating into the streaming ASR network, while emitting blank token remains the same. We also note that the proposed FastEmit regularization method is based on alignment probabilities instead of per-token or per-frame prediction of probability, thus we refer it as sequence-level emission regularization.

3. EXPERIMENTAL DETAILS
3.1. Latency Metrics
Our latency metrics of streaming ASR are motivated by real-world applications like Voice Search and Smart Home Assistants. In this work we mainly measure two types of latency metrics described below: (1) partial recognition latency on both LibriSpeech and MultiDomain datasets, and (2) endpointer latency [19] on MultiDomain dataset. A visual example of two latency metrics is illustrated in Figure 3. For both metrics, we report both 50-th (medium) and 90-th percentile values of all utterances in the test set to better characterize latency by excluding outlier utterances.
Partial Recognition (PR) Latency is deﬁned as the timestamps difference of two events as illustrated in Figure 3: (1) when the last token is emitted in the ﬁnalized recognition result, (2) the end of the speech when a user ﬁnishes speaking estimated by forced alignment. PR latency is especially descriptive of user experience in real-world

Fig. 3. A visual illustration of PR latency and EP latency metrics.
streaming ASR applications like Voice Search and Assistants. Moreover, PR latency is the lower bound for applying other techniques like Prefetching [11], by which streaming application can send early server requests based on partial/incomplete recognition hypotheses to retrieve relevant information and necessary resources for future actions. Finally, unlike other latency metrics that may depend on hardware, environment or system optimization, PR latency is inherented to streaming ASR models and thus can better characterize the emission latency of streaming ASR. It is also noteworthy that models that capture stronger contexts can emit a hypothesis even before they are spoken, leading to a negative PR latency.
Endpointer (EP) Latency is different from PR latency and it measures the timestamps difference between: (1) when the streaming ASR system predicts the end of the query (EOQ), (2) the end of the speech when a user ﬁnishes speaking estimated by forced alignment. As illustrated in Figure 3, EOQ can be implied by jointly predicting the </s> token with end-to-end Endpointing introduced in [19]. The endpointer can be used to close the microphone as soon as the user ﬁnishes speaking, but it is also important to avoid cutting off users while they are still speaking. Thus, the prediction of the </s> token has a higher latency compared with PR latency, as shown in Figure 3. Note that PR latency is also a lower bound of EP latency, thus reducing the PR latency is the main focus of this work.
3.2. Dataset and Training Details
We report our results on two datasets, a public dataset LibriSpeech [20] and an internal large-scale dataset MultiDomain [21].
Our main results and ablation studies will be presented on a widely used public dataset LibriSpeech [20], which consists of about 1000 hours of English reading speech. For data processing, we extract 80-channel ﬁlterbanks feature computed from a 25ms window with a stride of 10ms, use SpecAugment [22] for data augmentation, and train with the Adam optimizer. We use a single layer LSTM as the decoder. All of these training settings follow the previous work [8, 9] for fair comparison. We train our LibriSpeech models on 960 hours of LibriSpeech training set with labels tokenized using a 1,024 word-piece model (WPM), and report our test results on LibriSpeech TestClean and TestOther (noisy).
We also report our results a production dataset MultiDomain [21], which consists of 413,000 hours speech, 287 million utterances across multiple domains including Voice Search, YouTube, and Meetings. Multistyle training (MTR) [23] is used for noise robustness. These training and testing utterances are anonymized and hand-transcribed, and are representatives of Google’s speech recognition trafﬁc. All models are trained to predict labels tokenized using a 4,096 word-piece model (WPM). We report our results on a test set of 14K Voice Search utterances with duration less than 5.5 seconds long.

3.3. Model Architectures
FastEmit can be applied to any transducer model on any dataset without any extra effort. To demonstrate the effectiveness of our proposed method, we apply FastEmit on a wide range of transducer models including RNN-Transducer [5], Transformer-Transducer [6], ConvNet-Transducer [8] and Conformer-Transducer [9]. We refer the reader to the individual papers for more details of each model architecture. For each of our experiment, we keep the exact same training and testing settings including model size, model regularization (weight decay, variational noise, etc.), optimizer, learning rate schedule, input noise and augmentation, etc. All models are implemented, trained and benchmarked based on Lingvo toolkit [24].
All these model architectures are based on encoder-decoder transducers. The encoders are based on autoregressive models using uni-directional LSTMs, causal convolution and/or left-context attention layers (no future context is permitted). The decoders are based on prediction network and joint network similar to previous RNN-T models [1, 4, 10]. For all experiments on LibriSpeech, we report results directly after training with the transducer objective. For all our experiments on MultiDomain, results are reported with minimum word error rate (MWER) ﬁnetuning [18] for fair comparison.

4. RESULTS
In this section, we ﬁrst report our results on LibriSpeech dataset and compare with other streaming ASR networks. We next study the hyper-parameter λ in FastEmit to balance transducer loss and regularization loss. Finally, we conduct large-scale experiments on the MultiDomain production dataset and compare FastEmit with other methods [1, 2, 3] on a Voice Search test set.

4.1. Main Results on LibriSpeech

Table 1. Streaming ASR results on LibriSpeech dataset. We apply FastEmit to Large and Medium size streaming ContextNet [8] and Conformer [9].

Method

WER

WER

PR50

TestClean TestOther (ms)

PR90 (ms)

LSTM

4.7

11.1

80

180

Transformer

4.5

10.9

70

190

Conformer-M +FastEmit

4.6 3.7 (-0.9)

9.9 9.5 (-0.4)

140 -40 (-180)

280 80 (-200)

Conformer-L +FastEmit

4.5 3.5 (-1.0)

9.5 9.1 (-0.4)

110 -60 (-170)

230 70 (-160)

ContextNet-M +FastEmit

4.5 3.5 (-1.0)

10.0 8.6 (-1.4)

70 -110 (-180)

270 40 (-230)

ContextNet-L +FastEmit

4.4 3.1 (-1.3)

8.9 7.5 (-1.4)

50 -120 (-170)

210 30 (-180)

We ﬁrst present results of FastEmit on both Medium and Large size streaming ContextNet [8] and Conformer [9] in Table 1. We did a small hyper-parameter sweep of λ and set 0.01 for ContextNet and 0.004 for Conformer. FastEmit signiﬁcantly reduces PR latency by ∼ 200ms. It is noteworthy that streaming ASR models that capture stronger contexts can emit the full hypothesis even before they are spoken, leading to a negative PR latency. We also ﬁnd FastEmit even improves the recognition accuracy on LibriSpeech. By error analysis, the deletion errors have been signiﬁcantly reduced. As

LibriSpeech is long-form spoken-domain read speech, FastEmit encourages early emission of labels thus helps with vanishing gradients problem in long-form RNN-T [25], leading to less deletion errors.

4.2. Hyper-parameter λ in FastEmit

Table 2. Study of loss balancing hyper-parameter λ in FastEmit on LibriSpeech dataset, based on M-size streaming ContextNet [8].

FastEmit H-Param λ

WER

WER

PR50

TestClean TestOther (ms)

PR90 (ms)

0 (No FastEmit) 0.001 0.004 0.008 0.01 0.02 0.04

4.5 4.1 (-0.4) 3.5 (-1.0) 3.6 (-0.9) 3.5 (-1.0) 3.8 (-0.7) 4.4 (-0.1)

10.0 8.7 (-1.3) 8.4 (-1.6) 8.5 (-1.5) 8.6 (-1.4) 9.1 (-0.9) 10.0 (0.0)

70 60 (-10) -30 (-100) -80 (-150) -110 (-180) -170 (-240) -230 (-300)

270 190 (-80) 100 (-170) 50 (-220) 40 (-230) -30 (-300) -90 (-360)

Next we study the hyper-parameter λ of FastEmit regularization by applying different values on M-size streaming ContextNet [8]. As shown in Table 2, larger λ leads to lower PR latency of streaming models. But when the λ is larger than a certain threshold, the WER starts to degrade due to the regularization being too strong. Moreover, λ also offers ﬂexibility of WER-latency trade-offs.

4.3. Large-scale Experiments on MultiDomain

Table 3. Streaming ASR results of FastEmit RNN-T, Transformer-T and Conformer-T on a Voice Search test set compared with [2, 3, 10].

Method

WER

EP50 EP90 PR50 (ms) (ms) (ms)

PR90 (ms)

RNN-T

6.0

360 750 190

330

+CA [2, 3]

6.7 (+0.7) 450

860

-50 (-260) 60 (-250)

+MaskFrame 6.5 (+0.5) 250 730 100 (-90) 250 (-80)

+FastEmit

6.2 (+0.2) 330

650

-10 (-200) 180 (-150)

Transformer-T 6.1

400 780 220

370

+FastEmit

6.3 (+0.2) 390

740

60 (-160)

220 (-150)

Conformer-T 5.6

260 590 150

290

+FastEmit

5.8 (+0.2) 290

660

-110 (-260) 90 (-200)

Finally we show that FastEmit regularization method is also effective on the large scale production dataset MultiDomain. In Table 3, we apply FastEmit on RNN-Transducer [5], TransformerTransducer [6] and Conformer-Transducer [9]. For RNN-T, we also compare FastEmit with other methods [2, 3, 10]. All results are ﬁnetuned with minimum word error rate (MWER) training technique [18] for fair comparison. In Table 3, CA denotes constrained alignment [2, 3], MaskFrame denotes the idea of training RNNT models with incomplete speech by masking trailing n frames to encourage a stronger decoder thus can emit faster. We perform a small hyper-parameter search for both baselines CA and MaskFrame and report their WER, EP and PR latency on a Voice Search test set. FastEmit achieves 150 ∼ 300ms latency reduction with signiﬁcantly better accuracy over baseline methods in RNN-T [5], and generalizes further to Transformer-T [6] and Conformer-T [9]. By error analysis, as Voice Seach is short-query written-domain conversational speech, emitting faster leads to more errors. Nevertheless, among all techniques in Table 3, FastEmit achieves best WERlatency trade-off.

5. REFERENCES
[1] Bo Li, Shuo-yiin Chang, Tara N Sainath, Ruoming Pang, Yanzhang He, Trevor Strohman, and Yonghui Wu, “Towards fast and accurate streaming end-to-end asr,” in ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6069–6073.
[2] Has¸im Sak, Andrew Senior, Kanishka Rao, and Franc¸oise Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” arXiv preprint arXiv:1507.06947, 2015.
[3] Tara N. Sainath, Ruoming Pang, David Rybach, Basi Garc´ıa, and Trevor Strohman, “Emitting Word Timings with End-toEnd Models,” Proc. Interspeech, 2020.
[4] Alex Graves, “Sequence Transduction with Recurrent Neural Networks,” CoRR, vol. abs/1211.3711, 2012.
[5] Yanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang, Deepti Bhatia, Yuan Shangguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo-Yiin Chang, Kanishka Rao, and Alexander Gruenstein, “Streaming End-to-end Speech Recognition For Mobile Devices,” in Proc. ICASSP, 2019.
[6] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar, “Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7829–7833.
[7] Ching-Feng Yeh, Jay Mahadeokar, Kaustubh Kalgaonkar, Yongqiang Wang, Duc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, and Michael L Seltzer, “Transformertransducer: End-to-end speech recognition with self-attention,” arXiv preprint arXiv:1910.12977, 2019.
[8] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, ChungCheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, and Yonghui Wu, “Contextnet: Improving convolutional neural networks for automatic speech recognition with global context,” arXiv preprint arXiv:2005.03191, 2020.
[9] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., “Conformer: Convolutionaugmented transformer for speech recognition,” arXiv preprint arXiv:2005.08100, 2020.
[10] Tara N Sainath, Yanzhang He, Bo Li, Arun Narayanan, Ruoming Pang, Antoine Bruguier, Shuo-yiin Chang, Wei Li, Raziel Alvarez, Zhifeng Chen, et al., “A streaming on-device end-toend model surpassing server-side conventional model quality and latency,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6059–6063.
[11] Shuo-Yiin Chang, Bo Li, David Rybach, Yanzhang He, Wei Li, Tara Sainath, and Trevor Strohman, “Low latency speech recognition using end-to-end prefetching,” in Interspeech. ISCA, 2020.
[12] Jiahui Yu, Wei Han, Anmol Gulati, Chung-Cheng Chiu, Bo Li, Tara N. Sainath, Yonghui Wu, and Ruoming Pang, “Universal asr: Unify and improve streaming asr with full-context modeling,” arXiv preprint arXiv:2010.06030, 2020.

[13] Chengyi Wang, Yu Wu, Shujie Liu, Jinyu Li, Liang Lu, Guoli Ye, and Ming Zhou, “Low latency end-to-end streaming speech recognition with a scout network,” arXiv preprint arXiv:2003.10369, 2020.
[14] Ehsan Variani, Tom Bagby, Kamel Lahouel, Erik McDermott, and Michiel Bacchiani, “Sampled connectionist temporal classiﬁcation,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4959–4963.
[15] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen, Attend and Spell,” CoRR, vol. abs/1508.01211, 2015.
[16] Tara N. Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirko Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu, Ian McGraw, and ChungCheng Chiu, “Two-Pass End-to-End Speech Recognition,” Proc. Interspeech, 2019.
[17] Wei Li, James Qin, Chung-Cheng Chiu, Ruoming Pang, and Yanzhang He, “Parallel rescoring with transformer for streaming on-device speech recognition,” arXiv preprint arXiv:2008.13093, 2020.
[18] Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, and Anjuli Kannan, “Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models,” in Proc. ICASSP, 2018.
[19] Shuo-Yiin Chang, Bo Li, Tara N. Sainath, Gabor Simko, and Carolina Parada, “Endpoint Detection Using Grid Long ShortTerm Memory Networks for Streaming Speech Recognition,” in Proc. Interspeech, 2017.
[20] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.
[21] Arun Narayanan, Ananya Misra, Khe Chai Sim, Golan Pundak, Anshuman Tripathi, Mohamed Elfeky, Parisa Haghani, Trevor Strohman, and Michiel Bacchiani, “Toward domaininvariant speech recognition via large scale training,” in Proc. SLT. IEEE, 2018, pp. 441–447.
[22] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” arXiv preprint arXiv:1904.08779, 2019.
[23] Bo Li, Tara N Sainath, Khe Chai Sim, Michiel Bacchiani, Eugene Weinstein, Patrick Nguyen, Zhifeng Chen, Yanghui Wu, and Kanishka Rao, “Multi-dialect speech recognition with a single sequence-to-sequence model,” in Proc. ICASSP. IEEE, 2018, pp. 4749–4753.
[24] Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, et al., “Lingvo: a modular and scalable framework for sequence-to-sequence modeling,” arXiv preprint arXiv:1902.08295, 2019.
[25] Chung-Cheng Chiu, Wei Han, Yu Zhang, Ruoming Pang, Sergey Kishchenko, Patrick Nguyen, Arun Narayanan, Hank Liao, Shuyuan Zhang, Anjuli Kannan, et al., “A comparison of end-to-end models for long-form speech recognition,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 889–896.

