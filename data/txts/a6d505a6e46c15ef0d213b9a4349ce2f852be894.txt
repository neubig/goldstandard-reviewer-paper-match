arXiv:2111.00980v1 [cs.LG] 1 Nov 2021

Mixture Proportion Estimation and PU Learning: A Modern Approach
Saurabh Garg1, Yifan Wu1, Alex Smola2, Sivaraman Balakrishnan1, Zachary C. Lipton1 1Carnegie Mellon University 2Amazon Web Services
Abstract
Given only positive examples and unlabeled examples (from both positive and negative classes), we might hope nevertheless to estimate an accurate positiveversus-negative classiﬁer. Formally, this task is broken down into two subtasks: (i) Mixture Proportion Estimation (MPE)—determining the fraction of positive examples in the unlabeled data; and (ii) PU-learning—given such an estimate, learning the desired positive-versus-negative classiﬁer. Unfortunately, classical methods for both problems break down in high-dimensional settings. Meanwhile, recently proposed heuristics lack theoretical coherence and depend precariously on hyperparameter tuning. In this paper, we propose two simple techniques: Best Bin Estimation (BBE) (for MPE); and Conditional Value Ignoring Risk (CVIR), a simple objective for PU-learning. Both methods dominate previous approaches empirically, and for BBE, we establish formal guarantees that hold whenever we can train a model to cleanly separate out a small subset of positive examples. Our ﬁnal algorithm (TED)n, alternates between the two procedures, signiﬁcantly improving both our mixture proportion estimator and classiﬁer1.
1 Introduction
When deploying k-way classiﬁers in the wild, what can we do when confronted with data from a previously unseen class (k ` 1)? Theory dictates that learning under distribution shift is impossible absent assumptions. And yet people appear to exhibit this capability routinely. Faced with new surprising symptoms, doctors can recognize the presence of a previously unseen ailment and attempt to estimate its prevalence. Similarly, naturalists can discover new species, estimate their range and population, and recognize them reliably going forward.
To begin making this problem tractable, we might make the label shift assumption [37, 41, 29], i.e., that while the class balance ppyq can change, the class conditional distributions ppx|yq do not. Moreover, we might begin by focusing on the base case, where only one class has been seen previously, i.e., k “ 1. Here, we possess (labeled) positive data from the source distribution, and (unlabeled) data from the target distribution, consisting of both positive and negative instances. This problem has been studied in the literature as learning from positive and unlabeled data [8, 27] and has typically been broken down into two subtasks: (i) Mixture Proportion Estimation (MPE) where we estimate α, the fraction of positives among the unlabeled examples; and (ii) PU-learning where this estimate is incorporated into a scheme for learning a Positive-versus-Negative (PvN) binary classiﬁer.
Traditionally, MPE and PU-learning have been motivated by settings involving large databases where unlabeled examples are abundant and a small fraction of the total positives have been extracted. For example, medical records might be annotated indicating the presence of certain diagnoses, but the unmarked passages are not necessarily negative. This setup has also been motivated by protein and
1Code is available at https://github.com/acmi-lab/PU_learning
35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

Figure 1: Illustration of proposed methods. (left) Estimate of α with varying fraction of unlabeled examples in the top bin. The shaded region highlights the upper and lower conﬁdence bounds. BBE selects the top bin that minimizes the upper conﬁdence bound. (right) Accuracy and MPE estimate as training proceeds. Till 100-th epoch (vertical line), we perform PvU training, i.e., warm start for (TED)n. Post 100-th epoch, we continue with both (TED)n and PvU training. Note that (TED)n improves both classiﬁcation accuracy and MPE compared to PvU training. Results with Resnet-18 on binary-CIFAR. For details and comparisons with other methods, see Sec. 6.
gene identiﬁcation [16]. Databases in molecular biology often contain lists of molecules known to exhibit some characteristic of interest. However, many other molecules may exhibit the desired characteristic, even if this remains unknown to science.
Many methods have been proposed for both MPE [16, 12, 39, 35, 21, 4, 36, 20] and PU-learning [14, 11, 23]. However, classical MPE methods break down in high-dimensional settings [35] or yield estimators whose accuracy depends on restrictive conditions [12, 39]. On the other hand, most recent proposals either lack theoretical coherence, rely on heroic assumptions, or depend precariously on tuning hyperparameters that are, by the very problem setting, untunable. For PU learning, Elkan and Noto [16] suggest training a classiﬁer to distinguish positive from unlabeled data followed by a rescaling procedure. Du Plessis et al. [11] suggest an unbiased risk estimation framework for PU learning. However, these methods fail badly when applied with model classes capable of overﬁtting and thus implementations on high-dimensional datasets rely on extensive hyperparameter tuning and additional ad-hoc heuristics that do not transport effectively across datasets.
In this paper, we propose (i) Best Bin Estimation (BBE), an effective technique for MPE that produces consistent estimates αp?under mild assumptions and admits ﬁnite-sample statistical guarantees achieving the desired Op1{ nq rates; and (ii) learning with the Conditional Value Ignoring Risk (CVIR) objective, which discards the highest loss α fraction of examples on each training epoch, removing the
p incentive to overﬁt to the unlabeled positive examples. Both methods are simple to implement, compatible with arbitrary hypothesis classes (including deep networks), and dominate existing methods in our experimental evaluation. Finally, we combine the two in an iterated Transform-Estimate-Discard (TED)n framework that signiﬁcantly improves both MPE estimation error and classiﬁer error.
We build on label shift methods [29, 3, 2, 34, 17], that leverage black-box classiﬁers to reduce dimensionality, estimating the target label distribution as a functional of source and target pushforward distributions. While label shift methods rely on classiﬁers trained to separate previously seen classes, BBE is able to exploit a Positive-versus-Unlabeled (PvU) target classiﬁer, which gives each input a score indicating how likely it is to be a positive sample. In particular, BBE identiﬁes a threshold such that by estimating the ratio between the fractions of positive and unlabeled points receiving scores above the threshold, we obtain the mixture proportion α.
BBE works because in practice, for many datasets, PvU classiﬁers, even when uncalibrated, produce outputs with near monotonic calibration diagrams. Higher scores correspond to a higher proportion of positives, and when the positive data contains a separable sub-domain, i.e., a region of the input space where only the positive distribution has support, classiﬁers often exhibit a threshold above which the top bin contains mostly positive examples. We show that the existence of a (nearly) pure top bin is sufﬁcient for BBE to produce a (nearly) consistent estimate α, whose ﬁnite sample convergence
p
2

rates depend on the fraction of examples in the bin and whose bias depends on the purity of the bin. Crucially, we can estimate the optimal threshold from data.
We conduct a battery of experiments both to empirically validate our claim that BBE’s assumptions are mild and frequently hold in practice, and to establish the outperformance of BBE, CVIR, and (TED)n over the previous state of the art. We ﬁrst motivate BBE by demonstrating that in practice PvU classiﬁers tend to isolate a reasonably large, reasonably pure top bin. We then conduct extensive experiments on semi-synthetic data, adapting a variety of binary classiﬁcation datasets to the PU learning setup and demonstrating the superior performance of BBE and PU-learning with the CVIR objective. Moreover, we show that (TED)n, which combines the two in an iterative fashion, improves signiﬁcantly over previous methods across several architectures on a range of image and text datasets.
2 Related Work
Research on MPE and PU learning date to [9, 8, 27] (see review by [5]). Elkan and Noto [16] ﬁrst proposed to leverage a PvU classiﬁer to estimate the mixture proportion. Du Plessis and Sugiyama [13] propose a different method for estimating the mixture coefﬁcient based on Pearson divergence minimization. While they do not require a PvU classiﬁer, they suffer the same shortcoming. Both methods require that the positive and negative examples have disjoint support. Our requirements are considerably milder. Blanchard et al. [6] observe that without assumptions on the underlying positive and negative distributions, the mixture proportion is not identiﬁable. Furthermore, [6] provide an irreducibility condition that identiﬁes α and propose an estimator that converges to the true α. W?hile their estimator can converge arbitrarily slowly, Scott [39] showed faster convergence (Op1{ nq) under stronger conditions. Unfortunately, despite its appealing theoretical properties Blanchard et al. [6]’s estimator is computationally infeasible. Building on Blanchard et al. [6], Sanderson and Scott [38] and Scott [39] proposed estimating the mixture proportion from a ROC curve constructed for the PvU classiﬁer. However, when the PvU classiﬁer is not perfect, these methods are not clearly understood. Ramaswamy et al. [35] proposed the ﬁrst computationally feasible algorithm for MPE with convergence guarantees to the true proportion. Their method KM, requires embedding distributions onto an RKHS. However, their estimator underperforms on high dimensional datasets and scales poorly with large datasets. Bekker and Davis [4] proposed TIcE, hoping to identify a positive subdomain in the input space using decision tree induction. This method also underperforms in high-dimensional settings.
In the most similar works, Jain et al. [21] and Ivanov [20] explore dimensionality reduction using a PvU classiﬁer. Both methods estimate α through a procedure operating on the PvU classiﬁer’s output. However, neither methods has provided theoretical backing. [20] concede that their method often fails and returns a zero estimate, requiring that they fall back to a different estimator. Moreover while both papers state that their method require the Bayes-optimal PvU classiﬁer to identify α in the transformed space, we prove that even when hypothesis class is well speciﬁed for PvN learning, PvU training can fail to recover the Bayes-optimal scoring function. Furthermore, we also show that the heuristic estimator in Scott [39] can be thought of as using PvU classiﬁer for dimensionality reduction. While this heuristic is similar to our estimator in spirit, we show that the functional form of their estimator is different from ours and note that their heuristic enjoys no theoretical guarantee. By contrast, our estimator BBE is theoretically coherent under mild conditions and outperforms all of these methods empirically.
Given α, Elkan and Noto [16] propose a transformation via Bayes rule to obtain the PvN classiﬁer. They also propose a weighted objective, with weights given by the PvU classiﬁer. Other propose unbiased risk estimators [14, 11] which require the mixture proportion α. Du Plessis et al. [14] proposed an unbiased estimator with non-convex loss functions satisfying a speciﬁc symmetric condition, and subsequently Du Plessis et al. [11] generalized it to convex loss functions (denoted uPU in our experiments). in our experiments. Noting the problem of overﬁtting in modern overparameterized models, Kiryo et al. [23] propose a regularized extension that clips the loss on unlabeled data to zero. This is considered the current state-of-the-art in PU literature (denoted nnPU in our experiments). More recently, Ivanov [20] proposed DEDPUL, which ﬁnetunes the PvU classiﬁers using several heuristics, Bayes rule, and Expectation Maximization (EM). Since their method only applies a post-processing procedure, they rely on a good domain discriminator classiﬁer in the ﬁrst place and several hyperparameters for their heuristics. Several classical methods attempt to learn weights that identify reliable negative examples [30, 28, 26, 31, 44]. However, these earlier methods have not been successful with modern deep learning models.
3

Algorithm 1 Best Bin Estimation (BBE)

input : Validation positive (Xp) and unlabeled (Xu) samples. Blackbox model classiﬁer fp : X Ñ

r0, 1s. Hyperparameter 0 ă δ, γ ă 1.

1: Zp, Zu “ f pXpq, f pXuq.

2:

qupzq, qppzq “

ř
zi PZp

Irzi ěz s
,

ř
zi PZu

Irzi ěz s

for all z P r0, 1s.

pp

np

nu

ˆ

ˆ

˙˙

b

b

3:

Estimate

c p

:“

arg

mincPr0,1s

qqpuppccqq ` q1`pcγq

log2pn4{δq ` log2pn4{δq .

pp

pp

u

p

output

:

α

:“

qu pcq pp

p qppcq

pp

3 Problem Setup

By ||¨|| and x¨, ¨y, we denote the Euclidean norm and inner product, respectively. For a vector v P Rd, we use vj to denote its jth entry, and for an event E, we let I rEs denote the binary indicator of the event. By |A|, we denote the cardinality of set A. Let X P Rd be the input space and Y “ t´1, `1u
be the output space. Let P : X ˆ Y Ñ r0, 1s be the underlying joint distribution and let p denote its
corresponding density.

Let Pp and Pn be the class-conditional distributions for positive and negative class and pppxq “

ppx|y “ `1q and pnpxq “ ppx|y “ ´1q be the corresponding class-conditional densities. Pu

denotes the distribution of the unlabeled data and pu denotes its density. Let α P r0, 1s be the fraction

of positives among the unlabeled population, i.e., Pu “ αPp ` p1 ´ αqPn. When learning from

positive and unlabeled data, we obtain i.i.d. samples from the positive (class-conditional) distribution, which we denote as Xp “ tx1, x2, . . . , xnp u „ Pnpp and i.i.d samples from unlabeled distribution as

Xu

“

txnp`1,

xnp`2,

.

.

.

,

xnp`nu u

„

P

nu u

.

MPE is the problem of estimating α. Absent assumptions on Pp, Pn and Pu, the mixture proportion α is not identiﬁable [6]. Indeed, if Pu “ αPp ` p1 ´ αqPn, then any alternate decomposition of the form Pu “ pα ´ γqPp ` p1 ´ α ` γqP1n, for γ P r0, αq and P1n “ p1 ´ α ` γq´1pγPp ` p1 ´ αqPnq, is also valid. Since we do not observe samples from the distribution Pn, the parameter α is not identiﬁable. Blanchard et al. [6] formulate an irreducibility condition under which α is identiﬁable.
Intuitively, the condition restricts Pn to ensure that it can not be a (non-trivial) mixture of Pp and any other distribution. While this irreducibility condition makes α identiﬁable, in the worst-case,
the parameter α can be difﬁcult to estimate and any estimator must suffer an arbitrarily slow rate
of convergence [6]. In this paper, we propose mild conditions on the PvU classiﬁer that make α
identiﬁable and allows us to derive ﬁnite-sample convergence guarantees.

With PU learning, the aim is to learn a classiﬁer f : X Ñ r0, 1s to approximate ppy “ `1|xq. We

assume that we are given a loss function : r0, 1s ˆ Y Ñ R, such that pz, yq is the loss incurred by

predicting z when the true label is y. For a classiﬁer f and a sampled set X “ tx1, x2, . . . , xnu, we

let

Lp`pf

;

Xq

“

řn
i“1

pf pxiq, `1q{n denote the loss when predicting the samples as positive and

Lp´pf

;

Xq

“

řn
i“1

pf pxiq, ´1q{n the loss when predicting the samples as negative. For a sample

set

X

each

with

true

label

y,

we

deﬁne

0-1

error

as

Epypf ;

Xq

“

řn
i“1

I

rypf pxiq

´

tq

ď

0s

{n

for

some predeﬁned threshold t . Unless stated otherwise, the threshold is assumed to be 0.5.

4 Mixture Proportion Estimation
In this section, we introduce BBE, a new method that leverages a blackbox classiﬁer f to perform MPE and establish convergence guarantees. All proofs are relegated to App. B. To begin, we assume access to a ﬁxed classiﬁer f . For intuition, you may think of f as a PvU classifer trained on some portion fo the positive and unlabeled examples. In Sec. 5, we discuss other ways to obtain a suitable classiﬁer from PU data.
We now introduce some additional notation. Assume f transforms an input x P X to z P r0, 1s, i.e., z “ f pxq. For given probability density function p and a classiﬁer f , deﬁne a function qpzq “ şAz ppxqdx, where Az “ tx P X : f pxq ě zu for all z P r0, 1s. Intuitively, qpzq captures the cumulative density of points in a top bin, the proportion of input domain that is assigned a value larger than z by the classiﬁer f in the transformed space. We now deﬁne an empirical estimator qpzq given a
p

4

(a)

(b)

Figure 2: (a) Purity and size (in terms of fraction of unlabeled samples) in the top bin and (b)
Distribution of predicted probabilities (of being positive) for unlabeled training data as training proceeds with (TED)n. Results with ResNet-18 on binary-CIFAR. As in Fig. 1, we ﬁx W at 100. In
App. G.4, we show that as PvU training proceeds, the purity of top bin degrades and the distribution
of predicted probabilities of positives and negatives become less and less separable.

set

X

“

tx1,

x2,

.

.

.

,

xnu

sampled

iid

from

ppxq.

Let

Z

“

f pXq.

Deﬁne

qpzq p

“

řn
i“1

I

rzi

ě

zs

{n.

For each pdf pp, pn and pu, we deﬁne qp, qn and qu respectively.

Without any assumptions on the underlying distribution and the classiﬁer f , we aim to estimate
α˚ “ mincPr0,1s qupcq{qppcq with BBE. Later, under one mild assumption that empirically holds across numerous PU datasets, we show that α˚ “ α, i.e., α˚ matches the true mixture proportion α.

Our procedure proceeds as follows: First, given a held-out dataset of positive (Xp) and unlabeled examples (Xu), we push all examples through the classiﬁer f to obtain one-dimensional outputs Zp “ f pXpq and Zu “ f pXuq. Next, with Zp and Zu, we estimate qp and qu. Finally, we return the
pp ratio qupcq{qppcq at c that minimizes the upper conﬁdence bound (calculated using Lemma 1) at a
pppp p pre-speciﬁed level δ and a ﬁxed parameter γ P p0, 1q. Our method is summarized in Algorithm 1. For theoretical guarantees, we multiply the conﬁdence bound term with 1 ` γ for a small positive constant γ. Refer to App. B.1 for details. We now show that the proposed estimator comes with the

following guarantee:

Theorem 1. Deﬁne c˚ “ arg mincPr0,1s qupcq{qppcq. For minpnp, nuq ě 2 qlopgppc4˚{qδq and for every δ ą 0, the mixture proportion estimator α deﬁned in Algorithm 1 satisﬁes with probability 1 ´ δ:
p

˜d

d

¸

|α ´ α˚| ď c

p

qppc˚q

logp4{δq ` logp4{δq ,

nu

np

for some constant c ě 0.

Theorem 1 shows that with high probability, our estimate is close to α˚. The proof of the theorem is based on the following conﬁdence bound inequality:

Lemma 1. For every δ ą 0, with probability at least 1 ´ δ, we have for all c P r0, 1s

˜d

d

¸

qupcq qupcq

1

p´

ď

qppcq qppcq qppcq

p

p

logp4{δq ` qupcq logp4{δq .

2nu

qppcq 2np

Now, we discuss the convergence of our estimator to the true mixture proportion α. Since, pupxq “ αpppxq ` p1 ´ αqpnpxq, for all x P X , we have qupzq “ αqppzq ` p1 ´ αqqnpzq, for all z P r0, 1s.

Corollary 1. Deﬁne c˚ “ arg mincPr0,1s qnpcq{qppcq. Assume minpnp, nuq ě 2 qlopgppc4˚{qδq . For every δ ą 0, α (in Algorithm 1) satisﬁes with probability 1 ´ δ:
p

˜d

d

¸

α ´ c1 qppc˚q

logp4{δq nu `

logp4{δq np

ď α , and p

˜d

d

¸

α ď α ` p1 ´ αq qnpc˚q ` c2

p

qppc˚q qppc˚q

logp4{δq ` logp4{δq ,

nu

np

for some constant c1, c2 ě 0.

5

Algorithm 2 PU learning with Conditional Value Ignoring Risk (CVIR) objective

input : Labeled positive training data (Xp) and unlabeled training samples (Xu). Mixture proportion

estimate α.

1: Initialize a training model fθ and an stochastic optimization algorithm A.

2: Xn :“ Xu.

3: while training error Ep`pfθ; Xpq ` Ep´pfθ; Xnq is not converged do

4: Rank samples xu P Xu according to their loss values pfθpxuq, ´1q.

5: Xn :“ Xu,1´α where Xu,1´α denote the lowest ranked 1 ´ α fraction of samples.

6: Shufﬂe pXp, Xnq into B mini-batches. With pXpi , Xni q we denote i-th mini-batch.

7: for i “ 1 to B do ” ı

8:

Set the gradient ∇θ α ¨ Lp`pfθ; Xpi q ` p1 ´ αq ¨ Lp´pfθ; Xni q and update θ with algo. A.

9: end for 10: end while output : Trained classiﬁer fθ

As a corollary to Theorem 1, we show that our estimator α converges to the true α with convergence p
rate minpnp, nuq´1{2, as long as there exist a threshold cf P p0, 1q such that qppcf q ě p and qnpcf q “ 0 for some constant p ą 0. We refer to this condition as the pure positive bin property.
Note that in a more general case, our bound in Corollary 1 captures the tradeoff due to the proportion of negative examples in the top bin (bias) versus the proportion of positives in the top bin (variance).
Empirical Validation We now empirically validate the positive pure top bin property (Fig. 2). We observe that as PvU training proceeds, purity of the top bin improves for a ﬁxed fraction of samples in the top bin. Moreover, this behavior becomes more pronounced when learning a PvU classiﬁer with the CVIR objective proposed in the following section.
Comparison with existing methods Due to the intractability of Blanchard et al. [6] estimator, Scott [39] implements a heuristic based on identifying a point on the AUC curve such that the slope of the line segment between this point and (1,1) is minimized. While this approach is similar in spirit to our BBE method, there are some striking differences. First, the heuristic estimator in Scott [39] provides no theoretical guarantees, whereas we provide guarantees that BBE will converge to the best estimate achievable over all choices of the bin size and provide consistent estimates whenever a pure top bin exists. Second, while both estimates involve thresholds, the functional form of the estimates are different. Corroborating theoretical results of BBE, we observe that the choices in BBE create substantial differences in the empirical performance as observed in App. C. We work out details of comparison between Scott [39] heuristic and BBE in App. C.
On the other hand, recent works [21, 20] that use PvU classiﬁer for dimensionality reduction, discuss Bayes optimality of the PvU classiﬁer (or its one-to-one mapping) as a sufﬁcient condition to preserve α in transformed space. By contrast, we sho?w that the milder pure positive bin property is sufﬁcient to guarantee consistency and achieve Op1{ nq rates. Furthermore, in a simple toy setup in App. D, we show that even when the hypothesis class is well speciﬁed for PvN learning, it will not in general contain the Bayes optimal PvU classiﬁer and thus PvU training will not recover the Bayes-optimal scoring function, even in population. Contrarily, we show that any monotonic mapping of the Bayesoptimal PvU scoring function induces a positive pure top bin property. We leave further theoretical investigations concerning conditions under which a pure positive top bin arises to future work.

5 PU-Learning
Given positive and unlabeled data, we hope not only to identify α, but also to obtain a classiﬁer that distinguishes effectively between positive and negative samples. In supervised learning with separable data (e.g., cleanly labeled image data), overparameterized models generalize well even after achieving near-zero training error. However, with PvU training over-parameterized models can memorize the unlabeled positives, assigning them conﬁdently to the negative class, which can severely hurt generalization on PN data [43]. Moreover, while unbiased losses exist that estimate the PvN loss given PU data and the mixture proportion α, this unbiasedness only holds before the loss is optimized, and becomes ineffective with powerful deep learning models capable of memorization.
6

Algorithm 3 Transform-Estimate-Discard (TED)n

input : Positive data (Xp) and unlabeled samples (Xu). Hyperparameter W, δ.

1: Initialize a training model fθ and an stochastic optimization algorithm A. 2: Randomly split positive and unlabeled data into training Xp1, Xu1 and hold-out set (Xp2, Xu2).

3: Xn1 :“ Xu1.

{// Warm start with domain discrimination training}

4: for i “ 1 to W do 5: Shufﬂe pXp1, Xn1q into B mini-batches. With pXp1i, Xn1iq we denote i-th mini-batch.

6: for i “ 1 to B do

7:

Set

the

gradient

∇θ

” Lp` pfθ ;

Xp1iq

`

Lp´ pfθ ;

Xn1 i qı

and

update

θ

with

algorithm

A.

8: end for

9: end for

10: while training error Ep`pfθ; Xp1q ` Ep´pfθ; Xn1q is not converged do

11:

Estimate

α p

using

Algorithm

1

with

pXp2,

Xu2q

and

fθ

as

input.

12: Rank samples xu P Xu1 according to their loss values lpfθpxuq, ´1q.

13:

Xn1

:“

Xu1,1´α

where

Xu1,1´α

denote

the

lowest

ranked

1

´

α p

fraction

of

samples.

p

p

14: Train model fθ for one epoch on pXp1, Xn1q as in Lines 4-7.

15: end while

output : Trained classiﬁer fθ

A variety of heuristics, including ad-hoc early stopping criteria, have been explored [20], where training proceeds until the loss on unseen PU data ceases to decrease. However, this approach leads to severe under-ﬁtting (results in App. G.2). On the other hand, by regularizing the loss function, nnPU Kiryo et al. [23] mitigates overﬁtting issues due to memorization.
However, we observe that nnPU still leaves a substantial accuracy gap when compared to a model trained just on the positive and negative (from the unlabeled) data (ref. experiment in App. G.1). This leads us to ask the following question: can we improve performance over nnPU of a model just trained with PU data and bridge this gap? In an ideal scenario, if we could identify and remove all the positive points from the unlabeled data during training then we can hope to achieve improved performance over nnPU. Indeed, in practice, we observe that in the initial stages of PvU training, the model assigns much higher scores to positives than to negatives in the unlabeled data (Fig. 2(b)).
Inspired by this observation, we propose CVIR, a simple yet effective objective for PU learning. Below, we present our method assuming an access to the true MPE. Later, we combine BBE with CVIR optimization, yielding (TED)n, an alternating optimization that signiﬁcantly improves both the BBE estimates and the PvU classiﬁer.
Given a training set of positives Xp and unlabeled Xu and the mixture proportion α, we begin by ranking the unlabeled data according the predicted probability (of being positive) by our classiﬁer. Then, in every epoch of training, we create a (temporary) set of provisionally negative samples Xn by removing α fraction of the unlabeled samples currently scored as most positive. Next, we update our classiﬁer by minimize the loss on the positives Xp and provisional negatives Xn by treating them as negatives. We repeat this procedure until the training error on Xp and Xn converges. Likewise nnPU, note that this procedure does not need early stopping. Summary in Algorithm 2.
In App. E, we justify our loss function in the scenario when the positives and negatives are separable. For a more general scenario, we show that each step of our alternating procedure in CVIR cannot increase the population loss and hence, CVIR can only improve (or plateau) after every iteration.
(TED)n Integrating BBE and CVIR We are now ready to present our algorithm Transfer, Estimate and Discard (TED)n that combines BBE and CVIR objective.
First, we observe the interaction between BBE and CVIR objective. If we have an accurate mixture proportion estimate, then it leads to improved classiﬁer, in particular, we reject accurate number of prospective positive samples from unlabeled. Consequently, updating the classiﬁer to minimize loss on positive versus retained unlabeled improves purity of top bin. This leads to an obvious alternating procedure where at each epoch, we ﬁrst use BBE to estimate α and then update the classiﬁer with
p CVIR objective with α as input. We repeat this until training error has not converged. Our method is
p summarized in Algorithm 3.

7

Accuracy Mixture Proportion

85 80 75 70 65 60 55 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

1.0

0.8

0.6

0.4
(TED)n DEDPUL
0.2 EN Alphamax ( * )
0.0 0 500 True MPE

Ep10o0c0hs 1500

2000

Figure 3: Epoch wise results with ResNet-18 trained on binary-CIFAR when α is 0.5. Parallel results on other datasets and architectures in App. G.3. For both classiﬁcation and MPE, (TED)n substantially improves over existing methods. Additionally, (TED)n maintains the superior performance till
convergence removing the need for early stopping. Results aggregated over 3 seeds.

Note that we need to warm start with PvU (positive versus negative) training, since in the initial stages mixture proportion estimate is often close to 1 rejecting all the unlabeled examples. However, in next section, we show that our procedure is not sensitive to the choice of number of warm start epochs and in a few cases with large datasets, we can even get away without warm start (i.e., W “ 0) without hurting the performance. Moreover, recall that our aim is to distinguish positive versus negative examples among the unlabeled set where the proportion of positives is determined by the true mixture proportion α. However, unlike CVIR, we do not re-weight the losses in (TED)n. While true MPE α is unknown, one natural choice is to use the estimate α at each iteration. However, in
p our initial experiments, we observed that re-weighted objective with estimate α led to comparatively
p poor classiﬁcation performance due to presence of bias in estimate α in the initial iterations. We note
p that for deep neural networks (for which model mis-speciﬁcation is seldom a prominent concern) and when the underlying classes are separable (as with most image datasets), it is known that importance weighting has little to no effect on the ﬁnal classiﬁer [7]. Therefore, we may not need importancereweighting with (TED)n on separable datasets. Consequently, following earlier works [23, 11] we do not re-weight the loss with our (TED)n procedure. In future work, a simple empirical strategy can be explored where we ﬁrst obtain an estimate of α by running the full (TED)n procedure till
p convergence and then discarding the (TED)n classiﬁer, we use estimate α to train a fresh classiﬁer
p with CVIR procedure.
Finally, we discuss an important distinction with Dedpul which is also an alternating procedure. While in our algorithm, after updating mixture proportion estimate we retrain the classiﬁer, Dedpul ﬁxes the classiﬁer, obtains output probabilities and then iteratively updates the mixture proportion estimate (prior) and output probabilities (posterior). Dedpul doesn’t re-train the classiﬁer.

6 Experiments
Having presented our PU learning and MPE algorithms, we now compare their performance with other methods empirically. We mainly focus on vision and text datasets in our experiments. We include results on UCI datasets in App. G.7.
Datasets and Evaluation We simulate PU tasks on CIFAR-10 [24], MNIST [25], and IMDb sentiment analysis [32] datasets. We consider binarized versions of CIFAR-10 and MNIST. On CIFAR-10 dataset, we consider two classiﬁcation problems: (i) binarized CIFAR, i.e., ﬁrst 5 classes vs rest; (ii) Dog vs Cat in CIFAR. Similarly, on MNIST, we consider: (i) binarized MNIST, i.e., digits 0-4 vs 5-9; (ii) MNIST17, i.e., digit 1 vs 7. IMDb dataset is binary. For MPE, we use a held out PU validation set. To evaluate PU classiﬁers, we calculate accuracy on held out positive versus negative dataset. For baselines that suffer from issues due to overﬁtting on unlabeled data, we report results with an oracle early stopping criterion. In particular, we report the accuracy averaged over 10 iterations of the best performing model as evaluated on positive versus negative data. Note that we use this oracle stopping criterion only for previously proposed methods and not for methods proposed

8

Dataset

Model (TED)n BBE˚ DEDPUL˚ AlphaMax˚ EN˚ KM2 TiCE

Binarized CIFAR
CIFAR Dog vs Cat
Binarized MNIST MNIST17 IMDb

ResNet All Conv
MLP
ResNet All Conv
MLP
MLP
BERT

0.026 0.042 0.225
0.078 0.066
0.024
0.003
0.008

0.091 0.037 0.177
0.176 0.128
0.032
0.023
0.011

0.091 0.052 0.138
0.170 0.115
0.031
0.021
0.016

0.125 0.09 0.3
0.17 0.19
0.090
0.075
0.07

0.192 0.221 0.168 0.251 0.372

0.226 0.331 0.286 0.250

0.080 0.029 0.056

0.028 0.022 0.043

0.12 -

-

Table 1: Absolute estimation error when α is 0.5. "*" denote oracle early stopping as deﬁned in Sec. 6. (TED)n signiﬁcantly reduces estimation error when compared with existing methods. Results
reported by aggregating absolute error over 10 epochs and 3 seeds. For aggregate numbers with
standard deviation see App. G.6.

Dataset

(TED)n

CVIR

PvU˚ DEDPUL˚ nnPU

uPU˚

Model (unknown α) (known α) (known α) (unknown α) (known α) (known α)

ResNet

82.7

82.3

76.9

77.1

77.2

76.7

Binarized All Conv 77.9

78.1

75.8

77.1

73.4

72.5

CIFAR

MLP

64.2

66.9

61.6

62.6

63.1

64.0

CIFAR Dog ResNet

75.2

73.3

67.3

67.0

71.8

68.8

vs Cat All Conv 73.0

71.7

70.5

69.2

67.9

67.5

Binarized MLP

95.6

96.3

94.2

94.8

96.1

95.2

MNIST

MNIST17 MLP

98.7

98.7

96.9

97.7

98.4

98.4

IMDb

BERT

87.6

87.4

86.1

87.3

86.2

85.9

Table 2: Accuracy for PvN classiﬁcation with PU learning. "*" denote oracle early stopping as
deﬁned in Sec. 6. Results reported by aggregating over 10 epochs and 3 seeds. Both CVIR (with known MPE) and (TED)n (with unknown MPE) signiﬁcantly improve over previous baselines with
oracle early stopping and known MPE. For aggregate numbers with standard deviation see App. G.6.

in this work. This allows us to compare (TED)n with the best performance achievable by previous methods that suffer from over-ﬁtting issues. With nnPU and (TED)n, we report average accuracy over 10 iterations of the ﬁnal model.
Architectures For CIFAR datasets, we consider (fully connected) multilayer perceptrons (MLPs) with ReLU activations, all convolution nets [40], and ResNet18 [19]. For MNIST, we consider multilayer perceptrons (MLPs) with ReLU activations For the IMDb dataset, we ﬁne-tune an offthe-shelf uncased BERT model [10, 42]. We did not tune hyperparameters or the optimization algorithm—instead we use the same benchmarked hyperparameters and optimization algorithm for each dataset. For our method, we use cross-entropy loss. For uPU and nnPU, we use Adam [22] with sigmoid loss. We provide additional details about the datasets and architectures in App. F.
Mixture Proportion Estimation First, we discuss results for MPE (Table 1). We compare our method with KM2, TiCE, DEDPUL, AlphaMax and EN. Following earlier works [20, 35], we reduce datasets to 50 dimensions with PCA for KM2 and TiCE. We use existing implementation for other methods2. For BBE, DEDPUL and Alphamax, we use the same PvU classiﬁer as input. On CIFAR datasets, convolutional classiﬁer based estimators signiﬁcantly outperform KM2 and TiCE.
2DEDPUL: https://github.com/dimonenka/DEDPUL, KM: https://web.eecs.umich.edu/~cscott/code.html#kmpe, TiCE: https://dtai.cs.kuleuven.be/software/tice, and AlphaMax: https://github.com/Dzeiberg/AlphaMax

9

In contrast, the performance of KM2 is comparable to DEDPUL on MNIST datasets. On all datasets, (TED)n achieves lowest estimation error. With the same blackbox classiﬁer obtained with oracle early stopping, BBE performs similar or better than best alternate(s). Since overparamterized models start memorizing unlabeled samples negatives, the quality of MPE degrades substantially as PvU training proceeds for all methods but (TED)n as in Fig. 3 (epoch-wise results for on other tasks in App. G.3).
Classiﬁcation with known MPE Now, we discuss results for classiﬁcation with known α. We compare our method with uPU, nnPU3, DEDPUL and PvU training. Although, we solve both MPE and classiﬁcation, some comparison methods do not. Ergo, we compare our classiﬁcation algorithm with known MPE (Algorithm 2).
To begin, ﬁrst we note that nnPU and PvU training with CVIR doesn’t need early stopping. For all other methods, we report the best performance dictated by the aformentioned oracle stopping criterion. On all datasets, PvU training with CVIR leads to improved classiﬁcation performance when compared with alternate approaches (Table 2). Moreover, as training proceeds (Fig. 3), the performance of DEDPUL, PvU training and uPU substantially degrade. We repeated experiments with the early stopping criterion mentioned in DEDPUL (App. G.2), however, their early stopping criterion is too pessimistic resulting in poor results due to under-ﬁtting.
Classiﬁcation with unknown MPE Finally, we evaluate (TED)n, our alternating procedure for MPE and PU learning. Across many tasks, we observe substantial improvements over existing methods. Note that these improvements often are over an oracle early stopping baselines highlighting signiﬁcance of our procedure.
In App. G.5, we show that our procedure is not sensitive to warm start epochs W, and in many tasks with W “ 0, we observe minor-to-no differences in the performance of (TED)n. While for the experiments in this section, we used ﬁxed W “ 100, in the Appendix we show behavior with varying W. We also include ablations with different mixture proportions α.
7 Conclusion and Future Work
In this paper, we proposed two practical algorithms, BBE (for MPE) and CVIR optimization (for PU learning). Our methods outperform others empirically and BBE’s mixture proportion estimates leverage black box classiﬁers to produce (nearly) consistent estimates with ﬁnite sample convergence guarantees whenever we possess a classiﬁer with a (nearly) pure top bin. Moreover, (TED)n combines our procedures in an iterative fashion, achieving further gains. An important next direction is to extend our work to the multiclass problem [38], bridging work on label shift and PU learning. Here, we imagine that a deployed k-way classiﬁer may encounter not only label shift among previously seen classes ([29, 17]) but also, potentially, instances from one previously unseen class. We also plan to investigate distributional properties under which we can hope to reliably or approximately satisfy the pure positive bin property with an off-the-shelf classiﬁer trained on PvU data. While we improve signiﬁcantly over previous PU methods, there is still a gap between (TED)n’s performance and PvN training. We hope that our work can open a pathway towards further narrowing this gap.
Acknowledgements
We thank anonymous reviewers for their feedback during NeurIPS 2021 review process. This material is based on research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of Air Force Laboratory, DARPA or the U.S. Government. SB acknowledges funding from the NSF grants DMS-1713003, DMS-2113684 and CIF-1763734, as well as Amazon AI and a Google Research Scholar Award. ZL acknowledges Amazon AI, Salesforce Research, Facebook, UPMC, Abridge, the PwC Center, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI) via Department of Defense contract FA8702-15-D-0002, for their generous support of ACMI Lab’s research on machine learning under distribution shift.
3uPU and nnPU: https://github.com/kiryor/nnPUlearning
10

References
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, 2016.
[2] A. Alexandari, A. Kundaje, and A. Shrikumar. Adapting to label shift with bias-corrected calibration. In arXiv preprint arXiv:1901.06852, 2019.
[3] K. Azizzadenesheli, A. Liu, F. Yang, and A. Anandkumar. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations (ICLR), 2019.
[4] J. Bekker and J. Davis. Estimating the class prior in positive and unlabeled data through decision tree induction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.
[5] J. Bekker and J. Davis. Learning from positive and unlabeled data: a survey. Mach. Learn., 2020.
[6] G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. The Journal of Machine Learning Research, 11:2973–3009, 2010.
[7] J. Byrd and Z. C. Lipton. What is the effect of importance weighting in deep learning? In International Conference on Machine Learning (ICML), 2019.
[8] F. De Comité, F. Denis, R. Gilleron, and F. Letouzey. Positive and unlabeled examples help learning. In International Conference on Algorithmic Learning Theory, pages 219–230. Springer, 1999.
[9] F. Denis. Pac learning from positive statistical queries. In International Conference on Algorithmic Learning Theory, pages 112–126. Springer, 1998.
[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[11] M. Du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and unlabeled data. In International conference on machine learning, pages 1386–1394, 2015.
[12] M. C. Du Plessis and M. Sugiyama. Class prior estimation from positive and unlabeled data. IEICE TRANSACTIONS on Information and Systems, 97(5):1358–1362, 2014.
[13] M. C. Du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-prior change by distribution matching. Neural Networks, 50:110–119, 2014.
[14] M. C. Du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled data. Advances in neural information processing systems, 27:703–711, 2014.
[15] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages 642–669, 1956.
[16] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 213–220, 2008.
[17] S. Garg, Y. Wu, S. Balakrishnan, and Z. C. Lipton. A uniﬁed view of label shift estimation. arXiv preprint arXiv:2003.07554, 2020.
[18] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.
[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.
11

[20] D. Ivanov. DEDPUL: Difference-of-estimated-densities-based positive-unlabeled learning. arXiv preprint arXiv:1902.06965, 2019.
[21] S. Jain, M. White, M. W. Trosset, and P. Radivojac. Nonparametric semi-supervised learning of class proportions. arXiv preprint arXiv:1601.01944, 2016.
[22] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv Preprint arXiv:1412.6980, 2014.
[23] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In Advances in neural information processing systems, pages 1675–1685, 2017.
[24] A. Krizhevsky and G. Hinton. Learning Multiple Layers of Features from Tiny Images. Technical report, Citeseer, 2009.
[25] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86, 1998.
[26] W. S. Lee and B. Liu. Learning with positive and unlabeled examples using weighted logistic regression. In ICML, volume 3, pages 448–455, 2003.
[27] F. Letouzey, F. Denis, and R. Gilleron. Learning from positive and unlabeled examples. In International Conference on Algorithmic Learning Theory, pages 71–85. Springer, 2000.
[28] X. Li and B. Liu. Learning to classify texts using positive and unlabeled data. In IJCAI. Citeseer, 2003.
[29] Z. C. Lipton, Y.-X. Wang, and A. Smola. Detecting and Correcting for Label Shift with Black Box Predictors. In International Conference on Machine Learning (ICML), 2018.
[30] B. Liu, W. S. Lee, P. S. Yu, and X. Li. Partially supervised classiﬁcation of text documents. In ICML, 2002.
[31] B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. Building text classiﬁers using positive and unlabeled examples. In Third IEEE International Conference on Data Mining, pages 179–186. IEEE, 2003.
[32] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150, 2011.
[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019.
[34] S. Rabanser, S. Günnemann, and Z. C. Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[35] H. Ramaswamy, C. Scott, and A. Tewari. Mixture proportion estimation via kernel embeddings of distributions. In International conference on machine learning, pages 2052–2060, 2016.
[36] H. Reeve and A. Kabán. Exploiting geometric structure in mixture proportion estimation with generalised blanchard-lee-scott estimators. In Proceedings of the 30th International Conference on Algorithmic Learning Theory, pages 682–699. PMLR, 2019.
[37] M. Saerens, P. Latinne, and C. Decaestecker. Adjusting the Outputs of a Classiﬁer to New a Priori Probabilities: A Simple Procedure. Neural Computation, 2002.
[38] T. Sanderson and C. Scott. Class proportion estimation with application to multiclass anomaly rejection. In Artiﬁcial Intelligence and Statistics, pages 850–858, 2014.
12

[39] C. Scott. A rate of convergence for mixture proportion estimation, with application to learning from noisy labels. In Artiﬁcial Intelligence and Statistics, pages 838–846, 2015.
[40] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
[41] A. Storkey. When Training and Test Sets Are Different: Characterizing Learning Transfer. Dataset Shift in Machine Learning, 2009.
[42] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45. Association for Computational Linguistics, 2020.
[43] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
[44] D. Zhang and W. S. Lee. A simple probabilistic approach to learning from positive and unlabeled examples. In Proceedings of the 5th annual UK workshop on computational intelligence (UKCI), pages 83–87, 2005.
13

A Appendix

B Proofs from Sec. 4

Proof of Lemma 1. The proof primarily involves using DKW inequality [15] on qupcq and qppcq to

p

p

show convergence to their respective means qupcq and qppcq. First, we have

qupcq qupcq p´ qppcq qppcq p

1

“

|qupcq ¨ qppcq ´ qppcq ¨ qupcq ` qppcq ¨ qupcq ´ qppcq ¨ qupcq|

qupcq ¨ qupcq p

p

p

ď 1 |qupcq ´ qupcq| ` qupcq |qppcq ´ qppcq| . (1)

qppcq p

qppcq ¨ qupcq p

p

p

b Using DKW inequality, we have with probability 1 ´ δ: |qpppcq ´ qppcq| ď log2pn2p{δq for all c P r0, 1s.
b Similarly, we have with probability 1 ´ δ: |qpupcq ´ qupcq| ď log2pn2u{δq for all c P r0, 1s. Plugging this in (1), we have

˜d

d

¸

qupcq qupcq

1

p´

ď

qppcq qppcq qppcq

p

p

logp4{δq ` qupcq logp4{δq .

2nu

qppcq 2np

Proof of Theorem 1. The main idea of the proof is to use the conﬁdence bound derived in Lemma 1 at

c and use the fact that c minimizes the upper conﬁdence bound. The proof is split into two parts. First,

p

p

we derive a lower bound on qppcq and next, we use the obtained lower bound to derive conﬁdence pp

bound on α. All the statements in the proof simultaneously hold with probability 1 ´ δ. Recall, p

˜d

d

¸

c

:“

arg

min

qupcq p

`

1

logp4{δq ` p1 ` γq logp4{δq

and

(2)

p

cPr0,1s

qppcq p

qppcq p

2nu

2np

α

:“

qupcq pp

.

(3)

p qppcq

pp

Moreover,

c˚ :“ arg min qupcq and α˚ :“ qupc˚q .

(4)

cPr0,1s qppcq

qppc˚q

Part

1:

We

establish

lower

bound

on

qppcq. pp

Consider

c1

P

r0, 1s

such

that

qppc1q p

“

γ 2`

γ

qp p

p

c˚

q.

We

will now show that Algorithm 1 will select c ă c1. For any c P r0, 1s, we have with with probability p

1 ´ δ,

d

d

qppcq ´ logp4{δq ď qppcq and qupcq ´ logp4{δq ď qupcq . (5)

p

2np

2nu

p

Since qqupppcc˚˚qq ď qqupppccqq , we have

d

˜

d

¸

d

qupc˚q

logp4{δq

logp4{δq qupc˚q

logp4{δq

qupcq ě qppcq

´

ě qppcq ´

´

. (6)

p

qppc˚q

2nu

p

2np qppc˚q

2nu

Therefore, at c we have

˜d

d

¸

qupcq p

ě

α˚

´

1

logp4{δq ` qupc˚q logp4{δq .

(7)

qppcq

qppcq

2nu

qppc˚q 2np

p

p

14

Using Lemma 1 at c˚, we have

˜d

d

¸

qupcq qupc˚q ˆ 1

1 ˙ logp4{δq qupc˚q logp4{δq

p ěp ´

`

`

(8)

qppcq qppc˚q qppc˚q qppcq

2nu

qppc˚q 2np

p

p

p

p

˜d

d

¸

qupc˚q ˆ 1

1 ˙ logp4{δq logp4{δq

ěp ´

`

`

,

(9)

qppc˚q qppc˚q qppcq

2nu

2np

p

p

p

where the last inequality follows from the fact that α˚ “ qqupppcc˚˚qq ď 1. Furthermore, the upper

conﬁdence bound at c is lower bound as follows:

˜d

d

¸

qupcq 1 ` γ logp4{δq logp4{δq

p`

`

(10)

qppcq qppcq

2nu

2np

p

p

˜d

d

¸

qupc˚q ˆ 1 ` γ

1

1 ˙ logp4{δq logp4{δq

ěp `

´

´

`

(11)

qppc˚q

qppcq qppc˚q qppcq

2nu

2np

p

p

p

p

˜d

d

¸

qupc˚q ˆ γ

1 ˙ logp4{δq logp4{δq

“p `

´

`

(12)

qppc˚q qppcq qppc˚q

2nu

2np

p

p

p

Using (12) at c “ c1, we have the following lower bound on ucb at c1:

˜d

d

¸

qupc1q 1 ` γ

logp4{δq

logp4{δq

p`

`

(13)

qppc1q qppc1q

2nu

2np

p

p

˜d

d

¸

qupc˚q 1 ` γ

logp4{δq

logp4{δq

ěp `

`

,

(14)

qppc˚q qppc˚q

2nu

2np

p

p

Moreover from (12), we also have that the lower bound on ucb at c ě c1 is strictly greater than the lower bound on ucb at c1. Using deﬁnition of c, we have
p

˜d

d

¸

qupc˚q 1 ` γ

logp4{δq

logp4{δq

p`

`

(15)

qppc˚q qppc˚q

2nu

2np

p

p

˜d

d

¸

qupcq 1 ` γ logp4{δq logp4{δq

ěp p `

`

,

(16)

qppcq qppcq

2nu

2np

pp pp

and hence

c ď c1 .

(17)

p

Part 2: We now establish an upper and lower bound on α. We start with upper conﬁdence bound on p

α. By deﬁnition of c, we have

p

p

˜d

d

¸

qupcq 1 ` γ logp4{δq logp4{δq

p p`

`

(18)

qppcq qppcq

2nu

2np

pp pp

«

˜d

d

¸ﬀ

qupcq 1 ` γ logp4{δq logp4{δq

ď min p `

`

(19)

cPr0,1s qppcq qppcq

2nu

2np

p

p

˜d

d

¸

qupc˚q 1 ` γ

logp4{δq

logp4{δq

ďp `

`

.

(20)

qppc˚q qppc˚q

2nu

2np

p

p

Using Lemma 1 at c˚, we get

˜d

d

¸

qupc˚q qupc˚q

1

p

ď

`

qppc˚q qppc˚q qppc˚q

p

p

logp4{δq qupc˚q 2nu ` qppc˚q

logp4{δq 2np

˜d

d

¸

“ α˚ ` 1

logp4{δq ` α˚ logp4{δq .

(21)

qppc˚q

2nu

2np

p

15

Combining (20) and (21), we get

˜d

d

¸

α“

qupcq pp

ď α˚ `

2`γ

logp4{δq ` logp4{δq .

(22)

p qppcq

qppc˚q

2nu

2np

pp

p

b

Using DKW inequality on qppc˚q, we have qppc˚q ě qppc˚q ´

p

p

log2pn4{δq . Assuming np ě 2 qlo2gppc4˚{qδq ,

p

p

we get qppc˚q ď qppc˚q{2 and hence, p

˜d

d

¸

α ď α˚ ` 4 ` 2γ logp4{δq ` logp4{δq .

(23)

p

qppc˚q

2nu

2np

Finally, we now derive a lower bound on α. From Lemma 1, we have the following inequality at c

p

p

˜d

d

¸

qupcq qupcq 1

logp4{δq qupcq logp4{δq

p ďp p `

`p

.

(24)

qppcq qppcq qppcq

2nu

qppcq 2np

p pp pp

p

Since α˚ ď qqupppccpqq , we have
p

˜d

d

¸

α˚

ď

qupcq p

ď

qupcq pp

`

1

logp4{δq qupcq logp4{δq

`p

.

(25)

qppcq qppcq qppcq

2nu

qppcq 2np

p pp pp

p

Using (23), we obtain a very loose upper bound on qqpuppccpqq . Assuming minpnp, nuq ě 2 qlo2gppc4˚{qδq , we

pp p

p

have qqpupppccpqq ď α˚ ` 4 ` 2γ ď 5 ` 2γ. Using this in (25), we have
pp

˜d

d

¸

α˚

ď

qupcq pp

`

1

logp4{δq ` p5 ` 2γq logp4{δq .

(26)

qppcq qppcq

2nu

2np

pp pp

Moreover,

as

cě p

c1,

we

have

qppcq pp

ě

2

γ `γ

qp p

pc

˚

q

and

hence,

˜d

d

¸

α˚ ´ γ ` 2

logp4{δq

logp4{δq qupcq

` p5 ` 2γq

ď p p “ α.

(27)

γqppc˚q

2nu

2np

qppcq p

p

pp

As

we

assume

np

ě

2 qlo2gppc4˚{qδq ,

we

have

qppc˚q p

ď

qppc˚q{2,

which

implies

the

following

lower

bound

on α: p

˜d

d

¸

α˚ ´ 2γ ` 4 logp4{δq ` p5 ` 2γq logp4{δq ď α .

(28)

γqppc˚q

2nu

2np

p

Proof of Corollary 1. Note that since α ď α˚, the lower bound remains the same as in Theorem 1. For upper bound, plugging in qupcq “ αqppcq`p1´αqqnpcq, we have α˚ “ α`p1´αqqnpc˚q{qppc˚q
and hence, the required upper bound.

B.1 Note on γ in Algorithm 1

We multiply the upper bound in Lemma 1 to establish lower bound on qppcq. Otherwise, in an pp

extreme case, with γ “ 0, Algorithm 1 can select c with arbitrarily low qppcq (! qppc˚q) and hence

p

pp

poor concentration guarantee to the true mixture proportion. However, with a small positive γ, we

can obtain lower bound on qppcq and hence tight guarantees on the ratio estimate (qupcq{qppcq) in

pp

pppp

Theorem 1.

In our experiments, we choose γ “ 0.01. However, we didn’t observe any (signiﬁcant) differences in mixture proportion estimation even with γ “ 0. implying that we never observe qppcq taking
pp arbitrarily small values in our experiments.

16

Dataset

Model (TED)n BBE˚ DEDPUL˚ Scott˚

Binarized CIFAR CIFAR Dog vs Cat
Binarized MNIST
MNIST17

ResNet 0.018 0.072

ResNet 0.074 0.120

MLP MLP

0.021 0.003

0.028 0.008

0.075 0.113 0.027 0.006

0.091 0.158 0.063 0.037

Table 3: Absolute estimation error when α is 0.5. "*" denote oracle early stopping as deﬁned in Sec. 6. As mentioned in Scott [39] implementation in https://web.eecs.umich.edu/~cscott/code/mpe_v2.zip, we use the binomial inversion at δ instead of δ{n (rescaling using the union bound). Since we are using Binomial inversion at n discrete points simultaneously, we should use the union-bound penalty. However, using union bound penalty substantially increases the bias in their estimator.

C Comparison of BBE with Scott [39]

Heuristic estimator due to Scott [39] is motivated by the estimator in Blanchard et al. [6]. The estimator in Blanchard et al. [6] relies on VC bounds, which are known to be loose in typical deep learning situations. Therefore, Scott [39] proposed an heuristic implementation based on the minimum slope of any point in the ROC space to the point p1, 1q. To obtain ROC estimates, authors use direct binomial tail inversion (instead of VC bounds as in Blanchard et al. [6]) to obtain tight upper bounds for true positives and lower bounds for true negatives. Finally, using these conservatives estimates the estimator in Scott [39] is obtained as the minimum slope of any of the operating points to the point p1, 1q.

While the estimate of one minus true positives at a threshold t is similar in spirit to our number of unlabeled examples in the top bin and the estimate of one minus true negatives at a threshold t is similar in spirit to our number of positive examples in the unlabeled data, the functional form of these estimates are very different. Scott [39] estimator is the ratio of quantities obtained by binomial tail inversion (i.e. upper bound in the numerator and lower bound in the denominator). By contrast, the ﬁnal BBE estimate is simply the ratio of empirical CDFs at the optimal threshold. Mathematically, we have

αpScott “ qpupcScottq ` binvpnu, qpupcScottq, δ{nuq and (29)

qp p

pcScott

q

´

binvpnp

,

qp p

pcScott

q,

δ{np

q

αpBBE “ qpupcBBEq , (30) qpppcBBEq

where

cScott

“

arg mincPr0,1s

qu pcq`binvpnu ,qu pcq,δ {nu q

p

p

q pcq´binvpn ,q pcq,δ{n q

and

binvpnp, qppcq, δ{npq

is

the

tightest

pos-

pp

p pp

p

sible deviation bound for a binomial random variable [39] and and cBBE is given by Algorithm 1.

Moreover, Scott [39] provide no theoretical guarantees for their heuristic estimator αpScott. On the

hand, we provide guarantees that our estimator αpBBE will converge to the best estimate achievable over

all choices of the bin size and provide consistent estimates whenever a pure top bin exists. Supporting

theoretical results of BBE, we observe that these choices in BBE create substantial differences in the

empirical performance as observed in Table 3. We repeat experiment for MPE from Sec. 6 where we

compare other methods with the Scott [39] estimator as deﬁned in (29).

As a side note, a naive implementation of αpScott instead of (29) where we directly minimize the empirical ratio yields poor estimates due to noise introduced with ﬁnite samples. In our experiments, we observed that αpScott improves a lot over this naive estimator.

D Toy setup
Jain et al. [21] and Ivanov [20] discuss Bayes optimality of the PvU classiﬁer (or its one-to-one mapping) as a sufﬁcient condition to preserve α in transformed space. However, in a simple toy setup (in App. D), we show that even when the hypothesis class is well speciﬁed for PvN learning, it will not in general contain the Bayes optimal scoring function for PvU data and thus PvU training will not recover the Bayes-optimal scoring function, even in population.

17

(a)

Figure 4: Blue points show samples from the positive distribution and orange points show samples from the negative distribution. Unalabeled data is obtained by mixing positive and negative distribution with equal proportion. BCE (or Brier) loss minimization on P vs U data leads to a classiﬁers that is not consistent with the ranking of the Bayes optimal score function.

Consider a scenario with X “ R2. Assume points from the positive class are sampled uniformly from the interior of the triangle deﬁned by coordinates tp´1, 0.1q, p0, 4q, p1, 0.1qu and negative points are sampled uniformly from the interior of triangle deﬁned by coordinates tp´1, ´0.1q, p4, ´4q, p1, ´0.1qu. Ref. to Fig. 4 for a pictorial representation. Let mixture proportion be 0.5 for the unlabeled data. Given access to distribution of positive data and unlabeled data, we seek to train a linear classiﬁer to minimize logistic or Brier loss for PvU training.
Since we need a monotonic transformation of the Bayes optimal scoring function, we want to recover a predictor parallel to x-axis, the Bayes optimal classiﬁer for PvN training. However, minimizing the logistic loss (or Brier loss) using numerical methods, we obtain a predictor that is inclined at a non-zero acute angle to the x-axis. Thus, the PvU classiﬁer obtained fails to satisfy the sufﬁcient condition from Jain et al. [21] and Ivanov [20]. On the other hand, note that the linear classiﬁer obtained by PvU training satisﬁes the pure positive bin property.
Now we show that under the subdomain assumption [39, 35], any monotonic transformation of Bayes optimal scoring function induces positive pure bin property. First, we deﬁne the subdomain assumption.
Assumption 1 (Subdomain assumption). A family of subsets S Ď 2X , and distributions pp, pn are said to satisfy the anchor set condition with margin γ ą 0, if there exists a compact set A P S such that A Ď supppppq{suppppnq and pppAq ě γ.

Note that any monotonic mapping of the Bayes optimal scoring function can be represented by τ 1 “ g ˝ τ , where g is a monotonic function and

τ pxq “ "pppxq{pupxq if pppxq ą 0 (31)

0

o.w .

For any point x P A and x1 P X {A, we have τ pxq ą τ px1q which implies τ 1pxq ą τ 1px1q. Thus, any monotonic mapping of Bayes optimal scoring function yields the positive pure bin property with p ě γ.

E Analysis of CVIR
First we analyse our loss function in the scenario when the support of positives and negatives is separable. We assume that the true alpha α is known and we have access to populations of positive and unlabeled data. We also assume that their exists a separator f ˚ : X ÞÑ t0, 1u that can perfectly separate the positive and negative distribution, i.e., ş dxpppxqI rf ˚pxq ‰ 1s ` ş dxpnpxqI rf ˚pxq ‰ 0s “ 0. Our learning objective can be written as jointly optimizing a classiﬁer f and a weighting function w

18

on the unlabeled distribution:

ż

1ż

min dxpppxqlpf pxq, 1q `

dxpupxqwpxqlpf pxq, 0q ,

f PF ,w

1´α

ż

s.t. w : X ÞÑ r0, 1s , dxpupxqwpxq “ 1 ´ α .

(32)

The following proposition shows that minimizing the objective (32) on separable positive and negative distributions gives a perfect classiﬁer.
Proposition 1. For α P p0, 1q, if there exists a classiﬁer f ˚ P F that can perfectly separate the positive and negative distributions, optimizing objective (32) with 0-1 loss leads to a classiﬁer f that achieves 0 classiﬁcation error on the unlabeled distribution.

Proof. First we observe that having wpxq “ 1 ´ f ˚pxq leads to the objective value being minimized to 0 as well as a perfect classiﬁer f . This is because

1

ż

ż

dxpupxqp1 ´ f ˚pxqqlpf pxq, 0q “ dxpnpxqlpf pxq, 0q

1´α

thus the objective becomes classifying positive v.s. negative, which leads to a perfect classiﬁer if F contains one. Now we show that for any f such that the classiﬁcation error is non-zero then the objective (32) must be greater than zero no matter what w is. Suppose f satisﬁes

ż

ż

dxpppxqlpf pxq, 1q ` dxpnpxqlpf pxq, 0q ą 0 .

We know that either ş dxpppxqlpf pxq, 1q ą 0 or ş dxpnpxqlpf pxq, 0q ą 0 will hold. If ş dxpppxqlpf pxq, 1q ą 0 we know that (32) must be positive. If ş dxpppxqlpf pxq, 1q “ 0 and ş dxpnpxqlpf pxq, 0q ą 0 we have lpf pxq, 0q “ 1 almost everywhere in pppxq thus

1ż 1 ´ α dxpupxqwpxqlpf pxq, 0q

αż

ż

“ 1 ´ α dxpppxqwpxqlpf pxq, 0q ` dxpnpxqwpxqlpf pxq, 0q

αż

ż

“ 1 ´ α dxpppxqwpxq ` dxpnpxqwpxqlpf pxq, 0q .

If ş dxpppxqwpxq ą 0 we know that (32) must be positive. If ş dxpppxqwpxq “ 0, since we know that

ż

ż

ż

dxpupxqwpxq “ α dxpppxqwpxq ` p1 ´ αq dxpnpxqwpxq “ 1 ´ α

we have ş dxpnpxqwpxq “ 1 which means wpxq “ 1 almost everywhere in pnpxq. This leads to the fact that ş dxpnpxqlpf pxq, 0q ą 0 indicates ş dxpnpxqwpxqlpf pxq, 0q ą 0, which concludes the
proof.

The intuition is that, any classiﬁer that discards an α ą 0 proportion of negative distribution from r
unlabeled will have loss strictly greater than zero with our CVIR objective. Since only a perfect linear separator (with weights Ñ 8) can achieves loss Ñ 0, CVIR objective will (correctly) discard the α proportion of positive from unlabeled data achieving a classiﬁer that perfectly separates the data.

We leave theoretic investigation on non-separable distributions for future work. However, as an initial step towards a general theory, we show that in the population case one step of our alternating procedure cannot increase the loss.

Consider the following objective function

Lpft, wtq “ Ex„Pp rlpftpxq, 0qs ` Ex„Pu rwtpxqlpftpxq, 1qs

(33)

such that Ex„Pu rwpxqs “ 1 ´ α and wpxq P t0, 1u

19

Given ft and wt, CVIR can be summarized as the following two step iterative procedure: (i) Fix ft, optimize the loss to obtain wt`1; and (ii) Fix wt`1 and optimize the loss to obtain ft`1. By construction of CVIR, we select wt`1 such that we discard points with highest loss, and hence Lpft, wt`1q ď Lpft, wtq. Fixing wt`1, we minimize the Lpft, wt`1q to obtain ft`1 and hence Lpft`1, wt`1q ď Lpft, wt`1q. Combining these two steps, we get Lpft`1, wt`1q ď Lpft, wtq.
F Experimental Details
Below we present dataset details. We present experiments with MNIST Overlap in App. G.8.

Dataset Simulated PU Dataset P vs N

#Positives #Unlabeled Train Val Train Val

CIFAR10 MNIST IMDb

Binarized CIFAR CIFAR Dog vs Cat
Binarized MNIST MNIST 17
MNIST Overlap
IMDb

[0-4] vs [5-9] 12500 12500 2500 2500

3 vs 5

2500 2500 500 500

[0-4] vs [5-9] 15000 15000 2500 2500

1 vs 7

3000 3000 500 500

[0-7] vs [3-9] 150000 15000 2500 2500

pos vs neg 6250 6250 5000 5000

For CIFAR dataset, we also use the standard data augementation of random crop and horizontal ﬂip. PyTorch code is as follows:
(transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip())
F.1 Architecture and Implementation Details
All experiments were run on NVIDIA GeForce RTX 2080 Ti GPUs. We used PyTorch [33] and Keras with Tensorﬂow [1] backend for experiments.
For CIFAR10, we experiment with convolutional nets and MLP. For MNIST, we train MLP. In particular, we use ResNet18 [19] and all convolution net [40] . Implementation adapted from: https: //github.com/kuangliu/pytorch-cifar.git. We consider a 4-layered MLP. The PyTorch code for 4-layer MLP is as follows:
nn.Sequential(nn.Flatten(), nn.Linear(input_dim, 5000, bias=True), nn.ReLU(), nn.Linear(5000, 5000, bias=True), nn.ReLU(), nn.Linear(5000, 50, bias=True), nn.ReLU(), nn.Linear(50, 2, bias=True) )
For all architectures above, we use Xaviers initialization [18]. For all methods except nnPU and uPU, we do cross entropy loss minimization with SGD optimizer with momentum 0.9. For convolution architectures we use a learning rate of 0.1 and MLP architectures we use a learning rate of 0.05. For nnPU and uPU, we minimize sigmoid loss with ADAM optimizer with learning rate 0.0001 as advised in its original paper. For all methods, we ﬁx the weight decay param at 0.0005.
For IMDb dataset, we ﬁne-tune an off-the-shelf uncased BERT model [10]. Code adapted from Hugging Face Transformers [42]: https://huggingface.co/transformers/v3.1.0/custom_ datasets.html. For all methods except nnPU and uPU, we do cross entropy loss minimization
20

with Adam optimizer with learning rate 0.00005 (default params). With the same hyperparameters and Sigmoid loss, we could not train BERT with nnPU and uPU due to vanishing gradients. Instead we use learning rate 0.00001.
F.2 Division between training set and hold-out set
Since the training set is used to learn the classiﬁer (parameters of a deep neural network) and the hold-out set is just used to learn the mixture proportion estimate (scalar), we use a larger dataset for training. Throughout the experiments, we use an 80-20 split of the original set.
At a high level, we have an error bound on the mixture proportion estimate and w?e can use that to decide the split in general. As long as we use enough samples to make the Op1{ nq small in our bound in Theorem 1, we can use the rest of the samples to learn the classiﬁer.

G Additional Experiments
G.1 nnPU vs PN classiﬁcation
In this section, we compare the performance of nnPU and PvN training on the same positive and negative (from the unlabeled) data at α “ 0.5. We highlight the huge classiﬁcation performance gap between nnPU and PvN training and show that training with CVuO objective partially recovers the performance gap. Note, to train PvN classiﬁer, we use the same hyperparameters as that with PvU training.

Dataset

nnPU

CVuO

(TED)n

Model (known α) PvN (known α) (unknown α)

ResNet 76.8 86.9 82.6

82.7

Binarized All Conv 72.1 76.7 77.1

76.8

CIFAR

MLP

63.9 65.1 65.9

63.2

CIFAR Dog ResNet 72.6 80.4 74.0

76.1

vs Cat All Conv 68.4 77.9 71.0

72.2

Binarized MLP

95.9 96.7 96.4

95.9

MNIST

MNIST17 MLP

98.2 99.0 98.6

98.6

IMDb

BERT

86.2 89.1 87.4

88.1

Table 4: Accuracy for PvN classiﬁcation with nnPU, PvN, CVuO objective and (TED)n training. Results reported by aggregating aggregating over 10 epochs.

G.2 Under-Fitting due to pessimistic early stopping
Ivanov [20] explored the following heuristics for ad-hoc early stopping criteria: training proceeds until the loss on unseen PU data ceases to decrease. In particular, the authors suggested early stopping criterion based on the loss on unseen PU data doesn’t decrease in epochs separated by a pre-deﬁned window of length l. The early stopping is done when this happens consecutively for l epochs. However, this approach leads to severe under-ﬁtting. When we ﬁx l “ 5, we observe a signiﬁcant performance drop in CIFAR classiﬁcation and MPE.
With PvU training, the performance of ResNet model on Binarized CIFAR (in Table 2) drops from 78.3 (orcale stopping) to 60.4 (with early stopping). Similar on CIFAR CAT vs Dog, the performance of the same architecture drops from 71.6 (orcale stopping) to 58.4 (with early stopping). Note that the decrease in accuracy is less or not signiﬁcant for MNIST. With PvU training, the performance of MLP model on Binarized MNIST (in Table 2) drops from 94.5 (orcale stopping) to 94.1 (with early stopping). This is because we obtain good performance on MNIST early in training.
21

G.3 Results parallel to Fig. 3

Epoch wise results for all models for Binarized CIFAR, CIFAR Dog vs Cat, Binarized MNIST, MNIST 17 and IMDb.

Accuracy

85 80 75 70 65 60 55 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

Mixture Proportion

1.0

0.8

0.6

0.4
(TED)n DEDPUL
0.2 EN Alphamax ( * )
0.0 0 500 True MPE

Ep10o0c0hs 1500

2000

Figure 5: Epoch wise results with ResNet-18 network trained on CIFAR-binarized.

Accuracy

80 75 70 65 60 55 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

Mixture Proportion

1.0

0.8

0.6

0.4
(TED)n DEDPUL
0.2 EN Alphamax ( * )
0.0 0 500 True MPE

Ep10o0c0hs 1500

2000

Figure 6: Epoch wise results with All convolutional network trained on CIFAR-binarized.

Accuracy

66 64 62 60 58 56 54 52 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
250 Ep5o0c0hs 750 1000

Mixture Proportion

1.0

0.8

0.6

0.4

0.2

(TED)n DEDPUL

EN

0.0

0

True MPE
250

500

750 1000

Epochs

Figure 7: Epoch wise results with FCN trained on CIFAR-binarized.

22

Accuracy

75 70 65 60 55 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

Mixture Proportion

1.0

0.8

0.6

0.4

0.2

(TED)n DEDPUL

EN

0.0

0

True MPE
500

1000 1500 2000

Epochs

Figure 8: Epoch wise results with ResNet-18 trained on CIFAR Dog vs Cat.

Accuracy

75 70 65 60 55 50
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

Mixture Proportion

1.0

0.8

0.6

0.4

0.2

(TED)n DEDPUL

EN

0.0

0

True MPE
500

1000 1500 2000

Epochs

Figure 9: Epoch wise results with All convolutional network trained on CIFAR Dog vs Cat.

Accuracy

96 94 92 90 88 86 84
0

(TED)n Dedpul nnPU ( * ) uPU ( * ) PvU
500 Ep10o0c0hs 1500 2000

Mixture Proportion

1.0 0.8 0.6 0.4 0.2 0.0 0

(TED)n DEDPUL EN True MPE
500 Ep10o0c0hs 1500 2000

Figure 10: Epoch wise results with MLP trained on Binarized MNIST.

Accuracy

100

98

96

94

92

90

88 (TED)n

86

Dedpul nnPU ( * )

84

uPU ( * ) PvU

0 100 20E0poch3s00 400 500

Mixture Proportion

1.0 0.8 0.6 0.4 0.2 0.0 0

(TED)n DEDPUL EN True MPE
100 20E0poch3s00 400 500

Figure 11: Epoch wise results with MLP trained on MNIST 17.

23

Density

Density

Figure 12: Epoch wise results with BERT trained on IMDb. G.4 Overﬁtting on unlabeled data as PvU training proceeds

12 10 8 6 4 2 0 0.0 8 6 4 2 0 0.0

Epoch 5
pos neg

0.2 Ou0tp.4ut Prob0.6 Epoch 200

0.8 pos neg

0.2 O0u.4tput Pr0o.6b 0.8 1.0

Density

Density

233...505 Epoch 50 pneosg

2.0

1.5

1.0

0.5

0.0 0.0

0.2 Ou0t.p4ut Prob0.6

0.8

Epoch 400

8 6

pos neg

4

2

0 0.0 0.2 O0u.4tput Pr0o.6b 0.8 1.0

Density

Density

5

Epoch 100

4

pos neg

3

2

1

0 0.0 0.1 0.2 0O.3utp0u.4t Pro0.b5 0.6 0.7 0.8

Epoch 500

8

pos

6

neg

4

2

0 0.0 0.2 O0u.4tput Pr0o.6b 0.8 1.0

Figure 13: Score assigned by the classiﬁer to positive and negative points in the unlabeled training set as PvU training proceeds. As training proceeds, classiﬁer memorizes both positive and negative in unlabeled as negatives.

In Fig. 13, we show the distribution of unlabeled training points. We show that as positive versus unlabeled training proceeds with a ResNet-18 model on binarized CIFAR dataset, classiﬁer memorizes all the unlabeled data as negative assigning them very small scores (i.e., the probability of them being negative).

G.5 Ablations to (TED)n
Varying the number of warm start epochs We now vary the number of warm start epochs with (TED)n. We observe that increasing the number of warm start epochs doesn’t hurt (TED)n even when the classiﬁer at the end of the warm start training memorized PU training data due PvU training. While in many cases (TED)n training without warm start is able to recover the same performance, it fails to learn anything for CIFAR Dog vs Cat with all convolutional neural network. This highlights the need for warm start training with (TED)n.

Figure 14: Classiﬁcation and MPE results with varying warm start epochs W with (TED)n
Varying the true mixture proportion α Next, we vary α, the true mixture proportion and present results for MPE and classiﬁcation in Fig. 15. Overall, across all α, our method (TED)n is able to
24

achieve superior performance as compared to alternate algorithms. We omit high α for CIFAR and IMDb datasets as all the methods result in trivial accuracy and mixture proportion estimate.

Figure 15: MPE and Classiﬁcation results with varying mixture proportion. For each method we show results with the best performing architecture.

G.6 Classiﬁcation and MPE results with error bars

Dataset
Binarized CIFAR
CIFAR Dog vs Cat
Binarized MNIST MNIST17 IMDb

Model

(TED)n

BBE˚

DEDPUL˚

EN

KM2

TiCE

ResNet 0.026 ˘ 0.005 0.091 ˘ 0.027 0.091 ˘ 0.023 0.192 ˘ 0.007 All Conv 0.042 ˘ 0.003 0.037 ˘ 0.018 0.052 ˘ 0.017 0.221 ˘ 0.017 0.168 ˘ 0.207 0.194 ˘ 0.039
MLP 0.225 ˘ 0.013 0.177 ˘ 0.011 0.138 ˘ 0.009 0.372 ˘ 0.002

ResNet 0.078 ˘ 0.010 0.176 ˘ 0.015 All Conv 0.066 ˘ 0.015 0.128 ˘ 0.020

0.170 ˘ 0.010 0.226 ˘ 0.003 0.331 ˘ 0.238 0.286 ˘ 0.013 0.115 ˘ 0.014 0.250 ˘ 0.019

MLP 0.024 ˘ 0.001 0.032 ˘ 0.001 0.031 ˘ 0.003 0.080 ˘ 0.009 0.029 ˘ 0.008 0.056 ˘ 0.05

MLP 0.003 ˘ 0.000 0.023 ˘ 0.017 0.021 ˘ 0.011 0.028 ˘ 0.017 0.022 ˘ 0.003 0.043 ˘ 0.023

BERT 0.008 ˘ 0.001 0.011 ˘ 0.002 0.016 ˘ 0.005 0.07 ˘ 0.01

-

-

Table 5: Absolute estimation error when α is 0.5. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating absolute error over 10 epochs and 3 seeds.

Dataset

Model

(TED)n (unknown α)

CVIR (known α)

PvU˚

DEDPUL˚

nnPU

(known α) (unknown α) (known α)

uPU˚ (known α)

Binarized CIFAR
CIFAR Dog vs Cat
Binarized MNIST MNIST17
IMDb

ResNet 82.7 ˘ 0.13 82.3 ˘ 0.18 76.9 ˘ 1.12 All Conv 77.9 ˘ 0.29 78.1 ˘ 0.47 75.8 ˘ 0.75
MLP 64.2 ˘ 0.37 66.9 ˘ 0.28 61.6 ˘ 0.38
ResNet 75.2 ˘ 1.74 73.3 ˘ 0.94 67.3 ˘ 1.52 All Conv 73.0 ˘ 0.81 71.7 ˘ 0.47 70.5 ˘ 0.60
MLP 95.6 ˘ 0.42 96.3 ˘ 0.07 94.2 ˘ 0.58

MLP BERT

98.7 ˘ 0.25 98.7 ˘ 0.09 96.9 ˘ 1.51 87.6 ˘ 0.20 87.4 ˘ 0.25 86.1 ˘ 0.53

77.1 ˘ 1.52 77.1 ˘ 0.64 62.6 ˘ 0.30 67.0 ˘ 1.46 69.2 ˘ 0.86 94.8 ˘ 0.10
97.7 ˘ 0.62 87.3 ˘ 0.18

77.2 ˘ 1.03 76.7 ˘ 0.74 73.4 ˘ 1.31 72.5 ˘ 0.21 63.1 ˘ 0.79 64.0 ˘ 0.24 71.8 ˘ 0.33 68.8 ˘ 0.53 67.9 ˘ 0.52 67.5 ˘ 2.28 96.1 ˘ 0.14 95.2 ˘ 0.19
98.4 ˘ 0.20 98.4 ˘ 0.09 86.2 ˘ 0.25 85.9 ˘ 0.12

Table 6: Accuracy for PvN classiﬁcation with PU learning. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating over 10 epochs and 3 seeds.

25

G.7 Experiments on UCI dataset
In this section, we will present results on 5 UCI datasets.
Dataset #Positives #Unlabeled Train Val Train Val
concrete 162 162 81 81 mushroom 1304 1304 652 652
landsat 946 946 472 472 pageblock 185 185 92 92 spambase 604 604 302 302

We train a MLP with 2 hidden layers each with 512 units. The PyTorch code for 4-layer MLP is as follows:
nn.Sequential(nn.Flatten(), nn.Linear(input_dim, 512, bias=True), nn.ReLU(), nn.Linear(512, 512, bias=True), nn.ReLU(), nn.Linear(512, 2, bias=True), )
Similar to vision datasets and architectures, we do cross entropy loss minimization with SGD optimizer with momentum 0.9 and learning rate 0.1. For nnPU and uPU, we minimize sigmoid loss with ADAM optimizer with learning rate 0.0001 as advised in its original paper. For all methods, we ﬁx the weight decay param at 0.0005.

Dataset (TED)n BBE˚ DEDPUL˚ EN˚ KM2 TiCE

concrete mushroom
landsat pageblock spambase

0.071 0.001 0.022 0.007 0.006

0.152 0.015 0.021 0.066 0.047

0.176 0.014 0.012 0.041 0.077

0.239 0.013 0.080 0.135 0.127

0.099 0.038 0.037 0.008 0.062

0.268 0.069 0.027 0.298 0.276

Table 7: Absolute estimation error when α is 0.5. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating absolute error over 10 epochs.

Dataset

(TED)n

CVuO

PvU˚ DEDPUL˚ nnPU

uPU˚

(unknown α) (known α) (known α) (unknown α) (known α) (known α)

concrete mushroom
landsat pageblock spambase

86.3 96.4 93.8 95.7 89.4

80.1 96.3 93.1 95.7 88.1

83.1 98.7 93.4 95.1 89.2

83.7 98.7 92.4 94.5 86.8

83.2

84.4

97.5

93.9

92.9

92.3

93.9

93.9

88.5

87.7

Table 8: Accuracy for PvN classiﬁcation with PU learning. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating aggregating over 10 epochs.

On 4 out of 5 UCI datasets, our proposed methods are better than the best performing alternatives (Table 7 and Table 8).

26

G.8 Experiments on MNIST Overlap
Similar to binarized MNIST, we create a new dataset called MNIST Overlap, where the positive class contains digits from 0 to 7 and the negative class contains digits from 3 to 9. This creates a dataset with overlap between positive and negative support. Note that while the supports overlap, we sample images from the overlap classes with replacement, and hence, in absence of duplicates in the dataset, exact same images don’t appear both in positive and negative subsets.
We train MLP with the same hyperparameters as before. Our ﬁndings in Table 9 and Table 10 highlight superior performance of the proposed approaches in the cases of support overlap.

Dataset

(TED)n BBE˚ DEDPUL˚ EN˚ KM2 TiCE

MNIST Overlap 0.035 0.100 0.104 0.196 0.099 0.074

Table 9: Absolute estimation error when α is 0.5. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating absolute error over 10 epochs.

Dataset

(TED)n

CVuO

PvU˚ DEDPUL˚ nnPU

uPU˚

(unknown α) (known α) (known α) (unknown α) (known α) (known α)

MNIST Overlap 79.0

78.4

77.4

77.5

78.6

78.8

Table 10: Accuracy for PvN classiﬁcation with PU learning. "*" denote oracle early stopping as deﬁned in Sec. 6. Results reported by aggregating aggregating over 10 epochs.

27

