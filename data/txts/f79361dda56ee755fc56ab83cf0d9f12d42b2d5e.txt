Simple, Robust and Optimal Ranking from Pairwise Comparisons

Nihar B. Shah† nihar@eecs.berkeley.edu

Martin J. Wainwright†,∗ wainwrig@berkeley.edu

Department of EECS†, and Department of Statistics∗ University of California, Berkeley Berkeley, CA 94720

arXiv:1512.08949v2 [cs.LG] 27 Apr 2016

Abstract
We consider data in the form of pairwise comparisons of n items, with the goal of precisely identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) its computational eﬃciency leads to speed-ups of several orders of magnitude in computation time as compared to prior work; (b) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) it is an optimal method up to constant factors, meaning that it achieves the information-theoretic limits for recovering the top k-subset. We extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisﬁes a simple and natural monotonicity condition.
1 Introduction
Ranking problems involve a collection of n items, and some unknown underlying total ordering of these items. In many applications, one may observe (noisy) comparisons between various pairs of items. Examples include matches between football teams in tournament play; consumer’s preference ratings in marketing; and certain types of voting systems in politics. Given a set of such noisy comparisons between items, it is often of interest to ﬁnd the true underlying ordering of all n items, or alternatively, given some given positive integer k < n, to ﬁnd the subset of k most highly rated items. These two problems are the focus of this paper.
There is a substantial literature on the problem of ﬁnding approximate rankings based on noisy pairwise comparisons. A number of papers (e.g., [KMS07, BM08, Eri13]) consider models in which the probability of a pairwise comparison agreeing with the underlying order is identical across all pairs. These results break down when for one or more pairs, the probability of agreeing with the underlying ranking is either comes close to or is exactly equal to 21 . Another set of papers [Hun04, NOS12, HOX14, SPX14, SBB+16] work using parametric models of pairwise comparisons, and address the problem of recovering the parameters associated to every individual item. A more recent line of work [Cha14, SBGW16, SBW16] studies a more general class of models based on the notion of strong stochastic transitivity (SST), and derives conditions on recovering the pairwise comparison probabilities themselves. However, it remains unclear whether or not these results can directly extend to tight bounds
1

for the problem of recovery of the top k items. The works [JS08, MGCV11, AS12, DIS15] consider mixture models, in which every pairwise comparison is associated to a certain individual making the comparison, and it is assumed that the preferences across individuals can be described by a low-dimensional model.
Most related to our work are the papers [WJJ13, RA14, RGLA15, CS15], which we discuss in more detail here. Wauthier et al. [WJJ13] analyze a weighted counting algorithm to recover approximate rankings; their analysis applies to a speciﬁc model in which the pairwise comparison between any pair of items remains faithful to their relative positions in the true ranking with a probability common across all pairs. They consider recovery of an approximate ranking (under Kendall’s tau and maximum displacement metrics), but do not provide results on exact recovery. As the analysis of this paper shows, their bounds are quite loose: their results are tight only when there are a total of at least Θ(n2) comparisons. The pair of papers [RA14, RGLA15] by Rajkumar et al. consider ranking under several models and several metrics. In the part that is common with our setting, they show that the counting algorithm is consistent in terms of recovering the full ranking, which automatically implies consistency in exactly recovering the top k items. They obtain upper bounds on the sample complexity in terms of a separation threshold that is identical to a parameter ∆k deﬁned subsequently in this paper (see Section 3). However, as our analysis shows, their bounds are loose by at least an order of magnitude. They also assume a certain high-SNR condition on the probabilities, an assumption that is not imposed in our analysis.
Finally, in very recent work on this problem, Chen and Suh [CS15] proposed an algorithm called the Spectral MLE for exact recovery of the top k items. They showed that, if the pairwise observations are assumed to drawn according to the Bradley-Terry-Luce (BTL) parametric model [BT52, Luc59], the Spectral MLE algorithm recovers the k items correctly with high probability under certain regularity conditions. In addition, they also show, via matching lower bounds, that their regularity conditions are tight up to constant factors. While these guarantees are attractive, it is natural to ask how such an algorithm behaves when the data is not drawn from the BTL model. In real-world instances of pairwise ranking data, it is often found that parametric models, such as the BTL model and its variants, fail to provide accurate ﬁts (for instance, see the papers [DM59, ML65, Tve72, BW97] and references therein).
With this context, the main contribution of this paper is to analyze a classical countingbased method for ranking, often called the Copeland method [Cop51], and to show that it is simple, optimal and robust. Our analysis does not require that the data-generating mechanism follow either the BTL or other parametric assumptions, nor other regularity conditions such as stochastic transitivity. We show that the Copeland counting algorithm has the following properties:
• Simplicity: The algorithm is simple, as it just orders the items by the number of pairwise comparisons won. As we will subsequently see, the execution time of this counting algorithm is several orders of magnitude lower as compared to prior work.
• Optimality: We derive conditions under which the counting algorithm achieves the stated goals, and by means of matching information-theoretic lower bounds, show that these conditions are tight.
• Robustness: The guarantees that we prove do not require any assumptions on the pairwisecomparison probabilities, and the counting algorithm performs well for various classes of
2

data sets. In contrast, we ﬁnd that the spectral MLE algorithm performs poorly when the data is not drawn from the BTL model.
In doing so, we consider three diﬀerent instantiations of the problem of set-based recovery: (i) Recovering the top k items perfectly; (ii) Recovering the top k items allowing for a certain Hamming error tolerance; and (iii) a more general recovery problem for set families that satisfy a natural “set-monotonicity” condition. In order to tackle this third problem, we introduce a general framework that allows us to treat a variety of problems in the literature in an uniﬁed manner.
The remainder of this paper is organized as follows. We begin in Section 2 with background and a more precise formulation of the problem. Section 3 presents our main theoretical results on top-k recovery under various requirements. Section 4 provides the results of experiments on both simulated and real-world data sets. We provide all proofs in Section 5. The paper concludes with a discussion in Section 6.
2 Background and problem formulation
In this section, we provide a more formal statement of the problem along with background on various types of ranking models.
2.1 Problem statement
Given an integer n ≥ 2, we consider a collection of n items, indexed by the set [n] : = {1, . . . , n}. For each pair i = j, we let Mij denote the probability that item i wins the comparison with item j. We assume that that each comparison necessarily results in one winner, meaning that
1 Mij + Mji = 1, and Mii = 2 , (1)
where we set the diagonal for concreteness. For any item i ∈ [n], we deﬁne an associated score τi as
1n τi : = n Mij. (2)
j=1
In words, the score τi of any item i ∈ [n] corresponds to the probability that item i beats an item chosen uniformly at random from all n items.
Given a set of noisy pairwise comparisons, our goals are (a) to recover the k items with the maximum values of their scores; and (b) to recover the full ordering of all the items as deﬁned by the score vector. The notion of ranking items via their scores (2) generalizes the explicit rankings under popular models in the literature. Indeed, as we discuss shortly, most models of pairwise comparisons considered in the literature either implicitly or explicitly assume that the items are ranked according to their scores. Note that neither the scores {τi}i∈[n] nor the matrix M : = {Mij}i,j∈[n] of probabilities are assumed to be known.
More concretely, we consider a random-design observation model deﬁned as follows. Each pair is associated with a random number of noisy comparisons, following a binomial distribution with parameters (r, p), where r ≥ 1 is the number of trials and p ∈ (0, 1] is the probability of making a comparison on any given trial. Thus, each pair (i, j) is associated with a binomial random variable with parameters (r, p) that governs the number of comparisons between the
3

pair of items. We assume that the observation sequences for diﬀerent pairs are independent.
Note that in the special case p = 1, this random binomial model reduces to the case in which
we observe exactly r observations of each pair; in the special case r = 1, the set of pairs
compared form an (n, p) Erd˝os-R´enyi random graph.
In this paper, we begin in Section 3.1 by analyzing the problem of exact recovery. More precisely, for a given matrix M of pairwise probabilities, suppose that we let Sk∗ denote the (unknown) set of k items with the largest values of their respective scores, assumed to be
unique for concreteness.
Given noisy observations speciﬁed by the pairwise probabilities M , our goal is to establish
conditions under which there exists some algorithm Sk that identiﬁes k items based on the outcomes of various comparisons such that the probability PM (Sk = Sk∗) is very close to one. In the case of recovering the full ranking, our goal is to identify conditions that ensure that the probability PM ∩ (Sk = Sk∗) is close to one.
k∈[n]
In Section 3.2, we consider the problem of recovering a set of k items that approximates Sk∗ with a minimal Hamming error For any two subsets of [n], we deﬁne their Hamming distance
DH, also referred to as their Hamming error, to be the number of items that belong to exactly one of the two sets—that is

DH(A, B) = card {A ∪ B}\{A ∩ B} .

(3)

For a given user-deﬁned tolerance parameter h ≥ 0, we derive conditions that ensure that DH(Sk, Sk∗) ≤ 2h with high probability.
Finally, we generalize our results to the problem of satisfying any a general class of requirements on set families. These requirement are speciﬁed in terms of which k-sized subsets of the items are allowed, and is required to satisfy only one natural condition, that of setmonotonicity, meaning that replacing an item in an allowed set with a higher rank item should also be allowed. See Section 3.3 for more details on this general framework.

2.2 A range of pairwise comparison models
To be clear, our work makes no assumptions on the form of the pairwise comparison probabilities. However, so as to put our work in context of the literature, let us brieﬂy review some standard models uesd for pairwise comparison data.

Parametric models: A broad class of parametric models, including the Bradley-TerryLuce (BTL) model as a special case [BT52, Luc59], are based on assuming the existence of “quality” parameter wi ∈ R for each item i, and requiring that the probability of an item beating another is a speciﬁc function of the diﬀerence between their values. In the BTL model, the probability Mij that i beats j is given by the logistic model

1 Mij = 1 + e−(wi−wj) .

(4a)

More generally, parametric models assume that the pairwise comparison probabilities take the form

Mij = F (wi − wj),

(4b)

where F : R → [0, 1] is some strictly increasing cumulative distribution function.

4

By construction, any parametric model has the following property: if wi > wj for some pair of items (i, j), then we are also guaranteed that Mi > Mj for every item . As a consequence, we are guaranteed that τi > τj, which implies that ordering of the items in terms of their quality vector w ∈ Rn is identical to their ordering in terms of the score vector τ ∈ Rn. Consequently, if the data is actually drawn from a parametric model, then recovering
the top k items according to their scores is the same as recovering the top k items according
their respective quality parameters.

Strong Stochastic Transitivity (SST) class: The class of strong stochastic transitivity (SST) models is a superset of parametric models [SBGW16]. It does not assume the existence of a quality vector, nor does it assume any speciﬁc form of the probabilities as in equation (4a). Instead, the SST class is deﬁned by assuming the existence of a total ordering of the n items, and imposing the inequality constraints Mi ≥ Mj for every pair of items (i, j) where i is ranked above j in the ordering, and every item . One can verify that an ordering by the scores {τi}i∈[n] of the items lead to an ordering of the items that is consistent with that deﬁned by the SST class.

Thus, we see that in a broad class of models for pairwise ranking, the total ordering deﬁned by the score vectors (2) coincides with the underlying ordering used to deﬁne the models. In this paper, we analyze the performance of a counting algorithm, without imposing any modeling conditions on the family of pairwise probabilities. The next three sections establish theoretical guarantees on the recovery of the top k items under various requirements.

2.3 Copeland counting algorithm

The analysis of this paper focuses on a simple counting-based algorithm, often called the Copeland method [Cop51]. It can be also be viewed as a special case of the Borda count method [dB81], which applies more generally to observations that consist of rankings of two or more items. Here we describe how this method applies to the random-design observation model introduced earlier.
More precisely, for each distinct i, j ∈ [n] and every integer ∈ [r], let Yij ∈ {−1, 0, +1} represent the outcome of the th comparison between the pair i and j, deﬁned as

 0 no comparison between (i.j) in trial 

Yij = +1 if comparison is made and item i beats j

(5)

 −1 if comparison is made and item j beats i.

Note that this deﬁnition ensures that Yij = −Yji. For i ∈ [n], the quantity

Ni : =

1{Yij = 1}

(6)

j∈[n] ∈[r]

corresponds to the number of pairwise comparisons won by item i. Here we use 1{·} to denote
the indicator function that takes the value 1 if its argument is true, and the value 0 otherwise. For each integer k, the vector {Ni}ni=1 of number of pairwise wins deﬁnes a k-sized subset

Sk = i ∈ [n] | Ni is among the k highest number of pairwise wins ,

(7)

5

corresponding to the set of k items with the largest values of Ni. Otherwise stated, the set Sk corresponds to the rank statistics of the top k-items in the pairwise win ordering. (If there are any ties, we resolve them by choosing the indices with the smallest value of i.)

3 Main results
In this section, we present our main theoretical results on top-k recovery under the three settings described earlier. Note that the three settings are ordered in terms of increasing generality, with the advantage that the least general setting leads to the simplest form of theoretical claim.

3.1 Thresholds for exact recovery of the top k items
We begin with the goal of exactly recovering the k top-ranked items. As one might expect, the diﬃculty of this problem turns out to depend on the degree of separation between the top k items and the remaining items. More precisely, let us use (k) and (k + 1) to denote the indices of the items that are ranked kth and (k + 1)th respectively. With this notation, the k-separation threshold ∆k is given by

1n

1n

∆k : = τ(k) − τ(k+1) = n M(k)i − n M(k+1)i.

(8)

i=1

i=1

In words, the quantity ∆k is the diﬀerence in the probability of item (k) beating another item chosen uniformly at random, versus the same probability for item (k + 1).
As shown by the following theorem, success or failure in recovering the top k entries is determined by the size of ∆k relative to the number of items n, observation probability p and number of repetitions r. In particular, consider the family of matrices

Fk(α; n, p, r) : = M ∈ [0, 1]n×n | M + M T = 11T , and ∆k ≥ α log n .

(9)

npr

To simplify notation, we often adopt Fk(α) as a convenient shorthand for this set, where its dependence on (n, p, r) should be understood implicitly.
With this notation, the achievable result in part (a) of the following theorem is based on the estimator that returns the set Sk of the the k items deﬁned by the number of pairwise comparisons won, as deﬁned in equation (7). On the other hand, the lower bound in part (b) applies to any estimator, meaning any measurable function of the observations.

Theorem 1. (a) For any α ≥ 8, the maximum pairwise win estimator Sk from equation (7) satisﬁes

sup PM Sk = Sk∗ ≤ n114 .
M ∈Fk(α)

(10a)

(b) Conversely, suppose that n ≥ 7 and p ≥ l2ongrn . Then for any α ≤ 71 , the error probability of any estimator Sk is lower bounded as

sup PM Sk = Sk∗ ≥ 17 .
M ∈Fk(α)

(10b)

6

Remarks: First, it is important to note that the negative result in part (b) holds even if the supremum is further restricted to a particular parametric sub-class of Fk(α), such as the pairwise comparison matrices generated by the BTL model, or by the SST model. Our proof of the lower bound for exact recovery is based on a generalization of a construction introduced by Chen and Suh [CS15], one adapted to the general deﬁnition (8) of the separation threshold ∆k .
Second, we note that in the regime p < log n , standard results from random graph the2nr √
ory [ER60] can be used to show that there are at least n items (in expectation) that are never compared to any other item. Of course, estimating the rank is impossible in this pathological case, so we omit it from consideration.
Third, the two parts of the theorem in conjunction show that the counting algorithm is essentially optimal. The only room for improvement is in the diﬀerence between the value 8 of α in the achievable result, and the value 71 in the lower bound.

Theorem 1 can also be used to derive guarantees for recovery of other functions of the underlying ranking. Here we consider the problem of identifying the ranking of all n items, say denoted by the permutation π∗. In this case, we require that each of the separations {∆j}nj=−11 are suitably lower bounded: more precisely, we study models M that belong to the intersection ∩nj=−11Fj(γ).
Corollary 1. Let π be the permutation of the items speciﬁed by the number of pairwise comparisons won. Then for any α ≥ 8, we have

sup PM π = π∗
M ∈∩nj=−11Fj (α)

1 ≤ n13 .

Moreover, the separation condition on {∆j}jn=−11 that deﬁnes the set ∩jn=−11Fj(α) is unimprovable beyond constant factors.

This corollary follows from the equivalence between correct recovery of the ranking and recovering the top k items for every value of k ∈ [n].

Detailed comparison to related work: In the remainder of this subsection, we make

a detailed comparison to the related works [WJJ13, RA14, RGLA15, CS15] that we brieﬂy

discussed earlier in Section 1.

Wauthier et al. [WJJ13] analyze a weighted counting algorithm for approximate recovery

of

rankings;

they

work

under

a

model

in

which

Mij

=

1 2

+

γ

whenever

item

i

is

ranked

above

item j in an assumed underlying ordering. Here the parameter γ ∈ (0, 12 ] is independent of

(i, j), and as a consequence, the best ranked item is assumed to be as likely to meet the worst

item as it is to beat the second ranked item, for instance. They analyze approximate ranking

under Kendall tau and maximum displacement metrics. In order to have a displacement upper bounded by by some δ > 0, their bounds require the order of δn2γ52 pairwise comparisons.
In comparison, our model is more general in that we do not impose the γ-condition on the

pairwise probabiltiies. When specialized to the γ-model, the quantities {∆j}nj=1 in our analysis takes the form ∆j = 2nγ , and Corollary 1 shows that minnjl∈o[gnn] ∆2j = n3γlo2g n observations are
suﬃcient to recover the exact total ordering. Thus, for any constant δ, Corollary 1 guarantees recover with a multiplicative factor of order long2n smaller than that established by Wauthier et al. [WJJ13].

7

The pair of papers [RA14, RGLA15] by Rajkumar et al. consider ranking under several

models and several metrics. For the subset of their models common with our setting—namely,

Bradley-Terry-Luce and the so-called low noise models—they show that the counting algo-

rithm is consistent in terms of recovering the full ranking or the top subset of items. The

guarantees are obtained under a low-noise assumpotion: namely, that the probability of any

item i beating j is at least 21 + γ whenever item i is ranked higher than item j in an assumed underlying ordering. Their guarantees are based on a sample size of at least γlo2gµn2 , where µ

is a parameter lower bounded as µ ≥

1 n2

.

Once again, our setting allows for the parameter

γ to be arbitrarily close to zero, and furthermore as one can see from the discussion above,

our bounds are much stronger. Moreover, while Rajkumar et al. focus on upper bounds

alone, we also prove matching lower bounds on sample complexity showing that our results

are unimprovable beyond constant factors. It should be noted that Rajkumar et al. also

provide results for other types of ranking problems that lie outside the class of models treated

in the current paper.

Most recently, Chen and Suh [CS15] show that if the pairwise observations are assumed to

drawn according to the Bradley-Terry-Luce (BTL) parametric model (4a), then their proposed

Spectral MLE algorithm recovers the k items correctly with high probability when a certain

separation condition on the parameters {wi}ni=1 of the BTL model is satisﬁed. In addition, they also show, via matching lower bounds, that this separation condition are tight up to

constant factors. In real-world instances of pairwise ranking data, it is often found that

parametric models, such as the BTL model and its variants, fail to provide accurate ﬁts [DM59,

ML65, Tve72, BW97]. Our results make no such assumptions on the noise, and furthermore,

our notion of the ordering of the items in terms of their scores (2) strictly generalizes the

notion of the ordering with respect to the BTL parameters. In empirical evaluations presented

subsequently, we see that the counting algorithm is signiﬁcantly more robust to various kinds

of noise, and takes several orders of magnitude lesser time to compute.

Finally, in addition to the notion of exact recovery considered so far, in the next two

subsections we also derive tight guarantees for the Hamming error metric and more general

metrics inspired by the requirements of many relevant applications [IBS08, MTW05, BO03,

MAEA05, KS06, FLN03].

3.2 Approximate recovery under Hamming error

In the previous section, we analyzed performance in terms of exactly recovering the k-ranked subset. Although exact recovery is suitable for some applications (e.g., a setting with high stakes, in which any single error has a large price), there are other settings in which it may be acceptable to return a subset that is “close” to the correct k-ranked subset. In this section, we analyze this problem of approximate recovery when closeness is measured under the Hamming error. More precisely, for a given threshold h ∈ [0, k), suppose that our goal is to output a set k-sized set Sk such that its Hamming distance to the set Sk∗ of the true top k items, as deﬁned in equation (3), is bounded as

DH(Sk, Sk∗) ≤ 2h.

(11)

Our goal is to establish conditions under which it is possible (or impossible) to return an estimate Sk satisfying the bound (11) with high probability.1
1The requirement h < k is sensible because if h ≥ k, the problem is trivial: any two k-sized sets Sk and Sk∗ satisfy the bound DH(Sk, Sk∗) ≤ 2k ≤ 2h.

8

As before, we use (1), . . . , (n) to denote the permutation of the n items in decreasing order of their scores. With this notation, the following quantity plays a central role in our analysis:

∆k, h : = τ(k−h) − τ(k+h+1).

(12a)

Observe that ∆k, h is a generalization of the quantity ∆k deﬁned previously in equation (8); more precisely, the quantity ∆k corresponds to ∆k, h with h = 0. We then deﬁne a generalization of the family Fk(α; n, p, r), namely

Fk,h(α; n, p, r) : = M ∈ [0, 1]n×n | M + M T = 11T , and ∆k, h ≥ α log n . npr

(12b)

As before, we frequently adopt the shorthand Fk,h(α), with the dependence on (n, p, r) being understood implicitly.

Theorem 2. (a) For any α ≥ 8, the maximum pairwise win set Sk satisﬁes

sup PM DH(Sk, Sk∗) > 2h ≤ n114 .
M ∈Fk,h(α)

(13a)

(b)

Conversely,

in

the

regime

p≥

log n 2nr

and

for

given

constants
√

ν1, ν2

∈ (0, 1),

suppose

that

2h ≤ 1+1ν2 min{n1−ν1, k, n − k}. Then for any α ≤ ν114ν2 , any estimator Sk has error at

least

sup PM DH(Sk, Sk∗) > 2h ≥ 71 ,
M ∈Fk,h(α)

(13b)

for all n larger than a constant c(ν1, ν2).

This result is similar to that of Theorem 1, except that the relaxation of the exact recovery condition allows for a less constrained deﬁnition of the separation threshold ∆k, h. As with Theorem 1, the lower bound in part (b) applies even if probability matrix M is restricted to lie in a parametric model (such as the BTL model), or the more general SST class. The counting algorithm is thus optimal for estimation under the relaxed Hamming metric as well.
Finally, it is worth making a few comments about the constants appearing in these claims. We can weaken the lower bound on ∆k required in Theorem 2(a) at the expense of a lower probability of success; for instance, if we instead require that α ≥ 4, then the probability of error is guaranteed to be at most n−2. Subsequently in the paper, we provide the results of simulations with n = 500 items and α√= 4. On the other hand, in Theorem 2(b), if we impose the stronger upper bound α = O(1/ h log n), then we can remove the condition h ≤ n1−ν1.

3.3 An abstract form of k-set recovery
In earlier sections, we investigated recovery of the top k items either exactly or under a Hamming error. Exact recovery may be quite strict for certain applications, whereas the property of Hamming error allowing for a few of the top k items to be replaced by arbitrary items may be undesirable. Indeed, many applications have requirements that go beyond these metrics; for instance, see the papers [IBS08, MTW05, BO03, MAEA05, KS06, FLN03] and

9

references therein for some examples. In this section, we generalize the notion of exact or Hamming-error recovery in order to accommodate a fairly general class of requirements.
Both the exact and approximate Hamming recovery settings require the estimator to output a set of k items that are either exactly or approximately equal the true set of top k items. When is the estimate deemed successful? One way to think about the problem is as follows. The speciﬁed requirement of exact or approximate Hamming recovery is associated to a set of k-sized subsets of the n possible ranks. The estimator is deemed successful if the true ranks of the chosen k items equals one of these subsets. In our notion of generalized recovery, we refer to such sets as allowed sets. For example, in the case k = 3, we might say that the set {1, 4, 10} is allowed, meaning that an output consisting of the “ﬁrst”, “fourth” and “tenth” ranked items is considered correct.
In more generality, let S denote a family of k-sized subsets of [n], which we refer to as family of allowed sets. Notice that any allowed set is deﬁned by the positions of the items in the true ordering and not the items themselves.2 Once some true underlying ordering of the n items is ﬁxed, each element of the family S then speciﬁes a set of the items themselves. We use these two interpretations depending on the context — the deﬁnition in terms of positions to specify the requirements, and the deﬁnition in terms of the items to evaluate an estimator for a given underlying probability matrix M .
We let Sk† denote a k-set estimate, meaning a function that given a set of observations as input, returns a k-sized subset of [n] as output.
Deﬁnition 1 (S-respecting estimators). For any family S of allowed sets, a k-set estimate Sk† respects its structure if the set of k positions of the items in Sk† belongs to the set family S.
Our goal is to determine conditions on the set family S under which there exist estimators Sk† that respect its structure. In order to illustrate this deﬁnition, let us return to the examples treated thus far:
Example 1 (Exact and approximate Hamming recovery). The requirement of exact recovery of the top k items has S consisting of exactly one set, the set of the top k positions S = {[k]}. In the case of recovery with a Hamming error at most 2h, the set S of all allowed sets consists all k-sized subsets of [n] that contain at least (k − h) positions in the top k positions. For instance, in the case h = 1, k = 2 and n = 4, we have

S = {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4} .

Apart from these two requirements, there are several other requirements for top-k recovery popular in the literature [CCF+01, FLN03, BO03, MTW05, MAEA05, KS06, IBS08]. Let us
illustrate them with another example:

Example 2. Let π∗ : [n] → [n] denote the true underlying ordering of the n items. The following are four popular requirements on the set Sk† for top-k identiﬁcation, with respect to the true permutation π∗, for a pre-speciﬁed parameter ≥ 0.

(i) All items in the set Sk† must be contained contained within the top (1 + )k entries:

max π∗(i) ≤ (1 + )k.
i∈Sk†

(14a)

2In case of two or more items with identical scores, the choice of any of these items is considered valid.

10

(ii) The rank of any item in the set Sk† must lie within a multiplicative factor (1 + ) of the rank of any item not in the set Sk†:

max π∗(i) ≤ (1 + ) min π(j).

i∈Sk†

j∈[n]\Sk†

(14b)

(iii) The rank of any item in the set Sk† must lie within an additive factor any item not in the set Sk†:

max π∗(i) ≤ min π∗(j) + .

i∈Sk†

j∈[n]\Sk†

of the rank of (14c)

(iv) The sum of the ranks of the items in the set Sk† must be contained within a factor (1 + ) of the sums of ranks of the top k entries:

π∗(i) ≤ (1 +

1 ) k(k + 1).

2

i∈Sk†

(14d)

Note that each of these requirements reduces to the exact recovery requirement when = 0. Moreover, each of these requirements can be rephrased in terms of families of allowed sets. For instance, if we focus on requirement (i), then any k-sized subset of the top (1 + )k positions is an allowable set.
In this paper, we derive conditions that govern k-set recovery for allowable set systems that satisfy a natural “monotonicity” condition. Informally, the monotonicity condition requires that the set of k items resulting from replacing an item in an allowed set with a higher ranked item must also be an allowed set. More precisely, for any set {t1, . . . , tk} ⊆ [n], let Λ({t1, . . . , tk}) ⊆ 2[n] be the set deﬁned by all of its monotone transformations—that is

Λ({t1, . . . , tk}) : = {t1, . . . , tk} ⊆ [n] | tj ≤ tj for every j ∈ [k] .

Using this notation, we have the following:
Deﬁnition 2 (Monotonic set systems). The set S of allowed sets is a monotonic set system if

Λ(T ) ⊆ S for every T ∈ S.

(15)

One can verify that condition (15) is satisﬁed by the settings of exact and Hamming-error

recovery, as discussed in Example 1. The condition is also satisﬁed by all four requirements

discussed in Example 2.

The following theorem establishes conditions under which one can (or cannot) produce

an estimator that respects an allowable set requirement. In order to state it, recall the score

τi

:=

1 n

n j=1

Mij

,

as

previously

deﬁned

in

equation

(2)

for

each

i

∈

[n].

For notational

convenience, we also deﬁne τi : = −∞ for every i > n. Consider any monotonic family of

allowed sets S, and for some integer β ≥ 1, let T 1, . . . , T β ∈ S such that S = ∪ Λ(T b). For

b∈[β]

every b ∈ [β], let tb1 < · · · < tbk denote the entries of T b. We then deﬁne the critical threshold based on the scores:

∆S : = max min(τ(j) − τ(k+tb−j+1)).

b∈[β] j∈[k]

j

(16)

11

The term ∆S is a further generalization of the quantities ∆k and ∆k, h deﬁned in earlier sections. We also deﬁne a generalization FS(·) of the families Fk(·) and Fk(·) as
FS(α; n, p, r) : = M ∈ [0, 1]n×n | M + M T = 11T and ∆S ≥ α log n . (17) npr
As before, we use the shorthand FS(α), with the dependence on (n, p, r) being understood implicitly. Theorem 3. Consider any allowable set requirement speciﬁed by a monotonic set class S.
(a) For any α ≥ 8, the maximum pairwise win set Sk satisﬁes 1
M∈sFuSp(α) PM Sk ∈/ S ≤ n13 .
(b) Conversely, in the regime p ≥ l2ongrn , and for given constants µ1 ∈ (0, 1), µ2 ∈ ( 43 , 1], suppose that maxb∈[β] tbµ2k ≤ n2 and 8(1 − µ2)k ≤ n1−µ1. Then for any α smaller than a constant cu(µ1, µ2) > 0, any estimator Sk has error at least 1 M∈sFuSp(α) PM Sk ∈/ S ≥ 15 , (18)
for all n larger than a constant c0(µ1, µ2). A few remarks on the lower bound are in order. First, the lower bound continues to hold even if the probability matrix M is restricted to follow a parametric model such as BTL or restricted to lie in the SST class. Second, in terms of the threshold for α, the lower bound holds with cu(µ1, µ2) = 115 µ1 min 4(1−µ12)−1 , 12 . Third, it is worth noting that one must necessarily impose some conditions for the lower bound, along the lines of those required in Theorem 3(b) for the allowable sets to be “interesting” enough. As a concrete illustra√tion, consider the requirement deﬁned by the parameters b = 1, k = 1 and S = Λ({n − n}). For µ1 = µ2 = 190 , this requirement satisﬁes the condition 8(1 − µ2)k ≤ n1−µ1 but violates the condition t µ2k ≤ n2 . Now, a selection of k = 1 item made uniformly at random (independent of the data) satisﬁes this allowable set requirement with probability 1 − √1n . Given the success of such a random selection algorithm in this parameter regime, we see that the lower bounds therefore cannot be universal, but must require some conditions on the allowable sets.
4 Simulations and experiments
In this section, we empirically evaluate the performance of the counting algorithm and compare it with the Spectral MLE algorithm via simulations on synthetic data, as well as experiments using datasets from the Amazon Mechanical Turk crowdsourcing platform.
12

Fraction in error BTL Thurstone BTL+Outlier SST Mixture BTL < ∆k
Computation time (s) BTL Thurstone BTL+Outlier SST Mixture BTL < ∆k

Spectral MLE

Counting

1.0

103

0.8

102

101

0.6

100

0.4

10−1

10−2

0.2

10−3

0.0

10−4

(a)

(b)

Figure 1. Simulation results comparing Spectral MLE and the counting algorithm in terms of error rates for exact recovery of the top k items, and computation time. (a) Histogram of fraction of instances where the algorithm failed to recover the k items correctly, with each bar being the average value across 50 trials. The counting algorithm has 0% error across all problems, while the spectral MLE is accurate for parametric models (BTL, Thurstone), but increasingly inaccurate for other models. (b) Histogram plots of the maximum computation time taken by the counting algorithm and the minimum computation time taken by Spectral MLE across all trials. Even though this maximum-to-minimum comparison is unfair to the counting algorithm, it involves ﬁve or more orders of magnitude less computation.

4.1 Simulated data
We begin with simulations using synthetically generated data with n = 500 items and observation probability p = 1, and with pairwise comparison models ranging over six possible types. Panel (a) in Figure 1 provides a histogram plot of the associated error rates (with a bar for each one of these six models) in recovering the k = n/4 = 125 items for the counting algorithm versus the Spectral MLE algorithm. Each bar corresponds to the average over 50 trials. Panel (b) compares the CPU times of the two algorithms. The value of α (and in turn, the value of r) in the ﬁrst ﬁve models is as derived in Section 3.1. In more detail, the six model types are given by:
(I) Bradley-Terry-Luce (BTL) model: Recall that the theoretical guarantees for the Spectral MLE algorithm [CS15] are applicable to data that is generated from the BTL model (4a), and as guaranteed, the Spectral MLE algorithm gives a 100% accuracy under this model. The counting algorithm also obtains a 100% accuracy, but importantly, the counting algorithm requires a computational time that is ﬁve orders of magnitude lower than that of Spectral MLE.
(II) Thurstone model: The Thurstone model [Thu27] is another parametric model, with the function F in equation (4b) set as the cumulative distribution function of the standard Gaussian distribution. Both Spectral MLE and the counting algorithm gave 100% accuracy under this model.
(III) BTL model with one (non-transitive) outlier: This model is identical to BTL, with one modiﬁcation. Comparisons among (n − 1) of the items follow the BTL model as before, but the remaining item always beats the ﬁrst n4 items and always loses to each of the other items. We see that the counting algorithm continues to achieve an accuracy of 100%
13

as guaranteed by Theorem 1. The departure from the BTL model however prevents the Spectral MLE algorithm from identifying the top k items.
(IV) Strong stochastic transitivity (SST) model: We simulate the “independent diagonals” construction of [SBGW16] in the SST class. Spectral MLE is often unsuccessful in recovering the top k items, while the counting algorithm always succeeds.
(V) Mixture of BTL models: Consider two sets of people with opposing preferences. The ﬁrst set of people have a certain ordering of the items in their mind and their preferences follow a BTL model under this ordering. The second set of people have the opposite ordering, and their preferences also follow a BTL model under this opposite ordering. The overall preference probabilities is a mixture between these two sets of people. In the simulations, we observe that the counting algorithm is always successful while the Spectral MLE method often fails.
(VI) BTL with violation of separation condition: We simulate the BTL model, but with a choice of parameter r small enough that the value of α is about one-tenth of its recommended value in Section 3.1. We observe that the counting algorithm incurs lower errors than the Spectral MLE algorithm, thereby demonstrating its robustness.
To summarize, the performance of the two algorithms can be contrasted in the following way. When our stated lower bounds on α are satisﬁed, then consistent with our theoretical claims, the Copeland counting algorithm succeeds irrespective of the form of the pairwise probability distributions. The Spectral MLE algorithm performs well when the pairwise comparison probabilities are faithful to parametric models, but is often unsuccessful otherwise. Even when the condition on α is violated, the performance of the counting algorithm remains superior to that of the Spectral MLE.3 In terms of computational complexity, for every instance we simulated, the counting algorithm took several orders of magnitude less time as compared to Spectral MLE.
4.2 Experiments on data from Amazon Mechanical Turk
In this section, we describe experiments on real world datasets collected from the Amazon Mechanical Turk (mturk.com) commercial crowdsourcing platform.
4.2.1 Data
In order to evaluate the accuracy of the algorithms under consideration, we require datasets consisting of pairwise comparisons in which the questions can be associated with an objective and veriﬁable ground truth. To this end, we used the “cardinal versus ordinal” dataset from our past work [SBB+16]; three of the experiments performed in that paper are suitable for the evaluations here—namely, ones in which each question has a ground truth, and the pairs of items are chosen uniformly at random. The three experiments tested the workers’ general knowledge, audio, and visual understanding, and the respective tasks involved: (i) identifying the pair of cities with a greater geographical distance, (ii) identifying the higher frequency key of a piano, and (iii) identifying spelling mistakes in a paragraph of text. The number of items n in the three experiments were 16, 10 and 8 respectively. The total number of pairwise comparisons were 408, 265 and 184 respectively. The fraction of pairwise comparisons whose
3Note that part (b) of Theorem 1 is a minimax converse meaning that it appeals to the worst case scenario.
14

Hamming error Hamming error Hamming error

Distances
2.5 2.0 1.5 1.0 0.5 0.00.1 0.4 0.7 1.0
Subsampling probability

Spectral MLE Audio
1.5

Counting

1.0

0.5

0.00.1 0.4 0.7 1.0 Subsampling probability

2.0 Spelling mistakes
1.5 1.0 0.5 0.00.1 0.4 0.7 1.0
Subsampling probability

Figure 2. Evaluation of Spectral MLE and the counting algorithm on three datasets from Amazon Mechanical Turk in terms of the error rates for top k-subset recovery. The three panels plot the Hamming error when recovering the top k items in the three datasets when a qth fraction of the total data is used, for various values of subsampling probability q ∈ (0, 1]. The counting algorithm consistently outperforms the Spectral MLE algorithm.

outcomes were incorrect (as compared to the ground truth) in the raw data are 17%, 20% and 40% respectively.
4.2.2 Results
We compared the performance of the counting algorithm with that of the Spectral MLE algorithm. For each value of a “subsampling probability” q ∈ {0.1, 0.2, . . . , 1.0}, we subsampled a fraction q of the data and executed both algorithms on this subsampled data. We evaluated the performance of the algorithms on their ability to recover the top k = n4 items under the Hamming error metric.
Figure 2 shows the results of the experiments. Each point in the plots is an average across 100 trials. Observe that the counting algorithm consistently outperforms Spectral MLE. (We think that the erratic ﬂuctuations in the spelling mistakes data are a consequence of a high noise and a relatively small problem size.) Moreover, the Spectral MLE algorithm required about 5 orders of magnitude more computation time (not shown in the ﬁgure) as compared to counting. Thus the counting algorithm performs well on simulated as well as real data. It outperforms Spectral MLE not only when the number of items is large (as in the simulations) but also when the problem sizes are small as seen in these experiments.

5 Proofs

We now turn to the proofs of our main results. We continue to use the notation [i] to denote

the set {1, . . . , i} for any integer i ≥ 1. We ignore ﬂoor and ceiling conditions unless critical

to the proof.

Our lower bounds are based on a standard form of Fano’s inequality [CT12, Tsy08] for

lower bounding the probability of error in an L-ary hypothesis testing problem. We state a

version here for future reference. For some integer L ≥ 2, ﬁx some collection of distributions

{P1, . . . , PL}. Suppose that we observe a random variable Y that is obtained by ﬁrst sampling an index A uniformly at random from [L] = {1, . . . , L}, and then drawing Y ∼ PA.

(As a result, the variable Y is marginally distributed according to the mixture distribution

P

=

1 L

L a=1

Pa.)

Given

the

observation

Y

,

our

goal

is

to

“decode”

the

value

of

A,

correspond-

15

ing to the index of the underlying mixture component. Using Y to denote the sample space associated with the observation Y , Fano’s inequality asserts that any test function φ : Y → [L] for this problem has error probability lower bounded as

I(Y ; A) + log 2

P[φ(Y ) = A] ≥ 1 −

, log L

where I(Y ; A) denotes the mutual information between Y and A. A standard convexity argument for the mutual information yields the weaker bound

max DKL(Pa Pb) + log 2
a,b∈[L]
P[φ(Y ) = A] ≥ 1 − log L , (19)

We make use of this weakened form of Fano’s inequality in several proofs.

5.1 Proof of Theorem 1
We begin with the proof of Theorem 1, dividing our argument into two parts.

5.1.1 Proof of part (a)

For any pair of items (i, j), let us encode the outcomes of the r trials by an i.i.d. sequence Vi(j ) = [Xi(j ) Xj(i)]T of random vectors, indexed by ∈ [r]. Each random vector follows the distribution

P x(ij), x(ji)


1 − p   = pMij p(1 − Mij)   0

if (x(ij), x(ji)) = (0, 0) if (x(ij), x(ji)) = (1, 0) if (x(ij), x(ji)) = (0, 1)
otherwise.

With this encoding, the variable Wa : = ∈[r] z∈[n]\{a} Xa(rj) encodes the number of wins for item a.
Consider any item a ∈ Sk∗ which ranks among the top k in the true underlying ordering, and any item b ∈ [n]\Sk∗ which ranks outside the top k. We claim that with high probability, item a will win more pairwise comparisons than item b. More precisely, let Eba denote the
event that item b wins at least as many pairwise comparisons than a. We claim that

(i)

12 (rpn∆k)2

(ii) 1

P(Eba) ≤ exp

− rpn(2 − ∆

)+

2 rpn∆

≤ n16 .

(20)

k3

k

Given this bound, the probability that the counting algorithm will rank item b above a is no more than n−16. Applying the union bound over all pairs of items a ∈ Sk∗ and b ∈ [n]\Sk∗ yields P Sk = Sk∗ ≤ n−14 as claimed.
We note that inequality (ii) in equation (20) follows from inequality (i) combined with the
condition on ∆k that arises by setting α ≥ 8 as assumed in the hypothesis of the theorem.
Thus, it remains to prove inequality (i) in equation (20). By deﬁnition of Eba, we have

P(Eba) = P

Xb(z) −

Xa(z) ≥ 0 .

∈[r] z∈[n]\{b}

∈[r] z∈[n]\{a}

(21)

Wb

Wa

16

It is convenient to recenter the random variables. For every ∈ [r] and z ∈ [n]\{a, b}, deﬁne the zero-mean random variables
X(az) = Xa(z) − E[Xa(z)] = Xa(z) − pMaz and X(bz) = Xb(z) − E[Xb(z)] = Xb(z) − pMbz. Also, let
X(ab) = (Xa(b) − Xb(a)) − E[Xa(b) − Xb(a)] = (Xa(b) − Xb(a)) − (pMab − pMba). We then have

P(Eba) = P

X(bz) −

X(az) − X(ab) ≥ rp

Maz − Mbz .

∈[r] z∈[n\] {a,b}

z∈[n]\{a,b}

z∈[n]

Since a ∈ Sk∗ and b ∈ [n]\Sk∗, from the deﬁnition of ∆k, we have n∆k ≤ (Maz − Mbz),
z∈[n]
and consequently


P (Eba) ≤ P 
∈[r]

X(bz) −

X(az) − X(ab)

z∈[n\] {a,b}

z∈[n]\{a,b}

 ≥ rpn∆k .

(22)

By construction, all the random variables in the above inequality are zero-mean, mutually independent, and bounded in absolute value by 2. These properties alone would allow us to obtain a tail bound by Hoeﬀding’s inequality; however, in order to obtain the stated result (20), we need the more reﬁned result aﬀorded by Bernstein’s inequality (e.g., [BLM13]). In order to derive a bound of Bernstein type, the only remaining step is to bound the second moments of the random variables at hand. Some straightforward calculations yield

E[(−X(az))2] ≤ pMaz, E[(X(bz))2] ≤ pMbz, and E[(X(ab))2] ≤ pMab + pMba.

It follows that

E[(−X (az) )2 ]+

E[(X(bz))2] + E[(X(ab))2]

z∈[n]\{a,b}

z∈[n]\{a,b}





≤ p

(Maz + Mbz) + Mab + Mba

z∈[n]\{a,b}





(iii)
≤ p 2

Maz − n∆k

z∈[n]

(iv)
< pn(2 − ∆k),

where the inequality (iii) follows from the deﬁnition of ∆k, and step (iv) follows because Maz ≤ 1 for every z and Maa = 21 . Applying the Bernstein inequality now yields the stated bound (20)(i).

17

5.1.2 Proof of part (b)
The symmetry of the problem allows us to assume, without loss of generality, that k ≤ n2 . We prove a lower bound by ﬁrst constructing a ensemble of n − k + 1 diﬀerent problems, and considering the problem of distinguishing between them. For each a ∈ {k − 1, k, . . . , n}, let us deﬁne the k-sized subset S∗[a] : = {1, . . . , k − 1} ∪ {a}, and the associated matrix of pairwise probabilities

1  2 Miaj : = 12 + δ  12 − δ

if i, j ∈ S∗[a], or i, j ∈/ S∗[a] if i ∈ S∗[a] and j ∈/ S∗[a] if i ∈/ S∗[a] and j ∈ S∗[a],

where δ ∈ (0, 12 ) is a parameter to be chosen. We use Pa to denote probabilities taken under pairwise comparisons drawn according to the model M a.
One can verify that the construction above falls in the intersection of parametric models
and the SST model. In the parametric case, this construction amounts to having the parameters associated to every item in Sk∗ to have the same value, and those associated to every item in [n]\Sk∗ to have the same value. Also observe that for every such distribution Pa, the associated k-separation threshold ∆k = δ.
Any given set of observations can be described by the collection of random variables Y = {Yi(j ), j > i ∈ [n], ∈ [r]}. When the true underlying model is Pa, the random variable Yi(j ) follows the distribution

 0 Yi(j ) = i
 j

with probability 1 − p
with probability pMiaj with probability p(1 − Miaj).

The random variables {Yi(j )}i,j∈[n],i<j, ∈[r] are mutually independent, and the distribution Pa is a product distribution across pairs {i > j} and repetitions ∈ [r].

Let A ∈ {k, . . . , n} follow a uniform distribution over the index set, and suppose that given

A = a, our observations Y has components drawn according to the model Pa. Consequently,

the marginal distribution of Y is the mixture distribution L1

L a=1

Pa

over

all

L

=

n

−

k

+

1

models. Based on observing Y , our goal is to recover the correct index A = a of the underlying

model, which is equivalent to recovering the planted subset S∗[a]. We use the Fano bound (19)

to lower bound the error bound associated with any test for this problem. In order to apply

Fano’s inequality, the following result provides control over the Kullback-Leibler divergence

between any pair of probabilities involved.

Lemma 1. For any distinct pair a, b ∈ {k, . . . , n}, we have

DKL(Pa Pb) ≤

2npr .

(23)

1 4δ2

−

1

See the end of this section for the proof of this claim. Given this bound on the Kullback-Leibler divergence, Fano’s inequality (19) implies that
any estimator φ of A has error probability lower bounded as

2npr
1

+ log 2

P[φ(Y ) = A] ≥ 1 − 4δ2 −1

1 ≥.

log(n − k + 1) 7

18

Here the ﬁnal inequality holds whenever δ ≤ 71 lnogprn , p ≥ l2ongrn , n ≥ 7 and k ≤ n2 . The
condition p ≥ l2ongrn also ensures that δ < 21 thereby ensuring that our construction is valid. It only remains to prove Lemma 1.

5.1.3 Proof of Lemma 1

Since the distributions Pa and Pb are formed by components that are independent across edges i > j and repetitions ∈ [r], we have

DKL(Pa Pb) =

DKL(Pa(Xi(j )) Pb(Xi(j ))) = r

DKL(Pa(Xi(j1)) Pb(Xi(j1))),

∈[r] 1≤i<j≤n

1≤i<j≤n

where the second equality follows since the r trials are all independent and identically dis-
tributed.
We now evaluate each individual term in right hand side of the above equation. Consider
any i, j ∈ [n]. We divide our analysis into three disjoint cases: Case I: Suppose that i, j ∈ [n]\{a, b}. The distribution of Xi(j1) is identical across the distributions Pa and Pb. As a result, we ﬁnd that

DKL(Pa(Xi(j1)) Pb(Xi(j1))) = 0.

Case II: Suppose that i = a, j ∈ [n]\{a, b} or i = b, j ∈ [n]\{a, b}. We then have

a (1) b (1)

δ2

DKL(P

(Xij

)

P

(Xij

))

≤

p(1

−

δ)( 1

+

. δ)

2

2

Case III: Suppose that i = a, j = b. We then have

a (1) b (1)

(2δ)2

DKL(P

(Xij

)

P

(Xij

))

≤

p(1

−

δ)( 1

+

. δ)

2

2

Combining the bounds from all three cases, we ﬁnd that the KL divergence is upper bounded as

1

ab

δ2

(2δ)2

r DKL(P

P

)

≤

2(n

−

2)p ( 1

−

δ)( 1

+

δ)

+

p(1

−

δ)( 1

+

. δ)

2

2

2

2

Some simple algebraic manipulations yield the claimed result.

5.2 Proof of Corollary 1
We now turn to the proof of Corollary 1. Beginning with the claim of suﬃciency, it is easy to see that the ranking is correctly recovered whenever the top k items are correctly recovered for every value of k ∈ [n]. Consequently, one can apply the union bound to (10a) over all values of k ∈ [n] and this gives the desired upper bound.
Now turning to the claim of necessity, we ﬁrst introduce some notation to aid in subsequent discussion. Deﬁning the parameter ∆0 : = minj∈[n−1](τ(j) − τ(j+1)), we have shown that the lower bound
log n ∆0 ≥ 8 npr

19

is suﬃcient to guarantee exact recovery of the full ranking. Further, one must also have

1 n−1

1

1

∆0 ≤ n − 1

(τ(j)

−

τ(j+1))

=

n

−

1 (τ(1)

−

τ(n))

≤

n

−

. 1

j=1

Here we show that these two requirements are tight up to constant factors, meaning that

for

any

value

of

∆0

satisfying

∆0

≤

1 9

log n npr

and

∆0

≤

19 n−1 1 ,

there

are

instances

where

recovery of the underlying ranking fails with probability at least 710 for any estimator. Consider the following ensemble of (n − 1) diﬀerent problems, indexed by a ∈ [n − 1]. For

every value of a ∈ [n − 1], deﬁne a permutation πa of the n items as

 i + 1  πa(i) = i − 1
 i

if i = a if i = a + 1 otherwise.

In words, the permutation πa equals the identity permutation except for the swapping of items a and (a + 1). Deﬁne an associated matrix of pairwise-comparison probabilities M a as

Miaj = 21 − (πa(i) − πa(j))∆0,

and Mjai = 1 − Miaj. Let Pa denote the probabilities taken under pairwise comparisons drawn

according to the model M a.

The condition ∆0 ≤

11 9 n−1

ensures that this construction is a

valid probability distribution. One can then compute that under distribution Pa, the score

τia of any item i equals

τia = 12 − πa(i) − n +2 1 ∆0.

One can also verify that for any a ∈ [n − 1], and any i ∈ [n − 1], we have

τπaa(i) − τπaa(i+1) = ∆0,
where we have used the fact that πa(πa(i)) = i. The requirement imposed by the hypothesis is thus satisﬁed.
We now use Fano’s inequality (19) obtain the claimed lower bound. In order to apply this result, we ﬁrst obtain an upper bound on the Kullback-Leibler divergence between the probability distributions of the observed data under any pair of problems constructed above.

Lemma 2. For any distinct pair a, b ∈ [n − 1], we have

DKL(Pa Pb) ≤ 50npr∆20.

See the end of this section for the proof of this claim. Given this bound on the Kullback-Leibler divergence, the Fano bound (19) implies that
any method φ for identifying the true ranking has error probability

P[φ(Y ) = A] ≥ 1 − 50npr∆20 + log 2 ≥ 1 ,

log(n − 1)

70

where

the

ﬁnal

inequality

holds

whenever

∆0

≤

1 9

lnogprn and n ≥ 9.

The only remaining detail is the proof of Lemma 2.

20

5.2.1 Proof of Lemma 2
Since the distributions Pa and Pb are formed by components that are independent across edges i > j and repetitions ∈ [r], we have

DKL(Pa Pb) =

DKL(Pa(Xi(j )) Pb(Xi(j ))) = r

DKL(Pa(Xi(j1)) Pb(Xi(j1))),

∈[r] 1≤i<j≤n

1≤i<j≤n

where the second equality follows since the r trials are all independent and identically dis-
tributed.
We now evaluate each individual term in right hand side of the above equation. Consider
any i, j ∈ [n]. We divide our analysis into three disjoint cases: Case I: Suppose that i, j ∈ [n]\{a, a + 1, b, b + 1}. The distribution of Xi(j1) is identical across the distributions Pa and Pb. As a result, we ﬁnd that

DKL(Pa(Xi(j1)) Pb(Xi(j1))) = 0.

Case II: Alternatively, suppose i ∈ {a, a + 1, b, b + 1} and j ∈ [n]\{a, a + 1, b, b + 1} or if j ∈ {a, a + 1, b, b + 1} and i ∈ [n]\{a, a + 1, b, b + 1}. Then we have
DKL(Pa(Xi(j1)) Pb(Xi(j1))) ≤ 5p∆20,
where we have used the fact that Pa(Xi(j1)) and Pb(Xi(j1)) both take values in [ 178 , 1118 ] since ∆0 ≤ 91 n−1 1 . Case III: Otherwise, suppose that both i, j ∈ {a, a + 1, b, b + 1}. Then we have
DKL(Pa(Xi(j1)) Pb(Xi(j1))) ≤ 20p∆20.

Combining the bounds from the three cases, we ﬁnd that the KL divergence is upper bounded as
1 DKL(Pa Pb) ≤ 40(n − 4)p∆20 + 240p∆20 ≤ 50np∆20, r where we have used the assumption n ≥ 9 to obtain the ﬁnal inequality.
5.3 Proof of Theorem 2
We now turn to the proof of Theorem 2, beginning with part (a).
5.3.1 Proof of part (a)
Without loss of generality, we can assume that the true underlying ranking is the identity ranking, that is, item i is ranked at position i for every i ∈ [n]. Given the the lower bound α ≥ 8 is satisﬁed, Theorem 1 ensures that with probability at least 1−n−16, the counting estimator Sk ranks every item in {1, . . . , k − h} higher than every item in the set {k + h + 1, . . . , n}. Thus, we are guaranteed that either Sk ⊆ [k + h] and/or [k − h] ⊆ Sk. One can verify either case leads to |Sk ∩ [k]| ≥ k − h, thereby proving the claimed result.

21

5.3.2 Proof of part (b)
We assume without loss of generality that k ≤ n2 . (Otherwise, one can equivalently study the problem of recovering the last k items.) Since the case h = 0 is already covered by Theorem 1(b), we may also assume that h ≥ 1.
The proof involves construction of L ≥ 1 sets of probability matrices {M a}a∈[L] of the pairwise comparisons with the following two properties:
(i) For every a ∈ [L], let Ska ⊆ [n] denote the set of the top k items under the ath set of distributions. Then for every k-sized set S ∈ [n],
L
1{DH(S, Ska) ≤ 2h} ≤ 1.
a=1

(ii) If the underlying distribution a is chosen uniformly at random from this set of L distributions, then any estimator that attempts to identify the underlying distribution a ∈ [L] errs with probability at least 71 .
Now consider any estimator Sk for identifying the top k items Sk∗. Given property (i), whenever the estimator is successful under the Hamming error requirement DH(Sk, Sk∗) ≤ 2h, it must be able to uniquely identify the index a ∈ [L] of the underlying distribution of pairwise comparison probabilities. However, property (ii) mandates that any estimator for identifying the underlying distribution errs with a probability at least 17 . Assuming that such sets of probability distributions satisfying these two properties exist, putting these results together yields the claimed result.
We now proceed to construct probability distributions satisfying the two aforementioned properties. Consider any positive number ∆0 satisfying the upper bound

∆0 ≤ 1 ν1ν2 log n . (24) 14 npr

The L matrices {M a}a∈[L] of probability distributions we construct diﬀer only in a permutation of their rows and columns, and modulo this permutation, have identical values. In
other words, these L distributions diﬀer only in the identities of the n items and the values of the pairwise-comparison probabilities M(ai)(j) among the ordered sequence of the n items are identical across all distributions a ∈ [L].
For any ordering (1), . . . , (n) of the n items, for every a ∈ [L], set

1 2

+

∆0

if i ∈ [k] and j ∈/ [k]



M(ai)(j) =

1 2

−

∆0

if i ∈/ [k] and j ∈ [k]

(25)

 12 otherwise.

Note that the upper bound (24) on ∆0, coupled with the assumption p ≥ l2ongrn , ensures that

∆0 <

1 3

and hence that our deﬁnition (25) leads to a valid set of probabilities.

Given this

construction, the scores of the n items are τ(1) = · · · = τ(k) = τ(k+1) + ∆0 = · · · = τ(n) + ∆0.
√
The bound (24) ensures that the condition α ≤ ν114ν2 required by the hypothesis of the

theorem is satisﬁed.

22

It remains to specify the ordering of the n items in each set of probability distributions.
This speciﬁcation relies on the following lemma, that in turn uses a coding-theoretic result due to Levenshtein [Lev71]. It applies in the regime 2h ≤ 1+1ν2 min{n1−ν1, k, n − k} for some constants ν1 ∈ (0, 1) and ν2 ∈ (0, 1), and when n is larger than a (ν1, ν2)-dependent constant.

Lemma 3. Under the previously given conditions, there exists a subset {b1, . . . , bL} ⊆ {0, 1}n/2

with

cardinality

L

≥

e

9 10

ν

1

ν

2

h

log

n

,

and

such

that

DH(bj, 0) = 2(1 + ν2)h, and DH(bj, bk) > 4h for all j = k ∈ [L].

We prove this lemma at the end of this section. Given this lemma, we now complete the

proof of the theorem. Map the n2 items { n2 + 1, . . . , n} to the n2 bits in each of the strings

given by Lemma 3. For each

∈

[

e

9 10

ν

1

ν2

h

log

n

],

let

B

denote the 2(1 + ν2)h-sized subset of

{ n2 + 1, . . . , n} corresponding to the 2(1 + ν2)h positions equalling 1 in the th string. Also

deﬁne sets A = {1, . . . , k −2(1+ν2)h} and C = [n]\(A ∪B ). We note that this construction

is valid since 2h ≤ 1+1ν2 k.

We

now

construct

L

=

e

9 10

ν1

ν2

h

log

n

sets

of

pairwise

comparison

probability

distributions

M 1, . . . , M L and show that these sets satisfy the two required properties. As mentioned

earlier, each matrix of comparison-probabilities M takes values as given in (25), but diﬀers in

the underlying ordering of the n items. In particular, associate the set ∈ [L] of distributions

to any ordering of the n items that ranks every item in A higher than every item in B , and

every item in B in turn higher than every item in C . Then for any , the set of top k items

is given by A ∪ B . From the guarantees provided by Lemma 3, for any distinct , m ∈ [L],

we have DH(A ∪ B , Am ∪ Bm) ≥ 4h + 1. This construction consequently satisﬁes the ﬁrst

required property.

We now show that the construction also satisﬁes the second property: namely, it is diﬃcult

to identify the true index. We do so using Fano’s inequality (19), for which we denote the

probability distribution of the observations due to any matrix M , ∈ [L], as P .

We ﬁrst derive an upper bound on the Kullback-Leibler divergence between any two distri-

butions P and Pm of the observations. Observe that P (i j) = Pm(i j) only if i ∈ B ∪Bm

or j ∈ B ∪ Bm. In this case, we have DKL(P (i

j) Pm(i

j)) ≤ 4∆20 . Since both sets B

1 4

−∆

2 0

and Bm have a cardinality of 2(1 + ν2)h, aggregating over all possible observations across all

pairs, we obtain that

m

4∆20

DKL(P P ) ≤ 4(1 + ν2)hnpr 1 − ∆2 .

(26)

4

0

In

the

regime

p

≥

log n 2nr

and

∆0

≤

1 14

ν1νn2plorg n , we have ∆0

inequality

∆0

≤

1 14

ν1 log n npr

in

the

numerator

and

1 4

− ∆20

≥

1 4

−

of the right hand side of the bound (26), we ﬁnd that

≤ 1√ . Substituting the
14 2
1√ 2 in the denominator
14 2

DKL(P

Pm)

≤

3 ν1ν2h log n.

4

Now suppose that we drawn Y from some distribution chosen uniformly at random from {P1, . . . , PL}. Applying Fano’s inequality (19) ensures that any test φ for estimating the index

A of the chosen distribution must have error probability lower bounded as

P φ(Y ) = A] ≥

3 4

ν1

ν2

h

log

n

+

log

2

1 − 190 ν1ν2h log n

1 ≥.
7

23

Here the ﬁnal inequality holds as long as n is larger than some universal constant.

5.3.3 Proof of Lemma 3

We divide the proof into two cases depending on the value of h. Case I: h ≥ 2ν11ν2 : Let L denote the number of binary strings of length m0 such that
each has a Hamming weight w0 and each pair has a Hamming distance at least d0. It is
known [Lev71, JV04] that L can be lower bounded as:

L≥

m0

m0 w0

w0

≥

w0

.

d0 −1 2

w0 m0−w0

d0+1

ew0

min{d0,w0}/2

em0

min{d0,m0}/2

i=0

j

j

2 min{d0,w0}/2

min{d0,m0}/2

Note that for the setting at hand, we have m0 = n2 , w0 = 2(1 + ν2)h and d0 = 4h + 1. Since ν1 ∈ (0, 1) and ν2 ∈ (0, 1), we have the chain of inequalities

w0

<

d0

≤

4n1−ν1

(i)
<

n

=

m0,

2

where the inequality (i) holds when n is large enough. These relations allow for the simpliﬁ-

cation:


 log L ≥ log
 d0+1
2

m0 w0 w0

ew0 w0/2 em0

w0/2

d0/2

 
d0/2


= (w0 − d0/2) log m0 − w0 log w0 + d0 log d0 − d0 + w0 log(2e) − log((d0 + 1)/2).

2

2

Substituting the values of w0, d0 and m0 and then simplifying yields

1n

1

log L ≥ (2ν2h − 2 ) log 2 − 2(1 + ν2)h log(2(1 + ν2)h) + (2h + 2 ) log(4h + 1)

1 − (((3 + ν2)h) + 2 ) log(2e) − log(2h + 1)

1n

≥

(2ν2h

−

) log 2

2

−

2ν2h log(2(1

+

ν2)h)

−

c1h,

where c1 is a constant whose value depends only on (ν1, ν2). In the regime ν11ν2 ≤ 2h ≤ n11+−νν21 , some algebraic manipulations then yield

1n

9

log L

≥

(2ν1ν2h

−

) log 2

2

−

c1h

≥

ν1ν2h(log n

−

log 2

−

c1)

≥

10 ν1ν2h log n,

where the ﬁnal inequality holds when n is large enough.

Case II: h < 2ν11ν2 Consider a partition of the n2 bits into 4(1+nν2)h sets of size 2(1 + ν2)h each. Deﬁne an associated set of 4(1+nν2)h sets of binary strings, each of length n2 , with the ith string having ones in the positions corresponding to the ith set in the partition and zeros
elsewhere. Then each of these strings have a Hamming weight of 2(1 + ν2)h, and every pair has a Hamming distance at least 4(1 + ν2)h > 4h. The total number of such strings equals

n exp log

(i)

2(1 + ν2) (ii)

9

(iii)

≥ exp log n − log(

) ≥ exp log n) > exp 1.8ν1ν2h log n ,

4(1 + ν2)h

ν1ν2

10

where the inequalities (i) and (iii) are a result of operating in the regime h < 2ν11ν2 and the inequality (ii) assumes that n is greater than a (ν1, ν2)-dependent constant.

24

5.4 Proof of Theorem 3
We now turn to the proof of Theorem 3.

5.4.1 Proof of part (a)
For every i ∈ [n], let (i) denote the item ranked i according to their latent scores, as deﬁned in equation (2). Recall from the proof of Theorem 1 that for any u < v ∈ [n], the condition

log n τ(u) − τ(v) ≥ 8 npr

ensures that with probability at least 1 − n−14, every item in the set {(1), . . . , (u)} wins more comparisons than every item in the set {(v), . . . , (n)}. Consequently, if the set Sk contains any item in {(v), . . . , (n)}, then it must contain the entire set {(1), . . . , (u)}. In other words, at least one of the following must be true: either {(1), . . . , (u)} ⊆ Sk or Sk ⊆ {(1), . . . , (v − 1)}. Consequently, in the regime v = k + t − u + 1 for any 1 ≤ u ≤ k and u ≤ t ≤ n, we have that

|Sk ∩ {(1), . . . , (t)}| ≥ u.

(27)

Now consider any b ∈ [β] that satisﬁes the condition

min(τ(j) − τ(k+tb−j+1)) ≥ 8

j∈[k]

j

log n .
npr

For any j ∈ [k], setting u = j and v = (k + tbj − j + 1) in (27), and applying the union bound over all values of j ∈ [k] yields that
|Sk ∩ {(1), . . . , (tbj)}| ≥ j for every j ∈ [k], with probability at least 1 − n−13. Consequently, we have that
P Sk ∈ Λ(Tb) ≥ 1 − n−13,

completing the proof of the claim.

5.4.2 Proof of part (b)
In the regime tbµ2k ≤ n2 for every b ∈ [β], it suﬃces to show that any estimator Sk will incur an error lower bounded as
1 P |Sk ∩ {(1), . . . , (n/2)}| < µ2k ≥ 15 ,
where (i) denotes the item ranked i according to their latent scores according to equation (2). Our proof relies on the result and proof of the Hamming error case analyzed in Theo-
rem 2(b). To this end, let us set the parameter h of Theorem 2(b) as h = 2(1 − µ2)k. We claim that this value of h lies in the regime h ≤ 2(1+1 ν2) min{k, n − k, n1−ν1} for some values ν1 ∈ (0, 1) and ν2 ∈ (0, 1), as required by Theorem 2(b). This claim follows from the fact that
1 h = 2(1 − µ2)k ≤ 2(1 + ν2) k,

25

for ν2 = min{ 4(1−1µ2) − 1, 21 } ∈ (0, 1). Furthermore,

(i) n1−µ1 (ii)

h = 2(1 − µ2)k ≤

≤

1 n1−ν1

4

2(1 + ν2)

for ν1 = 190 µ1 ∈ (0, 1), where (i) is a result of our assumption 8(1 − µ2)k ≤ n1−µ1 and (ii) holds when n is large enough. This assumption also implies that k ≤ n − k for a large enough
value of n. We have now veriﬁed operation in the regime required by Theorem 2(b).
The construction in the proof of Theorem 2 is based on setting

τ(1) = · · · τ(k) = τ(k+1) + ∆0 = · · · = τ(n) + ∆0,

for any real number ∆0 in the interval 0, 114

ν1ν2 log n npr

. This condition is also satisﬁed

in our construction due to the assumed upper bound α ≤ 115 µ1 min 4(1−µ12)−1 , 21 . Conse-

quently, the result of Theorem 2(b) implies that in this setting, any estimator Sk will incur a Hamming error greater than h = 2(1 − ν2)k with probability at least 17 , or equivalently,

1 P |Sk ∩ {(1), . . . , (k)}| < (2µ2 − 1)k ≥ 7 .

Under this event, the estimator Sk contains at most (2µ2 − 1)k − 1 items from the set of top k items. In order to ensure it gets at least µ2k items from {(1), . . . , (n/2)}, the remaining 2(1 − µ2)k + 1 chosen items must have at least (1 − µ2)k + 1 items from {(k + 1), . . . , (n/2)}. However, in the construction, items (k + 1), . . . , (n) are indistinguishable from each other, and hence by symmetry these 2(1 − µ2)k + 1 chosen items must contain at least (1 − µ2)k + 1 items from the set {(n/2 + 1), . . . , (n)} with probability at least 21 . Putting these arguments together, we obtain that under this construction, any estimator Sk has error probability lower bounded as
1 P |Sk ∩ {(1), . . . , (n/2)}| < µ2k ≥ 14 . (28)
It remains to deal with a subtle technicality. The construction above involves items (k + 1), . . . , (n) with identical scores. Recall that in the deﬁnition of the user-deﬁned requirement, in case of multiple items with identical scores, we considered the choice of either of such items as valid. The following lemma helps overcome this issue. In order to state the lemma, we deﬁne |||M |||∞ : = max(i,j)∈[n]2 |Mij| for a matrix M ∈ Rn×n.
Lemma 4. Consider any two (n × n) matrices M a and M b of pairwise probabilities such that

|||M a − M b|||∞ ≤ , |||M a|||∞ ≥ , and |||M b|||∞ ≥

(29a)

for some ∈ [0, 1]. Then for any k-sized sets of items T1, . . . , Tβ ⊆ [n], and any estimator Sk, we have

| PMa(Sk ∈ {T1, . . . , Tβ}) − PMb(Sk ∈ {T1, . . . , Tβ}) |≤ 6n2r .

(29b)

26

See Section 5.4.3 for the proof of this claim.

Now consider an (n × n) pairwise probability matrix M whose entries takes values

  12 + ∆0 +



M

=

 1
2

+

∆0

(i)(j)  21 +

 1

2

if i ∈ [k] and j ∈ [n]\[n/2] if i ∈ [k] and j ∈ [n/2]\[k] if i ∈ [n/2]\[k] and j ∈ [n]\[n/2] otherwise,

and Mji = 1 − Mij, whenever i ≤ j. Set = 7−n2r. One can verify that under the probability matrix M , the scores of the n items satisfy the
relations

τ(1) = · · · = τ(k) = τ(k+1) + ∆0 = · · · = τ(n/2) + ∆0 = τ(n/2+1) + ∆0 + = · · · = τ(n) + ∆0 + .

The set of items {(1), . . . , (n/2)} are thus explicitly distinguished from the items {(n/2 +

1), . . . , (n)}. We now call upon Lemma 4 with M a = M , and M b as the matrix of probabilities

constructed in the proof of Theorem 2, where both sets have the same ordering of the items.

This

assignment

is

valid

given

that

∆0

<

1 3

and

= 7−n2r. Lemma 4 then implies that

any estimator that is S-respecting with probability at least 1 − 115 under M b must also be

S-respectiin with probability at least 1 − 141.5 under M a. But by equation (28), the latter

condition is impossible, which implies our claimed lower bound.

5.4.3 Proof of Lemma 4

Let Pa and Pb denote the probabilities induced by the matrices M a and M b respectively. Consider any ﬁxed observation Y1 ⊆ {0, 1, φ}r(n×n), where φ denotes the absence of an observation.
Given the bounds (29a), some algebra leads to

| Pa(Y = Y1) − Pb(Y = Y1) |≤ 2n2r ,

(30)

where Pa(Y = Y1) and Pb(Y = Y1) denote the probabilities of observing Y1 under Pa and Pb,
respectively. Now consider any estimator Sk, which is permitted to be randomized. Let L ≤ 3n2r denote
the total number of possible values of the observation Y , and let {Y1, . . . , YL} = {0, 1, φ}r(n×n)
denote the set of all possible valid values of the observation. For each i ∈ [L], let qi ∈ [0, 1]
denote the probability that the estimator Sk succeeds in satisfying the given requirement when the data observed equals Yi. (Recall that the given requirement is in terms of the actual
items and not their positions.) Then we have

P1(Sk ∈ {T1, . . . , Tβ}) − P2(Sk ∈ {T1, . . . , Tβ})

L

L

=

P1(Y = Yi)qi − P2(Y = Yi)qi

i=1

i=1

L
≤ | P1(Y = Yi) − P2(Y = Yi) | qi

i=1

(i) L

(ii)

≤ 2n2r qi ≤ 6n2r ,

i=1

as claimed, where step (i) follows from our earlier bound (30) and step (ii) uses the bound L ≤ 3n2r.

27

6 Discussion
In this paper, we analyzed the problem of recovering the k most highly ranked items based on observing noisy comparisons. We proved that an algorithm that simply selects the items that win the maximum number of comparisons is, up to constant factors, an informationtheoretically optimal procedure. Our results also extend to recovering the entire ranking of the items as a simple corollary. In empirical evaluations, this algorithm takes several orders of magnitude lower computation time while providing higher accuracy as compared to prior work. The results of this paper thus underscore the philosophy of Occam’s razor that the simplest answer is often correct.
There are number of open questions suggested by our work. The observation model considered here is based on a random number of observations for all pairs of comparisons. It would be interesting to extend our results to cases in which only speciﬁc subsets of pairs are observed. Moreover, we considered a random design setting where we do not have any control over which pairs are compared. The notion of allowable sets introduced in this paper apply to recovery of k-sized subsets of the items; such a formulation and associated results may apply to recovery of partial or total orderings of the items. A parallel line of literature (e.g., [KK13, BFSC+13, JKDN15]) studies settings in which the pairs to be compared can be chosen sequentially in a data-dependent manner, but to the best of our knowledge, this line of literature considers only the metric of exact recovery of the top k items. It is of interest to investigate the Hamming and allowable set recovery problems in such an active setting.
Acknowledgements
This work was partially supported by NSF grant CIF-31712-23800; Air Force Oﬃce of Scientiﬁc Research grant AFOSR-FA9550-14-1-0016; and Oﬃce of Naval Research grant DOD ONR-N00014. In addition, NBS was also supported in part by a Microsoft Research PhD fellowship.

References

[AS12]

A. Ammar and D. Shah. Eﬃcient rank aggregation using partial data. In ACM SIGMETRICS Performance Evaluation Review, 2012.

[BFSC+13] R. Busa-Fekete, B. Szorenyi, W. Cheng, P. Weng, and E. Hu¨llermeier. Topk selection based on adaptive sampling of noisy preferences. In International Conference on Machine Learning, 2013.

[BLM13]

S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press, 2013.

[BM08]

M. Braverman and E. Mossel. Noisy sorting without resampling. In Proc. ACMSIAM symposium on Discrete algorithms, pages 268–276, 2008.

[BO03]

B. Babcock and C. Olston. Distributed top-k monitoring. In Proceedings of the 2003 ACM SIGMOD international conference on Management of data, pages 28–39, 2003.

[BT52]

R. Bradley and M. Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, pages 324–345, 1952.

28

[BW97] [CCF+01]
[Cha14] [Cop51] [CS15] [CT12] [dB81] [DIS15] [DM59] [ER60] [Eri13] [FLN03] [HOX14] [Hun04] [IBS08] [JKDN15] [JS08] [JV04] [KK13] [KMS07] [KS06]

T. P. Ballinger and N. Wilcox. Decisions, error and heterogeneity. The Economic Journal, 107(443):1090–1105, 1997.
D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soﬀer. Static index pruning for information retrieval systems. In ACM SIGIR conference on Research and development in information retrieval, 2001.
S. Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177–214, 2014.
A. H. Copeland. A reasonable social welfare function. In University of Michigan Seminar on Applications of Mathematics to the social sciences, 1951.
Y. Chen and C. Suh. Spectral MLE: Top-k rank aggregation from pairwise comparisons. In International Conference on Machine Learning, 2015.
T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
J. C. de Borda. M´emoire sur les ´elections au scrutin. 1781.
W. Ding, P. Ishwar, and V. Saligrama. A topic modeling approach to ranking. In Conference on Artiﬁcial Intelligence and Statistics, 2015.
D. Davidson and J. Marschak. Experimental tests of a stochastic decision theory. Measurement: Deﬁnitions and theories, pages 233–69, 1959.
P. Erd˝os and A. R´enyi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5:17–61, 1960.
B. Eriksson. Learning to top-k search using pairwise comparisons. In Conference on Artiﬁcial Intelligence and Statistics, 2013.
R. Fagin, A. Lotem, and M. Naor. Optimal aggregation algorithms for middleware. Journal of computer and system sciences, 66(4):614–656, 2003.
B. Hajek, S. Oh, and J. Xu. Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, 2014.
D. Hunter. MM algorithms for generalized Bradley-Terry models. Annals of Statistics, pages 384–406, 2004.
I. F. Ilyas, G. Beskales, and M. A. Soliman. A survey of top-k query processing techniques in relational database systems. ACM Computing Surveys, 2008.
K. Jamieson, S. Katariya, A. Deshpande, and R. Nowak. Sparse dueling bandits. arXiv preprint arXiv:1502.00133, 2015.
S. Jagabathula and D. Shah. Inferring rankings under constrained sensing. In Advances in Neural Information Processing Systems, 2008.
T. Jiang and A. Vardy. Asymptotic improvement of the gilbert-varshamov bound on the size of binary codes. IEEE Transactions on Information Theory, 2004.
E. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Conference on Learning Theory, pages 228–251, 2013.
C. Kenyon-Mathieu and W. Schudy. How to rank with few errors. In Symposium on Theory of computing (STOC), pages 95–103. ACM, 2007.
B. Kimelfeld and Y. Sagiv. Finding and approximating top-k answers in keyword proximity search. In Symposium on Principles of database systems, 2006.

29

[Lev71]

V. I. Levenshtein. Upper-bound estimates for ﬁxed-weight codes. Problemy Peredachi Informatsii, 7(4):3–12, 1971.

[Luc59]

R. D. Luce. Individual choice behavior: A theoretical analysis. New York: Wiley, 1959.

[MAEA05] A. Metwally, D. Agrawal, and A. El Abbadi. Eﬃcient computation of frequent and top-k elements in data streams. In Database Theory-ICDT. 2005.

[MGCV11] I. Mitliagkas, A. Gopalan, C. Caramanis, and S. Vishwanath. User rankings from comparisons: Learning permutations in high dimensions. In Allerton Conference on Communication, Control, and Computing, 2011.

[ML65]

D. H. McLaughlin and R. D. Luce. Stochastic transitivity and cancellation of preferences between bitter-sweet solutions. Psychonomic Science, 1965.

[MTW05]

S. Michel, P. Triantaﬁllou, and G. Weikum. Klee: A framework for distributed top-k query algorithms. In International conference on Very large data bases, 2005.

[NOS12]

S. Negahban, S. Oh, and D. Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, 2012.

[RA14]

A. Rajkumar and S. Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In International Conference on Machine Learning, 2014.

[RGLA15]

A. Rajkumar, S. Ghoshal, L.-H. Lim, and S. Agarwal. Ranking from stochastic pairwise preferences: Recovering Condorcet winners and tournament solution sets at the top. In International Conference on Machine Learning, 2015.

[SBB+16]

N. B. Shah, S. Balakrishnan, J. Bradley, A. Parekh, K. Ramchandran, and M. J. Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. Journal on Machine Learning Research, 2016.

[SBGW16] N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. In International Conference on Machine Learning (ICML), 2016.

[SBW16]

N. B. Shah, S. Balakrishnan, and M. J. Wainwright. Feeling the Bern: Adaptive estimators for Bernoulli probabilities of pairwise comparisons. In International Symposium on Information Theory, 2016.

[SPX14]

H. Souﬁani, D. Parkes, and L. Xia. Computing parametric ranking models via rank-breaking. In International Conference on Machine Learning, 2014.

[Thu27]

L. L. Thurstone. A law of comparative judgment. Psychological Review, 34(4):273, 1927.

[Tsy08]

A. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. 2008.

[Tve72]

A. Tversky. Elimination by aspects: A theory of choice. Psychological review, 79(4):281, 1972.

[WJJ13]

F. Wauthier, M. Jordan, and N. Jojic. Eﬃcient ranking from pairwise comparisons. In International Conference on Machine Learning, 2013.

30

