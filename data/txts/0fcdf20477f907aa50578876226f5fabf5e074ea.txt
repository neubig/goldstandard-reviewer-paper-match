Correcting Exposure Bias for Link Recommendation

arXiv:2106.07041v1 [cs.LG] 13 Jun 2021

Shantanu Gupta 1 2 Hao Wang 3 Zachary C. Lipton 2 Yuyang Wang 4

Abstract
Link prediction methods are frequently applied in recommender systems, e.g., to suggest citations for academic papers or friends in social networks. However, exposure bias can arise when users are systematically underexposed to certain relevant items. For example, in citation networks, authors might be more likely to encounter papers from their own ﬁeld and thus cite them preferentially. This bias can propagate through naively trained link predictors, leading to both biased evaluation and high generalization error (as assessed by true relevance). Moreover, this bias can be exacerbated by feedback loops. We propose estimators that leverage known exposure probabilities to mitigate this bias and consequent feedback loops. Next, we provide a loss function for learning the exposure probabilities from data. Finally, experiments on semi-synthetic data based on real-world citation networks, show that our methods reliably identify (truly) relevant citations. Additionally, our methods lead to greater diversity in the recommended papers’ ﬁelds of study. The code is available at github.com/shantanu95/ exposure-bias-link-rec.
1. Introduction
Diverse application domains, including both citation networks and social networks, are characterized by graphstructured data. Here, nodes represent entities (like papers or users) and edges represent associations between two nodes (like citations, friendships, or follows). Link recommender systems (RSs) leverage node attributes and existing links to suggest new nodes that a given node should link to (Li et al., 2017; Bai et al., 2019; Ma et al., 2020). Typically, RSs are trained and evaluated directly on the observed graph, raising concerns about exposure bias—many missing links are false
1Work done while interning at Amazon 2Carnegie Mellon University 3Rutgers University 4Amazon Web Services (AWS) AI Labs. Correspondence to: Shantanu Gupta <shantang@cs.cmu.edu>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

negatives, and did not form due to lack of exposure rather than a lack of afﬁnity.
Consider the example of an RS that recommends relevant citations to authors given attributes of their paper (like title, abstract, etc.). In this case, equally relevant papers from different ﬁelds of study (FOS) might be less cited historically because authors have been preferentially exposed to papers in their own FOS. In the observed citation graph, a number of relevant papers are observed as not cited because the user was not exposed to those papers. Thus evaluating a link RS directly on the observed graph may yield a biased estimate of the true risk.
Exposure bias can exacerbate popularity bias, causing relevant but unpopular items to not be shown (Chen et al., 2020). In social networks, diverse recommendations can help users form links with communities they would otherwise not discover (Li et al., 2017; Branda˜o et al., 2013). In citation networks, exposure bias can also lead to lines of research being duplicated across ﬁelds. Examples include model-based science and linear canonical transforms, which were developed in isolation (Vincenot, 2018; Liberman & Wolf, 2015). Thus it would be valuable to have an RS that recommends relevant low-exposure nodes.
In this paper, we call the probability that a node is exposed to another node the propensity score; and we call the probability that, given exposure, a node links to another node the link probability. An RS trained directly on the observed data will underestimate the link probability for low propensity nodes relative to high propensity nodes. We demonstrate this with a simple example in the context of academic citation recommendation.
Example 1 (Exposure Bias). Let’s say that there are two FOS: Machine Learning (ML) and Physics (PH), with n papers in each. An ML researcher is looking for papers to cite. The probability of them being exposed to papers in ML and PH is 0.9 and 0.6, respectively. Given exposure, the probabilities that they cite papers from ML and PH are 0.8 and 0.8, respectively. In the observed data, we will see ≈ 0.72n(= 0.9 × 0.8n) ML papers cited and ≈ 0.48n(= 0.6 × 0.8n) PH papers cited. Thus, if we directly learn link probabilities from the observed data, the probability of citing a PH paper will be underestimated (0.48 instead of 0.8) more than that of an ML paper (0.72 instead of

Correcting Exposure Bias for Link Recommendation

0.8). This shows that equally relevant papers with lower propensity may be deemed less relevant.
To begin, we show that evaluating an RS naively on the observed data provides a misleading measure of its risk. Instead, we argue that an RS should be evaluated via the risk that would have been incurred had every user been exposed to every node. We call this the true risk. We propose three estimators of the true risk that use known propensity scores for estimation (Section 3). The key idea is to weight the positive and negative links using functions of the propensity scores and link probabilities. Each of the three estimators uses a different weighting scheme. We provide sufﬁcient conditions for when they will have lower bias than the naive estimator for the true risk. We then derive a generalization bound that shows that, with high probability, the true risk is close to the risk estimated by our proposed methods. We use this bound to motivate a loss function that can be used to simultaneously learn the link probabilities and propensity scores (Section 4). Next, under a simpliﬁed model of link recommendation, where nodes belong to one of a ﬁnite number of categories, we prove that feedback loops arise under exposure bias and that they worsen at a faster rate for lower propensity nodes (Section 5). We further show that accounting for exposure bias can help alleviate them.
We empirically validate our methods on real-world citation data from the Microsoft Academic Graph (MAG) (Sinha et al., 2015) (Section 6). Since true exposure values are not available in the real data, we construct a semi-synthetic data with simulated exposure and link probabilities. Our methods lead to higher precision and recall against true citations than the naive method. On real data, our methods maintain comparable performance to the naive method on metrics computed against the observed data and recommend more papers from different ﬁelds-of-study.
2. Related Work
There is a rich literature for correcting bias in RSs. Swaminathan & Joachims (2015) present a counterfactual risk minimization framework for learning from logged bandit feedback. Joachims et al. (2017) use a counterfactual inference framework to counteract selection bias in click data. Schnabel et al. (2016) propose unbiased performance estimators for RSs that use known propensity scores when explicit item ratings are observed with selection bias. Ma & Chen (2019) recover propensities under the low nuclear norm assumption. Wang et al. (2020b) use exposure data to construct a substitute for unobserved confounders. Wang et al. (2021) show that bandit algorithms can lead to an unfair allocation of exposure across arms, and to overcome this issue, they propose an alternative formulation, where each arm receives exposure proportional to its merit. The implicit feedback setting, where user interactions, such as

clicking and listening (as opposed to explicit ratings), are used to train the RS, is more closely related to our setting. It is known that in this setting, some negative examples are false negatives due to exposure bias (Jeunen, 2019, Section 4.1). Yang et al. (2018) use inverse propensity scoring to create an unbiased evaluator for this setting using inversepropensity-scoring based methods. Liang et al. (2016b) model exposure as a latent variable and incorporate it into a collaborative-ﬁltering approach. Liang et al. (2016a) use exposure and click models to re-weight samples to make unbiased predictions. Our work leverages ideas from these works, especially the approach of re-weighting samples to counter the bias. However, this work addresses the item recommendation regime and the methods do not translate to the link prediction setting.
Chang & Blei (2009) develop a relational topic model for link prediction in document graphs. Wang et al. (2017) extend this work by incorporating deep learning under the framework of Bayesian deep learning (Wang et al., 2015; Wang & Yeung, 2016; 2020). In social networks, learningbased methods and proximity-based methods are leveraged (Wang & Li, 2013; Li et al., 2017). Masrour et al. (2020) study ﬁlter bubbles in link prediction and propose a method to recommend more diverse links. Citation recommenders use paper data and metadata for training (Beel et al., 2016; Ma et al., 2020). Some systems use local citation contexts to improve predictions (Wang et al., 2020a; Haruna et al., 2017). In contrast, our goal in this work is to augment existing models such that they account for exposure bias during both training and evaluation.
Addressing feedback loops in RSs, Chaney et al. (2018) and Mansoury et al. (2020) use simulations to demonstrate that they can arise, amplifying popularity bias and user homogeneity. Sun et al. (2019) present several matrixfactorization-based debiasing algorithms to prevent feedback loops. Sinha et al. (2016) propose a method to identify the items affected by feedback loops and recover the user’s intrinsic preferences. Jiang et al. (2019) show that feedback loops can create echo-chambers and ﬁlter bubbles. Zhao et al. (2017) show that models amplify biases in training data and propose a constraint-based method to mitigate this. In contemporaneous work, Wang & Russakovsky (2021) extend this work and propose another metric for measuring bias and empirically show that it disentangles the direction of ampliﬁcation.
3. Estimating Risk under Exposure Bias
Notation. Our dataset is a directed graph G(V, E), where V = {v1, . . . , vn} is the set of n nodes and E is the set of edges, s.t. (i, j) ∈ E denotes a link from vi to vj. We denote by U = {(i, j) : i ∈ [n], j ∈ [n], s.t. i = j} the possible (including missing) links in the graph; by πij the

Correcting Exposure Bias for Link Recommendation

propensity, i.e., the probability that vi is exposed to vj; and by yij the link probability, i.e., the probability that vi links to vj conditional on exposure to vj. The binary random variable oij represents if vi links to vj assuming exposure to vj; the binary random variable aij represents if vi is exposed to vj; and the binary random variable oij representing if vi links to vj. Thus the data generating process for G(V, E) is as follows: ∀(i, j) ∈ U, we have

oij ∼ Ber(yij), aij ∼ Ber(πij),
oij = oij aij ,

where Ber(.) is the Bernoulli distribution. The predicted link probability is yij and the estimated propensity is πij. The predicted link outcome is oij = 1(yij ≥ 0.5). As an example, consider a citation graph. Here, each vi is an academic paper, πij is the probability that authors of vi are exposed to vj, and yij is the probability that vi cites vj conditional on exposure to vj.
Deﬁnition 1 (True Risk). This is the risk of the predictions y on the graph that would have been generated if all nodes were exposed to all other nodes, i.e., if ∀ (i, j) ∈ U , πij = 1. The true risk is deﬁned as





1

R(y) = Eo  |U |

δ(oij , yij )

(i,j)∈U

1

= |U |

[yijδ(1, yij) + (1 − yij)δ(0, yij)],

(i,j)∈U

where δ is some loss function (for example, log-loss).

True risk is different from the risk of the predictions on the observed graph as some relevant links are missing due to a lack of exposure. Thus the performance of an RS should be evaluated based on the true risk since it correctly accounts for relevant but low-exposure nodes.
In order to compare the biases and variances of the estimators we propose, we make Assumption 1 in this section. All proofs for this section are in Appendix A.
Assumption 1. The loss function δ satisﬁes the following:
1. It only depends on the predicted binary outcome, i.e., δ(oij , yij ) = δ(oij , oij ),
2. δ(0, 0) = δ(1, 1) = 0, and 3. δ(0, 1) = δ(1, 0) := ∆.

Lemma 3.1. The bias and variance of Rnaive(o) are

B(Rnaive) = E[Rnaive] − R(o)

∆

= |U |

yij(1 − πij)(1 − 2oij) ,

(i,j)∈U

∆2

Var(Rnaive) = |U |2

yij πij (1 − yij πij ).

(i,j)∈U

Lemma 3.1 shows that Rnaive is a biased estimator of the true risk. Rnaive will be unbiased only if either all nodes are exposed to all the others, i.e., if ∀(i, j) ∈ U , πij = 1, or if all nodes are irrelevant to all the others, i.e., if ∀(i, j) ∈ U , yij = 0. Thus evaluating an RS using Rnaive can be misleading. We propose three estimators that leverage learned propensities π and link probabilities y to weight the examples to correct for this bias.

Estimator Rw. The ﬁrst estimator we propose is

1

Rw(y, π) = |U |

wijδ(oij, oij), where (1)

(i,j)∈U

wij = oij + (1 − oij )ψij , ψij = 1 − yij . (2)

πij

1 − πij yij

In Rw, the positive examples are up-weighted according to the inverse propensity. The negative examples are downweighted (as ψij ≤ 1). Intuitively, this weighting corrects for the fact that, in the observed graph, some positive examples are observed as negative examples since the nodes are exposed according to the propensities π.
Lemma 3.2. The bias and variance of Rw are

∆ B(Rw) =

(1 − oij )yij 1 − πij +

|U | (i,j)∈U πij

oij (1 − yij − (1 − yij πij )ψij ) , (3)

∆2

Var(Rw) = |U |2

yij πij (1 − yij πij )vij ,

(i,j)∈U

where vij = 1 − oij + oij ψ2 .

πi2j

ij

Naive Estimator. One approach to estimating the true risk is to directly use the observed graph. We call this the naive estimator. It is deﬁned as

1

Rnaive(y) = |U |

δ(oij , oij ).

(i,j)∈U

Lemma 3.2 shows that Rw will be unbiased if the propensities and link probabilities are estimated correctly, i.e., if ∀(i, j) ∈ U , πij = πij and yij = yij. We later derive
sufﬁcient conditions for when Rw will have lower bias than Rnaive even if π and y are incorrectly estimated.

Correcting Exposure Bias for Link Recommendation

Estimator RPU. We adapt an unbiased estimator proposed by Bekker et al. (2019) for the positive-and-unlabeled (PU) setting. The idea is to remove an appropriate number of negative examples for each positive example. We have

1

RPU(y, π) = |U |

wij δ(oij , oij ) + wij δ(0, oij ) ,

(i,j)∈U

oij

1

where wij = πij + (1 − oij ), wij = oij

1− πij

.

We weight the positive examples by the inverse propensity and for each positive example, remove a negative example weighted by |wij|.
Lemma 3.3. The bias and variance of RPU are

∆ B(RPU) = |U | yij
(i,j)∈U

1 − πij πij

(1 − 2oij) ,

∆2

yij πij (1 − yij πij )

Var(RPU) = |U |2 (i,j)∈U πi2j .

RPU will be unbiased when ∀(i, j) ∈ U , πij = πij.

Estimator RAP. RAP adds positive examples for each negative example. It is deﬁned as

1

RAP(y, π) = |U |

wij δ(oij , oij ) + wij δ(1, oij ) ,

(i,j)∈U

where wij = oij + (1 − oij )ψij , wij = (1 − oij )τij , τij = yij (1 − πij ) . 1 − πij yij

Lemma 3.4. The bias and variance of RAP are

Assumption 2. For the graph G(V, E) with n nodes, the number of edges from each node is O(1). Thus the number of positive links |E| ∈ O(n). And the number of negative links (|U| − |E|) ∈ O(n2). Thus the number of negative links is much greater than the number of positive links for a large n. If the predictions y are close to the true values, we would expect the number of negative predictions (o = 0) to also be much larger than the number of positive predictions (o = 1). So we assume that the contribution of positive predictions to the bias is negligible.
Let U = U \ E. Under Assumption 2, the biases are

∆

B(Rnaive) ≈ |U |

yij (1 − πij ) ,

(i,j)∈U

∆ B(Rw) ≈ B(RPU) ≈

yij 1 − πij ,

|U | (i,j)∈U πij

∆

B(RAP) ≈ |U |

[(1 − πij )yij − (1 − πij yij )τij ] .

(i,j)∈U

Theorem 3.2 (Comparison of Biases). Under these approximations, a sufﬁcient condition for B(Rw) = B(RPU) < B(Rnaive) is
πij < πij < 1, ∀(i, j) ∈ U , 2 − πij

and for B(RAP) < B(Rnaive) is
πij < πij < 1 and 0 < yij < cyij, ∀(i, j) ∈ U 2 − πij where c = 2(1 − πij) ≥ 1.
1 − πij − πij yij + (2 − πij )πij yij

∆

B(RAP) = |U |

(1 − oij)[(1 − πij)yij−

(i,j)∈U

(1 − πij yij )τij ] + oij (1 − yij − (1 − yij πij )ψij ) ,

∆2

2

Var(RAP) = |U |2

yij πij (1 − yij πij )ψij ,

(i,j)∈U

where ψ is deﬁned in Eq. 2.

RAP is unbiased if ∀(i, j) ∈ U , πij = πij and yij = yij.
Theorem 3.1 (Comparison of Variances). For all values of π, y, we have Var(RAP) < Var(Rnaive), and Var(RAP) < Var(Rw) < Var(RPU).

In order to compare the biases, we make the following simplifying assumption.

Thus, if π are not too-underestimated and y are not toooverestimated, the proposed estimators will have lower bias than the naive estimator.
4. Learning Propensities and Link Probabilities
The previous section assumes known propensities (π) and link probabilities (y). We present a loss function that uses our proposed estimators from Section 3 to learn π and y. A natural approach might be to minimize the negative loglikelihood of the observed data:
π, y = argmin L(o|y, π),
π,y
where L(o|y, π) = (i,j)∈U −oij log(yij πij ) − (1 − oij) log(1−yijπij). However, this might not ensure that the

Correcting Exposure Bias for Link Recommendation

true risk remains small. We derive a generalization bound that motivates a different loss function (see Appendix B for the proof).
Deﬁnition 2 (Rademacher Complexity). Let F be a class
of functions (π, y). Each estimator R ∈ Rw, RPU, RAP can be written as |U1| (i,j)∈U r(oij , πij , yij ) for an appropriate function r (e.g. by Eq. 2, for Rw, we have r(oij, πij, yij) = wijδ(oij, yij)) . For R ∈
Rw, RPU, RAP , we deﬁne a quantity analogous to the Empirical Rademacher Complexity (Bartlett & Mendelson, 2002) as





1

Go(F , R) = Eσ (πs,yu)p∈F |U |

σij r(oij , πij , yij ) ,

(i,j)∈U

where σij are independent Rademacher random variables. And the Rademacher Complexity is G(F , Rw) = Eo[Go(F , Rw)].

Go(F , Rw) can be estimated from the data by taking a random sample of the variables σij and optimizing the above objective. Next, we present a generalization bound based on Go(F , Rw).
Theorem 4.1 (Generalization Bound). Let F be a class of functions (π, y). Let δ(oij, yij) ≤ η ∀(i, j) ∈ U and
πij ≥ > 0 ∀(i, j) ∈ U . Then, for R ∈ Rw, RPU, RAP , with probability at least 1 − δ, we have

R(y) ≤ R(y, π) + B(R) + 2G(F, R) + M ≤ R(y, π) + B(Rw) + 2G(F , Rw) + 3M,

where M = 42η|U2| log( 2δ ) and B(R) is the bias of R derived in Section 3.

Loss Function. The bound shows that R ∈ {Rw, RPU, RAP} is close to the true risk R. This suggests that we should choose π, y that lead to small values of R as this will also minimize the true risk with high probability. This motivates us to learn π, y by minimizing the following objective:

π, y = argmin L(o|y, π), subject to R(π, y) ≤ c,
π,y

where R ∈ {Rw, RPU, RAP} and c > 0 is some constant. In practice, we minimize the following relaxed version of this objective:

l(π, y) = λLL(o|π, y) + λRR(π, y),

(4)

where λR and λL are hyperparameters. One might try to minimize the loss function using only R by setting λL = 0.

This will not work because trivial solutions exist for all three risk functions: if ∀(i, j) ∈ U , yij = 1, then Rw(y, π) = 0; if ∀(i, j) ∈ U , πij = 1, yij > 0.5, then RPU = RAP = 0. Hence, we need to use λL > 0 during training to prevent the model from collapsing to these solutions. It is possible use parametric models like neural networks for y and π to incorporate information associated with the nodes (like user data or paper data). The parameters can be learned by using gradient-based methods by minimizing the loss in Eq. 4.
5. Feedback Loops
In this section, we analyze what happens when we train an RS repeatedly on data generated by users interacting with that system’s recommendations. We show that for an RS that does not account for exposure bias, the fraction of high-propensity nodes that are recommended continually increases over time. In other words, the system will progressively recommend fewer low-propensity nodes, even if they are relevant, as time goes on. Next, we show that correcting for exposure bias ensures that relevant low-propensity nodes keep being recommended. In this section, we assume that the attributes of the nodes take values in a discrete set.
Assumption 3. Each node belongs to one of C categories from the set C = {c1, . . . , cC }. Each category contains n nodes. V = {v1, . . . , vN } is the set of nodes and N = nC. The function γ : V → C maps a node to its category. The link probability yij and propensity πij depend only on the categories of the nodes, i.e., yij = ylm and πij = πlm if γ(vi) = γ(vl) and γ(vj) = γ(vm). Therefore, for any pair of nodes (vi, vj), the product πijyij depends only on the categories vi and vj belong to. Let quv = πijyij for some vi, vj s.t. γ(vi) = cu and γ(vj) = cv.
Iterative Training Process. We now describe the iterative training process for an RS that does not account for exposure bias. We will restrict our attention to analyzing the recommendations made for the n nodes in some category cu ∈ C. We assume that we make one recommendation for each node (this simpliﬁes exposition but is not necessary). At time step t, the fraction of nodes recommended from each category is represented by the (C − 1)-simplex κ(t). So out of the n nodes from cu, nκ(vt) of them are recommended a node from the category cv, where κ(vt) is the vth element of κ(t). Links are generated from the recommended nodes according to ground-truth propensities and link probabilities. Thus a node from cu creates a link to a recommended node from cv with probability quv. Since we are examining recommendations for category cu, we will drop the subscript u going forward, i.e., qv = quv. This gives us training data for the next iteration. We assume that a node only creates a link to nodes from the recommended nodes. In other words, links are not created to nodes that are not recommended.

Correcting Exposure Bias for Link Recommendation

The number of nodes linked to from category cv at time t is n(vt). Then n(vt) ∼ Binomial(nκ(vt), qv). During training,
the link probability is estimated as qv(t) = nn(vt) . We assume that, at time step t + 1, nodes from category cv are recommended with probability proportional to qv(t). This is akin to

recommending with some exploration (Kawale et al., 2015).

Let the (C − 1)-simplex denoting normalized estimates

be e(t+1) =

q1(t) , q2(t) , . . . , qC(t) , where S =

SS

S

C j=1

qj(t).

Thus the recommendations for the next step κ(t+1) have the distribution κ(t+1) ∼ n1 Multinomial(n, e(t+1)). This process is repeated at each time step. The initial training
data is generated by the user generating links according to
the ground-truth propensities and link probabilities.

Example 2. We illustrate the iterative training process with

a minimal example. Let C = {c1, c2}. We examine the rec-

ommendations made to nodes in c1. Let n = 100, q1,1 = 0.8 and q1,2 = 0.4. At time t, let κ(t) = [0.6, 0.4]. Infor-

mally, 60 of the recommended nodes are from c1 and the

remaining 40 from c2. The nodes create links to the rec-

ommended nodes with probabilities q1,1 and q1,2. There-

fore, the number of nodes linked that belong to c1 at

time t is n(1t) ∼ Binomial(60, q1 = 0.8) and similarly,

n(2t) ∼ Binomial(40, q2 = 0.4). Informally, the realized

values are n(1t) = 48 and n(2t) = 16. The estimated link

probabilities are q1(t)

=

0.

48,

q

(t 2

)

=

16 100

=

0.16 and

e(t+1) = 00..4684 , 00..1664 = [0.75, 0.25]. Then, at time t + 1,

we recommend nodes according to e(t+1), i.e., κ(t+1) ∼

1100 Multinomial(100, e(t+1)). The realized value of κ(1t+1) is likely to be greater than κ(1t). Thus more items from c1 are
likely to be recommended at time t+1 as compared to time t.

This provides some intuition for the existence of a feedback

loop: nodes that are linked less are in turn recommended

with a lower probability in the next time step.

We formally show the existence of feedback loops (see Appendix C for the proofs). We prove a ﬁnite-sample result which shows that, with high probability, the relative probability of recommending nodes from categories with higher values of qj keeps increasing over time.
Theorem 5.1. Suppose that qv > qw if v > w. Let κ(vtw) = κ(vt) . Let A(vtw) represent the event that relative fraction
κ(vt) +κ(wt)
of recommendations from cv to that from cw increases at time t, i.e., κ(vtw+1) > κ(vtw) . Let A(t) be the event that all relative fractions get skewed towards cv from cw if qv > qw, i.e., A(t) = (v,w)∈S A(vtw) , where S = {(v, w) : v ∈ [C], w ∈ [C], v > w}. Then, for constants , η > 0 that

only depend on κ(t) and q, we have

P(A(t)|κ(t)) ≥ 1 − 2C exp ≥ 1 − 2C exp

−2n

2 + η2 C2

n −O C2 .

Corollary 5.1. lim P(A(t)|κ(t)) = 1 if C3 ∈ o(n).
n→∞

Thus, at each time step, with high probability, nodes with low propensity are less likely to be recommended in the next time step. Therefore, if an RS does not correct for exposure bias, over time, even relevant nodes with low propensity are unlikely to be recommended. Next, we derive and analyze the rate at which the exposure bias exacerbates. Theorem 5.2. Suppose that qv > qw. As n → ∞, κ(vtw) →p 1 − 1+1ct , where c = qqwv .

Theorem 5.2 shows that the rate at which the bias exacerbates is dependent on the ratio qqwv . Therefore, the lower the propensity, the faster the probability of that node being
recommended reduces.
Corollary 5.2. Let ycucv = yij for some (i, j) s.t. u = γ(i) and v = γ(j) (γ is deﬁned after Assumption 3). We now assume that we have a consistent estimator qv(t) →p κ(vt)ygugv , where κ(vt) is the vth element of the simplex κ(t). Thus qv(t) is an estimator that negates the effect of exposure bias. As n → ∞, κ(vtw) →p 1 − 1+1ct , where c = yygguuggwv .

This shows that accounting for exposure bias can alleviate the feedback loop. Despite having low propensity, relevant papers will continue to be recommended.

6. Experiments
We validate our link recommendation methods on the task of citation recommendation. Given an input paper’s data (like title, abstract, etc.), the goal is to recommend papers that it should cite. We use the Microsoft Academic Graph (MAG) dataset (Sinha et al., 2015). MAG is a graph containing scientiﬁc papers and the citation relationships between them. It also contains the titles, abstracts, and FOS of the papers. In our experiments, we use subgraphs from the MAG by performing a breadth-ﬁrst search from some root node. For each paper, we concatenate the title and abstract and generate a 768-dimensional embedding for the text using the bert-as-service library (Xiao, 2018). We use a SciBERT model (Beltagy et al., 2019), which is a BERT model trained on scientiﬁc text, with this library. For each paper pi, we generate the embedding hi ∈ R768. The FOS in MAG are organised as a tree, where a child is a sub-ﬁeld of its parent. We only use the root-level FOS for each paper and there are 19 such FOS. We use Amazon Sagemaker (Liberty et al., 2020) to run our experiments.

Correcting Exposure Bias for Link Recommendation

Table 1. Evaluation metrics on the test set of the semi-synthetic data computed against known ground truth citation links.

MODEL
NO PROP. MLE Rw RPU RAP

PREC.
67.24 81.04 83.28 82.16 83.01

REC.
54.81 60.19 63.73 63.07 65.54

AUC
84.45 93.12 96.42 94.28 95.38

MAP
41.87 56.77 56.96 58.01 59.90

For simplicity, we assume that the propensities πij depend only on the FOS of papers pi and pj. Thus the propensity model is parameterized by θπ ∈ [0, 1]19×19. However, our methods can easily extend to more complicated parametric propensity estimators like neural networks. To model the link probability yij, we use the following model:

yij = σ(w (hi hj) + b),

(5)

where w ∈ R768 and b ∈ R are trainable parameters, is an element-wise product, and σ is the sigmoid function. We
use stochastic gradient descent to learn θπ, w and b using the loss function described in Eq. 4 with δ as the log-loss, i.e., δ(u, u) = −u log(u) − (1 − u) log(1 − u). For training, we use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10−4 and a batch size of 32.

6.1. Semi-Synthetic Dataset
Since we do not have ground truth exposure values in the MAG dataset, we cannot know whether a paper was not cited due to a lack of exposure or due to irrelevancy. As a result, we construct a semi-synthetic dataset with simulated propensity scores and link probabilities. We use a subset of 41,600 papers. We generate train-test-validation splits by taking a topological ordering of the nodes and use the subgraph created from the ﬁrst 70% for training, next 10% for validation, and the remaining 20% for testing. We use the real text and FOS for each paper. The simulated propensity matrix π is a 19 × 19 matrix with its diagonal and off-diagonal entries initialized from U (0.7, 1) and U (0.1, 0.3), respectively, where U (.) is the uniform distribution. The link probability is simulated using yij = σ(w (hi hj) + b), where σ is the sigmoid function, is element-wise product, and w, b are ﬁxed known vectors.
We show the evaluation metrics for ﬁve models on the test set computed against the simulated true citations (not the observed citations) (Table 1). No Prop is the model trained naively on the observed data using only the output model in Eq. 5. MLE is the model trained using the loss function in Eq. 4 with λR = 0. The remaining three are models trained using Rw, RPU, and RAP with λR = 10 and λL = 1. We see that all other estimators signiﬁcantly outperform No Prop.

Table 2. RMSE of the estimated risk with respect to the true risk computed using our proposed estimators. The ﬁrst column shows the risk used in the loss function in Eq. 4 to learn π and y.

TRAINED USING
NO PROP. MLE Rw RPU RAP

ESTIMATOR USED RNAIVE Rw RPU RAP

1.50

-

-

-

0.67 0.23 0.24 0.32

0.43 0.04 0.10 0.11

0.38 0.05 0.11 0.04

0.41 0.06 0.08 0.03

0 0
5
10
15

5

10

15

0 0 0.9

0.8 5
0.7

0.6

0.5

10

0.4

0.3 15 0.2

(a) Ground Truth

5

10

15

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

(b) Estimated

Figure 1. The estimated propensities propensities are close to the true simulated values when learned using Rw.

Additionally, our proposed estimators lead to improved performance over the MLE. We emphasize that these metrics are computed against true citations and thus are a measure of true risk which is the appropriate metric for evaluating an RS’s performance. This shows the utility of accounting for exposure bias and learning using our proposed loss function.
In this work, we tackle two separate (but related) challenges. The ﬁrst challenge is learning link probabilities in such a way that they are not underestimated due to exposure bias. The second challenge is evaluating a RS given learned link probabilities and propensity scores, i.e., computing a good estimate of the true risk. We demonstrate the efﬁcacy of our methods for the second challenge and show that our proposed weighting schemes lead to good estimates of the true risk (Table 2). We show the RMSE of the risk estimated using the proposed estimators with respect to the true risk. The ﬁrst column denotes risk function used to train the model (as described in Section 4). The rest of the columns denote the estimators used to estimate true risk using the learned propensities and link probabilities from the trained model in the ﬁrst column. We trained each model 10 times to compute the RMSE. The RMSE estimated using Rnaive is always greater than that of the other estimators, which shows that leveraging the learned propensities leads to substantially better estimates of the true risk (and thus more accurately evaluates the RS). The RMSE when trained using the MLE is higher than when trained using our proposed estimators, showing the beneﬁt of our proposed estimators over the

Correcting Exposure Bias for Link Recommendation

Table 3. Evaluation metrics for various models computed on the test sets of the two real-world citation datasets.

MODEL PREC. REC. F1 AUC MAP DATASET 1

NO PROP. MLE Rw RPU RAP

29.45 30.24 31.46 30.98 36.07

DATASET 2

78.30 77.84 78.02 78.94 76.08

42.81 43.56 44.84 44.49 48.94

84.44 84.41 84.74 85.24 84.67

24.10 24.60 25.60 25.11 28.58

NO PROP. MLE Rw RPU RAP

44.86 44.43 48.70 42.17 47.22

70.85 74.66 71.62 76.15 71.84

54.94 55.71 57.98 54.28 56.98

83.22 84.97 83.90 85.43 83.89

33.19 34.39 36.25 33.26 35.27

MLE. This also qualitatively validates the generalization bound proved in Section 4 by showing that minimizing R ∈ {Rw, RPU, RAP} also leads to small values of the true risk.
The heatmap of simulated propensities and estimated propensities when using Rw shows that the estimated propensities are close to the true propensities (Figure 1). The mean relative error between the true and estimated propensities is 19.47%, demonstrating that the training procedure recovers the propensities. Together, these results show that our methods successfully mitigate exposure bias in this dataset.
6.2. Real-World Datasets
We now evaluate our proposed method on a real-world citation network. We construct two datasets by using disjoint subgraphs of the MAG. The ﬁrst generated dataset has 2,442,008 papers and 7,577,886 edges. The second dataset has 1,328,664 papers and 1,469,899 edges. Thus the second graph is sparser than the ﬁrst one. The FOS distribution is also different in both datasets (see details in Appendix D). We use 70-10-20% train-validation-test splits generated similarly to the semi-synthetic dataset. We do not have access to true exposure values and thus we evaluate our methods against the observed citation links.
We show the evaluation metrics for the proposed estimators (Table 3). Since we do not have access to the true citation links in the real dataset, we compute these metrics over the observed links. In other words, this is a measure of the naive risk. We see that our proposed estimators achieve comparable metrics to No Prop. For both datasets, the best numbers for each metric are achieved by estimators other than No Prop. Moreover, the models using the weighted

% of recos from sam e FOS % of recos from sam e FOS

95

90

85

80

field 1

field 2

0 2 4 6 8 10 12 14 16 18 20 Num ber of iterations

(a) No Propensity

82

80

field 1

78

field 2

76

74

0 2 4 6 8 10 12 14 16 18 20 Num ber of iterations

(b) With Propensity (Rw)

Figure 2. The fraction of recommended papers from the same FOS over time.

estimators outperform the MLE estimator in both datasets. These results show that our proposed estimators achieve comparable performance even when evaluated on the observed citation data. Similarly, Table 4 shows link prediction metrics for various models computed against observed citations. Recall@100 refers to the recall in the top 100 recommendations averaged across all papers in the test set. Mean Rank is the mean rank of the cited papers averaged across all the papers. Entropy@100 of True Positives is the entropy in the FOS of the true positives in the top 100 recommendations for each paper; we use it to measure the diversity in the FOS of the recommendations. Our proposed estimators achieve comparable Recall@100 and Mean Rank to No Prop for both datasets. As expected, propensity based estimators have higher FOS entropy scores than No Prop, with Rw achieving the highest FOS entropy in both datasets. Thus our proposed estimators recommend more relevant papers from different FOS and still maintain comparable performance to No Prop.
At ﬁrst blush, the comparable performance to No Prop may not seem compelling. However, this is a strong result. Our goal is to correct exposure bias and minimize true risk, not observed (or naive) risk. Since Tables 3 and 4 are computed against observed citations, our proposed methods should not be expected to outperform No Prop as they are not trying to optimize metrics against the observed links. In Section 6.1, we showed that our methods correct exposure bias and achieve lower true risk. Coupled with those results, our goal in this section was to show that our methods do not lower performance even if evaluated against the standard evaluation metrics. We suspect that the negative log-likelihood term L(o|π, y) in Eq. 4 is likely responsible for the comparable performance against the observed risk. This is because, as seen from the results, the MLE also performs well as compared to No Prop..
6.3. Feedback Loops
We run simulations to examine what happens when a citation recommender is trained repeatedly on data collected from users interacting with its recommendations. We use

Correcting Exposure Bias for Link Recommendation

Table 4. Link prediction metrics for various models when evaluated on the test set of the real datasets.

MODEL

RECALL @100

DATASET 1

MEAN RANK

ENTROPY@100 TRUE POSITIVES

NO PROP. 24.39 2247.27

1.65

MLE

24.70 2891.40

1.73

Rw

25.03 2836.73

1.74

RPU

24.61 2875.13

1.73

RAP

26.66 2425.51

1.71

DATASET 2

NO PROP. 6.32 10170.26

1.06

MLE

6.07 10731.88

1.08

Rw

6.30 10873.19

1.12

RPU

5.92 10717.06

1.08

RAP

5.99 10801.11

1.10

7. Conclusion
Proposing three estimators to correct for exposure bias, we derive sufﬁcient conditions for when they exhibit lower bias than the naive estimator and incorporate them into a learning procedure. Theoretically, we prove that feedback loops can worsen exposure bias. Empirically, we show that proposed estimators improve performance against the true link probabilities, leading to better estimates of true risk, and combating feedback loops. Our methods can be extended to RSs that use different propensity or link probability models. Using domain knowledge (e.g., through graphical models) to improve propensity learning and empirically evaluating our methods in other link recommendation tasks are promising future directions. Exposure bias in link recommendation also raises fairness concerns. For example, in citation recommendation, certain authors or institutions might get unfair exposure which can be worsened by the RS. Investigating exposure bias correction methods for providing fairer recommendations would also be interesting future work.

the iterative training procedure described in Section 5. We construct a training set of 410 papers from the MAG, with their corresponding real FOS and text embeddings. For ease of exposition, we use an arbitrary but ﬁxed mapping to map the 19 FOS to two FOS. The synthetic propensities and link probabilities are simulated similarly to Sec 6.1. In the ﬁrst iteration, the models are trained using the observed citation network. For subsequent training iterations, the training data is generated as follows. For each paper, we recommend 20 papers. The probability of recommending a paper is proportional to its estimated citation probability. We then simulate the user’s interaction with the recommendations according to the known simulated propensities and citation probabilities. This generates the training set for the subsequent iteration. We then repeat this process.
We show how the fraction of recommended papers from the same FOS changes over multiple training iterations for models trained without propensity, i.e., No Prop and the model trained using Rw (Figure 2). We plot this time series for both FOS in our dataset. For No Prop, the fraction of papers recommended from the same FOS increases over time for both FOS (Figure 2a). This demonstrates the existence of a feedback loop that worsens exposure bias and reduces the number of papers recommended from a different FOS over time. On the other hand, when we train our models using Rw, the feedback loop no longer exists and the fraction of papers recommended from a different FOS remains stable over time (Figure 2b). This shows that our proposed estimator continues to recommend relevant papers from a different FOS and corrects the feedback loop.

References
Bai, X., Wang, M., Lee, I., Yang, Z., Kong, X., and Xia, F. Scientiﬁc paper recommendation: A survey. IEEE Access, 2019.
Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 2002.
Beel, J., Gipp, B., Langer, S., and Breitinger, C. Paper recommender systems: a literature survey. International Journal on Digital Libraries, 2016.
Bekker, J., Robberechts, P., and Davis, J. Beyond the selected completely at random assumption for learning from positive and unlabeled data. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2019.
Beltagy, I., Lo, K., and Cohan, A. Scibert: Pretrained language model for scientiﬁc text. In Empirical Methods in Natural Language Processing, 2019.
Branda˜o, M. A., Moro, M. M., Lopes, G. R., and Oliveira, J. P. Using link semantics to recommend collaborations in academic social networks. In Proceedings of the 22nd International Conference on World Wide Web, 2013.
Chaney, A. J., Stewart, B. M., and Engelhardt, B. E. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In ACM Conference on Recommender Systems, 2018.
Chang, J. and Blei, D. Relational topic models for document networks. In Artiﬁcial Intelligence and Statistics, 2009.

Correcting Exposure Bias for Link Recommendation

Chen, J., Dong, H., Wang, X., Feng, F., Wang, M., and He, X. Bias and debias in recommender system: A survey and future directions. arXiv preprint arXiv:2010.03240, 2020.
Haruna, K., Akmar Ismail, M., Suhendroyono, S., Damiasih, D., Pierewan, A. C., Chiroma, H., and Herawan, T. Context-aware recommender system: A review of recent developmental process and future research direction. Applied Sciences, 2017.
Jeunen, O. Revisiting ofﬂine evaluation for implicitfeedback recommender systems. In ACM Conference on Recommender Systems, 2019.
Jiang, R., Chiappa, S., Lattimore, T., Gyo¨rgy, A., and Kohli, P. Degenerate feedback loops in recommender systems. In AAAI/ACM Conference on AI, Ethics, and Society, 2019.
Joachims, T., Swaminathan, A., and Schnabel, T. Unbiased learning-to-rank with biased feedback. In ACM International Conference on Web Search and Data Mining, 2017.
Kawale, J., Bui, H. H., Kveton, B., Tran-Thanh, L., and Chawla, S. Efﬁcient thompson sampling for online matrix-factorization recommendation. In Advances in Neural Information Processing Systems, 2015.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Li, Z., Fang, X., and Sheng, O. R. L. A survey of link recommendation for social networks: methods, theoretical foundations, and future research directions. ACM Transactions on Management Information Systems (TMIS), 2017.
Liang, D., Charlin, L., and Blei, D. M. Causal inference for recommendation. In Causation: Foundation to Application, Workshop at UAI. AUAI, 2016a.
Liang, D., Charlin, L., McInerney, J., and Blei, D. M. Modeling user exposure in recommendation. In International Conference on World Wide Web, 2016b.
Liberman, S. and Wolf, K. B. Independent simultaneous discoveries visualized through network analysis: the case of linear canonical transforms. Scientometrics, 2015.
Liberty, E., Karnin, Z., Xiang, B., Rouesnel, L., Coskun, B., Nallapati, R., Delgado, J., Sadoughi, A., Astashonok, Y., Das, P., et al. Elastic machine learning algorithms in amazon sagemaker. In ACM SIGMOD International Conference on Management of Data, 2020.
Ma, S., Zhang, C., and Liu, X. A review of citation recommendation: from textual content to enriched context. Scientometrics, 2020.

Ma, W. and Chen, G. H. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. In Advances in Neural Information Processing Systems, 2019.
Mansoury, M., Abdollahpouri, H., Pechenizkiy, M., Mobasher, B., and Burke, R. Feedback loop and bias ampliﬁcation in recommender systems. arXiv preprint arXiv:2007.13019, 2020.
Masrour, F., Wilson, T., Yan, H., Tan, P.-N., and Esfahanian, A. Bursting the ﬁlter bubble: Fairness-aware network link prediction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020.
Schnabel, T., Swaminathan, A., Singh, A., Chandak, N., and Joachims, T. Recommendations as treatments: debiasing learning and evaluation. In International Conference on Machine Learning, 2016.
Shalev-Shwartz, S. and Ben-David, S. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Sinha, A., Shen, Z., Song, Y., Ma, H., Eide, D., Hsu, B.-J., and Wang, K. An overview of microsoft academic service (mas) and applications. In International Conference on World Wide Web, 2015.
Sinha, A., Gleich, D. F., and Ramani, K. Deconvolving feedback loops in recommender systems. In Advances in Neural Information Processing Systems, 2016.
Sun, W., Khenissi, S., Nasraoui, O., and Shafto, P. Debiasing the human-recommender system feedback loop in collaborative ﬁltering. In Companion Proceedings of The 2019 World Wide Web Conference, 2019.
Swaminathan, A. and Joachims, T. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, 2015.
Vincenot, C. E. How new concepts become universal scientiﬁc approaches: insights from citation network analysis of agent-based complex systems science. The Royal Society B: Biological Sciences, 2018.
Wang, A. and Russakovsky, O. Directional bias ampliﬁcation. arXiv preprint arXiv:2102.12594, 2021.
Wang, H. and Li, W.-J. Online egocentric models for citation networks. In Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013.
Wang, H. and Yeung, D.-Y. Towards bayesian deep learning: A framework and some existing methods. TDKE, 28(12): 3395–3408, 2016.

Correcting Exposure Bias for Link Recommendation
Wang, H. and Yeung, D.-Y. A survey on bayesian deep learning. ACM Computing Surveys (CSUR), 53(5):1–37, 2020.
Wang, H., Wang, N., and Yeung, D. Collaborative deep learning for recommender systems. In KDD, pp. 1235– 1244, 2015.
Wang, H., Shi, X., and Yeung, D.-Y. Relational deep learning: A deep latent variable model for link prediction. In Association for the Advancement of Artiﬁcial Intelligence, 2017.
Wang, J., Zhu, L., Dai, T., and Wang, Y. Deep memory network with bi-lstm for personalized context-aware citation recommendation. Neurocomputing, 2020a.
Wang, L., Bai, Y., Sun, W., and Joachims, T. Fairness of exposure in stochastic bandits. arXiv preprint arXiv:2103.02735, 2021.
Wang, Y., Liang, D., Charlin, L., and Blei, D. M. Causal inference for recommender systems. In ACM Conference on Recommender Systems, 2020b.
Xiao, H. bert-as-service. https://github.com/ hanxiao/bert-as-service, 2018.
Yang, L., Cui, Y., Xuan, Y., Wang, C., Belongie, S., and Estrin, D. Unbiased ofﬂine recommender evaluation for missing-not-at-random implicit feedback. In ACM Conference on Recommender Systems, 2018.
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. Men also like shopping: Reducing gender bias ampliﬁcation using corpus-level constraints. arXiv preprint arXiv:1707.09457, 2017.

Correcting Exposure Bias for Link Recommendation
A. Bias and Variance
Lemma A.1. Let X ∼ Bernoulli(θ) and Y = aX + b(1 − X), where a and b are some constants. Then Var(Y ) = θ(1 − θ)(a − b)2.

Proof of Lemma 3.1 Lemma. The bias and variance of Rnaive(o) are

B(Rnaive) = E[Rnaive] − R(o)

∆

= |U |

yij(1 − πij)(1 − 2oij) ,

(i,j)∈U

∆2

Var(Rnaive) = |U |2

yij πij (1 − yij πij ).

(i,j)∈U

Proof. We have

1

Rnaive(o, y) = |U |

δ(oij , oij )

(i,j)∈U

1

= |U |

[oijδ(1, oij) + (1 − oij)δ(0, oij)]

(i,j)∈U

1

∴ Eo[Rnaive(o, y)] = |U |

[yij πij δ(1, oij ) + (1 − yij πij )δ(0, oij )]

(i,j)∈U

1

= |U |

[yijπij(1 − oij)δ(1, 0) + (1 − yijπij)oijδ(0, 1)]

(i,j)∈U

∆

= |U |

[yij πij (1 − oij ) + (1 − yij πij )oij ] .

(i,j)∈U

The true risk is

1

R(y) = |U |

[yijδ(1, oij) + (1 − yij)δ(0, oij)]

(i,j)∈U

1

= |U |

[yij(1 − oij)δ(1, 0) + (1 − yij)oijδ(0, 1)]

(i,j)∈U

∆

= |U |

[yij (1 − oij ) + (1 − yij )oij ] .

(i,j)∈U

Thus the bias is

B(Rnaive) = E[Rnaive] − R(o)

∆

= |U |

[yij πij (1 − oij ) + (1 − yij πij )oij − yij (1 − oij ) − (1 − yij )oij ]

(i,j)∈U

∆

= |U |

yij(1 − πij)(1 − 2oij) .

(i,j)∈U

Correcting Exposure Bias for Link Recommendation

The variance is





1

Var(Rnaive) = Var  |U |

[oijδ(1, oij) + (1 − oij)δ(0, oij)]

(i,j)∈U

1

= |U |2

Var (oijδ(1, oij) + (1 − oij)δ(0, oij))

(i,j)∈U

1

2

= |U |2

yij πij (1 − yij πij ) (δ(1, oij ) − δ(0, oij ))

(i,j)∈U

∆2

= |U |2

yij πij (1 − yij πij ).

(i,j)∈U

(using Lemma A.1)

Lemmas 3.2, 3.3, and 3.4 can be proved similarly.

Proof of Theorem 3.1
Theorem (Comparison of Variances). For all values of π, y, we have Var(RAP) < Var(Rnaive), and Var(RAP) < Var(Rw) < Var(RPU)

Proof. First we show that Var(RAP) < Var(Rnaive). We have

∆2

2

Var(RAP) = |U |2

yij πij (1 − yij πij )ψij ,

(i,j)∈U

where ψij = 1 − yij < 1, 1 − πij yij

∆2

Var(Rnaive) = |U |2

yij πij (1 − yij πij ).

(i,j)∈U

Using the fact that ψi2j < 1 ∀(i, j) ∈ U , we get Var(RAP) < Var(Rnaive). Next, we show that Var(Rw) < Var(RPU):

∆2

Var(Rw) = |U |2

yij πij (1 − yij πij )

(i,j)∈U

∆2

yij πij (1 − yij πij )

Var(RPU) = |U |2 (i,j)∈U πi2j

1 − oij + oij ψ2

πi2j

ij

∆2

= |U |2

yij πij (1 − yij πij )

(i,j)∈U

1 − oij oij πi2j + πi2j

∆2

∴ Var(Rw) − Var(RPU) = |U |2

yij πij (1 − yij πij )oij

(i,j)∈U

ψ2 − 1 ij πi2j

∴ Var(Rw) − Var(RPU) < 0

1 because ψij < 1 and πij > 1

∴ Var(Rw) < Var(RPU).

Correcting Exposure Bias for Link Recommendation

Next, we show that Var(RAP) < Var(Rw):

∆2

Var(RAP) − Var(Rw) = |U |2

yij πij (1 − yij πij )(1 − oij )

(i,j)∈U

∴ Var(RAP) − Var(Rw) < 0

1 because ψij < 1 and πij > 1

∴ Var(RAP) < Var(Rw).

ψ2 − 1 ij πi2j

Proof of Theorem 3.2
Theorem (Comparison of Biases). Under the bias approximations, a sufﬁcient condition for B(Rw) = B(RPU) < B(Rnaive) is
πij < πij < 1, ∀(i, j) ∈ U , 2 − πij

and for B(RAP) < B(Rnaive) is

πij < πij < 1 and 0 < yij < cyij, ∀(i, j) ∈ U 2 − πij where c = 2(1 − πij) ≥ 1.
1 − πij − πij yij + (2 − πij )πij yij

Proof. We ﬁrst derive the sufﬁcient condition for B(Rw) = B(RPU) < B(Rnaive). We have

∆

B(Rnaive) ≈ |U |

yij (1 − πij ),

(i,j)∈U

∆ B(Rw) ≈ B(RPU) ≈

yij 1 − πij .

|U | (i,j)∈U πij

If 1 > πij > πij∀(i, j) ∈ U , we have

1 − πij πij

> 0 ∀(i, j) ∈ U

∆ ∴ B(Rw) ≈ B(RPU) ≈

yij 1 − πij .

|U | (i,j)∈U πij

∆ ∴ B(Rw) − B(Rnaive) = |U | yij
(i,j)∈U

πij − πij πij

∆

= |U |

yij πij

(i,j)∈U

1 1−
πij

< 0 (because πij < 1)

∴ B(Rw) < B(Rnaive).

If 0 < πij ≤ πij∀(i, j) ∈ U , we have

1 − πij πij

≤ 0 ∀(i, j) ∈ U

∆ ∴ B(Rw) ≈ B(RPU) ≈

yij πij − 1 . .

|U | (i,j)∈U πij

Correcting Exposure Bias for Link Recommendation

Then, a sufﬁcient condition for B(Rw) = B(RPU) < B(Rnaive) is

yij ∴ yij

πij − 1 < yij(1 − πij) ∀(i, j) ∈ U πij πij − 1 < yij(1 − πij) ∀(i, j) ∈ U πij
2 ∴ πij > 2 − πij ∀(i, j) ∈ U .

Next, we derive the sufﬁcient condition for RAP < Rnaive. Observe that

πij < πij < 1 ∀(i, j) ∈ U 2 − πij

∴ (1 − πij)yij − (1 − πijyij)τij ≥ 0 ∀(i, j) ∈ U

∆ ∴ RAP ≈

[(1 − πij )yij − (1 − πij yij )τij ] , where τij = yij (1 − πij ) .

|U |
(i,j)∈U

1 − πij yij

Therefore, when 2−πiπjij < πij < 1 ∀(i, j) ∈ U , a sufﬁcient condition for RAP < Rnaive is

(1 − πij )yij − (1 − πij yij )τij < yij (1 − πij ) ∀(i, j) ∈ U

∴ 0 < yij <

2(1 − πij) 1 − πij − πij yij + (2 − πij )πij yij

yij ∀(i, j) ∈ U .

B. Generalization Bound
Proof of Theorem 4.1 Theorem (Generalization Bound). Let F be a class of functions (π, y). Let δ(oij, yij) ≤ η ∀(i, j) ∈ U and πij ≥ > 0 ∀(i, j) ∈ U . Then, for R ∈ Rw, RPU, RAP , with probability at least 1 − δ, we have

R(y) ≤ R(y, π) + B(R) + 2G(F, R) + M

(6)

≤ R(y, π) + B(Rw) + 2G(F , Rw) + 3M,

(7)

where M = 42η|U2| log( 2δ ) and B(R) is the bias of R derived in Section 3.

Proof. We proceed similarly to the standard Rademacher complexity generalization bound proof (Shalev-Shwartz & Ben-David, 2014)[Ch. 26]. Observe that

R(y) = R(y) − Eo[R(o, y, π)] + Eo[R(o, y, π)]

≤ B(R) + Eo[R(o, y, π)].

(8)

Let Φ(o) = sup(π,y)∈F Eo[R(o, y, π)] − R(o, y, π) . Then

Eo[R(o, y, π)]| ≤ R(o, y, π) + Φ(o).

(9)

Now we upper bound Φ(o). Since δ(oij, yij) ≤ η ∀(i, j) and πij ≥ > 0, ∀(i, j) and ∀ R ∈ Rw, RPU, RAP , we have 2η
|Φ(o) − Φ(o˜)| ≤ ,

Correcting Exposure Bias for Link Recommendation

if o and o˜ differ in only one coordinate, i.e., oij = o˜ij for some (i, j) ∈ U and olm = o˜lm∀(l, m) ∈ U s.t. (i, j) = (l, m). Using McDiarmid’s Inequality, with probability at least 1 − δ, we have

Φ(o) ≤ E[Φ(o)] + C.

(10)

Next, we upper bound E[Φ(o)]. Let o¯ be a ghost sample independently drawn having the same distribution as o. We have

E[Φ(o)] = Eo sup Eo[R(o, y, π)] − R(o, y, π)
(π,y)∈F

= Eo sup Eo¯ R(o¯, y, π) − R(o, y, π) o
(π,y)∈F





1

1

= Eo (πs,yu)p∈F Eo¯  |U |

r(o¯ij, πij, yij) − |U |

r(oij , πij , yij )

(i,j)∈U

(i,j)∈U

 o







1

1

≤ Eo,o¯ (πs,yu)p∈F  |U |

r(o¯ij, πij, yij) − |U |

r(oij, πij, yij) (Jensen’s Inequality)

(i,j)∈U

(i,j)∈U







1

1

= Eo,o¯,σ (πs,yu)p∈F  |U |

σij r(o¯ij , πij , yij ) − |U |

σij r(oij , πij , yij )

(i,j)∈U

(i,j)∈U







1

1

= Eo,o¯,σ (πs,yu)p∈F  |U |

σij r(o¯ij , πij , yij ) + |U |

σij r(oij , πij , yij )

(i,j)∈U

(i,j)∈U











1

1

≤ Eo,o¯,σ (πs,yu)p∈F  |U |

σij r(o¯ij , πij , yij ) + (πs,yu)p∈F  |U |

σij r(oij , πij , yij )

(i,j)∈U

(i,j)∈U

= 2G(F, R).

(11)

Combining Eqs. 8, 9, 10, and 11, we get Eq. 6. Another application of McDiarmid’s Inequality allows us to obtain Eq. 7 from Eq. 6.

C. Feedback Loops
Lemma C.1 (Binomial Tail Bound). If the random variable Xn ∼ n1 Binomial(n, θ), then for P(|Xn − θ| > ) ≤ 2 exp −2n 2 .

> 0, we have

Proof. Observe that Xn ∈ [0, 1]. Applying Hoeffding’s inequality gives us the desired result.

Lemma C.2. Let n ∈ N and κ be a ﬁxed C − 1 simplex such that κvn ∈ N ∀v ∈ [C]. The random variable q˜v ∼ κv1n Binomial(κvn, qv), where qv ∈ (0, 1). Assume that qv > qw if v > w. We denote as e the following C − 1 simplex:
1 e = Z [κ1q˜1, κ2q˜2, . . . , κC q˜C ] , where Z = κiq˜i.
i∈[C ]

Let

evw

=

e

ev +e

= κ q˜κv+q˜κv q˜

and

κvw

=

κ

κv +κ

. Then for a constant ρvw such that

vw

ww ww

vw

κvκw(qv − qw)

0 < ρvw < qvκ2 + (qv + qw)κvκw + qwκ2 ,

v

w

we have

|q˜v − qv| < vw, |q˜w − qw| < vw =⇒ evw − κvw > ρvw,

ρvwqvκ2v − κvκw(qw − qv) + qwρvwκv(κv − κw)

for some constant vw s.t. 0 < vw <

ρvw(κ2 − κ2 ) − 2κvκw .

v

w

Correcting Exposure Bias for Link Recommendation
This is saying that, for (v, w) s.t. v > w the simplex e will be more skewed towards v than the simplex κ if the sampled q˜v and q˜w are close to their mean values qv and qw, respectively.

Proof. Observe that if |q˜v − qv| < vw and |q˜w − qw| < vw, then the lowest value that evw can take is

e(min) =

κv(qv − vw)

, and

vw

κv(qv − vw) + κw(qw + vw)

e(vmwin) − κvw > ρvw =⇒ evw − κvw > ρvw.

Therefore, we have

e(vmwin) − κvw > ρvw and vw < qw ⇐= κv(qv − vw) − κv > ρvw κv(qv − vw) + κw(qw + vw) κv + κw
(1)

κvκw(qv − qw)

and ρvw < qvκ2 + (qv + qw)κvκw + qwκ2 .

v

w

The inequality (1) above can further be simpliﬁed as

This completes the proof.

κv(qv − vw)

− κv > ρvw

κv(qv − vw) + κw(qw + vw) κv + κw

ρvwqvκ2v − κvκw(qw − qv) + qwρvwκv(κv − κw)

⇐= vw <

ρvw(κ2 − κ2 ) − 2κvκw .

v

w

Lemma C.3.

Let α be a ﬁxed C − 1 simplex and e be the following G − 1 simplex, e =

1 Z

[α1q˜1,

α2q˜2,

.

.

.

,

αC

q˜C

],

where

Z=

z∈[C] αzq˜z

and

the

vector

κ

∼

n1 Multinomial(n, e).

Let

evw

=

e

ev +e

= q˜ q˜+vq˜

and

κvw

=

κ

κv +κ

.

vw

ww

vw

Assume that |q˜z − qz| < constant ρ, we have

∀z ∈ [C] where qz ∈ (0, 1) are ﬁxed. If |κv − ev < ηCnw | and |κw − ew < ηCnw |, then for some

evw − κvw < ρ, when ηvw < ρ qv + qw . maxz∈[C] qz +

Proof.

If |κv − ev

<

ηnw C

|

and

|κw

−

ew

<

ηnw C

|,

then

the

smallest

value

that

κvw

can achieve is

(min)

ev

−

ηnw C

κvw

=

.

ev + ew

This means that

evw − κvw < ρ
⇐= evw − κ(vmwin) < ρ
ηnw
⇐⇒ C < ρ ev + ew
⇐⇒ ηnw < ρ(ev + ew). C

Correcting Exposure Bias for Link Recommendation

Since |q˜z − qz| < ∀z ∈ [C], we have

ev =

αv q˜v z∈[C] αzq˜z

> αv(qv − ) αv(qv − ) + z∈[C],z=v αz(qz + )

> αv(qv − ) , C maxz∈[C] αz(qz + )

and similarly ew >

αw(qw − )

C maxz∈[C] αw(qz + )

∴ ev + ew > αv(qv − ) + αw(qw − ) . C maxz∈[C](qz + )

Therefore, we can set ηvw such that

ηnw < ρ C

αv(qv − ) + αw(qw − ) C maxz∈[C](qz + )

∴ ηnw < ρ αv(qv − ) + αw(qw − ) . maxz∈[C] qz +

Proof of Theorem 5.1
Theorem. Suppose that qv > qw if v > w. Let κ(vtw) = κ(vt) . Let A(vtw) represent the event that relative fraction of
κ(vt) +κ(wt)
recommendations from cv to that from cw increases at time t, i.e., κ(vtw+1) > κ(vtw) . Let A(t) be the event that all relative fractions get skewed towards cv from cw if qv > qw, i.e., A(t) = (v,w)∈S A(vtw) , where S = {(v, w) : v ∈ [C], w ∈ [C], v > w}. Then, for constants , η > 0 that only depend on κ(t) and q, we have

P(A(t)|κ(t)) ≥ 1 − 2C exp ≥ 1 − 2C exp

−2n

2 + η2 C2

n −O C2 .

Proof. We know that the estimated probabilities qv(t) have distribution qv(t)|κ(t) ∼ n1 Binomial(nκ(vt), qv). The simplex with

normalized

probabilities

is

e(t+1)

=

1 Z

[q1(t)

,

q2(t)

,

.

.

.

,

qC(t)

],

where

Z

=

z∈[C] qz(t).

Let q˜v(t) = qv(t) . Observe that q˜v(t)|κ(t) ∼ 1 Binomial(nκ(vt), qv). We denote by e(vtw+1),

κ(vt)

nκ(vt)

e(vtw+1) =

e(vt+1) (t+1) (t+1) =

κ(vt)q˜v(t)

(t) (t)

(t) (t) .

ev + ew

κv q˜v + κw q˜w

There are two main parts to the proof. First, we show that, with high probability, e(vtw+1) − κ(vtw) > ρ ∀(v, w) ∈ S for some constant ρ. Then, we show that, with high probability, e(vtw+1) − κ(vtw+1) < ρ ∀(v, w) ∈ S. We combine these two results to show that, with high probability, κ(vtw+1) > κ(vtw) ∀(v, w) ∈ S.
Using Lemma C.2, for some (v, w) ∈ S, we know that for some constant ρvw such that

κ(vt)κ(wt)(qv − qw) 0 < ρvw < qv(κ(vt))2 + (qv + qw)κ(vt)κ(wt) + qw(κ(wt))2 ,

Correcting Exposure Bias for Link Recommendation

we have

|q˜v(t) − qv| ≤ vw and |q˜w(t) − qw| ≤ vw =⇒ e(vtw+1) − κ(vtw) ≥ ρvw,

ρvwqv(κ(vt))2 − κ(vt)κ(wt)(qw − qv) + qwρvwκ(vt)(κ(vt) − κ(wt))

for a constant vw s.t. 0 < vw <

ρvw((κ(vt))2 − (κ(wt))2) − 2κ(vt)κ(wt)

=⇒ P e(vtw+1) − κ(vtw) ≥ ρvw ≥ P |q˜v(t) − qv| ≤ vw, |q˜w(t) − qw| ≤ vw .

Intuitively, this is saying that e(vtw+1)−κ(vtw) > ρvw if q˜v(t) and q˜w(t) are close to qv and qw, respectively. Let ρ = min(v,w)∈S ρvw and = min(v,w)∈S vw. Then we have







P

e(vtw+1) − κ(vtw) ≥ ρ ≥ P 

|q˜z(t) − qz| ≤ 

(v,w)∈S

z ∈[C ]

(12)





= 1 − P  |q˜z(t) − qz| ≥ 

(13)

z∈[C]

C

≥ 1 − P |q˜z(t) − qz| ≥ (Union Bound)

(14)

z=1

C
≥ 1 − 2 exp −2n 2 (using Lemma C.1)

z=1

= 1 − 2C exp −2n 2 .

(15)

Now, we show that e(vtw+1) is close to κ(vtw+1). We know that κ(t+1) ∼ n1 Multinomial(n, e(t+1)). Let the event Q(t) = z∈[C] |q˜z(t) − qz| ≤ . Using Lemma C.3, we know that, under Q(t), for some constant ηvw, we have

e(vt+1) − κ(vt+1) < ηCvw and e(wt+1) − κ(wt+1) < ηCvw =⇒ e(vtw+1) − κ(vtw+1) < ρ,

κ(vt+1)(qv − ) + κ(wt+1)(qw − )

where 0 < ηvw <

(t+1)

maxz∈[C] κz (qz + )

=⇒ P e(vtw+1) − κ(vtw+1) < ρ Q(t) ≥ P e(vt+1) − κ(vt+1) < ηCvw , e(wt+1) − κ(wt+1) < ηCvw .

Intuitively, this is saying that e(vtw+1) − κ(vtw+1) < ρ if κ(vt+1) and κ(wt+1) are close to e(vt+1) and e(wt+1), respectively. Thus, for η = min(v,w)∈S ηvw, we have



P

e(vtw+1) − κ(vtw+1) ≤ ρ

(v,w)∈S





Q(t) ≥ P 

|e(zt+1) − κ(zt+1)| ≤ Cη 

z ∈[C ]





= 1−P

|e(zt+1) − κ(zt+1)| ≥ Cη 

z∈[C]

C
≥1− P
z=1

|e(zt+1) − κ(zt+1)| > Cη

(Union Bound)

2nη2

≥ 1 − 2C exp − C2 (using Lemma C.1).

(16)

Correcting Exposure Bias for Link Recommendation

Combining Eq. 15 and 16, we get the desired result as follows:







P

A(vtw)  = P 

κ(vtw+1) > κ(vtw) 

(v,w)∈S

(v,w)∈S





≥ P

e(vtw+1) − κ(vtw) ≥ ρ, e(vtw+1) − κ(vtw+1) ≤ ρ 

(v,w)∈S





≥P

|e(t+1) − κ(t+1)| ≤

η ,

|q˜(t) − qz| ≤



z

z

Cz



z ∈[C ]







= P

|e(zt+1) − κ(zt+1)| ≤ Cη

z ∈[C ]

Q(t) P  |q˜z(t) − qz| ≤ 
z ∈[C ]

2nη2 ≥ 1 − 2C exp − C2

1 − 2C exp −2n 2

≥ 1 − 2C exp −2n 2 + exp n
≥ 1 − 2C exp −O C2 .

2nη2 − C2

Proof of Theorem 5.2 Lemma C.4 (Convergence in Probability). Let Xn, Yn, and Z be random variables such that Xn →p Yn and Yn →p Z, then Xn →p Z.

Proof. For any > 0, we have
P(|Xn − Z| ≥ ) = P(|Xn − Yn + Yn − Z| ≥ ) ≤ P(|Xn − Yn| + |Yn − Z| ≥ ) ≤ P |Xn − Yn| ≥ 2 + P |Xn − Yn| ≥ 2 = 0.
Therefore, Xn →p Z. Theorem. Suppose that qv > qw. As n → ∞, κ(vtw) →p 1 − 1+1ct , where c = qqwv .

Proof. At time step t, the fraction of recommendations from each group is κt. From group gv, the user cites papers

according to probability qv.

Therefore,

qv(t)

→p

κ(vt) qv .

And the normalized estimate is e(t+1)

=

1 S

[

κ(1t

)

q

1

,

.

.

.

,

κ

(t C

)

q

C

],

where S = z∈[C] κ(zt)qz. Since κ(t+1) ∼ n1 Multinomial(n, e(t+1)), we have

κ(t+1) →p e(t+1)

κ(vt+1) κ(wt+1)

p qvκ(vt) →
qw κ(wt) κ(vt) =c . κ(wt)

(17)

Correcting Exposure Bias for Link Recommendation

Table 5. The distribution of the FOS in the two real-world datasets.

FOS

DATASET 1 DATASET 2

ART BIOLOGY BUSINESS CHEMISTRY COMPUTER SCIENCE ECONOMICS ENGINEERING ENVIRONMENTAL SCIENCE GEOGRAPHY GEOLOGY HISTORY MATERIALS SCIENCE MATHEMATICS MEDICINE PHILOSOPHY PHYSICS POLITICAL SCIENCE PSYCHOLOGY SOCIOLOGY

0.03% 26.48% 0.38% 10.11% 9.40% 2.51% 6.24% 0.13% 0.48% 1.45% 0.04% 3.06% 7.17% 21.28% 0.03% 2.99% 0.18% 7.49% 0.55%

0.08% 23.43% 0.10% 15.67% 3.42% 0.03% 17.98% 0.03% 0.40% 0.46% 0.03% 19.09% 1.03% 13.90% 0.01% 3.14% 0.01% 1.14% 0.05%

We know that κ(v1) →p c. Combining this with Eq. 17 and using Lemma C.4 recursively, we get
κ(w1)

κ(vt) p t →c
κ(wt)

1p

1

∴ 1 − κ(t) → 1 − 1 + ct (Continuous mapping theorem)

1+

v (t)

κw

κ(vt)

p

1

∴ κ(vt) + κ(wt) → 1 − 1 + ct

∴ κ(vtw) →p 1 − 1 +1 ct .

D. Experiments
Table 5 provides the distribution of the various FOS in both the datasets used for the real-world dataset experiments (Section 6.2). We can see that the FOS distributions are different. For example, Dataset 2 has substantially more Materials Science and Engineering papers.

