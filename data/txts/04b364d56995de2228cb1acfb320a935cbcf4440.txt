Robust Trust Region for Weakly Supervised Segmentation

Dmitrii Marin University of Waterloo, Canada
dmitrii.a.marin@gmail.com

Yuri Boykov University of Waterloo, Canada
yboykov@uwaterloo.ca

arXiv:2104.01948v2 [cs.CV] 1 Sep 2021

Abstract
Acquisition of training data for the standard semantic segmentation is expensive if requiring that each pixel is labeled. Yet, current methods signiﬁcantly deteriorate in weakly supervised settings, e.g. where a fraction of pixels is labeled or when only image-level tags are available. It has been shown that regularized losses—originally developed for unsupervised low-level segmentation and representing geometric priors on pixel labels—can considerably improve the quality of weakly supervised training. However, many common priors require optimization stronger than gradient descent. Thus, such regularizers have limited applicability in deep learning. We propose a new robust trust region approach1 for regularized losses improving the state-of-theart results. Our approach can be seen as a higher-order generalization of the classic chain rule. It allows neural network optimization to use strong low-level solvers for the corresponding regularizers, including discrete ones.
1. Introduction
Our paper proposes a higher-order optimization technique for neural network training. While focused on semantic image segmentation, our main algorithmic idea is simple and general - integrate the standard trust region principle into the context of backpropagation, i.e. the chain rule. We reinterpret the classic chain rule: instead of the chain of gradients/derivatives for a composition of functions, we formulate the corresponding chain of hidden optimization subproblems. Then, inspired by the trust region principle, we can substitute a standard linear approximation solver (gradient descent) at any chain with a better higher-order solver. In short, we replace the classic differentiation chain rule by the trust region chain rule in the context of backpropagation.
Our work is motivated by the well-known challenges presented to the gradient descent by typical regularization losses or geometric priors/energies ubiquitous in the context of weakly-supervised or unsupervised segmentation.
1 https://github.com/dmitrii- marin/robust_trust_region

To validate our approach, we present semantic segmentation results improving the state-of-the-art in the challenging setting where the training data has only a fraction of pixels labeled. The generality of our main principle (trust region chain rule) and our promising results for a difﬁcult problem encourage further research. In fact, this work applies trust region principle only to the last “chain” in the network. We discuss several promising extensions for future work.
The rest of the introduction is organized as follows. To create a speciﬁc context for our general approach to network training, we review loss functions relevant for weaklysupervised or unsupervised segmentation. First, Sec. 1.1 discusses several standard geometric priors, regularization energies, clustering criteria, and their powerful solvers originally developed for low-level segmentation or general machine learning. Then, Sec. 1.2 outlines the use of such regularization objectives as losses for network training in the context of weakly supervised semantic (high-level) segmentation. We also review the standard trust region principle (Sec. 1.4) and highlight our main contributions (Sec. 1.5) based on the general idea of applying trust region (with powerful solvers) to network training.
1.1. Regularized energies in low-level segmentation
Assuming discrete segmentation s ∈ {1, 2, . . . , K}N where K is the number of categories and N is the number of image pixels, one common low-level segmentation energy can be represented as

E(s) = − log P (Ii|si) + wij [si = sj] (1)

i

{i,j}∈N

where Ii is a low-level feature (e.g. intensity, color, texture) at pixel i with distribution functions P (·|k) for each category k, neighborhood system N describes any pairwise connectivity (typically 4-, 8-grid [5] or denser [35]), weights wij represent given pairwise afﬁnities (typically Gaussian kernel for low-level features Ii and Ij [7, 5, 54, 35]), and [·] is the Iverson bracket operator returning 1 if the argument is true and 0 otherwise. The energy above combines the log-likelihoods term enforcing consistency with given (low-level) feature distributions and a pairwise reg-

ularizer (Potts model) term enforcing geometric prior on

shape smoothness with alignment to image intensity edges.

The Potts model has several efﬁcient combinatorial [7]

and LP-relaxation solvers [33, 36]. Besides, there are many

regularization objectives that are closely related to the ﬁrst-

order shape regularization in (1), but derived from a dif-

ferent discrete or continuous formulation of the low-level

segmentation and equipped with their own efﬁcient solvers,

e.g. geodesic active contours [10], snakes [27], power wa-

tersheds [18], to name a few. Moreover, there are many

other regularization terms going beyond the basic ﬁrst-order

smoothness (boundary length) enforced by the Potts term in

(1). The extensions include curvature [57, 47, 46], Pn-Potts

[31], convexity [25, 26], etc.

Common continuous formulations of the low-level seg-

mentation use relaxed variable s ∈ ∆NK combining pixel-

speciﬁc

distributions

si

=

(s1i

,

.

.

.

,

s

K i

)

∈

∆K

over

K

cat-

egories, where ∆K is the probability simplex. In this case

the segmentation objective/energy should also be relaxed,

i.e., deﬁned over real-values arguments. For example, one

basic relaxation of the Potts segmentation energy in (1) is

−

ski log P (Ii|k) + wij si − sj 2 (2)

ik

{i,j}∈N

using a linear relaxation of the likelihood term and a quadratic relaxation of the Potts model. Note that there could be inﬁnitely many alternative relaxations. Any speciﬁc choice affects the properties of the relaxed solution, as well as the design of the corresponding optimization algorithm. For example, simple quadratic relaxation in (2) is convex suggesting simpler optimization, but its known to be a non-tight relaxation of the Potts model [53] leading to weaker regularization properties unrelated to geometry or shape. There are many better alternatives, e.g. using different norms [18] or other convex formulations [13, 12, 11]. The bilinear relaxation of the Potts term below

−

ski log P (Ii|k) + (1 − sk) W sk (3)

ik

k

is tight [53], but it is non-convex and, therefore, more difﬁcult to optimize. In the formula above, vector sk := (ski ) combines segmentation variables for soft-segment k, and
N × N afﬁnity matrix Wij = wij [{i, j} ∈ N ] represents the neighborhood system N and all pairwise (e.g. Gaussian)
afﬁnities wij between image pixels. Note that Potts regularization is closely related to the Normalized cut objective
k (1−1sk)W sWk sk for unsupervised segmentation [58]. It is common to combine energies like (1),(2),(3) with
constraints based on user interactions (weak supervision).
While there are different forms of such supervision, the
most basic one is based on adding the seed loss [5] deﬁned
over pixels in subset Ωseeds with user-speciﬁed category labels yi. Assuming si ∈ ∆K , it can be written as a partial

cross entropy (PCE) for pixels i ∈ Ωseeds

Eseeds(s) = −

log syi i

(4)

i∈Ωseeds

and, when restricted to one-hot si representing hard segmentation, it reduces to the hard constraints over seeds [5]. That is, for integer-valued si ∈ {1, . . . , K} the seed loss is equivalent to i∈Ωseeds λ [si = yi] for inﬁnitely large λ.
The log-likelihood loss, e.g. the ﬁrst term in (1) or (3), is common in low-level segmentation and its importance cannot be underestimated. In basic formulations, the distributions of (low-level) features P (·|k) can be assumed given for each category k. However, if such distributions are not known a priori, their representation P (·|θk) can explicitly include unknown distribution parameters θk for each category k. Then, the overall loss E(s, θ) adds θ = {θk} as an extra variable. Optimization of E(s, θ) over both s and θ corresponds to joint estimation of segmentation and maximum likelihood (ML) estimation of distribution parameters, as in well-known unsupervised low-level segmentation formulations by Zhu & Yuille [67] and Chan & Vese [14]. Similar ideas are also used in box-interaction methods [54].

1.2. Regularized losses in DNN segmentation
Unlike low-level segmentation methods based on readily available low-dimensional features (like color, texture, contrast edges), deep neural network (DNN) approaches to segmentation learn complex high-dimensional “deep” features that can discriminate semantic categories. Thus, one can refer to such methods as high-level segmentation, and to such learned features as high-level features.
The most standard way to train segmentation networks is based on full supervision requiring a large collection of images where all pixels are accurately labeled. Such training data is expensive to get. The training is based on minimizing the cross-entropy (CE) loss similar to the seed loss in low-level segmentation. For simplicity focusing on a single training image, CE loss is

ECE(s(θ)) = − log syi i (θ)

(5)

i

where s(θ) = f (θ) ∈ ∆NK is the (relaxed) segmentation output of the network f (θ) with parameters θ. For brevity, here and later in this paper we omit the actual test image from the arguments of the network function f .
The fundamental difference with low-level segmentation reviewed above is that instead of minimizing losses E directly over segmentation variable s, now the optimization arguments are parameters θ of the network producing such segmentation. Estimating parameters θ can be interpreted as learning deep features. Note that this task is much more complex than ML estimation of distribution parameters for

P (I|θ) in low-level segmentation with ﬁxed low-level features I, as reviewed above. This explains why network optimization requires a large set of fully labeled training images, rather then a single image (unlabeled or partiallylabeled), as in low-level segmentation.
The goal of weakly supervised segmentation is to train the network with as little supervision as possible. First of all, it is possible to train using only a subset of labeled pixels (seeds) in each image [32, 61] in exact analogy with (4)

EPCE(s(θ)) = −

log syi i (θ)

(6)

i∈Ωseeds

In particular, as shown in [61], this simple, but principled approach can outperform more complex heuristic-based techniques. To improve weakly-supervised training, it is also possible to use standard low-level regularizes, as in Sec. 1.1, that leverage a large number of unlabeled pixels [66, 32, 61, 62, 42]. For example, [62] achieves the stateof-the-art using bilinear relaxation of the Potts model in (3)

EPbol tts(s(θ)) = (1 − sk(θ)) W sk(θ) (7)
k

as an additional regularization loss over all (including unlabeled) pixels. For some ν > 0, their continuous total loss

E = EPCE + ν EPbol tts.

(8)

More generally, standard regularization losses from lowlevel segmentation are commonly used in the context of segmentation networks. Such losses and their solvers are ubiquitous in weak-supervision techniques using seeds or boxes to generate fully-labeled proposals [29, 39]. Optimization of low-level regularizers is also common for network’s output post-processing, typically improving performance during testing [16]. Also, the corresponding low-level solvers can be directly integrated as solution-improving layers [66].

1.3. Weakly supervised semantic segmentation
Weak supervision for deep neural network semantic segmentation comes in many different forms, e.g. image-level tags [50, 49, 32], scribbles/clicks [39, 61, 62, 42], and bounding boxes [49, 29, 28]. These works employ a large variety of strategies to compensate for the lack of labels. The concept of multiple instance learning (MIL) naturally ﬁts the weakly supervised setting. Since generic MIL methods produce small unsatisfactory segments, more specialized methods are needed. For example, methods [50, 28] impose constraints on the output of the neural network during learning. There are several segmentation-speciﬁc constraints, such as size bias, constraints on present labels, tightness [38], etc. [32, 62, 42] incorporate edge alignment constraints. Proposal generation methods [29, 39]

aim to generate/complete the ground truth to use fullysupervised learning. However, DNNs are vulnerable to errors in proposals. More robust approaches use EM [49] or ADMM [42] to iteratively correct errors in “proposals”.
Some related prior work on weakly supervised DNN segmentation [39] uses some speciﬁc non-robust version of the joint loss related to our approach. Similar losses (studied in segmentation since 1980s) do not imply similar algorithms. In particular, they iterate explicit low-level segmentation of super-pixels [21] and pixel-level network training, where at each iteration the network is trained from scratch2 and to convergence. They motivate such integration by improved results only. They also argue that “when network gradually learns semantic content, the high-level information can help with the graph-based scribble propagation”, suggesting their main focus on improved “proposals”. As shown in [61, 62], their method is outperformed by using only the partial cross entropy on seeds (6).
1.4. Classic trust region optimization
Trust region is a general approximate iterative local optimization method [4] allowing to use approximations with good solvers when optimizing arbitrarily complex functions. To optimize g(x), it solves sub-problem min x−xt ≤ g˜(x) where function g˜ ≈ g is an approximation that can be “trusted” in some region x − xt ≤ around the current solution. If g˜ is a linear expansion of g, this reduced to the gradient descent. More accurate higherorder approximations can be trusted over larger regions allowing larger steps. The sub-problem is often formulated as unconstrained Lagrangian optimization minx g˜(x) + λ x − xt where λ indirectly controls the step size.
1.5. Related optimization work and contributions
The ﬁrst-order methods based on stochastic gradient descent dominate deep learning due to their simplicity, efﬁciency, and scalability. However, they often struggle to escape challenging features of the loss proﬁle, e.g. “valleys”, as the gradients lack information on the curvature of the loss surface. Adam [30] combines gradients from many iterations to gather such curvature information. On the other hand, the second-order methods compute parameters update in the form ∆θ = H−1∇θE(f (θ)), c.f . (10), where H is the Hessian or its approximation. In neural networks, computing the Hessian is infeasible, so various approximations are used, e.g. diagonal or low-rank [2]. The efﬁcient computation of Hessian-vector products is possible [52, 56]; while solving linear systems with Hessian is still challenging [60]. Another group of methods is based on employing GaussianNewton matrix and K-FAC approximations [43, 1, 3, 48].
Our approach is related to the proximal methods [44], in particular to the proximal backpropagation [23] and penalty
2That is, resetting the network to the ImageNet pre-trained parameters.

method [9]. In these works, the “separation” of the gradient update into implicit layer-wise optimization problems is formulated as a gradient update of a certain energy function. Taylor et al. [63] use ADMM splitting approach to separate optimization over different layers in distributed fashion. These works focus on neural network parameter optimization replacing backpropagation altogether. In contrast to [9, 63, 23], we are primarily focused on optimization for complex loss functions in the context of the weakly supervised semantic segmentation, see Sec.1.2, while others focus on replacing the backpropagation in the intermediate layers. Also, unlike us, these methods use the squared Euclidean norm in their proximal formulations. Chen and Teboulle [15] generalize the proximal methods to Bregman divergences, a more general class of functions which includes both the Euclidean distance and KL-divergence. Nesterov in [45] uses the Euclidean norm with a higher power improving the convergence of the proximal method.
Our contribution are as follows:
• New trust region optimization for DNN segmentation integrating higher-order low-level solvers into training. Differentiability of the loss is not required as long as there is a good solver, discrete or continuous. The classic differentiation chain rule is replaced by the trust region chain rule in the context of backpropagation.
• The local optimization in trust region framework allows to use arbitrary metrics, instead of Euclidean distance implicit for the standard gradient descent. We discuss different metrics for the space of segmentations and motivate a robust version of KL-divergence.
• We show beneﬁts of our optimization for regularization losses in weakly supervised DNN segmentation, compared to the gradient descent. We set new stateof-the-art results for weakly supervised segmentation with scribbles achieving consistently the best performance at all levels of supervision, i.e. from pointclicks to full-length scribbles.
2. Trust region for loss optimization
Backpropagation is the dominant method for optimizing network losses during training. It represents the gradient descent with respect to model parameters θ where the gradient’s components are gradually accumulated using the classic chain rule while traversing the network layers starting from the output directly evaluated by the loss function.
Motivated by the use of hard-to-optimize regularization losses (Sec. 1.1) in the context of weakly-supervised segmentation (Sec. 1.2), we propose higher-order trust region approach to network training. While this general optimization approach can be developed for any steps of the backpropagation (i.e. chain rule) between internal layers, we fo-

cus on the very ﬁrst step where the loss function is composed with the network output

min E(f (θ))

(9)

θ∈Rm

where some scalar loss function E : Rn → R1

is deﬁned over n-dimensional output of a network/model f : Rm → Rn.

Since during training the network’s input is limited to ﬁxed examples, for simplicity we restrict the arguments of network function f to its training parameters θ ∈ Rm. Also note that, as a convention, this paper reserves the boldface font for vector functions (e.g. network model f ) and for matrix functions (e.g. model’s Jacobian J f ).
The main technical ideas of the trust region approach to network optimization (9) in this section are fairly general. However, to be speciﬁc and without any loss of generality, this and (particularly) later sections can refer to the output of the network as segmentation so that
Rn = RN×K

where N is the number of image pixels and K is the number of distinct semantic classes. This is not essential.
Our general trust region approach to (9) can be seen as a higher-order extension of the classic chain rule for the composition E ◦ f of the loss functions E and model f . For the classic chain rule in the standard backpropagation procedure, it is critical that both E and f are differentiable. In this case, the classic chain rule for the objective in (9) gives the following gradient descent update for parameters θ

∆θ = −α ∇E J f

(10)

where ∆θ ≡ θ − θt is an update of the model parameters from the current solution, α is the learning rate, ∇ is the gradient operator, and J f is the model’s Jacobian

∂fi J f := ∂θj .

We would like to rewrite the classic chain rule (10) in an
equivalent form explicitly using a variable for segmentation s ∈ Rn, which is an implicit (hidden) argument of the loss function E in (9). Obviously, equation (10) is equivalent to
two separate updates for the segmentation ∆s ≡ s − st and for the model parameters ∆θ ≡ θ − θt

∆s = − α ∇E

(11)

∆θ = ∆s J f

(12)

where the gradient ∇E is computed at the current segmentation st := f (θt). Note that s ∈ Rn represents points (e.g. segmentations) in the same space as the network output f (θ) ∈ Rn, the two should be clearly distinguished in
the discourse. We will refer to s as (explicit) segmentation
variable, while f (θ) is referred to as segmentation output.
The updates in (11) and (12) correspond to two distinct
optimization sub-problems. Clearly, (11) is the gradient de-
scent step for the loss E(s) locally optimizing its linear Taylor approximation E˜linear(s) = E(st) + ∇E ∆s over (explicit) segmentation variable s ∈ B(st) ⊂ Rn in a neighborhood (ball) around st

st+1 = arg min E˜linear(s).

(13)

s∈B (st )

While less obvious, it is easy to verify that θ-update in (12) is exactly the gradient descent step

1 ∆θ = − ∇θ

st+1 − f (θ) 2

(14)

2

corresponding to optimization of the least-squares objective

min st+1 − f (θ) 2

(15)

θ

based on the solution st+1 ≡ ∆s + f (θt) for problem (13). Our trust region approach to network training (9) is moti-
vated by the principled separation of the chain rule (10) into two sub-problems (13) and (15). Instead of the gradient descent, low-level optimization of the loss in (13) can leverage powerful higher-order solvers available for many popular loss functions, see Sec. 1.1. In particular, the majority of common robust loss functions for unsupervised or weaklysupervised computer vision problems are well-known to be problematic for the gradient descent. For example, their robustness (boundedness) leads to vanishing gradients and sensitivity to local minima. At the same time, the gradient descent can be left responsible for the least-squares optimization in (15). While it is still a hard problem due to size and non-convexity of the typical models f (θ), at least the extra difﬁculties introduced by complex losses E can be removed into a different sub-problem.
Formally, our trust-region approach to training (9) generalizes our interpretation of the classic chain rule in subproblems (13) and (15) as shown in iterative stages A, B:

STAGE A (low-level optimization)

st+1 = arg min E˜(s) + λ dA(s, f (θt)) (16)
s

STAGE B (network parameters update)

min dB(st+1, f (θ))

(17)

θ

⇓

∆θ = − γ ∇θ dB(st+1, f (θ))

(18)

where E˜ is some loss approximation, dA and dB are some distance/divergence measures. Instead of α in (11) and ﬁxed weight 12 in (14), the overall learning speed of our training procedure is controlled by two parameters: (A) scalar λ indirectly determining the step size from the current solution st = f (θt) in (16), and (B) scalar γ deﬁning the step size for the gradient descent in (18). While both λ and γ are important for the learning speed, we mostly refer to λ as a trust region parameter, while the term learning rate is reserved primarily for parameter γ in (18), as customary for the gradient descent step size in network optimization. Note that similarly to the gradient descent (10), stages A/B are iterated until convergence. While it is sensible to make several B-steps (18) in a row, in general, it is not necessary to wait for convergence in sub-problem (17) before the next A-step.
Our formulation offers several signiﬁcant generalizations of the classic chain rule. First, instead of the linear approximation (13) implied by the gradient descent (11), we target higher-order approximations of the loss E˜ in (16). In some cases, the exact loss E could be used3. The corresponding powerful low-level solvers for (16) are readily available for many types of useful robust losses, see Sec. 1.1. Note that for exact solvers when E˜ = E, one may argue for λ = 0 allowing the network to learn from the best solutions for regularized loss E implying global optima in (9). However, such ﬁxed proposals (Sec. 1.2) may result in overﬁtting to mistakes due to well-known biases/weaknesses in common regularizers. Constraining loss optimization (9) to the network output manifold in Rn motivates λ > 0 in (16). More discussion is in Sec. 5.1.
Second, besides continuous/differentiable losses required by the standard backpropagation (chain rule), our trust region approach (stages A/B) allows training based on losses deﬁned over discrete domains. There are several reasons why this extension is signiﬁcant. For example, besides continuous solvers, optimization in (16) now can use a signiﬁcantly larger pool of solvers including many powerful discrete/combinatorial methods. Moreover, this approach enables training of models with discrete decision functions, e.g. step function instead of sigmoid, or hard-max instead of the soft-max. This is further discussed in Sec. 5.1.
Third, the standard gradient descent (10) is implicitly deﬁned over Euclidean metric, that manifests itself in our equations (13) and (15) via the local neighborhood topology (Euclidean ball B) and the least-squares objective (squared Euclidean distance). In contrast, when replacing ball B(st) in (13) by the trust region term in (16), we explicitly deﬁne the trust region “shape” using function dA. It could be any application-speciﬁc distance metric, quasi- or pseudo-
3Note that parameter λ in (16) controls two properties: the size of the trust region for approximation E˜, as well as the network’s training speed. While using exact loss E˜ = E implies that the trust region for such “approximation” should be the whole domain (i.e. λ = 0), the competing interest of limiting the training speed in (17) may require λ > 0.

metric, divergence, etc. Similarly, any appropriately motivated distance, distortion, or divergence function dB in (17) can replace the least squares objective in (15).
On the negative side, our trust region formulation could be more expensive due to the computational costs of the low-level solvers in stage A. In practice, it is possible to amortize stage A over multiple iterations of stage B.

3. Robust metric for trust region
The choice of metrics dA and dB deﬁning the shape of the trust region above is application dependent. In the case of segmentation, the output of a neural network is typically obtained via the soft-max function. Hence, the space, in which the trust region operates, is the space of multiple categorical distributions over K categories: ∆NK .
Below, we generally discuss (robust) metrics over pairs of arbitrary probability distributions p, q in ∆NK . The goal of this section is to motivate our choice of metrics dA and dB in problems (16), (17) so that distribution p can be associated with the segmentation variable s, and distribution q can be associated with the network output f (θ). Besides this connection, the following discussion of metrics over probability distributions is independent of the context of networks.
Note, metrics dA or dB do not have to be proper distances for the purposes of trust region optimization. Instead, one may use any divergence measure deﬁned on space ∆NK . Let us consider the Kullback–Leibler divergence:

NK l

pli

NK

l

l

KL(p q) =

pi log ql = −

pi log qi − H(p)

i=1 l=1

i

i=1 l=1

where p, q ∈ ∆NK , and pli is the probability of pixel i to have label l, and H(p) is the entropy of distribution p.
A practically important case is when the distribution p is
degenerate or one-hot, i.e. for each pixel i there exists label yi such that pyi i = 1 and for any label k = yi probability pki = 0. In that case H(p) = 0 and

KL(p q) = − log qiyi ,

(19)

i

which is the cross-entropy or negative log-likelihood, a standard loss when q is the probability estimate outputted by a neural network. In the following we assume (19).
During the trust region procedure, intermediate solutions generated by a solver in (16) may have a noticeable amount of misclassiﬁed pixels. It is known that many standard losses for neural networks, including cross-entropy (19), can result in training sensitive to idiosyncrasies in the datasets including mistakes in the ground truth [24, 40, 22]. Therefore, a robust distance measure may be needed. Our experiments show that robustness is critical. We propose a simple error model depicted in graphical model in Fig. 1.

observed image I

hidden true
labeling Z

observed noisy
labeling Y

switch label with probability ε
Figure 1. The unknown true labeling Z corresponds to observed image I. The observed labeling Y is assumed to be generated from the true Z by a simple corruption model (20).

(a): − log qi1

(b): − log(a + b qi1)

qi1 = 1+exp1(−xi)
xi
1

Figure 2. Robust loss as function of logits xi. There are K = 2
classes; the ground truth label is yi = 1. If the current prediction qi1 is conﬁdent and does not coincide with yi, see xi 0 or qi1 ≈ 0 on the plot, robust loss (b) becomes ﬂatter avoiding the over-penalize in case of mistakes in the ground truth. In contrast,
standard cross-entropy (a) behaves linearly, which may be detri-
mental to learning if the ground truth is mistaken.

Let random variable Yi be the observed noisy label of pixel i and Zi be its hidden true label. We assume that the probability of observing label l given true label k is

1 − ε, l = k, Pr(Yi = l | Zi = k) = ε , l = k, (20)
K −1

where ε is called the outlier probability [37]. The probability of pixel i having label l given image I is

K

Pr(Yi = l|I) = Pr(Yi = l|Zi = z) Pr(Zi = z|I) =

z=1

= a + b Pr(Zi = l|I)

(21)

where

a

=

ε K −1

and

b

=

1−K

a.

The

probability

Pr(Zi =

z|I) is unknown and is replaced by probability estimate qil

yielding a robust version of divergence (19):

− log (a + b qiyi ) .

(22)

i

Figure 2 compares cross-entropy (19) with robust loss (22). Our robust cross-entropy (22) is related to a more general
approach for classiﬁcation [51, 59]. In [51], the corresponding robust cross-entropy (forward correction) is

− log q˜iyi

(23)

i

0.79

0.78

Accuracy

0.77

0.76

0.75

0.0

0.2

0.4

0.6

0.8

robustness parameter ε

Figure 3. Classiﬁcation accuracy on Fashion-MNIST dataset [65]
with noisy labels using a network with two convolutional, two
fully-connected layers and robust loss (22). The original labels were uniformly corrupted with probability 12 . The best accuracy is achieved at ε = 0.4, which is close to the actual noise level.

where q˜i = T qi, and qi is the vector of probability estimates at pixel i, and T = [Tlk] is the noise transition matrix: Tlk = Pr(Y = k | Z = l). The effect of different ε is shown in example in Fig. 3.
In practice, different pixels require different values of ε in (20). For example, in the scribble-based weakly supervised segmentation, the labels of seed pixels Ωseeds are known for sure. So, ε = 0 for such pixels, and ε > 0 for all other pixels. Thus, the robust “metric” is

KLε,Ωseeds (p q) = − log (a + b qiyi ) + − log qiyi .

i∈Ωseeds

i∈Ωseeds

(24)

In sum, we propose the following robust metrics for the

trust region iterations (16) and (18):

dA(p, q) = KL(p q), (25) dB(p, q) = KLε,Ωseeds (p q).

4. Results in weakly supervised segmentation

To validate our approach (16-18) we use standard efﬁcient discrete solvers [7] for the loss

E˜ = EPCE + EPotts

(26)

where EPotts(s) = {i,j}∈N wij [si = sj ] is the second (regularization) term in standard low-level energy (1). In
this case, optimization in (16) is limited to the corners of the simplex where EPCE reduces to the hard constraints over the seeds. In (16-18) we use robust metrics (25). The over-
all method is summarized in Alg. 1.
One natural baseline for Alg. 1 is a standard method
based on stochastic gradient descent (SGD) for regularized loss (8) proposed in [62], see Sec. 1.2. Indeed, EPboltts is a relaxation of E , Potts as discussed in Sec. 1.1. Thus, (8) is a relaxation of (26). Alg. 1 with combinatorial solver for E˜

in (26) can be seen as a discrete trust region approximation for (8). In general, our approach (16-18) allows other discrete or continuous solvers and/or other approximations E˜.
First, PCE-GD baseline is the standard SGD optimizing partial cross-entropy (6). It has been shown in [62, 61] that such approach outperforms more complex proposal (fake ground truth) generation methods such as [39]. Second, Grid-GD is the SGD over regularized loss (8) where the CRF neighbourhood is the standard 8-grid. Third, DenseGD is the approach of [62] that uses the common fullyconnected (dense) Potts CRF of [35].
We use the ScribbleSup [39] annotations for Pascal VOC 2012 [20] dataset. ScribbleSup supplies scribbles, i.e. a small subset of image pixels (≈ 3%) is labeled while the vast majority of pixels is left unlabeled.
4.1. Implementation details
In all our experiments we used DeeplabV3+ [17] with MobileNetV2 [55] as a backbone model.
Pretraining: We use the standard ImageNet [19] pretraining of the backbone models. In addition, before the optimization via Grid-GD (7) and Grid-TR (16-18) starts, the DeeplabV3+ models are pretrained by the PCE loss (6).
Meta-parameters: We train 60 epochs. We tuned the learning rates for all methods on the val set. Other metaparameters for competitive methods were set as in the corresponding papers/code. The learning rate is polynomial with power 0.9, momentum is 0.9, batch size is 12.
Grid-TR STAGE A (16): The low-level solver4 of the
4GCOv3.0: https://vision.cs.uwaterloo.ca/code/

Algorithm 1: Robust Trust Region for Potts model

1 Initialize model f using ImageNet pretraining ;

2 Tune parameters θ of model f by optimizing

PCE-GD loss (6) ;

3 Initialize γ with the base learning rate ;

4 repeat

5 for each image in dataset do

6

compute segmentation variable s via (16)

using metric dA in (25) and loss (26);

7 end

8 for M epochs do

9

for each image (batch) in dataset do

10

update the network parameters θ using

stochastic gradient descent for loss (17)

with robust metric dB in (25) ;

11

update rate γ in accord with schedule;

12

end

13 end

14 until required number of epochs is reached;

scribble length

0 0.3 0.5 0.8

1

full supervision

0.70

PCE-GD

0.50 0.57 0.59 0.61 0.61

Dense-GD

0.55 0.61 0.62 0.63 0.64

Grid-GD

0.54 0.60 0.62 0.64 0.64

Grid-TR (our) 0.57 0.63 0.64 0.66 0.67

Table 1. Results for ScribbleSup, see description in Figure 5.

grid CRF is the α-expansion [8, 34, 6] with 8-grid neighbourhood system. The max number of α-expansion iterations is 5 achieving convergence in most cases. We restrict the set of labels to those present in the image. We amortize the STAGE A compute time by integrating it with data loading. The training is 1.3 times slower than Dense-GD.
Grid-TR STAGE B (18): To amortize the time consumed by the graph cuts, we perform M = 5 epochs of neural network weights updates (17) for each update of the segmentation variables (18). We use a global learning rate schedule spanning throughout iterations. See Alg. 1.
4.2. Segmentation quality
The quantitative results of the weakly supervised training for semantic segmentation are presented in Figure 5 and Tab. 1. The results are presented with different levels of supervision varying from the clicks (denoted as length 0) to the full-length scribbles (denoted as length 1). Decreasing supervision results in degraded performance for all methods. We are interested to compare how different approaches perform at different levels of supervision. Our Grid-TR outperforms all the competitors at each level of supervision.
The examples of images and results shown in Fig. 4 demonstrate the advantages of our method, particularly w.r.t. edge alignment. Quantitatively, we evaluate the accuracy of semantic boundaries using standard trimaps [31, 35, 16, 41]. A trimap corresponds to a narrow band around the ground truth segment boundaries of varying width. An accuracy measure, e.g. mIoU, is computed for pixels within each band. The results are shown in Fig. 6 where our approach demonstrates superior performance.
5. Discussion
5.1. On parameter λ in (16)
As discussed below equations (16) - (18) in the paper, even for exact (global) solvers using E˜ = E in (16), the choice of λ = 0 could be sub-optimal, as demonstrated empirically here in Figure 7. As argued in the paper, while λ = 0 with an exact solver may seem like a good approach to training minθ E(f (θ)) suggesting globally optimal loss, empirically this leads to overﬁtting to mistakes or biases of the regularizer (e.g. the Potts model). One argument for λ > 0 discussed in the paper is that this corresponds to the constrained optimization of (9) over the network manifold

in Rn. Such formulation of the training could be preferred as constraining to neural networks can be seen as incorporation of the “deep priors”, e.g. [64]. One can also argue that local minima of E inside the manifold of the network output in Rn may be preferable to the global optimum of E due to limitations of the basic (but solvable) regularizers.
Empirically, λ = 0 in (16) leads to a ﬁxed set of proposals generated in a single run of stage A completely independent of the network. In contrast, λ > 0 leads to multiple distinct iterations of stage A where the network is in the feedback loop. Vice versa, instead of ﬁxed proposals, for λ > 0 the network is exposed to a substantially larger set of solutions in stage B reduces overﬁtting.
Moreover, the objective in (16) can be motivated on its own merits independently of the objective in (9). It can be seen as a low-level segmentation objective that integrates class likelihoods produced by the neural network, replacing the basic likelihoods using low-level features, e.g. colors, as discussed in Sec.1.1. Iterations A/B can be seen as joint segmentation and model estimation, as typical for well-known low-level segmentation methods like ZhuYuille [67], Chan-Vese [14], or GrabCut [54]. The main difference is that our stages A/B use “deep” models. In contrast to standard methods [67, 14, 54] estimating model parameters for some standard class of probability distributions (e.g. GMM) over ﬁxed low-level features like colors, we estimate deep models with millions of parameters that can be interpreted as learning high-level (semantic) features.
5.2. On discrete losses and decisions/activations
Our approach can train networks using discrete decisions/activations and losses deﬁned over discrete domains. For example, (16)-(18) do not require that E is differentiable. In particular, (16) can be optimized over “hard” segmentations s ∈ {0, 1}N×K ⊂ ∆NK even if the network produces soft segmentations f (θ) ∈ ∆NK , as long as dA in (16) can measure a distance between discrete and continuous solutions, e.g. KL(s, f ) for one-hot and soft distributions. It is also possible to train the models with discrete decision functions D(l) such that f (θ) = D(l(θ)) where l are logits. Then, all arguments in (16) are discrete. Optimization in (17) can be formulated over real-valued logits using dB measuring a distance to subset {l | D(l) = st+1} ⊂ RN×K .
Acknowledgements
We thank Yaoliang Yu for the insightful discussion on related proximal methods and pointing out related literature. We also thank Vladimir Kolmogorov for suggesting prior studies of the tightness of the Potts model relaxations.

iimmaaggee

ggrroouunndd ttrruutthh

PCPEC-EG-GDD(6)

DGernids-eG-GDD

DGenrisde--GGDD Grid-GTrRid-(T1R6,18)

Figure 4. Examples of the full-scribble training results, see Tab. 1 and Figure 5. Note the better edge alignment of our Grid-TR.

References
[1] Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kroneckerfactored approximations. 2017. 3
[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, August 2006. 3
[3] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 557–565, Inter-

national Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. 3
[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. 3
[5] Yuri Boykov and Marie-Pierre Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images. In ICCV, volume I, pages 105– 112, July 2001. 1, 2
[6] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision. IEEE transac-

Full Supervision Grid-GD

PCE-GD Grid-TR

Dense-GD

0.70

mean intersection over union (mIoU)

0.65

0.60

0.55

0.50

0.00

0.25

0.50

0.75

1.00

scribble length ratio

Figure 5. Segmentation performance on the val set of ScribbleSup [39, 20] using DeeplabV3+ [17] with MobileNetV2 [55] backbone. The supervision level varies horizontally, with 1 corresponding to the full scribbles. Our “Grid-TR” outperforms other competitors for all scribble lengths and provides a new state-of-the-art.

PCE-GD 0.6000

Dense-GD

Grid-GD

Grid-TR (our)

mean intersection over union (mIoU)

0.5500

0.5000

0.4500

0.4000 5

10

15

20

distance to the boundary (trimap width), px

Figure 6. The quality of segment boundary alignment. The networks were trained on the full-length scribbles.

tions on pattern analysis and machine intelligence, 26(9):1124–1137, 2004. 8
[7] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. IEEE transactions on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, November 2001. 1, 2, 7
[8] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):1222–1239, 2001. 8
[9] Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Artiﬁcial Intelligence and Statistics, pages 10–19. PMLR, 2014. 4
[10] Vicent Caselles, Ron Kimmel, and Guillermo Sapiro. Geodesic active contours. International journal of computer vision, 22(1):61–79, 1997. 2

mean intersection over union (mIoU)

0.6675
0.6650
0.6625
0.6600
0.6575 0.000 0.025 0.050 0.075
value of λ - Lagrange multiplier in (16)
Figure 7. Empirical evaluation of Lagrange multiplier λ for the Trust Region term in (16): the plot shows how mobile-net training quality depends on λ. The context is the weakly-supervised semantic segmentation in Sec.4 with regularization loss E (using 8grid Potts and full scribbles) based on our Trust Region chain rule with robust metric dB in (18). For λ = 0 equation (16) generates ﬁxed low-level segmentation proposals completely independent of the network. Then, the network overﬁts to mistakes in such proposals due to biases/weaknesses of the regularizer. As λ → ∞, trust region becomes too small and our approach loses its advantages due to better (e.g. higher-order) approximation E˜ in (16). Conceptually speaking, it should get closer to the results of gradient descent, which uses basic ﬁrst-order approximations.
[11] Antonin Chambolle, Daniel Cremers, and Thomas Pock. A convex approach to minimal partitions. SIAM Journal on Imaging Sciences, 5(4):1113–1158, 2012. 2
[12] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120–145, 2011. 2
[13] Tony Chan, S Esedoglu, and M Nikolova. Algorithms for ﬁnding global minimizers of image segmentation and denoising models. SIAM journal on applied mathematics, 66(5):1632–1648, 2006. 2
[14] Tony F Chan and Luminita A Vese. Active contours without edges. IEEE Transactions on image processing, 10(2):266–277, 2001. 2, 8
[15] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using bregman functions. SIAM Journal on Optimization, 3(3):538–543, 1993. 4
[16] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017. 3, 8
[17] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder

with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018. 7, 10
[18] Camille Couprie, Leo Grady, Laurent Najman, and Hugues Talbot. Power watershed: A unifying graph-based optimization framework. IEEE transactions on pattern analysis and machine intelligence, 33(7):1384–1399, 2010. 2
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 7
[20] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1):98–136, 2015. 7, 10
[21] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efﬁcient graph-based image segmentation. International journal of computer vision, 59(2):167–181, 2004. 3
[22] Benoˆıt Fre´nay and Michel Verleysen. Classiﬁcation in the presence of label noise: a survey. IEEE transactions on neural networks and learning systems, 25(5):845–869, 2013. 6
[23] Thomas Frerix, Thomas Mo¨llenhoff, Michael Moeller, and Daniel Cremers. Proximal backpropagation. In International Conference on Learning Representations, 2018. 3, 4
[24] Stuart Geman, Elie Bienenstock, and Rene´ Doursat. Neural networks and the bias/variance dilemma. Neural computation, 4(1):1–58, 1992. 6
[25] Lena Gorelick, Olga Veksler, Yuri Boykov, and Claudia Nieuwenhuis. Convexity shape prior for binary segmentation. IEEE transactions on Pattern Analysis and Machine Intelligence (PAMI), 39(2):258–271, February 2017. 2
[26] Hossam Isack, Lena Gorelick, Karin Ng, Olga Veksler, and Yuri Boykov. K-convexity shape priors for segmentation. In European Conference on Computer Vision (ECCV), Munich, Germany, September 2018. 2
[27] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active contour models. International journal of computer vision, 1(4):321–331, 1988. 2
[28] Hoel Kervadec, Jose Dolz, Shanshan Wang, Eric Granger, and Ismail Ben Ayed. Bounding boxes for weakly supervised segmentation: Global constraints get close to full supervision. In Proceedings of

the Third Conference on Medical Imaging with Deep Learning, volume 121 of Proceedings of Machine Learning Research, pages 365–381, Montreal, QC, Canada, 06–08 Jul 2020. PMLR. 3
[29] Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 876–885, 2017. 3
[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 3
[31] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order potentials for enforcing label consistency. International Journal of Computer Vision, 82(3):302–324, 2009. 2, 8
[32] Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles for weakly-supervised image segmentation. In European Conference on Computer Vision, pages 695–711. Springer, 2016. 3
[33] Vladimir Kolmogorov. Convergent Tree-Reweighted Message Passing for Energy Minimization. IEEE transactions on Pattern Analysis and Machine Intelligence, 28(10):1568–1583, October 2006. 2
[34] Vladimir Kolmogorov and Ramin Zabih. What energy functions can be minimized via graph cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004. 8
[35] Philipp Krahenbuhl and Vladlen Koltun. Efﬁcient inference in fully connected CRFs with Gaussian edge potentials. In NIPS, 2011. 1, 7, 8
[36] M Pawan Kumar, Vladimir Kolmogorov, and Philip Torr. An analysis of convex relaxations for map estimation of discrete mrfs. JMLR, 2009. 2
[37] Jan Larsen, L Nonboe, Mads Hintz-Madsen, and Lars Kai Hansen. Design of robust neural network classiﬁers. In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181), volume 2, pages 1205–1208. IEEE, 1998. 6
[38] Victor Lempitsky, Pushmeet Kohli, Carsten Rother, and Toby Sharp. Image segmentation with a bounding box prior. In 2009 IEEE 12th international conference on computer vision, pages 277–284. IEEE, 2009. 3
[39] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3159–3167, 2016. 3, 7, 10

[40] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on cybernetics, 43(3):1146–1151, 2013. 6
[41] Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai, Fei Yang, and Yuri Boykov. Efﬁcient segmentation: Learning downsampling near semantic boundaries. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2131–2141, 2019. 8
[42] Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Beyond gradient descent for regularized segmentation losses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10187–10196, 2019. 3
[43] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pages 2408–2417. PMLR, 2015. 3
[44] Bernard Martinet. Brief communication. Regularization of variational inequalities by successive approximations. French journal of informatics and operational research. Red Series, 4(R3):154–158, 1970. 3
[45] Yurii E Nesterov. Inexact accelerated high-order proximal-point methods. Technical report, CORE, 2020. 4
[46] Claudia Nieuwenhuis, Eno Toeppe, Lena Gorelick, Olga Veksler, and Yuri Boykov. Efﬁcient squared curvature. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014. 2
[47] Carl Olsson, Johannes Ulen, Yuri Boykov, and Vladimir Kolmogorov. Partial enumeration and curvature regularization. In International Conference on Computer Vision (ICCV), Sydney, Australia, December 2013. 2
[48] Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka. Largescale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12359–12367, 2019. 3
[49] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. In Proceedings of the IEEE international conference on computer vision, pages 1742– 1750, 2015. 3
[50] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In Proceedings of

the IEEE international conference on computer vision, pages 1796–1804, 2015. 3
[51] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1944–1952, 2017. 6
[52] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147–160, 1994. 3
[53] Pradeep Ravikumar and John Lafferty. Quadratic programming relaxations for metric labeling and markov random ﬁeld map estimation. In Proceedings of the 23rd international conference on Machine learning, pages 737–744, 2006. 2
[54] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut - interactive foreground extraction using iterated graph cuts. In ACM trans. on Graphics (SIGGRAPH), 2004. 1, 2, 8
[55] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510–4520, 2018. 7, 10
[56] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7):1723–1738, 2002. 3
[57] Alexander Shekhovtsov, Pushmeet Kohli, and Carsten Rother. Curvature prior for mrf-based segmentation and shape inpainting. In Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium, pages 41–51. Springer, 2012. 2
[58] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 22:888–905, 2000. 2
[59] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training convolutional networks with noisy labels. In 3rd International Conference on Learning Representations, ICLR 2015, 2015. 6
[60] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139–1147. PMLR, 2013. 3
[61] Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers. Normalized Cut Loss for Weakly-supervised CNN Segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 3, 7

[62] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and Yuri Boykov. On Regularized Losses for Weaklysupervised CNN Segmentation. In European Conference on Computer Vision (ECCV), 2018. 3, 7
[63] Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training neural networks without gradients: A scalable ADMM approach. In International conference on machine learning, pages 2722–2731, 2016. 4
[64] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9446–9454, 2018. 8

[65] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. 7
[66] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random ﬁelds as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1529–1537, 2015. 3
[67] Song Chun Zhu and Alan Yuille. Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 18(9):884– 900, Sept. 1996. 2, 8

