arXiv:1606.09632v3 [cs.LG] 10 Jan 2021

A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness
Nihar B. Shah∗, Sivaraman Balakrishnan and Martin J. Wainwright†
∗ Machine Learning Department and Computer Science Department Department of Statistics and Data Science Carnegie Mellon University
†Department of EECS and Department of Statistics University of California, Berkeley
Abstract
The task of aggregating and denoising crowd-labeled data has gained increased signiﬁcance with the advent of crowdsourcing platforms and massive datasets. We propose a permutationbased model for crowd labeled data that is a signiﬁcant generalization of the classical Dawid-Skene model, and introduce a new error metric by which to compare diﬀerent estimators. We derive global minimax rates for the permutation-based model that are sharp up to logarithmic factors, and match the minimax lower bounds derived under the simpler Dawid-Skene model. We then design two computationally-eﬃcient estimators: the WAN estimator for the setting where the ordering of workers in terms of their abilities is approximately known, and the OBI-WAN estimator where that is not known. For each of these estimators, we provide non-asymptotic bounds on their performance. We conduct synthetic simulations and experiments on real-world crowdsourcing data, and the experimental results corroborate our theoretical ﬁndings.
1 Introduction
Recent years have witnessed a surge of interest in the use of crowdsourcing for labeling massive datasets. Expert labels are often diﬃcult or expensive to obtain at scale, and crowdsourcing platforms allow for the collection of labels from a large number of low-cost workers. This paradigm, while enabling several new applications of machine learning, also introduces some key challenges: ﬁrst, low-cost workers are often non-experts and the labels they produce can be quite noisy, and second, data collected in this fashion has a high amount of heterogeneity with signiﬁcant diﬀerences in the quality of labels across workers and tasks. Thus, it is important to develop realistic models and scalable algorithms for aggregating and drawing meaningful inferences from the noisy labels obtained via crowdsourcing.
This paper focuses on objective labeling tasks involving binary choices, meaning that each question or task is associated with a single correct binary answer or label.1 There is a vast literature on the problem of estimation from noisy crowdsourced labels (e.g., [39, 33, 22, 21, 15, 28, 14, 5, 48, 13]). The bulk of this past work is based on the classical Dawid-Skene model [6], in which each worker i is associated with a single scalar parameter qiDS ∈ [0, 1], and it is assumed that the probability that worker i answers any question j correctly is given by the same scalar qiDS. Thus, the Dawid-Skene model imposes a homogeneity condition on the questions, one which is often not satisﬁed in practical
Author email addresses: nihars@cs.cmu.edu, siva@stat.cmu.edu, wainwrig@berkeley.edu. 1In this paper, we use the terms {question, task}, and {answer, label} in an interchangeable manner.
1

applications where some questions may be more diﬃcult than others. We note that the original model by Dawid and Skene [6] also allows for asymmetric errors across diﬀerent classes. In this paper, we focus on the setting with symmetric error probabilities, that has popularly come to be known as the “one-coin Dawid-Skene model”, and has been the focus of much of past literature [22, 21, 15, 5]. Both the asymmetric and symmetric models, however, are governed by restrictive parameter-based assumptions and assume homogeneity of questions.
Accordingly, in this paper, we propose and analyze a more general permutation-based model that allows the noise in the answer to depend on the particular question-worker pair. Within the context of such models, we propose and analyze a variety of estimation algorithms. One possible metric for analysis is the Hamming error, and there is a large body of past work [22, 21, 15, 14, 5, 48, 13] that provide suﬃcient conditions that guarantee zero Hamming error—meaning that every question is answered correctly—with high probability. Although the Hamming error can be suitable for the analysis of Dawid-Skene style models, we argue in the sequel that it is less appropriate for the heterogenous settings studied in this paper. Instead, when tasks have heterogenous diﬃculties, it is more natural to use a weighted metric that also accounts for the underlying diﬃculty of the tasks. Concretely, an estimator should be penalized less for making an error on a question that is intrinsically more diﬃcult. In this paper, we introduce and provide analysis under such a diﬃculty-weighted error metric. From a high-level perspective, the contributions of this paper can be summarized as follows:
• We introduce a new “permutation-based” model for crowd-labeled data, which is considerably richer than the popular Dawid-Skene class of models.
• In order to incorporate the richness in the model, we introduce a new diﬃculty-weighted loss that extends the popular Hamming loss. We prove non-asymptotic upper and lower bounds on the global minimax error under the diﬃculty-weighted loss, sharp up to logarithmic factors, for estimation under the permutation-based model. These bounds match those under the Dawid-Skene model up to logarithmic factors.
• We propose a computationally-eﬃcient estimator, termed the WAN estimator, for the setting where an approximate ordering of the workers in terms of their abilities is known. We show that under the permutation-based model, this estimator has strong guarantees for the 0-1 loss and also achieves the global minimax limits (up to logarithmic factors) for the diﬃculty-reweighted loss.
• We provide a computationally-eﬃcient estimator, termed the OBI-WAN estimator, when no prior information about the workers is known. This estimator achieves strong guarantees for the 0-1 loss under the Dawid-Skene model and an intermediate model, and simultaneously also has guarantees over the much richer permutation-based model thereby establishing its robustness to model speciﬁcation.
• We conduct synthetic simulations as well as real-world experiments using data from the Amazon Mechanical Turk crowdsourcing platform. These experiments reveal a strong performance of the OBI-WAN estimator in practice.
The remainder of this paper is organized as follows. In Section 2, we provide some background, setup the problems we address in this paper, and provide an overview of related literature. Section 3 is devoted to our main results. We present numerical simulations and real-world experiments in Section 4. We present proofs of the claimed theoretical results in Section 5. We conclude the paper with a discussion of future research directions in Section 6.
2

2 Background and model formulation
We begin with some background on existing crowd-labeling models, followed by an introduction to our proposed models; we conclude with a discussion of related work.

2.1 Observation model

Consider a crowdsourcing system that consists of n workers and d questions. We assume every
question has two possible answers, denoted by {−1, +1}, of which exactly one is correct. We let x∗ ∈ {−1, 1}d denote the binary vector of correct answers to all d questions. We model the question-answering via an unknown matrix Q∗ ∈ [0, 1]n×d whose (i, j)th entry, Q∗ij, represents the probability that worker i answers question j correctly. Otherwise, with probability 1 − Q∗ij, worker i gives the incorrect answer to question j. For future reference, note that the (one-coin) Dawid-Skene model involves a special case of such a matrix, namely one of the form Q∗ = qDS1T , where the vector qDS ∈ [0, 1]n corresponds to the vector of correctness probabilities, with a single scalar associated
with each worker.
We denote the response of worker i to question j by a variable Yij ∈ {−1, 0, 1}, where we set Yij = 0 if worker i is not asked question j, and set Yij to the answer (either −1 or 1) provided by the worker otherwise. We also assume that worker i is asked question j with probability pobs ∈ [0, 1], independently for every pair (i, j) ∈ [n] × [d], and that a worker is never asked the same question twice. We also make the standard assumption that given the values of x∗ and Q∗, the entries of Y
are all mutually independent. In summary, we observe a matrix Y which has independent entries
distributed as

x∗j  Yij = −x∗j  0

with probability pobs Q∗ij with probability pobs (1 − Q∗ij)
with probability (1 − pobs).

Given this random matrix Y , our goal is to estimate the binary vector x∗ ∈ {−1, 1}d of true labels. Obtaining non-trivial guarantees for this problem requires that some structure be imposed on
the probability matrix Q∗. The Dawid-Skene model is one form of such structure: it requires that the probability matrix Q∗ be rank one, with identical columns all equal to qDS ∈ Rn. As noted previously, this structural assumption on Q∗ is very strong. It assumes that each worker has a ﬁxed probability of answering a question correctly, and is likely to be violated in settings where some questions are more diﬃcult than others.
Accordingly, in this paper, we study a more general permutation-based model of the following form. We assume that there are two underlying orderings, both of which are unknown to us: ﬁrst, a permutation π∗ : [n] → [n] that orders the n workers in terms of their (latent) abilities, and second, a permutation σ∗ : [d] → [d] that orders the d questions with respect to their (latent) diﬃculties. In terms of these permutations, we assume that the probability matrix Q∗ obeys the following conditions:
• Worker monotonicity: For every pair of workers i and i such that π∗(i) < π∗(i ) and every question j, we have Q∗ij ≥ Q∗i j.
• Question monotonicity: For every pair of questions j and j such that σ∗(j) < σ∗(j ) and every worker i, we have Q∗ij ≥ Q∗ij .
In other words, the permutation-based model assumes the existence of a permutation of the rows and columns such that each row and each column of the permuted matrix Q∗ has non-increasing

3

entries. The rank of the resulting matrix is allowed to be as large as min{n, d}. It is straightforward to verify that the Dawid-Skene model corresponds to a particular type of such probability matrices, restricted to have identical columns.
In summary, we let CPerm denote the set of all possible values of matrix Q∗ under the proposed permutation-based model, that is,
CPerm : = Q ∈ [0, 1]n×d |there exist permutations (π, σ) such that question & worker monotonicity hold .

For future reference, we also use CDS : = Q ∈ CPerm | Q = qDS1T for some qDS ∈ [0, 1]n ,

to denote the subset of such matrices that are realizable under the Dawid-Skene assumption.

It should be noted that none of these models are identiﬁable without further constraints. For

instance, changing x∗ to −x∗ and Q∗ to (11T −Q∗) does not change the distribution of the observation

matrix Y . In the context of the Dawid-Skene model, several papers [22, 21, 14, 48] have resolved

this issue by requiring that n1

n i=1

qiDS

≥

21 + µ for some constant value µ > 0.

Although this

condition resolves the lack of identiﬁability, the underlying assumption—namely that every question

is answerable by a subset of the workers—can be violated in practice. In particular, one frequently

encounters questions that are too diﬃcult to answer by any of the hired workers, and for which

the worker’s answers are near uniformly random (e.g., see the papers [8, 38]). On the other hand,

empirical observations also show that workers in crowdsourcing platforms, as opposed to being

adversarial in nature, at worst provide random answers to labeling tasks [47, 8, 12, 11]. On this

basis, for certain results in the paper, we will consider the regime:

Q∗ij ≥ 12 ∀i ∈ [n], j ∈ [d].

(R1)

Note that neither the condition (R1) nor the condition n1 dominate one another.

n i=1

qiDS

≥

1 2

+µ

from

past

literature

2.2 Evaluating estimators

In this section, we introduce the criteria used to evaluate estimators in this paper. In formal terms, an estimator x is a measurable function that maps any observation matrix Y to a vector in the Boolean hypercube {−1, 1}d. The most popular way of assessing the performance of such an estimator is in terms of its (normalized) Hamming error

∗

1d

∗

dH(x, x ) : = d 1{xj = xj },

(1)

j=1

where 1{xj = x∗j } denotes a binary indicator which takes the value 1 if xj = x∗j , and 0 otherwise. A potential deﬁciency of the Hamming error is that it places a uniform weight on each question. As mentioned earlier, there are applications of crowdsourcing in which some subset of the questions are very diﬃcult, and no hired worker can answer reliably. In such settings, any estimator will have an inﬂated Hamming error, not due to any particular deﬁciencies of the estimator, but rather due to the intrinsic hardness of the assigned collection of questions. This error inﬂation will obscure possible diﬀerences between estimators.

4

Our goal in choosing an appropriate loss function is to allow for evaluation and comparison of various estimators. Thus, with the aforementioned issue in mind, we an alternative error measure that weights the Hamming error with the diﬃculty of each task. A more general class of error measures takes the form

∗ 1d

∗

∗

∗

LQ∗(x, x ) = d 1{xj = xj }Ψ(Q1j, . . . , Qnj),

(2)

j=1

for some function Ψ : [0, 1]n → R+ which captures the diﬃculty of estimating the answer to a question.

The Q∗-loss: In order to choose a suitable function Ψ, we note that past work on the Dawid-Skene model [22, 21, 15, 14, 5] has shown that the quantity

1n DS

2

n (2qi − 1) ,

(3)

i=1

popularly known as the collective intelligence of the crowd, is central to characterizing the overall diﬃculty of the crowd-sourcing problem under the Dawid-Skene assumption. A natural generalization, then, is to consider the weights

∗

∗

1n

∗

2

Ψ(Q1j, . . . , Qnj) = n

2Qij − 1

for each task j ∈ [d],

(4a)

i=1

which characterizes the diﬃculty of task j for a given collection of workers. This choice gives rise to the Q∗-loss function

LQ∗ (x, x∗) : = 1 d d
j=1

∗ 1n

∗

2

1{xj = xj } n (2Qij − 1)

i=1

(4b)

= 1 |||(Q∗ − 1 11T ) diag(x − x∗)|||2,

(4c)

dn

2

F

where diag(x − x∗) denotes the matrix in Rd×d whose diagonal entries are given by the vector x − x∗. Note that under the Dawid-Skene model (in which Q∗ = qDS1T ), this loss function reduces to

∗

1n DS

2 1d

∗

LQ∗(x, x ) = n (2qi − 1) d 1{xj = xj } ,

(5)

i=1

j=1

dH(x,x∗)

corresponding to the normalized Hamming error rescaled by the collective intelligence.
For future reference, let us summarize some properties of the function LQ∗: (a) it is symmetric in its arguments (x∗, x), and satisﬁes the triangle inequality; (b) it takes values in the interval [0, 1]; and (c) if for every question j ∈ [d], there exists a worker ∈ [n] such that Q∗j = 12 , then LQ∗ deﬁnes a metric; if not, it deﬁnes a pseudo-metric.

5

Regime of interest: In this paper, we focus on understanding the minimax risk as well as the risk of various computationally eﬃcient estimators. We work in a non-asymptotic framework where we are interested in evaluating the risk in terms of the triplet (n, d, pobs). We assume that pobs ≥ n1 , which ensures that on average, at least one worker answers any question. We also operate in the regime d ≥ n, which is commonplace in practical applications. Indeed, as also noted in earlier works [48], typical medium or large-scale crowdsourcing tasks employ tens to hundreds of workers, while the number of questions is on the order of hundreds to many thousands. We assume that the value of pobs is known. This is a mild assumption since it is straightforward to estimate pobs very accurately using its empirical expectation. We encompass the aforementioned conditions as the regime:

1 pobs ≥ n and d ≥ n.

(R2)

2.3 Related work
Having set up our model and notation, let us now relate it to past work in the area. For the problem of crowd labeling, the Dawid-Skene model [6] is the dominant paradigm, and has been widely studied [22, 21, 15, 28, 14, 5, 48]. Some papers have studied models that generalize the Dawid-Skene model. In a recent work, Khetan and Oh [23] analyze an extension of the Dawid-Skene model where a vector q ∈ Rn, capturing the abilities of the workers, is supplemented with a second vector h∗ ∈ [0, 1]d, and the likelihood of worker i correctly answering question j is set as qi(1 − h∗j + (1 − qi)h∗j ). Although this model now has (n + d) parameters instead of just n as in the Dawid-Skene model, it retains parametric-type assumptions. Each worker and each question is described by a single parameter, and in this model the probability of correctness takes a speciﬁc form governed by these parameters. In contrast, in the permutation-based model each worker-question pair is described by a single parameter. Our permutation-based model forms a strict superset of this class. Zhou et al. [50, 49] propose a model based on a certain minimax entropy principle, whereas Whitehill et al. [46] propose a parameter-based model that also incorporates question diﬃculties. However, the algorithms proposed in these papers [50, 49, 46] have yet to be rigorously analyzed.
In this paper, we introduce a class of models that are considerably more ﬂexible than the Dawid-Skene model, as well as a novel algorithm for estimation in such models, which we equip with some theoretical guarantees. The present paper also introduces another new algorithm for the setting in which an ordering of the workers in terms of their abilities is approximately known, for instance, based on some initial test. To be clear, the results of this paper have some limitations as compared to past work on the Dawid-Skene model, and we hope that these limitations will be removed in future work on the permutation-based model. Concretely, while the present paper addresses the setting of binary labels with symmetric error probabilities, several of these prior works also address settings with more than two classes, and where the probability of error of a worker may be asymmetric across the classes. The results presented in this paper have logarithmic factor gaps, that is, the ‘optimal’ results are optimal up to logarithmic factors, as stated throughout the paper. For the Dawid-Skene model, particularly under sparse observations, the past works [22, 21, 14, 48, 23] have results with sharper logarithmic factors. Finally, the guarantees provided in past results have error exponents that adapt to the underlying signal, whereas ours do not.
A related problem in the context of crowdsourcing is to estimate pairwise outcome probabilities from pairwise comparison data. In our past work [34, 35], we have considered this problem under an assumption of “strong stochastic transitivity (SST)”, which is a regularity condition related to the permutation-based model of this paper. Accordingly, parts of our proofs make use of metric entropy calculations from this past work. Unlike our previous work, the current paper involves an unknown

6

set of labels, as well as a signiﬁcantly diﬀerent observation model: in particular, the observed data couples the unknown matrix Q∗ with the unknown labels. Moreover, rather than estimating the unknown probabilities Q∗, our primary goal in this paper is to estimate these underlying labels, for which signiﬁcantly diﬀerent algorithmic ideas and proof techniques are required.
Finally, the problem of aggregating labels of crowdsourcing workers is conceptually similar to that of combining classiﬁers in an unsupervised context, each solving multiple classiﬁcation problems [32, 19]. Our work has implications for this line of research as well.

3 Main results
We now turn to the statement of our main results. We use c, cU, cL, c0, cH to denote positive universal constants that are independent of all other problem parameters. Recall that the Q∗-loss takes values in the interval [0, 1].

3.1 Minimax risk for estimation under the permutation-based model

We begin by proving sharp upper and lower bounds on the minimax risk for the permutation-based model CPerm. The upper bound is obtained via an analysis of the following least squares estimator

(xLS, QLS) ∈

arg min

|||p−ob1s Y − (2Q − 11T ) diag(x)|||2F.

(6)

x∈{−1,1}d, Q∈CPerm

In order to provide some intuition for this estimator, one can show (see the proof of Theorem 1(a) for details) that the unknowns x∗ and Q∗ are related to the mean of the observed matrix Y via the equality E[Y ] = pobs(2Q∗ − 11T ) diag(x∗). Consequently, the estimate (xLS, QLS) computed via the program (6) equals the true solution (x∗, Q∗) when Y is replaced by its population version.
We do not know of a computationally eﬃcient way to solve the optimization problem (6). Despite
this computational issue, our statistical analysis of this estimator serves to provide a benchmark for
comparing other computationally-eﬃcient estimators, to be discussed in the sequel. The following
theoretical guarantees hold in the regime (R1)∩(R2):

Theorem 1. (a) For any binary vector x∗ ∈ {−1, 1}d and any matrix Q∗ ∈ CPerm, the least squares estimator xLS has error at most

LQ∗(x , x∗) ≤ c 1 (log d)2,

(7a)

LS

U npobs

with probability at least 1 − e−cHd log(dn).
(b) There exists a matrix Q ∈ CDS such that any estimator x (which may even know the value of Q) has error at least

sup

E[L (x, x∗)] ≥ c

1 .

Q x∗∈{−1,1}d

L npobs

(7b)

We provide the this theorem in Sections 5.1 and 5.2. As a consequence of this result, we see that in terms of the (global) minimax risk under the Q∗-loss, there is only a polylogarithmic factor diﬀerence between the Dawid-Skene and the permutation-based models, despite the permutation-based model being considerably richer.
We note that while the upper bound of Theorem 1(a) is quite involved, the lower bound of Theorem 1(b) is a straightforward result of a simple “worst case” construction. This suggests that

7

the global minimax error be augmented with an investigation of local minimax errors under various subclasses of CPerm and various notions and values of the signal to noise ratio, which we leave as important future work.
The least squares estimator analyzed above also yields an accurate estimate of the probability matrix Q∗ in the Frobenius norm, useful in settings where the calibration of workers or questions might be of interest. Again, this result holds in the regime (R1)∩(R2):

Corollary 1. (a) For any x∗ ∈ {−1, 1}d and any Q∗ ∈ CPerm,, the least squares estimate QLS has error at most

1 |||Q

− Q∗|||2 ≤ c

1

log2 d,

(8a)

dn LS

F U npobs

with probability at least 1 − e−cHd log(dn). (b) Conversely, for any answer vector x∗ ∈ {−1, 1}d, any estimator Q (which is allowed to know the value of x∗) has error at least

sup

E[ 1 |||Q − Q∗|||2] ≥ c

1 .

Q∗∈CPerm dn

F

L npobs

(8b)

Please see Sections 5.3 and 5.4 for the proof of this corollary. We do not know if there exist computationally-eﬃcient estimators that can achieve the upper
bound on the sample complexity established in Theorem 1 over the entire permutation-based model class. In the following sections, we design and analyze polynomial-time estimators that have interesting (but suboptimal) guarantees over the permutation-based model and also useful guarantees over popular subclasses of the permutation-based model.

3.2 The WAN estimator: When workers’ ordering is (approximately) known

Several organizations employ crowdsourcing workers only after a thorough testing and calibration process. Motivated by this fact, we now turn to the study of the setting in which the workers are calibrated, in the sense that it is known how they are ordered in terms of their respective abilities. More formally, recall from Section 2.1 that any matrix Q∗ ∈ CPerm is associated with two permutations: a permutation of the workers in terms of their abilities, and a permutation of the questions in terms of their diﬃculty. In this section, we assume that the permutation of the workers is (approximately) known to the estimation algorithm. Note that the estimator does not know the permutation of the questions, nor does it know the values of the entries of Q∗.
Given a permutation π of the workers, our estimator consists of two steps, which we refer to as Windowing and Aggregating Na¨ıvely, respectively, and accordingly term the procedure as the WAN estimator: • Step 1 (Windowing): Compute the integer

kWAN ∈

arg max

k∈{p−ob1s log1.5(dn),...,n}

1
j∈[d]

Yπ−1(i)j ≥
i∈[k]

kpobs log1.5(dn) ,

(9a)

where ties in the argmax are broken arbitrarily. • Step 2 (Aggregating Na¨ıvely): Set xWAN(π) as a majority vote of the best kWAN workers—that is

[xWAN(π)]j ∈ arg max
b∈{−1,1}

kWAN
1{Yπ−1(i)j = b}
i=1

for every j ∈ [d].

(9b)

8

The windowing step ﬁnds a value kWAN such that the answers of the best kWAN workers to most questions are signiﬁcantly biased towards one of the options, thereby indicating that these workers are knowledgeable—or at least, are in agreement with each other. The second step then simply takes a majority vote of this set of the best kWAN workers. We remark that it is important to choose an appropriate value of kWAN (as done in Step 1), since an overly large value could include many random workers, thereby increasing the noise in the input to the second step; on the ﬂip side, choosing too small a value could eliminate too much of the “signal”. Both steps can be carried out in time O(nd).
For the case when π is an approximate ordering, we now establish a bound on the error of the WAN estimator. For every j ∈ [d], let Q∗j denote the jth column of Q∗; for any ordering π of the workers, Qπj denote the vector obtained by permuting the entries of Q∗j in the order given by π, that is, with the ﬁrst entry of Qπj corresponding to the best worker according to π, and so on. Also recall the notation π∗ representing the true permutation of the workers in terms of their actual abilities. As with all of our theoretical, results, the following claim holds in the regime (R1)∩(R2):
Theorem 2. For any matrix Q∗ ∈ CPerm and any binary vector x∗ ∈ {−1, 1}d, suppose that the WAN estimator is provided with the permutation π of workers. Consider the subset of the questions given by

log1.5(dn)

J : = j ∈ [d] | ∃kj ≥

s.t.

kj 1 3 )≥
(Q∗π−1(i)j −

pobs i=1

24

kj log1.5(dn) . pobs

(10a)

Then the WAN estimator correctly estimates the labels of all questions in set J with high probability:

P [xWAN(π)]j = x∗j for all j ∈ J ≥ 1 − e−cH log1.5(dn).

(10b)

We provide the proof of Theorem 2 in Section 5.5. At a high level, the theorem says that all questions that have some reasonable signal are estimated correctly by the WAN estimator. To gain intuition behind the notion of signal in (10a), let us consider pobs = 1 and consider the majority voting algorithm (that is, taking a majority vote over all n workers). A straightforward applica√tion of Hoeﬀding’s inequality yields that for any question j ∈ [d], the condition ni=1(Q∗ij − 12 ) = Ω( n) is suﬃcient for the majority voting estimator to estimate x∗j correctly (with high probability). Furthermore, in the appendix, we also show that there exist matrices Q∗ where this condition is also necessary. Theorem 2 says that the WAN algorithm can estimate a question correctly if there exists some subset of “top” workers (according to π), such that this condition for majority voting applies when restricted to only the answers from these workers.
While Theorem 2 (as well as other results in the sequel) focuses on exact recovery with high probability, we note that alternatively directly bounding the expected Hamming error may yield guarantees that go beyond what is captured in this result. We leave this interesting problem for future work.
Theorem 2 has an immediate corollary, one which provides guarantees on the WAN estimator in terms of certain norms of the matrix Q∗ which may be more interpretable, and also provides a guarantee on the Q∗-loss incurred by the WAN estimator. Again, these results hold in the regime (R1)∩(R2).

Corollary 2. For any matrix Q∗ ∈ CPerm and any binary vector x∗ ∈ {−1, 1}d, suppose that the WAN estimator is provided with the permutation π of workers. Then for every question j ∈ [d] such
that

∗ 1 2 5 log2.5(dn)

Qj − 2 2 ≥

, and pobs

Qπj − Qπj ∗ 2 ≤

Q∗j − 21 2 , 9 log(dn)

(11a)

9

we have

P([xWAN(π)]j = x∗j ) ≥ 1 − e−cH log1.5(dn).

(11b)

Consequently, if π is the correct permutation of the workers, then with probability at least 1 − e−cH log1.5(dn), we have

LQ∗(x (π), x∗) ≤ c 1 log2.5 d.

WAN

U npobs

(11c)

Please see Section 5.6 for the proof of Corollary 2.

The conditions (11a) required for the result of Corollary 2 are sharp up to logarithmic factors in

the following sense. The required approximation guarantee Qπj − Qπj ∗ 2 ≤ √Q9∗jlo−g(12dn2) , if weakened

to

Qπj − Qπj ∗

2≤2

Q∗j

−

1 2

2, would allow for any arbitrary permutation π.

This is because every

permutation π satisﬁes

Qπj − Qπj ∗ 2 ≤

Qπj

−

1 2

2+

Qπj ∗

−

1 2

2=2

Q∗j

−

1 2

2.

Secondly, there

exist constants c0 > 0 and cL > 0 such that if one were guaranteed a lower bound of only pco0bs on

Q∗j

−

1 2

2 2

instead

of

the

stated

condition

of

5 logp2.5(dn) , then there exists a Q∗ ∈ CDS satisfying this

weaker

condition

such

that

any

estimator

x

obs
incurs

an

error

at

least

P(xj

=

x∗j )

≥

cL.

Furthermore,

this lower bound holds not only when the ordering of workers is exactly known, but even when the

entire matrix Q∗ is known. The proof for this claim follows from the construction in the proof of

Theorem 1(b).

At this point, we recall from Theorem 1(b) the lower bound on the estimation error in the

Q∗-loss for any estimator. This lower bound applies to estimators that know not only the ordering

of the workers, but also the entire matrix Q∗. This lower bound matches the upper bound (11c) of

Corollary 2, and the two results in conjunction imply that the bound (11c) is sharp up to logarithmic

factors.

We conclude this section with a key insight obtained from our analysis of the WAN estimator.

Remark 1 (Insight for unknown worker ordering problem). The aforementioned results for the
WAN algorithm have the following useful implication for the setting when the ordering of workers is unknown, under either of the models CDS or CPerm. For any matrix Q∗ ∈ CPerm, there exists a set of workers SQ∗ ⊆ [n] such that the majority vote of the answers of the workers in SQ∗ incurs a small risk. Consequently, it suﬃces to design an estimator that identiﬁes a set of good workers and
computes a majority vote of their answers. The estimator need not attempt to infer the values of the entries of Q∗, as is otherwise required, for instance, to compute maximum likelihood estimates.

The estimator proposed in the next section is based on the observation in Remark 1.

3.3 The OBI-WAN estimator
In this section, we return to the setting where the ordering of the workers is unknown. We begin by presenting a computationally eﬃcient estimator.
Our proposed estimator operates in two steps. The ﬁrst step performs an Ordering Based on Inner-products (OBI), that is, computes an ordering of the workers based on an inner product with the data. The second step calls upon the WAN estimator from Section 3.2 with this ordering. We thus term our proposed estimator as the OBI-WAN estimator, x . OBI-WAN In order to make its description precise, we augment the notation of the WAN estimator xWAN(π) to let xWAN(π, Y ) to denote the estimate given by xWAN(π) operating on Y when given the permutation π of workers.

10

An important technical issue is that re-using the observe data Y to both determine an appropriate ordering of workers as well as to estimate the desired answers, results in a violation of important independence assumptions. We resolve this diﬃculty by partitioning the set of questions into two sets, and using the ordering estimated from one set to estimate the desired answers for the other set and vice versa. We provide a careful error analysis for this partitioning-based estimator in the sequel. In more precise terms, the OBI-WAN estimator xOBI-WAN is deﬁned by the following three steps:
• Step 0 (preliminary): Split the set of d questions into two sets, T0 and T1, with every question assigned to one of the two sets uniformly at random. Let Y0 and Y1 denote the corresponding submatrices of Y , containing the columns of Y associated to questions in T0 and T1 respectively.
• Step 1 (OBI): For ∈ {0, 1}, let
u ∈ arg max Y T u 2
u 2=1
denote the top eigenvector of Y Y T ; in order to resolve the global sign ambiguity of eigenvectors, we choose the global sign so that i∈[n][u ]2i 1{[u ]i > 0} ≥ i∈[n][u ]2i 1{[u ]i < 0}. Let π be the permutation of the n workers in order of the respective entries of u (with ties broken arbitrarily).
• Step 2 (WAN): Compute the quantities
xOBI-WAN(T0) : = xWAN(Y0, π1), and xOBI-WAN(T1) : = xWAN(Y1, π0),
corresponding to estimates of the answers for questions in the sets T0 and T1, respectively.
This completes the description of the OBI-WAN algorithm. We note that with regard to the use of the singular vectors of the observed data in the OBI step, previous works [22, 15, 32, 48, 19] also use singular vectors to estimate properties of the underlying parameters in crowdsourcing. In these previous works, this step is motivated by the fact that the spectrum of the population matrix E[Y Y T ] (or its mean-centered counterpart), can be related to the parameters that underlie the model. In the next three subsections, we provide guarantees for our OBI-WAN estimator under three model classes. Importantly, the guarantees for OBI-WAN hold simultaneously for all model classes, and the estimator does not know the true class to which the data actually belongs.
3.3.1 Guarantees for OBI-WAN under an intermediate model
In addition to the Dawid-Skene and the permutation-based models introduced earlier, we study the estimation problem in an intermediate model that lies between these two models. This intermediate model introduces a parameter h∗j ∈ [0, 1] that captures the diﬃculty of each question j ∈ [d], along with parameters q ∈ Rn associated with the workers as in the Dawid-Skene model. Under this intermediate model, the probability that worker i ∈ [n] correctly answers question j ∈ [d] (when the worker is asked the question) is given by
P(Yij = x∗j ) = qi(1 − h∗j ) + 12 h∗j , ∀ (i, j) such that Yij = 0. (12) Intuitively, the parameter h∗j corresponds to the diﬃculty of question j. When h∗j = 1, the worker is purely stochastic and provides random guesses, while for smaller values of h∗j the worker is more likely to provide a correct answer.
11

This modeling assumption leads to the class

CInt : = Q = q(1 − h)T + 1 1hT | for some q ∈ [0, 1]n, h ∈ [0, 1]d . 2

Note that we have the nested relation CDS ⊂ CInt; the Dawid-Skene model is a special case of CInt corresponding to h = 0. In the regime (R1), we further have CDS ⊂ CInt ⊂ CPerm.
Up to a bijective transformation of the parameters, the model (12) is identical to a recent model
proposed independently by Khetan and Oh [23], where the probability of a correct answer is assumed to be qi(1 − h∗j ) + (1 − qi)h∗j . The two models however arise from diﬀerent conceptual motivations: Khetan and Oh consider the probability of correctness as a convex combination of the worker’s
behavior qi and the opposite behavior (1 − qi), whereas our consideration of rarity of adversarial behavior leads to the probability of correctness set as a convex combination of the worker’s behavior qi and random responses 12 .
We now provide exact-recovery guarantees for the OBI-WAN estimator under this intermediate
model. As with our other results, the following theorem applies to the regime (R1)∩(R2):

Theorem 3. Consider any binary vector x∗ ∈ {−1, 1}d and any matrix Q∗ ∈ CInt associated with

vectors (q, h) satisfying q − 1 2 1 − h∗ 2 ≥ cd log2.5(dn) for a large enough constant c. Then for

22

2

pobs

every question j ∈ [d] such that

we have

∗2

1 2 5 log2.5(dn)

(1 − hj )

q− 2

2≥

, pobs

P([xOBI-WAN]j = x∗j ) ≥ 1 − e−cH log1.5(dn).

(13a) (13b)

Please see Section 5.7 for the proof of this theorem. See also Theorem 4 in the sequel, which provides a matching lower bound (up to logarithmic factors) for the special case of h∗ = 0.
We now provide some intuition about the OBI part of the OBI-WAN estimator, and we do so
in the context of Theorem 3. For simplicity in this explanation, let us ignore the sample splitting
step (Step 0) of the OBI-WAN algorithm and assume the OBI step (Step 1) is applied to the
entire observed data Y . Then under the model CInt, we can rewrite the observation matrix as Y = pobs(2q − 1)(1 − h)T diag(x∗) + W , where W is a “noise” matrix and pobs(2q − 1)(1 − h)T diag(x∗) is the “signal” in the observed data. In this representation, the signal is a matrix of rank one,
its top left singular vector equals 2q − 1 (up to a scaling), and the “magnitude of the signal” is |||pobs(2q − 1)(1 − h)T diag(x∗)|||op = pobs 2q − 1 2 1 − h 2. Furthermore, we show in the proof of Theorem 3 that the “magnitude of the noise” is bounded as |||W |||op ≤ c dpobs polylog(dn) with high probability. Consequently when the magnitude of signal exceeds the noise (condition stated in
the beginning of Theorem 3), the top left singular vector of Y approximately captures the ordering
of entries in (2q − 1) that represent the worker abilities. The following corollary now upper bounds the Q∗-loss for the OBI-WAN estimator under the
intermediate model in the regime (R1)∩(R2):

Corollary 3. For any Q∗ ∈ CInt and any vector x∗ ∈ {−1, 1}d, the estimate xOBI-WAN has error at most

LQ∗ (x

, x∗) ≤ c 1 log2.5 d,

OBI-WAN

U npobs

(14)

with probability at least 1 − e−cH log1.5(dn).

Please see Section 5.8 for the proof of this corollary. A comparison with the lower bound of Theorem 1(b) reveals that the bound (14) is tight up to logarithmic factors.

12

3.3.2 Guarantees for OBI-WAN under the Dawid-Skene model

In this section, we present results relating the performance of the OBI-WAN estimator under the

Dawid-Skene model. Unlike the rest of the paper, in this section the simplicity of the model allows

us to generalize in another direction: handling adversarial workers, that is, not being restricted to

regime

(R1)

and

allowing

qiDS

<

1 2

for

some

workers

i

∈

[n].

We introduce some additional notation. For the vector qDS ∈ [0, 1]n, we deﬁne two associated

vectors qDS+, qDS− ∈ [0, 1]n as qiDS+ = max{qiDS, 21 } and qiDS− = min{qiDS, 12 } for every i ∈ [n]. Then we have (qDS − 12 ) = (qDS+ − 21 ) + (qDS− − 21 ), with qDS+ representing normal workers and qDS−

representing adversarial workers who are more inclined to provide incorrect answers. The following

result holds in the regime (R2):

Theorem 4. Consider any Dawid-Skene matrix of the form Q∗ = qDS1T for some qDS ∈ [0, 1]n. Then:

(a) If qDS+ − 12 2 ≥ qDS− − 21 2 + 4 logp2o.b5s(dn) and (qDS − 12 )T 1 ≥ 0, then for any x∗ ∈ {−1, 1}d, the OBI-WAN estimator satisﬁes

P(xOBI-WAN = x∗) ≥ 1 − e−cH log1.5(dn).

(15a)

(b) Conversely, there exists a positive universal constant c such that for any qDS ∈ [ 110 , 190 ]n with qDS − 12 2 ≤ pocbs , any estimator x has (normalized) Hamming error at least

d1

∗

1

sup E

d 1{xi = xi }

≥. 10

x∗∈{−1,1}d i=1

(15b)

The proofs of the two parts of Theorem 4 are provided in Sections 5.9 and 5.10.

A couple of remarks are in order. For the following discussion, consider the two mild conditions qDS+ − 12 2 ≥ 1.01 qDS− − 12 2 and (qDS − 12 )T 1 > 0. We claim that under these mild conditions, the OBI-WAN estimator is optimal up to logarithmic factors. To see this, ﬁrst observe that the
lower bound in Theorem 4(b) implies that for any non-trivial recovery guarantee to hold, it must be the case that qDS − 21 2 > pocbs for some positive universal constant c. Now suppose that
qDS − 12 2 > c logp2o.b5s(dn) for a large enough positive constant c ; observe that this condition is only a logarithmic factor away from the necessary condition. Then under the mild aforementioned conditions, we have qDS+ − 12 2 ≥ qDS− − 21 2 + 4 logp2o.b5s(dn) . Part (a) of Theorem 4 then guarantees that the OBI-WAN estimator recovers the true answers x∗ with high probability.
Secondly, an application of Theorem 4 is to the setting that has been the focus of our paper, where we have no adversarial workers. In this case, we have qDS− = 0 and qDS+ = qDS, and the
upper and lower bounds match upto a logarithmic factor. The upper bound shows that when qDS − 12 2 ≥ 4 logp2o.b5s(dn) , the Hamming error is vanishingly small, whereas the lower bound shows
that there is a universal constant c such that the Hamming error is essentially as large as possible when qDS − 12 2 ≤ pocbs .

3.3.3 Guarantees for OBI-WAN under the permutation-based model
The previous two subsections provided strong guarantees for OBI-WAN for exact recovery and the Q∗-loss under the Dawid-Skene and intermediate models. A natural question that arises then is

13

how robust is OBI-WAN to mismatches with respect to the Dawid-Skene and intermediate models. We analyze OBI-WAN under the considerably richer permutation-based model class in this section.

Proposition 1. Consider any matrix Q∗ ∈ CPerm and any binary vector x∗ ∈ {−1, 1}d. For every question j ∈ [d] that such that

n∗1 3

(Qij

−

) 2

≥

4

i=1

n log1.5(dn), pobs

(16a)

the OBI-WAN estimator satisﬁes

P([xOBI-WAN]j = x∗j ) ≥ 1 − e−cH log1.5(dn).

(16b)

Consequently for any Q∗ ∈ CPerm and any x∗ ∈ {−1, 1}d, with probability at least 1 − e−cH log1.5(dn), the estimator incurs a Q∗-loss of at most

LQ∗(x , x∗) ≤ c √ 1 log d.

OBI-WAN

U npobs

(17)

See Section 5.11 for the proof of this result.
It is well known that the majority voting estimator is highly robust to model speciﬁcation (see,
for instance, the discussion in [14, Section 4.2]); this robustness perhaps underlies its popularity in practice. In the appendix, we show that the majority voting estimator achieves a rate Ω( √n1pobs ) in terms of the Q∗-loss in the worst case over the permutation-based model. Thus the guarantee (17)
for OBI-WAN matches the lower bound for majority voting. However, importantly, in addition to
this guarantee over CPerm, the OBI-WAN estimator simultaneously also achieves the strong exact recovery and Q∗-loss guarantees of Theorem 3, Corollary 3, and Theorem 4 over the simpler models
CInt and CDS.

4 Experiments
In this section, we report the results of a suite of experiments, on both synthetic and real-world data, so as to evaluate the OBI-WAN estimator which was introduced in Section 3.3. We compare OBI-WAN to the Spectral-EM estimator due to Zhang et al. [48], which to the best of our knowledge, has the strongest established guarantees in the literature. For the Spectral-EM estimator, we used an implementation provided by the authors of the paper [48]. The code for the OBI-WAN estimator as well as the constituent WAN estimator is freely available on the ﬁrst author’s website.

4.1 Simulations
We ﬁrst conduct synthetic simulations to evaluate various aspects of the algorithms. We conduct six sets of simulations as detailed below. The results from our simulations are plotted in Figure 1. The plots in the six panels (a) through (f) of the ﬁgure are discussed below.

(a)

Easy:

Q∗

= qDS1T

∈ CDS

where

qiDS

=

9 10

if

i<

n2 ,

and

qiDS

=

1 2

otherwise.

The

parameter

n

is

varied, and the regime of operation is (d = n, pobs = 1). In this setting, both estimators correctly

recover x∗.

14

0.0040 0.0030

OBI-WAN Spectral-EM

0.0020

0.0010

0.00010000 4000 700010000
n
(a) Easy

10-1

0.0040 0.0030 0.0020 0.0010 0.00010000 4000 700010000
n
(b) Few smart
0.012

0.09
0.06
0.03
0.010000 4000 700010000
n
(c) Adversarial
0.012

10-2

0.008

0.008

10-3

0.004

0.004

101-4000 4000 700010000
n
(d) In CPerm\CInt

0.0000.05 0.20 0.35 0.50
pobs
(e) Minimax lower bound

0.0000.05 0.20 0.35 0.50
pobs
(f) Super sparse

Figure 1: Results from numerical simulations comparing the Spectral-EM algorithm (circular markers) with OBI-WAN (square markers). The plots in panels (a)-(d) measure the Q∗-loss as a function of n, and the plots in panels (e)-(f) measure the Q∗-loss as a function of pobs. Each point is an average of over 20 trials. Recall that when Q∗ follows the Dawid-Skene model, as in panels (a)-(c), (e)-(f), the Hamming error is proportional to the Q∗-loss. Also note that the Y-axis of panel (d) is plotted on a logarithmic scale.
(b) Few smart: Q∗ = qDS1T ∈ CDS where qiDS = 190 if i < √n, and qiDS = 12 otherwise. The parameter n is varied, and the regime of operation (d = n, pobs = 1). Even though the data is drawn from the Dawid-Skene model, the error of Spectral-EM is much higher than that of the OBI-WAN estimator. Recall that the OBI-WAN estimator has guarantees of recovery over the entire Dawid-Skene class, unlike the estimators in prior literature. (c) Adversarial: Q∗ = qDS1T ∈ CDS where qiDS = 190 if i < n4 + √n, qiDS = 110 if i > 34n , and qiDS = 12 otherwise. The parameter n is varied, and the regime of operation is (d = n, pobs = 1). This set of simulations moves beyond the assumption that the entries of Q∗ are lower bounded by 12 , and allows for adversarial workers. The OBI-WAN estimator is successful in such a setting as well. (d) In CPerm but outside CInt: Q∗ij = 190 if (i < √n or j < d2 ), and Q∗ij = 12 otherwise. The
15

parameter n is varied, and the regime of operation is (d = n, pobs = 1). Here we have Q∗ ∈ CPerm\CInt. The Q∗-loss incurred by the OBI-WAN estimator decays as √1n , whereas the Q∗-loss
of Spectral-EM remains a constant.

(e) Minimax lower bound: Q∗ = qDS1T ∈ CDS where qiDS = 190 if i ≤ po5bs and qiDS = 12 otherwise. The parameter pobs is varied, and the regime of operation is (d = 1000, n = 1000). This setting is the cause of the minimax lower bound of Theorem 1(b). The error of both estimators, in this case, behaves in an almost identical manner with a scaling of po1bs .

(f) Super sparse:

Q∗ = qDS1T

∈

CDS

where

qiDS

=

9 10

if

i

≤

n 10

and

qiDS

=

1 2

otherwise.

The

parameter pobs is varied, and the regime of operation is (d = 1000, n = 1000). We see that the

OBI-WAN estimator incurs a relatively higher error when data is very sparse — more generally, we have observed a higher error when pobs = o( log2n(dn) ), and this gap is also reﬂected in our upper bounds for the OBI-WAN estimator in Theorem 3(a) and Theorem 4(a) that are loose by precisely

a polylogarithmic factor as compared to the associated lower bounds.

The relative beneﬁts and disadvantages of of the proposed OBI-WAN estimator, as observed from the simulations, may be summarized as follows. In terms of limitations, the error of OBI-WAN is higher than prior works when pobs is small (as observed in the super-sparse case) or when n and d are small (for instance, less than 200). On the positive side, the simulations reveal that the OBI-WAN estimator leads to accurate estimates in a variety of settings, providing guarantees over the CDS and CInt classes, and demonstrating signiﬁcant robustness in more general settings in comparison to the best known estimator in the literature.

4.2 Real-world crowdsourcing data
In this section we describe a set of six experiments conducted using real-world data from the Amazon Mechanical Turk crowdsourcing platform, ranging from visual recognition to knowledge elicitation. The experiments involved more than 200 workers in total. In each experiment, workers are asked to answer a number of questions, an we then employ statistical aggregation algorithms to estimate the ground truth answers. The results of these experiments are shown in Figure 2. As before, we compare the OBI-WAN estimator with the Spectral-EM estimator [48].
We now describe more details regarding the experiments. The error bars in each ﬁgure represent the standard error of the mean. The plots and error bars shown in each ﬁgure are obtained via 300 iterations per experiment of subsampling the worker’s answers with pobs = 1/3 and executing the two algorithms on the subsampled data.

(a) Dysplasia The experiment was based on 48 pictures of (biological) cells to workers. They had to classify each image as either “Mild dysplasia” where the ratio of the nucleus’ area to cytoplasm’s area is less than 33%, or as “severe dysplasia” where the ratio of the nucleus’ area to cytoplasm’s area is more than 33%. The images and the ground truth were obtained from the DTU/Herlev Pap Smear Database 2005 [20]. We collected responses from a total of 41 workers from Amazon Mechanical Turk, and Figure 2a depicts the results of this experiment. The data is freely available for download on the ﬁrst author’s website.

(b) Bridges The experiment was based on 21 images of bridges, and the task for any worker was to classify each image as either the golden bridge or not. The data for this experiment was collected in past work [38] from a total of 35 workers from Amazon Mechanical Turk. Figure 2b depicts the results from this data.

16

Spectral-EM

OBI-WAN

(Normalized) Hamming error (Normalized) Hamming error

100 10 1 10 2 10 3 10 4
(a) Dysplasia

100 10 1 10 2 10 3 10 4
(b) Bridges

100 10 1 10 2 10 3 10 4
(c) Dogs

100 10 1 10 2 10 3 10 4
(d) Flags

100 10 1 10 2 10 3 10 4
(e) People

100 10 1 10 2 10 3 10 4
(f) Textures

Figure 2: Results from experiments on the Amazon Mechanical Turk crowdsourcing platform comparing the Spectral-EM algorithm (left bars) with OBI-WAN (right bars). The y-axes of the plots are on a logarithmic scale and represent the Hamming error normalized by the number of questions (lower is better).

(c) Dogs The experiment comprised 85 images of dogs (from [24, 7]), and each worker was asked to identify the breed of each dog from ten provided options. The data was collected in past work [38] from a total of 35 workers from Amazon Mechanical Turk. The data is converted to binary choice form by choosing one breed uniformly at random in each iteration and considering a binary-choice task of identifying whether or not the dogs belong to this breed. Figure 2c plots the results of the experiments.
(d) Flags In this experiment, each worker was shown a series of 126 ﬂags. Each question required the worker to identify if a displayed ﬂag belonged to a place in Africa, Asia/Oceania, Europe, or neither of these. We use the data collected in the past work [38] which contains responses of 35 workers to all of the questions. We convert this data into binary choice format in the same manner as the dogs experiment above. Finally, we plot the results from this experiment in Figure 2d.
(e) People In this experiment, the names of 20 personalities were provided and the worker were asked to classify whether they were ever the President of the USA, President of India, Prime Minister of Canada, or neither of these. Responses from 35 workers were collected in past work [38], and we

17

convert this data to binary choice as per the aforementioned procedure. Figure 2e plots the results of this experiment.
(f) Textures As a ﬁnal experiment, we evaluated the performance of the algorithms on workers’ classiﬁcation of textures. Speciﬁcally, workers were asked to classify 24 images from the dataset [26, Dataset 1: Textured surfaces] into one of eight possible textures. We use the responses of the workers collected in [38], with a conversion to binary-choice as described above. The aggregate results from executing the two algorithms are depicted in Figure 2f.
All in all, the experiments reveal that OBI-WAN compares favorably to Spectral-EM.

5 Proofs
In this section, we present the proofs of main theoretical results. In the proofs, we use c, c1, c etc. to denote positive universal constants, and ignore ﬂoors and ceilings unless critical to the proof. We assume that n and d are greater than some universal constants; the case of smaller values of these parameters are then directly implied by only changing the constant prefactors.

5.1 Proof of Theorem 1(a): Minimax upper bound

We begin by proving the minimax upper bound stated in part (a) of Theorem 1. The proof is
divided into two parts, where in the ﬁrst part, we obtain an upper bound on the error term |||(2Q∗ − 11T ) diag(x∗) − (2QLS − 11T ) diag(xLS)|||2F, following which we convert this bound to one on LQ∗(x∗, xLS). follows along the lines of the proof of our previous work [34, Theorem 1(a)]. In relation of our present problem, one can think of the setting of [34] as that of estimating the matrix (2Q∗ − 11T ) when the value of x∗ is known. The primary additional challenge in the ﬁrst part of the present proof is to accommodate the additional uncertainty about x∗.
We begin with the ﬁrst part of the proof, where we bound the error in estimating the product term (2Q∗ − 11T ) diag(x∗). Let us rewrite our observation model in a “linearized” fashion that is convenient for subsequent analysis. In particular, let us deﬁne a random matrix W ∈ Rn×d with
entries independently drawn from the distribution

 1

−

pobs(2Q∗ij

−

1)x∗j





Wij = −1 − pobs(2Q∗ij − 1)x∗j
 −pobs(2Q∗ij − 1)x∗j

w.p. w.p. w.p.

pobs Q∗ij pobs Q∗ij 1 − pobs,

1+x∗j
2 1−x∗j
2

+ (1 − Q∗ij) + (1 − Q∗ij)

1−x∗j
2 1+x∗j
2

(18)

where “w.p.” is a shorthand for “with probability”. One can verify that E[W ] = 0, every entry of W is bounded by 2 in absolute value, and moreover that our observed matrix Y can be written in the form

1 Y = (2Q∗ − 11T ) diag(x∗) +

1 W.

(19)

pobs

pobs

Let Πn denote the set of all permutations of the n workers, and let Σd denote the set of all permutations of the d questions. For any pair of permutations (π, σ) ∈ Πn × Σd, deﬁne the set

CPerm(π, σ) : = Q ∈ [0, 1]n×d |Qij ≥ Qi j whenever

π(i) ≤ π(i ) and σ(j) ≤ σ(j ) ,

18

corresponding to the subset of CPerm consisting of matrices that are faithful to the permutations π and σ. For any ﬁxed x ∈ {−1, 1}d, π ∈ Πn and σ ∈ Σd, deﬁne the matrix

Q(π, σ, x) ∈ arg min C(Q, x),

Q∈CPerm(π,σ)

where C(Q, x) : = ||| 1 Y − (2Q − 11T ) diag(x)|||2.

pobs

F

Using this notation, we can rewrite the least squares estimator (6) in the compact form

(xLS, πLS, σLS) ∈ arg min C(Q(π, σ, x), x), and QLS = Q(πLS, σLS, xLS).
(π,σ)∈Πn×Σd x∈{−1,1}d

For the purposes of analysis, let us deﬁne the set

P : = (π, σ, x) ∈ Πn × Σd × {−1, 1}d | C(Q(π, σ, x), x) ≤ C(Q∗, x∗) .

(20)

With this set-up, we claim that it is suﬃcient to show the following: ﬁx a triplet (π, σ, x) ∈ P, for this ﬁxed triplet there is a universal constant c1 such that

P |||(2Q(π, σ, x) − 11T ) diag(x − x∗)|||2 ≤ c1 d log2 d ≥ 1 − e−4d log(dn).

(21)

F

pobs

Given this bound, since the cardinality of the set P is upper bounded by e3d log d (since d ≥ n), a union bound over all these permutations applied to (21) yields

T

∗2

d log2 d

−d log(dn)

P max |||(2Q(π, σ, x) − 11 ) diag(x − x )|||F ≤ c1
(π,σ,x)∈P

pobs

≥1−e

.

The set P is guaranteed to be non-empty since the true permutations π∗ and σ∗ corresponding to Q∗ and the true answer x∗ always lie in P, and consequently, the above tail bound yields the claimed result.
The remainder of our analysis is devoted to proving the bound (21). Given any triplet (π, σ, x) ∈ P, we deﬁne the matrices

V ∗ : = (2Q∗ − 11T ) diag(x∗), and V (π, σ, x) : = (2Q(π, σ, x) − 11T ) diag(x).

Henceforth, for brevity, we refer to the matrix V (π, σ, x) simply as V and the matrix Q(π, σ, x) simply as Q, since the values of the associated quantities (π, σ, x) are ﬁxed and clear from context.
Since (π, σ, x) ∈ P, the deﬁnition of set P in (20) yields the inequality

||| 1 Y − (2Q(π, σ, x) − 11T ) diag(x)|||2 ≤ ||| 1 Y − (2Q∗ − 11T ) diag(x∗)|||2.

pobs

F

pobs

F

Substituting the expression po1bs Y = (2Q∗ − 11T ) diag(x∗) + po1bs W from (19), we obtain the relations

|||(2Q∗ − 11T ) diag(x∗) + 1 W − (2Q(π, σ, x) − 11T ) diag(x)|||2

pobs

F

≤ |||(2Q∗ − 11T ) diag(x∗) + 1 W − (2Q∗ − 11T ) diag(x∗)|||2 = ||| 1 W |||2.

pobs

F

pobs F

19

Using the expansion (a + b)2 = a2 + b2 + 2ab, and substituting the expressions for V ∗ and V , we obtain following basic inequality

1 |||V

∗

−

V

|||2

≤

1

V − V ∗, W .

(22)

2

F pobs

The following lemma uses this inequality to obtain an upper bound on the quantity 12 |||V ∗ − V |||2F.

Lemma 1. There exists a universal constant c1 > 0 such that

∗

2

d log2 d

P |||V − V |||F ≤ c1 pobs

≥ 1 − e−4d log(dn).

(23)

See Section 5.1.1 for the proof of this lemma. This completes the ﬁrst part of the proof. In the second part of the proof, we now convert our bound (23) on the Frobenius norm |||V ∗ − V |||F
into one on the error in estimating x∗ under the Q∗-loss. The following lemma is useful for this
conversion:

Lemma

2.

For

any

pair

of

matrices

A1, A2

∈

n×d
R+

and

any

pair

of

vectors

v1, v2

∈

{−1, 1}d,

we

have

|||A1 diag(v1 − v2)|||2F ≤ 4|||A1 diag(v1) − A2 diag(v2)|||2F.

(24)

See Section 5.1.2 for the proof of this claim.
Recall our assumption that every entry of the matrices Q∗ and Q is at least 12 . Consequently, we can apply Lemma 2 with A1 = (Q∗ − 12 11T ), A2 = (Q − 12 11T ), v1 = x∗ and v2 = x to obtain the inequality

|||(Q∗ − 1 11T ) diag(x∗ − x)|||2 ≤ 4|||(Q∗ − 1 11T ) diag(x∗)−(Q − 1 11T ) diag(x)|||2

2

F

2

2

F

= 4|||V ∗ − V |||2F. (25)

Coupled with Lemma 1, this bound yields the desired result (21).

5.1.1 Proof of Lemma 1

Our proof of this lemma closely follows along the lines of the proof of a related result in our past work [34]. Denote the error in the estimate as ∆ : = V − V ∗. Then from the inequality (22), have

1 |||∆|||2 ≤ 1 W, ∆ .

(26)

2 F pobs

For the quadruplet (π, σ, x, V ∗) under consideration, deﬁne the set

VDIFF(π, σ, x, V ∗) : = α(V − V ∗) |V = (2Q − 11T ) diag(x),

Q ∈ CPerm(π, σ), α ∈ [0, 1] .

Since the terms π, σ, x and V ∗ are ﬁxed for the purposes of this proof, we will use the abbreviated notation VDIFF for VDIFF(π, σ, x, V ∗).
For each choice of radius t > 0, deﬁne the random variable

Z(t) : = sup
D∈VDIFF , |||D|||F≤t

1 D, W .
pobs

(27a)

20

Using the basic inequality (26), the Frobenius norm error |||∆|||F then satisﬁes the bound

1 |||∆|||2 ≤ 1 W, ∆ 2 F pobs

≤ Z |||∆|||F .

(27b)

Thus, in order to obtain a high probability bound, we need to understand the behavior of the random quantity Z(t).
One can verify that the set VDIFF is star-shaped, meaning that αD ∈ VDIFF for every α ∈ [0, 1] and every D ∈ V . DIFF Using this star-shaped property, we are guaranteed that there is a non-empty set of scalars δ0 > 0 satisfying the critical inequality

E[Z(δ0)] ≤ δ02 . 2

(27c)

Our interest is in an upper bound to the smallest (strictly) positive solution δ0 to the cri√tical inequality (27c), and moreover, our goal is to show that for every t ≥ δ0, we have |||∆|||F ≤ c tδ0 with high probability.
Deﬁne a “bad” event
1 At : = ∃∆ ∈ VDIFF | |||∆|||F ≥ tδ0 and pobs ∆, W ≥ 2|||∆|||F tδ0 . (28)

Now suppose the event At is true for some t ≥ δ0, and let ∆0 ∈ VDIFF be a matrix that satisﬁes the two conditions required for At to occur. Furthermore, since VDIFF is star-shaped, the function Z(t) grows at most linearly in t. Consequently whenever event At is true, we have |||∆0|||F ≥ δ0 and hence

δ0

(i) δ0

Z(δ0) ≥

Z(|||∆0|||F) ≥

1

(ii)

∆0, W ≥ 2δ0 tδ0,

|||∆0|||F

|||∆0|||F pobs

where inequality (i) follows from the deﬁnition of function Z and inequality (ii) uses the second condition in the deﬁnition of event At. As a consequence, we obtain the following bound on the probabilities of the associated events

P[At] ≤ P[Z(δ0) ≥ 2δ0 tδ0] for all t ≥ δ0.

The following lemma helps control the behavior of the random variable Z(δ0).

Lemma 3. For any δ > 0, the mean of Z(δ) is upper bounded as

E[Z(δ)] ≤ c1 n + d log2(nd), pobs

and for every u > 0, its tail probability is bounded as

−c2u2pobs P Z(δ) > E[Z(δ)] + u ≤ exp δ2 + E[Z(δ)] + u ,

where c1 and c2 are positive universal constants.

See Section 5.1.3 f√or the proof of this lemma. Setting u = δ0 tδ0 in the tail bound (29b), we ﬁnd that

P Z(δ0) > E[Z(δ0)]+δ0

√

−c2(δ0 tδ0)2pobs

tδ0

≤ exp

√ δ2 + E[Z(δ0)] + δ0 tδ0

0

, for all t > 0.

(29a) (29b)

21

√ By the deﬁnition of δ0 in (27c), we have E[Z(δ0)] ≤ δ02 ≤ δ0 tδ0 for any t ≥ δ0, and with these relations we obtain the bound

P[At] ≤ P[Z(δ0) ≥ 2δ0 tδ0 ≤ exp − c2 δ0 tδ0pobs , for all t ≥ δ0. 3

√

√

Consequently, either |||∆|||F ≤ tδ0, or we have |||∆|||F > tδ0. In the √latter case, conditioning on√the

complement Act , our basic inequality implies that 12 |||∆|||2F ≤ 2|||∆|||F tδ0 and hence |||∆|||F ≤ 4 tδ0.

Putting together the pieces yields that

P |||∆|||F ≤ 4 tδ0 ≥ 1 − exp − c2 δ0 tδ0pobs , valid for all t ≥ δ0. (30) 3

Finally, from the bound on the expected value of Z(t) in Lemma 3, we see that the critical

inequality (27c) is satisﬁed for δ0 = 2c1p(onb+s d) log(nd). Setting t = δ0 = equation (30) yields the claimed result.

2c1p(onb+s d) log(nd) in

5.1.2 Proof of Lemma 2

Consider any four scalars a1 ≥ 0, a2 ≥ 0, b1 ∈ {−1, 1} and b2 ∈ {−1, 1}. If b1 = b2 then (a1b1 − a1b2)2 = 0 ≤ (a1b1 − a2b2)2.

Otherwise we have b1 = −b2. In this case, since a1 and a2 have the same sign,

(a1b1

−

a2b2)2

≥

(a1b1)2

=

1 (a1b1

−

a1b2)2.

4

The two results above in conjunction yield the inequality (a1(b1 − b2))2 ≤ 4(a1b1 − a2b2)2. Applying the above argument to each entry of the matrices A1 diag(v1 − v2) and (A1 diag(v1) − A2 diag(v2)) yields the claim.

5.1.3 Proof of Lemma 3 We need to prove the upper bound (29a) on the mean, as well as the tail bound (29b).

Upper bounding the mean: We upper bound the mean by using Dudley’s entropy integral, as

well as some auxiliary results on metric entropy. Given a set C equipped with a metric ρ and a

tolerance parameter ≥ 0, we let log N ( , C, ρ) denote the -metric entropy of the class C in the

metric ρ. With this notation, the truncated form of Dudley’s entropy integral inequality2 yields

c E[Z(δ)] ≤ pobs

d−8 +

√ 2 nd 12 d−9

log N ( , VDIFF, |||.|||F)(∆ ) .

(31)

√

√

The upper limit of 2 nd in the integration is due to the fact |||D|||F ≤ 2 nd for every D ∈ V . DIFF

It is known [34] that the metric entropy of the set VDIFF is upper bounded as

max{n, d}2 max{n, d} 2

log N ( , VDIFF, |||.|||F) ≤ 8

2

log

for each > 0.

Combining this upper bound with the Dudley entropy integral (31), and observing that the integration has ≥ 12 d−9, the claimed upper bound (29a) follows.
2Here we use (∆ ) to denote the diﬀerential of , so as to avoid confusion with the number of questions d.

22

Bounding the tail probability of Z(δ): In order to establish the claimed tail bound (29b),
we use a Bernstein-type bound on the supremum of empirical processes due to Klein and Rio [25, Theorem 1.1c]. In particular, this result applies to a random variable of the form X† = supv∈V X, v , where X = (X1, . . . , Xm) is a vector of independent random variables taking values in [−1, 1], and V is some subset of [−1, 1]m. Their theorem guarantees that for any t > 0,

†

†

−t2

P X > E[X ] + t ≤ exp 2 sup E[ v, X 2] + 4E[X†] + 3t .

(32)

v∈V

In our setting, we apply this tail bound with the choices

1 X = W,

and

X† = 1

sup

1 D, W = pobsZ(δ).

2

2 D∈VDIFF,

2

|||D|||F≤δ

The entries of the matrix W are independently distributed with a mean of zero and a variance of at most 4pobs, and are bounded in absolute value by 2. As a result, we have E[ D, W 2] ≤ 4pobs|||D|||2F ≤ 4pobsδ2 for every D ∈ V . DIFF With these assignments, inequality (32) guarantees that
−(upobs)2 P pobsZ(δ) > pobsE[Z(δ)] + pobsu ≤ exp 2pobsδ2 + 2pobsE[Z(δ)] + 3upobs ,

for all u > 0, and some algebraic simpliﬁcations yield the claimed result.

5.2 Proof of Theorem 1(b): Minimax lower bound
We now turn to the proof of the minimax lower bound. For a numerical constant δ ∈ (0, 14 ) whose precise value is determined later, deﬁne the vector qDS ∈ [0, 1]n with entries

qiDS =

21 + δ
1 2

if i ≤ po1bs otherwise.

(33)

Set the probability matrix Q∗ ∈ [0, 1]n×d as Q∗ = qDS1T . Observe that we then have Q∗ ∈ CDS. One may assume that the matrix Q∗ is known to the estimator under consideration.
The Gilbert-Varshamov bound [16, 43] guarantees that for a universal constant c > 0, there is a collection β = exp(cd) binary vectors—that is, a collection of vectors {x1, . . . , xβ} all belonging to the Boolean hypercube {−1, 1}d—such that the normalized Hamming distance (1) between any
pair of vectors in this set is lower bounded as

1 dH(x , x ) ≥ 10 ,

for every , ∈ [β].

For each ∈ [β], let P denote the probability distribution of Y induced by setting x∗ = x . For the choice of Q∗ speciﬁed in (33), following some algebra, we obtain a upper bound on the
Kullback-Leibler divergence between any pair of distributions from this collection as

DKL(P P ) ≤ c dδ2 for every = ∈ [β],

for another constant c > 0. Combining the above observations with Fano’s inequality [3] yields that any estimator x has expected normalized Hamming error lower bounded as

∗

1

c dδ2 + log 2

E[dH(x, x )] ≥ 1 −

.

20

log β

23

Consequently, for the choice of Q∗ given by (33), the Q∗-loss is lower bounded as

E[LQ∗(x, x∗)] = 4δ2 E[dH(x, x∗)] ≥ 4δ2

c dδ2 + log 2 (i) c

1−

≥

,

pobs

n

20npobs

cd

npobs

for some constant c > 0 as claimed. Here inequality (i) follows by setting δ to be a suﬃciently small positive constant (depending on the values of c and c ).

5.3 Proof of Corollary 1(a)

In the proof of Theorem 1(a), we showed that there is a constant c1 > 0 such that

|||(2Q∗ − 11T )x∗ − (2Q − 11T )x |||2 ≤ c1 d log2 d,

LS

LS F

pobs

with probability at least 1 − e−d log(dn). Since all entries of the matrices 2Q∗ − 11T and 2QLS − 11T are non-negative, and since every entry of the vectors x∗ and xLS lies in {−1, 1}, some algebra yields
the bound

(2Q∗ij − 1) − (2[QLS]ij − 1) 2 ≤ (2Q∗ij − 1)x∗j − (2[QLS]ij − 1)[xLS]j 2,

for every i ∈ [n], j ∈ [d]. Combining these inequalities yields the claimed bound.

5.4 Proof of Corollary 1(b)
We begin by constructing a set, of cardinality β, of possible matrices Q∗, for some integer β > 1, and subsequently we show that it is hard to identify the true matrix if drawn from this set. We begin by deﬁning a β-sized collection of vectors {h1, . . . , hβ}, all contained in the set [ 12 , 1]d, as follows. The Gilbert-Varshamov bound [16, 43] guarantees a constant c ∈ (0, 1) such that there exists set of β = exp(cd) vectors, v1, . . . , vβ ∈ {−1, 1}d with the property that the normalized Hamming distance (1) between any pair of these vectors is lower bounded as

1 dH(v , v ) ≥ 10 ,

for every , ∈ [β].

Fixing some δ ∈ (0, 41 ), let us deﬁne, for each ∈ [β], the vector h ∈ Rd with entries

[h ]j : =

21 + δ
1 2

if [v ]j = 1 otherwise.

For each ∈ [β], deﬁne the matrix Q = 1(h )T , and let P denote the probability distribution of the observed data Y induced by setting Q∗ = Q and x∗ = 1. Since the entries of Y are all independent,
some algebra leads to the following upper bound on the Kullback-Leibler divergence between any
pair of distributions from this collection:

DKL(P P ) ≤ 4pobsndδ2 for every = ∈ [β].

Moreover, some simple calculation shows that the squared Frobenius norm distance between any two matrices in this collection is lower bounded as

|||Q − Q |||2 ≥ 1 dnδ2 F 10

for every = ∈ [β].

24

Combining the above observations with Fano’s inequality [3] yields that any estimator Q for Q∗ has mean squared error lower bounded as

∗

2

1

2

4pobsdnδ2 + log 2

d

E[|||Q

−

Q|||F]

≥

dnδ 20

1−

log β

≥c , pobs

where we have set δ2 = pocbsn for a small enough positive constant c , where c is another positive constant whose value may depend only on c and c .

5.5 Proof of Theorem 2: WAN under the permutation-based model

Observe that the windowing step of the WAN estimator identiﬁes a group of kWAN workers such that their aggregate responses towards questions are biased (towards either answer {−1, 1}) by at least
kWANpobs log1.5(dn). We ﬁrst derive three properties associated with having such a bias. These properties involve function γπ : [n] × [d] × {−1, 1} → R, where γπ(k, j, x) represents the amount of bias in the responses of the top k ∈ [n] workers for question j ∈ [d] towards the answer x ∈ {−1, 1}:

k

k

γπ(k, j, x) : = (1{Yπ−1(i)j = x} − 1{Yπ−1(i)j = −x}) = x Yπ−1(i)j .

i=1

i=1

A straightforward application of the Bernstein inequality [2], using the fact that the entries of the observed matrix Y are all independent, with moments bounded as
E[Yij] = 2pobs(Q∗ij − 1 )x∗j , and E[Yi2j] = pobs, 2
ensures that all three properties stated below are satisﬁed with probability at least 1 − ec log1.5(dn) for every question j ∈ [d] and every k ∈ {p−ob1s log1.5(dn), . . . , n}. For the remainder of the proof we work conditioned on the event where the following properties hold:

(P1) Suﬃcient condition for bias towards correct answer: If then γπ(k, j, x∗j ) ≥ kpobs log1.5(dn).

ki=1(Q∗π−1(i)j

−

21 )

≥

3 4

k logp1o.b5s(dn) ,

(P2) Necessary condition for bias towards any answer x ∈ {−1, 1}: γπ(k, j, x) ≥

only if x = x∗j and

ki=1(Q∗π−1(i)j

−

21 )

≥

1 4

k logp1o.b5s(dn) .

kpobs log1.5(dn)

(P3) Suﬃcient condition for aggregate to be correct: If γπ(k, j, x∗j ) > 0.

ki=1(Q∗π−1(i)j

−

21 )

≥

1 4

k logp1o.b5s(dn) , then

We now show that when these three properties hold, for any question j0 ∈ J, we must have that [xWAN(π)]j0 = x∗j0. In particular, we do so by exihibiting a question that is at least as hard as j0 on which the WAN estimator is deﬁnitely correct, and use the above properties to conclude that it
therefore must also be correct on the question j0.
Recall that by the deﬁnition (10a) of J, for any question j0 ∈ J, it must be the case that there exists a kj0 ≥ p−ob1s log1.5(dn) such that

kj0

13

(Q∗π−1(i)j − ) ≥

24

i=1

kj0 log1.5(dn). pobs

(34)

25

We deﬁne an associated set J0 as the set of questions that are at least as easy as question j0 according to the underlying permutation σ∗, that is,
J0 : = {j ∈ [d] | σ∗(j) ≤ σ∗(j0)}.
By the monotonicity of the columns of Q∗, every question in J0 also satisﬁes condition (34). For each positive integer k, deﬁne the set

J(k) : = j ∈ [d] γπ(k, j, x) ≥ kpobs log1.5(dn) for some x ∈ {−1, 1} .

Property (P1) ensures that every question in the set J0 is also in the set J(kj0). We then have

(i)
|J (kWAN)| ≥ |J (kj0 )| ≥ |J0|,

where step (i) uses the optimality of kWAN for the optimization problem in equation (9a). Given this, there are two possibilities: either (1) we have the equality J(kWAN) = J0, or (2) the set J(kWAN) contains some question not in the set J0. We address each of these possibilities in turn. Case 1: It suﬃces to observe by Properties (P1)–(P3), that the aggregate of the top kWAN workers is correct on every question in the set J(kWAN) and this implies that it must be the case that [xWAN(π)]j0 = x∗j0 as desired.
Case 2: In this case, there is some question j ∈/ J0 such that γπ(kWAN, j, x) ≥ kWANpobs log1.5(dn)
for some x ∈ {−1, 1}. Property (P2) guarantees that ki=WA1N (Q∗π−1(i)j − 21 ) ≥ 14 kWAN lpoogb1s.5(dn) and that x = x∗j . Now, since every question easier than j0 is in the set J0, question j must be more diﬃcult than j0, which implies that

kWAN ∗

11

(Qπ−1(i)j0

−

) 2

≥

4

i=1

kWAN log1.5(dn) .
pobs

Applying Property (P3), we can then conclude that [xWAN(π)]j0 = x∗j0 as desired.

5.6 Proof of Corollary 2

Theorem 2 guarantees that the WAN estimator correctly answers all questions that have some
reasonable signal. Note that the set (10a) is deﬁned in terms of the 1-norm of subvectors of columns of Q∗ − 12 , whereas the conditions

∗ 1 2 5 log2.5(dn)

π

π∗

Q∗j

−

1 2

2

Qj − 2 2 ≥ pobs

and Qj − Qj 2 ≤ 9 log(dn)

(35)

in the theorem claim are in terms of the 2-norm of the columns of Q∗. The following lemma allows us to connect the 1 and 2-norm constraints for any vector in a general class.

Lemma 4. For any vector v ∈ [0, 1]n such that v1 ≥ . . . ≥ vn, there must be some α ≥

1 2

v

2 2

such that

α

αv2

vi ≥

2.

(36)

i=1 2 log n

26

See Section 5.6.1 for the proof of this lemma.

We now complete the proof of the theorem. We may assume without loss of generality that

the rows of Q∗ are ordered to be non-decreasing downwards along any column, that is, that π∗ is

the identity permutation. Consider any question j ∈ [d] for which the permutation π satisﬁes the

bounds (35). For any ∈ [n], let g ∈ Rn denote a vector with ones in its ﬁrst posit√ions and zeros

elsewhere. The Cauchy-Schwarz inequality implies that (Qπj − 12 )T g

≥

(Q∗j

−

1 2

)T

g

−

Qπj − Q∗j 2.

By applying Lemma 4 to the vector Q∗j − 12 , we are guaranteed the existence of some value

k

≥

5 log2.5(dn) 2p

such

that

(Q∗j −

1 2

)

T

g

k

≥

Q∗j

−

1 2

2

2 lokg n . Consequently, we have the lower bound

obs

(Qπj − 1 )T gk ≥ 2

Q∗j − 12 2

k

√π ∗

− 2 log n

k Qj − Qj 2

(≥i) .37 Q∗j − 21 2

k (ii) 3 ≥
log(dn) 4

k log1.5(dn), pobs

where inequalities (i) and (ii) follow from conditions (35). Consequently, we can apply Theorem 2

for every such question j, thereby yielding the result (11b).

Finally, the claimed result (11c) on the Q∗-loss under the correct permutation is obtained by

considering a zero error (with high probability) for all questions j ∈ [d] for which

Q∗j

−

1 2

2 2

≥

5 logp2o.b5s(dn) and where each of the remaining (at most d) questions contribute a Q∗-loss of at most 5 longd2p.5ob(ds n) .

5.6.1 Proof of Lemma 4

We partition the proof into two cases depending on the value of v 22.

Case 1:

First, suppose that

1 2

v

2 2

≥

e.

In this case, we proceed via proof by contradiction.

If the

claim were false, then we would have

αv2 α 2 > vi ≥ αvα
2 log n i=1

for every α ≥

1 2

v

2 2

.

It would then follow that

n
vi2 =
i=1

1 2

v

2 2

−1

n

(i)

vi2 +

vi2 ≤

i=1

i=

1 2

v

2 2

12

n 2

2 v 2 −1+

vi

i=

1 2

v

2 2

12

n

v

2 2

< 2v2 +

, 2i log n

i=

1 2

v

2 2

where step (i) uses the fact that vi ∈ [0, 1]. Using the standard bound

assumption

1 2

v

2 2

≥ e, we ﬁnd that

21 v 22

n

+

i=

1 2

v

2 2

v 22 ≤ 2i log n

v 22.

b i=a

1 i

≤

log( eab )

and

the

The resulting chain of inequalities contradicts the deﬁnition of v 22.

27

Case 2:

Otherwise, we may assume that

1 2

v

2 2

<

e.

Observe that the case v = 0 trivially satisﬁes

the claim with α = 1, and hence we restrict attention to non-zero vectors. Deﬁne a vector v ∈ [0, 1]n

as

1 v = v.
v1

We ﬁrst prove the claim of the lemma for the vector v , that is, we prove that there exists some

value α ≥

12 v

2 2

such that

α

αv 2

vi ≥ 2 log n2 . (37)

i=1

Observe

that

1

=

v1

≥

···

≥

vn

≥

0.

If

1 2

v

2 2

≥

e,

then

our

claim

(37)

is

proved

via

the

analysis

of Case 1 above. Otherwise, we have that 12

v

2 2

≤e

and

v1

= 1.

Setting

α = 1,

we

obtain

the

inequalities

α
vi = 1 and
i=1

αv

2
2 ≤ 1,

2 log n

where we have used the assumption that n is large enough (concretely, n ≥ 16). We have thus

proved the bound (37), and it remains to translate this bound on v to an analogous bound on the

vector v. Observe that since v1 ≤ 1, we have the relation v 2 ≥ v 2. Using the same value of

α as that derived for vector v , we then obtain from (37) that this value α ≥

12 v

2 2

≥

1 2

v

2 2

satisﬁes

which establishes the claim.

α
v1 vi ≥ v1
i=1

αv

2
2,

2 log n

5.7 Proof of Theorem 3: OBI-WAN under the intermediate model
To simplify notation, let us deﬁne the vector r∗ : = q − 21 . Note that the value of the constant c in the statement of the theorem is speciﬁed later in the proof via equation (44) in Lemma 5.
Our proof of this case is divided into three parts, each corresponding to one of the three steps in the OBI-WAN algorithm. The ﬁrst step is to derive certain properties of the split of the questions. The second step is to derive approximation-guarantees on the outcome of the OBI step. The third and ﬁnal step is to show that this approximation guarantee ensures that the output of the WAN estimator meets the claimed error guarantee.

Step 1: Analyzing the split Our ﬁrst step is to exhibit a useful property of the split of the

questions—namely, that with high probability, the questions in the two sets T0 and T1 have a similar

total diﬃculty.

The random sets (T0, T1) chosen in the ﬁrst step can be obtained as follows: ﬁrst generate an

i.i.d. sequence { j}dj=1 of equiprobable {0, 1} variables, and then set T : = {j ∈ [d] | j = } for

∈ {0, 1}. Note that we have E[

j∈[d](1 − h∗j )2

j] =

1 2

1 − h∗

22, and E[

j∈[d]((1 − h∗j )2 j )2] =

1 2

j∈[d](1

−

h∗j )4

≤

1 2

1 − h∗

22.

Applying Bernstein’s inequality then guarantees that

P (1 − h∗j )2 > 23 1 − h∗ 22 ≤ exp − c 1 − h∗ 22
j∈T

for each ∈ {0, 1},

28

where c is a positive universal constant. We are thus guaranteed that

13 1 − h∗ 22 ≤ (1 − h∗j )2 ≤ 32 1 − h∗ 22 for both ∈ {1, 2}, (38)
j∈T

−cc log2.5 d
with probability at least 1 − e pobs , where we have used the fact that

c lopgo2b.s5 d . Now deﬁne the error event

E :=

∗ 6c log2.5 d

LQ∗ (xOBI-WAN, x ) >

.

npobs

1 − h∗ 2 ≥ cd log2.5 d ≥

2

pobs

r∗

2 2

Combining the sandwich relation (38) with the union bound, we ﬁnd that

P(E) =

P(E | T0 = T0, T1 = T1)P(T0 = T0, T1 = T1)

partitions T0,T1

−cc log2.5 d

≤

P(E | T0 = T0, T1 = T1)P(T0 = T0, T1 = T1) + e pobs .

partitions T0,T1 satisfying (38)

Consequently, in the rest of the proof we consider any partition (T0, T1) that satisﬁes the sandwich bound (38) and derive an upper bound on the error conditioned on this partition. In other words, it suﬃces to prove the following bound for any partition (T0, T1) satisfying (38):

P(E | T0 = T0, T1 = T1) ≤ e−c log1.5(dn),

(39)

for some positive universal constant c whose value may depend only on c. We note that conditioned on the partition (T0, T1), and for any ﬁxed values of Q∗ and x∗, the responses of the workers to the questions in one set are statistically independent of the responses in the other set. Consequently, we describe the proof for any one of the two partitions, and the overall result is implied by a union bound of the error guarantees for the two partitions. We use the notation to denote either one of the two partitions in the sequel, that is, ∈ {0, 1}.

Step 2: Guarantees for the OBI step Assume without loss of generality that the rows of the matrix Q∗ are ordered according to the abilities of the corresponding workers, that is, the entries of
q are arranged in a non-increasing order. Recall that π denotes the permutation of the workers in order of their respective values in u . Let r ∈ Rn denote the vector obtained by permuting the entries of r∗ in the order given by π . Thus the entries of r are identical to those of r∗ up
to a permutation; the ordering of the entries of r is identical to the ordering of the entries of u .
The following lemma—central for the proof of this theorem—establishes a relation between these
vectors. The proof of this lemma combines matrix perturbation theory with some careful algebraic
arguments.

Lemma 5. Suppose that condition (47) holds for a suﬃciently large constant c > 0. Then for any split (T0, T1) satisfying the relation (38), we have

∗2

r∗

2 2

−c log1.5 d

P r − r 2 > 9 log(dn) ≤ e

.

(40)

See Section 5.7.1 for the proof of this lemma.

At this point, we are now ready to apply the bound for the WAN estimator from Corollary 2.

29

Step 3: Guarantees for the WAN step Recall that for any choice of index ∈ {0, 1}, the

OBI step operates on the set T of questions, and the WAN step operates on the alternate set T1− .

Consequently, conditioned on the partition (T0, T1), the outcomes Y1− of the comparisons in set

(1 − ) are statistically independent of the permutation π obtained from set in the OBI step.

Consider any question j ∈ T1−

that satisﬁes the inequality

(1 − h∗)r∗ 2 ≥ 5 log2.5(dn) . We

j

2

p

obs

now claim that this question j satisﬁes the pair of conditions (11a) required by the statement of

Corollary 2. First observe that (1 − h∗j )r∗ is simply the jth column of the matrix (Q∗ − 12 ), we have

Q∗ − 1 2 ≥ 5 log2.5(dn) . The ﬁrst condition in (11a) is thus satisﬁed.

j 22

pobs

In order to establish the second condition, observe that a rescaling of the inequality (40) by the

non-negative scalar (1 − h∗j ) yields the bound

∗

∗ ∗2

(1 − h∗j )r∗

2 2

(1 − hj )r − (1 − hj )r 2 ≤ 9 log(dn)

for every j ∈ T1− .

(41)

Recall our notational assumption that the entries of q (and hence the rows of Q∗) are arranged

in order of the workers’ abilities, and that Qπ is a matrix obtained by permuting the rows of Q∗

according to a given permutation π. Also observe that the vector (1 − h∗j )r equals the jth column of (Qπ − 12 ), where π is the permutation of the workers obtained from the OBI step. Consequently, the approximation guarantee (41) implies that Qπj − Q∗j 2 ≤ √9Qlo∗jg(2dn) . Thus the second condition

in equation (11a) is also satisﬁed for the question j under consideration.

This allows us to apply the result of Corollary 2 for the WAN step, which yields that this

question j is decoded correctly with a probability at least 1 − e−c log1.5(dn), for some positive constant

c. This argument holds for every question j satisfying (1 − h∗)r∗ 2 ≥ 5 log2.5(dn) , and applying the

j

2

pobs

union bound shows that all these questions are decoded correctly with high probability.

5.7.1 Proof of Lemma 5 The proof of this lemma consists of three main steps: (i) First, we show that u is a good approximation for the vector of worker abilities r∗ up to a global
sign. (ii) We then show that the global sign is correctly identiﬁed with high probability. (iii) The ﬁnal step in the proof is to convert this guarantee to one on the permutation induced by u .

Step 1 We ﬁrst show that the vector u approximates r∗ up to a global sign. Assume without loss of generality that x∗j = 1 for every question j ∈ [d]. As in the proof of Theorem 1(a), we begin by rewriting the model in a “linearized” fashion which is convenient for our analysis. Let Q∗0 and Q∗1 denote the submatrices of Q∗ obtained by splitting its columns according to the sets T0 and T1. Then we have for ∈ {0, 1},

1 Y = (2Q∗ − 11T ) diag(x∗) +

1 W,

(42)

pobs

pobs

where conditioned on T0 and T1, the noise matrices W0, W1 ∈ Rn×d have entries independently drawn from the distribution (18). One can verify that the entries of W0 and W1 have a mean of zero, second moment upper bounded by 4pobs, and their absolute values are upper bounded by 2.

30

We now require a standard result on the perturbation of eigenvectors of symmetric matrices [42]. Consider a symmetric and positive semideﬁnite matrix M ∈ Rd×d, a second symmetric matrix ∆M ∈ Rd×d, and let M = M + ∆M . Let v ∈ Rd be an eigenvector associated to the largest eigenvalue of M . Likewise deﬁne v ∈ Rd as an eigenvector associated to the largest eigenvalue of M .
Then we are guaranteed [42] that

min{ v − v 2, v + v 2} ≤

2|||∆M |||op

,

(43)

max{λ1(M ) − λ2(M ) − 2|||∆M |||op, 0}

where λ1(M ) and λ2(M ) denote the largest and second largest eigenvalues of M , respectively. In order to apply the bound (43), we deﬁne the matrix R∗ : = Q∗ − 12 11T , as well as the matrices

M : = 1 Y Y T , M = 4R∗(R∗)T , and p2obs

∆M : = 2 W (R∗)T + 2 R∗W T + 1 W W T .

pobs

pobs

p2obs

Using our linearized observation model (42), it is straightforward to verify that these choices satisfy

the condition M = M + ∆M , so that the bound (43) can be applied. Recall that for any matrix Q∗ ∈ CInt, we have Q∗ = q(1 − h∗)T + 12 (h∗)T for some vectors
q ∈ [ 12 , 1]n and h∗ ∈ [0, 1]d. Also recall our deﬁnition of the associated quantity r∗ ∈ [0, 12 ]n as r∗ = q − 12 . We denote the magnitude of the vector r∗ as ρ : = r∗ 2.
With the notation introduced above, we are ready to apply the bound (43). First observe that
the matrix R∗ has a rank of one, and consequently |||R∗|||op = ρ j∈T (1 − h∗j )2. Conditioned on
the bound (38), we obtain

13 ρ 1 − h∗ 2 ≤ |||R∗|||op ≤

2 ρ

1 − h∗

2.

3

Moreover, the entries of the matrix W are independent, zero-mean, and have a second moment upper bounded by 4pobs. Consequently, known results on random matrices [1, Remark 3.13] guarantee that

|||W |||op ≤ c max{d, n}pobs log1.5 d ≤ c dpobs log1.5 d,

with probability at least 1 − e−c log1.5 d, where we have used the fact that d ≥ n and pobs ≥ n1 . These inequalities, in turn, imply that the top eigenvalue of M is lower bounded as λ1(M ) = |||R∗|||2op ≥ 13 ρ2 1 − h∗ 22, the second eigenvalue vanishes (that is, λ2(M ) = 0), and moreover that

|||∆M ||| ≤ 2 |||R∗||| |||W ||| + 1 |||W |||2

op pobs

op op p2obs

op

≤ c d log1.5 d (ρ 1 − h∗ 2√pobs + d log1.5 d). pobs

Recall the lower bound ρ 1 − h∗ 2 ≥ cd lpoogb2s.5 d , assumed in the statement of the lemma. Using these facts and doing some algebra, we ﬁnd that with probability at least 1 − e−c log1.5 d, for any
pair of sets T0 and T1 satisfying (38), we have the bound

1∗2

1∗2

1

1

d log1.5 d

min{ u − r ρ

2,

u

+r ρ

2} ≤ 36 ρ2 1 − h∗ 2

pobs

,

(44)

j2

where the prefactor 316 is obtained by setting the constant c > 20 to a large enough value.

31

Step 2

We now verify that the global sign is correctly identiﬁed. Recall our selection

n

n

[u ]2j 1{[u ]j > 0} ≥ [u ]2j 1{[u ]j < 0}.

j=1

j=1

Since every entry of the vector r∗ is non-negative, we have the inequality

1∗2

n

2

n 2

u+ r ρ

2≥

[u ]j 1{[u ]j > 0} ≥

[u ]j 1{[u ]j < 0},

j=1

j=1

and consequently,

u + ρ1 r∗ 22 ≥ 12 u 22.

(45a)

On the other hand, a version of the triangle inequality yields

2 u 22 + 2 u + ρ1 r∗ 22 ≥ ρ1 r∗ 22 = 1

(45b)

Now suppose that

u

− ρ1 r∗

2 2

≥

u + ρ1 r∗ 22. Then from our earlier result (44), we have the bound

1∗2

d log1.5 d

u+ r ρ

2 ≤ 36ρ2 1 − h∗ 2pobs ,

2

(45c)

with probability at least 1 − e−c log1.5(dn). Putting together the inequalities (45a), (45b) and (45c) and rearranging some terms yields the inequality

2

∗ 2 d log1.5 d

ρ 1−h 2 ≤

. 9pobs

This requirement contradicts our initial assumption ρ2 1 − h∗ 2 ≥ cd log2.5 d , with c > 20, thereby

2

pobs

proving that

u

− ρ1 r∗

2 2

<

u + ρ1 r∗ 22. Substituting this inequality into equation (44) yields the

bound

1∗2

1

d log1.5 d

u− r ρ

2 ≤ 36ρ2 1 − h∗ 2

pobs

.

(46)

j2

Step 3 The ﬁnal step of this proof is to convert the approximation guarantee (46) on u to an approximation guarantee on the vector r (which, recall, is a permutation of r∗ according to the permutation induced by u ). An additional lemma is useful for this step:
Lemma 6. For any ∈ {0, 1}, we have r − r∗ 2 ≤ 2 ρu − r∗ 2.
See Section 5.7.2 for the proof of this claim.

Combining Lemma 6 with the inequality (46) yields that for any choice of the set T0 and T1 satisfying the condition (38), with probability at least 1 − e−c log1.5 d, we have

∗2

1

d log1.5 d (i)

r∗

2 2

r − r 2 ≤ 18 1 − h∗ 2 pobs

≤

.

18 log(dn)

2

Here, inequality (i) follows from our earlier assumption that r∗ 2 1 − h∗ 2 ≥ c > 20.

cd lpoogb2s.5 d with

32

5.7.2 Proof of Lemma 6

Recall that the two vectors r and r∗ are identical up to a permutation. Now suppose r = r∗.

Then there must exist some position i ∈ [n − 1] such that [r∗]i < [r∗]i+1 and [r ]i ≥ [r ]i+1. Deﬁne

the vector r obtained by interchanging the entries in positions i and (i + 1) in r∗. The diﬀerence

∆ :=

r − ρu

2 2

−

r∗ − ρu

2 2

then

can

be

bounded

as

∆ = ([r ]i −ρ[u ]i)2 + ([r ]i+1 −ρ[u ]i+1)2 − ([r∗]i −ρ[u ]i)2 − ([r∗]i+1 −ρ[u ]i+1)2

= ([r∗]i+1 −ρ[u ]i)2 + ([r∗]i −ρ[u ]i+1)2 − ([r∗]i −ρ[u ]i)2 − ([r∗]i+1 −ρ[u ]i+1)2 = 2ρ([r∗]i+1 − [r∗]i)([u ]i+1 − [u ]i)

≤ 0,

where the ﬁnal inequality uses the fact that the ordering of the entries in the two vectors r and u are identical, which in turn implies that [u ]i ≥ [u ]i+1. We have thus shown an interchange of the entries i and (i + 1) in r∗, which brings it closer to the permutation of r , cannot increase the distance to the vector ρu . A recursive application of this argument leads to the inequality
r − ρu 2 ≤ r∗ − ρu 2. Applying the triangle inequality then yields
r − r∗ 2 ≤ r − ρu 2 + ρu − r∗ 2 ≤ 2 ρu − r∗ 2,

as claimed.

5.8 Proof of Corollary 3
First suppose the matrix Q∗ satisﬁes the condition

q − 1 2 1 − h∗ 2 ≥ cd log2.5(dn) (47)

2

pobs

for a large enough constant c whose value is determined by the result of Theorem 3. Applying

the result of Theorem 3, we obtain that every question j satisfying (1 − h∗)2 q − 1 2 ≥ 5 log2.5(dn)

j

22

pobs

is decoded correctly with a probability at least 1 − e−c log1.5(dn). The total contribution from the

remaining questions to the Q∗-loss is at most 5 lopgo2b.5sn(dn) . A union bound over all questions and both values of ∈ {0, 1} then yields the claim that the aggregate Q∗-loss is at most 5 lopgo2b.5sn(dn) with probability at least 1 − e−c log1.5(dn), for some positive constant c , as claimed in (39).

Otherwise, suppose that condition (47) is violated. Then for any arbitrary x ∈ {−1, 1}d, we have

∗

1

12

∗ 2 6c log2.5 d

LQ∗ (x, x

)≤ dn

q− 2

2

1−h

2≤

, npobs

as claimed, where we have made use of the fact that d ≥ n.

5.9 Proof of Theorem 4(a): OBI-WAN under the Dawid-Skene model
Throughout the proof, we make use the notation previously introduced in the proof of Theorem 3(a). As in this same proof, we condition on some choice of T0 and T1 that satisﬁes (38). The proof of this theorem follows the same structure as the proof of Theorem 3(a) and the lemmas within it. However, we must make additional arguments in order to account for adversarial workers. In the remainder of the proof, we consider any ∈ {0, 1}, and then apply the union bound across both values of . Our proof consists of the three steps:

33

(1) We ﬁrst show that the vector u is a good approximation to (qDS − 21 ) up to a global sign. (2) Second, we show that the global sign of r∗ is indeed recovered correctly. (3) Third, we establish guarantees on the performance of the WAN estimator for our setting. We work through each of these steps in turn.

Step 1 We ﬁrst show that the vector u is a good approximation to qDS − 12 up to a global sign. When Q∗ = qDS1T , we can set the vector h∗ = 0 in the proof of Theorem 3(a). We also have r∗ = qDS − 12 . With these assignments, the the arguments up to equation (44) in Lemma 5 continue to apply even for the present setting where qDS ∈ [0, 1]n. From these arguments, we obtain the following approximation guarantee (44) on recovering r∗ up to a global sign:

1∗2

1∗2

1 1 log1.5 d

min{ u − r ρ

2,

u

+r ρ

2} ≤ 36 ρ2

pobs

,

(48)

with probability at least 1 − e−c log1.5 d.

Step 2 The next step of the proof is to show that the global sign of r∗ is indeed recovered correctly. Deﬁne two pairs of vectors {u+, u−} and {r∗+, r∗−}, all lying in the unit cube [0, 1]n, with entries
[u+]i : = max{[u+]i, 0} and [u−]i : = min{[u−]i, 0} for every i ∈ [n]; [r∗+]i : = max{[r∗+]i, 0}, and [r∗−]i : = min{[r∗−]i, 0} for every i ∈ [n].

From the conditions assumed in the statement of the theorem, we have whereas from the choice of u in the OBI-WAN estimator, we have verify that

r∗+ 2 ≥ u+ 2 ≥

r∗− 2+ 4 logp2o.b5s(dn) , u− 2. One can also

u + ρ1 r∗ 22 ≥ u+ + ρ1 r∗− 22 + u− + ρ1 r∗+ 22.

(49a)

Now suppose that ρ1 r∗+ 2 ≥ u− 2 + bound

logρ22.p5o(bdsn) . Then from the triangle inequality, we obtain the

u− + 1 r∗+ 2 ≥ ρ

1 r∗+ 2 − u− 2 ≥ ρ

log2.5(dn) ρ2pobs .

(49b)

Otherwise we have that ρ1 r∗+ 2 < u− 2 + logρ22.p5o(bdsn) . In this case, we have

u+ + 1 r∗− 2 ≥ u+ 2 − 1 r∗− 2 ≥ u− 2 − 1 r∗+ 2 + 2

ρ

ρ

ρ

log2.5(dn)

≥

ρ2pobs .

log2.5(dn) ρ2pobs

(49c)

Putting together the conditions (49a), (49b) and (49c), we obtain the bound u + 1 r∗ 2 ≥ log2.5(dn) .

ρ2

ρ2 pobs

In conjunction with the result of equation (48), this bound guarantees the correct detection of the

34

global sign, that is, u − 1 r∗ 2 ≤ 1 1 log1.5 d . The deterministic inequality aﬀorded by Lemma 6
ρ 2 36 ρ2 pobs
then guarantees that

∗ 2 1 log1.5 d

r − r 2 ≤ 18 pobs ,

(50)

and this completes the analysis of the OBI part of the estimator.

Step 3 In the third step, we establish guarantees on the performance of the WAN estimator

for our setting. Recall that since the WAN estimator uses the permutation given by r and with

this permutation, acts on the observation Y1− of the other set of questions, the noise W1− is

statistically independent of the choice of r , when conditioned on the split (T0, T1). Assume without loss of generality that x∗ = 1 and that the rows of Q∗ are arranged according to the worker abilities,

meaning that qiDS ≥ qiDS for every i < i , or in other words, ri∗ ≥ ri∗ for every i < i . Recall our earlier notation of gk ∈ {0, 1}n denoting a vector with ones in its ﬁrst k positions and zeros elsewhere.

Now from the proof of Theorem 2 the following two properties ensure that the WAN estimator decodes every question correctly with probability at least 1 − e−c log1.5(dn): (i) There exists some

value k ≥ p−ob1s log1.5(dn) such that r , gk ≥ 34 k logp1.5(dn) , and (ii) for every k ∈ [n], it must be obs

that r , gk > − 41 k logp1o.b5s(dn) . Let us ﬁrst address property (i). Lemma 4 guarantees the existence

of some value k ≥

1 2

r∗

2 2

such that

√ r∗+, gk ≥ k r∗+ 2 .
log(dn)

If there exist multiple such values of k, then choose the smallest such value. Since the vector r∗ has its entries arranged in order, and since r∗+ 2 ≥ r∗− 2, we obtain the following relations for this chosen value of k:

√

r∗, gk = (r∗+)T gk ≥ k r∗+ 2 ≥ r∗ 2

log(dn)

2

k ≥
log(dn)

log2.5(dn) k .
pobs log(dn)

The Cauchy-Schwarz inequality then implies

r , gk ≥ r∗, gk − √k r − r∗ 2 (≥i) 3 4

k log1.5(dn) ,
pobs

where the inequality (i) also uses our earlier bound (50), thereby proving the ﬁrst property. Now towards the second property, we use the condition r∗, 1 ≥ 0. Since the entries of r∗ are arranged in order, we have r∗, gk ≥ 0 for every k ∈ [n]. Applying the Cauchy-Schwarz inequality yields

r , gk ≥ r∗, gk − √k r − r∗ 2 (>ii) − 1 4

k log1.5(dn) ,
pobs

where the inequality (ii) also uses our earlier bound (50), thereby proving the second property. This argument completes the proof of part (a).

35

5.10 Proof of Theorem 4(b): Converse result under the Dawid-Skene model

The Gilbert-Varshamov bound [16, 43] guarantees existence of a set of β vectors, x1, . . . , xβ ∈

{−1, 1}d such that the normalized Hamming distance (1) between any pair of vectors in this set is

lower bounded as dH(x , x ) ≥ 0.25, for every , ∈ [β], where β = exp(c1d) for some constant

c1 > 0. For each ∈ [β], let P denote the probability distribution of Y induced by setting x∗ = x .

When Q∗ = qDS1T for some qDS ∈ [ 110 , 190 ]n, we have an upper bound on the Kullback-Leibler

divergence between any pair of distributions =

∈ [β] as DKL(P

P ) ≤ 25 pobsd

qDS − 12

2 2

≤

25cd, where we have used the assumption qDS − 12 22 ≤ pocbs . Putting the above observations together

into Fano’s inequality [3] yields a lower bound on the expected value of the normalized Hamming

error (1) for any estimator x as:

E[d

(x, x∗)] ≥ 1

25cd + log 2 1−

(i) 1 ≥,

H

8

c1d

10

as claimed, where inequality (i) results from setting the value of c as a small enough positive constant.

5.11 Proof of Proposition 1: OBI-WAN under the permutation-based model
First, suppose that pobs < log1.n5(dn) . Then the condition (16a) is not satisﬁed for any question, and hence the ﬁrst part of the claim is trivially (vacuously) true. In this case, we also have

LQ∗(x , x∗) ≤ 1 ≤ √ 1 log(dn),

OBI-WAN

npobs

due to which the second claim also follows immediately.

Otherwise, we may assume that pobs ≥ log1.n5(dn) . For any index ∈ {0, 1}, consider an arbitrary permutation π . Observe that conditioned on the split (T0, T1), the data Y1− is independent of the
choice of the permutation π . Now consider any question j ∈ T1− that satisﬁes (16a). We then
apply Theorem 2 with the parameter kj = n in (10a), and note that the permutation π speciﬁed
in the statement of Theorem 2 does not matter when kj = n . This result guarantees that our estimator satisﬁes P([xOBI-WAN]j = x∗j ) ≥ 1 − e−c log1.5(dn). A union bound over all questions j ∈ [d] satisfying condition (16a) implies that all of these questions are decoded correctly with probability at least 1 − e−c log1.5(dn). Furthermore, all remaining questions can contribute a total of at most 32 √n1pobs log(dn) to the Q∗-loss. This yields the second part of the claim.

6 Discussion
We propose a new permutation-based model for crowdsourced labeling which is considerably more general than the popular Dawid-Skene model, provide a computationally-eﬃcient algorithm “OBIWAN”, and associated statistical guarantees and empirical evaluations. We hope that the desirable features of the permutation-based model will encourage researchers and practitioners to further build on the permutation-based core of this model.
This work gives rise to several open problems that are theoretically challenging and of interest to practitioners.
• The problem of establishing optimal minimax risk under the permutation-based model for computationally-eﬃcient estimators remains open, and is related to several problems [34, 10, 36]

36

involving permutations that have an unresolved diﬀerence in the computationally eﬃcient and ineﬃcient rates.3 It is of interest to reduce this gap in the future, possibly building on recent work [29, 27] on rates of computationally-eﬃcient algorithms for permutation-based models.
• In addition to the global minimax error, and it is of interest to obtain sharp bounds on adaptivity to the underlying noise levels under various models. Such adaptive bounds are obtained for the Dawid-Skene and intermediate models in the papers [22, 48, 13, 23].
• It will be useful to extend the proposed permutation-based model and associated algorithms to more general settings in crowdsourcing such as a ﬁxed design setup (i.e., where each worker answers a ﬁxed, given subset of questions), questions with more than two choices, and with asymmetric error probabilities of workers (two-coin Dawid-Skene model).
• Our results are loose by logarithmic factors. In the future, it will be of interest to tighten this gap, possibly via new results on the law of iterative logarithm in non-asymptotic regimes such as [4], and alongside understand its relation with such gaps in other problems involving shape-constrained estimation [17].
Finally, there are many other problem settings involving estimation from noisy (as well as biased and subjective) labelers, such as in peer review [40, 44, 31, 41, 45], and it is of interest to see whether permutation-based models and associated techniques can play a useful role in these applications.
Acknowledgements
This work was partially supported by Oﬃce of Naval Research MURI grant DOD-002888, Air Force Oﬃce of Scientiﬁc Research Grant AFOSR-FA9550-14-1-001, Oﬃce of Naval Research grant ONR-N00014, National Science Foundation Grants CIF: 31712-23800, CRII: CIF: 1755656, and CIF: 1763734. The work of NBS was also supported in part by a Microsoft Research PhD fellowship.
We thank the authors of the paper [48] for sharing their implementation of their Spectral-EM algorithm.
References
[1] A. S. Bandeira, R. Van Handel, et al. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016.
[2] S. Bernstein. On a modiﬁcation of Chebyshev’s inequality and of the error formula of Laplace. Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924.
[3] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
[4] A. Dalalyan, N. Schreuder, and V.-E. Brunel. A nonasymptotic law of iterated logarithm for general m-estimators. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1331–1341, 2020.
[5] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Conference on World Wide Web, pages 285–294, 2013.
3That said, there are related problems [37, 18] involving permutation-based models where statistically optimal techniques are computationally eﬃcient, and also adapt optimally to much more restrictive parameter-based models.
37

[6] A. Dawid and A. Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, pages 20–28, 1979.
[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009, pages 248–255. IEEE, 2009.
[8] C. Eickhoﬀ and A. de Vries. How crowdsourcable is your task. In Crowdsourcing for search and data mining, 2011.
[9] W. Feller. Generalization of a probability limit theorem of Cram´er. Transactions of the American Mathematical Society, 54(3):361–372, 1943.
[10] N. Flammarion, C. Mao, and P. Rigollet. Optimal rates of statistical seriation. Bernoulli, 25(1):623–653, 2019.
[11] U. Gadiraju, B. Fetahu, and R. Kawase. Training workers for improving performance in crowdsourcing microtasks. In Design for Teaching and Learning in a Networked World. 2015.
[12] U. Gadiraju, R. Kawase, S. Dietze, and G. Demartini. Understanding malicious behavior in crowdsourcing platforms: The case of online surveys. In ACM Conference on Human Factors in Computing Systems, 2015.
[13] C. Gao, Y. Lu, and D. Zhou. Exact exponent in optimal rates for crowdsourcing. In International Conference on Machine Learning (ICML), 2016.
[14] C. Gao and D. Zhou. Minimax optimal convergence rates for estimating ground truth from crowdsourced labels. arXiv preprint arXiv:1310.5764, 2013.
[15] A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators?: Crowdsourcing abuse detection in user-generated content. In ACM conference on Electronic commerce, 2011.
[16] E. N. Gilbert. A comparison of signalling alphabets. Bell System Technical Journal, 31(3):504– 522, 1952.
[17] Q. Han. Global empirical risk minimizers with” shape constraints” are rate optimal in general dimensions. arXiv preprint arXiv:1905.12823, 2019.
[18] R. Heckel, N. B. Shah, K. Ramchandran, M. J. Wainwright, et al. Active ranking from pairwise comparisons and when parametric assumptions do not help. The Annals of Statistics, 47(6):3099–3126, 2019.
[19] A. Jaﬀe, B. Nadler, and Y. Kluger. Estimating the accuracies of multiple classiﬁers without labeled data. In Artiﬁcial Intelligence and Statistics, pages 407–415, 2015.
[20] J. Jantzen, J. Norup, G. Dounias, and B. Bjerregaard. Pap-smear benchmark data for pattern classiﬁcation. Nature inspired Smart Information Systems (NiSIS 2005), pages 1–9, 2005.
[21] D. Karger, S. Oh, and D. Shah. Budget-optimal crowdsourcing using low-rank matrix approximations. In Annual Allerton Conference on Communication, Control, and Computing, 2011.
[22] D. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. In Advances in neural information processing systems, 2011.
38

[23] A. Khetan and S. Oh. Achieving budget-optimality with adaptive schemes in crowdsourcing. Advances in Neural Information Processing Systems, 29:4844–4852, 2016.
[24] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for ﬁne-grained image categorization. In First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, June 2011.
[25] T. Klein and E. Rio. Concentration around the mean for maxima of empirical processes. The Annals of Probability, 33(3):1060–1077, 2005.
[26] S. Lazebnik, C. Schmid, and J. Ponce. A sparse texture representation using local aﬃne regions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8):1265–1278, 2005.
[27] A. Liu and A. Moitra. Better algorithms for estimating non-parametric models in crowd-sourcing and rank aggregation. In Conference On Learning Theory, 2020.
[28] Q. Liu, J. Peng, and A. T. Ihler. Variational inference for crowdsourcing. In Advances in Neural Information Processing Systems, pages 692–700, 2012.
[29] C. Mao, A. Pananjady, and M. J. Wainwright. Breaking the 1/sqrt n barrier: Faster rates for permutation-based models in polynomial time. In Conference On Learning Theory, pages 2037–2042, 2018.
[30] J. Matouˇsek and J. Vondra´k. The probabilistic method. Lecture Notes, Department of Applied Mathematics, Charles University, Prague, 2001.
[31] R. Noothigattu, N. Shah, and A. Procaccia. Loss functions, axioms, and peer review. In ICML Workshop on Incentives in Machine Learning, July 2020.
[32] F. Parisi, F. Strino, B. Nadler, and Y. Kluger. Ranking and combining multiple predictors without labeled data. Proceedings of the National Academy of Sciences, 111(4):1253–1258, 2014.
[33] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. The Journal of Machine Learning Research, 99:1297–1322, 2010.
[34] N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. IEEE Transactions on Information Theory, 63(2):934–959, 2017.
[35] N. B. Shah, S. Balakrishnan, and M. J. Wainwright. Feeling the Bern: Adaptive estimators for bernoulli probabilities of pairwise comparisons. IEEE Transactions on Information Theory, 2019.
[36] N. B. Shah, S. Balakrishnan, and M. J. Wainwright. Low permutation-rank matrices: Structural properties and noisy completion. Journal of Machine Learning Research, 2019.
[37] N. B. Shah and M. J. Wainwright. Simple, robust and optimal ranking from pairwise comparisons. Journal of Machine Learning Research, 2018.
[38] N. B. Shah and D. Zhou. Double or nothing: multiplicative incentive mechanisms for crowdsourcing. The Journal of Machine Learning Research, 17(1):5725–5776, 2016.
39

[39] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 614–622. ACM, 2008.
[40] I. Stelmakh, N. Shah, and A. Singh. On testing for biases in peer review. In NeurIPS, 2019.
[41] I. Stelmakh, N. Shah, and A. Singh. PeerReview4All: Fair and accurate reviewer assignment in peer review. In Conference on Algorithmic Learning Theory (ALT), 2019.
[42] G. Stewart and J.-G. Sun. Matrix perturbation theory, 1990.
[43] R. Varshamov. Estimate of the number of signals in error correcting codes. In Dokl. Akad. Nauk SSSR, 1957.
[44] J. Wang and N. B. Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS, 2019.
[45] J. Wang, I. Stelmakh, Y. Wei, and N. Shah. Debiasing evaluations that are biased by evaluations. In AAAI, 2021.
[46] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, and P. L. Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing systems, pages 2035–2043, 2009.
[47] M.-C. Yuen, I. King, and K.-S. Leung. A survey of crowdsourcing systems. In IEEE Inernational Conference on Social Computing, 2011.
[48] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet em: A provably optimal algorithm for crowdsourcing. The Journal of Machine Learning Research, 17(1):3537–3580, 2016.
[49] D. Zhou, Q. Liu, J. C. Platt, C. Meek, and N. B. Shah. Regularized minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015.
[50] D. Zhou, J. Platt, S. Basu, and Y. Mao. Learning from the wisdom of crowds by minimax entropy. In Advances in Neural Information Processing Systems 25, pages 2204–2212, 2012.

Appendix: Analysis of the majority voting estimator

In this section, we analyze the majority voting estimator, given by

n

[xMV]j ∈ arg max 1{Yij = b}
b∈{−1,1} i=1

for every j ∈ [d].

Here we use 1{·} to denote the indicator function. The following theorem provides bounds on the risk of majority voting under the Q∗-semimetric
loss in the regime of interest (R2).

Proposition 2. For the majority vote estimator, the risk over the Dawid-Skene class is lower bounded as

sup sup E[LQ∗(x , x∗)] ≥ c √ 1 ,

x∗∈{−1,1}d Q∗∈CDS

MV

L npobs

(51)

for some positive constant cL.

40

A comparison of the bound (51) with the results of Theorem 1, Theorem 3(a) and Theorem 4 shows that the majority voting estimator is suboptimal in terms of the sample complexity. Since this suboptimality holds for the (smaller) Dawid-Skene model class, it also holds for the (larger) intermediate model class, as well as the permutation-based model class. The remainder of this section is devoted to the proof of this claim.

Proof of Proposition 2

We begin with a lower bound due to Feller [9] (see also [30, Theorem 7.3.1]) on the tail probability of a sum of independent random variables.

Lemma 7 (Feller). There exist positive universal constants c1 and c2 such that for any set of

independent random variables X1, . . . , Xn satisfying E[Xi] = 0 and |Xi| ≤ M for every i ∈ [n], if

n i=1

E[(Xi

)2

]

≥

c1

then

n
P Xi > t ≥ c2 exp 12
i=1

−t2 ni=1 E[(Xi)2] ,

for every t ∈ [0, ni=M1 2E√[(cX1i)2] ].

In what follows, we use Lemma 7 to derive the claimed lower bound on the error incurred by

the majority voting algorithm. To this end, let S ⊂ [n] denote the set of some |S| = Consider the following value of matrix Q∗:

2pnobs workers.

Q∗ij = 11
2

if i ∈ S otherwise.

Then for any question j ∈ [d], we have ni=1(2Q∗ij − 1)2 = 2pnobs . Now suppose that x∗j = −1 for every question j ∈ [d]. Then for every i ∈ S, the observations are
distributed as

0 Yij =
−1

with probability 1 − pobs with probability pobs,

and for every i ∈/ S, as

 0  Yij = −1
 1

with probability 1 − pobs with probability 0.5pobs with probability 0.5pobs.

Consider any question j ∈ [d]. Then in this setting, the majority voting estimator incorrectly

estimates the value of x∗j when

n i=1

Yij

>

0.

We

now

use

Lemma

7

to

obtain

a

lower

bound

on

the

probability of the occurrence of this event. Some simple algebra yields

n
E[Yij] = −|S|pobs and
i=1

n
E[(Yij)2] = npobs.
i=1

In order to satisfy the conditions required by the lemma, we assume that npobs > c1. Note that this condition makes the problem strictly easier than the condition npobs ≥ 1 assumed otherwise,

41

and aﬀects the lower bounds by at most a constant factor c1. An application of Lemma 7 with

t=−

n i=1

E[Yij ]

=

|S|pobs

now

yields

n
P( Yij > 0) ≥ c2 exp
i=1

−|S|2p2obs 12npobs

(i)
≥c,

for some constant c > 0 that may depend only on c1 and c2, where inequality (i) is a consequence of the choice |S| = 2pnobs .
Now that we have established a constant-valued lower bound on the probability of error in the estimation of x∗j for every j ∈ [d], for the value of Q∗ under consideration, we have

n ∗

∗

12

P([xMV]j = xj )

(Qij

−

) 2

≥

i=1

n c,
2pobs

and consequently E[LQ∗ (xMV, x∗)] ≥ √2ncpobs , as claimed.

42

