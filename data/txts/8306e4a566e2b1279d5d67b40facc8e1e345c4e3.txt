EF21 with Bells & Whistles

Oct 6, 2021

arXiv:2110.03294v1 [cs.LG] 7 Oct 2021

EF21 WITH BELLS & WHISTLES: PRACTICAL ALGORITHMIC EXTENSIONS OF MODERN ERROR FEEDBACK

Ilyas Fatkhullin* KAUST Saudi Arabia

Igor Sokolov KAUST Saudi Arabia

Eduard Gorbunov MIPT & Yandex Russia

Zhize Li† KAUST Saudi Arabia

Peter Richtárik KAUST Saudi Arabia

ABSTRACT
First proposed by Seide et al. (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is 𝑂(1/𝑇 2/3), the rate of gradient descent in the same regime is 𝑂(1/𝑇 )). Recently, Richtárik et al. (2021) (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deﬁciencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.

1 INTRODUCTION

In this paper, we consider the nonconvex distributed/federated optimization problem of the form

{︃

def

1

𝑛
∑︁

}︃

min 𝑓 (𝑥) =

𝑓𝑖(𝑥) ,

(1)

𝑥∈R𝑑

𝑛

𝑖=1

where 𝑛 denotes the number of clients/workers/devices/nodes connected with a server/master and client 𝑖 has an access to the local loss function 𝑓𝑖 only. The local loss of each client is allowed to have the online/expectation form

𝑓𝑖(𝑥) = E𝜉𝑖∼𝒟𝑖 [𝑓𝜉𝑖 (𝑥)] ,

(2)

or the ﬁnite-sum form

1

𝑚
∑︁

𝑓𝑖(𝑥) = 𝑚 𝑓𝑖𝑗(𝑥). (3)

𝑗=1

Problems of this structure appear in federated learning (Konecˇný et al., 2016; Kairouz, 2019), where training is performed directly on the clients’ devices. In a quest for state-of-the-art performance, machine learning practitioners develop elaborate model architectures and train their models on enormous data sets. Naturally, for training at this scale to be possible, one needs to rely on distributed computing (Goyal et al., 2017; You et al., 2020). Since in recent years remarkable empirical successes were obtained with massively over-parameterized models (Arora et al., 2018), which puts an extra strain on the communication links during training, recent research activity and practice
*The work of Ilyas Fatkhullin was performed during a Summer research internship conducted in the Optimization and Machine Learning Lab led by Peter Richtárik. At the time when this paper was ﬁrst released, Ilyas Fatkhullin was a master’s student at the Technical University of Munich, Germany.
†Corresponding author.

1

EF21 with Bells & Whistles

Oct 6, 2021

focuses on developing distributed optimization methods and systems capitalizing on (deterministic or randomized) lossy communication compression techniques to reduce the amount of communication trafﬁc.

A compression mechanism is typically formalized as an operator 𝒞 : R𝑑 ↦→ R𝑑 mapping hard-tocommunicate (e.g., dense) input messages into easy-to-communicate (e.g., sparse) output messages. The operator is allowed to be randomized, and typically operates on models Khaled & Richtárik (2019) or on gradients Alistarh et al. (2017); Beznosikov et al. (2020), both of which can be described as vectors in R𝑑. Besides sparsiﬁcation (Alistarh et al., 2018), typical examples of useful compression mechanisms include quantization (Alistarh et al., 2017; Horváth et al., 2019a) and low-rank approximation (Vogels et al., 2019; Safaryan et al., 2021).

There are two large classes of compression operators often studied in the literature: i) unbiased compression operators 𝒞, meaning that there exists 𝜔 ≥ 0 such that

E [𝒞(𝑥)] = 𝑥, E [︀‖𝒞(𝑥) − 𝑥‖2]︀ ≤ 𝜔‖𝑥‖2, ∀𝑥 ∈ R𝑑;

(4)

and ii) biased compression operators 𝒞, meaning that there exists 0 < 𝛼 ≤ 1 such that

E [︀‖𝒞(𝑥) − 𝑥‖2]︀ ≤ (1 − 𝛼) ‖𝑥‖2, ∀𝑥 ∈ R𝑑.

(5)

Note that the latter “biased” class contains the former one, i.e., if 𝒞 satisﬁes (4) with 𝜔, then a scaled version (1 + 𝜔)−1𝒞 satisﬁes (5) with 𝛼 = 1/(1+𝜔). While distributed optimization methods with unbiased compressors (4) are well understood (Alistarh et al., 2017; Khirirat et al., 2018; Mishchenko et al., 2019; Horváth et al., 2019b; Li et al., 2020; Li & Richtárik, 2021a; Li & Richtárik, 2020; Islamov et al., 2021; Gorbunov et al., 2021), biased compressors (5) are signiﬁcantly harder to analyze. One of the main reasons behind this is rooted in the observation that when deployed within distributed gradient descent in a naive way, biased compresors may lead to (even exponential) divergence (Karimireddy et al., 2019; Beznosikov et al., 2020). Error Feedback (EF) (or Error Compensation (EC))—a technique originally proposed by Seide et al. (2014)—emerged as an empirical ﬁx of this problem. However, this technique remained poorly understood until very recently.

Although several theoretical results were obtained supporting the EF framework in recent years (Stich et al., 2018; Alistarh et al., 2018; Beznosikov et al., 2020; Gorbunov et al., 2020; Qian et al., 2020; Tang et al., 2020; Koloskova et al., 2020), they use strong assumptions (e.g., convexity, bounded gradients, bounded dissimilarity), and do not get 𝒪(1/𝛼𝑇 ) convergence rates in the smooth nonconvex regime. Very recently, Richtárik et al. (2021) proposed a new EF mechanism called EF21, which uses standard smoothness assumptions only, and also enjoys the desirable 𝑂(1/𝛼𝑇 ) convergence rate for the nonconvex case (in terms of number of communication rounds 𝑇 this matches the best-known rate 𝒪((1+𝜔/√𝑛)/𝑇 ) obtained by Gorbunov et al. (2021) using unbiased compressors), improving the previous 𝑂(1/(𝛼𝑇 ) )2/3 rate of the standard EF mechanism (Koloskova et al., 2020).

2 OUR CONTRIBUTIONS
While Richtárik et al. (2021) provide a new theoretical SOTA for error feedback based methods, the authors only study their EF21 mechanism in a pure form, without any additional “bells and whistles” which are of importance in practice. In this paper, we aim to push the EF21 framework beyond its pure form by extending it in several directions of high theoretical and practical importance. In particular, we further enhance the EF21 mechanism with the following six useful and practical algorithmic extensions:
1. stochastic approximation, 2. variance reduction, 3. partial participation, 4. bidirectional compression, 5. momentum, and 6. proximal setting (regularization).
We do not stop at merely proposing these algorithmic enhancements: we derive strong convergence results for all of these extensions. Several of these techniques were never analyzed in conjunction

2

EF21 with Bells & Whistles

Oct 6, 2021

Setup

Method

Citation

Compl. (NC)

Compl. (PL)

Comment

gFruadlls EF21 Richtárik et al. (2021) 𝛼1𝜀2 𝛼1𝜇

Stoch. grads

Choco-SGD EF21-SGD EF21-SGD
EF21-PAGE

Koloskova et al. (2020) Richtárik et al. (2021)
NEW NEW

𝜀12 + 𝛼𝐺𝜀3 + 𝑛𝜎𝜀24 𝛼1𝜀2 + 𝛼𝜎32𝜀4
𝛼1𝜀2 + 1+𝛼3Δ𝜀i4nf
√
𝑚𝜀+21/𝛼 + 𝑚

N/A
𝛼1𝜇 + 𝜇2𝜎𝛼23𝜀 𝛼1𝜇 + 1𝜇+2Δ𝛼i3n𝜀f
√
𝑚𝜇+1/𝛼 + 𝑚

‖∇𝑓𝑖(𝑥)‖ ≤ 𝐺 UBV (Ex. 1)

IS (Ex. 2)

𝑚

𝑓𝑖(𝑥) =

1 𝑚

∑︀

𝑓𝑖𝑗 (𝑥)

𝑗=1

PP

EF21-PP

NEW

𝑝𝛼1𝜀2 (1) + 𝛼1𝜀2

𝑝𝛼1𝜇 (1) + 𝛼1𝜇

Full grads

BC DoubleSqueeze EF21-BC

Tang et al. (2020) NEW

𝜀12 + 𝜀Δ3 + 𝑛𝜎𝜀24
1 𝛼𝑤 𝛼𝑀 𝜀2

N/A
1 𝛼𝑤 𝛼𝑀 𝜇

E [‖𝒞(𝑥) − 𝑥‖] ≤ Δ Full grads

Mom.

M-CSER

EF21-HB

Xie et al. (2020)(2) 𝜀12 + (1−𝜂𝐺)𝛼𝜀3 N/A

NEW

(︁

)︁

1 𝜀2

1−1 𝜂 + 𝛼1

N/A

‖∇𝑓𝑖(𝑥)‖ ≤ 𝐺 Full grads

Prox

EF21-Prox

NEW

1 𝛼𝜀2

1 (3) 𝛼𝜇

Full grads

(1) Red term = number of communication rounds, blue term = expected number of gradient computations per client.

(2) Xie et al. (2020) consider Nesterov’s momentum. Moreover, they analyzed the version with stochastic gradients, bidirectional compression

and local steps. However, the derived result is not better than state-of-the-art ones with either stochastic gradients or bidirectional compression.

Therefore, to maintain the table compact, we do not include the results of Xie et al. (2020) in the other parts of the table. (3) This result is obtained under the generalized PŁ-condition for composite optimization problems (see Assumption 5 from Appendix I.2).

Table 1: Summary of the state-of-the-art complexity results for ﬁnding an 𝜀-stationary point, i.e., such a point 𝑥^ that E [︀‖∇𝑓 (𝑥^)‖2]︀ ≤ 𝜀2, for generally non-convex functions and an 𝜀-solution, i.e., such a point 𝑥^ that E [𝑓 (𝑥^) − 𝑓 (𝑥*)] ≤ 𝜀, for functions satisfying PŁ-condition using error-feedback type methods. By

(computation) complexity we mean the average number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd an

𝜀-stationary point (“Compl. (NC)”) or 𝜀-solution (“Compl. (PŁ)”). Removing the terms colored in blue from the

complexity bounds shown in the table, one can get communication complexity bounds, i.e., the total number

of communication rounds needed to ﬁnd an 𝜀-stationary point (“Compl. (NC)”) or 𝜀-solution (“Compl. (PŁ)”).

Dependences on the numerical constants, “quality” of the starting point, and smoothness constants are omitted

in the complexity bounds. Moreover, dependencies on log(1/𝜀) are also omitted in the column “Compl. (PŁ)”.

Abbreviations: “BC” = bidirectional compression, “PP” = partial participation; “Mom.” = momentum; 𝑇 = the

number of communications rounds needed to ﬁnd an 𝜀-stationary point; #grads = the number of (stochastic)

ﬁrst-order oracle calls needed to ﬁnd an 𝜀-stationary point. Notation: 𝛼 = the compression parameter, 𝛼𝑤 and

𝛼𝑀

=

the

compression

parameters

of

worker

and

master

nodes

respectively

for

EF21-BC,

𝜎2

=

1 𝑛

∑︀𝑛
𝑖=1

𝜎𝑖2

(see Example 1), Δinf = 𝑓 inf − 𝑛1 ∑︀𝑛𝑖=1 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1 𝑓𝑖i𝑗nf (see Example 2), 𝑝 = probability of sampling the client

in EF21-PP, 𝜂 = momentum parameter. To the best of our knowledge, combinations of error feedback with

partial participation (EF21-PP) and proximal versions of error feedback (EF21-Prox) were never analyzed in

the literature.

with the original EF mechanism before, and in cases where they were, our new results with EF21 are vastly superior. See Table 1 for an overview of our results. In summary, our results constitute the new algorithmic and theoretical state-of-the-art in the area of error feedback.
We now brieﬂy comment on each extension proposed in this paper:
◇ Stochastic approximation. The vanilla EF21 method requires all clients to compute the exact/full gradient in each round. While Richtárik et al. (2021) do consider a stochastic extension of EF21, they do not formalize their result, and only consider the simplistic scenario of uniformly bounded variance, which does not in general hold for stochasticity coming from subsampling (Khaled & Richtárik, 2020). However, exact gradients are not available in the stochastic/online setting (2), and in the ﬁnite-sum setting (3) it is more efﬁcient in practice to use subsampling and work with stochastic gradients instead. In our paper, we extend EF21 to a more general stochastic approximation framework than the simplistic framework considered in the original paper. Our method is called EF21-SGD (Algorithm 2); see Appendix D for more details.
◇ Variance reduction. As mentioned above, EF21 relies on full gradient computations at all clients. This incurs a high or unaffordable computation cost, especially when local clients hold large training sets, i.e., if 𝑚 is very large in (3). In the ﬁnite-sum setting (3), we enhance EF21 with a variance reduction technique to reduce the computational complexity. In particular, we adopt the simple and efﬁcient variance-reduced method PAGE (Li et al., 2021; Li, 2021b) (which is optimal for solving problems (3)) into EF21, and call the resulting method EF21-PAGE (Algorithm 3). See Appendix E for more details.
3

EF21 with Bells & Whistles

Oct 6, 2021

◇ Partial participation. The EF21 method proposed by Richtárik et al. (2021) requires full participation of clients for solving problem (1), i.e., in each round, the server needs to communicate with all 𝑛 clients. However, full participation is usually impractical or very hard to achieve in massively distributed (e.g., federated) learning problems (Konecˇný et al., 2016; Cho et al., 2020; Kairouz, 2019; Li & Richtárik, 2021b; Zhao et al., 2021). To remedy this situation, we propose a partial participation (PP) variant of EF21, which we call EF21-PP (Algorithm 4). See Appendix F for more details.

◇ Bidirectional compression. The vanilla EF21 method only considers upstream compression of the messages sent by the clients to the server. However, in some situations, downstream communication is also costly (Horváth et al., 2019a; Tang et al., 2020; Philippenko & Dieuleveut, 2020). In order to cater to these situations, we modify EF21 so that the server can also optionally compresses messages before communication. Our master compression is intelligent in that it employs the Markov compressor proposed in EF21 to be used at the devices. The proposed method, based on bidirectional compression, is EF21-BC (Algorithm 5). See Appendix G for more details.

◇ Momentum. A very successful and popular technique for enhancing both optimization and generalization is momentum/acceleration (Polyak, 1964; Nesterov, 1983; Lan & Zhou, 2015; AllenZhu, 2017; Lan et al., 2019; Li, 2021a). For instance, momentum is a key building block behind the widely-used Adam method (Kingma & Ba, 2014). In this paper, we add the well-known (Polyak) heavy ball momentum (Polyak, 1964; Loizou & Richtárik, 2020) to EF21, and call the resulting method EF21-HB (Algorithm 6). See Appendix H for more details.

◇ Proximal setting. It is common practice to solve regularized versions of empirical risk minimization problems instead of their vanilla variants (Shalev-Shwartz & Ben-David, 2014). We thus consider the composite/regularized/proximal problem

{︃

def

1

𝑛
∑︁

}︃

min Φ(𝑥) =

𝑓𝑖(𝑥) + 𝑟(𝑥) ,

(6)

𝑥∈R𝑑

𝑛

𝑖=1

where 𝑟(𝑥) : R𝑑 → R ∪ {+∞} is a regularizer, e.g., ℓ1 regularizer ‖𝑥‖1 or ℓ2 regularizer ‖𝑥‖22. To broaden the applicability of EF21 to such problems, we propose a proximal variant of EF21 to solve
the more general composite problems (6). We call this new method EF21-Prox (Algorithm 7). See
Appendix I for more details.

Our theoretical complexity results are summarized in Table 1. In addition, we also analyze EF21SGD, EF21-PAGE, EF21-PP, EF21-BC under Polyak-Łojasiewicz (PŁ) condition (Polyak, 1963; Lojasiewicz, 1963) and EF21-Prox under the generalized PŁ-condition (Li & Li, 2018) for composite optimization problems. Due to space limitations, we defer all the details about the analysis under the PŁ-condition to the appendix and provide only simpliﬁed rates in Table 1. We comment on some preliminary experimental results in Section 5. More experiments including deep learning experiments are presented in Appendix A.

3 METHODS

Since our methods are modiﬁcations of EF21, they share many features, and are presented in a uniﬁed
way in Table 2. At each iteration of the proposed methods, worker 𝑖 computes the compressed vector 𝑐𝑡𝑖 and sends it to the master. The methods differ in the way of computing 𝑐𝑡𝑖 but have similar (in case of EF21-SGD, EF21-PAGE, EF21-PP – exactly the same) update rules to the one of EF21:

𝑡+1

𝑡

𝑡

𝑡+1

𝑡𝑡

𝑡+1

1

𝑛
∑︁

𝑡+1

𝑡 1 ∑𝑛︁ 𝑡

𝑥 = 𝑥 − 𝛾𝑔 ,

𝑔𝑖 = 𝑔𝑖 + 𝑐𝑖,

𝑔= 𝑛

𝑔𝑖

=𝑔 + 𝑛

𝑐𝑖 .

(7)

𝑖=1

𝑖=1

The pseudocodes of the methods are given in the appendix. Below we brieﬂy describe each method.

◇ EF21-SGD: Error feedback and SGD. EF21-SGD is essentially EF21 but instead of the full gradients ∇𝑓𝑖(𝑥𝑡+1), workers compute the stochastic gradients 𝑔ˆ𝑖(𝑥𝑡+1), and use them to compute

𝑐𝑡𝑖 = 𝒞(𝑔ˆ𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡).

Despite the seeming simplicity of this extension, it is highly important for various applications of machine learning and statistics where exact gradients are either unavailable or prohibitively expensive to compute.

4

EF21 with Bells & Whistles

Oct 6, 2021

Update

𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡,

𝑛

𝑔𝑡

=

1 𝑛

∑︀

𝑔𝑖𝑡 ,

𝑖=1

𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡,

𝑔𝑡+1 = 𝑔𝑡 + 𝑏𝑡+1,

𝑏𝑡+1 = 𝒞𝑀 (𝑔𝑡+1 − 𝑔𝑡), ̃︀

𝑔𝑡+1 = 1 ∑︀𝑛 𝑔𝑡+1,

̃︀

𝑛 𝑖=1 ̃︀𝑖

𝑔𝑡+1 ̃︀𝑖

=

𝑔̃︀𝑖𝑡

+

𝑐𝑡𝑖

𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑣𝑡,

𝑣𝑡+1 = 𝜂𝑣𝑡 + 𝑔𝑡+1,

𝑔𝑡+1 = 𝑛1 ∑︀𝑛𝑖=1 𝑔𝑖𝑡+1, 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

𝑥𝑡+1 = prox𝛾𝑟 (︀𝑥𝑡 − 𝛾𝑔𝑡)︀, 𝑔𝑡+1 = 𝑛1 ∑︀𝑛𝑖=1 𝑔𝑖𝑡+1, 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

Method
EF21 EF21-SGD
EF21-PAGE
EF21-PP EF21-BC
EF21-HB EF21-Prox

Alg. # Alg. 1 Alg. 2 Alg. 3
Alg. 4 Alg. 5
Alg. 6 Alg. 7

𝑐𝑡𝑖 𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) 𝒞(𝑔^𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡)
𝒞(𝑣𝑖𝑡+1 − 𝑔𝑖𝑡)
𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) 0
𝒞𝑤 (∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡)
𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡)
𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡)

Comment

𝑔^𝑖(𝑥𝑡+1) satisﬁes As. 2

𝑏𝑡𝑖 ∼ Be(𝑝),

𝑣𝑖𝑡+1 = ∇𝑓𝑖(𝑥𝑡+1), if 𝑏𝑡𝑖 = 1,

𝑣𝑖𝑡+1

=

𝑣𝑖𝑡

+

1 𝜏

∑︀ ∇𝑓𝑖𝑗 (𝑥𝑡+1)

𝑖 𝑗∈𝐼𝑡

𝑖

− 𝜏1 ∑︀ ∇𝑓𝑖𝑗 (𝑥𝑡), if 𝑏𝑡𝑖 = 0,
𝑖 𝑗∈𝐼𝑡

𝑖

𝐼𝑖𝑡 is a minibatch, |𝐼𝑖𝑡| = 𝜏𝑖

if 𝑖 ∈ 𝑆𝑡

if 𝑖 ̸∈ 𝑆𝑡

Master broadcasts 𝑏𝑡+1; 𝒞𝑤 is used on the workers’ side, 𝒞𝑀 is used on the master’s side

𝜂 ∈ [0,1) – momentum parameter
For problem (6); prox𝛾𝑟(𝑥) is deﬁned in (91)

Table 2: Description of the methods developed and analyzed in the paper. For the ease of comparison, we also provide a description of EF21. In all methods only compressed vectors 𝑐𝑡𝑖 are transmitted from workers to the master and the master broadcasts non-compressed iterates 𝑥𝑡+1 (except EF21-BC, where the master broadcasts compressed vector 𝑏𝑡+1). Initialization of 𝑔𝑖0, 𝑖 = 1, . . . , 𝑛 can be arbitrary (possibly randomized). One possible choice is 𝑔𝑖0 = 𝒞(∇𝑓𝑖(𝑥0)). The pseudocodes for each method are given in the appendix.

◇ EF21-PAGE: Error feedback and variance reduction. In the ﬁnite-sum regime (3), variance
reduced methods usually perform better than vanilla SGD in many situations (Gower et al., 2020).
Therefore, for this setup we modify EF21 and combine it with variance reduction. In particular, this time we replace ∇𝑓𝑖(𝑥𝑡+1) in the formula for 𝑐𝑡𝑖 with the PAGE estimator (Li et al., 2021) 𝑣𝑖𝑡+1. With (typically small) probability 𝑝 this estimator equals the full gradient 𝑣𝑖𝑡+1 = ∇𝑓𝑖(𝑥𝑡+1), and with probability 1 − 𝑝 it is set to

𝑣𝑡+1 = 𝑣𝑡 + 1 ∑︁ (︀∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡))︀ ,

𝑖

𝑖 𝜏𝑖

𝑗∈𝐼𝑖𝑡

where 𝐼𝑖𝑡 is a minibatch of size 𝜏𝑖. Typically, the number of data points 𝑚 owned by each client is large, and 𝑝 ≤ 1/𝑚 when 𝜏𝑖 ≡ 1. As a result, computation of full gradients rarely happens during the optimization procedure: on average, once in every 𝑚 iterations only. Although it is possible to
use other variance-reduced estimators like in SVRG or SAGA, we use the PAGE-estimator: unlike
SVRG or SAGA, PAGE is optimal for smooth nonconvex optimization, and therefore gives the best
theoretical guarantees (we have obtained results for both SVRG and SAGA and indeed, they are worse,
and hence we do not include them).

Notice that unlike VR-MARINA (Gorbunov et al., 2021), which is a state-of-the-art distributed optimization method designed speciﬁcally for unbiased compressors and which also uses the PAGEestimator, EF21-PAGE does not require the communication of full (non-compressed) vectors at all. This is an important property of the algorithm since, in some distributed networks, and especially when 𝑑 is very large, as is the case in modern over-parameterized deep learning, full vector communication is prohibitive. However, unlike the rate of VR-MARINA, the rate of EF21-PAGE does not improve with increasing 𝑛. This is not a ﬂaw of our method, but rather an inevitable drawback of distributed methods that rely on biased compressors such as Top-𝑘.

◇ EF21-PP: Error feedback and partial participation. The extension of EF21 to the case of
partial participation of the clients is mathematically identical to EF21 up to the following change: 𝑐𝑡𝑖 = 0 for all clients 𝑖 ̸∈ 𝑆𝑡 ⊆ {1, . . . ,𝑛} that are not selected for communication at iteration 𝑡. In practice, 𝑐𝑡𝑖 = 0 means that client 𝑖 does not take part in the 𝑡-th communication round. Here the set 𝑆𝑡 ⊆ {1, . . . ,𝑛} is formed randomly such that Prob(𝑖 ∈ 𝑆𝑡) = 𝑝𝑖 > 0 for all 𝑖 = 1, . . . , 𝑛.

5

EF21 with Bells & Whistles

Oct 6, 2021

◇ EF21-BC: Error feedback and bidirectional compression. The simplicity of the EF21 mechanism allows us to naturally extend it to the case when it is desirable to have efﬁcient/compressed communication between the clients and the server in both directions. At each iteration of EF21-BC, clients compute and send to the master node 𝑐𝑡𝑖 = 𝒞𝑤(∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡) and update 𝑔̃︀𝑖𝑡+1 = 𝑔̃︀𝑖𝑡 + 𝑐𝑡𝑖 in the usual way, i.e., workers apply the EF21 mechanism. The key difference between EF21 and EF21-BC is that the master node in EF21-BC also uses this mechanism: it computes and broadcasts to the workers the compressed vector
𝑏𝑡+1 = 𝒞𝑀 (𝑔𝑡+1 − 𝑔𝑡) ̃︀

and updates

𝑔𝑡+1 = 𝑔𝑡 + 𝑏𝑡+1,

where 𝑔𝑡+1 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖𝑡+1.

Vector 𝑔𝑡

is maintained by the master and workers.

Therefore,

the

clients are able to update it via using 𝑔𝑡+1 = 𝑔𝑡 + 𝑏𝑡+1 and compute 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡 once they

receive 𝑏𝑡+1.

◇ EF21-HB: Error feedback with momentum. We consider classical Heavy-ball method (Polyak, 1964) with EF21 estimator 𝑔𝑡:

𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑣𝑡,

𝑣𝑡+1 = 𝜂𝑣𝑡 + 𝑔𝑡+1,

𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖,

𝑡+1

1

𝑛
∑︁

𝑡+1

𝑡 1 ∑𝑛︁ 𝑡

𝑔= 𝑛

𝑔𝑖

=𝑔 + 𝑛

𝑐𝑖 .

𝑖=1

𝑖=1

The resulting method is not better than EF21 in terms of the complexity of ﬁnding 𝜀-stationary point, i.e., momentum does not improve the theoretical convergence rate. Unfortunately, this is common issue for a wide range of results for momentum methods Loizou & Richtárik (2020). However, it is important to theoretically analyze momentum-extensions such as EF21-HB due to their importance in practice and generalization behaviour.

◇ EF21-Prox: Error feedback for composite problems. Finally, we make EF21 applicable to the
composite optimization problems (6) by simply taking the prox-operator from the right-hand side of the 𝑥𝑡+1 update rule (7):

𝑥𝑡+1 = prox

(︀𝑥𝑡

−

𝛾𝑔𝑡)︀

=

arg

min

{︂ 𝛾𝑟(𝑥)

+

1 ‖𝑥

−

(𝑥𝑡

−

}︂ 𝛾𝑔𝑡)‖2

.

𝛾𝑟

𝑥∈R𝑑

2

This trick is simple, but, surprisingly, EF21-Prox is the ﬁrst distributed method with error-feedback that provably converges for composite problems (6).

4 THEORETICAL CONVERGENCE RESULTS
In this section, we formulate a single corollary derived from the main convergence theorems for our six enhancements of EF21, and formulate the assumptions that we use in the analysis. The complete statements of the theorems and their proofs are provided in the appendices. In Table 1 we compare our new results with existing results.

4.1 ASSUMPTIONS In this subsection, we list and discuss the assumptions that we use in the analysis.

4.1.1 GENERAL ASSUMPTIONS

To derive our convergence results, we invoke the following standard smoothness assumption. Assumption 1 (Smoothness and lower boundedness). Every 𝑓𝑖 has 𝐿𝑖-Lipschitz gradient, i.e.,
‖∇𝑓𝑖(𝑥) − ∇𝑓𝑖(𝑦)‖ ≤ 𝐿𝑖 ‖𝑥 − 𝑦‖

for

all

𝑖

∈

[𝑛], 𝑥, 𝑦

∈

R𝑑,

and

𝑓 inf

def
=

inf 𝑥∈R𝑑

𝑓 (𝑥)

>

−∞.

We also assume that the compression operators used by all algorithms satisfy the following property.

6

EF21 with Bells & Whistles

Oct 6, 2021

Deﬁnition 1 (Contractive compressors). We say that a (possibly randomized) map 𝒞 : R𝑑 → R𝑑 is a contractive compression operator, or simply contractive compressor, if there exists a constant 0 < 𝛼 ≤ 1 such that

E [︀‖𝒞(𝑥) − 𝑥‖2]︀ ≤ (1 − 𝛼) ‖𝑥‖2, ∀𝑥 ∈ R𝑑.

(8)

We emphasize that we do not assume 𝒞 to be unbiased. Hence, our theory works with the Top-𝑘 (Alistarh et al., 2018) and the Rank-𝑟 (Safaryan et al., 2021) compressors, for example.

4.1.2 ADDTIONAL ASSUMPTIONS FOR EF21-SGD

We analyze EF21-SGD under the assumption that local stochastic gradients ∇𝑓𝜉𝑡 (𝑥𝑡) satisfy the 𝑖𝑗
following inequality (see Assumption 2 of Khaled & Richtárik (2020)).

Assumption 2 (General assumption for stochastic gradients). We assume that for all 𝑖 = 1, . . . ,𝑛 there exist parameters 𝐴𝑖, 𝐶𝑖 ≥ 0, 𝐵𝑖 ≥ 1 such that

[︁

]︁

E ‖∇𝑓𝜉𝑡 (𝑥𝑡)‖2 | 𝑥𝑡 ≤ 2𝐴𝑖 (︀𝑓𝑖(𝑥𝑡) − 𝑓𝑖inf )︀ + 𝐵𝑖‖∇𝑓𝑖(𝑥𝑡)‖2 + 𝐶𝑖,

(9)

𝑖𝑗

where1 𝑓𝑖inf = inf𝑥∈R𝑑 𝑓𝑖(𝑥) > −∞.

Below we provide two examples of stochastic gradients ﬁtting this assumption (for more detail, see (Khaled & Richtárik, 2020)).
Example 1. Consider ∇𝑓𝜉𝑡 (𝑥𝑡) such that 𝑖𝑗

[︁

]︁

[︂ ⃦

⃦2 ]︂

E ∇𝑓𝜉𝑖𝑡𝑗 (𝑥𝑡) | 𝑥𝑡 = ∇𝑓𝑖(𝑥𝑡) and E ⃦⃦∇𝑓𝜉𝑖𝑡𝑗 (𝑥𝑡) − ∇𝑓𝑖(𝑥𝑡)⃦⃦ | 𝑥𝑡 ≤ 𝜎𝑖2

for some 𝜎𝑖 ≥ 0. Then, due to variance decomposition,(9) holds with 𝐴𝑖 = 0, 𝐵𝑖 = 0, 𝐶𝑖 = 𝜎𝑖2.
Example 2. Let 𝑓𝑖(𝑥) = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1 𝑓𝑖𝑗 (𝑥), 𝑓𝑖𝑗 be 𝐿𝑖𝑗 -smooth and 𝑓𝑖i𝑗nf = inf𝑥∈R𝑑 𝑓𝑖𝑗 (𝑥) > −∞. Following Gower et al. (2019), we consider a stochastic reformulation

⎡

⎤

1

𝑚𝑖
∑︁

𝑓𝑖(𝑥) = E𝑣𝑖∼𝒟𝑖 [𝑓𝑣𝑖 (𝑥)] = E𝑣𝑖∼𝒟𝑖 ⎣ 𝑚𝑖 𝑓𝑣𝑖𝑗 (𝑥)⎦ ,

(10)

𝑗=1

where E𝑣𝑖∼𝒟𝑖 [𝑣𝑖𝑗] = 1. One can show (see Proposition 2 of Khaled & Richtárik (2020)) that under

the assumption that E𝑣𝑖∼𝒟𝑖 [︀𝑣𝑖2𝑗]︀ is ﬁnite for all 𝑗 stochastic gradient ∇𝑓𝜉𝑡 (𝑥𝑡) = ∇𝑓𝑣𝑡 (𝑥𝑡) with

𝑖𝑗

𝑖

𝑣𝑖𝑡 sampled from 𝒟𝑖 satisﬁes (9) with 𝐴𝑖 = max𝑗 𝐿𝑖𝑗E𝑣𝑖∼𝒟𝑖 [︀𝑣𝑖2𝑗]︀, 𝐵𝑖 = 1, 𝐶𝑖 = 2𝐴𝑖∆i𝑖nf , where

∆i𝑖nf = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1(𝑓𝑖inf − 𝑓𝑖i𝑗nf ). In particular, if Prob(∇𝑓𝜉𝑖𝑡𝑗 (𝑥𝑡) = ∇𝑓𝑖𝑗 (𝑥𝑡)) = ∑︀𝑚 𝑙=𝐿𝑖1𝑗 𝐿𝑖𝑙 , then 𝐴𝑖 = 𝐿𝑖 = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1 𝐿𝑖𝑗 , 𝐵𝑖 = 1, and 𝐶𝑖 = 2𝐴𝑖∆i𝑖nf .

Stochastic gradient 𝑔ˆ𝑖(𝑥𝑡) is computed using a mini-batch of 𝜏𝑖 independent samples satisfying (9):

𝑡

def

1

𝜏𝑖
∑︁

𝑡

𝑔ˆ𝑖(𝑥 ) =

∇𝑓𝜉𝑡 (𝑥 ).

𝜏𝑖 𝑗=1 𝑖𝑗

4.1.3 ADDITIONAL ASSUMPTIONS FOR EF21-PAGE

In the analysis of EF21-PAGE, we rely on the following assumption.

Assumption 3 (Average ℒ-smoothness). Let every 𝑓𝑖 have the form (3). Assume that for all 𝑡 ≥ 0, 𝑖 = 1, . . . , 𝑛, and batch 𝐼𝑖𝑡 (of size 𝜏𝑖), the minibatch stochastic gradients difference

∆̃︀ 𝑡

def
=

1

∑︁ (∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡))

𝑖 𝜏𝑖

𝑗∈𝐼𝑖𝑡

1When 𝐴𝑖 = 0 one can ignore the ﬁrst term in the right-hand side of (9), i.e., assumption inf𝑥∈R𝑑 𝑓𝑖(𝑥) > −∞ is not required in this case.

7

EF21 with Bells & Whistles

Oct 6, 2021

[︁

]︁

computed on the node 𝑖, satisﬁes E ∆̃︀ 𝑡𝑖 | 𝑥𝑡,𝑥𝑡+1 = ∆𝑡𝑖 and

[︂ ⃦
𝑡

⃦2
𝑡

]︂
𝑡 𝑡+1

ℒ2𝑖 𝑡+1

𝑡2

E ⃦⃦∆̃︀ 𝑖 − ∆𝑖⃦⃦ | 𝑥 , 𝑥

≤ ‖𝑥 − 𝑥 ‖ 𝜏𝑖

(11)

with

some

ℒ𝑖

≥

0,

where

∆𝑡𝑖

def
=

∇𝑓𝑖(𝑥𝑡+1) − ∇𝑓𝑖(𝑥𝑡).

We

also

deﬁne

def
ℒ̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

(1−𝑝𝑖)ℒ2𝑖 .
𝜏

𝑖

This assumption is satisﬁed for many standard/popular sampling strategies. For example, if 𝐼𝑖𝑡 is a full batch, then ℒ𝑖 = 0. Another example is uniform sampling on {1, . . . , 𝑚}, and each 𝑓𝑖𝑗 is
𝐿𝑖𝑗-smooth. In this regime, one may verify that ℒ𝑖 ≤ max1≤𝑗≤𝑚 𝐿𝑖𝑗.

4.2 MAIN RESULTS

Below we formulate the corollary establishing the complexities for each method. The complete version of this result is formulated and rigorously derived for each method in the appendix.

Corollary 1. (a) Suppose that Assumption 1 holds. Then, there exist appropriate choices of
parameters for EF21-PP, EF21-BC, EF21-HB, EF21-Prox such that the number of communi-
cation rounds 𝑇 and the (expected) number of gradient computations at each node #grad for these methods to ﬁnd an 𝜀-stationary point, i.e., a point 𝑥ˆ𝑇 such that

E [︀‖∇𝑓 (𝑥ˆ𝑇 )‖2]︀ ≤ 𝜀2

for EF21-PP, EF21-BC, EF21-HB and

E [︀‖𝒢𝛾(𝑥ˆ𝑇 )‖2]︀ ≤ 𝜀2

for EF21-Prox, where 𝒢𝛾(𝑥) = 1/𝛾 (︀𝑥 − prox𝛾𝑟(𝑥 − 𝛾∇𝑓 (𝑥)))︀, are

EF21-PP:

(︃ )︃

(︃ )︃

𝐿̃︀𝛿0

𝐿̃︀𝛿0

𝑇 = 𝒪 𝑝𝛼𝜀2 , #grad = 𝒪 𝛼𝜀2

EF21-BC: EF21-HB:

(︃

)︃

𝐿̃︀𝛿0

𝑇 = #grad = 𝒪 𝛼𝑤𝛼𝑀 𝜀2

(︃

)︃

𝐿̃︀𝛿0 (︂ 1 1 )︂

𝑇 = #grad = 𝒪 𝜀2 𝛼 + 1 − 𝜂

EF21-Prox:

(︃ )︃ 𝐿̃︀𝛿0
𝑇 = #grad = 𝒪 𝛼𝜀2 ,

where 𝐿̃︀ d=ef √︁ 𝑛1 ∑︀𝑛𝑖=1 𝐿2𝑖 , 𝛿0 d=ef 𝑓 (𝑥0) − 𝑓 inf (for EF21-Prox, 𝛿0 = Φ(𝑥0) − Φ𝑖𝑛𝑓 ), 𝑝 is the probability of sampling the client in EF21-PP, 𝛼𝑤 and 𝛼𝑀 are contraction factors for compressors applied on the workers’ and the master’s sides respectively in EF21-BC, and 𝜂 ∈ [0,1) is the momentum parameter in EF21-HB.

(b) If Assumptions 1 and 2 in the setup from Example 1 hold, then there exist appropriate choices of parameters for EF21-SGD such that the corresponding 𝑇 and the averaged number of gradient computations at each node #grad are

EF21-SGD:

(︃ )︃

(︃

)︃

𝐿̃︀𝛿0

𝐿̃︀𝛿0 𝐿̃︀𝛿0𝜎2

𝑇 = 𝒪 𝛼𝜀2 , #grad = 𝒪 𝛼𝜀2 + 𝛼3𝜀4 ,

where

𝜎

=

1 𝑛

∑︀𝑛
𝑖=1

𝜎𝑖2.

(c) If Assumptions 1 and 3 hold, then there exist appropriate choices of parameters for EF21-

PAGE such that the corresponding 𝑇 and #grad are

EF21-PAGE:

(︃ (𝐿 + ℒ)𝛿0

√𝑚ℒ𝛿0 )︃

̃︀ ̃︀

̃︀

𝑇 = 𝒪 𝛼𝜀2 + 𝜀2 ,

(︃ (𝐿 + ℒ)𝛿0 √𝑚ℒ𝛿0 )︃

̃︀ ̃︀

̃︀

#grad = 𝒪 𝑚 + 𝛼𝜀2 + 𝜀2 ,

8

EF21 with Bells & Whistles

Oct 6, 2021

√︁

where ℒ̃︀ =

1−𝑝 𝑛

∑︀𝑛
𝑖=1

ℒ2𝑖

and

𝜏𝑖

≡

𝜏

=

1.

Remark: We highlight some points for our results in Corollary 1 as follows:
• For EF21-PP and EF21-Prox, none of previous error feedback methods work on these two settings (partial participation and proximal/composite case). Thus, we provide the ﬁrst convergence results for them. Moreover, we show that the gradient (computation) complexity for both EF21-PP and EF21-Prox is 𝒪(1/𝛼𝜀), matching the original vanilla EF21. It means that we extend EF21 to both settings for free.
• For EF21-BC, we show 𝒪(1/𝛼𝑤𝛼𝑀 𝜀2) complexity result. In particular, if one uses constant ratio of compression (e.g., 10%), then 𝛼 ≈ 0.1. Then the result will be 𝒪(1/𝜀2). However, previous result of DoubleSqueeze is 𝒪(Δ/𝜀3) and it also uses more strict assumption for the compressors (E [‖𝒞(𝑥) − 𝑥‖] ≤ ∆). Even if we ignore this, our results for EF21-BC is better than the one for DoubleSqueeze by a large factor 1/𝜀.
• Similarly, our result for EF21-HB is roughly 𝒪(1/𝜀2) (note that the momentum parameter 𝜂 is usually constant such as 0.2, 0.4, 0.9 used in our experiments). However, previous result of M-CSER is roughly 𝒪(𝐺/𝜀3) and it is proven under an additional bounded gradient assumption. Similarly, our EF21-HB is better by a large factor 1/𝜀.
• For EF21-SGD and EF21-PAGE, we want to reduce the gradient complexity by using (variance-reduced) stochastic gradients instead of full gradient in the vanilla EF21. Note that 𝜎2 and ∆inf in EF21-SGD could be much smaller than 𝐺 in Choco-SGD since 𝐺 always depends on the dimension (and can be even inﬁnite), while 𝜎2 and ∆inf are mostly dimension-free parameters (particularly, they are very small if the functions/data samples are similar/close). Thus, for high dimensional problems (e.g., deep neural networks), EF21-SGD can be better than Choco-SGD. Besides, in the ﬁnite-sum case (3), especially if the number of data samples 𝑚 on each√client is not very large, then EF21-PAGE is much better since its complexity is roughly 𝒪( 𝑚/𝜀2) while EF21-SGD ones is roughly 𝒪(𝜎2/𝜀4).

5 EXPERIMENTS

In this section, we consider a logistic regression problem with a non-convex regularizer

⎧

⎫

⎨

1 ∑𝑁︁ (︀

(︀

⊤ )︀)︀

𝑑
∑︁

𝑥2𝑗

⎬

min 𝑓 (𝑥) =

𝑑

𝑁

log 1 + exp −𝑏𝑖𝑎𝑖 𝑥 + 𝜆

1 + 𝑥2 ,

(12)

𝑥∈R ⎩ 𝑖=1

𝑗=1

𝑗⎭

where 𝑎𝑖 ∈ R𝑑, 𝑏𝑖 ∈ {−1,1} are the training data, and 𝜆 > 0 is the regularization parameter, which is set to 𝜆 = 0.1 in all experiments. For all methods the stepsizes are initially chosen as the largest stepsize predicted by theory for EF21 (see Theorem 1), then they are tuned individually for each parameter setting. We provide more details on the datasets, hardware, experimental setups, and additional experiments, including deep learning experiments in Appendix A.

5.1 EXPERIMENT 1: FAST CONVERGENCE WITH VARIANCE REDUCTION
In our ﬁrst experiment, we showcase the computation and communication superiority of EF21-PAGE (Alg. 3) over EF21-SGD.
Figure 1 illustrates that, in all cases, EF21-PAGE perfectly reduces the accumulated variance and converges to the desired tolerance, whereas EF21-SGD is stuck at some accuracy level. Moreover, EF21-PAGE turns out to be surprisingly efﬁcient with small bathsizes (eg, 1.5% of the local data ) both in terms of the number of epochs and the # bits sent to the server per client. Interestingly, for most datasets, a further increase of bathsize does not considerably improve the convergence.

9

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

100 10 2 10 4 10 6
0
100 10 2 10 4 10 6
0

mushrooms, Top-2 100

w8a, Top-2

a9a, Top-2 100

phishing, Top-1 100

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5% EF21-PAGE; 512x; 25% EF21-PAGE; 1024x; 12.5% EF21-PAGE; 1024x; 1.5%
20e0pochs 400

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 64x; 1.5%
10e0pochs 200

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 128x; 1.5%
20 epoc4h0s 60

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5% EF21-PAGE; 16x; 25% EF21-PAGE; 16x; 12.5% EF21-PAGE; 16x; 1.5%
20 epoc4h0s 60

mushrooms, Top-2
EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5% EF21-PAGE; 512x; 25% EF21-PAGE; 1024x; 12.5% EF21-PAGE; 1024x; 1.5%
100#0b0i0ts/n 2(C000S0)0 300000

|| f(xt)||2

100 10 2 10 4 10 6
0

(a) Convergence in epochs.

w8a, Top-2

a9a, Top-2

100

100

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 64x; 1.5%
100#0b0i0ts/n 2(C000S0)0 300000

|| f(xt)||2

10 2 10 4 10 6
0

10 2

|| f(xt)||2

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5%

10 4

EF21-SGD; 512x; 1.5%

EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5%

10 6

EF21-PAGE; 128x; 1.5%

#bit5s0/n00(C0 S) 100000 0

phishing, Top-1
EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5% EF21-PAGE; 16x; 25% EF21-PAGE; 16x; 12.5% EF21-PAGE; 16x; 1.5%
#b2it0s0/n00(C S) 40000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by 𝑛.

Figure 1: Comparison of EF21-PAGE and EF21-SGD with tuned parameters. By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 25%, 12.5% and 1.5% we refer to batchsizes equal ⌊0.25𝑁𝑖⌋, ⌊0.125𝑁𝑖⌋ and ⌊0.015𝑁𝑖⌋ for all clients 𝑖 = 1, . . . ,𝑛, where 𝑁𝑖 denotes the size of local dataset.

5.2 EXPERIMENT 2: ON THE EFFECT OF PARTIAL PARTICIPATION OF CLIENTS
This experiment shows that EF21-PP (Alg. 4) can reduce communication costs and can be more practical than EF21. For this comparison, we consider 𝑛 = 100 and, therefore, apply a different data partitioning, see Table 5 from Appendix A for more details.
It is predicted by our theory (Corollary 1) that, in terms of the number of iterations/communication rounds, partial participation slows down the convergence of EF21 by a fraction of participating clients . We observe this behavior in practice as well (see Figure 2a). However, since for EF21-PP the communications are considerably cheaper it outperforms EF21 in terms of # number of bits sent to the server per client on average (see Figure 2).

100 10 2 10 4 10 6
0
100 10 2 10 4 10 6
0

mushrooms, Top-2

EF21; 1024x EF21-PP; 512x; 50%

100

EF21-PP; 256x; 25%

EF21-PP; 128x; 12.5% EF21-PP; 64x; 6.5%

10 2

w8a, Top-2

a9a, Top-2 100
10 2

phishing, Top-1 100
10 2

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 4 EF21; 64x

10 4 EF21; 128x

10 4 EF21; 16x

EF21-PP; 64x; 50%

EF21-PP; 64x; 50%

EF21-PP; 8x; 50%

10 6

EF21-PP; 32x; 25% EF21-PP; 16x; 12.5%

10 6

EF21-PP; 32x; 25% EF21-PP; 16x; 12.5%

10 6

EF21-PP; 4x; 25% EF21-PP; 2x; 12.5%

EF21-PP; 8x; 6.5%

EF21-PP; 8x; 6.5%

EF21-PP; 1x; 6.5%

com5m00u0nicatio1n0r0o0u0nds 15000 0 com1m00u0nication20ro0u0nds 3000 0 com25m0unicatio5n0r0ounds 750

0 com2m5u0nication5r0o0unds 750

mushrooms, Top-2
EF21; 1024x EF21-PP; 512x; 50% EF21-PP; 256x; 25% EF21-PP; 128x; 12.5% EF21-PP; 64x; 6.5%

(a) Convergence in communication rounds.

w8a, Top-2

a9a, Top-2

100

100

10 2

10 2

|| f(xt)||2

|| f(xt)||2

#bit1s/0n0(0C00 S)

10 4 10 6 200000 0

EF21; 64x EF21-PP; 64x; 50% EF21-PP; 32x; 25% EF21-PP; 16x; 12.5% EF21-PP; 8x; 6.5%
#bits5/0n0(0C0 S)

10 4 10 6 100000 0

EF21; 128x EF21-PP; 64x; 50% EF21-PP; 32x; 25% EF21-PP; 16x; 12.5% EF21-PP; 8x; 6.5%
10#0b0i0ts/n (C20S0)00

|| f(xt)||2

100 10 2 10 4 10 6 30000 0

phishing, Top-1
EF21; 16x EF21-PP; 8x; 50% EF21-PP; 4x; 25% EF21-PP; 2x; 12.5% EF21-PP; 1x; 6.5%
#b5i0ts0/0n (C S) 10000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by 𝑛.

Figure 2: Comparison of EF21-PP and EF21 with tuned parameters. By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 50%, 25% , 12.5% and 6.5% we refer to a number of participating clients equal to ⌊0.5𝑛⌋, ⌊0.25𝑛⌋, ⌊0.125𝑛⌋ and ⌊0.065𝑛⌋.

10

|| f(xt)||2

|| f(xt)||2

EF21 with Bells & Whistles

Oct 6, 2021

REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communicationefﬁcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems (NIPS), pp. 1709–1720, 2017.
Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and Cédric Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1200– 1205. ACM, 2017.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.
Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017.
Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Léon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceedings of the symposium on learning and data science, Paris, volume 8, pp. 2624–2633, 2009.
Léon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421–436. Springer, 2012.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1–27, 2011.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243v1, 2020.
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly converging error compensated SGD. In 34th Conference on Neural Information Processing Systems (NeurIPS), 2020.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. MARINA: Faster nonconvex distributed learning with compression. In International Conference on Machine Learning, pp. 3788–3798. PMLR, 2021. arXiv:2102.07845.
Robert M Gower, Mark Schmidt, Francis Bach, and Peter Richtárik. Variance-reduced methods for machine learning. Proceedings of the IEEE, 108(11):1968–1983, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. SGD: General analysis and improved rates. In International Conference on Machine Learning, pp. 5200–5209. PMLR, 2019.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016.
11

EF21 with Bells & Whistles

Oct 6, 2021

Samuel Horváth and Peter Richtárik. A better alternative to error feedback for communicationefﬁcient distributed learning. In 9th International Conference on Learning Representations (ICLR), 2021.
Samuel Horváth, Chen-Yu Ho, L’udovít Horváth, Atal Narayan Sahu, Marco Canini, and Peter Richtárik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019a.
Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019b.
Rustem Islamov, Xun Qian, and Peter Richtárik. Distributed second order methods with fast rates and compressed communication. arXiv preprint arXiv:2102.07158, 2021.
Peter et al Kairouz. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback ﬁxes SignSGD and other gradient compression schemes. In 36th International Conference on Machine Learning (ICML), 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, 2020.
Ahmed Khaled and Peter Richtárik. Gradient descent with compressed iterates. In NeurIPS Workshop on Federated Learning for Data Privacy and Conﬁdentiality, 2019.
Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world. arXiv preprint arXiv:2002.03329, 2020.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Anastasia Koloskova, Tao Lin, S. Stich, and Martin Jaggi. Decentralized deep learning with arbitrary communication compression. In International Conference on Learning Representations (ICLR), 2020.
Jakub Konecˇný, H. Brendan McMahan, Felix Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication efﬁciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, 2009.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.
Guanghui Lan, Zhize Li, and Yi Zhou. A uniﬁed variance-reduced accelerated gradient method for convex optimization. In Advances in Neural Information Processing Systems, pp. 10462–10472, 2019.
Zhize Li. ANITA: An optimal loopless accelerated variance-reduced gradient method. arXiv preprint arXiv:2103.11333, 2021a.
Zhize Li. A short note of page: Optimal convergence rates for nonconvex optimization. arXiv preprint arXiv:2106.09663, 2021b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5569–5579, 2018.
12

EF21 with Bells & Whistles

Oct 6, 2021

Zhize Li and Peter Richtárik. A uniﬁed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Zhize Li and Peter Richtárik. CANITA: Faster rates for distributed convex optimization with communication compression. arXiv preprint arXiv:2107.09461, 2021a.
Zhize Li and Peter Richtárik. ZeroSARAH: Efﬁcient nonconvex ﬁnite-sum optimization with zero full gradient computation. arXiv preprint arXiv:2103.01447, 2021b.
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtárik. Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning (ICML), pp. 5895–5904. PMLR, 2020.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning (ICML), pp. 6286–6295. PMLR, 2021. arXiv:2008.10898.
Nicolas Loizou and Peter Richtárik. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. Computational Optimization and Applications, 77:653–710, 2020.
Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les équations aux dérivées partielles, 117(87-89):2, 1963.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takácˇ, and Peter Richtárik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshufﬂing: Simple analysis with vast improvements. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17309–17320. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ c8cc6e90ccbff44c9cee23611711cdc4-[]Paper.pdf.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ 2). In Doklady AN USSR, volume 269, pp. 543–547, 1983.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees. arXiv preprint arXiv:2006.14591, 2020.
Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864–878, 1963.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics, 4(5):1–17, 1964.
Xun Qian, Peter Richtárik, and Tong Zhang. Error compensated distributed SGD can be accelerated. arXiv preprint arXiv:2010.00091, 2020.
Zheng Qu and Peter Richtárik. Coordinate descent with arbitrary sampling ii: Expected separable overapproximation. arXiv preprint arXiv:1412.8063, 2014.
Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and practically faster error feedback. arXiv preprint arXiv:2106.05203, 2021.
Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richtárik. FedNL: Making Newton-type methods applicable to federated learning. arXiv preprint arXiv:2106.02969, 2021.
13

EF21 with Bells & Whistles

Oct 6, 2021

Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algorithms. Cambridge University Press, 2014.
Sebastian U. Stich, J.-B. Cordonnier, and Martin Jaggi. Sparsiﬁed SGD with memory. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2020.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient compression for distributed optimization. In Neural Information Processing Systems, 2019.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster stochastic variance reduction algorithms. arXiv preprint arXiv:1810.10690, 2018.
Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li, and Haibin Lin. CSER: Communication-efﬁcient SGD with error reset. In Advances in Neural Information Processing Systems (NeurIPS), pp. 12593–12603, 2020.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-iid federated learning. arXiv preprint arXiv:2101.11203v3, 2021.
Tianbao Yang, Qihang Lin, and Zhe Li. Uniﬁed convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.
Haoyu Zhao, Zhize Li, and Peter Richtárik. FedPAGE: A fast local stochastic gradient method for communication-efﬁcient federated learning. arXiv preprint arXiv:2108.04755, 2021.

14

EF21 with Bells & Whistles

Oct 6, 2021

APPENDIX

TABLE OF CONTENTS

1 Introduction

1

2 Our Contributions

2

3 Methods

4

4 Theoretical Convergence Results

6

4.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.1.1 General Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.1.2 Addtional Assumptions for EF21-SGD . . . . . . . . . . . . . . . . . . . 7

4.1.3 Additional Assumptions for EF21-PAGE . . . . . . . . . . . . . . . . . . . 7

4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

5 Experiments

9

5.1 Experiment 1: Fast convergence with variance reduction . . . . . . . . . . . . . . 9

5.2 Experiment 2: On the effect of partial participation of clients . . . . . . . . . . . . 10

A Extra Experiments

16

A.1 Non-Convex Logistic Regression: Additional Experiments and Details . . . . . . . 16

A.2 Experiments with Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

A.3 Deep Learning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B Notations and Assumptions

22

C EF21

23

C.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 23

C.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 25

D Stochastic Gradients

27

D.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 28

D.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 32

E Variance Reduction

35

E.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 37

E.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 41

F Partial Participation

44

F.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 47

F.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 48

G Bidirectional Compression

49

G.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 51

G.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 53

H Heavy Ball Momentum

55

H.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 57

I Composite Case

63

I.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 65

I.2 Convergence under Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . . . 67

15

EF21 with Bells & Whistles

Oct 6, 2021

J Useful Auxiliary Results

70

J.1 Basic Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

J.2 Useful Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

A EXTRA EXPERIMENTS

In this section, we give missing details on the experiments from Section 5, and provide additional experiments.

A.1 NON-CONVEX LOGISTIC REGRESSION: ADDITIONAL EXPERIMENTS AND DETAILS
Datasets, hardware and implementation. We use standard LibSVM datasets (Chang & Lin, 2011), and split each dataset among 𝑛 clients. For experiments 1, 3, 4 and 5, we chose 𝑛 = 20 whereas for the experiment 2 we consider 𝑛 = 100. The ﬁrst 𝑛 − 1 clients own equal parts, and the remaining part, of size 𝑁 − 𝑛 · ⌊𝑁/𝑛⌋, is assigned to the last client. We consider the heterogeneous data distribution regime (i.e. we do not make any additional assumptions on data similarity between workers). A summary of datasets and details of splitting data among workers can be found in Tables 3 and 5. The algorithms are implemented in Python 3.8; we use 3 different CPU cluster node types in all experiments: 1) AMD EPYC 7702 64-Core; 2) Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz; 3) Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. In all algorithms involving compression, we use Top-𝑘 (Alistarh et al., 2017) as a canonical example of contractive compressor 𝒞, and ﬁx the compression ratio 𝑘/𝑑 ≈ 0.01, where 𝑑 is the number of features in the dataset. For all algorithms, at each iteration we compute the squared norm of the exact/full gradient for comparison of the methods performance. We terminate our algorithms either if they reach the certain number of iterations or the following stopping criterion is satisﬁed: ‖∇𝑓 (𝑥𝑡)‖2 ≤ 10−7.
In all experiments, the stepsize is set to the largest stepsize predicted by theory for EF21 multiplied by some constant multiplier which was individually tuned in all cases.

Dataset

𝑛 𝑁 (total # of datapoints) 𝑑 (# of features) k 𝑁𝑖

mushrooms 20

w8a

20

a9a

20

phishing 20

8,120 49,749 32,560 11,055

112 2 406 300 2 2,487 123 2 1,628
68 1 552

Table 3: Summary of the datasets and splitting of the data among clients for Experiments 1, 3, 4, and 5. Here 𝑁𝑖 denotes the number of datapoints per client.

Experiment 1: Fast convergence with variance reductions (extra details). The parameters 𝑝𝑖 of the PAGE estimator are set to 𝑝𝑖 = 𝑝 d=ef 𝑛1 ∑︀𝑛𝑖=1 𝜏𝑖+𝜏𝑖𝑁𝑖 , where 𝜏𝑖 is the batchsize for clients 𝑖 = 1, . . . ,𝑛 (see Table 4 for details). In our experiments, we assume that the sampling of Bernoulli random variable is performed on server side (which means that at each iteration for all clients 𝑏𝑡𝑖 = 1 or 𝑏𝑡𝑖 = 0). And if 𝑏𝑡𝑖 = 0, then in line 5 of Algorithm 3 𝐼𝑖𝑡 is sampled without replacement uniformly at random. Table 4 shows the selection of parameter 𝑝 for each experiment.
For each batchsize from the set2
{95%, 50%, 25%,12.5%, 6.5%,3%} ,
we tune the stepsize multiplier for EF21-PAGE within the set
{0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
The best pair (batchsize, stepsize multiplier) is chosen in such a way that it gives the best convergence in terms of #bits/𝑛(𝐶 → 𝑆). In the rest of the experiments, ﬁne tuning is performed in a similar fashion.
2By 50%, 25% (and so on) we refer to a batchsize, which is equals to ⌊0.5𝑁𝑖⌋, ⌊0.25𝑁𝑖⌋ (and so on) for all clients 𝑖 = 1, . . . ,𝑛.

16

EF21 with Bells & Whistles

Dataset
mushrooms w8a a9a phishing

25%
0.1992 0.1998 0.2 0.2

12.5%
0.1097 0.1108 0.1109 0.1111

1.5%
0.0146 0.0147 0.0145 0.0143

Table 4: Summary of the parameter choice of 𝑝.

Oct 6, 2021

Experiment 2: On the effect of partial participation of clients (extra details) In this experiment, we consider 𝑛 = 100 and, therefore, a different data partitioning, see Table 5 for the summary.

Dataset

𝑛 𝑁 (total # of datapoints) 𝑑 (# of features) k 𝑁𝑖

mushrooms 100

w8a

100

a9a

100

phishing 100

8,120 49,749 32,560 11,055

112 2 81 300 2 497 123 2 325 68 1 110

Table 5: Summary of the datasets and splitting of the data among clients for Experiment 5. Here 𝑁𝑖 denotes the number of datapoints per client.

We tune the stepsize multiplier for EF21-PP within the following set:
{0.125,0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096} .
Experiment 3: On the advantages of bidirectional biased compression. Our next experiment demonstrates that the application of the Server → Clients compression in EF21-BC (Alg. 5) does not signiﬁcantly slow down the convergence in terms of the communication rounds but requires much less bits to be transmitted. Indeed, Figure 3a illustrates that that it is sufﬁcient to communicate only 5% − 15% of data to perform similarly to EF21 (Alg. 1).3 Note that EF21 communicates full vectors from the Server → Clients, and, therefore, may have slower communication at each round. In Figure 3b we take into account only the number of bits sent from clients to the server, and therefore we observe the same behavior as in Figure 3a. However, if we care about the total number of bits (see Figure 3c), then EF21-BC considerably outperforms EF21 in all cases.

3The range 5% − 15% comes from the fractions 𝑘/𝑑 for each dataset. 17

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

10 4

10 4

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

com1m00u0nication20ro0u0nds

10 6 3000 0

com10m0u0nicatio2n0r0o0unds 3000

10 6 0

communic5a0t0ion rounds

10 6 1000 0

comm2u0n0ication rou4n0d0s

(a) Convergence in communication rounds. mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

10 4

10 4

10 4

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

#bit1s/0n0(0C00 S)

10 6 200000 0

#bi1ts0/0n0(0C0 S)

10 6 200000 0

20#00b0its/n (C400S0)0

10 6

60000

0

50#00bits/n1(C000S0) 15000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by 𝑛. mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

10 4

10 4

10 4

10 4

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

#bits/n (C1 S+S C)

10 6 1e26 0.0

#bits/n (C0.5S+S C)

10 6

11e.06

0

#bi2ts0/n00(0C0 S+S 40C0)000

10 6 0

#bits/n2(0C000S0+S C) 400000

(c) Convergence in terms of total number of bits sent from Clients to the Server plus the total number of bits broadcasted from Server to Clients divided by 𝑛.

Figure 3: Comparison of EF21-BC and EF21 with tuned stepsizes . By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 1) .

For each parameter 𝑘 in Server-Clients compression, we tune the stepsize multiplier for EF21-BC within the following set:
{0.125,0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
Experiment 4: On the cheaper computations via EF21-SGD. The fourth experiment (see Figure 4a) illustrates that EF21-SGD (Alg. 2) is the more preferable choice than EF21 for the cases when full gradient computations are costly. For each batchsize from the set4
{95%, 50%, 25%,12.5%, 6.5%,3%} ,
we tune the stepsize multiplier for EF21-SGD within the following set:
{0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
Figure 4a illustrates that EF21-SGD is able to reach a moderate tolerance in 5 − 10 epochs.
4By 50%, 25% (and so on) we refer to a batchsize, which is equals to ⌊0.5𝑁𝑖⌋, ⌊0.25𝑁𝑖⌋ (and so on) for all clients 𝑖 = 1, . . . ,𝑛.
18

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

mushrooms, Top-2

100

100

w8a, Top-2

a9a, Top-2 100

phishing, Top-1 100

10 2

10 2

10 2

10 2

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 4 EF21; 1024x

10 4 EF21; 64x

10 4 EF21; 64x

10 4 EF21; 16x

EF21-SGD; 2048x; 50%

EF21-SGD; 32x; 50%

EF21-SGD; 32x; 50%

EF21-SGD; 16x; 50%

10 6

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5%

10 6

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5%

10 6

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5%

10 6

EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5%

EF21-SGD; 128x; 1.5%

EF21-SGD; 512x; 1.5%

EF21-SGD; 512x; 1.5%

EF21-SGD; 16x; 1.5%

0

ep2o0chs

40

0

ep2o0chs

40

0

ep2o0chs

40

0

ep2o0chs

40

100 10 2 10 4 10 6
0

mushrooms, Top-2
EF21; 1024x EF21-SGD; 2048x; 50% EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5%
1#0b0it0s0/n0(C S)200000

|| f(xt)||2

100 10 2 10 4 10 6
0

(a) Convergence in epochs.

w8a, Top-2

a9a, Top-2

100

10 2

|| f(xt)||2

EF21; 64x EF21-SGD; 32x; 50% EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5%
5#00b0it0s/n (C 10S0)000

10 4 10 6
0

EF21; 64x EF21-SGD; 32x; 50% EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5% EF21-SGD; 512x; 1.5%
20#00b0its/n (4C000S0)

60000

|| f(xt)||2

100 10 2 10 4 10 6
0

phishing, Top-1
EF21; 16x EF21-SGD; 16x; 50% EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5%
10#00b0its/n (2C000S0) 30000

(b) Convergence in terms of the number of bits sent from Clients to the Server by each client.

Figure 4: Comparison of EF21-SGD and EF21 with tuned stepsizes. By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 50%, 25% (and so on) we refer to a batchsize, which is equals to ⌊0.5𝑁𝑖⌋, ⌊0.25𝑁𝑖⌋ (and so on) for all clients 𝑖 = 1, . . . ,𝑛.

However, due to the accumulated variance introduced by SGD, estimator EF21-SGD is stuck at some accuracy level (see Figure 4b), showing the usual behavior of the SGD observed in practice.

Experiment 5: On the effect of heavy ball momentum. In this experiment (see Figure 5), we show that for the majority of the considered datasets heavy ball acceleration used in EF21-HB (Alg. 6) improves the convergence of EF21 method. For every dataset (and correspondingly chosen parameter 𝑘) we tune momentum parameter 𝜂 in EF21-HB by making a grid search over all possible parameter values from 0.05 to 0.99 with the step 0.05. Finally, for our plots we pick 𝜂 ∈ {0.05, 0.2, 0.25, 0.4, 0.9} since the ﬁrst four values shows the best performance and 𝜂 = 0.9 is a popular choice in practice.
For each parameter 𝜂 from the set
{0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99} .
we perform a grid search of stepsize multiplier within the powers of 2:
{0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .

100 10 2 10 4 10 6
0

mushrooms, Top-2

EF21; 1024x EF21-HB; 1024x; = 0.05

100

EF21-HB; 512x; = 0.2

EF21-HB; 512x; = 0.25 EF21-HB; 512x; = 0.4

10 2

EF21-HB; 16x; = 0.9

10 4

#bit1s/0n0(0C00 S)

10 6 200000 0

w8a, Top-2

EF21; 64x EF21-HB; 64x; = 0.05 EF21-HB; 64x; = 0.2 EF21-HB; 64x; = 0.25 EF21-HB; 32x; = 0.4 EF21-HB; 4x; = 0.9
#50b0it0s0/n (C

S1)00000

100 10 2 10 4 10 6
0

a9a, Top-2
EF21; 64x EF21-HB; 64x; = 0.05 EF21-HB; 64x; = 0.2 EF21-HB; 64x; = 0.25 EF21-HB; 32x; = 0.4 EF21-HB; 4x; = 0.9
#2b0it0s0/n0(C S) 40000

100 10 2 10 4 10 6
0

phishing, Top-1
EF21; 16x EF21-HB; 16x; = 0.05 EF21-HB; 16x; = 0.2 EF21-HB; 8x; = 0.25 EF21-HB; 8x; = 0.4 EF21-HB; 1x; = 0.9

#bits1/n00(0C0 S)

20000

Figure 5: Comparison of EF21-HB and EF21 with tuned parameters in terms of total number of bits sent from Clients to the Server divided by 𝑛. By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 1) .

A.2 EXPERIMENTS WITH LEAST SQUARES

In this section, we conduct the experiments on a function satisfying the PŁ-condition (see Assumption 4). In particular, we consider the least squares problem:

{︃

1

𝑛
∑︁

}︃

min 𝑓 (𝑥) =

(𝑎⊤𝑖 𝑥 − 𝑏𝑖)2 ,

𝑥∈R𝑑

𝑛

𝑖=1

19

|| f(xt)||2 || f(xt)||2 || f(xt)||2 || f(xt)||2

EF21 with Bells & Whistles

Oct 6, 2021

f(xt) f(x * )

|| f(xt)||2

where 𝑎𝑖 ∈ R𝑑, 𝑏𝑖 ∈ {−1,1} are the training data. We use the same datasets as for the logistic regression problem.

Experiment: On the effect of heavy ball momentum in PŁ-setting. For PŁ-setting, EF21-HB also improves the convergence over EF21 for the majority of the datasets (see Figure 6). Stepsize and momentum parameter 𝜂 are chosen using the same strategy as for the logistic regression experiments (see section A.1).

104 102 100 10 2 10 4
0.0
103 100 10 3 10 6
0.0

mushrooms, Top-2

104

EF21; 2048x

EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15

102

EF21-HB; 2048x; = 0.3

EF21-HB; 8x; = 0.85

100

f(xt) f(x * )

10 2

#bits/n0.(5C S)

10 4 1e16.0 0.0

mushrooms, Top-2

EF21; 2048x EF21-HB; 2048x; = 0.05

103

EF21-HB; 2048x; = 0.15 EF21-HB; 2048x; = 0.3

100

EF21-HB; 8x; = 0.85

10 3

|| f(xt)||2

10 6

#bits/n0.(5C S)

1e16.0 0.0

w8a, Top-2
EF21; 4096x EF21-HB; 4096x; = 0.05 EF21-HB; 4096x; = 0.15 EF21-HB; 4096x; = 0.3 EF21-HB; 32x; = 0.85
#bi0ts.5/n (C S) 1.0 1e6

f(xt) f(x * )

104 102 100 10 2 10 4
0

w8a, Top-2

EF21; 4096x EF21-HB; 4096x; = 0.05

103

EF21-HB; 4096x; = 0.15 EF21-HB; 4096x; = 0.3

100

EF21-HB; 32x; = 0.85

10 3

|| f(xt)||2

10 6

#bi0ts.5/n (C S) 1.0 1e6

0

a9a, Top-2

104

EF21; 1024x

EF21-HB; 1024x; = 0.05 EF21-HB; 1024x; = 0.15

102

EF21-HB; 1024x; = 0.3

EF21-HB; 16x; = 0.85

100

f(xt) f(x * )

10 2

250#0b0it0s/n (C500S0)00

10 4 750000 0

a9a, Top-2

EF21; 1024x EF21-HB; 1024x; = 0.05

103

EF21-HB; 1024x; = 0.15 EF21-HB; 1024x; = 0.3

100

EF21-HB; 16x; = 0.85

10 3

|| f(xt)||2

10 6

250#0b0it0s/n (C500S0)00 750000 0

phishing, Top-1
EF21; 4096x EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15 EF21-HB; 1024x; = 0.3 EF21-HB; 4x; = 0.85
50#0b0its/n (1C000S0) 15000 phishing, Top-1
EF21; 4096x EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15 EF21-HB; 1024x; = 0.3 EF21-HB; 4x; = 0.85
50#0b0its/n (1C000S0) 15000

Figure 6: Comparison of EF21-HB and EF21 with tuned parameters in terms of total number of bits sent from Clients to the Server divided by 𝑛. By 1×, 2×, 4× (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 2) .

A.3 DEEP LEARNING EXPERIMENTS
In this experiment, the exact/full gradient ∇𝑓𝑖(𝑥𝑘+1) in the algorithm EF21-HB is replaced by its stochastic estimator (we later refer to this method as EF21-SGD-HB). We compare the resulting method with some existing baselines on a deep learning multi-class image classiﬁcation task. In particular, we compare our EF21-SGD-HB method to EF21+-SGD-HB5 , EF-SGD-HB6, EF21-SGD and EF-SGD on the problem of training ResNet18 (He et al., 2016) model on CIFAR-10 (Krizhevsky et al., 2009) dataset. For more details about the EF21+ and EF type methods and their applications in deep learning we refer reader to (Richtárik et al., 2021). We implement the algorithms in PyTorch (Paszke et al., 2019) and run the experiments on a single GPU NVIDIA GeForce RTX 2080 Ti. The dataset is split into 𝑛 = 8 equal parts. Total train set size for CIFAR-10 is 50,000. The test set for evaluation has 10,000 data points. The train set is split into batches of size 𝜏 = 32. The ﬁrst seven workers own an equal number of batches of data, while the last worker gets the rest. In our experiments, we ﬁx 𝑘 ≈ 0.05𝑑, 𝜏 = 32 and momentum parameter 𝜂 = 0.9.7 As it is usually done in deep learning applications, stochastic gradients are generated via so-called “shufﬂe once” strategy, i.e., workers randomly shufﬂe their datasets and then select minibatches using the obtained order (Bottou, 2009; 2012; Mishchenko et al., 2020). We tune the stepsize 𝛾 within the range {0.0625, 0.125, 0.25, 0.5, 1} and for each method we individually chose the one 𝛾 giving the highest accuracy score on test. For momentum methods, the best stepsize was 0.5, whereas for the non-momentum ones it was 0.125.
The experiments show (see Figure 7) that the train loss for momentum methods decreases slower than for the non-momentum ones, whereas for the test loss situation is the opposite. Finally, momentum methods show a considerable improvement in the accuracy score on the test set over the existing EF21-SGD and EF-SGD.
5EF21+-SGD-HB is the method obtained from EF21-SGD-HB via replacing EF21 by EF21+ compressor 6EF-SGD-HB is the method obtained from EF21-SGD-HB via replacing EF21 by EF compressor 7Here, 𝑑 is the number of model parameters. For ResNet18, 𝑑 = 11,511,784.
20

EF21 with Bells & Whistles

Oct 6, 2021

Test accuracy Train loss Test loss

80

60

40 20
0

1000

2000 3000 4000 communication rounds

EF21+-SGD-HB EF21-SGD-HB EF-SGD-HB EF21-SGD EF-SGD
5000 6000

101
EF21+-SGD-HB

EF21+-SGD-HB

EF21-SGD-HB

EF21-SGD-HB

EF-SGD-HB

2 × 100

EF-SGD-HB

100

EF21-SGD

EF21-SGD

EF-SGD

EF-SGD

10−1

10−2 0

1000

2000 3000 4000 5000 communication rounds

6000

100 0 1000 2000 3000 4000 5000 6000 communication rounds

Figure 7: Comparison of EF-SGD and EF21-SGD with EF-SGD-HB, EF21-SGD-HB, and EF21+SGD-HB with tuned stepsizes applied to train ResNet18 on CIFAR10.

21

EF21 with Bells & Whistles

Oct 6, 2021

EF21 EF21-SGD EF21-PP
EF21-PAGE
EF21-BC EF21-HB
EF21-Prox

𝑅𝑡 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2, 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 inf

𝑅𝑡 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 , 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 inf ,

𝑃𝑖𝑡 = ‖∇𝑓𝑖(𝑥𝑡) − 𝑣𝑖𝑡‖2 , 𝑉𝑖𝑡 = ‖𝑣𝑖𝑡 − 𝑔𝑖𝑡‖2 𝑃𝑖𝑡 = ‖𝑔𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡)‖2, 𝑅𝑡 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 , 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 inf
̃︀

𝑅𝑡 = (1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 , 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 inf

𝑅𝑡 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2, Φ(𝑥) = 𝑓 (𝑥) + 𝑟(𝑥), 𝛿𝑡 = Φ(𝑥𝑡) − Φ𝑖𝑛𝑓 ,

𝒢𝛾 (𝑥)

=

1 𝛾

(︀𝑥

−

prox𝛾 𝑟 (𝑥

−

𝛾∇𝑓 (𝑥)))︀

Table 6: Summary of frequently used notations in the proofs.

B NOTATIONS AND ASSUMPTIONS

We now introduce an additional assumption, which enables us to obtain a faster linear convergence result in different settings.
Assumption 4 (Polyak-Łojasiewicz). There exists 𝜇 > 0 such that 𝑓 (𝑥) − 𝑓 (𝑥⋆) ≤ 21𝜇 ‖∇𝑓 (𝑥)‖2 for all 𝑥 ∈ R𝑑, where 𝑥⋆ = arg min𝑥∈R𝑑 𝑓 .

Table 6 summarizes the most frequently used notations in our analysis. Additionally, we comment on

the main quantities here. We deﬁne 𝛿𝑡 d=ef 𝑓 (𝑥𝑡)−𝑓 inf 8, 𝑅𝑡 d=ef ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2. In the analysis of EF21-

HB, it is useful to adapt this notation to 𝑅𝑡 d=ef (1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2, where {𝑧𝑡}𝑡≥0 is the sequence

of

virtual

iterates

introduced

in

Section

H.

We

denote

𝐺𝑡𝑖

d=ef

‖∇𝑓𝑖(𝑥𝑡) − 𝑔𝑖𝑡‖2,

𝐺𝑡

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝐺𝑡𝑖

following Richtárik et al. (2021), where 𝑔𝑖𝑡 is an EF21 estimator at a node 𝑖. Throughout the paper

𝐿̃︀2

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

where

𝐿𝑖

is

a

smoothness

constant

for

𝑓𝑖(·),

𝑖

=

1, . . . , 𝑛

(see

Assumption

1).

8If, additionally, Assumption 4 holds, then 𝑓 inf can be replaced by 𝑓 (𝑥⋆) for 𝑥⋆ = arg min𝑥∈R𝑑 𝑓 (𝑥). 22

EF21 with Bells & Whistles

Oct 6, 2021

C EF21

For completeness, we provide here the detailed proofs for EF21 (Richtárik et al., 2021).

Algorithm 1 EF21

1:

Input:

starting point 𝑥0

∈ R𝑑; 𝑔𝑖0

∈ R𝑑

for 𝑖 =

1, . . . , 𝑛 (known by nodes); 𝑔0

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known by master); learning rate 𝛾 > 0

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡 and broadcasts 𝑥𝑡+1 to all nodes

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

5:

Compress 𝑐𝑡𝑖 = 𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

6:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

7: end for

8:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖

9: end for

Lemma 1. Let 𝒞 be a contractive compressor, then for all 𝑖 = 1, . . . , 𝑛

E [︀𝐺𝑡𝑖+1]︀

≤

[︁ (1 − 𝜃)E [︀𝐺𝑡𝑖]︀ + 𝛽𝐿2𝑖 E ⃦⃦𝑥𝑡+1

−

𝑥𝑡

⃦2 ⃦

]︁

,

and

(13)

E

[︀𝐺𝑡+1]︀

≤

(1

−

𝜃)E

[︀𝐺𝑡]︀

+

𝛽𝐿̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

,

(14)

def
where 𝜃 = 1 − (1 − 𝛼)(1 + 𝑠),

𝛽

def
=

(1

−

𝛼)

(︀1

+

𝑠−1)︀

for any 𝑠 > 0.

Proof. Deﬁne 𝑊 𝑡 d=ef {𝑔1𝑡 , . . . , 𝑔𝑛𝑡 , 𝑥𝑡, 𝑥𝑡+1}, then

E [︀𝐺𝑡𝑖+1]︀

=

E

[︀ E

[︀𝐺𝑡𝑖+1

|

𝑊 𝑡]︀]︀

=

E

[︁ E

[︁ ⃦⃦𝑔𝑖𝑡+1

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2

]︁]︁ | 𝑊𝑡

=

E

[︁ E

[︁ ⃦⃦𝑔𝑖𝑡

+

𝒞(∇𝑓𝑖(𝑥𝑡+1)

−

𝑔𝑖𝑡)

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2

]︁]︁ | 𝑊𝑡

(≤8) (1 − 𝛼)E [︁⃦⃦∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡⃦⃦2]︁

(≤𝑖) (1 − 𝛼)(1 + 𝑠)E [︁⃦⃦∇𝑓𝑖(𝑥𝑡) − 𝑔𝑖𝑡⃦⃦2]︁

+(1

−

𝛼)

(︀1

+

𝑠−1

)︀

⃦ ⃦∇𝑓𝑖

(𝑥𝑡+1

)

−

∇𝑓𝑖

(𝑥𝑡

⃦2 )⃦

(15)

(≤𝑖𝑖) (1 − 𝛼)(1 + 𝑠)E [︁⃦⃦∇𝑓𝑖(𝑥𝑡) − 𝑔𝑖𝑡⃦⃦2]︁

+(1

−

𝛼)

(︀1

+

𝑠−1)︀

𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

(𝑖≤𝑖𝑖) (1 − 𝜃)E [︁⃦⃦∇𝑓𝑖(𝑥𝑡) − 𝑔𝑖𝑡⃦⃦2]︁ + 𝛽𝐿2𝑖 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁ ,

where (𝑖) follows by Young’s inequality (118), (𝑖𝑖) holds by Assumption 1, and in (𝑖𝑖𝑖) we apply the deﬁnition of 𝜃 and 𝛽. Averaging the above inequalities over 𝑖 = 1, . . . , 𝑛, we obtain (14).

C.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 1. Let Assumption 1 hold, and let the stepsize in Algorithm 1 be set as

(︃

√︂ )︃−1

𝛽

0 < 𝛾 ≤ 𝐿 + 𝐿̃︀

.

(16)

𝜃

Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

E [︁⃦⃦∇𝑓 (𝑥ˆ𝑇 )⃦⃦2]︁ ≤ 2 (︀𝑓 (𝑥0) − 𝑓 inf )︀ + E [︀𝐺0]︀ , (17)

𝛾𝑇

𝜃𝑇

23

EF21 with Bells & Whistles

Oct 6, 2021

√︁

where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1

−

(1

−

𝛼)(1

+

𝑠),

𝛽

=

(1

−

𝛼)

(︀1

+

𝑠−1)︀

for

any

𝑠

>

0.

Proof. According to our notation, for Algorithm 1 𝑅𝑡 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2. By Lemma 1, we have

E [︀𝐺𝑡+1]︀ ≤ (1 − 𝜃) E [︀𝐺𝑡]︀ + 𝛽𝐿̃︀2E [︀𝑅𝑡]︀ .

(18)

Next, using Lemma 16 and Jensen’s inequality (119), we obtain the bound

(︂

)︂

⃦𝑛

⃦2

𝑓 (𝑥𝑡+1) ≤ 𝑓 (𝑥𝑡) − 𝛾2 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − 21𝛾 − 𝐿2 𝑅𝑡 + 𝛾2 ⃦⃦⃦ 𝑛1 ∑︁ (︀𝑔𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡))︀⃦⃦⃦

⃦ 𝑖=1

⃦

𝑡 𝛾⃦

𝑡 ⃦2 (︂ 1 𝐿 )︂ 𝑡 𝛾 1 ∑𝑛︁ ⃦ 𝑡

𝑡 ⃦2

≤ 𝑓 (𝑥 ) − ⃦∇𝑓 (𝑥 )⃦ − − 𝑅 +

2

2𝛾 2

2𝑛

⃦𝑔𝑖 − ∇𝑓𝑖(𝑥 )⃦

𝑖=1

= 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 𝐺𝑡. (19)

2

2𝛾 2

2

Subtracting 𝑓 inf from both sides of the above inequality, taking expectation and using the notation 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 inf , we get

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ . (20)

2

2𝛾 2

2

Then by adding (20) with a 2𝛾𝜃 multiple of (18) we obtain

E [︀𝛿𝑡+1]︀ + 𝛾 E [︀𝐺𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀

2𝜃

2

2𝛾 2

2

𝛾 +

(︁

)︁

𝛽𝐿̃︀2E [︀𝑅𝑡]︀ + (1 − 𝜃)E [︀𝐺𝑡]︀

2𝜃

= E [︀𝛿𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁

2𝜃

2

(︂ 1 −

𝐿 −−

𝛾

)︂ 𝛽𝐿2 E [︀𝑅𝑡]︀

̃︀

2𝛾 2 2𝜃

≤ E [︀𝛿𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ .

2𝜃

2

The last inequality follows from the bound 𝛾2 𝛽𝐿𝜃̃︀2 + 𝐿𝛾 ≤ 1, which holds because of Lemma 15 and our assumption on the stepsize. By summing up inequalities for 𝑡 = 0, . . . , 𝑇 − 1, we get

0 ≤ E [︁𝛿𝑇 + 𝛾 𝐺𝑇 ]︁ ≤ 𝛿0 + 𝛾 E [︀𝐺0]︀ − 𝛾 𝑇∑−︁1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ .

2𝜃

2𝜃

2

𝑡=0

Multiplying both sides by 𝛾2𝑇 , after rearranging we get

𝑇∑−︁1 1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ ≤ 2𝛿0 + E [︀𝐺0]︀ .

𝑇

𝛾𝑇

𝜃𝑇

𝑡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[︁ ⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

,

where

𝑥ˆ𝑇

is

chosen

from 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random.

Corollary 2. Let assumptions of Theorem 1 hold,
𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛, (︁ √︀ )︁−1
𝛾 = 𝐿 + 𝐿̃︀ 𝛽/𝜃 .

24

EF21 with Bells & Whistles

Oct 6, 2021

Then,

after

𝑇

iterations/communication

rounds

of

EF21

we

have

E

[︁ ⃦ ⃦∇

𝑓

(

𝑥ˆ

𝑇

)⃦⃦2

]︁

≤

𝜀2.

It

requires

(︃ )︃ 𝐿̃︀𝛿0
𝑇 = #grad = 𝒪 𝛼𝜀2

√︁

iterations/communications rounds/gradint computations at each node, where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝛿0 = 𝑓 (𝑥0) − 𝑓 𝑖𝑛𝑓 .

Proof. Since 𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛 , we have 𝐺0 = 0 and by Theorem 1

(︃ (𝑖) 2𝛿0 (𝑖𝑖) 2𝛿0

√︂ )︃ 𝛽

(𝑖𝑖𝑖)

2𝛿0

(︂

(︂ 2 )︂)︂

#grad = 𝑇 ≤ 𝛾𝜀2 ≤ 𝜀2

𝐿 + 𝐿̃︀ 𝜃

≤ 𝜀2 𝐿 + 𝐿̃︀ 𝛼 − 1

(︃

)︃

(︃

)︃

2𝛿0

2𝐿̃︀ (𝑖𝑣) 2𝛿0 𝐿̃︀ 2𝐿̃︀

6𝐿̃︀𝛿0

≤ 𝜀2 𝐿 + 𝛼 ≤ 𝜀2 𝛼 + 𝛼 = 𝛼𝜀2 ,

where in (𝑖) is due to the rate (17) given by Theorem 1. In two (𝑖𝑖) we plug in the stepsize, in (𝑖𝑖𝑖) we use Lemma 17, and (𝑖𝑣) follows by the inequalities 𝛼 ≤ 1, and 𝐿 ≤ 𝐿̃︀.

C.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION

Theorem 2. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 1 be set as

⎧ (︃

√︂ )︃−1 ⎫

⎨

2𝛽

𝜃⎬

0 < 𝛾 ≤ min 𝐿 + 𝐿̃︀

,

.

(21)

⎩ 𝜃 2𝜇 ⎭

Let Ψ𝑡 d=ef 𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆) + 𝛾𝜃 𝐺𝑡. Then for any 𝑇 ≥ 0, we have

E [︀Ψ𝑇 ]︀ ≤ (1 − 𝛾𝜇)𝑇 E [︀Ψ0]︀ ,

(22)

√︁

where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1

−

(1

−

𝛼)(1

+

𝑠),

𝛽

=

(1

−

𝛼)

(︀1

+

𝑠−1)︀

for

any

𝑠

>

0.

Proof. We proceed as in the previous proof, but use the PL inequality, subtract 𝑓 (𝑥⋆) from both sides of (19) and utilize the notation 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆)

𝛿𝑡+1 ≤ 𝛿𝑡 − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 𝐺𝑡

2

2𝛾 2

2

≤ 𝛿𝑡 − 𝛾𝜇 (︀𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆))︀ − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 𝐺𝑡.

2𝛾 2

2

= (1 − 𝛾𝜇)𝛿𝑡 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 𝐺𝑡.

2𝛾 2

2

Take expectation on both sides of the above inequality and add it with a 𝛾𝜃 multiple of (18), then

E [︀𝛿𝑡+1]︀ + E [︁ 𝛾 𝐺𝑡+1]︁ ≤ (1 − 𝛾𝜇)E [︀𝛿𝑡]︀ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀

𝜃

2𝛾 2

2

𝛾 +

(︁ (1

−

𝜃)E

[︀𝐺𝑡]︀

+

𝛽𝐿̃︀2E

)︁ [︀𝑅𝑡]︀

𝜃

=

(1

−

𝛾𝜇)E

[︀𝛿𝑡]︀

+

𝛾

(︂ 1

−

𝜃

)︂

E

[︀𝐺𝑡]︀

𝜃

2

(︃

)︃

− 1 − 𝐿 − 𝛽𝐿̃︀2𝛾 E [︀𝑅𝑡]︀ .

2𝛾 2 𝜃

25

EF21 with Bells & Whistles

Oct 6, 2021

Note that our assumption on the stepsize implies that 1 − 𝜃2 ≤ 1 − 𝛾𝜇 and 21𝛾 − 𝐿2 − 𝛽𝐿̃𝜃︀2𝛾 ≥ 0. The last inequality follows from the bound 𝛾2 2𝛽𝜃𝐿̃︀2 + 𝛾𝐿 ≤ 1, which holds because of Lemma 15 and our assumption on the stepsize. Thus,

E

[︁ 𝛿𝑡+1

+

𝛾

]︁ 𝐺𝑡+1

≤

(1

−

𝛾𝜇)E

[︁ 𝛿𝑡

+

𝛾

]︁ 𝐺𝑡

.

𝜃

𝜃

It remains to unroll the recurrence.

Corollary 3. Let assumptions of Theorem 2 hold,

𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

⎧ (︃

√︂ )︃−1 ⎫

⎨

2𝛽

𝜃⎬

𝛾 = min 𝐿 + 𝐿̃︀

,

.

⎩ 𝜃 2𝜇 ⎭

Then, after 𝑇 iterations/communication rounds of EF21 we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 (𝑥⋆)]︀ ≤ 𝜀. It requires

(︃

)︃

𝐿̃︀ (︂ 𝛿0 )︂

𝑇 = #grad = 𝒪

log

(23)

𝛼𝜇

𝜀

√︁

iterations/communications rounds/gradint computations at each node, where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝛿0 = 𝑓 (𝑥0) − 𝑓 𝑖𝑛𝑓 .

Proof. Notice that

⎧ (︃

√︂ )︃−1 ⎫

⎨

2𝛽

𝜃⎬

(𝑖)

{︃ (︂

√ (︂ 2 )︂)︂−1 1 − √1 − 𝛼 }︃

min 𝐿 + 𝐿̃︀

,

𝜇 ≥ min 𝜇 𝐿 + 𝐿̃︀ 2 − 1

,

⎩ 𝜃 2𝜇 ⎭

𝛼

2

⎧ (︃

√ )︃−1 ⎫

(𝑖𝑖)

⎨

2 2𝐿̃︀

𝛼⎬

≥ min 𝜇 𝐿 +

,

⎩ 𝛼 4⎭

⎧ (︃

√ )︃−1 ⎫

(𝑖𝑖𝑖)

⎨ (1 + 2 2)𝐿̃︀

𝛼⎬

≥ min 𝜇

,

⎩ 𝛼 4⎭

{︃

}︃

𝛼𝜇

𝛼

= min

√,

(1 + 2 2)𝐿̃︀ 4

{︂ 𝛼𝜇 𝛼 }︂ ≥ min ,
4𝐿̃︀ 4

𝛼𝜇

=

,

4𝐿̃︀

√ whe√re in (𝑖) we apply Lemma 17, and plug in 𝜃 = 1 − 1 − 𝛼 according to Lemma 17, (𝑖𝑖) follows by 1 − 𝛼 ≤ 1 − 𝛼/2, (𝑖𝑖𝑖) follows by the inequalities 𝛼 ≤ 1, and 𝐿 ≤ 𝐿̃︀.

Let 𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛 , then 𝐺0 = 0. Thus using (22) and the above computations, we arrive at

log (︁ 𝛿0 )︁
𝜀

(𝑖) 1

(︂ 𝛿0 )︂ 4𝐿 (︂ 𝛿0 )︂

̃︀

#grad

=

𝑇 ≤ log (1 − 𝛾𝜇)−1

≤

log 𝛾𝜇

𝜀

≤ log 𝛼𝜇

𝜀

,

where (𝑖) is due to (122).

26

EF21 with Bells & Whistles

Oct 6, 2021

D STOCHASTIC GRADIENTS

In this section, we study the extension of EF21 to the case when stochastic gradients are used instead of full gradients.

Algorithm 2 EF21-SGD

1:

Input: starting point 𝑥0

∈ R𝑑; 𝑔𝑖0

∈ R𝑑 (known by nodes); 𝑔0

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known

by

master);

learning rate 𝛾 > 0

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡 and broadcasts 𝑥𝑡+1 to all nodes

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do 5: Compute a stochastic gradient 𝑔ˆ𝑖(𝑥𝑡+1) = 𝜏1 ∑︀𝜏𝑗=1 ∇𝑓𝜉𝑖𝑡𝑗 (𝑥𝑡+1)

6:

Compress 𝑐𝑡𝑖 = 𝒞(𝑔ˆ𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

7:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

8: end for

9:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖

10: end for

Lemma 2. Let Assumptions 1 and 2 hold. Then for all 𝑡 ≥ 0 and all constants 𝜌,𝜈 > 0 EF21-SGD satisﬁes

E [︀𝐺𝑡+1]︀

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡]︀

+

𝛽ˆ1𝐿̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+𝐴̃︀𝛽ˆ2E [︀𝑓 (𝑥𝑡+1) − 𝑓 inf ]︀ + 𝐶̃︀𝛽ˆ2,

(24)

where 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀,

𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀ + (︁1 + 𝜌1 )︁, 𝐴̃︀ = max𝑖=1,...,𝑛 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) ,

𝑛 (︁

)︁

𝐶̃︀ = 𝑛1 ∑︀ 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) (︀𝑓 inf − 𝑓𝑖inf )︀ + 𝐶𝜏𝑖𝑖 .

𝑖=1

Proof. For all 𝜌,𝜈 > 0 we have

E [︀𝐺𝑡𝑖+1]︀

=

E

[︁ ⃦⃦𝑔𝑖𝑡+1

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

≤

(1

+

𝜌)E

[︁ ⃦ ⃦𝒞

(︀𝑔ˆ𝑖

(𝑥𝑡+1

)

−

𝑔𝑖𝑡

)︀

−

(︀𝑔ˆ𝑖

(𝑥𝑡+1

)

−

𝑔𝑖𝑡

)︀⃦2 ⃦

]︁

+

(︂ 1

+

1 )︂

E

[︁ ⃦⃦𝑔ˆ𝑖(𝑥𝑡+1)

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

𝜌

≤

(1

−

𝛼)

(1

+

𝜌)E

[︁ ⃦⃦𝑔𝑖𝑡

−

𝑔ˆ𝑖(𝑥𝑡+1)⃦⃦2]︁

+

(︂ 1

+

1 )︂

E

[︁ ⃦⃦𝑔ˆ𝑖(𝑥𝑡+1)

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

𝜌

≤

(1

−

𝛼)

(1

+

𝜌)(1

+

𝜈)E

[︁ ⃦⃦𝑔𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2]︁

+2 (1

−

𝛼) (1

+

(︂ 𝜌) 1

+

1

)︂

E

[︁ ⃦ ⃦∇

𝑓

𝑖

(𝑥

𝑡

+1

)

−

𝑔ˆ𝑖(𝑥𝑡+1)⃦⃦2]︁

𝜈

+2 (1

−

𝛼) (1

+

(︂ 𝜌) 1

+

1

)︂

E

[︁ ⃦ ⃦∇

𝑓

𝑖

(𝑥

𝑡

+1

)

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2]︁

𝜈

+

(︂ 1

+

1 )︂

E

[︁ ⃦⃦𝑔ˆ𝑖(𝑥𝑡+1)

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

𝜌

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡𝑖 ]︀

+

𝛽ˆ1𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+𝛽ˆ2

E

[︁ ⃦ ⃦𝑔ˆ𝑖

(𝑥𝑡+1

)

−

∇𝑓𝑖

(𝑥𝑡+1

⃦2 )⃦

]︁

,

27

EF21 with Bells & Whistles

Oct 6, 2021

where we introduced 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀, 𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀ + (︁1 + 𝜌1 )︁. Next we use independence of ∇𝑓𝜉𝑖𝑡𝑗 (𝑥𝑡), variance decomposition, and (9) to estimate the last term:

E [︀𝐺𝑡𝑖+1]︀

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡𝑖 ]︀

+

𝛽ˆ1𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

𝛽ˆ2

𝜏𝑖
∑︁

[︂ ⃦

𝑡+1

⃦2]︂
𝑡+1

+ 𝜏𝑖2 𝑗=1 E ⃦⃦∇𝑓𝜉𝑖𝑡𝑗 (𝑥 ) − ∇𝑓𝑖(𝑥 )⃦⃦

=

(1

−

𝜃ˆ)E

[︀𝐺𝑡𝑖 ]︀

+

𝛽ˆ1𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

𝛽ˆ2

𝜏𝑖
∑︁

(︂

[︂ ⃦

⃦2]︂ [︁

𝑡+1

⃦

)︂ 𝑡+1 ⃦2]︁

+ 𝜏𝑖2 𝑗=1 E ⃦⃦∇𝑓𝜉𝑖𝑡𝑗 (𝑥 )⃦⃦ − E ⃦∇𝑓𝑖(𝑥 )⃦

(≤9) (1 − 𝜃ˆ)E [︀𝐺𝑡𝑖]︀ + 𝛽ˆ1𝐿2𝑖 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁

2𝐴𝑖𝛽ˆ2 [︀ 𝑡+1

inf ]︀

𝛽ˆ2(𝐵𝑖 − 1)

[︁ ⃦

𝑡+1 ⃦2]︁ 𝐶𝑖𝛽ˆ2

+ 𝜏𝑖 E 𝑓𝑖(𝑥 ) − 𝑓𝑖 +

𝜏𝑖

E ⃦∇𝑓𝑖(𝑥 )⃦ + 𝜏𝑖

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡𝑖 ]︀

+

𝛽ˆ1𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

2(𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1))𝛽ˆ2 [︀ 𝑡+1

inf ]︀ 𝐶𝑖𝛽ˆ2

+

𝜏𝑖

E 𝑓𝑖(𝑥

) − 𝑓𝑖

+ 𝜏𝑖

Averaging the obtained inequality for 𝑖 = 1, . . . ,𝑛 we get

E [︀𝐺𝑡+1]︀

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡]︀

+

𝛽ˆ1𝐿̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

1

𝑛
∑︁

(︃ 2(𝐴𝑖

+

𝐿𝑖(𝐵𝑖

−

1))𝛽ˆ2

[︀

𝑡+1

inf ]︀ 𝐶𝑖𝛽ˆ2 )︃

+ 𝑛

𝜏𝑖

E 𝑓𝑖(𝑥

) − 𝑓𝑖

+ 𝜏𝑖

𝑖=1

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡]︀

+

𝛽ˆ1𝐿̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+ 1 ∑𝑛︁ (︃ 2(𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1))𝛽ˆ2 E [︀𝑓𝑖(𝑥𝑡+1) − 𝑓 inf ]︀)︃ 𝑛 𝑖=1 𝜏𝑖

𝛽ˆ2

𝑛
∑︁

(︂ 2(𝐴𝑖

+

𝐿𝑖(𝐵𝑖

−

1))

(︀

inf

inf )︀ 𝐶𝑖 )︂

+ 𝑛

𝜏𝑖

𝑓

− 𝑓𝑖

+ 𝜏𝑖

𝑖=1

≤

(1

−

𝜃ˆ)E

[︀𝐺𝑡]︀

+

𝛽ˆ1𝐿̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+

𝐴̃︀𝛽ˆ2E

[︀𝑓

(𝑥𝑡+1)

−

𝑓

inf

]︀

+

𝐶̃︀𝛽ˆ2

D.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 3. Let Assumptions 1 and 2 hold, and let the stepsize in Algorithm 2 be set as

⎛

√︃ ⎞−1

𝛽ˆ1

0 < 𝛾 ≤ ⎝𝐿 + 𝐿̃︀ 𝜃ˆ ⎠ ,

(25)

where 𝐿̃︀ = √︁ 𝑛1 ∑︀𝑛𝑖=1 𝐿2𝑖 , 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀, and

𝜌,𝜈

>

0 are some positive numbers.

Assume that batchsizes 𝜏1, . . . ,𝜏𝑖

are such that

𝛾𝐴̃︀𝛽^2 ^

<

1, where

2𝜃

𝐴̃︀ = max𝑖=1,...,𝑛 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) and 𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀ + (︁1 + 𝜌1 )︁. Fix 𝑇 ≥ 1 and

let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 with following probabilities:

Prob {︀𝑥ˆ𝑇 = 𝑥𝑡}︀ = 𝑤𝑡 , 𝑊𝑇

(︃ 𝛾𝐴̃︀𝛽ˆ2 )︃𝑡 𝑤𝑡 = 1 − 2𝜃ˆ ,

𝑇
∑︁ 𝑊𝑇 = 𝑤𝑡.
𝑡=0

28

EF21 with Bells & Whistles

Oct 6, 2021

Then

[︁ ⃦

𝑇 ⃦2]︁ 2(𝑓 (𝑥0) − 𝑓 inf )

E [︀𝐺0]︀

𝐶̃︀𝛽ˆ2

E ⃦∇𝑓 (𝑥ˆ )⃦ ≤ (︁ 𝛾𝐴𝛽^ )︁𝑇 + ˆ (︁ 𝛾𝐴𝛽^ )︁𝑇 + 𝜃ˆ ,

(26)

𝛾𝑇

1−

̃︀ 2 2𝜃^

𝜃𝑇

1−

̃︀ 2 2𝜃^

𝑛 (︁

)︁

where 𝐶̃︀ = 𝑛1 ∑︀ 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) (︀𝑓 inf − 𝑓𝑖inf )︀ + 𝐶𝜏𝑖𝑖 .

𝑖=1

Proof. We notice that inequality (20) holds for EF21-SGD as well, i.e., we have

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ .

2

2𝛾 2

2

Summing up the above inequality with a

𝛾 ^

multiple

of

(24),

we

derive

2𝜃

E [︂𝛿𝑡+1 + 2𝛾𝜃ˆ𝐺𝑡+1]︂ ≤ E [︀𝛿𝑡]︀ − 𝛾2 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 21𝛾 − 𝐿2 )︂ E [︀𝑅𝑡]︀ + 𝛾2 E [︀𝐺𝑡]︀

𝛾 +

(1 − 𝜃ˆ)E [︀𝐺𝑡]︀ +

𝛾

𝛽ˆ1𝐿̃︀2E [︀𝑅𝑡]︀

2𝜃ˆ

2𝜃ˆ

𝛾 +

𝐴̃︀𝛽ˆ2E [︀𝛿𝑡+1]︀ +

𝛾

𝐶̃︀𝛽ˆ2

2𝜃ˆ

2𝜃ˆ

≤

𝛾𝐴̃︀𝛽ˆ2 E [︀𝛿𝑡+1]︀ + E [︂𝛿𝑡 +

𝛾

]︂ 𝐺𝑡

−

𝛾

E

[︁⃦∇𝑓 (𝑥𝑡)⃦2]︁

+

𝛾

𝐶 𝛽ˆ

2𝜃ˆ

2𝜃ˆ

⃦ 2

⃦ 2𝜃ˆ ̃︀ 2

(︃ 1

𝐿 𝛾𝛽ˆ1𝐿̃︀2 )︃ [︀ 𝑡]︀

−

−− 2𝛾 2

2𝜃ˆ

E𝑅

(25)
≤

𝛾𝐴̃︀𝛽ˆ2 E [︀𝛿𝑡+1]︀ + E [︂𝛿𝑡 +

𝛾

]︂ 𝐺𝑡

−

𝛾

E

[︁⃦∇𝑓 (𝑥𝑡)⃦2]︁

+

𝛾

𝐶 𝛽ˆ

,

2𝜃ˆ

2𝜃ˆ

⃦ 2

⃦ 2𝜃ˆ ̃︀ 2

where 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀, 𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + (︁ )︁
𝜌) (︀1 + 𝜈1 )︀ + 1 + 𝜌1 , and 𝜌,𝜈 > 0 are some positive numbers. Next, we rearrange the terms

[︁ ⃦

𝑡 ⃦2]︁

(︃

(︃

2

[︂
𝑡

𝛾

]︂
𝑡

𝛾𝐴̃︀𝛽ˆ2 )︃ [︀ 𝑡+1]︀

)︃ 𝛾 [︀ 𝑡+1]︀

E ⃦∇𝑓 (𝑥 )⃦ ≤ 𝛾 E 𝛿 + 2𝜃ˆ𝐺 − 1 − 2𝜃ˆ E 𝛿 − 2𝜃ˆE 𝐺

𝐶̃︀𝛽ˆ2 + 𝜃ˆ

(︃

(︃

2

[︂
𝑡

𝛾

]︂
𝑡

𝛾𝐴̃︀𝛽ˆ2 )︃

[︂
𝑡+1

)︃

𝛾

]︂ [︀ 𝑡+1]︀

≤ 𝛾 E 𝛿 + 2𝜃ˆ𝐺 − 1 − 2𝜃ˆ E 𝛿 + 2𝜃ˆE 𝐺

𝐶̃︀𝛽ˆ2 + 𝜃ˆ ,

sum up the obtained inequalities for 𝑡 = 0,1, . . . ,𝑇 with weights 𝑤𝑡/𝑊𝑇 , and use the deﬁnition of 𝑥ˆ𝑇

E

[︁

⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

=

1 ∑𝑇︁ 𝑤𝑡E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁

𝑊𝐾 𝑡=0

2

𝑇 (︂ ∑︁

[︂

𝛾 ]︂

[︂

𝛾

]︂)︂

≤

𝑤𝑡E 𝛿𝑡 + 𝐺𝑡 − 𝑤𝑡+1E 𝛿𝑡+1 + E [︀𝐺𝑡+1]︀

𝛾𝑊𝑇 𝑡=0 2𝜃ˆ 2𝜃ˆ

𝐶̃︀𝛽ˆ2 + 𝜃ˆ 2𝛿0 E [︀𝐺0]︀ 𝐶̃︀𝛽ˆ2 ≤ 𝛾𝑊𝑇 + 𝜃ˆ𝑊𝑇 + 𝜃ˆ .

29

EF21 with Bells & Whistles

Oct 6, 2021

Finally, we notice

𝑇
∑︁

(︃ 𝛾𝐴̃︀𝛽ˆ2 )︃𝑇

𝑊𝑇 =

𝑤𝑡 ≥ (𝑇 + 1) min 𝑤𝑡 > 𝑇
𝑡=0,1,...,𝑇

1−

2𝜃ˆ

𝑡=0

that ﬁnishes the proof.

Corollary 4. Let assumptions of Theorem 3 hold, 𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

1

𝛾=

,

𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1

⌈︃ {︃ 2𝑇 𝛾 (𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1)) 𝛽ˆ2 8 (𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1)) 𝛽ˆ2 inf 4𝐶𝑖𝛽ˆ2 }︃⌉︃

𝜏𝑖 = max 1,

𝜃ˆ

,

𝜃ˆ𝜀2

𝛿𝑖 , 𝜃ˆ𝜀2

,

⌈︃

{︃ 16𝛿0

8E [︀𝐺0]︀ }︃⌉︃

𝑇 = max 𝛾𝜀2 , 𝜃ˆ𝜀2

,

where 𝛿𝑖inf = 𝑓 inf − 𝑓𝑖inf , 𝛿0 = 𝑓 (𝑥0) − 𝑓 inf . Then, after 𝑇 iterations of EF21-SGD we have

E

[︁ ⃦⃦∇𝑓 (𝑥ˆ𝑇

⃦2]︁ )⃦

≤

𝜀2.

It

requires

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ )︃ 𝑇 = 𝒪 𝛼𝜀2

iterations/communications rounds,

#grad𝑖 = 𝜏𝑖𝑇

(︁

)︁ (︁

)︁

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝐴ˆ𝑖(𝛿0 + 𝛿𝑖inf ) + 𝐶𝑖

=𝒪

𝛼𝜀2 +

𝛼3𝜀4

(𝐿̃︀𝛿0 + E [︀𝐺0]︀)𝐴ˆ𝑖E [︀𝐺0]︀ )︃ +
𝛼2(𝛼𝐿 + 𝐿̃︀)𝜀4

stochastic oracle calls for worker 𝑖, and

1

𝑛
∑︁

#grad = 𝑛 𝜏𝑖𝑇

𝑖=1

(︁

)︁ (︁

)︁

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀

1

𝑛
∑︁

𝐿̃︀𝛿0 + E [︀𝐺0]︀

𝐴ˆ𝑖(𝛿0 + 𝛿𝑖inf ) + 𝐶𝑖

=𝒪

𝛼𝜀2 + 𝑛

𝛼3𝜀4

𝑖=1

1

𝑛
∑︁

(𝐿̃︀𝛿0

+

E

[︀𝐺0]︀)𝐴ˆ𝑖E

[︀𝐺0]︀ )︃

+

𝑛 𝑖=1

𝛼2(𝛼𝐿 + 𝐿̃︀)𝜀4

stochastic oracle calls per worker on average, where 𝐴ˆ𝑖 = 𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1).

Proof. The given choice of 𝜏𝑖 ensures that (︁1 − 𝛾𝐴2̃︀𝜃^𝛽^2 )︁𝑇 = 𝒪(1) and 𝐶̃︀𝛽^2/𝜃^ ≤ 𝜀/2. Next, the choice of 𝑇 ensures that the right-hand side of (26) is smaller than 𝜀. Finally, after simple computation we get the expression for 𝜏𝑖𝑇 .

Corollary 5. Consider the setting described in Example 1. Let assumptions of Theorem 3 hold, 𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

1

𝛾=

,

𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1

⌈︃ {︃ 4𝜎𝑖2𝛽ˆ2 }︃⌉︃ 𝜏𝑖 = max 1, 𝜃ˆ𝜀2 ,

⌈︃

{︃ 16𝛿0

8E [︀𝐺0]︀ }︃⌉︃

𝑇 = max 𝛾𝜀2 , 𝜃ˆ𝜀2

,

30

EF21 with Bells & Whistles

Oct 6, 2021

where

𝛿0

=

𝑓 (𝑥0)

−

𝑓 inf .

Then,

after

𝑇

iterations

of

EF21-SGD

we

have

E

[︁ ⃦ ⃦∇

𝑓

(

𝑥ˆ

𝑇

)⃦⃦2

]︁

≤

𝜀2.

It

requires

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ )︃ 𝑇 = 𝒪 𝛼𝜀2

iterations/communications rounds,

⎛

(︁

)︁ ⎞

𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝜎𝑖2

#grad𝑖 = 𝜏𝑖𝑇 = 𝒪 ⎝ 𝛼𝜀2

+

𝛼3𝜀4

⎠

stochastic oracle calls for worker 𝑖, and

⎛

(︁

)︁ ⎞

1

𝑛
∑︁

𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝜎2

#grad = 𝑛

𝜏𝑖𝑇 = 𝒪 ⎝

𝛼𝜀2

+

𝛼3𝜀4

⎠

𝑖=1

stochastic

oracle

calls

per

worker

on

average,

where

𝜎2

=

1 𝑛

∑︀𝑛
𝑖=1

𝜎𝑖2.

Corollary 6. Consider the setting described in Example 2. Let assumptions of Theorem 3 hold,

𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

1

𝛾=

,

𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1

⌈︃

{︃

2𝑇 𝛾𝐿𝑖𝛽ˆ2

8𝐿𝑖𝛽ˆ2

inf

8𝐿𝑖

∆

inf 𝑖

𝛽ˆ2

}︃⌉︃

𝜏𝑖 = max 1, 𝜃ˆ , 𝜃ˆ𝜀2 𝛿𝑖 , 𝜃ˆ𝜀2

,

⌈︃

{︃ 16𝛿0

8E [︀𝐺0]︀ }︃⌉︃

𝑇 = max 𝛾𝜀2 , 𝜃ˆ𝜀2

,

where 𝛿𝑖inf = 𝑓 inf − 𝑓𝑖inf , 𝛿0 = 𝑓 (𝑥0) − 𝑓 inf , 𝐿𝑖 = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1 𝐿𝑖𝑗 , ∆i𝑖nf = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1(𝑓𝑖inf − 𝑓𝑖i𝑗nf ).

Then,

after

𝑇

iterations

of

EF21-SGD

we

have

E

[︁

⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

≤

𝜀2.

It

requires

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ )︃ 𝑇 = 𝒪 𝛼𝜀2

iterations/communications rounds,

#grad𝑖 = 𝜏𝑖𝑇

(︁

)︁

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ 𝐿̃︀𝛿0 + E [︀𝐺0]︀ (︀𝐿𝑖(𝛿0 + 𝛿𝑖inf ) + 𝐿𝑖∆i𝑖nf )︀

=𝒪

𝛼𝜀2 +

𝛼3𝜀4

(𝐿̃︀𝛿0 + E [︀𝐺0]︀)𝐿𝑖E [︀𝐺0]︀ )︃ +
𝛼2(𝛼𝐿 + 𝐿̃︀)𝜀4

stochastic oracle calls for worker 𝑖, and

1

𝑛
∑︁

#grad = 𝑛 𝜏𝑖𝑇

𝑖=1

(︁

)︁

(︃ 𝐿̃︀𝛿0 + E [︀𝐺0]︀

1

𝑛
∑︁

𝐿̃︀𝛿0 + E [︀𝐺0]︀

(︀𝐿𝑖(𝛿0 + 𝛿𝑖inf ) + 𝐿𝑖∆i𝑖nf )︀

=𝒪

𝛼𝜀2 + 𝑛

𝛼3𝜀4

𝑖=1

1

𝑛
∑︁

(𝐿̃︀𝛿0

+

E

[︀𝐺0]︀)𝐿𝑖E

[︀𝐺0]︀ )︃

+

𝑛 𝑖=1

𝛼2(𝛼𝐿 + 𝐿̃︀)𝜀4

stochastic oracle calls per worker on average.

31

EF21 with Bells & Whistles

Oct 6, 2021

D.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION

Theorem 4. Let Assumptions 1, 2, and 4 hold, and let the stepsize in Algorithm 2 be set as

⎧ ⎛

√︃ ⎞−1 ⎫

⎪ ⎨

2𝛽ˆ1

𝜃ˆ

⎪ ⎬

0 < 𝛾 ≤ min ⎪⎝𝐿 + 𝐿̃︀ 𝜃ˆ ⎠ , 2𝜇 ⎪ , (27)

⎩

⎭

where 𝐿̃︀ = √︁ 𝑛1 ∑︀𝑛𝑖=1 𝐿2𝑖 , 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀, and

𝜌,𝜈

>

0 are some positive numbers.

Assume that batchsizes 𝜏1, . . . ,𝜏𝑖 are such that

2𝐴̃︀𝛽^2 ^

≤

𝜇2 ,

𝜃

where 𝐴̃︀ = max𝑖=1,...,𝑛 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) and 𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀ + (︁1 + 𝜌1 )︁. Then for

all 𝑇 ≥ 1

[︂ E 𝛿𝑇

+

𝛾 𝐺𝑇 ]︂

≤

(︁ 1

−

𝛾𝜇 )︁𝑇

[︂ E 𝛿0

+

𝛾 𝐺0]︂

+

4 𝐶̃︀𝛽ˆ2,

(28)

𝜃ˆ

2

𝜃ˆ

𝜇𝜃ˆ

𝑛 (︁

)︁

where 𝐶̃︀ = 𝑛1 ∑︀ 2(𝐴𝑖+𝐿𝜏𝑖𝑖(𝐵𝑖−1)) (︀𝑓 inf − 𝑓𝑖inf )︀ + 𝐶𝜏𝑖𝑖 .

𝑖=1

Proof. We notice that inequality (20) holds for EF21-SGD as well, i.e., we have

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀

2

2𝛾 2

2

PŁ
≤

(1 − 𝛾𝜇)E [︀𝛿𝑡]︀ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ .

2𝛾 2

2

Summing

up

the

above

inequality

with

a

𝛾 ^

multiple

of

(24),

we

derive

𝜃

E

[︂ 𝛿𝑡+1

+

𝛾

]︂ 𝐺𝑡+1

≤

(1 − 𝛾𝜇)E [︀𝛿𝑡]︀ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀

𝜃ˆ

2𝛾 2

2

𝛾 +

(1

−

𝜃ˆ)E

[︀𝐺𝑡]︀

+

𝛾

𝛽ˆ1𝐿̃︀2E

[︀𝑅𝑡]︀

𝜃ˆ

𝜃ˆ

𝛾 +

𝐴̃︀𝛽ˆ2E

[︀𝛿𝑡+1]︀

+

𝛾

𝐶̃︀𝛽ˆ2

𝜃ˆ

𝜃ˆ

≤

𝛾𝐴̃︀𝛽ˆ2

E

[︀𝛿𝑡+1]︀

+

(1

−

𝛾𝜇)E

[︀𝛿𝑡]︀

+

(︃ 1

−

𝜃ˆ)︃

E

[︂ 𝛾

]︂ 𝐺𝑡

+

𝛾

𝐶 𝛽ˆ

𝜃ˆ

2

𝜃ˆ

𝜃ˆ ̃︀ 2

(︃ 1

𝐿 𝛾𝛽ˆ1𝐿̃︀2 )︃ [︀ 𝑡]︀

− −− 2𝛾 2

𝜃ˆ

E𝑅

(27)
≤

𝛾𝐴̃︀𝛽ˆ2 E [︀𝛿𝑡+1]︀

+

(1

−

[︂ 𝛾𝜇)E 𝛿𝑡

+

𝛾 𝐺𝑡]︂

+

𝛾 𝐶𝛽ˆ

,

𝜃ˆ

𝜃ˆ

𝜃ˆ ̃︀ 2

where 𝜃ˆ d=ef 1 − (1 − 𝛼) (1 + 𝜌)(1 + 𝜈), 𝛽ˆ1 d=ef 2 (1 − 𝛼) (1 + 𝜌) (︀1 + 𝜈1 )︀, 𝛽ˆ2 d=ef 2 (1 − 𝛼) (1 + (︁ )︁
𝜌) (︀1 + 𝜈1 )︀ + 1 + 𝜌1 , and 𝜌,𝜈 > 0 are some positive numbers. Next, we rearrange the terms

(︃

𝛾𝐴̃︀𝛽ˆ2 )︃

[︂
𝑡+1

𝛾

]︂
𝑡+1

[︃(︃

𝛾𝐴̃︀𝛽ˆ2 )︃ 𝑡+1

]︃ 𝛾 𝑡+1

1 − 𝜃ˆ E 𝛿 + 𝜃ˆ𝐺

≤ E 1 − 𝜃ˆ 𝛿 + 𝜃ˆ𝐺

≤

(1

−

𝛾𝜇)E

[︂ 𝛿𝑡

+

𝛾

]︂ 𝐺𝑡

+

𝛾

𝐶̃︀𝛽ˆ2

𝜃ˆ

𝜃ˆ

and divide both sides of the inequality by (︁1 − 𝛾𝐴̃𝜃^︀𝛽^2 )︁:

E

[︂ 𝛿𝑡+1

+

𝛾

]︂ 𝐺𝑡+1

≤

1 − 𝛾𝜇

E

[︂ 𝛿𝑡

+

𝛾

]︂ 𝐺𝑡

+

𝛾

𝐶̃︀𝛽ˆ2

𝜃ˆ 1 − 𝛾𝐴̃𝜃^︀𝛽^2 𝜃ˆ 𝜃ˆ (︁1 − 𝛾𝐴̃𝜃^︀𝛽^2 )︁

(120)
≤

(1

−

(︃ 𝛾𝜇) 1

+

2𝛾𝐴̃︀𝛽ˆ2 )︃ E [︂𝛿𝑡

+

𝛾 𝐺𝑡]︂

+

(︃ 1

+

2𝛾𝐴̃︀𝛽ˆ2 )︃

𝛾 𝐶𝛽ˆ

.

𝜃ˆ

𝜃ˆ

𝜃ˆ 𝜃ˆ ̃︀ 2

32

EF21 with Bells & Whistles

Oct 6, 2021

Since 2𝐴̃𝜃^︀𝛽^2 ≤ 𝜇2 and 𝛾 ≤ 𝜇2 , we have

E

[︂ 𝛿𝑡+1

+

𝛾

]︂ 𝐺𝑡+1

≤

𝜃ˆ

(121)
≤

Unrolling the recurrence, we get

(1

−

𝛾𝜇)

(︁ 1

+

𝛾𝜇 )︁

E

[︂ 𝛿𝑡

+

𝛾

]︂ 𝐺𝑡

+

2𝛾

𝐶̃︀𝛽ˆ2

2

𝜃ˆ

𝜃ˆ

(︁ 1

−

𝛾𝜇 )︁

E

[︂ 𝛿𝑡

+

𝛾

]︂ 𝐺𝑡

+

2𝛾

𝐶̃︀𝛽ˆ2.

2

𝜃ˆ

𝜃ˆ

[︂ E 𝛿𝑇

+

𝛾 𝐺𝑇 ]︂

≤

(︁ 1

−

𝛾𝜇 )︁𝑇

E

[︂ 𝛿0

+

𝛾

]︂ 𝐺0

+

2𝛾

𝐶 𝛽ˆ

𝑇 −1
∑︁ (︁

𝛾𝜇 )︁𝑡

1−

𝜃ˆ 2 𝜃ˆ 𝜃ˆ ̃︀ 2 𝑡=0 2

≤

(︁ 1

−

𝛾𝜇 )︁𝑇

E

[︂ 𝛿0

+

𝛾

]︂ 𝐺0

+

2𝛾

𝐶 𝛽ˆ

∞
∑︁ (︁

𝛾𝜇 )︁𝑡

1−

2 𝜃ˆ 𝜃ˆ ̃︀ 2 𝑡=0 2

=

(︁ 1

−

𝛾𝜇 )︁𝑇

E

[︂ 𝛿0

+

𝛾

]︂ 𝐺0

+

4 𝐶̃︀𝛽ˆ2

2

𝜃ˆ

𝜇𝜃ˆ

that ﬁnishes the proof.

Corollary 7. Let assumptions of Theorem 4 hold, 𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

⎧

⎫

⎨1

𝜃ˆ ⎬

𝛾 = min

,

,

⎩ 𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1 2𝜇 ⎭

⌈︃ {︃ 8 (𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1)) 𝛽ˆ2 64 (𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1)) 𝛽ˆ2 inf 32𝐶𝑖𝛽ˆ2 }︃⌉︃

𝜏𝑖 = max 1,

𝜇𝜃ˆ

,

𝜃ˆ𝜀𝜇

𝛿𝑖 , 𝜃ˆ𝜀𝜇

,

⌈︂ 2 (︂ 2𝛿0 [︂ 2𝛾𝐺0 ]︂)︂⌉︂

𝑇=

ln 𝛾𝜇

𝜀 + E 𝜃ˆ𝜀

,

where 𝛿𝑖inf = 𝑓 inf − 𝑓𝑖inf , 𝛿0 = 𝑓 (𝑥0) − 𝑓 inf . Then, after 𝑇 iterations of EF21-SGD we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 inf ]︀ ≤ 𝜀. It requires

(︃

)︃

𝐿̃︀ (︂ 𝛿0 [︂ 2𝐺0 ]︂)︂

𝑇 =𝒪

ln + E

𝜇𝛼 𝜀

𝐿̃︀𝜀

iterations/communications rounds,

#grad𝑖 = 𝜏𝑖𝑇

⎛⎛ 𝐿

𝐿̃︀

(︁ 𝐴ˆ𝑖

(𝜀

+

𝛿𝑖inf

)

+

𝐶𝑖

)︁

⎞

(︂ 𝛿0

⎞ [︂ 2𝐺0 ]︂)︂

̃︀

= 𝒪 ⎝⎝ + 𝜇𝛼

𝜇2𝛼3𝜀

⎠ ln 𝜀 + E 𝐿𝜀 ⎠

̃︀

stochastic oracle calls for worker 𝑖, and

1

𝑛
∑︁

#grad = 𝑛 𝜏𝑖𝑇

𝑖=1

⎛⎛ 𝐿

1

𝑛

𝐿̃︀

(︁ 𝐴ˆ𝑖

(𝜀

+

𝛿𝑖inf

)

+

𝐶𝑖

)︁

⎞

(︂ 𝛿0

⎞ [︂ 2𝐺0 ]︂)︂

̃︀

∑︁

= 𝒪 ⎝⎝ + 𝜇𝛼 𝑛

𝜇2𝛼3𝜀

⎠ ln 𝜀 + E 𝐿𝜀 ⎠

𝑖=1

̃︀

stochastic oracle calls per worker on average, where 𝐴ˆ𝑖 = 𝐴𝑖 + 𝐿𝑖(𝐵𝑖 − 1).

Proof.

The given choice of 𝜏𝑖 ensures that

2𝐴̃︀𝛽^2 ^

≤

𝜇 2

and 4𝐶̃︀𝛽^2/𝜇𝜃^ ≤

𝜀/2.

Next, the choice of 𝑇

𝜃

ensures that the right-hand side of (28) is smaller than 𝜀. Finally, after simple computation we get the

expression for 𝜏𝑖𝑇 .

33

EF21 with Bells & Whistles

Oct 6, 2021

Corollary 8. Consider the setting described in Example 1. Let assumptions of Theorem 4 hold, 𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

⎧

⎫

⎨1

𝜃ˆ ⎬

𝛾 = min

,

,

⎩ 𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1 2𝜇 ⎭

⌈︃ {︃ 32𝐶𝑖𝛽ˆ2 }︃⌉︃

𝜏𝑖 = max 1, 𝜃ˆ𝜀𝜇

,

⌈︂ 2 (︂ 2𝛿0 [︂ 2𝛾𝐺0 ]︂)︂⌉︂

𝑇 = ln 𝛾𝜇

𝜀 +E

𝜃ˆ𝜀

,

where 𝛿0 = 𝑓 (𝑥0) − 𝑓 inf . Then, after 𝑇 iterations of EF21-SGD we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 inf ]︀ ≤ 𝜀. It

requires

(︃

)︃

𝐿̃︀ (︂ 𝛿0 [︂ 2𝐺0 ]︂)︂

𝑇 =𝒪

ln + E

𝜇𝛼 𝜀

𝐿̃︀𝜀

iterations/communications rounds,

#grad𝑖 = 𝜏𝑖𝑇

(︃(︃

)︃

)︃

𝐿̃︀ 𝐿̃︀𝜎𝑖2

(︂ 𝛿0 [︂ 2𝐺0 ]︂)︂

= 𝒪 𝜇𝛼 + 𝜇2𝛼3𝜀 ln 𝜀 + E 𝐿𝜀

̃︀

stochastic oracle calls for worker 𝑖, and

1

𝑛
∑︁

#grad = 𝑛 𝜏𝑖𝑇

𝑖=1

(︃(︃

)︃

)︃

𝐿̃︀ 𝐿̃︀𝜎2

(︂ 𝛿0 [︂ 2𝐺0 ]︂)︂

= 𝒪 𝜇𝛼 + 𝜇2𝛼3𝜀 ln 𝜀 + E 𝐿𝜀

̃︀

stochastic

oracle

calls

per

worker

on

average,

where

𝜎2

=

1 𝑛

∑︀𝑛
𝑖=1

𝜎𝑖2.

Corollary 9. Consider the setting described in Example 2. Let assumptions of Theorem 4 hold,

𝜌 = 𝛼/2, 𝜈 = 𝛼/4,

⎧

⎫

⎨1

𝜃ˆ ⎬

𝛾 = min

,

,

⎩ 𝐿 + 𝐿̃︀√︁ 𝛽^𝜃^1 2𝜇 ⎭

⌈︃

{︃

8𝐿𝑖𝛽ˆ2

64𝐿𝑖𝛽ˆ2

inf

64

𝐿𝑖

∆

inf 𝑖

𝛽ˆ2

}︃⌉︃

𝜏𝑖 = max 1, 𝜇𝜃ˆ , 𝜃ˆ𝜀𝜇 𝛿𝑖 , 𝜃ˆ𝜀𝜇

,

⌈︂ 2 (︂ 2𝛿0 [︂ 2𝛾𝐺0 ]︂)︂⌉︂

𝑇=

ln 𝛾𝜇

𝜀 + E 𝜃ˆ𝜀

,

where 𝛿𝑖inf = 𝑓 inf − 𝑓𝑖inf , 𝛿0 = 𝑓 (𝑥0) − 𝑓 inf , 𝐿𝑖 = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1 𝐿𝑖𝑗 , ∆i𝑖nf = 𝑚1𝑖 ∑︀𝑚 𝑗=𝑖1(𝑓𝑖inf − 𝑓𝑖i𝑗nf ). Then, after 𝑇 iterations of EF21-SGD we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 inf ]︀ ≤ 𝜀. It requires

(︃

)︃

𝐿̃︀ (︂ 𝛿0 [︂ 2𝐺0 ]︂)︂

𝑇 =𝒪

ln + E

𝜇𝛼 𝜀

𝐿̃︀𝜀

iterations/communications rounds,

#grad𝑖 = 𝜏𝑖𝑇

(︃(︃ 𝐿̃︀

𝐿̃︀𝐿𝑖 (︀𝜀 + 𝛿𝑖inf + ∆i𝑖nf )︀ )︃ (︂ 𝛿0

)︃ [︂ 2𝐺0 ]︂)︂

=𝒪

+

𝜇𝛼

𝜇2𝛼3𝜀

ln 𝜀 + E 𝐿𝜀

̃︀

stochastic oracle calls for worker 𝑖, and

1

𝑛
∑︁

#grad = 𝑛 𝜏𝑖𝑇

𝑖=1

(︃(︃ 𝐿̃︀

1

𝑛
∑︁

𝐿̃︀𝐿𝑖

(︀𝜀

+

𝛿𝑖inf

+

∆i𝑖nf )︀ )︃

(︂ 𝛿0

)︃ [︂ 2𝐺0 ]︂)︂

=𝒪

+

𝜇𝛼 𝑛

𝜇2𝛼3𝜀

ln 𝜀 + E 𝐿𝜀

𝑖=1

̃︀

stochastic oracle calls per worker on average.

34

EF21 with Bells & Whistles

Oct 6, 2021

E VARIANCE REDUCTION

In this part, we modify the EF21 framework to better handle ﬁnite-sum problems with smooth summands. Unlike the online/streaming case where SGD has the optimal complexity (without additional assumption on the smoothness of stochastic trajectories) (Arjevani et al., 2019), in the ﬁnite sum regime, it is well-known that one can hope for convergence to the exact stationary point rather than its neighborhood. To achieve this, variance reduction techniques are instrumental. One approach is to apply a PAGE-estimator (Li et al., 2021) instead of a random minibatch applied in SGD. Note that PAGE has optimal com√plexity for nonconvex problems of the form (3). With Corollary 10, we illustrate that this 𝑂 (𝑚 + 𝑚/𝜀2) complexity is recovered for our Algorithm 3 when no compression is applied and 𝑛 = 1.

We show how to combine PAGE estimator with EF21 mechanism and call the new method EF21-
PAGE. At each step of EF21-PAGE, clients (nodes) either compute (with probability 𝑝) full gradients or use a recursive estimator 𝑣𝑖𝑡 + 𝜏1𝑖 ∑︀𝑗∈𝐼𝑖𝑡 (︀∇𝑓𝑖𝑗(𝑥𝑡+1) − ∇𝑓𝑖𝑗(𝑥𝑡))︀ (with probability 1 − 𝑝). Then each client applies a Markov compressor/EF21-estimator and sends the result to the master node.
Typically the number of data points 𝑚 is large, and 𝑝 < 1/𝑚. As a result, computation of full gradients
rarely happens during optimization procedure, on average, only once in every 𝑚 iterations.

Notice that unlike VR-MARINA (Gorbunov et al., 2021), which is a state-of-the-art distributed optimization method designed speciﬁcally for unbiased compressors and which also uses PAGEestimator, EF21-PAGE does not require the communication of full (not compressed) vectors at all. This is an important property of the algorithm since, in some distributed networks, and especially when 𝑑 is very large, as is the case in modern over-parameterized deep learning, full vector communication is prohibitive. However, unlike the rate of VR-MARINA, the rate of EF21-PAGE does not improve with the growth of 𝑛. This is not a ﬂaw of our method, but rather an inevitable drawback of the distributed methods that use biased compressions only.

Notations for this section. In this section, we use the following additional notations 𝑃𝑖𝑡 d=ef

‖∇𝑓𝑖(𝑥𝑡) − 𝑣𝑖𝑡‖2, 𝑃 𝑡

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝑃𝑖𝑡,

𝑉𝑖𝑡

d=ef

‖𝑣𝑖𝑡 − 𝑔𝑖𝑡‖2, 𝑉 𝑡

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝑉𝑖𝑡,

where

𝑣𝑖𝑡

is a PAGE

estimator.

Recall

that

𝐺𝑡

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝐺𝑡𝑖 ,

𝐺𝑡𝑖

d=ef

‖∇𝑓𝑖(𝑥𝑡)

−

𝑔𝑖𝑡‖2.

The main idea of the analysis in this section is to split the error in two parts 𝐺𝑡𝑖 ≤ 2𝑃𝑖𝑡 + 2𝑉𝑖𝑡, and bound them separetely.

Algorithm 3 EF21-PAGE

1:

Input: starting point 𝑥0

∈ R𝑑; 𝑔𝑖0, 𝑣𝑖0

∈ R𝑑 for 𝑖 = 1, . . . , 𝑛 (known by nodes); 𝑔0

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known by master); learning rate 𝛾 > 0; probabilities 𝑝𝑖 ∈ (0,1]; batch-sizes 1 ≤ 𝜏𝑖 ≤ 𝑚𝑖

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

5:

Sample 𝑏𝑡𝑖 ∼ Be(𝑝𝑖)

6:

If 𝑏𝑡𝑖 = 0, sample a minibatch of data samples 𝐼𝑖𝑡 with |𝐼𝑖𝑡| = 𝜏𝑖

⎧⎨∇𝑓𝑖(𝑥𝑡+1)

if 𝑏𝑡𝑖 = 1,

7:

𝑣𝑖𝑡+1 =

𝑣𝑖𝑡

+

1 𝜏

∑︀ (︀∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡))︀

if 𝑏𝑡𝑖 = 0

⎩

𝑖 𝑗∈𝐼𝑡

𝑖

8:

Compress 𝑐𝑡𝑖 = 𝒞(𝑣𝑖𝑡+1 − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

9:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

10: end for

11:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖

12: end for

13: Output: 𝑥ˆ𝑇 chosen uniformly from {𝑥𝑡}𝑡∈[𝑇 ]

Lemma 3. Let Assumption 3 hold, and let 𝑣𝑖𝑡+1 be a PAGE estimator, i. e. for 𝑏𝑡𝑖 ∼ Be(𝑝𝑖)

⎧⎨∇𝑓𝑖(𝑥𝑡+1)

if 𝑏𝑡𝑖 = 1,

𝑣𝑖𝑡+1 =

𝑣𝑖𝑡

+

1 𝜏

∑︀ (︀∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡))︀

if

𝑏𝑡𝑖 = 0,

(29)

⎩

𝑖 𝑗∈𝐼𝑡

𝑖

35

EF21 with Bells & Whistles

Oct 6, 2021

for all 𝑖 = 1, . . . , 𝑛, 𝑡 ≥ 0. Then

E

[︀𝑃

𝑡+1]︀

≤

(1

−

𝑝min)E

[︀𝑃

𝑡]︀

+

ℒ̃︀2E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

,

(30)

where ℒ̃︀ = 𝑛1 ∑︀𝑛𝑖=1 (1−𝜏𝑝𝑖𝑖)ℒ2𝑖 , 𝑝min = min𝑖=1,...,𝑛 𝑝𝑖.

Proof.

E

[︀𝑃

𝑡 𝑖

+1

]︀

=

E

[︁ ⃦⃦𝑣𝑖𝑡+1

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

⎡ ⃦

⃦2⎤

=

⃦ (1 − 𝑝𝑖)E ⎢⃦𝑣𝑡 +

1

⃦ ∑︁ (∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡)) − ∇𝑓𝑖(𝑥𝑡+1)⃦

⎥

⃦ ⎣

𝑖

⃦

𝜏𝑖

⃦ ⎦
⃦

⃦

𝑗∈𝐼𝑖𝑡

⃦

[︂ ⃦

⃦2]︂

= (1 − 𝑝𝑖)E ⃦⃦𝑣𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡) + ∆̃︀ 𝑡𝑖 − ∇𝑓𝑖(𝑥𝑡+1) + ∇𝑓𝑖(𝑥𝑡)⃦⃦

[︂ ⃦

⃦2]︂

= (1 − 𝑝𝑖)E ⃦⃦𝑣𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡) + ∆̃︀ 𝑡𝑖 − ∆𝑡𝑖⃦⃦

(=𝑖) (1 − 𝑝𝑖)E [︁⃦⃦𝑣𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡)⃦⃦2]︁ + (1 − 𝑝𝑖)E [︂⃦⃦⃦∆̃︀ 𝑡𝑖 − ∆𝑡𝑖⃦⃦⃦2]︂

(𝑖𝑖)

[︀ 𝑡]︀

(1 − 𝑝𝑖)ℒ2𝑖

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

≤ (1 − 𝑝𝑖)E 𝑃𝑖 + 𝜏𝑖 E ⃦𝑥 − 𝑥 ⃦

[︀ 𝑡]︀

(1 − 𝑝𝑖)ℒ2𝑖

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

≤ (1 − 𝑝min)E 𝑃𝑖 + 𝜏𝑖 E ⃦𝑥 − 𝑥 ⃦ ,

(31)

[︁

]︁

where equality (𝑖) holds because E ∆̃︀ 𝑡𝑖 − ∆𝑡𝑖 | 𝑥𝑡, 𝑥𝑡+1, 𝑣𝑖𝑡 = 0, and (𝑖𝑖) holds by Assumption 3.

It remains to average the above inequality over 𝑖 = 1, . . . , 𝑛.

Lemma 4. Let Assumptions 1 and 3 hold, let 𝑣𝑖𝑡+1 be a PAGE estimator, i. e. for 𝑏𝑡𝑖 ∼ Be(𝑝𝑖) and for all 𝑖 = 1, . . . , 𝑛, 𝑡 ≥ 0

⎧⎨∇𝑓𝑖(𝑥𝑡+1)

if 𝑏𝑡𝑖 = 1,

𝑣𝑖𝑡+1 =

𝑣𝑖𝑡

+

1 𝜏

∑︀ (︀∇𝑓𝑖𝑗 (𝑥𝑡+1) − ∇𝑓𝑖𝑗 (𝑥𝑡))︀

if

𝑏𝑡𝑖 = 0,

(32)

⎩

𝑖 𝑗∈𝐼𝑡

𝑖

and let 𝑔𝑖𝑡+1 be an EF21 estimator, i. e.

𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝒞(𝑣𝑖𝑡+1 − 𝑔𝑖𝑡), 𝑔𝑖0 = 𝒞 (︀𝑣𝑖0)︀

(33)

for all 𝑖 = 1, . . . , 𝑛, 𝑡 ≥ 0. Then

E

[︀𝑉

𝑡+1]︀

≤

(1

−

𝜃)E

[︀𝑉

𝑡]︀

+

2𝛽𝑝maxE

[︀𝑃

𝑡]︀

+

𝛽

(︁ 2𝐿̃︀2

+

)︁ ℒ̃︀2

E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

,

(34)

where ℒ̃︀ = 𝑛1 ∑︀𝑛𝑖=1 (1−𝜏𝑝𝑖𝑖)ℒ2𝑖 , 𝑝max = max𝑖=1,...,𝑛 𝑝𝑖, 𝜃 = 1−(1−𝛼)(1+𝑠), 𝛽 = (1−𝛼) (︀1 + 𝑠−1)︀ for any 𝑠 > 0.

Proof. Following the steps in proof of Lemma 1, but with ∇𝑓𝑖(𝑥𝑡+1) and ∇𝑓𝑖(𝑥𝑡) being substituted by their estimators 𝑣𝑖𝑡+1 and 𝑣𝑖𝑡, we end up with an analogue of (15)

E [︁⃦⃦𝑔𝑖𝑡+1 − 𝑣𝑖𝑡+1⃦⃦2]︁

≤

(1

−

𝜃)E

[︁ ⃦⃦𝑔𝑖𝑡

−

𝑣𝑖𝑡⃦⃦2]︁

+

𝛽E

[︁ ⃦⃦𝑣𝑖𝑡+1

−

𝑣𝑖𝑡⃦⃦2]︁

,

(35)

36

EF21 with Bells & Whistles

Oct 6, 2021

where 𝜃 = 1 − (1 − 𝛼)(1 + 𝑠), 𝛽 = (1 − 𝛼) (︀1 + 𝑠−1)︀ for any 𝑠 > 0. Then

E

[︀𝑉

𝑡 𝑖

]︀

=

E [︁⃦⃦𝑔𝑖𝑡+1 − 𝑣𝑖𝑡+1⃦⃦2]︁

(≤35) (1 − 𝜃)E [︁⃦⃦𝑔𝑖𝑡 − 𝑣𝑖𝑡⃦⃦2]︁ + 𝛽E [︁⃦⃦𝑣𝑖𝑡+1 − 𝑣𝑖𝑡⃦⃦2]︁

=

(1

−

𝜃)E

[︁ ⃦⃦𝑔𝑖𝑡

−

𝑣𝑖𝑡⃦⃦2]︁

+

𝛽E

[︁ E

[︁ ⃦⃦𝑣𝑖𝑡+1

−

𝑣𝑖𝑡⃦⃦2

|

𝑣𝑖𝑡,

]︁]︁ 𝑥𝑡,𝑥𝑡+1

(𝑖)
=

(1

−

𝜃)E

[︀𝑉𝑖𝑡]︀

+

𝛽𝑝𝑖E

[︁ ⃦⃦𝑣𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁

⎡ ⃦

⃦2⎤

+𝛽(1 − 𝑝𝑖)E ⎢⃦⃦⃦ 1 ∑︁ (︀∇𝑓𝑖𝑗(𝑥𝑡+1) − ∇𝑓𝑖𝑗(𝑥𝑡))︀⃦⃦⃦ ⎥

⎣⃦ 𝜏𝑖 ⃦ 𝑗∈𝐼𝑖𝑡

⎦ ⃦ ⃦

= (1 − 𝜃)E [︀𝑉𝑖𝑡]︀ + 𝛽𝑝𝑖E [︁⃦⃦𝑣𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡+1)⃦⃦2]︁ + 𝛽(1 − 𝑝𝑖)E [︂⃦⃦⃦∆̃︀ 𝑡𝑖⃦⃦⃦2]︂

(𝑖𝑖)
=

(1

−

𝜃)E

[︀𝑉𝑖𝑡]︀

+

2𝛽𝑝𝑖E

[︁ ⃦⃦𝑣𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2]︁

+2𝛽𝑝𝑖E [︁⃦⃦∇𝑓𝑖(𝑥𝑡+1) − ∇𝑓𝑖(𝑥𝑡)⃦⃦2]︁ + 𝛽(1 − 𝑝𝑖)E [︂⃦⃦⃦∆̃︀ 𝑡𝑖⃦⃦⃦2]︂

= (1 − 𝜃)E [︀𝑉𝑖𝑡]︀ + 2𝛽𝑝𝑖E [︀𝑃𝑖𝑡]︀ + 2𝛽𝑝𝑖E [︁⃦⃦∆𝑡𝑖⃦⃦2]︁ + 𝛽(1 − 𝑝𝑖)E [︂⃦⃦⃦∆̃︀ 𝑡𝑖⃦⃦⃦2]︂

(𝑖𝑖𝑖)
=

(1 − 𝜃)E [︀𝑉𝑖𝑡]︀ + 2𝛽𝑝𝑖E [︀𝑃𝑖𝑡]︀ + 𝛽(2𝑝𝑖 + 1 − 𝑝𝑖)E [︁⃦⃦∆𝑡𝑖⃦⃦2]︁

[︂ ⃦

⃦2]︂

+𝛽(1 − 𝑝𝑖)E ⃦⃦∆̃︀ 𝑡𝑖 − ∆𝑡𝑖⃦⃦

(≤𝑖𝑣) (1 − 𝜃)E [︀𝑉𝑖𝑡]︀ + 2𝛽𝑝𝑖E [︀𝑃𝑖𝑡]︀ + 𝛽(1 + 𝑝𝑖)𝐿2𝑖 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁

+𝛽 (1 − 𝑝𝑖)ℒ2𝑖 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁ 𝜏𝑖

[︀ 𝑡]︀

[︀ 𝑡]︀

(︂
2

(1 − 𝑝𝑖)ℒ2𝑖 )︂

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

≤ (1 − 𝜃)E 𝑉𝑖 + 2𝛽𝑝maxE 𝑃𝑖 + 𝛽 2𝐿𝑖 + 𝜏𝑖

E ⃦𝑥 − 𝑥 ⃦ ,

where in (𝑖) we use the deﬁnition of PAGE estimator (32), (𝑖𝑖) applies (119) with 𝑠 = 1, (𝑖𝑖𝑖) is due to bias-variance decomposition (123), (𝑖𝑣) makes use of Assumptions 1 and 3, and the last step is due to 𝑝𝑖 ≤ 1, 𝑝𝑖 ≤ 𝑝max .

It remains to average the above inequality over 𝑖 = 1, . . . , 𝑛.

E.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 5. Let Assumptions 1 and 3 hold, and let the stepsize in Algorithm 3 be set as

(︃ √︃

)︃−1

0<𝛾 ≤ 𝐿+

4𝛽 𝐿2

+

(︂ 3𝛽 2

𝑝max

+

1 )︂ ℒ2

.

(36)

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

E [︁⃦⃦∇𝑓 (𝑥ˆ𝑇 )⃦⃦2]︁ ≤ 2Ψ0 , (37) 𝛾𝑇

where Ψ𝑡 d=ef 𝑓 (𝑥𝑡) − 𝑓 inf + 𝛾𝜃 𝑉 𝑡 + 𝑝m𝛾in (︁1 + 2𝛽𝑝𝜃min )︁ 𝑃 𝑡, 𝑝max = max𝑖=1,...,𝑛 𝑝𝑖, 𝑝min =

√︁

min𝑖=1,...,𝑛 𝑝𝑖, 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1 − (1 − 𝛼)(1 + 𝑠),

𝛽

=

(1 − 𝛼) (︀1 + 𝑠−1)︀

for

any

𝑠 > 0.

37

EF21 with Bells & Whistles

Oct 6, 2021

Proof. We apply Lemma 16 and split the error ‖𝑔𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡)‖2 in two parts

𝑓 (𝑥𝑡+1) ≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 ⃦⃦𝑔𝑡 − ∇𝑓 (𝑥𝑡)⃦⃦2

2

2𝛾 2

2

≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡

2

2𝛾 2

+𝛾⃦⃦𝑔𝑡

−

𝑣𝑡⃦⃦2

+

𝛾E

[︁ ⃦⃦𝑣𝑡

−

∇𝑓

(𝑥𝑡)⃦⃦2]︁

≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡

2

2𝛾 2

1 ∑𝑛︁ ⃦ 𝑡 𝑡⃦2 1 ∑𝑛︁ ⃦ 𝑡

𝑡 ⃦2

+𝛾 𝑛

⃦𝑔𝑖 − 𝑣𝑖 ⃦

+𝛾 𝑛

⃦𝑣𝑖 − ∇𝑓𝑖(𝑥 )⃦

𝑖=1

𝑖=1

= 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾𝑉 𝑡 + 𝛾𝑃 𝑡, (38)

2

2𝛾 2

where we used notation 𝑅𝑡 = ‖𝛾𝑔𝑡‖2 = ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2, and applied (118) and (119).
Subtracting 𝑓 inf from both sides of the above inequality, taking expectation and using the notation 𝛿𝑡 = 𝑓 (𝑥𝑡+1) − 𝑓 inf , we get

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾E [︀𝑉 𝑡]︀ + 𝛾E [︀𝑃 𝑡]︀ . (39)

2

2𝛾 2

Further, Lemma 3 and 4 provide the recursive bounds for the last two terms of (39)

E [︀𝑃 𝑡+1]︀ ≤ (1 − 𝑝min)E [︀𝑃 𝑡]︀ + ℒ̃︀2E [𝑅𝑡] ,

(40)

(︁

)︁

E [︀𝑉 𝑡+1]︀ ≤ (1 − 𝜃)E [︀𝑉 𝑡]︀ + 𝛽 2𝐿̃︀2 + ℒ̃︀2 E [𝑅𝑡] + 2𝛽𝑝maxE [︀𝑃 𝑡]︀ .

(41)

Adding (39) with a 𝛾𝜃 multiple of (41) we obtain

E [︀𝛿𝑡+1]︀ + 𝛾 E [︀𝑉 𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾E [︀𝑉 𝑡]︀

𝜃

2

2𝛾 2

+𝛾E [︀𝑃 𝑡]︀ + 𝛾 (︀(1 − 𝜃) E [︀𝑉 𝑡]︀ + 𝐴𝑟𝑡 + 𝐶E [︀𝑃 𝑡]︀)︀ 𝜃

≤ 𝛿𝑡 + 𝛾 E [︀𝑉 𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 − 𝛾𝐴 )︂ E [︀𝑅𝑡]︀

𝜃

2

2𝛾 2 𝜃

+𝛾

(︂ 1

+

𝐶

)︂

E

[︀𝑃 𝑡]︀

,

𝜃

where

we

denote

𝐴

d=ef

𝛽

(︁ 2𝐿̃︀2

+

)︁ ℒ̃︀2 ,

𝐶

d=ef

2𝛽𝑝max.

Then adding the above inequality with a 𝑝m𝛾in (︀1 + 𝐶𝜃 )︀ multiple of (40), we get

38

EF21 with Bells & Whistles

Oct 6, 2021

E [︀Φ𝑡+1]︀

=

E [︀𝛿𝑡+1]︀ + 𝛾 E [︀𝑉 𝑡+1]︀ +

𝛾

(︂ 1

+

𝐶

)︂

E

[︀𝑃 𝑡+1]︀

𝜃

𝑝min

𝜃

≤ 𝛿𝑡 + 𝛾 E [︀𝑉 𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 − 𝛾𝐴 )︂ E [︀𝑅𝑡]︀

𝜃

2

2𝛾 2 𝜃

+𝛾

(︂ 1

+

𝐶

)︂

E

[︀𝑃 𝑡]︀

𝜃

𝛾 +

(︂ 1

+

𝐶

)︂

(︁ (1

−

𝑝min)E

[︀𝑃 𝑡]︀

+

ℒ̃︀2E

)︁ [︀𝑅𝑡]︀

𝑝min

𝜃

≤ E [︀𝛿𝑡]︀ + 𝛾 E [︀𝑉 𝑡]︀ + 𝛾 (︂1 + 𝐶 )︂ E [︀𝑃 𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁

𝜃

𝑝min

𝜃

2

(︂ 1 𝐿 𝛾𝐴 − −− −

𝛾

(︂ 1

+

𝐶

)︂

)︂ ℒ2

E

[︀𝑅𝑡]︀

̃︀

2𝛾 2 𝜃 𝑝min

𝜃

=

E

[︀Φ𝑡]︀

−

𝛾

E

[︁ ⃦ ⃦∇𝑓

(︀𝑥𝑡)︀⃦⃦2]︁

2

(︂ 1 𝐿 𝛾𝐴 − −− −

𝛾

(︂ 1

+

𝐶

)︂

)︂ ℒ2

E

[︀𝑅𝑡]︀

.

(42)

̃︀

2𝛾 2 𝜃 𝑝min

𝜃

The coefﬁcient in front of E [𝑅𝑡] simpliﬁes after substitution by 𝐴 and 𝐶

𝛾𝐴 +

𝛾

(︂ 1

+

𝐶 )︂ ℒ2

≤

2𝛽 𝐿2

+

(︂ 3𝛽

𝑝max

+

1

)︂ ℒ2.

̃︀

̃︀

̃︀

𝜃 𝑝min

𝜃

𝜃

𝜃 𝑝min 𝑝min

Thus by Lemma 15 and the stepsize choice

(︃ √︃

)︃−1

0<𝛾 ≤ 𝐿+

4𝛽 𝐿2

+

(︂ 3𝛽 2

𝑝max

+

1 )︂ ℒ2

(43)

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

the last term in (42) is not positive. By summing up inequalities for 𝑡 = 0, . . . , 𝑇 − 1, we get

0 ≤ E [︀Φ𝑇 ]︀ ≤ E [︀Φ0]︀ − 𝛾 𝑇∑−︁1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ . 2
𝑡=0

Multiplying both sides by 𝛾2𝑇 and rearranging we get

𝑇∑−︁1 1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ ≤ 2E [︀Φ0]︀ .

𝑇

𝛾𝑇

𝑡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[︁ ⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

,

where

𝑥ˆ𝑇

is

chosen

from 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random.

Corollary 10. Let assumptions of Theorem 5 hold,

𝑣𝑖0 = 𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

(︃ √︃

)︃−1

𝛾=

𝐿+

4𝛽 𝐿2

+

(︂ 3𝛽 2

𝑝max

+

1 )︂ ℒ2

,

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

𝑝𝑖 = 𝜏𝑖 , 𝑖 = 1, . . . , 𝑛. 𝜏𝑖 + 𝑚𝑖

Then,

after

𝑇

iterations/communication

rounds

of

EF21-PAGE

we

have

E

[︁

⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

≤

𝜀2.

It

requires

39

EF21 with Bells & Whistles

Oct 6, 2021

(︃ (𝐿̃︀ + ℒ̃︀)𝛿0 √︂ 𝑝max

√𝑚maxℒ̃︀𝛿0 )︃

𝑇 = 𝒪 𝛼𝜀2

𝑝min + 𝜀2

iterations/communications rounds,

(︃

𝜏𝑖(𝐿̃︀ + ℒ̃︀)𝛿0 √︂ 𝑝max 𝜏𝑖√𝑚maxℒ̃︀𝛿0 )︃

#grad𝑖 = 𝒪 𝑚𝑖 + 𝛼𝜀2

𝑝min + 𝜀2

stochastic oracle calls for worker 𝑖, and

(︃ 𝜏 (𝐿̃︀ + ℒ̃︀)𝛿0 √︂ 𝑝max 𝜏 √𝑚maxℒ̃︀𝛿0 )︃

#grad = 𝒪 𝑚 + 𝛼𝜀2

𝑝min + 𝜀2

stochastic

oracle

calls

per

worker

on

average,

where

𝜏

=

1 𝑛

∑︀𝑛
𝑖=1

𝜏𝑖

,

𝑚

=

1 𝑛

∑︀𝑛
𝑖=1

𝑚𝑖

,

𝑚max

=

max𝑖=1,...,𝑛 𝑚𝑖, 𝑝max = max𝑖=1,...,𝑛 𝑝𝑖, 𝑝min = min𝑖=1,...,𝑛 𝑝𝑖.

Proof. Notice that by Lemma 17 we have

√︃

√︃

𝐿+

4𝛽 𝐿2

+

(︂ 3𝛽 2

𝑝max

+

1 )︂ ℒ2

≤

𝐿+

16 𝐿2

+

(︂ 12 2

𝑝max

+

1 )︂ ℒ2

̃︀

̃︀

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

𝛼2

𝛼2 𝑝min 𝑝min

4 √︂ 24 𝑝max 2

≤ 𝐿 + 𝐿̃︀ +

+ ℒ̃︀

𝛼 √ 𝛼2 𝑝min 𝑝min √

4

24 √︂ 𝑝max

2

≤ 𝐿 + 𝐿̃︀ +

ℒ̃︀ + √ ℒ̃︀

𝛼

𝛼 𝑝min

𝑝min

√

√

5

24 √︂ 𝑝max

2

≤ 𝐿̃︀ +

ℒ̃︀ + √ ℒ̃︀

𝛼

𝛼 𝑝min

𝑝min

√

5 √︂ 𝑝max (︁

)︁

2

≤

𝐿̃︀ + ℒ̃︀ + √ ℒ̃︀

𝛼 𝑝min

𝑝min

5 √︂ 𝑝max (︁

)︁ √

≤

𝐿̃︀ + ℒ̃︀ + 2 𝑚maxℒ̃︀,

𝛼 𝑝min

√

√√

where we used 𝐿 ≤ 𝐿̃︀, 𝑝min ≤ 𝑝max, and the fact that 𝑎 + 𝑏 ≤ 𝑎 + 𝑏 for 𝑎, 𝑏 ≥ 0.

Then the number of communication rounds

2𝛿0 𝑇 ≤ 𝛾𝜀2

2𝛿0 (︂ 5 √︂ 𝑝max (︁

)︁ √

)︂

≤

𝜀2

𝛼

𝐿̃︀ + ℒ̃︀ + 2 𝑚maxℒ̃︀ 𝑝min

(︃ (𝐿̃︀ + ℒ̃︀)𝛿0 √︂ 𝑝max

√𝑚maxℒ̃︀𝛿0 )︃

= 𝒪 𝛼𝜀2 𝑝min + 𝜀2 .

At each worker, we have

#grad𝑖 = 𝑚𝑖 + 𝑇 (𝑝𝑖𝑚𝑖 + (1 − 𝑝𝑖)𝜏𝑖) = 𝑚𝑖 + 2𝑚𝑖𝜏𝑖 𝑇 𝜏𝑖 + 𝑚𝑖 ≤ 𝑚𝑖 + 2𝜏𝑖𝑇.

Averaging over 𝑖 = 1, . . . ,𝑛, we get

40

EF21 with Bells & Whistles

Oct 6, 2021

#grad ≤ 𝑚 + 2𝜏 𝑇 (︃ 𝜏 (𝐿̃︀ + ℒ̃︀)𝛿0 √︂ 𝑝max 𝜏 √𝑚maxℒ̃︀𝛿0 )︃
= 𝒪 𝑚 + 𝛼𝜀2 𝑝min + 𝜀2 .

E.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION Theorem 6. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 3 be set as

{︂ 𝜃 𝑝min }︂

0 < 𝛾 ≤ min 𝛾0, ,

,

(44)

2𝜇 2𝜇

(︂ √︂

)︂−1

def
where 𝛾0 = 0 < 𝛾 ≤

𝐿+

(︁

)︁

8𝜃𝛽 𝐿̃︀2 + 4 5𝜃𝛽 𝑝𝑝mmainx + 𝑝m1in ℒ̃︀2

√︁

, 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1 − (1 − 𝛼)(1 + 𝑠), 𝛽 = (1 − 𝛼) (︀1 + 𝑠−1)︀ for any 𝑠 > 0.

Let

Ψ𝑡

def
=

𝑓 (𝑥𝑡)

−

𝑓 (𝑥⋆)

+

2𝛾 𝑉

𝑡

+

2𝛾

(︁

)︁

1 + 4𝛽𝑝max 𝑃 𝑡. Then for any 𝑇 ≥ 0, we have

𝜃

𝑝min

𝜃

E [︀Ψ𝑇 ]︀ ≤ (1 − 𝛾𝜇)𝑇 E [︀Ψ0]︀ .

(45)

Proof. Similarly to the proof of Theorem 5 the inequalities (39), (40), (41) hold with 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆).
Adding (39) with a 2𝜃𝛾 multiple of (41) we obtain

E [︀𝛿𝑡+1]︀ + 2𝛾 E [︀𝑉 𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾E [︀𝑉 𝑡]︀

𝜃

2

2𝛾 2

+𝛾E [︀𝑃 𝑡]︀ + 2𝛾 (︀(1 − 𝜃) E [︀𝑉 𝑡]︀ + 𝐴𝑟𝑡 + 𝐶E [︀𝑃 𝑡]︀)︀ 𝜃

≤

𝛿𝑡

+

2𝛾

E

[︀𝑉

𝑡]︀

(︂ 1

−

𝜃 )︂

𝜃

2

− 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 − 2𝛾𝐴 )︂ E [︀𝑅𝑡]︀

2

2𝛾 2 𝜃

+𝛾

(︂ 1

+

2𝐶

)︂

E

[︀𝑃 𝑡]︀

,

𝜃

where

𝐴

d=ef

𝛽

(︁ 2𝐿̃︀2

+

)︁ ℒ̃︀2 ,

𝐶

d=ef

2𝛽𝑝max.

Then adding the above inequality with a 𝑝2m𝛾in (︀1 + 2𝜃𝐶 )︀ multiple of (40), we get

41

EF21 with Bells & Whistles

Oct 6, 2021

E [︀Ψ𝑡+1]︀

=

E [︀𝛿𝑡+1]︀ + 2𝛾 E [︀𝑉 𝑡+1]︀ +

2𝛾

(︂ 1

+

2𝐶

)︂

E

[︀𝑃 𝑡+1]︀

𝜃

𝑝min

𝜃

≤ 𝛿𝑡 + 𝛾 E [︀𝑉 𝑡]︀ (︂1 − 𝜃 )︂ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 − 2𝛾𝐴 )︂ E [︀𝑅𝑡]︀

𝜃

22

2𝛾 2 𝜃

+𝛾

(︂ 1

+

2𝐶

)︂

E

[︀𝑃 𝑡]︀

𝜃

2𝛾 +

(︂ 1

+

2𝐶

)︂

(︁ (1

−

𝑝min)E

[︀𝑃 𝑡]︀

+

ℒ̃︀2E

)︁ [︀𝑅𝑡]︀

𝑝min

𝜃

≤

E

[︀𝛿𝑡]︀

+

2𝛾

E

[︀𝑉

𝑡]︀

(︂ 1

−

𝜃

)︂

+

2𝛾

(︂ 1

+

2𝐶

)︂

E

[︀𝑃 𝑡]︀

(︁ 1

−

𝑝min

)︁

𝜃

2

𝑝min

𝜃

2

𝛾

[︁ ⃦

− E ∇𝑓

(︀𝑥𝑡)︀⃦2]︁

−

(︂

1

𝐿 2𝛾𝐴

−−

−

2𝛾

(︂ 1

+

2𝐶

)︂

)︂ ℒ2

E

[︀𝑅𝑡]︀

.

⃦

⃦

̃︀

2

2𝛾 2 𝜃 𝑝min

𝜃

(46)

PL inequality implies that 𝛿𝑡 − 𝛾2 ‖∇𝑓 (𝑥𝑡)‖2 ≤ (1 − 𝛾𝜇)𝛿𝑡. In view of the above inequality and our assumption on the stepsize ( 𝛾 ≤ 2𝜃𝜇 , 𝛾 ≤ 𝑝2m𝜇in ) , we get

E [︀Ψ𝑡+1]︀ ≤ (1 − 𝛾𝜇)E [︀Ψ𝑡]︀ − (︂ 1

𝐿 2𝛾𝐴

−−

−

2𝛾

(︂ 1

+

2𝐶

)︂

)︂ ℒ2

E

[︀𝑅𝑡]︀

.

̃︀

2𝛾 2 𝜃 𝑝min

𝜃

The coefﬁcient in front of E [𝑅𝑡] simpliﬁes after substitution by 𝐴 and 𝐶

2𝛾𝐴 +

2𝛾

(︂ 1

+

2𝐶

)︂

ℒ2

=

4𝛽 𝐿2 + (︂ 2𝛽 +

2

+ 8𝛽 𝑝max )︂ ℒ2

̃︀

̃︀

̃︀

𝜃

𝑝min

𝜃

𝜃

𝜃 𝑝min 𝜃 𝑝min

≤

4𝛽 𝐿2

+

(︂ 5𝛽 2

𝑝max

+

1

)︂ ℒ2.

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

Thus by Lemma 15 and the stepsize choice

(︃ √︃

)︃−1

0<𝛾 ≤ 𝐿+

8𝛽 𝐿2

+

(︂ 5𝛽 4

𝑝max

+

1 )︂ ℒ2

(47)

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

the last term in (46) is not positive.

E [︀Ψ𝑡+1]︀ ≤ (1 − 𝛾𝜇)E [︀Ψ𝑡]︀ . It remains to unroll the recurrence.

Corollary 11. Let assumptions of Theorem 6 hold,

𝑣𝑖0 = 𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

{︂ 𝜃 𝑝min }︂

𝛾 = min 𝛾0, ,

,

2𝜇 2𝜇

(︃ √︃

)︃−1

𝛾 = 𝐿+

8𝛽 𝐿2

+

(︂ 5𝛽 4

𝑝max

+

1 )︂ ℒ2

,

0

̃︀

̃︀

𝜃

𝜃 𝑝min 𝑝min

𝑝𝑖 = 𝜏𝑖 , 𝑖 = 1, . . . , 𝑛. 𝜏𝑖 + 𝑚𝑖

Then, after 𝑇 iterations of EF21-PAGE we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 inf ]︀ ≤ 𝜀. It requires

(︃ (︃

)︃

)︃

1 𝐿̃︀ + ℒ̃︀√︂ 𝑝max √

(︂ 𝛿0 )︂

𝑇 =𝒪

+ 𝑚maxℒ̃︀ ln

𝜇

𝛼

𝑝min

𝜀

42

EF21 with Bells & Whistles

Oct 6, 2021

iterations/communications rounds,

(︃

(︃

)︃

)︃

𝜏𝑖 𝐿̃︀ + ℒ̃︀√︂ 𝑝max √

(︂ 𝛿0 )︂

#grad𝑖 = 𝒪 𝑚𝑖 + 𝜇

𝛼

+ 𝑚maxℒ̃︀ ln

𝑝min

𝜀

stochastic oracle calls for worker 𝑖, and

(︃

(︃

)︃

)︃

𝜏 𝐿̃︀ + ℒ̃︀√︂ 𝑝max √

(︂ 𝛿0 )︂

#grad = 𝒪 𝑚 +

+ 𝑚maxℒ̃︀ ln

𝜇

𝛼

𝑝min

𝜀

stochastic

oracle

calls

per

worker

on

average,

where

𝜏

=

1 𝑛

∑︀𝑛
𝑖=1

𝜏𝑖

,

𝑚

=

1 𝑛

∑︀𝑛
𝑖=1

𝑚𝑖

,

𝑚max

=

max𝑖=1,...,𝑛 𝑚𝑖, 𝑝max = max𝑖=1,...,𝑛 𝑝𝑖, 𝑝min = min𝑖=1,...,𝑛 𝑝𝑖.

43

EF21 with Bells & Whistles

Oct 6, 2021

F PARTIAL PARTICIPATION

In this section, we provide an option for partial participation of the clients – a feature important in federated learning. Most of the works in compressed distributed optimization deal with full worker participation, i.e., the case when all clients are involved in computation and communication at every iteration. However, in the practice of federated learning, only a subset of clients are allowed to participate at each training round. This limitation comes mainly due to the following two reasons. First, clients (e.g., mobile devices) may wish to join or leave the network randomly. Second, it is often prohibitive to wait for all available clients since stragglers can signiﬁcantly slow down the training process. Although many existing works (Gorbunov et al., 2021; Horváth & Richtárik, 2021; Philippenko & Dieuleveut, 2020; Karimireddy et al., 2020; Yang et al., 2021; Cho et al., 2020) allow for partial participation, they assume either unbiased compressors or no compression at all. We provide a simple analysis of partial participation, which works with biased compressors and builds upon the EF21 mechanism.
The modiﬁed method (Algorithm 4) is called EF21-PP . At each iteration of EF21-PP , the master samples a subset 𝑆𝑡 of clients (nodes), which are required to perform computation. Note, that all other clients (nodes) 𝑖 ∈/ 𝑆𝑡 participate neither in the computation nor in communication at iteration 𝑡.
We allow for an arbitrary sampling strategy of a subset 𝑆𝑡 at the master node. The only requirement is that Prob (𝑖 ∈ 𝑆𝑡) = 𝑝𝑖 > 0 for all 𝑖 = 1, . . . , 𝑛, which is often referred to as a proper arbitrary sampling.9 Clearly, many poplular sampling procedures fell into this setting, for instance, independent sampling with/without replacement, 𝜏 -nice sampling. We do not discuss particular sampling strategies here, more on samplings can be found in (Qu & Richtárik, 2014).

Algorithm 4 EF21-PP (EF21 with partial participation)

1:

Input:

starting point 𝑥0

∈ R𝑑; 𝑔𝑖0

∈ R𝑑

for 𝑖 =

1, . . . , 𝑛 (known by nodes); 𝑔0

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known by master); learning rate 𝛾 > 0

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡

4: Master samples a subset 𝑆𝑡 of nodes (|𝑆𝑡| ≤ 𝑛) such that Prob (𝑖 ∈ 𝑆𝑡) = 𝑝𝑖 5: Master broadcasts 𝑥𝑡+1 to the nodes with 𝑖 ∈ 𝑆𝑡

6: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

7:

if 𝑖 ∈ 𝑆𝑡 then

8:

Compress 𝑐𝑡𝑖 = 𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

9:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

10:

end if

11:

if 𝑖 ∈/ 𝑆𝑡 then

12:

Do not change local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡

13:

end if

14: end for

15: Master updates 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡, 𝑐𝑡𝑖 = 0 for 𝑖 ∈/ 𝑆𝑡

16:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖

17: end for

Lemma 5. Then for Algorithm 4 holds

E

[︀𝐺𝑡+1]︀

≤

(1

−

𝜃𝑝)E

[︀𝐺𝑡]︀

+

𝐵E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

(48)

with 𝜃𝑝

def
=

𝜌𝑝𝑚𝑖𝑛 + 𝜃𝑝𝑚𝑎𝑥 − 𝜌 − (𝑝𝑚𝑎𝑥 − 𝑝𝑚𝑖𝑛), 𝐵

def
=

1 𝑛

∑︀𝑛
𝑖=1

(︀𝛽𝑝𝑖

+

(︀1

+

𝜌−1)︀

(1

−

𝑝𝑖))︀

𝐿2𝑖 ,

𝑝𝑚𝑎𝑥 d=ef max1≤𝑖≤𝑛 𝑝𝑖, 𝑝𝑚𝑖𝑛 d=ef min1≤𝑖≤𝑛 𝑝𝑖, 𝜃 = 1 − (1 + 𝑠)(1 − 𝛼), 𝛽 = (︀1 + 1𝑠 )︀ (1 − 𝛼) and small enough 𝜌, 𝑠 > 0.

Proof. By (13) in Lemma 1, we have for all 𝑖 ∈ 𝑆𝑡

E

[︀𝐺𝑡𝑖+1

|

𝑖

∈

𝑆𝑡]︀

≤

(1

−

𝜃)E

[︀𝐺𝑡𝑖 ]︀

+

𝛽𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2

|

𝑖

∈

]︁ 𝑆𝑡

(49)

9It is natural to focus on proper samplings only since otherwise there is a node 𝑖, which never communicaties. This would be a critical issue when trying to minimize (1) as we do not assume any similarity between 𝑓𝑖(·).

44

EF21 with Bells & Whistles

Oct 6, 2021

with 𝜃 = 1 − (1 + 𝑠)(1 − 𝛼), 𝛽 = (︀1 + 1𝑠 )︀ (1 − 𝛼) and arbitrary 𝑠 > 0.

Deﬁne 𝑊 𝑡 d=ef {𝑔1𝑡 , . . . , 𝑔𝑛𝑡 , 𝑥𝑡, 𝑥𝑡+1} and let 𝑖 ∈/ 𝑆𝑡, then

E [︀𝐺𝑡𝑖+1 | 𝑖 ∈/ 𝑆𝑡]︀

=

E

[︀ E

[︀𝐺𝑡𝑖+1

|

𝑊 𝑡]︀

|

𝑖

∈/

𝑆𝑡]︀

=

E

[︁ E

[︁ ⃦⃦𝑔𝑖𝑡+1

−

∇𝑓𝑖(𝑥𝑡+1)⃦⃦2

|

]︁ 𝑊𝑡

|

𝑖

∈/

]︁ 𝑆𝑡

≤

(1

+

𝜌)E

[︁ E

[︁ ⃦⃦𝑔𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2

|

𝑊

]︁
𝑡

|

𝑖

∈/

]︁ 𝑆𝑡

+

(︀1

+

𝜌−1)︀

E

[︁ E

[︁ ⃦⃦∇𝑓𝑖(𝑥𝑡+1)

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2

|

]︁ 𝑊𝑡

|

𝑖

∈/

]︁ 𝑆𝑡

≤ (1 + 𝜌)E [︀𝐺𝑡𝑖]︀

+

(︀1

+

𝜌−1)︀

E

[︁ ⃦⃦∇𝑓𝑖(𝑥𝑡+1)

−

∇𝑓𝑖(𝑥𝑡)⃦⃦2

|

𝑖

∈/

]︁ 𝑆𝑡

≤

(1

+

𝜌)E

[︀𝐺𝑡𝑖 ]︀

+

(︀1

+

𝜌−1)︀

𝐿2𝑖 E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

.

(50)

Combining (49) and (50), we get

E [︀𝐺𝑡+1]︀

= =
(49),(50)
≤
(𝑖)
≤
=

1

𝑛
∑︁

[︀ 𝑡+1]︀

𝑛 E 𝐺𝑖

𝑖=1

1

𝑛
∑︁

[︀ 𝑡+1

]︀ 1 ∑𝑛︁

[︀ 𝑡+1

]︀

𝑛 𝑝𝑖E 𝐺𝑖 | 𝑖 ∈ 𝑆𝑡 + 𝑛 (1 − 𝑝𝑖) E 𝐺𝑖 | 𝑖 ∈/ 𝑆𝑡

𝑖=1

𝑖=1

1

𝑛
∑︁

(︃

1

𝑛
∑︁

)︃ [︁

2]︁

(1 − 𝜃) 𝑛

𝑝𝑖E [︀𝐺𝑡𝑖]︀ + 𝛽 𝑛

𝑝𝑖𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦

𝑖=1

𝑖=1

1

𝑛
∑︁

+ (1 + 𝜌)

(1 − 𝑝𝑖) E [︀𝐺𝑡𝑖]︀

𝑛

𝑖=1

(︃

1

𝑛
∑︁

)︃ [︁

2]︁

+ (︀1 + 𝜌−1)︀ 𝑛

(1 − 𝑝𝑖)𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦

𝑖=1

1

𝑛
∑︁

(︃

1

𝑛
∑︁

)︃ [︁

2]︁

(1 − 𝜃)𝑝𝑚𝑎𝑥 𝑛 E [︀𝐺𝑡𝑖]︀ + 𝛽 𝑛 𝑝𝑖𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦

𝑖=1

𝑖=1

1

𝑛
∑︁

+ (1 + 𝜌) (1 − 𝑝𝑚𝑖𝑛)

E [︀𝐺𝑡𝑖]︀

𝑛

𝑖=1

(︃

1

𝑛
∑︁

)︃ [︁

2]︁

+ (︀1 + 𝜌−1)︀ 𝑛

(1 − 𝑝𝑖)𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦

𝑖=1

(︂

)︂

(1 − 𝜃)𝑝𝑚𝑎𝑥 + (1 + 𝜌)(1 − 𝑝𝑚𝑖𝑛) E [︀𝐺𝑡]︀

(︃

1

𝑛
∑︁

)︃

[︁

2]︁

+ 𝑛

(︀𝛽𝑝𝑖 + (︀1 + 𝜌−1)︀ (1 − 𝑝𝑖))︀ 𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦

𝑖=1

(︂

)︂

=

1 − (𝜌𝑝𝑚𝑖𝑛 + 𝜃𝑝𝑚𝑎𝑥 − 𝜌 − (𝑝𝑚𝑎𝑥 − 𝑝𝑚𝑖𝑛)) E [︀𝐺𝑡]︀

(︃

1

𝑛
∑︁

)︃

[︁

2]︁

+ 𝑛

(︀𝛽𝑝𝑖 + (︀1 + 𝜌−1)︀ (1 − 𝑝𝑖))︀ 𝐿2𝑖 E ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦ .

𝑖=1

=

(1

−

𝜃𝑝)

E

[︀𝐺𝑡]︀

+

𝐵E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

,

45

EF21 with Bells & Whistles

Oct 6, 2021

Lemma 6. [To simplify the rates for partial participation] Let 𝐵 and 𝜃𝑝 be deﬁned as in Theorems 7 and Theorems 8, and let 𝑝𝑖 = 𝑝 > 0 for all 𝑖 = 1, . . . , 𝑛 . Then there exist 𝜌, 𝑠 > 0 such that
𝑝𝛼 𝜃𝑝 ≥ 2 , (51)

(︃ )︃2

𝐵 4𝐿̃︀

0< ≤

.

(52)

𝜃𝑝

𝑝𝛼

Proof. Under the assumption that 𝑝𝑖 = 𝑝 for all 𝑖 = 1, . . . , 𝑛, the constants simplify to

𝜃𝑝 = 𝜌𝑝 + 𝜃𝑝 − 𝜌,

𝐵 = (︀𝛽𝑝 + (︀1 + 𝜌−1)︀ (1 − 𝑝))︀ 𝐿̃︀2,

𝑝𝑚𝑎𝑥 = 𝑝𝑚𝑖𝑛 = 𝑝.

Case I: let 𝛼 = 1, 𝑝 = 1, then the result holds trivially. √
Case II: let 0 < 𝛼 < 1, 𝑝 = 1, then 𝐵 = 𝛽𝐿̃︀2 , 𝜃𝑝 = 𝜃 = 1 − 1 − 𝛼 ≥ 𝛼2 and (52) follows by Lemma 17.

Case III: let 𝛼 = 1, and 0 < 𝑝 < 1, then 𝜃 = 1 , 𝛽 = 0 , 𝐵 = (︀1 + 𝜌−1)︀ (1−𝑝)𝐿̃︀2, 𝜃𝑝 = 𝑝−𝜌(1−𝑝). Then the choice 𝜌 = 2(1𝑝−𝛼𝑝) simpliﬁes
𝑝 𝜃𝑝 = 2 ,

𝐵 (︀1 + 𝜌−1)︀ (1 − 𝑝)𝐿̃︀2 2(1 − 𝑝)𝐿̃︀2 (︂ 2 )︂ 4𝐿̃︀2

=

=

𝜃𝑝

𝑝 − 𝜌(1 − 𝑝)

𝑝

𝑝 − 1 ≤ 𝑝2 .

Case IV: let 0 < 𝛼 < 1,and 0 < 𝑝 < 1.Then the choice of constants 𝜃 = 1 − (1 − 𝛼) (1 + 𝑠), 𝛽 = (1 − 𝛼) (︀1 + 1𝑠 )︀, 𝜌 = 4(1𝑝−𝛼𝑝) , 𝑠 = 4(1𝛼−𝛼) yields

𝑝𝜌 + 𝜃𝑝 − 𝜌 = 𝑝(𝜌 + 1 − (1 − 𝛼) (1 + 𝑠)) − 𝜌

= 𝑝𝛼 − 𝑝(1 − 𝛼)𝑠 − (1 − 𝑝)𝜌

1

= 𝑝𝛼.

(53)

2

Also

1 4 − 3𝛼 4

1 4(1 − 𝑝) + 𝛼𝑝 4 − 𝑝(4 − 𝛼) 4

1+ =

≤ , 1+ =

=

≤.

𝑠

𝛼

𝛼

𝜌

𝑝𝛼

𝑝𝛼

𝑝𝛼

Thus

(︁ )︁

(︁ )︁

𝐵 𝑝𝛽 + (1 − 𝑝) 1 + 𝜌1

𝑝(1 − 𝛼) (︀1 + 1𝑠 )︀ + (1 − 𝑝) 1 + 𝜌1

=

𝐿̃︀2 =

𝐿̃︀2

𝜃𝑝

𝑝(𝜌 + 𝜃) − 𝜌

12 𝑝𝛼

𝑝(1 − 𝛼) 4 + (1 − 𝑝) 4

≤

𝛼

𝑝𝛼 𝐿̃︀2

12 𝑝𝛼

4+ 4 ≤ 𝛼 𝑝𝛼 𝐿̃︀2
12 𝑝𝛼

8
≤ 1𝑝𝑝𝛼𝛼 𝐿̃︀2
2

16𝐿̃︀2

≤ 𝑝2𝛼2 .

(54)

46

EF21 with Bells & Whistles

Oct 6, 2021

F.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 7. Let Assumption 1 hold, and let the stepsize in Algorithm 4 be set as

(︃ √︃ )︃−1

𝐵

0<𝛾 ≤ 𝐿+

.

(55)

𝜃𝑝

Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

E [︁⃦⃦∇𝑓 (𝑥ˆ𝑇 )⃦⃦2]︁ ≤ 2 (︀𝑓 (𝑥0) − 𝑓 inf )︀ + E [︀𝐺0]︀ (56)

𝛾𝑇

𝜃𝑝𝑇

with 𝜃𝑝

=

𝜌𝑝𝑚𝑖𝑛 + 𝜃𝑝𝑚𝑎𝑥 − 𝜌 − (𝑝𝑚𝑎𝑥 − 𝑝𝑚𝑖𝑛), 𝐵

=

1 𝑛

∑︀𝑛
𝑖=1

(︀𝛽𝑝𝑖

+

(︀1

+

𝜌−1)︀

(1

−

𝑝𝑖))︀

𝐿2𝑖 ,

𝑝𝑚𝑎𝑥 = max1≤𝑖≤𝑛 𝑝𝑖, 𝑝𝑚𝑖𝑛 = min1≤𝑖≤𝑛 𝑝𝑖, 𝜃 = 1 − (1 + 𝑠)(1 − 𝛼), 𝛽 = (︀1 + 1𝑠 )︀ (1 − 𝛼) and

𝜌, 𝑠 > 0.

Proof. By (20), we have

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ . (57)

2

2𝛾 2

2

Lemma 5 states that

E [︀𝐺𝑡+1]︀ ≤ (1 − 𝜃𝑝)E [︀𝐺𝑡]︀ + 𝐵E [︀𝑅𝑡]︀

(58)

with 𝜃𝑝

=

𝜌𝑝𝑚𝑖𝑛 + 𝜃𝑝𝑚𝑎𝑥 − 𝜌 − (𝑝𝑚𝑎𝑥 − 𝑝𝑚𝑖𝑛), 𝐵

=

1 𝑛

∑︀𝑛
𝑖=1

(︀𝛽𝑝𝑖

+

(︀1

+

𝜌−1)︀

(1

−

𝑝𝑖))︀

𝐿2𝑖 ,

𝑝𝑚𝑎𝑥 = max1≤𝑖≤𝑛 𝑝𝑖, 𝑝𝑚𝑖𝑛 = min1≤𝑖≤𝑛 𝑝𝑖, 𝜃 = 1 − (1 + 𝑠)(1 − 𝛼), 𝛽 = (︀1 + 1𝑠 )︀ (1 − 𝛼) and

small enough 𝜌, 𝑠 > 0.

Adding (57) with a 2𝛾𝜃2 multiple of (58) and rearranging terms in the right hand side, we have

E [︀𝛿𝑡+1]︀ + 𝛾 E [︀𝐺𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀

2𝜃𝑝

2𝜃𝑝

− 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 − 𝛾𝐵 )︂ E [︀𝑅𝑡]︀

2

2𝛾 2 2𝜃

≤ E [︀𝛿𝑡]︀ + 𝛾 E [︀𝐺𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ .

2𝜃𝑝

2

The last inequality follows from the bound 𝛾2 𝜃𝐵𝑝 + 𝐿𝛾 ≤ 1, which holds because of Lemma 15 and our assumption on the stepsize. By summing up inequalities for 𝑡 = 0, . . . , 𝑇 − 1, we get

0 ≤ E [︁𝛿𝑇 + 𝛾 𝐺𝑇 ]︁ ≤ 𝛿0 + 𝛾 E [︀𝐺0]︀ − 𝛾 𝑇∑−︁1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ .

2𝜃

2𝜃

2

𝑡=0

Multiplying both sides by 𝛾2𝑇 , after rearranging we get

𝑇∑−︁1 1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ ≤ 2𝛿0 + E [︀𝐺0]︀ .

𝑇

𝛾𝑇

𝜃𝑇

𝑡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[︁ ⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

,

where

𝑥ˆ𝑇

is

chosen

from 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random.

Corollary 12.

Let assumptions of Theorem 7 hold,

𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

(︃ √︃ )︃−1

𝐵

𝛾 = 𝐿+

,

𝜃𝑝

𝑝𝑖 = 𝑝, 𝑖 = 1, . . . , 𝑛,

47

EF21 with Bells & Whistles

Oct 6, 2021

where 𝐵 and 𝜃𝑝 are given in Theorem 7. Then, after 𝑇 iterations/communication rounds of EF21-PP

we

have

E

[︁ ⃦⃦∇𝑓 (𝑥ˆ𝑇

⃦2]︁ )⃦

≤

𝜀2.

It

requires

(︃ )︃

𝐿̃︀𝛿0

𝑇 = #grad = 𝒪 𝑝𝛼𝜀2

(59)

iterations/communications rounds/gradint computations at each node.

Proof. Let 𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛 , then 𝐺0 = 0 and by Theorem 7

(︃

√︃ )︃

(︃

)︃

(𝑖) 2𝛿0 (𝑖𝑖) 2𝛿0

𝐵 (𝑖𝑖𝑖) 2𝛿0

4𝐿̃︀

#grad = 𝑇 ≤ 𝛾𝜀2 ≤ 𝜀2

𝐿 + 𝐿̃︀ 𝜃𝑝

≤ 𝜀2 𝐿 + 𝑝𝛼

(︃

)︃

(︃

)︃

2𝛿0

4𝐿̃︀ (𝑖𝑣) 2𝛿0 𝐿̃︀ 4𝐿̃︀

5𝐿̃︀𝛿0

≤ 𝜀2 𝐿 + 𝑝𝛼 ≤ 𝜀2 𝑝𝛼 + 𝑝𝛼 = 𝑝𝛼𝜀2 ,

where (𝑖) is due to the rate (56) given by Theorem 7. In two (𝑖𝑖) we use the largest possible stepsize (55), in (𝑖𝑖𝑖) we utilize Lemma 6, and (𝑖𝑣) follows by the inequalities 𝛼 ≤ 1, 𝑝 ≤ 1 and 𝐿 ≤ 𝐿̃︀.

F.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION

Theorem 8. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 4 be set as

⎧ (︃

√︃ )︃−1

⎫

0 < 𝛾 ≤ min ⎨ 𝐿 + 2𝐵 , 𝜃𝑝 ⎬ . (60)

⎩

𝜃𝑝

2𝜇 ⎭

Let Ψ𝑡 d=ef 𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆) + 𝜃𝛾𝑝 𝐺𝑡. Then for any 𝑇 ≥ 0, we have

E [︀Ψ𝑇 ]︀ ≤ (1 − 𝛾𝜇)𝑇 E [︀Ψ0]︀

(61)

with 𝜃𝑝

=

𝜌𝑝𝑚𝑖𝑛 + 𝜃𝑝𝑚𝑎𝑥 − 𝜌 − (𝑝𝑚𝑎𝑥 − 𝑝𝑚𝑖𝑛), 𝐵

=

1 𝑛

∑︀𝑛
𝑖=1

(︀𝛽𝑝𝑖

+

(︀1

+

𝜌−1)︀

(1

−

𝑝𝑖))︀

𝐿2𝑖 ,

𝑝𝑚𝑎𝑥 = max1≤𝑖≤𝑛 𝑝𝑖, 𝑝𝑚𝑖𝑛 = min1≤𝑖≤𝑛 𝑝𝑖, 𝜃 = 1 − (1 + 𝑠)(1 − 𝛼), 𝛽 = (︀1 + 1𝑠 )︀ (1 − 𝛼) and

𝜌, 𝑠 > 0.

Proof. Following the same steps as in the proof of Theorem 2, but using (58), and assumption on the stepsize (60), we obtain the result.

Corollary 13. Let assumptions of Theorem 8 hold,

𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

⎧ (︃

√︃ )︃−1

⎫

⎨

2𝐵

𝛾 = min 𝐿 +

⎩

𝜃𝑝

, 𝜃𝑝 ⎬ , 2𝜇 ⎭

𝑝𝑖 = 𝑝, 𝑖 = 1, . . . , 𝑛,

where 𝐵 and 𝜃𝑝 are given in Theorem 8. Then, after 𝑇 iterations/communication rounds of EF21-PP we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 (𝑥⋆)]︀ ≤ 𝜀. It requires

(︃

)︃

𝐿̃︀ (︂ 𝛿0 )︂

𝑇 = #grad = 𝒪

log

(62)

𝑝𝛼𝜇

𝜀

iterations/communications rounds/gradint computations at each node.

Proof. The proof is the same as for Corollary 3. The only difference is that Lemma 6 is needed to upper bound the quantities 1/𝜃𝑝 and 𝐵/𝜃𝑝, which appear in Theorem 8.

48

EF21 with Bells & Whistles

Oct 6, 2021

G BIDIRECTIONAL COMPRESSION

In the majority of applications, the uplink (Client → Server) communication is the bottleneck.

However, in some settings the downlink (Server → Client) communication can also slowdown

training. Tang et al. (2020) construct a mechanism which allows bidirectional biased compression.

(︁ )︁

Their method builds upon the original EF meachanism and they prove 𝒪

1
2

rate for general

𝑇 /3

nonconvex objectives. However, the main defﬁciency of this approach is that it requires an additional

assumption

of

bounded

magnitude

of

error

(there

exists

∆

>

0

such

that

[︁ E ‖𝒞(𝑥)

−

𝑥‖2]︁

≤

∆

for all 𝑥). In this section, we lift this limitation and propose a new method EF21-BC (Algorithm 5), which enjoys the desirable 𝒪 (︀ 𝑇1 )︀, and does not rely on additional assumptions.

Algorithm 5 EF21-BC (EF21 with bidirectional biased compression)

1:

Input:

starting

point

𝑥0

∈

R𝑑;

𝑔0,

𝑏0,

𝑔̃︀𝑖0

∈

R𝑑

for

𝑖

=

1, . . . , 𝑛

(known

by

nodes);

𝑔0 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖0

(known

by

master)

;

learning

rate

𝛾

>

0

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master updates 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

5:

Update 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡, 𝑔𝑡+1 = 𝑔𝑡 + 𝑏𝑡,

6:

compress 𝑐𝑡𝑖 = 𝒞𝑤(∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡), send 𝑐𝑡𝑖 to the master, and

7:

update local state 𝑔̃︀𝑖𝑡+1 = 𝑔̃︀𝑖𝑡 + 𝑐𝑡𝑖

8: end for

9:

Master

computes

𝑔𝑡+1 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖𝑡+1

via

𝑔𝑡+1 ̃︀

=

𝑔𝑡 ̃︀

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖 ,

10: compreses 𝑏𝑡+1 = 𝒞𝑀 (𝑔𝑡+1 − 𝑔𝑡), broadcast 𝑏𝑡+1 to workers ,

̃︀

11: and updates 𝑔𝑡+1 = 𝑔𝑡 + 𝑏𝑡+1

12: end for

Note that 𝒞𝑀 and 𝒞𝑤 stand for contractive compressors of the type 1 of master and workers respectively. In general, different 𝛼𝑀 and 𝛼𝑤 are accepted.

Notations

for

this

section:

𝑃𝑖𝑡

d=ef

‖𝑔̃︀𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)‖2,

𝑃𝑡

d=ef

1 𝑛

∑︀𝑛
𝑖=1

𝑃𝑖𝑡.

Lemma 7. Let Assumption 1 hold, 𝒞𝑤 be a contractive compressor, and 𝑔̃︀𝑖𝑡+1 be an EF21 estimator of ∇𝑓𝑖(𝑥𝑡+1), i. e.

𝑔̃︀𝑖𝑡+1 = 𝑔̃︀𝑖𝑡 + 𝒞𝑤(∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡)

(63)

for arbitrary 𝑔̃︀𝑖0 and all all 𝑖 = 1, . . . , 𝑛, 𝑡 ≥ 0. Then

E [︀𝑃 𝑡+1]︀ ≤ (1 − 𝜃𝑤)E [︀𝑃 𝑡]︀ + 𝛽𝑤𝐿̃︀2E [︀𝑅𝑡]︀ ,

(64)

def
where 𝜃𝑤 = 1 − (1 − 𝛼𝑤)(1 + 𝑠),

𝛽𝑤

def
=

(1

−

𝛼𝑤 )

(︀1

+

𝑠−1)︀

for any 𝑠 > 0.

Proof. The proof is the same as for Lemma 1.

Lemma 8. Let Assumption 1 hold, 𝒞𝑀 , 𝒞𝑤 be contractive compressors. Let 𝑔̃︀𝑖𝑡+1 be an EF21 estimator of ∇𝑓𝑖(𝑥𝑡+1), i. e.

𝑔̃︀𝑖𝑡+1 = 𝑔̃︀𝑖𝑡 + 𝒞𝑤(∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡),

(65)

and

let

𝑔𝑡+1

be

an

EF21

estimator

of

𝑔𝑡+1 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖𝑡+1,

i.

e.

𝑔𝑡+1 = 𝑔𝑡 + 𝒞𝑀 (𝑔𝑡+1 − 𝑔𝑡)

(66)

̃︀

for arbitrary 𝑔0, 𝑔̃︀𝑖0 and all 𝑖 = 1, . . . , 𝑛, 𝑡 ≥ 0. Then

E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡+1⃦⃦2]︁

≤

(1

−

𝜃𝑀

)E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

+

8𝛽𝑀

E

[︀𝑃

𝑡]︀

+

8𝛽𝑀

𝐿̃︀2E

[︀𝑅𝑡]︀

,

(67)

̃︀

̃︀

where

𝑔𝑡

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡,

𝑔𝑡 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖𝑡

,

𝜃𝑀

=

1 − (1 − 𝛼𝑀 )(1 + 𝜌),

𝛽𝑀

=

(1 − 𝛼𝑀 ) (︀1 +

𝜌−1)︀

for any 𝜌 > 0.

49

EF21 with Bells & Whistles

Oct 6, 2021

Proof. Similarly to the proof of Lemma 1, deﬁne 𝑊 𝑡 d=ef {𝑔1𝑡 , . . . , 𝑔𝑛𝑡 , 𝑥𝑡, 𝑥𝑡+1} and

E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡+1⃦⃦2]︁

=

E

[︁ E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡+1⃦⃦2

|

]︁]︁ 𝑊𝑡

̃︀

̃︀

=

E

[︁ E

[︁ ⃦⃦𝑔𝑡

+

𝒞𝑀 (𝑔𝑡+1

−

𝑔𝑡)

−

𝑔𝑡+1⃦⃦2

|

]︁]︁ 𝑊𝑡

̃︀

̃︀

(8)
≤

(1

−

𝛼𝑀

)E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡⃦⃦2]︁

̃︀

(𝑖)
≤

(1

−

𝛼𝑀

)(1

+

𝜌)E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

̃︀

+(1 − 𝛼𝑀 ) (︀1 + 𝜌−1)︀ ⃦⃦𝑔𝑡+1 − 𝑔𝑡⃦⃦2

̃︀

̃︀

=

(1

−

𝜃𝑀

)E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

+

𝛽𝑀

⃦⃦𝑔𝑡+1

−

𝑔𝑡⃦⃦2

,

(68)

̃︀

̃︀

̃︀

where (𝑖) follows by Young’s inequality (118), and in (𝑖𝑖) we use the deﬁnition of 𝜃𝑀 and 𝛽𝑀 .

Further we bound the last term in (68). Recall that

1

𝑛
∑︁

𝑔𝑡+1 = 𝑔𝑡 +

̃︀

̃︀

𝑐𝑡𝑖 .

(69)

𝑛

𝑖=1

where

𝑐𝑡𝑖

=

𝒞𝑤 (∇𝑓𝑖 (𝑥𝑡+1 )

−

𝑔̃︀𝑖𝑡)

and

𝑔𝑡 ̃︀

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔̃︀𝑖𝑡

.

Then

⎡ ⃦

𝑛

⃦2⎤

E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡⃦⃦2]︁

(=69)

E

⃦ ⃦𝑔𝑡

+

1

∑︁ 𝑐𝑡

−

⃦ 𝑔𝑡⃦

̃︀

̃︀

⎣⃦̃︀ 𝑛 𝑖 ̃︀ ⃦ ⎦

⃦

𝑖=1

⃦

⎡ ⃦

𝑛

⃦2⎤

= E ⎣⃦⃦⃦ 𝑛1 ∑︁ 𝑐𝑡𝑖⃦⃦⃦ ⎦

⃦ 𝑖=1 ⃦

(𝑖)

1

𝑛
∑︁

[︁ ⃦

𝑡⃦2]︁

≤ 𝑛

E ⃦𝑐𝑖⃦

𝑖=1

1

𝑛
∑︁

[︁

2]︁

= 𝑛

E ⃦⃦𝑐𝑡𝑖 − (︀∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡)︀ + (︀∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡)︀⃦⃦

𝑖=1

(118)

1

𝑛
∑︁

[︁

[︁

2 ]︁]︁

≤2 𝑛

E

E

⃦ ⃦𝒞𝑤

(︀∇𝑓𝑖(𝑥𝑡+1)

−

𝑔̃︀𝑖𝑡)︀

−

(︀∇𝑓𝑖(𝑥𝑡+1)

−

𝑔̃︀𝑖𝑡)︀⃦⃦

| 𝑊𝑡

𝑖=1

1

𝑛
∑︁

[︁

2]︁

+2 𝑛

E

⃦ ⃦∇𝑓𝑖

(𝑥𝑡+1

)

−

𝑔̃︀𝑖𝑡

⃦ ⃦

𝑖=1

(8)

1

𝑛
∑︁

[︁

2]︁

1

𝑛
∑︁

[︁

2]︁

≤ 2(1 − 𝛼𝑤) 𝑛

E ⃦⃦∇𝑓𝑖(𝑥𝑡+1) − 𝑔̃︀𝑖𝑡⃦⃦ + 2 𝑛

E

⃦ ⃦∇𝑓𝑖

(𝑥𝑡+1

)

−

𝑔̃︀𝑖𝑡

⃦ ⃦

𝑖=1

𝑖=1

1

𝑛
∑︁

[︁

2]︁

= 2(2 − 𝛼𝑤) 𝑛

E

⃦ ⃦∇𝑓𝑖

(𝑥𝑡+1

)

−

𝑔̃︀𝑖𝑡

⃦ ⃦

𝑖=1

(𝑖𝑖)

1

𝑛
∑︁

[︁

2]︁

<4 𝑛

E

⃦ ⃦∇𝑓𝑖

(𝑥𝑡+1

)

−

𝑔̃︀𝑖𝑡

⃦ ⃦

𝑖=1

1

𝑛
∑︁

[︁

2]︁

=4 𝑛

E ⃦⃦∇𝑓𝑖(𝑥𝑡+1) − ∇𝑓𝑖(𝑥𝑡) − (︀𝑔̃︀𝑖𝑡 − ∇𝑓𝑖(𝑥𝑡))︀⃦⃦

𝑖=1

(118) 1 ∑𝑛︁ ⃦ 𝑡

𝑡 ⃦2 1 ∑𝑛︁ [︁⃦

𝑡+1

𝑡 ⃦2]︁

≤8 𝑛

⃦𝑔̃︀𝑖 − ∇𝑓𝑖(𝑥 )⃦

+8 𝑛

E ⃦∇𝑓𝑖(𝑥 ) − ∇𝑓𝑖(𝑥 )⃦

𝑖=1

𝑖=1

(𝑖𝑖𝑖) 1 ∑𝑛︁ [︁⃦ 𝑡

𝑡 ⃦2]︁

[︁ 2 ⃦ 𝑡+1

𝑡⃦2]︁

≤8 𝑛

E ⃦𝑔̃︀𝑖 − ∇𝑓𝑖(𝑥 )⃦ + 8𝐿̃︀ E ⃦𝑥 − 𝑥 ⃦

𝑖=1

= 8E [︀𝑃 𝑡]︀ + 8𝐿̃︀2E [︀𝑅𝑡]︀ ,

(70)

50

EF21 with Bells & Whistles

Oct 6, 2021

where in (𝑖) we use (119), (𝑖𝑖) is due to 𝛼𝑤 > 0, (𝑖𝑖𝑖) holds by Assumption 1. In the last step we

apply

the

deﬁnition

of

𝑃𝑡

=

1 𝑛

∑︀𝑛
𝑖=1

‖𝑔̃︀𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)‖2,

and

𝑅𝑡

=

⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2

Finally, plugging (70) into (68), we conclude the proof.

G.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 9. Let Assumption 1 hold, and let the stepsize in Algorithm 5 be set as

(︃

√︃

)︃−1

16𝛽𝑀 2𝛽𝑤 (︂ 8𝛽𝑀 )︂

0 < 𝛾 ≤ 𝐿 + 𝐿̃︀

+

1+

(71)

𝜃𝑀

𝜃𝑤

𝜃𝑀

Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

E [︁⃦⃦∇𝑓 (𝑥ˆ𝑇 )⃦⃦2]︁ ≤ 2E [︀Ψ0]︀ , (72) 𝛾𝑇

where

Ψ𝑡

def
=

𝑓 (𝑥𝑡) − 𝑓 inf

+

𝛾

‖𝑔𝑡 − 𝑔𝑡‖2 + 𝛾

(︁ 1

+

8𝛽𝑀

)︁

𝑃 𝑡,

𝐿̃︀

=

√︁

1

∑︀𝑛

𝐿2,

𝜃𝑤

def
=

1 − (1 −

𝜃𝑀

̃︀

𝜃𝑤

𝜃𝑀

𝑛 𝑖=1 𝑖

𝛼𝑤)(1 + 𝑠),

𝛽𝑤

def
=

(1 − 𝛼𝑤) (︀1

+

𝑠−1)︀,

𝜃𝑀

def
=

1 − (1 − 𝛼𝑀 )(1 + 𝜌),

𝛽𝑀

def
=

(1 − 𝛼𝑀 ) (︀1

+

𝜌−1)︀

for any 𝜌, 𝑠 > 0.

Proof. We apply Lemma 16 and split the error ‖𝑔𝑡 − ∇𝑓 (𝑥𝑡)‖2 in two parts

𝑓 (𝑥𝑡+1) ≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾 ⃦⃦𝑔𝑡 − ∇𝑓 (𝑥𝑡)⃦⃦2

2

2𝛾 2

2

≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡

2

2𝛾 2

+𝛾⃦⃦𝑔𝑡 − 𝑔𝑡⃦⃦2 + 𝛾 ⃦⃦𝑔𝑡 − ∇𝑓 (𝑥𝑡)⃦⃦2

̃︀

̃︀

≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡

2

2𝛾 2

1 ∑𝑛︁ ⃦ 𝑡 𝑡⃦2 1 ∑𝑛︁ ⃦ 𝑡

𝑡 ⃦2

+𝛾 𝑛

⃦𝑔 − 𝑔 ⃦ + 𝛾

̃︀

𝑛

⃦𝑔̃︀𝑖 − ∇𝑓𝑖(𝑥 )⃦

𝑖=1

𝑖=1

= 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ 𝑅𝑡 + 𝛾⃦⃦𝑔𝑡 − 𝑔𝑡⃦⃦2 + 𝛾𝑃 𝑡, (73)

2

2𝛾 2

̃︀

where

we

used

notation

𝑅𝑡

=

‖𝛾𝑔𝑡‖2

=

⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2,

𝑃𝑡

=

1 𝑛

∑︀𝑛
𝑖=1

‖𝑔̃︀𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)‖2

and

applied

(118) and (119).

Subtracting 𝑓 inf from both sides of the above inequality, taking expectation and using the notation 𝛿𝑡 = 𝑓 (𝑥𝑡+1) − 𝑓 inf , we get

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀ + 𝛾E [︁⃦⃦𝑔𝑡 − 𝑔𝑡⃦⃦2]︁ + 𝛾E [︀𝑃 𝑡]︀ .

2

2𝛾 2

̃︀

(74)

Further, Lemma 7 and 8 provide the recursive bounds for the last two terms of (74)

E [︀𝑃 𝑡+1]︀ ≤ (1 − 𝜃𝑤)E [︀𝑃 𝑡]︀ + 𝛽𝑤𝐿̃︀2E [𝑅𝑡] ,

(75)

E

[︁ ⃦⃦𝑔𝑡+1

−

𝑔𝑡+1⃦⃦2]︁

≤

(1

−

𝜃𝑀

)E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

+

8𝛽𝑀

𝐿̃︀2E

[𝑅𝑡]

+

8𝛽𝑀

E

[︀𝑃

𝑡]︀

.

(76)

̃︀

̃︀

51

EF21 with Bells & Whistles

Oct 6, 2021

Summing up (74) with a 𝜃𝛾𝑀 multiple of (76) we obtain

E [︀𝛿𝑡+1]︀ + 𝛾 E [︁⃦⃦𝑔𝑡+1 − 𝑔𝑡+1⃦⃦2]︁ ≤ E [︀𝛿𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − (︂ 1 − 𝐿 )︂ E [︀𝑅𝑡]︀

𝜃𝑀

̃︀

2

2𝛾 2

+𝛾E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

+

𝛾E

[︀𝑃

𝑡]︀

̃︀

𝛾 +

(︁ (1

−

𝜃𝑀

)

E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁)︁

𝜃𝑀

̃︀

𝛾 +

(︁

)︁

8𝛽𝑀 𝐿̃︀2E [︀𝑅𝑡]︀ + 8𝛽𝑀 E [︀𝑃 𝑡]︀

𝜃𝑀

≤

E [︀𝛿𝑡]︀ +

𝛾

E

[︁ ⃦⃦𝑔𝑡

−

𝑔𝑡⃦⃦2]︁

−

𝛾

E

[︁ ⃦ ⃦∇𝑓

(︀𝑥𝑡)︀⃦⃦2]︁

𝜃𝑀

̃︀

2

(︃

)︃

− 1 − 𝐿 − 8𝛾𝛽𝑀 𝐿̃︀2 E [︀𝑅𝑡]︀

2𝛾 2

𝜃𝑀

+𝛾

(︂ 1

+

8𝛽𝑀

)︂

E

[︀𝑃 𝑡]︀

.

𝜃𝑀

(︁

)︁

Then adding the above inequality with a 𝜃𝛾𝑤 1 + 8𝜃𝛽𝑀𝑀 multiple of (75), we get

E [︀Ψ𝑡+1]︀ = E [︀𝛿𝑡+1]︀ + 𝛾 E [︁⃦⃦𝑔𝑡+1 − 𝑔𝑡+1⃦⃦2]︁ + 𝛾 (︂1 + 8𝛽𝑀 )︂ E [︀𝑃 𝑡+1]︀

𝜃𝑀

̃︀

𝜃𝑤

𝜃𝑀

(︃

)︃

≤ E [︀𝛿𝑡]︀ + 𝛾 E [︁⃦⃦𝑔𝑡 − 𝑔𝑡⃦⃦2]︁ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁ − 1 − 𝐿 − 8𝛾𝛽𝑀 𝐿̃︀2 E [︀𝑅𝑡]︀

𝜃𝑀

̃︀

2

2𝛾 2

𝜃𝑀

+𝛾

(︂ 1

+

8𝛽𝑀

)︂

E

[︀𝑃 𝑡]︀

𝜃𝑀

𝛾 +

(︂ 1

+

8𝛽𝑀

)︂

(︁ (1

−

𝜃𝑤 )E

[︀𝑃 𝑡]︀

+

𝛽𝑤 𝐿̃︀ 2 E

)︁ [︀𝑅𝑡]︀

𝜃𝑤

𝜃𝑀

≤ E [︀𝛿𝑡]︀ + 𝛾 E [︁⃦⃦𝑔𝑡 − 𝑔𝑡⃦⃦2]︁ + 𝛾 (︂1 + 8𝛽𝑀 )︂ E [︀𝑃 𝑡]︀ − 𝛾 E [︁⃦⃦∇𝑓 (︀𝑥𝑡)︀⃦⃦2]︁

𝜃𝑀

̃︀

𝜃𝑤

𝜃𝑀

2

(︃

)︃

− 1 − 𝐿 − 8𝛾𝛽𝑀 𝐿̃︀2 − 𝛾 (︂1 + 8𝛽𝑀 )︂ 𝛽𝑤𝐿̃︀2 E [︀𝑅𝑡]︀

2𝛾 2

𝜃𝑀

𝜃𝑤

𝜃𝑀

=

E

[︀Ψ𝑡]︀

−

𝛾

E

[︁ ⃦ ⃦∇𝑓

(︀𝑥𝑡)︀⃦⃦2]︁

2

(︃

)︃

− 1 − 𝐿 − 8𝛾𝛽𝑀 𝐿̃︀2 − 𝛾𝛽𝑤𝐿̃︀2 (︂1 + 8𝛽𝑀 )︂ E [︀𝑅𝑡]︀ . (77)

2𝛾 2

𝜃𝑀

𝜃𝑤

𝜃𝑀

Thus by Lemma 15 and the stepsize choice

(︃

√︃

)︃−1

16𝛽𝑀 2𝛽𝑤 (︂ 8𝛽𝑀 )︂

0 < 𝛾 ≤ 𝐿 + 𝐿̃︀

+

1+

(78)

𝜃𝑀

𝜃𝑤

𝜃𝑀

the last term in (77) is not positive. By summing up inequalities for 𝑡 = 0, . . . , 𝑇 − 1, we get

0 ≤ E [︀Ψ𝑇 ]︀ ≤ E [︀Ψ0]︀ − 𝛾 𝑇∑−︁1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ . 2
𝑡=0
Multiplying both sides by 𝛾2𝑇 and rearranging we get

𝑇∑−︁1 1 E [︁⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2]︁ ≤ 2E [︀Ψ0]︀ .

𝑇

𝛾𝑇

𝑡=0

52

EF21 with Bells & Whistles

Oct 6, 2021

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[︁ ⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)

⃦2 ⃦

]︁

,

where

𝑥ˆ𝑇

is

chosen

from 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random.

Corollary 14. Let assumption of Theorem 9 hold,

𝑔0 = ∇𝑓 (𝑥0), 𝑔̃︀𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

(︃

√︃

)︃−1

16𝛽𝑀 2𝛽𝑤 (︂ 8𝛽𝑀 )︂

𝛾 = 𝐿 + 𝐿̃︀

+

1+

,

𝜃𝑀

𝜃𝑤

𝜃𝑀

Then, after 𝑇

iterations/communication

rounds

of

EF21-BC

we

have

E

[︁ ⃦⃦∇𝑓 (𝑥ˆ𝑇

⃦2]︁ )⃦

≤

𝜀2.

It

requires

(︃

)︃

𝐿̃︀𝛿0

𝑇 = #grad = 𝒪 𝛼𝑤𝛼𝑀 𝜀2

(79)

iterations/communications rounds/gradint computations at each node.

Proof. Note that by Lemma 17 and 𝛼𝑀 , 𝛼𝑤 ≤ 1, we have

16𝛽𝑀 2𝛽𝑤 (︂ 8𝛽𝑀 )︂

4

4 (︂

4 )︂

+

1+

𝜃𝑀

𝜃𝑤

𝜃𝑀

≤ 16 𝛼2 + 2 𝛼2 1 + 8 𝛼2

𝑀

𝑤

𝑀

64 8 33

≤ 𝛼2 + 𝛼2 𝛼2

𝑀

𝑤𝑀

64 + 8 · 33 ≤ 𝛼2 𝛼2 .
𝑤𝑀

It remains to apply the steps similar to those in the proof of Corollary 2.

G.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION

Theorem 10. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 3 be set as

{︂ 𝜃𝑀 𝜃𝑤 }︂

0 < 𝛾 ≤ min 𝛾0, ,

,

(80)

2𝜇 2𝜇

(︂

√︂

)︂−1

where 𝛾0 d=ef 𝐿 + 𝐿̃︀ 32𝜃𝛽𝑀𝑀 + 4𝛽𝜃𝑤𝑤𝐿̃︀2 (︁1 + 16𝜃𝛽𝑀𝑀 )︁ , 𝐿̃︀ = √︁ 𝑛1 ∑︀𝑛𝑖=1 𝐿2𝑖 , 𝜃𝑤 d=ef 1−(1−𝛼𝑤)(1+

𝑠),

𝛽𝑤

def
=

(1 − 𝛼𝑤) (︀1

+

𝑠−1)︀,

𝜃𝑀

def
=

1 − (1 − 𝛼𝑀 )(1 + 𝜌),

𝛽𝑀

def
=

(1 − 𝛼𝑀 ) (︀1

+

𝜌−1)︀

for

any

𝜌, 𝑠 > 0.

Let

Ψ𝑡

def
=

𝑓 (𝑥𝑡)

−

𝑓 inf

+

𝛾

‖𝑔𝑡 − 𝑔𝑡‖2 +

𝛾

(︁

)︁

1 + 8𝛽𝑀 𝑃 𝑡. Then for any 𝑇 ≥ 0, we have

𝜃𝑀

̃︀

𝜃𝑤

𝜃𝑀

E [︀Ψ𝑇 ]︀ ≤ (1 − 𝛾𝜇)𝑇 E [︀Ψ0]︀ .

(81)

Proof. Similarly to the proof of Theorem 9 the inequalities (74), (75), (76) hold with 𝛿𝑡 = 𝑓 (𝑥𝑡) − 𝑓 (𝑥⋆).

It remains to apply the steps similar to those in the proof of Theorem 6.

Corollary 15. Let assumption of Theorem 10 hold,

𝑔0 = ∇𝑓 (𝑥0), 𝑔̃︀𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

⎛

√︃

⎞−1

{︂ 𝜃𝑀 𝜃𝑤 }︂

𝛾 = min 𝛾0, ,

,

2𝜇 2𝜇

32𝛽𝑀 4𝛽𝑤𝐿̃︀2 (︂ 16𝛽𝑀 )︂

𝛾0 = ⎝𝐿 + 𝐿̃︀

+

1+

⎠,

𝜃𝑀

𝜃𝑤

𝜃𝑀

53

EF21 with Bells & Whistles

Oct 6, 2021

Then, after 𝑇 iterations of EF21-PAGE we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 inf ]︀ ≤ 𝜀. It requires

(︃

)︃

𝐿̃︀

(︂ 𝛿0 )︂

𝑇 = #grad = 𝒪

ln

𝜇𝛼𝑤 𝛼𝑀

𝜀

iterations/communications rounds/gradint computations at each node.

54

EF21 with Bells & Whistles

Oct 6, 2021

H HEAVY BALL MOMENTUM
Notations for this section: 𝑅𝑡 = ‖𝛾𝑔𝑡‖2 = (1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2.
In this section, we study the momentum version of EF21. In particular, we focus on Polyak style momentum (Polyak, 1964; Yang et al., 2016). Let 𝑔𝑡 be a gradient estimator at iteration 𝑡, then the update rule of heavy ball (HB) is given by

𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑔𝑡 + 𝜂 (︀𝑥𝑡 − 𝑥𝑡−1)︀ ,
where 𝑥−1 = 𝑥0, 𝜂 ∈ [0, 1) is called the momentum parameter, and 𝛾 > 0 is the stepsize. The above update rule can be viewed as a combination of the classical gradient step
𝑦𝑡 = 𝑥𝑡 − 𝛾𝑔𝑡 followed by additional momentum step
𝑥𝑡+1 = 𝑦𝑡 + 𝜂 (︀𝑥𝑡 − 𝑥𝑡−1)︀ . Here the momentum term is added to accelerate the convergence and make the trajectory look like a smooth descent to the bottom of the ravine, rather than zigzag.
Equivalently, the update of HB can be implemented by the following two steps (Yang et al., 2016): {︂ 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑣𝑡 𝑣𝑡+1 = 𝜂𝑣𝑡 + 𝑔𝑡+1.
We are now ready to present the distributed variant of heavy ball method enhanced with a contractive compressor 𝒞, and EF21 mechanism, which we call EF21-HB (Algorithm 6). We present the complexity results in Theorem 11 and Corollary 16.

Algorithm 6 EF21-HB

1: Input: starting point 𝑥0 ∈ R𝑑; 𝑔𝑖0 ∈ R𝑑 for 𝑖 = 1, . . . , 𝑛 (known by nodes); 𝑣0 = 𝑔0 =

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known

by

master);

learning

rate

𝛾

>

0;

momentum

parameter

0

≤

𝜂

<

1

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑣𝑡 and broadcasts 𝑥𝑡+1 to all nodes

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

5:

Compress 𝑐𝑡𝑖 = 𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

6:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

7: end for

8:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡 +

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖 ,

and

𝑣𝑡+1

=

𝜂𝑣𝑡 + 𝑔𝑡+1

9: end for

In the analysis of EF21-HB, we assume by default that 𝑣−1 = 0.
Lemma 9. Let sequences {𝑥𝑡}𝑡≥0 , and {𝑣𝑡}𝑡≥0 be generated by Algorithm 6 and let the sequence {𝑧𝑡}𝑡≥0 be deﬁned as 𝑧𝑡+1 d=ef 𝑥𝑡+1 − 1𝛾−𝜂𝜂 𝑣𝑡 with 0 ≤ 𝜂 < 1. Then for all 𝑡 ≥ 0
𝑧𝑡+1 = 𝑧𝑡 − 𝛾 𝑔𝑡. 1−𝜂

Proof.

𝑧𝑡+1

(𝑖)
=

𝑥𝑡+1 − 𝛾𝜂 𝑣𝑡

1−𝜂

(𝑖𝑖)
=

𝑥𝑡 − 𝛾𝑣𝑡 − 𝛾𝜂 𝑣𝑡

1−𝜂

(𝑖𝑖𝑖)
=

𝑧𝑡 +

𝛾𝜂 𝑣𝑡−1 −

𝛾

𝑣𝑡

1−𝜂

1−𝜂

= 𝑧𝑡 − 𝛾 (︀𝑣𝑡 − 𝜂𝑣𝑡−1)︀ 1−𝜂

= 𝑧𝑡 − 𝛾 𝑔𝑡, 1−𝜂

55

EF21 with Bells & Whistles

Oct 6, 2021

where in (𝑖) and (𝑖𝑖𝑖) we use the deﬁnition of 𝑧𝑡+1 and 𝑧𝑡, in (𝑖𝑖) we use the step 𝑥𝑡+1 = 𝑥𝑡 − 𝛾𝑣𝑡 (line 3 of Algorithm 6). Finally, the last equality follows by the update 𝑣𝑡+1 = 𝜂𝑣𝑡 + 𝑔𝑡+1 (line 8 of
Algorithm 6).

Lemma 10. Let the sequence {𝑣𝑡}𝑡≥0 be deﬁned as 𝑣𝑡+1 = 𝜂𝑣𝑡 + 𝑔𝑡+1 with 0 ≤ 𝜂 < 1. Then

𝑇∑−︁1 ⃦⃦𝑣𝑡⃦⃦2 ≤ 1 𝑇∑−︁1 ⃦⃦𝑔𝑡⃦⃦2 .

(1 − 𝜂)2

𝑡=0

𝑡=0

Proof. Unrolling the given recurrence and noticing that 𝑣−1 = 0, we have 𝑣𝑡 = ∑︀𝑡𝑙=0 𝜂𝑡−𝑙𝑔𝑙. Deﬁne 𝐻 d=ef ∑︀𝑡𝑙=0 𝜂𝑙 ≤ 1−1 𝜂 . Then by Jensen’s inequality

𝑇 −1
∑︁ 2

𝑇 −1
∑︁

⃦𝑡 ⃦∑︁

𝜂𝑡−𝑙

⃦2 ⃦

⃦⃦𝑣𝑡⃦⃦ = 𝐻2 ⃦

𝑔𝑙⃦

𝑡=0 𝑡=0 ⃦⃦ 𝑙=0 𝐻 ⃦⃦

𝑇 −1
∑︁

𝑡
∑︁

𝜂𝑡−𝑙

2

≤ 𝐻2

⃦⃦𝑔𝑙⃦⃦

𝑡=0 𝑙=0 𝐻

𝑇 −1 𝑡
= 𝐻 ∑︁ ∑︁ 𝜂𝑡−𝑙 ⃦⃦𝑔𝑙⃦⃦2

𝑡=0 𝑙=0

1

𝑇 −1 𝑡
∑︁ ∑︁

2

≤

𝜂𝑡−𝑙

⃦⃦𝑔𝑙

⃦ ⃦

1 − 𝜂 𝑡=0 𝑙=0

1

𝑇 −1
∑︁

𝑇 −1
2 ∑︁

=

⃦⃦𝑔𝑙⃦⃦

𝜂𝑡−𝑙

1 − 𝜂 𝑙=0 𝑡=𝑙

≤ 1 𝑇∑−︁1 ⃦⃦𝑔𝑡⃦⃦2 . (1 − 𝜂)2
𝑡=0

Lemma 11. Let the sequence {𝑧𝑡}𝑡≥0 be deﬁned as 𝑧𝑡+1 d=ef 𝑥𝑡+1 − 1𝛾−𝜂𝜂 𝑣𝑡 with 0 ≤ 𝜂 < 1. Then

𝑇 −1

𝑇 −1

𝑇 −1

∑︁

E

[︀𝐺𝑡+1]︀

≤

(1

−

𝜃)

∑︁

E

[︀𝐺𝑡]︀

+

2𝛽𝐿̃︀2(1

+

4𝜂2)

∑︁

E

[︁ ⃦⃦𝑧𝑡+1

−

𝑧𝑡⃦⃦2]︁

,

𝑡=0

𝑡=0

𝑡=0

where 𝜃 = 1 − (1 − 𝛼)(1 + 𝑠), 𝛽 = (1 − 𝛼) (︀1 + 𝑠−1)︀ for any 𝑠 > 0.

Proof. Summing up the inequality in Lemma 1 (for EF21 estimator) for 𝑡 = 0, . . . , 𝑇 − 1, we have

𝑇 −1

𝑇 −1

𝑇 −1

∑︁

E

[︀𝐺𝑡+1]︀

≤

(1

−

𝜃)

∑︁

E

[︀𝐺𝑡]︀

+

𝛽𝐿̃︀2

∑︁

E

[︁ ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

.

(82)

𝑡=0

𝑡=0

𝑡=0

It remains to bound ∑︀𝑡𝑇=−01 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁. Notice that by deﬁnition of {𝑧𝑡}𝑡≥0, we have 𝑥𝑡+1 − 𝑥𝑡 = 𝑧𝑡+1 − 𝑧𝑡 + 𝛾𝜂 (︀𝑣𝑡 − 𝑣𝑡−1)︀ . 1−𝜂

56

EF21 with Bells & Whistles

Oct 6, 2021

Thus

𝑇 −1
∑︁ [︁

𝑇 −1

2]︁

∑︁ [︁

2]︁

2𝛾2𝜂2

𝑇 −1
∑︁

[︁

2]︁

⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦ ≤ 2 E ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦ +

E ⃦⃦𝑣𝑡 − 𝑣𝑡−1⃦⃦

E⃦

(1 − 𝜂)2

𝑡=0

𝑡=0

𝑡=0

𝑇 −1
∑︁ [︁

2]︁

2𝛾2𝜂2

𝑇 −1
∑︁

[︁

2]︁

= 2 E ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦ +

E ⃦⃦𝑔𝑡 − (1 − 𝜂)𝑣𝑡−1⃦⃦

(1 − 𝜂)2

𝑡=0

𝑡=0

𝑇 −1

∑︁

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

4𝛾2𝜂2

𝑇 −1
∑︁

[︁ ⃦

𝑡⃦2]︁

≤ 2 E ⃦𝑧 − 𝑧 ⃦ + (1 − 𝜂)2 E ⃦𝑔 ⃦

𝑡=0

𝑡=0

4𝛾2𝜂2

𝑇 −1
∑︁

+

(1 − 𝜂)2E [︀‖𝑣𝑡−1‖2]︀

(1 − 𝜂)2

𝑡=0

(𝑖)

𝑇 −1

∑︁

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

4𝛾2𝜂2

𝑇 −1
∑︁

[︁ ⃦

𝑡⃦2]︁

≤ 2 E ⃦𝑧 − 𝑧 ⃦ + (1 − 𝜂)2 E ⃦𝑔 ⃦

𝑡=0

𝑡=0

4𝛾2𝜂2

𝑇 −1
∑︁

[︁ ⃦

𝑡⃦2]︁

+ (1 − 𝜂)2 E ⃦𝑔 ⃦

𝑡=0

𝑇 −1

∑︁

[︁ ⃦ 𝑡+1

𝑡⃦2]︁

8𝛾2𝜂2

𝑇 −1
∑︁

[︁ ⃦

𝑡⃦2]︁

= 2 E ⃦𝑧 − 𝑧 ⃦ + (1 − 𝜂)2 E ⃦𝑔 ⃦

𝑡=0

𝑡=0

𝑇 −1

𝑇 −1

(𝑖𝑖)
=

2

∑︁

E

[︁ ⃦⃦𝑧𝑡+1

−

𝑧𝑡⃦⃦2]︁

+

8𝜂2

∑︁

E

[︁ ⃦⃦𝑧𝑡+1

−

𝑧𝑡⃦⃦2]︁

𝑡=0

𝑡=0

𝑇 −1

=

2(1

+

4𝜂2)

∑︁

E

[︁ ⃦⃦𝑧𝑡+1

−

𝑧𝑡⃦⃦2]︁

,

𝑡=0

where in (𝑖) we apply Lemma 10, and in (𝑖𝑖) Lemma 9 is utilized.

It remains to plug in the above inequality into (82)

Lemma 12. Let the sequence {𝑧𝑡}𝑡≥0 be generated as in Lemma 9, i.e., 𝑧𝑡+1 = 𝑧𝑡 − 1−𝛾 𝜂 𝑔𝑡, then for all 𝑡 ≥ 0
⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 ≤ 2𝐺𝑡 + 2(1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾2

with

𝐺𝑡

=

1 𝑛

∑︀𝑛
𝑖=1

‖∇𝑓𝑖

(𝑥𝑡

)

−

𝑔𝑖𝑡‖2.

Proof. Notice that for 𝛾 > 0 we have ∇𝑓 (𝑥𝑡) = ∇𝑓 (𝑥𝑡) − 𝑔𝑡 − 1−𝛾 𝜂 (𝑧𝑡+1 − 𝑧𝑡). Then

⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 ≤ 2 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 2 (1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾2

2 ∑𝑛︁ ⃦

𝑡

𝑡⃦2 2(1 − 𝜂)2 ⃦ 𝑡+1 𝑡⃦2

≤ 𝑛

⃦∇𝑓𝑖(𝑥 ) − 𝑔𝑖 ⃦ +

𝛾2

⃦𝑧 − 𝑧 ⃦ ,

𝑖=1

where the inequalities hold due to (118) with 𝑠 = 1, and (119).

H.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 11. Let Assumption 1 hold, and let the stepsize in Algorithm 6 be set as

(︃

√︂

)︃−1

(1 + 𝜂)𝐿 𝐿̃︀ 2𝛽

def

0 < 𝛾 < 2(1 − 𝜂)2 + 1 − 𝜂 𝜃 (1 + 4𝜂2) = 𝛾0,

(83)

57

EF21 with Bells & Whistles

Oct 6, 2021

where 0 ≤ 𝜂 < 1, 𝜃 = 1 − (1 − 𝛼)(1 + 𝑠), 𝛽 = (1 − 𝛼) (︀1 + 𝑠−1)︀, and 𝑠 > 0. Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

⎛

⎞

𝑇 −1

∑︁

[︁ ⃦

𝑡 ⃦2]︁

3𝛿0(1 − 𝜂) E [︀𝐺0]︀

1 3(1 − 𝜂)

E ⃦∇𝑓 (𝑥 )⃦ ≤

(︁

)︁ +

⎝2 +

(︁

)︁ ⎠ , (84)

𝛾

𝜃𝑇

2𝜆1 𝛾 1 − 𝛾

𝑡=0 𝑇 𝛾 1 − 𝛾0 𝛾0

where 𝜆1 d=ef 𝐿̃︀√︁ 2𝜃𝛽 (1 + 4𝜂2). If the stepsize is set to 0 < 𝛾 ≤ 𝛾0/2, then

⎛

⎞

𝑇 −1

∑︁

[︁ ⃦

𝑡 ⃦2]︁

6𝛿0(1 − 𝜂) E [︀𝐺0]︀

3(1 − 𝜂)

E ⃦∇𝑓 (𝑥 )⃦ ≤

+ ⎝2 + √︁

⎠ . (85)

𝑡=0

𝛾𝑇

𝑇𝜃

𝛾𝐿̃︀ 2𝛽 (1 + 4𝜂2)

𝜃

Proof. Consider the sequence 𝑧𝑡+1 d=ef 𝑥𝑡+1 − 1𝛾−𝜂𝜂 𝑣𝑡 with 0 ≤ 𝜂 < 1. Then Lemma 9 states that 𝑧𝑡+1 = 𝑧𝑡 − 1−𝛾 𝜂 𝑔𝑡. By 𝐿-smoothness of 𝑓 (·)

𝑓 (𝑧𝑡+1) − 𝑓 (𝑧𝑡) ≤ ⟨∇𝑓 (𝑧𝑡), 𝑧𝑡+1 − 𝑧𝑡⟩ + 𝐿 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 2

= ⟨∇𝑓 (𝑧𝑡) − 𝑔𝑡, 𝑧𝑡+1 − 𝑧𝑡⟩ + ⟨𝑔𝑡, 𝑧𝑡+1 − 𝑧𝑡⟩ + 𝐿 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 2

(=𝑖) ⟨∇𝑓 (𝑧𝑡) − 𝑔𝑡, 𝑧𝑡+1 − 𝑧𝑡⟩ − 1 − 𝜂 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 + 𝐿 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2

𝛾

2

= ⟨∇𝑓 (𝑧𝑡) − 𝑔𝑡, 𝑧𝑡+1 − 𝑧𝑡⟩ − (︂ 1 − 𝜂 − 𝐿 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾2

= ⟨∇𝑓 (𝑥𝑡) − 𝑔𝑡, 𝑧𝑡+1 − 𝑧𝑡⟩ + ⟨∇𝑓 (𝑧𝑡) − ∇𝑓 (𝑥𝑡), 𝑧𝑡+1 − 𝑧𝑡⟩

− (︂ 1 − 𝜂 − 𝐿 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾2

(≤𝑖𝑖) 1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 𝜆1 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 + 1 ⃦⃦∇𝑓 (𝑧𝑡) − ∇𝑓 (𝑥𝑡)⃦⃦2

2𝜆1

2

2𝜆2

+ 𝜆2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 − (︂ 1 − 𝜂 − 𝐿 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2

2

𝛾2

= 1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 1 ⃦⃦∇𝑓 (𝑧𝑡) − ∇𝑓 (𝑥𝑡)⃦⃦2

2𝜆1

2𝜆2

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 22 2

(𝑖≤𝑖𝑖) 1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 𝐿2 ⃦⃦𝑧𝑡 − 𝑥𝑡⃦⃦2

2𝜆1

2𝜆2

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 22 2

(≤𝑖𝑣) 1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 𝛾2𝜂2𝐿2 ⃦⃦𝑣𝑡−1⃦⃦2

2𝜆1

2𝜆2(1 − 𝜂)2

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 , 𝛾 22 2

where in (𝑖) Lemma 9 is applied, in (𝑖𝑖) the inequality (115) is applied twice for 𝜆1, 𝜆2 > 0, (𝑖𝑖𝑖) holds due to Assumption 1, and (𝑖𝑣) holds by deﬁnition of 𝑧𝑡 = 𝑥𝑡 − 1𝛾−𝜂𝜂 𝑣𝑡−1.

58

EF21 with Bells & Whistles

Oct 6, 2021

Summing up the above inequalities for 𝑡 = 0, . . . , 𝑇 − 1 (assuming 𝑣−1 = 0), we have

𝑓 (𝑧𝑇 ) ≤ 𝑓 (𝑧0) + 1 𝑇∑−︁1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 𝛾2𝜂2𝐿2 𝑇∑−︁1 ⃦⃦𝑣𝑡⃦⃦2

2𝜆1 𝑡=0

2𝜆2(1 − 𝜂)2 𝑡=0

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ 𝑇∑−︁1 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 22 2
𝑡=0

(≤𝑖) 𝑓 (𝑧0) + 1 𝑇∑−︁1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 + 𝛾2𝜂2𝐿2 𝑇∑−︁1 ⃦⃦𝑔𝑡⃦⃦2

2𝜆1 𝑡=0

2𝜆2(1 − 𝜂)4 𝑡=0

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ 𝑇∑−︁1 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 22 2
𝑡=0

(𝑖𝑖)

0

1 𝑇∑−︁1 ⃦

𝑡

𝑡⃦2

𝛾2𝜂2𝐿2 𝑇∑−︁1 (1 − 𝜂)2 ⃦ 𝑡+1 𝑡⃦2

= 𝑓 (𝑧 ) +

⃦∇𝑓 (𝑥 ) − 𝑔 ⃦ +

⃦𝑧 − 𝑧 ⃦

2𝜆1 𝑡=0

2𝜆2(1 − 𝜂)4 𝑡=0 𝛾2

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 )︂ 𝑇∑−︁1 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 22 2
𝑡=0

= 𝑓 (𝑧0) + 1 𝑇∑−︁1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑔𝑡⃦⃦2 2𝜆1 𝑡=0

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 −

𝜂2𝐿2

)︂ 𝑇 −1 ∑︁ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2

𝛾 2 2 2 2𝜆2(1 − 𝜂)2 𝑡=0

(𝑖≤𝑖𝑖) 𝑓 (𝑧0) + 1 𝑇∑−︁1 𝐺𝑡 2𝜆1 𝑡=0

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜆2 −

𝜂2𝐿2

)︂ 𝑇 −1 ∑︁ ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2

𝛾 2 2 2 2𝜆2(1 − 𝜂)2 𝑡=0

= 𝑓 (𝑧0) + 1 𝑇∑−︁1 𝐺𝑡 2𝜆1 𝑡=0

− (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜂𝐿 )︂ 𝑇∑−︁1 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2 𝛾 2 2 (1 − 𝜂)
𝑡=0

= 𝑓 (𝑧0) + 1 𝑇∑−︁1 𝐺𝑡 − (︂ 1 − 𝜂 − 𝐿 − 𝜆1 − 𝜂𝐿 )︂ 1 𝑇∑−︁1 𝑅𝑡,

2𝜆1 𝑡=0

𝛾

2 2 (1 − 𝜂) (1 − 𝜂)2

𝑡=0

where (𝑖) holds due to Lemma 10, in (𝑖𝑖) Lemma 9 is applied, in (𝑖𝑖𝑖) we apply ‖∇𝑓 (𝑥𝑡) − 𝑔𝑡‖2 ≤ 𝐺𝑡. Finally, in the last two steps we choose 𝜆2 = 1𝜂−𝐿𝜂 , and recall the deﬁnition 𝑅𝑡 = ‖𝛾𝑔𝑡‖2 = (1 − 𝜂)2 ⃦⃦𝑧𝑡+1 − 𝑧𝑡⃦⃦2.

Subtracting 𝑓 inf from both sides of the above inequality, taking expectation and using the notation 𝛿𝑡 = 𝑓 (𝑧𝑡) − 𝑓 inf , we get

[︀ 𝑇 ]︀

[︀ 0]︀ 1 𝑇∑−︁1 [︀ 𝑡]︀ (︂ 1 − 𝜂 𝐿 𝜆1

𝜂𝐿 )︂ 1 𝑇∑−︁1 [︀ 𝑡]︀

E 𝛿 ≤ E 𝛿 + 2𝜆1 E 𝐺 − 𝛾 − 2 − 2 − (1 − 𝜂) (1 − 𝜂)2 E 𝑅 .

𝑡=0

𝑡=0

(86)

By Lemma 11, we have

𝑇 −1
∑︁

𝑇 −1
∑︁

2𝛽𝐿̃︀2(1

+

4𝜂2)

𝑇 −1
∑︁

[︀𝐺𝑡+1]︀ ≤ (1 − 𝜃) E [︀𝐺𝑡]︀ +

E [︀𝑅𝑡]︀ .

(87)

E

(1 − 𝜂)2

𝑡=0

𝑡=0

𝑡=0

59

EF21 with Bells & Whistles

Oct 6, 2021

Next, we are going to add (86) with a 2𝜃1𝜆1 multiple of (87). First, let us "forget", for a moment, about all the terms involving 𝑅𝑡 and denote their sum appearing on the right hand side by ℛ, then

1

𝑇 −1
∑︁

1

𝑇 −1
∑︁

1

𝑇 −1
∑︁

[︀𝛿𝑇 ]︀ +

E [︀𝐺𝑡+1]︀ ≤ E [︀𝛿0]︀ +

E [︀𝐺𝑡]︀ + (1 − 𝜃)

E [︀𝐺𝑡]︀ + ℛ

E

2𝜃𝜆1

2𝜆1

2𝜆1

𝑡=0

𝑡=0

𝑡=0

1

𝑇 −1
∑︁

= E [︀𝛿0]︀ +

E [︀𝐺𝑡]︀ + ℛ.

2𝜃𝜆1 𝑡=0

Canceling out the same terms in both sides of the above inequality, we get

E [︀𝛿𝑇 ]︀ + 1 E [︀𝐺𝑇 ]︀ ≤ E [︀𝛿0]︀ + 1 E [︀𝐺0]︀ + ℛ,

2𝜃𝜆1

2𝜃𝜆1

where ℛ d=ef − (︁ 1−𝛾 𝜂 − 𝐿2 (︁1 + 12−𝜂𝜂 )︁ − 𝜆21 − 𝛽𝐿̃︀2(𝜃1𝜆+14𝜂2) )︁ (1−1𝜂)2 ∑︀𝑇𝑡=−01 E [𝑅𝑡].

√︁ Now choosing 𝜆1 = 𝐿̃︀ 2𝜃𝛽 (1 + 4𝜂2) and using the deﬁnition of 𝛾0 given by (83), i.e., 𝛾0 d=ef (︂ √︁ )︂−1
2((11+−𝜂𝜂))𝐿2 + 1−𝐿̃︀𝜂 2𝜃𝛽 (1 + 4𝜂2) , we have

(︃

)︃

1 − 𝜂 − 𝐿 (︂1 + 2𝜂 )︂ − 𝜆1 − 𝛽𝐿̃︀2(1 + 4𝜂2) 1

𝛾2

1−𝜂

2

𝜃𝜆1

(1 − 𝜂)2

(︃ 1 − 𝜂 𝐿 (︂

2𝜂 )︂

√︂ 2𝛽

)︃ 1

= 𝛾 − 2 1 + 1 − 𝜂 − 𝐿̃︀ 𝜃 (1 + 4𝜂2) (1 − 𝜂)2

(︃

√︂

)︃

= 1 − 𝐿 1 + 𝜂 − 𝐿̃︀ 2𝛽 (1 + 4𝜂2) 1

𝛾 2 (1 − 𝜂)2 1 − 𝜂 𝜃

1−𝜂

(︂ 1 1 )︂ 1

=−

.

𝛾 𝛾0 1 − 𝜂

Then

0 ≤ E [︀Φ𝑇 ]︀

d=ef

[︂ E 𝛿𝑇 +

1

]︂ 𝐺𝑇

2𝜃𝜆1

[︂

1

]︂ (︂ 1

1 )︂

1

𝑇 −1
∑︁

≤ E 𝛿0 +

𝐺0 − −

E [︀𝑅𝑡]︀

2𝜃𝜆1

𝛾 𝛾0 1 − 𝜂 𝑡=0

(︂ 1

1 )︂

1

𝑇 −1
∑︁

= E [︀Φ0]︀ − −

E [︀𝑅𝑡]︀ .

𝛾 𝛾0 1 − 𝜂 𝑡=0

After rearranging, we get

1 𝑇∑−︁1 [︀𝑅𝑡]︀ ≤ E [︀Φ0]︀ (1 − 𝜂) .

𝛾2 E
𝑡=0

(︁

)︁

𝛾 1− 𝛾

𝛾0

Summing the result of Lemma 12 over 𝑡 = 0, . . . , 𝑇 − 1 and applying expectation, we get

𝑇 −1

∑︁

[︁ ⃦

𝑡 ⃦2]︁ 𝑇∑−︁1 [︀ 𝑡]︀ 2 𝑇∑−︁1 [︀ 𝑡]︀

E ⃦∇𝑓 (𝑥 )⃦ ≤ 2 E 𝐺 + 𝛾2 E 𝑅 .

𝑡=0

𝑡=0

𝑡=0

Due to Lemma 11, the conditions of Lemma 18 hold with 𝐶 d=ef 2𝛽𝐿̃︀2 (11+−4𝜂𝜂)22 , 𝑠𝑡 = E [𝐺𝑡], 𝑟𝑡 = E [𝑅𝑡], thus

60

EF21 with Bells & Whistles

Oct 6, 2021

𝑇 −1
∑︁

E [︀𝐺0]︀

𝐶

𝑇 −1
∑︁

[︀𝐺𝑡]︀ ≤

+

E [︀𝑅𝑡]︀ .

E

𝜃

𝜃

𝑡=0

𝑡=0

Combining the above inequalities, we can continue with

𝑇 −1

∑︁

[︁ ⃦

𝑡 ⃦2]︁

𝑇∑−︁1 [︀ 𝑡]︀ 2 𝑇∑−︁1 [︀ 𝑡]︀

E ⃦∇𝑓 (𝑥 )⃦ ≤ 2 E 𝐺 + 𝛾2 E 𝑅

𝑡=0

𝑡=0

𝑡=0

2E [︀𝐺0]︀ (︂

𝛾2𝐶 )︂

1

𝑇 −1
∑︁

≤

+ 2+

E [︀𝑅𝑡]︀

𝜃

𝜃 𝛾2

𝑡=0

2E [︀𝐺0]︀ (︂ 𝛾2𝐶 )︂ E [︀Φ0]︀ (1 − 𝜂)

≤

+ 2+

(︁

)︁ .

𝜃 𝜃 𝛾 1 − 𝛾𝛾0

(︁

√︁ )︁−1

Note that for 𝛾 < 𝛾0 = 2((11+−𝜂𝜂))𝐿2 +

𝐶 𝜃

, we have

𝛾2𝐶

𝐶 𝜃

<

2 ≤ 1.

(88)

(︁

√︁ )︁

𝜃 (1+𝜂)𝐿2 + 𝐶

2(1−𝜂)

𝜃

Thus

𝑇 −1

∑︁

[︁ ⃦

𝑡 ⃦2]︁

2E [︀𝐺0]︀ 3E [︀Φ0]︀ (1 − 𝜂)

E ⃦∇𝑓 (𝑥 )⃦ ≤

+ (︁

)︁

𝜃

𝛾 1− 𝛾

𝑡=0

𝛾0

⎛

⎞

3𝛿0(1 − 𝜂) E [︀𝐺0]︀

1 3(1 − 𝜂)

= (︁

)︁ +

⎝2 +

(︁

)︁ ⎠ ,

𝛾 1 − 𝛾𝛾0 𝜃

2𝜆1 𝛾 1 − 𝛾𝛾0

√︁ where 𝜆1 = 𝐿̃︀ 2𝜃𝛽 (1 + 4𝜂2).

Corollary 16. Let assumptions of Theorem 11 hold,

𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

(︃

√︂

)︃−1

𝛾 = (1 + 𝜂)𝐿 + 𝐿̃︀ 2𝛽 (1 + 4𝜂2) .

2(1 − 𝜂)2 1 − 𝜂 𝜃

Then, after 𝑇

iterations/communication

rounds

of

EF21-HB

we

have

E

[︁ ⃦⃦∇𝑓 (𝑥ˆ𝑇

⃦2]︁ )⃦

≤

𝜀2.

It

requires

(︃

)︃

𝐿̃︀𝛿0 (︂ 1 1 )︂

𝑇 = #grad = 𝒪 𝜀2 𝛼 + 1 − 𝜂

(89)

iterations/communications rounds/gradint computations at each node.

Proof. Notice that by using 𝐿 ≤ 𝐿̃︀, 𝜂 < 1 and Lemma 17, we have

√︂

√︂

(1 + 𝜂)𝐿 𝐿̃︀ +

2𝛽 (1 + 4𝜂2) ≤

𝐿̃︀

𝐿̃︀ 10𝛽

+

2(1 − 𝜂)2 1 − 𝜂 𝜃

(1 − 𝜂)2 1 − 𝜂 𝜃

(︃

√ )︃

𝐿̃︀

1 2 10

≤

+

.

1−𝜂 1−𝜂 𝛼

61

EF21 with Bells & Whistles

Oct 6, 2021

Using the above inequality, (85), and (83), we get

(︃

√ )︃

6𝛿0(1 − 𝜂) 6𝛿0(1 − 𝜂) 𝐿̃︀

1 2 10

#grad = 𝑇 ≤ 𝛾𝜀2 ≤

𝜀2

+ 1−𝜂 1−𝜂 𝛼

(︃

√ )︃

6𝐿̃︀𝛿0 1 2 10

≤ 𝜀2 1 − 𝜂 + 𝛼 .

62

EF21 with Bells & Whistles

Oct 6, 2021

I COMPOSITE CASE

Now we focus on solving a composite optimization problem

def

1

𝑛
∑︁

min Φ(𝑥) =

𝑓𝑖(𝑥) + 𝑟(𝑥),

(90)

𝑥∈R𝑑

𝑛

𝑖=1

where each 𝑓𝑖(·) is 𝐿𝑖-smooth (possibly non-convex), 𝑟(·) is convex, and Φinf = inf𝑥∈R𝑑 Φ(𝑥) > −∞. This is a standard and important generalization of setting (1). Namely, it includes three special
cases.

• Smooth unconstrained optimization. Set 𝑟 ≡ 0, then we recover the initially stated problem formulation (1).
• Smooth optimization over convex set. Let 𝑟 = 𝛿𝑄 (indicator function of the set 𝑄), where 𝑄 is a nonempty closed convex set. Then (90) reduces to the problem of minimizing ﬁnite a sum of smooth (possibly non-convex) functions over a nonempty closed convex set

{︃

1

𝑛
∑︁

}︃

min 𝑥∈𝑄 𝑛

𝑓𝑖(𝑥) .

𝑖=1

• 𝑙1-regularized optimization. Choose 𝑟(𝑥) = 𝜆‖𝑥‖1 with 𝜆 > 0, then (90) amounts to the 𝑙1-regularized (also known as LASSO) problem

{︃

1

𝑛
∑︁

}︃

min

𝑓𝑖(𝑥) + 𝜆‖𝑥‖1 .

𝑥∈R𝑑 𝑛

𝑖=1

For any 𝛾 > 0, 𝑥 ∈ R𝑑, deﬁne a proximal mapping of function 𝑟(·) (prox-operator) as

prox

{︂ (𝑥) d=ef arg min 𝑟(𝑦) +

1

‖𝑦

−

}︂ 𝑥‖2 .

(91)

𝛾𝑟 𝑦∈R𝑑

2𝛾

Throughout this section, we assume that the master node can efﬁciently compute prox-operator at every iteration. This is a reasonable assumption, and in many cases (choices of 𝑟(·)) appearing in applications, there exists an analytical solution of (91), or its computation is cheap compared to the aggregation step.
To evaluate convergence in composite case, we deﬁne the generalized gradient mapping at a point 𝑥 ∈ R𝑑 with a parameter 𝛾

def 1 (︀

)︀

𝒢𝛾(𝑥) = 𝛾 𝑥 − prox𝛾𝑟(𝑥 − 𝛾∇𝑓 (𝑥)) .

(92)

One can verify that the above quantity is a well-deﬁned evaluation metric (Beck, 2017). Namely, for any 𝑥* ∈ R𝑑, it holds that 𝒢𝛾(𝑥) = 0 if and only if 𝑥* is a stationary point of (90), and in a special
case when 𝑟 ≡ 0, we have 𝒢𝛾(𝑥) = ∇𝑓 (𝑥).

Notations for this section: in this section we re-deﬁne 𝛿𝑡 d=ef Φ (𝑥𝑡) − Φinf

Lemma 13 (Gradient mapping bound).

Let 𝑥𝑡+1

def
=

prox𝛾 𝑟 (𝑥𝑡

−

𝛾𝑣𝑡),

then

E

[︁ ⃦ ⃦𝒢𝛾

(︀𝑥𝑡)︀⃦⃦2]︁

≤

2

[︁ E ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+

[︁ 2E ⃦⃦𝑣𝑡

−

∇

𝑓

(

𝑥

𝑡

)

⃦2 ⃦

]︁

.

(93)

𝛾2

63

EF21 with Bells & Whistles

Oct 6, 2021

Proof.

E [︁⃦⃦𝒢𝛾 (︀𝑥𝑡)︀⃦⃦2]︁ = 𝛾12 E [︁⃦⃦𝑥𝑡 − prox𝛾𝑟(𝑥𝑡 − 𝛾∇𝑓 (𝑥𝑡))⃦⃦2]︁

≤ 𝛾22 E [︁⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2]︁ + 𝛾22 E [︁⃦⃦𝑥𝑡+1 − prox𝛾𝑟(𝑥𝑡 − 𝛾∇𝑓 (𝑥𝑡))⃦⃦2]︁

=

2

[︁ E ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

𝛾2

+ 𝛾22 E [︁⃦⃦prox𝛾𝑟(𝑥𝑡 − 𝛾𝑣𝑡) − prox𝛾𝑟(𝑥𝑡 − 𝛾∇𝑓 (𝑥𝑡))⃦⃦2]︁

≤

2

[︁ E ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

𝛾2

+

2

[︁ E ⃦⃦(𝑥𝑡

−

𝛾𝑣𝑡)

−

(𝑥𝑡

−

𝛾∇𝑓 (𝑥𝑡))⃦⃦2]︁

𝛾2

=

2

[︁ E ⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2]︁

+

[︁ 2E ⃦⃦𝑣𝑡

−

∇

𝑓

(

𝑥

𝑡

))

⃦2 ⃦

]︁

,

(94)

𝛾2

where in the last inequality we apply non-expansiveness of prox-operator.

Lemma 14.

Let 𝑥𝑡+1

def
=

prox𝛾 𝑟 (𝑥𝑡

−

𝛾𝑣𝑡),

then

for

any

𝜆

>

0,

Φ (︀𝑥𝑡+1)︀ ≤ Φ (︀𝑥𝑡)︀ + 1 ⃦⃦𝑣𝑡 − ∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 − 𝜆 )︂ ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 . (95)

2𝜆

𝛾22

Proof. Deﬁne 𝑟˜(𝑥) d=ef 𝑟(𝑥)+ 21𝛾 ‖𝑥 − 𝑥𝑡 + 𝛾𝑣𝑡‖2, and note that 𝑥𝑡+1 = arg min𝑥∈R𝑑 {𝑟˜(𝑥)}. Since 𝑟˜(·) is 1/𝛾 - strongly convex, we have
𝑟˜(𝑥𝑡) ≥ 𝑟˜(𝑥𝑡+1) + 1 ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 , 2𝛾

𝑟(𝑥𝑡) + 1 ⃦⃦𝛾𝑣𝑡⃦⃦2 ≥ 𝑟(𝑥𝑡+1) + 1 ⃦⃦𝑥𝑡+1 − 𝑥𝑡 + 𝛾𝑣𝑡⃦⃦2 + 1 ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 .

2𝛾

2𝛾

2𝛾

Thus 𝑟(𝑥𝑡+1) − 𝑟(𝑥𝑡) ≤ − 1 ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 − ⟨𝑣𝑡, 𝑥𝑡+1 − 𝑥𝑡⟩. (96) 𝛾

By 𝐿 smoothness of 𝑓 (·), 𝑓 (︀𝑥𝑡+1)︀ − 𝑓 (︀𝑥𝑡)︀ ≤ ⟨︀∇𝑓 (︀𝑥𝑡)︀ , 𝑥𝑡+1 − 𝑥𝑡⟩︀ + 𝐿 ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 . (97) 2

Summing up (97) with (96) we obtain

Φ (︀𝑥𝑡+1)︀ − Φ (︀𝑥𝑡)︀ ≤ ⟨∇𝑓 (𝑥𝑡) − 𝑣𝑡, 𝑥𝑡+1 − 𝑥𝑡⟩ − (︂ 1 − 𝐿 )︂ ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 𝛾2

≤ 1 ⃦⃦∇𝑓 (𝑥𝑡) − 𝑣𝑡⃦⃦2 − (︂ 1 − 𝐿 − 𝜆 )︂ ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 .

2𝜆

𝛾22

We are now ready to present EF21-Prox and provide its convergence guarantees in general non-convex case.
64

EF21 with Bells & Whistles

Oct 6, 2021

I.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Algorithm 7 EF21-Prox

1:

Input:

starting point 𝑥0

∈ R𝑑; 𝑔𝑖0

∈ R𝑑

for 𝑖 =

1, . . . , 𝑛 (known by nodes); 𝑔0

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖0

(known by master); learning rate 𝛾 > 0

2: for 𝑡 = 0,1, 2, . . . , 𝑇 − 1 do

3: Master computes 𝑥𝑡+1 = prox𝛾𝑟 (𝑥𝑡 − 𝛾𝑔𝑡)

4: for all nodes 𝑖 = 1, . . . , 𝑛 in parallel do

5:

Compress 𝑐𝑡𝑖 = 𝒞(∇𝑓𝑖(𝑥𝑡+1) − 𝑔𝑖𝑡) and send 𝑐𝑡𝑖 to the master

6:

Update local state 𝑔𝑖𝑡+1 = 𝑔𝑖𝑡 + 𝑐𝑡𝑖

7: end for

8:

Master

computes

𝑔𝑡+1

=

1 𝑛

∑︀𝑛
𝑖=1

𝑔𝑖𝑡+1

via

𝑔𝑡+1

=

𝑔𝑡

+

1 𝑛

∑︀𝑛
𝑖=1

𝑐𝑡𝑖

9: end for

10: Output: 𝑥ˆ𝑇 chosen uniformly from {𝑥𝑡}𝑡∈[𝑇 ]

Theorem 12. Let Assumption 1 hold, 𝑟(·) be convex and Φinf = inf𝑥∈R𝑑 Φ(𝑥) > −∞. Set the

stepsize in Algorithm 7 as

(︃

√︂ )︃−1

𝐿

𝛽

def

0 < 𝛾 < 2 + 𝐿̃︀ 𝜃 = 𝛾0, (98)

√︁

where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1

−

(1

−

𝛼)(1

+

𝑠),

𝛽

=

(1

−

𝛼)

(︀1

+

𝑠−1)︀

for

any

𝑠

>

0.

Fix 𝑇 ≥ 1 and let 𝑥ˆ𝑇 be chosen from the iterates 𝑥0, 𝑥1, . . . , 𝑥𝑇 −1 uniformly at random. Then

⎛ √︃ ⎞

[︁ ⃦

𝑇 ⃦2]︁

4 (︀Φ0 − Φinf )︀ 2E [︀𝐺0]︀

1 1𝜃

E ⃦𝒢𝛾(𝑥ˆ )⃦ ≤

(︁

)︁ +

⎝1 + (︁

)︁

⎠ . (99)

𝑇 𝛾 1 − 𝛾𝛾0 𝜃𝑇

𝛾 1 − 𝛾𝛾0 𝐿̃︀ 𝛽

If the stepsize is set to 0 < 𝛾 ≤ 𝛾0/2, then

[︁

2]︁ 8 (︀Φ0 − Φinf )︀ 2E [︀𝐺0]︀ (︃

√︃ )︃ 2𝜃

E

⃦ ⃦𝒢

𝛾

(𝑥ˆ

𝑇

)

⃦ ⃦

≤

+

1+

.

𝛾𝑇

𝜃𝑇

𝛾𝐿̃︀ 𝛽

(100)

Proof. First, let us apply Lemma 14 with 𝑣𝑡 = 𝑔𝑡, 𝜆 > 0

Φ (︀𝑥𝑡+1)︀ ≤ Φ (︀𝑥𝑡)︀ + 1 ⃦⃦𝑔𝑡 − ∇𝑓 (︀𝑥𝑡)︀⃦⃦2 − (︂ 1 − 𝐿 − 𝜆 )︂ ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 .

2𝜆

𝛾22

(101)

Subtract Φinf from both sides, take expectation, and deﬁne 𝛿𝑡 = Φ (𝑥𝑡) − Φinf , 𝐺𝑡 =

1 𝑛

∑︀𝑛
𝑖=1

‖𝑔𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)‖2,

𝑅𝑡

=

⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2,

then

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − (︂ 1 − 𝐿 − 𝜆 )︂ E [︀𝑅𝑡]︀ + 1 E [︀𝐺𝑡]︀ .

𝛾22

2𝜆

(102)

Note that the proof of Lemma 1 does not rely on the update rule for 𝑥𝑡+1, but only on the way the estimator 𝑔𝑖𝑡+1 is constructed. Therefore, (14) also holds for the composite case

E [︀𝐺𝑡+1]︀ ≤ (1 − 𝜃)E [︀𝐺𝑡]︀ + 𝛽𝐿̃︀2E [︀𝑅𝑡]︀ .

(103)

Adding (102) with a 2𝜃1𝜆 multiple of (103) , we obtain

E [︀𝛿𝑡+1]︀ + 1 E [︀𝐺𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ + 1 E [︀𝐺𝑡]︀ + 1 − 𝜃 E [︀𝐺𝑡]︀ − (︂ 1 − 𝐿 − 𝜆 )︂ E [︀𝑅𝑡]︀

2𝜃𝜆

2𝜆

2𝜃𝜆

𝛾22

+ 1 𝛽𝐿̃︀2E [︀𝑅𝑡]︀ 2𝜃𝜆

=

E [︀𝛿𝑡]︀ +

1 E [︀𝐺𝑡]︀ − (︂ 1 − 𝐿 − 𝜆 −

𝛽

)︂ 𝐿2 E [︀𝑅𝑡]︀ .

̃︀

2𝜃𝜆

𝛾 2 2 2𝜃𝜆

65

EF21 with Bells & Whistles

Oct 6, 2021

By summing up inequalities for 𝑡 = 0, . . . , 𝑇 − 1, we arrive at

1

1

(︂ 1 𝐿 𝜆

𝛽

)︂ 𝑇 −1 ∑︁

0 ≤ E [︀𝛿𝑇 ]︀ + E [︀𝐺𝑇 ]︀ ≤ 𝛿0 + E [︀𝐺0]︀ − − − − 𝐿̃︀2

E [︀𝑅𝑡]︀ .

2𝜃𝜆 2𝜃𝜆 𝛾 2 2 2𝜃𝜆 𝑡=0

Thus

𝑇 −1
∑︁

(︂

1

)︂ (︂ 1 𝐿 𝜆 𝛽 )︂−1

E [︀𝑅𝑡]︀ ≤ 𝛿0 + E [︀𝐺0]︀

− − − 𝐿̃︀2

𝑡=0 2𝜃𝜆 𝛾 2 2 2𝜃𝜆

(︃

√︃

)︃ (︃

√︂ )︃−1

= 𝛿0 + 1 𝜃 E [︀𝐺0]︀ 1 − 𝐿 − 𝛽 𝐿̃︀2

2𝜃 𝛽𝐿̃︀2

𝛾2

𝜃

= 𝛾2𝐹0𝐵.

(104)

√︁ where in the ﬁrst equality we choose 𝜆 = 𝛽𝜃 𝐿̃︀2, and in the second we deﬁne 𝐹 0 d=ef 𝛿0 +

1 √︁

𝜃

E [︀𝐺0]︀,

𝐵

d=ef

(︂ 𝛾

−

𝐿𝛾2

−

√︁ )︂−1 𝛽 𝐿2𝛾2

=

(︁ 𝛾

−

𝛾2 )︁−1.

2𝜃 𝛽𝐿̃︀2

2

𝜃 ̃︀

𝛾0

By Lemma 13 with 𝑣𝑡 = 𝑔𝑡 we have

E

[︁ ⃦ ⃦𝒢𝛾

(︀𝑥ˆ𝑇

)︀⃦2]︁ ⃦

=

1 𝑇∑−︁1 E [︁⃦⃦𝒢𝛾 (︀𝑥𝑡)︀⃦⃦2]︁

𝑇

𝑡=0

2

𝑇 −1
∑︁

2

𝑇 −1
∑︁

≤

E [︀𝑅𝑡]︀ +

E [︀𝐺𝑡]︀

𝛾2𝑇

𝑇

𝑡=0

𝑡=0

(𝑖)

2

𝑇 −1
∑︁

2 E [︀𝐺0]︀

2

𝛽𝐿̃︀2

𝑇 −1
∑︁

≤

E [︀𝑅𝑡]︀ +

+

E [︀𝑅𝑡]︀

𝛾2𝑇

𝑇𝜃

𝑇𝜃

𝑡=0

𝑡=0

(≤𝑖𝑖) 2𝐹 0𝐵 + 2 E [︀𝐺0]︀ + 2 𝛽𝐿̃︀2 𝛾2𝐹 0𝐵

𝑇

𝑇𝜃

𝑇𝜃

(︃ 2𝐹 0𝐵

)︃ 𝛾2𝛽𝐿̃︀2

2 E [︀𝐺0]︀

=

1+

+

𝑇

𝜃

𝑇𝜃

2𝐹 0

(︃

)︃ 𝛾2𝛽𝐿̃︀2

2 E [︀𝐺0]︀

=

(︁

)︁ 1 +

+

,

𝑇 𝛾 1 − 𝛾𝛾0

𝜃

𝑇𝜃

where in (𝑖) we apply Lemma 18 with 𝐶 d=ef 𝛽𝐿̃︀2, 𝑠𝑡 d=ef E [𝐺𝑡], 𝑟𝑡 d=ef E [𝑅𝑡]. (𝑖𝑖) is due to (104).

(︂ √︁ )︂−1 Note that for 𝛾 < 𝐿2 + 𝛽𝜃 𝐿̃︀ , we have

𝛾2𝛽𝐿̃︀2 <

𝛽𝜃 𝐿̃︀2 ≤ 1.

𝜃

(︂ √︁ )︂2

𝐿 + 𝛽 𝐿̃︀

2

𝜃

(105)

Thus

[︁ ⃦

𝑇 ⃦2]︁

4𝐹 0

2 E [︀𝐺0]︀

E ⃦𝒢𝛾(𝑥ˆ )⃦ ≤

(︁

)︁ +

𝛾

𝑇𝜃

𝑇 𝛾 1 − 𝛾0

4𝛿0

2E [︀𝐺0]︀

2E [︀𝐺0]︀

√︃ 1𝜃

=

(︁

)︁ +

+ (︁

)︁

.

𝑇 𝛾 1 − 𝛾𝛾0

𝜃𝑇 𝑇 𝛾 1 − 𝛾𝛾0 𝜃 𝛽𝐿̃︀2

Set 𝛾 ≤ 𝛾0/2, then the bound simpliﬁes to

[︁

2]︁

8𝛿0 2E [︀𝐺0]︀ (︃

√︃ )︃ 2𝜃

E

⃦ ⃦𝒢

𝛾

(𝑥ˆ

𝑇

⃦ )⃦

≤

+

1+

.

𝛾𝑇

𝜃𝑇

𝛾 𝛽𝐿̃︀2

(106)

66

EF21 with Bells & Whistles

Oct 6, 2021

Corollary 17. Let assumptions of Theorem 12 hold,
𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛, (︁ √︀ )︁−1
𝛾 = 𝐿 + 2𝐿̃︀ 𝛽/𝜃 .

Then,

after

𝑇

iterations/communication

rounds

of

EF21-Prox

we

have

E

[︁

⃦ ⃦∇

𝑓

(𝑥ˆ

𝑇

)⃦⃦2

]︁

≤

𝜀2.

It

requires

(︃ )︃ 𝐿̃︀𝛿0
#grad = 𝒪 𝛼𝜀2 ,

√︁

where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝛿0

=

Φ(𝑥0)

−

Φ𝑖𝑛𝑓 .

Proof. The proof is the same as for Corollary 2.

I.2 CONVERGENCE UNDER POLYAK-ŁOJASIEWICZ CONDITION

In order to extend the analysis of Polyak-Łojasiewicz functions to composite optimization, we use the following Assumption 5 from (Li & Li, 2018; Wang et al., 2018).
Assumption 5 (Polyak-Łojasiewicz). There exists 𝜇 > 0 such that

‖𝒢𝛾(𝑥)‖2 ≥ 2𝜇 (Φ(𝑥) − Φ(𝑥⋆))

for all 𝑥 ∈ R𝑑, where 𝑥⋆ = arg min𝑥 Φ(𝑥).
Theorem 13. Let Assumptions 1 and 5 hold, 𝑟(·) be convex and Φinf = inf𝑥∈R𝑑 Φ(𝑥) > −∞. Set the stepsize in Algorithm 7 as

⎧ (︃

√︂ )︃−1

⎫

⎨

2𝛽

𝜃⎬

𝛾 ≤ min 𝐿 + 2𝐿̃︀

,

.

⎩

𝜃

√︁ 𝜇 + 𝜃𝐿̃︀ 2𝛽 ⎭

𝜃

(107)

Let

Ψ𝑡

def
=

Φ(𝑥𝑡)

−

Φ(𝑥⋆)

+

√︁ 1 𝐺𝑡 with 𝜆 = 2𝛽 𝐿̃︀. Then for any 𝑇

≥ 0, we have

𝜃𝜆

𝜃

E

[︀Ψ𝑇

]︀

≤

(︁ 1

−

𝛾𝜇 )︁𝑇

E

[︀Ψ0]︀

,

2

√︁

where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝜃

=

1

−

(1

−

𝛼)(1

+

𝑠),

𝛽

=

(1

−

𝛼)

(︀1

+

𝑠−1)︀

for

any

𝑠

>

0.

(108)

Proof. We start as in the previous proof, but subtract Φ(𝑥⋆) from both sides of (101) and deﬁne

𝛿𝑡

d=ef

Φ (𝑥𝑡)

−

Φ (𝑥⋆)

.

Recall

that

𝐺𝑡

=

1 𝑛

∑︀𝑛
𝑖=1

‖𝑔𝑖𝑡

−

∇𝑓𝑖(𝑥𝑡)‖2,

𝑅𝑡

=

⃦⃦𝑥𝑡+1

−

𝑥𝑡⃦⃦2.

Then

E [︀𝛿𝑡+1]︀ ≤ E [︀𝛿𝑡]︀ − (︂ 1 − 𝐿 − 𝜆 )︂ E [︀𝑅𝑡]︀ + 1 E [︀𝐺𝑡]︀ .

𝛾22

2𝜆

By Lemma 1, we have

E [︀𝐺𝑡+1]︀ ≤ (1 − 𝜃)E [︀𝐺𝑡]︀ + 𝛽𝐿̃︀2E [︀𝑅𝑡]︀ .

Then by adding (109) with a 𝜃1𝜆 multiple of (110) we obtain

67

(109) (110)

EF21 with Bells & Whistles

Oct 6, 2021

E [︀𝛿𝑡+1]︀ + 1 E [︀𝐺𝑡+1]︀

≤

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃

+

𝜃 )︂ E [︀𝐺𝑡]︀

−

(︂ 1

−

𝐿

−

𝜆 )︂ E [︀𝑅𝑡]︀

𝜃𝜆

𝜃𝜆

2

𝛾22

+ 1 𝛽𝐿̃︀2E [︀𝑅𝑡]︀ 𝜃𝜆

=

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃 )︂ E [︀𝐺𝑡]︀

−

(︂ 1

−

𝐿

−

𝜆

−

𝛽

)︂ 𝐿2 E [︀𝑅𝑡]︀

̃︀

𝜃𝜆

2

𝛾 2 2 𝜃𝜆

(︃

√︂ )︃

(𝑖)
=

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃

)︂

E

[︀𝐺𝑡]︀

−

1𝐿 −−

2𝛽 𝐿

E [︀𝑅𝑡]︀

̃︀

𝜃𝜆

2

𝛾2

𝜃

(𝑖𝑖)
≤

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃

)︂

E

[︀𝐺𝑡]︀

−

1 E [︀𝑅𝑡]︀ ,

𝜃𝜆

2

2𝛾

(111)

√︁ where in (𝑖) we choose 𝜆 = 2𝜃𝛽 𝐿̃︀2, (𝑖𝑖) is due to the stepsize choice (the ﬁrst term in minimum).
Next, combining Assumption 5 with Lemma 13, we have

2𝜇𝛿𝑡 = 2𝜇 (︀Φ(𝑥𝑡) − Φ(𝑥⋆))︀ ≤ ⃦⃦𝒢𝛾(𝑥𝑡)⃦⃦2 ≤ 2 𝑅𝑡 + 2𝐺𝑡, 𝛾2

and

−𝑅𝑡 ≤ −𝜇𝛾2𝛿𝑡 + 𝛾2𝐺𝑡.

(112)

Thus (111) can be further bounded as

E [Ψ]

=

[︂ E 𝛿𝑡+1 +

1

]︂ 𝐺𝑡+1

𝜃𝜆

≤

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃

)︂

E

[︀𝐺𝑡]︀

−

1 E [︀𝑅𝑡]︀

𝜃𝜆

2

2𝛾

(112)
≤

E [︀𝛿𝑡]︀ +

1

(︂ 1

−

𝜃

)︂

E

[︀𝐺𝑡]︀

−

𝛾𝜇 E

[︀𝛿𝑡]︀

+

𝛾

E

[︀𝐺𝑡]︀

𝜃𝜆

2

2

2

=

(︁ 1

−

𝛾𝜇 )︁

E

[︀𝛿𝑡]︀

+

1

(︂ 1

−

𝜃

+

𝛾𝜃𝜆 )︂ E [︀𝐺𝑡]︀

2

𝜃𝜆

22

≤

(︁ 1

−

𝛾𝜇 )︁

E

[︂ 𝛿𝑡

+

1

]︂ 𝐺𝑡 ,

2

𝜃𝜆

(113)

where the last inequality follows by our assumption on the stepsize (the second term in minimum). It remains to unroll the recurrence.

Corollary 18. Let assumptions of Theorem 13 hold,

𝑔𝑖0 = ∇𝑓𝑖(𝑥0), 𝑖 = 1, . . . , 𝑛,

⎧ (︃

√︂ )︃−1

⎫

⎨

2𝛽

𝜃⎬

𝛾 = min 𝐿 + 2𝐿̃︀

,

.

⎩

𝜃

√︁ 𝜇 + 𝜃𝐿̃︀ 2𝛽 ⎭

𝜃

Then, after 𝑇 iterations/communication rounds of EF21-Prox we have E [︀𝑓 (𝑥𝑇 ) − 𝑓 (𝑥⋆)]︀ ≤ 𝜀. It requires

(︃

)︃

𝜇 + 𝐿̃︀ (︂ 𝛿0 )︂

𝑇 = #grad = 𝒪

log

𝛼𝜇

𝜀

(114)

√︁

iterations/communications rounds/gradint computations at each node, where 𝐿̃︀ =

1 𝑛

∑︀𝑛
𝑖=1

𝐿2𝑖 ,

𝛿0 = Φ(𝑥0) − Φ𝑖𝑛𝑓 .

68

EF21 with Bells & Whistles

Proof. Note that by Lemma 17 we have

√︂

√

𝜇

2𝛽

4𝜇 2 2

+ 𝐿̃︀

≤

+ 𝐿̃︀

𝜃

𝜃

𝛼

𝛼

(︁

)︁

4 𝜇 + 𝐿̃︀

≤

.

𝛼

The remainder of the proof is the same as for Corollary 3.

Oct 6, 2021

69

EF21 with Bells & Whistles

Oct 6, 2021

J USEFUL AUXILIARY RESULTS

J.1 BASIC FACTS

For all 𝑎, 𝑏, 𝑥1, . . . , 𝑥𝑛 ∈ R𝑑, 𝑠 > 0 and 𝑝 ∈ (0,1] the following inequalities hold

‖𝑎‖2 𝑠‖𝑏‖2

⟨𝑎, 𝑏⟩ ≤

+

,

2𝑠

2

⟨𝑎 − 𝑏, 𝑎 + 𝑏⟩ = ‖𝑎‖2 − ‖𝑏‖2,

1 ‖𝑎‖2 − ‖𝑏‖2 ≤ ‖𝑎 + 𝑏‖2, 2
‖𝑎 + 𝑏‖2 ≤ (1 + 𝑠)‖𝑎‖2 + (1 + 1/𝑠)‖𝑏‖2,

⃦ 𝑛 ⃦2

𝑛

⃦ 1 ∑︁ ⃦

1 ∑︁ 2

⃦

𝑥𝑖⃦ ≤

‖𝑥𝑖‖ ,

⃦⃦ 𝑛 𝑖=1 ⃦⃦

𝑛 𝑖=1

(︁ 𝑝 )︁−1

1−

≤ 1 + 𝑝,

2

(︁ 𝑝 )︁

𝑝

1 + (1 − 𝑝) ≤ 1 − ,

2

2

log (1 − 𝑝) ≤ −𝑝.

(115) (116) (117) (118)
(119)
(120) (121) (122)

Bias-variance decomposition For a random vector 𝜉 ∈ R𝑑 and any deterministic vector 𝑥 ∈ R𝑑, the variance of 𝜉 can be decomposed as

E [︀‖𝜉 − E[𝜉]‖2]︀ = E [︀‖𝜉‖2]︀ − ‖E[𝜉]‖2

(123)

Tower property of mathematical expectation. For random variables 𝜉, 𝜂 ∈ R𝑑 we have E[𝜉] = E[E[𝜉 | 𝜂]]
under assumption that all expectations in the expression above are well-deﬁned.

(124)

J.2 USEFUL LEMMAS

Lemma 15 (Lemma 5 of (Richtárik et al., 2021)). If 0 ≤ 𝛾 ≤ √𝑎1+𝑏 , then 𝑎𝛾2 + 𝑏𝛾 ≤ 1. Moreover, {︁ }︁
the bound is tight up to the factor of 2 since √𝑎1+𝑏 ≤ min √1𝑎 , 1𝑏 ≤ √𝑎2+𝑏 .

Lemma 16 (Lemma 2 of (Li et al., 2021)).

Suppose that function 𝑓

is 𝐿-smooth and let 𝑥𝑡+1

def
=

𝑥𝑡 − 𝛾𝑔𝑡, where 𝑔𝑡 ∈ R𝑑 is any vector, and 𝛾 > 0 any scalar. Then we have

𝑓 (𝑥𝑡+1) ≤ 𝑓 (𝑥𝑡) − 𝛾 ⃦⃦∇𝑓 (𝑥𝑡)⃦⃦2 − (︂ 1 − 𝐿 )︂ ⃦⃦𝑥𝑡+1 − 𝑥𝑡⃦⃦2 + 𝛾 ⃦⃦𝑔𝑡 − ∇𝑓 (𝑥𝑡)⃦⃦2 . (125)

2

2𝛾 2

2

Lemma 17 (Lemma 3 of (Richtárik et al., 2021)). Let 0 < 𝛼 < 1 and for 𝑠 > 0 let 𝜃(𝑠) and 𝛽(𝑠) be deﬁned as

def
𝜃(𝑠) = 1 − (1 − 𝛼)(1 + 𝑠),

𝛽(𝑠)

def
=

(1

−

𝛼)(1

+

𝑠−1).

Then the solution of the optimization problem

{︂ 𝛽(𝑠)

𝛼 }︂

min

: 0<𝑠<

𝑠 𝜃(𝑠)

1−𝛼

is given by 𝑠* = √ 1 − 1. Furthermore, 𝜃(𝑠*) = 1 − √1 − 𝛼, 𝛽(𝑠*) = 1√−𝛼 and

1−𝛼

1− 1−𝛼

√︃

√

𝛽(𝑠*)

1

1 1−𝛼

2

𝜃(𝑠*) = √1 − 𝛼 − 1 = 𝛼 + 𝛼 − 1 ≤ 𝛼 − 1.

(126) (127)

In the trivial case 𝛼 = 1, we have 𝛽𝜃((𝑠𝑠)) = 0 for any 𝑠 > 0, and (127) is satisﬁed.

70

EF21 with Bells & Whistles

Oct 6, 2021

Lemma 18. Let (arbitrary scalar) non-negative sequences {𝑠𝑡}𝑡≥0, and {𝑟𝑡}𝑡≥0 satisfy

𝑇 −1

𝑇 −1

𝑇 −1

∑︁ 𝑠𝑡+1 ≤ (1 − 𝜃) ∑︁ 𝑠𝑡 + 𝐶 ∑︁ 𝑟𝑡

𝑡=0

𝑡=0

𝑡=0

for some parameters 𝜃 ∈ (0, 1], 𝐶 > 0. Then for all 𝑇 ≥ 0

𝑇∑−︁1 𝑠𝑡 ≤ 𝑠0 + 𝐶 𝑇∑−︁1 𝑟𝑡.

𝜃𝜃

𝑡=0

𝑡=0

Proof. We have

𝑇 −1

𝑇 −1

∑︁ 𝑠𝑡 − 𝑠0 ≤ ∑︁ 𝑠𝑡 + 𝑠𝑇 − 𝑠0

𝑡=0

𝑡=0

𝑇 −1
= ∑︁ 𝑠𝑡+1

𝑡=0

𝑇 −1

𝑇 −1

≤ (1 − 𝜃) ∑︁ 𝑠𝑡 + 𝐶 ∑︁ 𝑟𝑡

𝑡=0

𝑡=0

𝑇 −1

𝑇 −1

𝑇 −1

= ∑︁ 𝑠𝑡 − 𝜃 ∑︁ 𝑠𝑡 + 𝐶 ∑︁ 𝑟𝑡.

𝑡=0

𝑡=0

𝑡=0

Dividing both sides by 𝜃 > 0 and rearranging the terms, we get

𝑇∑−︁1 𝑠𝑡 ≤ 𝑠0 + 𝐶 𝑇∑−︁1 𝑟𝑡.

𝜃𝜃

𝑡=0

𝑡=0

(128)

71

