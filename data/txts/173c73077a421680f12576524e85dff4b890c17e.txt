arXiv:2102.06449v4 [math.ST] 23 Feb 2022

1
Two-Sample Test with Kernel Projected
Wasserstein Distance
Jie Wang, Rui Gao, Yao Xie
Abstract We develop a kernel projected Wasserstein distance for the two-sample test, an essential building block in statistics and machine learning: given two sets of samples, to determine whether they are from the same distribution. This method operates by ﬁnding the nonlinear mapping in the data space which maximizes the distance between projected distributions. In contrast to existing works about projected Wasserstein distance, the proposed method circumvents the curse of dimensionality more efﬁciently. We present practical algorithms for computing this distance function together with the non-asymptotic uncertainty quantiﬁcation of empirical estimates. Numerical examples validate our theoretical results and demonstrate good performance of the proposed method.
I. INTRODUCTION As a fundamental problem in statistical inference [63], two-sample hypothesis testing aims to determine whether two sets of samples come from the same distribution or not. This problem has broad applications in scientiﬁc discovery ﬁelds. For example, it can be applied in anomaly detection [1, 8, 54] to identify abnormal observations that follow a distinct distribution compared with typical observations. Similarly, in change-point detection [49, 61, 62], two-sample testing is essential to detect abrupt changes in streaming data. Other notable examples include model criticism [3, 11, 42], causal inference [43], and health care [55]. Parametric or low-dimensional testing scenarios have been the main focus in classical literature. When extra knowledge about the data distributions is available, one can design parametric tests, such as Hotelling’s two-sample test [27], Student’s t-test [48], etc. Non-parametric two-sample tests are more attractive when the exact parametric form of the data distributions is hard to
J. Wang and Y. Xie are with H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology. R. Gao is with Department of Information, Risk, and Operations Management, University of Texas at Austin.

2
specify. It is popular to design non-parametric tests using integral probability metrics, since the evaluation of the corresponding test statistics can be obtained based on samples without knowing the densities of data distributions. Some earlier works design tests using Kolmogorov-Smirnov distance [34, 50], total variation distance [25], and Wasserstein distance [14, 51]. However, it is not proper to use these tests for high-dimensional settings since the sample complexity for estimating those distance functions based on empirical samples suffers from the curse of dimensionality.
There is a strong need for developing non-parametric tests for high-dimensional data, especially for modern applications. A notable contribution is the two-sample test based on Maximum Mean Discrepancy (MMD) [9, 23, 24]. Although the power of MMD test with the median choice of kernel bandwidth decays quickly when the dimension of distributions increases [52], this test with properly chosen bandwidth does not have the curse of dimensionality issue for low-dimensional manifold data as pointed out in [9]. Unfortunately, the MMD test with optimized bandwidth still does not demonstrate good testing power for the small-sampled case as demonstrated numerically in this paper. In addition, recent works [59, 61] leverage the idea of dimensionality reduction for dealing with high-dimensional settings, which use the projected Wasserstein distance as the test statistic, i.e., the test statistic works by ﬁnding the linear projector such that the distance between projected distributions is maximized. However, a linear projector may not serve as an optimal design for maximizing the power of tests as demonstrated numerically in Section V.
In this paper, we present a new non-parametric two-sample test statistic aiming for the highdimensional setting based on a kernel projected Wasserstein (KPW) distance, with a nonlinear projector based on the reproducing kernel Hilbert space (RKHS) designed to optimize the test power via maximizing the probability distance between the distributions after projection. In addition, our contributions include the following:
• We develop a computationally efﬁcient algorithm for evaluating the KPW using a representer theorem to reformulate the problem into a ﬁnite-dimensional optimization problem and a block coordinate descent optimization algorithm which is guaranteed to ﬁnd an -stationary point with complexity O ( −3).
• To quantify the false detection rate, which is essential in setting the detection threshold, we develop non-asymptotic bounds for empirical KPW distance based on the covering number argument.
• We present numerical experiments to validate our theoretical results as well as demonstrate

3
the competitive performance of our proposed test using both synthetic and real data. Related Work. It is helpful to understand the structure of high-dimension distributions by lowdimensional projections. Notable methodologies include the principal component analysis (PCA) [33], kernel PCA [57], factor analysis [13], etc. Several works leverage this idea to design tests for high-dimensional data. [47] and [61] ﬁrst design tests by ﬁnding the worst-case linear projector that maximizes the distance between projected sample points in one dimension. Later [39] and [59] naturally extend this idea by developing a projector that maps sample points into d dimensional linear subspace with d ≥ 1, called projected Wasserstein distance. Efﬁcient optimization algorithms and statistical properties of this distance have been investigated in recent works [30, 40]. However, a linear projector cannot efﬁciently capture features from data with nonlinear patterns, limiting the performance of tests mentioned above for practical applications. It is therefore promising to use nonlinear dimensionality reduction for two-sample testing. Although nonlinear projectors can be obtained using neural networks [20], the sample complexity of the corresponding test statistic will have slow convergence rates since the neural network function class usually has high complexity in terms of the covering number. Recently kernel method has been demonstrated to be beneﬁcial for understanding data [6, 28, 35, 46] because of sharp sample complexity rate, low computational cost, and ﬂexible representation of features. This fact motivates us to use a nonlinear projector based on kernels to design tests. Compared with the linear projector, computing the corresponding statistic and analyzing its performance is more challenging since the function space cannot be parameterized by ﬁnite-dimensional coefﬁcients. We leverage the kernel trick to ﬁnish these two parts.
The remaining of this paper is organized as follows. Section II introduces some preliminary knowledge on two-sample testing and related probability distances, Section III outlines a practical algorithm for computing KPW distance, Section IV studies the uncertainty quantiﬁcation of empirical KPW distance, Section V demonstrates some numerical experiments, and Section VI presents some concluding remarks.
II. PROBLEM SETUP Let xn := {xi}ni=1 and ym := {yi}mi=1 be i.i.d. samples generated from distributions µ and ν supported on RD, respectively. Our goal is to design a two-sample test which, given samples xn and ym, decides to accept the null hypothesis H0 : µ = ν or reject H0 in favor of the alternative hypothesis H1 : µ = ν. Denote by T : (xn, ym) → {t0, t1} the two-sample test, where t0 means

4

we reject H1 and t1 means we accept H1 and reject H0. Deﬁne the type-I risk as the probability of rejecting hypothesis H0 when it is true, and the type-II risk as the probability of accepting H0 when µ = ν:

(I) n,m

=

Pxn∼µ,ym∼ν

T (xn, ym) = t1

,

under H0,

(II) n,m

=

Pxn∼µ,ym∼ν

T (xn, ym) = t0

,

under H1.

Given parameters α, β ∈ (0, 12 ), we aim at building a two-sample test such that, when applied to n-observation samples xn and m-observation samples ym, it has the type-I risk at most α (i.e., at level α) and the type-II risk at most β (i.e., of power 1 − β). Moreover, we want to ensure these speciﬁcations with sample sizes n, m as small as possible.
We propose a non-parametric test by considering the probability distance functions between two empirical distributions constructed from observed samples. Speciﬁcally, we design a test T such that the null hypothesis H0 is rejected when

D(µˆn, νˆm) > χ,

where D(·, ·) is a divergence quantifying the differences of two distributions, χ is a data-dependent threshold, and µˆn and νˆm are empirical distributions from n samples in µ and m samples in ν, respectively. Several existing tests can be uniﬁed into this framework by taking D(·, ·) as some special probability distances, including the MMD test, total variation distance test, etc. In this paper, we will design the divergence D based on the Wasserstein distance, and we specify the cost function c(x, y) = x − y 22.

Deﬁnition 1 (Wasserstein Distance). Given two distributions µ and ν, the Wasserstein distance

is deﬁned as

W (µ, ν) = min
π∈Π(µ,ν)

c(x, y) dπ(x, y),

where c(·, ·) denotes the cost function quantifying the distance between two points, and Π(µ, ν)

denotes the joint distribution with marginal distributions µ and ν.

Although Wasserstein distance has wide applications in machine learning, the ﬁnite-sample convergence rate of Wasserstein distance between empirical distributions is slow in highdimensional settings [17]. Therefore, it is not suitable for high-dimensional two-sample tests. Instead, existing works use the projection idea to rescue this issue.

5

Deﬁnition 2 (Projected Wasserstein Distance). Given two distributions µ and ν, deﬁne the projected Wasserstein distance as

PW (µ, ν) =

max

W (A#µ, A#ν) ,

A: RD→Rd,ATA=Id

where the operator # denotes the push-forward operator, i.e.,

A(z) ∼ A#µ for z ∼ µ, and we denote A as a linear operator such that A(z) = ATz with z ∈ RD and A ∈ RD×d.

This idea is demonstrated to be useful for breaking the curse of dimensionality for the original Wasserstein distance [40, 59]. However, a linear projector is not an optimal choice for dimensionality reduction. Instead, we will consider a nonlinear projector to obtain a more powerful two-sample test, and we use functions in vector-valued reproducing kernel Hilbert space (RKHS) for projection.

Deﬁnition 3 (Vector-valued RKHS). A function K : RD × RD → Rd×d is said to be a positive

semi-deﬁnite kernel if

NN
y¯i, K(x¯i, x¯j)y¯j ≥ 0
i=1 j=1

for any ﬁnite set of points {x¯i}Ni=1 in RD and {y¯i}Ni=1 in Rd. Given such a kernel, there exists an unique Rd-valued Hilbert space HK with the reproducing kernel K. For ﬁxed x ∈ RD and

y ∈ Rd, deﬁne the kernel section Kx with the action y as the mapping Kxy : RD → Rd such

that

(Kxy)(x ) = K(x , x)y, ∀x ∈ RD.

In particular, the Hilbert space HK satisﬁes the reproducing property:

∀f ∈ HK, f, Kxy HK = f (x), y .

Deﬁnition 4 (Kernel Projected Wasserstein Distance). Consider a Rd-valued RKHS H with the corresponding kernel function K. Given two distributions µ and ν, deﬁne the kernel projected Wasserstein (KPW) distance as

KPW (µ, ν) = max W (f #µ, f #ν) f ∈F
where the function class F = {f ∈ H : f H ≤ 1}.

6

Remark 1. For d = 1, when the kernel function K(x, y) = x, y , the KPW distance reduces

into the PW distance. However, these two distances are not the same for general d. Moreover,

existing works [2, 7, 45, 46] consider the design of the matrix-valued kernel function for d > 1

as

K(x, x ) = k(x, x ) · P,

(1)

where k(·, ·) denotes a scalar-valued kernel function and P ∈ Rd×d is a positive semi-deﬁnite matrix that encodes the relation between the output space. Such a design reduces the computational cost for applying vector-valued RKHS.

In this paper, we design the two-sample test as follows. We split the data points into training and testing datasets. We ﬁrst use the training set to train a nonlinear projector that maps data points into Rd-subspace, and then perform the permutation test on testing data points that are projected based on the trained projector. The detailed algorithm is presented in Algorithm 1. This test is guaranteed to exactly control the type-I error [22] because we evaluate the p-value of the test via the permutation approach. To obtain reliable two-sample tests, we also require the KPW distance satisﬁes the discriminative property that KPW (µ, ν) = 0 if and only if µ = ν. The following proposition reveals that this property holds by considering the vector-valued RKHS satisfying the universal property, the proof of which is provided in Appendix C. We also study how to compute the kernel projected distance and its related statistical properties in the following sections.

Proposition 1 (Discriminative Property of KPW). Denote by Cb(X ) the space of bounded and continuous Rd-valued functions on X . Assume that H is a universal vector-valued RKHS so that for any ε > 0 and f ∈ Cb(X ), there exists g ∈ H so that
f − g ∞ sup f (x) − g(x) 2 < ε.
x∈X
Then the KPW distance KPW (µ, ν) = 0 if and only if µ = ν.

III. COMPUTING KPW DISTANCE By the deﬁnition of Wasserstein distance, computing KPW (µˆn, νˆm) is equivalent to the following max-min problem:

max

min

πi,j

f (xi) − f (yj)

2 2

,

(2)

f ∈H: f 2H≤1

π∈Γ i,j

7

Algorithm 1 Permutation two-sample test using the KPW distance Require: Level α, number of permutation times Np, collected samples xn and ym.
1: Split data as xn = xTr ∪ xTe and ym = yTr ∪ yTe.

2: Formulate empirical distributions (µˆTr, νˆTr) corresponding to (xTr, yTr).

3: Obtain f as the (approximate) optimal projector to KPW (µˆTr, νˆTr).

4: Compute the statistic T = W (f #µˆTe, f #νˆTe).

5: for t = 1, . . . , Np do

6: Shufﬂe xTe ∪ yTe to obtain xT(te) and y(Tte).

7: Formulate empirical distributions (µˆT(te), νˆ(Tte)) corresponding to (xTe, yTe).

8:

Compute

the

statistic

for

permuted

samples

Tt

=

W

(f

#µˆ

Te (t)

,

f #νˆ(Tte)).

9: end for

Return the p-value N1p Nt=p1 1{Tt ≥ T }.

where Γ =

π

∈

n×m
R+

:

j πi,j = n1 ,

i πi,j

=

1 m

.

The computation of KPW distance has numerous challenges. It is crucial to design a suitable

kernel function to obtain low computational complexity and reliable testing power, which will

be discussed in Section V. Moreover, the function f ∈ H is a countable combination of

basis functions, i.e., the problem (2) is an inﬁnite-dimensional optimization. By developing the

representer theorem in Theorem 1, we are able to convert this problem into a ﬁnite-dimensional

problem. Finally, there is no theoretical guarantee for ﬁnding the global optimum since it is a non-

convex non-smooth optimization problem. Moreover, Sion’s minimax theorem is not applicable

because the problem (2) is not a convex programming: the inner minimization of quadratic

function makes the objective in (2) not concave in f in general. Based on this observation, we

only focus on optimization algorithms for ﬁnding a local optimum point in polynomial time.

Theorem 1 (Representer Theorem for KPW Distance). There exists an optimal solution to (2)

that admits the following expression:

n

m

fˆ = Kxi ax,i − Kyj ay,j,

i=1

j=1

where Kx(·) denotes the kernel section and ax,i, ay,j ∈ Rd for i = 1, . . . , n, j = 1, . . . , m are

coefﬁcients to be determined.

8

The proof of Theorem 1 is provided in Appendix D, in which standard representer theorem in literature [56, Theorem 1] is not applicable since the RKHS norm serves as a hard constraint instead of the regularization of the objective function. In order to express the optimal solution as the compact matrix form, deﬁne ax ∈ Rnd as the concatenation of coefﬁcients ax,i for i = 1, . . . , n and
Kz(xn) = K(z, x1) · · · K(z, xn) ∈ Rd×nd.

We also deﬁne the vector ay and matrix Kz(ym) likewise. Then we have

fˆ(z) = Kz(xn)ax − Kz(ym)ay, ∀z ∈ X .

Deﬁne the gram matrix K(xn, xn) as the n × n block matrix with the (i, j)-th block being K(xi, xj). The gram matrices K(xn, ym), K(ym, xn) and K(ym, ym) can be deﬁned likewise.

Denote by G the concatenation of gram matrices:

 K(xn, xn)
G= −K(ym, xn)

 −K(xn, ym)
, K(ym, ym)

and we assume that G is positive deﬁnite. Otherwise, we add the gram matrix with a small number times identity matrix to make it invertible. Substituting the expression of fˆ(z), z ∈ X

into (2), we obtain a ﬁnite-dimensional optimization problem:

max ω

min

πi,jci,j : ωTGω ≤ 1 ,

π∈Γ

i,j

where ω = [aTx , aTy ]T ∈ Rd(n+m), ci,j = Ai,jω 22, and

Ai,j = [Kxi(xn) − Kyj (xn), Kyj (ym) − Kxi(ym)].

Suppose that the inverse of G admits the Cholesky decomposition G−1 = U U T, then by the change of variable technique s = U −1ω, we obtain the norm-constrained optimization problem:

max

min

πi,jci,j : sTs ≤ 1 ,

(3)

s∈Rd(n+m)

π∈Γ i,j

and we can replace the constraint sTs ≤ 1 with sTs = 1 based on the fact that the norm function

satisﬁes the linear property. In other words, the decision variable s belongs to the Euclidean ball

Sd(n+m)−1 = {s ∈ Rd(n+m) : sTs = 1}.

For the ease of optimization, we consider the entropic regularization of the problem (3):

max

min

πi,jci,j − ηH(π) ,

(4)

s∈Sd(n+m)−1

π∈Γ i,j

9

in which we denote the entropy function H(π) = − i,j πi,j(log πi,j − 1). By the duality theory of entropic optimal transport [18] and the change-of-variable technique, (4) is equivalent to the following minimization problem:

min

F (u, v, s),

(5)

s∈Sd(n+m)−1 ,u∈Rn ,v ∈Rm

where

ci,j = Ai,jU s 22,

1 πi,j(u, v, s) = exp − η ci,j + ui + vj ,

1n

1m

F (u, v, s) = πi,j(u, v, s) − n ui − m vj.

i,j

i=1

j=1

The details for this deviation is deferred in Appendix D. Based on this formulation, we consider

a Riemannian block coordinate descent (BCD) method [26] for optimization, which updates a

block of variables by minimizing the objective function with respect to that block while ﬁxing

values of other blocks:

ut+1 = min F (u, vt, st),

(6a)

u∈Rn

vt+1 = min F (ut+1, v, st),

(6b)

v∈Rm

ζt+1 = ∇sπi,j(ut+1, vt+1, st),

(6c)

i,j

ξt+1 = Pst ζt+1 ,

(6d)

st+1 = Retrst − τ ξt+1 ,

(6e)

where the operator Ps(ζ) denotes the orthogonal projection of the vector ζ onto the tangent space of the manifold Sd(n+m)−1 at s:
Ps ζ = ζ − s, ζ s, s ∈ Sd(n+m)−1,

and the retraction on this manifold is deﬁned as

s − τξ

Retrs − τ ξ =

,

s ∈ Sd(n+m)−1.

(6f)

s − τξ

Note that the update steps (6a) and (6b) have closed-form expressions:

ut+1 = ut + log

1/n

,

(6g)

j πi,j(ut, vt, st)

i∈[n]

vt+1 = vt + log

1/m

,

(6h)

i πi,j (ut+1, vt, st) j∈[m]

10
Algorithm 2 BCD Algorithm for Solving (5) Require: Empirical distributions µˆn and νˆm.
1: Initialize v0, s0 2: for t = 0, 1, 2, . . . , T − 1 do 3: Update ut+1 according to (6g) 4: Update vt+1 according to (6h) 5: Update the Euclidean and Riemannian gradient ζt+1 and ξt+1, according to (6i) and (6d),
respectively. 6: Update st+1 according to (6e) 7: end for
Return u∗ = uT , v∗ = vT , s∗ = sT .
and the Euclidean gradient ζt+1 in (6c) can be computed using the chain rule: ζt+1 = − η1 U T πi,j(ut+1, vt+1, st)ATi,jAi,j U st. (6i)
i,j
The overall algorithm for solving the problem (5) is summarized in Algorithm 2. We provide details for efﬁcient implementation of the proposed algorithms in Appendix F. We also give a brief introduction to Riemannian optimization in Appendix B. The following theorem gives a convergence analysis of our proposed algorithm.The proof of this result is provided in Appendix D, which follows similar procedure in [30]. The main difference lies in establishing the descent lemma for updating the variable s on sphere instead of Stiefel manifold. Speciﬁcally, the procedure for ﬁnding the upper bound on the cost function ci,j, the Lipschitz constant for πi,j(u, v, s) in s, and the Lipschitz constants of the retraction operator (6f) will be different.
Theorem 2 (Convergence Analysis for BCD). We say that (uˆ, vˆ, sˆ) is a ( 1, 2)-stationary point of (5) if
GradsF (uˆ, vˆ, sˆ) ≤ 1, F (uˆ, vˆ, sˆ) − min F (u, v, sˆ) ≤ 2,
u,v
where GradsF (u, v, s) denotes the derivative of F with respect to s on the sphere Sd(n+m)−1. Let {ut, vt, st} be the sequence generated by Algorithm 2, then Algorithm 2 returns an ( 1, 2)-

11

stationary point in

11

T = O log(mn) · 3 + 2 ,

2

12

iterations, where the notation O(·) hides constants related to the initial guess (v0, s0) and the

term maxi,j Ai,jU .

Remark 2 (Complexity of Algorithm 2). Denote N = n ∨ m1. Note that the iteration (6g) and

(6h) can be implemented in O(N ) iterations. Second, the retraction step in (6e) requires O(dN )

arithmetic operations. Third, the computation of the Euclidean vector in (6c) can be implemented

in O(d3N 3) operations, and the projection step can be done in O(dN ) operations. Therefore,

the number of arithmetic operations in each iteration is of O(d3N 3). In summary, Algorithm 2

returns an ( 1, 2)-stationary point in

O

d3N 3 log(N ) ·

1 +

1

3

2

2

12

arithmetic operations. Note that this computational complexity is independent of the dimension

D of samples since we only need to compute the gram matrix as an input. The storage cost is of

O(d2N 2), in which the most expensive step is to store the gram matrix G.

IV. PERFORMANCE GUARANTEES

In this section, we build statistical properties of the empirical KPW distance, though in practice

we may not succeed in ﬁnding a global optimum solution to the non-convex optimization problem

(2). We assume the cost function for the Wasserstein distance has the form c(x, y) =

x−y

p 2

with p ∈ [1, ∞). Moreover, results throughout this section are based on the following assumption.

Assumption 1. For any x, x ∈ X , the matrix-valued kernel K(x, x ) is symmetric and satisﬁes

0 K(x, x ) BId.

Deﬁnition 5 ((Projection) Poincare Inequality). 1. A distribution µ is said to satisfy a Poincare inequality if there exists an M > 0 for X ∼ µ so that Var[f (X)] ≤ M E[ ∇f (X) 2] for any f satisfying E[f (X)2] < ∞ and E[ ∇f (X) 2] < ∞. 2. A distribution µ is said to satisfy a projection Poincare inequality if there exists an M > 0

1We denote a ∨ b for max{a, b} and a ∧ b for min{a, b}.

12

Fig. 1. Average values of KPW distances between empirical distributions µˆn and νˆn as the sample size n varies. Results are averaged for 10 independent trials and the shaded areas show the corresponding error bars.

for any f ∈ F and X ∼ f #µ so that Var[f (X)] ≤ M E[ ∇f (X) 2] for any f satisfying E[f (X)2] < ∞ and E[ ∇f (X) 2] < ∞.

Remark 3. The Poincare inequality characterizes the relation about the variance of a function and its derivative in the spirit of the Sobolev inequality. It is a standard technical assumption for investigating the empirical convergence of Wasserstein distance [38, 40], and is satisﬁed for various exponential measures such as the Gaussian distribution. See [37] for more examples.

Lemma 1. Assume that the distribution µ satisﬁes a projection Poincare inequality. Then

E[(KPW (µˆn, µ))1/p]

n−

1 (2p)∨d

(log

n)ζp,d

/p

+ n−1/(2∨p) log(n) + n−1/p log(n),

where ζp,d = 1{d = 2p}, and refers to ”less than” with a constant depending only on (p, B).

Lemma 2. Assume that the distribution µ satisﬁes a Poincare inequality, and any f ∈ F is L-Lipschitz. Then with probability at least 1 − α, it holds that
(KPW (µˆn, µ))1/p − E[(KPW (µˆn, µ))1/p] ≤ max log(1/α), log(1/α) n−1/(2∨p)L1/p,
where > 0 is a constant that depends on M .

Proof of two lemmas above follows similar covering number arguments in [40], the details of which are deferred in Appendix E. The main difference is that we incorporate the reproducing property of vector-valued RKHS to give a valid bound on the covering number of the RKHS ball

13

F. Based on these two lemmas and the triangular inequality for Wasserstein distance, we give a ﬁnite-sample guarantee for the convergence of the KPW distance in Theorem 3. Compared with the sample complexity of estimating Wasserstein distance, KPW distance does not suffer from the curse of dimensionality as the RKHS ball F has low complexity.

Theorem 3 (Finite-sample Guarantee). Suppose the target distributions µ = ν, which satisﬁes projection Poincare inequality and Poincare inequality. Moreover, any f ∈ F is L-Lipschitz. Take N = n ∧ m, then with probability at least 1 − 2α, it holds that

(KPW (µˆn, νˆm))1/p

N

−

1 (2p)∨d

(log

N

)ζp,d

/p

+ N −1/(2∨p) log(N ) + N −1/p log(N )

+ max log(1/α), log(1/α) N −1/(2∨p)L1/p.

A. Performance Guarantees for p ∈ [1, 2) When showing concentration results for p-Wasserstein distance with p ∈ [1, 2), however, it
is not necessary to rely on the Poincare inequality assumption. The main result for this case is summarized in Theorem 4 (see details in Appendix E-C).

Theorem 4 (Finite-sample Guarantee). Suppose the target distributions µ = ν. Then with probability at least 1 − 2α, it holds that

(KPW (µˆn, νm))1/p

N

−

1 (2p)∨d

(log

N

)ζp,d

/p

+ N 1/2−1/p log(N ) + N −1/p

where N = n ∧ m and

+ N 1/2−1/p

2 log .
α

refers to ”less than” with a constant depending only on (p, B).

B. Sample Complexity
We also numerically examine the sample complexity of the empirical KPW distance KPW (µˆn, νˆn) with µ = ν = N (0, ID), where n ∈ {10, 50, 125, 250, 500} and D ∈ {30, 50, 70, 100}. Figure 1 reports the average distances and the shaded areas show the corresponding error bars over 10 independent trials. We defer the detailed experiment setup and the plots of the computation time in Appendix G-A. From the plot we can see that the empirical KPW distances decay to zero

14

Fig. 2. Testing results on Gaussian distributions across different choices of dimension D. Left: power for Gaussian distributions, where the shifted covariance matrix is still diagonal; Middle: power for Gaussian distributions, where the shifted covariance matrix is non-diagonal; Right: Type-I error.
TABLE I AVERAGE TEST POWER AND STANDARD ERROR ABOUT DETECTING DISTRIBUTION ABUNDANCE CHANGE IN MNIST DATASET
ACROSS DIFFERENT CHOICES OF SAMPLE SIZE.

N
200 250 300 400 500
Avg.

MMD-NTK
0.639±0.029 0.763±0.010 0.813±0.016 0.881±0.013 0.950±0.002
0.809

MMD-O
0.696±0.006 0.781±0.002 0.869±0.002 0.956±0.003 0.988±0.000
0.858

ME
0.298±0.031 0.472±0.017 0.630±0.025 0.779±0.020 0.927±0.006
0.621

PW
0.302±0.033 0.369±0.030 0.524±0.023 0.591±0.044 0.782±0.040
0.513

KPW
0.663±0.015 0.785±0.014 0.928±0.001 0.978±0.000 1.000±0.000
0.870

quickly when the sample size n increases. Moreover, the distances with smaller values of d have faster decaying rates. Finally, the convergence behavior of the empirical KPW distances is nearly independent of the choice of D, which alleviates the issue of the curse of dimensionality for the original Wasserstein distance. These facts conﬁrm the ﬁnite-sample guarantee discussed in Theorem 3.
V. NUMERICAL EXPERIMENTS Throughout this section, we compare the performance of tests with the following procedures. (i) PW: the projected Wasserstein test where the projector is a linear mapping [59]; (ii) MMD-O: the MMD test with a Gaussian kernel whose bandwidth is optimized [41]; (iii) MMD-NTK: the test that combines both neural networks and MMD [10]; and (iv) ME: the mean embedding test with optimized hyper-parameters [32]. Implementation details on those baseline methods are

15
omitted in Appendix G-B. When dealing with synthetic datasets, we generate a single sample set as the training set to learn parameters for each method. Then we evaluate the power of tests on 100 new sample sets generated from the same distribution. When dealing with real datasets, we randomly take part of samples as the training set, and evaluate the power on 100 randomly chosen subsets from the remaining samples. The number of permutations in Algorithm 1 is set to be Np = 100. We control the type-I error for all tests at α = 0.05.
When using the KPW distance, we follow (1) to design kernels to decrease the computational complexity. More speciﬁcally, we choose the scalar-valued kernel k(·, ·) to be a standard Gaussian kernel with the bandwidth σ2, and
P = (1 − ρ)11T + ρId, with ρ ∈ [0, 1].
We use the cross-validation approach to select the hyper-parameters ρ and σ2, the details of which are deferred in Appendix G-C. The dimension d is pre-speciﬁed and ﬁxed into 3 in all experiments. We also present a study on the impact of hyper-parameters such as the projected dimension d and regularization parameter η in Appendix H.
A. Tests for Synthetic Datasets We ﬁrst investigate the performance when µ and ν are Gaussian distributions with diagonal
covariance matrices. Speciﬁcally, we take µ = N (0, ID) and ν = N (0, Σ) is the covariance shifted Gaussian, where the matrix Σ = diag(4, 4, 4, 1, . . . , 1). In other words, we only scale the ﬁrst three entries of the covariance matrix to make the high-dimensional testing problem challenging to handle. Fig. 2 reports the type-I and type-II errors for various tests across different choices of dimension D. We observe that both PW and KPW tests perform the best, while the power for other benchmark methods degrades quickly when the dimension D increases.
Next, we examine the case where ν has a non-diagonal covariance matrix. We take µ = N (0, ID) and ν = N (0, V ΣV T), where V is an orthogonal matrix with Vi,j = 2/(D + 1) sin(ijπ/(D+1)) and Σ = diag(5, 5, 5, 1, . . . , 1). Testing results for various choices of dimension D is reported in the middle of Fig. 2. In this case, the PW test performs slightly better than the KPW test. One possible explanation is that linear mapping seems to be the optimal choice for two-sample testing with covariance shifted Gaussian distributions. It is promising to design other types of matrix-valued kernel functions to improve performances of the KPW test.

16

Fig. 3. Testing results on Gaussian-mixture distributions. Left two: type-I and type-II errors across different choices of dimension D with ﬁxed sample size n = m = 200; Right two: type-I and type-II errors across different choices of sample size n = m with ﬁxed dimension D = 140.

Finally, we study the case where sample points are generated from high-dimensional Gaussian

mixture

distributions.

We

take

µ

=

1 2

N

(0,

ID

)

+

1 2

N

(∆2

,

ID

)

with

∆2

=

(1, 1, . . . , 1)

and

√

√

ν

=

1 2

N

(0

,

Σ1

)

+

1 2

N

(∆3

,

Σ2

)

with

∆3

=

(1 + 0.8/

D, . . . , 1 + 0.8/

D). Covariance matrix Σ1

is deﬁned with Σ1[1, 1] = Σ1[2, 2] = 4, Σ1[1, 2] = Σ1[2, 1] = −0.9, Σ1[i, i] = 1, 3 ≤ i ≤ D, and

Σ1[i, j] = 0 for indexes elsewhere. Covariance matrix Σ2 is deﬁned with Σ2[1, 2] = Σ2[2, 1] = 0.9,

Σ2[i, i] = 1, 1 ≤ i ≤ D, and Σ2[i, j] = 0 for indexes elsewhere. Testing results (type-I and type-II

errors) across different choices of dimension D for ﬁxed sample size n = m = 200 is presented

in the left two plots in Fig. 3. We also report results for increasing sample sizes n = m by

ﬁxing the dimension D = 140 in the right two plots in Fig. 3. From the plot, we can see that all

approaches have expected type-I error rates. Moreover, the tests based on PW and KPW distances

outperform other benchmark methods, which indicates that the idea of dimension reduction is

helpful for high-dimensional testing. The KPW test generally has the highest power in this case,

since the nonlinear projector in the unit ball of RKHS is ﬂexible enough to capture the differences

between distributions. Other experiment details of this subsection is omitted in Appendix G-D.

B. Tests for MNIST handwritten digits
We now perform two-sample tests on the MNIST dataset [36]. Let p be the distribution uniformly generated from the dataset, and q = 0.85p+0.15pcohort, where pcohort is the distribution from a class with digit 1. Both training and testing sample sizes are set to be N ∈ {200, 250, . . . , 500}. Before performing two-sample tests, we pre-process this dataset by taking the sigmoid transformation of each image such that all scaled pixels are within the interval [0, 1]. Table I presents the testing

17

TABLE II DELAY TIME FOR DETECTING THE TRANSITION IN MSRC-12 THAT CORRESPONDS TO FOUR USERS.

User MMD-NTK MMD-O ME PW KPW

1

36

2

8

3

15

4

22

73

82 47

33

7

97

9

1

13

27

2

20

83

69 16

12

Mean Std

20.25 12.0

44.0 68.8 18.50 16.5 39.5 30.1 19.8 13.5

power of various tests across different choices of N , from which we can see that the KPW test is competitive compared with other methods. We observe that performances of MMD-O in MNIST dataset are signiﬁcantly better than that in synthetic datasets provided in Section V-A. One possible explanation is that isotropic kernel functions will limit the power of MMD tests in some numerical examples [41, Section 3]. Average type-I error for various tests is presented in Table III in Appendix G-E, from which we can see all tests have the type-I error close to α = 0.05.
C. Human activity detection
Finally, we apply the KPW test to perform online change-point detection for human activity transition. We use a real-world dataset called the Microsoft Research Cambridge-12 (MSRC-12) Kinect gesture dataset [16]. After pre-processing, this dataset consists of actions from four people, each with 855 samples in R60, and with a change of action from bending to throwing at the time index 500. More experimental details are omitted in Appendix G-F. Fix the window size W = 100. We pre-train a nonlinear projector using the data (sample size as the window) before time index 300 and compute the null statistics for many times to obtain the true threshold such that the false alarm rate is controlled within α = 0.05. Then we perform online change-point detection based on a sliding window that moves forward with time. We compute the detection statistic by comparing the distribution between the block of data before time 300 and the data from the sliding window. We reject the null hypothesis and claim a change is happened if the statistic is above the threshold. Table II reports the delay time for detecting the behavior transition, from which we observe that the KPW test detects the change in the shortest time.

18
VI. CONCLUSION We proposed the KPW distance for the task of two-sample testing, which operates by ﬁnding the nonlinear mapping in the data space to maximize the distance between projected distributions. Practical algorithms together with uncertainty quantiﬁcation of empirical estimates are discussed to help with this task. The extension of this work is as follows. First, it is promising to consider milder technical assumptions than the projected Poincare inequality when establishing performance guarantees. Second, a meaningful research question is to determine the optimal hyper-parameters for the KPW test, including the projected subspace dimension d and the matrix-valued kernel function K. Third, it is desirable to study how to systematically pick the regularization parameter η to balance the trade-off between computational efﬁciency and accuracy of the obtained solution.
ACKNOWLEDGEMENTS This work is supported by NSF DMS-2134037, CCF-1650913, CMMI-2015787, DMS-1938106, and DMS-1830210. The authors would like to thank the Editor and the anonymous referees for the thoughtful comments and suggestions, which led to an improvement of the presentation.
REFERENCES [1] Ahmed, M., Mahmood, A. N., and Hu, J. (2016). A survey of network anomaly detection
techniques. Journal of Network and Computer Applications, 60:19–31. [2] Baldassarre, L., Rosasco, L., Barla, A., and Verri, A. (2010). Vector ﬁeld learning via spectral
ﬁltering. In Machine Learning and Knowledge Discovery in Databases, pages 56–71, Berlin, Heidelberg. Springer Berlin Heidelberg. [3] Binkowski, M., Sutherland, D. J., Arbel, M., and Gretton, A. (2018). Demystifying MMD GANs. In International Conference on Learning Representations. [4] Boumal, N., Absil, P.-A., and Cartis, C. (2018). Global rates of convergence for nonconvex optimization on manifolds. IMA Journal of Numerical Analysis, 39(1):1–33. [5] Boyd, S. and Vandenberghe, L. (2004). Convex optimization. Cambridge university press. [6] Brouard, C., d’Alche´ Buc, F., and Szafranski, M. (2011). Semi-supervised penalized output kernel regression for link prediction. In 28th International Conference on Machine Learning (ICML 2011), pages 593–600.

19
[7] Caponnetto, A., Micchelli, C. A., Pontil, M., and Ying, Y. (2008). Universal multi-task kernels. Journal of Machine Learning Research, 9(52):1615–1646.
[8] Chandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. ACM Computing Surveys, 41(3).
[9] Cheng, X. and Xie, Y. (2021a). Kernel mmd two-sample tests for manifold data. arXiv preprint arXiv:2105.03425.
[10] Cheng, X. and Xie, Y. (2021b). Neural tangent kernel maximum mean discrepancy. In Advances in Neural Information Processing Systems, volume 34.
[11] Chwialkowski, K., Strathmann, H., and Gretton, A. (2016). A kernel test of goodness of ﬁt. In Proceedings of the 33rd International Conference on Machine Learning, volume 48, pages 2606–2615.
[12] Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory. Wiley-Interscience. [13] Cudeck, R. (2000). Exploratory factor analysis. In Handbook of applied multivariate
statistics and mathematical modeling, pages 265–296. Elsevier. [14] del Barrio, E., Cuesta-Albertos, J. A., Matra´n, C., and Rodriguez-Rodriguez, J. M. (1999).
Tests of goodness of ﬁt based on the l2-wasserstein distance. Annals of Statistics, 27(4):1230– 1239. [15] Edelman, A., Arias, T. A., and Smith, S. T. (1998). The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303–353. [16] Fothergill, S., Mentis, H., Kohli, P., and Nowozin, S. (2012). Instructing people for training gestural interactive systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, page 1737–1746. Association for Computing Machinery. [17] Fournier, N. and Guillin, A. (2015). On the rate of convergence in wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3):707–738. [18] Genevay, A. (2019). Entropy-regularized optimal transport for machine learning. PhD thesis, Paris Sciences et Lettres (ComUE). [19] Genevay, A., Chizat, L., Bach, F., Cuturi, M., and Peyre´, G. (2019). Sample complexity of sinkhorn divergences. In Proceedings of the Twenty-Second International Conference on Artiﬁcial Intelligence and Statistics, volume 89, pages 1574–1583. [20] Genevay, A., Peyre´, G., and Cuturi, M. (2018). Learning generative models with sinkhorn divergences. In Proceedings of the Twenty-Second International Conference on Artiﬁcial Intelligence and Statistics, volume 84, pages 1608–1617.

20
[21] Gin, E. and Nickl, R. (2015). Mathematical Foundations of Inﬁnite-Dimensional Statistical Models. Cambridge University Press, USA.
[22] Good, P. (2013). Permutation tests: a practical guide to resampling methods for testing hypotheses. Springer Science & Business Media.
[23] Gretton, A., Borgwardt, K. M., Rasch, M. J., Scho¨lkopf, B., and Smola, A. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13:723–773.
[24] Gretton, A., Fukumizu, K., Harchaoui, Z., and Sriperumbudur, B. K. (2009). A fast, consistent kernel two-sample test. In Advances in Neural Information Processing Systems, volume 22, pages 673–681.
[25] Gyo¨rﬁ, L. and Van Der Meulen, E. C. (1991). A Consistent Goodness of Fit Test Based on the Total Variation Distance, pages 631–645. Springer Netherlands, Dordrecht.
[26] Hildreth, C. (1957). A quadratic programming procedure. Naval Research Logistics Quarterly, 4(1):79–85.
[27] Hotelling, H. (1931). The generalization of student’s ratio. Annals of Mathematical Statistics, 2(3):360–378.
[28] HQuang, M., Bazzani, L., and Murino, V. (2013). A unifying framework for vector-valued manifold regularization and multi-view learning. In Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 100–108.
[29] Hu, J., Liu, X., Wen, Z., and Yuan, Y. (2019). A brief introduction to manifold optimization. arXiv preprint arXiv:1906.05450.
[30] Huang, M., Ma, S., and Lai, L. (2021). A riemannian block coordinate descent method for computing the projection robust wasserstein distance. arXiv preprint arXiv:2012.05199.
[31] Jiang, B., Ma, S., So, A. M.-C., and Zhang, S. (2017). Vector transport-free svrg with general retraction for riemannian optimization: Complexity analysis and practical implementation. arXiv preprint arXiv:1705.09059.
[32] Jitkrittum, W., Szabo´, Z., Chwialkowski, K., and Gretton, A. (2016). Interpretable distribution features with maximum testing power. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 181–189.
[33] Jolliffe, I. (1986). Principal Component Analysis. Springer Verlag. [34] Jr., F. J. M. (1951). The kolmogorov-smirnov test for goodness of ﬁt. Journal of the
American Statistical Association, 46(253):68–78.

21
[35] Kadri, H., Rabaoui, A., Preux, P., Duﬂos, E., and Rakotomamonjy, A. (2013). Functional regularized least squares classi cation with operator-valued kernels. arXiv preprint arXiv:1301.2655.
[36] LeCun, Y. and Cortes, C. (2010). MNIST handwritten digit database. [37] Ledoux, M. (1999). Concentration of measure and logarithmic sobolev inequalities. Se´minaire
de probabilite´s de Strasbourg, 33:120–216. [38] Lei, J. (2020). Convergence and concentration of empirical measures under wasserstein
distance in unbounded functional spaces. Bernoulli, 26(1). [39] Lin, T., Fan, C., Ho, N., Cuturi, M., and Jordan, M. (2020). Projection robust wasserstein
distance and riemannian optimization. In Advances in Neural Information Processing Systems, volume 33, pages 9383–9397. [40] Lin, T., Zheng, Z., Chen, E., Cuturi, M., and Jordan, M. (2021). On projection robust optimal transport: Sample complexity and model misspeciﬁcation. In Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130, pages 262–270. [41] Liu, F., Xu, W., Lu, J., Zhang, G., Gretton, A., and Sutherland, D. J. (2020). Learning deep kernels for non-parametric two-sample tests. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 6316–6326. [42] Lloyd, J. R. and Ghahramani, Z. (2015). Statistical model criticism using kernel two sample tests. In Advances in Neural Information Processing Systems, pages 829–837. [43] Lopez-Paz, D. and Oquab, M. (2018). Revisiting classiﬁer two-sample tests. In International Conference on Learning Representations. [44] McDiarmid, C. (1989). On the method of bounded differences, pages 148–188. London Mathematical Society Lecture Note Series. Cambridge University Press. [45] Micchelli, C. A. and Pontil, M. A. (2005). On learning vector-valued functions. Neural Computation, 17(1):177–204. [46] Minh, H. Q. and Sindhwani, V. (2011). Vector-valued manifold regularization. In Proceedings of the 28th International Conference on International Conference on Machine Learning, page 57–64. [47] Mueller, J. and Jaakkola, T. (2015). Principal differences analysis: Interpretable characterization of differences between distributions. In Advances in Neural Information Processing Systems, volume 28. [48] Pfanzagl, J. and Sheynin, O. (1996). Studies in the history of probability and statistics xliv

22
a forerunner of the t-distribution. Biometrika, 83(4):891–898. [49] Poor, H. and Hadjiliadis, O. (2008). Quickest detection. Cambridge University Press. [50] Pratt, J. W. and Gibbons, J. D. (1981). Kolmogorov-Smirnov Two-Sample Tests, pages
318–344. Springer New York, New York, NY. [51] Ramdas, A., Garcia, N., and Cuturi, M. (2017). On wasserstein two-sample testing and
related families of nonparametric tests. Entropy, 19(2). [52] Reddi, S. J., Ramdas, A., PAczos, B., Singh, A., and Wasserman, L. (2015). On the decreasing
power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Proceedings of the 29th AAAI Conference on Artiﬁcial Intelligence, page 3571–3577. [53] Rockafellar, R. T. (1970). Convex analysis. Princeton Mathematical Series. Princeton University Press. [54] Savage, D., Zhang, X., Yu, X., Chou, P., and Wang, Q. (2014). Anomaly detection in online social networks. Social networks, 39:62–70. [55] Schober, P. and Vetter, T. (2019). Two-sample unpaired t tests in medical research. Anesthesia and analgesia, 129:911. [56] Scho¨lkopf, B., Herbrich, R., and Smola, A. J. (2001). A generalized representer theorem. In Helmbold, D. and Williamson, B., editors, Computational Learning Theory, pages 416–426. [57] Scho¨lkopf, B., Smola, A., and Mu¨ller, K.-R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299–1319. [58] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press. [59] Wang, J., Gao, R., and Xie, Y. (2021). Two-sample test using projected wasserstein distance. In Proceedings of IEEE International Symposium on Information Theory. [60] Wen, Z. and Yin, W. (2012). A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1):397–434. [61] Xie, L. and Xie, Y. (2021). Sequential change detection by optimal weighted 2 divergence. IEEE Journal on Selected Areas in Information Theory, pages 1–1. [62] Xie, L., Zou, S., Xie, Y., and Veeravalli, V. V. (2021). Sequential (quickest) change detection: Classical results and new directions. IEEE Journal on Selected Areas in Information Theory, 2(2). [63] Young, G. A., Severini, T. A., Young, G. A., Smith, R., Smith, R. L., et al. (2005). Essentials of statistical inference, volume 16. Cambridge University Press.

23

APPENDIX A

PRELIMINARY TECHNICAL RESULTS

Theorem 5 (Pinsker’s Inequality [12]). Consider two discrete probability distributions p = {pi}ni=1

and q = {qi}ni=1, then it holds that

n

pi 1

2

i=1 pi log qi ≥ 2 p − q 1.

Proposition 2 (Lipschitz Properties of Retraction Operator [4]). There exists constants L1, L2

such that the following inequalities hold:

Retrs(ζ) − s ≤ L1 ζ Retrs(ζ) − (s + ζ) ≤ L2 ζ 2.

Inspired from Appendix A.3 in [31], we are able to compute the constants in Proposition 2 explicitly: L1 = 1 and L2 = 12 . The proof is provided below.

Proof.

By deﬁnition, we have that Retrs(ζ) − s

2 2

=

=2

s+ζ

2

−s

s+ζ

2

1 1−
s+ζ 2

= 2 1 − (1 + ζi2)−1/2
i

≤ ζi2 = ζ 22.
i
where the second and the third equality is by using the relation sTζ = 0, and the inequality is based

on the relation 2(1 − (1 + z)−1/2) ≤ z with z = ζ.

i ζi2. Then it holds that Retrs(ζ) − (s + ζ) 2 ≤

Secondly, we can see that

Retrs(ζ) − (s + ζ)

2 2

=

s+ζ

2

− (s + ζ)

s+ζ

2

= (1 − s + ζ 2)2



2

= 1 −

1 + ζi2
i

≤ 14 ζ 42,

24

where the inequality is based on the relation that (1 − (1 + z)1/2)2 ≤ z2/4 with z = i ζi2.

Consequently it holds that

Retrs(ζ) − (s + ζ)

2

≤

1 2

ζ

2.

Theorem 6 (McDiarmid’s Inequality [44]). Let X1, . . . , Xn be independent random variables, where Xi has the support Xi. Let f : X1 ×X2 ×· · ·×Xn → R be any function with the (c1, . . . , cn) bounded difference property, i.e., for i ∈ {1, . . . , n} and for any (x1, . . . , xn), (x1, . . . , xn) that differs only in the i-th corodinate, we have

|f (x1, . . . , xn) − f (x1, . . . , xn)| ≤ ci.

Then for any t > 0, we have

2t2 Pr |f (X1, . . . , Xn) − E[f (X1, . . . , Xn)]| ≥ t ≤ 2 exp − ni=1 c2i .

Lemma 3 (Equivalent Deﬁnition for Sub-Gaussian variables (Lemma 2.3.2 in [21])). Assume

that E[ζ] = 0 and

t2 P{|ζ| ≥ t} ≤ 2C exp − 2σ2 , t > 0,

for some C ≥ 1 and σ > 0. Then the random variable ζ is sub-Gaussian with constant

σ˜2 = 12(2C + 1)σ2.

Theorem 7 (Poincare’s Inequality). Denote by µn the product of µ on ⊗ni=1Rd and µ ∈ P(Rd)

satisﬁes the Poincare’s inequality, i.e., there exists M > 0 for X ∼ µ so that Var[f (X)] ≤

M E[ ∇f (X) 2] for any f satisfying E[f (X)2] < ∞ and E[ ∇f (X) 22] < ∞. Consider a function

f on ⊗ni=1Rd satisfying E|f (X)| < ∞ and

n i=1

∇if (X) 2 ≤ α2, and max1≤i≤n

∇if (X)

≤

β almost surely. Then the following inequality holds for X ∼ µn:

Pr f (X) − E[f (X)] > t ≤ exp − 1 min(t/β, t2/α2) . K

25

APPENDIX B INTRODUCTION TO MANIFOLD OPTIMIZATION A brief introduction to manifold optimization can be found in [29]. In this section we list some related operators for solving manifold optimization problems. Traditional manifold optimization concerns with solving the following problem:

min f (x),

(7)

x∈M

where M is a Riemannian manifold and f is a real-valued function on M. A tangent vector ζx to M at a point x is deﬁned as a mapping so that there exists a curve γ on M satisfying

d(u(γ(t))) γ(0) = x, ζx[u] = dt |t=0, ∀u ∈ E(M),

where E(M) stands for the collection of real-valued functions deﬁned in a neighborhood of x.

Denote by TxM as the collection of all tangent vectors to M at a point x, which is called the tangent space to M at x. Deﬁne Px(z) as the projection of z into the tangent space at x. Based

on deﬁnitions listed above, we can deﬁne necessary operators for manifold optimization. The

Riemannian gradient of f at x is denoted as Gradf (x), which can be obtained by projecting the

gradient of f at x in the Euclidean space into the tangent space to M at x:

Gradf (x) = Px(∇f (x)).

Typical Riemannian manifolds include the Sphere and Stiefel manifold deﬁned as follows:

Sphere(n − 1) := {x ∈ Rn : x 2 = 1}, St(n, p) := {X ∈ Rn×p : XTX = Ip}.

We can express the tangent space together with the projection operator for these two types of manifolds in analytical form:

TxSphere(n − 1) = {z : zTx = 0}, Px(z) = (I − xxT)z

TxSt(n, p) = {Z : ZTX + XTZ = 0},

XTZ + ZTX PX(Z) = Z − X 2 .

When using ﬁrst-order methods to solve a manifold optimization problem, one also needs to deﬁne

the retraction operator associated with M, which is denoted as Retr. It is a smooth mapping

from the tangent budle ∪x∈MTxM to M satisfying that for any x ∈ M,

• Retrx(0x) = x, where 0x denotes the zero element in TxM;

26

• limζ∈TxM,ζ→0 Retrx(ζ)ζ−(x+ζ) = 0.

When M is a sphere, we choose the following retraction operator which can be implemented

efﬁciently:

x+ζ Retrx ζ = x + ζ ,

x ∈ Sphere(n − 1).

See [15] and [60] for discussions of retraction operators on the Stiefel manifold. The general

iteration update of ﬁrst-order methods for manifold optimization problem can be expressed as

xt+1 = Retrxt(−τ tζt),

where τ t is a well-deﬁned step size and ζt is the Riemannian gradient at xt. The computation of the projected Wasserstein distance relates to the optimization on a Stiefel manifold, while the computation of the KPW distance relates to the optimization on a sphere. A recent paper [4] investigated the Riemannian gradient methods that are guaranteed to converge into stationary points globally, the key proof technique of which relies on Proposition 2. We follow the similar proof idea to establish the convergence analysis for computing the KPW distance.

APPENDIX C TECHNICAL PROOFS IN SECTION II Proof of Remark 1. When taking the kernel function K(x, y) = x, y , the space

F = {a : aTa ≤ 1}.

Note that the cost function c(x, y) =

x−y

2 2

satisﬁes

c(mx, my) = m2c(x, y)

for

any

m ∈ R.

Hence we can argue that the maximizer of the KPW distance is obtained when aTa = 1, i.e.,

KPW (µ, ν) = max W (f #µ, f #ν) . f : RD→R, f (z)=aTz,aTa=1
This indicates that the KPW distance reduces into the PW distance.

Proof of Proposition 1. It is easy to see that µ = ν implies KPW (µ, ν) = 0. Now we show the converse. For ﬁxed x ∈ X , y ∈ Rd and a distribution µ, deﬁne the operator Kµ with the action y as a mapping Kµy : X → Rd so that

Kµy(x ) = (Kxy)(x ) dµ(x) = K(x , x)y dµ(x).

When KPW (µ, ν) = 0, we can see that

f #µ = f #ν, ∀f ∈ F,

27

which implies

0 = sup
f : f 2H≤1

Ef#µ[x] − Ef#ν[y] 2

= sup sup Eµ[ f (x), a ] − Eν[ f (y), a ]
f : f 2H≤1 a: a 2≤1

= sup sup Eµ[ f, Kxa H] − Eν[ f, Kya H]
f : f 2H≤1 a: a 2≤1

= sup sup f, (Kµ − Kν)a
f : f 2H≤1 a: a 2≤1

= sup (Kµ − Kν)a H.
a: a 2≤1

Equivalently, (Kµ − Kν)a H = 0 for any a so that a 2 ≤ 1. Since H is a Hilbert space, we imply that (Kµ − Kν)a is a zero function for any a satisfying a 2 ≤ 1. For any function f ∈ C(X), we make the expansion

Eµ[f (x)] − Eν[f (y)] 2 ≤ Eµ[f (x)] − Eµ[g(x)] 2 + Eµ[g(x)] − Eν[g(y)] 2 + Eν[g(y)] − Eν[f (y)] 2 . The ﬁrst term satisﬁes that

Eµ[f (x)] − Eµ[g(x)] 2 ≤ Eµ[ f (x) − g(x) 2] < ε, and the third term can be upper bounded likewise. For the second term, we have that

Eµ[g(x)] − Eν[g(y)] 2
= sup Eµ[ g(x), a ] − Eν[ g(y), a ]
a: a 2≤1
= sup Eµ[ g, Kxa ] − Eν[ g, Kya ]
a: a 2≤1
= sup g, (Kµ − Kν)a = 0,
a: a 2≤1
where the last equality is because that (Kµ −Kν)a is a zero function for any a satisfying a 2 ≤ 1. Hence, Eµ[f (x)] − Eν[f (y)] 2 < 2ε for any ε > 0 and f ∈ Cb(X ). Then we conclude that the distribution µ = ν.

28

APPENDIX D TECHNICAL PROOFS IN SECTION III

A. Deviation of Duality Reformulation (5)

We ﬁrst present the proof of the dual reformulation of the inner minimization problem in (4). By deﬁnition, the primal formulation can be expressed as:

1

1

min

πi,jci,j − η πi,j(log πi,j − 1) :

πi,j = , πi,j =

.

(8)

π≥0

n

m

i,j

i,j

j

i

The Lagrangian function becomes

L(π, u, v) = πi,jci,j −η πi,j(log πi,j −1)+ ui

i,j

i,j

i

Then the dual problem becomes

1

πi,j − n + vj

j

j

1 πi,j − m
i

max min L(π, u, v)

u,v

π≥0

1

= max −

u,v

n

1

ui − m

vj + min π≥0

i

j

πi,j ci,j + ui + vj − ηπi,j(log πi,j − 1)
i,j

1

1

= max −

u,v

n

ui − m

vj −

max
πi,j ≥0

i

j

i,j

−πi,j ci,j + ui + vj + ηπi,j(log πi,j − 1)

1

= max −

u,v

n

1 ui − m

vj − (ηφ)∗(ui + vj + ci,j)

i

j

i,j

1 = max −

1 ui −

vj − η exp − ui + vj + ci,j

u,v n i m j i,j η

where φ(w) = w log w − w and φ∗ denotes its conjugate [53]. Moreover, the dual optimal

value equals the primal optimal value because the Slater’s condition [5] for ﬁnite-dimensional

optimization is satisﬁed. Take ui = −ui/η and vj = −vj/η, the dual problem becomes

η

η

ci,j

max u ,v n

ui + m

vj − η

exp

− η

+ ui + vj

.

i

j

i,j

Therefore, the whole problem (4) becomes

η max

η ui +

vj − η exp − ci,j + ui + vj .

u,v,s n i m j i,j η

Or equivalently, we write it as the minimization problem:

1 −η × min −

1 ui −

vj + η exp − ci,j + ui + vj .

u,v,s n i m j i,j η

29

Remark 4. By adding the entropic regularization term ηH(π), we are able to derive an unconstrained optimization formulation on the sphere, thus reducing the computational cost for computing KPW distance. Besides, the induced optimal transport mapping between projected samples is usually stochastic instead of deterministic, which is robust to potential data outliers.

B. Proof of Theorem 1

Assume that fˆ is an optimal solution to the problem (2). Let S be the subspace

S=

nm
(Kxi − Kyj )ai,j : ai,j ∈ Rd .
i=1 j=1

Denote by S⊥ the orthogonal complement of S. Given a set X , denote by fX a function that lies

in the set X . Then by the projection theorem, there exists fˆS and fˆS⊥ such that fˆ = fˆS + fˆS⊥

and

fˆ

2 H

=

fˆS

2 H

+

fˆS⊥ 2H. It remains to show that fˆS shares the same objective value with

fˆ. For ﬁxed i, j, we have that

fˆ(xi) − fˆ(yj) 2 = max fˆ(xi) − fˆ(yj), ai,j ai,j : ai,j 2≤1

= max fˆ(xi), ai,j − fˆ(yj), ai,j ai,j : ai,j 2≤1

= max fˆ, Kxiai,j − fˆ, Kyj ai,j ai,j : ai,j 2≤1

= max fˆ, (Kxi − Kyj )ai,j ai,j : ai,j 2≤1

= max fˆS, (Kxi − Kyj )ai,j = fˆS(xi) − fˆS(yj) 2, ai,j : ai,j 2≤1

where the second last equality is because fˆS⊥ is orthogonal to the subspace S. It follows that

fˆ(xi) − fˆ(yj)

2 2

=

fˆS(xi) − fˆS(yj) 22. Therefore, there always exists an optimal solution that

lies in the subspace S, which means that there exists an optimal solution to (2) that admits the

following expression:

n
fˆ =

m
(Kxi − Kyj )ai,j.

i=1 j=1

Deﬁning ax,i =

m j=1

ai,j

and

ay,j

=

n i=1

ai,j

completes

the

proof.

Remark 5. From the proof we can also see that the representer theorem holds if replacing the square of the 2 norm in (2) with any p-th power of the 2 norm for p ≥ 2. However, we ﬁnd the development of optimization algorithms for the square of the 2 norm case is the simplest.

30

C. Proof of Theorem 2
In the following we give a iteration complexity analysis about Algorithm 2, the proof of which largely follows the idea in [30]. In particular, we ﬁrst establish the descent lemma for the update of each block of variables and then argue that the objective function is lower bounded. Based on these two facts, we ﬁnally build the iteration complexity result for Algorithm 2.

Lemma 4 (Lipschitzness of ∇sF (u, v, s)). Let {ut, vt, st}t be the sequence generated from Algorithm 2. The following inequality holds for any s ∈ Sd(n+m)−1 and λ ∈ [0, 1]:

where

∇sF (ut+1, vt+1, λs + (1 − λ)st) − ∇sF (ut+1, vt+1, st) ≤ λ st − s , = 2 AηU 2∞ + 4 AηU2 4∞ and AU ∞ = maxi,j Ai,j U 2.

Proof of Lemma 4. An intermediate result is that

πi,j(ut+1, vt+1, st) =
i

exp
i

− η1 ci,j[st] + uti+1

exp vjt+1

= exp − η1 ci,j[st] + uti+1 exp vjt i

1/m i πi,j(ut+1, vt, st)

1 =
m

i πi,j(ut+1, vt, st) = 1/m. i πi,j(ut+1, vt, st)

Then we can assert that i,j πi,j(ut+1, vt, st) = 1. For ﬁxed st, deﬁne sλ = λs + (1 − λ)st. Then

we have that

∇sF (ut+1, vt+1, st) − ∇sF (ut+1, vt+1, sλ)

= η2 πi,j(ut+1, vt+1, st)U TATi,jAi,jU st − πi,j(ut+1, vt+1, sλ)U TATi,jAi,jU sλ

i,j

i,j

≤ η2 πi,j(ut+1, vt+1, st)U TATi,jAi,jU (st − sλ) i,j

+ η2 U T πi,j(ut+1, vt+1, st) − πi,j(ut+1, vt+1, sλ) ATi,jAi,jU i,j

≤ η2 πi,j(ut+1, vt+1, st)U TATi,jAi,jU sλ − st i,j

+ η2 πi,j(ut+1, vt+1, st) − πi,j(ut+1, vt+1, sλ) U TATi,jAi,jU i,j

31

where the ﬁrst inequality is based on the constraint that sλ ≤ λ s + (1 − λ) st = 1. To upper bound the ﬁrst term, we ﬁnd

πi,j (ut+1, vt+1, st)U TATi,j Ai,j U
i,j
≤ πi,j(ut+1, vt+1, st) U TATi,jAi,jU 2 ≤ max Ai,jU 22. i,j i,j
To bound the second term, we ﬁnd that

πi,j(ut+1, vt+1, st) − πi,j(ut+1, vt+1, sλ) U TATi,jAi,jU
i,j

where

≤ max

Ai,j U

2 2

π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st)

1,

i,j

π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st) 1 :=

πi,j(ut+1, vt+1, sλ) − πi,j(ut+1, vt+1, st) .

i,j

Denote by H(π, s; η) the objective function for (3). Based on the strong convexity property, we

have that

∇πH(π(ut+1, vt+1, sλ), sλ; η) − ∇πH(π(ut+1, vt+1, st), sλ; η), π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st)

≥η

π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st)

2 1

Moreover, by simple calculation we ﬁnd

∇πH(π(u, v, s), s) = [ci,j + η log(πi,j(u, v, s))]i,j = [η(ui + vj)]i,j ,
where the second equality is by substituting the formulation of πi,j(u, v, s). Hence, we ﬁnd that the gradient ∇πH(π(u, v, s), s) only depends on u and v, which implies
∇πH(π(ut+1, vt+1, st), st; η) − ∇πH(π(ut+1, vt+1, st), sλ; η), π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st) ≥η π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st) 21.

It follows that

η π(ut+1, vt+1, sλ) − π(ut+1, vt+1, st) 1

≤ ∇πH(π(ut+1, vt+1, st), st; η) − ∇πH(π(ut+1, vt+1, st), sλ; η) ∞

= max

Ai,jU sλ

2 2

−

Ai,jU st

2 2

i,j

≤2 max

Ai,j U

2 2

sλ − st

.

i,j

32

where the inequality is by applying the following relation:

Ax1

2 2

−

Ax2

2 2

=

(x1

−

x2)T(ATAx1)

+

xT2 ATA(x1

−

x2)

≤ x1 − x2 ATAx1 + xT2 ATA x1 − x2

≤ 2 A 2 x1 − x2 .

In summary, the second term can be upper bounded as

πi,j(ut+1, vt+1, st) − πi,j(ut+1, vt+1, sλ) U TATi,jAi,jU
i,j
≤ 2 (maxi,j Ai,jU 22)2 sλ − st . η
Then applying the condition that sλ − st = λ s − st completes the proof.

Lemma 5 (Decrease of F in s). Let {ut, vt, st}t be the sequence generated from Algorithm 2.

The following inequality holds for any k ≥ 1:

F (ut+1, vt+1, st+1) − F (ut+1, vt+1, st) ≤ −

1

ξt+1 2.

8 AU 2∞L2/η + 2 L21

Proof of Lemma 5. Note that

F (ut+1, vt+1, st+1) − F (ut+1, vt+1, st) − ∇tF (ut+1, vt+1, st), st+1 − st

1

=

∇sF (ut+1, vt+1, λst+1 + (1 − λ)st) − ∇sF (ut+1, vt+1, st), st+1 − st dλ

0

1

≤

∇sF (ut+1, vt+1, λst+1 + (1 − λ)st) − ∇sF (ut+1, vt+1, st) st+1 − st dλ

0

1

≤

λ st+1 − st 2 dλ

0

= st+1 − st 2 =

2

2

≤ τ 2L21 ξt+1 2. 2

Retrst

− τ ξt+1

− st 2

where the second inequality is by applying Lemma 4, and the last inequality is by applying

33

Proposition 2. Moreover, we have that

∇sF (ut+1, vt+1, st), st+1 − st

= ∇sF (ut+1, vt+1, st), −τ ξt+1 + ∇sF (ut+1, vt+1, st), Retrst − τ ξt+1 − (st − τ ξt+1)

≤ − τ ξt+1 2 + ∇sF (ut+1, vt+1, st) 2 Retrst − τ ξt+1 − (st − τ ξt+1)

≤ − τ ξt+1 2 + ζt+1 2 · L2τ 2 ξt+1 2

≤ − τ ξt+1 2 + 2 AU 2∞L2τ 2 ξt+1 2. η

Combining those inequalities above implies that

k+1 k+1 k+1

k+1 k+1 k

2 AU 2∞L2

2

F (u , v , t ) − F (u , v , t ) ≤ −τ 1 −

η

+ 2 L1 τ

Taking τ =

1
2

2 gives the desired result.

4 AU ∞L2/η+ L1

ξt+1 2.

Lemma 6 (Decrease of F in v). Let {ut, vt, st}t be the sequence generated from Algorithm 2.

The following inequality holds for any k ≥ 1:

F (ut+1, vt+1, st) − F (ut+1, vt, st) ≤ − 21 1/m − π(ut+1, vt, st)T1 21.

where

1/m − π(ut+1, vt, st) 1 =
j

1 −
m

πi,j(ut+1, vt, st) .

i

Proof of Lemma 6. According to the expression of F , we have that

F (ut+1, vt+1, st) − F (ut+1, vt, st)

1m

= πi,j(ut+1, vt+1, st) − πi,j(ut+1, vt, st) +

(vjt − vjt+1)

m

i,j

i,j

j=1

1m

1m

=

(vjt − vjt+1) = −

log

m

m

j=1

j=1

1/m ,
i πi,j(ut+1, vt, st)

where the second equality is because that

πi,j(ut+1, vt+1, st) = m1 ,
i
πi,j(ut+1, vt, st) = n1 .
j

Therefore, applying the Pinsker’s inequality in Theorem 5 implies that

F

(ut+1,

vt+1,

st)

−

F

(ut+1,

vt,

st)

≤

1 −

2

2

m1 − πi,j(ut+1, vt, st) .

j

i

34

Lemma 7 (Decrease of F in u). Let {ut, vt, st}t be the sequence generated from Algorithm 2. The following inequality holds for any t ≥ 1:

F (ut+1, vt, st) − F (ut, vt, st) ≤ − 21 1/n − π(ut, vt, st)1 22.

where

1/n − π(ut, vt, st)1

2 2

=

i

2
n1 − πi,j(uk, vk, tk) . j

Proof of Lemma 7. For ﬁxed i ∈ [n], deﬁne

hi = πi,j(ut+1, vt, st) − πi,j(ut, vt, st) − n1 log

j

j

According to the expression of F ,

1/n j πi,j(ut, vt, st)

F (ut+1, vt, st) − F (ut, vt, st) = hi,
i
and it sufﬁces to provide an upper bound for hi, i ∈ [n]. By substituting the expression of ut+1

into hi, we have that

hi = πi,j(ut, vt, st)
j

1/n

1

− 1 − log

j πi,j(ut, vt, st)

n

1/n j πi,j(ut, vt, st)

1 =−

π(ut, vt, st)1

1 − log

1/n

n i n π(ut, vt, st)1 i

Deﬁne the function

(x) = 1 − x − 1 log 1/n + (x − 1/n)2.

n

nx

We can see that this function attains its maximum at x = 1/n, with

hi ≤ −

π(ut, vt, st)1 − 1 2 . in

(1/n) = 0. It follows that

The proof is completed.

Lemma 8. Let {ut, vt, st}t be the sequence generated from Algorithm 2, which is terminated when the following conditions hold:

ξt+1 ≤ 1,

1/n − π(ut, vt, st)1 2 ≤ 2 ,

4

AU

2 ∞

1/m − π(ut+1, vt, st)T1 1 ≤

2.

4

AU

2 ∞

Then {uT , vT , sT } is an ( 1, 2) stationary point of (5).

35

Proof of Lemma 8. The condition ξt+1 ≤ 1 directly implies that GradsF (uT , vT , sT ) ≤ 1.

Suppose that

π(uT , vT , sT )1 = r, π(uT , vT , sT )T1 = c,

where 1/n − r 2 ≤ 2/(4 AU 2∞) and 1/m − c 1 ≤ 2/(4 AU 2∞). Then we ﬁnd that

F (uT , vT , sT ) = min π

πi,jMi,j − ηH(π) :
i,j

and

min F (u, v, sT ) = min

u,v

π

πi,jMi,j − ηH(π) :
i,j

where Mi,j = Ai,jU sT 22. It follows that

πi,j = ri, πi,j = cj ,

j

i

1

1

πi,j = n , πi,j = m ,

j

i

F (uT , vT , sT ) − min F (u, v, sT ) u,v

≤η log(mn) + 2 1/m − c1

1×

AU

2 ∞

≤

2,

where the last inequality is by taking η = 2/(2 log(mn)).

Lemma 9 (Lower Boundedness of F ). Denote by (u∗, v∗, s∗) the global optimum of (5). Then

we have that

F (u∗, v∗, s∗) ≥ 1 − η1 AU 2∞.

Proof of Lemma 9. It is easy to show that

πi,j(u∗, v∗, s∗) = 1.
i,j

Moreover, for any (i, j), we have that ci,j ≤ AU 2∞. It follows that

exp − η1 AU 2∞ + u∗i + vj∗ ≤ πi,j ≤ 1,

and

therefore

u∗i

+

vj∗

≤

1 η

AU

2 ∞

for

any

(i, j).

Hence

we

conclude

that

1n

1m

1

πi,j(u∗, v∗, s∗) −

ui −

vj ≥ 1 − AU 2∞.

n

m

η

i,j

i=1

j=1

36

In the following we give a re-statement of Theorem 2 and the formal proof.

Theorem (Re-statement of Theorem 2). Choose parameters

1

τ= 4 AU

2 L /η +

L2 ,

∞2

1

η=

2,

2 log(mn)

= 2 AU 2∞ + 4 AU 4∞ ,

η

η2

and Algorithm 2 terminates when

ξt+1 ≤ 1,

1/n − π(ut, vt, st)1 2 ≤ 2 ,

4

AU

2 ∞

1/m − π(ut+1, vt, st)T1 1 ≤

2.

4

AU

2 ∞

We say that (uˆ, vˆ, sˆ) is a ( 1, 2)-stationary point of (5) if

GradsF (uˆ, vˆ, sˆ) ≤ 1,

F (uˆ, vˆ, sˆ) − min F (u, v, sˆ) ≤ 2, u,v

where GradsF (u, v, s) denotes the partial derivative of F with respect to the variable s on the

sphere Sd(n+m)−1. Then Algorithm 2 returns an ( 1, 2)-stationary point in iterations

11

T = O log(mn) · 3 + 2 .

2

12

Proof of Theorem 2. We can build the one-iteration descent result based on Lemma 5, Lemma 6,

and Lemma 7:

F (ut+1, vt+1, st+1) − F (ut, vt, st)

1 ≤−
2 1 =− 2

1/n − π(ut, vt, st)1 2 + 1 1/m − π(ut+1, vt, st)T1 2 +

1

ξt+1 2

22

1 8 AU 2∞L2/η + 2 L21

2

ttt 2

t+1 t t T 2

η2 ζt+1 2

1/n − π(u , v , s )1 2 + 1/m − π(u , v , s ) 1 1 + 2 AU 2 η(2L + L2) + 4 AU 4 L2

∞

2

1

∞1

Then we have that

F (uT , vT , sT ) − F (u0, v0, s0)

1 T −1 ≤−
2 t=0

ttt 2

t+1 t t T 2

η2 ζt+1 2

1/n − π(u , v , s )1 2 + 1/m − π(u , v , s ) 1 1 + 2 AU 2 η(2L + L2) + 4 AU 4 L2

∞

2

1

∞1

1

1

≤ − · min 2

1, 2

AU

2 η(2L

+ L2) + 4

AU

4 L2

∞

2

1

∞1

T −1
×
t=0

1/n − π(ut, vt, st)1

2 2

+

1/m − π(ut+1, vt, st)T1

2 1

+

η2

ξt+1

2 2

1

1

≤ − T · min 2

1, 2

AU

2 η(2L

+ L2) + 4

AU

4 L2

∞

2

1

∞1

· min

2

2

21, 16

2
AU

4

, 16

2
AU

4

.

∞

∞

37

Therefore,

T ≤[F (u0, v0, t0) − F (uT , vT , sT )] max 2, 4 AU 2∞η(2L2 + L21) + 8 AU 4∞L21

max

1

16

AU

4 ∞

16

AU

4 ∞

2,

2,

2

1

2

2

0 00

AU

2 ∞

2

2

42

≤ F (u , v , t ) − 1 + η

max 2, 4 AU ∞η(2L2 + L1) + 8 AU ∞L1

max

1

16

AU

4 ∞

16

AU

4 ∞

2,

2,

2

1

2

2

11

=O log(mn) · 3 + 2 .

2

12

38

APPENDIX E TECHNICAL PROOFS IN SECTION IV

A. Proof of Theorem 3

Proof of Lemma 1. Denote F = {f ∈ H : f H ≤ 1}. By the bias-variation decomposition, we have that

E[(KPW (µˆn, µ))1/p] ≤ sup E[(W (f #µˆn, f #µ))1/p]
f ∈F
+ E sup (W (f #µˆn, f #µ))1/p − E[(W (f #µˆn, f #µ))1/p] .
f ∈F
For ﬁxed f ∈ F, we can see that

E[(W

(f #µˆn,

f #µ))1/p]

≤

c n− 1 p (2p)∨d

(log

n)ζp,d/p

where cp is a constant depending only on p and

 1, ζp,d =
0,

if d = 2p, otherwise.

Now we start to upper bound the variation term. Deﬁne the empirical process

Xf = (W (f #µˆn, f #µ))1/p − E[(W (f #µˆn, f #µ))1/p].

It is easy to see that E[Xf ] = 0. Moreover, we can show that for ﬁxed f , the random variable Xf is sub-exponential. Denote by Z = {zi}ni=1 and Z = {zi}ni=1 i.i.d. samples from f #µ. Take g(Z) = (W (f #µˆn, f #µ))1/p. Then we have that
|g(Z) − g(Z(i))| ≤ (W (f #µˆn, f #µˆn))1/p ≤ n−1/(2∨p) Z − Z 2.

It follows that

n
∇ig(Z) 2 ≤ n−2/(2∨p),
i=1

max ∇ig(Z)
1≤i≤n

Then the Poincare’s inequality in Theorem 7 implies that

≤ n−1/p.

Pr{Xf ≥ t} ≤ exp −K−1 min{tn1/p, t2n2/(2∨p)} .

Hence we conclude that Xf is sub-exponential with parameters ( K/2n−1/(2∨p), (K/2)n−1/p). For the function space F, deﬁne the corresponding metric

d(f, f ) = f − f H.

39

Let X ∼ µ. Then for any f, f ∈ F, we have that

|Xf − Xf |

≤E (W (f #µˆn, f #µˆn))1/p + (W (f #µ, f #µ))1/p + E (W (f #µˆn, f #µˆn))1/p + (W (f #µ, f #µ))1/p

≤2 (E f (X) − f (X) p2)1/p +

1n n i=1

f (Xi) − f

(Xi)

p 2

1/p



+E

1n n i=1

f (Xi) − f

(Xi)

p 2

1/p .

Note that the following upper bound holds for any f, f ∈ F and x ∈ RD:

f (x) − f (x) 2 = max f (x) − f (x), a a: a 2≤1

= max f (x), a − f (x), a
a: a 2≤1

= max f, Kxa HK − f , Kxa HK a: a 2≤1

= max f − f , Kxa HK a: a 2≤1

≤ f − f HK × max Kxa HK a: a 2≤1

= f − f HK × max a: a 2≤1
√ = B f − f HK .

aTK(x, x)a

As a consequence, substituting this upper bound into the relation above implies that √
|Xf − Xf | ≤ 4 Bd(f, f ).

Applying the -net argument similar to the Dudley’s entropy integral bound [58, Theorem 5.22]

gives
E sup Xf
f ∈F

√√ ≤ inf 4 B + 2Kn−1/(2∨p)
>0

log N (F , d, ) + (K/2)n−1/p log N (F , d, )

Taking N (F , d, ) = 1 and = n−1/p implies that

E sup Xf
f ∈F

n−1/(2∨p) log(n) + n−1/p log(n).

Proof of Lemma 2. We start to upper bound the variance term (KPW (µˆn, µ))1/p − E[(KPW (µˆn, µ))1/p].

40

Denote by X = {xi}ni=1 and X = {xi}ni=1 i.i.d. samples from µ, and let g(X) = (KPW (µˆn, µ))1/p.

Based on the triangular inequality, we ﬁnd that

|g(X) − g(X )| ≤ n−1/p ≤ n−1/p

n

1/p

max f (xi) − f (xi) 2
f ∈F i=1

n

1/p

L xi − xi

i=1

≤ n−1/(2∨p)L1/p X − X .

It follows that

n
∇ig(Z) 2 ≤ n−2/(2∨p)L2/p,
i=1

max ∇ig(Z)
1≤i≤n

Then the Poincare’s inequality in Theorem 7 implies that

≤ n−1/pL1/p.

Pr{ (KPW (µˆn, µ))1/p − E[(KPW (µˆn, µ))1/p] ≥ t} ≤ exp −K−1 min{tn1/pL−1/p, t2n2/(2∨p)L−2/p} .

Substituting the right-hand-side with α completes the proof. Proof of Theorem 3. Based on the triangular inequality, we can see that
(KPW (µˆn, νˆm))1/p − (KPW (µ, ν))1/p ≤ (KPW (µˆn, µ))1/p + (KPW (νˆm, ν))1/p . It sufﬁces to upper bound (KPW (µˆn, µ))1/p and (KPW (νˆm, ν))1/p separately. By the biasvariance decomposition,
(KPW (µˆn, µ))1/p ≤ E[(KPW (µˆn, µ))1/p] + (KPW (µˆn, µ))1/p − E[(KPW (µˆn, µ))1/p] ,

where the ﬁrst term quantiﬁes the bias for empirical estimation, and the second term quantiﬁes the variance of estimation. The bias term can be upper bounded by applying Lemma 1, and the variance term can be upper bounded by applying Lemma 2. In summary, with probability at least 1 − α, it holds that

(KPW (µˆn, µ))1/p

max n−1/pK log(1/α), n−1/(2∨p) + n− (2p1)∨d (log n)ζp,d/p + n−1/(2∨p)

K log(1/α) L1/p log(n) + n−1/p log(n).

The upper bound for (KPW (νˆm, ν))1/p can be proceeded similarly.

41

B. Testing Performance

Based on the ﬁnite-sample guarantee in Theorem 3, we are able to characterize the performance of the KPW test. To make the type-I error below than α, we reject the null hypothesis as long as the empirical statistic KPW (µˆn, νˆm) ≥ γm,n, where

γm1/,pn ∼ max N −1/pK log(1/α), N −1/(2∨p) K log(1/α) L1/p

+

N

−

1 (2p)∨d

(log

N )ζp,d/p

+

N −1/(2∨p)

log(n) + N −1/p log(n).

For the alternative hypothesis, assume that target distributions µ and ν satisfy KPW (µ, ν) > γm,n. Then the type-II error can be upper bounded as

Pr H1 KPW (µˆn, νˆm) < γm,n

= Pr H1 KPW (µˆn, νˆm) − KPW (µ, ν) < γm,n − KPW (µ, ν)

= Pr H1 KPW (µ, ν) − KPW (µˆn, νˆm) > KPW (µ, ν) − γm,n

≤ Pr H1 |KPW (µ, ν) − KPW (µˆn, νˆm)| > KPW (µ, ν) − γm,n

E (KPW (µ, ν) − KPW (µˆn, νˆm))2

≤

2

.

KPW (µ, ν) − γm,n

C. Finite-sample Guarantee for p ∈ [1, 2)
In this subsection, we discuss the ﬁnite-sample guarantee for KPW distance with p-Wasserstein distance for p ∈ [1, 2). Note that it is not necessary to rely on the Poincare inequality or projection poincare inequality to obtain the result. We ﬁrst present several technical lemmas before showing the ﬁnal result.

Lemma 10. Based on Assumption 1, for f ∈ {f ∈ H : f H ≤ 1}, we have √
f (x) 2 ≤ B, ∀x ∈ RD.

Proof of Lemma 10. For ﬁxed x ∈ X , the norm of f (x) can be upper bounded as the following:

f (x)

2 2

=

f (x), f (x)

=

f, Kxf (x) H ≤

f

H Kxf (x) H ≤

Kxf (x) H.

42

In particular,

Kxf (x)

2 H

=

Kxf (x), Kxf (x) H

= Kxf (x) f (x), f (x)

= K(x, f (x))f (x), f (x)

= f (x)TK(x, f (x))f (x)

≤B

f (x)

2 2

Combining those two relations above implies the desired result.

Lemma 11. For p ∈ [1, 2), the bias term of empirical KPW distance can be upper bounded as E[(KP W (µˆn, µ))1/p] n− (2p1)∨d (log n)ζp,d/p + n1/2−1/p log(n) + n−1/p.
where ζp,d = 1 if d = 2p and ζp,d = 0 otherwise.

Proof of Lemma 11. Following the similar argument as in Lemma 1, we can see that

E[(KPW (µˆn, µ))1/p] ≤ sup E[(W (f #µˆn, f #µ))1/p]
f ∈F

+ E sup (W (f #µˆn, f #µ))1/p − E[(W (f #µˆn, f #µ))1/p] ,
f ∈F

and the ﬁrst term can also be bounded similarly. To upper bound the second term, deﬁne the

empirical process {Xf } as in Lemma 1. For ﬁxed f , the random variable Xf can be shown

to be sub-Gaussian. Denote by Z = {zi}ni=1 and Z(i) the sample set so that the i-th element is different. Take g(Z) = (W (f #µˆn, f #µ))1/p. Then we have that

|g(Z) − g(Z(i))| ≤ (W (f #µˆn, f #µˆn))1/p ≤ √
≤ n−1/p2 B.

1

1/p p

n f (zi) − f (zi) 2

Therefore, applying the McDiarmid’s inequality in Theorem 6 implies u2
Pr{|Xf | ≥ u} ≤ 2 exp − 2Bn1−2/p .

Applying Lemma 3 implies that for ﬁxed , the random variable Xf is sub-Gaussian with the parameter σ2 = 36Bn1−2/p. Then applying the -net argument similar to the Dudley’s entropy

integral bound [58, Theorem 5.22] gives

E sup Xf
f ∈F

≤ inf >0

√√ 4 B + 36Bn1−2/p

2 log N (F, d, ) .

43

Taking N (F , d, ) = 1 and = n−1/p implies that

E sup Xf
f ∈F

n1/2−1/p log(n) + n−1/p.

Lemma 12. For p ∈ [1, 2), with with probability at least 1 − α, it holds that

(KPW (µˆn, µ))1/p − E[(KPW (µˆn, µ))1/p] ≤ n1/2−1/p

2 2B log .

α

Proof of Lemma 12. Denote by Z = {zi}ni=1 and Z(i) the sample set so that the i-th element is different. Take g(Z) = (KPW (µˆn, µ))1/p. Then we can see that
√ |g(Z) − g(Z(i))| ≤ (KPW (µˆn, µˆn))1/p ≤ n−1/p2 B.

Then applying the McDiarmid’s inequality in Theorem 6 implies

1/p

1/p

u2

Pr (KPW (µˆn, µ)) − E[(KPW (µˆn, µ)) ] ≥ u ≤ 2 exp − 2Bn1−2/p .

Based on Lemma 11 and Lemma 12, we obtain the uncertainty quantiﬁcation result in Theorem 4.

44

APPENDIX F IMPLEMENTATION DETAILS FOR COMPUTING KPW DISTANCE

The variable s is initialized to be a uniform random vector over sphere. The dual variable

v is initialized to be a Gaussian random vector with unit covariance. When updating the

block of variables ut+1 and vt+1, we make the change of variables (u )t+1 = exp(ut+1) and

(v )t+1 = exp(vt+1). We update (u )t+1 and (v )t+1 instead to accelerate the computation:


 (u )t+1 =


 (v )t+1 =


j exp i exp



1/n



− η1 ci,j + (vj)t  i 

1/m



,

− η1 ci,j + (ui)t+1 

j

and we further store the matrix A with Ai,j = exp − η1 ci,j in advance to reduce the computational cost. The transport mapping πt+1 (πi,j(ut+1, vt+1, st))i,j can be formulated without going

through a for loop but only with multiplication operators:

πt+1 = (u )t+1 .* A .* [(v )t+1]T,

where the operator .* means we multiply two objects componentiwisely in terms of array broadcasting. When updating ζt+1, we ﬁrst formulate the matrix V t+1 with

Vit,j+1 =

πit,+j 1ATi,j Ai,j

i,j

and then continue the matrix multiplication procedure in (6i). Denote by Gi the i-th row block

of the gram matrix G, then

V t+1 =

πit,+j 1(Gi + Gn+j)T(Gi + Gn+j)

i,j

i,j

=

πit,+j 1(GTi Gi + GTn+j Gn+j + GTn+j Gi + GTi Gn+j ) .

i,j

i,j

Consequently, we can compute each of the four components in the formula above without

executing double for loops and then sum them up to obtain the matrix V t+1. During the numerical

implementation, we also ﬁnd that the computation is sensitive to the choice of η. This phenomenon

has also been observed when using Sinkhorn’s algorithm to compute Wasserstein distance or

projected Wasserstein distance. When η is too small, the iteration update may have numerical

45
instability issues. When η is too large, the obtained solution is far away from the optimal solution to the original KPW distance. We have tried the best to tune this parameter to make the algorithm maintain the best performance. How to tune this hyper-parameter systematically is left for future works.

46
Fig. 4. Mean computation time for computing KPW (µˆn, νˆn) for varying n. Results are averaged over 10 independent trials.
APPENDIX G DETAILS ABOUT EXPERIMENT A. Sample Complexity In this experiment, we ﬁx hyper-parameters σ2 = 1, ρ = 0.5 for computing KPW distances. The values of empirical KPW distances across different choices of sample size are reported in Figure 1, and the corresponding computation time is reported in Figure 4. From the plot we can see that it is efﬁcient to compute KPW distances with reasonably small sample size n and projected dimension d.
B. Conﬁgurations All methods are implemented using python 3.7 (Pytorch 1.1) on a MacBook Pro labtop with
32GB of memory. When running the code, there is no swapping of memory and the average CPU frequency is 3.2 GHZ. We compute the projected Wasserstein distance based on the ofﬁcial code in https://github.com/fanchenyou/PRW. We run the MMD-O test based on the code in https://github.com/fengliu90/DK-for-TST. We run the MMD-NTK test based on the code in https://github.com/xycheng/NTK-MMD. From extensive experiments we realize that MMD-NTK is the most computationally efﬁcient test, but its power does not scale the best. On the other hand, this method can be useful when performing a test for the large-sampled case, while our method may be intractable to compute in short time. We run the ME test based on the code in https://github.com/wittawatj/interpretable-test.

47

TABLE III AVERAGE TYPE-I ERROR AND STANDARD ERROR FOR TWO-SAMPLE TESTS IN MNIST DATASET ACROSS DIFFERENT CHOICES
OF SAMPLE SIZE.

N
200 250 300 400 500
Avg.

MMD-NTK
0.057±0.0010 0.051±0.0003 0.068±0.0006 0.049±0.0007 0.061±0.0006
0.057

MMD-O
0.056±0.0006 0.060±0.0001 0.055±0.0003 0.058±0.0002 0.054±0.0004
0.056

ME
0.044±0.0003 0.065±0.0002 0.059±0.0007 0.041±0.0002 0.060±0.0002
0.053

PW
0.056±0.0004 0.046±0.0003 0.056±0.0002 0.061±0.0006 0.049±0.0003
0.054

KPW
0.061±0.0005 0.048±0.0002 0.053±0.0001 0.056±0.0006 0.047±0.0004
0.053

C. Implementation of Cross-Validation The candidate choices of hyper-parameters ρ and σ2 are within the set {(ρ, σ2) : σ2 = a · σˆ2 : a ∈ {0.5, 1, 2}, ρ ∈ {0.25, 0.5, 0.75}},
where σˆ2 denotes the empirical median of pairwise distances between observations. To choose ρ and σ2, we further split the training set into the training and validation dataset, which contain 70% and 30% data, respectively. For each choice of hyper-parameters we use the training dataset to obtain a nonlinear projector and examine its hold-out performance on the validation dataset, which is quantiﬁed as the negative of the p-value for two-sample tests between two collection of samples in the validation dataset. We choose hyper-parameters ρ and σ2 with the best hold-out performance.
D. Tests for Synthetic Datasets When studying tests on Gaussian distributions, we take both the training and testing sample sizes
N to be 50. When reproducing the experiments corresponding to the left two ﬁgures in Fig. 3, we take the dimension D ∈ {20, 40, 60, 80, 100, 120, 140, 160}. When reproducing the experiments corresponding to the right two ﬁgures, we take the sample size n = m ∈ {80, 100, 140, 180, 250}.
E. Tests for MNIST handwritten digits Table III present the type-I error for various tests in MNIST dataset, from which we can see
that all tests have the type-I error close to α = 0.05.

48
F. Human activity detection The pre-processing of data is as follows. We ﬁrst remove frames in which the person is
standing still or with little movements. Then we delete the ﬁrst few frames to make the action of bending consist of 500 frames. Next we delete the last few frames to make the action of throwing consist of 355 frames. We take the window size W = 100. To perform online change point detection, we pre-train a nonlinear projector using the data before time index 300 and compute the null statistics for many times to obtain the true threshold. Then we compute the detection statistic by comparing the distribution between the block of data before time 300 and the data from the sliding window. We reject the null hypothesis and claim a change is happened if the statistic is above the threshold. The plot of the detection statistic over time after the time index 400 is presented in Fig. 5, and the delay detection time corresponding to all users are reported in Table II.
APPENDIX H IMPACT OF HYPER-PARAMETERS A. Impact of Projected Dimension d We prefer to choose the projected dimension d with relatively small values since the testing statistic will have poor sample complexity rate and is expansive to compute for large d. In this section, we examine the testing performance for different choices of d. In particular, we perform the KPW test on Gaussian distributions (with diagonal covariance matrices, D = 128 and n = m = 50) and Gaussian mixture distributions (with D = 100 and n = m = 100) following the setup in Section V-A, the results of which are reported in Fig. 6. From the plot we can see that the testing power is generally better for d > 1, which suggests that using vector-valued RKHS is better than using classical scalar-valued RKHS. Moreover, we observe the performance is insensitive to the choice of d as long as we take d > 1.
B. Impact of Entropic Regularization Parameter η As pointed out in [19], the entropic regularization in (4) could alerady improve the sample
complexity result of Wasserstein distance. We perform experiments in this subsection to validate the impact of the entropic regularization parameter η for the performance of KPW test. The generated data follows Gaussian distributions (with n = m = 100) or Gaussian mixture distributions (with

49
Fig. 5. Comparison of detection statistics from bending to throwing for various testing procedures. Black dash line indicates the true change-point. Each row corresponds to detection results for each user.
n = m = 200) with different choices of dimension D and ﬁxed sample size. Benchmark methods include 1) KPW test with η = 0 (here Wasserstein distance is computed exactly and we apply alternating optimization procedure as a heuristic); 2) Sinkhorn test with the same η as in the KPW test (in which we take the Sinkhorn divergence as the statistic and all training and testing samples are used); 3) Sinkhorn+ (using all data and post-selecting η with the best performance). Experiment results are reported in Fig. 7, from which we can see that even Sinkhorn+ test has the curse of dimension issue. Moreover, the KPW test with η = 0 has similar performance as the KPW test. Hence, we can assert that the KPW test is capable of alleviating the curse of dimension mainly due to the kernel projection operator instead of the entropic regularization.

50
Fig. 6. Average power for KPW test across different choices of projected dimension d. Left: Gaussian distribution; Right: Gaussian mixture distribution. Results are averaged over 10 independent trials.
Fig. 7. Average power for KPW tests and Sinkhorn tests across different choices of data dimension D. Left: Gaussian distribution; Right: Gaussian mixture distribution. Results are averaged over 10 independent trials.
APPENDIX I SOCIETAL IMPACT Two-sample testing is not only a fundamental problem in statistics but also growing increasing attention in machine learning. On the one hand, it plays a key role in modern applications such as anomaly detection and health care. On the other hand, it can help to design better algorithms for artiﬁcial intelligence such as GANs. Our work shows a competitive performance for dealing with high-dimensional data by nonlinear dimensionality reduction using kernel trick. It identiﬁes the difference between two collections of samples by extracting the most representative nonlinear features. We hope this work can be applied to design more powerful algorithms in those areas.

