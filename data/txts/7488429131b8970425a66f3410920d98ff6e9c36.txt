arXiv:2012.00714v1 [stat.ML] 1 Dec 2020

Debiasing Evaluations That are Biased by Evaluations
Jingyan Wang†, Ivan Stelmakh†, Yuting Wei∗ and Nihar B. Shah†
School of Computer Science† Department of Statistics & Data Science∗
Carnegie Mellon University {jingyanw, stiv}@cs.cmu.edu, ytwei@cmu.edu, nihars@cs.cmu.edu
Abstract It is common to evaluate a set of items by soliciting people to rate them. For example, universities ask students to rate the teaching quality of their instructors, and conference organizers ask authors of submissions to evaluate the quality of the reviews. However, in these applications, students often give a higher rating to a course if they receive higher grades in a course, and authors often give a higher rating to the reviews if their papers are accepted to the conference. In this work, we call these external factors the “outcome” experienced by people, and consider the problem of mitigating these outcome-induced biases in the given ratings when some information about the outcome is available. We formulate the information about the outcome as a known partial ordering on the bias. We propose a debiasing method by solving a regularized optimization problem under this ordering constraint, and also provide a carefully designed cross-validation method that adaptively chooses the appropriate amount of regularization. We provide theoretical guarantees on the performance of our algorithm, as well as experimental evaluations.
1 Introduction
It is common to aggregate information and evaluate items by collecting ratings on these items from people. In this work, we focus on the bias introduced by people’s observable outcome or experience from the entity under evaluation, and we call it the “outcome-induced bias”. Let describe this notion of bias with the help of two common applications – teaching evaluation and peer review.
Many universities use student ratings for teaching evaluation. However, numerous studies have shown that student ratings are aﬀected by the grading policy of the instructor [16, 26, 5]. For instance, as noted in [26, Chapter 4]:
“...the eﬀects of grades on teacher-course evaluations are both substantively and statistically important, and suggest that instructors can often double their odds of receiving high evaluations from students simply by awarding A’s rather than B’s or C’s.”
As a consequence, the association between student ratings and teaching eﬀectiveness can become negative [5], and student ratings serve as a poor predictor on the follow-on course achievement of the students [8, 6]:
“...teachers who are associated with better subsequent performance receive worst evaluations from their students.” [6]
The outcome we consider in teaching evaluation is the grades that the students receive in the course under evaluation1 and the goal is to correct for the bias in student evaluations induced by the grades given by the instructor.
1We use the term “grades” broadly to include letter grades, numerical scores, and rankings. We do not distinguish the diﬀerence between evaluation of a course and evaluation of the instructor teaching the course, and use them interchangeably.
1

An analogous issue arises in conference peer review, where conference organizers survey authors to rate their received reviews in order to understand the quality of the review process. It is well understood that authors are more likely to give higher ratings to a positive review than a to negative review [49, 36, 27]:
“Satisfaction had a strong, positive association with acceptance of the manuscript for publication... Quality of the review of the manuscript was not associated with author satisfaction.” [49]
Due to this problem, an author feedback experiment [36] conducted at the PAM 2007 conference concluded that:
“...some of the TPC members from academia paralleled the collected feedback to faculty evaluations within universities... while author feedback may be useful in pinpointing extreme cases, such as exceptional or problematic reviewers, it is not quite clear how such feedback could become an integral part of the process behind the organization of a conference.”
With this motivation, for the application of peer review, the outcome we consider is the review rating or paper decision received by the author, and the goal is to correct for the bias induced by it in the feedback provided by the author.
Although the existence of such bias is widely acknowledged, student and author ratings are still widely used [3], and such usage poses a number of issues. First, these biased ratings can be uninformative and unfair for instructors and reviewers who are not lenient. Second, instructors, under the possible consideration of improving their student-provided evaluation, may be incentivized to “teach to the test”, raising concerns such as inﬂating grades and reducing content [8]. Furthermore, author-provided ratings can be a factor for selecting reviewer awards [27], and student-provided ratings can be a heavily-weighted component for salary or promotion and tenure decision of the faculty members [3, 8, 5]. If the ratings are highly unreliable and sometimes even follow a trend that reverses the true underlying ordering, then naïvely using these ratings or simply taking their mean or median will not be suﬃcient. Therefore, interpreting and correcting these ratings properly is an important and practical problem.
The goal of this work is to mitigate such outcome-induced bias in ratings. Incidentally, in teaching evaluation and peer review, the “outcome” that people (students or authors) encounter in the process is the evaluation they receive (grades from instructors or reviews from reviewers), and hence we call this bias “evaluations that are biased by evaluations”. That said, we note that the general problem we consider here is applicable to other settings with outcomes that are not necessarily evaluations. For example, in evaluating whether a two-player card game is fair or not, the outcome can be whether the player wins or loses the game [34].
The key insight we use in this work is that the outcome (e.g., grades and paper decisions) is naturally available to those conduct the evaluation (e.g., universities and conference organizers). These observed outcomes provide directional information about the manner that evaluators are likely to be biased. For example, it is known [16, 26, 5] that students receiving higher grades are biased towards being more likely to give higher ratings to the course instructor than students receiving lower grades. To use this structural information, we model it as a known partial ordering constraint on the biases given people’s diﬀerent outcomes. This partial ordering, for instance, is simply a relation on the students based on their grades or ranking, or on the authors in terms of acceptance decisions of their papers.
1.1 Our contributions
We identify and formulate a problem of mitigating biases in evaluations that are biased by evaluations (Section 2). Speciﬁcally, this bias is induced by observable outcomes, and the outcomes are formulated as a known partial ordering constraint. We then propose an estimator that solves an optimization jointly in the true qualities and the bias, under the given ordering constraint (Section 3). The estimator includes a regularization term that balances the emphasis placed on bias versus noise. To determine the appropriate amount of regularization, we further propose a cross-validation algorithm that chooses the amount of regularization in a data-dependent manner by minimizing a carefully-designed validation error (Section 3.2).
2

We then provide a theoretical analysis of the performance of our proposed algorithm (Section 4). First, we show that our estimator, under the two extremal choices of the regularization hyperparameter (0 and ∞), converges to the true value in probability under only-bias (Section 4.2) and only-noise (Section 4.3) settings respectively. Moreover, our estimator reduces to the popular sample-mean estimator when the regularization hyperparameter is set to ∞, which is known to be minimax-optimal in the only-noise case. We then show (Section 4.4) that the cross-validation algorithm correctly converges to the solutions corresponding to hyperparameter values of 0 and ∞ in probability in the two aforementioned settings, under various conditions captured by our general formulation. We ﬁnally conduct synthetic and semi-synthetic experiments that establish the eﬀectiveness of our proposed approach via numerical experiments in more general settings not covered by the theoretical results (Section 5).
1.2 Related work
In terms of correcting rating biases, past work has studied the problem of adjusting student GPAs due to diﬀerent grading policies across courses and disciplines. Proposed models include introducing a single parameter for each course and each student solved by linear regression [9], and more complicated parametric generative models [25]. Though grade adjustment seems to be a perfect counterpart of teaching evaluation adjustment, the non-parametric ordering constraint we consider is unique to teaching evaluation, and do not have obvious counterpart in grade adjustment. For the application of peer review, there are many works [15, 29, 44, 35, 41, 48, 40, 14, 24, 31] addressing various biases and other issues in the review process, but to the best of our knowledge none of them addresses biases in author-provided feedback. It is of interest in the future to design schemes that combine our present work with these past works in order to jointly address multiple problems such as simultaneous existence of outcome-dependent bias and miscalibration.
In terms of the models considered, one statistical problem related to our work is the isotonic regression, where the goal is to estimate a set of parameters under a total ordering constraint (see, e.g. [2, 53, 30, 17]). Speciﬁcally, our problem becomes isotonic regression, if in our exact formulation (2) to be presented, we set λ = 0, x = 0 and the partial ordering to a total ordering.
Another type of related models in statistics literature concerns the semiparametric additive models (e.g. [19, 12, 51, 52]) with shape constraints [10]. In particular, one class of semiparametric additive models involves linear components and components with ordering (isotonic) constraints [21, 11, 33, 37]. Our optimization (2) falls within this class of semiparametric models, if we set the second term of 2-regularization to 0. To see the connection, we write the ﬁrst term of (2) in a linearized form as y − Ax − b 22, where y, b ∈ Rdn, x ∈ Rd and A ∈ Rdn×d is a 0/1 matrix that speciﬁes the course membership of each rating: if a rating is from course i, then in corresponding of row of A, the ith entry is 1 and all other entries are 0. Past work has studied the least-squares estimator for this problem, but the results such as consistency and asymptotic normality rely on assumptions such as A being random design or each coordinate of x being i.i.d., which are not applicable to our setting. The special 0/1 structure of A makes our problem unique and diﬀer from past work in terms of the theoretical analysis.
In terms of the technical approach, our estimator (Equation 2) is partly inspired by permutation-based models [38, 39] which focuses only on shape constraints rather than parameters, but with the key diﬀerence that here we can exploit the crucial information pertaining to the ordering of the bias.
The idea of adopting cross-validation to select the right amount of penalization is classical in statistics literature (see, e.g. [42, 28, 18]). Yet, this generic scheme cannot be directly applied to models where training samples are not exchangeable—in which case, both the sub-sampling step and the test-error estimation are highly non-trivial. Therefore caution needs to be exercised when order restrictions, therefore nonexchangeability, are involved. The cross-validation algorithm proposed in this work is partly inspired by the cross-validation used in nearly-isotonic regression [43]. In nearly-isotonic regression, the hard ordering constraint is replaced by a soft regularization term, and the extent of regularization is determined by crossvalidation. However, introducing the linear term of x as the quantity of interest signiﬁcantly changes the problem. Thus, our cross-validation algorithm and its analysis are quite diﬀerent.
3

2 Problem formulation

For ease of exposition, throughout the paper we describe our problem formulation using the running example of course evaluation, but we note that our problem formulation is general and applies to other problems under outcome-induced bias as well. Consider a set of d courses. Each course i ∈ [d] has an unknown true quality value x∗i ∈ R to be estimated. Each course is evaluated by n students.2 Denote yij ∈ R as the rating given by the jth student in course i, for each i ∈ [d] and j ∈ [n]. Note that we do not require the same set of n students to take all d courses; students in diﬀerent courses are considered diﬀerent individuals. We assume that each rating yij is given by:

yij = x∗i + bij + zij ,

(1)

where bij represents a bias term, and zij represents a noise term. We now describe these terms in more detail. The term zij captures the noise involved in the ratings, assumed to be i.i.d. across i ∈ [d] and j ∈ [n].
The term bij captures the bias that is induced by the observed “outcome” of student j experienced in course i. In the example of teaching evaluation, the outcome can be the grades of the students that are known to
the university, and the bias captures the extent that student ratings are aﬀected by their received grades.
Given these observed outcomes (grades), we characterize the information provided by these outcomes as a known partial ordering, represented by a collection of ordering constraints O ⊆ ([d] × [n])2. Each ordering
constraint is represented by two pairs of (i, j) indices. An ordering constraint ((i, j), (i , j )) ∈ O indicates
that the bias terms obey the relation bij ≤ bi j . We say that this ordering constraint is on the elements {(i, j)}i∈[d],j∈[n] and on the bias {bij }i∈[d],j∈[n] interchangeably. We assume the terms {bij }i∈[d],j∈[n] satisfy the partial ordering O. In teaching evaluations, the partial ordering O can be constructed by, for example,
taking ((i, j), (i , j )) ∈ O if and only if student j in course i receives a strictly higher grade than student j
in course i. For ease of notation, we denote Y ∈ Rd×n as the matrix of observations whose (i, j)th entry equals yij for
every i ∈ [d] and j ∈ [n]. We deﬁne matrices B ∈ Rd×n and Z ∈ Rd×n likewise. We denote x∗ ∈ Rd as the vector of {x∗i }i∈[d].

Goal. Our goal is to estimate the true quality values x∗ ∈ Rd. For model identiﬁability, we assume E[zij] = 0

and i∈[d],j∈[n] E[bij] = 0. An estimator takes as input the observations Y and the partial ordering O, and

outputs an estimate x ∈ Rd. We measure the performance of any estimator in terms of its (normalized)

squared

2

error

1 d

x − x∗ 22.

3 Proposed estimator

Our estimator takes as input the observations Y and the given partial ordering O. The estimator is associated with a tuning parameter λ ≥ 0, and is given by:

x(λ) ∈ arg min min

Y

− x1T

−B

2 F

+λ

B

2 F

,

(2)

x∈Rd

B∈Rd×n

B satisﬁes O

where 1 denotes the all-one vector of dimension n. We let B(λ) denote the value of B that attains the

minimum of the objective (2), so that the objective (2) is minimized at (x(λ), B(λ)). Ties are broken by

choosing the solution (x, B) such that B has the minimal Frobenius norm

B

2 F

.

We show that the estimator

under this tie-breaking rule deﬁnes a unique solution in Proposition 14 in Appendix C.2.1. Furthermore, as

explained in Appendix B.1, the optimization (2) is a convex quadratic programming (QP) in (x, B), and

therefore can be solved in polynomial time in terms of (d, n).

While the ﬁrst term

Y

− x1T

−B

2 F

of

(2) captures the squared diﬀerence between the bias-corrected

observations (Y − B) and the true qualities x1T , the second term

B

2 F

captures

the

magnitude

of

the

2For ease of exposition, we assume that each course is evaluated by n students, but the algorithms and the results extend to regimes where the number of students is diﬀerent across courses.

4

bias. Since the observations in (1) include both the bias B and the noise Z, there is fundamental ambiguity pertaining to the relative contributions of the bias and noise to the observations. The penalization parameter λ is introduced to balance the bias and the variance, and at the same time preventing overﬁtting to the noise. More speciﬁcally, consider the case when the noise level is relatively large and the partial ordering O is not suﬃciently restrictive — in which case, it is sensible to select a larger λ to prevent B overly ﬁtting the observations Y .
For the rest of this section, we ﬁrst describe intuition about the tuning parameter λ by considering two extreme choices of λ which are by themselves of independent interest. We then propose a carefully-designed cross-validation algorithm to choose the value of λ in a data-dependent manner.

3.1 Behavior of our estimator under some ﬁxed choices of λ
To facilitate understandings of the estimator (2), we discuss its behavior for two important choices of λ — 0 and ∞ — that may be of independent interest.

λ = 0: When λ = 0, intuitively the estimator (2) allows the bias term B to be arbitrary in order to best ﬁt the data, as long as it satisﬁes the ordering constraint O. Consequently with this choice, the estimator attempts to explain the observations Y as much as possible in terms of the bias. One may use this choice if domain knowledge suggests that bias considerably dominates the noise. Indeed, as we show subsequently in Section 4.2, our estimator with λ = 0 is consistent in a noiseless setting (when only bias is present), whereas common baselines are not.

λ = ∞: We now discuss the other extremity, namely when λ approaches inﬁnity. Intuitively, this case sets

the bias term to zero in (2) (note that B = 0 trivially satisﬁes any partial ordering O). Therefore, it aims to

explain the observations in terms of the noise. Formally we deﬁne (x(∞), B(∞)) = limλ→∞(x(λ), B(λ)). In

the subsequent result of Proposition 7, we show that this limit exists, where we indeed have B(∞) = 0 and

our

estimator

simply

reduces

to

the

sample

mean

as

[x(∞)]i

=

1 n

n j=1

yij

for

every

i

∈

[d].

We

thus

see

that

perhaps the most commonly used estimator for such applications — the sample mean — also lies in our

family of estimators speciﬁed in (2). Given the well-known guarantees of the sample mean in the absence of

bias (under reasonable conditions of the noise), one may use this choice if domain knowledge suggests that

noise is highly dominant as compared to the bias.

λ ∈ (0, ∞): More generally, the estimator interpolates between the behaviors at the two extremal values λ = 0 and ∞ when both bias and noise is present. As we increase λ from 0, the magnitude of the estimated bias B(λ) gradually decreases and eventually goes to 0 at λ = ∞. The estimator hence gradually explains the observations less in terms bias, and more in terms of noise. Our goal is to choose an appropriate value for λ, such that the contribution of bias versus noise determined by the estimator approximately matches the true relative contribution that generates the observations. The next subsection presents a principled method to choose the value for λ.

3.2 A cross-validation algorithm for selecting λ

We now present a carefully designed cross-validation algorithm to select the tuning parameter λ in a data-

driven manner. Our cross-validation algorithm determines an appropriate value of λ from a ﬁnite-sized set

of candidate values Λ ⊆ [0, ∞] that is provided to the algorithm. For any matrix A ∈ Rd×n, we deﬁne its

squared norm restricted to a subset of elements Ω ⊆ [d] × [n] as

A

2 Ω

=

(i,j)∈Ω A2ij . Let T denote the set of

all total orderings (of the dn elements) that are consistent with the partial ordering O. The cross-validation

algorithm is presented in Algorithm 1. It consists of two steps: a data-splitting step (Lines 1-8) and a

validation step (Lines 9-19).

5

Data-splitting step In the data-splitting step, our algorithm splits the observations {yij}i∈[d],j∈[n] into a training set Ωt ⊆ [d] × [n] and a validation set Ωv ⊆ [d] × [n]. To obtain the split, our algorithm ﬁrst samples uniformly at random a total ordering π0 from T (Line 2). For every course i ∈ [d], we ﬁnd the sub-ordering of the n elements within this course (that is, the ordering of the elements {(i, j)}j∈[n]) according to π0 (Line 4). For each consecutive pair of elements in this sub-ordering, we assign one element in this pair to the training set and the other element to the validation set uniformly at random (Lines 5-7). We note that in comparison to classical cross-validation methods, our algorithm uses the total ordering π0 to guide the split, instead of independently assigning each individual element to either the training set or the validation set uniformly at random. This splitting procedure ensures that for each element in the validation set there is an element that is “close” in the training set with respect to the partial ordering O. This property is useful for interpolation in the subsequent validation step.

Algorithm 1: Cross-validation. Inputs: observations Y , partial ordering O, and set Λ.

/* Step 1: Split the data */

1 Initialize the training and validation sets as Ωt ← {}, Ωv ← {}.

2 Sample a total ordering of π0 uniformly at random from the set T of all total orderings (of the dn elements) consistent with the partial ordering O.

3 foreach i ∈ [d] do

4 Find the sub-ordering of the n elements in course i according to π0, denoted in increasing order as

(i, j(1)), . . . , (i, j(n)).

5 for t = 1, . . . , n2 do

6

Assign (i, j(2t−1)), (i, j(2t)) to Ωt and Ωv, one each uniformly at random. If n is odd, assign the

last element (i, j(n)) to the validation set.

7 end

8 end

/* Step 2: Compute validation error */
9 foreach λ ∈ Λ do 10 Obtain (x(λ), B(λ)) as a solution to the following optimization problem:

(xλ, B(λ)) ∈ arg min
x∈Rd, B∈Rd×n, B satisﬁes O

Y

− x1T

−B

2 Ωt

+

λ

B

2Ωt ,

where ties are broken by minimizing B(λ) F . 11 foreach (i, j) ∈ Ωv do

12

foreach π ∈ T do

13

Find the element (iπ, jπ) ∈ Ωt that is closest to (i, j) with respect to π, and set

[b(πλ)]ij

=

b

(λ) iπ j

π

.

There

may

be

two

closest

elements

at

equal

distance

to

(i, j),

in

which

case call them (iπ, jπ) and (iπ, jπ) and set [b(πλ)]

= . b(πλ) π +b(πλ) π i1 j1 i2 j2

11

22

ij

2

14

end

15

Interpolate

the

bias

as

B(λ)

=

1 |T |

π∈T Bπ(λ).

16 end

17

Compute

the

CV

error

e(λ)

:=

1 |Ωv |

Y

− xλ1T − B(λ)

2 Ωv

.

18 end

19 Output λcv ∈ arg minλ∈Λ e(λ).

(Ties are broken arbitrarily)

Validation step Given the training set and the validation set, our algorithm iterates over the choices of λ ∈ Λ as follows. For each value of λ, the algorithm ﬁrst computes our estimator with penalization parameter
6

λ on the training set Ωt to obtain (x(λ), B(λ)). The optimization (Line 10) is done by replacing the Frobenius

norm on the two terms in the original objective (2) by the Frobenius norm restricted to Ωt. Note that this

modiﬁed objective is independent from the parameters {bij}(i,j)∈Ωv . Therefore, by the tie-breaking rule of

minimizing B(λ) F , we have [B(λ)]ij = 0 for each (i, j) ∈ Ωv.

Next, our algorithm evaluates these choices of λ by their corresponding cross-validation (CV) errors. The

high-level

idea

is

to

evaluate

the

ﬁtness

of

(x(λ), B(λ))

to

the

validation

set

Ωv,

by

computing

1 |Ωv |

Y −x(λ)1T −

B(λ)

2 Ωv

.

However, recall that the estimate B(λ)

only estimates the bias on the training set meaningfully, and

we have Bi(jλ) = 0 for each element (i, j) in the validation set Ωv. Therefore, we “synthesize” the estimated

bias B(λ) on the validation from the estimated bias B(λ) on the training set via an interpolation procedure

(Lines 11-16), as explained below.

Interpolation We now discuss how the algorithm interpolates the bias b(ijλ) at each element (i, j) ∈ Ωv from B(λ). We ﬁrst explain how to perform interpolation with respect to some given total ordering π (Line 13), and then compute a mean of these interpolations by iterating over π ∈ T (Line 15).

• Interpolating with respect to a total ordering (Line 13): Given some total ordering π, we ﬁnd

the element in the training set that is the closest to (i, j) in the total ordering π. We denote this

closest element from the training set as (iπ, jπ), and simply interpolate the bias at (i, j) with respect to

π

(denoted

[b(πλ)]ij )

using

the

value

of

biπjπ .

That

is,

we

set

[bπ(λ)]ij

=

b

(λ) iπ j

π

.

If

there

are

two

closest

elements of equal distance to (i, j) (one ranked higher than (i, j) and one lower than (i, j) in π), we

use the mean of the estimated bias B(λ) of these two elements. This step is similar to the CV error

computation in [43].

• Taking the mean over all total orderings in T (Line 15): After we ﬁnd the interpolated bias Bπ(λ) on the validation set with respect to each π, the ﬁnal interpolated bias b(λ) is computed as the mean of the interpolated bias over all total orderings π ∈ T . The reason for taking the mean over π ∈ T is as follows. When we interpolate by sampling a single ordering π ∈ T , this sampling of the ordering introduces randomness in terms of which training elements are chosen for which validation elements, and hence increasing the variance of the CV error.3 Taking the mean over all total orderings eliminates this source of the variance of the CV error due to sampling, and therefore leads to a better choice of λ.

After interpolating the bias B(λ) on the validation set, the CV error is computed as

1 |Ωv |

Y

− x(λ)1T −

B(λ)) Ωv (Line 17). Finally, the value of λcv ∈ Λ is chosen by minimizing the CV error (with ties broken

arbitrarily). This completes the description of the cross-validation algorithm.

Implementation Now we comment on two important operations in Algorithm 1: sampling a total ordering from the set T of total orderings consistent with the partial ordering O (Line 2), and iterating over the set T (Line 12). For sampling a total ordering from T uniformly at random, many algorithms have been proposed that are approximate [32, 7] or exact [22]. For iterating over T which can be computationally intractable, we approximate the true mean over T by sampling from T multiple times, and take their empirical mean. In many practical settings, the partial ordering contains a structure on which these two operations are simple to implement and run in polynomial time – we discuss a subclass of such partial orderings termed “group orderings” in the theoretical results (Section 4.1); this subclass of partial orderings is also evaluated in the experiments (Section 5).
3In more detail, this variance on the CV error due to sampling causes the algorithm to choose an excessively large λ to underestimate the bias. A large λ shrinks the the magnitude of the estimated bias towards 0, and therefore the estimated bias becomes closer to each other, reducing this variance – in the extreme case, if the estimated bias is 0 on all elements from the training set, then the interpolated bias is 0 in the validation set regardless of the ordering π, giving no variance due to sampling π.

7

4 Theoretical guarantees
We now present theoretical guarantees for our proposed estimator (cf. (2)) along with our cross-validation algorithm (Algorithm 1). In Section 4.2 and 4.3, we establish properties of our estimator at the two extremal choices of λ (λ = 0 and λ = ∞) for no noise and no bias settings respectively. Then in Section 4.4, we analyze the cross-validation algorithm. The proofs of all results are in Appendix C.

4.1 Preliminaries

Model assumptions: To introduce our theoretical guarantees, we start with several model assumptions that are used throughout the theoretical result of this paper. Speciﬁcally, we make the following assumptions on the model (1):

(A1) Noise: The noise terms {zij}i∈[d],j∈[n] are i.i.d. N (0, η2) for some constant η ≥ 0.

(A2)

Bias: The bias terms {bij}i∈[d],j∈[n] are marginally distributed as N (0, σ2) for some constant σ ≥ 0 unless speciﬁed otherwise, and obey one of the total orderings (selected uniformly at random from the set of total orderings) consistent with the partial ordering O. That is, we ﬁrst sample dn values i.i.d. from N (0, σ2), and then sample one total ordering uniformly at random from all total orderings consistent with the partial ordering O. Then we assign these dn values to {bij} according to the sampled total ordering.

(A3) Number of courses: The number of courses d is assumed to be a ﬁxed constant.

All theoretical results hold for any arbitrary x∗ ∈ Rd. It is important to note that the estimator (2) and the cross-validation algorithm (Algorithm 1) requires no knowledge of these distributions or standard deviation parameters σ and η.
Throughout the theoretical results, we consider the solution x(λcv) as solution at λ = λcv on the training set.
Our theoretical analysis focuses on a general subclass of partial orderings, termed “group orderings”, where each rating belongs to a group, and the groups are totally ordered.

Deﬁnition 1 (Group ordering). A partial ordering O is called a group ordering with r groups if there is a partition G1, . . . , Gr ⊆ [d] × [n] of the dn ratings such that ((i, j), (i , j )) ∈ O if and only if (i, j) ∈ Gk and (i , j ) ∈ Gk for some 1 ≤ k < k ≤ r.

Note that in Deﬁnition 1, if two samples are in the same group, we do not impose any relation restriction between these two samples.
Group orderings arise in many practical settings. For example, in course evaluation, the groups can be letter grades (e.g., {A, B, C, D, F} or {Pass, Fail}), or numeric scores (e.g., in the range of [0, 100]) of the students. The group ordering intuitively says that a student receiving a strictly higher grade is more positively biased in rating than a student receiving a lower grade. A total ordering is also group ordering, with the number of groups equal to the number of samples. We assume that the number of groups is r ≥ 2 since otherwise groups are vacuous.
Denote ik as the number of students of group k ∈ [r] in course i ∈ [d]. We further introduce some regularity conditions used in the theoretical results. The ﬁrst set of regularity conditions is motivated from the case where students receive a discrete set of letter grades.

Deﬁnition 2 (Group orderings with the single constant-fraction assumption). A group ordering is said to satisfy the single c-fraction assumption for some constants c ∈ (0, 1) if there exists some group k ∈ [r] such that ik > cn ∀ i ∈ [r].

Deﬁnition 3 (Group orderings with the all constant-fraction assumption). A group ordering of r groups is said to satisfy the all c-fraction assumption for some constant c ∈ (0, 1r ), if ik ≥ cn ∀ i ∈ [d], k ∈ [r].

8

Note that group orderings with all c-fractions is a subset of group orderings with single c-fraction. The ﬁnal regularity condition below is motivated from the scenario where student performances are totally ranked in the course.
Deﬁnition 4 (Total orderings with the constant-fraction interleaving assumption). Let O be a total ordering (of the dn elements {(i, j)}i∈[d],j∈[n]). We deﬁne an interleaving point as any number t ∈ [dn − 1], such that the tth and the (t + 1)th highest-ranked elements according to the total ordering O belong to diﬀerent courses. A total ordering O is said to satisfy the c-fraction interleaving assumption for some constant c ∈ (0, 1), if there are at least cn interleaving points in O.
With these preliminaries in place, we now present our main theoretical results.
4.2 λ = 0 is consistent when there is no noise
We ﬁrst consider the extremal case where there is only bias but no noise involved. The following theorem states that our estimator with λ = 0 is consistent in estimating the underlying quantity x∗, that is x(0) → x∗ in probability.
Theorem 5. [Consistency in estimating x∗] Suppose the assumptions (A1), (A2) and (A3) hold. Suppose there is no noise, or equivalently suppose η = 0 in (A1). Consider any x∗ ∈ Rd. Suppose the partial ordering is one of:
(a) any group ordering of r groups satisfying the all c-fraction assumption, where c ∈ (0, 1r ] is a constant, or
(b) any group ordering with d = 2 courses and 2 groups, or
(c) any total ordering.
Then for any > 0 and δ > 0, there exists an integer n0 (dependent on , δ, c, d, η), such that for every n ≥ n0 and every partial ordering satisfying at least one of the conditions (a), (b) or (c):
P x(0) − x∗ 2 < ≥ 1 − δ.
The proof of this result is provided in Appendix C.3. The convergence of the estimator to the true qualities x∗ implies the following corollary on ranking the true qualities x∗. In words, our estimator x(0) is consistent in comparing the true qualities x∗i and x∗i of any pair of courses i, i ∈ [d] with i = i , as long as their values are distinct.
Corollary 6 (Consistency on the ranking of x∗). Suppose the assumptions (A1), (A2) and (A3) hold. Consider any x∗ ∈ Rd. Assume there is no noise, or equivalently assume η = 0 in (A1). Then for any δ > 0, there exists an integer n0 (dependent on x∗, δ, c, d, η), such that for all n ≥ n0 and every partial ordering satisfying at least one of the conditions (a), (b) or (c) in Theorem 5:
P sign(xi − xi ) = sign(x∗i − x∗i ) ≥ 1 − δ for all i, i ∈ [d] such that i = i and x∗i = x∗i .
In Appendix A.1, we also evaluate the mean estimator. We show that under the conditions of Theorem 5, the mean estimator is provably not consistent. This is because the mean estimator does not account for the biases and only tries to correct for the noise. In order to obtain a baseline that accommodates the outcome-dependent bias (since to the best of our knowledge there is no prior literature on it), in Appendix A.2 we then propose a reweighted mean estimator. It turns out that our estimator at λ = 0 also theoretically outperforms this reweighted mean estimator (see Proposition 13 in Appendix A.2).
9

4.3 λ = ∞ is minimax-optimal when there is no bias
We now move to the other extremity of λ = ∞, and consider the other extremal case when there is only noise but no bias. Recall that we deﬁne the estimator at λ = ∞ as x(∞) = limλ→∞ x(λ). The following proposition states that this limit is well-deﬁned, and our estimator reduces to taking the sample mean at this limit.

Proposition 7 (Estimator at λ = ∞). The limit of (x(∞), B(∞)) := limλ→∞(x(λ), B(λ)) exists and is given by

1n

[x(∞)]i =

yij, for each i ∈ [d], and

n j=1

(3)

B(∞) = 0.

The proof of this result is provided in Appendix C.4. With no bias, estimating the true quality x∗ reduces to estimating the mean of a multivariate normal distribution with the covariance matrix η2Id, where Id denotes the identity matrix of size d × d. Standard results in the statistics literature imply that taking the sample mean is minimax-optimal in this setting if d is a ﬁxed dimension, formalized in the following proposition for completeness.

Proposition 8 (Implication of Example 15.8 in [47]). Let d ≥ 1 be a ﬁxed constant. Let Y = x∗1T + Z,
where x∗ ∈ Rd is an unknown vector and each entry of Z is i.i.d. N (0, η2) with unknown η. Then the sample mean estimator x = n1 Y 1 is minimax-optimal for the squared 2-risk d1 E x − x∗ 22, up to a constant factor that is independent of d.

This concludes the properties of our estimator at the two extremal cases.

4.4 Cross-validation eﬀectively selects λ

This section provides the theoretical guarantees for our proposed cross-validation algorithm. Speciﬁcally, we

show that in the two extremal cases, cross-validation outputs a solution that converges in probability to the

solutions at λ = 0 and λ = ∞, respectively. Note that the cross-validation algorithm is agnostic to the values

of σ and η, or any speciﬁc shape of the bias or the noise.

The ﬁrst result considers the case when there is only bias and no noise, and we show that cross-validation

obtains a solution that is close to the solution using a ﬁxed choice of λ = 0. The intuition for this result is as

follows. The CV error

Y

− x(λ)1T − B(λ)

2 Ωv

measures the diﬀerence between the bias-corrected observations

Y − B(λ) and the estimated qualities x(λ)1T . By construction, the values in x(λ)1T are identical within each

row. Hence, to minimize the CV error we want B(λ) to capture as much variance as possible within each

row of Y . Now consider λ = 0. In this case B(λ) correctly captures the intra-course variance of the bias on

the training set due to the noiseless assumption. Due to the nearest-neighbor interpolation, we expect that

the interpolated B(λ) captures most of the intra-course variance of the bias on the validation set, giving a

small CV error. However, for larger λ > 0, the bias estimated from the training set shrinks in magnitude

due to the regularization term. The bias B(λ) and hence B(λ) only capture a partial extent of the actual

bias in the observations. The rest of the uncaptured bias within each course contributes to the residue

Y

− x(λ)1T

− B(λ)

2 Ωv

,

giving

a

larger

CV

error.

Hence, cross-validation is likely to choose λ = 0 (or some

suﬃciently small value of λ). The following theorem shows that cross-validation is consistent in estimating

x∗ under the only-bias setting.

Theorem 9. Suppose the assumptions (A1), (A2) and (A3) hold. Consider any x∗ ∈ Rd. Suppose there is no noise, or equivalently suppose η = 0 in (A1). Suppose c ∈ (0, 1) is a constant. Suppose the partial ordering is either:

(a) any group ordering satisfying the all c-fraction assumption, or

(b) any total ordering with d = 2.

10

Let 0 ∈ Λ. Then for any δ > 0 and > 0, there exists some integer n0 (dependent on , δ, c, d, σ), such that for every n ≥ n0 and every partial ordering satisfying (a) or (b):
P x(λcv) − x∗ 2 < ≥ 1 − δ.

The proof of this result is provided in Appendix C.5. From Theorem 5 we have that the estimator x(0) (at λ = 0) is also consistent under the only-bias setting. Combining Theorem 5 with Theorem 9, we have x(λcv) approaches x(0). Formally, under the conditions of Theorem 9, we have
P x(λcv) − x(0) 2 < ≥ 1 − δ.

The next result considers the case when there is only noise and no bias, and we show that cross-validation

obtains a solution that is close to the solution using a ﬁxed choice of λ = ∞ (sample mean). Intuitively,

at small values of λ the estimator still tries to estimate a non-trivial amount of the interpolated bias B(λ).

However, any such non-trivial interpolated bias is erroneous since there is no bias in the observations to start

with, increasing the CV error

Y

− x(λ)1T

− B(λ)

2 Ωv

by doing a wrong bias “correction”.

On the other hand,

at λ = ∞ (or some λ that is suﬃciently large), the interpolated bias B(λ) is zero (or close to zero), which is

the right thing to do and hence gives a smaller CV error. The following theorem shows that cross-validation

is consistent in estimating x∗ under the only-noise setting.

Theorem 10. Suppose the assumptions (A1), (A2) and (A3) hold. Consider any x∗ ∈ Rd. Suppose there is no bias, or equivalently assume σ = 0 in (A2). Suppose c1, c2 ∈ (0, 1) are constants. Suppose the partial ordering is either:

(a) any group ordering satisfying the single c1-fraction assumption, or

(b) any total ordering satisfying the c2-fraction interleaving assumption with d = 2.
Let ∞ ∈ Λ. Then for any δ > 0 and > 0, there exists some integer n0 (dependent on , δ, c1, c2, d, η), such that for every n ≥ n0 and every partial ordering satisfying (a) or (b):

P x(λcv) − x∗ 2 < ≥ 1 − δ.

The proof of this result is provided in Appendix C.6. By the consistency of x(∞) implied from Proposition 8 under the only-noise setting, this result implies that the estimator x(λcv) approaches x(∞). Formally, under the conditions of Theorem 10, we have
P x(λcv) − x(∞) 2 < ≥ 1 − δ.

Recall that the sample mean estimator is commonly used and minimax-optimal in the absence of bias. This theorem suggests that our cross-validation algorithm, by adapting the amount of regularization in a data-dependent manner, recovers the sample mean estimator under the setting when sample mean is suitable (under only noise and no bias).
These two theorems, in conjunction to the properties of the estimator at λ = 0 and λ = ∞ given in Sections 4.2 and 4.3 respectively, indicate that our proposed cross-validation algorithm achieves our desired goal in the two extremal cases. The main intuition underlying these two results is that if the magnitude of the estimated bias from the training set aligns with the true amount of bias, the interpolated bias from the validation set also aligns with the true amount of bias and hence gives a small CV error. Extending this intuition to the general case where there is both bias and noise, one may expect cross-validation to still able to identify an appropriate value of λ.

11

5 Experiments

We now conduct experiments to evaluate our estimator and our cross-validation algorithm under various

settings. We consider the metric of the squared 2 error. To estimate the qualities using our cross-validation

algorithm, we ﬁrst use Algorithm 1 to obtain a value of the hyperparameter λcv; we then compute the

estimate x(λcv) as the solution to (2) at λ = λcv (that is, we solve (2) on the entire data combining the

training set and the validation set).4 Implementation details for the cross-validation algorithm (Algorithm 1)

are provided in Appendix B.1. Throughout the experiments, we use Λ = {2i : −9 ≤ i ≤ 5, i ∈ Z} ∪ {0, ∞}.

We also plot the error incurred by the best ﬁxed choice of λ ∈ Λ, where for each point in the plots, we pick

the value of λ ∈ Λ which minimizes the empirical 2 error over all ﬁxed choices in Λ. Note that this best

ﬁxed choice is not realizable in practice since we cannot know the actual value of the 2 error.

We compare our cross-validation algorithm with the mean, median, and also the reweighted mean estimator

introduced in Appendix A.2. The mean estimator is the sample mean for each course (same as our estimator

at

λ

=

∞)

deﬁned

as

[xmean]i

=

1 n

j∈[n] yij for each i ∈ [d], and the median estimator is deﬁned as

[xmed]i = median(yi1, . . . , yin) for each i ∈ [d]. The reweighted mean estimator is not applicable to total

orderings.

In the model (1), we assume that the noise terms {zij}i∈[d],j∈[n] and the bias terms {bij}i∈[d],j∈[n] follow

the assumptions (A1) and (A2) respectively for our theoretical results in Section 4.1. In our simulations, we

consider three cases for the amounts of bias and noise: only bias (σ = 1, η = 0), only noise (σ = 0, η = 1),

and both bias and noise (σ = 0.5, η = 0.5). Throughout the experiments we use x∗ = 0, and as explained in

Proposition 18 in Appendix C.2.1, the results remain the same for any value of x∗.

Each point in all the plots is computed as the empirical mean over 250 runs. Error bars in all the plots

represent the standard error of the mean.

5.1 Dependence on n
We ﬁrst focus on group orderings. We evaluate the performance of our estimator under diﬀerent values of n, under the following types of group orderings.
• Non-interleaving total ordering: We call a total ordering a “non-interleaving” total ordering, if the total ordering is b11 ≤ . . . ≤ b1n ≤ b21 ≤ . . . ≤ b2n ≤ . . . ≤ bd1 ≤ . . . bdn. In the non-interleaving total ordering, the values of the bias terms vary quite signiﬁcantly across courses. Our goal is to evaluate whether our estimator provides good estimates under such imbalanced bias.
• Interleaving total ordering: We call a total ordering an “interleaving” total ordering, if the total ordering is b11 ≤ b21 ≤ . . . ≤ bd1 ≤ b12 ≤ . . . ≤ bd2 ≤ b1n ≤ . . . ≤ bdn. In contrast to the noninterleaving total ordering, in the interleaving total ordering the bias terms are more balanced across diﬀerent courses, and we expect the mean and the median baselines to work well in this setting. Our goal is to evaluate whether the cross-validation algorithm deviates much from the baselines when the baselines work well.
• Binary ordering: We call a group ordering a “binary” ordering, if there are r = 2 groups. Speciﬁcally, we consider a group distribution where ( i1, i2) = (0.9n, 0.1n) for half of the courses i, and ( i1, i2) = (0.1n, 0.9n) for the other half of the courses i.
We consider d = 3 courses for the non-interleaving and interleaving total orderings, and consider d = 4 for the binary ordering. The results are shown in Fig. 1. In the non-interleaving case (Fig. 1a) and the binary case (Fig. 1c) where the distribution of the bias is quite imbalanced, our estimator performs better than the mean and median baselines when there is bias (with or without noise). The improvement is the most signiﬁcant in the case when there is only bias and no noise. In the case where there is only noise, our estimator still performs reasonably as compared to the the baselines – the performance of our estimator is
4Note that this is diﬀerent from the theoretical results in Section 4.4, where we solve (2) at λ = λcv only on the training set.

12

squared 2 error

Only bias (σ = 1, η = 0)

mean median

100

weighted mean CV
Both bias and noise (σ = 0.5, η = 0.5)

best ﬁxed λ
Only noise (σ = 0, η = 1)
100

10−1 10−2
2

10−1

10−1

10−2

5 10

25 50 100

2

5 10

25 50 100

2

n

n

(a) Non-interleaving total ordering

5 10

25 50 100

n

squared 2 error

10−1 10−2
2

10−1

10−1

10−2

10−2

5 10

25 50 100

2

5 10

25 50 100

2

n

n

(b) Interleaving total ordering

5 10

25 50 100

n

squared 2 error

10−1

10−1

10−1

10−2

10−2

10−2

10

20 30 40 50 70 100

10

20 30 40 50 70 100

10

20 30 40 50 70 100

n

n

n

(c) Binary ordering

Figure 1: The performance of our estimator (with cross-validation and with the best ﬁxed λ) for various values of n, compared to the mean, median and reweighted mean estimators.

13

Fraction of times

Only bias (σ = 1, η = 0)
0.3
0.2
0.1

Both bias and noise (σ = 0.5, η = 0.5)

Only noise (σ = 0, η = 1)

0.3 0.4

0.2

0.3

0.2
0.1 0.1

0.0 0 2516 614 116 14 1 4 16 ∞ 0.0 0 2516 614 116 14 1 4 16 ∞ 0.0 0 2516 614 116 14 1 4 16 ∞

λ

λ

λ

Figure 2: The histogram on the fraction of times each value of λ is chosen by cross-validation. Cross-validation is able to choose the value of λ adaptive to diﬀerent amounts of bias and noise.

worse, but this is not unexpected, because while our algorithm tries to compensate for possible bias, the mean and median baselines do not. Indeed, as the theory (Proposition 8) suggests, the mean estimator is ideal for the only-noise setting, but in practice we do not know whether we operate in this only-noise setting a priori. In the interleaving case where the bias is more balanced (Fig. 1b), our estimator performs on par with the baselines, and is still able to correct the small amount of bias in the only-bias case.
We also compare our estimator with the reweighted mean estimator in the binary case. Recall that the reweighted mean estimator is more specialized and not applicable to total orderings or more general partial orderings. Our estimator performs slightly better than the reweighted mean estimator in the two extremal (only-bias and only-noise) cases. In the noisy case, the best ﬁxed λ is better than the reweighted mean estimator but the cross-validation algorithm is worse. In general, we observe that there remains a non-trivial gap between the best ﬁxed λ and cross-validation in the noisy case (also see the non-interleaving total ordering in the noisy case). If prior knowledge about the relative amounts of bias and noise is given, we may be able to achieve better performance with our estimator by setting the value of λ manually.
5.2 Choices of λ by cross-validation
We inspect the choices of the hyperparameter λ made by our cross-validation algorithm. We use the binary setting from Section 5.1, with n = 50. The histograms in Fig. 2 plot the fraction of times that each value of λ ∈ Λ is chosen by cross-validation. When there is only bias, the chosen value of λ is small (with λ = 0 as the most chosen); when there is only noise, the chosen value of λ is large (with λ = ∞ as the most chosen). When there is both bias and noise, the value of λ lies in the middle of the two extremal cases. These trends align with our intuition and theoretical results about cross-validation in Section 4.4, and show that cross-validation is indeed able to adapt to diﬀerent amounts of bias and noise present in the data.
5.3 The regime of d > n
In our theoretical results from Section 4, we restricted our attention to the case where the number of courses d is a ﬁxed constant. We now evaluate the regime where the number of courses d becomes large compared to the number of students n, in order to test the general applicability of our estimator. We again consider the three types of group orderings from Section 5.1. We set n = 10 for the non-interleaving and interleaving total orderings, and n = 20 for the binary ordering.
The results with diﬀerent choices of d are shown in Fig. 3. The mean baseline has a ﬂat curve (except for the small sample-size regime of small values of d) and converges to some non-zero constant in all of the settings. The ﬂat curves come from the fact that the number of parameters (i.e., the number of courses d) grows linearly in the number of observations. The median baseline also has a relatively ﬂat curve, with the
14

exception that in the only-bias case for the interleaving ordering, the error decreases rapidly for small values of d, and eventually converges to a very small constant (not shown), because the median observations across courses have very close bias due to the interleaving ordering). Again, our estimator performs better than the mean and median baselines when there is bias. In the binary case, our estimator also performs better than the reweighted mean estimator for large values of d. One notable setting where our estimator does not perform as well is the only-noise case for the non-interleaving ordering. Note that this is a case not covered by the theory in Theorem 10(b) because the non-interleaving ordering does not satisfy the constant-fraction interleaving assumption. In this case, our estimator at λ = 0 (or small values of λ) incurs a large error. Therefore, despite the fact that we empirically observe that cross-validation still chooses large values of λ for a large fraction of times, due to the very large error when small values of λ are chosen, the overall error is still large. The reason that our estimator at λ = 0 (or small values of λ) gives a large error is that our estimator attempts to explain the data (that has no bias and only noise) as much as possible by the bias. Since in the non-interleaving ordering, course i has smaller bias than course (i + 1), our estimator at λ = 0 mistakenly estimates that xi is about a constant larger than xi+1 for each i ∈ [d − 1], incurring a large error.

5.4 General partial orderings
In our theoretical results from Section 4, we restricted our attention to group orderings. While group orderings cover a large range of common cases in practice, there may exist other types of partial orderings. We now consider the following two types of general partial orderings that are not group orderings to test the general applicability of our estimator.

• Total binary tree: We consider a binary tree, and denote the number of levels (depth) of the tree as . Each node in the tree represents a single element from the observations. Each node has a direct edge to both of its children, and the partial ordering is the set of all directed edges. Speciﬁcally, we consider d = 2 courses. In this case, the total number of observations dn is even. Therefore, we construct a binary tree with one (arbitrary) leaf node removed. We assign all the 2 −1 − 1 nodes from levels 1 to
( − 1) to the ﬁrst course, and assign all the 2 −1 − 1 nodes from level (leaf nodes) to the second course. This construction is conceptually similar to total orderings in group orderings, where each element takes a distinct role in the partial ordering. In this construction we have the relation dn = 2 − 2.

• Binary tree of 3 levels: We consider a binary tree of 3 levels and therefore 7 nodes in total. Each
node contains k elements. There is an ordering constraint between two elements if and only if there is
an edge between the corresponding nodes they belong to. We have the relation dn = 7k. We consider d = 3, and therefore we have n = 73 k. The three courses have the following assignment, where the elements in each level are sampled uniformly at random from all elements in this level:

–

Course

1:

all

k

elements

from

level

1;

k

elements

from

level

2;

k 3

elements

from

level

3,

– Course 2: k elements from level 2; 43 k elements from level 3,

– Course 3: 73 k elements from level 3.

This construction is conceptually similar to a group ordering with a constant number of groups.

We evaluate our estimator under these two types of tree partial orderings for various values of n (setting the values of and k accordingly). Given that the reweighted mean estimator is deﬁned only for group orderings, we also consider its two extensions that are tailored to tree orderings, termed “reweighted mean (node)” and “reweighted mean (level)” as explained in Appendix B.2. Similar to the case of group orderings, these two reweighted mean estimators are applicable to the binary tree of 3 levels but not the total binary tree.
The results are shown in Fig. 4. Again, when there is noise, we observe that our estimator performs better than the mean and median baselines in both of these two tree orderings. In the binary tree of 3 levels, the construction procedure speciﬁes the number of elements in each course from each level, but there is randomness in which nodes in the level these elements from belong to. Due to this randomness, the reweighted

15

Only bias (σ = 1, η = 0)

mean median

weighted mean CV
Both bias and noise (σ = 0.5, η = 0.5)

100

6 × 10−1

10−1

4 × 10−1 3 × 10−1 2 × 10−1

best ﬁxed λ
Only noise (σ = 0, η = 1)
100

squared 2 error

10−1

10−1

2

4

8

16

32

2

4

8

16

32

2

4

8

16

32

d

d

d

(a) Non-interleaving total ordering

squared 2 error

10−1

1.5 × 10−1 1.4 × 10−1 1.3 × 10−1

10−2

1.2 × 10−1 1.1 × 10−1

6 × 10−2

10−1

9 × 10−2

2

4

8

16

32

2

4

8

16

32

d

d

(b) Interleaving total ordering

2

4

8

16

32

d

3 × 10−1

2 × 10−1

10−1

10−1

10−1

squared 2 error

6 × 10−2

6 × 10−2

10−2

4 × 10−2

2

4

8

16

32

2

4

8

16

32

2

4

8

16

32

d

d

d

(c) Binary ordering

Figure 3: The performance of our estimator (with cross-validation and with the best ﬁxed λ) for various values of d, compared to the mean, median, and reweighted mean estimators.

16

squared 2 error

mean median
Only bias (σ = 1, η = 0)

weighted mean (node) weighted mean (level)
Both bias and noise (σ = 0.5, η = 0.5)

CV best ﬁxed λ
Only noise (σ = 0, η = 1)

10−1

10−1

10−1

10−2
10−3 3

10−2

10−2

7 15 31 63 127 255 511

3 7 15 31 63 127 255 511

3

n

n

(a) Total binary tree

7 15 31 63 127 255 511 n

squared 2 error

10−1

10−1

10−1

10−2 14

21

35

49

14

n

21

35

49

14

n

(b) Binary tree of 3 levels

21

35

49

n

Figure 4: The performance of our estimator (with cross-validation and with the best ﬁxed λ) compared to the mean, median, and two reweighted mean estimators, under two types of partial orderings that are not group orderings.

mean (node) estimator is not always applicable, and we use hollow squares to indicate these settings and only compute the error across the runs where the estimator is applicable. We observe that our cross-validation algorithm performs better than the two reweighted mean estimators in the only-bias case. When there is noise (with or without bias), our cross-validation algorithm performs on par while the best ﬁxed λ performs better than the reweighted mean estimators.
5.5 Semi-synthetic grading data
In this section we conduct a semi-synthetic experiment using real grading statistics. We use the grading data from Indiana University Bloomington [23], where the possible grades that students receive are A+ through D-, and F. We consider three ways to construct the group orderings:
• Fine grades: The 13 groups correspond to the grades of A+ through D-, and F.
• Coarse grades: The ﬁne grades are merged to 5 groups of A, B, C, D and F, where grades in {A+, A, A-} are all considered A, etc.
• Binary grades: The grades are further merged to 2 groups of P and F (meaning pass and fail), where all grades except F are considered P. According to the university’s policies, D- is the lowest passing grade.

17

squared 2 error

10−1

mean

median

CV (ﬁne)

reweighted mean (ﬁne)

10−2

CV (coarse)

reweighted mean (coarse)

CV (binary)

reweighted mean (binary)

0.00 0.25 0.50 0.75 1.00
σ
(a) Overall

squared 2 error

10−2

CV reweighted mean

3 × 10−2 2 × 10−2

10−2

CV reweighted mean

4 × 10−2

3 × 10−2

CV reweighted mean

0.00 0.25 0.50 0.75 1.00 σ
(b) Fine grades

2 × 10−2

0.00 0.25 0.50 0.75 1.00 σ

0.00 0.25 0.50 0.75 1.00 σ

(c) Coarse grades

(d) Binary grades

Figure 5: The performance of our estimator (with cross-validation) on semi-synthetic grading data, compared to the mean, median and reweighted mean estimators.

We use the grading data from the course “Business Statistics” from Spring 2020. This course consists of 10 sessions taught by multiple instructors. The average number of students per session is 50. We choose this course because this course has multiple sessions, so that the grading distributions across diﬀerent sessions are more balanced. Therefore, many common grades (A+ through B) appear in all sessions, allowing the reweighted mean estimator to use more observations and perform well. Instead, if we consider all 31 statistics courses taught in the semester, then the only grade appearing in all courses is A, and the reweighted mean estimator has to discard the data from all other grades.
We use the number of students and the grade distribution from this course, and synthesize the observations using our model (1) under the Gaussian assumptions (A2) and (A1). The true quality is set as x∗ = 0 (again the results are independent from the value of x∗); the bias is generated according to the group ordering induced by the ﬁne grades, with a marginal distribution of N (0, σ2), and the noise is generated i.i.d. from N (0, η2). We set η = 1 − σ, and consider diﬀerent choices of σ. The estimators are given one of the three group orderings listed above.
Note that the number of students is unequal in diﬀerent sessions of the course. The mean and median baselines are still deﬁned as taking the mean and median of each course respectively. The precise deﬁnitions of the reweighted mean estimator and our estimator are in Appendix B.3. We estimate the quality of the 10 sessions of the course individually, even if some sessions are taught by the same instructor.
The results are shown in Fig 5. As in previous simulations, the mean and median baselines do not perform well when there is considerable bias (corresponding to a large value of σ). As the number of groups increases from the binary grades to coarse grades and then to the ﬁne grades, the performance of both our estimator and the reweighted mean estimator improves, because the ﬁner orderings provide more information about the bias. Our estimator performs slightly better than the reweighted mean estimator for the ﬁne grades (Fig. 5b), and slightly better on a subset of values of σ for the coarse grades (Fig. 5c). For the binary grades, the error
18

of both our estimator and the reweighted mean estimator increases as the relative amount of bias increases (Fig. 5d). This increase is likely due to the model mismatch as the data is generated from ﬁne grades. In this case our estimator performs better than the reweighted mean estimator for large values of σ.
6 Discussion
Evaluations given by participants in various applications are often spuriously biased by the evaluations received by the participant. We formulate the problem of correcting such outcome-induced bias, and propose an estimator and a cross-validation algorithm to address it. The cross-validation algorithm adapts to data without prior knowledge of the relative extents of bias and noise. Access to any such prior knowledge can be challenging in practice, and hence not requiring such prior knowledge provides our approach more ﬂexibility.
Open problems There are a number of open questions of interest resulting out of this work. An interesting and important set of open questions pertains to extending our theoretical analysis of our estimator and cross-validation algorithm to more general settings: in the regime where there is both bias and noise, under other types of partial orderings, in a non-asymptotic regime, and in a high-dimensional regime with d n. In addition, while our work aims to correct biases that already exist in the data, it is also helpful to mitigate such biases during data elicitation itself. This may be done from a mechanism design perspective where we align the users with proper incentives to report unbiased data, or from a user-experience perspective where we design multitude of questions that jointly reveal the nature of any bias.
Limitations There are several caveats that need to be kept in mind when interpreting or using our work. First, our work only claims to address biases obeying the user-provided information such as biases associated with the grading practice of the instructor (which follow the ordering constraints), and does not address biases associated with aspects such as the demographics of the instructor (which may not align with the ordering constraints). Second, the user should be careful in supplying the appropriate ordering constraints to the algorithm, ensuring these constraints have been validated separately. Third, our theoretical guarantees hold under speciﬁc shape assumptions of the bias and the noise. Our algorithm is designed distribution-free, and we speculate similar guarantees to hold under other reasonable, well-behaved shape assumptions; however, formal guarantees under more general models remain open. Our algorithm consequently may be appropriate for use as an assistive tool along with other existing practices (e.g., sample mean) when making decisions, particularly in any high-stakes scenario. Aligned results between our algorithm and other practices give us more conﬁdence that the result is correct; diﬀerent results between our algorithm and other practices suggests need for additional information or deliberation before drawing a conclusion.
Acknowledgments
The work of J.W., I.S., and N.S. was supported in part by NSF CAREER award 1942124 and in part by NSF CIF 1763734. Y.W. was partially supported by the NSF grants CCF-2007911 and DMS-2015447.
References
[1] Dennis Amelunxen, Martin B. Lotz, Michael McCoy, and Joel A. Tropp. Living on the edge: phase transitions in convex programs with random data. Information and Inference, 3:224–294, 2014.
[2] R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference Under Order Restrictions: The Theory and Application of Isotonic Regression. Wiley, 1972.
[3] William E. Becker and Michael Watts. How departments of economics evaluate teaching. The American Economic Review, 89(2):344–349, 1999.
19

[4] D.P. Bertsekas. Convex Optimization Theory. Athena Scientiﬁc optimization and computation series. Athena Scientiﬁc, 2009.

[5] Anne Boring, Kellie Ottoboni, and Philip B. Stark. Student evaluations of teaching (mostly) do not measure teaching eﬀectiveness. ScienceOpen Research, 2016.

[6] Michela Braga, Marco Paccagnella, and Michele Pellizzari. Evaluating students’ evaluations of professors. Economics of Education Review, 41:71 – 88, 2014.

[7] Russ Bubley and Martin Dyer. Faster random generation of linear extensions. Discrete Mathematics, 201(1):81 – 88, 1999.

[8] Scott E. Carrell and James E. West. Does professor quality matter? Evidence from random assignment of students to professors. Working Paper 14081, National Bureau of Economic Research, June 2008.

[9] Jonathan P. Caulkins, Patrick D. Larkey, and Jifa Wei. Adjusting gpa to reﬂect course diﬃculty, Jun 1995.

[10] Yining Chen and Richard J. Samworth. Generalized additive and index models with shape constraints. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 2016.

[11] Guang Cheng. Semiparametric additive isotonic regression. Journal of Statistical Planning and Inference, 139(6):1980–1991, 2009.

[12] Jack Cuzick. Semiparametric additive regression. Journal of the Royal Statistical Society: Series B (Methodological), 54(3):831–843, 1992.

[13] Paul Deheuvels. The limiting behaviour of the maximal spacing generated by an i.i.d. sequence of gaussian random variables. Journal of Applied Probability, 22(4):816–827, 1985.

[14] T Fiez, N Shah, and L Ratliﬀ. A SUPER* algorithm to optimize paper bidding in peer review. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020.

[15] Hong Ge, Max Welling, and Zoubin Ghahramani. A Bayesian model for calibrating conference review scores, 2013. http://mlg.eng.cam.ac.uk/hong/unpublished/nips-review-model.pdf [Online; accessed 23-Dec-2019].

[16] Anthony G Greenwald and Gerald M Gillmore. Grading leniency is a removable contaminant of student ratings. The American psychologist, 52(11):1209–1217, November 1997.

[17] Piet Groeneboom and Geurt Jongbloed. Nonparametric estimation under shape constraints, volume 38. Cambridge University Press, 2014.

[18] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media, 2009.

[19] Trevor J. Hastie and Robert J. Tibshirani. Generalized additive models, volume 43. CRC press, 1990.

[20] Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963.

[21] Jian Huang. A note on estimating a partly linear model under monotonicity constraints. Journal of Statistical Planning and Inference, 107(1):343 – 351, 2002.

[22] Mark Huber. Fast perfect sampling from linear extensions. Discrete Mathematics, 306(4):420 – 428, 2006.

[23] Indiana University Bloomington. Grade distribution database. registrar.indiana.edu/index.php [Online; accessed 30-Sep-2020].

https://gradedistribution.

20

[24] Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In NeurIPS, 2020.
[25] Valen E. Johnson. An alternative to traditional gpa for evaluating student performance. Statist. Sci., 12(4):251–278, 11 1997.
[26] Valen E. Johnson. Grade Inﬂation: A Crisis in College Education. Springer New York, 1 edition, 2003.
[27] Aditya Khosla, Derek Hoiem, and Serge Belongie. Analysis of reviews for CVPR 2012. 2013.
[28] Ron Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In IJCAI, volume 14, pages 1137–1145. Montreal, Canada, 1995.
[29] Carole J. Lee. Commensuration bias in peer review. Philosophy of Science, 82(5):1272–1283, 2015.
[30] Enno Mammen and Kyusang Yu. Additive isotone regression. In Asymptotics: particles, processes and inverse problems, pages 179–195. Institute of Mathematical Statistics, 2007.
[31] Emaad Manzoor and Nihar B. Shah. Uncovering latent biases in text: Method and application to peer review. In INFORMS Workshop on Data Science, 2020.
[32] Peter Matthews. Generating a random linear extension of a partial order. The Annals of Probability, 19(3):1367–1392, 1991.
[33] Mary C. Meyer. Semi-parametric additive constrained regression. Journal of nonparametric statistics, 25(3):715–730, 2013.
[34] Mario D. Molina, Mauricio Bucca, and Michael W. Macy. It’s not just how the game is played, it’s whether you win or lose. Science Advances, 5(7), 2019.
[35] Ritesh Noothigattu, Nihar B. Shah, and Ariel Procaccia. Loss functions, axioms, and peer review. In ICML Workshop on Incentives in Machine Learning, 2020.
[36] Konstantina Papagiannaki. Author feedback experiment at pam 2007. SIGCOMM Comput. Commun. Rev., 37(3):73–78, July 2007.
[37] Cristina Rueda. Degrees of freedom and model selection in semiparametric additive monotone regression. Journal of Multivariate Analysis, 117:88–99, 2013.
[38] Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, and Martin J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. IEEE Transactions on Information Theory, 63(2):934–959, 2017.
[39] Nihar Bhadresh Shah. Learning from people. PhD thesis, UC Berkeley, 2017.
[40] Ivan Stelmakh, Nihar Shah, and Aarti Singh. On testing for biases in peer review. In NeurIPS, 2019.
[41] Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewer assignment in peer review. arXiv preprint arxiv:1806.06237, 2018.
[42] Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society: Series B (Methodological), 36(2):111–133, 1974.
[43] Ryan J. Tibshirani, Holger Hoeﬂing, and Robert Tibshirani. Nearly-isotonic regression. Technometrics, 53(1):54–61, 2011.
[44] Andrew Tomkins, Min Zhang, and William D. Heavlin. Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713, 2017.
21

[45] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.
[46] A.W. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer Series in Statistics. Springer, 1996.
[47] Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
[48] Jingyan Wang and Nihar B. Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS, 2019.
[49] Ellen J Weber, Patricia P Katz, Joseph F Waeckerle, and Michael L Callaham. Author perception of peer review: impact of review quality and acceptance on satisfaction. JAMA, 287(21):2790–2793, 2002.
[50] Yuting Wei, Martin J. Wainwright, and Adityanand Guntuboyina. The geometry of hypothesis testing over convex cones: Generalized likelihood ratio tests and minimax radii. Ann. Statist., 47(2):994–1024, 04 2019.
[51] Simon N. Wood. Stable and eﬃcient multiple smoothing parameter estimation for generalized additive models. Journal of the American Statistical Association, 99(467):673–686, 2004.
[52] Kyusang Yu, Enno Mammen, and Byeong U Park. Semi-parametric regression: Eﬃciency gains from modeling the nonparametric part. Bernoulli, 17(2):736–748, 2011.
[53] Cun-Hui Zhang. Risk bounds in isotonic regression. The Annals of Statistics, 30(2):528–555, 2002.

Appendices

A Auxiliary results
In this section, we present auxiliary theoretical results on comparing our estimator with the mean estimator (Appendix A.1) and a reweighted mean estimator that we introduce (Appendix A.2).

A.1 Comparison with the mean estimator

Recall

from

Section

5

that

the

mean

estimator

for

estimating

x∗

is

deﬁned

as

[xmean]i

=

1 n

j∈[n] yij for each

class i ∈ [d]. Taking the mean ignores the bias, and hence it is natural to expect that this estimator does not

perform well when the bias in the data is distributed unequally across classes. Intuitively, let us consider two

classes of diﬀerent quality. If students in a stronger class receive lower grades than students in a weaker class,

then the bias induced by this distribution of grades may result in the mean estimator ranking the classes

incorrectly. The following proposition formalizes this intuition and shows that the mean estimator indeed

fails to compare the qualities of courses in the only-bias setting.

Proposition 11. Suppose the assumptions (A1), (A2) and (A3) hold and there is no noise, or equivalently η = 0 in (A1). Suppose the partial ordering satisﬁes any one of the conditions in Theorem 5:

(a) any group ordering of r groups with all c-fractions, where c ∈ (0, 1r ) is a constant, or (b) any group ordering with d = 2 courses and r = 2 groups, or

(c) any total ordering.

22

Then there exist a partial ordering that satisﬁes any one of the conditions (a) (with any number of groups r ≥ 2), (b) or (c), true qualities x∗ ∈ Rd, a pair of courses i, i ∈ [d], and an integer n0 (dependent on the standard parameter σ of the distribution of the bias and the number of groups r in condition (a)), such that
for all n ≥ n0, we have

P sign ([xmean]i − [xmean]i ) = sign(x∗i − x∗i ) < 0.01.

The proof of this result is provided in Appendix C.7. Note that in condition (a) we require c = 1r . This requirement is necessary because if c = 1r , then the number of students in any course i ∈ [d] and any group k ∈ [r] has to be exactly cn. In this case, the bias is evenly distributed across all courses, and in this case the mean estimator is consistent. This negative result on comparing pairs of courses (combined with the fact that both model (1) and the mean estimator are shift invariant) implies the following negative result on estimation – the mean estimator xmean does not converge to the true x∗ in probability.
Corollary 12. Suppose the assumptions (A1), (A2) and (A3) hold and there is no noise, or equivalently η = 0 in (A1). Consider any x∗ ∈ Rd. Suppose the partial ordering satisﬁes Then there exist a partial ordering that satisﬁes any one of the conditions (a), (b) or (c), and there exists a constant > 0 such that for all n ≥ 1 we have

P

xmean − x∗

2 2

<

< 0.01.

Recall that our estimator at λ = 0 is consistent in both comparing the quality of any pair of courses (Corollary 6) and estimating the qualities (Theorem 5). In contrast, the negative results in Proposition 11 and Corollary 12 show that the mean estimator is not consistent in comparison or estimation. Moreover, these negative results are stronger, in that they show the probability of correct comparison or estimation not only does not converge to 1, but also can be arbitrarily small. The negative results on the mean estimator stem from the fact that the mean estimator completely ignores the fact that the bias is not evenly distributed across diﬀerent courses. We remedy this issue by proposing a second baseline – termed a reweighted mean estimator in the following subsection.

A.2 A reweighted mean estimator
The second baseline, deﬁned on group orderings only, re-weighs the observations to make the bias evenly distributed across courses, allowing to then take the mean. For each group k ∈ [r], denote k,min := mini∈[d] ik as the minimum number of students in group k among all courses. Denote R = {k ∈ [r] : k,min > 0} as the set of groups that appear in all courses. The reweighted mean estimator consists of the following two steps.

Reweighting step The estimator computes a weighted mean of each course i ∈ [d] as

[xrw]i = k,min yij .

(4)

k∈R k ∈R k ,min j:(i,j)∈Gk ik

Intuitively, the observations are reweighted in a way such that the bias distribution is balanced among courses. Speciﬁcally, for each course i ∈ [d] and each group k ∈ [r], this reweighted mean estimator computes its group mean j:(i,j)∈Gk yij , and weighs the contribution of this group mean to the overall mean by the factor
ik
of k,min . This reweighting can bee seen as the expected version of a sampling procedure, where for k ∈R k ,min
each course i ∈ [d] and each group k ∈ [r], we sample k,min out of ik observations so that the number of observations in group k is equal across all courses, and then take the mean on the sampled observations. Note that there are an inﬁnite number choices for the weights to balance the biases, and the choice in (4) motivated by sampling is quite natural. It has the property that if all courses have the same group distribution, then the reweighted mean reduces to sample mean.

23

Recentering step We use the assumption that the bias and noise are centered, that is, and i∈[d],j∈[n] E[zij] = 0. Under this assumption, we have

i∈[d]j∈[n] E[bij ] = 0

n1 E[yij] = n1 E[x∗i + bij + zij] = x∗i . (5)

i∈[d],j∈[n]

i∈[d],j∈[n]

i∈[d]

Hence, we shift xrw by a constant such that the empirical version of (5) holds, that is,

1 n

i∈[d],j∈[n] yij .

i∈d[xrw]i =





xrw ← xrw + − 1 [xrw]i + 1 yij  1 (6)

d

dn

i∈[d]

i∈[d],j∈[n]

This recentering step is necessary, because the expected mean of the bias over all courses after the reweighting

step may not be 0, as the reweighting step only aligns the bias across courses, but not necessarily to 0.

From (22b) in Lemma 17, our estimator also satisﬁes

i∈[d] xi

=

1 n

i∈[d],j∈[n] yij for all λ ∈ [0, ∞], so this

recentering also ensures a fair comparison with our estimator. Empirically we observe that the reweighted

mean estimator always performs better after the recentering step.

Note that reweighted mean is undeﬁned for total orderings. For group orderings with all constant fractions,

reweighted mean is also consistent. In this case, we present a simple example below, where our estimator at

λ = 0 still performs better than reweighted mean by a constant factor (uniform bias is assumed for analytical

tractability).

Proposition 13. Suppose the number of courses is d = 2. Suppose the number of groups is r = 2, with a
grade distribution of ( 11, 12) = ((rn, (1 − r)n) and ( 21, 22) = ((1 − r)n, rn) for some r ∈ (0, 1). Suppose there is no noise. Suppose bias in group 1 is generated i.i.d. from Unif[−1, 0], and bias in group 2 is generated
i.i.d. from Unif[0, 1]. Then the squared 2-risk for the reweighted mean estimator is xrw and for our estimator x(0) at λ = 0 is respectively

12 E xrw − x∗ 22 = 241n + 96r(11− r)n ≥ 121n 21 E x(0) − x∗ 22 = 241n + O n12 .

The proof of this result is provided in Appendix C.8. Note that the risk of our estimator is at most half

of the error of reweighted mean, if ignoring the higher-order term O

1 n2

.

B Additional experimental details
In this section, we provide additional details for the experiments in Section 5.
B.1 Implementation
We now discuss the implementation of our estimator.
Solving the optimization (Line 10 in Algorithm 1): We describe the implementation of solving the optimization (2) depending on the value of λ.
• λ = ∞: The estimator is computed as taking the mean of each course according to Proposition 7. • λ ∈ (0, ∞): In the proof of Proposition 14 we show that the objective 1 is strictly convex in (x, B) on
a convex domain. Hence, the problem is a QP with a unique solution. We solve for the QP using the CVXPY package.

24

• λ = 0: It can be shown that the objective (1) is still convex, but there may exist multiple solutions

before the tie-breaking. We ﬁrst obtain one solution of the QP using CVXPY, denoted (x0, b0). The

optimization (2) only has the ﬁrst term, which is an 2-projection from y to the convex domain

{x1T + b : x ∈ Rd, b ∈ Rd×n, b satisﬁes O}. Hence, the value of (x1T + b) is unique among all solutions

(x, b), and the set of solutions can be written as {(x, b) : x = x0 + u, b = b0 − u1T , u ∈ Rd}. We

implement the tie-breaking by solving u using CVXPY, minimizing

b 2F =

b0 − u1T

2 F

subject

to

the

ordering constraints on b = b0 − u1T .

Finally, we discuss a speed-up technique for solving the QP. For total orderings, the number of constraints in O is linear in the number of samples, whereas for general group orderings, the number of constraints in O can become quadratic, making the QP solver slow. To speed up the optimization, it can be shown that for all elements within any course and any group, the ordering of the estimated bias B at these elements is the same as the ordering of the observations Y at these elements. Therefore, among the constraints in O involving these elements, we only keep the constraints that involve the maximum and the minimum elements in this course and this group. Then we add the ordering of Y at these elements to the partial ordering O. This replacement reduces the number of constraints in O and speeds up the QP solver.

Sampling a total ordering from the partial ordering O (Line 2 in Algorithm 1): When O is a group ordering, sampling a total ordering uniformly at random is implemented by ﬁrst sorting the elements according to their group, and then permuting the them uniformly at random within each group.
When O is a tree or a group tree, we sample a total ordering using the following procedure. We ﬁrst take all elements at the root of the tree, and place them in the total ordering as the lowest-ranked elements (if there are multiple elements at the root, then permute them uniformly at random in the total ordering). Consider each sub-tree consisting of a child node of the root and all its descendants. For the remaining positions in the total ordering, we assign these positions to the sub-trees uniformly at random. Then we proceed recursively to sample a total ordering for each sub-tree, and ﬁll them back to their positions in the total ordering.

Interpolation (Line 15 in Algorithm 1): We sample 100 total orderings to approximate the interpolation.

B.2 Extending the reweighted mean estimator to tree orderings
We introduce the deﬁnitions of the two reweighted mean estimators on tree orderings used in the simulation in Section 5.4. Note that the reweighted mean estimator deﬁned in Appendix A.2 is with respect to the groups {Gk}k∈[r]. We replace the groups in the reweighted mean estimator by the following two partitions of the elements. Reweighted mean (node): Each subset in the partition consists of all elements in the same node of the tree. Reweighted mean (level): Each subset in the partition consists of all elements on the same level of the tree.

B.3 Extending our estimator and the reweighted mean estimator to an unequal number of students per course
In the semi-synthetic experiment in Section 5.5, the number of students is unequal in diﬀerent courses. We describe a natural extension of the reweighted mean estimator and our estimator to this case.
First, we explain how to format the observations back to a matrix form. Denote ni as the number of students in course i ∈ [d]. Let n = maxi∈[d] ni. Construct a matrix Y ∈ Rd×n, where the ﬁrst ni elements in each row i ∈ [d] correspond to the observations in this course, and the values of the remaining elements are set arbitrarily. Construct the set of observations Ω ∈ [d] × [n], where the ﬁrst ni elements in each row i ∈ [d] are in Ω. Estimation under an unequal number of students per course is equivalent to estimation given Y

25

(and its corresponding partial ordering O) restricted to the set Ω. It remains to deﬁne the reweighted mean estimator and our estimator restricted to any set Ω ∈ [d] × [n].

The reweighted mean estimator: In the deﬁnition of the the reweighted mean estimator in Appendix A.2, the reweighting step is the same (only using the observations in Ω). The recentering step restricted to Ω is deﬁned as:





xrw ← xrw + −

ni [xrw]i + 1

yij  1

|Ω|

|Ω|

i∈[d]

i∈[d],j∈[n]

Similar to Appendix A.2, after this recentering step, the reweighted mean estimator satisﬁes the empirical version of an equality (Eq. (21b) in Appendix C.2.1) that our estimator also satisﬁes.

Our estimator: We extend Algorithm 1 naturally to being restricted to a set Ω as follows. In the data-
splitting step, in Line 2, we replace the number of elements from dn to i∈[d] ni; in Lines 4-7, we replace the number of students from n to ni, and only ﬁnd the sub-ordering of the ni elements in Ω. The validation step
remains the same.

C Proofs
In this section, we provide proofs for all the theoretical claims made earlier. We begin by introducing some additional notation in Section C.1 which is used throughout the proofs. In Section C.2, we then provide certain preliminaries that are useful for the proofs. We then present the proofs in subsequent subsections.
For ease of notation, we ignore rounding throughout the proofs as it does not aﬀect the claimed results.

C.1 Notation

Training-validation split (Ωt, Ωv): By Algorithm 1, the number of elements restricted to the set Ωt or Ωv is the same for each course i. Hence, we denote nt and nv as the number of students per course in Ωt and Ωv respectively. Throughout the proofs, for simplicity we assume that n is even. In this case we have

nt

=

nv

=

n .

(7)

2

All the proofs extend to the case where n is odd under minor modiﬁcations. We deﬁne the elements in each course i ∈ [d] restricted to Ωt or Ωv as:

Ωti := {(i, j) ∈ Ωt} Ωvi := {(i, j) ∈ Ωv}.

We slightly abuse the notation and say j ∈ Ωti if (i, j) ∈ Ωti. Likewise for Ωvi .

Group orderings: Recall that from Deﬁnition 1 that Gk denotes the set of elements in group k ∈ [r]. We deﬁne
Gtk := Gk ∩ Ωt Gvk := Gk ∩ Ωv.
We denote the elements of group k ∈ [r] in course i ∈ [d] restricted to Ωv as:
Gik := Gk ∩ Ωi.

26

Furthermore, we deﬁne the elements of Gik restricted to Ωv as

Gtik := Gtk ∩ Ωti Gvik := Gvk ∩ Ωvi .
Again, we slightly abuse the notation and say j ∈ Gvik if (i, j) ∈ Gvik. We deﬁne ik as the the number of students of group k ∈ [r] in course i ∈ [d]. We deﬁne k as the number
of students of group k ∈ [r]. We denote −i,k as the number of students of group k ∈ [r] and not in course i. Namely,

ik := |Gik|

k := |Gk| =

ik

i∈[d]

(8a) (8b)

−i,k := |Gk \ Gik| =

i k.

i =i

(8c)

Furthermore, we deﬁne

t k

:=

Gtk

t ik

:=

Gtik

v k

:=

|Gvk |,

v ik

:=

|Gvik |.

(9a) (9b)

Total ordering: Consider the dn elements. We say that the element (i, j) is of rank t ∈ [dn] if (i, j) is the tth-smallest element in among the dn elements.
We denote tij as the rank of each element (i, j) ∈ [d] × [n]. We denote (it, jt) as the element of rank t ∈ [dn].

Observations Y and bias B: Denote the mean of all observations as

1 y = dn yij.
i∈[d],j∈[n]

(10)

Denote the mean of the observations in any course i ∈ [d] as

1n yi = n yij .
j=1

(11)

Likewise we denote the mean of the bias in any course i ∈ [d] as bi. We denote the mean of the bias of any course i ∈ [d] as

1

bGk =

bij .

k (i,j)∈Gk

Now restrict to group orderings. For any course i ∈ [d] and any group k ∈ [r], denote the smallest and the largest observation in course i and group k as

yik,max := max yij
j:(i,j)∈Gk
yik,min := min yij
j:(i,j)∈Gk

(12a) (12b)

We deﬁne bik,max and bik,min likewise. In addition, we deﬁne the smallest and the bias of any group k ∈ [r] as

bk,min = min bij
(i,j)∈Gk
bk,max = max bij .
(i,j)∈Gk

(13)

27

Statistics: We g as the p.d.f. of N (0, 1). Denote G and G−1 as the corresponding c.d.f., and the inverse
c.d.f., respectively. We slightly abuse notation and write P(X) as the p.d.f. of any continuous variable X. For a set of i.i.d. random variables X1, . . . , Xn, we denote X(k) as the kth order statistics of {Xi}ni=1. We
use the notation X(k:n) when we emphasize the sample size n.
Let d ≥ 2 be any integer, and let π be a total ordering of size d. We denote the monotonic cone with respect to π as M := θ ∈ Rd : θπ(1) ≤ . . . ≤ θπ(d) . For any vector x ∈ Rd, we denote the isotonic projection of x as

ΠM(x) := arg min x − u 22.

(14)

u∈Mπ

We denote M as the monotonic cone with respect to the identity ordering.

Our estimator and the cross-validation algorithm: Recall from Line 10 of Algorithm 1 that our estimator restricted to any set of elements Ω ⊆ [d] × [n] is deﬁned as the solution to:

arg min min Y − x1T − B 2 + λ B 2 ,

x∈Rd

B∈Rd×n

Ω

Ω

B satisﬁes O

(15)

with the ties broken by minimizing

B

2 F

.

We use the shorthand notation (x, B) to denote the solution (x(λ), b(λ)) to (15) when the value λ is clear

from the context. Likewise we use the shorthand notation B(λ) to denote the interpolated bias B(λ) obtained

in Line 15 of Algorithm 1.

Recall from Line 13 in Algorithm 1 that we ﬁnd the element (iπ, jπ) ∈ Ωt (or two elements (iπ1 , j1π), (iπ2 , j2π) ∈ Ωt) that is close to the considered element (i, j) ∈ Ωv in any total ordering π. We call these one or two

elements from Ωt as the “nearest-neighbor” of (i, j) with respect to π, denoted NN(i, j; π). Recall from Line 17

in Algorithm 1 that e(λ) denotes the CV error at λ.

Deﬁne the random variable Λ as the set

Λ := {λ ∈ [0, ∞] : x(λ) 2 > }.

(16)

Under x∗ = 0, the set Λ consists of the “bad” choices of λ whose estimate x(λ) incurs a large squared 2-error.

Taking the limit of n → ∞: For ease of notation, we deﬁne the limit of taking n → ∞ as follows. For example, in the statement of Theorem 5(a), we consider any ﬁxed > 0. Then the notation

lim P x(0) − x∗ 2 < = 1

(17)

n→∞

is considered equivalent to the original statement of Theorem 5(a) that for any δ > 0, there exists an integer n0, such that for every n ≥ n0 and every partial ordering satisfying the condition (a) we have

P x(0) − x∗ 2 < = 1.

The notation (17) has the alternative interpretation as follows. We construct a sequence of partial orderings {On}∞ n=1, where the partial ordering On is on d courses and n students and satisﬁes the condition (a). With n students, the estimator x(0) is provided the partial ordering On. We consider any such ﬁxed sequence {On}∞ n=1. Then the limit of n → ∞ in (17) is well-deﬁned.

C.2 Preliminaries
In this section we present preliminary results that are used in the subsequent proofs. Some of the preliminary results are deﬁned based on a set of elements Ω ⊆ [d] × [n]. We deﬁne the elements in each course i ∈ [d] as
Ωi := {(i, j) ∈ Ω}.

28

Again we say j ∈ Ωi if (i, j) ∈ Ωi. We deﬁne the number of elements in each course i ∈ [d] as ni := |Ωi|. Throughout the proofs, whenever a set Ω ⊆ [d] × [n] is considered, we assume the set Ω satisﬁes ni > 0
for each i ∈ [d] to avoid pathological cases. For ease of presentation, the order of the preliminary results does not exactly follow the sequential order that they are proved.

C.2.1 Properties of the estimator

In this section we present a list of properties of our estimator. We start with the following proposition. This proposition shows the existence and uniqueness of the solution to our estimator (15) under its tie-breaking rule for any λ ∈ [0, ∞). That is, the estimator is well-deﬁned on λ ∈ [0, ∞).

Proposition 14 (Existence of the estimator at λ ∈ [0, ∞)). For any λ ∈ [0, ∞) and any Ω ⊆ [d] × [n], there exists a unique solution to our estimator (2) under the tie-breaking rule, given any inputs Y ∈ Rd×n and any
partial ordering O.

The proof of this result is provided in Appendix C.9.1. Recall that the solution to (15) at λ = ∞ is deﬁned by taking the limit of λ → ∞ as:

x(∞) := lim x(λ)
λ→∞
B(∞) := lim B(λ).
λ→∞

(18a) (18b)

The following proposition shows the existence of the solution (18). That is, the limit in (18) is well-deﬁned. This proposition is a generalization of Proposition 7 to any set Ω ⊆ [d] × [n], and its proof is a straightforward generalization of the proof of Proposition 7 (Appendix C.4).

Proposition 15 (Existence of the estimator at λ = ∞). For any Ω ⊆ [d] × [n], the solution (x(∞), B(∞)) deﬁned in (18) exists. Moreover, we have

[x(∞)]i = 1

yij

ni j∈Ωi

B(∞) = 0.

∀i ∈ [d]

The following lemma gives a relation between x(λ) and B(λ) for any λ ∈ [0, ∞]. This basic relation is used in proving multiple properties of the estimator to be presented subsequently in this section.
Lemma 16. For any λ ∈ [0, ∞], and any Ω ⊆ [d] × [n], the solution (x(λ), B(λ)) to the estimator (15) satisﬁes

(λ) 1

xi

= ni

j∈Ωi

yij − b(ijλ)

∀i ∈ [d].

(19)

In particular, in the special case of Ω = [d] × [n], we have

(λ) 1

xi

= n

j∈[n]

yij − b(ijλ)

∀i ∈ [d].

(20)

The proof of this result is provided in Appendix C.9.2 The following property gives expressions of the sum of the elements in x and the sum of the elements in B.
Lemma 17. For any λ ∈ [0, ∞], any Ω ⊆ [d] × [n], the solution (x(λ), B(λ)) given any partial ordering O and any observations Y satisﬁes

b(ijλ) = 0
(i,j)∈Ω

nix(iλ) =

yij .

i∈[d]

(i,j)∈Ω

(21a) (21b)

29

In particular, in the special case of Ω = [d] × [n], we have

b(ijλ) = 0
i∈[d],j∈[d]

n x(iλ) =

yij .

i∈[d]

i∈[d],j∈[n]

(22a) (22b)

The proof of this result is provided in Appendix C.9.3. The following property shows a shift-invariant property of our estimator. This property is used so that we assume x∗ = 0 without loss of generality all the
proofs.

Proposition 18 (Shift-invariance of the estimator). Consider any Ω ⊆ [d] × [n], and any partial ordering O. Fix any λ ∈ [0, ∞]. Let (x(λ), B(λ)) be the solution of our estimator for any observations Y ∈ Rd×n given (O, λ, Ω). Consider any ∆x ∈ Rd. Then the solution of our estimator for the observations Y + ∆x1T given (O, λ, Ω) is (x(λ) + ∆x, B(λ)).

The proof of this result is provided in Appendix C.9.4. Note that the observation model (1) is shiftinvariant by deﬁnition. That is, consider any ﬁxed B, Z ∈ Rd×n, denote the observations with x∗ = 0 as Y . Then the observations with x∗ = ∆x is (Y + ∆x1T ). Hence, Proposition 18 implies the following corollary.
Corollary 19. Under the observation model (1), consider any ﬁxed bias B ∈ Rd×n and noise Z ∈ Rd×n. Suppose the solution of our estimator under x∗ = 0 is (x(λ), B(λ)) given any (O, λ, Ω). Then the solution under x∗ = ∆x is (x(λ) + ∆x, B(λ)).

Based on the result of Corollary 19, it can be further veriﬁed that the cross-validation algorithm (Algorithm 1) that uses our estimator is shift-invariant. Therefore, for all the proofs, we assume x∗ = 0 without loss of generality.
The following pair of lemmas (Lemma 20 and Lemma 21) converts between a bound on the diﬀerence of a pair of courses |xi − xi | and a bound on x 2. Lemma 20 is used in Theorem 9 and Theorem 10; Lemma 21 is used in Theorem 5. Recall the notation Λ := {λ ∈ [0, ∞] : x(λ) 2 > }.
Lemma 20. Suppose x∗ = 0. Consider random Ωt obtained by Algorithm 1. Suppose the observations are generate from either:
(a) The bias is marginally distributed as N (0, σ2) following assumption (A2) and there is no noise, or

(b) The noise is generated from N (0, η2) following assumption (A1), and there is no bias. For any constant > 0, our estimator x(λ) restricted to Ωt satisﬁes

lim P max x(iλ) − x(iλ) > √ ,

n→∞ i,i ∈[d]

d

∀λ ∈ Λ

= 1,

where the probability is taken over the randomness in the observations Y and the training set Ωt.

The proof of this result is provided in Appendix C.9.5.
Lemma 21. Suppose x∗ = 0. Suppose the observations follow part (a) of Lemma 20. Suppose the estimator is restricted to the set of either

(a) Ω = [d] × [n], or

(b) random Ωt obtained by Algorithm 1.

30

Fix any λ ∈ [0, ∞] and any > 0. Suppose we have

lim P max x(iλ) − x(iλ) <
n→∞ i,i ∈[n]

= 1.

(23)

Then we have

lim P x(λ) 2 <
n→∞

= 1,

where the probabilities are taken over the randomness in the observations Y and (for part (b)) in Ωt.

The proof of this result is provided in Appendix C.9.6. The following proposition gives a closed-form solution under d = 2 courses and r = 2 groups at λ = 0. This proposition is used for proving Theorem 5(b) and Proposition 13. Recall the deﬁnitions of y, yi, yik,min and yik,max from (10), (11) and (12).

Proposition 22. Consider d = 2 courses and any group ordering O with r = 2 groups. Let Ω = [d] × [n]. Suppose the bias B satisﬁes the partial ordering O, and there is no noise. Then the solution of our estimator (2) at λ = 0 has the closed-form expression x(0) = y + −1 · γ , where
12

 y22,min − y11,max 
γ = y21,max − y12,min
y2 − y1

if y22,min − y11,max < y2 − y1 if y21,max − y12,min > y2 − y1 o.w.

(24)

If some of {y11,max, y21,max, y12,min, y22,min} do not exist (i.e., when a certain course doesn’t have students of a certain group), then the corresponding case in (24) is ignored.
The proof of this result is provided in Appendix C.9.7

C.2.2 Order statistics
This section presents a few standard properties of order statistics. Consider n i.i.d. random variables {Xi}i∈[n] ordered as

X(1) ≤ . . . ≤ X(n).

Deﬁne the maximal spacing as

Mn := max (X(i+1) − X(i)).
1≤i≤n−1

(25)

The following standard result from statistics states that the maximum diﬀerence between adjacent order statistics converges to 0 for the Gaussian distribution.

Lemma 23. Let n > 1 be any integer. Let X1, . . . , Xn be i.i.d. N (0, 1). Then for any > 0, we have

lim P(Mn < ) = 1.
n→∞
For completeness, the proof of this result is provided in Appendix C.9.8. Denote G−1 as the inverse c.d.f. of N (0, 1). The following standard result from statistics states that the order statistics converges to the inverse c.d.f.
Lemma 24. Let X1, . . . , Xn be N (0, 1). Fix constant p ∈ (0, 1) and c ∈ R. Let {kn}∞ n=1 be a sequence such that knn = p + √cn + o √1n . We have

X(kn:n) −P→ G−1(p).

31

For completeness, the proof of this result is provided in Appendix C.9.9. The following standard result from statistics provides a simple bound on the maximum (and the minimum) of a set of i.i.d. Gaussian random variables.
Lemma 25. Let X1, . . . , Xn be i.i.d. N (0, σ2). Then we have

lim P max Xi < 2σ log n = 1
n→∞ i∈[n]

lim P max Xi − min Xi < 4σ log n = 1.

n→∞ i∈[n]

i∈[n]

C.2.3 Additional preliminaries

In this section, we present several more additional preliminary results that are used in the subsequent proofs.

The following result considers the number of students under the all constant-fraction assumption given

any training-validation split (Ωt, Ωv). Recall the deﬁnitions of

ik ,

k,

vik ,

t k

and

v k

from

(8)

and

(9).

Lemma 26. Assume ik ≥ 4 for each i ∈ [d] and k ∈ [r]. Consider any training-validation split (Ωt, Ωv) obtained by Algorithm 1. Then we have the deterministic relations

ik ≤ 4 ik ≤ 4

vik ≤ 3 4ik tik ≤ 3 4ik

∀i ∈ [d], k ∈ [r] ∀i ∈ [d], k ∈ [r]

(26a) (26b)

and

k≤ 4 k≤ 4

vk ≤ 34k tk ≤ 34k

∀k ∈ [r] ∀k ∈ [r].

(27a) (27b)

The proof of this result is provided in Appendix C.9.10. The following result considers any total ordering. It states that the ranks of the adjacent elements within Ωt, or the ranks of the adjacent elements between Ωt and Ωv diﬀer by at most a constant. Formally, for any 1 ≤ k1 < k2 ≤ dn, the element of rank k1 and the element of rank k2 are said to be adjacent within Ωt, if both elements are in Ωt, and elements of ranks k1 + 1 through k2 − 1 are all in Ωv. The two elements are said be be adjacent between Ωt and Ωv, if one of the
following is true:

• The elements of ranks k1 through (k2 − 1) are in Ωt, and the element of rank k2 is in Ωv; • The elements of ranks k1 through (k2 − 1) are in Ωv, and the element of rank k2 is in Ωt. Lemma 27. For any partition (Ωt, Ωv) obtained by Algorithm 1, for any 1 ≤ k1 < k2 ≤ dn, suppose that the element of rank k1 and the element of rank k2 are (a) adjacent within Ωt, or

(b) adjacent between Ωt and Ωv.

Then we have

k2 − k1 ≤ 2d + 1.

The proof of this result is provided in Appendix C.9.11. The following lemma bounds the mean of the bias terms using standard concentration inequalities.

32

Lemma 28. Consider any partial ordering O and any random Ωt obtained by Algorithm 1. Suppose that the bias is marginally distributed as N (0, 1) following assumption (A2). For any > 0, we have

 lim P 
n→∞
 lim P 
n→∞

1

1

nt

bij − n

bij <

j∈Ωti

j∈[n]



1

|Ωt|

bij <  = 1,

(i,j)∈Ωt

 =1

∀i ∈ [d],

(28a) (28b)

where the probabilities are over the randomness in B and in Ωt.

The proof of this result is provided in Appendix C.9.12.

C.3 Proof of Theorem 5
The proof follows notation in Appendix C.1 and preliminaries in Appendix C.2. By Corollary 19, we assume x∗ = 0 throughout the proof without loss of generality. We also assume without loss of generality that the standard deviation of the Gaussian bias is σ = 1. Given x∗ = 0 and the assumption that there is no noise, model (1) reduces to

Y = B.

(29)

Recall that ik denotes the number of observations in course i ∈ [d] of group k ∈ [r], and k denotes the number of observations of group k summed over all courses. For any positive constant c > 0, we deﬁne the set Sc as

Sc := (i, i ) ∈ [d]2 : ∃k ∈ [r] such that ik , i ,k+1 ≥ c .

(30)

k k+1

In words, the deﬁnition (30) says that for any pair of courses (i, i ) ∈ Sc, we have that course i takes at least c-fraction of observations in some group k ∈ [r], and course i takes at least c-fraction of observations in group (k + 1).
Before proving the three parts separately, we ﬁrst state a few lemmas that are used for more than one part. The ﬁrst lemma states that any (i, i ) ∈ Sc imposes a constraint on our estimator x(0) at λ = 0.

Lemma 29. Assume x∗ = 0. Consider bias marginally distributed as N (0, 1) following assumption (A2) and no noise. Let x(0) be the solution of our estimator at λ = 0. Fix any c > 0. For any (i, i ) ∈ Sc, we have that for any > 0,

lim P x(i0) − x(i0) <
n→∞

= 1.

(31)

The proof of this result is provided in Appendix C.10.1. To state the next lemma, we ﬁrst make the following deﬁnition of a “cycle” of courses.
Deﬁnition 30. Let L ≥ 2 be an integer. We say that (i1, i2, . . . , iL) ∈ [d]L is a “cycle” of courses with respect to Sc, if

(im, im+1) ∈ Sc and (iL, i1) ∈ Sc.

∀m ∈ [L − 1],

(32a) (32b)

The following lemma states that if there exists a cycle of courses, then the diﬀerence of the estimated quality x between any two courses in this cycle converges to 0 in probability.

33

Lemma 31. Fix any c > 0. Suppose d is a ﬁxed constant. Let (i1, i2, . . . , iL) ∈ [d]L for some L ≥ 2 be a cycle with respect to Sc. Then for any > 0 we have

lim P max xim − xim <
n→∞ m,m ∈[L]

= 1.

The proof of this result is provided in Appendix C.10.2. Now we prove the three parts of Theorem 5 respectively.

C.3.1 Proof of part (a)
For clarity of notation, we denote the constant in the all constant-fraction assumption as cf . Consider any i, i ∈ [d] and any k ∈ [r − 1]. We have

ik (≥i) cf n = cf , k dn d

where step (i) is true by the all c-fraction assumption from Deﬁnition 3. Hence, by the deﬁnition (30) of

Sc, we have (i, i ) ∈ S cf for every i, i ∈ [d]. Hence, (1, 2, . . . , d) is a cycle with respect to S cf according to

d

d

Deﬁnition 30. Applying Lemma 31 followed by Lemma 21(a) completes the proof.

C.3.2 Proof of part (b)
Without loss of generality we assume course 1 has more (or equal) students in group 1 than course 2, that is, we assume

11 ≥ 21.

(33)

Since we assume there are only two courses and two groups, we have

12 = n − 11 ≤ n − 21 = 22.

(34)

We ﬁx any constant > 0. We now bound the probability that |x2 − x1| < . Speciﬁcally, we separately bound the probability of x2 − x1 < , and the probability of x2 − x1 > − . Finally, we invoke Lemma 21 to complete the proof.

Bounding the probability of x2 − x1 < : By the deﬁnition (30) of Sc, it can be veriﬁed that given (33) and (34) we have (1, 2) ∈ S0.5 (taking k = 1). By Lemma 29, we have

lim P(x2 − x1 < ) = 1.

(35)

n→∞

Bounding the probability of x2 − x1 > − : By the closed-form solution in Proposition 22, we have x2 − x1 = γ where γ is deﬁned in (24) as

 y22,min − y11,max 
γ = y21,max − y12,min
y2 − y1

if y22,min − y11,max < y2 − y1 if y21,max − y12,min > y2 − y1 o.w.

(36)

Recall from the model (29) that Y = B, and hence we have the deterministic relation y22,min − y11,max = b22,min − b11,max ≥ 0 due to the assumption (A2) under the group ordering, and similarly we have the deterministic relation y21,max − y12,min ≤ 0. Consider the case of y2 − y1 ≥ 0. In this case, only the ﬁrst and the third cases in (36) are possible, and therefore we have 0 ≤ γ ≤ y2 − y1. Now consider the case of

34

y2 − y1 < 0. In this case, only the second and the third cases in (36) are possible, and we have y2 − y1 ≤ γ ≤ 0. Combining the two cases, we have the relation

x2 − x1 = γ > −

if y2 − y1 > − .

(37)

It suﬃces to bound the probability of y2 − y1 > − .

In what follows we show that limn→∞ P(y2 − y1 > − ) = 1. That is, we ﬁx some small δ > 0 and show

that P(y2 − y1 > − ) ≥ 1 − δ for all suﬃciently large d. The intuition is that course 2 has more students in

group 2, which is the group of greater values of the bias. Since according to assumption (A2) the bias is

assigned within each group uniformly at random, the set of observations in course 2 statistically dominates

the set of observations in course 1. Therefore, y2 should not be less than y1 by a large amount.

We ﬁrst condition on any ﬁxed values of bias ranked as b∗(1) ≤ . . . ≤ b∗(2n) (since we assume the number

of courses is d = 2). Denote the mean of bias of group 1 as b∗G1 = 11 k1=1 b∗(k) and the mean of bias of group

2

as

∗
bG

=

1

2

2

2kn= 1+1 b∗(k). Denote ∆B∗ := b∗(2n) − b∗(1) and denote ∆B := b∗(2n) − b∗(1). By Hoeﬀding’s

inequality without replacement [20, Section 6] on group 1 of course 1, we have



P

b1j −

j∈G11

∗
11bG1

≥ ∆B∗

1 11 log δ

 B∗ ≤ 2 exp

2 · ∆2B∗ log( 1δ )

−

∆2

B

=

2δ2

(i)
≤

δ ,

8

where (i) holds for any δ ∈ (0, 116 ). We apply Hoeﬀding’s inequaltiy without replacement for any i ∈ {1, 2} and any k ∈ {1, 2}. Using the fact that ik ≤ n for any i ∈ {1, 2} and any k ∈ {1, 2}, we have



∗

1

P

bij − ikbGk ≥ ∆B∗ n log δ

j∈Gik

 B∗ ≤ δ .
8

(38)

Taking a union bound of (38) over i ∈ {1, 2} and k ∈ {1, 2}, we have that with probability at least 1 − 2δ ,





1

y2 − y1 = n 

b2j +

b2j −

b1j −

b1j 

j∈G21

j∈G22

j∈G11

j∈G12

(i) 1 ≥
n

∗

∗

∗

∗

1

21bG1 + 22bG2 − 11bG1 − 12bG2 − 4∆B∗ n log δ

1

∗

∗

1

= n

( 21 − 11)bG1 + ( 22 − 12)bG2 − 4∆B∗

n log

δ

(ii) 1

∗

∗

1

= n

( 21 − 11)(bG1 − bG2 ) − 4∆B∗

n log

δ

(iii)
≥ −4∆B∗

log 1δ , n

(39)

where inequality (i) is true by (38), step (ii) is true because 11+ 12 = 21+ 22 and hence 21− 11 = −( 22− 12),

and

ﬁnally

step

(iii)

is

true

by

∗
bG

∗
≤ bG

due to the assumption (A2) of the bias and the group orderings.

1

2

Now we analyze the term ∆B in (39). By Lemma 25, there exists integer n0 such that for any n ≥ n0,

P ∆B ≤ 4 log 2n ≥ 1 − δ . (40) 2

35

√ Let n1 be a suﬃciently large such that n1 ≥ n0 and 16 log 2n1 · we have that for any n ≥ n0,

logn(1δ1 ) < . Then combining (40) with (39),

P (y2 − y1 > − ) =

P (y2 − y1 > − | B) · P(B) dB

B∈R2×n

≥

2×n P(y2 − y1 > − | B) · P(B) dB

B∈R√ :

∆B ≤4 log n

(i)

δ

≥ 1 − 2 · P(∆B ≤ 4 log 2n)

(ii)

δ2

≥ 1 − ≥ 1 − δ,

2

(41)

where inequality (i) is true by (39) due to the choice of n1, and inequality (ii) is true by (40). Combining (41) with (37), for any n ≥ n1, we have

P(x2 − x1 = γ > − ) ≥ P(y2 − y1 > − ) ≥ 1 − δ.

That is,

lim P(x2 − x1 > − ) = 1.

(42)

n→∞

Finally, combining Step 1 and Step 2, we take a union bound of (35) and (42), we have

lim P |x2 − x1| < = 1.

(43)

n→∞

Given (43), we invoke Lemma 21 and obtain

completing the proof.

lim P x 2 < = 1,
n→∞

C.3.3 Proof of part (c)

For total orderings, each observation forms its own group of size 1 (that is, k = 1 for all k ∈ [dn]). A bias term belonging to group some k ∈ [dn] is equivalent to the bias term being rank k. By the deﬁnition 30 of Sc,

if course i contains rank k and course i contains rank k + 1 then we have (i, i ) ∈ S1, because ik = i ,k+1 = 1

k

k+1

due to the total ordering.

The proof consists of four steps:

• In Step 1, we ﬁnd a partition of the courses, where each subset in this partition consists of courses i whose estimated qualities xi are close to each other.

• In Step 2, we use this partition to analyze |xi − xi |.

• In Step 3, we upper-bound the probability that |xi − xi | is large. If |xi − xi | is large, then we construct an alternative solution according to the partition and derive a contradiction that x cannot be the optimal compared to the alternative solution.

• In Step 4, we invoke Lemma 21 to convert the bound on |xi − xi | to a bound on x 2.

36

course 1 course 2 course 3

1

2

3

students

𝑉"

𝑉#

𝑉$

1

2

6

7

3

4

5

8

9

10

11

12

(a) The total ordering

1

2

3

𝑉"

𝑉#

(b) The procedure of constructing the

partition

Figure 6: An example for constructing the partition of hypernodes.

Step 1: Constructing the partition We describe the procedure to construct the partition of courses based on any given total ordering O. Without loss of generality, we assume that the minimal rank in course i is strictly less than the minimal rank in course (i + 1) for every i ∈ [d − 1]. That is, we have

min tij < min ti+1,j

j∈[n]

j∈[n]

∀i ∈ [d − 1].

(44)

The partition is constructed in steps. We ﬁrst describe the initialization of the partition. After the partition is initialized, we specify a procedure to “merge” subsets in the partition. We continue merging the subsets until there are no more subsets to merge according to a speciﬁed condition, and arrive at the ﬁnal partition.

Initialization We construct a directed graph of d nodes, where each node i ∈ [d] represents course i. We put a directed edge from node i to node i for every (i, i ) ∈ S1. Let V1, . . . , Vd ⊆ [d] be a partition of the d nodes. We initialize the partition as Vi = {i} for all i ∈ [d]. We also call each subset Vi as a “hypernode”.

Merging nodes We now merge the partition according to the following procedure. We ﬁnd a cycle (of directed edges) in the constructed graph, such that the nodes (courses) in this cycle belong to at least two diﬀerent hypernodes. If there are multiple such cycles, we arbitrarily choose one. We “merge” all the hypernodes involved in this cycle. Formally, we denote the hypernodes involved in this cycle as Vi1 , Vi2 , . . . , ViL . To merge these hypernodes we construct a new hypernode V = Vi1 ∪ Vi2 ∪ . . . ∪ ViL . Then we remove the hypernodes Vi1 , Vi2 , . . . , ViL from the partition, and add the merged hypernode V to the partition.
We continue merging hypernodes, until there exist no such cycles that involve at least two diﬀerent hypernodes. When we say we construct a partition we refer to this ﬁnal partition after all possible merges are completed.
An example is provided in Fig. 6. In this example we consider d = 3 courses and n = 4 students per course. We consider the total ordering in Fig. 6(a), where each integer in the table represents the rank of the corresponding element with respect to this total ordering. The top graph of Fig. 6(b) shows the constructed graph and the initialized partition. At initialization there is a cycle between course 1 and course 2 (that belong to diﬀerent hypernodes V1 and V2), so we merge the hypernodes V1 and V2 as shown in the bottom graph of Fig. 6(b). At this point, there are no more cycles that involve more than one hypernode, so the bottom graph is the ﬁnal constructed partition.
In what follows we state two properties of the partition. We deﬁne the length of a cycle as the number of edges in this cycle. The ﬁrst lemma states that within the same hypernode, any two courses included in a cycle whose length is upper-bounded.
Lemma 32. Consider the partition constructed from any total ordering O. Let V be any hypernode in this
partition. Then for any i, i ∈ V with i = i , there exists a cycle whose length is at most 2(d − 1), such that
the cycle includes both course i and course i .

37

The proof of this result is provided in Appendix C.10.3. The following lemma provides further properties on the constructed partition. We say that there exists an edge from hypernode V to V , if and only if there exists an edge from some node i ∈ V to some node i ∈ V . Denote s as the number of hypernodes in the partition. Denote the hypernodes as V1, . . . , Vs.
Lemma 33. Consider the partition constructed from any total ordering O. The hypernodes in this partition can be indexed in a way such that the only edges on the hypernodes are (Vm, Vm+1) for all m ∈ [s − 1]. Under this indexing of hypernodes, the nodes within each hypernodes are consecutive, and increasing in the indexing of the hypernodes. That is, there exist integers 0 = i1 < i2 < . . . < is+1 = d, such that Vm = {im + 1, . . . , im+1} for each m ∈ [s].
Moreover, for each m ∈ [s], the ranks of elements (with respect to the total ordering O) contained in the nodes of hypernode Vm are consecutive and increasing in the indexing of the hypernodes. That is, there exists integers 0 = t1 < t2 . . . < ts+1 = dn, such that ∪i∈Vm ∪j∈[n] {tij} = {tm + 1, . . . , tm+1}.
The proof of this result is provided in Appendix C.10.4. When we refer to a partition (V1, . . . , Vs), we speciﬁcally refer to the indexing of the hypernodes that satisﬁes Lemma 33.
As an example, in Fig. 6 we have V1 = {1, 2} and V2 = {3}. The ranks of elements in V1 are {1, . . . , 8}, and the ranks of elements in V2 are {9, . . . , 12}.

Step 2: Analyzing |xi − xi | using the partition Our goal in Step 2 and Step 3 is to prove the that for any > 0, we have

lim P max |xi − xi| <
n→∞ i,i ∈[n]

= 1.

Equivalently, denote the “bad” event as

Ebad := max |xi − xi| > 4d2 .
i,i ∈[n]

(45)

The goal is to prove limn→∞ P(Ebad) = 0. In Step 2, we deﬁne some high-probability event (namely, E1 ∩ E2 ∩ E3 to be presented), and show that it suﬃces to prove

lim P(Ebad, E1 ∩ E2 ∩ E3) = 0.
n→∞

The event E1 bounds |xi − xi| within each hypernode We ﬁrst bound |xi − xi| for i, i ∈ [d] within each hypernode. By Lemam 32, there exists a cycle of length at most 2(n − 1) between any two courses i, i within the same hypernode. Given assumption (A3) that n is a constant, by Lemma 31 we have that for each hypernode V ,

lim P max |xi − xi | < = 1.

(46)

n→∞ i,i ∈V

Since the number of hypernodes is at most d, taking a union bound of (46) across all hypernodes in the partition, we have

lim P max |xi − xi | < , ∀V hypernode in the partition = 1.

(47)

n→∞

i,i ∈V

E1

We denote this event in (47) as E1.

38

The event E2 bounds |xi − xi| across hypernodes We then bound |xi − xi| across diﬀerent hypernodes. We consider adjacent hypernodes Vm and Vm+1 for any m ∈ [s − 1]. By Lemma 33, there exists an edge from Vm to Vm+1. That is, there exists i ∈ Vm and i ∈ Vm+1 such that (i, i ) ∈ S1. By Lemma 29, we have

lim P (xi − xi < ) = 1.

(48)

n→∞

Since the number of hypernodes s is at most d, taking a union bound of (48) over all m ∈ [s − 1], we have

lim P
n→∞

min xi − xi < ,
i∈Vm,i ∈Vm+1 E2

∀m ∈ [s − 1]

= 1.

(49)

We denote this event in (49) as E2.

Deﬁne E3: Finally, we deﬁne E3 as the event that B is not a constant matrix. That is,

E3 = {∃i, i ∈ [d], j, j ∈ [n] : bij = bi j }.

Since by assumption (A2) (setting σ = 1) the bias terms {bij}i∈[d],j∈[n] are marginally distributed as N (0, 1), it is straightforward to see that the event E3 happens almost surely:

P(E3) = 1.

(50)

Decompose Ebad: We decompose the bad event Ebad as

P(Ebad) = P(Ebad, E1 ∩ E2 ∩ E3) + P(Ebad, E1 ∩ E2 ∩ E3)

≤ P(Ebad, E1 ∩ E2 ∩ E3) + P(E1 ∩ E2 ∩ E3).

(51)

Combining (47), (49) and (50), we have

lim P E1 ∩ E2 ∩ E3 = lim P(E1 ∪ E2 ∪ E3) ≤ lim P(E1) + P(E2) + P(E3) = 0.

(52)

n→∞

n→∞

n→∞

Combining (51) and (52), in order to show limn→∞ P(Ebad) = 0 it suﬃces to show limn→∞ P(Ebad, E1 ∩ E2 ∩ E3) = 0.

Step 3: Analyzing the event Ebad ∩ E1 ∩ E2 ∩ E3 In this step, we analyze the event Ebad ∩ E1 ∩ E2 ∩ E3, and identify a new partition (namely, {VL, VH} to be deﬁned) of the nodes. This new partition is used to drive a contradiction in Step 4.
First consider the case that the number of hypernodes is s = 1. In this case E1 and Ebad gives a direct contradiction, and we have Ebad ∩ E1 ∩ E2 ∩ E3 = ∅. We now analyze the case when the number of hypernodes is s ≥ 2. We arbitrarily ﬁnd one course from each hypernode and denote them as i1 ∈ V1, . . . , is ∈ Vs.
We condition on Ebad ∩ E1 ∩ E2 ∩ E3. Recall that by deﬁnition (45), the event Ebad requires that there exists i, i ∈ [d] such that

|xi − xi| > 4d2 .

(53)

By the deﬁnition (47) of E1, we have that i and i cannot be in the same hypernode. Hence, we assume i ∈ Vm and i ∈ Vm , and assume m < m without loss of generality. We bound xi − xi as

xi − xi = (xi − xim ) + (xim − xim ) −1 + . . . + (xim+1 − xim ) + (xim − xi )
(i)
< 2 + d < 4d2 ,

(54)

39

where (i) is true by events E1 and E2. equivalently

Combining (53) and (54), we must have xi xi − xi > 4d2 .

− xi < −4d2 , or (55)

We decompose xi − xi as

xi − xi = (xi − xim ) + (xim − xim+1 ) + . . . + (xim −1 − xim ) + (xim − xi )

(i)
< 2 + (xim − xim+1 ) + . . . + (xim −1 − xim ),

(56)

where (i) is due to event E1. Combining (55) and (56), we have

2 + (xim − xim+1 ) + . . . + (xim −1 − xim ) > xi − xi > 4d2 (xim − xim+1 ) + . . . + (xim −1 − xim ) > (4d2 − 2) > 3d2 .

Hence, we have

d · max{(xim − xim+1 ), . . . , (xim −1 − xim )} > 3d2 max{(xim − xim+1 ), . . . , (xim −1 − xim )} > 3d .

(57)

Without loss of generality, we assume that in (57) we have integer m∗ with m ≤ m∗ < m such that

xim∗ − xim∗+1 > 3d .

(58)

Now consider any m, m ∈ [s] such that m ≤ m∗ < m , and for any i ∈ Vm and i ∈ Vm , we have

xi − xi

=

(xi

−

xim )

+

(xim

−

xim+1 )

+

...

+

(xi∗ m

−

xim∗+1 )

+

...

+

(xim

−1

−

xim

)

+

(xim

− xi )

(i)
> −2 + 3d − d > ,

where (i) is by events E1 and E2 combined with (58). Equivalently, denote VL := V1 ∪ . . . ∪ Vm∗ and VH := Vm∗+1 ∪ . . . ∪ Vs, we have

xi − xi >

∀i ∈ VL, i ∈ VH.

(59)

Step 4: Showing P(Ebad, E1 ∩ E2 ∩ E3) = 0 by deriving a contradiction We consider any solution
(x, B) of our estimator at λ = 0 conditional on Ebad ∩ E1 ∩ E2 ∩ E3, and derive a contradiction. Hence, we have P(Ebad, E1 ∩ E2 ∩ E3) = 0.

Analyzing properties of B By Lemma 33, any bias term bij for i ∈ VL has a smaller rank than any bias term bij for i ∈ VH. Therefore, the mean of B over elements in VL is less than or equal to the mean of B over VH. That is, with the deﬁnition of bL and bH as

1

bL :=

bij

|VL| · n i∈VL j∈[n]

(60a)

1

bH :=

bij ,

|VH| · n i∈VH j∈[n]

(60b)

We have the deterministic relation bL ≤ bH. First consider the case of bL = bH. Since B obeys the total ordering O, we have B = c for some constant
c. Conditional on E3, it can be veriﬁed that for any c ∈ R, the objective (2) attained at (x, B) is strictly positive. Recall from the model (29) that Y = B. Hence, an objective (2) of 0 can be attained by the solution
(0, B). Contradiction to the assumption that (x, B) is the minimizer of the objective.
Now we consider the case of bL < bH. We have that either bL < 0 or bH > 0 (or both). Without loss of generality we assume bH > 0.

40

Constructing an alternative solution We now construct an alternative solution by increasing xi for

every course i ∈ VH by a tiny amount, and prove for contradiction that this alternative solution is preferred

by the tie-breaking rule of minimizing

B

2 F

.

We construct the alternative solution (x , B ) as

xi

if i ∈ VL

xi = xi + ∆ if i ∈ VH

(61)

B = Y − x 1T ,

for some suﬃciently small ∆ > 0 whose value is speciﬁed later. Since (x, B) is a solution, as discussed

previously it has to attain an objective of 0. By the construction (61), it can be veriﬁed that (x , B ) also

attains an objective of 0. In what remains for this step, we ﬁrst show that the alternative solution (x , B )

satisﬁes all ordering constraints by the total ordering O. Then we show that

B

2 F

<

B

2 F

,

and

therefore

(x , B ) is preferred by the tie-breaking rule over (x, B), giving a contradiction.

The alternative solution (x , B ) satisﬁes all ordering constraints in O attain an objective of 0, we have the deterministic relation
yij = xi + bij = xi + bij ∀i ∈ [d], j ∈ [n]. Consider any constraint ((i, j), (i , j )) ∈ O. If i, i ∈ VL, then we have

Since both (x, B) and (x , B ) (62)

bij − bi j

= yij − xi − (yi j
= yij − xi − (yi j
(i)
= bij − bi j < 0,

− xi ) − xi )

where (i) is true because by assumption (x, B) is the optimal solution, and hence B satisﬁes the ordering
constraint of bij ≤ bi j . Similarly if i, i ∈ VH, then (x , B ) also satisﬁes this ordering constraint. Finally, consider the case where one of {i, i } is in VL and the other is in VH. Due to Lemma 33 regarding the ranks combined with the deﬁnition of (VL, VH), it can only be the case that i ∈ VL and i ∈ VH. For any ∆ ∈ (0, ), we have that conditional on Ebad ∩ E1 ∩ E2 ∩ E3,

bij − bi j = (yij − xi) − (yi j − xi ) = (bij − xi) − (bi j − xi − ∆)
(i)
= (bij − bi j ) + (xi + ∆ − xi) < 0,
where (i) is true because the ordering constraint ((i, j), (i , j )) gives bij ≤ bi j . Moreover, we have xi −xi < − due to (59). Hence, all ordering constraints are satisﬁed by the alternative solution (x , B ).

The alternative solution (x , B ) satisﬁes B F < B F , thus preferred by tie-breaking

in the construction (61), we compute

B

2 F

as

B

2 F

=

(yij − xi)2 +

(yij − xi − ∆)2

i∈VL j∈[n]

i∈VH j∈[n]

(i)
=

(bij )2 +

(bij − ∆)2,

i∈VL j∈[n]

i∈VH j∈[n]

where (i) is true by (62). Taking the partial derivative of (63) with respect to ∆, we have





∂ ∂B∆ 2F = 2 |VH| · n∆ −

bij = 2|VH| · n(∆ − bH).

i∈VH j∈[n]

Plugging (63) (64)

41

By the assumption of bH > 0, the partial derivative (64) is strictly negative for any ∆ ∈ 0, bH . Contradiction

to the fact that B (corresponding to ∆ = 0) is the solution with the minimal Frobenius norm

B

2 F

.

Hence,

(x, B) cannot be a solution, and we have

P(Ebad, E1 ∩ E2 ∩ E3) = 0.

Step 4: Invoking Lemma 21 Recall from Step 2 that limn→∞ P(Ebad, E1 ∩ E2 ∩ E3) = 0 implies limn→∞ P(Ebad) = 0. Equivalently, for any > 0 we have

lim P max |xi − xi| <
n→∞ i,i ∈[d]

= 1.

Invoking Lemma 21 completes the proof.

C.4 Proof of Proposition 7
We denote (x(∞), B(∞)) as the values given by expression (3). We prove that

(x(∞), B(∞)) = lim (x(λ), B(λ)).
λ→∞

Denote the minimal value of the ﬁrst term in the objective (2) as

V ∗ := min Y − x1T − B 2 .

x∈Rd ,B ∈Rd×n

F

B satisﬁes O

Denote V as the value of the ﬁrst term attained at (x(∞), B(∞)). By the deﬁnition of V ∗ as the minimal value over the domain, we have V ≥ V ∗. We discuss the following two cases depending on the value of V .

Case of V = V ∗: We have that (x(∞), B(∞)) is the solution for any λ ∈ (0, ∞), because it attains the minimal value separately for the two terms in the objective (2). By Proposition 14, a unique solution exists for any λ ∈ (0, ∞). Hence, the limit limλ→∞(x(λ), B(λ)) exists and we have (x(∞), B(∞)) = limλ→∞(x(λ), B(λ)).

Case of V > V ∗: We ﬁrst show that limλ→∞ B(λ) = 0. That is, we show that for any > 0, there exists

some λ0 > 0, such that

B(λ)

2 F

<

for all λ ∈ (λ0, ∞).

Take λ0 = V −V ∗ , and assume for contradiction that there exists some λ∗ > λ0 such that

B(λ∗)

2 F

>

.

The objective (2) (setting λ = λ∗) attained by (x(λ∗), B(λ∗)) is lower-bounded by

Y

− x(λ∗) − B(λ∗)

2 2

+

λ∗

B(λ∗)

2 F

> V ∗ + λ0

> V ∗ + (V − V ∗) = V.

On the other hand, the objective attained by (x(∞), B(∞)) is V . Hence, (x(∞), B(∞)) attains a strictly smaller value of the objective than (x(λ∗), B(λ∗)) at λ = λ∗. Contradiction to the assumption that (x(λ∗), B(λ∗)) is
the solution at λ = λ∗. Hence, we have limλ→∞ B(λ) = 0. Combining the fact that limλ→∞ B(λ) = 0 with the relation (20) in Lemma 16 (at any λ ∈ [0, ∞)), we
have that for each i ∈ [d],

(λ) 1

xi

= n

j∈[n]

yij − b(ijλ)

1 → n yij
j∈[n]

as λ → ∞,

completing the proof.

42

C.5 Proof of Theorem 9
The proof follows notation in Appendix C.1 and preliminaries in Appendix C.2. By Corollary 19, we assume x∗ = 0 without loss of generality. We also assume without loss of generality that the standard deviation of the Gaussian bias distribution is σ = 1. Given x∗ = 0 and the assumption that there is no noise, model (1) reduces to:

Y = B.

(65)

Both part (a) and part (b) consist of 3 similar steps. We start with the ﬁrst step, and proceed separately for the two remaining steps for the two parts. Step 1: Showing the consistency of our estimator at λ = 0 restricted to the training set Ωt.
In the ﬁrst step, we show that our estimator is consistent under group orderings satisfying part (a) and part (b), on any ﬁxed training set Ωt ⊆ [d] × [n] obtained by Algorithm 1. Note that Theorem 5(a) and Theorem 5(c) give the desired consistency result when the data is full observations Ω = [d] × [n]. It remains to extend the proof of Theorem 5(a) and Theorem 5(c) to any Ωt given by Algorithm 1. The following theorem states that part (a) and part (c) of Theorem 5 still hold for the estimator (15) restricted to Ωt. We use (x(0), B(0)) to denote the solution to (15) restricted to Ωt for the remaining of the proof of Theorem 9.
Theorem 34 (Generalization of Theorem 5 to any Ωt). Consider any ﬁxed Ωt ⊆ [d] × [n] obtained by Algorithm 1. Suppose the partial ordering is one of

(a) any group ordering satisfying the all c-fraction assumption, or

(b) any total ordering.

Then for any > 0 and δ > 0, there exists an integer n0 (dependent on , δ, c, d), such that for every n ≥ n0 and every partial ordering satisfying one of the conditions (a) or (b), the estimator x(0) (as the solution
to (15) restricted to Ωt) satisﬁes

P x(0) − x∗ 2 < ≥ 1 − δ.

(66)

Equivalently, for any > 0, we have

lim P x(0) − x∗ 2 < = 1.

(67)

n→∞

The proof of this theorem is in Appendix C.11.1. Now we consider the consistency of the bias term B. Given the model (65), the objective (15) at λ = 0 equals 0 at the values of (x, B) = (0, B). Hence, objective (15) attains a value of 0 at the solution (x(0), B(0)). Therefore, we have the deterministic relation YΩt = [x(0)1T + B(0)]Ωt . For any (i, j) ∈ Ωt, we have

b(ij0)

=

Yij

−

x(i0)

(i)
=

bij

−

x(i0),

(68)

where equality (i) is true because of the model (65). Combining (68) with (67), we have that for any > 0,

lim P b(ij0) − bij < , ∀(i, j) ∈ Ωt = 1.

(69)

n→∞

This completes Step 1 of the proof. The remaining two steps are presented separately for the two parts.

C.5.1 Proof of part (a)
We ﬁx some constant 1 > 0 whose value is determined later. For clarity of notation, we denote the constant in the all constant-fraction assumption as cf . Step 2: Computing the validation error at λ = 0

43

We ﬁrst analyze the interpolated bias B(0). Recall that Gtk and Gvk denote the set of elements of group k ∈ [r] in the training set Ωt and the validation set Ωv, respectively. By symmetry of the interpolation expression in Line 15 of Algorithm 1 and Deﬁnition 1 of the group ordering, it can be veriﬁed that the interpolated bias bij is identical for all elements within any group k ∈ [r]. That is, for each k ∈ [r], we have

bij = bi j , for any (i, j), (i , j ) ∈ Gvk.

(70)

Denote bk := bij for any (i, j) ∈ Gtk. By (70), we have that bk is well-deﬁned. Denote the random variables btk and bvk as the mean of the (random) bias B in group k ∈ [r], over Gtk and Gvk, respectively. Denote the random variable bvik as the mean of the (random) B of group k ∈ [r] in course i ∈ [d] over Ωv. That is, we deﬁne

bt := 1

bij

k |Gtk|

(i,j)∈Gtk

bv := 1

bij

k |Gvk|

v

(i,j)∈Gk

bv := 1 ik |Gvik|

bij .
v

j∈Gik

(71) (72) (73)

Denote btk likewise as the mean of the estimated bias B over Gtk. Given Y = B from model (65), the validation error at λ = 0 is computed as:

e(0) = 1 |Ωv|

(0)

2

yij − xi − bij

(i,j)∈Ωv

1 = |Ωv|
i∈[d],k∈[r] j∈Gvik

(0)

2

bij − xi − bk .

(74)

We ﬁrst analyze the term bk in (74). The following lemma shows that the interpolation procedure in Algorithm 1 ensures that bk is close to btk, the mean of the estimated bias over Gtk.
Lemma 35. Consider any group ordering O that satisﬁes the all cf -fraction assumption, and any Ωt ⊆ [d]×[n] obtained by Algorithm 1. Then for any λ ∈ [0, ∞] we have the deterministic relation:

bk − bt ≤ 12 · max bij k cf dn (i,j)∈Ωt

∀k ∈ [r].

The proof of this result is provided in Appendix C.11.2. Combining Lemma 35 with the consistency (69) of B(0) from Step 1 and a bound on max(i,j)∈Ωt |bij| from Lemma 25, we have the following lemma.

Lemma 36. Under the same condition as Lemma 35, the interpolated bias at λ = 0 satisﬁes

lim P bk − btk < , ∀k ∈ [r] = 1.
n→∞
The proof of this result is provided in Appendix C.11.3. Recall that bGk denotes the the mean of the bias of any group k ∈ [r]. The following lemma gives concentration inequality results that the quantities bvik and btk are close to bk. Note that this lemma is on the bias B and does not involve any estimator.
Lemma 37. Consider any group ordering O that satisﬁes the all cf -fraction assumption. Consider any ﬁxed training-validation split (Ωt, Ωv) obtained by Algorithm 1. For any > 0, we have

lim P bvik − bGk < , ∀i ∈ [d], k ∈ [r] = 1
n→∞
lim P btk − bGk < , ∀k ∈ [r] = 1.
n→∞

(75a) (75b)

44

The proof of this result is provided in Appendix C.11.4. Combining Lemma 36 and (75) from Lemma 37 with a union bound, we have the following corollary.
Corollary 38. Consider any group ordering O that satisﬁes the all cf -fraction assumption. Consider any ﬁxed Ωt ⊆ [d] × [n] obtained by Algorithm 1. For any > 0, the interpolated bias at λ = 0 satisﬁes

lim P bvik − bk < , ∀i ∈ [d], k ∈ [r] = 1.
n→∞

Consider each i ∈ [d] and k ∈ [r]. The terms in the validation error (74) involving course i and group k are:

e(0) := 1 ik |Ωv| j∈Gv
ik

bij − x(i0) − bk





2

1

= |Ωv| 

2
bij − bk + |Gvik| · x2i − 2

bij − bk xi

j∈Gvik

j∈Gvik

(i) 1 = |Ωv|
j∈Gvik

bij − bk

T1

2 + ||GΩvivk|| x2i − 2||ΩGvvik| | · (bvik − bk)xi,

T2

T3

where (i) is true by the deﬁnition (73) of bvik. We now consider the three terms T1, T2 and T3 (dependent on i and k), respectively.

Term T2: By the convergence (67) of x(0) in Theorem 34(a), we have

lim P T ≤ |Gvik| 2, ∀i ∈ [d], k ∈ [r] = 1.

(76)

n→∞

2 |Ωv| 1

Term T3: We have

T3 ≤ 2 ||GΩvivk|| · bvik − bk · |xi| ≤ 2 bvik − bk · |xi|.

By combining the convergence (67) of x(0) in Theorem 34(a) and Corollary 38 with a union bound, we have

lim P T ≤ 2|Gvik| 2, ∀i ∈ [d], k ∈ [r] = 1.

(77)

n→∞ 3 |Ωv| 1

Term T1: We have

1 T1 = |Ωv|
j∈Gvik

bij − bk

2

1

= |Ωv|

2
bij − bvik + bvik − bk

j∈Gvik





1 = |Ωv| 

(bij − bvik)2 + |Gvik| · (bvik − bk)2 + 2

(bij − bvik)(bvik − bk)

j∈Gvik

j∈Gvik





(i) 1 = |Ωv| 

(bij − bvik)2 + |Gvik| · (bvik − bk)2

j∈Gvik

where inequality (i) holds because have

j∈Gv (bij − bvik) = 0 by the deﬁnition (73) of bvik. By Corollary 38, we ik





1

lim T1
n→∞

<

|Ωv|

(bij − bvik)2 + ||GΩvivk|| 21,

j∈Gvik

∀i ∈ [d], k ∈ [r] = 1.

(78)

45

Combining the three terms from (76), (77) and (78), we bound e(ik0) as



(0)

1

lim eik
n→∞

= T1 + T2 + T3 < |Ωv|

(bij − bvik)2 + 4||ΩGvvik| | 21,

j∈Gvik

 ∀i ∈ [d], k ∈ [r] = 1.

(79)

By the all cf -fraction assumption, the number of groups is upper-bounded by a constant as r ≤ c1f . Taking a union bound of (79) over i ∈ [d] and k ∈ [r], we have







lim P e(0) =
n→∞

(0)

1

eik < |Ωv|



(bij − bvik)2 + 4|Gvik| ·

2
1

=

1

i∈[d],k∈[r]

i∈[d],k∈[r] j∈Gvik





lim P e(0) < 1


n→∞

|Ωv|

(bij

− bvik)2

+4

2
1

=

1.

i∈[d],k∈[r] j∈Gvik

(80)

This completes Step 2 of bounding the validation error at λ = 0. Step 3: Computing the validation error at general λ ∈ Λ , and showing that it is greater than
the validation error at λ = 0 Recall from (16) the deﬁnition of the random set Λ := {λ ∈ [0, ∞] : x(λ) 2 > }. In this step, we show
that

lim P e(λ) > e(0), ∀λ ∈ Λ = 1.

(81)

n→∞

From (81), we have that the estimated quality x(λcv) by cross-validation satisﬁes

and consequently by the deﬁnition of Λ

lim (λcv ∈ Λ ) = 1
n→∞

It remains to prove (81).

lim P
n→∞

x(λcv) 2 <

= 1.

Proof of (81) For any i ∈ [d] and k ∈ [r], the terms in the validation error at any λ ∈ [0, ∞] involving course i and group k are computed as:

(λ)

1

eik = |Ωv|

(λ) (λ) 2

1

bij − xi − bk = |Ωv|

2
bij − bvik + bvik − xi − bk

j∈Gvik

j∈Gvik

(i) 1 = |Ωv|

(bij − bvik)2 + ||GΩvivk||

j∈Gvik

2
bvik − xi − bk ,

Tik

(82)

where (i) is true because j∈Gv (bij − bvik) = 0 by the deﬁnition (73) of bvik. Note that the ﬁrst term in (82) ik
is identical to the ﬁrst term in (79) from Step 2. We now analyze the second term Tik in (82). On the one
hand, by Lemma 20(a), we have

lim P max xi − xi > √ , ∀λ ∈ Λ = 1.

(83)

n→∞ i,i ∈[d]

d

On the other hand, taking a union bound of (75a) in Lemma 37 over i, i ∈ [d], we have

lim P |bvik − bvi k| < √ , ∀i, i ∈ [d], k ∈ [r] = 1.

(84)

n→∞

2d

46

Conditional on (83) and (84), for every λ ∈ Λ and for every k ∈ [r],

max bvik − xi − bk − bvi k − xi − bk
i,i ∈[d]

= max |(bvik − bvi k) − (xi − xi )|
i,i ∈[d]

≥ max (xi − xi ) − max |bvik − bvi k|

i,i ∈[d]

i,i ∈[d]

>√ − √ = √ . d 2d 2d

Hence, conditional on (83) and (84),

2
i,mi ∈a[xd] (bvik − xi − bk)2, (bvi k − xi − bk)2 ≥ 16d ∀k ∈ [r], ∀λ ∈ Λ . (85)

Now consider the terms Tik. By (26a) from Lemma 26 combined with the all cf -fraction assumption, we have

|Gvik| ≥ 1 · |Gik| ≥ cf n = cf .

(86)

|Ωv| |Ωv| 4

4|Ωv| 2d

Conditional on (83) and (84), for every λ ∈ Λ and i ∈ [d],

(i) cf max (Tik + Ti k) ≥

i,i ∈[d]

2d

2

2

bvik − xi − btk + bvi k − xi − btk

(≥ii) cf 2 = cf 2 , 2d 16d 32d2

where inequality (i) is true by (86), and inequality (ii) is true by (85). Now consider the validation error e(λ). Conditional on (83) and (84), for every λ ∈ Λ ,

e(λ) =

(λ) (i) 1 eik ≥ |Ωv|

(bij − bvik)2 +

(Tik + Ti k)

i∈[d],k∈[r]

i∈[d],k∈[r] j∈Gvik

i∈[d],k∈[r]

1

v 2 cf 2

> |Ωv|

(bij − bik) + 32d2 ,

i∈[d],k∈[r] j∈Gvik

where inequality (i) is true by plugging in (82). Hence,



(λ)

1

v 2 cf 2

lim e >

n→∞

|Ωv|

(bij − bik) + 32d2 ,

i∈[d],k∈[r] j∈Gvik

 ∀λ ∈ Λ  = 1.

(87)

We set

1

to

be

suﬃcient

small

such

that

4

2 1

<

. cf 2
32d2

Taking

a

union

bound

of

(87)

with

(80) from Step 2,

we have

lim P e(λ) > e(0), ∀λ ∈ Λ = 1,
n→∞

completing the proof of (81).

C.5.2 Proof of part (b)
We ﬁx some constant 1 > 0 whose value is determined later. Since the partial ordering O is assumed to be a total ordering, we also denote it as π. Step 2: Computing the validation error at λ = 0

47

For any element (i, j) ∈ Ωv, recall that NN(i, j; π) ⊆ [d] × [n] denotes the set (of size 1 or 2) of its nearest neighbors in the training set Ωt with respect to the total ordering π. We use NN(i, j) as the shorthand notation for NN(i, j; π). For any λ ∈ [0, ∞], we deﬁne the mean of the estimated bias over the nearest-neighbor set

b(λ) :=

1

b(λ)

NN(i,j) |NN(i, j)|

ij

(i ,j )∈NN(i,j)

Similarly, we deﬁne

1 bNN(i,j) :=

bi j .

|NN(i, j)|

(i ,j )∈NN(i,j)

Since O is a total ordering, the set of total orderings consistent with O = π is trivially itself, that is, T = {π}. Then in Line 15 of Algorithm 1, the interpolated bias for any element (i, j) ∈ Ωv is b(ijλ) = b(NλN)(i,j).
Recall from the model (65) that Y = B. The validation error at λ = 0 is computed as:

e(0) = 1 |Ωv|
(i,j)∈Ωv

(0)

(0) 2

bij − bNN(i,j) − xi

1 ≤ |Ωv|
(i,j)∈Ωv

(0)

(0) 2

bij − bNN(i,j) + bNN(i,j) − bNN(i,j) + xi

.

(88)

We consider the three terms inside the summation in (88) separately. For the ﬁrst term bij − bNN(i,j) , combining Lemma 27(b) with Lemma 23, we have

lim P bij − bNN(i,j) < 1, ∀(i, j) ∈ Ωv = 1
n→∞

(89)

For the second term |bNN(i,j) − b(N0N) (i,j)|, we have |bNN(i,j) − b(N0N) (i,j)| ≤ maxi∈[d],j∈[n]|bij − b(ij0)|. By the consistency (69) of B(0) from Step 1, we have

lim P |bNN(i,j) − b(N0N) (i,j)| < 1, ∀(i, j) ∈ Ωv = 1.
n→∞

(90)

For the third term x(i0), by (67) in Theorem 34(b), we have

lim P |xi| < 1, ∀i ∈ [d] = 1.

(91)

n→∞

Taking a union bound over the three terms (89), (90) and (91) and plugging them back to (88), the validation error at λ = 0 satisﬁes

lim

P

e(0)

≤

9

2 1

= 1.

(92)

n→∞

Step 3: Computing the validation error at general λ ∈ Λ , and showing that it is greater than
the validation error at λ = 0 Recall the deﬁnition Λ := {λ ∈ [0, ∞] : x(λ) 2 > }. In this step, we establish

lim (λcv ∈ Λ ) = 1.
n→∞
By Lemma 20(a) combined with the assumption that d = 2, we have

lim P x(1λ) − x(2λ) > √ , ∀λ ∈ Λ = 1.

(93)

n→∞

2

E

48

We denote the the event in (93) as E. We deﬁne

Λ2>1 := λ ∈ [0, ∞] : x(2λ) − x(1λ) > √ 2

(94a)

Then we have

Λ1>2 := λ ∈ [0, ∞] : x(1λ) − x(2λ) > √ . 2

(94b)

Λ ⊆ Λ2>1 ∪ Λ1>2 | E.

(95)

We ﬁrst analyze Λ2>1. We discuss the following two cases, depending on the comparison of the mean of the

bias for the two courses.

Case 1: j∈[n] b1j ≥ j∈[n] b2j

We denote the event that Case 1 happens as E1 := { j∈[n] b1j ≥ show

j∈[n] b2j}. In this case, our goal is to

lim P λcv ∈ Λ ∩ Λ2>1, E1 = lim (E1).

(96)

n→∞

n→∞

To show (96) it suﬃces to prove

lim P Λ ∩ Λ2>1 = ∅, E1
n→∞
We separately discuss the cases of λ = ∞ and λ ∈ ∞.

= lim P(E1).
n→∞

Showing ∞ ∈ Λ ∩ Λ2>1: Denote the mean of the bias in each course in the training set Ωt as bti :=

1 nt

j∈Ωt bij for i ∈ {1, 2}. By (28a) in Lemma 28, we have

i





lim P bt − 1

b1j < − = 0

n→∞  1 n

 8

j∈[n]





lim P bt − 1

b2j > = 0

n→∞  2 n

 8

j∈[n]

(97a) (97b)

Taking a union bound of (97), we have





lim P bt − bt > 1 (b1j − b2j) −

= 1.

n→∞  1 2 n

 4

j∈[n]

(98)

E
Denote this event in (98) as E . Hence, we have

bt1 − bt2 > − 4 (E , E1) (99)
Recall from Proposition 15 that we have our estimator at λ = ∞ equals to the sample mean per course. That is, x(∞) = bt1 . Hence, we have
bt2

By the deﬁnition of Λ2>1, we have

x(2∞) − x(1∞) < 4 (E , E1). ∞ ∈ Λ ∩ Λ2>1 | (E , E1).

(100)

49

Showing λ ∈ Λ ∩ Λ2>1 for general λ ∈ [0, ∞): As an overview, we assume there exists some λ ∈ Λ ∩ Λ2>1 \ {∞} and derive a contradiction.
Denote the mean of the bias in the training set Ωt as bt := |Ω1v| (i,j)∈Ωv bij = bt1+2 bt2 . Since λ ∈ Λ2>1, we have x2(λ) − x(1λ) > √2 . By (21b) in Lemma 17, we have
x(λ1) + x(λ2) = 2bt,

and hence x(λ) can be reparameterized as

x(λ) = bt + ∆ −1 , for some ∆ > √ .

1

22

(101)

The following lemma gives a closed-form formula for 2-regularized isotonic regression. Recall that M denotes the monotonic cone, and the isotonic projection for any y ∈ Rd is deﬁned in (14) as ΠM(y) = arg minu∈M y − u 22.
Lemma 39. Consider any y ∈ Rd and any λ ∈ [0, ∞). Then we have

min
u∈M

y − u 22 + λ u 22 = 1 +1 λ y − ΠM(y) 22 + 1 +λ λ y 22.

(102)

The proof of this result is provided in Appendix C.11.5. We denote the objective (15) under any ﬁxed x ∈ Rd as

L(x) := min Y − x1T − B 2 + λ B 2

B obeys π

Ωt

Ωt

(=i) 1 +1 λ (Y − x1T ) − Ππ(Y − x1T ) 2Ωt + 1 +λ λ Y − x1T 2Ωt ,

L1 (x)

L2 (x)

(103)

where equality (i) is true by (102) in Lemma 39. We now construct an alternative estimate x = bt 1 , and 1
show that

L(x) > L(x ) ∀λ ∈ Λ ∩ Λ2>1 \ {∞}.

We consider the two terms L1(x) and L2(x) in (103) separately.

Term L1: Recall from the model (65) that Y = B. Hence, Y satisﬁes the total ordering π, and hence Y − x 1T = Y − bt 1 1T satisﬁes the total ordering π. That is,
1n

Ππ(Y − x 1T ) = Y − x 1T .

Hence,

0 = L1(x ) ≤ L1(x(λ)) ∀λ ∈ [0, ∞].

(104)

Term L2: We have

L2(x) − L2(x ) =

Y − x(λ)1T

2 Ωt

−

Y − x 1T

2 Ωt





= (b1j − x(1λ))2 + (b2j − x(2λ))2 −  (b1j − x1)2 + (b2j − x2)2

j∈Ωt1

j∈Ωt2

j∈Ωt1

j∈Ωt2

= nt 2bt1(x1 − x(1λ)) + 2bt2(x2 − x(2λ)) + ((x(1λ))2 − (x1)2) + ((x(2λ))2 − (x2)2)

= nt[2∆(bt1 − bt2) + 2∆2]
(i)
= 2nt∆(bt1 − bt2 + ∆) > 0 | (E , E1),

50

where inequality (i) is true by combining (99) with (101). Hence, we have L2(x) > L2(x ), ∀λ ∈ Λ ∩ Λ2>1 \ {∞} | (E , E1).
Combining the term L1 from (104) and the term L2 from (105), we have

(105)

L(x(λ)) > L(x ), ∀λ ∈ Λ ∩ Λ2>1 \ {∞} (E , E1).

Contradiction to the assumption that x(λ) is optimal. Hence, we have

λcv ∈ Λ ∩ Λ2>1 \ {∞} | (E , E1).

(106)

Combining the cases of λ = ∞ from (100) and λ = ∞ from (106), we have

λcv ∈ Λ ∩ Λ2>1 | (E , E1).

Hence,

P (λcv ∈ Λ ∩ Λ2>1, E1) ≥ P(E , E1) = P(E1) − P(E1 ∩ E ) ≥ P(E1) − P(E )

(107)

Taking the limit of (107), we have

(i)
lim P (λcv ∈ Λ ∩ Λ2>1, E1) = lim P(E1),

n→∞

n→∞

(108)

where (i) is true by (98). Case 2: j∈[n] b1j < j∈[n] b2j
Denote the event that Case 2 happens as E2 := j∈[n] b1j < j∈[n] b2j . Our goal is to ﬁnd a set of elements on which the validation error is large. For any constant c > 0, we deﬁne the set:

Sc := {(j, j ) ∈ [n]2 : 0 < b2j − b1j < c}.

(109)

Let c

> 0 be a constant.

Denote

E

v c

,c

as the event that there exists distinct values (j1, . . . , jc n) and distinct

values (j1, . . . , jc n), such that (jk, jk) ∈ Sc ∩ Ωv for all k ∈ [c n]. That is, the set Sc ∩ Ωv contains a subset

of size at least c n of pairs (j, j ), such that each element b1j and b2j appears at most once in this subset.

We denote this subset as S .

The following lemma bounds the probability that Ecv ,c happens under case E2.

Lemma 40. Suppose d = 2. Assume the bias is distributed according to assumption (A2) with σ = 1. For any c > 0, there exists a constant c > 0 such that

lim P Ecv ,c ∩ E2 = lim P(E2).

n→∞

n→∞

The proof of this result is provided in Appendix C.11.6. Now consider the the validation error contributed by the pairs in the set S . We have

e(λ) ≤ 1 |Ωv|
(j,j )∈S

(λ)

(λ) 2

(λ)

(λ) 2

b1j − bNN(1,j) − x1

+ b2j − bNN(2,j ) − x2

.

(110)

We consider each individual term (j, j ) ∈ S . On the one hand, we have b1j < b2j by the deﬁnition (109) of Sc. Therefore, the element (1, j) is ranked lower than (2, j ) in the total ordering T . According to Algorithm 1, it can be veriﬁed that their interpolated bias satisﬁes

b(NλN) (1,j) ≤ b(NλN) (2,j )

∀λ ∈ [0, ∞].

(111)

51

On the other hand, we have

(i)
b1j − x1 − (b2j − x2) = (b1j − b2j ) + (x2 − x1) > − 2 + √2 = 5 ,

∀λ ∈ Λ ∩ Λ2>1

(E

v c

,

, E),

2

(112)

where (i) is true by the deﬁnition of Sc in (109) (setting c = 2 ), and the deﬁnition 94 of Λ2>1. Combining (111) and (112), we have that for all (j, j ) ∈ S :

(λ)

(λ) 2

b1j − bNN(1,j) − x1 +

b2j − b(λ)

− x(λ) 2 ≥ min min (v1 − u1)2 + (v2 − u2)2

NN(2,j )

2

u1,u2∈R v1,v2∈R

u1≤u2 v1−v2> 5

2
>, 50

∀λ ∈ Λ ∩ Λ2>1 (Ecv , , E). 2

(113)

Conditional on Ecv , , there are at least c n such non-overlapping pairs. Plugging (113) to (110), the validation 2
error is lower-bounded as

e(λ) ≥ 1 c n · 2 ≥ 2 c n · 2 = c 2 , |Ωv| 50 dn 50 25d

∀λ ∈ Λ ∩ Λ2>1 (Ecv , , E). 2

(114)

Setting the constant P e(λ) ≥ e(0),

1

to

be

a

suﬃciently

small

constant

such

that

9

2 1

<

c25d2 ,

we

have

(λ) c 2

2

(0)

∀λ ∈ Λ ∩ Λ2>1, E2 ≥ P e > 25d > 9 1 > e ,

∀λ ∈ Λ ∩ Λ2>1, E2

(λ) c 2

(0)

2

≥ P e > 25d , E2 − P e > 9 1, E2

(i)

≥P

Ecv , , E, E2

−P

e(0)

>

9

2 1

2

(115)

=P

E

v c

,

,E

−P

Ecv , , E, E2

−P

e(0)

>

9

2 1

,

2

2

(116)

where (i) is true by (114). Taking the limit of n → ∞ in (116), we have

lim P e(λ) ≥ e(0), ∀λ ∈ Λ ∩ Λ2>1, E2 = lim P(E2).

n→∞

n→∞

and (ii) is true by combining Lemma 40, (93) and (92) from Step 2. Equivalently,

lim P (λcv ∈ Λ ∩ Λ2>1, E2) = 1.
n→∞
Finally, combining the two cases from (108) and (117), we have

(117)

lim P (λcv ∈ Λ ∩ Λ2>1) = lim P (λcv ∈ Λ ∩ Λ2>1, E1) + lim P (λcv ∈ Λ

n→∞

n→∞

n→∞

= lim P(E1) + lim P(E2) = 1.

n→∞

n→∞

By a symmetric argument on the set Λ1>2, we have

∩ Λ2>1, E2)

Hence, we have

lim P (λcv ∈ Λ ∩ Λ1>2) = 1.
n→∞

(118a) (118b)

lim P (λcv ∈ Λ ) ≥ lim P (λcv ∈ Λ , E)

n→∞

n→∞

(i)

≥ lim P (λcv ∈ Λ ∩ Λ1>2, E) + lim P (λcv ∈ Λ ∩ Λ2>1, E)

n→∞

n→∞

(ii)
≥ lim P (λcv ∈ Λ ∩ Λ1>2) + P (λcv ∈ Λ ∩ Λ2>1) − 2 lim P(E) = 1,

n→∞

n→∞

where inequality (i) is true by (95), and equality (ii) is true by combining (118) with (93). This completes the proof.

52

C.6 Proof of Theorem 10
The proof follows notation in Appendix C.1 and preliminaries in Appendix C.2. Similar to the proof of Theorem 9, without loss of generality we assume x∗ = 0 and the standard deviation of the Gaussian noise is η = 1. Under this setting, the model (1) reduces to:

Y = Z.

(119)

The proof consists of 3 steps that are similar to the steps in Theorem 9. Both part (a) and part (b) share the
same ﬁrst two steps as follows. We ﬁx some constants 1, 2 > 0, whose values are determined later. Step 1: Showing the consistency of our estimator at λ = ∞ restricted to the training set Ωt
By Proposition 15, our estimator x(∞) at λ = ∞ is identical to taking the sample mean of each course. By the model (119), conditional on any training-validation split (Ωt, Ωv) given by Algorithm 1, each observation
is i.i.d. noise of N (0, 1). Recall from (7) that the number of observations in each course restricted to the training set Ωt is nt = n2 . Given the assumption (A3) that the number of courses d is a constant, sample mean on the training set Ωt is consistent. That is,

lim P x(∞) ∞ < 1 = 1.
n→∞

(120)

By Proposition 15, we have B(∞) = 0.
Step 2: Computing the validation error at λ = ∞ Recall from Algorithm 1 that the interpolated bias bij for any element (i, j) ∈ Ωv is computed as the
mean of the estimated bias B from its nearest neighbor set in the training set Ωt. Since the estimated bias is B(∞) = 0, the interpolated bias is B(∞) = 0. Recall the model (119) of Y = Z. The validation error at λ = ∞ is computed as

e(∞) = 1 |Ωv|
(i,j)∈Ωv

yij − x(i∞) − b(ij∞)

2

1

= |Ωv|

(i,j)∈Ωv

(∞) 2
zij − xi





1 = |Ωv| 

zij 2 −2

zij x(i∞) +

(x(i∞))2  .

(i,j)∈Ωv

(i,j)∈Ωv

(i,j)∈Ωv

T1

T2

T3
(121)

We consider the three terms T1, T2 and T3 in (121) separately. For the term T1, we have E[zi2j] = η2 = 1. The number of samples is |Ωv| = dnv = d n2 . By Hoeﬀding’s inequality we have





1

lim
n→∞

P



|Ω

v

|

zi2j < 1 + 1 = 1.

(i,j)∈Ωv

(122)

For the term T2, we have E[zij] = 0. By Hoeﬀding’s inequality and a union bound over i ∈ [d] we have



1

lim
n→∞

P



|Ω

v

|

zij < 1,

j∈Ωvi

 ∀i ∈ [d] = 1.

(123)

Combining (123) with the consistency result (120) on x(∞) from Step 1, we have

lim P 1 |T2| < d 2 = 1.

n→∞ |Ωv|

1

(124)

53

For the term T3, we have

1

2

|Ωv| T3

≤

max|xi|
i∈[d]

.

(125)

Combining (125) with the consistency result (120) on x(∞) from Step 1, we have

lim P 1 T3 < 2 = 1.

n→∞ |Ωv|

1

(126)

Taking a union bound of the terms T1, T2 and T3 from (122), (124) and (126) and plugging them back to (121), we have

lim P

e(∞) ≤ (1 +

1) + d

2 1

+

2 1

=

1

+

1

+ (d + 1)

2 1

= 1.

n→∞

(127)

Step 3 (preliminaries): Computing the validation error at general λ ∈ Λ , and showing that
it is greater than the validation error at λ = ∞ We set up some preliminaries for this step that are shared between part (a) and part (b). Then we discuss
the two parts separately. Recall from (16) the deﬁnition of Λ := {λ ∈ [0, ∞] : x(λ) 2 > }. In this step, we show that

Then from (128) we have

lim P e(λ) > e(∞), ∀λ ∈ Λ = 1.
n→∞

(128)

lim (λcv ∈ Λ ) = 1,
n→∞
yielding the result of Theorem 10. It is suﬃcient to establish (128). We now give some additional preliminary results for this step. By Lemma 20, we have

lim P max xi − xi > √ , ∀λ ∈ Λ

n→∞ i,i ∈[d]

d

= 1.

(129)

E

We denote this event in (129) as E. Both parts also use the following lemma that bounds the magnitude of the estimated bias B given some
value of x.
Lemma 41. Let Ω ⊆ [d] × [n] be any non-empty set. For any λ ∈ [0, ∞], the solution (x(λ), B(λ)) restricted to the set Ω satisﬁes the deterministic relation

max b(ijλ) ≤ max |yij | + x(λ) ∞.

(i,j)∈Ω

(i,j)∈Ω

(130)

The proof of this result is provided in Appendix C.12.1. Now we proceed diﬀerently for Step 3 for part (a) and part (b).

C.6.1 Proof of part (a)

Step 3 (continued): For clarity of notation, we denote the constant in the single constant-fraction as cf . We analyze the validation error at any λ ∈ Λ similar to Step 2. The diﬀerence is that Step 2 (at λ = ∞)
uses the consistency of x(∞) from Step 1 on to bound the validation error. However, x(λ) may not be consistent
for any general λ ∈ Λ . Hence, we consider the following two subsets of Λ depending on the value of x.
Similar to the proof of Theorem 9(a), by Algorithm 1 the interpolated bias for elements in each group k ∈ [r] is identical for all (i, j) ∈ Gvk. That is,

bij = bi j

∀(i, j), (i , j ) ∈ Gvk.

(131)

We denote the interpolated bias for group k as bk := bij for (i, j) ∈ Gvk.

54

Case 1: Λ1 := λ ∈ [0, ∞] : maxi,i ∈[d] xi − xi > 8 cdf . Let kf ∈ [r] be a group that satisﬁes the single cf -fraction assumption. By the deﬁnition of Λ1 we have
maxi,i ∈[d] (xi + bkf ) − (xi + bkf ) > 8 cdf for any λ ∈ Λ1, which implies that

d

max xi + bkf > 4
i∈[d]

cf

∀λ ∈ Λ1.

(132)

Combining (26a) from Lemma 26 with the single cf -fraction assumption, one can see

vikf ≥ i4kf > cf4n . Given (133), by Hoeﬀding’s inequality we have

(133)





lim P 

1{zij > 0} ≥ cf n  = 1

n→∞ j∈Gv 12

ikf





lim P 

1{zij < 0} ≥ cf n  = 1.

n→∞ j∈Gv 12

ikf

(134a) (134b)

We denote the event



 E1 :=

1{zij > 0} ≥ cf n ,

j∈Gv 12

ikf



 ∀i ∈ [d] ∩

1{zij < 0} ≥ cf n ,

 j∈Gv 12

ikf


 ∀i ∈ [d] .


(135)

Given that d is a constant by the assumption (A3), taking (134) with a union bound over i ∈ [d], we have

lim P(E1) = 1.
n→∞

(136)

Let i∗ be a random variable (as a function of λ) deﬁned as i∗ := arg maxi∈[d] xi + bkf where the tie is broken arbitrarily. Conditional on E1, for any λ ∈ Λ1 we have the deterministic relation

e(λ) = 1 |Ωv|
k∈[r] (i,j)∈Gvk

zij − x(iλ) − b(kλ)

2

1

≥

(zij − xi − bk )2

|Ωv|

f

(i,j)∈Gvkf

1 ≥

(zi∗j − xi∗ − bk )2

|Ωv|

f

j ∈Gvi∗ k

f

2

(≥i) 1 cf n 4 d

|Ωv| 12

cf

= 2 · cf n 16d = 8 , ∀λ ∈ Λ1 E1. dn 12 cf 3

(137)

where (i) is true by (132) and the deﬁnition (135) of E1. Combining (137) with (136), we have

lim P

e(λ)

≥

4 ,

n→∞

3

∀λ ∈ Λ1

≥ P (E1) = 1.

(138)

55

Case 2: Λ2 = Λ ∩ λ ∈ [0, ∞] : maxi,i ∈[d] xi − xi ≤ 8 cdf . Note that we have Λ ⊆ Λ1 ∪ Λ2 by the deﬁnition of Λ1 and Λ2. We decompose the validation error as:

e(λ) = 1 |Ωv|
k∈[r] (i,j)∈Gvk

(λ) (λ) 2
zij − xi − bk





= |Ω1v|  zi2j − 2 zij

(i,j)∈Ωv

k∈[r] (i,j)∈Gvk

x(iλ) + b(kλ)

+
k∈[r] (i,j)∈Gvk

(λ) (λ) 2
xi + bk 



1 = |Ωv| 

zi2j −2

zij x(iλ) +2

zij b(kλ) +

(i,j)∈Ωv

(i,j)∈Ωv

k∈[r] (i,j)∈Gvk

k∈[r] (i,j)∈Gvk

x(iλ) + b(kλ)


2
.

(139)

T1

T2

T3

T4

We analyze the four terms T1, T2, T3 and T4 in (139) separately.

Term T1:

Similar to (122) from Step 2, by Hoeﬀding’s inequality we have





1

lim
n→∞

P



|Ω

v

|

zi2j > 1 − 2 = 1.

(i,j)∈Ωv

(140)

Term T2: Recall that d is a constant by the assumption (A3). Similar to (123) from Step 2, by Hoeﬀding with a union bound over i ∈ [d], we have



1

lim
n→∞

P



|Ω

v

|

zij < ,

j∈Ωvi

 ∀i ∈ [d] = 1.

(141)

E2

Denote this event in (141) as E2. We now bound x ∞. By Hoeﬀding’s inequality, on the training Ωt we have:



1

lim
n→∞

P



|Ω

t

|

zij <

(i,j)∈Ωt

 1
 = 1. dcf

(142)

E2

Plugging (21b) in Lemma 17 to (142), we have

(λ)

1

d

xi = nt

zij < cf

i∈[d]

(i,j)∈Ωt

Combining (143) with the deﬁnition of Λ2, we have

d x ∞ ≤ 8 cf

∀λ ∈ Λ2, conditional on E2. ∀λ ∈ Λ2 E2.

(143) (144)

To see (144), assume for contradiction that (144) does not hold. Consider the case of xi∗ i∗ ∈ [d]. Then by the deﬁnition of Λ2, we have xi > 0 for all i ∈ [d]. Then we have Contradiction to (143). A similar argument applies if xi∗ < −8 cdf . Hence, (144) holds.

> 8 cdf i∈[d] xi

for some > 8 cdf .

56

Finally, combining (144) with (141), we have:

1

1

|Ωv| |T2| = |Ωv|

zij xi

(i,j)∈Ωv

d

≤ |Ωv| mi∈a[dx]

zij · x ∞ < 8d

(i,j)∈Ωv

d cf 2

Hence, we have

∀λ ∈ Λ2, conditional on (E2, E2).

1

d

(i)

lim P
n→∞

|Ωv| |T2| < 8d

, cf

∀λ ∈ Λ2

≥ lim P (E2 ∩ E2) = 1,
n→∞

where (i) is true by (141) and (142).

(145) (146)

Term T3: We use the following standard result derived from statistics.

Lemma 42. Consider any ﬁxed d ≥ 1. Let Z ∼ N (0, Id). Then we have





lim P 

sup

θT Z

≤

1
d4

=

1.

d→∞  θ 2=1



θ1 ≤...≤θd

For completeness, the proof of this lemma is in Appendix C.12.2. We now explain how to apply Lemma 42 on BΩt .

The ordering of B: Take any arbitrary total ordering π ∈ T that is consistent with the partial ordering O. Recall from (131) that the interpolated bias within each group k ∈ [r] is identical, so B satisﬁes the total ordering π.

Bounding B Ωt : We bound each bk. Recall that each bk is a mean of B on its nearest-neighbor set. Hence, we have

max|b | ≤ max b(λ) (≤i) max |y | + x(λ)

k k∈[r]

(i,j)∈Ωt ij

ij (i,j)∈Ωt

∞

∀λ ∈ [0, ∞],

(147)

where (i) is true by (130) in Lemma 41. We consider the term max(i,j)∈Ωv |yij| on the RHS of (147). Recall from the model (119) that Y = Z. Hence, we have

(i)
lim P max |yij| < 2 log dn = 1,
n→∞ (i,j)∈Ωv

(148)

E2

where (i) is true by Lemma 25. Plugging (148) and the bound on x ∞ from (144) to (147), we have that conditional on E2 and E2,

max|bk|≤ max |yij| + x(λ) ∞

k∈[r]

(i,j)∈Ωt

d ≤ 2 log dn + 8
cf

∀λ ∈ Λ2 (E2, E2 ).

57

Hence, we have

B Ωt ≤ |Ωt| · max bk ≤ √dnv 2 log dn + 8 d

k∈[r]

cf

∀λ ∈ Λ2 (E2, E2 ).

and therefore lim P
n→∞

√

d

B Ωt ≤ dnv 2 log dn + 8 cf , ∀λ ∈ Λ2 ≥ nl→im∞ P(E2 ∩ E2 ) = 1.

(149)

Applying Lemma 42: For the term T3, for any constant C > 0, we have

P

|T3|

<

C (dnt )

1 4

,

∀λ ∈ Λ2 ≥ P

T3

<

(dnt)

1 4

,

C

E3

∀λ ∈ Λ2 ∩

We have

√

√

Setting C = dnv 2 log dn + 8

P(E3 ∩ E4) = P(E4) + P(E3 ∩ E4) cdf , by (149) we have

P(E4) = 0.

B ≤ 1,
C
Ωt E4

∀λ ∈ Λ2

(150) (151) (152)

Applying Lemma 42 on BCΩt , we have

lim P(E3 ∩ E4) = 0.
n→∞
Plugging (152) and (153) to (151), we have

Combining (154) with (150), we have

lim P(E3 ∩ E4) = 0.
n→∞

(153) (154)

lim P

|T3|

<

C

(

dnt

)

1 4

=

(dnt

)

3 4

2 log dn + 8

d

,

∀λ ∈ Λ2

= 1.

n→∞

cf

Hence, we have

1

lim P
n→∞

|Ωv| |T3| < 2

= 1.

(155)

Term T4: Recall that kf denotes a group kf that satisﬁes the single cf -fraction assumption. By the deﬁnition of E from (129), we have

Therefore, we have

max (xi + bkf ) − (xi + bkf ) > √

i,i ∈[d]

d

∀λ ∈ Λ2, E.

2
i,mi ∈a[xd] (xi + bkf )2 + (xi + bkf )2 > 4d

∀λ ∈ Λ2 E.

(156) (157)

58

We bound the term T4 as

1

1

(i) 2

2

cf n

2

cf 2

|Ωv| T4 ≥ |Ωv|

(xi + bkf )

≥· dn

4

· 4d = 8d2

(i,j)∈Gvkf

where (i) is true by combining (133) and (157). Hence,

cf 2 P T4 ≥ 8d2

∀λ ∈ Λ2

≥ P (E) = 1.

∀λ ∈ Λ2 E,

(158)

Putting things together: Plugging the four terms from (140), (141), (155) and (158) respectively back to (139), we have

lim P e(λ) > (1 − 2) + 8d
n→∞

d

cf 2

cf 2 + 2 + 8d2 ,

∀λ ∈ Λ2

= 1.

(159)

Finally, combining the two cases from (138) and (159), we have

(λ) 8

d

cf 2

lim P e
n→∞

≥ ∧ 1 + 16d 3

cf 2 + 8d2

,

∀λ ∈ Λ

= 1.

(160)

Recall from (127) that the validation error at λ = ∞ is bounded as

lim P

e(∞) ≤ 1 +

1

+ (d + 1)

2 1

= 1.

n→∞

(161)

Combining (160) and (161) with choices of ( 1, 2) (dependent on , d, cf ) such that 83 ∧ 1 + 16d cdf 2 + c8fd22 > 1 + 1 + (d + 1) 21, we have

completing the proof.

lim P e(∞) > e(0), ∀λ ∈ Λ = 1,
n→∞

C.6.2 Proof of part (b)
For clarity of notation, we denote the constant in the constant-fraction interleaving assumption as cf . Since O is a total ordering, we also denote it as π.

Step 3 (continued): Combining (21b) with Hoeﬀding’s inequality, we have



1

lim P 
n→∞

|x1

+

x2|

=

nt

zij <

(i,j)∈Ωt

16 ∧√ ,
cf

E1

 ∀λ ∈ Λ  = 1.

We denote this event in (162) as E1.

59

(162)

Analyzing the number of interleaving points Let S ⊆ [2n − 1] denotes the interleaving points. Recall that (it, jt) denotes element of rank t, and tij denotes the rank of the element (i, j). We slightly abuse the notation to say (i, j) ∈ S if tij ∈ S, and also for other deﬁnitions of subsets of interleaving points later in the proof. Denote Si ⊆ S as the set of interleaving points in course i ∈ {1, 2}:

Si = S ∩ {t ∈ [2n − 1] : it = i}.

Denote Siv as the set of interleaving points in Si that are in the validation set: Siv = Si ∩ Ωv.

We deﬁne Spairs as a set of pairs of interleaving points as: Spairs := {(t, t ) ∈ [2n − 1]2 : t ∈ S1v, t ∈ S2v, t < t }.

Deﬁne Ec as the event that there exists distinct values (t1, t1, . . . , tcn, tcn) such that (tk, tk) ∈ Spairs for all k ∈ [cn]. That is, Spairs includes cn distinct pairs where each interleaving point appears at most once. We
deﬁne Spairs likewise as

Spairs := {(t, t ) ∈ [2n − 1]2 : t ∈ S2v, t ∈ S1v, t < t }.

and deﬁne Ec likewise.

The following lemma bounds the probability of the event E 1 and E 1 .

36

36

Lemma 43. Suppose d = 2. Then we have

lim P E 1 ∩ E 1 = 1.

n→∞

36

36

The proof of this result is provided in Appendix C.12.3. Denote S+ as the set of the half of the highest interleaving points and S− as the set of the half of the lowest interleaving points. That is, we deﬁne
S+ := S ∩ {t ∈ [2n − 1] : t > median(S)} S− := S ∩ {t ∈ [2n − 1] : t < median(S)}.

Furthermore, for i ∈ {1, 2}, we deﬁne

Siv+ := S+ ∩ Si ∩ Ωv Siv− := S− ∩ Si ∩ Ωv.

The following lemma lower-bounds the size of Siv+ and Siv−.

Lemma 44. We have

lim P |T | ≥ cf n ,

n→∞

36

∀T ∈ {S1v+, S1v−, S2v+, S2v−} = 1.
E2

The proof of this result is provided in Appendix C.12.4. We denote this event in Lemma 44 as E2.

Bounding the validation error Similar to part (a), we discuss the following two cases depending on the value of x.

60

Case 1: Λ1 = Λ ∩ λ ∈ [0, ∞] : x(1λ) < − √3c2f It can be veriﬁed that due to (162), we have

(λ)

32

16

(λ)

x1

< −√ cf

<

√ cf

< x2

∀λ ∈ Λ1 E.

By Hoeﬀding’s inequality combined with Lemma 44, we have





lim P 

1{zij > 0} > cf n  = 1

n→∞

96

(i,j)∈S1v−





lim P 

1{zij < 0} > cf n  = 1.

n→∞

96

(i,j)∈S2v+

Denote the event







 E3 :=

cf n  

1{zij > 0} >

∩

cf n 

1{zij < 0} >

.

(i,j)∈Sv−

96  (i,j)∈Sv+

96 

1

2

Taking a union bound of (164), we have

(163)
(164a) (164b)

lim P(E3) = 1.
n→∞

(165)

We slightly abuse the notation and denote bt as the value of the interpolated bias on the element of rank t. That is, we deﬁne bt := bitjt . It can be veriﬁed that bt is non-decreasing in t due to the nearest-neighbor interpolation in Algorithm 1. Hence, bt ≤ 0 for all t ∈ S− or bt ≥ 0 for all t ∈ S+.
First consider the case bt ≤ 0 for all t ∈ S−. We bound the validation error at λ ∈ Λ1 as:

e(λ) ≥ 1 |Ωv|
(i,j)∈S1v−
(≥i) |Ω1v| · S1v− ·

(λ) (λ) 2
zij − x1 − bij

16

2 (i) 1 cf n 256 8

0+ √ +0 ≥

=,

cf

n 96 cf 3

∀λ ∈ Λ1 (E1, E2, E3),

(166) (167)

where (i) is true by (163) and the deﬁnition of E3, and (ii) is true by the deﬁnition of E2. Hence, we have

lim

e(λ) ≥ 8

(i)
∀λ ∈ Λ1, {bt ≤ 0 for all t ∈ S−} ≥ P bt ≤ 0 for all t ∈ S− ,

n→∞

3

(168a)

where (i) is true by (162), Lemma 44 and (165). By a similar argument, we have

lim e(λ) ≥ 8 ∀λ ∈ Λ1, {bt ≥ 0 for all t ∈ S+} ≥ P bt ≥ 0 for all t ∈ S+ ,

n→∞

3

(168b)

Summing over (168), we have

lim P

e(λ)

≥

8 ,

n→∞

3

∀λ ∈ Λ1

= 1.

(169)

61

Case 2: Λ2 = Λ ∩ λ ∈ [0, ∞] : x(1λ) > − √3c2f It can be veriﬁed that due to (162), we have

32

48

− √cf < {x1, x2} < √cf .

(170)

Similar to Case 2 in part (a), we decompose the validation error at λ ∈ Λ2 as

e(λ) = 1 |Ωv|
(i,j)∈Ωv

(λ) (λ) 2
zij − xi − bij



1 = |Ωv| 

zi2j −2

zij x(iλ) −2 zij b(ijλ) +

(i,j)∈Ωv

(i,j)∈Ωv

(i,j)

(i,j)

x(iλ) + b(ijλ)


2
.

T1

T2

T3

T4

Given that x ∞ is bounded by a constant by (170), the analysis of the terms T1, T2 and T3 follows the proof in part (a). We have

1

lim P
n→∞

|Ωv| T1 > 1 − 2

= 1.



1

96

lim
n→∞

P



|

Ω

v

|

|T2| < √cf

(i,j)∈Ωv

 2 = 1.

1

lim P
n→∞

|Ωv| |T3| < 2

= 1.

(171a) (171b) (171c)

Now we consider the last term T4. Recall from (129) that

|x2 − x1| > √ ∀λ ∈ Λ2 E. 2

First consider the case of Λ2>1 := λ ∈ [0, ∞] : x(2λ) − x(1λ) > √2 . Consider any (t, t ) ∈ Spairs. By the deﬁnition of Spairs we have t < t . Hence, we have bt ≤ bt due to the nearest-neighbor interpolation in Algorithm 1. Hence, we have x2 + bt − (x1 + bt) > √2 and consequently

2
(x1 + bt)2 + (x2 + bt )2 > 8

∀λ ∈ Λ2 ∩ Λ2>1 E.

We bound the term T4 as:

1

1

T4 ≥

(x1 + bt)2 + (x2 + bt )2

|Ωv|

|Ωv|

(t,t )∈Spairs

(≥i) 1 · cf n · 2 = cf 2 2n 36 8 576

∀λ ∈ Λ2 ∩ Λ2>1

(E 1 , E), 36

(172a)

where inequality (i) is true by the deﬁnition of E 316 . Deﬁne Λ1>2 := λ ∈ [0, ∞] : x(1λ) − x(2λ) > √2 . With a similar argument, we have

1

cf 2

|Ωv| T4

≥

, 576

∀λ ∈ Λ2 ∩ Λ1>2 (E 1 , E). 36

(172b)

62

Combining (172), we have

1

cf 2

|Ωv| T4

≥

, 576

∀λ ∈ Λ2 (E 1 , E 1 , E).

36

36

By Lemma 43 and (129), we have

lim P
n→∞

1

cf 2

|Ωv| T4

≥

, 576

∀λ ∈ Λ2

≥ lim P E 1 , E 1 , E

n→∞

36

36

= 1.

(173)

Putting things together: Combining the four terms from (171) and (173), we have

(λ)

128

cf 2

128

lim P e > 1 − 2 − √ 2 − 2 2 + = 1 − 3 + √

n→∞

cf

576

cf

cf 2 + , ∀λ ∈ Λ2 = 1. 2 576

Combining the two cases from (169) and (174), we have

lim P

e(λ) > 8 ∧ 1 −

128 3+ √

n→∞

3

cf

cf 2

+

, ∀λ ∈ Λ2 = 1.

2 576

Recall from (127) that the validation error at λ = ∞ is bounded as (taking d = 2):

(174) (175)

lim P e(∞) ≤ 1 + 1 + 3 21, ∀λ ∈ Λ = 1.
n→∞
Combining (175) and (176) with choices of ( 1, 2) (dependent on , cf ) such that 83 ∧ 1 − 3 + √12c8f 1 + 1 + 3 21, we have

(176) 2 + c5f762 >

completing the proof.

lim P e(∞) > e(0), ∀λ ∈ Λ = 1,
n→∞

C.7 Proof of Proposition 11
To prove the claimed result, we construct partial orderings that satisfy each of the conditions (a), (b), and (c) separately, and show that the mean estimator fails under each construction. Intuitively, the mean estimator does not account for any bias, so we construct partial orderings where the mean of the bias diﬀers signiﬁcantly across courses, and show that the mean estimator fails on these construction. Without loss of generality we assume that the standard deviation parameter for the Gaussian distribution of the bias is σ = 1.

C.7.1 Proof of part (a)
We ﬁrst construct a partial ordering that satisﬁes the condition (a), and then bound the mean of each course to derive the claimed result. For clarity of notation, we denote the constant in the all constant-fraction assumption as cf .

Constructing the partial ordering: Recall from Deﬁnition 3 that the all cf -fraction assumption requires that each course i ∈ [d] has at least ik ≥ cf n students in each group k ∈ [r]. Let c0 = 1 − cf r. Due to the assumption that cf ∈ (0, 1r ), we have that c0 > 0 is a constant. We construct the following group ordering O, where the number of students in each course from each group is speciﬁed as
• Course 1: The course has (cf + c0)n students from group 1, and cf n students from each remaining group k ∈ {2, . . . , r}. That is,

1k =

(cf + c0)n cf n

if k = 1 if 2 ≤ k ≤ r.

(177a)

63

• Course 2: The course has (cf + c0)n students from group r, and cf n students from each remaining group k ∈ [r − 1]. That is,

2k = (cf + c0)n if 1 ≤ k ≤ r − 1

cf n.

if k = r.

(177b)

• Course i ≥ 3: The course has an equal number of students from each group k ∈ [r]. That is, for every 3 ≤ i ≤ d,

n ik = r

∀k ∈ [r].

It can be seen that this construction of the group ordering O is valid, satisfying the equality k∈[r] ik = n for each i ∈ [d]. Moreover, the group ordering O satisﬁes the all cf -fraction assumption. Intuitively, course 1 contains more students associated with negative bias (from group 1), and course 2 contains more students
associated with positive bias (from group k). The mean estimator underestimates the quality of course 1, and overestimates the quality of course 2. We construct some true qualities x∗ with x∗1 > x∗2, whose values are speciﬁed later in the proof.

Bounding the mean of each course: Denote the mean of the bias in any course i ∈ {1, 2} of group k ∈ [r] as bik := 1ik j∈Gik bij. Similar to the proof of Lemma 37 (see Appendix C.3.1 for its statement and Appendix C.11.4 for its proof), due to assumptions (A2) and (A3) we establish the following lemma.
Lemma 45. Consider any group ordering O that satisﬁes the all cf -fraction assumption. For any > 0, we have

lim P
n→∞

bik − bGk < , ∀i ∈ [d], k ∈ [r]
E1

= 1.

Denote this event in Lemma 45 as E1. Recall that k denotes the number of students in each group k ∈ [r]. From the construction of the group ordering O, we have 0 := 1 = r = (2cf + c0 + d−r 2 )n. Recall that b(k) denotes the kth order statistics of {bij}i∈[d],j∈[n]. By the assumption (A2) of the bias and the construction
of the partial ordering O, the group 1 contains the 1 lowest bias terms, {b(1), . . . , b( 0)}, and the group r contains the r highest bias terms, {b(dn− 0+1), . . . , b(dn)}. Hence, we have

b(

0 2

)

+

b(

0)

bG1 <

2

b(dn−

0)

+

b(dn−

0 2

)

bGr >

. 2

By the convergence of the order statistics from Lemma 24, it can be shown that there exists some constant c > 0 (dependent on d, r and cf ), such that

lim P bGr − bG1 > c
n→∞ E2

= 1.

Denote this event in (178) as E2. The mean estimator is computed as

[xmean]1 = x∗1 + n1
k∈[r]
[xmean]2 = x∗2 + n1
k∈[r]

1k b1k 2k b2k

(178)
(179a) (179b)

64

Taking the diﬀerence on (178), conditional on E1 and E2,

[xmean]2 − [xmean]1 = (x∗2 − x∗1) + n1 ( 2kb2k − 1kb1k)
k∈[r]

(>i) (x∗2 − x∗1) + n1

( 2kbGk − 1kbGk ) − 2

k∈[r]

(=i) (x∗2 − x∗1) + c0(br − bG1 ) − 2

(iii)
> (x∗2 − x∗1) + c0c − 2 .

(180)

where inequality (i) is true by the event E1, and equality (i) is true by plugging in the construction of the

group ordering from (177), and inequality (iii) is true by the deﬁnition (178) of E2. We set

=

c0 c 4

,

and

set

x∗1

=

c0 c 2

and

x∗2

=

0.

Then

by

(180)

we

have

P([xmean]2 − [xmean]1 > 0) = 1.

(181)

Combining (181) with the fact that x∗2 − x∗1 < 0, completing the proof of part (a).

C.7.2 Proof of part (b)
To construct the partial ordering, we set r = 2 and d = 2 in construction we used for part (a). This completes the proof of part (b).

C.7.3 Proof of part (c)
We construct a total ordering where the bias obeys the following order (same as the “non-interleaving” total ordering described in Section 5.1):

b11 ≤ . . . ≤ b1n ≤ b21 ≤ . . . ≤ b2n ≤ . . . ≤ bd1 ≤ . . . ≤ bdn.

In this construction, course 1 contains the n students with the lowest bias, and course d contains the n students with the highest bias. Recall that bi denotes the mean of the bias in course i ∈ [d]. We have

1

b

(

n 2

)

+

b(n)

b1 = n b1j < 2

j∈[n]

1

b

(dn−

n 2

)

+

b(dn)

br = n

b2j >

. 2

j∈[n]

Similar to part (a), by Lemma 24, there exists a positive constant c > 0 (dependent on d), such that

lim P br − b1 > c = 1.
n→∞
Let x∗1 = c and x∗2 = 0. We have lim P([xmean]r − [xmean]1 = x∗2 − x∗1 + b2 − b1 > 0) = 1.
n→∞
Combining (182) with the fact that x∗1 > x∗r completes the proof of part (c).

(182)

65

C.8 Proof of Proposition 13

By Corollary 19, we assume x∗ = 0 without loss of generality. Denote the bias of course 1 as {Uj}j∈[rn] in group 1, and {Vj}j∈[(1−r)n] in group 2. Denote the bias of course 2 as {Uj}j∈[(1−r)n] in group 1 and {Vj }j∈[rn] in group 2. We have Uj, Uj ∼ Unif[−1, 0] and Vj, Vj ∼ Unif[0, 1]. Denote the mean of {Uj}, {Vj}, {Uj} and
{Vj } as U , V , U and V respectively. We prove the claimed result respectively for the reweighted mean estimator (Appendix C.8.1) and for our estimator at λ = 0 (Appendix C.8.2). Both parts use the following standard result regarding the uniform distribution.

Lemma 46. Let X1, . . . , Xn be i.i.d. Unif[0, 1], we have

E ni=1 Xi 2 = 1 + 1 .

n

4 12n

C.8.1 The reweighted mean estimator

We follow the deﬁnition of the reweighted mean estimator deﬁned in Appendix A.2. In the reweighting step, by (4) we have

1 U +V xrw = 2 U + V .

(183)

In the recentering step, by (6) we have





1

1

xrw ← xrw + − 2

[xrw]i + 2n

yij  1

i∈{1,2}

i∈{1,2},j∈[n]

= xrw + − [xrw]1 + [xrw]2 + rnU + (1 − r)nV + (1 − r)nU + rnV 1

2

2n

= [xrw]1 − [xrw]2 1 + rU + (1 − r)V + (1 − r)U + rV 1

2

−1

2

(i) U + V − U − V 1

rU + (1 − r)V + (1 − r)U + rV

=

+

1,

4

−1

2

(184)

where equality (i) is true by plugging in (183) from the reweighting step. By symmetry, we have E[xrw]21 = E[xrw]22, so we only consider course 1. By (184), we have

2

2

E[xrw]21 (=i) E U + V −4U − V

rU + (1 − r)V + (1 − r)U + rV +E 2

1

2

2

2

2

11

= 16 E U + V + U + V − 4 · 2 2

+ 1 E (1 − r)2U 2 + r2V 2 + r2U 2 + (1 − r)2V 2 − 2 r2 + (1 − r)2

4

4

4

= 1 E U 2 + V 2 − 1 + 1 E r2U 2 + (1 − r)2V 2 − r2 + (1 − r)2

8

22

4

(ii) 1 1

1

1

1

1 1 r2 r2 (1 − r)2 (1 − r)2 r2 + (1 − r)2

= 8 4 + 12rn + 4 + 12(1 − r)n − 2 + 2 E 4 + 12rn + 4 + 12(1 − r)n − 4

11 1

1

=

+

+

96n r 1 − r 24n

1

1

=+

.

24n 96r(1 − r)n

66

where (i) is true because it can be veriﬁed by algebra that E

U +V −U −V 4

rU +(1−r)V +(1−r)U +rV 2

= 0,

and (ii) is true by Lemma 46. Finally, we have

21 E xrw ]22 = 12 E[xrw]21] + E[xrw]22 = E[xrw]21 = 241n + 96r(11− r)n ≥ 241n + 241n = 121n ,

where

the

inequality

holds

because

r(1 − r)

≤

1 4

for

every

r

∈

(0, 1).

C.8.2 Our estimator at λ = 0
Recall from Proposition 22 that for d = 2 courses and r = 2 groups, our estimator at λ = 0 has the closed-form expression x(0) = y + −1 · γ , where
12

 y22,min − y11,max 
γ = y21,max − y12,min
y2 − y1

if y22,min − y11,max < y2 − y1 if y21,max − y12,min > y2 − y1 o.w.

(185)

By (185), we have

12 E x(0) 22 = 12 E

γ y−

2
+

γ y+

2

= E[y2] + 1 E[γ2].

2

2

4

(186)

We analyze the two terms in (186) separately.

Term of E[y2] : For ease of notation, we denote the random variables

{Uj }j∈[n] := {Uj }j∈[rn] ∪ {Uj }j∈[(1−r)n] {Vj }j∈[n] := {Vj }j∈[(1−r)n] ∪ {Vj }j∈[rn]

Then {Uj}j∈[n] is i.i.d. Unif[−1, 0] and {Vj}j∈[n] is i.i.d. Unif[0, 1]. We have

E[y2] = E

i∈[n] Ui + 2n

2
i∈[n] Vi





1 = 4n2 E 

Ui2 +

Vi2 + 2

UiVj +

UiUj +

ViVj 

i∈[n]

i∈[n]

i∈[n],j∈[n]

i∈[n] j=i

i∈[n] j=i

1 =

n + n + 2n2

1 −

1

1

+ n(n − 1) + n(n − 1)

4n2 3 3

4

4

4

1 =.
24n

Term of E[γ2]: To analyze the term E[γ2], we use the following standard result from statistics.
Lemma 47. Let X1, . . . , Xn ∼ Unif[0, 1]. Let Xmin = mini∈[n] Xi. We have
1 E[Xmin] = n + 1 E[Xm2 in] = (n + 1)2(n + 2) .

(187)

67

We deﬁne

Umax := max Uj
j∈[rn]
Vmin := min Vj ,
j∈[(1−r)n]

and deﬁne Umax and Vmin likewise. By (185) it can be veriﬁed that we have the deterministic relation

|γ| ≤ (y22,min − y11,max) ∨ (y12,min − y21,max)
(i)
= (Vmin − Umax) ∨ (Vmin − Umax) ≤ Vmin − Umax + Vmin − Umax, where equality (i) is true by the assumption that there is no noise and the assumption of x∗ = 0. Therefore,

E[γ2] ≤ E [(Vmin − Umax) + (Vmin − Umax)]2 = E(Vmin − Umax)2 + E(Vmin − Umax)2 +2 E(Vmin − Umax)(Vmin − Umax) .

T1

T2

T3

We consider the three terms T1, T2 and T3 separately. For the term T1, by Lemma 47 we have

T1 = E[Vmin]2 + E[Um2 ax] − 2E[VminUmax]

2

1

6

= 2 · (rn + 1)(rn + 2) + 2 · (rn + 1)2 ≤ r2n2 .

(188)

Likewise, for the term T2 we have

6 T2 ≤ (1 − r)2n2 .

For the term T3, by Lemma 47 we have

2

2

4

T3 = rn + 1 · (1 − r)n + 1 ≤ r(1 − r)n2 .

Plugging the three terms back to (188), we have

E[γ2] ≤ 6 +

6

+

8

c =,

r2n2 (1 − r)2n2 r(1 − r)n2 n2

(189)

for some constant c > 0. Finally, plugging (187) and (189) back to (186), we have

completing the proof.

1 E

x(0)

2≤

1

+

c ,

2

24n 4n2

C.9 Proof of preliminaries
In this section, we present the proofs of the preliminary results presented in Appendix C.2.

C.9.1 Proof of Proposition 14
To avoid clutter of notation, we ﬁrst prove the case for Ω = [d] × [n], and then comment on the general case of Ω ⊆ [d] × [n].
Now consider Ω = [d] × [n], where our estimator (15) reduces to (2). We separately consider the cases of λ = 0 and λ ∈ (0, ∞).

68

Case of λ = 0 The objective (2) becomes

min min Y − x1T − B 2 =

min

Y −W 2.

x∈Rd B∈Rd×n

F

W ∈Rd×n

F

B satisﬁes O

W ∈{x1T +B|x∈Rd, B∈Rd×n, B satisﬁes O}

(190)

It can be veriﬁed that the set {x1T + B | x ∈ Rd, B ∈ Rd×n, B satisﬁes O} is a closed convex set. By the

Projection Theorem [4, Proposition 1.1.9], a unique minimizer W0 to the RHS of (190) exists. Therefore,

the set of minimizers to the LHS of (190) can be written as {(x, W0 − x1T ) | x ∈ Rd}. The tie-breaking rule

minimizes the Frobenius norm

B

2 F

.

That is, we solve

min W0 − x1T 2 .

x∈Rd

F

(191)

It can be veriﬁed that a unique solution to (191) exists, because the objective is quadratic in x. Hence, the tie-breaking rule deﬁnes a unique solution (x, B).

Case of λ ∈ (0, ∞) It can be veriﬁed that the objective (2) is strictly convex in (x, B). Therefore, there
exists at most one minimizer [4, Proposition 3.1.1].
It remains to prove that there exists a minimizer. It is straightforward to see that the objective is continuous in (x, B). We now prove that the objective is coercive on {(x, B) : x ∈ Rd, B ∈ Rd×n, B satisﬁes O}. That is,
for any constant M > 0, there exists a constant RM > 0, such that the objective at (x, B) is greater than M for all (x, B) in the domain {(x, B) : x ∈ Rd, B ∈ Rd×n, B satisﬁes O} with

x

2 2

+

B

2 F

> RM

(192)

Given coercivity, invoking Weierstrass’ Theorem [4, Proposition 3.2.1] completes the proof. We set

RM = d

1 1+ √
λ

√

21

M + max Y + M.

i∈[d],j∈[n]

λ

(193)

We discuss the following two cases depending on the value of

B

2 F

.

Case of B 2F ≥ Mλ

The second term of the objective (15) is lower-bounded as λ

B

2 F

≥ M.

Hence, the

objective (2) is at least M .

Case of B 2F < Mλ : Combining (192) and (193), we have

2

2

1√

2

x 2 > RM − B F > d (1 + √ ) M + max yij .

λ

i∈[d],j∈[n]

Hence, there exists some i∗ ∈ [d] such that

1√

|xi∗ | > (1 + √ ) M + max yij.

λ

i∈[d],j∈[n]

Consider the (i∗, j) entry in the matrix (Y − x1T − B) for any j ∈ [n]. We have

(Y − x1T − B)i∗j

≥ |xi∗ | − |yi∗j | − |bi∗j |

≥ |xi∗ | − max yij − B F
i∈[d],j∈[n]

(i)

1√

> 1+ √ M −

λ

M√ = M,
λ

(194)

69

where (i) is true by (194) and the assumption of the case that

B

2 F

<

λ1 M .

Hence, the second term in the

objective (2) is lower-bounded by

Y − x1T − B 2F ≥ (Y − x1T − B)i∗j 2 > M,

and therefore the objective (2) is greater than M .

Combining the two cases depending on

B

2 F

completes the proof of the coercivity of the objective (2) in

terms of (x, B). Invoking the Weierstrass’ Theorem [4, Proposition 3.2.1] completes the proof of Ω = [d] × [n].

Extending the proof to general Ω ⊆ [d] × [n]: For general Ω ⊆ [d] × [n], by a similar argument the solution (x, {bij}(i,j)∈Ω) exists and is unique. Note that the objective (15) is independent from {bij}(i,j)∈Ω,so we have bij = 0 for each (i, j) ∈ Ω. Hence, a unique solution (x, B) to (15) exists for general Ω.

C.9.2 Proof of Lemma 16
It is suﬃcient to prove the general version (19). First consider λ = ∞. It can be veriﬁed that the closed-form expression (3) for the solution at λ = ∞ satisﬁes the claimed relation (19).
It remains to consider the case of λ ∈ [0, ∞). Given the value of the solution B(λ), we solve for x(λ) by minimizing the ﬁrst term of the objective (2) as

min

Y − x1T − B(λ)

2 F

.

x∈Rd

(195)

Writing out all the terms in (195) and completing the square yields the claimed relation (19).

C.9.3 Proof of Lemma 17
It is suﬃcient to prove the general version (21). First consider the case of λ = ∞. It can be veriﬁed that the closed-form expression expressions (3) for the solution at λ = ∞ satisﬁes the claimed relations (22).
It remains to consider the case of λ ∈ [0, ∞). First we prove (21a). Assume for contradiction that (i,j)∈Ω bij = 0. Consider the set of alternative solutions (xγ, Bγ) parameterized by some γ ∈ R as

xγ = x + γ1d Bγ = B − γ1d1Tn .

(196a) (196b)

Note that the original solution (x, B) corresponds to γ = 0.

Since Bγ in (196) is obtained by subtracting all entries in the matrix by a constant γ, the bias term bγ

satisﬁes the partial ordering O for any γ ∈ R. Moreover, since by construction (196) the value of (xγ1d + bγ) is the same for all γ ∈ R, the ﬁrst term in the objective (2) is equal for all γ ∈ Rd. Now consider the second

term

Bγ 2Ω. Writing out the terms in

Bγ

2 Ω

and completing the square, we have

bγ

2 Ω

is minimized at

γ = |Ω1| (i,j)∈Ω bij = 0. Contradiction to the assumption that the solution at γ = 0 minimizes the objective,

completing the proof of (21a).

Now we prove (21b). By (19) from Lemma 16 and summing over i ∈ [d], we have

nixi =

(i)

(yij − bij ) =

(yij − bij ) =

yij ,

i∈[d]

i∈[d] jΩi

(i,j)∈Ω

(i,j)∈Ω

where equality (i) is true by (21a), completing the proof of (21b).

70

C.9.4 Proof of Proposition 18
First consider the case of λ = ∞, the claimed result can be veriﬁed using the closed-form expressions (3) at λ = ∞. It remains to consider the case of any λ ∈ [0, ∞). Assume for contradiction that the solution at Y + ∆x1T is not (x + ∆x, B), but instead (x + ∆x + u, B ) for some non-zero u ∈ Rd. By the optimality of (x + ∆x + u, B ), we have

(Y + ∆x1T ) − (x + ∆x + u)1T − B

2 Ω

+

λ

B

2 Ω

≤

(Y

+ ∆x1T ) − (x + ∆x)1T

−B

2 Ω

+

λ

B

2 Ω

(197)

Y − (x + u)1T − B

2 Ω

+

λ

B

2 Ω

≤

Y

− x1T

−B

2 Ω

+

λ

B

2Ω.

(198)

If strict inequality in (198) holds, then (x + u, B ) attains a strictly smaller objective on observations Y
given (O, λ, Ω) than (x, B). Contradiction to the assumption that (x, B) is optimal on the observations Y . Otherwise, equality holds in (198) and hence in (197). By the tie-breaking rule of the equality (197) on the observations (Y + ∆x1T ), we have

B

2 Ω

<

B 2Ω,

(199)

Combining (199) with the equality of (198) yields a contradiction to the assumption that (x, B) is optimal on the observations Y , and hence is chosen by the tie-breaking rule over the alternative solution (x + u, B ).

C.9.5 Proof of Lemma 20
The proof relies on (21b) from Lemma 17. Assume without loss of generality that x∗ = 0. We ﬁrst show that on the RHS of (21b), we have that (i,j)∈Ωt yij converges to 0 for random Ωt obtained by Algorithm 1.
Fix some constant 1 > 0 whose value is determined later. Part (b): For any ﬁxed Ωt, by Hoeﬀding’s inequality, we have





1

lim P 
n→∞

|Ωt|

yij < 1 = 1.

(i,j)∈Ωt

(200a)

Part (a): Given the assumption that x∗ = 0 and the assumption that there is no noise, we have Y = B. By (28b) from Lemma 28, we have





1

lim P 
n→∞

|Ωt|

yij < 1 = 1.

(i,j)∈Ωt

(200b)

The rest of the proof is the same for both parts. Denote the event in (200) as E. We now condition on E
and consider the LHS of (21b). By (7), the number of students in each course i ∈ [d] is nt = 12 n. Consider any λ ∈ [0, ∞] ∈ Λ . By the deﬁnition of Λ we have x(λ) 2 ≥ . There exists some i∗ such that |xi∗ | ≥ √d . Assume without loss of generality that xi∗ > √d . We now show that there exists some i such that xi ≤ 0. Assume for contradiction that xi > 0 for all i ∈ [d]. Then by (21b), we have

yij = nt xi ≥ ntxi∗ > n √ .

(i,j)∈Ωt

i∈[d]

2d

Therefore,

1

2n

2

|Ωt|

yij = dn 3 √d = 3d 23 .

(i,j)∈Ω

71

Setting

1 to be suﬃciently small such that

1<

2
3

yields a contradiction with E. Hence, conditional on E,

3d 2

there exists some i∗2 such that xi∗2 ≤ 0. Therefore, maxi,i ∈[d](xi − xi ) ≥ xi∗ − xi∗2 > √d . A similar argument

applies to the case of xi∗ < − √d . Hence, we have

max (xi − xi ) > √ , ∀λ ∈ Λ E.

i,i ∈[d]

d

Combining (201) with (200), we have

(201)

completing the proof.

lim max (xi − xi ), ∀λ ∈ Λ ≥ P(E) = 1,
n→∞ i,i ∈[d]

C.9.6 Proof of Lemma 21
We follow the proof of Lemma 20, we assume x∗ = 0 without loss of generality. Then ﬁx some constant 1 > 0, and estalish concentration inequalities on the RHS of (21b).

Part (b):

Same as (200b) from Lemma 20, we have





1

lim P 
n→∞

|Ωt|

yij < 1 = 1.

(i,j)∈Ωt

(202a)

Part (a): By Hoeﬀding’s inequality, we have





1

lim P 

yij < 1 = 1.

n→∞ dn

i∈[d],j∈[n]

(202b)

The rest of the proof is the same for both parts. Combining (202) with (21b), we have





1

lim P 

xi < 1 = 1.

n→∞

d

i∈[d]

(203)

Fix any value > 0. Denote E as the event that the events in both (23) and (203) hold. By a union bound of (23) and (203), we have

lim (E) = 1.
n→∞

(204)

Condition on E and consider the value of x(1λ). First consider the case of x1 > , then by (23) we have xi > 0 for each i ∈ [d]. Then

1

1

d xi = d xi > d

i∈[d]

i∈[d]

x1 > , E

A similar argument applies to the case of e x1 < − , and we have

1 d xi > d
i∈[d]

|x1| > , E

72

The same argument applies to each i ∈ [d]. We have

1 d xi > d
i∈[d]

x ∞ > ,E

Taking a suﬃciently small 1 such that 1 < d in (203) yields a contradiction. Hence, we have

lim P( x ∞ > , E) = 0.
n→∞

Hence,

lim P
n→∞

√ x 2> d

(i)
≤ lim P ( x ∞ > ) = lim

n→∞

n→∞

(ii)
x ∞ > , E ≤ lim P(E) = 0,
n→∞

where inequality (i) is true by (205) and (ii) is true by (204), completing the proof.

(205)

C.9.7 Proof of Proposition 22
Without loss of generality we assume x∗ = 0. By (22b) from Lemma 17 with the assumption that d = 2, we have 21 (x1 + x2) = y, and hence without loss of generality we parameterize x with some γ ∈ R as

−1 γ

xγ = y +

1

· 2

(206)

It remains to determine the value of γ. Given x∗ = 0 and the assumption that there is no noise, we have Y = B. By the assumption (A2) on
the bias, we have B obeys the ordering constraints O. Hence, setting (x, B) = (0, B) gives an objective of 0 in (2). Hence, at the optimal solution (xγ, Bγ), the objective (2) equals 0. At the optimal solution, we have

Bγ = Y − xγ 1T .

(207)

The rest of the proof consists of two steps in determining the value of γ. First, we ﬁnd the set of γ such

that Bγ satisﬁes the ordering constraint O. Then we ﬁnd the optimal γ from this set that is chosen by

tie-breaking, minimizing

Bγ

2 F

.

Step 1: Finding the set of γ that satisﬁes the ordering constraint Given Y = B, for any γR we have that Bγ satisﬁes all ordering constraints in O that are within the same course, that is, the ordering constraints in the form of ((i, j), (i, j )) ∈ O with i ∈ {1, 2}. Hence, we only need to consider ordering constraints involving both courses, that is, the ordering constraints in the form of ((i, j), (i , j )) with {i, i } = {1, 2}. It can be veriﬁed that these constraints involving both courses are satisﬁed if and only if

y11,max − x1 ≤ y22,min − x2 y21,max − x2 ≤ y12,min − x1.

(208)

Plugging the parameterization (206) of xγ into (208), we have

y21,max − y12,min ≤ γ ≤ y22,min − y11,max.

(209)

Note that the range in (209) is always non-empty, because given Y = B, we have y11,max ≤ y12,min and y21,max ≤ y22,min and hence y21,max − y12,min ≤ y22,min − y11,max.

73

Step 2: Finding the optimal γ from the range (209) minimizing

Bγ

2 F

tions (206) and (207), we write

Bγ

2 F

as

Bγ

2 F

=

Y − xγ 1T

2 F

(i)

γ2

γ2

= y1j − y + 2 + y2j − y − 2 .

j∈[n]

j∈[n]

Using the parameteriza(210)

Writing out the terms in (210) and completing the square, we have that minimizing to minimizing the term:

n

2

2 (γ − (y2 − y1))

Combining (209) and (211) gives the yields expression (24) for the optimal γ.

bγ

2 F

is equivalent

(211)

C.9.8 Proof of Lemma 23

The lemma is a direct consequence of the following result (given that almost-sure convergence implying convergence in probability).

Lemma 48 (Theorem 2 in [13]). Let X1, . . . , Xn be i.i.d. N (0, 1). We have

√ 2 log n
linm→s∞up log log n Mn = 1

almost surely,

where log is the logarithm of base 2.

C.9.9 Proof of Lemma 24
Let g be the p.d.f. of N (0, 1). Let Gn be the empirical c.d.f. and the empirical inverse c.d.f. of n i.i.d. samples from N (0, 1) and let G−n 1 be the inverse of Gn.
The claim is a straightforward combination of the following two lemmas. The ﬁrst lemma states that the empirical inverse c.d.f. converges to the true inverse c.d.f. The second lemma states that order statistics converges to the empirical inverse c.d.f.
Lemma 49 (Example 3.9.21 of [46], or Corollary 21.5 of [45]). Consider any ﬁxed p ∈ (0, 1). Assume that G is diﬀerentiable at G−1(p) and g(G−1(p)) > 0. Then we have
√n G−n 1(p) − G−1(p) −→d N 0, g2p((G1−−1(pp))) .
Lemma 50 (Lemma 21.7 in [45]). Fix constant p ∈ (0, 1). Let {kn}∞ n=1 be a sequence of integers such that knn = p + √cn + o √1n for some constant c. Then
√n X(kn:n) − G−n 1(p) −P→ g(G−c1(p))

C.9.10 Proof of Lemma 26

We consider any ﬁxed i ∈ [d], k ∈ [r], and any ﬁxed total ordering π0 generated by Line 2 of Algorithm 1. Note that the ik elements in Gik are consecutive with respect to the sub-ordering of π0 restricted to course i in Line 4 of Algorithm 1. Then it can be veriﬁed from Line 5-7 of Algorithm 1 that

2ik − 1 ≤ vik ≤ 2ik + 1,

(212)

It can be veriﬁed that (212) along with the assumption that ik ≥ 4 yields (26a). Summing (26a) over i ∈ [d] yields (27a). Finally, replacing the validation set Ωv by the training set Ωt in the proof of (26a) and (27a)
yields (26b) and (27b), respectively.

74

C.9.11 Proof of Lemma 27
We prove part (a) and part (b) together. Note that if the element of rank k1 and the element of rank k2 are adjacent within Ωt, or adjacent between Ωt and Ωv, the (k2 − k1 − 1) elements of ranks from k1 + 1 through k2 − 1 are within the same set (i.e., Ωt or Ωv). Assume for contradiction that k2 − k1 ≥ 2d + 2. Then the number of elements from rank k1 + 1 through k2 − 1 is at least k2 − k1 − 1 ≥ 2d + 1. Consider these elements. There exists a course i∗ such that the number of such elements within this course is at least 3. Given that these elements have consecutive ranks, they are consecutive within course i∗. Hence, two of these elements in course i∗ appear as the same pair of elements in Line 7 of Algorithm 1. According to Line 7 of Algorithm 1, one element in this pair is assigned to Ωt and the other element is assigned to Ωv. Contradiction to the assumption that all of these elements are from the same set.

C.9.12 Proof of Lemma 28

Proof of (28a): We consider any course i ∈ [d]. We ﬁrst ﬁx any value of B = B∗. Fix any π0 of the

dn elements (in Line 2 of Algorithm 1). Recall from Line 4 of Algorithm 1 that the sub-ordering of the n

elements in course i according to π0 is denoted as (i, j(1)), . . . , (i, j(n)).

Consider each pair (i, j(2t−1)) and (i, j(2t)) for t ∈

n 2

.

Algorithm

1

randomly

assigns

one

of

the

two

elements to the training set Ωt uniformly at random. Denote Ut as the the value from this pair that is

assigned to training set. Then we have

Ut =

b∗
i,j (2t−1)
b∗
i,j(2t)

with probability 0.5 with probability 0.5.

Denote ∆B := maxj∈[n] bij − minj∈[n] bij and denote ∆B∗ = maxj∈[n] b∗ij − minj∈[n] b∗ij . Recall from (7) that nt = n2 . Fix any δ > 0. By Hoeﬀding’s inequality, there exists n1 such that for all n ≥ n1,





1

1

P 

nt

Ut − nt E[Ut] < ∆B∗

t

∈

[

n 2

]

log n B = B∗ ≥ 1 − δ .

n



2

Equivalently, for all n ≥ n1,



1

1

lim P 
n→∞

nt

bij − n

bij

j∈Ωti

j∈[n]

< ∆B∗

log n n

 B = B∗ ≥ 1 − δ .
2

(213)

Now we analyze the term ∆B. By Lemma 25, we have that there exists n2 such that for all n ≥ n2,

δ P ∆B ≤ 4 log n ≥ 1 − 2 .

(214)

Fix any > 0. Take n0 to be suﬃciently large such that n0 ≥ max{n1, n2} and 4 √logn0n0 < . We have that for all n ≥ n0,









1

1

1

1

P  nt j∈Ωt bij − n j∈[n] bij <  = B∗∈Rd×n P  nt j∈Ωt bij − n j∈[n] bij <

i

i

B∗ · P(B∗) dB∗





1

1

≥ B∗∈R√d×n: P  nt

bij − n

bij <

∆B∗ ≤4 log n

j:(i,j)∈Ωt

j∈[n]

B · P(B∗) dB∗

(i)

δ

≥ 1 − 2 · P ∆B ≤ 4 log n

(ii)

δ2

≥ 1 − ≥ 1 − δ,

2

75

where inequality (i) is true by (213) and inequality (ii) is true by (214), completing the proof.

Proof of (28b): By Hoeﬀding’s inequality, we have that for any > 0,





1

lim P 

bij <  = 1.

n→∞ dn

i∈[d],j∈[n]

(215)

Recall from assumption (A3) that d is assumed to be a constant. Taking a union bound of (28a) over i ∈ [d] and (215), folloed by using the triangle inequality yields the claimed result.

C.10 Proof of auxiliary results for Theorem 5
In this section, we present the proofs of the auxiliary results for Theorem 5.

C.10.1 Proof of Lemma 29
Fix any c > 0 and ﬁx any (i, i ) ∈ Sc. Suppose k ∈ [r] satisﬁes the deﬁnition (30) corresponding to (i, i ). We prove that for any > 0 and δ > 0, there exists some n0 such that for all n ≥ n0,
P x(i0) − x(i0) < ≥ 1 − δ.
The proof consists of two steps. In the ﬁrst step, we consider the rank of the maximum bias in course i of group k (that is, max(i,j)∈Gik tij), and the rank of the minimum bias in course i of group (k + 1) (that is, min(i,j)∈Gi k+1 tij). We bound the diﬀerence between these two ranks, and then bound the diﬀerence between the values of these two terms. In the second step, we show that the ordering constraint imposed by this pair of bias terms leads to the claimed bound (31) on x(i0) − x(i0).

Step 1: Bounding the diﬀerence of a pair of bias terms Recall from (13) that bk,max denotes the

largest bias of group k, and bk+1,min denotes the smallest bias of group k + 1. We denote the rank of bk,max

as t. By the deﬁnition of group ordering, the value of t is deterministic and we have t =

k k =1

k . Then the

rank of bk+1,min is (t + 1).

Recall that bik,max denotes the largest bias in course i of group k, and bik,min denotes the smallest bias

in course i of group k. Let Tk be a random variable denoting the diﬀerence between the ranks of bk,max

and bik,max, and let Tk+1 be a random variable denoting the diﬀerence between the ranks of bk+1,min and

bi,k+1,min. Equivalently, the ranks of bik,max and bi+1,k+1,min are (t − Tk) and (t + 1 + Tk+1), respectively,

and we have Tk, Tk+1 ≥ 0.

Recall that the biases within a group are ordered uniformly at random among all courses. For any constant

integer t0 > 0, if we have Tk ≥ t0, then the bias terms corresponding to ranks of (t − t0 + 1), . . . , t are not

assigned to course i. Recall that −i,k = k − ik denotes the number of observations in group k that are not

in course i. We bound the random variable Tk as

t0 −1
P(Tk ≥ t0) =
m=0

−i,k − m < k −m

t0 (i)
−i,k ≤ (1 − c)t0 ,
k

(216)

where step (i) is true by the deﬁnition (30) of Sc. Similarly we have

P(Tk+1 ≥ t0) ≤ (1 − c)t0 .

(217)

Taking t0 = lolgog(1(−δ4 )c) and taking a union bound of (216) and (217), we have

P Tk + Tk+1 < 2t0

≥ P Tk < t0, Tk+1 < t0

≥

1

−

2(1

−

c)t0

=

1

−

δ .

2

(218)

76

By Lemma 23, there exists n0 such that for all n ≥ n0, we have

δ P M < 2t0 + 1 > 1 − 2 ,

(219)

where M is the maximum diﬀerence between a pair of bias terms of adjacent ranks, deﬁned as M := maxi∈[dn−1] b(i+1) − b(i). Taking a union bound of (219) with (218), we have that for all n ≥ n0

bi ,k+1,min − bik,max < [(t + 1 + Tk+1) − (t − Tk) + 1] · M ≤ (2t0 + 1)M < , with probability at least 1 − δ.

(220)

Due to the assumption of no noise and the assumption of x∗ = 0, the observation model (1) reduces to Y = B. In particular, we have yik,max = bik,max and yi ,k+1,min = bi ,k+1,min. Moreover, the solution (x, B) = (0, B) gives an objective (2) of 0 at λ = 0 due to Y = B. Therefore the solution (x(0), B(0)) by our estimator gives an objective of 0, satisfying the deterministic relation yij = x(i0) + b(ij0). By deﬁnition of the group ordering, the group ordering includes the constraint requiring b(ik0),max ≤ b(i0,)k+1,min. Therefore, this ordering constraint requires the solution (x(0), B(0)) to satisfy

b(i0,)k+1,min − b(ik0),max = (yi ,k+1,min − x(i0)) − (yik,max − x(i0)) = (bi ,k+1,min − x(i0)) − (bik,max − x(i0)) ≥ 0
Rearranging (221)and combining it with (220), we have that for all n ≥ n0,

(221)

P x(i0) − x(i0) ≤ bi ,k+1,min − bik,max< ≥ 1 − δ,

completing the proof.

C.10.2 Proof of Lemma 31
First of all, we assume that L ≤ d without loss of generality. This is because if L > d, then there exists a course i that appears twice in this cycle. We write the cycle as (i1, . . . , i, . . . , i , . . . , i, . . . , iL), where i ∈ [d] denotes some course appearing in between the two occurrences of i. We obtain a shortened cycle by replacing the segment (i, . . . , i , . . . i) with a single i. By shortening the cycle the set of courses that appear in this cycle remain the same. We keep shortening the cycle until L ≤ d.
Fix any > 0 and δ > 0. Recall from assumption (A3) that d is assumed to be a constant. By applying Lemma 29 on the L pairs in (32) of Sc, and taking a union bound over these L pairs, we have that there exists n0 such that for all n ≥ n0, with probability at least 1 − δ we simultaneously have

xm2

− xm1

<

, d

xm3

− xm2

<

, d

...

xmL

− xmL−1

<

, d

xm1

− xmL

<

. d

Consider any m < m with m, m ∈ [L]. Conditional on (222) we have

xim − xim = (xim − xim ) −1 + . . . + (xim+1 − xim ) < .

(222) (223)

77

On the other hand, conditional on (222) we also have xim − xim = (xim − xim−1 ) + . . . + (xi2 − xi1 ) + (xi1 − xiL ) + . . . + (xim +1 − xim ) <
Combining (223) and (224), we have that for all n ≥ n0,

Equivalently,

P xim − xim < , ∀m, m ∈ [L] ≥ 1 − δ.

(224)

completing the proof.

lim P max |xi − xi| <
n→∞ m,m ∈[L]

= 1,

C.10.3 Proof of Lemma 32
The proof consists of two steps. We ﬁrst show that if there exists a cycle including the nodes i, i ∈ V , then this cycle can be modiﬁed to construct a cycle of length at most 2(d − 1) including i and i . In the second step, we prove the existence of a cycle.

Constructing a cycle of length at most 2(d−1) given a cycle of arbitrary length Fix any hypernode V and any i, i ∈ V . We assume that there exists a cycle including the nodes i and i . By the deﬁnition of a cycle, this cycle includes a directed path i → i and a directed path i → i. If the directed path i → i has length greater than (d − 1), then there exists some course i ∈ [d] (which may or may not equal to i or i ) that appears at least twice in this cycle. Then we decompose the path into three sub-paths of i → i , i → i , and i → i . We remove the sub-path i → i , and concatenate the subpaths i → i and i → i , giving a new path i → i of strictly smaller length than the original path. We continue shortening the path until each course appears at most once in the path, and hence the path is of length at most (d − 1). Likewise we shorten the path i → i to have length at most (d − 1). Finally, combining these two paths i → i and i → i gives a cycle of length at most 2(d − 1), including nodes i and i .

Existence of a cycle of arbitrary length We prove the existence of a cycle including i and i by induction on the procedure that constructs the partition. At initialization, each hypernode contains a single course. The claim is trivially satisﬁed because for any hypernode V there do not exist i, i ∈ V with i = i . Now consider any merge step that merges hypernodes V1, . . . , VL for some L ≥ 2 during the construction of the partition. By deﬁnition, the merge occurs because there is a cycle that includes at least one course from each of the hypernodes V1, . . . , VL. We denote the course from Vm that is included the cycle as im ∈ Vm for each m ∈ [L]. If there exist multiple courses from Vm included in the cycle, we arbitrarily choose one as im). Denote the merged hypernode as V = V1 ∪ . . . ∪ VL. Now consider any two courses i and i from the same hypernode.
First consider the case of i and i are from a hypernode that is not V , then by the induction hypothesis there is a cycle including both i and i .
Now consider the case of i, i ∈ V . We have that i ∈ Vm and i ∈ Vm for some m, m ∈ [L]. If m = m , then by the induction hypothesis there is a cycle that includes both m and m . If m = m , then by the induction hypothesis, there is a directed path i → im within Vm (trivially if i = im), and a directed path im → i within Vm (trivially if i = im ). Moreover, by the deﬁnition of im and im , we have that im and im are included in a cycle. Hence, there exists a directed path im → im . Concatenating the paths i → im, im → im and im → i gives a path i → i . Likewise there exists a path i → i. Hence, for any i, i ∈ V , there exists a cycle that includes both i and i .

78

C.10.4 Proof of Lemma 33
The proof consists of four steps. The ﬁrst step gives a preliminary property on the graph, to be used in the later steps. The second step shows that each hypernode contains courses that are consecutive. The third step shows that the ranks of elements in each hypernode are consecutive. The fourth step shows that the edges only exist between hypernodes that are adjacent in their indexing.
Step 1: There exists a path from any course i to any course i with i < i Denote the minimal rank in course i and in course i as t and t , respectivly. By the assumption (44), we have t < t . We consider the courses corresponding to the elements of ranks t through t , denoted as (it, . . . , it ). For any integer k ∈ {t, . . . , t − 1} if ik = ik+1, then by the deﬁnition of Sc from (30) we have (ik, ik+1) ∈ S1 because these two elements have consecutive ranks. Hence, there is an edge ik → ik+1 by the construction of the graph. Concatenating all such edges {ik → ik+1}k∈{t,...,t −1}:ik=ik+1} gives a path i → i .
Step 2: Each hypernode contains consecutive nodes We prove that the nodes within each hypernode are consecutive. That is, for each hypernode V , there exist courses i, i ∈ [d] with i < i such that V = {i, i + 1, . . . , i }. It suﬃces to consider any course i such that i < i < i and show that i ∈ V . Assume for contradiction that i ∈ V . By Step 1, there exists a path i → i and also a path i → i . Since i, i ∈ V , by Lemma 32 there exists a path i → i. Hence, by concatenating these three paths i → i , i → i and i → i, we have a cycle that includes courses i, i and i that are involved in two diﬀerent hypernodes. Contradiction to the deﬁnition of the partition that there are no cycles including nodes from more than one hypernode in the ﬁnal partition, completing the proof that each hypernode contains consecutive nodes. Hence, we order the hypernodes as V1, . . . Vs, such that the indexing of the nodes increases with respect to the indexing of the hypernodes.
Step 3: The ranks in each hypernode are consecutive We show that the ranks of the elements within each hypernode are consecutive, and also in the increasing order of the indexing of the hypernodes. Assume for contradiction that there exists some element of rank t in Vm , and some element of rank t in Vm with m < m and t > t . Denote the corresponding courses as i ∈ Vm and i ∈ Vm . On the one hand, by Step 2 we have i < i due to m < m . Then by Step 1, we have a path i → i . On the other hand, we consider the elements of ranks {t , . . . , t} and construct a path i → i similar to the construction of the path in Step 1. Concatenating the paths i → i and i → i gives a cycle that include courses i ∈ Vm and i ∈ Vm that from two diﬀerent hypernodes. Contradiction to the deﬁnition of the partition that there does not exist cycles including more than one hypernode.
Step 4: The only edges on the hypernodes are (Vm, Vm+1) for all m ∈ [s − 1] For total orderings, the edges exist between elements of adjacent ranks. That is, consider the elements of ranks t and t + 1 for any t ∈ [dn − 1]. If their corresponding courses it and it+1 are diﬀerent, then there exists an edge it → it+1. Then Step 4 is a direct consequence of Step 3.
C.11 Proof of auxiliary results for Theorem 9
In this section, we present the proofs of the auxiliary results for Theorem 9.
C.11.1 Proof of Theorem 34
The proof closely follows part (a) and part (c) of Theorem 5 (see Appendix C.3). Therefore, we outline the modiﬁcations to the proof of Theorem 5, in order to extend to any Ωt ⊆ [d] × [n] obtained by Algorithm 1.
Proof Theorem 34(a) The proof closely follows the proof of Theorem 5(a) (see Appendix C.3.1) with the modiﬁcations discussed in what follows.
79

Extending Sc to Sct

Recall from (9) that

t ik

denotes

the

number

of

students

in

course

i

∈

[d]

of

group

k ∈ [r] restricted to the training set Ωt, and

t k

denotes

the

number

of

students

in

group

k

restricted

to

the

training set Ωt. We extend the deﬁnition (30) of Sc and deﬁne

tt
Sct := (i, i ) ∈ [d]2 : ∃k ∈ [r] such that itk , itk+1 ≥ c .
k k+1

Extending Lemma 29 to Sct restricted to the training set Ωt We show that Lemma 29 holds for any (i, i ) ∈ Sct, and the estimator (15) x(0) restricted to Ωt.
Denote btik,max as the largest bias in course i of group k restricted to the training set Ωt, and denote btk,max as the largest bias of group k restricted to the training set Ωt. We extend (216) to show that the diﬀerence between the ranks of btik,max and btk,max is bounded by some constant with high probability.
Moreover, it can be veriﬁed that the diﬀerence between the ranks of btk,max and bk,max is bounded by a constant with high probability. Combining these two bounds, the diﬀerence between the ranks of btik,max and bk,max is bounded by a constant with high probability. We deﬁne bti k+1,min and bk+1,min likewise, and extend (217) to show that the diﬀerence between the ranks of bti k+1,min and bk+1,min is bounded by a constant with high probability. Therefore, we extend 220 to:
bti k+1,min − btik,max < , with probability at least 1 − δ.
Following the rest of the original arguments for Lemma 29 (see Appendix C.10) completes the extension of Lemma 29 to being restricted to Ωt.

Extending Lemma 31 to Sct restricted to Ωt We replace the set Sc in Lemma 31 by the set Sct. It can be veriﬁed that Lemma 31 holds under this extension following its original proof (see Appendix C.10).

Extending the rest of the arguments For any i ∈ [d], k ∈ [r], by (26b) and (27b) from Lemma 26 we have

t

ik

ik ≥ 4 = ik .

t k

34k 3 k

Hence, any (i, i ) ∈ S cf , we have (i, i ) ∈ Stcf . The rest of the arguments follow from the original proof of

d

3d

Theorem 5(a) (see Appendix C.3.1).

Proof of Theorem 34(b) The proof closely follows the proof of Theorem 5(c) (see Appendix C.3.3) with the modiﬁcations discussed in what follows.

Extending Sc to Sct Recall that for total orderings, we have (i, i ) ∈ S1 if and only if there exists some k ∈ [dn − 1] such that course i contains the element of rank k, and course i contains the element of rank (k + 1). We deﬁne the following set St , where we consider the rank with respect to the total ordering restricted to the elements in Ωt. That is, we extend the deﬁnition (30) of Sc and deﬁne

 (i, i ) ∈ [d]2 : ∃1 ≤ k < k ≤ |Ωt|







  St :=

such that the element of rank k is in Ωti,

 
.

the element of rank k is in Ωti+1,





 

the elements of ranks (k + 1) through (k − 1) are in Ωv

 

(225)

80

Extending Lemma 29 By Lemma 27(a) we have that for any (i, i ) ∈ St , the corresponding values of k and k in (225) satisfy k − k ≤ 2d + 1. We deﬁne M as the maximal diﬀerence between elements that are adjacent within Ωt. Then by Lemma 23 we extend the bound of M in (219) to M as
δ P (M < ) > 1 − 2 .
Following the rest of the arguments in Appendix C.10.1, we have that Lemma 29 holds restricted to the training set Ωt.

Extending Lemma 31 to Sct restricted to Ωt We replace the set Sc in Lemma 31 by the set St . It can be veriﬁed that Lemma 31 holds under this extension following its original proof (see Appendix C.10).

Extending the rest of the arguments The rest of the arguments follow from the original proof of Theorem 5(c) (see Appendix C.3.3). Speciﬁcally, we replace the set S1 by St . We consider the total ordering
restricted to the training set Ωt. We extend the deﬁnition (60) of (bL, bH) to (bL, bH) deﬁned as:

bL := bH :=

1

|Ωt|

bij

i∈VL i i∈VL j∈Ωt

i

1

|Ωt|

bij .

i∈VH i i∈VH j∈Ωt

i

C.11.2 Proof of Lemma 35

We ﬁx any partial ordering O that satisﬁes the all cf -fraction assumption, and ﬁx any training-validation split (Ωt, Ωv) obtained by Algorithm 1. Recall that T denotes the set of all total orderings that are consistent
with the partial ordering O. Recall from Line 15 of Algorithm 1 that the interpolated bias is computed as:

B(λ) = 1 |T |

Bπ(λ),

π∈T

(226)

where recall from Line 13 of Algorithm 1 that [Bπ(λ)]ij for any (i, j) ∈ Ωv is computed as the mean value of B on the nearest-neighbor(s) of (i, j) with respect to the total ordering π. Recall that NN(i, j; π) denotes the set (of size 1 or 2) of the nearest neighbor(s) of (i, j). We have

[Bπ(λ)]ij = |NN(i1, j; π)|

Bi(πλj)π .

(iπ ,jπ )∈NN

(227)

Plugging (227) to (226), we have

(λ) 1

1

(λ)

Bij

= |T |

|NN(i, j; π)|

Biπjπ .

π∈T

(iπ ,jπ )∈NN

The remaining of the proof is outlined as follows. We decompose the summation over π ∈ T on the RHS
of (226) into two parts: total orderings π ∈ T where the set of nearest-neighbors NN(i, j; π) is within group
k, and total orderings π ∈ T where at least one nearest-neighbor in NN is outside group k. We show bk = btk in the ﬁrst case, and then show that the second case happens with low probability.
We consider any group k ∈ [r], and any element in the validation set of group k, that is, (i, j) ∈ Gvk. Let Tin ⊆ T denote the subset of total orderings where the nearest-neighbor set NN(i, j; π) is contained within
group k:

Tin := {π ∈ T : NN(i, j; π) ⊆ Gtk}.

81

Let Tout := T \ Tin denote the subset of total orderings where at least one nearest-neighbor from NN(i, j; π) is from outside group k. It can be veriﬁed by symmetry that the value of Bi(jλ) is identical for all (i, j) ∈ Gvk. Recall that we denote this value as bk := Bi(jλ) for (i, j) ∈ Gvk.

Case of π ∈ Tin: By the deﬁnition of Tin, we have NN(i, j; π) ⊆ Gtk. By symmetry, it can be veriﬁed that the mean of the nearest-neighbor set of the element (i, j) over Tin is simply the mean of all training elements
in Gtk. That is,

1 |Tin|

[B(λ)]ij = 1

π

|Gtk |

b(iλj) (=i) btk,

π∈Tin

(i ,j )∈Gtk

(228)

where step (i) is true by the deﬁnition of btk.

Case of π ∈ Tout: We bound the size of Tout. If a nearest-neighbor of the element (i, j) is outside group k, then this nearest-neighbor can only come from group (k − 1) or (k + 1). First consider the case where a nearest-neighbor is from group (k − 1). Assume that the element (i, j) is ranked t ∈ [ k] within the set Gk of all elements from group k with respect to π. A nearest-neighbor is from group (k − 1), only if all elements ranked 1 through t − 1 are all in the validation set (otherwise there is some training element whose rank is between 1 and (t − 1) within group k, and this element is closer to (i, j) than any element from group (k − 1), giving a contradiction). Out of the total orderings in T where (i, j) is ranked t within group k, the fraction of total orderings that the elements ranked 1 through (t − 1) within group k are all in the validation set Ωv is:

t−1 vk − i ≤ i=1 k − i

v t−1 (i)

k

<

k

3t ,
4

where (i) is true due to (27a) from Lemma 26. By symmetry, the fraction of π ∈ T such that (i, j) is placed
in each position t ∈ [ k] is 1 . Therefore, the fraction of total orderings that a nearest-neighbor is from group k
(k − 1) is upper-bounded by:

1k
k t=1

3 t 3 (i) 3

≤< ,

4

k dcf n

where inequality (i) holds because k = i∈[d] ik > dcf n due to the all cf -fraction assumption. By the same
argument, the fraction of total orderings that at least one nearest-neighbor is from group (k + 1) is also upper-bounded by dc3fn . Hence, we have

|Tout| < 6 . |T | dcf n

(229)

For any (i, j) ∈ Gvk, we have

1 bk = |T |

[Bπ(λ)]ij +

[Bπ(λ)]ij

(i) 1 =
|T |

|Tin| · btk +

[Bπ(λ)]ij ,

π∈Tin

π∈Tout

π∈Tout

where equality (i) is true by plugging in (228). Hence, we have

bk − btk

1 =
|T |

[Bπ(λ)]ij − btk

π∈Tout

1 ≤
|T |
π∈Tout

[Bπ(λ)]ij + btk

(i) 2|Tout|

(ii) 12

≤

max bij ≤

· max bij ,

|T | i∈[d],j∈[n]

cf dn i∈[d],j∈[n]

82

where inequality (i) is true because [Bπ(λ)]ij and btk are both the mean of B on a subset of its elements, so we have [Bπ(λ)]ij ≤ maxi∈[d],j∈[n] bij and btk ≤ maxi∈[d],j∈[n] bij . Then step (ii) is true by plugging in (229).
This completes the proof.

C.11.3 Proof of Corollary 36 Fix any > 0. By the consistency of B(0) from (69), we have

lim P Bi(j0) − Bij < , ∀(i, j) ∈ Ωt = 1.

n→∞

2

Since btk and btk are simply the mean of B and B over Gtk ⊆ Ωt. We have

(230)

For each k ∈ [r], we have

lim P btk − btk < , ∀k ∈ [r] = 1.

n→∞

2

(231)

bk − btk ≤ bk − btk + btk − btk

(i) 12

≤

·

max

b

+ bt − bt

cf dn i∈[d],j∈[n] ij

kk

12 ≤
cf dn

max |bij| + max bij − bij

i∈[d],j∈[n]

i∈[d],j∈[n]

+ btk − btk ,

(232)

where (i) is true by combining Lemma 35. In (232), we bound the term maxi∈[d],j∈[n]|bij| by Lemma 25 as

lim P max |bij| < 2 log dn = 1.
n→∞ i∈[d],j∈[n]

(233)

We bound the term maxi∈[d],j∈[n] bij − bij by (230), and the term ging (233), (230) and (231) into (232), we have

btk − btk

by (231).

Hence, plug-

Equivalently,

lim P
n→∞

bk − bt ≤ 12 2 log dn + + ,

k cf dn

22

∀k ∈ [r] = 1.

completing the proof.

lim P bk − btk ≤ , ∀k ∈ [r] = 1,
n→∞

C.11.4 Proof of Lemma 37

We ﬁx any training-validation split (Ωt, Ωv) and ﬁx any > 0 and δ > 0. We ﬁrst condition on any value of

the bias as B = B∗. Then the bias terms in Gvik (whose mean is bvik) can be considered as randomly sampling

v ik

values

from

the

k terms in Gk (whose mean is bk). Denote ∆B∗ := maxi∈[d],j∈[n] b∗ij − mini∈[d],j∈[n] b∗ij ,

and denote ∆B := maxi∈[d],j∈[n] bij − mini∈[d],j∈[n] bij. By Hoeﬀding’s inequality without replacement [20,

Section 6], we have

 P |bvik − b∗k| > ∆B∗

log 1δ
v ik

 B = B∗ ≤ 2 exp

2 vik∆2B∗ log

1 δ

−

v ∆2

ik B∗

=

2δ2

(i)
<

δ ,

2

(234)

83

where inequality (i) is true for any δ ∈ (0, 14 ). Invoking (26a) from Lemma 26 and using the all cf -fraction assumption, we have

vik ≥ 4ik > cf4n .

(235)

Combining (234) with (235), we have that for any δ ∈ (0, 14 ),

 P |bvik − b∗k| > 2∆B∗

log 1δ cf n

 B = B∗ < δ .
2

(236)

Now we analyze the term ∆B in (236). By Lemma 25, there exists integer n0 such that for any n ≥ n0,

δ P ∆B ≤ 4 log dn ≥ 1 − 2 .

(237)

√ Let n1 be a suﬃciently large constant such that n1 ≥ n0 and 8 log dn · with (236), for any n ≥ n1,

locgf(nδ1 ) < . Then combining (237)

P |bvik − bk| <

=

P |bvik − bk| <

B ∗ ∈Rd×n

B = B∗ · P(B∗) dB∗

≥ B∗∈R√d×n P |bvik − bk| <
∆B∗ ≤4 log dn

B · P(B) dB∗

(i)

δ

≥ 1 − 2 · P ∆B ≤

4 log dn

(ii)

δ2

≥ 1 − ≥ 1 − δ,

2

where inequality (i) is true by (236) and inequality (ii) is true by (237). Equivalently, we have

lim P |bvik − bk| <
n→∞

= 1.

(238)

Due to the all c-fraction assumption, the number of groups is upper-bounded as r ≤ c1f . Taking a union bound of (238) over i ∈ [d], k ∈ [r], we have

lim P |bvik − bk| < , ∀i ∈ [d], k ∈ [r] = 1,
n→∞
completing the proof of (75a). A similar argument yields (75b), where in (235) we invoke (27b) from Lemma 26 instead of (26a).

C.11.5 Proof of Lemma 39

In the proof, we use the following lemma.

Lemma 51. Let d ≥ 1 be an integer. For any y ∈ Rd, we have

arg min

y−u

2 2

+

λ

u

2 2

= arg min

ΠM(y) − u

2 2

+

λ

u

2 2

u∈M

u∈M

(239)

The proof of Lemma 51 is presented at the end of this section. We now derive a the closed-form solution to (239). Consider the optimization problem on the RHS of (239). We take the derivative of the objective

84

with respect to u, and solve for u by setting the derivative to 0. It can be veriﬁed that the unconstrained solution u∗un to the RHS of (239) is:

u∗un = 1 +1 λ ΠM(y).

(240)

Note that this unconstrained solution u∗un satisﬁes u∗un ∈ M, so u∗un is also the (constrained) solution to (239). Plugging (240) to the objective on the LHS of (239) and rearranging the terms complete the proof.

Proof of Lemma 51 We apply induction on the Pool-Adjacent-Violators algorithm (PAVA) [2, Section
1.2]. For completeness, the Pool-Adjacent-Violators algorithm is shown in Algorithm 2. For any integer d ≥ 1 and any input y ∈ Rd, PAVA returns arg minu∈M y − u 22.

Algorithm 2: The Pool-Adjacent-Violators algorithm (PAVA). Input: y ∈ Rd.

1 Initialize u = y

2 Initialize the partition P = {S1, . . . , Sd}, where Si = {i} for every i ∈ [d].

3 while u ∈ M do

4 Find any i ∈ [d] such that ui > ui+1. 5 Find S, S ∈ P such that i ∈ S and i + 1 ∈ S .

6

Update

ur

←

1 |S|+|S

|(

i∈S ui +

i∈S ui) for each r ∈ S ∪ S .

7 Update the partition as P ← P \ {S, S } + {S ∪ S }.

8 end

9 return u

Assume that the while loop in Algorithm 2 is executed T times. Let u(0) → u(1) → . . . → u(T ) be any sequence of the value of x obtained in Algorithm 2. We have u(0) = y and u(T ) = ΠMy. In what follows, we show that for any 0 ≤ t ≤ T − 1,

arg min

u(t) − u

2 2

+

λ

u

2 2

= arg min

u(t+1) − u

2 2

+

λ

u

22.

u∈M

u∈M

(241)

By induction on (241), we have

arg min

u(0) − u

2 2

+

λ

u

2 2

= arg min

u(T ) − u

2 2

+

λ

u

22.

u∈M

u∈M

(242)

Combining (242) with the fact that u(0) = y and u(T ) = ΠMy completes the proof.

Proof of (241): Consider any t such that 0 ≤ t ≤ T − 1. We consider Line 4-6 of PAVA in Algorithm 2. For clarity of notation, we denote the partition corresponding to u(t) as P (t) and the partition corresponding to u(t+1) as P (t+1). Then we have S, S ∈ P (t) and S ∪ S ∈ P (t+1).
First, by PAVA it is straightforward to verify that S and S both contain consecutive indices. That is,
there exists integers m1, m2 such that 1 ≤ m1 ≤ i < m2 ≤ d, such that

S = {m1, . . . , i} S = {i + 1, . . . , m2}.

Furthermore, by PAVA it can be veriﬁed that

a :=u(it) = u(it) ∀i, i ∈ S b :=u(it) = u(it) ∀i, i ∈ S z :=u(it+1) = u(it+1) ∀i, i ∈ S ∪ S .

(243a) (243b) (243c)

85

Denote these values in (243) as a, b and z, respectively. By the update of u in Line 6 of Algorithm 2, we have the relation

1

z=

(|S| · a + |S | · b) .

|S| + |S |

(244)

Denote u∗(t) and u∗(t+1) as the minimizer to the LHS and RHS of (241), respectively. Using (243), it can be veriﬁed that

a∗ :=ui∗(t) = ui∗(t) ∀i, i ∈ S b∗ :=ui∗(t) = u∗i (t) ∀i, i ∈ S
ui∗(t+1) = ui∗(t+1) ∀i, i ∈ S ∪ S .

(245a) (245b) (245c)

Denote the values in (245a) and (245b) as a∗ and b∗, respectively. We now show that a∗ = b∗. Assume for contradiction that a∗ = b∗. Since the solution u∗(t) ∈ M, we have
a∗ ≤ b∗. Hence, we have a∗ < b∗. By Line 4 of Algorithm 2, we have a > b. We construct the alternative
solution

vi∗(t) =

ui∗(t) |S|+1|S | (|S| · a∗ + |S| · b∗)

i∈S∪S i∈S∪S .

It can be veriﬁed that v∗(t) attains a strict strictly smaller objective than u∗(t) for the objective on the LHS of (241). Contradiction to the assumption that u∗(t) is the minimizer to the LHS of (241). Hence, we have a∗ = b∗, implying

ui∗(t) = u∗i (t) ∀i, i ∈ S ∪ S .

The LHS of (241) is equivalent to

arg min

(u(it) − xi)2 +

(u(it) − xi)2 + λ

u

2 2

u∈M,t∈R i∈S∪S

i∈S∪S

t=ui, ∀i,i ∈S∪S

arg min

(u(it) − xi)2 + |S| · (a − t)2 + |S | · (b − t)2 +λ u 22.

t=ui, u∀∈i,M i ∈S∪S i∈S∪S T

(246)

We write the term T as

T = |S| · a2 + |S | · b2 − 2 (|S| · a + |S | · b) · t + (|S| + |S |) · t2

|S| · a + |S |b

2

= (|S| + |S |) · |S| + |S | − t + term(a, b, S, S )

(i)
=

(|S|

+

|S

|)

·

(z

−

t)2

+

term(a,

b,

S,

S

),

(247)

where equality (i) is true by (244). Using the relation u(it) = u(it+1) for every i ∈ S ∪ S , the RHS of (241) is equivalent to

arg min

(u(it+1) − xi)2 +

(u(it+1) − xi)2 + λ

u

2 2

u∈M,t∈R i∈S∪S

i∈S∪S

t=ui, ∀i∈S∪S

arg min

(u(it) − xi)2 + (|S| + |S |) · (z − t)2 + λ u 22.

u∈M,t∈R i∈S∪S

t=ui, ∀i∈S∪S

(248)

The equivalence of the LHS and RHS of (241) can be veriﬁed by combining (246), (247), and (248).

86

C.11.6 Proof of Lemma 40

Let c > 0 be a constant. Denote Ec ,c as the event that the number of non-overlapping pairs in Sc (instead of Sc ∩ Ωv deﬁned for the event Ecv ,c) is at least c n. We delegate the main part of this proof to the following lemma.
Lemma 52. Suppose d = 2. Assume the bias is distributed according to assumption (A2) with σ = 1. For any c > 0, there exists a constant c > 0 such that

lim P (Ec ,c ∩ E2) = lim P(E2).

n→∞

n→∞

The proof this result is provided at the end of this section. We ﬁrst explain how to complete the proof of

Lemma 40 given Lemma 52. The proof of Lemma 52 is presented at the end of this section.

Conditional on Ec ,c, consider the c n non-overlapping pairs in Sc. We denote this subset of non-overlapping

pairs as S . For each t ∈ [ n2 ] in Lines 5-7 in Algorithm 1, consider the elements (1, j(2t−1)) and (1, j(2t)) in

Line 6 of Algorithm 1. If both (1, j(2t−1)) and (1, j(2t)) are involved in some pairs in S , then we arbitrarily

remove one of the pairs involving either (1, j(2t−1)) or (1, j(2t)) from S . After the removal, the size of

the remaining S is at least c2n . We repeat the same procedure to consider the elements (2, j(2t−1)) and

(2, j(2t)) and remove elements. After this second removal, the size of the remaining S is at least c4n . We now denote this set of non-overlapping pairs after the two removals as S . Now consider any remaining pair

(j, j ) ∈ S

.

The probability of (1, j) ∈ Ωv

is

1 2

and the probability of (2, j ) ∈ Ωv

is

12 .

Hence, the probability

of (j, j ) ∈ S ∩ Ωv is 14 . Due to the removal, all of the elements involved in S appear in diﬀerent pairs

during the training-validation split in Lines 5-7 in Algorithm 1. Hence, the probability of (j, j ) ∈ Ωv is

independent for each pair (j, j ) ∈ S . By Hoeﬀding’s inequality, we have

That is,

lim P |S ∩ Ωv| ≥ c n Ec ,c = 1.

n→∞

32

lim P Evc Ec ,c = 1.

n→∞

32 ,c

Hence, we have

P(Evc ∩ E2) ≥ P(Evc ∩ Ec ,c ∩ E2)

32 ,c

32 ,c

= P(Ec ,c ∩ E2) − P(Evc ∩ Ec ,c ∩ E2)
32 ,c

≥ P(Ec ,c ∩ E2) − P(Evc ∩ Ec ,c).
32 ,c

Taking the limit of n → ∞ in (250), we have

(249) (250)

(i)

lim P(Evc ∩ E2) ≥ lim P(E2),

n→∞

32 ,c

n→∞

where inequality (i) is true by combining Lemma 52 and (249), completing the proof of Lemma 40. It remains to prove Lemma 52.

Proof of Lemma 52 Recall the deﬁnition (109) of Sc = {(j, j ) ∈ [n]2 : 0 < b2j − b1j < c}. We ﬁrst convert the constraint 0 < b2j − b1j < c to a constraint on the ranks of the elements (1, j) and (2, j ).
Recall that g denotes the p.d.f. of N (0, 1). Recall that t(ij) is the rank of the element (i, j) (in the total ordering of all 2n elements since we assume d = 2). For any constant γ ∈ (0, 1/2), we deﬁne the following set of pairs:

(j, j ) ∈ [n]2 : γn < t1j < t2j < (2 − γ)n,

Rγ,c =

t2j − t1j ≤ cg( γ )n

.

2

87

The following lemma shows that Rγ,c is a subset of Sc for each γ > 0 with high probability, and therefore we only need to lower-bound the number of non-overlapping pairs in Rγ,c.
Lemma 53. For each c > 0, for any γ ∈ 0, 21 , we have

lim P Rγ,c ⊆ S2c = 1.
n→∞
The proof of this result is provided in Appendix C.11.7. Denote Eγ,c ,c as the event that the set Rγ,c contains at least c n non-overlapping pairs. We have that Eγ,c ,c is deterministic (depending on γ, c , c and the total ordering π). Then Lemma 53 implies that for any γ ∈ 0, 12 and any c ∈ (0, 1),

lim P Eγ,c ,c ∩ Ec ,2c = 0.
n→∞
In what follows, we establish that there exists γ > 0 and c > 0 such that

(251)

lim P Eγ,c ,c ∩ E2 = 0,
n→∞
where the choices of γ and c are speciﬁed later.

(252)

Proof of (252): Assume there exists maximally t such non-overlapping pairs in Rγ,c (that is, Rγ,c does not have any subset of non-overlapping pairs of size greater than t). Assume for contradiction that

t < min cg( γ2 ) , γ · n. 2

(253)

We “remove” these t pairs from the total ordering of 2n elements, and then there are 2(n − t) remaining elements after the removal. In what follows, we derive a contradiction by using the fact that theses elements are not in Rγ,c.
Denote the ranks corresponding to the remaining elements from course 2 with rank between (γn, (2 − γ)n] as j1 < . . . < jT . Since t elements are removed from each course, we have

T ≤ n − t.

(254)

Since there are (n − t) remaining elements in course 2, and the number of elements whose rank is outside the range (γn, (2 − γ)n] is 2γn, we also have T ≥ n − t − 2γn > 0. Denote the diﬀerence of the ranks between adjacent remaining elements in course 2 as

 j1 − γn − 1 
i = ji+1 − ji − 1
(2 − γ)n − ji

if i = 0 if 1 ≤ i ≤ T − 1 if i = T.

(255)

The deﬁnition (255) of is also visualized in Fig. 7.

ℓ- ℓ)

ℓ+

1

𝛾𝑛 𝑗)

𝑗*

𝑗+ (2 − 𝛾)𝑛

2𝑛

Figure 7: The deﬁnition (255) of .

By in the deﬁnition of (255), we have

T

(i)

i = (2 − 2γ)n − T ≥ (1 − 2γ)n + t,

i=0

88

where inequality (i) is true by (254). There are also (n − t) remaining elements in course 1. We consider the ranks where these elements
can be placed. Again, the number of positions outside the range (γn, (2 − γ)n] is 2γn. Therefore, at least (1 − 2γ)n − t elements form course 1 need to placed within the range of (γn, (2 − γ)n]. Inside this range, the cg γ2 n ranks before each element in course 2 cannot be placed, because otherwise this element from course 1 and the corresponding element from course 2 form a pair in Rγ,c. Contradiction to the assumption that a maximal subset of non-overlapping pairs has been removed. Hence, inside the range, the number of ranks where elements from course 1 can be placed is

T −1
max
i=0

γ i − cg 2 n, 0 + T .

Since we need to place at least (1 − 2γ)n − t elements from course 1 to these ranks, we have

T −1
max
i=0

γ i − cg 2 n, 0 + T ≥ (1 − 2γ)n − t.

(256)

Now we separately discuss the following two cases.

Case 1:

i ≥ cg

γ 2

n for some 0 ≤ i ≤ T − 1.

Then

consider

the

interval

[ji

−

cg

(

γ 2

)

n,

ji

)

.

On the one

hand, there cannot be elements from course 2 in this interval, because we deﬁne i as the diﬀerence of ranks

between elements ji+1 and ji that are already adjacent among elements in course 2. On the other hand, there

cannot be elements j from course 1 in this interval, because otherwise we have (j, ii) ∈ Rγ,c. Contradiction to the assumption that the removed subset of non-overlapping pairs is maximal. Hence, all of the cg γ2 n elements from this interval [ji − cg( γ2 )n, ji) have been removed, and we have t ≥ cg(2γ2 )n . Contradiction to the assumption (253).

Case 2:

i < cg

γ 2

n for all 0 ≤ i ≤ T − 1. Then inequality (256) reduces to

(i)
T ≥ (1 − 2γ)n − t ≥ (1 − 3γ)n,

(257)

where inequality (i) is true by the assumption (253) that t < γn. In what follows, we consider the construction of ranks of all elements (either removed or not) that
maximizes j∈[n](b2j − b1j). Then we show that under the assumption (253), we have





lim P 
n→∞

(b2j − b1j) < 0 = 1.

j∈[n]

Construction of the ranks: To maximize j(b2j − b1j), we want to assign elements in course 2 to higher ranks, and elements in course 1 to lower ranks. We consider the course assigned to the following ranges of the rank.
• Ranks ((2 − γ)n, 2n] : The size of this range is 2γn. We assign elements from the course 2 to these ranks, since these are the highest possible ranks.
• Ranks ((1 + 2γ)n, (2 − γ)n]: The size of this range is (1 − 3γ)n. Note that the rank jT is
(i)
jT = (2 − γ)n − T
(ii)
≤ (2 − γ)n − (1 − 3γ)n = (1 + 2γ)n,
where equality (i) is true by the deﬁnition (255), and inequality (ii) is true by (257). We consider the number of elements from course 2 in this range, remaining or removed. By the deﬁnition of jT

89

from (255) there cannot exist remaining elements from course 2 in this range. The number of removed elements from course 2 is t ≤ γn by assumption (253). Hence, the number of elements from course 2 in this range is at most γn. The other elements in this range are from course 1. Hence, the number of elements from course 1 in this range is at least (1 − 4γ)n. We assign the elements in course 2 to higher ranks than the elements in course 1.

• Ranks [1, (1 − 2γ)n] There are 4γn elements from course 1, and (1 − 2γ)n elements from course 2 that have not been assigned to ranks. We simply assign the (1 − 2γ)n elements from course 2 to be higher ranks than the 4γn elements from course 1.
This construction of ranks is also shown in Fig. 8. We denote S1L, S2L, S1H , S2H respectively as the sums of the subset of elements as shown in Fig. 8.

sum of the elements number of the elements

𝑆*+ course 1
4𝛾𝑛

𝑆,+ course 2 (1 − 2𝛾)𝑛

𝑆*course 1 (1 − 4𝛾)𝑛

𝑆,course 2
2𝛾𝑛

rank

1

4𝛾𝑛 (0.5 + 3𝛾)𝑛

(1 + 2𝛾)𝑛

1.5𝑛

(2 − 2𝛾)𝑛 2𝑛

𝑎*

𝑎,

𝑎4

𝑎5

𝑎6

𝑎7

𝑎8

Figure 8: Assignment of biases to the 2 courses.

The following lemma now bounds the diﬀerence between the sums of the bias in the two courses, under this construction. Lemma 54. Consider 2n i.i.d. samples from N (0, 1), ordered as X(1) ≤ . . . ≤ X(2n). Let
I1L := {1, . . . , 4γn} I2L := {4γn + 1, . . . , (1 + 2γ)n} I1H := {(2 − 2γ)n, . . . , 2n} I2H := {(2 − 2γ)n, . . . , 2n},
and let
I1 := I1L ∪ I1H , I2 := I2L ∪ I2H .
Then there exists some constant γ > 0, such that

lim
n→∞

X(i) − X(i) < 0

i∈I2

i∈I1

= 1.

The proof of this result is provided in Appendix C.11.8. Denote the constant γ in Lemma 54 as γ0. By Lemma 54, we have that under the assumption (253) of t < min cg(2γ20 ) , γ0 n, then





Equivalently, let c0 = min

cg(

γ0 2

)

γ0

lim P 
n→∞

(b2j − b1j) < 0 = 1.

j∈[n]

, we have

completing the proof of (252).

lim P Eγ0,c ,c ∩ E2 = 0,

n→∞

0

90

Combining (251) and (252): We have

lim P
n→∞

Ec0,c ∩ E2

= P(E2) − P(E2 ∩ Ec ,c) = P(E2) − P(E2 ∩ Ec ,c) = P(E2) − P(E2 ∩ Ec ,c ∩ Eγ0,c0,c) − P(E2 ∩ Ec0,c ∩ Eγ0,c0,c).

Taking the limit of n → ∞ in (258), we have

(i)
Ec ,c ∩ E2 = lim P(E2),

P0

n→∞

where equality (i) is true by combining (251) and (252). This completes the proof of Lemma 52.

(258)

C.11.7 Proof of Lemma 53

We show that for any (j, j ) ∈ Rγ,c we have (j, j ) ∈ S2c due to the assumption ((A2)). First, by the

deﬁnition of Rγ,c we have t1j < t2j , and hence b2j > b1j. It remains to show that b2j − b1j < c. We denote

(t0, . . . , tT )

:=

(γ, γ

+ cg( γ2 ), . . . , (2 − γ)),

where

T

=

2−2γ cg( γ )

which

is

a

constant.

Recall

that

b(k : 2n)

denotes

2

the kth order statistics among the 2n random variables. Recall that G−1 denotes the inverse c.d.f. of N (0, 1).

By Lemma 24 we have

b(tin : 2n) −P→ G−1 ti 2

∀0 ≤ i ≤ T.

(259)

Taking a union bound of (259) over 0 ≤ i ≤ T , we have

lim
n→∞

b(tin : 2n) − G−1 ti 2

c < ∀0 ≤ i ≤ T
2

E

= 1.

(260)

Denote this event in (260) as E. By the deﬁnition of Rγ,c, for any (j, j ) ∈ Rγ,c we have γn < t1j < t2j < (2 − γ)n and t2j − t1j < cg( γ2 )n. Hence, there exists some integer 0 ≤ i ≤ T − 2 such that tin ≤ t1j < t2j ≤ ti+2n. Conditional on the event E from (260), for any (j, j ) ∈ Rγ,c,

b2j − b1j ≤ b(ti+2n : 2n) − b(t n : 2n) < G−1 ti+2 − G−1 ti + c

i

2

2

< (ti+2 − ti) · max (G−1) (x) + c 2 x∈( γ2 ,1− γ2 )

(i)

γ

1

= cg · max

+c

2

x∈

(

γ 2

,1− γ2

)

g(x)

γ

1

= cg

2

· g

γ

+ c = 2c

E.

2

where

(i)

holds

due

to

the

equality

(G−1)

(x)

=

1 G (x)

=

1 g(x)

for

all

x

∈

(0, 1).

Hence,

Rγ,c

⊆

S2c

conditional

on E, and we have

(i)
lim P(Rγ,c ⊆ S2c) ≥ lim P(E) = 1,

n→∞

n→∞

where equality (i) is true by (260), completing the proof.

91

C.11.8 Proof of Lemma 54 We denote the random variables S1L, S2L, S1H and S2H as the sums over I1L, I2L, I1H and I2H , respectively. To bound these sums, we consider the values of X(i) at the following 7 ranks:
i ∈ {1, 4γn, (0.5 + 3γ)n, (1 + 2γ)n, 1.5n, (2 − 2γ)n, 2n},
as shown by the cross marks in Fig. 8. Let a ∈ R7. In what follows we condition on the event that
X (1), X (4γn), X ((0.5+3γ)n), X ((1+2γ)n), X (1.5n), X ((2−2γ)n), X (2n) T = a.
Denote the expected means of S1L, S2L, S1H and S2H conditional on a as µ1L|a, µ2L|a, µ1H|a and µ2H|a, respectively.

Bounding the sums S1L, S2L, S1H and S2H conditional on a: We ﬁrst consider the sum S2H . By Hoeﬀding’s inequality, we have

lim P
n→∞
lim P
n→∞
lim P
n→∞
lim P
n→∞

S1L − 4γnµ1L|a < (a7 − a1) n log n a = 1 S2L − (1 − 2γ)nµ2L|a < (a7 − a1) n log n a = 1 S1H − (1 − 4γ)nµ1H|a < (a7 − a1) n log n a = 1 S2H − 2γnµ2H|a < (a7 − a1) n log n a = 1.

(261a) (261b) (261c) (261d)

Taking a union bound of (261) and using the equality we have

i∈I2 X (i) −

i∈I1 X(i) = S2L + S2H − S1L − S1H ,

lim P
n→∞

X(i) − X(i)

i∈I2

i∈I1

log n ≤ n (1 − 2γ)µ2L|a − (1 − 4γ)µ1H|a + 2γµ2H|a − 4γµ1L|a + 4(a7 − a1) n a

T

We rearrange the terms in T as

= 1.

T = (1 − 4γ)(µ2L|a − µ1H|a) + 4γ(µ2H|a − µ1L|a) + 2γ(µ2L|a − µ2H|a) + 4(a7 − a1)

log n .
n

(262)

In what follows, we deﬁne a range A on the values of a, show that limn→∞ P(a ∈ A) = 1 and show that T < 0 conditional on any a ∈ A.

Deﬁning the range A and showing limn→∞ P(a ∈ A) = 1: We deﬁne the range A ⊆ R7 as

 a1 < G−1(1.5γ)







  

a2

> G−1(1.99γ)



   

√

A :=

 

a3 < G−1(0.25 + 1.5γ) + 0.01

 
∩

a1 > −√2 log 2n

.

a5 > G−1(0.75) − 0.01

a7 < 2 log 2n





  

a6

< G−1(1 − 0.99γ)

  





 

a7

> G−1(1 − 0.5γ)

 

(263)

92

By Lemma 24, we have
a2 −P→ G−1(2γ) a3 −P→ G−1(0.25 + 1.5γ) a5 −P→ G−1(0.75) a6 −P→ G−1(1 − γ).
Moreover, for the extremal values a1 and a7, we have that for any c ∈ R,
lim P(a1 < c) = 1
n→∞
lim P(a7 > c) = 1.
n→∞
Combining (264), (265) and Lemma 25, we have that for any γ > 0,
lim P(E) = 1.
n→∞

(264a) (264b) (264c) (264d)
(265a) (265b)

Analyzing the expected means µ1L|a, µ2L|a, µ1H|a, µ2H|a: We analyze the terms on the RHS of (262).

Term (µ2L|a − µ1H|a): any γ < 0.1,

We

have

µ2L

≤

a3 +a4 2

and

µ1H

≥

a4

+a5 2

.

Therefore,

conditional

on

any

a ∈ A,

for

a3 − a5 (i)

µ2L|a − µ1H|a ≤

≤ −0.5,

2

where inequality (i) is true by the deﬁnition (263) of A.

(266)

Term (µ2H − µ1L): Let X denote a random variable of N (0, 1). Conditional on any a ∈ A,

1

1

µ2H|a = √

2π P (a6 < X < a7)

a7

2

xe

−

x 2

dx

a6

1 =√

1

−

e

−

x2 2

a7

2π P (a6 < X < a7)

x=a6

1 ≤√

1

e

−

a26 2

2π P (a6 < X < a7)

(i) 1 1

[ ] G−1(1−0.99γ) 2

≤√

e−

2

,

2π 0.49γ

where (i) is true by the deﬁnition (263) of A. Similarly, conditional on the event E and on any a,

1 µ1L|a > − √

G−1(1.99γ) 2

1 e [ ] . −

2

2π 0.49γ

(267a) (267b)

Term: (µ2L|a − µ2H|a): For any a ∈ R7, we have (µ2L|a − µ2H|a) < 0.

(268)

93

Showing T < 0: a ∈ A,

Plugging the three terms from (266), (267) and (268) back to (262), conditional on any

11 T < −0.5(1 − 4γ) + 4 · √
2π 0.49

e−

[G−1

(1−0.99γ 2

)]2

+

e−

[G−1 (1.99γ )]2 2

+ 8 log n

log 2n .
n

As γ → 0, we have G−1(1.99γ) → −∞ and G−1(1 − 0.99γ) → ∞. It can be veriﬁed that there exists some suﬃciently small γ0 > 0, such that

Hence, we have

lim T < 0 a ∈ A.
n→∞

lim P
n→∞
completing the proof.

X(i) − X(i) ≤ 0

i∈I2

i∈I1

≥ lim

P (T < 0 | a) P(a)

n→∞ a∈R7

≥ lim P(a ∈ A) = 1,
n→∞

C.12 Proof of auxiliary results for Theorem 10
In this section, we present the proofs of the auxiliary results for Theorem 10.

C.12.1 Proof of Lemma 41

First, at λ = ∞ we have B(∞) = 0 by Proposition 7, and hence the claimed result is trivially true. Now consider any λ ∈ [0, ∞). We ﬁx any value of Y ∈ Rd×n and any value of x ∈ Rd. Denote U := Y −x1T .
By triangle’s inequality, we have max(i,j)∈Ω|uij| ≤ max(i,j)∈Ω|yij| + x ∞. It then suﬃces to establish the inequality

max |b(ijλ)| ≤ max |uij |,

(i,j)∈Ω

(i,j)∈Ω

where B(λ) is the solution to the optimization

arg min U − B 2Ω + λ B 2Ω,
B satisﬁes O

with ties broken by minimizing B 2Ω. Assume for contradiction that we have

max |b(ijλ)| > max |uij |.

(i,j)∈Ω

(i,j)∈Ω

(269) (270)

Denote umax := max(i,j)∈Ω uij and umin := min(i,j)∈Ω uij. Then we consider an alternative solution B constructed from B(λ) as:

 max(i,j)∈Ω uij  bij = b(ijλ)  min(i,j)∈Ω uij

if b(ijλ) ∈ (umax, ∞) b(ijλ) ∈ [umin, umax] if b(ijλ) ∈ (−∞, umin).

By the assumption (270), there exists some (i, j) ∈ Ω such that b(ijλ) ∈ [umin, umax]. Hence, we have B = B(λ). It can be veriﬁed that B satisﬁes the partial ordering O because B(λ) satisﬁes O. Furthermore, it can be veriﬁed that

U −B

2 Ω

<

U − B(λ)

2 Ω

94

and also

B

2 Ω

<

B(λ)

2 Ω

Hence, B attains a strictly smaller objective of (269) than B(λ). Contradiction to the assumption that B(λ) is the optimal solution of (269).

C.12.2 Proof of Lemma 42

Recall that the monotone cone is denoted as M := {θ ∈ Rd : θ1 ≤ . . . ≤ θd}, and ΠM denotes the

projection (14) onto M .

√

From known results on the monotone cone (see [1, Section 3.5]), we have E[ΠM Z] ≤ c log d for some

ﬁxed constant c > 0. Using the Moreau decomposition, we have (see [50, Eq. 20]):

E sup θT Z = E ΠM Z 2 ≤ c log d.
θ 2=1 θ∈M

Note that we have the deterministic equality supθ∈M, θ 2=1 θT Z ≥ 0 by taking θ = 0. By Markov’s inequality, we have

1

E supθ∈M, θ 2=1 θT Z

√ c log d

P sup θT Z > d 4 ≤

1

≤

1,

θ 2=1

d4

d4

θ∈M

completing the proof.

C.12.3 Proof of Lemma 43

In the proof, we ﬁrst bound the event E 1 , and then combine the events E 1 and E 1 .

36

36

36

Bounding E 1 We denote the interleaving points in Spairs as t(1) < . . . < t . (|Spairs|) It can be veriﬁed that 36
for any k ∈ [|Spairs| − 1], if t(k) ∈ S1 then then we have t(k+1) ∈ S2, and vice versa. Hence, we have

−1 ≤ |S1| − |S2| ≤ 1.

(271)

By Deﬁnition 4 of the cf -fraction interleaving assumption, we have

|S1| + |S2| = |S| ≥ cf n.

(272)

Combining (271) and (272), we have
|S1|, |S2| > cf n . 3
Suppose the smallest interleaving point in S1 is t1 := min S1. We now denote the interleaving points in the increasing order of their rank as:

. . . < t1 < t1 < . . . < t cf n < t cf n < . . . .

3

3

Then we have tk ∈ S1 and tk ∈ S2 for all k ∈

cf n 3

.

we construct the set of distinct pairs as:

Sv := (t2k−1, t2k) : k ∈ cf6n ∩ (Ωv × Ωv).

95

Now we lower-bound the size of Sv. For each k ∈

cf n 6

,

consider the probability that the pair (t2k−1, t2k)

is in Ωv. It can be veriﬁed that the elements of ranks {t2k−1}k∈[ cf6n ] are not adjacent in the sub-ordering

of π restricted to course 1, and hence appear in distinct pairs in Line 5-7 of Algorithm 1 when generating

the training-validation split of (Ωt, Ωv). Hence, the probability that each element {t2k−1}k∈[ cf6n ] is assigned

to Ωv is independently 12 . Similarly, the probability that each element {t2k}k∈[ cf6n ] is assigned to Ωv is 12 .

Hence, the probability of each pair (t2k−1, t2k) is assigned to Ωv is 41 . By Hoeﬀding’s inequality, we have

lim P |Sv| > cf n = 1.

n→∞

36

That is, limn→∞ P E 1 = 1. 36

Combining E 1 and E 1 By a similar argument, we have limn→∞ P E 1

36

36

36

of E 1 and E 1 completes the proof.

36

36

= 1. Taking a union bound

C.12.4 Proof of Lemma 44

Consider any T ∈ {S+ ∩ S1, S− ∩ S1, S+ ∩ S2, S− ∩ S2}. Similar to the proof of Lemma 43, using the fact that the interleaving points alternate between S1 and S2, we have

|T | > cf n . 6

We write the elements in T in the increasing order as k1 < . . . < k cfn < . . . < k|T |. It can be veriﬁed that 6
the elements in {t2k}k∈[ c1f2n ] appear in diﬀerent pairs when generating the training-validation split (Ωt, Ωv) in Line 5-7 of Algorithm 1. Hence, each element in {t2k}k∈[ c1f2n ] is assigned to Ωv independently with probability 12 . Using Hoeﬀding’s inequality, we lower-bound the size of T ∩ Ωv as:

lim P |T ∩ Ωv| > cf n = 1.

n→∞

36

(273)

Taking a union bound of (273) over T ∈ {S+ ∩ S1, S− ∩ S1, S+ ∩ S2, S− ∩ S2} completes the proof.

96

