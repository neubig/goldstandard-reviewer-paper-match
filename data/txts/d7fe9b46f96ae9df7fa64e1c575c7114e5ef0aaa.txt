1–17

Optimal Tensor Methods in Smooth Convex and Uniformly Convex Optimization

Alexander Gasnikov

GASNIKOV@YANDEX.RU

Moscow Institute of Physics and Technology, Institute for Information Transmission Problems, National Re-

search University Higher School of Economics

Pavel Dvurechensky

PAVEL.DVURECHENSKY@GMAIL.COM

Weierstrass Institute for Applied Analysis and Stochastics, Institute for Information Transmission Problems

Eduard Gorbunov Moscow Institute of Physics and Technology

EDUARD.GORBUNOV@PHYSTECH.EDU

Evgeniya Vorontsova Far Eastern Federal University

VORONTSOVAEA@GMAIL.COM

Daniil Selikhanovych

SELIHANOVICH.DO@PHYSTECH.EDU

Moscow Institute of Physics and Technology, Institute for Information Transmission Problems

Ce´sar A. Uribe Massachusetts Institute of Technology

CAURIBE@MIT.EDU

arXiv:1809.00382v11 [math.OC] 3 Feb 2019

September 2, 20181

Abstract

We consider convex optimization problems with the objective function having Lipshitz-continuous p-th order derivative, where p ≥ 1. We propose a new tensor method, which closes the gap be-

tween the lower O

ε

−

2 3p+1

and upper O

ε

−

p

1 +1

iteration complexity bounds for this class of

optimization problems. We also consider uniformly convex functions, and show how the proposed method can be accelerated under this additional assumption. Moreover, we introduce a p-th order condition number which naturally arises in the complexity analysis of tensor methods under this assumption. Finally, we make a numerical study of the proposed optimal method and show that in practice it is faster than the best known accelerated tensor method. We also compare the performance of tensor methods for p = 2 and p = 3 and show that the 3rd-order method is superior to the 2nd-order method in practice. Keywords: Convex optimization, unconstrained minimization, tensor methods, worst-case complexity, global complexity bounds, condition number

1. Introduction

In this paper, we consider the unconstrained convex optimization problem

f (x) → min ,

(1)

x∈Rn

1. The ﬁrst version of this paper appeared on September 2, 2018 in Russian. In the current version we present a translation into English of the main derivations and extend the analysis from the case of strongly convex objective to the case of uniformly convex objectives and add the numerical analysis of our results.

c A. Gasnikov, P. Dvurechensky, E. Gorbunov, E. Vorontsova, D. Selikhanovych & C.A. Uribe.

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

where f has p-th Lipschitz-continuous derivative with constant Mp. For p = 1, ﬁrst-order methods

are commonly used to solve this problem, i.e., gradient descent. The lower bound for the complexity

of these methods was proposed in (Nemirovsky and Yudin, 1983; Nesterov, 2004), and an optimal

method was introduced in (Nesterov, 1983). The case of p = 2, i.e., Newton-type methods, was well

understood only recently. A nearly optimal method was proposed in (Nesterov, 2008), an optimal

method was proposed in (Monteiro and Svaiter, 2013), and a lower bound was obtained in (Agarwal

and Hazan, 2018; Arjevani et al., 2018).

The idea of using higher order derivatives (starting from p ≥ 3) in optimization is known at

least since 1970’s, see Hoffmann and Kornstaedt (1978). Recently this direction of research became

of interest from the point of view of complexity bounds. In the unpublished preprint Baes (2009),

extending the estimating functions technique of Nesterov (2004), proposes accelerated high-order

(tensor) methods for convex problems with complexity O

1
MpRεp+1 p+1 , where p ≥ 1, ε is the

accuracy of the obtained solution xˆ, i.e., f (xˆ) − f ∗ ≤ ε, Mp is the Lipschitz constant of the p-th

derivative, and R is an estimate for the distance between a starting point and the closest solution.

Nevertheless, the author doubts that the obtained methods are implementable since the auxiliary

problem on each iteration is possibly non-convex. Agarwal and Hazan (2018); Arjevani et al. (2018)

construct lower complexity bounds O

Mp Rp+1 ε

2 5p+1

and O

Mp Rp+1 ε

2 3p+1

respectively for

the case f having Lipschitz p-th derivative and conjecture that the upper bound can be improved.

Nesterov (2018) proposes implementable tensor methods showing that an appropriately regularized

Taylor expansion of a convex function is again a convex function, thus making auxiliary problems

on each iteration of the tensor methods tractable. The author also provides an accelerated scheme

with complexity bound O

1
MpRεp+1 p+1 , shows that the complexity of each iteration for p = 3

is of the same order as for the case p = 2, and conjectures the existence of an optimal scheme with

complexity bound O

. MpRp+1
ε

2 3p+1

The optimal method for the case p = 1 has complexity O

1
M1εR2 2 (Nesterov, 1983) and

for p = 2 has the complexity O

2
M2εR3 7 (Monteiro and Svaiter, 2013), but the question of

existence of optimal methods for p ≥ 3 remains open. In this paper we extend the framework of Monteiro and Svaiter (2013) and propose optimal tensor methods for all p ≥ 1. Our approach is also based on regularized Taylor step of Nesterov (2018), and, thus, our optimal method for p = 2 is different from Monteiro and Svaiter (2013).
We also consider problem (1) under additional assumption that f is uniformly convex, i.e., there exist 2 ≤ q ≤ p + 1 and σq > 0 s.t.

f (y) ≥ f (x) + ∇f (x), y − x + σqq y − x q2, ∀x, y ∈ Q.

2

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Under this additional assumption, we show, how the restart technique can be applied to accelerate our method to obtain complexity



2



2



p+1−q  3p+1

O

Mp

3p+1
log

∆0

, q = p + 1;

O  Mp(∆0) q

+ log ∆0  , q < p + 1,

σp+1

2ε

 

p+1



σq q

2 ε

where f (x0) − f ∗ ≤ ∆0. This bound suggests a natural generalization of ﬁrst- and second-order condition number (Nesterov, 2008). If f is such that q = p+1, then the complexity of our algorithm depends only logarithmically on the starting point and is proportional to

2
(γp) 3p+1 ,

where γp =

Mp σ

is the p-th order condition number.

Nemirovsky and Yudin (1983); Nesterov

(2004) and Arjpe+v1ani et al. (2018) propose lower bounds for particular cases of strongly convex

functions (i.e., q = 2) with p = 1 and p = 2 respectively. Our upper bounds match them.

As a related work, we also mention Birgin et al. (2017); Cartis et al. (2018), who study com-

plexity bounds for tensor methods for ﬁnding approximate stationary points with the main focus

on non-convex optimization, which we do not consider in our work. Also the work in (Wibisono

et al., 2016) considers tensor methods from the variational perspective and obtains similar bounds

to those in Baes (2009). The ﬁrst version of this paper appeared in arXiv on September 2, 2018.

In December 2018, two months after that, Jiang et al. (2018); Bubeck et al. (2018) proposed an

algorithm, which is very similar to our Algorithm 1. Unlike them, we also analyze the case of uni-

formly convex functions and propose an algorithm, which is faster in this case, see our Algorithm 3.

Moreover, we are the ﬁrst to make a numerical study of tensor methods for p = 3 and show that

they work in practice.

Our contributions.

• We propose a new optimal tensor method and analyze its iteration complexity.

• We generalize this method for the case of uniformly convex objectives and propose a deﬁnition of p-th order condition number.

• We make a numerical study of the proposed method and show that our optimal method is faster than accelerated tensor method Nesterov (2018) in practice. We also compare the performance of tensor methods for p = 2 and p = 3 and show that the 3rd-order method is superior to the 2nd-order method in practice.

Notations and generalities. For p ≥ 1, we denote by ∇pf (x)[h1, ..., hp] the directional derivative of function f at x along directions hi ∈ Rn, i = 1, ..., p. ∇pf (x)[h1, ..., hp] is symmetric p-linear form and its norm is deﬁned as

∇pf (x) 2 = max {∇pf (x)[h1, ..., hp] : hi 2 ≤ 1, i = 1, ..., p}
h1,...,hp∈Rn
or equivalently

∇pf (x) 2 = max{|∇pf (x)[h, ..., h]| : h 2 ≤ 1, i = 1, ..., p}.
h∈Rn

3

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Here, for simplicity, · 2 is standard Euclidean norm, but our algorithm and derivations can be generalized for the Euclidean norm given by general a positive semi-deﬁnite matrix B. We consider convex, p times differentiable on R functions satisfying Lipschitz condition for p-th derivative

∇pf (x) − ∇pf (y) 2 ≤ Mp x − y 2, x, y ∈ Rn.

(2)

2. Optimal Tensor Method
Given a function f , numbers p ≥ 1 and M ≥ 0, deﬁne





Tf

p
 (x) ∈ Arg min

1 ∇rf (x) [y − x, ..., y − x] +

M

 y − x p+1 .

(3)

p,M

y∈Rn

r!

(p + 1)!

2

r=0

r



and given a number L ≥ 0 and point z ∈ Rn, we deﬁne

FL,z (x) f (x) + L2 x − z 22 . (4)

Theorem 1 Let sequence (xk, yk, uk), k ≥ 0 be generated by Algorithm 1. Then

f (yN ) − f ∗ ≤ cMp y0 − x∗ p2+1 , c = 2 3(p+14)2+4 (p + 1) .

N 3p+1 2

p!

Note that this bound allows to obtain an O

Mp Rp+1 ε

2 3p+1

iteration complexity. The imple-

mentability and cost of each iteration is discussed below in Section 2.3. The proof of Theorem 1 is based on the framework of Monteiro and Svaiter (2013), which is presented in the next subsection.

Algorithm 1 Optimal Tensor Method

Input: u0, y0 — starting points; N — iteration number; A0 = 0 Output: yN

1: for k = 0, 1, 2, . . . , N − 1 do

2: Choose Lk such that

1 ≤ 2(p + 1)Mp yk+1 − xk p−1 ≤ 1,

(5)

2

p!Lk

2

where

1/Lk + ak+1 =

1 L2k + 4Ak/Lk 2,

Ak+1 = Ak + ak+1,

{note that Lka2k = Ak+1}

xk = Ak yk + ak+1 uk,

Ak+1

Ak+1

yk+1 = TpF,pLMk,xpk (xk).

3: uk+1 = uk − ak+1∇f (yk+1) 4: end for 5: return yN

4

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

2.1. Accelerated hybrid proximal extragradient method
Monteiro and Svaiter (2013) introduced Algorithm 2 for convex optimization problems. To ﬁnd yk+1 on each iteration, the authors use gradient type method for the case p = 1 and a trust region Newton-type method for the case p = 2. Their analysis of the algorithm is based on the following Theorem.

Theorem 2 ( (Monteiro and Svaiter, 2013, Theorem 3.6 ) ) Let sequence (xk, yk, uk), k ≥ 0 be generated by Algorithm 2 and deﬁne R := y0 − x∗ 2. Then, for all N ≥ 0,

12 uN − x∗ 22 + AN · f yN − f (x∗) + 41 N AkLk−1 yk − xk−1 22 ≤ R22 , (6)
k=1

f yN − f (x∗) ≤ R2 , uN − x∗ ≤ R,

(7)

2AN

2

N

2

AkLk−1 yk − xk−1 ≤ 2R2.

(8)

2

k=1

We also need the following Lemma.

Lemma 3 ( (Monteiro and Svaiter, 2013, Lemma 3.7 a))) Let sequences {Ak, Lk}, k ≥ 0 be generated by Algorithm 2. Then, for all N ≥ 0,

1N

1

2

AN ≥

.

(9)

4 k=1 Lk−1

Algorithm 2 Accelerated hybrid proximal extragradient method

Input: u0, y0 — starting point; N — iteration number; A0 = 0 Output: yN

1: for k = 0, 1, 2, . . . , N − 1 do 2: Choose Lk and yk+1 s.t. ∇FLk,xk yk+1

2

≤

Lk 2

yk+1 − xk

2, where

1/Lk + ak+1 =

1 L2k + 4Ak/Lk 2,

Ak+1 = Ak + ak+1,

xk = Ak yk + ak+1 uk.

Ak+1

Ak+1

3: uk+1 = uk − ak+1∇f yk+1 . 4: end for 5: return yN

2.2. Proof of Theorem 1 It follows from Algorithm 1 that yk+1 = TpF,pLMk,xpk (xk), thus by (Nesterov, 2018, Lemma 1),

∇FLk,xk yk+1

≤ (p + 1) Mp yk+1 − xk p .

2

p!

2

5

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

At the same time, by the condition in step 2 of Algorithm, 1,

2(p + 1)Mp yk+1 − xk p−1 1.

p!Lk

2

Hence,

∇FLk,xk yk+1

≤ Lk yk+1 − xk

22

2

and we can apply the framework of the previous subsection. What remains is to estimate the growth

of AN , which is our next step.

By the condition in step 2 of Algorithm, 1,

1 yk+1 − xk p−1 ≥ θ,

(10)

Lk

2

where θ = 4(p+p1!)Mp . Using this inequality, we prove that

N

p+1

AkLkp−−11 ≤ 2R2θ− p−2 1 .

k=1

Indeed, from (8) and (10) we have that

N
2

p+1

N

1+ 2

θ p−1

AkLkp−−11 ≤

Ak Lk−1p−1

k=1

k=1

2

1 yk − xk−1 p−1 p−1

Lk−1

2

N
= AkLk−1
k=1

yk − xk−1

2 ≤ 2R2.
2

Further, from (11) it follows that

N k=1

1

1 ≥ θ p+1

Lk−1

p−1
(2R2) 2(p+1)

N

p−1

A 3p+1
k

k=1

3p+1 2(p+1)
.

(11) (12) (13)

To prove that, let us introduce new variables zk = 1 Lk−1 and consider the following optimization problem to ﬁnd the worst possble value of the l.h.s. in (13)

N

N

min zk s.t.

Akzk−γ ≤ C,

(14)

k=1

k=1

where in accordance with (11)

p+1 γ = 2p − 1,

C

=

2R

2

θ

−

2 p−1

.

Since the objective and constraints are separable, this problem can be solved explicitly by the La-

grange principle



1/γ

1N

1

1

zk =  C Ajγ+1  Akγ+1 .

j=1

6

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Hence,

N

1

min
N

zk = C1/γ

Akzk−γ ≤C k=1

k=1

From this inequality, (9) and (13), we have

N

1

Akγ+1

k=1

γ+1 γ
.

2

AN ≥ 1 θ p+1

4

(2R2)

p−1 p+1

N

p−1

A 3p+1
k

k=1

3p+1 p+1
.

(15)

From this inequality, we obtain that there exists a number c such that, for all N ≥ 0,

AN ≥

1 N 3p2+1 .

(16)

cMpRp−1

The derivation of exact value of the constant c can be found in Lemma 5 in Appendix. This ﬁnishes the proof.

2.3. Implementation details
First of all, Theorem 1 in Nesterov (2018) says that, by the appropriate choice M = pMp in (3), the subproblem for ﬁnding yk+1 in step 2 of Algorithm 1 is convex and, thus is tractable. Moreover, for p = 2 this step corresponds to the step of cubic regularized Newton method of Nesterov and Polyak (2006) and, as it is shown there, can be computed with the same complexity as solving a linear system. For the case p = 3, Nesterov (2018) showed that this step can be also computed efﬁciently. In both cases the complexity of calculating yk+1 is O˜ n2.37 .
Let us now discuss the process of ﬁnding such Lk that the inequality (5) holds. By construction,

 yk+1 = arg min  p 1 ∇rf xk
y∈Rn r=0 r!

y − xk, ..., y − xk
r

+ pMp (p + 1)!

y − xk p+1 + Lk

2

2







y − xk

2 2

.

 

This problem is strongly convex and, thus, has a unique solution for each Lk > 0. Hence, yk+1 is uniquely deﬁned by Lk. At the same time, if Lk → 0, yk+1 → y˜k with



p  y˜k ∈ Arg min

1 ∇rf

xk

y∈Rn r=0 r!

being a ﬁxed point. Whence,

y − xk, ..., y − xk
r

+ pMp (p + 1)!


y − xk p+1 2 

2(p + 1)Mp yk+1 − xk p−1 → +∞.

p!Lk

2

On the other hand, if Lk → +∞, yk+1 → xk and

2(p + 1)Mp yk+1 − xk p−1 → 0.

p!Lk

2

7

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

By the continuity of the dependence of yk+1 from Lk, we see that there exists such Lk that inequality (5) holds. Appropriate value of Lk can be found by an extended line-search procedure as in (Monteiro and Svaiter, 2013, Section 7). The details of complexity of the line-search can be found in Jiang et al. (2018); Bubeck et al. (2018), where the authors prove a bound of O˜(1) calls of
TpF,pLMk,xpk (xk) on each iteration.

3. Extension for Uniformly Convex Case
In this section, we additionally assume that the objective function is uniformly convex of degree q ≥ 2, i.e., there exists σq > 0 s.t.
f (y) ≥ f (x) + ∇f (x), y − x + σqq y − x q2, ∀x, y ∈ Q. (17)
We also assume that q ≤ p + 1. As a corollary,
f (y) ≥ f (x∗) + σqq y − x∗ q2, ∀y ∈ Q, (18)
where x∗ is a solution to problem (1). We show, how the restart technique can be used to accelerate Algorithm 1 under this additional assumption.

Algorithm 3 Restarted Optimal Tensor Method
Input: p, Mp, q, σq, z0, ∆0 s.t. f (z0) − f ∗ ≤ ∆0. 1: for k = 0, 1, ... do
2:



2 



p+1

 3p+1

Set ∆k = ∆0 · 2−k and Nk = max  2cMpp+q1 q ∆kp+q1−q 

   , 1 . (19) 

 


σq q

 


3: Set zk+1 = yNk as the output of Algorithm 1 started from zk and run for Nk steps. 4: Set k = k + 1.
5: end for Output: zk.

Theorem 4 Let sequence zk, k ≥ 0 be generated by Algorithm 3. Then σqq zk − x∗ q2 ≤ f (zk) − f ∗ ≤ ∆0 · 2−k,
and the total number of steps of Algorithm 1 is bounded by (c is deﬁned in (16))

p+1
2cq q

2
M (∆ ) · 2 + k. 3p+1

2 3p+1
p
2(p+1)

2(p+1−q)
0 q(3p+1)

k 2(p+1−q) −i q(3p+1)

σ q(3p+1)
q

i=0

8

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Proof Let us prove the ﬁrst statement of the Theorem by induction. For k = 0 it holds. If it holds for some k ≥ 0, by the choice of Nk, we have that

cMp
3p+1
Nk 2

q∆k σq

p+1
q ≤ ∆k . 2

By (18),

p+1

p+1

zk − x∗ p+1 ≤ q(f (zk) − f ∗)

q
≤

q∆k

q

2

σq

σq

since, by our assumption, q ≤ p + 1. Combining the above two inequalities and Theorem 1, we

obtain

f (z

) − f∗ ≤

cMp

zk − x∗

p+1 2

≤

∆k

=∆

.

k+1

3p+1
N2

2

k+1

k

2

It remains to bound the total number of steps of Algorithm 1. Denote c˜ =

p+1
2cq q

3p+1 .

2

k

M 3p+1
p

Ni ≤ c˜ 2(p+1)

i=0

σ q(3p+1)
q

2
k (∆0 · 2−i) 2q((p3+p1+−1q)) + k ≤ c˜ M2p(3pp++11) (∆0) 2q((p3+p1+−1q)) · k 2−i 2q((p3+p1+−1q)) + k.

i=0

σ q(3p+1)
q

i=0

Let us make several remarks on the complexity of the restarted scheme in different settings. It is easy to see from Theorem 4 that, to achieve an accuracy ε, i.e. to ﬁnd a point xˆ s.t. f (xˆ) − f ∗ ≤ ε, the number of tensor steps in Algorithm 3 is



2



O  M2p(3pp++11) (∆0) 2q((p3+p1+−1q)) + log2 ∆ε0  , q < p+1, and σ q(3p+1)
q

 2 M 3p+1
p
O  2(p+1)
σ q(3p+1)
q





+ 1 log2 ∆ε0  , q = p+1.

Theorem 4 suggests a natural generalization of ﬁrst- and second-order condition number Nesterov

(2008). If f is such that q = p+1, then the complexity of Algorithm 3 depends only logarithmically

on

the

starting

point

and

is

proportional

to

(γp

)

2 3p+1

,

where

γp

=

Mp

is the p-th order condition

number. Unfortunately, if q < p + 1, the complexity depends polinσop+m1ially on the initial objective

residual ∆0, which, in general, is not controlled.

An interesting special case is when q = 2 and p ≥ 2, and, as a consequence, q < p + 1.

As it can be seen from Theorem 2 (see also Bubeck et al. (2018)), the sequence, generated by

Algorithm 1 is bounded by some R = O( x0 − x∗ 2). Hence, the constant M2 can be estimated

as M2 ≤ MpRp−2. At the same time, in (Nesterov, 2008, Sect.6), it is shown that the Cubic

regularized Newton method Nesterov and Polyak (2006) has the region of quadratic convergence

given by {x : f (x) − f ∗ ≤ 2σM2222 ≤ 2Mp2Rσ222(p−2) }. To enter this region, Algorithm 3 requires

2



2



M 3p+1
p

p−1

∆0Mp2R2(p−2)

M 3p+1
p

p−1

Mp2∆p0−1

O  p+1 (∆0) 3p+1 + log2

σ2

 = O  p+1 (∆0) 3p+1 + log2 σp  ,

σ 3p+1
2

2

σ 3p+1
2

2

(20)

9

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

where we used inequality R2

≤

2∆0 σ

,

which

follows

from

(18).

After entering the region of

quadratic convergence, Algorithm 3 ca2n be switched to the Cubic regularized Newton method Nes-

terov and Polyak (2006), which has ﬁnal stage complexity, (Nesterov and Polyak, 2006, Sect. 6)

O log log σ23 = O log log

σ23

.

3/2 4 M22ε

3/2 4 Mp2R2(p−2)ε

Summing this inequality and (20) we obtain the total complexity of this switching procedure to obtain small accuracy ε. Note, that the second term in (20) is typically dominated by the ﬁrst one, so we can ignore it without loss of generality.
Finally, let us compare our upper bound with known lower bounds. For the case p = 1, q = 2, our complexity bound coincides with lower bound for ﬁrst-order methods Nemirovsky and Yudin (1983); Nesterov (2004). Arjevani et al. (2018) propose lower bounds for second-order methods for the case p = 2, q = 2 and our complexity bound coincides with their lower bound up to a change
of D = ∆σ20 , which is natural as, in this case f is strongly convex.

4. Numerical Analysis
In this section, we analyze and compare the performance of Algorithm 1 with the accelerated tensor method proposed in Nesterov (2018).
We study the numerical performance for two classes of functions. Initially, an universal parametric family of objective functions, which are difﬁcult for all tensor methods Nesterov (2018) deﬁned as

fm(x) = ηp+1 (Amx) − x1,

(21)

n

where, for integer parameter p

≥

1, ηp+1(x)

=

1 p+1

|xi|p+1, 2 ≤ m ≤ n, x ∈ Rn, Am is the

n × n block diagonal matrix: i=1

 1 −1 0 . . . 0   0 1 −1 . . . 0  Am = U0m In0−m , with Um =  ... ... . . . ...  , (22)  0 0 . . . 1 −1 
0 0 ... 0 1

and In is the identity n × n-matrix. For a detailed description of the high-order derivatives of this class of functions, and its optimality properties see Nesterov (2018).
Figure 1 shows the normalized optimality gap of the iterations generated by the accelerated tensor method from Nesterov (2018) in Figure1(a), and Algorithm 1 in Figure1(b). We denote the minimum function value as f ∗. For both results we have used p = 3, and n = k = {5, 10, 15, 20, 25}. These numerical results show that Algorithm 1 requires a much smaller number of iterations than the accelerated tensor method from Nesterov (2018) to reach the same optimality gap, namely 1 · 10−15, for the class of “bad” functions described in Nesterov (2018). For example, for the case where n = k = 25, Algorithm 1 has reached the desired accuracy in about 100 iterations, while the accelerated tensor method requires about 1 · 104.

10

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

10−1

(Nesterov,2018)

n=k=5 n = k = 15 n = k = 25

n = k = 10 n = k = 20

10−1

Algorithm 1

|f (xk)−f ∗| |f (x1)−f ∗| |f (xk)−f ∗| |f (x1)−f ∗|

10−8

10−8

10−15 0

0.2 0.4 0.6 Iterations
(a)

0.8 1 ·104

10−15

n=k=5 n = k = 15 n = k = 25

n = k = 10 n = k = 20

20 40 60 80 100 Iterations
(b)

Figure 1: A performance comparison between the accelerated tensor method in Nesterov (2018) (shown in (a)) and Algorithm 1 (shown in (b)). We minimize an instance of the family of functions in (21) with p = 3 and various values of dimension n and k. Note that the x-axis scaling on both ﬁgures is different.

As a second set of numerical results we study the performance of the proposed method for the non-regularized logistic regression problem. For this problem we are given a set of d data pairs {yi, wi} for 1 ≤ i ≤ d, where yi ∈ {1, −1} is the class label of object i, and wi ∈ Rn is the set of features of object i. We are interested in ﬁnding a vector x that solves the following optimization problem

1 d ln 1 + exp −y w , x → min .

(23)

d

ii

x∈Rn

i=1

Figure 2 shows the simulation results for the logistic regression problem in (23) for various datasets. Similarly as in Figure 1, we compare the performance of Algorithm 1, and the accelerated tensor method in Nesterov (2018). In Figure 2(a) and Figure 2(b), we generate synthetic data, where, initially we deﬁne a vector xˆ ∈ [−1, 1] with every entry is chosen uniformly at random. The set of features for each i, i.e., wi ∈ [−1, 1]n has also every entry chosen uniformly at random, ﬁnally each label is computed as yi = sign( wi, xˆ ). For Figure 2(a) we set n = 10 and d = 100, while in Figure 2(b) we set n = 100 and d = 1000. Figure 2(c) uses the mushroom dataset (n = 8124 and d = 112) Dheeru and Karra Taniskidou (2017), and Figure 2(d) uses the a9a dataset (n = 32561 and d = 123) Dheeru and Karra Taniskidou (2017).
For the logistic regression problem, we don’t have access to the optimal value function in general, thus, we plot only the cost function evaluated at the current iterate. As expected by the theoretic results, Algorithm 1 requires one order of magnitude less iterations than the accelerated tensor method from Nesterov (2018) to achieve the same function value.
In Appendix B, we numerically compare the performance of the accelerated tensor method from Nesterov (2018) for p = 2 and p = 3, as well as its accelerated and non-accelerated versions.

11

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Synthetic n = 10, d = 100 0.8

(Nesterov,2018)
Synthetic n = 100, d = 1000 0.8

Algorithm 1
mushroom dataset 0.8

a9a dataset 0.8

f (xk) f (xk) f (xk) f (xk)

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.2
0 101 102 103 104 Iterations
(a)

0.2
0 101 102 103 104 Iterations
(b)

0.2
0 101 102 103 104 Iterations
(c)

0.4
101 102 103 Iterations
(d)

Figure 2: Performance comparison for the non-regularized logistic regression problem between the accelerated tensor method from Nesterov (2018) and Algorithm 1. (a) Uses synthetic data with n = 10 and d = 100, (b) uses synthetic data with n = 100 and d = 1000, (c) uses the mushroom dataset (d = 8124 and n = 112) Dheeru and Karra Taniskidou (2017), and (d) uses the a9a dataset (d = 32561 and n = 123) Dheeru and Karra Taniskidou (2017).

Acknowledgments
The authors are grateful to Yurii Nesterov for fruitful discussions. The work of A. Gasnikov was supported by RFBR 18-29-03071 mk and was prepared within the framework of the HSE University Basic Research Program and funded by the Russian Academic Excellence Project ’5-100’, the work of P. Dvurechensky and E. Vorontsova was supported by RFBR 18-31-20005 mol-a-ved and the work of E. Gorbunov was supported by the grant of Russian’s President MD-1320.2018.1
References
Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In Se´bastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774–792. PMLR, 06–09 Jul 2018. URL http://proceedings.mlr.press/v75/agarwal18a. html.
Yossi Arjevani, Ohad Shamir, and Ron Shiff. Oracle complexity of second-order methods for smooth convex optimization. Mathematical Programming, May 2018. ISSN 1436-4646. doi: 10. 1007/s10107-018-1293-1. URL https://doi.org/10.1007/s10107-018-1293-1.
Michel Baes. Estimate sequence methods:extensions and approximations. Technical report, 2009. URL http://www.optimization-online.org/DB_FILE/2009/08/2372.pdf.
E. G. Birgin, J. L. Gardenghi, J. M. Mart´ınez, S. A. Santos, and Ph. L. Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models. Mathematical Programming, 163(1):359–368, May 2017. ISSN 1436-4646. doi: 10.1007/ s10107-016-1065-8. URL https://doi.org/10.1007/s10107-016-1065-8.
12

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION
Se´bastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Near-optimal method for highly smooth convex optimization. arXiv:1812.08026, 2018.
Coralia Cartis, Nicholas I. M. Gould, and Philippe L. Toint. Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models. arXiv:1708.04044, 2018.
Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine learning repository, 2017. URL http: //archive.ics.uci.edu/ml.
K. H. Hoffmann and H. J. Kornstaedt. Higher-order necessary conditions in abstract mathematical programming. Journal of Optimization Theory and Applications, 26(4):533–568, Dec 1978. ISSN 1573-2878. doi: 10.1007/BF00933151. URL https://doi.org/10.1007/ BF00933151.
Bo Jiang, Haoyue Wang, and Shuzhong Zhang. An optimal high-order tensor method for convex optimization. arXiv:1812.06557, 2018.
R. Monteiro and B. Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092–1125, 2013. doi: 10.1137/110833786. URL https://doi.org/10.1137/ 110833786.
A.S. Nemirovsky and D.B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. J. Wiley & Sons, New York, 1983.
Yu. Nesterov. Accelerating the cubic regularization of newton’s method on convex problems. Mathematical Programming, 112(1):159–181, Mar 2008. ISSN 1436-4646. doi: 10.1007/ s10107-006-0089-x. URL https://doi.org/10.1007/s10107-006-0089-x.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372–376, 1983.
Yurii Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.
Yurii Nesterov. Implementable tensor methods in unconstrained convex optimization. Technical report, CORE UCL, 2018. URL https://alfresco.uclouvain.be/ alfresco/service/guest/streamDownload/workspace/SpacesStore/ aabc2323-0bc1-40d4-9653-1c29971e7bd8/coredp2018_05web.pdf. CORE Discussion Paper 2018/05.
Yurii Nesterov and Boris Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006. ISSN 1436-4646. doi: 10.1007/ s10107-006-0706-8. URL http://dx.doi.org/10.1007/s10107-006-0706-8.
Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351– E7358, 2016.
13

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Optimal Tensor Methods in Smooth Convex and Uniformly Convex Optimization: Supplementary Material

Appendix A. Technical lemmas

Lemma 5 Consider the sequence {Ak}k≥0 of non-negative numbers such that

2

AN ≥ 1 θ p+1

4

(2R2)

p−1 p+1

N

p−1

A 3p+1
k

k=1

3p+1 p+1
,

(24)

where p ≥ 3, θ = 4(p+p1!)Mp and Mp, R > 0. Then for all N ≥ 0 we have

Ak ≥

1 k 3p2+1 ,

(25)

cMpRp−1

where

3(p+1)2 +4

c = 2 4 (p + 1)

(26)

p!

Proof We prove (25) by induction. For k = 1 we have

(24) 1

θ2 p+1

p−1

A1

≥

4

p−1
(2R2) p+1

A1p+1

2
⇐⇒ A1p+1

2 p+1

≥ 1 θ p−1 4 2 R p+1

2(p−1) p+1

⇐⇒ A1 ≥

p! .

2

3p+5 2

(p

+

1)MpRp−1

The last inequality implies (25) for p ≥ 3. Now let us assume that for all k ≤ N inequality (25) holds and N ≥ 1. Next we will establish (25) for k = N + 1. We have

(24) 1

θ2 p+1

AN +1

≥

4

(2R2)

p−1 p+1

2

≥ 1 θ p+1

4

(2R2)

p−1 p+1

(25) 1

θ2 p+1

≥

4

(2R2)

p−1 p+1

N +1 p−1
A 3p+1
k
k=1

N

p−1

A 3p+1
k

k=1

3p+1 p+1
3p+1 p+1

1 cMpRp−1

k p−1 N
3p+1

p−1 2

k=1

3p+1 p+1
.

If N = 1 then

2

AN+1 = A2 ≥ 1

θ p+1

3p+1

p−1

2 2 (2R2) p+1

If N > 1 we can write

1 cMpRp−1

p−1

p+1

3p+1

(2) 2 .

(27)

2

AN+1 ≥ 1 θ p+1

4

(2R2)

p−1 p+1

3p+1

1 p−1 p+1

N

p+1

p−1

cMpRp−1

1+ k 2

.

k=2

(28)

14

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

Since

p−1 2

≥

1

the

function

f (x)

=

x

is

convex

and,

as

a

consequence,

we

get

N
p−1

N
p−1

2

p+1

2

2

p+1 1

k 2 ≥ x 2 dx = p + 1 N 2 − p + 1 ≥ p + 1 N 2 − 2 .

k=2

1

(29)

Using this fact we continue:

(29) 1

θ2 p+1

AN +1

≥

4

(2R2)

p−1 p+1

2

≥ 1 θ p+1

4

(2R2)

p−1 p+1

1 cMpRp−1
1 cMpRp−1

p−1
p+1 1 + N p+2 1 2

p−1

N . p+1

3p+1 2

3p+1 p+1

For all N > 1 we have

N N +1

3p+1
2
=

1− 1 N +1

From this and (28) we obtain that for all N ≥ 1

3p+1 2
≥

1− 1 2

3p+1

2

1

= 3p+1 .

22

2

AN+1 ≥ 1

θ p+1

3p+1

p−1

2 2 (2R2) p+1

It remains to show that (26) implies

1 cMpRp−1

p−1

p+1

3p+1

(N + 1) 2 .

2

1

θ p+1

3p+1

p−1

2 2 (2R2) p+1

1 cMpRp−1

p−1

p+1

1

= cMpRp−1 .

Using θ = 4(p+p1!)Mp we get

1 2 3p+1
2

θ2 p+1 p−1
(2R2) p+1

1 p−1 p+1
cMpRp−1

2

3p+1

⇐⇒ c p+1 = 2 2

=

1

2
⇐⇒ c p+1

1

cMpRp−1

2 3p+1 2

p! 4(p + 1)

2
p+1 1 p−1 = 1
2 p+1

4(p + 1) p!

2

p+1

2 p−1 p+1

⇐⇒

c

=

2 (3p+1)(p+1) 4

4(p

+

1) 2 p−2 1

p!

3(p+1)2 +4
⇐⇒ c = 2 4 (p + 1) , p!

which is exactly what we have in (26).

Appendix B. Comparison of the accelerated tensor method from Nesterov (2018) for p = 2 and p = 3.
In this appendix, we numerically compare the performance of the accelerated tensor method proposed in (Nesterov, 2018), for p = 2 and p = 3. We also compare the accelerated and nonaccelerated version of this method.
15

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION

fm(x) in (21), m = 50, n = 100 1

0.95

|f (xk)−f ∗| |f (x1)−f ∗|

0.9 0.85 0.8

p = 2 Accelerated p=2
p = 3 Accelerated p=3
10 20 30 40 50 Iterations

Figure 3: Performance of tensor methods and accelerated tensor methods for p = 2 and p = 3 on a difﬁcult instance (21) for all unconstrained minimization tensor methods with n = 100 and m = 50.

Similarly as in Figure 1 and Figure 2, we present the numerical results for the class of bad functions deﬁned in (21) and one instance of the logistic regression problem.
In Figure 3, we compare the behavior of the following methods: 1) tensor method Nesterov (2018) for p = 3; 2) accelerated tensor method Nesterov (2018) for p = 3; 3) tensor method Nesterov (2018) for p = 2; 4) accelerated tensor method Nesterov (2018) for p = 2. Again, the optimal function value is denoted by f ∗. Interestingly, we obtain that the non-accelerated method outperforms the accelerated method for the ﬁrst m iterations. Since Theorem 4 from Nesterov (2018) works only for k ≤ m we don’t study the behaviour of the methods for larger number of iterations. Even in this simple setting it is still non-trivial how to implement tensor methods for such bad examples of functions.

Covertype dataset

0.8
p = 2 Accelerated

p = 3 Accelerated

p=2

0.6

p=3

f (xk)

0.4
20 40 60 80 100 Iterations
Figure 4: Function value achieved by the iterates of the accelerated tensor method for the logistic regression problem on the Covertype dataset Dheeru and Karra Taniskidou (2017). Number of samples d = 20000, dimension n = 55.
16

OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION In Figure 4, we consider the behaviour of the same set of methods as in Figure 3, but for logistic regression problem deﬁned in (23) on Covertype dataset Dheeru and Karra Taniskidou (2017). And again, we notice that in both cases non-accelerated version works better in our experiments First of all, we point out that tensor methods in general are non-trivial in implementation, so, it is interesting direction of the future work to get better implementation. Secondly, we conjecture that slow convergence that we see in our experiments is because of large Mp that we use. Due to tuning of the parameters one can obtain better convergence in practice.
17

