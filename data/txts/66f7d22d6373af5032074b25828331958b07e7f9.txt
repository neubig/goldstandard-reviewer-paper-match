When Differential Privacy Meets Interpretability: A Case Study

arXiv:2106.13203v2 [cs.CV] 25 Jun 2021

Rakshit Naidu * 1 2 3 Aman Priyanshu * 1 Aadith Kumar 1 4 Sasikanth Kotti 3 5 Haofan Wang 2 3 Fatemehsadat Mireshghallah 6 3

Abstract
Given the increase in the use of personal data for training Deep Neural Networks (DNNs) in tasks such as medical imaging and diagnosis, differentially private training of DNNs is surging in importance and there is a large body of work focusing on providing better privacy-utility trade-off. However, little attention is given to the interpretability of these models, and how the application of DP affects the quality of interpretations. We propose an extensive study into the effects of DP training on DNNs, especially on medical imaging applications, on the APTOS dataset.
1. Introduction
The application and development of Machine Learning (ML) in the ﬁeld of medicine and health has grown exponentially. With the recent advances in employing AI for health, one can see the great potential it holds (Priyanshu & Naidu, 2021; Kaissis et al., 2021). However, healthcare data contains sensitive information which must be handled under security protocols, which protect subject privacy. At the same time, model results and predictions must be interpretable allowing medical experts involved to study and validate the evaluations (Suriyakumar et al., 2021; Koker et al., 2021). This clearly identiﬁes a problem within computer vision, where both accountability and privacy must be addressed for certain use-cases.
DP (Dwork & Roth, 2014; Abadi et al., 2016) is deﬁned as an extensive tool that constitutes strong privacy guarantees for algorithms on a given data distribution by providing the overall patterns within the dataset while withholding information about individuals. Interpretable Machine learning is deﬁned as a collection of algorithmic implements, which allow the user to understand how the model arrived at a speciﬁc decision. These two technologies, if correctly lever-
*Equal contribution 1Manipal Institute of Technology 2Carnegie Mellon University 3OpenMined 4University of Pennsylvania 5IIT Jodhpur 6University of California, San Diego. Correspondence to: Rakshit Naidu <rnemakal@andrew.cmu.edu>.

aged can further reﬁne the present-day implementations and make them more practical, adding dimensions of trust, corrective feedback and contestability of claim to black-box models.
There are different models of applying differential privacy, based on where the “privacy barrier” is set, and after which stage in the pipeline we need to provide privacy guarantees (Mirshghallah et al., 2020; Bebensee, 2019), as shown in Figure 1. (1) Local DP is comprised of applying noise directly to the user data. This way, the data itself can be shared with untrusted parties and the leakage is bounded by the privacy budget ε. (2) Global DP, which is based on the assumption that there exists a trusted party, a centralized server, which collects the data, and then applies differentially private learning algorithms on the collected data to produce models or analysis with bounded information leakage. A prominent one of such algorithms is DP-SGD (Abadi et al., 2016; Chaudhuri et al., 2011), which consists of clipping gradients and adding noise at each step of the optimization. Due to this addition of noise, interpreting differentially private models is more difﬁcult than conventionally trained ones (Patel et al., 2020).
In this paper, we set out to explore this problem of interpreting differentially private models, in health use-cases, more extensively. We provide the ﬁrst benchmark of exploring interpretability, speciﬁcally through class activation maps, in DNNs trained using DP. We train DP-DNNs with a wide range of privacy budgets in both local and global DP settings, to study the effects they have on model interpretability. We utilize Grad-CAM (Selvaraju et al., 2016) as our interpretability method and use the Cats vs Dogs and APTOS (APT) datasets, to train our models in both general and medical setups. We show how different levels of privacy budget (ε) effect the interpretablity and utility of the model, and explore this three dimensional trade-off space for local vs. global application of DP.
2. Related Work
2.1. Differential Privacy
Deﬁnition 1: Given a randomized mechanism A and any two neighboring datasets D1, D2 (i.e. they differ by a single

When Differential Privacy Meets Interpretability: A Case Study

Patient Data

Local DP

Train Model

Trained Model
(a) Local Differential Privacy

Interpret

Train Model w/ DP-SGD

Interpret

Patient Data

Trained Model

(b) Global Differential Privacy with DP-SGD

Figure 1. We benchmark two application schemes of differential privacy: (a) local DP where noise is directly added to the data samples and (b) global DP where the raw data is collected and the noise is added during the training/analysis in a centralized manner.

individual data element), A is said to be (ε, δ)-differentially private for any subset S ⊆ R (where R denotes the range).
Pr [A (D1) ∈ S] ≤ eε · Pr [A (D2) ∈ S] + δ (1)

Here, ≥ 0, δ ≥ 0. A δ = 0 case corresponds to pure differential privacy, while both = 0, δ = 0 leads to an inﬁnitely high privacy domain. Finally, = ∞ provides no privacy (non-DP) guarantees.
The privacy in DP models can be quantiﬁed with parameters such as epsilon (ε) and delta (δ). DP-SGD (Abadi et al., 2016) adds noise to the gradients at each step during training using a clipping factor (S) and noise multiplier (z). The amount of noise added to the model can be linked to to the privacy level that the model can achieve. Theoretically, a lower value of signiﬁes a higher privacy level and this increased privacy degree is understandably, achieved at the expense of model performance due to the addition of the noise. The practical implication of this, however, includes a direct impact on the model performance and hence, its interpretability. Reduced privacy requirements allow the addition of noise where models can achieve appropriate performance without any signiﬁcant resource expense. On the other hand, high privacy standards add a signiﬁcantly large magnitude of noise which may lead to a decrease in the model interpretations. Further, noise addition may even lead to the non-convergence of some models in the worst case. In the scope of this paper, we take δ = 10−4 and experiment on various privacy levels = 0.5, 1, 5, 10.

2.2. Interpretability
In recent years, Convolutional Neural Networks (CNNs) have been involved in progressing major vision tasks such as image captioning (Huang et al., 2019), image classiﬁcation (Sultana et al., 2019) and semantic segmentation (Liu et al., 2018), to name a few. Despite these advancements, CNNs are treated as black-box architectures when applied to sensitive environments.
To solve explainability measures in CNNs, Class Activation Mappings (CAMs) was proposed for interpreting image

classiﬁcation tasks by Zhou et al.. CAMs refer to the linear combination of the weights and the activation maps produced by the Global Average Pooling (GAP) layer. In the scope of this paper, we utilize Grad-CAM (Selvaraju et al., 2016) for producing explanation maps. Grad-CAM is denoted by :

LcGrad−CAM = ReLU

αck Ak

(2)

k

where αck represents the neuron importance weights. αck =

1 Z

∂∂AYkc where Yc is the score computed for the tar-

ij

ij

get class, (i, j) represents the location of the pixel and Z

denotes the total number of pixels.

3. Evaluations
We evaluate on two datasets : APTOS and the Cats vs Dogs dataset. We use DP-SGD with a learning rate of 0.0001 and a batch size of 32 over the Cats vs Dogs dataset for 10 epochs. We utilize the APTOS Local DP (LDP) datasets as demonstrated by Singh & Sikka.
In Fig 2, we observe that as increases (degree of privacy decreases), we approach explanations with better quality.
It is well known that Gaussian distribution follows approximate DP (Balle & Wang, 2018) while the Laplacian dis-
1 tribution (with a parameter of ) satisﬁes -DP (Dwork &
Roth, 2014). Fig 3 shows test accuracies on both Gaussian and Laplacian noise distributions on DP-SGD. We notice that the gap between = 1 and = 5 is quite large (almost 25%) which can be explained by the decrease in variance or “spread” over which the noisy values are sampled.
We evaluate interpretability on DP-models using two metrics that were introduced by Chattopadhay et al. :

• Average Drop %: The Average Drop refers to the maximum positive difference in the predictions made by the prediction using the input image and the prediction using the saliency map. It is given as: Ni=1 max(0Y,Yicic−Oic) × 100. Here, Yic refers to the prediction score on class c using the input image i and Oic refers to the prediction score on class c using the saliency map produced over the input image i.
• Increase in Conﬁdence %: The Average Increase in Conﬁdence is denoted as: Ni=1 F un(YNic<Oic) × 100 where Fun refers to a boolean function which returns 1 if the condition inside the brackets is true, else the function returns 0. The symbols are referred to as shown in the above experiment for Average Drop.

When Differential Privacy Meets Interpretability: A Case Study

Metrics Average Drop (lower the better) % Average Inc (higher the better) % Average normalized input scores (higher the better) %

= 0.5 13.29 67.52 75.27

=1 11.66 62.50 79.10

=5 6.76 84.96 87.88

= 10 5.81 86.8 89.40

=∞ 2.05 95.04 97.22

Table 1. Results on VGG-16 network on the Cats vs Dogs dataset with Gaussian noise (approximate DP) in DP-SGD.

Metrics Average Drop (lower the better) % Average Inc (higher the better) % Average normalized input scores (higher the better) %

= 0.5 – – –

=1 9.61 66.84 86.48

=5 4.39 90.96 91.78

= 10 4.18 92.44 92.61

=∞ 2.05 95.04 97.22

Table 2. Results on VGG-16 network on the Cats vs Dogs dataset with Laplacian noise (pure DP) in DP-SGD. Note that = 0.5 results are not displayed as the model here is too private to infer relevant features.

Tables 1 and 2 showcase results of these metrics on the Cats vs Dogs dataset. We notice a consistent decrease with Average Drop and steady increase (except in the case of = 1) with Average Increase in Conﬁdence experiments. The ﬁnal metric displayed is the average scores on the point-wise multiplied input with the explanation maps which increases with the value of , as expected. The same metric is graphically shown in Fig 6a for DP-SGD with APTOS.
ResNet18 (Singh & Sikka, 2020) and AlexNet 1 are adopted in our work over the APTOS dataset. We compute the masked input via point-wise multiplication between the saliency map and the input, and calculate the average output scores of these masked inputs over the testing set. In Fig 6b, we observe the effect of S (clipping factor) on the output scores. As S2 increases (after S = 1), we notice a drastic decrease in accuracy with AlexNet. This result is quite intuitive as AlexNet is a much smaller network than the ResNet. Therefore, the effect of DP noise is more evident in AlexNet as it has ∼2M parameters while ResNet has ∼11M parameters (Trame`r & Boneh, 2020). We also notice signiﬁcant changes in the trends observed with Local DP as well (shown in 4a and 4b) with AlexNet probably due to the above reason. In Fig 6a, we notice a drop in accuracies in ε = 1 and a steep increase (nearly 15%) between ε = 10 and ε = ∞. This increase is attributed to the gap between having some privacy to no privacy.
In Fig 5, we see that the signiﬁcant visual difference in explanation arises from = 1 to = 5 in both 5a and 5b for both the classes (“cat” and “dog”). Fig 7 showcases DP-SGD test accuracy results on the APTOS dataset. We notice a substantial increase in a private model ( = 10) when compared to a non-private model ( = ∞).
1We train them using DP-SGD with 0.01 lr and 25 epochs. 2S ∝ σ

(a) ResNet18

(b) AlexNet
Figure 2. Grad-CAM Explanation maps on Local DP for APTOS dataset with Input image, = 0.25, 0.5, 1 (left to right).

Test Accuracy

100 Gaussian noise

Laplacian noise

75

50

25

0 0.5 1 5 10 Epsilon values

Figure 3. Comparison of Test accuracies on Cats vs Dogs with noise added in DP-SGD.

4. Conclusion & Future Work
In this work, we show how privacy could affect the overall interpretability in ML models. We quantitatively display results on different DP variants (local and global DP) on various privacy regimes. Our results (Figs 2, 4, 6, 5) portray promising directions in this area as transparency and accountability should complement ML models in order to understand how noise affects the training of DP-trained

When Differential Privacy Meets Interpretability: A Case Study

Average Drop

50 ResNet18

40

AlexNet

30

20

10 0.25

0.50

1.00

Epsilon values

(a) Average Drop (%)

100

90

ResNet18 AlexNet

80

70

60

50

40

30 0.25

0.50

1.00

Epsilon values

(b) Average Increase (%)

(a) Different ε Values (b) Different Clipping Factors

Average Increase

Figure 4. Average Drop and Average Increase in Conﬁdence scores on the APTOS dataset for Local DP. We notice that = 0.5 for AlexNet opposes the expected trend, probably due to the size of the network as AlexNet is a much smaller network when compared to ResNet18. Recent work by Trame`r & Boneh shows that models with lower parameters (smaller-sized models) exhibit signiﬁcant changes in accuracy when varying levels of DP noise is added.

Figure 6. Averaged accuracies of inputs masked with their explanations over AlexNet and ResNet networks for (a) different ε values and (b) different clipping factors (S) with ε = 1. We quantitatively show that there’s a signiﬁcant gap between privacy and explanation quality.

(a) Gaussian DP-SGD
(b) Laplacian DP-SGD Figure 5. Grad-CAM Explanation maps on “Cat” and “Dog” for Input image, = 0.5, 1, 5, 10, ∞ (left to right). Note that = 0.5 map for Laplacian variant is not displayed as the model here is too private to infer any relevant features. models. We observe that there are desirable points in the three dimensional trade-off space between privacy, utility

Test Accuracy

80 60 40 20 0 0.5

AlexNet

ResNet18

1Epsilon5value1s0

Figure 7. Comparison of Test accuracies on APTOS DP-SGD with ResNet18 and AlexNet.

and interpretablity, where all the three metrics can have meaningful values.
As future work, we hope to ﬁnd better heuristics for navigating this trade-off space and devise a framework for interpretability, catered to the characteristics of DP-trained models.

When Differential Privacy Meets Interpretability: A Case Study

References

Aptos 2019 diabetic retinopathy dataset.

URL

https://www.kaggle.com/c/

aptos2019-blindness-detection/data.

Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Oct 2016. doi: 10.1145/2976749.2978318. URL http://dx.doi.org/10.1145/2976749. 2978318.

Balle, B. and Wang, Y.-X. Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising, 2018.

Bebensee, B. Local differential privacy: a tutorial. arXiv preprint arXiv:1907.11908, 2019.

Chattopadhay, A., Sarkar, A., Howlader, P., and Balasubramanian, V. N. Grad-cam++: Generalized gradientbased visual explanations for deep convolutional networks. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), Mar 2018. doi: 10.1109/ wacv.2018.00097. URL http://dx.doi.org/10. 1109/WACV.2018.00097.

Chaudhuri, K., Monteleoni, C., and Sarwate, A. D. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3), 2011.

Dwork, C. and Roth, A. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3–4):211–407, August 2014. ISSN 1551-305X. doi: 10.1561/0400000042. URL https://doi.org/10. 1561/0400000042.

Huang, Y., Li, C., Li, T., Wan, W., and Chen, J. Image captioning with attribute reﬁnement. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 1820–1824, 2019.

Mirshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and Esmaeilzadeh, H. Privacy in deep learning: A survey. arXiv preprint arXiv:2004.12254, 2020.
Patel, N., Shokri, R., and Zick, Y. Model explanations with differential privacy. CoRR, abs/2006.09129, 2020. URL https://arxiv.org/abs/2006.09129.
Priyanshu, A. and Naidu, R. Fedpandemic: A cross-device federated learning approach towards elementary prognosis of diseases during a pandemic, 2021.
Selvaraju, R. R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Batra, D. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. CoRR, abs/1610.02391, 2016. URL http://arxiv.org/ abs/1610.02391.
Singh, S. and Sikka, H. Benchmarking differentially private residual networks for medical imagery. CoRR, abs/2005.13099, 2020. URL https://arxiv.org/ abs/2005.13099.
Sultana, F., Suﬁan, A., and Dutta, P. Advancements in image classiﬁcation using convolutional neural network. CoRR, abs/1905.03288, 2019. URL http://arxiv.org/ abs/1905.03288.
Suriyakumar, V. M., Papernot, N., Goldenberg, A., and Ghassemi, M. Chasing your long tails: Differentially private prediction in health care settings. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 723–734, 2021.
Trame`r, F. and Boneh, D. Differentially private learning needs better features (or much more data). CoRR, abs/2011.11660, 2020. URL https://arxiv.org/ abs/2011.11660.
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. Learning deep features for discriminative localization, 2015.

Kaissis, G., Ziller, A., Passerat-Palmbach, J., Ryffel, T., Usynin, D., Trask, A., Lima, I., Mancuso, J., Jungmann, F., Steinborn, M.-M., et al. End-to-end privacy preserving deep learning on multi-institutional medical imaging. Nature Machine Intelligence, 3(6):473–484, 2021.

Koker, T., Mireshghallah, F., Titcombe, T., and Kaissis, G. U-noise: Learnable noise masks for interpretable image segmentation. In 2021 IEEE International Conference on Image Processing (ICIP), September 2021.

Liu, X., Deng, Z., and Yang, Y. Recent progress in semantic image segmentation. CoRR, abs/1809.10198, 2018. URL http://arxiv.org/abs/1809.10198.

