Word Alignment by Fine-tuning Embeddings on Parallel Corpora
Zi-Yi Dou, Graham Neubig Language Technologies Institute, Carnegie Mellon University
{zdou,gneubig}@cs.cmu.edu

arXiv:2101.08231v4 [cs.CL] 12 Aug 2021

Abstract
Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but ﬁnetuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these ﬁne-tuned models. We perform experiments on ﬁve language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWESOME (Aligning Word Embedding Spaces Of Multilingual Encoders), with pre-trained models is available at https://github. com/neulab/awesome-align.
1 Introduction
Word alignment is a useful tool to tackle a variety of natural language processing (NLP) tasks, including learning translation lexicons (Ammar et al., 2016; Cao et al., 2019), cross-lingual transfer of language processing tools (Yarowsky et al., 2001; Pado´ and Lapata, 2009; Tiedemann, 2014; Agic´ et al., 2016; Mayhew et al., 2017; Nicolai and Yarowsky, 2019), semantic parsing (Herzig and Berant, 2018) and

Eine recheunndge entspÄnderwird
vor ##genowmermdeenn.

0.89 0.82 0.78 0.80 0.77 0.77 0.78 0.79 0.83 0.85 0.82 0.81 0.78 0.77 0.79 0.79 0.78 0.80 0.84 0.84 0.77 0.74 0.78 0.80 0.82 0.80 0.78 0.80 0.88 0.86 0.81 0.81 0.78 0.78 0.79 0.78 0.81 0.82 0.82 0.77 0.76 0.77 0.76 0.77 0.78 0.81 0.84 0.77 0.76 0.73 0.74 0.75 0.77 0.80 0.80 0.78 0.79 0.75 0.76 0.75 0.76 0.73 0.77 0.95
ThenecessaBrcyeorfroecrte##Fioinnew-iltlunbine gmade .

0.88 0.77 0.75 0.77 0.76 0.77 0.76 0.76 0.77 0.83 0.77 0.76 0.75 0.72 0.75 0.75 0.73 0.71 0.83 0.85 0.74 0.68 0.73 0.75 0.79 0.74 0.72 0.74 0.90 0.87 0.77 0.76 0.73 0.75 0.73 0.73 0.77 0.79 0.86 0.73 0.73 0.75 0.72 0.74 0.76 0.80 0.88 0.73 0.77 0.72 0.71 0.73 0.81 0.88 0.79 0.75 0.76 0.72 0.75 0.75 0.75 0.71 0.73 0.97
ThenecessarAcyofrtreectr#F#iionne-wtilul ninbeg made .

Figure 1: Cosine similarities between subword representations in a parallel sentence pair before and after ﬁne-tuning. Red boxes indicate the gold alignments.

speech recognition (Xu et al., 2019). In particular, word alignment plays a crucial role in many machine translation (MT) related methods, including guiding learned attention (Liu et al., 2016), incorporating lexicons during decoding (Arthur et al., 2016), domain adaptation (Hu et al., 2019), unsupervised MT (Ren et al., 2020) and automatic evaluation or analysis of translation models (Bau et al., 2018; Stanovsky et al., 2019; Neubig et al., 2019; Wang et al., 2020). However, with neural networks advancing the state of the arts in almost every ﬁeld of NLP, tools developed based on the 30-yearold IBM word-based translation models (Brown et al., 1993), such as GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013), remain popular choices for word alignment tasks.
One alternative to using statistical word-based translation models to learn alignments would be to instead train state-of-the-art neural machine translation (NMT) models on parallel corpora, and extract alignments therefrom, as examined by Luong et al. (2015); Garg et al. (2019); Zenkel et al. (2020). However, these methods have two disadvantages (also shared with more traditional alignment methods): (1) they are directional and the source and target side are treated differently and (2) they cannot easily take advantage of large-scale contextualized

word embeddings derived from language models (LMs) multilingually trained on monolingual corpora (Devlin et al., 2019; Lample and Conneau, 2019; Conneau et al., 2020), which have proven useful in other cross-lingual transfer settings (Libovicky` et al., 2019; Hu et al., 2020b). In the ﬁeld of word alignment, Sabet et al. (2020) have recently proposed methods to align words using multilingual contextualized embeddings and achieve good performance even in the absence of explicit training on parallel data, suggesting that these are an attractive alternative for neural word alignment.
In this paper, we investigate if we can combine the best of the two lines of approaches. Concretely, we leverage pre-trained LMs and ﬁne-tune them on parallel text with not only LM-based objectives, but also unsupervised objectives over the parallel corpus designed to improve alignment quality. Specifically, we propose a self-training objective, which encourages aligned words to have further closer contextualized representations, and a parallel sentence identiﬁcation objective, which enables the model to bring parallel sentences’ representations closer to each other. In addition, we propose to effectively extract alignments from these ﬁne-tuned models using probability thresholding or optimal transport.
We perform experiments on ﬁve different language pairs and demonstrate that our model can achieve state-of-the-art performance on all of them. In analysis, we ﬁnd that these approaches also generate more aligned contextualized representations after ﬁne-tuning (see Figure 1 as an example) and we can incorporate supervised signals within our paradigm. Importantly, we show that it is possible to train multilingual word aligners that can obtain robust performance even in zero-shot settings, making them a valuable tool that can be used out-ofthe-box with good performance over a wide variety of language pairs.
2 Methods
Formally, the task of word alignment can be deﬁned as: given a sentence x = x1, · · · , xn in the source language and its corresponding parallel sentence y = y1, · · · , ym in the target language, a word aligner needs to ﬁnd a set of pairs of source and target words:
A = { xi, yj : xi ∈ x, yj ∈ y},

where for each word pair xi, yj , xi and yj are semantically similar to each other within the context of the sentence.
In the following paragraphs, we will ﬁrst illustrate how we extract alignments from contextualized word embeddings, then describe our objectives designed to improve alignment quality.
2.1 Extracting Alignments from Embeddings
Contextualized word embedding models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) represent words using continuous vectors calculated in context, and have achieved impressive performance on a diverse array of NLP tasks. Multilingually trained word embedding models such as multilingual BERT can generate contextualized embeddings across different languages. These models can be used to extract contextualized word embeddings hx = hx1, · · · , hxn and hy = hy1, · · · , hym for each pair of parallel sentences x and y. Speciﬁcally, this is done by extracting the hidden states of the i-th layer of the model, where i is an empirically-chosen hyper-parameter. Given these contextualized word embeddings, we propose two methods to calculate unidirectional alignment scores based on probability simplexes and optimal transport. We then turn these alignment scores into alignment matrices and reconcile alignments in the forward and backward directions.
Probability Thresholding. In this method, for each word in the source/target sentence, we calculate a value on the probability simplex for each word in the aligned target/source sentence, and then select all values that exceed a particular threshold as “aligned” words. Concretely, taking inspiration from attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017), we take the contextualized embeddings hx and hy and compute the dot products between them and get the similarity matrix:
S = hxhTy .
Then, we apply a normalization function N to convert the similarity matrix into values on the probability simplex Sxy = N (S), and treat Sxy as the source-to-target alignment matrix. In this paper, we propose to use softmax and a sparse variant α-entmax (Peters et al., 2019) to do the normalization. Compared with the softmax function, α-entmax can produce sparse alignments for any α > 1 and assign non-zero probability to a short

Figure 2: Extracting word alignments from multilingual BERT using probability thresholding (softmax). Red boxes denote the gold alignments.

list of plausible word pairs, where a higher α will lead to a more sparse alignment.
Optimal Transport. The goal of optimal transport (Monge, 1781; Cuturi, 2013) is to ﬁnd a mapping that moves probability from one distribution to another, which can be used to ﬁnd an optimal matching of similar words between two sequences (Kusner et al., 2015). Formally, in a discrete optimal transport problem, we are given two point sets {xi}ni=1 and {yj}mj=1 associated with their probability distributions px and py where
i pxi = 1 and j pyj = 1. Also, a function C(xi, yj) deﬁnes the cost of moving point xi to yj. The goal of optimal transport is to ﬁnd a mapping that moves probability mass from {xi}ni=1 to {yj}mj=1 and the total cost of moving the mass between points is minimized. In other words, it ﬁnds the transition matrix Sxy that minimizes:

C(xi, yj)Sxyij,

(1)

i,j

where Sxy1m = px and SxTy1n = py. The resulting transition matrix is self-normalized and sparse (Swanson et al., 2020), making it appealing alternative towards extracting alignments from word embeddings.
In this paper, we propose to adapt optimal transport techniques to the task of word alignment. Concretely, we treat the parallel sentences x and y as two point sets and assume each word is uniformly distributed. The cost function is obtained by computing the pairwise distance (e.g. cosine distance) between hx and hy, and all the distance values are scaled to [0, 1] with min-max normalization. The optimal transition matrix Sxy to Equation 1 can be calculated using the Sinkhorn-Knopp matrix scaling algorithm (Sinkhorn and Knopp, 1967). If the value of Sxyij is high, xi and yj are likely to have

similar semantics and values that exceed a particular threshold will be considered as “aligned”.
Extracting Bidirectional Alignments. After we obtain both the source-to-target and target-tosource alignment probability matrices Sxy and Syx using the previous methods, we can deduce the ﬁnal alignment matrix by taking the intersection of the two matrices:
A = (Sxy > c) ∗ (SyTx > c),
where c is a threshold and Aij = 1 means xi and yj are aligned.
Note that growing heuristics such as grow-diagﬁnal (Och and Ney, 2000; Koehn et al., 2005) that are popular in statistical word aligners can also be applied in our alignment extraction algorithms, and we will demonstrate the effect of these heuristics in the experiment section.
Handling Subwords. Subword segmentation techniques (Sennrich et al., 2016; Kudo and Richardson, 2018) are widely used in training LMs, thus the above alignment extraction methods can only produce alignments on the subword level. To convert them to word alignments, we follow previous work (Sabet et al., 2020; Zenkel et al., 2020) and consider two words to be aligned if any of their subwords are aligned. Figure 2 shows a concrete example of how we extract word-level alignments from a pre-trained embedding model.
2.2 Fine-tuning Contextualized Embeddings for Word Alignment
While language models can be used to produce reasonable word alignments even without any ﬁnetuning (Sabet et al., 2020), we propose objectives that further improve their alignment ability if we have access to parallel data.

Masked Language Modeling (MLM). Gururangan et al. (2020) suggest that we can gain improvements in downstream tasks by further pretraining LMs on the task datasets. Therefore, we propose to ﬁne-tune the LMs with a masked language modeling objective on both the source and target side of parallel corpora. Speciﬁcally, given a pair of parallel sentences x and y, we choose 15% of the token positions randomly for both x and y, and for each chosen token, we replace it with (1) the [MASK] token 80% of the time (2) a random token 10% of the time and (3) unchanged 10% of the time. The model is trained to reconstruct the original tokens given the masked sentences xmask and ymask:

LMLM = log p(x|xmask) + log p(y|ymask). (2)

Translation Language Modeling (TLM). The MLM objective only requires monolingual data and the model cannot make direct connections between parallel sentences. To solve the issue, similarly to Lample and Conneau (2019), we concatenate parallel sentences x and y and perform MLM on the concatenated data. Compared with MLM, the translation language modeling (TLM) objective enable the model to align the source and target representations. Different from Lample and Conneau (2019), we feed source and target sentences twice in different orders instead of resetting the positions of target sentences:
LT LM = log p([x; y]|[xmask; ymask]) + log p([y; x]|[ymask; xmask]). (3)

Self-training Objective (SO). We also propose a self-training objective for ﬁne-tuning LMs which is similar to the EM algorithm used in the IBM models and the agreement constraints in Tamura et al. (2014). Speciﬁcally, at each training step, we ﬁrst use our alignment extraction methods (described in Section 2.1) to extract the alignment A for x and y, then maximize the following objective:

1 Sxy

SyTx

LSO = Aij ( ij + ij ). (4)

2n

m

i,j

Intuitively, this objective encourages words aligned in the ﬁrst pass of alignment to have further closer contextualized representations. In addition, because of the intersection operation during extraction, the self-training objective can ideally reduce

#Train Sents. #Test Sents.

De-En
1.9M 508

Fr-En
1.1M 447

Ro-En
450K 248

Ja-En
444K 582

Zh-En
40K 450

Table 1: Statistics of datasets.

spurious alignments and encourage the source-totarget and target-to-source alignments to be symmetrical to each other by exploiting their agreement (Liang et al., 2006).
Parallel Sentence Identiﬁcation (PSI). We also propose a contrastive parallel sentence identiﬁcation loss that attempts to make parallel sentences more similar than mismatched sentence pairs (Liu and Sun, 2015; Legrand et al., 2016). This encourages the overall alignments of embeddings on both word and sentence level to be closer together. Concretely, we randomly select a pair of parallel or non-parallel sentences x , y from the training data with equal probability. Then, the model is required to predict whether the two sampled sentences are parallel or not. The representation of the ﬁrst [CLS] token is fed into a multi-layer perceptron to output a prediction score s(x , y ). Denoting the binary label as l, the objective function can be written as:
LP SI = l log s(x , y ) + (1 − l) log(1 − s(x , y )). (5)
Consistency Optimization (CO). While the self-training objective can potentially improve the symmetricity between forward and backward alignments, following previous work on machine translation and multilingual representation learning (Cohn et al., 2016; Zhang et al., 2019; Hu et al., 2020a), we use an objective to explicitly encourage the consistency between the two alignment matrices. Speciﬁcally, we maximize the trace of SxTySyx:
trace(SxTySyx) LCO = min(m, n) . (6)
Our Final Objective. In summary, our training objective is a combination of the proposed objectives and we train the model with them jointly at each training step:
L = LMLM + LT LM + LSO + LP SI + βLCO,
where β is set to 0 or 1 in our experiments.

Model Baseline SimAlign fast align eﬂomal GIZA++ Zenkel et al. (2020) Chen et al. (2020) Ours
α-entmax
softmax

Setting
w/o ﬁne-tuning bilingual bilingual bilingual bilingual bilingual
w/o ﬁne-tuning bilingual multilingual (β = 0) multilingual (β = 1) zero-shot w/o ﬁne-tuning bilingual multilingual (β = 0) multilingual (β = 1) zero-shot

De-En
18.8 27.0 22.6 20.6 16.0 15.4
18.1 16.1 15.4 15.0 16.0 17.4 15.6 15.3 15.1 15.7

Fr-En
7.6 10.5 8.2 5.9 5.0 4.7
5.6 4.1 4.1 4.5 4.3 5.6 4.4 4.4 4.5 4.6

Ro-En
27.2 32.1 25.1 26.4 23.4 21.2
29.0 23.4 22.9 20.8 28.4 27.9 23.0 22.6 20.7 27.2

Ja-En
46.6 51.1 47.5 48.0 -
46.3 38.6 37.4 38.7 44.0 45.6 38.4 37.9 38.4 43.7

Zh-En
21.6 38.1 28.7 35.1 -
18.4 15.4 13.9 14.5 13.9 18.1 15.3 13.6 14.5 14.0

Table 2: Performance (AER) of our models in bilingual, multilingual and zero-shot settings. The best scores for each alignment extraction method are in bold and the overall best scores are in italicized bold.

3 Experiments
In this section, we ﬁrst present our main results, then conduct several ablation studies and analyses of our models.
3.1 Setup
Datasets. We perform experiments on ﬁve different language pairs, namely German-English (DeEn), French-English (Fr-En), Romanian-English (Ro-En), Japanese-English (Ja-En) and ChineseEnglish (Zh-En). For the De-En, Fr-En, Ro-En datasets, we follow the experimental setting of previous work (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020). The training and test data for Ro-En and Fr-En are provided by Mihalcea and Pedersen (2003). The Ro-En training data are also augmented by the Europarl v8 corpus (Koehn, 2005). For the De-En data, the Europarl v7 corpus is used as training data and the gold alignments are provided by Vilar et al. (2006). The Ja-En dataset is obtained from the Kyoto Free Translation Task (KFTT) word alignment data (Neubig, 2011), and the Japanese sentences are tokenized with the KyTea tokenizer (Neubig et al., 2011). The ZhEn dataset is obtained from the TsinghuaAligner website1. We treat their evaluation set as the training data and use the test set in Liu and Sun (2015)
1http://nlp.csai.tsinghua.edu.cn/˜ly/ systems/TsinghuaAligner/TsinghuaAligner. html

ignoring possible alignments. The De-En, En-Fr datasets contain the distinction between sure and possible alignment links. The statistics of these datasets are shown in Table 1. We use the Ja-En development set to tune the hyper-parameters.
Baselines. We compare our models with:
• fast align (Dyer et al., 2013): a popular statistical word aligner which is a simple, fast reparameterization of IBM Model 2.
• eﬂomal (O¨ stling and Tiedemann, 2016): an efﬁcient statistical word aligner using a Bayesian model with Markov Chain Monte Carlo (MCMC) inference.
• GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008): an implementation of IBM models. Following previous work (Zenkel et al., 2020), we use ﬁve iterations each for Model 1, the HMM model, Model 3 and Model 4.
• SimAlign (Sabet et al., 2020): a BERT-based word aligner that is not ﬁne-tuned on any parallel data. The authors propose three alignment extraction methods and we implement their IterMax model with default parameters.
• Zenkel et al. (2020) and Chen et al. (2020): two state-of-the-art neural word aligners based on MT models.

Implementation Details. Our main results are obtained by using the probability thresholding method on the contextualized embeddings in the 8-th layer of multilingual BERT-Base (mBERT; Devlin et al. (2019)) and we will discuss this choice in our ablation studies. We use the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 2e-5 and the batch size is set to 8. Following Peters et al. (2019), we set α to 1.5 for α-entmax. The threshold c is set to 0 for α-entmax and 0.001 for softmax and optimal transport. Unless otherwise stated, β is set to 0. We mainly evaluate the model performance using Alignment Error Rate (AER).
3.2 Main Results
We ﬁrst train our model on each individual language pair, then investigate if it is possible to train multilingual word aligners.
Bilingual Model Performance. From Table 2, we can see that our softmax model can achieve consistent improvements over the baseline models, demonstrating the effectiveness of our proposed method. Surprisingly, directly extracting alignments from mBERT (the w/o ﬁne-tuning setting) can already achieve better performance than the popular statistical word aligner GIZA++ on 4 out of 5 settings, especially in the Zh-En setting where the size of parallel data is small.
Multilingual Model Performance. We also randomly sample 200k parallel sentence pairs from each language pair (except for Zh-En where we take all of its 40k parallel sentences) and concatenate them together to train multilingual word aligners. As shown in Table 2, the multilingually trained word aligners can achieve further improvements and they consistently outperform our bilingual word aligners and all the baselines even though the size of training data for each individual language pair is smaller. The results demonstrate that we can indeed obtain a neural word aligner that has stateof-the-art and robust performance across different language pairs. We also test the performance of our consistency optimization objective in this setting. We can see that incorporating this objective (β=1) can signiﬁcantly improve the model performance on Ro-En, while it also deteriorates the Ja-En and Zh-En performance by a non-negligible margin. We ﬁnd that this is because the CO objective can signiﬁcantly improve the alignment recall while sacriﬁcing the precisions, and our Ro-En dataset

Prob. OT

Component
softmax α-entmax Cosine Dot Product Euclidean

De-En
17.4 18.1 24.4 25.4 20.7

Fr-En
5.6 5.6 15.7 17.1 15.1

Ro-En
27.9 29.0 33.7 34.1 33.3

Ja-En
45.6 46.3 54.0 54.2 53.2

Zh-En
18.1 18.4 31.1 30.9 29.8

Speed
33.22 32.36 3.36 3.82 3.05

Table 3: Comparisons of probability thresholding (Prob.) and optimal transport (OT) for alignment extraction. We try both softmax and α-entmax for probability thresholding and different cost functions for optimal transport. We measure both the extraction speed (#sentences/seconds) and the alignment quality (AER) on ﬁve language pairs, namely German-English (DeEn), French-English (Fr-En), Romanian-English (RoEn), Japanese-English (Ja-En), and Chinese-English (Zh-En). The best scores are in bold.

tends to favor models with high recall and the Ja-En and Zh-En datasets have an opposite tendency.
Zero-Shot Performance. In this paragraph, we want to ﬁnd out how our models perform on language pairs that it has never seen during training. To this end, for each language pair, we train our model with data of all the other language pairs and test its performance on the target language pair. Results in Table 2 demonstrate that training our models with parallel data on other language pairs can still improve the model performance on the target language pair. This is a very important result, as it indicates that our model can be used as a off-the-shelf tool for multilingual word alignment for any language supported by the underlying embeddings, regardless of whether parallel data has been used for training or not.
3.3 Ablation Studies
In this part, we compare the performance of different alignment extraction methods, pre-trained embedding models and training objectives.
Alignment Extraction Methods. We ﬁrst compare the performance of our two proposed alignment extraction methods, namely the probability thresholding and optimal transport techniques. We use the representations of the 8-th layer of mBERT following Sabet et al. (2020).
As shown in Table 3, probability thresholding methods can consistently outperform optimal transport by a large margin on the ﬁve language pairs. In addition, probability thresholding methods are much faster than optimal transport. softmax is marginally better than α-entmax, yet one advantage of α-entmax is that we do not need to manually set

Model mBERT XLM-15 (MLM) XLM-15 (MLM+TLM) XLM-100 (MLM) XLM-R

Layer
7 8 9 4 5 6 4 5 6 7 8 9 7 8 9

De-En
18.7 17.4 18.8 21.1 20.4 23.2 16.4 16.2 18.8 20.5 19.8 19.9 24.4 23.1 24.7

Fr-En
6.1 5.6 6.1 6.8 6.1 7.7 4.9 4.7 5.7 8.5 8.2 8.8 10.3 9.2 11.5

Zh-En
19.1 18.1 20.1 25.3 26.1 33.3 18.6 23.7 26.2 30.8 28.6 29.3 33.2 30.7 28.1

Table 4: Comparisons of different LMs in terms of AER. We extract alignments using softmax and take representations from different layers of LMs. The best scores for each individual model are in bold and the overall best scores are in italicized bold.

the threshold. Therefore, we use both softmax and α-entmax to obtain the main results.
Pre-trained Embedding Models. In this paragraph, we investigate the performance of three different types of pre-trained embedding models, including mBERT, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2020). For XLM, we have tried its three released models: 1) XLM-15 (MLM) pre-trained with MLM and supports 15 languages; 2) XLM-15 (MLM+TLM) pretrained with both the MLM and TLM objectives and supports 15 languages; 3) XLM-100 (MLM) pre-trained with MLM and supports 100 languages. We use softmax to extract the alignments.
Because XLM-15 does not support Japanese or Romanian, we only report the performance on the three other language pairs in Table 4. We take representations from different layers and report the performance of the best three layers. We can see that while XLM-15 (MLM+TLM) can achieve the best performance on De-En and Fr-En, the best layer is not consistent across language pairs. On the other hand, the optimal conﬁgurations for mBERT are consistent across language pairs. In addition, considering mBERT supports many more languages than XLM-15 (MLM+TLM), we will use mBERT in the following sections.
Training Objectives. We also conduct ablation studies on each of our training objectives. We can see from Table 5 that the self-training objective can best improve the model performance. Also,

the translation language modeling and parallel sentence identiﬁcation objectives can marginally beneﬁt the model. The masked language modeling objective, on the other hand, cannot always improve the model and can sometimes even deteriorate the model performance, possibly because the TLM objective already provides the model with sufﬁcient supervision signals.
3.4 Analysis
We conduct several analyses to better understand our models. Unless otherwise stated, we perform experiments on the softmax model using mBERT.
Incorporating Supervised Signals. We investigate if our models can beneﬁt from supervised signals. If we have access to word-level gold labels for word alignment, we can simply utilize them in our self-training objectives. Speciﬁcally, we can set Aij in Equation 4 to 1 if and only if they are aligned. In our experimental settings, we have gold labels for all the Zh-En sentences and 653 sentences from the Ja-En development set. Table 6 demonstrates that training our models with as few as 653 labeled sentences can dramatically improve the alignment quality, and combining labeled and unlabeled parallel data can further improve the model performance. This analysis demonstrate the generality of our models as they can also be applied in semisupervised settings.
Growing Heuristics. As stated in Section 2.1, because our alignment extraction methods essentially take the intersection of forward and backward alignments, growing heuristics can also be applied in our settings. The main motivation of growing heuristics is to improve the recall of the resulting alignments. While effective in statistical word aligners, as shown in Table 7, the growing heuristics only improve our alignment extraction method on the vanilla mBERT model in the Ro-En setting while degrading the model performance on all the other language pairs. After ﬁne-tuning, the growing heuristics can only hurt the model performance, possibly because the self-training objective encourages the forward and backward alignments to be symmetrical. Based on these results, we do not adopt the growing heuristics in our models.
Annotation Projection. Word alignment has been a useful tool in cross-lingual annotation projection (Yarowsky et al., 2001; Nicolai and Yarowsky, 2019). Therefore, it would be inter-

Model softmax

Objective
All All w/o MLM All w/o TLM All w/o SO All w/o PSI

De-En
15.3 15.3 15.5 16.9 15.4

Fr-En
4.4 4.4 4.7 4.8 4.4

Ro-En
22.6 22.8 22.9 23.0 22.7

Ja-En
37.9 38.6 39.7 39.1 37.9

Zh-En
13.6 13.7 14.0 15.4 13.8

Table 5: Ablation studies on our training objectives in multilingual settings.

Figure 3: An example of extracting alignments from our ﬁne-tuned model using softmax. Red boxes indicate the gold alignments. The ﬁne-tuned model can generate more accurate alignments then vanilla mBERT (Figure 2).

Lang. Unsup. Sup. Semi-Sup.

Zh-En 15.3 Ja-En 38.4

12.5 31.6 30.0

Table 6: Incorporating supervised word alignment signals into our model can further improve the model performance in terms of AER.

Model mBERT
Ours-Multi.

Ext.
X-En En-X softmax gd gd-ﬁnal
X-En En-X softmax gd gd-ﬁnal

De-En
24.7 22.6 17.4 18.7 18.6
20.2 18.1 15.3 16.3 16.5

Fr-En
14.4 12.2 5.6 9.2 9.3
12.9 9.3 4.4 8.1 8.3

Ro-En
31.9 32.0 27.9 27.0 26.9
25.4 25.9 22.6 23.1 23.2

Ja-En
54.7 52.7 45.6 48.5 48.7
42.1 41.7 37.9 38.2 38.7

Zh-En
27.4 29.9 18.1 23.4 23.2
19.3 23.5 13.6 18.3 18.5

Table 7: The grow-diag-ﬁnal heuristic can only improve our alignment extraction method in the Romanian-English setting without ﬁne-tuning. “gd” refers to grow-diag.

esting to see if our model can be beneﬁcial in these settings. To this end, we evaluate our model and baselines on cross-lingual named entity recognition (NER). We train a BERT-based NER model on the CoNLL 2003 English data (Tjong Kim Sang and De Meulder, 2003) and test it on the CoNLL 2002 Spanish data (Tjong Kim Sang, 2002). We use Google Translate to translate Spanish test set into English, predict the labels using the NER model, then project the labels from English to Spanish us-

Model
BERT-En (zero-shot) fast align GIZA++ SimAlign Ours

Prec. %
53.1 51.5 56.5 59.9 60.6

Rec. %
54.3 59.8 64.1 67.6 68.5

F1 %
52.7 55.2 60.0 63.5 64.3

Table 8: Our model is also effective in an annotation projection setting where we train a BERT-based NER model on English data and test it on Spanish data. The best scores are in bold.

ing word aligners. From Table 8, we can see that our model is also better than baselines in this setting, demonstrating its usefulness in cross-lingual annotation projection.
Sentence-Level Representation Transfer. We also test if the aligned representations are beneﬁcial for sentence-level cross-lingual transfer. In doing so, we perform experiments on XNLI (Conneau et al., 2018), which evaluates cross-lingual sentence representations in 15 languages on the task of natural language inference (NLI). We train our models with the provided 10k parallel data on the 15 languages, ﬁne-tune our model on the English NLI data, then test its performance on other languages. As shown in Table 9, our model can outperform the baseline, indicating the aligned word representations can also be helpful for sentencelevel cross-lingual transfer.

Model En Fr Es De El Bg Ru Tr Ar Vi Th Zh Hi Sw Ur Ave. mBERT 81.3 73.4 74.3 70.5 66.9 68.2 68.5 59.5 64.3 70.6 50.7 68.8 59.3 49.4 57.5 65.5 Ours 81.5 74.1* 74.9* 71.2* 67.1 68.7* 68.6 61.0* 66.2* 70.5 53.8* 69.1 59.8* 50.6* 58.6* 66.4*
Table 9: Results of mBERT and our ﬁne-tuned model on XNLI (Conneau et al., 2018). Our objectives can improve the model cross-lingual transfer ability. “*” denotes signiﬁcant differences using paired bootstrapping (p<0.05) .

Alignment Examples. We also conduct qualitative analyses as shown in Figure 1, 2 and 3. After ﬁne-tuning, the learned contextualized representations are more aligned, as the cosine distances between semantically similar words become closer, and the extracted alignments are more accurate. More examples are shown in Appendix B.
4 Related Work
Based on the IBM translation models (Brown et al., 1993), many statistical word aligners have been proposed (Vogel et al., 1996; O¨ stling and Tiedemann, 2016), including the current most popular tools GIZA++ (Och and Ney, 2000, 2003; Gao and Vogel, 2008) and fast align (Dyer et al., 2013).
Recently, there is a resurgence of interest in neural word alignment (Tamura et al., 2014; Alkhouli et al., 2018). Based on NMT models trained on parallel corpora, researchers have proposed several methods to extract alignments from them (Luong et al., 2015; Zenkel et al., 2019; Garg et al., 2019; Li et al., 2019) and successfully build an end-to-end neural model that can outperform statistical tools (Zenkel et al., 2020). However, there is an inherent discrepancy between translation and word alignment: translation models are directional and the source and target side are treated differently, while word alignment is a non-directional task. Therefore, certain adaptations are required for translation models to perform word alignment.
Another disadvantage of MT-based word aligners is that they cannot easily utilize contextualized embeddings. Using learned representations to improve word alignment have been investigated (Sabet et al., 2016; Pourdamghani et al., 2018). Recently, pre-trained LMs (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020) have proven to be useful in cross-lingual transfer (Libovicky` et al., 2019; Hu et al., 2020b). In word alignment, Sabet et al. (2020) propose effective methods to extract alignments from multilingual LMs without explicit training on parallel data. In this work, we propose better alignment extraction methods and combine the best of the two worlds by ﬁne-tuning contextualized embeddings on parallel data.

There are also work on supervised neural word alignment (Stengel-Eskin et al., 2019; Nagata et al., 2020). However, supervised data are not always accessible, making their methods inapplicable in many scenarios. In this paper, we demonstrate that our model can incorporate supervised signals if available and perform semi-supervised learning, which is a more realistic and general setting.
Some work on bilingual lexicon induction also share similar general ideas with ours. For example, Zhang et al. (2017) minimize the earth mover’s distance to match the embedding distributions from different languages. Similarly, Grave et al. (2019) present an algorithm to align point clouds with Procrustes (Scho¨nemann, 1966) in Wasserstein distance for unsupervised embedding alignment.
5 Discussion and Conclusion
We present a neural word aligner that achieves stateof-the-art performance on ﬁve diverse language pairs and obtains robust performance in zero-shot settings. We propose to ﬁne-tune multilingual embeddings with objectives suitable for word alignment and develop two alignment extraction methods. We also demonstrate its applications in semisupervised settings. We hope our word aligner can be a tool that can be used out-of-the-box with good performance over various language pairs. Future directions include designing better training objectives and experimenting on more language pairs.
Also, note that we mainly evaluate our word aligners using AER following previous work, which has certain limitations. For example, it may not be well-correlated with statistical machine translation performance Fraser and Marcu (2007) and different types of alignments can be suitable for different tasks or conditions (Lambert et al., 2012; Stymne et al., 2014). Although we have evaluated models in annotation projection and cross-lingual transfer settings, alternative metrics (Tiedemann, 2005; Søgaard and Wu, 2009; Ahrenberg, 2010) are also worth considering in the future.
Acknowledgement
We thank our reviewers for helpful suggestions.

References
Zˇ eljko Agic´, Anders Johannsen, Barbara Plank, He´ctor Mart´ınez Alonso, Natalie Schluter, and Anders Søgaard. 2016. Multilingual projection for parsing truly low-resource languages. Transactions of the Association for Computational Linguistics.
Lars Ahrenberg. 2010. Alignment-based proﬁling of europarl data in an english-swedish parallel corpus. In Proceedings of the International Conference on Language Resources and Evaluation.
Tamer Alkhouli, Gabriel Bretschner, and Hermann Ney. 2018. On the alignment problem in multi-head attention-based neural machine translation. In Proceedings of the Conference on Machine Translation.
Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A Smith. 2016. Massively multilingual word embeddings. arXiv preprint.
Philip Arthur, Graham Neubig, and Satoshi Nakamura. 2016. Incorporating discrete translation lexicons into neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Rrepresentations.
Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2018. Identifying and controlling important neurons in neural machine translation. In Proceedings of the International Conference on Learning Representations.
Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics.
Tom B. Brown, Benjamin Pickman Mann, Nick Ryder, Melanie Subbiah, Jean Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, G. Kru¨ger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric J Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. arXiv preprint.
Steven Cao, Nikita Kitaev, and Dan Klein. 2019. Multilingual alignment of contextual word representations. In Proceedings of the International Conference on Learning Representations.
Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and Qun Liu. 2020. Accurate word alignment induction

from neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haffari. 2016. Incorporating structural alignment biases into an attentional neural translation model. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal transport. Proceedings of the Advances in Neural Information Processing Systems.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing.
Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019. Jointly learning to align and translate with transformer models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Edouard Grave, Armand Joulin, and Quentin Berthet. 2019. Unsupervised alignment of embeddings with wasserstein procrustes. In Proceedinds of the International Conference on Artiﬁcial Intelligence and Statistics.

Suchin Gururangan, Ana Marasovic´, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Jonathan Herzig and Jonathan Berant. 2018. Decoupling structure and lexicon for zero-shot semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, and Graham Neubig. 2020a. Explicit alignment objectives for multilingual bidirectional encoders. arXiv preprint.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020b. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the International Conference on Machine Learning.
Junjie Hu, Mengzhou Xia, Graham Neubig, and Jaime G Carbonell. 2019. Domain adaptation of neural machine translation by lexicon induction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 iwslt speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation.
Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In Proceedings of the International Conference on Machine Learning.
Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and Andy Way. 2012. What types of word alignment improve statistical machine translation? Machine Translation.
Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. In Proceedings of the Advances in Neural Information Processing Systems.
Joe¨l Legrand, Michael Auli, and Ronan Collobert. 2016. Neural network-based word alignment through score aggregation. In Proceedings of the Conference on Machine Translation.

Xintong Li, Guanlin Li, Lemao Liu, Max Meng, and Shuming Shi. 2019. On the word alignment from neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Jindˇrich Libovicky`, Rudolf Rosa, and Alexander Fraser. 2019. How language-neutral is multilingual BERT? arXiv preprint.
Lemao Liu, Masao Utiyama, Andrew Finch, and Eiichiro Sumita. 2016. Neural machine translation with supervised attention. In Proceedings of the International Conference on Computational Linguistics.
Yang Liu and Maosong Sun. 2015. Contrastive unsupervised word alignment with non-local features. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint.
I. Loshchilov and F. Hutter. 2019. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attentionbased neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Bill MacCartney, Michel Galley, and Christopher D Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017. Cheap translation for cross-lingual named entity recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of Workshop on Building and Using Parallel Texts.
Gaspard Monge. 1781. Me´moire sur la the´orie des de´blais et des remblais. Histoire de l’Acade´mie Royale des Sciences de Paris.
Masaaki Nagata, Chousa Katsuki, and Masaaki Nishino. 2020. A supervised word alignment

method based on cross-language span prediction using multilingual BERT. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.
Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, and Xinyi Wang. 2019. compare-mt: A tool for holistic comparison of language generation systems. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: System Demonstrations.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese morphological analysis. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Garrett Nicolai and David Yarowsky. 2019. Learning morphosyntactic analyzers from the Bible via iterative annotation projection across 26 languages. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics.
Robert O¨ stling and Jo¨rg Tiedemann. 2016. Efﬁcient word alignment with Markov Chain Monte Carlo. The Prague Bulletin of Mathematical Linguistics.
Sebastian Pado´ and Mirella Lapata. 2009. Crosslingual annotation projection for semantic roles. Journal of Artiﬁcial Intelligence Research.
Ben Peters, Vlad Niculae, and Andre´ FT Martins. 2019. Sparse sequence-to-sequence models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Nima Pourdamghani, Marjan Ghazvininejad, and Kevin Knight. 2018. Using word vectors to improve word alignments for low resource machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.
Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and Shuai Ma. 2020. A retrieve-and-rewrite initialization method for unsupervised machine translation.

In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Masoud Jalili Sabet, Philipp Dufter, Franc¸ois Yvon, and Hinrich Schu¨tze. 2020. Simalign: High quality word alignments without parallel training data using static and contextualized embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: Findings.
Masoud Jalili Sabet, Heshaam Faili, and Gholamreza Haffari. 2016. Improving word alignment of rare words with word embeddings. In Proceedings of the International Conference on Computational Linguistics.
Peter H Scho¨nemann. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Richard Sinkhorn and Paul Knopp. 1967. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of Mathematics.
Anders Søgaard and Dekai Wu. 2009. Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars. In Proceedings of the International Conference on Parsing Technologies.
Gabriel Stanovsky, Noah A Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Elias Stengel-Eskin, Tzu-ray Su, Matt Post, and Benjamin Van Durme. 2019. A discriminative neural model for cross-lingual word alignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Sara Stymne, Jo¨rg Tiedemann, and Joakim Nivre. 2014. Estimating word alignment quality for smt reordering tasks. In Proceedings of the Workshop on Statistical Machine Translation.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence. Transactions of the Association for Computational Linguistics.
Kyle Swanson, Lili Yu, and Tao Lei. 2020. Rationalizing text matching: Learning sparse alignments via optimal transport. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2014. Recurrent neural networks for word alignment model. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.

Jo¨rg Tiedemann. 2005. Optimization of word alignment clues. Natural Language Engineering.
Jo¨rg Tiedemann. 2014. Rediscovering annotation projection for cross-lingual parser induction. In Proceedings of the International Conference on Computational Linguistics.
Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of the Conference on Natural Language Learning.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Conference on Natural Language Learning.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems.
David Vilar, Maja Popovic´, and Hermann Ney. 2006. AER: Do we need to “improve” our alignments? In Proceedings of the International Workshop on Spoken Language Translation.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the International Conference on Computational Linguistics.
Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. 2020. On the inference calibration of neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Hainan Xu, Shuoyang Ding, and Shinji Watanabe. 2019. Improving end-to-end speech recognition with pronunciation-assisted sub-word modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing.
Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013a. A lightweight and high performance monolingual word aligner. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013b. Semi-Markov phrase-based monolingual alignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the International Conference on Human Language Technology Research.

Thomas Zenkel, Joern Wuebker, and John DeNero. 2019. Adding interpretable attention to neural translation models improves word alignment. arXiv preprint.
Thomas Zenkel, Joern Wuebker, and John DeNero. 2020. End-to-end neural word alignment outperforms GIZA++. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Earth mover’s distance minimization for unsupervised bilingual lexicon induction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming Zhou, and Tong Xu. 2019. Regularizing neural machine translation by target-bidirectional agreement. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.

Model
Baseline Yao et al. (2013a) Yao et al. (2013b) Sultan et al. (2014)
Ours mBERT Ours-Multilingual Ours-Supervised

Prec. %
91.3 90.4 93.5
87.0 87.0 87.2

Rec.%
82.0 81.9 82.6
89.0 89.3 89.8

F1 %
86.4 85.9 87.6
88.0 88.1 88.5

Table 10: Our model is also effective in monolingual alignment settings.

A Implementation Details
We use the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 2e-5 and the batch size is set to 8. Following Peters et al. (2019), we set α to 1.5 for α-entmax. The threshold c is set to 0 for α-entmax and 0.001 for softmax and optimal transport. We train our models on one 2080 Ti for one epoch and it takes 3 to 24 hours for the model to converge depending on the size of the dataset. We evaluate the model performance using Alignment Error Rate (AER).
B Analysis
In this section, we conduct more analyses of our models.
Monolingual Alignment. We also investigate how our models perform in monolingual alignment settings. Previous methods (MacCartney et al., 2008; Yao et al., 2013a,b; Sultan et al., 2014) typically exploit external resources such as WordNet to tackle the problem. As shown in Table 10, mBERT can outperform previous methods in terms of recall and F1 without any ﬁne-tuning. Our multilingually ﬁne-tuned model can achieve better recall and slightly better F1 score than the vanilla mBERT model, and ﬁne-tuning our model with supervised signals can achieve further improvements.
Sensitivity Analysis. We also conduct a sensitivity analysis on the threshold c for our softmax alignment extraction method. As shown in Table 11, our method is relatively robust to this threshold. In particular, after ﬁne-tuning, the AERs change within 0.5% when varying the threshold.
Comparisons with IterMax. IterMax is the best alignment extraction method in SimAlign (Sabet et al., 2020). The results in the main paper have demonstrated that our alignment extraction methods are able to outperform IterMax. In Figure 4, we

Model

c. De-En Fr-En Ro-En Ja-En Zh-En

mBERT

1e-6 17.3 6.0 27.2 45.2 18.9 1e-5 17.3 5.9 27.4 45.1 18.6 1e-4 17.3 5.7 27.6 45.3 18.3 1e-3 17.4 5.6 27.9 45.6 18.1 1e-2 17.7 5.6 28.4 45.8 18.2 1e-1 18.1 5.6 28.9 46.3 18.3 5e-1 18.4 5.6 29.5 47.0 18.7

1e-6 15.4 4.6 22.7 38.2 14.1 1e-5 15.4 4.5 22.7 38.1 14.0 1e-4 15.3 4.5 22.6 37.9 13.9 Ours-Multilingual 1e-3 15.3 4.4 22.6 37.9 13.8 1e-2 15.3 4.3 22.7 37.9 13.8 1e-1 15.4 4.3 22.8 38.0 13.8 5e-1 15.4 4.2 23.0 38.2 13.9

Table 11: Our softmax alignment extraction method is relatively robust to the threshold c.

can see that the IterMax algorithm tends to sacriﬁce precision for a small improvements in recall, while our model can generate more accurate alignments.
Ablation Studies on Training Objectives. Table 12 presents more ablation studies on our training objectives. We can see that the self training objective is the most effective one, with the translation language modeling objective being the second and the parallel sentence identiﬁcation objective being the third. The masked language modeling objective can sometimes hurt the model performance, possibly because of the translation language modeling objective.
Experiments on More Language Pairs. We also test our alignment extraction methods on other language pairs following the setting of Sabet et al. (2020) without ﬁne-tuning as shown in Table 13.2
More Qualitative Examples. In addition to the examples provided in the main text, we also present some randomly sampled samples in Figure 5. We can clearly see that our model learns more aligned representations than the baseline model.

2Their English-Persian dataset is unavailable at the time of writing the paper.

Model Objective

Ours-Bilingual

All

All w/o MLM

α-entmax All w/o TLM All w/o SO

All w/o PSI

All

All w/o MLM

softmax

All w/o TLM All w/o SO

All w/o PSI

Ours-Multilingual All All w/o MLM
α-entmax All w/o TLM All w/o SO All w/o PSI All All w/o MLM
softmax All w/o TLM All w/o SO All w/o PSI

De-En
16.1 15.6 16.4 17.8 16.5 15.6 15.5 15.9 17.4 15.6
15.4 15.1 16.4 17.5 15.5 15.3 15.3 15.5 16.9 15.4

Fr-En
4.1 4.2 4.3 4.7 4.2 4.4 4.2 4.5 4.7 4.3
4.1 4.2 4.4 4.6 3.9 4.4 4.4 4.7 4.8 4.4

Ro-En
23.4 23.3 23.7 23.9 23.1 23.0 23.2 23.7 23.2 23.1
22.9 22.8 23.3 23.6 23.0 22.6 22.8 22.9 23.0 22.7

Ja-En
38.6 38.8 40.1 39.4 38.5 38.4 38.9 40.1 38.6 38.8
37.4 37.8 39.7 40.0 38.2 37.9 38.6 39.7 39.1 37.9

Zh-En
15.4 15.1 15.3 16.3 15.4 15.3 14.9 15.1 16.3 15.4
13.9 13.7 14.4 15.6 14.1 13.6 13.7 14.0 15.4 13.8

Table 12: Ablation studies on training objectives.

Model
GIZA++ SimAlign Ours (softmax, c=1e-3) Ours (softmax, c=1e-5) Ours (softmax, c=1e-7)

En-Cs
18.2 13.4 12.3 12.7 13.3

En-Hi
51.8 40.2 41.2 39.5 39.2

Table 13: Performance on more language pairs.

Ebenfailmls

0.77 0.75

0.87 0.80

0.75 0.76

0.73 0.76

0.73 0.71

0.75 0.84

0.75 0.83

0.75 0.77

0.74 0.76

0.75 0.74

0.75 0.74

0.79 0.78

Beredicehr

0.74 0.72

0.77 0.75

0.74 0.72

0.73 0.77

0.73 0.70

0.80 0.81

0.83 0.89

0.83 0.81

0.78 0.83

0.76 0.77

0.77 0.76

0.77 0.76

Pdruocdautkiotvnioorannel

0.75 0.73 0.78 0.77

0.76 0.74 0.75 0.76

0.75 0.73 0.75 0.74

0.72 0.74 0.75 0.74

0.74 0.70 0.71 0.73

0.78 0.76 0.75 0.76

0.83 0.79 0.80 0.79

0.95 0.83 0.83 0.82

0.85 0.91 0.83 0.82

0.81 0.80 0.95 0.86

0.82 0.81 0.87 0.95

0.77 0.76 0.78 0.80

e

softIwsraaisetl

0.80 0.93

0.81 0.77

0.80 0.74

0.75 0.73

0.71 0.70

0.77 0.73

0.76 0.74

0.77 0.75

0.78 0.75

0.76 0.78

0.78 0.78

0.82 0.79

##we#wis#eegdn. 0000....77784240 0000....77789462 0000....77779557 0000....77775355 0000....77775121 0000....77774246 0000....77774248 0000....77776457 0000....77773478 0000....77775447 0000....77775468 0000....77798484

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .

Cosine Distance

Intersection (a) mBERT Itermax

Itermax

Word-Level Alignment

(b) mBERT softmax

Ebenfailmls

0.75 0.72

0.92 0.73

0.72 0.73

0.68 0.74

0.70 0.70

0.69 0.86

0.66 0.80

0.68 0.72

0.66 0.75

0.70 0.70

0.70 0.72

0.72 0.75

ProBduekretvdiioocehnnr

0.71 0.67 0.75 0.68

0.72 0.68 0.68 0.67

0.72 0.67 0.73 0.67

0.68 0.76 0.65 0.72

0.74 0.68 0.71 0.66

0.79 0.79 0.73 0.74

0.77 0.87 0.73 0.75

0.77 0.71 0.90 0.75

0.72 0.79 0.74 0.91

0.71 0.69 0.75 0.72

0.74 0.69 0.77 0.74

0.72 0.71 0.72 0.70

eduscoafttwioanirsaetl

0.74 0.73 0.75

0.68 0.69 0.72

0.71 0.70 0.79

0.69 0.68 0.73

0.66 0.71 0.70

0.69 0.71 0.75

0.70 0.70 0.74

0.75 0.76 0.73

0.72 0.73 0.75

0.96 0.81 0.71

0.81 0.97 0.74

0.71 0.73 0.77

##wIes#wirsa#eeegdn.l 00000.....9777742238 00000.....7777752135 00000.....7777759985 00000.....7777742043 00000.....7777739673 00000.....7777752046 00000.....7667729724 00000.....7777754343 00000.....7667738725 00000.....7777761244 00000.....7777784357 00000.....8777905476

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .
Israel also points the way
in producttiohen educationoafl
software .

Cosine Distance

Intersection
(c) Fine-tuned IterMax

IterMax

Word-Level Alignment

(d) Fine-tuned softmax
Figure 4: Extracting alignments from our model using IterMax(Sabet et al., 2020) and our softmax method from the vanilla and ﬁne-tuned mBERT models.

France was the first to take
diplomatic and
human ##itarian
action .
France was the first to take
diplomatic and
human ##itarian
action .

Frankreich 0.96 0.83 0.78 0.78 0.75 0.75 0.78 0.70 0.75 0.76 0.79 0.83 hat 0.78 0.86 0.80 0.80 0.78 0.77 0.73 0.71 0.73 0.74 0.75 0.81 als 0.72 0.80 0.81 0.75 0.74 0.72 0.70 0.67 0.70 0.71 0.70 0.74
erstes 0.75 0.79 0.81 0.87 0.75 0.73 0.73 0.68 0.72 0.73 0.74 0.78 auf 0.72 0.76 0.77 0.76 0.78 0.83 0.76 0.77 0.76 0.77 0.78 0.77
diplomat 0.76 0.74 0.73 0.73 0.71 0.76 0.93 0.76 0.87 0.85 0.81 0.78 ##ischer 0.69 0.70 0.71 0.69 0.71 0.74 0.81 0.79 0.77 0.81 0.77 0.72
und 0.70 0.72 0.73 0.72 0.73 0.76 0.77 0.95 0.79 0.80 0.80 0.75 human 0.75 0.74 0.74 0.75 0.73 0.76 0.87 0.79 0.96 0.89 0.82 0.78 ##itä 0.73 0.74 0.73 0.73 0.72 0.75 0.85 0.79 0.89 0.91 0.80 0.77 ##rer 0.66 0.69 0.69 0.68 0.69 0.71 0.75 0.76 0.74 0.79 0.74 0.70 Ebene 0.73 0.74 0.73 0.75 0.73 0.77 0.79 0.81 0.79 0.81 0.85 0.78
re 0.74 0.77 0.74 0.76 0.75 0.80 0.77 0.73 0.77 0.76 0.82 0.78 ##agi 0.71 0.74 0.72 0.74 0.74 0.79 0.74 0.72 0.76 0.75 0.81 0.75 ##ert 0.75 0.80 0.77 0.78 0.78 0.80 0.76 0.73 0.74 0.77 0.82 0.80
. 0.80 0.81 0.79 0.79 0.77 0.77 0.78 0.73 0.76 0.78 0.81 0.96

0.97 0.77 0.71 0.73 0.71 0.72 0.74 0.68 0.70 0.70 0.73 0.76 0.74 0.85 0.75 0.73 0.76 0.74 0.68 0.69 0.68 0.67 0.71 0.75 0.70 0.78 0.81 0.73 0.76 0.70 0.67 0.69 0.66 0.66 0.68 0.71 0.72 0.72 0.72 0.90 0.70 0.69 0.69 0.65 0.67 0.66 0.67 0.70 0.71 0.74 0.75 0.71 0.80 0.79 0.72 0.77 0.69 0.69 0.75 0.74 0.72 0.69 0.67 0.69 0.68 0.72 0.93 0.73 0.80 0.78 0.75 0.75 0.69 0.68 0.69 0.68 0.69 0.71 0.83 0.76 0.74 0.78 0.73 0.70 0.68 0.69 0.69 0.67 0.71 0.71 0.74 0.97 0.74 0.73 0.74 0.72 0.69 0.67 0.66 0.69 0.67 0.68 0.78 0.74 0.97 0.88 0.74 0.71 0.68 0.67 0.64 0.66 0.66 0.68 0.78 0.73 0.88 0.92 0.72 0.70 0.66 0.66 0.66 0.66 0.68 0.67 0.72 0.72 0.76 0.83 0.70 0.68 0.71 0.70 0.68 0.70 0.68 0.74 0.72 0.75 0.70 0.72 0.81 0.72 0.71 0.74 0.67 0.70 0.71 0.83 0.72 0.70 0.70 0.69 0.81 0.73 0.69 0.72 0.66 0.69 0.71 0.83 0.70 0.68 0.69 0.68 0.81 0.71 0.71 0.77 0.70 0.72 0.73 0.85 0.72 0.70 0.69 0.71 0.81 0.75 0.76 0.76 0.72 0.72 0.74 0.74 0.75 0.73 0.72 0.73 0.76 0.98

Before Fine-tuning
0.83 0.82 0.78 0.76 0.69 0.72 0.75 0.74 0.80
Ein 0.81 0.87 0.82 0.76 0.71 0.75 0.75 0.76 0.79 entsprechende 0.76 0.82 0.78 0.74 0.66 0.71 0.73 0.72 0.74
#r 0.75 0.80 0.81 0.76 0.69 0.73 0.75 0.75 0.76 #Be 0.71 0.73 0.77 0.73 0.67 0.71 0.73 0.73 0.72 sch 0.69 0.72 0.72 0.70 0.67 0.68 0.71 0.70 0.69 ###lu 0.76 0.79 0.82 0.76 0.68 0.74 0.75 0.76 0.78 # #ß 0.79 0.79 0.79 0.88 0.75 0.79 0.81 0.76 0.82 # ist 0.79 0.76 0.76 0.82 0.79 0.84 0.80 0.75 0.79 noch 0.85 0.77 0.76 0.80 0.76 0.82 0.81 0.77 0.78 nicht 0.71 0.74 0.75 0.78 0.74 0.77 0.82 0.78 0.76 ge 0.65 0.68 0.68 0.69 0.68 0.70 0.73 0.73 0.70 #fa 0.73 0.75 0.74 0.74 0.69 0.74 0.77 0.80 0.77 ##ßt 0.74 0.74 0.77 0.80 0.71 0.79 0.80 0.78 0.79
wo#rden. No suchdBeceisfioonrheas Finase-tyuetnibneegn taken . 0.79 0.76 0.78 0.76 0.69 0.74 0.74 0.73 0.96

After Fine-tuning
0.78 0.79 0.75 0.75 0.72 0.71 0.76 0.74 0.74 0.73 0.86 0.75 0.69 0.71 0.74 0.69 0.74 0.73 0.70 0.80 0.75 0.71 0.69 0.71 0.70 0.73 0.72 0.69 0.72 0.81 0.69 0.65 0.67 0.70 0.74 0.72 0.68 0.71 0.85 0.71 0.65 0.70 0.70 0.76 0.72 0.68 0.72 0.83 0.70 0.68 0.70 0.71 0.75 0.71 0.69 0.74 0.86 0.71 0.67 0.70 0.70 0.77 0.74 0.71 0.71 0.73 0.90 0.76 0.76 0.81 0.74 0.75 0.74 0.69 0.69 0.74 0.82 0.87 0.72 0.71 0.71 0.88 0.68 0.70 0.73 0.71 0.77 0.73 0.72 0.71 0.71 0.71 0.75 0.75 0.71 0.74 0.81 0.85 0.73 0.68 0.68 0.72 0.70 0.68 0.71 0.74 0.83 0.70 0.70 0.70 0.75 0.71 0.67 0.72 0.75 0.87 0.73 0.71 0.71 0.73 0.81 0.75 0.80 0.87 0.80 0.74 0.71 0.70 0.74 0.72 0.69 0.72 0.71 0.72 0.96
No suchdecisionhas as yet been taken .
After Fine-tuning

Figure 5: Cosine similarities between subword representations in a parallel sentence pair before and after ﬁnetuning. Red boxes indicate the gold alignments.

