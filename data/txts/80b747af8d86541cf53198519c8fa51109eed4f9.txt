Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data

David Lowell Northeastern University lowell.d@husky.neu.edu

Brian E Howard Sciome LLC
brian.howard@sciome.com

Zachary C. Lipton Carnegie Mellon University
zlipton@cmu.edu

Byron C. Wallace Northeastern University b.wallace@northeastern.edu

arXiv:2010.11966v1 [cs.CL] 22 Oct 2020

Abstract
Unsupervised Data Augmentation (UDA) is a semi-supervised technique that applies a consistency loss to penalize differences between a model’s predictions on (a) observed (unlabeled) examples; and (b) corresponding ‘noised’ examples produced via data augmentation. While UDA has gained popularity for text classiﬁcation, open questions linger over which design decisions are necessary and over how to extend the method to sequence labeling tasks. In this paper, we re-examine UDA and demonstrate its efﬁcacy on several sequential tasks. Our main contribution is an empirical study of UDA to establish which components of the algorithm confer beneﬁts in NLP. Notably, although prior work has emphasized the use of clever augmentation techniques including back-translation, we ﬁnd that enforcing consistency between predictions assigned to observed and randomly substituted words often yields comparable (or greater) beneﬁts compared to these complex perturbation models. Furthermore, we ﬁnd that applying its consistency loss affords meaningful gains without any unlabeled data at all, i.e., in a standard supervised setting. In short: UDA need not be unsupervised, and does not require complex data augmentation to be effective.
1 Introduction
While the advent of large neural models has led to rapid progress on a wide spectrum of prediction benchmarks in NLP, these methods tend to require large amounts of training data. This limitation is particularly acute in domains such as information extraction from scientiﬁc documents, where unlabeled in-domain data is plentiful but labeled data is rare and requires signiﬁcant annotator experience to produce. The cost of acquiring data in such domains has spurred signiﬁcant interest in developing models that can achieve greater extraction accuracy,

even when the available labelled corpora are small (Nye et al., 2018; Maharana et al., 2018).
In this paper, we investigate unsupervised data augmentation (UDA) (Xie et al., 2019), a recently proposed semi-supervised learning method in which models are trained on both labeled and unlabeled in-domain data. The learning objective for the unlabeled component is to minimize the divergence between the model’s outputs on a given example and its outputs on a perturbed version of the same example. While this combination of data augmentation with a consistency loss was previously proposed as Invariant Representation Learning and demonstrated beneﬁcial in speech recognition by (Liang et al., 2018), UDA applies the method in a semisupervised setting, in a manner similar to virtual adversarial training (Miyato et al., 2018) and the authors demonstrated beneﬁts on both computer vision and Natural Language Processing (NLP) tasks.
Producing such perturbed examples requires specifying a data augmentation pipeline. Typically, these apply one or more transformations that (hopefully) tend not to alter the applicable label (Goodfellow et al., 2016). In computer vision, a number of straightforward and demonstrably effective data augmentation techniques, such as image ﬂipping, cropping, rotating, and various perturbations to the color spectrum have gained widespread adoption (Huang et al., 2016; Zagoruyko and Komodakis, 2016). More recently, these methods, among others, have been successfully applied in concert with UDA to improve performance on image classiﬁcation tasks (Xie et al., 2019).
By contrast, in natural language processing, there is less consensus about which perturbation models can be applied with conﬁdence that they will not change the applicability of the original label. To apply UDA in NLP, researchers have primarily focused on back-translation (Sennrich et al.,

2016; Edunov et al., 2018), generating paraphrases by applying a machine translation model to map a document into a pivot language and then back into the original language. In practice, this process produces augmentations of varying quality. Another problem is that back-translation is slow, and performance may depend on the choice of translation model. Incorporating large quantities of unlabeled data in the training process is also computationally expensive. Given these limitations—and to better characterize why and when UDA helps—we investigate whether the beneﬁts of UDA on NLP tasks can be achieved using less unlabeled data and/or simpler input perturbations. To this end, we investigate uniform random word replacement as an augmentation method. Random substitution for augmentation has been considered previously in the context of translation (Wang et al., 2018), and results using random replacement have recently been investigated in the UDA paper (Xie et al., 2019) for a single dataset on which it slightly underperforms back-translation. Here we deepen this analysis, showing that, surprisingly, random replacement is generally competitive with back-translation. Further, we ﬁnd that signiﬁcant increases in performance can be achieved by applying consistency loss on only small labeled data sets (although large volumes of in-domain unlabeled data provides further gains).
As an additional contribution, we adapt UDA to sequence tagging tasks, which are common in NLP. Back-translation is ill-suited to such tasks, because we lack alignment between spans of interest in the original text and the back-translated paraphrase. For these problems, we propose and evaluate word replacement augmentation strategies for sequence tagging. Interestingly, we observe that augmentation via uniform random word replacement yields improvements, but that it is more effective to employ a masked language model to predict ‘reasonable’ replacements for each word to be replaced.
2 Unsupervised Data Augmentation
UDA is a semi-supervised method in which a model is trained—in addition to the standard objectives on the labeled data—to make similar predictions for an observed example and a corresponding perturbed instance, produced via some data augmentation technique. Applying UDA requires specifying both (i) a consistency loss to be applied on a

(original, augmented) example pair; and (ii) a data augmentation technique to produce the perturbed examples in the ﬁrst place.
2.1 Consistency Loss
As originally proposed by Xie et al. (2019), UDA’s loss function is a sum over a supervised component and an unsupervised component. Assuming crossentropy loss for the former, the loss function is:

[− log(pˆ(y|x))] + λ [L(x)] (1)

x,y∈L

x∈U

where L is the set of labeled data, U is the set of unlabeled data, and λ weights the relative contribution of the unlabeled term to the total loss. The consistency loss L is deﬁned as the KL-Divergence between model predictions for the original and augmented examples.

L(x) = DKL{pˆ(y|x)||pˆ(y|q(x))} (2)

where q is a data perturbation operation and pˆ(y|x) is the probability distribution over labels output by the model given input x.
For sequence tagging tasks where examples correspond to multiple labels, we deﬁne the consistency loss as the average KL-Divergence between per-word model predictions for the original and augmented examples. Speciﬁcally, we replace the consistency loss above with

L(x) =

n j=1

DKL

(pˆ(yj

|x)||pˆ(yj

|q

(x)))

. (3)

n

Here, n denotes sequence length and pˆ(yj|x) are the predicted probabilities assigned by our model to labels corresponding to word j of sentence x.

2.2 Data Augmentation Strategies for Text
As deﬁned above, consistency loss (for both classiﬁcation and sequence tagging) requires specifying a data perturbation operation q that can be applied to observed instances, yielding a new, but similar example. If we assume that q transforms x such that q(x) and x share the same true label, then it is clearly desirable that a model would make similar predictions for x and q(x) (as encouraged by consistency loss).
However, there is a trade-off between the diversity of instances produced by q for x and the likelihood that these will share the same label as

x. For example, consider the strategy of paraphrasing via backtranslation. A valid paraphrase is one that, with high probability, shares the ground-truth label of the input; a diverse paraphrase is one that substantially differs from the original. Xie et al. (2019) observed that diversity is more important than validity when applying UDA to text classiﬁcation. This suggests that it may be possible to effectively use an alternative augmentation strategy that prioritizes diversity and simplicity at the expense of validity.
Uniform Random Word Replacement We propose a variant of q that performs a simple uniform random word replacement operation. Speciﬁcally, we deﬁne q(x) such that most of the time it copies directly from x, but with some probability p it replaces each xj in x with some other word xj drawn at random from the vocabulary of words that appear in U ∪ L. Formally:

q(x)j = xj with probability p

(4)

xj with probability 1 − p

This method is simple (and does not require a learned language model), but naive. It produces output which is diverse, but not necessarily valid or even grammatical. We compare this technique to two cleverer model-based data augmentation techniques: One for text classiﬁcation (proposed in prior work) and one suitable for sequence tagging (which we introduce here).
Back-Translation We use the back-translation machinery described by Xie et al. (2019). Speciﬁcally, this entails use of WMT’14 English-French (Bojar et al., 2014) translation models in both directions with random sampling with a tunable temperature in place of beam search for generation. We set our temperature to 0.9, one of the recommended settings in prior work (Xie et al., 2019).
Masked Language Model Back-translation is not suitable for use in UDA for sequence tagging tasks, as in these labels apply to tokens and it is not obvious how to align tokens in a given paraphrase with those in the original text. More speciﬁcally, as we have deﬁned it for sequence tagging (Equation 3), consistency loss penalizes dissimilarity between model predictions p(yj|x) and p(yj|q(x)) for all indices j. However, when q is deﬁned as a backtranslation process, there is no expectation that x’s and q(x)’s ground-truth labeling will be aligned.

They may not even be of the same length. Therefore, we instead consider word replacement strategies (at each index j), including (i.i.d.) random replacement, and a model-based word replacement strategy that attempts to ensure that the groundtruth labels of x and q(x) are aligned. Both of these involve individual word substitutions, so x and q(x) will have the same length.
For the model-based replacement strategy, we again deﬁne q such that it replaces a given word in x with probability p (otherwise copying from x). However, here we select xj using a masked language model. Speciﬁcally, we mask xj and use BERT (Devlin et al., 2018) to induce a probability distribution over all possible words (in its vocabularly) that might appear at position j. We then draw xj from the ten most probable words (excluding the original word xj) with probabilities proportional to the likelihood assigned to these words by BERT. We hypothesize that this method will provide a substantially greater expectation of validity than random replacement, on the assumption that BERT is sensitive enough to context that it is likely to replace words of one category with other words of the same category.
In Table 1 we show examples of random and BERT-based replacement of entities, in randomly selected sentence from CoNLL. As expected, random selection does not respect grammaticality or entity classes.BERT performs better, albeit imperfectly. For example, “aitken” is replaced with the ﬁrst token of a surname, “mc”, rather than a full surname and “antoine” is replaced with “he”. While the latter substitution is grammatical and semantically similar to the original, “he” is not considered a named entity in CoNLL.
3 Experimental Setup
We evaluate our proposed training method on four text classiﬁcation and three sequence tagging datasets. Of these, the classiﬁcation datasets include three benchmark sentiment sets (IMDB, Yelp, and Amazon), and one scientiﬁc classiﬁcation task (evidence inference). The sequence tagging datasets include one standard NER benchmark dataset (CoNLL-2003) and two scientiﬁc sequence labeling tasks (EBM-NLP and TAC). The scientiﬁc tasks are of particular interest for this work because it is expensive to collect annotations in these specialized domains.

ORG

PER PER

PER

aberdeen manager roy aitken said: “it’s unfortunate for us that antoine cannot play...

rangers Rand delegates

glen mc cancer peripheral

he 51,000

Table 1: Example replacements selected for entity tokens in a randomly selected sentence from CoNLL-2003 using different selection methods. In training, all words are equally likely to be selected for replacement. We focus on named entities in this example for illustrative purposes.

IMDB (Maas et al., 2011) is a sentiment classiﬁcation dataset consisting of movie reviews (25, 000 in the train set, 25, 000 in the test set) drawn from the IMDB website. Reviews with a score ≤ 4 out 10 are considered negative, those with scores ≥ 7 out of 10 are considered positive. Neutral reviews are not included.
Yelp (Zhang et al., 2015) is a sentiment classiﬁcation dataset comprising reviews drawn from Yelp (560, 000 in the train set, 38, 000 in the test set). One and two star reviews are considered negative. Three and four star reviews are considered positive.
Amazon (Zhang et al., 2015; McAuley and Leskovec, 2013) is a sentiment classiﬁcation dataset consisting of Amazon reviews (3, 600, 000 in the train set, 400, 000 in the test set). One and two star reviews are considered negative. Four and ﬁve star reviews are considered positive. Three star reviews are not included.
Evidence Inference We construct a classiﬁcation dataset derived from the Evidence Inference dataset (Lehman et al., 2019; DeYoung et al., 2020), a biomedical corpus in which the task is to infer the effect of an intervention on an outcome from an article describing a randomized controlled trial. The classes correspond to the intervention leading to a signiﬁcant increase, signiﬁcant decrease, or no signiﬁcant change in outcome. In the original task, the model must ﬁrst extract relevant evidence sentences from the full text article, and then make a prediction based on this. We evaluate in the ‘oracle’ setting, in which the model must only classify given relevant evidence sentences (∼17, 000 train examples, and ∼2, 000 instances in the test set).
CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) is an NER dataset consisting of annotated Reuters news articles (∼14, 000 sentences in the training set, ∼3, 000 in the test set), labeled with entity categories person, organization, location, and miscellaneous.

TAC (Schmitt et al., 2018) comprises annotated “materials and methods” sections from PubMed Central articles (∼5, 500 sentences in the training set, ∼6, 500 in the test set). Labels are available for 24 entity classes, of which we consider the two best represented: end point and test article.
EBMNLP EBMNLP (Nye et al., 2018) is a corpus of annotated abstracts drawn from medical articles describing medical randomized controlled trials (∼28, 000 sentences in the training set, ∼2, 000 in the test set). Spans are tagged as describing the patient population, the intervention studied, and the outcome measured in the trial being described.
For each dataset we simulate a pool of unlabeled data by hiding the annotations of the training set. We then create ﬁve distinct sets of labeled data (ten for sequence tagging tasks) by revealing the annotations for a random subset of the pool, forming a labeled training set L and an unlabeled training set U. For classiﬁcation tasks, we sample ten examples per class to form L, while for CoNLL and EBMNLP, we sample two hundred examples. For the smaller TAC dataset, we use one hundred.
Training details For each L, we then train a model both using only standard supervised learning over L and with an additional consistency loss, using either uniform random word replacement or more complex data augmentation technique (backtranslation for classiﬁcation; BERT-based replacement for sequence tagging). When training with consistency loss, we evaluate variants in which we apply this to both L and U, and where we apply it only over L. The latter corresponds to a standard supervised setting (with an additional loss term).
BERT’s pretraining task already incorporates unsupervised data (Devlin et al., 2018). We therefore also repeat the above experiments using ﬁnetuned weights instead of off-the-shelf pretrained weights. The ﬁnetuned weights are produced by training on BERT’s masked language model task with L ∪ U as the training data.
In our classiﬁcation experiments, we use a linear model on top of BERT (Devlin et al., 2018) as

a classiﬁer. For sequence tagging, we follow the architecture and hyperparameter choices in prior work Beltagy et al. (2019), which added a conditional random ﬁeld (Lafferty et al., 2001) on top of BERT representations. We train all models using Adam (Kingma and Ba, 2015) with a learning rate of 2e-05 for classiﬁcation and 1e-3 for sequence tagging.
In exploratory experiments we observed that model performance is relatively robust to the choice of λ (the weight assigned to the consistency loss term) when large quantities of unlabeled data are available. We therefore set λ to 1 in all of our semisupervised experiments. In out supervised learning only experiments, we found it necessary to compensate for the lack of unlabeled examples and the corresponding change in the relative weightings of standard and consistency losses. This can be effectively done by repeating labeled examples, imposing only the consistency loss for them, as though they were unlabeled. We use a ratio of 20 “unlabeled” examples to 1 labeled example in our supervised experiments.
In our augmentation procedure, we set p to 30%. For biomedical tasks (Evidence Inference, EBMNLP, TAC) model weights are initialized using SciBERT, a model pretrained over scientiﬁc papers (Beltagy et al., 2019). For all other tasks, we initialized parameters to the pretrained BERTBASE weights (Devlin et al., 2018). BERTBASE weights are used for CoNLL, and SciBERT weights are used for EBMNLP and TAC when using BERT as a masked-language model for data augmentation.
4 Results
4.1 Classiﬁcation
Figure 1 presents the results of our experiments for the classiﬁcation tasks. Both back-translation and random replacement perform well on the IMDB, Yelp, and Amazon datasets. Notably, random replacement consistently achieves results equivalent to or better than those attained with the more computationally complex back-translation method.
On the Evidence Inference task, UDA with backtranslation under-performs the supervised baseline, with a loss of 7.5 F1 when the full unlabeled dataset is used.This is perhaps unsurprising, given that the models used for back-translation were not trained on scientiﬁc text. These results suggest that the effectiveness of back-translation is contingent upon the domain similarity of the back-translation

model’s training data and that of the downstream task. By contrast, UDA with random replacement produces a modest but meaningful gain over the supervised baseline: 2.5 F1 with only the labeled data and 6 F1 with the full unlabeled dataset.
Across all classiﬁcation experiments—excepting Evidence Inference using back-translation— applying consistency loss to only the labeled data yields improvements over the supervised baseline, albeit less than what is achieved using the full amount of unsupervised data. Further, these gains are disproportionate to the quantity of data used to attain them. In the worst case (Yelp with back-translation), using only the supervised data results in only 20% of the potential performance improvement that could be attained using the full set of unlabeled data. In the best case (Amazon with random replacement), 48% of the potential gain can be achieved without using any unsupervised data at all, despite the fact that the labeled Amazon dataset represents less than 0.001% of the full set.
4.2 Sequence Tagging
Figure 2 presents results from sequence tagging experiments. BERT-based replacement provides a meaningful advantage over the supervised baseline on the CoNLL and TAC datasets. Random replacement also offers gains, but these are smaller and less consistent. Without access to the full unlabeled dataset, random replacement results in a small decrease in performance. With access to unlabeled data, it produces only a small beneﬁt on CoNLL. The gain on TAC is larger, but still smaller than that achieved using BERT-based replacement.
BERT-based replacement is more effective than random replacement for sequence tagging. But that random replacement provides any beneﬁts at all for such tasks is perhaps counter-intuitive, given that predictions are made at the word level for these tasks. It is therefore likely that random replacement will lead to a change in ground truth labeling for any replaced entities. We hypothesize that this training encourages the model to place greater weight on the context in which words appear. This may render models more robust in being able to recognize unfamiliar entities based on the contexts in which they appear.
UDA does not offer performance gains on the EBMNLP dataset using either augmentation strategy. When consistency loss is applied to only the

Accuracy Accuracy Accuracy F1 Score

90

Supervised Baseline

91.1 91.0

Back-translation

85

Random Replacement

80

75

70 66.0 66.0 65

60 59.0
Baseline

(SupUeDrvAised) (SemisUuDpeArvised)

(a) IMDB

100 95 90 85 80 75 72.6 70
65 Baseline

94.6 96.7 77.0 78.1
(SupUeDrvAised) (SemisUuDpeArvised)

(b) Yelp

95
90
85
80
75
70 65 64.7
Baseline

94.9 94.4 78.9 76.1
(SupUeDrvAised) (SemisUuDpeArvised)

(c) Amazon

75 70 65 60 57.1 55 50 45
40 Baseline

59.6 55.4

63.1 49.6

(SupUeDrvAised) (SemisUuDpeArvised)

(d) Evidence Inference

Figure 1: Comparison of performance achieved on classiﬁcation tasks using different variants of UDA. Each bar represents the average performance across ﬁve sets of labeled data (labeled data quantity is noted parenthetically). The supervised baseline represents standard ML on the supervised data set only, without any consistency loss. Supervised with consistency loss represents use of consistency loss, but only over the labeled data, with the unlabeled data discarded. Semisupervised with consistency loss represents use of consistency loss over the entire dataset, both labeled and unlabeled.

F1 Score F1 Score F1 Score

82 80 78 76 75.2 74 72
Baseline

Supervised Baseline Bert Replacement Random Replacement
78.9

75.7

76.4

74.5

(SupUeDrvAised) (SemisUuDpeArvised)

(a) CoNLL

42
40
38
36
34 32 32.2
Baseline

42.4
38.7
34.0 31.4
(SupUeDrvAised) (SemisUuDpeArvised)

(b) TAC

68 66 65.7 64 62 60 58
Baseline

65.4 65.7

62.3 59.4

(SupUeDrvAised) (SemisUuDpeArvised)

(c) EBMNLP

Figure 2: Comparison of performance achieved on sequence tagging tasks using different variants of UDA. Each bar represents the average performance across ten sets of labeled data (labeled data quantity is noted in parenthesis). The supervised baseline represents standard ML on the supervised data set only, without any consistency loss. Supervised with consistency loss represents use of consistency loss, but only over the labeled data, with the unlabeled data discarded. Semisupervised with consistency loss represents use of consistency loss over the entire dataset, both labeled and unlabeled.

labeled data, the performance is largely unchanged. However, when unlabeled data is incorporated, performance decreases by 3.4 F1 when using BERTbased replacement, and 6.3 F1 when using random replacement. Crowdsourced (lay) workers annotated EBMNLP’s training data, while doctors annotated the test data (Nye et al., 2018), and we speculate that this may play a role in the difference in observed performance, as we observed similar gains to those attained on CoNLL and TAC when performing exploratory studies on a development set. It may be that UDA performs poorly on EBMNLP relative to the supervised learning baseline because relying more heavily on context is harmful when the training set annotations are noisy. We note that the lay training set annotators consistently included more words in their labeled entity spans than the test set annotators (see Table 2). This may indicate that the training set annotators included context words which do not truly belong to an entity class in their spans. Encouraging the model to infer the implications of context

P IO Train 8.2 3.9 4.8 Test 6.5 1.8 3.7
Table 2: Average length of PIO spans in words for EBMNLP’s train and test sets
based on these misidentiﬁed context words may be compounding that error.
Our results indicate that unlabeled data is more critical for UDA in sequence tagging than in classiﬁcation. Here, in the best case (TAC with Replacement) we see UDA without unlabeled data achieves only 18% of the performance increse that may ultimately be achieved by including unlabeled data. This is lower than the worst case observed in the classiﬁcation task.
4.3 Varying Quantities of Labeled and Unlabeled Data
We also examine the question: how much unlabeled data is necessary? Incorporating additional unlabeled data extends the training process and we

Accuracy F1 Score
Accuracy

95.0 92.5 90.0 87.5 85.0 82.5 80.0 77.5 75.0
2 × 101

2U×n1l0a2bele2d×D1a0t3aset2S×iz1e04

2 × 105

74 72 70 68 66
1 × 102

Unlabele1d×D1a0t3aset Size

1 × 104

(a) Yelp

(b) CoNLL

Figure 3: Comparison of performance achieved using varying quantities of unlabeled data. Curves are averaged across 5 experiments for Yelp, and 10 for CoNLL. Each labeled dataset consists of 20 labeled examples for Yelp and 100 examples for CoNLL. Random replacement is used as the augmentation method for Yelp and BERT-based replacement is used as the augmentation method for CoNLL.

Semi-supervised UDA

Supervised UDA

80

Baseline

60

40

20

F1 Score

1 × 101

1 × 102

1 × 103

Labeled Dataset Size

1 × 104

Figure 4: Analysis of performance achieved using varying quantities of labeled data on the CoNLL set. The blue solid line represents the case that UDA is used and all unlabeled data is incorporated in training. The orange dashed line represents the case that UDA is used, but with only the labeled data. The green dotted line represents training without any consistency loss. Each curve is averaged across ten experiments. BERT-based replacement is used as the augmentation method.

hypothesize that, at some point, we will observe diminishing returns. To this end, we run experiments varying the quantity of unlabeled data used when training on the Yelp and CoNLL datasets. These results are presented in ﬁgure 3. For yelp, we begin to observe diminishing returns as we approach use of the full unlabeled set. Interestingly, on CoNLL, too high a quantity of unlabeled data appears to actually degrade performance.
We also analyze the change in performance when varying the amount of labeled data used for sequence labeling tasks. To investigate this, we train using UDA with 10, 100, 1000 or 10000 labeled sentences drawn from CoNLL. Results from these experiments are presented in Figure 4. We observe

100

Base Bert Weights Finetuned Bert Weights

91.1 93.1 91.0 93.0

90

85.5

88.0

80 73.9

70

66.0

66.0

60 59.0

50 upervaisseeldine SB

upervsliasetidon) (BackS-tran

pervnisdeodm) Su (Ra

upervsliasetidon) Seamciks-tran (B

pervnisdeodm) Semisu (Ra

Figure 5: Performance on IMDB using off-the-shelf pretrained BERT weights compared to BERT weights ﬁnetuned to IMDB (i.e., after continuing pretraining BERT on IMDB.

consistent, modest gains from using UDA in a semisupervised fashion, excepting the extreme ends of the curve where almost all or almost none of the data is labeled. In these two cases, we see performance with UDA and without UDA converge.
4.4 Finetuning BERT Weights
BERT’s pretraining tasks (masked-language modeling and next sentence prediction) already provide a method for incorporating unlabeled data (Devlin et al., 2018). Given this, we ﬁnetune by training BERT’s pretraining tasks on the full unlabeled datasets. We then investigate the resulting performance on the downstream tasks, to determine whether there is still a beneﬁt to using UDA with the full unlabeled dataset after that data has been incorporated into the model via ﬁnetuning.
Figure 5 presents results from this experiment for the IMDB dataset. Our results show that, when BERT’s weights have already been ﬁnetuned on the unlabeled data, incorporating that data again when training with UDA is less valuable. Applying UDA using only the labeled data and random replacement allows us to realize 74% of the possible performance increase when using the full unlabeled data. This is compared to only 22% when BERT has not been ﬁnetuned.
However, we still observe that performance gains do continue to accrue when unlabeled data is incorporated into UDA training, even when the BERT weights have been ﬁnetuned. Since training with UDA is comparatively computationally inexpensive to robustly ﬁnetuning BERT, it is likely

with CSoenmsisistuenpceyrvLisoesds (Random)
with CSoenmsisistuenpceyrvLisoesds (Back-translation)
CSounpsiesrtveinsecyd Lwoisths (Random)
CSounpsiesrtveinsecyd Lwoisths (Back-translation)
SupBearsveilsinede 55 60 65 70 75 80 85 90 95 Accuracy
Figure 6: Performance ranges on the Amazon dataset with spans indicating the minimum-to-maximum performance over 5 independent samples (of the labeled subset). Triangles indicate means.
practical and advantageous to use both in concert.
4.5 Variability of Results
Throughout our experiments we observed that performance varies greatly with the choice of data to label. Figure 6 illustrates the range of observed results on the Amazon dataset.1 The difference between the maximum performance and the minimum performance in the supervised baseline is 17.8 points of accuracy. The delta for UDA using only labeled data and random word replacement is even higher, at 21.4 points of accuracy. This has important implications for a practitioner: While one might reasonably have an expectation of achieving high performance on average, in practice only a single labeled dataset will be constructed and used for training. Our results show that, in a low resource classiﬁcation setting, such a practitioner might actually achieve signiﬁcantly lower or higher performance than expected.
This illustrates a further advantage of UDA. When exploiting a large quantity of unlabeled data, performance not only improves, but becomes consistent across labeled dataset choices as well. We observe similar trends across all classiﬁcation datasets, with the exception of Evidence Inference with random replacement, for which the variability of results remains relatively high, even when all unlabeled data is employed.
By contrast, we do not observe such trends in the sequence tagging tasks, where UDA variant choice does not consistently effect the variability
1Similar plots for other datasets are presented in the Appendix.

of performances across labeled set choices.
5 Conclusions
In this paper we have evaluated and extended Unsupervised Data Augmentation (UDA) in the context of NLP tasks. We proposed and evaluated new approaches for UDA suitable to classiﬁcation tasks and extended it to sequence tagging tasks by imposing a consistency loss over word label distributions. We showed that naive data augmentation methods may be just as effective as the complex, model based augmentations currently in use, and that performance improvements may still be attained even in the absence of any unlabeled data.
More speciﬁcally, we proposed a simple, effective augmentation method: randomly replace words with other words. The replacement word may be selected either uniformly at random, or by using BERT (Devlin et al., 2018) as a masked language model to induce a probability distribution over tokens.We found that the former method is effective for classiﬁcation tasks, and the latter for sequence tagging. We further investigated the practicality of using UDA without unlabeled data, applying a consistency loss only to a small labeled dataset. We experimentally evaluated various augmentation strategies and settings on four classiﬁcation datasets and three sequence tagging datasets.
We found reliable performance increases on all four classiﬁcation datasets. For classiﬁcation, we found that random word replacement is as effective as—and sometimes more effective than—backtranslation, which is what has been proposed in prior work Xie et al. (2019). In particular, random word-replacement is effective on our scientiﬁc classiﬁcation task, where back-translation is ineffective, perhaps due to its reliance on machine translation models not trained with scientiﬁc literature.
We found that both random replacement and BERT-based replacement are effective on two out of three sequence tagging tasks, with BERT-based replacement that we have proposed consistently outperforming random replacement. On the third sequence tagging dataset, we observed a degradation of performance when using any variety of UDA, which we hypothesize owes to the noisy annotation of this training set.
We found that UDA may produce meaningful increases in performance even when unlabeled data is not available, particularly if the weights have already been ﬁnetuned to the task. The magnitude of

this increase depends upon the task, and may be relatively large (as in the case of the Amazon dataset) or relatively small (as in the CoNLL dataset). In general this approach is more effective for classiﬁcation tasks than sequence tagging tasks.
To summarize our ﬁndings: UDA is effective in low-supervision natural language tasks, even when used with naive augmentation methods and without unlabeled data.
References
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: Pretrained language model for scientiﬁc text. In EMNLP.
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Alesˇ Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Jay DeYoung, Eric Lehman, Ben Nye, Iain J. Marshall, and Byron C. Wallace. 2020. Evidence inference 2.0: More data, better models.
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500, Brussels, Belgium. Association for Computational Linguistics.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2016. Densely connected convolutional networks.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C Wallace. 2019. Inferring which medical treatments work from reports of clinical trials. arXiv preprint arXiv:1904.01606.
Davis Liang, Zhiheng Huang, and Zachary C Lipton. 2018. Learning noise-invariant representations for robust speech recognition. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 56–63. IEEE.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.
Adyasha Maharana, Arpit Tandon, Eric Wimberley, Mihir Shah, Ruchir Shah, and Brian E Howard. 2018. A pragmatic approach to information extraction for systematic review.
Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13, page 165–172, New York, NY, USA. Association for Computing Machinery.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual adversarial training: a regularization method for supervised and semisupervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979– 1993.
Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain James Marshall, Ani Nenkova, and Byron C. Wallace. 2018. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. CoRR, abs/1806.04185.
Charles Schmitt, Vickie Walker, Ashley Williams, Arun Varghese, Yousuf Ahmad, Andy Rooney, and Mary Wolfe. 2018. Overview of the tac 2018 systematic review information extraction track.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86–96, Berlin, Germany. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.

Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut: an efﬁcient data augmentation algorithm for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856–861, Brussels, Belgium. Association for Computational Linguistics.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848.
Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. CoRR, abs/1605.07146.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation.

