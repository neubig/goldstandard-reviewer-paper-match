One size does not ﬁt all: Investigating strategies for differentially-private learning across NLP tasks
Manuel Senge and Timour Igamberdiev and Ivan Habernal Trustworthy Human Language Technologies Department of Computer Science Technical University of Darmstadt manuel.senge@web.de
{timour.igamberdiev, ivan.habernal}@tu-darmstadt.de www.trusthlt.org

arXiv:2112.08159v1 [cs.CL] 15 Dec 2021

Abstract
Preserving privacy in training modern NLP models comes at a cost. We know that stricter privacy guarantees in differentiallyprivate stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efﬁciency of DPSGD in NLP is inconclusive or even counterintuitive. In this short paper, we provide a thorough analysis of different privacy preserving strategies on seven downstream datasets in ﬁve different ‘typical’ NLP tasks with varying complexity using modern neural models. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance.
1 Introduction
In a world where “data is the new oil," preserving individual privacy is becoming increasingly important. However, modern pre-trained neural networks such as GPT-2 (Radford et al., 2019) are vulnerable to privacy attacks that could even reveal verbatim training data (Carlini et al., 2020).
An established method for protecting privacy using the differential privacy (DP) paradigm (Dwork and Roth, 2013) is to train networks with differentially private stochastic gradient descent (DPSGD) (Abadi et al., 2016). Although DP-SGD has been adopted to language modeling (McMahan et al., 2018; Hoory et al., 2021), the community lacks a thorough understanding of its usability across different NLP tasks. Some recent observations even seem counter-intuitive, such as the non-decreasing performance at extremely strict privacy values in named entity recognition (Jana and Biemann, 2021). As such, existing research on the suitability of DP-SGD for various NLP tasks remains largely inconclusive.

We thus asked the following research questions: Which models and training strategies provide the best trade-off between privacy and performance on different typical NLP tasks? And how exactly do increasing privacy requirements hurt the performance? To answer these questions, we conduct extensive experiments on seven datasets over ﬁve tasks, with several contemporary models and varying privacy regimes. This paper contributes to the NLP community by developing a deeper understanding of the various challenges that each task poses to privacy-preserving learning.1
2 Related work
A primer on differential privacy and an explanation of DP-SGD would unfortunately go beyond the scope of this short paper; please refer to (Igamberdiev and Habernal, 2021; Habernal, 2021; Abadi et al., 2016).
In NLP, several studies have utilized the DPSGD methodology, primarily for training language models. For instance, Kerrigan et al. (2020) study the effect of using DP-SGD on a GPT-2 model, as well as two simple feed-forward networks, pretraining on a large public dataset and ﬁne-tuning with differential privacy. Model perplexities are reported on the pre-trained models, but there are no additional experiments on downstream tasks. McMahan et al. (2018) train a differentially private LSTM language model, showing that it can behave similarly to models without DP when using a large enough dataset, with comparable accuracy. Hoory et al. (2021) train a differentially private BERT model using the MIMIC-III dataset (Johnson et al., 2016) and a privacy budget ε = 1.1, achieving comparable accuracy to the non-DP setting for a Medical Entity Extraction task.
There are only a few works that have investigated DP-SGD for speciﬁc downstream tasks in
1Code and data at https://github.com/trusthl t/dp-across-nlp-tasks

NLP. Jana and Biemann (2021) looked into the behavior of differential privacy on the CoNLL 2003 English NER dataset. Their ﬁndings show that no signiﬁcant drop is expected, even when using low epsilon values such as 1, and even as low as 0.022. This is a very unusual result and is assessed in our current work further below. Bagdasaryan et al. (2019) apply DP-SGD, among other tasks, to sentiment analysis of Tweets, reporting a very small drop in accuracy with epsilons of 8.99 and 3.87. With studies up to now being limited to only a few tasks and datasets, using disparate privacy budgets and metrics, there is a need for a more general investigation of the DP-SGD framework in the NLP domain.
3 Experimental setup
3.1 Tasks and dataset
We conducted experiments on seven widely-used datasets covering ﬁve different ‘typical’ NLP tasks. These include sentiment analysis (SA) of movie reviews (Maas et al., 2011) and natural language inference (NLI) (Bowman et al., 2015) as text classiﬁcation problems. For sequence tagging, we explored two tasks, in particular named entity recognition (NER) on CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) and Wikiann (Pan et al., 2017; Rahimi et al., 2019) and part-of-speech tagging (POS) on GUM (Zeldes, 2017) and EWT (Silveira et al., 2014). The third category is question answering (QA) on SQuAD 2.0 (Rajpurkar et al., 2018). We chose two sequence tagging tasks, each involving two datasets, to shed light on the ‘misteriously’ good performance on CoNLL in (Jana and Biemann, 2021). Table 1 summarizes the main properties of the datasets.

Task SA NLI
NER NER POS POS
QA

Dataset IMDb SNLI
CoNLL’03 Wikiann
GUM EWT
SQuAD 2.0

Size 50k documents
570k pairs
≈ 300k tokens ≈ 320k tokens ≈ 150k tokens ≈ 254k tokens
150k questions

Classes 2 3
9 7 17 17

Table 1: Datasets and their speciﬁcs. SQuAD contains 100k answerable and 50k unanswerable questions, where answerable questions are represented as the answer span positions.

3.2 Models and training strategies
We experimented with ﬁve different training (ﬁnetuning) strategies over two base models. As a simple baseline, we opted for (1) Bi-LSTM to achieve compatibility on NER with previous work (Jana and Biemann, 2021). Further, we employed BERTbase with different ﬁne-tuning approaches. We added (2) LSTM on top of ‘frozen’ BERT as in (Fang et al., 2020) (Tr/No/LSTM), a (3) simple softmax layer on top of ‘frozen’ BERT (Tr/No), and the same conﬁguration with (4) ﬁne-tuning only the last two layers of BERT (Tr/Last2) and ﬁnally (5) ﬁne-tuning complete BERT without the input embeddings layer (Tr/All).
In contrast to the non-private setup, the number of trainable parameters of these different models makes a big difference in DP-SGD, since the privacy-preserving noise added to the gradient grows with the gradient size and thus affects the performance.
4 Analysis of results
We will analyze in detail only transformer-based models, as LSTM performed worse across all setups. Also, we discuss only ε of 1.0, 5.0, and ∞ (non-private) as three representative privacy budgets. Random and majority baselines are reported only in the accompanying materials as they play a minor role in the drawn conclusions. All results are reported as macro-F1 (average of F1 per class).
4.1 Sentiment analysis
Sentiment analysis can be considered a less complex task, as the model only predicts two classes (positive, negative). Unsurprisingly, each nonprivate model achieves good results (Figure 1 left). Moreover, each model – except for the fully ﬁnetuned BERT model Tr/All – shows only a small performance drop with DP.
What is wrong with ﬁnetuning full BERT with DP-SGD? While fully ﬁne-tuned BERT is superior in non-private setup, DP-SGD behaves unexpectedly. Instead of making prediction more random with increasing DP noise, it simply predicts everything as negative with F1 = 0.46, even though the training data is well-balanced; see the confusion matrix in Table 4 in the Appendix. By contrast, ﬁne-tuning only the last two layers (Tr/Last2) achieves F1 > 0.7. This might be explained by the observation that semantics is spread across the

Figure 1: Macro F1 scores for non-private (ε = ∞) and two private conﬁgurations (ε ∈ {5; 1}) grouped by a particular model as explained in Section 3.2. A complementary task-speciﬁc plot is in Fig. 2 in the Appendix.

entire BERT model while higher layers are task speciﬁc (Rogers et al., 2020, Sec. 4.3) and that sentiment might be well predicted by local features (Madasu and Anvesh Rao, 2019) which are destroyed by the noise.

↓ gold
Entailment Contradiction
Neutral

Entail.
1356 1122 1217

Contrad.
1699 1780 1677

Neutral
313 335 325

4.2 Natural Language Inference
As opposed to sentiment analysis, NLI results show a different pattern. In the non-private setup, two models including some sort of ﬁne-tuning BERT, namely Tr/Last2 and Tr/All, outperform other models. This is not the case for DP training.

Table 2: Conf. matrix for Tr/Last2; ε = 1 on NLI.

↓ gold
Entailment Contradiction
Neutral

Entail.
2832 272 375

Contrad.
129 2530
604

Neutral
407 435 2240

What happened to BERT with DP on the last layers? Fine-tuning only last two layers (Tr/Last2) results in the worst performance in private regimes, e.g., for ε = 1 (Fig. 1 right). Our analysis of the confusion matrix shows that this model fails to predict neutral entailment, as shown in Table 2. We hypothesize that the DP noise destroys the higher, task-speciﬁc BERT layers, and thus fails on the complexity of the NLI task, that is to recognize cross-sentence linguistic and commonsense understanding. Furthermore, it might be easier to recognize entailment or contradiction using some linguistic ‘shortcuts’, e.g., similar words pairs, or dataset artifacts (Gururangan et al., 2018).
On the other hand, full ﬁne-tuning BERT (Tr/All) gives the best private performance, as can be also seen in Table 3 (contrast with Table 2). This shows that noisy gradient training spread across the full model makes it more robust for the down-stream task which, unlike sentiment analysis, does not depend on ‘local‘ features, e.g., word n-grams.

Table 3: Conf. matrix for Tr/All; ε = 1 on NLI.
4.3 Named Entity Recognition and POS-Tagging
While the SA and NLI datasets contain a wellbalanced class distribution, the four datasets chosen for the two sequence tagging tasks are heavily skewed (see Tables 5 and 6 in the Appendix). Our results show that this imbalance negatively affects all private models. Both NER and POStagging behave similarly when being exposed to DP, as only the most common tags are well predicted, namely the outside tag for NER and the tags for noun, punctuation, verb, pronoun, adposition, and determiner for POS-tagging. Tables 7 and 8 in the Appendix show the extreme differences in F1-scores for each class with ε = 1. Our ﬁndings on the effects of class imbalance are complementary to (Bagdasaryan et al., 2019) who showed negative impact of DP on underrepresented implicit

sub-groups in the feature space.2
Drawing conclusions using unsuitable metrics? While the average of all class F1-scores (macro F1-score) suffers from wrong predictions of underrepresented classes, accuracy remains unaffected. Therefore we suggest using macro F1 to evaluate differential private models which are trained on imbalanced datasets. The difference in accuracybased metric and macro F1 score explains the unintuitive invariance of NER to DP in (Jana and Biemann, 2021).
Non-private NER messes up preﬁx but DP fails on tag type. When closer observing NER, we show that without DP the model tends to correctly classify the type of tag (e.g. LOC, PER, ORG, MISC) but sometimes fails with the position (I, B preﬁx). This can be seen in the confusion matrix (Table 9 in the Appendix), examples being ‘I-LOC’ is very often falsely classiﬁed as ‘B-LOC’ or ‘IPER’ as ‘B-PER’. The same pattern is present for ‘I-ORG’, and ‘I-MISC’. However, when introducing DP affects the models’ predictions even further as now additionally to the position the tag itself gets falsely predicted. To exemplify, ‘I-MISC’ is falsely predicted as B-LOC’ 502 times and ‘I-ORG’ as ‘B-LOC’ 763 times, as can be seen in Table 10 in the Appendix.
NER is majority-voting with meaningful ε already, turns random only with very low ε. As we sketched in the introduction, Jana and Biemann (2021) showed that a differential private BiLSTM trained on NER shows almost no difference in accuracy compared to the non-private model. However, our experiments show that even with ε = 1 the model almost always predicts the outside tag, as can be seen in the confusion matrix in Table 11 in the Appendix. As mentioned before, the accuracy does not shift much since the outside tag is the majority. Yet, the F1-score more accurately evaluates the models, revealing the misclassiﬁcations (CoNLL accuracy: 0.81 vs. F1: 0.20; Wikiann accuracy: 0.51 vs. F1: 0.1).3 Even when choosing much smaller epsilons this behavior stays the same. We only were able to get worse results with an extremely low privacy budget ε = 0.00837 which
2A fraction of Tweets using African-American English vs. the majority using standard American English in sentiment analysis
3In the non-private setup, this discrepancy remains but it becomes obvious that a more complex model (e.g., BERT) solves the task better.

renders the model just random (see Table 12 in the Appendix).
4.4 Question Answering
Whereas models with more ﬁne-tuned layers improve non-private predictions (Tr/No < Tr/Last2 < Tr/All), with DP all models drop to 0.5 F1, no matter how strict the privacy is (ε = 1, 2, 5). We thus analyzed the predictions and found that throughout all DP models almost all questions are predicted as unanswerable. Since 50% of the test set is labeled as such, this solution allows the model to reach a 0.5 F1 score.4 This behavior mirrors the pattern observed in NER and POS-tagging as the model tries to ﬁnd a majority class once it is exposed to higher levels of noise in the DP setting. Overall this task is relatively challenging, with many possible output classes in the span prediction process. We suggest future work to consider investigating SQuAD version 1.0, as there are no unanswerable questions.
4.5 Performance drop with stricter privacy
While a performance drop is unsurprising with decreasing ε, there is no consistent pattern among tasks and models (see Fig. 2 in the Appendix). For instance, while the fully ﬁnetuned BERT model experiences a relatively large drop for sentiment analysis, its drop for NLI is almost negligible. The actual choice of the model should be therefore taken with a speciﬁc privacy requirement in mind.
5 Conclusion
We explored differentially-private training on seven down stream NLP datasets and found that (1) skewed class distribution, which is inherent to many NLP tasks, hurts performance with DP-SGD, (2) ﬁne-tuning and thus noisifying different transformer’s layers affects task-speciﬁc behavior so that no single approach generalizes over various tasks, unlike in a typical non-private setup, and (3) previous works have misinterpreted private NER due to an unsuitable evaluation metric. Future work should look into combining DP-SGD training with explainability of transformers to gain further insights.
4Here we used the ofﬁcial evaluation script for SQuAD 2.0

References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308–318, Vienna, Austria. ACM.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. 2019. Differential Privacy Has Disparate Impact on Model Accuracy. In Advances in Neural Information Processing Systems 32, pages 15479–15488, Vancouver, Canada. Curran Associates, Inc.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Association for Computational Linguistics.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting Training Data from Large Language Models. arXiv preprint.
Cynthia Dwork and Aaron Roth. 2013. The Algorithmic Foundations of Differential Privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4):211–407.
Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2020. Hierarchical Graph Network for Multi-hop Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8823–8838, Online. Association for Computational Linguistics.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation Artifacts in Natural Language Inference Data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107–112, New Orleans, LA. Association for Computational Linguistics.
Ivan Habernal. 2021. When differential privacy meets NLP: The devil is in the detail. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1522–1528, Punta Cana, Dominican Republic. Association for Computational Linguistics.
Shlomo Hoory, Amir Feder, Avichai Tendler, Soﬁa Erell, Alon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, and Yossi Matias. 2021. Learning and

Evaluating a Differentially Private Pre-trained Language Model. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1178–1189, Punta Cana, Dominican Republic. Association for Computational Linguistics.
Timour Igamberdiev and Ivan Habernal. 2021. PrivacyPreserving Graph Convolutional Networks for Text Classiﬁcation. arXiv preprint.
Abhik Jana and Chris Biemann. 2021. An Investigation towards Differentially Private Sequence Tagging in a Federated Framework. In Proceedings of the Third Workshop on Privacy in Natural Language Processing, pages 30–35, Online. Association for Computational Linguistics.
Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimiciii, a freely accessible critical care database. Scientiﬁc data, 3(1):1–9.
Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. Differentially private language models beneﬁt from public pre-training. CoRR, abs/2009.05886.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.
Avinash Madasu and Vijjini Anvesh Rao. 2019. Sequential Learning of Convolutional Features for Effective Text Classiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5657–5666, Hong Kong, China. Association for Computational Linguistics.
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learning Differentially Private Recurrent Language Models. In Proceedings of the 6th International Conference on Learning Representations, pages 1–14, Vancouver, BC, Canada.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Crosslingual Name Tagging and Linking for 282 Languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946–1958, Vancouver, Canada. Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. Technical report, OpenAI, San Francisco, California, USA.

Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Massively Multilingual Transfer for NER. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguistics.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784– 789, Melbourne, Australia. Association for Computational Linguistics.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology: What We Know About How BERT Works. Transactions of the Association for Computational Linguistics, 8:842–866.
Natalia Silveira, Timothy Dozat, Marie Catherine De Marneffe, Samuel R. Bowman, Miriam Connor, John Bauer, and Christopher D. Manning. 2014. A gold standard dependency corpus for English. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897–2904, Reykjavik, Iceland. European Language Resources Association (ELRA).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.
Amir Zeldes. 2017. The GUM corpus: creating multilayer resources in the classroom. Language Resources and Evaluation, 51(3):581–612.
A Hyperparametertuning and Privacy Settings
For each model, hyperparameter tuning is conducted, and learning rates in the range of 0.1 an 10−5 are tested. The batch size is set to 32 unless differential privacy and ﬁnetuning prohibit such a large size due to memory consumption. If this is the case the batch size is reduced by a power of 2. As the randomized response (section 3.2 in (Dwork and Roth, 2013)) is considered good privacy and corresponds to having an epsilon of ln(3) ≈ 1.1, we conduct our experiments with an epsilon of 1. Moreover, as this work aims to understand the behavior of differential privacy we additionally consider experiments with an epsilon of 2 and 5. To vary the degree of privacy applied to the model. Throughout all our experiments, we set the delta to 10−5.
To implement differential privacy, Opacus is used. To calculate the amount of noise that needs to

be added to reach a certain level of ( ,δ)-differential privacy, Tensorﬂow Privacy is chosen.
B Detailed tables and ﬁgures
↓ gold Neg Pos Neg 12497 3 Pos 12497 2
Table 4: Confusion matrix for the fully ﬁne-tuned BERT model Tr/All with ε = 1 on sentiment analysis.

dataset
NOUN PUNCT VERB PRON
ADP DET PROPN ADJ AUX ADV CCONJ PART NUM SCONJ
X INTJ SYM

GUM

train test

17,873 13,650 10,957 7,597 10,237 8,334 7,066 6,974 4,791 4,180 3,247 2,369 2,096 2,095
244 392 156

2,942 1,985 1,647 1,128 1,698 1,347 1,230 1,116 719 602 587 335 333 251
24 87 35

EWT

train test

34,781 23,679 23,081 18,577 17,638 16,285 12,946 12,477 12,343 10,548 6,707 5,567 3,999 3,843
847 688 599

4,132 3,106 2,655 2,158 2,018 1,896 2,076 1,693 1,495 1,225 739 630 536 387 139 120
92

Table 5: Distribution of all possible tags for POSTagging

1.0
0.8
0.6
0.4 1
1.0 0.8 0.6 0.4 0.2
1

SA GUM

1.0

0.8

0.6

0.4

0.2

10

1

1.0

0.8

0.6

0.4

10

1

NLI EWT

0.4

0.3

0.2

0.1

10

1

0.8

CoNLL QA

0.6

0.4

10

1

Wiki
1.0

0.8

0.6

0.4

10

1

10

Tr/No/LSTM Tr/No Tr/Last2 Tr/All
10

Figure 2: Comparison of performances (macro F1 score) per dataset with varying privacy budget ε ∈ {1, 2, 5, ∞} on the x-axis (note the log scale).

dataset
O I-LOC B-PER I-PER I-ORG I-MISC B-MISC B-LOC B-ORG

CoNLL’03

test train

38,554 1,919
0 2,773 2,491 909
9 6 5

170,524 1,157 6,600 4,528 3,704 1,155 3,438 7,140 6,321

Wikiann

test train

42,879 6,661 4,649 7,721 11,825
– – 5,023 4,974

85,665 13,664 9,345 15,085 23,668
– – 10,081 9,910

Table 6: Distribution of all possible tags for NER

O I-LOC B-PER I-PER I-ORG I-MISC B-MISC B-LOC B-ORG

CoNLL’03
0.98 0.00 0.00 0.67 0.01 0.00 0.00 0.00 0.00

Wikiann
0.86 0.46 0.69 0.57 0.54
– – 0.44 0.14

Table 7: F1-scores per class for NER using the fully ﬁne-tuned BERT Tr/All and ε = 1. Except for the O tag (highlighted) and I-PER, no other class is predicted with sufﬁcient performance on CoNLL.

NOUN PUNCT VERB PRON
ADP DET PROPN ADJ AUX ADV CCONJ PART NUM SCONJ
X INTJ SYM

GUM
0.66 0.85 0.64 0.65 0.73 0.81 0.17 0.13 0.41 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00

EWT
0.62 0.87 0.72 0.72 0.80 0.83 0.16 0.03 0.69 0.10 0.02 0.00 0.00 0.00 0.00 0.00 0.00

Table 8: F1-scores per class for POS-tagging using the fully ﬁne-tuned BERT Tr/All and ε = 1. Highlighted rows achieve usable performance.

↓ gold
O B-PER I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC

O
38207 0 17 0 42 0 22 0 51

B-PER
26 0
1556 0 27 0 7 0 9

I-PER
3 0 1150 0 3 1 3 0 3

B-ORG
47 0 23 0 1514 0 49 0 32

I-ORG
41 0 10 5 767 2 12 0 9

B-LOC
19 0
15 0 53 0 1558 0 23

I-LOC
9 0 1 0 29 1 232 0 4

B-MISC
66 0 1 0 42 0 30 4 594

I-MISC
80 0 0 0 14 2 5 5 183

Table 9: Confusion matrix for NER on CoNLL’03 for the fully-ﬁnetuned BERT model. It can be seen (highlighted) that the model sometimes falsely predicts the position of the tag. Yet, the tag itself is mostly correctly classiﬁed.

↓ gold
O B-PER I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC

O
37540 0 33 0
192 0 58 1
178

B-PER
61 0
1057 0 90 1 14 2 21

I-PER
38 0 1521 0 142 2 38 0 14

B-ORG
149 0
77 4 1157 2 169 1 110

I-ORG
0 0 0 0 9 0 0 0 0

B-LOC
304 0
48 1 763 1 1577 5 502

I-LOC
0 0 0 0 0 0 0 0 0

B-MISC
0 0 0 0 0 0 0 0 0

I-MISC
0 0 0 0 0 0 0 0 0

Table 10: Confusion matrix for NER on CoNLL’03 for the fully-ﬁnetuned BERT model with DP (ε = 1). It can be seen (highlighted) that the model sometimes falsely predicts the type of tag. The positional error is still present.

↓ gold
O I-LOC B-PER I-PER I-ORG I-MISC B-MISC B-LOC B-ORG

O
41021 1935 0 2821 2524 1013 9 6 5

I-LOC
52 0 0 2 3 1 0 0 0

B-PER
9 1 0 1 1 0 0 0 0

I-PER
6 0 0 0 1 0 0 0 0

I-ORG
9 0 0 0 0 0 0 0 0

I-MISC
32 1 0 3 2 1 0 0 0

B-MISC
4 0 0 0 1 0 0 0 0

B-LOC
4 0 0 0 0 0 0 0 0

B-ORG
5 1 0 0 0 0 0 0 0

Table 11: Confusion matrix for NER on CoNLL’03 for the BILSTM with DP (ε = 1). It can be seen that this model only predicts the outside tag (highlighted).

O I-LOC B-PER I-PER I-ORG I-MISC B-MISC B-LOC B-ORG

O
3464 130 0 234 271 76 1 0 2

I-LOC
3190 153 0 226 184 73 0 1 0

B-PER
4161 252 0 348 310 100 1 1 0

I-PER
21208 1032 0 1440 1161 561 4 4 2

I-ORG
170 11 0 20 9 4 0 0 0

I-MISC
6035 243 0 377 430 132 0 0 0

B-MISC
722 29 0 34 39 17 0 0 0

B-LOC
1993 84 0 140 115 49 3 0 1

B-ORG
144 2 0 3 8 3 0 0 0

Table 12: Confusion matrix for NER on CoNLL’03 for the BILSTM with DP (ε = 0.00837). It can be seen that with this much privacy the model only randomly chooses tags (highlighted).

