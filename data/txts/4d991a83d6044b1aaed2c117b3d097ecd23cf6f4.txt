IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

1

End-to-End Neural Diarization: Reformulating Speaker Diarization as
Simple Multi-label Classiﬁcation
Yusuke Fujita, Member, IEEE, Shinji Watanabe, Senior Member, IEEE, Shota Horiguchi, Member, IEEE, Yawen Xue, and Kenji Nagamatsu

arXiv:2003.02966v1 [eess.AS] 24 Feb 2020

Abstract—The most common approach to speaker diarization is clustering of speaker embeddings. However, the clusteringbased approach has a number of problems; i.e., (i) it is not optimized to minimize diarization errors directly, (ii) it cannot handle speaker overlaps correctly, and (iii) it has trouble adapting their speaker embedding models to real audio recordings with speaker overlaps. To solve these problems, we propose the End-to-End Neural Diarization (EEND), in which a neural network directly outputs speaker diarization results given a multi-speaker recording. To realize such an end-to-end model, we formulate the speaker diarization problem as a multi-label classiﬁcation problem and introduce a permutation-free objective function to directly minimize diarization errors. Besides its end-to-end simplicity, the EEND method can explicitly handle speaker overlaps during training and inference. Just by feeding multi-speaker recordings with corresponding speaker segment labels, our model can be easily adapted to real conversations. We evaluated our method on simulated speech mixtures and real conversation datasets. The results showed that the EEND method outperformed the state-of-the-art x-vector clusteringbased method, while it correctly handled speaker overlaps. We explored the neural network architecture for the EEND method, and found that the self-attention-based neural network was the key to achieving excellent performance. In contrast to conditioning the network only on its previous and next hidden states, as is done using bidirectional long short-term memory (BLSTM), self-attention is directly conditioned on all the frames. By visualizing the attention weights, we show that self-attention captures global speaker characteristics in addition to local speech activity dynamics, making it especially suitable for dealing with the speaker diarization problem.
Index Terms—speaker diarization, neural network, end-to-end, self-attention

I. INTRODUCTION
S PEAKER diarization is the process of partitioning an audio recording into homogeneous segments according to the speaker’s identity. Speaker diarization has a wide range of applications, such as generating written records of meetings and a turn-taking analysis of telephone conversations [1], [2]. It also improves automatic speech recognition performance in multi-speaker conversation scenarios in meetings (ICSI [3], [4], AMI [5], [6]) and home environments (CHiME-5 [6]– [10]).

Yusuke Fujita, Shota Horiguchi, Yawen Xue, Nagamatsu are with Research & Development Group, Japan, e-mail:{yusuke.fujita.su, shota.horiguchi.wk, kenji.nagamatsu.dm}@hitachi.com.
Shinji Watanabe is with Johns Hopkins University, shinjiw@ieee.org.

and Kenji Hitachi, Ltd., yawen.xue.wn,
USA, e-mail:

The most common approach to speaker diarization is based on clustering of speaker embeddings [11]–[18]. For instance, i-vectors [12], [13], [17], [19], d-vectors [18], [20], and xvectors [16], [21] are commonly used in speaker diarization tasks. These embeddings of short segments are partitioned into speaker clusters by using clustering algorithms, such as Gaussian mixture models [11], [12], agglomerative hierarchical clustering [11], [13], [16], [17], mean shift clustering [14], k-means clustering [15], [18], Links [18], [22], and spectral clustering [18]. These clustering-based diarization methods have shown themselves to be effective on various datasets (see the DIHARD Challenge 2018 activities, e.g., [23]–[25]).
However, such clustering-based methods have a number of problems. First, they cannot be optimized to minimize diarization errors directly because the clustering is a type of unsupervised learning process. Second, they have trouble handling speaker overlaps, since the clustering algorithms implicitly assume one speaker per segment. Furthermore, they have trouble adapting their speaker embedding models to real audio recordings with speaker overlaps, because the speaker embedding model has to be optimized with single-speaker non-overlapping segments. These problems hinder speaker diarization when it is applied to real audio recordings that usually contain speaker overlaps.
To solve these problems, we propose End-to-End Neural Diarization (EEND). Different from most of the other methods, EEND does not rely on clustering. Instead, a neural network directly outputs the joint speech activities of all speakers for each time frame, given an input of a multi-speaker audio recording. Our method can naturally handle speaker overlaps during the training and inference period by exploiting a multilabel classiﬁcation framework.
EEND is based on our previous studies [26], [27]. In [26], we proposed an optimal training scheme for a diarization model with a permutation-free objective function that provides minimal diarization errors. In [27], we extended that method by exploiting a self-attention-based neural network. Instead of a bidirectional long short-term memory (BLSTM) [28], we used a self-attention mechanism [29], [30], which resulted in a signiﬁcant performance improvement over the BLSTM-based model.
In this paper, we reformulate speaker diarization as a simple multi-label classiﬁcation, which is independent of the choice of neural network architecture: BLSTM or self-attention. Then we investigate the proposed method from various perspectives,

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

2

Speech vs non-speech

Speaker ID

Same/Diff. speaker
covariance

Acoustic features

Speech activity detection

Speaker embedding extraction

Scoring

Option
Speaker change

Clustering

Resegmentation

Diarization result

(a) Clustering-based method

Neural network

Acoustic features

Joint speech activity detection of all speakers

Diarization result

(b) EEND method

the dependencies between these models. They include, for example, joint modeling of speaker embedding extraction and scoring [16], [32] and joint modeling of SAD and speaker embedding [33]. However, the clustering process has remained unchanged because it is an unsupervised process.
In contrast to these methods, the EEND method uses only one neural network model, as depicted in Fig. 1(b). This method does not rely on clustering, and the model can be directly optimized with the reference diarization results of the training data.
This neural-network-based end-to-end approach, in which only one neural network model directly computes the ﬁnal outputs, has been successfully applied in a variety of tasks, including neural machine translation [34], [35], automatic speech recognition [36]–[38], and text-to-speech [39], [40]. The proposed method is also categorized as such an approach.

Fig. 1. System diagrams for speaker diarization. While clustering-based method requires three different models, EEND method requires one model.
by comparison the effect of different network architectures, visualizing latent representations, and evaluating it on multiple real datasets. The comparison of the network architectures revealed that a multi-head self-attention-based neural network is the key to achieving excellent performance. Experiments with different numbers of heads showed that the excellent performance could be obtained by making the number of heads sufﬁciently larger than the number of speakers. Experiments with different numbers of self-attention-based encoder blocks revealed that the EEND model performed better when it had more encoder blocks. By visualizing the latent representation, we showed that self-attention could capture global speaker characteristics in addition to local speech activity dynamics, making it especially suitable for dealing with the speaker diarization problem. The evaluation on the real datasets showed that the EEND method outperformed the state-of-the-art xvector clustering-based method, while it correctly handled the speaker overlaps.
II. RELATED WORK
A. Clustering-based methods
Clustering-based methods are commonly used for speaker diarization. We used i-vector/x-vector clustering-based systems [23], [24], [31] as the baselines in our experiments. A diagram of a typical clustering-based system is depicted in Fig. 1(a).
To build the system, one has to prepare three independent models: (i) a speech activity detection (SAD) model for discriminating speech and non-speech, (ii) a speaker embedding extraction model for speaker identiﬁcation, and (iii) a scoring model including the same/different speaker covariance matrices. None of these models can be trained to minimize the diarization errors directly. Optionally, a resegmentation process requires another model to reﬁne speaker change points to produce the ﬁnal diarization results.
Joint modeling methods have been studied in an effort to alleviate the complex preparation process and take into account

B. Clustering-free methods
The clustering process impares the model optimization aimed at minimizing diarization errors. To alleviate this problem, Zhang et al. proposed a clustering-free diarization method [41]. This method is the ﬁrst successful approach that does not cluster speaker embeddings and that is optimized with a diarization error minimization objective. The method formulates the speaker diarization problem on the basis of a factored probabilistic model, which consists of modules for determining speaker changes, speaker assignments, and feature generation. These models are jointly trained using input features and corresponding speaker labels. However, the SAD model and their speaker embedding (d-vector) extraction model have to be trained separately in their method. Moreover, their speakerchange model assumes one speaker for each segment, which hinders its application to speaker-overlapping speech.
In contrast to their method, the EEND method uses an endto-end neural network that accepts audio features as input and outputs the joint speech activities of multiple speakers. The network is optimized using the entire recording, including nonspeech and speaker overlaps, with a diarization-error-oriented objective.
C. Self-attention mechanism
The self-attention mechanism was originally proposed for extracting sentence embeddings for text processing [29]. Recently, the self-attention mechanism has shown superior performance in a variety of tasks, including machine translation [30], video classiﬁcation [42], and image segmentation [43]. For audio processing, a self-attention mechanism has been incorporated in acoustic modeling for ASR [44], [45], sound event detection [46], and speaker recognition [47]. For speaker diarization, the self-attention mechanism has been applied to the speaker embedding extraction model [25] and the scoring model [32] of clustering-based methods. This study describes a self-attention mechanism for clustering-free speaker diarization.

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

3

III. END-TO-END NEURAL DIARIZATION (EEND)
In this section, we describe a novel approach to speaker diarization problem exploiting a multi-label classiﬁcation framework with a permutation-free training scheme. We refer to the proposed method as EEND.

A. Speaker diarization as multi-label classiﬁcation

The speaker diarization task can be formulated as a proba-
bilistic multi-label classiﬁcation problem, as follows. Given an observation sequence of length T , X = (xt ∈
RF | t = 1, · · · , T ), from an audio signal, the speaker
diaization problem is one of estimating the corresponding speaker label sequence Y = (yt | t = 1, · · · , T ). Here, xt is an F -dimensional observation feature vector at time index t. Speaker label yt = [yt,c ∈ {0, 1} | c = 1, · · · , C] denotes a joint activity for multiple (C) speakers at time index
t. For example, yt,c = yt,c = 1 (c = c ) represent an overlap situation in which speakers c and c are both present
at time index t. Thus, determining Y is a sufﬁcient condition
to determine the speaker diarization information. The most probable speaker label sequence Yˆ is selected
from among all possible speaker label sequences Y, as follows:

Yˆ = arg max P (Y |X).

(1)

Y ∈Y

P (Y |X) can be factorized using the conditional independence assumption as follows:

P (Y |X) = P (yt|y1, · · · yt−1, X),

(2)

t

≈ P (yt|X) ≈

P (yt,c|X). (3)

t

tc

Here, we assume that the frame-wise posterior is conditioned on all inputs, and each speaker is present independently. The frame-wise posteriors can be estimated using a neuralnetwork-based model, as follows:

zt = [P (yt,1|X), · · · , P (yt,C |X)] = NNt(X) ∈ (0, 1)C , (4)
where NNt(·) is a neural network which accepts a sequence of input features and outputs zt, a C-dimensional vector of the frame-wise posteriors at time index t.

B. Permutation-free training
The difﬁculty of training the model described above is that the model must deal with speaker permutations: changing the order of speakers within a correct label sequence is also regarded as correct. An example of permutations in a twospeaker case is shown in Fig. 2. In this paper, we call this problem “label ambiguity.” This label ambiguity obstructs the training of the neural network when we use a standard binary cross-entropy loss function.
To cope with the label ambiguity problem, we employ the permutation-free training scheme, which considers all the permutations of the reference speaker labels. The permutationfree training scheme has been used in research on source separation [48]–[50]. Here, we apply a permutation-free loss

Permutation 1 (l(tφ=1)|t = 1, · · · , T )

Permutation 2 (l(tφ=2)|t = 1, · · · , T )

Permutation-free loss

BCE

minimum

BCE

Frame-wise posteriors
(zt|t = 1, · · · , T )

Neural network NNt(·)
Audio features X

Fig. 2. Two-speaker EEND model trained with permutation-free loss. The binary cross entropy (BCE) loss of frame-wise posteriors (zt | t = 0, · · · , T ) are computed with two permutations of reference labels.

Frame-wise posteriors (zt|t = 1, · · · , T )

Labels (lt|t = 1, · · · , T )

P blocks

Linear + Sigmoid (h(tP )|t = 1, · · · , T )
BLSTM block

One-hot vector

0: non-speech

1 3 2 0 1: speaker 1

2: speaker 2

DC loss

3: overlap

BLSTM block

Linear+Tanh+Norm (h(tq)|t = 1, · · · , T )

Audio features X

Fig. 3. BLSTM-based EEND model with Deep Clustering (DC) loss.
function to a temporal sequence of speaker labels. The neural network is trained to minimize the permutation-free loss between the output zt predicted using Eq. 4 and the reference speaker label lt ∈ {0, 1}C , as follows:
J PF = T1C φ∈pmerimn(C) BCE(lφt , zt), (5)
t
where perm(C) is the set of all the possible permutations of (1, . . . , C), and lφt is the φ-th permutation of the reference speaker label, and BCE(·, ·) is the binary cross entropy function between the label and the output.
IV. NEURAL NETWORK ARCHITECTURES FOR EEND In this section, we explore two different architectures of neural networks for the EEND method.
A. BLSTM-based neural network with Deep Clustering loss According to Eq. 4, the neural-network-based function
NNt(·) accepts a temporal sequence of feature vectors and

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

4

outputs a vector for each time frame. Thus, this function can be modeled with bi-directional long short-term memory (BLSTM) as depicted in Fig. 3. The input features are transformed as follows:

h(t1) = BLSTM(t1)(x1, · · · , xT ),

(6)

h(tp) = BLSTMt(p)(h(1p−1), · · · , h(Tp−1)) (2 ≤ p ≤ P ), (7)

zt = σ(Wh(tP ) + b),

(8)

where BLSTM(tp)(·) is a p-th BLSTM layer which accepts an input sequence and outputs hidden activations h(tp) ∈ R2H at time index t.1 W ∈ RC×2H and b ∈ RC are a linear projection matrix and bias, respectively. We use P -layer stacked BLSTMs.
Assuming that the neural network extracts speaker embeddings in lower layers and then performs temporal segmentation using higher layers, the middle layer activations can be regarded as the speaker embeddings. Therefore, we place a speaker embedding training criterion on the middle layer activations.
Here, the q-th layer activations h(tq) obtained from Eq. 7 are transformed into normalized V -dimensional embedding vt as follows:

vt = Normalize(Tanh(W(DC)h(tq) + b(DC))) ∈ RV , (9)

where W(DC) ∈ RV ×2H and b(DC) ∈ RV are a linear projection matrix and a bias, respectively. Tanh(·) is the element-wise hyperbolic tangent function and Normalize(·) is the L2 normalization function. We apply the Deep Clustering (DC) loss function [48] so that the embeddings are partitioned into speaker-dependent clusters as well as overlapping and non-speech clusters. For example, in a two-speaker case, we generate four clusters (Non-speech, Speaker 1, Speaker 2, and Overlapping) as shown in Fig. 3.
DC loss function is expressed as follows:

J DC = VV − L L

2 F

,

(10)

where V = [v1 · · · vT ] , and L ∈ RT ×2C is a matrix in which each row represents a one-hot vector converted from lt, where those elements are in the power set of speakers.
· F is the Frobenius norm. The loss function encourages the two embeddings at different time indices to be close together if they are in the same cluster and far away if they are in different clusters.
Next, we use multi-objective training introducing a mixing parameter α:

J MULTI = (1 − α)J PF + αJ DC.

(11)

B. Self-attention-based neural network
By using BLSTM, each output frame is conditioned only on its previous hidden state, subsequent hidden state and current input feature. In contrast, by using a self-attention mechanism [29], each output frame is directly conditioned on all input frames by computing the pairwise similarity between
1It is a concatenated vector of H-dimensional forward and backward LSTM outputs.

Frame-wise posteriors (zt|t = 1, · · · , T )

P blocks

Linear + Sigmoid
LayerNorm
(e(tP )|t = 1, · · · , T )
Encoder block
Encoder block
(e(t0)|t = 1, · · · , T )
Linear
Audio features X

E(p,FF) Position-wise FF
E¯ (p,SA) LayerNorm
Multi-head self-attention
E¯ (p−1) LayerNorm

Fig. 4. Self-attention-based EEND model.

all frame pairs. Here, we use a self-attention-based neural network instead of BLSTM, as depicted in Fig. 4. The input features are transformed as follows:

e(t0) = W0xt + b0 ∈ RD,

(12)

e(tp)

=

Enco

der

( t

p

)

(

e(1p

−

1)

,

·

·

·

, eT(p−1))

(1

≤

p

≤

P ).

(13)

Here, W0 ∈ RD×F and b0 ∈ RD project an input feature into D-dimensional vector. Encodert(p)(·) is the p-th encoder block which accepts an input sequence of D-dimensional vectors and outputs a D-dimensional vector e(tp) at time index t. We use P encoder blocks followed by the output layer for frame-wise posteriors.
The detailed architecture of the encoder block is depicted in Fig. 4. This conﬁguration of the encoder block is almost the same as the one in the Speech-Transformer introduced in [45], but without positional encoding. The encoder block has two sub-layers. The ﬁrst is a multi-head self-attention layer, and the second is a position-wise feed-forward layer.
1) Multi-head self-attention layer: The multi-head selfattention layer transforms a sequence of input vectors as follows. The sequence of vectors (e(tp−1)|t = 1, · · · , T ) is converted into a RT ×D matrix; that is followed by layer normalization [51]:

E¯ (p−1) = LayerNorm([e(1p−1) · · · e(Tp−1)] ) ∈ RT ×D. (14)

Then, query, key and value vectors are computed for each head h (1 ≤ h ≤ H) by using linear transformations:

Q(hp) = E¯ (p−1)Wh(p,Q) + 1b(hp,Q) K(hp) = E¯ (p−1)Wh(p,K) + 1b(hp,K) Vh(p) = E¯ (p−1)Wh(p,V ) + 1b(hp,V )

∈ RT ×d, ∈ RT ×d, ∈ RT ×d,

(15) (16) (17)

where d = D/H is the dimension of each head, H is the number of heads, Wh(p,Q), Wh(p,K), Wh(p,V ) ∈ RD×d are query, key, and value projection matrices, respectively.

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

5

bh(p,Q), b(hp,K), b(hp,V ) ∈ Rd are bias vectors and 1 is a T dimensional all-one vector. A pairwise similarity matrix A(hp) is computed using the dot products of the query vectors and
key vectors:

A(hp) = Q(hp)K(hp) ∈ RT ×T .

(18)

The pairwise similarity matrix A(hp) is scaled by 1/√d, and a
softmax function is applied to form the attention weight matrix Aˆ (hp):

Aˆ (p) = Softmax

A(p) √h

∈ RT ×T .

(19)

h

d

TABLE I STATISTICS OF TRAINING AND TEST SETS.

Traning sets SimBeta2 Real SimLarge Comb Test sets 1 2 3 4 5

Simulated (β = 2) SWBD+SRE Simu. (β = 2, 3, 5, 7) Real+SimLarge
Simulated (β = 2) Simulated (β = 3) Simulated (β = 5) CALLHOME [52] CSJ [53]

Num. of mixtures
100,000 26,172
400,000 426,172
500 500 500 148 54

Avg. dur. (sec)
87.6 304.7 126.4 137.3
87.3 103.8 137.1 72.1 766.3

Overlap ratio (%)
34.4 3.7
23.4 20.5
34.4 27.2 19.5 13.0 20.1

Then, using the attention weight matrix, the context vectors C(hp) are computed as a weighted sum of the value vectors :

C(hp) = Aˆ (hp)Vh(p) ∈ RT ×d.

(20)

Finally, the context vectors for all heads are concatenated and projected using an output projection matrix W(p,O) ∈ RD×D and a bias b(p,O):

E(p,SA) = [C(1p) · · · C(Hp)]W(p,O) + 1b(p,O) ∈ RT ×D. (21)

Following the self-attention layer, a residual connection and layer normalization are applied:

E¯ (p,SA) = LayerNorm(E¯ (p−1) + E(p,SA)) ∈ RT ×D. (22)

2) Position-wise feed-forward layer: The position-wise feed-forward layer transforms E¯(p,SA) as follows:

E(p,ﬀ) = ReLU(E¯ (p,SA)W1(p) + 1b(1p) ) ∈ RT ×dff , E(p,FF) = E(p,ﬀ)W2(p) + 1b(2p) ∈ RT ×D,

(23) (24)

where W1(p) ∈ RD×dff and b(1p) ∈ Rdff are the ﬁrst linear projection matrix and bias, respectively, and ReLU(·) is the
rectiﬁed linear unit activation function. dﬀ is the number of internal units in this layer. W2(p) ∈ Rdff ×D and b(2p) ∈ RD are the second linear projection matrix and bias, respectively.
Finally, the output of the encoder block e(tp) for each time frame is computed by applying a residual connection as
follows:

[e(1p) · · · e(Tp)] = (E¯ (p,SA) + E(p,FF))

(25)

3) Output layer for frame-wise posteriors: The frame-wise posteriors zt are calculated from e(tP ) (in Eq. 13) by using
layer normalization and a fully-connected layer as follows:

E¯ (P ) = LayerNorm([e(1P ) · · · e(TP )] ) ∈ RT ×D, (26)

[z1 · · · zT ] = σ(E¯ (P )W3 + 1b3 ) ,

(27)

where W3 ∈ RD×C and b3 ∈ RC are the linear projection matrix and bias, respectively, and σ(·) is the element-wise
sigmoid function.

V. EXPERIMENTAL SETUP
A. Data
To verify the effectiveness of the EEND method for various overlap situations, we prepared four training sets and ﬁve test sets, including simulated and real datasets. The statistics of the training and test sets are listed in Table I. The overlap ratio is computed as the ratio of the audio time during which two or more speakers are active to the audio time during which one or more speakers are active.
Note that the training data for the EEND method are different from those for the i-vector/x-vector clustering-based method. Whereas the clustering-based methods use singlespeaker segments for training their speaker embedding extraction models, the EEND method uses audio mixtures of multiple speakers. Such mixtures can be simulated inﬁnitely with a combination of single-speaker segments. Moreover, the EEND model can be trained with not only simulated mixtures but also real audio mixtures with speaker overlaps.
1) Simulated datasets: Each mixture was simulated by Algorithm 1. Unlike the mixture simulations of source separation studies [48], we consider a diarization-style mixture: each speech mixture should have dozens of utterances per speaker with reasonable silence intervals between utterances. The silence intervals are controlled by the average interval of β. Larger values of β generate speech with less overlap.
The set of utterances used in the simulation was comprised of the Switchboard-2 (Phase I, II, III), Switchboard Cellular (Part 1, Part2), and NIST Speaker Recognition Evaluation datasets (2004, 2005, 2006, 2008). All recordings are telephone speech sampled at 8 kHz. There are 6,381 speakers in total. We split them into 5,743 speakers for the training set and 638 speakers for the test set. Note that the set of utterances for the training set is identical to that of the Kaldi CALLHOME diarization v2 recipe [54]2, thereby enabling a fair comparison with the x-vector clustering-based method.
Since there are no time annotations in these corpora, we extracted utterances using speech activity detection (SAD) on the basis of time-delay neural networks and statistics pooling3.
The set of background noises was from the MUSAN corpus [55]. We used 37 recordings that are annotated as “background” noises. The set of 10,000 room impulse responses
2https://github.com/kaldi-asr/kaldi/tree/master/egs/callhome diarization 3The SAD model: http://kaldi-asr.org/models/m4

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

6

Algorithm 1: Mixture simulation.

Input: S, N , I, R U = {Us}s∈S

// Sets of speakers, noises, RIRs and SNRs // Set of utterance lists

Nspk Numax, Numin β

// #speakers per mixture // Max. and min. #utterances per speaker
// Average interval

Output: y

// Mixture

1 Sample a set of Nspk speakers S from S

2 X ←∅

// Set of Nspk speakers’ signals

3 forall s ∈ S do

4 xs ← ∅

// Concatenated signal

5 Sample i from I

// RIR

6 Sample Nu from {Numin, . . . , Numax}

7 for u = 1 to Nu do

8

Sample

δ

∼

1 β

exp

− βδ

// Interval

9

xs ← xs ⊕ 0(δ) ⊕ Us [u] ∗ i

10 X .add (xs)

11 Lmax = maxx∈X |x|

12 y ← x∈X x ⊕ 0(Lmax−|x|) 13 Sample n from N

// Background noise

14 Sample r from R

// SNR

15 Determine a mixing scale p from r, y, and n

16 n ← repeat n until the length of y is reached

17 y ← y + p · n

(RIRs) was from the Simulated Room Impulse Response Database used in [56]. The SNR values were sampled from 10, 15, and 20 dB. These sets of non-speech corpora were also used for training the x-vector and SAD models in the x-vector clustering-based method.
We generated two-speaker mixtures for each speaker with 10-20 utterances (Nspk = 2, Numin = 10, Numax = 20). For the simulated training set, 100,000 mixtures were generated with β = 2 (SimBeta2). In addition, four sets of 100,000 mixtures with different values of β (2, 3, 5, and 7) were combined to form 400,000 mixtures (SimLarge). For the simulated test set, 500 mixtures were generated with β = 2, 3, and 5. The overlap ratios of the simulated mixtures ranged from 19.5 to 34.4%.
2) Real datasets: We used real telephone speech recordings as the real training set (Real). A set of 26,172 twospeaker recordings were extracted from the recordings of the Switchboard-2 (Phase I, II, III), Switchboard Cellular (Part 1, Part 2), and NIST Speaker Recognition Evaluation datasets. The overlap ratio of the training data was 3.7%, far less than that of the simulated mixtures.
We evaluated the proposed method on real telephone conversations in the CALLHOME dataset [52]. We randomly split the two-speaker recordings from the CALLHOME dataset into two subsets: an adaptation set of 155 recordings and a test set of 148 recordings. The average overlap ratio of the test set was 13.0%.
In addition, we conducted an evaluation on the dialogue part of the Corpus of Spontaneous Japanese (CSJ) [53]. The

CSJ contains 54 two-speaker dialogue recordings4. They were recorded using headset microphones in separate soundproof rooms. The average overlap ratio of the CSJ test set was 20.1%, larger than the CALLHOME test set.
3) Combined datasets: For generalizing a model to various environments, we conducted experiments using both a simulated training set (SimLarge) and the real training set (Real). We refer to the dataset as the combined training set (Comb).
B. Model conﬁguration
1) Clustering-based systems: We compared the proposed method with two conventional clustering-based systems [23]: the i-vector system and x-vector system were created using the Kaldi CALLHOME diarization v1 and v2 recipes.
These recipes use agglomerative hierarchical clustering (AHC) with the probabilistic linear discriminant analysis (PLDA) scoring scheme. The number of clusters was ﬁxed to 2. Though the original recipes use oracle speech/nonspeech marks, we used the SAD model with the conﬁguration described in Sec. V-A.
2) BLSTM-based EEND system: We conﬁgured the BLSTM-based EEND system (BLSTM-EEND) described in Sec. IV-A. The input features were 23-dimensional log-Melﬁlterbanks with a 25-ms frame length and 10-ms frame shift. Each feature was concatenated with those from the previous seven frames and subsequent seven frames. To deal with a long audio sequence in our neural networks, we subsampled the concatenated features by a factor of ten. Consequently, a (23 × 15)-dimensional input feature was fed into the neural network every 100 ms.
We used a ﬁve-layer BLSTM with 256 hidden units in each layer. The second layer of the BLSTM outputs was used to form a 256-dimensional embedding; we then calculated the Deep Clustering loss in this embedding to discriminate different speakers. The mixing parameter α was set to 0.5. We used the Adam [57] optimizer with a learning rate of 10−3. The batch size was 10. The number of training epochs was 20.
Because the output of the neural network is the probability of speech activity for each speaker, a threshold is required to obtain a decision on speech activity for each frame. We set the threshold to 0.5. Furthermore, we applied 11-frame median ﬁltering to prevent production of unreasonably short segments.
For domain adaptation, the neural network was retrained using the CALLHOME adaptation set. We used the Adam optimizer with a learning rate of 10−6 and ran ﬁve epochs. For the postprocessing, we adjusted the threshold to 0.6 so that the DER of the adaptation set had the minimum value.
3) Self-attention-based EEND system: We conﬁgured a Self-attention-based EEND system (SA-EEND) as described in Sec. IV-B. Here, we used the same input features as were input to the BLSTM-EEND system. Note that the sequence length in the training stage was limited to 500 (50 seconds in audio time) because our system uses more memory than the BLSTM-based network does. Therefore, we split the input
4We excluded four out of 58 recordings that contain speakers in the ofﬁcial speech recognition evaluation sets.

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

7

TABLE II DERS (%) ON VARIOUS TEST SETS. FOR EEND SYSTEMS, THE CALLHOME (CH) RESULTS WERE OBTAINED WITH DOMAIN
ADAPTATION.

Clustering-based i-vector x-vector
BLSTM-EEND trained with SimBeta2 trained with Real
SA-EEND trained with SimBeta2 trained with Real trained with SimLarge trained with Comb

Simulated β=2 β=3 β=5

33.74 28.77

30.93 24.46

25.96 19.78

12.28 36.23

14.36 37.78

19.69 40.34

7.91 32.72 6.81 6.92

8.51 33.84 6.60 6.54

9.51 36.78 6.40 6.38

Real CH CSJ

12.10 27.99 11.53 22.96

26.03 39.33 23.07 25.37

13.66 10.76 14.03 11.99

22.31 20.50 21.84 22.26

audio recordings into non-overlapping 50-second segments. In the inference stage, we used the entire sequence for each recording.
We used two encoder blocks with 256 attention units containing four heads (P = 2, D = 256, H = 4). Note that most of our experiments were performed without residual connections in Eqs. 22 and 25. As described later in VI-F, adding residual connections further improved performance.
We used 1024 internal units in a position-wise feed-forward layer (dﬀ = 1024). We used the Adam optimizer with the learning rate scheduler described in [30]. The number of warm-up steps used in the learning rate scheduler was 25,000. The batch size was 64. The number of training epochs was 100. After 100 epochs, we used an averaged model obtained by averaging the model parameters of the last ten epochs. As with the BLSTM-EEND system, we applied 11-frame median ﬁltering.
For domain adaptation, the averaged model was retrained using the CALLHOME adaptation set. We used the Adam optimizer with a learning rate of 10−5 and ran 100 epochs. After 100 epochs, we used an averaged model obtained by averaging the model parameters of the last ten epochs.
C. Performance metric
We evaluated the systems with the diarization error rate (DER) [58]. Note that the DERs reported in many prior studies did not include misses or false alarm errors due to their using oracle speech/non-speech labels. Overlapping speech segments had also been excluded from the evaluation. For our DER computation, we evaluated all of the errors, including overlapping speech segments, because the proposed method includes both the speech activity detection and overlapping speech detection functionality. As is done typically, we used a collar tolerance of 250 ms at the start and end of each segment.
VI. RESULTS
A. Evaluation on simulated mixtures
DERs on various test sets are shown in Table II. The clustering-based systems performed poorly on heavily overlapping simulated mixtures. This result is within our expectations because the clustering-based systems did not consider speaker

TABLE III DERS (%) ON THE CALLHOME WITH AND WITHOUT DOMAIN
ADAPTATION.

x-vector clustering BLSTM-EEND
trained with SimBeta2 trained with Real SA-EEND trained with SimBeta2 trained with SimLarge trained with Real trained with Comb

w/o adaptation 11.53
43.84 31.01
17.42 16.31 12.66 14.50

with adaptatation N/A
26.03 23.07
13.66 14.03 10.76 11.99

TABLE IV DETAILED DERS (%) EVALUATED ON THE CALLHOME. DER IS COMPOSED OF MISSES (MI), FALSE ALARMS (FA), AND CONFUSION ERRORS (CF). THE SAD ERRORS ARE COMPOSED OF MISSES (MI) AND
FALSE ALARMS (FA) ERRORS.

Method i-vector x-vector SA-EEND
no-adapt adapted

DER 12.10 11.53
12.66 10.76

DER breakdown MI FA CF 7.74 0.54 3.82 7.74 0.54 3.25
7.42 3.93 1.31 6.68 2.40 1.68

SAD errors MI FA 1.4 0.5 1.4 0.5
3.3 0.6 2.3 0.5

overlaps; there were more misses when the overlap ratio was high.
The BLSTM-EEND system trained with the simulated training set (SimBeta2) showed a signiﬁcant DER reduction compared with the clustering-based systems on the simulated mixtures. Among the differing overlap ratios, it performed the best on the highest overlap ratio condition (β = 2). The BLSTM-EEND system worked well on the overlapping condition matched that of the training data.
The SA-EEND system trained with the simulated training set had signiﬁcantly fewer DERs compared with the BLSTMEEND system on every test set. As well as the BLSTM-EEND system, it showed the best performance on the highest overlap ratio condition (β = 2). However, the DER degradation under fewer overlapping conditions was smaller than that of the BLSTM-EEND system, which indicated that the self-attention blocks improved robustness to variable overlapping conditions.
Training the SA-EEND model with various overlap ratio conditions (SimLarge) showed an improvement over the single overlap ratio condition (SimBeta2) on every test set. It was revealed that overﬁtting to a speciﬁc overlap ratio could be mitigated by this multi-condition training.
B. Evaluation on real test sets
In contrast to the excellent performance on the simulated mixtures, the BLSTM-EEND system had inferior DERs to those of the clustering-based systems evaluated on the real test sets. Although the BLSTM-EEND system showed performance improvements when the training data were switched from simulated to real data, its DERs were still higher than those of the clustering-based systems.
The SA-EEND system trained with the simulated training set (SimBeta2) showed remarkable improvements on the real

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

8

Query Query Query
loss
Query

Spk 1 Spk 2
0 100 200 300 400 500 600 700 800
0 100 200 300 400 500 600 700 800 Key
Head 1

Spk 1 Spk 2
0 100 200 300 400 500 600 700 800
0 100 200 300 400 500 600 700 800 Key
Head 2

Spk 1 Spk 2
0 100 200 300 400 500 600 700 800
0 100 200 300 400 500 600 700 800 Key
Head 3

Spk 1 Spk 2
0 100 200 300 400 500 600 700 800
0 100 200 300 400 500 600 700 800 Key
Head 4

Fig. 5. Attention weight matrices at the second encoder block. The input was the CALLHOME test set (recording id: iagk). The model was trained with the real training set followed by domain adaptation. The top two rows show the reference speech activity of two speakers.

test sets of CALLHOME and CSJ, which indicates the strong generalization capability of the self-attention blocks. For the CSJ, even without domain adaptation, the SA-EEND system performed better than the x-vector clustering-based method. Training the SA-EEND model with various overlap ratio conditions (SimLarge) yielded excellent generalizations to real test sets.
The SA-EEND system trained with the real training set (Real) performed better than SimLarge on the real test sets. However, it had poor DERs on the simulated test sets. We believed that the result was due to the small number of mixtures and low overlap ratio of the real training set. Finally, the SA-EEND system trained with the combined dataset (Comb) showed an excellent generalization capability, which was obtained by feeding it various overlap ratio conditions.
C. Effect of domain adaptation
The EEND models trained with simulated training set were overﬁtted to the speciﬁc overlap ratio of the training set. We expected that the overﬁtting would be mitigated by using domain adaptation. DERs on the CALLHOME with and without domain adaptation are shown in Table III. As expected, the domain adaptation signiﬁcantly reduced the DER; our system thus achieved even better results than those of the x-vectorbased system.
A detailed DER comparison on the CALLHOME test set is shown in Table IV. The clustering-based systems had few SAD errors thanks to the robust SAD model trained with various noise-augmented data. However, there were numerous misses and confusion errors due to its lack of handling speaker overlaps. Compared with clustering-based systems, the proposed method produced signiﬁcantly fewer confusion and miss errors. The domain adaptation reduced all error types except confusion errors.
D. Visualization of self-attention
To analyze the behavior of the self-attention mechanism in our diarization system, Fig. 5 visualizes the attention weight matrix at the second encoder block, corresponding to Aˆ h(p=2)

H=1

0.6

H=2 H=4

H=8

0.5

H=16

0.4

0.3

0.2 0 20 40 epoch 60 80 100

Fig. 6. Loss curves on simulated validation set (β = 2) for different numbers of heads. These models were trained with SimBeta2.

in Eq. 19. Here, head 1 and head 2 have vertical lines at different positions. The vertical lines correspond to each speaker’s activity. The attention weight matrix with these vertical lines transformed the input features into the weighted mean of the same speaker frames. These heads actually captured the global speaker characteristics by computing the similarity between distant frames. Interestingly, heads 3 and 4 look like diagonal matrices, which result in local linear transforms. These heads are considered to act as speech/non-speech detectors. We conclude that the multi-head self-attention mechanism captures global speaker characteristics in addition to local speech activity dynamics, which leads to a reduction in DER.
E. Effect of varying number of heads in self-attention blocks
The analysis in Sec. VI-D indicated that the different heads represented different speakers. To verify the importance of multiple heads, we trained models with different numbers of heads. The loss curves with for those models are shown in Fig. 6. The loss decreased as the number of heads increased and this trend continued for a large number of epochs. Note that for the single-head (H = 1) experiment, we interrupted

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

9

TABLE V DERS (%) WITH DIFFERENT NUMBER OF HEADS. THE MODELS ARE
TRAINED WITH SIMBETA2.

Num. heads
2 4 8 16

β=2 12.60
7.91 6.84 7.19

Simulated β=3
13.42 8.51 7.06 7.52

β=5 16.12
9.51 7.85 7.88

Real CH CSJ 16.49 26.05 13.66 22.31 13.44 23.58 13.28 24.35

loss

0.50

P = 2, warm-up 25k

0.45

P = 4, warm-up 25k P = 4, warm-up 50k

0.40

P = 4, warm-up 100k

0.35

0.30

0.25

0.20

0.15

0 20 40 epoch 60 80 100

Fig. 7. Loss curves on simulated validation set (β = 2) for different numbers of layers and warm-up steps. These models were trained with SimBeta2.

the training because the losses were consistent, around 0.67 during the ﬁrst 12 epochs.
The DERs for different numbers of heads are shown in Table V. Here, performance improved as a result of increasing the number of heads. These results suggest that the SA-EEND models were trained to separate speakers via the global speaker characteristics represented by different heads, the required number of heads was at least the number of speakers, and more heads boosted performance.
F. Effect of varying number of encoder blocks and warm-up steps
As noted in Sec. V-B3, most of our experiments were performed without residual connections in Eqs. 22 and 25. In this section, we examined deeper model conﬁgurations using more encoder blocks with residual connections. The loss curves for different numbers of encoder blocks and warmup steps are shown in Fig. 7. The models with four encoder blocks reduced the validation loss compared with the one with two encoder blocks. Moreover, the validation loss was reduced by increasing the number of warm-up steps from 25,000 to 100,000. DERs for different numbers of encoder blocks are shown in Table VI. The results show that increasing the number of encoder blocks signiﬁcantly improved performance.
The EEND system achieved a DER of 9.54%, whereas the x-vector clustering-based system had a DER of 11.53% on the CALLHOME dataset. Moreover, EEND had a DER of 20.39% on the CSJ dataset, while the x-vector clustering-based system had 22.96%. EEND had DERs from 4.56% to 3.85% on the

TABLE VI DERS (%) FOR DIFFERENT NUMBERS OF ENCODER BLOCKS AND WARM-UP STEPS WITH/WITHOUT RESIDUAL CONNECTIONS. THE MODELS
WERE TRAINED WITH SIMBETA2

Enc. Warm. Res.

blocks steps con.

2

25k

N

2

25k

Y

4

25k

Y

4

50k

Y

4 100k

Y

x-vector clustering

β=2 7.91 7.36 5.66 5.01 4.56 28.77

Simulated β=3 8.51 7.59 5.39 4.64 4.50 24.46

β=5 9.51 7.78 5.01 4.10 3.85 19.78

Real CH CSJ 13.66 22.31 12.50 23.38 10.16 20.39 10.25 21.50 9.54 20.48 11.53 22.96

simulated test set, while the x-vector clustering-based system had 19.78% to 28.77%.
VII. CONCLUSION
We proposed End-to-End Neural Diarization (EEND), in which a neural network directly outputs speaker diarization results given a multi-speaker recording. We formulated the speaker diarization problem as a multi-label classiﬁcation problem and introduced a permutation-free objective function to minimize diarization errors directly. We evaluated our method on simulated speech mixtures and real conversation datasets. The results showed that EEND method outperformed that of the state-of-the-art x-vector clustering-based method, and it correctly handled speaker overlaps. We explored the neural network architecture for the EEND method, and found that the self-attention-based neural network was the key to achieving excellent performance. By visualizing the attention weights, we showed that self-attention captured the global speaker characteristics in addition to local speech activity dynamics, making it especially suitable for dealing with the speaker diarization problem. Experiments with different numbers of heads showed that the excellent performance could be obtained by making the number of heads sufﬁciently larger than the number of speakers. Finally, experiments with different numbers of encoder blocks revealed that the EEND model performed better when it had more encoder blocks.
REFERENCES
[1] S. E. Tranter and D. A. Reynolds, “An overview of automatic speaker diarization systems,” IEEE Trans. on ASLP, vol. 14, no. 5, pp. 1557– 1565, 2006.
[2] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, and O. Vinyals, “Speaker diarization: A review of recent research,” IEEE Trans. on ASLP, vol. 20, no. 2, pp. 356–370, 2012.
[3] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, “The ICSI meeting corpus,” in Proc. ICASSP, vol. I, 2003, pp. 364–367.
[4] O¨ . C¸ etin and E. Shriberg, “Overlap in meetings: ASR effects and analysis by dialog factors, speakers, and collection site,” in Proc. MLMI, 2006, pp. 212–224.
[5] S. Renals, T. Hain, and H. Bourlard, “Interpretation of multiparty meetings the AMI and Amida projects,” in 2008 Hands-Free Speech Communication and Microphone Arrays, 2008, pp. 115–118.
[6] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, and S. Watanabe, “Acoustic modeling for distant multi-talker speech recognition with single- and multi-channel branches,” in Proc. ICASSP, 2019, pp. 6630–6634.
[7] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines,” in Proc. Interspeech, 2018, pp. 1561–1565.

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

10

[8] J. Du, T. Gao, L. Sun, F. Ma, Y. Fang, D.-Y. Liu, Q. Zhang, X. Zhang, H.-K. Wang, J. Pan, J.-Q. Gao, C.-H. Lee, and J.-D. Chen, “The USTCiFlytek Systems for CHiME-5 Challenge,” in Proc. CHiME-5, 2018, pp. 11–15.
[9] C. Boeddeker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Heymann, and R. Haeb-Umbach, “Front-End Processing for the CHiME-5 Dinner Party Scenario,” in Proc. CHiME-5, 2018, pp. 35–40.
[10] N. Kanda, R. Ikeshita, S. Horiguchi, Y. Fujita, K. Nagamatsu, X. Wang, V. Manohar, N. E. Yalta Soplin, M. Maciejewski, S.-J. Chen, A. S. Subramanian, R. Li, Z. Wang, J. Naradowsky, L. P. Garcia-Perera, and G. Sell, “Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays,” in Proc. CHiME-5, 2018, pp. 6–10.
[11] S. Meignier, “LIUM SPKDIARIZATION: An open source toolkit for diarization,” in CMU SPUD Workshop, 2010.
[12] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” IEEE Trans. on ASLP, vol. 21, no. 10, pp. 2015–2028, 2013.
[13] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in Proc. SLT, 2014, pp. 413–417.
[14] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A study of the cosine distance-based mean shift for telephone speech diarization,” IEEE/ACM Trans. on ASLP, vol. 22, no. 1, pp. 217–227, 2014.
[15] D. Dimitriadis and P. Fousek, “Developing on-line speaker diarization system,” in Proc. Interspeech, 2017, pp. 2739–2743.
[16] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree, “Speaker diarization using deep neural network embeddings,” in Proc. ICASSP, 2017, pp. 4930–4934.
[17] M. Maciejewski, D. Snyder, V. Manohar, N. Dehak, and S. Khudanpur, “Characterizing performance of speaker diarization systems on far-ﬁeld speech using standard methods,” in Proc. ICASSP, 2018, pp. 5244–5248.
[18] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, and I. L. Moreno, “Speaker diarization with LSTM,” in Proc. ICASSP, 2018, pp. 5239– 5243.
[19] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Frontend factor analysis for speaker veriﬁcation,” IEEE Trans. on ASLP, vol. 19, no. 4, pp. 788–798, 2011.
[20] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, “Generalized end-to-end loss for speaker veriﬁcation,” in Proc. ICASSP, 2018, pp. 4879–4883.
[21] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in Proc. ICASSP, 2018, pp. 5329–5333.
[22] P. A. Mansﬁeld, Q. Wang, C. Downey, L. Wan, and I. L. Moreno, “Links: A high-dimensional online clustering method,” arXiv preprint arXiv:1801.10123, 2018.
[23] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, and S. Khudanpur, “Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge,” in Proc. Interspeech, 2018, pp. 2808–2812.
[24] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Z˘ mol´ıkova´, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot, L. Mos˘ner, and P. Mate˘jka, “BUT system for DIHARD speech diarization challenge 2018,” in Proc. Interspeech, 2018, pp. 2798–2802.
[25] L. Sun, J. Du, C. Jiang, X. Zhang, S. He, B. Yin, and C.-H. Lee, “Speaker diarization with enhancing speech for the ﬁrst DIHARD challenge,” in Proc. Interspeech, 2018, pp. 2793–2797.
[26] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. Interspeech, 2019 (to appear).
[27] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with self-attention,” in Proc. ASRU, 2019 (submitted).
[28] A. Graves and J. Schmidhuber, “Framewise phoneme classiﬁcation with bidirectional lstm and other neural network architectures,” Neural Networks, vol. 18, no. 5, pp. 602 – 610, 2005, iJCNN 2005.
[29] Z. Lin, M. Feng, C. Nogueira dos Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, “A structured self-attentive sentence embedding,” in Proc. ICLR, 2017.
[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. NIPS, 2017, pp. 5998–6008.
[31] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, “Speaker recognition for multi-speaker conversations using x-vectors,” in Proc. ICASSP, 2019, pp. 5796–5800.

[32] V. S. Narayanaswamy, J. J. Thiagarajan, H. Song, and A. Spanias, “Designing an effective metric learning pipeline for speaker diarization,” in Proc. ICASSP, 2019, pp. 5806–5810.
[33] V. A. Miasato Filho, D. A. Silva, and L. G. Depra Cuozzo, “Joint discriminative embedding learning, speech activity and overlap detection for the dihard speaker diarization challenge,” in Proc. Interspeech, 2018, pp. 2818–2822.
[34] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015.
[35] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in Proc. NIPS, 2014, pp. 3104–3112.
[36] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Proc. NIPS, 2015, pp. 577–585.
[37] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, 2016, pp. 4960–4964.
[38] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[39] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron: Towards end-to-end speech synthesis,” in Proc. Interspeech, 2017, pp. 4006–4010.
[40] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. Courville, and Y. Bengio, “Char2wav: End-to-end speech synthesis,” in ICLR Workshop, 2017.
[41] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully supervised speaker diarization,” in Proc. ICASSP, 2019, pp. 6301–6305.
[42] X. Wang, R. B. Girshick, A. Gupta, and K. He, “Non-local neural networks,” in Proc. CVPR, 2018, pp. 7794–7803.
[43] L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal self-attention network for referring image segmentation,” in Proc. CVPR, 2019, pp. 10 502–10 511.
[44] M. Sperber, J. Niehues, G. Neubig, S. Stu¨ker, and A. Waibel, “Selfattentional acoustic models,” in Proc. Interspeech, 2018, pp. 3723–3727.
[45] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition,” Proc. ICASSP, pp. 5884–5888, 2018.
[46] W. Jun and L. Shengchen, “Self-attention mechanism based system for DCASE2018 challenge task1 and task4,” in DCASE2018 Challenge, 2018.
[47] Y. Zhu, T. Ko, D. Snyder, B. Mak, and D. Povey, “Self-attentive speaker embeddings for text-independent speaker veriﬁcation,” in Proc. Interspeech, 2018, pp. 3573–3577.
[48] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35.
[49] D. Yu, M. Kolbæk, Z. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245.
[50] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Trans. on ASLP, vol. 25, no. 10, pp. 1901– 1913, 2017.
[51] J. Lei Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint arXiv:1607.06450, 2016.
[52] NIST, “2000 speaker recognition evaluation plan,” https:// www.nist.gov/sites/default/ﬁles/documents/2017/09/26/spk-2000plan-v1.0.htm .pdf, 2000.
[53] K. Maekawa, “Corpus of spontaneous japanese: Its design and evaluation,” in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.
[54] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.
[55] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech, and noise corpus,” arXiv preprints arXiv:1510.08484, 2015.
[56] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in Proc. ICASSP, 2017, pp. 5220–5224.
[57] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” in Proc. ICLR, 2015.

IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, XXXXX 2019

11

[58] NIST, “The 2009 (RT-09) rich transcription meeting recognition evaluation plan,” http://www.itl.nist.gov/iad/mig/tests/rt/2009/docs/rt09meeting-eval-plan-v2.pdf, 2009.

PLACE PHOTO HERE

Yusuke Fujita received the B.S. and M.S. degree in computer science from Waseda University, Tokyo, Japan, in 2003 and 2005, respectively. Currently, he is a Senior Researcher of Media Intelligent Systems Research Department, Hitachi, Ltd., Tokyo, Japan. He is also a Visiting Scholar at Johns Hopkins University, MD, USA. His research interests include speech recognition, speech separation and speaker diarization.

PLACE PHOTO HERE

Nagamatsu Kenji Biography text here.

Shinji Watanabe is an Associate Research Profes-

sor at Johns Hopkins University, Baltimore, MD,

USA. He received his B.S., M.S., PhD (Dr. Eng.)

PLACE PHOTO HERE

Degrees in 1999, 2001, and 2006, from Waseda University, Tokyo, Japan. He was a research scientist at NTT Communication Science Laboratories, Kyoto, Japan, from 2001 to 2011, a visiting scholar

in Georgia institute of technology, Atlanta, GA in

2009, and a Senior Principal Research Scientist at

Mitsubishi Electric Research Laboratories (MERL),

Cambridge, MA from 2012 to 2017. His research

interests include automatic speech recognition, speech enhancement, spoken

language understanding, and machine learning for speech and language

processing. He has been published more than 200 papers in top journals and

conferences, and received several awards including the best paper award from

the IEICE in 2003. He served an Associate Editor of the IEEE Transactions

on Audio Speech and Language Processing, and is a member of several

technical committees including the IEEE Signal Processing Society Speech

and Language Technical Committee (SLTC) and Machine Learning for Signal

Processing Technical Committee (MLSP).

PLACE PHOTO HERE

Shota Horiguchi received the B.S. degree in information and communication engineering and the M.S. degree in information science and technology from the University of Tokyo, Tokyo, Japan, in 2015 and 2017, respectively. He is currently a Researcher with Hitachi, Ltd, Tokyo, Japan.

PLACE PHOTO HERE

Yawen Xue Biography text here.

