Strategyprooﬁng Peer Assessment via Partitioning: The Price in Terms of Evaluators’ Expertise

Komal Dhull Carnegie Mellon University kdhull@andrew.cmu.edu

Steven Jecmen Carnegie Mellon University
sjecmen@cs.cmu.edu

Pravesh Kothari Carnegie Mellon University praveshk@andrew.cmu.edu

Nihar B. Shah Carnegie Mellon University
nihars@cs.cmu.edu

arXiv:2201.10631v2 [cs.GT] 12 Feb 2022

Abstract
Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientiﬁc papers, and peer assessment of employees in organizations. Since an individual’s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from diﬀerent subsets. Although this method ensures strategyproofness, each submission may require a diﬀerent type of expertise for eﬀective evaluation. In this paper, we focus on ﬁnding an assignment of evaluators to submissions that maximizes assigned evaluators’ expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators’ expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.
1 Introduction
Many applications require evaluation of certain submissions. When the number of submissions is large enough to make independent expert evaluations of all of them infeasible, the individuals who submitted are each asked to evaluate submissions made by their peers. In education, peer grading of homeworks has become increasingly prevalent in Massive Open Online Courses (MOOCs) [2–4] and conventional classrooms. In scientiﬁc research, peer review is used for grant proposals and conference paper submissions [5–7]. In the workplace, peer evaluation is frequently used to assess employee performance and determine employee promotions and bonuses [8, 9].
In many of these applications, peer assessment is competitive, meaning that the eventual outcome of a submission is impacted by the evaluations of other submissions. Examples include a class graded on a curve such that only a certain percentage receives an ‘A’ grade, a conference that intends to accept some ﬁxed fraction of the papers, an agency awarding grants under a certain budget, or a company with a limited number of promotions on oﬀer.
A key challenge in competitive peer assessment is that agents behave strategically: an agent may give low scores to the submissions they evaluate, in the hope that by hurting the chances of those submissions,
The author ordering is alphabetical due to coincidence and not by policy [1].
1

they increase the relative chance of a good outcome for their own submission. A controlled experiment [10] found that people indeed behave in such a strategic manner in competitive peer assessment. Furthermore, the work [11] shows that even a small fraction of agents behaving strategically in scientiﬁc peer review can signiﬁcantly lower the average quality of the accepted papers. It is thus vital to ensure the fairness and integrity of the process by developing mechanisms to prevent such strategic behavior. In fact, the NSF brieﬂy experimented with a method (introduced by [12]) that attempts to prevent strategic behavior in the peer review of research proposals [13], but this method does not come with theoretical guarantees.
By far the most well-studied way of ensuring strategyproofness is the partitioning method introduced in [14] and studied further in [15–22]. Under the partitioning method, submissions are partitioned into some number of subsets, and no agent is assigned a submission from the same subset as their own. The individual agent evaluations are then aggregated separately for each subset, so that any agent’s evaluations cannot inﬂuence the ﬁnal outcome for their own submission.
Apart from strategyproofness, another key aspect in assigning evaluators to submissions is matching based on expertise. For instance, in peer review of papers or proposals, not all agents have expertise for all papers or proposals. Similarly, in peer assessment within an organization, the peer assessors for any employee must be chosen to have a suitable understanding of that employee’s work. In peer grading of essays or projects, the assessors must have the relevant background to do a suitable evaluation. Since the goal of peer assessment is to evaluate each submission as competently as possible, it is important to ensure that each submission is assigned evaluators with suitable expertise, or in other words, to maximize the quality of the assignment of evaluators to submissions.
As both strategyproofness and assignment quality are crucial in many applications, our work studies the problem of ﬁnding a strategyproof assignment with maximum assignment quality. The key question we ask is: what is the price paid by strategyprooﬁng in terms of the assigned evaluators’ expertise? As a metric of evaluation, we use the ratio of the quality we obtain with strategyproofness to the maximum quality achievable without the strategyproofness constraint.
Our work contributes to the body of literature on analyzing the price of strategyproofness in various settings [21, 23–26]. This includes a line of work on impartial peer nomination/selection [14–20, 27], which focuses on selecting the best k submissions in a strategyproof manner given an proﬁle of evaluations. In contrast, we optimize the assignment of evaluators to submissions subject to a strategyproofness constraint and characterize the price of strategyproofness in terms of the assigned evaluators’ expertise. Further, our setting generalizes the standard peer selection setting, since evaluations may be used for various relative grading schemes other than best-k selection. The prior work closest to ours is [22], which considers the partitioning mechanism speciﬁcally for conference peer review. They provide an algorithm that utilizes partitioning and conduct empirical analysis on its quality. However, they provide no theoretical guarantees on their algorithm’s assignment quality.
With that background, we now list our main contributions:
1. We establish fundamental limits on the amount of compromise that must be made on the assignment quality in order to impose strategyproofness via partitioning.
2. We present polynomial-time computable algorithms that are optimal in the worst case.
3. We show that the problem of instance-wise optimal strategyproof assignment via partitioning is NPhard.
4. We conduct experimental evaluations on data from the peer-review process of the ICLR 2018 conference, where we ﬁnd that our algorithms achieve high-expertise assignments while producing fair partitions of papers.
We accomplish these goals using various techniques: applying combinatorial methods, drawing a connection to equitable graph coloring, and formulating our problem as a max-cut problem.
Apart from considering strategyproofness and assignment quality together, we note two points of contrast of our work as compared to the literature. First, previous works on strategyproof partitioning consider a
2

uniform random partition in order to ensure fairness: that is, to ensure that no partition contains disproportionately strong or disproportionately weak submissions. In our work, we analyze the random partition approach and use it as a baseline for the rest of our results. Moreover, we conduct experiments using data from the ICLR conference, which reveal that the non-random partition output by our algorithms does not result in any substantial unfairness. Second, the work [22], which deals with both assignment quality and strategyprooﬁng, considers arbitrary authorships where each submission may have multiple authors and each agent may have authored multiple submissions. In contrast, our theoretical analysis restricts attention to each agent having authored one submission and each submission being authored by one agent. Such one-to-one authorship occurs often in peer grading, peer assessment of employees, or peer review of certain proposals, and is equivalent to common settings in the strategyprooﬁng literature [14–21,27]. In Appendix A, we further provide an extension and empirical evaluation that handles arbitrary authorships.
All of the code for our algorithms and our empirical results is freely available online at https://github. com/sjecmen/optimal_strategyproof_assignment.
2 Background and Problem Formulation
We consider a setting of peer assessment between agents, where each agent ﬁrst submits some work for evaluation and is then assigned to evaluate other agents’ submissions. After evaluations have been completed, submissions can be compared based on the evaluation scores in order to determine any competitive outcomes, such as relative grades (in a classroom setting), accept/reject decisions (in conference peer review), or employee bonuses and promotions (in an organization).
2.1 Preliminaries
Let A = {a1, . . . , an} be the set of agents and let P = {p1, . . . , pn} be the set of submissions from the agents. We assume that each agent ai (i ∈ [n]) authors exactly one submission pi. (This is equivalent to common settings in the strategyprooﬁng literature [15–18,21]. Furthermore, we handle arbitrary authorships in Appendix A.)
A key focus of our work is the assignment of agents to submissions for review. Constructing a highquality assignment for peer assessment (in the absence of strategyprooﬁng requirements) is a well-studied problem, and is conducted in two phases. The ﬁrst phase involves computing a “similarity” between every agent-submission pair, a number between 0 and 1 where a higher value indicates a better match in terms of expertise. Similarities are computed in various ways [28–31]. Our work is agnostic to the method used to compute similarity scores. We assume we are given a matrix S ∈ [0, 1]n×n of ‘similarity scores’ for each agent-submission pair that capture the expertise of each agent to evaluate each submission. For any i ∈ [n], j ∈ [n], the (i, j)th entry of matrix S, denoted by si,j, represents the similarity between agent ai and submission pj, where a higher value means that one expects a better quality of evaluation.
2.2 Assignments
The second phase of the assignment process then uses the similarities to assign submissions to agents. For a predeﬁned value k ∈ Z+, an assignment with loads of k is deﬁned as a set M ⊆ A × P of assigned agent-submission pairs where each submission is assigned exactly k agents, each agent is assigned to exactly k submissions, and no agent is assigned to their own submission. It is important to note that in our applications of interest, the “load” k is typically a small constant independent of n, and we will assume so throughout this paper.
The assignment is chosen by maximizing a speciﬁed objective subject to the load constraints. By far the most common choice of objective is to maximize the sum of the assigned similarities [28, 32–36], and this approach is widely used in practice: for instance, in IJCAI, NeurIPS, AAAI and other conferences. Formally, for any assignment M ⊆ A × P, the total similarity is given by (ai,pj)∈M si,j. Fixing some k, deﬁne M∗S
3

as the maximum-similarity assignment

M∗S = argmax

si,j

M⊆A×P (ai,pj )∈M

subject to

I[(ai, pj) ∈ M] = k

ai ∈A

I[(ai, pj) ∈ M] = k
pj ∈P
(ai, pi) ∈ M

(1a)

∀pj ∈ P

(1b)

∀ai ∈ A

(1c)

∀ai ∈ A.

(1d)

The optimal assignment (without strategyproofness) M∗S can be found eﬃciently via standard methods such as min-cost ﬂow algorithms or linear programming. Let OptS be the similarity of M∗S (leaving dependence on k implicit in the notation); that is, OptS is the maximum value of the aforementioned objective under the stated constraints. When unambiguous, the subscript S may be omitted.
While we consider the aforementioned popular objective in most of our analysis, we note that another objective that is sometimes used is the leximin or max-min fairness of the assignment [37, 38], which we examine in Section 3.7.
2.3 Strategyproofness via Partitioning
Our goal in this paper is to ﬁnd maximum-similarity strategyproof assignments. A strategyproof assignment is one in which no agent can improve the outcome of their own submission by changing the evaluation they provide.
As introduced earlier, a standard method for constructing strategyproof assignments begins by partitioning the agents into two subsets. An assignment of agents to submissions is then found, where agents can only be assigned to submissions authored by agents in the other subset. After evaluations are completed, any relative grading (e.g., classroom grading or accept/reject decisions) is done independently within each subset. Thus, the evaluation provided by any agent cannot inﬂuence the ﬁnal outcome of their own submission.
In this paper, we use the term “strategyproof-via-partitioning” speciﬁcally to describe assignments produced in this way.

Deﬁnition 1. An assignment M is strategyproof-via-partitioning if there exists a partition of A into two subsets A1, A2 such that

(ai, pj) ∈ M ∀ai, aj ∈ At; ∀t ∈ {1, 2}

(2a)

A1 ∪ A2 = A; A1 ∩ A2 = ∅.

(2b)

In Section 3.6, we extend this deﬁnition to allow for partitioning into more than two subsets. Our goal is to ﬁnd a maximum-similarity strategyproof-via-partitioning assignment

argmax

si,j

A1,A2⊆A;M⊆A×P (ai,pj )∈M

subject to (1b) − (1d), (2a), (2b).

If an assignment satisﬁes (2a) for some partition, we say that assignment “respects” the partition; we say
that a pair (ai, pj) respects the partition if ai and aj are in diﬀerent subsets. Note that the load constraints imply |A1| = |A2| for any feasible solution, so we assume that n is even in all of our results; we also assume that k ≤ n2 for feasibility.
Given a partition (A1, A2), ﬁnding the maximum-similarity assignment can be done via standard methods by additionally disallowing any pairs violating constraint (2a). Thus, the primary question we consider in
this paper is how to optimally choose the partition in order to maximize the similarity of the resulting
assignment.

4

Algorithm 1 Random Partition
Input: A, P, S, k 1: Sample A1 uniformly at random from {A : A ⊆ A, |A | = |A|/2} 2: A2 ← A \ A1 3: M ← max-similarity assignment with loads k respecting (A1, A2) 4: return assignment M and partition (A1, A2)

2.4 Evaluation Metric
We evaluate a strategyproof-via-partitioning assignment algorithm in terms of the ratio between the similarity of the assignment it produces and OptS, the similarity of the optimal non-strategyproof assignment. Speciﬁcally, consider any assignment algorithm that, given input similarities S, produces a strategyproofvia-partitioning assignment denoted by MS. We evaluate its performance in terms of the worst-case input similarities as:

min
S:OptS >0

(ai,pj )∈MS si,j .
OptS

3 Theoretical Results
In this section, we present our main theoretical results.
3.1 Baseline: Random Partitioning
We begin with a result that provides a simple baseline for comparison: Algorithm 1 chooses a partition uniformly at random. This is the approach taken by most prior literature on partitioning-based mechanisms. It is easy to show that such a uniformly random partition can attain at least half of the optimal similarity.

Proposition 1. For any k and any S, Algorithm 1 ﬁnds a strategyproof-via-partitioning assignment with similarity at least 12 OptS in expectation.
Proof. Since it is feasible to assign all pairs in M∗S that respect the partition, Algorithm 1 achieves expected similarity





EM 

si,j  ≥

si,j(P[ai ∈ A1, aj ∈ A2] + P[aj ∈ A1, ai ∈ A2])

(ai,pj )∈M

(ai,pj )∈M∗S

n1 = si,j n − 1 2
(ai,pj )∈M∗S

1 ≥ 2 OptS.

Note that this bound on the expected performance of random partitioning is tight in the limit as n grows: in the worst-case over similarities, Algorithm 1 achieves exactly n−n 1 12 Opt similarity. This occurs when all agent-submission pairs assigned by M∗ have similarity 1, and all other pairs have similarity 0.
3.2 Worst-Case Upper Bound
Since 12 Opt is easily attainable, the next natural question is: how much better is achievable? We establish an upper bound of 2kk++11 Opt on the worst-case performance of any strategyproof-via-partitioning assignment algorithm.

5

Algorithm 2 Cycle-Breaking Algorithm

Input: Agents A, papers P, similarities S, load k

1: M∗S ← max-similarity assignment with loads 1

2: A1 ← ∅; A2 ← ∅

3: for cycle γ of length in M∗S do

4:

y ← mini∈[ ] sγi,γi+1

5: A ← ∅; B ← ∅

6: for i ∈ [ ] do

7:

j ← y + i mod

8:

if i odd then

9:

A ← A ∪ {aγj }

10:

else

11:

B ← B ∪ {aγj }

12:

end if

13: end for

14: if |A1| ≤ |A2| then

15:

A1 ← A1 ∪ A; A2 ← A2 ∪ B

16: else

17:

A1 ← A1 ∪ B; A2 ← A2 ∪ A

18: end if

19: end for

20: M ← max-similarity assignment with loads k respecting (A1, A2)

21: return assignment M and partition (A1, A2)

Theorem 1. For any k and any n, there exist similarities S for n agents such that no strategyproof-viapartitioning assignment has similarity greater than 2kk++11 OptS.
Proof. Place the agents into groups of size 2k + 1, leaving any remaining agents out. Within each complete group, number the agents from 0 to 2k. For all i from 0 to 2k, set the similarity of ai and pi+1, . . . , p(i+1+k) mod 2k+1 to 1. Set all other similarities to 0. On these similarities, M∗ can assign every similarity-1 pair, for a total of k(2k + 1) per group. The optimal partition splits each group into subsets of size k and k + 1, allowing at most k(k + 1) similarity-1 pairs to be assigned in each group.
3.3 Cycle-Breaking Algorithm
In this section, we present a simple algorithm that meets the upper bound of Theorem 1 when k = 1. Deﬁne a “cycle” γ of length in an assignment as an ordered list of indices γ1, . . . , γ such that agent
aγi is assigned to submission pγi+1 (deﬁning γ +1 = γ1). In any assignment with loads k = 1, the full set of indices [n] can be uniquely partitioned into such cycles, since each agent is assigned to one submission and each submission is assigned one agent.
Algorithm 2 works by splitting each cycle in the optimal k = 1 assignment across the partition in the way that maximizes similarity. The following theorem shows a lower bound on the similarity of the strategyproof-via-partitioning assignment produced by this algorithm when k = 1.
Theorem 2. When k = 1, for any S, Algorithm 2 ﬁnds a strategyproof-via-partitioning assignment with similarity at least 23 OptS in polynomial time.
Proof. (A1, A2) is a partition of A since each agent is included in exactly one cycle in M∗S. Further, |A1| = |A2| since agents are added to the partition to keep it as balanced as possible and we assume n is even.
We bound the value of the returned assignment M when k = 1. By construction, at most one agentsubmission pair in each cycle of M∗S does not respect the partition. Any cycle containing such a disallowed
6

Algorithm 3 Coloring Algorithm
Input: Agents A, papers P, similarities S, load k 1: M∗S ← max-similarity assignment with loads k 2: GM∗ ← directed graph representing M∗S 3: f ← equitable (2k + 2)-coloring of GM∗ 4: for T ∈ {T : T ⊆ [2k + 2], |T | = k + 1} do
5: AT ← {ai : vi ∈ V, f (vi) ∈ T } 6: AT ← {ai : vi ∈ V, f (vi) ∈ T } 7: xT ← ai∈AT ,aj∈AT si,j I[(ai, pj ) ∈ M∗S ] 8: end for 9: T ∗ = argmaxT xT 10: A1 ← AT ∗ ; A2 ← AT ∗ 11: M ← max-similarity assignment with loads k respecting (A1, A2) 12: return assignment M and partition (A1, A2)

pair must be of length at least three, and the disallowed pair must have the minimum similarity among all assigned pairs in the cycle. Since it is feasible to assign all pairs in M∗S that respect the partition, the value of the strategyproof-via-partitioning assignment must be at least 23 OptS.
The partitioning step can be done in O(n) time, since each agent is considered once, and ﬁnding the two
maximum-similarity matchings can be done with high probability in O(n3) time [39].

3.4 Coloring Algorithm

In this section, we present another algorithm for strategyproof peer assessment, which meets the upper bound
of Theorem 1 for any k. The algorithm begins by constructing a directed graph GM∗ representing the optimal assignment M∗. This graph contains one vertex vi for all i ∈ [n], and an edge (vi, vj) if (ai, pj) ∈ M∗. We then ﬁnd an equitable coloring of this graph, which is deﬁned as follows.

Deﬁnition 2. For any α ∈ Z+, an equitable α-coloring of a directed graph G = (V, E) is a function f : V → [α] such that f (vi) = f (vj) ∀(vi, vj) ∈ E and |{v : f (v) = x}| − |{v : f (v) = y}| ≤ 1 ∀x, y ∈ [α].

The following well-known result shows that an equitable coloring of limited size can be found in polynomial time.

Theorem 3. [40, 41] A graph G = (V, E) with maximum degree at most ∆ has an equitable ∆ + 1-coloring that can be found in O(∆|V |2) time.

Algorithm 3 uses this result as a subroutine to ﬁnd an equitable (2k + 2)-coloring of GM∗ . It then partitions the colors in the way that maximizes the total similarity of pairs in M∗ split by the partition.
The following result proves that this algorithm is worst-case optimal.

Theorem 4. For any k and any S, if n is divisible by 2k+2, Algorithm 3 ﬁnds a strategyproof-via-partitioning assignment with similarity at least 2kk++11 OptS in polynomial time.

Proof. Each vertex in GM∗ has in-degree and out-degree k, so the maximum (total) degree is at most 2k.

Therefore, Line 3 can be implemented using Theorem 3 as a subroutine. Further, since n is divisible by

2k + 2,

all

colors

have

exactly

n 2k+2

vertices

and

so

|A1|

=

|A2|.

Next, we bound the value of the returned assignment M. Suppose we modify Line 4 to choose T uniformly

at random from the set. Then, the expectation of xT in Line 7 is E [xT ] = (ai,pj)∈M∗S si,j 2(kk++11)−1 =
2kk++11 OptS. Therefore, xT ∗ ≥ 2kk++11 OptS. Since it is feasible to assign all pairs whose similarity is counted in xT ∗ , the assignment M has similarity at least xT ∗ .
Assuming k is constant, the time complexity of the partitioning step is dominated by the O(n2) time

taken to ﬁnd the equitable coloring. Finding the two maximum-similarity matchings can be done with high probability in O(n3) time [39].

7

Algorithm 4 Multi-Partition Algorithm Input: Agents A, papers P, similarities S, load k
1: M∗S ← max-similarity assignment with loads k 2: GM∗ ← directed graph representing M∗S 3: f ← equitable (2k + 1)-coloring of GM∗ 4: return assignment M∗S and partition with 2k + 1 subsets ({aj : vj ∈ V, f (vj) = i}i∈[2k+1])
The assumption that n is divisible by 2k + 2 is needed to guarantee that the partition is balanced. However, for arbitrary n, the subsets of the partition diﬀer in size by only k + 1 agents at most. If there are a small number of “reserve” agents who did not submit any work and are not used in M∗, these agents can provide any evaluations needed for a feasible assignment. Since k is a small constant (often ≤ 3), having access to enough reserve agents is likely not an issue in practice. For example, in a scientiﬁc peer review setting, many extra non-author reviewers are available; in a classroom setting, an instructor could grade the extra submissions.
3.5 Hardness
Although our algorithms are optimal on the worst-case input, one might hope for algorithms that can guarantee optimal performance on all inputs. However, the following result shows that when k ≥ 2, this is NP-hard.
Theorem 5. For any k ≥ 2, it is NP-hard to ﬁnd the optimal strategyproof-via-partitioning assignment, even when similarities are binary (that is, when S ∈ {0, 1}n×n).
Proof Sketch. The proof is by reduction from the “Simple Max Cut on Cubic Graphs” problem [42]. We construct an instance of the strategyproof-via-partitioning assignment problem where each agent corresponds to a vertex. For some orientation of the input graph, we set sij = 1 for each directed edge (vi, vj), and set similarities to zero elsewhere. These edges could all be assigned by M∗ when k ≥ 2, but the optimal strategyproof-via-partitioning assignment is limited to the max-cut value in the original graph.
The complete proof is provided in Appendix B.
3.6 Partitions With More Than Two Subsets
We now relax the deﬁnition of “strategyproof-via-partitioning” given in Deﬁnition 1. Rather than requiring that agents be partitioned into two subsets, we allow them to be partitioned into any constant (i.e., not depending on n) number of subsets. This slight relaxation of our problem formulation allows us to obtain a strategyproof-via-partitioning assignment that achieves total similarity OptS for any S.
Theorem 6. For any k ≥ 1 and any S, Algorithm 4 ﬁnds a partition of agents into 2k+1 subsets, where each subset contains either 2kn+1 or 2kn+1 agents, and a strategyproof-via-partitioning assignment respecting this partition in polynomial time. This assignment has total similarity OptS.
Proof. Each vertex in GM∗ has in-degree and out-degree k, so the maximum (total) degree is at most 2k. Therefore, by Theorem 3 we can ﬁnd an equitable (2k + 1)-coloring of GM∗ in O(n2) time. By Deﬁnition 2, the entirety of M∗S respects the partition induced by the coloring and so is strategyproof-via-partitioning with respect to this partition. Also by Deﬁnition 2, all color classes diﬀer in size by at most 1.
Algorithm 4 constructs a directed graph representing M∗ as described in Section 3.4. It then ﬁnds an equitable (2k + 1)-coloring using Theorem 3 and uses this coloring as the partition.
Although we can recover the entire optimal similarity with this method, increasing the number of subsets comes at the cost of reliability in determining the post-evaluation outcomes, since all relative outcomes must be chosen independently in each subset. In Section 4, we experimentally examine this cost.
8

Fraction of optimal similarity lost

0.4 random

0.3

cycle-breaking coloring

multi-partition

0.2

0.1

0.0 1

2

3

k

Figure 1: Assignment similarity lost on data from ICLR 2018.

3.7 Fairness Objective
So far we have analyzed the objective of maximizing total similarity (1) due to its widespread use. However, this objective has been found to result in imbalanced or unfair assignments [38]. An alternative proposed in the literature is to optimize the max-min fairness objective, which maximizes the total similarity assigned to the submission with minimum assigned similarity [37, 38, 43]. Formally, the problem of ﬁnding the optimal strategyproof-via-partitioning assignment under this objective is:

argmax

min

si,jI[(ai, pj) ∈ M]

A1,A2⊆A;M⊆A×P pj ∈P ai∈A

subject to (1b) − (1d), (2a), (2b).

Assignment algorithms optimizing this objective have been used in venues such as ICML 2020 and implemented in conference management platforms such as OpenReview.net.
In this section, we analyze the price of strategyprooﬁng under this max-min objective. The following result shows that unfortunately, we cannot hope to do well on this objective in the worst-case.
Theorem 7. For any k and any n ≥ 6, there exist similarities S on n agents such that the optimal non-strategyproof assignment has max-min objective value strictly greater than 0 while no strategyproof-viapartitioning assignment has a max-min objective value greater than 0.
Proof. Split the agents into two groups such that both groups have an odd number of agents at least 3; this is possible since we assume n is even. Within each group {aγ1 , . . . , aγ } of size , set similarities sγi,γi+1 = 1 for all i ∈ [ − 1] and sγ ,γ1 = 1. Set similarities to 0 elsewhere. On these similarities, the optimal nonstrategyproof assignment can assign every similarity-1 pair for a max-min fairness of 1. However, since the number of reviewers in each group is odd, any partition of A into two subsets must place two agents aγi , aγi+1 (or aγ , aγ1 ) from each group in the same subset. Therefore, some submission pγi+1 from each group will have an assigned similarity of 0.

4 Experimental Results
In this subsection, we experimentally examine the performance of algorithms for strategyproof-via-partitioning assignment.

9

Frequency in subset

400

Oral

Poster

400

Oral Poster

Workshop Reject

Workshop

300

300

Reject

Frequency in subset

200

200

100

100

0 cycle-breaking coloring multi-partition

0cycle-breaking coloring multi-partition

50 40 30 20 10 02

(a) Partitioned paper decisions,
k=1 50

40

30

20

10

4

6

8

02

Mean review score

Frequency in subset Frequency in subset

(b) Partitioned paper decisions, k=3

M4ean review6 score 8

30

20

10

02

4

6

8

Mean review score

(c) Partitioned paper scores for

(d) Partitioned paper scores for

the cycle-breaking algorithm, k = 1 the coloring algorithm, k = 1

(e) Partitioned paper scores for the multi-partition algorithm, k = 1

Figure 2: Partition quality on data from ICLR 2018.

Frequency in subset

4.1 Setup
We evaluate our algorithms on data from the peer-review process at ICLR 2018. We use similarities recreated in [22]. To evaluate the partition quality, we also use the actual review scores and the accept/reject decisions at the ICLR 2018 conference [44].
Since our algorithms require that each agent authors exactly one submission, we ﬁnd a maximum one-toone matching on the real authorship graph and use this as the authorship for our experiments. This resulted in matching 883 out of the 911 papers. We then discarded any reviewers and papers not included in the authorship graph. Any additional reviewers required for feasibility (due to the divisibility of n) have zero similarity with all papers.
We evaluate four partitioning algorithms: random partitioning (Algorithm 1), the cycle-breaking algorithm (Algorithm 2), the coloring algorithm (Algorithm 3), and the multi-partition algorithm (Algorithm 4). Since each paper received 3 reviews at ICLR 2018, we test values of k ∈ {1, 2, 3}.
Additional experimental results are available in Appendix C.
4.2 Assignment Similarity
We ﬁrst examine the similarity of the strategyproof-via-partitioning assignments produced by each algorithm. In Figure 1, we report the price of strategyproofness: the diﬀerence in total similarity between the proposed algorithm’s assignment and the optimal non-strategyproof assignment, as a fraction of the optimal assignment’s total similarity. Results for the random partitioning algorithm are averaged over 100 trials, with error bars representing standard error of the mean. As expected from our theoretical results, the multipartition algorithm achieves the full similarity of the optimal non-strategyproof assignment. On all values

10

Algorithm k p

D

Cycle-breaking - 0.9007 0.0373 Coloring 1 0.8902 0.0379 2 0.6445 0.0487 3 0.5389 0.0530
Multi-partition 1 0.4282 0.0702 2 0.6805 0.0742 3 0.3457 0.1142

Table 1: Results of the Kolmogorov-Smirnov test of whether the review scores in the two partitioned subsets are drawn from the same distribution.

of k, the cycle-breaking algorithm performs very well: it loses less than 1% of the optimal similarity when k = 1, and furthermore, it outperforms the coloring algorithm even for higher values of k (where it does not have theoretical guarantees). The coloring algorithm loses around 12% of the optimal similarity for all values of k. The baseline of random partitioning still loses less than 20% of the optimal similarity, but is outperformed by the other algorithms. Overall, on real data our algorithms perform quite well in terms of the quality of the assignment as compared to the optimal non-strategyproof assignment.
4.3 Partition Quality
We next examine whether the partitions produced by these algorithms place similar-quality papers into each subset, since under the partition-based method, the ﬁnal accept/reject decisions for papers are performed independently in each subset. In Figures 2a and 2b, we display the number of papers receiving each decision (oral presentation, poster presentation, invitation to workshop track, or rejection) in each subset of the partitions. For each algorithm, each bar displays the decisions for the papers in one subset of the partition. Across all algorithms and values of k, the partitions constructed have very similar numbers of papers receiving each decision in each subset.
Further, in Figures 2c, 2d, and 2e, we show the mean review scores given to each paper for the case of k = 1. In Figures 2c and 2d, the red and blue histograms correspond to the scores given to the papers in the two subsets of the algorithm’s partition, with the purple section indicating their overlap; in Figure 2e, the third subset is additionally indicated in yellow. For all algorithms, the distributions of scores appear very similar across subsets of the partition. Formally, we test the diﬀerence between the score distributions of diﬀerent subsets via the two-sample Kolmogorov-Smirnov test, a non-parametric test of the null hypothesis that the two samples came from the same distribution. Each sample is the set of scores given to the papers in one subset of the partition. We report the results of the test in Table 1, which contains the p-values of the test along with the eﬀect size D, deﬁned as the maximum diﬀerence between the empirical cdfs of the two samples. For the multi-partition algorithm, we test each pair of subsets and we report results for the pair with highest D. In all cases, the p-values are high, meaning that the test cannot reject the hypothesis that the subsets were drawn from the same distribution.
These experiments provide evidence that the partitions created by our algorithms do not have any substantial diﬀerence in the quality of papers in each subset.
5 Discussion
We jointly considered two key aspects of the peer-assessment process—strategyprooﬁng and assignment quality—and derived fundamental limits as well as designed computationally-eﬃcient algorithms that achieve these limits. Our theoretical and empirical contributions lead to several directions of future work.
A ﬁrst key direction of future work is to extend these theoretical results to arbitrary authorship graphs, as in conference peer review. We present a heuristic algorithm with an empirical evaluation in Appendix A, but the problem of establishing fundamental limits and optimal algorithms is open. Second, most of our work considered worst-case guarantees, while showing that it is NP-hard to attain instance-wise optimality.

11

However, our experimental results showed that our algorithms perform much better than worst-case on real-world instances. This suggests a theoretically interesting and practically useful direction of future work: designing algorithms with approximately-optimal instance-wise guarantees. Third, in contrast to past work, our partitions are non-random. Building on our experimental results revealing that these non-random partitions still result in subsets with roughly equal submission strengths, future work could dig deeper into this phenomenon both theoretically and empirically. Fourth, recent work [20] provides a strategyproof algorithm with theoretical guarantees that does not rely on partitioning. Even though partitioning is by far the dominant way of strategyprooﬁng, it is of interest to extend our results to such strategyprooﬁng methods that may not employ partitioning. Finally, there are various other types of strategic or dishonest behavior in peer assessment [45–53] and the design of computational methods to mitigate such behavior is vital. More generally, peer assessment is an important application with a broad set of challenges including subjectivity [54, 55], miscalibration [56, 57], biases [6, 58, 59], and others [7, 30, 31, 60, 61].
Acknowledgments
This work was supported by NSF CAREER award 1942124 and a Google Research Scholar Award. We thank Ariel Procaccia for helpful discussions.
References
[1] Jingyan Wang and Nihar Shah. There’s lots in a name (whereas there shouldn’t be). Research on Research blog. https://researchonresearch.blog/2018/11/28/theres-lots-in-a-name/ (accessed Jan 25, 2022), 2018.
[2] Nihar B. Shah, Joseph K. Bradley, Abhay Parekh, Martin Wainwright, and Kannan Ramchandran. A case for ordinal peer-evaluation in MOOCs. In NIPS Workshop on Data Driven Education, 2013.
[3] Jorge D´ıez Pela´ez, O´ scar Luaces Rodr´ıguez, Amparo Alonso Betanzos, Alicia Troncoso, and Antonio Bahamonde Rionda. Peer assessment in MOOCs using preference learning via matrix factorization. In NIPS Workshop on Data Driven Education, 2013.
[4] Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and Daphne Koller. Tuned models of peer assessment in MOOCs. arXiv:1307.2579, 2013.
[5] Nihar B. Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike von Luxburg. Design and analysis of the NIPS 2016 review process. arXiv:1708.09794, 2017.
[6] Andrew Tomkins, Min Zhang, and William D. Heavlin. Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713, 2017.
[7] Nihar B. Shah. An overview of challenges, experiments, and computational solutions in peer review. Preprint http://bit.ly/PeerReviewOverview (accessed Jan 25, 2022); To appear in CACM, December 2021.
[8] K.N. Wexley and Richard Klimoski. Performance appraisal: An update. Research in Personnel and Human Resources Management, 2:35–79, 01 1984.
[9] Alessandro Di Fiore and Marcio Souza. Are peer reviews the future of performance evaluations? Harvard Business Review, Jan 2021.
[10] Stefano Balietti, Robert L. Goldstone, and Dirk Helbing. Peer review and competition in the art exhibition game. Proceedings of the National Academy of Sciences, 113(30):8414–8419, 2016.
[11] Stefan Thurner and Rudolf Hanel. Peer-review in a world with rational scientists: Toward selection of the average. The European Physical Journal B, 84(4):707–711, 2011.
[12] Michael R. Merriﬁeld and Donald G. Saari. Telescope time without tears: A distributed approach to peer review. Astronomy & Geophysics, 50(4):4–16, 2009.
[13] Parinaz Naghizadeh and Mingyan Liu. Incentives, quality, and risks: A look into the NSF proposal review pilot. arXiv:1307.6528, 2013.
[14] Noga Alon, Felix Fischer, Ariel Procaccia, and Moshe Tennenholtz. Sum of us: Strategyproof selection from the selectors. In Conference on Theoretical Aspects of Rationality and Knowledge, pages 101–110. ACM, 2011.
[15] Ron Holzman and Herv´e Moulin. Impartial nominations for a prize. Econometrica, 81(1):173–196, 2013.
12

[16] Nicolas Bousquet, Sergey Norin, and Adrian Vetta. A near-optimal mechanism for impartial selection. In International Conference on Web and Internet Economics, pages 133–146. Springer, 2014.
[17] Felix Fischer and Max Klimm. Optimal impartial selection. SIAM Journal on Computing, 44(5):1263–1285, 2015.
[18] Haris Aziz, Omer Lev, Nicholas Mattei, Jeﬀrey S. Rosenschein, and Toby Walsh. Strategyproof peer selection: Mechanisms, analyses, and experiments. In AAAI, pages 397–403, 2016.
[19] Haris Aziz, Omer Lev, Nicholas Mattei, Jeﬀrey S. Rosenschein, and Toby Walsh. Strategyproof peer selection using randomization, partitioning, and apportionment. Artiﬁcial Intelligence, 2019.
[20] Nicholas Mattei, Paolo Turrini, and Stanislav Zhydkov. PeerNomination: Relaxing exactness for increased accuracy in peer selection. In IJCAI, pages 393–399, 2020.
[21] Anson Kahng, Yasmine Kotturi, Chinmay Kulkarni, David Kurokawa, and Ariel D. Procaccia. Ranking wily people who rank each other. In AAAI, 2018.
[22] Yichong Xu, Han Zhao, Xiaofei Shi, and Nihar B. Shah. On strategyproof conference peer review. In IJCAI, 2019.
[23] Ariel D. Procaccia and Moshe Tennenholtz. Approximate mechanism design without money. ACM Transactions on Economics and Computation (TEAC), 1(4):1–26, 2013.
[24] Shaddin Dughmi and Arpita Ghosh. Truthful assignment without money. In Proceedings of the 11th ACM conference on Electronic commerce, pages 325–334, 2010.
[25] Elias Koutsoupias. Scheduling without payments. Theory of Computing Systems, 54(3):375–387, 2014.
[26] Itai Ashlagi, Felix Fischer, Ian A. Kash, and Ariel D. Procaccia. Mix and match: A strategyproof mechanism for multi-hospital kidney exchange. Games and Economic Behavior, 91:284–296, 2015.
[27] David Kurokawa, Omer Lev, Jamie Morgenstern, and Ariel D. Procaccia. Impartial peer review. In IJCAI, pages 582–588, 2015.
[28] L. Charlin and R. S. Zemel. The Toronto Paper Matching System: An automated paper-reviewer assignment system. In ICML Workshop on Peer Reviewing and Publishing Models, 2013.
[29] David Mimno and Andrew McCallum. Expertise modeling for matching papers with reviewers. In ACM SIGKDD, pages 500—-509, 2007.
[30] Tanner Fiez, Nihar Shah, and Lillian Ratliﬀ. A SUPER* algorithm to optimize paper bidding in peer review. In UAI, pages 580–589, 2020.
[31] Reshef Meir, J´eroˆme Lang, Julien Lesca, Natan Kaminsky, and Nicholas Mattei. A market-inspired bidding scheme for peer review paper assignment. In Games, Agents, and Incentives Workshop at AAMAS, 2020.
[32] Judy Goldsmith and Robert H. Sloan. The AI conference paper assignment problem. In AAAI Workshop on Preference Handling for Artiﬁcial Intelligence, pages 53–57, 2007.
[33] Camillo J. Taylor. On the optimal assignment of conference papers to reviewers. Preprint, 2008.
[34] Wenbin Tang, Jie Tang, and Chenhao Tan. Expertise matching via constraint-based optimization. In International Conference on Web Intelligence and Intelligent Agent Technology, volume 1, pages 34–41, 2010.
[35] Laurent Charlin, Richard Zemel, and Craig Boutilier. A framework for optimizing paper matching. In UAI, pages 86–95, 2011.
[36] Baochun Li and Y. Thomas Hou. The new automated IEEE INFOCOM review assignment system. IEEE Network, 30(5):18–24, 2016.
[37] Naveen Garg, Telikepalli Kavitha, Amit Kumar, Kurt Mehlhorn, and Julia´n Mestre. Assigning papers to referees. Algorithmica, 58(1):119–136, 2010.
[38] Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewer assignment in peer review. In Algorithmic Learning Theory, 2019.
[39] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and Di Wang. Minimum cost ﬂows, MDPs, and 1-regression in nearly linear time for dense instances. arXiv:2101.05719, 2021.
[40] Andr´as Hajnal and Endre Szemer´edi. Proof of a conjecture of P. Erdos. Combinatorial Theory and its Applications, 2:601–623, 1970.
13

[41] Henry A. Kierstead, Alexandr V. Kostochka, Marcelo Mydlarz, and Endre Szemer´edi. A fast algorithm for equitable coloring. Combinatorica, 30(2):217–224, 2010.
[42] Mihalis Yannakakis. Node-and edge-deletion NP-complete problems. In ACM Symposium on Theory of Computing, pages 253–264, 1978.
[43] Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness constraints. In ACM KDD, 2019.
[44] Horace He. OpenReview explorer. https://github.com/Chillee/OpenReviewExplorer (accessed May 26, 2021), 2020.
[45] Ivan Stelmakh, Nihar Shah, and Aarti Singh. Catch me if i can: Detecting strategic behaviour in peer assessment. In AAAI, 2021.
[46] Mara Hvistendahl. China’s publication bazaar. Science, 342(6162):1035–1039, 2013. [47] Cat Ferguson, Adam Marcus, and Ivan Oransky. Publishing: The peer-review scam. Nature News, 515(7528):480,
2014. [48] Daniele Fanelli. How many scientists fabricate and falsify research? a systematic review and meta-analysis of
survey data. PLOS One, 4(5):e5738, 2009. [49] David B. Resnik, Christina Gutierrez-Ford, and Shyamal Peddada. Perceptions of ethical problems with scientiﬁc
journal peer review: An exploratory study. Science and Engineering Ethics, 14(3):305–310, 2008. [50] T. N. Vijaykumar. Potential organized fraud in ACM/IEEE computer architecture conferences. https://medium.
com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d (accessed Jan 25, 2022), 2020. [51] Michael L. Littman. Collusion rings threaten the integrity of computer science research. Communications of the ACM, 64(6):43–44, 2021. [52] Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In NeurIPS, 2020. [53] Ruihan Wu, Chuan Guo, Felix Wu, Rahul Kidambi, Laurens van der Maaten, and Kilian Weinberger. Making paper reviewing robust to bid manipulation attacks. arXiv:2102.06020, 2021. [54] Carole J Lee. Commensuration bias in peer review. Philosophy of Science, 82(5):1272–1283, 2015. [55] Ritesh Noothigattu, Nihar Shah, and Ariel Procaccia. Loss functions, axioms, and peer review. Journal of Artiﬁcial Intelligence Research, 2021. [56] Magnus Roos, Jo¨rg Rothe, and Bjo¨rn Scheuermann. How to calibrate the scores of biased reviewers by quadratic programming. In AAAI, 2011. [57] Jingyan Wang and Nihar B. Shah. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In AAMAS, 2019. [58] Ivan Stelmakh, Nihar Shah, and Aarti Singh. On testing for biases in peer review. In NeurIPS, 2019. [59] Emaad Manzoor and Nihar B Shah. Uncovering latent biases in text: Method and application to peer review. In AAAI, 2021. [60] Ivan Stelmakh, Nihar Shah, Aarti Singh, and Hal Daum´e III. Prior and prejudice: The novice reviewers’ bias against resubmissions in conference peer review. In CSCW, 2021. [61] Jingyan Wang, Ivan Stelmakh, Yuting Wei, and Nihar Shah. Debiasing evaluations that are biased by evaluations. In AAAI, 2021.
Appendices
A Heuristic Algorithm for Arbitrary Authorship
In this section, we propose an algorithm for strategyproof-via-partitioning assignment that can accommodate arbitrary authorship of submissions, as opposed to the one-to-one authorship that we assume in our problem formulation (Section 2). This algorithm is closely based on the cycle-breaking algorithm (Algorithm 2) from Section 3.3. We do not have any theoretical guarantees for this algorithm, but we provide evaluations on the ICLR 2018 dataset introduced in Section 4.
14

Algorithm 5 Heuristic Algorithm for Arbitrary Authorship

Input: agents A, papers P, similarities S, authorship graph U , paper load kp, maximum agent load ka
∗
1: M ← max-similarity assignment with loads (ka, kp)

2: {V1, . . . , VN } ← vertices of the connected components of U

3: A ← {ai : i ∈ [N ]}; P ← {pi : i ∈ [N ]}
4: for i, j ∈ [N ] do
∗
5: sij ← aa∈Vi,pb∈Vj sabI[(aa, pb) ∈ M ] + 6: end for

∗
aa∈Vj,pb∈Vi sabI[(aa, pb) ∈ M ]

7: M , (A1, A2) ← output of Algorithm 2 on input (A , P , S , k = 1)

8: T1 ← i:a ∈A Vi; T2 ← i:a ∈A Vi

i1

i2

9: M ← max-similarity assignment with loads (ka, kp) respecting (T1, T2)

10: return assignment M and partition (T1, T2)

A.1 Algorithm
Arbitrary authorship can be represented as a graph U where each agent and each submission are represented as vertices, and an edge between an agent and submission indicates that the agent authored that submission. Since authorship is not one-to-one, the number of agents and submissions may diﬀer and the agent and submission loads need not be the same. Deﬁne kp as the paper load and ka as the maximum agent load. A strategyproof-via-partitioning assignment algorithm in this setting will produce a partition of both agents and submissions, along with an assignment that respects this partition by assigning each submission only agents from the other subset.
Algorithm 5 works by taking a problem instance with arbitrary authorship, using it to construct a (fake) problem instance with one-to-one authorship, and running Algorithm 2 on this fake instance to ﬁnd a partition. Each agent in the fake instance corresponds to a connected component of the authorship graph U. Similarities between fake agents are set equal to the total similarity of pairs in the optimal non-strategyproof assignment that are split between the respective components.
We slightly modify Algorithm 2 to encourage more balanced partitions in this setting before calling it in Line 7. In Lines 14-18 of Algorithm 2, we take the larger of A and B and add it to the smaller of A1 and A2 as measured by the total size of the connected components represented within each set. In addition, we iterate through vertices (when ﬁnding cycles) in the order of largest connected component to smallest.
A.2 Experimental Results
We test Algorithm 5 on the ICLR 2018 dataset, using the full authorship graph from the conference. We set loads of kp = 3 and ka = 6, since these are standard conference loads [22]. As a baseline for comparison, we also test 100 trials of random partitioning, which chooses half of the connected components at random for each subset.
First, we see in Figure 3a that Algorithm 5 outperforms random partitioning in terms of similarity. Our algorithm loses less than 12% of the non-strategyproof optimal similarity, whereas the random partitioning loses 17.6% of optimal on average.
Finally, we examine the partition quality in a similar manner as in Section 4. In Figure 3b, we plot the proportion of papers within each subset of the partition produced by Algorithm 5 that received each decision. We see that the subsets have similar proportions of papers receiving each decision. In Figure 3c, we see that the two subsets also have similar distributions of paper scores, although the number of papers does diﬀer.
Our results are highly comparable to those of [22], who provide a diﬀerent partitioning algorithm and show that it loses 11.4% of the optimal similarity on the ICLR 2018 data with the same loads.
B Proof of Theorem 5
We ﬁrst prove the following supplementary lemma.

15

Fraction of optimal similarity lost
Relative frequency in subset
Frequency in subset

0.4 random 0.3 heuristic
0.2
0.1
0.0
(a) Assignment similarity lost

1.0

Oral

Poster

0.8

Workshop

Reject

0.6

0.4

0.2

0.0

heuristic

(b) Partitioned paper decisions

50

40

30

20

10

02

4

6

8

Mean review score

(c) Partitioned paper scores

Figure 3: Experimental results using Algorithm 5 on the authorship from ICLR 2018.

Lemma 1. Any graph G = (V, E) with maximum degree at most 4 can be oriented in polynomial time such that the in-degree and out-degree of all vertices are at most 2.
Proof. Consider the following procedure, which takes any arbitrary orientation of G and modiﬁes it that it obeys the desired properties. For a vertex v ∈ V , denote the out-degree of v by δo(v) and the in-degree of v by δi(v).
On each iteration, choose vo such that δo(vo) ≥ 3. Consider the set S of all vertices reachable on a directed path from vo. There cannot be any edges from S to vertices outside of S, so v∈S δo(v) ≤ v∈S δi(v). Since v∈S δi(v) + δo(v) ≤ 4|S|, v∈S δo(v) ≤ 2|S|. Therefore, there exists v ∈ S such that δo(v ) ≤ 1. Reversing the direction of all edges on the path from vo to v reduces δo(vo) by 1 and increases δi(vo) by 1, increases δo(v ) by 1 and decreases δi(v ) by 1, and does not change the in- or out-degree of any other vertices. Since δo(v ) ≤ 1 and δi(vo) ≤ 1 originally, this change does not cause any additional constraints to be violated. Therefore, this step can be repeated until all vertices satisfy δo(v) ≤ 2. Reversing the direction of all edges and repeating the entire procedure ensures that all vertices satisfy δi(v) ≤ 2 as well.
Each iteration takes O(|V |) time to ﬁnd a path to an appropriate v . The number of iterations is O(|V |), since each vertex is vo at most twice.
We now prove the main result, showing that it is unlikely that an instance-optimal algorithm for strategyproof-via-partitioning assignment exists if k ≥ 2. We reduce from the “Simple Max Cut on Cubic Graphs” problem, which is NP-complete [42]. An instance of this problem consists of an unweighted, undirected graph G = (V, E) where all vertices have degree 3 and an integer K. The question is: is there a partition of V that cuts at least K edges?
Fix any k ≥ 2. We reduce this problem to a decision variant of our problem, deﬁned as follows. Given agents A, submissions P, similarities S, and a number x, does there exist a strategyproof-via-partitioning assignment with loads of k such that the total similarity is at least x? If it is NP-hard to determine if there is a strategyproof-via-partitioning assignment with similarity at least x, ﬁnding the strategyproof-viapartitioning assignment with maximum similarity must also be NP-hard.
Consider any instance of the max cut problem G = (V, E), K. Construct a graph G by adding |V | disconnected vertices to G. By Lemma 1, we can ﬁnd an orientation of G in polynomial time to get a directed graph Gˆ = (Vˆ , Eˆ) such that all out-degrees and in-degrees are at most 2. For each vertex vi ∈ V , construct one agent ai and their submission pi. For each directed edge (vi, vj) ∈ Eˆ, set similarity Sij = 1; set all other similarities to 0. Set x = K.
Suppose that V has a partition (V1, V2) that cuts at least K edges. Add the disconnected vertices to the subsets so that |V1| = |V2| = |V |. Partition the corresponding agents in the same way. Assign the agent ai to submission pj for each directed edge (vi, vj) ∈ Eˆ cut by the partition. Each agent has an edge to at most 2 submissions and each submission has an edge to at most 2 agents, so these can all be assigned since

16

Frequency in subset Frequency in subset Frequency in subset

50

400

Oral

Poster

40

50

Workshop

40

300

Reject

30

30

200

20

20

100

10

10

0cycle-breaking coloring multi-partition

02

4

6

8

Mean review score

02

4

6

8

Mean review score

(a) Partitioned paper decisions, k=2 50

(b) Partitioned paper scores for
the cycle-breaking algorithm, k = 2 50

(c) Partitioned paper scores for the coloring algorithm, k = 2

Frequency in subset Frequency in subset

40

40

30

30

20

20

10

10

02

4

6

8

Mean review score

02

4

6

8

Mean review score

(d) Partitioned paper scores for

(e) Partitioned paper scores for

the cycle-breaking algorithm, k = 3 the coloring algorithm, k = 3

Figure 4: Additional experimental results on data from ICLR 2018.

k ≥ 2. Assign the remaining agents and submissions arbitrarily, which can be done since the partitions are balanced. This assignment has similarity at least x and is strategyproof-via-partitioning.
Suppose that V does not have a partition that cuts at least K edges. This means that there does not exist a partition of agents such that at least x agent-submission pairs with non-zero similarity can be assigned respecting the partition.
C Additional Experimental Results
In this section, we display additional experimental results regarding the partition quality of our algorithms on the data from ICLR 2018 (introduced in Section 4).
In Figure 4a, we display the number of papers receiving each decision in each subset of the partitions for k = 2, where each bar displays the decisions for the papers in one subset of the partition. The partitions constructed by all algorithms have very similar numbers of papers receiving each decision in each subset.
In Figures 4b-4e, we show the mean review scores given to each paper for the cases of k = 2 and k = 3. As before, the red and blue histograms correspond to the scores given to the papers in each subset of the algorithm’s partition, with the purple section indicating their overlap. For all algorithms, the distribution of scores appear very similar across subsets of the partition. Results for the multi-partition algorithm are not shown, as there are too many subsets for the histogram to be readable.

17

