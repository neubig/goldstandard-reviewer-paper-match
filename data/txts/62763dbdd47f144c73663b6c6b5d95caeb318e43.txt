Low Permutation-rank Matrices: Structural Properties and Noisy Completion

Nihar B. Shah
ML and CS Depts. CMU
nihars@cs.cmu.edu

Sivaraman Balakrishnan
Dept. of Statistics CMU
siva@stat.cmu.edu

Martin J. Wainwright
Dept. of EECS and Statistics UC Berkeley
wainwrig@berkeley.edu

arXiv:1709.00127v1 [stat.ML] 1 Sep 2017

Abstract
We consider the problem of noisy matrix completion, in which the goal is to reconstruct a structured matrix whose entries are partially observed in noise. Standard approaches to this underdetermined inverse problem are based on assuming that the underlying matrix has low rank, or is well-approximated by a low rank matrix. In this paper, we propose a richer model based on what we term the “permutation-rank” of a matrix. We ﬁrst describe how the classical non-negative rank model enforces restrictions that may be undesirable in practice, and how and these restrictions can be avoided by using the richer permutation-rank model. Second, we establish the minimax rates of estimation under the new permutation-based model, and prove that surprisingly, the minimax rates are equivalent up to logarithmic factors to those for estimation under the typical low rank model. Third, we analyze a computationally eﬃcient singular-valuethresholding algorithm, known to be optimal for the low-rank setting, and show that it also simultaneously yields a consistent estimator for the low-permutation rank setting. Finally, we present various structural results characterizing the uniqueness of the permutation-rank decomposition, and characterizing convex approximations of the permutation-rank polytope.
1 Introduction
In the problem of matrix completion, the goal is to reconstruct a matrix based on observations of a subset of its entries [Lau01]. Matrix completion has a variety of applications, including recommender systems [KBV09], image understanding [LS99], credit risk monitoring [VHVVD08], ﬂuorescence spectroscopy [GPH04], and modeling signal-adaptive audio eﬀects [SK11]. We refer the reader to the surveys [Gil14, DR16] for an overview of the vast literature on this topic. Throughout this paper, in order to provide a running example for our modeling, it will be convenient to refer back to a particular variant of a recommender system application. More concretely, suppose that there are n ≥ 2 users and d ≥ 2 items, as well as an unknown matrix M ∗ ∈ [0, 1]n×d that captures the users’ preferences for the items. Speciﬁcally, the (i, j)th entry of M ∗ represents the probability that user i likes item j. The problem is to estimate this preference matrix M ∗ ∈ [0, 1]n×d from observing users’ likes or dislikes for some subset of the items.
Following a long line of past work in this area (e.g., [CJSC13, Gro11, SAJ05, CR09, CT10, KMO10, Rec11, Cha14]), we consider the following form of random design observation model. For a given parameter pobs ∈ (0, 1] and for any user-item pair (i, j), we observe i’s rating for item j with probability pobs. We assume that when an entry (i, j) is observed, we observe a binary value—for
1

instance, {like, dislike} or {0, 1}—which arises as a Bernoulli realization of the true preference Mi∗j.1

More

formally,

we

observe

a

matrix

Y

∈

{0,

1 2

,

1}n×d,

where

 1 with probability pobsMi∗j

(user i likes item j)

Yij = 0 with probability pobs(1 − Mi∗j) (user i dislikes item j)

(1)

1 2

with probability 1 − pobs

(no data available),

for every (i, j) ∈ [n] × [d]. The goal is to estimate the underlying matrix M ∗ based on the observed matrix Y .
It is clear that, if no structural conditions are imposed on the underlying matrix M ∗, then this problem is ill-posed. A classical approach is to impose on a bound on either the rank or the non-negative rank of the matrix. We begin by describing the approach based on the non-negative rank, before turning to the alternative approach based on permutation rank that is the focus of this paper.

Non-negative rank: In the problem of non-negative low-rank matrix completion, the matrix M ∗ is assumed to have a factorization of the form
M∗ = UV T,

for some matrices

U

∈

Rn×r
+

and V

∈ Rd+×r.

Here the integer

r ∈ {1, . . . , min{d, n}}

is known as

the non-negative rank of the matrix. (As a corner case, we also have that the zero matrix is the

one and only matrix with a non-negative rank of r = 0.) It is often assumed that the non-negative

rank r is a known quantity, but in this paper, we make no such assumptions. For any value of

r ∈ {1, . . . , min{d, n}}, we let CNR(r) denote the set of all matrices with a non-negative factorization

of rank at most r—that is

CNR(r) : =

M

∈ [0, 1]n×d

|M

= UV T,

U

∈ R+n×r,

V

∈

Rd×r
+

.

For any matrix M , the smallest value of r such that M ∈ CNR(r) is termed its non-negative rank, and is denoted by r(M ).
In order to gain some intuition for the meaning of the non-negative rank, note that any matrix M ∈ CNR(r) can be written as a sum of the form
r
M = uℓ(vℓ)T .
ℓ=1
Here uℓ ∈ Rn+ and vℓ ∈ Rd+ are vectors such that uℓ(vℓ)T ∈ [0, 1]n×d for every ℓ ∈ [r]. Such a decomposition can be interpreted as the existence of r features, indexed by ℓ ∈ [r]. The d entries of vector vℓ represent the contribution of feature ℓ to the d respective items, and the n entries of vector uℓ represent the amounts by which the n respective users are inﬂuenced by feature ℓ. The popular overview article by Koren et al. [KBV09] provides an explanation for this assumption:
1Our results readily extend to any rating scheme with bounded values, such as ﬁve-star ratings. We focus on the binary case for purposes of brevity.

2

“Latent factor models are an alternative approach that tries to explain the ratings by characterizing both items and users on, say, 20 to 100 factors inferred from the ratings patterns. ... For movies, the discovered factors might measure obvious dimensions such as comedy versus drama, amount of action, or orientation to children; less welldeﬁned dimensions such as depth of character development or quirkiness; or completely uninterpretable dimensions. For users, each factor measures how much the user likes movies that score high on the corresponding movie factor.”
Concerns with non-negative rank: Let us now delve deeper into this model2 continuing in the context of movie recommendations for the sake of concreteness. Suppose there are r features that govern the movie watching experience; examples of such features include the amount of comedy content or the depth of character development. For any user i ∈ [n] and any feature ℓ ∈ [r], we let uℓi ∈ R+ denote the “aﬃnity” of user i towards feature ℓ, and for any movie j ∈ [d], we let vjℓ ∈ R+ denote the amount of content associated to feature ℓ in movie j. The conventional low non-negative rank model then assumes that the aﬃnity of user i towards movie j conditioned on feature ℓ is given by uℓi vjℓ. Consequently, for given feature ℓ, the entire behavior of each user and movie is governed by a pair of parameters, namely uℓi and vjℓ for user i and item j respectively. Such an assumption has some unnatural implications. For instance, consider any two movies, say A and B, and any two users, say X and Y . Then conditioned on any feature ℓ, we have the implication
Preference of user X for movie A = Preference of user Y for movie A . Preference of user X for movie B Preference of user Y for movie B
In words, the low non-negative rank model inherently imposes a condition that is potentially unrealistic—namely, that for any given feature, the ratio of preferences for any pair of movies is identical for all users. Likewise, for any given feature, the ratio of preferences of any pair of users is identical for all movies. With the goal of circumventing this possibly troublesome condition, let us now describe a generalization that we call permutation rank; it is the main focus of our paper.
Permutation rank: As with the ordinary rank, the permutation rank of the all-zeros matrix is zero. Otherwise, for any non-zero matrix, the permutation rank ρ takes values in the set {1, . . . , min{n, d}}. We begin by describing the set CPR(1) of matrices with permutation rank one, before describing how to extend to arbitrary ρ > 1. The set of matrices with permutation rank ρ = 1 is given by
CPR(1) : ={M ∈ [0, 1]n×d | ∃ permutations π1 : [n] → [n] and π2 : [d] → [d] such that Mij ≥ Mi′j′ for every quadruple (i, j, i′, j′) such that π1(i) ≥ π1(i′) and π2(j) ≥ π2(j′) }.
In words, a non-zero matrix is said to have a permutation rank of 1 if there exists a permutation of its rows and columns such that the entries of the resulting matrix are non-decreasing down any column and to the right along any row. Observe that any matrix with the conventional (nonnegative) rank equal to 1 also belongs to the set CPR(1). However, a matrix in CPR(1) can have any non-negative rank, meaning the set of matrices with a permutation-rank of 1 also includes some matrices with a full non-negative rank.
2A slightly diﬀerent, alternative interpretation is discussed in Appendix B.
3

We now extend the deﬁnition of the permutation rank to any integer ρ ∈ {1, . . . , min{n, d}}. In particular, the set of matrices with permutation rank ρ is given by

CPR(ρ) : = M ∈ [0, 1]n×d M =

ρ ℓ=1

Qℓ

for

some

matrices

Q1, . . . , Qρ

∈

CPR(1)

,

of matrices having a permutation-rank at most ρ. Note that this deﬁnition reduces to our previous one in the special case ρ = 1. Otherwise, for ρ > 1, the permutations deﬁning membership of each constituent matrix Qℓ in CPR(ρ) are allowed to be diﬀerent. For any matrix M , the smallest value of ρ such that M ∈ CPR(ρ) is termed its permutation rank, and is denoted by ρ(M ).
Revisiting the example of movie recommendations, the interpretation of this more general permutation-rank model is that conditioned on any feature ℓ ∈ [r], the preference ordering across movies continues to be consistent for diﬀerent users, but the values of these preferences need not be identical scalings of each other. Observe that the conventional non-negative matrix-completion setting CNR(r) is a special case of the permutation-rank matrix-completion setting where each matrix Qℓ is restricted to be of rank one. Whenever r < min{d, n}, we have the strict inclusion CNR(r) ⊂ CPR(r).

Outline and main contributions: Having discussed the limitations of the non-negative rank model and introduced the permutation rank, we now outline the remainder of the paper. In Section 2, we present our main results on the problem of estimating the matrix M ∗ (in the Frobenius norm) from partial and noisy observations. Speciﬁcally, we present a certain regularized least squares estimator, and prove that it achieves (nearly) minimax-optimal rates for estimation over the permutation-rank model. We also show that surprisingly, even if one considers the more restrictive non-negative rank model, and even if the rank is known, no estimator can achieve lower estimation error up to logarithmic factors. We also analyze the computationally eﬃcient Singular Value Thresholding (SVT) algorithm, and show that it yields consistent estimates over the permutationrank model, in addition to yielding the optimal estimate under the non-negative rank model. In Section 3, we establish some interesting properties of the permutation-rank model, and also derive certain relationships of this model with the non-negative rank model. In Section 4 we present the proofs of our results. We conclude the paper with a discussion in Section 5.
The paper also contains two appendices. Appendix A is devoted to negative results, where we show that certain intuitive algorithms provably fail. Appendix B describes an alternative interpretation of the non-negative rank model.

2 Main results on estimating M∗
We begin by considering the problem of estimating a low permutation rank matrix M ∗ based on noisy and partial observations. We ﬁrst analyze a computationally expensive estimator, based on regularizing the least-squares cost with a multiple of the permutation rank, and show that it achieves minimax-optimal rates up to logarithmic factors. We then turn to a polynomial-time algorithm based on nuclear norm regularization, which is equivalent singular value thresholding in the current set-up.

4

2.1 Optimal oracle inequalities for estimation

Suppose that we collect an observation matrix Y of the form (1), where the unknown matrix M ∗ belongs CPR. In this section, we analyze a regularized form of least-squares estimation, as applied to the recentered matrix

Y ′ : = 1 Y − 1 − pobs 11T .

(2a)

pobs

2pobs

We perform this recentering in order to obtain an unbiased estimate (Y ′) of the true matrix M ∗ in
the presence of missing observations, which is used in the least-squares estimator described below. As a sanity check, observe that when pobs = 1, we have the direct relation Y ′ = Y .
Letting ρ(M ) denote the permutation-rank of any matrix M , we then consider the estimator

MLS ∈ arg min
M ∈[0,1]n×d

|||Y ′ − M |||2 + ρ(M ) max{n, d} log2.01 d .

F

pobs

(2b)

Observe that importantly, the estimator MLS does not need to know the value of the true permutationrank of the underlying matrix. Moreover, while the estimator (as stated) is based on a known value of pobs, this assumption is not critical, since pobs can be estimated accurately from the observed matrix Y .
We now turn to some theoretical guarantees on the performance of this estimator. Rather than assuming that, for a given rank ρ, the target matrix M ∗ has permutation rank exactly equal to ρ, we instead provide bounds that depend on distances to the set of all matrices with a given permutation rank. More precisely, for any given tolerance ǫ ≥ 0, deﬁne the set

BP(ρ, ǫ) : = M ∈ [0, 1]n×d | ∃M ′ ∈ [0, 1]n×d s.t. ρ(M ′) ≤ ρ and |||M − M ′|||F ≤ ǫ ,

corresponding to the set of all matrices that are at most ǫ distant from the set of matrices with permutation rank ρ. Similarly, we deﬁne the set

BN(r, ǫ) : = M ∈ [0, 1]n×d | ∃M ′ ∈ [0, 1]n×d s.t. r(M ′) ≤ r and |||M − M ′|||F ≤ ǫ ,

corresponding to matrices that are at most ǫ away from some matrix with non-negative-rank r.
In stating the following theorem, as well as throughout the remainder of the paper, we use c, c′, c1 etc. to denote positive universal constants. The values of these constants may diﬀer from line to line.

Theorem 1. (a) For any matrix M ∗ ∈ [0, 1]n×d and any integer ρ ∈ [min{n, d}], the regularized least squares estimator MLS satisﬁes the upper bound

1 |||M − M ∗|||2 ≤ c min 1, min |||M − M ∗|||2F + ρ log2.01(nd) ,

(3a)

nd LS

F

1

M ∈CPR(ρ)

nd

min{n, d}pobs

with probability at least 1 − e−c0 max{n,d} log(nd).
(b) Conversely, for any integer ρ ∈ [min{n, d}], any scalar ǫ ≥ 0, and any estimator M , there exists a matrix M ∗ ∈ BN(ρ, ǫ) such that

E 1 |||M − M ∗|||2 ≥ c min 1, ǫ2 +

ρ

.

(3b)

nd

F

2

nd min{n, d}pobs

See Section 4.1 for the proof of this claim.

5

Interpretation as oracle inequality: The upper bound (3a) is an instance of an oracle inequality: it provides a family of upper bounds, one for each choice of the integer ρ ∈ [min{n, d}], on the estimation error associated with an arbitrary matrix M ∗ ∈ [0, 1]n×d. For each choice of ρ, the upper bound (3a) consists of two terms. The ﬁrst term, involving the minimum over M ∈ CPR(ρ), is a form of approximation error: it measures how well the unknown matrix M ∗ can be approximated with a matrix M of permutation rank at most ρ. The second term is a form of estimation error, measuring the diﬃculty of estimating a matrix that has permutation rank at most ρ. Since one such an upper bound holds for each choice of ρ, the bound (3a) shows that the estimator mimicks the behavior of an “oracle”, which is allowed to choose ρ so as to optimize the trade-oﬀ between the approximation and estimation error.

Sandwiching of the risk: The upper bound (3a) of Theorem 1 can equivalently be stated in the following manner. For any integer ρ ∈ [max{n, d}] and any scalar ǫ ≥ 0 such that M ∗ ∈ BP(ρ, ǫ),
the regularized least squares estimator MLS satisﬁes the upper bound

1 |||M − M ∗|||2 ≤ c min ǫ2 + ρ log2.01(nd) , 1 ,

(4a)

nd LS

F

1

nd min{n, d}pobs

with probability at least 1 − e−c0 max{n,d} log(max{nd}). On the other hand, since BN(ρ, ǫ) ⊆ BP(ρ, ǫ) for every value of ρ and ǫ, the lower bound (3b) of Theorem 1 implies the following result. For any integer ρ ∈ [min{n, d}], any scalar ǫ ≥ 0, and any estimator M , there exists a matrix M ∗ ∈ BN(ρ, ǫ) such that

E 1 |||M − M ∗|||2 ≥ c min 1, ǫ2 +

ρ

.

(4b)

nd

F

2

nd min{n, d}pobs

Comparing the bounds (4a) and (4b), we see that our results are sharp up to logarithmic factors.

Specialization to minimax risk: When suitably specialized to matrices that have some ﬁxed permutation (or non-negative) rank, Theorem 1 leads to sharp upper and lower bounds on the minimax risks for the problems of matrix completion over the sets CNR and CPR. In order for a clear comparison between the two bounds, let us index both the non-negative rank and the permutation-rank using a generic notation k—the meaning of the notation will be clear from the context.
Part (a) of Theorem 1 implies that for any value k ∈ [min{n, d}] and any matrix M ∗ ∈ CPR(k),
the regularized least squares estimator MLS satisﬁes the bound

d1n |||MLS − M ∗|||2F ≤ c1 min

k log2.01(nd) , 1 , min{n, d}pobs

with probability at least 1 − e−c0 max{n,d} log(nd). Within the set of matrices [0, 1]n×d under consideration, we have the deterministic upper bound n1d |||MLS − M ∗|||2F ≤ 1. Consequently, our high probability upper bound also implies a uniform bound on the mean-squared error over the set CPR(k)—that is

sup 1 E[|||M − M ∗|||2 ] ≤ c′ min k log2.01(nd) , 1 .

(5a)

M ∗∈CPR(k) dn

LS

F

1

min{n, d}pobs

6

Since CPR(k) is a superset of CNR(k), the same upper bound holds for the minimax risk over CNR(k):

sup 1 E[|||M − M ∗|||2] ≤ c′ min

M ∗∈CNR(k) dn

LS

F

1

ρ log2.01(nd) , 1 . min{n, d}pobs

(5b)

Conversely, part (b) of Theorem 1 implies that for any k ∈ [max{n, d}], the error incurred by any estimator M over the set CNR(k) is error lower bounded as

sup 1 E[|||M − M ∗|||2] ≥ c min

k ,1 .

(5c)

M ∗∈CNR(k) dn

F

2

min{n, d}pobs

Since CNR(k) ⊆ CPR(k), the error incurred by any estimator M over the set CPR(k) is also lower bounded as

sup 1 E[|||M − M ∗|||2] ≥ c min

M ∗∈CPR(k) dn

F

2

k ,1 . min{n, d}pobs

(5d)

We have thus characterized the minimax risk over both the families CPR or CNR, with bounds (5) that are matching up to logarithmic factors. An important consequence of our oracle and minimax results is the multi-fold beneﬁt of moving from the restrictive non-negative-rank assumptions to the more general permutation-rank assumptions. Fitting a permutation-rank k model when the true matrix actually has a non-negative rank of k leads to relatively little additional (overﬁtting) error. On the other hand, we show later in the paper that ﬁtting a non-negative rank k model when the true matrix actually has a permutation-rank of k can lead to very high error, due to model mismatch.

Link to past work: A special case of our present problem is equivalent to the setting considered
in our earlier work [SBGW17], corresponding to the case when the value of ρ is known and equal to 1, the matrix M ∗ is square with n = d, and all entries of M ∗ satisfy the shifted-skew-symmetry condition Mi∗j + Mj∗i = 1. The proof of the upper bound of Theorem 1(a) relies on a framework laid out in this earlier work [SBGW17], but augments the results obtained in this past work, both
in generalizing to broader families of matrices, but also providing oracle inequalities that allow for
matrices that need not be exactly low rank (in either the non-negative or permutation senses).
Theorem 1 provides guarantees on the estimation of “mixtures” of diﬀerent permutations, thereby
resolving an important open problem in the paper [SBGW17].

2.2 Computationally eﬃcient estimator
At this point, we do not know how to compute the regularized least squares estimator (2b) in an eﬃcient manner, and we suspect that it may be computationally intractable to do so. Consequently, in this section, we turn to analyzing a diﬀerent method based on singular value thresholding (SVT). Singular value thresholding has been used either directly or as a subroutine in several past papers on the conventional low-rank matrix completion problem (see, for example, the papers [CCS10, DG+14, Cha14]). This approach is appealing due to its computational simplicity, involving only computation of the singular value decomposition, followed by a single pointwise non-linearity; see Cai et al. [CO10] for a fast algorithm. In the context of the permutation rank completion problem, we show here that the SVT estimator is consistent for estimation under the

7

permutation-rank model, albeit with a rate that is suboptimal by a factor of min{n, d}pobs. Note that these guarantees hold without the estimator needing to know that the matrix is drawn from
a permutation-rank model, nor the value pobs of the permutation rank.

The SVT estimator is straightforward to describe.

From the observation matrix Y

∈

{0,

1 2

,

1}n×d,

we ﬁrst obtain the transformed observation matrix Y ′ as in equation (2a). Applying the singular

value decomposition yields the representation Y ′ = U DV T , where the (n × d) matrix D is diagonal,

whereas the (n × d) matrices U and V are orthonormal. For a threshold λ > 0 to be speciﬁed,

deﬁne another diagonal matrix Dλ with entries

[Dλ]jj = 0

if Djj < λ for each j ∈ [max{n, d}].

(6)

Djj − λ if Djj ≥ λ

Finally, the SVT estimator is given by MSVT = U DλV T .

The following theorem now establishes guarantees for the singular value thresholding estimator.

Theorem

2.

Suppose

that

pobs

≥

1 min{n,d}

log7(nd).

Then

for

any

matrix

M∗

∈

[0, 1]n×d,

the

SVT

estimator MSVT with threshold λ = 2.1 npo+bds satisﬁes the bound

1 |||M − M ∗|||2 ≤ c min

nd SVT

F

1 M ∈[0,1]n×d

min

ρ(M ) , r(M ) min{n, d}pobs min{n, d}pobs

+ 1 |||M ∗ − M |||2 , (7)

nd

F

with probability at least 1 − e−c0 max{n,d}.

Observe that the bound (7) on the risk of the SVT estimator has the term min{n, d} in the denominator of the ﬁrst expression in the √minimum, as opposed to the min{n, d} in the upper bound (3a) from Theorem 1. This form of “ n-suboptimality” arises in several permutation-based problems of this type studied in recent papers (e.g., [SBGW17, SBW16a, CM16, SBW16b, FMR16, PWC16]). In some cases, this gap—between the performance of any polynomial-time algorithm and the best algorithm—is known to be unavoidable [SBW16a] conditional on the planted clique conjecture. It is interesting to speculate whether such a computational complexity gap exists in the context of the permutation-rank model.
The proof techniques underlying Theorem 2 can also be used to establish previously known guarantees [KLT11, Cha14] for the non-negative rank model. In order to contrast with the permutationrank model, let us state one such guarantee here. It is known from previous results [KLT11, Cha14] for the non-negative rank model that for any matrix M ∗ ∈ [0, 1]n×d, the SVT estimator incurs an error upper bounded by

1 |||M − M ∗|||2 ≤ c′ r(M ∗) ,

(8)

nd SVT

F 1 min{n, d}pobs

with high probability. On the other hand, our permutation-based modeling approach yields a stronger guarantee for the classical SVT estimator—namely, setting M = M ∗ in our upper bound (8)

8

of Theorem 2 guarantees that the SVT estimator MSVT with threshold λ = 2.1 upper bound

npo+bds satisﬁes the

1 |||M − M ∗|||2 ≤ c min

r(M ∗) ,

ρ(M ∗)

,

(9)

nd SVT

F

1

min{n, d}pobs min{n, d}pobs

with high probability. The bound (9) can yield results that are much sharper as compared to what

may be obtained from the previously known guarantees (8) for the SVT estimator. For example, suppose n = d and pobs = 1. Consider the matrix M ∗ ∈ [0, 1]n×d given by

 1 Mi∗j = 01
2

if i > j if i < j if i = j.

Then we have r(M ∗) = n and ρ(M ∗) = 1. Consequently, the bound (8) from past literature yields an upper bound of

n1d |||MSVT − M ∗|||2F ≤ c′1,

whereas in contrast, our analysis (9) yields the sharper bound

n1d |||MSVT − M ∗|||2F ≤ c1 lo√g2n.01 n , (10)

with high probability. Moreover, in our earlier work [SBGW17], we have shown that for this choice of M ∗, the bound (10) is the best possible up to logarithmic factors for the SVT estimator with
any ﬁxed threshold λ.

3 Properties of permutation-rank models
In the previous section, we established some motivating properties of permutation-based models from the perspective of statistical estimation, in this section, we derive some more insights on the permutation-rank model.
3.1 Comparing permutation-rank and non-negative-rank
We begin by comparing the permutation-rank model with the conventional non-negative rank model. To this end, ﬁrst observe that the deﬁnitions of the two models immediately imply that the permutation-rank of any matrix is always upper bounded by its non-negative rank, that is, for any matrix M , we have ρ(M ) ≤ r(M ). A natural question that now arises is whether there is any additional general condition beyond this simple relation that constrains the two notions of the matrix rank. The following proposition shows that there is no other guaranteed relation between the two notions of matrix rank.
Proposition 1. For any values 0 < ρ ≤ r ≤ min{n, d}, there exist matrices whose permutationrank is ρ and non-negative rank is r.

9

A particular instance that underlies part of the proof of Proposition 1, associated to any pair

of values (ρ, r), is the following block matrix Mρ,r of size (n × d):





Jr−ρ+1 0 0

Mρ,r : =  0 Iρ−1 0 ,

0

00

where for any value k, Jk denotes an upper triangular matrix of size (k × k) with all entries on and above the diagonal set as 1, and let Ik denote the identity matrix of size (k × k). By construction, the matrix Mρ,r has a non-negative rank r(Mρ,r) = r and a permutation rank ρ(Mρ,r) = ρ.
We now investigate a second relation between the two models. Recall from our discussion earlier
that the assumptions of the permutation-rank model are much less restrictive than the assumptions
of the non-negative rank model. With this context, a natural question that arises is to quantify the
bias of an estimator that ﬁts a matrix of non-negative rank of k when the true underlying matrix
instead has a permutation rank of k. We answer this question using the notion of the Hausdorﬀ distance: For any two sets S1, S2 ∈ Rn×d, the Hausdorﬀ distance H(S1, S2) between the two sets in the squared Frobenius norm is deﬁned as

H(S1, S2) : = max sup inf |||M − M ′|||2F , sup inf |||M − M ′|||2F

(11)

M ∈S1 M ′∈S2

M ′∈S2 M ∈S1

The following proposition quantiﬁes the Hausdorﬀ distance between non-negative-rank and permutation-rank models.

Proposition

2.

For

any

positive

integer

k

≤

1 2

min{d, n},

the

Hausdorﬀ

distance

between

the

sets

CNR(k) and CPR(k) is lower bounded as

nd

H(CNR(k), CPR(k)) ≥ c3 k .

(12)

Proposition 2 helps quantify the bias on ﬁtting a non-negative rank model when the true matrix

follows

the

permutation-rank

model

as

follows.

Consider

any

positive

integer

k

≤

1 2

min{d, n},

and

any estimator Mk that outputs a matrix in CNR(k). Then since CNR(k) ⊆ ρ(k), the error incurred

by this estimator when the true matrix lies in the set CPR(k) is lower bounded as

sup 1 |||M − Mk|||2 ≥ 1 H(C (k), C (k)) ≥ c 1 ,

(13)

M ∗∈CPR(k) nd

F nd

NR

PR

3k

with probability 1.
Observe that when k is a constant (but n and d are allowed to grow), the right hand side of the
bound (12) becomes a constant, and this is the largest possible order-wise gap between any pair of matrices in [0, 1]n×d. Likewise, the right hand side of (13) becomes a constant, and this is the largest possible order-wise error for any estimator that outputs matrices in [0, 1]n×d.

3.2 No “good” convex approximation
In this section, we investigate a question about an important property of the permutation-based set, and in particular, its primitive CPR(1). There are various estimators including our regularized least squares estimator (2b) as well as those studied in the literature [SBGW17, SBW16a, SBW16b]

10

which require solving a an optimization problem over the set CPR(1). With this goal in mind, a natural question that arises is: Is the set CPR(1) is convex? If not, then does it at least have a “good” convex approximation? The following proposition answers these questions in the negative using the notion of the Hausdorﬀ distance between sets (11).

Proposition 3. The Hausdorﬀ distance (11) between the set of matrices with permutation-rank one and any arbitrary convex set C ⊆ Rn×n is lower bounded as

where c > 0 is a universal constant.

1 H(CPR(1), C) ≥ c, nd

A speciﬁc example of a convex set C is the convex hull of CPR(1). Then by deﬁnition we have the relation sup inf |||M1 − M2|||2F = 0. Consequently, Proposition 3 implies that
M1∈CPR(1) M2∈C

sup inf |||M1 − M2|||2F = Θ(nd),
M2∈C M1∈CPR(1)

thus showing that the convex hull of CPR(1) is a much larger set than CPR(1) itself. The proof of Proposition 3 relies on a more general result that we derive, one which relates
a certain notion of inherent (lack of) convexity of a set to the Hausdorﬀ distance between that set and any convex approximation. Note that this result does not preclude the possibility that an optimization procedure over a convex approximation to CPR(1) converges close enough to some element of CPR(1) itself. We leave the investigation of this possibility to future work.

3.3 On the uniqueness of decomposition

In this section, we investigate conditions for the uniqueness of the decomposition of any matrix
into its constituent components that have a permutation-rank of one. In the conventional setting of low non-negative rank matrix completion, several past works [DS03, TST05, LCP+08, Gil12, AGKM12] investigate the conditions required for uniqueness of the decomposition of matrices into
their constituent non-negative rank-one matrices. Here we consider an analogous question in the setting of permutation rank. More precisely, consider any matrix M ∈ [0, 1]n×d with a permutation-
rank decomposition of the form

ρ(M )

M=

M (ℓ),

ℓ=1

(14)

where M (ℓ) ∈ CPR(1) for every ℓ ∈ [ρ(M )]. Under what conditions on the matrix M is the set {M (1), . . . , M (ρ(M))} of constituent matrices unique? The following result provides a necessary
condition for uniqueness. In order to state the result, we use the notation 1 to denote the indicator
function, that is, 1{x} = 1 if x is true and 1{x} = 0 if x is false.

Proposition 4. A necessary condition for the uniqueness of a permutation-rank decomposition (14)
for any matrix M is that for every coordinate (i, j) ∈ [n] × [d], there is at most one ℓ ∈ [ρ(()M )] such that Mi(jℓ) is non-zero and distinct from all other entries of M (ℓ), that is,

1 Miℓj ∈/ {0} ∪ {Miℓ′j′ }i′∈[n],j′∈[d],

ℓ∈[ρ(M )]

(i′ ,j ′ )=(i,j )

≤1

for every (i, j) ∈ [n] × [d].

11

We note that the necessary condition continues to hold even if we restrict attention to only symmetric matrices. The necessary condition provided by Proposition 4 indicates that any suﬃcient condition for uniqueness of the decomposition must be quite strong. Moreover, we believe that the conditions for suﬃciency may be signiﬁcantly stronger than those necessitated by Proposition 4. The reason for such drastic requirements for uniqueness is the high-degree of ﬂexibility oﬀered by the permutation-rank model.
Let us illustrate the necessary condition from Proposition 4 with a simple example. Consider the following matrix M with n = d = 2 and ρ(M ) = 2 and decomposition into M (1), M (2) ∈ CPR(1):

M : = 1 .6 = 0 .3 + 1 .3

.6 1

.3 .9 .3 .1

Observe that the necessary condition obtained in Proposition 4 is required to hold for every coor-
dinate of the matrix. Let us ﬁrst evaluate this condition for the coordinate (1, 1) of the matrices. Since M1(11) = 0, there is at most one ℓ ∈ {1, 2} such that M1(1ℓ) is non-zero, and hence the coordinate (1, 1) satisﬁes the necessary condition. Moving on to coordinate (1, 2), we have M1(21) = M2(11) and M1(22) = M2(12); hence the coordinate (1, 2) also passes the necessary condition. The argument for coordinate (1, 2) also applies to coordinate (2, 1) since the matrices involved are symmetric. We ﬁnally test coordinate (2, 2). Observe that M2(21) ∈/ {0, M1(11), M1(21), M2(11)} and M2(22) ∈/ {0, M1(12), M1(22), M2(12)}. As a consequence, for both ℓ = 1 and ℓ = 2, we have that M2(2ℓ) is non-zero and distinct from all other entries of M (ℓ). The condition necessary for uniqueness is thus violated. Indeed, as guaranteed by
Proposition 4, there exist other decompositions of M with permutation-rank 2—for instance, the
decomposition

M = 1 .6 = 0 .4 + 1 .2

.6 1

.4 .9 .2 .1

is another example. Finally, we put the negative result on the decomposition into some practical perspective with an
analogy to tensor decompositions. The canonical polyadic (CP) decomposition of a tensor [Hit27] is not unique unless strong non-degeneracy conditions are imposed [Kru77]. From a theoretical perspective in many applications (for instance, in estimating latent variable models [HK13]) the CP decomposition is most useful or interpretable when it is unique. Furthermore, even when the decomposition is unique, computing it is NP-hard in the worst-case [H˚as90]. However, in practice the CP decomposition is often computed via ad-hoc methods that generate useful results [KB09].

4 Proofs
We now turn to the proofs of our main results. In all our proofs, we assume that the values of n and d are larger than certain universal constants, so as to avoid subcases having to do with small values of (n, d). We will also ignore ﬂoors and ceilings wherever they are not critical. These assumptions entail no ultimate loss of generality, since our results continue to hold for all values with diﬀerent constant prefactors. Throughout these and other proofs, we use the notation {c, c′, c0, c1, C, C′} and so on to denote positive constants whose values may change from line to line.

12

4.1 Proof of Theorem 1(a)

The proof of this theorem involves generalizing an argument used in our past work [SBGW17, Theorem 1]. In particular, the problem setting of our past work [SBGW17, Theorem 1] is a special case of the present problem, restricted to the case of square matrices (n = d) and permutation rank ρ = 1. Here we develop a number of additional techniques in order to handle the generalization to non-square matrices and arbitrary permutation rank.
We may assume without loss of generality that n ≤ d; otherwise, we can apply the same argument to the matrix transposes. It is straightforward to verify that the observation matrix Y ′ can equivalently be written in the linearized form

Y ′ = M∗ + 1 W ′, pobs

(15a)

where W ′ has entries that are independent, and are distributed as



pobs( 12

−

[M ∗]ij)

+

1 2

[W ′]ij

=

pobs( 12

−

[M ∗]ij)

−

1 2

pobs( 12 − [M ∗]ij)

with probability pobs[M ∗]ij with probability pobs(1 − [M ∗]ij) with probability 1 − pobs.

(15b)

We begin by introducing some additional notation in order to accommodate the arbitrary permutation-rank of M ∗ and the fact that each constituent component in CPR(1) can have any arbitrary permutation. For any pair of permutations π : [n] → [n] and σ : [d] → [d], we ﬁrst deﬁne
the set

CPR(1; π, σ) : = {M ∈ CPR(1) | rows and columns of M are ordered according to π and σ respectively}.

Now let Π denote the set of all possible permutations of d items, and let Σ denote the set of all possible permutations of the n users. Consider any value k ∈ [n], any sequence Π(k) : = (π1, . . . , πk) ∈ Πk and any sequence Σ(k) : = (σ1, . . . , σk) ∈ Σk. Deﬁne the set

CPR(k; Π(k), Σ(k)) : =

k
M = M (ℓ) M (ℓ) ∈ CPR(1; πℓ, σℓ) for every ℓ ∈ [k] ,
ℓ=1

and an associated estimator

MΠ(k),Σ(k) ∈

arg min

|||Y ′ − M |||2F.

M ∈CPR(k;Π(k),Σ(k))

Also deﬁne a matrix M0 as

M0 ∈ arg min |||M − M ∗|||2 + ρ(M )d log2.01 d ,

M ∈[0,1]n×d

F

pobs

as well as an associated set Γ as Γ : = (k, Π(k), Σ(k)) ∈ [n]×Πk ×Σk |||Y ′ −MΠ(k),Σ(k) |||2F + kd lpoogb2s.01 d ≤ |||Y ′ −M0|||2F + ρ(M0)pdolbosg2.01 d .

13

Note that the set Γ is guaranteed to be non-empty since the parameter and permutations corresponding to M0 always lie in Γ. We claim that for any (k, Π(k), Σ(k)) ∈ Γ, we have

P |||MΠ(k),Σ(k) − M0|||2F ≤ c1 ρ(M0)pdolbosg2.01 d ≥ 1 − e−4kd log d,

(16)

for some positive universal constant c1. Taking this result as given for the moment, under our assumption of d ≥ n, for any value of k the cardinality of the set Γ restricted to any k is at most e2kd log d. Hence a union bound over all k ∈ [n] and all permutations—applied to equation (16)—
yields

P (k,Π(km),aΣx(k))∈Γ |||MΠ(k),Σ(k) − M0|||2F ≤ c1 ρ(M0)pdolbosg2.01 d ≥ 1 − e−d log d.

(17)

From the deﬁnition of the regularized least squares estimator MLS in equation (2b) and the deﬁnition of set Γ above, we have that MLS must equal MΠ(k),Σ(k) for some (k, Π(k), Σ(k)) ∈ Γ. As a consequence, the tail bound (17) ensures that
P |||MLS − M0|||2F ≤ c1 ρ(M0)pdolbosg2.01 d ≥ 1 − e−d log d.
Finally, applying the triangle inequality yields the claimed result
P |||MLS − M ∗|||2F ≤ 2|||M ∗ − M0|||2F + 2c1 ρ(M0)pdolbosg2.01 d ≥ 1 − e−d log d.

Proof of the bound (16): The remainder of our proof is devoted to proving the claim (16). By deﬁnition, any (k, Π(k), Σ(k)) ∈ Γ must satisfy the inequality

|||Y − MΠ(k),Σ(k)|||2F + kd lpoogb2s.01 d ≤ |||Y − M0|||2F + ρ(M0)pdolbosg2.01 d .

Denoting the error in the estimate as ∆Π(k),Σ(k) : = MΠ(k),Σ(k) − M0, and using the linearized form (15a), some algebraic manipulations yield the basic inequality

1 |||∆

|||2 ≤ 1

2 Π(k),Σ(k) F pobs

W ′, ∆Π(k),Σ(k)

+ 1 (ρ(M0) − k)d log2.01 d .

2

pobs

(18)

Now consider the set of matrices

CDIFF(Π(k), Σ(k); M0) : = α(M − M0) | M ∈ CPR(k; Π(k), Σ(k)), α ∈ [0, 1] ,

(19)

and note that CDIFF(Π(k), Σ(k); M0) ⊆ [−1, 1]n×d. For each choice of radius t > 0, deﬁne the random variable

ZΠ(k),Σ(k) (t) : =

sup

MDIFF ∈CDIFF (Π(k) ,Σ(k) ;M0 ),

|||MDIFF|||F≤t

1 W ′, M .

pobs

DIFF

(20)

14

Using the basic inequality (18), the Frobenius norm error |||∆Π(k),Σ(k)|||F then satisﬁes the bound

1 |||∆

|||2 ≤ Z |||∆

||| + 1 (ρ(M0) − k)d log2.01 d .

2 Π(k),Σ(k) F

Π,Σ

Π(k),Σ(k) F

2

pobs

(21)

Thus, in order to obtain our desired bound, we need to understand the behavior of the random
quantity ZΠ(k),Σ(k)(t). Dy deﬁnition, the set CDIFF(Π(k), Σ(k); M0) is “star-shaped”, meaning that αMDIFF ∈ CDIFF(Π(k), Σ(k))
for every α ∈ [0, 1] and every MDIFF ∈ CDIFF(Π(k), Σ(k); M0). Using this star-shaped property, we are guaranteed that E[ZΠ(k),Σ(k)(δ)] grows at most linearly with δ. We are then in turn guaranteed the existence of some scalar δc > 0 satisfying the critical inequality

E[ZΠ(k),Σ(k)(δc)] ≤ δ2c2 .

(22)

Our interest is in an upper bound to the smallest (strictly) positive solution δc to the cri√tical inequality (22), and moreover, our goal is to show that for every t ≥ δc, we have |||∆|||F ≤ c tδc with high probability. To this end, deﬁne a “bad” event At as

At = ∃∆ ∈ CDIFF(Π(k), Σ(k); M0) | |||∆|||F ≥ tδc and po1bs W ′, ∆ ≥ 2|||∆|||F tδc . (23)

Using the star-shaped property of CDIFF(Π(k), Σ(k); M0), it follows by a rescaling argument that

P[At] ≤ P[ZΠ(k),Σ(k)(δc) ≥ 2δc tδc] for all t ≥ δc.

The following lemma helps control the behavior of the random variable ZΠ(k),Σ(k)(δc).

Lemma 1. For any δ > 0, the mean of ZΠ(k),Σ(k)(δ) is bounded as

E[ZΠ(k),Σ(k)(δ)] ≤ c1 max{kp,oρb(sM0)}d log2 d,

and for every u > 0, its tail probability is bounded as

P ZΠ(k),Σ(k)(δ) > E[ZΠ(k),Σ(k) (δ)] + u

≤ exp

−c2u2pobs

,

δ2 + E[ZΠ(k),Σ(k) (δ)] + u

where c1 and c2 are positive universal constants.

From this lemma, we have the tail bound

P ZΠ(k),Σ(k) (δc) > E[ZΠ(k),Σ(k)(δc)] + δc

−c2(δc√tδc)2pobs tδc ≤ exp δc2 + E[ZΠ(k),Σ(k)(δc)] + (δc√tδc) ,

for all t > 0.

By the deﬁnition of δc in (22), we have E[ZΠ(k),Σ(k)(δc)] ≤ δc2 ≤ δc√tδc for all t ≥ δc, and consequently

−c2(δc√tδc)2pobs

P[At] ≤ P[ZΠ(k),Σ(k) (δc) ≥ 2δc tδc ≤ exp

√ 3δc tδc

, for all t ≥ δc. (24)

15

√

√

Now we must have either |||∆Π(k),Σ(k)|||F ≤ tδc, or we have |||∆Π(k),Σ(k)|||F > tδc. In the latter case,

conditioning on the complement Act, our basic inequality (18) implies that

21 |||∆Π(k),Σ(k) |||2F ≤ 2|||∆Π(k),Σ(k) |||F

1 (ρ(M0) − k)d log2.01 d

tδc + 2

pobs

,

and hence

|||∆Π(k),Σ(k) |||F ≤ 4 tδc +

(ρ(M0) − k)d log2.01 d . pobs

(25)

Putting together the bounds (24) and (25) then yields

P |||∆Π(k),Σ(k) |||2F ≤ 32tδc + 2 (ρ(M0) −pkob)sd log2.01 d

≥ 1 − exp

− c2 δc 3

tδcpobs ,

for all t ≥ δc. (26)

Finally, from the bound on the expected value of ZΠ(k),Σ(k)(t) in Lemma 1, we see that the critical inequality (22) is satisﬁed for

δc = c1 max{ρ(M0), k}d log d. pobs
Setting t = c′δc in (26) for a large enough constant c′ yields P |||∆Π(k),Σ(k)|||F ≤ c′1ρp(oMbs0)d log2.01 d ≥ 1 − exp − 4 max{ρ(M0), k}d log d , (27)
for some constant c′1 > 0, thus proving the bound (16). It remains to prove Lemma 1.

Proof of Lemma 1 We break our proof into two parts, corresponding to bounds on the mean of the random variable ZΠ(k),Σ(k)(δ) followed by control of its tail behavior.

Bounding the mean: We begin by establishing an upper bound on the mean E[ZΠ(k),Σ(k)(δ)]. In order to obtain the desired upper bound, in the proof we carefully account for the rank k and the diﬀerent permutations associated to any matrix in CDIFF(Π(k), Σ(k); M0).
For convenience of analysis, we introduce a new random variable

ZΠ(k),Σ(k) : =

sup

W ′, MDIFF .

MDIFF ∈CDIFF (Π(k) ,Σ(k) ;M0 )

Then by deﬁnition, we have E[ZΠ(k),Σ(k)(δ)] ≤ po1bs E[ZΠ(k),Σ(k)] for every δ > 0. In addition, since

M0 ∈ CPR(k), it can be decomposed as M0 =

k ℓ=1

M0(ℓ)

,

for

some

matrices

M0(1), . . . , M0(k)

∈

CPR(1).

We introduce some additional notation for ease of exposition. If ρ(M0) < k, then let M0(ρ(M0)+1), . . . , M0k

denote all-zero matrices. Hence we can write M0 =

max{ρ(M0 ),k} ℓ=1

M0(ℓ).

If ρ(M0) > k then let

16

πk+1, . . . , πρ(M0) be arbitrary (but ﬁxed) permutations of n items and σk+1, . . . , σρ(M0) be arbitrary (but ﬁxed) permutations of d items. With this notation in place, we have the following deterministic
upper bound on the value of the random variable ZΠ(k),Σ(k):

max{ρ(M0),k}

ZΠ(k),Σ(k) ≤

sup

W ′, [MDIFF]ℓ .

ℓ=1

[MDIFF ]ℓ ∈CDIFF ({πℓ },{σℓ };M0(ℓ) )

We also recall our assumption that d ≥ n without loss of generality. Now let log N (ǫ, C, ||| · |||F) denote the ǫ metric entropy of class C ⊂ Rn×d in the Frobenius norm metric ||| · |||F. Then the truncated form of Dudley’s entropy integral inequality yields3

max{ρ(M0 ),k}

E[ZΠ(k),Σ(k) ] ≤

c

ℓ=1

d−8 +

2d 12 d−9

log N (ǫ, CDIFF({πℓ}, {σℓ}; M0(ℓ)), |||.|||F)(∆ǫ) ,

(28)

where we have used the fact that the diameter of the set CDIFF({πℓ}, {σℓ}; M0(ℓ)) is at most 2d in the Frobenius norm.
In our past work [SBGW17, Lemma 2], we derived a bound on the metric entropy of the set CDIFF({πℓ}, {σℓ}; M0(ℓ)) as:

(ℓ)

d2

d2

log N ǫ, CDIFF({πℓ}, {σℓ}; M0 ), ||| · |||F ≤ 16 ǫ2 log ǫ ,

for any ǫ > 0 and ℓ ∈ [k]. Substituting this bound on the metric entropy into the Dudley bound (28) yields

E[ZΠ(k),Σ(k) ] ≤ c′ max{ρ(M0), k}d log2 d.

The inequality E[ZΠ(k),Σ(k) (δ)] ≤ po1bs E[ZΠ(k),Σ(k) ] then yields the claimed result.

Bounding the tail: In order to establish the claimed tail bound on the deviations of ZΠ(k),Σ(k)(δ) above its mean, we use a Bernstein-type bound on the supremum of empirical processes due to Klein and Rio [KR+05, Theorem 1.1c], which we state in a simpliﬁed form here.

Lemma 2. Let X : = (X1, . . . , Xm) be any sequence of zero-mean, independent random variables, each taking values in [−1, 1]. Let V ⊂ [−1, 1]m be any measurable set of m-length vectors. Then for any u > 0, the supremum X† = supv∈V X, v satisﬁes the upper tail bound

P X† > E[X†] + u ≤ exp

−u2

.

2 supv∈V E[ v, X 2] + 4E[X†] + 3u

We now call upon Lemma 2 setting V = {MDIFF ∈ CDIFF(Π(k), Σ(k); M0) | |||MDIFF|||F ≤ δ}, X = W ′, and X† = pobsZΠ(k),Σ(k)(δ). The entries of the matrix W ′ are mutually independent, have a mean of zero, and are bounded by 1 in absolute value. Then we have E[X†] = pobsE[ZΠ(k),Σ(k)(δ)]
3Here we use (∆ǫ) to denote the diﬀerential of ǫ, so as to avoid confusion with the number of columns d.

17

and E[ MDIFF, W ′ 2] ≤ 4pobs|||MDIFF|||2F ≤ 4pobsδ2 for every MDIFF ∈ V. With these assignments, and some algebraic manipulations, we obtain that for every u > 0,

P ZΠ(k),Σ(k)(δ) > E[ZΠ(k),Σ(k) (δ)] + u

≤ exp

−u2pobs

,

8δ2 + 4E[ZΠ(k),Σ(k) (δ)] + 3u

as claimed.

4.2 Proof of Theorem 1(b)
Assume without loss of generality that d ≥ n. Throughout the proof, we ignore ﬂoor and ceiling conditions as these are not critical to the proof and aﬀect the lower bound by only a constant factor.
The Gilbert-Varshamov bound [Gil52, Var57] from coding theory guarantees the existence of

η : = exp c(dr + pobsǫ2)

binary vectors g1, . . . , gη, each of length (dr + pobsǫ2), such that the Hamming distance between any pair of vectors in this set is lower bounded as

DH (gℓ, gℓ′ ) ≥ dr + pobsǫ2 . 10
For some δ ∈ (0, 14 ) whose value is speciﬁed later, deﬁne a related set of vectors g1, . . . , gη as

gjℓ =

1 2

+

δ

1 2

−

δ

if gjℓ = 1 if gjℓ = 0,

for every ℓ ∈ [η] and j ∈ [dr+pobsǫ2]. Next deﬁne a set of “low rank” matrices G1, . . . , Gη ∈ [0, 1]n×d where the matrix Gℓ is obtained as follows. For each ℓ ∈ [η], arrange the ﬁrst rd entries of vector gℓ as the entries of an (r × d) matrix—this arrangement may be done in an arbitrary manner as long as it is consistent across every ℓ ∈ [η]. Now append a ( pobdsǫ2 × d) matrix at the bottom, whose entries comprise the last pobsǫ2 entries of the vector gℓ—again, this arrangement may be done in an arbitrary manner as long as it is consistent across every ℓ ∈ [η]. Now stack po1bs copies of the resulting (r + pobdsǫ2 ) × d matrix on top of each other to form a ( porbs + ǫd2 ) × d matrix. Note that our assumption ǫ2 + r mapxob{sn,d} ≤ nd, along with the assumption d ≥ n, implies that n ≥ porbs + ǫd2 . Append (n − ( porbs + ǫd2 )) rows of all zeros at the bottom of this matrix, and denote the resultant (n × d) matrix as Gℓ.
We now show that Gℓ ∈ BN(r, ǫ) for every ℓ ∈ [η], that is, we show that the matrix Gℓ ∈ [0, 1]n×d
can be decomposed into a sum of a low-rank matrix (of non-negative rank at most r) and a sparse matrix (number of non-zero entries at most ǫ2). First we set to zero the entries in Gℓ which correspond to the last pobsǫ2 entries of the vector gℓ. Let us denote the resulting matrix as Gℓ. Each row of the matrix Gℓ is either all zero or is identical to one among the ﬁrst r rows of Gℓ. Consequently we have r(Gℓ) ≤ r. Also observe that in the matrix (Gℓ −Gℓ), the number of non-zero entries is at most po1bs × pobsǫ2 = ǫ2, and furthermore, each of these entries lie in the interval [0, 1].

18

Hence we have |||Gℓ − Gℓ|||2F ≤ ǫ2. The matrix Gℓ thus satisﬁes all the requirements for membership in the set BN(r, ǫ).
For every ℓ ∈ [η], let Pℓ denote the probability distribution of the matrix Y obtained by setting M ∗ = Gℓ. One can verify that the set of matrices G1, . . . , Gη constructed above has the following two properties, for every pair ℓ = ℓ′ ∈ [η]:

DKL(Pℓ Pℓ′ ) ≤ c′δ2pobs dr + ǫ2 , pobs

and
|||Gℓ − Gℓ′|||2 ≥ δ2 dr + ǫ2 . F 10 pobs
Substituting these relations in Fano’s inequality [CT12] yields that when M ∗ is drawn uniformly at random from the set {G1, . . . , Gη}, any estimate M for M ∗ has squared Frobenius error at least

E[|||M − M ∗|||2] ≥ δ2 dr + ǫ2 F 20 pobs

c′ δ2 pobs

dr p

+ ǫ2

+ log 2

1−

obs
c(dr + pobsǫ2)

(i)
≥ c′′

dr + ǫ2 ,

pobs

where inequality (i) is obtained by choosing δ2 as a small enough constant (that depends only on c and c′). Recalling our assumption d ≥ n, and consequently replacing d by max{n, d} in the bound
yields the claimed result.

4.3 Proof of Theorem 2

We now turn to analysis of the singular-value thresholding (SVT) estimator. This proof is based

on the framework of a proof from our earlier work [SBGW17, Theorem 2], which can be seen as a

particular case with n = d and ρ = 1. We introduce certain additional tricks in order to generalize

the proof for general values of ρ and to obtain a sharp dependence on ρ. As in our previous proofs,

we may assume without loss of generality that n ≤ d.

Recall from equation (15a) that we can write our observation model as Y ′ = M ∗ + po1bs W ′, where W ′ ∈ [−1, 1]n×d is a zero-mean matrix with mutually independent entries. Also recall that

these entries follow the distribution



pobs( 21

−

[M ∗]ij)

+

1 2

with probability pobs[M ∗]ij

[W ′]ij

=

pobs( 12

−

[M ∗]ij)

−

1 2

with probability pobs(1 − [M ∗]ij)

(29)

pobs( 12 − [M ∗]ij)

with probability 1 − pobs.

For any matrix A, let σ1(A), σ2(A), . . . denote its singular values in descending order. Our proof of the upper bound is based on four lemmas. The ﬁrst lemma is a result from our
earlier work [SBGW17].

Lemma 3. ([SBGW17, Lemma 3]) If λ ≥ 1.01p|||oWbs′|||op , then

n
|||MSVT − M ∗|||2F ≤ c min
j=1

λ

2

,

σ

2 j

(M

∗

)

with probability at least 1 − c1e−c′n, where c, c1 and c′ are positive universal constants.

19

Our second lemma is an approximation-theoretic result that bounds the tail of the singular values of any matrix with a given permutation-rank or non-negative rank. The proof of this lemma builds on a construction due to Chatterjee [Cha14].

Lemma 4. (a) For any matrix M ∈ CPR(ρ) and any s ∈ {1, 2, . . . , n − 1}, we have

n σ2(M ) ≤ ndρ2 .

j

s

j=s+1

(b) For any matrix M ∈ CNR(r) and any s ∈ {1, 2, . . . , n − 1}, we have

n
σj2(M ) ≤ nd max
j=s+1

r − s,0 . r

Our third lemma controls the noise term W ′.

Lemma

5.

Suppose

that pobs ≥

1 min{n,d}

log7(nd).

Then given a random

matrix W ′

with entries

distributed according to the distribution (29), we have

P |||W ′|||op > 2.01 pobs(n + d) ≤ e−c′ max{n,d}.

Finally, our fourth lemma is a more general relation pertaining to matrices established in the paper [Sch68].

Lemma 6 ([Sch68]). For any pair of matrices A, B ∈ Rn×d with singular value decompositions A = U1D1V1T and B = U2D2V2T , it must be that

(U1

U

T 2

,

V1V2T ) ∈

arg min

|||A − U BV T |||2F

U ∈Rn×n, V ∈Rd×d

such that U T = U −1, V T = V −1.

(30)

Based on these four lemmas, we now complete the proof of the theorem. From Lemma 5 we see that the choice λ = 2.1 npo+bds guarantees that λ ≥ 1.01p|||oWbs′|||op with probability at least 1 − e−c′ max{n,d}. Consequently, the condition required for an application of Lemma 3 is satisﬁed,
and applying this lemma then yields the upper bound

n
|||MSVT − M ∗|||2F ≤ c min
j=1

podbs , σj2(M ∗)

(31)

with probability at least 1 − e−c′d, where we have also used our assumption that n ≤ d. Now consider any matrix M0 ∈ Rn×d. In what follows, we convert the bound (31) into one that
depends on the properties of M0, namely ρ(M0), r(M0) and |||M ∗ − M0|||F. Let ρ0 = ρ(M0) and
r0 = r(M0).

20

We ﬁrst have the following deterministic upper bound

n
min
j=1

podbs , σj2(M ∗)

n
≤ 2 min
j=1

podbs , σj2(M0)

n

+2

min

j=1

d , σj(M ∗) − min pobs

d

2

pobs , σj (M0)

n

d2

n ∗

2

≤ 2 min pobs , σj (M0) + 2 σj (M ) − σj(M0) ,

(32)

j=1

j=1

where the inequality (32) is a consequence of the more general result that (min{a, b1}−min{a, b2})2 ≤ (b1 − b2)2 for any three real numbers a, b1, and b2.
We now bound the two terms on the right hand side of (32) separately. For the second term, we call upon Lemma 6 with the choices A = M ∗ and B = M0. With this choice, some simple algebra yields that the minimum value of the objective in (30) equals nj=1(σj(M ∗) − σj(M0))2. On the other hand, the choice of U and V as identity matrices is feasible for (30), and the associated value of the objective equals |||M ∗ − M0|||2F. Consequently, we have the inequality

n

(σj (M ∗) − σj(M0))2 ≤ |||M ∗ − M0|||2F.

(33)

j=1

As for the ﬁrst term of (32), an application of Lemma 4(a) to the matrix M0 yields the bound

n
min
j=1

podbs , σj2(M0)

≤ min
s∈[n]

sd + (ρ0)2nd

pobs

s

n

≤ 3ρ0d pobs ,

(34)

where inequality (34) is obtained with the choice s = ⌈ρ0√pobsn⌉. Separately, an application of Lemma 4(b) to the matrix M0 yields

n
min
j=1

podbs , σj2(M0)

≤ min
s∈[n]

sd + nd max pobs

1− s ,0 r0

≤ r0d ,

(35)

pobs

where the inequality (34) is obtained with the choice s = r0. Combining the bounds (31), (32), (33), (34) and (35), we obtain the result that the inequality

|||MSVT − M ∗|||2F ≤ 2 min

√

3ρ0d √

n , r0d

pobs pobs

+ 2|||M ∗ − M0|||2F,

must hold with probability at least 1 − ec′d. Finally, recalling our assumption that d ≥ n and substituting n = min{n, d} and d = max{n, d} yields the claimed result.

21

Proof of Lemma 4 Part (a): Without loss of generality, assume that d ≥ n. We begin with an upper bound on the tail of the singular values of any matrix in CPR(1), that
is, of any matrix that has a permutation-rank of 1. The proof of this bound uses a construction due to Chatterjee [Cha14] for a rank s approximation of any matrix in CPR(1), for any value s ∈ [n]. We ﬁrst reproduce Chatterjee’s construction.
For a given matrix M ∈ CPR(1), deﬁne the vector τ ∈ Rd of column sums—namely, with entries τj = ni=1[M ]ij for j ∈ [d]. Using this vector, deﬁne a rank s approximation M to M by grouping the columns according to the vector τ according to the following procedure:
• Observing that each τj ∈ [0, n], divide the full interval [0, n] into s groups—say of the form
[0, n/s), [n/s, 2n/s), . . . [(s − 1)n/s, n].

If τj falls into the interval α for some α ∈ [s], then map column j to the group Gα of indices.
• For each α ∈ [s] such that group Gα is non-empty, choose a particular column index j′ ∈ Gα in an arbitrary fashion. For every other column index j ∈ Gα, set Mij = Mij′ for all i ∈ [n].

By construction, the matrix M has at most s distinct rows, and hence rank at most s. Now consider any column j ∈ [d] and suppose that j ∈ Gα for some α ∈ [s]. Let j′ denote the column chosen for the group Gα in the second step of the construction. Since M ∈ CPR(1), we must either
have Mij ≥ Mij′ = Mij for every i ∈ [n], or Mij ≤ Mij′ = Mij for every i ∈ [n]. Then we are guaranteed that

n

n

n

|Mij − Mij | =| (Mij − Mij ) |= |τj′ − τj| ≤ s ,

(36)

i=1

i=1

where we have used the fact the pair (τj, τj′) must lie in an interval of length at most n/s. This completes the description of Chatterjee’s construction.
In what follows, we use Chatterjee’s result in order to obtain our claimed bound on the tail of the spectrum of any matrix M ∈ CPR(ρ). We modify the result in a careful manner that allows us to obtain the desired dependence on the parameter ρ. Recall that any matrix M ∈ CPR(ρ) can be decomposed as

ρ
M = M (ℓ),
ℓ=1

for some matrices M (1), . . . , M (ρ) ∈ CPR(1). Let s = ρs . For every ℓ ∈ [r], let M (ℓ) be a rank

s

=

s ρ

approximation

of

M (ℓ)

obtained

from

Chatterjee’s

construction

above,

but

with

the

following

additional detail. Observe that in Chatterjee’s construction, the choice of column j′ from group

Gα is arbitrary. For our construction, we will make a speciﬁc choice of this column: we choose

the column whose entries have the smallest values among all columns in the group Gα. With this

choice, we have the property

Mi(jℓ) ≤ Mi(jℓ) for every ℓ ∈ [ρ], i ∈ [n], j ∈ [d].

(37)

22

Now let M : =

ρ ℓ=1

M

(ℓ).

Since

every

entry

of

every

matrix

M (ℓ)

is

non-negative,

we

have

that

every entry of M is also non-negative. We also claim that

ρ

(i) ρ

Mij = Mi(jℓ) ≤ Mi(jℓ) = Mij ≤ 1,

ℓ=1

ℓ=1

where the inequality (i) is a consequence of the set of inequalities (37). Thus we have that M ∈ [0, 1]n×d, M ∈ [0, 1]n×d, and that the rank of M is at most ρs. This result then yields the bound

n

nd

nd ρ

σj2(M ) ≤ |||M − M |||2F ≤

|Mij − Mij | =

| (Mi(jℓ) − Mi(jℓ))|

j=ρs+1

i=1 j=1

i=1 j=1 ℓ=1

(i) n d ρ

≤

|Mi(jℓ) − Mi(jℓ)|,

i=1 j=1 ℓ=1

where inequality (i) follows from the triangle inequality. Now recall that every matrix M (ℓ) is obtained from Chatterjee’s construction, and rewriting Chatterjee’s result (36) for the matrices M (ℓ) presently under consideration, we obtain

n (ℓ)

(ℓ) n

|Mij − Mij | ≤ s ,

i=1

for every ℓ ∈ [ρ]. As a consequence, we have

n 2

ρnd ρ2nd

σj (M ) ≤ s = s ,

j=ρs+1

where

we

have

substituted

the

relation

s

=

s ρ

to

obtain

the

ﬁnal

result.

Part (b): This result follows directly from the facts that the rank of M is at most r, and the square of its Frobenius norm is at most nd.

Proof of Lemma 5 Deﬁne an ((n + d) × (n + d)) matrix W ′′ as

W ′′ = √p1obs (W0′)T

W′ 0.

From (29) and the construction above, we have that the matrix W ′′ is symmetric, and has mutually
independent entries above the diagonal that have a mean of zero and a variance upper bounded by
1. Consequently, known results in random matrix√theory (e.g., see [Cha14, Theorem 3.4] or [Tao12, Theorem 2.3.21]) yield the bound |||W ′′|||op ≤ 2.01 n + d with probability at least 1 − e−c max{n,d}, under the assumption pobs ≥ min{1n,d} log7(nd). One can also verify that |||W ′′|||op = √p1obs |||W ′|||op, yielding the claimed result.

23

4.4 Proof of Proposition 1

We recall that for any integer k ≥ 0, the notation Jk denotes an upper triangular matrix of size

(k × k) with all entries on and above the diagonal set as 1, and Ik denotes the identity matrix of

size (k × k). Consider an (n × d) matrix M with the following block structure:





Jr−ρ+1 0 0

M : =  0 Iρ−1 0 .

0

00

In the remainder of the proof, we show that r(M ) = r and ρ(M ) = ρ. Using the ideas in the construction of M and the associated proof to follow, one can construct many other matrices that have a non-negative rank of r and a permutation-rank of ρ, for any given value 1 ≤ ρ ≤ r ≤ min{n, d}.
We partition the proof into four parts.

Proof of r(M ) ≤ r: One can write M as a sum of r matrices, each having a non-negative rank of one: for each non-zero row, consider a component matrix comprising that row and zeros elsewhere. Consequently, we have r(M ) ≤ r.
Proof of r(M ) ≥ r: Observe that the (conventional) rank of M ∗ equals r. Since the rank of any matrix is a lower bound on its non-negative rank, we have that r(M ) ≥ r. We have thus established that the non-negative rank of this matrix equals exactly r.

Proof of ρ(M ) ≤ ρ: Observe that the (n × d) matrix with Jr−ρ+1 as its top-left submatrix and zeros elsewhere has a permutation-rank of 1. Moreover, any (n × d) matrix with exactly one entry as 1 and the remaining entries 0 also has a permutation-rank of 1, and hence a (n × d) matrix with Iρ−1 as its submatrix and zeros elsewhere has a permutation-rank of at most (ρ − 1). Putting these arguments together, we obtain the bound ρ(M ) ≤ ρ.

Proof that ρ(M ) ≥ ρ: First observe that the matrix
I2×2 = 10 01
does not belong to CPR(1). It follows that any matrix containing I2×2 as a submatrix cannot belong to the set CPR(1). It further follows that for any positive integer k, the matrix Ik×k must have a permutation rank of at least k. Finally, observe that the matrix M contains Iρ×ρ as its submatrix (given by the intersection of rows {r − ρ, . . . , r} with the columns {r − ρ, . . . , r}). It follows that M must have a permutation rank of at least ρ, thereby proving the claim.

4.5 Proof of Proposition 2

We

assume

for

ease

of

exposition

that

n

and

d

are

divisible

by

k.

Otherwise,

since

k

≤

1 2

min{n, d},

one may take ﬂoors or ceilings which will change the result only by a constant factors. Since

CNR(k) ⊆ CPR(k), we have sup

inf |||M − M ′|||2F = 0. In what follows, we show that

M ∈CNR(k) M ′∈CPR(k)

sup

inf |||M − M ′|||2F ≥ cnkd .

M ∈CPR(k) M ′∈CNR(k)

24

Consider

the

block

matrix

M

∈

[0,

1]

n k

×

d k

:

M= 1 1 ,

(38)

10

where each of the four blocks is of size ( 2nk × 2dk ). The following lemma shows that the best rank-1 approximation to M has a large approximation error:
Lemma 7. For the matrix M deﬁned in (38), for any vectors u ∈ Rn and v ∈ Rd, it must be that

where c > 0 is a universal constant.

|||M − uvT |||2 ≥ c nd ,

F

k2

We now use the matrix M deﬁned in (38) to build the following block matrix M ∈ CPR(k):





M 0 ··· 0

 0 M · · · 0 

M

:=

 

...

...

...

...

 . 

0 0 ··· M

In words, the matrix M is a block-diagonal matrix where the diagonal has k copies of M .
Due to the block diagonal structure of M , the singular values of M are simply k copies of the singular values of its constituent matrix M . Consequently, we have that for any matrix M ′ ∈
CNR(k):

|||M

−

M ′|||2

≥

k(|||M |||2

−

|||M |||2

)

(i)
≥

c nd ,

F

F

op

k

as claimed, where the inequality (i) is a consequence of Lemma 7.

Proof of Lemma 7 Consider any value i ∈ [ 2nk ] and j ∈ [ 2dk ]. Then we claim that

(Mi,j − [uv T ]i,j )2 + (Mi+ 2nk ,j − [uv T ]i+ 2nk ,j )2+(Mi,j+ 2dk − [uv T ]i,j+ 2dk )2

+ (Mi+ n ,j+ d − [uv T ]i+ n ,j+ d )2 ≥ 0.01.

2k

2k

2k

2k

(39)

If not, then for the choice of M in (38), we must have [uvT ]i,j ∈ (0.9, 1.1), [uvT ]i+ n ,j ∈ (0.9, 1.1),

2k

[uvT ]i,j+ d ∈ (0.9, 1.1) and [uvT ]i+ n ,j+ d < 0.1. However, since [uvT ]i′,j′ = ui′vj′ for every

2k
coordinate

(i′,

j′),

we

also

have

2k

2k

[uvT ]i,j × [uv T ]i+ 2nk ,j+ 2dk = [uv T ]i+ 2nk ,j × [uv T ]i,j+ 2dk ,

which contradicts the required ranges of the individual coordinates. Summing the bound (39) over all values of i ∈ [ 2nk ] and j ∈ [ 2dk ] yields the claimed result.

25

4.6 Proof of Proposition 3

Consider any set S and any convex set C. We begin with a key lemma that establishes a relation between H(S, C) and a proposed notion of the inherent convexity of S.

Lemma 8. For any set S ⊆ [0, 1]n×d and any convex set C ⊆ [0, 1]n×d, it must be that

H(S, C) ≥ 2 sup

inf ||| 1 (M1 + M2) − M0|||2.

(40)

9 M1∈S, M2∈S M0∈S 2

F

The left hand side of inequality (40) is the Hausdorﬀ distance between the sets S and C in terms of the squared Frobenius norm. The right hand side of the inequality represents a notion of the inherent convexity of the set S.
With this lemma in place, we now complete the remainder of the proof. To this end, we set S = CPR(1), and let C be any convex set of [0, 1]-valued (n × d) matrices.
We now construct a pair of matrices M1 ∈ CPR(1) and M2 ∈ CPR(1) that we use to lower bound the right hand side of (40). Deﬁne matrices M1 ∈ CPR(1) and M2 ∈ CPR(1) as

[M1]ij =

1

if

i≤

n2 ,

j

≤

d 2

0 otherwise,

and

[M2]ij =

1

if

i>

n2 ,

j

>

d 2

0 otherwise.

It follows that the entries of the matrix 12 (M1 + M2) are given by:

[ 1 (M1 + M2)]ij =

1 2

if (i ≤ n2 , j ≤ d2 ) or (i > n2 , j > d2 )

2

0 otherwise.

Now consider any pair of integers (i, j) ∈ [⌊n/2⌋] × [⌊d/2⌋]. Then the (2 × 2) submatrix of 21 (M1 + M2) formed by its entries (i, j), (i + ⌈n/2⌉, j), (i, j + ⌈d/2⌉) and (i + ⌈n/2⌉, j + ⌈d/2⌉) equals

1 2

0.

0

1 2

It is easy to verify that there is a constant c > 0 such that the squared Frobenius norm distance between this rescaled identity matrix and any (2 × 2) matrix in CPR(1) is at least c. Since this argument holds for any choice of (i, j) ∈ [⌊n/2⌋] × [⌊d/2⌋], summing up the errors across each of these sets of entries yields

||| 21 (M1 + M2) − M |||2F ≥ c′nd, for every matrix M ∈ CPR(1),
where c′ > 0 is a universal constant. Finally, substituting this bound in Lemma 8 yields the claimed result.
It remains to prove Lemma 8.

26

Proof of Lemma 8. Consider any matrices M1 ∈ S and M2 ∈ S. From the deﬁnition of the Hausdorﬀ distance H, we know that there exist matrices M1 ∈ C and M2 ∈ C such that

|||Mi − Mi|||2F ≤ H(S, C), for i ∈ {1, 2}.

(41)

Since C is a convex set, we also have 21 (M1 + M2) ∈ C. Then from the deﬁnition of H, we also know that there exists a matrix M0 ∈ S such that
||| 21 (M1 + M2) − M0|||2F ≤ H(S, C). (42)
Finally, applying the triangle inequality to the bounds (41) and (42) yields
||| 21 (M1 + M2) − M0|||2F ≤ 3||| 12 (M1 + M2) − M0|||2F + 43 |||M1 − M1|||2F + 34 |||M2 − M2|||2F ≤ 9 H(S, C). 2

4.7 Proof of Proposition 4
Suppose there exists a coordinate pair (i, j) such the stated condition is violated. Then there must
exist two distinct values ℓ1 ∈ [ρ(M )] and ℓ2 ∈ [ρ(M )] that satisfy the following three conditions: (a) Mi(jℓ1) > 0 and Mi(jℓ2) > 0, (b) The value of Mi(jℓ1) is diﬀerent from all other entries in M (ℓ1), and (c) The value of Mi(jℓ2) is diﬀerent from all other entries in M (ℓ2).
In addition, the fact that Mi(jℓ1) + Mi(jℓ2) ∈ (0, 1) for every coordinate (i, j), along with condition (a) above, imply a fourth condition:
(d) Mi(jℓ1) < 1 and Mi(jℓ2) < 1. Now for any ǫ > 0, deﬁne Mǫ(ℓ1) and Mǫ(ℓ2) to be matrices obtained by replacing the (i, j)th
entries of the matrices M (ℓ1) and M (ℓ2) with (Mi(jℓ1)+ǫ) and (Mi(jℓ2)−ǫ) respectively. Now, conditions (b)–(d) in tandem imply that there exists some value ǫ > 0 such that all of the following properties
hold: (i) [Mǫ(ℓ1)]ij ∈ [0, 1], (ii) [Mǫ(ℓ2)]ij ∈ [0, 1], and
(iii) The relative ordering of the entries of M (ℓ1) is identical to the relative ordering of the entries of Mǫ(ℓ1); the relative ordering of the entries of M (ℓ1) is identical to the relative ordering of the entries of Mǫ(ℓ1).
Properties (i) and (ii) imply that Mǫ(ℓ1) ∈ [0, 1]n×d and Mǫ(ℓ2) ∈ [0, 1]n×d. Combined with property (iii), we also have Mǫ(ℓ1) ∈ CPR(1) and Mǫ(ℓ2) ∈ CPR(1). Finally, from the construction of the matrices Mǫ(ℓ1) and Mǫ(ℓ2), it is easy to see the relation

M = Mǫ(ℓ1) + Mǫ(ℓ2) +

M (i).

i∈[ρ(M )]\{ℓ1,ℓ2}

This decomposition of M is a diﬀerent, valid permutation-rank decomposition of M .

27

5 Discussion and future work
We posit that the conventional low-rank models for matrix completion and denoising are equivalent to “parametric” assumptions with undesirable implications. We propose a new permutation-rank approach and argue, by means of a philosophical discussion as well as theoretical guarantees, that this approach oﬀers signiﬁcant beneﬁts at little additional cost. Our work also contributes to a growing body of literature [Sha17, Part 1], [SBGW17, SW15, SBW16b, SBW16a, HSRW16, CM16, FMR16, CGMS17] on moving towards more ﬂexible models based on permutations that provide robustness to model mismatches.
Our work gives rise to some useful open problems that we hope to address in future work. In this paper, we established beneﬁts of the permutation-based approach for the matrix completion problem under the random design observation setting. In the literature, the classical low (nonnegative) rank matrix completion problem is more recently also studied under other observation models such as weighted random sampling [NW12], ﬁxed design [JNS13, Klo14], streaming/active learning [YLP15, JKN16, BZ16], or biased observation models [HND15], which are also of interest in the context of permutation-rank matrix completion. A second open problem is to close the gap between the statistically optimal minimax rate of estimation and the best known rate for polynomial-time computable algorithms for the permutation-rank model. Any solution to this problem may also contribute to the understanding of some other open problems in the literature (e.g., see [SBGW17, FMR16, SBW16b]) on the gap between the statistical and computational aspects of estimation under an unknown permutation.
Acknowledgments
The work of MJW and NBS was partially supported by DOD Advanced Research Projects Agency W911NF-16-1-0552 and National Science Foundation grant NSF-DMS-1612948. The work of SB was supported by NSF grant DMS-1713003.
Appendix
A Intuitive algorithms that provably fail
In this section, we present two intuitive polynomial-time computable algorithms for the permutationrank setting—one for estimating M ∗ from Y , and one for decomposing M ∗ into its constituent permutation-rank-one matrices—and show that these algorithms provably fail. Our goal in describing these negative results is as a complement to the positive results provided in the main text, and with the hope that the points of failure of these algorithms may form starting points for subsequent research.
A.1 An intuitive polynomial-time estimator for M∗ from Y
In this section, we consider the problem of estimating the matrix M ∗ from noisy and partial observations Y as deﬁned earlier in equation (1). For simplicity, we assume that pobs = 1. For any vector z ∈ Rm, we let vector z+ ∈ Rm with entries [z+]i = max{zi, 0} represent the positive component of z, and vector z− ∈ Rm with entries [z−]i = min{zi, 0} represent the negative component of z.
28

We ﬁrst provide some intuition and background to motivate the estimator we study in this

section, and then present a formal deﬁnition. Denote the permutation-rank of matrix M ∗ as

ρ∗ : = ρ(M ∗), and assume that the value of ρ∗ is known. The goal is to obtain an estimate

M

∈ CPR(ρ∗) from the observed matrix Y

such that the error

1 nd

|||M

∗

−

M

|||2F

is as small as possible.

For any such matrix M , let us use the following notation for its permutation-rank decomposition:

M=

ρ∗ ℓ=1

M

(ℓ)

where

M (ℓ)

∈

CPR(1)

for

every

ℓ

∈

[ρ∗].

Further,

we

let

π(ℓ)

and

σ(ℓ)

respectively

denote the permutation of the rows and columns of M (ℓ).

Past literature on computationally-eﬃcient estimation for such problems provides us with es-

timators with the following two distinct goals: (i) to estimate the permutations of the constituent

matrices in the permutation-rank decomposition, and (ii) estimators to compute the entries of the

constituent matrices given the permutations. For each of these two goals, we describe a natural

estimator below from past works.

Estimating the permutations via singular value decomposition: Compute the singular value de-

composition Y =

min{n,d} ℓ=1

a(ℓ)

[b(ℓ)

]T

such

that

the

vectors

{a(1), . . . , a(min{n,d})}

are

mutually

orthogonal, the vectors {b(1), . . . , b(min{n,d})} are mutually orthogonal, and [a(1)]T b(1) ≥ . . . ≥

[a(min{n,d})]T b(min{n,d}). In order to resolve a global sign ambiguity, we also mandate the condi-

tion |||a(+ℓ)|||2 ≥ |||a−(ℓ)|||2 for every ℓ ∈ [min{n, d}]. Finally, for each ℓ ∈ [ρ∗], set π(ℓ) and σ(ℓ) as the ordering of the entries of a(ℓ) and b(ℓ) respectively.

From past works [SBW16b], this estimator for the permutations is known to possess appealing

properties for the case when ρ∗ = 1. For instance, it is not hard to see that in a noiseless setting

where Y = M ∗, the estimator will yield exactly the row and column permutations of M ∗ ∈ CPR(1).

This fact is employed in the paper [SBW16b] to obtain consistent estimates of the permutations

associated to an unknown matrix in CPR(1) in the context of a “crowd labeling” problem. It is

also not hard to verify that the estimator is can be computed in a computationally-eﬃcient manner.

Estimating the entries via least squares, when given the permutations: Given some estimate π(1), σ(1), . . . , π(ρ∗), σ(ρ∗) of the permutations associated to the permutation-rank decomposition of M ∗, the following estimator M provides an estimate of the matrix M ∗ as well as the matrices in
its permutation-rank decomposition.

M ∈ arg min |||Y − M |||2F
M ∈[0,1]n×d

(43)

ρ∗
such that M = M (ℓ), and

ℓ=1
M (ℓ) ∈ CPR(1) with rows and columns ordered by (π(ℓ), σ(ℓ)), for every ℓ ∈ [ρ∗].

The aforementioned estimator M is a natural extension of the least-squares estimators studied in past works [SBGW17, SBW16a] for the case when M ∗ ∈ CPR(1). The estimator is known to have appealing properties from both the statistical and computational perspectives. From a computa-
tional standpoint, all of the constraints in the optimization program (43) can be expressed as a set of
(polynomial number of) linear inequalities, thereby making the optimization problem computationally tractable. From a statistical standpoint, if the given permutations (π(1), σ(1), . . . , π(ρ∗), σ(ρ∗))
are exactly (or approximately) equal to the permutations associated to a permutation-rank decomposition of M ∗, the proofs of the results in [SBGW17, SBW16a] as well as Theorem 1 in the present

29

paper imply that the estimator M is minimax optimal for estimating M ∗ from Y . The estimator M continues to remain statistically eﬃcient if the permutations are known up to a reasonable approximation.
Given the two intuitive estimators discussed above, a natural means to estimate M ∗ from Y is to concatenate these two estimators to obtain the following two-step estimator: Step 1: From Y , obtain an estimate (π(1), σ(1), . . . , π(ρ∗), σ(ρ∗)) of the permutations of the decomposition of M ∗ via the singular value decomposition-based estimator described above. Step 2: Using the estimates of the permutations, obtain an estimate M of M ∗ via the least squares projection (43).
We believe that when ρ∗ = 1, this estimator is not only consistent, but it has an expected error decaying at the rate O(min{n, d}−1/2). We now show that in fact as soon as one moves to the setting of ρ∗ > 1, this estimator is no longer even consistent—even if there is no noise.
Proposition 5. There exists a matrix M ∗ ∈ CPR(2) such that when Y = M ∗, the two-step estimator M has an error lower bounded as
n1d |||M ∗ − M |||2F ≥ c2,
with probability 1.
The proof of this result is provided in Appendix A.3.1. The proof also demonstrates an identical negative result for the following modiﬁed estimation algorithm: In computing M as above, instead of taking only the permutations of the top ρ∗ singular vectors, collect permutations from singular vectors until you obtain ρ∗ distinct permutations; then apply the least squares projection step to these ρ∗ distinct permutations.

A.2 An intuitive greedy algorithm for permutation-rank decomposition
Consider any matrix any matrix M ∈ [0, 1]n×d. The singular value decomposition of M into components having a (conventional) rank of one can be performed with the following greedy algorithm:

• Let k = 1 • While M =

k−1 ℓ=1

M

(ℓ):

– Let M (k) ∈ arg min |||M −
M ′=abT (a,b)∈Rn ×Rd
– k=k+1

k−1 ℓ=1

M (ℓ)

−

M ′|||F

• Output k as the rank of M and {M (1), . . . , M (k)} as its singular value decomposition.

An obvious question that arises is whether a similar greedy algorithm works to obtain a permutation-

rank decomposition.

To this end, consider any value q ≥ 1, and for any matrix M , let |||M |||q denote its entry-wise

norm |||M |||q : =

i,j (Mij )q

1
q.

Then the natural analogue of the aforedescribed algorithm in the

context of permutation-rank decomposition is as follows:

30

• Let ρ = 1

• While M =

ρ−1 ℓ=1

M

(ℓ)

:

– Let M (ρ) ∈ arg min |||M −
M ′∈CPR(1)
– ρ=ρ+1

ρ−1 ℓ=1

M

(ℓ)

−

M

′|||q

• Output ρ as the permutation-rank of M and {M (1), . . . , M (ρ)} as its permutation-decomposition

The following proposition investigates whether such an algorithm will work.

Proposition 6. For any values of n, d and ρ ≥ 2, there exists an (n × d) matrix M ∈ CPR(ρ) such that the above algorithm outputs a decomposition of permutation-rank at least (ρ + 1).

The guaranteed incorrectness of the permutation rank of the output of the algorithm also directly implies that the decomposition is also incorrect.

A.3 Proofs
We now present the proofs of the negative results introduced in this section.

A.3.1 Proof of Proposition 5

In what follows, for clarity of exposition, we ignore issues pertaining to ﬂoors and ceilings of numbers, as they aﬀect the results only by a constant factor.
We begin by deﬁning a matrix M ∗ ∈ CPR(2) as
M ∗ = M (1) + M (2),

with

M (1) = a(1)(b(1))T + a(2)(b(2))T and M (2) = a(3)(b(3))T .

Set

a(1) = [1 .9 · · · .9 a(2) = [0 .2 · · · .2 a(3) = [0 0 · · · 0

.8 · · · .8 −.1 · · · − .1
0 ··· 0

0 · · · 0 ]T 0 · · · 0 ]T 1 · · · 1 ]T ,

ν1(n−1)

ν2 (n−1)

ν3(n−1)

and

b(1) = [1 .9 · · · .9 b(2) = [0 .2 · · · .2 b(3) = [0 0 · · · 0

.8 · · · .8 −.1 · · · − .1
0 ··· 0

0 · · · 0 ]T 0 · · · 0 ]T 1 · · · 1 ]T ,

ν1 (d−1)

ν2(d−1)

ν3(d−1)

where ν1 = .684, ν2 = .304, and ν3 = .012. It is easy to verify that all entries of the matrices M ∗, M (1), M (2) lie in the interval [0, 1] and that M (1) ∈ CPR(1) and M (2) ∈ CPR(1) and M ∗ ∈ CPR(2)\CPR(1).
One can further verify the following properties of this construction:

31

1. a(ℓ), a(ℓ′) = 0 and b(ℓ), b(ℓ′) = 0 for every ℓ = ℓ′ ∈ {1, 2, 3}. 2. |||a(1)|||2 > |||a(2)|||2 > |||a(3)|||2 and |||b(1)|||2 > |||b(2)|||2 > |||b(3)|||2. 3. The (conventional) rank of M ∗ is 3. 4. a(1), a(2) and a(3) have diﬀerent permutations of their entries; likewise, b(1), b(2) and b(3) have
diﬀerent permutations of their entries. 5. |||a(+ℓ)|||2 ≥ |||a(−ℓ)|||2 and |||b(+ℓ)|||2 ≥ |||b−(ℓ)|||2 for every ℓ ∈ [3]. The ﬁve properties listed above imply that the following decomposition of M ∗,

3
M ∗ = a(ℓ)(b(ℓ))T ,
ℓ=1
is a valid singular value decomposition with the global signs of the constituent vectors satisfying the conditions of Step 1 of the algorithm. Consequently, the ρ∗ = 2 estimated permutations in Step 1 of the algorithm are those given by the respective orderings of the entries of the vectors {a(1), b(1)} and {a(2), b(2)}.
Observe that the entries 2 to (1+ν1(n−1)) of both a(1) and a(2) have values higher than the last ν3(n − 1) entries of these vectors, and hence this ordering is reﬂected in the respective permutations derived from these vectors. Likewise, the entries 2 through (1 + ν1(d − 1)) are ranked higher than the last ν3(d − 1) entries in the permutation derived from the vectors b(1) and b(2). Due to this collection of inequalities, the least squares program in Step 2 of the algorithm must mandate that

Mij ≥ Mi′j′ ≥ Mi′′j′′ ,

(44)

whenever 2 ≤ i, i′ ≤ 1 + ν1(n − 1); n − ν3(n − 1) < i′′ ≤ n; 2 ≤ j ≤ 1 + ν1(d − 1); and d − ν3(d − 1) < j′, j′′ ≤ d. However, for each coordinate in this range, we also have Mi∗j = .85, Mi∗′j′ = 0, Mi∗′′j′′ = 1. Consequently, any triplet of values (Mij, Mi′j′, Mi′′j′′) that follows the ordering (44) must necessarily incur an error lower bounded as

(Mij − Mi∗j )2 + (Mi′j′ − Mi∗′j′ )2 + (Mi′′j′′ − Mi∗′′j′′ )2 ≥ c,

for some universal constant c > 0. Summing up the errors over all the entries of the matrix in
the aforementioned coordinate set yields that any matrix M satisfying the constraints of the least squares problem must have squared Frobenius error at least |||M − M ∗|||2F ≥ c′nd, for some universal constant c′ > 0.

A.3.2 Proof of Proposition 6
First let n = d = ρ = 2. Consider the (2 × 2) matrix M deﬁned as
M : = 0 .6 . .6 .4
It is easy to verify that M ∈ CPR(2)\CPR(1). Let us now investigate the operation of the proposed algorithm on this matrix M . The following
lemma controls the ﬁrst step of the algorithm.

32

Lemma 9. When the input matrix M is as deﬁned above, the algorithm will select M (1) = 0 .4 .4 .4
in the ﬁrst iteration. As a consequence of this lemma, we have the following residual that is used for the subsequent
iterations of the algorithm: M − M (1) = 0 .2 . .2 0

It is easy to see the that the residual matrix (M − M (1)) ∈ CPR(2)\CPR(1). Also observe that in each iteration, the algorithm subtracts out a matrix in CPR(1) from the residual. Consequently, the algorithm will require at least two more iterations to terminate. The algorithm thus necessarily outputs a decomposition with ρ ≥ 3, as claimed.
Next we extend these arguments to any arbitrary values of n ≥ 2, d ≥ 2, ρ ≥ 2. Consider matrix M with entries:

• M11 = 0, M12 = M21 = .6, M22 = .4 • Mii = 1 for every i ∈ {3, . . . , ρ} • Mij = 0 for every other coordinate (i, j).

The matrix M has a block-diagonal structure with the top-left (2 × 2) block as one non-zero

component and (ρ − 2) other entries on the diagonal as (ρ − 2) additional non-zero components.

The rest of the proof is partitioned into two cases:

Case I: Suppose that at some step ρ of the algorithm, some entry of the residual matrix (M −

ρ ℓ=1

M

(ℓ)

is

strictly

negative.

Then

the

algorithm

will

never

terminate

because

every

subsequent

candidate matrix in the minimization step of the algorithm must lie in CPR(1) and hence must have

non-negative entries. The algorithm will thus output ρ = ∞.

Case II: Now suppose that the residual matrices always have non-negative entires. Then given the block-diagonal structure of M , any matrix M (1), M (2), . . . in the iterations of the algorithm can be non-zero in exactly one of these diagonal components. As a result, the overall decomposition yielded by the algorithm decouples into ρ individual decompositions of the ρ respective blocks, each of which will contribute a permutation-rank of at least 1. Moreover, from the arguments for the case of n = d = 2 above, we also have that the top-left (2 × 2) block will induce a decomposition of permutation-rank 3 in the algorithm. Putting the pieces together, we see that the matrix M will induce the proposed algorithm to output a decomposition of permutation-rank at least (ρ + 1), whereas ρ(M ) = ρ.

Proof of Lemma 9 Since M11 = 0, we must have M1(11) = 0. Now suppose the column ordering of M (1) is such that the ﬁrst column is greater than the second column. Then we must have M1(21) = 0. Since M (1) is the minimizer of the optimization program in the algorithm we must then have M2(11) = M21 and M2(21) = M22, and consequently |||M −M (1)|||q = M1(21) = .6. An analogous argument

33

holds if the ﬁrst row is greater than the second row in the permutation of M (1). Finally, suppose

that in the permutations of M (1), the second column is greater than the ﬁrst column and the second

row is greater than the ﬁrst row. Then we must have .4 ≥ M2(21) ≥ max{M1(21), M2(11)}. With this

condition, one can see that the minimizer of the optimization program is M1(21) = M2(11) = M2(21) = .4.

Consequently,

we

have

|||M

− M (1)|||q

=

1
.2 × 2 q

<

.6

for

any

q

≥

1.

Thus

the

algorithm

chooses

M (1) = 0 .4 . .4 .4

B Alternative interpretation of the non-negative rank model

In the non-negative rank model described in the introduction, one may wonder why the aﬃnity of

a user to a movie conditioned on a feature must be modeled as the product uℓi vjℓ of the separate

connections of the user and movie to the feature. Secondly, one may also wonder why the net aﬃnity

of a user to a movie is the sum of the aﬃnities across the features

r ℓ=1

uℓi vjℓ.

These

two

modeling

assumptions may sometimes be confusing, and hence in what follows, we present an alternative

interpretation of the low non-negative rank model for the recommender systems application.

Consider any feature ℓ ∈ [r]. The aﬃnities of users towards movies conditioned on this feature

is a matrix, say X(ℓ) ∈ [0, 1]n×d. The matrix X(ℓ) is assumed to have a (non-negative) rank of 1.

Hence the probability that user i likes movie j, when asked to judge only based on feature ℓ, equals

Xi(jℓ).

Now, every user is assumed to have their own way of weighing features to decide which movies

they like. Speciﬁcally, any user i ∈ [n] is associated to values α(i1), . . . , α(ir) such that α(iℓ) ≥ 0 for

every ℓ ∈ [r] and

r ℓ=1

α(iℓ)

=

1.

The

probability

that

user

i

likes

any

movie

j

is

assumed

to

be

the

convex combination

r
α(i ℓ) Xi(jℓ) .
ℓ=1

This completes the description of the model.

Let us verify that the resulting user-movie matrix has a non-negative rank of r. Recall the

assumption that X(ℓ) has a non-negative rank of 1, and let X(ℓ) = uℓ(vℓ)T for some vectors uℓ

and vℓ. Then the ith row of the overall user-movie matrix equals overall user-movie matrix equals

r ℓ=1

α(iℓ)uℓi (vℓ)T ,

and

hence

the

r
uℓ(vℓ)T ,
ℓ=1

where

α(ℓ)uℓ 

uℓ = 

1
...

1 .

α(nℓ)uℓn

This completes the alternative description of the non-negative rank model.

One can observe that the restriction

r ℓ=1

α(iℓ)

=

1

makes

this

model

slightly

more

restrictive

than the non-negative rank model described earlier in the main text. However, all of our results

on estimation for the non-negative rank model described in Section 2.1 continue to apply to this

model as well.

34

References

[AGKM12]
[BZ16] [CCS10] [CGMS17] [Cha14] [CJSC13] [CM16] [CO10] [CR09] [CT10] [CT12] [DG+14] [DR16]
[DS03]
[FMR16] [Gil52] [Gil12] [Gil14]

S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative matrix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 145–162. ACM, 2012.
M.-F. F. Balcan and H. Zhang. Noise-tolerant life-long matrix completion via adaptive sampling. In Advances In Neural Information Processing Systems, 2016.
J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.
X. Chen, S. Gopi, J. Mao, and J. Schneider. Competitive analysis of the top-k ranking problem. In ACM-SIAM Symposium on Discrete Algorithms, 2017.
S. Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177–214, 2014.
Y. Chen, A. Jalali, S. Sanghavi, and C. Caramanis. Low-rank matrix recovery from errors and erasures. IEEE Trans. Information Theory, 59(7):4324–4337, 2013.
S. Chatterjee and S. Mukherjee. On estimation in tournaments and graphs under monotonicity constraints. arxiv:1603.04556, 2016.
J.-F. Cai and S. Osher. Fast singular value thresholding without singular value decomposition. UCLA CAM Report, 5, 2010.
E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math., 9(6):717–772, December 2009.
E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans. Inf. Theor., 56(5):2053–2080, May 2010.
T. Cover and J. Thomas. Elements of information theory. John Wiley & Sons, 2012.
D. Donoho, M. Gavish, et al. Minimax risk of matrix denoising by singular value thresholding. The Annals of Statistics, 42(6):2413–2440, 2014.
M. A. Davenport and J. Romberg. An overview of low-rank matrix recovery from incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016.
D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition into parts? In Advances in neural information processing systems, 2003.
N. Flammarion, C. Mao, and P. Rigollet. Optimal rates of statistical seriation. arxiv:1607.02435, 2016.
E. N. Gilbert. A comparison of signalling alphabets. Bell System Technical Journal, 31(3):504–522, 1952.
N. Gillis. Sparse and unique nonnegative matrix factorization through data preprocessing. Journal of Machine Learning Research, 13(Nov):3349–3386, 2012.
N. Gillis. The why and how of nonnegative matrix factorization. Regularization, Optimization, Kernels, and Support Vector Machines, 12(257), 2014.

35

[GPH04] [Gro11] [H˚as90] [Hit27] [HK13]
[HND15] [HSRW16]
[JKN16]
[JNS13] [KB09] [KBV09] [Klo14] [KLT11]
[KMO10] [KR+05] [Kru77]
[Lau01] [LCP+08]

C. Gobinet, E. Perrin, and R. Huez. Application of non-negative matrix factorization to ﬂuorescence spectroscopy. In European Signal Processing Conference, 2004.
D. Gross. Recovering low-rank matrices from few coeﬃcients in any basis. IEEE Trans. Information Theory, 57(3):1548–1566, 2011.
J. H˚astad. Tensor rank is np-complete. Journal of Algorithms, 11(4):644–654, 1990.
F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Studies in Applied Mathematics, 6(1-4):164–189, 1927.
D. Hsu and S. M. Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In ACM Innovations in Theoretical Computer Science, 2013.
C.-J. Hsieh, N. Natarajan, and I. Dhillon. Pu learning for matrix completion. In International Conference on Machine Learning, pages 2445–2453, 2015.
R. Heckel, N. B. Shah, K. Ramchandran, and M. J. Wainwright. Active ranking from pairwise comparisons and when parametric assumptions don’t help. arxiv:1606.08842, 2016.
C. Jin, S. M. Kakade, and P. Netrapalli. Provable eﬃcient online matrix completion via non-convex stochastic gradient descent. In Advances in Neural Information Processing Systems, 2016.
P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In ACM symposium on Theory of computing, 2013.
T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51(3):455–500, 2009.
Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8), 2009.
O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 20(1):282–303, 2014.
V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, pages 2302–2329, 2011.
R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11(Jul):2057–2078, 2010.
T. Klein, E. Rio, et al. Concentration around the mean for maxima of empirical processes. The Annals of Probability, 33(3):1060–1077, 2005.
J. B. Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications, 18(2):95–138, 1977.
M. Laurent. Matrix completion problems. In The Encyclopedia of Optimization, pages 221—229. Kluwer Academic, 2001.
H. Laurberg, M. G. Christensen, M. D. Plumbley, L. K. Hansen, and S. H. Jensen. Theorems on positive data: On the uniqueness of NMF. Computational intelligence and neuroscience, 2008, 2008.

36

[LS99]

D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791, 1999.

[NW12]

S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May):1665–1697, 2012.

[PWC16]

A. Pananjady, M. J. Wainwright, and T. A. Courtade. Linear regression with an unknown permutation: Statistical and computational limits. arxiv:1608.02902, 2016.

[Rec11]

B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(Dec):3413–3430, 2011.

[SAJ05]

N. Srebro, N. Alon, and T. S. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Advances In Neural Information Processing Systems, 2005.

[SBGW17]

N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. IEEE Transactions on Information Theory, 2017.

[SBW16a]

N. B. Shah, S. Balakrishnan, and M. J. Wainwright. Feeling the Bern: Adaptive estimators for Bernoulli probabilities of pairwise comparisons. arxiv:1603.06881, 2016.

[SBW16b]

N. B. Shah, S. Balakrishnan, and M. J. Wainwright. A permutation-based model for crowd labeling: Optimal estimation and robustness. arxiv:1606.09632, 2016.

[Sch68]

P. H. Scho¨nemann. On two-sided orthogonal procrustes problems. Psychometrika, 33(1):19–33, 1968.

[Sha17]

N. Shah. Learning From People. PhD thesis, EECS Department, University of California, Berkeley, Jul 2017.

[SK11]

R. Sarver and A. Klapuri. Application of nonnegative matrix factorization to signaladaptive audio eﬀects. In Proc. DAFx, pages 249–252, 2011.

[SW15]

N. B. Shah and M. J. Wainwright. Simple, robust and optimal ranking from pairwise comparisons. arxiv:1512.08949, 2015.

[Tao12]

T. Tao. Topics in random matrix theory, volume 132. American Mathematical Society Providence, RI, 2012.

[TST05]

F. J. Theis, K. Stadlthanner, and T. Tanaka. First results on uniqueness of sparse non-negative matrix factorization. In European Signal Processing Conference, 2005.

[Var57]

R. Varshamov. Estimate of the number of signals in error correcting codes. In Dokl. Akad. Nauk SSSR, 1957.

[VHVVD08] A. Vandendorpe, N.-D. Ho, S. Vanduﬀel, and P. Van Dooren. On the parameterization of the creditrisk+ model for estimating credit portfolio risk. Insurance: Mathematics and Economics, 42(2):736–745, 2008.

[YLP15]

S.-Y. Yun, M. Lelarge, and A. Proutiere. Streaming, memory limited matrix completion with noise. arxiv:1504.03156, 2015.

37

