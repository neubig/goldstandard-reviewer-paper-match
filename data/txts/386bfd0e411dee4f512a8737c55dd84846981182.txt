TABBIE: Pretrained Representations of Tabular Data
Hiroshi Iida† Dung Thai‡ Varun Manjunatha§ Mohit Iyyer‡ †Sony Corporation ‡UMass Amherst §Adobe Research
hiroshi.iida@sony.com {dthai,miyyer}@cs.umass.edu
vmanjuna@adobe.com

arXiv:2105.02584v1 [cs.CL] 6 May 2021

Abstract
Existing work on tabular representationlearning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of tablebased prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model’s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.
1 Introduction
Large-scale self-supervised pretraining has substantially advanced the state-of-the-art in natural language processing (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019). More recently, these pretraining methods have been extended to jointly learn representations of tables as well as text (Herzig et al., 2020; Yin et al., 2020), which enables improved modeling of tasks such as question answering over tables. However, many practical problems involve semantic understanding of tabular data without additional text-based input, such as extracting tables from documents, retrieving similar columns or cells, and ﬁlling in missing information (Zhang and Balog, 2020). In this work, we design a pretraining methodology speciﬁcally for tables (Tabular Information Embedding or TABBIE) that resembles several core tasks in table extraction and decomposition pipelines and

step 1: corrupt 15% of cells

Size Medals France 3.6

Italy

5

Spain

4

TABBIE

step 3: train TABBIE to identify the corrupted cells

corrupt! real

real corrupt!

step 2: embed the table with TABBIE

real

real

real

real

Figure 1: TABBIE is a table embedding model trained to detect corrupted cells, inspired by the ELECTRA (Clark et al., 2020) objective function. This simple pretraining objective results in powerful embeddings of cells, columns, and rows, and it yields stateof-the-art results on downstream table-based tasks.

allows easy access to representations for different tabular substructures (cells, rows, and columns).
Existing table representation models such as TaBERT (Yin et al., 2020) and TaPas (Herzig et al., 2020) concatenate tabular data with an associated piece of text and then use BERT’s masked language modeling objective for pretraining. These approaches are computationally expensive due to the long sequences that arise from concatenating text with linearized tables, which necessitates truncating the input sequences1 to make training feasible. We show that TaBERT underperforms on downstream table-based applications that operate independent of external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to investigate an approach that preserves the full table during pretraining.
Our TABBIE architecture relies on two Transformers that independently encode rows and columns, respectively; their representations are pooled at each layer. This setup reduces the sequence length of each Transformer’s input, which cuts down on its complexity, while also allowing us
1 Herzig et al. (2020) use a ﬁxed limit of 128 tokens for both text and table, while Yin et al. (2020) drop all but three rows of the table during pretraining.

to easily extract representations of cells, rows, and columns. Additionally, TABBIE uses a simpliﬁed training objective compared to masked language modeling: instead of predicting masked cells, we repurpose ELECTRA’s objective function (Clark et al., 2020) for tabular pretraining by asking the model to predict whether or not each cell in a table is real or corrupted. We emphasize that this pretraining objective is a fundamental task in table structure decomposition pipelines (Nishida et al., 2017; Tensmeyer et al., 2019; Raja et al., 2020), in which incorrectly predicting row/column separators or cell boundaries leads to corrupted cell text. Unlike Clark et al. (2020), we do not require a separate “generator” model that produces corrupted candidates, as we observe that simple corruption processes (e.g., sampling cells from other tables, swapping cells within the same column) yield powerful representations after pretraining.
In a controlled comparison to TaBERT (pretraining on the same number of tables and using a similarly-sized model), we evaluate TABBIE on three table-based benchmarks: column population, row population, and column type prediction. On most conﬁgurations of these tasks, TABBIE achieves state-of-the-art performance, outperforming TaBERT and other baselines, while in others it performs competitively with TaBERT. Additionally, TABBIE was trained on 8 V100 GPUs in just over a week, compared to the 128 V100 GPUs used to train TaBERT in six days. A qualitative nearest-neighbor analysis of embeddings derived from TABBIE conﬁrms that it encodes complex semantic properties about textual and numeric cells and substructures. We release our pretrained models and code to support further advances on tablebased tasks.2
2 Model
TABBIE is a self-supervised pretraining approach trained exclusively on tables, unlike prior approaches (Herzig et al., 2020; Yin et al., 2020) that jointly model tables and associated text snippets. At a high level, TABBIE encodes each cell of a table using two different Transformer models (Vaswani et al., 2017), one operating across the rows of the table and the other across columns. At each layer, the representations from the row and column Transformers are averaged and then passed as input to the next layer, which produces a contextualized
2https://github.com/SFIG611/tabbie

representation of each cell within the table. We place a binary classiﬁer over TABBIE’s ﬁnal-layer cell representations to predict whether or not it has been corrupted, or replaced by an intruder cell during preprocessing, inspired by the ELECTRA objective of Clark et al. (2020). In the remainder of this section, we formalize both TABBIE’s model architecture and pretraining objective.
2.1 Model Architecture
TABBIE takes an M ×N table as input and produces embeddings xij for each cell (where i and j are row and column indices, respectively), as well as embeddings for individual columns cj and rows ri.
Initialization: We begin by initializing the cell embeddings xij using a pretrained BERT model (Devlin et al., 2018).3 Speciﬁcally, for each cell (i, j), we feed its contents into BERT and extract the 768-d [CLS] token representation. This step allows us to leverage the powerful semantic text encoder of BERT to compute representations of cells out-of-context, which is important because many tables contain cells with long-form text (e.g., Notes columns). Additionally, BERT has been shown to encode some degree of numeracy (Wallace et al., 2019), which helps represent cells with numerical content. We keep this BERT encoder ﬁxed during training to reduce computational expense. Finally, we add learned positional embeddings to each of the [CLS] vectors to form the initialization of xij. More speciﬁcally, we have two sets of positional embeddings, p(ir) ∈ RH and p(jc) ∈ RH , which model the position of rows and columns, respectively, and are randomly initialized and ﬁne-tuned via TABBIE’s self-supervised objective.
Contextualizing the cell embeddings: The cell embeddings we get from BERT are uncontextualized: they are computed in isolation of all of the other cells in the table. While methods such as TaBERT and TaPaS contextualize cell embeddings by linearizing the table into a single long sequence, we take a different and more computationally manageable approach. We deﬁne a row Transformer, which encodes cells across each row of the table, as well as a column Transformer, which does the same across columns.
Concretely, assume row i contains cell embeddings xi,1, xi,2, . . . , xi,N . We pass this se-
3We use the BERT-base-uncased model in all experiments.

Step 1: compute column and row embeddings using two separate Transformers
Row Transformer
[CLSCOL] [CLSCOL] [CLSCOL]

[CLSROW] Rank

Country Medals

Step 2: compute contextualized cell embeddings by averaging row/col embeddings

France

avg (

) =

Column Transformer

[CLSROW]

1

France

9

[CLSROW]

2

Italy

5

[CLSROW]

3

Spain

4

Step 3: feed these contextualized cell embeddings as input to the next layer
x12 layers

[CLSCOL]

[CLSCOL]

[CLSCOL]

[CLSROW]

Rank

Country

Medals

[CLSROW]

1

France

9

[CLSROW]

2

Italy

5

[CLSROW]

3

Spain

4

Figure 2: TABBIE’s computations at one layer. For a given table, the row Transformer contextualizes the representations of the cells in each row, while the column Transformer similarly contextualizes cells in each column. The ﬁnal cell representation is an average of the row and column embeddings, which is passed as input to the next layer. [CLS] tokens are prepended to each row and column to facilitate downstream tasks operating on table substructures.

quence of embeddings into a row Transformer block, which uses self-attention to produce contextualized output representations ri,1, ri,2, . . . , ri,N . Similarly, assume column j contains cell embeddings x1,j, x2,j, . . . , xM,j; the column Transformer produces contextualized representations c1,j, c2,j, . . . , cM,j. After running the two Transformers over all rows and columns, respectively, each cell (i, j) of a table is associated with a row embedding ri,j as well as a column embedding ci,j .
The ﬁnal step of cell contextualization is to compose the row and column embeddings together before feeding the result to the next layer. Intuitively, if we do not aggregate the two sets of embeddings together, subsequent layers of the model will only have access to information from a speciﬁc row or column, which prevents contextualization across the whole table. We implement this aggregation through simple averaging: speciﬁcally, at layer L of TABBIE, we compute cell embeddings as:

L+1 rLi,j + cLi,j

xi,j =

2

(1)

The new cell representations xLi,j+1 are then fed to the row and column Transformers at the next
layer L + 1.

Extracting representations of an entire row or column: The row and column Transformers deﬁned above produce separate representations for every cell in a particular row or column. However,

many table-related downstream tasks (e.g., retrieve similar columns from a huge dataset of tables to some query column) can beneﬁt from embeddings that capture the contents of an entire row or column. To enable this functionality in TABBIE, we simply prepend [CLSROW] and [CLSCOL] tokens to the beginning of each row and column in an input table as a preprocessing step. After pretraining, we can extract the ﬁnal-layer cell representations of these [CLS] tokens to use in downstream tasks.
2.2 Pretraining
Having described TABBIE’s model architecture, we turn now to its training objective. We adapt the selfsupervised ELECTRA objective proposed by Clark et al. (2020) for text representation learning, which places a binary classiﬁer over each word in a piece of text and asks if the word either is part of the original text or has been corrupted. While this objective was originally motivated as enabling more efﬁcient training compared to BERT’s masked language modeling objective, it is especially suited for tabular data, as corrupt cell detection is actually a fundamental task in table structure decomposition pipelines such as (Nishida et al., 2017; Tensmeyer et al., 2019; Raja et al., 2020), in which incorrectly predicted row/column separators or cell boundaries can lead to corrupted cell text.
In our extension of ELECTRA to tables, a binary classiﬁer takes a ﬁnal-layer cell embedding as input to decide whether it has been corrupted. More concretely, for cell (i, j), we compute the

corruption probability as

Pcorrupt(celli,j ) = σ(w xLi,j )

(2)

where L indexes TABBIE’s ﬁnal layer, σ is the sigmoid function, and w is a weight vector of the same dimensionality as the cell embedding. Our ﬁnal loss function is the binary cross entropy loss of this classiﬁer averaged across all cells in the table.

2.3 Cell corruption process
Our formulation diverges from Clark et al. (2020) in how the corrupted cells are generated. In ELECTRA, a separate generator model is trained with BERT’s masked language modeling objective to produce candidate corrupted tokens: for instance, given Jane went to the [MASK] to check on her experiments, the generator model might produce corrupted candidates such as lab or ofﬁce. Simpler corruption strategies, such as randomly sampling words from the vocabulary, cannot induce powerful representations of text because local syntactic and semantic patterns are usually sufﬁcient to detect obvious corruptions. For tabular data, however, we show that simple corruption strategies (Figure 3) that take advantage of the intra-table structure actually do yield powerful representations without the need of a separate generator network. More specifically, we use two different corruption strategies:

• Frequency-based cell sampling: Our ﬁrst strategy simply samples corrupt candidates from the training cell frequency distribution (i.e., more commonly-occurring cells are sampled more often than rare cells). One drawback of this method is that oftentimes it can result in samples that violate a particular column type (for instance, sampling a textual cell as a replacement for a cell in a numeric column). Despite its limitations, our analysis in Section 4 shows that this strategy alone results in strong performance on most downstream table-based tasks, although it does not result in a rich semantic understanding of intra-table semantics.

• Intra-table cell swapping: To encourage the model to learn ﬁne-grained distinctions between topically-similar data, our second strategy produces corrupted candidates by swapping two cells in the same table (Figure 3c, d). This task is more challenging than the

(a) original table

Rank Country

1 France

2

Italy

3

Spain

Gold 9 5 4

(b) sample cells from other tables

Rank Size Gold

1 France 3.6

2

Italy

5

3

Spain

4

(c) swap cells on the same row (d) swap cells on the same column

Rank 1 2 3

Country France
5 Spain

Gold 9
Italy 4

Rank 1 3 2

Country France
Italy Spain

Gold 9 5 4

Figure 3: The different cell corruption strategies used in our experiments.

frequency-based sampling strategy above, especially when the swapped cells occur within the same column. While it underperforms frequency-based sampling on downstream tasks, it qualitatively results in more semantic similarity among nearest neighbors of column and row embeddings.

2.4 Pretraining details
Data: We aim for as controlled of a comparison with TaBERT (Yin et al., 2020) as possible, as its performance on table QA tasks indicate the strength of its table encoder. TaBERT’s pretraining data was not publicly released at the time of our work, but their dataset consists of 26.6M tables from Wikipedia and the Common Crawl. We thus form a pretraining dataset of equivalent size by combining 1.8M Wikipedia tables with 24.8M preprocessed Common Crawl tables from Viznet (Hu et al., 2019).4
Experimental settings: We train TABBIE for seven epochs for just over a week on 8 V100 GPUs using mixed precision. TABBIE has 12 layers and a hidden dimensionality of 768 for both row and column Transformers, in an effort to be comparablysized to the TaBERT-Base model.5 Before computing the initial cell embeddings using BERT, we truncate each cell’s contents to the ﬁrst 300 characters, as some cells contain huge amounts of text. We also truncate tables to 30 rows and 20 columns to avoid memory issues (note that this is much larger than the three rows used by TaBERT), and
4The vast majority of text in these tables is in English. 5TABBIE is slightly larger than TaBERT-Base (170M to 133M parameters) because its row and column Transformers are the same size, while TaBERT places a smaller “vertical” Transformer over the output of a ﬁne-tuned BERT model.

predict Silver, Bronze, …

(a) col mn pop la ion

(b) ro pop la ion

predict 4, 5, …

, a _di /['C20LMS]238e[C_4LKS'] [CLS]

, a _di /['C20LMS]238e[C_4LKS'] [CLS]

[CLS] Rank C n G ld

[CLS] Rank C n G ld

[CLS] 1 F ance 9

[CLS] 1 F ance 9

[CLS] 2

I al

5

[CLS] 2

I al

5

[CLS] 3

S ain

4

[CLS] 3

S ain

4

(c) corr p ed cell classi ca ion

, a _di /['C20LMS]238e[C_4LKS'] [CLS]

[CLS] Rank C n G ld

[CLS] 1 F ance 9

[CLS] 2

I al 1259.2

[CLS] 3

S ain

4

(d) col mn pe predic ion predict
country , a _di /['C20LMS]238e[C_4LKS'] [CLS]

[CLS]

[CLS] 1 F ance 9

[CLS] 2

I al

5

[CLS] 3

S ain

4

predict corrupt

Figure 4: The inputs and outputs for each of our tablebased prediction tasks. Column type prediction does not include headers as part of the table.

our maximum batch size is set at 4,800 cells (on average, 104 tables per batch). We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5.
We compared two pretrained models trained with different cell corruption strategy for downstream tasks. The ﬁrst strategy (FREQ) uses exclusively a frequency-based cell sampling. The second strategy is a 50/50 mixture (MIX) of frequencybased sampling and intra-table cell swapping, where we additionally specify that half of the intratable swaps must come from the same row or column to make the objective more challenging.
3 Experiments
We validate TABBIE’s table representation quality through its performance on three downstream tablecentric benchmarks (column population, row population, and column type prediction) that measure semantic table understanding. In most conﬁgurations of these tasks, TABBIE outperforms TaBERT and other baselines to set new state-of-the-art numbers. Note that we do not investigate TABBIE’s performance on table-and-text tasks such as WikiTableQuestions (Pasupat and Liang, 2015), as our focus is not on integrating TABBIE into complex taskspeciﬁc pipelines (Liang et al., 2018), although this is an interesting avenue for future work.
3.1 Fine-tuning TABBIE
In all of our downstream experiments, we apply essentially the same ﬁne-tuning strategy to both TABBIE and TaBERT: we select a subset of its ﬁnallayer representations (i.e., cell or column representations) that correspond to the tabular substruc-

Task
Column population Row population Col. type prediction

Batch size
12 48 12

LR
1e-05 2e-05 2e-05

Max epochs
20 30 15

Table 1: Fine-tuning hyperparameters of each downstream task for TABBIE and TaBERT.

tures used in the downstream task, and we place a classiﬁer over these representations to predict the training labels. We select task-speciﬁc hyperparameters based on the size of each dataset (full details in Table 1) and report the test performance of the best-performing validation checkpoint. For both models, we backpropagate the downstream error signal into all of the model’s parameters (i.e., we do not “freeze” our pretrained model).
3.2 Column Population
In the column population task, which is useful for attribute discovery, tabular data augmentation, and table retrieval (Das Sarma et al., 2012), a model is given the ﬁrst N columns of a “seed” table and asked to predict the remaining column headers. Zhang and Balog (2017) compile a dataset for this task comprising 1.6M tables from Wikipedia with a test set of 1,000 tables, formulated as a multi-label classiﬁcation task with 127,656 possible header labels. Importantly, we remove all of the tables in the column population test set from our pretraining data to avoid inﬂating our results in case TABBIE memorizes the missing columns during pretraining.6
To ﬁne-tune TABBIE on this task, we ﬁrst concatenate the column [CLSCOL] embeddings of the seed table into a single vector and pass it through a single linear and softmax layer, training with a multi-label classiﬁcation objective (Mahajan et al., 2018). Our baselines include the generative probabilistic model (GPM) of Zhang and Balog (2017) as well as a word embedding-based extension called Table2VecH (TH) devised by Deng et al. (2019). As ﬁne-tuning on the full dataset is extremely expensive for TABBIE and TaBERT, we ﬁne-tune on a random subset of 100K training examples; as a further disadvantage to these, we do not use table captions (unlike GPM and GPM+TH) during training. Nevertheless, as Table 2 shows, TABBIE and TaBERT substantially outperform both
6Note that TaBERT’s pretraining data likely includes the test set tables, which may give it an advantage in our comparisons.

N

Method

MAP MRR Ndcg-10 Ndcg-20

GPM

25.1 37.5

-

-

1 GPM+TH 25.5 0.38.0 27.1 31.5

TaBERT

33.1 41.3 35.1

38.1

TABBIE (FREQ) 37.9 49.1 41.2

43.8

TABBIE (MIX) 37.1 48.7 40.4

43.1

GPM

28.5 40.4

-

-

2 GPM+TH 33.2 44.0 36.1 41.3

TaBERT

51.1 60.1 54.7

56.6

TABBIE (FREQ) 52.0 62.8 55.8

57.6

TABBIE (MIX) 51.7 62.3 55.6

57.2

GPM

28.5 35.5

-

-

3 GPM+TH 40.0 50.8 45.2 48.5

TaBERT

53.3 60.9 56.9

57.9

TABBIE (FREQ) 54.5 63.3 57.9

58.9

TABBIE (MIX) 54.1 62.3 57.4

58.7

Table 2: TABBIE outperforms all methods on the column population task, with the biggest improvement coming with just a single seed column (N = 1). Despite its simplicity, the FREQ corruption strategy yields better results than MIX.

baselines, and TABBIE consistently outperforms TaBERT regardless of how many seed columns are provided, especially with only one seed column. This result indicates that TABBIE encodes more semantics about headers and columns than TaBERT.
3.3 Row Population
The row population task is more challenging than column population: given the ﬁrst N rows of a table in which the ﬁrst column contains entities (e.g., “Country”), models must predict the remaining entries of the ﬁrst column. Making reasonable predictions of which entities best ﬁll the column requires understanding the full context of the seed table. The Zhang and Balog (2017) dataset also contains a split for row population, which we use to evaluate our models. Again, since the dataset is too large for our large embedding models, we sample a subset of tables for ﬁne-tuning.7 Our label space consists of 300K entities that occur at least twice in Wikipedia tables, and we again formulate this problem as multi-label classiﬁcation, this time on top of the ﬁrst column’s [CLSCOL] representation.8
On this task, TaBERT and TABBIE again outperform the baseline Entitables model (which uses external information in the form of table cap-
7We sample all tables that have at least ﬁve entries in the left-most column, which results in roughly 200K tables.
8Due to the large number of labels, we resort to negative sampling during training instead of the full softmax to cut down on ﬁne-tuning time. Negative samples are formed by uniform random sampling on the label space.

N

Method

MAP MRR Ndcg-10 Ndcg-20

Entitables

36.8 45.2

-

-

1 TaBERT 43.2 55.7 45.6 47.7

TABBIE (FREQ) 42.8 54.2 44.8

46.9

TABBIE (MIX) 42.6 54.7 45.1

46.8

Entitables

37.2 45.1

-

-

2 TaBERT 43.8 56.0 46.4 48.8

TABBIE (FREQ) 44.4 57.2 47.1

49.5

TABBIE (MIX) 43.7 55.7 46.2

48.6

Entitables

37.1 44.6

-

-

3 TaBERT 42.9 55.1 45.6 48.5

TABBIE (FREQ) 43.4 56.5 46.6

49.0

TABBIE (MIX) 42.9 55.5 45.9

48.3

Table 3: TABBIE outperforms baselines on row population when provided with more seed rows N , although TaBERT is superior given just a single seed row. Again, the FREQ strategy produces better results than MIX.

tions). When given only one seed row, TaBERT slightly outperforms TABBIE, but with more seed rows, TABBIE exhibits small improvements over TaBERT.
3.4 Column Type Prediction
While the prior two tasks involve predicting missing elements of a table, the column type prediction task involves predicting a high-level type of a particular column (e.g., name, age, etc.) without access to its header. This task is useful when indexing tables with missing column names, which happens relatively often in practice, or for schema matching(Hulsebos et al., 2019; Rahm and Bernstein, 2001), and like the other tasks, requires understanding the surrounding context. We evaluate our models on the same subset of VizNet Web Tables (Hu et al., 2019)9 created by Zhang et al. (2019) to evaluate their column type predictor, SATO10. They formulate this task as a multi-class classiﬁcation problem (with 78 classes), with a training set of 64,000 tables and a test set consisting of 16,000 tables. We set aside 6,400 training tables to form a validation for both TABBIE and TaBERT, and we ﬁne-tune each of these models with small random subsets of the training data (1000 and 10000 labeled tables) in addition to the full training set to evaluate their performance in a simulated lowresource setting.
Along with TaBERT, we compare with two recently-proposed column type prediction meth-
9Again, we ensure that none of the test tables in this dataset occur in TABBIE’s pretraining data.
10https://github.com/megagonlabs/sato

Method
Sherlock SATO TaBERT TABBIE (FREQ) TABBIE (MIX)

n=1000
84.7 84.7 84.1

n=10000
93.5 94.2 93.8

n=all
86.7 90.8 97.2 96.9 96.7

Table 4: Support-weighted F1-score of different models on column type prediction. TaBERT and TABBIE perform similarly in low resource settings (n=1000) and when the full training data is used (n=all).

ods: Sherlock (Hulsebos et al., 2019), which uses a multi-input neural network with hand-crafted features extracted from each column, and the aforementioned SATO (Zhang et al., 2019), which improves Sherlock by incorporating table context, topic model outputs, and label co-occurrence information. Table 4 shows the support-weighted F1score for each method. Similar to the previous two tasks, TABBIE and TaBERT signiﬁcantly outperform the prior state-of-the-art (SATO). Here, there are no clear differences between the two models, but both reach higher F1 scores than the other baselines even when given only 1,000 training examples, which demonstrates the power of table-based pretraining.
4 Analysis
The results in the previous section show that TABBIE is a powerful table representation method, outperforming TaBERT in many downstream task conﬁgurations and remaining competitive in the rest. In this section, we dig deeper into TABBIE’s representations by comparing them to TaBERT across a variety of quantitative and qualitative analysis tasks, including our own pretraining task of corrupt cell classiﬁcation, as well as embedding clustering and nearest neighbors. Taken as a whole, the analysis suggests that TABBIE is able to better capture ﬁne-grained table semantics.
4.1 Corrupt Cell Detection
We ﬁrst examine how TaBERT performs on TABBIE’s pretraining task of corrupt cell detection, which again is practically useful as a postprocessing step after table structure decomposition (Tensmeyer et al., 2019; Raja et al., 2020) because mistakes in predicting row/column/cell boundaries (sometimes compounded by OCR errors) can lead to inaccurate extraction. We ﬁne-tune TaBERT on 100K tables using the MIX corruption strategy for

Corruption Intra-row swap Intra-column swap Intra-table swap Random FREQ cell All

Method

Prec. Rec. F1

TaBERT

85.5 83.0 84.2

TABBIE (FREQ) 99.0 81.4 89.4

TABBIE (MIX) 99.6 95.8 97.7

TaBERT

31.2 19.0 23.7

TABBIE (FREQ) 90.9 22.3 35.8

TABBIE (MIX) 91.5 55.0 68.8

TaBERT

81.2 69.5 74.9

TABBIE (FREQ) 98.2 73.3 84.0

TABBIE (MIX) 98.4 86.2 91.9

TaBERT

86.7 87.0 86.8

TABBIE (FREQ) 99.3 98.2 98.8

TABBIE (MIX) 99.1 98.1 98.6

TaBERT

75.6 65.2 70.0

TABBIE (FREQ) 98.2 69.5 81.4

TABBIE (MIX) 97.8 84.1 90.5

Table 5: A ﬁne-grained comparison of different models on corrupt cell detection, with different types of corruption. TaBERT struggles on this task, especially in the challenging setting of intra-column swaps. Unlike our downstream tasks, the MIX strategy is far superior to FREQ here.

ten epochs, and construct a test set of 10K tables that are unseen by both TaBERT and TABBIE during pretraining. While TABBIE of course sees an order of magnitude more tables for this task during pretraining, this is still a useful experiment to determine if TaBERT’s pretraining objective enables it to easily detect corrupted cells.
As shown in Table 5, TaBERT performs signiﬁcantly worse than TABBIE on all types of corrupt cells (both random corruption and intra-table swaps). Additionally, intra-column swaps are the most difﬁcult for both models, as TABBIE achieves a 68.8 F1 on this subset compared to just 23.7 F1 by TaBERT. Interestingly, while the MIX strategy consistently performs worse than FREQ for the TABBIE models evaluated on the three downstream tasks in the previous section, it is substantially better at detecting more challenging corruptions, and is almost equivalent to detecting random cells sampled by FREQ. This result indicates that perhaps more complex table-based tasks are required to take advantage of representations derived using MIX corruption.
4.2 Nearest neighbors
We now turn to a qualitative analysis of the representations learned by TABBIE. In Figure 6 (top), we display the two nearest neighbor columns from our validation set to the date column marked by the red box. TABBIE is able to model the similarity of feb.

(a) Input table

# name

year

15 allysia junior

18 maria senior

17 emily sophomore

16 hydn sophomore

19 hayley sophomore

20 michelle graduate

(b) TABBIE (MIX)

#

name year

0.0% 0.1% 0.0%

100% 0.0% 0.0%

0.0% 0.0% 0.0%

99.9% 0.0% 0.0%

0.0% 0.0% 0.0%

0.0% 0.0% 0.0%

(c) TaBERT

#

name year

2.6% 1.6% 8.9%

3.2% 2.6% 1.9%

4.3% 7.6% 5.2%

2.2% 0.3% 0.5%

3.3% 3.3% 1.5%

4.0% 6.6% 2.9%

Figure 5: In this ﬁgure, (b) and (c) contain the predicted corruption probability of each cell in (a). Only TABBIE MIX is able to reliably identify violations of numerical trends in columns.

16 and saturday. february 5th despite the formatting difference, while TaBERT’s neighbors more closely resemble the original column. Figure 6 (bottom) shows that TABBIE’s nearest neighbors are less reliant on matching headers than TaBERT, as the neighbors all have different headers (nom, nombre, name).
4.3 Clustering
Are the embeddings produced by TABBIE useful for clustering and data discovery? To ﬁnd out, we perform clustering experiments on the FinTabNet dataset from Zheng et al. (2021). This dataset contains ∼110K tables from ﬁnancial reports of corporations in the S&P-500. We use the [CLS] embedding at the (0, 0) position (i.e., the top left-most cell in the table), extracted from a TABBIE model trained with the FREQ strategy, as a representative embedding for each table in the dataset. Next, we perform k-means clustering on these embeddings using the FAISS library (Johnson et al., 2017), with k=1024 centroids. While the FinTabNet dataset is restricted to the homogenous domain of ﬁnancial tables, these tables cluster into sub-types such as consolidated ﬁnancial tables, jurisdiction tables, insurance tables, etc. We then examine the contents of these clusters (Figure 7) and observe that TABBIE embeddings can not only be clustered into these sub-types, but also that tables from reports of the same company, but from different ﬁnancial years, are placed into the same cluster.
4.4 Identifying numeric trends
Next, we analyze how well TABBIE understands trends in numerical columns by looking at speciﬁc examples of our corrupt cell detection task. The ﬁrst column of the table in Figure 5 contains jersey numbers sorted in ascending order. We swap two cells in this column, 16 and 18, which violates

0 date opponent

TABBIE’s top-2 columns:

site

time

1 feb. 16 northern colorado

0lodvaeteland, co 1:00pm 0 odpatpeonetnimt e

optipmoenent

2 feb. 17 colorado mesa (ncaa div. ii)
3 feb. 22 utah state*

1 saturday, february 5th 1 c1o1l.u2m0 b1u2s:0c0repw.m. ch1r0is:0ti0aanm loveland, co 1:00pm
2 wednesday, february 2 i1n1t.r2a0-sq1u2a:d00p.m. @10ro:0b0eartme. pleheoeni lo9vtheland, co 7:00pm 3 11.21 12:00p.m. @ lanier

4 feb. 23 westminster

3losvateulardnady, ,cfoebr4u:a0r0yp1m2th4 c1o1l.o2r1ad6o:0r0appi.dms. @10h:a0r0laanmdalpeh(oteexnai

5 mar. 2 vs. uc-santa barbara

4latTusaevBsedEgaaRys,T,fn’esvbtrou1ap:0r-y02p1cm5othlu5mnh1so1:u.2s2ton2:d0y0npa.mm.o @10w:e0s0tabmurypchhoreisntii

6 mar. 3 7 mar. 7 8 mar. 9 9 mar.
10 10 mar.
16
0 nom

@ unlv loyola marymount simon fraser virginia tech vs. michigan state
artiste

da5telat0usdeavstweedgaeayesk,,fnedlvboarctu2eaat:i0roy0np1mo5pthp6oneu1n.1st..2u5nd7et:i0rm-01wep8i.(nmment.)nert (@tpv1o1ssp:.0)a0ladavimngg.mphpoheni

16los1fvaetebul1a.rdn1ad8y, ,cfosdeeabpyr5tut.:oa07nr0yap52ma06t0trha7msp1o2r.0tl2an5d1:3tip0md.pmab.yme.tro.snanfoe1xw1:0h0omapmeicchhcaraeislsatwiaganrl

27los2fvaetebul2a.rdn2ad6y, ,cfosdeeubprr4tau.:a0l1ur04ybpe2mv64st.0hp0atrmiootnstrea1l ipmr.ompca.kcitnghcab1ms2:30sptmevecpaasarkg(r

8 friday, march 4th 8 a1r2i.z0o5na6s:3ah0upa.mro.s vi7rg:0in0ipamacadtuecmsoyn,

3 lo3vel3and, cosep5t.:0201pmat saints

1 p.m. fox

9 saturday, march 5th 9 n1e2w.06yo7rk:3r0epd.mbu. lls@7:m00cnpammartaucson,

4 4ma4r. 4 sueapwt.-d2a8imvlse.rfcahlrcyosnlser 4010 pl.ams.vegafsox jeff gordon

5 lis5mlea,5ri.l11

ocrcat5.ck:20e0rpbmatrrpe1al0c5k01e02rs.09

7:00p.m. westlake 8:2a5tlpa.nmta. cbs, nﬂknevin harvic

TABBIE’s top-3 column1s1: 12.12 8:30p.m. @ calvert hall

0 no6m0 6man6or.m1b8re 0aodrtcoistd.tneg1aem2dedeuvraãlscae.errtlsiisopt4anr0isx0 dau1lbraupcmd.imãanr.linpgretcofioonx timdaeleprjiacrerett (

12 12.13 5:00p.m. @ theodore roosev

1 1 no7th1ing71mcabh7rari.nvg2ide5is d1p'aofeomc1rostoo.darne1mac9oir3ty:e4ax54ptg0rbie0gislisl0ds,'9al9esâs¬ioaf4mﬁ:11ci5hpreab.rzmrãsi³us.nrtoduitlesudnsfe0ors.9x9 v3e:er0l7elnio1itt,2ut9nseaâs¬dlev

2 2duorãn©8cee2 p82rixp8rimo ap2ppoeuc2rnsto.tcan2amah6eu4na:t1oea1st,gbpi1gius3itc0dac,c'91aahl92nes.âe1s¬ei5orsaf47cﬁ:1a:0c0h6hpu0e.aprmtes.mus.,r.puitisusdtnamfe0cos.ho9xe9usnv2te:cr5a6ernm1it,2ue9nle(âsm¬ dv.

1 1 run cold

holly golightly3 3:0h1ero3in093,ﬂ9au9m9nâae¬svoalftﬁa3pncenho3resevlorl.ans2auvsruit3ait:tu2avn2seg.sigrei 0dd,'s9akl9eisâns¬sioaf3cﬁ:1a5ch5hpue.armtesus.,rpuitisusdtnafe0cos.h9xe9s v2e:r35en1it,2u9neâs¬ v

2 2 indeed you do

holly golightly4 43:1b4a9d b4lo04ao,9pdu9rn.â1c¬u1oarfeﬁ4phmceha4raserlorranatsolauhar'ls4iut:z5u1n40dege0silgais0de,'9asl9teresâls¬liaosaf4cﬁ:a4ch0huefartt.esusw,rpouitirsusdttnhae0cs.h9e9s v2e:dr3a8enle1it,j2ua9nrerâse¬ttv(

10 10 bye

3

3

i let my daddy do that

holly golightly5

52:1b8i1g0g5e1o05a1r,T9gpaa9er1Bp.âr1E8¬i RleaTbf’ﬁ5srpnvacetchoi5rocrsevpigoraﬂ.n-ison31aurir6cah0ioet:5u2rlaumn10tmeg0osbisngeasia0:dr,'s9al9esâs¬ioaf4cﬁ:1a1ch7hpuem.armtesaus.r,rtpiuintisussdtnvafe0cois.hl9xle9es

v2e:r27en1it,2u9neâs¬ v
dale jarrett (

4 4 for all this

holly golightly 4:06 0,99 â¬ afﬁcher sur itunes

5 5 painted on

0 holly golightly

4:3n1o1m1

100a2,9p9r1.ân22¬o2ma0fﬁantcrathonilesvlotraem.sd2uer3gditauuvrn5ãsec0.se0papcrkixers

1 pt.amll.adealgbfauomx

bobby hami

1 1 nothing1 c1hatnhgee1scrap1cekrtdsroinwsanpi3(ão:¨4rci4egsinbarlã0v¨,ve9er9si:âo¬anl)lea[gfﬁreoacth.elrustuyr zitbaunrnuzecisbkanre,rshtaeupsh-eendimtioalnl:inddaei

6 6 a length of pipe

holly golightly 2

22:3o4n1c2e

102a3,9p29r1.ât32h¬e9a2cfrﬁap2nccekhoartdserpvora.inwss3auanpr0ui4(iãtd:¨ou1ucvn1bepses.vsabeprrtaãss0¨in,ov95tneh90s)e:0â[r¬afsenadata.fnﬁl1utceshptey.rmzsau.nrziitburanurfec,osksxtneeprhheanusm-eadlliitniodne:rd&ai

3 3 hero in3ﬂa3mtehse 3crap3cekrtdsroinwsanpi3(ãi:¨n2cs2etrsubmrãe0¨n,v9tea9sl:âv¬aesrsaeifozﬁnlc)ehn[efter astu.rluitbsutrynueczskanezrihbaur,ss-etedpithieonn:mdaali

4 4 bad blo4od4 the 4crap4cekrsdseocnwhasnb4(ar:ag1da4itoeleledn0it:,)9a[9lfleâag¬tr.olaucfsﬁotncyhszeparinrsizutiorbiatbru,rnusecteskpnheernhamusa-lelidnidtieorn&: dmai

5 5 big geo5rg5e just5fap5sceirsnseaoctnhioasnb0(a:og2ra1itgeilnleanl0:v,9re9ursbâiao¬tno).a[lfafﬁemcahet.enlrtuosssutoyr iztbuarnuzeciskbnaer,rhsateupsh-ednitmioanll:idnadi
Figure 6: Nearest neighbors6 6ojfust tfahsceinatidona(tdeub vaernsiodn) nom [feat. lusty zanzibar, stephen mallinder & columns from the tables on t7he7 julset ffats,cinfatrioonm(instrbumoetnhtal) - T[feAat.Blusty zanzibar, stephen mallinder &

BIE and TaBERT. TABBIE’s nearest neighbors exhibit

more diverse formatting and less reliance on the header,

which is an example of its semantic representation ca-

pability.

the increasing trend. Both TaBERT (ﬁne-tuned for corrupt cell detection) and TABBIE FREQ struggle to identify this swap, while TABBIE MIX is almost certain that the two cells have been corrupted. This qualitative result is further evidence that the MIX model has potential for more complex table-based reasoning tasks.
5 Related work
The staggering amount of structured relational data in the form of tables on the Internet has attracted considerable attention from researchers over the past two decades (Cafarella et al., 2008; Limaye et al., 2010; Venetis et al., 2011; Suchanek et al., 2007; Embley et al., 2006), with applications including retrieval (Das Sarma et al., 2012), schemamatching (Madhavan et al., 2001, 2005), and entity linking (Zhang et al., 2020).
Similar to popular large-scale language models pretrained on tasks involving unstructured natural language(Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019), our work is part of a recent trend of self-supervised models trained on structured tabular data. TaBERT (Yin et al., 2020) and TaPaS (Herzig et al., 2020) jointly model tables

Semantic type Table of contents

Sample Tables

Centroid No. 23

Investment income

table for Everest Re

190

Group

Market share table

for Phillip Morris

295

International

Figure 7: Sample tables from clusters obtained by running k-means on TABBIE’s [CLS] embeddings on the FinTabNet dataset. TABBIE not only clusters embeddings into reasonable semantic types, such as Table of Contents (ﬁrst row), but it also places tables of the same type from the same company into the same cluster (second and third rows). We provide the source images of the corresponding tables in this ﬁgure.

with text (typically captions or questions), and are thus more suited for tasks like question answering (Pasupat and Liang, 2015). For pretraining, TaBERT attempts to recover the name and datatype of masked column headers (masked column prediction), in addition to contents of a particular cell (cell value recovery). The pretraining objectives of TaPaS, on the other hand, encourage tabular textual entailment. In a concurrent work, the TUTA model (Wang et al., 2020) uses masked language modeling, cell-level cloze prediction, and tablecontext retrieval as pretraining objectives. Further, in addition to traditional position embeddings, this work accounts for the hierarchical nature of tabular data using tree-based positional embeddings. Similiarly, in Deng et al. (2020), the authors perform a variant of MLM called masked entity recovery. In contrast, TABBIE is pretrained strictly on tabular data and intended for more general-purpose tablebased tasks, and uses corrupt-cell classiﬁcation as its pretraining task.
6 Conclusion
In this paper, we proposed TABBIE, a selfsupervised pretraining method for tables without associated text. To reduce the computational cost of training our model, we repurpose the ELECTRA objective for corrupt cell detection, and we use two

separate Transformers for rows and columns to minimize complexity associated with sequence length. On three downstream table-based tasks, TABBIE achieves competitive or better performance to existing methods such as TaBERT, and an analysis reveals that its representations include a deep semantic understanding of cells, rows, and columns. We publicly release our TABBIE pretrained models and code to facilitate future research on tabular representation learning.
7 Ethics Statement
As with any research work that involves training large language models, we acknowledge that our work has a negative carbon impact on the environment. A cumulative of 1344 GPU-hours of computation was performed on Tesla V100 GPUs. Total emissions are estimated to be 149.19 kg of CO2 per run of our model (in total, there were two runs). While this is a signiﬁcant amount (equivalent to ≈ 17 gallons of fuel consumed by an average motor vehicle11), it is lower than TaBERT’s cost per run by more than a factor of 10 assuming a similar computing platform was used. Estimations were conducted using the Machine Learning Impact calculator presented in Lacoste et al. (2019).
11https://www.epa.gov/greenvehicles/

Acknowledgements
We thank the anonymous reviewers for their useful comments. We thank Christopher Tensmeyer for helpful comments and pointing us to relevant datasets for some of our experiments. We also thank the UMass NLP group for feedback during the paper writing process. This work was made possible by research awards from Sony Corp. and Adobe Inc. MI is also partially supported by award IIS-1955567 from the National Science Foundation (NSF).
References
Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. Webtables: Exploring the power of tables on the web. Proc. VLDB Endow., 1(1):538–549.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pretraining text encoders as discriminators rather than generators. In International Conference on Learning Representations.
Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding related tables. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD ’12, page 817–828, New York, NY, USA. Association for Computing Machinery.
Li Deng, Shuo Zhang, and Krisztian Balog. 2019. Table2vec: Neural word and entity embeddings for table population and retrieval. In Proceedings of SIGIR 2019.
Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2020. Turl: Table understanding through representation learning. Proc. VLDB Endow., 14(3):307–319.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
D. Embley, Matthew Hurst, D. Lopresti, and G. Nagy. 2006. Table-processing paradigms: a research survey. International Journal of Document Analysis and Recognition (IJDAR), 8:66–86.
Jonathan Herzig, P. Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pretraining. In ACL.
Kevin Zeng Hu, Snehalkumar (Neil) S. Gaikwad, Madelon Hulsebos, Michiel A. Bakker, Emanuel Zgraggen, César A. Hidalgo, Tim Kraska, Guoliang

Li, Arvind Satyanarayan, and Çagatay Demiralp. 2019. Viznet: Towards A large-scale visualization learning and benchmarking repository. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019, Glasgow, Scotland, UK, May 04-09, 2019.
M. Hulsebos, K. Hu, M. Bakker, Emanuel Zgraggen, Arvind Satyanarayan, T. Kraska, cCaugatay Demiralp, and C’esar A. Hidalgo. 2019. Sherlock: A deep learning approach to semantic data type detection. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700.
Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In Proceedings of the 32nd International Conference on Neural Information Processing Systems.
Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and searching web tables using entities, types and relationships. Proc. VLDB Endow., 3(1):1338–1347.
Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.
Jayant Madhavan, Philip A. Bernstein, AnHai Doan, and Alon Halevy. 2005. Corpus-based schema matching. In Proceedings of the 21st International Conference on Data Engineering, ICDE ’05, page 57–68, USA. IEEE Computer Society.
Jayant Madhavan, Philip A. Bernstein, and Erhard Rahm. 2001. Generic schema matching with cupid. In Proceedings of the 27th International Conference on Very Large Data Bases, VLDB ’01, page 49–58, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
D. Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Y. Li, Ashwin Bharambe, and L. V. D. Maaten. 2018. Exploring the limits of weakly supervised pretraining. In ECCV.

Kyosuke Nishida, Kugatsu Sadamitsu, Ryuichiro Higashinaka, and Yoshihiro Matsuo. 2017. Understanding the semantic structures of tables with a hybrid deep neural network architecture. In ThirtyFirst AAAI Conference on Artiﬁcial Intelligence.
Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Association for Computational Linguistics.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.
Erhard Rahm and Philip A. Bernstein. 2001. A survey of approaches to automatic schema matching. VLDB J., 10(4):334–350.
Sachin Raja, Ajoy Mondal, and C. V. Jawahar. 2020. Table structure recognition using top-down and bottom-up cues. In Computer Vision – ECCV 2020, pages 70–86, Cham. Springer International Publishing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web.
C. Tensmeyer, V. I. Morariu, B. Price, S. Cohen, and T. Martinez. 2019. Deep splitting and merging for table structure decomposition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 114–121.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ¥L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.
Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Pas¸ca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. 2011. Recovering semantics of tables on the web. Proc. VLDB Endow., 4(9):528–538.
Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Empirical Methods in Natural Language Processing.
Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. 2020. Structureaware pre-training for table understanding with treebased transformers. ArXiv, abs/2010.12537.
Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for joint understanding of textual and tabular data. In Annual Conference of the Association for Computational Linguistics (ACL).

Dan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon Hulsebos, Çag˘atay Demiralp, and Wang-Chiew Tan. 2019. Sato: Contextual semantic type detection in tables.
Shuo Zhang and Krisztian Balog. 2017. Entitables: Smart assistance for entity-focused tables. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.
Shuo Zhang and Krisztian Balog. 2020. Web table extraction, retrieval, and augmentation: A survey. ACM Trans. Intell. Syst. Technol.
Shuo Zhang, Edgar Meij, Krisztian Balog, and Ridho Reinanda. 2020. Novel entity discovery from web tables. In Proceedings of The Web Conference 2020, WWW ’20, page 1298–1308, New York, NY, USA. Association for Computing Machinery.
Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. 2021. Global table extractor (gte): A framework for joint table identiﬁcation and cell structure recognition using visual context. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 697–706.

