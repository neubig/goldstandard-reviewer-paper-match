Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders

Andrew Drozdov1,∗ Pat Verga1,∗

Mohit Yadav1,∗ Mohit Iyyer1 Andrew McCallum1

1College of Information and Computer Sciences University of Massachusetts Amherst

{adrozdov, pat, ymohit, miyyer, mccallum}@cs.umass.edu

arXiv:1904.02142v2 [cs.CL] 4 Apr 2019

Abstract
We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.
1 Introduction
Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classiﬁcation (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain speciﬁc treebanks with syntactic annotations is expensive and timeconsuming.
Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (Peters et al., 2018a; Devlin et al.,
∗Equal contribution, randomly ordered.

Under the current circumstances he says their scenario no longer seems unrealistic
Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth.
2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reﬂect syntactic and semantic regularities which can be leveraged by downstream tasks.
Our model builds on existing work developing latent tree chart parsers (Socher et al., 2011b; Le and Zuidema, 2015; Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting over all possible sub-trees (§2). Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference.
To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without requiring any supervised training data. DIORA incorporates the inside-outside algorithm (Baker, 1979; Lari and Young, 1990) into a latent tree chart parser. The bottom-up inside step calculates a representation for all possible constituents within a binary tree over the input sentence. This step is equivalent to the forward-pass of previous latent tree chart parsers (Maillard et al., 2017). These inside representations only encode the current subtree, ignor-

0.7 ⋅ + 0.3 ⋅

a¯(k) (a) Inside Pass

e(i, j) a(i, j)

e(i, j) a(i, j)

i0 j0
The cat drank

j1 i1
The cat drank The cat drank

(b) Outside Pass b(i, j)
b¯ (k) j0
i0
The cat drank

Figure 2: The inside and outside pass of DIORA for the input ‘the cat drank’. a) The inside pass: The blue inside vector a¯(k) for the phrase ‘the cat drank’ is a weighted average of the compositions for the two possible
segmentations - ((the cat), drank) and (the, (cat drank)). The scalar weights come from a learned compatibility function. b) The outside pass: The red outside vector ¯b(k) for the phrase ‘the cat’ is a function of the outside vector
of its parent ‘the cat drank’ and the inside vector of its sibling ‘drank’.

ing all outside context. Thus, we perform an additional top-down outside calculation for each node in the tree, providing external context into the subtree representations in each chart cell. The model is then trained with the objective that the outside representations of the leaf cells should reconstruct the corresponding leaf input word, analogous to masked language model (Devlin et al., 2019) pretraining, except by using dynamic programming we predict every word from a completely unmasked context. The single most likely tree can be recovered using the CKY algorithm and compatibility scores between constituents. Previous work either predict trees that are not well aligned with known treebanks (Yogatama et al., 2017; Choi et al., 2018), or has no mechanism for explicitly modeling phrases, requiring a complex procedure to extract syntactic structures (Shen et al., 2018).
To probe different properties of our model, we run experiments on unsupervised parsing, segment recall, and phrase representations. DIORA achieves multiple new state-of-the-art results for unsupervised constituency parsing (absolute improvements of 13.7%, 11.5%, and 7.8% on WSJ, WSJ-40, and MultiNLI), has a greater recall on more constituent types than a strong baseline, and produces meaningful phrase representations.
2 DIORA: Deep Inside-Outside Recursive Autoencoders
Our goal is to design a model and unsupervised training procedure that learns structure from raw text. The design of DIORA is based on our hypothesis is that the most effective compression of a sentence will be derived from following

the true syntactic structure of the underlying input. Our approach builds on previous latent tree chart parsers which are augmented with the insideoutside algorithm (Baker, 1979; Lari and Young, 1990) and trained to reproduce each input word from its outside context. Based on our hypothesis, loosely inspired by the linguistic “substitution principle” (Frege, 1960), the model will best reconstruct the input by discovering and exploiting syntactic regularities of the text.
The inside pass of our method recursively compresses the input sequence, at each step inputting the vector representations of the two children into a composition function (§2.1.1) that outputs an inside vector representation of the parent. This process continues up to the root of the tree, eventually yielding a single vector representing the entire sentence (Figure 2a). This is loosely analogous to the compression step of an autoencoder and equivalent to existing latent tree chart parsers forward pass (Maillard et al., 2017). Following this, we initiate the outside pass of our algorithm with a generic (root) representation that is learned as a separate parameter. As the outside step of the inside-outside algorithm (Figure 2b), we unfold until ﬁnally producing representations of the leaf nodes. These leaves are then optimized to reconstruct the input sentence as done in an autoencoder-based deep neural network.
2.1 Filling the Chart with Inside-Outside
Each inside representation is the root of a particularly sub-tree, and that representation is generated by considering only the descendant constituents within that sub-tree, ignoring any outside context. After the inside representations are calculated, we

perform a top-down outside pass to compute outside representations. The outside representations are encoded by looking at only the context of a given sub-tree. Once the chart is ﬁlled, each constituent k (cell in the chart) is associated with an inside vector a¯(k), an outside vector ¯b(k), inside compatibility score e¯(k) and outside compatibility score f¯(k).
The input to our model is a sentence x made up of T tokens, x0, x1, ..., xT −1. Each token xi has a corresponding pre-trained embedded vector vi.
2.1.1 Inside Pass
For each pair of neighboring constituents i and j 1, we compute a compatibility score and a composition vector. The score and vector that represent a particular span k are computed using a soft weighting over all possible pairs of constituents, that together fully cover the span (we refer to this set of constituent pairs as {k}).
Vectors for spans of length 1 are initialized as a non-linear transformation 2 of the embedded input vi, and the scores associated with these spans are set to 0:

x  σ 

o =  σ  (Uψvk + b)

u

tanh

a¯(k) = o + tanh(x u)

e¯(k) = 0

Higher levels of the chart are computed as a weighted summation of constituent pairs:

a¯(k) =

e(i, j) a(i, j)

i,j∈{k}

e¯(k) =

e(i, j) eˆ(i, j)

i,j∈{k}

The compatibility function eˆ is meant to produce a score for how likely a pair of neighboring cells are to be merged. We implement this as a bilinear function of the vectors from neighboring spans, using a learned parameter matrix S. We additionally add the individual scores from each two merging cells. Intuitively, these individual scores correspond to how likely each of the cells would
1The symbols i, j, and k are identiﬁers of spans from the input x. The symbol i∗ identiﬁes a token from the set of negative examples {x∗}.
2This function shares its bias term b with Composeα, although Uψ is not tied to any other weights.

exist in the ﬁnal binary tree independently. The formula for the compatibility function (and its normalized form e) is deﬁned as follows:

e(i, j) =

exp(eˆ(i, j)) exp(eˆ(ˆi, ˆj))

ˆi,ˆj∈{k}

eˆ(i, j) = φ(a¯(i), a¯(j); Sα) + e¯(i) + e¯(j)

Where the bilinear projection φ is deﬁned as:

φ(u, v; W ) = u W v
For the composition function a we used either a TreeLSTM (Tai et al., 2015) or a 2-layer MLP (see Appendix A.1 for more precise deﬁnitons on both methods). In order for the remainder of equations to remain agnostic to the choice of composition function, we refer to the function as Compose, which produces a hidden state vector h and, in the case of TreeLSTM, a cell state vector c, resulting in:

a(i, j) = Composeα(a¯(i), a¯(j))
2.1.2 Outside Pass
The outside computation is similar to the inside pass (depicted in Figure 2b).
The root node of the outside chart is learned as a bias. Descendant cells are predicted using a disambiguation over the possible outside contexts. Each component of the context consists of a sibling cell from the inside chart and a parent cell from the outside chart.
The function f is analogous to the function e. It is normalized over constituent pairs i, j for the span k, and is used to disambiguate among the many outside contexts. The function b generates a phrase representation for the missing sibling cell. Equations for the outside computation follow:

¯b(k) =

f (i, j) b(i, j)

i,j∈{k}

f¯(k) =

f (i, j) fˆ(i, j)

i,j∈{k}

b(i, j) = Composeβ(a¯(i), ¯b(j)) fˆ(i, j) = φ(a¯(i), ¯b(j); Sβ) + e¯(i) + f¯(j)

In the majority of our experiments, the Compose used in b shares parameters with a used
in the inside pass, as do the compatibility functions eˆ and fˆ (see §3.4 for results on the effects of
parameter sharing).

2.2 Training Objective
To train our model we use an autoencoder-like language modeling objective. In a standard autoencoder, the entire input x is compressed into a single lower dimensional representation. This representation, z, is then decompressed and trained to reconstruct x. In our model, we never condition the reconstruction of x on a single z because the root’s outside representation is initialized with a bias rather than the root’s own inside vector. Instead, we reconstruct x conditioned on the many sub-tree roots, each of which is only a compression of a subset of the input.
To approximate this reconstruction we use a max-margin loss considering a set {x∗} of N negative examples that are sampled according to their frequency from the vocabulary (further details in Appendix A.2). The terminal outside vector ¯b(i) is trained to predict its original input vi.
The per-instance loss function is described in Equation 1:

T −1 N −1

Lx =

max(0, 1 − ¯b(i) · a¯(i)

i=0 i∗=0

+ ¯b(i) · a¯(i∗)) (1)

The max-margin loss does not provide a gradient if the predicted vector is closer to its ground truth than the negative example by a margin greater than 1. For that reason, we also experimented with an objective based on cross-entropy, described in Equation 2:

N −1
Z∗ = exp(¯b(i) · a¯(i∗))

i∗=0

T −1

exp(¯b(i) · a¯(i))

Lx = − log exp(¯b(i) · a¯(i)) + Z∗ (2)

i=0

2.3 DIORA CKY Parsing

To obtain a parse with DIORA, we populate an inside and outside chart using the input sentence. We can extract the maximum scoring parse based on our single grammar rule using the CKY procedure (Kasami, 1966; Younger, 1967). The steps for this procedure are described in Algorithm 1 and its runtime complexity in Appendix A.3.

3 Experiments

To evaluate the effectiveness of DIORA, we run experiments on unsupervised parsing, unsuper-

Algorithm 1 Parsing with DIORA

1: procedure CKY(chart)

Initialize terminal values.

2: for each k ∈ chart | SIZE(k) = 1 do

3:

xk ← 0

Calculate a maximum score for each span,

and record a backpointer.

4: for each k ∈ chart do

5:

xk ← max [xi + xj + e(i, j)]

i,j∈{k}

6:

πki , πkj ← arg max[xi + xj + e(i, j)]

i,j∈{k}

Backtrack to get the maximal tree.

7: procedure BACKTRACK(k)

8:

if SIZE(k) = 1 then

9:

return k

10:

i ← BACKTRACK(πki )

11: j ← BACKTRACK(πkj )

12:

return (i, j)

13: return BACKTRACK(k ← root)

vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.2.
3.1 Unsupervised Parsing
We ﬁrst evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014).
In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel (Choi et al., 2018), an unsupervised chart parser. The latter two of these models are trained to predict the downstream task of natural language inference (NLI).
3https://github.com/iesl/diora 4We consider the PRPN models using LM stopping criteria, which outperformed UP.

3.1.1 Binarized WSJ and MultiNLI results
For the full WSJ test set and MultiNLI datasets we follow the experimental setup of previous work (Williams et al., 2018a). We binarize target trees using Stanford CoreNLP (Manning et al., 2014) and do not remove punctuation (experiments in §3.1.2 do remove punctuation).
Latent tree models have been shown to perform particularly poorly on attachments at the beginning and end of the sequence (Williams et al., 2018a). To address this, we incorporate a postprocessing heuristic (denoted as +PP in result tables)5. This heuristic simply attaches trailing punctuation to the root of the tree, regardless of its predicted attachment.
In Table 1, we see that DIORA+PP achieves the highest average and maximum F1 from ﬁve random restarts. This model achieves a mean F1 7 points higher than ON-LSTM and an increase of over 6.5 max F1 points. We also see that DIORA exhibits much less variance between random seeds than ON-LSTM. Additionally, we ﬁnd that PRPN-UP and DIORA beneﬁt much more from the +PP heuristic than PRPN-LM. This is consistent with qualitative analysis showing that DIORA and PRPN-UP incorrectly attach trailing punctuation much more often than PRPN-LM.
On the MultiNLI dataset, PRPN-LM is the top performing model without using the +PP heuristic while DIORA matches PRPN-UP (Table 2. Using the heuristic, DIORA greatly surpasses both variants of PRPN. However, it is worth noting that this is not a gold standard evaluation and instead evaluates a model’s ability to replicate the output of a trained parser (Manning et al., 2014). A second caveat is that SNLI (Bowman et al., 2015) and MultiNLI contain several non-newswire domains. Syntactic parsers often suffer signiﬁcant performance drops when predicting outside of the newswire domain that the models were trained on.
3.1.2 WSJ-10 and WSJ-40 results
We also compare our models to two subsets of the WSJ dataset that were used in previous unsupervised parsing evaluations. WSJ-10 and WSJ-40 contain sentences up to length 10 and 40 respectively after punctuation removal. We do not binarize either of these two splits in order to compare to previous work (see Appendix A.4 for more
5We did not have access to predictions or an implementation of the concurrent ON-LSTM model and therefore could not apply the +PP heuristic.

Model

F1µ

F1max δ

LB

13.1

13.1 12.4

RB

16.5

16.5 12.4

Random

21.4

21.4 5.3

Balanced

21.3

21.3 4.6

RL-SPINN†

13.2

13.2 -

ST-Gumbel - GRU† 22.8 ±1.6 25.0 -

PRPN-UP PRPN-LM ON-LSTM DIORA
PRPN-UP+PP PRPN-LM+PP DIORA+PP

38.3 ±0.5 39.8 5.9 35.0 ±5.4 42.8 6.2 47.7 ±1.5 49.4 5.6 48.9 ±0.5 49.6 8.0

-

45.2 6.7

-

42.4 6.3

55.7 ±0.4 56.2 8.5

Table 1: Full WSJ (test set) unsupervised unlabeled binary constituency parsing including punctuation. † indicates trained to optimize NLI task. Mean and max are calculated over ﬁve random restarts. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). The depth (δ) is the average tree height. +PP refers to post-processing heuristic that attaches trailing punctuation to the root of the tree. The top F1 value in each column is bolded.

Model
Random Balanced

F1median F1max δ

27.0

27.0 4.4

21.3

21.3 3.9

PRPN-UP

48.6

PRPN-LM

50.4

DIORA

51.2

- 4.9 - 5.1 53.3 6.4

PRPN-UP+PP PRPN-LM+PP DIORA+PP 59.0

54.8 5.2 50.4 5.1 59.1 6.7

Table 2: NLI unsupervised unlabeled binary constituency parsing comparing to CoreNLP predicted parses. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). F1 median and max are calculated over ﬁve random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work.

details on WSJ split differences). Not binarizing the target trees sets an upper-bound on the performance of our models, denoted as UB in Table 3.
We compare against previous notable models for this task: CCM (Klein and Manning, 2002) uses the EM algorithm to learn probable nested

bracketings over a sentence using gold or induced part-of-speech tags, and PRLG (Ponvert et al., 2011) performs constituent parsing through consecutive rounds of sentence chunking.
In Table 3, we see that DIORA outperforms the previous state of the art for WSJ-40, PRLG, in max F1. The WSJ-10 split has been difﬁcult for latent tree parsers such as DIORA, PRPN, and ONLSTM, none of which (including our model) are able to improve upon previous non-neural methods. However, when we compare trends between WSJ-10 and WSJ-40, we see that DIORA does a better job at extending to longer sequences.
3.2 Unsupervised Phrase Segmentation
In many scenarios, one is only concerned with extracting particular constituent phrases rather than a full parse. Common use cases would be identifying entities, noun phrases, or verb phrases for downstream analysis. To get an idea of how well our model can perform on phrase segmentation, we consider the maximum recall of spans in our predicted parse tree. We leave methods for cutting the tree to future work and instead consider the maximum recall of our model which serves as an upper bound on its performance. Recall here is the percentage of labeled constituents that appear in our predicted tree relative to the total number of constituents in the gold tree. These scores are separated by type and presented in Table 4.
In Table 4 we see the breakdown of constituent recall across the 10 most common types. DIORA achieves the highest recall across the most types and is the only model to perform effectively on verb-phrases. Interestingly, DIORA performs worse than PRPN-LM at prepositional phrases.
3.3 Phrase Similarity
One of the goals of DIORA is to learn meaningful representations for spans of text. Most language modeling methods focus only on explicitly modeling token representations and rely on ad-hoc postprocessing to generate representations for longer spans, typically relying on simple arithmetic functions of the individual tokens.
To evaluate our model’s learned phrase representations, we look at the similarity between spans of the same type within labeled phrase datasets. We look at two datasets. CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000) is a shallow parsing dataset containing spans of noun phrases, verb phrases, etc. CoNLL 2012 (Pradhan et al., 2012)

Model

WSJ-10

WSJ-40

F1µ

F1max F1µ

F1max

UB

87.8

87.8 85.7

85.7

LB

28.7

28.7 12.0

12.0

RB

61.7

61.7 40.7

40.7

CCM†

-

63.2 -

-

CCMgold† -

71.9 -

33.7

PRLG † -

72.1 -

54.6

PRPNNLI 66.3 ±0.8 68.5 -

-

PRPN‡

70.5 ±0.4 71.3 -

52.4

ON-LSTM‡ 65.1 ±1.7 66.8 -

-

DIORA

67.7 ±0.7 68.5 60.6 ±0.2 60.9

Table 3: WSJ-10 and WSJ-40 unsupervised nonbinary unlabeled constituency parsing with punctuation removed. † indicates that the model predicts a full, non-binary parse with additional resources. ‡ indicates model was trained on WSJ data and PRPNNLI was trained on MultiNLI data. CCM uses predicted POS tags while CCMgold uses gold POS tags. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). LB and RB are the left and right-branching baselines. UB is the upper bound attainable by a model that produces binary trees.

is a named entity dataset containing 19 different entity types.
For each of the labeled spans with length greater than one, we ﬁrst generate its phrase representation. We then calculate its cosine similarity to all other labeled spans. We then calculate if the label for that query span matches the labels for each of the K most similar other spans in the dataset. In Table 5 we report precision@K for both datasets and various values of K.
The ﬁrst baseline we compare against produces phrase representations from averaging contextinsensitive (CI) ELMo vectors of individual tokens with the span. The second uses sentenceinsensitive (SI) ELMo vectors, running the full ELMo over only the relevant tokens and ignoring the rest of the sentence. We also look at ELMo’s output when given the entire sentence. When analyzing our baselines that run the full ELMo, we follow the procedure described in (Peters et al., 2018b) and represent phrases as a function of its ﬁrst and last hidden state. We extract these states from the ﬁnal ELMo layer (3rd BiLSTM) as these consistently gave the best performance among other options. For DIORA, we use the concatenation of the inside and outside representations ([a¯; ¯b]).

Label
NP VP PP S SBAR ADJP QP ADVP PRN SINV

Count
297,872 168,605 116,338 87,714
24,743 12,263 11,441 5,817 2,971 2,563

DIORA
0.767 0.628 0.595 0.798 0.613 0.604 0.801 0.693 0.546 0.926

P-UP
0.687 0.393 0.497 0.639 0.403 0.342 0.336 0.392 0.127 0.904

P-LM
0.598 0.316 0.602 0.657 0.554 0.360 0.545 0.500 0.144 0.932

Table 4: Segment recall from WSJ separated by phrase type. The 10 most frequent phrase types are shown above, and the highest value in each row is bolded. P-UP=PRNP-UP, P-LM=PRPN-LM

For CoNLL 2000, we ﬁnd that our model outperforms all baselines for all values of K. This demonstrates DIORA’s ability to capture and represent syntactic information within phrases. For CoNLL 2012, we ﬁnd that DIORA outperforms both ELMoCI and ELMoSI while ELMo performs best overall. ELMoCI is surprisingly effective on this dataset even though it performed more poorly on CoNLL 2000. These results indicate that DIORA is capturing syntax quite well, but still has room to improve on more ﬁne-grained semantic representations.
3.4 Impact of Modeling Choices
To test the impact of our modeling choices, we compared the performance of two different losses and four different composition functions on the full WSJ validation set. The losses were covered in Equations 1 (Margin) and 2 (Softmax). The two primary methods of composition we considered were TreeLSTM (Tai et al., 2015) and MLP (a 2-hidden layer neural network). In addition, we experimented with a simple kernel of the MLP input [x; y; x y; x − y] and with a setting where both the inside and outside parameters are shared.
The results are shown in Table 6. We see that MLP composition consistently performs better than with TreeLSTM, that MLP beneﬁts from the Softmax loss, and that the best performance comes from sharing parameters. All other experimental results use this highly performant setting unless otherwise speciﬁed.

The convoy of about 100 vehicles was the first to make deliveries to the capital in about 10 days
The court ruled that the news media did n't reveal Twiggy 's problems at the time
Figure 3: DIORA can match the ground truth exactly.
Ferro also said it would cancel the unused portion of a 1987 buy-back plan for administrative reasons
Ferro also said it would cancel the unused portion of a 1987 buy-back plan for administrative reasons In the stands people waved ANC flags wore ANC T-shirts sang ANC songs and chanted ANC slogans In the stands people waved ANC flags wore ANC T-shirts sang ANC songs and chanted ANC slogans
Figure 4: At times, DIORA exhibits contrary behavior to the ground truth inevitably leading to some error. DIORA’s output is shown above the ground truth.6
The following month the company put itself up for sale
The following month the company put itself up for sale
He added that the U.S. has cut off aid to some rebel units when it was determined that those units broke the cease-fire He added that the U.S. has cut off aid to some rebel units when it was determined that those units broke the cease-fire
We simply do n't agree with that or the findings of their investigation
We simply do n't agree with that or the findings of their investigation
Figure 5: DIORA often groups verbs and particles (top), sometimes exactly as the ground truth (middle). Occasionally, errors are particle-like (bottom). DIORA’s output is shown above the ground truth.6
6Ground truth parses are binarized unless otherwise speciﬁed. All examples of DIORA parses are already binary. Some punctuation has been removed for easier readability.

Model
Random ELMoCI ELMoSI ELMo
DIORAIn/Out

Dim
800 1024 4096 4096
800

CoNLL 2000 P@1 P@10 P@100

0.684 0.962 0.970 0.987

0.683 0.955 0.964 0.983

0.680 0.957 0.955 0.974

0.990 0.985 0.979

CoNLL 2012 P@1 P@10 P@100

0.137 0.708 0.660 0.896

0.133 0.643 0.624 0.847

0.135 0.544 0.533 0.716

0.860 0.796 0.646

Table 5: P@1, P@10, and P@100 for labeled chunks from CoNLL-2000 and CoNLL 2012 datasets. For all metrics, higher is better. The top value in each column is bolded. Diora uses the concatenation of the inside and outside vector at each cell which performed better than either in isolation.

3.5 Qualitative Results
Looking at our model’s output, we see that some trees are an exact replication of the binarized ground truth (Fig. 3), or very close (Fig. 4). For future work we intend to explore common patterns in DIORA’s learned structure, although some patterns are already recognizable, such as the afﬁnity to group particles and verbs (Fig. 5).

4 Related Work
Latent Tree Learning A brief survey of neural latent tree learning models was covered in (Williams et al., 2018a). The ﬁrst positive result for neural latent tree parsing was shown in (Htut et al., 2018), which used a language modeling objective. The model in (Liu et al., 2018) uses an inside chart and an outside procedure to calculate marginal probabilities in order to align spans between sentences in entailment.

Composition
TreeLSTM TreeLSTM
MLP MLP MLPKernel MLPShared

Loss
Margin Softmax Margin Softmax Softmax Softmax

F1µ ∅ +PP
49.9 53.1 52.0 52.9 49.7 54.4 52.6 55.5 51.8 54.8 50.8 56.7

Table 6: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modiﬁed according to the post-processing heuristic (+P P ). The mean F1 is shown across three random seeds.

Neural Inside-Outside Parsers The InsideOutside Recursive Neural Network (IORNN) (Le and Zuidema, 2014) is closest to ours. It is a graph-based dependency parser that uses beam search and can reliably ﬁnd accurate parses when retaining a k-best list. In contrast, our model produces the most likely parse given the learned compatibility of the constituents. The Neural CRF Parser (Durrett and Klein, 2015), similar to DIORA, performs exact inference on the structure of a sentence, although requires a set of grammar rules and labeled parse trees during training. DIORA, like Liu et al. (2018), has a single grammar rule that applies to any pair of constituents and does not use structural supervision.
Learning from Raw Text Unsupervised learning of syntactic structure has been an active research area (Brill et al., 1990), including for unsupervised segmentation (Ando and Lee, 2000; Goldwater et al., 2009; Ponvert et al., 2011) and unsupervised dependency parsing (Spitkovsky et al., 2013). Some models exploit the availability of parallel corpora in multiple languages (Das and Petrov, 2011; Cohen et al., 2011). Others have shown that dependency parsing can be used for unsupervised constituency parsing (Spitkovsky et al., 2013; Klein and Manning, 2004), or that it’s effective to prune a random subset of possible trees (Bod, 2006). These approaches aren’t necessarily orthogonal to DIORA. For instance, our model may beneﬁt when combined with an unsupervised dependency parser.
5 Conclusion
In this work we presented DIORA, an unsupervised method for inducing syntactic trees and representations of constituent spans. We showed

inside-outside representations constructed with a latent tree chart parser and trained with an autoencoder language modeling objective learns syntactic structure of language effectively. In experiments on unsupervised parsing, chunking, and phrase representations we show our model is comparable to or outperforms previous methods, achieving the state-of-the-art performance on unsupervised unlabeled constituency parsing for the full WSJ (with punctuation), WSJ-40, and NLI datasets. We also show our model obtains higher segment recall than a comparable model and outperforms strong baselines on phrase representations on a chunking dataset.
While the current model seems to focus primarily on syntax, future work can improve the model’s ability to capture ﬁne-grained semantics. Potential avenues include training larger models over much larger corpora, extra unsupervised or weakly-supervised phrase classiﬁcation objectives, and other modeling enhancements. We are also eager to apply DIORA to other domains and languages which do not have rich linguistically annotated training sets.
Acknowledgements
We are grateful to Carolyn Anderson, Adina Williams, Phu Mon Htut, and our colleagues at UMass for help and advice, and to the UMass NLP reading group and the anonymous reviewers for feedback on drafts of this work. This work was supported in part by the Center for Intelligent Information Retrieval, in part by the National Science Foundation (NSF) grant numbers DMR1534431, IIS-1514053 and CNS-0958392. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect those of the sponsor.
References
Roee Aharoni and Yoav Goldberg. 2017. Towards string-to-tree neural machine translation. In Association for Computational Linguistics (ACL).
Rie Kubota Ando and Lillian Lee. 2000. Mostlyunsupervised statistical segmentation of japanese: Applications to kanji. In North American Association for Computational Linguistics (NAACL).
James K Baker. 1979. Trainable grammars for speech recognition. The Journal of the Acoustical Society of America, 65(S1):S132–S132.

Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Association for Computational Linguistics (ACL).

Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Empirical Methods in Natural Language Processing (EMNLP).

Eric Brill, David Magerman, Mitchell Marcus, and Beatrice Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In Information Technology, 1990.’Next Decade in Information Technology’, Proceedings of the 5th Jerusalem Conference on (Cat. No. 90TH0326-9), pages 380–389. IEEE.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciﬁc tree structures. In Advancement of Artiﬁcial Intelligence Conference on Artiﬁcial Intelligence (AAAI).

Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011. Unsupervised structure prediction with nonparallel multilingual guidance. In Empirical Methods in Natural Language Processing (EMNLP).

Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Association for Computational Linguistics (ACL).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. North American Association for Computational Linguistics (NAACL).

Greg Durrett and Dan Klein. 2015. Neural crf parsing. In Association for Computational Linguistics (ACL).

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017. Learning to parse and translate improves neural machine translation. In Association for Computational Linguistics (ACL).

Friedrich Ludwig Gottlob Frege. 1960. On sense and reference. In Zeitschrift fu¨r Philosophie und philosophische Kritik 100 (1892) 25-50; translated in Translations from the Philosophical Writings of Gottlob Frege (ed. by P. Geach and M. Black). Oxford.

Pablo Gamallo, Marcos Garcia, and Santiago

Ferna´ndez-Lanza. 2012.

Dependency-based

open information extraction. In Joint workshop on

unsupervised and semi-supervised learning in NLP.

Association for Computational Linguistics (ACL).

Sharon Goldwater, Thomas L Grifﬁths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112:21–54.

Luheng He, Kenton Lee, Omer Levy, and Luke Zettlemoyer. 2018. Jointly predicting predicates and arguments in neural semantic role labeling. In Association for Computational Linguistics (ACL).

Phu Mon Htut, Kyunghyun Cho, and Samuel R Bowman. 2018. Grammar induction with neural language models: An unusual replication. In Empirical Methods in Natural Language Processing (EMNLP): Short Paper.

Tadao Kasami. 1966. An efﬁcient recognition and syntax-analysis algorithm for context-free languages. Coordinated Science Laboratory Report no. R-257.

Diederik P. Kingma and Jimmy Ba. 2014. A method for stochastic optimization. abs/1412.6980.

Adam: CoRR,

Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Association for Computational Linguistics (ACL).

Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Association for Computational Linguistics (ACL).

Karim Lari and Steve J Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer speech & language, 4(1):35–56.

Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Empirical Methods in Natural Language Processing (EMNLP).

Phong Le and Willem Zuidema. 2015. The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization. In Empirical Methods in Natural Language Processing (EMNLP), pages 1155–1164.

Xin Li and Dan Roth. 2006. Learning question classiﬁers: the role of semantic information. Natural Language Engineering, 12(3):229–249.

Yang Liu, Matt Gardner, and Mirella Lapata. 2018. Structured alignment networks for matching sentences. In Empirical Methods in Natural Language Processing (EMNLP).

Jean Maillard, Stephen Clark, and Dani Yogatama. 2017. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms. arXiv preprint arXiv:1705.09189.

Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Association for Computational Linguistics (ACL): System Demonstrations.

Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).
Matthew E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Empirical Methods in Natural Language Processing (EMNLP).
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded ﬁnite state models. In Association for Computational Linguistics (ACL).
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Joint Conference on EMNLP and CoNLL-Shared Task.
Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. 2018. Neural language modeling by jointly learning syntax and lexicon. In International Conference on Learning Representations (ICLR).
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2019. Ordered neurons: Integrating tree structures into recurrent neural networks. In International Conference on Learning Representations (ICLR).
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems (NeurIPS).
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Empirical Methods in Natural Language Processing (EMNLP).
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In Empirical Methods in Natural Language Processing (EMNLP).
Charles Sutton and Andrew McCallum. 2005. Joint parsing and semantic role labeling. In Conference on Computational Natural Language Learning (CoNLL).
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations

from tree-structured long short-term memory networks. In Association for Computational Linguistics (ACL).
Pytorch Core Team. 2018. Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration. http://pytorch.org/. Accessed: 2018-09-26.
Erik F Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, pages 127–132.
Adina Williams, Andrew Drozdov, and Samuel R Bowman. 2018a. Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association of Computational Linguistics (TACL), 6:253–267.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018b. A broad-coverage challenge corpus for sentence understanding through inference. In North American Association for Computational Linguistics (NAACL).
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2017. Learning to compose words into sentences with reinforcement learning. In International Conference on Learning Representations (ICLR).
Daniel H Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and control, 10(2):189–208.
Poorya Zaremoodi and Gholamreza Haffari. 2018. Incorporating syntactic uncertainty in neural machine translation with a forest-to-sequence model. In International Conference on Computational Linguistics.

A Appendices
A.1 Composition and Input Transform TreeLSTM. The TreeLSTM (Tai et al., 2015) function produces a hidden state vector h and cell state vector c given two input vectors hi and hj.

x  σ 

fi  σ 
   fj =  σ  U    o  σ 

u

tanh

c = ci fi + cj

h = o + tanh(c)

0 ω hi + b + ω hj 0
0
fj + x u

The constant ω is set to 1 for the inside, 0 for the outside. U and b are learned.
MLP. MLP (Multi-Layer Perceptron) is a deep non-linear composition with the following form:

h = W1 (W0 hi, hj + b) + b1
The operator hi, hj is a concatenation [hi; hj]. For the MLPKernel hi, hj is more involved to support further interaction between the two input vectors [hi; hj; hi hj; hi − hj]. The variables W0, W1, b, b1 are learned and c is unused.
A.2 Training Details
Training Data. Sentences of length ≤ 20 from the SNLI and MultiNLI training sets. Optimization. We train our model using stochastic gradient descent with the Adam optimization algorithm (Kingma and Ba, 2014). Cells were normalized to have magnitude of 1, following Socher et al. (2011a). For instance, a¯(k) := a¯(k)/ a¯(k) 2. Gradients are clipped to a maximum L2-norm of 5. Hyperparameters. Chosen using grid search over cell-dimension {400D, 800D} and learning rate {2, 4, 8, 10, 20} · 10−4. Early Stopping. Using unlabeled parsing F1 against the binarized WSJ validation set. Vocabulary. The model is trained in an openvocabulary setting using pre-trained contextinsensitive character embeddings from ELMo (Peters et al., 2018a). Batching. Batches were constructed such that they contained sentences of uniform length. Using batch size 128 for 400D and 64 for 800D.

Sampling. N negatives are sampled for each batch. All experiments use N = 100. Training Steps. 1M parameter updates, taking 3 days using 4x Nvidia 1080ti.
A.3 Runtime Complexity
The runtime complexities for DIORA’s methods are shown in Table 7. The parallel column represents the complexity when the values for all constituent pairs are computed simultaneously, assuming that these computations are independent and do not depend on values that have yet to be computed. Linear complexity is theoretically feasible depending on batch size, input length, and number of computational cores. In practice, one might experience super-linear performance.
Although both the inside pass and outside pass have an upper bound of n3 operations, the outside pass will have more operations than the inside pass for sentences of length > 1.
As a point of reference, our implementation computes the loss over the entire WSJ corpus in 5 minutes 30 seconds at a rate of 3,500 words per second using a single GPU.

Method
Inside Pass Outside Pass Training Objective CKY

Serial
O(n3) O(n3) O(n · N ) O(n3)

Parallel
O(n) O(n) O(n) O(n)

Table 7: Runtime complexity for methods associated with DIORA in terms of sentence length n and number of negative examples per token N . Each column represents the complexity when the values for each constituent are computed serially or in parallel.
A.4 Reproducing Parsing Results
In Table 8, we’ve organized a reference for creating various splits of the WSJ for the purpose of evaluating unsupervised parsing. Some splits use only the test set (section 23), others use all of the training, validation, and test data. Optionally, punctuation is stripped and sentences greater than a speciﬁed length are ignored. Predictions can be compared to the full parse trees in the annotated data, or to a binarized version. The PARSEVAL speciﬁcation calculated bracketing F1 considering all spans, although some previous work diverts from PARSEVAL and ignores spans that are trivially correct (ones over the entire sentence).

WSJ WSJ-10 WSJ-40

Split

Test All

Test

w/ Punctuation Yes No

No

Max Length

∞

10

40

Binarized

Yes No

No

Trivial Spans Yes No

No

Table 8: Settings for unlabeled binary bracketing evaluation for different splits of the WSJ corpus.

A.5 Parse Trees
Examples of parse trees derived from the compatibility scores are shown in Figures 6, 7, and 8. Some punctuation has been removed for easier readability.

Mr. van Dover said the crystal changes his team introduced apparently pins the magnetic fields in place preventing them from lowering current-carrying capacity

Len Kessler a financial publicist in New York sometimes uses it to get the attention of journalists who try to avoid him Ms. Browning says she believes a recapitalization involving employee ownership would succeed only if the pilots relent on their demand for control

In addition a big loan that First Boston made to Ohio Mattress Co was n't repaid on time when its 450 million junk financing for a buy-out of the bedding company was withdrawn In addition a big loan that First Boston made to Ohio Mattress Co was n't repaid on time when its 450 million junk financing for a buy-out of the bedding company was withdrawn In its latest compilation of performance statistics Moody 's Investors Service found that investment-grade bonds posted a total return of 2.7 % in October while junk bonds showed a negative return of 1.5 % In its latest compilation of performance statistics Moody 's Investors Service found that investment-grade bonds posted a total return of 2.7 % in October while junk bonds showed a negative return of 1.5 % Within a year Kao Corp. a major cosmetics company plans to eliminate 1,000 clerical jobs by putting on a central computer network some work such as credit reports currently performed in 22 separate offices Within a year Kao Corp. a major cosmetics company plans to eliminate 1,000 clerical jobs by putting on a central computer network some work such as credit reports currently performed in 22 separate offices Authorities at London 's Heathrow Airport are investigating the disappearance of a Paul Gauguin watercolor Young Tahitian Woman in a Red Pareo that has two sketches on its verso -LRB- opposite -RRB- side Authorities at London 's Heathrow Airport are investigating the disappearance of a Paul Gauguin watercolor Young Tahitian Woman in a Red Pareo that has two sketches on its verso -LRB- opposite -RRB- side But in its ruling last April the New York court said that all producers of the anti-miscarriage drug should share liability when the manufacturer of a specific dose ca n't be determined But in its ruling last April the New York court said that all producers of the anti-miscarriage drug should share liability when the manufacturer of a specific dose ca n't be determined The Fed said the Comptroller of the Currency is expected to begin a Community Reinvestment Act examination of First Union 's Florida and North Carolina banking units in the next two weeks The Fed said the Comptroller of the Currency is expected to begin a Community Reinvestment Act examination of First Union 's Florida and North Carolina banking units in the next two weeks
The appeals-court decision last year was particularly surprising because the same court had dismissed a similar case in 1970 involving singer Nancy Sinatra and a tire ad also a Young & Rubicam product The appeals-court decision last year was particularly surprising because the same court had dismissed a similar case in 1970 involving singer Nancy Sinatra and a tire ad also a Young & Rubicam product The resulting # 1.9 billion merchandise trade deficit was partly offset by an assumed surplus of # 300 million in so-called invisible items which include income from investments services and official transfers The resulting # 1.9 billion merchandise trade deficit was partly offset by an assumed surplus of # 300 million in so-called invisible items which include income from investments services and official transfers For the third quarter net premiums were 742 million up 9.6 % from 677 million in last year 's quarter because of the expiration of the National Indemnity quota share reinsurance agreement For the third quarter net premiums were 742 million up 9.6 % from 677 million in last year 's quarter because of the expiration of the National Indemnity quota share reinsurance agreement
BANKERS ACCEPTANCES: 8.45 % 30 days ; 8.33 % 60 days ; 8.32 % 90 days ; 8.15 % 120 days ; 8.06 % 150 days ; 7.96 % 180 days BANKERS ACCEPTANCES: 8.45 % 30 days ; 8.33 % 60 days ; 8.32 % 90 days ; 8.15 % 120 days ; 8.06 % 150 days ; 7.96 % 180 days

Under the new plan being considered the notes would reset annually at a rate to maintain a market value of 101 But the Gutfreund workers went ahead anyway only to be captured in flagrante by Joan Postel who called the police Given the seismic history of the Bay Area it seems to me that a 6.9 earthquake is a foreseeable event A rash of one-time charges left Ashland Oil with a loss of 39 million for its fiscal fourth quarter

At the 932 million T. Rowe Price High Yield Fund investors yanked out about 182 million in the past two months At the 932 million T. Rowe Price High Yield Fund investors yanked out about 182 million in the past two months
Import values are calculated on a cost insurance and freight -LRB- c.i.f -RRB- basis while exports are accounted for on a free-on-board -LRB- f.o.b -RRB- basis Import values are calculated on a cost insurance and freight -LRB- c.i.f -RRB- basis while exports are accounted for on a free-on-board -LRB- f.o.b -RRB- basis The three units are a nationwide pharmaceutical and health-products distributor a small sporting-goods chain and a combination catalog showroom and toy-store chain The three units are a nationwide pharmaceutical and health-products distributor a small sporting-goods chain and a combination catalog showroom and toy-store chain

Separately Mr. Brady said he asked the Working Group on Financial Markets to determine whether futures margins are too low
In a classic defense of a personal-injury case the consultants concentrate on encouraging the jury to shift the blame
The wholesaler of cash and carry merchandise reported fiscal fourthquarter earnings that were better than analysts had expected
Figure 6: Examples where DIORA achieves 100% accuracy compared with the binarized ground truth.

Dreyfus alone has seen its money market funds grow from 1 billion in 1975 to closes to 15 billion today Dreyfus alone has seen its money market funds grow from 1 billion in 1975 to closes to 15 billion today
Also it was not a funny time over here what with the Vietnam War the '68 Democratic convention assassinations and riots Also it was not a funny time over here what with the Vietnam War the '68 Democratic convention assassinations and riots The Tennessee Valley Authority issued 4 billion in bonds in the federal utility 's first public debt offering in 15 years

The Tennessee Valley Authority issued 4 billion in bonds in the federal utility 's first public debt offering in 15 years
Figure 7: Examples where DIORA achieves 100% recall compared with the raw (n-ary) ground truth, but less than 100% accuracy on the binarized ground truth. DIORA is shown above the ground truth. DIORA’s output is shown above the ground truth.

On Tuesday the House approved a labor-backed amendment that would require the Transportation Department to reject airline acquisitions if the person seeking to purchase a carrier had run two or more airlines previously that have filed for protection from creditors under Chapter 11 of the federal Bankruptcy Code On Tuesday the House approved a labor-backed amendment that would require the Transportation Department to reject airline acquisitions if the person seeking to purchase a carrier had run two or more airlines previously that have filed for protection from creditors under Chapter 11 of the federal Bankruptcy Code
There is also speculation that Mr. Newhouse could bring in a powerhouse businessman or another Newhouse family member to run the business side in combination with a publishing executive like Robert Gottlieb who left Random House 's Alfred A. Knopf to run the New Yorker also owned by the Newhouse family There is also speculation that Mr. Newhouse could bring in a powerhouse businessman or another Newhouse family member to run the business side in combination with a publishing executive like Robert Gottlieb who left Random House 's Alfred A. Knopf to run the New Yorker also owned by the Newhouse family
The Warner Bros. studio and Sony signaled they are close to a settlement yesterday asking a Los Angeles Superior Court to postpone a hearing scheduled for tomorrow on Warner 's request for a preliminary injunction blocking Mr. Guber and Mr. Peters from taking the top posts at Columbia Pictures Entertainment Inc The Warner Bros. studio and Sony signaled they are close to a settlement yesterday asking a Los Angeles Superior Court to postpone a hearing scheduled for tomorrow on Warner 's request for a preliminary injunction blocking Mr. Guber and Mr. Peters from taking the top posts at Columbia Pictures Entertainment Inc
Rep. Edwards the California Democrat is one who pledges that he would immediately challenge Mr. Bush in the courts arguing a line-item veto would expand a president 's powers far beyond anything the framers of the Constitution had in mind Rep. Edwards the California Democrat is one who pledges that he would immediately challenge Mr. Bush in the courts arguing a line-item veto would expand a president 's powers far beyond anything the framers of the Constitution had in mind
The Merc received considerable criticism in 1987 when it was discovered that its compliance director Kevin P. Conway who then was responsible for policing the exchange 's busy oil and metal pits was engaged in other personal business activities on Exchange time including out-of-state trips according to a New York Merc report prepared last year The Merc received considerable criticism in 1987 when it was discovered that its compliance director Kevin P. Conway who then was responsible for policing the exchange 's busy oil and metal pits was engaged in other personal business activities on Exchange time including out-of-state trips according to a New York Merc report prepared last year For example one of my favorite movies is the 1949 British comedy Kind Hearts and Coronets in which the entire comedy is based on actor Dennis Price 's murdering eight titled relatives -LRB- all played by Alec Guinness -RRB- because they snubbed his mother and stand in the way of his acquiring the family title For example one of my favorite movies is the 1949 British comedy Kind Hearts and Coronets in which the entire comedy is based on actor Dennis Price 's murdering eight titled relatives -LRB- all played by Alec Guinness -RRB- because they snubbed his mother and stand in the way of his acquiring the family title
The leveraged buy-out firm of Kohlberg Kravis Roberts & Co. which owns 46 % of the common equity of SCI TV indicated in the debt plan that it would reduce its equity stake to 15 % giving the rest of its stake to bondholders in the restructuring The leveraged buy-out firm of Kohlberg Kravis Roberts & Co. which owns 46 % of the common equity of SCI TV indicated in the debt plan that it would reduce its equity stake to 15 % giving the rest of its stake to bondholders in the restructuring
BUSINESSLAND INC. San Jose computer retail company annual sales of 1.1 billion NYSE said all 16 corporate office and stores in the area were open with the exception of a retail center in San Francisco 's business district BUSINESSLAND INC. San Jose computer retail company annual sales of 1.1 billion NYSE said all 16 corporate office and stores in the area were open with the exception of a retail center in San Francisco 's business district
Recognition also said it obtained a commitment from Chemical Bank and Bank of Boston to convert an estimated 18 million in bank debt to a new 24-month secured term loan to be repaid through the sale of certain assets Recognition also said it obtained a commitment from Chemical Bank and Bank of Boston to convert an estimated 18 million in bank debt to a new 24-month secured term loan to be repaid through the sale of certain assets
The prices of cattle and hog futures contracts dropped sharply because traders speculated that the stock market plunge Friday will linger in the minds of U.S. consumers long enough to prompt them to rein in their spending at the supermarket which would hurt demand for beef and pork The prices of cattle and hog futures contracts dropped sharply because traders speculated that the stock market plunge Friday will linger in the minds of U.S. consumers long enough to prompt them to rein in their spending at the supermarket which would hurt demand for beef and pork
Figure 8: DIORA can perform close to the ground truth even on long sentences. In this ﬁgure, n-ary trees are shown for the ground truth. DIORA’s output is shown above the ground truth.

