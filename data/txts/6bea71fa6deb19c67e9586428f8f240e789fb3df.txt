Improved Algorithms for Linear Stochastic Bandits

Yasin Abbasi-Yadkori
abbasiya@ualberta.ca Dept. of Computing Science
University of Alberta

Da´vid Pa´l
dpal@google.com Dept. of Computing Science
University of Alberta

Csaba Szepesva´ri
szepesva@ualberta.ca Dept. of Computing Science
University of Alberta

Abstract
We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales.

1 Introduction
Linear stochastic bandit problem is a sequential decision-making problem where in each time step we have to choose an action, and as a response we receive a stochastic reward, expected value of which is an unknown linear function of the action. The goal is to collect as much reward as possible over the course of n time steps. The precise model is described in Section 1.2.
Several variants and special cases of the problem exist differing on what the set of available actions is in each round. For example, the standard stochastic d-armed bandit problem, introduced by Robbins (1952) and then studied by Lai and Robbins (1985), is a special case of linear stochastic bandit problem where the set of available actions in each round is the standard orthonormal basis of Rd. Another variant, studied by Auer (2002) under the name “linear reinforcement learning”, and later in the context of web advertisement by Li et al. (2010), Chu et al. (2011), is a variant when the set of available actions changes from time step to time step, but has the same ﬁnite cardinality in each step. Another variant dubbed “sleeping bandits”, studied by Kleinberg et al. (2008), is the case when the set of available actions changes from time step to time step, but it is always a subset of the standard orthonormal basis of Rd. Another variant, studied by Dani et al. (2008), Abbasi-Yadkori et al. (2009), Rusmevichientong and Tsitsiklis (2010), is the case when the set of available actions does not change between time steps but the set can be an almost arbitrary, even inﬁnite, bounded subset of a ﬁnite-dimensional vector space. Related problems were also studied by Abe et al. (2003), Walsh et al. (2009), Dekel et al. (2010).
In all these works, the algorithms are based on the same underlying idea—the optimism-in-theface-of-uncertainty (OFU) principle. This is not surprising since they are solving almost the same problem. The OFU principle elegantly solves the exploration-exploitation dilemma inherent in the problem. The basic idea of the principle is to maintain a conﬁdence set for the vector of coefﬁcients of the linear function. In every round, the algorithm chooses an estimate from the conﬁdence set and an action so that the predicted reward is maximized, i.e., estimate-action pair is chosen optimistically. We give details of the algorithm in Section 2.
1

Thus the problem reduces to the construction of conﬁdence sets for the vector of coefﬁcients of the linear function based on the action-reward pairs observed in the past time steps. This is not an easy problem, because the future actions are not independent of the actions taken in the past (since the algorithm’s choices of future actions depend on the random conﬁdence set constructed from past data). In fact, several authors (Auer, 2000, Li et al., 2010, Walsh et al., 2009) fell victim of making a mistake because they did not recognize this issue. Correct solutions require new martingale techniques which we provide here.
The smaller conﬁdence sets one is able to construct, the better regret bounds one obtains for the resulting algorithm, and, more importantly, the better the algorithm performs empirically. With our new technique, we vastly reduce the size of the conﬁdence sets of Dani et al. (2008) and Rusmevichientong and Tsitsiklis (2010). First, our conﬁdence sets are valid uniformly over all time steps, which immediately saves log(n) factor by avoiding the otherwise needed union bound. Second, our conﬁdence sets are “more empirical” in the sense that some worst-case quantities from the old bounds are replaced by empirical quantities that are always smaller, sometimes substantially. As a result, our experiments show an order-of-magnitude improvement over the CONFIDENCEBALL algorithm of Dani et al. (2008). To construct our conﬁdence sets, we prove a new martingale tail inequality. The new inequality is derived using techniques from the theory of self-normalized processes (de la Pen˜a et al., 2004, 2009).
Using our conﬁdence sets, we modify the UCB algorithm (Auer, 2002) for the d-armed bandit problem and show that with probability 1 − δ, the regret of the modiﬁed algorithm is O(d log(1/δ)/∆) where ∆ is the difference between the expected rewards of the best and the second best action. In particular, note that the regret does not depend on n. This seemingly contradicts the result of Lai and Robbins (1985) who showed that the expected regret of any algorithm is at least ( i=i∗ 1/D(pj | pi∗ ) − o(1)) log n where pi∗ and pi are the reward distributions of the optimal arm and arm i respectively and D is the Kullback-Leibler divergence. However, our algorithm receives δ as an input, and thus its expected regret depends on δ. With δ = 1/n our algorithm has the same expected regret bound, O((d log n)/∆), as Auer (2002) has shown for UCB.
For the general linear stochastic bandit problem, we improve regret of the CONFIDENCEBALL
algorithm of Dani et al. (2008). They showed that its regret is at most O(d log(n) n log(n/δ)) with probability at least 1 − δ. We modify their algor√ithm so that it uses our new conﬁdence sets and we show that its regret is at most O(d log(n) n + dn log(n/δ)) which is roughly
improvement a multiplicative factor log(n). Dani et al. (2008) prove also a problem dependent regret bound. Namely, they show that the regret of their algorithm is O( d∆2 log(n/δ) log2(n)) where ∆ is the “gap” as deﬁned in (Dani et al., 2008). For our modiﬁed algorithm we prove an improved O( log(∆1/δ) (log(n) + d log log n)2) bound.

1.1 Notation
We use x p to denote the p-norm of a vector x ∈ Rd. For a√positive deﬁnite matrix A ∈ Rd×d, the weighted 2-norm of vector x ∈ Rd is deﬁned by x A = x Ax. The inner product is denoted by ·, · and the weighted inner-product x Ay = x, y A. We use λmin(A) to denote the minimum eigenvalue of the positive deﬁnite matrix A. For any sequence {at}∞ t=0 we denote by ai:j the sub-sequence ai, ai+1, . . . , aj.

1.2 The Learning Model

In each round t, the learner is given a decision set Dt ⊆ Rd from which he has to choose an action Xt. Subsequently he observes reward Yt = Xt, θ∗ + ηt where θ∗ ∈ Rd is an unknown parameter and ηt is a random noise satisfying E[ηt | X1:t, η1:t−1] = 0 and some tail-constraints, to be speciﬁed soon.

The goal of the learner is to maximize his total reward

n t=1

Xt, θ∗

accumulated over the course

of n rounds. Clearly, with the knowledge of θ∗, the optimal strategy is to choose in round t the

point x∗t = argmaxx∈D x, θ∗ that maximizes the reward. This strategy would accumulate total

reward

n t=1

x

∗ t

,

θ∗

t
. It is thus natural to evaluate the learner relative to this optimal strategy. The

difference of the learner’s total reward and the total reward of the optimal strategy is called the

2

for t := 1, 2, . . . do
(Xt, θt) = argmax(x,θ)∈Dt×Ct−1 x, θ Play Xt and observe reward Yt Update Ct end for

Figure 1: OFUL ALGORITHM

pseudo-regret (Audibert et al., 2009a) of the algorithm and it can be formally written as

n

n

n

Rn =

x

∗ t

,

θ∗

−

Xt, θ∗ =

x∗t − Xt, θ∗ .

t=1

t=1

t=1

As compared to the regret, the pseudo-regret has the same expected value, but lower variance
because the additive noise ηt is removed. However, the omitted quantity is uncontrollable, hence we have no interest in including it in our results (the omitted quantity would also cancel, if ηt was a sequence which is independently selected of X1:t.) In what follows, for simplicity we use the word regret instead of the more precise pseudo-regret in connection to Rn.

The goal of the algorithm is to keep the regret Rn as low as possible. As a bare minimum, we require that the algorithm is Hannan consistent, i.e., Rn/n → 0 with probability one.

In order to obtain meaningful upper bounds on the regret, we will place assumptions on {Dt}∞ t=1, θ∗ and the distribution of {ηt}∞ t=1. Roughly speaking, we will need to assume that {Dt}∞ t=1 lies in a bounded set. We elaborate on the details of the assumptions later in the paper.

However, we state the precise assumption on the noise sequence {ηt}∞ t=1 now. We will assume that ηt is conditionally R-sub-Gaussian where R ≥ 0 is a ﬁxed constant. Formally, this means that

∀λ ∈ R

E eληt | X1:t, η1:t−1 ≤ exp λ2R2 . 2

The sub-Gaussian condition automatically implies that E[ηt | X1:t, η1:t−1] = 0. Furthermore, it also implies that Var[ηt | Ft] ≤ R2 and thus we can think of R2 as the (conditional) variance of the noise. An example of R-sub-Gaussian ηt is a zero-mean Gaussian noise with variance at most R2, or a bounded zero-mean noise lying in an interval of length at most 2R.

2 Optimism in the Face of Uncertainty
A natural and successful way to design an algorithm is the optimism in the face of uncertainty principle (OFU). The basic idea is that the algorithm maintains a conﬁdence set Ct−1 ⊆ Rd for the parameter θ∗. It is required that Ct−1 can be calculated from X1, X2, . . . , Xt−1 and Y1, Y2, . . . , Yt−1 and “with high probability” θ∗ lies in Ct−1. The algorithm chooses an optimistic
estimate θt = argmaxθ∈Ct−1 (maxx∈Dt x, θ ) and then chooses action Xt = argmaxx∈Dt x, θt
which maximizes the reward according to the estimate θt. Equivalently, and more compactly, the algorithm chooses the pair
(Xt, θt) = argmax x, θ ,
(x,θ)∈Dt ×Ct−1
which jointly maximizes the reward. We call the resulting algorithm the OFUL ALGORITHM for “optimism in the face of uncertainty linear bandit algorithm”. Pseudo-code of the algorithm is given in Figure 1.
The crux of the problem is the construction of the conﬁdence sets Ct. This construction is the subject of the next section.

3 Self-Normalized Tail Inequality for Vector-Valued Martingales
Since the decision sets {Dt}∞ t=1 can be arbitrary, the sequence of actions Xt ∈ Dt is arbitrary as well. Even if {Dt}∞ t=1 is “well-behaved”, the selection rule that OFUL uses to choose Xt ∈ Dt

3

generates a sequence {Xt}∞ t=1 with complicated stochastic dependencies that are hard to handle.
Therefore, for the purpose of deriving conﬁdence sets it is easier to drop any assumptions on {Xt}∞ t=1 and pursue a more general result.

If we consider the σ-algebra Ft = σ(X1, X2, . . . , Xt+1, η1, η2, . . . , ηt) then Xt becomes Ft−1measurable and ηt becomes Ft-measurable. Relaxing this a little bit, we can assume that {Ft}∞ t=0 is any ﬁltration of σ-algebras such that for any t ≥ 1, Xt is Ft−1-measurable and ηt is Ft-measurable and therefore Yt = Xt, θ∗ + ηt is Ft-measurable. This is the setup we consider for derivation of
the conﬁdence sets.

The sequence {St}∞ t=0, St =

t s=1

ηtXt,

is

a

martingale

with

respect

{Ft}∞ t=0

which

happens

to

be crucial for the construction of the conﬁdence sets for θ∗. The following theorem shows that with

high probability the martingale stays close to zero. Its proof is given in Appendix A

Theorem 1 (Self-Normalized Bound for Vector-Valued Martingales). Let {Ft}∞ t=0 be a ﬁltration. Let {ηt}∞ t=1 be a real-valued stochastic process such that ηt is Ft-measurable and ηt is conditionally R-sub-Gaussian for some R ≥ 0 i.e.

∀λ ∈ R

E eληt | Ft−1 ≤ exp λ2R2 . 2

Let {Xt}∞ t=1 be an Rd-valued stochastic process such that Xt is Ft−1-measurable. Assume that V is a d × d positive deﬁnite matrix. For any t ≥ 0, deﬁne

t
V t = V + XsXs
s=1

t
St = ηsXs .
s=1

Then, for any δ > 0, with probability at least 1 − δ, for all t ≥ 0,

2

2

det(V t)1/2 det(V )−1/2

St V −1 ≤ 2R log t

δ

.

Note that the deviation of the martingale St 2V − t 1 is measured by the norm weighted by the matrix
−1
V t which is itself derived from the martingale, hence the name “self-normalized bound”.

4 Construction of Conﬁdence Sets

Let θt be the 2-regularized least-squares estimate of θ∗ with regularization parameter λ > 0:

θt = (X1:tX1:t + λI)−1X1:tY1:t

(1)

where X1:t is the matrix whose rows are X1 , X2 , . . . , Xt and Y1:t = (Y1, . . . , Yt) . The following theorem shows that θ∗ lies with high probability in an ellipsoid with center at θt. Its proof can be found in Appendix B.
Theorem 2 (Conﬁdence Ellipsoid). Assume the same as in Theorem 1, let V = Iλ, λ > 0, deﬁne Yt = Xt, θ∗ + ηt and assume that θ∗ 2 ≤ S. Then, for any δ > 0, with probability at least 1 − δ, for all t ≥ 0, θ∗ lies in the set


 Ct = θ ∈ Rd :


θt − θ ≤ R
Vt

2 log

det(V t)1/2 det(λI)−1/2 δ


 + λ1/2 S .


Furthermore, if for all t ≥ 1, Xt 2 ≤ L then with probability at least 1 − δ, for all t ≥ 0, θ∗ lies in the set

C = θ ∈ Rd : θt − θ ≤ R d log 1 + tL2/λ + λ1/2 S .

t

Vt

δ

4

The above bound could be compared with a similar bound of Dani et al. (2008) whose bound, under identical conditions, states that (with appropriate initialization) with probability 1 − δ,

for all t large enough θt − θ∗ ≤ R max
Vt

t2 8

t2

128 d log(t) log

, log

δ3

δ

, (2)

where large enough means that t satisﬁes 0 < δ < t2e−1/16. Denote by βt(δ) the right-hand side in the last bound. The restriction on t comes from the fact that βt(δ) ≥ 2d(1 + 2 log(t)) is needed in the proof of the last inequality of their Theorem 5.

On the other hand, Rusmevichientong and Tsitsiklis (2010) proved that for any ﬁxed t ≥ 2, for any 0 < δ < 1, with probability at least 1 − δ,

θt − θ∗ ≤ 2 κ2R log t d log(t) + log(1/δ) + λ1/2S ,
Vt

where κ = 3 + 2 log((L2 + trace(V ))/λ. To get a uniform bound one can use a union bound

with δt = δ/t2. Then

∞ t=2

δt

=

δ( π62

− 1)

≤

δ.

This

thus

gives

that

for

any

0

<

δ

<

1,

with

probability at least 1 − δ,

∀t ≥ 2,

θt − θ∗ ≤ 2κ2R log t d log(t) + log(t2/δ) + λ1/2S ,
Vt

This is tighter than (2), but is still lagging behind the result of Theorem 2. Note that the new conﬁ-

dence set seems to require the computation of a determinant of a matrix, a potentially expensive step.

However, one can speed up the computation by using the matrix determinant lemma, exploiting that

the matrix whose determinant is needed is obtained via a rank-one update (cf. the proof of Lemma 11

in the Appendix). This way, the determinant can be kept up-to-date with linear time computation.

5 Regret Analysis of the OFUL ALGORITHM

We now give a bound on the regret of the OFUL algorithm when run with conﬁdence sets Cn constructed in Theorem 2 in the previous section. We will need to assume that expected rewards are bounded. We can view this as a bound on θ∗ and the bound on the decision sets Dt. The next theorem states a bound on the regret of the algorithm. Its proof can be found in Appendix C.
Theorem 3 (The regret of the OFUL algorithm). Assume that for all t and all x ∈ Dt, x, θ∗ ∈ [−1, 1] and let λ ≥ 1. Then, with probability at least 1 − δ, the regret of the OFUL algorithm satisﬁes
∀n ≥ 0, Rn ≤ 4 nd log(λ + nL/d) λ1/2S + R 2 log(1/δ) + d log(1 + nL/(λd)) .

Figure 2 shows the experiments with the new conﬁdence set. The regret of OFUL is signiﬁcantly better compared to the regret of CONFIDENCEBALL of Dani et al. (2008). The ﬁgure also shows a version of the algorithm that has a similar regret to the algorithm with the new bound, but spends about 350 times less computation in this experiment. Next, we explain how we can achieve this computation saving.

5.1 Saving Computation

In this section, we show that we essentially need to recompute θt only O(log n) times up to time n and hence saving computations.1 The idea is to recompute θt whenever det(Vt) increases by a constant factor (1 + C). We call the resulting algorithm the RARELY SWITCHING OFUL algorithm and its pseudo-code is given in Figure 3. As the next theorem shows its regret bound is essentially the same as the regret for OFUL.
Theorem 4. Under the same assumptions as in Theorem 3, with probability at least 1 − δ, for all n ≥ 0, the regret of the RARELY SWITCHING OFUL ALGORITHM satisﬁes

nL Rn ≤ 4 (1 + C)nd log λ + d

√

nL

1

n

λS + R d log 1 +

+ 2 log + 4 d log .

λd

δ

d

1Note this is very different than the common “doubling trick” in online learning literature. The doubling is used to cope with a different problem. Namely, the problem when the time horizon n is unknown ahead of time.

5

Regret

3000 2500 2000 1500 1000
500 0 0

New bound Old bound New bound with rare switching

2000

4000 6000 Time

8000

10000

Figure 2: The application of the new bound to a linear bandit problem. A 2-dimensional linear bandit, where the parameters vector and the actions are from the unit ball. The regret of OFUL is signiﬁcantly better compared to the regret of CONFIDENCEBALL of Dani et al. (2008). The noise is a zero mean Gaussian with standard deviation σ = 0.1. The probability that conﬁdence sets fail is δ = 0.0001. The experiments are repeated 10 times.

Input: Constant C > 0
τ = 1 {This is the last time step that we changed θt} for t := 1, 2, . . . do
if det(Vt) > (1 + C) det(Vτ ) then
(Xt, θt) = argmax(x,θ)∈Dt×Ct−1 θ, x . τ = t. end if
Xt = argmaxx∈Dt θτ , x . Play Xt and observe reward Yt. end for

Figure 3: The RARELY SWITCHING OFUL ALGORITHM

Average regret

0.2
0.15
0.1
0.05
0 0 0.2 0.4 0.6 0.8 1 C
Figure 4: Regret against computation. We ﬁxed the number of times the algorithm is allowed to update its action in OFUL. For larger values of C, the algorithm changes action less frequently, hence, will play for a longer time period. The ﬁgure shows the average regret obtained during the given time periods for the different values of C. Thus, we see that by increasing C, one can actually lower the average regret per time step for a given ﬁxed computation budget.

The proof of the theorem is given in Appendix D. Figure 4 shows a simple experiment with the RARELY SWITCHING OFUL ALGORITHM.
5.2 Problem Dependent Bound Let ∆t be the “gap” at time step t as deﬁned in (Dani et al., 2008). (Intuitively, ∆t is the difference between the rewards of the best and the “second best” action in the decision set Dt.) We consider
6

the smallest gap ∆¯ n = min1≤t≤n ∆t. This includes the case when the set Dt is the same polytope in every round or the case when Dt is ﬁnite.

The regret of OFUL can be upper bounded in terms of (∆¯ n)n as follows.

Theorem 5. Assume that λ ≥ 1 and θ∗ 2 ≤ S where S ≥ 1. With probability at least 1 − δ, for all n ≥ 1, the regret of the OFUL satisﬁes

16R2λS2

64R2λS2L

Rn ≤ ∆¯

log(Ln) + (d − 1) log ∆¯ 2

n

n

dλ + nL2

2

+ 2(d − 1) log d log

+ 2 log(1/δ) + 2 log(1/δ) .

d

The proof of the theorem can be found in the Appendix E.

The problem dependent regret of (Dani et al., 2008) scales like O( d∆2 log3 n), while our bound

scales

like

O(

1 ∆

(log2

n

+

d

log

n

+

d2

log

log

n)),

where

∆

=

inf n

∆¯ n.

6 Multi-Armed Bandit Problem

In this section we show that a modiﬁed version of UCB has with high probability constant regret.

Let µi be the expected reward of action i = 1, 2, . . . , d. Let µ∗ = max1≤i≤d µi be the expected reward of the best arm, and let ∆i = µ∗ − µi, i = 1, 2, . . . , d, be the “gaps” with respect to the best arm. We assume that if we choose action It in round t we obtain reward µIt + ηt. Let Ni,t denote the number of times that we have played action i up to time t, and Xi,t denote the average of the rewards received by action i up to time t. We construct conﬁdence intervals for the expected
rewards µi based on Xi,t in the following lemma. (The proof can be found in the Appendix F.)
Lemma 6 (Conﬁdence Intervals). Assuming that the noise ηt is conditionally 1-sub-Gaussian. With probability at least 1 − δ,

∀i ∈ {1, 2, . . . , d}, ∀t ≥ 0 |Xi,t − µi| ≤ ci,t ,

where

(1 + Ni,t)

d(1 + Ni,t)1/2

ci,t = Ni2,t 1 + 2 log δ . (3)

Using these conﬁdence intervals, we modify the UCB algorithm of Auer et al. (2002) and change the action selection rule accordingly. Hence, at time t, we choose the action

It = argmax Xi,t + ci,t.

(4)

i

We call this algorithm UCB(δ).

The main difference between UCB(δ) and UCB is that the length of conﬁdence interval ci,t depends neither on n, nor on t. This allows us to prove the following result that the regret of UCB(δ) is constant. (The proof can be found in the Appendix G.)
Theorem 7 (Regret of UCB(δ)). Assume that the noise ηt is conditionally 1-sub-Gaussian, with probability at least 1 − δ, the total regret of the UCB(δ) is bounded as

16 2d

Rn ≤

3∆i + ∆i log ∆iδ .

i:∆i >0

Lai and Robbins (1985) prove that for any suboptimal arm j,
log t E Ni,t ≥ D(pj , p∗) ,
where, p∗ and pj are the reward density of the optimal arm and arm j respectively, and D is the KL-divergence. This lower bound does not contradict Theorem 7, as Theorem 7 only states a high

7

800 New bound
600 Old bound

Regret

400

200

0

0

2000 4000 6000 8000 10000

Time

Figure 5: The regret of UCB(δ) against-time when it uses either the conﬁdence bound based on Hoeffding’s inequality, or the bound in (3). The results are shown for a 10-armed bandit problem, where the mean value of each arm is ﬁxed to some values in [0, 1]. The regret of UCB(δ) is improved with the new bound. The noise is a zero-mean Gaussian with standard deviation σ = 0.1. The value of δ is set to 0.0001. The experiments are repeated 10 times and the average is shown, together with the error bars.

probability upper bound for the regret. Note that UCB(δ) takes delta as its input. Because with probability δ, the regret in time t can be t, on expectation, the algorithm might have a regret of tδ. Now if we select δ = 1/t, then we get O(log t) upper bound on the expected regret.
If one is interested in an average regret result, then, with slight modiﬁcation of the proof technique one can obtain an identical result to what Auer et al. (2002) proves.
Figure 5 shows the regret of UCB(δ) when it uses either the conﬁdence bound based on Hoeffding’s inequality, or the bound in (3). As can be seen, the regret of UCB(δ) is improved with the new bound.
Coquelin and Munos (2007), Audibert et al. (2009a) prove similar high-probability constant regret bounds for variations of the UCB algorithm. Compared to their bounds, our bound is tighter thanks to that with the new self-normalized tail inequality we can avoid one union bound. The improvement can also be seen in experiment as the curve that we get for the performance of the algorithm of Coquelin and Munos (2007) is almost exactly the same as the curve that is labeled “Old Bound” in Figure 5.
7 Conclusions
In this paper, we showed how a novel tail inequality for vector-valued martingales allows one to improve both the theoretical analysis and empirical performance of algorithms for various stochastic bandit problems. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. Further, we modify and improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement, stemming from the construction of smaller conﬁdence sets. To our knowledge, ours is the ﬁrst, theoretically well-founded algorithm, whose performance is practical for this latter problem. We also proposed a novel variant of the algorithm with which we can save a large amount of computation without sacriﬁcing performance.
We expect that the novel tail inequality will also be useful in a number of other situations thanks to its self-normalized form and that it holds for stopped martingales and thus can be used to derive bounds that hold uniformly in time. In general, the new inequality can be used to improve deviation bounds which use a union bound (over time). Since many modern machine learning techniques rely on having tight high-probability bounds, we expect that the new inequality will ﬁnd many applications. Just to mention a few examples, the new inequality could be used to improve the computational complexity of the HOO algorithm Bubeck et al. (2008) (when it is used with a ﬁxed δ, by avoiding union bounds, or the need to know the horizon, or the doubling trick) or to improve the bounds derived by Garivier and Moulines (2008) for UCB for changing environments, or the stopping rules and racing algorithms of Mnih et al. (2008).
8

References
Yasin Abbasi-Yadkori, Andra´s Antos, and Csaba Szepesva´ri. Forced-exploration based algorithms for playing in stochastic linear bandits. In COLT Workshop on On-line Learning with Limited Feedback, 2009.
Naoki Abe, Alan W. Biermann, and Philip M. Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37:263293, 2003.
A. Antos, V. Grover, and Cs. Szepesva´ri. Active learning in heteroscedastic noise. Theoretical Computer Science, 411(29-30):2712–2728, 2010.
J.-Y. Audibert, R. Munos, and Csaba Szepesva´ri. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902, 2009a.
Jean-Yves Audibert, Re´mi Munos, and Csaba Szepesva´ri. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 19:1876–1902, 2009b.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002.
Peter Auer. Using upper conﬁdence bounds for online learning. In Proceedings of the 41st Annual Symposium on Foundations of Computer Science (FOCS 2000), pages 270–279. IEEE, 2000.
Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3:397–422, 2002.
S. Bubeck, R. Munos, G. Stoltz, and Cs. Szepesva´ri. Online optimization in X-armed bandits. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, NIPS-21, pages 201–208, 2008.
Nicolo` Cesa-Bianchi and Ga´bor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), 2011.
Pierre-Arnaud Coquelin and Re´mi Munos. Bandit algorithms for tree search. In UAI, 2007.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback. In Rocco Servedio and Tong Zhang, editors, Proceedings of the 21st Annual Conference on Learning Theory (COLT 2008), pages 355–366, 2008.
Victor H. de la Pen˜a, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws. Annals of Probability, 32(3): 1902–1933, 2004.
Victor H. de la Pen˜a, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and Statistical Applications. Springer, 2009.
Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Robust selective sampling from single and multiple teachers. In Adam Tauman Kalai and Mehryar Mohri, editors, Proceedings of the 23rd Annual Conference on Learning Theory (COLT 2010), 2010.
David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100–118, 1975.
A Garivier and E Moulines. On upper-conﬁdence bound policies for non-stationary bandit problems. Technical report, LTCI, Dec 2008.
Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. Machine learning, pages 1–28, 2008.
Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6:4–22, 1985.
Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models with applications to identiﬁcation and control of dynamic systems. The Annals of Statistics, 10(1): 154–166, 1982.
Tze Leung Lai, Herbert Robbins, and Ching Zong Wei. Strong consistency of least squares estimates in multiple regression. Proceedings of the National Academy of Sciences, 75(7):3034–3036, 1979.
9

Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web (WWW 2010), pages 661–670. ACM, 2010.
V. Mnih, Cs. Szepesva´ri, and J.-Y. Audibert. Empirical Bernstein stopping. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, pages 672–679. ACM, 2008. doi: http://doi.acm.org/10.1145/1390156.1390241.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527–535, 1952.
Paat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395–411, 2010.
G.W. Stewart and Ji-guang Sun. Matrix Perturbation Theory. Academic Press, 1990. Thomas J. Walsh, Istva´n Szita, Carlso Diuk, and Michael L. Littman. Exploring compact
reinforcement-learning representations with linear regression. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2009), pages 591–598. AUAI Press, 2009.
10

A Proof of Theorem 1

For the proof of Theorem 1 we will need the following two lemmas. Both lemmas use the same assumptions and notation as the theorem. The ﬁrst lemma is a standard supermartingale argument adapted to randomly stopped, vector valued processes.
Lemma 8. Let λ ∈ Rd be arbitrary and consider for any t ≥ 0

λ

t ηs λ, Xs 1

2

Mt = exp

R − 2 λ, Xs .

s=1

Let τ be a stopping time with respect to the ﬁltration {Ft}∞ t=0. well-deﬁned and
E[Mτλ] ≤ 1 .

Then Mτλ is almost surely

Proof of Lemma 8. We claim that {Mtλ}∞ t=0 is a supermartingale. Let
Dtλ = exp ηt λR, Xt − 12 λ, Xt 2 .
Observe that by conditional R-sub-Gaussianity of ηt we have E[Dtλ | Ft−1] ≤ 1. Clearly, Dtλ is Ft-measurable, as is Mtλ. Further,
E[Mtλ | Ft−1] = E[M1λ · · · Dtλ−1Dtλ | Ft−1] = D1λ · · · Dtλ−1 E[Dtλ | Ft−1] ≤ Mtλ−1 ,
showing that {Mtλ}∞ t=0 is indeed a supermartingale and in fact E[Mtλ] ≤ 1. Now, we argue that Mτλ is well-deﬁned. By the convergence theorem for nonnegative supermartingales, M∞λ = limt→∞ Mtλ is almost surely well-deﬁned. Hence, Mτλ is indeed well-deﬁned independently of whether τ < ∞ holds or not. Next, we show that E[Mτλ] ≤ 1. For this let Qλt = Mmλin{τ,t} be a stopped version of (Mtλ)t. By Fatou’s Lemma, E[Mτλ] = E[lim inft→∞ Qλt ] ≤ lim inft→∞ E[Qλt ] ≤ 1, showing that E[Mτλ] ≤ 1 indeed holds.

The next lemma uses the “method of mixtures” technique (cf. Chapter 11, de la Pen˜a et al. 2009). In fact, the lemma could also be derived from Theorem 14.7 of de la Pen˜a et al. (2009).
Lemma 9 (Self-normalized bound for vector-valued martingales). Let τ be a stopping time with respect to the ﬁltration {Ft}∞ t=0. Then, for δ > 0, with probability 1 − δ,

2

2

det(V τ )1/2 det(V )−1/2

Sτ V −1 ≤ 2R log τ

δ

.

Proof of Lemma 9. Without loss of generality, assume that R = 1 (by appropriately scaling St, this can always be achieved). Let

t
Vt = XsXs
s=1

Mtλ = exp λ, St − 21 λ 2Vt .

Notice that by Lemma 8, the mean of Mτλ is not larger than one.
Let Λ be a Gaussian random variable which is independent of all the other random variables and whose covariance is V −1. Deﬁne
Mt = E[MtΛ | F∞] ,
where F∞ is the tail σ-algebra of the ﬁltration i.e. the σ-algebra generated by the union of the all events in the ﬁltration. Clearly, we still have E[Mτ ] = E[E[MτΛ | Λ]] ≤ 1.

11

Let us calculate Mt. Let f denote the density of Λ and for a positive deﬁnite matrix P let c(P ) = (2π)d/ det(P ) = exp(− 21 x P x)dx. Then,

Mt = exp
Rd

12

λ, St

− 2

λ Vt

f (λ) dλ

= exp − 12 λ − Vt−1St 2Vt + 21 St 2Vt−1
Rd

f (λ) dλ

=

1

exp

1

St

2
−1

1 exp −

λ − V −1St 2 + λ 2

dλ .

c(V )

2 Vt d

2

t

Vt

V

R

Elementary calculation shows that if P is positive semi-deﬁnite and Q is positive deﬁnite

x − a 2P + x 2Q = x − (P + Q)−1P a 2P +Q + a 2P − P a 2(P +Q)−1 .

Therefore,

λ − Vt−1St

2 V

+

λ 2V

=

t

=

λ − (V + Vt)−1St λ − (V + Vt)−1St

2+
V +Vt
2+
V +Vt

Vt−1St 2Vt − St 2(V +Vt)−1 St 2Vt−1 − St 2(V +Vt)−1 ,

which gives

1 Mt = c(V ) exp

1

2

2 St (V +Vt)−1

exp

1 −

λ − (V + Vt)−1St 2

dλ

Rd

2

V +Vt

c(V + Vt)

1

2

det(V ) 1/2

1

2

= c(V ) exp 2 St (V +Vt)−1 = det(V + Vt)

exp 2 St (V +Vt)−1 .

Now, from E[Mτ ] ≤ 1, we obtain

2

det(V + Vτ )1/2

Pr Sτ (V +Vτ )−1 > 2 log δ det(V )1/2





 exp 12 Sτ 2(V +Vτ )−1 

= Pr 

1 > 1





δ−1 det(V + Vτ ) det(V ) 2





 exp 12 Sτ 2(V +Vτ )−1 

≤ E

1





δ−1 det(V + Vτ ) det(V ) 2

= E[Mτ ]δ ≤ δ,

thus ﬁnishing the proof.

Proof of Theorem 1. We will use a stopping time construction, which goes back at least to Freedman (1975). Deﬁne the bad event

Bt(δ) =

ω∈Ω :

St 2V − t 1 > 2R2 log

det(V t)1/2 det(V )−1/2 δ

We are interested in bounding the probability that t≥0 Bt(δ) happens. Deﬁne τ (ω) = min{t ≥ 0 : ω ∈ Bt(δ)}, with the convention that min ∅ = ∞. Then, τ is a stopping time. Further,

Bt(δ) = {ω : τ (ω) < ∞}.
t≥0

12

Thus, by Lemma 9





Pr  Bt(δ) = Pr [τ < ∞]
t≥0

= Pr Sτ V2 − τ 1 > 2R2 log

≤ Pr ≤δ.

Sτ 2V − τ 1 > 2R2 log

det(V τ )1/2 det(V )−1/2 δ
det(V τ )1/2 det(V )−1/2 δ

,τ <∞

B Proof of Theorem 2

We will need the following lemma.

Lemma 10 (Determinant-Trace Inequality). Suppose X1, X2, . . . , Xt ∈ Rd and for any 1 ≤ s ≤ t,

Xs 2 ≤ L. Let V t = λI +

t s=1

XsXs

for some λ > 0. Then,

det(V t) ≤ (λ + tL2/d)d .

Proof of Lemma 10. Let α1, α2, . . . , αd be the eigenvalues of V t. Since V t is positive deﬁnite,

its eigenvalues are positive. Also, note that det(V t) =

t s=1

αs

and

trace(V

t)

=

t s=1

αs.

By

inequality of arithmetic and geometric means

√

α1 + α2 + · · · + αd

d α1α2 · · · αd ≤ d .

Therefore, det(V n) ≤ (trace(V n)/d)d. It remains to upper bound the trace:

t
trace(V n) = trace(λI) + trace XsXs
s=1

t

= dλ +

Xs

2 2

≤

dλ

+

tL2

s=1

and the lemma follows.

Proof of Theorem 2. Let η = (η1, η2, . . . , ηt) . To avoid clutter let X = X1:t and Y = Y1:t. Using

θt = (X = (X = (X

X + λI)−1X X + λI)−1X X + λI)−1X

(Xθ∗ + η) η + (X X + λI)−1(X X + λI)θ∗ − λ(X η + θ∗ − λ(X X + λI)−1θ∗ ,

X + λI)−1θ∗

we get

x θt − x θ∗ = x (X X + λI)−1X η − λx (X X + λI)−1θ∗

= x, X η V −1 − λ x, θ∗ V −1 ,

t

t

−1
where V t = X X + λI. Note that V t is positive deﬁnite (thanks to λ > 0) and hence so is V t , so the above inner product is well-deﬁned. Using the Cauchy-Schwarz inequality, we get

|x θt − x θ∗| ≤ x V −1 t

X η V − t 1 + λ θ∗ Vt−1

≤ x V − t 1

X η V −1 + λ1/2 θ∗ 2 , t

where we used that θ∗ V2 −1 ≤ 1/λmin(V t) θ∗ 22 ≤ 1/λ θ∗ 22. By Theorem 1 with V = λI, for t
any δ > 0, with probability at least 1 − δ,

∀t ≥ 0,

X η V − t 1 ≤ R

2 log

det(V t)1/2 det(λI)−1/2 δ

13

Therefore, on the event where this inequality holds, one also has

∀t ≥ 0, ∀x ∈ Rd

 |x θt−x θ∗| ≤ x V −1 R
t

2 log

det(V t)1/2 det(λI)−1/2 δ

 + λ1/2 θ∗  .

Plugging in x = V t(θt − θ∗) and using θ∗ 2 ≤ S, we get





2
θ − θ ≤ V (θ − θ )

R 2 log det(V t)1/2 det(λI)−1/2 + λ1/2 S . (5)

t∗ Vt

tt

∗  −1
V

δ



t

Now, Vt(θt − θ∗) = −1 θt − θ∗ and therefore dividing both sides by θt − θ∗ gives

Vt

Vt

Vt

θt − θ∗ ≤ R
Vt

2 log

det(V t)1/2 det(λI)−1/2 δ

+ λ1/2 S .

In other words, θ∗ ∈ Ct. Similarly, we can derive the second, worst-case, bound.

C Proof of Theorem 3

Lemma 11. Let {Xt}∞ t=1 be a sequence in Rd, V a d × d positive deﬁnite matrix and deﬁne

Vt =V +

t s=1

XsXs

.

Then,

we

have

that

det(V n)

n

2

log

≤

det(V )

X . t V − t−11

t=1

Further, if Xt 2 ≤ L for all t, then

n
min
t=1

1, Xt 2V − t−11

≤ 2(log det(V n) − log det V ) ≤ 2(d log((trace(V ) + nL2)/d) − log det V ) ,

and ﬁnally, if λmin(V ) ≥ max(1, L2) then

n 2

det(V n)

Xt V − t−11 ≤ 2 log det(V ) .

t=1

Proof. Elementary algebra gives

−1/2

−1/2

det(V n) = det(V n−1 + XnXn ) = det(V n) det(I + V n−1 Xn(V n Xn) )

n

= det(V n−1) (1 + Xn−1 V2 − n−1 1 ) = det(V ) 1 + Xt V2 − t−11 , (6)
t=1

where we used that all the eigenvalues of a matrix of the form I + xx are one except one eigenvalue, which is 1 + x 2 and which corresponds to the eigenvector x. Using log(1 + t) ≤ t,
we can bound log det(V t) by

t

log det(V t) ≤ log(det(V )) +

X . 2
t V − t−11

t=1

Combining x ≤ 2 log(1 + x), which holds when x ∈ [0, 1], and (6), we get

n
min
t=1

1, Xt 2V − t−11

n
≤ 2 log 1 + Xt 2V − t−11
t=1

= 2(log det(V t) − log det V ).

14

The trace of V n is bounded by trace(V ) + nL2 if Xt 2 ≤ L. Hence, det(V n) =

trace(V )+tL2 d

d

and therefore,

d i=1

λi

≤

log det(V t) ≤ d log((trace(V ) + tL2)/d),

ﬁnishing the proof of the second inequality. The sum

n t=1

Xt 2V −1

can itself be up-

t−1

per bounded as a function of log det(V t) provided that λmin(V ) is large enough. Notice

Xt

2 V −1

≤ λ−m1in(V t−1)

Xt−1

2 ≤ L2/λmin(V ). Hence, we get that if λmin(V ) ≥ max(1, L2),

t−1

det(V n) n

2

det(V n)

log

≤

det V

Xt

V − t−11 ≤ 2 log

. det(V )

t=1

Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea

goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around

Theorem 11.7 in the book by Cesa-Bianchi and Lugosi (2006)). Note that Lemmas B.9–B.11 of

Rusmevichientong and Tsitsiklis (2010) also give a bound on

t k=1

mk−1

2V −1 , with an essen-

k−1

tially identical argument. Alternatively, one can use the bounding technique of Auer (2002) (see the

proof of Lemma 13 there on pages 412–413) to derive a bound like

t k=1

mk−1 V2 −1

≤ Cd log t

k−1

for a suitable chosen constant C > 0.

Proof. Lets decompose the instantaneous regret as follows: rt = θ∗, x∗ − θ∗, Xt ≤ θt, Xt − θ∗, Xt

(since (Xt, θt) is optimistic)

= θt − θ∗, Xt

= θt−1 − θ∗, Xt + θ˜t − θt−1, Xt

≤ θt−1 − θ∗ V t−1 Xt V − t−11 + θ˜t − θt−1 V t−1 Xt V − t−11

≤ 2 βt−1(δ) Xt V −1 ,

(7)

t−1

where the one but last step holds by Cauchy-Schwarz. Using (7) and the fact that rt ≤ 2, we get that

rt ≤ 2 min( βt−1(δ) Xt V −1 , 1) ≤ 2 βt−1(δ) min( Xt V −1 , 1) .

t−1

t−1

Thus, thanks to λ ≥ 1, with probability at least 1 − δ, for all n ≥ 0

Rn ≤

n
n rt2 ≤
t=1

n

8βn(δ)n

min( Xt 2V −1 , 1) ≤ 4 t

t=1

βn(δ)n log(det(Vn))

≤ 4 nd log(λ + nL/d) λ1/2S + R 2 log(1/δ) + d log(1 + nL/(λd)) ,

where the last two steps follow from Lemma 11.

D Proof of Theorem 4

First, we prove the following lemma:

Lemma 12. Let A, B and C be positive semi-deﬁnite matrices such that A = B + C. Then, we

have that

x Ax det(A)

sup

≤

.

x=0 x Bx det(B)

15

Proof. We consider ﬁrst a simple case. Assume that C = mm where m ∈ Rd and B positive deﬁnite. Let x = 0 be an arbitrary vector. Using the Cauchy-Schwartz inequality, we get
(x m)2 = (x B1/2B−1/2m)2 ≤ B1/2x 2 B−1/2m 2 = x 2B m 2B−1 .

Thus,

x (B + mm )x ≤ x Bx + x 2B m 2B−1 = (1 + m 2B−1 ) x 2B

and so

x Ax

2

x

≤1+ Bx

m B−1 .

We also have that

det(A) = det(B + mm ) = det(B) det(I + B−1/2m(B−1/2m) ) = det(B)(1 + m 2B−1 ),

thus ﬁnishing the proof of this case.

If A = B + m1m1 + · · · + mt−1mt−1, then deﬁne Vs = B + m1m1 + · · · + ms−1ms−1 and use

x Ax = x Vtx x Vt−1x . . . x V2x . x Bx x Vt−1x x Vt−2x x Bx By the above argument, since all the terms are positive, we get

x Ax ≤ det(Vt) det(Vt−1) . . . det(V2) = det(Vt) = det(A) . x Bx det(Vt−1) det(Vt−2) det(B) det(B) det(B)
This ﬁnishes the proof of this case.
Now, if C is a positive deﬁnite matrix, then the eigendecomposition of C gives C = U ΛU , where U is orthonormal and Λ is positive diagonal matrix. This, in fact gives that C can be written as the sum of at most d rank-one matrices, ﬁnishing the proof for the general case.

Proof of Theorem 4. Let τt be the smallest time step ≤ t such that θt = xτt . By an argument similar to the one used in Theorem 3, we have
rt ≤ (θˆτt − θ∗) xt + (θ˜τt − θˆτt ) xt .
We also have that for all θ ∈ Cτt−1 and any x ∈ Rd,

|(θ − θˆτt ) x| ≤ Vt1/2(θ − θˆτ ) x Vt−1x

≤ Vτ1t/2(θ − θˆτt )

det(Vt) det(Vτt )

√ ≤ 1 + C Vτ1/2(θ − θˆτ ) x
t

x Vt−1x Vt−1x

≤ (1 + C)βτt x Vt−1x,
where the second step follows from Lemma 12, and the third step follows from the fact that at time t we have det(Vt) < (1 + C) det(Vτt ). The rest of the argument is identical to that of Theorem 3. We conclude that with probability at least 1 − δ, for all n ≥ 0,

Rn ≤ 4 (1 + C)nd log(λ + nL/d) λ1/2S + R 2 log 1/δ + d log(1 + nL/(λd))

E Proof of Theorem 5
First we state a matrix perturbation theorem from Stewart and Sun (1990). 16

Theorem 13 (Stewart and Sun (1990), Corollary 4.9). Let A be a d×d symmetric matrix with eigenvalues ν1 ≥ ν2 ≥ . . . ≥ νd, E be a symmetric d × d matrix with eigenvalues e1 ≥ e2 ≥ . . . ≥ ed, and V = A + E denote a symmetric perturbation of A such that the eigenvalues of V are ν˜1 ≥ ν˜2 ≥ . . . ≥ ν˜d. Then, for i = 1, 2, . . . , d,
ν˜i ∈ [νi + ed, νi + e1] .

Proof of Theorem 5. First we bound the regret in terms of log det(VT ). We have that

Rn = n rt ≤ n rt2 ≤ 16βn(δ) log(det(VT )), (8)

∆

∆

t=1

t=1

where the ﬁrst inequality follows from the fact that either rt = 0 or ∆ < rt, and the second inequality can be extracted from the proof of Theorem 3. Let bt be the number of times we have played a sub-optimal action (an action xs for which θ∗, x∗ − θ∗, xs ≥ ∆) up to time t. Next we bound log det(Vt) in terms of bt. We bound the eigenvalues of Vt by using Theorem 13.

Let Et = ts:xs=x∗ xsxs and At = Vt − Et = (t − bt)x∗x∗ . The only non-zero eigenvalue
of (t − bt)x∗x∗ is (t − bt)L∗, where L∗ = x∗ x∗ ≤ L. Let the eigenvalues of Vt and Et be λ1 ≥ · · · ≥ λd and e1 ≥ · · · ≥ ed respectively. By Theorem 13, we have that

λ1 ∈ [(t − bt)L∗ + ed, (t − bt)L∗ + e1] and ∀i ∈ {2, . . . , d}, λi ∈ [ed, e1].

Thus,

d
det(Vt) = λi ≤ ((t − bt)L∗ + e1)e1d−1 ≤ ((t − bt)L + e1)e1d−1.
i

Therefore,

log det(Vt) ≤ log((t − bt)L + e1) + (d − 1) log e1.

Because trace(E) = ts:xs=x∗ trace(xsxs ) ≤ Lbt, we conclude that e1 ≤ Lbt. Thus,

log det(Vt) ≤ log((t − bt)L + Lbt) + (d − 1) log(Lbt)

= log(Lt) + (d − 1) log(Lbt).

(9)

With some calculations, we can show that

dλ + tL2

12

βt log det Vt ≤ 4R2λS2(2 log(1/δ) + log det Vt)2 ≤ 4R2λS2 d log

+ 2 log ,

d

δ

(10)

where the second inequality follows from Lemma 11. Hence,

16βt

64R2λS2

dλ + tL2

12

bt ≤ ∆2 log(det(Vt)) ≤ ∆2

d log

+ 2 log ,

d

δ

(11)

where the ﬁrst inequality follows from R(t) ≥ bt∆. Thus, with probability 1 − δ, for all n ≥ 0,

Rn

≤ 16βn log(det(Vn)) ∆
≤ 64R2λS2 (log(det(Vn)) + 2 log(1/δ))2 ∆
≤ 16R2λS2 (log(Ln) + (d − 1) log(Lbn) + 2 log(1/δ))2 ∆

16R2λS2

64R2λS2L

≤ ∆

log(Ln) + (d − 1) log ∆2 + 2(d − 1) log

dλ + nL2

d log

+ 2 log(1/δ)

d

2
+ 2 log(1/δ)

where the ﬁrst step follows from (8), the second step follows from the ﬁrst inequality in (10), the third step follows from (9), and the last step follows from the second inequality in (11).

17

F Proof of Lemma 6

Proof. Fix an arm i. We apply Theorem 1 with d = 1, Xt = t ∈ {0, 1} where depending on

whether we have pulled the arm i in time step t or not (i.e. an optional skipping process). Using

V = I = 1, we have V t = 1 +

t s=1

s = 1 + Ni,t and thus we get

| St V −1 =
t

ts=1 sηs| . 1 + Ni,t

We also have log det(V t) = log(1 + Ni,t). Thus, we get, with probability 1 − δ

t

(1 + Ni,t)1/2

∀t ≥ 0, sηs ≤ (1 + Ni,t) 1 + 2 log δ . (12)

s=1

Diving through by Ni,t we get

1t

(1 + Ni,t)

(1 + Ni,t)1/2

∀t ≥ 0, |Xt,i − µi| = Ni,t s=1 sηs ≤ Ni2,t 1 + 2 log δ .

Replacing δ by δ/d and a union bound over all arms ﬁnishes the proof.

If we apply Doob’s optional skipping and Hoeffding-Azuma, with a union bound (see, e.g., the paper of Bubeck et al. (2008)), we would get, for any 0 < δ < 1, t ≥ 2, with probability 1 − δ,

s

2t

∀s ∈ {0, . . . , t}, kηk ≤ 2Ni,s log δ . (13)

k=1

The major difference between these bounds is that (13) depends explicitly on t, while (12) does not. This has the positive effect that one need not recompute the bound if Ni,t does not grow, which helps e.g. in the paper of Bubeck et al. (2008) to improve the computational complexity of the HOO algorithm. Also, the coefﬁcient of the leading term in (12) under the square root is 1, whereas in (13) it is 2.

Instead of a union bound, it is possible to use a “peeling device” to replace the conservative log t
factor in the above bound by essentially log log t. This is done e.g. in Garivier and Moulines (2008) in their Theorem 22.2 From their derivations, the following one sided, uniform bound can
be extracted (see Remark 24, page 19): For any 0 < δ < 1, t ≥ 2, with probability 1 − δ,

s

4 Ni,s

6 log t

∀s ∈ {0, . . . , t}, kηk ≤ 1.99 log δ . (14)

k=1

As noted by Garivier and Moulines (2008), due to the law of iterated logarithm, the scaling of the right-hand side as a function of t cannot be improved in the worst-case. However, this leaves open the possibility of deriving a maximal inequality which depends on t only through Ni,t.

G Proof of Theorem 7

Proof. Suppose the conﬁdence intervals do not fail. If we play action i, the upper estimate of the action is above µ∗. Hence,
ci,s ≥ ∆i . 2
Substituting ci,s and squaring gives

Ni2,s − 1

Ni2,s

4

d(1 + Ni,s)1/2

Ni,s + 1 ≤ Ni,s + 1 ≤ ∆2i 2 log δ .

2They give their theorem as ratios, which they should not, since their inequality then fails to hold for Ni,t = 0. However, this is easy to remedy by reformulating their result as we do it here.

18

By using Lemma 8 of Antos et al. (2010), we get that for all s ≥ 0

16 2d Ni,s ≤ 3 + ∆2i log ∆iδ .

Thus, using Rn = bounded by

i=i∗ ∆iNi,n, we get that with probability at least 1 − δ, the total regret is

16 2d

Rn ≤

3∆i + ∆i log ∆iδ .

i:∆i >0

19

