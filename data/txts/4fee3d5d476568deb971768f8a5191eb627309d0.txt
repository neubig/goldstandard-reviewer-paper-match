Stability of Gradient Learning Dynamics in Continuous Games: Vector Action Spaces

arXiv:2011.05562v2 [cs.GT] 14 Jan 2021 y1 (player 2) y1 (fast player)

Benjamin J. Chasnov, Daniel Calderone, Behc¸et Ac¸ıkmes¸e, Samuel A. Burden, Lillian J. Ratliff

Abstract— Towards characterizing the optimization landscape of games, this paper analyzes the stability of gradientbased dynamics near ﬁxed points of two-player continuous games. We introduce the quadratic numerical range as a method to characterize the spectrum of game dynamics and prove the robustness of equilibria to variations in learning rates. By decomposing the game Jacobian into symmetric and skew-symmetric components, we assess the contribution of a vector ﬁeld’s potential and rotational components to the stability of differential Nash equilibria. Our results show that in zero-sum games, all Nash are stable and robust; in potential games, all stable points are Nash. For general-sum games, we provide a sufﬁcient condition for instability. We conclude with a numerical example in which learning with timescale separation results in faster convergence.
I. INTRODUCTION
The study of learning in games is experiencing a resurgence in the control theory [21], [23], [24], optimization [13], [15], and machine learning [5], [7], [8], [10], [16] communities. Partly driving this resurgence is the prospect for game-theoretic analysis to yield machine learning algorithms that generalize better or are more robust. A natural paradigm for learning in games is gradient play since updates in large decision spaces can be performed locally while still guaranteeing local convergence in many problems [7], [15].
Towards understanding the optimization landscape in such formulations, dynamical systems theory is emerging as a principal tool for analysis and ultimately synthesis [1]–[3], [13], [14]. One of the primary means to understand the optimization landscape of games is the eigenstructure and spectrum of the Jacobian of the learning dynamics in a neighborhood of a stationary point. However, as has been demonstrated [13], not all attractors of the learning dynamics are game theoretically meaningful. Furthermore, structural heterogeneity in the learning algorithms employed by players can drastically change the convergence behavior.
The local stability of a hyperbolic ﬁxed point in a nonlinear system can be assessed by examining the eigenstructure of the linearized dynamics [11], [22]. However, in a game context extra structure comes from the underlying game—that is, players are constrained to move only along directions over which they have control. They can only
B. Chasnov, S. Burden, and L.J. Ratliff are with the Department of Electrical and Computer Engineering, University of Washington, Seattle, WA 98115 {bchasnov,sburden,ratliffl}@uw.edu
D. Calderone and B. Ac¸ıkmes¸e are with the Department of Aeronautics and Astronautics, University of Washington, Seattle, WA 98115 {djcal,behcet}@uw.edu
Funding for this work is provided by NSF Award #1836819 and NIH 5T90DA032436-09.

Potential Rotational With timescale separation ( >1)

x1 (player 1) x1 (player 1)

x1 (slow player) x1 (slow player)

Fig. 1. Game dynamics with rotational components can converge at a faster rate with timescale separation. We plot slices of the vector ﬁeld and learning trajectories of mostly potential and mostly rotational learning dynamics from Example 4. The game Jacobian at the equilibrium decomposes into J = (1 − ε)S + εA, where S = S is symmetric and A = −A is skewsymmetric. For the mostly potential system (red, ε = 0.1), players converge to the equilibrium without cycling. For the mostly rotational system (blue, ε = 0.9), players without timescale separation cycle around the equilibrium. Players with timescale separation take advantage of the rotational vector ﬁeld to converge faster to the equilibrium, as shown in the right-most plot. See Fig. 4 for a continuation of this example.

control their individual actions, as opposed to the entire state of the dynamical system corresponding to the learning rules being applied by the agents. It has been observed in earlier work that not all stable attractors of gradient play are local Nash equilibria and not all local Nash equilibria are stable attractors of gradient play [13]. Furthermore, changes in players’ learning rates—which corresponds to scaling rows of the Jacobian—can change an equilibrium from being stable to unstable and vice versa [7].
To summarize, there is a subtle but extremely important difference between learning dynamics and traditional nonlinear dynamical systems: alignment conditions are important for distinguishing between equilibria that have gametheoretic meaning versus those which are simply stable attractors of learning rules. Furthermore, features of learning dynamics such as learning rates can play an important role in shaping not only equilibria but also alignment properties. Motivated by these observations, along with the recent resurgence of applications of learning in games in control, optimization, and machine learning, in this paper we provide an in-depth analysis of the spectral properties of gradientbased learning in two-player continuous games.
Contributions: Our main results are bounds on the spectrum of gradient-based learning dynamics near equilibria (Theorem 1, Theorem 2) and robustness guarantees of differential Nash equilibria to variations in learning rates (Theorem 3, Theorem 4) in two important classes of twoplayer continuous games: zero-sum (adversarial) and potential (implicitly cooperative). Moreover, we prove a sufﬁcient condition for instability of learning dynamics (Theorem 5). Finally, we include numerical examples (Section V) which

provide further insights into the theoretical results. More restrictive results applicable only in scalar action
spaces were presented in an earlier conference paper [6]. The present paper concerns the more general case of vector action spaces, introduces a novel decomposition of a general-sum game into its zero-sum and potential pieces, and applies a new analysis tool (quadratic numerical rage) to study stability of learning.
II. PRELIMINARIES
This section contains game-theoretic preliminaries, mathematical formalism, and a description of the gradient-based learning paradigm studied in this paper.
A. Game-Theoretic Preliminaries
A 2-player continuous game G = (f1, f2) is a pair of cost functions deﬁned on a shared strategy space X = X1 × X2 where player (agent) i ∈ I = {1, 2} has cost fi : X → R. In this paper, the results apply to games with smooth costs fi ∈ Cr(X, R) for r = 2. Agent i’s set of feasible strategies is the di-dimensional open and precompact set Xi ⊆ Rdi .
The most common and arguably natural notion of an equilibrium in continuous games is due to Nash [18].
Deﬁnition 1 (Local Nash equilibrium): A joint action proﬁle x = (x1, x2) ∈ W1 × W2 ⊂ X1 × X2 is a local Nash equilibrium on W1 × W2 if, for each player i ∈ I, fi(xi , x−i) ≤ fi(xi, x−i), ∀xi ∈ Wi. A local Nash equilibrium can equivalently be deﬁned as in terms of best response maps: xi ∈ arg minxi fi(xi, x−i). From this perspective, local optimality conditions for players’ optimization problems give rise to the notion of a differential Nash equilibrium [20], [21]; non-degenerate differential Nash are known to be generic and structurally stable amongst local Nash equilibria in sufﬁciently smooth games [19]. Let Difi denote the derivative of fi with respect to xi and, analogously, let Di(Difi) ≡ Di2fi be player i’s individiaul Hessian.
Deﬁnition 2: For continuous game G = (f1, f2) where fi ∈ C2(X1 × X2, R), a joint action proﬁle (x1, x2) ∈ X1 × X2 is a differential Nash equilibrium if Difi(x1, x2) = 0 and Di2fi(x1, x2) > 0 for each i ∈ I. A differential Nash equilibrium is a strict local Nash equilibrium [20, Thm. 1]. Furthermore, the conditions Difi(x ) = 0 and Di2fi(x ) ≥ 0 are necessary for a local Nash equilibrium [20, Prop. 2].
Learning processes in games, and their study, arose as one of the explanations for how players grapple with one another in seeking an equilibrium [9]. In the case of sufﬁciently smooth games, gradient-based learning is a natural learning rule for myopic players.
B. Gradient-based Learning as a Dynamical System
At timestep t ∈ N, a myopic agent i updates its current action xi(t) by following the gradient of its individual cost fi given the decisions of its competitors x−i. The synchronous

adaptive process that arises is the discrete-time dynamical system

xi(t + 1) = xi(t) − γiDifi(xi(t), x−i(t))

(1)

for each i ∈ I where Difi is the gradient of player i’s cost with respect to xi and γi is player i’s learning rate.
1) Stability: Recall that a matrix A is called Hurwitz
if its spectrum lies in the open left-half complex plane C◦−. Furthermore, we often say such a matrix is stable in particular when A corresponds to the dynamics of a linear system x˙ = Ax or the linearization of a nonlinear system around a ﬁxed point of the dynamics.1
It is known that (1) will converge locally asymptotically
to a differential Nash equilibrium if the local linearization is
a contraction [7]. Let

g(x) = (D1f1(x), D2f2(x))

(2)

be the vector of individual gradients and let Dg(x) be its Jacobian—i.e., the game Jacobian. Further, let spec A ⊂ C denote the spectrum of the matrix A, and ρ(A) its spectral radius. Then, x is locally exponentially stable if and only if ρ(I − γ1ΛDg(x )) < 1, where τ = γ2/γ1 and Λ = blockdiag(Id1 , τ Id2 ) is a diagonal matrix and Idi is the identity matrix of dimension di. The map I − γ1ΛDg(x ) is the local linearization of (1). Hence, to study stability (and, in turn, convergence) properties it is useful to analyze the spectrum of not only the map I − γ1ΛDg(x ) but also ΛDg(x ) itself.
2) Partitioning the Game Jacobian: Let x = (x1, x2) be a joint action proﬁle such that g(x ) = 0. Towards better understanding the spectral properties of Dg(x ) (respectively, ΛDg(x )), we partition Dg(x ) into blocks:

J (x ) = −D12f1(x ) −D21f2(x )

−D12f1(x ) = J11

−D22f2(x )

J21

J12 . J22
(3)

A differential Nash equilibrium (the second order conditions

of which are sufﬁcient for a local Nash equilibrium) is such

that J11 < 0 and J22 < 0. On the other hand, as noted above, J is Hurwitz or stable if spec (J(x )) ⊂ C◦−. Moreover,
since the diagonal blocks are symmetric, J is similar to the

matrix in Fig 2. For the remainder of the paper, we will study

the Dg at a given ﬁxed point x as deﬁned in (3).

J(x , y ) ∼

Fig. 2. Similarity: the game Jacobian in (3) is similar to a matrix with diagonal blockdiagonals. The off-diagonals are arbitrary.

3) Classes of Games: Different classes of games can be characterized via J. For instance, a zero-sum game, where f1 ≡ −f2, is such that J12 = −J21. On the other hand, a game G = (f1, f2) is a potential game if and only if D12f1 ≡ D21f2 [17, Thm. 4.5], which implies that J12 = J21.
1The Hartman-Grobman theorem [22] states that around any hyperbolic ﬁxed point of a nonlinear system, there is a neighborhood on which the nonlinear system is stable if the spectrum of the Jacobian lies in C◦−.

C. Spectrum of Block Matrices
One useful tool for characterizing the spectrum of a block operator matrix is the numerical range and quadratic numerical range, both of which contain the operator’s spectrum [25] and therefore all of its eigenvalues. The numerical range of J is deﬁned by

W (J ) = { J x, x : x ∈ Cd1+d2 , x = 1} ⊂ C,

and is convex. Given a block operator J, let

Jv,w = J11v, v J12w, v

(4)

J21v, w J22w, w

where v ∈ Cd1 and w ∈ Cd2 . The quadratic numerical range of J, deﬁned by

W 2(J ) =

spec(Jv,w ),

(5)

v ∈S1 ,w∈S2

is the union of the spectra of (4) where spec(·) denotes the spectrum of its argument and Si = {v ∈ Cdi : v = 1}, It is, in general, a non-convex subset of C. The quadratic numerical range (5) is equivalent to the set of solutions of the characteristic polynomial
λ2 − λ( J11v, v + J22w, w ) + J11v, v J22w, w (6)
− J12v, w J21w, v = 0
for v ∈ S1 and w ∈ S2. We use the notation Jx, y = x∗Jy to denote the inner product. Note that W 2(J) is a subset of W (J) and, as previously noted, contains spec(J). Albeit non-convex, W 2(J) provides a tighter characterization of the spectrum.2
Observing that the quadratic numerical range for a block 2 × 2 matrix J derived from a game on a ﬁnite dimensional Euclidean space reduces to characterizing the spectrum of 2 × 2 matrices, we ﬁrst characterize stability properties of scalar 2-player continuous games.

III. STABILITY IN ZERO-SUM AND POTENTIAL GAMES
In this section, we give stability results for 2-player continuous games on vector action spaces. Consider a game (f1, f2). Recall from the preliminaries that f1, f2 ∈ C2(X1× X2, R) and X1 ⊆ Rd1 , X2 ⊆ Rd2 , are di-dimensional actions spaces. Let x be a ﬁxed point of (2) such that g(x ) = 0. We study the gradient learning dynamics given in (1) near ﬁxed points x by analyzing the spectral properties of the Jacobian of g.

A. Jacobian Decomposition

We decompose the game Jacobian,

J (x ) = J11 P + 0 Z ,

(7)

P J22

−Z 0

where

P

=

1 2

(J12

+

J21

)

and

Z

=

1 2

(J12

−

J21

).

As

we

will

see, P represents the potential-like part and Z represents the

zero-sum part. The spectrum of J(x ) is contained in the

2There are numerous computational approaches for estimating the W (·) and W 2(·) (see, e.g., [12, Sec. 6]).

quadratic numerical range W2(J(x )) which contains the spectrum of the matrices

a p+z

Jv,w = p∗ − z∗ d

(8)

where a = J11v, v , d = J22w, w p = P v, w , and z = Zw, v constructed for unit-length complex numbers
v ∈ S1, w ∈ S2. Hence, to show the stability of a particular ﬁxed point x , we must show that for all v ∈ S1, w ∈ S2, the spectrum of (8) is contained in the left-half complex plane.
For game (f1, f2) with Jacobian J(x ), deﬁne the following for i = 1, 2: λ−i = min spec(Jii), λ+i = max spec(Jii). Additionally, deﬁne

λ− = min{λ−1 , λ−2 }, λ+ = max{λ+1 , λ+2 },

λ = 21 (λ−1 + λ−2 ), λ = 21 (λ+1 + λ+2 ).

These terms depend on the spectrum of the players’ individual Hessians and will be useful in deriving bounds on the spectrum of J(x ).

B. Zero-sum games (P = 0, Z = J12)

The next theorem is our main result on zero-sum games, giving tight bounds on the spectrum of J (i.e. bounds on the real and imaginary eigenvalues) near ﬁxed points of the game. Recall that for zero-sum game (f, −f ), the interaction term is Z = −D12f (x).
Theorem 1 (Spectrum of Zero-Sum Game Dynamics): Consider a zero-sum game G = (f, −f ) and ﬁxed point x . The Jacobian J(x ) = −Dg(x ) of the dynamics x˙ = −g(x) is such that

spec (J(x )) ∩ R ⊂ λ−, λ+

(9)

and spec (J(x ))\R is contained in

z ∈ C : Re(z) ∈ λ, λ , |Im(z)| ≤ Z . (10)
Furthermore, if λ+2 < λ−1 or λ+1 < λ−2 then the following two implications hold for δ = λ−1 − λ+2 or δ = λ−2 − λ+1 , respectively: (i) Z ≤ δ/2 =⇒ spec(J(x)) ⊂ R; (ii) Z > δ/2 =⇒ spec(J(x))\R ⊂ {z ∈ C : |Im(z)| ≤
Z 2 − δ2/4}.
Proof: Observe that det(Jv,w(x) − λI) = det(Jv,w(x) − λ¯I) for v ∈ S1 and w ∈ S2 since D12f (x) and −D22f (x) are symmetric, which implies that W 2(J(x)) = W 2(J(x))∗. Since −w∗D12f (x) vv∗D12f (x)w ≤ 0, (9) and (10) follow from [25, Prop. 1.2.6], and (i) and (ii) follow from [26, Lem. 5.1-(ii)].
The following result shows that for zero-sum games, all differential Nash equilibria are stable under the gradient dynamics.
Corollary 1 (Stability in Zero-Sum Games): Consider a zero-sum game G = (f, −f ) on ﬁnite dimensional action spaces X1, X2. If x is a differential Nash equilibrium of G, then x is a locally stable equilibrium of x˙ = −g(x).
Proof: From Theorem 1, we have that the real parts of the spectrum of J(x ) are upper-bounded by λ+. If x is

a differential Nash equilibrium, then λ+ < 0. Thus, x is a locally exponentially stable equilibrium.
While the Corollary above appears in [13, Prop. 3.7], the novelty is showing that it is the special result of Theorem 1.
Remark 1: Zero-sum games can have stable non-Nash equilibria. Players can get stuck at these spurious attractors of the learning dynamics where the individual Hessians are not necessarily positive deﬁnite, i.e. players may converge to a point that is not a local minimum of their own cost.
The following applies Theorem 1 and is an example of Remark 1.
Example 1: Consider the game (f, −f ) with cost f : R2× R2 → R given by
f (x, y) = −x21 + 3x22 − (2y12 + 6y22) + b(2y1x1 + y2x2)
with z ∈ R. Direct comput√ation shows that the origin is a stable equilibrium for |b| > 2, i.e. b needs to be sufﬁciently large enough for the dynamics to be stable. Moreover, by Theorem 1, the imaginary parts of the spectrum of the game Jacobian are bounded by ±2|b|. This example demonstrates that interaction in zero-sum games can be necessary for stability.

C. Potential games (P = J12, Z = 0)
Recall that for potential games with potential function φ, the interaction term is P = −D12φ(x).
Theorem 2 (Spectrum of Potential Game Dynamics): Consider a potential game G = (f1, f2). Let

δ± = P tan

1 arctan

2P

.

2

|λ±1 − λ±2 |

The Jacobian J(x) = −Dg(x) of the dynamics x˙ = −g(x) at ﬁxed points x is such that spec J(x) ⊂ R and

λ− − δ− ≤ min spec (J (x )) ≤ λ− λ+ ≤ max spec (J (x )) ≤ λ+ + δ+.

(11a) (11b)

Furthermore, if λ+2 < λ−1 , then spec J (x ) ∩ (λ+2 , λ−1 ) is empty. If λ+1 < λ−2 , then spec J (x ) ∩ (λ−1 , λ+2 ) is empty.
Proof: Inequalities in (11) follow from [25, Prop. 1.2.4] and last statements follow from [25, Cor. 1.2.3].
The following result shows that for potential games, all stable equilibria of the gradient dynamics are Nash.
Corollary 2 (Stability in Potential Games): Consider a potential game G = (f1, f2) on ﬁnite dimensional action spaces X1, X2. If x is a locally exponentially stable equilibrium of x˙ = −g(x), then x is a differential Nash equilibrium of G.
Proof: If x is stable, then max spec J(x) < 0. From (11b) we have that max{λ+1 , λ+2 } < 0. Therefore x is a differential Nash equilibrium.
Remark 2: Potential games can have unstable Nash equilibria. That is, players can have local minimum of their costs which the gradient learning dynamics cannot converge to due to contribution of the interaction term.
The next example applies Theorem 2 and is an example of Remark 2.

Example 2: Consider the game (f1, f2) with costs fi : R2 × R2 → R, i = 1, 2, given by
f1(x, y) = x21 + 2x22 + p(x1y1 + x2y2), f2(x, y) = 3y12 + 4y22 + p(x1y1 + x2y2)
with p ∈ R. Direct c√omputation shows that the origin is unstable for |p| > 2 3, i.e. in contrast to the zero-sum case in Example 1, larger interaction term causes instability. By Theorem 2, we have that δ = p tan(arctan(p/2)/2) =
p2 + 4 − 2. Thus, the game Jacobian has eigenvalues that are in [−8 − δ, −2 + δ]. This example demonstrates that in potential games, a lower bound on the interaction term can be necessary for stability.
Remark 3: In the setting of Theorem 2, we remark that if J22 is invertible (without loss of generality), then the equilibrium is stable if and only if the Schur complement of J (x) is negative, i.e. J11 − P J2−21P < 0. Corollary 2 ensures that this equilibrium is also a differential Nash equilibrium. The proof of this statement is immediate from the properties of deﬁnite symmetric matrices (see, e.g., [4]).
D. Robustness to Variation in Time-scale Separation
Recall that Λ = blockdiag(Id1 , τ Id2 ) where τ is the learning rate ratio of the two players (Sec. II-B.2). Below, we study the stability of x˙ = −Λg(x). Our ﬁrst result in this setting shows that differential Nash equilibria in zero-sum games are robust in variation in learning rates.
Theorem 3 (Robuesntess of Nash in Zero-sum Games): Consider a zero-sum game G = (f1, f2) = (f, −f ) with game Jacobian J(x) = −ΛDg(x). Suppose that x is a differential Nash equilibrium. Then, x is a locally stable equilibrium of x˙ = −Λg(x) for any learning rate ratio τ .
Proof: First, observe that a = J11v, v and d = J22w, w are negative real numbers for any v ∈ S1 and w ∈ S2 by assumption that x is a differential Nash equilibrium, i.e. −Di2fi(x ) < 0 for each i ∈ {1, 2}. Second, observe that for zero-sum games, z = Zw, v = − Zv, w ∗. Therefore, for x to be stable, the eigenvalues of
az Jv,w = −τ z∗ τ d
must all be negative. Hence, we compute the trace and determinant conditions to be tr(Jv,w) = λ1+λ2 = a+τ d and det(Jv,w) = λ1λ2 = τ (ad+|z|2). Notice that, τ (ad+|z|2) > 0 ⇐⇒ ad + |z|2 > 0, and a + τ d < 0 ⇐⇒ a + d < 0. Since a, d < 0 and τ > 0, both of the trace and determinant conditions for stability are satisﬁed, i.e. tr(Jv,w) < 0 and det(Jv,w) > 0. Hence, x is a stable equilibrium of x˙ = −Λg(x).
The stability of x˙ = −Λg(x) implies that there exists a range of learning rates γ such that x(t + 1) = x(t) − γΛg(x(t)) is locally asymptotically stable.
On the other hand, differential Nash equilibria of potential games are not robust to variation in learning rates in general. However, the following theorem provides a sufﬁcient condition that guarantees its robustness.

W (J11) δ W (J22) λ− λ λ+1 λ−2 λ λ+

J12 2 − δ2/4 C

(a) Zero-sum game where δ = λ−2 − λ+1 > 0 and J12 > δ/2

δP− W (J11)

W (J22) δP+ R

λ−

λ+1

λ−2

λ+

(b) Potential game where λ−2 − λ+1 > 0.

Fig. 3. Spectrum of learning dynamics near a ﬁxed point in zero-sum and potential games. We illustrate Theorem 1 (a, zero-sum game) and Theorem 2 (b, potential game). The highlighted and thick regions contain the spectrum of the linearized dynamics.

Theorem 4 (Robustness of Nash in Potential Games):
Consider a potential game (f1, f2) with potential function φ and game Jacobian J(x) = −ΛDg(x). Suppose x is a differential Nash equilibrium. Then, x is a locally stable equilibrium of x˙ = −Λg(x) for any learning rate ratio τ > 0 if λ−1 λ−2 > max spec(P P ).
Proof: First, observe that a = J11v, v and d = J22w, w are both negative real numbers for any v ∈ S1 and w ∈ S2 by assumption that x is a differential Nash equilibrium, i.e. −Di2fi(x ) < 0 for each i ∈ {1, 2}. Second, observe that for potential games, p = P w, v = P v, w . Therefore, for x to be stable, the eigenvalues of
ap Jv,w = τ p∗ τ d
must all have negative real components. Hence, we compute the the trace and determinant conditions to be tr(Jv,w) = λ1 + λ2 = a + τ d and det(Jv,w) = λ1λ2 = τ (ad − |p|2). Notice that a + τ d < 0 ⇐⇒ a + d < 0 and τ (ad − |p|2) > 0 ⇐⇒ ad − |p|2 > 0 ⇐⇒ ad > |p|2 > 0. In terms of the original matrix, the condition (max spec(−J22)) (max spec(−J11)) > max spec(P P ) implies that ad > |p|2 for all v, w ∈ S1 × S2. Therefore, λ−1 λ−2 > max spec(P P ) implies that x is stable for all τ > 0.

IV. INSTABILITY IN GENERAL-SUM GAMES

As a complementary result to the stability analysis for zero-sum and potential games, we provide a sufﬁcient condition for the instability of ﬁxed points of gradient-based learning dynamics in general sum games. Our results quantiﬁes the contribution of the off-diagonal interaction terms of (3) in destabilizing equilibria.
We begin by expressing the game Jacobian as the sum of symmetric and skew-symmetric matrices, J = 12 (J + J ) + 21 (J − J ). Let R be a rotation that diagonalizes 12 (J + J ) and sorts the eigenvalues so that J decomposes into

RJ R = M+ 0 + Z1 Z2

(12)

0 M−

−Z2 Z3

where M+ > 0, M− ≤ 0 are diagonal and Z1 and Z3 are skew-symmetric. Let λ−(M+) > 0 be the minimum eigenvalue of M+ and λ+(M−) ≤ 0 be the maximum
eigenvalue of M−.
Theorem 5 (Sufﬁcient Conditions for Instability in Games):
Consider general-sum game G = (f1, f2) with fi ∈ C2(X1 × X2, R) where Xi is di-dimensional for each i = 1, 2. At a ﬁxed point x , spec J(x ) ⊂ C◦− if

Z2

<

1 2

|λ+(M−)| + |λ−(M+)|

< |λ−(M+)|

(13)

with M+, M− and Z2 deﬁned in (12).
Proof: Since Z1 and Z3 are skew-symmetric we have that Re M− + Z3) ≤ λ+(M−) ≤ 0 and 0 ≤ λ−(M+) ≤
Re W (M+ + Z1) [25, Prop. 1.1.12].
The result above works by bounding a non-empty subset of the eigenvalues of J in C◦+ to guarantee instability. The inequalities in (13) are the block matrix equivalent of being
inside the circle of radius h2 + p2 in the scalar case [6]. Example 3: Consider a game (f1, f2) with costs fi : R2 ×
R2 → R, i = 1, 2 given by

f1(x, y) = −x21 + 3x22 − x1x2 + 8x1y1 − 2x2y2, f2(x, y) = y12 + 4y22 − y1y2 + 2x1y2 + 2x2y1.

By diagonalizing the symmetric component of the game Jacobian, we have that M+ = 4.8, M− = − diag(4.4, 5.7, 8.7). By applying this rotation to the skew-symmetric component, we have that Z2 = 4.0 using the Euclidean norm. Since Z2 < 4.6 < 4.8, we have that the origin is unstable.

V. NUMERICAL EXAMPLE
Example 4: We explore how timescale separation can improve the convergence of game dynamics. In particular, we show that when the vector ﬁeld has enough rotational component, timescale separation can lead to a well-conditioned Jacobian and thus faster convergence. Consider a zero-sum game G = (f, −f ) on R2 × R2 with cost given by
f (x, y) = (1 − ε) x21 + 23 x22 − 2y12 − 25 y22 + εx By
and the matrix B is such that each entry is Bij = 1 for each i, j except for B22 = −1. The parameter 0 ≤ ε ≤ 1 controls the amount of rotation in the game vector ﬁeld. When ε = 0, the game Jacobian is symmetric; when ε = 1, the game Jacobian is skew-symmetric. The decomposition of the Jacobian is J = (1 − ε)S + εA where S = S and A = −A . Suppose agents descend their individual gradient with learning rates γ1, γ2 = τ γ1, yielding discrete-time dynamics
x(t + 1) = x(t) − γ1D1f (x(t), y(t)) (14)
y(t + 1) = y(t) + γ1τ D2f (x(t), y(t)).

We initialize x(0), y(0) to a vector of ones and simulate (14) with γ1 = 10−3. We plot the 2-norm of the iterates in Fig. 4a.
Recall that the spectrum of ΛJ(x, y) at an equilibrium (x, y) determines its stability and that the spectral radius of I + γΛJ(x, y) determines the convergence rate of the discrete-time system above, where Λ = blockdiag(I1, τ I2). These quantities with varying τ > 0 are plotted in Fig. 4b.

Distance to equilibrium

Uniform learning rates ( = 1)

100

10 2

10 4

= 0.9

10 6

= 0.1

0 100 200 300 Iterations

Timescale separation ( = 28)
0 100 200 300 Iterations

(a) The rotational system (blue) with timescale separation (right) achieves the fastest convergence by taking advantage of the rotational vector ﬁeld.

Spectral Radius

Discrete-time update

1.00

0.98

0.96

0.94

0 1

20 28 40

Time-scale separation ( )

τ
τ τ

(b) The spectral radius of I + γ1ΛJ(z) for the discrete-time update and the eigenvalues of ΛJ(z) for the continuous-time system z˙ = −Λg(z) at equilibrium z = (x, y) = 0 for increasing learning rate ratio τ > 0.
Fig. 4. Faster convergence with timescale separation. (a) Timescale separation improves convergence rates of systems with mostly rotational dynamics (blue). (b) The spectral radius and spectrum of the discrete-time and continuous-time updates, respectively, show that at τ ≈ 28, the mostlyrotational system achieves fastest convergence because it takes advantage of the imaginary eigenvalues to achieve a smaller spectral radius.

By learning with different rates γi, players can take advantage of the rotational component of a vector ﬁeld to converge faster. For ε = 0.9, the system converges fastest with τ ≈ 28. This is indicated by the blue curves in Fig. 4a, black squares in Fig. 4b, and the right plot in Fig. 1.
VI. CONCLUSION
We characterize local stability of Nash equilibria in twoplayer games by analyzing the spectrum of the gradient learning dynamics. By decomposing the game Jacobian into zero-sum and potential game components, we assess how each term contributions to the stability of Nash or nonNash equilibria. We provide tight bounds on the spectrum of the learning dynamics near ﬁxed-points. Such results give valuable insights into the interaction of algorithms and optimization landscape of settings most accurately modeled as games.
In the numerical example, we demonstrate an important trade-off between timescale separation between agents and the skew-symmetric component of the learning dynamics. Agents learning at different rates can result in faster convergence if the vector ﬁeld has enough rotational component. This indicates a future direction of research: to optimize convergence rate given the strength of the skew-symmetric component of the game.
REFERENCES
[1] David Balduzzi, Wojiech M Czarnecki, Thomas W Anthony, Ian M Gemp, Edward Hughes, Joel Z Leibo, Georgios Piliouras, and Thore Graepel. Smooth markets: A basic mechanism for organizing gradientbased learners. Proc. Inter. Conf. Learning Representations, 2020.

[2] Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A closer look at the optimization landscapes of generative adversarial networks. Proc. Inter. Conf. Learning Representations, 2020.
[3] Victor Boone and Georgios Piliouras. From Darwin to Poincare´ and von Neumann: Recurrence and Cycles in Evolutionary and Algorithmic Game Theory. In Inter. Conf. Web and Internet Economics, pages 85–99, 2019.
[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2004.
[5] Jingjing Bu, Lillian J Ratliff, and Mehran Mesbahi. Global convergence of policy gradient for sequential zero-sum linear quadratic dynamic games. arXiv preprint arXiv:1911.04672, 2019.
[6] Benjamin Chasnov, Dan Calderone, Behc¸et Ac¸ıkmes¸e, Samuel A Burden, and Lillian J Ratliff. Stability of gradient learning dynamics in continuous games: Scalar action spaces. In IEEE Conf. on Decision and Control, December 2020.
[7] Benjamin Chasnov, Lillian Ratliff, Eric Mazumdar, and Samuel Burden. Convergence Analysis of Gradient-Based Learning in Continuous Games. In Proc. Uncertainty in Artiﬁcial Intelligence, 2019.
[8] Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study. Proc. Inter. Conf. Machine Learning, 2020.
[9] Drew Fudenberg and David K Levine. The theory of learning in games. MIT press, 1998.
[10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems, 2014.
[11] Hassan K Khalil. Nonlinear systems theory. Prentice Hall, 2002. [12] Heinz Langer, A Markus, V Matsaev, and C Tretter. A new concept
for block operator matrices: the quadratic numerical range. Linear algebra and its applications, 330(1-3):89–112, 2001. [13] Eric Mazumdar, Lillian J Ratliff, and Shankar Sastry. On gradientbased learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103–131, 2020. [14] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In Proc. 29th Ann. ACM-SIAM Symp. Discrete Algorithms, pages 2703–2717. SIAM, 2018. [15] Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1-2):465–507, 2019. [16] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. Proc. Inter. Conf. Learning Representations, 2017. [17] Dov Monderer and Lloyd S Shapley. Potential games. Games and economic behavior, 14(1):124–143, 1996. [18] John Nash. Non-cooperative games. Annals of mathematics, pages 286–295, 1951. [19] L. J. Ratliff, S. A. Burden, and S. S. Sastry. Genericity and structural stability of non-degenerate differential Nash equilibria. In Proc. Amer. Control Conf., pages 3990–3995, 2014. [20] Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. Characterization and computation of local Nash equilibria in continuous games. In Proc. 51st Ann. Allerton Conf. Communication, Control, and Computing, pages 917–924. IEEE, 2013. [21] Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. On the Characterization of Local Nash Equilibria in Continuous Games. IEEE Trans Automa. Control, 61(8):2301–2307, 2016. [22] S. Shankar Sastry. Nonlinear systems: analysis, stability, and control. Springer-Verlag New York, 1999. [23] Yujie Tang and Na Li. Distributed zero-order algorithms for nonconvex multi-agent optimization. In Proc. 57th Allerton Conf. Communication, Control, and Computing, pages 781–786, 2019. [24] T. Tatarenko and M. Kamgarpour. Learning Nash Equilibria in Monotone Games. In Proc. IEEE Conf. Decision and Control, pages 3104–3109, 2019. [25] Christiane Tretter. Spectral theory of block operator matrices and applications. World Scientiﬁc, 2008. [26] Christiane Tretter. Spectral inclusion for unbounded block operator matrices. J. functional analysis, 256(11):3806–3829, 2009.

