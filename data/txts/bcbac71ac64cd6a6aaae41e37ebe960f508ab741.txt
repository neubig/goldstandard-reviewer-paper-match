Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge
Pat Verga*, Haitian Sun*, Livio Baldini Soares, William W. Cohen Google Research
{patverga, haitiansun, liviobs, wcohen}@google.com

arXiv:2007.00849v1 [cs.CL] 2 Jul 2020

Abstract
Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information. However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes. Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge. We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks. More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.
1 Introduction
Over the past several years, large pretrained language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019) have shifted the NLP modeling paradigm from approaches based on pipelines of task-speciﬁc architectures to those based on pretraining followed by ﬁnetuning, where a large language model discovers useful linguistic properties of syntax and semantics through massive self-supervised training, and then small amounts of task speciﬁc training data are used to ﬁne-tune that model (perhaps with small architectural modiﬁcations). More recently, similar approaches have been explored for knowl-
*Equal contribution

edge representation and reasoning (KRR) with researchers asking questions like ‘Language Models as Knowledge Bases?’ (Petroni et al., 2019). Results suggest that (Roberts et al., 2020; Brown et al., 2020) the answer is a resounding ‘sort of’ (Poerner et al., 2019): while language models can be coerced to answer factual queries, they still lack many of the properties that knowledge bases typically have. In particular, when evaluating LMas-KRR models there are three explanations for why a model outputs a correct answer; 1) The model has successfully performed some reasoning or generalization required to make a novel inference, 2) the dataset contains some statistical biases that the model is exploiting, or 3) the model has memorized the exact answer, potentially from pretraining data that overlaps with the test cases.1. In short, knowledge encoded only in a LM’s parameters is generally opaque.
To address these problems, we propose an interface between explicit, symbolically bound memories and sub-symbolic distributed neural models. In addition to making more of a language model’s behavior interpretable, our approach has several other important beneﬁts. First, there is a massive amount of useful information that has been created and curated in structured databases. Sometimes this information either does not occur in text at all (such as a new product that hasn’t come out yet) or is very difﬁcult to interpret from the text (such as in scientiﬁc, technical, or legal documents). In our framework, new knowledge can be inserted by updating the symbolically bound memory. Second, pre-trained language models appear to require training on very large corpora
1This is a real possibility: for example, the T5 training data contains a large portion of the sources from which TriviaQA was derived, and attempts at avoiding leakage in GPT3 by looking at large ngram exact match do not account for trivial surface form changes.

to obtain good factual coverage—and the massive web corpora required by these data-hungry models contain huge amounts of sexist, racist, and incorrect assertions (Bolukbasi et al., 2016; Sun et al., 2019b). Our approach makes it possible to obtain better factual coverage of assertions chosen from selected trusted sources, by inserting this trusted factual content into the symbolic memory.
We propose to incorporate an external fact memory into a neural language model. This model forms its predictions by integrating contextual embeddings with retrieved knowledge from an external memory, where those memories are bound to symbolic facts which can be added and modiﬁed. We evaluate our model’s performance empirically on two benchmark question answering datasets; FreebaseQA and WebQuestionsSP (section 4.2). In section 5.2, we show how we can inject new memories at inference time enabling our model to correctly answer questions about pairs of entities that were never observed in the pretraining text corpus. Finally, in section 5.3 we examine to what extent our model is capable of iteratively updating by overwriting prior memories with new facts. We modify facts such that they actually contradict the original pretraining data, and show that our model is capable of answering correspondingly modiﬁed question answer pairs. In these experiments we show that end users can inject new knowledge and change existing facts by manipulating only the symbolically bound memories without retraining any parameters of the model.
2 Related Work
Knowledge bases (KBs) have been a core component of AI since the beginning of the ﬁeld (Newell and Simon, 1956; Newell et al., 1959). Widely available public KBs have been invaluable in research and industry (Bollacker et al., 2008; Auer et al., 2007) and many companies have created massive KBs as the backbones of their most important products (Google, 2012; Dong, 2017).
While traditional KBs were purely symbolic, recent advances in large language models trained through self supervision (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019; Brown et al., 2020) have been shown to encode an impressive amount of factual information. This has led to research on the extent to which a neural language model can serve as a KB (Roberts et al., 2020; Petroni et al., 2019), and other research on how

to best evaluate the factual knowledge in language models (Poerner et al., 2019).
While large LMs appear to absorb KB-like information as a preproduct of pretraining, there has also been many prior approaches proposed that explicitly embed symbolic knowledge representations into neural embedding space. Various neural-symbolic methods have attempted to unify these two extremes (Pinkas, 1991; de Penning et al., 2011; Besold et al., 2017) including many cognitive architectures which used hybrid symbolic and subsymbolic systems (Laird et al., 2017), and more recently, compositional query languages for embedding KBs that are similar to symbolic KB query languages (Cohen et al., 2017; Hamilton et al., 2018; Ren et al., 2020; Cohen et al., 2020). One system especially related to our proposal is EmQL (Sun et al., 2020), which includes a construct quite similar to the “fact memory” used in our Facts-As-Experts model. Unlike this work, however, EmQL did not embed its fact memory into a language model, which can be ﬁnetuned for many NLP tasks: instead EmQL must be used with task-speciﬁc query templates and integrated into some task-speciﬁc architecture.
More recently, the past decade has seen huge amount of work on knowledge base embeddings (Bordes et al., 2013; Lin et al., 2015; Trouillon et al., 2017; Dettmers et al., 2018) which enable generalization through similarities between learned embeddings. This idea has also been extended with works looking at ways of incorporating raw text and symbolic KGs into a shared embedding space (Riedel et al., 2013; Verga et al., 2016), to be jointly reasoned over (Sun et al., 2018, 2019a), or to treat text as a replacement for a knowledge base (Dhingra et al., 2019).
Large external memories have been incorporated into different types of memory networks operating over latent parameters (Weston et al., 2014; Miller et al., 2016), entity memories (Henaff et al., 2016; Fe´vry et al., 2020), relations (Logan et al., 2019), and embedded text passages (Guu et al., 2020; Lewis et al., 2020). Our work directly extends one of these models, the Entities-as-Experts (EaE) model (Fe´vry et al., 2020), one of several models that inject knowledge of entities by constructing a memory containing embedded entity representations. Unlike prior models, EaE learns entity representations end-to-end, rather than using representations from a separately-trained KB

embedding system (Logan et al., 2019). Our work extends EaE by introducing a symbolic memory of triples which is constructed from these learned entity representations, and as in EaE, the entity representations are learned end-to-end.
3 Model
3.1 Facts-as-Experts (FaE)
Our Facts-as-Experts (FaE) model (see Figure 1) builds an interface between a neural language model and a symbolic knowledge graph. This model builds on the recently-proposed Entities as Experts (EaE) language model Fe´vry et al. (2020), which extends the same transformer (Vaswani et al., 2017) architecture of BERT (Devlin et al., 2019) with an additional external memory for entities. After training EaE, the embedding associated with an entity will (ideally) capture information about the textual context in which that entity appears, and by inference, the entity’s semantic properties. In FaE, we include an additional memory called a fact memory, which encodes triples from a symbolic KB. Each triple is constructed compositionally from the EaE-learned embeddings of the entities that comprise it. This fact memory is represented with a key-value memory, and can be used to retrieve entities given their properties in the KB. This combination results in a neural language model which learns to access information in a the symbolic knowledge graph.
3.2 Deﬁnitions
We represent a Knowledge Base K as a set of triples (s, r, o) where s, o ∈ E are the subject and object entities and r ∈ R is the relation, where E and R are pre-deﬁned vocabularies of entities and relations in the knowledge base K. A text corpus C is a collection of paragraphs2 {p1, . . . , p|C|}. Let M be the set of entity mentions in the corpus C. A mention mi is deﬁned as (em, spm, tpm), i.e. entity em is mentioned in paragraph p starting from the token at position spm and ends on tpm. Since we don’t consider multi-paragraph operations in this paper, we will usually drop the superscript p and use sm and tm for brevity.
2We use the term paragraph to describe a text span that is roughly paragraph length (128 token pieces in our experiments). In reality the text spans do not follow paragraph boundaries.

3.3 Input
The input to our model is a piece of text which can be either a question in the case of ﬁne tuning (see section 3.8) or an arbitrary span as in pre-training (see section 3.7). Our pretraining input is constructed as cloze-type Question Answering (QA) task. Formally, given a paragraph p = {w1, . . . , w|p|} with mentions {m1, . . . , mn}, we pick a mention mi and replace all tokens from smi to tmi with a special [MASK] token. We consider the entity in E named by the masked entity to be the answer to the cloze question q. Mentions in the paragraph other than this masked entity are referred as below as context mentions. For example, in the cloze question, {‘Charles’, ‘Darwin’, ‘was’, ‘born’, ‘in’, [MASK], [MASK], ‘in’, ‘1809’, ‘.’, ‘His’, ‘proposition’, . . . }, “Charles Darwin” is a context entity in mention m1 = (‘Charles Darwin’, 1, 2), and “United Kingdom” is the answer entity in the masked mention mans = (‘United Kingdom’, 6, 7).
Our model learns to jointly link entities from context mentions mi using entity-aware contextual embeddings (§3.4) and predict answer entities using knowledge-enhanced embeddings (§3.6). This process will be introduced in more detail in the following sections.
3.4 Entity-aware Contextual Embeddings
We follow the Entities-as-Experts (EaE) (Fe´vry et al., 2020) model to train an external entity memory. The EaE model is illustrated in the left part of Figure 1. This model interleaves standard Transformer layers with layers that access an entity memory (see Vaswani et al. (2017) for details on the transformer architecture). EaE inputs a paragraph (or question) containing unlinked entities with known boundaries3 (i.e., the index of the start and end of each mention is provided, but the identity of the entity mentioned is not.) Given a question q = {w1, . . . , w|q|} with a list of context mentions mi = (emi, smi, tmi) and the answer eans from the masked mention mans = (eans, sans, tans), the contextual embedding h(il) is the output at the i’th token of the l’th intermediate transformer layer.
h(il), . . . , h(|ql)| = Transformer({w1, . . . , w|q|})
3Fe´vry et al. (2020) also showed the model is capable of learning to predict these boundaries. For simplicity, in this work we assume they are given.

Figure 1: Facts-as-Experts model architecture. The model takes a piece of text (a question during ﬁne-tuning or arbitrary text during pre-training) and ﬁrst contextually encodes it with an entity enriched transformer. The part of the model within the dashed line is exactly the Entities-as-Experts model from Fe´vry et al. (2020). The model uses the contextually encoded MASK token as a query to the fact memory. In this case, the contextual query chooses the fact key (Charles Darwin, born in) which returns the a set of values {United Kingdom} (The value set can be multiple entity objects such as the case from calling the key [United Kingdom, has city]) . The returned object representation is incorporated back into the context in order to make the ﬁnal prediction. Note that the entities in facts (both in keys and values) are shared with the EaE entity memory.

These contextual embeddings are used to compute query vectors that interface with an external entity memory E ∈ R|E|×de, which is a large matrix containing a vector for each entity in E. To construct a query vector, we concatenate the context embeddings for the mention mi’s start and end tokens, h(slm) i and h(tm l)i , and project them into the entity’s embedding space. We compute the attention weights over the embeddings of the full entity vocabulary, and use this to produce the attentionweighted sum of entity embeddings ulmi. This result is then projected back to the dimension of the contextual token embeddings, and added to what would have been the input to the next layer of the Transformer.
h(ml)i = WTe [h(slm) i ; h(tm l)i ] (1) u(ml)i = softmax(h(ml)i , E) × E (2) h˜(jl+1) = h(jl) + WT2 u(ml)i , smi < j < tmi (3)
Let h(jT ) be the contextual embedding of the j’th token after the ﬁnal transformer layer T . Similar to the query construction in the intermediate transformer layer in Eq. 1, EaE constructs the query

vector h(mTi) for mention mi and use it to predict the context entities emˆ i. This query vector is called an entity-aware contextual query in the rest of this paper and denoted as cmi for brevity. This query vector is trained with a cross-entopy loss against Iemi , the one-hot label of entity emi.
eˆmi = argmaxei∈E (cTmi ei) lossctx = cross entropy(softmax(cmi, E), Iemi )
As shown in Fe´vry et al. (2020), supervision on the intermediate entity access is beneﬁcial for learning entity-aware contextual embeddings. We compute an entity memory access loss using the intermediate query vector in Eq. 1.
lossent = cross entropy(softmax(h(ml)i, E), Iemi )
In pretraining the FaE model, we used a slightly different pre-training process than was used in EaE. In EaE, mentions in the same paragraphs are independently masked with some probability and jointly trained in one example.4 In FaE, in addition
4EaE is also jointly trained on mention detection. Please refer to Fe´vry et al. (2020) for more information.

to the randomly masked context mentions, FaE picks exactly one of the mentions and masks it. Predicting this masked entity requires additional access to the fact memory which will be discussed in the next section.
3.5 Fact Memory
FaE inherits the external entity memory E from the EaE model and adds another fact memory which contains triples from the knowledge base K (see the right side of Figure 1). The fact memory shares its on entity representations with the entity memory embeddings in E, but each element of the fact memory corresponds to a symbolic substructure, namely a key-value pair ((s, r), {o1, . . . , on}). The key (s, r) is a (subject entity, relation) pair, and the corresponding value {o1, . . . , on} is the list of object entities associated with s and r, i.e. (s, r, oi) ∈ K for i = {1, . . . , n}. Hence, conceptually, KB triples with the same subject entity and relation are grouped into a single element. We call the subject and relation pair aj = (s, r) ∈ A a head pair and the list of objects bj = {o1, . . . , on} ∈ B a tail set5, and will encode K as a new structure K = (A, B), with |A| = |B|. Notice that K contains same information as K, but can be encoded as as a key-value memory: elements are scored using the keys (s, r) from head pairs A, and values from the tail sets B are returned.
In more detail, we encode a head pair aj = (s, r) ∈ A in the embedding space as follows. Let E ∈ R|E|×de be the entity embeddings trained in Sec 3.4, and R ∈ R|R|×dr be embeddings of relations R in the knowledge base K. We encode a head pair a as:
aj = WTa [s; r] ∈ Rda
where s ∈ E and r ∈ R are the embeddings of subject s and relation r, and Wa is a learned linear transformation matrix. A ∈ R|A|×da is the embedding matrix of all head pairs in A.
The query vector to access the fact memory is derived from contextual embeddings and projected to the same embedding space as the head pairs A. For a masked mention mans = (eans, sans, tans), de-
5The size of the tail set bj can be large for a popular head pair (s, r). In such case, we randomly select a few tails and drop the rest of them. The maximum size of the tail set is 32 in the experiments in this paper.

ﬁne a query vector

vmans = WTf [h(sTan)s ; h(taTns)] (4)

where h(sTan)s and h(taTns) are the contextual embeddings at the start and end tokens for the mention mans, and Wf is the linear transformation matrix into the embedding space of head pairs A.
Head pairs in A are scored by the query vector vmans and the top k head pairs with the largest inner product scores are retrieved. This retrieval process on the fact memory is distantly supervised. We deﬁne a head pair to be a distantly supervised positive example ads = (s, r) for a passage if its subject entity s is named by a context mention mi and the masked entity eans is an element of the corresponding tail set, i.e. eans ∈ bds. In cases that no distantly supervised positive example exists for a passage, we introduce add a special example that retrieves a “null” fact from the knowledge base, where the “null” fact has a special snull head entity and special rnull relatio: i.e. ads = (snull, rnull) and its tail set is empty. This distant supervision is encoded by a loss function

TOPk(vmans , A) = argmaxk,j∈{1,...,|A|}aTj vmans lossfact = cross entropy(softmax(vmans , A), Iads )

Here the tail sets associated with the top k scored head pairs, i.e. {bj|j ∈ TOPk(v, A)}, will be returned from the fact memory. We will discuss integrating the retrieved tail sets bj’s to the context in the following section.

3.6 Integrating Knowledge and Context
Tail sets retrieved from the fact memory are next aggregated and integrated with the contextual embeddings. Recall that a tail set bj returned from the fact memory is the set of entities {o1, . . . , on} s.t. (s, r, oi) ∈ K for i ∈ {1, . . . , n} with the associated aj = (s, r). Let oi ∈ E be the embedding of entity oi. We encode the returned tail set bj as a weighted centroid of the embeddings of entities in the tail set bj.

bj =

αioi ∈ Rde

oi∈bj

where αi is a context-dependent weight of the object entity oi. To compute the weights αi, we use a process similar to Eq. 4, and we compute a second query vector zmans to score the entities inside thee tail set bj. The weights αi are the softmax of the

inner products between the query vector zmans and the embeddings of entities in the tail set bj.

zmans = WTb [h(sTan)s ; h(taTns)] (5)

α = exp (oTi zmans )

(6)

i

exp (oTl zmans )

ol∈bj

where Wb is yet another transformation matrix different from We in Eq. 1 and Wf in Eq. 4.
The top k tail sets bj are further aggregated using weights βj, which are the softmax of the retrieval (inner product) scores of the top k head pairs aj. This leads to a single vector fmans that we call the knowledge embedding for the masked mention mans.

fmans =

βj bj

(7)

j∈TOPk(vmans ,A)

exp (aTj vmans )

βj =

exp (aT vm ) (8)

t∈TOPk(vmans ,A)

t

ans

Intuitively fmans is the result of retrieving a set of entities from the fact memory. We expect FaE
should learn to jointly use the contextual query cmans and knowledge query fmans to predict the masked entity, i.e. use external knowledge re-
trieved from the fact memory if there exists an oracle head pair aorc = (s, r) s.t. eans ∈ borc, or fall back to contextual query to make predictions otherwise. We compute the integrated query qmans with a mixing weight λ. λ is the probability of predicting the “null” head anull in the fact memory access step, i.e. whether an oracle head pair aorc exists in the knowledge base.

λ = P (y = anull) qmans = λ · cmans + (1 − λ) · fmans
The query vector qmans is called a knowledgeenhanced contextual query. This query vector ﬁnally is used to predict the masked entity. Again, we optimized it with a cross-entropy loss.
eˆans = argmaxei∈E (qTmans ei) lossans = cross entropy(softmax(qmans , E), Ieans )
3.7 Pretraining
FaE is jointly trained to predict context entities and the masked entity. Context entities are predicted using the contextual embeddings described in § 3.4; intermediate supervision with oracle entity linking labels is provided in the entity memory access step for context entities; the masked

entity is predicted using the knowledge-enhanced contextual embeddings (§ 3.6); and distant supervised fact labels are also provided at training time. The ﬁnal training loss is the unweighted sum of the four losses:
losspretrain = lossent + lossctx + lossfact + lossans
3.8 Finetuning on Question Answering
In the Open-domain Question Answering task, questions are posed in natural language, e.g. “Where was Charles Darwin born?”, and answered by a sequence of tokens, e.g. “United Kingdom”. In this paper, we focus on a subset of open-domain questions that are answerable using entities from a knowledge base. In the example above, the answer “United Kingdom” is an entity in Wikidata whose identity is Q145.
We convert an open-domain question to an input of FaE by appending the special [MASK] token to the end of the question, e.g. {‘Where’, ‘was’, ‘Charles’, ‘Darwin’, ‘born’, ‘?’, [MASK]}. The task is to predict the entity named by mask. Here, “Charles Darwin” is a context entity, which is also referred to as question entity in the ﬁnetuning QA task.
At ﬁnetuning time, entity embeddings E and relation embeddings R are ﬁxed, and we ﬁnetune all transformer layers and the four transformation matrices: Wa, Wb, We, Wf . Parameters are tuned to optimize unweighted sum of the the fact memory retrieval loss lossfact and the ﬁnal answer prediction loss lossans. If multiple answers are available, the training label Ieans becomes a k-hot vector uniformly normalized across the answers.
lossﬁnetune = lossfact + lossans
4 Experiments
4.1 Datasets
We evaluate our model on two Open-domain Question Answering datasets: FreebaseQA (Jiang et al., 2019) and WebQuestionsSP (Yih et al., 2015) (See table 1 for data statistics). Both datasets are created from Freebase. To align with our pretraining task, we convert the entity ids from Freebase to Wikidata. FreebaseQA. FreebaseQA is derived from TriviaQA and several other trivia resources (See Jiang et al. (2019) for full details). Every answer can be resolved to at least one entity and each question contains at least one question entity ei. Additionally, there exists at least one relational path

FreebaseQA WebQuestionsSP

Train Dev Test
Train Dev Test

Full Dataset
20358 2308 3996
2798 300 1639

Wikidata Answerable
12535 2464 2440
1388 153 841

Table 1: Dataset stats. Number of examples in train, dev, and test splits for our three different experimental setups. Full are the original unaltered datasets. Wikidata Answerable keeps only examples where at least one question entity and answer entity are mappable to Wikidata and there is at least one fact between them in our set of facts.

in Freebase between the question entity ei and the answer eans. The path must be either a one-hop path, or a two-hop path passing through a mediator (CVT) node, and is veriﬁed by human raters. 72% of the question entities and 80% of the answer entities are mappable to Wikidata, and 91.7% of the questions are answerable by at least one answer entity that is mappable to Wikidata. WebQuestionsSP. WebQuestionsSP is constructed from Freebase and contains 4737 natural language questions (3098 training and 1639 test). Questions in the dataset are linked to corresponding Freebase entities and relations. We mapped question entities and answer entities to their Wikidata ids. 87.9% of the questions are answerable by at least one answer entity that is mappable to Wikidata. Subset of questions answerable by KB triples. Both of these datasets were constructed so that that all questions are answerable using the FreeBase KB, which was last updated in 2016. Because our pretraining corpus is derived from larger and more recent versions of Wikipedia, we elected to use a KB constructed from Wikidata instead. Use of the more recent Wikidata KB means that some questions are no longer answerable using the KB, so we also created a second reduced version of the datasets called Wikidata Answerable. These subsets only contains questions that are answerable by triples from our Wikidata-based KB. The model should learn to rely on the KB to answer the questions.
4.2 Pretraining
FaE is pretrained on Wikipedia and Wikidata. Text in Wikipedia is chunked into 128 token pieces. To

compute the entity-linking loss lossent, we use as training data entities linked to the 1 million most frequently linked-to Wikidata entities. Text pieces without such entities are dropped. This results in 30.58 million text pieces from Wikipedia. As described in § 3.2, we generate n training examples from a piece of text containing n entity mentions, where each mention serves as the masked target for its corresponding example, and other entity mentions in the example are treated as context entities6. This conversion results in 85.58 million pre-training examples. The knowledge base K is a subset of Wikidata that contains all facts with subject and object entity pairs that co-occur at least 10 times on Wikipedia pages.7 This results in a KB containing 1.54 million KB triples from Wikidata (or 3.08 million if reverse triples are included). Below, this is called the full setting of pretraining—we will also train on subsets of this example set, as described below. We pretrain the model for 500,000 steps with the batch size 2048, and we set k = 1 in the TOPk operation for fact memory access.
4.3 Results
We compare FaE with three baseline models: FOFE (Jiang et al., 2019), EmQL (Sun et al., 2020), and Entity-as-Expert (EaE) (Fe´vry et al., 2020). FOFE is a feed-forward language model designed to encode long sequences and was the previous state-of-the-art model on the FreebaseQA dataset. EmQL was introduced as a query embedding on knowledge bases and is the previous state-of-the-art model on WebQuestionsSP. EaE has been discussed above, and our EaE models are trained using the same hyperparameters and optimization settings as FaE in order to make them as comparable as possible.
Table 2 compares the FaE model to the baseline model. With the full pre-training and ﬁne-tuning datasets, we outperform the baseline models on the FreebaseQA dataset by nearly 10 points. Performance on WebQuestionsSP in the Full Dataset setting is relatively lower, however this is largely explained due to the incompleteness of the KB due to mapping between Freebase and Wikidata—only 51.3% of the questions in WebQuestionsSP are
6We additionally mask context entities randomly with probability .15
7This leads to more KB triples than entity pairs, since a pair of subject and object entities can be associated with more than one relation.

Data
FOFE EmQL EaE FaE (ours) EaE no ﬁnetune FaE (ours) no ﬁnetune

FreebaseQA

Full WikiData Dataset Answerable

37.0

-

-

-

53.4

59.1

63.3

73.9

18.3

24.8

19.7

26.9

WebQuestionsSP

Full

Wikidata

Dataset Answerable

67.6

-

75.5

74.6

46.3

61.4

56.1

78.5

12.8

21.4

15.9

24.6

Table 2: Conventional Setting Evaluation. Accuracy on FreebaseQA and WebQuestionsSP datasets. We pretrain our models on the full unﬁltered Wikipedia text corpus. In the Full Data column, we report scores on the original unﬁltered data splits for each dataset. In the WikiData Answerable column, we ﬁlter each split to only contain examples where at least one question and answer entity are mappable to WikiData and our WikiData knowledge graph contains some fact connecting them. Nearly all FreebaseQA and WebQuestionsSP entity pairs that are mappable to WikiData co-occur in the Wikipedia pretraining text. Models marked “no ﬁnetune” were not ﬁnetuned.

answerable using facts from our KB. In contrast, both FOFE and EmQL have complete coverage as they both use the full applicable subset of Freebase.
However, if we instead consider only questions answerable using our dataset (the column labeled “Wikidata Answerable”) FaE substantially outperforms EmQL. In this case, both models have complete knowledge base coverage. Additionally, in the Wikidata Answerable setting in FreebaseQA, the gap between EaE and FaE grows even larger to nearly 14 points.
Interestingly, EaE and FaE even answer many questions correctly without any ﬁne-tuning at all (denoted “no ﬁnetune” in the tables. Both models answer around a quarter of the answerable questions for both datasets in this zero-shot setting with FaE having a slight advantage.
5 Modiﬁable Knowledge Base
5.1 Filtering to Avoid Pretrain, Finetune, and Test Overlap
We are interested primarily in the ability of models to use external knowledge to answer questions, rather than learning to recognize paraphrases of semantically identical questions. Unfortunately, analysis of the two datasets showed that many of the test answers also appear as answers to some training-set question: this is the case for 75.0% of answers in the test data for FreebaseQA, and 57.5% of the answers in WebQuestionsSP. This raises the possibility that some of the performance of the models can be attributed to simply memorizing speciﬁc question/answer pairs, perhaps in addition to recognizing paraphrases of the ques-

tion from its pretraining data. To resolve this issue, we discard questions in
the training data that contain answers which overlap with answers to questions in the dev and test data. We end up with 9144/2308/3996 data (train/dev/test) in FreebaseQA and 1348/151/1639 data in WebQuestionsSP. This setting is referred to as Fine-tune column in table 3 which shows the effects of different ﬁlterings of the data. The column denoted None has no ﬁltering and is the same as the Full Dataset setting in table 2. In the column labeled Pretrain, for every question answer entity pair in our ﬁnetuning dataset (coming from any split), we ﬁlter every example from our Wikipedia pretraining corpus where those pair of entities co-occur. Additionally, we ﬁlter every fact from our fact memory containing any of these entity pairs. In this way, the model will be unable to simple memorize paraphrases of question answer pairs that it observed in the text. Finally, the All column combines both pretrain and ﬁne tune ﬁltering. We see that the models perform substantiall worse when these ﬁlterings are applied and they are forced to rely on the ability to reason across multiple examples, and in the case of FaE, the fact memory.
5.2 Injecting New Facts into Memory
Because our model deﬁnes facts symbolically, it can in principle reason over new facts injected into its memory, without retraining any parameters of the model. To test how well the model is able to perform this task in practice, we look at how well the model can perform given full knowledge, ﬁltered knowledge, and injected knowledge. The gap between the ﬁltered knowledge setting and in-

Filter Type
EaE FaE (ours)

None
53.4 63.3

FreebaseQA

Pretrain Fine-tune

45.2

45.8

57.5

56.5

WebQuestionsSP

All None Pretrain Fine-tune All

28.6 46.3 45.4 48.0 56.1 55.4

30.9 29.4 40.7 39.2

Table 3: Effects of Different Data Filtering. The column denoted None has no ﬁltering and is the same as the Full Dataset setting in table 2. Pretrain removes all entity pair overlap between the eval datasets (all splits) and the pretraining text and kb. The Fine-tune column removes all entity pair overlap between the eval train and test splits. The All column combines both pretrain and ﬁne tune ﬁltering.

jected knowledge setting should demonstrate how well the model is able to incorporate newly introduced facts.
The results are shown in Table 4. We always use the ﬁltered Finetune subset of the data (see §5.1) to avoid overlap between ﬁnetuning train and test data. In the “Full” column, we pretrain and ﬁnetune the FaE model with the full knowledge base and corpus. In the “Filter” setting, facts about the ﬁnetuning data are hidden from the model at both pretraining and ﬁnetuning time. In this case, the model should fall back to the language model to predict the answer. As shown in Table 4, the performance of FaE and EaE are close. In the “Inject Facts” setting, Facts are hidden at pretraining time, but are injected at test time. The results show that FaE can effectively use the newly injected facts to make prediction, i.e. an absolute improvement of 9.3% compared to the “Filter” setting. EaE does not have a natural mechanism for integrating this new information8.
5.3 Updating Stale Memories
One of the main motivations for our model is to address the need for knowledge representations that can avoid stale data by incrementally updating as the world changes. To probe this ability, we simulate an extreme version of this scenario where all answers to QA pairs in the FreebaseQA test set are replaced with plausible alternatives. For each QA pair, we replace the original answer entity eoriginal with another entity from our vocabulary enew that has 1) been used as an object in at least one of the same relation types that eoriginal was an object in, and 2) shares at least three Wikipedia categories with eoriginal. We use the same pretrained models from section 4.2. We ﬁne-tune the
8There are various heuristics one could apply for ﬁnetuning a standard language model on this type of data by applying one or a small number of gradient steps on textualized facts. We are currently exploring to what extent this is effective and what knowledge is lost during that additional learning.

ﬁltered FreebaseQA train set and perform early stopping on the unmodiﬁed FreebaseQA dev set. Overall, FaE is able to utilize the modiﬁed KB to make the correct prediction for 30% of questions.
While this is an encouraging result, the decrease in performance compared to the unmodiﬁed evaluation set (nearly twice as many incorrect predictions) shows that the mixing between conextual representations and knowledge requires further research. In section 5.2 FaE was able to easily adapt to using newly injected facts because they were consistent with the pretraining corpus. These were facts that did not appear in the model’s pretraining data but they also did not contradict that data. In the case of updating stale memories, we are instead giving the model new information that in some cases (such as in this experiment) explicitly contradict the knowledge stored in its latent parameters, and this inconsistency makes the mixing much more difﬁcult. Addressing this issue as well as the even more difﬁcult problem of deleting knowledge is a main focus of ongoing and future research.
6 Conclusion
In this paper, we presented a method for interfacing a neural language model with an interpretable symbolically bound memory. We used that interface to change the output of the language model by modifying only the non-parametric memories and without any additional training. We demonstrated the effectiveness of this method by performing comparably or better than a high performing language model on factoid question answering while integrating new facts unseen in pretraining data. We even showed that we can modify facts, such that they contradict the initial pre training text, and our model is still largely able to answer these questions correctly.

EaE FaE (ours)

FreebaseQA

Full Filter Inject Facts

45.8 28.6

28.6

56.5 38.7

48.0

WebQuestionsSP

Full Filter Inject Facts

30.9 29.4

29.4

40.7 32.3

39.2

Table 4: Injecting New Facts. In the Full setting the model is exposed to full knowledge in the pretraining data and KB. In the Filter setting, the models have access to no direct knowledge about question answer entity pairs from either the pretraining corpus or KB. In the Inject Facts setting, the pretraining corpus and training KB are still Filtered, but at inference time, new facts are injected into the models memory allowing it to recover most of the drop from the Full setting. In all cases, we remove the overlap between the ﬁnetune train and eval sets.

References
So¨ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.
Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Ku¨hnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima, et al. 2017. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint arXiv:1711.03902.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems, pages 4349–4357.
Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in neural information processing systems, pages 2787–2795.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
William W Cohen, Haitian Sun, R Alex Hofer, and Matthew Siegler. 2020. Scalable neural methods for reasoning with a symbolic knowledge base. arXiv preprint arXiv:2002.06115. Appeared in ICLR2020.
William W Cohen, Fan Yang, and Kathryn Rivard Mazaitis. 2017. Tensorlog: Deep learning meets probabilistic dbs. arXiv preprint arXiv:1707.05390.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, and William W Cohen. 2019. Differentiable reasoning over a virtual knowledge base. In International Conference on Learning Representations.
Luna Dong. 2017. Amazon product graph.
Thibault Fe´vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.
Google. 2012. Introducing the knowledge graph: things, not strings.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909.
Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. 2018. Embedding logical queries on knowledge graphs. In Advances in Neural Information Processing Systems, pages 2026– 2037.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2016. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969.
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Freebaseqa: A new factoid qa data set matching triviastyle question-answer pairs with freebase. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational

Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 318–323.
John E Laird, Christian Lebiere, and Paul S Rosenbloom. 2017. A standard model of the mind: Toward a common computational framework across artiﬁcial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine, 38(4):13–26.
Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Twenty-ninth AAAI conference on artiﬁcial intelligence.
Robert Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, and Sameer Singh. 2019. Baracks wife hillary: Using knowledge graphs for fact-aware language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5962–5971.
Alexander Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1400–1409.
Allen Newell, J. C. Shaw, and Herbert A. Simon. 1959. Report on a general problem-solving program. In Proceedings of the International Conference on Information Processing.
Allen Newell and Herbert Simon. 1956. The logic theory machine–a complex information processing system. IRE Transactions on information theory, 2(3):61–79.
H Leo H de Penning, Artur S d’Avila Garcez, Lu´ıs C Lamb, and John-Jules C Meyer. 2011. A neuralsymbolic cognitive agent for online learning and reasoning. In Twenty-Second International Joint Conference on Artiﬁcial Intelligence.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237.
Fabio Petroni, Tim Rockta¨schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.

Gadi Pinkas. 1991. Symmetric neural networks and propositional logic satisﬁability. Neural Computation, 3(2):282–291.
Nina Poerner, Ulli Waltinger, and Hinrich Schu¨tze. 2019. Bert is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised qa. arXiv preprint arXiv:1911.03681.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.
Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. arXiv preprint arXiv:2002.05969. Appeared in ICLR-2020.
Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74–84.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910.
Haitian Sun, Andrew O Arnold, Tania Bedrax-Weiss, Fernando Pereira, and William W Cohen. 2020. Guessing what’s plausible but remembering what’s true: Accurate neural reasoning for questionanswering. arXiv preprint arXiv:2004.03658.
Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019a. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2380– 2390.
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4231– 4242.
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019b. Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976.
The´o Trouillon, Christopher R Dance, E´ ric Gaussier, Johannes Welbl, Sebastian Riedel, and Guillaume Bouchard. 2017. Knowledge graph completion via

complex tensor factorization. The Journal of Machine Learning Research, 18(1):4735–4772.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.
Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, and Andrew McCallum. 2016. Multilingual relation extraction using compositional universal schema. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 886–896.
Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916.
Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321–1331, Beijing, China. Association for Computational Linguistics.

