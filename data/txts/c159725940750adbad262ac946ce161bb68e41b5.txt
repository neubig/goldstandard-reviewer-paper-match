ATTENTION-BASED ASR WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS
Yuya Fujita1, Aswin Shanmugam Subramanian2, Motoi Omachi1, Shinji Watanabe2
1Yahoo Japan Corporation, Tokyo, JAPAN 2Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA
{yuyfujit, momachi}@yahoo-corp.jp, {aswin, shinjiw}@jhu.edu

arXiv:1912.11793v2 [eess.AS] 20 Feb 2020

ABSTRACT
End-to-end (E2E) automatic speech recognition (ASR) with sequenceto-sequence models has gained attention because of its simple model training compared with conventional hidden Markov model based ASR. Recently, several studies report the state-of-the-art E2E ASR results obtained by Transformer. Compared to recurrent neural network (RNN) based E2E models, training of Transformer is more efﬁcient and also achieves better performance on various tasks. However, self-attention used in Transformer requires computation quadratic in its input length. In this paper, we propose to apply lightweight and dynamic convolution to E2E ASR as an alternative architecture to the self-attention to make the computational order linear. We also propose joint training with connectionist temporal classiﬁcation, convolution on the frequency axis, and combination with self-attention. With these techniques, the proposed architectures achieve better performance than RNN-based E2E model and performance competitive to state-of-the-art Transformer on various ASR benchmarks including noisy/reverberant tasks.
Index Terms— End-to-end, transformer, lightweight convolution, dynamic convolution
1. INTRODUCTION
In the research ﬁeld of automatic speech recognition (ASR), endto-end (E2E) models are becoming mainstream. There are several advantages compared to conventional hidden Markov model based approaches. The conventional approaches include several components such as lexicon, acoustic and language models. Each component is trained independently with a different criterion, which does not correspond to globally optimizing the whole ASR system. Also, some linguistic knowledge is necessary to create a lexicon. On the other hand, E2E models are composed of a single neural network so there is no need to train multiple components separately or use any linguistic knowledge.
Many efforts have been made [1–7] to improve the performance of E2E models hence currently state-of-the-art E2E methods are comparable to or sometimes better than conventional methods. For example, Google researchers reported their E2E model outperforms conventional methods with their in-house voice search data [8]. Also, with data augmentation, the performance of publicly available corpora such as Librispeech [9] and Switchboard [10] is better than highly tuned conventional systems [11]. Those papers described above use a recurrent neural network (RNN) as its key component. However, one disadvantage of RNNs is the inefﬁciency of back propagation computation with GPU compared to non-recurrent models. This is because of the sequential structure of RNNs.
To overcome this disadvantage, Transformer was proposed [12]. Transformer does not use any recurrent architecture hence efﬁcient

parallelization of the back propagation computation is possible. Transformer was originally proposed for neural machine translation and is also applied to ASR in [13, 14]. By joint training and decoding with connectionist temporal classiﬁcation (CTC) [15], Transformer’s performance is superior to RNN-based models [16, 17].
Although the parallel computation of Transformer is more efﬁcient than RNNs, the computational complexity of the self-attention layer used in Transformer is quadratic in the input feature length, which requires huge GPU memory. To make this complexity linear, this paper employs convolution instead of self-attention [18]. However, the number of parameters and computational complexity of regular convolution is quadratic in the dimension of the input feature. Depthwise convolution [19] which performs convolution independently over every dimension is an option to reduce the number of parameters and the computational complexity to linear. This enables stacking more layers and using a longer kernel length than regular convolution.
In order to further reduce the number of parameters, lightweight and dynamic convolution methods are proposed for neural machine translation [20]. They reduce the parameters by sharing convolution weights between dimensions. Dynamic convolution is a variant of lightweight convolution where the convolution weight is dynamically predicted through an additional linear layer, which takes a current input feature only. Compared to self-attention, they only look at a limited context but performance improvement and faster training compared to Transformer are reported in machine translation.
This paper proposes to apply lightweight and dynamic convolution architectures to E2E ASR and conﬁrms better performance than RNN and performance competitive to state-of-the-art Transformer. The contributions of this paper are as follows:
• Applied lightweight and dynamic convolution methods to E2E ASR for the ﬁrst time.
• Joint training with CTC and lightweight or dynamic convolution.
• Additional lightweight or dynamic convolution along the frequency axis.
• Combination of different layer types. For example, selfattention for the encoder and lightweight convolution for the decoder.
• Implement those methods using the open source ASR toolkit ESPnet [21] to make it publicly available1.
There are several prior studies of E2E ASR with convolutional networks. In [22], a convolutional layer is applied to the attentionbased encoder-decoder model [2] but still recurrent layers remain. E2E ASR with only regular convolution is proposed in [23–25]. In [26], they use a modiﬁed convolution on the encoder side but the
1https://github.com/espnet/espnet/pull/1599

decoder is an RNN. To the best of our knowledge, our work is the ﬁrst to apply lightweight and dynamic convolution to ASR tasks.

2. CONVENTIONAL AND PROPOSED METHOD

This section describes attention-based E2E models by starting from the general formulation. Let X = {xt ∈ Rdfeat |t = 1, · · · , T feat} be an acoustic feature sequence and C = {cl ∈ V|l = 1, · · · , L} be a token sequence where dfeat is the dimension of the input feature and T feat is its length. V is a set of distinct tokens and L is the length of
a token sequence. Training of the E2E model aims to maximize the
following posterior probability:

pe2e(C|X) = ΠLl=1p(cl|c1:l−1, X).

(1)

The difference between various E2E models is in how to deﬁne the probability distribution in Eq.(1). From the next subsection, the conventional and proposed E2E models are introduced.

2.1. Attention-based model with RNN

In the attention-based model with RNN, pe2e(C|X) is deﬁned as

follows [1, 2, 5]. First, an Encoder(·) RNN takes an input feature sequence X and outputs the following de-dimensional encoded fea-

ture sequence B:

B = Encoder(X).

(2)

Here, B = {bt ∈ Rde |t = 1, · · · , T enc}, and T enc ≤ T feat denotes

a subsampled input length. Then, attention weight αlt and context

vector rl are calculated as:

αlt = Attention(ql−1, bt),

(3)

T enc
rl = t=1 αltbt, (4)

where ql is a hidden vector output from Decoder(·) RNN. Finally, the probability distribution over output tokens is calculated from Decoder(·) RNN as:

p(cl|c1:l−1, X) = Decoder(rl, ql−1, cl−1).

(5)

By substituting Eq.(5) into Eq.(1), pe2e(C|X) of the attention based model with RNN is deﬁned. In most cases, long short-term memory (LSTM) RNN is used as Encoder(·) and Decoder(·).

2.2. Transformer
In contrast to the attention based model with RNN, Transformer does not use any recurrent connection. Instead, self-attention based on multi-head scaled dot product is used as a key component [12].

2.2.1. Multi-head scaled dot product attention

Scaled dot product attention is deﬁned as:

QKT

Attention(Q, K, V) = Softmax( √ )V,

(6)

dk

where Q ∈ RT q×dq , K ∈ RT k×dk , and V ∈ RT v×dv denote query, key, and value matrices, respectively. T q, T k, and T v, are the length of each elements and dq, dk, and dv are the dimensions of each el-
ements. In Transformer, this attention mechanism is extended to a
multi-head one, i.e.,

Ui = Attention(QWQi , KWKi , VWVi ), (7) MultiHead(Q, K, V) = Concat(U1, · · · , UHh )WO, (8)

where Hh is the number of heads. The dimension of Q, K, V are set as dq = dk = dv = datt, WiQ, WiK, WiV ∈ R , datt×datt/Hh and WO ∈ Rdatt×datt . datt is the dimension of MultiHead(·) input. Self-
attention is the above multi-head attention layer with the scaled dot product whose Q, K, V are the same:

SelfAttention(V) = MultiHead(V, V, V).

(9)

2.2.2. Encoder and decoder of Transformer

This subsection introduces p(cl|c1:l−1, X) for Transformer. First, input sequence X is transformed into matrix XE ∈ RT ss×datt by an input embedding and subsampling operation where T SS is the se-
quence length after subsampling. Then, encoded feature E is calcu-
lated as follows:

 Z(0) =XE + P, 




 

ZA(n) =Z(n) + SelfAttention(Z(n)),

(10)

 Z(n+1) =ZA(n) + FFn(ZA(n)),







E =Z(N),

where n = 1, · · · , N − 1 is the index of encoder layer and N is the layer number. P ∈ RT ss×datt is the positional encoding, deﬁned as:

pi,2j = sin(i/100002j/datt ), pi,2j+1= cos(i/100002j/datt ). (11)

FFn(·) is deﬁned as:

FFn(ZA(n)) = ReLU(ZA(n)Wn1 + bn1)Wn2 + bn2, (12)

where Wn1 ∈ Rdatt×dff , Wn2 ∈ Rdff×datt , bn1 ∈ Rdff , bn2 ∈ Rdatt
are parameters to be learned.
Next, (c1, · · · , cl−1) is transformed to a real-valued matrix CE ∈ RL×datt by another input embedding. Decoder takes E and CE and calculates [p(c2|c1, X), · · · , p(cl|c1:cl 1 , X)] as follows.
−

 Y(0) =CE + P, 




 

Y(Am) =Y(m) + SelfAttention(Y(m)),

(13)

 Y(Sm) =Y(Am) + MultiHead(Y(Am), E, E),



  Y(m+1)

=Y(Sm)

+

FFm(Y(Sm)),

[p(c2|c1, X), · · · , p(cl|c1:cl 1 , X)] = Softmax(Y(M)Wﬁn + bﬁn). − (14)
m = 1, · · · , M − 1 is the layer index of the decoder and M is the layer number. Wﬁn ∈ Rdatt×dchar and bﬁn ∈ Rdchar are parameters to be learned and dchar is the number of distinct tokens.

2.3. Lightweight and dynamic convolution

Lightweight and dynamic convolution proposed in [20] just replaces the self-attention layer of Transformer. That is, replacing SelfAttention(·) which takes only value V as argument in Eq.(10) and Eq.(13) with LConvLayer(·) or DConvLayer(·) introduced in this subsection.
As explained in Sec.1, lightweight convolution (LConv(·)) reduces the number of parameters by sharing weights:

K

LConv(V, WL) = i,j

w⌈LjHS/dv⌉,k ×v(i+k−⌈ K2+1 ⌉),j , (15)

k=1

Model ID SA LC DC
LC2D DC2D SA-LC SA-DC SA-LC2D SA-DC2D

Encoder SelfAttention LConvLayer DConvLayer LConv2DLayer DConv2DLayer SelfAttention SelfAttention SelfAttention SelfAttention

Decoder SelfAttention LConvLayer DConvLayer LConv2DLayer DConv2DLayer LConvLayer DConvLayer LConv2DLayer DConv2DLayer

Table 1. List of all combinations of layers for encoder and decoder used in experiments.

where WL ∈ RHS×K is a convolution kernel to be learned and HS
is a weight sharing parameter. Compared to depthwise convolution, the number of parameters is reduced from dvK to HSK by setting HS < dv.
Dynamic convolution (DConv(·)) uses a kernel estimated from
the current input:

DConv(V) = LConv(V, WDvt),

(16)

where WD ∈ RHS×K×dv is a parameter to be learned. Then, LConvLayer(·) and DConvLayer(·) are deﬁned as fol-
lows:

LConvLayer(V) = LConv(GLU(VWI), WL)WP, (17)

DConvLayer(V) = DConv(GLU(VWI))WP,

(18)

where WI ∈ Rdv×2dv and WP ∈ Rdv×dv are parameters to be learned. GLU(·) is a gated linear unit [27].

2.3.1. Adding convolution along the frequency axis

We propose to add convolution along the frequency axis to lightweight and dynamic convolution. This is inspired by a variant of LSTMs that perform recurrence along the additional frequency axis [28, 29].
In case of lightweight convolution the frequency axis convolution is performed in contrast to Eq.(15) as:

K

LConvF(V, wF) = i,j

wkF × vi,(j+k−⌈ K2+1 ⌉),

k=1

(19)

where wF ∈ RK are kernel weights to be estimated during training. Similarly to Eq.(16), dynamic convolution along the frequency
axis is performed as:

DConvF(V) = LConvF(V, WUvt),

(20)

where WU ∈ RK×dv is a learnable parameter. Then, these outputs are concatenated with the output of lightweight
or dynamic convolution. Finally, LConv2DLayer(·) and DConv2DLayer(·) are deﬁned as:

LConv2DLayer(V)

= Concat LConv(G, WL), LConvF(G, wF) WR, (21)

DConv2DLayer(V)

= Concat (DConv(G), DConvF(G)) WR,

(22)

where WR ∈ R2dv×dv is a parameter to be learned and G = GLU(VWI).

2.4. Combination of different layers
We also propose using a different layer to SelfAttention(·) between the encoder and decoder of Transformer. All combinations that we tried are summarized in Table 1. Using SelfAttention(·) as decoder and convolutional layer as encoder do not work well according to preliminary experiments so has been omitted.
2.5. Joint training and decoding with CTC
Same as in [5,16], joint training and decoding with CTC objective is used. Linear transformation and Softmax are applied to the encoded feature E then fed to CTC loss. The CTC loss is interpolated with pe2e and used as the objective function. CTC helps the attentionbased decoder to learn monotonic alignment which results in faster convergence and performance improvement.
3. EXPERIMENTS
3.1. Setup
In order to evaluate our proposed method, we used four corpora, CSJ [30], Librispeech [9], REVERVB [31] and CHiME4 [32]. As a baseline, the CTC/Attention hybrid method proposed in [5] was used. The default recipe of ESPnet toolkit [21] was used. We call this baseline RNN.
Another baseline was Transformer proposed in [16]. The recipe published by the author of ESPnet was used (N = 12, M = 6, datt = 256, Hh = 4) but gradient accumulation was increased to 8 because it resulted in better performance. This corresponds to SA in Table 1.
For REVERB, single channel training with all the 8 channel simulation data was used. During evaluation the 8 channel data are processed by frontend of weighted prediction error (WPE) [33] followed by delay-and-sum beamforming (BeamformIt) [34]. For CHiME4, again single channel training with all the 6 channel data was used. For evaluation, the 5 channel data was used (second channel was excluded) with BeamformIt as frontend.
For lightweight and dynamic convolutions and our proposed architecture, basically the same recipe for Transformer mentioned above was used. The differences were batch size and gradient accumulation used for DC. Batch size was set to 48 and gradient accumulation was 11 batches in order to stabilize training. Also, DropConnect [20] with probability 0.1 was used. The recipes we used will be made publicly available in the ESPnet repository.
3.2. Results
We ﬁrst looked for the best hyper parameter such as kernel length of encoder Ke and decoder Kd and weight sharing HS with small amount of training data. Note that Ke, Kd and HS are the same for all layers. Character error rate (CER) of CSJ trained with only Academic lecture data (271 hours) is shown in Table 2. We picked the best hyperparameter for each model while keeping the total number of parameters almost the same. Because of space limitation not all experiments are shown but larger kernel length led to better CER while smaller HS had limited impact. By combining self-attention and convolutional layer, CER is better than RNN and is competitive to state-of-the-art Transformer. Adding convolution along the frequency axis has little effect on CER.
With the best hyperparameter obtained above, we evaluated our methods with a large amount of training data and noisy/reverberant

Librispeech

CSJ 271h

CSJ 581h

dev

test

Model ID HS Ke Kd eval1 eval2 eval3 eval1 eval2 eval3 clean other clean other

RNN N.A. N.A N.A 8.5 6.3 15.9 6.5 4.8 5.1 4.0 12.0 4.1 12.8

SA

N.A. N.A N.A 7.1 5.0 12.6 5.5 3.9 4.3 3.7 9.6 3.9 9.8

LC

4 101 71

DC

4 101 71

LC2D

16 101 71

DC2D

2

31 11

SA-LC

8 N.A. 31

SA-DC

8 N.A. 31

SA-LC2D 4 N.A. 11

SA-DC2D 4 N.A. 11

7.6 5.3 13.0 5.9 4.2 4.6 3.5 10.2 3.7 10.7 7.9 5.5 13.5 6.2 4.2 4.5 3.5 10.5 3.6 10.8 7.6 5.4 12.8 5.8 4.1 4.4 3.4 10.3 3.7 10.6 8.2 5.8 13.2 6.5 4.5 4.7 3.6 11.5 3.8 11.6 7.0 4.9 12.6 5.6 4.1 4.3 3.8 9.6 4.2 9.8 7.1 5.0 12.3 5.6 3.8 4.1 4.2 9.9 4.6 10.2 7.1 4.9 12.5 5.5 4.1 4.2 3.9 9.6 4.3 9.7 7.0 4.9 12.6 5.6 4.0 4.1 3.5 9.6 3.9 9.6

Table 2. Character error rate (CER) of CSJ and word error rate (WER) of Librispeech. Using self-attention for encoder and convolutional layer for decoder (SA-{LC,DC,LC2D,DC2D} in Table 1) yield better performance than RNN and performance competitive to state-of-the-art Transformer. (SA in Table 1).

REVERB Simulated

REVERB Real

CHiME4

Room 1

Room 2

Room 3

Room 1

dev

test

Model ID HS Ke Kd Near Far Near Far Near Far Near

Far simu real simu real

RNN SA

N.A. N.A N.A N.A. N.A N.A

5.7 5.7 5.6 6.1

5.7 5.9 6.2 6.4

5.9 6.3 18.4 5.9 6.1 10.9

19.2 10.4 13.7 10.1

9.9 20.4 19.2 8.9 16.9 15.8

LC DC LC2D DC2D

4 101 71

4 101 71

16 101 71

2

31 11

6.3 6.5 6.5 6.7 5.1 5.9 6.0 6.1

6.7 6.5 6.8 7.0 5.8 5.8 6.2 6.2

6.5 7.0 12.7 7.1 6.8 16.2 5.6 6.1 11.0 6.1 6.9 16.0

14.0 10.5 9.1 18.4 16.5 17.0 11.0 8.7 19.1 17.4 13.1 10.1 8.2 17.5 16.0 16.9 11.6 10.1 21.6 19.4

SA-LC

8 N.A. 31

SA-DC

8 N.A. 31

SA-LC2D 4 N.A. 11

SA-DC2D 4 N.A. 11

4.7 4.9 4.6 4.9 4.8 5.4 8.5 4.5 4.9 4.9 5.1 5.3 5.3 9.3 4.4 4.7 4.7 4.8 4.7 5.2 8.0 4.5 4.5 4.9 5.0 5.1 5.3 7.5

11.8 9.6 8.6 16.3 16.1 12.4 9.3 8.2 16.7 16.0 12.0 9.5 8.3 16.7 15.4 12.1 9.6 8.3 16.2 15.3

Table 3. Word error rate (WER) of REVERB and CHiME4. Using self-attention for encoder and convolutional layer for decoder (SA{LC,DC,LC2D,DC2D} in Table 1) yield better performance than RNN and performance competitive to state-of-the-art Transformer.

data. CER of CSJ (581 hours) and word error rate (WER) of Librispeech (960 hours) are shown in Table 2. WER of REVERB and CHiME4 are shown in Table 3. For all the test sets, the combination of self-attention and convolutional layer achieved better performance than RNN and performance competitive to state-of-the-art Transformer. For clean sets of Librispeech, models with only a convolutional layer yield the best performance.
3.3. Discussion
By using lightweight or dynamic convolution for both encoder and decoder, performance is comparable to, or better than, RNN except for some sets of REVERB and CHiME4. For clean sets of Librispeech, performance is even better than Transformer. This means the convolutional layer, which looks only at a limited context, is more suitable for E2E-ASR of clean speech than RNN which encodes the whole utterance.
By using self-attention to encoder and convolutional layer to decoder, performance for all test sets is reaching to, or better than, state-of-the-art Transformer. Especially for REVERB and CHiME4, performance gains are larger than on other corpora. This means, at least for the decoder side, a convolutional layer is better probably because the limited context could eliminate the effect of the wrong

recognition history caused by noisy speech to be maintained for the entire utterance during beam search.
The effectiveness of adding convolution along the frequency axis is dependent on the test set. For example, on all sets of Librispeech, test sets of CHiME4 and real sets of REVERB, SA-DC2D is better than SA-DC. However, on another sets it is not always the case. Further analysis is necessary to conclude the effectiveness of adding convolution along the frequency axis.
4. CONCLUSION
In this paper lightweight and dynamic convolution originally proposed for machine translation is applied to ASR. In theory, training is faster because computational complexity is linear in input length while Transformer is quadratic. We also propose to use joint training and decoding with the CTC objective, convolution along the frequency axis and combination of self-attention and convolutional layer to achieve better performance. ASR experiments on various corpora showed our proposed method yields better performance than RNN and performance competitive to state-of-the-art Transformer. Detailed analysis of the results and computational efﬁciency are left as future work.

5. REFERENCES
[1] J. K. Chorowski et al., “Attention-based models for speech recognition,” in Proc. Advances in Neural Information Processing Systems (NIPS) 28, pp. 577–585. 2015.
[2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, March 2016, pp. 4960– 4964.
[3] D. Amodei et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” in Proc. of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016, pp. 173–182.
[4] R. Prabhavalkar et al., “A comparison of sequence-to-sequence models for speech recognition,” in Proc. Interspeech 2017, 2017, pp. 939–943.
[5] S. Watanabe et al., “Hybrid ctc/attention architecture for endto-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, Dec 2017.
[6] A. Zeyer, K. Irie, R. Schlter, and H. Ney, “Improved training of end-to-end attention models for speech recognition,” in Proc. Interspeech 2018, 2018, pp. 7–11.
[7] C. Lscher et al., “RWTH ASR Systems for LibriSpeech: Hybrid vs Attention,” in Proc. Interspeech 2019, 2019, pp. 231– 235.
[8] C. Chiu et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in Proc. ICASSP, April 2018, pp. 4774–4778.
[9] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” in Proc. ICASSP, April 2015, pp. 5206–5210.
[10] J. J. Godfrey, E. C. Holliman, and J. McDaniel, “Switchboard: telephone speech corpus for research and development,” in Proc. ICASSP, March 1992, vol. 1, pp. 517–520 vol.1.
[11] D. S. Park et al., “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 2613–2617.
[12] A. Vaswani et al., “Attention is all you need,” in Proc. Advances in Neural Information Processing Systems (NIPS) 30, 2017, pp. 5998–6008.
[13] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a norecurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP. IEEE, 2018, pp. 5884–5888.
[14] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequenceto-sequence speech recognition with the transformer in mandarin chinese,” in Proc. Interspeech 2018, 2018, pp. 791–795.
[15] A. Graves, S. Ferna´ndez, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369–376.
[16] S. Karita et al., “Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classiﬁcation and Language Model Integration,” in Proc. Interspeech 2019, 2019, pp. 1408–1412.
[17] S. Karita et al., “A comparative study on transformer vs RNN in speech applications,” arXiv preprint arXiv:1909.06317, 2019.

[18] J. Gehring et al., “Convolutional sequence to sequence learning,” in Proc. of the 34th International Conference on Machine Learning (ICML), 2017, pp. 1243–1252.
[19] L. Sifre, Rigid-motion scattering for image classiﬁcation, Ph.D. thesis, 2014.
[20] F. Wu et al., “Pay less attention with lightweight and dynamic convolutions,” in Proc. International Conference on Learning Representations (ICLR), 2019.
[21] S. Watanabe et al., “Espnet: End-to-end speech processing toolkit,” in Proc. Interspeech 2018, 2018, pp. 2207–2211.
[22] Y. Zhang, W. Chan, and N. Jaitly, “Very deep convolutional networks for end-to-end speech recognition,” in Proc. ICASSP, March 2017, pp. 4845–4849.
[23] Y. Zhang et al., “Towards end-to-end speech recognition with deep convolutional neural networks,” in Proc. Interspeech 2016, 2016, pp. 410–414.
[24] N. Zeghidour et al., “Fully convolutional speech recognition,” arXiv preprint arXiv:1812.06864, 2018.
[25] J. Li et al., “Jasper: An End-to-End Convolutional Neural Acoustic Model,” in Proc. Interspeech 2019, 2019, pp. 71–75.
[26] A. Hannun, A. Lee, Q. Xu, and R. Collobert, “Sequenceto-Sequence Speech Recognition with Time-Depth Separable Convolutions,” in Proc. Interspeech 2019, 2019, pp. 3785– 3789.
[27] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling with gated convolutional networks,” in Proc. of the 34th International Conference on Machine Learning (ICML), 2017, pp. 933–941.
[28] T. N. Sainath and B. Li, “Modeling time-frequency patterns with LSTM vs. convolutional architectures for LVCSR tasks,” in Proc. Interspeech 2016, 2016, pp. 813–817.
[29] J. Li, A. Mohamed, G. Zweig, and Y. Gong, “LSTM time and frequency recurrence for automatic speech recognition,” in Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), December 2015.
[30] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous speech corpus of Japanese,” in Proc. the Second International Conference on Language Resources and Evaluation (LREC’00), 2000.
[31] K. Kinoshita et al., “A summary of the reverb challenge: stateof-the-art and remaining challenges in reverberant speech processing research,” EURASIP Journal on Advances in Signal Processing, vol. 2016, no. 1, pp. 7, Jan 2016.
[32] E. Vincent et al., “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” Computer Speech & Language, vol. 46, pp. 535 – 557, 2017.
[33] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B. Juang, “Speech dereverberation based on variancenormalized delayed linear prediction,” IEEE Transactions on ASLP, vol. 18, no. 7, pp. 1717–1731, 2010.
[34] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on ASLP, vol. 15, no. 7, pp. 2011–2022, 2007.

