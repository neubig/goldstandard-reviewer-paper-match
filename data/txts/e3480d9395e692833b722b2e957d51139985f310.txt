General-Purpose Question-Answering with MACAW
Oyvind Tafjord and Peter Clark
Allen Institute for Artiﬁcial Intelligence, Seattle, WA, U.S.A. {oyvindt,peterc}@allenai.org

arXiv:2109.02593v1 [cs.CL] 6 Sep 2021

Abstract
Despite the successes of pretrained language models, there are still few high-quality, general-purpose QA systems that are freely available. In response, we present MACAW, a versatile, generative question-answering (QA) system that we are making available to the community. MACAW is built on UniﬁedQA, itself built on T5, and exhibits strong performance, zero-shot, on a wide variety of topics, including outperforming GPT-3 by over 10% (absolute) on Challenge300, a suite of 300 challenge questions, despite being an order of magnitude smaller (11 billion vs. 175 billion parameters). In addition, MACAW allows different permutations (“angles”) of its inputs and outputs to be used, for example MACAW can take a question and produce an answer; or take an answer and produce a question; or take an answer and question, and produce multiple-choice options. We describe the system, and illustrate a variety of question types where it produces surprisingly good answers, well outside the training setup. We also identify question classes where it still appears to struggle, offering insights into the limitations of pretrained language models. MACAW is freely available, and we hope that it proves useful to the community.1
1 Introduction
Although modern pretrained language models have proved surprisingly effective at solving datasets, e.g., (Radford et al., 2018; Raffel et al., 2020; Khashabi et al., 2020a), there are still few high-quality, general-purpose, off-theshelf question-answering (QA) systems freely available. UniﬁedQA (Khashabi et al., 2020a) is a powerful QA system, but mainly trained for span prediction and multiplechoice selection rather than answer generation. GPT-3 appears powerful, but is not freely available to the public (Brown et al., 2020). One nearest to our goal is Google’s T5-based CBQA (closed-book QA) system (Roberts et al., 2020), but in our tests of the T5-CBQA model trained on Natural Questions (Kwiatkowski et al., 2019), it did not perform as well as MACAW (Section 4.2).2 Similarly, the new
1MACAW is available at https://github.com/allenai/macaw 2There are other T5-CBQA versions alternatively trained on WebQuestions and TriviaQA that we did not evaluate, although NaturalQuestions is arguably the most general and varied of the alternate training sets. See https://github.com/googleresearch/google-research/tree/master/t5 closed book qa

Q: How would you make a house conduct electricity? → A: paint it with a metal paint
Q: What is an implication of a cat being an animal? → A: it has feelings
Q: Could a newborn look over the top of a fully grown horseradish plant? → A: no, because the plant is so tall
Q: Why couldn’t Aristotle have used a cellphone? → A: he was born before cellphones
Q: A young boy was rushed to the hospital emergency room, but the ER doctor saw the boy and refused to operate. ”This boy is my son,” the doctor said. But the doctor wasn’t the boy’s father. How could this be? [Intended answer to the riddle: The doctor was the boy’s mom] → A: the doctor mistook the boy for his own son [Reasonable alternative answer!]
Figure 1: A few examples of the more impressive answers given by MACAW. More examples (both good and bad) are given in Section 5.
Jurassic-1 model from AI21 Labs3 appears powerful, but again did not perform as well in our QA tests (Section 4.2). In response, we are releasing MACAW, a general-purpose, T5-based QA system that we have found to be to be surprisingly strong, zero-shot. In this paper, we describe how MACAW was trained, illustrate the variety of capabilities it has, and also identify various question classes it still struggles with. We hope MACAW proves useful for the community.
MACAW has three interesting features. First, it often produces high-quality answers to questions far outside the domain it was trained on, sometimes surprisingly so. Several examples are shown in Figure 1, and we show numerous other examples later in this paper (Section 5.1). However, it can also make mistakes. We also give a selection of these, and attempt to characterize where its weaknesses are (Section 5.2).
Second, MACAW allows different permutations (“an-
3https://www.ai21.com/blog/announcing-ai21-studio-andjurassic-1

gles”) of inputs and outputs to be used. For example, we can give it a question and get an answer; or give it an answer and get a question; or give it a question and answer and get a set of multiple-choice (MC) options for that question. This multi-angle QA capability4 allows versatility in the way MACAW can be used, include recursively using outputs as new inputs to the system. While other researchers have explored permuting inputs and outputs to some degree, e.g., (Hase et al., 2020), MACAW has such capabilities built into its machinery.
Finally, MACAW also generates explanations as an optional output (or even input) element. Although MACAW’s explanations are of typically of lower quality than its answers, and are not full chains of reasoning, the fact it can generate plausible explanations at all is an unusual feature.
We ﬁrst describe multi-angle training and how MACAW was trained. We then report quantitative experiments with MACAW, including comparing its zero-shot performance with several other large language models on Challenge300, a suite of 300 challenge questions designed to push various limits of question-answering behavior. We ﬁnd MACAW outperforms other large-scale models by over 10% (absolute) on this dataset, including GPT-3 despite being an order-of-magnitude smaller (11B MACAW vs. 175B GPT3). We then give a qualitative analysis of MACAW’s behavior, identifying classes of problems where it succeeds and also where it fails. Finally we reﬂect on its behavior and offer MACAW to the community. MACAW is available at https://github.com/allenai/macaw.
2 Multi-Angle Question-Answering
2.1 Slots, Values and Angles
We take advantage of the ﬂexible nature of text-to-text transformers like T5 (Raffel et al., 2020) to train models across multiple “angles” for each dataset. Each example in the dataset is considered as a set of slots Si and corresponding values Vi. An angle Ai = {Ssi } → {Sti } then corresponds to a speciﬁc set of source slots Ssi and a set of target slots Sti , and the associated task is to predict the values of the target slots given the source values.
For instance, in a multiple-choice QA dataset, like ARC (Clark et al., 2018) or RACE (Lai et al., 2017), the slots might be Q (question), M (MC options), A (correct answer), C (context). The usual task is represented by the primary angle QMC→A (given question, MC choices and context, what is the correct answer?). Other angles might include QC→A (answer the question without seeing the MC choices), QAC→M (generate plausible MC choices), AC→QM (given answer and context, generate a question and answer options). See Figure 2 for more examples.
The semantics of slots are deﬁned by what MACAW saw during training (Section 3). During training, the context (C) contains either a passage or retrieved text relevant to the question, and the explanation (E) consists of a few (typically two or three) general sentences relevant to the answer
4Hence then model’s name MACAW (“Multi-angle c(q)uestionanswering”).

Slot: Example value C (context, e.g., from IR): Roller skating is a pop-
ular hobby these days. Roller skates have four wheels.... Q (question): Which surface is best for rollerskating? M (multiple-choice (MC) options): (A) gravel (B) blacktop (C) sand A (answer): blacktop E (explanation): A wheeled vehicle requires smooth surfaces.
Angle: Description QMC→AE: Generate answer and explanation given
question, choices and context (primary angle). QM→AE: Same, but in absence of retrieved context QMC→A: Only generate answer QC→A: Generate answer without access to MC op-
tions QMEC→A: Also include explanation in input QAC→M: Generate plausible MC options given
question, answer and context AC→QM: Generate plausible question and MC op-
tions, given answer and context
Figure 2: The different slots (input/output elements) and sample angles supported by MACAW.
(but not a formal chain of reasoning). Examples are given in Figure 2 (upper box) and Section 2.6.
For each dataset we collect a set of angles which would be considered reasonable tasks to perform. E.g., in the RACE dataset the context is usually critical to answer a situated question (e.g., “What does the doctor think of Heelys?”) so we do not consider the QM→A angle without the context, while this angle is appropriate for ARC where the context is just a potentially helpful retrieved text passage.
2.2 Text Encoding of Angles
We employ a simple text format to encode an angle input/output pair {Ss1 , Ss2 , ...} → {St1 , St2 , ...}:
INPUT: "$St1 $ ; $St2 $ ; ... $Ss1 $ = Vs1 ; $Ss2 $ = Vs2 ; ..." OUTPUT: "$St1 $ = Vt1 ; $St2 $ = Vt2 ; ..."
In other words, in the INPUT ﬁeld the desired output slots Sti are listed without any associated value, and the input slots Ssi are listed with their corresponding input values. For instance, to provide the “question” and “mcoptions” (multiple-choice options) as inputs, and request the “answer” and “explanation” slots in the output, the INPUT format might look as below, resulting in the corresponding OUTPUT from MACAW:
INPUT: "$answer$ ; $explanation$ ; $question$ = Which surface is best for rollerskating? ; $mcoptions$ = (A) gravel (B) sand (C)

2

blacktop" OUTPUT: "$answer$ = blacktop ; $explanation$ = A wheeled vehicle requires smooth
surfaces."
2.3 Ordering of Slots within an Angle
We can either treat the input slots as an unordered set or in a certain ﬁxed order. Given the nature of the transformer encoder, it is not expected that the input order has great signiﬁcance. In practice we scramble the order of the input and output slots during training, except putting the ”context” slot at the end as it tends to be the one that might run over the token limit (512 subword tokens in the case of T5).
If there are multiple output slots, such as producing both answer and explanation, their ordering might carry more signiﬁcance due to the left-to-right nature of decoding. E.g., ﬁrst producing explanation followed by answer, is technically generating the answer conditioned on the already generated explanation. Again, for simplicity and practicality, for MACAW we train (and evaluate) with randomly scrambled orders of output slots.
2.4 Sampling of Angles during Training
We describe the precise training of MACAW shortly. During training, we sample the possible angles across the training set rather than considering every angle for every training instance. The training recipe includes the following:
• Each angle can have a heuristic scaling factor for how often it is sampled relative to others (used as weak bias for which angles are more meaningful).
• We iterate through the training instances multiple times (especially if there are many angles and not that many training instances)
• If a sampled angle does not exist for a training instance (e.g., the explanation value is only available for a subset of instances), the angle is resampled. This allows handling of heterogenous datasets where slots are partially available.
For evaluation we generate all angles for every instance (with random scrambling of the slot orders if that was the chosen mode during training, as was done for the MACAW model).
2.5 Decoding and Evaluation
MACAW’s default decoding is done with greedy decoding, optionally with a small beam search, which is appropriate for well-deﬁned slot values like answers. For more openended slot values, like question generation, MACAW also supports sampling (e.g., nucleus sampling (Holtzman et al., 2020)), allowing alternate outputs to be generated.
When the full output string has been generated, e.g., Section 2.2, it is straightforward to parse it with a regular expression pattern to extract the produced slots and values. These can then be evaluated according the their usual metrics. If an expected slot is missing, it is counted as a failure, but in practice this almost never happens.

An alternative decoding approach, most suitable for multiple-choice answers, is to forcibly generate each answer choice, and the product of probabilities of the output tokens then provide a conﬁdence score for each answer choice.
2.6 Examples of Different Angles Some examples of using MACAW with different angles are shown below, where the right arrow (“→”) indicates MACAW’s generated output, given the inputs. Note that some of these angles were not seen during training (in fact, only the ﬁrst two are seen during training), yet MACAW is still able to reliably handle them. (The full list of angles seen in training is described shortly in Section 3.2 and Table 3).
Q→A: Given a question, produce an answer.
Q: What gas do producers produce that consumers need to survive? → A: oxygen
Q→AE: Given a question, produce an answer + explanation.
Q: Which force pulls objects to the ground? → A: gravity → E: Gravitational force causes objects that have mass to be pulled down on a planet.
A→QE: Given an answer, produce a question + explanation.
A: elephant → Q: Which animal has the largest ears? → E: The ears of an elephant are the largest.
A→QM: Given an answer, produce a question + multiplechoice options.
A: car battery → Q: Which of these items is necessary for a car to start? → M: (A) car battery (B) windshield wiper blade (C) car radio (D) car radio antenna
E→QA: Given an explanation, generate a question + answer.
E: The leaves of a plant convert sunlight into food for the plant. → Q: How do plants get energy? → A: from the sun
2.7 Output Conﬁdences MACAW can also output the generation scores for a set of candidate answers, conventionally invoked with a “X” (eXplicit outputs) slot, e.g.,:
Q: What is the largest animal in the world? X: (A) mouse (B) whale (C) elephant → A: whale (0.007), elephant (0.005), mouse (1.4e-8)
Note the conﬁdences do not add to one, as other answers (e.g., “blue whale”) are possible but are not listed. To further condition the naturally generated answers, the question can be formulated as multiple-choice using the “M” slot as well:

3

Datasets BoolQ, NarrativeQA, SQuAD 2.0 ARC, OBQA
RACE, MCTest

Angles QC→A, AC→Q
QMC→A, QC→A, QM→A, QAC→M, MAC→Q, AC→QM QMC→A, QC→A, QAC→M, MAC→Q

Table 1: Datasets and angles used in training of multi-angle UniﬁedQA (the slots are Q=Question, C=Context, M=MC options, A=Answer).

Q: What is the largest animal in the world? M: (A) mouse (B) whale (C) elephant X: (A) mouse (B) whale (C) elephant → A: whale (0.999), elephant (3.9e-5), mouse (2.4e11)
In this case the conﬁdences, which are the product of the internal output token probabilities, do tend to add up to one as the model is strongly biased towards picking one of the answers in from the “M” slot.
3 Training MACAW
MACAW is built on top of the text-to-text pretrained T5 transformer (Raffel et al., 2020), by ﬁrst training a multi-angle version version of UniﬁedQA (Khashabi et al., 2020b), followed by further ﬁne-tuning on science questions with explanations, using the ARC and ARC-DA datasets along with explanations from WorldTree (Jansen et al., 2018).
3.1 Multi-Angle UniﬁedQA
The multi-angle version of UniﬁedQA was trained on the 7 core datasets with associated angles listed in Table 1. The 11B model was ﬁnetuned for 120k steps starting from T511B with batch size of 8 and the Adafactor optimizer. These datasets vary greatly in size (from 1.5k to 130k training instances), following UniﬁedQA we sample equally from the 7 datasets. For direct comparison we also trained a similar single-angle version using the same setup.
For the ARC and OBQA datasets, the context (“C”) consists of 10 sentences retrieved from a general text corpus based on the question text plus each of multiple-choice options (ranked by IR score, but always keeping the top result for each option).5
The performance of these models on the primary angle is very similar to the original UniﬁedQA model. Table 2 shows a comparison between the scores of the single-angle and multi-angle models on the development sets, showing the multi-angle is generally not much behind the single-angle variant, while providing more general functionality through the alternate angles.
5For this we use the Aristo Corpus, a Web-crawled corpus containing 280GB of general and science-related sentences augmented with ≈80k additional science textbook sentences (Clark et al., 2016).

Model→ Dataset↓ BoolQ NarrativeQA SQuAD 2.0 ARC MCTest OBQA RACE

Single-Angle 11B 90.8 66.5 91.1 88.6 96.6 87.4 88.0

Multi-Angle UniﬁedQA

11B 3B large (770M)

90.3 89.1

85.4

66.8 65.4

62.8

90.3 89.4

86.8

87.0 81.9

72.2

95.9 94.4

90.9

88.4 81.8

71.4

87.7 84.4

79.2

Table 2: Model performance (averaged over UniﬁedQA datasets (dev partition), measuring accuracy except for SQuAD 2.0 (token F1) and NarrativeQA (ROUGE-L)). Multi-angle UniﬁedQA retains performance compared with single-angle (for same size models, columns 1 and 2), while adding multi-angle capabilities. Evaluation is on the primary angle (QC→A for the 3 ﬁrst datasets, QMC→A for the other 4). ARC includes both the Easy and Challenge categories.

Dataset ARC
ARC-DA

Angles QMC→AE, AQC→M, CQME→A, QME→A, QE→A, QMC→A, QC→AE, QM→AE, QMAC→E, QMA→E QC→AE, Q→AE, QC→A, Q→A, CQE→A, QE→A, AE→Q, AC→Q, QA→E, AQC→E

Table 3: Datasets and angles used in training of MACAW (with slots as in Table 1 plus E=Explanation).

We train multi-angle UniﬁedQA in three sizes based on T5-11B, T5-3B, and T5-large. As seen in Table 2, for some of the datasets there is a signiﬁcant drop in evaluation scores for smaller sizes, but the scores are still high in general.
3.2 MACAW
For the ﬁnal MACAW model, we further ﬁne-tune multiangle UniﬁedQA on the ARC dataset as well as the ARC-DA dataset, a dataset of Direct Answer (“open response”, “freeform”) science questions (Bhakthavatsalam et al., 2021) (with 1250 questions in the training set).
For each question we add an input context (“C”) based on retrieval from a text corpus as described in the previous section (for ARC-DA the retrieval is based only on question text as there are no answer options available).
We also add an explanation (“E”) to each question using data from the WorldTree V2 explanation bank (Jansen et al., 2018). WorldTree contains explanations for a subset of the questions in ARC and ARC-DA (the fraction of questions covered is about 65% for ARC and 50% for ARC-DA). We construct a short explanation paragraph by randomly shufﬂing the sentences marked as “CENTRAL” (in the few cases with more than 5 such sentences, we sample 5 of them).
With ﬁve available input/output slots there is a plethora of possible angles to train on. We select a subset that seem the most interesting, as listed in Table 3, and use these for ﬁne-tuning for 6k further steps.

4

Figure 3: Average score of the four different models on different categories of questions (ignoring categories with less than ﬁve questions). The numbers in parentheses denotes the number of questions in each category. Categories are ordered by averageof-averages (highest to lowest), i.e.,, the models together perform best on general knowledge and worst on false presuppositions. The tabular version of this data is in the Appendix.

Angle→

QMC QMC QMEC QM QM QME

Dataset+Model↓ →AE →A →A →AE →A →A

ARC-Easy:

MACAW (11B) 90.9 91.2 94.0 85.1 84.9 91.4

MACAW-3B

87.5 87.9 91.6 77.7 76.7 85.3

MACAW-large

82.5 82.5 86.1 66.3 63.9 79.8

ARC (Challenge):

MACAW (11B) 76.9 76.9 86.3 74.6 74.6 86.6

MACAW-3B

69.6 68.2 80.9 66.2 67.9 77.9

MACAW-large

53.9 57.2 66.9 48.2 50.5 67.2

Table 4: Scores for the answer output slot A on ARC (Easy and Challenge) multiple-choice development sets, for six different angles.

4 Quantitative Performance of MACAW
4.1 The ARC dataset
While this paper mainly focuses on an analysis of MACAW’s capabilities and limitations, we note that the ”answerfocused” variant MACAW-answer-11B is at the time of publication at the top of the leaderboards for the datasets ARC (with a score of 81.4%),6 ARC-Easy (92.7%),7 and ARCDA (81%).8 This variant was trained without the explanation slot and with a focus on the answer-output angles. This model is also available in our software release.9
To get a sense of the variation with model size, Table 4 gives scores on the ARC development set for the smaller MACAW-3B and MACAW-large in addition to the
6https://leaderboard.allenai.org/arc/submissions/public 7https://leaderboard.allenai.org/arc easy/submissions/public 8https://leaderboard.allenai.org/geniearcda/submissions/public 9https://github.com/allenai/macaw

default MACAW (11B). There is a clear performance drop for smaller sizes, but the smaller models still provide good performance and might be more practical for deployment and further research iterations.
In Table 4, we observe that if the answer explanation is included in the input angle (result columns 3 and 6), the answer accuracy signiﬁcantly improves. This is perhaps not surprising as the explanation typically strongly hints at (or even includes) the right answer (the effect is larger than indicated in the tables, since only a subset of questions actually have explanations in the dataset).
One could hypothesize that feeding the model’s own explanation back as input could also help (ﬁrst run QC→AE, then use the E output as input to QEC→A), but from smallscale tests this generally only had a minor effect on the score, while tending to make the precision-recall curves look worse (presumably because originally uncertain, incorrect answers, will now get reinforced through the explanation to higher conﬁdence).
4.2 The Challenge300 Dataset
We also assembled a dataset of 300 challenge questions, called Challenge300, based on our attempts to “break” MACAW using a wide variety of question types. Most of the questions were created from scratch, probing different styles of problem, plus a handful were drawn from the excellent challenge questions in (Davis, 2016) and (Marcus and Davis, 2020a). We recorded all the questions tried (both those MACAW got right, and those it got wrong), rather than cherry-picking good/bad cases. We also performed a loose classiﬁcation of those questions into 22 different categories, described in Table 5. Note that this categorization is somewhat approximate, as questions can fall into more than one category (in such cases, we attempted to select the domi-

5

Category commonsense comparison entity substitution entity tracking estimation example generation explanation false presupposition general knowledge generation history human behavior hypothetical math meta-reasoning riddle
science spatial steps story understanding
temporal
Winograd

# Qns 38 2 4 13 4 2 14 9 70 1 2 5 29 2 6 2
41 11 15 25
2
3

Description + Example Obvious (to a person) facts about the world If I put some cheese in the fridge, will it melt? Relation between two entities How do pandas and parrots differ? Find a suitable replacement entity for a task How would you bang in tent pegs without a hammer? Tracking entity states over time My house is red. I painted my house white. What color is my house now? Fermi-style numeric estimation problems (Kalyan et al., 2021) How many banknotes can you ﬁt in a school bus? Create an illustration of a general phenomenon If you let go of an object, then gravity pulls it to the ground. What is an example of this phenomenon? “Why...?” questions Why do houses have roofs? Trick questions that presuppose something that is not true (Kim et al., 2021) What year did Tom Hanks land on the moon? General facts about the world What is shiplap? Production of prose Tell me a story about a car accident. Questions about world history What were the causes of World War II? Questions involving human emotions I feel sad. What could I do to cheer myself up? Questions about hypothetical and/or counterfactual situations If plastic was a conductor, then would a plastic spoon conduct electricity? Numeric computations What is 241 + 7864? Questions requiring reﬂection about reasoning itself What is an incorrect implication of a cat being an animal? Trick stories with a non-obvious explanation A young boy was rushed to the hospital emergency room, but the ER doctor saw the boy and refused to operate. ”This boy is my son,” the doctor said. But the doctor wasn’t the boy’s father. How could this be? Questions in the general area of science What gases are involved in photosynthesis? Various spatial reasoning tasks John is left of Sue. Where is Sue relative to John? List the sequence of actions to achieve a goal What are the steps involved in replacing a light bulb? Tests for facts implicit in a short story I crashed my car. When I ﬁnally left the hospital, all I wanted to do was sleep. I had to call a taxi. Why was I in hospital? Reasoning including temporal constraints (example below from (Marcus and Davis, 2020a)) Moshe posted on Facebook a photograph showing Maurice Ravel, Francois Poulenc, Frederic Mompou, and Erik Satie. Satie died in 1925. Poulenc was born in 1899. So the photograph must have been taken when? Winograd schema questions (requires commonsense for pronoun resolution) (Levesque et al., 2011) The elephant couldn’t ﬁt into the box because it was too big. What was too big?

Table 5: Categories of questions in the Challenge300 dataset.

6

Model T5-CBQA (T5.1.1.XXL, NaturalQ) Jurassic-1 (jumbo, T=0) GPT-3 (davinci T=0) MACAW (11B)

Score (%) 57.3 64.9 64.9 75.0

# incoherent 28 12 10 2

Table 6: Scores on the Challenge300 dataset, plus absolute number of incoherent (nonsensical) answers produced. MACAW signiﬁcantly outperforms the other systems on this dataset. All models are applied zero-shot.

nant category). However, it is still informative for analyzing the successes and failures of MACAW, which we discuss in detail in Section 5 shortly.
For comparison, we also gave the Challenge300 questions to T5-CBQA (size XXL)10, GPT-3 (davinci)11, and the recent Jurassic-1 (jumbo) model from AI21 Labs12. As the questions are direct answer (“open response”), with (typically) multiple, valid answers, we scored the answers manually. We also noted the (rare) cases where an answer was incoherent, i.e., was nonsensical (rather than just wrong). For example, for the question ”Mary owns a canary named Paul. Did Paul has any ancestors who were alive in the year 1750?” the answer ”Paul” was marked as incoherent. The Challenge300 dataset, answers given by each of the four systems tested, and their scores are available as part of the MACAW release.
The results on this dataset are shown in Table 6. On this small but challenging test, MACAW signiﬁcantly outperforms the other systems (+10% absolute), as well as having fewer incoherent answers.
We also computed the average scores per question category, shown in Figure 3, ignoring categories with fewer than ﬁve questions (where averages are more unreliable). The tabular version of this data, including all categories, is also provided in the Appendix. At a high level, the general trends are similar among the different language models tested, e.g., they all tend to excel at general knowledge questions, but struggle with questions based on false presuppositions.
5 Qualitative Results and Analysis
We now present a qualitative analysis of MACAW’s strengths and weaknesses, based on the Challenge300 questions. Note that most of these questions are well outside the domain of the training questions, and no other context is supplied to MACAW to answer them beyond the questions themselves. Also note that all these examples are using MACAW zeroshot.
We ﬁrst present some of the impressive (to us) examples of MACAW’s behavior. We then show some cases where it
10The most powerful CBQA model, built on T5-11B with further pretraining using salient span masking (SSM), https://huggingface.co/google/t5-xxl-ssm-nq
11With prompt simply ”Q: <question> A:”. The GPT-3 continuation invariably contains the answer followed by more QA pairs labeled ”Q:” ”A:”. We truncate off those additional QA pairs.
12https://www.ai21.com/blog/announcing-ai21-studio-andjurassic-1

produces wrong answers, and characterize those failures.
5.1 MACAW: Good Examples Below, Q denotes the question posed to MACAW, while A denotes MACAW’s generated answer. We also tentatively speculate about what skills the answers appear to reveal. 1. Entity Substitution: Identifying a suitable replacement item. We observe similar good answers for several other probes (e.g., if no stove, use a microwave).
Q: James went camping in the woods, but forgot to bring a hammer to bang the tent pegs in. What else might he use? → A: rocks Q: James went camping in the woods, but forgot to bring a hammer to bang the tent pegs in. Why might a rock be a useful substitute for the hammer? → A: it is hard
2. Hypotheticals: MACAW shows some ability to reason about novel (hypothetical/counterfactual) situations. The second constrast query below suggests that MACAW was doing more than just guessing.
Q: If plastic was a conductor, then would a plastic spoon conduct electricity? → A: yes Q: If plastic was an insulator, then would a plastic spoon conduct electricity? → A: no
Similarly, MACAW gives a suprisingly insightful answer about a novel situation:
Q: How would you make a house conduct electricity? → A: paint it with a metal paint
Finally the two examples below hints at some awareness of the role of an object (here, a roof) in a hypothetical situation:
Q: If my house did not have a roof, and it rained, what would happen to my bed upstairs? → A: it would get wet Q: If my house had a roof, and it rained, what would happen to my bed upstairs? → A: it would stay dry
as well as correctly reasoning about a hypothetical bird:
Q: If a bird didn’t have wings, how would it be affected? → A: it would be unable to ﬂy
3. Meta-reasoning: This surprising behavior hints at knowing what an implication is, and being able to illustrate that knowledge in a speciﬁc context.
Q: What is an implication of a cat being an animal? → A: it has feelings Q: What is an incorrect implication of a cat being an animal? → A: it is a human

7

4. False presuppositions: Questions based on a false premise, such as the below, frequently fool language models (Kim et al., 2021). The example below is somewhat well-known, e.g., Google (incorrectly) answers 1995, but MACAW surprisingly handles this gracefully. In general, however, MACAW is fooled by these kind of examples (Section 5.2).
Q: What year did Tom Hanks land on the moon? → A: no year
5. Story Understanding: In our tests, MACAW generally answers reading comprehension questions about short stories well, for example:
Q: Karen was assigned a roommate her ﬁrst year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating. What happened that might cause Karen to become good friends with her roommate? → A: they had a good time together
6. Commonsense: The below suggests MACAW has some commonsense awareness about the relationships between melting, temperature, and fridges.
Q: If I put some cheese in the fridge, will it melt? → A: no, because it will be too cold
7. Comparisions: MACAW is able to answer unusual comparison questions, such as below. Such questions are interesting as they are unlikely to have been seen in MACAW’s/T5’s training corpora.
Q: How do pandas and parrots differ? → A: pandas have black and white fur, while parrots have brightly colored feathers and are usually green or yellow in color
5.2 MACAW: Example Failures
MACAW answered about 24% of the Challenge300 questions incorrectly. Below we show some examples of these failures, and categorize the nature of the problem that it struggles with. Again, note that all these examples are using MACAW zero-shot. 1. Non-trivial Arithmetic: MACAW generally fails at nontrivial arithmetic, although it appears to know the answer is some number. MACAW will handle simple arithmetic correctly, e.g., Q: What is 7 + 5? rightarrow 12 [correct]; Q: What is 25 + 59? → 84 [correct], but with larger numbers (3 or more digits) it typically fails. Interestingly, the answers are often in the right ballpark, e.g., below. Similarly, Q: What is 42623 + 543456? → 870315 [incorrect, though the right order of magnitude].
Q: What is 241 + 7864? → A: 12345 [wrong]

2. Entity Tracking and State Changes: MACAW struggles with stories about a changing world state. The below example is similar to those in the bAbI dataset (Tasks 2 and 3) (Weston et al., 2016). While models can be trained for reasoning about processes and a changing world state, e.g., (Henaff et al., 2016; Dalvi et al., 2018; Tandon et al., 2018), MACAW does not have this capability zero-shot.
Q: John was in the bedroom. John put a coin in a bag. Then John took the bag to the kitchen. Then John took the coin out of the bag. John took the bag to the garden. Where was the bag at the end? → A: in the kitchen [wrong]
Similarly, MACAW struggles to combine counting and object tracking, as illustrated below.
Q: Imagine an empty cup. Now put a coin in the cup. Now take the coin out of the cup. How many coins are in the cup now? → A: 1 [wrong]
A ﬁnal example of struggling to track worlds is below:
Q: My house is red. I painted my house white. What color is my house now? → A: it is now white [correct] Q: My house is blue. I painted my house red. What color was my house before I painted it? → A: blue [correct] Q: My house is red. My neighbor’s house is blue. I painted my house white. What color is my house now? → A: it’s the same color as my neighbor’s house [wrong] Q: My house is red. I painted my neighbor’s house white. What color is my house now? → A: half red, half white [wrong]
3. Spatial Reasoning: MACAW is unreliable at non-trivial spatial reasoning, as
illustrated below, suggesting it is not constructing a latent spatial layout during question interpretation:
Q: John is left of Sue. Where is Sue relative to John? → A: to his right [correct] Q: John is left of Sue. Sue is left of Mike. Where is Mike relative to John? → A: to his right [correct] Q: John is left of Sue. Sue is left of Mike. Where is Sue relative to John? → A: to his left [wrong]
4. Hypotheticals: Meaning-Shifting Modiﬁers Below, a subtle change (“electric car”) fools MACAW.
Q: If an electric car did not have any gas in it, how would it be affected? → A: it would not move [wrong]
5. Some Winograd Schema: Winograd Schema questions ask a model to correctly resolve a pronoun, typically requiring commonsense (Levesque et al., 2011). While MACAW generally answers them correctly, it can make mistakes such as the example below.

8

Q: The elephant couldn’t ﬁt into the box because it was too big. What was too big? → A: the box [wrong]
6. False Presuppositions: In general, MACAW is fooled by questions with false presuppositions, such as below. (A counterexample was shown earlier in Section 5.1 where MACAW noted Tom Hanks has not been to the moon).
Q: How old was Mark Zuckerberg when he founded Google? -→ A: 17 [wrong, trick question]
7. Commonsense: While MACAW correctly answers many questions apparently requiring commonsense, it can make errors, such as the below:
Q: What happens if I drop a glass on a bed of feathers? → A: the glass shatters [wrong]
Similarly, MACAW fails the following false presupposition, commonsense question adapted from from (Davis, 2016):
Q: Sally’s favorite cow died yesterday. When will the cow be alive again? → A: in a few days [wrong]
Marcus and Davis have similarly probed GPT3 with other commonsense problems (asking for a sensible continuation of a setup), and note that GPT3 fails a substantial number of them (Marcus and Davis, 2020b). While we have not probed such commonsense capabilities systematically for MACAW, we expect it will similarly fail many of them also. We note that in our results by category in Figure 6, all the models perform relatively poorly on commonsense questions (compared with other types).
Orthogonal to our question categories, we also note the following interesting behaviors of MACAW: Overly Brief Answers: MACAW typically generates short answers, sometimes so short that they can barely be considered correct, such as below:
Q: How does photosynthesis work? → A: sun shines on plants [highly incomplete]
Iterative queries: Below, we ask a simple question, then re-ask the question but ask for a different answer. MACAW is able to sustain this for several iterations, but eventually breaks down. (Below, the manual question and MACAW answer are shown together on the same line.)
What is made of metal? a car What is made of metal, besides a car? a door What is made of metal, besides a car and a door? a bicycle What is made of metal, besides a car, a door, and a bicycle? a spoon What is made of metal, besides a car, a door, a bicycle, and a spoon? a spoon [Now repeating an answer]
Generating Narratives: We can similarly ask MACAW to generate a plausible event sequence (“story”) by iteratively

giving a scenario, asking “What happens next?”, and then adding the answer back into the question and re-asking it. For example:

Some kids are planning a rollerskating race. happens next? They practice. Some kids are planning a rollerskating race. practice. What happens next? They fall. Some kids are planning a rollerskating race. practice. They fall. What happens next? ...

What They They

Eventually MACAW starts repeating answers, as illustrated below as a continuation of the earlier questions. The sequence of events in the question below reﬂect MACAW’s earlier answers to the “....What happens next?” questions.

Some kids are planning a rollerskating race. They practice. They fall. They get up and try again. They fall again. They give up. They lose interest in the sport. They stop trying. They never learn. They never learn. They never learn. They never learn. ... What happens next? They never learn.

While a possibly plausible sequence, this is hardly a good story.

5.3 Other Models’ Answers
While our focus here is on MACAW, we note that the three other models tested (GPT-3, T5-CBQA, and Jurassic-1) similarly exhibit moments of both brilliance and ignorance on Challenge300, with overall lower scores than MACAW (Table 6). The full list of all the models’ answers is included in the MACAW release.

5.4 Harmful and Offensive Answers: A Note of Caution
As a ﬁnal word of caution: like other pretrained models that have seen potentially harmful/offensive text in pretraining, MACAW is capable of producing biased and/or offensive answers depending on the question, a phenomenon of concern and signiﬁcant attention in the community, e.g., (Li et al., 2020; Zhou et al., 2021). Care must be used when deploying large-scale language models such as MACAW in practical settings.

6 Summary
To assist other researchers, we have released MACAW, a high-quality, T5-based QA system that exempliﬁes both the power and limits of current pretrained language models. MACAW exhibits strong performance, zero-shot, on a wide variety of topics, including outperforming GPT-3 by over 10% (absolute) on Challenge300, a suite of 300 challenge questions, despite being an order of magnitude smaller (11 billion vs. 175 billion parameters). In addition, a MACAWbased model currently tops the leaderboards on the ARC datasets (Section 4.1). One might consider MACAW as a language model highly optimized for question-answering tasks, including allowing different permutations of input/output slots (“angles”) related to question-answering.

9

We have also illustrated some surprisingly impressive answers MACAW produces, as well as some categories of questions that it still struggles with, providing insights into the strengths and weaknesses of MACAW and likely other transformer-based QA systems. We hope that MACAW proves useful to the community, both as a zero-shot QA system, and as a strong starting point for further ﬁne-tuning on speciﬁc tasks where training data is available and the highest precision possible is required. MACAW is available at https://github.com/allenai/macaw.
References
S. Bhakthavatsalam, D. Khashabi, T. Khot, B. D. Mishra, K. Richardson, A. Sabharwal, C. Schoenick, O. Tafjord, and P. Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. ArXiv, abs/2102.03315, 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In NeurIPS, 2020.
P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge. ArXiv, abs/1803.05457, 2018.
P. Clark, O. Etzioni, T. Khot, A. Sabharwal, O. Tafjord, P. D. Turney, and D. Khashabi. Combining retrieval, statistics, and inference to answer elementary science questions. In AAAI, 2016.
B. Dalvi, L. Huang, N. Tandon, W. tau Yih, and P. Clark. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In NAACL-HLT, 2018.
E. Davis. How to write science questions that are easy for people and hard for computers. AI Mag., 37:13–22, 2016.
P. Hase, S. Zhang, H. Xie, and M. Bansal. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? In EMNLP, 2020.
M. Henaff, J. Weston, A. D. Szlam, A. Bordes, and Y. LeCun. Tracking the world state with recurrent entity networks. In ICLR, 2016.
A. Holtzman, J. Buys, M. Forbes, and Y. Choi. The curious case of neural text degeneration. ArXiv, abs/1904.09751, 2020.
P. A. Jansen, E. Wainwright, S. Marmorstein, and C. T. Morrison. Worldtree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In LREC, 2018. Also arXiv:1802.03052.
A. Kalyan, A. Kumar, A. Chandrasekaran, A. Sabharwal, and P. Clark. How much coffee was consumed during

emnlp 2019? fermi problems: A new reasoning challenge for ai. In EMNLP, 2021.

D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi. Uniﬁedqa: Crossing format boundaries with a single qa system. In EMNLP, 2020a.

D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi. Uniﬁedqa: Crossing format boundaries with a single QA system. In EMNLP, 2020b.

N. Kim, E. Pavlick, B. K. Ayan, and D. Ramachandran. Which linguist invented the lightbulb? presupposition veriﬁcation for question-answering. ArXiv, abs/2101.00391, 2021.

T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.

G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Largescale reading comprehension dataset from examinations. In EMNLP, 2017.

H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In KR, 2011.

T. Li, D. Khashabi, T. Khot, A. Sabharwal, and V. Srikumar. Unqovering stereotyping biases via underspeciﬁed questions. In EMNLP, 2020.

G. Marcus and E. Davis.

Experiments test-

ing gpt-3’s ability at commonsense reason-

ing: Results. Technical report, NYU, 2020a.

(https://cs.nyu.edu/∼davise/papers/GPT3CompleteTests.html).

G. Marcus and E. Davis. Gpt-3, bloviator: Openai’s language generator has no idea what it’s talking about. MIT Technology Review, Aug 2020b.

A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pretraining. Technical report, OpenAI, 2018.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.

A. Roberts, C. Raffel, and N. M. Shazeer. How much knowledge can you pack into the parameters of a language model? In EMNLP, 2020.

N. Tandon, B. Dalvi, J. Grus, W. tau Yih, A. Bosselut, and P. Clark. Reasoning about actions and state changes by injecting commonsense knowledge. In EMNLP, 2018.

J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards AI-Complete question answering: A set of prerequisite toy tasks. In ICLR, 2016.

X. Zhou, M. Sap, S. Swayamdipta, N. A. Smith, and Y. Choi. Challenges in automated debiasing for toxic language detection. In EACL, 2021.

10

Appendix: Average Scores of Models on the Challenge300 Question Categories

Table 7 provides the histogram data from Figure 3 in tabular form, plus remaining question categories with fewer than 5 questions that were not included in the histogram (where average scores may be unreliable).

Qn Category commonsense comparison entity substitution entity tracking estimation example generation explanation false presupposition general knowledge generation history human behavior hypothetical math meta-reasoning riddle science spatial steps story understanding temporal Winograd ALL

# Qns 38 2 4 13 4 2 14 9 70 1 2 5 29 2 6 2 41 11 15 25 2 3 300

MACAW 0.50 1.00 1.00 0.50 0.88 1.00 0.68 0.11 0.93 1.00 1.00 0.70 0.78 0.00 1.00 1.00 0.76 0.73 0.87 0.88 0.25 0.67 0.75

GPT-3 0.53 0.50 0.63 0.62 1.00 1.00 0.43 0.00 0.79 1.00 1.00 0.60 0.59 0.50 0.67 0.50 0.65 0.77 0.73 0.72 0.00 1.00 0.65

Model T5-CBQA 0.42 0.50 1.00 0.65 0.75 0.50 0.68 0.00 0.73 0.00 1.00 0.90 0.71 0.00 0.67 0.00 0.43 0.45 0.31 0.77 0.25 0.00 0.57

Jurassic-1 0.47 1.00 1.00 0.54 0.50 0.00 0.64 0.00 0.80 1.00 1.00 0.60 0.52 0.00 0.33 0.50 0.74 0.82 0.77 0.72 0.25 1.00 0.65

Average of Averages 0.48 0.75 0.91 0.58 0.78 0.63 0.61 0.03 0.81 0.75 1.00 0.70 0.65 0.13 0.67 0.50 0.65 0.69 0.67 0.77 0.19 0.67 0.66

Table 7: Average score of models on different question categories in Challenge300. (See Figure 3 for histogram).

11

