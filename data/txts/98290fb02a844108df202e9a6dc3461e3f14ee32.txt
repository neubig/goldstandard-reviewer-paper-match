Nonconvex-Nonconcave Min-Max Optimization with a Small Maximization Domain
Dmitrii M. Ostrovskii∗ Babak Barazandeh† Meisam Razaviyayn‡

arXiv:2110.03950v1 [math.OC] 8 Oct 2021

Abstract

We study the problem of ﬁnding approximate ﬁrst-order stationary points in optimization

problems of the form minx∈X maxy∈Y f (x, y), where the sets X, Y are convex and Y is compact.

The objective function f is smooth, but assumed neither convex in x nor concave in y. Our

approach relies upon replacing the function f (x, ⋅) with its k-th order Taylor approximation

(in y) and ﬁnding a near-stationary point in the resulting surrogate problem. To guarantee its

success, we establish the following result: let the Euclidean diameter of Y be small in terms of

the

target

accuracy

ε,

namely

O

(ε

2 k+1

)

for

k

∈

N

and

O(ε)

for

k

=

0,

with

the

constant

factors

controlled by certain regularity parameters of f ; then any ε-stationary point in the surrogate

problem remains O(ε)-stationary for the initial problem. Moreover, we show that these upper

bounds are nearly optimal: the aforementioned reduction provably fails when the diameter of

Y is larger. For 0 ⩽ k ⩽ 2 the surrogate function can be eﬃciently maximized in y; our general approximation result then leads to eﬃcient algorithms for ﬁnding a near-stationary point in

nonconvex-nonconcave min-max problems, for which we also provide convergence guarantees.

1 Introduction

In the past few years, min-max optimization has become popular among practitioners due to its

relevance for machine learning applications—in particular, when training generative adversarial

networks (GANs) [1, 2, 3, 4], robust machine learning [5, 6, 7, 8, 9, 10], in fair statistical infer-

ence [11, 12, 13, 14, 15, 16], in reinforcement learning [17], distributed optimization and learning

over networks [18, 19, 20, 21, 22], and for optimal resource allocation in multi-agent systems [23, 24].

The common task arising in these applications, as well as in many others, is solving optimization

problems of the general form

min max f (x, y),

(P)

x∈X y∈Y

where X, Y are some convex sets in the corresponding high-dimensional Euclidean spaces and f is
smooth in both variables, possibly in a heterogeneous manner. Min-max optimization has been an
active area of research since the early works of Nash, von Neumann, and Morgenstern [25, 26, 27].
A very important subclass of such problems where f is convex-concave—i.e., f (⋅, y) is convex and f (x, ⋅) is concave for all (x, y) ∈ X ×Y —has been studied extensively from the algorithmic viewpoint starting from the work of Nemirovski [28], who proposed a ﬁrst-order algorithm with O(1 T )

∗Deparment of Mathematics, University of Southern California, Los Angeles, USA. Email: dostrovs@usc.edu †Splunk Inc. Email: bbarazandeh@splunk.com ‡Viterbi School of Engineering, University of Southern California, Los Angeles, USA. Email: razaviya@usc.edu

1

dimension-independent convergence rate, generalizing the extragradient algorithm from the early work of Korpelevich [29]. This has been followed by many extensions (e.g., [30, 31, 32, 33, 34]) and applications in machine learning, statistics, and signal processing; see [35, 36, 37] and references therein. Convex-concave min-max optimization remains an active area up to this day: see, e.g., the recent works [38, 39] establishing the matching upper and lower complexity bounds under strong convexity-concavity; the recent proliferation of works on last-iterate convergence [40, 41, 42, 43]; the work [44] extending the functionality of CVX [45] (a.k.a. “disciplined convex programming”) to convex-concave min-max problems and monotone variational inequalities [46].
Meanwhile, many modern applications where (P) appears fall beyond the convex-concave scenario. For example, in the standard formulation of GANs f is neither convex in x nor concave in y [1]; this is also the case in the task of adversarially-robust deep neural network training [47]; other applications of nonconvex-nonconcave min-max optimization can be found in the recent review article [48]. Problems in which f is concave y but nonconvex in x arise in fair inference “`a la R´enyi” [12] and when minimizing the maximum of smooth functions [49] (in the latter case f (x, ⋅) is even aﬃne). Moreover, machine learning under distributional uncertainty [10, 50, 9], power control for wireless communication [51], and special formulations of the task of learning from multiple domains [52] all lead to nonconvex-concave min-max optimization. In the past couple of years, these new applications reignited a substantial interest across the optimization theory community for analyzing (P) in the general nonconvex-nonconcave setup, the nonconvex-concave setup as an approachable stepping stone towards it, and in other “intermediate” scenarios, where f (⋅, y) is nonconvex, and f (x, ⋅) is not concave but has some other special structure allowing to eﬃciently solve the nested maximization problem—that is, evaluate at any point x ∈ X the primal function1

ϕ(x) ∶= max f (x, y).

(1)

y∈Y

The nonconvex-concave case, ﬁrst addressed in [53], is by now quite well understood. Generally, we lose all hope of actually solving (P)—or minimizing ϕ(⋅)—when f (⋅, y) is nonconvex, as we then deal with a nonconvex minimization problem even for a singleton Y . Hence, one has to be satisﬁed here with approximating a local minimizer of ϕ(⋅), or a local saddle point (also called a Nash equillibrium) in (P)—i.e., a point (xNE, yNE) ∈ X × Y such that
f (xNE, y) ⩽ f (xNE, yNE) ⩽ f (x, yNE) for all (x, y) ∈ X × Y in a neighborhood of (xNE, yNE).

In fact, even these two tasks can be too ambitious, and most of recent studies have been focused on the tasks of approximating a ﬁrst-order stationary point in (P) up to accuracy ε > 0, as measured by the norm of either the proximal gradient of f or the gradient of the standard Moreau envelope of ϕ(⋅). (The notion of the Moreau envelope, essential in the context of our work, shall be recalled in Section 2.) Painting with a broad brush, these two accuracy measures turn out to be (essentially) equivalent in the nonconvex-concave case, and settling on one of them is largely a matter of personal preference; see [54, Section 5] for a technical discussion of this equivalence. In this line of research, the works [54, 55, 56, 57] demonstrated that an ε-ﬁrst-order stationary point with respect to the Moreau envelope criterion, referred to as ε-FOSP from now on, can be found in Õ(ε−3) queries of the
1Note also that with f (⋅, y) nonconvex, the minimax theorem does not apply, and the order of min and max in (P) becomes important. As such, there is a crucial diﬀerence between nonconvex-concave and convex-nonconcave instances of (P): the former ones are easier, as in them the primal function ϕ(⋅), cf. (1) is merely hard to minimize, while in the convex-nonconcave case we typically cannot even evaluate it at a point.

2

gradient of f , and in Õ(ε−2) queries when f (x, ⋅) is strongly concave.2 The latter of these estimates was recently shown to be worst-case optimal [58, 59, 60], and it is widely believed—although, to the
best of our knowledge, not yet proved—that the former estimate is also tight. Furthermore, in the
absence of concavity, but assuming access to an abstract maximization oracle evaluating ϕ(x), the authors of [61] observed that an algorithm earlier proposed in [62] allows to compute an ε-FOSP in O(ε−4) queries of the maximization oracle and of the x-gradient.

Nonconvex-nonconcave problems. By contrast, our understanding of the general nonconvex-
nonconcave setup is rather fragmentary. One delicate issue here is that the two viewpoints on local optimality in (P)—the one focusing on ﬁnding a (local) minimizer x∗ of ϕ(x) = maxy∈Y f (x, y), historically due to Stackelberg [63], and the game-theoretic paradigm of Nash [25], where one aims
at ﬁnding a local Nash equilibrium—might result in quite diﬀerent notions of (local) optima and
stationary point when f (x, ⋅) is not concave [61]. Indeed: consider, as an illustration, the problem

min max f (x, y) ∶= xy − 31 y3 .

(2)

x∈R y∈[−2,2]

By computing ϕ(x) one can verify that x∗ = 1 is a unique local (hence also global) minimizer of ϕ(x), and maxy∈[−2,2] f (1, y) is attained at y∗ = −2; in other words, (1, −2) is a unique solution to (2). On the other hand, a unique ﬁrst-order Nash equilibrium, that is (xFNE, yFNE) such that
⟨∇xf (xFNE, yFNE), x − xFNE⟩ ⩾ 0 and ⟨∇yf (xFNE, yFNE), y − yFNE⟩ ⩾ 0 for all (x, y) ∈ X × Y, (3)

is the point (0, 0). Finally, (2) does not have any (local) Nash equilibrium, and these conclusions remain valid if x restricted to a compact set (e.g., X = [0, 4]). Meanwhile, infx∈X ϕ(x) is always ﬁnite in (P), and is attained whenever ϕ(x) is bounded from below on X (in particular, whenever X is compact), and must satisfy the necessary condition ∂ϕ(x∗) ∋ 0 in terms of the weak subdiﬀerential. (To make the paper self-contained, we provide background on weak convexity and
weak subdiﬀerentials in Appendix D; for the present non-technical discussion this is irrelevant.) To-
gether with the above example, this observation makes a case for the Stackerberg approach as the
one better suited for the general nonconvex-nonconcave scenario, and we adhere to this approach
in our work. Another argument in favor of the Stackelberg approach is that in most of the modern
applications of nonconvex-nonconcave min-max optimization we mentioned, the actual practical
goal is to minimize the primal function, not to ﬁnd a local Nash equilibrium.

The second challenge posed by nonconvex-nonconcave min-max optimization is an exceptional
algorithmic diﬃculty of ﬁnding (approximately) even a local ﬁrst-order stationary point or Nash
equilibrium when no additional structure is imposed on (P). In particular, in [64] it is shown that if X, Y = [0, 1]d are hypercubes and f is L-smooth, G-Lipschitz, and takes values in [−B, B], then:

(i) The problem of exhibiting (x, y) ∈ [0, 1]2d such that

∇f (x, y)

√⩽

1 24

or

detecting

that

no

such

point exists,3 is FNP-complete (in d) in the regime L = d, G = d, B = 1 [64, Theorem 4.1].4

(ii) At least one (ǫ, r)-approximate local Nash equilibrium, deﬁned as (x∗, y∗) ∈ [0, 1]2d such that f (x∗, y) − ǫ < f (x∗, y∗) < f (x, y∗) + ǫ ∀(x, y) ∈ [0, 1]2d ∶ max{ x − x∗ , y − y∗ } ⩽ r,

2We use the standard “big-O” notation: g = O(h) or g ≲ h both hide an absolute constant, i.e., means that g ⩽ ch for some c > 0 uniformly over all allowed pairs of values of g, h; similarly, g = Õ(h) is a shorthand for g = O(h log(h + e)).
3In what follows, ⋅ stands for the standard Euclidean norm on a given Euclidean space (deﬁned by the context). 4See [65, 66] for a technical background on the complexity classes FNP, PPAD—in particular, in the context of (P).

3

is guaranteed to exist in the local regime r < 2L ǫ; however exhibiting it is PPAD-hard already when r ⩾ L ǫ and max{L, G, B, 1 ǫ, 1 r} = O(poly(d)) [64, Theorems 4.3 and 4.4].
These two negative results demonstrate to us that approximating even a stationary point, or a local min-max point for large enough neighborhood, is virtually impossible without imposing additional structure on (P). As such, existing positive results rely on adding this structure in one way or another. In particular, a common methodology is to “mimic” the case of a VI with a monotone operator [28] by imposing pseudomonotonicity, the Minty condition, or contraction of the best response mappings [67, 68, 69, 70, 71, 72, 73, 74, 75, 76]. However, these assumptions are restrictive and rarely satisﬁed in modern applications; see [48] for a detailed discussion. In addition, they are tied to the Nashian paradigm, wheras applications tend to call for the Stackelbergian one. Other interesting approaches rely upon restricting the nature and level of coupling between the variables [77] or the power of the max-player [78]. Yet another recent line of research advocated an alternative to the local Nash equilibria—“greedy adversarial equilibria” that are computationally feasibile, but at the expense of a certain loss of transparency and interpretability [79, 80]. Finally, a growing body of literature is devoted to the asymptotic behavior of algorithms [81, 82, 83, 84, 85], and structural results for GANs and adversarial training [86, 87, 88, 89].
Going back to the hardness results (i)–(ii) established in [64], the latter of them hints at a possibility to control the complexity of (P) through the size of a feasible set. Our work explores this possibility; let us now present the main ideas behind our approach.
Outline of our approach. Our work is motivated by a trivial observation: ﬁnding an approximate ﬁrst-order stationary point of (P) becomes a computationally feasible task when Y is a singleton, as in this case we deal with a smooth minimization problem, so an approximate stationary point can be found by running projected gradient descent. Furthermore, one can hope that the computational tractability is preserved when Y is not singleton but is small. Then, letting fˆk(x, ⋅) be the kth-order Taylor approximation of f (x, ⋅) for some k ⩾ 0, one can advocate the following two-step approach.

1o. Prove that any ε-FOSP in the surrogate min-max problem is also an O(ε)-FOSP in (P), due to fˆk(x, ⋅) approximating f (x, ⋅) over the (small) set Y well enough for our purposes.
2o. Find ε-FOSP in the surrogate problem—and thus O(ε)-FOSP in (P)—by exploiting the structure of fˆk(x, ⋅) such as, e.g., linearity for k = 1 or quadratic structure for k = 2.

A natural question immediately arising in connection with this strategy, namely with 1o, is:

How small the diameter of Y has to be in order for the reduction in step 1o to be valid?

One would expect this question to have a nontrivial answer: indeed, it seems unlikely that a Taylor approximation would work over a set of a constant diameter.5 As such, one may expect that

diam(Y ) ≲ εp(k)

(4)

5Note, however, that this would not immediately contradict the results of [64], as the ℓ2-diameter of [0, 1]d is √d.

4

allows for the reduction in 1o to work, with p(k) > 0 deﬁned by the order k of Taylor approximation, and the hidden constant depending on the regularity properties of f . Later on we shall verify this hypothesis, proving explicit bounds of the form (4) for arbitrary k, under the natural regularity assumptions on f , and showing that they are not only suﬃcient, but also necessary—that is, the reduction in 1o may fail for diam(Y ) beyond the allowed threshold. Finally, we shall also implement step 2o of the strategy by designing eﬃcient algorithms for solving the surrogate min-max problem.

Applications of nonconvex-nonconcave min-max optimization with a small maximiza-
tion domain. In some applications in machine learning and signal processing, it is quite natural to assume that diam(Y ) is relatively small. Such is the case, for example, in the task of training neural networks robust against adversarial attacks, where diam(Y ) corresponds to the magnitude of the attack performed, and typically has to be small in order for the attack to remain undetected; meanwhie, the target accuracy often does not have to be optimized very accurately [5, 7]. More recently, the work [90] demonstrated that adding a small adversarial perturbation to the trained statistical model allows to avoid sharp local minima of the training loss; this approach, termed sharpness-aware training, results in optimization problems of the form minw max u ⩽r L(w + u), where L(⋅) is the training loss functional, u is the perturbation of a model w, and r should be small as the analysis relies upon linearizing L(⋅) near w. More generally, robust design of any system against small perturbations of certain parameters may lead to min-max problems of form (P) with a small maximization domain Y .

Paper organization and summary of contributions. In a nutshell, our work is concerned
with implementing both steps 1o–2o of the proposed approach; combined together, they result in
eﬃcient algorithms for ﬁnding an ε-FOSP in (P). The rest of the paper is organized as follows.
In Section 2 we set oﬀ by stating our assumptions about f . In a nutshell, we grant Lipchitzness of the x-gradient ∇xf and of the order-k diﬀerential tensor ∇ky⋯yf , for given k ∈ N ∪ {0}, in accordance with our intention of using the k-order Taylor expansion of f (x, ⋅). We then recall some
mathematical background on (P), including the deﬁnitions of the Moreau envelope and approximate
ﬁrst-order stationary points, and ﬁnally deﬁne the reduction in 1o in a rigorous way.
In Section 3 we formulate, discuss, and prove our ﬁrst result: the general bound of the form (4),
holding for arbitrary k ∈ N ∪ {0} under the matching regularity assumption (i.e., with the same k). Informally, we show (cf. Theorem 3.1) that, for any k ∈ N ∪ {0}, an arbitrary ε-FOSP in the counterpart of (P) with fˆk instead of f as objective function, is also an ε-FOSP in (P) whenever

⎧⎪⎪ ε2(k + 1)!

1 k+1

ε ⎫⎪⎪

diam(Y ) ≲ max ⎨⎪⎪⎩ λρk

, µ ⎬⎪⎪⎭ ,

(5)

where λ, µ, and ρk are the uniform over (x, y) ∈ X × Y Lipschitz constants of ∇xf (⋅, y), ∇xf (x, ⋅),

and

∇ky⋯yf (x, ⋅)

respectively.

This

gives

(4)

with

p(k) =

2 k+1

for

ε

below

a

certain

(constant)

level.

In Section 4 we present our second series of results, showing that the bound (5) is nearly tight

(namely, up to the (k + 1)-fold inﬂation of the term ε µ). We do this by constructing speciﬁc in-

stances of (P), carefully choosing the center yˆ ∈ Y of the Taylor approximation, and then exhibiting a point x ∈ X which is stationary in the approximated problem, while not being an ε-FOSP in (P).
In Section 5 we carry out step 2o of the strategy. To this end, we suggest three algorithms based

on the Taylor approximation with k ∈ {0, 1, 2}. For k = 0 and k = 1 they are based on gradient

5

descent and descent-ascent; the resulting oracle complexity is O(ε−2), and diam(Y ) ≲ ε is allowed, cf. Theorems 5.1 and 5.2. For k = 2, our algorithm allows for a larger diameter O(ε2 3) as per (5). Yet, this improvement has a price: Y has to be a Euclidean ball, we require access to higher-order derivatives of f , and the oracle complexity grows to O(ε− 133 ); see Theorem 5.3 for the precise claim.
Notation. We use the standard O(⋅) and Õ(⋅) notation: given two functions g, h > 0 we use g = O(h) and g ≲ h as shorthands for saying that g ⩽ ch for some c > 0 uniformly over all simultaneously allowed pairs of values of g, h; similarly, g = Õ(h) is a shorthand for g = O(h log(h + e)). The symbol c denotes a numerical constant whose value is unimportant and might change from line to line. We use abridged notation ∇xf (x, y), ∇yf (x, y) for the partial gradients of f in the ﬁrst and second argument evaluated at some (x, y) ∈ X × Y , and similarly for higher-order tensors of partial derivatives (see Section 2). ⋅ stands for the standard Euclidean norm on a given Euclidean space (deﬁned by the context) when the argument is a vector, and for the induced operator norm when the argument is a k-tensor (such tensors shall arize from the Taylor expansion of f (x, ⋅)). Other notation shall be introduced when necessary.
Most of our results are rather technical, and we defer the proofs to the appendix. An exception is made for Theorem 3.1, as its proof is not so technical and illuminates the mechanism behind (5).

2 Standing assumptions, deﬁnitions, and technical background

We shall focus on (P) assuming that X, Y are two convex sets with non-empty interior in the corresponding Euclidean spaces Ex, Ey; Y is compact and its Euclidean diameter is bounded by D:

D ⩾ diam(Y ) ∶= max y′ − y .
y,y′ ∈ Y

Let ⋅ be the (Euclidean) operator norms on the spaces of multilinear forms on Ex⊗i×Ey⊗j for 0 ⩽ i ⩽ 2 and 0 ⩽ j ⩽ k, where k is a non-negative integer. We grant two assumptions on the regularity of f .

Assumption 1 (First-order smoothness in x). The partial gradient ∇xf (x, y) exists and is Lipschitzcontinuous on X × Y . More precisely, for some λ > 0, µ ⩾ 0, any x, x′ ∈ X and y, y ∈ Y, it holds that

∇xf (x′, y′) − ∇xf (x, y) ⩽ λ x′ − x + µ y′ − y .

(A1)

Assumption 2 (k-order smoothness in y). For given integer k ⩾ 0, the tensor ∇kyk f (x, y) of k-order partial derivatives of f in y exists and is Lipschitz-continuous on X × Y : for some ρk, σk, τk ⩾ 0, any x, x′ ∈ X and y, y ∈ Y, it holds that

∇kyk f (x′, y′) − ∇kyk f (x, y) ⩽ ρk y′ − y + σk x′ − x .

(A2)

Moreover, the tensor ∇kxy+k1f incorporating the partial derivatives of order k in y and 1 in x (in this order) exists and is Lipschitz continuous in x: for some τk ⩾ 0, any x′, x ∈ X and y ∈ Y , one has

∇kxy+k1f (x′, y) − ∇kxy+k1f (x, y) ⩽ τk x′ − x .

(A3)

Assumption 2 merits some discussion concerning the cases k ∈ {0, 1, 2} that are most interesting from the algorithmic point of view (cf. Section 5).

6

• When k = 0, (A3) follows from (A1) with τ0 = λ, and so does not give any extra restrictions. As for (A2), it then requires that f is ρ0-Lipschitz in y and σ0-Lipschitz in x. In fact, such
Lipschitzness conditions are not necessary for our approximation results to hold, although they
can possibly improve the approximation bounds in Theorem 3.1 in the case k = 0, which is of marginal practical interest. Meanwhile, the Lipschitzness in x is used in one of the algorithms
proposed in Section 5. Even so, we have the variational bounds σ0 ⩽ min{λ diam(X), µD} provided that X contains in its interior a ﬁrst-order stationary point of f (⋅, y) for any y ∈ Y .

• When k = 1, condition (A2) reduces to the Lipschitzness of the partial gradient ∇yf , namely

∇yf (x′, y′) − ∇yf (x, y) ⩽ ρ1 y′ − y + σ1 x′ − x .

(6)

Moreover, due to (A3) we can assume, without loss of generality, that σ1 = µ. Meanwhile, (A3) is a second order condition: it implies that ∇2x2f is diﬀerentiable in y almost everywhere on X, and allows to preserve weak convexity in x after the Taylor expansion in y. Note that (A3) holds with τ1 = 0 (in fact, with τk = 0 for k ⩾ 1) for bilinearly-coupled (BC) objectives:

f (x, y) = p(x) + ⟨Ax, y⟩ + q(y).

(BC)

• When k = 2, condition (A2) reduces to the Lipschitzness of the partial Hessian ∇2y2f , namely

∇2y2f (x′, y′) − ∇2y2 f (x, y) ⩽ ρ2 y′ − y + σ2 x′ − x .

(7)

Clearly, under (BC) for k ⩾ 2 one has σk = 0, and also ρk = 0 if, in addition, q(y) is quadratic. Finally, we grant a mild regularity assumption allowing to diﬀerentiate ∇kxy+k1f in x under the integral (we use Lebesgue measures on Ex and Ey); it is trivially satisﬁed if ∇kx2+y2k f exists everywhere.
Assumption 3. Deﬁne the set-valued map x ↦ Yx′ ⊆ Y where Yx′ is the set of y for which ∇kx2+y2k f (x, y) does not exist. Its graph, i.e., the subset of X × Y deﬁned as {(x, y) ∶ x ∈ X, y ∈ Yx′}, is measurable.

Moreau envelope and the notion of FOSP. In this work we use a stationarity criterion

based on the Moreau envelope of the primal function. To deﬁne it formally, we ﬁrst have to

remind some standard deﬁnitions; readers familiar with the notion of Moreau envelope for weakly

convex functions can safely skip this part, while those looking for more details may refer to [62, 91]

or [54, Sec. 5]. Recall that ϕ(x) ∶= maxy∈Y f (x, y) is called the primal function for (P), cf. (1). Typically ϕ is non-smooth (unless f (x, ⋅) is strictly concave), and so its gradient is not deﬁned

on X. However, under Assumption 1 ϕ is λ-weakly convex—that is, for any x ∈ X the function

given

by

ϕ(⋅) +

λ 2

⋅

2

is

convex,

and

(¯λ − λ)-strongly

convex

if

regularization

is

with

¯λ 2

⋅

2 for λ¯ > λ.

(In order to streamline the presentation while keeping the paper self-contained, we provide necessary

background on weakly convex functions and weak subdiﬀerentials in Appendix D.) This allows to

deﬁne

the

Moreau

envelope

of

ϕ,

as

a

function

of

x

∈

X,

by

minimizing

ϕ(⋅)

+

λ¯ 2

⋅ −x 2 for given x.

Deﬁnition 1 (Moreau envelope). Let φ ∶ X → R be λ-weakly convex. Given an ¯λ > λ, the function

φ¯λ(x) ∶= min

φ(u)

+

¯λ 2

u−x 2

(8)

u∈X

is called the ¯λ-Moreau envelope of φ, and the unique solution x+φ ¯λ(x) to (8) is called the proximal mapping of x (corresponding to φ with stepsize 1 ¯λ).

7

It is well known (see, e.g., [91, Lemma 2.2]) that ϕ¯λ is C1-smooth when λ¯ > λ, and moreover it has O(λ)-Lipschitz gradient whenever ¯λ = (1 + c)λ for c > 0. The standard practice (see, e.g., [62]) is to simply choose ¯λ = 2λ and use the gradient norm ∇ϕ2λ(⋅) as the accuracy measure. Indeed, from the ﬁrst-order optimality conditions for (8) one can easily conclude (cf., e.g., [91, p. 4]) that

∇ϕ2λ(x) = 2λ(x − x+) ∈ ∂(ϕ + IX )(x+),

(9)

where x+ = x+ϕ (2λ)(x) for arbitrary x ∈ X, IX is the indicator function of X, and ∂(⋅) is the Fr´echet (or weak) subdiﬀerential (see Appendix D.1 for details). As a result, in the x-unconstrained case (X = Ex) the inequality ∇ϕ2λ(x) ⩽ ε for some x ∈ X implies the existence of a point x+ = x+(x) within O(ε) distance from x, such that ϕ has a subgradient at x+ with the norm at most ε. A similar
result holds in the constrained case if the subgradient norm of ϕ is replaced with an appropriate
inaccuracy measure based on projection onto X; see [54, Prop. 5.1] also given as Proposition D.1
in our paper. These results motivate us to introduce the following deﬁnition.

Deﬁnition 2 (Approximate ﬁrst-order stationary points). The point x ∈ X is called (ε, λ′)-ﬁrstorder stationary ((ε, λ′)-FOSP) for (P) if ∇ϕλ′ (x) ⩽ ε.

Taylor expansion and the surrogate problem. Let us formally deﬁne the Taylor expansion of f (x, ⋅) to be used from now on. Recall that our approach relies on replacing f (x, ⋅) with its k-order Taylor approximation fˆk(x, ⋅) for a non-negative integer k and a ﬁxed “center” point yˆ ∈ Y :

fˆk(x, y) = j=k0 j1! ∇jyj f (x, yˆ) [(y − yˆ)j ].

(TE)

Here T [yj] is the diagonal evaluation of tensor T ∶ Ey⊗j → R on y ∈ Ey—that is, T [yj] ∶= T [y, ..., y]. In particular, in the most practically important cases k ∈ {0, 1, 2}, (TE) reduces to fˆ0(x, y) = f (x, yˆ),
fˆ1(x, y) = f (x, yˆ) + ⟨∇yf (x, yˆ), y − yˆ⟩ , and fˆ2(x, y) = fˆ1(x, y) + 12 ⟨y − yˆ, ∇2y2f (x, yˆ)(y − yˆ)⟩. (10)

Note that for k ⩽ 2 we can eﬃciently maximize fˆk(x, ⋅): indeed, fˆ0(x, ⋅) = f (x, yˆ) is constant, fˆ1(x, ⋅) is aﬃne, and fˆk(x, ⋅) for k = 2 is a (nonconcave) quadratic that can be globally maximized using the gradient oracle ∇yfˆ2(x, ⋅) when Y is a ball [92]. From now on, we focus on the surrogate problem

min max fˆk(x, y)
x∈X y∈Y

(Pk )

and let ϕˆ(x) ∶= maxy∈Y fˆk(x, y) be the corresponding primal function, with k omitted for brevity.

3 Upper bounds on the admissible diameter
We are now in the position to rigorously formulate the question we ﬁrst posed in the introduction:

For k ⩾ 0, what D allows to guarantee the existence of ¯λ ≲ λ and c > 0 such that any (cε, 2¯λ)-FOSP for (Pk), regardless of the choice of yˆ, is (ε, 2¯λ)-FOSP for (P)?

8

In this section, our goal is to answer this question, and such an answer will be given in Theo-
rem 3.1. But before, let us clarify a subtle point about it: namely, note that we should expect c < 1 and ¯λ > λ. Indeed, replacing f with its Taylor approximation is likely to cause some deterioration of accuracy as measured by the gradient norm of the Moreau envelope, so we cannot expect, say, (2ε, 2¯λ)-FOSP for (Pk) to also be an (ε, 2¯λ)-FOSP for (P). Similarly, considering ¯λ = λ would imply that the surrogate primal function ϕˆ(⋅) is λ-weakly convex (cf. Deﬁnition 1), but in fact such
a guarantee is not available. Fortunately, it is not hard to prove (cf. Lemma 3.3 below) that under
the regularity assumptions granted in Section 2 and a weaker bound on D than the one imposed in Theorem 3.1, the function ϕˆ(⋅) is ¯λ-weakly convex with λ¯ = (1 + o(ε))λ.
To prepare the ground for proving Theorem 3.1, we ﬁrst obtain the bounds for approximating f with fˆk uniformly over (x, y) ∈ X × Y , in terms of the function value, the x-gradient, and x-Hessian.

Lemma 3.1. Grant (A2) with some k ⩾ 0 and possibly σk = ∞. Then for any x ∈ X, y ∈ Y one has f (x, y) − fˆk(x, y) ⩽ ρkDk+1 . (k + 1)!

Lemma 3.2. Let k ⩾ 0 be given. Grant (A1) if k = 0 (possibly with λ = ∞) and grant (A2) (possibly with ρk = ∞, but with ∇kyk f (⋅, y) absolutely continuous ∀y ∈ Y ). Then for any x ∈ X, y ∈ Y one has

∇xf (x, y) − ∇xfˆk(x, y)

⎧⎪⎪⎪min{µD, 2σ0} for k = 0, ⩽ ⎨⎪⎪⎪⎩ 2σkk!Dk for k ⩾ 1.

Lemma 3.3. Given k ⩾ 0, grant (A1) (possibly with µ = ∞). If k ⩾ 1 grant (A3), Assumption 3, and continuity of ∇kyk f (x, ⋅) for all x ∈ X. Then ∇xfˆk(⋅, y) is λ¯k-Lipschitz in x for any y ∈ Y with

λ¯k ∶= λ + 2τkkD! k 1{k ⩾ 1}. (11)

Note that an immediate corollary of (11) is that ϕˆ is ¯λk-weakly convex. Lemmas 3.1 to 3.3 are proved by integrating the remainder term of the Taylor expansion; however, in the case of Lemma 3.3
some technicalities arise; while they could be easily resolved by imposing an extra order of regularity (namely by requiring that ∇kx2+y2k f exists everywhere), we manage to avoid this condition via a careful application of Assumption 3; see Appendix A.

Next we present the ﬁrst of our main results: a general upper bound on the admissible diameter of Y —i.e., the one allowing to replace (P) with (Pk) when searching for FOSPs.

Theorem 3.1. Given k ⩾ 0, grant Assumptions 1 to 3, and let x∗ be ( 16 ε, 2¯λk)-FOSP for (Pk) with ¯λk deﬁned in (11). Then x∗ is (ε, 2¯λk)-FOSP for (P) as long as

⎧⎪⎪

λρ0D ⎫⎪⎪ ε

min ⎨⎪⎪⎩µD, 2σ0, 50 ⎬⎪⎪⎭ ⩽ 24 when k = 0,

(12)

⎧⎪⎪⎪

2σk Dk

λ¯kρkDk+1 ⎫⎪⎪⎪ ε

min ⎨⎪⎪⎪⎩µD + k! , 50 ⋅ (k + 1)! ⎬⎪⎪⎪⎭ ⩽ 24 when k ⩾ 1. (13)

We shall present the proof of Theorem 3.1 in Section 3.2 and investigate its tightness in Section 4. But before doing all this, let us discuss the implications of this result.

9

3.1 Discussion of Theorem 3.1

Zeroth-order approximation. Condition (12) can be satisﬁed even without Assumption 2, i.e.,
when f is not Lipschitz (recall that Assumption 1 already implies (A3) with τ0 = λ). Indeed, in this case (12) still gives a non-trivial condition 4µD ⩽ ε. In the “x-FOSP eﬀectively unconstrained” scenario where f (⋅, y) has a stationary point in the interior of X for any y ∈ Y , we have the bounds

σ0 ⩽ min{µD, λdiam(X)}.

(14)

Typically, there is no reason to believe the set X to be small or even bounded (in contrast to Y ),
so the second bound above is usually non-restrictive. However, the ﬁrst bound is not vacuous; in
particular, in the scenario in question it shows that (12) may indeed beneﬁt from σ0 replacing µD. Another interesting scenario is the “y-FOSP eﬀectively unconstrained” one, where f (x, ⋅) has a
stationary point in the interrior of Y for any x ∈ X. Assuming that ∇yf is ρ1-Lipschitz (cf. (A2)) we then have a variational bound ρ0 ⩽ 2ρ1D, so (12) gets weaker than (13) with k = 1, which reads

min µ, (λ + τ1D)ρ1 D ≲ ε.

(15)

Note, that this scenario is not too “pathological”: the interior stationary point does not have to be
a maximizer of f (x, ⋅). In particular, nonconcave polynomials, while maximized on the boundary of a compact domain, tend to have interior saddle points (e.g., f = y12 − y22 on Y = {y ∈ R2 ∶ y ⩽ 1}).

Leading-order terms and bilinearly-coupled objectives. In the case k ⩾ 1, condition (13) improves over the baseline µD ≲ ε via the square-root term arising due to Assumption 2. In fact,
omitting higher-order in D additive terms we reduce (13) to the independent of σk and τk condition

min µD, λρkDk+1 ≲k ε,

(16)

where we hide a k-dependent constant factor in ≲k for brevity. When solved for D this amounts to

⎧⎪⎪ ε

ε2

k

1 +1

⎫⎪⎪

D ≲k max ⎨⎪⎪⎩ µ , λρk ⎬⎪⎪⎭ .

(17)

This approximation of (13) becomes exact for BC objectives (cf. (BC)), since in this case for k ⩾ 1 one has τk = 0 and σk = µ1{k = 1}, so higher-order in D additive terms in (13) simply vanish. In Section 4 we demonstrate that the approximation in (16)–(17) is also tight for BC problems,
by exhibiting the worst-case examples on which the bound (16) is attained. Moreover, approxi-
mation (16)–(17) applies beyond the class (BC), in the (typical) scenario where the smoothness
parameters in Assumptions 1 and 2 are ﬁxed, as long as ε is below a certain threshold. Indeed, in
this case D must shrink with ε in order to guarantee (13), and for ε small enough the higher-order
(in D) additive terms in (13) are dominated by the lower-order ones. More precisely, in Appendix A
we show that it suﬃces to have

⎧⎪⎪⎪⎪⎪ λ3ρ1 1 2 ⎪⎪⎪⎪ τ12

when k = 1,

ε ≲k ⎨⎪⎪⎪⎪

⎧⎪⎪⎪ µk

1 k−1

λ2k+1ρkk 21k ⎫⎪⎪⎪

(18)

⎪⎪⎪⎪⎪⎩ min ⎨⎪⎪⎪⎩ σk

, τ k+1
k

⎬⎪⎪⎪⎭ when k > 1.

10

Coupling-independent behavior for small ε. Observe that condition (17) reads D = O(ε) in 2
the case k = 1, and simpliﬁes to D = O(ε k+1 ) when k > 1 and for small enough ε: namely, whenever

ε ≲k

µk+1 λρk

1 k−1
.

(19)

Thus, increasing the approximation order above k = 1 allows to gain in terms of the range of D for which Theorem 3.1 applies and (P) can be replaced with (Pk) when searching for FOSPs. Moreover,
in this “high-accuracy” regime the critical D becomes coupling-independent for k > 1, deﬁned solely by the “homogeneous” parameters λ, ρk and the target ε; meanwhile, µ deﬁnes the moment of
transition to this regime from the initial D = O(ε) one, as ε is driven below the threshold in (19). For k = 1 there is no such “elbow eﬀect.” Here the critical diamet√er is D = O(ε) over the whole
range of ε, with µ appearing in t√he hidden constant factor 1 min{µ, λρ1}. Thu√s, here we beneﬁt from low interaction levels (µ ≲ λρ1) while not suﬀering from higher ones (µ ≳ λρ1), regardless
of ε > 0. Of course, beyond the BC class interaction does manifest in higher order, via τ1 in (18). For k = 0 the “elbow” is “in reverse”: we start with D = O(ε1 2) for large ε, and switch
to D = O(ε) critical diameter as ε passes the threshold λρ0 µ which corresponds to (19) with k = 0. In the scenario where Assumption 2 holds simultaneously for k ∈ {0, 1}, and we can choose between approximations w√ith these orders, the gain for k = 1 is marginal, only in the constant factor: namely, 1 min{µ, λρ1} instead of 1 µ—and this eﬀect only manifests on high interaction levels: µ2 ≳ λρ1. In fact, even this marginal comparative disadvantage of zeroth-order approximation
disappears in the “y unconstrained as per FOSP” scenario, where f (x, ⋅) has a stationary point
inside Y for any x ∈ X: indeed, in this case (12) automatically follows from (15) due to the variational bound ρ0 ⩽ 2ρ1D.

3.2 Proof of Theorem 3.1
The result follows by combining Propositions 3.1 and 3.2 which we formulate and prove next. These propositions correspond to the two choices for the minimum in (13), and we prove each of them under minimal assumptions; the full Assumptions 1 and 2 are required to have both results simultaneously.
Proposition 3.1. For k ⩾ 0 and λ¯k given by (11), under the premise of Lemmas 3.1–3.3 one has

∇ϕˆ2¯λk (x) − ∇ϕ2¯λk (x) ⩽

8¯λkρkDk+1 , (k + 1)!

∀x ∈ X.

Proof. Clearly, ϕ(⋅) is λ-weakly convex (cf. Assumption 1), hence also λ¯k-weakly convex. Moreover, by Lemma 3.3 the function ϕˆ(⋅) = maxy∈Y fˆk(⋅, y) is also ¯λk-weakly convex. Whence by (9) we have
∇ϕ2¯λk (x) = 2¯λk(x − x+), ∇ϕˆ2¯λk (x) = 2¯λk(x − xˆ+), with x+, xˆ+ being the associated proximal-point operators:

x+ = argmin{ϕ(x′) + ¯λk x′ − x 2}, xˆ+ = argmin{ϕˆ(x′) + λ¯k x′ − x 2}.

x′∈X

x′∈X

11

As a result, ∇ϕ2¯λk (x) − ∇ϕˆ2¯λk (x) = 2¯λk xˆ+ − x+ , and we can focus on bounding xˆ+ − x+ . To this end, since the function ϕ(⋅) + ¯λk ⋅ −x 2 is λ¯k-strongly convex and minimized at x+, we have

that

21 ¯λk xˆ+ − x+ 2 ⩽ ϕ(xˆ+) + ¯λk xˆ+ − x 2 − ϕ(x+) − ¯λk x+ − x 2.

(20)

On the other hand, since the function ϕˆ(⋅) + λ¯ ⋅ −x is ¯λ-strongly convex and minimized at xˆ+,

21 ¯λk xˆ+ − x+ 2 ⩽ ϕˆ(x+) + ¯λk x+ − x 2 − ϕˆ(xˆ+) − ¯λk xˆ+ − x 2.

Summing the two inequalities we get

¯λk xˆ+ − x+ 2 ⩽ ϕˆ(x+) − ϕ(x+) + ϕ(xˆ+) − ϕˆ(xˆ+) ⩽ 2 sup ϕˆ(x) − ϕ(x) .
x∈X

Finally, let x ∈ X be arbitrary and yˆ∗ = yˆ∗(x) be such that ϕˆ(x) = fˆ(x, yˆ∗). Then Lemma 3.1 gives ϕˆ(x) − ϕ(x) ⩽ fˆk(x, yˆ∗) − f (x, yˆ∗) ⩽ (ρkkD+k1+)1! , ∀x ∈ X.

The same estimate holds for ϕ(x) − ϕˆ(x) which is bounded from above by f (x, y∗) − fˆk(x, y∗) with y∗ = y∗(x) such that ϕ(x) = f (x, y∗)), and thus for supx∈X ϕˆ(x)−ϕ(x) . The result follows.

Proposition 3.2. Let k ⩾ 0, ¯λk be as in (11). Grant Assumption 1 and the assumptions of Lemmas 3.2 and 3.3. Then for any x∗ ∈ X such that ∇ϕˆ2¯λk (x∗) ⩽ ε, one has

∇ϕ2¯λk (x∗) − ∇ϕˆ2¯λk (x∗)

⎧⎪⎪⎪⎪4 µD + 2σkDk + ε ⩽ ⎨⎪⎪⎪⎪⎩4 (min{µD,k2!σ0} + ε)

when k ⩾ 1, when k = 0.

Proof. First observe that ϕ(⋅) and ϕˆ(⋅) are λ¯k-weakly convex by Lemma 3.3. Hence, ϕ(⋅)+λ¯k ⋅−x∗ 2 and ϕˆ(⋅) + ¯λk ⋅ −x∗ 2 are ¯λ-strongly convex, and their corresponding minimizers x+, xˆ+ satisfy (cf. (20))

12 λ¯k xˆ+ − x+ 2 ⩽ ¯λk xˆ+ − x∗ 2 + ϕ(xˆ+) − ϕ(x+) − ¯λ x+ − x∗ 2

⩽ 4¯λk

xˆ+ − x∗

2

+

ϕ(xˆ+)

−

ϕ(x+)

−

3 4

¯λk

xˆ+ − x+

2.

(21)

Here in the ﬁnal step we used the inequality

xˆ+ − x+

2

⩽

4 3

x+ − x∗

2+4

xˆ+ − x∗

2

which can be easily deduced from the triangle inequality. Furthermore, by Proposition D.1 we have

∇ϕ2¯λk (x∗) = 2¯λk(x∗ − x+), ∇ϕˆ2¯λk (x∗) = 2¯λk(x∗ − xˆ+).

Hence ∇ϕˆ2¯λk (x∗) − ∇ϕ2¯λk (x∗) = 2¯λk xˆ+ − x+ so we can focus on bounding xˆ+ − x+ using (21). Now observe that x∗, being an (ε, 2¯λk)-FOSP for (Pk) with a ¯λk-weakly convex primal function ϕˆ(⋅), satisﬁes the premise of Proposition D.1, so that

2¯λk xˆ+ − x∗ ⩽ ε, min SX (xˆ+, ξ, 2¯λk) ⩽ ε,

(22)

ξ∈∂ϕˆ(xˆ+)

12

cf. (90), with functional SX (x, ξ, λ′) deﬁned by

S2X (x, ξ, λ′) ∶= 2λ′ max − ⟨ξ, u − x⟩ − λ2′ u − x 2
u∈X

for arbitrary x ∈ X, ξ ∈ Ex, and λ′ > 0. By (21) and the ﬁrst bound in (22) we get

21 (¯λk xˆ+ − x+ )2 ⩽ ε2 + ¯λk ϕ(xˆ+) − ϕ(x+) − 34 λ¯k x+ − xˆ+ 2 .

Meanwhile, convexity of ϕ(⋅) + 12 ¯λk ⋅ −xˆ+ 2 implies that ϕ(xˆ+) − ϕ(x+) − ¯λ2k xˆ+ − x+ 2 ⩽ ⟨ξ+, xˆ+ − x+⟩ for any ξ+ ∈ ∂ϕ(xˆ+). Using this fact and letting ξˆX+ ∈ Argminξ∈∂ϕˆ(xˆ+) SX (xˆ+, ξ, 2¯λk), cf. (22), we get

(¯λk xˆ+ − x+ )2 ⩽ 2ε2 + 2¯λk − ⟨ξ+, x+ − xˆ+⟩ − 41 ¯λk xˆ+ − x+ 2

⩽ 2ε2 + 2¯λk − ⟨ξˆX+ , x+ − xˆ+⟩ − 14 λ¯k xˆ+ − x+ 2 + xˆ+ − x+ ⋅ ξˆX+ − ξ+

⩽

2ε2

+

2S2X

(xˆ+,

ξˆX+

,

1 2

λ¯k

)

+

2¯λk

xˆ+ − x+

⋅

ξˆX+ − ξ+ .

(23)

We furthermore have SX(xˆ+, ξˆX+ , 21 ¯λk) ⩽ ε due to (22) and the well-known fact that SX(x, ξ, λ¯) is non-decreasing in ¯λ > 0. (This monotonicity property follows from the proximal Polyak-Lojasiewicz inequality—see, e.g., [93, Lem. 1].) Meanwhile, by a version of Danskin’s theorem (cf. Lemma D.1 in the appendix), ξˆX+ belongs to the closed convex hull of the set of active x-gradients of fˆk at xˆ+:

ξˆX+ ∈ conv ∇xfˆk(xˆ+, y), y ∈ Argmax fˆk(xˆ+, y) .
y∈Y

Simlarly, we can choose ξ+ = ∇xf (xˆ+, y∗) for y∗ ∈ Argmaxy∈Y f (xˆ+, y), whence by convexity of ⋅ :

ξˆX+ − ξ+ ⩽ max y∈Y ∇xfˆk(xˆ+, y) − ∇xf (xˆ+, y∗) . Thus, in the case k = 0 we have ξˆX+ − ξ+ ⩽ min{µD, 2σ0} by Lemma 3.2. On the other hand, in the case k ⩾ 1 we take arbitrary y¯ ∈ Argmaxy∈Y ∇xfˆk(xˆ+, y) − ∇xf (xˆ+, y∗) and use Lemma 3.2 to get

ξˆX+ − ξ+ ⩽ ∇xfˆk(xˆ+, y¯) − ∇xf (xˆ+, y∗) ⩽ ∇xf (xˆ+, y¯) − ∇xf (xˆ+, y∗) + ∇xfˆk(xˆ+, y¯) − ∇xf (xˆ+, y¯)
Finally, plugging the obtained estimates for SX (xˆ+, ξˆX+ , 12 λ¯k) and ξˆX+ − ξ+

⩽ µD + 2σkk!Dk . into (23) we have that

(¯λk xˆ+ − x+ )2 ⩽ 4ε2 + 2¯λk xˆ+ − x+ min{µD, 2σ0} 1{k = 0} + µD + 2σkk!Dk 1{k ⩾ 1} .

Solving this inequality for ¯λk xˆ+ − x+

=

1 2

∇ϕˆ2¯λk (x∗) − ∇ϕ2¯λk (x∗)

we conclude the proof.

4 Nearly matching lower bounds on the admissible diameter

Our next goal is to prove that conditions (12)–(13) in Theorem 3.1 are tight in leading-order terms.
Namely, for any k ∈ N ∪ {0} we exhibit instances of (P) such that the corresponding approximate problem (Pk) has an exact FOSP which is not (ε, 2λ)-FOSP in (P) for any accuracy ε in the range

ε ≲ 1 min µD, λρkDk+1 .

(24)

k+1

k!

13

(For simplicity, in this section we let D = diam(Y ); this is anyway the case in all instances to be exhibited.) The objectives in these instances satisfy Assumption 1, Assumption 2 with appropriate k,
and Assumption 3. Moreover, for k ⩾ 1 we use bilinearly-coupled objectives (cf. (BC)), so Assumption 2 is satisﬁed with τk = 0 (hence, ¯λk in (13) simpliﬁes to λ, cf. (11)) and σk = µ1{k = 0} (hence, the additive to µD term in (13) disappears). Thus, for k ⩾ 1 our lower bound (24) is tight over the BC subclass up to a O(1 k) factor; for k = 0 it misses the σ0 term in the minimum, typically anyway large (cf. (14) and the accompanying discussion). In this sense, the condition Theorem 3.1
turns out to be nearly tight.
Let us now specify our problem instances. Consider a family of functions on R × R in the form

λx2

sρ y k+1

Fk,s,λ,µ,ρ(x, y) ∶= − 2 + µxy + (k + 1)!

for k ∈ N ∪ {0}, s ∈ {±1, 0}, λ > 0, µ ⩾ 0, ρ ⩾ 0. (25)

Note that Fk,s,λ,µ,ρ is BC, concave in x, and convex or concave in y depending on s. We also consider

λx2 ρy ⎛ ⎛ λ ⎞ ⎞

Sλ,ρ,D(x, y) ∶= − 4 + 2 ⎝tanh ⎝ ρD x⎠ − 1⎠ for λ > 0, ρ ⩾ 0, D ⩾ 0.

(26)

Each of our hard instances of the form (P) is speciﬁed by choosing one of these functions as the
objective, X ⊆ R, and Y = [a, a + D] with some shift a ∈ R, while allowing for varying λ, µ, ρ, and D. In the next lemma (proved in Appendix A) we establish the smoothness properties of these
functions.

Lemma 4.1. For any k ∈ N ∪ {0}, s = ±1, λ > 0, µ ⩾ 0, ρ ∈ R, and D ⩾ 0, the following claims hold:
1. Function Fk,s,λ,µ,ρ satisﬁes Assumption 1 on R × R (and thus also on R × [a, a + D] for any a ∈ R). Moreover, function Sλ,ρ,D satisﬁes Assumption 1 on R × [0, D] provided that µ ⩾ 2λρ D.
2. Let r = µ2Dλ . Assumption 2 with k = 0, ρ0 = ρ, and σ0 = µD is satisﬁed by function F0,0,λ,µ,0 on [−r, r] × [− 21 D, 12 D] if µ ⩽ 2λρ D, and by function Sλ,ρ,D on [−r, r] × [0, D] if µ ⩾ 2λρ D.
3. When k ⩾ 1 and a ∈ R, function Fk,s,λ,µ,ρ on R × [a, a + D] satisﬁes Assumption 2 with ρk = ρ, σk = µ1{k = 1}, and τk = 0—in other words, Assumption 2 restricted to the objective class (BC).

Due to signiﬁcant diﬀerences in the statements and analyses, we separately consider the cases k = 0, k = 1, and k ⩾ 2. We begin with the case k = 0, where we use F0,0,λ,µ,ρ or Sλ,ρ,D depending on the level of coupling (cf. claims 1 and 2 of Lemma 4.1) and obtain the following result.

Proposition

4.1.

For

λ, µ, ρ, D

> 0,

let

X

=

[−

µD 2λ

,

µD 2λ

],

fˆ ≡ fˆ0,

cf.

(TE),

with

f, Y, yˆ

to

be

deﬁned.

1. For f = F0,0,λ,µ,0, Y = [− 21 D, 21 D] one can ﬁnd yˆ ∈ Y and x∗ ∈ X such that ϕˆ′2λ(x∗) = 0 while

ϕ′2λ(x∗) ⩾ µ2D .

2. If µ ⩾

2λρ D, then for f = Sλ,ρ,D, Y = [0, D] there exist yˆ ∈ Y , x∗ ∈ X such that ϕˆ′2λ(x∗) = 0,

√

ϕ′2λ(x∗) ⩾

λρD . 3

14

When combined with the ﬁrst two claims of Lemma 4.1, Proposition 4.1 establishes (24) for k = 0. As a result, we verify tightness of Theorem 3.1 in the case of zeroth-order approximation. Note,

however, that Lemma 4.1 is restricted to the regime σ0 ≈ µD; hence, the term σ0 in the left-hand side of (12)—which is beneﬁcial when σ0 ≪ µD—is not captured in the result we have just obtained.
Next we address the case of ﬁrst-order approximation. Here we consider instances of (P) with

objective given by F1,−1,λ,µ,ρ or F1,1,λ,µ¯,ρ¯ for some µ¯ ⩽ µ and ρ¯ ⩽ ρ, depending on the regi√on of parameters (as controlled by the relative level of coupling µ compared to the geometric mean λρ1

of the “homogeneous” Lipschitz constants, cf. (A2) with k = 1). Here we obtain the following result.

Proposition

4.2.

For

λ, µ, ρ, D > 0,

set

Y

=

[−

1 2

D,

1 2

D],

fˆ ≡ fˆ1,

cf.

(TE),

with

f, yˆ

to

be

deﬁned.

1. If µ ⩽ λρ 2, then for f = F1,−1,λ,µ,ρ, one can ﬁnd yˆ ∈ Y , x∗ ∈ R such that ϕˆ′2λ(x∗) = 0 while

ϕ′2λ(x∗) ⩾ µ3D .

2. If µ ⩾ λρ 2, then for f = F1,1,λ,µ¯,ρ¯ with some µ¯ ⩽ µ and ρ¯ ⩽ ρ, one can ﬁnd yˆ ∈ Y, x∗ ∈ R such that ϕˆ′2λ(x∗) = 0 while

ϕ′2λ(x∗) ⩾

λρD2 . 8

By combining Proposition 4.2 with claims 1 and 3 of Lemma 4.1, we establish (24) for k = 1, and thus verify tightness of Theorem 3.1 in the case of ﬁrst-order approximation (cf. (13)),
without imposing any restrictions on the problem parameters. More precisely, our lower bound ε ≳
min{µD, λρD2} on the approximation accuracy matches the upper bound (13) for bilinearlycoupled objectives (BC), and replaces ¯λ1 = λ + 2τ1D with λ in the general case; this is only a minor modiﬁcation since D = O(ε) is anyway required in order for the guarantees in Theorem 3.1 and Proposition 4.2 to be applicable.
It remains to cover the case of approximation with k ⩾ 2. To this end, we focus on the instances of (P) with f = F1,−1,λ,µ¯,ρ or f = F1,1,λ,µ¯,ρ for some µ¯ ⩽ µ depending on the level of interaction µ.

Proposition 4.3. For λ, µ, ρ, D > 0 and k ⩾ 2, set fˆ = fˆk, cf. (TE), with f, Y, yˆ to be deﬁned, and

let

λρDk−1

µcr ∶=

k! .

(27)

1. If µ ⩽ µcr, then for Y = [0, D] and f = Fk,−1,λ,µ¯,ρ with some µ¯ ⩽ µ one can ﬁnd yˆ ∈ Y, x∗ ∈ R such that ϕˆ′2λ(x∗) = 0 while ϕ′2λ(x∗) ⩾ µ2Dk

2.

If µ ⩾ µcr, then for Y

=

[−

1 2

D,

1 2

D]

and

f

= Fk,1,λ,µ¯,ρ

with some µ¯ ⩽ µ one can ﬁnd yˆ ∈ Y , x∗ ∈ R

such that ϕˆ′2λ(x∗) = 0 while

ϕ′2λ(x∗) ⩾ µ2crkD

By combining Proposition 4.3 with claims 1 and 3 of Lemma 4.1, we establish (24) for k ⩾ 2, Thus, we verify that for BC objectives,√the bound in Theorem 3.1 is tight, up to the two terms in (13) being divided by O(k) and O( k) correspondingly; removing this gap is left for future
work.

15

On the proofs of Propositions 4.1 to 4.3. While our choice of objective functions used

in Propositions 4.1 and 4.3 is quite natural (except, perhaps, for (25) used in the case k = 0), the actual proofs of these propositions, as given in Appendix B, require quite a lot of technical work,

and are way less straightforward than the proof of Theorem 3.1. In all three cases, the analysis

relies upon carefully choosing the center yˆ of Taylor expansion and exhibiting x∗ which is stationary

for ϕˆ2λ, yet such that ϕ′2λ(x∗) is large. The choices of yˆ, x∗ depend on the problem parameters and approximation order; in particular, they are diﬀerent in the two regimes of strong/weak coupling.

The analysis in Proposition 4.3 is especially delicate, notably due to our ambitious goal of matching

the bound of Theorem 3.1 up to a polynomial in k gap. (For example, the reader may verify that

merely

replacing

Y

= [0, D]

with

[−

1 2

D,

1 2

D]

in

the

case

µ < µcr

would

result

in

the

extra

2k

factor.)

We hypothesize that the extra O(1 k) factor in Proposition 4.3, as compared to (13), can be removed. However, this will require analyzing a diﬀerent family of objectives, as our analysis of (25) seems to be tight. In any case, such an improvement is of little practical interest: approximation with k ≫ 1 does not lead to eﬃcient algorithms for searching FOSPs: as we shall see next, such schemes rely on solving the nested maximization problem, which becomes a daunting task for k > 2.

5 Eﬃcient algorithms for the search of ﬁrst-order stationary points
Our goal in this section is to implement step 2o of the strategy outlined in the introduction. To this end, we exploit the guarantee in Theorem 3.1 by replacing the task of ﬁnding FOSPs in (P) with that of ﬁnding FOSPs in (Pk) with k ∈ {0, 1, 2}, and propose eﬃcient algorithms for solving the latter task. Overall, as k increases, the proposed algorithms require access to higher derivatives of f in y and their oracle complexity estimates also deteriorate (especially when transitioning from k = 1 to k = 2). On the other hand, increasing k allows us to handle larger diameter of Y for the same ε.
Note that, despite the fact that our “approximation theory” in Sections 3 and 4 carefully handles the general case of arbitrary k, we do not propose algorithms based on Taylor approximations of order k ⩾ 3. This is because already for cubic approximation (i.e., when k = 3), solving the nested maximization problem becomes a daunting task: as it is shown in [94, Theorem 4], maximization of a general trilinear form is NP-hard even when it is available explicitly (in its tensor representation).

5.1 Algorithms based on the constant and linear approximations
In the case k = 0, we ﬁx arbitrary yˆ ∈ Y and consider (Pk) with objective fˆ0(x, y) = f (x, yˆ), that is

min f (x, yˆ).

(P0)

x∈X

This is a nonconvex minimization problem with a smooth objective ϕˆ(x) = f (x, yˆ), so we can ﬁnd a near-stationary point via projected gradient descent. This approach is summarized in Algorithm 1. It produces a point xT satisfying ∇ϕˆ(xT ) ≲ ε in O(1 ε2) iterations, with one projected gradient
step (in x) per iteration. Using Lemma C.1 stated below, this implies ∇ϕˆ2λ(xT ) ≲ ε in terms of
the approximate Moreau envelope, which then results in the desired guarantee ∇ϕ2λ(xT ) ≲ ε by
applying Theorem 3.1 with k = 0. We shall rigorously state these results later on in Theorem 5.1.

16

Algorithm 1 FOSP search based on the constant approximation of f (x, ⋅)

Input: x0 ∈ X, yˆ ∈ Y , γx > 0, T ∈ N 1: x∗ = x0; ε∗ = +∞

2: for t ∈ {0, 1, ..., T − 1} do

3: x̃t+1 = xt − γx∇xf (xt, yˆ)

4: xt+1 = ΠX [x̃t+1]

5: ▷ Maintain the best iterate so far

6:

ε2t =

∇xf (xt, yˆ)

2−

1 γ2

x̃t+1 − xt+1 2

7: if εt < ε∗ then

x

8:

x∗ = xt; ε∗ = εt

9: end if

10: end for Output: x∗

Another approach we advocate here is based upon focusing on the nonconvex-aﬃne problem

min max fˆ1(x, y),
x∈X y∈Y

(P1)

which corresponds to (Pk) with k = 1 (the choice of yˆ ∈ Y is again arbitrary). General nonconvexconcave problems can be solved by a simple gradient descent-ascent (GDA) scheme combined with quadratic regularization in y, with iteration complexity O(ε−5) [53].6 More elaborate algorithmic schemes based on the proximal-point method have O(ε−3) iteration complexity [54, 95, 55, 57]. Here we propose a GDA-type scheme in the form of Algorithm 2. With it we manage to guarantee O(ε−2)
iteration complexity, by exploiting the special properties of (P1):

•

fˆ1(x,

⋅)

is

aﬃne,

so

fˆ1(x,

⋅)

+

ρ 2

⋅ −yˆ 2 is maximized via a single projected gradient ascent step.

• (P1) has to be solved in the regime ε ≳ D min{µ, (¯λ1ρ1)1 2} where ε-FOSPs for (P1) translate to ε-FOSPs for (P) via Theorem 3.1 (cf. (13) with k = 1). This allows to regularize with ρ = ρ1 and results in ε-independent smoothness O(¯λ1 + µ2 ρ1) of the corresponding primal function.

6This estimate follows from the complexity O(ε−x 2εy−3 2) of ﬁnding an (εx, εy)-approximate ﬁrst-order Nash equilibrium by such method, see [54, p. 3], combined with [54, Proposition 5.5] which veriﬁes that the x-component of such a point is an O(εx)-FOSP as long as εy = O(ε2x ). Note that the well-known result [96, Proposition 4.12] commonly used
for such a reduction in recent works, is erroneous—see the discussion immediately following [54, Proposition 5.2].

17

Algorithm 2 FOSP search based on the linear approximation of f (x, ⋅)

Input: x0 ∈ X, yˆ ∈ Y , γx > 0, T ∈ N, Coupled ∈ {0, 1}, γy > 0 1: x∗ = x0; y∗ = yt; ε∗ = +∞ 2: for t ∈ {0, 1, ..., T − 1} do
3: if Coupled then

4:

yt = ΠY [yˆ + γy∇yf (xt, yˆ)]

5:

x̃t+1 = xt − γx[∇xf (xt, yˆ) + ∇2xyf (xt, yˆ)(yt − yˆ)]

6: else

▷ γy needed in this case only

7:

Choose yt ∈ Argmaxy∈Y ⟨∇yf (xt, yˆ), y⟩

8:

x̃t+1 = xt − γx∇xf (xt, yt)

9: end if

10: xt+1 = ΠX [x̃t+1]

11: ▷ Maintain the best iterate so far

12:

ε2t

=

1 γ2

x̃t+1 − xt

2 − γ12

x̃t+1 − xt+1

2

13:

if

εt

x
<

ε∗

then

x

14:

x∗ = xt; y∗ = yt; ε∗ = εt

15: end if

16: end for Output: x∗

Algorithm 2 admits O(ε−2) iteration complexity estimate as in the case of Algorithm 1. That
said, compared to Algorithm 2, the new algorithm has two advantages: a slightly increased range of available accuracies in the strongly-coupled regime µ2 ⩾ λ¯1ρ1; a smaller leading factor in the complexity estimate – depending on the primal gap, rather than the full duality gap. However, these improvements come at a price: computing ∇xfˆ1(x, y) requires access to a partial Hessian-
vector product oracle for f , namely

(x, y) ↦ ∇2xyf (x, yˆ)(y − yˆ),

(28)

since ∇xfˆ1(xt, yt) = ∇xf (xt, yˆ)+∇2xyf (xt, yˆ)(yt −yˆ). On the other hand, for weakly-coupled problems – i.e., when µ2 ⩽ ¯λ1ρ1 – Algorithm 2 uses a simpliﬁed approach: (i) the descent step is performed in the negative direction of ∇xf (xt, yt) instead of ∇xfˆ1(xt, yt); (ii) the gradient ascent step is replaced by the full maximization of the linear model fˆ1(xt, ⋅). These two properties allow us to avoid Hessian-vector product (28), and also to access Y through the (weaker) linear maximization oracle.

Next we state convergence guarantees for Algorithms 1 and 2 (see Appendix C for the proofs).

Theorem

5.1.

Grant

Assumption

1.

Running

Algorithm

1

with

γx

=

1 λ

and

number

of

iterations

T ⩾ 300λ[ϕ(xε02) − ψ(yˆ)] , (29) where ψ(y) ∶= minx∈X f (x, y) is the dual function of (P), guarantees ∇ϕˆ2λ(x∗) ⩽ ε 6. Moreover, we have ∇ϕ2λ(x∗) ⩽ ε (i.e., in terms of initial problem (P)) as long as

24µD ⩽ ε.

(30)

18

Note that the factor ϕ(x0)−ψ(yˆ) in (29) is the duality gap for the point (x0, yˆ); by weak duality, it is lower-bounded by the sum of the dual gap maxy∈Y ψ(y) − ψ(yˆ) and the primal gap

∆ ∶= ϕ(x0) − min ϕ(x).

(31)

x∈X

As we shall see next, Algorithm 2 admits a slightly diﬀerent (and typically better) comlexity estimate, in which the full duality gap is replaced with the primal gap, and λ with O(¯λ1 + µ2 ρ1). In addition, we relax condition (30) by replacing µ with O(min{µ, (¯λ1ρ1)1 2}).

Theorem 5.2. Grant Assumptions 1 to 3 for k = 1, let λ¯1 = λ + 2τ1D (cf. (11)), and assume that

200 min{µ, (¯λ1ρ1)1 2}D ⩽ ε.

(32)

Running Algorithm 2 with γx = 3¯λ1+1µ2 ρ1 , Coupled = 1{µ ⩾ (¯λ1ρ1)1 2} and γy = ρ11 if Coupled = 1, for

µ2 700¯λ1∆

T ⩾ 3 + ¯λ1ρ1

ε2 + 1

(33)

iterations, with ∆ being the initial primal gap (cf. (31)), results in x∗ ∈ X for which ∇ϕ2¯λ1 (x∗) ⩽ ε.

Remark 1. As a criterion for selecting the “best” iterate, in Algorithms 1 and 2 we use the quantity

ε2t = γ1x2 x̃t+1 − xt 2 − x̃t+1 − xt+1 2 ,

where x̃t+1 is the result of the gradient descent step from xt prior to projection (i.e., xt+1 = ΠX [x̃t+1]). In fact, εt = SX (xt, γ1x (xt −x̃t+1), γ1x ), where SX is the functional used in the proof of Proposition 3.2. Using this criterion, instead of the gradient norm (which is a weaker criterion, cf. [97, Theorem 4.3])
allows to work with the Moreau envelope (cf., in particular, Lemma C.1 in Appendix C), and seems
to be necessary already in the nonconvex-concave setup (see [54, Proposition 5.5]).

In what follows next, we propose another algorithm based on the quadratic approximation of f .

5.2 An algorithm based on the quadratic approximation
Finally, we propose a more sophisticated method in which we focus on the quadratic approximation

min max fˆ2(x, y).
x∈X y∈Y

(P2)

(Recall that fˆ2(x, ⋅) is the quadratic approximation of f (x, ⋅) in y with arbitrary choice of yˆ ∈ Y , cf. (10).) Compared to the previous ones, the approach we are about to present has an additional limitation: Y must be a Euclidean ball in Rd. For simplicity, we shall assume that Y has diameter precisely D, and is origin-centered—i.e., Y = Bd(D) with
Bd(D) ∶= {y ∈ Rd ∶ y ⩽ 21 D}.

Note that centering Y in the origin is not a limitation: Assumptions 1 and 2 are preserved under shifts of y. The construction of our method rests upon the following two observations.

19

We ﬁrst recall, following [62], that an (ε 6, 2¯λ2)-FOSP in (P2) can be found by running O(ε−4) iterations of a (projected) subgradient scheme on the associated to (P2) primal function ϕˆ(x), which is ¯λ2-weakly convex (cf. Lemma 3.3). By Theorem 3.1, this also gives an (ε, 2¯λ2)-FOSP in (P2). Each iteration of the subgradient scheme amounts to alternating between a maximization step in y, i.e., ﬁnding y∗ = y∗(x) ∈ Argmaxy∈Y fˆ2(x, y), and a projected gradient descent step on fˆ2(⋅, y∗). Moreover, the analysis in [62] (cf. also [61, Theorem 31]) shows that this complexity is preserved under objective value errors of up to O(ε2 ¯λ2) in the maximization step.
Our second observation is that, despite the corresponding objective fˆ2(x, ⋅) being nonconcave, the maximization steps can be eﬃciently performed by running a ﬁrst-order algorithm on fˆ2(x, ⋅). To this end, we make use of the recent result of [92], who showed that the Krylov subspace of dimension Õ(Dδ−1 2) contains a δ-accurate maximizer of a nonconcave quadratic form on a Euclidean ball.
Krylov-type schemes can usually be eﬃciently implemented via a Lanczos-type method (see [98]), with Õ(Dδ−1 2) matrix-vector products to ﬁnd a δ-accurate maximizer.
Below we present Algorithm 3 which adapts the general subgradient scheme [62] to the present situation, assuming access to an abstract maximization oracle ApproxMax(fˆ2(x, ⋅), Y, δ) returning a δ-accurate maximizer of fˆ2(x, ⋅) over Y . Eﬃcient implementation of this oracle, in the form of Algorithm 4 is discussed in Appendix E. Observe that Algorithm 3 can be run in a simpliﬁed (or “naive”) regime, in which the descent step is performed using ∇xf (⋅, yt) rather than ∇xfˆ2(⋅, yt), so there is no need of higher-order oracles used otherwise (cf. line 7 of Algorithm 3).

Algorithm 3 FOSP search based on the quadratic approximation of f (x, ⋅)

Input: x0 ∈ X, yˆ ∈ Y , γx > 0, T ∈ N, δ > 0, Naive ∈ {0, 1}

1: for t ∈ {0, 1, ..., T − 1} do 2: yt = ApproxMax(fˆ2(xt, ⋅), Y, δ)

▷ implemented in Algorithm 4 (cf. Appendix E)

3: if Naive then

4:

xt+1 = ΠX [xt − γx∇xf (xt, yt)]

5: else

6:

xt+1 = ΠX [xt − γx∇xfˆ(xt, yt)],

7:

where ∇xfˆ(xt, yt) = ∇xf (xt, yˆ) + ∇2xyf (xt, yˆ)(yt − yˆ) + ∇3xyyf (xt, yˆ)[⋅, yt − yˆ, yt − yˆ]

8: end if

9: end for

Output: xs, where s ∈ {0, ..., T − 1} is sampled uniformly at random

We now present a convergence guarantee for Algorithm 3.

Proposition 5.1. Grant Assumptions 1 to 3 for k = 2, let λ¯2 = λ + τ2D2 (cf. (11)), and assume

that

⎧⎪⎪

2 ¯λ2ρ2D3 ⎫⎪⎪

24 min ⎨⎪⎪⎩µD + σ2D , 300 ⎬⎪⎪⎭ ⩽ ε. (34)

Furthermore, assume that f (⋅, y) is σ0-Lipschitz for any y ∈ Y . Finally, for δ > 0 and any x ∈ X, let ApproxMax(∇yfˆ2(x, ⋅), Y, δ) output yδ = yδ(x) ∈ Y such that fˆ2(x, yδ) ⩾ maxy∈Y fˆ2(x, y) − δ. Then:

1. Algorithm 3 run with Naive = 0 and the choice of parameters (for ﬁxed p ∈ (0, 1))

1 γx = σ0 + σ2D2

∆ + ρ2D3

4p ε2

6 ⋅ 106 λ¯2(∆ + ρ2D3)(σ0 + σ2D2)2

λ¯2T , δ = 104 ⋅ ¯λ2 , T ⩾ p2 ⋅

ε4

, (35)

20

for ∆ deﬁned in (31), with probability at least 1 − p outputs xs ∈ X such that ∇ϕ2¯λ2 (xs) ⩽ ε. 2. Moreover, the output of Algorithm 3 run with Naive = 1 has the same property if 24σ2D2 ⩽ ε√p.

We prove Proposition 5.1 in Appendix C. The ﬁrst claim is proved by following the footsteps

of [61, Theorem 31] up to minor modiﬁcations: ﬁrst, parameters σ0 and ∆ have to be adjusted for

the use of ϕˆ instead of ϕ; second, the bound in expectation is replaced with a ﬁxed-probability one.

The argument proceeds by establishing O(ε−4) complexity in terms of the surrogate ϕˆ2¯λ2 (xs) , and

then applying Theorem 3.1 under the high-probability event, which allows to control ϕ2¯λ2 (xs) . The second claim, pertaining to a simpliﬁed (“naive”) variant of the algorithm, where descent
is performed in the direction of ∇xf (⋅, yt) rather than that of ∇xfˆ2(⋅, yt), is proved by controlling trheqeurireisnuglttinhgatpσe2rDtu2rb≲aεti√onp souf ﬃϕcˆe2s¯λ2f(oxrt)th.e “Tnhaiivsep”eartpuprrboaatcihontotuwronrsk.ouNt ottoe tbheatOu(pn−d1er2σ(23D4)2,),thsios

requirement is very weak: it is either satisﬁed right away if the minimum in (34) is attained on the

ﬁrst argument, or follows from (34) when D is smaller than an ε-independent threshold—namely, when σ22D ≲ λ¯2ρ2. Moreover, in the latter case—which is of main interest, as otherwise there is

no advantage in using Algorithm 3 over Algorithms 1 and 2 anyway—the number of iterations as

per (35) becomes

1 O ¯λ2∆σ02 + ¯λ2∆ + σ02 + 1 .

p

ε4

ε2

Implementation of the max-oracle. Next we show how to implement ApproxMax(fˆ2(x, ⋅), Y, δ) in the case where Y is a Euclidean ball. To this end, for g ∈ Rd and a symmetric H ∈ Rd×d, we let
ΨH,g(y) = 21 y⊺Hy + g⊺y (36) be the corresponding quadratic form, and we aim at eﬃciently solving problems of the form

max ΨH,g(y)
y∈Bd (D)

(37)

up to accuracy δ > 0 in objective value given access to g and the matrix-vector multiplication oracle y ↦ Hy. In order to accomplish this goal, we shall exploit the following result from [92].

Proposition 5.2 ([92, Corollary 5.2]). Consider the joint Krylov subspace

K2m(H, {g, ξ}) ∶= span {Hjg, Hj ξ}j∈{0,...,m−1}

(38)

of dimension min{2m, d} with ξ ∼ Uniform(Sd−1). For any R ⩾ 0 and q ∈ (0, 1), w.p. ⩾ 1 − q one has

√

max ΨH,g(y) −

max

4 H R2 ΨH,g(y) ⩽

2 + log2

2

d

.

y ⩽R

y ∈ K2m(H,{g,ξ})∶ y ⩽R

m2

q

Proposition 5.2 immediately implies that whenever

m⩾D

√

H 2 + log2 2 d ,

δ

q

21

the corresponding joint Krylov subspace K2m(H, {g, ξ}) with probability at least 1 − q contains

a δ-suboptimal solution to (37) in terms of objective value. Now, since fˆ2(x, y) = ΨHˆ (x),gˆ(x)(y)

with

gˆ(x) = ∇yf (x, yˆ) and Hˆ (x) = ∇2y2f (x, yˆ),

(39)

we conclude that, granted Assumption 2 with k = 1 (more precisely, ﬁniteness of ρ1, cf. (A2)), any optimal solution to the problem

max

ΨH(x),g(x)(y)

y ∈ K2m(Hˆ (x),{gˆ(x),ξ})∶ y ⩽ 12 D

with

⎡⎢⎢ ⎧⎪⎪⎪ m = ⎢⎢⎢⎢min ⎨⎪⎪⎪⎩D

√

ρ1 2 + log2 2 d

δ

q

d ⎫⎪⎪⎪⎤⎥⎥ , 2 ⎬⎪⎪⎪⎭⎥⎥⎥⎥ (40)

implements the query ApproxMax(fˆ2(x, ⋅), Bd(D), δ) with probability at least 1 − q. On the other hand, as discussed in [92], the computational burden of solving (40) to machine precision is dominated by O(m) calls of the oracle (x, y) ↦ [gˆ(x), Hˆ (x)y], inner products, and elementwise vector operations on Ey (typically y ↦ Hˆ (x)y is the most expensive of these operations).7 Now, by recalling Proposition 5.1 and plugging in the value of δ from (35), we arrive at the following result.

Theorem 5.3. Grant the premise of Proposition 5.1—that is, Assumptions 1 to 3 with k = 2; σ0Lipschitzness of f (⋅, y) for any y ∈ Y , and condition (34) on D. Moreover, assume that Y = Bd(D), and ∇yf (x, ⋅) is ρ1-Lipschitz for all x, i.e., ∇yf (x, y′)−∇yf (x, y) ⩽ ρ1 y′ −y ∀x ∈ X and y, y′ ∈ Y . Choosing p ∈ (0, 1) and q = (0, 1 − p), run Algorithm 3 with Naive = 0, parameters γx, δ, T chosen according to (35), and the oracle x ↦ ApproxMax(fˆ2(x, ⋅), Y, δ) implemented by running Algorithm 4 with g = gˆ(x), H = Hˆ (x) (cf. (39)), R = 12 D, and m = ⌈min{M, d 2}⌉ with

50 D M= ε

√ 2 + log2 2T d
q

ρ1λ¯2 p

⎡⎢(34) ⎢⎢ ⩽ ⎢⎣

80 min{(¯λ2ρ2ε)1 3, 24µ}

ρ1λ¯2

2 T √d ⎤⎥⎥

p 1 + log q ⎥⎥⎦ .

Then the resulting point xs ∈ X satisﬁes ∇ϕ2¯λ2 (xs) ⩽ ε with probability at least 1 − (p + q), and is constructed by performing O(T ) calls of the oracle

(x, y) ↦∇xfˆ2(x, y) = ∇xf (x, yˆ) + ∇2xyf (x, yˆ)(y − yˆ) + ∇3xyyf (x, yˆ)[⋅, y − yˆ, y − yˆ]

(41)

and projections onto X, and O(M T ) calls of the oracle (x, y) ↦ (∇yf (x, yˆ), ∇2y2f (x, yˆ)(y − yˆ)), inneMr oprreoodvuecrt,sifonweEiyn, aadndditeiolenmaesnstuwmisee tvheactto2r4σo2pDer2a⩽tioεn√spo(ncfE. yth. e second claim of Proposition 5.1),
then running Algorithm 3 with Naive = 1 produces xs ∈ X with the same property while using the oracle (x, y) ↦ ∇xf (x, y) instead of (41).

Comparing this result with Theorems 5.1 and 5.2 we see that for Algorithm 3 the allowed range of D improves from O(ε) to O(ε2 3), but this happens at the price of a signiﬁcantly deteriorated complexity—from O(ε−2) to O(ε−13 3). We leave open the questions of whether the latter complexity estimate can be improved, and whether one can smoothly interpolate between the two complexities.
7Such an implementation is discussed in [92, Appendix A], but somewhat informally, and no pseudocode of an algorithm is given. For this reason, in Appendix E we provide a formal algorithm (following the footsteps of [92]) and analyze its complexity. Note that this can also be useful in the broader context of nonconvex quadratic optimization.

22

A Deferred proofs for Section 3

A.1 Proof of Lemma 3.1

Take arbitrary yˆ, y ∈ Y and x ∈ X. Let yt = (1 − t)yˆ + ty for t ∈ [0, 1], and deﬁne ψx ∶ [0, 1] → R

by ψx(t) ∶= f (x, yt). Clearly, f (x, y) = ψx(1). Moreover, ψx(⋅) has the ﬁrst k derivatives in the

form

ψx(j)(t) = ∇jyj f (x, yt)[(y − yˆ)j ], 0 ⩽ j ⩽ k;

(42)

thus by (TE) we have

fˆk(x, y) = j=k0 j1! ψx(j)(0).

Now, it follows from (A2) with x′ = x that ψx(k) is absolutely continuous on [0, 1], hence its derivative exists almost everywhere on [0, 1] and is given by

ψx(k+1)(t) = ∇kyk++11 f (x, yt)[(y − yˆ)k+1].

(43)

Thus, expressing the Taylor expansion remainder ψx(1) − ∑kj=0 j1! ψx(j)(0) in the integral form we get

f (x, y) − fˆk(x, y) = 0 1 (1 −k!t)k ψx(k+1)(t) dt = 0 1 (1 −k!t)k ∇kyk++11 f (x, yt)[(y − yˆ)k+1] dt. (44) Whence we arrive at
f (x, y) − fˆk(x, y) ⩽ y − yˆ k+1 0 1 (1 −k!t)k ∇kyk++11 f (x, yt) dt ⩽ ρkDk+1 0 1 (1 −k!t)k dt = (ρkkD+k1+)1! .

The lemma is proved.

A.2 Proof of Lemma 3.2
For k = 0 the result is obvious: we have ∇xfˆ0(x, y) = ∇xf (x, yˆ), so ∇xf (x, y) − ∇xfˆ(x, yˆ) can be bounded via Assumption 1 or via triangle inequality and Lipschitzness of f (⋅, y). For k ⩾ 1, ﬁx yˆ and arbitrary y ∈ Y and x ∈ X, and let yt = (1 − t)yˆ + ty for t ∈ [0, 1]. As in the proof of Lemma 3.1 we deﬁne ψx ∶ [0, 1] → R as ψx(t) ∶= f (x, yt), and observe that (42)–(44) are still valid. (Indeed, imposing (A2) with σk = ∞ suﬃces for ψx(k) to be absolutely continuous on [0, 1], and hence for its derivative to exists almost everywhere on [0, 1] and to be given by (43).) As a result, we have that

f (x, y) − fˆk(x, y) =

1 (1 − t)k ψx(k+1)(t) dt = − ψx(k)(0) +

1 (1 − t)k−1 ψ(k)(t) dt
x

0 k!

k!

0 (k − 1)!

= 0 1 ((1k−−t)1k)−!1 ∇kyk f (x, yt) − ∇kyk f (x, yˆ) [(y − yˆ)k]dt, (45)

where we ﬁrst used integration by parts and then (42). Taking the partial gradient in x we have

⟨∇xf (x, y) − ∇xfˆk(x, y), u⟩ = 0 1 ((1k−−t)1k)−!1 ∇kxy+k1f (x, yˆ)[(y − yˆ)k; u] − ∇kxy+k1f (x, yt)[(y − yˆ)k; u] dt,

23

where u ∈ Ex is arbitrary, we used the abridged notation [(y − yˆ)k; u] ∶= [(y − yˆ), .., (y − yˆ); u] for tensor evaluation, and the right-hand side is well-deﬁned by the premise of the lemma. Taking
supremum over the unit ball in Ex and combining Jensen’s inequality with (A3), we arrive at

∇xf (x, y) − ∇xfˆk(x, y) ⩽ y − yˆ k 0 1 ((1k−−t)1k)−!1

∇kxy+k1f (x, yˆ) + ∇kxy+k1f (x, yt)

2σk Dk dt ⩽ k! .

The lemma is proved.

A.3 Proof of Lemma 3.3

For k = 0 the result is clear from ∇xfˆ0(x, y) = ∇xf (x, yˆ). When k ⩾ 1, it suﬃces to prove that ∇2x2fˆk(x, y) − ∇2x2 f (x, y) ⩽ 2τkkD! k (46)
for all y ∈ Y almost everywhere on X. Indeed, (A1) with µ = ∞ is equivalent to ∇2x2f (x, y) ⩽ λ holding for all y ∈ Y almost everywhere on X, whence (46) would imply (by the triangle inequality) that ∇2x2fˆk(x, y) ⩽ λ¯k for all y ∈ Y almost everywhere on X, which is equivalent to (11). Hence it only remains to verify (46). This can be done via (45) (which is valid by continuity of ∇kyk f (x, ⋅)):

f (x, y) − fˆk(x, y) = 0 1 ((1k−−t)1k)−!1 ∇kyk f (x, yt) − ∇kyk f (x, yˆ) [(y − yˆ)k]dt.

Now observe that by (A3), for any y ∈ Y tensor ∇kx2+y2k f (x, y) exists and satisﬁes ∇kx2+y2k f (x, y) ⩽ τk almost everywhere on X. Fix y ∈ Y , yˆ ∈ Y , and x ∈ X, and assume w.l.o.g. that x ∈ X is such that ∇kx2+y2k f (x, yt) exists (and hence ∇kx2+y2k f (x, yt) ⩽ τk) for all yt ∈ [yˆ, y]. Then, for any u, v ∈ Ex,

⟨v, ∇2x2 f (x, y) − ∇2x2 fˆk(x, y) u⟩ = 0 1 ((1k−−t)1k)−!1 ∇xk2+y2k f (x, yt) − ∇kx2+y2k f (x, yˆ) [(y − yˆ)k; u, v] dt. (47)
Whence, taking supremum over u, v on the unit sphere, by Jensen’s inequality and (A3) we arrive
at

∇2x2f (x, y) − ∇2x2 fˆk(x, y) ⩽ y − yˆ k 0 1 ((1k−−t)1k)−!1 ∇kx2+y2k f (x, yt) + ∇kx2+y2k f (x, yˆ) dt

⩽ 2τkDk 1 (1 − t)k−1 dt = 2τkDk .

0 (k − 1)!

k!

Finally, observe that these estimates remain valid, for any y ∈ Y and almost all x ∈ X, even if ∇kx2+y2k f is not guaranteed to exist everwhere on X ×Y . Indeed, for any x ∈ X deﬁne Yx′ as the set of all y′ ∈ Y where ∇kx2+y2k f (x, y′) does not exist. Consider the graph of the set-valued map x ↦ Yx′, that is,

Γ ∶= {(x, y′) ∶ x ∈ X, y′ ∈ Yx′} ⊂ X × Y.

By Assumption 3, Γ is (mX × mY )-measurable (here mX , mY are the Lebesgue measures on X,Y ).
Hence its restriction Γ∗ on X × [yˆ, y] is measurable with respect to the induced measure m∗ = mX×[yˆ,y], and we can apply Fubini’s theorem:

m∗(Γ∗) = x∈X m[yˆ,y](Yx′) dx = y′∈[yˆ,y] mX (Xy′ ) dy′, where Xy′ ∶= {x ∈ X ∶ y′ ∈ Yx′}.

24

By (A3) we have mX (Xy′ ) = 0 for any y′ ∈ Y , whence m∗(Γ∗) = 0 by the second representation of m∗(Γ∗), and therefore m[yˆ,y](Yx′) = 0 for almost all x ∈ X (by the ﬁrst representation of m∗(Γ∗)). This shows that, for any choice of y, yˆ ∈ Y , formula (47) is valid for almost all x (the integrand exists almost everywhere on [0, 1]), and so the ﬁnal estimate is preserved.

A.4 Justiﬁcation of (18)

We ﬁrst consider the case k > 1. Recall that we have to show that condition min µD, λρkDk+1 ≲k ε, for suitable constant factors that might depend on k, implies (13) under (18)—i.e., provided that

⎧⎪⎪⎪ µk

1 k−1

λ2k+1ρk

1 2k

⎫⎪⎪⎪

ε ≲k min ⎨⎪⎪⎪⎩ σk

,

k ⎬.

τkk+1

⎪⎪⎪⎭

It suﬃces to show that (18) implies σkDk ≲k µD when µD ≲ ε, and τkDk ≲k λ when λρkDk+1 ≲k ε. The ﬁrst of these implications follows from the ﬁrst part of (18): indeed, under its premise we have

1

µk k−1

µD ≲ ε ≲k

,

σk

whence σkDk ≲k µD follows by taking power k − 1 > 0. For the second implication, the premise gives
1
λρkDk+1 ≲k ε2 ≲k λ2τkk++11ρkk k ,
k

whence

Dk+1

≲k

(λ

k+1
τk) k

,

that

is

τkDk

≲k

λ

by

taking

power

k k+1

>

0.

Both

implications

are

proved.

Finally, in the case k = 1 the ﬁrst implication holds trivially, as σ1 = µ w.l.o.g. On the other

hand,

our

previous

argument

for

the

second

implication

applies

here

as

well

(since

k k+1

> 0).

B Proofs for Section 4

B.1 Proof of Lemma 4.1
The ﬁrst claim is straightforward for Fk,s,λ,µ,ρ. For Sλ,ρ,D, as 0 ⩽ tanh′(x) ⩽ 1 and −1 ⩽ tanh′′(x) ⩽ 1,

∂2

λ⎛

y

′′⎛

∂x2 Sλ,ρ,D(x, y) = 2 ⎝−1 + D tanh ⎝

λ ⎞⎞

ρD

x ⎠⎠

∈

[−λ,

0],

∂2

1

∂x∂y Sλ,ρ,D(x, y) = 2

λρ tanh′ ⎛

D

⎝

λ⎞ 1

ρD

x ⎠

∈

0, 2

λρ

µ

D ⊆ 0, 2√2 .

For the second claim, we ﬁrst note that ∂∂y F0,s,λ,µ,0 = µx and ∂∂x F0,s,λ,µ,0 = µy−λx. So if µ ⩽ then we have on [−r, r] × [−D 2, D 2] that

∂

µ2D

∂y F0,0,λ,µ,0 ⩽ 2λ ⩽ ρ,

∂ ∂x F0,0,λ,µ,0 ⩽ µD.

2λρ D,

25

On the other hand, if µ ⩾ 2λρ D, we have on [−r, r] × [0, D] that

∂

ρ⎛ ⎛

∂y Sλ,ρ,D = 2 ⎝tanh ⎝

λ⎞ ⎞

ρD

x ⎠

−

1⎠

∈

[−ρ,

0],

∂

y

∂x Sλ,ρ,D = 2

λρ tanh′ ⎛

D

⎝

λ ⎞ λx 1

x− ρD ⎠

2

⩽2

µD λρD + 4 < µD,

where we used that −1 ⩽ tanh(x) ⩽ 1 and 0 ⩽ tanh′(x) ⩽ 1. The third claim is straightforward.

B.2 Proof of Proposition 4.1

1o.

As

per

the

ﬁrst

claim,

let

f

(x,

y)

=

−

1 2

λx2

+

µxy,

X

= [−r, r],

Y

= [R, R]

for

R=D

2

and

r ∶= µR λ.

(48)

Clearly, ϕ(x) = − 21 λx2 + µR x = λ(− 12 x2 + r x ), whence ϕ2λ(x) = λ minu∈X{(u − x)2 − 12 u2 + r u }. Let x+(x) be the (constrained) minimizer. The unconstrained minimizer is given by [2x]r, where

[z]r ∶= (max{ z , r} − r) sign(z)

(49)

is the soft thresholding operator. Now, observe that whenever x ⩽ r, one has [2x]r ⩽ r, and therefore x+(x) = [2x]r. Whence by (9) (cf. also Proposition D.1 in appendix) for any x ∈ X we

have

ϕ′2λ(x) = 2λ(x − [2x]r).

(50)

On the other hand, we have that ϕˆ(x) = f (x, yˆ) and thus ϕˆ2λ(x) = λ minu∈X {(u−x)2 − 12 u2 +uµyˆ λ}. Here the unconstrained minimizer is given by 2x − µyˆ λ, hence xˆ+(x) = 2x − µyˆ λ for the actual (constrained) minimizer as long as 2x − µyˆ λ ⩽ r. Now, let us choose

x∗ = r 2 yˆ = R 2.

Clearly, 2x∗ − µyˆ λ = r 2, therefore ϕˆ′2λ(x∗) = 2λ(x∗ − xˆ+(x∗)) = 0. Meanwhile, due to (50) we have

ϕ′2λ(x∗)

=

2λ(x∗

−

[2x∗ ]r )

=

2λ(

1 2

r

−

[r]r )

=

λr

=

µR.

2o. Now consider f (x, y) = Sλ,ρ,D(x, y), cf. (26), on [−r, r] × [0, D]. Since tanh(⋅) ⩽ 1 on R, we have ϕ(x) = f (x, 0) = − 41 λx2 and ϕ2λ(x) = λ min−r⩽u⩽r (u − x)2 − 41 u2 , with unconstrained minimizer given by 34 x. Hence, as long as x ⩽ 3r 4, we have ϕ2λ(x) = −λx2 3 and ϕ′2λ(x) = −2λx 3.
On the other hand, for the choice yˆ = 2D 3 we have

λx2 ρD ⎛ ⎛ ϕˆ(x) = f (x, yˆ) = − 4 + 3 ⎝tanh ⎝

λ x⎞ − 1⎞ . ρD ⎠ ⎠

Since ϕˆ(⋅) is diﬀerentiable on X = [−r, r], the set of its points on X with vanishing derivative coincides with such set for ϕˆ2λ(⋅). Let us now ﬁnd x∗ ∈ [−r, r] for which ϕˆ′(x∗) = 0, i.e., solutions

to

λ x∗ cosh2 ⎛

ρD

⎝

ρλD x∗⎞⎠ = 23 .

26

The unique solution is x∗ = c ρD λ with c ∈ (0.51, 0.52). Using that µ ⩾

that

x∗ < 0.√522µλD < 38µλD = 34r ,

whence

ϕ′2λ(x∗) = −2λx∗

√ 3 < − λρD

3.

2λρ D, we conclude

B.3 Proof of Proposition 4.2

1o. Assume that µ ⩽ λρ 2. Recall that f (x, y) = − 21 λx2 + µxy − 21 ρy2. Hence for yˆ = 0 we

have

fˆ(x, y)

=

−

1 2

λx2

+

µxy

and

ϕˆ(x)

=

λ(−

1 2

x2

+

r

x

)

where

r

= µR

λ

with

R=

12 D

(cf.

(48)).

This

results in ϕˆ′2λ(x) = 2λ([2x]r − x) and x∗ = r being a stationary point for ϕˆ2λ (cf. (50)).

Meanwhile, the maximum in ϕ(x) = − 12 λx2 + max y ⩽R{µxy − 21 ρy2} is eﬀectively unconstrained—

attained at µx ρ—whenever

x

⩽ ρR

µ;

thus,

for

such

x

we

have

ϕ(x) =

1 2

µ2 ρ − λ

x2 and hence

ϕ′(x) = (µ2 ρ − λ)x.

Moreover, by the ﬁrst-order optimality condition 2λ(x − x+(x)) ∈ ∂ϕ′(x+(x)), cf. (8), we express the proximal mapping x+(x) = x+ϕ 2λ(x) as the solution to 2λ(x+(x) − x) + µ2 ρ − λ x+(x) = 0, that is
x+(x) = λ2ρλ+ρxµ2 , whenever λ2ρλ+ρ xµ2 ⩽ ρµR . In particular, the above expression is valid for x∗ = r: indeed, recalling that µ ⩽ λρ 2, we obtain

2λρr

2µR ρR

λρ + µ2 < 2r =

λ

⩽

. µ

To verify the ﬁrst claim of the proposition, it remains to observe that

′

+

2λρ

λρ − µ2 2λr 2µR

−ϕ2λ(r) = 2λ(x (r) − r) = 2λr λρ + µ2 − 1 = 2λr λρ + µ2 ⩾ 3 = 3 ,

where the inequality is due to µ2 ⩽ λρ 2.

√

2o. We shall now prove the second claim. Deﬁne ρ¯ = ρ 4, µ¯ ∶= λρ 2 = 2λρ¯, and r¯ ∶= µ¯R λ

(cf. (48)), so that f (x, y) = F1,1,λ,µ¯,ρ¯ = − 21 λx2 + µ¯xy + 12 ρ¯y2.

As such, ϕ(x) = λ

−

1 2

x2

+

r¯

x

+ 12 ρ¯R2

and ϕ2λ(x) = λ minu∈R{(u − x)2 − 21 u2 + r u } + 21 ρ¯R2. This implies the same result as in (50), namely

ϕ′2λ(x) = 2λ(x − [2x]¯r).

(51)

On the other hand, fˆ ≡ fˆ1

at any yˆ ∈ Y

is

given

by

fˆ(x,

y)

=

−

21 λx2

+

(µ¯x

+

ρ¯yˆ)y

−

1 2

ρ¯yˆ2;

in

particular,

for yˆ = R we have fˆ(x, y) = − 21 λx2 + (µ¯x + ρ¯R)y − 12 ρ¯R2 and ϕˆ(x) = − 21 λx2 + R µ¯x + ρ¯R − 12 ρ¯R2, thus

ϕˆ2λ(x) = λ min (u − x)2 − u2 + r¯ u + ρ¯R − ρ¯R2 .

(52)

u∈R

2

µ¯

2

By the optimality condition, the minimizer is given by xˆ+(x) = [2x + ρ¯R µ¯]¯r − ρ¯R µ¯, so we arrive at

ϕˆ′2λ(x) = 2λ x + ρ¯R − 2x + ρ¯R .

(53)

µ¯

µ¯ r¯

27

We conclude that x∗ = −ρ¯R µ¯ is stationary for ϕˆ2λ(⋅): indeed, plugging in µ¯ = √2λρ¯ we have −ϕˆ′2λ(x∗) = 2λ [−ρ¯R µ¯]¯r = 2λ [−r¯ 2]r¯ = 0. Meanwhile, due to (51) we conclude that

−ϕ′2λ(x∗) = 2λ ρ¯R + − 2ρ¯R = 2λ ρ¯R + [−r¯]¯r = 2λρ¯R =

µ¯

µ¯ ¯r

µ¯

µ¯

This concludes the proof.

2λρ¯R2 =

λρD2 8.

B.4 Proof of Proposition 4.3

B.4.1 Case µ ⩽ µcr

Recall that here we use Y = [0, D] and

λx2

ρ y k+1

f (x, y) = Fk,−1,λ,µ,ρ(x, y) = − 2 + µ¯xy − (k + 1)! .

with

some

µ¯ ⩽

µ

yet

to

be

chosen.

Let

yˆ =

0;

then

fˆ(x, y)

=

−

1 2

λx2

+

µ¯xy

is

maximized

on

{0, D},

so

λx2 ϕˆ(x) = − 2 + µ¯D max{x, 0}.

Clearly, the point

x∗ = µ¯λD

is stationary for ϕˆ(⋅), and thus for ϕˆ2λ(⋅) as well. It remains to lower-bound ϕ′2λ(x∗) . To this end, note that ∂∂y f (x, y) = µ¯x − k1! ρ y k sign(y), so f (x, ⋅) has a unique unconstrained maximizer y¯ = y¯(x)

which

is

given

as

the

solution

to

µ¯x

−

1 k!

ρ

y

k sign(y)

=

0;

in

other

words,

y¯(x)

=

( x µ¯k!

ρ)1

k sign(x).

Clearly, we have that ϕ(x) = f (x, y¯(x)) for any x ∈ R such that y¯(x) ∈ [0, D]—in other words, when

ρDk

0 ⩽ µ¯x ⩽ k! .

(54)

As a result, for such x function ϕ(x) is diﬀerentiable, and we have

λx2 k xµ¯k! 1 k

ϕ(x) = − 2 + k + 1 ρ

µx,

′

xµ¯k! 1 k

ϕ (x) = −λx + µ¯ ρ

.

Now, recall (cf. (9)) that we have 2λ(x − x+(x)) ∈ ∂ϕ(x+(x)) for the proximal mapping x+(x) corresponding to ϕ with stepsize 21λ . Therefore, we can compute x+(x) for given x ⩾ 0 by solving

+ µ¯ x+µ¯k! 1 k

x +λ ρ

= 2x

(55)

for x+ ⩾ 0 (such a solution is clearly unique) and verifying that the solution satisﬁes µ¯x+ ⩽ k1! ρDk (cf. (54)). It is clear that, for any x ⩾ 0, the corresponding solution x+(x) to (55) satisﬁes the

bounds

µ¯ 2xµ¯k! 1 k +

2x − λ ρ

⩽ x (x) ⩽ 2x.

(56)

28

To this end, let x+ = x+(x∗) for x∗ = µ¯λD , and µ¯ = 12 µ [⩽ 21 µcr]. By (27) and the upper bound in (56), + 2µ¯2D ρDk
µ¯x ⩽ λ ⩽ 2 ⋅ k! , which veriﬁes our characterization of x+ as the solution to (55) for chosen x∗. On the other hand,

µ¯ 2x∗µ¯k! 1 k µ¯D 2µ¯2k! 1 k x∗

λρ

= λ λρDk−1 ⩽ 21 k ,

whence by the lower bound in (56) we get x+ ⩾ (2 − 2−1 k)x∗ and, using (9), arrive at −ϕ′2λ(x∗) = 2λ(x+ − x∗) ⩾ (1 − 2−1 k)µD > µ2Dk .
Here the ﬁnal step uses k ⩾ 2 and the fact that the function t ↦ (1 − 1t )t increases on [1, +∞].

B.4.2 Case µ ⩾ µcr

Denote R = D 2 and let

µ¯R r¯ ∶= λ

for some µ¯ ⩽ µcr [⩽ µ] yet to be chosen. Recall that here we use Y = [−R, R] and

λx2

ρ y k+1

f (x, y) = Fk,1,λ,µ¯,ρ(x, y) = − 2 + µ¯xy + (k + 1)! .

Clearly

ϕ(x)

=

λ(− 21 x2 + r¯ x ) + (ρkR+k1+)1! ,

therefore

ϕ2λ(x)

=

λ

minu∈R{(u

−

x)2

−

1 2

u2

+

r¯

u

}

+

ρRk+1 (k+1)!

and

ϕ′2λ(x) = 2λ(x − [2x]¯r),

(57)

cf.

(51).

Now,

note

that

g(y)

=

1 (k+1)!

y

k+1

is

k

times

continuously

diﬀerentiable

with

j-th

derivative

g(j)(y) = y k(+k1−+j1si−gnj()y! )j , j ⩽ k. (58)

Whence by the binomial formula we conclude that, for any yˆ ∈ [0, R],

fˆ(x, y) = − λx2 + µ¯xy + ρ yk+1 − (y − yˆ)k+1 .

2

(k + 1)!

From now on, we consider two cases depending on the parity of k (the case of odd k being harder).

Case of even k. In this case we choose µ¯ = µcr, yˆ = R, and observe that the resulting func-

tion fˆ(x, ⋅) is convex on [−R, R]. Indeed, in terms of the rescaled variable z = y R we have

that fˆ(x, y) = h(x, z) with h(x, z) = − 12 λx2 + µ¯Rxz + ρRk+1p(z) and function p ∶ [−1, 1] → R

given by

zk+1 − (z − 1)k+1

p(z) = (k + 1)! .

(59)

29

Let us verify that p is convex on [−1, 1].

Indeed:

on one hand, for z ∈ [−1, 1] one has

z z−1

⩽

1

2,

thus ( z−z1 )k−1 ⩽ 1 using that k − 1 is odd; on the other hand, (z − 1)k−1 ⩽ 0 for z ∈ [−1, 1]. As a

result,

′′

zk−1 − (z − 1)k−1

p (z) = (k − 1)! ⩾ 0, ∀z ∈ [−1, 1].

As such, p(⋅) is convex on [−1, 1]; h(x, ⋅) is convex on [−1, 1] and maximized at an endpoint, so

that

x2

ρRk+1

(2k+1 − 1)ρRk+1

ϕˆ(x) = max h(x, z) = λ
z=±1

−2

+ max

r¯x + λ(k + 1)! , −r¯x +

λ(k + 1)!

x2

(2k − 1)ρRk

= λ − 2 + ¯r x − µ¯(k + 1)!

+ 2kρRk+1 (k + 1)!

and (cf. (52)–(53))

′

(2k − 1)ρRk

(2k − 1)ρRk

ϕˆ2λ(x) = 2λ x − µ¯(k + 1)! − 2x − µ¯(k + 1)! r¯ .

Now, observe that, due to (27), the point

∗ (2k − 1)ρRk x = µ¯(k + 1)!

satisﬁes

∗

ρDk

2r¯

x ⩽ µ¯(k + 1)! ⩽ k + 1 .

As a result, we have [x∗]¯r = 0 and 0 ⩽ [2x∗]¯r ⩽ [2x∗] 3 x∗ = 12 x∗; therefore ϕˆ′2λ(x∗) = 0 and 2

′∗

∗

∗

∗ (2k − 1)λρRk (1 − 2−k)µcrD µcrD

ϕ2λ(x ) = 2λ(x − [2x ]¯r) ⩾ λx = µcr(k + 1)! = k + 1 ⩾ 2k .

Case of odd k. Note that here we have k ⩾ 3 by the premise of the theorem. We choose yˆ =

(1 −

k1 )R,

so

that

fˆ(x, y)

=

h(x, z)

with

h(x, z)

=

−

1 2

λx2

+

µ¯Rxz

+

ρRk+1q(z),

for

z

=

y

R

and

q(z)

given by

q(z) = (k +1 1)! zk+1 − z − 1 − k1 k+1 . (60)

We choose µ¯ as follows:

2λρDk−1

1

1 k−1

µ¯ =

k! 1 − 2k 1 − k

(61)

Since k > 2, we have √12 µcr ⩽ µ¯ ⩽ µcr (cf. (27)). Now, let us show that

x∗ = − 1 − k1 ¯r

is a stationary point for ϕˆ (and thus also for ϕˆ2λ).

30

• We ﬁrst observe that

∂ h(x∗, z) = µ¯Rx∗ + ρRk+1q′(z) = − 1 − 1 µ¯2R2 + ρRk+1 zk − z − 1 − 1

k
.

(62)

∂z

kλ

k!

k

Let us ﬁnd all stationary points z∗ of h(x∗, ⋅) on R. Plugging the expression for µ¯ (cf. (61))

into the right-hand side of (62) and dividing over k1! Rk+1

1

−

1 k

k > 0, we arrive at the equation

wk − (w − 1)k = 2k − 1

(63)

in terms of w = kk−1 z, and guess two solutions: w1∗ = −1 and w2∗ = 2. Moreover, w ↦ wk −(w−1)k is a strictly convex function (to see this, note that k − 2 > 0 is odd, and the function u ↦ uk−2 increases on R modulo the sole ﬁxed point u = 0), therefore (63) has no other (real) solutions.

•

Since

k ⩾ 3,

we

have

1<

k k−1

⩽

32 ,

so

only

one

of

the

two

solutions,

namely

w1∗ = −1,

belongs

to

the range

−

k k−1

,

k k−1

of w = w(z) corresponding to z ∈ [−1, 1]. As such,

z∗ = − 1 − k1

is a unique stationary point of h(x∗, ⋅) on [−1, 1]. Moreover, since k − 1 is even, we have that

∂∂z22 h(x∗, z∗) = (ρkR−k+11)! (z∗)k−1 − z∗ − 1 − k1

k−1 = (1 − 2k−1) 1 − 1 k−1 ρRk+1 < 0, k (k − 1)!

so z∗ is a local maximizer—and hence also a unique global maximizer—of h(x∗, ⋅) on [−1, 1].

• Finally, by a version of Danskin’s theorem (see, e.g., Lemma D.1 in appendix) we have that

ϕˆ′(x∗) = ∂∂x h(x∗, z∗) = µ¯Rz∗ − λx∗ = λ(r¯z∗ − x∗) = 0.

We have just veriﬁed that x∗ is a stationary point for ϕˆ(⋅). On the other hand, we observe that

−ϕ′2λ(x∗) = 2λ([2x∗]¯r − x∗) = 2λ This concludes the proof.

1 − k1 r¯+ −2 1 − k1 r¯ r¯ = 2λkr¯ = µ¯kD > µ2crkD .

C Proofs for Section 5

The proofs of Theorems 5.1 and 5.2 rely on the following technical result extracted from [54].

Lemma C.1 (cf. [54, Proposition 5.5]). For x ∈ X, ξ ∈ Ex, ¯λ > 0 deﬁne functionals SX , SY by

S2X (x, ξ, λ¯) ∶= 2¯λ max

−

⟨ξ,

u

−

x⟩

−

¯λ 2

u−x

2

for x ∈ X, ξ ∈ Ex, ¯λ > 0,

u∈X

(64)

S2Y (y, η, ρ¯) ∶= 2ρ¯max

− ⟨η, v − y⟩ −

ρ¯ 2

v−y

2

for y ∈ Y, η ∈ Ey, ρ¯ > 0.

v∈Y

31

1. Under Assumption 1, for fˆ ≡ fˆ0, at any x ∈ X (and regardless of the choice of yˆ ∈ Y ) we have

∇ϕˆ2λ(x) ⩽ 2SX (x, ∇xf (x, yˆ), λ).

(65)

2. Under Assumptions 1 to 3 with k = 1, for fˆ ≡ fˆ1, at any x ∈ X and y ∈ Y we have

∇ϕˆ2¯λ (x)

⩽

2

⎛ SX

(x,

∇x

fˆ(x,

y),

λ¯1

)

+

λ SY (y, −∇yfˆ(x, y), ρ1) +

⎞ λρ1D .

(66)

1

⎝

ρ1

⎠

Moreover, for y○[= y○(x)] ∈ Argmaxy∈Y fˆ(x, y) we have ∇ϕˆ2¯λ1 (x) ⩽ 2SX (x, ∇xfˆ(x, y○), λ¯1).
Proof. Inequality (66) follows from [54, Proposition 5.5] after recovering the constant factors from the proof (see [54, Appendix A]), bounding the second term in the right-hand side of the resulting inequality (cf. [54, Eq. (5.6)]) via Cauchy-Schwarz, and extracting the square root. For y○ = y○(x) the corresponding term vanishes, and we get ∇ϕˆ2¯λ1 (x) ⩽ 2SX (x, ∇xfˆ(x, y○), λ¯1) directly from [54, Eq. (5.6)]. We obtain (65) by the same argument, as in this case fˆ ≡ fˆ0 is formally maximized at yˆ for any x ∈ X. Note that the argument cannot be extended to k ⩾ 2: in this case fˆk(x, ⋅) is not concave, so [54, Proposition 5.5] cannot be applied anymore.

C.1 Proof of Theorem 5.1
We ﬁrst observe that, with γx = 1 λ, the quantity εt computed in line 6 of Algorithm 1 is nothing else but SX (xt, ∇xfˆ(xt, yˆ), λ). Indeed:

S2X (xt, ∇xf (xt, yˆ), λ) = 2λ max

−

⟨∇xf

(xt,

yˆ),

x

−

xt⟩

−

λ 2

x − xt

2

x∈X

=

1 γ2

max

− 2⟨γx∇xf (xt, yˆ), x − xt⟩ − x − xt 2

(67)

x x∈X

=

∇xf (xt, yˆ)

2−

1 γ2

min

xt − γx∇xf (xt, yˆ) − x 2 = ε2t .

x x∈X

Thus, Algorithm 1 maintains x∗ such that SX (x∗, ∇xf (x∗, yˆ), λ) = min0⩽t⩽T −1 SX (xt, ∇xf (xt, yˆ), λ).

On the other hand, by the descent lemma (cf. Assumption 1), for each t ∈ {0, ..., T − 1} we have

that

f

(xt+1,

yˆ)

⩽

f

(xt,

yˆ)

+

⟨∇xf

(xt,

yˆ),

xt+1

−

xt⟩

+

λ 2

xt+1 − xt

2

= f (xt, yˆ) + min

⟨∇xf

(xt,

yˆ),

x

−

xt⟩

+

λ 2

x − xt

2

x∈X

=

f

(xt,

yˆ)

−

1 2λ

S2X (xt,

∇xf

(xt,

yˆ),

λ),

where the ﬁrst equality follows from the deﬁnition of xt+1. Whence, via telescoping and (29) we get

S2X (x∗, ∇xf (x∗, yˆ), λ) =

min

S2X (xt, ∇xf (xt, yˆ), λ) ⩽

1

T −1
S2X (xt, ∇xf (xt, yˆ), λ)

0⩽t⩽T −1

T t=0

2λ [f (x0, yˆ) − f (xT , yˆ)] 2λ [ϕ(x0) − ψ(yˆ)] ε2

⩽

T

⩽

T

< 144 .

By the ﬁrst claim of Lemma C.1 (cf. (65)) this gives ∇ϕˆ2λ(x∗) ⩽ 2ε∗ ⩽ ε 6. Finally, (30) implies the premise of Theorem 3.1 with k = 0; applying it we arrive at ∇ϕ2λ(x∗) ⩽ ε.

32

C.2 Proof of Theorem 5.2

1o. We ﬁrst consider the case µ ⩽ (¯λ1ρ1)1 2, so that Coupled = 0 and condition (32) reduces to

200µD ⩽ ε.

(68)

As fˆ ∶= fˆ1, line 7 of Algorithm 2 reads yt ∈ Argmaxy∈Y fˆ(xt, y); then the second claim of Lemma C.1 (with yt = y○) guarantees that

∇ϕˆ2¯λ1 (xt) ⩽ 2SX (xt, ∇xfˆ(xt, yt), λ¯1). (69)

Let us now upper-bound SX(xt, ∇xfˆ(xt, yt), λ¯1). To this end, for δ > 0 to be chosen later, consider

δ freg(x, y) ∶= f (x, y) −

y − yˆ 2,

ϕreg(x) ∶= max freg(x, y).

(70)

2

y∈Y

Since freg(x, ⋅) is δ-strongly concave, by Danskin’s theorem (cf. [53, Lemma 24]) ϕreg is diﬀerentiable; ∇ϕreg(x) = ∇xf (x, yreg(x)) with yreg(x) ∶= argmaxy∈Y freg(x, y), and is λreg-Lipschitz with λreg ∶= λ + µδ2 . Now, let us choose δ = µ¯λ12 , so that λreg = λ + ¯λ1 ∈ [¯λ1, 2¯λ1]. By the descent lemma

ϕreg(xt+1) − ϕreg(xt)

⩽ ⟨∇ϕreg(xt), xt+1 − xt⟩ + λ2reg xt+1 − xt 2 = ⟨∇xf (xt, yreg(xt)), xt+1 − xt⟩ + λ2reg xt+1 − xt ⩽ ⟨∇xf (xt, yt), xt+1 − xt⟩ + 3λ4reg xt+1 − xt 2 + λ1reg ∇xf (xt, yreg(xt)) − ∇xf (xt, yt) 2

⩽ ⟨∇xf (xt, yt), xt+1 − xt⟩ + 3λreg xt+1 − xt 2 + µ2D2

4

λreg

⩽ ⟨∇xf (xt, yt), xt+1 − xt⟩ + 1 3¯λ1 + µ2

2

ρ1

xt+1 − xt 2 + µ2D2 ¯λ1

= min ⟨∇xf (xt, yt), x − xt⟩ + 1 3¯λ1 + µ2

x∈X

2

ρ1

x − xt 2 + µ2D2 ¯λ1

= − 6¯λ1 + 12µ2 ρ1 S2X xt, ∇xf (xt, yt), 3¯λ1 + µρ12 + µλ¯2D1 2

⩽ − 8¯λ11 S2X xt, ∇xf (xt, yt), 3¯λ1 + µρ12 + µ¯λ2D1 2 .

2
(71)

Here the second inequality is by Cauchy-Schwarz, the next one is via Assumption 1, and the subsequent identities are by the deﬁnitions of xt+1 and γx (cf. line 10 of Algorithm 2). (Note that the factor µ2 ρ1 can be upper-bounded with ¯λ1 by the standing assumption, but we avoid this in order for our estimates to have a similar form as in the strongly coupled case (cf. 2o) which is yet to be considered.) We now proceed as in (67): by telescoping (71) we get

S2X x∗, ∇xf (x∗, y∗), 3¯λ1 + µ2 = min S2X xt, ∇xf (xt, yt), 3¯λ1 + µ2

ρ1 0⩽t⩽T −1

ρ1

⩽

1

T −1
S2X

T t=0

xt, ∇xf (xt, yt), 3¯λ1 + µ2 ρ1

⩽ 8¯λ1[ϕreg(x0) − ϕreg(xT )] + 8µ2D2 T

⩽ 8¯λ1[ϕ(x0) − ϕ(xT )] + 4¯λ1δD2 + 8µ2D2 ⩽ 8¯λ1[ϕ(x0) − ϕ(xT )] + 12µ2D2.

(72)

T

T

T

33

As a result,

S2X (x∗, ∇xfˆ(x∗, y∗), λ¯1) ⩽ S2X (x∗, ∇xfˆ(x∗, y∗), 4¯λ1)

= 8¯λ1 max −⟨∇xfˆ(x∗, y∗), x − x∗⟩ − 2¯λ1 x − x∗ 2
x∈X

⩽ 8¯λ1 max −⟨∇xf (x∗, y∗), x − x∗⟩ − 3¯λ1 x − x∗ 2

x∈X

2

+ 4 ∇xf (x∗, y∗) − ∇xfˆ(x∗, y∗) 2

= 34 S2X (x∗, ∇xf (x∗, y∗), 3¯λ1) + 4 ∇xf (x∗, y∗) − ∇xfˆ(x∗, y∗) 2

⩽ 34 S2X (x∗, ∇xf (x∗, y∗), 3¯λ1) + 16µ2D2

⩽ 32¯λ1[ϕ(x0) − ϕ(xT )] + 32µ2D2 ⩽ 11ε2 1 + 3 < ε2 .

3T

2100 40000 144

Here the ﬁrst estimate relies on the proximal Polyak-Lojasiewicz (PL) inequality that ensures that SX(x, ξ, λ¯) is non-decreasing in λ¯ (cf. [93, Lemma 1]); the second one is by Cauchy-Schwarz,
the third one is by Lemma 3.2, the fourth one is by (72), and the last one is by (33)–(68). Finally, returning to (69) we get ∇ϕˆ2¯λ1 (x∗) ⩽ ε 6, and by Theorem 3.1 this results in ∇ϕ2¯λ1 (x∗) ⩽ ε.
2o. We now consider the case µ ⩾ (¯λ1ρ1)1 2, where Algorithm 2 is run with Coupled = 1. By (32),

200 ¯λ1ρ1D ⩽ ε.

(73)

Casting

the

update

in

line

4

of

Algorithm

2

as

yt

=

argmaxy∈Y

{⟨∇yf (xt,

yˆ),

y

−

yˆ⟩

−

ρ1 2

y − yˆ

2} gives

⟨∇yf (xt, yˆ) − ρ1(yt − yˆ), y − yt⟩ ⩽ 0, ∀y ∈ Y,

from to the ﬁrst-order optimality condition. As a result, for any 0 ⩽ t ⩽ T − 1 we have

S2Y (yt, −∇yfˆ(xt, yt), ρ1) = S2Y (yt, −∇yf (xt, yˆ), ρ1) = 2ρ1 max
y∈Y
⩽ 2ρ21 max
y∈Y

⟨∇yf (xt, yˆ), y − yt⟩ − ρ1 y − yt 2 2
− 12 y − yt 2 + ⟨yt − yˆ, y − yt⟩ ⩽ ρ21D2.

where in the ﬁnal step we maximized over the whole Ey. Whence by Lemma C.1 (cf. (66)) we get

∇ϕˆ2¯λ1 (xt) ⩽ 2SX (xt, ∇xfˆ(xt, yt), λ¯1) + 4 λρ1D. (74)

Our next goal is to estimate SX (xt, ∇xfˆ(xt, yt), λ¯1) via a telescoping argument. To this end, let

f̃reg(x, y) ∶= fˆ1(x, y) − ρ1 y − yˆ 2, ϕ̃reg(x) ∶= max f̃reg(x, y).

(75)

2

y∈Y

Recall that ∇xfˆ1(⋅, y) is λ¯1-Lipschitz for any y ∈ Y (cf. Lemma 3.3); moreover, f̃reg(x, ⋅) is ρ1strongly concave. Therefore by Danskin’s theorem (cf. [53, Lemma 24]), ∇ϕ̃reg(x) = ∇xfˆ(x, ỹreg(x)) with ỹreg(x) ∶= argmaxy∈Y f̃reg(x, y), and is ̃λreg-Lipschitz with ̃λreg ∶= ¯λ1 + µ2 ρ1. Moreover, we have

yt = ỹreg(xt),

34

as seen by looking at line 4 of Algorithm 2 again. Whence, by the descent lemma (cf. (71)) we get

ϕ̃reg(xt+1) − ϕ̃reg(xt) ⩽ ⟨∇ϕ̃reg(xt), xt+1 − xt⟩ + ̃λreg xt+1 − xt 2 2

= ⟨∇xfˆ(xt, yt), xt+1 − xt⟩ + ̃λreg xt+1 − xt 2 2

⩽ ⟨∇xfˆ(xt, yt), xt+1 − xt⟩ + 1 3¯λ1 + µ2

2

ρ1

xt+1 − xt 2

= min ⟨∇xfˆ(xt, yt), x − xt⟩ + 1 3¯λ1 + µ2

x∈X

2

ρ1

x − xt 2

⩽ − 6¯λ1 + 12µ2 ρ1 S2X xt, ∇xfˆ(xt, yt), 3¯λ1 + µρ12 .

Here the penultimate line follows by recasting the update in line 5 of Algorithm 2 as

̃xt+1 = xt − γx∇xf (xt, yˆ) − γx∇2xyf (xt, yˆ)(yt − yˆ) = xt − γx∇xfˆ(xt, yt).

Moreover, for the same reason we have (cf. (67))

ε2t = ∇xfˆ(xt, yt) 2 − 1 min xt − γx∇xfˆ(xt, yt) − x 2 = S2X xt, ∇xfˆ(xt, yt), 3¯λ1 + µ2 .

(76)

γx2 x∈X

ρ1

Whence, proceeding as in (72) we get

S2X x∗, ∇xfˆ(x∗, y∗), 3¯λ1 + µ2 = min S2X xt, ∇xfˆ(xt, yt), 3¯λ1 + µ2

ρ1 0⩽t⩽T −1

ρ1

⩽

1

T −1
S2X

xt, ∇xfˆ(xt, yt), 3¯λ1 + µ2

T t=0

ρ1

⩽ 6¯λ1 + 2µ2 ϕ̃reg(x0) − ϕ̃reg(xT )

ρ1

T

(i) 2 ⩽T

3¯λ1 + µ2 ρ1

ϕˆ(x0) − ϕˆ(xT ) + ρ1D2 2

(ii) 2 ⩽T

3¯λ1 + µ2 ρ1

ϕ(x0) − ϕ(xT ) + 3ρ1D2 . 2

Here

for

(i)

we

used

the

two

inequalities

ϕ̃reg (x0 ) − ϕ̃reg (xT

)

⩽

ϕˆ(x0)−maxy∈Y

{fˆ1(xT

,

y)−

ρ1 2

y −yˆ 2}

and

ϕˆ(xT

)

⩽

maxy∈Y

{fˆ1(xT

,

y)

−

ρ1 2

y − yˆ 2} + ρ12D2

(cf. (75)); for (ii) we applied Lemma 3.1 to the

35

right-hand side of ϕˆ(x) − ϕ(x) ⩽ maxy∈Y f (x, y) − fˆ1(x, y) . Returning to (74) we now obtain that

∇ϕˆ2¯λ (x∗) ⩽ 2SX xt, ∇xfˆ(xt, yt), 3¯λ1 + µ2 + 4 λρ1D

1

ρ1

⩽ T8 3¯λ1 + µρ12 ϕ(x0) − ϕ(xT ) + 3ρ12D2 + 4 λρ1D

(7⩽3) T8 3¯λ1 + µρ12 ϕ(x0) − ϕ(xT ) + 80030ε02¯λ1 + 5ε0

8 ⩽T

(33) ⎛

⩽

ε ⎝

3¯λ1 + µ2 (ϕ(x0) − ϕ(xT )) + 3ε

ρ1

100

8 2√3 1 ⎞ ε

700 + 100 + 50 ⎠ < 6 .

1

µ2

ε

T 1 + 3¯λ1ρ1 + 50

Finally, by applying Theorem 3.1 we conclude that ∇ϕ2¯λ1 (x∗) ⩽ ε as required.

C.3 Proof of Proposition 5.1

We ﬁrst observe that fˆ(⋅, y) ≡ fˆ2(⋅, y) is (σ0 + σ2D2)-Lipschitz for any y ∈ Y as can be seen from

∇xfˆ(x, y) ⩽ ∇xf (x, y) + ∇xfˆ(x, y) − ∇xf (x, y) ⩽ σ0 + σ2D2,

(77)

where the last step is by Lemma 3.2. Moreover, by Lemma 3.3 ∇xfˆ(⋅, y) is λ¯2-Lipschitz, and thus ϕˆ is ¯λ2-weakly convex. These two observations allow us to adapt the analysis of the projected

subgradient method from [61, Theorem 31] (initially carried out in [62]) to Algorithm 3 in order to establish that x∗ is an approximate FOSP for (P2) – and after that allude to Theorem 3.1.
For brevity, let fˆ ≡ fˆ2 and ϕˆ = maxy∈Y fˆ(x, y). Observe that, for any x ∈ X and iterate (xt, yt),

ϕˆ(x) ⩾ fˆ(x, yt) ⩾ fˆ(xt, yt) + ⟨∇xfˆ(xt, yt), x − xt⟩ − λ¯2 x − xt 2 2

⩾ ϕˆ(xt) − δ + ⟨∇xfˆ(xt, yt), x − xt⟩ − ¯λ2 x − xt 2,

(78)

2

where the last step is by deﬁnition of ApproxMax. Moreover, let x+t = argminx∈X{ϕˆ(x)+ ¯λ2 x − xt 2} be the proximal mapping of xt, so that (cf. (9))

∇ϕˆ2¯λ2 (xt) = 2¯λ2(xt − x+t ).

(79)

1o. Let Algorithm 3 be run with Naive = 0, i.e., xt+1 = ΠX [xt − γx∇xfˆ(xt, yt)] (cf. line 6). Then

ϕˆ2¯λ2 (xt+1) ⩽ ϕˆ(x+t ) + ¯λ2 xt+1 − x+t 2

⩽ ϕˆ(x+t ) + ¯λ2 xt − γx∇xfˆ(xt, yt) − x+t 2

(77)
⩽

ϕˆ(x+t )

+

¯λ2

xt − x+t

2 + 2γx¯λ2⟨∇xfˆ(xt, yt), x+t − xt⟩ + γx2λ¯2(σ0 + σ2D2)2

(80)

= ϕˆ2¯λ2 (xt) + 2γx¯λ2⟨∇xfˆ(xt, yt), x+t − xt⟩ + γx2λ¯2(σ0 + σ2D2)2

(78)
⩽ ϕˆ2¯λ

(xt) + 2γx¯λ2

ϕˆ(x+t ) − ϕˆ(xt) + δ + ¯λ2 x+t − xt 2

+ γx2¯λ2(σ0 + σ2D2)2,

2

2

36

where the second line relies on the projection lemma. Repeating this for t ∈ {0, ..., T − 1} we get

ϕˆ2¯λ2 (xT ) ⩽ ϕˆ2¯λ2 (x0) + 2γx¯λ2 Tt=−01 ϕˆ(x+t ) − ϕˆ(xt) + δ + ¯λ22 x+t − xt 2 + γx2T ¯λ2(σ0 + σ2D2)2,

whence, by rearranging and dividing over 2γx¯λ2T ,

T1 Tt=−01 ϕˆ(xt) − ϕˆ(x+t ) − ¯λ22 x+t − xt 2 ⩽ ϕˆ2¯λ2 (x20γ)x−¯λϕ2ˆT2¯λ2 (xT ) + γx(σ0 +2σ2D2)2 + δ. (81)

On the other hand, by λ¯2-strong convexity of ϕˆ(⋅) + λ¯2 ⋅ −xt 2 and the deﬁnition of x+t we get

ϕˆ(xt) − ϕˆ(x+t ) − λ¯2 x+t − xt 2 = ϕˆ(xt) + λ¯2 2 = ϕˆ(xt) + λ¯2 ⩾ ¯λ2 x+t − xt
Plugging this into (81) we arrive at

xt − xt 2 − (ϕˆ(x+t ) + λ¯2 x+t − xt 2) + λ¯22 x+t − xt 2

xt − xt 2 − min ϕˆ(x+) + ¯λ2 x+ − x 2 + ¯λ2 x+ − x 2

x∈X

t

tt

2t t

2 (7=9)

∇ϕˆ2¯λ (xt) 2

2
4¯λ2

.

(82)

E[ ∇ϕˆ2¯ (xs) 2] = 1 T −1 ∇ϕˆ2¯ (xt) 2 ⩽ 2¯λ2 ϕˆ2¯λ2 (x0) − ϕˆ2¯λ2 (xT ) + 2¯λ2γx(σ0 + σ2D2)2 + 4¯λ2δ

λ2

T t=0

λ2

γxT

(a)
⩽

2¯λ2

[ϕˆ(x0)

−

ϕˆ(xT )]

+

2¯λ2γx(σ0

+

σ2D2)2

+

4¯λ2δ

γxT

(b)
⩽

2¯λ2(∆

+

ρ2D3)

+

2¯λ2γx(σ0

+

σ2D2)2

+

4¯λ2δ;

(83)

γxT

here for (a) we used that ϕˆ2¯λ2(x0) = minx∈X {ϕˆ(x)+λ¯2 x−x0 2} ⩽ ϕˆ(x0); for (b) we used Lemma 3.1. Whence by Markov’s inequality, for any p ∈ (0, 1) with probability at least 1 − p one has

∇ϕˆ2¯λ (xs) 2 ⩽ 4 ¯λ2(∆ + ρ2D3) + γx¯λ2(σ0 + σ2D2)2 + 2¯λ2δ

2

p

γxT

⩽

8

⎛ (σ0

+

σ2D2)

p⎝

¯λ2(∆ + ρ2D3) + ¯λ2δ⎞ <

ε2 ,

T

⎠ 150

where for the second line we substituted γx from (35) (note that this choice of γx balances the two terms) and then performed an explicit calculation by plugging in T and δ from (35). Finally, under
this event (34) allows to apply Theorem 3.1 (replacing ε with ε 6) and conclude that ∇ϕ2¯λ2 (xs) ⩽ ε.
2o. Conforming with the second claim we intend to prove, from now on we shall assume that

24σ2D2 ⩽ ε√p,

(84)

37

and consider the iterates of Algorithm 3 with Naive = 1—i.e., xt+1 = ΠX [xt−γx∇xf (xt, yt)] (cf. line 4). First of all, let us correct (80) for the discrepancy between ∇xf (xt, yt) and ∇xfˆ(xt, yt): to this end,

ϕˆ2¯λ2 (xt+1) ⩽ ϕˆ(x+t ) + ¯λ2 xt+1 − x+t 2

(a)
⩽

ϕˆ(x+t )

+

¯λ2

xt − γx∇xf (xt, yt) − x+t

2

(b)
⩽

ϕˆ(x+t )

+

λ¯2

x+t − xt 2 + 2γx⟨∇xf (xt, yt), x+t − xt⟩ + γx2σ02

= ϕˆ2¯λ2 (xt) + 2¯λ2γx⟨∇xf (xt, yt), x+t − xt⟩ + ¯λ2γx2σ02

(c)
⩽ ϕˆ2¯λ

(xt) + 2¯λ2γx

⟨∇xfˆ(xt, yt), x+t − xt⟩ + σ2D2 x+t − xt

+ λ¯2γx2σ02

2

(d)
⩽ ϕˆ2¯λ

(xt) + 2γxλ¯2

2

⟨∇xfˆ(xt, yt), x+ − x ⟩ + ¯λ2 x+ − x 2 + σ22D4

t t 2t t

2¯λ2

+ λ¯2γx2σ02

(78)
⩽ ϕˆ2¯λ

(xt) + 2γx¯λ2

2

ϕˆ(x+) − ϕˆ(x ) + δ + λ¯2 x+ − x 2 + σ22D4

t

t

tt

2¯λ2

+ ¯λ2γx2σ02.

Here (a) is by the projection lemma; (b) by the Lipschitz assumption; (c) by Cauchy-Schwarz and Lemma 3.2; (d) by uv ⩽ (u2 + v2) 2 for u, v ∈ R. Whence we arrive at a counterpart of (81):

T1 Tt=−01 ϕˆ(xt) − ϕˆ(x+t ) − λ¯2 x+t − xt 2 ⩽ ϕˆ2¯λ2 (x20γ)x−¯λϕ2ˆT2¯λ2 (xT ) + γx2σ02 + δ + σ222¯λD24 . (85)

On the other hand, in the same spirit as for (82) we get

ϕˆ(xt) − ϕˆ(x+t ) − λ¯2 x+t − xt 2 = ϕˆ(xt) + ¯λ2 xt − xt 2 − (ϕˆ(x+t ) + λ¯2 x+t − xt 2)

= ϕˆ(xt) + ¯λ2 xt − xt 2 − min ϕˆ(x+t ) + ¯λ2 x+t − xt 2

x∈X

(86)

⩾ ¯λ22 x+t − xt 2 (7=9) ∇ϕˆ28¯λ¯λ2 (2xt) 2 .

Plugging (86) into (85) and proceeding in the same way as in (83), we conclude that w.p. ⩾ 1 − p,

∇ϕˆ2¯λ2 (xs) 2 ⩽ p8

¯λ2(∆ + ρ2D3) + γx¯λ2σ2 + 2¯λ2δ + σ2D4

γxT

0

2

16 ⎛ ⩽ p ⎝σ0

¯λ2(∆ + ρ2D3)

+ λ¯2δ⎞ +

8σ22D4

(84)
⩽

ε2

+

ε2

<

ε2 .

T

⎠p

75 72 36

In order to verify the claim, it remains to apply Theorem 3.1.

D Background on weak convexity and the Moreau envelope notion
D.1 Characterization of weakly convex functions
Let Ex be a ﬁnite-dimensional Euclidean space with inner product ⟨⋅, ⋅⟩ and norm x = ⟨x, x⟩. Moreover, we identify with Ex with its dual space Ex∗ (in particular ⋅ ∗ = ⋅ ). Also we let R¯ = (∞, +∞] and tacitly assume that all arising convex functions are lsc and proper (not identically +∞).

38

Recall that any such function φ ∶ Ex → R¯ admits a variational representation as the pointwise
supremum over a non-empty family of aﬃne functions, and this is in fact a criterion (see, e.g., [99,
Thm. 8.13]). The set of all aﬃne minorants of φ at given x ∈ Ex generates the corresponding set of linear functionals, called the subdiﬀerential of φ at x and denoted ∂φ(x)—i.e.,

∂φ(x) ∶= {ξ ∈ Ex ∶ φ(x′) ⩾ φ(x) + ⟨ξ, x′ − x⟩ ∀x′ ∈ Ex}.

(87)

This set is non-empty at any interior point of eﬀective domain dom(φ) ∶= {x ∈ Ex ∶ φ(x) < +∞}, and its elements are called subgradients of φ at x. (For simplicity, we assume int(dom(φ)) is non-empty.)

Deﬁnition

3.

φ

∶

Ex

→

R¯

is

called

λ-weakly

convex

(for

λ

⩾

0)

if

the

function

φ(⋅) +

λ 2

⋅

2 is

convex.

From the above characterization of (proper, lsc) convex functions in terms of subdiﬀerentials it

immediately

follows

that

φ

is

λ-weakly

convex

if

and

only

if

the

function

φλ,x(⋅)

∶=

φ(⋅)

+

λ 2

⋅ −x 2

is convex for any x ∈ Ex. Indeed, by Deﬁnition 3 φλ,0 is convex; meanwhile, for any x ∈ Ex we have

φλ,x(u)

=

φλ,0(u)

+

λ 2

u−x 2− u 2 .

By applying (87) to φλ,0 we see that φλ,x has subdiﬀerential ∂φλ,x(u) = ∂φλ,0(u)−λx (the diﬀerence being in the Minkowski sense), and thus is convex with dom(φλ,x) = dom(φ). It is then natural to deﬁne the weak subdiﬀerential of φ at x by

∂φ(x) = ∂φλ,x(x).

Equivalently, ∂φ(u) = ∂φλ,x(u) + λ(x − u) for any u, x ∈ Ex—or, in other words,

∂φ(x) = {ξ ∈ Ex ∶

φ(x′)

⩾

φ(x)

+

⟨ξ,

x′

−

x⟩

−

λ 2

x′ − x 2,

∀x, x′ ∈ Ex}.

(88)

Thus, ∂φ(x) belongs to the Fr´echet (local) subdiﬀerential at x, deﬁned as the set of ξ ∈ Ex satisfying

φ(x′) ⩾ φ(x) + ⟨ξ, x′ − x⟩ + o( x′ − x ) as x′ → x.

(89)

cf. [99, Def. 8.3]. Generally, the property in (89) is much weaker than that in (88); however, for weakly convex functions the weak and Fr´echet subdiﬀerentials coincide (see [99, Theorem 12.17]).

Hessian-based criterion. Recall Alexandrov’s theorem ([100], see also [101, Thm. 2.1]): a convex φ ∶ Ex → R¯ is twice diﬀerentiable almost everywhere (and hence ∇2φ ≽ 0) in the interior of its domain. Under C1 regularity it is a criterion: if φ ∶ Ex → R¯ is continuously diﬀerentiable on int(dom(φ)), and ∇2φ ≽ 0 almost everywhere on int(dom(φ)), then φ is convex. From Definition 3 we see that this criterion extends to λ-weakly convex functions if we replace ∇2φ ≽ 0 with ∇2φ ≽ −λI.
D.2 Upper envelope as a weakly convex function
Let X ⊆ Ex be convex with non-empty interior. If φ → X is λ-smooth, i.e., such that ∇φ exists and
∇φ(x′) − ∇φ(x) ⩽ λ x′ − x ∀x, x′ ∈ X,

39

then φ is λ-weakly convex. (This follows from the Hessian-vased criterion above, as ∇φ(x) exists

almost everywhere on X and its eigenvalues are in [−λ, λ].) More generally, the upper envelope

of a family of such functions is λ-weakly convex. That is, given f ∶ X × Y (in the setup of (P))

satisfying Assumption 1 with λ < ∞, the primal function ϕ(x) ∶= maxy∈Y f (x, y) is λ-weakly convex.

Indeed,

note

that

fλ,x′(⋅, y)

=

f (⋅, y)

+

λ 2

⋅ −x′ 2 is convex for any y ∈ Y

(cf. Deﬁnition 3), therefore

ϕ(x)

+

λ 2

x − x′

2 = max fλ,x′ (x, y)

y∈Y

is convex, i.e., ϕ is λ-weakly convex. Moreover, ∂ϕ(x) can be expressed explicitly through ∇xf (x, y).

Lemma D.1. Let f ∶ X × Y → R be continuous and satisfy Assumption 1 with λ < ∞, X be convex, and Y be convex and compact. Then ∂ϕ(x) is the closed convex hull of the set of active gradients:

∂ϕ(x) = conv{∇xf (x, y∗), ∀y∗ ∈ Argmaxy∈Y f (x, y)}.

Proof. We apply Danskin’s theorem ([102, Sec. B.5]) to the function ϕλ,x′(x) = maxy∈Y fλ,x′(x, y),
∂ϕλ,x′ (x) = conv{∇xfλ,x′(x, y∗), y∗ ∈ Argmaxy∈Y fλ,x′(x, y)}, = conv{∇xf (x, y∗) + λ(x − x′), y∗ ∈ Argmaxy∈Y f (x, y)}.

This holds because each fλ,x′(⋅, y) is convex, and the set of maximizers for fλ,x′(x, y) (over y ∈ Y ) is the same as for f (x, y). It remains to note that ∂ϕ(x) = ∂ϕλ,x′(x) − λ(x − x′) by Deﬁnition 3.

D.3 Characterization of (ε, 2λ)-FOSP in terms of the primal function
The following property of (ε, 2λ)-FOSP for (P) has been leveraged in the proof Proposition 3.2. It extends [91, Lem. 2.2] to the case X ≠ Ex with guarantee (90) in terms of ∂ϕ rather than ∂(ϕ + IX ). Proposition D.1 ([54, Proposition 5.1]). Let φ ∶ X → R be λ-weakly convex, then it holds that

∇φ2λ(x) = 2λ(x − x+(x)), where x+(x) = x+1 φ(x) for any x ∈ X. 2λ

where φ2λ is the 2λ-Moreau envelope of φ (cf. Deﬁnition 1). Moreover, if ∇φ2λ(x) ⩽ ε, then

min SX (x+, ξ, 2λ) ⩽ ε,

(90)

ξ∈∂φ(x+)

where we deﬁne the functional S2X (x, ξ, λ¯) ∶= 2¯λ maxu∈X

−

⟨ξ,

u

−

x⟩

−

¯λ 2

u−x

2

for x, ξ ∈ Ex, λ¯ > 0.

E Solving (40): quadratic optimization on a joint Krylov subspace
Following [92, Appendix A], in this section we describe a procedure for maximizing a general quadratic function ΨH,g(y), cf. (36), over the intersection of Bd(2R) and the joint Krylov subspace
K2m = K2m(H, {g, ξ}) ∶= span {Hj g, Hj ξ}j∈{0,...,m−1}
in O(m) computations of the matrix-vector product y ↦ Hy and elementwise vector operations on Ey (including the inner product). This procedure is given in pseudocode in Algorithm 4. It consists of two steps which we are now about to outline and discuss.

40

Finding the right basis for K2m. In the ﬁrst stage of the algorithm (up to line 16) we construct

an orthonormal basis of K2m in which the associated linear operator (corresponding to H in the initial basis), when restricted to K2m, is represented by a block tridiagonal matrix H̃ with blocks of size 2 (which pertains to the two vectors {g, ξ} generating K2m). In other words, we construct H̃ ∈ R2m×2m and Q ∈ Rd×2m such that H̃ = Q⊺HQ, where Q has orthonormal columns (i.e., Q⊺Q = I2m)

and

⎡⎢α1 β2⊺

⎤⎥

H̃ = ⎢⎢⎢⎢⎢β2 ⋱⋱ ⋱⋱ βm⊺ ⎥⎥⎥⎥⎥ ,

⎢⎣

βm αm⎥⎦

where each αi ∈ R2×2 is symmetric and each βi ∈ R2×2 is upper-triangular (so H̃ is pentadiagonal). In the large-scale context, where H is accessed through the y ↦ Hy oracle, the one can construct
such a decomposition in a computationally feasible manner via the block Lanczos process [103, 104]:

1. Form q1 ∈ Rd×2 as the Gram-Schmidt orthonormalization of {g, ξ}. 2. Compute α1 = q1⊺Hq1 and put β1 = 02×2. (We let 0a×b be the a × b zero matrix.)

3. For t ∈ {1, m − 1} iterate:

qt′+1 = Hqt − qtαt − qt−1βt⊺, (qt+1, βt+1) = QR(qt′+1),
αt+1 = qt⊺+1Hqt+1.

Here QR(A) is the QR decomposition of a squared matrix A—i.e., the ordered pair (U, R) of square
matrices such that A = U R, U is orthogonal, and R is upper-triangular. To see that the above process is sound, observe that it incrementally builds up qt, αt, βt that satisfy the matrix relations

⎡⎢⎢α1 β2⊺

⎤⎥⎥

⎡⎢⎢ 0 ⎤⎥⎥

⎢⎢β2 ⋱ ⋱ ⎥⎥ H[q1 ⋯ qt] = [q1 ⋯ qt qt+1] ⎢⎢ ⋱ ⋱ β⊺ ⎥⎥ for t ∈ {1, ..., m − 1},

and

Hqm = [q1 ⋯ qm] ⎢⎢⎢ 0⋮ ⎥⎥⎥

⎢⎢⎢ βt αtt ⎥⎥⎥

⎢⎢⎢βm⊺ ⎥⎥⎥

⎢⎣

βt+1 ⎥⎦

⎢⎣αm⎥⎦

while ensuring that βt’s are upper-triangular and qt’s form an orthonormal system. These relations amount to the identity HQ = QH̃ with Q = [q1 ⋯ qm]—and thus to Q⊺HQ = H̃ as desired. The ﬁrst part of Algorithm 4 (until line 16) implements the block Lanczos process with an explicitly
rendered QR factorization step.

Passing to the equivalent problem and solving it. Once H̃ and Q have been constructed, we compute ̃g = Q⊺g and the top eigenvalue ω0 of H̃ (cf. line 18). The ﬁrst of these two steps amounts to O(m) inner products, and the second one can be done in O(m) arithmetic operations since H̃ is pentadiagonal. Using these data, we recast the initial maximization problem (cf. (40))
by changing the variable y to z = Q⊺y ∈ R2m—i.e., we pass to

max

1 2

z⊺H̃

z

+

̃g⊺z.

z∈B2m(2R)

(91)

41

The new problem is equivalent to (40) in the following sense: if z is δ-suboptimal in (91), then y = Qz is δ-suboptimal in (40). Thus we can focus on (91) and then recover a candidate solution to (40) by multiplying with Q (which has essentially the same computational cost as forming ̃g = Q⊺g from g).
The ﬁnal part of Algorithm 4 (after line 18) consists of solving (91). Here we proceed as follows.
1. We ﬁrst test if H̃ is negative-semideﬁnite (by inspecting sign(ω0)) and, if yes, whether B2m(2R) contains an unconstrained maximizer z0—an optimal solution to the linear system H̃z + ̃g = 0. If the answers to both questions are positive, we in fact found a maximizer on B2m(2R), so we output Qz0 and terminate. Note that here if H̃ is negative-deﬁnite, then z0 exists, is unique, and is given by −H̃−1̃g; otherwise (i.e., when ω0 = 0), H̃ is rank-deﬁcient, so H̃z + ̃g = 0 is solvable whenever ̃g is in the range of H̃, and then the least-norm solution is given by z0 = −H̃ †̃g, where H̃† is the generalized inverse of H̃ (i.e., H̃† has the same eigenspaces as H̃ and reciprocal nonzero eigenvalues). Note that H̃ † can be computed in O(m) arithmetic operations
(a.o.).

2. If the above test failed (and thus we have not terminated), then ΨH̃,̃g has a unique maximizer at the boundary of B2m(2R), and this maximizer also solves the strictly concave problem

zω =

argmax

1 2

z⊺(H̃

−

ωI )z

+

̃g⊺z

z∈B2m(2R)

for the unique value of ω > max{ω, 0} such that zω = R (see, e.g., [105, Corollary 7.2.2]). Thus, we can proceed by a root-ﬁnding method of choice: evaluation of the mapping ω ↦ zω amounts to solving a well-deﬁned linear system with a pentadiagonal matrix and thus can
be done in O(m) a.o.; thus, we ﬁnd a high-accuracy maximizer—exact one in ﬂoating point
arithmetic—in O(m) a.o. In particular, a good option is practice is to seek for the root
of ψ(ω) − 1 R with ψ(ω) = 1 zω by Newton’s method as discussed in [105, Chapter 7].

Time and memory expenses. By carefully inspecting Algorithm 4 one may verify that the Lanczos process takes O(1) matrix-vector products and elementwise vector operations in Ey per iteration, so O(m) such operations in total. Computation of ̃g takes O(m) inner products, and those of ω0, H̃ †, and (H̃ − ωI)−1 each take O(m) a.o. Finally, as we previously discussed, a root ﬁnder performs O(1) evaluations of ω in the ﬂoating-point model of computation. Hence, the total computational cost of Algorithm 4 in a.o. is

O(m(dy + time(HVP))),

(92)

where time(HVP) is the cost of computing y ↦ Hy, and dy = dim(Ey). As for the memory cost, it is dominated by

O(mdy + mem(HVP)),

(93)

where the ﬁrst term allows to explicitly store Q (which we need in order to report the ﬁnal solution
Qz0 or Qzω), and the second term is the memory needed for computing y ↦ Hy (i.e., mem(HVP) ≲ d2y or less if H has special structure). Note also that we can replace the term mdy in (93) with m+dy = O(dy) if Algorithm 4 in the situation where there is no need to report the solution in Ey (e.g., if only
the optimal value is of interest). Indeed, apart from reporting the ﬁnal solution, we only need Q when computing ̃g = Q⊺g = [q1⊺g; ...; qm⊺ g]—but this can be done incrementally during the Lanczos process, without ever having to memorize more than O(1) columns of Q at once.

42

Algorithm

4

Approximate

maximization

of

a

quadratic

ΨH,g(y) =

1 2

y⊺Hy

+

g⊺y

on

a

ball

Y

=

Bd(2R)

Input: g ∈ Rd, oracle y ↦ Hy, R > 0, m ∈ N 1: ▷ Restrict search to the joint Krylov subspace K2m(H, {g, ξ}), cf. (38), by using Proposition 5.2.

2: ▷ Blockwise Lanczos iterations for K2m = K2m(H, {g, ξ}) with blocks of size 2 ([92, Appendix A]); results in a block tridiagonal matrix H̃ = Q⊺HQ ∈ R2m×2m with Q ∈ Rd×2m: Q⊺Q = I.
3: ▷ qt ∈ Rd×2, αt, βt ∈ R2×2

4: q0 = 0d×2 5: Draw ξ ∼ Uniform(Sd−1); 6: q1 = [u, v]; α1 = q1⊺Hq1

u=

1 g

g;

w = ξ − ⟨ξ, u⟩u;

▷ d × 2 zero matrix v = w1 w
▷ q1 is an orthonormalization of [g, ξ]

7: for t ∈ {1, ..., m − 1} do

8: qt′+1 = Hqt − qtαt − qt−1βt⊺

9: ▷ Compute (qt+1, βt+1) = QR(qt′+1) via Gram-Schmidt:

10: [u′, v′] = qt′+1

11:

u=

1 u′

u′;

w = v′ − ⟨v′, u⟩ u;

v = w1 w

▷ β1 never used since q0 = 0 ▷ extract columns from qt′+1

⟨u′, u⟩ ⟨v′, u⟩ 12: qt+1 = [u, v]; βt+1 = 0 ⟨v′, v⟩

▷ qt+1 is an orthonormalization of qt′+1 = qt+1βt+1

13: αt+1 = qt⊺+1Hqt+1

14: end for

15: Q = q1 ⋯ qm

⎡⎢α1 β2⊺

⎤⎥

16: H̃ = ⎢⎢⎢⎢⎢β2 ⋱⋱ ⋱⋱ βm⊺ ⎥⎥⎥⎥⎥

⎢⎣

βm αm⎥⎦

17: ̃g = Q⊺g 18: ω0 = λmax(H̃ )

▷ columns of Q form an orthonormal basis for K2m ▷ H̃ = Q⊺HQ ∈ R2m×2m is block tridiagonal with 2 × 2 blocks

19: ▷ Eliminate the interior case: ΨH,g restricted to K2m is concave and maximized inside the ball:

20: if ω0 < 0 then

▷ H̃ is full-rank, so the unconstrained maximizer z0 exists and is unique

21: z0 = −H̃ −1̃g

22: end if

23: if ω0 == 0 and H̃(H̃2)†H̃̃g == ̃g then 24: z0 = −H̃ †̃g

▷ H̃ is rank-deﬁcient, but ̃g is in its range

25: end if

26: if z0 is deﬁned and z0 ⩽ R then 27: return Qz0

28: end if

29: ▷ If we have not terminated yet, the maximizer is on the boundary ([105, Corollary 7.2.2]) 30: Find ω > max{ω0, 0} such that the norm of zω = −(H̃ − ωI)−1̃g is R 31: return Qzω
43

References
[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 27, 2014.
[2] M. Sanjabi, M. Razaviyayn, J. Ba, and J. D. Lee. On the convergence and robustness of training gans with regularized optimal transport. Advances in Neural Information Processing Systems, 2018:7091–7101, 2018.
[3] S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 271–279, 2016.
[4] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein GANs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 5769–5779, 2017.
[5] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
[6] F. Pourbabaee. Robust experimentation in the continuous time bandit problem. Economic Theory, pages 1–31, 2020.
[7] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled trade-oﬀ between robustness and accuracy. In International Conference on Machine Learning, pages 7472–7482. PMLR, 2019.
[8] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.
[9] W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classiﬁers? In International Conference on Machine Learning, pages 2029–2037. PMLR, 2018.
[10] A. Sinha, H. Namkoong, and J. Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.
[11] A. Rezaei, R. Fathony, O. Memarrast, and B. Ziebart. Fairness for robust log loss classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 5511–5518, 2020.
[12] S. Baharlouei, M. Nouiehed, A. Beirami, and M. Razaviyayn. R´enyi fair inference. In 8th International Conference on Learning Representations (ICLR), 2020.
[13] B. H. Zhang, B. Lemoine, and M. Mitchell. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335–340, 2018.
44

[14] D. Xu, S. Yuan, L. Zhang, and X. Wu. Fairgan: Fairness-aware generative adversarial networks. In 2018 IEEE International Conference on Big Data, pages 570–575. IEEE, 2018.
[15] A. Lowy, R. Pavan, S. Baharlouei, M. Razaviyayn, and A. Beirami. Fermi: Fair empirical risk minimization via exponential R´enyi mutual information. arXiv preprint arXiv:2102.12586, 2021.
[16] T. Adel, I. Valera, Z. Ghahramani, and A. Weller. One-network adversarial fairness. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 2412–2420, 2019.
[17] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pages 1125–1134. PMLR, 2018.
[18] M. Rabbat and R. Nowak. Distributed optimization in sensor networks. In Proceedings of the 3rd international symposium on Information processing in sensor networks, pages 20–27, 2004.
[19] N. Li and J. R. Marden. Designing games for distributed optimization. IEEE Journal of Selected Topics in Signal Processing, 7(2):230–242, 2013.
[20] D. Hajinezhad, M. Hong, T. Zhao, and Z. Wang. Nestt: A nonconvex primal-dual splitting method for distributed and stochastic optimization. Advances in neural information processing systems, 29:3215–3223, 2016.
[21] M. Hong, D. Hajinezhad, and M.-M. Zhao. Prox-pda: The proximal primal-dual algorithm for fast distributed nonconvex optimization and learning over networks. In International Conference on Machine Learning, pages 1529–1538. PMLR, 2017.
[22] M. Hong, M. Razaviyayn, and J. Lee. Gradient primal-dual algorithm converges to secondorder stationary solution for nonconvex distributed optimization over networks. In International Conference on Machine Learning, pages 2009–2018. PMLR, 2018.
[23] G. Scutari, D. P. Palomar, and S. Barbarossa. Optimal linear precoding strategies for wideband noncooperative systems based on game theory—part i: Nash equilibria. IEEE Transactions on Signal Processing, 56(3):1230–1249, 2008.
[24] G. Scutari and J.-S. Pang. Joint sensing and power allocation in nonconvex cognitive radio games: Nash equilibria and distributed algorithms. IEEE Transactions on Information Theory, 59(7):4626–4661, 2013.
[25] J. F. Nash. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36(1):48–49, 1950.
[26] J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295–320, 1928.
[27] J. von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton university press, 1953.
45

[28] A. Nemirovski. Prox-method with rate of convergence O(1 t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229–251, 2004.
[29] G. M. Korpelevich. Extragradient method for ﬁnding saddle points and other problems. Matekon, 13(4):35–49, 1977.
[30] A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.
[31] N. He, A. Juditsky, and A. Nemirovski. Mirror prox algorithm for multi-term composite minimization and semi-separable problems. Computational Optimization and Applications, 61(2):275–319, 2015.
[32] N. He and Z. Harchaoui. Semi-proximal mirror-prox for nonsmooth composite minimization. In Proceedings of the 28th International Conference on Neural Information Processing Systems, volume 2, pages 3411–3419, 2015.
[33] A. Juditsky and A. Nemirovski. Solving variational inequalities with monotone operators on domains given by linear minimization oracles. Mathematical Programming, 156(1-2):221–256, 2016.
[34] B. Cox, A. Juditsky, and A. Nemirovski. Decomposition techniques for bilinear saddle point problems and variational inequalities with aﬃne monotone operators. Journal of Optimization Theory and Applications, 172(2):402–435, 2017.
[35] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(88):2543–2596, 2010.
[36] A. Juditsky and A. Nemirovski. First order methods for nonsmooth convex large-scale optimization, ii: utilizing problem’s structure. Optimization for Machine Learning, pages 149–183, 2011.
[37] Y. Nesterov and A. Nemirovski. On ﬁrst-order algorithms for l1/nuclear norm minimization. Acta Numerica, 22:509–575, 2013.
[38] Y. Wang and J. Li. Improved algorithms for convex-concave minimax optimization. arXiv preprint arXiv:2006.06359, 2020.
[39] J. Zhang, M. Hong, and S. Zhang. On lower iteration complexity bounds for the convex concave saddle point problems. Mathematical Programming, pages 1–35, 2021.
[40] C. Daskalakis and I. Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. arXiv preprint arXiv:1807.04252, 2018.
[41] J. Abernethy, K. A. Lai, and A. Wibisono. Last-iterate convergence rates for min-max optimization. arXiv preprint arXiv:1906.02027, 2019.
[42] N. Golowich, S. Pattathil, C. Daskalakis, and A. Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on Learning Theory, pages 1758–1784. PMLR, 2020.
46

[43] C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Linear last-iterate convergence in constrained saddle-point optimization. arXiv preprint arXiv:2006.09517, 2020.
[44] A. Juditsky and A. Nemirovski. On well-structured convex-concave saddle point problems and variational inequalities with monotone operators. arXiv preprint arXiv:2102.01002, 2021.
[45] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1.
[46] D. Kinderlehrer and G. Stampacchia. An introduction to variational inequalities and their applications. SIAM, 2000.
[47] L. Rice, E. Wong, and Z. Kolter. Overﬁtting in adversarially robust deep learning. In International Conference on Machine Learning, pages 8093–8104. PMLR, 2020.
[48] M. Razaviyayn, T. Huang, S. Lu, M. Nouiehed, M. Sanjabi, and M. Hong. Nonconvex minmax optimization: Applications, challenges, and recent theoretical advances. IEEE Signal Processing Magazine, 37(5):55–66, 2020.
[49] J. Zhang, P. Xiao, R. Sun, and Z.-Q. Luo. A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems. arXiv preprint arXiv:2010.15768, 2020.
[50] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. International Conference on Learning Representations (ICLR), 2020.
[51] S. Lu, I. Tsaknakis, M. Hong, and Y. Chen. Hybrid block successive approximation for onesided non-convex min-max problems: algorithms and applications. IEEE Transactions on Signal Processing, 68:3676–3691, 2020.
[52] Q. Qian, S. Zhu, J. Tang, R. Jin, B. Sun, and H. Li. Robust optimization over multiple domains. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4739–4746, 2019.
[53] M. Nouiehed, M. Sanjabi, J. D. Lee, and M. Razaviyayn. Solving a class of non-convex minmax games using iterative ﬁrst order methods. Advances in Neural Information Processing Systems, 32, 2019.
[54] D. M. Ostrovskii, A. Lowy, and M. Razaviyayn. Eﬃcient search of ﬁrst-order Nash equilibria in nonconvex-concave smooth min-max problems. arXiv:2002.07919, 2020.
[55] K. K. Thekumparampil, P. Jain, P. Netrapalli, and S. Oh. Eﬃcient algorithms for smooth minimax optimization. arXiv:1907.01543, 2019.
[56] T. Lin, C. Jin, and M. Jordan. Near-optimal algorithms for minimax optimization. arXiv preprint arXiv:2002.02417, 2020.
[57] R. Zhao. A primal dual smoothing framework for max-structured nonconvex optimization. arXiv preprint arXiv:2003.04375, 2020.
47

[58] Y. Han, G. Xie, and Z. Zhang. Lower complexity bounds of ﬁnite-sum optimization problems: The results and construction. arXiv preprint arXiv:2103.08280, 2021.
[59] S. Zhang, J. Yang, C. Guzm´an, N. Kiyavash, and N. He. The complexity of nonconvexstrongly-concave minimax optimization. arXiv preprint arXiv:2103.15888, 2021.
[60] H. Li, Y. Tian, J. Zhang, and A. Jadbabaie. Complexity lower bounds for nonconvex-stronglyconcave min-max optimization. arXiv preprint arXiv:2104.08708, 2021.
[61] C. Jin, P. Netrapalli, and M. Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In International Conference on Machine Learning, pages 4880–4889. PMLR, 2020.
[62] D. Davis and D. Drusvyatskiy. Stochastic subgradient method converges at the rate O(k−1 4) on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018.
[63] H. von Stackelberg. Marktform und gleichgewicht. J. Springer, 1934.
[64] C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. arXiv preprint arXiv:2009.09623, 2020.
[65] C. H. Papadimitriou. On the complexity of the parity argument and other ineﬃcient proofs of existence. Journal of Computer and system Sciences, 48(3):498–532, 1994.
[66] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a Nash equilibrium. SIAM Journal on Computing, 39(1):195–259, 2009.
[67] F. Facchinei and J.-S. Pang. Finite-dimensional variational inequalities and complementarity problems. Springer Science & Business Media, 2007.
[68] J. S. Pang and M. Razaviyayn. A uniﬁed distributed algorithm for non-cooperative games with non-convex and non-diﬀerentiable objectives. In Big Data over Networks, chapter 4, pages 101–134. Cambridge University Press, 2016.
[69] C. D. Dang and G. Lan. On the convergence properties of non-Euclidean extragradient methods for variational inequalities with generalized monotone operators. Computational Optimization and applications, 60(2):277–310, 2015.
[70] P. Mertikopoulos, H. Zenati, B. Lecouat, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Mirror descent in saddle-point problems: Going the extra (gradient) mile. arXiv preprint arXiv:1807.02629, 2018.
[71] H. Raﬁque, M. Liu, Q. Lin, and T. Yang. Weakly-convex–concave min–max optimization: provable algorithms and applications in machine learning. Optimization Methods and Software, pages 1–35, 2021.
[72] C. Song, Z. Zhou, Y. Zhou, Y. Jiang, and Y. Ma. Optimistic dual extrapolation for coherent non-monotone variational inequalities. arXiv preprint arXiv:2103.04410, 2021.
[73] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Eﬃcient methods for structured nonconvexnonconcave min-max optimization. In Proceedings of the 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130, pages 2746–2754. PMLR, 2021.
48

[74] S. Lee and D. Kim. Semi-anchored multi-step gradient descent ascent method for structured nonconvex-nonconcave composite minimax problems. arXiv preprint arXiv:2105.15042, 2021.
[75] W. Liu, A. Mokhtari, A. Ozdaglar, S. Pattathil, Z. Shen, and N. Zheng. A decentralized proximal point-type method for saddle point problems. arXiv preprint arXiv:1910.14380, 2019.
[76] B. Barazandeh, D. A. Tarzanagh, and G. Michailidis. Solving a class of non-convex min-max games using adaptive momentum methods. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3625–3629. IEEE, 2021.
[77] B. Grimmer, H. Lu, P. Worah, and V. Mirrokni. The landscape of nonconvex-nonconcave minimax optimization. arXiv preprint arXiv:2006.08667, 2020.
[78] T. Fiez, C. Jin, P. Netrapalli, and L. J. Ratliﬀ. Minimax optimization with smooth algorithmic adversaries. arXiv preprint arXiv:2106.01488, 2021.
[79] O. Mangoubi and N. K. Vishnoi. Greedy adversarial equilibrium: an eﬃcient alternative to nonconvex-nonconcave min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 896–909, 2021.
[80] V. Keswani, O. Mangoubi, S. Sachdeva, and N. K. Vishnoi. A convergent and dimensionindependent ﬁrst-order algorithm for min-max optimization. arXiv preprint arXiv:2006.12376, 2020.
[81] Y. Wang, G. Zhang, and J. Ba. On solving minimax optimization locally: A follow-the-ridge approach. arXiv preprint arXiv:1910.07512, 2019.
[82] C. Domingo-Enrich, S. Jelassi, A. Mensch, G. Rotskoﬀ, and J. Bruna. A mean-ﬁeld analysis of two-player zero-sum games. arXiv preprint arXiv:2002.06277, 2020.
[83] E. V. Mazumdar, M. I. Jordan, and S. S. Sastry. On ﬁnding local Nash equilibria (and only local Nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.
[84] T. Fiez and L. Ratliﬀ. Gradient descent-ascent provably converges to strict local minmax equilibria with a ﬁnite timescale separation. arXiv preprint arXiv:2009.14820, 2020.
[85] Y.-P. Hsieh, P. Mertikopoulos, and V. Cevher. The limits of min-max optimization algorithms: convergence to spurious non-critical sets. arXiv preprint arXiv:2006.09065, 2020.
[86] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017.
[87] L. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems, pages 1825–1835, 2017.
[88] T. Liang and J. Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 907–915, 2019.
49

[89] F. Farnia and A. Ozdaglar. arXiv:2002.09124, 2020.

GANs may have no Nash equilibria.

arXiv preprint

[90] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for eﬃciently improving generalization. In International Conference on Learning Representations, 2020.

[91] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207–239, 2019.

[92] Y. Carmon and J. C. Duchi. First-order methods for nonconvex quadratic minimization. SIAM Review, 62(2):395–436, 2020.

[93] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795–811. Springer, 2016.

[94] Y. Nesterov. Random walk in a simplex and quadratic optimization over convex polytopes. Technical report, CORE, 2003.

[95] W. Kong and R. D. C. Monteiro. An accelerated inexact proximal point method for solving nonconvex-concave min-max problems. arXiv:1905.13433, 2019.

[96] T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.

[97] B. Barazandeh and M. Razaviyayn. Solving non-convex non-diﬀerentiable min-max games using proximal gradient method. In Proceedings of the 45th International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3162–3166. IEEE, 2020.

[98] N. I. M. Gould, S. Lucidi, M. Roma, and P. L. Toint. Solving the trust-region subproblem using the Lanczos method. SIAM Journal on Optimization, 9(2):504–525, 1999.

[99] R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009.

[100] A. Alexandrov. An application of the theorem on the invariance of domains to existence proofs. Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematicheskaya, 3(3):243–256, 1939.

[101] R. T. Rockafellar. Second-order convex analysis. Journal of Nonlinear Convex Analysis, 1(1-16):84, 1999.

[102] D. P. Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334–334, 1997.

[103] J. Cullum and W. E. Donath. A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices. In 1974 IEEE Conference on Decision and Control, pages 505–509. IEEE, 1974.

[104] G. H. Golub and R. Underwood. The block Lanczos method for computing eigenvalues. In Mathematical software, pages 361–377. Elsevier, 1977.

[105] A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust-region methods. SIAM, 2000.

50

