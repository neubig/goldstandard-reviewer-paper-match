Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance

arXiv:2110.14508v1 [cs.LG] 27 Oct 2021

Justin Lim∗ MIT CSAIL and IMES
Cambridge, MA justinl@mit.edu

Christina X Ji∗ MIT CSAIL and IMES
Cambridge, MA cji@mit.edu

Michael Oberst∗ MIT CSAIL and IMES
Cambridge, MA moberst@mit.edu

Saul Blecker NYU Langone New York, NY saul.blecker@nyulangone.org

Leora Horwitz NYU Langone New York, NY leora.horwitz@nyulangone.org

David Sontag MIT CSAIL and IMES
Cambridge, MA dsontag@csail.mit.edu

Abstract
Individuals often make diﬀerent decisions when faced with the same context, due to personal preferences and background. For instance, judges may vary in their leniency towards certain drug-related oﬀenses, and doctors may vary in their preference for how to start treatment for certain types of patients. With these examples in mind, we present an algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. We formalize this as a causal inference problem, seeking a region where the assignment of decision-maker has a large causal eﬀect on the decision. Our algorithm ﬁnds such a region by maximizing an empirical objective, and we give a generalization bound for its performance. In a semi-synthetic experiment, we show that our algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, we apply our algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge.
1 Introduction
Understanding heterogeneity in decision-making is an established problem in medicine (Birkmeyer et al., 2013; Corallo et al., 2014; De Jong et al., 2006), consumer choice (Ortega et al., 2011; Scarpa et al., 2005), and law (Kang et al., 2012; Kleinberg et al., 2018; Arnold et al., 2018). In the context of medicine, this is referred to as the study of practice variation (Atsma et al., 2020; Cabana et al., 1999), where it is often observed that doctors, facing the same clinical context, will make diﬀerent decisions. Likewise, in a legal context, judges often diﬀer in their leniency in their decisions regarding bail (Kleinberg et al., 2018), juvenile incarceration (Aizer and Doyle Jr, 2013), the use of alternatives to incarceration (Di Tella and Schargrodsky, 2013), and incarceration length (Kling, 2006). In some scenarios this variation may be justiﬁed: The best medical treatment may not be obvious. In others, it may be grossly unfair, as in the case of racial bias in bail decisions (Arnold et al., 2018).
∗Equal contribution
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

In this work, we tackle the question of how to ﬁnd and characterize this variation in the ﬁrst place. In particular, we present a learning algorithm for identifying a “region of heterogeneity”, deﬁned as a subset of all contexts (e.g., patients, cases) for which the identity of the decision-maker substantially aﬀects the decision. In medicine, a better understanding of treatment variation can inform the development and dissemination of clinical guidelines. In the legal domain, characterizing the cases where judges vary most in their leniency may help with investigating potential issues of fairness.
We formalize characterizing the region of heterogeneity as a causal inference problem: We want to characterize examples where changing the decision-maker would have resulted in a diﬀerent decision. The challenge is two-fold: First, we only observe a single decision-maker per example, so we cannot directly observe how (for instance) multiple judges would have decided the same case. Second, our data on individual decision-makers is often scarce. For instance, in Section 5, we consider a medical dataset with more than 400 doctors, each of whom has fewer than 9 samples on average.
We will refer to decision-makers as “agents”, and our contributions are as follows: In Section 2, we propose an objective deﬁned in terms of counterfactual decisions across diﬀerent agents, and show that this objective can be identiﬁed from observational data. Moreover, this objective does not require the use of agent-speciﬁc statistical models, making it amenable to our sparse setting. In Section 3, we give an iterative algorithm to identify regions of disagreement by maximizing this objective and provide intuition (in the form of a generalization bound) for the factors that drive its performance. In Section 4, we use a semi-synthetic dataset, derived from crowd-sourced recidivism predictions, to demonstrate that our algorithm recovers the correct region of heterogeneity accurately, even when there are many agents. Finally, in Section 5, we apply our algorithm to a real-world healthcare dataset and conﬁrm that it recovers intuitive regions of variation in ﬁrst-line diabetes treatment. We conclude with a discussion of related work and implications. Our code is available at https://github.com/clinicalml/finding-decision-heterogeneity-regions.
Our algorithm does not determine whether variation is inherently good or bad or how it should be addressed. Rather, more careful study with domain experts would be required to determine if variation can (or should be) reduced and how. In addition, false discovery of variation is possible and could have a negative impact. We expect that validation on independent datasets would be required in real-world applications, using the regions identiﬁed by our method as plausible hypotheses to test.
2 Characterizing Heterogeneity from a Causal Perspective
2.1 Notation
Let the data be drawn from a distribution P(X, A, Y), where X is a random variable representing context (or features), A is a discrete agent, and Y ∈ {0, 1} is the binary decision. The spaces of all X and A are denoted as X and A with realized values as lower case x and a, respectively. Indicator variables 1 [·] are one if the statement inside the brackets is true and zero otherwise. For a subset S ⊆ X, 1 [x ∈ S ] is sometimes written as a function S (x), where S : X → {0, 1}. A subset S may have several disjoint regions. E [Y|X ∈ S ] denotes the average Y across samples in S . For instance, if S = {X : X0 < 10}, then E [Y|X0 < 10] is a scalar average of Y among samples with X0 < 10.
2.2 Heterogeneity as a Causal Contrast
Our conceptual goal is to identify a region S ⊆ X where diﬀerent agents tend to make diﬀerent decisions even when faced with the same context. We can formalize this in the language of potential outcomes from the causal inference literature (Pearl, 2009; Hernán and Robins, 2020), which for clarity we will refer to as potential decisions: In particular, we denote Y(a) to be the potential decision of agent a. The fundamental challenge of causal inference is that we do not observe all potential decisions {Y(a) : a ∈ A} for each sample, but only a single decision Y. With this in mind, we will make the following assumptions, standard in the literature on causal eﬀect estimation.
Assumption 1 (Causal Identiﬁcation Assumptions). (i) Consistency: Y = y, A = a =⇒ Y(a) = y, and (ii) No Unmeasured Confounding (NUC): For all a ∈ A, Y(a) ⊥⊥ A | X.
Consistency links the potential Y(a) to the observed Y, and NUC says that there are no unobserved factors that inﬂuence both the assignment of agents and the decision itself. For instance, the quasirandom assignment of cases to judges conditioned on features X satisﬁes NUC (Kleinberg et al., 2018).
2

NUC may be violated if key aspects of the case (e.g., misdemeanor vs. felony) are omitted as features. For instance, misdemeanor and felony cases may be seen by diﬀerent judges and have diﬀerent decision processes, but this variation is not due to agent preferences. Given these assumptions, we propose a causal measure of agent-speciﬁc bias, deﬁned as a contrast between potential decisions.
Deﬁnition 1 (Conditional Relative Agent Bias). For an agent a ∈ A and a subset S ⊆ X, the conditional relative agent bias is deﬁned as

E[Y(a) − Y(π(x)) | A = a, X ∈ S ]

(1)

where Y(a) is the potential decision of agent a, and Y(π(x)) a E[Y(a ) | x]π(a | x) denotes the expected potential decision under the agent assignment distribution π(a | x) P(A = a | X = x).

Note that under Assumption 1, Y(π(x)) = E[Y | x],2 but here we emphasize its causal interpretation as the expected decision of a random agent. Equation (1) represents the relative diﬀerence between the decision of an agent (on their particular distribution of cases in the region) and the potential decision
of a random agent. In particular, Equation (1) can be written as follows under Assumption 1

E[Y(a) − Y(π) | A = a, X ∈ S ] = E[Y(a) − Y(π) | X = x]p(x | A = a, X ∈ S )dx, (2)
x∈S
where we shorten Y(π(x)) to Y(π). This is the average diﬀerence (over p(x | a), restricted to those x in the set S ) of the conditional expected diﬀerence between Y(a) and Y(π). For example, suppose that the agent a is a judge who is particularly lenient on bail decisions for felony arrests (the region S ), and Y = 1 denotes granting bail. Then imagine the following counterfactual: Take the felony cases that are assigned to this judge and reassign each individual case, described by x, to a random judge a , proportionally to p(a | x). We may then observe, on average, that the bail rate would decrease, because most judges are less lenient than judge a, corresponding to a positive value of Equation (1).
Equation (1) has the additional advantage of being easy to estimate: Under Assumption 1, it can be rewritten3 as E[Y − E[Y | X] | A = a, X ∈ S ], the expected residual in predicting (using the conditional expectation E[Y | X]) the decision of an agent a across the context x typically seen by that agent.

2.3 Formalizing a Causal Objective

Our primary goal is to discover a region S where substantial heterogeneity exists across agents. To do so, we deﬁne an aggregate objective across a group G of agents, where G(a) ∈ {0, 1} is an indicator function for membership.

Q(S , G)

P(A = a | X ∈ S )E[Y(a) − Y(π) | A = a, X ∈ S ],

(3)

a:G(a)=1

We now show that this quantity can be identiﬁed and estimated from observational data without requiring agent-speciﬁc statistical models, before discussing the interpretation of this objective.
Theorem 1 (Causal Identiﬁcation). Under Assumption 1, Q(S , G) can be identiﬁed as

Q(S , G) = ES [Cov(Y, G | X)] = ES [(Y − E[Y | X])G],

(4)

where ES [·] E[· | X ∈ S ] and Cov(Y, G | X) is the conditional covariance.

Theorem 1 and other theoretical results are proven in Appendix A. The result follows from proving
that the agent-speciﬁc bias (Deﬁnition 1) is identiﬁable using the expected conditional covariance between Y and the binary indicator 1 [A = a]. With this in mind, we optimize the following objective, where the set S is constrained to be at least a certain size β and S is a hypothesis class of functions S .

max Q(S , G) s.t., P(S ) ≥ β,

(5)

S ∈S,G

Interpretation: Intuitively, this objective measures the disagreement between the agents in the group G(a) = 1 and the overall average E[Y | X] on the region S . Hence, the choice of group is important for interpreting the objective: If G(a) = 1 for all agents, the objective will be zero for any set S , as can be seen from Equation (4), applying the deﬁnition of the conditional expectation.
2See Proposition A1 for a short proof, and Proposition A2 for the derivation of Equation (2). 3See Proposition A3 in Appendix A.1.

3

Accordingly, we seek a region S for which the partially maximized objective L(S ) maxG Q(S , G) is large: This partial maximization is obtained by taking G(a) = 1 whenever the conditional relative
agent bias of agent a (on the set S ) is non-negative. Thus, Equation (5) can be re-written as

max Q(S , G) = P(A = a | X ∈ S ) |E[Y(a) − Y(π) | A = a, X ∈ S ]|+ ,

(6)

G

a∈A

where |x|+ max(x, 0), and this objective becomes an average over agents who have a positive bias. This population objective is also equivalent (up to a constant factor) to the (weighted) sum of the
magnitude of each agent’s conditional relative agent bias. See Proposition A4 in Appendix A.1.

Lack of Overlap: We have not made the overlap or positivity assumption that P(A = a | x) > 0 for all x, a. While this assumption is required to identify conditional causal eﬀects E[Y(a) − Y(a ) | X] (Nie and Wager, 2017; Wager and Athey, 2018; Shalit et al., 2017), it is not required for identifying our
causal contrast. Our problem only requires each context has a positive probability of being seen by
more than one decision maker. For instance, suppose that S contains both misdemeanors and felonies and there are four judges a0, . . . , a3. If judges a0 and a1 make bail decisions exclusively for felonies while judges a2 and a3 make bail decisions exclusively for misdemeanors, our measure captures disagreement between a0 and a1 and between a2 and a3 even though comparisons between a0 and a2 or other pairs are impossible to make. Thus, we have chosen to compare Y(a) to the decisions of viable alternative agents, weighted by their probability p(a | x) of being selected for such a case.

3 Identifying Regions with Heterogeneity

In Section 3.1, we introduce an iterative optimization algorithm for a ﬁnite sample version of Objective (5) that alternatingly optimizes S and G. In Sections 3.2 and 3.3, we discuss practical heuristics for choosing the region size parameter β on training data and validating if the resulting region generalizes to held-out data. Finally, we build intuition for the factors that inﬂuence performance of this algorithm via a generalization bound in Section 3.4 under simplifying assumptions.

3.1 Iterative Optimization Algorithm

We let Qˆ(S , G) be the empirical analog of Q(S , G) (Equation 4), which we can write as follows,

Qˆ(S , G)

1

(ya j − f (xa j)) · G(a) · 1 xa j ∈ S .

(7)

a, j 1 xa j ∈ S a, j

where f (x) denotes a model of the conditional expectation f (x) ≈ E[Y | X = x]. For simplicity of notation, we assume that there are R samples (indexed by j) for each of a ﬁnite set of N agents (indexed by a), giving N · R samples in total.

Our algorithm (Algorithm 1) takes as input the data {(xa j, ya j)} and a minimum region size β, and outputs a model h(x) and a threshold value b that describe a region of heterogeneity S = {x ∈ X; h(x) ≥ b}. Starting with S = X (the entire space), the algorithm identiﬁes the grouping that maximizes Qˆ(S , G), then uses that grouping to identify the region maximizing the same quantity, repeating this process until convergence. The algorithm uses a classiﬁer f (x) to estimate E[Y | X = x] and a regression model h(x) to estimate the conditional covariance of the decision Y and the grouping G at X = x. Note that we can use any supervised learning algorithms for f and h, allowing us to learn interpretable regions as part of the algorithm if h(x) is interpretable (e.g., decision trees). If suﬃcient data is available, samples can be split into three parts for estimating f (x) in line 2, computing G(a) in lines 5-8, and training h(x) and estimating the (1 − β)-th quantile in line 10. We do not perform this
sample splitting because our sample sizes are already small. Under-ﬁtting f (x) by further restricting
the sample size could lead to false discovery if f (x) does not capture the variation explained by X.

Optimizing over G given S . Given a region S , our ﬁrst result identiﬁes the grouping G : A → {0, 1} that maximizes Qˆ(S , G) and shows that it can be expressed in terms of Qˆ(S , 1 [A = a]). Proposition 1. Given S ⊆ X, Qˆ(S , G) is maximized over the space of functions G : A → {0, 1} at GS , where GS (a) = 1 Qˆ(S , 1 [A = a]) ≥ 0 .

Intuitively, this proposition states that to maximize the empirical expected conditional covariance of the decision and grouping on a region, we must group agents by whether their residuals ya j − f (xa j) are (on average) positive or negative on S .

4

Algorithm 1 Identifying regions with variation

1: Input: Data {{xa j, ya j}Rj=1}aN=1, minimum region size β. 2: Fit a model f (x) to E(Y | X = x).

3: Initialize S = X.

4: repeat

5: for a = 1, . . . , N do

6: Compute Qˆ (S , 1 [A = a]), where Qˆ (S , 1 [A = a]) j 1[1xaj∈S ] j(ya j − f (xa j))1 xa j ∈ S ,

7:

Set G(a) = 1 if Qˆ(S , 1 [A = a]) ≥ 0 and 0 otherwise.

8: end for

9: Compute ba j = (ya j − f (xa j))G(a), a = 1, . . . , N, j = 1, . . . , R.

10: Fit a model h(x) to predict ba j from xa j, and let b be the (1 − β)-th quantile of h(xa j).

11: S ← S .

12: S ← {xa j; h(xa j) ≥ b}.

13: until S = S or iteration limit reached.

14: Output: Model h and threshold b, deﬁning a region S = {x ∈ X; h(x) ≥ b}.

Optimizing over S given G. To optimize Qˆ(S , G) for a ﬁxed grouping G over the hypothesis class

S, we train a model h(x) to predict (ya j − f (xa j))G(a) given xa j, where h ∈ H. Using h as an estimate

in Eq. 7, we ﬁnd a set S to maximize the quantity

1 1[x∈S ]

x∈S h(x), the empirical conditional

x

expectation of h(x) over S . This quantity is maximized (subject to our β constraint) by including the

largest β-fraction of the h(xa j) in S . Hence, we pick b as the (1 − β)-th quantile of h(xa j) and choose

our region as Sˆ G = {x ∈ X; h(x) ≥ b}.

3.2 Tuning the Region Size Parameter
For real datasets, we need to choose β without knowledge of the true value. Given that our objective can be calculated on held-out data using the functions S , G, a seemingly obvious approach would be to compute Q(S , G) on a validation set and select the β that leads to the highest Q(S , G). However, for a ﬁxed data distribution, smaller values of β tend to produce higher values of Q(S , G), and there is a trade-oﬀ between ﬁnding a smaller region of higher variation and a larger region that may include areas of lower (but still meaningful) variation. This motivated our original constraint P(S ) ≥ β.
To select β, we propose a heuristic inspired by permutation-based hypothesis testing (Wasserman, 2004). We compare the training objective to a reference distribution of values (for the same β) that might be seen even if all agents followed the same policy. For each candidate β, we (i) run our algorithm and compute the objective on training data qobs Qˆ(Sˆ , Gˆ). (ii) For T iterations, we randomly shuﬄe the agents and re-run the algorithm to get a new objective value. This gives us a distribution Pˆ null over Q(S , G) from a distribution where P(X, Y) and P(A) are unchanged but Y, X ⊥⊥ A. (iii) Finally, we compute a p-value pβ = Pˆ null(Q > qobs) and choose the β with the smallest p-value. In Section 4.2, we ﬁnd that this heuristic empirically recovers the true β value in semi-synthetic settings.

3.3 Validation of the Region
We may wish to validate the learned region Sˆ independently of the grouping Gˆ. In particular, ﬁnding Gˆ is not our main goal, and we observe in our semi-synthetic experiments that our algorithm can ﬁnd the true region S even when the grouping Gˆ is fairly poor (due to few samples per agent), as shown in Appendix B.2. We can optimize over G in Q(S , G) to obtain an objective that depends only on S and can be used to compare regions. By Proposition 1, we obtain an empirical analog of Equation (6) as

Lˆ(S ) max Qˆ(S , G) =

1

(ya j − f (xa j))1 xa j ∈ S ,

(8)

G a, j 1 xa j ∈ S a j +

where |x|+ is equal to the positive part [x]+ = max(x, 0) as before. We then use this objective Lˆ(S ) to answer the following question: Does our chosen region Sˆ yield a signiﬁcantly higher objective value on test data than a randomly selected region of the same size? An example of this analysis is given in Table 1 for the real-data experiment in Section 5.

5

3.4 Generalization Error

We give a generalization bound for Algorithm 1 to build intuition for the factors that inﬂuence performance. To derive this bound, we consider a simpliﬁed setting, where there exists a set S , G such that the following set of assumptions hold.
Assumption 2 (Group-based variation). For all x ∈ S , E[Y | X = x, A = a] = E[Y | X = x, G (a)] and for all x S , E[Y | X = x, A = a] = E[Y | X = x]
Assumption 3 (Non-zero relative biases). There exists a constant α > 0 such that for all x ∈ S ,
E[Y | X = x, G (A) = 1] − E[Y | X = x] > α, and E[Y | X = x, G (A) = 0] − E[Y | X = x] < −α,
Assumption 4 (All agents see samples in S ). There exists a constant ω > 0, such that for every a ∈ A, P(X ∈ S | A = a) > ωP(X ∈ S ).

Note that under these assumptions, S , G maximize the objective Q(S , G) (see Appendix A.4), so we will refer to them as S ∗, G∗ for the remainder of this section. Assumption 2 says that there are two groups of agents, who follow two distinct decision policies within a region S ∗ but follow an identical decision policy outside of S ∗. Assumption 3 says that one group has a positive bias across all of S ∗, relative to the average over both groups, and the other group has a negative bias. To simplify

the analysis, we also make Assumption 4 that every agent has some non-zero chance of observing

some contexts X in the region, but note that we do not require that p(x | a) > 0 for all x ∈ S .

Under these assumptions, we demonstrate that the ﬁrst iteration of Algorithm 1 will ﬁnd, with high probability, a region Sˆ whose value Q(Sˆ , G∗) (for the same grouping G∗ deﬁned above) is close to that of the optimal S ∗. Note that we do not claim that the iterative algorithm ﬁnds the globally optimal

solution. For simplicity, we assume that f (x) perfectly recovers E[Y | X]. This can be relaxed at

the cost of additional terms in the bound that go to zero as the overall sample size increases. Under Assumptions 2, 3, and 4, assume that P(S ∗) = β. For the informal version presented here, we assume that P(S ∗) = P(Sˆ ) = β, where Sˆ is returned by our algorithm, and that exactly a β-fraction of our samples fall into S ∗ and Sˆ (in the Appendix, we give a version without these simpliﬁcations).

Theorem 2.

Under the assumptions above, if S ∗

∈

S and R

>

2 ln 2 α2 β2 ω2

,

the

ﬁrst

iteration

of

Algorithm

1

returns Sˆ such that, with probability at least 1 − δ, Q(S ∗, G∗) − Q(Sˆ , G∗) ≤ , where

2 ln(3/δ)

2

 

3η(1

−

η)

 

1

 

2 ln(12/δ) 

= βN · R + β η + δ · N  + β 2R(S, N · R) + 4 N · R  ,

where R(S, N · R) is the Rademacher complexity of S, and η exp −Rα2β2ω2/2 .

The term η plays an important role: It bounds the expected misclassiﬁcation error P(Gˆ(a) G∗(a)). For suﬃciently large R, we have that η < 1/2 with high probability, i.e., we have a better-than-random chance of identifying the group for an individual agent. Moreover, η decreases as we increase the number of samples R for each agent, the separation α between the two groups on S ∗, the region size β, and the constant ω. For suﬃciently small η, our algorithm discovers a region whose value (in terms of Q(S , G∗)) is close to that of the optimal region. The generalization bound improves as the
number of agents N increases, the number of samples R for each agent increases, or the complexity
of the hypothesis class decreases. The latter is measured here by the Rademacher complexity R(S, N · R) of our hypothesis class S, which can be bounded by standard arguments. In conclusion,
under some additional assumptions, Algorithm 1 identiﬁes an approximately optimal solution with
high probability after one iteration. We show via semi-synthetic experiments in Appendix B.3 that
convergence is generally fast in practice.

4 Semi-Synthetic Experiment: Recidivism Prediction
For conceptual motivation in the introduction, we discussed the legal system: As a potential application of our method, one could determine types of cases for which the idiosyncratic preferences of judges have a signiﬁcant impact on their decisions. Lacking data on judge decisions with suﬃcient context, we turn to the more controlled setting of human predictions of recidivism. Dataset: We use publicly available data from Lin et al. (2020),4 who ask participants on Amazon’s Mechanical Turk platform to make recidivism predictions based on information present in the
4Available at https://github.com/stanford-policylab/recidivism-predictions

6

Region AUC

Drug possession
1.0

0.9

0.8

0.7

0.6

0.5

0.4

Direct TARNet

0.3

Iterative (Ours)

0

20

40

60

Num agents

80

0

Misdemeanor under 35

20

40

60

80

Num agents

Figure 1: Comparison of our method and best baselines at identifying region of heterogeneity, as measured by the held-out test AUC for classifying samples into the true region of heterogeneity. Total number of samples is ﬁxed. Baselines are described in Section 4.1. Uncertainty bands give 95% intervals for the mean derived via bootstrapping over 10 random seeds using seaborn (Waskom, 2021). Left: Region is modelled using a ridge regression in the drug possession semi-synthetic set-up. Right: Region is modelled using a random forest for the misdemeanor under age 35 set-up.

“Correctional Oﬀender Management Proﬁling for Alternative Sanctions” (COMPAS) dataset for Broward County, FL (Dressel and Farid, 2018). Participants (or “agents”) are shown 5 risk factors: age, gender, number of prior convictions, number of juvenile felony charges, and number of juvenile misdemeanor charges. The charge in question is also given, as well as whether the charge is a misdemeanor or felony. The dataset contains 4550 cases evaluated by 87 participants.
Semi-Synthetic Policy Generation: To benchmark our method, we generate semi-synthetic data where we have access to a “ground truth” region of heterogeneity. We retain the features presented to the original participants and construct two policies, which we refer to as the “base” and “alternative” policies: For the base policy, we learn a logistic regression model on the binary decisions across the whole dataset. For the alternative policy, an extra positive term is added to the logistic regression for samples within the region. We construct two scenarios with diﬀerent regions of variation: (1) all drug possession charges, and (2) all misdemeanor charges where the individual is 35 years old or younger. These make up 22% and 21% of the data, respectively. Then, we generate synthetic agents (randomly assigned to cases) and assign half of the agents to the base policy and half to the alternative. Synthetic decisions are then sampled from the logistic regressions. For each scenario, the two groups of agents follow the same stochastic policy outside of the region, and one group systematically prefers Y = 1 within the region. More details can be found in Appendix B.1.
4.1 Performance versus Baselines
Baselines: We compare how well our approach identiﬁes the true region of heterogeneity with several baselines. To our knowledge, the problem of ﬁnding regions of heterogeneity with a large number of agents has not been studied before. Many causal inference methods for treatment eﬀect estimation are designed for a single, binary treatment. However, naively estimating the treatment eﬀect between each pair of providers would scale O |A|2 . Therefore, we develop new baselines. Some (including the causal forest and U-learner adaptations described in Appendix B.2) are based on causal inference methods augmented to identify a region of heterogeneity and grouping of agents where possible.
Direct models: This baseline measures how much adding the agent to the feature set improves prediction of decisions. We ﬁt logistic regressions with and without the agent feature to estimate E[Y | A, X] and E[Y | X]. For each sample (x, y, a), we compute |y − E[Y | X = x]| − |y − E[Y | A = a, X = x]| to quantify how much the model with agents outperforms the model without agents. Then, we ﬁt a “region model” to predict this quantity from X. This region model is either a ridge regression, decision tree, or random forest model. Finally, we compute the top β quantile of predictions from the region model in the training and validation sets and use this cut-oﬀ to select points in the test set.
TARNet: A treatment-agnostic representation network (Shalit et al., 2017) models the outcomes of all treatments for each sample by learning a shared representation of the sample features and then
7

p-value Density / Count

Avg. p-value by
0.5 0.4 0.3 0.2 0.1
0.0 0.1 0.2 0.3 0.4

Chosen value of
5 4 3 2 1 0 0.0 0.2 0.4 0.6

(a)

(b)

Figure 2: Results of tuning β over 30 semi-synthetic datasets. (a) Average p-value for each candidate β, with 95% uncertainty intervals for the mean generated by bootstrapping. The dotted vertical line represents the true value of β. (b) Distribution of β with the smallest p-value over the datasets.

having a separate prediction head for each treatment. We implement the shared representation model using a 2-layer neural network with ReLU and dropout. Each prediction head is a linear layer with a sigmoid function. TARNet predicts E[Y | X, A] for every agent for each sample. VarA[E[Y | X, A]] measures the variation across all agents if they had seen context X and is analogous to our objective without grouping. We predict this quantity with the region models as in the direct model baseline.
Results: We evaluate how well the algorithms identify the samples within the region of heterogeneity when we vary the number of agents among 2, 5, 10, 20, 40, and 87, where 87 is the number of agents in the original dataset. Figure 1 shows the best overall region models for each set-up, with the other models deferred to Appendix B.2. The metric in Figure 1 is the region AUC, deﬁned as how well the model classiﬁes whether samples belong in the region of heterogeneity when compared to the true region. Algorithm 1 consistently performs well for both semi-synthetic set-ups, especially when the number of agents increases to a realistic level (and the number of samples per agent decreases). The direct baseline deteriorates very rapidly as the number of agents increases in the drug possession set-up, while the TARNet baseline deteriorates rapidly in the misdemeanor under age 35 set-up. Refer to Appendix B.2 for additional baseline details, region models, and evaluation metrics. We also show that our method is robust in a set-up with more than 2 agent groups in Appendix B.4.
4.2 Tuning the Region Size
We validate the proposed approach of tuning β (discussed in Section 3.2), by applying the methodology to our semi-synthetic setting here. We sample 30 semi-synthetic datasets and consider candidate values of β in [0.02, 0.42] in increments of 0.04. For each proposed value of β we use T = 40 random permutations of the agents. Figure 2 presents results for the misdemeanor under age 35 set-up. As the candidate value of β increases (up to the true value of β), the p-value decreases, and the distribution of selected β values are centered on the true value of β.
5 Real-data Experiment: First-Line Diabetes Treatment
We apply our algorithm on a real-world dataset consisting of ﬁrst-line (initial) treatment for type 2 diabetes and examine how the treatment variation we discover aligns with clinical knowledge. We present an additional real-world experiment (on Parkinson’s disease) in Appendix D.
Data and Setup: We use an observational, de-identiﬁed, dataset provided by a large health insurer in the United States, spanning from 2010 to 2020. The task is to classify ﬁrst-line treatment decisions between metformin (Y = 0)–the typical recommendation from the American Diabetes Association– and other common ﬁrst-line treatments such as sitagliptin, glipizide, glimepiride, or glyburide (Y = 1) (American Diabetes Association, 2010; Hripcsak et al., 2016). As relevant clinical features, we include the patients’ most recent eGFR (mL/min/1.73m2) and creatinine (mg/dL) measurements,
8

Table 1: Objective values L(S ) for the learned region on the training and test datasets, along with the distribution of values for randomly generated regions S rand given as mean (standard deviation).

Metric Subset Value

L(Sˆ ) L(Sˆ )
L(S rand)

Train Test Test

0.1029 0.0924 0.0507 (0.0073)

incidence of heart failure, and treatment date. Because treatment date does not deﬁne a type of patient, we omit it from the region model. However, including it in the outcome model is essential because of increasing use of metformin over time. The agent indicator A is the group practice of the doctor responsible for the patient’s treatment. 3,980 patients and 447 group practices are included in our cohort. After requiring at least 4 patients per agent, 3,576 patients and 176 group practices are included. This ﬁlter ensures each group practice has at least 1 sample in the training and validation sets and at least 2 samples in the test set. In this experiment, we choose β = 0.25 as input to our algorithm. See Appendix C for additional cohort deﬁnition and set-up details.
Interpretation of Results: To interpret the region, we use decision trees as our region model h(x). The tree is visualized in Appendix C. The decision tree identiﬁes the region of heterogeneity as the union of (i) eGFR below 71.5 and (ii) eGFR above 98.5 and creatinine above 0.815. These regions align with clinical intuition. In the ﬁrst region, low eGFR values indicate impaired renal function (Group et al., 2009), which is a contraindication for metformin since it is traditionally associated with increased risk of lactic acidosis (Tahrani et al., 2007). Still, treatment decisions can vary here because guidelines for managing patients with eGFR below 45 are lacking (Group et al., 2015). Note that this region provides an example of how our algorithm works when overlap is not satisﬁed. Although 33 of 176 group practices do not see patients with these features, we can still conclude that this is a region of heterogeneity among the 143 agents with cases. In the second region, there are no obvious contraindications for metformin. Thus, understanding why some doctors on average only prescribe metformin 78% of the time to patients in this region may help us identify whether this is a region in which we can standardize practice.
Assessing Signiﬁcance: In Table 1, we perform a sanity check, assessing whether our algorithm identiﬁes a region S whose variation in held-out data is higher than that of a randomly selected region, using the partially optimized objective L(S ) = maxG Q(S , G) laid out in Section 3.3 to compare regions directly. Table 1 shows that L(Sˆ ) is similar on the training and test data. Furthermore, we compare L(Sˆ ) on the test set to the distribution of L(S rand), where S rand are random subsets of the test data of the same size as Sˆ . We compute the latter distribution using 100 random subsets and observe that the test value of L(Sˆ ) is more than two standard deviations above the mean of the latter. This gives us conﬁdence that the discovered region S generalizes to a region of heterogeneity beyond the training set. We direct the reader to Appendix C for additional analyses, such as evaluating the stability of the region over multiple folds of splitting the data.
6 Related Work
Beyond previously mentioned connections to causal eﬀect estimation, we highlight a few areas of research that share similar goals to our own.
Agent-speciﬁc Models of Decisions: Prior works have modeled agent-speciﬁc decision-making processes by estimating a separate model for each agent. Abaluck et al. (2016) model heterogeneity in physician tendency to run diagnostic tests. Chan Jr et al. (2019) estimate radiologist skill based on diagnosis and miss rates. In our setting, unlike in diagnosis, there is no “correct” decision that can be incorporated into the model. Ribers and Ullrich (2020) assume there is provider-speciﬁc noise in determining patient type, which aﬀects the pay-oﬀ functions for deciding whether to prescribe antibiotics. When only a few decisions are observed per agent, these agent-speciﬁc models cannot be estimated reliably. Currie and MacLeod (2017) also incorporate physician beliefs and skill into a pay-oﬀ function. They estimate an aggregate logistic choice model (for C-sections) across all physicians and then learn how individual physicians deviate from this model. They do not learn the regions where this deviation occurs, as they focus on how heterogeneity is associated with
9

downstream outcomes. Norris (2020) looks for disagreement between agents but relies on some cases being seen by multiple agents. We assume each case is seen by only one decision-maker.
Conditional Independence Testing: While our objective maximizes a causal notion of dependence, one could instead ask if Y is conditionally independent of A given X. Many metrics exist for testing conditional independence, such as the Hilbert-Schmidt independence criterion (HSIC) (Fukumizu et al., 2007; Zhang et al., 2012), conditional mutual information (Runge, 2018), conditional correlation (Ramsey, 2014), and expected conditional covariance (Shah and Peters, 2020). We give the last a causal interpretation under some assumptions and seek a region that maximizes it, in lieu of testing.
Hierarchical / Mixture Models: Bayesian methods are often used to estimate group-level eﬀects, such as a per-physician eﬀect on patient outcomes (Tuerk et al., 2008), where group identiﬁers are included as a categorical feature in a multi-level generalized linear model (Gelman and Hill, 2006). Alternatively, one could assume a conditional mixture model (Bishop, 2006), where agents belong to latent clusters that each have their own policy. However, both of these methods require parametric assumptions on the distribution of P[Y | x, a], and even so, the optimal mixture model is not necessarily identiﬁable when both the clusters and policies are unknown (Dasgupta and Schulman, 2007). By contrast, our method does not require making particular parametric assumptions about E[Y | x, a] and seeks to learn the region of heterogeneity directly.
Feature Evaluation: Checking for heterogeneity can also be framed as feature evaluation, where we would like to evaluate whether adding the agent identiﬁer will increase predictive power. Feature evaluation methods typically maximize dependence between selected features and labels, utilizing measures similar to those in conditional independence testing, such as the HSIC (Song et al., 2007). Other methods use the correlation of the new feature with the loss gradient of the current predictor as a measure of utility (Koepke and Bilenko, 2012). In contrast, we focus not on checking marginally for the predictive power of agent identiﬁers, but rather identifying a region.
Crowdsourcing: Our work is conceptually related to identifying which samples are diﬃcult to label via crowdsourcing annotations (Karger et al., 2014; Whitehill et al., 2009). Most crowdsourcing models are generative with latent variables for the correct sample labels, sample diﬃculty, agent expertise, and agent bias. They then optimize for the likelihood of the observed labels. The set of diﬃcult samples is analogous to our region of heterogeneity. The main diﬀerence with our problem is that we do not require any notion of the “correct” label.
7 Discussion
In this work, we take a causal perspective on ﬁnding regions where agents (i.e., decision-makers) have heterogeneous preferences, formalizing this heterogeneity in terms of counterfactual (or “potential”) decisions across agents. We propose a causal measure of agent-speciﬁc biases and give an objective that aggregates this bias over agents. This objective can be identiﬁed from observational data and written in terms of an expected conditional covariance. Importantly, for our applications, this does not require building agent-speciﬁc models or assuming overlap across all agents.
We give an iterative algorithm to ﬁnd a region that maximizes this objective. Then, we demonstrate in semi-synthetic experiments that our algorithm accurately recovers the region of heterogeneity and scales well with the number of agents. In contrast, performance of baslines deteriorates when the number of agents increases. Although our experiments have low-dimensional spaces, we hypothesize our algorithm would scale well to high-dimensional feature spaces since the average policy and region models can handle high-dimensional input spaces. Finally, on a real-world medical dataset, we show that our algorithm can yield insights that align with clinical knowledge.
Our work is motivated by understanding variation in human decision-making. In the judicial domain, our method may help unearth types of cases where decisions are highly dependent on the judge assigned to the case. In the medical domain, our approach may identify types of patients where new guidelines may be required to help doctors make decisions more consistent with standard of care. Domain expertise is required to determine the implications of the regions discovered by our method. Beyond these applications, we see our approach as a useful data science tool for understanding heterogeneity in decisions that appears to be driven by individual-level preferences.
10

Acknowledgements: We would like to thank Aaron Smith-McLallen, James Denyer, Luogang Wei, Johnathon (Kyle) Armstrong, Neil Dixit, Aditya Sai, and the rest of the data science group at Independence Blue Cross, whose expertise, data, and support enabled the diabetes experiment. We would also like to thank Rebecca Boiarsky for converting the diabetes data to the OMOP common data model format, Monica Agrawal for her helpful comments, and other members of the lab for insightful discussions. We are also grateful to Charles Venuto and Monica Javidnia for their advice on Parkinson’s. This work was supported in part by Independence Blue Cross, Oﬃce of Naval Research Award No. N00014-17-1-2791, an Abdul Latif Jameel fellowship, and a NSF CAREER award.
References
Jason Abaluck, Leila Agha, Chris Kabrhel, Ali Raja, and Arjun Venkatesh. 2016. The determinants of productivity in medical testing: Intensity and allocation of care. American Economic Review 106, 12 (2016), 3730–64.
Anna Aizer and Joseph J Doyle Jr. 2013. Juvenile Incarceration, Human Capital and Future Crime: Evidence from Randomly-Assigned Judges. Technical Report w19102. National Bureau of Economic Research.
American Diabetes Association. 2010. Standards of medical care in diabetes—2010. Diabetes care 33, Supplement 1 (2010), S11–S61.
David Arnold, Will Dobbie, and Crystal S Yang. 2018. Racial Bias in Bail Decisions. The Quarterly Journal of Economics 133, 4 (May 2018), 1885–1932.
Femke Atsma, Glyn Elwyn, and Gert Westert. 2020. Understanding unwarranted variation in clinical practice: a focus on network eﬀects, reﬂective medicine and learning health systems. International Journal for Quality in Health Care 32, 4 (jun 2020), 271–274. https://doi.org/10.1093/ intqhc/mzaa023
John D Birkmeyer, Bradley N Reames, Peter McCulloch, Andrew J Carr, W Bruce Campbell, and John E Wennberg. 2013. Understanding of regional variation in the use of surgery. The Lancet 382, 9898 (2013), 1121–1129.
Christopher M Bishop. 2006. Pattern recognition and machine learning. springer.
Michael D Cabana, Cynthia S Rand, Neil R Powe, Albert W Wu, Modena H Wilson, Paul-André C Abboud, and Haya R Rubin. 1999. Why don’t physicians follow clinical practice guidelines?: A framework for improvement. Jama 282, 15 (1999), 1458–1465.
David C Chan Jr, Matthew Gentzkow, and Chuan Yu. 2019. Selection with variation in diagnostic skill: Evidence from radiologists. Technical Report. National Bureau of Economic Research.
Ashley N Corallo, Ruth Croxford, David C Goodman, Elisabeth L Bryan, Divya Srivastava, and Therese A Stukel. 2014. A systematic review of medical practice variation in OECD countries. Health Policy 114, 1 (2014), 5–14.
Janet Currie and W Bentley MacLeod. 2017. Diagnosing expertise: Human capital, decision making, and performance among physicians. Journal of labor economics 35, 1 (2017), 1–43.
Sanjoy Dasgupta and Leonard J Schulman. 2007. A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. Journal of Machine Learning Research 8 (2007), 203–226.
Judith D De Jong, Gert P Westert, Ronald Lagoe, and Peter P Groenewegen. 2006. Variation in hospital length of stay: do physicians adapt their length of stay decisions to what is usual in the hospital where they work? Health Services Research 41, 2 (2006), 374–394.
Rafael Di Tella and Ernesto Schargrodsky. 2013. Criminal Recidivism after Prison and Electronic Monitoring. The Journal of Political Economy 121, 1 (2013), 28–73.
Julia Dressel and Hany Farid. 2018. The accuracy, fairness, and limits of predicting recidivism. Science advances 4, 1 (Jan. 2018), eaao5580.
11

Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Schölkopf. 2007. Kernel measures of conditional dependence.. In NeurIPS, Vol. 20. 489–496.
Andrew Gelman and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.
Christopher G Goetz, Stanley Fahn, Pablo Martinez-Martin, Werner Poewe, Cristina Sampaio, Glenn T Stebbins, Matthew B Stern, Barbara C Tilley, Richard Dodel, Bruno Dubois, et al. 2007. Movement Disorder Society-sponsored revision of the Uniﬁed Parkinson’s Disease Rating Scale (MDS-UPDRS): process, format, and clinimetric testing plan. Movement disorders 22, 1 (2007), 41–47.
Guideline Development Group, Henk Bilo, Luis Coentrão, Cécile Couchoud, Adrian Covic, Johan De Sutter, Christiane Drechsler, Luigi Gnudi, David Goldsmith, James Heaf, et al. 2015. Clinical practice guideline on management of patients with diabetes and chronic kidney disease stage 3b or higher (eGFR< 45 mL/min). Nephrology Dialysis Transplantation 30, suppl_2 (2015), ii1–ii142.
Kidney Disease: Improving Global Outcomes (KDIGO) CKD-MBD Work Group et al. 2009. KDIGO clinical practice guideline for the diagnosis, evaluation, prevention, and treatment of Chronic Kidney Disease-Mineral and Bone Disorder (CKD-MBD). Kidney international. Supplement 113 (2009), S1–S130.
PD Med Collaborative Group et al. 2014. Long-term eﬀectiveness of dopamine agonists and monoamine oxidase B inhibitors compared with levodopa as initial treatment for Parkinson’s disease (PD MED): a large, open-label, pragmatic randomised trial. The Lancet 384, 9949 (2014), 1196–1205.
M A Hernán and J M Robins. 2020. Causal Inference: What If. Chapman & Hall/CRC, Boca Raton.
George Hripcsak, Patrick B Ryan, Jon D Duke, Nigam H Shah, Rae Woong Park, Vojtech Huser, Marc A Suchard, Martijn J Schuemie, Frank J DeFalco, Adler Perotte, et al. 2016. Characterizing treatment pathways at scale using the OHDSI network. Proceedings of the National Academy of Sciences 113, 27 (2016), 7329–7336.
Jerry Kang, Judge Mark Bennett, Devon Carbado, Pam Casey, Nilanjana Dasgupta, David Faigman, Rachel Godsil, Anthony G. Greenwald, Justin Levinson, and Jennifer Mnookin. 2012. Implicit bias in the courtroom. UCLA Law Review 59, 5 (2012), 1124–1186.
David R Karger, Sewoong Oh, and Devavrat Shah. 2014. Budget-optimal task allocation for reliable crowdsourcing systems. Operations Research 62, 1 (2014), 1–24.
Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2018. Human Decisions and Machine Predictions. The Quarterly Journal of Economics 133, 1 (Feb. 2018), 237–293.
Jeﬀrey R Kling. 2006. Incarceration Length, Employment, and Earnings. The American economic review 96, 3 (June 2006), 863–876.
Hoyt Koepke and Mikhail Bilenko. 2012. Fast prediction of new feature utility. arXiv preprint arXiv:1206.4680 (2012).
Zhiyuan (Jerry) Lin, Jongbin Jung, Sharad Goel, and Jennifer Skeem. 2020. The Limits of Human Predictions of Recidivism. Science Advances 6, 7 (2020). https://doi.org/10.1126/sciadv. aaz0652 arXiv:https://advances.sciencemag.org/content/6/7/eaaz0652.full.pdf
Kenneth Marek, Danna Jennings, Shirley Lasch, Andrew Siderowf, Caroline Tanner, Tanya Simuni, Chris Coﬀey, Karl Kieburtz, Emily Flagg, Sohini Chowdhury, et al. 2011. The parkinson progression marker initiative (PPMI). Progress in neurobiology 95, 4 (2011), 629–635.
Sharon Muzerengi and Carl E Clarke. 2015. Initial drug treatment in Parkinson’s disease. bmj 351 (2015).
Xinkun Nie and Stefan Wager. 2017. Quasi-oracle estimation of heterogeneous treatment eﬀects. arXiv preprint arXiv:1712.04912 (2017).
12

S Norris. 2020. Examiner inconsistency: Evidence from refugee decisions. Technical Report. Working paper.
David L Ortega, H Holly Wang, Laping Wu, and Nicole J Olynk. 2011. Modeling heterogeneity in consumer preferences for select food safety attributes in China. Food Policy 36, 2 (2011), 318–324. https://www.sciencedirect.com/science/article/pii/S0306919210001442
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019).
Judea Pearl. 2009. Causality: Models, Reasoning, and Inference (2nd ed.). Cambridge University Press.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
Joseph D Ramsey. 2014. A scalable conditional independence test for nonlinear, non-gaussian data. arXiv preprint arXiv:1401.5031 (2014).
Microsoft Research. 2019. EconML: A Python Package for ML-Based Heterogeneous Treatment Eﬀects Estimation. https://github.com/microsoft/EconML. Version 0.x.
Michael A Ribers and Hannes Ullrich. 2020. Machine Predictions and Human Decisions with Variation in Payoﬀs and Skill. (2020).
Jakob Runge. 2018. Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In International Conference on Artiﬁcial Intelligence and Statistics. PMLR, 938–947.
Riccardo Scarpa, George Philippidis, and Fiorenza Spalatro. 2005. Product-country images and preference heterogeneity for Mediterranean food products: A discrete choice framework. Agribusiness 21, 3 (2005), 329–349.
Rajen D. Shah and Jonas Peters. 2020. The Hardness of Conditional Independence Testing and the Generalised Covariance Measure. arXiv:1804.07203 [math.ST]
Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. Vol. 9781107057. Cambridge University Press. https://doi.org/10.1017/ CBO9781107298019
Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual treatment eﬀect: generalization bounds and algorithms. In International Conference on Machine Learning. PMLR, 3076–3085.
Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. 2007. Supervised feature selection via dependence estimation. In Proceedings of the 24th international conference on Machine learning. 823–830.
AA Tahrani, GI Varughese, JH Scarpello, and FWF Hanna. 2007. Metformin, heart failure, and lactic acidosis: is metformin absolutely contraindicated? Bmj 335, 7618 (2007), 508–512.
Peter W Tuerk, Martina Mueller, and Leonard E Egede. 2008. Estimating physician eﬀects on glycemic control in the treatment of diabetes: methods, eﬀects sizes, and implications for treatment policy. Diabetes care 31, 5 (May 2008), 869–873.
Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous treatment eﬀects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018), 1228–1242.
Michael L. Waskom. 2021. seaborn: statistical data visualization. Journal of Open Source Software 6, 60 (2021), 3021. https://doi.org/10.21105/joss.03021
13

Larry Wasserman. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer, New York, NY.
Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier Movellan, and Paul Ruvolo. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. Advances in neural information processing systems 22 (2009), 2035–2043.
Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2012. Kernel-based conditional independence test and application in causal discovery. arXiv preprint arXiv:1202.3775 (2012).
14

Appendix
This appendix contains the following sections:
• Proofs: Section A contains proofs of theoretical results presented in the main paper. • Semi-Synthetic Experiment: Additional Details: Section B contains additional details
and experimental results for the semi-synthetic experiment presented in Section 4. • Diabetes Experiment: Additional Details: Section C contains additional details and
experimental results for the real-data diabetes experiment presented in Section 5. • Additional Real-Data Experiment: Parkinson’s: Section D contains an additional real-
data experiment, on a medical dataset of patients with Parkinson’s disease.
Most of our experiments were run on CPUs, with only the TARNet baseline run on a GEForce GTX GPU. We estimate the compute time to be on the order of 100 hours.

A Proofs of Theoretical Results

A.1 Proof of Minor Claims in Section 2
Proposition A1. Under the assumptions of consistency and no-unmeasured-confounding (NUC), the following equivalence holds
Y(π(x)) = E[Y | x], where Y(π(x)) is deﬁned as a E[Y(a ) | X = x]π(a | x), where π(a | x) P(A = a | X = x).

Proof. Based on the deﬁnition of Y(π(x)), we can write it as follows, using the fact that Y is binary.

Y (π( x))

P(Y(a ) = 1 | X = x)π(a | x)
a

= P(Y(a ) = 1 | A = a , X = x)P(A = a | X = x)
a

= P(Y = 1 | A = a , X = x)P(A = a | X = x)
a

= P(Y = 1, A = a | X = x)
a
= E[Y | X = x]

(NUC) (Consistency)

Proposition A2. Under the assumptions of consistency and no-unmeasured-confounding (NUC), the following equivalence holds

E[Y(a) − Y(π) | A = a, X ∈ S ] = E[Y(a) − Y(π) | x]p(x | A = a, X ∈ S )dx,
x∈S

Proof. This can be seen as follows, noting that by our deﬁnition of Y(π(x)) as a function of x, E[Y(π(x)) | X = x] = Y(π(x))

E[Y(a) − Y(π) | A = a, X ∈ S ] = = =

E[(Y(a) − E[Y | x]) | A = a, X = x]p(x | a, X ∈ S )dx
x∈S
(E[Y(a) | X = x] − E[Y | x]) p(x | a, X ∈ S )dx
x∈S
E[Y(a) − Y(π) | x]p(x | A = a, X ∈ S )dx
x∈S

(NUC)

15

Proposition A3. Under the assumptions of consistency and no-unmeasured-confounding (NUC), the conditional relative agent bias can be written as
E[Y(a) − Y(π) | A = a, X ∈ S ] = E[Y − E[Y | X] | A = a, X ∈ S ]

Proof. By Propositions A1 and A2, we can re-write the conditional relative agent bias as

E[Y(a) − Y(π) | A = a, X ∈ S ]

= E[Y(a) − Y(π) | x]p(x | A = a, X ∈ S )dx
x∈S
= E[Y(a) − E[Y | x] | x]p(x | A = a, X ∈ S )dx
x∈S
= E[Y(a) | x]p(x | A = a, X ∈ S )dx
x∈S
− E[E[Y | x] | x]p(x | A = a, X ∈ S )dx
x∈S
= E[Y | X = x, A = a]p(x | A = a, X ∈ S )dx
x∈S
− E[E[Y | x] | x]p(x | A = a, X ∈ S )dx
x∈S
= E[Y | X = x, A = a, X ∈ S ]p(x | A = a, X ∈ S )dx
x∈S
− E[E[Y | x] | X = x, A = a, X ∈ S ]p(x | A = a, X ∈ S )dx
x∈S
= E[Y − E[Y | x] | X = x, A = a, X ∈ S ]p(x | A = a, X ∈ S )dx
x∈S
=E[Y − E[Y | x] | A = a, X ∈ S ],

Prop. A2 Prop. A1
(NUC)

where the third-to-last line follows from the fact that the event {X = x ∧ A = a} ⇐⇒ {X = x ∧ A = a ∧ X ∈ S } over the set of x that we are integrating over, and thus does not change the
conditional expectation of Y. Meanwhile, E[Y | x] is a function of x alone, and so the conditional expectation is equivalent if we condition on additional information E[E[Y | x] | x] = E[E[Y | x] | X = x, A = a, X ∈ S ] as long as this conditional expectation is well-deﬁned, which it will be wherever p(x | A = a, X ∈ S ) > 0.

Proposition A4. The partially maximized population objective from Equation (6) is equivalent (up to a factor of 2) to a weighted sum of the absolute value of each agent’s conditional relative agent bias. In other words:
P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]|+
a∈A
= 1 P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]| 2 a∈A

Proof.
E[Y − E[Y | X] | X ∈ S ] = P(A = a | X ∈ S )E[Y − E[Y | X] | A = a, X ∈ S ]
a∈A
= P(A = a | X ∈ S ) (|E[Y − E[Y | X] | A = a, X ∈ S ]|+ + |E[Y − E[Y | X] | A = a, X ∈ S ]|−)
a∈A
=0

16

where |x|− = min(x, 0) denotes the negative part, and where the last line follows from the fact that E[Y − E[Y | X] | X ∈ S ] = 0, from the deﬁnition of the conditional expectation. This implies that
P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]|+
a∈A
= − P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]|− ,
a∈A
while the weighted sum of the absolute values is given by
P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]|
a∈A
= P(A = a | X ∈ S ) (|E[Y − E[Y | X] | A = a, X ∈ S ]|+ − |E[Y − E[Y | X] | A = a, X ∈ S ]|−)
a∈A
= 2 P(A = a | X ∈ S ) |E[Y − E[Y | X] | A = a, X ∈ S ]|+
a∈A
where the last line follows from the fact that the weighted sum of the positive parts is equal to the (negative) weighted sum of the negative parts.

A.2 Proof of Theorem 1

Theorem (Causal Identiﬁcation). Under Assumption 1, Q(S , G) can be identiﬁed as

Q(S , G) = ES [Cov(Y, G | X)] = ES [(Y − E[Y | X])G]

(9)

where ES [·] E[· | X ∈ S ], and Cov(Y, G | X) is the conditional covariance.

First, we will prove the following lemma: Lemma 1. Under Assumption 1, we can write the expected conditional covariance as follows for binary random variables Y, 1 [A = a]
E[Cov(Y, 1 [A = a] | x) | X ∈ S ] = P(A = a | X ∈ S )E[Y(a) − Y(π(x)) | A = a, X ∈ S ]

Proof. The conditional covariance can be written as follows
Cov(Y, 1 [A = a] | x) = E[(Y − E[Y | x])1 [A = a] | x] = P(Y = 1, A = a | x) − P(Y = 1 | x)P(A = a | x) = (E[Y | A = a, x] − E[Y | x])P(A = a | x) = (E[Y(a) | x] − E[Y | x])P(A = a | x) = E[Y(a) − Y | x]P(A = a | x)
where in the penultimate line, we use our causal assumptions to write that
E[Y | A = a, x] = E[Y(a) | A = a, x] = E[Y(a) | x],
by consistency and no-unmeasured-confounding, respectively. To get the expected conditional covariance, we integrate this over x ∈ S to arrive at
E[Cov(Y, A | x) | X ∈ S ] = E[Y(a) − Y | x]P(A = a | x)p(x | X ∈ S )dx
x∈S
= E[Y(a) − Y | x]P(A = a | x, X ∈ S )p(x | X ∈ S )dx
x∈S
= E[Y(a) − Y | x]P(A = a, X = x | X ∈ S )
x∈S
= P(A = a | X ∈ S ) E[Y(a) − Y | x]p(x | A = a, X ∈ S )
x∈S
= P(A = a | X ∈ S )E[Y(a) − E[Y | x]] | A = a, X ∈ S ]
where in the second line, we note that P(A = a | x) = P(A = a | x, X ∈ S ), since the event {X = x} ⊂ {X ∈ S } for any x ∈ S .

17

With this in hand, we can prove Theorem 1 by noting that we can write the function G as G(A) = a:G(a)=1 1 [A = a]. Using this, we can write that

E[Cov(Y, G(A) | x) | X ∈ S ] = E[Cov(Y,

1 [A = a] | x) | X ∈ S ]

a:G(a)=1

= E[

Cov(Y, 1 [A = a] | x) | X ∈ S ]

a:G(a)=1

=

E[Cov(Y, 1 [A = a] | x) | X ∈ S ]

a:G(a)=1

=

P(A = a | X ∈ S )E[Y(a) − Y(π(x)) | A = a, X ∈ S ]

a:G(a)=1

where the second equality follows from linearity of the conditional covariance, the third line follows from linearity of expectation, and the last line follows from Lemma 1

A.3 Proof of Covariance Identity
In the main text, we claimed that E[Cov(U, V | X)] = E[(U − E[U | X])V] for binary U, V. This is a known fact, but we give a short a proof here for completeness.

Proof. Let U, V be binary random variables, then

Cov(U, V | X) = E[(U − E[U | X])(V − E[V | X]) | X]

= E[(U − E[U | X])V | X] − E[(U − E[U | X])E[V | X] | X]

= E[(U − E[U | X])V | X].

(10)

Here, Eq. (10) follows since for any bounded f (X),

E[(U − E[U | X]) f (X) | X] = f (X)E[U − E[U | X] | X] = 0.

The result follows from taking expectation with respect to X.

A.4 Proof that S , G Maximizes Objective 5

Proof. For any S and G, we have:

Q(S , G) = ES [(Y − E[Y | X])G]







= ES (Y − E[Y | X])  G(a)1 [A = a]
a

= G(a)ES [(Y − E[Y | X])1 [A = a]]
a

= G(a)ES [Y − E[Y | X] | A = a]P(A = a | X ∈ S ).
a

This quantity is maximized by picking G(a) = 1 if ES [Y − E[Y | X] | A = a] ≥ 0 and G(a) = 0 otherwise. If S were disjoint from S , ES [Y − E[Y | X] | A = a] = 0 by Assumption 2. If S intersects S , Assumption 3 implies that ES [Y − E[Y | X] | A = a] is positive if G (a) = 1 and negative otherwise. In both cases, G maximizes Q(S , G) for a ﬁxed S , i.e. Q(S , G) ≤ Q(S , G ). By Assumptions 2 and
3, we know that Cov(Y, G | X = x) > 0 for all x ∈ S , and Cov(Y, G | X) = 0 for all x S . Since
P(S ) = β and Objective 5 requires P(S ) ≥ β, the optimal choice must be to take S = S . Therefore, Q(S , G ) must maximize our objective, as desired, which justiﬁes our writing them as S ∗, G∗.

18

A.5 Proof of Proposition 1

Proof. Notice that Qˆ(S , G) =

1 a, j 1 xa j ∈ S

(ya j − f (xa j)) · G(a) · 1 xa j ∈ S
a, j

= G(a) 1

a

a, j 1 xa j ∈ S

(ya j − f (xa j)) · 1 xa j ∈ S
j

= G(a)Qˆ(S , 1 [A = a]).
a
To maximize this quantity, the optimal choice is GS , where GS (a) = 1 if Qˆ(S , 1 [A = a]) ≥ 0, and GS (a) = 0 otherwise. To minimize this quantity, the opposite choice suﬃces, giving us GcS .

A.6 Proof of Theorem 2

We ﬁrst present the full version of the Theorem, without the simpliﬁcations. Let β denote the fraction of our N · R samples that fall in the region S ∗. This is the version of the Theorem that we will prove:

Theorem (Theorem 2, Formal). Under the same assumptions as in Section 3.4, as long as S ∗ is in

our

hypothesis

class

S

and

R

>

2 ln 2 α2 β2 ω2

,

the

ﬁrst

iteration

of

Algorithm

1

returns

Sˆ

such

that,

with

probability at least 1 − δ, Q(S ∗, G∗) − Q(Sˆ , G∗) ≤ , where

2 ln(3/δ)

1

1

 

3η(1

−

η)

 

= β N · R + β + β η + δ · N 

1

1

1

 

+

−

+

2R(S, N · R) + 4

Pˆ (Sˆ ) P(Sˆ ) P(Sˆ ) 

where R(S, N · R) is the Rademacher complexity of S, and

η = exp −Rα2β2ω2 . 2

2 ln(12/δ)  N · R  ,

For simplicity, we assume that X is a continuous random variable with a well deﬁned density P(X = x).

We ﬁrst bound the error rate in estimating the grouping on the ﬁrst iteration.

Lemma 2. Under Assumptions 2, 3, and 4, let Gˆ be the grouping returned by the ﬁrst iteration of

Algorithm 1. Then for every a ∈ A,

P[Gˆ(a) G∗(a)] ≤ η,

where

η exp −Rα2β2ω2 . (11) 2

with α deﬁned in Assumption 3, β given as input to the algorithm, and ω deﬁned in Assumption 4. As

long

as

R

>

2 ln 2 α2 β2 ω2

,

then

η

<

1/2

Proof. Choose some agent a, and assume that G∗(a) = 1, as the argument is symmetric if G∗(a) = 0.

Deﬁne Qˆ a

1 R

Rj=1(ya j − f (xa j)) as shorthand for the sample average Qˆ (S , 1 [A = a]), where S = X

for the ﬁrst iteration. Then

Gˆ(a) = 1 [Qa ≥ 0] ,

and since G∗(a) = 1, it suﬃces to bound the probability that Qa < 0. The expected value of Qa is

given by

E[Qa] = E[Y − E[Y | X] | A = a]

= E (Y − E[Y | X])1 X ∈ S ∗ + (Y − E[Y | X])1 X S ∗ | A = a

(12)

= E[(Y − E[Y | X])1 X ∈ S ∗ | A = a]

= E[Y − E[Y | X] | X = x, A = a]P(X = x | A = a) dx
S˜

S˜ {x : x ∈ S ∗ ∧ p(x | a) > 0}

≥ αP(X = x | A = a) dx

(13)

S˜

= αP(X ∈ S ∗ | A = a)

(14)

≥ αβω,

(15)

19

where the second term in Eq. (12) is zero by Assumption 2; in the inequality on Eq. (13) we have used Assumptions 2 and 3 to lower-bound E[Y − E[Y | X] | X = x, A = a] (recall that G∗(a) = 1), and in Eq. (15) we have used Assumption 4 to lower bound P(X ∈ S ∗ | A = a) and we have used the deﬁnition of β as P(X ∈ S ∗).
Because f (x) = E[Y | X] by assumption, Qˆa is an average of i.i.d samples (ya j − f (xa j)) drawn from P(Y − E[Y | X] | A = a). Since ya j ∈ {0, 1} and f (xi, j) ∈ [0, 1], each sample is bounded by the interval [−1, 1]. By Hoeﬀding’s inequality, the probability of misclassiﬁcation is bounded as
P(Qˆ a ≤ 0) = P(E[Qa] − Qˆ a ≥ E[Qa]))
≤ exp −Rα2β2ω2 =: η. 2
since E[Qa] ≥ αβω. A symmetric argument holds for G∗(a) = 0.

Proof of Theorem 2. After the ﬁrst iteration, Algorithm 1 returns a grouping Gˆ and a subset Sˆ . We are interested in the quality of this set Sˆ , relative to the optimal set S ∗. For an agent a, let gˆa Gˆ(a), and let ga G∗(a). For notational convenience, let ra j ya j − f (xa j) be the residual in predicting the treatment. We now have NR samples of the form ra j · gˆa. We let Qˆ be the empirical expected conditional covariance computed from samples, so that

Qˆ(S , G∗) =

1

ra j · ga · 1 xa j ∈ S ,

(16)

a, j 1 xa j ∈ S a, j

and similarly for Qˆ(S , Gˆ). Let Sˆ = arg maxS Qˆ(S , Gˆ), then we can expand by adding and subtracting identical terms

Q(S ∗, G∗) − Q(Sˆ , G∗)

=[Q(S ∗, G∗) − Qˆ(S ∗, Gˆ)] + [Qˆ(S ∗, Gˆ) − Qˆ(Sˆ , Gˆ)] + [Qˆ(Sˆ , Gˆ) − Q(Sˆ , G∗)]

≤[Q(S ∗, G∗) − Qˆ(S ∗, Gˆ)] + [Qˆ(Sˆ , Gˆ) − Q(Sˆ , G∗)]

(17)

=[Q(S ∗, G∗) − Qˆ (S ∗, G∗)] + [Qˆ (S ∗, G∗) − Qˆ (S ∗, Gˆ)]+

[Qˆ(Sˆ , Gˆ) − Qˆ(Sˆ , G∗)] + [Qˆ(Sˆ , G∗) − Q(Sˆ , G∗)],

(18)

where in Eq. (17) we have used the fact that Sˆ is the maximizer of Qˆ(S , Gˆ) in our hypothesis class, and the assumption that S ∗ is in our hypothesis class. We will bound these terms in order.

Bounding the ﬁrst term of Eq. (18): For any S , let NS be the number of samples xa j ∈ S . Since ra j · ga ≤ 1, we have by Hoeﬀding’s inequality that for any 0 > 0,

P(Q(S ∗, G∗) − Qˆ (S ∗, G∗) > 0) ≤ exp − NS2∗ 02  = exp − β N2R 02  , (19)

where we have used the fact that NS ∗ = β NR, by deﬁnition of β .

Bounding the second, third terms of Eq. (18): For any S ,

Qˆ (S , G∗) − Qˆ (S , Gˆ) = 1 ra j · (ga − gˆa) · 1 xa j ∈ S NS a, j

1

≤

ra j · |(ga − gˆa)| · 1 xa j ∈ S

NS a, j

1

RN

 

1

 

≤

|ga − gˆa| = 

|ga − gˆa| ,

(20)

NS a, j

NS  N a



where for simplicity, there are R samples per agent by assumption. Note that |ga − gˆa| is 1 if ga is
misclassiﬁed, and 0 otherwise. Each gˆa is independently distributed, and by Lemma 2, pa P(ga gˆa) ≤ η. Then E[ a |ga − gˆa|] = a pa, and Var[ a |ga − gˆa|] = a pa(1 − pa) ≤ Nη(1 − η), recalling that η < 1/2. By Chebyshev’s inequality

 

 

η(1 − η)

P  |ga − gˆa| − pa > N 1 ≤ N 2 ,

(21)

a

a

1

20

so that we can bound N1 a |ga − gˆa| with high probability by 1 + η. Note that by the same logic above, the second and third terms of Eq. (18) can be bounded together by observing that their sum is

bounded by

RN

RN

 

1

 

+ 

|ga − gˆa|

NS ∗ NSˆ  N a



Bounding the fourth term: Consider G∗ : A → {0, 1} to be ﬁxed, and let S be our hypothesis class of functions S : X → {0, 1}. For any S : X → {0, 1}, deﬁne f (Z) (Y − E[Y | X]) · G∗(A) · S (X),
then (deﬁning n as NR, our total number of samples)

Qˆ(Sˆ , G∗) =

1

 1 n

 



f (Z ) ,

and

Q(Sˆ , G∗) = 1 E[ f (Z)],

ˆ (Sˆ )  n

i 

P(Sˆ )

P

i=1

where to deal with the fact that P(S ) Pˆ(S ), we can write that

Qˆ(Sˆ , G∗) − Q(Sˆ , G∗)

1

1 1n

1  1 n

 

= Pˆ (Sˆ ) − P(Sˆ ) n f (Zi) + P(Sˆ )  n f (Zi) − E[ f (Z)]

i=1

i=1

1

1

1  1 n

 

≤

−

+



Pˆ (Sˆ ) P(Sˆ ) P(Sˆ )  n

f (Zi) − E[ f (Z)] ,

i=1

and we can bound the last term by standard learning theory arguments. In particular, this can be seen as a weighted loss, where f (Z) = W(Z) · S (Z), for W (Y − E[Y | X]) · G(A). Note that each S ∈ S deﬁnes some f ∈ F . In particular, we can write that with probability at least 1 − δ1, for all f ∈ F , we have

 1 n

 

sup 

f (Zi) − E f (Z) ≤ 2R(F , n) + 4 (2 ln(4/δ1))/n

f ∈F  n i=1



≤ 2R(S, n) + 4 (2 ln(4/δ1))/n

(22)

where we have used the fact that | f (Z)| ≤ 1, and where R is the Rademacher complexity (Thm. 26.5.2 of Shalev-Shwartz and Ben-David, 2014). In the second line, we use the fact that F = φ ◦ S , where φ(S ) = W · S is a 1-Lipschitz function, since |W| ≤ 1. By the contraction lemma (Lemma 26.9 of Shalev-Shwartz and Ben-David, 2014), we have it that R(F , n) ≤ R(S, n).
Combining bounds: By the union bound, the bounds in Eq. (19), Eq. (21) and Eq. (22) hold with probability at least 1 − δ, where

δ = exp − β N2R 02  + η(1N−2η) + δ1. (23)
1
Hence, with probability at least 1 − δ, we can bound Eq. (18) by

Q(S ∗, G∗) − Q(Sˆ , G∗) ≤ 0 + 1 + 1 (η + 1) + 1 − 1

ββ

Pˆ (Sˆ ) P(Sˆ )

+ (1/P(Sˆ )) 2R(S, NR) + 4 (2 ln(4/δ1))/NR ,

where we have used NR/NS ∗ = 1/β and NR/NSˆ = 1/β, by deﬁnition of β and because Sˆ is chosen to be a β-fraction of the dataset. Finally, we simplify by setting the three terms in Eq. 23 to be equal, and expressing 0 and 1 in terms of δ:

2 ln(3/δ)

3η(1 − η)

0 = β NR , 1 =

δN , δ1 = δ/3

from which the desired result follows.

21

B Semi-Synthetic Experiments: Additional Details
B.1 Semi-Synthetic Setup Details
Additional Dataset Details: The Stanford study on human predictions of recidivism only included participants who passed both attention checks in the assessment. Participants were provided information about the baseline recidivism rates and charge. We use the responses from participants who were not given feedback so that decisions from the same participant are independent and identically distributed. The participants provide probabilistic predictions, which we convert into binary decisions by thresholding at 50%. By construction of the survey, no participant can predict exactly 50%.
Semi-Synthetic Policy Generation (Details): For both alternative policies (as logistic regression models), we add a new binary feature corresponding to the binary region indicator (1 if the sample falls in the region, 0 otherwise). The weight of this new coeﬃcient is 1.5 for the alternative policy and 0 for the base policy. We give additional details on the diﬀerences between the base and alternative policies below:
• Drug Possession: On average, the base and alternative policies predict recidivism 56% and 61% of the time, respectively, outside the subset of drug-possession charges. Within the subset, they predict recidivism 60% and 87% of the time, respectively.
• Misdemeanor and Age at Most 35: On average, the base and alternative policies predict recidivism 56% and 62% of the time, respectively, outside the subset of misdemeanor charges for individuals with age at most 35. Within the subset, the averages are 48% and 79% respectively.
B.2 Baseline Details
In this section, we describe implementation details for the two baselines discussed in section 4.1, two additional baselines we created, limitations of the baselines, and additional evaluation on the semi-synthetic set-ups.
Direct Model: The agents are included in a one-hot encoding, with the last agent dropped to prevent co-linearity. The agents are partitioned into two equal-size groups based on their logistic regression coeﬃcients. Logistic regressions (LR), ridge regressions (RR), decision trees (DT), and random forests (RF) are via scikit-learn (Pedregosa et al., 2011). For logistic regressions, we tune the L2 regularization constant (C) among 10, 1, 0.1, 0.01, 0.001, 0.0001, and 0.00001. For ridge regressions, we tune the L2 regularization constant (α) among 0.01, 0.1, 1, 10, and 100. For decision trees and random forests, we tune the minimum samples per leaf among 10, 25, and 100. For random forests, we also tune the number of trees among 10, 25, and 100.
TARNet: Our optimizer performs stochastic gradient descent with zero momentum on the binary cross entropy loss function from PyTorch (Paszke et al., 2019). We tune the learning rate among 0.0001, 0.001, and 0.01 for the drug possession set-up and among 0.0001, 0.0005, 0.001, 0.005, and 0.01 for the misdemeanor set-up. We tune the number of units in each of the layers among 10 and 20 for the drug possession set-up and among 10, 20, and 30 for the misdemeanor set-up. Finally, we tune dropout among 0.05 and 0.25. We train the model for 200 epochs and use the epoch that has the lowest validation loss.
U-learners: A standard U-learner (Nie and Wager, 2017) is designed to estimate the eﬀect of a binary treatment by predicting treatment from features T = f (X), predicting outcome from features Y = g (X), and then predicting the ratio between the residuals of the previous two models from features (Y − gˆ (X)) / T − fˆ (X) = h (X). To adapt this model to multiple discrete treatments, we ﬁt fi j for every pair of agents Ai and A j using only the samples from those two agents. This means there is a separate hi j for each pair of agents. The amount of variation for a sample x is deﬁned as
i, j∈A |hˆi j(x)|. As in the direct models, a model is ﬁt to predict this quantity, and the top β quantile of these predictions is used to identify region membership. To create Gˆ, we ﬁrst identify a pair of agents that are most dissimilar using the sum of absolute values of the diﬀerence between that pair across all samples within the region. All the other agents are added to the partition based on which provider in the starting pair they are closer to.
22

Causal Forest Adaptations: A causal forest (Wager and Athey, 2018) resembles a random forest with the splits chosen instead to maximize the diﬀerence in the eﬀect of the treatment on outcome. A causal forest can only predict the diﬀerence between two treatments. The naïve adaptation of ﬁtting O |A|2 causal forests between each pair of agents becomes computationally infeasible when |A| is large. Instead, we use the following heuristic to learn a partition with causal forests: Initialize the groupings G0 and G1 with a few agents based on the U-learner predictions. We start with the most dissimilar pair as deﬁned above. Then, if a provider is much closer to one of the two starting providers (deﬁned as the sum of absolute diﬀerences being at most a tenth of the sum for the other pair), add that provider to the group with the closer provider. To add agent a, compute a causal forest for the diﬀerence in treatment choice between G0 ∪ a and G1 and the diﬀerence between G0 and G1 ∪ a. We use the EconML implementation of causal forests (Research, 2019). For hyperparameter tuning, predictions Ua0,a1 (X) from the U-learner are used as "oracle" predictions. The "oracle" predictions between groups G0, G1 are deﬁned as a0∈G0,a1∈G1 Ua0,a1 (X). We compare each pair of causal forests and add a to G0 if the former has a larger sum of the absolute diﬀerence across all samples. Otherwise, add a to G1. Repeat until all agents have been added to a group. A region model is applied to the predictions from the ﬁnal causal forest.
Hyperparameter Tuning: For both the baselines and our algorithm, we tune the hyperparameters for each piece of the model separately, e.g. each decision tree piece of the model is tuned separately. The data is split into train, validation, and test 60/20/20 stratiﬁed by provider, guaranteeing at least 1 sample per provider in each of the sets. The same validation set is used for all steps in a single model. After the best hyperparameter is selected, the model is retrained on both the training and validation data.
Baseline Limitations: A limitation of the TARNet baseline is that it does not learn a partition of providers. A limitation of the other baselines except for the U-learner is that the provider partition is based on all samples, not just samples within the region.
Additional Baseline Results: Other metrics we consider are the precision and recall of the region and the accuracy of the partition. To compute accuracy, we compare the learned groups 0 and 1 with the true groups 0 and 1, respectively, and with the true groups 1 and 0, respectively, and take the higher of the two. Figure 3 shows that the ridge regression model we highlight in the main paper ﬁts the region better than the other two models for large numbers of agents for the drug possession set-up. On the other hand, for the misdemeanor under age 35 set-up, Figure 4 shows that the random forest model ﬁts the region better. TARNet is the best-performing baseline for the drug possession set-up, while the direct model is the best-performing baseline for the misdemeanor set-up. Our model outperforms both in all cases. The partition is diﬃcult for any model to learn in both set-ups when the number of agents increases. Despite this challenge, our algorithm is still able to recover the region reasonably well.
B.3 Convergence Analysis
We derive a generalization bound on the performance of the iterative algorithm after one iteration in Section 3.4. Here we examine empirical performance across additional iterations. To do so, we compute the AUC of the region in each iteration and how many iterations the algorithm needs to converge. Note that because AUC is not the objective that we are optimizing, there are no guarantees that it increases monotonically across iterations. As seen in Figures 5-6, in both semi-synthetic set-ups, for small numbers of agents, convergence is almost immediate. For larger numbers of agents, the algorithm almost always converges. The exceptions are where the algorithm enters what appears to be a cycle. This problem can likely be addressed by re-initializing the model if the algorithm is stuck in a cycle.
B.4 Robustness Analysis
We examine robustness of the model to violations of the assumption that there are 2 agent groups. For instance, it may be more realistic for agents to have a wide spectrum of preferences. We represent this in our semi-synthetic set-up by varying the coeﬃcient on the region indicator variable described in Appendix B.1 in equally spaced steps between -1.5 and 1.5. The number of agents is held constant at 40. The region and the policy outside the region are the same as before. This corresponds to varying the average predicted probabilities of recidivism within the subset in the groups between around 30%
23

Region AUC

Ridge regression region
0.8 0.6 0.4

Decision tree region

Random forest region

Region precision

0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2

Region recall

Partition accuracy

0.9

Direct TARNet

Iterative (Ours)

0.8

U-learner

Causal forest

0.7

0.6

0.5 0

20

40

60

80

0

20

40

60

80

0

20

40

60

80

Num agents

Num agents

Num agents

Figure 3: Comparison to baseline approaches for drug possession semi-synthetic set-up. Uncertainty bands represent 95% intervals for the mean derived via bootstrapping computed using seaborn (Waskom, 2021). Our method with a ridge regression region model is the best at identifying the region across all combinations of outcome and region models.

and 86% for the drug possession set-up and between around 17% and 79% for the misdemeanor under age 35 set-up. We compare our method with the direct baseline with all region models. As seen in Figure 7, the ridge regression model is most robust to this violation in the drug possession set-up. In fact, performance actually improves when the number of agent groups increases. We hypothesize this may occur for two reasons: 1. The ridge regression model parametrizes the region best, as that was also the globally best model in the 2-group speciﬁcation. 2. The agents can still be divided into two groups based on above and below average preference, so this violation of the 2-group assumption is relatively simple. Figure 8 shows that all 3 region models are somewhat robust to increasing numbers of groups for both the iterative and direct baseline in the misdemeanor under 35 set-up.

24

Region AUC

1.0 Ridge regression region
0.8 0.6 0.4

Decision tree region

Random forest region

Region precision

1.0 0.8 0.6 0.4 0.2
1.0 0.8 0.6 0.4 0.2

Region recall

Partition accuracy

0.9

Direct TARNet

Iterative (Ours)

0.8

U-learner

Causal forest

0.7

0.6

0.5 0

20

40

60

80 0

20

40

60

80 0

20

40

60

80

Num agents

Num agents

Num agents

Figure 4: Comparison to baseline approaches for misdemeanor semi-synthetic set-up. Uncertainty bands represent 95% intervals for the mean derived via bootstrapping computed using seaborn (Waskom, 2021). Our method with a random forest region model is the best at identifying the region across all combinations of outcome and region models.

25

Region AUC

Region AUC

1.0 Ridge regression region 10 agents
0.8 0.6 0.4 0.2
1.0 Decision tree region 10 agents
0.8 0.6 0.4 0.2
1.0 Random forest region 10 agents
0.8 0.6 0.4 0.2
0 15 3I0teratio4n5 60 75

Ridge regression region 20 agents Decision tree region 20 agents Random forest region 20 agents
0 15 3I0teratio4n5 60 75

Ridge regression region 40 agents Decision tree region 40 agents Random forest region 40 agents
0 15 3I0teratio4n5 60 75

Ridge regression region 87 agents Decision tree region 87 agents Random forest region 87 agents
0 15 3I0teratio4n5 60 75

Region AUC

Figure 5: Convergence of iterative algorithm in drug possession semi-synthetic set-up. Each of the 10 lines in each plot represents a dataset generated from a diﬀerent random seed. Region AUC is computed after the partition is updated. Algorithm terminates when partition does not change. Although the iterative algorithm was run for up to 100 iterations, the number of iterations plotted was truncated to show all terminations. 2 and 5 agents are omitted because they closely resemble the plots for 10 agents.

Region AUC

Region AUC

1.0 Ridge regression region 10 agents
0.8 0.6 0.4
1.0 Decision tree region 10 agents
0.8 0.6 0.4
1.0 Random forest region 10 agents
0.8 0.6 0.4
0 10 2I0teratio3n0 40 50

Ridge regression region 20 agents Decision tree region 20 agents Random forest region 20 agents
0 10 2I0teratio3n0 40 50

Ridge regression region 40 agents Decision tree region 40 agents Random forest region 40 agents
0 10 2I0teratio3n0 40 50

Ridge regression region 87 agents Decision tree region 87 agents Random forest region 87 agents
0 10 2I0teratio3n0 40 50

Region AUC

Figure 6: Convergence of iterative algorithm in misdemeanor semi-synthetic set-up. See Figure 5 for description.

26

Region AUC

Region precision

1.0 0.9 0.8 0.7 0.6 0.5 0.4 1.0 0.9 0.8 0.7 0.6 0.5 0.4 1.0 0.9 0.8 0.7 0.6 0.5 0.4 2

Ridge regression region

4 Num g6roups 8

10 2

Decision tree region

Random forest region

Model Iterative Direct

4 Num g6roups 8

10 2

4 Num g6roups 8

10

Region recall

Figure 7: Robustness to the assumption that there are 2 agent groups in the drug possession semisynthetic set-up. The 40 agents in each set-up are roughly equally divided into 2, 3, 5, and 10 groups. Uncertainty bands represent 95% intervals for the mean derived via bootstrapping computed using seaborn (Waskom, 2021).

Region AUC

Region precision

1.0 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 2

Ridge regression region

4 Num g6roups 8

10 2

Decision tree region

Random forest region

Model Iterative Direct

4 Num g6roups 8

10 2

4 Num g6roups 8

10

Region recall

Figure 8: Robustness to the assumption that there are 2 agent groups in the misdemeanor under age 35 semi-synthetic set-up. See Figure 7 for description.

27

C Diabetes Experiment: Additional Details
Data Details, Train/Validation/Test Split: The data set of de-identiﬁed health insurance claims was provided by a large insurance company. That company obtained the relevant consent from individuals to use their data, and gave us permission to use the data for research purposes. A waiver of informed consent was obtained in compliance of HIPAA.
We require at least 3 years of observation before the ﬁrst diabetes treatment to ensure that the observed treatment is indeed ﬁrst-line. We also require at least one diagnosis code of diabetes mellitus and at least one A1C measurement at least 6.5 prior to ﬁrst-line treatment. Patients who had at least one diagnosis code related to type 1 diabetes mellitus prior to ﬁrst-line treatment or gestational diabetes (pregnancy, neonatal diabetes, or diabetes of the young) in the 1 year prior to ﬁrst-line treatment are excluded. The exclusion of gestational diabetes is important to prevent confounding since those patients may see specialized providers and receive diﬀerent treatments. Patients who received more than one ﬁrst-line treatment or any treatment besides metformin, DPP-4 inhibitors, or sulfonylureas are also excluded.
We only include agents with at least 4 samples. For each agent, 37.5% of samples are placed in the training set, 12.5% in validation, and 50% in test to ensure the test set is suﬃciently large for computing the partition in L Sˆ on the test set. We require at least one sample per agent in each of the training and validation sets and two in the test set. Treatment date is converted to seconds, and all features are normalized to have mean 0 and standard deviation 1.
Selecting the outcome model: For the outcome model, we consider three hypothesis classes: logistic regressions, decision trees, and random forests. The models are tuned as described in Appendix B.2. We select a random forest for three reasons: 1. Random forests have the highest AUC on the validation set, as shown in Table 2. 2. Random forests are better calibrated in the discovered region, as shown in Table 3. 3. The partial dependence plots in Figure 9 show reasonable relations between each feature and the outcome. These results are reported for the fold that was selected based on signiﬁcance and calibration statistics.

Table 2: Validation AUCs for outcome model for diabetes experiment.

Model

Test AUC

Logistic regression Decision tree Random forest

0.6957 0.7351 0.8283

Table 3: Calibration of outcome model on regions selected with each outcome model for diabetes experiment. Comparison of average true and predicted outcomes in region among training samples.

Model

True average Predicted average

Logistic regression Decision tree Random forest

0.2485 0.2023 0.2343

0.1839 0.1688 0.2226

Visualizing the Region: To describe the region in an interpretable way, we use a decision tree for the region model h(x) described in Algorithm 1 with β = 0.25 and a maximum of 5 iterations. The minimum number of samples per leaf for the decision tree region model is tuned among larger choices (50, 100, 200) for a more interpretable region. Our algorithm outputs a decision tree h(x), shown in Figure 10a and a threshold of b = 0.0741 in the training Q(S , G) values. The region S = {x ∈ X; h(x) ≥ b} consists of the two nodes indicated in red. To verify the generalizability of this result, we use a held-out test set to compute the metric L Sˆ . In the test set, this metric is also greater
on these two nodes than on all other nodes in the decision tree.
We zoom into the ﬁrst region in Figure 10b. We only include decisions made by providers with at least two samples in that node. Providers in group G = 0 prefer to initiate treatment with metformin, while providers in group G = 1 prefer to initiate with sitagliptin, glipizide, glimepiride, or glyburide. This

28

Partial dependence

60 80 100 120 0.6 0.8 1.0 1.2

eGFR

Creatinine

Partial dependence

0.0 0.2 0.4 0.6 0.8 1.0 Heart disease

1.40 1.45 1.50 Treatment date 1e9

Figure 9: Partial dependence plot of random forest outcome model in diabetes experiment

group-speciﬁc bias (blue vs. orange lines) generally holds across the range of GFR and creatinine values, indicating that this preference is not explained by the patient’s features.
Assessing Stability: We also measure the stability of the region and grouping identiﬁed by our method by performing cross-validation. We split the training and validation data into 4 equally sized portions and assign 1 portion as the validation set. The only overlap between the 4 validation folds are samples that belong to agents with fewer than 4 observations in the training and validation set to ensure that all validation folds contain at least 1 sample per agent. (1) A region is stable if points that are selected for the region from most training folds are also selected when they are in the validation fold. If we look at the points that are in 1 validation fold, among the 461 points that are in the region for 2 to 3 training folds, 309 are also selected when they belong to the validation fold. (2) We also examine the consistency of the test region. With an average test region size of 553.75 points, 350.75 points are in the test region for at least 3 folds (each point in only 3 folds contributes weight 0.75). (3) We assess stability of the grouping by examining whether pairs of providers are consistently on the same or opposite sides of the grouping. Among the 12,181 pairs of providers that have at least 1 training, validation, or test sample in the region in at least 3 of the folds, 10,251 pairs have the same relationship in at least 3 of the folds. All 3 of these statistics suggest our algorithm arrives at similar regions and groupings regardless of how the training and validation samples are split.
D Additional Real-Data Experiment: First-Line Parkinson’s Treatment
Context and Data: The Parkinson’s Progression Markers Initiative (PPMI) is an observational study that follows Parkinson’s patients starting within 2 years of diagnosis in their de novo cohort (Marek et al., 2011). The study collected data across the US, Europe, Israel, and Australia between 2010 and 2018. We examine decisions between the two most common ﬁrst-line treatments, levodopa and rasagiline. Clinical trials are interested in assessing the eﬀects of these treatments (Group et al., 2014). For context, we include age, disease duration, and a motor assessment (the total from part II and III of the Movement Disorder Society Uniﬁed Parkinson’s Disease Rating Scale (MDS-UPDRS)) (Goetz et al., 2007). The features are normalized to have mean 0 and standard deviation 1. The cohort consists of 260 patients at 23 study sites, which we use as “agents” A. Note that while treatment decisions are not made at study sites, these sites capture rough geographic locations across which there may be heterogeneity in treatment. We ﬁx the outcome model to be a decision tree and the region size β = 0.25.
29

Frequency

Node 0 eGFR > 71.5?

No
Node 1 Q(S , G) = 0.097 G=0 MET: 169/205 (82%) G=1 MET: 110/213 (52%)

Yes

Node 2 Creatinine > 0.815?

No

Yes

...

Node 6

eGFR > 98.5?

No

Node 7 Creatinine > 0.995?

No

Yes

Yes
Node 10 Q(S , G) = 0.049 G=0 MET: 113/117 (97%) G=1 MET: 69/88 (78%)

Node 8 Q(S , G) = 0.002 G=0 MET: 114/126 (90%) G=1 MET: 97/105 (92%)

Node 9 Q(S , G) = 0.000 G=0 MET: 83/93 (89%) G=1 MET: 82/96 (85%)

(a)

Percentage Metformin

Percentage Metformin

100 Node 1 50

80

40

60

30

40

20

20

10

0 30

35

40

45

50

55

60

65

70 0

eGFR

Node 1

100

50

80

40

60

30

40

20

20

10

0 0.6

0.8

1.0

1.2

1.4

1.6

1.8

2.0 0

Creatinine

(b)

Frequency

Figure 10: (a) Decision tree identifying regions of disagreement for ﬁrst-line treatment decisions in diabetes, indicated in red. MET: Metformin. G=0 denotes the group found to prefer metformin as an initial treatment, and G=1 indicates the group with the opposing preference. Numbers at leaves computed on a held-out test set. Q(S , G) values are the L Sˆ metric computed on the test set; number
of samples are computed using only providers with at least two samples in the region of heterogeneity. (b) Variation in Node 1 of the tree in Figure 10a, one of the regions of disagreement in the diabetes dataset. The colors denote group membership: blue is G = 0, and orange is G = 1. The lines indicate the proportion of decisions in each bin where the agent prescribed metformin. The gap between the lines illustrates that the group-speciﬁc bias towards metformin generally holds across patient features. Only agents with at least two samples in the region are included. The histograms show the total number of samples in each bin.

Interpretation of Results: The decision tree in Figure 11 shows that the selected region includes patients who are either above age 70 or both above age 62 and diagnosed at least around 1.5 years prior to ﬁrst-line treatment. The region in the test set includes 13 patients in group 0 and 40 patients in group 1. Patients in the region in the test set are older on average (72.3 versus 64.3 for entire test set), have had Parkinson’s longer (1.9 years versus 1.5 years) and have higher MDS-UPDRS part II + III scores (41.7 vs 37.5). Empirically, when stratiﬁed on age or MDS-UPDRS, patients in the top 33rd percentile (67 for age, 29 for MDS-UPDRS) are more likely to be given levodopa, while patients in the lower 67th are more likely to receive rasagiline.
This region likely captures where treatment switches from rasagiline to levodopa. Our results align with clinical guidelines that levodopa may be better for patients with more motor and cognitive impairment due to fewer side eﬀects (Muzerengi and Clarke, 2015), as these patients also tend to be older and have had the disease longer.
Assessing Signiﬁcance: In Table 4, we assess whether the region S identiﬁes variation in held-out data better than a randomly selected region. As in the diabetes experiment, we observe that the test statistics is close to the training statistic and more than 2 standard deviations from the average test statistic for random regions of the same size, suggesting the discovered region of heterogeneity generalizes beyond the training set.
PPMI Disclaimer: Data used in the preparation of this article were obtained from the Parkinson’s Progression Markers Initiative (PPMI) database (www.ppmi-info.org/data). For up-to-date information on the study, visit www.ppmi-info.org. PPMI – a public-private partnership – is funded by the Michael J. Fox Foundation for Parkinson’s Research and funding partners, including abbvie, AcureX therapeutics, Allergan, Aligning Science Across Parkinson’s, Avid Radiopharmaceuticals, Bial Biotech, Biogen, BioLegend, Bristol Myers Squibb, Calico, Celgene, Dacapo brainscience,
30

Node 0 Age > 70.502?

No

Yes

Node 1 Disease duration
> 1.418 years

Node 2 Q(S , G) = 0.272 G = 0 LEV: 0/7 (0%) G = 1 LEV: 24/27 (88%)

No

Yes

...

Node 3

Age > 62.461

No

Yes

Node 4 Q(S , G) = 0.000 G = 0 LEV: 1/6 (17%) G = 1 LEV: 2/5 (40%)

Node 5 Q(S , G) = 0.047 G = 0 LEV: 2/6 (33%) G = 1 LEV: 8/13 (62%)

Figure 11: Decision tree identifying regions of disagreement for ﬁrst-line treatment decisions in Parkinson’s, indicated in red. LEV: Levodopa. G=1 denotes the group found to prefer levodopa as an initial treatment, and G=0 indicates the group with the opposing preference. See Figure 10a for
explanation.

Table 4: Objective values L(S ) for the learned region on the training and test datasets, along with the distribution of values for randomly generated regions S rand given as mean (standard deviation).

Metric Subset Value

L(Sˆ ) L(Sˆ )
L(S rand)

Train Test Test

0.2743 0.2170 0.1200 (0.0222)

Denali, Edmond J. Safra Philanthropic Foundation, 4D Pharma PLC, GE Healthcare, Genentech, GlaxoSmithKline, Golub Capital, Handl Therapeutics, insitro, Janssen Neuroscience, Lilly, Lundbeck, Merck, Meso Scale Discovery, Neurocrine biosciences, Pﬁzer, Piramal, Prevail Therapeutics, Roche, Sanoﬁ Genzyme, Servier, Takeda, Teva, ucb, verily, Voyager Therapeutics, and Yumanity Therapeutics.

31

