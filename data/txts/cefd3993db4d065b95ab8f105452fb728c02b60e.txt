Can We Automate Scientiﬁc Reviewing?

Weizhe Yuan Carnegie Mellon University weizhey@cs.cmu.edu

Pengfei Liu ∗ Carnegie Mellon University
pliu3@cs.cmu.edu

Graham Neubig Carnegie Mellon University gneubig@cs.cmu.edu

arXiv:2102.00176v1 [cs.CL] 30 Jan 2021

TL;QR
This paper proposes to use NLP models to generate reviews for scientiﬁc papers . The model is trained on the ASAPReview dataset and evaluated on a set of metrics to evaluate the quality of the generated reviews . It is found that the model is not very good at summarizing the paper , but it is able to generate more detailed reviews that cover more aspects of the paper than those created by humans . The paper also ﬁnds that both human and automatic reviewers exhibit varying degrees of bias and biases , and that the system generate more biased reviews than human reviewers.(“Too Long; Quick Read”, this paragraph, is generated by our system.)
Abstract
The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientiﬁc publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a signiﬁcant challenge. In this work, we ask the question “can we automate scientiﬁc reviewing?”, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate ﬁrst-pass peer reviews for scientiﬁc papers. Arguably the most difﬁcult part of this is deﬁning what a “good” review is in the ﬁrst place, so we ﬁrst discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted
∗Corresponding author.

summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We ﬁnally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1
1 Introduction
The number of published papers is growing exponentially (Tabah, 1999; De Bellis, 2009; Bornmann and Mutz, 2015). While this may be positively viewed as indicating acceleration of scientiﬁc progress, it also poses great challenges for researchers, both in reading and synthesizing the relevant literature for one’s own beneﬁt, and for performing peer review of papers to vet their correctness and merit. With respect to the former, a large body of existing work explores automatic summarization of a paper or a set of papers for automatic survey generation (Mohammad et al., 2009; Jha et al., 2013, 2015b,a; Yasunaga et al., 2019b; Cohan et al., 2018b; Xing et al., 2020). However, despite the fact that peer review is an important, but laborious part of our scientiﬁc process, automatic systems to aid in the peer review process remain relatively underexplored. Bartoli et al. (2016) investigated the feasibility of generating reviews by surface-level term replacement and sentence reordering, and Wang et al. (2020) (contempora-

neously and independently) propose a two-stage information extraction and summarization pipeline to generate paper reviews. However, both do not extensively evaluate the quality or features of the generated review text.
In this work, we are concerned with providing at least a preliminary answer to the ambitious overarching question: can we automate scientiﬁc reviewing? Given the complexity of understanding and assessing the merit of scientiﬁc contributions, we do not expect an automated system to be able to match a well-qualiﬁed and meticulous human reviewer at this task any time soon. However, some degree of review automation may assist reviewers in their assessments, or provide guidance to junior reviewers who are just learning the ropes of the reviewing process. Towards this goal, we examine two concrete research questions, the answers to which are prerequisites to building a functioning review assistant: Q1: What are the desiderata of a good automatic reviewing system, and how can we quantify them for evaluation? Before developing an automatic review system, we ﬁrst must quantify what constitutes a good review in the ﬁrst place. The challenge of answering this question is that a review commonly involves both objective (e.g. “lack of details necessary to replicate the experimental protocol”) and subjective aspects (e.g. “lack of potential impact”). Due to this subjectivity, deﬁning a “good” review is itself somewhat subjective.
As a step towards tackling this challenge, we argue that it is possible to view review generation as a task of aspect-based scientiﬁc paper summarization, where the summary not only tries to summarize the core idea of a paper, but also assesses speciﬁc aspects of that paper (e.g. novelty or potential impact). We evaluate review quality from multiple perspectives, in which we claim a good review not only should make a good summary of a paper but also consist of factually correct and fair comments from diverse aspects, together with informative evidence.
To operationalize these concepts, we build a dataset of reviews, named ASAP-Review1 from machine learning domain, and make ﬁne-grained annotations of aspect information for each review, which provides the possibility for a richer evaluation of generated reviews.
1ASpect-enhAnced Peer Review dataset

Q2: Using state-of-the-art NLP models, to what extent can we realize these desiderata? We provide an initial answer to this question by using the aforementioned dataset to train state-of-the-art summarization models to generate reviews from scientiﬁc papers, and evaluate the output according to our evaluation metrics described above. We propose different architectural designs for this model, which we dub ReviewAdvisor (§4), and comprehensively evaluate them, interpreting their relative advantages. Lastly, we highlight our main observations and conclusions: (1) What are review generation systems (not) good at? Most importantly, we ﬁnd the constructed automatic review system generates non-factual statements regarding many aspects of the paper assessment, which is a serious ﬂaw in a high-stakes setting such as reviewing. However, there are some bright points as well. For example, it can often precisely summarize the core idea of the input paper, which can be either used as a draft for human reviewers or help them (or general readers) quickly understand the main idea of the paper to be reviewed (or pre-print papers). It can also generate reviews that cover more aspects of the paper’s quality than those created by humans, and provide evidence sentences from the paper. These could potentially provide a preliminary template for reviewers and help them quickly identify salient information in making their assessment. (2) Will the system generate biased reviews? Yes. We present methods to identify and quantify potential biases in reviews (§5.3), and ﬁnd that both human and automatic reviewers exhibit varying degrees of bias. (i) regarding native vs. non-native English speakers: papers of native English speakers tend to obtain higher scores on “Clarity” from human reviewers than non-native English ones,2 but the automatic review generators narrow this gap. Additionally, system reviewers are harsher than human reviewers when commenting regarding the paper’s “Originality” for non-native English speakers. (ii) regarding anonymous vs. non-anonymous submissions: both human reviewers and system reviewers favor non-anonymous papers, which have been posted on non-blind preprint
2Whether this actually qualiﬁes as “bias” is perhaps arguable. Papers written by native English speakers may be more clear due to lack of confusing grammatical errors, but the paper may actually be perfectly clear but give the impression of not being clear because of grammatical errors.

servers such as arXiv3 before the review period, more than anonymous papers in all aspects.
Based on above mentioned issues, we claim that a review generation system can not replace human reviewers at this time, instead, it may be helpful as part of a machine-assisted human review process. Our research also enlightens what’s next in pursuing a better method for automatic review generation or assistance and we summarize eight challenges that can be explored for future directions in §7.2.

2 What Makes a Good Peer Review?
Although peer review has been adopted by most journals and conferences to identify important and relevant research, its effectiveness is being continuously questioned (Smith, 2006; Langford and Guzdial, 2015; Tomkins et al., 2017; Gao et al., 2019; Rogers and Augenstein, 2020).
As concluded by Jefferson et al. (2002b): “Until we have properly deﬁned the objectives of peerreview, it will remain almost impossible to assess or improve its effectiveness.” Therefore we ﬁrst discuss the possible objectives of peer review.

2.1 Peer Review for Scientiﬁc Research
A research paper is commonly ﬁrst reviewed by several committee members who usually assign one or several scores and give detailed comments. The comments, and sometimes scores, cover diverse aspects of the paper (e.g. “clarity,” “potential impact”; detailed in §3.2.1), and these aspects are often directly mentioned in review forms of scientiﬁc conferences or journals.4
Then a senior reviewer will often make a ﬁnal decision (i.e., “reject” or “accept”) and provide comments summarizing the decision (i.e., a metareview).
After going through many review guidelines5 and resources about how to write a good review6

3https://arxiv.org/

4For example, one example from ACL can be found at:

https://acl2018.org/downloads/acl 2018 review form.html

5https://icml.cc/Conferences/2020/ReviewerGuidelines

https://NeurIPS.cc/Conferences/2020/PaperInformation/

ReviewerGuidelines,

https://iclr.cc/Conferences/2021/

ReviewerGuide

6https://players.brightcove.net/3806881048001/

rFXiCa5uY default/index.html?videoId=

4518165477001, https://soundcloud.com/nlp-highlights/

77-on-writing-quality-peer-reviews-with-noah-a-smith, https:

//www.aclweb.org/anthology/2020.acl-tutorials.4.pdf, https:

//2020.emnlp.org/blog/2020-05-17-write-good-reviews

we summarize some of the most frequently mentioned desiderata below:
1. Decisiveness: A good review should take a clear stance, selecting high-quality submissions for publication and suggesting others not be accepted (Jefferson et al., 2002a; Smith, 2006).
2. Comprehensiveness: A good review should be well-organized, typically starting with a brief summary of the paper’s contributions, then following with opinions gauging the quality of a paper from different aspects. Many review forms explicitly require evaluation of different aspects to encourage comprehensiveness.
3. Justiﬁcation: A good review should provide speciﬁc reasons for its assessment, particularly whenever it states that the paper is lacking in some aspect. This justiﬁcation also makes the review more constructive (another oft-cited desiderata of reviews), as these justiﬁcations provide hints about how the authors could improve problematic aspects in the paper (Xiong and Litman, 2011).
4. Accuracy: A review should be factually correct, with the statements contained therein not being demonstrably false.
5. Kindness: A good review should be kind and polite in language use.
Based on above desiderata, we make a ﬁrst step towards evaluation of reviews for scientiﬁc papers and characterize a “good” review from multiple perspectives.
2.2 Multi-Perspective Evaluation Given input paper D and meta-review Rm, our goal is to evaluate the quality of review R, which can be either manually or automatically generated. We also introduce a function DEC(D) ∈ {1, −1} that indicates the ﬁnal decision of a given paper reached by the meta-review: “accept” or “reject”. Further, REC(R) ∈ {1, 0, −1} represents the acceptance recommendation of a particular review: “accept,” “neutral,” or “reject (see Appendix A.3 for details).
Below, we discuss evaluation metrics that can be used to approximate the desiderata of reviews described in the previous section. And we have summarized them in Tab. 1.

Desiderata Decisiveness Comprehen. Justiﬁcation Accuracy
Others

Metrics
RACC ACOV AREC INFO SACC ACON ROUGE BERTScore

Range
[-1, 1] [0, 1] [0, 1] [0, 1] [0, 1] [0, 1] [0, 1] [-1, 1]

Automated
No Yes Yes No No No Yes Yes

Table 1: Evaluation metrics from different perspectives. “Range” represents the range value of each metric. “Automated” denotes if metrics can be obtained automatically.

2.2.1 D1: Decisiveness
First, we tackle the decisiveness, as well as accuracy of the review’s recommendation, through Recommendation Accuracy (RACC). Here we use the ﬁnal decision regarding a paper and measure whether the acceptance implied by the review R is consistent with the actual accept/reject decision of the reviewed paper. It is calculated as:
RAcc(R) = DEC(D) × REC(R) (1)
A higher score indicates that the review more decisively and accurately makes an acceptance recommendation.
2.2.2 D2: Comprehensiveness
A comprehensive review should touch on the quality of different aspects of the paper, which we measure using a metric dubbed Aspect Coverage (ACOV). Speciﬁcally, given a review R, aspect coverage measures how many aspects (e.g. clarity) in a predeﬁned aspect typology (in our case, §3.2.1) have been covered by R.
In addition, we propose another metric Aspect Recall (AREC), which explicitly takes the metareview Rm into account. Because the meta-review is an authoritative summary of all the reviews for a paper, it provides an approximation of which aspects, and with which sentiment polarity, should be covered in a review. Aspect recall counts how many aspects in meta-review Rm are covered by general review R, with higher aspect recall indicating better agreement with the meta-review.7
7Notably, this metric potentially biases towards high scores for reviews that were considered in the writing of the metareview. Therefore, higher aspect recall is not the only goal, and should be taken together with other evaluation metrics.

2.2.3 D3: Justiﬁcation
As deﬁned in §2.1, a good peer review should provide hints about how the author could improve problematic aspects. For example, when reviewers comment: “this paper lacks important references”, they should also list these relevant works. To satisfy this justiﬁcation desideratum, we deﬁne a metric called Informativeness (INFO) to quantify how many negative comments8 are accompanied by corresponding evidence.
First, let nna(R) denote the number of aspects in R with negative sentiment polarity. nnae(R) denotes the number of aspects with negative sentiment polarity that are supported by evidence. The judgement of supporting evidence is conducted manually (details in Appendix A.3). INFO is calculated as:
Info(R) = nnae(R) (2) nna(R)
And we set it to be 1 when there are no negative aspects mentioned in a review.
2.3 D4: Accuracy
We use two measures to evaluate the accuracy of assessments. First, we use Summary Accuracy (SACC) to measure how well a review summarizes contributions of a paper. It takes value of 0, 0.5, or 1, which evaluates the summary part of the review as incorrect/absent, partially correct, and correct. The correctness judgement is performed manually, with details listed in Appendix A.3.
INFO implicitly requires that negative aspects should be supported with evidence, ignoring the quality of this evidence. However, to truly help to improve the quality of a paper, the evidence for negative aspects should be factual as well. Here we propose Aspect-level Constructiveness (ACON), the percentage of the supporting statements nnae(R) that are judged as valid support by human annotators. If nnae(R) is 0, we set its ACON as 1. This metric will implicitly favor reviews that do not provide enough evidence for negative aspects. However, in this case, the INFO of those reviews will be rather low. The details of evaluating “validity” are also described in Appendix A.3.
8We only consider whether the reviewer has provided enough evidence for negative opinions since we ﬁnd that most human reviewers rarely provide evidence for their positive comments.

2.4 D5: Kindness
While kindness is very important in maintaining a positive research community, accurately measuring it computationally in a nuanced setting such as peer review is non-trivial. Thus, we leave the capturing of kindness in evaluation to future work.
2.5 Similarity to Human Reviews
For automatically generated reviews, we also use Semantic Equivalence metrics to measure the similarity between generated reviews and reference reviews. The intuition is that while human reviewers are certainly not perfect, knowing how close our generated reviews are to existing human experts may be informative. Here, we investigate two speciﬁc metrics: ROUGE (Lin and Hovy, 2003) and BERTScore (Zhang et al., 2019). The former measures the surface-level word match while the latter measures the distance in embedding space. Notably, for each source input, there are multiple reference reviews. When aggregating ROUGE and BERTScore, we take the maximum instead of average since it is not necessary for generated reviews to be close to all references.
3 Dataset
Next, in this section we introduce how we construct a review dataset with more ﬁne-grained metadata, which can be used for system training and the multiple perspective evaluation of reviews.
3.1 Data Collection
The advent of the Open Peer Review system9 makes it possible to access review data for analysis or model training/testing. One previous work (Kang et al., 2018) attempts to collect reviews from several prestigious publication venues including the Conference of the Association of Computational Linguistics (ACL) and the International Conference on Learning Representations (ICLR). However, there were not nearly as many reviews accumulated in OpenReview at that time10 and other private reviews only accounted for a few hundred. Therefore we decided to collect our own dataset Aspect-enhanced Peer Review (ASAP-Review).
We crawled ICLR papers from 2017-2020 through OpenReview11 and NeurIPS papers from
9https://openreview.net/ 10During that time, there are no reviews of ICLR from 2018 to 2020 nor reviews of NeurIPS from 2018 to 2019. 11https://openreview.net

2016-2019 through NeurIPS Proceedings.12 For each paper’s review, we keep as much metadata information as possible. Speciﬁcally, for each paper, we include following metadata information that we can obtain from the review web page:
• Reference reviews, which are written by a committee member.
• Meta reviews, which are commonly written by an area chair (senior committee member).
• Decision, which denotes a paper’s ﬁnal “accept” or “reject” decision.
• Other information like url, title, author, etc.
We used Allenai Science-parse13 to parse the pdf of each paper and keep the structured textual information (e.g., titles, authors, section content and references). The basic statistics of our ASAP-Review dataset is shown in Tab. 2.

Accept Reject Total Avg. Full Text Length Avg. Review Length # of Reviews # of Reviews per Paper

ICLR
1,859 3,333 5,192 7,398
445 15,728
3.03

NeurIPS
3,685 0
3,685 5,916
411 12,391
3.36

Both
5,544 3,333 8877 6782
430 28,119
3.17

Table 2: Basic statistics of ASAP-Review dataset. Note that NeurIPS only provide reviews for accepted papers to the public.

3.2 Aspect-enhanced Review Dataset
Although reviews exhibit internal structure, for example, as shown in Fig. 3, reviews commonly start with a paper summary, followed by different aspects of opinions, together with evidence. In practice, this useful structural information cannot be obtained directly. Considering that ﬁne-grained information about the various aspects touched on by the review plays an essential role in review evaluation, we conduct aspect annotation of those reviews. To this end, we ﬁrst (i) introducing an aspect typology and (ii) perform human annotation.
3.2.1 Aspect Typology and Polarity
We deﬁne a typology that contains 8 aspects, which follows the ACL review guidelines14 with small
12http://papers.NeurIPS.cc 13https://github.com/allenai/science-parse 14https://acl2018.org/downloads/acl 2018 review form. html. We manually inspected several review guidelines from ML conferenecs and found the typology in ACL review guideline both general and comprehensive.

Step 1: Human Annotation

This paper …
… I have some concerns …

SUM MOT SUB

Step 2: Train a Tagger
BERT
( token1, aspect1 ) ( token2, aspect2 )
……

Step 3: Post-process heuristic rules
1,200 1,000
800 600 400 200

Step 4: Human Evaluation

Positive ·104
3 2.5
2 1.5
1 0.5

Negative

missing aspect

MOT ORI SOU SUB REP CMP CLA
Figure 1: Data annotation(ap) Hipumealni-nlaebe.led dataset.

MOT ORI SOU SUB REP CMP CLA
(b) Automatic-labeled dataset.

modiﬁcations, which are Summary (SUM), Motivation/Impact (MOT) , Originality (ORI), Sound- ·102
1,2001,12020
ness/Correctness (SOU), Substance (SUB), Repli- 1,0001,10000 cability (REP), Meaningful Comparison (CMP) 800 8080 and Clarity (CLA). The detailed elaborations of 460000 46004600 each aspect can be found in Supplemental Mate- 200 2020

Positive
Positive
Positive ··110044 33 ·104
3
22..525.5 222
11..515.5
111
00..505.5

Negative
Negative Negative

rial B.1. Inside the parentheses are what we will refer to each aspect for brevity. To take into account whether the comments regarding each aspect

MOMTMO(OOaTT)RH(IOOau)RRSmIHOI auUSSnmOO-USaUlanUb-SSBleUaUlbBeRBedlEeRRdPdEEaPdtCPaaM sCtCaeMM Pst.ePPtC. LCCALLAA
(a) Human-labeled dataset.

MMMOOOT(T(bTb)O)OOAARRRuIuIItoSSOmOUUatiSScScUU-U-llBaBaBbbReReRlEleEEedPPdPdCCdCaMMatMatPPsaPesCteC.LCtA.LLAA
(b) Automatic-labeled dataset.

are positive or negative, we also mark whether the comment is positive or negative for every aspect (except summary).

Figure 2: (a) and (b) represent distributions over seven aspects obtained by human and BERT-based tagger respectively. Red bins represent positive sentiment while green ones suggest negative sentiment. We omit “Sum”

3.2.2 Aspect Annotation

aspect since there is no polarity deﬁnition of it.

Overall, the data annotation involves four steps that are shown in Fig. 1.
Step 1: Manual Annotation To manually annotate aspects in reviews, we ﬁrst set up a data annotation platform using Doccano.15 We asked 6 students from ML/NLP backgrounds to annotate the dataset. We asked them to tag an appropriate text span that indicates a speciﬁc aspect. For example, “ The results are new [Positive Originality] and important to this ﬁeld [Positive Motivation]”. The detailed annotation guideline can be found in Supplemental Material B.1. Each review is annotated by two annotators and the lowest pair-wise Cohen kappa is 0.653, which stands for substantial agreement. In the end, we obtained 1,000 human-annotated reviews in total. The aspect statistics in this dataset are shown in Fig. 2-(a).
Step 2: Training an Aspect Tagger Since there are over 20,000 reviews in our dataset, using human labor to annotate them all is unrealistic. Therefore, we use the annotated data we do have to train an aspect tagger and use it to annotate the remaining reviews. The basic architecture of our aspect tagger contains a pre-trained model BERT (Devlin et al.,
15https://github.com/doccano/doccano

2019) and a multi-layer perceptron. The training details can be found in Appendix A.4.
Step 3: Post-processing However, after inspecting the automatically labeled dataset, we found that there appears to be some common problems such as interleaving different aspects and inappropriate boundaries. To address those problems, we used seven heuristic rules to reﬁne the prediction results and they were executed sequentially. The detailed heuristics can be found in Appendix A.5. An example of our model prediction after applying heuristic rules is shown in Appendix A.6. Fig. 2-(b) shows the distribution of all reviews over different aspects. As can be seen, the relative number of different aspects and the ratio of positive to negative are very similar across human and automatic annotation.
Step 4: Human Evaluation To evaluate the data quality of reviews’ aspects, we conduct human evaluation. Speciﬁcally, we measure both aspect precision and aspect recall for our deﬁned 15 aspects.
We randomly chose 300 samples from our automatically annotated dataset and assigned each sample to three different annotators to judge the annotation quality. As before, these annotators are all from ML/NLP backgrounds.
The detailed calculation for aspect precision and

Accept

  

2

Aspect Summary
Decoder
Motivation
Encoder
Originality
We propose
Soaunenwd…n. ess
Substance
Replicability
Clarity
Comparison

MPLoP1larity

Precision 95%

+

94%

–
The
+
–CLA
MLP2
+

paper CLA

72%
reads well
95% CLA94%CLA
95%

–

92%

+

90%

–

90%

+

100%

–

77%

Intro.

+

97%

–

C9E2%

+

Abs.8+5C%E

–

94%

Oracle

Recall 100% 89% 71% 87% 80% 98% 79% 94% 78% 50% 71% 92% 73% 100% 94%

Table 3: Fine-grained aspect preRcefiesreinocne and recall for each aspect. + denotes positive and – denotes negative.

aspect recall can be found in Appendix A.7. Under these criteria, we achieved 92.75% aspect precision and 85.19% aspect recall. The ﬁne-grained aspect precision and aspect recall for each aspect is shown in Tab. 3. The aspect recall for positive replicability is low. This is due to the fact that there are very few mentions of positive replicability. And in our human evaluation case, the system identiﬁed one out of two, which results in 50%. Other than that, the precision and recall are much higher.16
Besides, one thing to mention is that our evaluation criterion is very strict, and it thus acts as a lower bound for these two metrics.
4 Scientiﬁc Review Generation
4.1 Task Formulation
The task of scientiﬁc review generation can be conceptualized an aspect-based scientiﬁc paper summarization task but with a few important differences. Speciﬁcally, most current works summarize a paper (i) either from an “author view” that only use content written by the author to form a summary (Cohan et al., 2018a; Xiao and Carenini,
16The recall numbers for negative aspects are lower than positive aspects. However, we argue that this will not affect the ﬁdelity of our analysis much because (i) we observe that the imperfect recall is mostly (over 85%) caused by partial recognition of the same negative aspect in a review instead of inability to recognize at least one. This will not affect our calculation of Aspect Coverage and Aspect Recall very much. (ii) The imperfect recall will slightly pull up Aspect Score (will discuss in §5.3.1), but the trend will remain the same.

…… We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely ......
Vaswani et al. (2017) showed that not only self-attention can improve a method based on RNNs or convolutions, but also that it is su cient for constructing a powerful model obtaining state-of-the-art performance on the machine translation task.

Author View Reader View

This paper presents an approach for machine translation

using attention based layers ……

[Summary]

The paper reads well and is easy to follow …… [Clarity]

The experimental setup is clear and provides enough

details for replication ……

[Replicability]

Reviewer View

Figure 3: Summarization from three different views for

the paper “Atten…t…ioWne IpsropAoslel aYnoewusNimpelee dne”tw(oVrkaasrcwhiatenctiuree,t tahel.,
2017). SummarTeriasnsforrommer, tbharseede svoileelywson(aauttetnhtoiorn, rmeeachdaenirs,mrse, viewer) comes fdriospmenstinhgewpithapreecurr’rsenacebasntdracocntv,olcutiitoanns ecnetir(eily.e......,. a

paper that cites tVhaisswapnai petearl.)(2a0n17d) spheoweerdrtehvatineowt ornelysspeelf-catttievnetiloyn.

can improve a method based on RNNs or convolutions,

but also that it is su cient for constructing a powerful

2019; Erera etmmaoalcdh.e,ilne2obt0rtaa1nins9lian;tgioCnsttaoatsehk-.oaf-nthee-atrtaple.r,fo2rm0a1nc8e ao;n Cthae -

chola et al., 2020b), (ii) or from a “reader view”

that argues a paThpisepra’pser spruesmenmts aanrayppsrohacohufolrdmatachkinee tiranntsolatiaonc-

using attention based layers ……

[Summary]

count the viewThoefpatpheroresaedsiwneltl haned irseeasseyatorfcohllowco……mm[Culanritiyt]y

(Qazvinian andThRe eaxdpeerivm,en2ta0l 0se8tu;pCisochleaarnanadnpdroGvideosheanrouiaghn,

2017; Yasunagdaetaeilts faorl.re,p2lic0at1io9n …a…).

[Replicability]

In this work, we extend the view of scientiﬁc

paper summarization from “author” or “reader” to

“reviewer”, and claim that a good summary of a

scientiﬁc paper can not only reﬂect the core idea

but also contains critical comments from different

aspects made by domain experts, which usually

requires knowledge beyond the source paper itself.

The advantages lie in: (i) authors: helping them

identify weak points in their paper and make it

stronger. (ii) reviewers: relieving them from some

of the burden of reviewing process. (iii) readers:

helping them quickly grasp the main idea of the

paper and letting them know what “domain experts”

(our system) comments on the paper are. The three

views of scientiﬁc paper summarization are shown

in Fig. 3.

Author View Reader View Reviewer View

4.2 System Design
Despite the fact that our dataset contains fewer training samples compared with other benchmark summarization datasets, the few-shot learning ability of recent contextualized pre-trained models (Radford et al., 2019; Brown et al., 2020; Cachola et al., 2020a) still put training a passable review generation system from this dataset within grasp. We use BART (Lewis et al., 2019), which is a denoising autoencoder for pretraining sequence-to-

Density

1.3

Oracle

CE method

1.2

1.1

1.0

0.9

0.8

0.70.0 0.2 0.4 0.6 0.8 1.0
Selected Sentence Position

Figure 4: Selected sentence position distribution. We use the relative position of each sentence with regard to the whole article, thus taking values from 0 to 1.

sequence models, as our pre-trained model since it has shown superior performance on multiple generation tasks.
However, even if we can take the advantage of this pre-trained model, how to deal with lengthy text in the context of using a pre-trained model (BART, for example, has a standard length limit of 1024 since it was pre-trained on texts of this size) remains challenging. After multiple trials, we opted for a two-stage method detailed below, and describe other explorations that were less effective in Appendix A.8.
4.2.1 Two-stage Systems for Long Documents
Instead of regarding text generation as a holistic process, we decompose it into two steps, using an extract-then-generate paradigm (Chen and Bansal, 2018; Gehrmann et al., 2018; Subramanian et al., 2019; Dou et al., 2020). Speciﬁcally, we ﬁrst perform content selection, extracting salient text pieces from source documents (papers), then generate summaries based on these extracted texts.
To search for an effective way to select content that is most useful for constructing a review generation system, we operationalize the ﬁrst extraction step in several ways. One thing to notice is that the extraction methods we use here mainly focus on heuristics. We leave more complicated selection methods for future work.
Oracle Extraction First, for comparison purposes, we construct an oracle for each paper which is the extraction that achieves highest average ROUGE scores with respect to reference reviews, speciﬁcally using the greedy method described in Nallapati et al. (2017). Note that for each paper with multiple reviews, we construct multiple oracles for that paper. We assume that oracle extractions can reﬂect where reviewers pay more atten-

tion to when they are writing reviews. The selected sentence position distribution in oracles is shown in Fig. 4.
Section-based Extraction Scientiﬁc papers are highly structured. As a convention, a scientiﬁc paper usually describes problem background, related work comparison, as well as its own contributions in the introduction part. Regarding this method, we only use the introduction section, which can be regarded as a baseline model.
Cross-entropy (CE) Method Extraction Here we select salient sentences from the full text range. The way we do so is through a two-step selection process:
1. Select sentences containing certain informative keywords (e.g. propose) which are detailed in Appendix A.9. Those selected sentences form a set S.
2. Select a subset S ⊆ S such that sentences in S cover diverse content and satisfy a length constraint.
In the second step, we use the cross-entropy method introduced in Feigenblat et al. (2017) where we select diverse content by maximizing unigram entropy. The details of this two-step process can be found in Appendix A.9. The selected sentence position distribution using this method is shown in Fig. 4. We can see that the extractor tends to select sentences from the beginning of a paper as well as the ending part of a paper just as the oracle extractor does. This makes sense because the beginning part is the introduction part which talks about the essence of the whole paper and the ending part mostly contains the analysis of experimental results and conclusions etc.
Hybrid Extraction We combine the abstract of a paper and its CE extraction to form a hybrid of both.
4.2.2 Aspect-aware Summarization
Typically in the extract-then-generate paradigm, we can just use the extractions directly and build a sequence-to-sequence model to generate text. Here, in order to generate reviews with more diverse aspects and to make it possible to interpret the generated reviews through the lens of their internal structure, we make a step towards a generation framework involving extract-then-generate-and-predict.

400

200



MOT ORI SOU SUB REP CMP CLA

Intro. CE Abs.+CE Oracle Reference

Decoder

Speciﬁcally, instead of existing aspect-based sum- (without Appendix) as source document.17 And MLP1
marization works that explicitly Etnackoedear spects as Thewepaﬁpelrterreeads pwaepllers with full text fewer than 100

input (Angelidis and Lapata, 2018; Frermann and CLAworCdLAs sinCLcAe thCLeAy don’t contain enough information

Klementiev, 2019; Hayashi et al.aW,nee2pwr0o…p2.os0e ), we use our annotated aspects (§3.2) as additional infor-

for models to learn. For reviews, we only use 100-1024 word reviews 18 for training due to

mation, and design an auxiliary task that aimsMtLoP2 predict aspects of generated texts (reviews). Fig. 5 illustrates the general idea of this.
MLP1

computational efﬁciency, which account for
92.57% of all the reviews. This results in 8,742 unique papers …a…ndWe2p5r,o9po8s6e apnaepwers-imrepvleienwetwporakiarrschiintecture, the total, the split oTfraonsuforrmdaert,asbeasteids ssohloelwy noninaTttaenbt.io4n. mechanisms,
dispensing with recurrence and convolutions entirely ......

Decoder
Encoder
We propose a new ….

The paper reads well

CLA CLA MLP2

CLA CLA

Figure 5: Aspect-aware summarization.

The loss of this model is shown in Eq. 3

L = Lseq2seq + αLseqlab

(3)

where Lseq2seq denotes sequence to sequence loss which is the negative log likelihood of the correct next tokens, and Lseqlab denotes sequence labeling loss which is the negative log likelihood of the correct labels of next tokens. α is a hyper-parameter (α = 0.1) that is tuned to maximize aspect coverage on the development set.

5 Experiment
In this section, we investigate using our proposed review generation systems with state-of-the-art pre-trained models, to what extent can we realize desiderata of reviews that we deﬁned in §2.2. We approach this goal by two concrete questions: (1) What are review generation systems (not) good at? (2) Will systems generate biased reviews?

5.1 Settings
Here we consider three extraction strategies in §4.2.1 as well as two generation frameworks, one is the vanilla sequence to sequence model, the other is jointly sequence to sequence and sequence labeling.
Dataset We use our constructed dataset ASAP-Review described in §3 to conduct experiments. For each paper, we use full text

Vaswani etTarla. i(n2017) Vshaolwideadtitohnat noTt eosntly self-attention Unique papecrsan improv6e,9a93method b8a7s4ed on RN8N7s5or convolutions, Paper-reviewbputaiarslso th2a0t,7i5t 7is su c2i,e5n7t1for con2s,t6ru5c8ting a powerful
model obtaining state-of-the-art performance on the
Table 4: mDaacthainseptlriatnoslfatAioSnAtaPs-k.Review.

Model

This paper presents an approach for machine translation
As muesnintgioanttendtioinnba§se4d.2la,yetrhs e……pre-trained[Summary]

sequence-to-seqThueepnacpeermreoaddsewlewll eanudsisedeaissy tBoAfoRlloTw. …F…or [Clarity] all models, we Tihneiteiaxpliezriemdentthael smetoupdeisl wcleeairgahntds uprsoivnidges enough the checkpointd:et“aiblsaforr tre-plilcaatirong…e…-cnn” which[Riesplicability] pre-trained on “CNN/DM” dataset (Hermann et al., 2015).19 For extract-then-generate-and-predict

framework, we add another multilayer perceptron

on top of the BART decoder, and initialize it with

0.0 mean and 0.02 standard deviation. We use

the Adam optimizer(Kingma and Ba, 2014) with a

linear learning rate scheduler which increases the learning rate linearly from 0 to 4e−5 in the ﬁrst

10% steps (the warmup period) and then decreases

the learning rate linearly to 0 throughout the rest

of training steps. We ﬁnetuned our models on the

whole dataset for 5 epochs. We set a checkpoint

at the end of every epoch and ﬁnally took the one

with the lowest validation loss.

During generation, we used beam search decod-

ing with beam size 4. Similarly to training time,

we set a minimum length of 100 and a maximum

length of 1024. A length penalty of 2.0 and trigram

blocking (Paulus et al., 2017) were used as well.

5.2 What are Systems Good and Bad at?
Based on the evaluation metrics we deﬁned in §2.2, we conduct both automatic evaluation and human evaluation to characterize both reference reviews and generated reviews, aiming to analyze what subtasks of review generation automatic systems can
17If a paper has more than 250 sentences, we truncate it and take the ﬁrst 250 sentences when we do the extraction step.
18As measured by BART’s subword tokenizer. 19We also tried “bart-large-xsum” checkpoint which is pre-trained on “XSUM dataset (Narayan et al., 2018)”, however that results in much shorter reviews, and sentences in it tend to be succinct.

Autho Reade Review

Desiderata Metric HUMAN
INTRO CE ABSCE
INTRO CE ABSCE

Aspect √× √× √×

Decisive. RACC 30.32
– – –
-15.38† -11.54† -23.08† -30.77† -30.77† -38.46†

Comprehen. Justiﬁcation Accuracy

ACOV AREC

INFO

ACON SACC

49.85 58.66

97.97

75.67 90.77

EXTRACTIVE

–

–

–

–

–

–

–

–

–

–

–

–

–

–

–

EXTRACTIVE+ABSTRACTIVE

50.37 51.50
62.64† 63.96†
55.37† 56.91†

55.52† 58.24
60.73 61.62†
58.31 57.56

100.00† 99.29
99.29 100.00†
98.21 98.21

43.78† 32.51†
39.17† 34.46†
34.75† 35.21†

83.93 80.36†
78.57† 69.64†
92.86 87.50

R-1 –
38.62 38.56 37.55
41.39 41.31 42.37 42.27 43.11 42.99

Others

R-2 R-L

–

–

8.84 25.11 7.81 25.94 8.53 25.85

11.53 11.41
11.72 11.62
12.24 12.19

38.52 38.38
39.86 39.73
40.18 40.12

BS –
29.22 29.11 31.99
42.29 42.33 41.78 41.71 42.90 42.63

Table 5: Results of the baseline models as well as different aspect-enhanced models under diverse automated evaluation metrics. “BS” represents BERTScore. † denotes that the difference between system generated reviews and human reviews are statistically signiﬁcant (p-value < 0.05 using 10,000 paired bootstrap resampling (Efron, 1992) tests with 0.8 sample ratio).

do passably at, and also where they fail. The aspect information in each review is obtained using aspect tagger we trained in §3.2.
Automatic Evaluation Automatic evaluation metrics include Aspect Coverage (ACOV), Aspect Recall (AREC) and Semantic Equivalence (ROUGE, BERTScore). Notably, for each source input, there are multiple reference reviews. When aggregating ROUGE and BERTScore20, we take the maximum instead of average. And when aggregating other metrics for human reviews, we take the average for each source document. The results are shown in Tab. 5.
Human Evaluation Metrics that require human labor include Recommendation Accuracy (RACC), Informativeness (INFO), Aspect-level Constructiveness (ACON) and Summary Accuracy (SACC). We select 28 papers from ML/NLP/CV/RL domains. None of these papers are in the training set. Details regarding human judgment are shown in Appendix A.3. The evaluation results are shown in Tab. 5.
Overall, we make the following observations:
5.2.1 Weaknesses
Review generation system will generate nonfactual statements for many aspects of the paper
20We have used our own custom baseline to rescale BERTScore, details can be found in Appendix A.3.

assessment, which is a serious ﬂaw in a high-stakes setting.
Lacking High-level Understanding Speciﬁcally, when using metrics that require higher level understanding of the source paper like Recommendation Accuracy and Aspect-level Constructiveness, proposed systems achieved much lower performance, with even the smallest gaps between systems and humans being 41.86% for Recommendation Accuracy and 31.89% for Aspect-level Constructiveness compared to reference reviews. This means our systems cannot precisely distinguish high-quality papers from low-quality papers and the evidence for negative aspects is not reliable most of the time.21
Imitating Style After careful inspection, we ﬁnd that some of sentences will appear frequently in different generated results. For example, the sentence “The paper is well-written and easy to follow” appears in more than 90% of generated reviews due to the fact that in the training data, this exact sentence appears in more than 10% of papers. This suggests that the style of generated reviews tend to be inﬂuenced
21Although there exist varying degrees of performance differences on RACC and ACon for different systems, we only ﬁnd one pair of systems perform statistically different on ACon.

by high-frequency sentence patterns in training samples.
Lack of Questioning Generated reviews ask few questions about the paper content, which is an important component in peer reviewing. In the reference reviews, the average number of questions per review is 2.04, while it is only 0.32 in generated reviews.
5.2.2 Advantages
We ﬁnd that review generation systems can often precisely summarize the core idea of the input paper, and generate reviews that cover more aspects of the paper’s quality than those created by human reviewers. Systems with aspect information are also aspect-aware and evidence sensitive as we will discuss below.
Comprehensiveness In terms of Aspect Coverage and Informativeness, our systems can outperform human reviewers by at most 14.11% and 2.03% respectively, suggesting that even reviews from the reviewers may also fall short on our deﬁned criterion regarding comprehensiveness.
Good Summarization Current systems can correctly summarize the contributions of papers most of the time as shown by Summary Accuracy. 4 out of 6 systems can achieve over 80% accuracy and statistical signiﬁcance tests show that gaps between top-3 systems and human reviewers are not signiﬁcant. This means that in terms of summarizing the paper content, current systems can achieve comparable performance to human reviewers.
5.2.3 System Comparisons
We also look into how systems with different settings are diverse in performance and make the following observations.
Summarization Paradigms By looking at ROUGE (R) and BERTScore (BS), we can see that “extractive + abstractive”-based methods can consistently outperform pure extractive methods, with the smallest gaps of 2.69, 2.57, 12.44, 9.72 for R-1, R-1, R-L and BS respectively. This demonstrates the necessity of using abstractive summarization which can generate reviews that are close both in meaning as well as language use to human reviews.
Extraction Strategies We can see that it is more effective to use extracted text from the full paper

to aid the generation process, resulting in higher aspect coverage compared with solely using introduction information. This is reasonable since models can obtain more diverse input from the full text.
System Diagnosis Our ﬁne-grained evaluation metrics enable us to compare different systems and interpret their relative merits. For example, as discussed before, our systems can achieve higher Informativeness than reference reviews while suffering from much lower Aspect-level Constructiveness. This means that if we want our systems to match the performance of real reviewers, we should focus on improving the constructiveness of our systems instead of aiming for methods that provide better evidence for negative aspects (which are not factually correct most of the time in the ﬁrst place).
5.2.4 Case Study
To get an intuitive understanding of how aspectenhanced review generation system worked, we perform analysis on a real case. (More analysis can be found in Appendix A.10.) Speciﬁcally, since our aspect-enhanced model is trained based on multitask learning framework, it would be interesting to see how well the tokens are generated associated with corresponding aspects. We take our aspectenhanced model using CE extraction to conduct this experiment. Tab. 6 shows an example review when we do so.
We can see that the model can not only generate ﬂuent text but also be aware of what aspect it is going to generate as well as the correct polarity of that aspect. Although the generated aspects are often small segments and there are some minor alignment issues, the model is clearly aspect-aware.
5.3 Will System Generate Biased Reviews?
Biases in text are prevalent, but often challenging to detect (Manzoor and Shah, 2020; Stelmakh et al., 2019). For example, in natural language processing, researchers are trying to identify societal biases (e.g, gender) in data and learning systems on different tasks (Bolukbasi et al., 2016; Zhao et al., 2018; Stanovsky et al., 2019). However, previous works on analyzing bias in scientiﬁc peer review usually focus on disparities in numerical feedback instead of text. Manzoor and Shah (2020) recently uncover latent bias in peer review based on review text. In this work, besides designing a model to generate reviews, we also perform an analysis of

(Rr, G0)

Aspect score Aspect score

summary

clarity +

substance +

soundness -

substance -

Δ( , , ) = ( , ) − ( , )

Δ( , , ) = ( , ) − ( , )

This paper stud(ies, th) e problem of transfer learn(in,g f)rom a single pre-trained network onto a new task . The authors pro-

pose a method of combining the outputs of multiple pre-trainable classiﬁers by training on their raw predictions and then

ﬁne-tuning on(th,e t)arget samples .( T,he)proposed method is based on the use of ( non-linear ) maximal correlation analysis( th, at) originated with Hirschfeld [ 9 ] to this (pro, bl)em . The paper is wellwritten and easy to follow . The experimental

results sGhow that theGproposed method oGutperforms tGhe state-of-the-art methods on the CIFAR-100 , Stanford Dogs , and

0

1

0

1

Tiny RImefeargenecNe eretvdieawtas.sets . However , iGt eisnenraotetdcrleevairewtos. me how the performance of the method is affected by the number of

target training samples . It would be better if the authors can provide some theoretical analysis on the effect of the size of

the target dataset .

Table 6: Illustration of generated tokens associated with corresponding aspects. + denotes positive sentiment. denotes negative sentiment.

δ( , G) =

( , )− ( , ) (, )

δ( , G) =

( , )− ( , ) (, )

Aspect score Aspect score

(, )

(, )

(, )

(, )

G0

G1

Reference reviews.

G0

G1

Generated reviews.

Figure 6: Aspect score AS(R, Gi) and disparity δ(R, G) in reference reviews (Rr) and generated re-
views (Rg). G = [G0, G1] denotes different groups.

Rg and reference reviews Rr and can be formally calculated as:
∆(Rg, Rr, G) = δ(Rg, G) − δ(Rr, G) (4)
where G = [G0, G1] denotes different groups based on a given partition criterion. Positive value means generated reviews favor group G0 more compared to reference reviews, and vice versa.
In this work, we group reviews from two perspectives. The basic statistics are shown in Tab. 7.

bias, in which we propose a method to identify and quantify biases both in human-labeled and systemgenerated data in a more ﬁne-grained fashion.
5.3.1 Measuring Bias in Reviews
To characterize potential biases existing in reviews, we (i) ﬁrst deﬁne an aspect score, which calculates the percentage of positive occurrences22 of each aspect. The polarity of each aspect is obtained based on our learned tagger in §3.2; (ii) then we aim to observe if different groups Gi (e.g., groups whether the paper is anonymous during reviewing or is not anonymous) of reviews R would exhibit disparity δ(R, G) in different aspects. The calculation of disparity can be visualized in Fig. 6.
Based on above two deﬁnitions, we characterize bias in two ways respectively: (1) spider chart, which directly visualizes aspect scores of different groups of reviews w.r.t each aspect. (2) disparity difference, which represents the difference between disparities in generated reviews
22If an aspect does not appear in a review, then we count the score for that aspect 0.5 (stands for neutral). Details see Appendix A.11.

Total Acc.%

Native
651 66.51%

Non-native
224 50.00%

Anonym.
613 57.59%

Non-anonym.
217 78.34%

Table 7: Test set statistics based on nativeness and anonymity.

Nativeness We categorize all papers in test set into “native” (G0) and “non-native” (G1) based on whether there is at least one native speaker in the author list as well as whether the institution is in an English-speaking country.23
Anonymity We categorize all papers in test set into “anonymous” (G0) and “non-anonymous” (G1) based on whether the paper has been released as a pre-print before a half month after the conference submission deadline.24
Here we take our model with introduction extraction as an example to showcase how to use the
23We used https://www.familysearch.org/en/ to decide the nationality of an author. In cases where all authors are not from an english-speaking country, we look into the institution information to further decide the attribution of the paper based on whether the institution is from an english-speaking country.
24We discard papers from ICLR 2017 since the reviewing process was single blind.

MOT

Native Non-native

MOT

CMP

ORI

CMP

ORI

MOT

Anonymous Non-anonymous

MOT

CMP

ORI

CMP

ORI

CLA

SOU CLA

SOU CLA

SOU CLA

SOU

REP

SUB

(a) Reference reviews.

REP

SUB

(b) Generated reviews.

REP

SUB

(c) Reference reviews.

REP

SUB

(d) Generated reviews.

Figure 7: Spider chart of aspect scores with respect to different groups.

MOT

ORI

SOU

SUB

REP

CLA

CMP

Total

Nativeness

-0.72

+18.71

+3.84

-3.66

+0.73

-13.32

+2.40

43.39

Anonymity

-5.69

-4.43

+2.76

-0.64

+5.65

+5.80

+3.02

28.00

Figure 4: Spider chart of aspect scores with respect to di erent groups.
Table 8: Disparity differences regarding nativeness and anonymity. Total is the sum of absolute value of disparity

difference.

ﬁne-grained aspect information in our dataset to do bias analysis. We list the bias analysis for other models in Appendix A.12.
5.3.2 Nativeness Analysis
Spider Chart Generally, Native papers receive higher score in most aspects in both reference reviews and generated reviews. Speciﬁcally, for human reviews: (1) By looking at Fig. 7-(a), there is a signiﬁcant gap in Clarity, which is reasonable since non-native authors may have more trouble conveying their ideas. (2) Scores of the two groups are much closer in other aspects.
For system-generated reviews: As shown in Fig. 7-(b), the auto-review system narrows the disparity in Clarity but ampliﬁes it in Originality, meaning that system reviewers are harsher than human reviewers when commenting the paper’s “Originality” for non-native English speakers. This observation suggests that a review system can generate biased reviews in some aspects, which would lead to unfair comments. Therefore, a system should be de-biased before it come to use.
Disparity Difference Through spider chart, gaps between different groups are relatively small and hard to discern. Besides, those gaps can only show the absolute favor for a certain group in different aspects. We are also interested in whether generated reviews are more in favor of a certain group compared to reference reviews. To do this, we calculate disparity differences and list them in Tab. 8.

As shown in Tab. 8, for Originality and Clarity, the disparity difference is +18.71 and −13.32 which means that the system favours native papers in Originality and non-native papers in Clarity compared to human reviewers. This observation is consistent with spider chart. Besides, varying degrees of bias are presented in Tab. 8. For example, for Motivation and Replicability, the disparity difference is less than 1, which suggests little bias while in other aspects, the bias is much larger.
5.3.3 Anonymity Analysis
Spider Chart By looking at Fig. 7-(c) and Fig. 7-(d), we ﬁnd that both human reviewers and system reviewers favor non-anonymous papers in all aspects. Speciﬁcally, for human reviews: we ﬁnd gaps are non-negligible in Soundness, Clarity and Meaningful Comparison while for system-generated reviews, we observe that gaps are considerable in Motivation, Originality, Soundness. This observation is interesting since human reviewers may be aware of the identity of the authors due to non-anonimity which may affect the reviews they write. However, our system is not aware of that and its preference towards non-anonymous paper probably suggests some quality difference.25
Disparity Difference By looking at Tab. 8, we ﬁnd that the largest absolute disparity difference
25Non-anonymous papers are more likely to have been rejected before and therefore are revised many more times.

regarding anonymity is 5.80 compared to 18.71 regarding nativeness. This suggests that regarding anonymity, our system’s preference does not diverge that much from human reviewers. Also, the total aspect bias regarding anonymity is 28.00, much smaller compared to total aspect bias regarding nativeness (43.00). This also suggests that our model is less sensitive to anonymity compared to nativeness.
The observations above are probably related to some superﬁcial heuristics existing in peer review. For example, when reviewers detect some grammar mistakes, they may assume that the authors are not native and then bias towards rejecting the paper by claiming some clarity issues. Another example is that there may exist differences in the research topics pursued by different subgroups (e.g., different countries), the bias regarding nativeness may also suggest the favor of certain topics in the reviewing process. Those superﬁcial heuristics should be discouraged and deserve further investigation in future research.
6 Related Work
Scientiﬁc Review Generation There has been a relative paucity of work on scientiﬁc review generation, other than Bartoli et al. (2016)’s work investigating the feasibility of generating fake reviews by surface-level term replacement and sentence reordering etc. In addition contemporaneous and independent work by Wang et al. (2020) proposes a two-stage information extraction and summarization pipeline to generate paper reviews. Their evaluation focuses mainly on the accuracy of information extraction, and the evaluation of the generated summaries is somewhat precursory, assessing only a single criterion “constructiveness and validity” manually over 50 papers. Our paper (1) proposes a wide variety of diagnostic criteria on review quality, (2) uses a very different summarization methodology, and (3) evaluates the generated results extensively.
Peer Review Peer review is an essential component of the research cycle and is adopted by most journals and conferences to identify important and relevant research. However, at the same time it is easy to identify many issues: expensiveness, slowness, existence of inconsistency (Langford and Guzdial, 2015) and bias (Tomkins et al., 2017), etc.
Some efforts have been put into analyzing the peer review process including automating review

assignment (Jin et al., 2017; Nguyen et al., 2018; Anjum et al., 2019; Jecmen et al., 2020), examining bias problems (Tomkins et al., 2017; Stelmakh et al., 2019), examining consistency problems (Langford and Guzdial, 2015) and performing sentiment analysis on reviews (Wang and Wan, 2018; Chakraborty et al., 2020). Several decision classiﬁcation methods have been explored to help make accept or reject decision given a paper. Those methods are either based on textual (Kang et al., 2018; Qiao et al., 2018) or visual (Von Bearnensquash, 2010; Huang, 2018) information. However, they do not directly alleviate review load, as our paper aims to do.
7 Discussion and Future Directions
We ﬁrst summarize what we have achieved in this work and how the current ReviewAdvisor system can potentially help in a reviewing process. Then we discuss challenges and potential directions for the automatic review generation task, which, hopefully, encourages more future researchers to explore this task, and in the right direction.
7.1 Machine-assisted Review Systems
Instead of replacing a human reviewer, a better position for ReviewAdvisor is to regard it as a machineassisted review system. Although there is still a large room for improvement, our results indicate that even with current technology:
(1) Based on the evaluation of §5.2, Summary Accuracy of our systems is quite high, suggesting that it can be either used for reviewers to ﬁnish the description of Summary, or help general readers to quickly understand the core idea of recently preprinted papers (e.g., papers from arXiv).
(2) Based on evaluation of §5.2, reviews generated by ReviewAdvisor can cover more aspects and generate more informative reviews. Although the associated opinions may suffer from constructiveness problems, they still may be useful since they can provide a preliminary template for reviewers, especially enabling junior or non-native English reviewers to know what a review generally should include and how to phrase each aspect. Additionally, for each aspect (e.g., Clarity), our system can provide relevant evidence sentences from the paper, helping reviewers quickly identify salient information when reviewing the paper (Detailed example in our Appendix A.10).

7.2 Challenges and Promising Directions
7.2.1 Model
(1) Long Document Modeling: The average length of one scientiﬁc paper is commonly larger than 5,000 words, far beyond the input text’s length that mainstream neural sequence models (e.g., LSTM, Transformer) or pre-trained models (e.g., BERT, BART) normally use. This work (in §4.2.1) bypasses the difﬁculty by using a two-stage system, but other strategies should be explored. (2) Pre-trained Models for Scientiﬁc Domain: Although previous works, as exempliﬁed by (Beltagy et al., 2019) have pre-trained BERT on scientiﬁc domain, we observe that using these models with transformer decoders perform much worse than BART on sequence generation tasks in terms of ﬂuency and coherence, which calls for general sequence to sequence models pre-trained on scientiﬁc domain for higher-quality review generation. (3) Structure Information: Review generation systems could get a deeper understanding of a given research paper if structural information can be provided. To this end, outputs from scientiﬁc paperbased information extraction tasks (Hou et al., 2019; Jain et al., 2020) can be utilized to guide review generation. (4) External Knowledge: Besides the paper itself, review systems can also rely on external knowledge, such as a citation graphs constructed based on more scientiﬁc papers or a knowledge graph connecting concepts across different papers (Luan et al., 2018; Lo et al., 2020). Also, recently, August et al. (2020) compile a set of writing strategies drawn from a wide range of prescriptive sources, it would be also valuable to transfer this knowledge into the auto-review system.
7.2.2 Datasets
(5) More Open, Fine-grained Review Data: In this work, we annotate ﬁne-grained information (aspects) of each review manually. However, this information could potentially be obtained directly from the peer review system. How to access this information appropriately would be an important and valuable step in the future. (6) More Accurate and Powerful Scientiﬁc Paper Parsers: Existing parsing tools (e.g. science-parse, grobid) for scientiﬁc papers are commonly designed for certain speciﬁc paper templates, and also still struggle at extracting ﬁne-grained information, such as the content of tables and ﬁgures.

7.2.3 Evaluation
(7) Fairness and Bias in Generated Text: In this work, we make a step towards identifying and quantifying two types of biases existing in human and system-generated reviews. Future works can explore more along this direction based on our dataset that contains ﬁne-grained aspect annotation. (8) Factuality and Reliability: A generated review should be factually correct (Wadden et al., 2020) which also poses challenge to the current evaluation methodology. In addition to generating a review, a reliable system should also provide a level of conﬁdence with respect to the current comment. Moreover, whether review scores are calibrated is another valuable question.
7.3 Conclusion
In answer to the titular question of “can we automate scientiﬁc review,” the answer is clearly “not yet”. However, we believe the models, data, and analysis tools presented in this paper will be useful as a starting point for systems that can work in concert with human reviewers to make their job easier and more effective.
Acknowledgment
This work could not be accomplished without the help of many researchers. We would like to thank people for their generous support, especially,
Volunteer to help us with the human evaluation: Ga´bor Berend, Zhouhan Lin, William W. Cohen, Pengcheng Yin, Tiange Luo, Yuki M. Asano, Junjie Yan, Tuomas Haarnoja, Dandan Guo, Jie Fu, Lei Chen, Jinlan Fu, Jiapeng Wu, Wenshan Wang, Ziyi Dou, Yixin Liu, Junxian He, Bahetiyaer Bare, Saizheng Zhang, Jiateng Xie, Spyros Gidaris, Marco Federici, Junji Dai, Zihuiwen Ye Jie Zhou, Yufang Liu, Yue Zhang, Ruifeng Xu, Zhenghua Li, Chunting Zhou, Yang Wei.
This work lasted nearly a year, from the initial idea discussion (2020.02.28) to completing the ﬁrst version of draft (2021.01.29). This is the year from the beginning of the COVID-19 epidemic to its outbreak. Thanks for this fun and challenging project that punctuates my dull life at home. Thank Weizhe, for her patience, persistence and her willingness to work with me to complete this crazy idea. Thanks a lot for Graham’s endless help on this project. The story is not over, and our system is still evolving.

References
Stefanos Angelidis and Mirella Lapata. 2018. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3675–3686, Brussels, Belgium. Association for Computational Linguistics.
Omer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei Hwu, and JinJun Xiong. 2019. PaRe: A paperreviewer matching approach using a common topic space. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 518–528, Hong Kong, China. Association for Computational Linguistics.
Tal August, Lauren Kim, Katharina Reinecke, and Noah A. Smith. 2020. Writing strategies for science communication: Data and computational analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5327–5344, Online. Association for Computational Linguistics.
Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, and Fabiano Tarlao. 2016. Your paper has been accepted, rejected, or whatever: Automatic generation of scientiﬁc paper reviews. In International Conference on Availability, Reliability, and Security, pages 19– 28. Springer.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientiﬁc text. arXiv preprint arXiv:1903.10676.
Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings.
Lutz Bornmann and Ru¨diger Mutz. 2015. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Journal of the Association for Information Science and Technology, 66(11):2215–2222.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld. 2020a. TLDR: Extreme summarization of scientiﬁc documents. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4766–4777, Online. Association for Computational Linguistics.
Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S. Weld. 2020b. Tldr: Extreme summarization of scientiﬁc documents. ArXiv, abs/2004.15011.

Souvic Chakraborty, Pawan Goyal, and Animesh

Mukherjee. 2020.

Aspect-based sentiment

analysis of scientiﬁc reviews. arXiv preprint

arXiv:2006.03257.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 675–686.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, W. Chang, and Nazli Goharian. 2018a. A discourse-aware attention model for abstractive summarization of long documents. In NAACL-HLT.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018b. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Arman Cohan and Nazli Goharian. 2017. Scientiﬁc article summarization using citation-context and article’s discourse structure. arXiv preprint arXiv:1704.06619.

Nicola De Bellis. 2009. Bibliometrics and citation analysis: from the science citation index to cybermetrics. scarecrow press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.

Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014.

Bradley Efron. 1992. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pages 569–593. Springer.

Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, O. Nakash, O. Boni, Haggai Roitman, Doron Cohen, B. Weiner, Y. Mass, Or Rivlin, G. Lev, Achiya Jerbi, Jonathan Herzig, Yufang Hou, Charles Jochim, Martin Gleize, F. Bonin, and D. Konopnicki. 2019. A summarization system for scientiﬁc documents. In EMNLP/IJCNLP.

Guy Feigenblat, Haggai Roitman, Odellia Boni, and David Konopnicki. 2017. Unsupervised queryfocused multi-document summarization using the

cross entropy method. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’17, page 961–964, New York, NY, USA. Association for Computing Machinery.
Lea Frermann and Alexandre Klementiev. 2019. Inducing document structure for aspect-based summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6263–6273, Florence, Italy. Association for Computational Linguistics.
Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, and Yusuke Miyao. 2019. Does my rebuttal matter? insights from a major NLP conference. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1274–1290, Minneapolis, Minnesota. Association for Computational Linguistics.
Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109.
Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. 2020. Wikiasp: A dataset for multi-domain aspectbased summarization. Transactions of the Association for Computational Linguistics (TACL).
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1684–1692.
Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2019. Identiﬁcation of tasks, datasets, evaluation metrics, and numeric scores for scientiﬁc leaderboards construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5203–5213, Florence, Italy. Association for Computational Linguistics.
Jia-Bin Huang. 2018. Deep paper gestalt. arXiv preprint arXiv:1812.08775.
Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020. SciREX: A challenge dataset for document-level information extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7506–7516, Online. Association for Computational Linguistics.

Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B Shah, Vincent Conitzer, and Fei Fang. 2020. Mitigating manipulation in peer review via randomized reviewer assignments. arXiv preprint arXiv:2006.16437.
Tom Jefferson, Philip Alderson, Elizabeth Wager, and Frank Davidoff. 2002a. Effects of editorial peer review: a systematic review. Jama, 287(21):2784– 2786.
Tom Jefferson, Elizabeth Wager, and Frank Davidoff. 2002b. Measuring the quality of editorial peer review. Jama, 287(21):2786–2790.
Rahul Jha, Amjad Abu-Jbara, and Dragomir Radev. 2013. A system for summarizing scientiﬁc topics starting from keywords. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 572–577, Soﬁa, Bulgaria. Association for Computational Linguistics.
Rahul Jha, Reed Coke, and Dragomir R. Radev. 2015a. Surveyor: A system for generating coherent survey articles for scientiﬁc topics. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA, pages 2167–2173. AAAI Press.
Rahul Jha, Catherine Finegan-Dollak, Ben King, Reed Coke, and Dragomir Radev. 2015b. Content models for survey generation: A factoid-based evaluation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 441–450, Beijing, China. Association for Computational Linguistics.
Jian Jin, Qian Geng, Qian Zhao, and Lixue Zhang. 2017. Integrating the trend of research interest for reviewer assignment. In Proceedings of the 26th International Conference on World Wide Web Companion, pages 1233–1241.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A dataset of peer reviews (peerread): Collection, insights and nlp applications. In Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), New Orleans, USA.
Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
John Langford and Mark Guzdial. 2015. The arbitrariness of reviews, and advice for school administrators.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training

for natural language generation, translation, and comprehension. ArXiv, abs/1910.13461.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 150–157.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. Association for Computational Linguistics.
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identiﬁcation of entities, relations, and coreference for scientiﬁc knowledge graph construction. arXiv preprint arXiv:1808.09602.
Emaad Manzoor and Nihar B. Shah. 2020. Uncovering latent biases in text: Method and application to peer review.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientiﬁc paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 584–592, Boulder, Colorado. Association for Computational Linguistics.
Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. ArXiv, abs/1611.04230.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.
Jennifer Nguyen, Germa´n Sa´nchez-Herna´ndez, Nu´ria Agell, Xari Rovira, and Cecilio Angulo. 2018. A decision support tool using order weighted averaging for conference review assignment. Pattern Recognition Letters, 105:114–120.
Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scientiﬁc paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, page 689–696, USA. Association for Computational Linguistics.

Feng Qiao, Lizhen Xu, and Xiaowei Han. 2018. Modularized and attention-based recurrent convolutional neural network for automatic academic paper aspect scoring. In International Conference on Web Information Systems and Applications, pages 68–76. Springer.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and T. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. ArXiv, abs/1911.05507.
Anna Rogers and Isabelle Augenstein. 2020. What can we do to improve peer review in NLP? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1256–1262, Online. Association for Computational Linguistics.
Reuven Y Rubinstein and Dirk P Kroese. 2013. The cross-entropy method: a uniﬁed approach to combinatorial optimization, Monte-Carlo simulation and machine learning. Springer Science & Business Media.
R. Smith. 2006. Peer review: A ﬂawed process at the heart of science and journals. Journal of the Royal Society of Medicine, 99:178 – 182.
Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.
Ivan Stelmakh, Nihar Shah, and Aarti Singh. 2019. On testing for biases in peer review. In Advances in Neural Information Processing Systems, pages 5286– 5296.
Sandeep Subramanian, Raymond Li, Jonathan Pilault, and Christopher Pal. 2019. On extractive and abstractive neural document summarization with transformer language models. arXiv preprint arXiv:1909.03186.
Albert N Tabah. 1999. Literature dynamics: Studies on growth, diffusion, and epidemics. Annual review of information science and technology (ARIST), 34:249–86.
Andrew Tomkins, Min Zhang, and William D Heavlin. 2017. Reviewer bias in single-versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.

Carven Von Bearnensquash. 2010. Paper gestalt. Secret Proceedings of Computer Vision and Pattern Recognition (CVPR).
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or ﬁction: Verifying scientiﬁc claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, Online. Association for Computational Linguistics.
Ke Wang and Xiaojun Wan. 2018. Sentiment analysis of peer review texts for scholarly papers. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 175– 184.
Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, and Nazneen Fatema Rajani. 2020. Reviewrobot: Explainable paper review generation based on knowledge synthesis. In Proceedings of INLG.
Wen Xiao and Giuseppe Carenini. 2019. Extractive summarization of long documents by combining global and local context. ArXiv, abs/1909.08089.
Xinyu Xing, Xiaosheng Fan, and Xiaojun Wan. 2020. Automatic generation of citation texts in scholarly papers: A pilot study. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181–6190, Online. Association for Computational Linguistics.
Wenting Xiong and Diane Litman. 2011. Automatically predicting peer-review helpfulness. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 502–507, Portland, Oregon, USA. Association for Computational Linguistics.
Michihiro Yasunaga, Jungo Kasai, Rui Zhang, A. R. Fabbri, Irene Li, D. Friedman, and Dragomir R. Radev. 2019a. Scisummnet: A large annotated corpus and content-impact models for scientiﬁc paper summarization with citation networks. In AAAI.
Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R. Fabbri, Irene Li, Dan Friedman, and Dragomir R. Radev. 2019b. Scisummnet: A large annotated corpus and content-impact models for scientiﬁc paper summarization with citation networks. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 7386–7393. AAAI Press.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv, pages arXiv– 1904.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.

A Appendices
A.1 Reviews of this Paper Written by Our Model
Notably, the following review is generated based on the paper without this review as well as the TL;QR section (The original version can be found here: https://drive.google.com/ﬁle/d/ 1nC4kCaaeqKRiajcvK75g421Ku9Jog1n9/view? usp=sharing). And we directly put the system output here without any grammar check. Summary : This paper presents an approach to evaluate the quality of reviews generated by an automatic summarization system for scientiﬁc papers . The authors build a dataset of reviews , named ASAP-Review1 , from machine learning domain , and make ﬁne-grained annotations of aspect information for each review , which provides the possibility for a richer evaluation of generated reviews . They train a summarization model to generate reviews from scientiﬁc papers , and evaluate the output according to our evaluation metrics described above . They propose different architectural designs for this model , which they dub ReviewAdvisor , and comprehensively evaluate them , interpreting their relative advantages and disadvantages . They ﬁnd that both human and automatic reviewers exhibit varying degrees of bias regarding native English speakers vs non-native English speakers , and ﬁnd that native speakers tend to obtain higher scores on “ Clarity ” and “ Potential Impact ” . The paper is well-written and easy to follow . Strengths : 1 . The proposed approach is novel and interesting . 2 .The paper is easy to read and well-organized . 3 .The evaluation metrics are wellmotivated . 4 .The authors have done a good job of evaluating the proposed approach . Weaknesses : 1 ) The evaluation metrics used in this paper are not well-deﬁned . For example , what is the deﬁnition of “ good review quality ” ? What is the criteria for a good review ? 2 ) It is not clear to me how the authors deﬁne “ factually correct ” , “ fair ” or “ non-factual ” in Section 3.2 . 3 ) The authors should provide more details about the evaluation metrics in the paper . For instance , what are the criteria used in Table 1 and Table 2 ? What are the metrics used for the evaluation in Table 3 and Table 4 ? 4 ) It would be better if the authors can provide more explanations about the results of Table 2 and Table 3 . 5 ) In Table 3 , the authors mentioned that “ we found the constructed automatic review system generates nonfactual state-

ments regarding many aspects of the paper assess1ASpect-enhanced-Anced Peer Review dataset , which is a serious ﬂaw in a high-stakes setting such as reviewing . However , there are some bright points as well . ” However , it is unclear to me why the authors found this problem . 6 ) In Section 4.3 , it seems that the authors did not provide any explanation about why the human reviewers are biased . 7 ) In Figure 1 , it would be good to provide more information about the training data . 8 ) In section 4.4 , it will be better to provide some explanation about how the human reviews are generated .
A.2 Screenshot of Our Demo System
A.3 Details for Evaluation Metrics
REC In §2.2, the REC function we deﬁne corresponds to the recommendation sentiment of a review, with {−1, 0, 1} representing negative, neutral and positive.
To decide the sentiment of a reference review, we use the rating information from reviewers: (i) if the rating corresponds to marginal accept or marginal reject, then we regard it as neutral; (ii) if the rating is above marginal accept, then we regard it as positive; (iii) otherwise, we regard it as negative.
To decide the sentiment of a generated review, two members of the project team judged the sentiment polarity of a review. If they agreed with each other, then we uses the agreed-upon sentiment, if they disagreed with each other, then we label the sentiment of that review as neutral. The Cohen kappa of two annotators is 0.5778, which is commonly referred to as “moderate” agreement.
INFO The judgement of evidence for negative aspects was conducted by a member of the project team, who judged whether each identiﬁed negative aspect was accompanied by evidence irrespective of the correctness of the evidence. In other words, as long as there is a reason, we count that as an evidence.
SACC & ACON The judgement of summary accuracy and valid support for negative aspects are performed by one of the ﬁrst authors of the reviewed paper. Summary and each negative aspect in the review should be scored 0, 0.5 or 1 which represent agree, partially agree and disagree respectively. We provide authors with the following instructions:
“We have created a Google doc for your paper, and

Figure 8: Generated reviews with aspects by our ReviewAdvisor demo associated with evidence sentences from the paper “Deep Residual Learning for Image Recognition” (He et al., 2016).

you can score the summary as well as each aspect with its corresponding comments inside the red brackets. "1" denotes agree, "0.5" denotes partially agree, "0" denotes disagree. You only need to assign a score based on your judgment. For summary, agree means that you think it’s factually correct. For each aspect, agree means that you think the strength/weakness the reviewer points out is reasonable or constructive.”
BERTScore Regarding BERTScore, we apply the same rescaling procedure following Zhang et al. (2019), which does not affect the ranking ability of BERTScore, but make the scores more discriminative.
A.4 Training of Aspect Tagger
We formulate the annotation process as a sequence labeling problem where the input is a sequence consisting of n words S = w1, · · · , wn, and the target is a sequence of tags one for each word T = t1, · · · , tn. We aim to ﬁnd a mapping f such that T = f (S) can convey reasonable aspect information in the input sequence.
We ﬁrst segment each review into multiple sentences and consider each sentence as an individual training example.26 For a tokenized sequence contains n tokens (w1, w2, · · · , wn), we use BERT to get a contextualized representation for each token (e1, e2, · · · , en), where ei represents the vector for i-th token. Then those contextualized representations can be used as features for token classiﬁcation:
pi = softmax(Wei + b)
where W and b are tunable parameters of the multilayer perceptron. pi is a vector that represents the probability of token i being assigned to different aspects.
26We also tried using larger context such as paragraph, but found out the results less satisfying since the model identiﬁed fewer aspects.

We use the negative log likelihood of the correct labels as training loss:
L = − log ptj
t∈T
where j is the label of token t, and T denotes all the tokens.
We used 900 annotated reviews for training and 100 for validation which is equivalent to using 16,543 training data and 1,700 validation data since we consider sentence as the basic individual training sample. The initial BERT checkpoint we used is “bert-large-cased” which is the large version of BERT with an uncased vocabulary. We used Adam optimizer (Kingma and Ba, 2014) with a learning rate of 5e−5 to ﬁnetune our model. We trained for 5 epochs and saved the model that achieved lowest loss on validation set as our aspect tagger.
A.5 Heuristics for Reﬁning Prediction Results
The seven heuristic rules used for reﬁning the prediction results are listed below. Examples of those rules are shown in Tab. 9.
1. If there are no other tags (they are tagged as “O” which stands for Outside) between two “summary” tags, then replace all tags between them with “summary” tag.
2. If there are multiple discontinuous text spans tagged as “summary”, we keep the ﬁrst one and discard others.
3. If the punctuation is separately tagged and is different from its neighbors, we replace its tag to “O”.
4. If two identical tags are separated by a single other tag, then replace this tag with its right neighbor’s tag.
5. If there exists a single token with a tag and its neighbors are “O”, then replace this tag to ‘O”.
6. For a “non-summary” “non-O” tag span, if its neighbors are “O” and the start/end of this span is not special symbol (for example, punctuations or other symbols that have 1 length), then we expand from its start/end until we meet other “non-O” tag or special symbol.

Heuristics 1
2 3 4 5 6
7

Before
The authors present a method for learning Hamiltonian functions[Summary] · · · [O] this is[Summary] · · · [O] that maps past observations to a latent p, q space in a VAE-like fashion.[Summary]
This paper proposes a new representation learning model for graph optimization, Graph2Seq . [Summary] · · · the theorems are very interesting . [Positive Originality] · · · The performance of Graph2Seq is remarkable.[Summary]
The proposed idea is novel[Positive Originality] .[Positive Motivation] The paper is well written and easy to follow.[Positive Clarity]
The overall notion of[Positive Originality] learnning[O] a Hamiltonian network directly is a great one.[Positive Originality]
It is[O] clearly[Positive Clarity] geared towards DNN practitioners.[O]
In contrast , this aspect[O] is missing from other work on ML[Negative Meaningful Comparison] for optimization.[O]
The authors propose a novel approach to estimate unbalanced optimal transport between sampled measures that scales well in the dimension and in the number of samples · · · The effectiveness of the approach[Summary] is shown on some tasks.[O]

After The authors present a method for learning Hamiltonian functions · · · this is · · · that maps past observations to a latent p, q space in a VAE-like fashion.[Summary] This paper proposes a new representation learning model for graph optimization, Graph2Seq . [Summary] · · · the theorems are very interesting . [Positive Originality] · · · The performance of Graph2Seq is remarkable.[O] The proposed idea is novel[Positive Originality] .[O] The paper is well written and easy to follow.
[Positive Clarity]
The overall notion of learning a Hamiltonian network directly is a great one.[Positive Originality]
It is clearly geared towards DNN practitioners.[O]
In contrast, this aspect is missing from other work on ML for optimization.[Negative Meaningful Comp.]
The authors propose a novel approach to estimate unbalanced optimal transport between sampled measures that scales well in the dimension and in the in the number of samples · · · The effectiveness of the approach is shown on some tasks.[Summary]

Table 9: Examples of seven heuristic rules used for reﬁneing prediction results.

7. If the “summary” span does not end with a period, then we truncate or extend it at most ﬁve words to make it ends with a period.
A.6 An Example of Automatically Annotated Reviews
Tab. 10 illustrates an annotated review after using our trained aspect tagger and heuristic reﬁning rules in Appendix A.5. Although here we do not add separate polarity tags to avoid visual burden, the polarity of each aspect the model predicts is correct.
A.7 Calculation of Aspect Precision and Aspect Recall
To measure aspect precision, we asked three annotators to decide whether each aspect span the model predicted is accurate. They were asked to delete a tagged span if they regarded it as inappropriate. We denote all prediction spans as M, and the ﬁltered spans from annotators as F1, F2 and F3. We

represent nS as the total number of text spans in S. Here we deﬁne correct spans as
C = {l|l ∈ F1, l ∈ F2, l ∈ F3}
The aspect precision is calculated using Formula 5.
Precision = nC (5) nM
For measuring aspect recall, we asked three annotators to label aspect spans that they identiﬁed while the model ignored. We denote the additional labeled spans from one annotator as A where A = {a1, a2, · · · , anA}, ai represents a text span. We denote the additional labeled spans from other two annotators as B and C.
We deﬁne common ignored spans for every two annotators as below. | · | denotes the number of tokens in a span and ∩ takes the intersect span

summary

originality

clarity

meaningful comparison

motivation

substance

This paper studies the graph embedding problem by using encoder-decoder method . The experimental study on real network data sets show the features extracted by the proposed model is good forclassiﬁcation . Strong points of this paper: 1. The idea of using the methods from natural language processing to graph mining is quite interesting . 2. The organization of the paper is clear Weak points of this paper: 1. Comparisons with state-of-art-methods ( Graph Kernels ) is missing . 2. The problem is not well motivated, are there any application of this . What is the difference from the graph kernel methods ? The comparison with graph kernel is missing . 3. Need more experiment to demonstrate the power of their feature extraction methods . ( Clustering, Search, Prediction etc.) 4. Presentation of the paper is weak . There are lots of typos and unclear statements.

Table 10: An example of automatically labeled reviews.

between two spans.
I1 = {ai ∩ bj| |ai ∩ bj| > 0.5} min {|ai|, |bj|}
I2 = {bi ∩ cj| |bi ∩ cj| > 0.5} min {|bi|, |cj|}
I3 = {ai ∩ cj| |ai ∩ cj| > 0.5} min {|ai|, |cj|}
We also deﬁne common ignored spans for three annotators as below.
I = {ai ∩ bj ∩ ck| |ai ∩ bj ∩ ck| > 0.3} min {|ai|, |bj|, |ck|}
where ai, bj, ck are text spans from A, B, C respectively. We assume all the spans the model predicts are correct. Then we can calculate total number of spans using Formula 6.
n =nM + nA + nB + nC − nI1 − nI2 − (6) nI3 + nI
The aspect recall is calculated using Formula 7.
Recall = nM (7) n
A.8 Adjusting BART for Long Documents The ﬁrst attempts we made to directly adjust BART for long text either expanded its positional encodings or segmented the input text and dealt with each segment individually. Below are three ways we attempted.
Arc-I: Position Encoding Expanded BART Since the original BART model is pretrained on 512 sequence length and ﬁntuned on 1024 sequence

length.27 We followed this approach and tried copying the ﬁrst 1024 position encodings periodically for longer sequence and ﬁnetuned the model on our own dataset.
Arc-II: Independently-windowed BART In this architecture, we simply chunked the documents into multiple windows with 1024 window size, and then use BART to encode them separately. The ﬁnal output of the encoder side is the concatenation of those window outputs. The decoder can then generate texts as normal while attending to the whole input representations.
Arc-III: Dependently-windowed BART In Arc-II, we ignore the interdependence between each chunk which may lead to incoherence in generated texts. Here, to model the inter-window dependencies, we followed the approach introduced in Rae et al. (2020). We kept a compressive memory of the past and used this memory to compute the representation of new window. The ﬁnal output of the encoder side is the concatenation of those window outputs as in Arc-II.
However, we found that none of these adjustments could generate satisfying ﬂuent and coherent texts according to our experiments. Common problems include interchanges between ﬁrst and third person narration (They... Our model...), contradiction between consecutive sentences, more descriptive texts and fewer opinions, etc.
A.9 CE Extraction Details
The basic sentence statistics of our ASAP-Review dataset is listed in Tab. 12.
27https://github.com/pytorch/fairseq/issues/1413

KEYWORDS

ﬁnd show imply study bound apply

prove design reduce metric better result

examine explore propose observe present develop

address analyze explain beneﬁt compare measure

suggest achieve perform improve dataset evaluate

baseline maximize minimize increase decrease discover

optimize efﬁcient effective introduce interpret experiment

outperform generalize understand investigate demonstrate state-of-the-art

Table 11: Predeﬁned keywords for ﬁltering sentences.

ICLR NeurIPS Both

Avg. Sentence Num. 216

198

209

Table 12: Sentence statistics of ASAPReview dataset. “Avg. Sentence Num.” denotes average sentence number in a paper.

We use two steps to extract salient sentences from a source document: (i) Keywords ﬁltering, (ii) Cross-entropy method
A.9.1 Keywords Filtering
We have predeﬁned 48 keywords and in the ﬁrst stage, we select sentences containing those keywords as well as their inﬂections. The 48 keywords are shown in Tab. 11. After applying keywords ﬁltering, the statistics of selected sentences are shown in Tab. 13.

ICLR NeurIPS Both

Avg. Sentence Num. 97

85

92

Table 13: Sentence statistics of selected sentences after keywords ﬁltering. “Avg. Sentence Num.” denotes average selected sentence number in a paper.

A.9.2 Cross Entropy Method Following Feigenblat et al. (2017)’s approach in unsupervised summaization. We formalize the sentence extraction problem as a combinatorial optimization problem. Speciﬁcally, we deﬁne the performance function R as below.
R(S) = − pS(w) log pS(w) (8)
w∈S
Count(w) pS(w) = Len(S) (9)
Where S represents the concatenation of selected sentences, Len(S) represents the number of words in S while Count(w) represents the number of times w appears in S. The intuition behind this

performance function is that we want to select sentences that can cover more diverse words. Note that when calculating R(S), we do preprocessing steps (i.e. lowercasing, removing punctuation, removing stop words etc.).
For each paper containing n sentences, we aim to ﬁnd a binary vector p = (p1, · · · , pn) in which pi indicates whether the i-th sentence is selected such that the conbination of selected sentences achieves highest performance score and also contains fewer than 3028 sentences. We did this by using Cross Entropy Method (Rubinstein and Kroese, 2013). The algorithm is shown below.

1. For each paper containing n sentences, we ﬁrst assume that each sentence is equally likely to be selected. We start with p0 = (1/2, 1/2, ..., 1/2). Let t := 1.

2. Draw a sample X1, · · · , XN of Bernoulli vectors with success probability vector pt−1. For each vector, concatenate the sentences selected and get N sequences S1, · · · , SN . Calculate the performance scores R(Si) for all i, and order them from smallest to biggest, R(1) ≤ R(2) ≤ · · · ≤ R(N). Let γt be (1 − ρ) sample quantile of the performances: γt = R( (1−ρ)N ).

3. Use the same sample to calculate pˆt = (pˆt,1, · · · , pˆt,n) via

pˆt,j =

N i=1

I{R(Si)≥γt}I{Xij

=1}

N i=1

I{R(Si)≥γt}

(10)

where I{c} takes the value 1 if c is satisﬁed, otherwise 0.

4. Perform a smoothed update.

pt = αpˆt + (1 − α)pt−1

(11)

28This number is chosen according to our empirical observations. We need to extract sentences that can ﬁt BART’s input length (1024).

5. If the value of γt hasn’t changed for 3 iterations, then stop. Otherwise, set t := t + 1 and return to step 2.
The elements in pt will converge to either very close to 0 or very close to 1. And we can sample from the converged pt to get our extraction.
We chose N = 1000, ρ = 0.05 and α = 0.7 when we ran this algorithm. If we happen to select more than 30 sentences in a sample, we drop this sample. Note that we slightly decrease the initial probability when there are more than 90 sentences after ﬁltering to ensure enough sample number in the ﬁrst few iterations.
A.10 Detailed Analysis and Case Study
We take our aspect-enhanced model using CE extraction to conduct case study. Tab. 16 lists ﬁve examples for each aspect the model mentions. It can be seen that the language use of generated reviews are pretty close to real reviewers.
Evidence-sensitive For aspect-enhanced model, It would also be interesting to trace back to the evidence when the model generates a speciﬁc aspect. To do that we inspect where the model attends when it generates a speciﬁc aspect by looking at the attention values with respect to the source input.29
And interestingly, we found that the model attends to the reasonable place when it generates a speciﬁc aspect. Fig. 9 presents the attention heatmap of several segment texts, the bottom of the ﬁgure shows aspects the model generates. There are some common patterns we found when we examined the attention values between the source input and output.

4. When it generates meaningful comparison, it will attend to places contains “et al.”
A.11 Calculation of Aspect Score For accepted (rejected) papers, we calculate the average aspect score for each aspect.
The aspect score of a review is calculated as follows.
• If an aspect does not appear in a review, then we count the score for this aspect as 0.5 (which stands for neutral)
• If an aspect appears in a review, we denote its occurrences as O = {o1, o2, · · · , on} where n is the total number of occurrences. And we denote the positive occurrences of this aspect as Op = {op1, op2, · · · , opn} where pn is the total number of positive occurrences. The aspect score is calculated using Formula 12.
Aspect Score = pn (12) n
A.12 Bias Analysis for All Models Here, following the methods we proposed in §5.3.1, we list the bias analysis for all models in Fig. 10, Fig. 11, Tab. 14, Tab. 15.
B Supplemental Material
B.1 Dataset Annotation Guideline The annotation guideline for annotating aspects in reviews can be found at https://github.com/neulab/ReviewAdvisor/ blob/main/materials/AnnotationGuideline.pdf

1. When the model generates summary, it will attend to sentences that contain strong indicators like “we propose” or “we introduce”.

2. When it generates originality, it will attend to previous work part as well as places describing contributions of this work.

3. When it generates substance, it will attend to experiment settings and number of experiments conducted;
29The way we aggregate attention values is to take the maximum, no matter is to aggregate tokens to a word or to aggregate different attention heads or to aggregate words to an aspect.

Figure 9: Attention heatmap between source document and generated reviews. + denotes positive sentiment and − denotes negative sentiment.

Native Non-native

MOT

CMP

ORI

MOT

CMP

ORI

MOT

CMP

ORI

MOT

CMP

ORI

CLA

SOU

CLA

SOU

CLA

SOU

REP

SUB

(a) Introduction

MOT

CMP

ORI

REP

SUB

(b) Introduction + aspect

MOT

CMP

ORI

REP

SUB

(c) CE

MOT

CMP

ORI

CLA

SOU

CLA

SOU

CLA

SOU

CLA

SOU

REP

SUB

(d) Reference reviews

REP

SUB

(e) CE + aspect

REP

SUB

(f) Abstract + CE

REP

SUB

(g) Abstract + CE + aspect

Figure 10: Spider chart of aspect scores for all models with regard to nativeness.

INTRO INTRO+ASPECT CE CE+ASPECT ABSTRACT+CE ABSTRACT+CE+ASPECT

MOT

ORI

SOU

-0(b.7)2Gener+a1te8d.7r1eviews+. 3.84 +3.12 +15.75 +6.14 +2.56 +18.33 +11.16 +1.13 +24.77 +28.78 +1.77 +23.01 +3.79 +1.72 +22.23 +12.94

SUB
-3.66 +0.66 -13.41 -2.92 +0.44 -8.30

REP
+0.73 -10.61 -3.71 -3.18 +0.37 -0.38

CLA
-13.32 -13.50 -9.94 -12.02 -15.18 -13.40

CMP
+2.40 +19.05 +13.49 +18.36 -2.13 +0.89

Total
43.39 68.84 72.58 91.18 46.69 59.86

Table 14: Disparity differences regarding nativeness. Total is the sum of absolute value of disparity difference.

Anonymous Non-anonymous

MOT

CMP

ORI

MOT

CMP

ORI

MOT

CMP

ORI

MOT

CMP

ORI

CLA

SOU

CLA

SOU

CLA

SOU

REP

SUB

(a) Introduction

MOT

CMP

ORI

REP

SUB

(b) Introduction + aspect

MOT

CMP

ORI

REP

SUB

(c) CE

MOT

CMP

ORI

CLA

SOU

CLA

SOU

CLA

SOU

CLA

SOU

REP

SUB

(d) Reference reviews

REP

SUB

(e) CE + aspect

REP

SUB

(f) Abstract + CE

REP

SUB

(g) Abstract + CE + aspect

Figure 11: Spider chart of aspect scores for all models with regard to anonymity.

INTRO INTRO + ASPECT CE CE+ASPECT ABSTRACT+CE ABSTRACT+CE+ASPECT

MOT
-5.69 -3.53 +1.89 -4.20 +3.18 +5.45

ORI
-4.43 -1.65 -1.18 -12.32 -0.05 +2.49

SOU
+2.76 +7.85 +0.05 -0.52 -7.96 +2.80

SUB
-0.64 +0.01 -0.44 -2.57 -3.73 +5.69

REP
+5.65 +5.93 +13.09 +2.70 +2.25 +1.33

CLA
+5.80 +11.02 +8.00 +8.75 +8.69 +8.03

CMP
+3.02 +4.20 -2.56 -10.31 -12.02 -3.79

Total
28.00 34.20 27.21 41.37 37.88 29.59

Table 15: Disparity differences regarding anonymity. Total is the sum of absolute value of disparity difference.

Motivation
1. The motivation of using the conditional prior is unclear. 2. I think this paper will be of interest to the NeurIPS community. 3. The idea of continual learning is interesting and the method is well motivated. 4. Overall, I think this paper is a good contribution to the ﬁeld of adversarial robustness. 5. It is hard to understand the motivation of the paper and the motivation behind the proposed methods.
Originality
1. This paper presents a novel approach to cross-lingual language model learning. 2. The novelty of the paper is limited . The idea of using low rank matrices is not new. 3. The proposed method seems to be very similar to the method of Dong et al. ( 2018 ). 4. The idea of using neural networks to learn edit representations is interesting and novel . 5. The proposed method seems to be a simple extension of the batched-E-step method proposed by Shazeer
et al.
Soundness
1. This assumption is not true in practice . 2. The experimental results are not very convincing . 3. But the authors do not provide any theoretical justiﬁcation for this claim. 4. The theoretical results are sound and the experimental results are convincing. 5. The paper does not provide any insights on the reasons for the success of the supervised methods.
Substance
1. The experiments are well-conducted. 2. The ablation study in Section A.1.1 is not sufﬁcient. 3. It would be better to show the performance on a larger dataset. 4. The authors should show the performance on more difﬁcult problems. 5. The experiments are extensive and show the effectiveness of the proposed method.
Replicability
1. It is not clear how the network is trained. 2. The authors should provide more details about the experiments. 3. The authors should provide more details about the hyperparameters. 4. The authors should provide more details about the training procedure. 5. It would be better if the authors can provide more details about the hyperparameters of LST.
Meaningful Comparison
1. The author should compare with [ 1 , 2 , 3 ] and [ 4 ] . 2. The authors should compare the proposed method with existing methods . 3. It would be more convincing if the authors can compare with other methods such as AdaGrad. 4. authors should compare the performance with the state-of-the-art methods in real-world applications . 5. I also think the paper should compare the performance of intrinsic fear with the other methods proposed
in [ 1 , 2 , 3 , 4 , 5 ].
Clarity
1. There are some typos in the paper. 2. The paper is well-written and easy to follow. 3. It is not clear to me how to interpret the results in Table 1. 4. It would be better if the authors can provide a more detailed explanation of the difference. 5. The paper is not well organized . It is hard to follow the description of the proposed method.
Table 16: Examples for different aspect mention from generated reviews.

