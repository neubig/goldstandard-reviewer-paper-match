HIERARCHICAL MULTITASK LEARNING FOR CTC-BASED SPEECH RECOGNITION
Kalpesh Krishna1, Shubham Toshniwal2, Karen Livescu2
1University of Massachusetts, Amherst, USA 2TTI-Chicago, USA
kalpesh@cs.umass.edu {shtoshni, klivescu}@ttic.edu

arXiv:1807.06234v2 [cs.CL] 7 Mar 2019

ABSTRACT Previous work has shown that neural encoder-decoder speech recognition can be improved with hierarchical multitask learning, where auxiliary tasks are added at intermediate layers of a deep encoder. We explore the effect of hierarchical multitask learning in the context of connectionist temporal classiﬁcation (CTC)-based speech recognition, and investigate several aspects of this approach. Consistent with previous work, we observe performance improvements on telephone conversational speech recognition (speciﬁcally the Eval2000 test sets) when training a subword-level CTC model with an auxiliary phone loss at an intermediate layer. We analyze the effects of a number of experimental variables (like interpolation constant and position of the auxiliary loss function), performance in lowerresource settings, and the relationship between pretraining and multitask learning. We observe that the hierarchical multitask approach improves over standard multitask training in our higher-data experiments, while in the low-resource settings standard multitask training works well. The best results are obtained by combining hierarchical multitask learning and pretraining, which improves word error rates by 3.4% absolute on the Eval2000 test sets.
Index Terms— end-to-end, speech recognition, connectionist temporal classiﬁcation, multitask learning, pretraining
1. INTRODUCTION Modern automatic speech recognition (ASR) systems are increasingly moving toward neural “end-to-end” architectures that map audio directly to text, with no interpretable intermediate labels. These models typically pass the input audio features through a multi-layer neural encoder, most often a recurrent neural network (RNN), to obtain a higher-level representation. The output text is then obtained by passing this representation through an RNN-based decoder [1] or using connectionist temporal classiﬁcation (CTC) [2]. This is in contrast to traditional ASR systems, which typically involve several separately trained components such as frame classiﬁers, acoustic models, lexicons, and language models. These individual modules correspond to different levels of representation, such as triphone states, phonemes, graphemes or words.
There is strong evidence that intermediate encoder layers of endto-end architectures, both for ASR and for other tasks, implicitly learn intermediate representations between the input and the ﬁnal output. End-to-end ASR systems appear to learn intermediate phonetic representations [3]; language models learn syntactic representations in lower layers [4]; and deep CNNs trained for image classiﬁcation learn about curves, edges, and object parts in the image [5].
Some recent work has introduced additional auxiliary loss functions at intermediate model layers and have found improvements on the primary task. This is a form of multitask learning [6], but unlike

most multitask learning, the secondary loss function is not applied at the ﬁnal output layer. For example, part-of-speech tag supervision at lower layers can improve performance of a neural syntactic chunker or tagger [7]. More recently, phonetic recognition and frame-level state classiﬁcation losses applied to intermediate layer representations have been found to improve a character-level encoder-decoder recognizer on conversational telephone speech [8], and various levels of intermediate-layer subword losses improve a subword-based CTC recognizer [9].
In this paper we investigate the beneﬁt of a lower-layer phonelevel CTC loss on a subword-level CTC model for speech recognition, trained on the Switchboard 300-hour training set [10] and tested on the Eval2000 test sets [11]. We investigate the effect of the mixing weight between the two CTC losses (Section 5.1), as well as the choice of layer for the phone CTC loss (Section 5.2). We also evaluate our models in lower-resource settings, using only a fraction of the training data (Section 5.3). We compare the hierarchical multitask approach to pretraining with the phone CTC loss and also combine the two approaches (Section 5.4). Finally, we qualitatively compare the CTC alignments produced by the baseline and proposed models (Section 5.5). Throughout the paper, we also evaluate our model’s performance on the auxiliary phonetic recognition task.
2. RELATED WORK Multitask learning (MTL) has been a common tool in machine learning for some time [6], and recent work has found that end-to-end neural speech recognition models can beneﬁt from this approach [8, 12, 13]. Among the ﬁrst work, to our knowledge, to use hierarchical multitask learning in speech recognition is that of Ferna´ndez et al. [14], which used a hierarchical CTC model, similar to ours although only two layers deep, with phoneme label prediction as an auxiliary task in training a spoken digit sequence recognizer. Their experiments, with a ﬁxed interpolation constant and phoneme prediction loss applied at the ﬁrst layer of a two-layer deep RNN encoder, showed no improvement over the baseline. Rao et al. [13] use a hierarchical CTC model for multi-accent speech recognition which is trained with the ASR loss applied at the topmost layer of a deep RNN encoder and a phoneme loss for different accents applied at a ﬁxed intermediate layer with a ﬁxed weight [13]. Audhkhasi et al. [15] experiment with different strategies to pretrain a CTC model with a phonetic loss and then continue training it in a hierarchical MTL framework combining the phonetic loss with the primary ASR loss. Their experiments found that pretraining alone worked as well as the combination of pretraining and multitask learning. In our exploration of a larger space of pretraining + hierarchical multitask learning models, we do ﬁnd a performance advantage with this combined approach (see Section 5.4). Toshniwal et al. [8] ex-

periment with hierarchical MTL in recurrent encoder-decoder models for ASR, and found that applying an auxiliary loss (a phonetic recognition loss, a frame-level state classiﬁcation loss, or both) at an intermediate encoder layer improves performance over both a baseline model and standard MTL with all losses at the topmost layer. Sanabria et al. [9] used a hierarchical CTC model and varied the CTC vocabulary in their auxiliary tasks (character level, and a variety of subword units corresponding to different vocabulary sizes), obtaining excellent ﬁnal results.
Our work here can be seen as an extension of these several past attempts to use variations of hierarchical multitask learning in neural speech recognition. We contribute a thorough investigation of a number of previously unexplored aspects of the hierarchical MTL space, and ﬁnd consistently larger improvements.

3. MODEL

Denote the input acoustic feature sequence x = (x1, x2, ..., xT ).

In all of our models, the input sequence is passed through a multi-

layer bidirectional long short-term memory (LSTM) network [16].

Let the intermediate representation at the ith BiLSTM layer (that

is, the output hidden state sequence after the ith layer) be hi =

(

h

i 1

,

h

i 2

,

...,

h

i T

)

.

3.1. Connectionist Temporal Classiﬁcation

Connectionist temporal classiﬁcation (CTC) is an approach for sequence labeling that uses a neural N -layer “encoder”, which maps the input sequence x to a sequence of hidden states hN , followed by a softmax to produce posterior probabilities of frame-level labels (referred to as “CTC labels”) p(πt|hNt ) for each label πt ∈ C. The posterior probability of a complete frame-level label sequence is given by the product of the individual frame posteriors:

p(π = π1, π2, . . . , πT |x) = p(πt|hNt )

(1)

t

The CTC label set C consists of all of the possible true output labels plus a “blank” symbol. Given a CTC label sequence, the hypothesized ﬁnal label sequence is given by collapsing consecutive identical frame CTC labels followed by removing blank symbols. We use B(π) to denote the collapsing function. All of the model parameters are learned jointly using the CTC loss function, which is the log posterior probability of the training label sequence z = z1, z2 . . . , zL given input sequence x,

log p(z|x) = log

p(π|x)

(2)

π∈B −1 (z)

= log

p(πt|hNt )

(3)

π∈B−1(z) t

Model parameters are learned using gradient descent; the gradient is computed via the forward/backward technique [2].

3.2. Baseline Model With a multi-layer bidirectional LSTM (BiLSTM) as our “encoder”, we obtain a number of layer-speciﬁc hidden state sequences, h1, h2, ..., hN , where N is the number of layers in the BiLSTM. The ﬁnal encoder layer is followed by a softmax to produce label posteriors. In this paper we use subword-level labels1 which offer a
1By “subword” here we refer speciﬁcally to sequence of characters contained within a word.

good tradeoff between open-vocabulary decoding and lexical constraints. These types of output labels have been used recently to build state-of-the-art open-vocabulary systems in speech recognition [17] and machine translation [18], speciﬁcally based on “wordpieces” constructed with byte pair encoding algorithm [19, 20].

3.3. Hierarchical Multitask Training Our primary objective is the subword-level CTC loss, applied to the softmax output after the ﬁnal (N th) encoder layer. In our hierarchical multitask approach, shown in Figure 1, we add a second softmax after the ith encoder layer, to which we apply a phone-level CTC loss. This softmax layer is added in parallel to the main model, and is used only at training time. At test time only the main subwordlevel CTC model is used. The choice of layer i for the phone loss is a tunable hyperparameter. The multitask loss is a convex combination of the two loss functions, using a tunable interpolation parameter λ:

L = λLsubword(hN , z) + (1 − λ)Lphone(hi, z)

(4)

While we use a phone-level auxiliary task here, the hierarchical multitask learning idea is general and can be used with any relevant intermediate loss, or multiple intermediate losses (as in [8]).

Final Layer

N
h Subword CTC

Intermediate Layer

i h
Phone CTC

First Layer Stacking Consecutive
Frames Input Layer
Fig. 1. A sketch of the hierarchical multitask learning model. Pairs of consecutive input frames are concatenated for a reduction in time resolution at the input. The Subword CTC loss is applied to the ﬁnal layer. The Phone CTC loss is applied to an intermediate BiLSTM layer.
4. EXPERIMENTAL SETUP 4.1. Data We train all models on the Switchboard corpus (LDC97S62) [10], which contains roughly 300 hours of conversational telephone speech. Following the Kaldi recipe [21], we reserve the ﬁrst 4K utterances as a development set. Since the training set has many repetitions of short utterances (like “uh-huh”), we remove duplicates beyond a count threshold of 300. The ﬁnal training set has about 192K utterances. For evaluation, we use the HUB5 Eval2000 data set (LDC2002S09) [11], consisting of two subsets: Switchboard (SWB), which is similar in style to the training set, and CallHome

(CH), which contains conversations between friends and family. For input features, we use 40-dimensional log-mel ﬁlterbank features along with their deltas, with per-speaker mean and variance normalization. For obtaining the phone labels, we map the words to their canonical pronunciations using the lexicon obtained from running the Kaldi Switchboard training recipe.
4.2. Model Details and Inference Our encoder is a 5-layer bidirectional LSTM network [22], with 320 hidden units in each direction. A dropout [23] mask has been applied on the output of each RNN layer [24] (with dropout rate 0.1). We concatenate every two consecutive input vectors (as in [25, 26]), which reduces the time resolution by a factor of two and speeds up computation. For inference, we use greedy decoding with no language model.
We use a ﬁxed vocabulary of 1000 wordpieces, which includes all of the characters to ensure open-vocabulary coverage. This vocabulary is generated using a variant of the byte pair encoding algorithm [19] implemented in the SentencePiece library by Google.2
4.3. Training Details We bucket training data by utterance length into 5 buckets, restricting utterances within a minibatch to come from a single bucket for training efﬁciency. Different minibatch sizes are used for different buckets. For the shortest utterances a larger batch size of 128 is used while for the longest sequences the batch size is reduced to 32. We evaluate our models every 500 updates (3006 updates per epoch), measuring the word error rate (WER) on development data. We use Adam [27] for optimization with a ﬁxed learning rate of 0.001 for the ﬁrst 25K updates. Subsequently, the learning rate is reduced by a factor of 2 in case the current development WER is worse than the worst development WER of the previous 3 checkpoints. Training stops when there is no improvement in development WER for ten consecutive checkpoints. Early stopping on development set WER has been used to choose the ﬁnal model for evaluation; that is, we use the model checkpoint with the best development set WER for ﬁnal evaluation. All models are trained on a single GPU using TensorFlow r1.4 [28].
5. RESULTS For our ﬁrst experiment, we establish that our models are comparable to prior work and that hierarchical multitask training improves performance over a baseline CTC model. The baseline model corresponds to ﬁxing λ = 1 (that is, using the subword CTC loss only). We compare this baseline to a model trained with a multitask objective with λ = 0.5 and the auxiliary phone CTC loss applied on the third layer (i = 3) of the 5-layer encoder. The results are given in Table 1, along with several other recently published results in the same setting of lexicon-free recognition without a language model on the same test sets. We observe a signiﬁcant performance boost when including the auxiliary phone CTC loss. Our models are competitive with recently published results. Zenkel et al. [20] report performance improvements using larger subword vocabularies, which we have not experimented with here. Zeyer et al. [29] use an attention model with a novel layer-wise pretraining scheme.
We next analyze several aspects of our approach. 2https://github.com/google/sentencepiece

Table 1. Word error rates (%) of several models on the Switchboard development set and on Eval2000. SWB, CH = Switchboard, CallHome partitions of the Eval2000 corpus. Enc-Dec = Encoderdecoder.

Model

Dev

Eval2000

SWB CH full

Our models

Baseline (λ = 1.0)

25.5 21.5 33.8 27.7

Multitask (λ = 0.5, i = 3) 21.5 18.6 30.8 24.7

Enc-Dec Zeyer et al. [29]

- 13.1 26.1 19.7

CTC Zweig et al. [26] Zenkel et al. [20] Sanabria et al. [9] Audhkhasi et al. [30]

- 24.7 37.1 - 17.8 30.4 - 17.4 28.5 - 14.6 23.6 -

CTC + Multitask Sanabria et al. [9]

- 14.0 25.5 -

5.1. Effect of the Interpolation Weight Next we explore how the mixing weight between the two terms in the multitask loss affects performance. We ﬁx the auxiliary phone CTC loss on the fourth layer of the encoder (i = 4) and investigate the dependence of performance on the multitask interpolation constant λ. Development set results are shown in Figure 2. The multitask model with any value of λ = 1 outperforms the baseline (λ = 1), with the best performance attained at λ = 0.7.
While phonetic recognition is not a goal of our work, it is interesting to study how the auxiliary model performs, and whether there is a correlation between performance of the auxiliary and main models. We therefore also plot the phonetic error rate (PER) of the models as well. We observe a mostly monotonic improvement in PER with decreasing λ, that is with increasing weight on the phone CTC loss. There is a slightly worse performance for λ = 0 (13.5%) compared to λ = 0.1 (13.4%). This suggests that, if we view the phonetic recognition task as the primary task, additional supervision from the higher-level subword CTC task might help for a suitably small interpolation constant, although the effect is at best very small in our setting. Note that there is no single interpolation constant that optimizes performance for both tasks.
5.2. Position of Auxiliary Loss We next investigate the effect of the position of the auxiliary phone CTC loss function. We ﬁx λ = 0.5 and vary i, the layer to which the phone CTC loss is applied. The development set results are given in the right-most column (the 100% column) of Table 2. All models match or outperform the baseline (λ = 1.0), with i = 3 giving the best WER. We obtain better performance for i = 3, 4 than for i = 5, which is consistent with the previously observed beneﬁt of applying the auxiliary loss to intermediate representations, rather than to the ﬁnal output representation as in standard multitask learning [8]. We also note that i = 1 produces considerably worse performance than i = 2, 3, 4, 5. We again also evaluate the phonetic recognition output, in terms of phonetic error rate (PER), produced by the auxiliary phone CTC model in our multitask models; see the 100% column in

Error rate
Word error rate (WER) in %

26 24 22 20 18 16 14
0.0

PER

0.2

0.4

0.6

0.8

Interpolation constant

WER 1.0

Fig. 2. Effect of varying the interpolation constant λ on the development set word error rate (WER, %) and phonetic error rate (PER, %).

30

Baseline 100%

Multitask 50%

28

Multitask 100%

26

24

22

1

2

3

4

5

Layer of phone CTC loss

Fig. 3. Development set word error rate as the position of the phone CTC loss (value of i) is varied, when training on 50% of the training set or the full (100%) training set. For reference, we also plot the result of the baseline model trained on the full data set (“Baseline 100%”). The complete results are given in Table 2.

Table 3. There is a consistent, fairly large improvement in PER as the phone CTC loss is placed on higher layers. This is in contrast with the WER, which has a trough at intermediate values of i. In other words, there is no single placement of the auxiliary loss that is best for both tasks.
These results – in particular, the worse WER at i = 1 and the monotonic improvement in PER with i – may seem to be in contrast with earlier work analyzing learned neural representations [3], which reported that the ﬁrst layer of a deep CTC recognizer (specifically, DeepSpeech 2 [31]) is most useful for phonetic classiﬁcation. However, in this earlier work the model was trained only with the ﬁnal character-level loss, and the phonetic classiﬁcation tasks were trained on the resulting ﬁxed representations. We therefore do not consider these results to be conﬂicting. This contrast does, however, indicate that it would be challenging to attempt to ﬁnd the optimal layer for an auxiliary loss using an ofﬂine test on the baseline model.

Table 2. Development set word error rate (WER, %) as the training set size is varied (10% to 100% of the standard 300-hour training set) and as the position of the auxiliary phone CTC loss is varied. “X” indicates that the model did not converge.

Model

10% 25% 50% 100%

Baseline (λ = 1.0)

X 39.7 29.1 25.5

Multitask (λ = 0.5)

i=1

48.4 36.1 28.6 25.5

i=2

44.7 35.3 26.8 23.0

i=3

45.6 33.4 25.7 21.5

i=4

44.7 33.7 26.5 22.4

i=5

44.7 33.2 26.8 22.8

5.3. Reduced Training Data
We next investigate the effect of hierarchical multitask training in lower-data settings, speciﬁcally with only 10%, 25%, and 50% of the full 300-hour Switchboard training set. To obtain a fraction of the full training set, we sample the same fraction of data from each of the ﬁve buckets which have utterances of different length distributions (Section 4.3). This is done to roughly maintain the same utterance length distribution in the partial training data. We ﬁx λ = 0.5 and vary i = 1, 2, 3, 4, 5. We also train the baseline model (λ = 1.0) on all of the partial training sets.
The resulting word error rates (WER) are given in the 10%, 25% and 50% columns of Table 2. For the smallest training set (10%), the baseline model (λ = 1.0) fails to converge, while all of the multitask models do converge. This is in line with prior results [8] indicating that hierarchical multitask learning not only boosts performance of end-to-end ASR systems, but also eases optimization. As in the full data setting (100% column of Table 2), all of the multitask models outperform the baseline (λ = 1.0), and the improvement is greater for i = 2, 3, 4, 5 than for i = 1. Interestingly, in the lowest-data settings (10% and 25%), unlike in the 50% and 100% data setting, there does not appear to be a beneﬁt when placing the auxiliary task

Table 3. Development set phonetic error rate (PER, %) of the multitask model (with λ = 0.5) as the training set size is varied (10% to 100% of the standard 300-hour training set) and as the position of the phone CTC loss is varied.

Model 10% 25% 50% 100%

i=1 i=2 i=3 i=4

47.5 41.1 37.1 36.2 35.0 31.0 25.2 22.8 31.5 23.6 19.9 17.0 27.9 21.8 16.8 15.0

i=5

26.8 19.7 15.6 13.2

on intermediate BiLSTM layers: The model with i = 5 matches or slightly outperforms i = 1, 2, 3, 4 in the low 10% and 25% training data settings. We add a graphical comparison between the 50% data and 100% data setting in Figure 3. We note that at the 50% data setting, the best hierarchical multitask model is able to almost match the baseline model using the full data set (25.7% vs. 25.5% WER). This is consistent with the intuition that multitask training should improve generalization.
As before, we present results on the auxiliary phonetic recognition task in the partial data setting in Table 3. A regular trend is observed, with PER always improving signiﬁcantly as the phone CTC loss is placed on higher layers (moving down in Table 3). For the 10% and 25% settings, i = 5 is the auxiliary loss position that maximizes performance on both tasks.
5.4. Pretraining vs. Multitask Training Hierarchical multitask learning is intended to improve the learnt intermediate representations via additional auxiliary supervision at intermediate layers. Another way of potentially achieving this effect is by pretraining the lower model layers with the auxiliary loss such as the phonetic loss. For example, Audhkhasi et al. [15, 30] reported signiﬁcant improvements by pretraining a whole-word BiLSTM CTC network with a phone CTC loss, as well as more modest improvements in a hierarchical setting where all but the last BiLSTM layer were pretrained and the full model was then trained with multitask (word + phone CTC) learning.
This approach bears resemblance to transfer learning for neural models, which is often used in computer vision [32], and also sometimes in natural language processing (NLP) [4] and speech recognition [15]. In a typical transfer learning approach, a model is initially pretrained on a basic task (e.g., language modelling in NLP or image classiﬁcation in computer vision). The learned weights of the ﬁrst few layers are then used to initialize the lower layers of models for other tasks (such as sentiment classiﬁcation or question answering for NLP). This approach is a form of pretraining, but with the pretraining typically done on a different data set from that of the ﬁnal task.
In our pretraining experiments, we start out by training a 3-, 4- or 5-layer model with only the phone CTC loss (λ = 0.0 for i = 3, 4, 5). For this pretraining phase, we follow an optimization scheme identical to that of Section 4, with early stopping based on development set phonetic error rates instead of word error rates. After this pretraining, we initialize a 5-layer subword CTC model with the learnt phone CTC weights for the ﬁrst i layers, and randomly for the remaining layers3, and train all the layers with only the subwordlevel CTC loss (λ = 1.0) until convergence (we call these models “Pretrain”).
Finally, we also consider combining pretraining and multitask learning. In this approach we begin as in the “Pretrain” models, by pretraining i layers with a phone CTC loss and then initializing a 5-layer model with the pretrained weights on the lower layers and random weights on the remaining layers. However, in the combined approach (“Pretrain + Multitask”), we keep the phone softmax on layer i and train with a multitask phone CTC + subword CTC loss with λ = 0.5.
We present our results in Table 4. Both “Multitask” and “Pretrain” improve over the baseline for all values of i, with the best development WER at i = 3 and i = 4 respectively. While the multitask models outperform the pretrained ones on the development set,
3The ﬁnal softmax layer in the subword-level CTC model is randomly initialized regardless of i.

they have nearly identical performance on the test sets. However, for all values of i, the “Pretrain + Multitask” model outperforms the corresponding “Multitask” and “Pretrain” models. For all three model types, a hierarchical CTC model (i = 3, 4) outperforms the vanilla multitask setting (i = 5). The ﬁnal best-performing model is “Pretrain + Multitask” with i = 4, obtaining a 3.6% absolute reduction in word error rate on the Switchboard test set and a 3.1% absolute reduction on CallHome.4
As in our previous experiments, we also examine the development set phonetic error rates in Table 4. For “Pretrain”, the PER is calculated after pretraining on the Phone CTC loss, and before the actual training the subword-level CTC model. For “Multitask” and “Pretrain + Multitask”, the ﬁnal trained phone CTC model is evaluated. We observe a consistent trend: the best PERs are obtained with i = 5 across all model types, and for a given i the “Pretrain + Multitask” model performs best, followed by “Pretrain” and then “Multitask”.

Table 4. A comparison of pretraining and multitask learning on our Switchboard development set and evaluation of the tuned models on Eval2000. SWB, CH = Switchboard, CallHome partitions of the Eval2000 corpus. All of the “Multitask” models are trained with λ = 0.5. i denotes the position of the phone CTC loss throughout the experiment. PER, WER refer to development set phonetic error rate and word error rate. PER for the “Pretrain” models has been calculated right after pretraining (and before training the subword CTC model).

Model

Dev

Eval2000

PER WER SWB CH full

Baseline (λ = 1.0)

-

25.5 21.5 33.8 27.7

Multitask i=3 i=4 i=5

17.0 21.5 18.6 30.8 24.7

15.0 22.4 -

-

-

13.2 22.8 -

-

-

Pretrain i=3 i=4 i=5

15.2 22.2 -

-

-

13.5 22.0 18.6 30.7 24.7

13.1 23.7 -

-

-

Pretrain + Multitask

i=3

14.9 21.4 -

-

-

i=4

12.9 21.2 17.9 30.7 24.3

i=5

12.8 23.3 -

-

-

5.5. Example Alignments Finally, we examine the frame-level CTC outputs for the baseline model (λ = 1.0) and a multitask model (λ = 0.5, i = 4) and compare them in Figure 4. These sequences are obtained from a greedy decoding of the subword-level output layer of the two models, and the phone output layer of the multitask model. We compare our outputs with the Mississippi State ground-truth word alignments [33].
We notice that the multitask model generally (but not always) outputs its output token one frame (20ms) earlier than the baseline model does, perhaps indicating its higher conﬁdence in prediction. Alignments for 128 development set examples can be
4It may be possible to further improve results by also tuning λ.

Baseline subword

Multitask subword

Multitask phone

f

Ground Truth

for for
er for

the the the dh dh dh ah the

last

last

ll

ae

last

s

t

Baseline subword Multitask subword Multitask phone Ground Truth

the the dh ah
the

ss

se

co co co

se

eh

k

second

nd nd co nd nd
ax n n d

Fig. 4. Alignment of per-frame CTC outputs for the baseline and proposed model, compared with ground-truth alignments. The “ ” token refers to the CTC blank symbol.

found at http://martiansideofthemoon.github.io/ ctcalign/.
6. CONCLUSION
We take a step forward in end-to-end neural speech recognition by presenting a detailed analysis of a hierarchical multitask learning framework for CTC-based recognition. This is a general approach for deep end-to-end neural models, where we use some prior knowledge about intermediate tasks to design auxiliary tasks applied at an intermediate layer during training. In our case we use phonetic recognition as an intermediate task along the way to word (or in this case, wordpiece) recognition.
The key takeaways from our paper are the following: 1) Multitask learning consistently improves performance. In our higherresource settings, hierarchical multitask learning is better than the vanilla multitask approach, whereas the vanilla multitask approach performs better in the lower-resource experiments 2) Pretraining with the auxiliary task and multitask learning both improve performance in a hierarchical setting, but the best results are obtained by combining the two approaches, again in a hierarchical fashion 3) In general there is no single task interpolation constant that gives the best results on both the main task (word recognition) and auxiliary (phonetic recognition) task. The approach therefore is helpful for the main task but at best only very slightly for the lower-level task. It is worth further considering in future work, however, whether higher-level tasks can also help lower-level ones. 4) Multitask learning assists optimization as well as generalization, reconﬁrming prior work.
Our best-performing models, which combine pretraining and multitask learning, achieve 3.4% absolute improvement in word error rate on Eval2000 over the baseline subword-level CTC model. This is a larger improvement, over a stronger baseline, than has been found previously for encoder-decoder speech recognition models on the same data [8], and using a single auxiliary task unlike the combination of multiple tasks in [8]. Additional auxiliary tasks may further improve CTC-based models as well.
Future work includes developing ways of pre-determining the best design and hyperparameter choices (auxiliary task, interpolation constant, and position of auxiliary loss) to avoid training and testing many complete multitask models; further study of hierarchical multitask training on an even larger range of data set sizes; and exploration of additional auxiliary tasks.

7. REFERENCES
[1] Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning phrase representations using rnn encoder-decoder for statistical machine translation,” in Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.
[2] Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber, “Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,” in Proc. International Conference on Machine Learning (ICML), 2006.
[3] Yonatan Belinkov and James Glass, “Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems,” in Proc. Neural Information Processing Systems (NIPS), 2017.
[4] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer., “Deep contextualized word representations,” in Proc. Human Language Technology/Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL), 2018.
[5] Matthew D Zeiler and Rob Fergus, “Visualizing and understanding convolutional networks,” in Proc. European Conference on Computer Vision (ECCV), 2014.
[6] Rich Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, 1997.
[7] Anders Søgaard and Yoav Goldberg, “Deep multi-task learning with low level tasks supervised at lower layers,” in Proc. Association for Computational Linguistics (ACL), 2016.
[8] Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu, “Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition,” in Proc. Interspeech, 2017.
[9] Ramon Sanabria and Florian Metze, “Hierarchical Multi Task Learning With CTC,” in Proc. IEEE Spoken Language Technology Workshop (SLT), 2018.
[10] John J Godfrey, Edward C Holliman, and Jane McDaniel, “Switchboard: Telephone speech corpus for research and development,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1992.

[11] LD Consortium et al., “2000 HUB5 English Evaluation Speech LDC2002S09,” Web Download. Philadelphia: Linguistic Data Consortium, 2002.
[12] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.
[13] Kanishka Rao and Hasim Sak, “Multi-Accent Speech Recognition with Hierarchical Grapheme Based Models,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.
[14] Santiago Fernndez, Alex Graves, and Jrgen Schmidhuber, “Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks.,” in Proc. International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2007.
[15] Kartik Audhkhasi, Bhuvana Ramabhadran, George Saon, Michael Picheny, and David Nahamoo, “Direct Acousticsto-Word Models for English Conversational Speech Recognition,” in Proc. Interspeech, 2017.
[16] Sepp Hochreiter and Ju¨rgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, 1997.
[17] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Katya Gonina, et al., “State-of-the-art Speech Recognition With Sequence-to-Sequence Models,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018.
[18] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” CoRR, vol. abs/1609.08144, 2016.
[19] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural Machine Translation of Rare Words with Subword Units,” in Proc. Association for Computational Linguistics (ACL), 2016.
[20] Thomas Zenkel, Ramon Sanabria, Florian Metze, and Alex Waibel, “Subword and Crossword Units for CTC Acoustic Models,” in Proc. Interspeech, 2017.
[21] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi Speech Recognition Toolkit,” in Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.
[22] Alex Graves, Santiago Ferna´ndez, and Ju¨rgen Schmidhuber, “Bidirectional LSTM networks for improved phoneme classiﬁcation and recognition,” in International Conference on Artiﬁcial Neural Networks, 2005.
[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Dropout: a simple way to prevent neural networks from overﬁtting,” Journal of Machine Learning Research, vol. 15, no. 1, 2014.
[24] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, “Recurrent Neural Network Regularization,” CoRR, vol. abs/1409.2329, 2014.
[25] Has¸im Sak, Andrew Senior, Kanishka Rao, and Franc¸oise Beaufays, “Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition,” in Proc. Interspeech, 2015.

[26] Geoffrey Zweig, Chengzhu Yu, Jasha Droppo, and Andreas Stolcke, “Advances in all-neural speech recognition,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.
[27] Diederik P Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” in Proc. International Conference on Learning Representations (ICLR), 2015.
[28] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al., “Tensorﬂow: A System for Large-Scale Machine Learning,” in Proc. USENIX Conference on Operating Systems Design and Implementation (OSDI), 2016.
[29] Albert Zeyer, Kazuki Irie, Ralf Schlu¨ter, and Hermann Ney, “Improved training of end-to-end attention models for speech recognition,” in Proc. Interspeech, 2018.
[30] Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, and Michael Picheny, “Building competitive direct acoustics-to-word models for English conversational speech recognition,” in Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018.
[31] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al., “Deep speech 2: End-to-end speech recognition in English and Mandarin,” in Proc. International Conference on Machine Learning (ICML), 2016.
[32] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic, “Learning and transferring mid-level image representations using convolutional neural networks,” in Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2014.
[33] Neeraj Deshmukh, Aravind Ganapathiraju, Andi Gleeson, Jonathan Hamaker, and Joseph Picone, “Resegmentation of switchboard,” in Fifth international conference on spoken language processing, 1998.

