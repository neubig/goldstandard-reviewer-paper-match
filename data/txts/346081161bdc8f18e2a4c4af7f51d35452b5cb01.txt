Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies

Q1 Did Aristotle use a laptop?

Mor Geva1,2, Daniel Khashabi2, Elad Segal1, Tushar Khot2, Q2 Was AristotlDe aalivne wRheonth3, Jonathan Berant1,2
the laptop was invented?

implicit 1Tel Aviv Unievxeprlicsiitty 2Allen Institute for AI 3University of Pennsylvania

E

morgDeva@mail.tau.ac.il, {danielk,tushark}@allenai.org,

“Aristotle (384-322 BC) was

e12l.. WWahhdee.nn sdwiadesAgtrhiasetlolatp@letoglipvmein?aveinlted.?com, danroth@seas.upenn.edu

a philosopher…”

3. Is #2 before #1? joberant@cs.tau.ac.il

arXiv:2101.02235v1 [cs.CL] 6 Jan 2021

“The ﬁrst laptop was… in 1980”

A ANobstract

A key limitation in current datasets for

multi-hop reasoning is that the required

ste1p.sWfhoernadnisdwAerrisintogtlethleiveq?uestion areretmrieevna-l tio2n.eWd hinenitweaxsptlhiceitlalyp.toIpninthveisntwedo?rk, rweterieivna-l

tro3d.uIcse#2STbRefAoTreEG#1Y?QA, a question aonpsewraetri-on

ing (QA) benchmark where the required rea-

Q1 Did Arisstoontilengusseteaps arQe2imWpalsicAitriisntothtlee qauliveestwiohne,nand laptop?should be inferretdheuslainpgtopa wstarastiengvye.ntAedf?un-

damental challenge in this setup is how to

implicit

explicit

elicit such creative questions from crowd-

D sourcing workers, while coverEing a broad

1. When did Aristotle live?

“Aristotle (384

2. Wrhaenngewaosf tpheotleanpttioapl isntvreantetgeide?s. W–e3p2r2oBpoCs)ewas a

3. Isa#2dabteafocroell#e1c?tion procedure that pchoilmosboinpehser…”

term-based priming to inspire annotators,

careful control over the annotato“Tr hpeoﬁprusltal-aptop

tion, andAadNvoersarial ﬁltering for ewlimasi.n..aitnin1g980.”

reasoning shortcuts. Moreover, we anno-

tate each question with (1) a decomposition

into reasoning steps for answering it, and (2)

Wikipedia paragraphs that contain the an-

swers to each step. Overall, STRATEGYQA

includes 2,780 examples, each consisting of

a strategy question, its decomposition, and

evidence paragraphs. Analysis shows that

questions in STRATEGYQA are short, topic-

diverse, and cover a wide range of strate-

gies. Empirically, we show that humans per-

form well (87%) on this task, while our best

baseline reaches an accuracy of ∼ 66%.

1 Introduction
Developing models that successfully reason over multiple parts of their input has attracted substantial attention recently, leading to the creation of many multi-step reasoning Question Answering (QA) benchmarks (Welbl et al., 2018; Talmor and Berant, 2018; Khashabi et al., 2018; Yang et al., 2018; Dua et al., 2019; Suhr et al., 2019).
Commonly, the language of questions in such benchmarks explicitly describes the process for deriving the answer. For instance (Figure 1, Q2), the question “Was Aristotle alive when the laptop was

Q1 Did Aristotle use a laptop?
implicit

Q2 Was Aristotle alive when the laptop was invented?
explicit

D 1. When did Aristotle live?
2. When was the laptop invented?
3. Is #2 before #1?

E “Aristotle (384-322 BC) was a philosopher…”

A No

“The ﬁrst laptop was… in 1980”

Figure 1: Questions in STRATEGYQA (Q1) require implicit decomposition into reasoning steps (D), for which we annotate supporting evidence from Wikipedia (E). This is in contrast to multi-step questions that explicitly specify the reasoning process (Q2).

invented?” explicitly speciﬁes the required reasoning steps. However, in real-life questions, reasoning is often implicit. For example, the question “Did Aristotle use a laptop?” (Q1) can be answered using the same steps, but the model must infer the strategy for answering the question – temporal comparison, in this case.
Answering implicit questions poses several challenges compared to answering their explicit counterparts. First, retrieving the context is difﬁcult as there is little overlap between the question and its context (Figure 1, Q1 and ‘E’). Moreover, questions tend to be short, lowering the possibility of the model exploiting shortcuts in the language of the question. In this work, we introduce STRATEGYQA, a boolean QA benchmark focusing on implicit multi-hop reasoning for strategy questions, where a strategy is the ability to infer from a question its atomic sub-questions. In contrast to previous benchmarks (Khot et al., 2020a; Yang et al., 2018), questions in STRATEGYQA are not limited to predeﬁned decomposition patterns and cover a wide range of strategies that humans apply when answering questions.
Eliciting strategy questions using crowdsourcing is non-trivial. First, authoring such questions requires creativity. Past work often col-

lected multi-hop questions by showing workers an entire context, which led to limited creativity and high lexical overlap between questions and contexts and consequently to reasoning shortcuts (Khot et al., 2020a; Yang et al., 2018). An alternative approach, applied in Natural Questions (Kwiatkowski et al., 2019) and MS-MARCO (Nguyen et al., 2016), overcomes this by collecting real user questions. However, can we elicit creative questions independently of the context and without access to users?
Second, an important property in STRATEGYQA is that questions entail diverse strategies. While the example in Figure 1 necessitates temporal reasoning, there are many possible strategies for answering questions (Table 1). We want a benchmark that exposes a broad range of strategies. But crowdsourcing workers often use repetitive patterns, which may limit question diversity.
To overcome these difﬁculties, we use the following techniques in our pipeline for eliciting strategy questions: (a) we prime crowd workers with random Wikipedia terms that serve as a minimal context to inspire their imagination and increase their creativity; (b) we use a large set of annotators to increase question diversity, limiting the number of questions a single annotator can write; and (c) we continuously train adversarial models during data collection, slowly increasing the difﬁculty in question writing and preventing recurring patterns (Bartolo et al., 2020).
Beyond the questions, as part of STRATEGYQA, we annotated: (a) question decompositions: a sequence of steps sufﬁcient for answering the question (‘D’ in Figure 1), and (b) evidence paragraphs: Wikipedia paragraphs that contain the answer to each decomposition step (‘E’ in Figure 1). STRATEGYQA is the ﬁrst QA dataset to provide decompositions and evidence annotations for each individual step of the reasoning process.
Our analysis shows that STRATEGYQA necessitates reasoning on a wide variety of knowledge domains (physics, geography, etc.) and logical operations (e.g. number comparison). Moreover, experiments show that STRATEGYQA poses a combined challenge of retrieval and QA, and while humans perform well on these questions, even strong systems struggle to answer them.
In summary, the contributions of this work are:
1. Deﬁning strategy questions – a class of question requiring implicit multi-step reasoning.

2. STRATEGYQA, the ﬁrst benchmark for implicit multi-step QA, that covers a diverse set of reasoning skills. STRATEGYQA consists of 2,780 questions, annotated with their decomposition and per-step evidence.
3. A novel annotation pipeline designed to elicit quality strategy questions, with minimal context for priming workers.
The dataset and codebase are publicly available at https://allenai.org/data/ strategyqa.
2 Strategy Questions
2.1 Desiderata
We deﬁne strategy questions by characterizing their desired properties. Some properties, such as whether the question is answerable, also depend on the context used for answering the question. In this work, we assume this context is a corpus of documents, speciﬁcally, Wikipedia, which we assume provides correct content.
Multi-step Strategy questions are multi-step questions, that is, they comprise a sequence of single-step questions. A single-step question is either (a) a question that can be answered from a short text fragment in the corpus (e.g. steps 1 and 2 in Figure 1), or (b) a logical operation over answers from previous steps (e.g. step 3 in Figure 1). A strategy question should have at least two steps for deriving the answer. Example multiand single- step questions are provided in Table 2. We deﬁne the reasoning process structure in §2.2.
Feasible Questions should be answerable from paragraphs in the corpus. Speciﬁcally, for each reasoning step in the sequence, there should be sufﬁcient evidence from the corpus to answer the question. For example, the answer to the question “Would a monocle be appropriate for a cyclop?” can be derived from paragraphs stating that cyclops have one eye and that a monocle is used by one eye at the time. This information is found in our corpus, Wikipedia, and thus the question is feasible. In contrast, the question “Does Justin Beiber own a Zune?” is not feasible, because answering it requires going through Beiber’s belongings, and this information is unlikely to be found in Wikipedia.
Implicit A key property distinguishing strategy questions from prior multi-hop questions is their

Question Can one spot helium? (No)
Would Hades and Osiris hypothetically compete for real estate in the Underworld? (Yes) Would a monocle be appropriate for a cyclop? (Yes) Should a ﬁnished website have lorem ipsum paragraphs? (No) Is it normal to ﬁnd parsley in multiple sections of the grocery store? (Yes)

Implicit facts Helium is a gas, Helium is odorless, Helium is tasteless, Helium has no color
Hades was the Greek god of death and the Underworld. Osiris was the Egyptian god of the Underworld.
Cyclops have one eye. A monocle helps one eye at a time.
Lorem Ipsum paragraphs are meant to be temporary. Web designers always remove lorem ipsum paragraphs before launch.
Parsley is available in both fresh and dry forms. Fresh parsley must be kept cool. Dry parsley is a shelf stable product.

Table 1: Example strategy questions and the implicit facts needed for answering them.

Question

MS IM Explanation

Was Barack Obama born in the United States? (Yes)
Do cars use drinking water to power their engine? (No)
Are sharks faster than crabs? (Yes)
Was Tom Cruise married to the female star of Inland Empire? (No) Are more watermelons grown in Texas than in Antarctica? (Yes) Would someone with a nosebleed beneﬁt from Coca? (Yes)

The question explicitly states the required information for the answer – the birth place of Barack Obama. The answer is likely to be found in a single text fragment in Wikipedia. The question explicitly states the required information for the answer – the liquid used to power car engines. The answer is likely to be found in a single text fragment in Wikipedia. The question explicitly states the required reasoning steps: 1) How fast are sharks? 2) How fast are crabs? 3) Is #1 faster than #2? The question explicitly states the required reasoning steps: 1) Who is the female star of Inland Empire? 2) Was Tom Cruise married to #2? The answer can be derived through geographical/botanical reasoning that the climate in Antarctica does not support growth of watermelons. The answer can be derived through biological reasoning that Coca constricts blood vessels, and therefore, serves to stop bleeding.

Table 2: Example questions demonstrating the multi-step (MS) and implicit (IM) properties of strategy questions.

implicit nature. In explicit questions, each step in the reasoning process can be inferred from the language of the question directly. For example, in Figure 1, the ﬁrst two questions are explicitly stated, one in the main clause and one in the adverbial clause. Conversely, reasoning steps in strategy questions require going beyond the language of the question. Due to language variability, a precise deﬁnition of implicit questions based on lexical overlap is elusive, but a good rule-of-thumb is the following: if the question decomposition can be written with a vocabulary limited to words from the questions, their inﬂections, and function words, then it is an explicit question. If new content words must be introduced to describe the reasoning process, the question is implicit. Examples for implicit and explicit questions are in Table 2.
Deﬁnite A type of questions we wish to avoid are non-deﬁnitive questions, such as “Are hamburgers considered a sandwich?” and “Does chocolate taste better than vanilla?” for which there is no clear answer. We would like to col-

lect questions where the answer is deﬁnitive or, at least, very likely, based on the corpus. E.g., consider the question “Does wood conduct electricity?”. Although it is possible that a damp wood will conduct electricity, the answer is generally no.
To summarize, strategy questions are multi-step questions with implicit reasoning (a strategy) and a deﬁnitive answer that can be reached given a corpus. We limit ourselves to Boolean yes/no questions, which limits the output space, but lets us focus on the complexity of the questions, which is the key contribution. Example strategy questions are in Table 1, and examples that demonstrate the mentioned properties are in Table 2. Next (§2.2), we describe additional structures annotated during data collection.
2.2 Decomposing Strategy Questions
Strategy questions involve complex reasoning that leads to a yes/no answer. To guide and evaluate the QA process, we annotate every example with a description of the expected reasoning process.
Prior work used rationales or supporting facts,

i.e., text snippets extracted from the context (DeYoung et al., 2020; Yang et al., 2018; Kwiatkowski et al., 2019; Khot et al., 2020a) as evidence for an answer. However, reasoning can rely on elements that are not explicitly expressed in the context. Moreover, answering a question based on relevant context does not imply that the model performs reasoning properly (Jiang and Bansal, 2019).
Inspired by recent work (Wolfson et al., 2020), we associate every question-answer pair with a strategy question decomposition. A decomposition of a question q is a sequence of n steps s(1), s(2), ..., s(n) required for computing the answer to q. Each step s(i) corresponds to a singlestep question and may include special references, which are placeholders referring to the result of a previous step s(j). The last decomposition step (i.e. s(n)) returns the ﬁnal answer to the question. Table 3 shows decomposition examples.
Wolfson et al. (2020) targeted explicit multistep questions (ﬁrst row in Table 3), where the decomposition is restricted to a small vocabulary derived almost entirely from the original question. Conversely, decomposing strategy questions requires using implicit knowledge, and thus decompositions can include any token that is needed for describing the implicit reasoning (rows 2-4 in Table 3). This makes the decomposition task signiﬁcantly harder for strategy questions.
In this work, we distinguish between two types of required actions for executing a step. Retrieval: a step that requires retrieval from the corpus, and operation, a logical function over answers to previous steps. In the second row of Table 3, the ﬁrst two steps are retrieval steps, and the last step is an operation. A decomposition step can require both retrieval and an operation (see last row in Table 3).
To verify that steps are valid single-step questions that can be answered using the corpus (Wikipedia), we collect supporting evidence for each retrieval step and annotate operation steps. A supporting evidence is one or more paragraphs that provide an answer to the retrieval step.
In summary, each example in our dataset contains a) a strategy question, b) the strategy question decomposition, and c) supporting evidence per decomposition step. Collecting strategy questions and their annotations is the main challenge of this work, and we turn to this next.

Question
Did the Battle of Peleliu or the Seven Days Battles last longer?

Decomposition
(1) How long did the Battle of Peleliu last? (2) How long did the Seven Days Battle last? (3) Which is longer of #1 , #2?

Can the President of Mexico vote in New Mexico primaries?
Can a microwave melt a Toyota Prius battery?
Would it be common to ﬁnd a penguin in Miami?

(1) What is the citizenship requirement for voting in New Mexico? (2) What is the citizenship requirement of any President of Mexico? (3) Is #2 the same as #1?
(1) What kind of battery does a Toyota Prius use? (2) What type of material is #1 made out of? (3) What is the melting point of #2? (4) Can a microwave’s temperature reach at least #3?
(1) Where is a typical penguin’s natural habitat? (2) What conditions make #1 suitable for penguins? (3) Are all of #2 present in Miami?

Table 3: Explicit (row 1) and strategy (rows 2-4) question decompositions. We mark words that are explicit (italic) or implicit in the input (bold).

3 Data Collection Pipeline
Our goal is to establish a procedure for collecting strategy questions and their annotations at scale. To this end, we build a multi-step crowdsourcing1 pipeline designed for encouraging worker creativity, while preventing biases in the data.
We break the data collection into three tasks: question writing (§3.1), question decomposition (§3.2), and evidence matching (§3.3). In addition, we implement mechanisms for quality assurance (§3.4). An overview of the data collection pipeline is in Figure 2.
3.1 Creative Question Writing (CQW)
Generating natural language annotations through crowdsourcing (e.g., question generation) is known to suffer from several shortcomings. First, when annotators generate many instances, they use recurring patterns that lead to biases in the data. (Gururangan et al., 2018; Geva et al., 2019). Second, when language is generated conditioned on a long context, such as a paragraph, annotators use similar language (Kwiatkowski et al., 2019), leading to high lexical overlap and hence, inadvertently, to an easier problem. Moreover, a unique
1We use Amazon Mechanical Turk as our framework.

CQW

TERMS

T Silk A Yes

Q Is silk denatured by heat?

F1 Silk is a natural protein ﬁber F2 Protein is denatured by heat

SQD Q + A + F1 + F2

S1 What kind of ﬁber is silk made of? S2 Is #1 denatured by heat?

P1 Silk P2 Denaturation

EVM Q + A + S1 + S2

E1 ‘Silk’ is a natural protein ﬁber E2 ‘Denaturation’ is a process...

CORPUS

Figure 2: Overview of the data collection pipeline. First (CQW, §3.1), a worker is presented with a term (T) and an expected answer (A) and writes a question (Q) and the facts (F1,F2) required to answer it. Next, the question is decomposed (SQD, §3.2) into steps (S1, S2) along with Wikipedia page titles (P1,P2) that the worker expects to ﬁnd the answer in. Last (EVM, §3.3), decomposition steps are matched with evidence from Wikipedia (E1, E2).

property of our setup is that we wish to cover a broad and diverse set of strategies. Thus, we must discourage repeated use of the same strategy.
We tackle these challenges on multiple fronts. First, rather than using a long paragraph as context, we prime workers to write questions given single terms from Wikipedia, reducing the overlap with the context to a minimum. Second, to encourage diversity, we control the population of annotators, making sure a large number of annotators contribute to the dataset. Third, we use model-inthe-loop adversarial annotations (Dua et al., 2019; Khot et al., 2020a; Bartolo et al., 2020) to ﬁlter our questions, and only accept questions that fool our models. While some model-in-the-loop approaches use ﬁxed pre-trained models to eliminate “easy” questions, we continuously update the models during data collection to combat the use of repeated patterns or strategies.
We now provide a description of the task, and elaborate on these methods (Figure 2, upper row).
Task description Given a term (e.g., silk), a description of the term, and an expected answer (yes or no), the task is to write a strategy question about the term with the expected answer, and the facts required to answer the question.
Priming with Wikipedia terms Writing strategy questions from scratch is difﬁcult. To inspire worker creativity, we ask to write questions about terms they are familiar with or can easily understand. The terms are titles of “popular”2 Wikipedia pages. We provide workers only with a short description of the given term. Then, workers use their background knowledge and web search skills to form a strategy question.
2We ﬁlter pages based on the number of contributors and the number of backward links from other pages.

Controlling the answer distribution We ask workers to write questions where the answer is set to be ‘yes’ or ‘no’. To balance the answer distribution, the expected answer is dynamically sampled inversely proportional to the ratio of ‘yes’ and ‘no’ questions collected until that point.
Model-in-the-loop ﬁltering To ensure questions are challenging and reduce recurring language and reasoning patterns, questions are only accepted when veriﬁed by two sets of online solvers. We deploy a set of 5 pre-trained models (termed PTD) that check if the question is too easy. If at least 4 out of 5 answer the question correctly, it is rejected. Second, we use a set of 3 models (called FNTD) that are continuously ﬁne-tuned on our collected data and are meant to detect biases in the current question set. A question is rejected if all 3 solvers answer it correctly. The solvers are ROBERTA (Liu et al., 2019) models ﬁne-tuned on different auxiliary datasets; details in §5.1.
Auxiliary sub-task We ask workers to provide the facts required to answer the question they have written, for several reasons: 1) it helps workers frame the question writing task and describe the reasoning process they have in mind, 2) it helps reviewing their work, and 3) it provides useful information for the decomposition step (§3.2).
3.2 Strategy Question Decomposition (SQD)
Once a question and the corresponding facts are written, we generate the strategy question decomposition (Figure 2, middle row). We annotate decompositions before matching evidence in order to avoid biases stemming from seeing the context.
The decomposition strategy for a question is not always obvious, which can lead to undesirable explicit decompositions. For example, a possible ex-

plicit decomposition for Q1 (Figure 1) might be (1) What items did Aristotle use? (2) Is laptop in #1?; but the ﬁrst step is not feasible. To guide the decomposition, we provide workers with the facts written in the CQW task to show the strategy of the question author. Evidently, there can be many valid strategies and the same strategy can be phrased in multiple ways – the facts only serve as a soft guidance.
Task description Given a strategy question, a yes/no answer, and a set of facts, the task is to write the steps needed to answer the question.
Auxiliary sub-task We observe that in some cases, annotators write explicit decompositions, which often lead to infeasible steps that cannot be answered from the corpus. To help workers avoid explicit decompositions, we ask them to specify, for each decomposition step, a Wikipedia page they expect to ﬁnd the answer in. This encourages workers to write decomposition steps for which it is possible to ﬁnd answers in Wikipedia, and leads to feasible strategy decompositions, with only a small overhead (the workers are not required to read the proposed Wikipedia page).
3.3 Evidence Matching (EVM)
We now have a question and its decomposition. To ground them in context, we add a third task of evidence matching (Figure 2, bottom row).
Task description Given a question and its decomposition (a list of single-step questions), the task is to ﬁnd evidence paragraphs on Wikipedia for each retrieval step. Operation steps that do not require retrieval (§2.2) are marked as operation.
Controlling the matched context Workers search for evidence on Wikipedia. We index Wikipedia3 and provide a search interface where workers can drag-and-drop paragraphs from the results shown on the search interface. This guarantees that annotators choose paragraphs we included in our index, at a pre-determined paragraph-level granularity.
3.4 Data Veriﬁcation Mechanisms
Task qualiﬁcations For each task, we hold qualiﬁcations that test understanding of the task, and manually review several examples. Workers who follow the requirements are granted access to our
3We use the Wikipedia Cirrus dump from 11/05/2020.

tasks. Our qualiﬁcations are open to workers from English speaking countries who have high reputation scores. Additionally, the authors regularly review annotations to give feedback and prevent noisy annotations.
Real-time automatic checks For CQW, we use heuristics to check question validity, e.g., whether it ends with a question mark, and that it doesn’t use language that characterizes explicit multi-hop questions (for instance, having multiple verbs). For SQD, we check that the decomposition structure forms a directed acyclic graph, i.e. (i) each decomposition step is referenced by (at least) one of the following steps, such that all steps are reachable from the last step; and (ii) steps don’t form a cycle. In the EVM task, a warning message is shown when the worker marks an intermediate step as an operation (an unlikely scenario).
Inter-task feedback At each step of the pipeline, we collect feedback about previous steps. To verify results from the CQW task, we ask workers to indicate whether the given answer is incorrect (in the SQD, EVM tasks), or if the question is not deﬁnitive (in the SQD task) (§2.1). Similarly, to identify non-feasible questions or decompositions, we ask workers to indicate if there is no evidence for a decomposition step (in the EVM task).
Evidence veriﬁcation task After the EVM step, each example comprises a question, its answer, decomposition and supporting evidence. To verify that a question can be answered by executing the decomposition steps against the matched evidence paragraphs, we construct an additional evidence veriﬁcation task (EVV). In this task, workers are given a question, its decomposition and matched paragraphs, and are asked to answer the question in each decomposition step purely based on the provided paragraphs. Running EVV on a subset of examples during data collection, helps identify issues in the pipeline and in worker performance.
4 The STRATEGYQA Dataset
We run our pipeline on 1,799 Wikipedia terms, allowing a maximum of 5 questions per term. We update our online ﬁne-tuned solvers (FNTD) every 1K questions. Every question is decomposed once, and evidence is matched for each decomposition by 3 different workers. The cost of annotating a full example is $4.

# of questions % “yes” questions # of unique terms # of unique decomposition steps # of unique evidence paragraphs # of occurrences of the top trigram # of question writers # of ﬁltered questions Avg. question length (words) Avg. decomposition length (steps) Avg. # of paragraphs per question

Train
2290 46.8% 1333 6050 9251
31 23 2821 9.6 2.93 2.33

Test
490 46.1%
442 1347 2136
5 6 484 9.8 2.92 2.29

Table 4: STRATEGYQA statistics. Filtered questions were rejected by the solvers (§3.1). The train and test sets of question writers are disjoint. The “top trigram” is the most common trigram.

To encourage diversity in strategies used in the questions, we recruited new workers throughout data collection. Moreover, periodic updates of the online solvers prevent workers from exploiting shortcuts, since the solvers adapt to the training distribution. Overall, there were 29 question writers, 19 decomposers, and 54 evidence matchers participating in the data collection.
We collected 2,835 questions, out of which 55 were marked as having an incorrect answer during SQD (§3.2). This results in a collection of 2,780 veriﬁed strategy questions, for which we create an annotator-based data split (Geva et al., 2019). We now describe the dataset statistics (§4.1), analyze the quality of the examples, (§4.2) and explore the reasoning skills in STRATEGYQA (§4.3).
4.1 Dataset Statistics
We observe (Table 4) that the answer distribution is roughly balanced (yes/no). Moreover, questions are short (< 10 words), and the most common trigram occurs in roughly 1% of the examples. This indicates that the language of the questions is both simple and diverse. For comparison, the average question length in the multi-hop datasets HOTPOTQA (Yang et al., 2018) and COMPLEXWEBQUESTIONS (Talmor and Berant, 2018) is 13.7 words and 15.8 words, respectively. Likewise, the top trigram in these datasets occurs in 9.2% and 4.8% of their examples, respectively.
More than half of the generated questions are ﬁltered by our solvers, pointing to the difﬁculty of generating good strategy questions. We release all 3,305 ﬁltered questions as well.
To characterize the reasoning complexity required to answer questions in STRATEGYQA, we

proportion

0.5 0.4 0.3 0.2 0.1 0.0 1 2 3 4 5
decomposition length

train test 0#1ev2ide3nc4e p5ar6ag7rap8hs9

Figure 3: The distributions of decomposition length (left) and the number of evidence paragraphs (right). The majority of the questions in STRATEGYQA require a reasoning process comprised of ≥ 3 steps, of which about 2 steps involve retrieving external knowledge.

multi-step single-step

implicit

81

1

82

explicit

14.5

3.5

18

95.5

4.5

100

Table 5: Distribution over the implicit and multi-step properties (§2) in a sample of 100 STRATEGYQA questions, annotated by two experts (we average the expert decisions). Most questions are multi-step and implicit. Annotator agreement is substantial for both the implicit (κ = 0.73) and multi-step (κ = 0.65) properties.

examine the decomposition length and the number of evidence paragraphs. Figure 3 and Table 4 (bottom) show the distributions of these properties are centered around 3-step decompositions and 2 evidence paragraphs, but a considerable portion of the dataset requires more steps and paragraphs.
4.2 Data Quality
Do questions in STRATEGYQA require multistep implicit reasoning? To assess the quality of questions, we sampled 100 random examples from the training set, and had two experts (authors) independently annotate whether the questions satisfy the desired properties of strategy questions (§2.1). We ﬁnd that most of the examples (81%) are valid multi-step implicit questions, 82% of questions are implicit, and 95.5% are multi-step (Table 5).
Do questions in STRATEGYQA have a deﬁnitive answer? We let experts review the answers to 100 random questions, allowing access to the Web. We then ask them to state for every question whether they agree or disagree with the provided answer. We ﬁnd that the experts agree with the answer in 94% of the cases, and disagree only in 2%. For the remaining 4%, either the question was

ambiguous, or the annotators could not ﬁnd a deﬁnite answer on the Web. Overall, this suggests that questions in STRATEGYQA have clear answers.
What is the quality of the decompositions? We randomly sampled 100 decompositions and asked experts to judge their quality. Experts judged if the decomposition is explicit or utilizes a strategy. We ﬁnd that 83% of the decompositions validly use a strategy to break down the question. The remaining 17% decompositions are explicit, however, in 14% of the cases the original question is already explicit. Second, experts checked if the phrasing of the decomposition is “natural”, i.e., it reﬂects the decomposition of a person that does not already know the answer. We ﬁnd that 89% of the decompositions express a “natural” reasoning process, while 11% may depend on the answer. Last, we asked experts to indicate any potential logical ﬂaws in the decomposition, but no such cases occurred in the sample.
Would different annotators use the same decomposition strategy? We sample 50 examples, and let two different workers decompose the questions. Comparing the decomposition pairs, we ﬁnd that a) for all pairs, the last step returns the same answer, b) in 44 out of 50 pairs, the decomposition pairs follow the same reasoning path , and c) in the other 6 pairs, the decompositions either follow a different reasoning process (5 pairs) or one of the decompositions is explicit (1 pair). This shows that different workers usually use the same strategy when decomposing questions.
Is the evidence for strategy questions in Wikipedia? Another important property is whether questions in STRATEGYQA can be answered based on context from our corpus, Wikipedia, given that questions are written independently of the context. To measure evidence coverage, in the EVM task (§3.3), we provide workers with a checkbox for every decomposition step, indicating whether only partial or no evidence could be found for that step. Recall that three different workers match evidence for each decomposition step. We ﬁnd that 88.3% of the questions are fully covered: evidence was matched for each step by some worker. Moreover, in 86.9% of the questions, at least one worker found evidence for all steps. Last, in only 0.5% of the examples, all three annotators could not match evidence for any of the steps. This suggests

that overall, Wikipedia is a good corpus for questions in STRATEGYQA, that were written independently of the context.
Do matched paragraphs provide evidence? We assess the quality of matched paragraphs by analyzing both example-level and step-level annotations. First, we sample 217 decomposition steps with their corresponding paragraphs matched by one of the three workers. We let 3 different crowdworkers decide whether the paragraphs provide evidence for the answer to that step. We ﬁnd that in 93% of the cases, the majority vote is that the evidence is valid.4
Next, we analyze annotations of the veriﬁcation task (§3.4), where workers are asked to answer all decomposition steps based only on the matched paragraphs. We ﬁnd that the workers could answer sub-questions and derive the correct answer in 82 out of 100 annotations. Moreover, in 6 questions indeed there was an error in evidence matching, but another worker that annotated the example was able to compensate for the error, leading to 88% of the questions where evidence matching succeeds. In the last 12 cases indeed evidence is missing, and is possibly absent from Wikipedia.
Lastly, we let experts review the paragraphs matched by one of the three workers to all the decomposition steps of a question, for 100 random questions. We ﬁnd that for 79 of the questions the matched paragraphs provide sufﬁcient evidence for answering the question. For 12 of the 21 questions without sufﬁcient evidence, the experts indicated they would expect to ﬁnd evidence in Wikipedia, and the worker probably could not ﬁnd it. For the remaining 9 questions, they estimated that evidence is probably absent from Wikipedia.
In conclusion, 93% of the paragraphs matched at the step-level were found to be valid. Moreover, when considering single-worker annotations, ∼80% of the questions are matched with paragraphs that provide sufﬁcient evidence for all retrieval steps. This number increases to 88% when aggregating the annotations of three workers.
Do different annotators match the same evidence paragraphs? To compare the evidence paragraphs matched by different workers, we check whether for a given decomposition step, the same paragraph IDs are retrieved by different annotators. Given two non-empty sets of paragraph
4With moderate annotator agreement of κ = 0.42.

Strategy

Example

%

Physical

“Can human nails carve a statue 13

out of quartz?”

Biological

“Is a platypus immune from 11

cholera?”

Historical

“Were mollusks an ingredient in 10

the color purple?”

Temporal

“Did the 40th president of the 10

United States forward lolcats to

his friends?”

Deﬁnition

“Are quadrupeds represented on 8

Chinese calendar?”

Cultural

“Would a compass attuned to 5

Earth’s magnetic ﬁeld be a bad

gift for a Christmas elf?”

Religious

“Was Hillary Clinton’s deputy 5

chief of staff in 2009 baptised?”

Entertainment “Would Garﬁeld enjoy a trip to 4

Italy?”

Sports

“Can Larry King’s ex-wives form 4

a water polo team?”

Table 6: Top strategies in STRATEGYQA and their frequency in a 100 example subset (accounting for 70% of the analyzed examples).

IDs P1, P2, annotated by two workers, we compute the Jaccard coefﬁcient J (P1, P2) = ||PP11∪∩PP22|| . In addition, we take the sets of corresponding Wikipedia page IDs T1, T2 for the matched paragraphs, and compute J(T1, T2). Note that a score of 1 is given to two identical sets, while a score of 0 corresponds to sets that are disjoint. The average similarity score is 0.43 for paragraphs and 0.69 for pages. This suggests that evidence for a decomposition step can be found in more than one paragraph in the same page, or in different pages.
4.3 Data Diversity
We aim to generate creative and diverse questions. We now analyze diversity in terms of the required reasoning skills and question topic.
Reasoning skills To explore the required reasoning skills in STRATEGYQA, we sampled 100 examples and let two experts (authors) discuss and annotate each example with a) the type of strategy for decomposing the question, and b) the required reasoning and knowledge skills per decomposition step. We then aggregate similar labels (e.g. botanical → biological) and compute the proportion of examples each strategy/reasoning skill is required for (an example can have multiple strategy labels).
Table 6 demonstrates the top strategies, showing that STRATEGYQA contains a broad set of strategies. Moreover, diversity is apparent (Fig-

commonsense 2%

mechanical 3%

botanical 4%
law 2%

life-style 3%

is member
of 10%

medical 4%

cultural 7%

political 5%

chemistry 1%
has parts 7%

preconditions
4%

semantic similarity
3%

physical 12%

set inclusion
19%

entertainment
10%

education 4%
language 1%

temporal 9%

entity comparison
6% spatial
4%

historical 34%

biological 25%

religious 6%
deﬁnition 8%

sports 9%

astrology 1%

cause &
eﬀect 4%

technological
14%

activity 2%

contradiction
1%

pop-culture 4%

location comparison
1%

number comparison
18%

geographical 13%

literature 4%

food 8%

semantic

equivalence

temporal

3%

com

pari

son

int

set ersec

tion

7%

4%

set comparison
1%

music 5%

Figure 4: Reasoning skills in STRATEGYQA; each skill is associated with the proportion of examples it is required for. Domain-related and logical reasoning skills are marked in blue and orange (italic), respectively.

proportion (%)

10 5 0

human taxon profession disease type of sport food ingredient anatomical structure group of organisms chemical element business television series
war chemical compound
film ethnic group

Figure 5: The top 15 categories of terms used to prime workers for question writing and their proportion.
ure 4) in terms of both domain-related reasoning (e.g. biological and technological) and logical functions (e.g. set inclusion and “is member of”). While the reasoning skills sampled from questions in STRATEGYQA do not necessarily reﬂect their prevalence in a “natural” distribution, we argue that promoting research on methods for inferring strategies is an important research direction.
Question topics As questions in STRATEGYQA were triggered by Wikipedia terms, we use the “instance of” Wikipedia property to characterize the topics of questions.5 Figure 5 shows the distribution of topic categories in STRATEGYQA. The distribution shows STRATEGYQA is very diverse, with the top two categories (“human” and “taxon”, i.e. a group of organisms) covering only a quarter
5It is usually a 1-to-1 mapping from a term to a Wikipedia category. In cases of 1-to-many, we take the ﬁrst category.

Answer accuracy

87%

Strategy match

86%

Decomposition usage 14%

Average # searches 1.25

Table 7: Human performance in answering questions. Strategy match is computed by comparing the explanation provided by the expert with the decomposition. Decomposition usage and the number of searches are computed based on information provided by the expert.

of the data, and a total of 609 topic categories. We further compare the diversity of STRATE-
GYQA to HOTPOTQA, a multi-hop QA dataset over Wikipedia paragraphs. To this end, we sample 739 pairs of evidence paragraphs associated with a single question in both datasets, and map the pair of paragraphs to a pair of Wikipedia categories using the “instance of” property. We ﬁnd that there are 571 unique category pairs in STRATEGYQA, but only 356 unique category pairs in HOTPOTQA. Moreover, the top two category pairs in both of the datasets (“human-human”, “taxon-taxon”) constitute 8% and 27% of the cases in STRATEGYQA and HOTPOTQA, respectively. This demonstrates the creativity and breadth of category combinations in STRATEGYQA.
4.4 Human Performance
To see how well humans answer strategy questions, we sample a subset of 100 questions from STRATEGYQA and have experts (authors) answer questions, given access to Wikipedia articles and an option to reveal the decomposition for every question. In addition, we ask them to provide a short explanation for the answer, the number of searches they conducted to derive the answer, and to indicate whether they have used the decomposition. We expect humans to excel at coming up with strategies for answering questions. Yet, humans are not necessarily an upper bound because ﬁnding the relevant paragraphs is difﬁcult and could potentially be performed better by machines.
Table 7 summarizes the results. Overall, humans infer the required strategy and answer the questions with high accuracy. Moreover, the low number of searches shows that humans leverage background knowledge, as they can answer some of the intermediate steps without search. An error analysis shows that the main reason for failure (10%) is difﬁculty to ﬁnd evidence, and the rest of the cases (3%) are due to ambiguity in the question that could lead to the opposite answer.

5 Experimental Evaluation
In this section, we conduct experiments to answer the following questions: a) How well do pretrained language models (LMs) answer strategy questions? b) Is retrieval of relevant context helpful? and c) Are decompositions useful for answering questions that require implicit knowledge?
5.1 Baseline Models
Answering strategy questions requires external knowledge that cannot be obtained by training on STRATEGYQA alone. Therefore, our models and online solvers (§3.1) are based on pre-trained LMs, ﬁne-tuned on auxiliary datasets that require reasoning. Speciﬁcally, in all models we ﬁne-tune ROBERTA (Liu et al., 2019) on a subset of:
• BOOLQ (Clark et al., 2019): A dataset for boolean question answering.
• MNLI (Williams et al., 2018): A large natural language inference (NLI) dataset. The task is to predict if a textual premise entails, contradicts or is neutral with respect to the hypothesis.
• TWENTY QUESTIONS (20Q): A collection of 50K short commonsense boolean questions.6
• DROP (Dua et al., 2019): A large dataset for numerical reasoning over paragraphs.
Models are trained in two conﬁgurations: • No context : The model is fed with the question
only, and outputs a binary prediction using the special CLS token. • With context : We use BM25 (Robertson et al., 1995) to retrieve context from our corpus, while removing stop words from all queries. We examine two retrieval methods: a) question-based retrieval: by using the question as a query and taking the top k = 10 results, and b) decomposition-based retrieval: by initiating a separate query for each (gold or predicted) decomposition step and concatenating the top k = 10 results of all steps (sorted by retrieval score). In both cases, the model is fed with the question concatenated to the retrieved context, truncated to 512 tokens (the maximum input length of ROBERTA), and outputs a binary prediction.
Predicting decompositions We train a seq-toseq model, termed BARTDECOMP, that given a question, generates its decomposition token-by-
6https://github.com/allenai/ twentyquestions

Model
RO B E RTA∅ ( 2 0 Q ) RO B E RTA∅ ( 2 0 Q + B O O L Q ) RO B E RTA∅ ( B O O L Q ) ROBERTAIR-Q (BOOLQ) ROBERTAIR-Q (MNLI+BOOLQ)

Solver group(s) PTD, FNTD PTD, FNTD PTD, FNTD PTD PTD

Table 8: QA models used as online solvers during data collection (§3.1). Each model was ﬁne-tuned on the datasets mentioned in its name.

token. Speciﬁcally, we ﬁne-tune BART (Lewis et al., 2020) on STRATEGYQA decompositions.
Baseline models As our base model, we train a model as follows: We take a ROBERTA (Liu et al., 2019) model and ﬁne-tune it on DROP, 20Q and BOOLQ (in this order). The model is trained on DROP with multiple output heads, as in Segal et al. (2020), which are then replaced with a single Boolean output.7 We call this model ROBERTA*.
We use ROBERTA* and ROBERTA to train the following models on STRATEGYQA: without context (ROBERTA*∅), with question-based retrieval (ROBERTA*IR-Q, ROBERTAIR-Q), and with predicted decomposition-based retrieval (RO B E RTA * I R - D ).
We also present four oracle models: • ROBERTA*ORA-P: uses the gold paragraphs
(no retrieval). • ROBERTA*IR-ORA-D: performs retrieval with
the gold decomposition. • ROBERTA*lOaRstA-s-tePp-D: exploits both the gold de-
composition and the gold paragraphs. We ﬁnetune ROBERTA on BOOLQ and SQUAD (Rajpurkar et al., 2016) to obtain a model that can answer single-step questions. We then run this model on STRATEGYQA to obtain answers for all decomposition sub-questions, and replace all placeholder references with the predicted answers. Last, we ﬁne-tune ROBERTA* to answer the last decomposition step of STRATEGYQA, for which we have supervision. • ROBERTA*lOaRstA-s-tePp-D-raw: ROBERTA* that is ﬁnetuned to predict the answer from the gold paragraphs and the last step of the gold decomposition, without replacing placeholder references.
Online solvers For the solvers integrated in the data collection process (§3.1), we use three nocontext models and two question-based retrieval models. The solvers are listed in Table 8.
7For brevity, exact details on model training and hyperparameters will be released as part of our codebase.

Model
MAJORITY ROBERTA*∅ ROBERTAIR-Q ROBERTA*IR-Q ROBERTA*IR-D
ROBERTA*IR-ORA-D ROBERTA*ORA-P ROBERTA*lOasRt-Ast-ePp--Draw ROBERTA*lOasRt-Ast-ePp-D

Accuracy
53.9 63.6 ± 1.3 53.6 ± 1.0 63.6 ± 1.0 61.7 ± 2.2
62.0 ± 1.3 70.7 ± 0.6 65.2 ± 1.4 72.0 ± 1.0

Recall@10
0.174 0.174 0.195
0.282 -

Table 9: QA accuracy (with standard deviation across 7 experiments), and retrieval performance, measured by Recall@10, of baseline models on the test set.

5.2 Results
Strategy QA performance Table 9 summarizes the results of all models (§5.1). ROBERTA*IR-Q substantially outperforms ROBERTAIR-Q, indicating that ﬁne-tuning on related auxiliary datasets before STRATEGYQA is crucial. Hence, we focus on ROBERTA* for all other results and analysis.
Strategy questions pose a combined challenge of retrieving the relevant context, and deriving the answer based on that context. Training without context shows a large accuracy gain of 53.9 → 63.6 over the majority baseline. This is far from human performance, but shows that some questions can be answered by a large LM ﬁne-tuned on related datasets without retrieval. On the other end, training with gold paragraphs raises performance to 70.7. This shows that high-quality retrieval lets the model effectively reason over the given paragraphs. Last, using both gold decompositions and retrieval further increases performance to 72.0, showing the utility of decompositions.
Focusing on retrieval-based methods, we observe that question-based retrieval reaches an accuracy of 63.6 and retrieval with gold decompositions results in an accuracy of 62.0. This shows that the quality of retrieval even with gold decompositions is not high enough to improve the 63.6 accuracy obtained by ROBERTA*∅, a model that uses no context. Retrieval with predicted decompositions results in an even lower accuracy of 61.7. We also analyze predicted decompositions below.
Retrieval evaluation A question decomposition describes the reasoning steps for answering the question. Therefore, using the decomposition for retrieval may help obtain the relevant context and improve performance. To test this, we directly compare performance of question- and

decomposition-based retrieval with respect to the annotated gold paragraphs. We compute Recall@10, i.e., the fraction of the gold paragraphs retrieved in the top-10 results of each method. Since there are 3 annotations per question, we compute Recall@10 for each annotation and take the maximum as the ﬁnal score. For a fair comparison, in decomposition-based retrieval, we use the top-10 results across all steps.
Results (Table 9) show that retrieval performance is low, partially explaining why retrieval models do not improve performance compared to ROBERTA*∅, and demonstrating the retrieval challenge in our setup. Gold decompositionbased retrieval substantially outperforms questionbased retrieval, showing that using the decomposition for retrieval is a promising direction for answering multi-step questions. Still, predicted decomposition-based retrieval does not improve retrieval compared to question-based retrieval, showing better decomposition models are needed.
To understand the low retrieval scores, we analyzed the query results of 50 random decomposition steps. Most failure cases are due to the shallow pattern matching done by BM25, e.g., failure to match synonyms. This shows that indeed there is little word overlap between decomposition steps and the evidence, as intended by our pipeline design. In other examples, either a key question entity was missing because it was represented by a reference token, or the decomposition step had complex language, leading to failed retrieval. This analysis suggests that advances in neural retrieval might be beneﬁcial for STRATEGYQA.
Human retrieval performance To quantify human performance in ﬁnding gold paragraphs, we ask experts to ﬁnd evidence paragraphs for 100 random questions. For half of the questions we also provide decomposition. We observe average Recall@10 of 0.586 and 0.513 with and without the decomposition, respectively. This shows that humans signiﬁcantly outperform our IR baselines. However, humans are still far from covering the gold paragraphs, since there are multiple valid evidence paragraphs (§4.2), and retrieval can be difﬁcult even for humans. Lastly, using decompositions improves human retrieval, showing decompositions indeed are useful for ﬁnding evidence.
Predicted decompositions Analysis shows that BARTDECOMP’s decompositions are grammati-

cal and well-structured. Interestingly, the model generates strategies, but often applies them to questions incorrectly. E.g., the question “Can a lifeboat rescue people in the Hooke Sea?” is decomposed to “1) What is the maximum depth of the Hooke Sea? 2) How deep can a lifeboat dive? 3) Is #2 greater than or equal to #1?”. While the decomposition is well-structured, it uses a wrong strategy (lifeboats do not dive).
6 Related Work
Prior work has typically let annotators write questions based on an entire context (Khot et al., 2020a; Yang et al., 2018; Dua et al., 2019; Mihaylov et al., 2018; Khashabi et al., 2018). In this work, we prime annotators with minimal information (few tokens) and let them use their imagination and own wording to create questions. A related priming method was recently proposed by Clark et al. (2020), who used the ﬁrst 100 characters of a Wikipedia page.
Among multi-hop reasoning datasets, our dataset stands out in that it requires implicit decompositions. Two recent datasets (Khot et al., 2020a; Mihaylov et al., 2018) have considered questions requiring implicit facts. However, they are limited to speciﬁc domain strategies, while in our work we seek diversity in this aspect.
Most multi-hop reasoning datasets do not fully annotate question decomposition (Yang et al., 2018; Khot et al., 2020a; Mihaylov et al., 2018). This issue has prompted recent work to create question decompositions for existing datasets (Wolfson et al., 2020), and to train models that generate question decompositions (Perez et al., 2020; Khot et al., 2020b; Min et al., 2019). In this work, we annotate question decompositions as part of the data collection.
7 Conclusion
We present STRATEGYQA, the ﬁrst dataset of implicit multi-step questions requiring a wide-range of reasoning skills. To build STRATEGYQA, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies. Questions in STRATEGYQA are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.

Acknowledgement
We thank Tomer Wolfson for helpful feedback and the REVIZ team at Allen Institute for AI, particularly Michal Guerquin and Sam Skjonsberg. This research was supported in part by the Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800). Dan Roth is partly supported by ONR contract N00014-191-2620 and DARPA contract FA8750-19-2-1004, under the Kairos program. This work was completed in partial fulﬁllment for the Ph.D degree of Mor Geva.
References
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662–678.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Minnesota. Association for Computational Linguistics.
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics (TACL), 8:454–470.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443–4458. Association for Computational Linguistics.

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics (NAACL).
Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? An investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1161–1166, Hong Kong, China. Association for Computational Linguistics.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107–112, New Orleans, Louisiana. Association for Computational Linguistics.
Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. In Association for Computational Linguistics (ACL).
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252–262, New Orleans, Louisiana. Association for Computational Linguistics.
Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020a. QASC: A dataset for question answering via sentence composition. In AAAI.
Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2020b.

Text modular networks: Learning to decompose tasks in the language of existing models. arXiv preprint arXiv:2009.00751.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics (TACL), 7:453–466.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880. Association for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, Brussels, Belgium. Association for Computational Linguistics.
Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097–6109, Florence, Italy. Association for Computational Linguistics.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and

Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Workshop on Cognitive Computing at NIPS.
Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864–8880.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).
Stephen Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. 1995. Okapi at TREC-3. In Overview of the Third Text REtrieval Conference (TREC-3), pages 109– 126. Gaithersburg, MD: NIST.
Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2020. A simple and effective model for answering multispan questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3074– 3080.
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418–6428, Florence, Italy. Association for Computational Linguistics.
Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In North American Chapter of the Association for Computational Linguistics (NAACL).
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multihop reading comprehension across documents. Transactions of the Association for Computational Linguistics (TACL), 6:287–302.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus

for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics (TACL).
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Empirical Methods in Natural Language Processing (EMNLP).

